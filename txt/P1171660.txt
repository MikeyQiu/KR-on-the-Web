9
1
0
2
 
r
p
A
 
3
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
0
0
3
0
.
9
0
8
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2019

TRAINING FOR FASTER ADVERSARIAL ROBUSTNESS
VERIFICATION VIA INDUCING RELU STABILITY

Kai Y. Xiao Vincent Tjeng Nur Muhammad (Mahi) Shaﬁullah Aleksander M ˛adry
Massachusetts Institute of Technology
Cambridge, MA 02139
{kaix, vtjeng, nshafiul, madry}@mit.edu

ABSTRACT

We explore the concept of co-design in the context of neural network veriﬁcation.
Speciﬁcally, we aim to train deep neural networks that not only are robust to ad-
versarial perturbations but also whose robustness can be veriﬁed more easily. To
this end, we identify two properties of network models – weight sparsity and so-
called ReLU stability – that turn out to signiﬁcantly impact the complexity of the
corresponding veriﬁcation task. We demonstrate that improving weight sparsity
alone already enables us to turn computationally intractable veriﬁcation problems
into tractable ones. Then, improving ReLU stability leads to an additional 4–13x
speedup in veriﬁcation times. An important feature of our methodology is its “uni-
versality,” in the sense that it can be used with a broad range of training procedures
and veriﬁcation approaches.

1

INTRODUCTION

Deep neural networks (DNNs) have recently achieved widespread success in image classiﬁcation
(Krizhevsky et al., 2012), face and speech recognition (Taigman et al., 2014; Hinton et al., 2012),
and game playing (Silver et al., 2016; 2017). This success motivates their application in a broader
set of domains, including more safety-critical environments. This thrust makes understanding the
reliability and robustness of the underlying models, let alone their resilience to manipulation by
malicious actors, a central question. However, predictions made by machine learning models are
often brittle. A prominent example is the existence of adversarial examples (Szegedy et al., 2014):
imperceptibly modiﬁed inputs that cause state-of-the-art models to misclassify with high conﬁdence.

There has been a long line of work on both generating adversarial examples, called attacks (Carlini
and Wagner, 2017b;a; Athalye et al., 2018a;b; Uesato et al., 2018; Evtimov et al., 2017), and training
models robust to adversarial examples, called defenses (Goodfellow et al., 2015; Papernot et al.,
2016; Madry et al., 2018; Kannan et al., 2018). However, recent research has shown that most
defenses are ineffective (Carlini and Wagner, 2017a; Athalye et al., 2018a; Uesato et al., 2018).
Furthermore, even for defenses such as that of Madry et al. (2018) that have seen empirical success
against many attacks, we are unable to conclude yet with certainty that they are robust to all attacks
that we want these models to be resilient to.

This state of affairs gives rise to the need for veriﬁcation of networks, i.e., the task of formally
proving that no small perturbations of a given input can cause it to be misclassiﬁed by the network
model. Although many exact veriﬁers1 have been designed to solve this problem, the veriﬁcation
process is often intractably slow. For example, when using the Reluplex veriﬁer of Katz et al. (2017),
even verifying a small MNIST network turns out to be computationally infeasible. Thus, addressing
this intractability of exact veriﬁcation is the primary goal of this work.

Our Contributions
Our starting point is the observation that, typically, model training and veriﬁcation are decoupled
and seen as two distinct steps. Even though this separation is natural, it misses a key opportunity:
the ability to align these two stages. Speciﬁcally, applying the principle of co-design during model

1Also sometimes referred to as combinatorial veriﬁers.

1

Published as a conference paper at ICLR 2019

training is possible: training models in a way to encourage them to be simultaneously robust and
easy-to-exactly-verify. This insight is the cornerstone of the techniques we develop in this paper.

In this work, we use the principle of co-design to develop training techniques that give models
that are both robust and easy-to-verify. Our techniques rely on improving two key properties of
networks: weight sparsity and ReLU stability. Speciﬁcally, we ﬁrst show that natural methods for
improving weight sparsity during training, such as (cid:96)1-regularization, give models that can already be
veriﬁed much faster than current methods. This speedup happens because in general, exact veriﬁers
beneﬁt from having fewer variables in their formulations of the veriﬁcation task. For instance, for
exact veriﬁers that rely on linear programming (LP) solvers, sparser weight matrices means fewer
variables in those constraints.

We then focus on the major speed bottleneck of current approaches to exact veriﬁcation of ReLU
networks: the need of exact veriﬁcation methods to “branch,” i.e., consider two possible cases for
each ReLU (ReLU being active or inactive). Branching drastically increases the complexity of
veriﬁcation. Thus, well-optimized veriﬁers will not need to branch on a ReLU if it can determine
that the ReLU is stable, i.e.
that the ReLU will always be active or always be inactive for any
perturbation of an input. This motivates the key goal of the techniques presented in this paper: we
aim to minimize branching by maximizing the number of stable ReLUs. We call this goal ReLU
stability and introduce a regularization technique to induce it.

Our techniques enable us to train weight-sparse and ReLU stable networks for MNIST and CIFAR-
10 that can be veriﬁed signiﬁcantly faster. Speciﬁcally, by combining natural methods for inducing
weight sparsity with a robust adversarial training procedure (cf. Goodfellow et al. (2015)), we are
able to train networks for which almost 90% of inputs can be veriﬁed in an amount of time that
is small2 compared to previous veriﬁcation techniques. Then, by also adding our regularization
technique for inducing ReLU stability, we are able to train models that can be veriﬁed an addi-
tional 4–13x times as fast while maintaining state-of-the-art accuracy on MNIST. Our techniques
show similar improvements for exact veriﬁcation of CIFAR models. In particular, we achieve the
following veriﬁcation speed and provable robustness results for (cid:96)∞ norm-bound adversaries:

Dataset

Epsilon

Provable Adversarial Accuracy Average Solve Time (s)

MNIST

(cid:15) = 0.1
(cid:15) = 0.2
(cid:15) = 0.3

CIFAR

(cid:15) = 2/255
(cid:15) = 8/255

94.33%
89.79%
80.68%

45.93%
20.27%

0.49
1.13
2.78

13.50
22.33

Our network for (cid:15) = 0.1 on MNIST achieves provable adversarial accuracies comparable with the
current best results of Wong et al. (2018) and Dvijotham et al. (2018), and our results for (cid:15) = 0.2 and
(cid:15) = 0.3 achieve the best provable adversarial accuracies yet. To the best of our knowledge, we also
achieve the ﬁrst nontrivial provable adversarial accuracy results using exact veriﬁers for CIFAR-10.

Finally, we design our training techniques with universality as a goal. We focus on improving the
input to the veriﬁcation process, regardless of the veriﬁer we end up using. This is particularly
important because research into network veriﬁcation methods is still in its early stages, and our co-
design methods are compatible with the best current veriﬁers (LP/MILP-based methods) and should
be compatible with any future improvements in veriﬁcation.

Our code is available at https://github.com/MadryLab/relu_stable.

2 BACKGROUND AND RELATED WORK

Exact veriﬁcation of networks has been the subject of many recent works (Katz et al., 2017; Ehlers,
2017; Carlini et al., 2017; Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). To
understand the context of these works, observe that for linear networks, the task of exact veriﬁcation
is relatively simple and can be done by solving a LP. For more complex models, the presence of
nonlinear ReLUs makes veriﬁcation over all perturbations of an input much more challenging. This

2We chose our time budget for veriﬁcation to be 120 seconds per input image.

2

Published as a conference paper at ICLR 2019

is so as ReLUs can be active or inactive depending on the input, which can cause exact veriﬁers
to “branch" and consider these two cases separately. The number of such cases that veriﬁers have
to consider might grow exponentially with the number of ReLUs, so veriﬁcation speed will also
grow exponentially in the worst case. Katz et al. (2017) further illustrated the difﬁculty of exact
veriﬁcation by proving that it is NP-complete. In recent years, formal veriﬁcation methods were
developed to verify networks. Most of these methods use satisﬁability modulo theory (SMT) solvers
(Katz et al., 2017; Ehlers, 2017; Carlini et al., 2017) or LP and Mixed-Integer Linear Programming
(MILP) solvers (Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). However,
all of them are limited by the same issue of scaling poorly with the number of ReLUs in a network,
making them prohibitively slow in practice for even medium-sized models.

One recent approach for dealing with the inefﬁciency of exact veriﬁers is to focus on certiﬁcation
methods3 (Wong and Kolter, 2018; Wong et al., 2018; Dvijotham et al., 2018; Raghunathan et al.,
2018; Mirman et al., 2018; Sinha et al., 2018). In contrast to exact veriﬁcation, these methods do not
solve the veriﬁcation task directly; instead, they rely on solving a relaxation of the veriﬁcation prob-
lem. This relaxation is usually derived by overapproximating the adversarial polytope, or the space
of outputs of a network for a region of possible inputs. These approaches rely on training models in
a speciﬁc manner that makes certiﬁcation of those models easier. As a result, they can often obtain
provable adversarial accuracy results faster. However, certiﬁcation is fundamentally different from
veriﬁcation in two primary ways. First, it solves a relaxation of the original veriﬁcation problem. As
a result, certiﬁcation methods can fail to certify many inputs that are actually robust to perturbations
– only exact veriﬁers, given enough time, can give conclusive answers on robustness for every single
input. Second, certiﬁcation approaches fall under the paradigm of co-training, where a certiﬁcation
method only works well on models speciﬁcally trained for that certiﬁcation step. When used as a
black box on arbitrary models, the certiﬁcation step can yield a high rate of false negatives. For
example, Raghunathan et al. (2018) found that their certiﬁcation step was signiﬁcantly less effective
when used on a model trained using Wong and Kolter (2018)’s training method, and vice versa. In
contrast, we design our methods to be universal. They can be combined with any standard train-
ing procedure for networks and will improve exact veriﬁcation speed for any LP/MILP-based exact
veriﬁer. Our methods can also decrease the amount of overapproximation incurred by certiﬁcation
methods like Wong and Kolter (2018); Dvijotham et al. (2018). Similar to most of the certiﬁcation
methods, our technique can be made to have very little training time overhead.

Finally, subsequent work of Gowal et al. (2018) shows how applying interval bound propagation
during training, combined with MILP-based exact veriﬁcation, can lead to provably robust networks.

3 TRAINING VERIFIABLE NETWORK MODELS

We begin by discussing the task of verifying a network and identify two key properties of networks
that lead to improved veriﬁcation speed: weight sparsity and so-called ReLU stability. We then use
natural regularization methods for inducing weight sparsity as well as a new regularization method
for inducing ReLU stability. Finally, we demonstrate that these methods signiﬁcantly speed up
veriﬁcation while maintaining state-of-the-art accuracy.

3.1 VERIFYING ADVERSARIAL ROBUSTNESS OF NETWORK MODELS

Deep neural network models. Our focus will be on one of the most common architectures for
state-of-the-art models: k-layer fully-connected feed-forward DNN classiﬁers with ReLU non-
linearities4. Such models can be viewed as a function f (·, W, b), where W and b represent the
weight matrices and biases of each layer. For an input x, the output f (x, W, b) of the DNN is

3These works use both “veriﬁcation” and “certiﬁcation” to describe their methods. For clarity, we use “cer-
tiﬁcation” to describe their approaches, while we use “veriﬁcation” to describe exact veriﬁcation approaches.
For a more detailed discussion of the differences, see Appendix F.

4Note that this encompasses common convolutional network architectures because every convolutional layer

can be replaced by an equivalent fully-connected layer.

3

Published as a conference paper at ICLR 2019

deﬁned as:

z0 = x
ˆzi = zi−1Wi + bi
zi = max( ˆzi, 0)

(1)
(2)
(3)
(4)
Here, for each layer i, we let ˆzij denote the jth ReLU pre-activation and let ˆzij(x) denote the value
of ˆzij on an input x. ˆzk−1 is the ﬁnal, output layer with an output unit for each possible label (the
logits). The network will make predictions by selecting the label with the largest logit.

for i = 1, 2, . . . , k − 1
for i = 1, 2, . . . , k − 2

f (x, W, b) = ˆzk−1

Adversarial robustness. For a network to be reliable, it should make predictions that are robust
– that is, it should predict the same output for inputs that are very similar. Speciﬁcally, we want
the DNN classiﬁer’s predictions to be robust to a set Adv(x) of possible adversarial perturbations
of an input x. We focus on (cid:96)∞ norm-bound adversarial perturbations, where Adv(x) = {x(cid:48)
:
||x(cid:48) − x||∞ ≤ (cid:15)} for some constant (cid:15), since it is the most common one considered in adversarial
robustness and veriﬁcation literature (thus, it currently constitutes a canonical benchmark). Even so,
our methods can be applied to other (cid:96)p norms and broader sets of perturbations.
Veriﬁcation of network models. For an input x with correct label y, a perturbed input x(cid:48) can cause
a misclassiﬁcation if it makes the logit of some incorrect label ˆy larger than the logit of y on x(cid:48). We
can thus express the task of ﬁnding an adversarial perturbation as the optimization problem:

f (x(cid:48), W )y − f (x(cid:48), W )ˆy

min
x(cid:48),ˆy
subject to x(cid:48) ∈ Adv(x)

An adversarial perturbation exists if and only if the objective of the optimization problem is negative.

Adversarial accuracies. We deﬁne the true adversarial accuracy of a model to be the fraction of
test set inputs for which the model is robust to all allowed perturbations. By deﬁnition, evaluations
against speciﬁc adversarial attacks like PGD or FGSM provide an upper bound to this accuracy,
while certiﬁcation methods provide lower bounds. Given sufﬁcient time for each input, an exact
veriﬁer can prove robustness to perturbations, or ﬁnd a perturbation where the network makes a
misclassiﬁcation on the input, and thus exactly determine the true adversarial accuracy. This is in
contrast to certiﬁcation methods, which solve a relaxation of the veriﬁcation problem and can not
exactly determine the true adversarial accuracy no matter how much time they have.

However, such exact veriﬁcation may take impractically long for certain inputs, so we instead com-
pute the provable adversarial accuracy, which we deﬁne as the fraction of test set inputs for which
the veriﬁer can prove robustness to perturbations within an allocated time budget (timeout). Simi-
larly to certiﬁable accuracy, this accuracy constitutes a lower bound on the true adversarial accuracy.
A model can thus, e.g., have high true adversarial accuracy and low provable adversarial accuracy if
veriﬁcation of the model is too slow and often fails to complete before timeout.

Also, in our evaluations, we chose to use the MILP exact veriﬁer of Tjeng et al. (2019) when per-
forming experiments, as it is both open source and the fastest veriﬁer we are aware of.

3.2 WEIGHT SPARSITY AND ITS IMPACT ON VERIFICATION SPEED

The ﬁrst property of network models that we want to improve in order to speed up exact veriﬁcation
of those models is weight sparsity. Weight sparsity is important for veriﬁcation speed because many
exact veriﬁers rely on solving LP or MILP systems, which beneﬁt from having fewer variables.
We use two natural regularization methods for improving weight sparsity: (cid:96)1-regularization and
small weight pruning. These techniques signiﬁcantly improve veriﬁcation speed – see Table 1.
Verifying even small MNIST networks is almost completely intractable without them. Speciﬁcally,
the veriﬁer can only prove robustness of an adversarially-trained model on 19% of inputs with a one
hour budget per input, while the veriﬁer can prove robustness of an adversarially-trained model with
(cid:96)1-regularization and small weight pruning on 89.13% of inputs with a 120 second budget per input.

Interestingly, even though adversarial training improves weight sparsity (see Appendix B) it was
still necessary to use (cid:96)1-regularization and small weight pruning. These techniques further promoted
weight sparsity and gave rise to networks that were much easier to verify.

4

Published as a conference paper at ICLR 2019

Dataset

Epsilon

Training
Method

Test Set
Accuracy

Provable Adversarial
Accuracy

Average
Solve Time (s)

MNIST (cid:15) = 0.1

1 Adversarial Training
+(cid:96)1-Regularization
2
+Small Weight Pruning
3
4

+ReLU Pruning (control)

99.17%
99.00%
98.99%
98.94%

19.00%
82.17%
89.13%
91.58%

2970.43
21.99
11.71
6.43

Table 1: Improvement in provable adversarial accuracy and veriﬁcation solve times when incremen-
tally adding natural regularization methods for improving weight sparsity and ReLU stability into
the model training procedure, before veriﬁcation occurs. Each row represents the addition of another
method – for example, Row 3 uses adversarial training, (cid:96)1-regularization, and small weight pruning.
Row 4 adds ReLU pruning (see Appendix A). Row 4 is the control model for MNIST and (cid:15) = 0.1
that we present again in comparisons in Tables 2 and 3. We use a 3600 instead of 120 second timeout
for Row 1 and only veriﬁed the ﬁrst 100 images (out of 10000) because verifying it took too long.

3.3 RELU STABILITY

Next, we target the primary speed bottleneck of exact veriﬁcation: the number of ReLUs the veriﬁer
has to branch on. In our paper, this corresponds to the notion of inducing ReLU stability. Before we
describe our methodology, we formally deﬁne ReLU stability.

(2)).

Given an input x, a set of allowed perturbations Adv(x), and a ReLU, exact veriﬁers may need
to branch based on the possible pre-activations of the ReLU, namely ˆzij(Adv(x)) = {ˆzij(x(cid:48)) :
x(cid:48) ∈ Adv(x)} (cf.
If there exist two perturbations x(cid:48), x(cid:48)(cid:48) in the set Adv(x) such that
sign(ˆzij(x(cid:48))) (cid:54)= sign(ˆzij(x(cid:48)(cid:48))), then the veriﬁer has to consider that for some perturbed inputs the
ReLU is active (zij = ˆzij) and for other perturbed inputs inactive (zij = 0). The more such cases
the veriﬁer faces, the more branches it has to consider, causing the complexity of veriﬁcation to in-
crease exponentially. Intuitively, a model with 1000 ReLUs among which only 100 ReLUs require
branching will likely be much easier to verify than a model with 200 ReLUs that all require branch-
ing. Thus, it is advantageous for the veriﬁer if, on an input x with allowed perturbation set Adv(x),
the number of ReLUs such that

sign(ˆzij(x(cid:48))) = sign(ˆzij(x)) ∀x(cid:48) ∈ Adv(x)
is maximized. We call a ReLU for which (5) holds on an input x a stable ReLU on that input. If (5)
does not hold, then the ReLU is an unstable ReLU.

(5)

Directly computing whether a ReLU is stable on a given input x is difﬁcult because doing so would
involve considering all possible values of ˆzij(Adv(x)). Instead, exact veriﬁers compute an upper
bound ˆuij and a lower bound ˆlij of ˆzij(Adv(x)). If 0 ≤ ˆlij or ˆuij ≤ 0, they can replace the ReLU
with the identity function or the zero function, respectively. Otherwise, if ˆlij < 0 < ˆuij, these
veriﬁers then determine that they need to “branch” on that ReLU. Thus, we can rephrase (5) as

sign(ˆuij) = sign(ˆlij)

(6)

We will discuss methods for determining these upper and lower bounds ˆuij, ˆlij in Section 3.3.2.

3.3.1 A REGULARIZATION TECHNIQUE FOR INDUCING RELU STABILITY: RS LOSS

As we see from equation (6), a function that would indicate exactly when a ReLU is stable is
F ∗(ˆuij, ˆlij) = sign(ˆuij) · sign(ˆlij). Thus, it would be natural to use this function as a regular-
izer. However, this function is non-differentiable and if used in training a model, would provide no
useful gradients during back-propagation. Thus, we use the following smooth approximation of F ∗
(see Fig. 1) which provides the desired gradients:

F (ˆuij, ˆlij) = − tanh(1 + ˆuij · ˆlij)

We call the corresponding objective RS Loss, and show in Fig. 2a that using this loss function as
a regularizer effectively decreases the number of unstable ReLUs. RS Loss thus encourages ReLU
stability, which, in turn, speeds up exact veriﬁcation - see Fig. 2b.

5

Published as a conference paper at ICLR 2019

Figure 1: Plot and contour plot of the function F (x, y) = − tanh(1 + x · y)

3.3.2 ESTIMATING RELU UPPER AND LOWER BOUNDS ON ACTIVATIONS

A key aspect of using RS Loss is determining the upper and lower bounds ˆuij, ˆlij for each ReLU (cf.
(6)). The bounds for the inputs z0 (cf. (1)) are simple – for the input x, we know x − (cid:15) ≤ z0 ≤ x + (cid:15),
so ˆu0 = x − (cid:15), ˆl0 = x + (cid:15). For all subsequent zij, we estimate bounds using either the naive interval
arithmetic (IA) approach described in Tjeng et al. (2019) or an improved version of it. The improved
version is a tighter estimate but uses more memory and training time, and thus is most effective on
smaller networks. We present the details of naive IA and improved IA in Appendix C.

Interval arithmetic approaches can be implemented relatively efﬁciently and work well with back-
propagation because they only involve matrix multiplications. This contrasts with how exact veri-
ﬁers compute these bounds, which usually involves solving LPs or MILPs. Interval arithmetic also
overestimates the number of unstable ReLUs. This means that minimizing unstable ReLUs based
on IA bounds will provide an upper bound on the number of unstable ReLUs determined by exact
veriﬁers. In particular, IA will properly penalize every unstable ReLU.

Improved IA performs well in practice, overestimating the number of unstable ReLUs by less than
0.4% in the ﬁrst 2 layers of MNIST models and by less than 36.8% (compared to 128.5% for naive
IA) in the 3rd layer. Full experimental results are available in Table 4 of Appendix C.3.

3.3.3

IMPACT OF RELU STABILITY IMPROVEMENTS ON VERIFICATION SPEED

We provide experimental evidence that RS Loss regularization improves ReLU stability and speeds
up average veriﬁcation times by more than an order of magnitude in Fig. 2b. To isolate the effect of
RS Loss, we compare MNIST models trained in exactly the same way other than the weight on RS
Loss. When comparing a network trained with a RS Loss weight of 5e−4 to a network with a RS
Loss weight of 0, the former has just 16% as many unstable ReLUs and can be veriﬁed 65x faster.
The caveat here is that the former has 1.00% lower test set accuracy.

We also compare veriﬁcation speed with and without RS Loss on MNIST networks for different
values of (cid:15) (0.1, 0.2, and 0.3) in Fig. 2c. We choose RS Loss weights that cause almost no test set
accuracy loss (less than 0.50% - See Table 3) in these cases, and we still observe a 4–13x speedup
from RS Loss. For CIFAR, RS Loss gives a smaller speedup of 1.6–3.7x (See Appendix E).

3.3.4

IMPACT OF RELU STABILITY IMPROVEMENTS ON PROVABLE ADVERSARIAL
ACCURACY

As the weight on the RS Loss used in training a model increases, the ReLU stability of the model
will improve, speeding up veriﬁcation and likely improving provable adversarial accuracy. However,
like most regularization methods, placing too much weight on RS Loss can decrease the model ca-
pacity, potentially lowering both the true adversarial accuracy and the provable adversarial accuracy.
Therefore, it is important to choose the weight on RS Loss carefully to obtain both high provable
adversarial accuracy and faster veriﬁcation speeds.

To show the effectiveness of RS Loss in improving provable adversarial accuracy, we train two
networks for each dataset and each value of (cid:15). One is a “control” network that uses all of the natural
improvements for inducing both weight sparsity ((cid:96)1-regularization and small weight pruning) and
ReLU stability (ReLU pruning - see Appendix A). The second is a “+RS” network that uses RS

6

Published as a conference paper at ICLR 2019

(a)

(b)

(c)

Figure 2: (a) Average number of unstable ReLUs by layer and (b) average veriﬁcation solve times
of 6 networks trained with different weights on RS Loss for MNIST and (cid:15) = 0.1 . Averages are
taken over all 10000 MNIST test set inputs. Both metrics improve signiﬁcantly with increasing
RS Loss weight. An RS Loss weight of 0 corresponds to the control network, while an RS Loss
weight of 0.00012 corresponds to the “+RS” network for MNIST and (cid:15) = 0.1 in Tables 2 and 3. (c)
Improvement in the average time taken by a veriﬁer to solve the veriﬁcation problem after adding
RS Loss to the training procedure, for different (cid:15) on MNIST. The weight on RS Loss was chosen so
that the “+RS” models have test set accuracies within 0.50% of the control models.

Loss in addition to all of the same natural improvements. This lets us isolate the incremental effect
of adding RS Loss to the training procedure.

In addition to attaining a 4–13x speedup in MNIST veriﬁcation times (see Fig. 2c), we achieve
higher provable adversarial accuracy in every single setting when using RS Loss. This is especially
notable for the hardest veriﬁcation problem we tackle – proving robustness to perturbations with (cid:96)∞
norm-bound 8/255 on CIFAR-10 – where adding RS Loss nearly triples the provable adversarial
accuracy from 7.09% to 20.27%. This improvement is primarily due to veriﬁcation speedup induced
by RS Loss, which allows the veriﬁer to ﬁnish proving robustness for far more inputs within the 120
second time limit. These results are shown in Table 2.

Table 2: Provable Adversarial Accuracies for the control and “+RS” networks in each setting.

MNIST, (cid:15) = 0.1 MNIST, (cid:15) = 0.2 MNIST, (cid:15) = 0.3 CIFAR, (cid:15) = 2/255 CIFAR, (cid:15) = 8/255

Control
+RS

91.58
94.33

86.45
89.79

77.99
80.68

45.53
45.93

7.09
20.27

4 EXPERIMENTS

4.1 EXPERIMENTS ON MNIST AND CIFAR

In addition to the experimental results already presented, we compare our control and “+RS” net-
works with the best available results presented in the state-of-the-art certiﬁable defenses of Wong
et al. (2018), Dvijotham et al. (2018), and Mirman et al. (2018) in Table 3. We compare their test
set accuracy, PGD adversarial accuracy (an evaluation of robustness against a strong 40-step PGD
adversarial attack), and provable adversarial accuracy. Additionally, to show that our method can
scale to larger architectures, we train and verify a “+RS (Large)” network for each dataset and (cid:15).

In terms of provable adversarial accuracies, on MNIST, our results are signiﬁcantly better than those
of Wong et al. (2018) for larger perturbations of (cid:15) = 0.3, and comparable for (cid:15) = 0.1. On CIFAR-
10, our method is slightly less effective, perhaps indicating that more unstable ReLUs are necessary
to properly learn a robust CIFAR classiﬁer. We also experienced many more instances of the veriﬁer
reaching its allotted 120 second time limit on CIFAR, especially for the less ReLU stable control
networks. Full experimental details for each model in Tables 1, 2, and 3, including a breakdown of
veriﬁcation solve results (how many images did the veriﬁer: A. prove robust B. ﬁnd an adversarial
example for C. time out on), are available in Appendix E.

7

Published as a conference paper at ICLR 2019

Table 3: Comparison of test set, PGD adversarial, and provable adversarial accuracy of networks
trained with and without RS Loss. We also provide the best available certiﬁable adversarial and
PGD adversarial accuracy of any single models from Wong et al. (2018), Dvijotham et al. (2018),
and Mirman et al. (2018) for comparison, and highlight the best provable accuracy for each (cid:15).
* The provable adversarial accuracy for “+RS (Large)” is only computed for the ﬁrst 1000 images
because the veriﬁer performs more slowly on larger models.
** Dvijotham et al. (2018); Mirman et al. (2018) use a slightly smaller (cid:15) = 0.03 = 7.65/255.
† Mirman et al. (2018) computes results over 500 images instead of all 10000.
†† Mirman et al. (2018) uses a slightly smaller (cid:15) = 0.007 = 1.785/255.

Dataset

Epsilon

Test Set
Accuracy

PGD Adversarial

Provable/Certiﬁable
Accuracy Adversarial Accuracy

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

Training
Method

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†,††

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.**
Mirman et al.†, **

98.94%
98.68%
98.95%

98.92%
98.80%
99.00%

98.40%
98.10%
98.21%

97.75%
97.33%
97.54%

85.13%
96.60%

64.64%
61.12%
61.41%

68.28%
62.00%

50.69%
40.45%
42.81%

28.67%
48.64%
54.20%

95.12%
95.13%
96.58%

-
97.13%
97.60%

93.14%
93.14%
94.19%

91.64%
92.05%
93.25%

-
93.80%

51.58%
49.92%
50.61%

-
54.60%

31.28%
26.78%
28.69%

-
32.72%
40.00%

91.58%
94.33%
95.60%

96.33%
95.56%
96.60%

86.45%
89.79%
89.10%

77.99%
80.68%
59.60%

56.90%
82.00%

45.53%
45.93%
41.40%

53.89%
52.20%

7.09%
20.27%
19.80%

21.78%
26.67%
35.20%

4.2 EXPERIMENTAL METHODS AND DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. For each setting of dataset
(MNIST or CIFAR) and (cid:15), we ﬁnd a suitable weight on RS Loss via line search (See Table 6 in
Appendix D). The same weight is used for each ReLU. During training, we used improved IA for
ReLU bound estimation for “+RS” models and use naive IA for “+RS (Large)” models because of
memory constraints. For ease of comparison, we trained our networks using the same convolutional
DNN architecture as in Wong et al. (2018). This architecture uses two 2x2 strided convolutions with
16 and 32 ﬁlters, followed by a 100 hidden unit fully connected layer. For the larger architecture,
we also use the same “large” architecture as in Wong et al. (2018). It has 4 convolutional layers with
32, 32, 64, and 64 ﬁlters, followed by 2 fully connected layers with 512 hidden units each.

For veriﬁcation, we used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019).
Model solves were parallelized over 8 CPU cores. When verifying an image, the veriﬁer of Tjeng
et al. (2019) ﬁrst builds a model, and second, solves the veriﬁcation problem (See Appendix D.2 for
details). We focus on reporting solve times because that is directly related to the task of veriﬁcation
itself. All build times for the control and “+RS” models on MNIST that we presented were between
4 and 10 seconds, and full results on build times are also presented in Appendix E.

Additional details on our experimental setup (e.g. hyperparameters) can be found in Appendix D.

8

Published as a conference paper at ICLR 2019

5 CONCLUSION

In this paper, we use the principle of co-design to develop training methods that emphasize veri-
ﬁcation as a goal, and we show that they make verifying the trained model much faster. We ﬁrst
demonstrate that natural regularization methods already make the exact veriﬁcation problem sig-
niﬁcantly more tractable. Subsequently, we introduce the notion of ReLU stability for networks,
present a method that improves a network’s ReLU stability, and show that this improvement makes
veriﬁcation an additional 4–13x faster. Our method is universal, as it can be added to any training
procedure and should speed up any exact veriﬁcation procedure, especially MILP-based methods.

Prior to our work, exact veriﬁcation seemed intractable for all but the smallest models. Thus, our
work shows progress toward reliable models that can be proven to be robust, and our techniques can
help scale veriﬁcation to even larger networks.

Many of our methods appear to compress our networks into more compact, simpler forms. We
hypothesize that the reason that regularization methods like RS Loss can still achieve very high
accuracy is that most models are overparametrized in the ﬁrst place. There exist clear parallels
between our methods and techniques in model compression (Han et al., 2016; Cheng et al., 2017b) –
therefore, we believe that drawing upon additional techniques from model compression can further
improve the ease-of-veriﬁcation of networks. We also expect that there exist objectives other than
weight sparsity and ReLU stability that are important for veriﬁcation speed. If so, further exploring
the principle of co-design for those objectives is an interesting future direction.

ACKNOWLEDGEMENTS

This work was supported by the NSF Graduate Research Fellowship under Grant No. 1122374,
by the NSF grants CCF-1553428 and CNS-1815221, and by Lockheed Martin Corporation under
award number RPP2016-002. We would like to thank Krishnamurthy Dvijotham, Ludwig Schmidt,
Michael Sun, Dimitris Tsipras, and Jonathan Uesato for helpful discussions.

9

Published as a conference paper at ICLR 2019

REFERENCES

Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense
In International Conference on

of security: Circumventing defenses to adversarial examples.
Machine Learning (ICML), 2018a.

Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. In International Conference on Machine Learning (ICML), pages 284–293, 2018b.

Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
In Proceedings of ACM Workshop on Artiﬁcial Intelligence and Security

detection methods.
(AISec), 2017a.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks.

In

Security and Privacy (SP), 2017 IEEE Symposium on, 2017b.

Nicholas Carlini, Guy Katz, Clark Barrett, and David L. Dill. Ground-truth adversarial examples.

2017.

networks. 2017a.

Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artiﬁcial neural

Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration

for deep neural networks. 2017b.

Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training veriﬁed learners with learned ver-
iﬁers. 2018.

Rüdiger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In Auto-

mated Technology for Veriﬁcation and Analysis, 2017.

Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir
Rahmati, and Dawn Song. Robust physical-world attacks on machine learning models. 2017.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. In International Conference on Learning Representations (ICLR), 2015.

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. 2018.

Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. In International Conference on Learning
Representations (ICLR), 2016.

Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, An-
drew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep
neural networks for acoustic modeling in speech recognition: The shared views of four research
groups. In IEEE Signal Processing Magazine, 2012.

Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. 2018.

Guy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer. Reluplex: An efﬁcient

smt solver for verifying deep neural networks. 2017.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations (ICLR), 2015.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012.

Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward relu

neural networks. 2017.

10

Published as a conference paper at ICLR 2019

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations (ICLR), 2018.

Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning (ICML), 2018.

Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation
as a defense to adversarial perturbations against deep neural networks. In Proceedings of IEEE
Symposium on Security and Privacy (SP), 2016.

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial exam-

ples. In International Conference on Learning Representations (ICLR), 2018.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 2017.

Aman Sinha, Hongseok Namkoong, and John Duchi. Certiﬁable distributional robustness with
principled adversarial training. In International Conference on Learning Representations (ICLR),
2018.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations (ICLR), 2014.

Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap
In Computer Vision and Pattern Recognition

to human-level performance in face veriﬁcation.
(CVPR), 2014.

Robert Tibshirani. Regression shrinkage and selection via the lasso. In Journal of the Royal Statis-

tical Society, Series B, 1994.

Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In International Conference on Learning Representations (ICLR), 2019.

Jonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial
risk and the dangers of evaluating against weak attacks. In International Conference on Machine
Learning (ICML), 2018.

Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer

adversarial polytope. In International Conference on Machine Learning (ICML), 2018.

Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and Zico Kolter. Scaling provable adversarial

defenses. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

11

Published as a conference paper at ICLR 2019

APPENDIX

A NATURAL IMPROVEMENTS

A.1 NATURAL REGULARIZATION FOR INDUCING WEIGHT SPARSITY

All of the control and “+RS” networks in our paper contain natural improvements that improve
weight sparsity, which reduce the number of variables in the LPs solved by the veriﬁer. We ob-
served that the two techniques we used for weight sparsity ((cid:96)1-regularization and small weight prun-
ing) don’t hurt test set accuracy but they dramatically improve provable adversarial accuracy and
veriﬁcation speed.

1. (cid:96)1-regularization: We use a weight of 2e−5 on MNIST and a weight of 1e−5 on CIFAR.
We chose these weights via line search by ﬁnding the highest weight that would not hurt
test set accuracy.

2. Small weight pruning: Zeroing out weights in a network that are very close to zero. We

choose to prune weights less than 1e−3.

A.2 A BASIC IMPROVEMENT FOR INDUCING RELU STABILITY: RELU PRUNING

We also use a basic idea to improve ReLU stability, which we call ReLU pruning. The main idea is
to prune away ReLUs that are not necessary.

We use a heuristic to test whether a ReLU in a network is necessary. Our heuristic is to count how
many training inputs cause the ReLU to be active or inactive. If a ReLU is active (the pre-activation
satisﬁes ˆzij(x) > 0) for every input image in the training set, then we can replace that ReLU with
the identity function and the network would behave in exactly the same way for all of those images.
Similarly, if a ReLU is inactive (ˆzij(x) < 0) for every training image, that ReLU can be replaced by
the zero function.

Extending this idea further, we expect that ReLUs that are rarely used can also be removed without
signiﬁcantly changing the behavior of the network. If only a small fraction (say, 10%) of the input
images activate a ReLU, then replacing the ReLU with the zero function will only slightly change the
network’s behavior and will not affect the accuracy too much. We provide experimental evidence of
this phenomenon on an adversarially trained ((cid:15) = 0.1) MNIST model. Conservatively, we decided
that pruning away ReLUs that are active on less than 10% of the training set or inactive on less than
10% of the training set was reasonable.

Figure 3: Removing some ReLUs does not hurt test set accuracy or accuracy against a PGD adver-
sary

12

Published as a conference paper at ICLR 2019

B ADVERSARIAL TRAINING AND WEIGHT SPARSITY

It is worth noticing that adversarial training against (cid:96)∞ norm-bound adversaries alone already makes
networks easier to verify by implicitly improving weight sparsity. Indeed, this can be shown clearly
in the case of linear networks. Recall that a linear network can be expressed as f (x) = W x + b.
Thus, an (cid:96)∞ norm-bound perturbation of the input x will produce the output

f (x(cid:48)) = x(cid:48)W + b

= xW + b + (x(cid:48) − x)W
≤ f (x) + (cid:15)||W ||1

where the last inequality is just Hölder’s inequality.
In order to limit the adversary’s ability to
perturb the output, adversarial training needs to minimize the ||W ||1 term, which is equivalent to (cid:96)1-
regularization and is known to promote weight sparsity (Tibshirani, 1994). Relatedly, Goodfellow
et al. (2015) already pointed out that adversarial attacks against linear networks will be stronger
when the (cid:96)1-norm of the weight matrices is higher.

Even in the case of nonlinear networks, adversarial training has experimentally been shown to im-
prove weight sparsity. For example, models trained according to Madry et al. (2018) and Wong and
Kolter (2018) often learn many weight-sparse layers, and we observed similar trends in the mod-
els we trained. However, it is important to note that while adversarial training alone does improve
weight sparsity, it is not sufﬁcient by itself for efﬁcient exact veriﬁcation. Additional regularization
like (cid:96)1-regularization and small weight pruning further promotes weight sparsity and gives rise to
networks that are much easier to verify.

13

Published as a conference paper at ICLR 2019

C INTERVAL ARITHMETIC

C.1 NAIVE INTERVAL ARITHMETIC

Naive IA determines upper and lower bounds for a layer based solely on the upper and lower bounds
of the previous layer.
Deﬁne W + = max(W, 0), W − = min(W, 0), u = max(ˆu, 0), and l = max(ˆl, 0). Then the bounds
on the pre-activations of layer i can be computed as follows:
i + li−1W −
i + ui−1W −

ˆui = ui−1W +
ˆli = li−1W +

i + bi
i + bi

(8)

(7)

As noted in Tjeng et al. (2019) and also Dvijotham et al. (2018), this method is efﬁcient but can lead
to relatively conservative bounds for deeper networks.

C.2

IMPROVED INTERVAL ARITHMETIC

We improve upon naive IA by exploiting ReLUs that we can determine to always be active. This
allows us to cancel symbols that are equivalent that come from earlier layers of a network.

We will use a basic example of a neural network with one hidden layer to illustrate this idea. Suppose
that we have the scalar input z0 with l0 = 0, u0 = 1, and the network has the following weights and
biases:

W1 = [1 − 1] ,

b1 = [2

2] , W2 =

b2 = 0

(cid:21)
(cid:20)1
1

,

Naive IA for the ﬁrst layer gives ˆl1 = l1 = [2
2], and applying naive IA to
the output ˆz2 gives ˆl2 = 3, ˆu2 = 5. However, because ˆl1 > 0, we know that the two ReLUs in the
hidden layer are always active and thus equivalent to the identity function. Then, the output is

1], ˆu1 = u1 = [3

ˆz2 = z11 + z12 = ˆz11 + ˆz12 = (z0 + 2) + (−z0 + 2) = 4

Thus, we can obtain the tighter bounds ˆl2 = ˆu2 = 4, as we are able to cancel out the z0 terms.
We can write this improved version of IA as follows. First, letting Wk denote row k of matrix W ,
we can deﬁne the “active” part of W as the matrix WA, where

(WA)k =

(cid:40)

Wk
0

if ˆli−1 > 0
if ˆli−1 ≤ 0

Deﬁne the “non-active” part of W as

WN = W − WA
Then, using the same deﬁnitions for the notation W +, W −, u, l as before, we can write down the
following improved version of IA which uses information from the previous 2 layers.

ˆui = ui−1Wi

+
N + li−1Wi

−
N + bi
+ ui−2(Wi−1WiA)+ + li−2(Wi−1WiA)− + bi−1WiA
−
N + bi
+ li−2(Wi−1WiA)+ + ui−2(Wi−1WiA)− + bi−1WiA
We are forced to to use li−1,j and ui−1,j if we can not determine whether or not the ReLU corre-
sponding to the activation zi−1,j is active, but we use li−2 and ui−2 whenever possible.

+
N + ui−1Wi

ˆli = li−1Wi

We now deﬁne some additional notation to help us extend this method to any number of layers. We
now seek to deﬁne fn, which is a function which takes in four sequences of length n – upper bounds,
lower bounds, weights, and biases – and outputs the current layer’s upper and lower bounds.

What we have derived so far from (7) and (8) is the following

f1(ui−1, li−1, Wi, bi) = (ui−1W +

i + li−1W −

i + bi, li−1W +

i + ui−1W −

i + bi)

14

Published as a conference paper at ICLR 2019

Let u denote a sequence of upper bounds. Let uz denote element z of the sequence, and let u[z:]
denote the sequence without the ﬁrst z elements. Deﬁne notation for l, W, and b similarly.

Then, using the fact that WN Z = (W Z)N and WAZ = (W Z)A, we can show that the following
recurrence holds:

fn+1(u, l, W, b) = f1(u1, l1, W1N , b1)

+ fn(u[1:], l[1:], (W2W1A, W[2:]), (b2W1A, b[2:]))

(9)

Let u(x,y) denote the sequence (ux, ux−1, · · · , uy), and deﬁne l(x,y), W(x,y), and b(x,y) similarly.
Then, if we want to compute the bounds on layer k using all information from the previous k layers,
we simply have to compute fk(u(k−1,0), l(k−1,0), W(k,1), b(k,1)).

From the recurrence 9, we see that using information from all previous layers to compute bounds for
layer k takes O(k) matrix-matrix multiplications. Thus, using information from all previous layers
to compute bounds for all layers of a d layer neural network only involves O(d2) additional matrix
multiplications, which is still reasonable for most DNNs. This method is still relatively efﬁcient
because it only involves matrix multiplications – however, needing to perform matrix-matrix multi-
plications as opposed to just matrix-vector multiplications results in a slowdown and higher memory
usage when compared to naive IA. We believe the improvement in the estimate of ReLU upper and
lower bounds is worth the time trade-off for most networks.

C.3 EXPERIMENTAL RESULTS ON IMPROVED IA AND NAIVE IA

In Table 4, we show empirical evidence that the number of unstable ReLUs in each layer of a MNIST
network, as estimated by improved IA, tracks the number of unstable ReLUs determined by the exact
veriﬁer quite well. We also present estimates determined via naive IA for comparison.

Dataset

Epsilon Training Estimation

Method Method

Unstable ReLUs Unstable ReLUs Unstable ReLUs
in 1st Layer

in 2nd Layer

in 3rd Layer

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

Control

+RS

Control

+RS

Control

+RS

Exact
61.14
Improved IA 61.14
61.14
Naive IA

21.64
Exact
Improved IA 21.64
21.64
Naive IA

Exact
17.47
Improved IA 17.47
17.47
Naive IA

Exact
29.91
Improved IA 29.91
29.91
Naive IA

Exact
36.76
Improved IA 36.76
36.76
Naive IA

24.43
Exact
Improved IA 24.43
24.43
Naive IA

185.30
185.96 (+0.4%)
188.44 (+1.7%)

31.73
43.40 (+36.8%)
69.96 (+120.5%)

64.73
64.80 (+0.1%)
65.34 (+0.9%)

14.67
18.97 (+29.4%)
33.51 (+128.5%)

37.92
48.88 (+28.9%)
69.75 (+84.0%)

24.05
28.40 (+18.1%)
40.47 (+68.3%)

28.64
31.19 (+8.9%)
32.13 (+12.2%)

83.42
83.44 (+0.02%)
83.52 (+0.1%)

40.74
46.00 (+12.9%)
48.27 (+18.5%)

142.95
142.95
142.95

54.47
54.47
54.47

48.47
48.47
48.47

Table 4: Comparison between the average number of unstable ReLUs as found by the exact veriﬁer
of Tjeng et al. (2019) and the estimated average number of unstable ReLUs found by improved IA
and naive IA. We compare these estimation methods on the control and “+RS” networks for MNIST
that we described in Section 3.3.4

15

Published as a conference paper at ICLR 2019

C.4 ON THE CONSERVATIVE NATURE OF IA BOUNDS

The upper and lower bounds we compute on each ReLU via either naive IA or improved IA are
conservative. Thus, every unstable ReLU will always be correctly labeled as unstable, while stable
ReLUs can be labeled as either stable or unstable. Importantly, every unstable ReLU, as estimated
by IA bounds, is correctly labeled and penalized by RS Loss. The trade-off is that stable ReLUs
mislabeled as unstable will also be penalized, which can be an unnecessary regularization of the
model.

In Table 5 we show empirically that we can achieve the following two objectives at once when using
RS Loss combined with IA bounds.

1. Reduce the number of ReLUs labeled as unstable by IA, which is an upper bound on the

true number of unstable ReLUs as determined by the exact veriﬁer.

2. Achieve similar test set accuracy and PGD adversarial accuracy as a model trained without

RS Loss.

Dataset

Epsilon Training
Method

Estimation
Method

Total Labeled
Unstable ReLUs Accuracy

Test Set

PGD Adversarial
Accuracy

MNIST (cid:15) = 0.1

Control
+RS

Improved IA 290.5
Improved IA 105.4

Control (Large) Naive IA
Naive IA
+RS (Large)

835.8
150.3

98.94%
98.68% (-0.26%)

95.12%
95.13% (+0.01%)

99.04%
98.95% (-0.09%)

96.32%
96.58% (+0.26%)

Table 5: The addition of RS Loss results in far fewer ReLUs labeled as unstable for both 3-layer and
6-layer (Large) networks. The decrease in test set accuracy as a result of this regularization is small.

Even though IA bounds are conservative, these results show that it is still possible to decrease the
number of ReLUs labeled as unstable by IA without signiﬁcantly degrading test set accuracy. When
comparing the Control and “+RS” networks for MNIST and (cid:15) = 0.1, adding RS Loss decreased
the average number of ReLUs labeled as unstable (using bounds from Improved IA) from 290.5 to
105.4 with just a 0.26% loss in test set accuracy. The same trend held for deeper, 6-layer networks,
even when the estimation method for upper and lower bounds was the more conservative Naive IA.

16

Published as a conference paper at ICLR 2019

D FULL EXPERIMENTAL SETUP

D.1 NETWORK TRAINING DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. Following the prior
examples of Wong and Kolter (2018) and Dvijotham et al. (2018), we introduced a small tweak
where we increased the adversary strength linearly from 0.01 to (cid:15) over ﬁrst half of training and kept
it at (cid:15) for the second half. We used this training schedule to improve convergence of the training
process.

For MNIST, we trained for 70 epochs using the Adam optimizer (Kingma and Ba, 2015) with a
learning rate of 1e−4 and a batch size of 32. For CIFAR, we trained for 250 epochs using the Adam
optimizer with a learning rate of 1e−4. When using naive IA, we used a batch size of 128, and
when using improved IA, we used a batch size of 16. We used a smaller batch size in the latter
case because improved IA incurs high RAM usage during training. To speed up training on CIFAR,
we only added in RS Loss regularization in the last 20% of the training process. Using this same
sped-up training method on MNIST did not signiﬁcantly affect the results.

Dataset

Epsilon

(cid:96)1 weight RS Loss weight

MNIST 0.1
MNIST 0.2
MNIST 0.3
CIFAR 2/255
CIFAR 8/255

2e−5
2e−5
2e−5
1e−5
1e−5

12e−5
1e−4
12e−5
1e−3
2e−3

Table 6: Weights chosen using line search for (cid:96)1 regularization and RS Loss in each setting

For each setting, we ﬁnd a suitable weight on RS Loss via line search. The same weight is used for
each ReLU. The ﬁve weights we chose are displayed above in Table 6, along with weights chosen
for (cid:96)1-regularization.

We also train “+RS” models using naive IA to show that our technique for inducing ReLU stability
can work while having small training time overhead – full details on “+RS (Naive IA)” networks are
in Appendix E.

D.2 VERIFIER OVERVIEW

The MILP-based exact veriﬁer of Tjeng et al. (2019), which we use, proceeds in two steps for every
input. They are the model-build step and the solve step.

First, the veriﬁer builds a MILP model based on the neural network and the input. In particular, the
veriﬁer will compute upper and lower bounds on each ReLU using a speciﬁc bound computation al-
gorithm. We chose the default bound computation algorithm in the code, which uses LP to compute
bounds. LP bounds are tighter than the bounds computed via IA, which is another option available in
the veriﬁer. The model-build step’s speed appeared to depend primarily on the tightening algorithm
(IA was faster than LP) and the number of variables in the MILP (which, in turn, depends on the
sparsity of the weights of the neural network). The veriﬁer takes advantage of these bounds by not
introducing a binary variables into the MILP formulation if it can determine that a particular ReLU
is stable. Thus, using LP as the tightening algorithm resulted in higher build times, but led to easier
MILP formulations.

Next, the veriﬁer solves the MILP using an off-the-shelf MILP solver. The solver we chose was
the commercial Gurobi Solver, which uses a branch-and-bound method for solving MILPs. The
solver’s speed appeared to depend primarily on the number of binary variables in the MILP (which
corresponds to the number of unstable ReLUs) as well as the total number of variables in the MILP
(which is related to the sparsity of the weight matrices). While these two numbers are strongly
correlated with solve times, some solves would still take a long time despite having few binary

17

Published as a conference paper at ICLR 2019

variables. Thus, understanding what other properties of neural networks correspond to MILPs that
are easy or hard to solve is an important area to explore further.

D.3 VERIFIER DETAILS

We used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019) using the default
settings of the code. We allotted 120 seconds for veriﬁcation of each input datapoint using the
default model build settings. We ran our experiments using the commercial Gurobi Solver (version
7.5.2), and model solves were parallelized over 8 CPU cores with Intel Xeon CPUs @ 2.20GHz
processors. We used computers with 8–32GB of RAM, depending on the size of the model being
veriﬁed. All computers used are part of an OpenStack network.

18

Published as a conference paper at ICLR 2019

E FULL EXPERIMENTAL VERIFICATION RESULTS

Dataset

Epsilon

Training
Method

Test
Set Adversarial
Accuracy

Accuracy

Total
Provable
Upper Adversarial Unstable
Accuracy
Bound

Avg
Avg
Build
Solve
ReLUs Time (s) Time (s)

PGD Veriﬁer

Adversarial Training*
+(cid:96)1-regularization
+Small Weight Pruning
+ReLU Pruning (Control)

MNIST (cid:15) = 0.1

Wong et al. (2018)***

+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

99.17%
99.00%
98.99%
98.94%
98.68%
98.53%
98.95%

98.40%
98.10%
98.08%
98.21%
95.06%

97.75%
97.33%
97.06%
97.54%

64.64%
61.12%
57.83%
61.41%

50.69%
40.45%
46.19%
42.81%

95.04% 96.00%
95.25% 95.98%
95.38% 94.93%
95.12% 94.45%
95.13% 94.38%
94.86% 94.54%
96.58% 95.60%

93.14% 90.71%
93.14% 89.98%
91.68% 88.87%
94.19% 90.40%
-
89.03%

91.64% 83.83%
92.05% 81.70%
89.19% 79.12%
93.25% 83.70%

51.58% 50.23%
49.92% 47.79%
47.03% 45.33%
50.61% 51.00%

31.28% 33.46%
26.78% 22.74%
29.66% 26.07%
28.69% 25.20%

19.00%
82.17%
89.13%
91.58%
94.33%
94.32%
95.60%

86.45%
89.79%
85.54%
89.10%
80.29%

77.99%
80.68%
76.70%
59.60%

45.53%
45.93%
44.44%
41.40%

7.09%
20.27%
18.90%
19.80%

1517.9
505.3
502.7
278.2
101.0
158.3
119.5

2970.43
21.99
11.71
6.43
0.49
0.96
0.27

198.3
108.4
217.2
133.0
-

160.9
101.5
179.0
251.2

360.0
234.3
170.1
196.7

665.9
54.2
277.8
246.5

9.41
1.13
8.50
2.93
-

11.80
2.78
6.43
37.45

21.75
13.50
6.30
29.88

82.91
22.33
33.63
20.14

650.93
79.13
19.30
9.61
4.98
4.82
156.74

7.15
4.43
4.67
171.10
-

5.14
4.34
4.00
166.39

66.42
52.58
47.11
335.97

73.28
38.84
23.66
401.72

Table 7: Full results on natural improvements from Table 1, control networks (which use all of the
natural improvements and ReLU pruning), and “+RS” networks from Tables 2 and 3. While we are
unable to determine the true adversarial accuracy, we provide two upper bounds and a lower bound.
Evaluations of robustness against a strong 40-step PGD adversary (PGD adversarial accuracy) gives
one upper bound, and the veriﬁer itself gives another upper bound because it can also prove that
the network is not robust to perturbations on certain inputs by ﬁnding adversarial examples. The
veriﬁer simultaneously ﬁnds the provable adversarial accuracy, which is a lower bound on the true
adversarial accuracy. The gap between the veriﬁer upper bound and the provable adversarial accu-
racy (veriﬁer lower bound) corresponds to inputs that the veriﬁer times out on. These are inputs that
the veriﬁer can not prove to be robust or not robust in 120 seconds. Build times and solve times
are reported in seconds. Finally, average solve time includes timeouts. In other words, veriﬁcation
solves that time out contribute 120 seconds to the total solve time.
* The “Adversarial Training” network uses a 3600 instead of 120 second timeout and is only veriﬁed
for the ﬁrst 100 images because verifying it took too long.
** The “+RS (Large)” networks are only veriﬁed for the ﬁrst 1000 images because of long build
times.
*** Wong et al. (2018); Dvijotham et al. (2018), and Mirman et al. (2018), which we compare to in
Table 3, do not report results on MNIST, (cid:15) = 0.2 in their papers. We ran the publicly available code
of Wong et al. (2018) on MNIST, (cid:15) = 0.2 to generate these results for comparison.

19

Published as a conference paper at ICLR 2019

F DISCUSSION ON VERIFICATION AND CERTIFICATION

Exact veriﬁcation and certiﬁcation are two related approaches to formally verifying properties of
neural networks, such as adversarial robustness.
In both cases, the end goal is formal veriﬁca-
tion. Certiﬁcation methods, which solve an easier-to-solve relaxation of the exact veriﬁcation prob-
lem, are important developments because exact veriﬁcation previously appeared computationally
intractable for all but the smallest models.

For the case of adversarial robustness, certiﬁcation methods exploit a trade-off between provable
robustness and speed. They can fail to provide certiﬁcates of robustness for some inputs that are
actually robust, but they will either ﬁnd or fail to ﬁnd certiﬁcates of robustness quickly. On the other
hand, exact veriﬁers will always give the correct answer if given enough time, but exact veriﬁers can
sometimes take many hours to formally verify robustness on even a single input.

In general, the process of training a robust neural network and then formally verifying its robustness
happens in two steps.

• Step 1: Training
• Step 2: Veriﬁcation or Certiﬁcation

Most papers on certiﬁcation, including Wong and Kolter (2018); Wong et al. (2018); Dvijotham
et al. (2018); Raghunathan et al. (2018) and Mirman et al. (2018), propose a method for step 2
(the certiﬁcation step), and then propose a training objective in step 1 that is directly related to
their method for step 2. We call this paradigm “co-training.” In Raghunathan et al. (2018), they
found that using their step 2 on a model trained using Wong and Kolter (2018)’s step 1 resulted in
extremely poor provable robustness (less than 10%), and the same was true when using Wong and
Kolter (2018)’s step 2 on a model trained using their step 1.

We focus on MILP-based exact veriﬁcation as our step 2, which encompasses the best current exact
veriﬁcation methods. The advantage of using exact veriﬁcation for step 2 is that it will be accurate,
regardless of what method is used in step 1. The disadvantage of using exact veriﬁcation for step
2 is that it could be extremely slow. For our step 1, we used standard robust adversarial training.
In order to signiﬁcantly speed up exact veriﬁcation as step 2, we proposed techniques that could be
added to step 1 to induce weight sparsity and ReLU stability.

In general, we believe it is important to develop effective methods for step 1, given that step 2 is
exact veriﬁcation. However, ReLU stability can also be beneﬁcial for tightening the relaxation of
certiﬁcation approaches like that of Wong et al. (2018) and Dvijotham et al. (2018), as unstable
ReLUs are the primary cause of the overapproximation that occurs in the relaxation step. Thus, our
techniques for inducing ReLU stability can be useful for certiﬁcation as well.

Finally, in recent literature on veriﬁcation and certiﬁcation, most works have focused on formally
verifying the property of adversarial robustness of neural networks. However, veriﬁcation of other
properties could be useful, and our techniques to induce weight sparsity and ReLU stability would
still be useful for veriﬁcation of other properties for the exact same reasons that they are useful in
the case of adversarial robustness.

20

9
1
0
2
 
r
p
A
 
3
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
0
0
3
0
.
9
0
8
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2019

TRAINING FOR FASTER ADVERSARIAL ROBUSTNESS
VERIFICATION VIA INDUCING RELU STABILITY

Kai Y. Xiao Vincent Tjeng Nur Muhammad (Mahi) Shaﬁullah Aleksander M ˛adry
Massachusetts Institute of Technology
Cambridge, MA 02139
{kaix, vtjeng, nshafiul, madry}@mit.edu

ABSTRACT

We explore the concept of co-design in the context of neural network veriﬁcation.
Speciﬁcally, we aim to train deep neural networks that not only are robust to ad-
versarial perturbations but also whose robustness can be veriﬁed more easily. To
this end, we identify two properties of network models – weight sparsity and so-
called ReLU stability – that turn out to signiﬁcantly impact the complexity of the
corresponding veriﬁcation task. We demonstrate that improving weight sparsity
alone already enables us to turn computationally intractable veriﬁcation problems
into tractable ones. Then, improving ReLU stability leads to an additional 4–13x
speedup in veriﬁcation times. An important feature of our methodology is its “uni-
versality,” in the sense that it can be used with a broad range of training procedures
and veriﬁcation approaches.

1

INTRODUCTION

Deep neural networks (DNNs) have recently achieved widespread success in image classiﬁcation
(Krizhevsky et al., 2012), face and speech recognition (Taigman et al., 2014; Hinton et al., 2012),
and game playing (Silver et al., 2016; 2017). This success motivates their application in a broader
set of domains, including more safety-critical environments. This thrust makes understanding the
reliability and robustness of the underlying models, let alone their resilience to manipulation by
malicious actors, a central question. However, predictions made by machine learning models are
often brittle. A prominent example is the existence of adversarial examples (Szegedy et al., 2014):
imperceptibly modiﬁed inputs that cause state-of-the-art models to misclassify with high conﬁdence.

There has been a long line of work on both generating adversarial examples, called attacks (Carlini
and Wagner, 2017b;a; Athalye et al., 2018a;b; Uesato et al., 2018; Evtimov et al., 2017), and training
models robust to adversarial examples, called defenses (Goodfellow et al., 2015; Papernot et al.,
2016; Madry et al., 2018; Kannan et al., 2018). However, recent research has shown that most
defenses are ineffective (Carlini and Wagner, 2017a; Athalye et al., 2018a; Uesato et al., 2018).
Furthermore, even for defenses such as that of Madry et al. (2018) that have seen empirical success
against many attacks, we are unable to conclude yet with certainty that they are robust to all attacks
that we want these models to be resilient to.

This state of affairs gives rise to the need for veriﬁcation of networks, i.e., the task of formally
proving that no small perturbations of a given input can cause it to be misclassiﬁed by the network
model. Although many exact veriﬁers1 have been designed to solve this problem, the veriﬁcation
process is often intractably slow. For example, when using the Reluplex veriﬁer of Katz et al. (2017),
even verifying a small MNIST network turns out to be computationally infeasible. Thus, addressing
this intractability of exact veriﬁcation is the primary goal of this work.

Our Contributions
Our starting point is the observation that, typically, model training and veriﬁcation are decoupled
and seen as two distinct steps. Even though this separation is natural, it misses a key opportunity:
the ability to align these two stages. Speciﬁcally, applying the principle of co-design during model

1Also sometimes referred to as combinatorial veriﬁers.

1

Published as a conference paper at ICLR 2019

training is possible: training models in a way to encourage them to be simultaneously robust and
easy-to-exactly-verify. This insight is the cornerstone of the techniques we develop in this paper.

In this work, we use the principle of co-design to develop training techniques that give models
that are both robust and easy-to-verify. Our techniques rely on improving two key properties of
networks: weight sparsity and ReLU stability. Speciﬁcally, we ﬁrst show that natural methods for
improving weight sparsity during training, such as (cid:96)1-regularization, give models that can already be
veriﬁed much faster than current methods. This speedup happens because in general, exact veriﬁers
beneﬁt from having fewer variables in their formulations of the veriﬁcation task. For instance, for
exact veriﬁers that rely on linear programming (LP) solvers, sparser weight matrices means fewer
variables in those constraints.

We then focus on the major speed bottleneck of current approaches to exact veriﬁcation of ReLU
networks: the need of exact veriﬁcation methods to “branch,” i.e., consider two possible cases for
each ReLU (ReLU being active or inactive). Branching drastically increases the complexity of
veriﬁcation. Thus, well-optimized veriﬁers will not need to branch on a ReLU if it can determine
that the ReLU is stable, i.e.
that the ReLU will always be active or always be inactive for any
perturbation of an input. This motivates the key goal of the techniques presented in this paper: we
aim to minimize branching by maximizing the number of stable ReLUs. We call this goal ReLU
stability and introduce a regularization technique to induce it.

Our techniques enable us to train weight-sparse and ReLU stable networks for MNIST and CIFAR-
10 that can be veriﬁed signiﬁcantly faster. Speciﬁcally, by combining natural methods for inducing
weight sparsity with a robust adversarial training procedure (cf. Goodfellow et al. (2015)), we are
able to train networks for which almost 90% of inputs can be veriﬁed in an amount of time that
is small2 compared to previous veriﬁcation techniques. Then, by also adding our regularization
technique for inducing ReLU stability, we are able to train models that can be veriﬁed an addi-
tional 4–13x times as fast while maintaining state-of-the-art accuracy on MNIST. Our techniques
show similar improvements for exact veriﬁcation of CIFAR models. In particular, we achieve the
following veriﬁcation speed and provable robustness results for (cid:96)∞ norm-bound adversaries:

Dataset

Epsilon

Provable Adversarial Accuracy Average Solve Time (s)

MNIST

(cid:15) = 0.1
(cid:15) = 0.2
(cid:15) = 0.3

CIFAR

(cid:15) = 2/255
(cid:15) = 8/255

94.33%
89.79%
80.68%

45.93%
20.27%

0.49
1.13
2.78

13.50
22.33

Our network for (cid:15) = 0.1 on MNIST achieves provable adversarial accuracies comparable with the
current best results of Wong et al. (2018) and Dvijotham et al. (2018), and our results for (cid:15) = 0.2 and
(cid:15) = 0.3 achieve the best provable adversarial accuracies yet. To the best of our knowledge, we also
achieve the ﬁrst nontrivial provable adversarial accuracy results using exact veriﬁers for CIFAR-10.

Finally, we design our training techniques with universality as a goal. We focus on improving the
input to the veriﬁcation process, regardless of the veriﬁer we end up using. This is particularly
important because research into network veriﬁcation methods is still in its early stages, and our co-
design methods are compatible with the best current veriﬁers (LP/MILP-based methods) and should
be compatible with any future improvements in veriﬁcation.

Our code is available at https://github.com/MadryLab/relu_stable.

2 BACKGROUND AND RELATED WORK

Exact veriﬁcation of networks has been the subject of many recent works (Katz et al., 2017; Ehlers,
2017; Carlini et al., 2017; Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). To
understand the context of these works, observe that for linear networks, the task of exact veriﬁcation
is relatively simple and can be done by solving a LP. For more complex models, the presence of
nonlinear ReLUs makes veriﬁcation over all perturbations of an input much more challenging. This

2We chose our time budget for veriﬁcation to be 120 seconds per input image.

2

Published as a conference paper at ICLR 2019

is so as ReLUs can be active or inactive depending on the input, which can cause exact veriﬁers
to “branch" and consider these two cases separately. The number of such cases that veriﬁers have
to consider might grow exponentially with the number of ReLUs, so veriﬁcation speed will also
grow exponentially in the worst case. Katz et al. (2017) further illustrated the difﬁculty of exact
veriﬁcation by proving that it is NP-complete. In recent years, formal veriﬁcation methods were
developed to verify networks. Most of these methods use satisﬁability modulo theory (SMT) solvers
(Katz et al., 2017; Ehlers, 2017; Carlini et al., 2017) or LP and Mixed-Integer Linear Programming
(MILP) solvers (Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). However,
all of them are limited by the same issue of scaling poorly with the number of ReLUs in a network,
making them prohibitively slow in practice for even medium-sized models.

One recent approach for dealing with the inefﬁciency of exact veriﬁers is to focus on certiﬁcation
methods3 (Wong and Kolter, 2018; Wong et al., 2018; Dvijotham et al., 2018; Raghunathan et al.,
2018; Mirman et al., 2018; Sinha et al., 2018). In contrast to exact veriﬁcation, these methods do not
solve the veriﬁcation task directly; instead, they rely on solving a relaxation of the veriﬁcation prob-
lem. This relaxation is usually derived by overapproximating the adversarial polytope, or the space
of outputs of a network for a region of possible inputs. These approaches rely on training models in
a speciﬁc manner that makes certiﬁcation of those models easier. As a result, they can often obtain
provable adversarial accuracy results faster. However, certiﬁcation is fundamentally different from
veriﬁcation in two primary ways. First, it solves a relaxation of the original veriﬁcation problem. As
a result, certiﬁcation methods can fail to certify many inputs that are actually robust to perturbations
– only exact veriﬁers, given enough time, can give conclusive answers on robustness for every single
input. Second, certiﬁcation approaches fall under the paradigm of co-training, where a certiﬁcation
method only works well on models speciﬁcally trained for that certiﬁcation step. When used as a
black box on arbitrary models, the certiﬁcation step can yield a high rate of false negatives. For
example, Raghunathan et al. (2018) found that their certiﬁcation step was signiﬁcantly less effective
when used on a model trained using Wong and Kolter (2018)’s training method, and vice versa. In
contrast, we design our methods to be universal. They can be combined with any standard train-
ing procedure for networks and will improve exact veriﬁcation speed for any LP/MILP-based exact
veriﬁer. Our methods can also decrease the amount of overapproximation incurred by certiﬁcation
methods like Wong and Kolter (2018); Dvijotham et al. (2018). Similar to most of the certiﬁcation
methods, our technique can be made to have very little training time overhead.

Finally, subsequent work of Gowal et al. (2018) shows how applying interval bound propagation
during training, combined with MILP-based exact veriﬁcation, can lead to provably robust networks.

3 TRAINING VERIFIABLE NETWORK MODELS

We begin by discussing the task of verifying a network and identify two key properties of networks
that lead to improved veriﬁcation speed: weight sparsity and so-called ReLU stability. We then use
natural regularization methods for inducing weight sparsity as well as a new regularization method
for inducing ReLU stability. Finally, we demonstrate that these methods signiﬁcantly speed up
veriﬁcation while maintaining state-of-the-art accuracy.

3.1 VERIFYING ADVERSARIAL ROBUSTNESS OF NETWORK MODELS

Deep neural network models. Our focus will be on one of the most common architectures for
state-of-the-art models: k-layer fully-connected feed-forward DNN classiﬁers with ReLU non-
linearities4. Such models can be viewed as a function f (·, W, b), where W and b represent the
weight matrices and biases of each layer. For an input x, the output f (x, W, b) of the DNN is

3These works use both “veriﬁcation” and “certiﬁcation” to describe their methods. For clarity, we use “cer-
tiﬁcation” to describe their approaches, while we use “veriﬁcation” to describe exact veriﬁcation approaches.
For a more detailed discussion of the differences, see Appendix F.

4Note that this encompasses common convolutional network architectures because every convolutional layer

can be replaced by an equivalent fully-connected layer.

3

Published as a conference paper at ICLR 2019

deﬁned as:

z0 = x
ˆzi = zi−1Wi + bi
zi = max( ˆzi, 0)

(1)
(2)
(3)
(4)
Here, for each layer i, we let ˆzij denote the jth ReLU pre-activation and let ˆzij(x) denote the value
of ˆzij on an input x. ˆzk−1 is the ﬁnal, output layer with an output unit for each possible label (the
logits). The network will make predictions by selecting the label with the largest logit.

for i = 1, 2, . . . , k − 1
for i = 1, 2, . . . , k − 2

f (x, W, b) = ˆzk−1

Adversarial robustness. For a network to be reliable, it should make predictions that are robust
– that is, it should predict the same output for inputs that are very similar. Speciﬁcally, we want
the DNN classiﬁer’s predictions to be robust to a set Adv(x) of possible adversarial perturbations
of an input x. We focus on (cid:96)∞ norm-bound adversarial perturbations, where Adv(x) = {x(cid:48)
:
||x(cid:48) − x||∞ ≤ (cid:15)} for some constant (cid:15), since it is the most common one considered in adversarial
robustness and veriﬁcation literature (thus, it currently constitutes a canonical benchmark). Even so,
our methods can be applied to other (cid:96)p norms and broader sets of perturbations.
Veriﬁcation of network models. For an input x with correct label y, a perturbed input x(cid:48) can cause
a misclassiﬁcation if it makes the logit of some incorrect label ˆy larger than the logit of y on x(cid:48). We
can thus express the task of ﬁnding an adversarial perturbation as the optimization problem:

f (x(cid:48), W )y − f (x(cid:48), W )ˆy

min
x(cid:48),ˆy
subject to x(cid:48) ∈ Adv(x)

An adversarial perturbation exists if and only if the objective of the optimization problem is negative.

Adversarial accuracies. We deﬁne the true adversarial accuracy of a model to be the fraction of
test set inputs for which the model is robust to all allowed perturbations. By deﬁnition, evaluations
against speciﬁc adversarial attacks like PGD or FGSM provide an upper bound to this accuracy,
while certiﬁcation methods provide lower bounds. Given sufﬁcient time for each input, an exact
veriﬁer can prove robustness to perturbations, or ﬁnd a perturbation where the network makes a
misclassiﬁcation on the input, and thus exactly determine the true adversarial accuracy. This is in
contrast to certiﬁcation methods, which solve a relaxation of the veriﬁcation problem and can not
exactly determine the true adversarial accuracy no matter how much time they have.

However, such exact veriﬁcation may take impractically long for certain inputs, so we instead com-
pute the provable adversarial accuracy, which we deﬁne as the fraction of test set inputs for which
the veriﬁer can prove robustness to perturbations within an allocated time budget (timeout). Simi-
larly to certiﬁable accuracy, this accuracy constitutes a lower bound on the true adversarial accuracy.
A model can thus, e.g., have high true adversarial accuracy and low provable adversarial accuracy if
veriﬁcation of the model is too slow and often fails to complete before timeout.

Also, in our evaluations, we chose to use the MILP exact veriﬁer of Tjeng et al. (2019) when per-
forming experiments, as it is both open source and the fastest veriﬁer we are aware of.

3.2 WEIGHT SPARSITY AND ITS IMPACT ON VERIFICATION SPEED

The ﬁrst property of network models that we want to improve in order to speed up exact veriﬁcation
of those models is weight sparsity. Weight sparsity is important for veriﬁcation speed because many
exact veriﬁers rely on solving LP or MILP systems, which beneﬁt from having fewer variables.
We use two natural regularization methods for improving weight sparsity: (cid:96)1-regularization and
small weight pruning. These techniques signiﬁcantly improve veriﬁcation speed – see Table 1.
Verifying even small MNIST networks is almost completely intractable without them. Speciﬁcally,
the veriﬁer can only prove robustness of an adversarially-trained model on 19% of inputs with a one
hour budget per input, while the veriﬁer can prove robustness of an adversarially-trained model with
(cid:96)1-regularization and small weight pruning on 89.13% of inputs with a 120 second budget per input.

Interestingly, even though adversarial training improves weight sparsity (see Appendix B) it was
still necessary to use (cid:96)1-regularization and small weight pruning. These techniques further promoted
weight sparsity and gave rise to networks that were much easier to verify.

4

Published as a conference paper at ICLR 2019

Dataset

Epsilon

Training
Method

Test Set
Accuracy

Provable Adversarial
Accuracy

Average
Solve Time (s)

MNIST (cid:15) = 0.1

1 Adversarial Training
+(cid:96)1-Regularization
2
+Small Weight Pruning
3
4

+ReLU Pruning (control)

99.17%
99.00%
98.99%
98.94%

19.00%
82.17%
89.13%
91.58%

2970.43
21.99
11.71
6.43

Table 1: Improvement in provable adversarial accuracy and veriﬁcation solve times when incremen-
tally adding natural regularization methods for improving weight sparsity and ReLU stability into
the model training procedure, before veriﬁcation occurs. Each row represents the addition of another
method – for example, Row 3 uses adversarial training, (cid:96)1-regularization, and small weight pruning.
Row 4 adds ReLU pruning (see Appendix A). Row 4 is the control model for MNIST and (cid:15) = 0.1
that we present again in comparisons in Tables 2 and 3. We use a 3600 instead of 120 second timeout
for Row 1 and only veriﬁed the ﬁrst 100 images (out of 10000) because verifying it took too long.

3.3 RELU STABILITY

Next, we target the primary speed bottleneck of exact veriﬁcation: the number of ReLUs the veriﬁer
has to branch on. In our paper, this corresponds to the notion of inducing ReLU stability. Before we
describe our methodology, we formally deﬁne ReLU stability.

(2)).

Given an input x, a set of allowed perturbations Adv(x), and a ReLU, exact veriﬁers may need
to branch based on the possible pre-activations of the ReLU, namely ˆzij(Adv(x)) = {ˆzij(x(cid:48)) :
x(cid:48) ∈ Adv(x)} (cf.
If there exist two perturbations x(cid:48), x(cid:48)(cid:48) in the set Adv(x) such that
sign(ˆzij(x(cid:48))) (cid:54)= sign(ˆzij(x(cid:48)(cid:48))), then the veriﬁer has to consider that for some perturbed inputs the
ReLU is active (zij = ˆzij) and for other perturbed inputs inactive (zij = 0). The more such cases
the veriﬁer faces, the more branches it has to consider, causing the complexity of veriﬁcation to in-
crease exponentially. Intuitively, a model with 1000 ReLUs among which only 100 ReLUs require
branching will likely be much easier to verify than a model with 200 ReLUs that all require branch-
ing. Thus, it is advantageous for the veriﬁer if, on an input x with allowed perturbation set Adv(x),
the number of ReLUs such that

sign(ˆzij(x(cid:48))) = sign(ˆzij(x)) ∀x(cid:48) ∈ Adv(x)
is maximized. We call a ReLU for which (5) holds on an input x a stable ReLU on that input. If (5)
does not hold, then the ReLU is an unstable ReLU.

(5)

Directly computing whether a ReLU is stable on a given input x is difﬁcult because doing so would
involve considering all possible values of ˆzij(Adv(x)). Instead, exact veriﬁers compute an upper
bound ˆuij and a lower bound ˆlij of ˆzij(Adv(x)). If 0 ≤ ˆlij or ˆuij ≤ 0, they can replace the ReLU
with the identity function or the zero function, respectively. Otherwise, if ˆlij < 0 < ˆuij, these
veriﬁers then determine that they need to “branch” on that ReLU. Thus, we can rephrase (5) as

sign(ˆuij) = sign(ˆlij)

(6)

We will discuss methods for determining these upper and lower bounds ˆuij, ˆlij in Section 3.3.2.

3.3.1 A REGULARIZATION TECHNIQUE FOR INDUCING RELU STABILITY: RS LOSS

As we see from equation (6), a function that would indicate exactly when a ReLU is stable is
F ∗(ˆuij, ˆlij) = sign(ˆuij) · sign(ˆlij). Thus, it would be natural to use this function as a regular-
izer. However, this function is non-differentiable and if used in training a model, would provide no
useful gradients during back-propagation. Thus, we use the following smooth approximation of F ∗
(see Fig. 1) which provides the desired gradients:

F (ˆuij, ˆlij) = − tanh(1 + ˆuij · ˆlij)

We call the corresponding objective RS Loss, and show in Fig. 2a that using this loss function as
a regularizer effectively decreases the number of unstable ReLUs. RS Loss thus encourages ReLU
stability, which, in turn, speeds up exact veriﬁcation - see Fig. 2b.

5

Published as a conference paper at ICLR 2019

Figure 1: Plot and contour plot of the function F (x, y) = − tanh(1 + x · y)

3.3.2 ESTIMATING RELU UPPER AND LOWER BOUNDS ON ACTIVATIONS

A key aspect of using RS Loss is determining the upper and lower bounds ˆuij, ˆlij for each ReLU (cf.
(6)). The bounds for the inputs z0 (cf. (1)) are simple – for the input x, we know x − (cid:15) ≤ z0 ≤ x + (cid:15),
so ˆu0 = x − (cid:15), ˆl0 = x + (cid:15). For all subsequent zij, we estimate bounds using either the naive interval
arithmetic (IA) approach described in Tjeng et al. (2019) or an improved version of it. The improved
version is a tighter estimate but uses more memory and training time, and thus is most effective on
smaller networks. We present the details of naive IA and improved IA in Appendix C.

Interval arithmetic approaches can be implemented relatively efﬁciently and work well with back-
propagation because they only involve matrix multiplications. This contrasts with how exact veri-
ﬁers compute these bounds, which usually involves solving LPs or MILPs. Interval arithmetic also
overestimates the number of unstable ReLUs. This means that minimizing unstable ReLUs based
on IA bounds will provide an upper bound on the number of unstable ReLUs determined by exact
veriﬁers. In particular, IA will properly penalize every unstable ReLU.

Improved IA performs well in practice, overestimating the number of unstable ReLUs by less than
0.4% in the ﬁrst 2 layers of MNIST models and by less than 36.8% (compared to 128.5% for naive
IA) in the 3rd layer. Full experimental results are available in Table 4 of Appendix C.3.

3.3.3

IMPACT OF RELU STABILITY IMPROVEMENTS ON VERIFICATION SPEED

We provide experimental evidence that RS Loss regularization improves ReLU stability and speeds
up average veriﬁcation times by more than an order of magnitude in Fig. 2b. To isolate the effect of
RS Loss, we compare MNIST models trained in exactly the same way other than the weight on RS
Loss. When comparing a network trained with a RS Loss weight of 5e−4 to a network with a RS
Loss weight of 0, the former has just 16% as many unstable ReLUs and can be veriﬁed 65x faster.
The caveat here is that the former has 1.00% lower test set accuracy.

We also compare veriﬁcation speed with and without RS Loss on MNIST networks for different
values of (cid:15) (0.1, 0.2, and 0.3) in Fig. 2c. We choose RS Loss weights that cause almost no test set
accuracy loss (less than 0.50% - See Table 3) in these cases, and we still observe a 4–13x speedup
from RS Loss. For CIFAR, RS Loss gives a smaller speedup of 1.6–3.7x (See Appendix E).

3.3.4

IMPACT OF RELU STABILITY IMPROVEMENTS ON PROVABLE ADVERSARIAL
ACCURACY

As the weight on the RS Loss used in training a model increases, the ReLU stability of the model
will improve, speeding up veriﬁcation and likely improving provable adversarial accuracy. However,
like most regularization methods, placing too much weight on RS Loss can decrease the model ca-
pacity, potentially lowering both the true adversarial accuracy and the provable adversarial accuracy.
Therefore, it is important to choose the weight on RS Loss carefully to obtain both high provable
adversarial accuracy and faster veriﬁcation speeds.

To show the effectiveness of RS Loss in improving provable adversarial accuracy, we train two
networks for each dataset and each value of (cid:15). One is a “control” network that uses all of the natural
improvements for inducing both weight sparsity ((cid:96)1-regularization and small weight pruning) and
ReLU stability (ReLU pruning - see Appendix A). The second is a “+RS” network that uses RS

6

Published as a conference paper at ICLR 2019

(a)

(b)

(c)

Figure 2: (a) Average number of unstable ReLUs by layer and (b) average veriﬁcation solve times
of 6 networks trained with different weights on RS Loss for MNIST and (cid:15) = 0.1 . Averages are
taken over all 10000 MNIST test set inputs. Both metrics improve signiﬁcantly with increasing
RS Loss weight. An RS Loss weight of 0 corresponds to the control network, while an RS Loss
weight of 0.00012 corresponds to the “+RS” network for MNIST and (cid:15) = 0.1 in Tables 2 and 3. (c)
Improvement in the average time taken by a veriﬁer to solve the veriﬁcation problem after adding
RS Loss to the training procedure, for different (cid:15) on MNIST. The weight on RS Loss was chosen so
that the “+RS” models have test set accuracies within 0.50% of the control models.

Loss in addition to all of the same natural improvements. This lets us isolate the incremental effect
of adding RS Loss to the training procedure.

In addition to attaining a 4–13x speedup in MNIST veriﬁcation times (see Fig. 2c), we achieve
higher provable adversarial accuracy in every single setting when using RS Loss. This is especially
notable for the hardest veriﬁcation problem we tackle – proving robustness to perturbations with (cid:96)∞
norm-bound 8/255 on CIFAR-10 – where adding RS Loss nearly triples the provable adversarial
accuracy from 7.09% to 20.27%. This improvement is primarily due to veriﬁcation speedup induced
by RS Loss, which allows the veriﬁer to ﬁnish proving robustness for far more inputs within the 120
second time limit. These results are shown in Table 2.

Table 2: Provable Adversarial Accuracies for the control and “+RS” networks in each setting.

MNIST, (cid:15) = 0.1 MNIST, (cid:15) = 0.2 MNIST, (cid:15) = 0.3 CIFAR, (cid:15) = 2/255 CIFAR, (cid:15) = 8/255

Control
+RS

91.58
94.33

86.45
89.79

77.99
80.68

45.53
45.93

7.09
20.27

4 EXPERIMENTS

4.1 EXPERIMENTS ON MNIST AND CIFAR

In addition to the experimental results already presented, we compare our control and “+RS” net-
works with the best available results presented in the state-of-the-art certiﬁable defenses of Wong
et al. (2018), Dvijotham et al. (2018), and Mirman et al. (2018) in Table 3. We compare their test
set accuracy, PGD adversarial accuracy (an evaluation of robustness against a strong 40-step PGD
adversarial attack), and provable adversarial accuracy. Additionally, to show that our method can
scale to larger architectures, we train and verify a “+RS (Large)” network for each dataset and (cid:15).

In terms of provable adversarial accuracies, on MNIST, our results are signiﬁcantly better than those
of Wong et al. (2018) for larger perturbations of (cid:15) = 0.3, and comparable for (cid:15) = 0.1. On CIFAR-
10, our method is slightly less effective, perhaps indicating that more unstable ReLUs are necessary
to properly learn a robust CIFAR classiﬁer. We also experienced many more instances of the veriﬁer
reaching its allotted 120 second time limit on CIFAR, especially for the less ReLU stable control
networks. Full experimental details for each model in Tables 1, 2, and 3, including a breakdown of
veriﬁcation solve results (how many images did the veriﬁer: A. prove robust B. ﬁnd an adversarial
example for C. time out on), are available in Appendix E.

7

Published as a conference paper at ICLR 2019

Table 3: Comparison of test set, PGD adversarial, and provable adversarial accuracy of networks
trained with and without RS Loss. We also provide the best available certiﬁable adversarial and
PGD adversarial accuracy of any single models from Wong et al. (2018), Dvijotham et al. (2018),
and Mirman et al. (2018) for comparison, and highlight the best provable accuracy for each (cid:15).
* The provable adversarial accuracy for “+RS (Large)” is only computed for the ﬁrst 1000 images
because the veriﬁer performs more slowly on larger models.
** Dvijotham et al. (2018); Mirman et al. (2018) use a slightly smaller (cid:15) = 0.03 = 7.65/255.
† Mirman et al. (2018) computes results over 500 images instead of all 10000.
†† Mirman et al. (2018) uses a slightly smaller (cid:15) = 0.007 = 1.785/255.

Dataset

Epsilon

Test Set
Accuracy

PGD Adversarial

Provable/Certiﬁable
Accuracy Adversarial Accuracy

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

Training
Method

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†,††

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.**
Mirman et al.†, **

98.94%
98.68%
98.95%

98.92%
98.80%
99.00%

98.40%
98.10%
98.21%

97.75%
97.33%
97.54%

85.13%
96.60%

64.64%
61.12%
61.41%

68.28%
62.00%

50.69%
40.45%
42.81%

28.67%
48.64%
54.20%

95.12%
95.13%
96.58%

-
97.13%
97.60%

93.14%
93.14%
94.19%

91.64%
92.05%
93.25%

-
93.80%

51.58%
49.92%
50.61%

-
54.60%

31.28%
26.78%
28.69%

-
32.72%
40.00%

91.58%
94.33%
95.60%

96.33%
95.56%
96.60%

86.45%
89.79%
89.10%

77.99%
80.68%
59.60%

56.90%
82.00%

45.53%
45.93%
41.40%

53.89%
52.20%

7.09%
20.27%
19.80%

21.78%
26.67%
35.20%

4.2 EXPERIMENTAL METHODS AND DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. For each setting of dataset
(MNIST or CIFAR) and (cid:15), we ﬁnd a suitable weight on RS Loss via line search (See Table 6 in
Appendix D). The same weight is used for each ReLU. During training, we used improved IA for
ReLU bound estimation for “+RS” models and use naive IA for “+RS (Large)” models because of
memory constraints. For ease of comparison, we trained our networks using the same convolutional
DNN architecture as in Wong et al. (2018). This architecture uses two 2x2 strided convolutions with
16 and 32 ﬁlters, followed by a 100 hidden unit fully connected layer. For the larger architecture,
we also use the same “large” architecture as in Wong et al. (2018). It has 4 convolutional layers with
32, 32, 64, and 64 ﬁlters, followed by 2 fully connected layers with 512 hidden units each.

For veriﬁcation, we used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019).
Model solves were parallelized over 8 CPU cores. When verifying an image, the veriﬁer of Tjeng
et al. (2019) ﬁrst builds a model, and second, solves the veriﬁcation problem (See Appendix D.2 for
details). We focus on reporting solve times because that is directly related to the task of veriﬁcation
itself. All build times for the control and “+RS” models on MNIST that we presented were between
4 and 10 seconds, and full results on build times are also presented in Appendix E.

Additional details on our experimental setup (e.g. hyperparameters) can be found in Appendix D.

8

Published as a conference paper at ICLR 2019

5 CONCLUSION

In this paper, we use the principle of co-design to develop training methods that emphasize veri-
ﬁcation as a goal, and we show that they make verifying the trained model much faster. We ﬁrst
demonstrate that natural regularization methods already make the exact veriﬁcation problem sig-
niﬁcantly more tractable. Subsequently, we introduce the notion of ReLU stability for networks,
present a method that improves a network’s ReLU stability, and show that this improvement makes
veriﬁcation an additional 4–13x faster. Our method is universal, as it can be added to any training
procedure and should speed up any exact veriﬁcation procedure, especially MILP-based methods.

Prior to our work, exact veriﬁcation seemed intractable for all but the smallest models. Thus, our
work shows progress toward reliable models that can be proven to be robust, and our techniques can
help scale veriﬁcation to even larger networks.

Many of our methods appear to compress our networks into more compact, simpler forms. We
hypothesize that the reason that regularization methods like RS Loss can still achieve very high
accuracy is that most models are overparametrized in the ﬁrst place. There exist clear parallels
between our methods and techniques in model compression (Han et al., 2016; Cheng et al., 2017b) –
therefore, we believe that drawing upon additional techniques from model compression can further
improve the ease-of-veriﬁcation of networks. We also expect that there exist objectives other than
weight sparsity and ReLU stability that are important for veriﬁcation speed. If so, further exploring
the principle of co-design for those objectives is an interesting future direction.

ACKNOWLEDGEMENTS

This work was supported by the NSF Graduate Research Fellowship under Grant No. 1122374,
by the NSF grants CCF-1553428 and CNS-1815221, and by Lockheed Martin Corporation under
award number RPP2016-002. We would like to thank Krishnamurthy Dvijotham, Ludwig Schmidt,
Michael Sun, Dimitris Tsipras, and Jonathan Uesato for helpful discussions.

9

Published as a conference paper at ICLR 2019

REFERENCES

Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense
In International Conference on

of security: Circumventing defenses to adversarial examples.
Machine Learning (ICML), 2018a.

Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. In International Conference on Machine Learning (ICML), pages 284–293, 2018b.

Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
In Proceedings of ACM Workshop on Artiﬁcial Intelligence and Security

detection methods.
(AISec), 2017a.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks.

In

Security and Privacy (SP), 2017 IEEE Symposium on, 2017b.

Nicholas Carlini, Guy Katz, Clark Barrett, and David L. Dill. Ground-truth adversarial examples.

2017.

networks. 2017a.

Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artiﬁcial neural

Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration

for deep neural networks. 2017b.

Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training veriﬁed learners with learned ver-
iﬁers. 2018.

Rüdiger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In Auto-

mated Technology for Veriﬁcation and Analysis, 2017.

Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir
Rahmati, and Dawn Song. Robust physical-world attacks on machine learning models. 2017.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. In International Conference on Learning Representations (ICLR), 2015.

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. 2018.

Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. In International Conference on Learning
Representations (ICLR), 2016.

Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, An-
drew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep
neural networks for acoustic modeling in speech recognition: The shared views of four research
groups. In IEEE Signal Processing Magazine, 2012.

Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. 2018.

Guy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer. Reluplex: An efﬁcient

smt solver for verifying deep neural networks. 2017.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations (ICLR), 2015.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012.

Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward relu

neural networks. 2017.

10

Published as a conference paper at ICLR 2019

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations (ICLR), 2018.

Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning (ICML), 2018.

Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation
as a defense to adversarial perturbations against deep neural networks. In Proceedings of IEEE
Symposium on Security and Privacy (SP), 2016.

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial exam-

ples. In International Conference on Learning Representations (ICLR), 2018.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 2017.

Aman Sinha, Hongseok Namkoong, and John Duchi. Certiﬁable distributional robustness with
principled adversarial training. In International Conference on Learning Representations (ICLR),
2018.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations (ICLR), 2014.

Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap
In Computer Vision and Pattern Recognition

to human-level performance in face veriﬁcation.
(CVPR), 2014.

Robert Tibshirani. Regression shrinkage and selection via the lasso. In Journal of the Royal Statis-

tical Society, Series B, 1994.

Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In International Conference on Learning Representations (ICLR), 2019.

Jonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial
risk and the dangers of evaluating against weak attacks. In International Conference on Machine
Learning (ICML), 2018.

Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer

adversarial polytope. In International Conference on Machine Learning (ICML), 2018.

Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and Zico Kolter. Scaling provable adversarial

defenses. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

11

Published as a conference paper at ICLR 2019

APPENDIX

A NATURAL IMPROVEMENTS

A.1 NATURAL REGULARIZATION FOR INDUCING WEIGHT SPARSITY

All of the control and “+RS” networks in our paper contain natural improvements that improve
weight sparsity, which reduce the number of variables in the LPs solved by the veriﬁer. We ob-
served that the two techniques we used for weight sparsity ((cid:96)1-regularization and small weight prun-
ing) don’t hurt test set accuracy but they dramatically improve provable adversarial accuracy and
veriﬁcation speed.

1. (cid:96)1-regularization: We use a weight of 2e−5 on MNIST and a weight of 1e−5 on CIFAR.
We chose these weights via line search by ﬁnding the highest weight that would not hurt
test set accuracy.

2. Small weight pruning: Zeroing out weights in a network that are very close to zero. We

choose to prune weights less than 1e−3.

A.2 A BASIC IMPROVEMENT FOR INDUCING RELU STABILITY: RELU PRUNING

We also use a basic idea to improve ReLU stability, which we call ReLU pruning. The main idea is
to prune away ReLUs that are not necessary.

We use a heuristic to test whether a ReLU in a network is necessary. Our heuristic is to count how
many training inputs cause the ReLU to be active or inactive. If a ReLU is active (the pre-activation
satisﬁes ˆzij(x) > 0) for every input image in the training set, then we can replace that ReLU with
the identity function and the network would behave in exactly the same way for all of those images.
Similarly, if a ReLU is inactive (ˆzij(x) < 0) for every training image, that ReLU can be replaced by
the zero function.

Extending this idea further, we expect that ReLUs that are rarely used can also be removed without
signiﬁcantly changing the behavior of the network. If only a small fraction (say, 10%) of the input
images activate a ReLU, then replacing the ReLU with the zero function will only slightly change the
network’s behavior and will not affect the accuracy too much. We provide experimental evidence of
this phenomenon on an adversarially trained ((cid:15) = 0.1) MNIST model. Conservatively, we decided
that pruning away ReLUs that are active on less than 10% of the training set or inactive on less than
10% of the training set was reasonable.

Figure 3: Removing some ReLUs does not hurt test set accuracy or accuracy against a PGD adver-
sary

12

Published as a conference paper at ICLR 2019

B ADVERSARIAL TRAINING AND WEIGHT SPARSITY

It is worth noticing that adversarial training against (cid:96)∞ norm-bound adversaries alone already makes
networks easier to verify by implicitly improving weight sparsity. Indeed, this can be shown clearly
in the case of linear networks. Recall that a linear network can be expressed as f (x) = W x + b.
Thus, an (cid:96)∞ norm-bound perturbation of the input x will produce the output

f (x(cid:48)) = x(cid:48)W + b

= xW + b + (x(cid:48) − x)W
≤ f (x) + (cid:15)||W ||1

where the last inequality is just Hölder’s inequality.
In order to limit the adversary’s ability to
perturb the output, adversarial training needs to minimize the ||W ||1 term, which is equivalent to (cid:96)1-
regularization and is known to promote weight sparsity (Tibshirani, 1994). Relatedly, Goodfellow
et al. (2015) already pointed out that adversarial attacks against linear networks will be stronger
when the (cid:96)1-norm of the weight matrices is higher.

Even in the case of nonlinear networks, adversarial training has experimentally been shown to im-
prove weight sparsity. For example, models trained according to Madry et al. (2018) and Wong and
Kolter (2018) often learn many weight-sparse layers, and we observed similar trends in the mod-
els we trained. However, it is important to note that while adversarial training alone does improve
weight sparsity, it is not sufﬁcient by itself for efﬁcient exact veriﬁcation. Additional regularization
like (cid:96)1-regularization and small weight pruning further promotes weight sparsity and gives rise to
networks that are much easier to verify.

13

Published as a conference paper at ICLR 2019

C INTERVAL ARITHMETIC

C.1 NAIVE INTERVAL ARITHMETIC

Naive IA determines upper and lower bounds for a layer based solely on the upper and lower bounds
of the previous layer.
Deﬁne W + = max(W, 0), W − = min(W, 0), u = max(ˆu, 0), and l = max(ˆl, 0). Then the bounds
on the pre-activations of layer i can be computed as follows:
i + li−1W −
i + ui−1W −

ˆui = ui−1W +
ˆli = li−1W +

i + bi
i + bi

(8)

(7)

As noted in Tjeng et al. (2019) and also Dvijotham et al. (2018), this method is efﬁcient but can lead
to relatively conservative bounds for deeper networks.

C.2

IMPROVED INTERVAL ARITHMETIC

We improve upon naive IA by exploiting ReLUs that we can determine to always be active. This
allows us to cancel symbols that are equivalent that come from earlier layers of a network.

We will use a basic example of a neural network with one hidden layer to illustrate this idea. Suppose
that we have the scalar input z0 with l0 = 0, u0 = 1, and the network has the following weights and
biases:

W1 = [1 − 1] ,

b1 = [2

2] , W2 =

b2 = 0

(cid:21)
(cid:20)1
1

,

Naive IA for the ﬁrst layer gives ˆl1 = l1 = [2
2], and applying naive IA to
the output ˆz2 gives ˆl2 = 3, ˆu2 = 5. However, because ˆl1 > 0, we know that the two ReLUs in the
hidden layer are always active and thus equivalent to the identity function. Then, the output is

1], ˆu1 = u1 = [3

ˆz2 = z11 + z12 = ˆz11 + ˆz12 = (z0 + 2) + (−z0 + 2) = 4

Thus, we can obtain the tighter bounds ˆl2 = ˆu2 = 4, as we are able to cancel out the z0 terms.
We can write this improved version of IA as follows. First, letting Wk denote row k of matrix W ,
we can deﬁne the “active” part of W as the matrix WA, where

(WA)k =

(cid:40)

Wk
0

if ˆli−1 > 0
if ˆli−1 ≤ 0

Deﬁne the “non-active” part of W as

WN = W − WA
Then, using the same deﬁnitions for the notation W +, W −, u, l as before, we can write down the
following improved version of IA which uses information from the previous 2 layers.

ˆui = ui−1Wi

+
N + li−1Wi

−
N + bi
+ ui−2(Wi−1WiA)+ + li−2(Wi−1WiA)− + bi−1WiA
−
N + bi
+ li−2(Wi−1WiA)+ + ui−2(Wi−1WiA)− + bi−1WiA
We are forced to to use li−1,j and ui−1,j if we can not determine whether or not the ReLU corre-
sponding to the activation zi−1,j is active, but we use li−2 and ui−2 whenever possible.

+
N + ui−1Wi

ˆli = li−1Wi

We now deﬁne some additional notation to help us extend this method to any number of layers. We
now seek to deﬁne fn, which is a function which takes in four sequences of length n – upper bounds,
lower bounds, weights, and biases – and outputs the current layer’s upper and lower bounds.

What we have derived so far from (7) and (8) is the following

f1(ui−1, li−1, Wi, bi) = (ui−1W +

i + li−1W −

i + bi, li−1W +

i + ui−1W −

i + bi)

14

Published as a conference paper at ICLR 2019

Let u denote a sequence of upper bounds. Let uz denote element z of the sequence, and let u[z:]
denote the sequence without the ﬁrst z elements. Deﬁne notation for l, W, and b similarly.

Then, using the fact that WN Z = (W Z)N and WAZ = (W Z)A, we can show that the following
recurrence holds:

fn+1(u, l, W, b) = f1(u1, l1, W1N , b1)

+ fn(u[1:], l[1:], (W2W1A, W[2:]), (b2W1A, b[2:]))

(9)

Let u(x,y) denote the sequence (ux, ux−1, · · · , uy), and deﬁne l(x,y), W(x,y), and b(x,y) similarly.
Then, if we want to compute the bounds on layer k using all information from the previous k layers,
we simply have to compute fk(u(k−1,0), l(k−1,0), W(k,1), b(k,1)).

From the recurrence 9, we see that using information from all previous layers to compute bounds for
layer k takes O(k) matrix-matrix multiplications. Thus, using information from all previous layers
to compute bounds for all layers of a d layer neural network only involves O(d2) additional matrix
multiplications, which is still reasonable for most DNNs. This method is still relatively efﬁcient
because it only involves matrix multiplications – however, needing to perform matrix-matrix multi-
plications as opposed to just matrix-vector multiplications results in a slowdown and higher memory
usage when compared to naive IA. We believe the improvement in the estimate of ReLU upper and
lower bounds is worth the time trade-off for most networks.

C.3 EXPERIMENTAL RESULTS ON IMPROVED IA AND NAIVE IA

In Table 4, we show empirical evidence that the number of unstable ReLUs in each layer of a MNIST
network, as estimated by improved IA, tracks the number of unstable ReLUs determined by the exact
veriﬁer quite well. We also present estimates determined via naive IA for comparison.

Dataset

Epsilon Training Estimation

Method Method

Unstable ReLUs Unstable ReLUs Unstable ReLUs
in 1st Layer

in 2nd Layer

in 3rd Layer

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

Control

+RS

Control

+RS

Control

+RS

Exact
61.14
Improved IA 61.14
61.14
Naive IA

21.64
Exact
Improved IA 21.64
21.64
Naive IA

Exact
17.47
Improved IA 17.47
17.47
Naive IA

Exact
29.91
Improved IA 29.91
29.91
Naive IA

Exact
36.76
Improved IA 36.76
36.76
Naive IA

24.43
Exact
Improved IA 24.43
24.43
Naive IA

185.30
185.96 (+0.4%)
188.44 (+1.7%)

31.73
43.40 (+36.8%)
69.96 (+120.5%)

64.73
64.80 (+0.1%)
65.34 (+0.9%)

14.67
18.97 (+29.4%)
33.51 (+128.5%)

37.92
48.88 (+28.9%)
69.75 (+84.0%)

24.05
28.40 (+18.1%)
40.47 (+68.3%)

28.64
31.19 (+8.9%)
32.13 (+12.2%)

83.42
83.44 (+0.02%)
83.52 (+0.1%)

40.74
46.00 (+12.9%)
48.27 (+18.5%)

142.95
142.95
142.95

54.47
54.47
54.47

48.47
48.47
48.47

Table 4: Comparison between the average number of unstable ReLUs as found by the exact veriﬁer
of Tjeng et al. (2019) and the estimated average number of unstable ReLUs found by improved IA
and naive IA. We compare these estimation methods on the control and “+RS” networks for MNIST
that we described in Section 3.3.4

15

Published as a conference paper at ICLR 2019

C.4 ON THE CONSERVATIVE NATURE OF IA BOUNDS

The upper and lower bounds we compute on each ReLU via either naive IA or improved IA are
conservative. Thus, every unstable ReLU will always be correctly labeled as unstable, while stable
ReLUs can be labeled as either stable or unstable. Importantly, every unstable ReLU, as estimated
by IA bounds, is correctly labeled and penalized by RS Loss. The trade-off is that stable ReLUs
mislabeled as unstable will also be penalized, which can be an unnecessary regularization of the
model.

In Table 5 we show empirically that we can achieve the following two objectives at once when using
RS Loss combined with IA bounds.

1. Reduce the number of ReLUs labeled as unstable by IA, which is an upper bound on the

true number of unstable ReLUs as determined by the exact veriﬁer.

2. Achieve similar test set accuracy and PGD adversarial accuracy as a model trained without

RS Loss.

Dataset

Epsilon Training
Method

Estimation
Method

Total Labeled
Unstable ReLUs Accuracy

Test Set

PGD Adversarial
Accuracy

MNIST (cid:15) = 0.1

Control
+RS

Improved IA 290.5
Improved IA 105.4

Control (Large) Naive IA
Naive IA
+RS (Large)

835.8
150.3

98.94%
98.68% (-0.26%)

95.12%
95.13% (+0.01%)

99.04%
98.95% (-0.09%)

96.32%
96.58% (+0.26%)

Table 5: The addition of RS Loss results in far fewer ReLUs labeled as unstable for both 3-layer and
6-layer (Large) networks. The decrease in test set accuracy as a result of this regularization is small.

Even though IA bounds are conservative, these results show that it is still possible to decrease the
number of ReLUs labeled as unstable by IA without signiﬁcantly degrading test set accuracy. When
comparing the Control and “+RS” networks for MNIST and (cid:15) = 0.1, adding RS Loss decreased
the average number of ReLUs labeled as unstable (using bounds from Improved IA) from 290.5 to
105.4 with just a 0.26% loss in test set accuracy. The same trend held for deeper, 6-layer networks,
even when the estimation method for upper and lower bounds was the more conservative Naive IA.

16

Published as a conference paper at ICLR 2019

D FULL EXPERIMENTAL SETUP

D.1 NETWORK TRAINING DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. Following the prior
examples of Wong and Kolter (2018) and Dvijotham et al. (2018), we introduced a small tweak
where we increased the adversary strength linearly from 0.01 to (cid:15) over ﬁrst half of training and kept
it at (cid:15) for the second half. We used this training schedule to improve convergence of the training
process.

For MNIST, we trained for 70 epochs using the Adam optimizer (Kingma and Ba, 2015) with a
learning rate of 1e−4 and a batch size of 32. For CIFAR, we trained for 250 epochs using the Adam
optimizer with a learning rate of 1e−4. When using naive IA, we used a batch size of 128, and
when using improved IA, we used a batch size of 16. We used a smaller batch size in the latter
case because improved IA incurs high RAM usage during training. To speed up training on CIFAR,
we only added in RS Loss regularization in the last 20% of the training process. Using this same
sped-up training method on MNIST did not signiﬁcantly affect the results.

Dataset

Epsilon

(cid:96)1 weight RS Loss weight

MNIST 0.1
MNIST 0.2
MNIST 0.3
CIFAR 2/255
CIFAR 8/255

2e−5
2e−5
2e−5
1e−5
1e−5

12e−5
1e−4
12e−5
1e−3
2e−3

Table 6: Weights chosen using line search for (cid:96)1 regularization and RS Loss in each setting

For each setting, we ﬁnd a suitable weight on RS Loss via line search. The same weight is used for
each ReLU. The ﬁve weights we chose are displayed above in Table 6, along with weights chosen
for (cid:96)1-regularization.

We also train “+RS” models using naive IA to show that our technique for inducing ReLU stability
can work while having small training time overhead – full details on “+RS (Naive IA)” networks are
in Appendix E.

D.2 VERIFIER OVERVIEW

The MILP-based exact veriﬁer of Tjeng et al. (2019), which we use, proceeds in two steps for every
input. They are the model-build step and the solve step.

First, the veriﬁer builds a MILP model based on the neural network and the input. In particular, the
veriﬁer will compute upper and lower bounds on each ReLU using a speciﬁc bound computation al-
gorithm. We chose the default bound computation algorithm in the code, which uses LP to compute
bounds. LP bounds are tighter than the bounds computed via IA, which is another option available in
the veriﬁer. The model-build step’s speed appeared to depend primarily on the tightening algorithm
(IA was faster than LP) and the number of variables in the MILP (which, in turn, depends on the
sparsity of the weights of the neural network). The veriﬁer takes advantage of these bounds by not
introducing a binary variables into the MILP formulation if it can determine that a particular ReLU
is stable. Thus, using LP as the tightening algorithm resulted in higher build times, but led to easier
MILP formulations.

Next, the veriﬁer solves the MILP using an off-the-shelf MILP solver. The solver we chose was
the commercial Gurobi Solver, which uses a branch-and-bound method for solving MILPs. The
solver’s speed appeared to depend primarily on the number of binary variables in the MILP (which
corresponds to the number of unstable ReLUs) as well as the total number of variables in the MILP
(which is related to the sparsity of the weight matrices). While these two numbers are strongly
correlated with solve times, some solves would still take a long time despite having few binary

17

Published as a conference paper at ICLR 2019

variables. Thus, understanding what other properties of neural networks correspond to MILPs that
are easy or hard to solve is an important area to explore further.

D.3 VERIFIER DETAILS

We used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019) using the default
settings of the code. We allotted 120 seconds for veriﬁcation of each input datapoint using the
default model build settings. We ran our experiments using the commercial Gurobi Solver (version
7.5.2), and model solves were parallelized over 8 CPU cores with Intel Xeon CPUs @ 2.20GHz
processors. We used computers with 8–32GB of RAM, depending on the size of the model being
veriﬁed. All computers used are part of an OpenStack network.

18

Published as a conference paper at ICLR 2019

E FULL EXPERIMENTAL VERIFICATION RESULTS

Dataset

Epsilon

Training
Method

Test
Set Adversarial
Accuracy

Accuracy

Total
Provable
Upper Adversarial Unstable
Accuracy
Bound

Avg
Avg
Build
Solve
ReLUs Time (s) Time (s)

PGD Veriﬁer

Adversarial Training*
+(cid:96)1-regularization
+Small Weight Pruning
+ReLU Pruning (Control)

MNIST (cid:15) = 0.1

Wong et al. (2018)***

+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

99.17%
99.00%
98.99%
98.94%
98.68%
98.53%
98.95%

98.40%
98.10%
98.08%
98.21%
95.06%

97.75%
97.33%
97.06%
97.54%

64.64%
61.12%
57.83%
61.41%

50.69%
40.45%
46.19%
42.81%

95.04% 96.00%
95.25% 95.98%
95.38% 94.93%
95.12% 94.45%
95.13% 94.38%
94.86% 94.54%
96.58% 95.60%

93.14% 90.71%
93.14% 89.98%
91.68% 88.87%
94.19% 90.40%
-
89.03%

91.64% 83.83%
92.05% 81.70%
89.19% 79.12%
93.25% 83.70%

51.58% 50.23%
49.92% 47.79%
47.03% 45.33%
50.61% 51.00%

31.28% 33.46%
26.78% 22.74%
29.66% 26.07%
28.69% 25.20%

19.00%
82.17%
89.13%
91.58%
94.33%
94.32%
95.60%

86.45%
89.79%
85.54%
89.10%
80.29%

77.99%
80.68%
76.70%
59.60%

45.53%
45.93%
44.44%
41.40%

7.09%
20.27%
18.90%
19.80%

1517.9
505.3
502.7
278.2
101.0
158.3
119.5

2970.43
21.99
11.71
6.43
0.49
0.96
0.27

198.3
108.4
217.2
133.0
-

160.9
101.5
179.0
251.2

360.0
234.3
170.1
196.7

665.9
54.2
277.8
246.5

9.41
1.13
8.50
2.93
-

11.80
2.78
6.43
37.45

21.75
13.50
6.30
29.88

82.91
22.33
33.63
20.14

650.93
79.13
19.30
9.61
4.98
4.82
156.74

7.15
4.43
4.67
171.10
-

5.14
4.34
4.00
166.39

66.42
52.58
47.11
335.97

73.28
38.84
23.66
401.72

Table 7: Full results on natural improvements from Table 1, control networks (which use all of the
natural improvements and ReLU pruning), and “+RS” networks from Tables 2 and 3. While we are
unable to determine the true adversarial accuracy, we provide two upper bounds and a lower bound.
Evaluations of robustness against a strong 40-step PGD adversary (PGD adversarial accuracy) gives
one upper bound, and the veriﬁer itself gives another upper bound because it can also prove that
the network is not robust to perturbations on certain inputs by ﬁnding adversarial examples. The
veriﬁer simultaneously ﬁnds the provable adversarial accuracy, which is a lower bound on the true
adversarial accuracy. The gap between the veriﬁer upper bound and the provable adversarial accu-
racy (veriﬁer lower bound) corresponds to inputs that the veriﬁer times out on. These are inputs that
the veriﬁer can not prove to be robust or not robust in 120 seconds. Build times and solve times
are reported in seconds. Finally, average solve time includes timeouts. In other words, veriﬁcation
solves that time out contribute 120 seconds to the total solve time.
* The “Adversarial Training” network uses a 3600 instead of 120 second timeout and is only veriﬁed
for the ﬁrst 100 images because verifying it took too long.
** The “+RS (Large)” networks are only veriﬁed for the ﬁrst 1000 images because of long build
times.
*** Wong et al. (2018); Dvijotham et al. (2018), and Mirman et al. (2018), which we compare to in
Table 3, do not report results on MNIST, (cid:15) = 0.2 in their papers. We ran the publicly available code
of Wong et al. (2018) on MNIST, (cid:15) = 0.2 to generate these results for comparison.

19

Published as a conference paper at ICLR 2019

F DISCUSSION ON VERIFICATION AND CERTIFICATION

Exact veriﬁcation and certiﬁcation are two related approaches to formally verifying properties of
neural networks, such as adversarial robustness.
In both cases, the end goal is formal veriﬁca-
tion. Certiﬁcation methods, which solve an easier-to-solve relaxation of the exact veriﬁcation prob-
lem, are important developments because exact veriﬁcation previously appeared computationally
intractable for all but the smallest models.

For the case of adversarial robustness, certiﬁcation methods exploit a trade-off between provable
robustness and speed. They can fail to provide certiﬁcates of robustness for some inputs that are
actually robust, but they will either ﬁnd or fail to ﬁnd certiﬁcates of robustness quickly. On the other
hand, exact veriﬁers will always give the correct answer if given enough time, but exact veriﬁers can
sometimes take many hours to formally verify robustness on even a single input.

In general, the process of training a robust neural network and then formally verifying its robustness
happens in two steps.

• Step 1: Training
• Step 2: Veriﬁcation or Certiﬁcation

Most papers on certiﬁcation, including Wong and Kolter (2018); Wong et al. (2018); Dvijotham
et al. (2018); Raghunathan et al. (2018) and Mirman et al. (2018), propose a method for step 2
(the certiﬁcation step), and then propose a training objective in step 1 that is directly related to
their method for step 2. We call this paradigm “co-training.” In Raghunathan et al. (2018), they
found that using their step 2 on a model trained using Wong and Kolter (2018)’s step 1 resulted in
extremely poor provable robustness (less than 10%), and the same was true when using Wong and
Kolter (2018)’s step 2 on a model trained using their step 1.

We focus on MILP-based exact veriﬁcation as our step 2, which encompasses the best current exact
veriﬁcation methods. The advantage of using exact veriﬁcation for step 2 is that it will be accurate,
regardless of what method is used in step 1. The disadvantage of using exact veriﬁcation for step
2 is that it could be extremely slow. For our step 1, we used standard robust adversarial training.
In order to signiﬁcantly speed up exact veriﬁcation as step 2, we proposed techniques that could be
added to step 1 to induce weight sparsity and ReLU stability.

In general, we believe it is important to develop effective methods for step 1, given that step 2 is
exact veriﬁcation. However, ReLU stability can also be beneﬁcial for tightening the relaxation of
certiﬁcation approaches like that of Wong et al. (2018) and Dvijotham et al. (2018), as unstable
ReLUs are the primary cause of the overapproximation that occurs in the relaxation step. Thus, our
techniques for inducing ReLU stability can be useful for certiﬁcation as well.

Finally, in recent literature on veriﬁcation and certiﬁcation, most works have focused on formally
verifying the property of adversarial robustness of neural networks. However, veriﬁcation of other
properties could be useful, and our techniques to induce weight sparsity and ReLU stability would
still be useful for veriﬁcation of other properties for the exact same reasons that they are useful in
the case of adversarial robustness.

20

9
1
0
2
 
r
p
A
 
3
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
0
0
3
0
.
9
0
8
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2019

TRAINING FOR FASTER ADVERSARIAL ROBUSTNESS
VERIFICATION VIA INDUCING RELU STABILITY

Kai Y. Xiao Vincent Tjeng Nur Muhammad (Mahi) Shaﬁullah Aleksander M ˛adry
Massachusetts Institute of Technology
Cambridge, MA 02139
{kaix, vtjeng, nshafiul, madry}@mit.edu

ABSTRACT

We explore the concept of co-design in the context of neural network veriﬁcation.
Speciﬁcally, we aim to train deep neural networks that not only are robust to ad-
versarial perturbations but also whose robustness can be veriﬁed more easily. To
this end, we identify two properties of network models – weight sparsity and so-
called ReLU stability – that turn out to signiﬁcantly impact the complexity of the
corresponding veriﬁcation task. We demonstrate that improving weight sparsity
alone already enables us to turn computationally intractable veriﬁcation problems
into tractable ones. Then, improving ReLU stability leads to an additional 4–13x
speedup in veriﬁcation times. An important feature of our methodology is its “uni-
versality,” in the sense that it can be used with a broad range of training procedures
and veriﬁcation approaches.

1

INTRODUCTION

Deep neural networks (DNNs) have recently achieved widespread success in image classiﬁcation
(Krizhevsky et al., 2012), face and speech recognition (Taigman et al., 2014; Hinton et al., 2012),
and game playing (Silver et al., 2016; 2017). This success motivates their application in a broader
set of domains, including more safety-critical environments. This thrust makes understanding the
reliability and robustness of the underlying models, let alone their resilience to manipulation by
malicious actors, a central question. However, predictions made by machine learning models are
often brittle. A prominent example is the existence of adversarial examples (Szegedy et al., 2014):
imperceptibly modiﬁed inputs that cause state-of-the-art models to misclassify with high conﬁdence.

There has been a long line of work on both generating adversarial examples, called attacks (Carlini
and Wagner, 2017b;a; Athalye et al., 2018a;b; Uesato et al., 2018; Evtimov et al., 2017), and training
models robust to adversarial examples, called defenses (Goodfellow et al., 2015; Papernot et al.,
2016; Madry et al., 2018; Kannan et al., 2018). However, recent research has shown that most
defenses are ineffective (Carlini and Wagner, 2017a; Athalye et al., 2018a; Uesato et al., 2018).
Furthermore, even for defenses such as that of Madry et al. (2018) that have seen empirical success
against many attacks, we are unable to conclude yet with certainty that they are robust to all attacks
that we want these models to be resilient to.

This state of affairs gives rise to the need for veriﬁcation of networks, i.e., the task of formally
proving that no small perturbations of a given input can cause it to be misclassiﬁed by the network
model. Although many exact veriﬁers1 have been designed to solve this problem, the veriﬁcation
process is often intractably slow. For example, when using the Reluplex veriﬁer of Katz et al. (2017),
even verifying a small MNIST network turns out to be computationally infeasible. Thus, addressing
this intractability of exact veriﬁcation is the primary goal of this work.

Our Contributions
Our starting point is the observation that, typically, model training and veriﬁcation are decoupled
and seen as two distinct steps. Even though this separation is natural, it misses a key opportunity:
the ability to align these two stages. Speciﬁcally, applying the principle of co-design during model

1Also sometimes referred to as combinatorial veriﬁers.

1

Published as a conference paper at ICLR 2019

training is possible: training models in a way to encourage them to be simultaneously robust and
easy-to-exactly-verify. This insight is the cornerstone of the techniques we develop in this paper.

In this work, we use the principle of co-design to develop training techniques that give models
that are both robust and easy-to-verify. Our techniques rely on improving two key properties of
networks: weight sparsity and ReLU stability. Speciﬁcally, we ﬁrst show that natural methods for
improving weight sparsity during training, such as (cid:96)1-regularization, give models that can already be
veriﬁed much faster than current methods. This speedup happens because in general, exact veriﬁers
beneﬁt from having fewer variables in their formulations of the veriﬁcation task. For instance, for
exact veriﬁers that rely on linear programming (LP) solvers, sparser weight matrices means fewer
variables in those constraints.

We then focus on the major speed bottleneck of current approaches to exact veriﬁcation of ReLU
networks: the need of exact veriﬁcation methods to “branch,” i.e., consider two possible cases for
each ReLU (ReLU being active or inactive). Branching drastically increases the complexity of
veriﬁcation. Thus, well-optimized veriﬁers will not need to branch on a ReLU if it can determine
that the ReLU is stable, i.e.
that the ReLU will always be active or always be inactive for any
perturbation of an input. This motivates the key goal of the techniques presented in this paper: we
aim to minimize branching by maximizing the number of stable ReLUs. We call this goal ReLU
stability and introduce a regularization technique to induce it.

Our techniques enable us to train weight-sparse and ReLU stable networks for MNIST and CIFAR-
10 that can be veriﬁed signiﬁcantly faster. Speciﬁcally, by combining natural methods for inducing
weight sparsity with a robust adversarial training procedure (cf. Goodfellow et al. (2015)), we are
able to train networks for which almost 90% of inputs can be veriﬁed in an amount of time that
is small2 compared to previous veriﬁcation techniques. Then, by also adding our regularization
technique for inducing ReLU stability, we are able to train models that can be veriﬁed an addi-
tional 4–13x times as fast while maintaining state-of-the-art accuracy on MNIST. Our techniques
show similar improvements for exact veriﬁcation of CIFAR models. In particular, we achieve the
following veriﬁcation speed and provable robustness results for (cid:96)∞ norm-bound adversaries:

Dataset

Epsilon

Provable Adversarial Accuracy Average Solve Time (s)

MNIST

(cid:15) = 0.1
(cid:15) = 0.2
(cid:15) = 0.3

CIFAR

(cid:15) = 2/255
(cid:15) = 8/255

94.33%
89.79%
80.68%

45.93%
20.27%

0.49
1.13
2.78

13.50
22.33

Our network for (cid:15) = 0.1 on MNIST achieves provable adversarial accuracies comparable with the
current best results of Wong et al. (2018) and Dvijotham et al. (2018), and our results for (cid:15) = 0.2 and
(cid:15) = 0.3 achieve the best provable adversarial accuracies yet. To the best of our knowledge, we also
achieve the ﬁrst nontrivial provable adversarial accuracy results using exact veriﬁers for CIFAR-10.

Finally, we design our training techniques with universality as a goal. We focus on improving the
input to the veriﬁcation process, regardless of the veriﬁer we end up using. This is particularly
important because research into network veriﬁcation methods is still in its early stages, and our co-
design methods are compatible with the best current veriﬁers (LP/MILP-based methods) and should
be compatible with any future improvements in veriﬁcation.

Our code is available at https://github.com/MadryLab/relu_stable.

2 BACKGROUND AND RELATED WORK

Exact veriﬁcation of networks has been the subject of many recent works (Katz et al., 2017; Ehlers,
2017; Carlini et al., 2017; Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). To
understand the context of these works, observe that for linear networks, the task of exact veriﬁcation
is relatively simple and can be done by solving a LP. For more complex models, the presence of
nonlinear ReLUs makes veriﬁcation over all perturbations of an input much more challenging. This

2We chose our time budget for veriﬁcation to be 120 seconds per input image.

2

Published as a conference paper at ICLR 2019

is so as ReLUs can be active or inactive depending on the input, which can cause exact veriﬁers
to “branch" and consider these two cases separately. The number of such cases that veriﬁers have
to consider might grow exponentially with the number of ReLUs, so veriﬁcation speed will also
grow exponentially in the worst case. Katz et al. (2017) further illustrated the difﬁculty of exact
veriﬁcation by proving that it is NP-complete. In recent years, formal veriﬁcation methods were
developed to verify networks. Most of these methods use satisﬁability modulo theory (SMT) solvers
(Katz et al., 2017; Ehlers, 2017; Carlini et al., 2017) or LP and Mixed-Integer Linear Programming
(MILP) solvers (Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). However,
all of them are limited by the same issue of scaling poorly with the number of ReLUs in a network,
making them prohibitively slow in practice for even medium-sized models.

One recent approach for dealing with the inefﬁciency of exact veriﬁers is to focus on certiﬁcation
methods3 (Wong and Kolter, 2018; Wong et al., 2018; Dvijotham et al., 2018; Raghunathan et al.,
2018; Mirman et al., 2018; Sinha et al., 2018). In contrast to exact veriﬁcation, these methods do not
solve the veriﬁcation task directly; instead, they rely on solving a relaxation of the veriﬁcation prob-
lem. This relaxation is usually derived by overapproximating the adversarial polytope, or the space
of outputs of a network for a region of possible inputs. These approaches rely on training models in
a speciﬁc manner that makes certiﬁcation of those models easier. As a result, they can often obtain
provable adversarial accuracy results faster. However, certiﬁcation is fundamentally different from
veriﬁcation in two primary ways. First, it solves a relaxation of the original veriﬁcation problem. As
a result, certiﬁcation methods can fail to certify many inputs that are actually robust to perturbations
– only exact veriﬁers, given enough time, can give conclusive answers on robustness for every single
input. Second, certiﬁcation approaches fall under the paradigm of co-training, where a certiﬁcation
method only works well on models speciﬁcally trained for that certiﬁcation step. When used as a
black box on arbitrary models, the certiﬁcation step can yield a high rate of false negatives. For
example, Raghunathan et al. (2018) found that their certiﬁcation step was signiﬁcantly less effective
when used on a model trained using Wong and Kolter (2018)’s training method, and vice versa. In
contrast, we design our methods to be universal. They can be combined with any standard train-
ing procedure for networks and will improve exact veriﬁcation speed for any LP/MILP-based exact
veriﬁer. Our methods can also decrease the amount of overapproximation incurred by certiﬁcation
methods like Wong and Kolter (2018); Dvijotham et al. (2018). Similar to most of the certiﬁcation
methods, our technique can be made to have very little training time overhead.

Finally, subsequent work of Gowal et al. (2018) shows how applying interval bound propagation
during training, combined with MILP-based exact veriﬁcation, can lead to provably robust networks.

3 TRAINING VERIFIABLE NETWORK MODELS

We begin by discussing the task of verifying a network and identify two key properties of networks
that lead to improved veriﬁcation speed: weight sparsity and so-called ReLU stability. We then use
natural regularization methods for inducing weight sparsity as well as a new regularization method
for inducing ReLU stability. Finally, we demonstrate that these methods signiﬁcantly speed up
veriﬁcation while maintaining state-of-the-art accuracy.

3.1 VERIFYING ADVERSARIAL ROBUSTNESS OF NETWORK MODELS

Deep neural network models. Our focus will be on one of the most common architectures for
state-of-the-art models: k-layer fully-connected feed-forward DNN classiﬁers with ReLU non-
linearities4. Such models can be viewed as a function f (·, W, b), where W and b represent the
weight matrices and biases of each layer. For an input x, the output f (x, W, b) of the DNN is

3These works use both “veriﬁcation” and “certiﬁcation” to describe their methods. For clarity, we use “cer-
tiﬁcation” to describe their approaches, while we use “veriﬁcation” to describe exact veriﬁcation approaches.
For a more detailed discussion of the differences, see Appendix F.

4Note that this encompasses common convolutional network architectures because every convolutional layer

can be replaced by an equivalent fully-connected layer.

3

Published as a conference paper at ICLR 2019

deﬁned as:

z0 = x
ˆzi = zi−1Wi + bi
zi = max( ˆzi, 0)

(1)
(2)
(3)
(4)
Here, for each layer i, we let ˆzij denote the jth ReLU pre-activation and let ˆzij(x) denote the value
of ˆzij on an input x. ˆzk−1 is the ﬁnal, output layer with an output unit for each possible label (the
logits). The network will make predictions by selecting the label with the largest logit.

for i = 1, 2, . . . , k − 1
for i = 1, 2, . . . , k − 2

f (x, W, b) = ˆzk−1

Adversarial robustness. For a network to be reliable, it should make predictions that are robust
– that is, it should predict the same output for inputs that are very similar. Speciﬁcally, we want
the DNN classiﬁer’s predictions to be robust to a set Adv(x) of possible adversarial perturbations
of an input x. We focus on (cid:96)∞ norm-bound adversarial perturbations, where Adv(x) = {x(cid:48)
:
||x(cid:48) − x||∞ ≤ (cid:15)} for some constant (cid:15), since it is the most common one considered in adversarial
robustness and veriﬁcation literature (thus, it currently constitutes a canonical benchmark). Even so,
our methods can be applied to other (cid:96)p norms and broader sets of perturbations.
Veriﬁcation of network models. For an input x with correct label y, a perturbed input x(cid:48) can cause
a misclassiﬁcation if it makes the logit of some incorrect label ˆy larger than the logit of y on x(cid:48). We
can thus express the task of ﬁnding an adversarial perturbation as the optimization problem:

f (x(cid:48), W )y − f (x(cid:48), W )ˆy

min
x(cid:48),ˆy
subject to x(cid:48) ∈ Adv(x)

An adversarial perturbation exists if and only if the objective of the optimization problem is negative.

Adversarial accuracies. We deﬁne the true adversarial accuracy of a model to be the fraction of
test set inputs for which the model is robust to all allowed perturbations. By deﬁnition, evaluations
against speciﬁc adversarial attacks like PGD or FGSM provide an upper bound to this accuracy,
while certiﬁcation methods provide lower bounds. Given sufﬁcient time for each input, an exact
veriﬁer can prove robustness to perturbations, or ﬁnd a perturbation where the network makes a
misclassiﬁcation on the input, and thus exactly determine the true adversarial accuracy. This is in
contrast to certiﬁcation methods, which solve a relaxation of the veriﬁcation problem and can not
exactly determine the true adversarial accuracy no matter how much time they have.

However, such exact veriﬁcation may take impractically long for certain inputs, so we instead com-
pute the provable adversarial accuracy, which we deﬁne as the fraction of test set inputs for which
the veriﬁer can prove robustness to perturbations within an allocated time budget (timeout). Simi-
larly to certiﬁable accuracy, this accuracy constitutes a lower bound on the true adversarial accuracy.
A model can thus, e.g., have high true adversarial accuracy and low provable adversarial accuracy if
veriﬁcation of the model is too slow and often fails to complete before timeout.

Also, in our evaluations, we chose to use the MILP exact veriﬁer of Tjeng et al. (2019) when per-
forming experiments, as it is both open source and the fastest veriﬁer we are aware of.

3.2 WEIGHT SPARSITY AND ITS IMPACT ON VERIFICATION SPEED

The ﬁrst property of network models that we want to improve in order to speed up exact veriﬁcation
of those models is weight sparsity. Weight sparsity is important for veriﬁcation speed because many
exact veriﬁers rely on solving LP or MILP systems, which beneﬁt from having fewer variables.
We use two natural regularization methods for improving weight sparsity: (cid:96)1-regularization and
small weight pruning. These techniques signiﬁcantly improve veriﬁcation speed – see Table 1.
Verifying even small MNIST networks is almost completely intractable without them. Speciﬁcally,
the veriﬁer can only prove robustness of an adversarially-trained model on 19% of inputs with a one
hour budget per input, while the veriﬁer can prove robustness of an adversarially-trained model with
(cid:96)1-regularization and small weight pruning on 89.13% of inputs with a 120 second budget per input.

Interestingly, even though adversarial training improves weight sparsity (see Appendix B) it was
still necessary to use (cid:96)1-regularization and small weight pruning. These techniques further promoted
weight sparsity and gave rise to networks that were much easier to verify.

4

Published as a conference paper at ICLR 2019

Dataset

Epsilon

Training
Method

Test Set
Accuracy

Provable Adversarial
Accuracy

Average
Solve Time (s)

MNIST (cid:15) = 0.1

1 Adversarial Training
+(cid:96)1-Regularization
2
+Small Weight Pruning
3
4

+ReLU Pruning (control)

99.17%
99.00%
98.99%
98.94%

19.00%
82.17%
89.13%
91.58%

2970.43
21.99
11.71
6.43

Table 1: Improvement in provable adversarial accuracy and veriﬁcation solve times when incremen-
tally adding natural regularization methods for improving weight sparsity and ReLU stability into
the model training procedure, before veriﬁcation occurs. Each row represents the addition of another
method – for example, Row 3 uses adversarial training, (cid:96)1-regularization, and small weight pruning.
Row 4 adds ReLU pruning (see Appendix A). Row 4 is the control model for MNIST and (cid:15) = 0.1
that we present again in comparisons in Tables 2 and 3. We use a 3600 instead of 120 second timeout
for Row 1 and only veriﬁed the ﬁrst 100 images (out of 10000) because verifying it took too long.

3.3 RELU STABILITY

Next, we target the primary speed bottleneck of exact veriﬁcation: the number of ReLUs the veriﬁer
has to branch on. In our paper, this corresponds to the notion of inducing ReLU stability. Before we
describe our methodology, we formally deﬁne ReLU stability.

(2)).

Given an input x, a set of allowed perturbations Adv(x), and a ReLU, exact veriﬁers may need
to branch based on the possible pre-activations of the ReLU, namely ˆzij(Adv(x)) = {ˆzij(x(cid:48)) :
x(cid:48) ∈ Adv(x)} (cf.
If there exist two perturbations x(cid:48), x(cid:48)(cid:48) in the set Adv(x) such that
sign(ˆzij(x(cid:48))) (cid:54)= sign(ˆzij(x(cid:48)(cid:48))), then the veriﬁer has to consider that for some perturbed inputs the
ReLU is active (zij = ˆzij) and for other perturbed inputs inactive (zij = 0). The more such cases
the veriﬁer faces, the more branches it has to consider, causing the complexity of veriﬁcation to in-
crease exponentially. Intuitively, a model with 1000 ReLUs among which only 100 ReLUs require
branching will likely be much easier to verify than a model with 200 ReLUs that all require branch-
ing. Thus, it is advantageous for the veriﬁer if, on an input x with allowed perturbation set Adv(x),
the number of ReLUs such that

sign(ˆzij(x(cid:48))) = sign(ˆzij(x)) ∀x(cid:48) ∈ Adv(x)
is maximized. We call a ReLU for which (5) holds on an input x a stable ReLU on that input. If (5)
does not hold, then the ReLU is an unstable ReLU.

(5)

Directly computing whether a ReLU is stable on a given input x is difﬁcult because doing so would
involve considering all possible values of ˆzij(Adv(x)). Instead, exact veriﬁers compute an upper
bound ˆuij and a lower bound ˆlij of ˆzij(Adv(x)). If 0 ≤ ˆlij or ˆuij ≤ 0, they can replace the ReLU
with the identity function or the zero function, respectively. Otherwise, if ˆlij < 0 < ˆuij, these
veriﬁers then determine that they need to “branch” on that ReLU. Thus, we can rephrase (5) as

sign(ˆuij) = sign(ˆlij)

(6)

We will discuss methods for determining these upper and lower bounds ˆuij, ˆlij in Section 3.3.2.

3.3.1 A REGULARIZATION TECHNIQUE FOR INDUCING RELU STABILITY: RS LOSS

As we see from equation (6), a function that would indicate exactly when a ReLU is stable is
F ∗(ˆuij, ˆlij) = sign(ˆuij) · sign(ˆlij). Thus, it would be natural to use this function as a regular-
izer. However, this function is non-differentiable and if used in training a model, would provide no
useful gradients during back-propagation. Thus, we use the following smooth approximation of F ∗
(see Fig. 1) which provides the desired gradients:

F (ˆuij, ˆlij) = − tanh(1 + ˆuij · ˆlij)

We call the corresponding objective RS Loss, and show in Fig. 2a that using this loss function as
a regularizer effectively decreases the number of unstable ReLUs. RS Loss thus encourages ReLU
stability, which, in turn, speeds up exact veriﬁcation - see Fig. 2b.

5

Published as a conference paper at ICLR 2019

Figure 1: Plot and contour plot of the function F (x, y) = − tanh(1 + x · y)

3.3.2 ESTIMATING RELU UPPER AND LOWER BOUNDS ON ACTIVATIONS

A key aspect of using RS Loss is determining the upper and lower bounds ˆuij, ˆlij for each ReLU (cf.
(6)). The bounds for the inputs z0 (cf. (1)) are simple – for the input x, we know x − (cid:15) ≤ z0 ≤ x + (cid:15),
so ˆu0 = x − (cid:15), ˆl0 = x + (cid:15). For all subsequent zij, we estimate bounds using either the naive interval
arithmetic (IA) approach described in Tjeng et al. (2019) or an improved version of it. The improved
version is a tighter estimate but uses more memory and training time, and thus is most effective on
smaller networks. We present the details of naive IA and improved IA in Appendix C.

Interval arithmetic approaches can be implemented relatively efﬁciently and work well with back-
propagation because they only involve matrix multiplications. This contrasts with how exact veri-
ﬁers compute these bounds, which usually involves solving LPs or MILPs. Interval arithmetic also
overestimates the number of unstable ReLUs. This means that minimizing unstable ReLUs based
on IA bounds will provide an upper bound on the number of unstable ReLUs determined by exact
veriﬁers. In particular, IA will properly penalize every unstable ReLU.

Improved IA performs well in practice, overestimating the number of unstable ReLUs by less than
0.4% in the ﬁrst 2 layers of MNIST models and by less than 36.8% (compared to 128.5% for naive
IA) in the 3rd layer. Full experimental results are available in Table 4 of Appendix C.3.

3.3.3

IMPACT OF RELU STABILITY IMPROVEMENTS ON VERIFICATION SPEED

We provide experimental evidence that RS Loss regularization improves ReLU stability and speeds
up average veriﬁcation times by more than an order of magnitude in Fig. 2b. To isolate the effect of
RS Loss, we compare MNIST models trained in exactly the same way other than the weight on RS
Loss. When comparing a network trained with a RS Loss weight of 5e−4 to a network with a RS
Loss weight of 0, the former has just 16% as many unstable ReLUs and can be veriﬁed 65x faster.
The caveat here is that the former has 1.00% lower test set accuracy.

We also compare veriﬁcation speed with and without RS Loss on MNIST networks for different
values of (cid:15) (0.1, 0.2, and 0.3) in Fig. 2c. We choose RS Loss weights that cause almost no test set
accuracy loss (less than 0.50% - See Table 3) in these cases, and we still observe a 4–13x speedup
from RS Loss. For CIFAR, RS Loss gives a smaller speedup of 1.6–3.7x (See Appendix E).

3.3.4

IMPACT OF RELU STABILITY IMPROVEMENTS ON PROVABLE ADVERSARIAL
ACCURACY

As the weight on the RS Loss used in training a model increases, the ReLU stability of the model
will improve, speeding up veriﬁcation and likely improving provable adversarial accuracy. However,
like most regularization methods, placing too much weight on RS Loss can decrease the model ca-
pacity, potentially lowering both the true adversarial accuracy and the provable adversarial accuracy.
Therefore, it is important to choose the weight on RS Loss carefully to obtain both high provable
adversarial accuracy and faster veriﬁcation speeds.

To show the effectiveness of RS Loss in improving provable adversarial accuracy, we train two
networks for each dataset and each value of (cid:15). One is a “control” network that uses all of the natural
improvements for inducing both weight sparsity ((cid:96)1-regularization and small weight pruning) and
ReLU stability (ReLU pruning - see Appendix A). The second is a “+RS” network that uses RS

6

Published as a conference paper at ICLR 2019

(a)

(b)

(c)

Figure 2: (a) Average number of unstable ReLUs by layer and (b) average veriﬁcation solve times
of 6 networks trained with different weights on RS Loss for MNIST and (cid:15) = 0.1 . Averages are
taken over all 10000 MNIST test set inputs. Both metrics improve signiﬁcantly with increasing
RS Loss weight. An RS Loss weight of 0 corresponds to the control network, while an RS Loss
weight of 0.00012 corresponds to the “+RS” network for MNIST and (cid:15) = 0.1 in Tables 2 and 3. (c)
Improvement in the average time taken by a veriﬁer to solve the veriﬁcation problem after adding
RS Loss to the training procedure, for different (cid:15) on MNIST. The weight on RS Loss was chosen so
that the “+RS” models have test set accuracies within 0.50% of the control models.

Loss in addition to all of the same natural improvements. This lets us isolate the incremental effect
of adding RS Loss to the training procedure.

In addition to attaining a 4–13x speedup in MNIST veriﬁcation times (see Fig. 2c), we achieve
higher provable adversarial accuracy in every single setting when using RS Loss. This is especially
notable for the hardest veriﬁcation problem we tackle – proving robustness to perturbations with (cid:96)∞
norm-bound 8/255 on CIFAR-10 – where adding RS Loss nearly triples the provable adversarial
accuracy from 7.09% to 20.27%. This improvement is primarily due to veriﬁcation speedup induced
by RS Loss, which allows the veriﬁer to ﬁnish proving robustness for far more inputs within the 120
second time limit. These results are shown in Table 2.

Table 2: Provable Adversarial Accuracies for the control and “+RS” networks in each setting.

MNIST, (cid:15) = 0.1 MNIST, (cid:15) = 0.2 MNIST, (cid:15) = 0.3 CIFAR, (cid:15) = 2/255 CIFAR, (cid:15) = 8/255

Control
+RS

91.58
94.33

86.45
89.79

77.99
80.68

45.53
45.93

7.09
20.27

4 EXPERIMENTS

4.1 EXPERIMENTS ON MNIST AND CIFAR

In addition to the experimental results already presented, we compare our control and “+RS” net-
works with the best available results presented in the state-of-the-art certiﬁable defenses of Wong
et al. (2018), Dvijotham et al. (2018), and Mirman et al. (2018) in Table 3. We compare their test
set accuracy, PGD adversarial accuracy (an evaluation of robustness against a strong 40-step PGD
adversarial attack), and provable adversarial accuracy. Additionally, to show that our method can
scale to larger architectures, we train and verify a “+RS (Large)” network for each dataset and (cid:15).

In terms of provable adversarial accuracies, on MNIST, our results are signiﬁcantly better than those
of Wong et al. (2018) for larger perturbations of (cid:15) = 0.3, and comparable for (cid:15) = 0.1. On CIFAR-
10, our method is slightly less effective, perhaps indicating that more unstable ReLUs are necessary
to properly learn a robust CIFAR classiﬁer. We also experienced many more instances of the veriﬁer
reaching its allotted 120 second time limit on CIFAR, especially for the less ReLU stable control
networks. Full experimental details for each model in Tables 1, 2, and 3, including a breakdown of
veriﬁcation solve results (how many images did the veriﬁer: A. prove robust B. ﬁnd an adversarial
example for C. time out on), are available in Appendix E.

7

Published as a conference paper at ICLR 2019

Table 3: Comparison of test set, PGD adversarial, and provable adversarial accuracy of networks
trained with and without RS Loss. We also provide the best available certiﬁable adversarial and
PGD adversarial accuracy of any single models from Wong et al. (2018), Dvijotham et al. (2018),
and Mirman et al. (2018) for comparison, and highlight the best provable accuracy for each (cid:15).
* The provable adversarial accuracy for “+RS (Large)” is only computed for the ﬁrst 1000 images
because the veriﬁer performs more slowly on larger models.
** Dvijotham et al. (2018); Mirman et al. (2018) use a slightly smaller (cid:15) = 0.03 = 7.65/255.
† Mirman et al. (2018) computes results over 500 images instead of all 10000.
†† Mirman et al. (2018) uses a slightly smaller (cid:15) = 0.007 = 1.785/255.

Dataset

Epsilon

Test Set
Accuracy

PGD Adversarial

Provable/Certiﬁable
Accuracy Adversarial Accuracy

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

Training
Method

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†,††

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.**
Mirman et al.†, **

98.94%
98.68%
98.95%

98.92%
98.80%
99.00%

98.40%
98.10%
98.21%

97.75%
97.33%
97.54%

85.13%
96.60%

64.64%
61.12%
61.41%

68.28%
62.00%

50.69%
40.45%
42.81%

28.67%
48.64%
54.20%

95.12%
95.13%
96.58%

-
97.13%
97.60%

93.14%
93.14%
94.19%

91.64%
92.05%
93.25%

-
93.80%

51.58%
49.92%
50.61%

-
54.60%

31.28%
26.78%
28.69%

-
32.72%
40.00%

91.58%
94.33%
95.60%

96.33%
95.56%
96.60%

86.45%
89.79%
89.10%

77.99%
80.68%
59.60%

56.90%
82.00%

45.53%
45.93%
41.40%

53.89%
52.20%

7.09%
20.27%
19.80%

21.78%
26.67%
35.20%

4.2 EXPERIMENTAL METHODS AND DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. For each setting of dataset
(MNIST or CIFAR) and (cid:15), we ﬁnd a suitable weight on RS Loss via line search (See Table 6 in
Appendix D). The same weight is used for each ReLU. During training, we used improved IA for
ReLU bound estimation for “+RS” models and use naive IA for “+RS (Large)” models because of
memory constraints. For ease of comparison, we trained our networks using the same convolutional
DNN architecture as in Wong et al. (2018). This architecture uses two 2x2 strided convolutions with
16 and 32 ﬁlters, followed by a 100 hidden unit fully connected layer. For the larger architecture,
we also use the same “large” architecture as in Wong et al. (2018). It has 4 convolutional layers with
32, 32, 64, and 64 ﬁlters, followed by 2 fully connected layers with 512 hidden units each.

For veriﬁcation, we used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019).
Model solves were parallelized over 8 CPU cores. When verifying an image, the veriﬁer of Tjeng
et al. (2019) ﬁrst builds a model, and second, solves the veriﬁcation problem (See Appendix D.2 for
details). We focus on reporting solve times because that is directly related to the task of veriﬁcation
itself. All build times for the control and “+RS” models on MNIST that we presented were between
4 and 10 seconds, and full results on build times are also presented in Appendix E.

Additional details on our experimental setup (e.g. hyperparameters) can be found in Appendix D.

8

Published as a conference paper at ICLR 2019

5 CONCLUSION

In this paper, we use the principle of co-design to develop training methods that emphasize veri-
ﬁcation as a goal, and we show that they make verifying the trained model much faster. We ﬁrst
demonstrate that natural regularization methods already make the exact veriﬁcation problem sig-
niﬁcantly more tractable. Subsequently, we introduce the notion of ReLU stability for networks,
present a method that improves a network’s ReLU stability, and show that this improvement makes
veriﬁcation an additional 4–13x faster. Our method is universal, as it can be added to any training
procedure and should speed up any exact veriﬁcation procedure, especially MILP-based methods.

Prior to our work, exact veriﬁcation seemed intractable for all but the smallest models. Thus, our
work shows progress toward reliable models that can be proven to be robust, and our techniques can
help scale veriﬁcation to even larger networks.

Many of our methods appear to compress our networks into more compact, simpler forms. We
hypothesize that the reason that regularization methods like RS Loss can still achieve very high
accuracy is that most models are overparametrized in the ﬁrst place. There exist clear parallels
between our methods and techniques in model compression (Han et al., 2016; Cheng et al., 2017b) –
therefore, we believe that drawing upon additional techniques from model compression can further
improve the ease-of-veriﬁcation of networks. We also expect that there exist objectives other than
weight sparsity and ReLU stability that are important for veriﬁcation speed. If so, further exploring
the principle of co-design for those objectives is an interesting future direction.

ACKNOWLEDGEMENTS

This work was supported by the NSF Graduate Research Fellowship under Grant No. 1122374,
by the NSF grants CCF-1553428 and CNS-1815221, and by Lockheed Martin Corporation under
award number RPP2016-002. We would like to thank Krishnamurthy Dvijotham, Ludwig Schmidt,
Michael Sun, Dimitris Tsipras, and Jonathan Uesato for helpful discussions.

9

Published as a conference paper at ICLR 2019

REFERENCES

Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense
In International Conference on

of security: Circumventing defenses to adversarial examples.
Machine Learning (ICML), 2018a.

Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. In International Conference on Machine Learning (ICML), pages 284–293, 2018b.

Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
In Proceedings of ACM Workshop on Artiﬁcial Intelligence and Security

detection methods.
(AISec), 2017a.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks.

In

Security and Privacy (SP), 2017 IEEE Symposium on, 2017b.

Nicholas Carlini, Guy Katz, Clark Barrett, and David L. Dill. Ground-truth adversarial examples.

2017.

networks. 2017a.

Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artiﬁcial neural

Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration

for deep neural networks. 2017b.

Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training veriﬁed learners with learned ver-
iﬁers. 2018.

Rüdiger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In Auto-

mated Technology for Veriﬁcation and Analysis, 2017.

Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir
Rahmati, and Dawn Song. Robust physical-world attacks on machine learning models. 2017.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. In International Conference on Learning Representations (ICLR), 2015.

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. 2018.

Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. In International Conference on Learning
Representations (ICLR), 2016.

Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, An-
drew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep
neural networks for acoustic modeling in speech recognition: The shared views of four research
groups. In IEEE Signal Processing Magazine, 2012.

Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. 2018.

Guy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer. Reluplex: An efﬁcient

smt solver for verifying deep neural networks. 2017.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations (ICLR), 2015.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012.

Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward relu

neural networks. 2017.

10

Published as a conference paper at ICLR 2019

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations (ICLR), 2018.

Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning (ICML), 2018.

Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation
as a defense to adversarial perturbations against deep neural networks. In Proceedings of IEEE
Symposium on Security and Privacy (SP), 2016.

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial exam-

ples. In International Conference on Learning Representations (ICLR), 2018.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 2017.

Aman Sinha, Hongseok Namkoong, and John Duchi. Certiﬁable distributional robustness with
principled adversarial training. In International Conference on Learning Representations (ICLR),
2018.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations (ICLR), 2014.

Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap
In Computer Vision and Pattern Recognition

to human-level performance in face veriﬁcation.
(CVPR), 2014.

Robert Tibshirani. Regression shrinkage and selection via the lasso. In Journal of the Royal Statis-

tical Society, Series B, 1994.

Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In International Conference on Learning Representations (ICLR), 2019.

Jonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial
risk and the dangers of evaluating against weak attacks. In International Conference on Machine
Learning (ICML), 2018.

Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer

adversarial polytope. In International Conference on Machine Learning (ICML), 2018.

Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and Zico Kolter. Scaling provable adversarial

defenses. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

11

Published as a conference paper at ICLR 2019

APPENDIX

A NATURAL IMPROVEMENTS

A.1 NATURAL REGULARIZATION FOR INDUCING WEIGHT SPARSITY

All of the control and “+RS” networks in our paper contain natural improvements that improve
weight sparsity, which reduce the number of variables in the LPs solved by the veriﬁer. We ob-
served that the two techniques we used for weight sparsity ((cid:96)1-regularization and small weight prun-
ing) don’t hurt test set accuracy but they dramatically improve provable adversarial accuracy and
veriﬁcation speed.

1. (cid:96)1-regularization: We use a weight of 2e−5 on MNIST and a weight of 1e−5 on CIFAR.
We chose these weights via line search by ﬁnding the highest weight that would not hurt
test set accuracy.

2. Small weight pruning: Zeroing out weights in a network that are very close to zero. We

choose to prune weights less than 1e−3.

A.2 A BASIC IMPROVEMENT FOR INDUCING RELU STABILITY: RELU PRUNING

We also use a basic idea to improve ReLU stability, which we call ReLU pruning. The main idea is
to prune away ReLUs that are not necessary.

We use a heuristic to test whether a ReLU in a network is necessary. Our heuristic is to count how
many training inputs cause the ReLU to be active or inactive. If a ReLU is active (the pre-activation
satisﬁes ˆzij(x) > 0) for every input image in the training set, then we can replace that ReLU with
the identity function and the network would behave in exactly the same way for all of those images.
Similarly, if a ReLU is inactive (ˆzij(x) < 0) for every training image, that ReLU can be replaced by
the zero function.

Extending this idea further, we expect that ReLUs that are rarely used can also be removed without
signiﬁcantly changing the behavior of the network. If only a small fraction (say, 10%) of the input
images activate a ReLU, then replacing the ReLU with the zero function will only slightly change the
network’s behavior and will not affect the accuracy too much. We provide experimental evidence of
this phenomenon on an adversarially trained ((cid:15) = 0.1) MNIST model. Conservatively, we decided
that pruning away ReLUs that are active on less than 10% of the training set or inactive on less than
10% of the training set was reasonable.

Figure 3: Removing some ReLUs does not hurt test set accuracy or accuracy against a PGD adver-
sary

12

Published as a conference paper at ICLR 2019

B ADVERSARIAL TRAINING AND WEIGHT SPARSITY

It is worth noticing that adversarial training against (cid:96)∞ norm-bound adversaries alone already makes
networks easier to verify by implicitly improving weight sparsity. Indeed, this can be shown clearly
in the case of linear networks. Recall that a linear network can be expressed as f (x) = W x + b.
Thus, an (cid:96)∞ norm-bound perturbation of the input x will produce the output

f (x(cid:48)) = x(cid:48)W + b

= xW + b + (x(cid:48) − x)W
≤ f (x) + (cid:15)||W ||1

where the last inequality is just Hölder’s inequality.
In order to limit the adversary’s ability to
perturb the output, adversarial training needs to minimize the ||W ||1 term, which is equivalent to (cid:96)1-
regularization and is known to promote weight sparsity (Tibshirani, 1994). Relatedly, Goodfellow
et al. (2015) already pointed out that adversarial attacks against linear networks will be stronger
when the (cid:96)1-norm of the weight matrices is higher.

Even in the case of nonlinear networks, adversarial training has experimentally been shown to im-
prove weight sparsity. For example, models trained according to Madry et al. (2018) and Wong and
Kolter (2018) often learn many weight-sparse layers, and we observed similar trends in the mod-
els we trained. However, it is important to note that while adversarial training alone does improve
weight sparsity, it is not sufﬁcient by itself for efﬁcient exact veriﬁcation. Additional regularization
like (cid:96)1-regularization and small weight pruning further promotes weight sparsity and gives rise to
networks that are much easier to verify.

13

Published as a conference paper at ICLR 2019

C INTERVAL ARITHMETIC

C.1 NAIVE INTERVAL ARITHMETIC

Naive IA determines upper and lower bounds for a layer based solely on the upper and lower bounds
of the previous layer.
Deﬁne W + = max(W, 0), W − = min(W, 0), u = max(ˆu, 0), and l = max(ˆl, 0). Then the bounds
on the pre-activations of layer i can be computed as follows:
i + li−1W −
i + ui−1W −

ˆui = ui−1W +
ˆli = li−1W +

i + bi
i + bi

(7)

(8)

As noted in Tjeng et al. (2019) and also Dvijotham et al. (2018), this method is efﬁcient but can lead
to relatively conservative bounds for deeper networks.

C.2

IMPROVED INTERVAL ARITHMETIC

We improve upon naive IA by exploiting ReLUs that we can determine to always be active. This
allows us to cancel symbols that are equivalent that come from earlier layers of a network.

We will use a basic example of a neural network with one hidden layer to illustrate this idea. Suppose
that we have the scalar input z0 with l0 = 0, u0 = 1, and the network has the following weights and
biases:

W1 = [1 − 1] ,

b1 = [2

2] , W2 =

b2 = 0

(cid:21)
(cid:20)1
1

,

Naive IA for the ﬁrst layer gives ˆl1 = l1 = [2
2], and applying naive IA to
the output ˆz2 gives ˆl2 = 3, ˆu2 = 5. However, because ˆl1 > 0, we know that the two ReLUs in the
hidden layer are always active and thus equivalent to the identity function. Then, the output is

1], ˆu1 = u1 = [3

ˆz2 = z11 + z12 = ˆz11 + ˆz12 = (z0 + 2) + (−z0 + 2) = 4

Thus, we can obtain the tighter bounds ˆl2 = ˆu2 = 4, as we are able to cancel out the z0 terms.
We can write this improved version of IA as follows. First, letting Wk denote row k of matrix W ,
we can deﬁne the “active” part of W as the matrix WA, where

(WA)k =

(cid:40)

Wk
0

if ˆli−1 > 0
if ˆli−1 ≤ 0

Deﬁne the “non-active” part of W as

WN = W − WA
Then, using the same deﬁnitions for the notation W +, W −, u, l as before, we can write down the
following improved version of IA which uses information from the previous 2 layers.

ˆui = ui−1Wi

+
N + li−1Wi

−
N + bi
+ ui−2(Wi−1WiA)+ + li−2(Wi−1WiA)− + bi−1WiA
−
N + bi
+ li−2(Wi−1WiA)+ + ui−2(Wi−1WiA)− + bi−1WiA
We are forced to to use li−1,j and ui−1,j if we can not determine whether or not the ReLU corre-
sponding to the activation zi−1,j is active, but we use li−2 and ui−2 whenever possible.

+
N + ui−1Wi

ˆli = li−1Wi

We now deﬁne some additional notation to help us extend this method to any number of layers. We
now seek to deﬁne fn, which is a function which takes in four sequences of length n – upper bounds,
lower bounds, weights, and biases – and outputs the current layer’s upper and lower bounds.

What we have derived so far from (7) and (8) is the following

f1(ui−1, li−1, Wi, bi) = (ui−1W +

i + li−1W −

i + bi, li−1W +

i + ui−1W −

i + bi)

14

Published as a conference paper at ICLR 2019

Let u denote a sequence of upper bounds. Let uz denote element z of the sequence, and let u[z:]
denote the sequence without the ﬁrst z elements. Deﬁne notation for l, W, and b similarly.

Then, using the fact that WN Z = (W Z)N and WAZ = (W Z)A, we can show that the following
recurrence holds:

fn+1(u, l, W, b) = f1(u1, l1, W1N , b1)

+ fn(u[1:], l[1:], (W2W1A, W[2:]), (b2W1A, b[2:]))

(9)

Let u(x,y) denote the sequence (ux, ux−1, · · · , uy), and deﬁne l(x,y), W(x,y), and b(x,y) similarly.
Then, if we want to compute the bounds on layer k using all information from the previous k layers,
we simply have to compute fk(u(k−1,0), l(k−1,0), W(k,1), b(k,1)).

From the recurrence 9, we see that using information from all previous layers to compute bounds for
layer k takes O(k) matrix-matrix multiplications. Thus, using information from all previous layers
to compute bounds for all layers of a d layer neural network only involves O(d2) additional matrix
multiplications, which is still reasonable for most DNNs. This method is still relatively efﬁcient
because it only involves matrix multiplications – however, needing to perform matrix-matrix multi-
plications as opposed to just matrix-vector multiplications results in a slowdown and higher memory
usage when compared to naive IA. We believe the improvement in the estimate of ReLU upper and
lower bounds is worth the time trade-off for most networks.

C.3 EXPERIMENTAL RESULTS ON IMPROVED IA AND NAIVE IA

In Table 4, we show empirical evidence that the number of unstable ReLUs in each layer of a MNIST
network, as estimated by improved IA, tracks the number of unstable ReLUs determined by the exact
veriﬁer quite well. We also present estimates determined via naive IA for comparison.

Dataset

Epsilon Training Estimation

Method Method

Unstable ReLUs Unstable ReLUs Unstable ReLUs
in 1st Layer

in 2nd Layer

in 3rd Layer

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

Control

+RS

Control

+RS

Control

+RS

Exact
61.14
Improved IA 61.14
61.14
Naive IA

21.64
Exact
Improved IA 21.64
21.64
Naive IA

Exact
17.47
Improved IA 17.47
17.47
Naive IA

Exact
29.91
Improved IA 29.91
29.91
Naive IA

Exact
36.76
Improved IA 36.76
36.76
Naive IA

24.43
Exact
Improved IA 24.43
24.43
Naive IA

185.30
185.96 (+0.4%)
188.44 (+1.7%)

31.73
43.40 (+36.8%)
69.96 (+120.5%)

64.73
64.80 (+0.1%)
65.34 (+0.9%)

14.67
18.97 (+29.4%)
33.51 (+128.5%)

37.92
48.88 (+28.9%)
69.75 (+84.0%)

24.05
28.40 (+18.1%)
40.47 (+68.3%)

28.64
31.19 (+8.9%)
32.13 (+12.2%)

83.42
83.44 (+0.02%)
83.52 (+0.1%)

40.74
46.00 (+12.9%)
48.27 (+18.5%)

142.95
142.95
142.95

54.47
54.47
54.47

48.47
48.47
48.47

Table 4: Comparison between the average number of unstable ReLUs as found by the exact veriﬁer
of Tjeng et al. (2019) and the estimated average number of unstable ReLUs found by improved IA
and naive IA. We compare these estimation methods on the control and “+RS” networks for MNIST
that we described in Section 3.3.4

15

Published as a conference paper at ICLR 2019

C.4 ON THE CONSERVATIVE NATURE OF IA BOUNDS

The upper and lower bounds we compute on each ReLU via either naive IA or improved IA are
conservative. Thus, every unstable ReLU will always be correctly labeled as unstable, while stable
ReLUs can be labeled as either stable or unstable. Importantly, every unstable ReLU, as estimated
by IA bounds, is correctly labeled and penalized by RS Loss. The trade-off is that stable ReLUs
mislabeled as unstable will also be penalized, which can be an unnecessary regularization of the
model.

In Table 5 we show empirically that we can achieve the following two objectives at once when using
RS Loss combined with IA bounds.

1. Reduce the number of ReLUs labeled as unstable by IA, which is an upper bound on the

true number of unstable ReLUs as determined by the exact veriﬁer.

2. Achieve similar test set accuracy and PGD adversarial accuracy as a model trained without

RS Loss.

Dataset

Epsilon Training
Method

Estimation
Method

Total Labeled
Unstable ReLUs Accuracy

Test Set

PGD Adversarial
Accuracy

MNIST (cid:15) = 0.1

Control
+RS

Improved IA 290.5
Improved IA 105.4

Control (Large) Naive IA
Naive IA
+RS (Large)

835.8
150.3

98.94%
98.68% (-0.26%)

95.12%
95.13% (+0.01%)

99.04%
98.95% (-0.09%)

96.32%
96.58% (+0.26%)

Table 5: The addition of RS Loss results in far fewer ReLUs labeled as unstable for both 3-layer and
6-layer (Large) networks. The decrease in test set accuracy as a result of this regularization is small.

Even though IA bounds are conservative, these results show that it is still possible to decrease the
number of ReLUs labeled as unstable by IA without signiﬁcantly degrading test set accuracy. When
comparing the Control and “+RS” networks for MNIST and (cid:15) = 0.1, adding RS Loss decreased
the average number of ReLUs labeled as unstable (using bounds from Improved IA) from 290.5 to
105.4 with just a 0.26% loss in test set accuracy. The same trend held for deeper, 6-layer networks,
even when the estimation method for upper and lower bounds was the more conservative Naive IA.

16

Published as a conference paper at ICLR 2019

D FULL EXPERIMENTAL SETUP

D.1 NETWORK TRAINING DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. Following the prior
examples of Wong and Kolter (2018) and Dvijotham et al. (2018), we introduced a small tweak
where we increased the adversary strength linearly from 0.01 to (cid:15) over ﬁrst half of training and kept
it at (cid:15) for the second half. We used this training schedule to improve convergence of the training
process.

For MNIST, we trained for 70 epochs using the Adam optimizer (Kingma and Ba, 2015) with a
learning rate of 1e−4 and a batch size of 32. For CIFAR, we trained for 250 epochs using the Adam
optimizer with a learning rate of 1e−4. When using naive IA, we used a batch size of 128, and
when using improved IA, we used a batch size of 16. We used a smaller batch size in the latter
case because improved IA incurs high RAM usage during training. To speed up training on CIFAR,
we only added in RS Loss regularization in the last 20% of the training process. Using this same
sped-up training method on MNIST did not signiﬁcantly affect the results.

Dataset

Epsilon

(cid:96)1 weight RS Loss weight

MNIST 0.1
MNIST 0.2
MNIST 0.3
CIFAR 2/255
CIFAR 8/255

2e−5
2e−5
2e−5
1e−5
1e−5

12e−5
1e−4
12e−5
1e−3
2e−3

Table 6: Weights chosen using line search for (cid:96)1 regularization and RS Loss in each setting

For each setting, we ﬁnd a suitable weight on RS Loss via line search. The same weight is used for
each ReLU. The ﬁve weights we chose are displayed above in Table 6, along with weights chosen
for (cid:96)1-regularization.

We also train “+RS” models using naive IA to show that our technique for inducing ReLU stability
can work while having small training time overhead – full details on “+RS (Naive IA)” networks are
in Appendix E.

D.2 VERIFIER OVERVIEW

The MILP-based exact veriﬁer of Tjeng et al. (2019), which we use, proceeds in two steps for every
input. They are the model-build step and the solve step.

First, the veriﬁer builds a MILP model based on the neural network and the input. In particular, the
veriﬁer will compute upper and lower bounds on each ReLU using a speciﬁc bound computation al-
gorithm. We chose the default bound computation algorithm in the code, which uses LP to compute
bounds. LP bounds are tighter than the bounds computed via IA, which is another option available in
the veriﬁer. The model-build step’s speed appeared to depend primarily on the tightening algorithm
(IA was faster than LP) and the number of variables in the MILP (which, in turn, depends on the
sparsity of the weights of the neural network). The veriﬁer takes advantage of these bounds by not
introducing a binary variables into the MILP formulation if it can determine that a particular ReLU
is stable. Thus, using LP as the tightening algorithm resulted in higher build times, but led to easier
MILP formulations.

Next, the veriﬁer solves the MILP using an off-the-shelf MILP solver. The solver we chose was
the commercial Gurobi Solver, which uses a branch-and-bound method for solving MILPs. The
solver’s speed appeared to depend primarily on the number of binary variables in the MILP (which
corresponds to the number of unstable ReLUs) as well as the total number of variables in the MILP
(which is related to the sparsity of the weight matrices). While these two numbers are strongly
correlated with solve times, some solves would still take a long time despite having few binary

17

Published as a conference paper at ICLR 2019

variables. Thus, understanding what other properties of neural networks correspond to MILPs that
are easy or hard to solve is an important area to explore further.

D.3 VERIFIER DETAILS

We used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019) using the default
settings of the code. We allotted 120 seconds for veriﬁcation of each input datapoint using the
default model build settings. We ran our experiments using the commercial Gurobi Solver (version
7.5.2), and model solves were parallelized over 8 CPU cores with Intel Xeon CPUs @ 2.20GHz
processors. We used computers with 8–32GB of RAM, depending on the size of the model being
veriﬁed. All computers used are part of an OpenStack network.

18

Published as a conference paper at ICLR 2019

E FULL EXPERIMENTAL VERIFICATION RESULTS

Dataset

Epsilon

Training
Method

Test
Set Adversarial
Accuracy

Accuracy

Total
Provable
Upper Adversarial Unstable
Accuracy
Bound

Avg
Avg
Build
Solve
ReLUs Time (s) Time (s)

PGD Veriﬁer

Adversarial Training*
+(cid:96)1-regularization
+Small Weight Pruning
+ReLU Pruning (Control)

MNIST (cid:15) = 0.1

Wong et al. (2018)***

+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

99.17%
99.00%
98.99%
98.94%
98.68%
98.53%
98.95%

98.40%
98.10%
98.08%
98.21%
95.06%

97.75%
97.33%
97.06%
97.54%

64.64%
61.12%
57.83%
61.41%

50.69%
40.45%
46.19%
42.81%

95.04% 96.00%
95.25% 95.98%
95.38% 94.93%
95.12% 94.45%
95.13% 94.38%
94.86% 94.54%
96.58% 95.60%

93.14% 90.71%
93.14% 89.98%
91.68% 88.87%
94.19% 90.40%
-
89.03%

91.64% 83.83%
92.05% 81.70%
89.19% 79.12%
93.25% 83.70%

51.58% 50.23%
49.92% 47.79%
47.03% 45.33%
50.61% 51.00%

31.28% 33.46%
26.78% 22.74%
29.66% 26.07%
28.69% 25.20%

19.00%
82.17%
89.13%
91.58%
94.33%
94.32%
95.60%

86.45%
89.79%
85.54%
89.10%
80.29%

77.99%
80.68%
76.70%
59.60%

45.53%
45.93%
44.44%
41.40%

7.09%
20.27%
18.90%
19.80%

1517.9
505.3
502.7
278.2
101.0
158.3
119.5

2970.43
21.99
11.71
6.43
0.49
0.96
0.27

198.3
108.4
217.2
133.0
-

160.9
101.5
179.0
251.2

360.0
234.3
170.1
196.7

665.9
54.2
277.8
246.5

9.41
1.13
8.50
2.93
-

11.80
2.78
6.43
37.45

21.75
13.50
6.30
29.88

82.91
22.33
33.63
20.14

650.93
79.13
19.30
9.61
4.98
4.82
156.74

7.15
4.43
4.67
171.10
-

5.14
4.34
4.00
166.39

66.42
52.58
47.11
335.97

73.28
38.84
23.66
401.72

Table 7: Full results on natural improvements from Table 1, control networks (which use all of the
natural improvements and ReLU pruning), and “+RS” networks from Tables 2 and 3. While we are
unable to determine the true adversarial accuracy, we provide two upper bounds and a lower bound.
Evaluations of robustness against a strong 40-step PGD adversary (PGD adversarial accuracy) gives
one upper bound, and the veriﬁer itself gives another upper bound because it can also prove that
the network is not robust to perturbations on certain inputs by ﬁnding adversarial examples. The
veriﬁer simultaneously ﬁnds the provable adversarial accuracy, which is a lower bound on the true
adversarial accuracy. The gap between the veriﬁer upper bound and the provable adversarial accu-
racy (veriﬁer lower bound) corresponds to inputs that the veriﬁer times out on. These are inputs that
the veriﬁer can not prove to be robust or not robust in 120 seconds. Build times and solve times
are reported in seconds. Finally, average solve time includes timeouts. In other words, veriﬁcation
solves that time out contribute 120 seconds to the total solve time.
* The “Adversarial Training” network uses a 3600 instead of 120 second timeout and is only veriﬁed
for the ﬁrst 100 images because verifying it took too long.
** The “+RS (Large)” networks are only veriﬁed for the ﬁrst 1000 images because of long build
times.
*** Wong et al. (2018); Dvijotham et al. (2018), and Mirman et al. (2018), which we compare to in
Table 3, do not report results on MNIST, (cid:15) = 0.2 in their papers. We ran the publicly available code
of Wong et al. (2018) on MNIST, (cid:15) = 0.2 to generate these results for comparison.

19

Published as a conference paper at ICLR 2019

F DISCUSSION ON VERIFICATION AND CERTIFICATION

Exact veriﬁcation and certiﬁcation are two related approaches to formally verifying properties of
neural networks, such as adversarial robustness.
In both cases, the end goal is formal veriﬁca-
tion. Certiﬁcation methods, which solve an easier-to-solve relaxation of the exact veriﬁcation prob-
lem, are important developments because exact veriﬁcation previously appeared computationally
intractable for all but the smallest models.

For the case of adversarial robustness, certiﬁcation methods exploit a trade-off between provable
robustness and speed. They can fail to provide certiﬁcates of robustness for some inputs that are
actually robust, but they will either ﬁnd or fail to ﬁnd certiﬁcates of robustness quickly. On the other
hand, exact veriﬁers will always give the correct answer if given enough time, but exact veriﬁers can
sometimes take many hours to formally verify robustness on even a single input.

In general, the process of training a robust neural network and then formally verifying its robustness
happens in two steps.

• Step 1: Training
• Step 2: Veriﬁcation or Certiﬁcation

Most papers on certiﬁcation, including Wong and Kolter (2018); Wong et al. (2018); Dvijotham
et al. (2018); Raghunathan et al. (2018) and Mirman et al. (2018), propose a method for step 2
(the certiﬁcation step), and then propose a training objective in step 1 that is directly related to
their method for step 2. We call this paradigm “co-training.” In Raghunathan et al. (2018), they
found that using their step 2 on a model trained using Wong and Kolter (2018)’s step 1 resulted in
extremely poor provable robustness (less than 10%), and the same was true when using Wong and
Kolter (2018)’s step 2 on a model trained using their step 1.

We focus on MILP-based exact veriﬁcation as our step 2, which encompasses the best current exact
veriﬁcation methods. The advantage of using exact veriﬁcation for step 2 is that it will be accurate,
regardless of what method is used in step 1. The disadvantage of using exact veriﬁcation for step
2 is that it could be extremely slow. For our step 1, we used standard robust adversarial training.
In order to signiﬁcantly speed up exact veriﬁcation as step 2, we proposed techniques that could be
added to step 1 to induce weight sparsity and ReLU stability.

In general, we believe it is important to develop effective methods for step 1, given that step 2 is
exact veriﬁcation. However, ReLU stability can also be beneﬁcial for tightening the relaxation of
certiﬁcation approaches like that of Wong et al. (2018) and Dvijotham et al. (2018), as unstable
ReLUs are the primary cause of the overapproximation that occurs in the relaxation step. Thus, our
techniques for inducing ReLU stability can be useful for certiﬁcation as well.

Finally, in recent literature on veriﬁcation and certiﬁcation, most works have focused on formally
verifying the property of adversarial robustness of neural networks. However, veriﬁcation of other
properties could be useful, and our techniques to induce weight sparsity and ReLU stability would
still be useful for veriﬁcation of other properties for the exact same reasons that they are useful in
the case of adversarial robustness.

20

9
1
0
2
 
r
p
A
 
3
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
0
0
3
0
.
9
0
8
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2019

TRAINING FOR FASTER ADVERSARIAL ROBUSTNESS
VERIFICATION VIA INDUCING RELU STABILITY

Kai Y. Xiao Vincent Tjeng Nur Muhammad (Mahi) Shaﬁullah Aleksander M ˛adry
Massachusetts Institute of Technology
Cambridge, MA 02139
{kaix, vtjeng, nshafiul, madry}@mit.edu

ABSTRACT

We explore the concept of co-design in the context of neural network veriﬁcation.
Speciﬁcally, we aim to train deep neural networks that not only are robust to ad-
versarial perturbations but also whose robustness can be veriﬁed more easily. To
this end, we identify two properties of network models – weight sparsity and so-
called ReLU stability – that turn out to signiﬁcantly impact the complexity of the
corresponding veriﬁcation task. We demonstrate that improving weight sparsity
alone already enables us to turn computationally intractable veriﬁcation problems
into tractable ones. Then, improving ReLU stability leads to an additional 4–13x
speedup in veriﬁcation times. An important feature of our methodology is its “uni-
versality,” in the sense that it can be used with a broad range of training procedures
and veriﬁcation approaches.

1

INTRODUCTION

Deep neural networks (DNNs) have recently achieved widespread success in image classiﬁcation
(Krizhevsky et al., 2012), face and speech recognition (Taigman et al., 2014; Hinton et al., 2012),
and game playing (Silver et al., 2016; 2017). This success motivates their application in a broader
set of domains, including more safety-critical environments. This thrust makes understanding the
reliability and robustness of the underlying models, let alone their resilience to manipulation by
malicious actors, a central question. However, predictions made by machine learning models are
often brittle. A prominent example is the existence of adversarial examples (Szegedy et al., 2014):
imperceptibly modiﬁed inputs that cause state-of-the-art models to misclassify with high conﬁdence.

There has been a long line of work on both generating adversarial examples, called attacks (Carlini
and Wagner, 2017b;a; Athalye et al., 2018a;b; Uesato et al., 2018; Evtimov et al., 2017), and training
models robust to adversarial examples, called defenses (Goodfellow et al., 2015; Papernot et al.,
2016; Madry et al., 2018; Kannan et al., 2018). However, recent research has shown that most
defenses are ineffective (Carlini and Wagner, 2017a; Athalye et al., 2018a; Uesato et al., 2018).
Furthermore, even for defenses such as that of Madry et al. (2018) that have seen empirical success
against many attacks, we are unable to conclude yet with certainty that they are robust to all attacks
that we want these models to be resilient to.

This state of affairs gives rise to the need for veriﬁcation of networks, i.e., the task of formally
proving that no small perturbations of a given input can cause it to be misclassiﬁed by the network
model. Although many exact veriﬁers1 have been designed to solve this problem, the veriﬁcation
process is often intractably slow. For example, when using the Reluplex veriﬁer of Katz et al. (2017),
even verifying a small MNIST network turns out to be computationally infeasible. Thus, addressing
this intractability of exact veriﬁcation is the primary goal of this work.

Our Contributions
Our starting point is the observation that, typically, model training and veriﬁcation are decoupled
and seen as two distinct steps. Even though this separation is natural, it misses a key opportunity:
the ability to align these two stages. Speciﬁcally, applying the principle of co-design during model

1Also sometimes referred to as combinatorial veriﬁers.

1

Published as a conference paper at ICLR 2019

training is possible: training models in a way to encourage them to be simultaneously robust and
easy-to-exactly-verify. This insight is the cornerstone of the techniques we develop in this paper.

In this work, we use the principle of co-design to develop training techniques that give models
that are both robust and easy-to-verify. Our techniques rely on improving two key properties of
networks: weight sparsity and ReLU stability. Speciﬁcally, we ﬁrst show that natural methods for
improving weight sparsity during training, such as (cid:96)1-regularization, give models that can already be
veriﬁed much faster than current methods. This speedup happens because in general, exact veriﬁers
beneﬁt from having fewer variables in their formulations of the veriﬁcation task. For instance, for
exact veriﬁers that rely on linear programming (LP) solvers, sparser weight matrices means fewer
variables in those constraints.

We then focus on the major speed bottleneck of current approaches to exact veriﬁcation of ReLU
networks: the need of exact veriﬁcation methods to “branch,” i.e., consider two possible cases for
each ReLU (ReLU being active or inactive). Branching drastically increases the complexity of
veriﬁcation. Thus, well-optimized veriﬁers will not need to branch on a ReLU if it can determine
that the ReLU is stable, i.e.
that the ReLU will always be active or always be inactive for any
perturbation of an input. This motivates the key goal of the techniques presented in this paper: we
aim to minimize branching by maximizing the number of stable ReLUs. We call this goal ReLU
stability and introduce a regularization technique to induce it.

Our techniques enable us to train weight-sparse and ReLU stable networks for MNIST and CIFAR-
10 that can be veriﬁed signiﬁcantly faster. Speciﬁcally, by combining natural methods for inducing
weight sparsity with a robust adversarial training procedure (cf. Goodfellow et al. (2015)), we are
able to train networks for which almost 90% of inputs can be veriﬁed in an amount of time that
is small2 compared to previous veriﬁcation techniques. Then, by also adding our regularization
technique for inducing ReLU stability, we are able to train models that can be veriﬁed an addi-
tional 4–13x times as fast while maintaining state-of-the-art accuracy on MNIST. Our techniques
show similar improvements for exact veriﬁcation of CIFAR models. In particular, we achieve the
following veriﬁcation speed and provable robustness results for (cid:96)∞ norm-bound adversaries:

Dataset

Epsilon

Provable Adversarial Accuracy Average Solve Time (s)

MNIST

(cid:15) = 0.1
(cid:15) = 0.2
(cid:15) = 0.3

CIFAR

(cid:15) = 2/255
(cid:15) = 8/255

94.33%
89.79%
80.68%

45.93%
20.27%

0.49
1.13
2.78

13.50
22.33

Our network for (cid:15) = 0.1 on MNIST achieves provable adversarial accuracies comparable with the
current best results of Wong et al. (2018) and Dvijotham et al. (2018), and our results for (cid:15) = 0.2 and
(cid:15) = 0.3 achieve the best provable adversarial accuracies yet. To the best of our knowledge, we also
achieve the ﬁrst nontrivial provable adversarial accuracy results using exact veriﬁers for CIFAR-10.

Finally, we design our training techniques with universality as a goal. We focus on improving the
input to the veriﬁcation process, regardless of the veriﬁer we end up using. This is particularly
important because research into network veriﬁcation methods is still in its early stages, and our co-
design methods are compatible with the best current veriﬁers (LP/MILP-based methods) and should
be compatible with any future improvements in veriﬁcation.

Our code is available at https://github.com/MadryLab/relu_stable.

2 BACKGROUND AND RELATED WORK

Exact veriﬁcation of networks has been the subject of many recent works (Katz et al., 2017; Ehlers,
2017; Carlini et al., 2017; Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). To
understand the context of these works, observe that for linear networks, the task of exact veriﬁcation
is relatively simple and can be done by solving a LP. For more complex models, the presence of
nonlinear ReLUs makes veriﬁcation over all perturbations of an input much more challenging. This

2We chose our time budget for veriﬁcation to be 120 seconds per input image.

2

Published as a conference paper at ICLR 2019

is so as ReLUs can be active or inactive depending on the input, which can cause exact veriﬁers
to “branch" and consider these two cases separately. The number of such cases that veriﬁers have
to consider might grow exponentially with the number of ReLUs, so veriﬁcation speed will also
grow exponentially in the worst case. Katz et al. (2017) further illustrated the difﬁculty of exact
veriﬁcation by proving that it is NP-complete. In recent years, formal veriﬁcation methods were
developed to verify networks. Most of these methods use satisﬁability modulo theory (SMT) solvers
(Katz et al., 2017; Ehlers, 2017; Carlini et al., 2017) or LP and Mixed-Integer Linear Programming
(MILP) solvers (Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). However,
all of them are limited by the same issue of scaling poorly with the number of ReLUs in a network,
making them prohibitively slow in practice for even medium-sized models.

One recent approach for dealing with the inefﬁciency of exact veriﬁers is to focus on certiﬁcation
methods3 (Wong and Kolter, 2018; Wong et al., 2018; Dvijotham et al., 2018; Raghunathan et al.,
2018; Mirman et al., 2018; Sinha et al., 2018). In contrast to exact veriﬁcation, these methods do not
solve the veriﬁcation task directly; instead, they rely on solving a relaxation of the veriﬁcation prob-
lem. This relaxation is usually derived by overapproximating the adversarial polytope, or the space
of outputs of a network for a region of possible inputs. These approaches rely on training models in
a speciﬁc manner that makes certiﬁcation of those models easier. As a result, they can often obtain
provable adversarial accuracy results faster. However, certiﬁcation is fundamentally different from
veriﬁcation in two primary ways. First, it solves a relaxation of the original veriﬁcation problem. As
a result, certiﬁcation methods can fail to certify many inputs that are actually robust to perturbations
– only exact veriﬁers, given enough time, can give conclusive answers on robustness for every single
input. Second, certiﬁcation approaches fall under the paradigm of co-training, where a certiﬁcation
method only works well on models speciﬁcally trained for that certiﬁcation step. When used as a
black box on arbitrary models, the certiﬁcation step can yield a high rate of false negatives. For
example, Raghunathan et al. (2018) found that their certiﬁcation step was signiﬁcantly less effective
when used on a model trained using Wong and Kolter (2018)’s training method, and vice versa. In
contrast, we design our methods to be universal. They can be combined with any standard train-
ing procedure for networks and will improve exact veriﬁcation speed for any LP/MILP-based exact
veriﬁer. Our methods can also decrease the amount of overapproximation incurred by certiﬁcation
methods like Wong and Kolter (2018); Dvijotham et al. (2018). Similar to most of the certiﬁcation
methods, our technique can be made to have very little training time overhead.

Finally, subsequent work of Gowal et al. (2018) shows how applying interval bound propagation
during training, combined with MILP-based exact veriﬁcation, can lead to provably robust networks.

3 TRAINING VERIFIABLE NETWORK MODELS

We begin by discussing the task of verifying a network and identify two key properties of networks
that lead to improved veriﬁcation speed: weight sparsity and so-called ReLU stability. We then use
natural regularization methods for inducing weight sparsity as well as a new regularization method
for inducing ReLU stability. Finally, we demonstrate that these methods signiﬁcantly speed up
veriﬁcation while maintaining state-of-the-art accuracy.

3.1 VERIFYING ADVERSARIAL ROBUSTNESS OF NETWORK MODELS

Deep neural network models. Our focus will be on one of the most common architectures for
state-of-the-art models: k-layer fully-connected feed-forward DNN classiﬁers with ReLU non-
linearities4. Such models can be viewed as a function f (·, W, b), where W and b represent the
weight matrices and biases of each layer. For an input x, the output f (x, W, b) of the DNN is

3These works use both “veriﬁcation” and “certiﬁcation” to describe their methods. For clarity, we use “cer-
tiﬁcation” to describe their approaches, while we use “veriﬁcation” to describe exact veriﬁcation approaches.
For a more detailed discussion of the differences, see Appendix F.

4Note that this encompasses common convolutional network architectures because every convolutional layer

can be replaced by an equivalent fully-connected layer.

3

Published as a conference paper at ICLR 2019

deﬁned as:

z0 = x
ˆzi = zi−1Wi + bi
zi = max( ˆzi, 0)

(1)
(2)
(3)
(4)
Here, for each layer i, we let ˆzij denote the jth ReLU pre-activation and let ˆzij(x) denote the value
of ˆzij on an input x. ˆzk−1 is the ﬁnal, output layer with an output unit for each possible label (the
logits). The network will make predictions by selecting the label with the largest logit.

for i = 1, 2, . . . , k − 1
for i = 1, 2, . . . , k − 2

f (x, W, b) = ˆzk−1

Adversarial robustness. For a network to be reliable, it should make predictions that are robust
– that is, it should predict the same output for inputs that are very similar. Speciﬁcally, we want
the DNN classiﬁer’s predictions to be robust to a set Adv(x) of possible adversarial perturbations
of an input x. We focus on (cid:96)∞ norm-bound adversarial perturbations, where Adv(x) = {x(cid:48)
:
||x(cid:48) − x||∞ ≤ (cid:15)} for some constant (cid:15), since it is the most common one considered in adversarial
robustness and veriﬁcation literature (thus, it currently constitutes a canonical benchmark). Even so,
our methods can be applied to other (cid:96)p norms and broader sets of perturbations.
Veriﬁcation of network models. For an input x with correct label y, a perturbed input x(cid:48) can cause
a misclassiﬁcation if it makes the logit of some incorrect label ˆy larger than the logit of y on x(cid:48). We
can thus express the task of ﬁnding an adversarial perturbation as the optimization problem:

f (x(cid:48), W )y − f (x(cid:48), W )ˆy

min
x(cid:48),ˆy
subject to x(cid:48) ∈ Adv(x)

An adversarial perturbation exists if and only if the objective of the optimization problem is negative.

Adversarial accuracies. We deﬁne the true adversarial accuracy of a model to be the fraction of
test set inputs for which the model is robust to all allowed perturbations. By deﬁnition, evaluations
against speciﬁc adversarial attacks like PGD or FGSM provide an upper bound to this accuracy,
while certiﬁcation methods provide lower bounds. Given sufﬁcient time for each input, an exact
veriﬁer can prove robustness to perturbations, or ﬁnd a perturbation where the network makes a
misclassiﬁcation on the input, and thus exactly determine the true adversarial accuracy. This is in
contrast to certiﬁcation methods, which solve a relaxation of the veriﬁcation problem and can not
exactly determine the true adversarial accuracy no matter how much time they have.

However, such exact veriﬁcation may take impractically long for certain inputs, so we instead com-
pute the provable adversarial accuracy, which we deﬁne as the fraction of test set inputs for which
the veriﬁer can prove robustness to perturbations within an allocated time budget (timeout). Simi-
larly to certiﬁable accuracy, this accuracy constitutes a lower bound on the true adversarial accuracy.
A model can thus, e.g., have high true adversarial accuracy and low provable adversarial accuracy if
veriﬁcation of the model is too slow and often fails to complete before timeout.

Also, in our evaluations, we chose to use the MILP exact veriﬁer of Tjeng et al. (2019) when per-
forming experiments, as it is both open source and the fastest veriﬁer we are aware of.

3.2 WEIGHT SPARSITY AND ITS IMPACT ON VERIFICATION SPEED

The ﬁrst property of network models that we want to improve in order to speed up exact veriﬁcation
of those models is weight sparsity. Weight sparsity is important for veriﬁcation speed because many
exact veriﬁers rely on solving LP or MILP systems, which beneﬁt from having fewer variables.
We use two natural regularization methods for improving weight sparsity: (cid:96)1-regularization and
small weight pruning. These techniques signiﬁcantly improve veriﬁcation speed – see Table 1.
Verifying even small MNIST networks is almost completely intractable without them. Speciﬁcally,
the veriﬁer can only prove robustness of an adversarially-trained model on 19% of inputs with a one
hour budget per input, while the veriﬁer can prove robustness of an adversarially-trained model with
(cid:96)1-regularization and small weight pruning on 89.13% of inputs with a 120 second budget per input.

Interestingly, even though adversarial training improves weight sparsity (see Appendix B) it was
still necessary to use (cid:96)1-regularization and small weight pruning. These techniques further promoted
weight sparsity and gave rise to networks that were much easier to verify.

4

Published as a conference paper at ICLR 2019

Dataset

Epsilon

Training
Method

Test Set
Accuracy

Provable Adversarial
Accuracy

Average
Solve Time (s)

MNIST (cid:15) = 0.1

1 Adversarial Training
+(cid:96)1-Regularization
2
+Small Weight Pruning
3
4

+ReLU Pruning (control)

99.17%
99.00%
98.99%
98.94%

19.00%
82.17%
89.13%
91.58%

2970.43
21.99
11.71
6.43

Table 1: Improvement in provable adversarial accuracy and veriﬁcation solve times when incremen-
tally adding natural regularization methods for improving weight sparsity and ReLU stability into
the model training procedure, before veriﬁcation occurs. Each row represents the addition of another
method – for example, Row 3 uses adversarial training, (cid:96)1-regularization, and small weight pruning.
Row 4 adds ReLU pruning (see Appendix A). Row 4 is the control model for MNIST and (cid:15) = 0.1
that we present again in comparisons in Tables 2 and 3. We use a 3600 instead of 120 second timeout
for Row 1 and only veriﬁed the ﬁrst 100 images (out of 10000) because verifying it took too long.

3.3 RELU STABILITY

Next, we target the primary speed bottleneck of exact veriﬁcation: the number of ReLUs the veriﬁer
has to branch on. In our paper, this corresponds to the notion of inducing ReLU stability. Before we
describe our methodology, we formally deﬁne ReLU stability.

(2)).

Given an input x, a set of allowed perturbations Adv(x), and a ReLU, exact veriﬁers may need
to branch based on the possible pre-activations of the ReLU, namely ˆzij(Adv(x)) = {ˆzij(x(cid:48)) :
x(cid:48) ∈ Adv(x)} (cf.
If there exist two perturbations x(cid:48), x(cid:48)(cid:48) in the set Adv(x) such that
sign(ˆzij(x(cid:48))) (cid:54)= sign(ˆzij(x(cid:48)(cid:48))), then the veriﬁer has to consider that for some perturbed inputs the
ReLU is active (zij = ˆzij) and for other perturbed inputs inactive (zij = 0). The more such cases
the veriﬁer faces, the more branches it has to consider, causing the complexity of veriﬁcation to in-
crease exponentially. Intuitively, a model with 1000 ReLUs among which only 100 ReLUs require
branching will likely be much easier to verify than a model with 200 ReLUs that all require branch-
ing. Thus, it is advantageous for the veriﬁer if, on an input x with allowed perturbation set Adv(x),
the number of ReLUs such that

sign(ˆzij(x(cid:48))) = sign(ˆzij(x)) ∀x(cid:48) ∈ Adv(x)
is maximized. We call a ReLU for which (5) holds on an input x a stable ReLU on that input. If (5)
does not hold, then the ReLU is an unstable ReLU.

(5)

Directly computing whether a ReLU is stable on a given input x is difﬁcult because doing so would
involve considering all possible values of ˆzij(Adv(x)). Instead, exact veriﬁers compute an upper
bound ˆuij and a lower bound ˆlij of ˆzij(Adv(x)). If 0 ≤ ˆlij or ˆuij ≤ 0, they can replace the ReLU
with the identity function or the zero function, respectively. Otherwise, if ˆlij < 0 < ˆuij, these
veriﬁers then determine that they need to “branch” on that ReLU. Thus, we can rephrase (5) as

sign(ˆuij) = sign(ˆlij)

(6)

We will discuss methods for determining these upper and lower bounds ˆuij, ˆlij in Section 3.3.2.

3.3.1 A REGULARIZATION TECHNIQUE FOR INDUCING RELU STABILITY: RS LOSS

As we see from equation (6), a function that would indicate exactly when a ReLU is stable is
F ∗(ˆuij, ˆlij) = sign(ˆuij) · sign(ˆlij). Thus, it would be natural to use this function as a regular-
izer. However, this function is non-differentiable and if used in training a model, would provide no
useful gradients during back-propagation. Thus, we use the following smooth approximation of F ∗
(see Fig. 1) which provides the desired gradients:

F (ˆuij, ˆlij) = − tanh(1 + ˆuij · ˆlij)

We call the corresponding objective RS Loss, and show in Fig. 2a that using this loss function as
a regularizer effectively decreases the number of unstable ReLUs. RS Loss thus encourages ReLU
stability, which, in turn, speeds up exact veriﬁcation - see Fig. 2b.

5

Published as a conference paper at ICLR 2019

Figure 1: Plot and contour plot of the function F (x, y) = − tanh(1 + x · y)

3.3.2 ESTIMATING RELU UPPER AND LOWER BOUNDS ON ACTIVATIONS

A key aspect of using RS Loss is determining the upper and lower bounds ˆuij, ˆlij for each ReLU (cf.
(6)). The bounds for the inputs z0 (cf. (1)) are simple – for the input x, we know x − (cid:15) ≤ z0 ≤ x + (cid:15),
so ˆu0 = x − (cid:15), ˆl0 = x + (cid:15). For all subsequent zij, we estimate bounds using either the naive interval
arithmetic (IA) approach described in Tjeng et al. (2019) or an improved version of it. The improved
version is a tighter estimate but uses more memory and training time, and thus is most effective on
smaller networks. We present the details of naive IA and improved IA in Appendix C.

Interval arithmetic approaches can be implemented relatively efﬁciently and work well with back-
propagation because they only involve matrix multiplications. This contrasts with how exact veri-
ﬁers compute these bounds, which usually involves solving LPs or MILPs. Interval arithmetic also
overestimates the number of unstable ReLUs. This means that minimizing unstable ReLUs based
on IA bounds will provide an upper bound on the number of unstable ReLUs determined by exact
veriﬁers. In particular, IA will properly penalize every unstable ReLU.

Improved IA performs well in practice, overestimating the number of unstable ReLUs by less than
0.4% in the ﬁrst 2 layers of MNIST models and by less than 36.8% (compared to 128.5% for naive
IA) in the 3rd layer. Full experimental results are available in Table 4 of Appendix C.3.

3.3.3

IMPACT OF RELU STABILITY IMPROVEMENTS ON VERIFICATION SPEED

We provide experimental evidence that RS Loss regularization improves ReLU stability and speeds
up average veriﬁcation times by more than an order of magnitude in Fig. 2b. To isolate the effect of
RS Loss, we compare MNIST models trained in exactly the same way other than the weight on RS
Loss. When comparing a network trained with a RS Loss weight of 5e−4 to a network with a RS
Loss weight of 0, the former has just 16% as many unstable ReLUs and can be veriﬁed 65x faster.
The caveat here is that the former has 1.00% lower test set accuracy.

We also compare veriﬁcation speed with and without RS Loss on MNIST networks for different
values of (cid:15) (0.1, 0.2, and 0.3) in Fig. 2c. We choose RS Loss weights that cause almost no test set
accuracy loss (less than 0.50% - See Table 3) in these cases, and we still observe a 4–13x speedup
from RS Loss. For CIFAR, RS Loss gives a smaller speedup of 1.6–3.7x (See Appendix E).

3.3.4

IMPACT OF RELU STABILITY IMPROVEMENTS ON PROVABLE ADVERSARIAL
ACCURACY

As the weight on the RS Loss used in training a model increases, the ReLU stability of the model
will improve, speeding up veriﬁcation and likely improving provable adversarial accuracy. However,
like most regularization methods, placing too much weight on RS Loss can decrease the model ca-
pacity, potentially lowering both the true adversarial accuracy and the provable adversarial accuracy.
Therefore, it is important to choose the weight on RS Loss carefully to obtain both high provable
adversarial accuracy and faster veriﬁcation speeds.

To show the effectiveness of RS Loss in improving provable adversarial accuracy, we train two
networks for each dataset and each value of (cid:15). One is a “control” network that uses all of the natural
improvements for inducing both weight sparsity ((cid:96)1-regularization and small weight pruning) and
ReLU stability (ReLU pruning - see Appendix A). The second is a “+RS” network that uses RS

6

Published as a conference paper at ICLR 2019

(a)

(b)

(c)

Figure 2: (a) Average number of unstable ReLUs by layer and (b) average veriﬁcation solve times
of 6 networks trained with different weights on RS Loss for MNIST and (cid:15) = 0.1 . Averages are
taken over all 10000 MNIST test set inputs. Both metrics improve signiﬁcantly with increasing
RS Loss weight. An RS Loss weight of 0 corresponds to the control network, while an RS Loss
weight of 0.00012 corresponds to the “+RS” network for MNIST and (cid:15) = 0.1 in Tables 2 and 3. (c)
Improvement in the average time taken by a veriﬁer to solve the veriﬁcation problem after adding
RS Loss to the training procedure, for different (cid:15) on MNIST. The weight on RS Loss was chosen so
that the “+RS” models have test set accuracies within 0.50% of the control models.

Loss in addition to all of the same natural improvements. This lets us isolate the incremental effect
of adding RS Loss to the training procedure.

In addition to attaining a 4–13x speedup in MNIST veriﬁcation times (see Fig. 2c), we achieve
higher provable adversarial accuracy in every single setting when using RS Loss. This is especially
notable for the hardest veriﬁcation problem we tackle – proving robustness to perturbations with (cid:96)∞
norm-bound 8/255 on CIFAR-10 – where adding RS Loss nearly triples the provable adversarial
accuracy from 7.09% to 20.27%. This improvement is primarily due to veriﬁcation speedup induced
by RS Loss, which allows the veriﬁer to ﬁnish proving robustness for far more inputs within the 120
second time limit. These results are shown in Table 2.

Table 2: Provable Adversarial Accuracies for the control and “+RS” networks in each setting.

MNIST, (cid:15) = 0.1 MNIST, (cid:15) = 0.2 MNIST, (cid:15) = 0.3 CIFAR, (cid:15) = 2/255 CIFAR, (cid:15) = 8/255

Control
+RS

91.58
94.33

86.45
89.79

77.99
80.68

45.53
45.93

7.09
20.27

4 EXPERIMENTS

4.1 EXPERIMENTS ON MNIST AND CIFAR

In addition to the experimental results already presented, we compare our control and “+RS” net-
works with the best available results presented in the state-of-the-art certiﬁable defenses of Wong
et al. (2018), Dvijotham et al. (2018), and Mirman et al. (2018) in Table 3. We compare their test
set accuracy, PGD adversarial accuracy (an evaluation of robustness against a strong 40-step PGD
adversarial attack), and provable adversarial accuracy. Additionally, to show that our method can
scale to larger architectures, we train and verify a “+RS (Large)” network for each dataset and (cid:15).

In terms of provable adversarial accuracies, on MNIST, our results are signiﬁcantly better than those
of Wong et al. (2018) for larger perturbations of (cid:15) = 0.3, and comparable for (cid:15) = 0.1. On CIFAR-
10, our method is slightly less effective, perhaps indicating that more unstable ReLUs are necessary
to properly learn a robust CIFAR classiﬁer. We also experienced many more instances of the veriﬁer
reaching its allotted 120 second time limit on CIFAR, especially for the less ReLU stable control
networks. Full experimental details for each model in Tables 1, 2, and 3, including a breakdown of
veriﬁcation solve results (how many images did the veriﬁer: A. prove robust B. ﬁnd an adversarial
example for C. time out on), are available in Appendix E.

7

Published as a conference paper at ICLR 2019

Table 3: Comparison of test set, PGD adversarial, and provable adversarial accuracy of networks
trained with and without RS Loss. We also provide the best available certiﬁable adversarial and
PGD adversarial accuracy of any single models from Wong et al. (2018), Dvijotham et al. (2018),
and Mirman et al. (2018) for comparison, and highlight the best provable accuracy for each (cid:15).
* The provable adversarial accuracy for “+RS (Large)” is only computed for the ﬁrst 1000 images
because the veriﬁer performs more slowly on larger models.
** Dvijotham et al. (2018); Mirman et al. (2018) use a slightly smaller (cid:15) = 0.03 = 7.65/255.
† Mirman et al. (2018) computes results over 500 images instead of all 10000.
†† Mirman et al. (2018) uses a slightly smaller (cid:15) = 0.007 = 1.785/255.

Dataset

Epsilon

Test Set
Accuracy

PGD Adversarial

Provable/Certiﬁable
Accuracy Adversarial Accuracy

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

Training
Method

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†,††

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.**
Mirman et al.†, **

98.94%
98.68%
98.95%

98.92%
98.80%
99.00%

98.40%
98.10%
98.21%

97.75%
97.33%
97.54%

85.13%
96.60%

64.64%
61.12%
61.41%

68.28%
62.00%

50.69%
40.45%
42.81%

28.67%
48.64%
54.20%

95.12%
95.13%
96.58%

-
97.13%
97.60%

93.14%
93.14%
94.19%

91.64%
92.05%
93.25%

-
93.80%

51.58%
49.92%
50.61%

-
54.60%

31.28%
26.78%
28.69%

-
32.72%
40.00%

91.58%
94.33%
95.60%

96.33%
95.56%
96.60%

86.45%
89.79%
89.10%

77.99%
80.68%
59.60%

56.90%
82.00%

45.53%
45.93%
41.40%

53.89%
52.20%

7.09%
20.27%
19.80%

21.78%
26.67%
35.20%

4.2 EXPERIMENTAL METHODS AND DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. For each setting of dataset
(MNIST or CIFAR) and (cid:15), we ﬁnd a suitable weight on RS Loss via line search (See Table 6 in
Appendix D). The same weight is used for each ReLU. During training, we used improved IA for
ReLU bound estimation for “+RS” models and use naive IA for “+RS (Large)” models because of
memory constraints. For ease of comparison, we trained our networks using the same convolutional
DNN architecture as in Wong et al. (2018). This architecture uses two 2x2 strided convolutions with
16 and 32 ﬁlters, followed by a 100 hidden unit fully connected layer. For the larger architecture,
we also use the same “large” architecture as in Wong et al. (2018). It has 4 convolutional layers with
32, 32, 64, and 64 ﬁlters, followed by 2 fully connected layers with 512 hidden units each.

For veriﬁcation, we used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019).
Model solves were parallelized over 8 CPU cores. When verifying an image, the veriﬁer of Tjeng
et al. (2019) ﬁrst builds a model, and second, solves the veriﬁcation problem (See Appendix D.2 for
details). We focus on reporting solve times because that is directly related to the task of veriﬁcation
itself. All build times for the control and “+RS” models on MNIST that we presented were between
4 and 10 seconds, and full results on build times are also presented in Appendix E.

Additional details on our experimental setup (e.g. hyperparameters) can be found in Appendix D.

8

Published as a conference paper at ICLR 2019

5 CONCLUSION

In this paper, we use the principle of co-design to develop training methods that emphasize veri-
ﬁcation as a goal, and we show that they make verifying the trained model much faster. We ﬁrst
demonstrate that natural regularization methods already make the exact veriﬁcation problem sig-
niﬁcantly more tractable. Subsequently, we introduce the notion of ReLU stability for networks,
present a method that improves a network’s ReLU stability, and show that this improvement makes
veriﬁcation an additional 4–13x faster. Our method is universal, as it can be added to any training
procedure and should speed up any exact veriﬁcation procedure, especially MILP-based methods.

Prior to our work, exact veriﬁcation seemed intractable for all but the smallest models. Thus, our
work shows progress toward reliable models that can be proven to be robust, and our techniques can
help scale veriﬁcation to even larger networks.

Many of our methods appear to compress our networks into more compact, simpler forms. We
hypothesize that the reason that regularization methods like RS Loss can still achieve very high
accuracy is that most models are overparametrized in the ﬁrst place. There exist clear parallels
between our methods and techniques in model compression (Han et al., 2016; Cheng et al., 2017b) –
therefore, we believe that drawing upon additional techniques from model compression can further
improve the ease-of-veriﬁcation of networks. We also expect that there exist objectives other than
weight sparsity and ReLU stability that are important for veriﬁcation speed. If so, further exploring
the principle of co-design for those objectives is an interesting future direction.

ACKNOWLEDGEMENTS

This work was supported by the NSF Graduate Research Fellowship under Grant No. 1122374,
by the NSF grants CCF-1553428 and CNS-1815221, and by Lockheed Martin Corporation under
award number RPP2016-002. We would like to thank Krishnamurthy Dvijotham, Ludwig Schmidt,
Michael Sun, Dimitris Tsipras, and Jonathan Uesato for helpful discussions.

9

Published as a conference paper at ICLR 2019

REFERENCES

Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense
In International Conference on

of security: Circumventing defenses to adversarial examples.
Machine Learning (ICML), 2018a.

Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. In International Conference on Machine Learning (ICML), pages 284–293, 2018b.

Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
In Proceedings of ACM Workshop on Artiﬁcial Intelligence and Security

detection methods.
(AISec), 2017a.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks.

In

Security and Privacy (SP), 2017 IEEE Symposium on, 2017b.

Nicholas Carlini, Guy Katz, Clark Barrett, and David L. Dill. Ground-truth adversarial examples.

2017.

networks. 2017a.

Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artiﬁcial neural

Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration

for deep neural networks. 2017b.

Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training veriﬁed learners with learned ver-
iﬁers. 2018.

Rüdiger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In Auto-

mated Technology for Veriﬁcation and Analysis, 2017.

Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir
Rahmati, and Dawn Song. Robust physical-world attacks on machine learning models. 2017.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. In International Conference on Learning Representations (ICLR), 2015.

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. 2018.

Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. In International Conference on Learning
Representations (ICLR), 2016.

Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, An-
drew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep
neural networks for acoustic modeling in speech recognition: The shared views of four research
groups. In IEEE Signal Processing Magazine, 2012.

Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. 2018.

Guy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer. Reluplex: An efﬁcient

smt solver for verifying deep neural networks. 2017.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations (ICLR), 2015.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012.

Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward relu

neural networks. 2017.

10

Published as a conference paper at ICLR 2019

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations (ICLR), 2018.

Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning (ICML), 2018.

Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation
as a defense to adversarial perturbations against deep neural networks. In Proceedings of IEEE
Symposium on Security and Privacy (SP), 2016.

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial exam-

ples. In International Conference on Learning Representations (ICLR), 2018.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 2017.

Aman Sinha, Hongseok Namkoong, and John Duchi. Certiﬁable distributional robustness with
principled adversarial training. In International Conference on Learning Representations (ICLR),
2018.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations (ICLR), 2014.

Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap
In Computer Vision and Pattern Recognition

to human-level performance in face veriﬁcation.
(CVPR), 2014.

Robert Tibshirani. Regression shrinkage and selection via the lasso. In Journal of the Royal Statis-

tical Society, Series B, 1994.

Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In International Conference on Learning Representations (ICLR), 2019.

Jonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial
risk and the dangers of evaluating against weak attacks. In International Conference on Machine
Learning (ICML), 2018.

Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer

adversarial polytope. In International Conference on Machine Learning (ICML), 2018.

Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and Zico Kolter. Scaling provable adversarial

defenses. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

11

Published as a conference paper at ICLR 2019

APPENDIX

A NATURAL IMPROVEMENTS

A.1 NATURAL REGULARIZATION FOR INDUCING WEIGHT SPARSITY

All of the control and “+RS” networks in our paper contain natural improvements that improve
weight sparsity, which reduce the number of variables in the LPs solved by the veriﬁer. We ob-
served that the two techniques we used for weight sparsity ((cid:96)1-regularization and small weight prun-
ing) don’t hurt test set accuracy but they dramatically improve provable adversarial accuracy and
veriﬁcation speed.

1. (cid:96)1-regularization: We use a weight of 2e−5 on MNIST and a weight of 1e−5 on CIFAR.
We chose these weights via line search by ﬁnding the highest weight that would not hurt
test set accuracy.

2. Small weight pruning: Zeroing out weights in a network that are very close to zero. We

choose to prune weights less than 1e−3.

A.2 A BASIC IMPROVEMENT FOR INDUCING RELU STABILITY: RELU PRUNING

We also use a basic idea to improve ReLU stability, which we call ReLU pruning. The main idea is
to prune away ReLUs that are not necessary.

We use a heuristic to test whether a ReLU in a network is necessary. Our heuristic is to count how
many training inputs cause the ReLU to be active or inactive. If a ReLU is active (the pre-activation
satisﬁes ˆzij(x) > 0) for every input image in the training set, then we can replace that ReLU with
the identity function and the network would behave in exactly the same way for all of those images.
Similarly, if a ReLU is inactive (ˆzij(x) < 0) for every training image, that ReLU can be replaced by
the zero function.

Extending this idea further, we expect that ReLUs that are rarely used can also be removed without
signiﬁcantly changing the behavior of the network. If only a small fraction (say, 10%) of the input
images activate a ReLU, then replacing the ReLU with the zero function will only slightly change the
network’s behavior and will not affect the accuracy too much. We provide experimental evidence of
this phenomenon on an adversarially trained ((cid:15) = 0.1) MNIST model. Conservatively, we decided
that pruning away ReLUs that are active on less than 10% of the training set or inactive on less than
10% of the training set was reasonable.

Figure 3: Removing some ReLUs does not hurt test set accuracy or accuracy against a PGD adver-
sary

12

Published as a conference paper at ICLR 2019

B ADVERSARIAL TRAINING AND WEIGHT SPARSITY

It is worth noticing that adversarial training against (cid:96)∞ norm-bound adversaries alone already makes
networks easier to verify by implicitly improving weight sparsity. Indeed, this can be shown clearly
in the case of linear networks. Recall that a linear network can be expressed as f (x) = W x + b.
Thus, an (cid:96)∞ norm-bound perturbation of the input x will produce the output

f (x(cid:48)) = x(cid:48)W + b

= xW + b + (x(cid:48) − x)W
≤ f (x) + (cid:15)||W ||1

where the last inequality is just Hölder’s inequality.
In order to limit the adversary’s ability to
perturb the output, adversarial training needs to minimize the ||W ||1 term, which is equivalent to (cid:96)1-
regularization and is known to promote weight sparsity (Tibshirani, 1994). Relatedly, Goodfellow
et al. (2015) already pointed out that adversarial attacks against linear networks will be stronger
when the (cid:96)1-norm of the weight matrices is higher.

Even in the case of nonlinear networks, adversarial training has experimentally been shown to im-
prove weight sparsity. For example, models trained according to Madry et al. (2018) and Wong and
Kolter (2018) often learn many weight-sparse layers, and we observed similar trends in the mod-
els we trained. However, it is important to note that while adversarial training alone does improve
weight sparsity, it is not sufﬁcient by itself for efﬁcient exact veriﬁcation. Additional regularization
like (cid:96)1-regularization and small weight pruning further promotes weight sparsity and gives rise to
networks that are much easier to verify.

13

Published as a conference paper at ICLR 2019

C INTERVAL ARITHMETIC

C.1 NAIVE INTERVAL ARITHMETIC

Naive IA determines upper and lower bounds for a layer based solely on the upper and lower bounds
of the previous layer.
Deﬁne W + = max(W, 0), W − = min(W, 0), u = max(ˆu, 0), and l = max(ˆl, 0). Then the bounds
on the pre-activations of layer i can be computed as follows:
i + li−1W −
i + ui−1W −

ˆui = ui−1W +
ˆli = li−1W +

i + bi
i + bi

(8)

(7)

As noted in Tjeng et al. (2019) and also Dvijotham et al. (2018), this method is efﬁcient but can lead
to relatively conservative bounds for deeper networks.

C.2

IMPROVED INTERVAL ARITHMETIC

We improve upon naive IA by exploiting ReLUs that we can determine to always be active. This
allows us to cancel symbols that are equivalent that come from earlier layers of a network.

We will use a basic example of a neural network with one hidden layer to illustrate this idea. Suppose
that we have the scalar input z0 with l0 = 0, u0 = 1, and the network has the following weights and
biases:

W1 = [1 − 1] ,

b1 = [2

2] , W2 =

b2 = 0

(cid:21)
(cid:20)1
1

,

Naive IA for the ﬁrst layer gives ˆl1 = l1 = [2
2], and applying naive IA to
the output ˆz2 gives ˆl2 = 3, ˆu2 = 5. However, because ˆl1 > 0, we know that the two ReLUs in the
hidden layer are always active and thus equivalent to the identity function. Then, the output is

1], ˆu1 = u1 = [3

ˆz2 = z11 + z12 = ˆz11 + ˆz12 = (z0 + 2) + (−z0 + 2) = 4

Thus, we can obtain the tighter bounds ˆl2 = ˆu2 = 4, as we are able to cancel out the z0 terms.
We can write this improved version of IA as follows. First, letting Wk denote row k of matrix W ,
we can deﬁne the “active” part of W as the matrix WA, where

(WA)k =

(cid:40)

Wk
0

if ˆli−1 > 0
if ˆli−1 ≤ 0

Deﬁne the “non-active” part of W as

WN = W − WA
Then, using the same deﬁnitions for the notation W +, W −, u, l as before, we can write down the
following improved version of IA which uses information from the previous 2 layers.

ˆui = ui−1Wi

+
N + li−1Wi

−
N + bi
+ ui−2(Wi−1WiA)+ + li−2(Wi−1WiA)− + bi−1WiA
−
N + bi
+ li−2(Wi−1WiA)+ + ui−2(Wi−1WiA)− + bi−1WiA
We are forced to to use li−1,j and ui−1,j if we can not determine whether or not the ReLU corre-
sponding to the activation zi−1,j is active, but we use li−2 and ui−2 whenever possible.

+
N + ui−1Wi

ˆli = li−1Wi

We now deﬁne some additional notation to help us extend this method to any number of layers. We
now seek to deﬁne fn, which is a function which takes in four sequences of length n – upper bounds,
lower bounds, weights, and biases – and outputs the current layer’s upper and lower bounds.

What we have derived so far from (7) and (8) is the following

f1(ui−1, li−1, Wi, bi) = (ui−1W +

i + li−1W −

i + bi, li−1W +

i + ui−1W −

i + bi)

14

Published as a conference paper at ICLR 2019

Let u denote a sequence of upper bounds. Let uz denote element z of the sequence, and let u[z:]
denote the sequence without the ﬁrst z elements. Deﬁne notation for l, W, and b similarly.

Then, using the fact that WN Z = (W Z)N and WAZ = (W Z)A, we can show that the following
recurrence holds:

fn+1(u, l, W, b) = f1(u1, l1, W1N , b1)

+ fn(u[1:], l[1:], (W2W1A, W[2:]), (b2W1A, b[2:]))

(9)

Let u(x,y) denote the sequence (ux, ux−1, · · · , uy), and deﬁne l(x,y), W(x,y), and b(x,y) similarly.
Then, if we want to compute the bounds on layer k using all information from the previous k layers,
we simply have to compute fk(u(k−1,0), l(k−1,0), W(k,1), b(k,1)).

From the recurrence 9, we see that using information from all previous layers to compute bounds for
layer k takes O(k) matrix-matrix multiplications. Thus, using information from all previous layers
to compute bounds for all layers of a d layer neural network only involves O(d2) additional matrix
multiplications, which is still reasonable for most DNNs. This method is still relatively efﬁcient
because it only involves matrix multiplications – however, needing to perform matrix-matrix multi-
plications as opposed to just matrix-vector multiplications results in a slowdown and higher memory
usage when compared to naive IA. We believe the improvement in the estimate of ReLU upper and
lower bounds is worth the time trade-off for most networks.

C.3 EXPERIMENTAL RESULTS ON IMPROVED IA AND NAIVE IA

In Table 4, we show empirical evidence that the number of unstable ReLUs in each layer of a MNIST
network, as estimated by improved IA, tracks the number of unstable ReLUs determined by the exact
veriﬁer quite well. We also present estimates determined via naive IA for comparison.

Dataset

Epsilon Training Estimation

Method Method

Unstable ReLUs Unstable ReLUs Unstable ReLUs
in 1st Layer

in 2nd Layer

in 3rd Layer

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

Control

+RS

Control

+RS

Control

+RS

Exact
61.14
Improved IA 61.14
61.14
Naive IA

21.64
Exact
Improved IA 21.64
21.64
Naive IA

Exact
17.47
Improved IA 17.47
17.47
Naive IA

Exact
29.91
Improved IA 29.91
29.91
Naive IA

Exact
36.76
Improved IA 36.76
36.76
Naive IA

24.43
Exact
Improved IA 24.43
24.43
Naive IA

185.30
185.96 (+0.4%)
188.44 (+1.7%)

31.73
43.40 (+36.8%)
69.96 (+120.5%)

64.73
64.80 (+0.1%)
65.34 (+0.9%)

14.67
18.97 (+29.4%)
33.51 (+128.5%)

37.92
48.88 (+28.9%)
69.75 (+84.0%)

24.05
28.40 (+18.1%)
40.47 (+68.3%)

28.64
31.19 (+8.9%)
32.13 (+12.2%)

83.42
83.44 (+0.02%)
83.52 (+0.1%)

40.74
46.00 (+12.9%)
48.27 (+18.5%)

142.95
142.95
142.95

54.47
54.47
54.47

48.47
48.47
48.47

Table 4: Comparison between the average number of unstable ReLUs as found by the exact veriﬁer
of Tjeng et al. (2019) and the estimated average number of unstable ReLUs found by improved IA
and naive IA. We compare these estimation methods on the control and “+RS” networks for MNIST
that we described in Section 3.3.4

15

Published as a conference paper at ICLR 2019

C.4 ON THE CONSERVATIVE NATURE OF IA BOUNDS

The upper and lower bounds we compute on each ReLU via either naive IA or improved IA are
conservative. Thus, every unstable ReLU will always be correctly labeled as unstable, while stable
ReLUs can be labeled as either stable or unstable. Importantly, every unstable ReLU, as estimated
by IA bounds, is correctly labeled and penalized by RS Loss. The trade-off is that stable ReLUs
mislabeled as unstable will also be penalized, which can be an unnecessary regularization of the
model.

In Table 5 we show empirically that we can achieve the following two objectives at once when using
RS Loss combined with IA bounds.

1. Reduce the number of ReLUs labeled as unstable by IA, which is an upper bound on the

true number of unstable ReLUs as determined by the exact veriﬁer.

2. Achieve similar test set accuracy and PGD adversarial accuracy as a model trained without

RS Loss.

Dataset

Epsilon Training
Method

Estimation
Method

Total Labeled
Unstable ReLUs Accuracy

Test Set

PGD Adversarial
Accuracy

MNIST (cid:15) = 0.1

Control
+RS

Improved IA 290.5
Improved IA 105.4

Control (Large) Naive IA
Naive IA
+RS (Large)

835.8
150.3

98.94%
98.68% (-0.26%)

95.12%
95.13% (+0.01%)

99.04%
98.95% (-0.09%)

96.32%
96.58% (+0.26%)

Table 5: The addition of RS Loss results in far fewer ReLUs labeled as unstable for both 3-layer and
6-layer (Large) networks. The decrease in test set accuracy as a result of this regularization is small.

Even though IA bounds are conservative, these results show that it is still possible to decrease the
number of ReLUs labeled as unstable by IA without signiﬁcantly degrading test set accuracy. When
comparing the Control and “+RS” networks for MNIST and (cid:15) = 0.1, adding RS Loss decreased
the average number of ReLUs labeled as unstable (using bounds from Improved IA) from 290.5 to
105.4 with just a 0.26% loss in test set accuracy. The same trend held for deeper, 6-layer networks,
even when the estimation method for upper and lower bounds was the more conservative Naive IA.

16

Published as a conference paper at ICLR 2019

D FULL EXPERIMENTAL SETUP

D.1 NETWORK TRAINING DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. Following the prior
examples of Wong and Kolter (2018) and Dvijotham et al. (2018), we introduced a small tweak
where we increased the adversary strength linearly from 0.01 to (cid:15) over ﬁrst half of training and kept
it at (cid:15) for the second half. We used this training schedule to improve convergence of the training
process.

For MNIST, we trained for 70 epochs using the Adam optimizer (Kingma and Ba, 2015) with a
learning rate of 1e−4 and a batch size of 32. For CIFAR, we trained for 250 epochs using the Adam
optimizer with a learning rate of 1e−4. When using naive IA, we used a batch size of 128, and
when using improved IA, we used a batch size of 16. We used a smaller batch size in the latter
case because improved IA incurs high RAM usage during training. To speed up training on CIFAR,
we only added in RS Loss regularization in the last 20% of the training process. Using this same
sped-up training method on MNIST did not signiﬁcantly affect the results.

Dataset

Epsilon

(cid:96)1 weight RS Loss weight

MNIST 0.1
MNIST 0.2
MNIST 0.3
CIFAR 2/255
CIFAR 8/255

2e−5
2e−5
2e−5
1e−5
1e−5

12e−5
1e−4
12e−5
1e−3
2e−3

Table 6: Weights chosen using line search for (cid:96)1 regularization and RS Loss in each setting

For each setting, we ﬁnd a suitable weight on RS Loss via line search. The same weight is used for
each ReLU. The ﬁve weights we chose are displayed above in Table 6, along with weights chosen
for (cid:96)1-regularization.

We also train “+RS” models using naive IA to show that our technique for inducing ReLU stability
can work while having small training time overhead – full details on “+RS (Naive IA)” networks are
in Appendix E.

D.2 VERIFIER OVERVIEW

The MILP-based exact veriﬁer of Tjeng et al. (2019), which we use, proceeds in two steps for every
input. They are the model-build step and the solve step.

First, the veriﬁer builds a MILP model based on the neural network and the input. In particular, the
veriﬁer will compute upper and lower bounds on each ReLU using a speciﬁc bound computation al-
gorithm. We chose the default bound computation algorithm in the code, which uses LP to compute
bounds. LP bounds are tighter than the bounds computed via IA, which is another option available in
the veriﬁer. The model-build step’s speed appeared to depend primarily on the tightening algorithm
(IA was faster than LP) and the number of variables in the MILP (which, in turn, depends on the
sparsity of the weights of the neural network). The veriﬁer takes advantage of these bounds by not
introducing a binary variables into the MILP formulation if it can determine that a particular ReLU
is stable. Thus, using LP as the tightening algorithm resulted in higher build times, but led to easier
MILP formulations.

Next, the veriﬁer solves the MILP using an off-the-shelf MILP solver. The solver we chose was
the commercial Gurobi Solver, which uses a branch-and-bound method for solving MILPs. The
solver’s speed appeared to depend primarily on the number of binary variables in the MILP (which
corresponds to the number of unstable ReLUs) as well as the total number of variables in the MILP
(which is related to the sparsity of the weight matrices). While these two numbers are strongly
correlated with solve times, some solves would still take a long time despite having few binary

17

Published as a conference paper at ICLR 2019

variables. Thus, understanding what other properties of neural networks correspond to MILPs that
are easy or hard to solve is an important area to explore further.

D.3 VERIFIER DETAILS

We used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019) using the default
settings of the code. We allotted 120 seconds for veriﬁcation of each input datapoint using the
default model build settings. We ran our experiments using the commercial Gurobi Solver (version
7.5.2), and model solves were parallelized over 8 CPU cores with Intel Xeon CPUs @ 2.20GHz
processors. We used computers with 8–32GB of RAM, depending on the size of the model being
veriﬁed. All computers used are part of an OpenStack network.

18

Published as a conference paper at ICLR 2019

E FULL EXPERIMENTAL VERIFICATION RESULTS

Dataset

Epsilon

Training
Method

Test
Set Adversarial
Accuracy

Accuracy

Total
Provable
Upper Adversarial Unstable
Accuracy
Bound

Avg
Avg
Build
Solve
ReLUs Time (s) Time (s)

PGD Veriﬁer

Adversarial Training*
+(cid:96)1-regularization
+Small Weight Pruning
+ReLU Pruning (Control)

MNIST (cid:15) = 0.1

Wong et al. (2018)***

+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

99.17%
99.00%
98.99%
98.94%
98.68%
98.53%
98.95%

98.40%
98.10%
98.08%
98.21%
95.06%

97.75%
97.33%
97.06%
97.54%

64.64%
61.12%
57.83%
61.41%

50.69%
40.45%
46.19%
42.81%

95.04% 96.00%
95.25% 95.98%
95.38% 94.93%
95.12% 94.45%
95.13% 94.38%
94.86% 94.54%
96.58% 95.60%

93.14% 90.71%
93.14% 89.98%
91.68% 88.87%
94.19% 90.40%
-
89.03%

91.64% 83.83%
92.05% 81.70%
89.19% 79.12%
93.25% 83.70%

51.58% 50.23%
49.92% 47.79%
47.03% 45.33%
50.61% 51.00%

31.28% 33.46%
26.78% 22.74%
29.66% 26.07%
28.69% 25.20%

19.00%
82.17%
89.13%
91.58%
94.33%
94.32%
95.60%

86.45%
89.79%
85.54%
89.10%
80.29%

77.99%
80.68%
76.70%
59.60%

45.53%
45.93%
44.44%
41.40%

7.09%
20.27%
18.90%
19.80%

1517.9
505.3
502.7
278.2
101.0
158.3
119.5

2970.43
21.99
11.71
6.43
0.49
0.96
0.27

198.3
108.4
217.2
133.0
-

160.9
101.5
179.0
251.2

360.0
234.3
170.1
196.7

665.9
54.2
277.8
246.5

9.41
1.13
8.50
2.93
-

11.80
2.78
6.43
37.45

21.75
13.50
6.30
29.88

82.91
22.33
33.63
20.14

650.93
79.13
19.30
9.61
4.98
4.82
156.74

7.15
4.43
4.67
171.10
-

5.14
4.34
4.00
166.39

66.42
52.58
47.11
335.97

73.28
38.84
23.66
401.72

Table 7: Full results on natural improvements from Table 1, control networks (which use all of the
natural improvements and ReLU pruning), and “+RS” networks from Tables 2 and 3. While we are
unable to determine the true adversarial accuracy, we provide two upper bounds and a lower bound.
Evaluations of robustness against a strong 40-step PGD adversary (PGD adversarial accuracy) gives
one upper bound, and the veriﬁer itself gives another upper bound because it can also prove that
the network is not robust to perturbations on certain inputs by ﬁnding adversarial examples. The
veriﬁer simultaneously ﬁnds the provable adversarial accuracy, which is a lower bound on the true
adversarial accuracy. The gap between the veriﬁer upper bound and the provable adversarial accu-
racy (veriﬁer lower bound) corresponds to inputs that the veriﬁer times out on. These are inputs that
the veriﬁer can not prove to be robust or not robust in 120 seconds. Build times and solve times
are reported in seconds. Finally, average solve time includes timeouts. In other words, veriﬁcation
solves that time out contribute 120 seconds to the total solve time.
* The “Adversarial Training” network uses a 3600 instead of 120 second timeout and is only veriﬁed
for the ﬁrst 100 images because verifying it took too long.
** The “+RS (Large)” networks are only veriﬁed for the ﬁrst 1000 images because of long build
times.
*** Wong et al. (2018); Dvijotham et al. (2018), and Mirman et al. (2018), which we compare to in
Table 3, do not report results on MNIST, (cid:15) = 0.2 in their papers. We ran the publicly available code
of Wong et al. (2018) on MNIST, (cid:15) = 0.2 to generate these results for comparison.

19

Published as a conference paper at ICLR 2019

F DISCUSSION ON VERIFICATION AND CERTIFICATION

Exact veriﬁcation and certiﬁcation are two related approaches to formally verifying properties of
neural networks, such as adversarial robustness.
In both cases, the end goal is formal veriﬁca-
tion. Certiﬁcation methods, which solve an easier-to-solve relaxation of the exact veriﬁcation prob-
lem, are important developments because exact veriﬁcation previously appeared computationally
intractable for all but the smallest models.

For the case of adversarial robustness, certiﬁcation methods exploit a trade-off between provable
robustness and speed. They can fail to provide certiﬁcates of robustness for some inputs that are
actually robust, but they will either ﬁnd or fail to ﬁnd certiﬁcates of robustness quickly. On the other
hand, exact veriﬁers will always give the correct answer if given enough time, but exact veriﬁers can
sometimes take many hours to formally verify robustness on even a single input.

In general, the process of training a robust neural network and then formally verifying its robustness
happens in two steps.

• Step 1: Training
• Step 2: Veriﬁcation or Certiﬁcation

Most papers on certiﬁcation, including Wong and Kolter (2018); Wong et al. (2018); Dvijotham
et al. (2018); Raghunathan et al. (2018) and Mirman et al. (2018), propose a method for step 2
(the certiﬁcation step), and then propose a training objective in step 1 that is directly related to
their method for step 2. We call this paradigm “co-training.” In Raghunathan et al. (2018), they
found that using their step 2 on a model trained using Wong and Kolter (2018)’s step 1 resulted in
extremely poor provable robustness (less than 10%), and the same was true when using Wong and
Kolter (2018)’s step 2 on a model trained using their step 1.

We focus on MILP-based exact veriﬁcation as our step 2, which encompasses the best current exact
veriﬁcation methods. The advantage of using exact veriﬁcation for step 2 is that it will be accurate,
regardless of what method is used in step 1. The disadvantage of using exact veriﬁcation for step
2 is that it could be extremely slow. For our step 1, we used standard robust adversarial training.
In order to signiﬁcantly speed up exact veriﬁcation as step 2, we proposed techniques that could be
added to step 1 to induce weight sparsity and ReLU stability.

In general, we believe it is important to develop effective methods for step 1, given that step 2 is
exact veriﬁcation. However, ReLU stability can also be beneﬁcial for tightening the relaxation of
certiﬁcation approaches like that of Wong et al. (2018) and Dvijotham et al. (2018), as unstable
ReLUs are the primary cause of the overapproximation that occurs in the relaxation step. Thus, our
techniques for inducing ReLU stability can be useful for certiﬁcation as well.

Finally, in recent literature on veriﬁcation and certiﬁcation, most works have focused on formally
verifying the property of adversarial robustness of neural networks. However, veriﬁcation of other
properties could be useful, and our techniques to induce weight sparsity and ReLU stability would
still be useful for veriﬁcation of other properties for the exact same reasons that they are useful in
the case of adversarial robustness.

20

9
1
0
2
 
r
p
A
 
3
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
0
0
3
0
.
9
0
8
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2019

TRAINING FOR FASTER ADVERSARIAL ROBUSTNESS
VERIFICATION VIA INDUCING RELU STABILITY

Kai Y. Xiao Vincent Tjeng Nur Muhammad (Mahi) Shaﬁullah Aleksander M ˛adry
Massachusetts Institute of Technology
Cambridge, MA 02139
{kaix, vtjeng, nshafiul, madry}@mit.edu

ABSTRACT

We explore the concept of co-design in the context of neural network veriﬁcation.
Speciﬁcally, we aim to train deep neural networks that not only are robust to ad-
versarial perturbations but also whose robustness can be veriﬁed more easily. To
this end, we identify two properties of network models – weight sparsity and so-
called ReLU stability – that turn out to signiﬁcantly impact the complexity of the
corresponding veriﬁcation task. We demonstrate that improving weight sparsity
alone already enables us to turn computationally intractable veriﬁcation problems
into tractable ones. Then, improving ReLU stability leads to an additional 4–13x
speedup in veriﬁcation times. An important feature of our methodology is its “uni-
versality,” in the sense that it can be used with a broad range of training procedures
and veriﬁcation approaches.

1

INTRODUCTION

Deep neural networks (DNNs) have recently achieved widespread success in image classiﬁcation
(Krizhevsky et al., 2012), face and speech recognition (Taigman et al., 2014; Hinton et al., 2012),
and game playing (Silver et al., 2016; 2017). This success motivates their application in a broader
set of domains, including more safety-critical environments. This thrust makes understanding the
reliability and robustness of the underlying models, let alone their resilience to manipulation by
malicious actors, a central question. However, predictions made by machine learning models are
often brittle. A prominent example is the existence of adversarial examples (Szegedy et al., 2014):
imperceptibly modiﬁed inputs that cause state-of-the-art models to misclassify with high conﬁdence.

There has been a long line of work on both generating adversarial examples, called attacks (Carlini
and Wagner, 2017b;a; Athalye et al., 2018a;b; Uesato et al., 2018; Evtimov et al., 2017), and training
models robust to adversarial examples, called defenses (Goodfellow et al., 2015; Papernot et al.,
2016; Madry et al., 2018; Kannan et al., 2018). However, recent research has shown that most
defenses are ineffective (Carlini and Wagner, 2017a; Athalye et al., 2018a; Uesato et al., 2018).
Furthermore, even for defenses such as that of Madry et al. (2018) that have seen empirical success
against many attacks, we are unable to conclude yet with certainty that they are robust to all attacks
that we want these models to be resilient to.

This state of affairs gives rise to the need for veriﬁcation of networks, i.e., the task of formally
proving that no small perturbations of a given input can cause it to be misclassiﬁed by the network
model. Although many exact veriﬁers1 have been designed to solve this problem, the veriﬁcation
process is often intractably slow. For example, when using the Reluplex veriﬁer of Katz et al. (2017),
even verifying a small MNIST network turns out to be computationally infeasible. Thus, addressing
this intractability of exact veriﬁcation is the primary goal of this work.

Our Contributions
Our starting point is the observation that, typically, model training and veriﬁcation are decoupled
and seen as two distinct steps. Even though this separation is natural, it misses a key opportunity:
the ability to align these two stages. Speciﬁcally, applying the principle of co-design during model

1Also sometimes referred to as combinatorial veriﬁers.

1

Published as a conference paper at ICLR 2019

training is possible: training models in a way to encourage them to be simultaneously robust and
easy-to-exactly-verify. This insight is the cornerstone of the techniques we develop in this paper.

In this work, we use the principle of co-design to develop training techniques that give models
that are both robust and easy-to-verify. Our techniques rely on improving two key properties of
networks: weight sparsity and ReLU stability. Speciﬁcally, we ﬁrst show that natural methods for
improving weight sparsity during training, such as (cid:96)1-regularization, give models that can already be
veriﬁed much faster than current methods. This speedup happens because in general, exact veriﬁers
beneﬁt from having fewer variables in their formulations of the veriﬁcation task. For instance, for
exact veriﬁers that rely on linear programming (LP) solvers, sparser weight matrices means fewer
variables in those constraints.

We then focus on the major speed bottleneck of current approaches to exact veriﬁcation of ReLU
networks: the need of exact veriﬁcation methods to “branch,” i.e., consider two possible cases for
each ReLU (ReLU being active or inactive). Branching drastically increases the complexity of
veriﬁcation. Thus, well-optimized veriﬁers will not need to branch on a ReLU if it can determine
that the ReLU is stable, i.e.
that the ReLU will always be active or always be inactive for any
perturbation of an input. This motivates the key goal of the techniques presented in this paper: we
aim to minimize branching by maximizing the number of stable ReLUs. We call this goal ReLU
stability and introduce a regularization technique to induce it.

Our techniques enable us to train weight-sparse and ReLU stable networks for MNIST and CIFAR-
10 that can be veriﬁed signiﬁcantly faster. Speciﬁcally, by combining natural methods for inducing
weight sparsity with a robust adversarial training procedure (cf. Goodfellow et al. (2015)), we are
able to train networks for which almost 90% of inputs can be veriﬁed in an amount of time that
is small2 compared to previous veriﬁcation techniques. Then, by also adding our regularization
technique for inducing ReLU stability, we are able to train models that can be veriﬁed an addi-
tional 4–13x times as fast while maintaining state-of-the-art accuracy on MNIST. Our techniques
show similar improvements for exact veriﬁcation of CIFAR models. In particular, we achieve the
following veriﬁcation speed and provable robustness results for (cid:96)∞ norm-bound adversaries:

Dataset

Epsilon

Provable Adversarial Accuracy Average Solve Time (s)

MNIST

(cid:15) = 0.1
(cid:15) = 0.2
(cid:15) = 0.3

CIFAR

(cid:15) = 2/255
(cid:15) = 8/255

94.33%
89.79%
80.68%

45.93%
20.27%

0.49
1.13
2.78

13.50
22.33

Our network for (cid:15) = 0.1 on MNIST achieves provable adversarial accuracies comparable with the
current best results of Wong et al. (2018) and Dvijotham et al. (2018), and our results for (cid:15) = 0.2 and
(cid:15) = 0.3 achieve the best provable adversarial accuracies yet. To the best of our knowledge, we also
achieve the ﬁrst nontrivial provable adversarial accuracy results using exact veriﬁers for CIFAR-10.

Finally, we design our training techniques with universality as a goal. We focus on improving the
input to the veriﬁcation process, regardless of the veriﬁer we end up using. This is particularly
important because research into network veriﬁcation methods is still in its early stages, and our co-
design methods are compatible with the best current veriﬁers (LP/MILP-based methods) and should
be compatible with any future improvements in veriﬁcation.

Our code is available at https://github.com/MadryLab/relu_stable.

2 BACKGROUND AND RELATED WORK

Exact veriﬁcation of networks has been the subject of many recent works (Katz et al., 2017; Ehlers,
2017; Carlini et al., 2017; Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). To
understand the context of these works, observe that for linear networks, the task of exact veriﬁcation
is relatively simple and can be done by solving a LP. For more complex models, the presence of
nonlinear ReLUs makes veriﬁcation over all perturbations of an input much more challenging. This

2We chose our time budget for veriﬁcation to be 120 seconds per input image.

2

Published as a conference paper at ICLR 2019

is so as ReLUs can be active or inactive depending on the input, which can cause exact veriﬁers
to “branch" and consider these two cases separately. The number of such cases that veriﬁers have
to consider might grow exponentially with the number of ReLUs, so veriﬁcation speed will also
grow exponentially in the worst case. Katz et al. (2017) further illustrated the difﬁculty of exact
veriﬁcation by proving that it is NP-complete. In recent years, formal veriﬁcation methods were
developed to verify networks. Most of these methods use satisﬁability modulo theory (SMT) solvers
(Katz et al., 2017; Ehlers, 2017; Carlini et al., 2017) or LP and Mixed-Integer Linear Programming
(MILP) solvers (Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). However,
all of them are limited by the same issue of scaling poorly with the number of ReLUs in a network,
making them prohibitively slow in practice for even medium-sized models.

One recent approach for dealing with the inefﬁciency of exact veriﬁers is to focus on certiﬁcation
methods3 (Wong and Kolter, 2018; Wong et al., 2018; Dvijotham et al., 2018; Raghunathan et al.,
2018; Mirman et al., 2018; Sinha et al., 2018). In contrast to exact veriﬁcation, these methods do not
solve the veriﬁcation task directly; instead, they rely on solving a relaxation of the veriﬁcation prob-
lem. This relaxation is usually derived by overapproximating the adversarial polytope, or the space
of outputs of a network for a region of possible inputs. These approaches rely on training models in
a speciﬁc manner that makes certiﬁcation of those models easier. As a result, they can often obtain
provable adversarial accuracy results faster. However, certiﬁcation is fundamentally different from
veriﬁcation in two primary ways. First, it solves a relaxation of the original veriﬁcation problem. As
a result, certiﬁcation methods can fail to certify many inputs that are actually robust to perturbations
– only exact veriﬁers, given enough time, can give conclusive answers on robustness for every single
input. Second, certiﬁcation approaches fall under the paradigm of co-training, where a certiﬁcation
method only works well on models speciﬁcally trained for that certiﬁcation step. When used as a
black box on arbitrary models, the certiﬁcation step can yield a high rate of false negatives. For
example, Raghunathan et al. (2018) found that their certiﬁcation step was signiﬁcantly less effective
when used on a model trained using Wong and Kolter (2018)’s training method, and vice versa. In
contrast, we design our methods to be universal. They can be combined with any standard train-
ing procedure for networks and will improve exact veriﬁcation speed for any LP/MILP-based exact
veriﬁer. Our methods can also decrease the amount of overapproximation incurred by certiﬁcation
methods like Wong and Kolter (2018); Dvijotham et al. (2018). Similar to most of the certiﬁcation
methods, our technique can be made to have very little training time overhead.

Finally, subsequent work of Gowal et al. (2018) shows how applying interval bound propagation
during training, combined with MILP-based exact veriﬁcation, can lead to provably robust networks.

3 TRAINING VERIFIABLE NETWORK MODELS

We begin by discussing the task of verifying a network and identify two key properties of networks
that lead to improved veriﬁcation speed: weight sparsity and so-called ReLU stability. We then use
natural regularization methods for inducing weight sparsity as well as a new regularization method
for inducing ReLU stability. Finally, we demonstrate that these methods signiﬁcantly speed up
veriﬁcation while maintaining state-of-the-art accuracy.

3.1 VERIFYING ADVERSARIAL ROBUSTNESS OF NETWORK MODELS

Deep neural network models. Our focus will be on one of the most common architectures for
state-of-the-art models: k-layer fully-connected feed-forward DNN classiﬁers with ReLU non-
linearities4. Such models can be viewed as a function f (·, W, b), where W and b represent the
weight matrices and biases of each layer. For an input x, the output f (x, W, b) of the DNN is

3These works use both “veriﬁcation” and “certiﬁcation” to describe their methods. For clarity, we use “cer-
tiﬁcation” to describe their approaches, while we use “veriﬁcation” to describe exact veriﬁcation approaches.
For a more detailed discussion of the differences, see Appendix F.

4Note that this encompasses common convolutional network architectures because every convolutional layer

can be replaced by an equivalent fully-connected layer.

3

Published as a conference paper at ICLR 2019

deﬁned as:

z0 = x
ˆzi = zi−1Wi + bi
zi = max( ˆzi, 0)

(1)
(2)
(3)
(4)
Here, for each layer i, we let ˆzij denote the jth ReLU pre-activation and let ˆzij(x) denote the value
of ˆzij on an input x. ˆzk−1 is the ﬁnal, output layer with an output unit for each possible label (the
logits). The network will make predictions by selecting the label with the largest logit.

for i = 1, 2, . . . , k − 1
for i = 1, 2, . . . , k − 2

f (x, W, b) = ˆzk−1

Adversarial robustness. For a network to be reliable, it should make predictions that are robust
– that is, it should predict the same output for inputs that are very similar. Speciﬁcally, we want
the DNN classiﬁer’s predictions to be robust to a set Adv(x) of possible adversarial perturbations
of an input x. We focus on (cid:96)∞ norm-bound adversarial perturbations, where Adv(x) = {x(cid:48)
:
||x(cid:48) − x||∞ ≤ (cid:15)} for some constant (cid:15), since it is the most common one considered in adversarial
robustness and veriﬁcation literature (thus, it currently constitutes a canonical benchmark). Even so,
our methods can be applied to other (cid:96)p norms and broader sets of perturbations.
Veriﬁcation of network models. For an input x with correct label y, a perturbed input x(cid:48) can cause
a misclassiﬁcation if it makes the logit of some incorrect label ˆy larger than the logit of y on x(cid:48). We
can thus express the task of ﬁnding an adversarial perturbation as the optimization problem:

f (x(cid:48), W )y − f (x(cid:48), W )ˆy

min
x(cid:48),ˆy
subject to x(cid:48) ∈ Adv(x)

An adversarial perturbation exists if and only if the objective of the optimization problem is negative.

Adversarial accuracies. We deﬁne the true adversarial accuracy of a model to be the fraction of
test set inputs for which the model is robust to all allowed perturbations. By deﬁnition, evaluations
against speciﬁc adversarial attacks like PGD or FGSM provide an upper bound to this accuracy,
while certiﬁcation methods provide lower bounds. Given sufﬁcient time for each input, an exact
veriﬁer can prove robustness to perturbations, or ﬁnd a perturbation where the network makes a
misclassiﬁcation on the input, and thus exactly determine the true adversarial accuracy. This is in
contrast to certiﬁcation methods, which solve a relaxation of the veriﬁcation problem and can not
exactly determine the true adversarial accuracy no matter how much time they have.

However, such exact veriﬁcation may take impractically long for certain inputs, so we instead com-
pute the provable adversarial accuracy, which we deﬁne as the fraction of test set inputs for which
the veriﬁer can prove robustness to perturbations within an allocated time budget (timeout). Simi-
larly to certiﬁable accuracy, this accuracy constitutes a lower bound on the true adversarial accuracy.
A model can thus, e.g., have high true adversarial accuracy and low provable adversarial accuracy if
veriﬁcation of the model is too slow and often fails to complete before timeout.

Also, in our evaluations, we chose to use the MILP exact veriﬁer of Tjeng et al. (2019) when per-
forming experiments, as it is both open source and the fastest veriﬁer we are aware of.

3.2 WEIGHT SPARSITY AND ITS IMPACT ON VERIFICATION SPEED

The ﬁrst property of network models that we want to improve in order to speed up exact veriﬁcation
of those models is weight sparsity. Weight sparsity is important for veriﬁcation speed because many
exact veriﬁers rely on solving LP or MILP systems, which beneﬁt from having fewer variables.
We use two natural regularization methods for improving weight sparsity: (cid:96)1-regularization and
small weight pruning. These techniques signiﬁcantly improve veriﬁcation speed – see Table 1.
Verifying even small MNIST networks is almost completely intractable without them. Speciﬁcally,
the veriﬁer can only prove robustness of an adversarially-trained model on 19% of inputs with a one
hour budget per input, while the veriﬁer can prove robustness of an adversarially-trained model with
(cid:96)1-regularization and small weight pruning on 89.13% of inputs with a 120 second budget per input.

Interestingly, even though adversarial training improves weight sparsity (see Appendix B) it was
still necessary to use (cid:96)1-regularization and small weight pruning. These techniques further promoted
weight sparsity and gave rise to networks that were much easier to verify.

4

Published as a conference paper at ICLR 2019

Dataset

Epsilon

Training
Method

Test Set
Accuracy

Provable Adversarial
Accuracy

Average
Solve Time (s)

MNIST (cid:15) = 0.1

1 Adversarial Training
+(cid:96)1-Regularization
2
+Small Weight Pruning
3
4

+ReLU Pruning (control)

99.17%
99.00%
98.99%
98.94%

19.00%
82.17%
89.13%
91.58%

2970.43
21.99
11.71
6.43

Table 1: Improvement in provable adversarial accuracy and veriﬁcation solve times when incremen-
tally adding natural regularization methods for improving weight sparsity and ReLU stability into
the model training procedure, before veriﬁcation occurs. Each row represents the addition of another
method – for example, Row 3 uses adversarial training, (cid:96)1-regularization, and small weight pruning.
Row 4 adds ReLU pruning (see Appendix A). Row 4 is the control model for MNIST and (cid:15) = 0.1
that we present again in comparisons in Tables 2 and 3. We use a 3600 instead of 120 second timeout
for Row 1 and only veriﬁed the ﬁrst 100 images (out of 10000) because verifying it took too long.

3.3 RELU STABILITY

Next, we target the primary speed bottleneck of exact veriﬁcation: the number of ReLUs the veriﬁer
has to branch on. In our paper, this corresponds to the notion of inducing ReLU stability. Before we
describe our methodology, we formally deﬁne ReLU stability.

(2)).

Given an input x, a set of allowed perturbations Adv(x), and a ReLU, exact veriﬁers may need
to branch based on the possible pre-activations of the ReLU, namely ˆzij(Adv(x)) = {ˆzij(x(cid:48)) :
x(cid:48) ∈ Adv(x)} (cf.
If there exist two perturbations x(cid:48), x(cid:48)(cid:48) in the set Adv(x) such that
sign(ˆzij(x(cid:48))) (cid:54)= sign(ˆzij(x(cid:48)(cid:48))), then the veriﬁer has to consider that for some perturbed inputs the
ReLU is active (zij = ˆzij) and for other perturbed inputs inactive (zij = 0). The more such cases
the veriﬁer faces, the more branches it has to consider, causing the complexity of veriﬁcation to in-
crease exponentially. Intuitively, a model with 1000 ReLUs among which only 100 ReLUs require
branching will likely be much easier to verify than a model with 200 ReLUs that all require branch-
ing. Thus, it is advantageous for the veriﬁer if, on an input x with allowed perturbation set Adv(x),
the number of ReLUs such that

sign(ˆzij(x(cid:48))) = sign(ˆzij(x)) ∀x(cid:48) ∈ Adv(x)
is maximized. We call a ReLU for which (5) holds on an input x a stable ReLU on that input. If (5)
does not hold, then the ReLU is an unstable ReLU.

(5)

Directly computing whether a ReLU is stable on a given input x is difﬁcult because doing so would
involve considering all possible values of ˆzij(Adv(x)). Instead, exact veriﬁers compute an upper
bound ˆuij and a lower bound ˆlij of ˆzij(Adv(x)). If 0 ≤ ˆlij or ˆuij ≤ 0, they can replace the ReLU
with the identity function or the zero function, respectively. Otherwise, if ˆlij < 0 < ˆuij, these
veriﬁers then determine that they need to “branch” on that ReLU. Thus, we can rephrase (5) as

sign(ˆuij) = sign(ˆlij)

(6)

We will discuss methods for determining these upper and lower bounds ˆuij, ˆlij in Section 3.3.2.

3.3.1 A REGULARIZATION TECHNIQUE FOR INDUCING RELU STABILITY: RS LOSS

As we see from equation (6), a function that would indicate exactly when a ReLU is stable is
F ∗(ˆuij, ˆlij) = sign(ˆuij) · sign(ˆlij). Thus, it would be natural to use this function as a regular-
izer. However, this function is non-differentiable and if used in training a model, would provide no
useful gradients during back-propagation. Thus, we use the following smooth approximation of F ∗
(see Fig. 1) which provides the desired gradients:

F (ˆuij, ˆlij) = − tanh(1 + ˆuij · ˆlij)

We call the corresponding objective RS Loss, and show in Fig. 2a that using this loss function as
a regularizer effectively decreases the number of unstable ReLUs. RS Loss thus encourages ReLU
stability, which, in turn, speeds up exact veriﬁcation - see Fig. 2b.

5

Published as a conference paper at ICLR 2019

Figure 1: Plot and contour plot of the function F (x, y) = − tanh(1 + x · y)

3.3.2 ESTIMATING RELU UPPER AND LOWER BOUNDS ON ACTIVATIONS

A key aspect of using RS Loss is determining the upper and lower bounds ˆuij, ˆlij for each ReLU (cf.
(6)). The bounds for the inputs z0 (cf. (1)) are simple – for the input x, we know x − (cid:15) ≤ z0 ≤ x + (cid:15),
so ˆu0 = x − (cid:15), ˆl0 = x + (cid:15). For all subsequent zij, we estimate bounds using either the naive interval
arithmetic (IA) approach described in Tjeng et al. (2019) or an improved version of it. The improved
version is a tighter estimate but uses more memory and training time, and thus is most effective on
smaller networks. We present the details of naive IA and improved IA in Appendix C.

Interval arithmetic approaches can be implemented relatively efﬁciently and work well with back-
propagation because they only involve matrix multiplications. This contrasts with how exact veri-
ﬁers compute these bounds, which usually involves solving LPs or MILPs. Interval arithmetic also
overestimates the number of unstable ReLUs. This means that minimizing unstable ReLUs based
on IA bounds will provide an upper bound on the number of unstable ReLUs determined by exact
veriﬁers. In particular, IA will properly penalize every unstable ReLU.

Improved IA performs well in practice, overestimating the number of unstable ReLUs by less than
0.4% in the ﬁrst 2 layers of MNIST models and by less than 36.8% (compared to 128.5% for naive
IA) in the 3rd layer. Full experimental results are available in Table 4 of Appendix C.3.

3.3.3

IMPACT OF RELU STABILITY IMPROVEMENTS ON VERIFICATION SPEED

We provide experimental evidence that RS Loss regularization improves ReLU stability and speeds
up average veriﬁcation times by more than an order of magnitude in Fig. 2b. To isolate the effect of
RS Loss, we compare MNIST models trained in exactly the same way other than the weight on RS
Loss. When comparing a network trained with a RS Loss weight of 5e−4 to a network with a RS
Loss weight of 0, the former has just 16% as many unstable ReLUs and can be veriﬁed 65x faster.
The caveat here is that the former has 1.00% lower test set accuracy.

We also compare veriﬁcation speed with and without RS Loss on MNIST networks for different
values of (cid:15) (0.1, 0.2, and 0.3) in Fig. 2c. We choose RS Loss weights that cause almost no test set
accuracy loss (less than 0.50% - See Table 3) in these cases, and we still observe a 4–13x speedup
from RS Loss. For CIFAR, RS Loss gives a smaller speedup of 1.6–3.7x (See Appendix E).

3.3.4

IMPACT OF RELU STABILITY IMPROVEMENTS ON PROVABLE ADVERSARIAL
ACCURACY

As the weight on the RS Loss used in training a model increases, the ReLU stability of the model
will improve, speeding up veriﬁcation and likely improving provable adversarial accuracy. However,
like most regularization methods, placing too much weight on RS Loss can decrease the model ca-
pacity, potentially lowering both the true adversarial accuracy and the provable adversarial accuracy.
Therefore, it is important to choose the weight on RS Loss carefully to obtain both high provable
adversarial accuracy and faster veriﬁcation speeds.

To show the effectiveness of RS Loss in improving provable adversarial accuracy, we train two
networks for each dataset and each value of (cid:15). One is a “control” network that uses all of the natural
improvements for inducing both weight sparsity ((cid:96)1-regularization and small weight pruning) and
ReLU stability (ReLU pruning - see Appendix A). The second is a “+RS” network that uses RS

6

Published as a conference paper at ICLR 2019

(a)

(b)

(c)

Figure 2: (a) Average number of unstable ReLUs by layer and (b) average veriﬁcation solve times
of 6 networks trained with different weights on RS Loss for MNIST and (cid:15) = 0.1 . Averages are
taken over all 10000 MNIST test set inputs. Both metrics improve signiﬁcantly with increasing
RS Loss weight. An RS Loss weight of 0 corresponds to the control network, while an RS Loss
weight of 0.00012 corresponds to the “+RS” network for MNIST and (cid:15) = 0.1 in Tables 2 and 3. (c)
Improvement in the average time taken by a veriﬁer to solve the veriﬁcation problem after adding
RS Loss to the training procedure, for different (cid:15) on MNIST. The weight on RS Loss was chosen so
that the “+RS” models have test set accuracies within 0.50% of the control models.

Loss in addition to all of the same natural improvements. This lets us isolate the incremental effect
of adding RS Loss to the training procedure.

In addition to attaining a 4–13x speedup in MNIST veriﬁcation times (see Fig. 2c), we achieve
higher provable adversarial accuracy in every single setting when using RS Loss. This is especially
notable for the hardest veriﬁcation problem we tackle – proving robustness to perturbations with (cid:96)∞
norm-bound 8/255 on CIFAR-10 – where adding RS Loss nearly triples the provable adversarial
accuracy from 7.09% to 20.27%. This improvement is primarily due to veriﬁcation speedup induced
by RS Loss, which allows the veriﬁer to ﬁnish proving robustness for far more inputs within the 120
second time limit. These results are shown in Table 2.

Table 2: Provable Adversarial Accuracies for the control and “+RS” networks in each setting.

MNIST, (cid:15) = 0.1 MNIST, (cid:15) = 0.2 MNIST, (cid:15) = 0.3 CIFAR, (cid:15) = 2/255 CIFAR, (cid:15) = 8/255

Control
+RS

91.58
94.33

86.45
89.79

77.99
80.68

45.53
45.93

7.09
20.27

4 EXPERIMENTS

4.1 EXPERIMENTS ON MNIST AND CIFAR

In addition to the experimental results already presented, we compare our control and “+RS” net-
works with the best available results presented in the state-of-the-art certiﬁable defenses of Wong
et al. (2018), Dvijotham et al. (2018), and Mirman et al. (2018) in Table 3. We compare their test
set accuracy, PGD adversarial accuracy (an evaluation of robustness against a strong 40-step PGD
adversarial attack), and provable adversarial accuracy. Additionally, to show that our method can
scale to larger architectures, we train and verify a “+RS (Large)” network for each dataset and (cid:15).

In terms of provable adversarial accuracies, on MNIST, our results are signiﬁcantly better than those
of Wong et al. (2018) for larger perturbations of (cid:15) = 0.3, and comparable for (cid:15) = 0.1. On CIFAR-
10, our method is slightly less effective, perhaps indicating that more unstable ReLUs are necessary
to properly learn a robust CIFAR classiﬁer. We also experienced many more instances of the veriﬁer
reaching its allotted 120 second time limit on CIFAR, especially for the less ReLU stable control
networks. Full experimental details for each model in Tables 1, 2, and 3, including a breakdown of
veriﬁcation solve results (how many images did the veriﬁer: A. prove robust B. ﬁnd an adversarial
example for C. time out on), are available in Appendix E.

7

Published as a conference paper at ICLR 2019

Table 3: Comparison of test set, PGD adversarial, and provable adversarial accuracy of networks
trained with and without RS Loss. We also provide the best available certiﬁable adversarial and
PGD adversarial accuracy of any single models from Wong et al. (2018), Dvijotham et al. (2018),
and Mirman et al. (2018) for comparison, and highlight the best provable accuracy for each (cid:15).
* The provable adversarial accuracy for “+RS (Large)” is only computed for the ﬁrst 1000 images
because the veriﬁer performs more slowly on larger models.
** Dvijotham et al. (2018); Mirman et al. (2018) use a slightly smaller (cid:15) = 0.03 = 7.65/255.
† Mirman et al. (2018) computes results over 500 images instead of all 10000.
†† Mirman et al. (2018) uses a slightly smaller (cid:15) = 0.007 = 1.785/255.

Dataset

Epsilon

Test Set
Accuracy

PGD Adversarial

Provable/Certiﬁable
Accuracy Adversarial Accuracy

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

Training
Method

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†,††

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.**
Mirman et al.†, **

98.94%
98.68%
98.95%

98.92%
98.80%
99.00%

98.40%
98.10%
98.21%

97.75%
97.33%
97.54%

85.13%
96.60%

64.64%
61.12%
61.41%

68.28%
62.00%

50.69%
40.45%
42.81%

28.67%
48.64%
54.20%

95.12%
95.13%
96.58%

-
97.13%
97.60%

93.14%
93.14%
94.19%

91.64%
92.05%
93.25%

-
93.80%

51.58%
49.92%
50.61%

-
54.60%

31.28%
26.78%
28.69%

-
32.72%
40.00%

91.58%
94.33%
95.60%

96.33%
95.56%
96.60%

86.45%
89.79%
89.10%

77.99%
80.68%
59.60%

56.90%
82.00%

45.53%
45.93%
41.40%

53.89%
52.20%

7.09%
20.27%
19.80%

21.78%
26.67%
35.20%

4.2 EXPERIMENTAL METHODS AND DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. For each setting of dataset
(MNIST or CIFAR) and (cid:15), we ﬁnd a suitable weight on RS Loss via line search (See Table 6 in
Appendix D). The same weight is used for each ReLU. During training, we used improved IA for
ReLU bound estimation for “+RS” models and use naive IA for “+RS (Large)” models because of
memory constraints. For ease of comparison, we trained our networks using the same convolutional
DNN architecture as in Wong et al. (2018). This architecture uses two 2x2 strided convolutions with
16 and 32 ﬁlters, followed by a 100 hidden unit fully connected layer. For the larger architecture,
we also use the same “large” architecture as in Wong et al. (2018). It has 4 convolutional layers with
32, 32, 64, and 64 ﬁlters, followed by 2 fully connected layers with 512 hidden units each.

For veriﬁcation, we used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019).
Model solves were parallelized over 8 CPU cores. When verifying an image, the veriﬁer of Tjeng
et al. (2019) ﬁrst builds a model, and second, solves the veriﬁcation problem (See Appendix D.2 for
details). We focus on reporting solve times because that is directly related to the task of veriﬁcation
itself. All build times for the control and “+RS” models on MNIST that we presented were between
4 and 10 seconds, and full results on build times are also presented in Appendix E.

Additional details on our experimental setup (e.g. hyperparameters) can be found in Appendix D.

8

Published as a conference paper at ICLR 2019

5 CONCLUSION

In this paper, we use the principle of co-design to develop training methods that emphasize veri-
ﬁcation as a goal, and we show that they make verifying the trained model much faster. We ﬁrst
demonstrate that natural regularization methods already make the exact veriﬁcation problem sig-
niﬁcantly more tractable. Subsequently, we introduce the notion of ReLU stability for networks,
present a method that improves a network’s ReLU stability, and show that this improvement makes
veriﬁcation an additional 4–13x faster. Our method is universal, as it can be added to any training
procedure and should speed up any exact veriﬁcation procedure, especially MILP-based methods.

Prior to our work, exact veriﬁcation seemed intractable for all but the smallest models. Thus, our
work shows progress toward reliable models that can be proven to be robust, and our techniques can
help scale veriﬁcation to even larger networks.

Many of our methods appear to compress our networks into more compact, simpler forms. We
hypothesize that the reason that regularization methods like RS Loss can still achieve very high
accuracy is that most models are overparametrized in the ﬁrst place. There exist clear parallels
between our methods and techniques in model compression (Han et al., 2016; Cheng et al., 2017b) –
therefore, we believe that drawing upon additional techniques from model compression can further
improve the ease-of-veriﬁcation of networks. We also expect that there exist objectives other than
weight sparsity and ReLU stability that are important for veriﬁcation speed. If so, further exploring
the principle of co-design for those objectives is an interesting future direction.

ACKNOWLEDGEMENTS

This work was supported by the NSF Graduate Research Fellowship under Grant No. 1122374,
by the NSF grants CCF-1553428 and CNS-1815221, and by Lockheed Martin Corporation under
award number RPP2016-002. We would like to thank Krishnamurthy Dvijotham, Ludwig Schmidt,
Michael Sun, Dimitris Tsipras, and Jonathan Uesato for helpful discussions.

9

Published as a conference paper at ICLR 2019

REFERENCES

Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense
In International Conference on

of security: Circumventing defenses to adversarial examples.
Machine Learning (ICML), 2018a.

Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. In International Conference on Machine Learning (ICML), pages 284–293, 2018b.

Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
In Proceedings of ACM Workshop on Artiﬁcial Intelligence and Security

detection methods.
(AISec), 2017a.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks.

In

Security and Privacy (SP), 2017 IEEE Symposium on, 2017b.

Nicholas Carlini, Guy Katz, Clark Barrett, and David L. Dill. Ground-truth adversarial examples.

2017.

networks. 2017a.

Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artiﬁcial neural

Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration

for deep neural networks. 2017b.

Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training veriﬁed learners with learned ver-
iﬁers. 2018.

Rüdiger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In Auto-

mated Technology for Veriﬁcation and Analysis, 2017.

Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir
Rahmati, and Dawn Song. Robust physical-world attacks on machine learning models. 2017.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. In International Conference on Learning Representations (ICLR), 2015.

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. 2018.

Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. In International Conference on Learning
Representations (ICLR), 2016.

Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, An-
drew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep
neural networks for acoustic modeling in speech recognition: The shared views of four research
groups. In IEEE Signal Processing Magazine, 2012.

Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. 2018.

Guy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer. Reluplex: An efﬁcient

smt solver for verifying deep neural networks. 2017.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations (ICLR), 2015.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012.

Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward relu

neural networks. 2017.

10

Published as a conference paper at ICLR 2019

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations (ICLR), 2018.

Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning (ICML), 2018.

Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation
as a defense to adversarial perturbations against deep neural networks. In Proceedings of IEEE
Symposium on Security and Privacy (SP), 2016.

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial exam-

ples. In International Conference on Learning Representations (ICLR), 2018.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 2017.

Aman Sinha, Hongseok Namkoong, and John Duchi. Certiﬁable distributional robustness with
principled adversarial training. In International Conference on Learning Representations (ICLR),
2018.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations (ICLR), 2014.

Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap
In Computer Vision and Pattern Recognition

to human-level performance in face veriﬁcation.
(CVPR), 2014.

Robert Tibshirani. Regression shrinkage and selection via the lasso. In Journal of the Royal Statis-

tical Society, Series B, 1994.

Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In International Conference on Learning Representations (ICLR), 2019.

Jonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial
risk and the dangers of evaluating against weak attacks. In International Conference on Machine
Learning (ICML), 2018.

Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer

adversarial polytope. In International Conference on Machine Learning (ICML), 2018.

Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and Zico Kolter. Scaling provable adversarial

defenses. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

11

Published as a conference paper at ICLR 2019

APPENDIX

A NATURAL IMPROVEMENTS

A.1 NATURAL REGULARIZATION FOR INDUCING WEIGHT SPARSITY

All of the control and “+RS” networks in our paper contain natural improvements that improve
weight sparsity, which reduce the number of variables in the LPs solved by the veriﬁer. We ob-
served that the two techniques we used for weight sparsity ((cid:96)1-regularization and small weight prun-
ing) don’t hurt test set accuracy but they dramatically improve provable adversarial accuracy and
veriﬁcation speed.

1. (cid:96)1-regularization: We use a weight of 2e−5 on MNIST and a weight of 1e−5 on CIFAR.
We chose these weights via line search by ﬁnding the highest weight that would not hurt
test set accuracy.

2. Small weight pruning: Zeroing out weights in a network that are very close to zero. We

choose to prune weights less than 1e−3.

A.2 A BASIC IMPROVEMENT FOR INDUCING RELU STABILITY: RELU PRUNING

We also use a basic idea to improve ReLU stability, which we call ReLU pruning. The main idea is
to prune away ReLUs that are not necessary.

We use a heuristic to test whether a ReLU in a network is necessary. Our heuristic is to count how
many training inputs cause the ReLU to be active or inactive. If a ReLU is active (the pre-activation
satisﬁes ˆzij(x) > 0) for every input image in the training set, then we can replace that ReLU with
the identity function and the network would behave in exactly the same way for all of those images.
Similarly, if a ReLU is inactive (ˆzij(x) < 0) for every training image, that ReLU can be replaced by
the zero function.

Extending this idea further, we expect that ReLUs that are rarely used can also be removed without
signiﬁcantly changing the behavior of the network. If only a small fraction (say, 10%) of the input
images activate a ReLU, then replacing the ReLU with the zero function will only slightly change the
network’s behavior and will not affect the accuracy too much. We provide experimental evidence of
this phenomenon on an adversarially trained ((cid:15) = 0.1) MNIST model. Conservatively, we decided
that pruning away ReLUs that are active on less than 10% of the training set or inactive on less than
10% of the training set was reasonable.

Figure 3: Removing some ReLUs does not hurt test set accuracy or accuracy against a PGD adver-
sary

12

Published as a conference paper at ICLR 2019

B ADVERSARIAL TRAINING AND WEIGHT SPARSITY

It is worth noticing that adversarial training against (cid:96)∞ norm-bound adversaries alone already makes
networks easier to verify by implicitly improving weight sparsity. Indeed, this can be shown clearly
in the case of linear networks. Recall that a linear network can be expressed as f (x) = W x + b.
Thus, an (cid:96)∞ norm-bound perturbation of the input x will produce the output

f (x(cid:48)) = x(cid:48)W + b

= xW + b + (x(cid:48) − x)W
≤ f (x) + (cid:15)||W ||1

where the last inequality is just Hölder’s inequality.
In order to limit the adversary’s ability to
perturb the output, adversarial training needs to minimize the ||W ||1 term, which is equivalent to (cid:96)1-
regularization and is known to promote weight sparsity (Tibshirani, 1994). Relatedly, Goodfellow
et al. (2015) already pointed out that adversarial attacks against linear networks will be stronger
when the (cid:96)1-norm of the weight matrices is higher.

Even in the case of nonlinear networks, adversarial training has experimentally been shown to im-
prove weight sparsity. For example, models trained according to Madry et al. (2018) and Wong and
Kolter (2018) often learn many weight-sparse layers, and we observed similar trends in the mod-
els we trained. However, it is important to note that while adversarial training alone does improve
weight sparsity, it is not sufﬁcient by itself for efﬁcient exact veriﬁcation. Additional regularization
like (cid:96)1-regularization and small weight pruning further promotes weight sparsity and gives rise to
networks that are much easier to verify.

13

Published as a conference paper at ICLR 2019

C INTERVAL ARITHMETIC

C.1 NAIVE INTERVAL ARITHMETIC

Naive IA determines upper and lower bounds for a layer based solely on the upper and lower bounds
of the previous layer.
Deﬁne W + = max(W, 0), W − = min(W, 0), u = max(ˆu, 0), and l = max(ˆl, 0). Then the bounds
on the pre-activations of layer i can be computed as follows:
i + li−1W −
i + ui−1W −

ˆui = ui−1W +
ˆli = li−1W +

i + bi
i + bi

(8)

(7)

As noted in Tjeng et al. (2019) and also Dvijotham et al. (2018), this method is efﬁcient but can lead
to relatively conservative bounds for deeper networks.

C.2

IMPROVED INTERVAL ARITHMETIC

We improve upon naive IA by exploiting ReLUs that we can determine to always be active. This
allows us to cancel symbols that are equivalent that come from earlier layers of a network.

We will use a basic example of a neural network with one hidden layer to illustrate this idea. Suppose
that we have the scalar input z0 with l0 = 0, u0 = 1, and the network has the following weights and
biases:

W1 = [1 − 1] ,

b1 = [2

2] , W2 =

b2 = 0

(cid:21)
(cid:20)1
1

,

Naive IA for the ﬁrst layer gives ˆl1 = l1 = [2
2], and applying naive IA to
the output ˆz2 gives ˆl2 = 3, ˆu2 = 5. However, because ˆl1 > 0, we know that the two ReLUs in the
hidden layer are always active and thus equivalent to the identity function. Then, the output is

1], ˆu1 = u1 = [3

ˆz2 = z11 + z12 = ˆz11 + ˆz12 = (z0 + 2) + (−z0 + 2) = 4

Thus, we can obtain the tighter bounds ˆl2 = ˆu2 = 4, as we are able to cancel out the z0 terms.
We can write this improved version of IA as follows. First, letting Wk denote row k of matrix W ,
we can deﬁne the “active” part of W as the matrix WA, where

(WA)k =

(cid:40)

Wk
0

if ˆli−1 > 0
if ˆli−1 ≤ 0

Deﬁne the “non-active” part of W as

WN = W − WA
Then, using the same deﬁnitions for the notation W +, W −, u, l as before, we can write down the
following improved version of IA which uses information from the previous 2 layers.

ˆui = ui−1Wi

+
N + li−1Wi

−
N + bi
+ ui−2(Wi−1WiA)+ + li−2(Wi−1WiA)− + bi−1WiA
−
N + bi
+ li−2(Wi−1WiA)+ + ui−2(Wi−1WiA)− + bi−1WiA
We are forced to to use li−1,j and ui−1,j if we can not determine whether or not the ReLU corre-
sponding to the activation zi−1,j is active, but we use li−2 and ui−2 whenever possible.

+
N + ui−1Wi

ˆli = li−1Wi

We now deﬁne some additional notation to help us extend this method to any number of layers. We
now seek to deﬁne fn, which is a function which takes in four sequences of length n – upper bounds,
lower bounds, weights, and biases – and outputs the current layer’s upper and lower bounds.

What we have derived so far from (7) and (8) is the following

f1(ui−1, li−1, Wi, bi) = (ui−1W +

i + li−1W −

i + bi, li−1W +

i + ui−1W −

i + bi)

14

Published as a conference paper at ICLR 2019

Let u denote a sequence of upper bounds. Let uz denote element z of the sequence, and let u[z:]
denote the sequence without the ﬁrst z elements. Deﬁne notation for l, W, and b similarly.

Then, using the fact that WN Z = (W Z)N and WAZ = (W Z)A, we can show that the following
recurrence holds:

fn+1(u, l, W, b) = f1(u1, l1, W1N , b1)

+ fn(u[1:], l[1:], (W2W1A, W[2:]), (b2W1A, b[2:]))

(9)

Let u(x,y) denote the sequence (ux, ux−1, · · · , uy), and deﬁne l(x,y), W(x,y), and b(x,y) similarly.
Then, if we want to compute the bounds on layer k using all information from the previous k layers,
we simply have to compute fk(u(k−1,0), l(k−1,0), W(k,1), b(k,1)).

From the recurrence 9, we see that using information from all previous layers to compute bounds for
layer k takes O(k) matrix-matrix multiplications. Thus, using information from all previous layers
to compute bounds for all layers of a d layer neural network only involves O(d2) additional matrix
multiplications, which is still reasonable for most DNNs. This method is still relatively efﬁcient
because it only involves matrix multiplications – however, needing to perform matrix-matrix multi-
plications as opposed to just matrix-vector multiplications results in a slowdown and higher memory
usage when compared to naive IA. We believe the improvement in the estimate of ReLU upper and
lower bounds is worth the time trade-off for most networks.

C.3 EXPERIMENTAL RESULTS ON IMPROVED IA AND NAIVE IA

In Table 4, we show empirical evidence that the number of unstable ReLUs in each layer of a MNIST
network, as estimated by improved IA, tracks the number of unstable ReLUs determined by the exact
veriﬁer quite well. We also present estimates determined via naive IA for comparison.

Dataset

Epsilon Training Estimation

Method Method

Unstable ReLUs Unstable ReLUs Unstable ReLUs
in 1st Layer

in 2nd Layer

in 3rd Layer

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

Control

+RS

Control

+RS

Control

+RS

Exact
61.14
Improved IA 61.14
61.14
Naive IA

21.64
Exact
Improved IA 21.64
21.64
Naive IA

Exact
17.47
Improved IA 17.47
17.47
Naive IA

Exact
29.91
Improved IA 29.91
29.91
Naive IA

Exact
36.76
Improved IA 36.76
36.76
Naive IA

24.43
Exact
Improved IA 24.43
24.43
Naive IA

185.30
185.96 (+0.4%)
188.44 (+1.7%)

31.73
43.40 (+36.8%)
69.96 (+120.5%)

64.73
64.80 (+0.1%)
65.34 (+0.9%)

14.67
18.97 (+29.4%)
33.51 (+128.5%)

37.92
48.88 (+28.9%)
69.75 (+84.0%)

24.05
28.40 (+18.1%)
40.47 (+68.3%)

28.64
31.19 (+8.9%)
32.13 (+12.2%)

83.42
83.44 (+0.02%)
83.52 (+0.1%)

40.74
46.00 (+12.9%)
48.27 (+18.5%)

142.95
142.95
142.95

54.47
54.47
54.47

48.47
48.47
48.47

Table 4: Comparison between the average number of unstable ReLUs as found by the exact veriﬁer
of Tjeng et al. (2019) and the estimated average number of unstable ReLUs found by improved IA
and naive IA. We compare these estimation methods on the control and “+RS” networks for MNIST
that we described in Section 3.3.4

15

Published as a conference paper at ICLR 2019

C.4 ON THE CONSERVATIVE NATURE OF IA BOUNDS

The upper and lower bounds we compute on each ReLU via either naive IA or improved IA are
conservative. Thus, every unstable ReLU will always be correctly labeled as unstable, while stable
ReLUs can be labeled as either stable or unstable. Importantly, every unstable ReLU, as estimated
by IA bounds, is correctly labeled and penalized by RS Loss. The trade-off is that stable ReLUs
mislabeled as unstable will also be penalized, which can be an unnecessary regularization of the
model.

In Table 5 we show empirically that we can achieve the following two objectives at once when using
RS Loss combined with IA bounds.

1. Reduce the number of ReLUs labeled as unstable by IA, which is an upper bound on the

true number of unstable ReLUs as determined by the exact veriﬁer.

2. Achieve similar test set accuracy and PGD adversarial accuracy as a model trained without

RS Loss.

Dataset

Epsilon Training
Method

Estimation
Method

Total Labeled
Unstable ReLUs Accuracy

Test Set

PGD Adversarial
Accuracy

MNIST (cid:15) = 0.1

Control
+RS

Improved IA 290.5
Improved IA 105.4

Control (Large) Naive IA
Naive IA
+RS (Large)

835.8
150.3

98.94%
98.68% (-0.26%)

95.12%
95.13% (+0.01%)

99.04%
98.95% (-0.09%)

96.32%
96.58% (+0.26%)

Table 5: The addition of RS Loss results in far fewer ReLUs labeled as unstable for both 3-layer and
6-layer (Large) networks. The decrease in test set accuracy as a result of this regularization is small.

Even though IA bounds are conservative, these results show that it is still possible to decrease the
number of ReLUs labeled as unstable by IA without signiﬁcantly degrading test set accuracy. When
comparing the Control and “+RS” networks for MNIST and (cid:15) = 0.1, adding RS Loss decreased
the average number of ReLUs labeled as unstable (using bounds from Improved IA) from 290.5 to
105.4 with just a 0.26% loss in test set accuracy. The same trend held for deeper, 6-layer networks,
even when the estimation method for upper and lower bounds was the more conservative Naive IA.

16

Published as a conference paper at ICLR 2019

D FULL EXPERIMENTAL SETUP

D.1 NETWORK TRAINING DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. Following the prior
examples of Wong and Kolter (2018) and Dvijotham et al. (2018), we introduced a small tweak
where we increased the adversary strength linearly from 0.01 to (cid:15) over ﬁrst half of training and kept
it at (cid:15) for the second half. We used this training schedule to improve convergence of the training
process.

For MNIST, we trained for 70 epochs using the Adam optimizer (Kingma and Ba, 2015) with a
learning rate of 1e−4 and a batch size of 32. For CIFAR, we trained for 250 epochs using the Adam
optimizer with a learning rate of 1e−4. When using naive IA, we used a batch size of 128, and
when using improved IA, we used a batch size of 16. We used a smaller batch size in the latter
case because improved IA incurs high RAM usage during training. To speed up training on CIFAR,
we only added in RS Loss regularization in the last 20% of the training process. Using this same
sped-up training method on MNIST did not signiﬁcantly affect the results.

Dataset

Epsilon

(cid:96)1 weight RS Loss weight

MNIST 0.1
MNIST 0.2
MNIST 0.3
CIFAR 2/255
CIFAR 8/255

2e−5
2e−5
2e−5
1e−5
1e−5

12e−5
1e−4
12e−5
1e−3
2e−3

Table 6: Weights chosen using line search for (cid:96)1 regularization and RS Loss in each setting

For each setting, we ﬁnd a suitable weight on RS Loss via line search. The same weight is used for
each ReLU. The ﬁve weights we chose are displayed above in Table 6, along with weights chosen
for (cid:96)1-regularization.

We also train “+RS” models using naive IA to show that our technique for inducing ReLU stability
can work while having small training time overhead – full details on “+RS (Naive IA)” networks are
in Appendix E.

D.2 VERIFIER OVERVIEW

The MILP-based exact veriﬁer of Tjeng et al. (2019), which we use, proceeds in two steps for every
input. They are the model-build step and the solve step.

First, the veriﬁer builds a MILP model based on the neural network and the input. In particular, the
veriﬁer will compute upper and lower bounds on each ReLU using a speciﬁc bound computation al-
gorithm. We chose the default bound computation algorithm in the code, which uses LP to compute
bounds. LP bounds are tighter than the bounds computed via IA, which is another option available in
the veriﬁer. The model-build step’s speed appeared to depend primarily on the tightening algorithm
(IA was faster than LP) and the number of variables in the MILP (which, in turn, depends on the
sparsity of the weights of the neural network). The veriﬁer takes advantage of these bounds by not
introducing a binary variables into the MILP formulation if it can determine that a particular ReLU
is stable. Thus, using LP as the tightening algorithm resulted in higher build times, but led to easier
MILP formulations.

Next, the veriﬁer solves the MILP using an off-the-shelf MILP solver. The solver we chose was
the commercial Gurobi Solver, which uses a branch-and-bound method for solving MILPs. The
solver’s speed appeared to depend primarily on the number of binary variables in the MILP (which
corresponds to the number of unstable ReLUs) as well as the total number of variables in the MILP
(which is related to the sparsity of the weight matrices). While these two numbers are strongly
correlated with solve times, some solves would still take a long time despite having few binary

17

Published as a conference paper at ICLR 2019

variables. Thus, understanding what other properties of neural networks correspond to MILPs that
are easy or hard to solve is an important area to explore further.

D.3 VERIFIER DETAILS

We used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019) using the default
settings of the code. We allotted 120 seconds for veriﬁcation of each input datapoint using the
default model build settings. We ran our experiments using the commercial Gurobi Solver (version
7.5.2), and model solves were parallelized over 8 CPU cores with Intel Xeon CPUs @ 2.20GHz
processors. We used computers with 8–32GB of RAM, depending on the size of the model being
veriﬁed. All computers used are part of an OpenStack network.

18

Published as a conference paper at ICLR 2019

E FULL EXPERIMENTAL VERIFICATION RESULTS

Dataset

Epsilon

Training
Method

Test
Set Adversarial
Accuracy

Accuracy

Total
Provable
Upper Adversarial Unstable
Accuracy
Bound

Avg
Avg
Build
Solve
ReLUs Time (s) Time (s)

PGD Veriﬁer

Adversarial Training*
+(cid:96)1-regularization
+Small Weight Pruning
+ReLU Pruning (Control)

MNIST (cid:15) = 0.1

Wong et al. (2018)***

+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

99.17%
99.00%
98.99%
98.94%
98.68%
98.53%
98.95%

98.40%
98.10%
98.08%
98.21%
95.06%

97.75%
97.33%
97.06%
97.54%

64.64%
61.12%
57.83%
61.41%

50.69%
40.45%
46.19%
42.81%

95.04% 96.00%
95.25% 95.98%
95.38% 94.93%
95.12% 94.45%
95.13% 94.38%
94.86% 94.54%
96.58% 95.60%

93.14% 90.71%
93.14% 89.98%
91.68% 88.87%
94.19% 90.40%
-
89.03%

91.64% 83.83%
92.05% 81.70%
89.19% 79.12%
93.25% 83.70%

51.58% 50.23%
49.92% 47.79%
47.03% 45.33%
50.61% 51.00%

31.28% 33.46%
26.78% 22.74%
29.66% 26.07%
28.69% 25.20%

19.00%
82.17%
89.13%
91.58%
94.33%
94.32%
95.60%

86.45%
89.79%
85.54%
89.10%
80.29%

77.99%
80.68%
76.70%
59.60%

45.53%
45.93%
44.44%
41.40%

7.09%
20.27%
18.90%
19.80%

1517.9
505.3
502.7
278.2
101.0
158.3
119.5

2970.43
21.99
11.71
6.43
0.49
0.96
0.27

198.3
108.4
217.2
133.0
-

160.9
101.5
179.0
251.2

360.0
234.3
170.1
196.7

665.9
54.2
277.8
246.5

9.41
1.13
8.50
2.93
-

11.80
2.78
6.43
37.45

21.75
13.50
6.30
29.88

82.91
22.33
33.63
20.14

650.93
79.13
19.30
9.61
4.98
4.82
156.74

7.15
4.43
4.67
171.10
-

5.14
4.34
4.00
166.39

66.42
52.58
47.11
335.97

73.28
38.84
23.66
401.72

Table 7: Full results on natural improvements from Table 1, control networks (which use all of the
natural improvements and ReLU pruning), and “+RS” networks from Tables 2 and 3. While we are
unable to determine the true adversarial accuracy, we provide two upper bounds and a lower bound.
Evaluations of robustness against a strong 40-step PGD adversary (PGD adversarial accuracy) gives
one upper bound, and the veriﬁer itself gives another upper bound because it can also prove that
the network is not robust to perturbations on certain inputs by ﬁnding adversarial examples. The
veriﬁer simultaneously ﬁnds the provable adversarial accuracy, which is a lower bound on the true
adversarial accuracy. The gap between the veriﬁer upper bound and the provable adversarial accu-
racy (veriﬁer lower bound) corresponds to inputs that the veriﬁer times out on. These are inputs that
the veriﬁer can not prove to be robust or not robust in 120 seconds. Build times and solve times
are reported in seconds. Finally, average solve time includes timeouts. In other words, veriﬁcation
solves that time out contribute 120 seconds to the total solve time.
* The “Adversarial Training” network uses a 3600 instead of 120 second timeout and is only veriﬁed
for the ﬁrst 100 images because verifying it took too long.
** The “+RS (Large)” networks are only veriﬁed for the ﬁrst 1000 images because of long build
times.
*** Wong et al. (2018); Dvijotham et al. (2018), and Mirman et al. (2018), which we compare to in
Table 3, do not report results on MNIST, (cid:15) = 0.2 in their papers. We ran the publicly available code
of Wong et al. (2018) on MNIST, (cid:15) = 0.2 to generate these results for comparison.

19

Published as a conference paper at ICLR 2019

F DISCUSSION ON VERIFICATION AND CERTIFICATION

Exact veriﬁcation and certiﬁcation are two related approaches to formally verifying properties of
neural networks, such as adversarial robustness.
In both cases, the end goal is formal veriﬁca-
tion. Certiﬁcation methods, which solve an easier-to-solve relaxation of the exact veriﬁcation prob-
lem, are important developments because exact veriﬁcation previously appeared computationally
intractable for all but the smallest models.

For the case of adversarial robustness, certiﬁcation methods exploit a trade-off between provable
robustness and speed. They can fail to provide certiﬁcates of robustness for some inputs that are
actually robust, but they will either ﬁnd or fail to ﬁnd certiﬁcates of robustness quickly. On the other
hand, exact veriﬁers will always give the correct answer if given enough time, but exact veriﬁers can
sometimes take many hours to formally verify robustness on even a single input.

In general, the process of training a robust neural network and then formally verifying its robustness
happens in two steps.

• Step 1: Training
• Step 2: Veriﬁcation or Certiﬁcation

Most papers on certiﬁcation, including Wong and Kolter (2018); Wong et al. (2018); Dvijotham
et al. (2018); Raghunathan et al. (2018) and Mirman et al. (2018), propose a method for step 2
(the certiﬁcation step), and then propose a training objective in step 1 that is directly related to
their method for step 2. We call this paradigm “co-training.” In Raghunathan et al. (2018), they
found that using their step 2 on a model trained using Wong and Kolter (2018)’s step 1 resulted in
extremely poor provable robustness (less than 10%), and the same was true when using Wong and
Kolter (2018)’s step 2 on a model trained using their step 1.

We focus on MILP-based exact veriﬁcation as our step 2, which encompasses the best current exact
veriﬁcation methods. The advantage of using exact veriﬁcation for step 2 is that it will be accurate,
regardless of what method is used in step 1. The disadvantage of using exact veriﬁcation for step
2 is that it could be extremely slow. For our step 1, we used standard robust adversarial training.
In order to signiﬁcantly speed up exact veriﬁcation as step 2, we proposed techniques that could be
added to step 1 to induce weight sparsity and ReLU stability.

In general, we believe it is important to develop effective methods for step 1, given that step 2 is
exact veriﬁcation. However, ReLU stability can also be beneﬁcial for tightening the relaxation of
certiﬁcation approaches like that of Wong et al. (2018) and Dvijotham et al. (2018), as unstable
ReLUs are the primary cause of the overapproximation that occurs in the relaxation step. Thus, our
techniques for inducing ReLU stability can be useful for certiﬁcation as well.

Finally, in recent literature on veriﬁcation and certiﬁcation, most works have focused on formally
verifying the property of adversarial robustness of neural networks. However, veriﬁcation of other
properties could be useful, and our techniques to induce weight sparsity and ReLU stability would
still be useful for veriﬁcation of other properties for the exact same reasons that they are useful in
the case of adversarial robustness.

20

9
1
0
2
 
r
p
A
 
3
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
0
0
3
0
.
9
0
8
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2019

TRAINING FOR FASTER ADVERSARIAL ROBUSTNESS
VERIFICATION VIA INDUCING RELU STABILITY

Kai Y. Xiao Vincent Tjeng Nur Muhammad (Mahi) Shaﬁullah Aleksander M ˛adry
Massachusetts Institute of Technology
Cambridge, MA 02139
{kaix, vtjeng, nshafiul, madry}@mit.edu

ABSTRACT

We explore the concept of co-design in the context of neural network veriﬁcation.
Speciﬁcally, we aim to train deep neural networks that not only are robust to ad-
versarial perturbations but also whose robustness can be veriﬁed more easily. To
this end, we identify two properties of network models – weight sparsity and so-
called ReLU stability – that turn out to signiﬁcantly impact the complexity of the
corresponding veriﬁcation task. We demonstrate that improving weight sparsity
alone already enables us to turn computationally intractable veriﬁcation problems
into tractable ones. Then, improving ReLU stability leads to an additional 4–13x
speedup in veriﬁcation times. An important feature of our methodology is its “uni-
versality,” in the sense that it can be used with a broad range of training procedures
and veriﬁcation approaches.

1

INTRODUCTION

Deep neural networks (DNNs) have recently achieved widespread success in image classiﬁcation
(Krizhevsky et al., 2012), face and speech recognition (Taigman et al., 2014; Hinton et al., 2012),
and game playing (Silver et al., 2016; 2017). This success motivates their application in a broader
set of domains, including more safety-critical environments. This thrust makes understanding the
reliability and robustness of the underlying models, let alone their resilience to manipulation by
malicious actors, a central question. However, predictions made by machine learning models are
often brittle. A prominent example is the existence of adversarial examples (Szegedy et al., 2014):
imperceptibly modiﬁed inputs that cause state-of-the-art models to misclassify with high conﬁdence.

There has been a long line of work on both generating adversarial examples, called attacks (Carlini
and Wagner, 2017b;a; Athalye et al., 2018a;b; Uesato et al., 2018; Evtimov et al., 2017), and training
models robust to adversarial examples, called defenses (Goodfellow et al., 2015; Papernot et al.,
2016; Madry et al., 2018; Kannan et al., 2018). However, recent research has shown that most
defenses are ineffective (Carlini and Wagner, 2017a; Athalye et al., 2018a; Uesato et al., 2018).
Furthermore, even for defenses such as that of Madry et al. (2018) that have seen empirical success
against many attacks, we are unable to conclude yet with certainty that they are robust to all attacks
that we want these models to be resilient to.

This state of affairs gives rise to the need for veriﬁcation of networks, i.e., the task of formally
proving that no small perturbations of a given input can cause it to be misclassiﬁed by the network
model. Although many exact veriﬁers1 have been designed to solve this problem, the veriﬁcation
process is often intractably slow. For example, when using the Reluplex veriﬁer of Katz et al. (2017),
even verifying a small MNIST network turns out to be computationally infeasible. Thus, addressing
this intractability of exact veriﬁcation is the primary goal of this work.

Our Contributions
Our starting point is the observation that, typically, model training and veriﬁcation are decoupled
and seen as two distinct steps. Even though this separation is natural, it misses a key opportunity:
the ability to align these two stages. Speciﬁcally, applying the principle of co-design during model

1Also sometimes referred to as combinatorial veriﬁers.

1

Published as a conference paper at ICLR 2019

training is possible: training models in a way to encourage them to be simultaneously robust and
easy-to-exactly-verify. This insight is the cornerstone of the techniques we develop in this paper.

In this work, we use the principle of co-design to develop training techniques that give models
that are both robust and easy-to-verify. Our techniques rely on improving two key properties of
networks: weight sparsity and ReLU stability. Speciﬁcally, we ﬁrst show that natural methods for
improving weight sparsity during training, such as (cid:96)1-regularization, give models that can already be
veriﬁed much faster than current methods. This speedup happens because in general, exact veriﬁers
beneﬁt from having fewer variables in their formulations of the veriﬁcation task. For instance, for
exact veriﬁers that rely on linear programming (LP) solvers, sparser weight matrices means fewer
variables in those constraints.

We then focus on the major speed bottleneck of current approaches to exact veriﬁcation of ReLU
networks: the need of exact veriﬁcation methods to “branch,” i.e., consider two possible cases for
each ReLU (ReLU being active or inactive). Branching drastically increases the complexity of
veriﬁcation. Thus, well-optimized veriﬁers will not need to branch on a ReLU if it can determine
that the ReLU is stable, i.e.
that the ReLU will always be active or always be inactive for any
perturbation of an input. This motivates the key goal of the techniques presented in this paper: we
aim to minimize branching by maximizing the number of stable ReLUs. We call this goal ReLU
stability and introduce a regularization technique to induce it.

Our techniques enable us to train weight-sparse and ReLU stable networks for MNIST and CIFAR-
10 that can be veriﬁed signiﬁcantly faster. Speciﬁcally, by combining natural methods for inducing
weight sparsity with a robust adversarial training procedure (cf. Goodfellow et al. (2015)), we are
able to train networks for which almost 90% of inputs can be veriﬁed in an amount of time that
is small2 compared to previous veriﬁcation techniques. Then, by also adding our regularization
technique for inducing ReLU stability, we are able to train models that can be veriﬁed an addi-
tional 4–13x times as fast while maintaining state-of-the-art accuracy on MNIST. Our techniques
show similar improvements for exact veriﬁcation of CIFAR models. In particular, we achieve the
following veriﬁcation speed and provable robustness results for (cid:96)∞ norm-bound adversaries:

Dataset

Epsilon

Provable Adversarial Accuracy Average Solve Time (s)

MNIST

(cid:15) = 0.1
(cid:15) = 0.2
(cid:15) = 0.3

CIFAR

(cid:15) = 2/255
(cid:15) = 8/255

94.33%
89.79%
80.68%

45.93%
20.27%

0.49
1.13
2.78

13.50
22.33

Our network for (cid:15) = 0.1 on MNIST achieves provable adversarial accuracies comparable with the
current best results of Wong et al. (2018) and Dvijotham et al. (2018), and our results for (cid:15) = 0.2 and
(cid:15) = 0.3 achieve the best provable adversarial accuracies yet. To the best of our knowledge, we also
achieve the ﬁrst nontrivial provable adversarial accuracy results using exact veriﬁers for CIFAR-10.

Finally, we design our training techniques with universality as a goal. We focus on improving the
input to the veriﬁcation process, regardless of the veriﬁer we end up using. This is particularly
important because research into network veriﬁcation methods is still in its early stages, and our co-
design methods are compatible with the best current veriﬁers (LP/MILP-based methods) and should
be compatible with any future improvements in veriﬁcation.

Our code is available at https://github.com/MadryLab/relu_stable.

2 BACKGROUND AND RELATED WORK

Exact veriﬁcation of networks has been the subject of many recent works (Katz et al., 2017; Ehlers,
2017; Carlini et al., 2017; Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). To
understand the context of these works, observe that for linear networks, the task of exact veriﬁcation
is relatively simple and can be done by solving a LP. For more complex models, the presence of
nonlinear ReLUs makes veriﬁcation over all perturbations of an input much more challenging. This

2We chose our time budget for veriﬁcation to be 120 seconds per input image.

2

Published as a conference paper at ICLR 2019

is so as ReLUs can be active or inactive depending on the input, which can cause exact veriﬁers
to “branch" and consider these two cases separately. The number of such cases that veriﬁers have
to consider might grow exponentially with the number of ReLUs, so veriﬁcation speed will also
grow exponentially in the worst case. Katz et al. (2017) further illustrated the difﬁculty of exact
veriﬁcation by proving that it is NP-complete. In recent years, formal veriﬁcation methods were
developed to verify networks. Most of these methods use satisﬁability modulo theory (SMT) solvers
(Katz et al., 2017; Ehlers, 2017; Carlini et al., 2017) or LP and Mixed-Integer Linear Programming
(MILP) solvers (Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). However,
all of them are limited by the same issue of scaling poorly with the number of ReLUs in a network,
making them prohibitively slow in practice for even medium-sized models.

One recent approach for dealing with the inefﬁciency of exact veriﬁers is to focus on certiﬁcation
methods3 (Wong and Kolter, 2018; Wong et al., 2018; Dvijotham et al., 2018; Raghunathan et al.,
2018; Mirman et al., 2018; Sinha et al., 2018). In contrast to exact veriﬁcation, these methods do not
solve the veriﬁcation task directly; instead, they rely on solving a relaxation of the veriﬁcation prob-
lem. This relaxation is usually derived by overapproximating the adversarial polytope, or the space
of outputs of a network for a region of possible inputs. These approaches rely on training models in
a speciﬁc manner that makes certiﬁcation of those models easier. As a result, they can often obtain
provable adversarial accuracy results faster. However, certiﬁcation is fundamentally different from
veriﬁcation in two primary ways. First, it solves a relaxation of the original veriﬁcation problem. As
a result, certiﬁcation methods can fail to certify many inputs that are actually robust to perturbations
– only exact veriﬁers, given enough time, can give conclusive answers on robustness for every single
input. Second, certiﬁcation approaches fall under the paradigm of co-training, where a certiﬁcation
method only works well on models speciﬁcally trained for that certiﬁcation step. When used as a
black box on arbitrary models, the certiﬁcation step can yield a high rate of false negatives. For
example, Raghunathan et al. (2018) found that their certiﬁcation step was signiﬁcantly less effective
when used on a model trained using Wong and Kolter (2018)’s training method, and vice versa. In
contrast, we design our methods to be universal. They can be combined with any standard train-
ing procedure for networks and will improve exact veriﬁcation speed for any LP/MILP-based exact
veriﬁer. Our methods can also decrease the amount of overapproximation incurred by certiﬁcation
methods like Wong and Kolter (2018); Dvijotham et al. (2018). Similar to most of the certiﬁcation
methods, our technique can be made to have very little training time overhead.

Finally, subsequent work of Gowal et al. (2018) shows how applying interval bound propagation
during training, combined with MILP-based exact veriﬁcation, can lead to provably robust networks.

3 TRAINING VERIFIABLE NETWORK MODELS

We begin by discussing the task of verifying a network and identify two key properties of networks
that lead to improved veriﬁcation speed: weight sparsity and so-called ReLU stability. We then use
natural regularization methods for inducing weight sparsity as well as a new regularization method
for inducing ReLU stability. Finally, we demonstrate that these methods signiﬁcantly speed up
veriﬁcation while maintaining state-of-the-art accuracy.

3.1 VERIFYING ADVERSARIAL ROBUSTNESS OF NETWORK MODELS

Deep neural network models. Our focus will be on one of the most common architectures for
state-of-the-art models: k-layer fully-connected feed-forward DNN classiﬁers with ReLU non-
linearities4. Such models can be viewed as a function f (·, W, b), where W and b represent the
weight matrices and biases of each layer. For an input x, the output f (x, W, b) of the DNN is

3These works use both “veriﬁcation” and “certiﬁcation” to describe their methods. For clarity, we use “cer-
tiﬁcation” to describe their approaches, while we use “veriﬁcation” to describe exact veriﬁcation approaches.
For a more detailed discussion of the differences, see Appendix F.

4Note that this encompasses common convolutional network architectures because every convolutional layer

can be replaced by an equivalent fully-connected layer.

3

Published as a conference paper at ICLR 2019

deﬁned as:

z0 = x
ˆzi = zi−1Wi + bi
zi = max( ˆzi, 0)

(1)
(2)
(3)
(4)
Here, for each layer i, we let ˆzij denote the jth ReLU pre-activation and let ˆzij(x) denote the value
of ˆzij on an input x. ˆzk−1 is the ﬁnal, output layer with an output unit for each possible label (the
logits). The network will make predictions by selecting the label with the largest logit.

for i = 1, 2, . . . , k − 1
for i = 1, 2, . . . , k − 2

f (x, W, b) = ˆzk−1

Adversarial robustness. For a network to be reliable, it should make predictions that are robust
– that is, it should predict the same output for inputs that are very similar. Speciﬁcally, we want
the DNN classiﬁer’s predictions to be robust to a set Adv(x) of possible adversarial perturbations
of an input x. We focus on (cid:96)∞ norm-bound adversarial perturbations, where Adv(x) = {x(cid:48)
:
||x(cid:48) − x||∞ ≤ (cid:15)} for some constant (cid:15), since it is the most common one considered in adversarial
robustness and veriﬁcation literature (thus, it currently constitutes a canonical benchmark). Even so,
our methods can be applied to other (cid:96)p norms and broader sets of perturbations.
Veriﬁcation of network models. For an input x with correct label y, a perturbed input x(cid:48) can cause
a misclassiﬁcation if it makes the logit of some incorrect label ˆy larger than the logit of y on x(cid:48). We
can thus express the task of ﬁnding an adversarial perturbation as the optimization problem:

f (x(cid:48), W )y − f (x(cid:48), W )ˆy

min
x(cid:48),ˆy
subject to x(cid:48) ∈ Adv(x)

An adversarial perturbation exists if and only if the objective of the optimization problem is negative.

Adversarial accuracies. We deﬁne the true adversarial accuracy of a model to be the fraction of
test set inputs for which the model is robust to all allowed perturbations. By deﬁnition, evaluations
against speciﬁc adversarial attacks like PGD or FGSM provide an upper bound to this accuracy,
while certiﬁcation methods provide lower bounds. Given sufﬁcient time for each input, an exact
veriﬁer can prove robustness to perturbations, or ﬁnd a perturbation where the network makes a
misclassiﬁcation on the input, and thus exactly determine the true adversarial accuracy. This is in
contrast to certiﬁcation methods, which solve a relaxation of the veriﬁcation problem and can not
exactly determine the true adversarial accuracy no matter how much time they have.

However, such exact veriﬁcation may take impractically long for certain inputs, so we instead com-
pute the provable adversarial accuracy, which we deﬁne as the fraction of test set inputs for which
the veriﬁer can prove robustness to perturbations within an allocated time budget (timeout). Simi-
larly to certiﬁable accuracy, this accuracy constitutes a lower bound on the true adversarial accuracy.
A model can thus, e.g., have high true adversarial accuracy and low provable adversarial accuracy if
veriﬁcation of the model is too slow and often fails to complete before timeout.

Also, in our evaluations, we chose to use the MILP exact veriﬁer of Tjeng et al. (2019) when per-
forming experiments, as it is both open source and the fastest veriﬁer we are aware of.

3.2 WEIGHT SPARSITY AND ITS IMPACT ON VERIFICATION SPEED

The ﬁrst property of network models that we want to improve in order to speed up exact veriﬁcation
of those models is weight sparsity. Weight sparsity is important for veriﬁcation speed because many
exact veriﬁers rely on solving LP or MILP systems, which beneﬁt from having fewer variables.
We use two natural regularization methods for improving weight sparsity: (cid:96)1-regularization and
small weight pruning. These techniques signiﬁcantly improve veriﬁcation speed – see Table 1.
Verifying even small MNIST networks is almost completely intractable without them. Speciﬁcally,
the veriﬁer can only prove robustness of an adversarially-trained model on 19% of inputs with a one
hour budget per input, while the veriﬁer can prove robustness of an adversarially-trained model with
(cid:96)1-regularization and small weight pruning on 89.13% of inputs with a 120 second budget per input.

Interestingly, even though adversarial training improves weight sparsity (see Appendix B) it was
still necessary to use (cid:96)1-regularization and small weight pruning. These techniques further promoted
weight sparsity and gave rise to networks that were much easier to verify.

4

Published as a conference paper at ICLR 2019

Dataset

Epsilon

Training
Method

Test Set
Accuracy

Provable Adversarial
Accuracy

Average
Solve Time (s)

MNIST (cid:15) = 0.1

1 Adversarial Training
+(cid:96)1-Regularization
2
+Small Weight Pruning
3
4

+ReLU Pruning (control)

99.17%
99.00%
98.99%
98.94%

19.00%
82.17%
89.13%
91.58%

2970.43
21.99
11.71
6.43

Table 1: Improvement in provable adversarial accuracy and veriﬁcation solve times when incremen-
tally adding natural regularization methods for improving weight sparsity and ReLU stability into
the model training procedure, before veriﬁcation occurs. Each row represents the addition of another
method – for example, Row 3 uses adversarial training, (cid:96)1-regularization, and small weight pruning.
Row 4 adds ReLU pruning (see Appendix A). Row 4 is the control model for MNIST and (cid:15) = 0.1
that we present again in comparisons in Tables 2 and 3. We use a 3600 instead of 120 second timeout
for Row 1 and only veriﬁed the ﬁrst 100 images (out of 10000) because verifying it took too long.

3.3 RELU STABILITY

Next, we target the primary speed bottleneck of exact veriﬁcation: the number of ReLUs the veriﬁer
has to branch on. In our paper, this corresponds to the notion of inducing ReLU stability. Before we
describe our methodology, we formally deﬁne ReLU stability.

(2)).

Given an input x, a set of allowed perturbations Adv(x), and a ReLU, exact veriﬁers may need
to branch based on the possible pre-activations of the ReLU, namely ˆzij(Adv(x)) = {ˆzij(x(cid:48)) :
x(cid:48) ∈ Adv(x)} (cf.
If there exist two perturbations x(cid:48), x(cid:48)(cid:48) in the set Adv(x) such that
sign(ˆzij(x(cid:48))) (cid:54)= sign(ˆzij(x(cid:48)(cid:48))), then the veriﬁer has to consider that for some perturbed inputs the
ReLU is active (zij = ˆzij) and for other perturbed inputs inactive (zij = 0). The more such cases
the veriﬁer faces, the more branches it has to consider, causing the complexity of veriﬁcation to in-
crease exponentially. Intuitively, a model with 1000 ReLUs among which only 100 ReLUs require
branching will likely be much easier to verify than a model with 200 ReLUs that all require branch-
ing. Thus, it is advantageous for the veriﬁer if, on an input x with allowed perturbation set Adv(x),
the number of ReLUs such that

sign(ˆzij(x(cid:48))) = sign(ˆzij(x)) ∀x(cid:48) ∈ Adv(x)
is maximized. We call a ReLU for which (5) holds on an input x a stable ReLU on that input. If (5)
does not hold, then the ReLU is an unstable ReLU.

(5)

Directly computing whether a ReLU is stable on a given input x is difﬁcult because doing so would
involve considering all possible values of ˆzij(Adv(x)). Instead, exact veriﬁers compute an upper
bound ˆuij and a lower bound ˆlij of ˆzij(Adv(x)). If 0 ≤ ˆlij or ˆuij ≤ 0, they can replace the ReLU
with the identity function or the zero function, respectively. Otherwise, if ˆlij < 0 < ˆuij, these
veriﬁers then determine that they need to “branch” on that ReLU. Thus, we can rephrase (5) as

sign(ˆuij) = sign(ˆlij)

(6)

We will discuss methods for determining these upper and lower bounds ˆuij, ˆlij in Section 3.3.2.

3.3.1 A REGULARIZATION TECHNIQUE FOR INDUCING RELU STABILITY: RS LOSS

As we see from equation (6), a function that would indicate exactly when a ReLU is stable is
F ∗(ˆuij, ˆlij) = sign(ˆuij) · sign(ˆlij). Thus, it would be natural to use this function as a regular-
izer. However, this function is non-differentiable and if used in training a model, would provide no
useful gradients during back-propagation. Thus, we use the following smooth approximation of F ∗
(see Fig. 1) which provides the desired gradients:

F (ˆuij, ˆlij) = − tanh(1 + ˆuij · ˆlij)

We call the corresponding objective RS Loss, and show in Fig. 2a that using this loss function as
a regularizer effectively decreases the number of unstable ReLUs. RS Loss thus encourages ReLU
stability, which, in turn, speeds up exact veriﬁcation - see Fig. 2b.

5

Published as a conference paper at ICLR 2019

Figure 1: Plot and contour plot of the function F (x, y) = − tanh(1 + x · y)

3.3.2 ESTIMATING RELU UPPER AND LOWER BOUNDS ON ACTIVATIONS

A key aspect of using RS Loss is determining the upper and lower bounds ˆuij, ˆlij for each ReLU (cf.
(6)). The bounds for the inputs z0 (cf. (1)) are simple – for the input x, we know x − (cid:15) ≤ z0 ≤ x + (cid:15),
so ˆu0 = x − (cid:15), ˆl0 = x + (cid:15). For all subsequent zij, we estimate bounds using either the naive interval
arithmetic (IA) approach described in Tjeng et al. (2019) or an improved version of it. The improved
version is a tighter estimate but uses more memory and training time, and thus is most effective on
smaller networks. We present the details of naive IA and improved IA in Appendix C.

Interval arithmetic approaches can be implemented relatively efﬁciently and work well with back-
propagation because they only involve matrix multiplications. This contrasts with how exact veri-
ﬁers compute these bounds, which usually involves solving LPs or MILPs. Interval arithmetic also
overestimates the number of unstable ReLUs. This means that minimizing unstable ReLUs based
on IA bounds will provide an upper bound on the number of unstable ReLUs determined by exact
veriﬁers. In particular, IA will properly penalize every unstable ReLU.

Improved IA performs well in practice, overestimating the number of unstable ReLUs by less than
0.4% in the ﬁrst 2 layers of MNIST models and by less than 36.8% (compared to 128.5% for naive
IA) in the 3rd layer. Full experimental results are available in Table 4 of Appendix C.3.

3.3.3

IMPACT OF RELU STABILITY IMPROVEMENTS ON VERIFICATION SPEED

We provide experimental evidence that RS Loss regularization improves ReLU stability and speeds
up average veriﬁcation times by more than an order of magnitude in Fig. 2b. To isolate the effect of
RS Loss, we compare MNIST models trained in exactly the same way other than the weight on RS
Loss. When comparing a network trained with a RS Loss weight of 5e−4 to a network with a RS
Loss weight of 0, the former has just 16% as many unstable ReLUs and can be veriﬁed 65x faster.
The caveat here is that the former has 1.00% lower test set accuracy.

We also compare veriﬁcation speed with and without RS Loss on MNIST networks for different
values of (cid:15) (0.1, 0.2, and 0.3) in Fig. 2c. We choose RS Loss weights that cause almost no test set
accuracy loss (less than 0.50% - See Table 3) in these cases, and we still observe a 4–13x speedup
from RS Loss. For CIFAR, RS Loss gives a smaller speedup of 1.6–3.7x (See Appendix E).

3.3.4

IMPACT OF RELU STABILITY IMPROVEMENTS ON PROVABLE ADVERSARIAL
ACCURACY

As the weight on the RS Loss used in training a model increases, the ReLU stability of the model
will improve, speeding up veriﬁcation and likely improving provable adversarial accuracy. However,
like most regularization methods, placing too much weight on RS Loss can decrease the model ca-
pacity, potentially lowering both the true adversarial accuracy and the provable adversarial accuracy.
Therefore, it is important to choose the weight on RS Loss carefully to obtain both high provable
adversarial accuracy and faster veriﬁcation speeds.

To show the effectiveness of RS Loss in improving provable adversarial accuracy, we train two
networks for each dataset and each value of (cid:15). One is a “control” network that uses all of the natural
improvements for inducing both weight sparsity ((cid:96)1-regularization and small weight pruning) and
ReLU stability (ReLU pruning - see Appendix A). The second is a “+RS” network that uses RS

6

Published as a conference paper at ICLR 2019

(a)

(b)

(c)

Figure 2: (a) Average number of unstable ReLUs by layer and (b) average veriﬁcation solve times
of 6 networks trained with different weights on RS Loss for MNIST and (cid:15) = 0.1 . Averages are
taken over all 10000 MNIST test set inputs. Both metrics improve signiﬁcantly with increasing
RS Loss weight. An RS Loss weight of 0 corresponds to the control network, while an RS Loss
weight of 0.00012 corresponds to the “+RS” network for MNIST and (cid:15) = 0.1 in Tables 2 and 3. (c)
Improvement in the average time taken by a veriﬁer to solve the veriﬁcation problem after adding
RS Loss to the training procedure, for different (cid:15) on MNIST. The weight on RS Loss was chosen so
that the “+RS” models have test set accuracies within 0.50% of the control models.

Loss in addition to all of the same natural improvements. This lets us isolate the incremental effect
of adding RS Loss to the training procedure.

In addition to attaining a 4–13x speedup in MNIST veriﬁcation times (see Fig. 2c), we achieve
higher provable adversarial accuracy in every single setting when using RS Loss. This is especially
notable for the hardest veriﬁcation problem we tackle – proving robustness to perturbations with (cid:96)∞
norm-bound 8/255 on CIFAR-10 – where adding RS Loss nearly triples the provable adversarial
accuracy from 7.09% to 20.27%. This improvement is primarily due to veriﬁcation speedup induced
by RS Loss, which allows the veriﬁer to ﬁnish proving robustness for far more inputs within the 120
second time limit. These results are shown in Table 2.

Table 2: Provable Adversarial Accuracies for the control and “+RS” networks in each setting.

MNIST, (cid:15) = 0.1 MNIST, (cid:15) = 0.2 MNIST, (cid:15) = 0.3 CIFAR, (cid:15) = 2/255 CIFAR, (cid:15) = 8/255

Control
+RS

91.58
94.33

86.45
89.79

77.99
80.68

45.53
45.93

7.09
20.27

4 EXPERIMENTS

4.1 EXPERIMENTS ON MNIST AND CIFAR

In addition to the experimental results already presented, we compare our control and “+RS” net-
works with the best available results presented in the state-of-the-art certiﬁable defenses of Wong
et al. (2018), Dvijotham et al. (2018), and Mirman et al. (2018) in Table 3. We compare their test
set accuracy, PGD adversarial accuracy (an evaluation of robustness against a strong 40-step PGD
adversarial attack), and provable adversarial accuracy. Additionally, to show that our method can
scale to larger architectures, we train and verify a “+RS (Large)” network for each dataset and (cid:15).

In terms of provable adversarial accuracies, on MNIST, our results are signiﬁcantly better than those
of Wong et al. (2018) for larger perturbations of (cid:15) = 0.3, and comparable for (cid:15) = 0.1. On CIFAR-
10, our method is slightly less effective, perhaps indicating that more unstable ReLUs are necessary
to properly learn a robust CIFAR classiﬁer. We also experienced many more instances of the veriﬁer
reaching its allotted 120 second time limit on CIFAR, especially for the less ReLU stable control
networks. Full experimental details for each model in Tables 1, 2, and 3, including a breakdown of
veriﬁcation solve results (how many images did the veriﬁer: A. prove robust B. ﬁnd an adversarial
example for C. time out on), are available in Appendix E.

7

Published as a conference paper at ICLR 2019

Table 3: Comparison of test set, PGD adversarial, and provable adversarial accuracy of networks
trained with and without RS Loss. We also provide the best available certiﬁable adversarial and
PGD adversarial accuracy of any single models from Wong et al. (2018), Dvijotham et al. (2018),
and Mirman et al. (2018) for comparison, and highlight the best provable accuracy for each (cid:15).
* The provable adversarial accuracy for “+RS (Large)” is only computed for the ﬁrst 1000 images
because the veriﬁer performs more slowly on larger models.
** Dvijotham et al. (2018); Mirman et al. (2018) use a slightly smaller (cid:15) = 0.03 = 7.65/255.
† Mirman et al. (2018) computes results over 500 images instead of all 10000.
†† Mirman et al. (2018) uses a slightly smaller (cid:15) = 0.007 = 1.785/255.

Dataset

Epsilon

Test Set
Accuracy

PGD Adversarial

Provable/Certiﬁable
Accuracy Adversarial Accuracy

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

Training
Method

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†,††

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.**
Mirman et al.†, **

98.94%
98.68%
98.95%

98.92%
98.80%
99.00%

98.40%
98.10%
98.21%

97.75%
97.33%
97.54%

85.13%
96.60%

64.64%
61.12%
61.41%

68.28%
62.00%

50.69%
40.45%
42.81%

28.67%
48.64%
54.20%

95.12%
95.13%
96.58%

-
97.13%
97.60%

93.14%
93.14%
94.19%

91.64%
92.05%
93.25%

-
93.80%

51.58%
49.92%
50.61%

-
54.60%

31.28%
26.78%
28.69%

-
32.72%
40.00%

91.58%
94.33%
95.60%

96.33%
95.56%
96.60%

86.45%
89.79%
89.10%

77.99%
80.68%
59.60%

56.90%
82.00%

45.53%
45.93%
41.40%

53.89%
52.20%

7.09%
20.27%
19.80%

21.78%
26.67%
35.20%

4.2 EXPERIMENTAL METHODS AND DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. For each setting of dataset
(MNIST or CIFAR) and (cid:15), we ﬁnd a suitable weight on RS Loss via line search (See Table 6 in
Appendix D). The same weight is used for each ReLU. During training, we used improved IA for
ReLU bound estimation for “+RS” models and use naive IA for “+RS (Large)” models because of
memory constraints. For ease of comparison, we trained our networks using the same convolutional
DNN architecture as in Wong et al. (2018). This architecture uses two 2x2 strided convolutions with
16 and 32 ﬁlters, followed by a 100 hidden unit fully connected layer. For the larger architecture,
we also use the same “large” architecture as in Wong et al. (2018). It has 4 convolutional layers with
32, 32, 64, and 64 ﬁlters, followed by 2 fully connected layers with 512 hidden units each.

For veriﬁcation, we used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019).
Model solves were parallelized over 8 CPU cores. When verifying an image, the veriﬁer of Tjeng
et al. (2019) ﬁrst builds a model, and second, solves the veriﬁcation problem (See Appendix D.2 for
details). We focus on reporting solve times because that is directly related to the task of veriﬁcation
itself. All build times for the control and “+RS” models on MNIST that we presented were between
4 and 10 seconds, and full results on build times are also presented in Appendix E.

Additional details on our experimental setup (e.g. hyperparameters) can be found in Appendix D.

8

Published as a conference paper at ICLR 2019

5 CONCLUSION

In this paper, we use the principle of co-design to develop training methods that emphasize veri-
ﬁcation as a goal, and we show that they make verifying the trained model much faster. We ﬁrst
demonstrate that natural regularization methods already make the exact veriﬁcation problem sig-
niﬁcantly more tractable. Subsequently, we introduce the notion of ReLU stability for networks,
present a method that improves a network’s ReLU stability, and show that this improvement makes
veriﬁcation an additional 4–13x faster. Our method is universal, as it can be added to any training
procedure and should speed up any exact veriﬁcation procedure, especially MILP-based methods.

Prior to our work, exact veriﬁcation seemed intractable for all but the smallest models. Thus, our
work shows progress toward reliable models that can be proven to be robust, and our techniques can
help scale veriﬁcation to even larger networks.

Many of our methods appear to compress our networks into more compact, simpler forms. We
hypothesize that the reason that regularization methods like RS Loss can still achieve very high
accuracy is that most models are overparametrized in the ﬁrst place. There exist clear parallels
between our methods and techniques in model compression (Han et al., 2016; Cheng et al., 2017b) –
therefore, we believe that drawing upon additional techniques from model compression can further
improve the ease-of-veriﬁcation of networks. We also expect that there exist objectives other than
weight sparsity and ReLU stability that are important for veriﬁcation speed. If so, further exploring
the principle of co-design for those objectives is an interesting future direction.

ACKNOWLEDGEMENTS

This work was supported by the NSF Graduate Research Fellowship under Grant No. 1122374,
by the NSF grants CCF-1553428 and CNS-1815221, and by Lockheed Martin Corporation under
award number RPP2016-002. We would like to thank Krishnamurthy Dvijotham, Ludwig Schmidt,
Michael Sun, Dimitris Tsipras, and Jonathan Uesato for helpful discussions.

9

Published as a conference paper at ICLR 2019

REFERENCES

Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense
In International Conference on

of security: Circumventing defenses to adversarial examples.
Machine Learning (ICML), 2018a.

Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. In International Conference on Machine Learning (ICML), pages 284–293, 2018b.

Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
In Proceedings of ACM Workshop on Artiﬁcial Intelligence and Security

detection methods.
(AISec), 2017a.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks.

In

Security and Privacy (SP), 2017 IEEE Symposium on, 2017b.

Nicholas Carlini, Guy Katz, Clark Barrett, and David L. Dill. Ground-truth adversarial examples.

2017.

networks. 2017a.

Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artiﬁcial neural

Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration

for deep neural networks. 2017b.

Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training veriﬁed learners with learned ver-
iﬁers. 2018.

Rüdiger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In Auto-

mated Technology for Veriﬁcation and Analysis, 2017.

Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir
Rahmati, and Dawn Song. Robust physical-world attacks on machine learning models. 2017.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. In International Conference on Learning Representations (ICLR), 2015.

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. 2018.

Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. In International Conference on Learning
Representations (ICLR), 2016.

Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, An-
drew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep
neural networks for acoustic modeling in speech recognition: The shared views of four research
groups. In IEEE Signal Processing Magazine, 2012.

Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. 2018.

Guy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer. Reluplex: An efﬁcient

smt solver for verifying deep neural networks. 2017.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations (ICLR), 2015.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012.

Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward relu

neural networks. 2017.

10

Published as a conference paper at ICLR 2019

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations (ICLR), 2018.

Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning (ICML), 2018.

Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation
as a defense to adversarial perturbations against deep neural networks. In Proceedings of IEEE
Symposium on Security and Privacy (SP), 2016.

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial exam-

ples. In International Conference on Learning Representations (ICLR), 2018.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 2017.

Aman Sinha, Hongseok Namkoong, and John Duchi. Certiﬁable distributional robustness with
principled adversarial training. In International Conference on Learning Representations (ICLR),
2018.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations (ICLR), 2014.

Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap
In Computer Vision and Pattern Recognition

to human-level performance in face veriﬁcation.
(CVPR), 2014.

Robert Tibshirani. Regression shrinkage and selection via the lasso. In Journal of the Royal Statis-

tical Society, Series B, 1994.

Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In International Conference on Learning Representations (ICLR), 2019.

Jonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial
risk and the dangers of evaluating against weak attacks. In International Conference on Machine
Learning (ICML), 2018.

Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer

adversarial polytope. In International Conference on Machine Learning (ICML), 2018.

Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and Zico Kolter. Scaling provable adversarial

defenses. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

11

Published as a conference paper at ICLR 2019

APPENDIX

A NATURAL IMPROVEMENTS

A.1 NATURAL REGULARIZATION FOR INDUCING WEIGHT SPARSITY

All of the control and “+RS” networks in our paper contain natural improvements that improve
weight sparsity, which reduce the number of variables in the LPs solved by the veriﬁer. We ob-
served that the two techniques we used for weight sparsity ((cid:96)1-regularization and small weight prun-
ing) don’t hurt test set accuracy but they dramatically improve provable adversarial accuracy and
veriﬁcation speed.

1. (cid:96)1-regularization: We use a weight of 2e−5 on MNIST and a weight of 1e−5 on CIFAR.
We chose these weights via line search by ﬁnding the highest weight that would not hurt
test set accuracy.

2. Small weight pruning: Zeroing out weights in a network that are very close to zero. We

choose to prune weights less than 1e−3.

A.2 A BASIC IMPROVEMENT FOR INDUCING RELU STABILITY: RELU PRUNING

We also use a basic idea to improve ReLU stability, which we call ReLU pruning. The main idea is
to prune away ReLUs that are not necessary.

We use a heuristic to test whether a ReLU in a network is necessary. Our heuristic is to count how
many training inputs cause the ReLU to be active or inactive. If a ReLU is active (the pre-activation
satisﬁes ˆzij(x) > 0) for every input image in the training set, then we can replace that ReLU with
the identity function and the network would behave in exactly the same way for all of those images.
Similarly, if a ReLU is inactive (ˆzij(x) < 0) for every training image, that ReLU can be replaced by
the zero function.

Extending this idea further, we expect that ReLUs that are rarely used can also be removed without
signiﬁcantly changing the behavior of the network. If only a small fraction (say, 10%) of the input
images activate a ReLU, then replacing the ReLU with the zero function will only slightly change the
network’s behavior and will not affect the accuracy too much. We provide experimental evidence of
this phenomenon on an adversarially trained ((cid:15) = 0.1) MNIST model. Conservatively, we decided
that pruning away ReLUs that are active on less than 10% of the training set or inactive on less than
10% of the training set was reasonable.

Figure 3: Removing some ReLUs does not hurt test set accuracy or accuracy against a PGD adver-
sary

12

Published as a conference paper at ICLR 2019

B ADVERSARIAL TRAINING AND WEIGHT SPARSITY

It is worth noticing that adversarial training against (cid:96)∞ norm-bound adversaries alone already makes
networks easier to verify by implicitly improving weight sparsity. Indeed, this can be shown clearly
in the case of linear networks. Recall that a linear network can be expressed as f (x) = W x + b.
Thus, an (cid:96)∞ norm-bound perturbation of the input x will produce the output

f (x(cid:48)) = x(cid:48)W + b

= xW + b + (x(cid:48) − x)W
≤ f (x) + (cid:15)||W ||1

where the last inequality is just Hölder’s inequality.
In order to limit the adversary’s ability to
perturb the output, adversarial training needs to minimize the ||W ||1 term, which is equivalent to (cid:96)1-
regularization and is known to promote weight sparsity (Tibshirani, 1994). Relatedly, Goodfellow
et al. (2015) already pointed out that adversarial attacks against linear networks will be stronger
when the (cid:96)1-norm of the weight matrices is higher.

Even in the case of nonlinear networks, adversarial training has experimentally been shown to im-
prove weight sparsity. For example, models trained according to Madry et al. (2018) and Wong and
Kolter (2018) often learn many weight-sparse layers, and we observed similar trends in the mod-
els we trained. However, it is important to note that while adversarial training alone does improve
weight sparsity, it is not sufﬁcient by itself for efﬁcient exact veriﬁcation. Additional regularization
like (cid:96)1-regularization and small weight pruning further promotes weight sparsity and gives rise to
networks that are much easier to verify.

13

Published as a conference paper at ICLR 2019

C INTERVAL ARITHMETIC

C.1 NAIVE INTERVAL ARITHMETIC

Naive IA determines upper and lower bounds for a layer based solely on the upper and lower bounds
of the previous layer.
Deﬁne W + = max(W, 0), W − = min(W, 0), u = max(ˆu, 0), and l = max(ˆl, 0). Then the bounds
on the pre-activations of layer i can be computed as follows:
i + li−1W −
i + ui−1W −

ˆui = ui−1W +
ˆli = li−1W +

i + bi
i + bi

(8)

(7)

As noted in Tjeng et al. (2019) and also Dvijotham et al. (2018), this method is efﬁcient but can lead
to relatively conservative bounds for deeper networks.

C.2

IMPROVED INTERVAL ARITHMETIC

We improve upon naive IA by exploiting ReLUs that we can determine to always be active. This
allows us to cancel symbols that are equivalent that come from earlier layers of a network.

We will use a basic example of a neural network with one hidden layer to illustrate this idea. Suppose
that we have the scalar input z0 with l0 = 0, u0 = 1, and the network has the following weights and
biases:

W1 = [1 − 1] ,

b1 = [2

2] , W2 =

b2 = 0

(cid:21)
(cid:20)1
1

,

Naive IA for the ﬁrst layer gives ˆl1 = l1 = [2
2], and applying naive IA to
the output ˆz2 gives ˆl2 = 3, ˆu2 = 5. However, because ˆl1 > 0, we know that the two ReLUs in the
hidden layer are always active and thus equivalent to the identity function. Then, the output is

1], ˆu1 = u1 = [3

ˆz2 = z11 + z12 = ˆz11 + ˆz12 = (z0 + 2) + (−z0 + 2) = 4

Thus, we can obtain the tighter bounds ˆl2 = ˆu2 = 4, as we are able to cancel out the z0 terms.
We can write this improved version of IA as follows. First, letting Wk denote row k of matrix W ,
we can deﬁne the “active” part of W as the matrix WA, where

(WA)k =

(cid:40)

Wk
0

if ˆli−1 > 0
if ˆli−1 ≤ 0

Deﬁne the “non-active” part of W as

WN = W − WA
Then, using the same deﬁnitions for the notation W +, W −, u, l as before, we can write down the
following improved version of IA which uses information from the previous 2 layers.

ˆui = ui−1Wi

+
N + li−1Wi

−
N + bi
+ ui−2(Wi−1WiA)+ + li−2(Wi−1WiA)− + bi−1WiA
−
N + bi
+ li−2(Wi−1WiA)+ + ui−2(Wi−1WiA)− + bi−1WiA
We are forced to to use li−1,j and ui−1,j if we can not determine whether or not the ReLU corre-
sponding to the activation zi−1,j is active, but we use li−2 and ui−2 whenever possible.

+
N + ui−1Wi

ˆli = li−1Wi

We now deﬁne some additional notation to help us extend this method to any number of layers. We
now seek to deﬁne fn, which is a function which takes in four sequences of length n – upper bounds,
lower bounds, weights, and biases – and outputs the current layer’s upper and lower bounds.

What we have derived so far from (7) and (8) is the following

f1(ui−1, li−1, Wi, bi) = (ui−1W +

i + li−1W −

i + bi, li−1W +

i + ui−1W −

i + bi)

14

Published as a conference paper at ICLR 2019

Let u denote a sequence of upper bounds. Let uz denote element z of the sequence, and let u[z:]
denote the sequence without the ﬁrst z elements. Deﬁne notation for l, W, and b similarly.

Then, using the fact that WN Z = (W Z)N and WAZ = (W Z)A, we can show that the following
recurrence holds:

fn+1(u, l, W, b) = f1(u1, l1, W1N , b1)

+ fn(u[1:], l[1:], (W2W1A, W[2:]), (b2W1A, b[2:]))

(9)

Let u(x,y) denote the sequence (ux, ux−1, · · · , uy), and deﬁne l(x,y), W(x,y), and b(x,y) similarly.
Then, if we want to compute the bounds on layer k using all information from the previous k layers,
we simply have to compute fk(u(k−1,0), l(k−1,0), W(k,1), b(k,1)).

From the recurrence 9, we see that using information from all previous layers to compute bounds for
layer k takes O(k) matrix-matrix multiplications. Thus, using information from all previous layers
to compute bounds for all layers of a d layer neural network only involves O(d2) additional matrix
multiplications, which is still reasonable for most DNNs. This method is still relatively efﬁcient
because it only involves matrix multiplications – however, needing to perform matrix-matrix multi-
plications as opposed to just matrix-vector multiplications results in a slowdown and higher memory
usage when compared to naive IA. We believe the improvement in the estimate of ReLU upper and
lower bounds is worth the time trade-off for most networks.

C.3 EXPERIMENTAL RESULTS ON IMPROVED IA AND NAIVE IA

In Table 4, we show empirical evidence that the number of unstable ReLUs in each layer of a MNIST
network, as estimated by improved IA, tracks the number of unstable ReLUs determined by the exact
veriﬁer quite well. We also present estimates determined via naive IA for comparison.

Dataset

Epsilon Training Estimation

Method Method

Unstable ReLUs Unstable ReLUs Unstable ReLUs
in 1st Layer

in 2nd Layer

in 3rd Layer

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

Control

+RS

Control

+RS

Control

+RS

Exact
61.14
Improved IA 61.14
61.14
Naive IA

21.64
Exact
Improved IA 21.64
21.64
Naive IA

Exact
17.47
Improved IA 17.47
17.47
Naive IA

Exact
29.91
Improved IA 29.91
29.91
Naive IA

Exact
36.76
Improved IA 36.76
36.76
Naive IA

24.43
Exact
Improved IA 24.43
24.43
Naive IA

185.30
185.96 (+0.4%)
188.44 (+1.7%)

31.73
43.40 (+36.8%)
69.96 (+120.5%)

64.73
64.80 (+0.1%)
65.34 (+0.9%)

14.67
18.97 (+29.4%)
33.51 (+128.5%)

37.92
48.88 (+28.9%)
69.75 (+84.0%)

24.05
28.40 (+18.1%)
40.47 (+68.3%)

28.64
31.19 (+8.9%)
32.13 (+12.2%)

83.42
83.44 (+0.02%)
83.52 (+0.1%)

40.74
46.00 (+12.9%)
48.27 (+18.5%)

142.95
142.95
142.95

54.47
54.47
54.47

48.47
48.47
48.47

Table 4: Comparison between the average number of unstable ReLUs as found by the exact veriﬁer
of Tjeng et al. (2019) and the estimated average number of unstable ReLUs found by improved IA
and naive IA. We compare these estimation methods on the control and “+RS” networks for MNIST
that we described in Section 3.3.4

15

Published as a conference paper at ICLR 2019

C.4 ON THE CONSERVATIVE NATURE OF IA BOUNDS

The upper and lower bounds we compute on each ReLU via either naive IA or improved IA are
conservative. Thus, every unstable ReLU will always be correctly labeled as unstable, while stable
ReLUs can be labeled as either stable or unstable. Importantly, every unstable ReLU, as estimated
by IA bounds, is correctly labeled and penalized by RS Loss. The trade-off is that stable ReLUs
mislabeled as unstable will also be penalized, which can be an unnecessary regularization of the
model.

In Table 5 we show empirically that we can achieve the following two objectives at once when using
RS Loss combined with IA bounds.

1. Reduce the number of ReLUs labeled as unstable by IA, which is an upper bound on the

true number of unstable ReLUs as determined by the exact veriﬁer.

2. Achieve similar test set accuracy and PGD adversarial accuracy as a model trained without

RS Loss.

Dataset

Epsilon Training
Method

Estimation
Method

Total Labeled
Unstable ReLUs Accuracy

Test Set

PGD Adversarial
Accuracy

MNIST (cid:15) = 0.1

Control
+RS

Improved IA 290.5
Improved IA 105.4

Control (Large) Naive IA
Naive IA
+RS (Large)

835.8
150.3

98.94%
98.68% (-0.26%)

95.12%
95.13% (+0.01%)

99.04%
98.95% (-0.09%)

96.32%
96.58% (+0.26%)

Table 5: The addition of RS Loss results in far fewer ReLUs labeled as unstable for both 3-layer and
6-layer (Large) networks. The decrease in test set accuracy as a result of this regularization is small.

Even though IA bounds are conservative, these results show that it is still possible to decrease the
number of ReLUs labeled as unstable by IA without signiﬁcantly degrading test set accuracy. When
comparing the Control and “+RS” networks for MNIST and (cid:15) = 0.1, adding RS Loss decreased
the average number of ReLUs labeled as unstable (using bounds from Improved IA) from 290.5 to
105.4 with just a 0.26% loss in test set accuracy. The same trend held for deeper, 6-layer networks,
even when the estimation method for upper and lower bounds was the more conservative Naive IA.

16

Published as a conference paper at ICLR 2019

D FULL EXPERIMENTAL SETUP

D.1 NETWORK TRAINING DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. Following the prior
examples of Wong and Kolter (2018) and Dvijotham et al. (2018), we introduced a small tweak
where we increased the adversary strength linearly from 0.01 to (cid:15) over ﬁrst half of training and kept
it at (cid:15) for the second half. We used this training schedule to improve convergence of the training
process.

For MNIST, we trained for 70 epochs using the Adam optimizer (Kingma and Ba, 2015) with a
learning rate of 1e−4 and a batch size of 32. For CIFAR, we trained for 250 epochs using the Adam
optimizer with a learning rate of 1e−4. When using naive IA, we used a batch size of 128, and
when using improved IA, we used a batch size of 16. We used a smaller batch size in the latter
case because improved IA incurs high RAM usage during training. To speed up training on CIFAR,
we only added in RS Loss regularization in the last 20% of the training process. Using this same
sped-up training method on MNIST did not signiﬁcantly affect the results.

Dataset

Epsilon

(cid:96)1 weight RS Loss weight

MNIST 0.1
MNIST 0.2
MNIST 0.3
CIFAR 2/255
CIFAR 8/255

2e−5
2e−5
2e−5
1e−5
1e−5

12e−5
1e−4
12e−5
1e−3
2e−3

Table 6: Weights chosen using line search for (cid:96)1 regularization and RS Loss in each setting

For each setting, we ﬁnd a suitable weight on RS Loss via line search. The same weight is used for
each ReLU. The ﬁve weights we chose are displayed above in Table 6, along with weights chosen
for (cid:96)1-regularization.

We also train “+RS” models using naive IA to show that our technique for inducing ReLU stability
can work while having small training time overhead – full details on “+RS (Naive IA)” networks are
in Appendix E.

D.2 VERIFIER OVERVIEW

The MILP-based exact veriﬁer of Tjeng et al. (2019), which we use, proceeds in two steps for every
input. They are the model-build step and the solve step.

First, the veriﬁer builds a MILP model based on the neural network and the input. In particular, the
veriﬁer will compute upper and lower bounds on each ReLU using a speciﬁc bound computation al-
gorithm. We chose the default bound computation algorithm in the code, which uses LP to compute
bounds. LP bounds are tighter than the bounds computed via IA, which is another option available in
the veriﬁer. The model-build step’s speed appeared to depend primarily on the tightening algorithm
(IA was faster than LP) and the number of variables in the MILP (which, in turn, depends on the
sparsity of the weights of the neural network). The veriﬁer takes advantage of these bounds by not
introducing a binary variables into the MILP formulation if it can determine that a particular ReLU
is stable. Thus, using LP as the tightening algorithm resulted in higher build times, but led to easier
MILP formulations.

Next, the veriﬁer solves the MILP using an off-the-shelf MILP solver. The solver we chose was
the commercial Gurobi Solver, which uses a branch-and-bound method for solving MILPs. The
solver’s speed appeared to depend primarily on the number of binary variables in the MILP (which
corresponds to the number of unstable ReLUs) as well as the total number of variables in the MILP
(which is related to the sparsity of the weight matrices). While these two numbers are strongly
correlated with solve times, some solves would still take a long time despite having few binary

17

Published as a conference paper at ICLR 2019

variables. Thus, understanding what other properties of neural networks correspond to MILPs that
are easy or hard to solve is an important area to explore further.

D.3 VERIFIER DETAILS

We used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019) using the default
settings of the code. We allotted 120 seconds for veriﬁcation of each input datapoint using the
default model build settings. We ran our experiments using the commercial Gurobi Solver (version
7.5.2), and model solves were parallelized over 8 CPU cores with Intel Xeon CPUs @ 2.20GHz
processors. We used computers with 8–32GB of RAM, depending on the size of the model being
veriﬁed. All computers used are part of an OpenStack network.

18

Published as a conference paper at ICLR 2019

E FULL EXPERIMENTAL VERIFICATION RESULTS

Dataset

Epsilon

Training
Method

Test
Set Adversarial
Accuracy

Accuracy

Total
Provable
Upper Adversarial Unstable
Accuracy
Bound

Avg
Avg
Build
Solve
ReLUs Time (s) Time (s)

PGD Veriﬁer

Adversarial Training*
+(cid:96)1-regularization
+Small Weight Pruning
+ReLU Pruning (Control)

MNIST (cid:15) = 0.1

Wong et al. (2018)***

+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

99.17%
99.00%
98.99%
98.94%
98.68%
98.53%
98.95%

98.40%
98.10%
98.08%
98.21%
95.06%

97.75%
97.33%
97.06%
97.54%

64.64%
61.12%
57.83%
61.41%

50.69%
40.45%
46.19%
42.81%

95.04% 96.00%
95.25% 95.98%
95.38% 94.93%
95.12% 94.45%
95.13% 94.38%
94.86% 94.54%
96.58% 95.60%

93.14% 90.71%
93.14% 89.98%
91.68% 88.87%
94.19% 90.40%
-
89.03%

91.64% 83.83%
92.05% 81.70%
89.19% 79.12%
93.25% 83.70%

51.58% 50.23%
49.92% 47.79%
47.03% 45.33%
50.61% 51.00%

31.28% 33.46%
26.78% 22.74%
29.66% 26.07%
28.69% 25.20%

19.00%
82.17%
89.13%
91.58%
94.33%
94.32%
95.60%

86.45%
89.79%
85.54%
89.10%
80.29%

77.99%
80.68%
76.70%
59.60%

45.53%
45.93%
44.44%
41.40%

7.09%
20.27%
18.90%
19.80%

1517.9
505.3
502.7
278.2
101.0
158.3
119.5

2970.43
21.99
11.71
6.43
0.49
0.96
0.27

198.3
108.4
217.2
133.0
-

160.9
101.5
179.0
251.2

360.0
234.3
170.1
196.7

665.9
54.2
277.8
246.5

9.41
1.13
8.50
2.93
-

11.80
2.78
6.43
37.45

21.75
13.50
6.30
29.88

82.91
22.33
33.63
20.14

650.93
79.13
19.30
9.61
4.98
4.82
156.74

7.15
4.43
4.67
171.10
-

5.14
4.34
4.00
166.39

66.42
52.58
47.11
335.97

73.28
38.84
23.66
401.72

Table 7: Full results on natural improvements from Table 1, control networks (which use all of the
natural improvements and ReLU pruning), and “+RS” networks from Tables 2 and 3. While we are
unable to determine the true adversarial accuracy, we provide two upper bounds and a lower bound.
Evaluations of robustness against a strong 40-step PGD adversary (PGD adversarial accuracy) gives
one upper bound, and the veriﬁer itself gives another upper bound because it can also prove that
the network is not robust to perturbations on certain inputs by ﬁnding adversarial examples. The
veriﬁer simultaneously ﬁnds the provable adversarial accuracy, which is a lower bound on the true
adversarial accuracy. The gap between the veriﬁer upper bound and the provable adversarial accu-
racy (veriﬁer lower bound) corresponds to inputs that the veriﬁer times out on. These are inputs that
the veriﬁer can not prove to be robust or not robust in 120 seconds. Build times and solve times
are reported in seconds. Finally, average solve time includes timeouts. In other words, veriﬁcation
solves that time out contribute 120 seconds to the total solve time.
* The “Adversarial Training” network uses a 3600 instead of 120 second timeout and is only veriﬁed
for the ﬁrst 100 images because verifying it took too long.
** The “+RS (Large)” networks are only veriﬁed for the ﬁrst 1000 images because of long build
times.
*** Wong et al. (2018); Dvijotham et al. (2018), and Mirman et al. (2018), which we compare to in
Table 3, do not report results on MNIST, (cid:15) = 0.2 in their papers. We ran the publicly available code
of Wong et al. (2018) on MNIST, (cid:15) = 0.2 to generate these results for comparison.

19

Published as a conference paper at ICLR 2019

F DISCUSSION ON VERIFICATION AND CERTIFICATION

Exact veriﬁcation and certiﬁcation are two related approaches to formally verifying properties of
neural networks, such as adversarial robustness.
In both cases, the end goal is formal veriﬁca-
tion. Certiﬁcation methods, which solve an easier-to-solve relaxation of the exact veriﬁcation prob-
lem, are important developments because exact veriﬁcation previously appeared computationally
intractable for all but the smallest models.

For the case of adversarial robustness, certiﬁcation methods exploit a trade-off between provable
robustness and speed. They can fail to provide certiﬁcates of robustness for some inputs that are
actually robust, but they will either ﬁnd or fail to ﬁnd certiﬁcates of robustness quickly. On the other
hand, exact veriﬁers will always give the correct answer if given enough time, but exact veriﬁers can
sometimes take many hours to formally verify robustness on even a single input.

In general, the process of training a robust neural network and then formally verifying its robustness
happens in two steps.

• Step 1: Training
• Step 2: Veriﬁcation or Certiﬁcation

Most papers on certiﬁcation, including Wong and Kolter (2018); Wong et al. (2018); Dvijotham
et al. (2018); Raghunathan et al. (2018) and Mirman et al. (2018), propose a method for step 2
(the certiﬁcation step), and then propose a training objective in step 1 that is directly related to
their method for step 2. We call this paradigm “co-training.” In Raghunathan et al. (2018), they
found that using their step 2 on a model trained using Wong and Kolter (2018)’s step 1 resulted in
extremely poor provable robustness (less than 10%), and the same was true when using Wong and
Kolter (2018)’s step 2 on a model trained using their step 1.

We focus on MILP-based exact veriﬁcation as our step 2, which encompasses the best current exact
veriﬁcation methods. The advantage of using exact veriﬁcation for step 2 is that it will be accurate,
regardless of what method is used in step 1. The disadvantage of using exact veriﬁcation for step
2 is that it could be extremely slow. For our step 1, we used standard robust adversarial training.
In order to signiﬁcantly speed up exact veriﬁcation as step 2, we proposed techniques that could be
added to step 1 to induce weight sparsity and ReLU stability.

In general, we believe it is important to develop effective methods for step 1, given that step 2 is
exact veriﬁcation. However, ReLU stability can also be beneﬁcial for tightening the relaxation of
certiﬁcation approaches like that of Wong et al. (2018) and Dvijotham et al. (2018), as unstable
ReLUs are the primary cause of the overapproximation that occurs in the relaxation step. Thus, our
techniques for inducing ReLU stability can be useful for certiﬁcation as well.

Finally, in recent literature on veriﬁcation and certiﬁcation, most works have focused on formally
verifying the property of adversarial robustness of neural networks. However, veriﬁcation of other
properties could be useful, and our techniques to induce weight sparsity and ReLU stability would
still be useful for veriﬁcation of other properties for the exact same reasons that they are useful in
the case of adversarial robustness.

20

9
1
0
2
 
r
p
A
 
3
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
0
0
3
0
.
9
0
8
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2019

TRAINING FOR FASTER ADVERSARIAL ROBUSTNESS
VERIFICATION VIA INDUCING RELU STABILITY

Kai Y. Xiao Vincent Tjeng Nur Muhammad (Mahi) Shaﬁullah Aleksander M ˛adry
Massachusetts Institute of Technology
Cambridge, MA 02139
{kaix, vtjeng, nshafiul, madry}@mit.edu

ABSTRACT

We explore the concept of co-design in the context of neural network veriﬁcation.
Speciﬁcally, we aim to train deep neural networks that not only are robust to ad-
versarial perturbations but also whose robustness can be veriﬁed more easily. To
this end, we identify two properties of network models – weight sparsity and so-
called ReLU stability – that turn out to signiﬁcantly impact the complexity of the
corresponding veriﬁcation task. We demonstrate that improving weight sparsity
alone already enables us to turn computationally intractable veriﬁcation problems
into tractable ones. Then, improving ReLU stability leads to an additional 4–13x
speedup in veriﬁcation times. An important feature of our methodology is its “uni-
versality,” in the sense that it can be used with a broad range of training procedures
and veriﬁcation approaches.

1

INTRODUCTION

Deep neural networks (DNNs) have recently achieved widespread success in image classiﬁcation
(Krizhevsky et al., 2012), face and speech recognition (Taigman et al., 2014; Hinton et al., 2012),
and game playing (Silver et al., 2016; 2017). This success motivates their application in a broader
set of domains, including more safety-critical environments. This thrust makes understanding the
reliability and robustness of the underlying models, let alone their resilience to manipulation by
malicious actors, a central question. However, predictions made by machine learning models are
often brittle. A prominent example is the existence of adversarial examples (Szegedy et al., 2014):
imperceptibly modiﬁed inputs that cause state-of-the-art models to misclassify with high conﬁdence.

There has been a long line of work on both generating adversarial examples, called attacks (Carlini
and Wagner, 2017b;a; Athalye et al., 2018a;b; Uesato et al., 2018; Evtimov et al., 2017), and training
models robust to adversarial examples, called defenses (Goodfellow et al., 2015; Papernot et al.,
2016; Madry et al., 2018; Kannan et al., 2018). However, recent research has shown that most
defenses are ineffective (Carlini and Wagner, 2017a; Athalye et al., 2018a; Uesato et al., 2018).
Furthermore, even for defenses such as that of Madry et al. (2018) that have seen empirical success
against many attacks, we are unable to conclude yet with certainty that they are robust to all attacks
that we want these models to be resilient to.

This state of affairs gives rise to the need for veriﬁcation of networks, i.e., the task of formally
proving that no small perturbations of a given input can cause it to be misclassiﬁed by the network
model. Although many exact veriﬁers1 have been designed to solve this problem, the veriﬁcation
process is often intractably slow. For example, when using the Reluplex veriﬁer of Katz et al. (2017),
even verifying a small MNIST network turns out to be computationally infeasible. Thus, addressing
this intractability of exact veriﬁcation is the primary goal of this work.

Our Contributions
Our starting point is the observation that, typically, model training and veriﬁcation are decoupled
and seen as two distinct steps. Even though this separation is natural, it misses a key opportunity:
the ability to align these two stages. Speciﬁcally, applying the principle of co-design during model

1Also sometimes referred to as combinatorial veriﬁers.

1

Published as a conference paper at ICLR 2019

training is possible: training models in a way to encourage them to be simultaneously robust and
easy-to-exactly-verify. This insight is the cornerstone of the techniques we develop in this paper.

In this work, we use the principle of co-design to develop training techniques that give models
that are both robust and easy-to-verify. Our techniques rely on improving two key properties of
networks: weight sparsity and ReLU stability. Speciﬁcally, we ﬁrst show that natural methods for
improving weight sparsity during training, such as (cid:96)1-regularization, give models that can already be
veriﬁed much faster than current methods. This speedup happens because in general, exact veriﬁers
beneﬁt from having fewer variables in their formulations of the veriﬁcation task. For instance, for
exact veriﬁers that rely on linear programming (LP) solvers, sparser weight matrices means fewer
variables in those constraints.

We then focus on the major speed bottleneck of current approaches to exact veriﬁcation of ReLU
networks: the need of exact veriﬁcation methods to “branch,” i.e., consider two possible cases for
each ReLU (ReLU being active or inactive). Branching drastically increases the complexity of
veriﬁcation. Thus, well-optimized veriﬁers will not need to branch on a ReLU if it can determine
that the ReLU is stable, i.e.
that the ReLU will always be active or always be inactive for any
perturbation of an input. This motivates the key goal of the techniques presented in this paper: we
aim to minimize branching by maximizing the number of stable ReLUs. We call this goal ReLU
stability and introduce a regularization technique to induce it.

Our techniques enable us to train weight-sparse and ReLU stable networks for MNIST and CIFAR-
10 that can be veriﬁed signiﬁcantly faster. Speciﬁcally, by combining natural methods for inducing
weight sparsity with a robust adversarial training procedure (cf. Goodfellow et al. (2015)), we are
able to train networks for which almost 90% of inputs can be veriﬁed in an amount of time that
is small2 compared to previous veriﬁcation techniques. Then, by also adding our regularization
technique for inducing ReLU stability, we are able to train models that can be veriﬁed an addi-
tional 4–13x times as fast while maintaining state-of-the-art accuracy on MNIST. Our techniques
show similar improvements for exact veriﬁcation of CIFAR models. In particular, we achieve the
following veriﬁcation speed and provable robustness results for (cid:96)∞ norm-bound adversaries:

Dataset

Epsilon

Provable Adversarial Accuracy Average Solve Time (s)

MNIST

(cid:15) = 0.1
(cid:15) = 0.2
(cid:15) = 0.3

CIFAR

(cid:15) = 2/255
(cid:15) = 8/255

94.33%
89.79%
80.68%

45.93%
20.27%

0.49
1.13
2.78

13.50
22.33

Our network for (cid:15) = 0.1 on MNIST achieves provable adversarial accuracies comparable with the
current best results of Wong et al. (2018) and Dvijotham et al. (2018), and our results for (cid:15) = 0.2 and
(cid:15) = 0.3 achieve the best provable adversarial accuracies yet. To the best of our knowledge, we also
achieve the ﬁrst nontrivial provable adversarial accuracy results using exact veriﬁers for CIFAR-10.

Finally, we design our training techniques with universality as a goal. We focus on improving the
input to the veriﬁcation process, regardless of the veriﬁer we end up using. This is particularly
important because research into network veriﬁcation methods is still in its early stages, and our co-
design methods are compatible with the best current veriﬁers (LP/MILP-based methods) and should
be compatible with any future improvements in veriﬁcation.

Our code is available at https://github.com/MadryLab/relu_stable.

2 BACKGROUND AND RELATED WORK

Exact veriﬁcation of networks has been the subject of many recent works (Katz et al., 2017; Ehlers,
2017; Carlini et al., 2017; Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). To
understand the context of these works, observe that for linear networks, the task of exact veriﬁcation
is relatively simple and can be done by solving a LP. For more complex models, the presence of
nonlinear ReLUs makes veriﬁcation over all perturbations of an input much more challenging. This

2We chose our time budget for veriﬁcation to be 120 seconds per input image.

2

Published as a conference paper at ICLR 2019

is so as ReLUs can be active or inactive depending on the input, which can cause exact veriﬁers
to “branch" and consider these two cases separately. The number of such cases that veriﬁers have
to consider might grow exponentially with the number of ReLUs, so veriﬁcation speed will also
grow exponentially in the worst case. Katz et al. (2017) further illustrated the difﬁculty of exact
veriﬁcation by proving that it is NP-complete. In recent years, formal veriﬁcation methods were
developed to verify networks. Most of these methods use satisﬁability modulo theory (SMT) solvers
(Katz et al., 2017; Ehlers, 2017; Carlini et al., 2017) or LP and Mixed-Integer Linear Programming
(MILP) solvers (Tjeng et al., 2019; Lomuscio and Maganti, 2017; Cheng et al., 2017a). However,
all of them are limited by the same issue of scaling poorly with the number of ReLUs in a network,
making them prohibitively slow in practice for even medium-sized models.

One recent approach for dealing with the inefﬁciency of exact veriﬁers is to focus on certiﬁcation
methods3 (Wong and Kolter, 2018; Wong et al., 2018; Dvijotham et al., 2018; Raghunathan et al.,
2018; Mirman et al., 2018; Sinha et al., 2018). In contrast to exact veriﬁcation, these methods do not
solve the veriﬁcation task directly; instead, they rely on solving a relaxation of the veriﬁcation prob-
lem. This relaxation is usually derived by overapproximating the adversarial polytope, or the space
of outputs of a network for a region of possible inputs. These approaches rely on training models in
a speciﬁc manner that makes certiﬁcation of those models easier. As a result, they can often obtain
provable adversarial accuracy results faster. However, certiﬁcation is fundamentally different from
veriﬁcation in two primary ways. First, it solves a relaxation of the original veriﬁcation problem. As
a result, certiﬁcation methods can fail to certify many inputs that are actually robust to perturbations
– only exact veriﬁers, given enough time, can give conclusive answers on robustness for every single
input. Second, certiﬁcation approaches fall under the paradigm of co-training, where a certiﬁcation
method only works well on models speciﬁcally trained for that certiﬁcation step. When used as a
black box on arbitrary models, the certiﬁcation step can yield a high rate of false negatives. For
example, Raghunathan et al. (2018) found that their certiﬁcation step was signiﬁcantly less effective
when used on a model trained using Wong and Kolter (2018)’s training method, and vice versa. In
contrast, we design our methods to be universal. They can be combined with any standard train-
ing procedure for networks and will improve exact veriﬁcation speed for any LP/MILP-based exact
veriﬁer. Our methods can also decrease the amount of overapproximation incurred by certiﬁcation
methods like Wong and Kolter (2018); Dvijotham et al. (2018). Similar to most of the certiﬁcation
methods, our technique can be made to have very little training time overhead.

Finally, subsequent work of Gowal et al. (2018) shows how applying interval bound propagation
during training, combined with MILP-based exact veriﬁcation, can lead to provably robust networks.

3 TRAINING VERIFIABLE NETWORK MODELS

We begin by discussing the task of verifying a network and identify two key properties of networks
that lead to improved veriﬁcation speed: weight sparsity and so-called ReLU stability. We then use
natural regularization methods for inducing weight sparsity as well as a new regularization method
for inducing ReLU stability. Finally, we demonstrate that these methods signiﬁcantly speed up
veriﬁcation while maintaining state-of-the-art accuracy.

3.1 VERIFYING ADVERSARIAL ROBUSTNESS OF NETWORK MODELS

Deep neural network models. Our focus will be on one of the most common architectures for
state-of-the-art models: k-layer fully-connected feed-forward DNN classiﬁers with ReLU non-
linearities4. Such models can be viewed as a function f (·, W, b), where W and b represent the
weight matrices and biases of each layer. For an input x, the output f (x, W, b) of the DNN is

3These works use both “veriﬁcation” and “certiﬁcation” to describe their methods. For clarity, we use “cer-
tiﬁcation” to describe their approaches, while we use “veriﬁcation” to describe exact veriﬁcation approaches.
For a more detailed discussion of the differences, see Appendix F.

4Note that this encompasses common convolutional network architectures because every convolutional layer

can be replaced by an equivalent fully-connected layer.

3

Published as a conference paper at ICLR 2019

deﬁned as:

z0 = x
ˆzi = zi−1Wi + bi
zi = max( ˆzi, 0)

(1)
(2)
(3)
(4)
Here, for each layer i, we let ˆzij denote the jth ReLU pre-activation and let ˆzij(x) denote the value
of ˆzij on an input x. ˆzk−1 is the ﬁnal, output layer with an output unit for each possible label (the
logits). The network will make predictions by selecting the label with the largest logit.

for i = 1, 2, . . . , k − 1
for i = 1, 2, . . . , k − 2

f (x, W, b) = ˆzk−1

Adversarial robustness. For a network to be reliable, it should make predictions that are robust
– that is, it should predict the same output for inputs that are very similar. Speciﬁcally, we want
the DNN classiﬁer’s predictions to be robust to a set Adv(x) of possible adversarial perturbations
of an input x. We focus on (cid:96)∞ norm-bound adversarial perturbations, where Adv(x) = {x(cid:48)
:
||x(cid:48) − x||∞ ≤ (cid:15)} for some constant (cid:15), since it is the most common one considered in adversarial
robustness and veriﬁcation literature (thus, it currently constitutes a canonical benchmark). Even so,
our methods can be applied to other (cid:96)p norms and broader sets of perturbations.
Veriﬁcation of network models. For an input x with correct label y, a perturbed input x(cid:48) can cause
a misclassiﬁcation if it makes the logit of some incorrect label ˆy larger than the logit of y on x(cid:48). We
can thus express the task of ﬁnding an adversarial perturbation as the optimization problem:

f (x(cid:48), W )y − f (x(cid:48), W )ˆy

min
x(cid:48),ˆy
subject to x(cid:48) ∈ Adv(x)

An adversarial perturbation exists if and only if the objective of the optimization problem is negative.

Adversarial accuracies. We deﬁne the true adversarial accuracy of a model to be the fraction of
test set inputs for which the model is robust to all allowed perturbations. By deﬁnition, evaluations
against speciﬁc adversarial attacks like PGD or FGSM provide an upper bound to this accuracy,
while certiﬁcation methods provide lower bounds. Given sufﬁcient time for each input, an exact
veriﬁer can prove robustness to perturbations, or ﬁnd a perturbation where the network makes a
misclassiﬁcation on the input, and thus exactly determine the true adversarial accuracy. This is in
contrast to certiﬁcation methods, which solve a relaxation of the veriﬁcation problem and can not
exactly determine the true adversarial accuracy no matter how much time they have.

However, such exact veriﬁcation may take impractically long for certain inputs, so we instead com-
pute the provable adversarial accuracy, which we deﬁne as the fraction of test set inputs for which
the veriﬁer can prove robustness to perturbations within an allocated time budget (timeout). Simi-
larly to certiﬁable accuracy, this accuracy constitutes a lower bound on the true adversarial accuracy.
A model can thus, e.g., have high true adversarial accuracy and low provable adversarial accuracy if
veriﬁcation of the model is too slow and often fails to complete before timeout.

Also, in our evaluations, we chose to use the MILP exact veriﬁer of Tjeng et al. (2019) when per-
forming experiments, as it is both open source and the fastest veriﬁer we are aware of.

3.2 WEIGHT SPARSITY AND ITS IMPACT ON VERIFICATION SPEED

The ﬁrst property of network models that we want to improve in order to speed up exact veriﬁcation
of those models is weight sparsity. Weight sparsity is important for veriﬁcation speed because many
exact veriﬁers rely on solving LP or MILP systems, which beneﬁt from having fewer variables.
We use two natural regularization methods for improving weight sparsity: (cid:96)1-regularization and
small weight pruning. These techniques signiﬁcantly improve veriﬁcation speed – see Table 1.
Verifying even small MNIST networks is almost completely intractable without them. Speciﬁcally,
the veriﬁer can only prove robustness of an adversarially-trained model on 19% of inputs with a one
hour budget per input, while the veriﬁer can prove robustness of an adversarially-trained model with
(cid:96)1-regularization and small weight pruning on 89.13% of inputs with a 120 second budget per input.

Interestingly, even though adversarial training improves weight sparsity (see Appendix B) it was
still necessary to use (cid:96)1-regularization and small weight pruning. These techniques further promoted
weight sparsity and gave rise to networks that were much easier to verify.

4

Published as a conference paper at ICLR 2019

Dataset

Epsilon

Training
Method

Test Set
Accuracy

Provable Adversarial
Accuracy

Average
Solve Time (s)

MNIST (cid:15) = 0.1

1 Adversarial Training
+(cid:96)1-Regularization
2
+Small Weight Pruning
3
4

+ReLU Pruning (control)

99.17%
99.00%
98.99%
98.94%

19.00%
82.17%
89.13%
91.58%

2970.43
21.99
11.71
6.43

Table 1: Improvement in provable adversarial accuracy and veriﬁcation solve times when incremen-
tally adding natural regularization methods for improving weight sparsity and ReLU stability into
the model training procedure, before veriﬁcation occurs. Each row represents the addition of another
method – for example, Row 3 uses adversarial training, (cid:96)1-regularization, and small weight pruning.
Row 4 adds ReLU pruning (see Appendix A). Row 4 is the control model for MNIST and (cid:15) = 0.1
that we present again in comparisons in Tables 2 and 3. We use a 3600 instead of 120 second timeout
for Row 1 and only veriﬁed the ﬁrst 100 images (out of 10000) because verifying it took too long.

3.3 RELU STABILITY

Next, we target the primary speed bottleneck of exact veriﬁcation: the number of ReLUs the veriﬁer
has to branch on. In our paper, this corresponds to the notion of inducing ReLU stability. Before we
describe our methodology, we formally deﬁne ReLU stability.

(2)).

Given an input x, a set of allowed perturbations Adv(x), and a ReLU, exact veriﬁers may need
to branch based on the possible pre-activations of the ReLU, namely ˆzij(Adv(x)) = {ˆzij(x(cid:48)) :
x(cid:48) ∈ Adv(x)} (cf.
If there exist two perturbations x(cid:48), x(cid:48)(cid:48) in the set Adv(x) such that
sign(ˆzij(x(cid:48))) (cid:54)= sign(ˆzij(x(cid:48)(cid:48))), then the veriﬁer has to consider that for some perturbed inputs the
ReLU is active (zij = ˆzij) and for other perturbed inputs inactive (zij = 0). The more such cases
the veriﬁer faces, the more branches it has to consider, causing the complexity of veriﬁcation to in-
crease exponentially. Intuitively, a model with 1000 ReLUs among which only 100 ReLUs require
branching will likely be much easier to verify than a model with 200 ReLUs that all require branch-
ing. Thus, it is advantageous for the veriﬁer if, on an input x with allowed perturbation set Adv(x),
the number of ReLUs such that

sign(ˆzij(x(cid:48))) = sign(ˆzij(x)) ∀x(cid:48) ∈ Adv(x)
is maximized. We call a ReLU for which (5) holds on an input x a stable ReLU on that input. If (5)
does not hold, then the ReLU is an unstable ReLU.

(5)

Directly computing whether a ReLU is stable on a given input x is difﬁcult because doing so would
involve considering all possible values of ˆzij(Adv(x)). Instead, exact veriﬁers compute an upper
bound ˆuij and a lower bound ˆlij of ˆzij(Adv(x)). If 0 ≤ ˆlij or ˆuij ≤ 0, they can replace the ReLU
with the identity function or the zero function, respectively. Otherwise, if ˆlij < 0 < ˆuij, these
veriﬁers then determine that they need to “branch” on that ReLU. Thus, we can rephrase (5) as

sign(ˆuij) = sign(ˆlij)

(6)

We will discuss methods for determining these upper and lower bounds ˆuij, ˆlij in Section 3.3.2.

3.3.1 A REGULARIZATION TECHNIQUE FOR INDUCING RELU STABILITY: RS LOSS

As we see from equation (6), a function that would indicate exactly when a ReLU is stable is
F ∗(ˆuij, ˆlij) = sign(ˆuij) · sign(ˆlij). Thus, it would be natural to use this function as a regular-
izer. However, this function is non-differentiable and if used in training a model, would provide no
useful gradients during back-propagation. Thus, we use the following smooth approximation of F ∗
(see Fig. 1) which provides the desired gradients:

F (ˆuij, ˆlij) = − tanh(1 + ˆuij · ˆlij)

We call the corresponding objective RS Loss, and show in Fig. 2a that using this loss function as
a regularizer effectively decreases the number of unstable ReLUs. RS Loss thus encourages ReLU
stability, which, in turn, speeds up exact veriﬁcation - see Fig. 2b.

5

Published as a conference paper at ICLR 2019

Figure 1: Plot and contour plot of the function F (x, y) = − tanh(1 + x · y)

3.3.2 ESTIMATING RELU UPPER AND LOWER BOUNDS ON ACTIVATIONS

A key aspect of using RS Loss is determining the upper and lower bounds ˆuij, ˆlij for each ReLU (cf.
(6)). The bounds for the inputs z0 (cf. (1)) are simple – for the input x, we know x − (cid:15) ≤ z0 ≤ x + (cid:15),
so ˆu0 = x − (cid:15), ˆl0 = x + (cid:15). For all subsequent zij, we estimate bounds using either the naive interval
arithmetic (IA) approach described in Tjeng et al. (2019) or an improved version of it. The improved
version is a tighter estimate but uses more memory and training time, and thus is most effective on
smaller networks. We present the details of naive IA and improved IA in Appendix C.

Interval arithmetic approaches can be implemented relatively efﬁciently and work well with back-
propagation because they only involve matrix multiplications. This contrasts with how exact veri-
ﬁers compute these bounds, which usually involves solving LPs or MILPs. Interval arithmetic also
overestimates the number of unstable ReLUs. This means that minimizing unstable ReLUs based
on IA bounds will provide an upper bound on the number of unstable ReLUs determined by exact
veriﬁers. In particular, IA will properly penalize every unstable ReLU.

Improved IA performs well in practice, overestimating the number of unstable ReLUs by less than
0.4% in the ﬁrst 2 layers of MNIST models and by less than 36.8% (compared to 128.5% for naive
IA) in the 3rd layer. Full experimental results are available in Table 4 of Appendix C.3.

3.3.3

IMPACT OF RELU STABILITY IMPROVEMENTS ON VERIFICATION SPEED

We provide experimental evidence that RS Loss regularization improves ReLU stability and speeds
up average veriﬁcation times by more than an order of magnitude in Fig. 2b. To isolate the effect of
RS Loss, we compare MNIST models trained in exactly the same way other than the weight on RS
Loss. When comparing a network trained with a RS Loss weight of 5e−4 to a network with a RS
Loss weight of 0, the former has just 16% as many unstable ReLUs and can be veriﬁed 65x faster.
The caveat here is that the former has 1.00% lower test set accuracy.

We also compare veriﬁcation speed with and without RS Loss on MNIST networks for different
values of (cid:15) (0.1, 0.2, and 0.3) in Fig. 2c. We choose RS Loss weights that cause almost no test set
accuracy loss (less than 0.50% - See Table 3) in these cases, and we still observe a 4–13x speedup
from RS Loss. For CIFAR, RS Loss gives a smaller speedup of 1.6–3.7x (See Appendix E).

3.3.4

IMPACT OF RELU STABILITY IMPROVEMENTS ON PROVABLE ADVERSARIAL
ACCURACY

As the weight on the RS Loss used in training a model increases, the ReLU stability of the model
will improve, speeding up veriﬁcation and likely improving provable adversarial accuracy. However,
like most regularization methods, placing too much weight on RS Loss can decrease the model ca-
pacity, potentially lowering both the true adversarial accuracy and the provable adversarial accuracy.
Therefore, it is important to choose the weight on RS Loss carefully to obtain both high provable
adversarial accuracy and faster veriﬁcation speeds.

To show the effectiveness of RS Loss in improving provable adversarial accuracy, we train two
networks for each dataset and each value of (cid:15). One is a “control” network that uses all of the natural
improvements for inducing both weight sparsity ((cid:96)1-regularization and small weight pruning) and
ReLU stability (ReLU pruning - see Appendix A). The second is a “+RS” network that uses RS

6

Published as a conference paper at ICLR 2019

(a)

(b)

(c)

Figure 2: (a) Average number of unstable ReLUs by layer and (b) average veriﬁcation solve times
of 6 networks trained with different weights on RS Loss for MNIST and (cid:15) = 0.1 . Averages are
taken over all 10000 MNIST test set inputs. Both metrics improve signiﬁcantly with increasing
RS Loss weight. An RS Loss weight of 0 corresponds to the control network, while an RS Loss
weight of 0.00012 corresponds to the “+RS” network for MNIST and (cid:15) = 0.1 in Tables 2 and 3. (c)
Improvement in the average time taken by a veriﬁer to solve the veriﬁcation problem after adding
RS Loss to the training procedure, for different (cid:15) on MNIST. The weight on RS Loss was chosen so
that the “+RS” models have test set accuracies within 0.50% of the control models.

Loss in addition to all of the same natural improvements. This lets us isolate the incremental effect
of adding RS Loss to the training procedure.

In addition to attaining a 4–13x speedup in MNIST veriﬁcation times (see Fig. 2c), we achieve
higher provable adversarial accuracy in every single setting when using RS Loss. This is especially
notable for the hardest veriﬁcation problem we tackle – proving robustness to perturbations with (cid:96)∞
norm-bound 8/255 on CIFAR-10 – where adding RS Loss nearly triples the provable adversarial
accuracy from 7.09% to 20.27%. This improvement is primarily due to veriﬁcation speedup induced
by RS Loss, which allows the veriﬁer to ﬁnish proving robustness for far more inputs within the 120
second time limit. These results are shown in Table 2.

Table 2: Provable Adversarial Accuracies for the control and “+RS” networks in each setting.

MNIST, (cid:15) = 0.1 MNIST, (cid:15) = 0.2 MNIST, (cid:15) = 0.3 CIFAR, (cid:15) = 2/255 CIFAR, (cid:15) = 8/255

Control
+RS

91.58
94.33

86.45
89.79

77.99
80.68

45.53
45.93

7.09
20.27

4 EXPERIMENTS

4.1 EXPERIMENTS ON MNIST AND CIFAR

In addition to the experimental results already presented, we compare our control and “+RS” net-
works with the best available results presented in the state-of-the-art certiﬁable defenses of Wong
et al. (2018), Dvijotham et al. (2018), and Mirman et al. (2018) in Table 3. We compare their test
set accuracy, PGD adversarial accuracy (an evaluation of robustness against a strong 40-step PGD
adversarial attack), and provable adversarial accuracy. Additionally, to show that our method can
scale to larger architectures, we train and verify a “+RS (Large)” network for each dataset and (cid:15).

In terms of provable adversarial accuracies, on MNIST, our results are signiﬁcantly better than those
of Wong et al. (2018) for larger perturbations of (cid:15) = 0.3, and comparable for (cid:15) = 0.1. On CIFAR-
10, our method is slightly less effective, perhaps indicating that more unstable ReLUs are necessary
to properly learn a robust CIFAR classiﬁer. We also experienced many more instances of the veriﬁer
reaching its allotted 120 second time limit on CIFAR, especially for the less ReLU stable control
networks. Full experimental details for each model in Tables 1, 2, and 3, including a breakdown of
veriﬁcation solve results (how many images did the veriﬁer: A. prove robust B. ﬁnd an adversarial
example for C. time out on), are available in Appendix E.

7

Published as a conference paper at ICLR 2019

Table 3: Comparison of test set, PGD adversarial, and provable adversarial accuracy of networks
trained with and without RS Loss. We also provide the best available certiﬁable adversarial and
PGD adversarial accuracy of any single models from Wong et al. (2018), Dvijotham et al. (2018),
and Mirman et al. (2018) for comparison, and highlight the best provable accuracy for each (cid:15).
* The provable adversarial accuracy for “+RS (Large)” is only computed for the ﬁrst 1000 images
because the veriﬁer performs more slowly on larger models.
** Dvijotham et al. (2018); Mirman et al. (2018) use a slightly smaller (cid:15) = 0.03 = 7.65/255.
† Mirman et al. (2018) computes results over 500 images instead of all 10000.
†† Mirman et al. (2018) uses a slightly smaller (cid:15) = 0.007 = 1.785/255.

Dataset

Epsilon

Test Set
Accuracy

PGD Adversarial

Provable/Certiﬁable
Accuracy Adversarial Accuracy

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

Training
Method

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†

Control
+RS
+RS (Large)*

Wong et al.
Mirman et al.†,††

Control
+RS
+RS (Large)*

Wong et al.
Dvijotham et al.**
Mirman et al.†, **

98.94%
98.68%
98.95%

98.92%
98.80%
99.00%

98.40%
98.10%
98.21%

97.75%
97.33%
97.54%

85.13%
96.60%

64.64%
61.12%
61.41%

68.28%
62.00%

50.69%
40.45%
42.81%

28.67%
48.64%
54.20%

95.12%
95.13%
96.58%

-
97.13%
97.60%

93.14%
93.14%
94.19%

91.64%
92.05%
93.25%

-
93.80%

51.58%
49.92%
50.61%

-
54.60%

31.28%
26.78%
28.69%

-
32.72%
40.00%

91.58%
94.33%
95.60%

96.33%
95.56%
96.60%

86.45%
89.79%
89.10%

77.99%
80.68%
59.60%

56.90%
82.00%

45.53%
45.93%
41.40%

53.89%
52.20%

7.09%
20.27%
19.80%

21.78%
26.67%
35.20%

4.2 EXPERIMENTAL METHODS AND DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. For each setting of dataset
(MNIST or CIFAR) and (cid:15), we ﬁnd a suitable weight on RS Loss via line search (See Table 6 in
Appendix D). The same weight is used for each ReLU. During training, we used improved IA for
ReLU bound estimation for “+RS” models and use naive IA for “+RS (Large)” models because of
memory constraints. For ease of comparison, we trained our networks using the same convolutional
DNN architecture as in Wong et al. (2018). This architecture uses two 2x2 strided convolutions with
16 and 32 ﬁlters, followed by a 100 hidden unit fully connected layer. For the larger architecture,
we also use the same “large” architecture as in Wong et al. (2018). It has 4 convolutional layers with
32, 32, 64, and 64 ﬁlters, followed by 2 fully connected layers with 512 hidden units each.

For veriﬁcation, we used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019).
Model solves were parallelized over 8 CPU cores. When verifying an image, the veriﬁer of Tjeng
et al. (2019) ﬁrst builds a model, and second, solves the veriﬁcation problem (See Appendix D.2 for
details). We focus on reporting solve times because that is directly related to the task of veriﬁcation
itself. All build times for the control and “+RS” models on MNIST that we presented were between
4 and 10 seconds, and full results on build times are also presented in Appendix E.

Additional details on our experimental setup (e.g. hyperparameters) can be found in Appendix D.

8

Published as a conference paper at ICLR 2019

5 CONCLUSION

In this paper, we use the principle of co-design to develop training methods that emphasize veri-
ﬁcation as a goal, and we show that they make verifying the trained model much faster. We ﬁrst
demonstrate that natural regularization methods already make the exact veriﬁcation problem sig-
niﬁcantly more tractable. Subsequently, we introduce the notion of ReLU stability for networks,
present a method that improves a network’s ReLU stability, and show that this improvement makes
veriﬁcation an additional 4–13x faster. Our method is universal, as it can be added to any training
procedure and should speed up any exact veriﬁcation procedure, especially MILP-based methods.

Prior to our work, exact veriﬁcation seemed intractable for all but the smallest models. Thus, our
work shows progress toward reliable models that can be proven to be robust, and our techniques can
help scale veriﬁcation to even larger networks.

Many of our methods appear to compress our networks into more compact, simpler forms. We
hypothesize that the reason that regularization methods like RS Loss can still achieve very high
accuracy is that most models are overparametrized in the ﬁrst place. There exist clear parallels
between our methods and techniques in model compression (Han et al., 2016; Cheng et al., 2017b) –
therefore, we believe that drawing upon additional techniques from model compression can further
improve the ease-of-veriﬁcation of networks. We also expect that there exist objectives other than
weight sparsity and ReLU stability that are important for veriﬁcation speed. If so, further exploring
the principle of co-design for those objectives is an interesting future direction.

ACKNOWLEDGEMENTS

This work was supported by the NSF Graduate Research Fellowship under Grant No. 1122374,
by the NSF grants CCF-1553428 and CNS-1815221, and by Lockheed Martin Corporation under
award number RPP2016-002. We would like to thank Krishnamurthy Dvijotham, Ludwig Schmidt,
Michael Sun, Dimitris Tsipras, and Jonathan Uesato for helpful discussions.

9

Published as a conference paper at ICLR 2019

REFERENCES

Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense
In International Conference on

of security: Circumventing defenses to adversarial examples.
Machine Learning (ICML), 2018a.

Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. In International Conference on Machine Learning (ICML), pages 284–293, 2018b.

Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
In Proceedings of ACM Workshop on Artiﬁcial Intelligence and Security

detection methods.
(AISec), 2017a.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks.

In

Security and Privacy (SP), 2017 IEEE Symposium on, 2017b.

Nicholas Carlini, Guy Katz, Clark Barrett, and David L. Dill. Ground-truth adversarial examples.

2017.

networks. 2017a.

Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artiﬁcial neural

Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration

for deep neural networks. 2017b.

Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training veriﬁed learners with learned ver-
iﬁers. 2018.

Rüdiger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In Auto-

mated Technology for Veriﬁcation and Analysis, 2017.

Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir
Rahmati, and Dawn Song. Robust physical-world attacks on machine learning models. 2017.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. In International Conference on Learning Representations (ICLR), 2015.

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. 2018.

Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. In International Conference on Learning
Representations (ICLR), 2016.

Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, An-
drew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep
neural networks for acoustic modeling in speech recognition: The shared views of four research
groups. In IEEE Signal Processing Magazine, 2012.

Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. 2018.

Guy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer. Reluplex: An efﬁcient

smt solver for verifying deep neural networks. 2017.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations (ICLR), 2015.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012.

Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward relu

neural networks. 2017.

10

Published as a conference paper at ICLR 2019

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations (ICLR), 2018.

Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning (ICML), 2018.

Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation
as a defense to adversarial perturbations against deep neural networks. In Proceedings of IEEE
Symposium on Security and Privacy (SP), 2016.

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial exam-

ples. In International Conference on Learning Representations (ICLR), 2018.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 2017.

Aman Sinha, Hongseok Namkoong, and John Duchi. Certiﬁable distributional robustness with
principled adversarial training. In International Conference on Learning Representations (ICLR),
2018.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations (ICLR), 2014.

Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap
In Computer Vision and Pattern Recognition

to human-level performance in face veriﬁcation.
(CVPR), 2014.

Robert Tibshirani. Regression shrinkage and selection via the lasso. In Journal of the Royal Statis-

tical Society, Series B, 1994.

Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In International Conference on Learning Representations (ICLR), 2019.

Jonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial
risk and the dangers of evaluating against weak attacks. In International Conference on Machine
Learning (ICML), 2018.

Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer

adversarial polytope. In International Conference on Machine Learning (ICML), 2018.

Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and Zico Kolter. Scaling provable adversarial

defenses. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

11

Published as a conference paper at ICLR 2019

APPENDIX

A NATURAL IMPROVEMENTS

A.1 NATURAL REGULARIZATION FOR INDUCING WEIGHT SPARSITY

All of the control and “+RS” networks in our paper contain natural improvements that improve
weight sparsity, which reduce the number of variables in the LPs solved by the veriﬁer. We ob-
served that the two techniques we used for weight sparsity ((cid:96)1-regularization and small weight prun-
ing) don’t hurt test set accuracy but they dramatically improve provable adversarial accuracy and
veriﬁcation speed.

1. (cid:96)1-regularization: We use a weight of 2e−5 on MNIST and a weight of 1e−5 on CIFAR.
We chose these weights via line search by ﬁnding the highest weight that would not hurt
test set accuracy.

2. Small weight pruning: Zeroing out weights in a network that are very close to zero. We

choose to prune weights less than 1e−3.

A.2 A BASIC IMPROVEMENT FOR INDUCING RELU STABILITY: RELU PRUNING

We also use a basic idea to improve ReLU stability, which we call ReLU pruning. The main idea is
to prune away ReLUs that are not necessary.

We use a heuristic to test whether a ReLU in a network is necessary. Our heuristic is to count how
many training inputs cause the ReLU to be active or inactive. If a ReLU is active (the pre-activation
satisﬁes ˆzij(x) > 0) for every input image in the training set, then we can replace that ReLU with
the identity function and the network would behave in exactly the same way for all of those images.
Similarly, if a ReLU is inactive (ˆzij(x) < 0) for every training image, that ReLU can be replaced by
the zero function.

Extending this idea further, we expect that ReLUs that are rarely used can also be removed without
signiﬁcantly changing the behavior of the network. If only a small fraction (say, 10%) of the input
images activate a ReLU, then replacing the ReLU with the zero function will only slightly change the
network’s behavior and will not affect the accuracy too much. We provide experimental evidence of
this phenomenon on an adversarially trained ((cid:15) = 0.1) MNIST model. Conservatively, we decided
that pruning away ReLUs that are active on less than 10% of the training set or inactive on less than
10% of the training set was reasonable.

Figure 3: Removing some ReLUs does not hurt test set accuracy or accuracy against a PGD adver-
sary

12

Published as a conference paper at ICLR 2019

B ADVERSARIAL TRAINING AND WEIGHT SPARSITY

It is worth noticing that adversarial training against (cid:96)∞ norm-bound adversaries alone already makes
networks easier to verify by implicitly improving weight sparsity. Indeed, this can be shown clearly
in the case of linear networks. Recall that a linear network can be expressed as f (x) = W x + b.
Thus, an (cid:96)∞ norm-bound perturbation of the input x will produce the output

f (x(cid:48)) = x(cid:48)W + b

= xW + b + (x(cid:48) − x)W
≤ f (x) + (cid:15)||W ||1

where the last inequality is just Hölder’s inequality.
In order to limit the adversary’s ability to
perturb the output, adversarial training needs to minimize the ||W ||1 term, which is equivalent to (cid:96)1-
regularization and is known to promote weight sparsity (Tibshirani, 1994). Relatedly, Goodfellow
et al. (2015) already pointed out that adversarial attacks against linear networks will be stronger
when the (cid:96)1-norm of the weight matrices is higher.

Even in the case of nonlinear networks, adversarial training has experimentally been shown to im-
prove weight sparsity. For example, models trained according to Madry et al. (2018) and Wong and
Kolter (2018) often learn many weight-sparse layers, and we observed similar trends in the mod-
els we trained. However, it is important to note that while adversarial training alone does improve
weight sparsity, it is not sufﬁcient by itself for efﬁcient exact veriﬁcation. Additional regularization
like (cid:96)1-regularization and small weight pruning further promotes weight sparsity and gives rise to
networks that are much easier to verify.

13

Published as a conference paper at ICLR 2019

C INTERVAL ARITHMETIC

C.1 NAIVE INTERVAL ARITHMETIC

Naive IA determines upper and lower bounds for a layer based solely on the upper and lower bounds
of the previous layer.
Deﬁne W + = max(W, 0), W − = min(W, 0), u = max(ˆu, 0), and l = max(ˆl, 0). Then the bounds
on the pre-activations of layer i can be computed as follows:
i + li−1W −
i + ui−1W −

ˆui = ui−1W +
ˆli = li−1W +

i + bi
i + bi

(7)

(8)

As noted in Tjeng et al. (2019) and also Dvijotham et al. (2018), this method is efﬁcient but can lead
to relatively conservative bounds for deeper networks.

C.2

IMPROVED INTERVAL ARITHMETIC

We improve upon naive IA by exploiting ReLUs that we can determine to always be active. This
allows us to cancel symbols that are equivalent that come from earlier layers of a network.

We will use a basic example of a neural network with one hidden layer to illustrate this idea. Suppose
that we have the scalar input z0 with l0 = 0, u0 = 1, and the network has the following weights and
biases:

W1 = [1 − 1] ,

b1 = [2

2] , W2 =

b2 = 0

(cid:21)
(cid:20)1
1

,

Naive IA for the ﬁrst layer gives ˆl1 = l1 = [2
2], and applying naive IA to
the output ˆz2 gives ˆl2 = 3, ˆu2 = 5. However, because ˆl1 > 0, we know that the two ReLUs in the
hidden layer are always active and thus equivalent to the identity function. Then, the output is

1], ˆu1 = u1 = [3

ˆz2 = z11 + z12 = ˆz11 + ˆz12 = (z0 + 2) + (−z0 + 2) = 4

Thus, we can obtain the tighter bounds ˆl2 = ˆu2 = 4, as we are able to cancel out the z0 terms.
We can write this improved version of IA as follows. First, letting Wk denote row k of matrix W ,
we can deﬁne the “active” part of W as the matrix WA, where

(WA)k =

(cid:40)

Wk
0

if ˆli−1 > 0
if ˆli−1 ≤ 0

Deﬁne the “non-active” part of W as

WN = W − WA
Then, using the same deﬁnitions for the notation W +, W −, u, l as before, we can write down the
following improved version of IA which uses information from the previous 2 layers.

ˆui = ui−1Wi

+
N + li−1Wi

−
N + bi
+ ui−2(Wi−1WiA)+ + li−2(Wi−1WiA)− + bi−1WiA
−
N + bi
+ li−2(Wi−1WiA)+ + ui−2(Wi−1WiA)− + bi−1WiA
We are forced to to use li−1,j and ui−1,j if we can not determine whether or not the ReLU corre-
sponding to the activation zi−1,j is active, but we use li−2 and ui−2 whenever possible.

+
N + ui−1Wi

ˆli = li−1Wi

We now deﬁne some additional notation to help us extend this method to any number of layers. We
now seek to deﬁne fn, which is a function which takes in four sequences of length n – upper bounds,
lower bounds, weights, and biases – and outputs the current layer’s upper and lower bounds.

What we have derived so far from (7) and (8) is the following

f1(ui−1, li−1, Wi, bi) = (ui−1W +

i + li−1W −

i + bi, li−1W +

i + ui−1W −

i + bi)

14

Published as a conference paper at ICLR 2019

Let u denote a sequence of upper bounds. Let uz denote element z of the sequence, and let u[z:]
denote the sequence without the ﬁrst z elements. Deﬁne notation for l, W, and b similarly.

Then, using the fact that WN Z = (W Z)N and WAZ = (W Z)A, we can show that the following
recurrence holds:

fn+1(u, l, W, b) = f1(u1, l1, W1N , b1)

+ fn(u[1:], l[1:], (W2W1A, W[2:]), (b2W1A, b[2:]))

(9)

Let u(x,y) denote the sequence (ux, ux−1, · · · , uy), and deﬁne l(x,y), W(x,y), and b(x,y) similarly.
Then, if we want to compute the bounds on layer k using all information from the previous k layers,
we simply have to compute fk(u(k−1,0), l(k−1,0), W(k,1), b(k,1)).

From the recurrence 9, we see that using information from all previous layers to compute bounds for
layer k takes O(k) matrix-matrix multiplications. Thus, using information from all previous layers
to compute bounds for all layers of a d layer neural network only involves O(d2) additional matrix
multiplications, which is still reasonable for most DNNs. This method is still relatively efﬁcient
because it only involves matrix multiplications – however, needing to perform matrix-matrix multi-
plications as opposed to just matrix-vector multiplications results in a slowdown and higher memory
usage when compared to naive IA. We believe the improvement in the estimate of ReLU upper and
lower bounds is worth the time trade-off for most networks.

C.3 EXPERIMENTAL RESULTS ON IMPROVED IA AND NAIVE IA

In Table 4, we show empirical evidence that the number of unstable ReLUs in each layer of a MNIST
network, as estimated by improved IA, tracks the number of unstable ReLUs determined by the exact
veriﬁer quite well. We also present estimates determined via naive IA for comparison.

Dataset

Epsilon Training Estimation

Method Method

Unstable ReLUs Unstable ReLUs Unstable ReLUs
in 1st Layer

in 2nd Layer

in 3rd Layer

MNIST (cid:15) = 0.1

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

Control

+RS

Control

+RS

Control

+RS

Exact
61.14
Improved IA 61.14
61.14
Naive IA

21.64
Exact
Improved IA 21.64
21.64
Naive IA

Exact
17.47
Improved IA 17.47
17.47
Naive IA

Exact
29.91
Improved IA 29.91
29.91
Naive IA

Exact
36.76
Improved IA 36.76
36.76
Naive IA

24.43
Exact
Improved IA 24.43
24.43
Naive IA

185.30
185.96 (+0.4%)
188.44 (+1.7%)

31.73
43.40 (+36.8%)
69.96 (+120.5%)

64.73
64.80 (+0.1%)
65.34 (+0.9%)

14.67
18.97 (+29.4%)
33.51 (+128.5%)

37.92
48.88 (+28.9%)
69.75 (+84.0%)

24.05
28.40 (+18.1%)
40.47 (+68.3%)

28.64
31.19 (+8.9%)
32.13 (+12.2%)

83.42
83.44 (+0.02%)
83.52 (+0.1%)

40.74
46.00 (+12.9%)
48.27 (+18.5%)

142.95
142.95
142.95

54.47
54.47
54.47

48.47
48.47
48.47

Table 4: Comparison between the average number of unstable ReLUs as found by the exact veriﬁer
of Tjeng et al. (2019) and the estimated average number of unstable ReLUs found by improved IA
and naive IA. We compare these estimation methods on the control and “+RS” networks for MNIST
that we described in Section 3.3.4

15

Published as a conference paper at ICLR 2019

C.4 ON THE CONSERVATIVE NATURE OF IA BOUNDS

The upper and lower bounds we compute on each ReLU via either naive IA or improved IA are
conservative. Thus, every unstable ReLU will always be correctly labeled as unstable, while stable
ReLUs can be labeled as either stable or unstable. Importantly, every unstable ReLU, as estimated
by IA bounds, is correctly labeled and penalized by RS Loss. The trade-off is that stable ReLUs
mislabeled as unstable will also be penalized, which can be an unnecessary regularization of the
model.

In Table 5 we show empirically that we can achieve the following two objectives at once when using
RS Loss combined with IA bounds.

1. Reduce the number of ReLUs labeled as unstable by IA, which is an upper bound on the

true number of unstable ReLUs as determined by the exact veriﬁer.

2. Achieve similar test set accuracy and PGD adversarial accuracy as a model trained without

RS Loss.

Dataset

Epsilon Training
Method

Estimation
Method

Total Labeled
Unstable ReLUs Accuracy

Test Set

PGD Adversarial
Accuracy

MNIST (cid:15) = 0.1

Control
+RS

Improved IA 290.5
Improved IA 105.4

Control (Large) Naive IA
Naive IA
+RS (Large)

835.8
150.3

98.94%
98.68% (-0.26%)

95.12%
95.13% (+0.01%)

99.04%
98.95% (-0.09%)

96.32%
96.58% (+0.26%)

Table 5: The addition of RS Loss results in far fewer ReLUs labeled as unstable for both 3-layer and
6-layer (Large) networks. The decrease in test set accuracy as a result of this regularization is small.

Even though IA bounds are conservative, these results show that it is still possible to decrease the
number of ReLUs labeled as unstable by IA without signiﬁcantly degrading test set accuracy. When
comparing the Control and “+RS” networks for MNIST and (cid:15) = 0.1, adding RS Loss decreased
the average number of ReLUs labeled as unstable (using bounds from Improved IA) from 290.5 to
105.4 with just a 0.26% loss in test set accuracy. The same trend held for deeper, 6-layer networks,
even when the estimation method for upper and lower bounds was the more conservative Naive IA.

16

Published as a conference paper at ICLR 2019

D FULL EXPERIMENTAL SETUP

D.1 NETWORK TRAINING DETAILS

In our experiments, we use robust adversarial training (Goodfellow et al., 2015) against a strong
adversary as done in Madry et al. (2018) to train various DNN classiﬁers. Following the prior
examples of Wong and Kolter (2018) and Dvijotham et al. (2018), we introduced a small tweak
where we increased the adversary strength linearly from 0.01 to (cid:15) over ﬁrst half of training and kept
it at (cid:15) for the second half. We used this training schedule to improve convergence of the training
process.

For MNIST, we trained for 70 epochs using the Adam optimizer (Kingma and Ba, 2015) with a
learning rate of 1e−4 and a batch size of 32. For CIFAR, we trained for 250 epochs using the Adam
optimizer with a learning rate of 1e−4. When using naive IA, we used a batch size of 128, and
when using improved IA, we used a batch size of 16. We used a smaller batch size in the latter
case because improved IA incurs high RAM usage during training. To speed up training on CIFAR,
we only added in RS Loss regularization in the last 20% of the training process. Using this same
sped-up training method on MNIST did not signiﬁcantly affect the results.

Dataset

Epsilon

(cid:96)1 weight RS Loss weight

MNIST 0.1
MNIST 0.2
MNIST 0.3
CIFAR 2/255
CIFAR 8/255

2e−5
2e−5
2e−5
1e−5
1e−5

12e−5
1e−4
12e−5
1e−3
2e−3

Table 6: Weights chosen using line search for (cid:96)1 regularization and RS Loss in each setting

For each setting, we ﬁnd a suitable weight on RS Loss via line search. The same weight is used for
each ReLU. The ﬁve weights we chose are displayed above in Table 6, along with weights chosen
for (cid:96)1-regularization.

We also train “+RS” models using naive IA to show that our technique for inducing ReLU stability
can work while having small training time overhead – full details on “+RS (Naive IA)” networks are
in Appendix E.

D.2 VERIFIER OVERVIEW

The MILP-based exact veriﬁer of Tjeng et al. (2019), which we use, proceeds in two steps for every
input. They are the model-build step and the solve step.

First, the veriﬁer builds a MILP model based on the neural network and the input. In particular, the
veriﬁer will compute upper and lower bounds on each ReLU using a speciﬁc bound computation al-
gorithm. We chose the default bound computation algorithm in the code, which uses LP to compute
bounds. LP bounds are tighter than the bounds computed via IA, which is another option available in
the veriﬁer. The model-build step’s speed appeared to depend primarily on the tightening algorithm
(IA was faster than LP) and the number of variables in the MILP (which, in turn, depends on the
sparsity of the weights of the neural network). The veriﬁer takes advantage of these bounds by not
introducing a binary variables into the MILP formulation if it can determine that a particular ReLU
is stable. Thus, using LP as the tightening algorithm resulted in higher build times, but led to easier
MILP formulations.

Next, the veriﬁer solves the MILP using an off-the-shelf MILP solver. The solver we chose was
the commercial Gurobi Solver, which uses a branch-and-bound method for solving MILPs. The
solver’s speed appeared to depend primarily on the number of binary variables in the MILP (which
corresponds to the number of unstable ReLUs) as well as the total number of variables in the MILP
(which is related to the sparsity of the weight matrices). While these two numbers are strongly
correlated with solve times, some solves would still take a long time despite having few binary

17

Published as a conference paper at ICLR 2019

variables. Thus, understanding what other properties of neural networks correspond to MILPs that
are easy or hard to solve is an important area to explore further.

D.3 VERIFIER DETAILS

We used the most up-to-date version of the exact veriﬁer from Tjeng et al. (2019) using the default
settings of the code. We allotted 120 seconds for veriﬁcation of each input datapoint using the
default model build settings. We ran our experiments using the commercial Gurobi Solver (version
7.5.2), and model solves were parallelized over 8 CPU cores with Intel Xeon CPUs @ 2.20GHz
processors. We used computers with 8–32GB of RAM, depending on the size of the model being
veriﬁed. All computers used are part of an OpenStack network.

18

Published as a conference paper at ICLR 2019

E FULL EXPERIMENTAL VERIFICATION RESULTS

Dataset

Epsilon

Training
Method

Test
Set Adversarial
Accuracy

Accuracy

Total
Provable
Upper Adversarial Unstable
Accuracy
Bound

Avg
Avg
Build
Solve
ReLUs Time (s) Time (s)

PGD Veriﬁer

Adversarial Training*
+(cid:96)1-regularization
+Small Weight Pruning
+ReLU Pruning (Control)

MNIST (cid:15) = 0.1

Wong et al. (2018)***

+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

Control
+RS
+RS (Naive IA)
+RS (Large)**

MNIST (cid:15) = 0.2

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CIFAR (cid:15) = 8/255

99.17%
99.00%
98.99%
98.94%
98.68%
98.53%
98.95%

98.40%
98.10%
98.08%
98.21%
95.06%

97.75%
97.33%
97.06%
97.54%

64.64%
61.12%
57.83%
61.41%

50.69%
40.45%
46.19%
42.81%

95.04% 96.00%
95.25% 95.98%
95.38% 94.93%
95.12% 94.45%
95.13% 94.38%
94.86% 94.54%
96.58% 95.60%

93.14% 90.71%
93.14% 89.98%
91.68% 88.87%
94.19% 90.40%
-
89.03%

91.64% 83.83%
92.05% 81.70%
89.19% 79.12%
93.25% 83.70%

51.58% 50.23%
49.92% 47.79%
47.03% 45.33%
50.61% 51.00%

31.28% 33.46%
26.78% 22.74%
29.66% 26.07%
28.69% 25.20%

19.00%
82.17%
89.13%
91.58%
94.33%
94.32%
95.60%

86.45%
89.79%
85.54%
89.10%
80.29%

77.99%
80.68%
76.70%
59.60%

45.53%
45.93%
44.44%
41.40%

7.09%
20.27%
18.90%
19.80%

1517.9
505.3
502.7
278.2
101.0
158.3
119.5

2970.43
21.99
11.71
6.43
0.49
0.96
0.27

198.3
108.4
217.2
133.0
-

160.9
101.5
179.0
251.2

360.0
234.3
170.1
196.7

665.9
54.2
277.8
246.5

9.41
1.13
8.50
2.93
-

11.80
2.78
6.43
37.45

21.75
13.50
6.30
29.88

82.91
22.33
33.63
20.14

650.93
79.13
19.30
9.61
4.98
4.82
156.74

7.15
4.43
4.67
171.10
-

5.14
4.34
4.00
166.39

66.42
52.58
47.11
335.97

73.28
38.84
23.66
401.72

Table 7: Full results on natural improvements from Table 1, control networks (which use all of the
natural improvements and ReLU pruning), and “+RS” networks from Tables 2 and 3. While we are
unable to determine the true adversarial accuracy, we provide two upper bounds and a lower bound.
Evaluations of robustness against a strong 40-step PGD adversary (PGD adversarial accuracy) gives
one upper bound, and the veriﬁer itself gives another upper bound because it can also prove that
the network is not robust to perturbations on certain inputs by ﬁnding adversarial examples. The
veriﬁer simultaneously ﬁnds the provable adversarial accuracy, which is a lower bound on the true
adversarial accuracy. The gap between the veriﬁer upper bound and the provable adversarial accu-
racy (veriﬁer lower bound) corresponds to inputs that the veriﬁer times out on. These are inputs that
the veriﬁer can not prove to be robust or not robust in 120 seconds. Build times and solve times
are reported in seconds. Finally, average solve time includes timeouts. In other words, veriﬁcation
solves that time out contribute 120 seconds to the total solve time.
* The “Adversarial Training” network uses a 3600 instead of 120 second timeout and is only veriﬁed
for the ﬁrst 100 images because verifying it took too long.
** The “+RS (Large)” networks are only veriﬁed for the ﬁrst 1000 images because of long build
times.
*** Wong et al. (2018); Dvijotham et al. (2018), and Mirman et al. (2018), which we compare to in
Table 3, do not report results on MNIST, (cid:15) = 0.2 in their papers. We ran the publicly available code
of Wong et al. (2018) on MNIST, (cid:15) = 0.2 to generate these results for comparison.

19

Published as a conference paper at ICLR 2019

F DISCUSSION ON VERIFICATION AND CERTIFICATION

Exact veriﬁcation and certiﬁcation are two related approaches to formally verifying properties of
neural networks, such as adversarial robustness.
In both cases, the end goal is formal veriﬁca-
tion. Certiﬁcation methods, which solve an easier-to-solve relaxation of the exact veriﬁcation prob-
lem, are important developments because exact veriﬁcation previously appeared computationally
intractable for all but the smallest models.

For the case of adversarial robustness, certiﬁcation methods exploit a trade-off between provable
robustness and speed. They can fail to provide certiﬁcates of robustness for some inputs that are
actually robust, but they will either ﬁnd or fail to ﬁnd certiﬁcates of robustness quickly. On the other
hand, exact veriﬁers will always give the correct answer if given enough time, but exact veriﬁers can
sometimes take many hours to formally verify robustness on even a single input.

In general, the process of training a robust neural network and then formally verifying its robustness
happens in two steps.

• Step 1: Training
• Step 2: Veriﬁcation or Certiﬁcation

Most papers on certiﬁcation, including Wong and Kolter (2018); Wong et al. (2018); Dvijotham
et al. (2018); Raghunathan et al. (2018) and Mirman et al. (2018), propose a method for step 2
(the certiﬁcation step), and then propose a training objective in step 1 that is directly related to
their method for step 2. We call this paradigm “co-training.” In Raghunathan et al. (2018), they
found that using their step 2 on a model trained using Wong and Kolter (2018)’s step 1 resulted in
extremely poor provable robustness (less than 10%), and the same was true when using Wong and
Kolter (2018)’s step 2 on a model trained using their step 1.

We focus on MILP-based exact veriﬁcation as our step 2, which encompasses the best current exact
veriﬁcation methods. The advantage of using exact veriﬁcation for step 2 is that it will be accurate,
regardless of what method is used in step 1. The disadvantage of using exact veriﬁcation for step
2 is that it could be extremely slow. For our step 1, we used standard robust adversarial training.
In order to signiﬁcantly speed up exact veriﬁcation as step 2, we proposed techniques that could be
added to step 1 to induce weight sparsity and ReLU stability.

In general, we believe it is important to develop effective methods for step 1, given that step 2 is
exact veriﬁcation. However, ReLU stability can also be beneﬁcial for tightening the relaxation of
certiﬁcation approaches like that of Wong et al. (2018) and Dvijotham et al. (2018), as unstable
ReLUs are the primary cause of the overapproximation that occurs in the relaxation step. Thus, our
techniques for inducing ReLU stability can be useful for certiﬁcation as well.

Finally, in recent literature on veriﬁcation and certiﬁcation, most works have focused on formally
verifying the property of adversarial robustness of neural networks. However, veriﬁcation of other
properties could be useful, and our techniques to induce weight sparsity and ReLU stability would
still be useful for veriﬁcation of other properties for the exact same reasons that they are useful in
the case of adversarial robustness.

20

