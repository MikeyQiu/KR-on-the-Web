Joint Parsing and Generation for Abstractive Summarization

Kaiqiang Song,1 Logan Lebanoff,1 Qipeng Guo2
Xipeng Qiu,2 Xiangyang Xue,2 Chen Li,3 Dong Yu,3 Fei Liu1
1Computer Science Department, University of Central Florida
2School of Computer Science, Fudan University 3Tencent AI Lab, Bellevue, WA
{kqsong,loganlebanoff}@knights.ucf.edu, {qpguo16,xpqiu,xyxue}@fudan.edu.cn
{ailabchenli,dyu}@tencent.com, feiliu@cs.ucf.edu

9
1
0
2
 
v
o
N
 
3
2
 
 
]
L
C
.
s
c
[
 
 
1
v
9
8
3
0
1
.
1
1
9
1
:
v
i
X
r
a

Abstract

Sentences produced by abstractive summarization systems
can be ungrammatical and fail to preserve the original mean-
ings, despite being locally ﬂuent. In this paper we propose to
remedy this problem by jointly generating a sentence and its
syntactic dependency parse while performing abstraction. If
generating a word can introduce an erroneous relation to the
summary, the behavior must be discouraged. The proposed
method thus holds promise for producing grammatical sen-
tences and encouraging the summary to stay true-to-original.
Our contributions of this work are twofold. First, we present
a novel neural architecture for abstractive summarization that
combines a sequential decoder with a tree-based decoder in a
synchronized manner to generate a summary sentence and its
syntactic parse. Secondly, we describe a novel human evalu-
ation protocol to assess if, and to what extent, a summary re-
mains true to its original meanings. We evaluate our method
on a number of summarization datasets and demonstrate com-
petitive results against strong baselines.

Introduction
It is crucial for a summary to not only condense the source
text but also render itself grammatical. Without grammati-
cal sentences, a summary can be ineffective, because human
brain derives meaning from the sentence as a whole rather
than individual words. Abstractive summarization has made
considerable recent progress (See, Liu, and Manning 2017;
Chen and Bansal 2018; Kryscinski et al. 2018). Nonetheless,
studies suggest that system summaries remain imperfect. A
summary sentence can be ungrammatical and fail to convey
the intended meaning, despite its local ﬂuency (Song, Zhao,
and Liu 2018; Lebanoff et al. 2019a). In Table 1, we show
example abstractive summaries produced by neural abstrac-
tive summarizers. The ﬁrst summary has failed to conform
to grammar and other summaries changed the original mean-
ings. These summaries not only mislead the reader but also
hinder the applicability of summarization techniques in real-
world scenarios.

In this paper, we attempt to remedy this problem by intro-
ducing a new architecture to jointly generate a summary sen-
tence and its syntactic parse, while performing abstraction.

Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Source

Today, because of a CNN story and the generosity of
donors from around the world, Kekula wears scrubs
bearing the emblem of the Emory University ...

Summ. CNN story and generosity of donors from around the world,

Kekula wears scrubs ...

Source

Summ.

Source

In its propaganda, ISIS has been using Abu Ghraib and
other cases of Western abuse to legitimize its current
actions in Iraq as the latest episodes ...

In its propaganda, ISIS is being used by the Islamic State
in Iraq and Syria ...

Both state and foreign investments in Vietnam’s agriculture
have been not sufﬁcient enough, while local farmers have
to pay fees to contribute to building rural roads ...

Summ.

Vietnam’s agriculture not sufﬁcient enough

Table 1: Example summaries generated by neural abstractive
summarizers. They are manually re-cased for readability.

This is a non-trivial task, as the method must tightly couple
summarization and parsing algorithms, which are two sig-
niﬁcant branches of NLP. A joint model for generating sum-
mary sentences and parse trees can be more appealing than
a pipeline method. The latter may suffer from error propaga-
tion, e.g., an ill-formed summary sentence can lead to more
parsing errors. Further, a joint method mimics the human be-
havior, e.g., an editor writes a summary and makes correc-
tions instantly as the text is written. She needs not to ﬁnish
the whole summary in order to correct errors. A method that
incrementally produces a summary sentence and its syntac-
tic parse aligns with this observation.

Our proposed joint model seeks to transform the source
sequence to a linearized parse tree of the summary sequence.
The model seamlessly integrates a shift-reduce dependency
parser into a summarization system employing the encoder-
decoder architecture. A “SHIFT” operation leads the summa-
rizer to generate a new word by copying it from the source
text or choosing a word from the vocabulary; whereas a
“REDUCE” operation adds a dependency arc between words
of the partial summary. The challenge of this task is to con-
struct effective representations that support both tasks, as
they require different contextual representations. We pro-
pose to couple a sequential decoder for predicting new sum-

mary words and a tree-based decoder for predicting depen-
dency arcs, and ensure both decoders work in a synchronized
fashion. We also introduce an important addition making use
of topological sorting of tree nodes to accelerate the training
procedure, making the framework computationally feasible.
Our research contributions can be summarized as follows:

• we propose to simultaneously decode sentences and their
syntactic parses while performing abstraction. Our work
represents a ﬁrst attempt toward joint abstractive summa-
rization and parsing that holds promise for improved sen-
tence grammaticality and truthful summaries;

• we present a novel neural architecture coupling a sequen-
tial and a tree decoder to generate summary sentences and
parse trees simultaneously. Experiments are performed on
a variety of summarization datasets to demonstrate the ef-
fectiveness of the proposed method;

• we describe a new human evaluation protocol to assess if
an abstractive summary has preserved the original mean-
ings, and importantly, if it has introduced any new mean-
ings that are nonexistent in the original text. The last fac-
tor is largely under-investigated in the literature.1

Related Work
Recent years have seen increasing interest in summarization
using encoder-decoder models (Rush, Chopra, and Weston
2015; Nallapati et al. 2016; See, Liu, and Manning 2017;
Celikyilmaz et al. 2018; Lebanoff et al. 2019b). An encoder
condenses the source text to a ﬁx-length vector and a de-
coder unrolls it to a summary. An encoder (or decoder) can
be realized using recurrent networks (Chen et al. 2016; Tan,
Wan, and Xiao 2017; Cohan et al. 2018; Lebanoff, Song,
and Liu 2018; Gehrmann, Deng, and Rush 2018), convolu-
tional networks (Chopra, Auli, and Rush 2016; Narayan, Co-
hen, and Lapata 2018), or Transformer (Devlin et al. 2018;
Liu et al. 2018; Song et al. 2020). To generate a summary
word, a decoder can copy a word from the source text or
select an unseen word from the vocabulary. This ﬂexibility
allows for diverse lexical choices. Nevertheless, with greater
ﬂexibility comes the increased risk of producing ill-formed
summary sentences that are ungrammatical and fail to pre-
serve the original meanings.

Parsing the source text to identify summary-worthy tex-
tual units has been exploited in the past. Marcu (1997; 1998)
utilizes discourse structure generated by an RST parser to
identify summary units that are central to the claims of the
document. A number of recent studies have explored con-
stituency and dependency grammars (Daum´e III and Marcu
2002; Clarke and Lapata 2008; Martins and Smith 2009;
Filippova 2010; Berg-Kirkpatrick, Gillick, and Klein 2011;
Wang et al. 2013; Durrett, Berg-Kirkpatrick, and Klein
2016), rhetorical structure (Christensen et al. 2013; Yoshida
et al. 2014; Li, Thadani, and Stent 2016), and abstract mean-
ing representation (Liu et al. 2015; Liao, Lebanoff, and Liu
2018; Hardy and Vlachos 2018) to generate compressive
and abstractive summaries. In this paper we emphasize that

1We make our implementation and models publicly available at

https://github.com/ucfnlp/joint-parse-n-summarize

target-side syntactic analysis is especially important to en-
sure the well-formedness of abstractive summaries, because
generating summary words and predicting relations between
words are interleaved operations.

Summarization and parsing are traditionally regarded as
separate tasks. These systems are now both realized using
neural sequence-to-sequence models, making it possible to
tackle both tasks in a single framework. There have been a
variety of studies examining neural dependency parsers us-
ing transition- and graph-based algorithms (Dyer et al. 2015;
Kiperwasser and Goldberg 2016; Dozat and Manning 2017;
Ma et al. 2018). Our method, inspired by the recurrent neu-
ral network grammar (RNNG; Dyer et al., 2016) that de-
scribes a generative probabilistic model for parsing and lan-
guage modeling (Kuncoro et al. 2017), offers a way to per-
form summary generation and parsing in a synchronized
manner. Incorporating syntax is found to improve transla-
tion (Li et al. 2017a; Eriguchi, Tsuruoka, and Cho 2017;
Wu et al. 2017; Wang et al. 2018). But to date, there has
been little work to simultaneously generate a sentence and
its syntactic parse, combining summarization with parsing
techniques. Our aim is not to improve existing parsers but
to leveraging parsing for abstractive summarization. Pars-
ing is essentially a structured prediction problem, whereas
summarization involves information reduction from source
to target, which poses an important challenge. In the follow-
ing section, we describe our model in detail.

Our Approach

Our goal is to transform a source text x containing one or
more sentences to a target sequence containing a linearized
parse tree of the summary, represented by yT . We expect a
summary to contain a single sentence, as our focus is to im-
prove sentence grammaticality.2 We use dependency gram-
mar as syntactic representation of the summary. Dependency
is useful for semantic tasks and transition-based parsing al-
gorithms are efﬁcient, linear-time in the sequence length.3

Problem formulation Our target sequence yT consists
of interleaved GEN(w) and REDUCE-L/R operations that in-
crementally build a dependency parse tree. Table 2 shows an
example. The second column contains yT and the third col-
umn contains partial dependency trees stored in a stack. A
GEN(w) operation pushes a summary word w to the stack;
REDUCE-L creates a left arc between the top and second top
word in the stack, where the top word is the head; REDUCE-
R creates a right arc where the top word is the dependent. We
choose not to label the arcs, as this work focuses on gener-
ating well-structured sentences but not on predicting labels.
The decoding process comes to an end when there is a single

2When a multi-sentence summary is desired, it is possible to
generate summary sentences repeatedly from selected subsets of
source sentences, as suggested by recent studies (Chen and Bansal
2018; Gehrmann, Deng, and Rush 2018).

3Our method is also general enough to allow other syntac-
tic/semantic formalisms such as the constituency grammar or ab-
stract meaning representation (Banarescu et al. 2013; Konstas et
al. 2017) to be exploited in future work.

t yT

Stack

R (root node)
R a
R a man
R a man

1
–
2 GEN(a)
3 GEN(man)
4
REDUCE-L
5 GEN(escaped) R a man escaped
R a man escaped
REDUCE-L
6
R a man escaped
7 GEN(from)
R a man escaped
8 GEN(prison)
R a man escaped
9
REDUCE-L
R a man escaped
10 REDUCE-R
R a man escaped
11 REDUCE-R

from
from prison
from prison
from prison
from prison

Table 2: Illustration of the decoding process. A summary
sentence “a man escaped from prison” and its dependency
structure are simultaneously generated. The second column
shows the target sequence yT and the third column contains
partial parse trees stored in a stack.

Figure 1: fdecoder tree (top) consumes the partial tree repre-
sentations of time t one by one to build hidden representa-
tion hTt ; fdecoder seq (bottom) consumes the embeddings of
summary words to build partial summary representation hy
t .

tree remaining in the stack. A summary y can be obtained
from yT by retrieving all GEN operations.

We aim to predict the target sequence yT conditioned
on the source x. The process proceeds incrementally. As il-
lustrated in Eq. (1), P (yT |x) is factorized over time steps.
P (yTt = o|yT<t, x) denotes the probability of a parsing op-
eration, where o ∈ {REDUCE-L, REDUCE-R, GEN} and GEN
is unlexicalized. P (yTt = w|yT<t, x) represents the proba-
bility of generating a summary word w at the t-th step; the
word can either be copied from the source text or selected
from the vocabulary.

P (yT |x) =

(cid:104)
(cid:89)

t

P (yTt = o|yT<t, x)
(cid:124)
(cid:123)(cid:122)
(cid:125)
parsing

(1)

× P (yTt = w|yT<t, x)

(cid:124)

(cid:123)(cid:122)
summarization

(cid:125)

1[o=GEN]

(cid:105)

At training time, the ground-truth sequence ˆyT is avail-
able, P (ˆyTt = w|ˆyT<t, x) needs only be computed for cer-
tain steps where the parsing operation is GEN, as indicated
by 1[o = GEN]. Our loss term corresponds to the condi-
tional log-likelihood which can be separately calculated for
parsing and summarization operations (Eq. (2)). During in-
ference, we calculate P (yTt |yT<t, x) as a joint distribution
over parsing and summarization operations, where yTt ∈
{REDUCE-L, REDUCE-R, GEN(w)}.

logP (ˆyT |x)=

logP (ˆyTt =o|ˆyT<t,x)

(2)

(cid:105)

(cid:104)(cid:88)

t
(cid:104) (cid:88)

+

t:ot=GEN

(cid:105)
logP (ˆyTt =w|ˆyT<t,x)

Neural representations A crucial next step is to build
neural representations to support both tasks. Predicting the
next parsing operation requires us to build an effective rep-
resentation for partial parse trees, denoted by hTt at the t-th
step, whereas predicting the next summary word suggests an
effective representation for the partial summary, represented

by hy
t . We envision both tasks to beneﬁt from a context vec-
tor cx
t that encodes source content that is deemed important
for the t-th decoding step. We next describe a new architec-
ture building representations for hTt , hy

t , and cx
t .

We model partial trees using stack-LSTM (Dyer et al.
2015; 2016). Our stack maintains a set of partial trees at any
time t; they are shown in the t-th row of Table 2. For each
partial tree, we build a vector representation for it by recur-
sively applying a syntactic composition function (Eq. (3)).
The representation is built from bottom up, shown in the
dotted circle of Figure 1. A left arc (REDUCE-L) pops two el-
ements from the stack. It then applies the composition func-
tion to create a new representation gnew head and push it onto
the stack; similarly for right arc (REDUCE-R). A GEN(w) op-
eration pushes the embedding of a summary word e(w) to
the stack.4 Kuncoro et al. (2017) report that the composition
function learns to compute a tree representation by preserv-
ing the semantics of the head word, which ﬁts our task.

gnew head=tanh(Wg[ghead||gdependent]+bg)

(3)

We introduce an LSTM, denoted by fdecoder tree, to con-
sume the partial tree representations of time t one by one to
build the hidden representation hTt . An illustration is pre-
sented in Figure 1. E.g., when t=7, the stack contains 3
partial trees and we build a vector representation for each.
fdecoder tree is unrolled 3 steps and its last hidden state is
hTt . Similarly, we build the partial summary representation
hy
t using an LSTM denoted by fdecoder seq, which consumes
the embeddings of summary words. For example, when t=7,
there are 5 words in the partial summary. fdecoder seq is un-
rolled 5 steps and its last hidden state is used as hy
t . Note that
for some steps, e.g., t=9, no summary words are generated,
we copy hy

1.
t ) encoding the source content that is
deemed important for the t-th decoding step is crucial to our
method. Important source content can not only aid in the pre-
diction of future summary words but also parsing operations.

t from its previous step hy

A context vector (cx

t
−

4e(w) has the same size as partial tree representations g.

We build the context vector cx
t in two steps. First, we encode
the source text x using a two-layer bidirectional LSTM de-
noted by fencoder. We use {hx
i } to denote the encoder hidden
states, where i is the index of source words. Next, we char-
acterize the interaction between encoder and decoder hid-
den states using an attention mechanism (Eq. (4)). We con-
catenate the partial tree and partial summary representations
[hTt ||hy
t ] to form the decoder state. The score St,i measures
the importance of the i-th source word to the t-th decoding
step. A context vector cx
t is then constructed as the weighted
sum of source representations (Eq. (5)).
St,i = w(cid:62) tanh(Wd[hTt ||hy
t = softmax(St)hx
cx

t ] + Wehx
i )

(4)
(5)

Prediction We predict summary words P (yTt = w|yT<t, x)
and parsing operations P (yTt = o|yT<t, x) with these rep-
resentations. We expect historical parsing operations to be
helpful for the latter task, i.e., the sequence of {REDUCE-L,
REDUCE-R, GEN(w)} operations shown in Table 2. We thus
use an LSTM to encode the sequence of past operations and
its last hidden state is denoted by hOt . A parsing operation is
predicted based on [hTt ||hOt ||cx
t ], and we apply the softmax
to obtain a distribution over parsing operations (Eq. (7)).

(6)

(7)

t ] + ba)

(cid:101)hTt = tanh(Wa[hTt ||hOt ||cx
P (yTt = o|yT<t, x) = softmax(Wo(cid:101)hTt )
A summarizer should allow a summary word to be copied
from the source text or generated from the vocabulary. We
implement a soft switch following See et al. (2017), where
λ = σ(Wz[hy
t ]) + bz) is the likelihood of gen-
erating a summary word from the vocabulary. The genera-
tion probability is deﬁned in Eqs. (8-9). If a word w occurs
once or more times in the source text, its copy probability
((cid:80)
i:wi=w αt,i) is the sum of its attention scores over all the
occurrences, where αt,i=softmaxi(St). If a word w appears
in both the vocabulary and source text, P (yTt = w|·) is a
weighted sum of the generation and copy probabilities.

t ||hTt ||cx

t ||hTt ||cx

(cid:101)hy
t =tanh(Wc[hy
(cid:101)P (yTt =w|yT<t,x) =softmax(Ww(cid:101)hy
t )
P (yTt =w|·)=λ (cid:101)P (yTt =w|·)+(1−λ)(cid:80)

t ]+bc)

i:wi=wαt,i

(8)

(9)

Acceleration Obtaining partial tree representations (hTt )
can be computationally expensive, because hTt has to be
computed bottom-up according to the topology of a parse
tree. Further, parse trees in a mini-batch exhibit distinct
topology, making it difﬁcult to execute parallely; frame-
works such as DyNet (Neubig and et al. 2017) often process
one instance at a time. In this work we instead propose to
arrange the tree nodes of all instances into groups accord-
ing to their topological order; representations for nodes of
the same group (hTt ) are computed in parallel. For example,
in Figure 1, the nodes marked with “1” are ﬁrst processed,
followed by nodes marked with “2” and so forth. This strat-
egy allows for mini-batch training with parse trees of distinct
topology and maximizing the usage of computing resources.

GIGAWORD
NEWSROOM
CNN/DM-R
WEBMERGE

|y|

8.41
10.18
13.89
31.43

Train

Dev

Test

4,020,581
199,341
472,872
1,331,515

4,096
21,530
25,326
40,879

1,951
21,382
20,122
43,934

Table 3: Statistics of our datasets. |y| is number of words.

Experiments
We present our datasets, settings, baselines, qualitative and
quantitative evaluation of our proposed method. We then dis-
cuss our ﬁndings and shed light on future work.

Data and Hyperparameters
We conduct experiments on a variety of datasets to gauge
the effectiveness of our proposed method. We experiment
with GIGAWORD (Parker 2011) and NEWSROOM (Grusky,
Naaman, and Artzi 2018). GIGAWORD contains about 10M
articles gathered from seven news sources (1995-2010);
NEWSROOM is a more recent effort containing 1.3M arti-
cles (1998-2017) collected from 38 news agencies. We use
the standard data splits and follow the same procedure as
Rush et al. (2015) to process both datasets. The task of GI-
GAWORD and NEWSROOM is to reduce the ﬁrst sentence of
a news article to a title-like summary.

The CNN/DM dataset (Hermann et al. 2015) has been
extensively studied. We use the version provided by See et
al. (2017) but formulate it as a sentence summarization task.
We aim to condense a source sentence to a well-formed
summary sentence. The source sentences are obtained by
pairing each summary sentence with its most similar sen-
tence in the article according to averaged R-1, R-2, and R-
L F-scores (Lin 2004). We denote this reduced dataset as
“CNN/DM-R.” It is distinct from GIGAWORD and NEWS-
ROOM because its ground-truth summaries are full grammat-
ical sentences, whereas the latter are article titles that appear
enticing but not necessarily be full sentences.

We further experiment on many-to-one sentence summa-
rization, where the goal is to fuse multiple source sentences
to a summary sentence. Existing datasets for sentence fusion
are often small, containing thousands of instances (Thadani
and McKeown 2013). In this work we present a novel use of
a newly released dataset—WebSplit (Narayan et al. 2017).
The dataset was originally developed for sentence simpliﬁ-
cation, where a lengthy source sentence is to be converted to
multiple, simpler sentences for ease of understanding. Im-
portantly, we swap the source and target sequences, so that
the task becomes fusing multiple source sentences to a well-
formed summary sentence. We name this task WEBMERGE
to avoid confusion. On average, a source text contains 4.4
sentences and the target is a single sentence. A (source, tar-
get) pair is accompanied by a set of semantic triples in the
form of “subject|property|object” and the semantics remain
unchanged during merging. We utilize these triples for hu-
man evaluation (§). In Table 3, we provide statistics of all
datasets used in this study.
Hyperparameters We create an input vocabulary to con-

System

ABS
ABS+
Luong-NMT
RAS-LSTM
RAS-Elman
ASC+FSC1
lvt2k-1sent
lvt5k-1sent
Multi-Task
SEASS
DRGD
Struct+2Way+Word
EntailGen+QuesGen

GenParse-BASE (This work)
GenParse-FULL (This work)

Gigaword Test Set
R-2

R-1

R-L

29.55
29.76
33.10
32.55
33.78
34.17
32.67
35.30
32.75
36.15
36.27
35.47
35.98

35.21
36.61

11.32
11.88
14.45
14.70
15.97
15.94
15.59
16.64
15.35
17.54
17.57
17.66
17.76

17.10
18.85

26.42
26.96
30.71
30.03
31.15
31.92
30.64
32.62
30.82
33.63
33.62
33.52
33.63

32.88
34.33

Table 4: Summarization results on Gigaword dataset. Our
GenParse systems perform on par with or superior to state-
of-the-art systems on the standard test set.

tains word appearing 5 times or more in the dataset; the out-
put vocabulary contains the most frequent 10k words. We
set all LSTM hidden states to be 256 dimensions. Because
datasets containing both summaries and human-annotated
dependency parses are unavailable, we use the Stanford
parser (Chen and Manning 2014) to obtain parse trees for
reference summaries. During training, we use a batch size
of 64 and Adam (Kingma and Ba 2015) for parameter op-
timization, with lr=1e-3, betas=[0.9,0.999], and eps=1e-8.
We apply gradient clipping of [-5,5], and a weight decay
of 1e-6. At decoding time, we apply beam search with ref-
erence (Tan, Wan, and Xiao 2017) to generate summary se-
quences. K=10 is the beam size.

Experimental Results

Summarization We present summarization results on all
datasets. Evaluation is performed using the automatic metric
of ROUGE (Lin 2004), which measures the n-gram overlap
between system and reference summaries, as well as human
evaluation of grammaticality and preservation of meanings.
We discuss our ﬁndings at the end.

In Table 4, we present summarization results on the Gi-
gaword test set containing 1951 instances. We are able to
compare our system, denoted by GenParse, with a vari-
ety of state-of-the-art neural abstractive summarizers; they
are described below. Our system can be a valuable addition
to existing neural summarizers, as it performs summariza-
tion and parsing jointly on the target-side to improve sen-
tence grammaticality. We explore two variants of our sys-
tem: GenParse-FULL represents the full model; GenParse-
BASE is an ablated model where we drop the tree-decoder
to test its impact on summarization performance; this cor-
responds to removing hTt and hOt
in all equations. All other
components remain the same. As shown in Table 4, our Gen-
Parse system performs on par with or superior to state-of-

the-art systems on the standard Gigaword test set. The full
model yields the highest R-2 score of 18.85. It outperforms
the GenParse-BASE model, demonstrating the effectiveness
of coupling a sequential decoder with a tree-based decoder
in a synchronized manner.

• ABS and ABS+ (Rush, Chopra, and Weston 2015) are the ﬁrst
work using an encoder-decoder architecture for summarization;
• Luong-NMT (Chopra, Auli, and Rush 2016) re-implements the

attentive encoder-decoder of Luong et al. (2015);

• RAS-LSTM and RAS-Elman (Chopra, Auli, and Rush 2016) de-
scribe a convolutional attentive encoder that ensures the decoder
focuses on appropriate words at each step of generation;

• ASC+FSC1 (Miao and Blunsom 2016) presents a generative
auto-encoding sentence compression model jointly trained on
labelled/unlabelled data;

• lvt2k-1sent and lvt5k-1sent (Nallapati et al. 2016) address issues
in the encoder-decoder model, including modeling keywords,
capturing sentence-to-word structure, and handling rare words;
• Multi-Task w/ Entailment (Pasunuru and Bansal 2018) combines

entailment with summarization in a multi-task setting;

• DRGD (Li et al. 2017b) describes a deep recurrent generative
decoder learning latent structure of summary sequences via vari-
ational inference;

• Struct+2Way+Word (Song, Zhao, and Liu 2018) describes a
structure infused copy mechanism for sentence summarization;
• EntailGen+QuesGen (Guo, Pasunuru, and Bansal 2018) is a
multi-task architecture to perform summarization with question
generation and entailment generation in one framework.

In Table 5 we present summarization results on the
NEWSROOM, CNN/DM-R, and WEBMERGE datasets. The
task of WEBMERGE is to fuse multiple source sentences to a
well-formed summary sentence while keeping the semantics
unchanged; the task of NEWSROOM and CNN/DM-R is sen-
tence summarization, but not document summarization. Be-
cause of that, the ROUGE scores presented in Table 5 should
not be directly compared with other published results. In-
stead, we train the pointer-generator networks with coverage
mechanism (PointerGen; See et al. 2017), one of the best
performed neural abstractive summarizers, on the train split
of each dataset, then report results on the test split; we apply
a similar process to our GenParse systems. We observe that
the GenParse-FULL model consistently outperforms strong
baselines across all datasets. The results are outstanding be-
cause our system jointly performs summarization and de-
pendency parsing; it involves an increased task complexity
than performing summarization only; and our full model is
able to excel on this task.
Dependency parsing We expect dependency relations of
a summary to be the same or similar to those of the source
text or reference summary in order to preserve the origi-
nal meanings. Generating a summary word means certain
dependency relations are simultaneously added to the sum-
mary. For example, in Table 2, generating the word escaped
leads a dependency relation man←escaped to be included in
the summary. In this section we demonstrate that by learning
to jointly summarize and parse, our system can effectively
improve the preservation of dependency relations.5

5We cannot compute parsing accuracy, because system and ref-

System

ROUGE-1
R

F

P

ROUGE-2
R

P

F

ROUGE-L
R

F

P

NEWSROOM

PointerGen (See, Liu, and Manning 2017) 43.73 38.83 39.94 21.82 18.97 19.56 40.15 35.65 36.66
41.88 36.00 37.65 20.04 16.90 17.70 38.73 33.33 34.84
GenParse-BASE (This work)
45.17 39.77 41.06 23.48 20.17 20.89 41.82 36.81 38.01
GenParse-FULL (This work)

CNN/DM-R

PointerGen (See, Liu, and Manning 2017) 50.91 49.82 49.26 34.73 33.32 33.16 48.10 46.95 46.49
48.24 46.52 46.46 31.44 29.62 29.82 45.43 43.72 43.71
GenParse-BASE (This work)
50.15 53.11 50.49 34.51 35.99 34.38 47.33 50.00 47.60
GenParse-FULL (This work)

WEBMERGE

PointerGen (See, Liu, and Manning 2017) 54.73 49.22 50.90 25.80 23.08 23.89 40.60 36.67 37.84
37.79 35.86 36.23 12.63 11.99 12.09 28.87 27.59 27.77
GenParse-BASE (This work)
62.26 54.69 57.24 32.10 28.41 29.58 48.13 42.54 44.41
GenParse-FULL (This work)

Table 5: Summarization results on Newsroom, CNN/DM-R, and WebMerge datasets. Our GenParse-FULL method jointly
decodes a summary and its dependency structure using a novel architecture that performs competitively against strong baselines.
It outperforms both pointer-generator networks and the ablated model GenParse-BASE without using the tree-decoder.

NEWSROOM dataset, respectively. As shown in Figure 2,
GenParse-FULL consistently outperforms other systems on
preserving source and reference summary relations.

Abstractive summaries can contain paraphrases of source
descriptions and we thus compare relations using both strict
and lenient measures. A strict measure requires exact match
of words. E.g., two relations w1A←w1B and w2A←w2B are
equal if w1A is the same as w2A, and w1B is the same
as w2B. A lenient measure computes Sim(w1A,w2A) and
Sim(w1B,w2B) and it requires both scores to be greater than
a threshold σ. We vary the threshold value along the x-
axis to produce the plots in Figure 2. We deﬁne Sim(·,·) as
the cosine similarity of word embeddings; and a value of
1.0 corresponds to strict match. Overall, we notice that the
GenParse-FULL method performs exceptionally well on re-
taining relations on the CNN/DM-R dataset. It achieves an F-
score of 56.7% (σ=1) / 67.8% (σ=0.7) for source relations,
and 28.5% (σ=1) / 46.8% (σ=0.7) for reference summary re-
lations. This ﬁnding suggests that the proposed joint summa-
rization and parsing method performs the best on summaries
that contain full grammatical sentences, as is the case with
CNN/DM-R, and this matches our expectation.

Human evaluation We proceed by introducing a novel
human evaluation protocol assessing system summaries for
grammaticality and preservation of original meanings. A
quantitative measure is important because it allows us to
compare different systems regarding to what extent their
abstractive summaries preserve the original meanings and
whether the summaries contain any falsiﬁed content that are
nonexistent in the original texts. The latter is particularly
under-investigated in the past. Our evaluation is made possi-
ble by utilizing RDF triples provided in WEBMERGE.

Table 6 illustrates the evaluation process. We present a
summary to a group of human judges. They are instructed
to rank this summary among four peers for grammaticality.
Next, we require the judges to answer a set of binary ques-
tions on (Q2) if the summary has conveyed the meaning of a
given RDF triple, and (Q3) if the summary has conveyed any
additional meanings that are not in the collection of triples.
In particular, an RDF (Resource Description Format) triple

Figure 2: F-scores of systems on preserving relations of ref-
erence summaries (top) and source texts (bottom). We vary
the threshold from 1.0 (strict match) to 0.7 in the x-axis to al-
low for strict and lenient matching of dependency relations.

In Figure 2 we demonstrate to what extent system sum-
maries preserve relations of source texts and reference
summaries. We contrast our system GenParse-BASE and
GenParse-FULL that jointly performs summarization and
parsing, against the strong baseline of PointerGen that ﬁrst
generates abstractive summaries then parses them using the
off-the-shelf Stanford parser (Chen and Manning 2014).
Dependency relations of source texts and reference sum-
maries are also obtained using the Stanford parser. We cal-
culate F-scores on preserving reference summary relations
(top) and source relations (bottom) and on CNN/DM-R and

erence summaries use different words and their dependency struc-
tures are not directly comparable.

Albert B White was born in 1856 and died on July
in Parkersburg, West Virginia.

Q1

Q2a

How would you rank this summary for grammaticality?
(cid:3) 1st (best) (cid:3)x 2nd
Does this summary convey the following meaning?

(cid:3) 3rd (cid:3) 4th (worst)

(Albert B. White
(cid:3)x Yes (cid:3) No

|

birthYear

1856)

|

Q2b Does this summary convey the following meaning?

deathPlace

(Albert B. White
(cid:3)x Yes (cid:3) No
Does this summary convey any additional meanings not

Parkersburg, West Virginia)

|

|

Q3

covered by the above triples?
(cid:3)x Yes (cid:3) No

Table 6: We present a summary to a group of human judges.
They are instructed to assess the summary for grammatical-
ity and preservation of original meanings.

is of the form subject | property | object and it is used for
meaning representation (Narayan et al. 2017). The number
of triples per instance varies from 1 to 7. A successful sum-
mary should preserve the meanings of all RDF triples and it
shall not introduce any additional meanings. As an example,
the summary A in Table 6 has introduced undesired content
during abstraction (died on July), it thus makes factual errors
that can mislead the reader.

Peer summaries are generated by GenParse-BASE, Point-
erGen, and GenParse-FULL. We further include human sum-
maries for comparison; the order of presentation of sum-
maries is randomized. We sample 100 instances from the test
set of WEBMERGE and employ 5 human judges on Amazon
mechanical turk (mturk.com) to perform the task; they are
rewarded $0.1 per HIT. Importantly, we are able to ﬁlter out
low-quality responses from AMT judges using their answers
for human summaries, as they are expected to answer unan-
imously yes for Q2 and no for Q3. We expect this method to
improve the quality and objectivity of human evaluation.

We present evaluation results in Table 7. It is not surpris-
ing that human summaries are ranked 1st on grammatical-
ity. Our GenParse-FULL method consistently outperforms
its counterparts and it is ranked 2nd best followed by Point-
erGen and GenParse-BASE.6 We report the system accuracy
on preserving source semantics (Q2) and preventing system
summaries from changing original meanings (¬Q3). Our
method (GenParse-FULL) again excels in both cases. But
the scores (46.8 and 53.4) suggest that ensuring abstractive
summaries to preserve source content remains a challenging
task, and similar ﬁndings are revealed by Cao et al. (2018)
and See et al. (2017). Our results are highly encouraging.
The human evaluation protocol is particularly meaningful to
quantitatively measure to what extent system-generated ab-
stractive summaries remain true-to-original.

6We perform pairwise comparisons between systems. Results
reveal that there is no signiﬁcant difference between GenParse-
BASE and PointerGen. All other differences are statistically sig-
niﬁcant according to a one-way ANOVA with posthoc Tukey HSD
test (p <0.01).

Grammaticality

Meaning
2nd 3rd 4th Q2 ¬Q3

1st

73.7 15.3
100
5.9
Human
8.5 23.7 30.5 37.3 15.8
GenParse-BASE
5.1 18.6 35.6 40.7 38.5
PointerGen
GenParse-FULL 12.7 42.4 28.0 17.0 46.8

5.1

100
12.7
50.8
53.4

Table 7: Human assessment of grammaticality and seman-
tic accuracy of various summaries. Our GenParse-FULL
achieves the best results on both aspects among all systems.

Conclusion
We propose to jointly summarize and parse to improve the
grammaticality and truthfulness of summaries. We introduce
a neural model combining a sequential decoder with a tree-
based decoder and ensure both work in a synchronized man-
ner. Experimental results show that our method performs on
par with or superior to state-of-the-art systems on standard
test sets. It surpasses strong baselines on human evaluation
of grammaticality and preservation of meanings.

Acknowledgments
We are grateful to the reviewers for their valuable comments
and suggestions. This research was supported in part by the
National Science Foundation grant IIS-1909603.

References
Banarescu, L.; Bonial, C.; Cai, S.; Georgescu, M.; Grifﬁtt, K.; Her-
mjakob, U.; Knight, K.; Koehn, P.; Palmer, M.; and Schneider, N.
2013. Abstract meaning representation for sembanking. In Pro-
ceedings of Linguistic Annotation Workshop.
Berg-Kirkpatrick, T.; Gillick, D.; and Klein, D. 2011. Jointly learn-
ing to extract and compress. In Proc. of ACL.
Cao, Z.; Wei, F.; Li, W.; and Li, S. 2018. Faithful to the original:
Fact aware neural abstractive summarization. In Proc. of AAAI.
Celikyilmaz, A.; Bosselut, A.; He, X.; and Choi, Y. 2018. Deep
communicating agents for abstractive summarization. In NAACL.
Chen, Y.-C., and Bansal, M. 2018. Fast abstractive summarization
with reinforce-selected sentence rewriting. In Proc. of ACL.
Chen, D., and Manning, C. D. 2014. A fast and accurate depen-
dency parser using neural networks. In Proc. of EMNLP.
Chen, Q.; Zhu, X.; Ling, Z.-H.; Wei, S.; and Jiang, H. 2016.
Distraction-based neural nets for doc. summarization. In IJCAI.
Chopra, S.; Auli, M.; and Rush, A. M. 2016. Abstractive sentence
summarization with attentive recurrent neural nets. In NAACL.
Christensen, J.; Mausam; Soderland, S.; and Etzioni, O. 2013. To-
wards coherent multi-document summarization. In NAACL.
Clarke, J., and Lapata, M. 2008. Global inference for sentence
compression: An integer linear programming approach. JAIR.
Cohan, A.; Dernoncourt, F.; Kim, D. S.; Bui, T.; Kim, S.; Chang,
W.; and Goharian, N. 2018. A discourse-aware attention model for
abstractive summarization of long documents. In NAACL.
Daum´e III, H., and Marcu, D. 2002. A noisy-channel model for
document compression. In Proc. of ACL.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. BERT:
pre-training of deep bidirectional transformers for language under-
standing. arXiv:1810.04805.

Dozat, T., and Manning, C. D. 2017. Deep biafﬁne attention for
neural dependency parsing. In Proc. of ICLR.
Durrett, G.; Berg-Kirkpatrick, T.; and Klein, D. 2016. Learning-
based single-document summarization with compression and
anaphoricity constraints. In Proc. of ACL.
Dyer, C.; Ballesteros, M.; Ling, W.; Matthews, A.; and Smith, N. A.
2015. Transition-based dependency parsing with stack long short-
term memory. In Proc. of ACL.
Dyer, C.; Kuncoro, A.; Ballesteros, M.; and Smith, N. A. 2016.
Recurrent neural network grammars. In Proc. of NAACL.
Eriguchi, A.; Tsuruoka, Y.; and Cho, K. 2017. Learning to parse
and translate improves neural machine translation. In ACL.
Filippova, K. 2010. Multi-sentence compression: Finding shortest
paths in word graphs. In Proc. of COLING.
Gehrmann, S.; Deng, Y.; and Rush, A. M. 2018. Bottom-up ab-
stractive summarization. In Proc. of EMNLP.
Grusky, M.; Naaman, M.; and Artzi, Y. 2018. NEWSROOM: A
dataset of 1.3 million summaries with diverse extractive strategies.
In Proc. of NAACL-HLT.
Guo, H.; Pasunuru, R.; and Bansal, M. 2018. Soft, layer-speciﬁc
multi-task summarization with entailment and question generation.
In Proc. of ACL.
Hardy, H., and Vlachos, A. 2018. Guided neural language gener-
ation for abstractive summarization using abstract meaning repre-
sentation. In Proc. of EMNLP.
Hermann, K. M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.; Kay,
W.; Suleyman, M.; and Blunsom, P. 2015. Teaching machines to
read and comprehend. In Proc. of NIPS.
Kingma, D. P., and Ba, J. 2015. Adam: A method for stochastic
optimization. In Proc. of ICLR.
Kiperwasser, E., and Goldberg, Y. 2016. Simple and accurate
dependency parsing using bidirectional LSTM feature representa-
tions. Trans. of the Association for Computational Linguistics.
Konstas, I.; Iyer, S.; Yatskar, M.; Choi, Y.; and Zettlemoyer, L.
2017. Neural AMR: Sequence-to-sequence models for parsing and
generation. In Proc. of ACL.
Kryscinski, W.; Paulus, R.; Xiong, C.; and Socher, R. 2018. Im-
proving abstraction in text summarization. In EMNLP.
Kuncoro, A.; Ballesteros, M.; Kong, L.; Dyer, C.; Neubig, G.; and
Smith, N. A. 2017. What do recurrent neural network grammars
learn about syntax? In Proc. of EACL.
Lebanoff, L.; Muchovej, J.; Dernoncourt, F.; Kim, D. S.; Kim, S.;
Chang, W.; and Liu, F. 2019a. Analyzing sentence fusion in ab-
stractive summarization. In Wksp. on New Frontiers in Summ.
Lebanoff, L.; Song, K.; Dernoncourt, F.; Kim, D. S.; Kim, S.;
Chang, W.; and Liu, F. 2019b. Scoring sentence singletons and
pairs for abstractive summarization. In ACL.
Lebanoff, L.; Song, K.; and Liu, F. 2018. Adapting the neural
encoder-decoder framework from single to multi-document sum-
marization. In EMNLP.
Li, J.; Xiong, D.; Tu, Z.; Zhu, M.; Zhang, M.; and Zhou, G. 2017a.
Modeling source syntax for neural machine translation. In ACL.
Li, P.; Lam, W.; Bing, L.; and Wang, Z. 2017b. Deep recurrent
generative decoder for abstractive text summarization. In EMNLP.
Li, J. J.; Thadani, K.; and Stent, A. 2016. The role of discourse
units in near-extractive summarization. In Proc. of SIGDIAL.
Liao, K.; Lebanoff, L.; and Liu, F. 2018. Abstract meaning repre-
sentation for multi-document summarization. In COLING.

Lin, C.-Y. 2004. ROUGE: a package for automatic evaluation of
summaries. In ACL Wksp. on Text Summarization Branches Out.
Liu, F.; Flanigan, J.; Thomson, S.; Sadeh, N.; and Smith, N. A.
2015. Toward abstractive summarization using semantic represen-
tations. In Proc. of NAACL.
Liu, P. J.; Saleh, M.; Pot, E.; Goodrich, B.; Sepassi, R.; Kaiser, L.;
and Shazeer, N. 2018. Generating wikipedia by summarizing long
sequences. In Proc. of ICLR.
Luong, M.-T.; Pham, H.; and Manning, C. D. 2015. Effective ap-
proaches to attention-based neural machine translation. In EMNLP.
Ma, X.; Hu, Z.; Liu, J.; Peng, N.; Neubig, G.; and Hovy, E. 2018.
Stack-pointer networks for dependency parsing. In ACL.
Marcu, D. 1997. From discourse structures to text summaries. In
Intelligent Scalable Text Summarization.
Marcu, D. 1998. Improving summarization through rhetorical pars-
ing tuning. In Sixth Workshop on Very Large Corpora.
Martins, A. F. T., and Smith, N. A. 2009. Summarization with a
joint model for sentence extraction and compression. In Proc. of
the ACL Workshop on ILP for Natural Language Processing.
Miao, Y., and Blunsom, P. 2016. Language as a latent variable:
Discrete generative models for sentence compression. In EMNLP.
Nallapati, R.; Zhou, B.; dos Santos, C.; Gulcehre, C.; and Xiang, B.
2016. Abstractive text summarization using sequence-to-sequence
rnns and beyond. In Proceedings of SIGNLL.
Narayan, S.; Gardent, C.; Cohen, S. B.; and Shimorina, A. 2017.
Split and rephrase. In Proc. of EMNLP.
Narayan, S.; Cohen, S. B.; and Lapata, M. 2018. Don’t give me the
details, just the summary! Topic-aware convolutional neural net-
works for extreme summarization. In Proc. of EMNLP.
Neubig, G., and et al. 2017. DyNet: The dynamic neural network
toolkit. arXiv preprint arXiv:1701.03980.
Parker, R. 2011. English Gigaword ﬁfth edition LDC2011T07.
Philadelphia: Linguistic Data Consortium.
Pasunuru, R., and Bansal, M. 2018. Multi-reward reinforced sum-
marization with saliency and entailment. In Proc. of NAACL.
Rush, A. M.; Chopra, S.; and Weston, J. 2015. A neural attention
model for sentence summarization. In Proceedings of EMNLP.
See, A.; Liu, P. J.; and Manning, C. D. 2017. Get to the point:
Summarization with pointer-generator networks. In Proc. of ACL.
Song, K.; Wang, B.; Feng, Z.; Ren, L.; and Liu, F. 2020. Control-
ling the amount of verbatim copying in abstractive summarization.
In AAAI.
Song, K.; Zhao, L.; and Liu, F. 2018. Structure-infused copy mech-
anisms for abstractive summarization. In Proc. of COLING.
Tan, J.; Wan, X.; and Xiao, J. 2017. Abstractive document sum-
marization with a graph-based attentional neural model. In ACL.
Thadani, K., and McKeown, K. 2013. Supervised sentence fusion
with single-stage inference. In Proc. of IJCNLP.
Wang, L.; Raghavan, H.; Castelli, V.; Florian, R.; and Cardie, C.
2013. A sentence compression based framework to query-focused
multi-document summarization. In Proceedings of ACL.
Wang, X.; Pham, H.; Yin, P.; and Neubig, G. 2018. A tree-based
decoder for neural machine translation. In Proc. of EMNLP.
Wu, S.; Zhang, D.; Yang, N.; Li, M.; and Zhou, M.
Sequence-to-dependency neural machine translation. In ACL.
Yoshida, Y.; Suzuki, J.; Hirao, T.; and Nagata, M.
2014.
Dependency-based discourse parser for single-document summa-
rization. In Proc. of EMNLP.

2017.

