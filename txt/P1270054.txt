1

9
1
0
2
 
t
c
O
 
7
1
 
 
]

R

I
.
s
c
[
 
 
1
v
2
9
7
7
0
.
0
1
9
1
:
v
i
X
r
a

Cascading: Association Augmented Sequential
Recommendation

XU CHEN, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University
KENAN CUI, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University
YA ZHANG, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University
YANFENG WANG, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University

Recently, recommendation according to sequential user behaviors has shown promising results in many
application scenarios. Generally speaking, real-world sequential user behaviors usually reflect a hybrid of
sequential influences and association relationships. However, most existing sequential recommendation
methods mainly concentrate on sequential relationships while ignoring association relationships. In this paper,
we propose a unified method that incorporates item association and sequential relationships for sequential
recommendation. Specifically, we encode the item association as relations in item co-occurrence graph
and mine it through graph embedding by GCNs. In the meanwhile, we model the sequential relationships
through a widely used RNNs based sequential recommendation method named GRU4Rec. The two parts are
connected into an end-to-end network with cascading style, which guarantees that representations for item
associations and sequential relationships are learned simultaneously and make the learning process maintain
low complexity. We perform extensive experiments on three widely used real-world datasets: TaoBao, Amazon
Instant Video and Amazon Cell Phone and Accessories. Comprehensive results have shown that the proposed
method outperforms several state-of-the-art methods. Furthermore, a qualitative analysis is also provided to
encourage a better understanding about association relationships in sequential recommendation and illustrate
the performance gain is exactly from item association.

CCS Concepts: •Information systems → Collaborative filtering;

Additional Key Words and Phrases: sequential user behavior, sequential recommendation, graph embedding,
item co-occurrence, association relationships, sequential relationships

ACM Reference format:
Xu Chen, Kenan Cui, Ya Zhang, and Yanfeng Wang. 2016. Cascading: Association Augmented Sequential
Recommendation. 1, 1, Article 1 (January 2016), 29 pages.
DOI: 10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Recommendation, as an information filtering method, has been extended to a wide range of real-
world applications such as product recommendation [42], video recommendation [36] and app
recommendation [54]. In recent years, an important trend in recommendation is to consider the
order of user behaviors [19, 22, 52, 55], which has shown promising results by capturing behavior
relationships. According to the way to incorporate sequential information, existing methods can be
summarized into two categories. Traditional methods utilize sequential information as contextual
features during training [11, 26, 56], which cannot model the high-order sequential relationships.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2016 ACM. XXXX-XXXX/2016/1-ART1 $15.00
DOI: 10.1145/nnnnnnn.nnnnnnn

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:2

X. Chen et al.

Fig. 1. An example to illustrate the sequential relationships and association relationships in consuming
sequence. The directed edge among items indicates the sequential relationships and undirected edge
means the association relationships.

Recent sequential recommendation methods inherently incorporate temporal information with
Recurrent Neural Networks (RNNs) [19, 52, 55]. And these methods are also explored by introducing
attention mechanism [10, 40], personalization [10] and auxiliary information (e.g. heterogeneous
attributes, knowledge base) [6, 21, 34] in order to model sequential user behavior better.

However, the real-world user behaviors usually reflect a hybrid of sequential relationships and
association relationships. The orders observed in behavior sequences do not necessarily reflect
relationships among behaviors. For example, Figure 1 illustrates an user’s purchase sequence of
phone and accessories. Clearly, the transition from iPhone 6s (step 1) to phone accessories (step
2,3,4,5) contains sequential relationships due to purchase causality. On the other hand, the purchase
order of cellphone accessories in fact could be arbitrary and actually reflects their association.

Although RNNs based sequential modeling methods are capable of capturing the sequential
relationships among user behaviors, they ignore the association relationships among items. We
here thus propose an unified model named CAASR (Cascading: Association Augmented Sequential
Recommendation) to simultaneously capture association relationships and sequential relationships
for sequential recommendation. The general idea of CAASR is illustrated in Figure 2. The item
association features are mined from item graph by GCNs, and the sequential relationships are
modeled by a widely used RNNs based sequential Recommendation method named GRU4Rec [19].
The two are connected in cascades and learned with one single training target. Because the
adopted GCNs does not support mini-batch training, we design a specific graph embedding lookup
layer which can combine GCNs and RNNs based sequential modeling method adaptively. Table 1
compares CAASR with other methods in various perspectives. The CAASR is the only method that
mines item association and generalizes RNNs based sequential recommendation.

Our contributions are summarized as three following points:

• In this paper, we firstly concentrate on modeling both the item association relationships
and sequential relationships for sequential recommendation, and develop a novel method

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

Cascading: Association Augmented Sequential Recommendation

1:3

Fig. 2. The general architecture of CAASR. The item graph and GCNs aim to mine the item association
relationships and the sequential model aims to capture the sequential relationships in sequential user
behavior.

Table 1. Comparison of CAASR with other methods in different perspectives. P-Cofactor and P-
GraphAE both are methods of capturing item association relationships in parallel style. Speciﬁcally, P-
Cofactor adopts co-factorization technique in [33], while P-GraphAE utilizes widely used Autoencoder
regularization technique in [4]. CAASR is our proposed method in cascading style.

Method

sequential
relationships

item
association

association
combination style

BPR
BPR+KNN
GRU4Rec
P-Cofactor
P-GraphAE
CAASR

×
×
(cid:88)
(cid:88)
(cid:88)
(cid:88)

×
×
×
(cid:88)
(cid:88)
(cid:88)

-
-
-
parallel
parallel
cascading

generalize
RNNs based
sequential recommendation
×
×
×
×
×
(cid:88)

named CAASR. To the best we know, CAASR is the first model that concentrates on item
association relationships in RNNs based sequential recommendation.

• To seamlessly combine item association relationships and sequential relationships, we
explore a better cascading style to incorporate them rather than the widely used parallel
regularization technique.

• We conduct extensive experiments on three real-world datasets. Comprehensive results
demonstrate the superiority over state-of-the-art models both quantitatively and qualita-
tively.

The rest of this paper is organized as follows. Section 2 introduces recent related works, in-
cluding sequential recommendation, item association relationships in recommendation and graph
embedding methods. Problem definition and a concise introduction of GCNs are given in section 3.
Section 4 provides the details about proposed methods. Experiments and results are illustrated in
section 5, and the results of our algorithm are verified both quantitatively and qualitatively. Finally,
conclusion and future work are given in section 6.

2 RELATED WORK

2.1 Towards Recommendation Methods for Sequential User Behavior
Most sequential recommendation methods are designed to capture and utilize the sequential
relationships from sequence data.

Traditional Methods. Previously, sequential information is usually used as a contextual feature
during the training phase, in which timestamp is considered as an additional resource to enrich the

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:4

X. Chen et al.

model features. However, these methods [11, 23] based on handcrafted features are consuming and
not applicable. Also TimeSVD++ [27] tends to exploit temporal signals through SVD technique.
Tensor factorization is another way of applying temporal information such as [31]. They typically
cannot model high-order sequential relationships from chronological data [10]. There are also
several sequential models often decomposing the problem into two parts: user preference modeling
and sequential modeling. For example, Factorized Personalized Markov Chain (FPMC) [44] is
a classic sequential recommendation that fuses MF and factorized Markov Chain to model user
preference and sequential relationships respectively. Recently, inspired by transnational metric
embeddings [2], TransRec [16] unifies sequential recommendation and a specific metric embedding
approach together, modeling each user as a translating vector from his/her last visited item to the
next one.

Deep Learning Based Methods. Recently, deep learning methods present excellent repre-
sentation learning ability, like Convolutional Neural Network (CNNs) [24] and Recurrent Neural
Networks (RNNs) [45]. RNNs is one promising approach in capturing the sequential relationships of
user preference via user-item interaction data [22, 52, 55]. Wu et.al [52] and Hidasi et.al [19] utilize
Long Short Term Memory (LSTM) [12] and Gated Recurrent Unit (GRU) [7] respectively to model
sequential relationships of user representation from chronological data. Then, in [20], both user
representation and item one-hot vectors are merged into one model based on [19]. Tim et.al [10]
point that approaches in [19, 20] do not explicitly model individual users, thus they introduce a
variant of GRU model that utilizes pair-wise loss to generate personalized recommendation. Due
to the well performance of attention mechanism in Neural Machine Translation (NMT) [1] and
video captioning tasks [5, 53], attention mechanism is introduced into recommendation as well
in [10, 30, 35, 40]. In [10], they design a kind of attention gated cell to regulate the original gating
process in GRU. Interacting Attention-gated Recurrent Networks (IARN), proposed in [40], extends
recurrent networks for both modeling user and item dynamics with a novel gating mechanism. A
novel attention scheme is also designed to allow the user-side and item-side recurrent networks to
interact with each other. STAMP [35], a novel short-term attention/memory priority mechanism is
proposed to emphasize general interests of users. As for auxiliary information, item heterogeneous
attributes and knowledge base has been applied in sequential recommendation in [34] and [6, 21]
respectively.

2.2 Item Association in Recommendation
Although item association is rarely modeled in RNNs based sequential recommendation, it has been
widely used in non-sequential recommendation methods [4, 33, 38]. VideoReach in [38] presents
a better video recommendation method based on multi-modal content relevance. While in many
recommendation scenarios, contents of items are difficult to obtain. In item-based collaborative
filtering, an item-item similarity matrix encoding item relevance is defined from user behavior data
to directly apply the similarity matrix to predict missing preferences. This method is not practical
and highly sensitive to the choice of similarity metric and data normalization [18]. While inspired
by the exploration of co-factorizing multiple relational matrices in Collective Matrix Factorization
(CMF) [47], Cofactor [33] constructs SPPMI matrix from item co-occurrence matrix and applies
co-factorization as the regularization term of item representation in Matrix Factorization (MF)
method [28]. This co-factorization technique is also applied into ranking recommendation in [4].

2.3 Graph Embedding Methods
Representation learning about nodes in graphs has arisen as one hot research area. These methods
mainly focus on two kinds of directions.

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

Cascading: Association Augmented Sequential Recommendation

1:5

Random walk Based. Recently, skip-gram model [39] has shown its success in natural language
processing. This method has opened new ways for feature learning of discrete objects such as
words. Perozzi et.al [41] discover that random walks of nodes in graph lead to similar power-law
distribution like words in natural language processing. Thus they regard the random walks on
graph as sentences and leverage skip-gram model for learning latent node representation. In [49],
Tang et.al define a node’s context by its neighborhoods. Node2Vec [13] defines biased random walk
to control the Bread First Search (BFS) and Deep First Search (DFS). These random walk motivated
works are based on skip-gram and Levy et.al [29] point that skip-gram with negative sampling
(SGNS) [39] is actually conducting implicit matrix factorization.

Graph Convolutional Networks. Graph Convolutional Networks is the other representative

methodology for this research area from spectral graph analysis.

In spectral graph theory [8], complex geometric structures in graphs can be studied with spectral
filters. Graph Convolutional Network learns graph structure from a spectral graph view. It is a
kind of network that learns local stationary features of nodes in spectral graph domain.

In the convolution theorem [37], convolution is defined as linear operators that diagonalized in
Fourier basis. And this basis is the eigenvectors of graph Laplacian operator. While this kind of
filter defined in spectral domain cannot naturally be localized and the computation is costly because
of multiplication with graph Fourier. These limitations are overcome by a special choice of filter
parameterization in [3, 9]. In [9], the authors apply Chebyshev polynomials to reach a recursive
formulation of graph convolution operation. Compared with work in [3], method in [9] provides a
strict control over local support of filters and is more computationally efficient by avoiding using the
eigenvalue decomposition of graph Laplacian matrix. Meanwhile, better performance is achieved
as well. GCNs is similar to convolution operation in CNNs, while this is in spectral domain for
graph data. Kipf et.al [25] simplifies GCNs in [9] through stacking convolutional layers instead
of summation of different Chebshev orders. Velickovic et.al [50] regard graph convolution as the
aggregation problem of node representation and they introduce multi-head attention mechanism to
learn attentive weights for each nodes during aggregation process. GraphSAGE in [14] generalizes
graph convolution network to unseen nodes and proposes an inductive learning framework that
leverages node feature information to efficiently generate node embeddings. Note that we mainly
focus on [9] here. A concise introduction of GCNs exploited in our paper would be provided later.

3 PRELIMINARY

3.1 Problem Deﬁnition
We firstly introduce the notations utilized in our paper and give a clear problem definition in
recommendation for sequential user behavior data. In our paper, we denote U as the set of users
and I as the set of items. Our task concentrates on implicit feedback in user behavior data, where
feedback between user u ∈ U and item i ∈ I at time step t is denoted as 1 if they have interaction
or 0 if not. By ascendingly sorting the interaction data of each user according to time, we can
form the sequence data for the user as {i1
represents the item that user
u has interacted with at time step t and nu is the length of interaction data for user u. Given an
interaction sequence {i1
u }, our aim lies in predicting the top M items that the user would most
probably interact with at time step t + 1. And the notations utilized in this paper are summarized
in Table 2.

u }, where it
u

u , ..., inu

u , ..., it

u , ..., it

3.2 Graph Convolutional Networks
Graph Convolutional Networks (GCNs) is one essential ingredient for our CAASR method, thus
we give a concise introduction about it. GCNs is an operation that aims to learn superior node

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:6

X. Chen et al.

Table 2. Notations in this paper.

Notation
U
I
N
M
it
u
A
D
IN
L
(cid:101)L
K
d
Tk (x)
LCAAS R
Ls
S
LP −Cof actor
LP −Gr aphAE

Description
the user set
the item set
number of users (sequences)
number of items
the item that user u interacted at step t
item graph adjacent matrix
degree matrix of A
the identity matrix of size N
symmetric normalized Laplacian matrix of item graph
rescaled symmetric normalized Laplacian matrix of item graph
Chebshev order of graph convolution in this paper
dimension of embedding
Chenshev polynomial with order k
loss function of CAASR method
loss function of pure RNNs based sequential recommendation method
SPPMI matrix
loss function of P-Cofactor method
loss function of P-GraphAE method

representation considering dependence among nodes. It is neither limited to the task nor the
model [9, 14, 25, 50].

Spectral convolution on graphs is defined as the multiplication of a signal x ∈ RN with a

parameterized filter дθ in the Fourier domain, i.e.:

дθ (cid:63) x = U дθ (Λ)U T x
where (cid:63) represents the graph convolution operation. U and Λ denote the matrix of eigenvectors and
eigenvalues of the graph Laplacian L = IN − D− 1
2 = U ΛU T , respectively. And U T x indicates
the graph Fourier transform of x. And a polynomial filter is taken in [9] as дθ (Λ) = (cid:205)K
k=0 θk Λk .
While this convolution filter involves the eigen-decomposition of L which might be computationally
expensive for large graphs. To circumvent this problem, дθ (Λ) could be well-approximated by a
truncated expansion in terms of Chebshev polynomials Tk (x) up to K th order [15]:

2 AD− 1

(1)

дθ (Λ) ≈

θkTk ((cid:101)Λ) ⇒ дθ (cid:63) x ≈

θkTk ((cid:101)L)x

(2)

K
(cid:213)

k =0

K
(cid:213)

k =0

where (cid:101)Λ = 2
λmax
polynomails are recursively defined as Tk (x) = 2xTk −1(x) − Tk−2(x) with T0 = 1 and T1 = x.

L −IN . λmax denotes the largest eigenvalue of L. The Chebshev

Λ−IN and (cid:101)L = 2
λmax

From above we could clearly see that GCNs learn each node representation from the spectral graph
domain. The size of one node’s neighbors is called the receptive field. It is enlarged through increasing
Chebshev order K just like the L-hop in a graph, which could encourage more neighborhood
information when learning node representation.

4 THE PROPOSED METHOD
In this section, we demonstrate: 1) how to sufficiently mine item association relationships merely
from user behavior data; 2) how to combine this item association relationships with sequential

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

Cascading: Association Augmented Sequential Recommendation

1:7

(a) cascading style

(b) parallel style

Fig. 3. In sequential user behavior, combine item association relationships and sequential relation-
ships in two kinds of styles. Item association mining module aims to model the item association
relationships and the RNNs based sequential recommendation aims to capture the sequential rela-
tionships.

Fig. 4. The architecture of our proposed CAASR model. The pngcascading procedure is well
designed by three components: Association augmented graph embedding, Embedding lookup layer
and Sequential recommendation.

relationships in different styles. Both cascading and parallel style are explored and shown in
Figure 3.

We firstly introduce the general architecture of our proposed cascading CAASR model illustrated
in Figure 4. Apart from this, two parallel styles of combining this item association pattern are also
explored and analyzed later. Details are illustrated as follows.

4.1 Association Augmented Graph Embedding
As we summarized before, the real-world user consuming sequence is usually a hybrid of association
relationships and sequential relationships. And recent RNNs based sequential recommendation
methods such as GRU4Rec are capable of capturing the sequential relationships but fail to empha-
size the item association characteristic. We expect to model this item association pattern while
performing sequential recommendation task.

As for mining the item association relationships, there are several existing non-sequential
recommendation methods and the typical one is [33].
In [33], an item co-occurrence matrix
is constructed from user behavior data and co-factorization technique is utilized. Whereas we

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:8

X. Chen et al.

Fig. 5. An example to illustrate the item graph construction process. The threshold c is set to 2 in this
example.

find this technique does not work well in RNNs based sequential user behavior modeling. This
happens mainly because it is incompatible to apply L2 regularization between deeply represented
representation by RNNs and shallowly learned representation by traditional matrix factorization.
Instead, as one kind of expression style of graph data, this co-occurrence matrix intrinsically reflects
association relationship among items. In order to extract item association information, we use
item co-occurrence data to construct the item graph which could be easily processed by graph
embedding methods.

Considering graph embedding methods, they can be categorized into two main groups, the
random walk based and GCNs. The random walk based graph embedding methods do not support
end-to-end training as well as rely a lot on the quality of random walk sequences. In contrast, GCNs,
developed from spectral graph signal processing [46], has robust and superior performance over
random walk based methods, which has been proved in [9, 25]. It is neither limited to the model
nor the task, allowing an end-to-end and flexible combination for sequential recommendation.
Therefore, we leverage graph convolution on the item co-occurrence graph to learn association
augmented item embeddings as prior knowledge for describing items.

Graph Construction The item co-occurrence graph can be obtained from user-item interactions.
We count the number of user consuming sequence that two items Ii , Ij co-occur. By this way, a
matrix containing the frequency that every two items co-occur is formed. We set the frequency
number larger than a threshold c to 1 and the rest as 0 to form a sparse adjacent matrix A of the
item graph. This adjacent matrix A ∈ RN ×N (N is the number of items) is symmetric here, indicting
the symmetric relationship between every two items. We give an example to illustrate this process
in Figure 5.

2 AD− 1

To prepare for the subsequent convolution operation in spectral domain, several operations need
to be performed according to graph convolutional theory in [9]. Firstly, we obtain the normalized
graph Laplacian L through L = IN − D− 1
2 , where D ∈ RN ×N is the diagonal degree matrix with
Dii = (cid:205)
j Ai j and IN is the identity matrix of size N . Then, the re-scaled version of L is formulated
as ˜L = 2
λmax

L − IN and λmax is the maximum eigenvalue of L.

Graph Convolution. In order to extract item association information from item graph A, this
item graph needs to be filtered in spectral domain. Suppose we have the item representation input
X ∈ RN ×C , N is the number of items and C is the input channel for each item. Here we only use
the one-hot vector representation for all items as X in all our model training process, this means
X = IN and IN is an identity matrix with size N . The convolution on item graph can be generalized
as below:

Z = дθ (cid:63) X ≈

Tk ( ˜L)X Θk

K
(cid:213)

k=0

(3)

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

Cascading: Association Augmented Sequential Recommendation

1:9

where (cid:63) indicates the graph convolution operation defined in spectral domain. K is the number of
order of Chebyshev polynomials in Laplacian. Θk ∈ RC×d is the filter parameter that needs to be
learned for order k. Z ∈ RN ×d denotes the spectral graph embeddings of all items, and it contains
all the associated structure information of items. Specifically, the Chebyshev polynomial Tk ( ˜L) is a
sparse matrix, which can be recursively generated by using the definition in Eq. 4. And Tk ( ˜L)X Θk
can be efficiently implemented as the product of a sparse matrix with a dense matrix by using
Tensorflow1 or Pytorch2.

Tk ( ˜L) = 2 ˜LTk−1( ˜L) − Tk −2( ˜L)
T0( ˜L) = 1,T1( ˜L) = ˜L

(4)

This association augmented graph embedding operation is illustrated is Figure 4 as the first
component in shade. The input X is filtered by K filters and outputs K feature matrices each with
shape N × d that capture different orders of associated features. Summarization of K order features
leads to the final convolutional features of all items Z ∈ RN ×d .

4.2 Embedding Lookup Layer
Through the spectral graph filtering, we get associated embeddings Z of items. Note that in most
neural networks, mini-batch training approach is utilized due to both low memory usage and better
model stability. As for graph convolution in [9, 25], mini-batch strategy cannot guarantee the
original graph structure during training. Therefore we cannot train the spectral filter parameters Θ
in mini-batch manner. Instead, we input the whole rescaled normalized graph Laplacian matrix ˜L in
sparse manner. To seamlessly combine association augmented graph embedding and downstream
sequential relationships modeling module with mini-batch training style, we design a graph
embedding lookup layer to bridge the gap. In this manner, not only the graph structure is reserved
but also the latter RNNs based sequential relationships modeling module can still be trained with
mini-batch approach.

This embedding lookup layer is shown in Figure 4 as the second component with shade. In each
forward and backward training process of final loss, considering we have a mini-batch item index
that is [1, 3, 4, 5, 7, 8, N ] at step t, then we lookup the corresponding association graph embeddings
Z that belong to items [1, 3, 5, 7, 8, N ]. And the chosen mini-batch association item embedding at
step t is denoted as zt . This means that in each epoch training process, one item graph embedding
may be trained several times, which makes the learning process more stable and fast converged.
And this derivable graph embedding lookup operation can be formulated as:

zt = flookup (Z , It

index )

(5)

where function flookup (·) denotes the lookup operation on association augmented graph embeddings
Z . zt ∈ Rs×d denotes the graph embedding of mini-batch with the corresponding item index It
,
and s is the mini-batch size.

index

4.3 Sequential Recommendation
Association augmented item embeddings from the former part is aimless. The latter sequential
recommendation part captures the sequential relationships and refines associated embeddings for
recommendation. This sequential recommendation part has been studied by many techniques such
as attention mechanism [10, 40], personalization [10] and auxiliary information (e.g. heterogenous
attributes, knowledge base) [6, 21, 34]. While our model concentrates on modeling both item

1https://www.tensorflow.org/
2https://pytorch.org/

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:10

X. Chen et al.

association relationships and sequential relationships integrally in sequential recommendation,
thus we adopt one typical RNNs based sequential recommendation model named GRU4Rec [19]
as the sequential relationships modeling module in our experiments3. GRU4rec is based on GRU
which is a variant of RNN unit that aims to deal with the vanishing gradient problem. GRU4Rec also
supports user cold-start problem due to the fact that it does not specify a user in training process,
which is practical in real scenarios. The superior performance has pushed it as the state-of-art
RNNs based sequential recommendation method.

After we obtaining association augmented graph embeddings of items, we could sample the
mini-batch spectral embeddings zt ∈ Rs×d at step t. Cascading manner allows the association
information flow through GRU to be formulated as follows:
(1) the update gate:

(2) the reset gate:

(3) the candidate activation function ˆ

ct = σ (ztWc + ht −1

Uc )

r t = σ (ztWr + ht −1
ht is:

Ur )

ˆ
ht = tanh(ztWh + (r t (cid:12) ht −1)Uh)

(4) the output state ht at step t is given by:

(6)

(7)

(8)

ht = (1 − ct ) (cid:12) ht −1 + ct (cid:12) ˆ
ht
where W = {Wc ,Wr ,Wz } ∈ R3×d ×d and U = {Uc , Ur , Uz } ∈ R3×d ×d are all parameters in GRU cell
we need to learn. σ denotes the siдmoid activation function and (cid:12) is element-wise product.

(9)

4.4 Network Learning
Since implicit feedback in recommendation appears more common in real scenarios, we apply our
model with BPR loss [43] which is a powerful pairwise metric for implicit feedback data. And we
adopt the same mini-batch negative sampling approach introduced in GRU4Rec. Suppose u is one
user, p and q denote the positive and negative item respectively. Therefore, the preference of user u
for positive item over negative item at time step t is ˆr t
u )T (zt +1

, which is represented as:

= (ht

(10)

upq

)

q

p − zt +1
is regarded as user representation at time step t, zt +1

ˆr t
upq

where ht
are respectively the spectral
u
graph embedding of positive sample item and negative sample item at t + 1 step. The model aims to
predict which item the user may interact with at next step, thus the supervised information comes
from its next step.

and zt +1

q

p

The objective function is parameterized as LCAAS R :

LCAAS R = (cid:213)

− ln(σ (ˆrupq)) + λΩ
2

||Ω||2
F

(u,p,q)

(11)

where σ (ˆrupq) indicates the probability that user u prefers item p than item q. Ω denotes the
network parameters. λΩ
plays as the regularization term in this objective function. λΩ ≥ 0 is
the hyperparameter for regularization. A concise description of CAASR is illustrated in Algorithm 1

2 ||Ω||2
F

3Note that this sequential recommendation module can be substituted with other RNNs based sequential models.

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

Cascading: Association Augmented Sequential Recommendation

1:11

ALGORITHM 1: CAASR
Input: 1) An item adjacency matrix A ∈ RN ×N constructed from user behavior;
2) An item feature matrix X ∈ RN ×F , X could be the identity matrix IN if there is no feature, and we adopt
X = IN in all our experiments;
3) Kernel size K for convolution operation;
4) Training sequence data from user behavior.
Output: For each interaction at step t of a sequence, give the top M recommended items that the user most

probably might interact with at next step t + 1.

Calculate the Chebshev polynomial TK ((cid:101)L) by Eq. 4 to support convolution operation.
while not converged do

for i in {1,2,…,max-batch} do

# training triplets
Generate training triplets (u, p, q) from batch sequence data, where rup = 1, ruq = 0 and ruq = 0 is
sampled through mini-batch sampling method depicted in GRU4Rec.

# association augmented graph embedding
Calculate item association graph embedding Z by Eq. 3 ;
Sample mini-batch item association embedding zt at step t through embedding lookup layer depicted
in Section 4.2;

# general sequential recommendation model
According to Section 4.3, apply general RNNs based method (e.g. GRU4Rec) to obtain user
representation ht
u
Update model parameters based on loss defined in Eq. 11.

;

end

end

4.5 Connection to GRU4Rec
From Figure 4, we can see that GRU4Rec is the sequential recommendation component of our
model. In our method, the Chebyshev polynomial Tk (x) degrades to IN when K=0. In this case,
Eq. 3 can be written as:

Z = дθ ∗ X ≈ T0( ˜L)X Θ0 = X Θ0,

(12)

where Θ0 ∈ RC×d is exactly same to the embedding parameter for input X in GRU4Rec. That is to
say our CAASR can degrade to general RNNs based sequential recommendation when K=0.

In addition, when the number of filters K = 0, the spectral filter takes zero-hop of item’s neigh-
borhoods, meaning no consideration of the item’s neighborhoods and item association information.
Our model with a K > 0 order Chebyshev polynomial can capture a high order association pattern
among items.

4.6 Parallel Style Association Augmented Models
Apart from the proposed CAASR method in cascading style, item association relationships could
also be combined with sequential relationships in two kinds of parallel styles. One typical way
of incorporating it is co-factorization developed from non-sequential recommendation. And the
other parallel style is the Autoencoder with L2 regularization which is widely used in many deep
learning models aiming to merge one specific information to the other. We would give a concise
introduction about these two parallel styles to better demonstrate the advantages of our CAASR in
cascading style.

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:12

X. Chen et al.

ALGORITHM 2: P-Cofactor
Input: 1) The SPPMI matrix S ∈ RN ×N defined in Eq. 15;
2) Training sequence data from user behavior.
Output: For each interaction at step t of a sequence, give the top M recommended items that the user most

probably might interact with at next step t + 1.

while not converged do

for i in {1,2,…,max-batch} do

# training triplets
Generate training triplets (u, p, q) input sequence behavior, where rup = 1, ruq = 0 and ruq = 0 is
sampled through mini-batch sampling method depicted in GRU4Rec.

# parallel co-factorization model
Update model parameters based on loss LP −Cof actor

defined in Eq. 16.

end

end

4.6.1 Co-factorization Regularized. Inspired by the widely used co-factorization tech-
nique in non-sequential recommendation, we transfer it to sequential recommendation to introduce
the item association information. This combination model named P-Cofactor is depicted in Figure 6.
Following method in [33], the point-wise mutual information (PMI ) between item Ii and Ij is

defined as:

Empirically, it is estimated as:

PMI (Ii , Ij ) = loд

P(Ii , Ij )
P(Ii )P(Ij )

PMI (Ii , Ij ) = loд

#(Ii , Ij ) · #pairs
#(Ii ) · #(Ij )

where #(Ii , Ij ) denotes the number of times that item Ii and Ij co-occur. #(Ii ) = (cid:205)
#(Ij ) = (cid:205)
i
Point-wise Mutual Information (SPPMI) is defined as:

#(Ii , Ij ) and
#(Ii , Ij ). #pairs is the total number of co-occur pairs. Then the (sparse) Shifted Positive

j

SPPMI (Ii , Ij ) = max {PMI (Ii , Ij ) − loд(o), 0}
where o is the hyperparameter that controls the sparsity of SPPMI matrix. If we denote the item
SPPMI matrix as S, the loss function LP −Cof act or for P-Cofactor could be formalized as Eq. 16.

(15)

(13)

(14)

(16)

LP −Cof act or = Ls + (cid:213)

(Si j − Zcon,i · Z T

seq, j )2
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

si j(cid:44)0
(cid:125)
(cid:123)(cid:122)
(cid:124)
S P P M I mat r ix f actor izat ion

where Ls denotes the typical sequential recommendation module loss sharing the same formula in
Eq. 11. And Zseq ∈ RN ×d is the item embedding shared by both the sequential recommendation
module and SPPMI matrix factorization module. Zcon ∈ RN ×d is the item context embedding for
Zseq. From the definition of LP −Cof act or , we could summarize that this co-factorization technique
is actually performing regularization for the learning process of item embedding in sequential
recommendation. The concise learning process is illustrated in Algorithm 2.

4.6.2 Graph Autoencoder Regularized. As one kind of unsupervised learning methods,
Autoencoder, together with L2 regularization, has been widely applied in combining two kinds
of knowledge, such as Stacked Denoising Autoencoder (SDAE) [51] and Variational Autoencoder
Inspired by this, we construct item graph Autoencoder here
(VAE) [32] in recommendation.

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

(17)

(18)

Cascading: Association Augmented Sequential Recommendation

1:13

to extract item association pattern in low-dimensional vector representation. Together with L2
regularization technique, the architecture of graph Autoencoder regularized model named P-
GraphAE is shown in Figure 7.

In P-GraphAE, Graph Autoencoder (GraphAE) is an essential part which mines the inherent
association relationships in item graph. And the graph convolution operation in encoder is exactly
(cid:48) as the latent
the same with description in Section 4.1. To make a clear description, we denote Z
representation of nodes in item graph for P-GraphAE and it shares the same definition in Eq. 3 as:

(cid:48) =

Z

Tk ( ˜L)X Θ

(cid:48)
k

K
(cid:213)

k =0

As for the decoder, the aim lies in reconstructing the links in item graph. In practice, recon-
structing all non-existent links in A would easily lead to over-fitting, thus we randomly sample
m non-existent links to reconstruct. In our experiments, m is 5 times larger than the number of
existent links for all three datasets. Denote the training links set as C, we could formulate the loss
of P-GraphAE as:

LP −Gr aphAE =Ls + (cid:213)

−[Ai jloд(A

(i, j)∈ C
(cid:124)

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:48)

i j ) + (1 − Ai j )loд(1 − A
i j )]
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:48)

(cid:125)

(cid:123)(cid:122)
Gr aphAE loss

+

(cid:48)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

− Zseq ||2
F
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)
(cid:123)(cid:122)
embeddinд r eдul ar izat ion

||Z
(cid:124)

(cid:48) = siдmoid(Z

where Ls denotes the typical sequential recommendation module loss sharing the same formula
(cid:48) and Zseq denotes
in Eq. 11. And A
the latent item representation for GraphAE and sequential recommendation module respectively.
performs L2 regularization and merges the item association information from item
||Z
graph to sequential recommendation module. A concise illustration of this P-GraphAE is shown in
Algorithm 3.

(cid:48)T ) is the predicted adjacent matrix. Z

(cid:48) − Zseq ||2
F

(cid:48) · Z

4.7 Comparison Between Cascading Style and Parallel Style
The way of combining this item association relationships and sequential relationships matters
a lot both on model performance and complexity. Recent typical way of introducing associated
information to existed recommendation involves parallel regularization of the corresponding
representation [4, 33]. This parallel regularization style puts a distance restriction on two different
representations from distinctive domains. It is impractical to choose one specific distance manner
and tune the regularization hyperparameter. Instead, the item association relationships describe
the relational features for each item, which indicates the inherent knowledge of items. Hence, a
cascading way to combine this item association relationships with sequential modeling is expected
to perform better.
In addition, parallel regularization involves additional reconstruction and
regularization operation of the item graph, which makes it gain more complexity than cascading
style.

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:14

X. Chen et al.

ALGORITHM 3: P-GraphAE
Input: 1) An item adjacency matrix A ∈ RN ×N constructed from user behavior;
2) An item feature matrix X ∈ RN ×F , X could be the identity matrix IN if there is no feature, and we adopt
X = IN in all our experiments;
3) Kernel size K for convolution operation;
4) Training sequence data from user behavior.
Output: For each interaction at step t of a sequence, give the top M recommended items that the user most

probably might interact with at next step t + 1.

Calculate the Chebshev polynomial TK ((cid:101)L) by Eq. 4 to support convolution operation.
while not converged do

for i in {1,2,…,max-batch} do

# training triplets
Generate training triplets (u, p, q) from batch sequence data, where rup = 1, ruq = 0 and ruq = 0 is
sampled through mini-batch sampling method depicted in GRU4Rec.

# parallel Graph Autoencoder model
Calculate item association graph embedding Z
Update model parameters based on loss defined in Eq. 18.

(cid:48) by Eq. 17 in encoder;

end

end

Fig. 6. The architecture of our proposed P-Cofactor model. P-Cofactor directly introduces the co-
factorization technique from non-sequential recommendation to mine item association relationships
and combines it with sequential relationships in parallel style.

5 EXPERIMENTS AND RESULTS

5.1 Datasets and Experimental Settings

5.1.1 Datasets. To demonstrate the effectiveness of our model, we use three real-world
datasets: Amazon Instant Video (AIV)4, Amazon Cell Phones and Accessories (ACPA)4 and TaoBao5.
AIV and ACPA are both collected by MCAuley et.al [17] and these datasets contain explicit
In our

feedbacks with ratings range from 1 to 5 from Amazon6 during 1996.05 and 2014.07.
experimental settings, we binary all the explicit feedbacks.

4http://jmcauley.ucsd.edu/data/amazon/links.html
5https://tianchi.aliyun.com/datalab/dataSet.html?spm=5176.100073.0.0.61c835eeI6T5UL&dataId=52
6https://www.amazon.com/

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

Cascading: Association Augmented Sequential Recommendation

1:15

Fig. 7. The architecture of our proposed P-GraphAE model. P-GraphAE utilizes graph Autoencoder
and popular L2 regularization technique to merge item association relationships and sequential
relationships in parallel style.

Taobao is a dataset from clothing matching competition on TianChi7 platform. The user purchase
history, from 2014.6.14 to 2015.6.15, is considered as our sequence data here. Both these three
datasets contain URLs to images of products. Note that in our experiments, we utilize these images
for analysis rather than for training. We only use user-item interactions with timestamps to train
our model.

All these three datasets need to be preprocessed for better model learning. Following [19], we
filtered users less than n feedbacks and items less than m interactions for all datasets. n is 5,15,20
for AIV, ACPA and Taobao and m is 5,10,10 for AIV, ACPA and Taobao respectively. Especially for
TaoBao dataset, we find a serious click farming problem on it. Thus we also filter users that n > 75.
Users whose number of unique items in history less than 10 are also filtered.

Table 3. Statistics of three datasets after preprocessing. Interactions are non-zero entries. Data
density and graph density refer to the density of user-item interaction matrix and adjacent matrix of
item co-occurrence graph.

AIV

ACPA TaoBao

17,400
1,273
6,798
#users
15,366
8,816
4,733
#items
477,447
25,427
50,858
#interactions
7.517
27.451
20.911
avg.len
data density
0.158% 0.226% 0.178%
graph density 0.025% 0.011% 0.034%

After preprocessing, the statistics of these three datasets are summarized in Table 3. Follow-
ing [19], for each dataset, we randomly sample 80% sequences as train data, and the rest 20% as test
data. Note that one sequence is either in the train set or the test set. As it is a common operation in
recommendation research, items not seen during training stage are filtered out for the test set.

7https://tianchi.aliyun.com/

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:16

X. Chen et al.

5.1.2 Evaluation Metrics. Sequential Recommendation is usually with implicit feedback, in
which we are expected to correctly predict the next item that the user will probably interact with.
A good recommendation means that the target item should be among the first few recommended
items. Thus in accordance with recent recommendation evaluation metrics, we adopt Recall@k
and MRR@k as our evaluation metrics.

5.1.3 Baselines. We compare our CAASR with the following baselines:

• BPR. Bayesian Personalized Ranking (BPR) [43] is one matrix factorization method for
implicit feedback. BPR cannot be directly applied to sequential recommendation because
new user does not have his or her representation vector. Thus following by GRU4Rec, we
regard the average item feature vectors of items in user’s history as the user representation.
• BPR+KNN. K-Nearest Neighborhoods (KNN) is one common method in many practical
Recommendation Systems. By learning item vectors in BPR model, we can utilize KNN at
each time step to predict what the next item that the user will interact with. The similarity
among items is defined as cosine similarity. This provides a baseline that concentrates on
user’s short-term interest compared to BPR.

• GRU4Rec. GRU4Rec [19] is one typical RNNs based sequential recommendation method
which captures the sequential relationships in user’s sequential data. It also supports also
supports user cold-start problem due to the fact that it does not specify a user in training
process, which is practical in real scenarios. Outperformed performance makes it become
one state-of-the-art RNNs based method for sequential recommendation.

• P-Cofator. Following item association relationships mining approach in non-sequential
recommendation [33], P-Cofactor introduces co-factorization technique to incorporate item
association relationships and sequential relationships in sequential recommendation. This
co-factorization technique actually performs one kind of regularization for RNNs based
sequential recommendation.

• P-GraphAE. P-GraphAE is the parallel extension of our cascading CAASR model, which
applies popular Autoencoder and L2 regularization approach to combine item association
relationships and sequential relationships in sequential recommendation. Apart from
P-Cofactor, it serves as the other parallel style to make comparison with CAASR.

Remind that our CAASR concentrates on modeling both item association relationships and sequen-
tial relationships in sequential user behavior, and we adopt GRU4Rec as the sequential relationships
modeling method. This makes GRU4Rec become a perfect contrast baseline for CAASR. P-Cofactor
and P-GraphAE are methods aiming to incorporate item association relationships with sequential
relationships parallelly. Thus they form as the parallel baseline methods for our cascading method.

5.1.4 Experimental Settings. For all models, we set the maximum iteration up to 30. Batch
training size is 50. Latent dimension size d ranges in [50,100,150,200, 250,300], and the best d is
chosen for each model according to the performance on test set.

For BPR and BPR+KNN, we use a L2 regularization term and the hyper-parameter is 0.01 to
avoid over-fitting. Only one GRU layer is utilized for GRU4Rec and models based on GRU4Rec. For
simplicity, we set λΩ = 0 and apply Dropout [48] technique for regularization. Dropout rate is 0.2
for all deep learning based methods. Parameters are initialized with uniform distribution ranges
from -0.1 to 0.1. RMSprop is adopted for optimization. Table 4 shows the hyper-parameter setting
for all models on three datasets.

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

Cascading: Association Augmented Sequential Recommendation

1:17

Table 4. Hyper-parameter setting for all models used in our experiments on three datasets. The
Chebyshev polynomial order K for our model ranges in [3,4,5]. Limited to our computational resource,
K = 3 is adopted for TaoBao without testing K = 4, 5. Dropout rate means the value of drop rate.

Parameter
BPR/BPR+KNN
latent factors
learning rate
regularization
GRU4Rec/P-Cofactor
hidden units
learning rate
Dropout rate
P-GraphAE/CAASR
hidden units
learning rate
Dropout rate
K-order

AIV

ACPA

TaoBao

50/300
0.01/0.01
0.01/0.01

50/50
0.01/0.01
0.2/0.2

50/250
0.01/0.01
0.01/0.01

250/50
0.01/0.01
0.2/0.2

300/300
0.01/0.01
0.01/0.01

150/100
0.01/0.01
0.2/0.2

100/300
0.001/0.001
0.2/0.2
5/5

150/300
0.001/0.001
0.2/0.2
4/4

300/300
0.001/0.001
0.2/0.2
3/3

Table 5. Overall performance of all models on three datasets. The relative improvement of our CAASR
over GRU4Rec is given in the table. Model names with * refer to our proposed methods. Compared to
CAASR, the t-test results of other baselines are as well shown in the table. And ‡ means p-value¡0.01,
† indicates p-value¡0.05 and − means p-value¿0.05.

Dataset

Method

@10

@20

AIV

ACPA

TaoBao

BRR
BPR+KNN
GRU4Rec
P-Cofactor∗
P-GraphAE∗

CAASR∗

BRR
BPR+KNN
GRU4Rec
P-Cofactor∗
P-GraphAE∗

CAASR∗

BRR
BPR+KNN
GRU4Rec
P-Cofactor∗
P-GraphAE∗

CAASR∗

Recall
0.0643‡
0.1041‡
0.1084‡
0.1081‡
0.1157‡
0.1271
(+17.25%)
0.0027‡
0.0303‡
0.0714‡
0.0571‡
0.0586‡
0.0827
(+15.82%)
0.0683‡
0.0601‡
0.1472‡
0.1406‡
0.1578‡
0.1654
(+12.36%)

MRR
0.0189‡
0.0314‡
0.0505‡
0.0491‡
0.0524‡
0.0585
(+15.84%)
0.0006‡
0.0089‡
0.0337‡
0.0223‡
0.0243‡
0.0357
(+5.93%)
0.0250‡
0.0179‡
0.0721‡
0.0691‡
0.0700‡
0.0731
(+1.38%)

Recall
0.0793‡
0.1280‡
0.1505‡
0.1534‡
0.1632‡
0.1778
(+18.13%)
0.0072‡
0.0423‡
0.0997‡
0.0882‡
0.0877‡
0.1228
(+23.16%)
0.1004‡
0.0873‡
0.1984‡
0.1897‡
0.2109‡
0.2186
(+10.18%)

MRR
0.0200‡
0.0331‡
0.0536‡
0.0525‡
0.0558‡
0.0623
(+16.23%)
0.0009‡
0.0098‡
0.0358‡
0.0237‡
0.0364‡
0.0387
(+8.10%)
0.0272‡
0.0197‡
0.0759‡
0.0727‡
0.0746†
0.0771
(+1.58%)

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:18

5.2 Quantitative Results

X. Chen et al.

5.2.1 Overall Performance. In this subsection, we show the performance of all models in
Table 5. To better highlight the performance of our model, the relative performance compared to
GRU4Rec is also illustrated in Table 5.

(1) From this table, we can observe that deep learning based methods (GRU4Rec, P-GraphAE
and CAASR) yield superior performance compared to BPR and BPR+KNN. It is reasonable due to
the superior representation learning ability of deep learning.

(2) Our proposed model achieves the best performance on all three datasets. Compared to the
state-of-the-art GRU4Rec, we even achieve a 23.16% relative improvement at Recall@20 on ACPA
and 16.23% relative improvement at MRR@20 on AIV. Remind that the only difference between our
CAASR and GRU4Rec is the additional item association embedding part. And this demonstrates
the significance of modeling item association relationships for sequential user behavior.

(3) From the table, it is clear that P-Cofactor developed from GRU4Rec performs worse than
GRU4Rec almost on each dataset. Especially on ACPA, it is even 1.43% and 1.15% lower than
GRU4Rec on Recall@10 and Recall@20 respectively. Remind that P-Cofactor directly introduces co-
factorization technique to incorporate the item association relationships into RNNs based sequential
recommendation method. The result indicates that co-factorization technique from non-sequential
recommendation actually harms the performance of GRU4Rec. This happens mainly because it
is incompatible to apply L2 regularization between deeply learned representation by RNNs and
shallowly represented representation through traditional matrix factorization.

(4) Compared to P-Cofactor, P-GraphAE and CAASR both adopt graph convolution operation.
And the parallel style P-GraphAE generally performs slightly better than GRU4Rec on AIV and
TaoBao, while it fails on ACPA. In contrast, our CAASR in cascading style always performs better
than GRU4Rec and P-GraphAE on all datasets. Regarding Recall@20 on ACPA, it achieves a 2.31%
and 3.46% improvement compared to GRU4Rec and P-GraphAE respectively. These results illustrate
the correctness of cascading style rather than parallel style. In addition, compared to P-GraphAE,
the cascading CAASR has lower complexity and generalizes the input of RNNs based sequential
recommendation. This allows it to degrade to general RNNs based sequential recommendation
model under specific condition while P-GraphAE fails.

(5) To verify whether the improvement of CAASR is statistical significant, we conduct t-test
here and the result of p-value in shown in Table. 5 with different markers ‡, † and −. In this table,
p-value refers to the comparison between CASSR and other baselines. The p-values in Table. 5 are
all less than 0.01 or 0.05, which validates the improvements of our CAASR are statistical significant.

5.2.2 Performance on Different Latent Dimensions. In Figure 8, for all models, we
show their performance on Recall@20 and MRR@20 with different latent dimensions. In Figure 8
(a)(c)(e), we can see that our model reaches a better performance of Recall@20 on three datasets
almost at every latent dimension. As for MRR@20 in Figure 8 (b)(d)(f), our model can still perform
better when the latent dimension is high. Note that Recall@20 and MRR@20 are different kinds of
evaluation metrics. Better Recall@20 does not mean an exactly better MRR@20. We compare our
model both in recall ability and ranking ability.

Moreover, Figure 8 indicates that BRR, BPR+KNN and GRU4Rec model tend to have their best
performance at low dimension. While our CAASR favors reaching its best performance at high
dimension. This is probably because both BPR and GRU4Rec take deficient utilization of inputs.
When the dimension is high, the information in learned vectors tends to be redundant and yields
to be not robust on test set. While CAASR learns item association relationships in spectral graph

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

Cascading: Association Augmented Sequential Recommendation

1:19

(a) AIV - Recall@20

(b) AIV - MRR@20

(c) ACPA - Recall@20

(d) ACPA - MRR@20

(e) TaoBao - Recall@20

(f) TaoBao - MRR@20

Fig. 8. Performance of Recall@20 and MRR@20 with different latent dimensions on three datasets.
(a)(c)(e): Recall@20 on three datasets. (b)(d)(f): MRR@20 on three datasets.

domain with more information flow. This needs to match a higher dimension to adequately encode
the data.

From Figure 8, it is also obvious that P-Cofactor generally does worse performance than GRU4Rec
at every dimension, enhancing the claim that co-factorization developed from non-sequential
recommendation could not bring reliable gain for recommendation in RNNs based sequential
recommendation. As for Recall@20 and MRR@20 of P-GraphAE, it shares a similar trend with
CAASR which tends to achieve a better performance along with the increase of latent dimension.
Whereas P-GraphAE with higher complexity still cannot perform better than CAASR, which
emphasizes the advantage of the better cascading style.

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:20

X. Chen et al.

(a) AIV - Train BPR Loss

(b) AIV - Recall@20

(c) AIV - MRR@20

(d) ACPA - Train BPR Loss

(e) ACPA - Recall@20

(f) ACPA - MRR@20

(g) TaoBao - Train BPR Loss

(h) TaoBao - Recall@20

(i) TaoBao - MRR 20

Fig. 9. Train loss and performance of Recall@20 and MRR@20 with training epochs on three datasets.
(a)(b)(c): train BPR loss on three datasets; (d)(e)(f): Recall@20 on three datasets; (g)(h)(i): MRR@20
on three datasets. The results of GRU4Rec, P-Cofactor, P-GraphAE and CAASR are reported here
due to the fact that they share the same BPR loss in sequential training.

5.2.3 Training Loss and Performance. In order to explore the influence of the association
relationships and the way of combining it with sequential relationships captured by RNNs based
sequential recommendation method, we show the train BPR loss, test Recall@20 and test MRR@20
along with training epochs on all three datasets in Figure 9.

It is clear from Figure 9 (a)(d)(g) that CAASR converges faster and achieves a lower BPR loss on
all three datasets than the other three methods. As for Recall@20 and MRR@20 on AIV shown
in Figure 9 (b)(c), CAASR reaches its best performance at epoch less than 5 and later training
causes over-fitting, while the other three models need more training epochs. As for performance
on ACPA and TaoBao in Figure 9 (e)(f)(h)(i), CAASR outperforms the others along the training
process and the gap is more obvious in less training epochs, which reflects the fast convergence
rate of CAASR. In summary, combining item association relationships and sequential pattern in
cascading style fastens the convergence rate of BPR loss, as well as helps the RNNs based sequential
recommendation reach a better learning point for final recommendation target.

5.2.4 Parameter Analysis on Latent Dimension d and Chebyshev Order K. As the
Chebyshev polynomial order K is involved in our method, it is curious to see whether a higher
Chebyshev polynomial order is beneficial to the recommendation accuracy. Towards this target, we
further investigate our CAASR with different K on two datasets: AIV and ACPA (TaoBao dataset is

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

Cascading: Association Augmented Sequential Recommendation

1:21

Table 6. Recall@20 of CAASR with different Chebshev order K and latent dimension d on two
datasets.

Table 7. MRR@20 of CAASR with different Chebshev order K and latent dimension d on two datasets.

Factors

50

100

200

250

300

150
AIV
0.158
0.166
0.167
ACPA
0.101
0.105
0.103

150
AIV
0.050
0.055
0.058
ACPA
0.029
0.031
0.031

0.142
0.142
0.147

0.096
0.102
0.098

0.152
0.161
0.163

0.098
0.104
0.099

0.045
0.047
0.046

0.027
0.032
0.031

0.048
0.050
0.053

0.030
0.033
0.032

K=3
K=4
K=5

K=3
K=4
K=5

K=3
K=4
K=5

K=3
K=4
K=5

0.158
0.167
0.170

0.100
0.114
0.110

0.167
0.167
0.173

0.104
0.116
0.112

0.163
0.174
0.178

0.106
0.123
0.117

0.054
0.056
0.056

0.031
0.036
0.033

0.056
0.059
0.062

0.033
0.037
0.032

0.055
0.060
0.062

0.029
0.039
0.035

Factors

50

100

200

250

300

not implemented here due to our limited computational resource). The results of both Recall@20
and MRR@20 on two datasets are summarized in Table 6 and 7.

As we can see when at the same model capability (with the same latent factors), with the increase
of Chebyshev polynomial order K ranges in [3,4,5], CAASR performs best at K = 5 on AIV. While
for ACPA, CAASR tends to reach its best performance at K = 4. Remind that K denotes the number
of hops from the central node. Too large K may lead to consider uninformative neighborhoods.
Therefor the proper K tends to be related to the specific dataset.

5.2.5 Sparse Train Data. Different sparse train data may have different influence on different
methods. The sparse train data directly leads to sparse item graph, which makes it meaningful to
explore whether our graph embedding based methods could still outperform general GRU4Rec.
Therefore, we conduct an experiment about the model performance with different ratio of train
data while keeping the test data fixed. Figure 10 shows the corresponding result on AIV, ACPA and
TaoBao.

From Figure 10, we can see that, in general, all model performance rises with the increase of
train data. Our cascading CAASR is capable of outperforming other methods under different train
data conditions in terms of both general GRU4Rec method and the parallel methods (P-Cofactor
and P-GraphAE). In Figure 10 (c)(f) on TaoBao, P-GraphAE poses slightly better than CAASR under
some train data ratio conditions while it certainly fails on AIV and ACPA in Figure 10(a)(b)(d)(e).
Instead, the casacading shares a lower computational complexity and more robust performance
than P-GraphAE, which verifies the meaning that we combine item association relationships with
sequential relationships in cascading style for sequential user behavior.

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:22

X. Chen et al.

(a) AIV - Recall@20

(b) ACPA - Recall@20

(c) TaoBao - Recall@20

(d) AIV - MRR@20

(e) ACPA - MRR@20

(f) TaoBao - MRR@20

Fig. 10. Performance of Recall@20 and MRR@20 with different ratio of train data and Chebshev K
on three datasets. (a)(b)(c): Recall@20 on three datasets. (d)(e)(f): MRR@20 on three datasets.

5.2.6 Dropout Regularization. In our method, we apply Dropout to play as the regulariza-
tion technique for RNNs based methods. In order to study the influence of Dropout, we explore the
model performance with different Dropout ratios along the training process on three datasets and
the results are shown in Figure 11.

From the figure, we can see that different Dropout may have different influence on different
datasets. In Figure 11 (a)(d)(g) for three datasets, it is obvious that no dropout (Dropout=0.0) will
encourage the model to obtain lower loss, yet fit the train data better. While the model performance
varies a lot with Dropout=0.0 for three datasets. For instance, in Figure 11 (e)(f) for ACPA and
(h)(i) for TaoBao, CAASR can still reach a not bad performance when Dropout rate equals 0.0,
which means no Dropout technique is utilized. This mainly due to that the data distribution is
complex and no Dropout may encourage the model to fit data better. In contrary, no Dropout
technique is not suitable for AIV dataset in Figure 11 (b)(c), because it is clearly that CAASR
with no Dropout over-fits the data a lot. And Dropout rate equals 0.6 for AIV presents better
performance, which indicates CAASR needs some regularization to avoid over-fitting. In summary,
the Dropout parameter is better chosen according to specific dataset in order to robustly match the
data distribution.

5.3 Qualitative Results

5.3.1 Sequential Recommendation. Remind that our CAASR method integrally considers
the item association relationships and sequential relationships for recommendation in sequential
user behavior. While the typical RNNs based sequential recommendation method GRU4Rec mainly
focus on modeling the sequential relationships. Therefore, it is curious to see whether CAASR
benefits from item association in real recommendation sequence and we conduct an experiment
to verify about this. In particular, Figure 12 (a)(c) show two examples in test set on ACPA and
TaoBao datasets respectively. The corresponding predictions of MRR@20 at each step are stated in
Figure 12 (b) and Figure 12 (d) respectively.

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

Cascading: Association Augmented Sequential Recommendation

1:23

(a) AIV - Train BPR Loss

(b) AIV - Recall@20

(c) AIV - MRR@20

(d) ACPA - Train BPR Loss

(e) ACPA - Recall@20

(f) ACPA - MRR@20

(g) TaoBao - Train BPR Loss

(h) TaoBao - Recall@20

(i) TaoBao - MRR@20

Fig. 11. Train loss and performance of Recall@20 and MRR@20 with different Dropout ratio on three
datasets of CAASR. Dropout=0.0 means the Dropout keep probability equals 1.0. (a)(b)(c): train BPR
loss on three datasets; (d)(e)(f): Recall@20 on three datasets; (g)(h)(i): MRR@20 on three datasets.

ACPA is a dataset of user consuming history on cell phones and accessories. Sequence items in
user1’s history are shown in Figure 12 (a). Compared to cell phone in step 1 and its accessories
at the following stpng, phone screen savers at step 2, vehicle charger at step 3, phone cases at
step 4,5,7 and mobile source at step 6, these items do not have obvious sequential relationships,
they present more kind of association relationships. We notice that in Figure 12 (b), our model
CAASR exactly predicts the phone sticker at step 2 after the cell phone consumption at step 1,
while GRU4Rec fails. This means our model is capable of capturing item association relationships
that GRU4Rec misses. At step 4 and step 5, items both are phone shells. Regarding item association
relationships among items, our model exactly predicts the phone shell at step 5 and puts it at the
first place. This is very practical in real scenarios.

Items in TaoBao dataset take the records of user purchase history on fashion and clothing. From
the consuming sequence of user2 in Figure 12 (c), we notice that the user was inclined to buy some
warm clothes like at step 1, step 3 and step 4 mainly due to the whether. From step 4 to step 5,
the user began to buy some cool clothing. In fact, change from warm clothes to cool ones indeed
exists sequential relationships because of the change of seasons. While in one particular season
or whether, like items at stpng 1,2,3,4 and items at stpng 5,6,7,8, there is no particular sequential
relationships. We note that the pale brown trousers at step 6 and black trousers at step 8 are more
like association relationships rather than sequential relationships. Therefore in Figure 12 (d), we

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:24

X. Chen et al.

(a) user1 sequence on ACPA

(b) predictions for user1 at each step on ACPA

(c) user2 sequence on TaoBao

(d) predictions for user2 at each step on TaoBao

Fig. 12. Examples of sequence recommendation between GRU4Rec and our CAASR on ACPA and
TaoBao datasets. Predictions at each step are listed and the MRR@20 values are tagged as well.
Deeper orange means better MRR@20. And 0 denotes that model does not correctly predict the right
one in top20.

can see that our CAASR, involved with this kind of association relationships, achieves a better
performance than GRU4Rec at these stpng.

5.3.2 Item Embedding Analysis. Both GRU4Rec and our model attempt to learn item
embeddings from user-item interaction data. Similar item embeddings are expected to share similar
properties in terms of user-item interaction. However, GRU4Rec mainly leverages sequential
relationships to learn item representation, whereas our model integrates both the sequential
relationships and association relationships. Therefore, our model is expected to better capture the
item association relationships compared to GRU4Rec.

i j (i, j = 1, · · · , N ) and S 2

To verify the above hypothesis, we calculate the pair-wise item cosine similarity for all item
pairs based on learned item embeddings. Let S 1
i j (i, j = 1, · · · , N ) denote the
cosine similarity of items i and j based on item embeddings for GRU4Rec and CAASR respectively.
We take 8 item pairs with the largest difference S + = S 2 − S 1. The corresponding item images are
shown in Figure 13 (a-h). We can see that the item pairs are mainly complementary products like
skirt with sandals in Figure 13 (b)(d). This suggests that compared to GRU4Rec embeddings, our
CAASR model can better capture the association relationships among complementary products.
We also take 8 item pairs with the largest difference S − = S 1 − S 2. The corresponding item images
are shown in Figure 13 (i-p). In this figure, we see that item pairs mainly focus on items in same
category like shoes in Figure 13 (i)(j) or irrelevant item pairs like shoes with washing liquid in

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

Cascading: Association Augmented Sequential Recommendation

1:25

(a)

(e)

(i)

(m)

(b)

(f)

(j)

(n)

(c)

(g)

(k)

(o)

(d)

(h)

(l)

(p)

Fig. 13. Comparison of cosine similarity based on two types of item embeddings on TaoBao dataset.
The blue bar and yellow bar represent cosine similarity based on GRU4Rec item embedding and
CAASR item embedding respectively. Item pairs with top 8 largest differences in the two types of
cosine similarity are shown. (a-h): CAASR is with higher similarity. (i-p): GRU4Rec is with higher
similarity.

Figure 13 (l) and sandals with snow boots in Figure 13 (k). This suggests that compared to CAASR
embedding, GRU4Rec captures relatively pointless association relationships among products.

This matches well with our expectation as we introduce the item association graph to model
the association relationships in sequential user behavior. Our model is able to recommend the
associated and complementary items. This characteristic benefits the recommendation accuracy in
real scenarios because a user tends to buy an associated or complementary item at next step rather
than some irrelevant products.

6 CONCLUSION AND FUTURE WORK
In this paper, we point that both the association relationships and sequential relationships are existed
in sequential user behavior data. These two inherent characteristics make it essential to capture
both of them for sequential recommendation. Although RNNs based sequential recommendation
methods are capable of model sequential relationships, they fail to put emphasize on association
relationships. In this case, we propose a cascading CAASR model that unifiedly incorporates item
association relationships and sequential relationships for sequential recommendation in cascading
style. In addition, two parallel styles of combining this item association relationships and sequential
relationships are explored and analyzed in this paper to demonstrate the advantages of cascading
style. To the best of our knowledge, CAASR is the first model that concentrates on item association
relationships in RNNs based sequential recommendation. We conduct extensive experiments on
three real-world datasets: AIV, ACPA and TaoBao. Results demonstrate the effectiveness of our
model both quantitatively and qualitatively.

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:26

X. Chen et al.

In future, we will explore one more general and efficient framework to combine the association
relationships and sequential relationships for modeling sequential user behavior and make better
recommendation performance in this scenario.

ACKNOWLEDGMENTS

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

Cascading: Association Augmented Sequential Recommendation

1:27

REFERENCES
[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to

align and translate. arXiv preprint arXiv:1409.0473 (2014).

[2] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating
embeddings for modeling multi-relational data. In Advances in neural information processing systems. 2787–2795.
[3] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2013. Spectral networks and locally connected

networks on graphs. arXiv preprint arXiv:1312.6203 (2013).

[4] Da Cao, Liqiang Nie, Xiangnan He, Xiaochi Wei, Shunzhi Zhu, and Tat-Seng Chua. 2017. Embedding factorization
models for jointly recommending items and user generated lists. In Proceedings of the 40th International ACM SIGIR
Conference on Research and Development in Information Retrieval. ACM, 585–594.

[5] Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, and Tat-Seng Chua. 2017. Sca-cnn: Spatial and
channel-wise attention in convolutional networks for image captioning. In Computer Vision and Pattern Recognition
(CVPR), 2017 IEEE Conference on. IEEE, 6298–6306.

[6] Xu Chen, Hongteng Xu, Yongfeng Zhang, Jiaxi Tang, Yixin Cao, Zheng Qin, and Hongyuan Zha. 2018. Sequential
recommendation with user memory networks. In Proceedings of the Eleventh ACM International Conference on Web
Search and Data Mining. ACM, 108–116.

[7] Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural

machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259 (2014).

[8] Fan RK Chung and Fan Chung Graham. 1997. Spectral graph theory. Number 92. American Mathematical Soc.
[9] Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional neural networks on graphs with

fast localized spectral filtering. In Advances in Neural Information Processing Systems. 3844–3852.

[10] Tim Donkers, Benedikt Loepp, and J¨urgen Ziegler. 2017. Sequential user-based recurrent neural network recommen-

dations. In Proceedings of the Eleventh ACM Conference on Recommender Systems. ACM, 152–160.

[11] Huiji Gao, Jiliang Tang, Xia Hu, and Huan Liu. 2013. Exploring temporal effects for location recommendation on
location-based social networks. In Proceedings of the 7th ACM conference on Recommender systems. ACM, 93–100.

[12] Alex Graves. 2012. Long Short-Term Memory. Springer Berlin Heidelberg. 1735–1780 pages.
[13] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd

ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 855–864.

[14] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Advances

in Neural Information Processing Systems. 1024–1034.

[15] David K Hammond, Pierre Vandergheynst, and R´emi Gribonval. 2011. Wavelets on graphs via spectral graph theory.

Applied and Computational Harmonic Analysis 30, 2 (2011), 129–150.

[16] Ruining He, Wang-Cheng Kang, and Julian McAuley. 2017. Translation-based recommendation. In Proceedings of the

Eleventh ACM Conference on Recommender Systems. ACM, 161–169.

[17] Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual evolution of fashion trends with one-class
collaborative filtering. In proceedings of the 25th international conference on world wide web. International World Wide
Web Conferences Steering Committee, 507–517.

[18] Jon Herlocker, Joseph A Konstan, and John Riedl. 2002. An empirical analysis of design choices in neighborhood-based

collaborative filtering algorithms. Information retrieval 5, 4 (2002), 287–310.

[19] Balzs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations

with Recurrent Neural Networks. (2016).

[20] Bal´azs Hidasi, Massimo Quadrana, Alexandros Karatzoglou, and Domonkos Tikk. 2016. Parallel recurrent neural
network architectures for feature-rich session-based recommendations. In Proceedings of the 10th ACM Conference on
Recommender Systems. ACM, 241–248.

[21] Jin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong Wen, and Edward Y Chang. 2018.

Improving Sequential
Recommendation with Knowledge-Enhanced Memory Networks. In The 41st International ACM SIGIR Conference on
Research & Development in Information Retrieval. ACM, 505–514.

[22] How Jing and Alexander J Smola. 2017. Neural survival recommender. In Proceedings of the Tenth ACM International

Conference on Web Search and Data Mining. ACM, 515–524.

[23] Alexandros Karatzoglou, Xavier Amatriain, Linas Baltrunas, and Nuria Oliver. 2010. Multiverse recommendation:
n-dimensional tensor factorization for context-aware collaborative filtering. In Proceedings of the fourth ACM conference
on Recommender systems. ACM, 79–86.

[24] Nikhil Ketkar. 2017. Convolutional neural networks. In Deep Learning with Python. Springer, 63–78.
[25] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv

preprint arXiv:1609.02907 (2016).

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

1:28

X. Chen et al.

[26] Noam Koenigstein, Gideon Dror, and Yehuda Koren. 2011. Yahoo! music recommendations: modeling music ratings
with temporal dynamics and item taxonomy. In Proceedings of the fifth ACM conference on Recommender systems. ACM,
165–172.

[27] Yehuda Koren. 2009. Collaborative filtering with temporal dynamics. In Proceedings of the 15th ACM SIGKDD

international conference on Knowledge discovery and data mining. ACM, 447–456.

[28] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems.

[29] Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. In Advances in neural

Computer 8 (2009), 30–37.

information processing systems. 2177–2185.

[30] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017. Neural attentive session-based
recommendation. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. ACM,
1419–1428.

[31] Xin Li, Mingming Jiang, Huiting Hong, and Lejian Liao. 2017. A Time-Aware Personalized Point-of-Interest Recom-
mendation via High-Order Tensor Factorization. ACM Transactions on Information Systems. 35, 4, Article 31 (June
2017), 23 pages. https://doi.org/10.1145/3057283

[32] Xiaopeng Li and James She. 2017. Collaborative variational autoencoder for recommender systems. In Proceedings of

the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 305–314.

[33] Dawen Liang, Jaan Altosaar, Laurent Charlin, and David M Blei. 2016. Factorization meets the item embedding:
Regularizing matrix factorization with item co-occurrence. In Proceedings of the 10th ACM conference on recommender
systems. ACM, 59–66.

[34] Kuan Liu, Xing Shi, and Prem Natarajan. 2018. A Sequential Embedding Approach for Item Recommendation with

Heterogeneous Attributes. arXiv preprint arXiv:1805.11008 (2018).

[35] Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. 2018. STAMP: short-term attention/memory priority model
for session-based recommendation. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. ACM, 1831–1839.

[36] Wei Lu, Fu-Lai Chung, Wenhao Jiang, Martin Ester, and Wei Liu. 2018. A Deep Bayesian Tensor-Based System for
Video Recommendation. ACM Transactions on Information Systems. 37, 1, Article 7 (Dec. 2018), 22 pages. https:
//doi.org/10.1145/3233773

[37] St´ephane Mallat. 1999. A wavelet tour of signal processing. Academic press.
[38] Tao Mei, Bo Yang, Xian-Sheng Hua, and Shipeng Li. 2011. Contextual Video Recommendation by Multimodal
Relevance and User Feedback. ACM Transactions on Information Systems. 29, 2, Article 10 (April 2011), 24 pages.
https://doi.org/10.1145/1961209.1961213

[39] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in

vector space. arXiv preprint arXiv:1301.3781 (2013).

[40] Wenjie Pei, Jie Yang, Zhu Sun, Jie Zhang, Alessandro Bozzon, and David MJ Tax. 2017. Interacting Attention-gated
Recurrent Networks for Recommendation. In Proceedings of the 2017 ACM on Conference on Information and Knowledge
Management. ACM, 1459–1468.

[41] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In
Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 701–710.
[42] Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng Guo, Yong Yu, and Xiuqiang He. 2018.
Product-Based Neural Networks for User Response Prediction over Multi-Field Categorical Data. ACM Transactions on
Information Systems. 37, 1, Article 5 (Oct. 2018), 35 pages. https://doi.org/10.1145/3233770

[43] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized
ranking from implicit feedback. In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence.
AUAI Press, 452–461.

[44] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized markov chains for
next-basket recommendation. In Proceedings of the 19th international conference on World wide web. ACM, 811–820.
[45] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning representations by back-propagating

errors. nature 323, 6088 (1986), 533.

[46] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. 2013. The emerging field
of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains.
IEEE Signal Processing Magazine 30, 3 (2013), 83–98.

[47] Ajit P Singh and Geoffrey J Gordon. 2008. Relational learning via collective matrix factorization. In Proceedings of the

14th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 650–658.

[48] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple
way to prevent neural networks from overfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929–1958.

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

Cascading: Association Augmented Sequential Recommendation

1:29

[49] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information
network embedding. In Proceedings of the 24th International Conference on World Wide Web. International World Wide
Web Conferences Steering Committee, 1067–1077.

[50] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph

attention networks. arXiv preprint arXiv:1710.10903 1, 2 (2017).

[51] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015. Collaborative deep learning for recommender systems. In
Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 1235–
1244.

[52] Chao-Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J Smola, and How Jing. 2017. Recurrent recommender networks.

In Proceedings of the tenth ACM international conference on web search and data mining. ACM, 495–503.

[53] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua
Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In International conference
on machine learning. 2048–2057.

[54] Yuan Yao, Wayne Xin Zhao, Yaojing Wang, Hanghang Tong, Feng Xu, and Jian Lu. 2017. Version-Aware Rating
Prediction for Mobile App Recommendation. ACM Transactions on Information Systems. 35, 4, Article 38 (June 2017),
33 pages. https://doi.org/10.1145/3015458

[55] Feng Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2016. A dynamic recurrent model for next basket
recommendation. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in
Information Retrieval. ACM, 729–732.

[56] Quan Yuan, Gao Cong, Zongyang Ma, Aixin Sun, and Nadia Magnenat Thalmann. 2013. Time-aware point-of-interest
recommendation. In Proceedings of the 36th international ACM SIGIR conference on Research and development in
information retrieval. ACM, 363–372.

, Vol. 1, No. 1, Article 1. Publication date: January 2016.

