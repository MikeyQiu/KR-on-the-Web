SparseMAP: Differentiable Sparse Structured Inference

Vlad Niculae 1 André F. T. Martins 2 Mathieu Blondel 3 Claire Cardie 1

8
1
0
2
 
n
u
J
 
0
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
3
2
2
4
0
.
2
0
8
1
:
v
i
X
r
a

Abstract

Structured prediction requires searching over a
combinatorial number of structures. To tackle
it, we introduce SparseMAP: a new method for
sparse structured inference, and its natural loss
function. SparseMAP automatically selects only
a few global structures:
it is situated between
MAP inference, which picks a single structure,
and marginal inference, which assigns nonzero
probability to all structures, including implausi-
ble ones. SparseMAP can be computed using
only calls to a MAP oracle, making it applicable
to problems with intractable marginal inference,
e.g., linear assignment. Sparsity makes gradient
backpropagation efﬁcient regardless of the struc-
ture, enabling us to augment deep neural networks
with generic and sparse structured hidden lay-
ers. Experiments in dependency parsing and nat-
ural language inference reveal competitive accu-
racy, improved interpretability, and the ability to
capture natural language ambiguities, which is
attractive for pipeline systems.

1. Introduction

Structured prediction involves the manipulation of dis-
crete, combinatorial structures, e.g., trees and alignments
(Bakır et al., 2007; Smith, 2011; Nowozin et al., 2014). Such
structures arise naturally as machine learning outputs, and
as intermediate representations in deep pipelines. However,
the set of possible structures is typically prohibitively large.
As such, inference is a core challenge, often sidestepped
by greedy search, factorization assumptions, or continuous
relaxations (Belanger & McCallum, 2016).

1Cornell University,

de Telecomunicações, Lisbon, Portugal
nication Science Laboratories, Kyoto,
spondence
to:
F. T. Martins
ieu Blondel
<cardie@cs.cornell.edu>.

Ithaca, NY 2Unbabel & Instituto
3NTT Commu-
Corre-
Vlad Niculae <vlad@vene.ro>, André
<andre.martins@unbabel.com>,
Math-
Claire Cardie

<mathieu@mblondel.org>,

Japan.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

(cid:63)

MAP

SparseMAP
(cid:63)

argmax
(1, 0, 0)

sparsemax
(.6, .4, 0)

softmax
(.5, .3, .2)

(cid:52)

Marginal
(cid:63)

M

Figure 1. Left: in the unstructured case, softmax and sparsemax
can be interpreted as regularized, differentiable arg max approxi-
mations; softmax returns dense solutions while sparsemax favors
sparse ones. Right: in this work, we extend this view to structured
inference, which consists of optimizing over a polytope M, the
convex hull of all possible structures (depicted: the arborescence
polytope, whose vertices are trees). We introduce SparseMAP as a
structured extension of sparsemax: it is situated in between MAP
inference, which yields a single structure, and marginal inference,
which returns a dense combination of structures.

In this paper, we propose an appealing alternative: a new
inference strategy, dubbed SparseMAP, which encourages
sparsity in the structured representations. Namely, we seek
solutions explicitly expressed as a combination of a small,
enumerable set of global structures. Our framework departs
from the two most common inference strategies in struc-
tured prediction: maximum a posteriori (MAP) inference,
which returns the highest-scoring structure, and marginal
inference, which yields a dense probability distribution over
structures. Neither of these strategies is fully satisfactory:
for latent structure models, marginal inference is appealing,
since it can represent uncertainty and, unlike MAP inference,
it is continuous and differentiable, hence amenable for use
in structured hidden layers in neural networks (Kim et al.,
2017). It has, however, several limitations. First, there are
useful problems for which MAP is tractable, but marginal in-
ference is not, e.g., linear assignment (Valiant, 1979; Taskar,
2004). Even when marginal inference is available, case-by-
case derivation of the backward pass is needed, sometimes
producing fairly complicated algorithms, e.g., second-order
expectation semirings (Li & Eisner, 2009). Finally, marginal
inference is dense: it assigns nonzero probabilities to all
structures and cannot completely rule out irrelevant ones.
This can be statistically and computationally wasteful, as
well as qualitatively harder to interpret.

In this work, we make the following contributions:

2. Preliminaries

SparseMAP: Differentiable Sparse Structured Inference

1. We propose SparseMAP: a new framework for sparse
structured inference (§3.1). The main idea is illustrated
in Figure 1. SparseMAP is a twofold generalization:
ﬁrst, as a structured extension of the sparsemax transfor-
mation (Martins & Astudillo, 2016); second, as a con-
tinuous yet sparse relaxation of MAP inference. MAP
yields a single structure and marginal inference yields
a dense distribution over all structures. In contrast, the
SparseMAP solutions are sparse combinations of a small
number of often-overlapping structures.

2. We show how to compute SparseMAP effectively, re-
quiring only a MAP solver as a subroutine (§3.2), by
exploiting the problem’s sparsity and quadratic curvature.
Noticeably, the MAP oracle can be any arbitrary solver,
e.g., the Hungarian algorithm for linear assignment,
which permits tackling problems for which marginal
inference is intractable.

3. We derive expressions for gradient backpropagation
through SparseMAP inference, which, unlike MAP, is
differentiable almost everywhere (§3.3). The backward
pass is fully general (applicable to any type of structure),
and it is efﬁcient, thanks to the sparsity of the solutions
and to reusing quantities computed in the forward pass.

4. We introduce a novel SparseMAP loss for structured pre-
diction, placing it into a family of loss functions which
generalizes the CRF and structured SVM losses (§4).
Inheriting the desirable properties of SparseMAP infer-
ence, the SparseMAP loss and its gradients can be com-
puted efﬁciently, provided access to MAP inference.

Our experiments demonstrate that SparseMAP is useful
both for predicting structured outputs, as well as for learn-
ing latent structured representations. On dependency pars-
ing (§5.1), structured output networks trained with the
SparseMAP loss yield more accurate models with sparse,
interpretable predictions, adapting to the ambiguity (or lack
thereof) of test examples. On natural language inference
(§5.2), we learn latent structured alignments, obtaining good
predictive performance, as well as useful natural visualiza-
tions concentrated on a small number of structures.1

∈

∈

Rm, b

Rn, [a; b]
Notation. Given vectors a
∈
Rm+n denotes their concatenation; given matrices A
∈
Rm×k, B
Rn×k, we denote their row-wise stacking as
∈
R(m+n)×k. We denote the columns of a matrix A
[A; B]
by aj; by extension, a slice of columns of A is denoted AI
. We denote the canonical simplex by
for a set of indices
I
Rd : y
y
i=1 yi = 1
, and the indicator
(cid:52)
}
(cid:23)
{
function of a predicate p as I[p] =

1 if p, 0 otherwise

0, (cid:80)d

d :=

∈

∈

.

{

}

1 General-purpose dynet and pytorch implementations avail-

able at https://github.com/vene/sparsemap.

2.1. Regularized Max Operators: Softmax, Sparsemax

As a basis for the more complex structured case, we ﬁrst
consider the simple problem of selecting the largest value
in a vector θ

Rd. We denote the vector mapping

∈

arg max(θ) := arg max

θ(cid:62)y.

y∈(cid:52)d

When there are no ties, arg max has a unique solution ei
peaking at the index i of the highest value of θ. When
there are ties, arg max is set-valued. Even assuming no
ties, arg max is piecewise constant, and thus is ill-suited for
direct use within neural networks, e.g., in an attention mech-
anism. Instead, it is common to use softmax, a continuous
and differentiable approximation to arg max, which can be
seen as an entropy-regularized arg max

y∈(cid:52)d

(cid:80)

softmax(θ) := arg max

θ(cid:62)y + H(y) =

exp θ
i=1 exp θi

(cid:80)d

(1)

where H(y) =
−
entropy. Since exp
·

i yi ln yi, i.e. the negative Shannon
> 0 strictly, softmax outputs are dense.

By replacing the entropic penalty with a squared (cid:96)2 norm,
Martins & Astudillo (2016) introduced a sparse alternative
to softmax, called sparsemax, given by

sparsemax(θ) := arg max

θ(cid:62)y

y∈(cid:52)d

= arg min

y

y∈(cid:52)d (cid:107)

−

y

2
2
(cid:107)

1
2 (cid:107)
2
2 .
(cid:107)

−

θ

(2)

Both softmax and sparsemax are continuous and differen-
tiable almost everywhere; however, sparsemax encourages
sparsity in its outputs. This is because it corresponds to an
Euclidean projection onto the simplex, which is likely to hit
its boundary as the magnitude of θ increases. Both mech-
anisms, as well as variants with different penalties (Nic-
ulae & Blondel, 2017), have been successfully used in
attention mechanisms, for mapping a score vector θ to
a d-dimensional normalized discrete probability distribu-
tion over a small set of choices. The relationship between
arg max, softmax, and sparsemax, illustrated in Figure 1,
sits at the foundation of SparseMAP.

2.2. Structured Inference

In structured prediction, the space of possible outputs is
typically very large: for instance, all possible labelings of
a length-n sequence, spanning trees over n nodes, or one-
to-one alignments between two sets. We may still write
optimization problems such as maxD
s=1 θs, but it is imprac-
tical to enumerate all of the D possible structures and, in
turn, to specify the scores for each structure in θ.

SparseMAP: Differentiable Sparse Structured Inference

Instead, structured problems are often parametrized through
structured log-potentials (scores) θ := A(cid:62)η, where A
∈
Rk×D is a matrix that speciﬁes the structure of the problem,
Rk is lower-dimensional parameter vector, i.e.,
and η
∈
D. For example, in a factor graph (Kschischang
k
et al., 2001) with variables U and factors F , θ is given by

(cid:28)

θs :=

ηU,i(si) +

ηF,f (sf ),

(cid:88)

i∈U

(cid:88)

f ∈F

where ηU and ηF are unary and higher-order log-potentials,
and si and sf are local conﬁgurations at variable and fac-
tor nodes. This can be written in matrix notation as θ =
M (cid:62)ηU +N (cid:62)ηF for suitable matrices
, ﬁtting the
}
assumption above with A = [M ; N ] and η = [ηU ; ηF ].

M , N

{

We can then rewrite the MAP inference problem, which
seeks the highest-scoring structure, as a k-dimensional prob-
Rk to denote conﬁg-
lem, by introducing variables [u; v]
∈
urations at variable and factor nodes:2

MAPA(η) := arg max
u:=M y
y∈(cid:52)D

θ(cid:62)y

= arg max

u: [u;v]∈MA

U u + η(cid:62)
η(cid:62)

F v,

(3)

M

A :=

[u; v] : u = M y, v = N y, y
{

is
where
the marginal polytope (Wainwright & Jordan, 2008), with
one vertex for each possible structure (Figure 1). However,
as previously said, since it is equivalent to a D-dimensional
arg max, MAP is piecewise constant and discontinuous.

∈ (cid:52)

}

D

Negative entropy regularization over y, on the other hand,
yields marginal inference,

Sequence tagging. Consider a sequence of n items, each
assigned one out of a possible m tags. In this case, a global
structure s is a joint assignment of tags (t1,
, tn). The
matrix M is nm-by-mn–dimensional, with columns ms
∈
nm := [et1, ..., etn ] indicating which tag is assigned
0, 1
{
}
to each variable in the global structure s. N is nm2-by-
mn–dimensional, with ns encoding the transitions between
consecutive tags, i.e., ns(i, a, b) := I[ti−1 = a & ti = b].
The Viterbi algorithm provides MAP inference and forward-
backward provides marginal inference (Rabiner, 1989).

· · ·

Non-projective dependency parsing. Consider a sentence
of length n. Here, a structure s is a dependency tree: a
rooted spanning tree over the n2 possible arcs (for example,
the arcs above the sentences in Figure 3). Each column
n2
ms
encodes a tree by assigning a 1 to its arcs.
0, 1
}
∈ {
N is empty,
A is known as the arborescence polytope
(Martins et al., 2009). MAP inference may be performed
by maximal arborescence algorithms (Chu & Liu, 1965;
Edmonds, 1967; McDonald et al., 2005), and the Matrix-
Tree theorem (Kirchhoff, 1847) provides a way to perform
marginal inference (Koo et al., 2007; Smith & Smith, 2007).

M

Linear assignment. Consider a one-to-one matching (lin-
ear assignment) between two sets of n nodes. A global
n2
structure s is a n-permutation, and a column ms
can be seen as a ﬂattening of the corresponding permu-
tation matrix. Again, N is empty.
A is the Birkhoff
polytope (Birkhoff, 1946), and MAP inference can be per-
formed by, e.g., the Hungarian algorithm (Kuhn, 1955) or
the Jonker-Volgenant algorithm (Jonker & Volgenant, 1987).
Noticeably, marginal inference is known to be #P-complete
(Valiant, 1979; Taskar, 2004, Section 3.5). This makes it an
open problem how to use matchings as latent variables.

0, 1
}

∈ {

M

MarginalA(η) := arg max
u:=M y
y∈(cid:52)D

θ(cid:62)y + H(y)

3. SparseMAP

= arg max

u: [u;v]∈MA

U u + η(cid:62)
η(cid:62)

F v + HA(u, v).

(4)
Marginal inference is differentiable, but may be more difﬁ-
cult to compute; the entropy HA(u, v) = H(y) itself lacks
a closed form (Wainwright & Jordan, 2008, §4.1.2). Gradi-
ent backpropagation is available only to specialized problem
instances, e.g. those solvable by dynamic programming (Li
& Eisner, 2009). The entropic term regularizes y toward
more uniform distributions, resulting in strictly dense solu-
tions, just like in the case of softmax (Equation 1).

Armed with the parallel between structured inference and
regularized max operators described in §2, we are now ready
to introduce SparseMAP, a novel inference optimization
problem which returns sparse solutions.

3.1. Deﬁnition

We introduce SparseMAP by regularizing the MAP infer-
ence problem in Equation 3 with a squared (cid:96)2 penalty on
2
the returned posteriors, i.e., 1
2. Denoting, as above,
θ := A(cid:62)η, the result is a quadratic optimization problem,

u
(cid:107)

2 (cid:107)

Interesting types of structures, which we use in the experi-
ments described in Section 5, include the following.

SparseMAPA(η) := arg max
u:=M y
y∈(cid:52)D

θ(cid:62)y

1
2 (cid:107)

−

M y

2
2
(cid:107)

2We use the notation arg maxu: [u;v]∈M to convey that the
maximization is over both u and v, but only u is returned. Sepa-
rating the variables as [u; v] loses no generality and allows us to
isolate the unary posteriors u as the return value of interest.

= arg max
u: [u,v]∈MA

U u + η(cid:62)
η(cid:62)
F v

1
2 (cid:107)

u

−

2
2 .
(cid:107)
(5)

SparseMAP: Differentiable Sparse Structured Inference

A linear approximation to f around a point [u(cid:48); v(cid:48)] is

ˆf (u, v) := (

uf )(cid:62)u+(

vf )(cid:62)v = (ηU

u(cid:48))(cid:62)u+η(cid:62)

F v.

∇

∇

−

M

Minimizing ˆf over
is exactly MAP inference with ad-
u(cid:48). Intuitively, at each step
justed variable scores ηU
−
we seek a high-scoring structure while penalizing sharing
variables with already-selected structures Vanilla CG simply
adds the new structure to the active set at every iteration.
The pairwise and away-step variants trade off between the
direction toward the new structure, and away from one of
the already-selected structures. More sophisticated variants
have been proposed (Garber & Meshi, 2016) which can
provide sparse solutions when optimizing over a polytope.

Active set method. Importantly, the SparseMAP problem
in Equation 5 has quadratic curvature, which the general
CG algorithms may not optimally leverage. For this reason,
we consider the active set method for constrained QPs: a
generalization of Wolfe’s min-norm point algorithm (Wolfe,
1976), also used in structured prediction for the quadratic
subproblems by Martins et al. (2015). The active set algo-
rithm, at each iteration, updates an estimate of the solution
support by adding or removing one constraint to/from the
active set; then it solves the Karush–Kuhn–Tucker (KKT)
system of a relaxed QP restricted to the current support.

Comparison. Both algorithms enjoy global linear conver-
gence with similar rates (Lacoste-Julien & Jaggi, 2015),
but the active set algorithm also exhibits exact ﬁnite
convergence—this allows it, for instance, to capture the op-
timal sparsity pattern (Nocedal & Wright, 1999, Ch. 16.4 &
16.5). Vinyes & Obozinski (2017) provide a more in-depth
discussion of the connections between the two algorithms.
We perform an empirical comparison on a dependency pars-
ing instance with random potentials. Figure 2 shows that
active set substantially outperforms all CG variants, both in
terms of objective value as well as in the solution sparsity,
suggesting that the quadratic curvature makes SparseMAP
solvable in very few iterations to high accuracy. We there-
fore use the active set solver in the remainder of the paper.

3.3. Backpropagating Gradients through SparseMAP

In order to use SparseMAP as a neural network layer trained
with backpropagation, one must compute products of the
SparseMAP Jacobian with a vector p. Computing the Ja-
cobian of an optimization problem is an active research
topic known as argmin differentiation, and is generally difﬁ-
cult. Fortunately, as we show next, argmin differentiation is
always easy and efﬁcient in the case of SparseMAP.

Figure 2. Comparison of solvers on the SparseMAP optimization
problem for a tree factor with 20 nodes. The active set solver
converges much faster and to a much sparser solution.

The quadratic penalty replaces the entropic penalty from
marginal inference (Equation 4), which pushes the solutions
to the strict interior of the marginal polytope. In conse-
quence, SparseMAP favors sparse solutions from the faces
of the marginal polytope
A, as illustrated in Figure 1.
For the structured prediction problems mentioned in Sec-
tion 2.2, SparseMAP would be able to return, for example,
a sparse combination of sequence labelings, parse trees, or
matchings. Moreover, the strongly convex regularization
on u ensures that SparseMAP has a unique solution and is
differentiable almost everywhere, as we will see.

M

3.2. Solving SparseMAP

We now tackle the optimization problem in Equation 5.
Although SparseMAP is a QP over a polytope, even de-
scribing it in standard form is infeasible, since enumerating
the exponentially-large set of vertices is infeasible. This
prevents direct application of, e.g., the generic differentiable
QP solver of Amos & Kolter (2017). We instead focus on
SparseMAP solvers that involve a sequence of MAP prob-
lems as a subroutine—this makes SparseMAP widely appli-
cable, given the availability of MAP implementations for
various structures. We discuss two such methods, one based
on the conditional gradient algorithm and another based
on the active set method for quadratic programming. We
provide a full description of both methods in Appendix A.

Conditional gradient. One family of such solvers is based
on the conditional gradient (CG) algorithm (Frank & Wolfe,
1956; Lacoste-Julien & Jaggi, 2015), considered in prior
work for solving approximations of the marginal inference
problem (Belanger et al., 2013; Krishnan et al., 2015). Each
step must solve a linearized subproblem. Denote by f the
SparseMAP objective from Equation 5,

f (u, v) := η(cid:62)

U u + η(cid:62)
F v

1
2 (cid:107)

u

2
2 .
(cid:107)

−

The gradients of f with respect to the two variables are

uf (u(cid:48), v(cid:48)) = ηU

u(cid:48),

vf (u(cid:48), v(cid:48)) = ηV .

∇

−

∇

Proposition 1 Denote a SparseMAP solution by y(cid:63) and
. Then, SparseMAP is
s : ys > 0
its support by
}
{

:=

I

differentiable almost everywhere with Jacobian

by Ω(cid:52) its restriction to

RD, i.e.,

SparseMAP: Differentiable Sparse Structured Inference

) = D(

)(cid:62)given by

∂u(cid:63)
∂η

I

= M D(
(cid:40)(cid:0)I
0

−

d(

)s :=

I

)A(cid:62), where D(

I
1T Z1 Z11T (cid:1) zs,

1

Z := (MI

(cid:62)MI)−1.

I

,

s
s /

∈ I
∈ I

O

(k

The proof, given in Appendix B, relies on the KKT con-
ditions of the SparseMAP QP. Importantly, because D(
)
I
is zero outside of the support of the solution, computing
the Jacobian only requires the columns of M and A corre-
sponding to the structures in the active set. Moreover, when
using the active set algorithm discussed in §3.2, the matrix
Z is readily available as a byproduct of the forward pass.
).
The backward pass can, therefore, be computed in

|I|
Our approach for gradient computation draws its efﬁciency
from the solution sparsity and does not depend on the type
of structure considered. This is contrasted with two related
lines of research. The ﬁrst is “unrolling” iterative inference
algorithms, for instance belief propagation (Stoyanov et al.,
2011; Domke, 2013) and gradient descent (Belanger et al.,
2017), where the backward pass complexity scales with the
number of iterations. In the second, employed by Kim et al.
(2017), when inference can be performed via dynamic pro-
gramming, backpropagation can be performed using second-
order expectation semirings (Li & Eisner, 2009) or more
general smoothing (Mensch & Blondel, 2018), in the same
time complexity as the forward pass. Moreover, in our ap-
proach, neither the forward nor the backward passes involve
logarithms, exponentiations or log-domain classes, avoiding
the slowdown and stability issues normally incurred.

In the unstructured case, since M = I, Z is also an iden-
tity matrix, uncovering the sparsemax Jacobian (Martins &
Astudillo, 2016). In general, structures are not necessarily
orthogonal, but may have degrees of overlap.

4. Structured Fenchel-Young Losses
and the SparseMAP Loss

With the efﬁcient algorithms derived above in hand, we
switch gears to deﬁning a SparseMAP loss function. Struc-
tured output prediction models are typically trained by min-
imizing a structured loss measuring the discrepancy be-
tween the desired structure (encoded, for instance, as an
indicator vector y = es) and the prediction induced by the
log-potentials η. We provide here a general family of struc-
tured prediction losses that will make the newly proposed
SparseMAP loss arise as a very natural case. Below, we let
R denote a convex penalty function and denote
Ω : RD

→

D

(cid:52)
(cid:40)

⊂

,

∞

Ω(y), y

D;
D.

y /

∈ (cid:52)
∈ (cid:52)

Ω(cid:52)(y) :=

The Fenchel convex conjugate of Ω(cid:52) is

Ω(cid:63)

(cid:52)(θ) := sup
y∈RD

θ(cid:62)y

Ω(cid:52)(y) = sup
y∈(cid:52)D

−

θ(cid:62)y

−

Ω(y).

We next introduce a family of structured prediction losses,
named after the corresponding Fenchel-Young duality gap.

Deﬁnition 1 (Fenchel-Young losses) Given
convex
penalty function Ω : RD
D)-dimensional
matrix A = [M ; N ] encoding the structure of the problem,
we deﬁne the following family of structured losses:

R, and a (k

→

×

a

(cid:96)Ω,A(η, y) := Ω(cid:63)

(cid:52)(A(cid:62)η) + Ω(cid:52)(y)

η(cid:62)Ay.

(6)

−

This family, studied in more detail in (Blondel et al., 2018),
includes the commonly-used structured losses:

• Structured perceptron (Collins, 2002): Ω

0;

≡

• Structured SVM (Taskar et al., 2003; Tsochantaridis
, ¯y) for a cost function ρ, where ¯y

et al., 2004): Ω
≡
is the true output;

ρ(
·

• CRF (Lafferty et al., 2001): Ω

H;

≡ −

• Margin CRF (Gimpel & Smith, 2010):
, ¯y).
·

H + ρ(

≡ −

Ω

This leads to a natural way of deﬁning SparseMAP losses,
by plugging the following into Equation 6:
• SparseMAP loss: Ω(y) = 1
2 (cid:107)
• Margin SparseMAP: Ω(y) = 1

2
2,
(cid:107)
M y

M y

2 (cid:107)

2
2 + ρ(y, ¯y).
(cid:107)

It is well-known that the subgradients of structured percep-
tron and SVM losses consist of MAP inference, while the
CRF loss gradient requires marginal inference. Similarly,
the subgradients of the SparseMAP loss can be computed
via SparseMAP inference, which in turn only requires MAP.
The next proposition states properties of structured Fenchel-
Young losses, including a general connection between a loss
and its corresponding inference method.

Proposition 2 Consider a convex Ω and a structured model
Rk×D. Denote the inference
deﬁned by the matrix A
∈
objective fΩ(y) := η(cid:62)Ay
Ω(y), and a solution y(cid:63) :=
arg max
y∈(cid:52)D

−
fΩ(y). Then, the following properties hold:

1. (cid:96)Ω,A(η, y)

0, with equality when fΩ(y) = fΩ(y(cid:63));

≥

2. (cid:96)Ω,A(η, y) is convex, ∂(cid:96)Ω,A(η, y)

(cid:51)
3. (cid:96)tΩ,A(η, y) = t(cid:96)Ω(η/t, y) for any t

A(y(cid:63)

y);
−
R, t > 0.

∈

SparseMAP: Differentiable Sparse Structured Inference

Table 1. Unlabeled attachment accuracy scores for dependency
parsing, using a bi-LSTM model (Kiperwasser & Goldberg, 2016).
SparseMAP and its margin version, m-SparseMAP, produce the
best parser on 4/5 datasets. For context, we include the scores
of the CoNLL 2017 UDPipe baseline, which is trained under the
same conditions (Straka & Straková, 2017).
vi

Loss

en

zh

ro

ja

Structured SVM 87.02
86.74
86.90
87.34

CRF
SparseMAP
m-SparseMAP

81.94
83.18
84.03
82.63

69.42
69.10
69.71
70.87

87.58
87.13
87.35
87.63

96.24
96.09
96.04
96.03

UDPipe baseline

87.68

82.14

69.63

87.36

95.94

Proof is given in Appendix C. Property 1 suggests that pmin-
imizing (cid:96)Ω,A aligns models with the true label. Property
2 shows how to compute subgradients of (cid:96)Ω,A provided
Rk. Com-
access to the inference output [u(cid:63); v(cid:63)] = Ay(cid:63)
bined with our efﬁcient procedure described in Section 3.2,
it makes the SparseMAP losses promising for structured
prediction. Property 3 suggests that the strength of the
penalty Ω can be adjusted by simply scaling η. Finally, we
remark that for a strongly-convex Ω, (cid:96)Ω,A can be seen as a
smoothed perceptron loss; other smoothed losses have been
explored by Shalev-Shwartz & Zhang (2016).

∈

5. Experimental Results

In this section, we experimentally validate SparseMAP on
two natural language processing applications, illustrating
the two main use cases presented: structured output predic-
tion with the SparseMAP loss (§5.1) and structured hidden
layers (§5.2). All models are implemented using the dynet
library v2.0.2 (Neubig et al., 2017).

5.1. Dependency Parsing with the SparseMAP Loss

We evaluate the SparseMAP losses against the commonly
used CRF and structured SVM losses. The task we focus on
is non-projective dependency parsing: a structured output
task consisting of predicting the directed tree of grammatical
dependencies between words in a sentence (Jurafsky & Mar-
tin, 2018, Ch. 14). We use annotated Universal Dependency
data (Nivre et al., 2016), as used in the CoNLL 2017 shared
task (Zeman et al., 2017). To isolate the effect of the loss,
we use the provided gold tokenization and part-of-speech
tags. We follow closely the bidirectional LSTM arc-factored
parser of Kiperwasser & Goldberg (2016), using the same
model conﬁguration; the only exception is not using exter-
nally pretrained embeddings. Parameters are trained using
Adam (Kingma & Ba, 2015), tuning the learning rate on the
10−3, expanded by a factor of 2 if the
grid
best model is at either end.

.5, 1, 2, 4, 8

} ×

{

Table 2. Test accuracy scores for natural language inference with
structured and unstructured variants of ESIM. In parentheses: the
percentage of pairs of words with nonzero alignment scores.

ESIM variant

MultiNLI

SNLI

softmax
sequential
matching

76.05 (100%)
75.54 (13%)
76.13
(8%)

86.52 (100%)
86.62 (19%)
86.05 (15%)

family and in terms of the amount of training data (ranging
from 1,400 sentences for Vietnamese to 12,525 for English).
Test set results (Table 1) indicate that the SparseMAP losses
outperform the SVM and CRF losses on 4 out of the 5
languages considered. This suggests that SparseMAP is
a good middle ground between MAP-based and marginal-
based losses in terms of smoothness and gradient sparsity.

Moreover, as illustrated in Figure 4, the SparseMAP loss
encourages sparse predictions: models converge towards
sparser solutions as they train, yielding very few ambigu-
ous arcs. When conﬁdent, SparseMAP can predict a single
tree. Otherwise, the small set of candidate parses returned
can be easily visualized, often indicating genuine linguistic
ambiguities (Figure 3). Returning a small set of parses, also
sought concomittantly by Keith et al. (2018), is valuable in
pipeline systems, e.g., when the parse is an input to a down-
stream application: error propagation is diminished in cases
where the highest-scoring tree is incorrect (which is the case
for the sentences in Figure 3). Unlike K-best heuristics,
SparseMAP dynamically adjusts its output sparsity, which
is desirable on realistic data where most instances are easy.

5.2. Latent Structured Alignment
for Natural Language Inference

In this section, we demonstrate SparseMAP for inferring
latent structure in large-scale deep neural networks. We
focus on the task of natural language inference, deﬁned as
the classiﬁcation problem of deciding, given two sentences
(a premise and a hypothesis), whether the premise entails
the hypothesis, contradicts it, or is neutral with respect to it.

We consider novel structured variants of the state-of-the-art
ESIM model (Chen et al., 2017). Given a premise P of
length m and a hypothesis H of length n, ESIM:

1. Encodes P and H with an LSTM.

2. Computes alignment scores G

Rm×n; with gij the

inner product between the P word i and H word j.

∈

3. Computes P-to-H and H-to-P alignments using row-wise,

respectively column-wise softmax on G.

4. Augments P words with the weighted average of its

aligned H words, and vice-versa.

We experiment with 5 languages, diverse both in terms of

5. Passes the result through another LSTM, then predicts.

SparseMAP: Differentiable Sparse Structured Inference

1.0
1.0

1.0
1.0

1.0

.32

.45

1.0

1.0

1.0

1.0

1.0

.55

.68

.24

.76

(cid:63) They did a vehicle wrap for my Toyota Venza that looks amazing .

(cid:63) the broccoli looks browned around the edges .

Figure 3. Example of ambiguous parses from the UD English validation set. SparseMAP selects a small number of candidate parses (left:
three, right: two), differing from each other in a small number of ambiguous dependency arcs. In both cases, the desired gold parse is
among the selected trees (depicted by the arcs above the sentence), but it is not the highest-scoring one.

Figure 4. Distribution of the tree sparsity (top) and arc sparsity (bottom) of SparseMAP solutions during training on the Chinese dataset.
Shown are respectively the number of trees and the average number of parents per word with nonzero probability.

We consider the following structured replacements for the
independent row-wise and column-wise softmaxes (step 3):

Sequential alignment. We model the alignment of p to
h as a sequence tagging instance of length m, with n pos-
sible tags corresponding to the n words of the hypothesis.
Through transition scores, we enable the model to capture
continuity and monotonicity of alignments: we parametrize
transitioning from word t1 to t2 by binning the distance
t2
1, 0, 1, 2 or more
.
}
We similarly parametrize the initial alignment using bins
1, 2 or more
,
{
}
}
allowing the model to express whether an alignment starts
at the beginning or ends on the ﬁnal word of h; formally

and the ﬁnal alignment as

t1 into 5 groups,

2 or less,

2 or less,

1
−

{−

{−

−

−

ηF (i, t1, t2) :=






wbin(t2−t1)
wstart
wend

bin(t2)

bin(t1)

0 < i < n,
i = 0,

i = n.

We align p to h applying the same method in the other direc-
tion, with different transition scores w. Overall, sequential
alignment requires learning 18 additional scalar parameters.

Matching alignment. We now seek a symmetrical align-
ment in both directions simultaneously. To this end, we cast
the alignment problem as ﬁnding a maximal weight bipar-
tite matching. We recall from §2.2 that a solution can be
found via the Hungarian algorithm (in contrast to marginal
inference, which is #P-complete). When n = m, maximal

matchings can be represented as permutation matrices, and
= m some words remain unaligned. SparseMAP
when n
returns a weighted average of a few maximal matchings.
This method requires no additional learned parameters.

We evaluate the two models alongside the softmax baseline
on the SNLI (Bowman et al., 2015) and MultiNLI (Williams
et al., 2018) datasets.3 All models are trained by SGD,
with 0.9
learning rate decay at epochs when the validation
×
accuracy is not the best seen. We tune the learning rate on
the grid (cid:8)2k : k
(cid:9), extending the range
}
−
if the best model is at either end. The results in Table 2 show
that structured alignments are competitive with softmax in
terms of accuracy, but are orders of magnitude sparser. This
sparsity allows them to produce global alignment structures
that are interpretable, as illustrated in Figure 5.

∈ {−

5,

4,

−

−

6,

3

Interestingly, we observe computational advantages of spar-
sity. Despite the overhead of GPU memory copying, both
training and validation in our latent structure models take
roughly the same time as with softmax and become faster as
the models grow more certain. For the sake of comparison,
Kim et al. (2017) report a 5
slow-down in their structured
×
attention networks, where they use marginal inference.

3We split the MultiNLI matched validation set into equal vali-

dation and test sets; for SNLI we use the provided split.

SparseMAP: Differentiable Sparse Structured Inference

a
gentleman
overlooking
a
neighborhood
situation
.

a
police
ofﬁcer
watches
a
situation
closely
.

(a) softmax

(b) sequence

(c) matching

Figure 5. Latent alignments on an example from the SNLI validation set, correctly predicted as neutral by all compared models. The
premise is on the y-axis, the hypothesis on the x-axis. Top: columns sum to 1; bottom: rows sum to 1. The matching alignment mechanism
yields a symmetrical alignment, and is thus shown only once. Softmax yields a dense alignment (nonzero weights are marked with a
border). The structures selected by sequential alignment are overlayed as paths; the selected matchings are displayed in the top right.

6. Related Work

Structured attention networks. Kim et al. (2017) and
Liu & Lapata (2018) take advantage of the tractability of
marginal inference in certain structured models and de-
rive specialized backward passes for structured attention.
In contrast, our approach is modular and general: with
SparseMAP, the forward pass only requires MAP inference,
and the backward pass is efﬁciently computed based on the
forward pass results. Moreover, unlike marginal inference,
SparseMAP yields sparse solutions, which is an appealing
property statistically, computationally, and visually.

K-best inference. As it returns a small set of structures,
SparseMAP brings to mind K-best inference, often used
in pipeline NLP systems for increasing recall and handling
uncertainty (Yang & Cardie, 2013). K-best inference can
be approximated (or, in some cases, solved), roughly K
times slower than MAP inference (Yanover & Weiss, 2004;
Camerini et al., 1980; Chegireddy & Hamacher, 1987;
Fromer & Globerson, 2009). The main advantages of
SparseMAP are convexity, differentiablity, and modular-
ity, as SparseMAP can be computed in terms of MAP sub-
problems. Moreover, it yields a distribution, unlike K-best,
which does not reveal the gap between selected structures,

Learning permutations. A popular approach for differen-
tiable permutation learning involves mean-entropic optimal
transport relaxations (Adams & Zemel, 2011; Mena et al.,
2018). Unlike SparseMAP, this does not apply to general

structures, and solutions are not directly expressible as com-
binations of a few permutations.

Regularized inference. Ravikumar et al. (2010), Meshi
et al. (2015), and Martins et al. (2015) proposed (cid:96)2 per-
turbations and penalties in various related ways, with the
goal of solving LP-MAP approximate inference in graph-
ical models. In contrast, the goal of our work is sparse
structured prediction, which is not considered in the afore-
mentioned work. Nevertheless, some of the formulations in
their work share properties with SparseMAP; exploring the
connections further is an interesting avenue for future work.

7. Conclusion

We introduced a new framework for sparse structured infer-
ence, SparseMAP, along with a corresponding loss function.
We proposed efﬁcient ways to compute the forward and
backward passes of SparseMAP. Experimental results illus-
trate two use cases where sparse inference is well-suited. For
structured prediction, the SparseMAP loss leads to strong
models that make sparse, interpretable predictions, a good
ﬁt for tasks where local ambiguities are common, like many
natural language processing tasks. For structured hidden
layers, we demonstrated that SparseMAP leads to strong,
interpretable networks trained end-to-end. Modular by de-
sign, SparseMAP can be applied readily to any structured
problem for which MAP inference is available, including
combinatorial problems such as linear assignment.

SparseMAP: Differentiable Sparse Structured Inference

Acknowledgements

We thank Tim Vieira, David Belanger, Jack Hessel, Justine
Zhang, Sydney Zink, the Unbabel AI Research team, and
the three anonymous reviewers for their insightful com-
ments. This work was supported by the European Re-
search Council (ERC StG DeepSPIN 758969) and by the
Fundação para a Ciência e Tecnologia through contracts
UID/EEA/50008/2013, PTDC/EEI-SII/7092/2014 (Learn-
Big), and CMUPERI/TIC/0046/2014 (GoLocal).

References

Adams, R. P. and Zemel, R. S. Ranking via sinkhorn propa-

gation. arXiv e-prints, 2011.

Amos, B. and Kolter, J. Z. OptNet: Differentiable optimiza-

tion as a layer in neural networks. In ICML, 2017.

Bakır, G., Hofmann, T., Schölkopf, B., Smola, A. J., Taskar,
B., and Vishwanathan, S. V. N. Predicting Structured
Data. The MIT Press, 2007.

Belanger, D. and McCallum, A. Structured prediction en-

ergy networks. In ICML, 2016.

Belanger, D., Sheldon, D., and McCallum, A. Marginal
inference in MRFs using Frank-Wolfe. In NIPS Workshop
on Greedy Opt., FW and Friends, 2013.

Belanger, D., Yang, B., and McCallum, A. End-to-end
learning for structured prediction energy networks. In
ICML, 2017.

Birkhoff, G. Tres observaciones sobre el algebra lineal.

Univ. Nac. Tucumán Rev. Ser. A, 5:147–151, 1946.

Blondel, M., Martins, A. F., and Niculae, V. Learning clas-
siﬁers with Fenchel-Young losses: Generalized entropies,
margins, and algorithms. arXiv e-prints, 2018.

Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D.
A large annotated corpus for learning natural language
inference. In EMNLP, 2015.

Boyd, S. and Vandenberghe, L. Convex Optimization. Cam-

bridge University Press, 2004.

Camerini, P. M., Fratta, L., and Mafﬁoli, F. The k best
spanning arborescences of a network. Networks, 10(2):
91–109, 1980.

Chegireddy, C. R. and Hamacher, H. W. Algorithms for
ﬁnding K-best perfect matchings. Discrete Applied Math-
ematics, 18(2):155 – 165, 1987.

Chen, Q., Zhu, X., Ling, Z.-H., Wei, S., Jiang, H., and
Inkpen, D. Enhanced LSTM for natural language infer-
ence. In ACL, 2017.

Chu, Y.-J. and Liu, T.-H. On the shortest arborescence of a
directed graph. Science Sinica, 14:1396–1400, 1965.

Clarke, F. H. Optimization and Nonsmooth Analysis. SIAM,

1990.

Collins, M. Discriminative training methods for Hidden
Markov Models: Theory and experiments with perceptron
algorithms. In EMNLP, 2002.

Domke, J. Learning graphical model parameters with ap-
proximate marginal inference. IEEE T. Pattern. Anal., 35
(10):2454–2467, 2013.

Edmonds, J. Optimum branchings. J. Res. Nat. Bur. Stand.,

71B:233–240, 1967.

Math, 1(73-77), 1949.

Fenchel, W. On conjugate convex functions. Canad. J.

Frank, M. and Wolfe, P. An algorithm for quadratic pro-

gramming. Nav. Res. Log., 3(1-2):95–110, 1956.

Fromer, M. and Globerson, A. An LP view of the M -best

MAP problem. In NIPS, 2009.

Garber, D. and Meshi, O.

Linear-memory and
decomposition-invariant linearly convergent conditional
In NIPS,
gradient algorithm for structured polytopes.
2016.

Gimpel, K. and Smith, N. A. Softmax-margin CRFs: Train-
ing log-linear models with cost functions. In NAACL,
2010.

Jonker, R. and Volgenant, A. A shortest augmenting path al-
gorithm for dense and sparse linear assignment problems.
Computing, 38(4):325–340, 1987.

Jurafsky, D. and Martin, J. H. Speech and Language Pro-

cessing (3rd ed.). draft, 2018.

Keith, K., Blodgett, S. L., and Oâ ˘A ´ZConnor, B. Monte
Carlo syntax marginals for exploring and using depen-
dency parses. In NAACL, 2018.

Kim, Y., Denton, C., Hoang, L., and Rush, A. M. Structured

attention networks. In ICLR, 2017.

Kingma, D. and Ba, J. Adam: A method for stochastic

optimization. In ICLR, 2015.

Kiperwasser, E. and Goldberg, Y. Simple and accurate
dependency parsing using bidirectional LSTM feature
representations. TACL, 4:313–327, 2016.

Kirchhoff, G. Ueber die auﬂösung der gleichungen, auf
welche man bei der untersuchung der linearen vertheilung
galvanischer ströme geführt wird. Annalen der Physik,
148(12):497–508, 1847.

Koo, T., Globerson, A., Carreras Pérez, X., and Collins, M.
Structured prediction models via the matrix-tree theorem.
In EMNLP, 2007.

Krishnan, R. G., Lacoste-Julien, S., and Sontag, D. Barrier
Frank-Wolfe for marginal inference. In NIPS, 2015.

Kschischang, F. R., Frey, B. J., and Loeliger, H.-A. Factor
graphs and the sum-product algorithm. IEEE T. Inform.
Theory, 47(2):498–519, 2001.

SparseMAP: Differentiable Sparse Structured Inference

Kuhn, H. W. The Hungarian method for the assignment

problem. Nav. Res. Log., 2(1-2):83–97, 1955.

selected applications in speech recognition. P. IEEE, 77
(2):257–286, 1989.

Lacoste-Julien, S. and Jaggi, M. On the global linear con-
vergence of Frank-Wolfe optimization variants. In NIPS,
2015.

Lafferty, J. D., McCallum, A., and Pereira, F. C. N. Condi-
tional Random Fields: Probabilistic models for segment-
ing and labeling sequence data. In ICML, 2001.

Li, Z. and Eisner, J. First-and second-order expectation
semirings with applications to minimum-risk training on
translation forests. In EMNLP, 2009.

Liu, Y. and Lapata, M. Learning structured text representa-

tions. TACL, 6:63–75, 2018.

Martins, A. F. and Astudillo, R. F. From softmax to sparse-
max: A sparse model of attention and multi-label classiﬁ-
cation. In ICML, 2016.

Martins, A. F., Smith, N. A., and Xing, E. P. Concise integer
linear programming formulations for dependency parsing.
In ACL-IJCNLP, 2009.

Martins, A. F., Figueiredo, M. A., Aguiar, P. M., Smith,
N. A., and Xing, E. P. AD3: Alternating directions dual
decomposition for MAP inference in graphical models.
JMLR, 16(1):495–545, 2015.

McDonald, R., Crammer, K., and Pereira, F. Online large-
margin training of dependency parsers. In ACL, 2005.

Mena, G., Belanger, D., Linderman, S., and Snoek, J. Learn-
ing latent permutations with Gumbel-Sinkhorn networks.
In ICLR, 2018.

Mensch, A. and Blondel, M. Differentiable dynamic pro-
gramming for structured prediction and attention.
In
ICML, 2018.

Meshi, O., Mahdavi, M., and Schwing, A. Smooth and
strong: MAP inference with linear convergence. In NIPS,
2015.

Neubig, G., Dyer, C., Goldberg, Y., Matthews, A., Am-
mar, W., Anastasopoulos, A., Ballesteros, M., Chiang,
D., Clothiaux, D., Cohn, T., et al. DyNet: The dynamic
neural network toolkit. preprint arXiv:1701.03980, 2017.

Ravikumar, P., Agarwal, A., and Wainwright, M. J. Message-
passing for graph-structured linear programs: Proximal
methods and rounding schemes. JMLR, 11:1043–1080,
2010.

Shalev-Shwartz, S. and Zhang, T. Accelerated proximal
stochastic dual coordinate ascent for regularized loss min-
imization. Math. Program., 155(1):105–145, 2016.

Smith, D. A. and Smith, N. A. Probabilistic models of

nonprojective dependency trees. In EMNLP, 2007.

Smith, N. A. Linguistic Structure Prediction. Synthesis
Lectures on Human Language Technologies. Morgan and
Claypool, May 2011.

Stoyanov, V., Ropson, A., and Eisner, J. Empirical risk
minimization of graphical model parameters given ap-
proximate inference, decoding, and model structure. In
AISTATS, 2011.

Straka, M. and Straková, J. Tokenizing, POS tagging, lem-
matizing and parsing UD 2.0 with UDPipe. In CoNLL
Shared Task, 2017.

Taskar, B. Learning Structured Prediction Models: A Large
Margin Approach. PhD thesis, Stanford University, 2004.

Taskar, B., Guestrin, C., and Koller, D. Max-Margin Markov

Networks. In NIPS, 2003.

Tsochantaridis, I., Hofmann, T., Joachims, T., and Altun, Y.
Support vector machine learning for interdependent and
structured output spaces. In ICML, 2004.

Valiant, L. G. The complexity of computing the permanent.

Theor. Comput. Sci., 8(2):189–201, 1979.

Vinyes, M. and Obozinski, G. Fast column generation for

atomic norm regularization. In AISTATS, 2017.

Wainwright, M. J. and Jordan, M. I. Graphical models,
exponential families, and variational inference. Found.
Trends Mach. Learn., 1(1–2):1–305, 2008.

Williams, A., Nangia, N., and Bowman, S. R. A broad-
coverage challenge corpus for sentence understanding
through inference. In NAACL, 2018.

Niculae, V. and Blondel, M. A regularized framework for
sparse and structured neural attention. In NIPS, 2017.

Wolfe, P. Finding the nearest point in a polytope. Mathe-

matical Programming, 11(1):128–149, 1976.

Nivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y., Hajic,
J., Manning, C. D., McDonald, R. T., Petrov, S., Pyysalo,
S., Silveira, N., et al. Universal Dependencies v1: A
multilingual treebank collection. In LREC, 2016.

Nocedal, J. and Wright, S. Numerical Optimization.

Springer New York, 1999.

Nowozin, S., Gehler, P. V., Jancsary, J., and Lampert, C. H.

Advanced Structured Prediction. MIT Press, 2014.

Rabiner, L. R. A tutorial on Hidden Markov Models and

Yang, B. and Cardie, C. Joint inference for ﬁne-grained

opinion extraction. In ACL, 2013.

Yanover, C. and Weiss, Y. Finding the M most probable
conﬁgurations using loopy belief propagation. In NIPS,
2004.

Zeman, D., Popel, M., Straka, M., Hajic, J., Nivre, J., Ginter,
F., Luotolahti, J., Pyysalo, S., Petrov, S., Potthast, M.,
et al. CoNLL 2017 shared task: Multilingual parsing
from raw text to universal dependencies. CoNLL, 2017.

SparseMAP: Differentiable Sparse Structured Inference

Supplementary material

A. Implementation Details for SparseMAP Solvers

A.1. Conditional Gradient Variants

We adapt the presentation of vanilla, away-step and pairwise conditional gradient of Lacoste-Julien & Jaggi (2015).

Recall the SparseMAP optimization problem (Equation 5), which we rewrite below as a minimization, to align with the
formulation in (Lacoste-Julien & Jaggi, 2015)

SparseMAPA(η) := arg min

f (u, v),

where f (u, v) :=

u: [u,v]∈MA

1
2 (cid:107)

u
(cid:107)

2
2 −

η(cid:62)

U u

η(cid:62)

F v.

−

The gradients of the objective function f w.r.t. the two variables are

uf (u(cid:48), v(cid:48)) = u(cid:48)

ηU ,

vf (u(cid:48), v(cid:48)) =

ηV .

−

∇

−

∇

The ingredients required to apply conditional gradient algorithms are solving linear minimization problem, selecting the
away step, computing the Wolfe gap, and performing line search.

Linear minimization problem. For SparseMAP, this amounts to a MAP inference call, since
(cid:10)

uf (u(cid:48), v(cid:48)), u(cid:11) + (cid:10)

vf (u(cid:48), v(cid:48)), v(cid:11)

arg min
[u,v]∈MA

= arg min
[u,v]∈MA

∇
(u(cid:48)

∇

ηU )(cid:62)u

η(cid:62)

F v

−

−

=

[ms; ns] : s
{

∈

MAPA(ηU

u(cid:48), ηF )
}

.

−

where we assume MAPA yields the set of maximally-scoring structures.

Away step selection. This step involves searching the currently selected structures in the active set
goal: ﬁnding the structure maximizing the linearization

I

with the opposite

uf (u(cid:48), v(cid:48)), ms

vf (u(cid:48), v(cid:48)), ns

(cid:11)

arg max
s∈I
= arg max
s∈I

(cid:10)

∇
(u(cid:48)

ηU )(cid:62)ms

−

−

(cid:11) + (cid:10)

∇
F ns

η(cid:62)

Wolfe gap. The gap at a point d = [du; dv] is given by

gap(d, u(cid:48)) := (cid:10)

uf (u(cid:48), v(cid:48)), du

(cid:11) + (cid:10)
(cid:11) + (cid:10)ηF , dv

−∇
(cid:11).

u(cid:48), du

−∇
= (cid:10)ηU

−

vf (u(cid:48), v(cid:48)), dv

(cid:11)

(7)

Line search. Once we have picked a direction d = [du; dv], we can pick the optimal step size by solving a simple
optimization problem. Let uγ := u(cid:48) + γdu, and vγ := v(cid:48) + γdv. We seek γ so as to optimize

Setting the gradient w.r.t. γ to 0 yields

arg min
γ∈[0,γmax]

f (uγ, vγ)

uf (uγ, vγ)(cid:11) + (cid:10)dv,

0 =

f (uγ, vγ)

∂
∂γ
= (cid:10)du,
∇
= (cid:10)du, u(cid:48) + γdu
du
= γ
(cid:107)

−
2
2 + u(cid:48)(cid:62)du
(cid:107)

ηU

−

∇
(cid:11) + (cid:10)dv,
η(cid:62)d

−

vf (uγ, vγ)(cid:11)
(cid:11)

ηF

SparseMAP: Differentiable Sparse Structured Inference

We may therefore compute the optimal step size γ as

(cid:32)

(cid:32)

γ = max

0, min

γmax,

(cid:33)(cid:33)

η(cid:62)d

−
du

u(cid:48)(cid:62)du
2
2
(cid:107)

(cid:107)

(8)

Algorithm 1 Conditional gradient for SparseMAP

s(0)
1: Initialization:
2: for t = 0 . . . tmax do
3:
4:

MAPA(ηU
arg max
w∈I(t)

←
←

s
w

−
(ηU

←

−

MAPA(ηU , ηF );

(0) =

s(0)
{

}

I

; y(0) = es(0);

[u(0); v(0)] = as(0)

u(t), ηF );

u(t))(cid:62)mw + η(cid:62)

F nw;

dF
dW

←
←

as
−
[u(t); v(t)]

[u(t); v(t)]
aw

−

(forward direction)
(away direction)

d

d

5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22: end for

if gap(dF, u(t)) < (cid:15) then

return u(t)

(Equation 7)

end if
if variant = vanilla then

dF;

←

γmax

1

←

else if variant = pairwise then

dF + dW;

γmax

yw

←
else if variant = away-step then

←

if gap(dF, u(t))

gap(dW, u(t)) then

dF;

dA;

←

d
else
d
←
end if

≥
γmax

1

←

γmax

yw/(1

yw)

←

−

end if
Compute step size γ
[u(t+1); v(t+1)]
Update

←

I

(Equation 8)
[u(t); v(t)] + d

(t+1) and y(t+1) accordingly.

A.2. The Active Set Algorithm

We use a variant of the active set algorithm (Nocedal & Wright, 1999, Ch. 16.4 & 16.5) as proposed for the quadratic
subproblems of the AD3 algorithm; our presentation follows (Martins et al., 2015, Algorithm 3). At each step, the active set
algorithm solves a relaxed variant of the SparseMAP QP, relaxing the non-negativity constraint on y, and restricting the
solution to the current active set

I

minimizeyI ∈R|I|

1
2 (cid:107)

MIyI

2
2 −

(cid:107)

η(cid:62)AIyI

subject to 1(cid:62)yI = 1

whose solution can be found by solving the KKT system

(cid:20)M (cid:62)
I MI 1
1(cid:62)
0

(cid:21) (cid:20)yI
τ

(cid:21)

(cid:21)

(cid:20)A(cid:62)
I η
1

.

=

At each iteration, the (symmetric) design matrix in Equation 9 is updated by adding or removing a row and a column;
therefore its inverse (or a decomposition) may be efﬁciently maintained and updated.

Line search. The optimal step size for moving a feasible current estimate y(cid:48) toward a solution ˆy of Equation 9, while
keeping feasibility, is given by (Martins et al., 2015, Equation 31)

When γ

1 this update zeros out a coordinate of y(cid:48); otherwise,

remains the same.

≤

I

(cid:18)

γ = min

1, min
s∈I, y(cid:48)

s>ˆys

(cid:19)

y(cid:48)
s

y(cid:48)
s −

ˆys

(9)

(10)

SparseMAP: Differentiable Sparse Structured Inference

Algorithm 2 Active Set algorithm for SparseMAP

MAPA(ηU , ηF );

←

(0) =

s(0)
{

}

I

; y(0) = es(0);

[u(0); v(0)] = as(0)

Solve the relaxed QP restricted to
if ˆy = y(t) then

I

(t); get ˆy, ˆτ , ˆu = M ˆy

(Equation 9)

≤

MAPA(ηU

s
←
if gap(as, ˆu)
return u(t)

s(0)
1: Initialization:
2: for t = 0 . . . tmax do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
end if
15:
16: end for

I
end if

←
S

← I

else

else

(t+1)

ˆu, ηF )

−
ˆτ then
(Equation 7)

(t)

s
}

∪ {

Compute step size γ
y(t+1)
Update

(1
(t+1) if necessary

−

(Equation 10)
γ)y(t) + γ ˆy (sparse update)

B. Computing the SparseMAP Jacobian: Proof of Proposition 1

Recall that SparseMAP is deﬁned as the u(cid:63) that maximizes the value of the quadratic program (Equation 5),

g(ηU , ηF ) := max

U u + η(cid:62)
η(cid:62)
F v

[u;v]∈MA

1
2 (cid:107)

u
(cid:107)

2
2 .

−

2 norm is strongly convex, there is always a unique minimizer u(cid:63) (implying that SparseMAP is well-deﬁned), and
otherwise(cid:9) is smooth in u, implying that

As the (cid:96)2
the convex conjugate of the QP in (11), g∗(u, v) = (cid:8) 1
SparseMAP (which only returns u) is Lipschitz-continuous and thus differentiable almost everywhere.

2
2 , [u; v]
(cid:107)

∈ M

−∞

2 (cid:107)

A;

u

We now rewrite the QP in Equation 11 in terms of the convex combination of vertices of the marginal polytope

min
y∈(cid:52)D

1
2 (cid:107)

M y

2
2 −
(cid:107)

θ(cid:62)y

where θ := A(cid:62)η

We use the optimality conditions of problem 12 to derive an explicit relationship between u(cid:63) and x. At an optimum, the
following KKT conditions hold

M (cid:62)M y(cid:63)

−

λ(cid:63) + τ (cid:63)1 = θ
1(cid:62)y(cid:63) = 1
y(cid:63)
0
λ(cid:63)
0
λ(cid:63)(cid:62)y(cid:63) = 0

≥

≥

MI

(cid:62)MIy(cid:63)

I + τ (cid:63)1 = θI
I = 1

1(cid:62)y(cid:63)

Let

denote the support of y(cid:63), i.e.,

=

s : y(cid:63)

I

{

. From Equation 17 we have λI = 0 and therefore
s > 0
}

I

Solving for y(cid:63)

I in Equation 18 we get a direct expression

yI

(cid:63) = (MI

(cid:62)MI)−1(θI

τ (cid:63)1) = Z(θI

τ (cid:63)1).

−

−

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

SparseMAP: Differentiable Sparse Structured Inference

where we introduced Z = (M (cid:62)M )−1. Solving for τ (cid:63) yields

τ (cid:63) =

1
1T Z1

(cid:0)1T ZθI

1(cid:1)

−

Plugging this back and left-multiplying by MI we get
(cid:18)

u(cid:63) = MIy(cid:63)

I = MIZ

θI

1
1(cid:62)Z1

−

1(cid:62)ZθI1 +

(cid:19)

1
1(cid:62)Z1

1

Note that, in a neighborhood of η, the support of the solution
is constant. (On the measure-zero set of points where
the support changes, SparseMAP is subdifferentiable and our assumption yields a generalized Jacobian (Clarke, 1990).)
Differentiating w.r.t. the score of a conﬁguration θs, we get the expression
1T Z1 Z11T (cid:1) zs

(cid:40)

I

1

∂u(cid:63)
∂θs

=

M (cid:0)I
0

−

s
s /

∈ I
∈ I

Since θs = a(cid:62)

s η, by the chain rule, we get the desired result

(20)

(21)

∂u(cid:63)
∂η

=

∂u(cid:63)
∂θ

A(cid:62).

C. Fenchel-Young Losses: Proof of Proposition 2

We recall that the structured Fenchel-Young loss deﬁned by a convex Ω : RD

R and a matrix A is deﬁned as

(cid:96)Ω,A : Rk

D

R,

(cid:96)Ω,A(η, y) := Ω∗

(cid:52)(A(cid:62)η) + Ω(cid:52)(y)

η(cid:62)Ay.

× (cid:52)

→

→

−

Since Ω(cid:52) is the restriction of a convex function to a convex set, it is convex (Boyd & Vandenberghe, 2004, Section 3.1.2).

Property 1. From the Fenchel-Young inequality (Fenchel, 1949; Boyd & Vandenberghe, 2004, Section 3.3.2), we have

In particular, when θ = A(cid:62)η,

Equality is achieved when

where we used the fact that y

∈ (cid:52)

θ(cid:62)y

Ω∗

(cid:52)(θ) + Ω(cid:52)(y).

≤

0

η(cid:62)Ay + Ω∗

(cid:52)(A(cid:62)η) + Ω(cid:52)(y)

≤ −
= (cid:96)Ω,A(η, y).

Ω∗
η(cid:62)Ay(cid:48)

(cid:52)(A(cid:62)η) = η(cid:62)Ay
Ω(y(cid:48)) = η(cid:62)Ay

max
y(cid:48)∈(cid:52)d
d. The second part of the claim follows.

−

−

−

Ω(cid:52)(y)

Ω(y),

⇐⇒

Property 2. To prove convexity in η, we rewrite the loss, for ﬁxed y, as

(cid:96)Ω,A(η) = h(A(cid:62)η) + const, where h(θ) = Ω∗

(cid:52)(θ)

θ(cid:62)y.

−

Ω∗
(cid:52) is a convex conjugate, and thus itself convex. Linear functions are convex, and the sum of two convex functions is
convex, therefore h is convex. Finally, the composition of a convex function with a linear function is convex as well, thus
the function (cid:0)hA(cid:62)(cid:1) is convex. Convexity of (cid:96)Ω,A in η directly follows. Convexity in y is straightforward, as the sum of a
convex and a linear function (Boyd & Vandenberghe, 2004, Sections 3.2.1, 3.2.2, 3.3.1).

Property 3. This follows from the scaling property of the convex conjugate (Boyd & Vandenberghe, 2004, Section 3.3.2)
(tΩ)∗(θ) = tΩ∗(t−1θ)

Denoting η(cid:48) = t−1η, we have that

(cid:96)tΩ,A(η, y) = (tΩ(cid:52))∗(A(cid:62)η) + tΩ(cid:52)(y)

η(cid:62)Ay

= tΩ∗
= t(cid:0)Ω∗

(cid:52)(A(cid:62)η(cid:48)) + tΩ(cid:52)(y)
(cid:52)(A(cid:62)η(cid:48)) + Ω(cid:52)(y)

−

−

−
η(cid:62)Ay
η(cid:48)(cid:62)Ay(cid:1) = t(cid:96)Ω,A(t−1η, y).

