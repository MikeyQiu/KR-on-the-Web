Recurrent Neural Network for Text Classiﬁcation
with Multi-Task Learning

Pengfei Liu Xipeng Qiu∗ Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
{pﬂiu14,xpqiu,xjhuang}@fudan.edu.cn

6
1
0
2
 
y
a
M
 
7
1
 
 
]
L
C
.
s
c
[
 
 
1
v
1
0
1
5
0
.
5
0
6
1
:
v
i
X
r
a

Abstract

Neural network based methods have obtained great
progress on a variety of natural language process-
ing tasks. However, in most previous works, the
models are learned based on single-task super-
vised objectives, which often suffer from insufﬁ-
cient training data. In this paper, we use the multi-
task learning framework to jointly learn across mul-
tiple related tasks. Based on recurrent neural net-
work, we propose three different mechanisms of
sharing information to model text with task-speciﬁc
and shared layers. The entire network is trained
jointly on all these tasks. Experiments on four
benchmark text classiﬁcation tasks show that our
proposed models can improve the performance of a
task with the help of other related tasks.

1 Introduction

Distributed representations of words have been widely used
in many natural language processing (NLP) tasks. Follow-
ing this success, it is rising a substantial interest to learn
the distributed representations of the continuous words, such
as phrases, sentences, paragraphs and documents [Socher et
al., 2013; Le and Mikolov, 2014; Kalchbrenner et al., 2014;
Liu et al., 2015a]. The primary role of these models is to rep-
resent the variable-length sentence or document as a ﬁxed-
length vector. A good representation of the variable-length
text should fully capture the semantics of natural language.

The deep neural networks (DNN) based methods usually
need a large-scale corpus due to the large number of parame-
ters, it is hard to train a network that generalizes well with
limited data. However, the costs are extremely expensive
to build the large scale resources for some NLP tasks. To
deal with this problem, these models often involve an un-
supervised pre-training phase. The ﬁnal model is ﬁne-tuned
with respect to a supervised training criterion with a gradient
based optimization. Recent studies have demonstrated signif-
icant accuracy gains in several NLP tasks [Collobert et al.,
2011] with the help of the word representations learned from
the large unannotated corpora. Most pre-training methods

∗Corresponding author.

are based on unsupervised objectives such as word predic-
tion for training [Collobert et al., 2011; Turian et al., 2010;
Mikolov et al., 2013]. This unsupervised pre-training is effec-
tive to improve the ﬁnal performance, but it does not directly
optimize the desired task.

Multi-task learning utilizes the correlation between related
tasks to improve classiﬁcation by learning tasks in parallel.
Motivated by the success of multi-task learning [Caruana,
1997], there are several neural network based NLP models
[Collobert and Weston, 2008; Liu et al., 2015b] utilize multi-
task learning to jointly learn several tasks with the aim of
mutual beneﬁt. The basic multi-task architectures of these
models are to share some lower layers to determine common
features. After the shared layers, the remaining layers are
split into the multiple speciﬁc tasks.

In this paper, we propose three different models of sharing
information with recurrent neural network (RNN). All the re-
lated tasks are integrated into a single system which is trained
jointly. The ﬁrst model uses just one shared layer for all the
tasks. The second model uses different layers for different
tasks, but each layer can read information from other layers.
The third model not only assigns one speciﬁc layer for each
task, but also builds a shared layer for all the tasks. Besides,
we introduce a gating mechanism to enable the model to se-
lectively utilize the shared information. The entire network is
trained jointly on all these tasks.

Experimental results on four text classiﬁcation tasks show
that the joint learning of multiple related tasks together can
improve the performance of each task relative to learning
them separately.

Our contributions are of two-folds:

• First, we propose three multi-task architectures for
RNN. Although the idea of multi-task learning is not
new, our work is novel to integrate RNN into the multi-
learning framework, which learns to map arbitrary text
into semantic vector representations with both task-
speciﬁc and shared layers.

• Second, we demonstrate strong results on several text
classiﬁcation tasks. Our multi-task models outperform
most of state-of-the-art baselines.

2 Recurrent Neural Network for

Speciﬁc-Task Text Classiﬁcation

The primary role of the neural models is to represent the
variable-length text as a ﬁxed-length vector. These models
generally consist of a projection layer that maps words, sub-
word units or n-grams to vector representations (often trained
beforehand with unsupervised methods), and then combine
them with the different architectures of neural networks.

There are several kinds of models to model text, such as
Neural Bag-of-Words (NBOW) model, recurrent neural net-
work (RNN) [Chung et al., 2014], recursive neural network
(RecNN) [Socher et al., 2012; Socher et al., 2013] and con-
volutional neural network (CNN) [Collobert et al., 2011;
Kalchbrenner et al., 2014]. These models take as input the
embeddings of words in the text sequence, and summarize its
meaning with a ﬁxed length vectorial representation.

Among them, recurrent neural networks (RNN) are one
of the most popular architectures used in NLP problems be-
cause their recurrent structure is very suitable to process the
variable-length text.

2.1 Recurrent Neural Network
A recurrent neural network (RNN) [Elman, 1990] is able to
process a sequence of arbitrary length by recursively applying
a transition function to its internal hidden state vector ht of
the input sequence. The activation of the hidden state ht at
time-step t is computed as a function f of the current input
symbol xt and the previous hidden state ht−1
(cid:26)0

t = 0

ht =

f (ht−1, xt) otherwise

(1)

It is common to use the state-to-state transition function f
as the composition of an element-wise nonlinearity with an
afﬁne transformation of both xt and ht−1.

Traditionally, a simple strategy for modeling sequence is
to map the input sequence to a ﬁxed-sized vector using one
RNN, and then to feed the vector to a softmax layer for clas-
siﬁcation or other tasks [Cho et al., 2014].

Unfortunately, a problem with RNNs with transition func-
tions of this form is that during training, components of the
gradient vector can grow or decay exponentially over long
sequences [Hochreiter et al., 2001; Hochreiter and Schmid-
huber, 1997]. This problem with exploding or vanishing gra-
dients makes it difﬁcult for the RNN model to learn long-
distance correlations in a sequence.

Long short-term memory network (LSTM) was proposed
by [Hochreiter and Schmidhuber, 1997] to speciﬁcally ad-
dress this issue of learning long-term dependencies. The
LSTM maintains a separate memory cell inside it that up-
dates and exposes its content only when deemed necessary.
A number of minor modiﬁcations to the standard LSTM unit
have been made. While there are numerous LSTM variants,
here we describe the implementation used by Graves [2013].
We deﬁne the LSTM units at each time step t to be a col-
lection of vectors in Rd: an input gate it, a forget gate ft, an
output gate ot, a memory cell ct and a hidden state ht. d is
the number of the LSTM units. The entries of the gating vec-
tors it, ft and ot are in [0, 1]. The LSTM transition equations

Figure 1: Recurrent Neural Network for Classiﬁcation

are the following:

(2)
(3)
(4)
(5)

it = σ(Wixt + Uiht−1 + Vict−1),
ft = σ(Wf xt + Uf ht−1 + Vf ct−1),
ot = σ(Woxt + Uoht−1 + Voct),
˜ct = tanh(Wcxt + Ucht−1),
ct = f i
t (cid:12) ct−1 + it (cid:12) ˜ct,
ht = ot (cid:12) tanh(ct),

(6)
(7)
where xt is the input at the current time step, σ denotes the
logistic sigmoid function and (cid:12) denotes elementwise multi-
plication. Intuitively, the forget gate controls the amount of
which each unit of the memory cell is erased, the input gate
controls how much each unit is updated, and the output gate
controls the exposure of the internal memory state.

2.2 Task-Speciﬁc Output Layer
In a single speciﬁc task, a simple strategy is to map the input
sequence to a ﬁxed-sized vector using one RNN, and then to
feed the vector to a softmax layer for classiﬁcation or other
tasks.

Given a text sequence x = {x1, x2, · · · , xT }, we ﬁrst use a
lookup layer to get the vector representation (embeddings) xi
of the each word xi. The output at the last moment hT can be
regarded as the representation of the whole sequence, which
has a fully connected layer followed by a softmax non-linear
layer that predicts the probability distribution over classes.

Figure 1 shows the unfolded RNN structure for text classi-

ﬁcation.

The parameters of the network are trained to minimise the

cross-entropy of the predicted and true distributions.

L(ˆy, y) = −

yj
i log(ˆyj

i ),

(8)

N
(cid:88)

C
(cid:88)

i=1

j=1

i is the ground-truth label; ˆyj

where yj
i is prediction probabil-
ities; N denotes the number of training samples and C is the
class number.

3 Three Sharing Models for RNN based

Multi-Task Learning

Most existing neural network methods are based on super-
vised training objectives on a single task [Collobert et al.,
2011; Socher et al., 2013; Kalchbrenner et al., 2014]. These
methods often suffer from the limited amounts of training
data. To deal with this problem, these models often involve
an unsupervised pre-training phase. This unsupervised pre-
training is effective to improve the ﬁnal performance, but it
does not directly optimize the desired task.

(a) Model-I: Uniform-Layer Architecture

(b) Model-II: Coupled-Layer Architecture

Model-II: Coupled-Layer Architecture
In Model-II, we
assign a LSTM layer for each task, which can use the infor-
mation for the LSTM layer of the other task.

Given a pair of tasks (m, n), each task has own LSTM in
the task-speciﬁc model. We denote the outputs at step t of
two coupled LSTM layer are h(m)

t
To better control signals ﬂowing from one task to another
task, we use a global gating unit which endows the model
with the capability of deciding how much information it
should accept. We re-deﬁne Eqs. (5) and the new memory
content of an LSTM at m-th task is computed by:

and h(n)

.

t

˜ct

(m) = tanh


W(m)

c xt +



g(i→m)U (i→m)

c

h(i)
t−1



(cid:88)

i∈{m,n}

where g(i→m) = σ(W(m)
are same to the standard LSTM.

g xt +U(i)

g h(i)

(11)
t−1). The other settings

This model can be used to jointly learning for every two
and

tasks. We can get two task speciﬁc representations h(m)
h(n)
T

for tasks m and n receptively.

T

Model-III: Shared-Layer Architecture Model-III also as-
signs a separate LSTM layer for each task, but introduces a
bidirectional LSTM layer to capture the shared information
for all the tasks.

and
−→
h (s)

−→
h (s)
t
t =

LSTMs at step t as
of shared layer is h(s)

We denote the outputs of the forward and backward
←−
h (s)
respectively. The output
t
←−
h (s)
t

t ⊕
To enhance the interaction between task-speciﬁc layers and
the shared layer, we use gating mechanism to endow the neu-
rons in task-speciﬁc layer with the ability to accept or refuse
the information passed by the neuron in shared layers. Unlike
Model-II, we compute the new state for LSTM as follows:

.

˜c(m)
t = tanh

W(m)

c xt + g(m)U(m)

c h(m)

(cid:16)

g xt + U(m)
where g(m) = σ(W(m)
h(s)
g xt + U(s→m)
σ(W(m)

t ).

g

(cid:17)

c h(s)

t−1 + g(s→m)U(s)

,
(12)
t−1) and g(s→m) =

g h(m)

t

4 Training
The task-speciﬁc representations, which emittd by the muti-
task architectures of all of the above, are ultimately fed into
different output layers, which are also task-speciﬁc.

ˆy(m) = softmax(W(m)h(m) + b(m)),

(13)

(c) Model-III: Shared-Layer Architecture

Figure 2: Three architectures for modelling text with multi-
task learning.

Motivated by the success of multi-task learning [Caruana,
1997], we propose three multi-task models to leverage super-
vised data from many related tasks. Deep neural model is
well suited for multi-task learning since the features learned
from a task may be useful for other tasks. Figure 2 gives an
illustration of our proposed models.

Model-I: Uniform-Layer Architecture
In Model-I, the
different tasks share a same LSTM layer and an embedding
layer besides their own embedding layers.

For task m, the input ˆxt consists of two parts:

t = x(m)
ˆx(m)

t ⊕ x(s)

t

,

t

, x(s)
t

where x(m)
denote the task-speciﬁc and shared word
embeddings respectively, ⊕ denotes the concatenation opera-
tion.

The LSTM layer is shared for all tasks. The ﬁnal sequence

representation for task m is the output of LSMT at step T .

(9)

where ˆy(m) is prediction probabilities for task m, W(m) is
the weight which needs to be learned, and b(m) is a bias term.
Our global cost function is the linear combination of cost

function for all joints.

φ =

λmL(ˆy(m), y(m))

(14)

M
(cid:88)

m=1

h(m)
T = LST M (ˆx(m)).

(10)

where λm is the weights for each task m respectively.

Type
Dataset
Sentence
SST-1
Sentence
SST-2
SUBJ
Sentence
IMDB Document

Train Size Dev. Size Test Size Class Averaged Length Vocabulary Size
5
2
2
2

8544
6920
9000
25,000

2210
1821
1000
25,000

18K
15K
21K
392K

1101
872
-
-

19
18
21
294

Table 1: Statistics of the four datasets used in this paper.

It is worth noticing that labeled data for training each
task can come from completely different datasets. Follow-
ing [Collobert and Weston, 2008], the training is achieved in
a stochastic manner by looping over the tasks:

1. Select a random task.
2. Select a random training example from this task.
3. Update the parameters for this task by taking a gradient

step with respect to this example.

4. Go to 1.

Fine Tuning For model-I and model-III, there is a shared
layer for all the tasks. Thus, after the joint learning phase, we
can use a ﬁne tuning strategy to further optimize the perfor-
mance for each task.

Pre-training of the shared layer with neural language
model For model-III, the shared layer can be initialized
by an unsupervised pre-training phase. Here, for the shared
LSTM layer in Model-III, we initialize it by a language model
[Bengio et al., 2007], which is trained on all the four task
dataset.

5 Experiment
In this section, we investigate the empirical performances of
our proposed three models on four related text classiﬁcation
tasks and then compare it to other state-of-the-art models.

5.1 Datasets
To show the effectiveness of multi-task learning, we choose
four different text classiﬁcation tasks about movie review.
Each task have own dataset, which is brieﬂy described as fol-
lows.

• SST-1 The movie reviews with ﬁve classes (negative,
somewhat negative, neutral, somewhat positive, posi-
tive) in the Stanford Sentiment Treebank1 [Socher et al.,
2013].

• SST-2 The movie reviews with binary classes. It is also

from the Stanford Sentiment Treebank.

• SUBJ Subjectivity data set where the goal is to classify
each instance (snippet) as being subjective or objective.
[Pang and Lee, 2004]

• IMDB The IMDB dataset2 consists of 100,000 movie
reviews with binary classes [Maas et al., 2011]. One
key aspect of this dataset is that each movie review has
several sentences.

1http://nlp.stanford.edu/sentiment.
2http://ai.stanford.edu/˜amaas/data/

sentiment/

The ﬁrst three datasets are sentence-level, and the last
dataset is document-level. The detailed statistics about the
four datasets are listed in Table 1.

5.2 Hyperparameters and Training

The network is trained with backpropagation and the
gradient-based optimization is performed using the Adagrad
update rule [Duchi et al., 2011]. In all of our experiments,
the word embeddings are trained using word2vec [Mikolov
et al., 2013] on the Wikipedia corpus (1B words). The vo-
cabulary size is about 500,000. The word embeddings are
ﬁne-tuned during training to improve the performance [Col-
lobert et al., 2011]. The other parameters are initialized by
randomly sampling from uniform distribution in [-0.1, 0.1].
The hyperparameters which achieve the best performance on
the development set will be chosen for the ﬁnal evaluation.
For datasets without development set, we use 10-fold cross-
validation (CV) instead.

The ﬁnal hyper-parameters are as follows. The embedding
size for speciﬁc task and shared layer are 64. For Model-I,
there are two embeddings for each word, and both their sizes
are 64. The hidden layer size of LSTM is 50. The initial
learning rate is 0.1. The regularization weight of the parame-
ters is 10−5.

5.3 Effect of Multi-task Training

Model
Single Task
Joint Learning
+ Fine Tuning

SST-1
45.9
46.5
48.5

SST-2
85.8
86.7
87.1

SUBJ
91.6
92.0
93.4

IMDB Avg∆
88.5
89.9
90.8

-
+0.8
+2.0

Table 2: Results of the uniform-layer architecture.

Model
Single Task
SST1-SST2
SST1-SUBJ
SST1-IMDB
SST2-SUBJ
SST2-IMDB
SUBJ-IMDB

SST-1
45.9
48.9
46.3
46.9
-
-
-

SST-2
85.8
87.4
-
-
86.5
86.8
-

SUBJ
91.6
-
92.2
-
92.5
-
92.7

IMDB Avg∆
88.5
-
-
89.5
-
89.8
89.3

-
+2.3
+0.5
+1.0
+0.8
+1.2
+0.9

Table 3: Results of the coupled-layer architecture.

We ﬁrst compare our our proposed models with the stan-
dard LSTM for single task classiﬁcation. We use the im-
plementation of Graves [2013]. The unfolded illustration is
shown in Figure 1.

Model
Single Task
Joint Learning
+ LM
+ Fine Tuning

SST-1
45.9
47.1
47.9
49.6

SST-2
85.8
87.0
86.8
87.9

SUBJ
91.6
92.5
93.6
94.1

IMDB Avg∆
88.5
90.7
91.0
91.3

-
+1.4
+1.9
+2.8

Table 4: Results of the shared-layer architecture.

Model
SST-1
42.4
NBOW
44.4
MV-RNN
45.7
RNTN
48.5
DCNN
PV
44.6
Tree-LSTM 50.6
49.6
Multi-Task

SST-2
80.5
82.9
85.4
86.8
82.7
86.9
87.9

SUBJ
91.3
-
-
-
90.5
-
94.1

IMDB
83.62
-
-
-
91.7
-
91.3

Table 2-4 show the classiﬁcation accuracies on the four
datasets. The second line (“Single Task”) of each table shows
the result of the standard LSTM for each individual task.

Table 5: Results of shared-layer multi-task model against
state-of-the-art neural models.

Uniform-layer Architecture For the ﬁrst uniform-layer ar-
chitecture, we train the model on four datasets simultane-
ously. The LSTM layer is shared across all the tasks. The
average improvement of the performances on four datasets is
0.8%. With the further ﬁne-tuning phase, the improvement
achieves 2.0% on average.

Coupled-layer Architecture For the second coupled-layer
architecture, the information is shared with a pair of tasks.
Therefore, there are six combinations for the four datasets.
We train six models on the different pairs of datasets. We can
ﬁnd that the pair-wise joint learning also improves the perfor-
mances. The more relevant the tasks are, the more signiﬁcant
the improvements are. Since SST-1 and SST-2 are from the
same corpus, their improvements are more signiﬁcant than
the other combinations. The improvement is 2.3% on aver-
age with simultaneously learning on SST-1 and SST-2.

Shared-layer Architecture The shared-layer architecture
is more general than uniform-layer architecture. Besides a
shared layer for all the tasks, each task has own task-speciﬁc
layer. As shown in Table 4, we can see that the average
improvement of the performances on four datasets is 1.4%,
which is better than the uniform-layer architecture. We also
investigate the strategy of unsupervised pre-training towards
shared LSTM layer. With the LM pre-training, the perfor-
mance is improved by an extra 0.5% on average. Besides, the
further ﬁne-tuning can signiﬁcantly improve the performance
by an another 0.9%.

To recap, all our proposed models outperform the baseline
of single-task learning. The shared-layer architecture gives
the best performances. Moreover, compared with vanilla
LSTM, our proposed three models don’t cause much extra
computational cost while converge faster. In our experiment,
the most complicated model-III, costs 2.5 times as long as
vanilla LSTM.

5.4 Comparisons with State-of-the-art Neural

Models

We compare our model with the following models:

• NBOW The NBOW sums the word vectors and applies a
non-linearity followed by a softmax classiﬁcation layer.

• MV-RNN Matrix-Vector Recursive Neural Network

with parse trees [Socher et al., 2012].

• RNTN Recursive Neural Tensor Network with tensor-
based feature function and parse trees [Socher et al.,
2013].

• DCNN Dynamic Convolutional Neural Network with
dynamic k-max pooling [Kalchbrenner et al., 2014].
• PV Logistic regression on top of paragraph vectors [Le
and Mikolov, 2014]. Here, we use the popular open
source implementation of PV in Gensim3.

• Tree-LSTM A generalization of LSTMs to tree-

structured network topologies. [Tai et al., 2015]

Table 5 shows the performance of the shared-layer archi-
tecture compared with the competitor models, which shows
our model is competitive for the neural-based state-of-the-art
models.

Although Tree-LSTM outperforms our model on SST-1, it
needs an external parser to get the sentence topological struc-
ture. It is worth noticing that our models are compatible with
the other RNN based models. For example, we can easily
extend our models to incorporate the Tree-LSTM model.

5.5 Case Study
To get an intuitive understanding of what is happening when
we use the single LSTM or the shared-layer LSTM to pre-
dict the class of text, we design an experiment to analyze
the output of the single LSTM and the shared-layer LSTM
at each time step. We sample two sentences from the SST-2
test dataset, and the changes of the predicted sentiment score
at different time steps are shown in Figure 3. To get more
insights into how the shared structures inﬂuences the speciﬁc
task. We observe the activation of global gates g(s), which
controls signals ﬂowing from one shared LSTM layer to task-
spciﬁc layer, to understand the behaviour of neurons. We plot
evolving activation of global gates g(s) through time and sort
the neurons according to their activations at the last time step.
For the sentence “A merry movie about merry
period people’s life.”, which has a positive sen-
timent, while the standard LSTM gives a wrong prediction.
The reason can be inferred from the activation of global gates
g(s). As shown in Figure 3-(c), we can see clearly the neurons
are activated much when they take input as “merry”, which
indicates the task-speciﬁc layer takes much information from
shared layer towards the word “merry”, and this ultimately
makes the model give a correct prediction.

3https://github.com/piskvorky/gensim/

(a)

(c)

(b)

(d)

Figure 3: (a)(b) The change of the predicted sentiment score at different time steps. Y-axis represents the sentiment score,
while X-axis represents the input words in chronological order. The red horizontal line gives a border between the positive and
negative sentiments. (c)(d) Visualization of the global gate’s (g(s)) activation.

Another case “Not everything works, but the

average is higher than in Mary and most
other recent comedies.” is positive and has a little
complicated semantic composition. As shown in Figure
3-(b,d), simple LSTM cannot capture the structure of “but
...
higher than ” while our model is sensitive to
it, which indicates the shared layer can not only enrich the
meaning of certain words, but can teach some information of
structure to speciﬁc task.

5.6 Error Analysis
We analyze the bad cases induced by our proposed shared-
layer model on SST-2 dataset. Most of the bad cases can be
generalized into two categories

Complicated Sentence Structure Some sentences in-
volved complicated structure can not be handled prop-
erly, such as double negation “it never fails to
engage us.”
and subjunctive sentences “Still, I
thought it could have been more.”. To solve
these cases, some architectural improvements are necessary,
such as tree-based LSTM [Tai et al., 2015].

Sentences Required Reasoning The sentiments of some
sentences can be mislead if only considering the literal mean-
ing. For example, the sentence “I tried to read the
time on my watch.”
expresses negative attitude to-
wards a movie, which can be understood correctly by rea-
soning based on common sense.

6 Related Work
Neural networks based multi-task learning has proven effec-
tive in many NLP problems [Collobert and Weston, 2008;
Liu et al., 2015b].

Collobert and Weston [2008] used a shared representa-
tion for input words and solve different traditional NLP tasks
such as part-of-Speech tagging and semantic role labeling

within one framework. However, only one lookup table is
shared, and the other lookup-tables and layers are task spe-
ciﬁc. To deal with the variable-length text sequence, they
used window-based method to ﬁx the input size.

Liu et al. [2015b] developed a multi-task DNN for learning
representations across multiple tasks. Their multi-task DNN
approach combines tasks of query classiﬁcation and ranking
for web search. But the input of the model is bag-of-word
representation, which lose the information of word order.

Different with the two above methods, our models are
based on recurrent neural network, which is better to model
the variable-length text sequence.

More recently, several multi-task encoder-decoder net-
works were also proposed for neural machine translation
[Dong et al., 2015; Firat et al., 2016], which can make use
of cross-lingual information. Unlike these works, in this pa-
per we design three architectures, which can control the in-
formation ﬂow between shared layer and task-speciﬁc layer
ﬂexibly, thus obtaining better sentence representations.

7 Conclusion and Future Work

In this paper, we introduce three RNN based architectures
to model text sequence with multi-task learning. The differ-
ences among them are the mechanisms of sharing information
among the several tasks. Experimental results show that our
models can improve the performances of a group of related
tasks by exploring common features.

In future work, we would like to investigate the other shar-

ing mechanisms of the different tasks.

Acknowledgments

We would like to thank the anonymous reviewers for their
valuable comments. This work was partially funded by Na-
tional Natural Science Foundation of China (No. 61532011,

61473092, and 61472088),
the National High Technol-
ogy Research and Development Program of China (No.
2015AA015408).

References
[Bengio et al., 2007] Yoshua Bengio, Pascal Lamblin, Dan
Popovici, Hugo Larochelle, et al. Greedy layer-wise train-
ing of deep networks. Advances in neural information pro-
cessing systems, 19:153, 2007.

[Caruana, 1997] Rich Caruana. Multitask learning. Machine

learning, 28(1):41–75, 1997.

[Cho et al., 2014] Kyunghyun Cho, Bart van Merrienboer,
Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using
rnn encoder-decoder for statistical machine translation. In
Proceedings of EMNLP, 2014.

[Chung et al., 2014] Junyoung Chung, Caglar Gulcehre,
KyungHyun Cho, and Yoshua Bengio. Empirical evalua-
tion of gated recurrent neural networks on sequence mod-
eling. arXiv preprint arXiv:1412.3555, 2014.

[Collobert and Weston, 2008] Ronan Collobert and Jason
Weston. A uniﬁed architecture for natural language pro-
cessing: Deep neural networks with multitask learning. In
Proceedings of ICML, 2008.

[Collobert et al., 2011] Ronan Collobert,

Jason Weston,
L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and
Pavel Kuksa. Natural language processing (almost) from
The Journal of Machine Learning Research,
scratch.
12:2493–2537, 2011.

[Dong et al., 2015] Daxiang Dong, Hua Wu, Wei He, Dian-
hai Yu, and Haifeng Wang. Multi-task learning for multi-
ple language translation. In Proceedings of the ACL, 2015.
[Duchi et al., 2011] John Duchi, Elad Hazan, and Yoram
Singer. Adaptive subgradient methods for online learn-
ing and stochastic optimization. The Journal of Machine
Learning Research, 12:2121–2159, 2011.

[Elman, 1990] Jeffrey L Elman. Finding structure in time.

Cognitive science, 14(2):179–211, 1990.

[Firat et al., 2016] Orhan Firat, Kyunghyun Cho,

and
Yoshua Bengio. Multi-way, multilingual neural machine
arXiv
translation with a shared attention mechanism.
preprint arXiv:1601.01073, 2016.

[Graves, 2013] Alex Graves. Generating sequences with re-
current neural networks. arXiv preprint arXiv:1308.0850,
2013.

[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and
J¨urgen Schmidhuber. Long short-term memory. Neural
computation, 9(8):1735–1780, 1997.

[Hochreiter et al., 2001] Sepp Hochreiter, Yoshua Bengio,
Paolo Frasconi, and J¨urgen Schmidhuber. Gradient ﬂow
in recurrent nets: the difﬁculty of learning long-term de-
pendencies, 2001.

[Kalchbrenner et al., 2014] Nal Kalchbrenner,

Edward
Grefenstette, and Phil Blunsom. A convolutional neural

network for modelling sentences. In Proceedings of ACL,
2014.

[Le and Mikolov, 2014] Quoc V. Le and Tomas Mikolov.
Distributed representations of sentences and documents.
In Proceedings of ICML, 2014.

[Liu et al., 2015a] PengFei Liu, Xipeng Qiu, Xinchi Chen,
Shiyu Wu, and Xuanjing Huang. Multi-timescale long
short-term memory neural network for modelling sen-
tences and documents. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing,
2015.

[Liu et al., 2015b] Xiaodong Liu, Jianfeng Gao, Xiaodong
He, Li Deng, Kevin Duh, and Ye-Yi Wang. Representa-
tion learning using multi-task deep neural networks for se-
mantic classiﬁcation and information retrieval. In NAACL,
2015.

[Maas et al., 2011] Andrew L Maas, Raymond E Daly, Pe-
ter T Pham, Dan Huang, Andrew Y Ng, and Christopher
Potts. Learning word vectors for sentiment analysis.
In
Proceedings of the ACL, pages 142–150, 2011.

[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg
Efﬁcient estimation of
arXiv preprint

Corrado, and Jeffrey Dean.
word representations in vector space.
arXiv:1301.3781, 2013.

[Pang and Lee, 2004] Bo Pang and Lillian Lee. A sentimen-
tal education: Sentiment analysis using subjectivity sum-
In Proceedings of
marization based on minimum cuts.
ACL, 2004.

[Socher et al., 2011] Richard Socher, Jeffrey Pennington,
Eric H Huang, Andrew Y Ng, and Christopher D Man-
ning. Semi-supervised recursive autoencoders for predict-
In Proceedings of EMNLP,
ing sentiment distributions.
2011.

[Socher et al., 2012] Richard Socher, Brody Huval, Christo-
pher D Manning, and Andrew Y Ng. Semantic composi-
tionality through recursive matrix-vector spaces. In Pro-
ceedings of EMNLP, pages 1201–1211, 2012.

[Socher et al., 2013] Richard Socher, Alex Perelygin, Jean Y
Wu, Jason Chuang, Christopher D Manning, Andrew Y
Ng, and Christopher Potts. Recursive deep models for
semantic compositionality over a sentiment treebank. In
Proceedings of EMNLP, 2013.

[Tai et al., 2015] Kai Sheng Tai, Richard Socher,

and
Christopher D Manning.
Improved semantic representa-
tions from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075, 2015.

[Turian et al., 2010] Joseph Turian, Lev Ratinov, and Yoshua
Bengio. Word representations: a simple and general
In Proceedings of
method for semi-supervised learning.
ACL, 2010.

