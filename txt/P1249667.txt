0
2
0
2
 
n
a
J
 
1
2
 
 
]
L
C
.
s
c
[
 
 
2
v
6
1
6
0
1
.
2
1
9
1
:
v
i
X
r
a

Siamese Networks for Large-Scale Author Identification

Siamese Networks for Large-Scale Author Identiﬁcation

Chakaveh Saedi
Mark Dras
Macquarie University, Department of Computing, Sydney, Australia

chakaveh.saedi@hdr.mq.edu.au
mark.dras@mq.edu.au

Abstract

Authorship attribution is the process of identifying the author of a text. Classiﬁcation-
based approaches work well for small numbers of candidate authors, but only similarity-
based methods are applicable for larger numbers of authors or for authors beyond the train-
ing set. While deep learning methods have been applied to classiﬁcation-based approaches,
applications to similarity-based applications have been limited, and most similarity-based
methods only embody static notions of similarity.

Siamese networks have been used to develop learned notions of similarity in one-shot
image tasks, and also for tasks of mostly semantic relatedness in NLP. We examine their
application to the stylistic task of authorship attribution on datasets with large numbers of
authors, looking at multiple energy functions and neural network architectures, and show
that they can substantially outperform both classiﬁcation- and existing similarity-based
approaches. We also ﬁnd an unexpected relationship between choice of energy function
and number of authors, in terms of performance.

1. Introduction

Authorship attribution is the task of identifying the author of a text, with real-world applica-
tions in, for example, law enforcement (Koppel, Schler, & Messeri, 2008) and recommender
systems (Alharthi, Inkpen, & Szpakowicz, 2018). This has included some publicly promi-
nent use of computational methods, for instance in uncovering the true identity of author
Robert Galbraith.1

Apart from early statistical methods, approaches to authorship attribution can be di-
vided into classiﬁcation-based and similarity-based (Stamatatos, 2009). Classiﬁcation-based
approaches standardly use machine learning, are the most common, and now include tech-
niques using deep learning (Ruder, Ghaﬀari, & Breslin, 2016). These have successfully tack-
led basic versions of the problem, mostly with small numbers (< 50) of authors. Similarity-
based approaches are better suited to large numbers of candidate authors; there are fewer
of them, notably the Writeprints method of Abbasi and Chen (2008) and the method of
Koppel, Schler, and Argamon (2011), with the latter the core of two of the winners of PAN
authorship shared tasks2 (Seidman, 2013; Khonji & Iraqi, 2014) and a standard inference
attacker for the PAN shared task on authorship obfuscation. Koppel et al. (2011) also
note that reducing authorship attribution to instances of the binary authorship veriﬁcation
problem — determining if a given document is by a particular author or not — permits
authorship attribution in cases where the author is not one of the known candidates, and

1. JK Rowling, writing in a new genre: https://on.natgeo.com/2RNvkWq
2. https://pan.webis.de/

1

Saedi & Dras

is more naturally suited to similarity-based models. However, most existing methods use
only a static notion of similarity over ﬁxed features, rather than a learned one.

Deep learning semantic similarity models have been used extensively for various NLP
tasks like QA and image captioning (e.g. the DSSM models of Yih, He, and Meek (2014)
and Fang, Gupta, Iandola, Srivastava, Deng, Dollr, Gao, He, Mitchell, Platt, Zitnick, and
Zweig (2015), respectively), duplicate question detection (Rodrigues, Saedi, Maraev, Silva,
& Branco, 2017), and semantic composition (Cheng & Kartsaklis, 2015).

A task that has parallels to our own comes from image processing: building on the
original use of Siamese networks for signature veriﬁcation by Bromley, Guyon, LeCun,
S¨ackinger, and Shah (1994), Koch, Zemel, and Salakhutdinov (2015) use deep Siamese
networks to learn a notion of similarity between images, where the generality of this notion
is evaluated via one-shot recognition. This style of Siamese network has been adapted
for a range of semantics-based NLP tasks, such as for sentence similarity by Mueller and
Thyagarajan (2016) and for tasks like paraphrase identiﬁcation by Yin, Sch¨utze, Xiang,
and Zhou (2016). A proposal to use them for the stylistic task of author identiﬁcation
came from Dwyer (2017), who noted that within NLP they have been used only for short
texts rather than the longer sort standardly used for author identiﬁcation; while the idea
was appealing, results in that work were not positive, and performed in some cases worse
than the baseline. Very recently Boenninghoﬀ, Nickel, Zeiler, and Kolossa (2019) applied
Siamese networks to short social media texts on a relatively small PAN dataset, producing
some positive results.

We deﬁne a range of Siamese-based architectures for authorship attribution on the sorts
of texts standardly used in this task and across large numbers of authors, and evaluate them
in both known-author and one-shot learning contexts; as part of this, we examine the eﬀect
of choice of energy function, sub-network structure and text representation. We show that
they can outperform both a strong classiﬁcation-based baseline and, in one-shot contexts,
the key similarity-based method of Koppel et al. (2011), on datasets with large numbers
of authors. We also ﬁnd an interesting interaction between energy function and number of
authors, and clear preferences for choice of sub-network type and text representation.

2. Related Work

Authorship Identiﬁcation Stamatatos (2009) surveys approaches up until 2009: we
noted in §1 the division into classiﬁcation-based and similarity-based approaches, the latter
of which is better suited to large numbers of authors. A key work following the survey
was the similarity-based approach of Koppel et al. (2011). The method represents texts by
vectors of space-free character 4-grams, and then repeatedly samples features from these
vectors and takes the cosine similarity between the vectors consisting of these sampled
features; Koppel and Winter (2014) later found that the Ruzicka metric produced better
results. Like the majority of work in authorship identiﬁcation, these are applied to longer
texts; in this speciﬁc instance, to blog posts taken from 10,000 authors.3 Given the successful
application to a very large number of authors, we use this as a baseline method in this paper.
Much work on authorship identiﬁcation since then has appeared in PAN shared tasks:
the years with attribution setups like this paper were 2011, 2012 and 2018. These attribution

3. Taken from blogger.com.

2

Siamese Networks for Large-Scale Author Identification

Figure 1: Siamese network architecture.

tasks have required choosing among small numbers of authors, e.g. 3 for 2012 (Juola,
2012) up to 20 for 2018 (Kestemont, Tschuggnall, Stamatatos, Daelemans, Specht, Stein,
& Potthast, 2018). For the most part systems in these tasks use conventional ML: the
2018 winner used an ensemble classiﬁer (Cust´odio & Paraboni, 2018) and the runner-up a
linear SVM (Murauer, Tschuggnall, & Specht, 2018). As noted in §1, two earlier winners in
non-attribution setups (Seidman, 2013; Khonji & Iraqi, 2014) were based on the similarity
approach of Koppel et al. (2011), which we used as a baseline. Another exception to
conventional ML was the 2015 winner, Bagnall (2015), using an RNN-based classiﬁer with
shared state but diﬀerent softmax layer for each author: the architecture is not generally
applicable.

Outside the PAN framework, some work is speciﬁc to certain authorship contexts and
not purely stylistic: e.g. Chen and Sun (2017) and Zhang, Huang, Yu, Zhang, and Chawla
(2018) on scientiﬁc authorship, incorporating publication content and references. Notable
work on purely stylistic authorship identiﬁcation, as in this paper, included the use of
LDA by Seroussi, Zukerman, and Bohnert (2011), both within an SVM and using Hellinger
distance, to handle large numbers of authors; this was extended in Seroussi, Zukerman,
and Bohnert (2014). Mohsen, El-Makky, and Ghanem (2016) used feature extraction via a
stack denoising auto encoder and then classiﬁcation via SVM. Ruder et al. (2016) proposed
a CNN classiﬁcation model which outperformed Seroussi et al. (2011) and various other
conventional ML approaches on up to 50 authors across a range of datasets; given the set of
comparators and the relatively large number of authors used for a classiﬁcation approach,
we use it in this paper as a second baseline.

3

Saedi & Dras

Siamese Architectures Siamese networks were ﬁrst used for verifying signatures, by
framing it as an image matching problem (Bromley et al., 1994). The key features of the
Siamese network were that it consisted of twin sub-networks, linked together by an energy
function (Fig 1). The weights on the sub-networks are tied, so that the sub-networks are
always identical:
inputs are then mapped into the same space, and the energy function
represents some notion of distance between them. Siamese networks were updated for deep
learning by Koch et al. (2015) for the task of general image recognition. The sub-networks
were convolutional neural networks (CNNs), and to the outputs of the ﬁnal layers of these
CNNs the weighted L1 distance was calculated and a sigmoid activation applied; a cross-
entropy objective was then used in training.

In the ﬁeld of NLP, most work on Siamese networks has been applied at the level of
sentences or below. Mueller and Thyagarajan (2016) applied a Siamese network structured
like that of Koch et al. (2015) to the problem of sentence similarity using the SICK dataset,
where pairs of sentences have been assigned similarity scores derived from human judge-
ments. Their architecture similarly used L1 (Manhattan) distance, but the sub-networks
were LSTMs. At around the same time, Yin et al. (2016) deﬁned an attention-based model
motivated by the Siamese architecture of Bromley et al. (1994), and applied it to the tasks of
answer selection, paraphrase identiﬁcation and textual entailment. Much subsequent work
has been similar in terms of applications: to answer selection or question answering (Das,
Yenala, Chinnakotla, & Shrivastava, 2016; Tay, Tuan, & Hui, 2018; Hu, 2018; Lai, Bui, & Li,
2018), sentence similarity (Reimers & Gurevych, 2019), job title normalisation (Neculoiu,
Versteegh, & Rotaru, 2016), matching e-commerce items (Shah, Kopru, & Ruvini, 2018),
learning argumentation (Joshi, Baldwin, Sinnott, & Paris, 2018; Gleize, Shnarch, Choshen,
Dankin, Moshkowich, Aharonov, & Slonim, 2019), and detecting funnier tweets (Baziotis,
Pelekis, & Doulkeridis, 2017). In some cases Siamese networks have been applied to word-
based rather than sentence-based problems, such as identifying cognates (Rama, 2016) or
antonyms (Etcheverry & Wonsever, 2019). In other cases the application of Siamese net-
works is secondary to the main task, such as relation extraction (Rossiello, Gliozzo, Farrell,
Fauceglia, & Glass, 2019) or supervised topic modelling (Huang, Rao, Liu, Xie, & Wang,
2018). The most typical conﬁguration is to use some kind of RNN as sub-network and
cosine similarity for the energy function; but CNNs are also used in sub-networks, and for
energy functions L1 and L2 are also used, along with some less common alternatives such as
a hyperbolic distance function (Tay et al., 2018) or one based on LSTM-based importance
weighting (Hu, 2018).

There have been two attempts to use Siamese networks for author identiﬁcation. The
ﬁrst was Dwyer (2017), who observed that within NLP they have been used only for short
texts rather than the longer sort standardly used for author identiﬁcation, which is sup-
ported by the above summary of applications of Siamese networks to NLP. For sub-networks,
Dwyer (2017) used fully connected networks, and L2 as the energy function. Experimen-
tally, on data from the PAN 2014 and 2015 tasks, he found that results were fairly poor,
in some cases worse than a random baseline. Very recently Boenninghoﬀ et al. (2019) ap-
plied Siamese networks to short social media texts, using an architecture with LSTMs as
sub-networks and an energy function based on Euclidean distance. This was applied to the
relatively small PAN 2016 dataset, and produced some positive results.

4

Siamese Networks for Large-Scale Author Identification

3. The Model

Our architecture follows the basic structure of Koch et al. (2015), and of Mueller and
Thyagarajan (2016) for sentence similarity; these both used L1 distance for the energy
function, although Koch et al. (2015) used CNNs for the sub-networks while Mueller and
Thyagarajan (2016) used LSTMs. The goal of our network is to produce similarity scores
for text pairs such that pairs by the same authors have high scores and those by diﬀerent
authors have lower scores.

Below we deﬁne the components of our primary models. We also note alternative model

choices for the sub-networks, which we examine after the main results.

Sub-networks Like Koch et al. (2015), we used CNNs here, in line with the observation of
Kim (2014) that CNNs are good at text classiﬁcation. Our primary sub-network architecture
is similar to that of Ruder et al. (2016), a high-performing CNN classiﬁcation approach to
authorship attribution. The input for our primary model is character-level: Ruder et al.
(2016) found that character-level input almost always worked best, and the representation
is also character-level in Koppel et al. (2011), in line with the observations of Keˇselj, Peng,
Cercone, and Thomas (2003) about stylistic authorship classiﬁcation. Each sub-network
consists of an embedding layer, four convolutional layers, and a dense layer. The activation
functions are tanh for convolutional layers and sigmoid for dense layers.

We do also examine the eﬀect of diﬀerent choices here: choosing word-level input instead
of character-level input, and LSTMs instead of CNNs. For the LSTM alternatives, we look
at both unidirectional (left-to-right) and bidirectional. In these, we use a hidden layer with
200 nodes.

Energy functions Koch et al. (2015) considered both the L1 and L2 distances between
the outputs of the ﬁnal layers of their sub-networks (vectors v1, v2 in our Fig 1), and found
that L1 worked better for their image matching task. Adapting their notation, we use this
same function for our distance calculation:

p = σ(

αj |v(j)

1 − v(j)

2

|)

(cid:88)

j

where vi is the output of the ﬁnal layer of sub-network i (in our case, the dense layer after
the convolutional layers) and v(j)
the jth element of it; αj the additional parameters that
are learned by the model during training, weighting the importance of the component-wise
distance; and σ(·) the sigmoid activation function. This deﬁnes the ﬁnal fully-connected
layer for the network which joins the two siamese twins. When applied to our CNN sub-
networks described above, we refer to the architecture as SiamL1.

i

We also observe, however, that in text-related tasks cosine similarity is commonly used:
this is the measure used in Koppel et al. (2011) and many text-based Siamese or DSSM
models, as discussed in §2. We therefore introduce a variant of SiamL1 where the distance
calculation is the complement of the cosine similarity between v1 and v2, similar to Rodrigues
et al. (2017). As this is a scalar quantity, there is no ﬁnal dense layer. The energy function
is then:

p = cossim(v1, v2)

5

Saedi & Dras

4. Experimental Setup

4.1 Evaluation Framework

Known Author vs One-Shot We consider two types of evaluation. The ﬁrst is the
one-shot evaluation of Koch et al. (2015). Here the set of authors in the test set is disjoint
with respect to the authors in the training set. Classiﬁcation approaches do not apply here,
as there is no way to build a model of a previously unseen author. Similarity approaches
will only work to the extent that they embody general notions of stylistic similarity between
authors. We refer to this as the OneShot setup. In OneShot, the training set consists of
2/3 of the authors, as described below.

The second type of evaluation is common in authorship attribution: while the texts
in the training and test sets are diﬀerent, the same set of authors is represented in both.
We refer to this as the KnownAuth setup. Classiﬁcation approaches are applicable here,
as well as similarity; for the similarity approaches, what they embody could involve both
properties of speciﬁc authors and general models of authorial similarity. In KnownAuth,
the training set consists of 3/4 of the texts.

Veriﬁcation vs N-way As in Koch et al. (2015), we begin with the task of veriﬁcation:
Are two texts by the same author? We use this solely to investigate how our Siamese models
perform on their fundamental task of scoring similar authors high and diﬀerent authors low.
The main task, also framed as in Koch et al. (2015), is N-way evaluation: Given a text
T by author A, select the text out of N candidates that is also by A; there will be exactly
one by A among the N .

The N -way evaluation applies to both KnownAuth and OneShot frameworks. The
similarity approaches choose the candidate from among the N that has the highest similarity
score to T . For the classiﬁcation approach in KnownAuth (§4.3), the candidate that is
chosen is the author with the highest network prediction among the N .

4.2 Data

Datasets There are several datasets previously employed for author identiﬁcation, in-
cluding various PAN datasets;4 the Enron emails corpus; a set of IMDB reviews; and the
Blog Authorship Corpus (Schler, Koppel, Argamon, & Pennebaker, 2006), a large sample
of personal blogs collected from blogger.com.5

We use the last of these as it includes a suﬃciently large number of authors for our
purposes: we extracted a subset of 1950 authors that contains all blogs with at least 1500
words, and retain as the text the ﬁrst 1000 words. The average number of samples per
author is 2.83, and the average vocabulary size under character-level tokenization is 270.
We refer to this dataset as bl-2K.

In addition, we use a more recent dataset put together by Fernandes, Dras, and McIver
(2019). Like the PAN 2018 attribution task, it consists of fanﬁction; we choose this dataset

4. While the PAN datasets have been used by a number of authors, they are small for our many-author
setup (the largest has 180 authors) and too small to train a decent deep learning model. With respect to
using a model trained on a diﬀerent domain, cross-domain authorship attribution is very much an open
problem (Kestemont et al., 2018).

5. http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm

6

Siamese Networks for Large-Scale Author Identification

DB

SiamL1
ff-100
0.553
ff-1K
0.980
ff-5K
0.982
ff-10K 0.982
bl-2K

0.958

Siamcos

0.504
0.947
0.960
0.958

0.935

Table 1: Veriﬁcation: accuracy on ff, bl-2K datasets.

as it has more authors. It was collected from fanfiction.net from the ﬁve most popular
fandoms (“Harry Potter”, “Hunger Games”, “Lord of the Rings”, “Percy Jackson and the
Olympians” and “Twilight”).6 We observe that having authors writing on similar topics
(within a small number of “fandoms”) means that methods cannot rely on topic cues. From
this we have put together 4 subsets of varying numbers of randomly chosen authors (100,
1K, 5K and 10K). Each text consists of 2000 words. The average number of samples per
author is 2.1, and the average vocabulary size under character-level tokenization is 365. We
will refer to these datasets as ff-n, where n is the number of authors.

For all datasets, we did not employ any speciﬁc pre-processing such as lemmatization
or lower-casing, nor did we replace digits, letters or punctuation, as these can be indicators
of authorship.

Training Data To produce a reasonable number of samples, we divide each text into 8
pieces. In order to generate same/diﬀerent author pairs for training the Siamese networks,
the pieces are divided into 4 chunks, which are then paired (Appendix A gives details of the
data preparation and resulting number of pairs). The ﬁnal train and test sets are balanced
in the number of similar and diﬀerent pairs. We keep 10% of the training set for validation
data.

Test Data and Evaluation Metric For N -way evaluation, we randomly create 500
sets of N -way authors from the appropriate test set (KnownAuth or OneShot), and
we calculate the accuracy in predicting the correct author. Final results are based on the
average of three runs of diﬀerent sets of 500. For veriﬁcation, we report results on all
elements of the test set.

4.3 Baselines

Similarity As noted in §1, the most prominent authorship similarity-based method is by
Koppel et al. (2011). To our knowledge, this is the only available method that can be used
in our one-shot experimental setup.7 We used as a starting point code from a reproducibil-
ity study (Potthast, Braun, Buz, Duﬀhauss, Friedrich, G¨ulzow, K¨ohler, L¨otzsch, M¨uller,
M¨uller, Paßmann, Reinke, Rettenmeier, Rometsch, Sommer, Tr¨ager, Wilhelm, Stein, Sta-

6. Available at https://github.com/ChakavehSaedi/Siamese-Author-Identification.
7. The other main method (Abbasi & Chen, 2008) appears not to have an available implementation or

suﬃcient detail for reimplementation.

7

Saedi & Dras

matatos, & Hagen, 2016); we reimplemented it to improve performance. We refer to this
as Koppel.

Classiﬁcation As noted in §3, the sub-networks in our Siamese architecture are similar
to the high-performing method of Ruder et al. (2016) (see §2). We use an individual sub-
network as our classiﬁcation architecture. We refer to this as cnn.

As another baseline, we consider the type of approach based on language model pre-
training that has recently come to dominate performance in many NLP tasks. In these, pre-
trained language representations can be used either as additional features in a task-speciﬁc
architecture (e.g. ELMo: Peters, Neumann, Iyyer, Gardner, Clark, Lee, and Zettlemoyer
(2018)) or via transfer learning and the ﬁne-tuning of parameters for a speciﬁc task (e.g.
GPT: Radford, Narasimhan, Salimans, and Sutskever (2018)). BERT (Devlin, Chang, Lee,
& Toutanova, 2019) is an approach that in 2019 has produced state-of-the-art performance
on a range of NLP tasks set up as the GLUE benchmark8 (Wang, Singh, Michael, Hill,
Levy, & Bowman, 2018): it gave the best performance on all tasks in this suite, including
sentiment classiﬁcation, prediction of grammatical acceptability, textual similarity, para-
phrase, and natural language inference; improvements on many of the tasks were quite
large with respect to previous state of the art. Later analysis (Tenney, Das, & Pavlick,
2019) has shown that BERT can perform across levels of linguistic analysis, from low (e.g.
part-of-speech tagging) to high (e.g. semantic roles).

We therefore use BERT ﬁne-tuned for our classiﬁcation task as our second baseline. We
do this by feeding the output of BERT to a dense layer, and carrying out a small amount
of extra training.

4.4 Implementation Details

Siamese Networks
layer, four convolutional layers, and a dense layer (resp. Emb, Convn, D in Table 2).

In terms of structure, each sub-network consists of an embedding

Type

Emb Conv1 Conv2 Conv3 Conv4

D

Size
Filter Size

300

350
1

300
2

250
3

400

250
3

Table 2: Network parameters

The Siamese networks are trained on the veriﬁcation task, for at most 25 epochs. All
initializations are random, and training is restarted if after the 10th epoch the veriﬁcation
accuracy is smaller than 0.5.

The epoch we select for the ﬁnal result is the second or third best in veriﬁcation accuracy
on the validation set, whichever has the lower validation loss. (We observed on validation
data that the epoch with the best N -way accuracy was earlier than the epoch with the best
veriﬁcation accuracy.)

The hyper-parameters are 0.0005 as learning rate, Adam as optimizer, and batch size

of 25.

For the LSTM variants, hyperparameters have the same settings.

8. https://gluebenchmark.com/

8

Siamese Networks for Large-Scale Author Identification

Koppel Koppel has few parameters. The maximum number of character 4-grams is set
to 20,000 as in the replication code; the actual number of character 4-grams in our data is
always lower than this. The replication code samples 50% of the features, and repeats this
100 times, which Koppel et al. (2011) found to produce good results. The replication code
also by default uses the Ruzicka metric rather than cosine similarity (which we also found to
perform better). There is an additional parameter σ, a threshold for a ‘don’t know’ option;
we always make a choice, and so set σ to be 0.

We reimplemented the replication code to be more eﬃcient, in order to run on larger
numbers of authors: the replication code did not, for example, have eﬃcient implementa-
tions of vector arithmetic. We veriﬁed that the replication code and our reimplementation
performed the same on the PAN 2011 and 2012 and ff-100 datasets. Results in the paper
are all from our reimplementation.

CNN The CNN classiﬁcation model is trained for at most 150 epochs, and the epoch
with the best validation accuracy across all classes is selected.

BERT To ﬁne-tune BERT for authorship attribution, we trained for 3 epochs, as did
Devlin et al. (2019) for all GLUE benchmark tasks. BERT takes as input sentences, so
we segmented our input at the period character. (Other segmentations produced similar
results, although they declined more steeply for larger N .)

5. Experimental Results

5.1 Veriﬁcation

Table 1 gives the results for author veriﬁcation for our two Siamese variants. As expected,
it starts at essentially random when there
accuracy improves with more training data:
are only 100 authors to learn a notion of similarity from, increasing rapidly when there are
1000 authors to 0.980 for SiamL1 and 0.947 for Siamcos; there is no improvement for 10000
authors. SiamL1 is better at all sizes than Siamcos. The scores on bl-2K are slightly lower
than might be expected from the number of authors.

5.2 N -Way One-Shot

The veriﬁcation results above indicate that 100 authors do not provide enough data for the
Siamese networks to train, and that results for 10000 authors are no better than for 5000
authors. For the N -way one-shot scenario, then, Table 3 presents results for ff-1K, ff-5K
and bl-2K. We make the following observations:

• All results are much higher than chance (= 1/N ), and naturally degrade as N in-

creases.

• For small to moderate N , both Siamese networks are clearly better than Koppel. On
ff-5K, only SiamL1 continues to be better, and still by quite a large margin: Figure 2
illustrates this trend.

• Siamcos in each case starts oﬀ the highest, but drops the most.

9

Saedi & Dras

ff-1K

ff-5K

bl-2K

N Koppel SiamL1
0.941
2
0.907
3
0.855
5
0.731
10
0.583
20
0.407
50
0.325
100
0.275
300

0.826
0.717
0.653
0.509
0.407
0.285
0.220
0.148

2
3
5
10
20
50
100
500
1000

2
3
5
10
20
50
100
500

0.802
0.685
0.601
0.479
0.379
0.275
0.235
0.132
0.132

0.838
0.732
0.659
0.579
0.461
0.401
0.345
0.226

0.951
0.929
0.843
0.723
0.610
0.465
0.382
0.317
0.284

0.925
0.895
0.809
0.680
0.551
0.431
0.373
0.266

Siamcos

0.973
0.950
0.904
0.820
0.699
0.481
0.313
0.123

0.975
0.961
0.938
0.866
0.777
0.553
0.365
0.119
0.072

0.966
0.921
0.883
0.786
0.663
0.469
0.324
0.087

10

Table 3: Results under the OneShot scenario on ff-1K, ff-5K and bl-2K: N -way clas-

siﬁcation accuracy.

Siamese Networks for Large-Scale Author Identification

Figure 2: Results on ff-5K under the OneShot scenario: accuracy under N -way classiﬁ-

cation.

• Koppel performs relatively better on bl-2K (which was its original test corpus),
comparing like N s against the performance of Koppel on the other corpora; it ﬁts
with the small relative drop in performance of the Siamese networks found in the
veriﬁcation results in Table 1. This may be because the original texts were smaller.

Model Alternatives
In addition to the architectural choices for our primary model as
described in §3, we also tried word-level inputs, and these as expected performed consistently
worse indicating stylistic features can be better identiﬁed through characters. Table 4 shows
a comparison between word- and character-level inputs on ff-1K under SiamL1.
It is
apparent that the diﬀerence is large and gets dramatically larger as N increases. In the
pre-deep-learning era, Keˇselj et al. (2003) argued that character-level representations better
capture stylistic characteristics for authorship; this is supported by these results.

N

2

3

5

10

100

0.941
Char
Word 0.624

0.907
0.436

0.855
0.280

0.731
0.142

0.325
0.018

Table 4: N-way accuracy under word- and character-level

inputs on ff-1K under the

OneShot scenario

Another alternative discussed in §3 was to use LSTMs for sub-networks; results, again
on ff-1K, are in Table 5. Here also the results of the alternative left-to-right LSTMs are
substantially worse than for CNNs; the biLSTMs are better than the LtR LSTMs, but also
worse than CNNs except under the 100-way setup. In addition, the LSTM-based networks
are much slower to train, taking orders of magnitude longer: for the 100-way setup, one
epoch took around a day. They are clearly infeasible for the size of text standardly used
for authorship attribution.

11

Saedi & Dras

N

2

3

5

10

100

CNN
LSTM (LtR)
LSTM (bi-dir)

0.941
0.484
0.898

0.907
0.332
0.878

0.731
0.116
0.780

0.325
0.04
0.526

Table 5: N-way accuracy using LSTMs vs CNNs on ff-1K under the OneShot scenario

We also considered both L2 as a variant of SiamL1 and the Ruzicka or minmax metric
as a variant of Siamcos, as this latter has been found to be an improvement of Koppel
et al. (2011) by Koppel and Winter (2014). Again, results were consistently poorer and we
do not present them.

5.3 N -Way Known Author
Table 6 shows the results under the KnownAuth scenario: we chose the smallest of the
three datasets from Table 3, ff-1K, so that the classiﬁcation approach would be competi-
tive.

• For the smallest case, of N = 2, CNN classiﬁcation does better than the traditional
Koppel similarity, although it degrades much more quickly as N grows: this conforms
to the general belief that similarity methods work better for large numbers of authors.

• BERT follows a similar pattern. It starts slightly lower than CNN — as it uses word-
level representations, this is not unexpected, in spite of its strong performance on
other tasks — but degrades more slowly.

• Our new methods, SiamL1 and Siamcos, behave similarly to the OneShot scenario.
Siamcos starts as the highest at N = 2, but degrades fastest; it outperforms other
methods until at least N = 10. After this point Koppel is the best.

• Comparing Table 3 and Table 6, it can be seen that the Siamese scores are uniformly
lower for equivalent N . This is because the network receives as input only 3/4 of the
data per author (with 1/4 held out for KnownAuth testing). We would expect that
with quantities of training text per author that are similar to the OneShot scenario,
we would see the same higher levels of accuracy for the Siamese methods.

6. Conclusion

In this work we have presented an investigation of the application Siamese network ar-
chitecture to large-scale stylistic author attribution. Our system learns a general notion
of authorship, strongly outperforming the key similarity-based method in one-shot N -way
evaluation, and also performs well in a known-author context. Interestingly, for large num-
bers of authors, the L1 metric that is more common in image processing works better
in a Siamese network than the cosine measure typically used in NLP semantic similarity,
although cosine similarity is better for small numbers of authors.

12

Siamese Networks for Large-Scale Author Identification

N

2
3
5
10
50
100
500

cnn BERT Koppel SiamL1
0.826
0.855
0.756
0.728
0.624
0.601
0.496
0.488
0.270
0.242
0.246
0.172
0.164
0.083

0.828
0.848
0.627
0.516
0.262
0.214
0.100

0.816
0.717
0.625
0.523
0.331
0.307
0.210

Siamcos

0.890
0.836
0.754
0.612
0.200
0.096
0.026

Table 6: Results under the KnownAuth scenario on ff-1K: N -way classiﬁcation accuracy.

Further work could explore other architectures used for one-shot tasks in image pro-
cessing, such as the matching networks of Vinyals, Blundell, Lillicrap, Kavukcuoglu, and
Wierstra (2016).

Appendix A. Data Preparation

Training Data To produce a reasonable number of samples, we divide each text into 8
pieces; if there are N authors and M documents per author, this gives 8 × N × M pieces.
In order to generate same/diﬀerent author pairs for training the Siamese networks, the
pieces are divided into 4 chunks. Pieces included in the ﬁrst two chunks for an author A
(colored blue in Figure 3(a)) are randomly paired together to create same-author pairs. For
diﬀerent-author pairs, pieces in the third chunk (light gray in Figure 3(a)) for author A are
paired with pieces in the fourth chunk (dark gray in Figure 3(a)) for some other author B;
both selections (of author B and of sample piece) are randomized. In this way, we make sure
none of the samples forming similar pairs are used more than once. Figure 3(b) illustrates
a schematic train/test-set where TijAi stands for the jth piece written by the ith author.

Table 7 shows the number of pairs making up these datasets.

OneShot

KnownAuth
Train

dataset

Test Train

Test

ff-100
600
ff-1K
6044
ff-5K 32350
ff-10K 69710
bl-2K 13410

200
2010
10658
22818

520
5350
29650
64670

280
2700
13700
29120

2580

12730

6380

13

Table 7: Number of pairs included in the train and test sets for the Siamese network.

Saedi & Dras

14

Figure 3: Schematic view of (a) how pieces are randomly paired to create similar and dif-
ferent entries for train and test. (b) Test-set data for binary classiﬁcation (author
veriﬁcation). (c) One set of 5-way-1-shot task randomly selected from test-set;
where TijAi stands for the jth piece written by the ith author.

Siamese Networks for Large-Scale Author Identification

References

Abbasi, A., & Chen, H. (2008). Writeprints: A Stylometric Approach to Identity-Level Iden-
tiﬁcation and Similarity Detection in Cyberspace. ACM Transactions on Information
Systems, 26 (2).

Alharthi, H., Inkpen, D., & Szpakowicz, S. (2018). Authorship identiﬁcation for literary
book recommendations. In Proceedings of the 27th International Conference on Com-
putational Linguistics, pp. 390–400.

Bagnall, D. (2015). Author identiﬁcation using multi-headed recurrent neural networks.

arXiv preprint arXiv:1506.04891.

Baziotis, C., Pelekis, N., & Doulkeridis, C. (2017). DataStories at SemEval-2017 Task 6:
Siamese LSTM with Attention for Humorous Text Comparison. In Proceedings of the
11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 390–395,
Vancouver, Canada. Association for Computational Linguistics.

Boenninghoﬀ, B., Nickel, R. M., Zeiler, S., & Kolossa, D. (2019). Similarity Learning for Au-
thorship Veriﬁcation in Social Media. In Proceedings of the International Conference
on Acoustics, Speech and Signal Processing (ICASSP).

Bromley, J., Guyon, I., LeCun, Y., S¨ackinger, E., & Shah, R. (1994). Signature veriﬁcation
In Advances in neural information

using a “siamese” time delay neural network.
processing systems, pp. 737–744.

Chen, T., & Sun, Y. (2017). Task-guided and path-augmented heterogeneous network
embedding for author identiﬁcation. In Proceedings of the Tenth ACM International
Conference on Web Search and Data Mining, pp. 295–304. ACM.

Cheng, J., & Kartsaklis, D. (2015). Syntax-Aware Multi-Sense Word Embeddings for Deep
Compositional Models of Meaning. In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing, pp. 1531–1542, Lisbon, Portugal.
Association for Computational Linguistics.

Cust´odio, J. E., & Paraboni, I. (2018). Each-usp ensemble cross-domain authorship attri-

bution. Working Notes Papers of the CLEF.

Das, A., Yenala, H., Chinnakotla, M., & Shrivastava, M. (2016). Together we stand: Siamese
Networks for Similar Question Retrieval. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pp. 378–387,
Berlin, Germany. Association for Computational Linguistics.

Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep
bidirectional transformers for language understanding.
In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.
4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Dwyer, G. (2017). Novel Approaches to Authorship Attribution. Master’s thesis, University

of Groningen and Saarland University.

Etcheverry, M., & Wonsever, D. (2019). Unraveling Antonym’s Word Vectors through a
Siamese-like Network. In Proceedings of the 57th Annual Meeting of the Association

15

Saedi & Dras

for Computational Linguistics, pp. 3297–3307, Florence, Italy. Association for Com-
putational Linguistics.

Fang, H., Gupta, S., Iandola, F. N., Srivastava, R. K., Deng, L., Dollr, P., Gao, J., He, X.,
Mitchell, M., Platt, J. C., Zitnick, C. L., & Zweig, G. (2015). From captions to visual
concepts and back. In CVPR, pp. 1473–1482. IEEE Computer Society.

Fernandes, N., Dras, M., & McIver, A. (2019). Generalised Diﬀerential Privacy for Text
Document Processing. In Proceedings of Principles of Security and Trust (POST),
Vol. 11426 of LNCS, pp. 123–148.

Gleize, M., Shnarch, E., Choshen, L., Dankin, L., Moshkowich, G., Aharonov, R., & Slonim,
N. (2019). Are You Convinced? Choosing the More Convincing Evidence with a
Siamese Network. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, pp. 967–976, Florence, Italy. Association for Computa-
tional Linguistics.

Hu, S. (2018). Somm: Into the Model.

In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing, pp. 1153–1159, Brussels, Belgium.
Association for Computational Linguistics.

Huang, M., Rao, Y., Liu, Y., Xie, H., & Wang, F. L. (2018). Siamese network-based super-
vised topic modeling. In Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pp. 4652–4662, Brussels, Belgium. Association for
Computational Linguistics.

Joshi, A., Baldwin, T., Sinnott, R. O., & Paris, C. (2018). UniMelb at SemEval-2018 Task
12: Generative Implication using LSTMs, Siamese Networks and Semantic Represen-
tations with Synonym Fuzzing. In Proceedings of The 12th International Workshop on
Semantic Evaluation, pp. 1124–1128, New Orleans, Louisiana. Association for Com-
putational Linguistics.

Juola, P. (2012). An overview of the traditional authorship attribution subtask.. In CLEF

(Online Working Notes/Labs/Workshop).

Kestemont, M., Tschuggnall, M., Stamatatos, E., Daelemans, W., Specht, G., Stein, B.,
& Potthast, M. (2018). Overview of the Author Identiﬁcation Task at PAN-2018
Cross-domain Authorship Attribution and Style Change Detection. In CLEF 2018
Evaluation Labs and Workshop.

Keˇselj, V., Peng, F., Cercone, N., & Thomas, C. (2003). N-Gram-Based Author Proﬁles for
Authorship Attribution. In Proceedings of the Paciﬁc Association for Computational
Linguistics (PACLING), pp. 255–264.

Khonji, M., & Iraqi, Y. (2014). A Slightly-modiﬁed GI-based Author-veriﬁer with Lots of

Features (ASGALF). In Working Notes for CLEF 2014 Conference.

Kim, Y. (2014). Convolutional Neural Networks for Sentence Classiﬁcation. In Proceed-
ings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 1746–1751. Association for Computational Linguistics.

Koch, G., Zemel, R., & Salakhutdinov, R. (2015). Siamese neural networks for one-shot

image recognition. In ICML Deep Learning Workshop, Vol. 2.

16

Siamese Networks for Large-Scale Author Identification

Koppel, M., Schler, J., & Argamon, S. (2011). Authorship attribution in the wild. Language

Resources and Evaluation, 45 (1), 83–94.

Koppel, M., Schler, J., & Messeri, E. (2008). Authorship Attribution in Law Enforcement
Scenarios. In Cantor, P., & Shapira, B. (Eds.), Security Informatics and Terrorism -
Patrolling the Web. IOS Press NATO Series.

Koppel, M., & Winter, Y. (2014). Determining if two documents are written by the same

author. JASIST, 65 (1), 178–187.

Lai, T. M., Bui, T., & Li, S. (2018). A Review on Deep Learning Techniques Applied
to Answer Selection. In Proceedings of the 27th International Conference on Com-
putational Linguistics, pp. 2132–2144, Santa Fe, New Mexico, USA. Association for
Computational Linguistics.

Mohsen, A. M., El-Makky, N. M., & Ghanem, N. (2016). Author identiﬁcation using deep
In 2016 15th IEEE International Conference on Machine Learning and

learning.
Applications (ICMLA), pp. 898–903. IEEE.

Mueller, J., & Thyagarajan, A. (2016). Siamese Recurrent Architectures for Learning Sen-
tence Similarity. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intel-
ligence (AAAI), pp. 2786–2792. AAAI Press.

Murauer, B., Tschuggnall, M., & Specht, G. (2018). Dynamic parameter search for cross-

domain authorship attribution. Working Notes of CLEF.

Neculoiu, P., Versteegh, M., & Rotaru, M. (2016). Learning Text Similarity with Siamese
Recurrent Networks. In Proceedings of the 1st Workshop on Representation Learning
for NLP, pp. 148–157, Berlin, Germany. Association for Computational Linguistics.

Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L.
(2018). Deep contextualized word representations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Papers), pp. 2227–2237, New Orleans,
Louisiana. Association for Computational Linguistics.

Potthast, M., Braun, S., Buz, T., Duﬀhauss, F., Friedrich, F., G¨ulzow, J., K¨ohler, J.,
L¨otzsch, W., M¨uller, F., M¨uller, M., Paßmann, R., Reinke, B., Rettenmeier, L.,
Rometsch, T., Sommer, T., Tr¨ager, M., Wilhelm, S., Stein, B., Stamatatos, E., &
Hagen, M. (2016). Who Wrote the Web? Revisiting Inﬂuential Author Identiﬁcation
Research Applicable to Information Retrieval. In Ferro, N., Crestani, F., Moens, M.-
F., Mothe, J., Silvestri, F., Di Nunzio, G., Hauﬀ, C., & Silvello, G. (Eds.), Advances
in Information Retrieval. 38th European Conference on IR Research (ECIR 16), Vol.
9626 of Lecture Notes in Computer Science, pp. 393–407, Berlin Heidelberg New York.
Springer.

Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language

understanding with unsupervised learning. Tech. rep., OpenAI.

Rama, T. (2016). Siamese Convolutional Networks for Cognate Identiﬁcation. In Proceed-
ings of COLING 2016, the 26th International Conference on Computational Linguis-
tics: Technical Papers, pp. 1018–1027, Osaka, Japan. The COLING 2016 Organizing
Committee.

17

Saedi & Dras

Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese
BERT-Networks.
In Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), pp. 3980–3990, Hong Kong, China. Associ-
ation for Computational Linguistics.

Rodrigues, J. A., Saedi, C., Maraev, V., Silva, J., & Branco, A. (2017). Ways of asking and
replying in duplicate question detection. In Proceedings of the 6th Joint Conference
on Lexical and Computational Semantics (* SEM 2017), pp. 262–270.

Rossiello, G., Gliozzo, A., Farrell, R., Fauceglia, N., & Glass, M. (2019). Learning Relational
Representations by Analogy using Hierarchical Siamese Networks. In Proceedings of
the 2019 Conference of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),
pp. 3235–3245, Minneapolis, Minnesota. Association for Computational Linguistics.

Ruder, S., Ghaﬀari, P., & Breslin, J. G. (2016). Character-level and multi-channel con-
arXiv preprint

volutional neural networks for large-scale authorship attribution.
arXiv:1609.06686.

Schler, J., Koppel, M., Argamon, S., & Pennebaker, J. (2006). Eﬀects of age and gender on
blogging. In Proceedings of the AAAI Spring Symposia on Computational Approaches
to Analyzing Weblogs.

Seidman, S. (2013). Authorship Veriﬁcation Using the Imposters Method.

In Working

Notes for CLEF 2013 Conference.

Seroussi, Y., Zukerman, I., & Bohnert, F. (2011). Authorship attribution with latent dirich-
In Proceedings of the ﬁfteenth conference on computational natural

let allocation.
language learning, pp. 181–189. Association for Computational Linguistics.

Seroussi, Y., Zukerman, I., & Bohnert, F. (2014). Authorship Attribution with Topic Mod-

els. Computational Linguistics, 40 (2), 269–310.

Shah, K., Kopru, S., & Ruvini, J.-D. (2018). Neural Network based Extreme Classiﬁcation
and Similarity Models for Product Matching. In Proceedings of the 2018 Conference
of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 3 (Industry Papers), pp. 8–15, New Orleans
- Louisiana. Association for Computational Linguistics.

Stamatatos, E. (2009). A survey of modern authorship attribution methods. Journal of the

American Society for information Science and Technology, 60 (3), 538–556.

Tay, Y., Tuan, L., & Hui, S. (2018). Hyperbolic Representation Learning for Fast and Eﬃ-
cient Neural Question Answering. In Proceedings of the Eleventh ACM International
Conference on Web Search and Data Mining (WSDM), pp. 583–591.

Tenney, I., Das, D., & Pavlick, E. (2019). BERT rediscovers the classical NLP pipeline.
In Proceedings of the 57th Annual Meeting of the Association for Computational Lin-
guistics, pp. 4593–4601, Florence, Italy. Association for Computational Linguistics.

Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., & Wierstra, D. (2016). Matching
Networks for One Shot Learning. In Proceedings of the 30th International Conference

18

Siamese Networks for Large-Scale Author Identification

on Neural Information Processing Systems, NIPS’16, pp. 3637–3645, USA. Curran
Associates Inc.

Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. (2018). GLUE: A
multi-task benchmark and analysis platform for natural language understanding. In
Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting
Neural Networks for NLP, pp. 353–355, Brussels, Belgium. Association for Computa-
tional Linguistics.

Yih, W.-t., He, X., & Meek, C. (2014). Semantic Parsing for Single-Relation Question
Answering. In Proceedings of the 52nd Annual Meeting of the Association for Com-
putational Linguistics, pp. 643–648. Association for Computational Linguistics.

Yin, W., Sch¨utze, H., Xiang, B., & Zhou, B. (2016). ABCNN: Attention-Based Convolu-
tional Neural Network for Modeling Sentence Pairs. Transactions of the Association
for Computational Linguistics, 4, 259–272.

Zhang, C., Huang, C., Yu, L., Zhang, X., & Chawla, N. V. (2018). Camel: Content-aware
and meta-path augmented metric learning for author identiﬁcation. In Proceedings of
the 2018 World Wide Web Conference, pp. 709–718. International World Wide Web
Conferences Steering Committee.

19

