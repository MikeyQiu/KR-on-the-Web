DeepPath: A Reinforcement Learning Method for
Knowledge Graph Reasoning

Wenhan Xiong and Thien Hoang and William Yang Wang
Department of Computer Science
University of California, Santa Barbara
Santa Barbara, CA 93106 USA
{xwhan,william}@cs.ucsb.edu, thienhoang@umail.ucsb.edu

8
1
0
2
 
l
u
J
 
7
 
 
]
L
C
.
s
c
[
 
 
3
v
0
9
6
6
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

We study the problem of learning to reason
in large scale knowledge graphs (KGs).
More speciﬁcally, we describe a novel re-
inforcement learning framework for learn-
ing multi-hop relational paths: we use a
policy-based agent with continuous states
based on knowledge graph embeddings,
which reasons in a KG vector space by
sampling the most promising relation to
extend its path. In contrast to prior work,
our approach includes a reward function
that takes the accuracy, diversity, and ef-
ﬁciency into consideration. Experimen-
tally, we show that our proposed method
outperforms a path-ranking based algo-
rithm and knowledge graph embedding
methods on Freebase and Never-Ending
Language Learning datasets.1

1

Introduction

Deep neural networks for acoustic modeling in
speech recognitionIn recent years, deep learn-
ing techniques have obtained many state-of-the-
art results in various classiﬁcation and recognition
problems (Krizhevsky et al., 2012; Hinton et al.,
2012; Kim, 2014). However, complex natural lan-
guage processing problems often require multi-
ple inter-related decisions, and empowering deep
learning models with the ability of learning to rea-
son is still a challenging issue. To handle com-
plex queries where there are no obvious answers,
intelligent machines must be able to reason with
existing resources, and learn to infer an unknown
answer.

More speciﬁcally, we situate our study in the
context of multi-hop reasoning, which is the task

1Code and the NELL dataset are available at https://

github.com/xwhan/DeepPath.

of learning explicit inference formulas, given a
large KG. For example, if the KG includes the
beliefs such as Neymar plays for Barcelona, and
Barcelona are in the La Liga league, then ma-
chines should be able to learn the following for-
mula: playerPlaysForTeam(P,T) ∧ teamPlaysIn-
League(T,L) ⇒ playerPlaysInLeague(P,L). In the
testing time, by plugging in the learned formulas,
the system should be able to automatically infer
the missing link between a pair of entities. This
kind of reasoning machine will potentially serve
as an essential components of complex QA sys-
tems.

In recent years, the Path-Ranking Algorithm
(PRA) (Lao et al., 2010, 2011a) emerges as a
promising method for learning inference paths in
large KGs. PRA uses a random-walk with restarts
based inference mechanism to perform multiple
bounded depth-ﬁrst search processes to ﬁnd rela-
tional paths. Coupled with elastic-net based learn-
ing, PRA then picks more plausible paths using
supervised learning. However, PRA operates in
a fully discrete space, which makes it difﬁcult to
evaluate and compare similar entities and relations
in a KG.

In this work, we propose a novel approach
for controllable multi-hop reasoning: we frame
the path learning process as reinforcement learn-
ing (RL). In contrast to PRA, we use translation-
based knowledge based embedding method (Bor-
des et al., 2013) to encode the continuous state of
our RL agent, which reasons in the vector space
environment of the knowledge graph. The agent
takes incremental steps by sampling a relation to
extend its path. To better guide the RL agent for
learning relational paths, we use policy gradient
training (Mnih et al., 2015) with a novel reward
function that jointly encourages accuracy, diver-
sity, and efﬁciency. Empirically, we show that our
method outperforms PRA and embedding based

methods on a Freebase and a Never-Ending Lan-
guage Learning (Carlson et al., 2010a) dataset.
Our contributions are three-fold:

• We are the ﬁrst to consider reinforcement
learning (RL) methods for learning relational
paths in knowledge graphs;

• Our learning method uses a complex reward
function that considers accuracy, efﬁciency,
and path diversity simultaneously, offering
better control and more ﬂexibility in the path-
ﬁnding process;

• We show that our method can scale up to
large scale knowledge graphs, outperform-
ing PRA and KG embedding methods in two
tasks.

In the next section, we outline related work in
path-ﬁnding and embedding methods in KGs. We
describe the proposed method in Section 3. We
show experimental results in Section 4. Finally,
we conclude in Section 5.

2 Related Work

The Path-Ranking Algorithm (PRA) method (Lao
et al., 2011b) is a primary path-ﬁnding approach
that uses random walk with restart strategies for
multi-hop reasoning. Gardner et al. (2013; 2014)
propose a modiﬁcation to PRA that computes fea-
ture similarity in the vector space. Wang and
Cohen (2015) introduce a recursive random walk
approach for integrating the background KG and
text—the method performs structure learning of
logic programs and information extraction from
text at the same time. A potential bottleneck for
random walk inference is that supernodes connect-
ing to large amount of formulas will create huge
fan-out areas that signiﬁcantly slow down the in-
ference and affect the accuracy.

Toutanova et al. (2015) provide a convolutional
neural network solution to multi-hop reasoning.
They build a CNN model based on lexicalized de-
pendency paths, which suffers from the error prop-
agation issue due to parse errors. Guu et al. (2015)
uses KG embeddings to answer path queries. Zeng
et al. (2014) described a CNN model for rela-
tional extraction, but it does not explicitly model
the relational paths. Neelakantan et al. (2015) pro-
pose a recurrent neural networks model for model-
ing relational paths in knowledge base completion
(KBC), but it trains too many separate models, and

therefore it does not scale. Note that many of the
recent KG reasoning methods (Neelakantan et al.,
2015; Das et al., 2017) still rely on ﬁrst learning
the PRA paths, which only operates in a discrete
space. Comparing to PRA, our method reasons
in a continuous space, and by incorporating vari-
ous criteria in the reward function, our reinforce-
ment learning (RL) framework has better control
and more ﬂexibility over the path-ﬁnding process.
Neural symbolic machine (Liang et al., 2016)
is a more recent work on KG reasoning, which
also applies reinforcement learning but has a dif-
ferent ﬂavor from our work. NSM learns to com-
pose programs that can ﬁnd answers to natural lan-
guage questions, while our RL model tries to add
new facts to knowledge graph (KG) by reasoning
on existing KG triples.
In order to get answers,
NSM learns to generate a sequence of actions that
can be combined as a executable program. The ac-
tion space in NSM is a set of predeﬁned tokens. In
our framework, the goal is to ﬁnd reasoning paths,
thus the action space is relation space in the KG. A
similar framework (Johnson et al., 2017) has also
been applied to visual reasoning tasks.

3 Methodology

In this section, we describe in detail our RL-based
framework for multi-hop relation reasoning. The
speciﬁc task of relation reasoning is to ﬁnd re-
liable predictive paths between entity pairs. We
formulate the path ﬁnding problem as a sequen-
tial decision making problem which can be solved
with a RL agent. We ﬁrst describe the environ-
ment and the policy-based RL agent. By interact-
ing with the environment designed around the KG,
the agent learns to pick the promising reasoning
paths. Then we describe the training procedure of
our RL model. After that, we describe an efﬁcient
path-constrained search algorithm for relation rea-
soning with the paths found by the RL agent.

3.1 Reinforcement Learning for Relation

Reasoning

The RL system consists of two parts (see Fig-
ure 1). The ﬁrst part is the external environment
E which speciﬁes the dynamics of the interaction
between the agent and the KG. This environment
is modeled as a Markov decision process (MDP).
A tuple < S, A, P, R > is deﬁned to represent
the MDP, where S is the continuous state space,
A = {a1, a2, ..., an} is the set of all available ac-

Figure 1: Overview of our RL model. Left: The KG environment E modeled by a MDP. The dotted arrows (partially) show the
existing relation links in the KG and the bold arrows show the reasoning paths found by the RL agent. −1 denotes the inverse
of an relation. Right: The structure of the policy network agent. At each step, by interacting with the environment, the agent
learns to pick a relation link to extend the reasoning paths.

tions, P(St+1 = s(cid:48)|St = s, At = a) is the transi-
tion probability matrix, and R(s, a) is the reward
function of every (s, a) pairs.

is

the system,

The second part of

the RL
agent,
represented as a policy network
πθ(s, a) = p(a|s; θ) which maps the state vec-
tor to a stochastic policy. The neural network
parameters θ are updated using stochastic gra-
dient descent. Compared to Deep Q Network
(DQN) (Mnih et al., 2013), policy-based RL
methods turn out to be more appropriate for our
knowledge graph scenario. One reason is that
for the path ﬁnding problem in KG, the action
space can be very large due to complexity of the
relation graph. This can lead to poor convergence
properties for DQN. Besides, instead of learning
a greedy policy which is common in value-based
methods like DQN, the policy network is able to
learn a stochastic policy which prevent the agent
from getting stuck at an intermediate state. Before
we describe the structure of our policy network,
we ﬁrst describe the components (actions, states,
rewards) of the RL environment.

Actions Given the entity pairs (es, et) with
relation r, we want the agent to ﬁnd the most
linking these entity pairs.
informative paths

Beginning with the source entity es, the agent use
the policy network to pick the most promising
relation to extend its path at each step until it
reaches the target entity et. To keep the output
dimension of the policy network consistent, the
action space is deﬁned as all the relations in the
KG.

States The entities and relations in a KG are
naturally discrete atomic symbols. Since exist-
ing practical KGs like Freebase (Bollacker et al.,
2008) and NELL (Carlson et al., 2010b) often have
huge amounts of triples.
It is impossible to di-
rectly model all the symbolic atoms in states. To
capture the semantic information of these sym-
bols, we use translation-based embeddings such as
TransE (Bordes et al., 2013) and TransH (Wang
et al., 2014) to represent the entities and relations.
These embeddings map all the symbols to a low-
dimensional vector space. In our framework, each
state captures the agent’s position in the KG. After
taking an action, the agent will move from one en-
tity to another. These two are linked by the action
(relation) just taken by the agent. The state vector
at step t is given as follows:

st = (et, etarget − et)

where et denotes the embeddings of the current

entity node and etarget denotes the embeddings of
the target entity. At the initial state, et = esource.
We do not incorporate the reasoning relation in
the state, because the embedding of the reasoning
relation remain constant during path ﬁnding,
which is not helpful in training. However, we
ﬁnd out that by training the RL agent using a set
of positive samples for one particular relation,
the agent can successfully discover the relation
semantics.

Rewards There are a few factors that contribute to
the quality of the paths found by the RL agent. To
encourage the agent to ﬁnd predictive paths, our
reward functions include the following scoring cri-
teria:
Global accuracy: For our environment settings,
the number of actions that can be taken by the
agent can be very large. In other words, there are
much more incorrect sequential decisions than the
correct ones. The number of these incorrect de-
cision sequences can increase exponentially with
the length of the path. In view of this challenge,
the ﬁrst reward function we add to the RL model
is deﬁned as follows:

(cid:40)

rGLOBAL =

if the path reaches etarget

+1,
−1, otherwise

the agent is given an ofﬂine positive reward +1 if
it reaches the target after a sequence of actions.
Path efﬁciency: For the relation reasoning task,
we observe that short paths tend to provide more
reliable reasoning evidence than longer paths.
Shorter chains of relations can also improve the
efﬁciency of the reasoning by limiting the length
of the RL’s interactions with the environment. The
efﬁciency reward is deﬁned as follows:

rEFFICIENCY =

1
length(p)

where path p is deﬁned as a sequence of relations
r1 → r2 → ... → rn.
Path diversity: We train the agent to ﬁnd paths us-
ing positive samples for each relation. These train-
ing sample (esource, etarget) have similar state rep-
resentations in the vector space. The agent tends
to ﬁnd paths with similar syntax and semantics.
These paths often contains redundant information
since some of them may be correlated. To encour-
age the agent to ﬁnd diverse paths, we deﬁne a di-
versity reward function using the cosine similarity

between the current path and the existing ones:

rDIVERSITY = −

cos(p, pi)

1
|F |

|F |
(cid:88)

i=1

i=1 ri represents the path embed-

where p = (cid:80)n
ding for the relation chain r1 → r2 → ... → rn.
Policy Network We use a fully-connected neu-
ral network to parameterize the policy function
π(s; θ) that maps the state vector s to a proba-
bility distribution over all possible actions. The
neural network consists of two hidden layers, each
followed by a rectiﬁer nonlinearity layer (ReLU).
The output layer is normalized using a softmax
function (see Figure 1).

3.2 Training Pipeline

In practice, one big challenge of KG reasoning is
that the relation set can be quite large. For a typ-
ical KG, the RL agent is often faced with hun-
dreds (thousands) of possible actions.
In other
words, the output layer of the policy network of-
ten has a large dimension. Due to the complexity
of the relation graph and the large action space,
if we directly train the RL model by trial and er-
rors, which is typical for RL algorithms, the RL
model will show very poor convergence proper-
ties. After a long-time training, the agents fails
to ﬁnd any valuable path. To tackle this prob-
lem, we start our training with a supervised policy
which is inspired by the imitation learning pipeline
used by AlphaGo (Silver et al., 2016). In the Go
game, the player is facing nearly 250 possible le-
gal moves at each step. Directly training the agent
to pick actions from the original action space can
be a difﬁcult task. AlphaGo ﬁrst train a supervised
policy network using experts moves. In our case,
the supervised policy is trained with a randomized
breadth-ﬁrst search (BFS).

Supervised Policy Learning For each relation,
we use a subset of all the positive samples (en-
tity pairs) to learn the supervised policy. For each
positive sample (esource, etarget), a two-side BFS
is conducted to ﬁnd same correct paths between
the entities. For each path p with a sequence of
relations r1 → r2 → ... → rn, we update the pa-
rameters θ to maximize the expected cumulative
reward using Monte-Carlo Policy Gradient (RE-

INFORCE) (Williams, 1992):

J(θ) = Ea∼π(a|s;θ)(

Rst,at)

(cid:88)

t

(cid:88)

(cid:88)

=

t

a∈A

π(a|st; θ)Rst,at

(1)

where J(θ) is the expected total rewards for one
episode. For supervised learning, we give a re-
ward of +1 for each step of a successful episode.
By plugging in the paths found by the BFS, the
approximated gradient used to update the policy
network is shown below:

∇θJ(θ) =

π(a|st; θ)∇θ log π(a|st; θ)

≈ ∇θ

log π(a = rt|st; θ)

(2)

(cid:88)

(cid:88)

t

a∈A
(cid:88)

t

where rt belongs to the path p.

However, the vanilla BFS is a biased search al-
gorithm which prefers short paths. When plug-
ging in these biased paths, it becomes difﬁcult
for the agent to ﬁnd longer paths which may po-
tentially be useful. We want the paths to be
controlled only by the deﬁned reward functions.
To prevent the biased search, we adopt a sim-
ple trick to add some random mechanisms to the
BFS. Instead of directly searching the path be-
tween esource and etarget, we randomly pick a in-
termediate node einter and then conduct two BFS
between (esource, einter) and (einter, etarget). The
concatenated paths are used to train the agent. The
supervised learning saves the agent great efforts
learning from failed actions. With the learned ex-
perience, we then train the agent to ﬁnd desirable
paths.
Retraining with Rewards To ﬁnd the reasoning
paths controlled by the reward functions, we use
reward functions to retrain the supervised policy
network. For each relation, the reasoning with one
entity pair is treated as one episode. Starting with
the source node esource, the agent picks a relation
according to the stochastic policy π(a|s), which is
a probability distribution over all relations, to ex-
tend its reasoning path. This relation link may lead
to a new entity, or it may lead to nothing. These
failed steps will cause the agent to receive negative
rewards. The agent will stay at the same state af-
ter these failed steps. Since the agent is following
a stochastic policy, the agent will not get stuck by
repeating a wrong step. To improve the training ef-
ﬁciency, we limit the episode length with an upper

Algorithm 1: Retraining Procedure with re-
ward functions
1 Restore parameters θ from supervised policy;
2 for episode ← 1 to N do

Initialize state vector st ← s0
Initialize episode length steps ← 0
while num steps < max length do

Randomly sample action a ∼ π(a|st)
Observe reward Rt, next state st+1
// if the step fails
if Rt = −1 then

Save < st, a > to Mneg

if success or steps = max length
then

break

Increment num steps

// penalize failed steps
Update θ using
g ∝ ∇θ
Mneg
if success then

(cid:80)

log π(a = rt|st; θ)(−1)

Rtotal ← λ1rGLOBAL + λ2rEFFICIENCY +
λ3rDIVERSITY
Update θ using
g ∝ ∇θ

t log π(a = rt|st; θ)Rtotal

(cid:80)

3

4

5

6

7

8

9

10

11

12

13

14

15

bound max length. The episode ends if the agent
fails to reach the target entity within max length
steps. After each episode, the policy network is
updated using the following gradient:

∇θJ(θ) = ∇θ

log π(a = rt|st; θ)Rtotal

(3)

(cid:88)

t

where Rtotal is the linear combination of the de-
ﬁned reward functions. The detail of the retrain
process is shown in Algorithm 1. In practice, θ is
updated using the Adam Optimizer (Kingma and
Ba, 2014) with L2 regularization.

3.3 Bi-directional Path-constrained Search

Given an entity pair, the reasoning paths learned
by the RL agent can be used as logical formulas
to predict the relation link. Each formula is veri-
ﬁed using a bi-directional search. In a typical KG,
one entity node can be linked to a large number
of neighbors with the same relation link. A sim-
ple example is the relation personNationality−1,
which denotes the inverse of personNationality.
Following this link, the entity United States can
If the for-
reach numerous neighboring entities.

Algorithm 2: Bi-directional search for path
veriﬁcation
1 Given a reasoning path

p : r1 → r2 → ... → rn
2 for (ei, ej) in test set D do
start ← 0; end ← n
3
lef t ← ∅; right ← ∅
while start < end do

5

4

lef tEx ← ∅; rightEx ← ∅
if len(left) < len(right) then

Extend path on the left side
Add connected nodes to lef tEx
lef t ← lef tEx

else

Extend path on the right side
Add connected nodes to rightEx
right ← rightEx

if lef t ∩ right (cid:54)= ∅ then

return True

else

return False

6

7

8

9

10

11

12

13

14

15

16

17

18

mula consists of such links, the number of inter-
mediate entities can exponentially increase as we
follow the reasoning formula. However, we ob-
serve that for these formulas, if we verify the for-
mula from the inverse direction. The number of in-
termediate nodes can be tremendously decreased.
Algorithm 2 shows a detailed description of the
proposed bi-directional search.

4 Experiments

To evaluate the reasoning formulas found by our
RL agent, we explore two standard KG reason-
link prediction (predicting target en-
ing tasks:
tities) and fact prediction (predicting whether an
unknown fact holds or not). We compare our
method with both path-based methods and embed-
ding based methods. After that, we further analyze
the reasoning paths found by our RL agent. These
highly predictive paths validate the effectiveness
of the reward functions. Finally, we conduct a ex-
periment to investigate the effect of the supervised
learning procedure.

4.1 Dataset and Settings

Table 1 shows the statistics of the two datasets
we conduct our experiments on. Both of them

Dataset
FB15K-237
NELL-995

# Ent.
14,505
75,492

# R.
237
200

# Triples
310,116
154.213

# Tasks
20
12

Table 1: Statistics of the Datasets. # Ent. denotes the number
of unique entities and # R. denotes the number of relations

are subsets of larger datasets. The triples in
FB15K-237 (Toutanova et al., 2015) are sampled
from FB15K (Bordes et al., 2013) with redun-
dant relations removed. We perform the reasoning
tasks on 20 relations which have enough reason-
ing paths. These tasks consists of relations from
different domains like Sports, People, Locations,
Film, etc. Besides, we present a new NELL sub-
set that is suitable for multi-hop reasoning from
the 995th iteration of the NELL system. We ﬁrst
remove the triples with relation generalizations or
haswikipediaurl. These two relations appear more
than 2M times in the NELL dataset, but they have
no reasoning values. After this step, we only se-
lect the triples with Top-200 relations. To facilitate
path ﬁnding, we also add the inverse triples. For
each triple (h, r, t), we append (t, r−1, h) to the
datasets. With these inverse triples, the agent is
able to step backward in the KG.

i

For each reasoning task ri, we remove all the
triples with ri or r−1
from the KG. These removed
triples are split into train and test samples. For
the link prediction task, each h in the test triples
{(h, r, t)} is considered as one query. A set of
candidate target entities are ranked using different
methods. For fact prediction, the true test triples
are ranked with some generated false triples.

4.2 Baselines and Implementation Details

Most KG reasoning methods are based on either
path formulas or KG embeddings. we explore
methods from both of these two classes in our ex-
periments. For path based methods, we compare
our RL model with the PRA (Lao et al., 2011a)
algorithm, which has been used in a couple of rea-
soning methods (Gardner et al., 2013; Neelakan-
tan et al., 2015). PRA is a data-driven algorithm
using random walks (RW) to ﬁnd paths and obtain
path features. For embedding based methods, we
evaluate several state-of-the-art embeddings de-
signed for knowledge base completion, such as
TransE (Bordes et al., 2013), TransH (Wang et al.,
2014), TransR (Lin et al., 2015) and TransD (Ji
et al., 2015) .

The implementation of PRA is based on the

Tasks

RL

TransE

TransR

Tasks

PRA

RL

TransE

TransR

FB15K-237

0.955
0.531
0.823
0.441
0.457
0.670
0.969
0.783
0.309
0.514

0.896
0.403
0.641
0.386
0.563
0.642
0.804
0.554
0.390
0.361

PRA

0.987
0.441
0.846
0.349
0.601
0.663
0.960
0.829
0.281
0.426

teamSports
birthPlace
personNationality
ﬁlmDirector
ﬁlmWrittenBy
ﬁlmLanguage
tvLanguage
capitalOf
organizationFounded
musicianOrigin
...

0.784
0.417
0.720
0.399
0.605
0.641
0.906
0.493
0.339
0.379

athletePlaysForTeam 0.547
athletePlaysInLeague
0.841
athleteHomeStadium 0.859
0.474
0.791
0.811
0.681
0.668
0.700
0.599

athletePlaysSport
teamPlaySports
orgHeadquaterCity
worksFor
bornLocation
personLeadsOrg
orgHiredPerson
...

NELL-995

0.750
0.960
0.890
0.957
0.738
0.790
0.711
0.757
0.795
0.742

0.627
0.773
0.718
0.876
0.761
0.620
0.677
0.712
0.751
0.719

0.673
0.912
0.722
0.963
0.814
0.657
0.692
0.812
0.772
0.737

Overall

0.541

0.572

0.532

0.540

0.675

0.796

0.737

0.789

Table 2: Link prediction results (MAP) on two datasets.

code released by (Lao et al., 2011a). We use the
TopK negative mode to generate negative samples
for both train and test samples. For each pos-
itive samples, there are approximately 10 corre-
sponding negative samples. Each negative sample
is generated by replacing the true target entity t
with a faked one t(cid:48)
in each triple (h, r, t). These
positive and negative test pairs generated by PRA
make up the test set for all methods evaluated in
this paper. For TransE,R,H,D, we learn a separate
embedding matrix for each reasoning task using
the positive training entity pairs. All these embed-
dings are trained for 1,000 epochs. 2

Our RL model make use of TransE to get the
continuous representation of the entities and rela-
tions. We use the same dimension as TransE, R
to embed the entities. Speciﬁcally, the state vec-
tor we use has a dimension of 200, which is also
the input size of the policy network. To reason
using the path formulas, we adopt a similar lin-
ear regression approach as in PRA to re-rank the
paths. However, instead of using the random walk
probabilities as path features, which can be com-
putationally expensive, we simply use binary path
features obtained by the bi-directional search. We
observe that with only a few mined path formulas,
our method can achieve better results than PRA’s
data-driven approach.

4.3 Results

4.3.1 Quantitative Results

Link Prediction This task is to rank the target en-
tities given a query entity. Table 2 shows the mean
average precision (MAP) results on two datasets.

2The implementation we used can be found at https:

//github.com/thunlp/Fast-TransX

Fact Prediction Results

Methods FB15K-237 NELL-995

RL
TransE
TransH
TransR
TransD

0.311
0.277
0.309
0.302
0.303

0.493
0.383
0.389
0.406
0.413

Table 3: Fact prediction results (MAP) on two datasets.

# of Reasoning Paths

Tasks

worksFor
teamPlaySports
teamPlaysInLeague
athletehomestadium
organizationHiredPerson
...
Average #

PRA

247
113
69
37
244

RL

25
27
21
11
9

137.2

20.3

Table 4: Number of reasoning paths used by PRA and our RL
model. RL achieved better MAP with a more compact set of
learned paths.

Since path-based methods generally work better
than embedding methods for this task, we do not
include the other two embedding baselines in this
table. Instead, we spare the room to show the de-
tailed results on each relation reasoning task.

For the overall MAP shown in the last row of the
table, our approach signiﬁcantly outperforms both
the path-based method and embedding methods on
two datasets, which validates the strong reasoning
ability of our RL model. For most relations, since
the embedding methods fail to use the path infor-

Figure 2: The distribution of paths lengths on two datasets

Figure 3: The success ratio (succ10) during training. Task:
athletePlaysForTeam.3

mation in the KG, they generally perform worse
than our RL model or PRA. However, when there
are not enough paths between entities, our model
and PRA can give poor results. For example,
for the relation ﬁlmWrittenBy, our RL model only
ﬁnds 4 unique reasoning paths, which means there
is actually not enough reasoning evidence existing
in the KG. Another observation is that we always
get better performance on the NELL dataset. By
analyzing the paths found from the KGs, we be-
lieve the potential reason is that the NELL dataset
has more short paths than FB15K-237 and some
of them are simply synonyms of the reasoning re-
lations.
Fact Prediction Instead of ranking the target en-
tities, this task directly ranks all the positive and
negative samples for a particular relation. The
PRA is not included as a baseline here, since the
PRA code only gives a target entity ranking for
each query node instead of a ranking of all triples.
Table 3 shows the overall results of all the meth-
ods. Our RL model gets even better results on this
task. We also observe that the RL model beats all
the embedding baselines on most reasoning tasks.

4.3.2 Qualitative Analysis of Reasoning Paths

To analyze the properties of reasoning paths, we
show a few reasoning paths found by the agent
in Table 5. To illustrate the effect of the efﬁ-
ciency reward function, we show the path length
distributions in Figure 2. To interpret these paths,
take the personNationality relation for example,
the ﬁrst reasoning path indicates that if we know
facts placeOfBirth(x,y) and locationContains(z,y)
then it is highly possible that person x has nation-
ality z. These short but predictive paths indicate
the effectiveness of the RL model. Another im-
portant observation is that our model use much

fewer reasoning paths than PRA, which indicates
that our model can actually extract the most reli-
able reasoning evidence from KG. Table 4 shows
some comparisons about the number of reasoning
paths. We can see that, with the pre-deﬁned re-
ward functions, the RL agent is capable of picking
the strong ones and ﬁlter out similar or irrelevant
ones.

4.3.3 Effect of Supervised Learning

As mentioned in Section 3.2, one major challenge
for applying RL to KG reasoning is the large ac-
tion space. We address this issue by applying
supervised learning before the reward retraining
step. To show the effect of the supervised train-
ing, we evaluate the agent’s success ratio of reach-
ing the target within 10 steps (succ10) after differ-
ent number of training episodes. For each train-
ing episode, one pair of entities (esource, etarget)
in the train set is used to ﬁnd paths. All the cor-
rect paths linking the entities will get a +1 global
reward. We then plug in some true paths for train-
ing. The succ10 is calculated on a held-out test set
that consists of 100 entity pairs. For the NELL-
995 dataset, since we have 200 unique relations,
the dimension of the action space will be 400 af-
ter we add the backward actions. This means that
random walks will get very low succ10 since there
may be nearly 40010 invalid paths. Figure 3 shows
the succ10 during training. We see that even the
agent has not seen the entity before, it can actually
pick the promising relation to extend its path. This
also validates the effectiveness of our state repre-
sentations.

3The conﬁdence band is generated using 50 different runs.

Relation

Reasoning Path

ﬁlmCountry

personNationality

tvProgramLanguage

personBornInLocation

athletePlaysForTeam

ﬁlmReleaseRegion
featureFilmLocation → locationContains−1
actorFilm−1 → personNationality
placeOfBirth→ locationContains−1
peoplePlaceLived → locationContains−1
peopleMarriage → locationOfCeremony → locationContains−1

tvCountryOfOrigin → countryOfﬁcialLanguage
tvCountryOfOrigin → ﬁlmReleaseRegion−1 → ﬁlmLanguage
tvCastActor → ﬁlmLanguage

personBornInCity
graduatedUniversity → graduatedSchool−1 → personBornInCity
personBornInCity → atLocation−1 → atLocation
athleteHomeStadium → teamHomeStadium−1
athletePlaysSport → teamPlaysSport−1
athleteLedSportsTeam

personLeadsOrganization

worksFor
organizationTerminatedPerson−1
mutualProxyFor−1

Table 5: Example reasoning paths found by our RL model. The ﬁrst three relations come from the FB15K-237 dataset. The
others are from NELL-995. Inverses of existing relations are denoted by −1.

5 Conclusion and Future Work

Acknowledgments

In this paper, we propose a reinforcement learn-
ing framework to improve the performance of re-
lation reasoning in KGs. Speciﬁcally, we train a
RL agent to ﬁnd reasoning paths in the knowledge
base. Unlike previous path ﬁnding models that are
based on random walks, the RL model allows us
to control the properties of the found paths. These
effective paths can also be used as an alternative to
PRA in many path-based reasoning methods. For
two standard reasoning tasks, using the RL paths
as reasoning formulas, our approach generally out-
performs two classes of baselines.

For

future studies, we plan to investigate
the possibility of incorporating adversarial learn-
ing (Goodfellow et al., 2014) to give better re-
wards than the human-deﬁned reward functions
Instead of designing rewards
used in this work.
according to path characteristics, a discriminative
model can be trained to give rewards. Also, to ad-
dress the problematic scenario when the KG does
not have enough reasoning paths, we are interested
in applying our RL framework to joint reasoning
with KG triples and text mentions.

We gratefully acknowledge
support of
NVIDIA Corporation with the donation of one Ti-
tan X Pascal GPU used for this research.

the

References

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. ACM.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems, pages 2787–2795.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010a. Toward an architecture for never-
ending language learning. In AAAI.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010b. Toward an architecture for never-
In Proceedings of the
ending language learning.
Twenty-Fourth Conference on Artiﬁcial Intelligence
(AAAI 2010).

Rajarshi Das, Arvind Neelakantan, David Belanger,
and Andrew McCallum. 2017. Chains of reasoning
over entities, relations, and text using recurrent neu-
ral networks. EACL.

Ni Lao, Jun Zhu, Xinwang Liu, Yandong Liu, and
William W Cohen. 2010. Efﬁcient relational learn-
ing with hidden variable detection. In NIPS, pages
1234–1242.

Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
Improving learning
and Tom M Mitchell. 2013.
and inference in a large knowledge-base using latent
syntactic cues. In EMNLP, pages 833–838.

Matt Gardner, Partha Pratim Talukdar, Jayant Krishna-
murthy, and Tom Mitchell. 2014. Incorporating vec-
tor space similarity in random walk inference over
knowledge bases.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in Neural Information
versarial nets.
Processing Systems, pages 2672–2680.

Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space.
In
EMNLP.

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al. 2012. Deep neural networks for
acoustic modeling in speech recognition: The shared
views of four research groups. IEEE Signal Process-
ing Magazine, 29(6):82–97.

Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun
Zhao. 2015. Knowledge graph embedding via dy-
namic mapping matrix. In ACL (1), pages 687–696.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zit-
Inferring and exe-
nick, and Ross Girshick. 2017.
cuting programs for visual reasoning. arXiv preprint
arXiv:1705.03633.

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012.
Imagenet classiﬁcation with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.

Ni Lao, Tom Mitchell, and William W Cohen. 2011a.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 529–539. Association for Computa-
tional Linguistics.

Ni Lao, Tom M. Mitchell, and William W. Cohen.
2011b. Random walk inference and learning in a
large scale knowledge base. In EMNLP, pages 529–
539. ACL.

Chen Liang, Jonathan Berant, Quoc Le, Kenneth D
Neural symbolic
Forbus, and Ni Lao. 2016.
machines: Learning semantic parsers on free-
arXiv preprint
base with weak supervision.
arXiv:1611.00020.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In AAAI,
pages 2181–2187.

Volodymyr Mnih, Koray Kavukcuoglu, David Sil-
ver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. 2013. Playing atari
with deep reinforcement learning. arXiv preprint
arXiv:1312.5602.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. Na-
ture, 518(7540):529–533.

Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional vector space mod-
els for knowledge base completion. arXiv preprint
arXiv:1504.06662.

David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George Van Den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, et al. 2016. Mastering
the game of go with deep neural networks and tree
search. Nature, 529(7587):484–489.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases. In EMNLP, volume 15, pages
1499–1509. Citeseer.

William Yang Wang and William W Cohen. 2015.
Joint information extraction and reasoning: A scal-
able statistical relational learning approach. In ACL.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In AAAI, pages 1112–1119.
Citeseer.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
Jun Zhao, et al. 2014. Relation classiﬁcation via
In COLING,
convolutional deep neural network.
pages 2335–2344.

DeepPath: A Reinforcement Learning Method for
Knowledge Graph Reasoning

Wenhan Xiong and Thien Hoang and William Yang Wang
Department of Computer Science
University of California, Santa Barbara
Santa Barbara, CA 93106 USA
{xwhan,william}@cs.ucsb.edu, thienhoang@umail.ucsb.edu

8
1
0
2
 
l
u
J
 
7
 
 
]
L
C
.
s
c
[
 
 
3
v
0
9
6
6
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

We study the problem of learning to reason
in large scale knowledge graphs (KGs).
More speciﬁcally, we describe a novel re-
inforcement learning framework for learn-
ing multi-hop relational paths: we use a
policy-based agent with continuous states
based on knowledge graph embeddings,
which reasons in a KG vector space by
sampling the most promising relation to
extend its path. In contrast to prior work,
our approach includes a reward function
that takes the accuracy, diversity, and ef-
ﬁciency into consideration. Experimen-
tally, we show that our proposed method
outperforms a path-ranking based algo-
rithm and knowledge graph embedding
methods on Freebase and Never-Ending
Language Learning datasets.1

1

Introduction

Deep neural networks for acoustic modeling in
speech recognitionIn recent years, deep learn-
ing techniques have obtained many state-of-the-
art results in various classiﬁcation and recognition
problems (Krizhevsky et al., 2012; Hinton et al.,
2012; Kim, 2014). However, complex natural lan-
guage processing problems often require multi-
ple inter-related decisions, and empowering deep
learning models with the ability of learning to rea-
son is still a challenging issue. To handle com-
plex queries where there are no obvious answers,
intelligent machines must be able to reason with
existing resources, and learn to infer an unknown
answer.

More speciﬁcally, we situate our study in the
context of multi-hop reasoning, which is the task

1Code and the NELL dataset are available at https://

github.com/xwhan/DeepPath.

of learning explicit inference formulas, given a
large KG. For example, if the KG includes the
beliefs such as Neymar plays for Barcelona, and
Barcelona are in the La Liga league, then ma-
chines should be able to learn the following for-
mula: playerPlaysForTeam(P,T) ∧ teamPlaysIn-
League(T,L) ⇒ playerPlaysInLeague(P,L). In the
testing time, by plugging in the learned formulas,
the system should be able to automatically infer
the missing link between a pair of entities. This
kind of reasoning machine will potentially serve
as an essential components of complex QA sys-
tems.

In recent years, the Path-Ranking Algorithm
(PRA) (Lao et al., 2010, 2011a) emerges as a
promising method for learning inference paths in
large KGs. PRA uses a random-walk with restarts
based inference mechanism to perform multiple
bounded depth-ﬁrst search processes to ﬁnd rela-
tional paths. Coupled with elastic-net based learn-
ing, PRA then picks more plausible paths using
supervised learning. However, PRA operates in
a fully discrete space, which makes it difﬁcult to
evaluate and compare similar entities and relations
in a KG.

In this work, we propose a novel approach
for controllable multi-hop reasoning: we frame
the path learning process as reinforcement learn-
ing (RL). In contrast to PRA, we use translation-
based knowledge based embedding method (Bor-
des et al., 2013) to encode the continuous state of
our RL agent, which reasons in the vector space
environment of the knowledge graph. The agent
takes incremental steps by sampling a relation to
extend its path. To better guide the RL agent for
learning relational paths, we use policy gradient
training (Mnih et al., 2015) with a novel reward
function that jointly encourages accuracy, diver-
sity, and efﬁciency. Empirically, we show that our
method outperforms PRA and embedding based

methods on a Freebase and a Never-Ending Lan-
guage Learning (Carlson et al., 2010a) dataset.
Our contributions are three-fold:

• We are the ﬁrst to consider reinforcement
learning (RL) methods for learning relational
paths in knowledge graphs;

• Our learning method uses a complex reward
function that considers accuracy, efﬁciency,
and path diversity simultaneously, offering
better control and more ﬂexibility in the path-
ﬁnding process;

• We show that our method can scale up to
large scale knowledge graphs, outperform-
ing PRA and KG embedding methods in two
tasks.

In the next section, we outline related work in
path-ﬁnding and embedding methods in KGs. We
describe the proposed method in Section 3. We
show experimental results in Section 4. Finally,
we conclude in Section 5.

2 Related Work

The Path-Ranking Algorithm (PRA) method (Lao
et al., 2011b) is a primary path-ﬁnding approach
that uses random walk with restart strategies for
multi-hop reasoning. Gardner et al. (2013; 2014)
propose a modiﬁcation to PRA that computes fea-
ture similarity in the vector space. Wang and
Cohen (2015) introduce a recursive random walk
approach for integrating the background KG and
text—the method performs structure learning of
logic programs and information extraction from
text at the same time. A potential bottleneck for
random walk inference is that supernodes connect-
ing to large amount of formulas will create huge
fan-out areas that signiﬁcantly slow down the in-
ference and affect the accuracy.

Toutanova et al. (2015) provide a convolutional
neural network solution to multi-hop reasoning.
They build a CNN model based on lexicalized de-
pendency paths, which suffers from the error prop-
agation issue due to parse errors. Guu et al. (2015)
uses KG embeddings to answer path queries. Zeng
et al. (2014) described a CNN model for rela-
tional extraction, but it does not explicitly model
the relational paths. Neelakantan et al. (2015) pro-
pose a recurrent neural networks model for model-
ing relational paths in knowledge base completion
(KBC), but it trains too many separate models, and

therefore it does not scale. Note that many of the
recent KG reasoning methods (Neelakantan et al.,
2015; Das et al., 2017) still rely on ﬁrst learning
the PRA paths, which only operates in a discrete
space. Comparing to PRA, our method reasons
in a continuous space, and by incorporating vari-
ous criteria in the reward function, our reinforce-
ment learning (RL) framework has better control
and more ﬂexibility over the path-ﬁnding process.
Neural symbolic machine (Liang et al., 2016)
is a more recent work on KG reasoning, which
also applies reinforcement learning but has a dif-
ferent ﬂavor from our work. NSM learns to com-
pose programs that can ﬁnd answers to natural lan-
guage questions, while our RL model tries to add
new facts to knowledge graph (KG) by reasoning
on existing KG triples.
In order to get answers,
NSM learns to generate a sequence of actions that
can be combined as a executable program. The ac-
tion space in NSM is a set of predeﬁned tokens. In
our framework, the goal is to ﬁnd reasoning paths,
thus the action space is relation space in the KG. A
similar framework (Johnson et al., 2017) has also
been applied to visual reasoning tasks.

3 Methodology

In this section, we describe in detail our RL-based
framework for multi-hop relation reasoning. The
speciﬁc task of relation reasoning is to ﬁnd re-
liable predictive paths between entity pairs. We
formulate the path ﬁnding problem as a sequen-
tial decision making problem which can be solved
with a RL agent. We ﬁrst describe the environ-
ment and the policy-based RL agent. By interact-
ing with the environment designed around the KG,
the agent learns to pick the promising reasoning
paths. Then we describe the training procedure of
our RL model. After that, we describe an efﬁcient
path-constrained search algorithm for relation rea-
soning with the paths found by the RL agent.

3.1 Reinforcement Learning for Relation

Reasoning

The RL system consists of two parts (see Fig-
ure 1). The ﬁrst part is the external environment
E which speciﬁes the dynamics of the interaction
between the agent and the KG. This environment
is modeled as a Markov decision process (MDP).
A tuple < S, A, P, R > is deﬁned to represent
the MDP, where S is the continuous state space,
A = {a1, a2, ..., an} is the set of all available ac-

Figure 1: Overview of our RL model. Left: The KG environment E modeled by a MDP. The dotted arrows (partially) show the
existing relation links in the KG and the bold arrows show the reasoning paths found by the RL agent. −1 denotes the inverse
of an relation. Right: The structure of the policy network agent. At each step, by interacting with the environment, the agent
learns to pick a relation link to extend the reasoning paths.

tions, P(St+1 = s(cid:48)|St = s, At = a) is the transi-
tion probability matrix, and R(s, a) is the reward
function of every (s, a) pairs.

is

the system,

The second part of

the RL
agent,
represented as a policy network
πθ(s, a) = p(a|s; θ) which maps the state vec-
tor to a stochastic policy. The neural network
parameters θ are updated using stochastic gra-
dient descent. Compared to Deep Q Network
(DQN) (Mnih et al., 2013), policy-based RL
methods turn out to be more appropriate for our
knowledge graph scenario. One reason is that
for the path ﬁnding problem in KG, the action
space can be very large due to complexity of the
relation graph. This can lead to poor convergence
properties for DQN. Besides, instead of learning
a greedy policy which is common in value-based
methods like DQN, the policy network is able to
learn a stochastic policy which prevent the agent
from getting stuck at an intermediate state. Before
we describe the structure of our policy network,
we ﬁrst describe the components (actions, states,
rewards) of the RL environment.

Actions Given the entity pairs (es, et) with
relation r, we want the agent to ﬁnd the most
linking these entity pairs.
informative paths

Beginning with the source entity es, the agent use
the policy network to pick the most promising
relation to extend its path at each step until it
reaches the target entity et. To keep the output
dimension of the policy network consistent, the
action space is deﬁned as all the relations in the
KG.

States The entities and relations in a KG are
naturally discrete atomic symbols. Since exist-
ing practical KGs like Freebase (Bollacker et al.,
2008) and NELL (Carlson et al., 2010b) often have
huge amounts of triples.
It is impossible to di-
rectly model all the symbolic atoms in states. To
capture the semantic information of these sym-
bols, we use translation-based embeddings such as
TransE (Bordes et al., 2013) and TransH (Wang
et al., 2014) to represent the entities and relations.
These embeddings map all the symbols to a low-
dimensional vector space. In our framework, each
state captures the agent’s position in the KG. After
taking an action, the agent will move from one en-
tity to another. These two are linked by the action
(relation) just taken by the agent. The state vector
at step t is given as follows:

st = (et, etarget − et)

where et denotes the embeddings of the current

entity node and etarget denotes the embeddings of
the target entity. At the initial state, et = esource.
We do not incorporate the reasoning relation in
the state, because the embedding of the reasoning
relation remain constant during path ﬁnding,
which is not helpful in training. However, we
ﬁnd out that by training the RL agent using a set
of positive samples for one particular relation,
the agent can successfully discover the relation
semantics.

Rewards There are a few factors that contribute to
the quality of the paths found by the RL agent. To
encourage the agent to ﬁnd predictive paths, our
reward functions include the following scoring cri-
teria:
Global accuracy: For our environment settings,
the number of actions that can be taken by the
agent can be very large. In other words, there are
much more incorrect sequential decisions than the
correct ones. The number of these incorrect de-
cision sequences can increase exponentially with
the length of the path. In view of this challenge,
the ﬁrst reward function we add to the RL model
is deﬁned as follows:

(cid:40)

rGLOBAL =

if the path reaches etarget

+1,
−1, otherwise

the agent is given an ofﬂine positive reward +1 if
it reaches the target after a sequence of actions.
Path efﬁciency: For the relation reasoning task,
we observe that short paths tend to provide more
reliable reasoning evidence than longer paths.
Shorter chains of relations can also improve the
efﬁciency of the reasoning by limiting the length
of the RL’s interactions with the environment. The
efﬁciency reward is deﬁned as follows:

rEFFICIENCY =

1
length(p)

where path p is deﬁned as a sequence of relations
r1 → r2 → ... → rn.
Path diversity: We train the agent to ﬁnd paths us-
ing positive samples for each relation. These train-
ing sample (esource, etarget) have similar state rep-
resentations in the vector space. The agent tends
to ﬁnd paths with similar syntax and semantics.
These paths often contains redundant information
since some of them may be correlated. To encour-
age the agent to ﬁnd diverse paths, we deﬁne a di-
versity reward function using the cosine similarity

between the current path and the existing ones:

rDIVERSITY = −

cos(p, pi)

1
|F |

|F |
(cid:88)

i=1

i=1 ri represents the path embed-

where p = (cid:80)n
ding for the relation chain r1 → r2 → ... → rn.
Policy Network We use a fully-connected neu-
ral network to parameterize the policy function
π(s; θ) that maps the state vector s to a proba-
bility distribution over all possible actions. The
neural network consists of two hidden layers, each
followed by a rectiﬁer nonlinearity layer (ReLU).
The output layer is normalized using a softmax
function (see Figure 1).

3.2 Training Pipeline

In practice, one big challenge of KG reasoning is
that the relation set can be quite large. For a typ-
ical KG, the RL agent is often faced with hun-
dreds (thousands) of possible actions.
In other
words, the output layer of the policy network of-
ten has a large dimension. Due to the complexity
of the relation graph and the large action space,
if we directly train the RL model by trial and er-
rors, which is typical for RL algorithms, the RL
model will show very poor convergence proper-
ties. After a long-time training, the agents fails
to ﬁnd any valuable path. To tackle this prob-
lem, we start our training with a supervised policy
which is inspired by the imitation learning pipeline
used by AlphaGo (Silver et al., 2016). In the Go
game, the player is facing nearly 250 possible le-
gal moves at each step. Directly training the agent
to pick actions from the original action space can
be a difﬁcult task. AlphaGo ﬁrst train a supervised
policy network using experts moves. In our case,
the supervised policy is trained with a randomized
breadth-ﬁrst search (BFS).

Supervised Policy Learning For each relation,
we use a subset of all the positive samples (en-
tity pairs) to learn the supervised policy. For each
positive sample (esource, etarget), a two-side BFS
is conducted to ﬁnd same correct paths between
the entities. For each path p with a sequence of
relations r1 → r2 → ... → rn, we update the pa-
rameters θ to maximize the expected cumulative
reward using Monte-Carlo Policy Gradient (RE-

INFORCE) (Williams, 1992):

J(θ) = Ea∼π(a|s;θ)(

Rst,at)

(cid:88)

t

(cid:88)

(cid:88)

=

t

a∈A

π(a|st; θ)Rst,at

(1)

where J(θ) is the expected total rewards for one
episode. For supervised learning, we give a re-
ward of +1 for each step of a successful episode.
By plugging in the paths found by the BFS, the
approximated gradient used to update the policy
network is shown below:

∇θJ(θ) =

π(a|st; θ)∇θ log π(a|st; θ)

≈ ∇θ

log π(a = rt|st; θ)

(2)

(cid:88)

(cid:88)

t

a∈A
(cid:88)

t

where rt belongs to the path p.

However, the vanilla BFS is a biased search al-
gorithm which prefers short paths. When plug-
ging in these biased paths, it becomes difﬁcult
for the agent to ﬁnd longer paths which may po-
tentially be useful. We want the paths to be
controlled only by the deﬁned reward functions.
To prevent the biased search, we adopt a sim-
ple trick to add some random mechanisms to the
BFS. Instead of directly searching the path be-
tween esource and etarget, we randomly pick a in-
termediate node einter and then conduct two BFS
between (esource, einter) and (einter, etarget). The
concatenated paths are used to train the agent. The
supervised learning saves the agent great efforts
learning from failed actions. With the learned ex-
perience, we then train the agent to ﬁnd desirable
paths.
Retraining with Rewards To ﬁnd the reasoning
paths controlled by the reward functions, we use
reward functions to retrain the supervised policy
network. For each relation, the reasoning with one
entity pair is treated as one episode. Starting with
the source node esource, the agent picks a relation
according to the stochastic policy π(a|s), which is
a probability distribution over all relations, to ex-
tend its reasoning path. This relation link may lead
to a new entity, or it may lead to nothing. These
failed steps will cause the agent to receive negative
rewards. The agent will stay at the same state af-
ter these failed steps. Since the agent is following
a stochastic policy, the agent will not get stuck by
repeating a wrong step. To improve the training ef-
ﬁciency, we limit the episode length with an upper

Algorithm 1: Retraining Procedure with re-
ward functions
1 Restore parameters θ from supervised policy;
2 for episode ← 1 to N do

Initialize state vector st ← s0
Initialize episode length steps ← 0
while num steps < max length do

Randomly sample action a ∼ π(a|st)
Observe reward Rt, next state st+1
// if the step fails
if Rt = −1 then

Save < st, a > to Mneg

if success or steps = max length
then

break

Increment num steps

// penalize failed steps
Update θ using
g ∝ ∇θ
Mneg
if success then

(cid:80)

log π(a = rt|st; θ)(−1)

Rtotal ← λ1rGLOBAL + λ2rEFFICIENCY +
λ3rDIVERSITY
Update θ using
g ∝ ∇θ

t log π(a = rt|st; θ)Rtotal

(cid:80)

3

4

5

6

7

8

9

10

11

12

13

14

15

bound max length. The episode ends if the agent
fails to reach the target entity within max length
steps. After each episode, the policy network is
updated using the following gradient:

∇θJ(θ) = ∇θ

log π(a = rt|st; θ)Rtotal

(3)

(cid:88)

t

where Rtotal is the linear combination of the de-
ﬁned reward functions. The detail of the retrain
process is shown in Algorithm 1. In practice, θ is
updated using the Adam Optimizer (Kingma and
Ba, 2014) with L2 regularization.

3.3 Bi-directional Path-constrained Search

Given an entity pair, the reasoning paths learned
by the RL agent can be used as logical formulas
to predict the relation link. Each formula is veri-
ﬁed using a bi-directional search. In a typical KG,
one entity node can be linked to a large number
of neighbors with the same relation link. A sim-
ple example is the relation personNationality−1,
which denotes the inverse of personNationality.
Following this link, the entity United States can
If the for-
reach numerous neighboring entities.

Algorithm 2: Bi-directional search for path
veriﬁcation
1 Given a reasoning path

p : r1 → r2 → ... → rn
2 for (ei, ej) in test set D do
start ← 0; end ← n
3
lef t ← ∅; right ← ∅
while start < end do

5

4

lef tEx ← ∅; rightEx ← ∅
if len(left) < len(right) then

Extend path on the left side
Add connected nodes to lef tEx
lef t ← lef tEx

else

Extend path on the right side
Add connected nodes to rightEx
right ← rightEx

if lef t ∩ right (cid:54)= ∅ then

return True

else

return False

6

7

8

9

10

11

12

13

14

15

16

17

18

mula consists of such links, the number of inter-
mediate entities can exponentially increase as we
follow the reasoning formula. However, we ob-
serve that for these formulas, if we verify the for-
mula from the inverse direction. The number of in-
termediate nodes can be tremendously decreased.
Algorithm 2 shows a detailed description of the
proposed bi-directional search.

4 Experiments

To evaluate the reasoning formulas found by our
RL agent, we explore two standard KG reason-
link prediction (predicting target en-
ing tasks:
tities) and fact prediction (predicting whether an
unknown fact holds or not). We compare our
method with both path-based methods and embed-
ding based methods. After that, we further analyze
the reasoning paths found by our RL agent. These
highly predictive paths validate the effectiveness
of the reward functions. Finally, we conduct a ex-
periment to investigate the effect of the supervised
learning procedure.

4.1 Dataset and Settings

Table 1 shows the statistics of the two datasets
we conduct our experiments on. Both of them

Dataset
FB15K-237
NELL-995

# Ent.
14,505
75,492

# R.
237
200

# Triples
310,116
154.213

# Tasks
20
12

Table 1: Statistics of the Datasets. # Ent. denotes the number
of unique entities and # R. denotes the number of relations

are subsets of larger datasets. The triples in
FB15K-237 (Toutanova et al., 2015) are sampled
from FB15K (Bordes et al., 2013) with redun-
dant relations removed. We perform the reasoning
tasks on 20 relations which have enough reason-
ing paths. These tasks consists of relations from
different domains like Sports, People, Locations,
Film, etc. Besides, we present a new NELL sub-
set that is suitable for multi-hop reasoning from
the 995th iteration of the NELL system. We ﬁrst
remove the triples with relation generalizations or
haswikipediaurl. These two relations appear more
than 2M times in the NELL dataset, but they have
no reasoning values. After this step, we only se-
lect the triples with Top-200 relations. To facilitate
path ﬁnding, we also add the inverse triples. For
each triple (h, r, t), we append (t, r−1, h) to the
datasets. With these inverse triples, the agent is
able to step backward in the KG.

i

For each reasoning task ri, we remove all the
triples with ri or r−1
from the KG. These removed
triples are split into train and test samples. For
the link prediction task, each h in the test triples
{(h, r, t)} is considered as one query. A set of
candidate target entities are ranked using different
methods. For fact prediction, the true test triples
are ranked with some generated false triples.

4.2 Baselines and Implementation Details

Most KG reasoning methods are based on either
path formulas or KG embeddings. we explore
methods from both of these two classes in our ex-
periments. For path based methods, we compare
our RL model with the PRA (Lao et al., 2011a)
algorithm, which has been used in a couple of rea-
soning methods (Gardner et al., 2013; Neelakan-
tan et al., 2015). PRA is a data-driven algorithm
using random walks (RW) to ﬁnd paths and obtain
path features. For embedding based methods, we
evaluate several state-of-the-art embeddings de-
signed for knowledge base completion, such as
TransE (Bordes et al., 2013), TransH (Wang et al.,
2014), TransR (Lin et al., 2015) and TransD (Ji
et al., 2015) .

The implementation of PRA is based on the

Tasks

RL

TransE

TransR

Tasks

PRA

RL

TransE

TransR

FB15K-237

0.955
0.531
0.823
0.441
0.457
0.670
0.969
0.783
0.309
0.514

0.896
0.403
0.641
0.386
0.563
0.642
0.804
0.554
0.390
0.361

PRA

0.987
0.441
0.846
0.349
0.601
0.663
0.960
0.829
0.281
0.426

teamSports
birthPlace
personNationality
ﬁlmDirector
ﬁlmWrittenBy
ﬁlmLanguage
tvLanguage
capitalOf
organizationFounded
musicianOrigin
...

0.784
0.417
0.720
0.399
0.605
0.641
0.906
0.493
0.339
0.379

athletePlaysForTeam 0.547
athletePlaysInLeague
0.841
athleteHomeStadium 0.859
0.474
0.791
0.811
0.681
0.668
0.700
0.599

athletePlaysSport
teamPlaySports
orgHeadquaterCity
worksFor
bornLocation
personLeadsOrg
orgHiredPerson
...

NELL-995

0.750
0.960
0.890
0.957
0.738
0.790
0.711
0.757
0.795
0.742

0.627
0.773
0.718
0.876
0.761
0.620
0.677
0.712
0.751
0.719

0.673
0.912
0.722
0.963
0.814
0.657
0.692
0.812
0.772
0.737

Overall

0.541

0.572

0.532

0.540

0.675

0.796

0.737

0.789

Table 2: Link prediction results (MAP) on two datasets.

code released by (Lao et al., 2011a). We use the
TopK negative mode to generate negative samples
for both train and test samples. For each pos-
itive samples, there are approximately 10 corre-
sponding negative samples. Each negative sample
is generated by replacing the true target entity t
with a faked one t(cid:48)
in each triple (h, r, t). These
positive and negative test pairs generated by PRA
make up the test set for all methods evaluated in
this paper. For TransE,R,H,D, we learn a separate
embedding matrix for each reasoning task using
the positive training entity pairs. All these embed-
dings are trained for 1,000 epochs. 2

Our RL model make use of TransE to get the
continuous representation of the entities and rela-
tions. We use the same dimension as TransE, R
to embed the entities. Speciﬁcally, the state vec-
tor we use has a dimension of 200, which is also
the input size of the policy network. To reason
using the path formulas, we adopt a similar lin-
ear regression approach as in PRA to re-rank the
paths. However, instead of using the random walk
probabilities as path features, which can be com-
putationally expensive, we simply use binary path
features obtained by the bi-directional search. We
observe that with only a few mined path formulas,
our method can achieve better results than PRA’s
data-driven approach.

4.3 Results

4.3.1 Quantitative Results

Link Prediction This task is to rank the target en-
tities given a query entity. Table 2 shows the mean
average precision (MAP) results on two datasets.

2The implementation we used can be found at https:

//github.com/thunlp/Fast-TransX

Fact Prediction Results

Methods FB15K-237 NELL-995

RL
TransE
TransH
TransR
TransD

0.311
0.277
0.309
0.302
0.303

0.493
0.383
0.389
0.406
0.413

Table 3: Fact prediction results (MAP) on two datasets.

# of Reasoning Paths

Tasks

worksFor
teamPlaySports
teamPlaysInLeague
athletehomestadium
organizationHiredPerson
...
Average #

PRA

247
113
69
37
244

RL

25
27
21
11
9

137.2

20.3

Table 4: Number of reasoning paths used by PRA and our RL
model. RL achieved better MAP with a more compact set of
learned paths.

Since path-based methods generally work better
than embedding methods for this task, we do not
include the other two embedding baselines in this
table. Instead, we spare the room to show the de-
tailed results on each relation reasoning task.

For the overall MAP shown in the last row of the
table, our approach signiﬁcantly outperforms both
the path-based method and embedding methods on
two datasets, which validates the strong reasoning
ability of our RL model. For most relations, since
the embedding methods fail to use the path infor-

Figure 2: The distribution of paths lengths on two datasets

Figure 3: The success ratio (succ10) during training. Task:
athletePlaysForTeam.3

mation in the KG, they generally perform worse
than our RL model or PRA. However, when there
are not enough paths between entities, our model
and PRA can give poor results. For example,
for the relation ﬁlmWrittenBy, our RL model only
ﬁnds 4 unique reasoning paths, which means there
is actually not enough reasoning evidence existing
in the KG. Another observation is that we always
get better performance on the NELL dataset. By
analyzing the paths found from the KGs, we be-
lieve the potential reason is that the NELL dataset
has more short paths than FB15K-237 and some
of them are simply synonyms of the reasoning re-
lations.
Fact Prediction Instead of ranking the target en-
tities, this task directly ranks all the positive and
negative samples for a particular relation. The
PRA is not included as a baseline here, since the
PRA code only gives a target entity ranking for
each query node instead of a ranking of all triples.
Table 3 shows the overall results of all the meth-
ods. Our RL model gets even better results on this
task. We also observe that the RL model beats all
the embedding baselines on most reasoning tasks.

4.3.2 Qualitative Analysis of Reasoning Paths

To analyze the properties of reasoning paths, we
show a few reasoning paths found by the agent
in Table 5. To illustrate the effect of the efﬁ-
ciency reward function, we show the path length
distributions in Figure 2. To interpret these paths,
take the personNationality relation for example,
the ﬁrst reasoning path indicates that if we know
facts placeOfBirth(x,y) and locationContains(z,y)
then it is highly possible that person x has nation-
ality z. These short but predictive paths indicate
the effectiveness of the RL model. Another im-
portant observation is that our model use much

fewer reasoning paths than PRA, which indicates
that our model can actually extract the most reli-
able reasoning evidence from KG. Table 4 shows
some comparisons about the number of reasoning
paths. We can see that, with the pre-deﬁned re-
ward functions, the RL agent is capable of picking
the strong ones and ﬁlter out similar or irrelevant
ones.

4.3.3 Effect of Supervised Learning

As mentioned in Section 3.2, one major challenge
for applying RL to KG reasoning is the large ac-
tion space. We address this issue by applying
supervised learning before the reward retraining
step. To show the effect of the supervised train-
ing, we evaluate the agent’s success ratio of reach-
ing the target within 10 steps (succ10) after differ-
ent number of training episodes. For each train-
ing episode, one pair of entities (esource, etarget)
in the train set is used to ﬁnd paths. All the cor-
rect paths linking the entities will get a +1 global
reward. We then plug in some true paths for train-
ing. The succ10 is calculated on a held-out test set
that consists of 100 entity pairs. For the NELL-
995 dataset, since we have 200 unique relations,
the dimension of the action space will be 400 af-
ter we add the backward actions. This means that
random walks will get very low succ10 since there
may be nearly 40010 invalid paths. Figure 3 shows
the succ10 during training. We see that even the
agent has not seen the entity before, it can actually
pick the promising relation to extend its path. This
also validates the effectiveness of our state repre-
sentations.

3The conﬁdence band is generated using 50 different runs.

Relation

Reasoning Path

ﬁlmCountry

personNationality

tvProgramLanguage

personBornInLocation

athletePlaysForTeam

ﬁlmReleaseRegion
featureFilmLocation → locationContains−1
actorFilm−1 → personNationality
placeOfBirth→ locationContains−1
peoplePlaceLived → locationContains−1
peopleMarriage → locationOfCeremony → locationContains−1

tvCountryOfOrigin → countryOfﬁcialLanguage
tvCountryOfOrigin → ﬁlmReleaseRegion−1 → ﬁlmLanguage
tvCastActor → ﬁlmLanguage

personBornInCity
graduatedUniversity → graduatedSchool−1 → personBornInCity
personBornInCity → atLocation−1 → atLocation
athleteHomeStadium → teamHomeStadium−1
athletePlaysSport → teamPlaysSport−1
athleteLedSportsTeam

personLeadsOrganization

worksFor
organizationTerminatedPerson−1
mutualProxyFor−1

Table 5: Example reasoning paths found by our RL model. The ﬁrst three relations come from the FB15K-237 dataset. The
others are from NELL-995. Inverses of existing relations are denoted by −1.

5 Conclusion and Future Work

Acknowledgments

In this paper, we propose a reinforcement learn-
ing framework to improve the performance of re-
lation reasoning in KGs. Speciﬁcally, we train a
RL agent to ﬁnd reasoning paths in the knowledge
base. Unlike previous path ﬁnding models that are
based on random walks, the RL model allows us
to control the properties of the found paths. These
effective paths can also be used as an alternative to
PRA in many path-based reasoning methods. For
two standard reasoning tasks, using the RL paths
as reasoning formulas, our approach generally out-
performs two classes of baselines.

For

future studies, we plan to investigate
the possibility of incorporating adversarial learn-
ing (Goodfellow et al., 2014) to give better re-
wards than the human-deﬁned reward functions
Instead of designing rewards
used in this work.
according to path characteristics, a discriminative
model can be trained to give rewards. Also, to ad-
dress the problematic scenario when the KG does
not have enough reasoning paths, we are interested
in applying our RL framework to joint reasoning
with KG triples and text mentions.

We gratefully acknowledge
support of
NVIDIA Corporation with the donation of one Ti-
tan X Pascal GPU used for this research.

the

References

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. ACM.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems, pages 2787–2795.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010a. Toward an architecture for never-
ending language learning. In AAAI.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010b. Toward an architecture for never-
In Proceedings of the
ending language learning.
Twenty-Fourth Conference on Artiﬁcial Intelligence
(AAAI 2010).

Rajarshi Das, Arvind Neelakantan, David Belanger,
and Andrew McCallum. 2017. Chains of reasoning
over entities, relations, and text using recurrent neu-
ral networks. EACL.

Ni Lao, Jun Zhu, Xinwang Liu, Yandong Liu, and
William W Cohen. 2010. Efﬁcient relational learn-
ing with hidden variable detection. In NIPS, pages
1234–1242.

Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
Improving learning
and Tom M Mitchell. 2013.
and inference in a large knowledge-base using latent
syntactic cues. In EMNLP, pages 833–838.

Matt Gardner, Partha Pratim Talukdar, Jayant Krishna-
murthy, and Tom Mitchell. 2014. Incorporating vec-
tor space similarity in random walk inference over
knowledge bases.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in Neural Information
versarial nets.
Processing Systems, pages 2672–2680.

Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space.
In
EMNLP.

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al. 2012. Deep neural networks for
acoustic modeling in speech recognition: The shared
views of four research groups. IEEE Signal Process-
ing Magazine, 29(6):82–97.

Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun
Zhao. 2015. Knowledge graph embedding via dy-
namic mapping matrix. In ACL (1), pages 687–696.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zit-
Inferring and exe-
nick, and Ross Girshick. 2017.
cuting programs for visual reasoning. arXiv preprint
arXiv:1705.03633.

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012.
Imagenet classiﬁcation with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.

Ni Lao, Tom Mitchell, and William W Cohen. 2011a.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 529–539. Association for Computa-
tional Linguistics.

Ni Lao, Tom M. Mitchell, and William W. Cohen.
2011b. Random walk inference and learning in a
large scale knowledge base. In EMNLP, pages 529–
539. ACL.

Chen Liang, Jonathan Berant, Quoc Le, Kenneth D
Neural symbolic
Forbus, and Ni Lao. 2016.
machines: Learning semantic parsers on free-
arXiv preprint
base with weak supervision.
arXiv:1611.00020.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In AAAI,
pages 2181–2187.

Volodymyr Mnih, Koray Kavukcuoglu, David Sil-
ver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. 2013. Playing atari
with deep reinforcement learning. arXiv preprint
arXiv:1312.5602.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. Na-
ture, 518(7540):529–533.

Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional vector space mod-
els for knowledge base completion. arXiv preprint
arXiv:1504.06662.

David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George Van Den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, et al. 2016. Mastering
the game of go with deep neural networks and tree
search. Nature, 529(7587):484–489.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases. In EMNLP, volume 15, pages
1499–1509. Citeseer.

William Yang Wang and William W Cohen. 2015.
Joint information extraction and reasoning: A scal-
able statistical relational learning approach. In ACL.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In AAAI, pages 1112–1119.
Citeseer.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
Jun Zhao, et al. 2014. Relation classiﬁcation via
In COLING,
convolutional deep neural network.
pages 2335–2344.

DeepPath: A Reinforcement Learning Method for
Knowledge Graph Reasoning

Wenhan Xiong and Thien Hoang and William Yang Wang
Department of Computer Science
University of California, Santa Barbara
Santa Barbara, CA 93106 USA
{xwhan,william}@cs.ucsb.edu, thienhoang@umail.ucsb.edu

8
1
0
2
 
l
u
J
 
7
 
 
]
L
C
.
s
c
[
 
 
3
v
0
9
6
6
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

We study the problem of learning to reason
in large scale knowledge graphs (KGs).
More speciﬁcally, we describe a novel re-
inforcement learning framework for learn-
ing multi-hop relational paths: we use a
policy-based agent with continuous states
based on knowledge graph embeddings,
which reasons in a KG vector space by
sampling the most promising relation to
extend its path. In contrast to prior work,
our approach includes a reward function
that takes the accuracy, diversity, and ef-
ﬁciency into consideration. Experimen-
tally, we show that our proposed method
outperforms a path-ranking based algo-
rithm and knowledge graph embedding
methods on Freebase and Never-Ending
Language Learning datasets.1

1

Introduction

Deep neural networks for acoustic modeling in
speech recognitionIn recent years, deep learn-
ing techniques have obtained many state-of-the-
art results in various classiﬁcation and recognition
problems (Krizhevsky et al., 2012; Hinton et al.,
2012; Kim, 2014). However, complex natural lan-
guage processing problems often require multi-
ple inter-related decisions, and empowering deep
learning models with the ability of learning to rea-
son is still a challenging issue. To handle com-
plex queries where there are no obvious answers,
intelligent machines must be able to reason with
existing resources, and learn to infer an unknown
answer.

More speciﬁcally, we situate our study in the
context of multi-hop reasoning, which is the task

1Code and the NELL dataset are available at https://

github.com/xwhan/DeepPath.

of learning explicit inference formulas, given a
large KG. For example, if the KG includes the
beliefs such as Neymar plays for Barcelona, and
Barcelona are in the La Liga league, then ma-
chines should be able to learn the following for-
mula: playerPlaysForTeam(P,T) ∧ teamPlaysIn-
League(T,L) ⇒ playerPlaysInLeague(P,L). In the
testing time, by plugging in the learned formulas,
the system should be able to automatically infer
the missing link between a pair of entities. This
kind of reasoning machine will potentially serve
as an essential components of complex QA sys-
tems.

In recent years, the Path-Ranking Algorithm
(PRA) (Lao et al., 2010, 2011a) emerges as a
promising method for learning inference paths in
large KGs. PRA uses a random-walk with restarts
based inference mechanism to perform multiple
bounded depth-ﬁrst search processes to ﬁnd rela-
tional paths. Coupled with elastic-net based learn-
ing, PRA then picks more plausible paths using
supervised learning. However, PRA operates in
a fully discrete space, which makes it difﬁcult to
evaluate and compare similar entities and relations
in a KG.

In this work, we propose a novel approach
for controllable multi-hop reasoning: we frame
the path learning process as reinforcement learn-
ing (RL). In contrast to PRA, we use translation-
based knowledge based embedding method (Bor-
des et al., 2013) to encode the continuous state of
our RL agent, which reasons in the vector space
environment of the knowledge graph. The agent
takes incremental steps by sampling a relation to
extend its path. To better guide the RL agent for
learning relational paths, we use policy gradient
training (Mnih et al., 2015) with a novel reward
function that jointly encourages accuracy, diver-
sity, and efﬁciency. Empirically, we show that our
method outperforms PRA and embedding based

methods on a Freebase and a Never-Ending Lan-
guage Learning (Carlson et al., 2010a) dataset.
Our contributions are three-fold:

• We are the ﬁrst to consider reinforcement
learning (RL) methods for learning relational
paths in knowledge graphs;

• Our learning method uses a complex reward
function that considers accuracy, efﬁciency,
and path diversity simultaneously, offering
better control and more ﬂexibility in the path-
ﬁnding process;

• We show that our method can scale up to
large scale knowledge graphs, outperform-
ing PRA and KG embedding methods in two
tasks.

In the next section, we outline related work in
path-ﬁnding and embedding methods in KGs. We
describe the proposed method in Section 3. We
show experimental results in Section 4. Finally,
we conclude in Section 5.

2 Related Work

The Path-Ranking Algorithm (PRA) method (Lao
et al., 2011b) is a primary path-ﬁnding approach
that uses random walk with restart strategies for
multi-hop reasoning. Gardner et al. (2013; 2014)
propose a modiﬁcation to PRA that computes fea-
ture similarity in the vector space. Wang and
Cohen (2015) introduce a recursive random walk
approach for integrating the background KG and
text—the method performs structure learning of
logic programs and information extraction from
text at the same time. A potential bottleneck for
random walk inference is that supernodes connect-
ing to large amount of formulas will create huge
fan-out areas that signiﬁcantly slow down the in-
ference and affect the accuracy.

Toutanova et al. (2015) provide a convolutional
neural network solution to multi-hop reasoning.
They build a CNN model based on lexicalized de-
pendency paths, which suffers from the error prop-
agation issue due to parse errors. Guu et al. (2015)
uses KG embeddings to answer path queries. Zeng
et al. (2014) described a CNN model for rela-
tional extraction, but it does not explicitly model
the relational paths. Neelakantan et al. (2015) pro-
pose a recurrent neural networks model for model-
ing relational paths in knowledge base completion
(KBC), but it trains too many separate models, and

therefore it does not scale. Note that many of the
recent KG reasoning methods (Neelakantan et al.,
2015; Das et al., 2017) still rely on ﬁrst learning
the PRA paths, which only operates in a discrete
space. Comparing to PRA, our method reasons
in a continuous space, and by incorporating vari-
ous criteria in the reward function, our reinforce-
ment learning (RL) framework has better control
and more ﬂexibility over the path-ﬁnding process.
Neural symbolic machine (Liang et al., 2016)
is a more recent work on KG reasoning, which
also applies reinforcement learning but has a dif-
ferent ﬂavor from our work. NSM learns to com-
pose programs that can ﬁnd answers to natural lan-
guage questions, while our RL model tries to add
new facts to knowledge graph (KG) by reasoning
on existing KG triples.
In order to get answers,
NSM learns to generate a sequence of actions that
can be combined as a executable program. The ac-
tion space in NSM is a set of predeﬁned tokens. In
our framework, the goal is to ﬁnd reasoning paths,
thus the action space is relation space in the KG. A
similar framework (Johnson et al., 2017) has also
been applied to visual reasoning tasks.

3 Methodology

In this section, we describe in detail our RL-based
framework for multi-hop relation reasoning. The
speciﬁc task of relation reasoning is to ﬁnd re-
liable predictive paths between entity pairs. We
formulate the path ﬁnding problem as a sequen-
tial decision making problem which can be solved
with a RL agent. We ﬁrst describe the environ-
ment and the policy-based RL agent. By interact-
ing with the environment designed around the KG,
the agent learns to pick the promising reasoning
paths. Then we describe the training procedure of
our RL model. After that, we describe an efﬁcient
path-constrained search algorithm for relation rea-
soning with the paths found by the RL agent.

3.1 Reinforcement Learning for Relation

Reasoning

The RL system consists of two parts (see Fig-
ure 1). The ﬁrst part is the external environment
E which speciﬁes the dynamics of the interaction
between the agent and the KG. This environment
is modeled as a Markov decision process (MDP).
A tuple < S, A, P, R > is deﬁned to represent
the MDP, where S is the continuous state space,
A = {a1, a2, ..., an} is the set of all available ac-

Figure 1: Overview of our RL model. Left: The KG environment E modeled by a MDP. The dotted arrows (partially) show the
existing relation links in the KG and the bold arrows show the reasoning paths found by the RL agent. −1 denotes the inverse
of an relation. Right: The structure of the policy network agent. At each step, by interacting with the environment, the agent
learns to pick a relation link to extend the reasoning paths.

tions, P(St+1 = s(cid:48)|St = s, At = a) is the transi-
tion probability matrix, and R(s, a) is the reward
function of every (s, a) pairs.

is

the system,

The second part of

the RL
agent,
represented as a policy network
πθ(s, a) = p(a|s; θ) which maps the state vec-
tor to a stochastic policy. The neural network
parameters θ are updated using stochastic gra-
dient descent. Compared to Deep Q Network
(DQN) (Mnih et al., 2013), policy-based RL
methods turn out to be more appropriate for our
knowledge graph scenario. One reason is that
for the path ﬁnding problem in KG, the action
space can be very large due to complexity of the
relation graph. This can lead to poor convergence
properties for DQN. Besides, instead of learning
a greedy policy which is common in value-based
methods like DQN, the policy network is able to
learn a stochastic policy which prevent the agent
from getting stuck at an intermediate state. Before
we describe the structure of our policy network,
we ﬁrst describe the components (actions, states,
rewards) of the RL environment.

Actions Given the entity pairs (es, et) with
relation r, we want the agent to ﬁnd the most
linking these entity pairs.
informative paths

Beginning with the source entity es, the agent use
the policy network to pick the most promising
relation to extend its path at each step until it
reaches the target entity et. To keep the output
dimension of the policy network consistent, the
action space is deﬁned as all the relations in the
KG.

States The entities and relations in a KG are
naturally discrete atomic symbols. Since exist-
ing practical KGs like Freebase (Bollacker et al.,
2008) and NELL (Carlson et al., 2010b) often have
huge amounts of triples.
It is impossible to di-
rectly model all the symbolic atoms in states. To
capture the semantic information of these sym-
bols, we use translation-based embeddings such as
TransE (Bordes et al., 2013) and TransH (Wang
et al., 2014) to represent the entities and relations.
These embeddings map all the symbols to a low-
dimensional vector space. In our framework, each
state captures the agent’s position in the KG. After
taking an action, the agent will move from one en-
tity to another. These two are linked by the action
(relation) just taken by the agent. The state vector
at step t is given as follows:

st = (et, etarget − et)

where et denotes the embeddings of the current

entity node and etarget denotes the embeddings of
the target entity. At the initial state, et = esource.
We do not incorporate the reasoning relation in
the state, because the embedding of the reasoning
relation remain constant during path ﬁnding,
which is not helpful in training. However, we
ﬁnd out that by training the RL agent using a set
of positive samples for one particular relation,
the agent can successfully discover the relation
semantics.

Rewards There are a few factors that contribute to
the quality of the paths found by the RL agent. To
encourage the agent to ﬁnd predictive paths, our
reward functions include the following scoring cri-
teria:
Global accuracy: For our environment settings,
the number of actions that can be taken by the
agent can be very large. In other words, there are
much more incorrect sequential decisions than the
correct ones. The number of these incorrect de-
cision sequences can increase exponentially with
the length of the path. In view of this challenge,
the ﬁrst reward function we add to the RL model
is deﬁned as follows:

(cid:40)

rGLOBAL =

if the path reaches etarget

+1,
−1, otherwise

the agent is given an ofﬂine positive reward +1 if
it reaches the target after a sequence of actions.
Path efﬁciency: For the relation reasoning task,
we observe that short paths tend to provide more
reliable reasoning evidence than longer paths.
Shorter chains of relations can also improve the
efﬁciency of the reasoning by limiting the length
of the RL’s interactions with the environment. The
efﬁciency reward is deﬁned as follows:

rEFFICIENCY =

1
length(p)

where path p is deﬁned as a sequence of relations
r1 → r2 → ... → rn.
Path diversity: We train the agent to ﬁnd paths us-
ing positive samples for each relation. These train-
ing sample (esource, etarget) have similar state rep-
resentations in the vector space. The agent tends
to ﬁnd paths with similar syntax and semantics.
These paths often contains redundant information
since some of them may be correlated. To encour-
age the agent to ﬁnd diverse paths, we deﬁne a di-
versity reward function using the cosine similarity

between the current path and the existing ones:

rDIVERSITY = −

cos(p, pi)

1
|F |

|F |
(cid:88)

i=1

i=1 ri represents the path embed-

where p = (cid:80)n
ding for the relation chain r1 → r2 → ... → rn.
Policy Network We use a fully-connected neu-
ral network to parameterize the policy function
π(s; θ) that maps the state vector s to a proba-
bility distribution over all possible actions. The
neural network consists of two hidden layers, each
followed by a rectiﬁer nonlinearity layer (ReLU).
The output layer is normalized using a softmax
function (see Figure 1).

3.2 Training Pipeline

In practice, one big challenge of KG reasoning is
that the relation set can be quite large. For a typ-
ical KG, the RL agent is often faced with hun-
dreds (thousands) of possible actions.
In other
words, the output layer of the policy network of-
ten has a large dimension. Due to the complexity
of the relation graph and the large action space,
if we directly train the RL model by trial and er-
rors, which is typical for RL algorithms, the RL
model will show very poor convergence proper-
ties. After a long-time training, the agents fails
to ﬁnd any valuable path. To tackle this prob-
lem, we start our training with a supervised policy
which is inspired by the imitation learning pipeline
used by AlphaGo (Silver et al., 2016). In the Go
game, the player is facing nearly 250 possible le-
gal moves at each step. Directly training the agent
to pick actions from the original action space can
be a difﬁcult task. AlphaGo ﬁrst train a supervised
policy network using experts moves. In our case,
the supervised policy is trained with a randomized
breadth-ﬁrst search (BFS).

Supervised Policy Learning For each relation,
we use a subset of all the positive samples (en-
tity pairs) to learn the supervised policy. For each
positive sample (esource, etarget), a two-side BFS
is conducted to ﬁnd same correct paths between
the entities. For each path p with a sequence of
relations r1 → r2 → ... → rn, we update the pa-
rameters θ to maximize the expected cumulative
reward using Monte-Carlo Policy Gradient (RE-

INFORCE) (Williams, 1992):

J(θ) = Ea∼π(a|s;θ)(

Rst,at)

(cid:88)

t

(cid:88)

(cid:88)

=

t

a∈A

π(a|st; θ)Rst,at

(1)

where J(θ) is the expected total rewards for one
episode. For supervised learning, we give a re-
ward of +1 for each step of a successful episode.
By plugging in the paths found by the BFS, the
approximated gradient used to update the policy
network is shown below:

∇θJ(θ) =

π(a|st; θ)∇θ log π(a|st; θ)

≈ ∇θ

log π(a = rt|st; θ)

(2)

(cid:88)

(cid:88)

t

a∈A
(cid:88)

t

where rt belongs to the path p.

However, the vanilla BFS is a biased search al-
gorithm which prefers short paths. When plug-
ging in these biased paths, it becomes difﬁcult
for the agent to ﬁnd longer paths which may po-
tentially be useful. We want the paths to be
controlled only by the deﬁned reward functions.
To prevent the biased search, we adopt a sim-
ple trick to add some random mechanisms to the
BFS. Instead of directly searching the path be-
tween esource and etarget, we randomly pick a in-
termediate node einter and then conduct two BFS
between (esource, einter) and (einter, etarget). The
concatenated paths are used to train the agent. The
supervised learning saves the agent great efforts
learning from failed actions. With the learned ex-
perience, we then train the agent to ﬁnd desirable
paths.
Retraining with Rewards To ﬁnd the reasoning
paths controlled by the reward functions, we use
reward functions to retrain the supervised policy
network. For each relation, the reasoning with one
entity pair is treated as one episode. Starting with
the source node esource, the agent picks a relation
according to the stochastic policy π(a|s), which is
a probability distribution over all relations, to ex-
tend its reasoning path. This relation link may lead
to a new entity, or it may lead to nothing. These
failed steps will cause the agent to receive negative
rewards. The agent will stay at the same state af-
ter these failed steps. Since the agent is following
a stochastic policy, the agent will not get stuck by
repeating a wrong step. To improve the training ef-
ﬁciency, we limit the episode length with an upper

Algorithm 1: Retraining Procedure with re-
ward functions
1 Restore parameters θ from supervised policy;
2 for episode ← 1 to N do

Initialize state vector st ← s0
Initialize episode length steps ← 0
while num steps < max length do

Randomly sample action a ∼ π(a|st)
Observe reward Rt, next state st+1
// if the step fails
if Rt = −1 then

Save < st, a > to Mneg

if success or steps = max length
then

break

Increment num steps

// penalize failed steps
Update θ using
g ∝ ∇θ
Mneg
if success then

(cid:80)

log π(a = rt|st; θ)(−1)

Rtotal ← λ1rGLOBAL + λ2rEFFICIENCY +
λ3rDIVERSITY
Update θ using
g ∝ ∇θ

t log π(a = rt|st; θ)Rtotal

(cid:80)

3

4

5

6

7

8

9

10

11

12

13

14

15

bound max length. The episode ends if the agent
fails to reach the target entity within max length
steps. After each episode, the policy network is
updated using the following gradient:

∇θJ(θ) = ∇θ

log π(a = rt|st; θ)Rtotal

(3)

(cid:88)

t

where Rtotal is the linear combination of the de-
ﬁned reward functions. The detail of the retrain
process is shown in Algorithm 1. In practice, θ is
updated using the Adam Optimizer (Kingma and
Ba, 2014) with L2 regularization.

3.3 Bi-directional Path-constrained Search

Given an entity pair, the reasoning paths learned
by the RL agent can be used as logical formulas
to predict the relation link. Each formula is veri-
ﬁed using a bi-directional search. In a typical KG,
one entity node can be linked to a large number
of neighbors with the same relation link. A sim-
ple example is the relation personNationality−1,
which denotes the inverse of personNationality.
Following this link, the entity United States can
If the for-
reach numerous neighboring entities.

Algorithm 2: Bi-directional search for path
veriﬁcation
1 Given a reasoning path

p : r1 → r2 → ... → rn
2 for (ei, ej) in test set D do
start ← 0; end ← n
3
lef t ← ∅; right ← ∅
while start < end do

5

4

lef tEx ← ∅; rightEx ← ∅
if len(left) < len(right) then

Extend path on the left side
Add connected nodes to lef tEx
lef t ← lef tEx

else

Extend path on the right side
Add connected nodes to rightEx
right ← rightEx

if lef t ∩ right (cid:54)= ∅ then

return True

else

return False

6

7

8

9

10

11

12

13

14

15

16

17

18

mula consists of such links, the number of inter-
mediate entities can exponentially increase as we
follow the reasoning formula. However, we ob-
serve that for these formulas, if we verify the for-
mula from the inverse direction. The number of in-
termediate nodes can be tremendously decreased.
Algorithm 2 shows a detailed description of the
proposed bi-directional search.

4 Experiments

To evaluate the reasoning formulas found by our
RL agent, we explore two standard KG reason-
link prediction (predicting target en-
ing tasks:
tities) and fact prediction (predicting whether an
unknown fact holds or not). We compare our
method with both path-based methods and embed-
ding based methods. After that, we further analyze
the reasoning paths found by our RL agent. These
highly predictive paths validate the effectiveness
of the reward functions. Finally, we conduct a ex-
periment to investigate the effect of the supervised
learning procedure.

4.1 Dataset and Settings

Table 1 shows the statistics of the two datasets
we conduct our experiments on. Both of them

Dataset
FB15K-237
NELL-995

# Ent.
14,505
75,492

# R.
237
200

# Triples
310,116
154.213

# Tasks
20
12

Table 1: Statistics of the Datasets. # Ent. denotes the number
of unique entities and # R. denotes the number of relations

are subsets of larger datasets. The triples in
FB15K-237 (Toutanova et al., 2015) are sampled
from FB15K (Bordes et al., 2013) with redun-
dant relations removed. We perform the reasoning
tasks on 20 relations which have enough reason-
ing paths. These tasks consists of relations from
different domains like Sports, People, Locations,
Film, etc. Besides, we present a new NELL sub-
set that is suitable for multi-hop reasoning from
the 995th iteration of the NELL system. We ﬁrst
remove the triples with relation generalizations or
haswikipediaurl. These two relations appear more
than 2M times in the NELL dataset, but they have
no reasoning values. After this step, we only se-
lect the triples with Top-200 relations. To facilitate
path ﬁnding, we also add the inverse triples. For
each triple (h, r, t), we append (t, r−1, h) to the
datasets. With these inverse triples, the agent is
able to step backward in the KG.

i

For each reasoning task ri, we remove all the
triples with ri or r−1
from the KG. These removed
triples are split into train and test samples. For
the link prediction task, each h in the test triples
{(h, r, t)} is considered as one query. A set of
candidate target entities are ranked using different
methods. For fact prediction, the true test triples
are ranked with some generated false triples.

4.2 Baselines and Implementation Details

Most KG reasoning methods are based on either
path formulas or KG embeddings. we explore
methods from both of these two classes in our ex-
periments. For path based methods, we compare
our RL model with the PRA (Lao et al., 2011a)
algorithm, which has been used in a couple of rea-
soning methods (Gardner et al., 2013; Neelakan-
tan et al., 2015). PRA is a data-driven algorithm
using random walks (RW) to ﬁnd paths and obtain
path features. For embedding based methods, we
evaluate several state-of-the-art embeddings de-
signed for knowledge base completion, such as
TransE (Bordes et al., 2013), TransH (Wang et al.,
2014), TransR (Lin et al., 2015) and TransD (Ji
et al., 2015) .

The implementation of PRA is based on the

Tasks

RL

TransE

TransR

Tasks

PRA

RL

TransE

TransR

FB15K-237

0.955
0.531
0.823
0.441
0.457
0.670
0.969
0.783
0.309
0.514

0.896
0.403
0.641
0.386
0.563
0.642
0.804
0.554
0.390
0.361

PRA

0.987
0.441
0.846
0.349
0.601
0.663
0.960
0.829
0.281
0.426

teamSports
birthPlace
personNationality
ﬁlmDirector
ﬁlmWrittenBy
ﬁlmLanguage
tvLanguage
capitalOf
organizationFounded
musicianOrigin
...

0.784
0.417
0.720
0.399
0.605
0.641
0.906
0.493
0.339
0.379

athletePlaysForTeam 0.547
athletePlaysInLeague
0.841
athleteHomeStadium 0.859
0.474
0.791
0.811
0.681
0.668
0.700
0.599

athletePlaysSport
teamPlaySports
orgHeadquaterCity
worksFor
bornLocation
personLeadsOrg
orgHiredPerson
...

NELL-995

0.750
0.960
0.890
0.957
0.738
0.790
0.711
0.757
0.795
0.742

0.627
0.773
0.718
0.876
0.761
0.620
0.677
0.712
0.751
0.719

0.673
0.912
0.722
0.963
0.814
0.657
0.692
0.812
0.772
0.737

Overall

0.541

0.572

0.532

0.540

0.675

0.796

0.737

0.789

Table 2: Link prediction results (MAP) on two datasets.

code released by (Lao et al., 2011a). We use the
TopK negative mode to generate negative samples
for both train and test samples. For each pos-
itive samples, there are approximately 10 corre-
sponding negative samples. Each negative sample
is generated by replacing the true target entity t
with a faked one t(cid:48)
in each triple (h, r, t). These
positive and negative test pairs generated by PRA
make up the test set for all methods evaluated in
this paper. For TransE,R,H,D, we learn a separate
embedding matrix for each reasoning task using
the positive training entity pairs. All these embed-
dings are trained for 1,000 epochs. 2

Our RL model make use of TransE to get the
continuous representation of the entities and rela-
tions. We use the same dimension as TransE, R
to embed the entities. Speciﬁcally, the state vec-
tor we use has a dimension of 200, which is also
the input size of the policy network. To reason
using the path formulas, we adopt a similar lin-
ear regression approach as in PRA to re-rank the
paths. However, instead of using the random walk
probabilities as path features, which can be com-
putationally expensive, we simply use binary path
features obtained by the bi-directional search. We
observe that with only a few mined path formulas,
our method can achieve better results than PRA’s
data-driven approach.

4.3 Results

4.3.1 Quantitative Results

Link Prediction This task is to rank the target en-
tities given a query entity. Table 2 shows the mean
average precision (MAP) results on two datasets.

2The implementation we used can be found at https:

//github.com/thunlp/Fast-TransX

Fact Prediction Results

Methods FB15K-237 NELL-995

RL
TransE
TransH
TransR
TransD

0.311
0.277
0.309
0.302
0.303

0.493
0.383
0.389
0.406
0.413

Table 3: Fact prediction results (MAP) on two datasets.

# of Reasoning Paths

Tasks

worksFor
teamPlaySports
teamPlaysInLeague
athletehomestadium
organizationHiredPerson
...
Average #

PRA

247
113
69
37
244

RL

25
27
21
11
9

137.2

20.3

Table 4: Number of reasoning paths used by PRA and our RL
model. RL achieved better MAP with a more compact set of
learned paths.

Since path-based methods generally work better
than embedding methods for this task, we do not
include the other two embedding baselines in this
table. Instead, we spare the room to show the de-
tailed results on each relation reasoning task.

For the overall MAP shown in the last row of the
table, our approach signiﬁcantly outperforms both
the path-based method and embedding methods on
two datasets, which validates the strong reasoning
ability of our RL model. For most relations, since
the embedding methods fail to use the path infor-

Figure 2: The distribution of paths lengths on two datasets

Figure 3: The success ratio (succ10) during training. Task:
athletePlaysForTeam.3

mation in the KG, they generally perform worse
than our RL model or PRA. However, when there
are not enough paths between entities, our model
and PRA can give poor results. For example,
for the relation ﬁlmWrittenBy, our RL model only
ﬁnds 4 unique reasoning paths, which means there
is actually not enough reasoning evidence existing
in the KG. Another observation is that we always
get better performance on the NELL dataset. By
analyzing the paths found from the KGs, we be-
lieve the potential reason is that the NELL dataset
has more short paths than FB15K-237 and some
of them are simply synonyms of the reasoning re-
lations.
Fact Prediction Instead of ranking the target en-
tities, this task directly ranks all the positive and
negative samples for a particular relation. The
PRA is not included as a baseline here, since the
PRA code only gives a target entity ranking for
each query node instead of a ranking of all triples.
Table 3 shows the overall results of all the meth-
ods. Our RL model gets even better results on this
task. We also observe that the RL model beats all
the embedding baselines on most reasoning tasks.

4.3.2 Qualitative Analysis of Reasoning Paths

To analyze the properties of reasoning paths, we
show a few reasoning paths found by the agent
in Table 5. To illustrate the effect of the efﬁ-
ciency reward function, we show the path length
distributions in Figure 2. To interpret these paths,
take the personNationality relation for example,
the ﬁrst reasoning path indicates that if we know
facts placeOfBirth(x,y) and locationContains(z,y)
then it is highly possible that person x has nation-
ality z. These short but predictive paths indicate
the effectiveness of the RL model. Another im-
portant observation is that our model use much

fewer reasoning paths than PRA, which indicates
that our model can actually extract the most reli-
able reasoning evidence from KG. Table 4 shows
some comparisons about the number of reasoning
paths. We can see that, with the pre-deﬁned re-
ward functions, the RL agent is capable of picking
the strong ones and ﬁlter out similar or irrelevant
ones.

4.3.3 Effect of Supervised Learning

As mentioned in Section 3.2, one major challenge
for applying RL to KG reasoning is the large ac-
tion space. We address this issue by applying
supervised learning before the reward retraining
step. To show the effect of the supervised train-
ing, we evaluate the agent’s success ratio of reach-
ing the target within 10 steps (succ10) after differ-
ent number of training episodes. For each train-
ing episode, one pair of entities (esource, etarget)
in the train set is used to ﬁnd paths. All the cor-
rect paths linking the entities will get a +1 global
reward. We then plug in some true paths for train-
ing. The succ10 is calculated on a held-out test set
that consists of 100 entity pairs. For the NELL-
995 dataset, since we have 200 unique relations,
the dimension of the action space will be 400 af-
ter we add the backward actions. This means that
random walks will get very low succ10 since there
may be nearly 40010 invalid paths. Figure 3 shows
the succ10 during training. We see that even the
agent has not seen the entity before, it can actually
pick the promising relation to extend its path. This
also validates the effectiveness of our state repre-
sentations.

3The conﬁdence band is generated using 50 different runs.

Relation

Reasoning Path

ﬁlmCountry

personNationality

tvProgramLanguage

personBornInLocation

athletePlaysForTeam

ﬁlmReleaseRegion
featureFilmLocation → locationContains−1
actorFilm−1 → personNationality
placeOfBirth→ locationContains−1
peoplePlaceLived → locationContains−1
peopleMarriage → locationOfCeremony → locationContains−1

tvCountryOfOrigin → countryOfﬁcialLanguage
tvCountryOfOrigin → ﬁlmReleaseRegion−1 → ﬁlmLanguage
tvCastActor → ﬁlmLanguage

personBornInCity
graduatedUniversity → graduatedSchool−1 → personBornInCity
personBornInCity → atLocation−1 → atLocation
athleteHomeStadium → teamHomeStadium−1
athletePlaysSport → teamPlaysSport−1
athleteLedSportsTeam

personLeadsOrganization

worksFor
organizationTerminatedPerson−1
mutualProxyFor−1

Table 5: Example reasoning paths found by our RL model. The ﬁrst three relations come from the FB15K-237 dataset. The
others are from NELL-995. Inverses of existing relations are denoted by −1.

5 Conclusion and Future Work

Acknowledgments

In this paper, we propose a reinforcement learn-
ing framework to improve the performance of re-
lation reasoning in KGs. Speciﬁcally, we train a
RL agent to ﬁnd reasoning paths in the knowledge
base. Unlike previous path ﬁnding models that are
based on random walks, the RL model allows us
to control the properties of the found paths. These
effective paths can also be used as an alternative to
PRA in many path-based reasoning methods. For
two standard reasoning tasks, using the RL paths
as reasoning formulas, our approach generally out-
performs two classes of baselines.

For

future studies, we plan to investigate
the possibility of incorporating adversarial learn-
ing (Goodfellow et al., 2014) to give better re-
wards than the human-deﬁned reward functions
Instead of designing rewards
used in this work.
according to path characteristics, a discriminative
model can be trained to give rewards. Also, to ad-
dress the problematic scenario when the KG does
not have enough reasoning paths, we are interested
in applying our RL framework to joint reasoning
with KG triples and text mentions.

We gratefully acknowledge
support of
NVIDIA Corporation with the donation of one Ti-
tan X Pascal GPU used for this research.

the

References

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. ACM.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems, pages 2787–2795.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010a. Toward an architecture for never-
ending language learning. In AAAI.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010b. Toward an architecture for never-
In Proceedings of the
ending language learning.
Twenty-Fourth Conference on Artiﬁcial Intelligence
(AAAI 2010).

Rajarshi Das, Arvind Neelakantan, David Belanger,
and Andrew McCallum. 2017. Chains of reasoning
over entities, relations, and text using recurrent neu-
ral networks. EACL.

Ni Lao, Jun Zhu, Xinwang Liu, Yandong Liu, and
William W Cohen. 2010. Efﬁcient relational learn-
ing with hidden variable detection. In NIPS, pages
1234–1242.

Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
Improving learning
and Tom M Mitchell. 2013.
and inference in a large knowledge-base using latent
syntactic cues. In EMNLP, pages 833–838.

Matt Gardner, Partha Pratim Talukdar, Jayant Krishna-
murthy, and Tom Mitchell. 2014. Incorporating vec-
tor space similarity in random walk inference over
knowledge bases.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in Neural Information
versarial nets.
Processing Systems, pages 2672–2680.

Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space.
In
EMNLP.

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al. 2012. Deep neural networks for
acoustic modeling in speech recognition: The shared
views of four research groups. IEEE Signal Process-
ing Magazine, 29(6):82–97.

Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun
Zhao. 2015. Knowledge graph embedding via dy-
namic mapping matrix. In ACL (1), pages 687–696.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zit-
Inferring and exe-
nick, and Ross Girshick. 2017.
cuting programs for visual reasoning. arXiv preprint
arXiv:1705.03633.

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012.
Imagenet classiﬁcation with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.

Ni Lao, Tom Mitchell, and William W Cohen. 2011a.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 529–539. Association for Computa-
tional Linguistics.

Ni Lao, Tom M. Mitchell, and William W. Cohen.
2011b. Random walk inference and learning in a
large scale knowledge base. In EMNLP, pages 529–
539. ACL.

Chen Liang, Jonathan Berant, Quoc Le, Kenneth D
Neural symbolic
Forbus, and Ni Lao. 2016.
machines: Learning semantic parsers on free-
arXiv preprint
base with weak supervision.
arXiv:1611.00020.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In AAAI,
pages 2181–2187.

Volodymyr Mnih, Koray Kavukcuoglu, David Sil-
ver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. 2013. Playing atari
with deep reinforcement learning. arXiv preprint
arXiv:1312.5602.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. Na-
ture, 518(7540):529–533.

Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional vector space mod-
els for knowledge base completion. arXiv preprint
arXiv:1504.06662.

David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George Van Den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, et al. 2016. Mastering
the game of go with deep neural networks and tree
search. Nature, 529(7587):484–489.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases. In EMNLP, volume 15, pages
1499–1509. Citeseer.

William Yang Wang and William W Cohen. 2015.
Joint information extraction and reasoning: A scal-
able statistical relational learning approach. In ACL.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In AAAI, pages 1112–1119.
Citeseer.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
Jun Zhao, et al. 2014. Relation classiﬁcation via
In COLING,
convolutional deep neural network.
pages 2335–2344.

DeepPath: A Reinforcement Learning Method for
Knowledge Graph Reasoning

Wenhan Xiong and Thien Hoang and William Yang Wang
Department of Computer Science
University of California, Santa Barbara
Santa Barbara, CA 93106 USA
{xwhan,william}@cs.ucsb.edu, thienhoang@umail.ucsb.edu

8
1
0
2
 
l
u
J
 
7
 
 
]
L
C
.
s
c
[
 
 
3
v
0
9
6
6
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

We study the problem of learning to reason
in large scale knowledge graphs (KGs).
More speciﬁcally, we describe a novel re-
inforcement learning framework for learn-
ing multi-hop relational paths: we use a
policy-based agent with continuous states
based on knowledge graph embeddings,
which reasons in a KG vector space by
sampling the most promising relation to
extend its path. In contrast to prior work,
our approach includes a reward function
that takes the accuracy, diversity, and ef-
ﬁciency into consideration. Experimen-
tally, we show that our proposed method
outperforms a path-ranking based algo-
rithm and knowledge graph embedding
methods on Freebase and Never-Ending
Language Learning datasets.1

1

Introduction

Deep neural networks for acoustic modeling in
speech recognitionIn recent years, deep learn-
ing techniques have obtained many state-of-the-
art results in various classiﬁcation and recognition
problems (Krizhevsky et al., 2012; Hinton et al.,
2012; Kim, 2014). However, complex natural lan-
guage processing problems often require multi-
ple inter-related decisions, and empowering deep
learning models with the ability of learning to rea-
son is still a challenging issue. To handle com-
plex queries where there are no obvious answers,
intelligent machines must be able to reason with
existing resources, and learn to infer an unknown
answer.

More speciﬁcally, we situate our study in the
context of multi-hop reasoning, which is the task

1Code and the NELL dataset are available at https://

github.com/xwhan/DeepPath.

of learning explicit inference formulas, given a
large KG. For example, if the KG includes the
beliefs such as Neymar plays for Barcelona, and
Barcelona are in the La Liga league, then ma-
chines should be able to learn the following for-
mula: playerPlaysForTeam(P,T) ∧ teamPlaysIn-
League(T,L) ⇒ playerPlaysInLeague(P,L). In the
testing time, by plugging in the learned formulas,
the system should be able to automatically infer
the missing link between a pair of entities. This
kind of reasoning machine will potentially serve
as an essential components of complex QA sys-
tems.

In recent years, the Path-Ranking Algorithm
(PRA) (Lao et al., 2010, 2011a) emerges as a
promising method for learning inference paths in
large KGs. PRA uses a random-walk with restarts
based inference mechanism to perform multiple
bounded depth-ﬁrst search processes to ﬁnd rela-
tional paths. Coupled with elastic-net based learn-
ing, PRA then picks more plausible paths using
supervised learning. However, PRA operates in
a fully discrete space, which makes it difﬁcult to
evaluate and compare similar entities and relations
in a KG.

In this work, we propose a novel approach
for controllable multi-hop reasoning: we frame
the path learning process as reinforcement learn-
ing (RL). In contrast to PRA, we use translation-
based knowledge based embedding method (Bor-
des et al., 2013) to encode the continuous state of
our RL agent, which reasons in the vector space
environment of the knowledge graph. The agent
takes incremental steps by sampling a relation to
extend its path. To better guide the RL agent for
learning relational paths, we use policy gradient
training (Mnih et al., 2015) with a novel reward
function that jointly encourages accuracy, diver-
sity, and efﬁciency. Empirically, we show that our
method outperforms PRA and embedding based

methods on a Freebase and a Never-Ending Lan-
guage Learning (Carlson et al., 2010a) dataset.
Our contributions are three-fold:

• We are the ﬁrst to consider reinforcement
learning (RL) methods for learning relational
paths in knowledge graphs;

• Our learning method uses a complex reward
function that considers accuracy, efﬁciency,
and path diversity simultaneously, offering
better control and more ﬂexibility in the path-
ﬁnding process;

• We show that our method can scale up to
large scale knowledge graphs, outperform-
ing PRA and KG embedding methods in two
tasks.

In the next section, we outline related work in
path-ﬁnding and embedding methods in KGs. We
describe the proposed method in Section 3. We
show experimental results in Section 4. Finally,
we conclude in Section 5.

2 Related Work

The Path-Ranking Algorithm (PRA) method (Lao
et al., 2011b) is a primary path-ﬁnding approach
that uses random walk with restart strategies for
multi-hop reasoning. Gardner et al. (2013; 2014)
propose a modiﬁcation to PRA that computes fea-
ture similarity in the vector space. Wang and
Cohen (2015) introduce a recursive random walk
approach for integrating the background KG and
text—the method performs structure learning of
logic programs and information extraction from
text at the same time. A potential bottleneck for
random walk inference is that supernodes connect-
ing to large amount of formulas will create huge
fan-out areas that signiﬁcantly slow down the in-
ference and affect the accuracy.

Toutanova et al. (2015) provide a convolutional
neural network solution to multi-hop reasoning.
They build a CNN model based on lexicalized de-
pendency paths, which suffers from the error prop-
agation issue due to parse errors. Guu et al. (2015)
uses KG embeddings to answer path queries. Zeng
et al. (2014) described a CNN model for rela-
tional extraction, but it does not explicitly model
the relational paths. Neelakantan et al. (2015) pro-
pose a recurrent neural networks model for model-
ing relational paths in knowledge base completion
(KBC), but it trains too many separate models, and

therefore it does not scale. Note that many of the
recent KG reasoning methods (Neelakantan et al.,
2015; Das et al., 2017) still rely on ﬁrst learning
the PRA paths, which only operates in a discrete
space. Comparing to PRA, our method reasons
in a continuous space, and by incorporating vari-
ous criteria in the reward function, our reinforce-
ment learning (RL) framework has better control
and more ﬂexibility over the path-ﬁnding process.
Neural symbolic machine (Liang et al., 2016)
is a more recent work on KG reasoning, which
also applies reinforcement learning but has a dif-
ferent ﬂavor from our work. NSM learns to com-
pose programs that can ﬁnd answers to natural lan-
guage questions, while our RL model tries to add
new facts to knowledge graph (KG) by reasoning
on existing KG triples.
In order to get answers,
NSM learns to generate a sequence of actions that
can be combined as a executable program. The ac-
tion space in NSM is a set of predeﬁned tokens. In
our framework, the goal is to ﬁnd reasoning paths,
thus the action space is relation space in the KG. A
similar framework (Johnson et al., 2017) has also
been applied to visual reasoning tasks.

3 Methodology

In this section, we describe in detail our RL-based
framework for multi-hop relation reasoning. The
speciﬁc task of relation reasoning is to ﬁnd re-
liable predictive paths between entity pairs. We
formulate the path ﬁnding problem as a sequen-
tial decision making problem which can be solved
with a RL agent. We ﬁrst describe the environ-
ment and the policy-based RL agent. By interact-
ing with the environment designed around the KG,
the agent learns to pick the promising reasoning
paths. Then we describe the training procedure of
our RL model. After that, we describe an efﬁcient
path-constrained search algorithm for relation rea-
soning with the paths found by the RL agent.

3.1 Reinforcement Learning for Relation

Reasoning

The RL system consists of two parts (see Fig-
ure 1). The ﬁrst part is the external environment
E which speciﬁes the dynamics of the interaction
between the agent and the KG. This environment
is modeled as a Markov decision process (MDP).
A tuple < S, A, P, R > is deﬁned to represent
the MDP, where S is the continuous state space,
A = {a1, a2, ..., an} is the set of all available ac-

Figure 1: Overview of our RL model. Left: The KG environment E modeled by a MDP. The dotted arrows (partially) show the
existing relation links in the KG and the bold arrows show the reasoning paths found by the RL agent. −1 denotes the inverse
of an relation. Right: The structure of the policy network agent. At each step, by interacting with the environment, the agent
learns to pick a relation link to extend the reasoning paths.

tions, P(St+1 = s(cid:48)|St = s, At = a) is the transi-
tion probability matrix, and R(s, a) is the reward
function of every (s, a) pairs.

is

the system,

The second part of

the RL
agent,
represented as a policy network
πθ(s, a) = p(a|s; θ) which maps the state vec-
tor to a stochastic policy. The neural network
parameters θ are updated using stochastic gra-
dient descent. Compared to Deep Q Network
(DQN) (Mnih et al., 2013), policy-based RL
methods turn out to be more appropriate for our
knowledge graph scenario. One reason is that
for the path ﬁnding problem in KG, the action
space can be very large due to complexity of the
relation graph. This can lead to poor convergence
properties for DQN. Besides, instead of learning
a greedy policy which is common in value-based
methods like DQN, the policy network is able to
learn a stochastic policy which prevent the agent
from getting stuck at an intermediate state. Before
we describe the structure of our policy network,
we ﬁrst describe the components (actions, states,
rewards) of the RL environment.

Actions Given the entity pairs (es, et) with
relation r, we want the agent to ﬁnd the most
linking these entity pairs.
informative paths

Beginning with the source entity es, the agent use
the policy network to pick the most promising
relation to extend its path at each step until it
reaches the target entity et. To keep the output
dimension of the policy network consistent, the
action space is deﬁned as all the relations in the
KG.

States The entities and relations in a KG are
naturally discrete atomic symbols. Since exist-
ing practical KGs like Freebase (Bollacker et al.,
2008) and NELL (Carlson et al., 2010b) often have
huge amounts of triples.
It is impossible to di-
rectly model all the symbolic atoms in states. To
capture the semantic information of these sym-
bols, we use translation-based embeddings such as
TransE (Bordes et al., 2013) and TransH (Wang
et al., 2014) to represent the entities and relations.
These embeddings map all the symbols to a low-
dimensional vector space. In our framework, each
state captures the agent’s position in the KG. After
taking an action, the agent will move from one en-
tity to another. These two are linked by the action
(relation) just taken by the agent. The state vector
at step t is given as follows:

st = (et, etarget − et)

where et denotes the embeddings of the current

entity node and etarget denotes the embeddings of
the target entity. At the initial state, et = esource.
We do not incorporate the reasoning relation in
the state, because the embedding of the reasoning
relation remain constant during path ﬁnding,
which is not helpful in training. However, we
ﬁnd out that by training the RL agent using a set
of positive samples for one particular relation,
the agent can successfully discover the relation
semantics.

Rewards There are a few factors that contribute to
the quality of the paths found by the RL agent. To
encourage the agent to ﬁnd predictive paths, our
reward functions include the following scoring cri-
teria:
Global accuracy: For our environment settings,
the number of actions that can be taken by the
agent can be very large. In other words, there are
much more incorrect sequential decisions than the
correct ones. The number of these incorrect de-
cision sequences can increase exponentially with
the length of the path. In view of this challenge,
the ﬁrst reward function we add to the RL model
is deﬁned as follows:

(cid:40)

rGLOBAL =

if the path reaches etarget

+1,
−1, otherwise

the agent is given an ofﬂine positive reward +1 if
it reaches the target after a sequence of actions.
Path efﬁciency: For the relation reasoning task,
we observe that short paths tend to provide more
reliable reasoning evidence than longer paths.
Shorter chains of relations can also improve the
efﬁciency of the reasoning by limiting the length
of the RL’s interactions with the environment. The
efﬁciency reward is deﬁned as follows:

rEFFICIENCY =

1
length(p)

where path p is deﬁned as a sequence of relations
r1 → r2 → ... → rn.
Path diversity: We train the agent to ﬁnd paths us-
ing positive samples for each relation. These train-
ing sample (esource, etarget) have similar state rep-
resentations in the vector space. The agent tends
to ﬁnd paths with similar syntax and semantics.
These paths often contains redundant information
since some of them may be correlated. To encour-
age the agent to ﬁnd diverse paths, we deﬁne a di-
versity reward function using the cosine similarity

between the current path and the existing ones:

rDIVERSITY = −

cos(p, pi)

1
|F |

|F |
(cid:88)

i=1

i=1 ri represents the path embed-

where p = (cid:80)n
ding for the relation chain r1 → r2 → ... → rn.
Policy Network We use a fully-connected neu-
ral network to parameterize the policy function
π(s; θ) that maps the state vector s to a proba-
bility distribution over all possible actions. The
neural network consists of two hidden layers, each
followed by a rectiﬁer nonlinearity layer (ReLU).
The output layer is normalized using a softmax
function (see Figure 1).

3.2 Training Pipeline

In practice, one big challenge of KG reasoning is
that the relation set can be quite large. For a typ-
ical KG, the RL agent is often faced with hun-
dreds (thousands) of possible actions.
In other
words, the output layer of the policy network of-
ten has a large dimension. Due to the complexity
of the relation graph and the large action space,
if we directly train the RL model by trial and er-
rors, which is typical for RL algorithms, the RL
model will show very poor convergence proper-
ties. After a long-time training, the agents fails
to ﬁnd any valuable path. To tackle this prob-
lem, we start our training with a supervised policy
which is inspired by the imitation learning pipeline
used by AlphaGo (Silver et al., 2016). In the Go
game, the player is facing nearly 250 possible le-
gal moves at each step. Directly training the agent
to pick actions from the original action space can
be a difﬁcult task. AlphaGo ﬁrst train a supervised
policy network using experts moves. In our case,
the supervised policy is trained with a randomized
breadth-ﬁrst search (BFS).

Supervised Policy Learning For each relation,
we use a subset of all the positive samples (en-
tity pairs) to learn the supervised policy. For each
positive sample (esource, etarget), a two-side BFS
is conducted to ﬁnd same correct paths between
the entities. For each path p with a sequence of
relations r1 → r2 → ... → rn, we update the pa-
rameters θ to maximize the expected cumulative
reward using Monte-Carlo Policy Gradient (RE-

INFORCE) (Williams, 1992):

J(θ) = Ea∼π(a|s;θ)(

Rst,at)

(cid:88)

t

(cid:88)

(cid:88)

=

t

a∈A

π(a|st; θ)Rst,at

(1)

where J(θ) is the expected total rewards for one
episode. For supervised learning, we give a re-
ward of +1 for each step of a successful episode.
By plugging in the paths found by the BFS, the
approximated gradient used to update the policy
network is shown below:

∇θJ(θ) =

π(a|st; θ)∇θ log π(a|st; θ)

≈ ∇θ

log π(a = rt|st; θ)

(2)

(cid:88)

(cid:88)

t

a∈A
(cid:88)

t

where rt belongs to the path p.

However, the vanilla BFS is a biased search al-
gorithm which prefers short paths. When plug-
ging in these biased paths, it becomes difﬁcult
for the agent to ﬁnd longer paths which may po-
tentially be useful. We want the paths to be
controlled only by the deﬁned reward functions.
To prevent the biased search, we adopt a sim-
ple trick to add some random mechanisms to the
BFS. Instead of directly searching the path be-
tween esource and etarget, we randomly pick a in-
termediate node einter and then conduct two BFS
between (esource, einter) and (einter, etarget). The
concatenated paths are used to train the agent. The
supervised learning saves the agent great efforts
learning from failed actions. With the learned ex-
perience, we then train the agent to ﬁnd desirable
paths.
Retraining with Rewards To ﬁnd the reasoning
paths controlled by the reward functions, we use
reward functions to retrain the supervised policy
network. For each relation, the reasoning with one
entity pair is treated as one episode. Starting with
the source node esource, the agent picks a relation
according to the stochastic policy π(a|s), which is
a probability distribution over all relations, to ex-
tend its reasoning path. This relation link may lead
to a new entity, or it may lead to nothing. These
failed steps will cause the agent to receive negative
rewards. The agent will stay at the same state af-
ter these failed steps. Since the agent is following
a stochastic policy, the agent will not get stuck by
repeating a wrong step. To improve the training ef-
ﬁciency, we limit the episode length with an upper

Algorithm 1: Retraining Procedure with re-
ward functions
1 Restore parameters θ from supervised policy;
2 for episode ← 1 to N do

Initialize state vector st ← s0
Initialize episode length steps ← 0
while num steps < max length do

Randomly sample action a ∼ π(a|st)
Observe reward Rt, next state st+1
// if the step fails
if Rt = −1 then

Save < st, a > to Mneg

if success or steps = max length
then

break

Increment num steps

// penalize failed steps
Update θ using
g ∝ ∇θ
Mneg
if success then

(cid:80)

log π(a = rt|st; θ)(−1)

Rtotal ← λ1rGLOBAL + λ2rEFFICIENCY +
λ3rDIVERSITY
Update θ using
g ∝ ∇θ

t log π(a = rt|st; θ)Rtotal

(cid:80)

3

4

5

6

7

8

9

10

11

12

13

14

15

bound max length. The episode ends if the agent
fails to reach the target entity within max length
steps. After each episode, the policy network is
updated using the following gradient:

∇θJ(θ) = ∇θ

log π(a = rt|st; θ)Rtotal

(3)

(cid:88)

t

where Rtotal is the linear combination of the de-
ﬁned reward functions. The detail of the retrain
process is shown in Algorithm 1. In practice, θ is
updated using the Adam Optimizer (Kingma and
Ba, 2014) with L2 regularization.

3.3 Bi-directional Path-constrained Search

Given an entity pair, the reasoning paths learned
by the RL agent can be used as logical formulas
to predict the relation link. Each formula is veri-
ﬁed using a bi-directional search. In a typical KG,
one entity node can be linked to a large number
of neighbors with the same relation link. A sim-
ple example is the relation personNationality−1,
which denotes the inverse of personNationality.
Following this link, the entity United States can
If the for-
reach numerous neighboring entities.

Algorithm 2: Bi-directional search for path
veriﬁcation
1 Given a reasoning path

p : r1 → r2 → ... → rn
2 for (ei, ej) in test set D do
start ← 0; end ← n
3
lef t ← ∅; right ← ∅
while start < end do

5

4

lef tEx ← ∅; rightEx ← ∅
if len(left) < len(right) then

Extend path on the left side
Add connected nodes to lef tEx
lef t ← lef tEx

else

Extend path on the right side
Add connected nodes to rightEx
right ← rightEx

if lef t ∩ right (cid:54)= ∅ then

return True

else

return False

6

7

8

9

10

11

12

13

14

15

16

17

18

mula consists of such links, the number of inter-
mediate entities can exponentially increase as we
follow the reasoning formula. However, we ob-
serve that for these formulas, if we verify the for-
mula from the inverse direction. The number of in-
termediate nodes can be tremendously decreased.
Algorithm 2 shows a detailed description of the
proposed bi-directional search.

4 Experiments

To evaluate the reasoning formulas found by our
RL agent, we explore two standard KG reason-
link prediction (predicting target en-
ing tasks:
tities) and fact prediction (predicting whether an
unknown fact holds or not). We compare our
method with both path-based methods and embed-
ding based methods. After that, we further analyze
the reasoning paths found by our RL agent. These
highly predictive paths validate the effectiveness
of the reward functions. Finally, we conduct a ex-
periment to investigate the effect of the supervised
learning procedure.

4.1 Dataset and Settings

Table 1 shows the statistics of the two datasets
we conduct our experiments on. Both of them

Dataset
FB15K-237
NELL-995

# Ent.
14,505
75,492

# R.
237
200

# Triples
310,116
154.213

# Tasks
20
12

Table 1: Statistics of the Datasets. # Ent. denotes the number
of unique entities and # R. denotes the number of relations

are subsets of larger datasets. The triples in
FB15K-237 (Toutanova et al., 2015) are sampled
from FB15K (Bordes et al., 2013) with redun-
dant relations removed. We perform the reasoning
tasks on 20 relations which have enough reason-
ing paths. These tasks consists of relations from
different domains like Sports, People, Locations,
Film, etc. Besides, we present a new NELL sub-
set that is suitable for multi-hop reasoning from
the 995th iteration of the NELL system. We ﬁrst
remove the triples with relation generalizations or
haswikipediaurl. These two relations appear more
than 2M times in the NELL dataset, but they have
no reasoning values. After this step, we only se-
lect the triples with Top-200 relations. To facilitate
path ﬁnding, we also add the inverse triples. For
each triple (h, r, t), we append (t, r−1, h) to the
datasets. With these inverse triples, the agent is
able to step backward in the KG.

i

For each reasoning task ri, we remove all the
triples with ri or r−1
from the KG. These removed
triples are split into train and test samples. For
the link prediction task, each h in the test triples
{(h, r, t)} is considered as one query. A set of
candidate target entities are ranked using different
methods. For fact prediction, the true test triples
are ranked with some generated false triples.

4.2 Baselines and Implementation Details

Most KG reasoning methods are based on either
path formulas or KG embeddings. we explore
methods from both of these two classes in our ex-
periments. For path based methods, we compare
our RL model with the PRA (Lao et al., 2011a)
algorithm, which has been used in a couple of rea-
soning methods (Gardner et al., 2013; Neelakan-
tan et al., 2015). PRA is a data-driven algorithm
using random walks (RW) to ﬁnd paths and obtain
path features. For embedding based methods, we
evaluate several state-of-the-art embeddings de-
signed for knowledge base completion, such as
TransE (Bordes et al., 2013), TransH (Wang et al.,
2014), TransR (Lin et al., 2015) and TransD (Ji
et al., 2015) .

The implementation of PRA is based on the

Tasks

RL

TransE

TransR

Tasks

PRA

RL

TransE

TransR

FB15K-237

0.955
0.531
0.823
0.441
0.457
0.670
0.969
0.783
0.309
0.514

0.896
0.403
0.641
0.386
0.563
0.642
0.804
0.554
0.390
0.361

PRA

0.987
0.441
0.846
0.349
0.601
0.663
0.960
0.829
0.281
0.426

teamSports
birthPlace
personNationality
ﬁlmDirector
ﬁlmWrittenBy
ﬁlmLanguage
tvLanguage
capitalOf
organizationFounded
musicianOrigin
...

0.784
0.417
0.720
0.399
0.605
0.641
0.906
0.493
0.339
0.379

athletePlaysForTeam 0.547
athletePlaysInLeague
0.841
athleteHomeStadium 0.859
0.474
0.791
0.811
0.681
0.668
0.700
0.599

athletePlaysSport
teamPlaySports
orgHeadquaterCity
worksFor
bornLocation
personLeadsOrg
orgHiredPerson
...

NELL-995

0.750
0.960
0.890
0.957
0.738
0.790
0.711
0.757
0.795
0.742

0.627
0.773
0.718
0.876
0.761
0.620
0.677
0.712
0.751
0.719

0.673
0.912
0.722
0.963
0.814
0.657
0.692
0.812
0.772
0.737

Overall

0.541

0.572

0.532

0.540

0.675

0.796

0.737

0.789

Table 2: Link prediction results (MAP) on two datasets.

code released by (Lao et al., 2011a). We use the
TopK negative mode to generate negative samples
for both train and test samples. For each pos-
itive samples, there are approximately 10 corre-
sponding negative samples. Each negative sample
is generated by replacing the true target entity t
with a faked one t(cid:48)
in each triple (h, r, t). These
positive and negative test pairs generated by PRA
make up the test set for all methods evaluated in
this paper. For TransE,R,H,D, we learn a separate
embedding matrix for each reasoning task using
the positive training entity pairs. All these embed-
dings are trained for 1,000 epochs. 2

Our RL model make use of TransE to get the
continuous representation of the entities and rela-
tions. We use the same dimension as TransE, R
to embed the entities. Speciﬁcally, the state vec-
tor we use has a dimension of 200, which is also
the input size of the policy network. To reason
using the path formulas, we adopt a similar lin-
ear regression approach as in PRA to re-rank the
paths. However, instead of using the random walk
probabilities as path features, which can be com-
putationally expensive, we simply use binary path
features obtained by the bi-directional search. We
observe that with only a few mined path formulas,
our method can achieve better results than PRA’s
data-driven approach.

4.3 Results

4.3.1 Quantitative Results

Link Prediction This task is to rank the target en-
tities given a query entity. Table 2 shows the mean
average precision (MAP) results on two datasets.

2The implementation we used can be found at https:

//github.com/thunlp/Fast-TransX

Fact Prediction Results

Methods FB15K-237 NELL-995

RL
TransE
TransH
TransR
TransD

0.311
0.277
0.309
0.302
0.303

0.493
0.383
0.389
0.406
0.413

Table 3: Fact prediction results (MAP) on two datasets.

# of Reasoning Paths

Tasks

worksFor
teamPlaySports
teamPlaysInLeague
athletehomestadium
organizationHiredPerson
...
Average #

PRA

247
113
69
37
244

RL

25
27
21
11
9

137.2

20.3

Table 4: Number of reasoning paths used by PRA and our RL
model. RL achieved better MAP with a more compact set of
learned paths.

Since path-based methods generally work better
than embedding methods for this task, we do not
include the other two embedding baselines in this
table. Instead, we spare the room to show the de-
tailed results on each relation reasoning task.

For the overall MAP shown in the last row of the
table, our approach signiﬁcantly outperforms both
the path-based method and embedding methods on
two datasets, which validates the strong reasoning
ability of our RL model. For most relations, since
the embedding methods fail to use the path infor-

Figure 2: The distribution of paths lengths on two datasets

Figure 3: The success ratio (succ10) during training. Task:
athletePlaysForTeam.3

mation in the KG, they generally perform worse
than our RL model or PRA. However, when there
are not enough paths between entities, our model
and PRA can give poor results. For example,
for the relation ﬁlmWrittenBy, our RL model only
ﬁnds 4 unique reasoning paths, which means there
is actually not enough reasoning evidence existing
in the KG. Another observation is that we always
get better performance on the NELL dataset. By
analyzing the paths found from the KGs, we be-
lieve the potential reason is that the NELL dataset
has more short paths than FB15K-237 and some
of them are simply synonyms of the reasoning re-
lations.
Fact Prediction Instead of ranking the target en-
tities, this task directly ranks all the positive and
negative samples for a particular relation. The
PRA is not included as a baseline here, since the
PRA code only gives a target entity ranking for
each query node instead of a ranking of all triples.
Table 3 shows the overall results of all the meth-
ods. Our RL model gets even better results on this
task. We also observe that the RL model beats all
the embedding baselines on most reasoning tasks.

4.3.2 Qualitative Analysis of Reasoning Paths

To analyze the properties of reasoning paths, we
show a few reasoning paths found by the agent
in Table 5. To illustrate the effect of the efﬁ-
ciency reward function, we show the path length
distributions in Figure 2. To interpret these paths,
take the personNationality relation for example,
the ﬁrst reasoning path indicates that if we know
facts placeOfBirth(x,y) and locationContains(z,y)
then it is highly possible that person x has nation-
ality z. These short but predictive paths indicate
the effectiveness of the RL model. Another im-
portant observation is that our model use much

fewer reasoning paths than PRA, which indicates
that our model can actually extract the most reli-
able reasoning evidence from KG. Table 4 shows
some comparisons about the number of reasoning
paths. We can see that, with the pre-deﬁned re-
ward functions, the RL agent is capable of picking
the strong ones and ﬁlter out similar or irrelevant
ones.

4.3.3 Effect of Supervised Learning

As mentioned in Section 3.2, one major challenge
for applying RL to KG reasoning is the large ac-
tion space. We address this issue by applying
supervised learning before the reward retraining
step. To show the effect of the supervised train-
ing, we evaluate the agent’s success ratio of reach-
ing the target within 10 steps (succ10) after differ-
ent number of training episodes. For each train-
ing episode, one pair of entities (esource, etarget)
in the train set is used to ﬁnd paths. All the cor-
rect paths linking the entities will get a +1 global
reward. We then plug in some true paths for train-
ing. The succ10 is calculated on a held-out test set
that consists of 100 entity pairs. For the NELL-
995 dataset, since we have 200 unique relations,
the dimension of the action space will be 400 af-
ter we add the backward actions. This means that
random walks will get very low succ10 since there
may be nearly 40010 invalid paths. Figure 3 shows
the succ10 during training. We see that even the
agent has not seen the entity before, it can actually
pick the promising relation to extend its path. This
also validates the effectiveness of our state repre-
sentations.

3The conﬁdence band is generated using 50 different runs.

Relation

Reasoning Path

ﬁlmCountry

personNationality

tvProgramLanguage

personBornInLocation

athletePlaysForTeam

ﬁlmReleaseRegion
featureFilmLocation → locationContains−1
actorFilm−1 → personNationality
placeOfBirth→ locationContains−1
peoplePlaceLived → locationContains−1
peopleMarriage → locationOfCeremony → locationContains−1

tvCountryOfOrigin → countryOfﬁcialLanguage
tvCountryOfOrigin → ﬁlmReleaseRegion−1 → ﬁlmLanguage
tvCastActor → ﬁlmLanguage

personBornInCity
graduatedUniversity → graduatedSchool−1 → personBornInCity
personBornInCity → atLocation−1 → atLocation
athleteHomeStadium → teamHomeStadium−1
athletePlaysSport → teamPlaysSport−1
athleteLedSportsTeam

personLeadsOrganization

worksFor
organizationTerminatedPerson−1
mutualProxyFor−1

Table 5: Example reasoning paths found by our RL model. The ﬁrst three relations come from the FB15K-237 dataset. The
others are from NELL-995. Inverses of existing relations are denoted by −1.

5 Conclusion and Future Work

Acknowledgments

In this paper, we propose a reinforcement learn-
ing framework to improve the performance of re-
lation reasoning in KGs. Speciﬁcally, we train a
RL agent to ﬁnd reasoning paths in the knowledge
base. Unlike previous path ﬁnding models that are
based on random walks, the RL model allows us
to control the properties of the found paths. These
effective paths can also be used as an alternative to
PRA in many path-based reasoning methods. For
two standard reasoning tasks, using the RL paths
as reasoning formulas, our approach generally out-
performs two classes of baselines.

For

future studies, we plan to investigate
the possibility of incorporating adversarial learn-
ing (Goodfellow et al., 2014) to give better re-
wards than the human-deﬁned reward functions
Instead of designing rewards
used in this work.
according to path characteristics, a discriminative
model can be trained to give rewards. Also, to ad-
dress the problematic scenario when the KG does
not have enough reasoning paths, we are interested
in applying our RL framework to joint reasoning
with KG triples and text mentions.

We gratefully acknowledge
support of
NVIDIA Corporation with the donation of one Ti-
tan X Pascal GPU used for this research.

the

References

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. ACM.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems, pages 2787–2795.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010a. Toward an architecture for never-
ending language learning. In AAAI.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010b. Toward an architecture for never-
In Proceedings of the
ending language learning.
Twenty-Fourth Conference on Artiﬁcial Intelligence
(AAAI 2010).

Rajarshi Das, Arvind Neelakantan, David Belanger,
and Andrew McCallum. 2017. Chains of reasoning
over entities, relations, and text using recurrent neu-
ral networks. EACL.

Ni Lao, Jun Zhu, Xinwang Liu, Yandong Liu, and
William W Cohen. 2010. Efﬁcient relational learn-
ing with hidden variable detection. In NIPS, pages
1234–1242.

Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
Improving learning
and Tom M Mitchell. 2013.
and inference in a large knowledge-base using latent
syntactic cues. In EMNLP, pages 833–838.

Matt Gardner, Partha Pratim Talukdar, Jayant Krishna-
murthy, and Tom Mitchell. 2014. Incorporating vec-
tor space similarity in random walk inference over
knowledge bases.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in Neural Information
versarial nets.
Processing Systems, pages 2672–2680.

Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space.
In
EMNLP.

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al. 2012. Deep neural networks for
acoustic modeling in speech recognition: The shared
views of four research groups. IEEE Signal Process-
ing Magazine, 29(6):82–97.

Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun
Zhao. 2015. Knowledge graph embedding via dy-
namic mapping matrix. In ACL (1), pages 687–696.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zit-
Inferring and exe-
nick, and Ross Girshick. 2017.
cuting programs for visual reasoning. arXiv preprint
arXiv:1705.03633.

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012.
Imagenet classiﬁcation with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.

Ni Lao, Tom Mitchell, and William W Cohen. 2011a.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 529–539. Association for Computa-
tional Linguistics.

Ni Lao, Tom M. Mitchell, and William W. Cohen.
2011b. Random walk inference and learning in a
large scale knowledge base. In EMNLP, pages 529–
539. ACL.

Chen Liang, Jonathan Berant, Quoc Le, Kenneth D
Neural symbolic
Forbus, and Ni Lao. 2016.
machines: Learning semantic parsers on free-
arXiv preprint
base with weak supervision.
arXiv:1611.00020.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In AAAI,
pages 2181–2187.

Volodymyr Mnih, Koray Kavukcuoglu, David Sil-
ver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. 2013. Playing atari
with deep reinforcement learning. arXiv preprint
arXiv:1312.5602.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. Na-
ture, 518(7540):529–533.

Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional vector space mod-
els for knowledge base completion. arXiv preprint
arXiv:1504.06662.

David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George Van Den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, et al. 2016. Mastering
the game of go with deep neural networks and tree
search. Nature, 529(7587):484–489.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases. In EMNLP, volume 15, pages
1499–1509. Citeseer.

William Yang Wang and William W Cohen. 2015.
Joint information extraction and reasoning: A scal-
able statistical relational learning approach. In ACL.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In AAAI, pages 1112–1119.
Citeseer.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
Jun Zhao, et al. 2014. Relation classiﬁcation via
In COLING,
convolutional deep neural network.
pages 2335–2344.

