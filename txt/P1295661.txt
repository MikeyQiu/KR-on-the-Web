Convolutional Gated Recurrent Networks for Video Segmentation

Mennatullah Siam ∗

Sepehr Valipour ∗ Martin Jagersand

Nilanjan Ray

University of Alberta
{mennatul,valipour,mj7,nray1}@ualberta.ca

6
1
0
2
 
v
o
N
 
1
2
 
 
]

V
C
.
s
c
[
 
 
2
v
5
3
4
5
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

Semantic segmentation has recently witnessed major
progress, where fully convolutional neural networks have
shown to perform well. However, most of the previous work
focused on improving single image segmentation. To our
knowledge, no prior work has made use of temporal video
information in a recurrent network. In this paper, we in-
troduce a novel approach to implicitly utilize temporal data
in videos for online semantic segmentation. The method re-
lies on a fully convolutional network that is embedded into
a gated recurrent architecture. This design receives a se-
quence of consecutive video frames and outputs the seg-
mentation of the last frame. Convolutional gated recurrent
networks are used for the recurrent part to preserve spa-
tial connectivities in the image. Our proposed method can
be applied in both online and batch segmentation. This ar-
chitecture is tested for both binary and semantic video seg-
mentation tasks. Experiments are conducted on the recent
benchmarks in SegTrack V2, Davis, CityScapes, and Syn-
thia. Using recurrent fully convolutional networks improved
the baseline network performance in all of our experiments.
Namely, 5% and 3% improvement of F-measure in Seg-
Track2 and Davis respectively, 5.7% improvement in mean
IoU in Synthia and 3.5% improvement in categorical mean
IoU in CityScapes. The performance of the RFCN network
depends on its baseline fully convolutional network. Thus
RFCN architecture can be seen as a method to improve its
baseline segmentation network by exploiting spatiotempo-
ral information in videos.

1. Introduction

Semantic segmentation, which provides pixel-wise
labels, has witnessed a tremendous progress recently.
As shown in [14][16][29][25],
it outputs dense predic-
tions and partitions the image to semantically meaningful
parts. It has numerous applications including autonomous
driving[28][21][7], augmented reality[15] and robotics[23]

∗Authors contributed equally

[26]. The work in [14] presented a fully convolutional net-
work and provides a method for end-to-end training of se-
mantic segmentation. It yields a coarse heat-map followed
by in-network upsampling to get dense predictions. Fol-
lowing the work of fully convolutional networks, many at-
tempts were made to improve single image semantic seg-
mentation. In [16] a full deconvolution network is presented
with stacked deconvolution layers. The work in [25] pro-
vided a method to incorporate contextual information using
recurrent neural networks. However, one missing element
is that the real-world is not a set of still images. In real-time
camera or recorded video, much information is perceived
from temporal cues. For example, the difference between
a walking or standing person is hardly recognizable in still
images but it is obvious in a video.

Figure 1: Overview of the Proposed Method of Recurrent
FCN. The recurrent part is unrolled for better visualisation

Video segmentation has been extensively investigated
using classical approaches. The work in [18] reviews the lit-
erature in binary video segmentation. It mainly focuses on
semi-supervised approaches[1] [19][20] that propagate the
labels in one or more annotated frames to the entire video.
In [17] a method that uses a combination of Recurrent Neu-
ral Networks (RNN) and CNN for RGB-D video segmen-
tation is presented. However, their proposed architecture is
difﬁcult to train because of the vanishing gradient. It does
not utilize pre-trained networks and it cannot process large
images as number of their parameters is quadratic with re-
spect to the input size.

1

Gated Recurrent Architectures alleviate the vanishing
or exploding gradients problem in recurrent networks. Long
Short Term Memory (LSTM) architecture is one of the ear-
liest attempt to design these networks[10]. They are suc-
cessfully employed in many tasks. Captioning and text pro-
cessing in particular [11] [8]. Gated Recurrent Unit (GRU)
[5] is a more recent gated architecture. It is shown that GRU
has similar performance to LSTM but with reduced num-
ber of gates thus fewer parameters[6]. The main bottleneck
with these previous architectures is that they only work with
vectors and therefore, do not preserve spatial information in
images or feature maps. In[2] convolutional GRU is intro-
duced for learning spatio-temporal features from videos and
used for video captioning and action recognition.

Inspired by these methods we design a gated recurrent
FCN architecture to solve many of the shortcomings of the
previous approaches. Contributions include:

• A novel architecture that can incorporate temporal data
into FCN for video segmentation. A Convolutional
Gated Recurrent FCN architecture is designed to ef-
ﬁciently utilize spatiotemporal information.

• An end-to-end training method for online video seg-

mentation.

• An experimental analysis on video binary segmenta-
tion and video semantic segmentation is presented on
recent benchmarks.

An overview of the suggested method is shown in Figure
1. Where a sliding window of input video frames is fed
to the recurrent fully convolutional network(RFCN) and re-
sulted in the segmentation of the last frame. The paper is
structured as follows. In Section 2 necessary background
is discussed. The proposed method is presented in details
in section 3. Section 4 presents experimental results and
discussion on recent benchmarks. Finally, section 5 sum-
marizes the article and presents potential future directions.

2. Background

This section will review FCN and RNN which will be

repeatedly referred to throughout the article.

2.1. Fully Convolutional Networks (FCN)

Convolutional neural networks that are initially designed
with image classiﬁcation tasks in mind. Later, it became ap-
parent that CNN can also be used for segmentation by doing
pixel-wise classiﬁcation. However, dense pixel-wise label-
ing is extremely inefﬁcient using regular CNN. In [13] the
idea of using a fully convolutional neural network that is
trained for pixel-wise semantic segmentation is presented.
In this approach, all the fully connected layers of CNN net-
works are replaced with convolutional layers. This design

allows the network to accommodate any input size since it
is not restricted to a ﬁxed output size, fully connected lay-
ers. More importantly, now it is possible to get a course
segmentation output (called heat-map) by only one forward
pass of the network.

This coarse map needs to be up-sampled to the original
size of the input image. Simple bi-linear interpolation can
be used however, an adaptive up-sampling is shown to have
a better result. In [13] a new layer with learnable ﬁlters that
applies up-sampling within the network is presented. It is
an efﬁcient way to learn the up-sampling weights through
back-propagation. These types of layers are commonly
known as deconvolution. The ﬁlters of deconvolution layers
can be seen as a basis to reconstruct the input image or just
to increase the spatial size of feature maps. Skip architec-
ture can be used for an even ﬁner segmentation. In this ar-
chitecture heat maps from earlier pooling layers are merged
with the ﬁnal heatmap for an improved segmentation. This
architecture is termed as FCN-16s or FCN-8s based on the
pooling layers that are used.

2.2. Recurrent Neural Networks

Recurrent Neural Networks[24] can be applied on a se-
quence if inputs and are able to capture the temporal relation
between them. A hidden unit in each recurrent cell allows
it to have a dynamic memory that is changing according to
what it had hold before and the new input. The simplest
recurrent unit can be modeled as in equation 1.

ht = θφ(ht−1) + θxxt
yt = θyφ(ht)

(1a)

(1b)

Where, h is the hidden unit, x is the input, y is the output, t
is the current time step and φ is the activation function.

When propagating the error in recurrent units, due to the
chain law, we see that the derivative of each node is depen-
dent on all of earlier nodes. This chain dependency can be
arbitrary long based on length of the input vector. It was
observed that it will cause vanishing gradient problem, es-
pecially for longer input vectors[4]. Gated recurrent archi-
tectures have been proposed as a solution and they were em-
pirically successful in many tasks. Two popular choices of
these architectures are presented in this section.

2.2.1 Long Short Term Memory (LSTM)

LSTM[10] utilizes three gates to control the ﬂow of signal
within the cell. These gates are input, output and forget
gate and each of them has its own set of weights. These
weights can be learned with back-propagation. At the infer-
ence stage, the values in the hidden unit changes based on
the sequence of inputs that is has seen and can be roughly
interpreted as a memory. This memory can be used for the
prediction of the current state. Equations 2 shows how the

2

gates values and hidden states are computed. it, ft and ot
are the gates and ct and ht are the internal and the hidden
state respectively.

it = σ(Wxixt + Whiht−1 + bi)
ft = σ(Wxf xt + Whf ht−1 + bf )
ot = σ(Wxoxt + Whoht−1 + bo)
gt = σ(Wxcxt + Whcht−1 + bc)
ct = ft (cid:12) ct−1 + it (cid:12) gt
ht = ot (cid:12) φ(ct)

(2a)

(2b)

(2c)

(2d)

(2e)

(2f)

2.2.2 Gated Recurrent Unit (GRU)

GRU uses the same gated principal of LSTM but with a
simpler architecture. Therefore, it is not as computationally
expensive as LSTM and uses less memory. Equations 3 de-
scribe the mathematical model of the GRU. Here, rt and zt
are the gates and ht is the hidden state.

zt = σ(Whzht−1 + Wxzxt + bz)
rt = σ(Whrht−1 + Wxrxt + br)
ˆht = Φ(Wh(rt (cid:12) ht−1) + Wxxt + b)
ht = (1 − zt) (cid:12) ht−1 + z (cid:12) ˆht

(3a)

(3b)

(3c)

(3d)

GRU is simpler than LSTM since the output gate is re-
moved from the cell and the output ﬂow is controlled by two
other gates indirectly. The cell memory is also updated dif-
ferently in GRU. LSTM updates its hidden state by summa-
tion over ﬂow after input gate and forget gate. In the other
hand, GRU assumes a correlation between memorizing and
forgetting and controls both by one gate only zt.

3. Method

An overview of the method is presented in Figure 1. A
recurrent fully convolutional network (RFCN) is designed
that utilizes the spatiotemporal information for video seg-
mentation. The recurrent unit in the network can either be
LSTM, GRU or Conv-GRU (which is explained in 3.2). A
sliding window over the video frames is used as input to
the network. This allows on-line video segmentation as op-
posed to off-line batch processing. The window data is for-
warded through the RFCN to yield a segmentation for the
last frame in the sliding window. Note that the recurrent
unit can be applied on the coarse segmentation (heat map)
or intermediate feature maps. The network is trained in an
end-to-end fashion using pixel-wise classiﬁcation logarith-
mic loss. Two main approaches are explored in our method:
(1) conventional recurrent units, and (2) convolutional re-
current units 1. Speciﬁcally, four different network archi-
tectures under these two approaches are used as detailed in
the following sections.

3.1. Conventional Recurrent Architecture for Seg-

mentation

RFC-Lenet is a fully convolutional version of Lenet.
Lenet is a well known shallow network. Because it is com-
mon, we used it for baseline comparisons on synthetic data.
We embed this model in a recurrent node to capture tempo-
ral data. The ﬁnal network is named as RFC-Lenet in Table
1.

The output of deconvolution a 2D map of dense predic-
tions that is then ﬂattened into 1D vector as the input to a
conventional recurrent unit. The recurrent unit takes this
vector for each frame in the sliding window and outputs the
segmentation of the last frame (Figure 1).

RFC-12s is another architecture that is used for baseline
comparisons, to compare end-to-end and decoupled train-
ing as detailed in section 4. The RFC-Lenet architecture
requires a large weight matrix in the recurrent unit since
it processes vectors of the ﬂattened full sized image. One
way to overcome this problem is to apply the recurrent layer
on the down-sampled heatmap before deconvolution. This
leads to this second architecture termed as RFC-12s in Ta-
ble 1. In this network, vectorized coarse output maps are
given to the recurrent unit. The recurrent unit operates on a
sequence of these coarse maps and produces a coarse map
corresponding the last frame in the sequence. Later, the de-
convolution layer generates dense prediction from the out-
put the recurrent unit. In this way, the recurrent unit is al-
lowed to work on much smaller vectors and therefore re-
duces the variance in the network.

3.2. Convolutional Gated Recurrent Architecture

(Conv-GRU) for Segmentation

Conventional recurrent units are designed for process-
ing text data not images. Therefore, using them on images
without any modiﬁcation causes two main issues. 1) The
size of weight parameters becomes very large since vector-
ized images are large 2) Spatial connectivity between pixels
are ignored. For example, using a recurrent unit on a feature
map with the spatial size of h × w and number of channels
c requires c × (h.w)2 number of weights. This will cause
a memory bottleneck and inefﬁcient computations. It will
also create a larger search space for the optimizer, thus it
will be harder to train.

Convolutional recurrent units, akin to regular convolu-
tional layer, convolve three dimensional weights with their
input. Therefore, to convert a gated architecture to a convo-
lutional one, dot products should be replaced with convolu-
tions. Equations 4 show this modiﬁcation for the GRU. The
weights are of size of kh × kw × c × f where kh, kw, c and
f are kernel’s height and width, number of input channels,
and number of ﬁlters, respectively. Learning ﬁlters that con-
volve with the entire image instead of learning individual
weights for each pixel, makes it much more efﬁcient. This

3

Table 1: Proposed networks in detail. F (n) is a ﬁlter with size of n × n. P (n) indicates n zero padding. S(n) shows the
stride in the convolution layer. D(n) is number of feature maps generated by the layer n (It is same as the previous layer if it
is not mentioned).

RFC-Lenet
input: 28×28

Conv: F(5), P(10), D(20)
Relu
Pool 2×2
Conv: F(5), D(50)
Relu
Pool(2×2)
Conv: F(3), D(500)
Relu
Conv: F(1), D(1)

e
d
o
N

t
n
e
r
r
u
c
e
R

Network Architectures

RFC-12s
input: 120×180

Conv: F(5), S(3), P(10), D(20)
Relu
Pool 2×2
Conv: F(5), D(50)
Relu
Pool(2×2)
Conv: F(3), D(500)
Relu
Conv: F(1), D(1)

e
d
o
N

t
n
e
r
r
u
c
e
R

e
d
o
N

t
n
e
r
r
u
c
e
R

-

-

DeConv: F(10), S(4)
Flatten
GRU: W(784×784)

Flatten
GRU: W(100×100)
DeConv: F(10), S(4)

RFC-VGG
input: 240×360

Conv: F(11), S(4), P(40), D(64)
Relu
Pool 3×3
Conv: F(5), P(2) D(256)
Relu
Pool(3×3)
Conv: F(3), P(1) D(256)
Relu
Conv: F(3), P(1) D(256)
Relu
Conv: F(3), P(1) D(256)
Relu
Conv: F(3), D(512)
Conv: F(3), D(128)
ConvGRU: F(3), D(128)
Conv: F(1), D(1)
DeConv: F(20), S(8)

Figure 2: RFC-VGG architecture. A sequence of images is given as input to the network. The output of the embedded FC
inside the recurrent unit is given to a Conv-GRU layer. One last convolutional layer maps the output of the recurrent unit into
a coarse segmentation map. Then, the deconvolutional layer up-samples the coarse segmentation into a dense segmentation.

layer can be applied on either ﬁnal heat map or intermediate
feature maps.

zt = σ(Whz ∗ ht−1 + Wxz ∗ xt + bz)
rt = σ(Whr ∗ ht−1 + Wxr ∗ xt + br)
ˆht = Φ(Wh ∗ (rt (cid:12) ht−1) + Wx ∗ xt + b)
ht = (1 − zt) (cid:12) ht−1 + z (cid:12) ˆht

(4a)

(4b)

(4c)

(4d)

RFC-VGG in Table 1 is an example of this approach, where

intermediate feature maps are fed into a convolutional gated
recurrent unit. Then a convolutional layer converts its out-
put to a heat map.
It is based on VGG-F [22] network.
The reason for switching to the RFC-VGG architecture is to
use pre-trained weights from VGG-F. Initializing weights of
our ﬁlters by VGG-F trained weights, alleviates over-ﬁtting
problems as these weights are the result of extensive train-
ing on Imagenet dataset. The last two pooling layers are
dropped from VGG-F to allow a ﬁner segmentation with a

4

reduced network. Figure 2 shows the detailed architecture
of RFC-VGG.

RFCN-8s is the recurrent version of FCN-8s architec-
ture and is used in our semantic segmentation experiments.
FCN-8s network is commonly used in many state of the art
segmentation methods as it provides more detailed segmen-
tation.
It is loaded with a pre-trained with VGG-16 and
it employs the skip architecture that combines pool3 and
pool4 layers, with the ﬁnal layer to have a ﬁner segmenta-
tion. In RFCN-8s the convolutional gated recurrent unit is
placed before pool3 layer where the skip connections start
branching.

4. Experiments

This section describes the experimental analysis and re-
sults. First, the datasets are presented followed by the train-
ing method and hyper-parameters used. Then both quantita-
tive and qualitative analyses are presented. All experiments
are performed on our implemented open source library that
supports convolutional gated recurrent architectures. The
implementation is based on Theano [3] and supports using
different FCN architectures as a recurrent node. The key
features of this implementation are: (1) The ability to use
any arbitrary CNN or FCN architecture as a recurrent node.
(2) Support for
In order to utilize temporal information.
three gated recurrent architectures which are, LSTM, GRU,
and Conv-GRU. (3) It includes deconvolution layer for in
the network upsampling and supports skip architecture for
ﬁner segmentation. A public version of the code for the li-
brary along with the trained models will be published after
the anonymous review.

4.1. Datasets

In this paper six datasets are used: 1) Moving MNIST.
2) Change detection[9]. 3) Segtrack version 2[12]. 4)
Densely Annotated VIdeo Segmentation (Davis) [18]. 5)
Synthia[21]. 6) CityScapes[7]

Moving MNIST dataset is synthesized from original
MNIST by moving the characters in random but consis-
tent directions. The labels for segmentation is generated
by thresholding input images after translation. We consider
each translated image as a new frame. Therefore we can
have arbitrary length image sequences.

Change Detection Dataset[9] This dataset provides re-
alistic, diverse set of videos with pixel-wise labeling of
moving objects. The dataset includes both indoor and out-
door scenes. It focuses on moving object segmentation. In
the foreground detection, videos with similar objects were
selected such as humans or cars. Accordingly, we chose
six videos: Pedestrians, PETS2006, Badminton, CopyMa-
chine, Ofﬁce, and Sofa.

SegTrack V2[12] is a collection of fourteen video se-
quences with objects of interest manually segmented. The

dataset has sequences with both single or multiple objects.
In the latter case, we consider all the segmented objects as
one and we perform foreground segmentation.

Davis[18] dataset includes ﬁfty densely annotated videos
with pixel accurate groundtruth for the most salient object.
Multiple challenges are included in the dataset such as oc-
clusions, illumination changes, fast motion, motion blur and
nonlinear deformation.

Synthia[21] is a synthetic semantic segmentation dataset
It contains pixel level annotations for
for urban scenes.
thirteen classes.
It has over 200,000 images with differ-
ent weather conditions (rainy, sunset, winter) and seasons
(summer, fall). Since the dataset is large only a portion of
it from Highway sequence for summer condition is used for
our experiments.

CityScapes[7] is a real dataset focused on urban scenes
gathered by capturing videos while driving in different
cities. It contains 5000 ﬁnely annotated 20000 coarsely an-
notated images for thirty classes. The coarse annotation in-
cludes segmentation for all frames in the video and each
twentieth image in the video sequence is ﬁnely annotated.
It provides various locations (ﬁfty cities) and weather con-
ditions throughout different seasons.

4.2. Results

The main experiments’ setup includes using Adadelta
[27] for optimization as it gave much faster convergence
than stochastic gradient descent. The loss function used
throughout the experiments is the logistic loss, and the max-
imum number of epochs used for training is 500. The eval-
uation metrics used for the binary video segmentation is
precision, recall, F-measure and IoU. Metrics formulation
is shown in 5, 6 and 7 where tp, fp, fn denote true posi-
tives, false positives, and false negatives respectively. As
for multi-class segmentation mean class IoU, per-class IoU,
mean category IoU and per-category IoU is used. Note that
category IoU considers only category of classes instead of
the speciﬁc classes when computing tp, fp and fn.

precision =

, recall =

tp
tp + f p

tp
tp + f n

F − measure =

2 ∗ precision ∗ recall
precision + recall

IoU =

tp
tp + f p + f n

(5)

(6)

(7)

In the ﬁrst set pf experiments conducted, a fully con-
volutional VGG is used as a baseline denoted as FC-VGG
and is compared against the recurrent version RFC-VGG.
To avoid overﬁtting, ﬁrst ﬁve layers of the network are ini-
tialized with the weights of a pre-trained networked and
only lightly tuned. Table 2 shows the results of the experi-
ments on SegTrackV2 and DAVIS datasets. In these exper-
iments, the data is split into half for training and the other

5

Figure 3: Qualitative results of experiments with SegtracV2 and Davis datasets, where network prediction are overlaid on the
input. The top row is for FC-VGG and the bottom row is for RFC-VGG.

Table 2: Comparison of RFC-VGG with its baseline counterpart on DAVIS and SegTrack

SegTrack V2

DAVIS

FC-VGG

RFC-VGG

FC-VGG

RFC-VGG

Precision

Recall

F-measure

IoU

0.7759

0.6810

0.7254

0.7646

0.8325

0.7280

0.7767

0.8012

0.6834

0.5454

0.6066

0.6836

0.7233

0.5586

0.6304

0.6984

FC-VGG Extra Conv

0.7519

0.7466

0.7493

0.7813

half as keep out test set. RFC-VGG outperforms the FC-
VGG architecture on both datasets with about 3% and 5%
on DAVIS and SegTrack respectively. A comparison be-
tween using RFC-VGG versus using an extra convolutional
layer with the same ﬁlter size (FC-VGG Extra Conv) is also
presented. This result ensures that using the recurrent net-
work to employ temporal data is the reason for the boost
of performance not just merely adding extra convolutional
ﬁlters.

Figure 3 shows the qualitative analysis of RFC-VGG
against FC-VGG. It shows that utilizing temporal informa-
tion through the recurrent unit gives better segmentation for
the object. This can be contributed to the implicit learning
of the motion of segmented objects in the recurrent units.
It also shows that using conv-GRU as the recurrent unit en-
ables the extraction of temporal information from feature
maps. Note that the performance of the RFCN network de-
pends on its baseline fully convolutional network. Thus,
RFCN networks can be seen as a method to improve their
baseline segmentation network by embedding them into a
recurrent module that utilizes temporal data.

The same architecture was used for semantic segmen-
tation on synthia dataset after modifying it to support the
thirteen classes. A comparison between FC-VGG and RFC-
VGG is presented in terms of mean class IoU and per-class

IoU for some of the classes. Table3 presents the results
on synthia dataset. RFC-VGG has 5.7% over FC-VGG in
terms of mean class IoU. It also shows the per-class IoU
generally improves in the case of RFC-VGG. Interestingly,
the highest improvement is with the car and pedestrian class
that beneﬁts the most from a learned motion pattern com-
pared to sky or buildings that are mostly static. Figure4 ﬁrst
row shows the qualitative analysis on Synthia. The second
image shows the car’s enhanced segmentation with RFC-
VGG.

Finally, experimental results on cityscapes dataset using
FCN-8s and its recurrent version RFCN-8s is shown in Ta-
ble4. It uses mean category IoU and per-category IoU for
the evaluation. It clearly demonstrates that RFCN-8s out-
performs FCN-8s with 3.5% on mean category IoU. RFCN-
8s generally improves on the per-category IoU, with the
highest improvement in vehicle category. Hence, again the
highest improvement is in the category that is affected the
most by temporal data. Figure 4 bottom row shows the
qualitative evaluation on cityscapes data to compare FCN-
8s versus RFCN-8s. The third image clearly shows that the
moving bus is better segmented with the recurrent version.
Note that the experiments were conducted on images with
less resolution than the original data and with a reduced ver-
sion of FCN-8s due to memory constraints. Therefore, ﬁner

6

Figure 4: Qualitative results of experiments with Synthia and cityscapes datasets, where network prediction are overlaid on
the input. First row: Synthia with FC-VGG. Second row: Synthia with RFC-VGG. Third row: CityScapes with FCN-8s.
Forth row: CityScapes with RFCN-8s:

Table 3: Semantic Segmentation Results on Synthia Highway Summer Sequence for RFC-VGG compared to FC-VGG

Mean Class IoU

Per-Class IoU

Car

Pedestrian

Sky

Building Road

Sidewalk

Fence Vegetation

Pole

FC-VGG

RFC-VGG

0.755

0.812

0.504

0.275

0.946

0.958

0.840

0.957

0.762

0.883

0.718

0.566

0.487

0.964

0.961

0.907

0.968

0.865

0.909

0.742

Table 4: Semantic Segmentation Results on CityScapes for RFCN-8s compared to FCN-8s

Category IoU

Per-category IoU

Flat

Nature

Sky

Construction Vehicle

FCN-8s

0.53

0.917

0.710

0.792

0.683

RFCN-8s

0.565

0.928

0.739

0.814

0.719

0.585

0.652

categories such as human and object are poorly segmented.
However, using original resolution will ﬁx this problem and
its recurrent version should have better results as well.

4.3. Additional Experiments

In this section, experiments using conventional recurrent
layers for segmentation is presented. These experiments

7

provide further analysis on different recurrent units and
their effects on the RFCN. A comparison between end-to-
end training versus the decoupled one is also presented.
The moving MNIST and change detection datasets are
used for this part. Images of MNIST dataset are relatively
small (28×28) which allows us to test our RFC-Lenet
network 1. A fully convolutional Lenet
is compared
against RFC-Lenet. Table 5 shows the results that were
obtained. The results of RFC-Lenet with GRU is better
than FC-Lenet with 2% improvement. Note that GRU gave
better results than LSTM in as well.

Table 5: Precision, Recall, and F-measure on FC-Lenet,
LSTM, GRU, and RFC-Lenet tested on the moving MNIST
dataset

Precision

Recall

F-measure

FC-Lenet

LSTM

GRU

RFC-Lenet

0.868

0.941

0.955

0.96

0.922

0.786

0.877

0.877

0.894

0.856

0.914

0.916

We used real data from motion detection benchmark for
the second set of experiments. The training and test splits
are 70% and 30% from each sequence throughout these
experiments. Baseline FC-12s is compared against its re-
current version, RFC-12s. It is also compared against the
decoupled training of the FC-12s and the recurrent unit.
Where GRU is trained on the heat map output from FC-12s.
Table 6 shows the results of these experiments, where the
RFC-12s network had a 1.4% improvement over FC-12s.
We observe less relative improvement compared to using
Conv-GRU because in regular GRU spatial connectivities
are ignored. However, incorporating the temporal data still
helped the segmentation accuracy.

5. Conclusion and Future Work

We presented a novel method that exploits implicit tem-
poral information in videos to improve segmentation. This
approach utilizes convolutional gated recurrent network
which allows it to use preceding frames in segmenting the
current frame. We performed extensive experiments on six
datasets with different segmentation objective. We showed
that embedding FCN networks as a recurrent module,
consistently improved the results through out different
datasets. Speciﬁcally, a 5% improvement in Segtrack and
3% improvement in Davis in F-measure over a plain fully

Table 6: Precision, Recall, and F-measure on motion detec-
tion dataset. FCN-12s is the baseline FCN network. RFC-
12s is its counterpart recurrent network. RFC-12s is trained
in two ways. Decoupled (D) where ﬁrst, the GRU layer
alone is trained with the rest of the network ﬁxed and then
the whole network ﬁnely tuned together. End-to-end (EE)
where the whole network is trained at once.

Precision

Recall

F-measure

FC-12s

0.827

0.585

0.685

RFC-12s (D)

0.835

0.587

RFC-12s (EE)

0.797

0.623

0.69

0.7

convolutional network; a 5.7% improvement on Synthia in
mean IoU, and 3.5% improvement on CityScapes in mean
category IoU, over the plain fully convolutional network.
Our suggested architecture can be applied on any FCN
like single frame segmentation and then be used to process
videos in an online fashion with an improved performance.

For future work, we would like to enhance the results of
the semantic segmentation and apply our recurrent method
to more single-image segmentation networks, for a more
complete comparison with the state of the art. Another di-
rection is to explore the potential of incorporating shape
constraints from the depth data within the network. Thus
combining motion and shape cues for better video segmen-
tation.

References

[1] V. Badrinarayanan, F. Galasso, and R. Cipolla. Label prop-
In Computer Vision and Pat-
agation in video sequences.
tern Recognition (CVPR), 2010 IEEE Conference on, pages
3265–3272. IEEE, 2010.

[2] N. Ballas, L. Yao, C. Pal, and A. Courville. Delving deeper
into convolutional networks for learning video representa-
tions. arXiv preprint arXiv:1511.06432, 2015.

[3] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Good-
fellow, A. Bergeron, N. Bouchard, and Y. Bengio. Theano:
new features and speed improvements. Deep Learning and
Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
[4] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term
dependencies with gradient descent is difﬁcult. Neural Net-
works, IEEE Transactions on, 5(2):157–166, 1994.

[5] K. Cho, B. van Merri¨enboer, D. Bahdanau, and Y. Bengio.
On the properties of neural machine translation: Encoder-
decoder approaches. arXiv preprint arXiv:1409.1259, 2014.
[6] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical
evaluation of gated recurrent neural networks on sequence
modeling. arXiv preprint arXiv:1412.3555, 2014.

8

images for semantic segmentation of urban scenes. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3234–3243, 2016.

[22] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[23] V. Vineet, O. Miksik, M. Lidegaard, M. Nießner,
S. Golodetz, V. A. Prisacariu, O. K¨ahler, D. W. Murray,
S. Izadi, P. Perez, and P. H. S. Torr. Incremental dense se-
mantic stereo fusion for large-scale semantic scene recon-
struction. In IEEE International Conference on Robotics and
Automation (ICRA), 2015.

[24] O. Vinyals, S. V. Ravuri, and D. Povey. Revisiting recur-
rent neural networks for robust asr. In Acoustics, Speech and
Signal Processing (ICASSP), 2012 IEEE International Con-
ference on, pages 4085–4088. IEEE, 2012.

[25] F. Visin, K. Kastner, A. Courville, Y. Bengio, M. Matteucci,
and K. Cho. Reseg: A recurrent neural network for object
segmentation. arXiv preprint arXiv:1511.07053, 2015.
[26] D. Wolf, J. Prankl, and M. Vincze. Enhancing semantic seg-
mentation for robotics: The power of 3-d entangled forests.
IEEE Robotics and Automation Letters, 1(1):49–56, 2016.

[27] M. D. Zeiler. Adadelta: an adaptive learning rate method.

arXiv preprint arXiv:1212.5701, 2012.

[28] H. Zhang, A. Geiger, and R. Urtasun. Understanding high-
level semantics by modeling trafﬁc patterns. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 3056–3063, 2013.

[29] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. H. Torr. Conditional random
In Proceedings of the
ﬁelds as recurrent neural networks.
IEEE International Conference on Computer Vision, pages
1529–1537, 2015.

[7] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
arXiv preprint arXiv:1604.01685, 2016.
[8] J. Donahue, L. Anne Hendricks,

S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual
In Proceedings of the IEEE
recognition and description.
Conference on Computer Vision and Pattern Recognition,
pages 2625–2634, 2015.

[9] N. Goyette, P.-M. Jodoin, F. Porikli, J. Konrad, and P. Ishwar.
Changedetection. net: A new change detection benchmark
dataset. In Computer Vision and Pattern Recognition Work-
shops (CVPRW), 2012 IEEE Computer Society Conference
on, pages 1–8. IEEE, 2012.

[10] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[11] J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully
convolutional localization networks for dense captioning.
arXiv preprint arXiv:1511.07571, 2015.

[12] F. Li, T. Kim, A. Humayun, D. Tsai, and J. M. Rehg. Video
segmentation by tracking many ﬁgure-ground segments. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 2192–2199, 2013.

[13] G. Lin, C. Shen, I. Reid, et al. Efﬁcient piecewise training
of deep structured models for semantic segmentation. arXiv
preprint arXiv:1504.01013, 2015.

[14] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3431–3440, 2015.

[15] O. Miksik, V. Vineet, M. Lidegaard, R. Prasaath, M. Nießner,
S. Golodetz, S. L. Hicks, P. P´erez, S. Izadi, and P. H. Torr.
The semantic paintbrush: Interactive 3d mapping and recog-
nition in large outdoor spaces. In Proceedings of the 33rd
Annual ACM Conference on Human Factors in Computing
Systems, pages 3317–3326. ACM, 2015.

[16] H. Noh, S. Hong, and B. Han. Learning deconvolution net-
work for semantic segmentation. In Proceedings of the IEEE
International Conference on Computer Vision, pages 1520–
1528, 2015.

[17] M. S. Pavel, H. Schulz, and S. Behnke. Recurrent convolu-
tional neural networks for object-class segmentation of rgb-
In Neural Networks (IJCNN), 2015 International
d video.
Joint Conference on, pages 1–8. IEEE, 2015.

[18] F. Perazzi, J. P.-T. B. McWilliams, L. Van Gool, M. Gross,
and A. Sorkine-Hornung. A benchmark dataset and evalua-
tion methodology for video object segmentation.

[19] F. Perazzi, O. Wang, M. Gross, and A. Sorkine-Hornung.
Fully connected object proposals for video segmentation. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 3227–3234, 2015.

[20] S. A. Ramakanth and R. V. Babu. Seamseg: Video object
segmentation using patch seams. In 2014 IEEE Conference
on Computer Vision and Pattern Recognition, pages 376–
383. IEEE, 2014.

[21] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M.
Lopez. The synthia dataset: A large collection of synthetic

9

