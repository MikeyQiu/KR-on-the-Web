8
1
0
2
 
t
c
O
 
3
 
 
]

G
L
.
s
c
[
 
 
2
v
5
1
1
1
1
.
9
0
8
1
:
v
i
X
r
a

Weighted Spectral Embedding of Graphs

Thomas Bonald1, Alexandre Hollocou2, and Marc Lelarge2

1Telecom ParisTech, Paris, France
2Inria, Paris, France

October 4, 2018

Abstract

We present a novel spectral embedding of graphs that incorporates weights assigned to the nodes,
quantifying their relative importance. This spectral embedding is based on the ﬁrst eigenvectors of
some properly normalized version of the Laplacian. We prove that these eigenvectors correspond to the
conﬁgurations of lowest energy of an equivalent physical system, either mechanical or electrical, in which
the weight of each node can be interpreted as its mass or its capacitance, respectively. Experiments on
a real dataset illustrate the impact of weighting on the embedding.

1

Introduction

Many types of data can be represented as graphs. Edges may correspond to actual links in the data (e.g.,
users connected by some social network) or to levels of similarity induced from the data (e.g., users having
liked a large common set of movies). The resulting graph is typically sparse in the sense that the number of
edges is much lower than the total number of node pairs, which makes the data hard to exploit.

A standard approach to the analysis of sparse graphs consists in embedding the graph in some vectorial
space of low dimension, typically much smaller than the number of nodes [15, 19, 4]. Each node is represented
by some vector in the embedding space so that close nodes in the graph (linked either directly or through
many short paths in the graph) tend to be represented by close vectors in terms of the Euclidian distance.
Standard learning techniques can then be applied to this dense vectorial representation of the graph to
recommend new links, rank nodes or ﬁnd clusters of nodes for instance [7, 3].

The most popular technique for graph embedding is based on the spectral decomposition of the graph
Laplacian, each dimension of the embedding space corresponding to an eigenvector of the Laplacian matrix
[5, 13, 2, 11, 17, 12]. This spectral embedding can be interpreted in terms of a random walk in the graph
[10, 14]. In the full embedding space (including all eigenvectors), the square distance between two vectors
is proportional to the mean commute time of the random walk between the two corresponding nodes: close
nodes in the graph tend to be close in the embedding space. Viewing this random walk as the path followed
by electrons in the corresponding electrical network, with nodes linked by resistors, these square distances
can also be interpreted as the eﬀective resistances between pairs of nodes [16].

In this paper, we address the issue of the spectral embedding of graphs including node weights. We shall
see that existing spectral embedding techniques implicitly consider either unit weights or so-called internal
node weights, depending on the Laplacian used in the spectral decomposition. We here consider the node
weights as some additional information representing the relative importance of the nodes, independently of
the graph structure. The weight of a node may reﬂect either its value, its multiplicity if each node represents
a category of users or items, or the reliability of the associate data, for instance. Surprisingly, this notion
of weight is common for vectoral data (see, e.g., the weighted version of the k-means clustering algorithm
[8, 6, 9]) but not for graph data, where weights are typically assigned to edges but not to nodes, apart from
those induced from the edges.

1

Our main contribution is a spectral embedding of the graph, we refer to as the weighted spectral em-
bedding, that incorporates the node weights. It is based on the spectral decomposition of some properly
normalized version of the Laplacian. We prove that, when all eigenvectors are used, this embedding is equiv-
alent to the regular spectral embedding shifted so that the origin is the center of mass of the embedding.
In practice, only the ﬁrst eigenvectors are included to get an embedding in low dimension. We show that
these eigenvectors can be interpreted as the levels of lowest energy of a physical system, either a mechanical
system where nodes are linked by springs (the edges) and have diﬀerent masses (the node weights), or an
electrical network where nodes are linked by resistors (the edges) and connected to the ground by capacitors
with diﬀerent capacitances (the node weights). In particular, the weighted spectral embedding can not be
derived from the regular spectral embedding in low dimension. Experiments conﬁrm that these embeddings
diﬀer signiﬁcantly in practice.

The weighted spectral embedding can also be interpreted in terms of a random walk in the graph, where
nodes are visited in proportion to their weights. In the full embedding space (including all eigenvectors), the
square distances between vectors are proportional to the mean commute times of this random walk between
the corresponding nodes, as for unit weights. In fact, these mean commute times depend on the weights
through their sum only, which explains why the pairwise distances of the embedding are independent of
the weights, up to some multiplicative constant. This property is somewhat counter-intuitive as the mean
hitting time of one node from another does depend on the weights. We shall explain this apparent paradox
by some symmetry property of each equivalent physical system.

The rest of the paper is organized as follows. We ﬁrst introduce the model and the notations. We
then present the regular spectral embedding and its interpretation in terms of a random walk in the graph.
The weighted version of this random walk is introduced in Section 4. Section 5 presents the weighted
spectral embedding and extends the results known for the regular spectral embedding. The analogies with
a mechanical system and an electrical network are described in Sections 6 and 7, respectively. Experiments
on real data are presented in Section 8. Section 9 concludes the paper.

2 Model

We consider a connected, undirected graph of n nodes, without self-loops. We denote by A its adjacency
matrix. In the absence of edge weights, this is a binary, symmetric matrix, with Aij = 1 if and only if there
is an edge between nodes i and j. In the presence of edge weights, Aij is the weight of the edge between
nodes i and j, if any, and is equal to 0 otherwise.

Let d = Ae, where e is the n-dimensional vector of ones. The components d1, . . . , dn of the vector d
are equal to the actual node degrees in the absence of edge weights (A is a binary matrix) and to the total
weights of incident edges otherwise (A is a non-negative matrix). We refer to d1, . . . , dn as the internal node
weights.

Nodes are assigned positive weights w1, . . . , wn corresponding to their relative importances. These node

weights are external parameters, independent of the graph. We denote by w the vector (w1, . . . , wn).

3 Spectral embedding

We ﬁrst present the regular spectral embedding, without taking the node weights w1, . . . , wn into account.
Let D = diag(d) be the diagonal matrix of internal node weights. The Laplacian matrix is deﬁned by

This is a symmetric matrix. It is positive semi-deﬁnite on observing that:

Rn,

v
∀

∈

vT Lv = X

Aij(vj

vi)2.

−

i<j

L = D

A.

−

2

The spectral theorem yields

L = U ΛU T ,

where Λ = diag(λ1, . . . , λn) is the diagonal matrix of eigenvalues of L, with 0 = λ1 < λ2
≤
U = (u1, . . . , un) is the matrix of corresponding eigenvectors, with U T U = I and u1 = e/√n.

. . .

λn, and

≤

Spectral embedding. Let X = √Λ+U T , where Λ+ = diag(0, 1/λ2, . . . , 1/λn) denotes the pseudo-inverse
of Λ. The columns x1, . . . , xn of the matrix X deﬁne an embedding of the nodes in Rn, each dimension
corresponding to an eigenvector of the Laplacian. Observe that the ﬁrst component of each vector x1, . . . , xn
is equal to 0, reﬂecting the fact that the ﬁrst eigenvector is not informative. Since Xe = 0, the centroid of
the n vectors is the origin:

The Gram matrix of X is the pseudo-inverse of the Laplacian:

1
n

n

X
i=1

xi = 0.

X T X = U Λ+U T = L+.

Random walk. Consider a random walk in the graph where the transition rate from node i to node j is
Aij . Speciﬁcally, the walker stays at node i an exponential time with parameter di, then moves from node
i to node j with probability Pij = Aij /di. This deﬁnes a continuous-time Markov chain with generator
matrix
L and uniform stationary distribution. The sequence of nodes visited by the random walk forms a
discrete-time Markov chain with transition matrix P = D−1A.

−

Hitting times. Let Hij be the mean hitting time of node j from node i. Observe that Hii = 0. The
following results, proved in [10, 14], will be extended to weighted spectral embedding in Section 4. We denote
by ei the n-dimensional unit vector on component i.

Proposition 1 The matrix H satisﬁes:

Proposition 2 The solution H to equation (3) with zero diagonal entries satisﬁes:

LH = eeT

nI.

−

i, j, Hij = n(ej

ei)T L+ej.

∀

−

(1)

(2)

(3)

(4)

Since L+ = X T X, we obtain:

Hij = n(xj

xi)T xj .

−

We deduce the mean commute time between nodes i and j:

||
In view of (2), the mean hitting time of node j from steady state is:

−

||

Cij = Hij + Hji = n

xi

xj

2.

hj =

1
n

n

X
i=1

Hij = n

xj

2.

||

||

3

Cosine similarity. Observe that:

Let Sij = cos(xi, xj ). This is the cosine-similarity between nodes i and j. We have:

nxT

i xj = hj

Hij = hi

Hji.

−

−

Sij =

hj
Hij
−
phihj

=

hi
Hji
−
phihj

,

Sij =

hi + hj

Cij

−
2phihj

.

that is

Thus the cosine-similarity between any vectors xi, xj can be interpreted in terms of the mean commute time
between the corresponding nodes i, j relative to their mean hitting times.

4 Random walk with weights

In this section, we introduce a modiﬁed version of the random walk, that takes the node weights into account.
Recall that all weights are assumed positive. We denote by

n
i=1 wi the total node weight.

w

= P

|

|

Random walk. We modify the random walk as follows: the transition rate from node i to node j is now
Aij /wi. Thus the walker stays at node i an exponential time with parameter di/wi, then moves from node
i to node j with probability Pij . Observe that the previously considered random walk corresponds to unit
W −1L, with W = diag(w), and
weights. We get a continuous-time Markov chain with generator matrix
stationary distribution π = w/

−

w

.

|

|

Hitting times. Let Hij be the mean hitting time of node j from node i. Observe that Hii = 0.

(5)

(6)

Proposition 3 The matrix H satisﬁes:

Proof. We have Hii = 0 while for all i

= j,

LH = weT

w

I.

− |

|

Hij =

+

PikHkj.

wi
di

n

X
k=1

Thus the matrix (I
eT L = 0, this diagonal matrix is

P )H

−

−

w

I.

−|

|

D−1weT is diagonal. Equivalently, the matrix LH

weT is diagonal. Since
(cid:3)

−

Proposition 4 The solution H to equation (6) with zero diagonal entries satisﬁes:

i, j, Hij =

w

(ej

ei)T L+(ej

π).

∀
I satisﬁes eT M = 0 so the seeked solution is of the form:

−

−

|

|

Proof. The matrix M = weT

w

− |

|

for some n-dimensional vector h. Thus,

Since Hjj = 0, we get:

H = L+(weT

w

I) + ehT ,

− |

|

Hij =

w

|

|

i L+(π
eT

−

ej) + hj.

hj =

w

j L+(π
eT

−|

|

ej).

−

4

Let ¯x be the center of mass of the vectors x1, . . . , xn:

Since L+ = X T X, we obtain:

Thus the mean hitting times are the same as without node weights, up to the multiplicative constant
and the shift of the origin to the center of mass ¯x of the vectors x1, . . . , xn.

In particular, the mean commute time between any nodes i and j depends on the weights through their

sum only:

The mean hitting time of node j from steady state is:

¯x = Xπ =

πixi.

n

X
i=1

Hij =

w

(xj

|

|

−

xi)T (xj

¯x).

−

Cij = Hij + Hji =

w

xi

|

|||

−

xj

2.

||

hj =

πiHij =

w

xj

|

|||

¯x

2.

−

||

n

X
i=1

−

Sij =

hi + hj

Cij

−
2phihj

.

Cosine similarity. Let Sij = cos(xi
obtain as above:

−

¯x, xj

¯x). This is the cosine-similarity between nodes i and j. We

In particular, the relative mean commute times, as deﬁned by (5), depend on the weights w1, . . . , wn through
the center of mass ¯x of the vectors x1, . . . , xn only.

5 Weighted spectral embedding

W −1L of the weighted random
We now introduce the weighted spectral embedding. The generator matrix
walk is not symmetric in general. Thus we consider the following normalized version of the Laplacian, we
refer to as the weighted Laplacian:

−

This matrix is symmetric, positive semi-deﬁnite. In the particular case where W = D, this is known as the
symmetric normalized Laplacian.
The spectral theorem yields

LW = W

2 LW

− 1

− 1
2 .

LW = ˆU ˆΛ ˆU T ,

where ˆΛ = diag(ˆλ1, . . . , ˆλn) is the diagonal matrix of eigenvalues of LW , with 0 = ˆλ1 < ˆλ2
ˆU = (ˆu1, . . . , ˆun) is the matrix of corresponding eigenvectors, with ˆU T ˆU = I and ˆu1 = √π.

. . .

≤

≤

We have the following explicit expression for the pseudo-inverse of the weighted Laplacian:

Proposition 5 The pseudo-inverse of LW is:

Proof. Let M be the matrix deﬁned by the right-hand side of (9). Using the fact that Le = 0, we get:

which proves that M is the pseudo-inverse of LW .

L+

W = W

1
2 (I

eπT )L+(I

πeT )W

1
2 .

−

−

LW M LW = W

− 1
2 ,

− 1

− 1

2 LL+LW
− 1
2 L+W

2 = LW ,

= W

5

(cid:3)

(7)

w

|

|

(8)
ˆλn, and

(9)

(cid:3)

Generalized eigenvalue problem. Let V = W − 1

2 ˆU . We have:

LV = W V ˆΛ,

with V T W V = I. Thus the columns v of the matrix V are solutions to the generalized eigenvalue problem:

Lv = λW v,

(10)

with corresponding eigenvalues λ = ˆλ1, . . . , ˆλn. The ﬁrst column, associated to the eigenvalue ˆλ1 = 0, satisﬁes
v

e, while the others satisfy wT v = 0.

∝

Spectral embedding. Let Y = √Λ+ ˆU T W − 1
2 . The columns y1, . . . , yn of the matrix Y deﬁne an embed-
ding of the nodes, we refer to as the weighted spectral embedding. As for the regular spectral embedding,
the ﬁrst component of each vector y1, . . . , yn is equal to 0. Since Y w = 0, the center of mass of y1, . . . , yn
lies at the origin:

n

X
i=1

wiyi = 0.

The Gram matrix of Y is:

In view of Proposition 5,

Y T Y = W

− 1

2 ˆU ˆΛ+ ˆU T W

− 1

2 = W

− 1

2 L+

W W

− 1
2 .

i W

i yj = eT
yT
= (ei

= (xi

− 1

2 L+
W W
π)T L+(ej
¯x)T (xj

−

−

−

− 1

2 ej,

π)T ,

−
¯x).

Thus the considered weighted spectral embedding is equivalent to the regular spectral embedding, shifted so
that the origin is the center of mass ¯x. In particular, the distances between vectors in the embedding space
can be interpreted in terms of mean hitting times of the random walk, as shown in Section 4.

6 A mechanical system

Consider the mechanical system consisting of n point particles of respective masses w1, . . . , wn sliding along
a bar without friction. Particles i and j are linked by a spring satisfying Hooke’s law with stiﬀness Aij .

Eigenmodes. Assume that the bar has a uniform circular motion with angular velocity ω around some
ﬁxed axis. We denote by v1, . . . , vn the locations of the particles along the bar, with the axis taken as the
origin. By Newton’s second law of motion, the system is in equilibrium if and only if

i = 1, . . . , n,

∀

Aij (vj

vi) =

−

−

wiviω2,

n

X
j=1

that is

Lv = ω2W v.

= 0, then v is a solution to the generalized eigenvector problem (10) with eigenvalue λ = ω2. We refer

If v
to these equilibrium states as the eigenmodes of the mechanical system.

The ﬁrst eigenmode v

e, for ω2 = 0, corresponds to the absence of motion. The other eigenmodes give
the possible angular velocities of the system, equal to the square roots of the eigenvalues ˆλ2, . . . , ˆλn of LW .
Any such eigenmode v satisﬁes wT v = 0, meaning that the center of mass of the system is at the origin.

∝

6

Potential energy. The mechanical potential energy of the system in any state v is:

If v is an eigenmode of the mechanical system, this is equal to:

1
2

λ
2

vT Lv.

vT W v.

As vT W v is the moment of inertia of the system and λ = ω2 is the square of the angular velocity, this
corresponds to the angular kinetic energy. For a unit moment of inertia vT W v = 1, we obtain λ = vT Lv,
so that the eigenvalues of the weighted Laplacian LW can be viewed as (twice) the levels of energy of the
eigenmodes, for unit moments of inertia. The weighted spectral embedding reduced to the k ﬁrst eigenvectors
of LW can then be interpreted as that induced by the k eigenmodes of lowest potential energies of the
mechanical system, for unit moments of inertia.

Dirichlet problem. For any i
= j, assume the positions of the point particles i and j are set to 1 and 0,
respectively. Let v be the vector of positions at equilibrium, in the absence of rotation. By Newton’s ﬁrst
law of motion,

for some constant α equal to the force F exerted on both i and j (in opposite directions). This force does
not depend on the masses of the point particles.
The solution to this Dirichlet problem is:

Lv = α(ei

ej),

−

v = αL+(ei

ej) + βe,

−

for some constant β. Using the fact that vi = 1 and vj = 0, we get:

vk =

ej)T L+(ei
ej)T L+(ei

(ek
(ei

−
−

ej)
ej)

−
−

(xk

=

xj )

xj )T (xi
xj
xi

−

||

−
||

−
2

and

Observe in particular that vk
random walk, given by (7), satisﬁes:

∈

[0, 1] for all nodes k. The mean hitting time of node j from node i by the

where ¯v is the center of mass of the system:

By symmetry, we have:

−
We deduce the mean commute time between nodes i and j:

Hji = |

|

(1

¯v).

Cij = Hij + Hji = |

|

.

w
F

α =

2 .

xi

||

−

xj

||

1

w
F

Hij = |

|

¯v,

¯v =

πkvk.

n

X
k=1

w
F

7

This symmetry in the solutions to the Dirichlet problem explains why, unlike the mean hitting times, the
mean commute times depend on the weights through their sum only. The mean commute time between
two nodes of the graph is inversely proportional to the force exerted between the two corresponding point
particles of the mechanical system, which is independent of the particle masses.

7 An electrical network

Consider the electrical network induced by the graph, with a resistor of conductance Aij between nodes i and
j and a capacitor of capacitance wi between node i and the ground. The move of electrons in the network
is precisely described by the random walk deﬁned in Section 4.

Eigenmodes. Let v(t) be the vector of electric potentials at time t, starting from some arbitrary initial
state v. By Ohm’s law,

i = 1, . . . , n, wi

=

Aij (vj(t)

vi(t)),

−

∀

dvi(t)
dt

n

X
j=1

that is

The solution to this diﬀerential equation is:

If the initial state v is a solution to the generalized eigenvector problem (10), we obtain:

Since the eigenvectors of LW form a basis of Rn, any initial state v can be written as a linear combination
of solutions to the generalized eigenvector problem, we refer to as the eigenmodes of the electrical network.
The ﬁrst eigenmode v
e, for λ = 0, corresponds to a static system, without discharge. The other
eigenmodes give the possible discharge rates of the system, equal to the eigenvalues ˆλ2, . . . , ˆλn of LW . Any
such eigenmode v satisﬁes wT v = 0, meaning that the total charge accumulated in the capacitors is null.

∝

Energy dissipation. The energy dissipated by the resistors for the vector of electric potentials v is:

If v be an eigenmode of the electrical network, this is equal to

Observing that 1
2 vT W v is the electrical potential energy of the system, we deduce that λ can be viewed as
the energy dissipated by the resistors for a unit electric potential energy. In particular, the weighted spectral
embedding reduced to the k ﬁrst eigenvectors of LW can be interpreted as that induced by the k eigenmodes
of lowest energy dissipation of the electrical network, for unit electric potential energies.

Dirichlet problem. For any i
respectively. Let v be the vector of electric potentials at equilibrium. We have:

= j, assume the electric potentials of nodes i and j are set to 1 and 0,

for some constant α equal to the current ﬂowing from i to j. This Dirichlet problem is the same as for the
mechanical network, and thus the same results apply. In particular, we get vk
[0, 1] for all nodes k, and
the current between i to j is:

∈

W

dv(t)
dt

=

Lv(t).

−

v(t) = e

−W −1Ltv.

v(t) = e

−λtv.

vT Lv.

vT W v.

1
2

λ
2

Lv = α(ei

ej),

−

α =

1

2 .

xi

||

−

xj

||

8

This is the intensity of the current I generated by a unit potential diﬀerence betweeen i and j. Its inverse
is known as the eﬀective resistance between i and j. The mean hitting time of node j from node i by the
random walk satisﬁes:

q
I
where q is the total charge accumulated in the capacitors:

Hij =

,

Observe that this expression for the mean hitting time can be seen as a consequence of Little’s law, as I is the
intensity of positive charges entering the network and q the mean number of positive charges accumulated
in the network.

By symmetry, the total charge accumulated in the capacitors when the respective electric potentials of i

and j are set to 0 and 1 is:

so that:

We deduce the mean commute time between nodes i and j:

q =

wkvk.

n

X
k=1

n

X
k=1

wk(1

vk) =

w

q,

−

|

| −

Hji = |

w

| −
I

q

.

Cij = Hij + Hji = |

|

.

w
I

Again, the symmetry in the solutions to the Dirichlet problem explains why the mean commute times
depend on the weights through their sum only. The mean commute time between two nodes of the graph is
proportional to the eﬀective resistance between the two corresponding points of the electrical network, which
is independent of the capacitors.

8 Experiments

We now illustrate the results on a real dataset. The considered graph is that formed by articles of Wikipedia
for Schools1, a selection of articles of Wikipedia for children [18]. Speciﬁcally, we have extracted the largest
connected component of this graph, considered as undirected. The resulting graph has 4,589 nodes (the
articles) and 106,644 edges (the hyperlinks between these articles). The graph is undirected and unweighted.
Both the dataset and the Python code used for the experiments are available online2.

Global clustering. We ﬁrst apply k-means clustering3 to the following embeddings of the graph, each in
dimension k = 100:

•

•

the regular spectral embedding, X, restricted to rows 2, . . . , k + 1, say ˆX;

the shifted spectral embedding, ˆX

ˆXπ1T , where π = w/

w

and 1 is the k-dimensional vector of ones;

−

|

|

the weighted spectral embedding, Y , restricted to rows 2, . . . , k + 1, say ˆY .

•
1https://schools-wikipedia.org
2https://github.com/tbonald/spectral_embedding
3The algorithm is k-means++ [1] 100 random initializations.

9

Here the vector of weights w is taken equal to d, the vector of internal node weights. In particular, the
weighted spectral embedding is that following from the spectral decomposition of the normalized Laplacian,
D− 1

2 LD− 1
Each embedding is normalized so that nodes are represented by k-dimensional unitary vectors. This is
equivalent to consider the distance induced by the cosine similarity in the original embedding. In particular,
the regular spectral embedding and the shifted spectral embedding give diﬀerent clusterings.

2 . Observe that the ﬁrst row of each embedding X and Y is equal to zero and thus discarded.

Tables 1 and 2 show the top articles of the clusters found for each embedding, when the number of clusters
is set to 20, with the size of each cluster. The selected articles for each cluster correspond to the nodes of
highest degrees among the 50% closest nodes from the center of mass of the cluster in the embedding space,
with unit weights for the regular spectral embedding and internal node weights d for the shifted spectral
embedding and the weighted spectral embedding.

France, Italy, Spain, Latin, Netherlands

India, Japan, China, United Nations
Earth, Sun, Physics, Hydrogen, Moon, Astronomy

Size Top articles
1113 Australia, Canada, North America, 20th century
326 UK, England, London, Scotland, Ireland
250 US, New York City, BBC, 21st century, Los Angeles
227
218
200 Mammal, Fish, Horse, Cattle, Extinction
200
198 Water, Agriculture, Coal, River, Antarctica
197 Germany, World War II, Russia, World War I
187 Mexico, Brazil, Atlantic Ocean, Argentina
185 Human, Philosophy, Slavery, Religion, Democracy
184
177 Gold, Iron, Oxygen, Copper, Electron, Color
170
159
158 Africa, South Africa, Time zone, Portugal
157
141 Washington, D.C., President of the United States
72
70

Dinosaur, Fossil, Reptile, Cretaceous, Jurassic
Paris, Art, Architecture, Painting, Hist. of painting

Egypt, Turkey, Israel, Islam, Iran, Middle East
English, 19th century, William Shakespeare, Novel

Plant, Rice, Fruit, Sugar, Wine, Maize, Cotton

Europe, Scientiﬁc classiﬁcation, Animal, Asia

Table 1: Global clustering of Wikipedia for Schools by weighted spectral embedding.

The ﬁrst observation is that the three embeddings are very diﬀerent. The choice of the Laplacian (regular
or normalized) matters, because the dimension k is much smaller than the number of nodes n (recall that the
shifted spectral embedding and the weighted spectral embedding are equivalent when k = n). The second
observation is that the weighted spectral embedding seems to better capture the structure of the graph.
Apart from the ﬁrst cluster, which is signiﬁcantly larger than the others and thus may contain articles
on very diﬀerent topics, the other clusters look meaningful. The regular spectral embedding puts together
articles as diﬀerent as Meteorology, British English and Number, while the shifted embedding groups together
articles about Sanskrit, Light and The Simpsons, that should arguably appear in diﬀerent clusters.

10

Earth, Water, Iron, Sun, Oxygen, Copper, Color
Scotland, Ireland, Wales, Manchester, Royal Navy

Fatty acid, List of vegetable oils, Biodiesel
Train, Canadian Paciﬁc Railway, Denver, Colorado
Eye, Retina, Animation, Glasses, Lego, Retinol
Finance, Supply and demand, Stock, Accountancy

Size Top articles
1666 Germany, India, Africa, Spain, Russia, Asia
526 Chordate, Binomial nomenclature, Bird, Mammal
508
439
300 New York City, Los Angeles, California, Jamaica
246 North America, German language, Rome, Kenya
English, Japan, Italy, 19th century
195
Language, Mass media, Library, Engineering, DVD
131
103
Jazz, Piano, Guitar, Music of the United States
91 Microsoft, Linux, Microsoft Windows, Algorithm
British monarchy, Bristol, Oxford, Paul of Tarsus
67
58
Tropical cyclone, Bermuda, Hurricane Andrew
49 Mathematics, Symmetry, Geometry, Algebra, Euclid
48
47
38
27
19 Meteorology, British English, Number, Vowel
17
Newcastle upon Tyne, Isambard Kingdom Brunel
14 Wikipedia, Jimmy Wales, Wikimedia Foundation
Size Top articles
452 Chordate, Binomial nomenclature, Bird, Mammal
446 UK, Europe, France, English language, Japan
369 Germany, Spain, Soviet Union, Sweden, Poland
Earth, Water, Iron, Sun, Oxygen, Copper, Color
369
India, Africa, Russia, New Zealand, River, Snow
353
328
Egypt, Greece, Middle Ages, Roman Empire, Nazism
286 United States, New York City, Petroleum, Finland
271
238 Time zone, Turkey, Portugal, Israel, Currency
217 Rice, Fruit, Bacteria, Wine, DNA, Flower, Banana
178
175
156
125 Atlantic Ocean, Morocco, Algeria, Barbados
124 Opera, Folk music, Elvis Presley, Bob Dylan
117 Agriculture, Ocean, Geology, Ecology, Pollution
112 Christianity, Switzerland, Judaism, Bible, Deity
108
94
71 Mining, Food, Geography, Engineering, Transport

British monarchy, Bristol, Charles II of England
Internet, Hebrew language, Language, Mass media
Physics, Ancient Egypt, Science, Astronomy, Time

South America, Paciﬁc Ocean, Tourism, Colombia
Film, Sanskrit, Light, The Simpsons, Eye, Shark

British Empire, 17th century, Winston Churchill

11

Table 2: Global clustering of Wikipedia for Schools by regular (top) and shifted (bottom) spectral embed-
dings.

Selective clustering. We are now interested in the clustering of some selection of the articles. We take
the 667 articles in the People category. A naive approach consists in considering the subgraph induced by
these nodes. But this is not satisfactory as the similarity between articles in the People category depends
strongly on their connections through articles that not in this category. Indeed, the subgraph of nodes in
the People category is not even connected. Our approach consists in assigning a multiplicative factor of 10
to articles in the People category. Speciﬁcally, we set wi = 10
di if article i belongs to the People category,
and wi = di otherwise. We compare the three previous embeddings, for this vector of weights.

×

Tables 3 and 4 show the top articles in the People category for the 20 clusters found for each embedding.
The selected articles for each cluster correspond to the nodes of highest degrees in the People category. We
also indicate the number of articles in the People category in each cluster. Observe that the last cluster
obtained with the regular spectral embedding has no article in the People category. Again, the impact of
weighting is signiﬁcant. The weighted spectral embedding, which is adapted to the People category, seems
to better capture the structure of the graph. Except for the ﬁrst, which is much larger than the others, the
clusters tend to group together articles of the People category that are closely related.

Count Top articles

228 William Shakespeare, Pope John Paul II
Jorge Luis Borges, Rabindranath Tagore
60
Elizabeth II of the UK, Winston Churchill, Tony Blair
55
Charles II of England, Elizabeth I of England
49
Ronald Reagan, Bill Clinton, Franklin Roosevelt
48
Alexander the Great, Genghis Khan, Muhammad
25
Adolf Hitler, Joseph Stalin, Vladimir Lenin
24
Napoleon I, Charlemagne, Louis XIV of France
23
Jesus, Homer, Julius Caesar, Virgil
22
Aristotle, Plato, Charles Darwin, Karl Marx
22
George W. Bush, Condoleezza Rice, Nelson Mandela
21
Elvis Presley, Paul McCartney, Bob Dylan
18
Isaac Newton, Galileo Galilei, Ptolemy
15
Albert Einstein, Gottfried Leibniz, Bertrand Russell
13
Pete Sampras, Boris Becker, Tim Henman
10
William Thomson, 1st Baron Kelvin, Humphry Davy
9
Carolus Linnaeus, James Cook, Gerald Durrell
8
Bill Gates, Richard Stallman, Ralph Nader
8
Floyd Mayweather Jr., Lucy, Lady Duﬀ-Gordon
5
Vasco da Gama, Idit Harel Caperton, Reza Shah
4

Table 3: Selective clustering of Wikipedia for Schools by weighted spectral clustering.

12

Count Top articles

−
Count Top articles

196
95
90
75
45
28
25
24
22
10
10
9
9
8
5
5
5
4
2
0

182
113
69
46
31
26
25
24
23
19
18
17
16
12
11
10
8
7
5
5

Carolus Linnaeus, Adolf Hitler, Jesus, Aristotle
Christina Aguilera, Andy Warhol, Auguste Rodin
Julius Caesar, Martin Luther King, Jr., Euclid
Tony Blair, Victoria of the UK, Charles II of England
Ronald Reagan, Franklin D. Roosevelt, Gerald Ford
Abraham Lincoln, George Washington, John Adams
Igor Stravinsky, Johann Sebastian Bach
Albert Einstein, Gottfried Leibniz, Isaac Newton
George W. Bush, Napoleon I of France, Plato
Pete Sampras, Boris Becker, Tim Henman, Pat Cash
Fanny Blankers-Koen, Rosa Parks, Donald Bradman
Alexander the Great, Frederick II of Prussia
Mahatma Gandhi, Buddha, Muhammad Ali Jinnah
Columba, Edwin of Northumbria, Macbeth of Scotland
Bill Clinton, Richard Stallman, Linus Torvalds
Elizabeth II of the UK, David Beckham, Wayne Rooney
Archbishop of Canterbury, Harold Wilson
Leonardo da Vinci, Neil Armstrong, Wright brothers
George III of the UK, Matthew Brettingham

Adolf Hitler, William Shakespeare
Elizabeth II of the UK, George W. Bush
Tony Blair, Victoria of the UK, Elizabeth I of England
Ronald Reagan, Franklin Roosevelt, Jimmy Carter
Henry James, Igor Stravinsky, Ezra Pound
Paul McCartney, Bob Dylan, Edgar Allan Poe
Jesus, Charlemagne, Genghis Khan, Homer, Ptolemy
Albert Einstein, Gottfried Leibniz, Isaac Newton
Charles Darwin, Galileo Galilei, Nikola Tesla
Plato, John Locke, Max Weber, Friedrich Nietzsche
Rabindranath Tagore, Mahatma Gandhi, Buddha
Margaret Thatcher, David Cameron, George VI
Condoleezza Rice, Nelson Mandela, Gerald Ford
Dwight D. Eisenhower, Ernest Hemingway
Aristotle, Alexander the Great, Fred. II of Prussia
Pete Sampras, Boris Becker, Tim Henman
Muhammad, Norman Borlaug, Osama bin Laden
Bill Clinton, Bill Gates, Richard Stallman
Leonardo da Vinci, Neil Armstrong
Carolus Linnaeus, Christopher Columbus, Paul Kane

13

Table 4: Selective clustering of Wikipedia for Schools by regular (top) and shifted (bottom) spectral cluster-
ing.

9 Conclusion

We have proposed a novel spectral embedding of graphs that takes node weights into account. We have proved
that the dimensions of this embedding correspond to diﬀerent conﬁgurations of an equivalent physical system,
either mechanical or electrical, with node weights corresponding to masses or capacitances, respectively.

A practically interesting consequence of our work is in the choice of the Laplacian, when there are no
other information on the node weights than the graph itself. Thanks to weighted spectral embedding, we see
that the two versions of the Laplacian, regular and normalized, correspond in fact to two relative importance
of nodes, given respectively by unit weights and internal node weights.

References

[1] D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding.

In Proceedings of
the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027–1035. Society for
Industrial and Applied Mathematics, 2007.

[2] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.

Neural computation, 15(6):1373–1396, 2003.

[3] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning: going

beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017.

[4] H. Cai, V. W. Zheng, and K. Chang. A comprehensive survey of graph embedding: problems, techniques

and applications. IEEE Transactions on Knowledge and Data Engineering, 2018.

[5] F. R. Chung. Spectral graph theory. American Mathematical Soc., 1997.

[6] I. S. Dhillon, Y. Guan, and B. Kulis. Kernel k-means: spectral clustering and normalized cuts.

In
Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 551–556. ACM, 2004.

[7] F. Fouss, A. Pirotte, J.-M. Renders, and M. Saerens. Random-walk computation of similarities between
nodes of a graph with application to collaborative recommendation. IEEE Transactions on knowledge
and data engineering, 19(3):355–369, 2007.

[8] Z. Huang. Extensions to the k-means algorithm for clustering large data sets with categorical values.

Data mining and knowledge discovery, 2(3):283–304, 1998.

[9] K. Kerdprasop, N. Kerdprasop, and P. Sattayatham. Weighted k-means for density-biased clustering.
In International Conference on Data Warehousing and Knowledge Discovery, pages 488–497. Springer,
2005.

[10] L. Lov´asz. Random walks on graphs. Combinatorics, Paul Erdos is eighty, 2:1–46, 1993.

[11] U. Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395–416, Dec. 2007.

[12] M. Newman. Spectral methods for community detection and graph partitioning. Phys. Rev. E, 88:042822,

Oct 2013.

[13] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In Advances

in neural information processing systems, pages 849–856, 2002.

[14] H. Qiu and E. R. Hancock. Clustering and embedding using commute times. IEEE Transactions on

Pattern Analysis and Machine Intelligence, 29(11), 2007.

14

[15] A. Robles-Kelly and E. R. Hancock. A Riemannian approach to graph embedding. Pattern Recognition,

40(3):1042–1056, 2007.

[16] P. Snell and P. Doyle. Random walks and electric networks. Free Software Foundation, 2000.

[17] D. A. Spielman. Spectral graph theory and its applications. In Foundations of Computer Science, 2007.

FOCS’07. 48th Annual IEEE Symposium on, pages 29–38. IEEE, 2007.

[18] R. West, J. Pineau, and D. Precup. Wikispeedia: An online game for inferring semantic distances

between concepts. In IJCAI, 2009.

[19] S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: A
general framework for dimensionality reduction. IEEE transactions on pattern analysis and machine
intelligence, 29(1):40–51, 2007.

15

