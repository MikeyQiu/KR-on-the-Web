7
1
0
2
 
v
o
N
 
9
2
 
 
]
L
C
.
s
c
[
 
 
2
v
6
8
1
5
0
.
1
1
7
1
:
v
i
X
r
a

False Positive and Cross-relation Signals in Distant
Supervision Data

Anca Dumitrache
Vrije Universiteit Amsterdam,
CAS IBM Netherlands
anca.dumitrache@vu.nl

Lora Aroyo
Vrije Universiteit Amsterdam
lora.aroyo@vu.nl

Chris Welty
Google Research, New York
cawelty@gmail.com

Abstract

Distant supervision (DS) is a well-established method for relation extraction from
text, based on the assumption that when a knowledge-base contains a relation
between a term pair, then sentences that contain that pair are likely to express the
relation. In this paper, we use the results of a crowdsourcing relation extraction
task to identify two problems with DS data quality: the widely varying degree
of false positives across different relations, and the observed causal connection
between relations that are not considered by the DS method. The crowdsourcing
data aggregation is performed using ambiguity-aware CrowdTruth metrics, that
are used to capture and interpret inter-annotator disagreement. We also present
preliminary results of using the crowd to enhance DS training data for a relation
classiﬁcation model, without requiring the crowd to annotate the entire set.

1

Introduction

Distant supervision (DS) [11, 14] is a well-established semi-supervised method for performing re-
lation extraction from text. It is based on the assumption that, when a knowledge-base contains a
relation between a pair of terms, then any sentence that contains that pair is likely to express the
relation. This approach can generate false positives, as not every mention of a term pair in a sen-
tence means a relation is also expressed [9]. Furthermore, dependencies between the semantics of
the relations such as causality or contradiction are also not considered by the DS methodology. It is
often assumed that these disadvantages are compensated for by the scale of the data a DS method
can produce, or can be largely overcome with crowdsourced human annotation [1].

In this paper we identify two speciﬁc problems we have found with distant supervision training
data: the widely varying degree of false positives across different TAC-KBP relation types, and the
observed causal connection between relations. We expose these problems using a novel approach to
gathering human annotated data, CrowdTruth [3, 4, 2], analyze them, and offer preliminary heuristic
and statistical approaches to incorporating them back into DS-based training, that provides better
sentence-level relation extraction results, without requiring crowdsourcing on the full set of data.

2 Background

In recent years, researchers have explored unsupervised methods for correcting DS data. For the
task of knowledge base completion, [9] applied memory networks both to correct false positives in
the data, and to capture dependencies between relations. For the same task, [10] developed a loss
function that works with multi-label data, in order to capture co-occurring relations. For relation
classiﬁcation from sentences, [7] learn embeddings that capture cross-signals between relations.
However, these approaches are dependent on training data that can express relation semantics with

6th Workshop on Automated Knowledge Base Construction (AKBC 2017) at NIPS 2017, Long Beach, CA,
USA.

at least some accuracy. The initial experiments presented in this paper show the error rate in the
DS data can be so high that unsupervised learning becomes unreliable when it comes to capturing
cross-relation signals.

Crowdsourcing is a well-used approach to correcting the mistakes in DS by scaling out cheap human
annotation. We have been studying the problem of collecting human annotations from the crowd
using the CrowdTruth methodology [2]. Our method differs in that it gathers many annotations for
the same examples, to better reﬂect properties like ambiguity, human error and spam, and the target
semantics [3]. We have used it successfully to improve DS results for the task of medical relation
extraction [8], achieving annotation quality equivalent to that provided by medical experts, at less
than half the cost.

3 Data and Annotation with CrowdTruth

We began by having the crowd annotate 2,500 sentences from the NIST TAC-KBP 2013 English
Slotﬁlling data that were annotated with DS. We split the data in half into a test and dev set. We
focused the crowd annotations on a subset of 16 relations (Fig.1) that occur between terms of types
P erson, Organization and Location. We ran a multiple-choice crowdsourcing task on Crowd-
ﬂower,1 asking 15 workers to annotate each sentence with the appropriate relations, or choose the
option N ON E if none of the relations presented apply. Workers are encouraged to select all rela-
tions that apply. Each worker was paid $0.05 per sentence. The data is available online.2

Traditionally, crowdsourcing annotations are aggregated by looking at the consensus of the workers,
through methods such as majority vote. This process is based on a simpliﬁed notion of truth, under
the assumption that a single right annotation exists for each example. However, in reality, truth is
not universal and is strongly inﬂuenced by a variety of factors, such as the ambiguity that is inherent
in natural language, resulting in artiﬁcial data that does not represent the diverse perspectives of the
crowd.
To address this issue, we proposed the CrowdTruth metrics3 of aggregating crowdsourcing data
while also capturing and interpreting inter-annotator disagreement. The main basis for measuring
quality in CrowdTruth is the triangle of disagreement, which links together sentences, workers, and
relations. It allows us to assess the quality of each worker, the clarity of each sentence, and the
ambiguity, similarity and frequency of each relation. The triangle model expresses how ambiguity
in any of the components of the triangle disseminates and inﬂuences the other components. For
example, an unclear sentence or an ambiguous relation would cause more disagreement between
workers, and thus, both need to be accounted for when measuring the quality of the workers, which
is then used to identify the spam workers. Among the CrowdTruth measures discussed in this paper,
we calculate the per-relation false positive (FP) rate and the causal power between relation pairs
(RCP) over the dev set. Spam removal was performed as well, but the details of this process are not
relevant for the paper.

For every sentence, the annotations of each worker form a binary worker vector, where the relations
selected are equal to ‘1’, and the rest to ‘0’. The sentence vector is the sum of all worker vectors
for the given sentence. Then for each relation, we compute the sentence-relation score of relation
i (SRSi) as the ratio of workers that picked that relation over the total of number of workers. The
SRS measures how clearly the relation is expressed in the sentence, and is used as a continuous truth
measure in a lot of our work. In order to make our results compatible with the AKBC community’s
discrete truth metrics (e.g. P, R, F1), we have chosen a threshold of 0.5 per relation, corresponding
to the majority vote, that allows for multiple relations to be considered correct in a sentence. False
positive rates are then computed per relation on the dev set with this threshold.

Causal power [5] is an estimate of the probability that the presence of one relation implies the pres-
ence of another. Given two relations i and j, RCP (Ri, Rj) = [P (Rj|Ri) − P (Rj|¬Ri)]/[1 −
P (Rj|¬Ri)], where P (Ri) is the probability that relation Ri is annotated in the sentence. This
probability can be calculated on a micro basis giving us the probability of one worker annotating
two relations together; the macro RCP calculates the probabilities in the sentence vectors, capturing

1http://crowdflower.com
2https://github.com/CrowdTruth/Open-Domain-Relation-Extraction
3https://git.io/v5iTB

2

causality as a result of two relations being annotated together in the same sentence, but not nec-
essarily by the same workers. We found micro RCP to be vastly inferior to macro RCP, which is
further evidence of the value of having multiple workers per sentence, and only include the macro
RCP results in this paper.

In addition to the dev and test sets, we also used a training set of 235,000 sentences annotated by
distant supervision from freebase relations, used in [13]. This data was used as a baseline for
training a relation extraction classiﬁer based on [12]. The model is a convolutional neural network,
adapted to be both multi-class and multi-label – we use a sigmoid cross-entropy loss function instead
of softmax cross-entropy, and the ﬁnal layer is normalized with the sigmoid function instead of
softmax – in order to make it possible for more than one relation to hold between two terms in one
sentence. To evaluate the relation extraction model on CrowdTruth data with discrete AKBC metrics,
we set a comparable threshold of 0.5 on the model conﬁdence score, separating between negative
and positive labels. However, the loss function is not computed using binary labels generated by the
threshold, but using the continuous labels in both the train and test sets.

4 Analysis

Figure 1: DS ratio of false positive over all positive labels, using the crowd as ground truth.

Using the SRS as a ground truth at a 0.5 threshold, Fig. 1 shows the correctness of the DS labels
on the dev set. There is considerable variation in DS data quality across relations. The origin and
place of death relations scored particularly badly, with more than 90% false positives. With such
a high error rate in some relations, it is arguable that any classiﬁer could learn anything meaningful,
regardless of algorithm or quantity of data.

Manual error analysis on the dev set showed that many sentences contain a P erson - Location
pair, where freebase speciﬁed both that the person resided in and died at that location. This makes
intuitive sense, people tend to die in the places they live.
In most of these cases, the sentence
expressed only the places of residence relation, leading to the false positives. The origin relation
data suffers from the same problem. Table 5 in the Appendix shows several examples of these
sentences. This led us to consider a heuristic solution to this problem as a headroom study as well
as a statistical solution. Both are discussed in Section 5.

The results of the macro RCP analysis for six of the relations we analyzed (Tab.1) shows that
the place of birth relation has a high causal power (0.64) over origin, meaning that when
place of birth is annotated in a sentence, origin is also likely to appear, with the inverse causal
power at 0.88. This high co-causality seems to indicate a confusion between the two relations.
Note also that these two relations have signiﬁcant differences in causal power in the DS-based
data. In contrast, place of death has a high causal power over places of residence in the DS

3

relation subset:

RCP for
place of death

place of birth (PoB), origin (O), places of residence
Table 1:
(PoR),
(EoM),
f ounded organization (FO),
top employee or member (TEoM). The scores show the causal power RCP (Ri, Rj) of relations Ri
in the rows, over the relations Rj in the columns. Signiﬁcant changes between crowd annotation based causal
power and distant supervision are in bold.

employee or member

(PoD),

PoB

PoB
1

Table 2: Crowd-based RCP
PoR
0.17
0.31
1
-0.01
-0.09
0.11
0.13

O
0.64
1
0.56
-0.03
-0.07
-0.36
-0.38

O 0.88
PoR
0.42
PoD -0.03
FO -0.07
EoM -0.45
TEoM -0.5

Table 3: DS-based RCP.

PoD
-0.12
-0.16
-0.1
1
-0.06
-0.47
-0.45

FO
-0.19
-0.29
-0.59
-0.04
1
0.62
0.86

EoM TEoM
-0.21
-0.2
-0.22
-0.22
0.13
0.12
-0.05
-0.05
0.13
0.1
0.82
1
0.86
1

PoB
1

PoB

O -0.02
PoR
0.65
PoD -0.06
FO -0.08
EoM -0.35
TEoM -0.16

O
-0.6
1
-0.33
-0.18
-0.06
0.35
-0.1

PoR
0.55
-0.11
1
0.17
-0.09
-0.42
-0.17

PoD
-0.14
-0.16
0.45
1
-0.06
-0.21
-0.12

FO
-0.54
-0.16
-0.7
-0.18
1
0.46
0.34

EoM TEoM
-0.57
-0.48
-0.15
0.19
-0.75
-0.68
-0.19
-0.13
0.09
0.09
0.66
1
0.24
1

data (0.45), reﬂecting the high error rate of place of death caused by the overlap in the KB with
places of residence.

In the crowd data we see a much higher co-causality for employee or member and
top employee or member, with only a slight preference in the data for what we expect to be the
“correct” causal direction (that top employee or member causes employee or member), but in
the DS-based analysis, the incorrect interpretation drops a lot. In manual error analysis we observed
that these are properties of the data set, which talk about more famous people who tend to be leaders
and founders, not “regular” employees. Table 6 in the Appendix shows several examples sentences
with false negative DS labels due to missing causality.

All told, the differences (in bold in Tab.1) between the crowd and DS-based causal power accounts
for some of the classiﬁcation errors in our trained system, and we expect them to be a signiﬁcant
cause of error in systems that try to learn cross-relation signals from DS data alone.

non-symmetric

the
f ounded organization,

top employee or member
Among
and
causes
top employee or member causes f ounded org. These again appear to be properties of the
data set.

employee or member

f ounded org,

pairs we

causes

causal

that

see

5 Enhancing Distant Supervision with CrowdTruth

We expect that the metrics from CrowdTruth annotation can be used to systematically enhance DS
data at scale, without requiring the crowd to annotate the entire set. As a preliminary headroom
exercise, we ran three experiments to test a few simple heuristic characterizations of our analysis,
and compared them to a baseline. In each experiment, we changed only the DS training set (using
the methods described below). We used the data in our previously held-out test set as an evaluation
target, again processing the continuous SRS scores with a threshold of 0.5 to yield discrete truth
values for calculating P, R, and F. Results are shown in Tab. 4.

1. DS: The baseline to which the other experiments are compared. The per-relation training

labels are binary based on the results of DS.

2. DS merged: Based on the results of the causality analysis, the training set is augmented
to reﬂect the highest cross-relation signals. We merge relations with symmetric RCP
(origin and place of birth), and add the implied relation in the case of asymmetric RCP
(employee or member and top employee or member). To merge, the DS baseline data
is updated so that the symmetric relations always co-occur, and adding caused relation
whenever the caused relation appears. This approach shows a huge improvement across
the board over the baseline, with the overall highest P and F.

3. DS RCP: Instead of manually identifying merged relations, the training data is augmented
by using the RCP scores. When a relation i has a positive DS label for a given sentence,
the labels of all other relations j (cid:54)= i are updated by adding the macro RCP that i has over
j. The maximum value for the label is clipped at 1, to keep scores in the [0, 1] interval.
The training labels in this set have continuous values, as opposed to the binary values in
the previous two sets. The formula for updating the training label for relation j in sentence
s is: DS RCP (s, i) = max[1, DS(s, j) + (cid:80)
i(cid:54)=j RCP (i, j) · DS(s, i)], where DS(s, i)

4

is the DS label of relation i in sentence s. This method was comparable in precision to the
baseline, but scored a huge win in recall. The recall increase makes sense, though we have
yet to investigate or explain the lack of increase in precision.

4. DS FP: Our analysis showed that the place of death relation was a large source of
false positives in the DS data, because most of the positives were actually express-
ing places of residence.
In every sentence in the DS training set that had a 1 for
place of death, we updated the score by subtracting its false positive ratio, which was
used in the loss function as described above. This did not impact the results over the base-
line, mainly because there were not many place of death relations in the DS data nor the
test set, and any improvement did not impact the overall result. We are conﬁdent that more
systematic treatment of false positive rates will improve performance.

Table 4: Precision & Recall at 20,000 training steps.
Precision Recall F1 score
0.22
0.33
0.48
0.22

DS
DS merged
DS RCP
DS FP

0.2
0.37
0.27
0.21

0.19
0.43
0.19
0.21

6 Discussion

The preliminary results are not overwhelming, but highly indicative. There is considerable headroom
in cross-relation signals, and a more robust approach holds promise to eliminate manual analysis,
and work as part of an overall pipeline that includes partial crowd data. We have shown a very
signiﬁcant variation in the false positive rate in distant supervision data, and it seems extremely
likely that this can be exploited to improve training.

We are currently considering experiments that take advantage of another aspect of our CrowdTruth
method: the identiﬁcation of ambiguity in sentences where workers do not agree on the outcome.
We believe a more continuous truth measure as opposed to the rather arbitrary discrete measure will
be productive. Finally, we are particularly excited about the possibility of using our approach in
conjunction with logical reasoning approaches such as those reported in [6]. In this case, we are
looking at more informed data that reﬂects human understanding and properties of the data set, to
discover candidate relation pairs for investigating rules.

Appendix

Sentence

Table 5: Example sentences with false positive place of death and origin DS labels due to multiple relations
in the KB over P erson - Location term types.

Relation

Crowd
SRS

DS label

After growing up on Cat Island, Tony McKay moved to
New York City at age 17 to study architecture.

The ﬁlm is based very loosely on the lives of Wolfgang
Amadeus Mozart and Antonio Salieri, two composers
who lived in Vienna, Austria.
Marku Ribas is the side more Black music of this group
and was Bob Marley’s friend in the 1970s, Jamaica,
where he lived.

Osama bin Laden had moved from Saudi Arabia to
Sudan during the 1990-91 Gulf War.

place of death

places of residence

place of death

places of residence

places of residence

origin

origin

places of residence

0.004

0.995

0.074

0.865

0

0.87

0.3

0.74

1

1

1

1

1

1

1

1

5

Table 6: Example sentences with false negative employee or member and origin DS labels due to missing
causal connections.

Sentence

Relation

Crowd
SRS

DS label

China on Monday ofﬁcially appointed Donald
Tsang as Hong Kong’s chief executive for a second
term.
More than 3,000 taxi drivers blocked Rome’s
historic centre Wednesday to protest extra licences
given by mayor Walter Veltroni.
Early years Joey Harrington was born and raised in
Portland, Oregon, where he has resided his entire
life.
Nelli Zhiganshina (born March 31, 1987 in
Moscow, Russia) is a Russian ice dancer who
currently represents Germany.

employee or member

top employee or member

employee or member

top employee or member

place of birth

origin

origin

place of birth

0.623

0.753

0.529

0.841

0.645

0.867

0.555

0.791

0

1

0

1

0

1

0

1

References

[1] G. Angeli, J. Tibshirani, J. Wu, and C. Manning. Combining distant and partial supervision for

relation extraction. In EMNLP, pages 1556–1567, 2014.

[2] L. Aroyo and C. Welty. Crowd Truth: Harnessing disagreement in crowdsourcing a relation

extraction gold standard. Web Science 2013. ACM, 2013.

[3] L. Aroyo and C. Welty. The Three Sides of CrowdTruth. Journal of Human Computation,

[4] L. Aroyo and C. Welty. Truth is a lie: Crowd truth and the seven myths of human annotation.

[5] P. W. Cheng. From covariation to causation: A causal power theory. Psychological Review,

1:31–34, 2014.

AI Magazine, 36(1):15–24, 2015.

104(2):367–405, 1997.

[6] T. Demeester, T. Rockt¨aschel, and S. Riedel. Regularizing relation representations by ﬁrst-

order implications. In AKBC@ NAACL-HLT, pages 75–80, 2016.

[7] C. N. dos Santos, B. Xiang, and B. Zhou. Classifying relations by ranking with convolu-
In Proceedings of the 53rd Annual Meeting of the Association for
tional neural networks.
Computational Linguistics and the 7th International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31,
2015, Beijing, China, Volume 1: Long Papers, pages 626–634. The Association for Computer
Linguistics, 2015.

[8] A. Dumitrache, L. Aroyo, and C. Welty. Crowdsourcing ground truth for medical relation ex-
traction. ACM Trans. Interact. Intell. Syst., Special Issue on Human-Centered Machine Learn-
ing (in publication), 2017.

[9] X. Feng, J. Guo, B. Qin, T. Liu, and Y. Liu. Effective deep memory networks for distant su-
pervised relation extraction. In C. Sierra, editor, Proceedings of the Twenty-Sixth International
Joint Conference on Artiﬁcial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25,
2017, pages 4002–4008. ijcai.org, 2017.

[10] X. Jiang, Q. Wang, P. Li, and B. Wang. Relation extraction with multi-instance multi-label

convolutional neural networks. In COLING, 2016.

[11] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant supervision for relation extraction without

labeled data. In Proc. of IJCNLP 2009: Volume 2, pages 1003–1011. ACL, 2009.

[12] T. Nguyen and R. Grishman. Relation extraction: Perspective from convolutional neural net-

works. In Proc. of NAACL-HLT, pages 39–48, 2015.

[13] S. Riedel, L. Yao, A. McCallum, and B. M. Marlin. Relation extraction with matrix factoriza-
tion and universal schemas. In L. Vanderwende, H. D. III, and K. Kirchhoff, editors, Proc. of
NAACL-HLT, pages 74–84. The Association for Computational Linguistics, 2013.

[14] C. Welty, J. Fan, D. Gondek, and A. Schlaikjer. Large scale relation detection. In Proc. of the

NAACL HLT FAM-LbR, pages 24–33, 2010.

6

7
1
0
2
 
v
o
N
 
9
2
 
 
]
L
C
.
s
c
[
 
 
2
v
6
8
1
5
0
.
1
1
7
1
:
v
i
X
r
a

False Positive and Cross-relation Signals in Distant
Supervision Data

Anca Dumitrache
Vrije Universiteit Amsterdam,
CAS IBM Netherlands
anca.dumitrache@vu.nl

Lora Aroyo
Vrije Universiteit Amsterdam
lora.aroyo@vu.nl

Chris Welty
Google Research, New York
cawelty@gmail.com

Abstract

Distant supervision (DS) is a well-established method for relation extraction from
text, based on the assumption that when a knowledge-base contains a relation
between a term pair, then sentences that contain that pair are likely to express the
relation. In this paper, we use the results of a crowdsourcing relation extraction
task to identify two problems with DS data quality: the widely varying degree
of false positives across different relations, and the observed causal connection
between relations that are not considered by the DS method. The crowdsourcing
data aggregation is performed using ambiguity-aware CrowdTruth metrics, that
are used to capture and interpret inter-annotator disagreement. We also present
preliminary results of using the crowd to enhance DS training data for a relation
classiﬁcation model, without requiring the crowd to annotate the entire set.

1

Introduction

Distant supervision (DS) [11, 14] is a well-established semi-supervised method for performing re-
lation extraction from text. It is based on the assumption that, when a knowledge-base contains a
relation between a pair of terms, then any sentence that contains that pair is likely to express the
relation. This approach can generate false positives, as not every mention of a term pair in a sen-
tence means a relation is also expressed [9]. Furthermore, dependencies between the semantics of
the relations such as causality or contradiction are also not considered by the DS methodology. It is
often assumed that these disadvantages are compensated for by the scale of the data a DS method
can produce, or can be largely overcome with crowdsourced human annotation [1].

In this paper we identify two speciﬁc problems we have found with distant supervision training
data: the widely varying degree of false positives across different TAC-KBP relation types, and the
observed causal connection between relations. We expose these problems using a novel approach to
gathering human annotated data, CrowdTruth [3, 4, 2], analyze them, and offer preliminary heuristic
and statistical approaches to incorporating them back into DS-based training, that provides better
sentence-level relation extraction results, without requiring crowdsourcing on the full set of data.

2 Background

In recent years, researchers have explored unsupervised methods for correcting DS data. For the
task of knowledge base completion, [9] applied memory networks both to correct false positives in
the data, and to capture dependencies between relations. For the same task, [10] developed a loss
function that works with multi-label data, in order to capture co-occurring relations. For relation
classiﬁcation from sentences, [7] learn embeddings that capture cross-signals between relations.
However, these approaches are dependent on training data that can express relation semantics with

6th Workshop on Automated Knowledge Base Construction (AKBC 2017) at NIPS 2017, Long Beach, CA,
USA.

at least some accuracy. The initial experiments presented in this paper show the error rate in the
DS data can be so high that unsupervised learning becomes unreliable when it comes to capturing
cross-relation signals.

Crowdsourcing is a well-used approach to correcting the mistakes in DS by scaling out cheap human
annotation. We have been studying the problem of collecting human annotations from the crowd
using the CrowdTruth methodology [2]. Our method differs in that it gathers many annotations for
the same examples, to better reﬂect properties like ambiguity, human error and spam, and the target
semantics [3]. We have used it successfully to improve DS results for the task of medical relation
extraction [8], achieving annotation quality equivalent to that provided by medical experts, at less
than half the cost.

3 Data and Annotation with CrowdTruth

We began by having the crowd annotate 2,500 sentences from the NIST TAC-KBP 2013 English
Slotﬁlling data that were annotated with DS. We split the data in half into a test and dev set. We
focused the crowd annotations on a subset of 16 relations (Fig.1) that occur between terms of types
P erson, Organization and Location. We ran a multiple-choice crowdsourcing task on Crowd-
ﬂower,1 asking 15 workers to annotate each sentence with the appropriate relations, or choose the
option N ON E if none of the relations presented apply. Workers are encouraged to select all rela-
tions that apply. Each worker was paid $0.05 per sentence. The data is available online.2

Traditionally, crowdsourcing annotations are aggregated by looking at the consensus of the workers,
through methods such as majority vote. This process is based on a simpliﬁed notion of truth, under
the assumption that a single right annotation exists for each example. However, in reality, truth is
not universal and is strongly inﬂuenced by a variety of factors, such as the ambiguity that is inherent
in natural language, resulting in artiﬁcial data that does not represent the diverse perspectives of the
crowd.
To address this issue, we proposed the CrowdTruth metrics3 of aggregating crowdsourcing data
while also capturing and interpreting inter-annotator disagreement. The main basis for measuring
quality in CrowdTruth is the triangle of disagreement, which links together sentences, workers, and
relations. It allows us to assess the quality of each worker, the clarity of each sentence, and the
ambiguity, similarity and frequency of each relation. The triangle model expresses how ambiguity
in any of the components of the triangle disseminates and inﬂuences the other components. For
example, an unclear sentence or an ambiguous relation would cause more disagreement between
workers, and thus, both need to be accounted for when measuring the quality of the workers, which
is then used to identify the spam workers. Among the CrowdTruth measures discussed in this paper,
we calculate the per-relation false positive (FP) rate and the causal power between relation pairs
(RCP) over the dev set. Spam removal was performed as well, but the details of this process are not
relevant for the paper.

For every sentence, the annotations of each worker form a binary worker vector, where the relations
selected are equal to ‘1’, and the rest to ‘0’. The sentence vector is the sum of all worker vectors
for the given sentence. Then for each relation, we compute the sentence-relation score of relation
i (SRSi) as the ratio of workers that picked that relation over the total of number of workers. The
SRS measures how clearly the relation is expressed in the sentence, and is used as a continuous truth
measure in a lot of our work. In order to make our results compatible with the AKBC community’s
discrete truth metrics (e.g. P, R, F1), we have chosen a threshold of 0.5 per relation, corresponding
to the majority vote, that allows for multiple relations to be considered correct in a sentence. False
positive rates are then computed per relation on the dev set with this threshold.

Causal power [5] is an estimate of the probability that the presence of one relation implies the pres-
ence of another. Given two relations i and j, RCP (Ri, Rj) = [P (Rj|Ri) − P (Rj|¬Ri)]/[1 −
P (Rj|¬Ri)], where P (Ri) is the probability that relation Ri is annotated in the sentence. This
probability can be calculated on a micro basis giving us the probability of one worker annotating
two relations together; the macro RCP calculates the probabilities in the sentence vectors, capturing

1http://crowdflower.com
2https://github.com/CrowdTruth/Open-Domain-Relation-Extraction
3https://git.io/v5iTB

2

causality as a result of two relations being annotated together in the same sentence, but not nec-
essarily by the same workers. We found micro RCP to be vastly inferior to macro RCP, which is
further evidence of the value of having multiple workers per sentence, and only include the macro
RCP results in this paper.

In addition to the dev and test sets, we also used a training set of 235,000 sentences annotated by
distant supervision from freebase relations, used in [13]. This data was used as a baseline for
training a relation extraction classiﬁer based on [12]. The model is a convolutional neural network,
adapted to be both multi-class and multi-label – we use a sigmoid cross-entropy loss function instead
of softmax cross-entropy, and the ﬁnal layer is normalized with the sigmoid function instead of
softmax – in order to make it possible for more than one relation to hold between two terms in one
sentence. To evaluate the relation extraction model on CrowdTruth data with discrete AKBC metrics,
we set a comparable threshold of 0.5 on the model conﬁdence score, separating between negative
and positive labels. However, the loss function is not computed using binary labels generated by the
threshold, but using the continuous labels in both the train and test sets.

4 Analysis

Figure 1: DS ratio of false positive over all positive labels, using the crowd as ground truth.

Using the SRS as a ground truth at a 0.5 threshold, Fig. 1 shows the correctness of the DS labels
on the dev set. There is considerable variation in DS data quality across relations. The origin and
place of death relations scored particularly badly, with more than 90% false positives. With such
a high error rate in some relations, it is arguable that any classiﬁer could learn anything meaningful,
regardless of algorithm or quantity of data.

Manual error analysis on the dev set showed that many sentences contain a P erson - Location
pair, where freebase speciﬁed both that the person resided in and died at that location. This makes
intuitive sense, people tend to die in the places they live.
In most of these cases, the sentence
expressed only the places of residence relation, leading to the false positives. The origin relation
data suffers from the same problem. Table 5 in the Appendix shows several examples of these
sentences. This led us to consider a heuristic solution to this problem as a headroom study as well
as a statistical solution. Both are discussed in Section 5.

The results of the macro RCP analysis for six of the relations we analyzed (Tab.1) shows that
the place of birth relation has a high causal power (0.64) over origin, meaning that when
place of birth is annotated in a sentence, origin is also likely to appear, with the inverse causal
power at 0.88. This high co-causality seems to indicate a confusion between the two relations.
Note also that these two relations have signiﬁcant differences in causal power in the DS-based
data. In contrast, place of death has a high causal power over places of residence in the DS

3

relation subset:

RCP for
place of death

place of birth (PoB), origin (O), places of residence
Table 1:
(PoR),
(EoM),
f ounded organization (FO),
top employee or member (TEoM). The scores show the causal power RCP (Ri, Rj) of relations Ri
in the rows, over the relations Rj in the columns. Signiﬁcant changes between crowd annotation based causal
power and distant supervision are in bold.

employee or member

(PoD),

PoB

PoB
1

Table 2: Crowd-based RCP
PoR
0.17
0.31
1
-0.01
-0.09
0.11
0.13

O
0.64
1
0.56
-0.03
-0.07
-0.36
-0.38

O 0.88
PoR
0.42
PoD -0.03
FO -0.07
EoM -0.45
TEoM -0.5

Table 3: DS-based RCP.

PoD
-0.12
-0.16
-0.1
1
-0.06
-0.47
-0.45

FO
-0.19
-0.29
-0.59
-0.04
1
0.62
0.86

EoM TEoM
-0.21
-0.2
-0.22
-0.22
0.13
0.12
-0.05
-0.05
0.13
0.1
0.82
1
0.86
1

PoB
1

PoB

O -0.02
PoR
0.65
PoD -0.06
FO -0.08
EoM -0.35
TEoM -0.16

O
-0.6
1
-0.33
-0.18
-0.06
0.35
-0.1

PoR
0.55
-0.11
1
0.17
-0.09
-0.42
-0.17

PoD
-0.14
-0.16
0.45
1
-0.06
-0.21
-0.12

FO
-0.54
-0.16
-0.7
-0.18
1
0.46
0.34

EoM TEoM
-0.57
-0.48
-0.15
0.19
-0.75
-0.68
-0.19
-0.13
0.09
0.09
0.66
1
0.24
1

data (0.45), reﬂecting the high error rate of place of death caused by the overlap in the KB with
places of residence.

In the crowd data we see a much higher co-causality for employee or member and
top employee or member, with only a slight preference in the data for what we expect to be the
“correct” causal direction (that top employee or member causes employee or member), but in
the DS-based analysis, the incorrect interpretation drops a lot. In manual error analysis we observed
that these are properties of the data set, which talk about more famous people who tend to be leaders
and founders, not “regular” employees. Table 6 in the Appendix shows several examples sentences
with false negative DS labels due to missing causality.

All told, the differences (in bold in Tab.1) between the crowd and DS-based causal power accounts
for some of the classiﬁcation errors in our trained system, and we expect them to be a signiﬁcant
cause of error in systems that try to learn cross-relation signals from DS data alone.

non-symmetric

the
f ounded organization,

top employee or member
Among
and
causes
top employee or member causes f ounded org. These again appear to be properties of the
data set.

employee or member

f ounded org,

pairs we

causes

causal

that

see

5 Enhancing Distant Supervision with CrowdTruth

We expect that the metrics from CrowdTruth annotation can be used to systematically enhance DS
data at scale, without requiring the crowd to annotate the entire set. As a preliminary headroom
exercise, we ran three experiments to test a few simple heuristic characterizations of our analysis,
and compared them to a baseline. In each experiment, we changed only the DS training set (using
the methods described below). We used the data in our previously held-out test set as an evaluation
target, again processing the continuous SRS scores with a threshold of 0.5 to yield discrete truth
values for calculating P, R, and F. Results are shown in Tab. 4.

1. DS: The baseline to which the other experiments are compared. The per-relation training

labels are binary based on the results of DS.

2. DS merged: Based on the results of the causality analysis, the training set is augmented
to reﬂect the highest cross-relation signals. We merge relations with symmetric RCP
(origin and place of birth), and add the implied relation in the case of asymmetric RCP
(employee or member and top employee or member). To merge, the DS baseline data
is updated so that the symmetric relations always co-occur, and adding caused relation
whenever the caused relation appears. This approach shows a huge improvement across
the board over the baseline, with the overall highest P and F.

3. DS RCP: Instead of manually identifying merged relations, the training data is augmented
by using the RCP scores. When a relation i has a positive DS label for a given sentence,
the labels of all other relations j (cid:54)= i are updated by adding the macro RCP that i has over
j. The maximum value for the label is clipped at 1, to keep scores in the [0, 1] interval.
The training labels in this set have continuous values, as opposed to the binary values in
the previous two sets. The formula for updating the training label for relation j in sentence
s is: DS RCP (s, i) = max[1, DS(s, j) + (cid:80)
i(cid:54)=j RCP (i, j) · DS(s, i)], where DS(s, i)

4

is the DS label of relation i in sentence s. This method was comparable in precision to the
baseline, but scored a huge win in recall. The recall increase makes sense, though we have
yet to investigate or explain the lack of increase in precision.

4. DS FP: Our analysis showed that the place of death relation was a large source of
false positives in the DS data, because most of the positives were actually express-
ing places of residence.
In every sentence in the DS training set that had a 1 for
place of death, we updated the score by subtracting its false positive ratio, which was
used in the loss function as described above. This did not impact the results over the base-
line, mainly because there were not many place of death relations in the DS data nor the
test set, and any improvement did not impact the overall result. We are conﬁdent that more
systematic treatment of false positive rates will improve performance.

Table 4: Precision & Recall at 20,000 training steps.
Precision Recall F1 score
0.22
0.33
0.48
0.22

DS
DS merged
DS RCP
DS FP

0.2
0.37
0.27
0.21

0.19
0.43
0.19
0.21

6 Discussion

The preliminary results are not overwhelming, but highly indicative. There is considerable headroom
in cross-relation signals, and a more robust approach holds promise to eliminate manual analysis,
and work as part of an overall pipeline that includes partial crowd data. We have shown a very
signiﬁcant variation in the false positive rate in distant supervision data, and it seems extremely
likely that this can be exploited to improve training.

We are currently considering experiments that take advantage of another aspect of our CrowdTruth
method: the identiﬁcation of ambiguity in sentences where workers do not agree on the outcome.
We believe a more continuous truth measure as opposed to the rather arbitrary discrete measure will
be productive. Finally, we are particularly excited about the possibility of using our approach in
conjunction with logical reasoning approaches such as those reported in [6]. In this case, we are
looking at more informed data that reﬂects human understanding and properties of the data set, to
discover candidate relation pairs for investigating rules.

Appendix

Sentence

Table 5: Example sentences with false positive place of death and origin DS labels due to multiple relations
in the KB over P erson - Location term types.

Relation

Crowd
SRS

DS label

After growing up on Cat Island, Tony McKay moved to
New York City at age 17 to study architecture.

The ﬁlm is based very loosely on the lives of Wolfgang
Amadeus Mozart and Antonio Salieri, two composers
who lived in Vienna, Austria.
Marku Ribas is the side more Black music of this group
and was Bob Marley’s friend in the 1970s, Jamaica,
where he lived.

Osama bin Laden had moved from Saudi Arabia to
Sudan during the 1990-91 Gulf War.

place of death

places of residence

place of death

places of residence

places of residence

origin

origin

places of residence

0.004

0.995

0.074

0.865

0

0.87

0.3

0.74

1

1

1

1

1

1

1

1

5

Table 6: Example sentences with false negative employee or member and origin DS labels due to missing
causal connections.

Sentence

Relation

Crowd
SRS

DS label

China on Monday ofﬁcially appointed Donald
Tsang as Hong Kong’s chief executive for a second
term.
More than 3,000 taxi drivers blocked Rome’s
historic centre Wednesday to protest extra licences
given by mayor Walter Veltroni.
Early years Joey Harrington was born and raised in
Portland, Oregon, where he has resided his entire
life.
Nelli Zhiganshina (born March 31, 1987 in
Moscow, Russia) is a Russian ice dancer who
currently represents Germany.

employee or member

top employee or member

employee or member

top employee or member

place of birth

origin

origin

place of birth

0.623

0.753

0.529

0.841

0.645

0.867

0.555

0.791

0

1

0

1

0

1

0

1

References

[1] G. Angeli, J. Tibshirani, J. Wu, and C. Manning. Combining distant and partial supervision for

relation extraction. In EMNLP, pages 1556–1567, 2014.

[2] L. Aroyo and C. Welty. Crowd Truth: Harnessing disagreement in crowdsourcing a relation

extraction gold standard. Web Science 2013. ACM, 2013.

[3] L. Aroyo and C. Welty. The Three Sides of CrowdTruth. Journal of Human Computation,

[4] L. Aroyo and C. Welty. Truth is a lie: Crowd truth and the seven myths of human annotation.

[5] P. W. Cheng. From covariation to causation: A causal power theory. Psychological Review,

1:31–34, 2014.

AI Magazine, 36(1):15–24, 2015.

104(2):367–405, 1997.

[6] T. Demeester, T. Rockt¨aschel, and S. Riedel. Regularizing relation representations by ﬁrst-

order implications. In AKBC@ NAACL-HLT, pages 75–80, 2016.

[7] C. N. dos Santos, B. Xiang, and B. Zhou. Classifying relations by ranking with convolu-
In Proceedings of the 53rd Annual Meeting of the Association for
tional neural networks.
Computational Linguistics and the 7th International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31,
2015, Beijing, China, Volume 1: Long Papers, pages 626–634. The Association for Computer
Linguistics, 2015.

[8] A. Dumitrache, L. Aroyo, and C. Welty. Crowdsourcing ground truth for medical relation ex-
traction. ACM Trans. Interact. Intell. Syst., Special Issue on Human-Centered Machine Learn-
ing (in publication), 2017.

[9] X. Feng, J. Guo, B. Qin, T. Liu, and Y. Liu. Effective deep memory networks for distant su-
pervised relation extraction. In C. Sierra, editor, Proceedings of the Twenty-Sixth International
Joint Conference on Artiﬁcial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25,
2017, pages 4002–4008. ijcai.org, 2017.

[10] X. Jiang, Q. Wang, P. Li, and B. Wang. Relation extraction with multi-instance multi-label

convolutional neural networks. In COLING, 2016.

[11] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant supervision for relation extraction without

labeled data. In Proc. of IJCNLP 2009: Volume 2, pages 1003–1011. ACL, 2009.

[12] T. Nguyen and R. Grishman. Relation extraction: Perspective from convolutional neural net-

works. In Proc. of NAACL-HLT, pages 39–48, 2015.

[13] S. Riedel, L. Yao, A. McCallum, and B. M. Marlin. Relation extraction with matrix factoriza-
tion and universal schemas. In L. Vanderwende, H. D. III, and K. Kirchhoff, editors, Proc. of
NAACL-HLT, pages 74–84. The Association for Computational Linguistics, 2013.

[14] C. Welty, J. Fan, D. Gondek, and A. Schlaikjer. Large scale relation detection. In Proc. of the

NAACL HLT FAM-LbR, pages 24–33, 2010.

6

7
1
0
2
 
v
o
N
 
9
2
 
 
]
L
C
.
s
c
[
 
 
2
v
6
8
1
5
0
.
1
1
7
1
:
v
i
X
r
a

False Positive and Cross-relation Signals in Distant
Supervision Data

Anca Dumitrache
Vrije Universiteit Amsterdam,
CAS IBM Netherlands
anca.dumitrache@vu.nl

Lora Aroyo
Vrije Universiteit Amsterdam
lora.aroyo@vu.nl

Chris Welty
Google Research, New York
cawelty@gmail.com

Abstract

Distant supervision (DS) is a well-established method for relation extraction from
text, based on the assumption that when a knowledge-base contains a relation
between a term pair, then sentences that contain that pair are likely to express the
relation. In this paper, we use the results of a crowdsourcing relation extraction
task to identify two problems with DS data quality: the widely varying degree
of false positives across different relations, and the observed causal connection
between relations that are not considered by the DS method. The crowdsourcing
data aggregation is performed using ambiguity-aware CrowdTruth metrics, that
are used to capture and interpret inter-annotator disagreement. We also present
preliminary results of using the crowd to enhance DS training data for a relation
classiﬁcation model, without requiring the crowd to annotate the entire set.

1

Introduction

Distant supervision (DS) [11, 14] is a well-established semi-supervised method for performing re-
lation extraction from text. It is based on the assumption that, when a knowledge-base contains a
relation between a pair of terms, then any sentence that contains that pair is likely to express the
relation. This approach can generate false positives, as not every mention of a term pair in a sen-
tence means a relation is also expressed [9]. Furthermore, dependencies between the semantics of
the relations such as causality or contradiction are also not considered by the DS methodology. It is
often assumed that these disadvantages are compensated for by the scale of the data a DS method
can produce, or can be largely overcome with crowdsourced human annotation [1].

In this paper we identify two speciﬁc problems we have found with distant supervision training
data: the widely varying degree of false positives across different TAC-KBP relation types, and the
observed causal connection between relations. We expose these problems using a novel approach to
gathering human annotated data, CrowdTruth [3, 4, 2], analyze them, and offer preliminary heuristic
and statistical approaches to incorporating them back into DS-based training, that provides better
sentence-level relation extraction results, without requiring crowdsourcing on the full set of data.

2 Background

In recent years, researchers have explored unsupervised methods for correcting DS data. For the
task of knowledge base completion, [9] applied memory networks both to correct false positives in
the data, and to capture dependencies between relations. For the same task, [10] developed a loss
function that works with multi-label data, in order to capture co-occurring relations. For relation
classiﬁcation from sentences, [7] learn embeddings that capture cross-signals between relations.
However, these approaches are dependent on training data that can express relation semantics with

6th Workshop on Automated Knowledge Base Construction (AKBC 2017) at NIPS 2017, Long Beach, CA,
USA.

at least some accuracy. The initial experiments presented in this paper show the error rate in the
DS data can be so high that unsupervised learning becomes unreliable when it comes to capturing
cross-relation signals.

Crowdsourcing is a well-used approach to correcting the mistakes in DS by scaling out cheap human
annotation. We have been studying the problem of collecting human annotations from the crowd
using the CrowdTruth methodology [2]. Our method differs in that it gathers many annotations for
the same examples, to better reﬂect properties like ambiguity, human error and spam, and the target
semantics [3]. We have used it successfully to improve DS results for the task of medical relation
extraction [8], achieving annotation quality equivalent to that provided by medical experts, at less
than half the cost.

3 Data and Annotation with CrowdTruth

We began by having the crowd annotate 2,500 sentences from the NIST TAC-KBP 2013 English
Slotﬁlling data that were annotated with DS. We split the data in half into a test and dev set. We
focused the crowd annotations on a subset of 16 relations (Fig.1) that occur between terms of types
P erson, Organization and Location. We ran a multiple-choice crowdsourcing task on Crowd-
ﬂower,1 asking 15 workers to annotate each sentence with the appropriate relations, or choose the
option N ON E if none of the relations presented apply. Workers are encouraged to select all rela-
tions that apply. Each worker was paid $0.05 per sentence. The data is available online.2

Traditionally, crowdsourcing annotations are aggregated by looking at the consensus of the workers,
through methods such as majority vote. This process is based on a simpliﬁed notion of truth, under
the assumption that a single right annotation exists for each example. However, in reality, truth is
not universal and is strongly inﬂuenced by a variety of factors, such as the ambiguity that is inherent
in natural language, resulting in artiﬁcial data that does not represent the diverse perspectives of the
crowd.
To address this issue, we proposed the CrowdTruth metrics3 of aggregating crowdsourcing data
while also capturing and interpreting inter-annotator disagreement. The main basis for measuring
quality in CrowdTruth is the triangle of disagreement, which links together sentences, workers, and
relations. It allows us to assess the quality of each worker, the clarity of each sentence, and the
ambiguity, similarity and frequency of each relation. The triangle model expresses how ambiguity
in any of the components of the triangle disseminates and inﬂuences the other components. For
example, an unclear sentence or an ambiguous relation would cause more disagreement between
workers, and thus, both need to be accounted for when measuring the quality of the workers, which
is then used to identify the spam workers. Among the CrowdTruth measures discussed in this paper,
we calculate the per-relation false positive (FP) rate and the causal power between relation pairs
(RCP) over the dev set. Spam removal was performed as well, but the details of this process are not
relevant for the paper.

For every sentence, the annotations of each worker form a binary worker vector, where the relations
selected are equal to ‘1’, and the rest to ‘0’. The sentence vector is the sum of all worker vectors
for the given sentence. Then for each relation, we compute the sentence-relation score of relation
i (SRSi) as the ratio of workers that picked that relation over the total of number of workers. The
SRS measures how clearly the relation is expressed in the sentence, and is used as a continuous truth
measure in a lot of our work. In order to make our results compatible with the AKBC community’s
discrete truth metrics (e.g. P, R, F1), we have chosen a threshold of 0.5 per relation, corresponding
to the majority vote, that allows for multiple relations to be considered correct in a sentence. False
positive rates are then computed per relation on the dev set with this threshold.

Causal power [5] is an estimate of the probability that the presence of one relation implies the pres-
ence of another. Given two relations i and j, RCP (Ri, Rj) = [P (Rj|Ri) − P (Rj|¬Ri)]/[1 −
P (Rj|¬Ri)], where P (Ri) is the probability that relation Ri is annotated in the sentence. This
probability can be calculated on a micro basis giving us the probability of one worker annotating
two relations together; the macro RCP calculates the probabilities in the sentence vectors, capturing

1http://crowdflower.com
2https://github.com/CrowdTruth/Open-Domain-Relation-Extraction
3https://git.io/v5iTB

2

causality as a result of two relations being annotated together in the same sentence, but not nec-
essarily by the same workers. We found micro RCP to be vastly inferior to macro RCP, which is
further evidence of the value of having multiple workers per sentence, and only include the macro
RCP results in this paper.

In addition to the dev and test sets, we also used a training set of 235,000 sentences annotated by
distant supervision from freebase relations, used in [13]. This data was used as a baseline for
training a relation extraction classiﬁer based on [12]. The model is a convolutional neural network,
adapted to be both multi-class and multi-label – we use a sigmoid cross-entropy loss function instead
of softmax cross-entropy, and the ﬁnal layer is normalized with the sigmoid function instead of
softmax – in order to make it possible for more than one relation to hold between two terms in one
sentence. To evaluate the relation extraction model on CrowdTruth data with discrete AKBC metrics,
we set a comparable threshold of 0.5 on the model conﬁdence score, separating between negative
and positive labels. However, the loss function is not computed using binary labels generated by the
threshold, but using the continuous labels in both the train and test sets.

4 Analysis

Figure 1: DS ratio of false positive over all positive labels, using the crowd as ground truth.

Using the SRS as a ground truth at a 0.5 threshold, Fig. 1 shows the correctness of the DS labels
on the dev set. There is considerable variation in DS data quality across relations. The origin and
place of death relations scored particularly badly, with more than 90% false positives. With such
a high error rate in some relations, it is arguable that any classiﬁer could learn anything meaningful,
regardless of algorithm or quantity of data.

Manual error analysis on the dev set showed that many sentences contain a P erson - Location
pair, where freebase speciﬁed both that the person resided in and died at that location. This makes
intuitive sense, people tend to die in the places they live.
In most of these cases, the sentence
expressed only the places of residence relation, leading to the false positives. The origin relation
data suffers from the same problem. Table 5 in the Appendix shows several examples of these
sentences. This led us to consider a heuristic solution to this problem as a headroom study as well
as a statistical solution. Both are discussed in Section 5.

The results of the macro RCP analysis for six of the relations we analyzed (Tab.1) shows that
the place of birth relation has a high causal power (0.64) over origin, meaning that when
place of birth is annotated in a sentence, origin is also likely to appear, with the inverse causal
power at 0.88. This high co-causality seems to indicate a confusion between the two relations.
Note also that these two relations have signiﬁcant differences in causal power in the DS-based
data. In contrast, place of death has a high causal power over places of residence in the DS

3

relation subset:

RCP for
place of death

place of birth (PoB), origin (O), places of residence
Table 1:
(PoR),
(EoM),
f ounded organization (FO),
top employee or member (TEoM). The scores show the causal power RCP (Ri, Rj) of relations Ri
in the rows, over the relations Rj in the columns. Signiﬁcant changes between crowd annotation based causal
power and distant supervision are in bold.

employee or member

(PoD),

PoB

PoB
1

Table 2: Crowd-based RCP
PoR
0.17
0.31
1
-0.01
-0.09
0.11
0.13

O
0.64
1
0.56
-0.03
-0.07
-0.36
-0.38

O 0.88
PoR
0.42
PoD -0.03
FO -0.07
EoM -0.45
TEoM -0.5

Table 3: DS-based RCP.

PoD
-0.12
-0.16
-0.1
1
-0.06
-0.47
-0.45

FO
-0.19
-0.29
-0.59
-0.04
1
0.62
0.86

EoM TEoM
-0.21
-0.2
-0.22
-0.22
0.13
0.12
-0.05
-0.05
0.13
0.1
0.82
1
0.86
1

PoB
1

PoB

O -0.02
PoR
0.65
PoD -0.06
FO -0.08
EoM -0.35
TEoM -0.16

O
-0.6
1
-0.33
-0.18
-0.06
0.35
-0.1

PoR
0.55
-0.11
1
0.17
-0.09
-0.42
-0.17

PoD
-0.14
-0.16
0.45
1
-0.06
-0.21
-0.12

FO
-0.54
-0.16
-0.7
-0.18
1
0.46
0.34

EoM TEoM
-0.57
-0.48
-0.15
0.19
-0.75
-0.68
-0.19
-0.13
0.09
0.09
0.66
1
0.24
1

data (0.45), reﬂecting the high error rate of place of death caused by the overlap in the KB with
places of residence.

In the crowd data we see a much higher co-causality for employee or member and
top employee or member, with only a slight preference in the data for what we expect to be the
“correct” causal direction (that top employee or member causes employee or member), but in
the DS-based analysis, the incorrect interpretation drops a lot. In manual error analysis we observed
that these are properties of the data set, which talk about more famous people who tend to be leaders
and founders, not “regular” employees. Table 6 in the Appendix shows several examples sentences
with false negative DS labels due to missing causality.

All told, the differences (in bold in Tab.1) between the crowd and DS-based causal power accounts
for some of the classiﬁcation errors in our trained system, and we expect them to be a signiﬁcant
cause of error in systems that try to learn cross-relation signals from DS data alone.

non-symmetric

the
f ounded organization,

top employee or member
Among
and
causes
top employee or member causes f ounded org. These again appear to be properties of the
data set.

employee or member

f ounded org,

pairs we

causes

causal

that

see

5 Enhancing Distant Supervision with CrowdTruth

We expect that the metrics from CrowdTruth annotation can be used to systematically enhance DS
data at scale, without requiring the crowd to annotate the entire set. As a preliminary headroom
exercise, we ran three experiments to test a few simple heuristic characterizations of our analysis,
and compared them to a baseline. In each experiment, we changed only the DS training set (using
the methods described below). We used the data in our previously held-out test set as an evaluation
target, again processing the continuous SRS scores with a threshold of 0.5 to yield discrete truth
values for calculating P, R, and F. Results are shown in Tab. 4.

1. DS: The baseline to which the other experiments are compared. The per-relation training

labels are binary based on the results of DS.

2. DS merged: Based on the results of the causality analysis, the training set is augmented
to reﬂect the highest cross-relation signals. We merge relations with symmetric RCP
(origin and place of birth), and add the implied relation in the case of asymmetric RCP
(employee or member and top employee or member). To merge, the DS baseline data
is updated so that the symmetric relations always co-occur, and adding caused relation
whenever the caused relation appears. This approach shows a huge improvement across
the board over the baseline, with the overall highest P and F.

3. DS RCP: Instead of manually identifying merged relations, the training data is augmented
by using the RCP scores. When a relation i has a positive DS label for a given sentence,
the labels of all other relations j (cid:54)= i are updated by adding the macro RCP that i has over
j. The maximum value for the label is clipped at 1, to keep scores in the [0, 1] interval.
The training labels in this set have continuous values, as opposed to the binary values in
the previous two sets. The formula for updating the training label for relation j in sentence
s is: DS RCP (s, i) = max[1, DS(s, j) + (cid:80)
i(cid:54)=j RCP (i, j) · DS(s, i)], where DS(s, i)

4

is the DS label of relation i in sentence s. This method was comparable in precision to the
baseline, but scored a huge win in recall. The recall increase makes sense, though we have
yet to investigate or explain the lack of increase in precision.

4. DS FP: Our analysis showed that the place of death relation was a large source of
false positives in the DS data, because most of the positives were actually express-
ing places of residence.
In every sentence in the DS training set that had a 1 for
place of death, we updated the score by subtracting its false positive ratio, which was
used in the loss function as described above. This did not impact the results over the base-
line, mainly because there were not many place of death relations in the DS data nor the
test set, and any improvement did not impact the overall result. We are conﬁdent that more
systematic treatment of false positive rates will improve performance.

Table 4: Precision & Recall at 20,000 training steps.
Precision Recall F1 score
0.22
0.33
0.48
0.22

DS
DS merged
DS RCP
DS FP

0.2
0.37
0.27
0.21

0.19
0.43
0.19
0.21

6 Discussion

The preliminary results are not overwhelming, but highly indicative. There is considerable headroom
in cross-relation signals, and a more robust approach holds promise to eliminate manual analysis,
and work as part of an overall pipeline that includes partial crowd data. We have shown a very
signiﬁcant variation in the false positive rate in distant supervision data, and it seems extremely
likely that this can be exploited to improve training.

We are currently considering experiments that take advantage of another aspect of our CrowdTruth
method: the identiﬁcation of ambiguity in sentences where workers do not agree on the outcome.
We believe a more continuous truth measure as opposed to the rather arbitrary discrete measure will
be productive. Finally, we are particularly excited about the possibility of using our approach in
conjunction with logical reasoning approaches such as those reported in [6]. In this case, we are
looking at more informed data that reﬂects human understanding and properties of the data set, to
discover candidate relation pairs for investigating rules.

Appendix

Sentence

Table 5: Example sentences with false positive place of death and origin DS labels due to multiple relations
in the KB over P erson - Location term types.

Relation

Crowd
SRS

DS label

After growing up on Cat Island, Tony McKay moved to
New York City at age 17 to study architecture.

The ﬁlm is based very loosely on the lives of Wolfgang
Amadeus Mozart and Antonio Salieri, two composers
who lived in Vienna, Austria.
Marku Ribas is the side more Black music of this group
and was Bob Marley’s friend in the 1970s, Jamaica,
where he lived.

Osama bin Laden had moved from Saudi Arabia to
Sudan during the 1990-91 Gulf War.

place of death

places of residence

place of death

places of residence

places of residence

origin

origin

places of residence

0.004

0.995

0.074

0.865

0

0.87

0.3

0.74

1

1

1

1

1

1

1

1

5

Table 6: Example sentences with false negative employee or member and origin DS labels due to missing
causal connections.

Sentence

Relation

Crowd
SRS

DS label

China on Monday ofﬁcially appointed Donald
Tsang as Hong Kong’s chief executive for a second
term.
More than 3,000 taxi drivers blocked Rome’s
historic centre Wednesday to protest extra licences
given by mayor Walter Veltroni.
Early years Joey Harrington was born and raised in
Portland, Oregon, where he has resided his entire
life.
Nelli Zhiganshina (born March 31, 1987 in
Moscow, Russia) is a Russian ice dancer who
currently represents Germany.

employee or member

top employee or member

employee or member

top employee or member

place of birth

origin

origin

place of birth

0.623

0.753

0.529

0.841

0.645

0.867

0.555

0.791

0

1

0

1

0

1

0

1

References

[1] G. Angeli, J. Tibshirani, J. Wu, and C. Manning. Combining distant and partial supervision for

relation extraction. In EMNLP, pages 1556–1567, 2014.

[2] L. Aroyo and C. Welty. Crowd Truth: Harnessing disagreement in crowdsourcing a relation

extraction gold standard. Web Science 2013. ACM, 2013.

[3] L. Aroyo and C. Welty. The Three Sides of CrowdTruth. Journal of Human Computation,

[4] L. Aroyo and C. Welty. Truth is a lie: Crowd truth and the seven myths of human annotation.

[5] P. W. Cheng. From covariation to causation: A causal power theory. Psychological Review,

1:31–34, 2014.

AI Magazine, 36(1):15–24, 2015.

104(2):367–405, 1997.

[6] T. Demeester, T. Rockt¨aschel, and S. Riedel. Regularizing relation representations by ﬁrst-

order implications. In AKBC@ NAACL-HLT, pages 75–80, 2016.

[7] C. N. dos Santos, B. Xiang, and B. Zhou. Classifying relations by ranking with convolu-
In Proceedings of the 53rd Annual Meeting of the Association for
tional neural networks.
Computational Linguistics and the 7th International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31,
2015, Beijing, China, Volume 1: Long Papers, pages 626–634. The Association for Computer
Linguistics, 2015.

[8] A. Dumitrache, L. Aroyo, and C. Welty. Crowdsourcing ground truth for medical relation ex-
traction. ACM Trans. Interact. Intell. Syst., Special Issue on Human-Centered Machine Learn-
ing (in publication), 2017.

[9] X. Feng, J. Guo, B. Qin, T. Liu, and Y. Liu. Effective deep memory networks for distant su-
pervised relation extraction. In C. Sierra, editor, Proceedings of the Twenty-Sixth International
Joint Conference on Artiﬁcial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25,
2017, pages 4002–4008. ijcai.org, 2017.

[10] X. Jiang, Q. Wang, P. Li, and B. Wang. Relation extraction with multi-instance multi-label

convolutional neural networks. In COLING, 2016.

[11] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant supervision for relation extraction without

labeled data. In Proc. of IJCNLP 2009: Volume 2, pages 1003–1011. ACL, 2009.

[12] T. Nguyen and R. Grishman. Relation extraction: Perspective from convolutional neural net-

works. In Proc. of NAACL-HLT, pages 39–48, 2015.

[13] S. Riedel, L. Yao, A. McCallum, and B. M. Marlin. Relation extraction with matrix factoriza-
tion and universal schemas. In L. Vanderwende, H. D. III, and K. Kirchhoff, editors, Proc. of
NAACL-HLT, pages 74–84. The Association for Computational Linguistics, 2013.

[14] C. Welty, J. Fan, D. Gondek, and A. Schlaikjer. Large scale relation detection. In Proc. of the

NAACL HLT FAM-LbR, pages 24–33, 2010.

6

7
1
0
2
 
v
o
N
 
9
2
 
 
]
L
C
.
s
c
[
 
 
2
v
6
8
1
5
0
.
1
1
7
1
:
v
i
X
r
a

False Positive and Cross-relation Signals in Distant
Supervision Data

Anca Dumitrache
Vrije Universiteit Amsterdam,
CAS IBM Netherlands
anca.dumitrache@vu.nl

Lora Aroyo
Vrije Universiteit Amsterdam
lora.aroyo@vu.nl

Chris Welty
Google Research, New York
cawelty@gmail.com

Abstract

Distant supervision (DS) is a well-established method for relation extraction from
text, based on the assumption that when a knowledge-base contains a relation
between a term pair, then sentences that contain that pair are likely to express the
relation. In this paper, we use the results of a crowdsourcing relation extraction
task to identify two problems with DS data quality: the widely varying degree
of false positives across different relations, and the observed causal connection
between relations that are not considered by the DS method. The crowdsourcing
data aggregation is performed using ambiguity-aware CrowdTruth metrics, that
are used to capture and interpret inter-annotator disagreement. We also present
preliminary results of using the crowd to enhance DS training data for a relation
classiﬁcation model, without requiring the crowd to annotate the entire set.

1

Introduction

Distant supervision (DS) [11, 14] is a well-established semi-supervised method for performing re-
lation extraction from text. It is based on the assumption that, when a knowledge-base contains a
relation between a pair of terms, then any sentence that contains that pair is likely to express the
relation. This approach can generate false positives, as not every mention of a term pair in a sen-
tence means a relation is also expressed [9]. Furthermore, dependencies between the semantics of
the relations such as causality or contradiction are also not considered by the DS methodology. It is
often assumed that these disadvantages are compensated for by the scale of the data a DS method
can produce, or can be largely overcome with crowdsourced human annotation [1].

In this paper we identify two speciﬁc problems we have found with distant supervision training
data: the widely varying degree of false positives across different TAC-KBP relation types, and the
observed causal connection between relations. We expose these problems using a novel approach to
gathering human annotated data, CrowdTruth [3, 4, 2], analyze them, and offer preliminary heuristic
and statistical approaches to incorporating them back into DS-based training, that provides better
sentence-level relation extraction results, without requiring crowdsourcing on the full set of data.

2 Background

In recent years, researchers have explored unsupervised methods for correcting DS data. For the
task of knowledge base completion, [9] applied memory networks both to correct false positives in
the data, and to capture dependencies between relations. For the same task, [10] developed a loss
function that works with multi-label data, in order to capture co-occurring relations. For relation
classiﬁcation from sentences, [7] learn embeddings that capture cross-signals between relations.
However, these approaches are dependent on training data that can express relation semantics with

6th Workshop on Automated Knowledge Base Construction (AKBC 2017) at NIPS 2017, Long Beach, CA,
USA.

at least some accuracy. The initial experiments presented in this paper show the error rate in the
DS data can be so high that unsupervised learning becomes unreliable when it comes to capturing
cross-relation signals.

Crowdsourcing is a well-used approach to correcting the mistakes in DS by scaling out cheap human
annotation. We have been studying the problem of collecting human annotations from the crowd
using the CrowdTruth methodology [2]. Our method differs in that it gathers many annotations for
the same examples, to better reﬂect properties like ambiguity, human error and spam, and the target
semantics [3]. We have used it successfully to improve DS results for the task of medical relation
extraction [8], achieving annotation quality equivalent to that provided by medical experts, at less
than half the cost.

3 Data and Annotation with CrowdTruth

We began by having the crowd annotate 2,500 sentences from the NIST TAC-KBP 2013 English
Slotﬁlling data that were annotated with DS. We split the data in half into a test and dev set. We
focused the crowd annotations on a subset of 16 relations (Fig.1) that occur between terms of types
P erson, Organization and Location. We ran a multiple-choice crowdsourcing task on Crowd-
ﬂower,1 asking 15 workers to annotate each sentence with the appropriate relations, or choose the
option N ON E if none of the relations presented apply. Workers are encouraged to select all rela-
tions that apply. Each worker was paid $0.05 per sentence. The data is available online.2

Traditionally, crowdsourcing annotations are aggregated by looking at the consensus of the workers,
through methods such as majority vote. This process is based on a simpliﬁed notion of truth, under
the assumption that a single right annotation exists for each example. However, in reality, truth is
not universal and is strongly inﬂuenced by a variety of factors, such as the ambiguity that is inherent
in natural language, resulting in artiﬁcial data that does not represent the diverse perspectives of the
crowd.
To address this issue, we proposed the CrowdTruth metrics3 of aggregating crowdsourcing data
while also capturing and interpreting inter-annotator disagreement. The main basis for measuring
quality in CrowdTruth is the triangle of disagreement, which links together sentences, workers, and
relations. It allows us to assess the quality of each worker, the clarity of each sentence, and the
ambiguity, similarity and frequency of each relation. The triangle model expresses how ambiguity
in any of the components of the triangle disseminates and inﬂuences the other components. For
example, an unclear sentence or an ambiguous relation would cause more disagreement between
workers, and thus, both need to be accounted for when measuring the quality of the workers, which
is then used to identify the spam workers. Among the CrowdTruth measures discussed in this paper,
we calculate the per-relation false positive (FP) rate and the causal power between relation pairs
(RCP) over the dev set. Spam removal was performed as well, but the details of this process are not
relevant for the paper.

For every sentence, the annotations of each worker form a binary worker vector, where the relations
selected are equal to ‘1’, and the rest to ‘0’. The sentence vector is the sum of all worker vectors
for the given sentence. Then for each relation, we compute the sentence-relation score of relation
i (SRSi) as the ratio of workers that picked that relation over the total of number of workers. The
SRS measures how clearly the relation is expressed in the sentence, and is used as a continuous truth
measure in a lot of our work. In order to make our results compatible with the AKBC community’s
discrete truth metrics (e.g. P, R, F1), we have chosen a threshold of 0.5 per relation, corresponding
to the majority vote, that allows for multiple relations to be considered correct in a sentence. False
positive rates are then computed per relation on the dev set with this threshold.

Causal power [5] is an estimate of the probability that the presence of one relation implies the pres-
ence of another. Given two relations i and j, RCP (Ri, Rj) = [P (Rj|Ri) − P (Rj|¬Ri)]/[1 −
P (Rj|¬Ri)], where P (Ri) is the probability that relation Ri is annotated in the sentence. This
probability can be calculated on a micro basis giving us the probability of one worker annotating
two relations together; the macro RCP calculates the probabilities in the sentence vectors, capturing

1http://crowdflower.com
2https://github.com/CrowdTruth/Open-Domain-Relation-Extraction
3https://git.io/v5iTB

2

causality as a result of two relations being annotated together in the same sentence, but not nec-
essarily by the same workers. We found micro RCP to be vastly inferior to macro RCP, which is
further evidence of the value of having multiple workers per sentence, and only include the macro
RCP results in this paper.

In addition to the dev and test sets, we also used a training set of 235,000 sentences annotated by
distant supervision from freebase relations, used in [13]. This data was used as a baseline for
training a relation extraction classiﬁer based on [12]. The model is a convolutional neural network,
adapted to be both multi-class and multi-label – we use a sigmoid cross-entropy loss function instead
of softmax cross-entropy, and the ﬁnal layer is normalized with the sigmoid function instead of
softmax – in order to make it possible for more than one relation to hold between two terms in one
sentence. To evaluate the relation extraction model on CrowdTruth data with discrete AKBC metrics,
we set a comparable threshold of 0.5 on the model conﬁdence score, separating between negative
and positive labels. However, the loss function is not computed using binary labels generated by the
threshold, but using the continuous labels in both the train and test sets.

4 Analysis

Figure 1: DS ratio of false positive over all positive labels, using the crowd as ground truth.

Using the SRS as a ground truth at a 0.5 threshold, Fig. 1 shows the correctness of the DS labels
on the dev set. There is considerable variation in DS data quality across relations. The origin and
place of death relations scored particularly badly, with more than 90% false positives. With such
a high error rate in some relations, it is arguable that any classiﬁer could learn anything meaningful,
regardless of algorithm or quantity of data.

Manual error analysis on the dev set showed that many sentences contain a P erson - Location
pair, where freebase speciﬁed both that the person resided in and died at that location. This makes
intuitive sense, people tend to die in the places they live.
In most of these cases, the sentence
expressed only the places of residence relation, leading to the false positives. The origin relation
data suffers from the same problem. Table 5 in the Appendix shows several examples of these
sentences. This led us to consider a heuristic solution to this problem as a headroom study as well
as a statistical solution. Both are discussed in Section 5.

The results of the macro RCP analysis for six of the relations we analyzed (Tab.1) shows that
the place of birth relation has a high causal power (0.64) over origin, meaning that when
place of birth is annotated in a sentence, origin is also likely to appear, with the inverse causal
power at 0.88. This high co-causality seems to indicate a confusion between the two relations.
Note also that these two relations have signiﬁcant differences in causal power in the DS-based
data. In contrast, place of death has a high causal power over places of residence in the DS

3

relation subset:

RCP for
place of death

place of birth (PoB), origin (O), places of residence
Table 1:
(PoR),
(EoM),
f ounded organization (FO),
top employee or member (TEoM). The scores show the causal power RCP (Ri, Rj) of relations Ri
in the rows, over the relations Rj in the columns. Signiﬁcant changes between crowd annotation based causal
power and distant supervision are in bold.

employee or member

(PoD),

PoB

PoB
1

Table 2: Crowd-based RCP
PoR
0.17
0.31
1
-0.01
-0.09
0.11
0.13

O
0.64
1
0.56
-0.03
-0.07
-0.36
-0.38

O 0.88
PoR
0.42
PoD -0.03
FO -0.07
EoM -0.45
TEoM -0.5

Table 3: DS-based RCP.

PoD
-0.12
-0.16
-0.1
1
-0.06
-0.47
-0.45

FO
-0.19
-0.29
-0.59
-0.04
1
0.62
0.86

EoM TEoM
-0.21
-0.2
-0.22
-0.22
0.13
0.12
-0.05
-0.05
0.13
0.1
0.82
1
0.86
1

PoB
1

PoB

O -0.02
PoR
0.65
PoD -0.06
FO -0.08
EoM -0.35
TEoM -0.16

O
-0.6
1
-0.33
-0.18
-0.06
0.35
-0.1

PoR
0.55
-0.11
1
0.17
-0.09
-0.42
-0.17

PoD
-0.14
-0.16
0.45
1
-0.06
-0.21
-0.12

FO
-0.54
-0.16
-0.7
-0.18
1
0.46
0.34

EoM TEoM
-0.57
-0.48
-0.15
0.19
-0.75
-0.68
-0.19
-0.13
0.09
0.09
0.66
1
0.24
1

data (0.45), reﬂecting the high error rate of place of death caused by the overlap in the KB with
places of residence.

In the crowd data we see a much higher co-causality for employee or member and
top employee or member, with only a slight preference in the data for what we expect to be the
“correct” causal direction (that top employee or member causes employee or member), but in
the DS-based analysis, the incorrect interpretation drops a lot. In manual error analysis we observed
that these are properties of the data set, which talk about more famous people who tend to be leaders
and founders, not “regular” employees. Table 6 in the Appendix shows several examples sentences
with false negative DS labels due to missing causality.

All told, the differences (in bold in Tab.1) between the crowd and DS-based causal power accounts
for some of the classiﬁcation errors in our trained system, and we expect them to be a signiﬁcant
cause of error in systems that try to learn cross-relation signals from DS data alone.

non-symmetric

the
f ounded organization,

top employee or member
Among
and
causes
top employee or member causes f ounded org. These again appear to be properties of the
data set.

employee or member

f ounded org,

pairs we

causes

causal

that

see

5 Enhancing Distant Supervision with CrowdTruth

We expect that the metrics from CrowdTruth annotation can be used to systematically enhance DS
data at scale, without requiring the crowd to annotate the entire set. As a preliminary headroom
exercise, we ran three experiments to test a few simple heuristic characterizations of our analysis,
and compared them to a baseline. In each experiment, we changed only the DS training set (using
the methods described below). We used the data in our previously held-out test set as an evaluation
target, again processing the continuous SRS scores with a threshold of 0.5 to yield discrete truth
values for calculating P, R, and F. Results are shown in Tab. 4.

1. DS: The baseline to which the other experiments are compared. The per-relation training

labels are binary based on the results of DS.

2. DS merged: Based on the results of the causality analysis, the training set is augmented
to reﬂect the highest cross-relation signals. We merge relations with symmetric RCP
(origin and place of birth), and add the implied relation in the case of asymmetric RCP
(employee or member and top employee or member). To merge, the DS baseline data
is updated so that the symmetric relations always co-occur, and adding caused relation
whenever the caused relation appears. This approach shows a huge improvement across
the board over the baseline, with the overall highest P and F.

3. DS RCP: Instead of manually identifying merged relations, the training data is augmented
by using the RCP scores. When a relation i has a positive DS label for a given sentence,
the labels of all other relations j (cid:54)= i are updated by adding the macro RCP that i has over
j. The maximum value for the label is clipped at 1, to keep scores in the [0, 1] interval.
The training labels in this set have continuous values, as opposed to the binary values in
the previous two sets. The formula for updating the training label for relation j in sentence
s is: DS RCP (s, i) = max[1, DS(s, j) + (cid:80)
i(cid:54)=j RCP (i, j) · DS(s, i)], where DS(s, i)

4

is the DS label of relation i in sentence s. This method was comparable in precision to the
baseline, but scored a huge win in recall. The recall increase makes sense, though we have
yet to investigate or explain the lack of increase in precision.

4. DS FP: Our analysis showed that the place of death relation was a large source of
false positives in the DS data, because most of the positives were actually express-
ing places of residence.
In every sentence in the DS training set that had a 1 for
place of death, we updated the score by subtracting its false positive ratio, which was
used in the loss function as described above. This did not impact the results over the base-
line, mainly because there were not many place of death relations in the DS data nor the
test set, and any improvement did not impact the overall result. We are conﬁdent that more
systematic treatment of false positive rates will improve performance.

Table 4: Precision & Recall at 20,000 training steps.
Precision Recall F1 score
0.22
0.33
0.48
0.22

DS
DS merged
DS RCP
DS FP

0.2
0.37
0.27
0.21

0.19
0.43
0.19
0.21

6 Discussion

The preliminary results are not overwhelming, but highly indicative. There is considerable headroom
in cross-relation signals, and a more robust approach holds promise to eliminate manual analysis,
and work as part of an overall pipeline that includes partial crowd data. We have shown a very
signiﬁcant variation in the false positive rate in distant supervision data, and it seems extremely
likely that this can be exploited to improve training.

We are currently considering experiments that take advantage of another aspect of our CrowdTruth
method: the identiﬁcation of ambiguity in sentences where workers do not agree on the outcome.
We believe a more continuous truth measure as opposed to the rather arbitrary discrete measure will
be productive. Finally, we are particularly excited about the possibility of using our approach in
conjunction with logical reasoning approaches such as those reported in [6]. In this case, we are
looking at more informed data that reﬂects human understanding and properties of the data set, to
discover candidate relation pairs for investigating rules.

Appendix

Sentence

Table 5: Example sentences with false positive place of death and origin DS labels due to multiple relations
in the KB over P erson - Location term types.

Relation

Crowd
SRS

DS label

After growing up on Cat Island, Tony McKay moved to
New York City at age 17 to study architecture.

The ﬁlm is based very loosely on the lives of Wolfgang
Amadeus Mozart and Antonio Salieri, two composers
who lived in Vienna, Austria.
Marku Ribas is the side more Black music of this group
and was Bob Marley’s friend in the 1970s, Jamaica,
where he lived.

Osama bin Laden had moved from Saudi Arabia to
Sudan during the 1990-91 Gulf War.

place of death

places of residence

place of death

places of residence

places of residence

origin

origin

places of residence

0.004

0.995

0.074

0.865

0

0.87

0.3

0.74

1

1

1

1

1

1

1

1

5

Table 6: Example sentences with false negative employee or member and origin DS labels due to missing
causal connections.

Sentence

Relation

Crowd
SRS

DS label

China on Monday ofﬁcially appointed Donald
Tsang as Hong Kong’s chief executive for a second
term.
More than 3,000 taxi drivers blocked Rome’s
historic centre Wednesday to protest extra licences
given by mayor Walter Veltroni.
Early years Joey Harrington was born and raised in
Portland, Oregon, where he has resided his entire
life.
Nelli Zhiganshina (born March 31, 1987 in
Moscow, Russia) is a Russian ice dancer who
currently represents Germany.

employee or member

top employee or member

employee or member

top employee or member

place of birth

origin

origin

place of birth

0.623

0.753

0.529

0.841

0.645

0.867

0.555

0.791

0

1

0

1

0

1

0

1

References

[1] G. Angeli, J. Tibshirani, J. Wu, and C. Manning. Combining distant and partial supervision for

relation extraction. In EMNLP, pages 1556–1567, 2014.

[2] L. Aroyo and C. Welty. Crowd Truth: Harnessing disagreement in crowdsourcing a relation

extraction gold standard. Web Science 2013. ACM, 2013.

[3] L. Aroyo and C. Welty. The Three Sides of CrowdTruth. Journal of Human Computation,

[4] L. Aroyo and C. Welty. Truth is a lie: Crowd truth and the seven myths of human annotation.

[5] P. W. Cheng. From covariation to causation: A causal power theory. Psychological Review,

1:31–34, 2014.

AI Magazine, 36(1):15–24, 2015.

104(2):367–405, 1997.

[6] T. Demeester, T. Rockt¨aschel, and S. Riedel. Regularizing relation representations by ﬁrst-

order implications. In AKBC@ NAACL-HLT, pages 75–80, 2016.

[7] C. N. dos Santos, B. Xiang, and B. Zhou. Classifying relations by ranking with convolu-
In Proceedings of the 53rd Annual Meeting of the Association for
tional neural networks.
Computational Linguistics and the 7th International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31,
2015, Beijing, China, Volume 1: Long Papers, pages 626–634. The Association for Computer
Linguistics, 2015.

[8] A. Dumitrache, L. Aroyo, and C. Welty. Crowdsourcing ground truth for medical relation ex-
traction. ACM Trans. Interact. Intell. Syst., Special Issue on Human-Centered Machine Learn-
ing (in publication), 2017.

[9] X. Feng, J. Guo, B. Qin, T. Liu, and Y. Liu. Effective deep memory networks for distant su-
pervised relation extraction. In C. Sierra, editor, Proceedings of the Twenty-Sixth International
Joint Conference on Artiﬁcial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25,
2017, pages 4002–4008. ijcai.org, 2017.

[10] X. Jiang, Q. Wang, P. Li, and B. Wang. Relation extraction with multi-instance multi-label

convolutional neural networks. In COLING, 2016.

[11] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant supervision for relation extraction without

labeled data. In Proc. of IJCNLP 2009: Volume 2, pages 1003–1011. ACL, 2009.

[12] T. Nguyen and R. Grishman. Relation extraction: Perspective from convolutional neural net-

works. In Proc. of NAACL-HLT, pages 39–48, 2015.

[13] S. Riedel, L. Yao, A. McCallum, and B. M. Marlin. Relation extraction with matrix factoriza-
tion and universal schemas. In L. Vanderwende, H. D. III, and K. Kirchhoff, editors, Proc. of
NAACL-HLT, pages 74–84. The Association for Computational Linguistics, 2013.

[14] C. Welty, J. Fan, D. Gondek, and A. Schlaikjer. Large scale relation detection. In Proc. of the

NAACL HLT FAM-LbR, pages 24–33, 2010.

6

