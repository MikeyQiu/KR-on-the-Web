Word Semantic Similarity for Morphologically Rich Languages

Kalliopi Zervanou 1, Elias Iosif 2 and Alexandros Potamianos 3
1 SAIL, University of Southern California, CA 90089, USA
2 Athena Research and Innovation Center, Maroussi 15125, Athens, Greece
3 School of ECE, National Technical Univ. of Athens, Zografou 15780, Greece
Abstract

In this work, we investigate the role of morphology on the performance of semantic similarity for morphologically rich languages, such
as German and Greek. The challenge in processing languages with richer morphology than English, lies in reducing estimation error
while addressing the semantic distortion introduced by a stemmer or a lemmatiser. For this purpose, we propose a methodology for
selective stemming, based on a semantic distortion metric. The proposed algorithm is tested on the task of similarity estimation between
words using two types of corpus-based similarity metrics: co-occurrence-based and context-based. The performance on morphologically
rich languages is boosted by stemming with the context-based metric, unlike English, where the best results are obtained by the
co-occurrence-based metric. A key ﬁnding is that the estimation error reduction is different when a word is used as a feature, rather than
when it is used as a target word.

Keywords: distributional semantic models, lexical semantics, morphology, morphologically rich languages

1.

Introduction

Semantic similarity is the building block for numer-
ous applications of natural language processing, such as
grammar induction (Meng and Siu, 2002) and affective
text categorisation (Malandrakis et al., 2011). Recently,
there has been an increased research interest in devis-
ing data-driven approaches for estimating semantic sim-
ilarity between words, phrases, and sentences. However,
data-driven approaches to semantic similarity estimation
mainly focus on the English language (e.g. the SemEval
sentence-level semantic similarity challenges). Little ev-
idence currently exists on how these algorithms port to
other languages, especially languages characterised by
greater syntactic and/or morphological variability than
English.
Distributional semantic models (DSMs) (Baroni and
Lenci, 2010) are based on the distributional hypothe-
sis of meaning (Harris, 1954) in assuming that semantic
similarity between words is a function of the overlap of
their linguistic contexts. DSMs can be categorised into
structured models, which employ syntactic relationships
between words (Grefenstette, 1994; Baroni and Lenci,
2010), and unstructured models, which employ a bag-
of-words model (Iosif and Potamianos, 2010). DSMs
are typically constructed from co-occurrence statistics of
word tuples, extracted either of existing corpora, or cor-
pora speciﬁcally harvested from the web.
In a recent
work, Iosif and Potamianos (2013) proposed general-
purpose, language-agnostic algorithms for estimating se-
mantic similarity, without any linguistic resources other
than a corpus created via web queries. They demonstrate
that the main reason behind the success of the proposed
methods lies in the construction of semantic networks

and semantic neighbourhoods that capture smooth co-
occurrence and context similarity statistics.
In this work, we investigate the performance of these
network DSMs on European languages with richer mor-
phology than English. Our objective is not only to report
the performance of the semantic similarity algorithms on
these languages, but also to investigate the role of mor-
phology on performance. Moreover, we propose a selec-
tive stemming algorithm, so as to reduce the vocabulary
size in our corpus, while the overall semantic distance
between the original and the stemmed version of the text
is controlleded by a semantic distortion metric.
In addition to English, the languages investigated in this
work are German and (Modern) Greek. Although all
three languages are synthetic/fusional languages (Sapir,
1921), German (Bane, 2008) and Greek present a richer
morphology than English, with Greek being the rich-
est of the three in terms of inﬂectional variation (Ko-
liopoulou, 2013).

2. Morphologically Rich Languages
Morphologically rich languages are characterised by
highly productive morphological processes (inﬂection,
agglutination, compounding) which may result in a very
large number of word forms for a given root form
(Sarikaya et al., 2009; Tsarfaty et al., 2013). Moreover,
the lexical information for each word form in a morpho-
logically rich language is augmented with other types
of information, such as those related to its grammatical
role and syntactic function or temporal information and
pronominal clitics.
Using the communication protocol analogy for language,
one may assume that on average all languages have the
same semantic entropy, namely semantic information is

1642

transmitted with approximately the same efﬁciency in all
languages. Thus, a reduction in the number of symbols
available in one layer of the language code (e.g., phonol-
ogy) will be typically compensated in another layer (e.g.,
morphology). For example, English and German are
richer phonemically than Greek, but poorer in terms of
inﬂectional afﬁxes and while syntax in Greek has a rel-
atively ﬂexible constituent ordering, German and En-
glish have very strict syntactic constraints regarding con-
stituent order.
Although the overall efﬁciency of language as a com-
munication protocol may remain intact, morphologically
rich languages pose challenges to lexical semantics and
natural language processing (NLP) algorithms. This is
due to the fact that morphological productivity in those
languages results in an increase of vocabulary size for
NLP applications. From a lexical semantics perspective,
the reduction of symbols at a given lexical or syntactic
layer of the language and the increase in ambiguity that
this reduction entails at the given layer, has an adverse
impact on the efﬁciency of semantic similarity estima-
tions. In particular, the main features used for estimating
the semantic similarity between two words are:

• their direct co-occurrence in text, and

• the overlap of their context

Although these features remain rather unaffected by lex-
ical variation and ambiguity, the accuracy in statistical
similarity estimations is gravely impacted. For example,
when the number of words increases, co-occurring words
are also more diverse and similarity estimations should
rely on sparser examples. In addition to the number of
words, the second feature, the context is also affected
by syntactic variation, since context can be more or less
variable depending on the language. For these reasons,
we need to devise semantic similarity algorithms that re-
duce vocabulary size with minimum increase in ambigu-
ity and respective distortion in the semantics of the cor-
pus.

3. Corpus-Based Similarity Metrics
In this section, we discuss in more detail the methods
used in building corpus-based semantic similarity mod-
els. First, we brieﬂy present the motivation and the deﬁ-
nition of co-occurence and context-based similarity met-
rics and, subsequently, we present the rationale and the
methodology in building network-based DSMs and how
these have been implemented in this work.

3.1. Co-occurrence-based metrics
The underlying assumption of co-occurrence-based met-
rics is that the co-appearance of words in a speciﬁc con-
In
textual environment indicates semantic relatedness.

this work, we employ a widely used co-occurrence-based
metric, the Dice coefﬁcient metric. The Dice coefﬁcient
between words wi and wj is deﬁned as follows:

D(wi, wj) = 2f (wi,wj )

f (wi)+f (wj ) ,

(1)

where f (.) denotes the frequency of word occurrence/co-
occurrence. Here, the word co-occurrence is considered
at the sentential level, while D can be also deﬁned with
respect to broader contextual environments, such as the
paragraph level (V´eronis, 2004).

3.2. Context-based metrics
The fundamental assumption behind context-based met-
rics is that similarity of context implies similarity of
meaning (Harris, 1954). In this work, a contextual win-
dow of size 2H + 1 words is built for the word of interest
wi and lexical features are extracted. For every instance
of wi in the corpus, H words left and right of wi compose
a feature vector vi. For a given value of H, the context-
based semantic similarity between two words, wi and wj,
is computed as the cosine of their feature vectors:

QH (wi, wj) = vi.vj

||vi|| ||vj || .

(2)

The elements of feature vectors can be weighted accord-
ing to various schemes (Iosif and Potamianos, 2010). In
the work presented in this paper, we use a binary scheme.

3.3. Network-based DSMs
In this section, we summarise the main ideas of network-
based DSMs, as proposed in Iosif and Potamianos
(2013). The rationale behind the implementation of
network-based DSMs has been twofold. First, the net-
work is a parsimonious representation of the corpus
statistics pertaining to semantic similarity estimation of
word-pairs. Second and most importantly, the network
representation allows for discovering relations that are
not directly observable in the data; such relations emerge
via the systematic covariation of similarity metrics.
The proposed network is deﬁned, under a symmetric
similarity metric, as an undirected graph F = (V, E),
where the set of vertices V consists of all words in our
vocabulary set L, and the set of edges E consists of all
the links between these vertices. The edges between
words in the network are determined and weighted ac-
cording to the pairwise semantic similarity of the ver-
tices.
In building this network, for each reference word wi that
is included in the vocabulary set L, wi ∈ L, we consider
a sub-graph of F , Fi = (Ni, Ei), where the set of ver-
tices Ni includes n members of L in total, which, in turn,
are linked to wi via an Ei set of edges. This Fi sub-graph
is referred to as the semantic neighbourhood of wi. The
members of Ni (i.e. the neighbours of wi) are selected

1643

according to a semantic similarity metric with respect to
wi, either co-occurrence-based D, or context-based QH ,
as deﬁned in Eq. (1) and Eq. (2), above. Thus, the n most
similar words to wi are selected.
The notion of semantic neighbourhood (Iosif and
Potamianos, 2013) is utilised in this work in two seman-
tic similarity metrics, the estimation of maximum simi-
larity of neighbourhoods and the sum of squared neigh-
bourhood similarities metric. In the following subsec-
tions, we present these two semantic neighbourhood met-
rics in more detail.

3.3.1. Maximum Similarity of Neighbourhoods
This metric is based on the hypothesis that the similarity
of two words, wi and wj, can be estimated by the maxi-
mum similarity of their respective sets of neighbours, and
is deﬁned as follows:

Mn(wi, wj) = max{αij, αji}

(3)

where:

αij = max
x ∈ Nj

S(wi, x), αji = max
y ∈ Ni

S(wj, y)

αij (or αji) denotes the maximum similarity between wi
(or wj) and the neighbours of wj (or wi). This similarity
is computed according to a similarity metric S, in this
work either D, or QH . Ni and Nj denote the sets of
neighbours for wi and wj, respectively.
The deﬁnition of maximum similarity Mn is motivated
by the maximum sense similarity assumption (Resnik,
1995). In our work, the underlying assumption is that
the most salient information in the neighbours of a word
constitute semantic features denoting the senses of this
word.

3.3.2. Sum of Squared Neighbourhood Similarities
While in maximum similarity Mn metric we consider
only the most salient information in the neighbours of
a word, i.e. only the most similar neighbours in the Nj
set to wi and the most similar neighbours in the Ni set
to wj, in the sum of squared neighbourhood similarities
metric, we exploit all information available in the Ni and
Nj neighbourhood sets of wi and wj for the estimation
of their semantic similarity.
In particular, the sum of squared neighbourhood similar-
ities estimation, for a given pair of words wi and wj, is
deﬁned as follows:

Eθ

n(wi, wj) =

S(wi, x)θ +

S(wj, y)θ

(cid:32)

(cid:88)

x ∈ Nj

(cid:88)

y ∈ Ni

(cid:33) 1

θ

above. The contribution of all neighbours to the ﬁnal
similarity score Eθ
n(wi, wj) is performed by summing
the squares (θ = 2) of similarities between wi and the
neighbours of wj. The same calculation is repeated for
wj and the neighbours of wi, so as to make Eθ
n(wi, wj)
symmetric. The Eθ=2
n metric is unbounded, because the
yielding similarity scores range within [0, ∞]. The con-
tribution of each word–to–neighbour similarity is non-
linearly weighted, using the square of the respective sim-
ilarity score. The motivation behind using θ > 1 is
that more similar words in the neighbourhoods should
be weighted more in the ﬁnal similarity decision1. Note
that as θ goes to ∞, Eθ

n and Mn become equivalent.

4. DSMs for Morphologically Rich

Languages

Typically, NLP and information retrieval applications ad-
dress the issue of morphological word form variation by
applying text normalisation techniques, such as stem-
Information retrieval research,
ming or lemmatisation.
for example, has since its very early beginnings intro-
duced stemming as a preprocessing step in building doc-
ument and query representations. Currently, both stem-
ming and lemmatisation constitute a text preprocessing
step in many NLP applications, so as to reduce the total
number of surface word forms, both in cases where word
forms are used as targets, such as in statistical language
modelling, as well as in cases where word forms are used
as features, such as in semantic similarity estimation us-
ing context-based metrics. The main challenge in de-
veloping DSMs for morphologically rich languages lies
in the large vocabulary size which affects the efﬁciency
of semantic similarity estimations, since these typically
model each word form as a separate word.
A reduction of vocabulary size by means of stemming
may succeed in improving statistical estimations of word
occurrence or co-occurrence. However, reducing words
to a stemmed or (less-so) lemmatised form also intro-
duces a shift in semantics, where for example people and
peoples have quite different meaning. We therefore ob-
serve two opposing forces in action, one where stemming
has a positive effect in reducing estimation error by re-
ducing the number of word forms and respective sparsity,
and one where stemming introduces semantic distortion
by grouping morphologically similar words into poten-
tially semantically disparate groups.
In this section, we discuss our proposal for explicitly op-
timising the decision on which words to stem using a se-
mantic distortion metric. Suppose that we have a set L
of words, where wi ∈ L and a set K of stemming rules,

(4)
where Ni is the set of neighbours of word wi, and S, in
this work, is the Dice similarity metric, deﬁned in Eq. (1),

1Despite the resemblance between the Eθ=2

n metric and the
Euclidean distance, no assumption is adopted here about the
semantic neighbourhoods being metric spaces under S.

1644

where rk ∈ K. Then the stemmed version of a word wi
would be deﬁned as lk(wi), where lk(.) is the stem of wi
after rule rk is applied.
We also have a corpus-based semantic similarity metric
S(wi, wj) between two words – or word stems, wi and
wj, normalised in [0, 1]. For each rule rk, the average
semantic distortion C is computed as the average dis-
tance between the original word wi and its stemmed form
lk(wi). Speciﬁcally:

C(rk) ∝

(cid:80)N

i=1 f (wi)[1 − S(wi, lk(wi))]
i=1 f (wi)[1 − δ(wi, lk(wi))]

(cid:80)N

(5)

where f (wi) assigns a weight based on the frequency of
the word form wi in the corpus, and δ(., .) is Kronecher’s
delta, a value that is set to 1, when wi ≡ lk(wi), and 0,
otherwise. Estimating the semantic similarity between a
word wi and its stemmed form lk(wi) is not straightfor-
ward, because the stem lk(wi) typically does not appear
Instead, we deﬁne S(wi, lk(wi)) as the
in the corpus.
average similarity between wi and the subset W of all
words in the vocabulary L sharing the same stem after
rk is applied to wi.
The semantic distortion C(rk) is computed for each
stemming rule rk. Then, the rules are ranked according
to their respective C(rk), in increasing order.

5. Experimental Procedure and Results

5.1. Baseline Experiments
For our baseline experiments, we have used a vocabulary
set of 10, 000 noun word forms in all three languages.
For each word form in this set, an individual query was
formulated and the 1000 top ranked results, i.e. docu-
ment snippets were retrieved using the Yahoo! search
engine2. The respective corpus for each language is cre-
ated by aggregating the web snippets for all nouns in our
vocabulary set.
Subsequently, we estimated semantic similarity, with
and without stemming, using Dice coefﬁcient for co-
occurrence similarity (D), cosine similarity with window
size 1 (QH=1) for context-based similarity, and the two
variants of network-based metrics, i.e. Mn and Eθ=2
.
For stemming, we have used for English and German,
the respective Snowball stemmers3. For Greek there was
no general purpose stemmer available, thus no results for
Greek are reported in this section.
The performance of these similarity metrics and the ef-
fect of stemming were evaluated for the task of seman-
tic similarity between nouns. Pearson’s correlation co-
efﬁcient was used as evaluation metric to compare es-
timated similarities against human ratings. For English

n

2http://www.yahoo.com//
3snowball.tartarus.org

and German, we used the evaluation datasets presented
in Rubenstein and Goodenough (1965) and Gurevych
(2005), respectively. Of these evaluation datasets, we in-
cluded only those word pairs that were covered by our
networks: 57 out of 65 pairs for English, and 63 out of
65 pairs for German.

Language

English

German

Similarity Corpus processing
None
0.60
0.55
0.87
0.86
0.24
0.41
0.65
0.67

metric
D
QH=1
Mn
Eθ=2
n
D
QH=1
Mn
Eθ=2
n

Stem.
0.52
0.02
0.61
0.57
0.27
0.21
0.68
0.65

Table 1: Pearson’s correlation to human similarity judge-
ments for unstemmed and stemmed data, and a vocabu-
lary of 10K words, nouns only.

The results in semantic similarity estimation are shown
on Table 1. For the network-based metrics, Mn and
Eθ=2
, the performance is reported for neighbourhood
n
size n = 100. For neighbour selection both methods
used Dice coefﬁcient D, while for similarity estimation,
we have used cosine similarity QH=1 for Mn and Dice
coefﬁcient for Eθ=2
. The effect of the neighbourhood
size n and the combinations of D and QH=1 in neigh-
bour selection and similarity computation are discussed
in detail in Iosif and Potamianos (2013).

n

For the co-occurrence-based metric D, stemming ap-
pears to slightly improve the performance for German.
It is also quite notable that when stemming is used for
English the performance is consistently worse in all four
metrics, with the context-based metric QH=1 presenting
the worst results. The performance of the context-based
metric QH=1 for German deteriorates by stemming, al-
beit less than in English.
Both network-based metrics, Mn and Eθ=2
perform bet-
ter than D and QH=1. However, use of stemming de-
grades the performance for English in both Mn and
Eθ=2
the use of stemming with Mn
n
improves performance, whereas stemming with Eθ=2
slightly decreases performance. Although not reported
the effect of stemming is rather beneﬁcial for
here,
Greek, as discussed in the next section.

. For German,

n

n

Overall, the stemming effect seems to be a function of the
used similarity metric and the morphological richness of
the language under investigation.

1645

5.2. Stemming Rules Ranking Experiments
In order to acquire a better picture of the effects of stem-
ming and the resulting semantic distortion, we attempted
to cover the largest possible set of words in our vocabu-
lary and of all grammatical parts of speech, rather than
nouns only. For this purpose, we compiled our vocabu-
laries for each of the languages under investigation, us-
ing the intersection of the vocabulary of the entire set of
Wikipedia pages and the aspell dictionaries. This process
resulted in a vocabulary of 135,436 word forms for En-
glish, 332,456 word forms for German and 407,752 word
forms for Greek. Our corpus is acquired using the same
methodology as in the baseline experiments, namely by
aggregating the 1000 top ranked web snippets retrieved
for each word form in our vocabulary set. For stem-
ming, we have used for English and German, the Snow-
ball stemmers, whereas for Greek, we have developed a
general purpose stemmer4.
Similarity metrics are used in two stages:

(i) First, in order to compute the individual rule se-
mantic distortion, we need to estimate the similarity
between a word and its stem, as deﬁned in Eq.(5).
For estimating the similarity between the surface
word form wi and its stem lk(wi), S(wi, lk(wi)),
we use Eθ=2
, because, as also shown on Table 1,
network-based metrics achieve better performance.

n

(ii) Second, in the ﬁnal similarity metric estimation
between a pair of words, as deﬁned in Eq.(1-4),
co-occurrence-based metric Dice coefﬁcient D and
context-based QH=1 similarity metrics are used.

For the context-based metric QH=1, we have experi-
mented with three stemming scenarios:

(i) using stemming only for the target words (i.e. the

pairs)

(ii) using stemming only for the context words

(iii) stemming both the target and the context words.

Once the semantic distortion is estimated, the stemming
rules are ranked by increasing distortion value. Subse-
quently, a percentage of stemming rules is selected by
thresholding the semantic distortion criterion C.
Results are reported for co-occurrence D and context-
based QH=1 similarity metrics. The evaluation datasets
used in these experiments were the same as those used
in our baseline experiments. In addition, due to lack of
evaluation datasets for the Greek language, a new dataset
of 200 pairs was compiled by experts and results are re-
ported also on these datasets. The semantic similarity

4This stemmer is an extended and modiﬁed version of the

Greek Stemmer developed by Ntais (2006).

1646

Figure 1: Performance of co-occurrence metric D as a
function of the percentage of stemming rules applied.

performance reported is Pearson’s correlation with hu-
man similarity judgement.
The obtained correlation for the co-occurrence-based
metric is depicted in Fig. 1, as a function of the per-
centage of the applied stemming rules. This is shown
for all three languages. We observe that the achieved
correlation is signiﬁcantly higher for English, compared
to Greek and German. Moreover, it is shown that at
60% of the rules, the performance is slightly improved
for both English and Greek, unlike German, where stem-
ming does not seem to affect an overall low performance.
Fig. 2 illustrates the obtained correlation, as a function
of the percentage of the applied stemming rules, for the
context-based metric QH=1. Fig. 2(a) refers to the appli-
cation of stemming to the target words only, Fig. 2(b), to
context words only, and Fig. 2(c), to both the target and
the context words. We observe that Greek performs over-
all better in all three approaches, while German achieves
the lowest performance.
The best results are achieved by the application of stem-
ming to the target words only (cf. Fig. 2(a)), where a
slight gain in performance is achieved for Greek, when
30% of the top stemming rules is applied. Rule selection
seems to play a more signiﬁcant role in the stemming
of contextual features approach, as this is illustrated in
Fig. 2(b), where the performance steeply degrades, espe-
cially for English, above the 40% of stemming rules.

6. Conclusions

In this work, we have investigated the role of morphology
in the performance of unstructured DSMs. For this pur-
pose, in addition to English, we have experimented with
two European languages, German and Greek, which are

(a)

(b)

Figure 2: Performance of context-based metric QH=1 as a function of the percentage of stemming rules applied. Using
stemming: (a) only for the target words, (b) only for context words, (c) for both the target and the context words.

characterised by richer morphology than English. We
proposed a methodology for selective stemming, based
on a semantic distortion metric which counterbalances
the similarity estimation error reduction, achieved by text
normalisation techniques, such as stemming, with the se-
mantic distortion that such a process entails.

We observed that selective stemming application with the
co-occurrence-based metric is beneﬁcial for languages
characterised by a poorer morphology, such as English,
while stemming in combination with a context-based
metric deteriorates performance. This phenomenon is
reversed in a morphologically richer language, such as
Greek, where poor performance with the co-occurrence-
based metric is greatly improved with the context-based
metric. This may be attributed to the number of features
considered for similarity estimation. In particular, in the
co-occurrence-based metric, the estimation of similarity

relies solely on the statistics of the co-occurring words.
Conversely, in the context-based metric, the number of
lexical features considered is much larger. Thus, selec-
tive stemming for a morphologically rich language re-
duces the vocabulary size and improves the similarity es-
timation. The performance of German is low overall and
stemming does not seem to affect much the performance.
A possible explanation for this might lie in morpholog-
ical phenomena, such as compounding, which also in-
crease vocabulary size, but cannot be addressed by a sim-
ple text normalisation technique, such as stemming. The
formation of ad-hoc single word compounds is a particu-
lar characteristic of German that is potentially responsi-
ble for the low performance in similarity estimations.

The key observation is that the estimation error reduc-
tion is different, when a word is used as a feature, where
errors are averaged typically among many different fea-

(c)

1647

Malandrakis, N., Potamianos, A.,

and
Narayanan, S.
(2011). Kernel models for affective
lexicon creation. In Proc. Interspeech, pages 2977–
2980.

Iosif, E.,

Meng, H. and Siu, K.-C.

(2002). Semi-automatic
acquisition of semantic structures for understand-
ing domain-speciﬁc natural language queries. IEEE
Transactions on Knowledge and Data Engineering,
14(1):172–181.

Ntais, G. (2006). Development of a stemmer for the
Greek language. Master’s thesis, Stockholm Univer-
sity – Royal Institute of Technology.

Resnik, P. (1995). Using information content to evaluate
semantic similarity in a taxonomy. In Proc. of Inter-
national Joint Conference for Artiﬁcial Intelligence,
pages 448–453.

Rubenstein, H. and Goodenough, J. B. (1965). Contex-
tual correlates of synonymy. Communications of the
ACM, 8(10):627–633.

Sapir, E.

(1921). Language: An Introduction to the
Study of Speech. New York: Harcourt, Brace, 1921;
Bartleby.com, 2000.

(2009).

Sarikaya, R., Kirchhoff, K., Schultz, T., and Hakkani-
Tur, D.
Introduction to the special issue
on processing morphologically rich languages. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 17(5):861–862.

Tsarfaty, R., Seddah, D., Kuebler, S., and Nivre, J.
(2013). Parsing Morphologically Rich Languages: In-
troduction to the Special Issue. Computational Lin-
guistics, 39(1):15–22.

V´eronis, J.

(2004). Hyperlex: Lexical cartography
for information retrieval. Computer Speech and Lan-
guage, 18(3):223–252.

tures, than when it is used as a target word. For exam-
ple, in the context-based semantic similarity estimation it
makes sense to perform stemming for the words proper
that we are estimating the semantic similarity for, but less
so for the context feature vector, where we have poten-
tially enough statistics, especially if a large window is
used.
As future work, we plan to investigate more reﬁned stem-
ming rule selection and semantic similarity performance
for the network-based metrics when using stemming.
Additionally, we would like to investigate in more detail
the effect of stemming in relation to the particularities of
derivational vs. inﬂectional morphology, and the role of
preﬁxes in semantics. Finally, we are interested in inves-
tigating other languages, such as Romance languages, or
languages characterised by agglutinative morphological
phenomena, such as Turkish.

7. Acknowledgements
Elias Iosif and Alexandros Potamianos were partially
funded by the EU-IST FP7 PortDial project (“Language
Resources for Portable Multilingual Spoken Dialog Sys-
tems”), grant number 296170. The authors wish to
thank Maria Giannoudaki for the development of the
Greek evaluation dataset and preliminary experiments,
and Ioannis Klasinas for corpora creation.

8. References
Bane, M. (2008). Quantifying and measuring morpho-
logical complexity. In Proc. of the 26th West Coast
Conference on Formal Linguistics, pages 67–76.

Baroni, M. and Lenci, A. (2010). Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673–721.

Grefenstette, G. (1994). Explorations in Automatic The-

saurus Discovery. Kluwer Academic Publishers.

Gurevych, I. (2005). Using the structure of a conceptual
network in computing semantic relatedness. In Proc.
of the 2nd International Joint Conference on Natural
Language Processing.

Harris, Z.

(1954). Distributional structure. Word,

10(23):146–162.

Iosif, E. and Potamianos, A. (2010). Unsupervised se-
mantic similarity computation between terms using
IEEE Transactions on Knowledge
web documents.
and Data Engineering, 22(11):1637–1647.

Iosif, E. and Potamianos, A. (2013). Similarity com-
putation using semantic networks created from web-
harvested data. Natural Language Engineering (DOI:
10.1017/S1351324913000144).

Koliopoulou, M. (2013). Issues of Modern Greek and
German compounding: a contrastive approach. Ph.D.
thesis, University of Patras.

1648

