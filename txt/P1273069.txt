The Sample-Complexity of General Reinforcement Learning

Tor Lattimore and Marcus Hutter and Peter Sunehag

Research School of Computer Science
Australian National University

tor.lattimore,marcus.hutter,peter.sunehag

{

@anu.edu.au
}

July 2013

Abstract

Contents

We present a new algorithm for general reinforcement learning where the true environment
is known to belong to a ﬁnite class of N arbitrary models. The algorithm is shown to be
near-optimal for all but O(N log2 N ) time-steps with high probability. Inﬁnite classes are also
considered where we show that compactness is a key criterion for determining the existence of
uniform sample-complexity bounds. A matching lower bound is given for the ﬁnite case.

1 Introduction
2 Notation
3 Finite Case
4 Compact Case
5 Unbounded Environment Classes
6 Lower Bound
7 Conclusions
References
A Technical Results
B Constants
C Table of Notation

Reinforcement learning; sample-complexity; exploration exploitation.

Keywords

2
3
4
12
12
13
13
14
15
16
16

3
1
0
2
 
g
u
A
 
2
2
 
 
]

G
L
.
s
c
[
 
 
1
v
8
2
8
4
.
8
0
3
1
:
v
i
X
r
a

1

1

Introduction

Reinforcement Learning (RL) is the task of learning policies that lead to nearly-optimal rewards
where the environment is unknown. One metric of the eﬃciency of an RL algorithm is sample-
complexity, which is a high probability upper bound on the number of time-steps when that
algorithm is not nearly-optimal that holds for all environment in some class. Such bounds are
typically shown for very speciﬁc classes of environments, such as (partially observable/factored)
Markov Decision Processes (MDP) and bandits. We consider more general classes of environments
[0, 1]
where at each time-step an agent takes an action a
and an observation o
O, which are generated stochastically by the environment and may depend
arbitrarily on the entire history sequence.

A where-upon it receives reward r

∈

∈

∈

a1

a2

a3

a4

a5

a6

a7

agent

environment

r1

o1

r2

o2

r3

o3

r4

o4

r5

o5

r6

o6

r7

o7

Figure 1: Agent/Environment Interaction

We present a new reinforcement learning algorithm, named Maximum Exploration Reinforce-
ment Learning (MERL), that accepts as input a ﬁnite set
of arbitrary environ-
ments, an accuracy ǫ, and a conﬁdence δ. The main result is that MERL has a sample-complexity
of

ν1,
{

, νN

M

· · ·

:=

}

˜O

N

−

ǫ2(1

(cid:18)

γ)3 log2

N

,

δǫ(1

γ)

−

(cid:19)

M

γ) is the eﬀective horizon determined by discount rate γ. We also consider the case
where 1/(1
−
where
is inﬁnite, but compact with respect to a particular topology. In this case, a variant
of MERL has the same sample-complexity as above, but where N is replaced by the size of the
smallest ǫ-cover. A lower bound is also given that matches the upper bound except for logarithmic
factors. Finally, if

is non-compact then in general no ﬁnite sample-complexity bound exists.

M

Related work. Many authors have worked on the sample-complexity of RL in various settings.
The simplest case is the multiarmed bandit problem that has been extensively studied with vary-
ing assumptions. The typical measure of eﬃciency in the bandit literature is regret, but sample-
complexity bounds are also known and sometimes used. The next step from bandits is ﬁnite state
MDPs, of which bandits are an example with only a single state. There are two main settings when
MDPs are considered, the discounted case where sample-complexity bounds are proven and the
undiscounted (average reward) case where regret bounds are more typical. In the discounted set-
ting the upper and lower bounds on sample-complexity are now extremely reﬁned. See Strehl et al.
[2009] for a detailed review of the popular algorithms and theorems. More recent work on closing
the gap between upper and lower bounds is by Szita and Szepesv´ari [2010], Lattimore and Hutter
[2012], Azar et al. [2012]. In the undiscounted case it is necessary to make some form of ergodicity
assumption as without this regret bounds cannot be given. In this work we avoid ergodicity assump-
tions and discount future rewards. Nevertheless, our algorithm borrows some tricks used by UCRL2
Auer et al. [2010]. Previous work for more general environment classes is somewhat limited. For
factored MDPs there are known bounds, see Chakraborty and Stone [2011] and references there-in.
Even-dar et al. [2005] give essentially unimprovable exponential bounds on the sample-complexity
of learning in ﬁnite partially observable MDPs. Odalric-Ambrym et al. [2013] show regret bounds

2

for undiscounted RL where the true environment is assumed to be ﬁnite, Markov and commu-
nicating, but where the state is not directly observable. As far as we know there has been no
work on the sample-complexity of RL when environments are completely general, but asymptotic
results have garnered some attention with positive results by Hutter [2002], Ryabko and Hutter
[2008], Sunehag and Hutter [2012] and (mostly) negative ones by Lattimore and Hutter [2011b].
Perhaps the closest related worked is Diuk et al. [2009], which deals with a similar problem in the
rather diﬀerent setting of learning the optimal predictor from a class of N experts. They obtain an
O(N log N ) bound, which is applied to the problem of structure learning for discounted ﬁnite-state
factored MDPs. Our work generalises this approach to the non-Markov case and compact model
classes.

2 Notation

∈

x
⌉

⌈

The deﬁnition of environments is borrowed from the work of Hutter [2005], although the notation
is slightly more formal to ease the application of martingale inequalities.
General. N =
is the natural numbers. For the indicator function we write [[x = y]] = 1
0, 1, 2,
{
if x = y and 0 otherwise. We use
is its
size and A∗ is the set of all ﬁnite strings (sequences) over A. If x and y are sequences then x ⊏ y
means that x is a preﬁx of y. Unless otherwise mentioned, log represents the natural logarithm.
For random variable X we write EX for its expectation. For x

for logical and/or respectively. If A is a set then

is the ceiling function.

· · · }

A
|

and

R,

∧

∨

|

∗

∗

}

}

F

O

O

×

×

×

H

H

If h

∈ H

⊂
H

∈ H

∈ H

:= A

) and

Γh : h

∗ := (A

:= σ(
{

∞ : h ⊏ h′

∗, Γh :=
∗

h′
{
∈ H
ℓ(h) = t
∧

×
R)∗ is the set of ﬁnite history sequences.

R be ﬁnite sets of actions, observations
Environments and policies. Let A, O and R
∞ is the set of inﬁnite history sequences
and rewards respectively and
R.
while
∗ then ℓ(h) is the
number of action/observation/reward tuples in h. We write at(h), ot(h), rt(h) for the tth ac-
tion/observation/reward of history sequence h. For h
is the
∈ H
}
cylinder set. Let
) be σ-algebras.
Γh : h
t := σ(
{
F
An environment µ is a set of conditional probability distributions over observation/reward pairs
A. An environment and policy
given the history so far. A policy π is a function π :
interact sequentially to induce a measure, Pµ,π, on ﬁltered probability space (
). For
∞,
convenience, we abuse notation and write Pµ,π(h) := Pµ,π(Γh). If h ⊏ h′ then conditional prob-
t+d
trk(h) is the d-step return
k=t γk
abilities are Pµ,π(h′
Rt(h; d). Given history ht with ℓ(ht) = t, the value function is
function and Rt(h) := limd
deﬁned by V π
ht).
ht] where the expectation is taken with respect to Pµ,π(
·|
V π
µ , which
µ (ht) := limd
with our assumptions is known to exist Lattimore and Hutter [2011a]. The value of the optimal
π∗
µ
µ . In general, µ denotes the true environment while ν is a model. π will typically
policy is V ∗µ := V
be the policy of the algorithm under consideration. Q∗µ(h, a) is the value in history h of following
policy π∗µ except for the ﬁrst time-step when action a is taken.
is a set of environments (models).
Sample-complexity. Policy π is ǫ-optimal in history h and environment µ if V ∗µ (h)
The sample-complexity of a policy π in environment class
probability, π is ǫ-optimal for all but Λ time-steps for all µ
be the number of time-steps when π is not ǫ-optimal.

ǫ.
is the smallest Λ such that, with high
to

µ (ht; d). The optimal policy for environment µ is π∗µ := arg maxπ V π
V π

h) := Pµ,π(h′)/Pµ,π(h). Rt(h; d) :=

µ (ht; d) := E[Rt(h; d)
|

. Deﬁne Lǫ

V π
µ (h)

∪ {∞}

∈ M

µ,π :

{F

M

M

→∞

→∞

P

→

→

H

H

H

−

≤

N

F

∞

}

−

|

,

t

Lǫ

µ,π(h) :=

V ∗µ (ht)

V π
µ (ht) > ǫ

,

−

∞

t=1
X

(cid:2)(cid:2)

(cid:3)(cid:3)

where ht is the length t preﬁx of h. The sample-complexity of policy π is Λ with respect to accuracy
µ
ǫ and conﬁdence 1

δ if P

< δ,

Lǫ

.

µ,π(h) > Λ

−

∀

∈ M

(cid:8)

(cid:9)

3

3 Finite Case

t

M

M

⊆ M

We start with the ﬁnite case where the true environment is known to belong to a ﬁnite set of
models,
. The Maximum Exploration Reinforcement Learning algorithm is model-based in the
, where models are eliminated once they become implausible.
sense that it maintains a set,
The algorithm operates in phases of exploration and exploitation, choosing to exploit if it knows all
plausible environments are reasonably close under all optimal policies and explore otherwise. This
method of exploration essentially guarantees that MERL is nearly optimal whenever it is exploiting
and the number of exploration phases is limited with high probability. The main diﬃculty is
specifying what it means to be plausible. Previous authors working on ﬁnite environments, such as
MDPs or bandits, have removed models for which the transition probabilities are not suﬃciently
close to their empirical estimates. In the more general setting this approach fails because states
(histories) are never visited more than once, so suﬃcient empirical estimates cannot be collected.
Instead, we eliminate environments if the reward we actually collect over time is not suﬃciently
close to the reward we expected given that environment.

Before giving the explicit algorithm, we explain the operation of MERL more formally in two
parts. First we describe how it chooses to explore and exploit and then how the model class is
maintained. See Figure 2 for a diagram of how exploration and exploitation occurs.

Exploring and exploiting. At each time-step t MERL computes the pair of environments ν, ν
in the model class

t and the policy π maximising the diﬀerence

M

∆ := V π

ν (h; d)

V π
ν (h; d),

d :=

−

log

.

1

γ

(1

γ)ǫ

1

−

8

−

If ∆ > ǫ/4, then MERL follows policy π for d time-steps, which we call an exploration phase.
Otherwise, for one time-step it follows the optimal policy with respect to the ﬁrst environment
currently in the model class. Therefore, if MERL chooses to exploit, then all policies and environ-
ments in the model class lead to similar values, which implies that exploiting is near-optimal. If
MERL explores, then either V π
V π
ν (h; d) > ǫ/8, which will
allow us to apply concentration inequalities to eventually eliminate either ν (the upper bound) or
ν (the lower bound).

µ (h; d) > ǫ/8 or V π
V π

ν (h; d)

µ (h; d)

−

−

The model class. An exploration phase is a κ-exploration phase if ∆

[ǫ2κ

−

2, ǫ2κ

−

1), where

∈

κ

∈ K

:=

0, 1, 2,

, log2

· · ·

ǫ(1

γ)

1

−

+ 2

.

(cid:27)

(cid:26)
and each κ

, MERL associates a counter E(ν, κ), which is
For each environment ν
. At the end of each κ-exploration
incremented at the start of a κ-exploration phase if ν
}
phase MERL calculates the discounted return actually received during that exploration phase
R

γ)] and records the values

[0, 1/(1

∈ M

∈ K

ν, ν

∈ {

∈

−

X(ν, κ) := (1
X(ν, κ) := (1

γ)(V π
γ)(R

ν (h; d)
R)
−
V π
ν (h; d)),

−

−
−

where h is the history at the start of the exploration phase. So X(ν, κ) is the diﬀerence between
the return expected if the true model was ν and the actual return and X(ν, κ) is the diﬀerence
between the actual return and the expected return if the true model was ν. Since the expected
value of R is V π
µ (h; d), and ν,ν are upper and lower bounds respectively, the expected values of
both X(ν, κ) and X(ν, κ) are non-negative and at least one of them has expectation larger than
(1

−
MERL eliminates environment ν from the model class if the cumulative sum of X(ν, κ) over all
is suﬃciently large, but it tests this condition only when the
αj
(1, 2) as deﬁned in

exploration phases where ν
ν, ν
counts E(ν, κ) has increased enough since the last test. Let αj :=

γ)ǫ/8.

for α

∈ {

}

∈

(cid:6)

(cid:7)

4

∈

the algorithm. MERL only tests if ν should be removed from the model class when E(ν, κ) = αj
N. This restriction ensures that tests are not performed too often, which allows us
for some j
,
to apply the union bound without losing too much. Note that if the true environment µ
}
then Eµ,πX(µ, κ) = 0, which will ultimately be enough to ensure that µ remains in the model class
with high probability. The reason for using κ to bucket exploration phases will become apparent
later in the proof of Lemma 3.

ν, ν

∈ {

(1

γ log

Algorithm 1 MERL
1: Inputs: ǫ, δ and M := {ν1, ν2, · · · , νN }.
2: t = 1 and h empty history
8
3: d := 1
γ)ǫ , δ1 :=
1
−
4: α := 4√N
4√N
1
−
5: E(ν, κ) := 0,
6: loop
7:
8:
9:

32
αj
∀ν ∈ M and κ ∈ N
(cid:6)

Π := {π∗ν : ν ∈ M}
ν, ν, π := arg max

V π
ν (h; d) − V π

and αj :=

repeat

δ
N 3/2

|K|

(cid:7)

−

ν,ν
,π
if ∆ := V π
ν (h; d) − V π

∈M

Π

∈

ν (h; d) > ǫ/4 then

ν (h; d)

˜h = h and R = 0
for j = 0 → d do

R = R + γj rt(h)
Act(π)

κ ∈ N : ∆ > ǫ2κ

end for
κ := min
E(ν, κ) = E(ν, κ) + 1 and E(ν, κ) = E(ν, κ) + 1
X(ν, κ)E(ν,κ) = (1 − γ)(V π
X(ν, κ)E(ν,κ) = (1 − γ)(R − V π

ν (˜h; d) − R)
ν (˜h; d))

(cid:8)

(cid:9)

−

.

2

else

end if

i := min {i : νi ∈ M} and Act(π∗νi )

until ∃ν ∈ M, κ, j ∈ N such that E(ν, κ) = αj and

10:

11:
12:
13:
14:
15:
16:
17:
18:

19:
20:
21:
22:
23:

E(ν,κ)

Xi=1

X(ν, κ)i ≥

2E(ν, κ) log

r

E(ν, κ)
δ1

.

24: M = M − {ν}
25: end loop
26: function Act(π)
27:
28:
29: end function

Take action at = π(h) and receive reward and observation rt, ot from environment
t ← t + 1 and h ← hatotrt

Subscripts. For clarity, we have omitted subscripts in the pseudo-code above. In the analysis we
will refer to Et(ν, κ) and
respectively at time-step t. We write
νt for νi in line 21 and similarly πt := π∗νt .
Phases. An exploration phase is a period of exactly d time-steps, starting at time-step t if

t for the values of E(ν, κ) and

M

M

1. t is not currently in an exploration phase.

2. ∆ := V π

ν (ht; d)

V π
ν (ht; d) > ǫ/4.

−

We say it is a ν-exploration phase if ν = ν or ν = ν and a κ-exploration phase if ∆
[ǫ2κ

∈
2. It is a (ν, κ)-exploration phase if it satisﬁes both

[ǫκ, 2ǫκ) where ǫκ := ǫ2κ

2, ǫ2κ

1)

−

−

−

≡

5

of the previous statements. We say that MERL is exploiting at time-step t if t is not in an explo-
ration phase. A failure phase is also a period of d time-steps and starts in time-step t if

1. t is not in an exploration phase or earlier failure phase

2. V ∗µ (ht)

V π
µ (ht) > ǫ.

−

Unlike exploration phases, the algorithm does not depend on the failure phases, which are only
used in the analysis, An exploration or failure phase starting at time-step t is proper if µ
t.
The eﬀective horizon d is chosen to ensure that V π
ǫ/8 for all π, µ and h.

∈ M

µ (h; d)

V π
µ (h)

≥

−

V ∗µ (h)

V π
µ (h) > ǫ

−

V π
ν (h; d)

V π
ν (h; d) = 4ǫ

−

V π
ν (h; d)

V π
ν (h; d) = ǫ

−

explore, κ = 4

explore, κ = 2

t

exploiting

exploiting

failure phase

Figure 2: Exploration/exploitation/failure phases, d = 4

Test statistics. We have previously remarked that most traditional model-based algorithms with
sample-complexity guarantees record statistics about the transition probabilities of an environ-
ment. Since the environments are assumed to be ﬁnite, these statistics eventually become accurate
(or irrelevant) and the standard theory on the concentration of measure can be used for hypoth-
esis testing.
In the general case, environments can be inﬁnite and so we cannot collect useful
statistics about individual transitions. Instead, we use the statistics X(ν, κ), which are dependent
on the value function rather than individual transitions. These satisfy Eµ,π[X(µ, κ)i] = 0 while
αk
Eµ,π[X(ν, κ)i]
0 for all ν
i=1 X(ν, κ)i, which
will satisfy certain martingale inequalities.

t. Testing is then performed on the statistic

∈ M

≥

t

M

⊆ M

Updates. As MERL explores, it updates its model class,
, by removing environments
that have become implausible. This is comparable to the updating of conﬁdence intervals for
algorithms such as MBIE (Strehl and Littman, 2005) or UCRL2 (Auer et al., 2010).
In MBIE,
the conﬁdence interval about the empirical estimate of a transition probability is updated after
every observation. A slight theoretical improvement used by UCRL2 is to only update when the
number of samples of a particular statistic doubles. The latter trick allows a cheap application of
the union bound over all updates without wasting too many samples. For our purposes, however,
we need to update slightly more often than the doubling trick would allow. Instead, we check if
an environment should be eliminated if the number of (ν, κ)-exploration phases is exactly αj for
(1, 2). Since the growth of αj is still exponential, the
some j where αj :=
union bound will still be applicable.

and α := 4√N
4√N
−

1 ∈

αj

(cid:6)

(cid:7)

Probabilities. For the remainder of this section, unless otherwise mentioned, all probabilities
is the
and expectations are with respect to Pµ,π where π is the policy of Algorithm 1 and µ
true environment.

∈ M

P

6

9N
Analysis. Deﬁne Gmax := 2
γ)2δ1 and Emax := 2
2
ǫ2(1
ǫ2(1
high probability bounds on the number of failure and exploration phases respectively.

γ)2 log2

γ)2 log2

16N

ǫ2(1

9N
2
γ)2δ1 , which are
ǫ2(1

|K|

−

−

−

−

16N

Theorem 1. Let µ
1. Then

=

ν1, ν2,

∈ M

{

νN

}

· · ·

be the true environment and π be the policy of Algorithm

P

Lǫ

µ,π(h)

d

(Gmax + Emax)

≥

·

δ.

≤

(cid:9)

If lower order logarithmic factors are dropped then the sample-complexity bound of MERL

given by Theorem 1 is ˜O

. Theorem 1 follows from three lemmas.

(cid:8)
γ)3 log2

ǫ2(1

N

−

N
δǫ(1

γ)

−

Lemma 2. µ

(cid:16)

(cid:17)
t for all t with probability 1
−

∈ M

δ/4.

Lemma 3. The number of proper failure phases is bounded by

Gmax :=

216N
ǫ2(1

−

γ)2 log2
|K|

29N

ǫ2(1

γ)2δ1

−

with probability at least 1

δ
2 .

−

Lemma 4. The number of proper exploration phases is bounded by

Emax :=

216N

ǫ2(1

−

γ)2 log2

29N

ǫ2(1

γ)2δ1

−

with probability at least 1

δ
4 .

−

Proof of Theorem 1. Applying the union bound to the results of Lemmas 2, 3 and 4 gives the
following with probability at least 1

δ.

−

1. There are no non-proper exploration or failure phases.

2. The number of proper exploration phases is at most Emax.

3. The number of proper failure phases is at most Gmax.

If π is not ǫ-optimal at time-step t then t is either in an exploration or failure phase. Since both
are exactly d time-steps long the total number of time-steps when π is sub-optimal is at most
(cid:4)
d

(Gmax + Emax).

·

We now turn our attention to proving Lemmas 2, 3 and 4. Of these, Lemma 4 is more concep-

tually challenging while Lemma 3 is intuitively unsurprising, but technically diﬃcult.
Proof of Lemma 2. If µ is removed from

, then there exists a κ and j

N such that

M

∈

αj

i=1
X

X(µ, κ)i

2αj log

≥

r

αj
δ1

.

˜Xi :=

Xi
0
(

(µ, κ)

E

if i
≤
∞
otherwise.

Fix a κ

, E

(µ, κ) := limt Et(µ, κ) and Xi := X(µ, κ)i. Deﬁne a sequence of random variables

∈ K

∞

1 and EBi = 0. That it is a
Now we claim that Bn :=
martingale with zero expectation follows because if t is the time-step at the start of the exploration
phase associated with variable Xi, then E[Xi
1 because discounted returns
are bounded in [0, 1/(1

Bi+1 −
|
Bi
Bi+1 −

γ)] and by the deﬁnition of Xi.

t] = 0.

˜Xi is a martingale with

| ≤

| ≤

Bi

P

|F

|

n
i=1

−

7

For all j

N, we have by Azuma’s inequality that

∈

P

Bαj ≥
(cid:26)

r

2αj log

αj
δ1 (cid:27)

δ1
αj

.

≤

Apply the union bound over all j.

j
∃

∈

N : Bαj ≥

P

(cid:26)

r

2αj log

αj
δ1 (cid:27)

≤

∞

j=1
X

δ1
αj

.

Complete the result by the union bound over all κ, applying Lemma 10 (see Appendix) and the
(cid:4)
deﬁnition of δ1 to bound

δ/4.

κ

∈K

∞j=1

δ1
αj ≤

We are now ready to give a high-probability bound on the number of proper exploration phases.
If MERL starts a proper exploration phase at time-step t then at least one of the following holds:

P

P

1. E[X(ν, κ)E(ν,κ)|F
2. E[X(ν, κ)E(ν,κ)|F

t] > (1

γ)ǫ/8.

t] > (1

γ)ǫ/8.

−

−

t] = 0, which ensures that µ remains in

This contrasts with E[X(µ, κ)E(µ,κ)|F
for all time-steps.
If one could know which of the above statements were true at each time-step then it would be
comparatively easy to show by means of Azuma’s inequality that all environments that are not ǫ-
close are quickly eliminated after O(
γ)2 ) ν-exploration phases, which would lead to the desired
bound. Unfortunately though, the truth of (1) or (2) above cannot be determined, which greatly
increases the complexity of the proof.
Proof of Lemma 4. Fix a κ
and let Emax,κ be a constant to be chosen later. Let ht be the
history at the start of some κ-exploration phase. We say an (ν, κ)-exploration phase is ν-eﬀective
if

1
ǫ2(1

∈ K

M

−

E[X(ν, κ)E(ν,κ)|F

t]

γ)(V π

µ (ht; d)

V π
ν (ht; d))

−

(1

≡
> (1

−

γ)ǫκ/2

−
and ν-eﬀective if the same condition holds for ν. Now since t is the start of a proper exploration
phase we have that µ

t and so

∈ M

V π
ν (ht; d)
V π
ν (ht; d)

V π
µ (ht; d)
V π
ν (ht; d) > ǫκ.

≥

V π
ν (ht; d)

≥

−

Therefore every proper exploration phase is either ν-eﬀective or ν-eﬀective.
ν Et(ν, κ), which is twice the number of κ-exploration phases at time t and E

Let Et,κ :=
,κ := limt Et,κ,
which is twice the total number of κ-exploration phases.1 Let Ft(ν, κ) be the number of ν-eﬀective
P
(ν, κ)-exploration phases up to time-step t. Since each proper κ-exploration phase is either ν-
Et,κ/2. Applying Lemma 8 to yν := Et(ν, κ)/Et,κ
ν Ft(ν, κ)
eﬀective or ν-eﬀective or both,
,κ > Emax,κ then there exists a t′ and ν such that
and xν := Ft(ν, κ)/Et(ν, κ) shows that if E
Et′,κ = Emax,κ and

≥
∞

P

∞

which implies that

Ft′ (ν, κ)

≥ r

Emax,κEt′ (ν, κ)
4N

(a)

≥

Et′ (ν, κ)
√4N

,

1Note that it is never the case that ν = ν at the start of an exploration phase, since in this case ∆ = 0.

(1)

(2)

Ft′ (ν, κ)2

Emax,κEt′ (ν, κ) ≥

1
4N

,

8

Et′ (ν, κ). Let Z(ν) be the event that there exists a t′
where (a) follows because Emax,κ = Et′,κ
≥
Z(ν)
satisfying (1). We will shortly show that P
}
{

). Therefore

< δ/(4N

|K|

P

E

{

∞

,κ > Emax,κ

P

ν : Z(ν)

} ≤

{∃

} ≤

P

Z(ν)
}
{

δ/(4

)

|K|

≤

ν
X
∈M

Finally take the union bound over all κ and let

Emax :=

Emax,κ,

1
2

κ
X
∈K
2 Emax,κ because Emax,κ is a high-probability upper bound on E

where we used 1
the number of κ-exploration phases.

,κ, which is twice

∞

Bounding P {Z(ν)} < δ/(4N |K|). Fix a ν
with Xi
exploration phase. Deﬁne a sequence

, XE∞(ν,κ) be the sequence
:= X(ν, κ)i and let ti be the corresponding time-step at the start of the ith (ν, κ)-

and let X1, X2,

∈ M

· · ·

Yi :=

Xi
0

(

E[Xi

ti]

|F

−

(ν, κ)

E

if i
≤
∞
otherwise

Let λ(E) :=
N is
j

∈

q

2E log E

δ1 . Now if Z(ν), then the largest time-step t

t′ with Et(ν, t) = αj for some

≤

t := max

t
{

≤

t′ :

j
∃

∈

N s.t. αj = Et(ν, t)
}

,

which exists and satisﬁes

1. Et(ν, κ) = αj for some j.

2. E

(ν, κ) > Et(ν, κ).

∞
3. Ft(ν, κ)

Et(ν, κ)Emax,κ/(16N ).

4. Et(ν, κ)

p
Emax,κ/(16N ).

≥

≥

where parts 1 and 2 are straightforward and parts 3 and 4 follow by the deﬁnition of
was chosen speciﬁcally for this part of the proof. Since E
exploration phase starting at time-step t, ν must remain in

, which
(ν, κ) > Et(ν, κ), at the end of the
. Therefore

αj
{

}

∞
M

λ(αj )

αj

αj

Xi

(b)

≥

Yi +

ǫκ(1

−

γ)Ft(ν, κ)
2

i=1
X
ǫκ(1

Yi +

γ)
−
8 r

αjEmax,κ
N

,

(a)

≥

(c)

≥

i=1
X
αj

i=1
X

(3)

where in (a) we used the deﬁnition of the conﬁdence interval of MERL. In (b) we used the deﬁnition
of Yi and the fact that EXi
γ)/2 if Xi is eﬀective. Finally we
used the lower bound on the number of eﬀective ν-exploration phases, Ft(ν, κ) (part 3 above). If
Emax,κ := 2
γ)2 and b = 1/δ1 we
ǫ2
κ(1
obtain

≥
9N
γ)2δ1 , then by applying Lemma 9 with a = 2
2
ǫ2
ǫ2(1
κ(1

0 for all i and EXi

γ)2 log2

ǫκ(1

11N

9N

≥

−

−

−

−

29N

Emax,κ

Emax,κ

≥

ǫ2
κ(1

−

γ)2 log

δ1 ≥

ǫ2
κ(1

29N

−

γ)2 log

αj
δ1

9

Multiplying both sides by αj and rearranging and using the deﬁnition of λ(αj) leads to

ǫκ(1

γ)
−
8 r

αjEmax,κ
N

≥

2λ(αj).

Inserting this into Equation (3) shows that Z(ν) implies that there exists an αj such that

λ(αj). Now by the same argument as in the proof of Lemma 2, Bn :=
1. Therefore by Azuma’s inequality

−
with

Bi

Bi+1 −

|

| ≤

P

αj
i=1 Yi

≤
n
i=1 Yi is a martingale

P

i=1
X
Finally apply the union bound over all j.

αj

P

(

Yi

λ(αj )

≤ −

) ≤

δ1
αj

.

Recall that if MERL is exploiting at time-step t, then πt is the optimal policy with respect to
the ﬁrst environment in the model class. To prove Lemma 3 we start by showing that in this case
πt is nearly-optimal.

Lemma 5. Let t be a time-step and ht be the corresponding history. If µ
exploiting (not exploring), then V ∗µ (ht)

V πt
µ (ht)

5ǫ/8.

∈ M

t and MERL is

−

≤

Proof of Lemma 5. Since MERL is not exploring

V ∗µ (ht)

V πt
µ (ht)

V ∗µ (ht; d)

V πt
µ (ht; d) +

−

ǫ
8

π∗
µ
νt (ht; d)

V

V πt
νt (ht; d) + 5ǫ/8

−

−

(a)

≤
(b)

≤
(c)

≤

5ǫ/8,

(a) follows by truncating the value function. (b) follows because µ
(c) is true since πt is the optimal policy in νt.

∈ M

t and MERL is exploiting.
(cid:4)

Lemma 5 is almost suﬃcient to prove Lemma 3. The only problem is that MERL only follows

πt = π∗νt until there is an exploration phase. The idea to prove Lemma 3 is as follows:

1. If there is a low probability of entering an exploration phase within the next d time-steps
following policy πt, then π is nearly as good as πt, which itself is nearly optimal by Lemma
5.

2. The number of time-steps when the probability of entering an exploration phase within the
next d time-steps is high is unlikely to be too large before an exploration phase is triggered.
Since there are not many exploration phases with high probability, there are also unlikely to
be too many time-steps when π expects to enter one with high probability.

Before the proof of Lemma 3 we remark on an easier to prove (but weaker) version of Theorem 1.
5ǫ/8 < ǫ. Therefore if we
If MERL is exploiting then Lemma 5 shows that V ∗µ (h)
V π
µ ), then we
cared about the number of time-steps when this is not the case (rather than V ∗µ −
would already be done by combining Lemmas 4 and 5.
Proof of Lemma 3. Let t be the start of a proper failure phase with corresponding history, h.
V π
Therefore V ∗µ (h)
µ (h) > ǫ. By Lemma 5, V ∗µ (h)
5ǫ/8 + V πt
V π
µ (h) and so

V π
µ (h) = V ∗µ (h)

µ (h) + V πt
V πt

Q∗µ(h, π(h))

V π
µ (h)

µ (h)

≤

−

−

−

−

−

≤

µ −

(cid:4)

(4)

V πt
µ (h)

V π
µ (h)

−

3ǫ
8

.

≥

10

κ
H

We deﬁne set

κ
H
for the ﬁrst time since t. Let
of h that are at most d long and trigger κ-exploration phases. Therefore

∗ to be the set of extensions of h that trigger κ-exploration phases. Formally
κ if h ⊏ h′ and h′ triggers a κ-exploration phase
H
, which is the set of extensions
κ
∈ H

∗ is the preﬁx free set such that h′ in

h′ : h′
{

t + d
}

κ,d :=

ℓ(h′)

⊂ H

⊂ H

H

≤

∧

3ǫ
8

V πt
µ (h)

V π
µ (h)

−

(a)

≤
(b)
=

(c)

≤

(d)

≤

(e)

≤

P (h′

h)γℓ(h′

)

t

−

|

V πt
µ (h′)

V π
µ (h′)

−

(cid:0)
V πt
µ (h′)

P (h′

h)

(cid:0)

(cid:0)

|

|

|

P (h′

h)

V ∗µ (h′; d)

P (h′

h)4ǫκ +

ǫ
4

,

V π
µ (h′)

+

−

(cid:1)
V π
µ (h′; d)

−

(cid:1)

ǫ
8

+

ǫ
4

(cid:1)

h′
κ
∈Hκ
∈K X
X

h′
κ
∈K X
X

∈Hκ,d

h′
κ
∈K X
X

∈Hκ,d

h′
κ
∈K X
X

∈Hκ,d

(a) follows from Equation (4). (b) by noting that that π = πt until an exploration phase is triggered.
(c) by replacing
γ)ǫ/8. (d) by
H
V πt
substituting V ∗µ (h′)
µ (h′) and by using the eﬀective horizon to truncate the value functions.
(e) by the deﬁnition of a κ-exploration phase.

κ,d and noting that if h′

κ,d, then γℓ(h′

κ
∈ H

κ with

− H

(1

H

≤

−

≥

−

)

t

|

κ

−

h′

≥

h)

3/

2−

P (h′

∈Hκ,d

Since the maximum of a set is greater than the average, there exists a κ

such that
, which is the probability that MERL encounters a κ-exploration
|K|
, tGκ be the sequence of time-
,
phase within d time-steps from h. Now ﬁx a κ and let t1, t2,
P
steps such that ti is the start of a failure phase and the probability of a κ-exploration phase within
be the event that a κ-exploration
the next d time-steps is at least 2−
. Let Yi
phase does occur within d time-steps of ti and deﬁne an auxiliary inﬁnite sequence ˜Y1, ˜Y2,
by
˜Yi := Yi if i
Gκ and 1 otherwise. Let Eκ be the number of κ-exploration phases and Gmax,κ
Yi and
be a constant to be chosen later and suppose Gκ > Gmax,κ, then
Emax,κ or Eκ > Emax,κ, where the latter follows because Yi = 1 implies a
either
≤
κ-exploration phase occurred. Therefore

Gmax,κ
i=1

Gmax,κ
i=1

Gmax,κ
i=1

˜Yi =

∈ K

∈ {

0, 1

|K|

· · ·

· · ·

· · ·

˜Yi

3/

P

P

≤

}

−

κ

P

P

Gκ > Gmax,κ
{

}

Gmax,κ

P

≤

P

≤








i=1
X
Gmax,κ

i=1
X

˜Yi < Emax,κ


+ P

Eκ > Emax,κ
{

}


˜Yi < Emax,κ


+

δ

.

4

|K|

We now choose Gmax,κ suﬃciently large to bound the ﬁrst term in the display above by δ/(4
By the deﬁnition of ˜Yi and Yi, if i
1. Setting

).
and for i > Gκ, ˜Yi is always

Gκ then E[ ˜Yi

|K|

|K|

2−

3/

ti]

|F





≤

≥

−

κ

Gmax,κ := 2κ+4
217N
ǫǫκ(1

=

|K|

Emax,κ

γ)2 log2
|K|
−

29N

γ)2δ1
˜Yi] > 2Emax,κ and an application of Azuma’s inequality to the
is suﬃcient to guarantee E[
martingale diﬀerence sequence completes the result. Finally we apply the union bound over all κ
(cid:4)
and set Gmax :=

P
N Gmax,κ >

Gmax,κ
i=1

Gmax,κ.

ǫ2(1

−

κ

∈

P

κ

∈K

P

11

4 Compact Case

In the last section we presented MERL and proved a sample-complexity bound for the case when
the environment class is ﬁnite.
In this section we show that if the number of environments is
inﬁnite, but compact with respect to the topology generated by a natural metric, then sample-
complexity bounds are still possible with a minor modiﬁcation of MERL. The key idea is to use
compactness to cover the space of environments with ǫ-balls and compute statistics on these balls
rather than individual environments. Since all environments in the same ǫ-ball are suﬃciently close,
the resulting statistics cannot be signiﬁcantly diﬀerent and all analysis goes through identically to
the ﬁnite case. Deﬁne a topology on the space of all environments induced by the pseudo-metric

d(ν1, ν2) := sup
h,π |

V π
ν1 (h)

V π
ν2 (h)
|

.

−

Theorem 6. Let
satisﬁes

M

be compact and coverable by N ǫ-balls then a modiﬁcation of Algorithm 1

The main modiﬁcation is to deﬁne statistics on elements of the cover, rather than speciﬁc

(cid:8)

P

L2ǫ

µ,π(h)

d

(Gmax + Emax)

≥

·

δ.

≤

(cid:9)

environments.

1. Let U1,

, UN be an ǫ-cover of

· · ·

.

M

2. At each time-step choose U and U such that ν

U and ν

U .

∈
on elements of the cover, rather than environments, by

∈

3. Deﬁne statistics

X

{

}

X(U , κ)E(U,κ) := inf
U
∈
X(U , κ)E(U,κ) := inf
U
∈

ν

ν

(1

(1

−

−

γ)(R

−
γ)(V π
ν (h)

V π
ν (h))

R)

−

4. If there exists a U where the test fails then eliminate all environments in that cover.

The proof requires only small modiﬁcations to show that with high probability the U containing
the true environment is never discarded, while those not containing the true environment are if
tested suﬃciently often.

5 Unbounded Environment Classes

If the environment class is non-compact then we cannot in general expect ﬁnite sample-complexity
bounds. Indeed, even asymptotic results are usually not possible.

Theorem 7. There exist non-compact

for which no agent has a ﬁnite PAC bound.

M
is the set of all environments. Then for any policy

The obvious example is when

M
includes an environment that is tuned to ensure the policy acts sub-optimally inﬁnitely often. A
more interesting example is the class of all computable environments, which is non-compact and
also does not admit algorithms with uniform ﬁnite sample-complexity. See negative results by
Lattimore and Hutter [2011b] for counter-examples.

M

12

6 Lower Bound

|

|

S

M

is the class of ﬁnite MDPs with

In speciﬁc cases, the bound in Theorem 1 is
We now turn our attention to the lower bound.
very weak. For example, if
states then a natural covering
leads to a PAC bound with exponential dependence on the state-space while it is known that the
true dependence is at most quadratic. This should not be surprising since information about the
transitions for one state gives information about a large subset of
, not just a single environment.
We show that the bound in Theorem 1 is unimprovable for general environment classes except for
logarithmic factors. That is, there exists a class of environments where Theorem 1 is nearly tight.
and N
. The rewards and transitions are depicted in Figure 3 where the
actions, A =
, aN
}
where for νk we set ǫ(ai) =
transition probabilities depend on the action. Let
can be
[[i = k]]ǫ(1
viewed as a set of bandits with rewards in (0, 1/(1
γ)). In the bandit domain tight lower bounds
on sample-complexity are known and given in Mannor and Tsitsiklis [2004]. These results can be
applied as in Strehl et al. [2009] and Lattimore and Hutter [2012] to show that no algorithm has
sample-complexity less than O(

γ). Therefore in environment νk, ak is the optimal action in state 1.

The simplest counter-example is a set of MDPs with four states, S =

ν1,
{

0, 1,

, νN

a1,

⊖}

M

M

M

· · ·

· · ·

:=

⊕

−

−

{

}

{

N

,

γ)3 log 1

δ ).

ǫ2(1

−

q := 2

1/γ

−

1
2 −

ǫ(a)

1
r = 0

⊖r = 0

1
2 + ǫ(a)

1

p

−

1

q

−

⊕r = 1

q

0
r = 0

1

q

−

p := 1/(2

γ)

−

Figure 3: Counter-example

7 Conclusions

Summary. The Maximum Exploration Reinforcement Learning algorithm was presented. For
ﬁnite classes of arbitrary environments a sample-complexity bound was given that is linear in the
number of environments. We also presented lower bounds that show that in general this cannot
be improved except for logarithmic factors. Learning is also possible for compact classes with the
sample complexity depending on the size of the smallest ǫ-cover where the distance between two
environments is the diﬀerence in value functions over all policies and history sequences. Finally,
for non-compact classes of environments sample-complexity bounds are typically not possible.

Running time. The running time of MERL can be arbitrary large since computing the policy
maximising ∆ depends on the environment class used. Even assuming the distribution of obser-
vation/rewards given the history can be computed in constant time, the values of optimal policies
can still only be computed in time exponential in the horizon.

13

