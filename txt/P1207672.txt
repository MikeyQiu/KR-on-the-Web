8
1
0
2
 
g
u
A
 
3
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
0
7
1
0
.
4
0
7
1
:
v
i
X
r
a

Journal of Machine Learning Research 18 (2018) 1-78

Submitted 11/17; Published 6/18

Learning Certiﬁably Optimal Rule Lists for Categorical Data

Elaine Angelino
Department of Electrical Engineering and Computer Sciences
University of California, Berkeley, Berkeley, CA 94720

elaine@eecs.berkeley.edu

Nicholas Larus-Stone
Daniel Alabi
Margo Seltzer
School of Engineering and Applied Sciences
Harvard University, Cambridge, MA 02138

nlarusstone@alumni.harvard.edu
alabid@g.harvard.edu
margo@eecs.harvard.edu

Cynthia Rudin∗
Department of Computer Science and Department of Electrical and Computer Engineering
Duke University, Durham, NC 27708

cynthia@cs.duke.edu

Editor: Maya Gupta
∗To whom correspondence should be addressed.

Abstract
We present the design and implementation of a custom discrete optimization technique for
building rule lists over a categorical feature space. Our algorithm produces rule lists with
optimal training performance, according to the regularized empirical risk, with a certiﬁcate
of optimality. By leveraging algorithmic bounds, eﬃcient data structures, and computa-
tional reuse, we achieve several orders of magnitude speedup in time and a massive reduc-
tion of memory consumption. We demonstrate that our approach produces optimal rule
lists on practical problems in seconds. Our results indicate that it is possible to construct
optimal sparse rule lists that are approximately as accurate as the COMPAS proprietary
risk prediction tool on data from Broward County, Florida, but that are completely inter-
pretable. This framework is a novel alternative to CART and other decision tree methods
for interpretable modeling.
Keywords: rule lists, decision trees, optimization, interpretable models, criminal justice
applications

1. Introduction

As machine learning continues to gain prominence in socially-important decision-making,
the interpretability of predictive models remains a crucial problem. Our goal is to build
models that are highly predictive, transparent, and easily understood by humans. We use
rule lists, also known as decision lists, to achieve this goal. Rule lists are predictive models
composed of if-then statements; these models are interpretable because the rules provide a
reason for each prediction (Figure 1).

Constructing rule lists, or more generally, decision trees, has been a challenge for more
than 30 years; most approaches use greedy splitting techniques (Rivest, 1987; Breiman
et al., 1984; Quinlan, 1993). Recent approaches use Bayesian analysis, either to ﬁnd a locally
optimal solution (Chipman et al., 1998) or to explore the search space (Letham et al., 2015;

c(cid:13)2018 Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v18/17-716.html.

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 23) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

Figure 1: An example rule list that predicts two-year recidivism for the ProPublica data

set, found by CORELS.

Yang et al., 2017). These approaches achieve high accuracy while also managing to run
reasonably quickly. However, despite the apparent accuracy of the rule lists generated by
these algorithms, there is no way to determine either if the generated rule list is optimal
or how close it is to optimal, where optimality is deﬁned with respect to minimization of a
regularized loss function.

Optimality is important, because there are societal implications for a lack of optimality.
Consider the ProPublica article on the Correctional Oﬀender Management Proﬁling for Al-
ternative Sanctions (COMPAS) recidivism prediction tool (Larson et al., 2016). It highlights
a case where a black box, proprietary predictive model is being used for recidivism predic-
tion. The authors hypothesize that the COMPAS scores are racially biased, but since the
model is not transparent, no one (outside of the creators of COMPAS) can determine the
reason or extent of the bias (Larson et al., 2016), nor can anyone determine the reason for
any particular prediction. By using COMPAS, users implicitly assumed that a transparent
model would not be suﬃciently accurate for recidivism prediction, i.e., they assumed that
a black box model would provide better accuracy. We wondered whether there was indeed
no transparent and suﬃciently accurate model. Answering this question requires solving a
computationally hard problem. Namely, we would like to both ﬁnd a transparent model that
is optimal within a particular pre-determined class of models and produce a certiﬁcate of
its optimality, with respect to the regularized empirical risk. This would enable one to say,
for this problem and model class, with certainty and before resorting to black box methods,
whether there exists a transparent model. While there may be diﬀerences between train-
ing and test performance, ﬁnding the simplest model with optimal training performance is
prescribed by statistical learning theory.

To that end, we consider the class of rule lists assembled from pre-mined frequent item-
sets and search for an optimal rule list that minimizes a regularized risk function, R. This
is a hard discrete optimization problem. Brute force solutions that minimize R are compu-
tationally prohibitive due to the exponential number of possible rule lists. However, this is
a worst case bound that is not realized in practical settings. For realistic cases, it is possible
to solve fairly large cases of this problem to optimality, with the careful use of algorithms,
data structures, and implementation techniques.

We develop specialized tools from the ﬁelds of discrete optimization and artiﬁcial intel-
ligence. Speciﬁcally, we introduce a special branch-and bound algorithm, called Certiﬁably
Optimal RulE ListS (CORELS), that provides the optimal solution according to the train-
ing objective, along with a certiﬁcate of optimality. The certiﬁcate of optimality means that
we can investigate how close other models (e.g., models provided by greedy algorithms) are
to optimal.

2

Learning Certifiably Optimal Rule Lists

Within its branch-and-bound procedure, CORELS maintains a lower bound on the
minimum value of R that each incomplete rule list can achieve. This allows CORELS to
prune an incomplete rule list (and every possible extension) if the bound is larger than
the error of the best rule list that it has already evaluated. The use of careful bounding
techniques leads to massive pruning of the search space of potential rule lists. The algorithm
continues to consider incomplete and complete rule lists until it has either examined or
eliminated every rule list from consideration. Thus, CORELS terminates with the optimal
rule list and a certiﬁcate of optimality.

The eﬃciency of CORELS depends on how much of the search space our bounds allow us
to prune; we seek a tight lower bound on R. The bound we maintain throughout execution is
a maximum of several bounds that come in three categories. The ﬁrst category of bounds are
those intrinsic to the rules themselves. This category includes bounds stating that each rule
must capture suﬃcient data; if not, the rule list is provably non-optimal. The second type of
bound compares a lower bound on the value of R to that of the current best solution. This
allows us to exclude parts of the search space that could never be better than our current
solution. Finally, our last type of bound is based on comparing incomplete rule lists that
capture the same data and allows us to pursue only the most accurate option. This last
class of bounds is especially important—without our use of a novel symmetry-aware map,
we are unable to solve most problems of reasonable scale. This symmetry-aware map keeps
track of the best accuracy over all observed permutations of a given incomplete rule list.

We keep track of these bounds using a modiﬁed preﬁx tree, a data structure also known
as a trie. Each node in the preﬁx tree represents an individual rule; thus, each path in the
tree represents a rule list such that the ﬁnal node in the path contains metrics about that
rule list. This tree structure, together with a search policy and sometimes a queue, enables a
variety of strategies, including breadth-ﬁrst, best-ﬁrst, and stochastic search. In particular,
we can design diﬀerent best-ﬁrst strategies by customizing how we order elements in a
priority queue. In addition, we are able to limit the number of nodes in the trie and thereby
enable tuning of space-time tradeoﬀs in a robust manner. This trie structure is a useful way
of organizing the generation and evaluation of rule lists.

We evaluated CORELS on a number of publicly available data sets. Our metric of
success was 10-fold cross-validated prediction accuracy on a subset of the data. These data
sets involve hundreds of rules and thousands of observations. CORELS is generally able to
ﬁnd an optimal rule list in a matter of seconds and certify its optimality within about 10
minutes. We show that we are able to achieve better or similar out-of-sample accuracy on
these data sets compared to the popular greedy algorithms, CART and C4.5.

CORELS targets large (not massive) problems, where interpretability and certiﬁable
optimality are important. We illustrate the eﬃcacy of our approach using (1) the ProPublica
COMPAS data set (Larson et al., 2016), for the problem of two-year recidivism prediction,
and (2) stop-and-frisk data sets from the NYPD (New York Police Department, 2016) and
the NYCLU (New York Civil Liberties Union, 2014), to predict whether a weapon will
be found on a stopped individual who is frisked or searched. On these data, we produce
certiﬁably optimal, interpretable rule lists that achieve the same accuracy as approaches
such as random forests. This calls into question the need for use of a proprietary, black box
algorithm for recidivism prediction.

3

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Our work overlaps with the thesis of Larus-Stone (2017). We have also written a pre-
liminary conference version of this article (Angelino et al., 2017), and a report highlighting
systems optimizations of our implementation (Larus-Stone et al., 2018); the latter includes
additional empirical measurements not presented here.

Our code is at https://github.com/nlarusstone/corels, where we provide the C++
implementation we used in our experiments (§6). Kaxiras and Saligrama (2018) have also
created an interactive web interface at https://corels.eecs.harvard.edu, where a user
can upload data and run CORELS from a browser.

2. Related Work

Since every rule list is a decision tree and every decision tree can be expressed as an
equivalent rule list, the problem we are solving is a version of the “optimal decision tree”
problem, though regularization changes the nature of the problem (as shown through our
bounds). The optimal decision tree problem is computationally hard, though since the late
1990’s, there has been research on building optimal decision trees using optimization tech-
niques (Bennett and Blue, 1996; Dobkin et al., 1996; Farhangfar et al., 2008). A particularly
interesting paper along these lines is that of Nijssen and Fromont (2010), who created a
“bottom-up” way to form optimal decision trees. Their method performs an expensive search
step, mining all possible leaves (rather than all possible rules), and uses those leaves to form
trees. Their method can lead to memory problems, but it is possible that these memory
issues can be mitigated using the theorems in this paper.1 None of these methods used the
tight bounds and data structures of CORELS.

Because the optimal decision tree problem is hard, there are a huge number of algo-
rithms such as CART (Breiman et al., 1984) and C4.5 (Quinlan, 1993) that do not perform
exploration of the search space beyond greedy splitting. Similarly, there are decision list
and associative classiﬁcation methods that construct rule lists iteratively in a greedy way
(Rivest, 1987; Liu et al., 1998; Li et al., 2001; Yin and Han, 2003; Sokolova et al., 2003;
Marchand and Sokolova, 2005; Vanhoof and Depaire, 2010; Rudin et al., 2013). Some ex-
ploration of the search space is done by Bayesian decision tree methods (Dension et al.,
1998; Chipman et al., 2002, 2010) and Bayesian rule-based methods (Letham et al., 2015;
Yang et al., 2017). The space of trees of a given depth is much larger than the space of rule
lists of that same depth, and the trees within the Bayesian tree algorithms are grown in a
top-down greedy way. Because of this, authors of Bayesian tree algorithms have noted that
their MCMC chains tend to reach only locally optimal solutions. The RIPPER algorithm
(Cohen, 1995) is similar to the Bayesian tree methods in that it grows, prunes, and then
locally optimizes. The space of rule lists is smaller than that of trees, and has simpler struc-
ture. Consequently, Bayesian rule list algorithms tend to be more successful at escaping
local minima and can introduce methods of exploring the search space that exploit this
structure—these properties motivate our focus on lists. That said, the tightest bounds for
the Bayesian lists (namely, those of Yang et al., 2017, upon whose work we build), are not
nearly as tight as those of CORELS.

1. There is no public version of their code for distribution as of this writing.

4

Learning Certifiably Optimal Rule Lists

Tight bounds, on the other hand, have been developed for the (immense) literature on
building disjunctive normal form (DNF) models; a good example of this is the work of Rijn-
beek and Kors (2010). For models of a given size, since the class of DNF’s is a proper subset
of decision lists, our framework can be restricted to learn optimal DNF’s. The ﬁeld of DNF
learning includes work from the ﬁelds of rule learning/induction (e.g., early algorithms by
Michalski, 1969; Clark and Niblett, 1989; Frank and Witten, 1998) and associative classiﬁ-
cation (Vanhoof and Depaire, 2010). Most papers in these ﬁelds aim to carefully guide the
search through the space of models. If we were to place a restriction on our code to learn
DNF’s, which would require restricting predictions within the list to the positive class only,
we could potentially use methods from rule learning and associative classiﬁcation to help
order CORELS’ queue, which would in turn help us eliminate parts of the search space
more quickly.

Some of our bounds, including the minimum support bound (§3.7, Theorem 10), come
from Rudin and Ertekin (2016), who provide ﬂexible mixed-integer programming (MIP)
formulations using the same objective as we use here; MIP solvers in general cannot compete
with the speed of CORELS.

CORELS depends on pre-mined rules, which we obtain here via enumeration. The litera-
ture on association rule mining is huge, and any method for rule mining could be reasonably
substituted.

CORELS’ main use is for producing interpretable predictive models. There is a grow-
ing interest in interpretable (transparent, comprehensible) models because of their societal
importance (see R¨uping, 2006; Bratko, 1997; Dawes, 1979; Vellido et al., 2012; Giraud-
Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans et al., 2011; Freitas, 2014). There are
now regulations on algorithmic decision-making in the European Union on the “right to an
explanation” (Goodman and Flaxman, 2016) that would legally require interpretability of
predictions. There is work in both the DNF literature (R¨uckert and Raedt, 2008) and deci-
sion tree literature (Garofalakis et al., 2000) on building interpretable models. Interpretable
models must be so sparse that they need to be heavily optimized; heuristics tend to produce
either inaccurate or non-sparse models.

Interpretability has many meanings, and it is possible to extend the ideas in this work
to other deﬁnitions of interpretability; these rule lists may have exotic constraints that help
with ease-of-use. For example, Falling Rule Lists (Wang and Rudin, 2015a) are constrained
to have decreasing probabilities down the list, which makes it easier to assess whether an
observation is in a high risk subgroup. In parallel to this paper, we have been working on
an algorithm for Falling Rule Lists (Chen and Rudin, 2018) with bounds similar to those
presented here, but even CORELS’ basic support bounds do not hold for the falling case,
which is much more complicated. One advantage of the approach taken by Chen and Rudin
(2018) is that it can handle class imbalance by weighting the positive and negative classes
diﬀerently; this extension is possible in CORELS but not addressed here.

The models produced by CORELS are predictive only; they cannot be used for policy-
making because they are not causal models, they do not include the costs of true and false
positives, nor the cost of gathering information. It is possible to adapt CORELS’ frame-
work for causal inference (Wang and Rudin, 2015b), dynamic treatment regimes (Zhang
et al., 2015), or cost-sensitive dynamic treatment regimes (Lakkaraju and Rudin, 2017) to

5

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 23) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if p1 then predict q1
else if p2 then predict q2
else if p3 then predict q3
else predict q0

Figure 2: The rule list d = (r1, r2, r3, r0). Each rule is of the form rk = pk → qk, for all
k = 0, . . . , 3. We can also express this rule list as d = (dp, δp, q0, K), where
dp = (p1, p2, p3), δp = (1, 1, 1, 1), q0 = 0, and K = 3. This is the same 3-rule list
as in Figure 1, that predicts two-year recidivism for the ProPublica data set.

help with policy design. CORELS could potentially be adapted to handle these kinds of
interesting problems.

3. Learning Optimal Rule Lists

In this section, we present our framework for learning certiﬁably optimal rule lists. First, we
deﬁne our setting and useful notation (§3.1) and then the objective function we seek to min-
imize (§3.2). Next, we describe the principal structure of our optimization algorithm (§3.3),
which depends on a hierarchically structured objective lower bound (§3.4). We then derive
a series of additional bounds that we incorporate into our algorithm, because they enable
aggressive pruning of our state space.

3.1 Notation

We restrict our setting to binary classiﬁcation, where rule lists are Boolean functions; this
framework is straightforward to generalize to multi-class classiﬁcation. Let {(xn, yn)}N
n=1
denote training data, where xn ∈ {0, 1}J are binary features and yn ∈ {0, 1} are labels.
Let x = {xn}N

n=1, and let xn,j denote the j-th feature of xn.

n=1 and y = {yn}N

A rule list d = (r1, r2, . . . , rK, r0) of length K ≥ 0 is a (K + 1)-tuple consisting of K
distinct association rules, rk = pk → qk, for k = 1, . . . , K, followed by a default rule r0.
Figure 2 illustrates a rule list, d = (r1, r2, r3, r0), which for clarity, we sometimes call a K-
rule list. An association rule r = p → q is an implication corresponding to the conditional
statement, “if p, then q.” In our setting, an antecedent p is a Boolean assertion that evaluates
to either true or false for each datum xn, and a consequent q is a label prediction. For
example, (xn,1 = 0) ∧ (xn,3 = 1) → (yn = 1) is an association rule. The ﬁnal default rule r0
in a rule list can be thought of as a special association rule p0 → q0 whose antecedent p0
simply asserts true.

Let d = (r1, r2, . . . , rK, r0) be a K-rule list, where rk = pk → qk for each k = 0, . . . , K.
We introduce a useful alternate rule list representation: d = (dp, δp, q0, K), where we deﬁne
dp = (p1, . . . , pK) to be d’s preﬁx, δp = (q1, . . . , qK) ∈ {0, 1}K gives the label predictions
associated with dp, and q0 ∈ {0, 1} is the default label prediction. For example, for the
rule list in Figure 1, we would write d = (dp, δp, q0, K), where dp = (p1, p2, p3), δp = (1, 1, 1),
q0 = 0, and K = 3. Note that ((), (), q0, 0) is a well-deﬁned rule list with an empty preﬁx;
it is completely deﬁned by a single default rule.

Let dp = (p1, . . . , pk, . . . , pK) be an antecedent list, then for any k ≤ K, we deﬁne dk

(p1, . . . , pk) to be the k-preﬁx of dp. For any such k-preﬁx dk

p =
p, we say that dp starts with dk
p.

6

Learning Certifiably Optimal Rule Lists

For any given space of rule lists, we deﬁne σ(dp) to be the set of all rule lists whose preﬁxes
start with dp:

σ(dp) = {(d(cid:48)

p, δ(cid:48)

p, q(cid:48)

0, K(cid:48)) : d(cid:48)

p starts with dp}.

(1)

If dp = (p1, . . . , pK) and d(cid:48)
and extends it by a single antecedent, we say that dp is the parent of d(cid:48)
child of dp.

p = (p1, . . . , pK, pK+1) are two preﬁxes such that d(cid:48)

p starts with dp
p is a

p and that d(cid:48)

A rule list d classiﬁes datum xn by providing the label prediction qk of the ﬁrst rule rk
whose antecedent pk is true for xn. We say that an antecedent pk of antecedent list dp
captures xn in the context of dp if pk is the ﬁrst antecedent in dp that evaluates to true
for xn. We also say that a preﬁx captures those data captured by its antecedents; for a rule
list d = (dp, δp, q0, K), data not captured by the preﬁx dp are classiﬁed according to the
default label prediction q0.

Let β be a set of antecedents. We deﬁne cap(xn, β) = 1 if an antecedent in β captures
p starts

p be preﬁxes such that d(cid:48)

datum xn, and 0 otherwise. For example, let dp and d(cid:48)
with dp, then d(cid:48)

p captures all the data that dp captures:

{xn : cap(xn, dp)} ⊆ {xn : cap(xn, d(cid:48)

p)}.

Now let dp be an ordered list of antecedents, and let β be a subset of antecedents in dp.
Let us deﬁne cap(xn, β | dp) = 1 if β captures datum xn in the context of dp, i.e., if the ﬁrst
antecedent in dp that evaluates to true for xn is an antecedent in β, and 0 otherwise. Thus,
cap(xn, β | dp) = 1 only if cap(xn, β) = 1; cap(xn, β | dp) = 0 either if cap(xn, β) = 0, or if
cap(xn, β) = 1 but there is an antecedent α in dp, preceding all antecedents in β, such that
cap(xn, α) = 1. For example, if dp = (p1, . . . , pk, . . . , pK) is a preﬁx, then

cap(xn, pk | dp) =

¬ cap(xn, pk(cid:48))

∧ cap(xn, pk)

(cid:32) k−1
(cid:94)

k(cid:48)=1

(cid:33)

indicates whether antecedent pk captures datum xn in the context of dp. Now, deﬁne
supp(β, x) to be the normalized support of β,

and similarly deﬁne supp(β, x | dp) to be the normalized support of β in the context of dp,

supp(β, x) =

cap(xn, β),

supp(β, x | dp) =

cap(xn, β | dp),

(2)

(3)

Next, we address how empirical data constrains rule lists. Given training data (x, y), an
antecedent list dp = (p1, . . . , pK) implies a rule list d = (dp, δp, q0, K) with preﬁx dp, where
the label predictions δp = (q1, . . . , qK) and q0 are empirically set to minimize the number
of misclassiﬁcation errors made by the rule list on the training data. Thus for 1 ≤ k ≤ K,
label prediction qk corresponds to the majority label of data captured by antecedent pk in

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

7

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

the context of dp, and the default q0 corresponds to the majority label of data not captured
by dp. In the remainder of our presentation, whenever we refer to a rule list with a particular
preﬁx, we implicitly assume these empirically determined label predictions.

Our method is technically an associative classiﬁcation method since it leverages pre-

mined rules.

3.2 Objective Function

We deﬁne a simple objective function for a rule list d = (dp, δp, q0, K):

R(d, x, y) = (cid:96)(d, x, y) + λK.

(4)

This objective function is a regularized empirical risk; it consists of a loss (cid:96)(d, x, y), mea-
suring misclassiﬁcation error, and a regularization term that penalizes longer rule lists.
(cid:96)(d, x, y) is the fraction of training data whose labels are incorrectly predicted by d. In
our setting, the regularization parameter λ ≥ 0 is a small constant; e.g., λ = 0.01 can be
thought of as adding a penalty equivalent to misclassifying 1% of data when increasing a
rule list’s length by one association rule.

3.3 Optimization Framework

Our objective has structure amenable to global optimization via a branch-and-bound frame-
work. In particular, we make a series of important observations, each of which translates
into a useful bound, and that together interact to eliminate large parts of the search space.
We discuss these in depth in what follows:

• Lower bounds on a preﬁx also hold for every extension of that preﬁx. (§3.4, Theorem 1)

• If a rule list is not accurate enough with respect to its length, we can prune all

extensions of it. (§3.4, Lemma 2)

• We can calculate a priori an upper bound on the maximum length of an optimal rule

list. (§3.5, Theorem 6)

• Each rule in an optimal rule list must have support that is suﬃciently large. This allows
us to construct rule lists from frequent itemsets, while preserving the guarantee that
we can ﬁnd a globally optimal rule list from pre-mined rules. (§3.7, Theorem 10)

• Each rule in an optimal rule list must predict accurately. In particular, the number of
observations predicted correctly by each rule in an optimal rule list must be above a
threshold. (§3.7, Theorem 11)

• We need only consider the optimal permutation of antecedents in a preﬁx; we can

omit all other permutations. (§3.10, Theorem 15 and Corollary 16)

• If multiple observations have identical features and opposite labels, we know that any
model will make mistakes. In particular, the number of mistakes on these observations
will be at least the number of observations with the minority label. (§3.14, Theorem 20)

8

Learning Certifiably Optimal Rule Lists

3.4 Hierarchical Objective Lower Bound

We can decompose the misclassiﬁcation error in (4) into two contributions corresponding
to the preﬁx and the default rule:

(cid:96)(d, x, y) ≡ (cid:96)p(dp, δp, x, y) + (cid:96)0(dp, q0, x, y),

where dp = (p1, . . . , pK) and δp = (q1, . . . , qK);

(cid:96)p(dp, δp, x, y) =

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn]

is the fraction of data captured and misclassiﬁed by the preﬁx, and

(cid:96)0(dp, q0, x, y) =

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn]

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

1
N

N
(cid:88)

n=1

is the fraction of data not captured by the preﬁx and misclassiﬁed by the default rule.
Eliminating the latter error term gives a lower bound b(dp, x, y) on the objective,

b(dp, x, y) ≡ (cid:96)p(dp, δp, x, y) + λK ≤ R(d, x, y),

(5)

where we have suppressed the lower bound’s dependence on label predictions δp because
they are fully determined, given (dp, x, y). Furthermore, as we state next in Theorem 1,
b(dp, x, y) gives a lower bound on the objective of any rule list whose preﬁx starts with dp.

Theorem 1 (Hierarchical objective lower bound) Deﬁne b(dp, x, y) as in (5). Also,
deﬁne σ(dp) to be the set of all rule lists whose preﬁxes starts with dp, as in (1). Let d =
(dp, δp, q0, K) be a rule list with preﬁx dp, and let d(cid:48) = (d(cid:48)
0, K(cid:48)) ∈ σ(dp) be any rule
list such that its preﬁx d(cid:48)

p starts with dp and K(cid:48) ≥ K, then b(dp, x, y) ≤ R(d(cid:48), x, y).

p, q(cid:48)

p, δ(cid:48)

Proof Let dp = (p1, . . . , pK) and δp = (q1, . . . , qK); let d(cid:48)
δ(cid:48)
p = (q1, . . . , qK, qK+1, . . . , qK(cid:48)). Notice that d(cid:48)
additional mistakes:

p = (p1, . . . , pK, pK+1, . . . , pK(cid:48)) and
p yields the same mistakes as dp, and possibly

(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) =

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn]

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=1

=

1
N

N
(cid:88)

(cid:32) K
(cid:88)

n=1

k=1

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn] +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn]

(cid:33)

K(cid:48)
(cid:88)

k=K+1

= (cid:96)p(dp, δp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] ≥ (cid:96)p(dp, δp, x, y),

(6)

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K+1

where in the second equality we have used the fact that cap(xn, pk | d(cid:48)
for 1 ≤ k ≤ K. It follows that

p) = cap(xn, pk | dp)

b(dp, x, y) = (cid:96)p(dp, δp, x, y) + λK

≤ (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + λK(cid:48) = b(d(cid:48)

p, x, y) ≤ R(d(cid:48), x, y).

(7)

9

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Algorithm 1 Branch-and-bound for learning rule lists.

m=1, training data (x, y) = {(xn, yn)}N

Input: Objective function R(d, x, y), objective lower bound b(dp, x, y), set of antecedents
n=1, initial best known rule list d0 with
S = {sm}M
objective R0 = R(d0, x, y); d0 could be obtained as output from another (approximate)
algorithm, otherwise, (d0, R0) = (null, 1) provide reasonable default values
Output: Provably optimal rule list d∗ with minimum objective R∗

(dc, Rc) ← (d0, R0)
Q ← queue( [ ( ) ] )
while Q not empty do
dp ← Q.pop( )
d ← (dp, δp, q0, K)
if b(dp, x, y) < Rc then
R ← R(d, x, y)
if R < Rc then

(dc, Rc) ← (d, R)

end if
for s in S do

if s not in dp then
Q.push( (dp, s) )

end if

end for

end if
end while
(d∗, R∗) ← (dc, Rc)

(cid:46) Initialize best rule list and objective
(cid:46) Initialize queue with empty preﬁx
(cid:46) Stop when queue is empty
(cid:46) Remove preﬁx dp from the queue
(cid:46) Set label predictions δp and q0 to minimize training error
(cid:46) Bound: Apply Theorem 1
(cid:46) Compute objective of dp’s rule list d
(cid:46) Update best rule list and objective

(cid:46) Branch: Enqueue dp’s children

(cid:46) Identify provably optimal solution

To generalize, consider a sequence of preﬁxes such that each preﬁx starts with all previ-
ous preﬁxes in the sequence. It follows that the corresponding sequence of objective lower
bounds increases monotonically. This is precisely the structure required and exploited by
branch-and-bound, illustrated in Algorithm 1.

Speciﬁcally, the objective lower bound in Theorem 1 enables us to prune the state
space hierarchically. While executing branch-and-bound, we keep track of the current best
(smallest) objective Rc, thus it is a dynamic, monotonically decreasing quantity. If we
encounter a preﬁx dp with lower bound b(dp, x, y) ≥ Rc, then by Theorem 1, we do not need
to consider any rule list d(cid:48) ∈ σ(dp) whose preﬁx d(cid:48)
p starts with dp. For the objective of such
a rule list, the current best objective provides a lower bound, i.e., R(d(cid:48), x, y) ≥ b(d(cid:48)
p, x, y) ≥
b(dp, x, y) ≥ Rc, and thus d(cid:48) cannot be optimal.

Next, we state an immediate consequence of Theorem 1.

Lemma 2 (Objective lower bound with one-step lookahead) Let dp be a K-preﬁx
and let Rc be the current best objective. If b(dp, x, y) + λ ≥ Rc, then for any K(cid:48)-rule list
d(cid:48) ∈ σ(dp) whose preﬁx d(cid:48)

p starts with dp and K(cid:48) > K, it follows that R(d(cid:48), x, y) ≥ Rc.

10

Learning Certifiably Optimal Rule Lists

Proof By the deﬁnition of the lower bound (5), which includes the penalty for longer
preﬁxes,

R(d(cid:48)

p, x, y) ≥ b(d(cid:48)

p, x, y) = (cid:96)p(d(cid:48)
= (cid:96)p(d(cid:48)
= b(dp, x, y) + λ(K(cid:48) − K) ≥ b(dp, x, y) + λ ≥ Rc.

p, x, y) + λK(cid:48)
p, x, y) + λK + λ(K(cid:48) − K)

p, δ(cid:48)
p, δ(cid:48)

(8)

Therefore, even if we encounter a preﬁx dp with lower bound b(dp, x, y) ≤ Rc, as long
p that start with and are longer

as b(dp, x, y) + λ ≥ Rc, then we can prune all preﬁxes d(cid:48)
than dp.

3.5 Upper Bounds on Preﬁx Length

In this section, we derive several upper bounds on preﬁx length:

• The simplest upper bound on preﬁx length is given by the total number of available

antecedents. (Proposition 3)

• The current best objective Rc implies an upper bound on preﬁx length. (Theorem 4)

• For intuition, we state a version of the above bound that is valid at the start of

execution. (Corollary 5)

length. (Theorem 6)

• By considering speciﬁc families of preﬁxes, we can obtain tighter bounds on preﬁx

In the next section (§3.6), we use these results to derive corresponding upper bounds on the
number of preﬁx evaluations made by Algorithm 1.

Proposition 3 (Trivial upper bound on preﬁx length) Consider a state space of all
rule lists formed from a set of M antecedents, and let L(d) be the length of rule list d.
M provides an upper bound on the length of any optimal rule list d∗ ∈ argmind R(d, x, y),
i.e., L(d) ≤ M .

Proof Rule lists consist of distinct rules by deﬁnition.

At any point during branch-and-bound execution, the current best objective Rc implies

an upper bound on the maximum preﬁx length we might still have to consider.

Theorem 4 (Upper bound on preﬁx length) Consider a state space of all rule lists
formed from a set of M antecedents. Let L(d) be the length of rule list d and let Rc be the
current best objective. For all optimal rule lists d∗ ∈ argmind R(d, x, y)

L(d∗) ≤ min

(cid:23)

(cid:18)(cid:22) Rc
λ

(cid:19)

, M

,

11

(9)

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

where λ is the regularization parameter. Furthermore, if dc is a rule list with objective
R(dc, x, y) = Rc, length K, and zero misclassiﬁcation error, then for every optimal rule
list d∗ ∈ argmind R(d, x, y), if dc ∈ argmind R(d, x, y), then L(d∗) ≤ K, or otherwise if
dc /∈ argmind R(d, x, y), then L(d∗) ≤ K − 1.

Proof For an optimal rule list d∗ with objective R∗,

λL(d∗) ≤ R∗ = R(d∗, x, y) = (cid:96)(d∗, x, y) + λL(d∗) ≤ Rc.

The maximum possible length for d∗ occurs when (cid:96)(d∗, x, y) is minimized; combining with
Proposition 3 gives bound (9).

For the rest of the proof, let K∗ = L(d∗) be the length of d∗. If the current best rule

list dc has zero misclassiﬁcation error, then

λK∗ ≤ (cid:96)(d∗, x, y) + λK∗ = R(d∗, x, y) ≤ Rc = R(dc, x, y) = λK,

and thus K∗ ≤ K. If the current best rule list is suboptimal, i.e., dc /∈ argmind R(d, x, y),
then

λK∗ ≤ (cid:96)(d∗, x, y) + λK∗ = R(d∗, x, y) < Rc = R(dc, x, y) = λK,

in which case K∗ < K, i.e., K∗ ≤ K − 1, since K is an integer.

The latter part of Theorem 4 tells us that if we only need to identify a single instance
of an optimal rule list d∗ ∈ argmind R(d, x, y), and we encounter a perfect K-rule list with
zero misclassiﬁcation error, then we can prune all preﬁxes of length K or greater.

Corollary 5 (Simple upper bound on preﬁx length) Let L(d) be the length of rule
list d. For all optimal rule lists d∗ ∈ argmind R(d, x, y),

L(d∗) ≤ min

(cid:23)

(cid:18)(cid:22) 1
2λ

(cid:19)

, M

.

(10)

Proof Let d = ((), (), q0, 0) be the empty rule list; it has objective R(d, x, y) = (cid:96)(d, x, y) ≤
1/2, which gives an upper bound on Rc. Combining with (9) and Proposition 3 gives (10).

For any particular preﬁx dp, we can obtain potentially tighter upper bounds on preﬁx

length for the family of all preﬁxes that start with dp.

Theorem 6 (Preﬁx-speciﬁc upper bound on preﬁx length) Let d = (dp, δp, q0, K) be
a rule list, let d(cid:48) = (d(cid:48)
p starts with dp, and
let Rc be the current best objective. If d(cid:48)

0, K(cid:48)) ∈ σ(dp) be any rule list such that d(cid:48)
p has lower bound b(d(cid:48)

p, x, y) < Rc, then

p, q(cid:48)

p, δ(cid:48)

(cid:18)

K(cid:48) < min

K +

(cid:22) Rc − b(dp, x, y)
λ

(cid:23)

(cid:19)

, M

.

(11)

12

Learning Certifiably Optimal Rule Lists

Proof First, note that K(cid:48) ≥ K, since d(cid:48)

p starts with dp. Now recall from (7) that

b(dp, x, y) = (cid:96)p(dp, δp, x, y) + λK ≤ (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + λK(cid:48) = b(d(cid:48)

p, x, y),

and from (6) that (cid:96)p(dp, δp, x, y) ≤ (cid:96)p(d(cid:48)
gives

p, δ(cid:48)

p, x, y). Combining these bounds and rearranging

b(d(cid:48)

p, x, y) = (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + λK + λ(K(cid:48) − K)

≥ (cid:96)p(dp, δp, x, y) + λK + λ(K(cid:48) − K) = b(dp, x, y) + λ(K(cid:48) − K).

(12)

Combining (12) with b(d(cid:48)

p, x, y) < Rc and Proposition 3 gives (11).

We can view Theorem 6 as a generalization of our one-step lookahead bound (Lemma 2),
as (11) is equivalently a bound on K(cid:48) − K, an upper bound on the number of remain-
ing ‘steps’ corresponding to an iterative sequence of single-rule extensions of a preﬁx dp.
Notice that when d = ((), (), q0, 0) is the empty rule list, this bound replicates (9), since
b(dp, x, y) = 0.

3.6 Upper Bounds on the Number of Preﬁx Evaluations

In this section, we use our upper bounds on preﬁx length from §3.5 to derive corresponding
upper bounds on the number of preﬁx evaluations made by Algorithm 1. First, we present
Theorem 7, in which we use information about the state of Algorithm 1’s execution to
calculate, for any given execution state, upper bounds on the number of additional preﬁx
evaluations that might be required for the execution to complete. The relevant execution
state depends on the current best objective Rc and information about preﬁxes we are
planning to evaluate, i.e., preﬁxes in the queue Q of Algorithm 1. We deﬁne the number
of remaining preﬁx evaluations as the number of preﬁxes that are currently in or will be
inserted into the queue.

We use Theorem 7 in some of our empirical results (§6, Figure 18) to help illustrate
the dramatic impact of certain algorithm optimizations. The execution trace of this upper
bound on remaining preﬁx evaluations complements the execution traces of other quanti-
ties, e.g., that of the current best objective Rc. After presenting Theorem 7, we also give
two weaker propositions that provide useful intuition. In particular, Proposition 9 is a prac-
tical approximation to Theorem 7 that is signiﬁcantly easier to compute; we use it in our
implementation as a metric of execution progress that we display to the user.

Theorem 7 (Fine-grained upper bound on remaining preﬁx evaluations) Con-
sider the state space of all rule lists formed from a set of M antecedents, and consider Algo-
rithm 1 at a particular instant during execution. Let Rc be the current best objective, let Q
be the queue, and let L(dp) be the length of preﬁx dp. Deﬁne Γ(Rc, Q) to be the number of
remaining preﬁx evaluations, then

Γ(Rc, Q) ≤

(cid:88)

f (dp)
(cid:88)

dp∈Q

k=0

(M − L(dp))!
(M − L(dp) − k)!

,

(13)

13

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

where

f (dp) = min

(cid:18)(cid:22) Rc − b(dp, x, y)

(cid:23)

(cid:19)

, M − L(dp)

.

λ

Proof The number of remaining preﬁx evaluations is equal to the number of preﬁxes that
are currently in or will be inserted into queue Q. For any such preﬁx dp, Theorem 6 gives
an upper bound on the length of any preﬁx d(cid:48)

p that starts with dp:

(cid:18)

L(d(cid:48)

p) ≤ min

L(dp) +

(cid:22) Rc − b(dp, x, y)
λ

(cid:23)

(cid:19)

, M

≡ U (dp).

This gives an upper bound on the number of remaining preﬁx evaluations:

Γ(Rc, Q) ≤

P (M − L(dp), k) =

(cid:88)

U (dp)−L(dp)
(cid:88)

dp∈Q

k=0

(cid:88)

f (dp)
(cid:88)

dp∈Q

k=0

(M − L(dp))!
(M − L(dp) − k)!

,

where P (m, k) denotes the number of k-permutations of m.

Proposition 8 is strictly weaker than Theorem 7 and is the starting point for its deriva-
tion. It is a na¨ıve upper bound on the total number of preﬁx evaluations over the course
of Algorithm 1’s execution. It only depends on the number of rules and the regularization
parameter λ; i.e., unlike Theorem 7, it does not use algorithm execution state to bound the
size of the search space.

Proposition 8 (Upper bound on the total number of preﬁx evaluations) Deﬁne
Γtot(S) to be the total number of preﬁxes evaluated by Algorithm 1, given the state space of
all rule lists formed from a set S of M rules. For any set S of M rules,

Γtot(S) ≤

K
(cid:88)

k=0

M !
(M − k)!

,

where K = min((cid:98)1/2λ(cid:99), M ).

Proof By Corollary 5, K ≡ min((cid:98)1/2λ(cid:99), M ) gives an upper bound on the length of any
optimal rule list. Since we can think of our problem as ﬁnding the optimal selection and
permutation of k out of M rules, over all k ≤ K,

Γtot(S) ≤ 1 +

P (M, k) =

K
(cid:88)

k=1

K
(cid:88)

k=0

M !
(M − k)!

.

Our next upper bound is strictly tighter than the bound in Proposition 8. Like Theo-
rem 7, it uses the current best objective and information about the lengths of preﬁxes in the

14

Learning Certifiably Optimal Rule Lists

queue to constrain the lengths of preﬁxes in the remaining search space. However, Proposi-
tion 9 is weaker than Theorem 7 because it leverages only coarse-grained information from
the queue. Speciﬁcally, Theorem 7 is strictly tighter because it additionally incorporates
preﬁx-speciﬁc objective lower bound information from preﬁxes in the queue, which further
constrains the lengths of preﬁxes in the remaining search space.

Proposition 9 (Coarse-grained upper bound on remaining preﬁx evaluations)
Consider a state space of all rule lists formed from a set of M antecedents, and consider
Algorithm 1 at a particular instant during execution. Let Rc be the current best objective,
let Q be the queue, and let L(dp) be the length of preﬁx dp. Let Qj be the number of preﬁxes
of length j in Q,

Qj = (cid:12)

(cid:12){dp : L(dp) = j, dp ∈ Q}(cid:12)
(cid:12)

and let J = argmaxdp∈Q L(dp) be the length of the longest preﬁx in Q. Deﬁne Γ(Rc, Q) to
be the number of remaining preﬁx evaluations, then

Γ(Rc, Q) ≤

J
(cid:88)

j=1

Qj

(cid:32)K−j
(cid:88)

k=0

(M − j)!
(M − j − k)!

(cid:33)

,

where K = min((cid:98)Rc/λ(cid:99), M ).

Proof The number of remaining preﬁx evaluations is equal to the number of preﬁxes that
are currently in or will be inserted into queue Q. For any such remaining preﬁx dp, Theorem 4
gives an upper bound on its length; deﬁne K to be this bound: L(dp) ≤ min((cid:98)Rc/λ(cid:99), M ) ≡ K.
For any preﬁx dp in queue Q with length L(dp) = j, the maximum number of preﬁxes that
start with dp and remain to be evaluated is:

K−j
(cid:88)

k=0

P (M − j, k) =

K−j
(cid:88)

k=0

(M − j)!
(M − j − k)!

,

where P (T, k) denotes the number of k-permutations of T . This gives an upper bound on
the number of remaining preﬁx evaluations:

Γ(Rc, Q) ≤

J
(cid:88)

j=0

Qj

(cid:32)K−j
(cid:88)

k=0

P (M − j, k)

=

Qj

(cid:33)

J
(cid:88)

j=0

(cid:32)K−j
(cid:88)

k=0

(M − j)!
(M − j − k)!

(cid:33)

.

3.7 Lower Bounds on Antecedent Support

In this section, we give two lower bounds on the normalized support of each antecedent in
any optimal rule list; both are related to the regularization parameter λ.

15

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Theorem 10 (Lower bound on antecedent support) Let d∗ = (dp, δp, q0, K) be any
optimal rule list with objective R∗, i.e., d∗ ∈ argmind R(d, x, y). For each antecedent pk
in preﬁx dp = (p1, . . . , pK), the regularization parameter λ provides a lower bound on the
normalized support of pk,

λ ≤ supp(pk, x | dp).

(14)

Proof Let d∗ = (dp, δp, q0, K) be an optimal rule list with preﬁx dp = (p1, . . . , pK) and
0, K − 1) derived from d∗ by
labels δp = (q1, . . . , qK). Consider the rule list d = (d(cid:48)
deleting a rule pi → qi, therefore d(cid:48)
p = (q1, . . . , qi−1,
i+1, . . . , q(cid:48)
q(cid:48)

k need not be the same as qk, for k > i and k = 0.
The largest possible discrepancy between d∗ and d would occur if d∗ correctly classiﬁed

p, δ(cid:48)
p = (p1, . . . , pi−1, pi+1, . . . , pK) and δ(cid:48)

K), where q(cid:48)

p, q(cid:48)

all the data captured by pi, while d misclassiﬁed these data. This gives an upper bound:
R(d, x, y) = (cid:96)(d, x, y) + λ(K − 1) ≤ (cid:96)(d∗, x, y) + supp(pi, x | dp) + λ(K − 1)

= R(d∗, x, y) + supp(pi, x | dp) − λ
= R∗ + supp(pi, x | dp) − λ

(15)

where supp(pi, x | dp) is the normalized support of pi in the context of dp, deﬁned in (3),
and the regularization ‘bonus’ comes from the fact that d is one rule shorter than d∗.

At the same time, we must have R∗ ≤ R(d, x, y) for d∗ to be optimal. Combining this
with (15) and rearranging gives (14), therefore the regularization parameter λ provides a
lower bound on the support of an antecedent pi in an optimal rule list d∗.

Thus, we can prune a preﬁx dp if any of its antecedents captures less than a fraction λ
of data, even if b(dp, x, y) < R∗. Notice that the bound in Theorem 10 depends on the
antecedents, but not the label predictions, and thus does not account for misclassiﬁcation
error. Theorem 11 gives a tighter bound by leveraging this additional information, which
speciﬁcally tightens the upper bound on R(d, x, y) in (15).

Theorem 11 (Lower bound on accurate antecedent support) Let d∗ be any opti-
mal rule list with objective R∗, i.e., d∗ = (dp, δp, q0, K) ∈ argmind R(d, x, y). Let d∗ have
preﬁx dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK). For each rule pk → qk in d∗, deﬁne ak
to be the fraction of data that are captured by pk and correctly classiﬁed:

ak ≡

cap(xn, pk | dp) ∧ 1[qk = yn].

1
N

N
(cid:88)

n=1

The regularization parameter λ provides a lower bound on ak:

(16)

(17)

0, K − 1) be the rule list derived from d∗ by
Proof As in Theorem 10, let d = (d(cid:48)
deleting a rule pi → qi. Now, let us deﬁne (cid:96)i to be the portion of R∗ due to this rule’s
misclassiﬁcation error,

p, δ(cid:48)

p, q(cid:48)

(cid:96)i ≡

cap(xn, pi | dp) ∧ 1[qi (cid:54)= yn].

1
N

N
(cid:88)

n=1

λ ≤ ak.

16

Learning Certifiably Optimal Rule Lists

The largest discrepancy between d∗ and d would occur if d misclassiﬁed all the data captured
by pi. This gives an upper bound on the diﬀerence between the misclassiﬁcation error of d
and d∗:

(cid:96)(d, x, y) − (cid:96)(d∗, x, y) ≤ supp(pi, x | dp) − (cid:96)i

=

=

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

cap(xn, pi | dp) −

cap(xn, pi | dp) ∧ 1[qi (cid:54)= yn]

1
N

N
(cid:88)

n=1

cap(xn, pi | dp) ∧ 1[qi = yn] = ai,

where we deﬁned ai in (16). Relating this bound to the objectives of d and d∗ gives

R(d, x, y) = (cid:96)(d, x, y) + λ(K − 1) ≤ (cid:96)(d∗, x, y) + ai + λ(K − 1)

= R(d∗, x, y) + ai − λ
= R∗ + ai − λ.

(18)

Combining (18) with the requirement R∗ ≤ R(d, x, y) gives the bound λ ≤ ai.

Thus, we can prune a preﬁx if any of its rules correctly classiﬁes less than a fraction λ
of data. While the lower bound in Theorem 10 is a sub-condition of the lower bound in
Theorem 11, we can still leverage both—since the sub-condition is easier to check, check-
ing it ﬁrst can accelerate pruning. In addition to applying Theorem 10 in the context of
constructing rule lists, we can furthermore apply it in the context of rule mining (§3.1).
Speciﬁcally, it implies that we should only mine rules with normalized support of at least λ;
we need not mine rules with a smaller fraction of observations.2 In contrast, we can only
apply Theorem 11 in the context of constructing rule lists; it depends on the misclassiﬁ-
cation error associated with each rule in a rule list, thus it provides a lower bound on the
number of observations that each such rule must correctly classify.

3.8 Upper Bound on Antecedent Support

In the previous section (§3.7), we proved lower bounds on antecedent support; in Ap-
pendix A, we give an upper bound on antecedent support. Speciﬁcally, Theorem 21 shows
that an antecedent’s support in a rule list cannot be too similar to the set of data not
captured by preceding antecedents in the rule list. In particular, Theorem 21 implies that
we should only mine rules with normalized support less than or equal to 1 − λ; we need
not mine rules with a larger fraction of observations. Note that we do not otherwise use
this bound in our implementation, because we did not observe a meaningful beneﬁt in
preliminary experiments.

3.9 Antecedent Rejection and its Propagation

In this section, we demonstrate further consequences of our lower (§3.7) and upper bounds
(§3.8) on antecedent support, under a uniﬁed framework we refer to as antecedent rejec-
tion. Let dp = (p1, . . . , pK) be a preﬁx, and let pk be an antecedent in dp. Deﬁne pk to have

2. We describe our application of this idea in Appendix E, where we provide details on data processing.

17

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

insuﬃcient support in dp if it does not obey the bound in (14) of Theorem 10. Deﬁne pk
to have insuﬃcient accurate support in dp if it does not obey the bound in (17) of Theo-
rem 11. Deﬁne pk to have excessive support in dp if it does not obey the bound in (37) of
Theorem 21 (Appendix A). If pk in the context of dp has insuﬃcient support, insuﬃcient
accurate support, or excessive support, let us say that preﬁx dp rejects antecedent pK. Next,
in Theorem 12, we describe large classes of related rule lists whose preﬁxes all reject the
same antecedent.

Theorem 12 (Antecedent rejection propagates) For any preﬁx dp = (p1, . . . , pK), let
φ(dp) denote the set of all preﬁxes d(cid:48)
p such that the set of all antecedents in dp is a subset
of the set of all antecedents in d(cid:48)
p, i.e.,

φ(dp) = {d(cid:48)

p = (p(cid:48)

1, . . . , p(cid:48)

K(cid:48)) s.t. {pk : pk ∈ dp} ⊆ {p(cid:48)

κ : p(cid:48)

κ ∈ d(cid:48)

p}, K(cid:48) ≥ K}.

(19)

Let d = (dp, δp, q0, K) be a rule list with preﬁx dp = (p1, . . . , pK−1, pK), such that dp rejects
its last antecedent pK, either because pK in the context of dp has insuﬃcient support, insuf-
ﬁcient accurate support, or excessive support. Let dK−1
= (p1, . . . , pK−1) be the ﬁrst K − 1
antecedents of dp. Let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp = (P1, . . . , PK(cid:48)−1,
PK(cid:48), . . . , Pκ) such that Dp starts with DK(cid:48)−1
) and antecedent
PK(cid:48) = pK. It follows that preﬁx Dp rejects PK(cid:48) for the same reason that dp rejects pK, and
furthermore, D cannot be optimal, i.e., D /∈ argmind† R(d†, x, y).

= (P1, . . . , PK(cid:48)−1) ∈ φ(dK−1

p

p

p

Proof Combine Proposition 13, Proposition 14, and Proposition 22. The ﬁrst two are
found below, and the last in Appendix A.

Theorem 12 implies potentially signiﬁcant computational savings. We know from Theo-
rems 10, 11, and 21 that during branch-and-bound execution, if we ever encounter a preﬁx
dp = (p1, . . . , pK−1, pK) that rejects its last antecedent pK, then we can prune dp. By The-
orem 12, we can also prune any preﬁx d(cid:48)
p whose antecedents contains the set of antecedents
in dp, in almost any order, with the constraint that all antecedents in {p1, . . . , pK−1} pre-
cede pK. These latter antecedents are also rejected directly by the bounds in Theorems 10,
11, and 21; this is how our implementation works in practice. In a preliminary implemen-
tation (not shown), we maintained additional data structures to support the direct use of
Theorem 12. We leave the design of eﬃcient data structures for this task as future work.

Proposition 13 (Insuﬃcient antecedent support propagates) First deﬁne φ(dp) as
in (19), and let dp = (p1, . . . , pK−1, pK) be a preﬁx, such that its last antecedent pK has
insuﬃcient support, i.e., the opposite of the bound in (14): supp(pK, x | dp) < λ. Let dK−1
=
(p1, . . . , pK−1), and let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp = (P1, . . . , PK(cid:48)−1,
PK(cid:48), . . . , Pκ), such that Dp starts with DK(cid:48)−1
) and PK(cid:48) = pK.
It follows that PK(cid:48) has insuﬃcient support in preﬁx Dp, and furthermore, D cannot be
optimal, i.e., D /∈ argmind R(d, x, y).

= (P1, . . . , PK(cid:48)−1) ∈ φ(dK−1

p

p

p

18

Learning Certifiably Optimal Rule Lists

Proof The support of pK in dp depends only on the set of antecedents in dK

p = (p1, . . . , pK):

supp(pK, x | dp) =

cap(xn, pK | dp) =

(cid:0)¬ cap(xn, dK−1

)(cid:1) ∧ cap(xn, pK)

p

N
(cid:88)

n=1
N
(cid:88)

1
N

1
N

=

(cid:32)K−1
(cid:94)

n=1

k=1

(cid:33)

¬ cap(xn, pk)

∧ cap(xn, pK) < λ,

and the support of PK(cid:48) in Dp depends only on the set of antecedents in DK(cid:48)

p = (P1, . . . , PK(cid:48)):

supp(PK(cid:48), x | Dp) =

cap(xn, PK(cid:48) | Dp) =

¬ cap(xn, Pk)

∧ cap(xn, PK(cid:48))

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

1
N

1
N

1
N

N
(cid:88)

(cid:32)K(cid:48)−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

n=1

k=1

≤

=

(cid:33)

(cid:33)

(cid:33)

¬ cap(xn, pk)

∧ cap(xn, PK(cid:48))

¬ cap(xn, pk)

∧ cap(xn, pK)

= supp(pK, x | dp) < λ.

(20)

The ﬁrst inequality reﬂects the condition that DK(cid:48)−1
set of antecedents in DK(cid:48)−1
reﬂects the fact that PK(cid:48) = pK. Thus, P (cid:48)
by Theorem 10, D cannot be optimal, i.e., D /∈ argmind R(d, x, y).

), which implies that the
, and the next equality
K has insuﬃcient support in preﬁx Dp, therefore

contains the set of antecedents in dK−1

∈ φ(dK−1
p

p

p

p

Proposition 14 (Insuﬃcient accurate antecedent support propagates) Let φ(dp)
denote the set of all preﬁxes d(cid:48)
p such that the set of all antecedents in dp is a subset of
the set of all antecedents in d(cid:48)
p, as in (19). Let d = (dp, δp, q0, K) be a rule list with preﬁx
dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK), such that the last antecedent pK has insuﬃ-
cient accurate support, i.e., the opposite of the bound in (17):

1
N

N
(cid:88)

n=1

cap(xn, pK | dp) ∧ 1[qK = yn] < λ.

Let dK−1
p
(P1, . . . , Pκ) and labels ∆p = (Q1, . . . , Qκ), such that Dp starts with DK(cid:48)−1
∈ φ(dK−1
p
and furthermore, D /∈ argmind† R(d†, x, y).

= (p1, . . . , pK−1) and let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp =
= (P1, . . . , PK(cid:48)−1)
) and PK(cid:48) = pK. It follows that PK(cid:48) has insuﬃcient accurate support in preﬁx Dp,

p

19

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Proof The accurate support of PK(cid:48) in Dp is insuﬃcient:

1
N

N
(cid:88)

n=1

cap(xn, PK(cid:48) | Dp) ∧ 1[QK(cid:48) = yn]

=

≤

=

=

≤

1
N

1
N

1
N

1
N

1
N

N
(cid:88)

(cid:32)K(cid:48)−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

k=1

n=1

N
(cid:88)

n=1
N
(cid:88)

n=1

(cid:33)

(cid:33)

(cid:33)

¬ cap(xn, Pk)

∧ cap(xn, PK(cid:48)) ∧ 1[QK(cid:48) = yn]

¬ cap(xn, pk)

∧ cap(xn, PK(cid:48)) ∧ 1[QK(cid:48) = yn]

¬ cap(xn, pk)

∧ cap(xn, pK) ∧ 1[QK(cid:48) = yn]

cap(xn, pK | dp) ∧ 1[QK(cid:48) = yn]

cap(xn, pK | dp) ∧ 1[qK = yn] < λ.

The ﬁrst inequality reﬂects the condition that DK(cid:48)−1
), the next equality reﬂects
the fact that PK(cid:48) = pK. For the following equality, notice that QK(cid:48) is the majority class
label of data captured by PK(cid:48) in Dp, and qK is the majority class label of data captured
by PK in dp, and recall from (20) that supp(PK(cid:48), x | Dp) ≤ supp(pK, x | dp). By Theorem 11,
D /∈ argmind† R(d†, x, y).

∈ φ(dK−1
p

p

Propositions 13 and 14, combined with Proposition 22 (Appendix A), constitute the

proof of Theorem 12.

3.10 Equivalent Support Bound

If two preﬁxes capture the same data, and one is more accurate than the other, then there
is no beneﬁt to considering preﬁxes that start with the less accurate one. Let dp be a
preﬁx, and consider the best possible rule list whose preﬁx starts with dp. If we take its
antecedents in dp and replace them with another preﬁx with the same support (that could
include diﬀerent antecedents), then its objective can only become worse or remain the same.
Formally, let Dp be a preﬁx, and let ξ(Dp) be the set of all preﬁxes that capture exactly
the same data as Dp. Now, let d be a rule list with preﬁx dp in ξ(Dp), such that d has
the minimum objective over all rule lists with preﬁxes in ξ(Dp). Finally, let d(cid:48) be a rule
list whose preﬁx d(cid:48)
p starts with dp, such that d(cid:48) has the minimum objective over all rule
lists whose preﬁxes start with dp. Theorem 15 below implies that d(cid:48) also has the minimum
objective over all rule lists whose preﬁxes start with any preﬁx in ξ(Dp).

Theorem 15 (Equivalent support bound) Deﬁne σ(dp) to be the set of all rule lists
whose preﬁxes start with dp, as in (1). Let d = (dp, δp, q0, K) be a rule list with preﬁx
dp = (p1, . . . , pK), and let D = (Dp, ∆p, Q0, κ) be a rule list with preﬁx Dp = (P1, . . . , Pκ),

20

Learning Certifiably Optimal Rule Lists

such that dp and Dp capture the same data, i.e.,

{xn : cap(xn, dp)} = {xn : cap(xn, Dp)}.

If the objective lower bounds of d and D obey b(dp, x, y) ≤ b(Dp, x, y), then the objective of
the optimal rule list in σ(dp) gives a lower bound on the objective of the optimal rule list
in σ(Dp):

min
d(cid:48)∈σ(dp)

R(d(cid:48), x, y) ≤ min

R(D(cid:48), x, y).

D(cid:48)∈σ(Dp)

(21)

Proof See Appendix B for the proof of Theorem 15.

Thus, if preﬁxes dp and Dp capture the same data, and their objective lower bounds obey
b(dp, x, y) ≤ b(Dp, x, y), Theorem 15 implies that we can prune Dp. Next, in Sections 3.11
and 3.12, we highlight and analyze the special case of preﬁxes that capture the same data
because they contain the same antecedents.

3.11 Permutation Bound

If two preﬁxes are composed of the same antecedents, i.e., they contain the same antecedents
up to a permutation, then they capture the same data, and thus Theorem 15 applies.
Therefore, if one is more accurate than the other, then there is no beneﬁt to considering
preﬁxes that start with the less accurate one. Let dp be a preﬁx, and consider the best
possible rule list whose preﬁx starts with dp. If we permute its antecedents in dp, then its
objective can only become worse or remain the same.

Formally, let P = {pk}K

k=1 be a set of K antecedents, and let Π be the set of all K-preﬁxes
corresponding to permutations of antecedents in P . Let preﬁx dp in Π have the minimum
preﬁx misclassiﬁcation error over all preﬁxes in Π. Also, let d(cid:48) be a rule list whose preﬁx d(cid:48)
p
starts with dp, such that d(cid:48) has the minimum objective over all rule lists whose preﬁxes start
with dp. Corollary 16 below, which can be viewed as special case of Theorem 15, implies
that d(cid:48) also has the minimum objective over all rule lists whose preﬁxes start with any
preﬁx in Π.

p, q(cid:48)

p, δ(cid:48)

0, K(cid:48)) : d(cid:48)

Corollary 16 (Permutation bound) Let π be any permutation of {1, . . . , K}, and de-
ﬁne σ(dp) = {(d(cid:48)
p starts with dp} to be the set of all rule lists whose preﬁxes
start with dp. Let d = (dp, δp, q0, K) and D = (Dp, ∆p, Q0, K) denote rule lists with preﬁxes
dp = (p1, . . . , pK) and Dp = (pπ(1), . . . , pπ(K)), respectively, i.e., the antecedents in Dp cor-
respond to a permutation of the antecedents in dp. If the objective lower bounds of d and D
obey b(dp, x, y) ≤ b(Dp, x, y), then the objective of the optimal rule list in σ(dp) gives a
lower bound on the objective of the optimal rule list in σ(Dp):

min
d(cid:48)∈σ(dp)

R(d(cid:48), x, y) ≤ min

R(D(cid:48), x, y).

D(cid:48)∈σ(Dp)

Proof Since preﬁxes dp and Dp contain the same antecedents, they both capture the same
data. Thus, we can apply Theorem 15.

21

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Thus if preﬁxes dp and Dp have the same antecedents, up to a permutation, and their
objective lower bounds obey b(dp, x, y) ≤ b(Dp, x, y), Corollary 16 implies that we can
prune Dp. We call this symmetry-aware pruning, and we illustrate the subsequent compu-
tational savings next in §3.12.

3.12 Upper Bound on Preﬁx Evaluations with Symmetry-aware Pruning

Here, we present an upper bound on the total number of preﬁx evaluations that accounts for
the eﬀect of symmetry-aware pruning (§3.11). Since every subset of K antecedents generates
an equivalence class of K! preﬁxes equivalent up to permutation, symmetry-aware pruning
dramatically reduces the search space.

First, notice that Algorithm 1 describes a breadth-ﬁrst exploration of the state space of
rule lists. Now suppose we integrate symmetry-aware pruning into our execution of branch-
and-bound, so that after evaluating preﬁxes of length K, we only keep a single best preﬁx
from each set of preﬁxes equivalent up to a permutation.

Theorem 17 (Upper bound on preﬁx evaluations with symmetry-aware pruning)
Consider a state space of all rule lists formed from a set S of M antecedents, and consider
the branch-and-bound algorithm with symmetry-aware pruning. Deﬁne Γtot(S) to be the total
number of preﬁxes evaluated. For any set S of M rules,

Γtot(S) ≤ 1 +

K
(cid:88)

k=1

1
(k − 1)!

·

M !
(M − k)!

,

where K = min((cid:98)1/2λ(cid:99), M ).

Proof By Corollary 5, K ≡ min((cid:98)1/2λ(cid:99), M ) gives an upper bound on the length of any op-
timal rule list. The algorithm begins by evaluating the empty preﬁx, followed by M preﬁxes
of length k = 1, then P (M, 2) preﬁxes of length k = 2, where P (M, 2) is the number of size-2
subsets of {1, . . . , M }. Before proceeding to length k = 3, we keep only C(M, 2) preﬁxes of
length k = 2, where C(M, k) denotes the number of k-combinations of M . Now, the number
of length k = 3 preﬁxes we evaluate is C(M, 2)(M − 2). Propagating this forward gives

Γtot(S) ≤ 1 +

C(M, k − 1)(M − k + 1) = 1 +

K
(cid:88)

k=1

K
(cid:88)

k=1

1
(k − 1)!

·

M !
(M − k)!

.

Pruning based on permutation symmetries thus yields signiﬁcant computational savings.
Let us compare, for example, to the na¨ıve number of preﬁx evaluations given by the upper
bound in Proposition 8. If M = 100 and K = 5, then the na¨ıve number is about 9.1 × 109,
while the reduced number due to symmetry-aware pruning is about 3.9 × 108, which is
smaller by a factor of about 23. If M = 1000 and K = 10, the number of evaluations falls
from about 9.6 × 1029 to about 2.7 × 1024, which is smaller by a factor of about 360,000.

While 1024 seems infeasibly enormous, it does not represent the number of rule lists we
evaluate. As we show in our experiments (§6), our permutation bound in Corollary 16 and

22

Learning Certifiably Optimal Rule Lists

our other bounds together conspire to reduce the search space to a size manageable on a
single computer. The choice of M = 1000 and K = 10 in our example above corresponds to
the state space size our eﬀorts target. K = 10 rules represents a (heuristic) upper limit on
the size of an interpretable rule list, and M = 1000 represents the approximate number of
rules with suﬃciently high support (Theorem 10) we expect to obtain via rule mining (§3.1).

3.13 Similar Support Bound

We now present a relaxation of Theorem 15, our equivalent support bound. Theorem 18
implies that if we know that no extensions of a preﬁx dp are better than the current best
objective, then we can prune all preﬁxes with support similar to dp’s support. Understanding
how to exploit this result in practice represents an exciting direction for future work; our
implementation (§5) does not currently leverage the bound in Theorem 18.

Theorem 18 (Similar support bound) Deﬁne σ(dp) to be the set of all rule lists whose
preﬁxes start with dp, as in (1). Let dp = (p1, . . . , pK) and Dp = (P1, . . . , Pκ) be preﬁxes
that capture nearly the same data. Speciﬁcally, deﬁne ω to be the normalized support of data
captured by dp and not captured by Dp, i.e.,

ω ≡

¬ cap(xn, Dp) ∧ cap(xn, dp).

Similarly, deﬁne Ω to be the normalized support of data captured by Dp and not captured
by dp, i.e.,

Ω ≡

¬ cap(xn, dp) ∧ cap(xn, Dp).

We can bound the diﬀerence between the objectives of the optimal rule lists in σ(dp) and
σ(Dp) as follows:

min
D†∈σ(Dp)

d†∈σ(dp)

R(D†, x, y) − min

R(d†, x, y) ≥ b(Dp, x, y) − b(dp, x, y) − ω − Ω,

(24)

where b(dp, x, y) and b(Dp, x, y) are the objective lower bounds of d and D, respectively.

Proof See Appendix C for the proof of Theorem 18.

Theorem 18 implies that if preﬁxes dp and Dp are similar, and we know the optimal

objective of rule lists starting with dp, then

min
D(cid:48)∈σ(Dp)

d(cid:48)∈σ(dp)

R(D(cid:48), x, y) ≥ min

R(d(cid:48), x, y) + b(Dp, x, y) − b(dp, x, y) − χ

≥ Rc + b(Dp, x, y) − b(dp, x, y) − χ,

where Rc is the current best objective, and χ is the normalized support of the set of data
captured either exclusively by dp or exclusively by Dp. It follows that

min
D(cid:48)∈σ(Dp)

R(D(cid:48), x, y) ≥ Rc + b(Dp, x, y) − b(dp, x, y) − χ ≥ Rc

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

(22)

(23)

23

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if b(Dp, x, y) − b(dp, x, y) ≥ χ. To conclude, we summarize this result and combine it with
our notion of lookahead from Lemma 2. During branch-and-bound execution, if we demon-
strate that mind(cid:48)∈σ(dp) R(d(cid:48), x, y) ≥ Rc, then we can prune all preﬁxes that start with any
preﬁx D(cid:48)

p in the following set:

(cid:40)

(cid:41)

D(cid:48)

p : b(D(cid:48)

p, x, y) + λ − b(dp, x, y) ≥

cap(xn, dp) ⊕ cap(xn, D(cid:48)
p)

,

1
N

N
(cid:88)

n=1

where the symbol ⊕ denotes the logical operation, exclusive or (XOR).

3.14 Equivalent Points Bound

The bounds in this section quantify the following: If multiple observations that are not
captured by a preﬁx dp have identical features and opposite labels, then no rule list that
starts with dp can correctly classify all these observations. For each set of such observations,
the number of mistakes is at least the number of observations with the minority label within
the set.

Consider a data set {(xn, yn)}N

m=1. Deﬁne dis-
tinct observations to be equivalent if they are captured by exactly the same antecedents,
i.e., xi (cid:54)= xj are equivalent if

n=1 and also a set of antecedents {sm}M

1
M

M
(cid:88)

m=1

1[cap(xi, sm) = cap(xj, sm)] = 1.

Notice that we can partition a data set into sets of equivalent points; let {eu}U
u=1 enumerate
these sets. Let eu be the equivalent points set that contains observation xi. Now deﬁne θ(eu)
to be the normalized support of the minority class label with respect to set eu, e.g., let

eu = {xn : ∀m ∈ [M ], 1[cap(xn, sm) = cap(xi, sm)]},

and let qu be the minority class label among points in eu, then

θ(eu) =

1[xn ∈ eu] 1[yn = qu].

(25)

1
N

N
(cid:88)

n=1

The existence of equivalent points sets with non-singleton support yields a tighter ob-
jective lower bound that we can combine with our other bounds; as our experiments demon-
strate (§6), the practical consequences can be dramatic. First, for intuition, we present a
general bound in Proposition 19; next, we explicitly integrate this bound into our framework
in Theorem 20.

Proposition 19 (General equivalent points bound) Let d = (dp, δp, q0, K) be a rule list,
then

R(d, x, y) ≥

θ(eu) + λK.

U
(cid:88)

u=1

24

Learning Certifiably Optimal Rule Lists

Proof Recall that the objective is R(d, x, y) = (cid:96)(d, x, y) + λK, where the misclassiﬁcation
error (cid:96)(d, x, y) is given by

(cid:96)(d, x, y) = (cid:96)0(dp, q0, x, y) + (cid:96)p(dp, δp, x, y)

=

1
N

(cid:32)

N
(cid:88)

n=1

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn] +

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn]

.

(cid:33)

K
(cid:88)

k=1

Any particular rule list uses a speciﬁc rule, and therefore a single class label, to classify
all points within a set of equivalent points. Thus, for a set of equivalent points u, the rule
list d correctly classiﬁes either points that have the majority class label, or points that have
the minority class label. It follows that d misclassiﬁes a number of points in u at least as
great as the number of points with the minority class label. To translate this into a lower
bound on (cid:96)(d, x, y), we ﬁrst sum over all sets of equivalent points, and then for each such
set, count diﬀerences between class labels and the minority class label of the set, instead of
counting mistakes:

(cid:96)(d, x, y)

=

≥

1
N

1
N

U
(cid:88)

N
(cid:88)

(cid:32)

u=1

n=1

U
(cid:88)

N
(cid:88)

(cid:32)

u=1

n=1

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn] +

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn]

1[xn ∈ eu]

¬ cap(xn, dp) ∧ 1[yn = qu] +

cap(xn, pk | dp) ∧ 1[yn = qu]

1[xn ∈ eu].

(cid:33)

(cid:33)

(26)

Next, we factor out the indicator for equivalent point set membership, which yields a term
that sums to one, because every datum is either captured or not captured by preﬁx dp.

(cid:96)(d, x, y) =

¬ cap(xn, dp) +

cap(xn, pk | dp)

∧ 1[xn ∈ eu] 1[yn = qu]

(cid:33)

U
(cid:88)

N
(cid:88)

(cid:32)

1
N

1
N

1
N

=

=

u=1

n=1

U
(cid:88)

N
(cid:88)

u=1
U
(cid:88)

n=1
N
(cid:88)

u=1

n=1

(¬ cap(xn, dp) + cap(xn, dp)) ∧ 1[xn ∈ eu] 1[yn = qu]

1[xn ∈ eu] 1[yn = qu] =

θ(eu),

U
(cid:88)

u=1

where the ﬁnal equality applies the deﬁnition of θ(eu) in (25). Therefore, R(d, x, y) =
(cid:96)(d, x, y) + λK ≥ (cid:80)U

u=1 θ(eu) + λK.

Now, recall that to obtain our lower bound b(dp, x, y) in (5), we simply deleted the
default rule misclassiﬁcation error (cid:96)0(dp, q0, x, y) from the objective R(d, x, y). Theorem 20
obtains a tighter objective lower bound via a tighter lower bound on the default rule mis-
classiﬁcation error, 0 ≤ b0(dp, x, y) ≤ (cid:96)0(dp, q0, x, y).

K
(cid:88)

k=1
K
(cid:88)

k=1

K
(cid:88)

k=1

25

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Theorem 20 (Equivalent points bound) Let d be a rule list with preﬁx dp and lower
bound b(dp, x, y), then for any rule list d(cid:48) ∈ σ(d) whose preﬁx d(cid:48)

p starts with dp,

R(d(cid:48), x, y) ≥ b(dp, x, y) + b0(dp, x, y),

(27)

where

b0(dp, x, y) =

¬ cap(xn, dp) ∧ 1[xn ∈ eu] 1[yn = qu].

(28)

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

Proof See Appendix D for the proof of Theorem 20.

4. Incremental Computation

For every preﬁx dp evaluated during Algorithm 1’s execution, we compute the objective
lower bound b(dp, x, y) and sometimes the objective R(d, x, y) of the corresponding rule
list d. These calculations are the dominant computations with respect to execution time.
This motivates our use of a highly optimized library, designed by Yang et al. (2017), for
representing rule lists and performing operations encountered in evaluating functions of rule
lists. Furthermore, we exploit the hierarchical nature of the objective function and its lower
bound to compute these quantities incrementally throughout branch-and-bound execution.
In this section, we provide explicit expressions for the incremental computations that are
central to our approach. Later, in §5, we describe a cache data structure for supporting our
incremental framework in practice.

For completeness, before presenting our incremental expressions, let us begin by writing
down the objective lower bound and objective of the empty rule list, d = ((), (), q0, 0), the
ﬁrst rule list evaluated in Algorithm 1. Since its preﬁx contains zero rules, it has zero preﬁx
misclassiﬁcation error and also has length zero. Thus, the empty rule list’s objective lower
bound is zero, i.e., b((), x, y) = 0. Since none of the data are captured by the empty preﬁx,
the default rule corresponds to the majority class, and the objective corresponds to the
default rule misclassiﬁcation error, i.e., R(d, x, y) = (cid:96)0((), q0, x, y).

Now, we derive our incremental expressions for the objective function and its lower
p, q(cid:48)
bound. Let d = (dp, δp, q0, K) and d(cid:48) = (d(cid:48)
0, K + 1) be rule lists such that preﬁx dp =
(p1, . . . , pK) is the parent of d(cid:48)
p = (q1, . . . ,
qK, qK+1) be the corresponding labels. The hierarchical structure of Algorithm 1 enforces
that if we ever evaluate d(cid:48), then we will have already evaluated both the objective and ob-
jective lower bound of its parent, d. We would like to reuse as much of these computations as
possible in our evaluation of d(cid:48). We can write the objective lower bound of d(cid:48) incrementally,

p = (p1, . . . , pK, pK+1). Let δp = (q1, . . . , qK) and δ(cid:48)

p, δ(cid:48)

26

Learning Certifiably Optimal Rule Lists

with respect to the objective lower bound of d:

b(d(cid:48)

p, δ(cid:48)
p, x, y) = (cid:96)p(d(cid:48)
N
(cid:88)

p, x, y) + λ(K + 1)
K+1
(cid:88)

cap(xn, pk | d(cid:48)

=

1
N

n=1

k=1

p) ∧ 1[qk (cid:54)= yn] + λ(K + 1)

(29)

= (cid:96)p(dp, δp, x, y) + λK + λ +

cap(xn, pK+1 | d(cid:48)

p) ∧ 1[qK+1 (cid:54)= yn]

1
N

N
(cid:88)

n=1

= b(dp, x, y) + λ +

cap(xn, pK+1 | d(cid:48)

p) ∧ 1[qK+1 (cid:54)= yn]

= b(dp, x, y) + λ +

¬ cap(xn, dp) ∧ cap(xn, pK+1) ∧ 1[qK+1 (cid:54)= yn].

(30)

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

Thus, if we store b(dp, x, y), then we can reuse this quantity when computing b(d(cid:48)
p, x, y).
Transforming (29) into (30) yields a signiﬁcantly simpler expression that is a function of
the stored quantity b(dp, x, y). For the objective of d(cid:48), ﬁrst let us write a na¨ıve expression:

R(d(cid:48), x, y) = (cid:96)(d(cid:48), x, y) + λ(K + 1) = (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + (cid:96)0(d(cid:48)

p, q(cid:48)

0, x, y) + λ(K + 1)

=

1
N

N
(cid:88)

K+1
(cid:88)

n=1

k=1

1
N

N
(cid:88)

n=1

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] +

¬ cap(xn, d(cid:48)

p) ∧ 1[q(cid:48)

0 (cid:54)= yn] + λ(K + 1).

(31)

Instead, we can compute the objective of d(cid:48) incrementally with respect to its objective lower
bound:

R(d(cid:48), x, y) = (cid:96)p(d(cid:48)
= b(d(cid:48)

p, x, y) + (cid:96)0(d(cid:48)
p, δ(cid:48)
p, q(cid:48)
p, x, y) + (cid:96)0(d(cid:48)
N
(cid:88)

p, q(cid:48)
0, x, y)

0, x, y) + λ(K + 1)

= b(d(cid:48)

p, x, y) +

¬ cap(xn, d(cid:48)

p) ∧ 1[q(cid:48)

0 (cid:54)= yn]

1
N

1
N

n=1
N
(cid:88)

n=1

= b(d(cid:48)

p, x, y) +

¬ cap(xn, dp) ∧ (¬ cap(xn, pK+1)) ∧ 1[q(cid:48)

0 (cid:54)= yn].

(32)

The expression in (32) is simpler to compute than that in (31), because the former reuses
p, x, y), which we already computed in (30). Note that instead of computing R(d(cid:48), x, y)
b(d(cid:48)
incrementally from b(d(cid:48)
p, x, y) as in (32), we could have computed it incrementally from
R(d, x, y). However, doing so would in practice require that we store R(d, x, y) in addition
to b(dp, x, y), which we already must store to support (30). We prefer the incremental
approach suggested by (32) since it avoids this additional storage overhead.

We present an incremental branch-and-bound procedure in Algorithm 2, and show the
incremental computations of the objective lower bound (30) and objective (32) as two
separate functions in Algorithms 3 and 4, respectively. In Algorithm 2, we use a cache to

27

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Algorithm 2 Incremental branch-and-bound for learning rule lists, for simplicity, from a
cold start. We explicitly show the incremental objective lower bound and objective functions
in Algorithms 3 and 4, respectively.

Input: Objective function R(d, x, y), objective lower bound b(dp, x, y), set of antecedents
S = {sm}M
Output: Provably optimal rule list d∗ with minimum objective R∗

m=1, training data (x, y) = {(xn, yn)}N

n=1, regularization parameter λ

dc ← ((), (), q0, 0)
(cid:46) Initialize current best rule list with empty rule list
Rc ← R(dc, x, y)
(cid:46) Initialize current best objective
(cid:46) Initialize queue with empty preﬁx
Q ← queue( [ ( ) ] )
C ← cache( [ ( ( ) , 0 ) ] ) (cid:46) Initialize cache with empty preﬁx and its objective lower bound
(cid:46) Optimization complete when the queue is empty
while Q not empty do
(cid:46) Remove a length-K preﬁx dp from the queue
dp ← Q.pop( )
(cid:46) Look up dp’s lower bound in the cache
b(dp, x, y) ← C.ﬁnd(dp)
(cid:46) Bit vector indicating data not captured by dp
u ← ¬ cap(x, dp)
(cid:46) Evaluate all of dp’s children
for s in S do

if s not in dp then
Dp ← (dp, s)
v ← u ∧ cap(x, s)
b(Dp, x, y) ← b(dp, x, y) + λ + IncrementalLowerBound(v, y, N )
if b(Dp, x, y) < Rc then

(cid:46) Branch: Generate child Dp
(cid:46) Bit vector indicating data captured by s in Dp

(cid:46) Bound: Apply bound from Theorem 1

R(D, x, y) ← b(Dp, x, y) + IncrementalObjective(u, v, y, N )
D ← (Dp, ∆p, Q0, K + 1)
if R(D, x, y) < Rc then

(cid:46) ∆p, Q0 are set in the incremental functions

(dc, Rc) ← (D, R(D, x, y)) (cid:46) Update current best rule list and objective

end if
Q.push(Dp)
C.insert(Dp, b(Dp, x, y))

(cid:46) Add Dp to the queue
(cid:46) Add Dp and its lower bound to the cache

end if

end if

end for
end while
(d∗, R∗) ← (dc, Rc)

(cid:46) Identify provably optimal rule list and objective

store preﬁxes and their objective lower bounds. Algorithm 2 additionally reorganizes the
structure of Algorithm 1 to group together the computations associated with all children
of a particular preﬁx. This has two advantages. The ﬁrst is to consolidate cache queries: all
children of the same parent preﬁx compute their objective lower bounds with respect to the
parent’s stored value, and we only require one cache ‘ﬁnd’ operation for the entire group
of children, instead of a separate query for each child. The second is to shrink the queue’s
size: instead of adding all of a preﬁx’s children as separate queue elements, we represent
the entire group of children in the queue by a single element. Since the number of children
associated with each preﬁx is close to the total number of possible antecedents, both of these
eﬀects can yield signiﬁcant savings. For example, if we are trying to optimize over rule lists

28

Learning Certifiably Optimal Rule Lists

Algorithm 3 Incremental objective lower bound (30) used in Algorithm 2.

Input: Bit vector v ∈ {0, 1}N indicating data captured by s, the last antecedent in Dp,
bit vector of class labels y ∈ {0, 1}N , number of observations N
Output: Component of D’s misclassiﬁcation error due to data captured by s

function IncrementalLowerBound(v, y, N )

nv = sum(v)
w ← v ∧ y
nw = sum(w)
if nw/nv > 0.5 then

return (nv − nw)/N

else

return nw/N

end if
end function

(cid:46) Number of data captured by s, the last antecedent in Dp
(cid:46) Bit vector indicating data captured by s with label 1
(cid:46) Number of data captured by s with label 1

(cid:46) Misclassiﬁcation error of the rule s → 1

(cid:46) Misclassiﬁcation error of the rule s → 0

Algorithm 4 Incremental objective function (32) used in Algorithm 2.

Input: Bit vector u ∈ {0, 1}N indicating data not captured by Dp’s parent preﬁx, bit
vector v ∈ {0, 1}N indicating data not captured by s, the last antecedent in Dp, bit
vector of class labels y ∈ {0, 1}N , number of observations N
Output: Component of D’s misclassiﬁcation error due to its default rule

function IncrementalObjective(u, v, y, N )

f ← u ∧ ¬ v
nf = sum(f )
g ← f ∧ y
ng = sum(g)
if ng/nf > 0.5 then

(cid:46) Bit vector indicating data not captured by Dp
(cid:46) Number of data not captured by Dp
(cid:46) Bit vector indicating data not captured by Dp with label 1
(cid:46) Number of data not captued by Dp with label 1

return (nf − ng)/N

(cid:46) Misclassiﬁcation error of the default label prediction 1

return ng/N

(cid:46) Misclassiﬁcation error of the default label prediction 0

else

end if
end function

formed from a set of 1000 antecedents, then the maximum queue size in Algorithm 2 will
be smaller than that in Algorithm 1 by a factor of nearly 1000.

29

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

5. Implementation

We implement our algorithm using a collection of optimized data structures that we describe
in this section. First, we explain how we use a preﬁx tree (§5.1) to support the incremental
computations that we motivated in §4. Second, we describe several queue designs that
implement diﬀerent search policies (§5.2). Third, we introduce a symmetry-aware map (§5.3)
to support symmetry-aware pruning (Corollary 16, §3.11). Next, we summarize how these
data structures interact throughout our model of incremental execution (§5.4). In particular,
Algorithms 5 and 6 illustrate many of the computational details from CORELS’ inner
loop, highlighting each of the bounds from §3 that we use to prune the search space. We
additionally describe how we garbage collect our data structures (§5.5). Finally, we explore
how our queue can be used to support custom scheduling policies designed to improve
performance (§5.6).

5.1 Preﬁx Tree

Our incremental computations (§4) require a cache to keep track of preﬁxes that we have
already evaluated and that are also still under consideration by the algorithm. We implement
this cache as a preﬁx tree, a data structure also known as a trie, which allows us to eﬃciently
represent structure shared between related preﬁxes. Each node in the preﬁx tree encodes
an individual rule rk = pk → qk. Each path starting from the root represents a preﬁx, such
that the ﬁnal node in the path also contains metadata associated with that preﬁx. For a
preﬁx dp = (p1, . . . , pK), let ϕ(dp) denote the corresponding node in the trie. The metadata
at node ϕ(dp) supports the incremental computation and includes:

• An index encoding pK, the last antecedent.

• The objective lower bound b(dp, x, y), deﬁned in (5), the central bound in our frame-

work (Theorem 1).

• The lower bound on the default rule misclassiﬁcation error b0(dp, x, y), deﬁned in (28),

to support our equivalent points bound (Theorem 20).

• An indicator denoting whether this node should be deleted (see §5.5).

• A representation of viable extensions of dp, i.e., length K + 1 preﬁx that start with dp

and have not been pruned.

For evaluation purposes and convenience, we store additional information in the preﬁx tree;
for a preﬁx dp with corresponding rule list d = (dp, δp, q0, K), the node ϕ(dp) also stores:

• The length K; equivalently, node ϕ(dp)’s depth in the trie.

• The label prediction qK corresponding to antecedent pK.

• The default rule label prediction q0.

• Ncap, the number of samples captured by preﬁx dp, as in (34).

• The objective value R(d, x, y), deﬁned in (4).

Finally, we note that we implement the preﬁx tree as a custom C++ class.

30

Learning Certifiably Optimal Rule Lists

5.2 Queue

The queue is a worklist that orders exploration over the search space of possible rule lists;
every queue element corresponds to a leaf in the preﬁx tree, and vice versa. In our imple-
mentation, each queue element points to a leaf; when we pop an element oﬀ the queue, we
use the leaf’s metadata to incrementally evaluate the corresponding preﬁx’s children.

We order entries in the queue to implement several diﬀerent search policies. For exam-
ple, a ﬁrst-in-ﬁrst-out (FIFO) queue implements breadth-ﬁrst search (BFS), and a priority
queue implements best-ﬁrst search. In our experiments (§6), we use the C++ Standard
Template Library (STL) queue and priority queue to implement BFS and best-ﬁrst search,
respectively. For CORELS, priority queue policies of interest include ordering by the lower
bound, the objective, or more generally, any function that maps preﬁxes to real values;
stably ordering by preﬁx length and inverse preﬁx length implement BFS and depth-ﬁrst
search (DFS), respectively. In our released code, we present a uniﬁed implementation, where
we use the STL priority queue to support BFS, DFS, and several best-ﬁrst search policies.
As we demonstrate in our experiments (§6.8), we ﬁnd that using a custom search strategy,
such as ordering by the lower bound, usually leads to a faster runtime than BFS.

We motivate the design of additional custom search strategies in §5.6. In preliminary
work (not shown), we also experimented with stochastic exploration processes that bypass
the need for a queue by instead following random paths from the root to leaves; developing
such methods could be an interesting direction for future work. We note that these search
policies are referred to as node selection strategies in the MIP literature. Strategies such as
best-ﬁrst (best-bound) search and DFS are known as static methods, and the framework
we present in §5.6 has the spirit of estimate-based methods (Linderoth and Savelsbergh,
1999).

5.3 Symmetry-aware Map

The symmetry-aware map supports the symmetry-aware pruning justiﬁed in §3.10. In our
implementation, we speciﬁcally leverage our permutation bound (Corollary 16), though it is
also possible to directly exploit the more general equivalent support bound (Theorem 15).
We use the C++ STL unordered map to keep track of the best known ordering of each
evaluated set of antecedents. The keys of our symmetry-aware map encode antecedents in
canonical order, i.e., antecedent indices in numerically sorted order, and we associate all
permutations of a set of antecedents with a single key. Each key maps to a value that
encodes the best known preﬁx in the permutation group of the key’s antecedents, as well
as the objective lower bound of that preﬁx.

Before we consider adding a preﬁx dp to the trie and queue, we check whether the map
already contains a permutation π(dp) of that preﬁx. If no such permutation exists, then
we insert dp into the map, trie, and queue. Otherwise, if a permutation π(dp) exists and
the lower bound of dp is better than that of π(dp), i.e., b(dp, x, y) < b(π(dp), x, y), then we
update the map and remove π(dp) and its entire subtree from the trie; we also insert dp into
the trie and queue. Otherwise, if there exists a permutation π(dp) such that b(π(dp), x, y) ≤
b(dp, x, y), then we do nothing, i.e., we do not insert dp into any data structures.

31

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

5.4 Incremental Execution

Mapping our algorithm to our data structures produces the following execution strategy,
which we also illustrate in Algorithms 5 and 6. We initialize the current best objective Rc
and rule list dc. While the trie contains unexplored leaves, a scheduling policy selects the
next preﬁx dp to extend; in our implementation, we pop elements from a (priority) queue,
until the queue is empty. Then, for every antecedent s that is not in dp, we construct a new
preﬁx Dp by appending s to dp; we incrementally calculate the lower bound b(Dp, x, y),
the objective R(D, x, y), of the associated rule list D, and other quantities used by our
algorithm, summarized by the metadata ﬁelds of the (potential) preﬁx tree node ϕ(Dp).

If the objective R(D, x, y) is less than the current best objective Rc, then we update Rc
and dc. If the lower bound of the new preﬁx Dp is less than the current best objective,
then as described in §5.3, we query the symmetry-aware map for Dp; if we insert d(cid:48)
p into
the symmetry-aware map, then we also insert it into the trie and queue. Otherwise, then
by our hierarchical lower bound (Theorem 1), no extension of Dp could possibly lead to a
rule list with objective better than Rc, thus we do not insert Dp into the tree or queue. We
also leverage our other bounds from §3 to aggressively prune the search space; we highlight
each of these bounds in Algorithms 5 and 6, which summarize the computations and data
structure operations performed in CORELS’ inner loop. When there are no more leaves
to explore, i.e., the queue is empty, we output the optimal rule list. We can optionally
terminate early according to some alternate condition, e.g., when the size of the preﬁx tree
exceeds some threshold.

5.5 Garbage Collection

During execution, we garbage collect the trie. Each time we update the minimum objective,
we traverse the trie in a depth-ﬁrst manner, deleting all subtrees of any node with lower
bound larger than the current minimum objective. At other times, when we encounter a
node with no children, we prune upwards, deleting that node and recursively traversing
the tree towards the root, deleting any childless nodes. This garbage collection allows us
to constrain the trie’s memory consumption, though in our experiments we observe the
minimum objective to decrease only a small number of times.

In our implementation, we cannot immediately delete preﬁx tree leaves because each
corresponds to a queue element that points to it. The C++ STL priority queue is a wrapper
container that prevents access to the underlying data structure, and thus we cannot access
elements in the middle of the queue, even if we know the relevant identifying information. We
therefore have no way to update the queue without iterating over every element. We address
this by marking preﬁx tree leaves that we wish to delete (see §5.1), deleting the physical
nodes lazily, after they are popped from the queue. Later, in our section on experiments (§6),
we refer to two diﬀerent queues that we deﬁne here: the physical queue corresponds to the
C++ queue, and thus all preﬁx tree leaves, and the logical queue corresponds only to those
preﬁx tree leaves that have not been marked for deletion.

32

Learning Certifiably Optimal Rule Lists

Algorithm 5 The inner loop of CORELS, which evaluates all children of a preﬁx dp.
1[xn ∈ eu][yn = qu]

Deﬁne z ∈ {0, 1}N , s.t. zn = (cid:80)U

u=1

Deﬁne b(dp, x, y) and u = ¬ cap(x, dp)
for s in S if s not in dp then do

(cid:46) eu is the equivalent points set containing xn and qu is the minority class label of eu (§3.14)
(cid:46) u is a bit vector indicating data not captured by dp
(cid:46) Evaluate all of dp’s children
(cid:46) Branch: Generate child Dp
(cid:46) Bit vector indicating data captured by s in Dp
(cid:46) Number of data captured by s, the last antecedent in Dp

(cid:46) Lower bound on antecedent support (Theorem10)

(cid:46) Bit vector indicating data captured by s with label 1
(cid:46) Number of data captured by s with label 1

(cid:46) Number of correct predictions by the new rule s → 1

(cid:46) Number of correct predictions by the new rule s → 0

Dp ← (dp, s)
v ← u ∧ cap(x, s)
nv = sum(v)
if nv/N < λ then
continue

end if
w ← v ∧ y
nw = sum(w)
if nw/nv ≥ 0.5 then

nc ← nw

else

nc ← nv − nw

end if
if nc/N < λ then
continue

(cid:46) Lower bound on accurate antecedent support (Theorem 11)

end if
δb ← (nv − nc)/N
b(Dp, x, y) ← b(dp, x, y) + λ + δb
if b(Dp, x, y) ≥ Rc then

(cid:46) Misclassiﬁcation error of the new rule
(cid:46) Incremental lower bound (30)
(cid:46) Hierarchical objective lower bound (Theorem 1)

continue

end if
f ← u ∧ ¬ v
nf = sum(f )
g ← f ∧ y
ng = sum(g)
if ng/nf ≥ 0.5 then

δR ← (nf − ng)/N

else

δR ← ng/N

end if
R(D, x, y) ← b(Dp, x, y) + δR
D ← (Dp, ∆p, Q0, K + 1)
if R(D, x, y) < Rc then

(cid:46) Bit vector indicating data not captured by Dp
(cid:46) Number of data not captured by Dp
(cid:46) Bit vector indicating data not captured by Dp with label 1
(cid:46) Number of data not captued by Dp with label 1

(cid:46) Misclassiﬁcation error of the default label prediction 1

(cid:46) Misclassiﬁcation error of the default label prediction 0

(cid:46) Incremental objective (32)
(cid:46) ∆p, Q0 are set in the incremental functions

(dc, Rc) ← (D, R(D, x, y))
(cid:46) Update current best rule list and objective
GarbageCollectPrefixTree(Rc) (cid:46) Delete nodes with lower bound ≥ Rc − λ (§5.5),
using the Lookahead bound (Lemma 2)
(cid:46) Lower bound on the default rule misclassiﬁcation
error deﬁned in (28)
(cid:46) Equivalent points bound (Theorem 20)
combined with the Lookahead bound (Lemma 2)

end if
b0(Dp, x, y) ← sum(f ∧ z)/N
b ← b(Dp, x, y) + b0(Dp, x, y)
if b + λ ≥ Rc then

continue

end if
CheckMapAndInsert(Dp, b)

end for

(cid:46) Check the Permutation bound (Corollary 16) and
possibly insert Dp into data structures (Algorithm 6)

33

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Algorithm 6 Possibly insert a preﬁx into CORELS’ data structures, after ﬁrst checking the
symmetry-aware map, which supports search space pruning triggered by the permutation
bound (Corollary 16). For further context, see Algorithm 5.

T is the preﬁx tree (§5.1)
Q is the queue, for concreteness, a priority queue ordered by the lower bound (§5.2)
P is the symmetry-aware map (§5.3)

function CheckMapAndInsert(Dp, b)

π0 ← sort(Dp)
(Dπ, bπ) ← P .ﬁnd(π0)
if Dπ exists then
if b < bπ then

P .update(π0, (Dp, b))
T .delete subtree(Dπ)
T .insert(ϕ(Dp))
Q.push(Dp, b)

else

end if

else

P .insert(π0, (Dp, b))
T .insert(ϕ(Dp))
Q.push(Dp, b)

end if
end function

(cid:46) Dp’s antecedents in canonical order
(cid:46) Look for a permutation of Dp

(cid:46) Dp is better than Dπ
(cid:46) Replace Dπ with Dp in the map
(cid:46) Delete Dπ and its subtree from the preﬁx tree
(cid:46) Add node for Dp to the preﬁx tree
(cid:46) Add Dp to the queue

(cid:46) Add Dp to the map
(cid:46) Add node for Dp to the preﬁx tree
(cid:46) Add Dp to the queue

pass

(cid:46) Dp is inferior to Dπ, thus do not insert it into any data structures

5.6 Custom Scheduling Policies

In our setting, an ideal scheduling policy would immediately identify an optimal rule list,
and then certify its optimality by systematically eliminating the remaining search space.
This motivates trying to design scheduling policies that tend to quickly ﬁnd optimal rule
lists. When we use a priority queue to order the set of preﬁxes to evaluate next, we are free
to implement diﬀerent scheduling policies via the ordering of elements in the queue. This
motivates designing functions that assign higher priorities to ‘better’ preﬁxes that we believe
are more likely to lead to optimal rule lists. We follow the convention that priority queue
elements are ordered by keys, such that keys with smaller values have higher priorities.

We introduce a custom class of functions that we call curiosity functions. Broadly, we
think of the curiosity of a rule list d as the expected objective value of another rule list d(cid:48)
that is related to d; diﬀerent models of the relationship between d and d(cid:48) lead to diﬀerent
curiosity functions. In general, the curiosity of d is, by deﬁnition, equal to the sum of the
expected misclassiﬁcation error and the expected regularization penalty of d(cid:48):

C(dp, x, y) ≡ E[R(d(cid:48), x, y)] = E[(cid:96)(d(cid:48)

p, δ(cid:48)

p, x, y)] + λE[K(cid:48)].

(33)

34

Learning Certifiably Optimal Rule Lists

Next, we describe a simple curiosity function for a rule list d with preﬁx dp. First,

let Ncap denote the number of observations captured by dp, i.e.,

Ncap ≡

cap(xn, dp).

(34)

N
(cid:88)

n=1

We now describe a model that generates another rule list d(cid:48) = (d(cid:48)
sume that preﬁx d(cid:48)
antecedent in d(cid:48)
then, the expected length of d(cid:48)

0, K(cid:48)) from dp. As-
p starts with dp and captures all the data, such that each additional
p captures as many ‘new’ observations as each antecedent in dp, on average;

p, δ(cid:48)

p, q(cid:48)

p is

E[K(cid:48)] =

N
Ncap/K

.

Furthermore, assume that each additional antecedent in d(cid:48)
antecedent in dp, on average, thus the expected misclassiﬁcation error of d(cid:48)

p makes as many mistakes as each

p is

E[(cid:96)(d(cid:48)

p, δ(cid:48)

p, x, y)] = E[(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y)] + E[(cid:96)0(d(cid:48)

= E[(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y)] = E[K(cid:48)]

p, q(cid:48)
0, x, y)]
(cid:18) (cid:96)p(dp, δp, x, y)
K

(cid:19)

.

0, x, y) is zero because we assume
p captures all the data. Inserting (35) and (36) into (33) thus gives curiosity for this

Note that the default rule misclassiﬁcation error (cid:96)0(d(cid:48)
that d(cid:48)
model:

p, q(cid:48)

(35)

(36)

C(dp, x, y) =

(cid:96)p(dp, δp, x, y) + λK

(cid:19)

(cid:19) (cid:18)

(cid:18) N
Ncap

(cid:32)

=

1
N

N
(cid:88)

n=1

(cid:33)−1

cap(xn, dp)

b(dp, x, y) =

b(dp, x, y)
supp(dp, x)

,

where for the second equality, we used the deﬁnitions in (34) of Ncap and in (5) of dp’s lower
bound, and for the last equality, we used the deﬁnition in (2) of dp’s normalized support.

The curiosity for a preﬁx dp is thus also equal to its objective lower bound, scaled by
the inverse of its normalized support. For two preﬁxes with the same lower bound, curiosity
gives higher priority to the one that captures more data. This is a well-motivated scheduling
strategy if we model preﬁxes that extend the preﬁx with smaller support as having more
‘potential’ to make mistakes. We note that using curiosity in practice does not introduce
new bit vector or other expensive computations; during execution, we can calculate curiosity
as a simple function of already derived quantities.

In preliminary experiments, we observe that using a priority queue ordered by curiosity
sometimes yields a dramatic reduction in execution time, compared to using a priority queue
ordered by the objective lower bound. Thus far, we have observed signiﬁcant beneﬁts on
speciﬁc small problems, where the structure of the solutions happen to render curiosity
particularly eﬀective (not shown). Designing and studying other ‘curious’ functions, that
are eﬀective in more general settings, is an exciting direction for future work.

35

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Data set

Prediction problem

N

ProPublica Two-year recidivism
NYPD
Weapon possession
Weapon possession
NYCLU

6,907
325,800
29,595

Positive
fraction
0.46
0.03
0.05

Resample
training set
No
Yes
Yes

Training
set size
6,217
566,839
50,743

Test
set size
692
32,580
2,959

Table 1: Summary of data sets and prediction problems. The last ﬁve columns report the
total number of observations, the fraction of observations with the positive class
label, whether we resampled the training set due to class imbalance, and the sizes
of each training and test set in our 10-fold cross-validation studies.

6. Experiments

Our experimental analysis addresses ﬁve questions: How does CORELS’ predictive perfor-
mance compare to that of COMPAS scores and other algorithms? (§6.4, §6.5, and §6.6)
How does CORELS’ model size compare to that of other algorithms? (§6.6) How rapidly do
the objective value and its lower bound converge, for diﬀerent values of the regularization
parameter λ? (§6.7) How much does each of the implementation optimizations contribute
to CORELS’ performance? (§6.8) How rapidly does CORELS prune the search space? (§6.7
and §6.8) Before proceeding, we ﬁrst describe our computational environment (§6.1), as
well as the data sets and prediction problems we use (§6.2), and then in Section 6.3 show
example optimal rule lists found by CORELS.

6.1 Computational Environment

All timed results ran on a server with an Intel Xeon E5-2699 v4 (55 MB cache, 2.20 GHz)
processor and 264 GB RAM, and we ran each timing measurement separately, on a single
hardware thread, with nothing else running on the server. Except where we mention a
memory constraint, all experiments can run comfortably on smaller machines, e.g., a laptop
with 16 GB RAM.

6.2 Data Sets and Prediction Problems

Our evaluation focuses on two socially-important prediction problems associated with re-
cent, publicly-available data sets. Table 1 summarizes the data sets and prediction problems,
and Table 2 summarizes feature sets extracted from each data set, as well as antecedent sets
we mine from these feature sets. We provide some details next. For further details about
data sets, preprocessing steps, and antecedent mining, see Appendix E.

6.2.1 Recidivism Prediction

For our ﬁrst problem, we predict which individuals in the ProPublica COMPAS data
set (Larson et al., 2016) recidivate within two years. This data set contains records for
all oﬀenders in Broward County, Florida in 2013 and 2014 who were given a COMPAS
score pre-trial. Recidivism is deﬁned as being charged with a new crime within two years

36

Learning Certifiably Optimal Rule Lists

Data set

ProPublica
ProPublica
NYPD
NYPD
NYCLU

Feature Categorical Binary
features
attributes
13
6
17
7
28
5
20
3
28
5

set
A
B
C
D
E

Mined
antecedents
122
189
28
20
46

of clauses
2
2
1
1
1

No
No
No
No
Yes

Max number Negations

Table 2: Summary of feature sets and mined antecedents. The last ﬁve columns report
the number of categorical attributes, the number of binary features, the average
number of mined antecedents, the maximum number of clauses in each antecedent,
and whether antecedents include negated clauses.

after receiving a COMPAS assessment; the article by Larson et al. (2016), and their code,3
provide more details about this deﬁnition. From the original data set of records for 7,214
individuals, we identify a subset of 6,907 records without missing data. For the majority of
our analysis, we extract a set of 13 binary features (Feature Set A), which our antecedent
mining framework combines into M = 122 antecedents, on average (folds ranged from con-
taining 121 to 123 antecedents). We also consider a second, similar antecedent set in §6.3,
derived from a superset of Feature Set A that includes 4 additional binary features (Feature
Set B).

6.2.2 Weapon Prediction

For our second problem, we use New York City stop-and-frisk data to predict whether a
weapon will be found on a stopped individual who is frisked or searched. For experiments
in Sections 6.3 and 6.5 and Appendix G, we compile data from a database maintained by
the New York Police Department (NYPD) (New York Police Department, 2016), from years
2008-2012, following Goel et al. (2016). Starting from 2,941,390 records, each describing an
incident involving a stopped person, we ﬁrst extract 376,488 records where the suspected
crime was criminal possession of a weapon (CPW).4 From these, we next identify a subset
of 325,800 records for which the individual was frisked and/or searched; of these, criminal
possession of a weapon was identiﬁed in only 10,885 instances (about 3.3%). Resampling
due to class imbalance, for 10-fold cross-validation, yields training sets that each contain
566,839 datapoints. (We form corresponding test sets without resampling.) From a set of
5 categorical features, we form a set of 28 single-clause antecedents corresponding to 28
binary features (Feature Set C). We also consider another, similar antecedent set, derived
from a subset of Feature Set C that excludes 8 location-speciﬁc binary features (Feature
Set D).

In Sections 6.3, 6.6, 6.7, and 6.8, we also use a smaller stop-and-frisk data set, derived by
the NYCLU from the NYPD’s 2014 data (New York Civil Liberties Union, 2014). From the

3. Data and code used in the analysis by Larson et al. (2016) can be found at https://github.com/

propublica/compas-analysis.

4. We ﬁlter for records that explicitly match the string ‘CPW’; we note that additional records, after

converting to lowercase, contain strings such as ‘cpw’ or ‘c.p.w.’

37

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

Figure 3: Example optimal rule lists that predict two-year recidivism for the ProPublica
data set (Feature Set B, M = 189), found by CORELS (λ = 0.005), across 10
cross-validation folds. While some input antecedents contain features for race, no
optimal rule list includes such an antecedent. Every optimal rule list is the same
or similar to one of these examples, with preﬁxes containing the same rules, up
to a permutation, and same default rule.

original data set of 45,787 records, each describing an incident involving a stopped person,
we identify a subset of 29,595 records for which the individual was frisked and/or searched.
Of these, criminal possession of a weapon was identiﬁed in about 5% of instances. As with
the larger NYPD data set, we resample the data to form training sets (but not to form
test sets). From the same set of 5 categorical features as in Feature Set C, we form a set of
M = 46 single-clause antecedents, including negations (Feature Set E).

6.3 Example Optimal Rule Lists

To motivate Feature Set A, described in Appendix E, which we used in most of our analysis
of the ProPublica data set, we ﬁrst consider Feature Set B, a larger superset of features.

Figure 3 shows optimal rule lists learned by CORELS, using Feature Set B, which
additionally includes race categories from the ProPublica data set (African American, Cau-
casian, Hispanic, Other5). For Feature Set B, our antecedent mining procedure generated an
average of 189 antecedents, across folds. None of the optimal rule lists contain antecedents
that directly depend on race; this motivated our choice to exclude race, by using Feature
Set A, in our subsequent analysis. For both feature sets, we replaced the original ProPublica
age categories (<25, 25-45, >45) with a set that is more ﬁne-grained for younger individuals
(18-20, 21-22, 23-25, 26-45, >45). Figure 4 shows example optimal rule lists that CORELS
learns for the ProPublica data set (Feature Set A, λ = 0.005), using 10-fold cross validation.
Figures 5 and 6 show example optimal rule lists that CORELS learns for the NYCLU
(λ = 0.01) and NYPD data sets. Figure 6 shows optimal rule lists that CORELS learns for
the larger NYPD data set.

While our goal is to provide illustrative examples, and not to provide a detailed analysis
nor to advocate for the use of these speciﬁc models, we note that these rule lists are short and
easy to understand. For the examples and regularization parameter choices in this section,

5. We grouped the original Native American (<0.003), Asian (<0.005), and Other (<0.06) categories.

38

Learning Certifiably Optimal Rule Lists

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

Figure 4: Example optimal rule lists that predict two-year recidivism for the ProPublica
data set (Feature Set A, M = 122), found by CORELS (λ = 0.005), across 10
cross-validation folds. Feature Set A is a subset of Feature Set B (Figure 3) that
excludes race features. Optimal rule lists found using the two feature sets are
very similar. The upper and lower rule lists are representative of 7 and 3 folds,
respectively. Each of the remaining 8 solutions is the same or similar to one of
these, with preﬁxes containing the same rules, up to a permutation, and the same
default rule. See Figure 20 in Appendix F for a complete listing.

if (location = transit authority) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = suspicious object) then predict yes
else predict no

Figure 5: An example rule list that predicts whether a weapon will be found on a stopped in-
dividual who is frisked or searched, for the NYCLU stop-and-frisk data set. Across
10 cross-validation folds, the other optimal rule lists found by CORELS (λ = 0.01)
contain the same or equivalent rules, up to a permutation. See also Figure 23 in
Appendix F.

the optimal rule lists are relatively robust across cross-validation folds: the rules are nearly
the same, up to permutations of the preﬁx rules. For smaller values of the regularization
parameter, we observe less robustness, as rule lists are allowed to grow in length. For the
sets of optimal rule lists represented in Figures 3, 4, and 5, each set could be equivalently
expressed as a DNF rule; e.g., this is easy to see when the preﬁx rules all predict the
positive class label and the default rule predicts the negative class label. Our objective is
not designed to enforce any of these properties, though some may be seen as desirable.

As we demonstrate in §6.6, optimal rule lists learned by CORELS achieve accuracies
that are competitive with a suite of other models, including black box COMPAS scores. See
Appendix F for additional listings of optimal rule lists found by CORELS, for each of our
prediction problems, across cross-validation folds, for diﬀerent regularization parameters λ.

39

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Weapon prediction (λ = 0.01, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else predict no

Weapon prediction (λ = 0.01, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (inside or outside = outside) then predict no
else predict yes

Weapon prediction (λ = 0.005, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

Weapon prediction (λ = 0.005, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

Figure 6: Example optimal rule lists for the NYPD stop-and-frisk data set, found by
CORELS. Feature Set C contains attributes for ‘location’ and ‘city’, while Fea-
ture Set D does not. For each choice of regularization parameter and feature set,
the rule lists learned by CORELS, across all 10 cross-validation folds, contain
the same or equivalent rules, up to a permutation, with the exception of a single
fold (Feature Set C, λ = 0.005). For a complete listing, see Figures 21 and 22 in
Appendix F.

6.4 Comparison of CORELS to the Black Box COMPAS Algorithm

The accuracies of rule lists learned by CORELS are competitive with scores generated by
the black box COMPAS algorithm at predicting two-year recidivism for the ProPublica
data set (Figure 9). Across 10 cross-validation folds, optimal rule lists learned by CORELS
(Figure 4, λ = 0.005) have a mean test accuracy of 0.665, with standard deviation 0.018. The
COMPAS algorithm outputs scores between 1 and 10, representing low (1-4), medium (5-7),
and high (8-10) risk for recidivism. As in the analysis by Larson et al. (2016), we interpret a
medium or high score as a positive prediction for two-year recidivism, and a low score as a
negative prediction. Across the 10 test sets, the COMPAS algorithm scores obtain a mean
accuracy of 0.660, with standard deviation 0.019.

Figure 7 shows that CORELS and COMPAS perform similarly across both black and
white individuals. Both algorithms have much higher true positive rates (TPR’s) and false
positive rates (FPR’s) for blacks than whites (left), and higher true negative rates (TNR’s)

40

Learning Certifiably Optimal Rule Lists

Figure 7: Comparison of TPR and FPR (left), as well as TNR and FNR (right), for diﬀerent
races in the ProPublica data set, for CORELS and COMPAS, across 10 cross-
validation folds.

and false negative rates (FNR’s) for whites than blacks (right). The fact that COMPAS has
higher FPR’s for blacks and higher FNR’s for whites was a central observation motivating
ProPublica’s claim that COMPAS is racially biased (Larson et al., 2016). The fact that
CORELS’ models are so simple, with almost the same results as COMPAS, and contain
only counts of past crimes, age, and gender, indicates possible explanations for the uneven
predictions of both COMPAS and CORELS among blacks and whites. In particular, blacks
evaluated within Broward County tend to be younger and have longer criminal histories
within the data set, (on average, 4.4 crimes for blacks versus 2.6 crimes for whites) leading
to higher FPR’s for blacks and higher FNR’s for whites. This aspect of the data could help
to explain why ProPublica concluded that COMPAS was racially biased.

Similar observations have been reported for other datasets, namely that complex ma-
chine learning models do not have an advantage over simpler transparent models (Tollenaar
and van der Heijden, 2013; Bushway, 2013; Zeng et al., 2017). There are many deﬁnitions of
fairness, and it is not clear whether CORELS’ models are fair either, but it is much easier to
debate about the fairness of a model when it is transparent. Additional fairness constraints
or transparency constraints can be placed on CORELS’ models if desired, though one would
need to edit our bounds (§3) and implementation (§5) to impose more constraints.

Regardless of whether COMPAS is racially biased (which our analysis does not indicate is
necessarily true as long as criminal history and age are allowed to be considered as features),
COMPAS may have many other fairness defects that might be considered serious. Many of
COMPAS’s survey questions are direct inquiries about socioeconomic status. For instance,
a sample COMPAS survey6 asks: “Is it easy to get drugs in your neighborhood?,” “How
often do you have barely enough money to get by?,” “Do you frequently get jobs that don’t
pay more than minimum wage?,” “How often have you moved in the last 12 months?”
COMPAS’s survey questions also ask about events that were not caused by the person who

6. A sample COMPAS survey contributed by Julia Angwin, ProPublica, can be found at https://www.

documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html.

41

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

is being evaluated, such as: “If you lived with both parents and they later separated, how
old were you at the time?,” “Was one of your parents ever sent to jail or prison?,” “Was
your mother ever arrested, that you know of?”

The fact that COMPAS requires over 130 questions to be answered, many of whose
answers may not be veriﬁable, means that the computation of the COMPAS score is prone
to errors. Even the Arnold Foundation’s “public-safety assessment” (PSA) score—which is
completely transparent, and has only 9 factors—has been miscalculated in serious criminal
trials, leading to a recent lawsuit (Westervelt, 2017). It is substantially more diﬃcult to
obtain the information required to calculate COMPAS scores than PSA scores (with over 14
times the number of survey questions). This signiﬁcant discrepancy suggests that COMPAS
scores are more fallible than PSA scores, as well as even simpler models, like those produced
by CORELS. Some of these problems could be alleviated by using only data within electronic
records that can be automatically calculated, instead of using information entered by hand
and/or collected via subjective surveys.

The United States government pays Northpointe (now called Equivant) to use COMPAS.
In light of our observations that CORELS is as accurate as COMPAS on a real-world data
set where COMPAS is used in practice, CORELS predicts similarly to COMPAS for both
blacks and whites, and CORELS’ models are completely transparent, it is not clear what
value COMPAS scores possess. Our experiments also indicate that the proprietary survey
data required to compute COMPAS scores has not boosted its prediction accuracy above
that of transparent models in practice.

Risk predictions are important for the integrity of the judicial system; judges cannot be
expected to keep entire databases in their heads to calculate risks, whereas models (when
used correctly) can help to ensure equity. Risk prediction models also have the potential
to heavily impact how eﬃcient the judicial system is, in terms of bail and parole decisions;
eﬃciency in this case means that dangerous individuals are not released, whereas non-
dangerous individuals are granted bail or parole. High stakes decisions, such as these, are
ideal applications for machine learning algorithms that produce transparent models from
high dimensional data.

Currently, justice system data does not support highly accurate risk predictions, but
current risk models are useful in practice, and these risk predictions will become more
accurate as more and higher quality data are made available.

6.5 Comparison of CORELS to a Heuristic Model for Weapon Prediction

CORELS generates simple, accurate models for the task of weapon prediction, using the
NYPD stop-and-frisk data set. Our approach oﬀers a principled alternative to heuristic
models proposed by Goel et al. (2016), who develop a series of regression models to analyze
racial disparities in New York City’s stop-and-frisk policy for a related, larger data set. In
particular, the authors arrive at a heuristic that they suggest could potentially help police
oﬃcers more eﬀectively decide when to frisk and/or search stopped individuals, i.e., when
such interventions are likely to discover criminal possession of a weapon (CPW). Starting
from a full regression model with 7,705 variables, the authors reduce this to a smaller model
with 98 variables; from this, they keep three variables with the largest coeﬃcients. This gives

42

Learning Certifiably Optimal Rule Lists

a heuristic model of the form ax + by + cz ≥ T , where

x = 1[stop reason = suspicious object]
y = 1[stop reason = suspicious bulge]
z = 1[additional circumstances = sights and sounds of criminal activity],

and T is a threshold, such that the model predicts CPW when the threshold is met or
exceeded. We focus on their approach that uses a single threshold, rather than precinct-
speciﬁc thresholds. To increase ease-of-use, the authors round the coeﬃcients to the nearest
integers, which gives (a, b, c) = (3, 1, 1); this constrains the threshold to take one of six
values, T ∈ {0, 1, 2, 3, 4, 5}. To employ this heuristic model in the ﬁeld, “. . . oﬃcers simply
need to add at most three small, positive integers . . . and check whether the sum exceeds a
ﬁxed threshold. . . ” (Goel et al., 2016).

Figure 8 directly compares various models learned by CORELS to the heuristic models,
using the same data set as Goel et al. (2016) and 10-fold cross-validation. Recall that we
train on resampled data to correct for class imbalance; we evaluate with respect to test sets
that have been formed without resampling. For CORELS, the models correspond to the
rule lists illustrated in Figure 6 from Section 6.3, and Figures 21 and 22 in Appendix F, we
consider both Feature Sets C and D and both regularization parameters λ = 0.005 and 0.01.
The top panel plots the fraction of weapons recovered as a function of the fraction of stops
where the individual was frisked and/or searched. Goel et al. (2016) target models that
eﬃciently recover a majority of weapons (while also minimizing racial disparities, which
we do not address here). Interestingly, the models learned by CORELS span a signiﬁcant
region that is not available to the heuristic model, which would require larger or non-integer
parameters to access the region. The region is possibly desirable, since it includes models
(λ = 0.005, bright red) that recover a majority (≥ 50%) of weapons (that are known in the
data set). More generally, CORELS’ models all recover at least 40% of weapons on average,
i.e., more weapons than any of the heuristic models with T ≥ 2, which recover less than
25% of weapons on average. At the same time, CORELS’ models all require well under 25%
of stops—signiﬁcantly less than the heuristic model with T = 1, which requires over 30% of
stops to recover a fraction of weapons comparable to the CORELS model that recovers the
most weapons.

The bottom panel in Figure 8 plots both TPR and FPR and labels model size, for each
of the models in the top panel. For the heuristic, we deﬁne model size as the number of
model parameters; for CORELS, we use the number of rules in the rule list, which is equal
to the number of leaves when we view a rule list as a decision tree. The heuristic models all
have 4 parameters, while the diﬀerent CORELS models have either 3 or approximately 5
rules. CORELS’ models are thus approximately as small, interpretable, and transparent as
the heuristic models; furthermore, their predictions are straightforward to compute, without
even requiring arithmetic.

6.6 Predictive Performance and Model Size for CORELS and Other Algorithms

We ran a 10-fold cross validation experiment using CORELS and eight other algorithms:
logistic regression, support vector machines (SVM), AdaBoost, CART, C4.5, random for-

43

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 8: Weapon prediction with the NYPD stop-and-frisk data set, for various models
learned by CORELS and the heuristic model by Goel et al. (2016), across 10
cross-validation folds. Note that the fraction of weapons recovered (top) is equal
to the TPR (bottom, open markers). Markers above the dotted horizontal lines at
the value 0.5 correspond to models that recover a majority of weapons (that are
known in the data set). Top: Fraction of weapons recovered as a function of the
fraction of stops where the individual was frisked and/or searched. In the legend,
entries for CORELS (red markers) indicate the regularization parameter (λ) and
whether or not extra location features were used (“location”); entries for the
heuristic model (blue markers) indicate the threshold value (T ). The results we
report for the heuristic model are our reproduction of the results reported in Figure
9 by Goel et al. (2016) (ﬁrst four open circles in that ﬁgure, from left to right;
we exclude the trivial open circle showing 100% of weapons recovered at 100% of
stops, obtained by setting the threshold at 0). Bottom: Comparison of TPR (open
markers) and FPR (solid markers) for various CORELS and heuristic models.
Models are sorted left-to-right by TPR. Markers and abbreviated horizontal tick
labels correspond to the legend in the top ﬁgure. Numbers in the plot label model
size; there was no variation in model size across folds, except for a single fold for
CORELS (λ = 0.005, Feature Set C), which found a model of size 6.

44

Learning Certifiably Optimal Rule Lists

Figure 9: Two-year recidivism prediction for the ProPublica COMPAS data set. Compari-
son of CORELS and a panel of nine other algorithms: logistic regression (GLM),
support vector machines (SVM), AdaBoost, CART, C4.5, random forests (RF),
RIPPER, scalable Bayesian rule lists (SBRL), and COMPAS. For CORELS, we
use regularization parameter λ = 0.005.

est (RF), RIPPER, and scalable Bayesian rule lists (SBRL).7 We use standard R packages,
with default parameter settings, for the ﬁrst seven algorithms.8 We use the same antecedent
sets as input to the two rule list learning algorithms, CORELS and SBRL; for the other
algorithms, the inputs are binary feature sets corresponding to the single clause antecedents
in the aforementioned antecedent sets (see Appendix E).

Figure 9 shows that for the ProPublica data set, there were no statistically signiﬁcant
diﬀerences in test accuracies across algorithms, the diﬀerence between folds was far larger
than the diﬀerence between algorithms. These algorithms also all perform similarly to the
black box COMPAS algorithm. Figure 10 shows that for the NYCLU data set, logistic
regression, SVM, and AdaBoost have the highest TPR’s and also the highest FPR’s; we show
TPR and FPR due to class imbalance. For this problem, CORELS obtains an intermediate
TPR, compared to other algorithms, while achieving a relatively low FPR. We conclude
that CORELS produces models whose predictive performance is comparable to or better
than those found via other algorithms.

Figures 11 and 12 summarize diﬀerences in predictive performance and model size for
CORELS and other tree (CART, C4.5) and rule list (RIPPER, SBRL) learning algorithms.
Here, we vary diﬀerent algorithm parameters, and increase the number of iterations for
SBRL to 10,000. For two-year recidivism prediction with the ProPublica data set (Fig-
ure 11), we plot both training and test accuracy, as a function of the number of leaves
in the learned model. Due to class imbalance for the weapon prediction problem with the
NYCLU stop-and-frisk data set (Figure 12), we plot both true positive rate (TPR) and
false positive rate (FPR), again as a function of the number of leaves. For both problems,
CORELS can learn short rule lists without sacriﬁcing predictive performance. For listings
of example optimal rule lists that correspond to the results for CORELS summarized here,

7. For SBRL, we use the C implementation at https://github.com/Hongyuy/sbrlmod. By default, SBRL

sets η = 3, λ = 9, the number of chains to 11 and iterations to 1,000.

8. For CART, C4.5 (J48), and RIPPER, we use the R packages rpart, RWeka, and caret, respectively. By
default, CART uses complexity parameter cp = 0.01 and C4.5 uses complexity parameter C = 0.25.

45

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 10: TPR (left) and FPR (right) for the test set, for CORELS and a panel of seven
other algorithms, for the weapon prediction problem with the NYCLU stop-
and-frisk data set. Means (white squares), standard deviations (error bars), and
values (colors correspond to folds), for 10-fold cross-validation experiments. For
CORELS, we use λ = 0.01. Note that we were unable to execute RIPPER for
the NYCLU problem.

Figure 11: Training and test accuracy as a function of model size, across diﬀerent methods,
for two-year recidivism prediction with the ProPublica COMPAS data set. In
the legend, numbers in parentheses are algorithm parameters that we vary for
CORELS (λ), CART (cp), C4.5 (C), and SBRL (η, λ, i), where i is the num-
ber of iterations. Legend markers and error bars indicate means and standard
deviations, respectively, across cross-validation folds. Small circles mark training
accuracy means. None of the models exhibit signiﬁcant overﬁtting; mean training
accuracy never exceeds mean test accuracy by more than about 0.01.

see Appendix F. Also see Figure 25 in Appendix G; it uses the larger NYPD data set and
is similar to Figure 12.

In Figure 4, we used CORELS to identify short rule lists, depending on only three
features—age, prior convictions, and sex—that achieve test accuracy comparable to COM-
PAS (Figure 9, also see Angelino et al., 2017). If we restrict CORELS to search the space
of rule lists formed from only age and prior convictions (λ = 0.005), the optimal rule lists it

46

Learning Certifiably Optimal Rule Lists

Figure 12: TPR (top) and FPR (bottom) for the test set, as a function of model size, across
diﬀerent methods, for weapon prediction with the NYCLU stop-and-frisk data
set. In the legend, numbers in parentheses are algorithm parameters, as in Fig-
ure 11. Legend markers and error bars indicate means and standard deviations,
respectively, across cross-validation folds. C4.5 ﬁnds large models for all tested
parameters.

if (priors > 3) then predict yes
else if (age < 25) and (priors = 2 − 3) then predict yes
else predict no

Figure 13: When restricted to two features (age, priors), CORELS (λ = 0.005) ﬁnds the

same rule list across 10 cross-validation folds.

ﬁnds achieve test accuracy that is again comparable to COMPAS. CORELS identiﬁes the
same rule list across all 10 folds of 10-fold cross-validation experiments (Figure 13). In work
subsequent to ours (Angelino et al., 2017), Dressel and Farid (2018) conﬁrmed this result,
in the sense that they used logistic regression to construct a linear classiﬁer with age and
prior convictions, and also achieved similar accuracy to COMPAS. However, computing a
logistic regression model requires multiplication and addition, and their model cannot easily
be computed, in the sense that it requires a calculator (and thus is potentially error-prone).
Our rule lists require no such computation.

47

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

CORELS with diﬀerent regularization parameters (NYCLU stop-and-frisk data set)

Total
time (s)
.61 (.03)
70 (6)
1600 (100)

Time to
optimum (s)
.002 (.001)
.008 (.002)
56 (74)

Lower bound
evaluations (×106)
.070 (.004)
7.5 (.6)
150 (10)

Max evaluated
preﬁx length
6
11
16-17
Total queue
insertions (×103)
2.2 (.1)
210 (20)
4400 (300)

Optimal
preﬁx length
2
3
6-10
Max queue
size (×103)
.9 (.1)
130 (10)
2500 (170)

λ
.04
.01
.0025

λ
.04
.01
.0025

Table 3: Summary of CORELS executions,

for the NYCLU stop-and-frisk data set
(M = 46), for same three regularization parameter (λ) values as in Figure 12.
The columns report the total execution time, time to optimum, maximum evalu-
ated preﬁx length, optimal preﬁx length, number of times we completely evaluate
a preﬁx dp’s lower bound b(dp, x, y), total number of queue insertions (this number
is equal to the number of cache insertions), and the maximum queue size. For pre-
ﬁx lengths, we report single values or ranges corresponding to the minimum and
maximum observed values; in the other columns, we report means (and standard
deviations) over 10 cross-validation folds. See also Figures 14 and 15.

6.7 CORELS Execution Traces, for Diﬀerent Regularization Parameters

In this section, we illustrate several views of CORELS execution traces, for the NYCLU stop-
and-frisk data set with M = 46 antecedents, for the same three regularization parameters
(λ = .04, .01, .025) as in Figure 12.

Table 3 summarizes execution traces across all 10 cross-validation folds. For each value
of λ, CORELS achieves the optimum in a small fraction of the total execution time. As λ
decreases, these times increase because the search problems become more diﬃcult, as is
summarized by the observation that CORELS must evaluate longer preﬁxes; consequently,
our data structures grow in size. We report the total number of elements inserted into the
queue and the maximum queue size; recall from §5 that the queue elements correspond to
the trie’s leaves, and that the symmetry-aware map elements correspond to the trie’s nodes.
The upper panels in Figure 14 plot example execution traces, from a single cross-
validation fold, of both the current best objective value Rc and the lower bound b(dp, x, y)
of the preﬁx dp being evaluated. These plots illustrate that CORELS certiﬁes optimality
when the lower bound matches the objective value. The lower panels in Figure 14 plot corre-
sponding traces of an upper bound on the size of the remaining search space (Theorem 7),
and illustrate that as λ decreases, it becomes more diﬃcult to eliminate regions of the
search space. For Figure 14, we dynamically and incrementally calculate (cid:98)log10 Γ(Rc, Q)(cid:99),
which adds some computational overhead; we do not calculate this elsewhere unless noted.
Figure 15 visualizes the elements in CORELS’ logical queue, for each of the executions in
Figure 14. Recall from §5.5 that the logical queue corresponds to elements in the (physical)

48

Learning Certifiably Optimal Rule Lists

Figure 14: Example executions of CORELS,

for the NYCLU stop-and-frisk data set
(M = 46). See also Table 3 and Figure 15. Top: Objective value (solid line)
and lower bound (dashed line) for CORELS, as a function of wall clock time (log
scale). Numbered points along the trace of the objective value indicate when the
length of the best known rule list changes and are labeled by the new length.
For each value of λ, a star marks the optimum objective value and time at which
it was achieved. Bottom: (cid:98)log10 Γ(Rc, Q)(cid:99), as a function of wall clock time (log
scale), where Γ(Rc, Q) is the upper bound on remaining search space size (Theo-
rem 7). Rightmost panels: For visual comparison, we overlay the execution traces
from the panels to the left, for the three diﬀerent values of λ.

queue that have not been garbage collected from the trie; these are preﬁxes that CORELS
has already evaluated and whose children the algorithm plans to evaluate next. As an
execution progresses, longer preﬁxes are placed in the queue; as λ decreases, the algorithm
must spend more time evaluating longer and longer preﬁxes.

6.8 Eﬃcacy of CORELS Algorithm Optimizations

This section examines the eﬃcacy of each of our bounds and data structure optimizations.
We remove a single bound or data structure optimization from our ﬁnal implementation
and measure how the performance of our algorithm changes. We examine these performance
traces on both the NYCLU and the ProPublica data sets, and highlight the result that on
diﬀerent problems, the relative performance improvements of our optimizations can vary.

Table 4 provides summary statistics for experiments using the full CORELS implementa-
tion (ﬁrst row) and ﬁve variants (subsequent rows) that each remove a speciﬁc optimization:
(1) Instead of a priority queue (§5.2) ordered by the objective lower bound, we use a queue
that implements breadth-ﬁrst search (BFS). (2) We remove checks that would trigger prun-
ing via our lower bounds on antecedent support (Theorem 10) and accurate antecedent
support (Theorem 11). (3) We remove the eﬀect of our lookahead bound (Lemma 2), which
otherwise tightens the objective lower bound by an amount equal to the regularization pa-
rameter λ. (4) We disable the symmetry-aware map (§5.3), our data structure that enables
pruning triggered by the permutation bound (Corollary 16). (5) We do not identify sets
of equivalent points, which we otherwise use to tighten the objective lower bound via the
equivalent points bound (Theorem 20).

49

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 15: Summary of CORELS’ logical queue, for the NYCLU stop-and-frisk data set
(M = 46), for same three regularization parameters as in Figure 12 and Table 3.
Solid lines plot the numbers of preﬁxes in the logical queue (log scale), colored by
length (legend), as a function of wall clock time (log scale). All plots are generated
using a single, representative cross-validation training set. For each execution,
the gray shading ﬁlls in the area beneath the total number of queue elements,
i.e., the sum over all lengths; we also annotate the total time in seconds, marked
with a dashed vertical line.

Removing any single optimization increases total execution time, by varying amounts
across these optimizations. Similar to our experiments in §6.7, we always encounter the
optimal rule list in far less time than it takes to certify optimality. As in Table 3, we report
metrics that are all proxies for how much computational work our algorithm must perform;
these metrics thus scale with the overall slowdown with respect to CORELS execution time
(Table 4, ﬁrst column).

Figure 16 visualizes execution traces of the elements in CORELS’ logical queue, similar
to Figure 15, for a single, representative cross-validation fold. Panels correspond to diﬀerent
removed optimizations, as in Table 4. These plots demonstrate that our optimizations reduce
the number of evaluated preﬁxes and are especially eﬀective at limiting the number of longer
evaluated preﬁxes. For the ProPublica data set, the most important optimization is the
equivalent points bound—without it, we place preﬁxes of at least length 10 in our queue,
and must terminate these executions before they are complete. In contrast, CORELS and
most other variants evaluate only preﬁxes up to at most length 5, except for the variant
without the lookahead bound, which evaluates preﬁxes up to length 6.

Table 5 and Figure 17 summarize an analogous set of experiments for the NYCLU data
set. Note that while the equivalent points bound proved to be the most important opti-
mization for the ProPublica data set, the symmetry-aware map is the crucial optimization
for the NYCLU data set.

Finally, Figure 18 highlights the most signiﬁcant algorithm optimizations for our pre-
diction problems: the equivalent points bound for the ProPublica data set (left) and the

50

Learning Certifiably Optimal Rule Lists

Per-component performance improvement (ProPublica data set)

Slow-
down
—
1.1×
1.5×
13.3×
8.4×

Total time
(min)
.98 (.6)
1.03 (.6)
1.5 (.9)
12.3 (6.2)
9.1 (6.4)

Time to
optimum (s)
1 (1)
2 (4)
1 (2)
1 (1)
2 (3)

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound* >130 (2.6) >180× >1400 (2000)
Total queue
insertions (×106)
.29 (.2)
.33 (.2)
.40 (.2)
3.6 (1.8)
2.5 (1.7)
>510 (1.1)

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound*

Lower bound
evaluations (×106)
26 (15)
27 (16)
42 (25)
320 (160)
250 (180)

>940 (5)

Max evaluated
preﬁx length
5
5
5
6
5
≥11
Max queue
size (×106)
.24 (.1)
.20 (.1)
.33 (.2)
3.0 (1.5)
2.4 (1.7)
>500 (1.2)

Table 4: Per-component performance improvement, for the ProPublica data set (λ = 0.005,
M = 122). The columns report the total execution time, time to optimum, maxi-
mum evaluated preﬁx length, number of times we completely evaluate a preﬁx dp’s
lower bound b(dp, x, y), total number of queue insertions (which is equal to the
number of cache insertions), and maximum logical queue size. The ﬁrst row shows
CORELS; subsequent rows show variants that each remove a speciﬁc implemen-
tation optimization or bound. (We are not measuring the cumulative eﬀects of
removing a sequence of components.) All rows represent complete executions that
certify optimality, except those labeled ‘No equivalent points bound,’ for which
each execution was terminated due to memory constraints, once the size of the
cache reached 5 × 108 elements, after consuming ∼250GB RAM. In all but the
ﬁnal row and column, we report means (and standard deviations) over 10 cross-
validation folds. We also report the mean slowdown in total execution time, with
respect to CORELS. In the ﬁnal row, we report the mean (and standard deviation)
of the incomplete execution time and corresponding slowdown, and a lower bound
on the mean time to optimum; in the remaining ﬁelds, we report minimum values
across folds. See also Figure 16.
* Only 7 out of 10 folds achieve the optimum before being terminated.

symmetry-aware map for the NYCLU data set (right). For CORELS (thin lines) with
the ProPublica recidivism data set (left), the objective drops quickly, achieving the op-
timal value within a second. CORELS certiﬁes optimality in about a minute—the objective
lower bound steadily converges to the optimal objective (top) as the search space shrinks
(bottom). As in Figure 14, we dynamically and incrementally calculate (cid:98)log10 Γ(Rc, Q)(cid:99),
where Γ(Rc, Q) is the upper bound (13) on remaining search space size (Theorem 7); this
adds some computational overhead. In the same plots (left), we additionally highlight a

51

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 16: Summary of the logical queue’s contents, for full CORELS (top left) and ﬁve
variants that each remove a speciﬁc implementation optimization or bound, for
the ProPublica data set (λ = 0.005, M = 122). See also Table 4. Solid lines plot
the numbers of preﬁxes in the logical queue (log scale), colored by length (legend),
as a function of wall clock time (log scale). All plots are generated using a single,
representative cross-validation training set. The gray shading ﬁlls in the area
beneath the total number of queue elements for CORELS, i.e., the sum over
all lengths in the top left ﬁgure. For comparison, we replicate the same gray
region in the other ﬁve subﬁgures. For each execution, we indicate the total time
in seconds, relative to the full CORELS implementation (T = 21 s), and with a
dashed vertical line. The execution without the equivalent points bound (bottom
right) is incomplete.

separate execution of CORELS without the equivalent points bound (Theorem 20) (thick
lines). After more than 2 hours, the execution is still far from complete; in particular, the
lower bound is far from the optimum objective value (top) and much of the search space
remains unexplored (bottom). For the NYCLU stop-and-frisk data set (right), CORELS
achieves the optimum objective in well under a second, and certiﬁes optimality in a lit-
tle over a minute. CORELS without the permutation bound (Corollary 16), and thus the
symmetry-aware map, requires more than an hour, i.e., orders of magnitude more time, to
complete (thick lines).

52

Learning Certifiably Optimal Rule Lists

Per-component performance improvement (NYCLU stop-and-frisk data set)

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound

Slow-
down
—
2.0×
1.1×
1.6×

Time to
optimum (µs)
8.9 (.1)
110 (10)
8.8 (.8)
7.3 (1.8)

Total
time (min)
1.1 (.1)
2.2 (.2)
1.2 (.1)
1.7 (.2)
> 73 (5)
4 (.3)

3.8×

> 68× > 7.6 (.4)
6.4 (.9)
Total queue
insertions (×105)
2.0 (.2)
4.1 (.4)
2.1 (.2)
3.2 (.3)
> 1000 (0)
9.4 (.7)

Lower bound
evaluations (×106)
7 (1)
14 (1)
8 (1)
11 (1)
> 390 (40)
33 (2)

Max evaluated
preﬁx length
11
11
11
11-12
> 10
14
Max queue
size (×105)
1.3 (.1)
1.4 (.1)
1.3 (.1)
2.1 (.2)
> 900 (10)
6.0 (.4)

Table 5: Per-component performance improvement, as in Table 4, for the NYCLU stop-and-
frisk data set (λ = 0.01, M = 46). All rows except those labeled ‘No symmetry-
aware map’ represent complete executions. A single fold running without a
symmetry-aware map required over 2 days to complete, so in order to run all
10 folds above, we terminated execution after the preﬁx tree (§5.1) reached 108
nodes. See Table 4 for a detailed caption, and also Figure 17.

Algorithmic
approach
CORELS
Brute force
Brute force
CORELS (1984)

Max evaluated Lower bound
evaluations
preﬁx length
2.8 × 107
5
2.5 × 1010
5
5.0 × 1020
10
2.8 × 107
5

Predicted runtime

36 seconds

9.0 hours ≈ 3.3 × 104 s
21 × 106 years ≈ 6.5 × 1014 s
13.5 days ≈ 1.2 × 106 s

Table 6: Algorithmic speedup for the ProPublica data set (λ = 0.005, M = 122). Solving
this problem using brute force is impractical due to the inability to explore rule
lists of reasonable lengths. Removing only the equivalent points bound requires
exploring preﬁxes of up to length 10 (see Table 4), a clearly intractable problem.
Even with all of our improvements, however, it is only recently that processors
have been fast enough for this type of discrete optimization algorithm to succeed.

6.9 Algorithmic Speedup

Table 6 shows the overall speedup of CORELS compared to a na¨ıve implementation and
demonstrates the infeasibility of running our algorithm 30 years ago. Consider an execution
of CORELS for the ProPublica data set, with M = 122 antecedents, that evaluates preﬁxes

53

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 17: Summary of the logical queue’s contents, for full CORELS (top left) and ﬁve
variants that each remove a speciﬁc implementation optimization or bound, for
the NYCLU stop-and-frisk data set (λ = 0.01, M = 46), as in Table 5. The ex-
ecution without the symmetry-aware map (bottom center) is incomplete. See
Figure 16 for a detailed caption.

up to length 5 in order to certify optimality (Table 4). A brute force implementation that
na¨ıvely considers all preﬁxes of up to length 5 would evaluate 2.5 × 1010 preﬁxes. As shown
in Figure 4, the optimal rule list has preﬁx length 3, thus the brute force algorithm would
identify the optimal rule list. However, for this approach to certify optimality, it would
have to consider far longer preﬁxes. Without our equivalent points bound, but with all of
our other optimizations, we evaluate preﬁxes up to at least length 10 (see Table 4 and
Figure 16)—thus a brute force algorithm would have to evaluate preﬁxes of length 10 or
longer. Na¨ıvely evaluating all preﬁxes up to length 10 would require looking at 5.0 × 1020
diﬀerent preﬁxes.

However, CORELS examines only 28 million preﬁxes in total—a reduction of 893×
compared to examining all preﬁxes up to length 5 and a reduction of 1.8 × 1013 for the case
of length 10. On a laptop, we require about 1.3 µs to evaluate a single preﬁx (given by
dividing the number of lower bound evaluations by the total time in Table 4). Our runtime
is only about 36 seconds, but the na¨ıve solutions of examining all preﬁxes up to lengths 5

54

Learning Certifiably Optimal Rule Lists

Figure 18: Execution progress of CORELS and selected variants,

for the ProPublica
(λ = 0.005, M = 122) (left) and NYCLU (λ = 0.01, M = 46) (right) data sets.
Top: Objective value (thin solid lines) and lower bound (dashed lines) for
CORELS, as a function of wall clock time (log scale). Numbered points along the
trace of the objective value indicate when the length of the best known rule list
changes, and are labeled by the new length. CORELS quickly achieves the opti-
mal value (star markers), and certiﬁes optimality when the lower bound matches
the objective value. On the left, a separate and signiﬁcantly longer execution of
CORELS without the equivalent points (Theorem 20) bound remains far from
complete, and its lower bound (thick solid line) far from the optimum. On the
right, a separate execution of CORELS without the permutation bound (Corol-
lary 16), and thus the symmetry-aware map, requires orders of magnitude more
time to complete. Bottom: (cid:98)log10 Γ(Rc, Q)(cid:99), as a function of wall clock time (log
scale), where Γ(Rc, Q) is the upper bound (13) on remaining search space size
(Theorem 7). For these problems, the equivalent points (left) and permutation
(right) bounds are responsible for the ability of CORELS to quickly eliminate
most of the search space (thin solid lines); the remaining search space decays
much more slowly without these bounds (thick solid lines).

and 10 would take 9 hours and 21 million years, respectively. It is clear that brute force
would not scale to larger problems.

We compare our current computing circumstances to those of 1984, the year when CART
was published. Moore’s law holds that computing power doubled every 18 months from 1984
to 2006. This is a period of 264 months, which means computing power has gone up by at
least a factor of 32,000 since 1984. Thus, even with our algorithmic and data structural
improvements, CORELS would have required about 13.5 days in 1984—an unreasonable
amount of time. Our advances are meaningful only because we can run them on a modern

55

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

system. Combining our algorithmic improvements with the increase in modern processor
speeds, our algorithm runs more than 1013 times faster than a na¨ıve implementation would
have in 1984. This helps explain why neither our algorithm, nor other branch-and-bound
variants, had been developed before now.

7. Summary and Future Work on Bounds

Here, we highlight our most signiﬁcant bounds, as well as directions for future work based
on bounds that we have yet to leverage in practice.

In empirical studies, we found our equivalent support (§3.10, Theorem 15) and equiv-
alent points (§3.14, Theorem 20) bounds to yield the most signiﬁcant improvements in
algorithm performance. In fact, they sometimes proved critical for ﬁnding solutions and
proving optimality, even on small problems.

Accordingly, we would hope that our similar support bound (§3.13, Theorem 18) could
be useful; understanding how to eﬃciently exploit this result in practice represents an
important direction for future work. In particular, this type of bound may lead to principled
approximate variants of our approach.

We presented several sets of bounds in which at least one bound was strictly tighter than
the other(s). For example, the lower bound on accurate antecedent support (Theorem 11)
is strictly tighter than the lower bound on accurate support (Theorem 10). It might seem
that we should only use this tighter bound, but in practice, we can use both—the looser
bound can be checked before completing the calculation required to check the tighter bound.
Similarly, the equivalent support bound (Theorem 15) is more general than the special case
of the permutation bound (Corollary 16). We have implemented data structures, which we
call symmetry-aware maps, to support both of these bounds, but have not yet identiﬁed an
eﬃcient approach for supporting the more general equivalent points bound. A good solution
may be related to the challenge of designing an eﬃcient data structure to support the similar
support bound.

We also presented results on antecedent rejection that unify our understanding of our
lower (§3.7) and upper bounds (§3.8) on antecedent support. In a preliminary implemen-
tation (not described here), we experimented with special data structures to support the
direct use of our observation that antecedent rejection propagates (§3.9, Theorem 12). We
leave the design of eﬃcient data structures for this task as future work.

During execution, we ﬁnd it useful to calculate an upper bound on the size of the
remaining search space—e.g., via Theorem 7, or the looser Proposition 9, which incurs
less computational overhead—since these provide meaningful information about algorithm
progress and allow us to estimate the remaining execution time. As we illustrated in Sec-
tion 6.8, these calculations also help us qualify the impact of diﬀerent algorithmic bounds,
e.g., by comparing executions that keep or remove bounds.

When our algorithm terminates, it outputs an optimal solution of the training optimiza-
tion problem, with a certiﬁcate of optimality. On a practical note, our approach can also
provide useful results even for incomplete executions. As shown earlier, we have empirically
observed that our algorithm often identiﬁes the optimal rule list very quickly, compared to
the total time required to prove optimality, e.g., in seconds, versus minutes, respectively.
Furthermore, our objective’s lower bounds allow us to place an upper bound on the size of

56

Learning Certifiably Optimal Rule Lists

the remaining search space, and provides guarantees on the quality of a solution output by
an incomplete execution.

The order in which we evaluate preﬁxes can impact the rate at which we prune the search
space, and thus the total runtime. We think that it is possible to design search policies that
signiﬁcantly improve performance.

8. Conclusion and More Possible Directions for Future Work

Finally, we would like to clarify some limitations of CORELS. As far as we can tell, CORELS
is the current best algorithm for solving a specialized optimal decision tree problem. While
our approach scales well to large numbers of observations, it could have diﬃculty proving
optimality for problems with many possibly relevant features that are highly correlated,
when large regions of the search space might be challenging to exclude.

CORELS is not designed for raw image processing or other problems where the features
themselves are not interpretable. It could instead be used as a ﬁnal classiﬁer for image
processing problems where the features were created beforehand; for instance, one could
create classiﬁers for each part of an image, and use CORELS to create a ﬁnal combined
classiﬁer. The notions of interpretability used in image classiﬁcation tend to be completely
diﬀerent from those for structured data where each feature is separately meaningful (e.g.,
see Li et al., 2018). For structured data, decision trees, along with scoring systems, tend
to be popular forms of transparent models. Scoring systems are sparse linear models with
integer coeﬃcients, and they can also be created from data (Ustun and Rudin, 2017, 2016).

In some of our experiments, CORELS produces a DNF formula by coincidence, but it
might be possible to create a much simpler version of CORELS that only produces DNF
formulae. This could build oﬀ previous algorithms for creating an optimal DNF formula
(Rijnbeek and Kors, 2010; Wang et al., 2016, 2017).

CORELS does not automatically rank the subgroups in order of the likelihood of a
positive outcome; doing so would require an algorithm such as Falling Rule Lists (Wang
and Rudin, 2015a; Chen and Rudin, 2018), which forces the estimated probabilities to de-
crease along the list. Furthermore, while CORELS does not technically produce estimates
of P(Y = 1 | x), one could form such an estimate by computing the empirical proportion
ˆP(Y = 1 | x obeys pk) for each antecedent pk. CORELS is also not designed to assist with
causal inference applications, since it does not estimate the eﬀect of a treatment via the con-
ditional diﬀerence P(Y = 1 | treatment = True, x) − P(Y = 1 | treatment = False, x). Alter-
native algorithms that estimate conditional diﬀerences with interpretable rule lists include
Causal Falling Rule Lists (Wang and Rudin, 2015b), Cost-Eﬀective Interpretable Treatment
Regimes (CITR) (Lakkaraju and Rudin, 2017), and an approach by Zhang et al. (2015) for
constructing interpretable and parsimonious treatment regimes. Alternatively, one could
use a complex machine learning model to predict outcomes for the treatment group and a
separate complex model for the control group that would allow counterfactuals to be esti-
mated for each observation; from there, CORELS could be applied to produce a transparent
model for personalized treatment eﬀects. A similar approach to this was taken by Goh and
Rudin (2018), who use CORELS to understand a black box causal model.

57

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

CORELS could be adapted to handle cost-sensitive learning or weighted regularization.
This would require creating more general versions of our theorems, which would be an
extension of this work.

While CORELS does not directly handle continuous variables, we have found that it is
not diﬃcult in practice to construct a rule set that is suﬃcient for creating a useful model.
It may be possible to use techniques such as Fast Boxes (Goh and Rudin, 2014) to discover
useful and interpretable rules for continuous data that can be used within CORELS.

An interesting direction for future research would be to create a hybrid interpretable/
black box model in the style of Wang (2018), where the rule list would eliminate large
parts of the space away from the decision boundary, and the observations that remain are
evaluated by a black box model rather than a default rule.

Lastly, CORELS does not create generic single-variable-split decision trees. CORELS
optimizes over rule lists, which are one-sided decision trees; in our setting, the leaves of
these ‘trees’ are conjunctions. It may be possible to generalize ideas from our approach to
handle generic decision trees, which could be an interesting project for future work. There
are more symmetries to handle in that case, since there would be many equivalent decision
trees, leading to challenges in developing symmetry-aware data structures.

Acknowledgments

E.A. conducted most of this work while supported by the Miller Institute for Basic Research
in Science, University of California, Berkeley, and hosted by Prof. M.I. Jordan at RISELab.
C.D.R. is supported in part by MIT-Lincoln Labs and the National Science Foundation
under IIS-1053407. E.A. would like to thank E. Jonas, E. Kohler, and S. Tu for early
implementation guidance, A. D’Amour for pointing out the work by Goel et al. (2016),
V. Kanade, S. McCurdy, J. Schleier-Smith and E. Thewalt for helpful conversations, and
members of RISELab, SAIL, and the UC Berkeley Database Group for their support and
feedback. We thank H. Yang and B. Letham for sharing advice and code for processing data
and mining rules, B. Coker for his critical advice on using the ProPublica COMPAS data set,
as well as V. Kaxiras and A. Saligrama for their recent contributions to our implementation
and for creating the CORELS website. We are very grateful to our editor and anonymous
reviewers.

Appendix A. Excessive Antecedent Support

Theorem 21 (Upper bound on antecedent support) Let d∗ = (dp, δp, q0, K) be any op-
timal rule list with objective R∗, i.e., d∗ ∈ argmind R(d, x, y), and let dp = (p1, . . . , pk−1,
pk, . . . , pK) be its preﬁx. For each k ≤ K, antecedent pk in dp has support less than or equal
to the fraction of all data not captured by preceding antecedents, by an amount greater than
the regularization parameter λ:

supp(pk, x | dp) ≤ 1 − supp(dk−1

p

, x) − λ,

(37)

where dk−1
that there also exists a shorter optimal rule list d(cid:48) = (dK−1

p = (p1, . . . , pk−1). For the last antecedent, i.e., when pk = pK, equality implies

0, K − 1) ∈ argmind R(d, x, y).

p, q(cid:48)

, δ(cid:48)

p

58

Learning Certifiably Optimal Rule Lists

Proof First, we focus on the last antecedent pK+1 in a rule list d(cid:48). Let d = (dp, δp, q0, K)
be a rule list with preﬁx dp = (p1, . . . , pK) and objective R(d, x, y) ≥ R∗, where R∗ ≡
minD R(D, x, y) is the optimal objective. Let d(cid:48) = (d(cid:48)
0, K + 1) be a rule list whose
preﬁx d(cid:48)
p = (p1, . . . , pK, pK+1) starts with dp and ends with a new antecedent pK+1. Sup-
pose pK+1 in the context of d(cid:48)
p captures nearly all data not captured by dp, except for a
fraction (cid:15) upper bounded by the regularization parameter λ:

p, δ(cid:48)

p, q(cid:48)

1 − supp(dp, x) − supp(pK+1, x | d(cid:48)

p) ≡ (cid:15) ≤ λ.

Since d(cid:48)
p starts with dp, its preﬁx misclassiﬁcation error is at least as great; the only discrep-
ancy between the misclassiﬁcation errors of d and d(cid:48) can come from the diﬀerence between
the support of the set of data not captured by dp and the support of pK+1:

|(cid:96)(d(cid:48), x, y) − (cid:96)(d, x, y)| ≤ 1 − supp(dp, x) − supp(pK+1, x | d(cid:48)

p) = (cid:15).

The best outcome for d(cid:48) would occur if its misclassiﬁcation error were smaller than that
of d by (cid:15), therefore

R(d(cid:48), x, y) = (cid:96)(d(cid:48), x, y) + λ(K + 1)

≥ (cid:96)(d, x, y) − (cid:15) + λ(K + 1) = R(d, x, y) − (cid:15) + λ ≥ R(d, x, y) ≥ R∗.

d(cid:48) is an optimal rule list, i.e., d(cid:48) ∈ argminD R(D, x, y), if and only if R(d(cid:48), x, y) = R(d, x, y) =
R∗, which requires (cid:15) = λ. Otherwise, (cid:15) < λ, in which case

R(d(cid:48), x, y) ≥ R(d, x, y) − (cid:15) + λ > R(d, x, y) ≥ R∗,

therefore d(cid:48) is not optimal, i.e., d(cid:48) /∈ argminD R(D, x, y). This demonstrates the desired
result for k = K.

In the remainder, we prove the bound in (37) by contradiction, in the context of a rule
list d(cid:48)(cid:48). Let d and d(cid:48) retain their deﬁnitions from above, thus as before, that the data not
captured by d(cid:48)

p has normalized support (cid:15) ≤ λ, i.e.,

1 − supp(d(cid:48)

p, x) = 1 − supp(dp, x) − supp(pK+1, x | d(cid:48)

p) = (cid:15) ≤ λ.

Thus for any rule list d(cid:48)(cid:48) whose preﬁx d(cid:48)(cid:48)
p and ends
with one or more additional rules, each additional rule pk has support supp(pk, x | d(cid:48)(cid:48)
p) ≤
(cid:15) ≤ λ, for all k > K + 1. By Theorem 10, all of the additional rules have insuﬃcient sup-
port, therefore d(cid:48)(cid:48)

p = (p1, . . . , pK+1, . . . , pK(cid:48)) starts with d(cid:48)

p cannot be optimal, i.e., d(cid:48)(cid:48) /∈ argminD R(D, x, y).

Similar to Theorem 10, our lower bound on antecedent support, we can apply Theo-
rem 21 in the contexts of both constructing rule lists and rule mining (§3.1). Theorem 21
implies that if we only seek a single optimal rule list, then during branch-and-bound ex-
ecution, we can prune a preﬁx if we ever add an antecedent with support too similar to
the support of the set of data not captured by the preceding antecedents. One way to view
this result is that if d = (dp, δp, q0, K) and d(cid:48) = (d(cid:48)
0, K + 1) are rule lists such that d(cid:48)
p
starts with dp and ends with an antecedent that captures all or nearly all data not captured
by dp, then the new rule in d(cid:48) behaves similar to the default rule of d. As a result, the
misclassiﬁcation error of d(cid:48) must be similar to that of d, and any reduction may not be
suﬃcient to oﬀset the penalty for longer preﬁxes.

p, δ(cid:48)

p, q(cid:48)

59

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Proposition 22 (Excessive antecedent support propagates) Deﬁne φ(dp) as in (19),
and let dp = (p1, . . . , pK) be a preﬁx, such that its last antecedent pK has excessive support,
i.e., the opposite of the bound in (37):

supp(pK, x | dp) > 1 − supp(dK−1

p

, x) − λ,

p

where dK−1
= (p1, . . . , pK−1). Let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp =
(P1, . . . , Pκ) such that Dp starts with DK(cid:48)−1
) and PK(cid:48) = pK. It
follows that PK(cid:48) has excessive support in preﬁx Dp, and furthermore, D /∈ argmind R(d, x, y).

= (P1, . . . , PK(cid:48)−1) ∈ φ(dK−1

p

p

Proof Since DK(cid:48)

p = (P1, . . . , PK(cid:48)) contains all the antecedents in dp, we have that

supp(DK(cid:48)

p , x) ≥ supp(dp, x).

Expanding these two terms gives

supp(DK(cid:48)

p , x) = supp(DK(cid:48)−1

p

, x) + supp(PK(cid:48), x | Dp)

≥ supp(dp, x) = supp(dK−1

p

, x) + supp(pK, x | dp) > 1 − λ.

Rearranging gives

supp(PK(cid:48), x | Dp) > 1 − supp(DK(cid:48)−1

p

, x) − λ,

thus PK(cid:48) has excessive support in Dp. By Theorem 21, D /∈ argmind R(d, x, y).

Appendix B. Proof of Theorem 15 (Equivalent Support Bound)

We begin by deﬁning four related rule lists. First, let d = (dp, δp, q0, K) be a rule list with
preﬁx dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK). Second, let D = (Dp, ∆p, Q0, κ) be
a rule list with preﬁx Dp = (P1, . . . , Pκ) that captures the same data as dp, and labels
0, K(cid:48)) ∈ σ(dp) be any rule list whose preﬁx
∆p = (Q1, . . . , Qκ). Third, let d(cid:48) = (d(cid:48)
starts with dp, such that K(cid:48) ≥ K. Denote the preﬁx and labels of d(cid:48) by d(cid:48)
p = (p1, . . . , pK,
0, κ(cid:48)) ∈
p, Q(cid:48)
p, ∆(cid:48)
pK+1, . . . , pK(cid:48)) and δp = (q1, . . . , qK(cid:48)), respectively. Finally, deﬁne D(cid:48) = (D(cid:48)
σ(Dp) to be the ‘analogous’ rule list, i.e., whose preﬁx D(cid:48)
p = (P1, . . . , Pκ, Pκ+1, . . . , Pκ(cid:48)) =
(P1, . . . , Pκ, pK+1, . . . , pK(cid:48)) starts with Dp and ends with the same K(cid:48) − K antecedents
as d(cid:48)

p. Let ∆(cid:48)
Next, we claim that the diﬀerence in the objectives of rule lists d(cid:48) and d is the same as
the diﬀerence in the objectives of rule lists D(cid:48) and D. Let us expand the ﬁrst diﬀerence as

p = (Q1, . . . , Qκ(cid:48)) denote the labels of D(cid:48).

p, δ(cid:48)

p, q(cid:48)

R(d(cid:48), x, y) − R(d, x, y) = (cid:96)(d(cid:48), x, y) + λK(cid:48) − (cid:96)(d, x, y) − λK
= (cid:96)p(d(cid:48)

p, x, y) + (cid:96)0(d(cid:48)

0, x, y) − (cid:96)p(dp, δp, x, y) − (cid:96)0(dp, q0, x, y) + λ(K(cid:48) − K).

p, q(cid:48)

p, δ(cid:48)

Similarly, let us expand the second diﬀerence as

R(D(cid:48), x, y) − R(D, x, y) = (cid:96)(D(cid:48), x, y) + λκ(cid:48) − (cid:96)(D, x, y) − λκ
= (cid:96)p(D(cid:48)

p, x, y) + (cid:96)0(D(cid:48)

p, ∆(cid:48)

p, Q(cid:48)

0, x, y) − (cid:96)p(Dp, ∆p, x, y) − (cid:96)0(Dp, Q0, x, y) + λ(K(cid:48) − K),

60

Learning Certifiably Optimal Rule Lists

where we have used the fact that κ(cid:48) − κ = K(cid:48) − K.

The preﬁxes dp and Dp capture the same data. Equivalently, the set of data that is not

captured by dp is the same as the set of data that is not captured by Dp, i.e.,

{xn : ¬ cap(xn, dp)} = {xn : ¬ cap(xn, Dp)}.

Thus, the corresponding rule lists d and D share the same default rule, i.e., q0 = Q0, yielding
the same default rule misclassiﬁcation error:

Similarly, preﬁxes d(cid:48)
same default rule misclassiﬁcation error:

p and D(cid:48)

p capture the same data, and thus rule lists d(cid:48) and D(cid:48) have the

(cid:96)0(dp, q0, x, y) = (cid:96)0(Dp, Q0, x, y).

(cid:96)0(dp, q0, x, y) = (cid:96)0(Dp, Q0, x, y).

At this point, to demonstrate our claim relating the objectives of d, d(cid:48), D, and D(cid:48), what
p and dp is

remains is to show that the diﬀerence in the misclassiﬁcation errors of preﬁxes d(cid:48)
the same as that between D(cid:48)

p and Dp. We can expand the ﬁrst diﬀerence as

(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) − (cid:96)p(dp, δp, x, y) =

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn],

where we have used the fact that since d(cid:48)
p starts with dp, the ﬁrst K rules in d(cid:48)
same mistakes as those in dp. Similarly, we can expand the second diﬀerence as

p make the

(cid:96)p(D(cid:48)

p, ∆(cid:48)

p, x, y) − (cid:96)p(Dp, ∆p, x, y) =

cap(xn, Pk | D(cid:48)

p) ∧ 1[Qk (cid:54)= yn]

cap(xn, pk | D(cid:48)

p) ∧ 1[Qk (cid:54)= yn]

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn]

(38)

n=1
p, δ(cid:48)

k=K+1
p, x, y) − (cid:96)p(dp, δp, x, y).

= (cid:96)p(d(cid:48)

To justify the equality in (38), we observe ﬁrst that preﬁxes D(cid:48)
p start with κ and K
antecedents, respectively, that capture the same data. Second, preﬁxes D(cid:48)
p end with
exactly the same ordered list of K(cid:48) − K antecedents, therefore for any k = 1, . . . , K(cid:48) − K,
antecedent Pκ+k = pK+k in D(cid:48)
p. It follows that
the corresponding labels are all equivalent, i.e., Qκ+k = qK+k, for all k = 1, . . . , K(cid:48) − K, and
consequently, the preﬁx misclassiﬁcation error associated with the last K(cid:48) − K antecedents
of d(cid:48)
p. We have therefore shown that the diﬀerence between the
objectives of d(cid:48) and d is the same as that between D(cid:48) and D, i.e.,

p captures the same data as pK+k captures in d(cid:48)

p is the same as that of D(cid:48)

p and d(cid:48)

p and d(cid:48)

R(d(cid:48), x, y) − R(d, x, y) = R(D(cid:48), x, y) − R(D, x, y).

(39)

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K+1

N
(cid:88)

κ(cid:48)
(cid:88)

n=1

N
(cid:88)

k=κ+1
K(cid:48)
(cid:88)

n=1

N
(cid:88)

k=K+1
K(cid:48)
(cid:88)

1
N

1
N

1
N

=

=

61

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Next, suppose that the objective lower bounds of d and D obey b(dp, x, y) ≤ b(Dp, x, y),

therefore

R(d, x, y) = (cid:96)p(dp, δp, x, y) + (cid:96)0(dp, q0, x, y) + λK
= b(dp, x, y) + (cid:96)0(dp, q0, x, y)
≤ b(Dp, x, y) + (cid:96)0(dp, q0, x, y) = b(Dp, x, y) + (cid:96)0(Dp, Q0, x, y) = R(D, x, y).

Now let d∗ be an optimal rule list with preﬁx constrained to start with dp,

d∗ ∈ argmin
d†∈σ(dp)

R(d†, x, y),

and let K∗ be the length of d∗. Let D∗ be the analogous κ∗-rule list whose preﬁx starts
with Dp and ends with the same K∗ − K antecedents as d∗, where κ∗ = κ + K∗ − K.
By (39),

R(d∗, x, y) − R(d, x, y) = R(D∗, x, y) − R(D, x, y).

Furthermore, we claim that D∗ is an optimal rule list with preﬁx constrained to start
with Dp,

(40)

(41)

(42)

D∗ ∈ argmin
D†∈σ(Dp)

R(D†, x, y).

To demonstrate (42), we consider two separate scenarios. In the ﬁrst scenario, preﬁxes dp
and Dp are composed of the same antecedents, i.e., the two preﬁxes are equivalent up to a
permutation of their antecedents, and as a consequence, κ = K and κ∗ = K∗. Here, every
rule list d(cid:48)(cid:48) ∈ σ(dp) that starts with dp has an analogue D(cid:48)(cid:48) ∈ σ(Dp) that starts with Dp,
such that d(cid:48)(cid:48) and D(cid:48)(cid:48) obey (39), and vice versa, and thus (42) is a direct consequence of (41).
In the second scenario, preﬁxes dp and Dp are not composed of the same antecedents.
Deﬁne φ = {pk : (pk ∈ dp) ∧ (pk /∈ Dp)} to be the set of antecedents in dp that are not in Dp,
and deﬁne Φ = {Pk : (Pk ∈ Dp) ∧ (Pk /∈ dp)} to be the set of antecedents in Dp that are not
in dp; either φ (cid:54)= ∅, or Φ (cid:54)= ∅, or both.

Suppose φ (cid:54)= ∅, and let p ∈ φ be an antecedent in φ. It follows that there exists a subset
of rule lists in σ(Dp) that do not have analogues in σ(dp). Let D(cid:48)(cid:48) ∈ σ(Dp) be such a rule
list, such that its preﬁx D(cid:48)(cid:48)
p = (P1, . . . , Pκ, . . . , p, . . . ) starts with Dp and contains p among
its remaining antecedents. Since p captures a subset of the data that dp captures, and Dp
captures the same data as dp, it follows that p does not capture any data in D(cid:48)(cid:48)

p , i.e.,

1
N

N
(cid:88)

n=1

cap(xn, p | D(cid:48)(cid:48)

p ) = 0 ≤ λ.

By Theorem 10, antecedent p has insuﬃcient support in D(cid:48)(cid:48), and thus D(cid:48)(cid:48) cannot be op-
timal, i.e., D(cid:48)(cid:48) /∈ argminD†∈σ(Dp) R(D†, x, y). By a similar argument, if Φ (cid:54)= ∅ and P ∈ Φ,
and d(cid:48)(cid:48) ∈ σ(dp) is any rule list whose preﬁx starts with dp and contains antecedent P , then d(cid:48)(cid:48)
cannot be optimal, i.e., d(cid:48)(cid:48) /∈ argmind†∈σ(dp) R(d†, x, y).

62

Learning Certifiably Optimal Rule Lists

To ﬁnish justifying claim (42) for the second scenario, ﬁrst deﬁne

τ (dp, Φ) ≡ {d(cid:48)(cid:48) = (d(cid:48)(cid:48)

p, δ(cid:48)(cid:48)

p , q(cid:48)(cid:48)

0 , K(cid:48)(cid:48)) : d(cid:48)(cid:48) ∈ σ(dp) and pk /∈ Φ, ∀pk ∈ d(cid:48)(cid:48)

p} ⊂ σ(dp)

to be the set of all rule lists whose preﬁxes start with dp and do not contain any antecedents
in Φ. Now, recognize that the optimal preﬁxes in τ (dp, Φ) and σ(dp) are the same, i.e.,

argmin
d†∈τ (dp,Φ)

R(d†, x, y) = argmin
d†∈σ(dp)

R(d†, x, y),

and similarly, the optimal preﬁxes in τ (Dp, φ) and σ(Dp) are the same, i.e.,

argmin
D†∈τ (Dp,φ)

R(D†, x, y) = argmin
D†∈σ(Dp)

R(D†, x, y).

Since we have shown that every d(cid:48)(cid:48) ∈ τ (dp, Φ) has a direct analogue D(cid:48)(cid:48) ∈ τ (Dp, φ), such
that d(cid:48)(cid:48) and D(cid:48)(cid:48) obey (39), and vice versa, we again have (42) as a consequence of (41).
We can now ﬁnally combine (40) and (42) to obtain the desired inequality in (21):

min
d(cid:48)∈σ(dp)

R(d(cid:48), x, y) = R(d∗, x, y) ≤ R(D∗, x, y) = min

R(D(cid:48), x, y).

D(cid:48)∈σ(Dp)

Appendix C. Proof of Theorem 18 (Similar Support Bound)

We begin by deﬁning four related rule lists. First, let d = (dp, δp, q0, K) be a rule list with
preﬁx dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK). Second, let D = (Dp, ∆p, Q0, κ) be a
rule list with preﬁx Dp = (P1, . . . , Pκ) and labels ∆p = (Q1, . . . , Qκ). Deﬁne ω as in (22)
and Ω as in (23), and require that ω, Ω ≤ λ. Third, let d(cid:48) = (d(cid:48)
0, K(cid:48)) ∈ σ(dp) be
any rule list whose preﬁx starts with dp, such that K(cid:48) ≥ K. Denote the preﬁx and la-
bels of d(cid:48) by d(cid:48)
p = (p1, . . . , pK, pK+1, . . . , pK(cid:48)) and δp = (q1, . . . , qK(cid:48)), respectively. Finally,
p, ∆(cid:48)
deﬁne D(cid:48) = (D(cid:48)
p =
(P1, . . . , Pκ, Pκ+1, . . . , Pκ(cid:48)) = (P1, . . . , Pκ, pK+1, . . . , pK(cid:48)) starts with Dp and ends with the
same K(cid:48) − K antecedents as d(cid:48)

0, κ(cid:48)) ∈ σ(Dp) to be the ‘analogous’ rule list, i.e., whose preﬁx D(cid:48)

p = (Q1, . . . , Qκ(cid:48)) denote the labels of D(cid:48).

p. Let ∆(cid:48)

p, Q(cid:48)

The smallest possible objective for D(cid:48), in relation to the objective of d(cid:48), reﬂects both
the diﬀerence between the objective lower bounds of D and d and the largest possible
discrepancy between the objectives of d(cid:48) and D(cid:48). The latter would occur if d(cid:48) misclassiﬁed
all the data corresponding to both ω and Ω while D(cid:48) correctly classiﬁed this same data,
thus

p, δ(cid:48)

p, q(cid:48)

R(D(cid:48), x, y) ≥ R(d(cid:48), x, y) + b(Dp, x, y) − b(dp, x, y) − ω − Ω.

(43)

Now let D∗ be an optimal rule list with preﬁx constrained to start with Dp,

and let κ∗ be the length of D∗. Also let d∗ be the analogous K∗-rule list whose preﬁx starts
with dp and ends with the same κ∗ − κ antecedents as D∗, where K∗ = K + κ∗ − κ. By (43),

D∗ ∈ argmin
D†∈σ(Dp)

R(D†, x, y),

63

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

we obtain the desired inequality in (24):

min
D†∈σ(Dp)

R(D†, x, y) = R(D∗, x, y)

≥ R(d∗, x, y) + b(Dp, x, y) − b(dp, x, y) − ω − Ω
≥ min

R(d†, x, y) + b(Dp, x, y) − b(dp, x, y) − ω − Ω.

d†∈σ(dp)

Appendix D. Proof of Theorem 20 (Equivalent Points Bound)

We derive a lower bound on the default rule misclassiﬁcation error (cid:96)0(dp, q0, x, y), analogous
to the lower bound (26) on the misclassiﬁcation error (cid:96)(d, x, y) in the proof of Proposition 19.
As before, we sum over all sets of equivalent points, and then for each such set, we count
diﬀerences between class labels and the minority class label of the set, instead of counting
mistakes made by the default rule:

(cid:96)0(dp, q0, x, y) =

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn]

1
N

1
N

1
N

N
(cid:88)

n=1
U
(cid:88)

N
(cid:88)

u=1
U
(cid:88)

n=1
N
(cid:88)

u=1

n=1

=

≥

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn] 1[xn ∈ eu]

¬ cap(xn, dp) ∧ 1[yn = qu] 1[xn ∈ eu] = b0(dp, x, y),

(44)

where the ﬁnal equality comes from the deﬁnition of b0(dp, x, y) in (28). Since we can write
the objective R(d, x, y) as the sum of the objective lower bound b(dp, x, y) and default rule
misclassiﬁcation error (cid:96)0(dp, q0, x, y), applying (44) gives a lower bound on R(d, x, y):

R(d, x, y) = (cid:96)p(dp, δp, x, y) + (cid:96)0(dp, q0, x, y) + λK = b(dp, x, y) + (cid:96)0(dp, q0, x, y)

≥ b(dp, x, y) + b0(dp, x, y).

(45)

It follows that for any rule list d(cid:48) ∈ σ(d) whose preﬁx d(cid:48)

p starts with dp, we have

R(d(cid:48), x, y) ≥ b(d(cid:48)

p, x, y) + b0(d(cid:48)

p, x, y).

(46)

Finally, we show that the lower bound on R(d, x, y) in (45) is not greater than the lower

bound on R(d(cid:48), x, y) in (46). First, let us deﬁne

Υ(d(cid:48)

p, K, x, y) ≡

cap(xn, pk | d(cid:48)

p) ∧ 1[xn ∈ eu] 1[yn = qu].

(47)

1
N

U
(cid:88)

N
(cid:88)

K(cid:48)
(cid:88)

u=1

n=1

k=K+1

64

Learning Certifiably Optimal Rule Lists

Now, we write a lower bound on b(d(cid:48)

p, x, y) with respect to b(dp, x, y):

b(d(cid:48)

p, x, y) = (cid:96)p(d(cid:48)

p, δp, x, y) + λK(cid:48) =

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] + λK(cid:48)

= (cid:96)p(dp, δp, x, y) + λK +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] + λ(K(cid:48) − K)

= b(dp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] + λ(K(cid:48) − K)

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=1

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K+1

U
(cid:88)

N
(cid:88)

K(cid:48)
(cid:88)

u=1

n=1

U
(cid:88)

N
(cid:88)

k=K+1
K(cid:48)
(cid:88)

1
N

1
N

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

1
N

(cid:32)

= b(dp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] 1[xn ∈ eu] + λ(K(cid:48) − K)

≥ b(dp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[yn = qu] 1[xn ∈ eu] + λ(K(cid:48) − K)

= b(dp, x, y) + Υ(d(cid:48)

n=1

k=K+1

u=1
p, K, x, y) + λ(K(cid:48) − K),

(48)

where the last equality uses (47). Next, we write b0(dp, x, y) with respect to b0(d(cid:48)

p, x, y),

b0(dp, x, y) =

¬ cap(xn, dp) ∧ 1[xn ∈ eu] 1[yn = qu]

=

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

¬ cap(xn, d(cid:48)

p) +

cap(xn, pk | d(cid:48)
p)

∧ 1[xn ∈ eu] 1[yn = qu]

K(cid:48)
(cid:88)

k=K+1

(cid:33)

= b0(d(cid:48)

p, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[xn ∈ eu] 1[yn = qu].

(49)

1
N

U
(cid:88)

N
(cid:88)

K(cid:48)
(cid:88)

u=1

n=1

k=K+1

Rearranging (49) gives

b0(d(cid:48)

p, x, y) = b0(dp, x, y) − Υ(d(cid:48)

p, K, x, y).

(50)

Combining (46) with ﬁrst (50) and then (48) gives the desired inequality in (27):

p, x, y)

p, x, y) + b0(d(cid:48)
p, x, y) + b0(dp, x, y) − Υ(d(cid:48)

R(d(cid:48), x, y) ≥ b(d(cid:48)
= b(d(cid:48)
≥ b(dp, x, y) + Υ(d(cid:48)
= b(dp, x, y) + b0(dp, x, y) + λ(K(cid:48) − K) ≥ b(dp, x, y) + b0(dp, x, y).

p, K, x, y) + λ(K(cid:48) − K) + b0(dp, x, y) − Υ(d(cid:48)

p, K, x, y)

p, K, x, y)

Appendix E. Data Processing Details and Antecedent Mining

In this appendix, we provide details regarding datasets used in our experiments (Section 6).

65

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

E.1 ProPublica Recidivism Data Set

Table 7 shows the 6 attributes and corresponding 17 categorical values that we use for the
ProPublica data set. From these, we construct 17 single-clause antecedents, for example,
(age = 23 − 25). We then combine pairs of these antecedents as conjunctions to form two-
clause antecedents, e.g., (age = 23 − 25) ∧ (priors = 2 − 3). By virtue of our lower bound
on antecedent support, (Theorem 10, §3.7), we eliminate antecedents with support less
than 0.005 or greater than 0.995, since λ = 0.005 is the smallest regularization parameter
value we study for this problem. With this ﬁltering step, we generate between 121 and 123
antecedents for each fold; without it, we would instead generate about 130 antecedents as
input to our algorithm.

Note that we exclude the ‘current charge’ attribute (which has two categorical values,
‘misdemeanor’ and ‘felony’); for individuals in the data set booked on multiple charges, this
attribute does not appear to consistently reﬂect the most serious charge.

Feature
sex
age
juvenile felonies
juvenile misdemeanors
juvenile crimes
priors

Value range
—
18-96
0-20
0-13
0-21
0-38

Categorical values
male, female
18-20, 21-22, 23-25, 26-45, >45
0, >0
0, >0
0, >0
0, 1, 2-3, >3

Count
2
5
2
2
2
4

Table 7: Categorical features (6 attributes, 17 values) from the ProPublica data set. We
construct the feature juvenile crimes from the sum of juvenile felonies, juvenile
misdemeanors, and the number of juvenile crimes that were neither felonies nor
misdemeanors (not shown).

E.2 NYPD Stop-and-frisk Data Set

This data set is larger than, but similar to the NYCLU stop-and-frisk data set, described
next.

E.3 NYCLU Stop-and-frisk Data Set

The original data set contains 45,787 records, each describing an incident involving a stopped
person; the individual was frisked in 30,345 (66.3%) of records and and searched in 7,283
(15.9%). In 30,961 records, the individual was frisked and/or searched (67.6%); of those,
a criminal possession of a weapon was identiﬁed 1,445 times (4.7% of these records). We
remove 1,929 records with missing data, as well as a small number with extreme values
for the individual’s age—we eliminate those with age < 12 or > 89. This yields a set of
29,595 records in which the individual was frisked and/or searched. To address the class
imbalance for this problem, we sample records from the smaller class with replacement. We
generate cross-validation folds ﬁrst, and then resample within each fold. In our 10-fold cross-
validation experiments, each training set contains 50,743 observations. Table 8 shows the 5

66

Learning Certifiably Optimal Rule Lists

categorical attributes that we use, corresponding to a total of 28 values. Our experiments
use these antecedents, as well as negations of the 18 antecedents corresponding to the two
features stop reason and additional circumstances, which gives a total of 46 antecedents.

Feature
stop reason

additional
circumstances

city
location

inside or outside

Values
suspicious object, ﬁts description, casing,
acting as lookout, suspicious clothing,
drug transaction, furtive movements,
actions of violent crime, suspicious bulge
proximity to crime scene, evasive response,
associating with criminals, changed direction,
high crime area, time of day,
sights and sounds of criminal activity,
witness report, ongoing investigation
Queens, Manhattan, Brooklyn, Staten Island, Bronx
housing authority, transit authority,
neither housing nor transit authority
inside, outside

Count
9

9

5
3

2

Table 8: Categorical features (5 attributes, 28 values) from the NYCLU data set.

Appendix F. Example Optimal Rule Lists, for Diﬀerent Values of λ

For each of our prediction problems, we provide listings of optimal rule lists found by
CORELS, across 10 cross-validation folds, for diﬀerent values of the regularization param-
eter λ. These rule lists correspond to the results for CORELS summarized in Figures 11
and 12 (§6.6). Recall that as λ decreases, optimal rule lists tend to grow in length.

F.1 ProPublica Recidivism Data Set

We show example optimal rule lists that predict two-year recidivism. Figure 19 shows exam-
ples for regularization parameters λ = 0.02 and 0.01. Figure 20 shows examples for λ = 0.005;
Figure 4 (§6.3) showed two representative examples.

For the largest regularization parameter λ = 0.02 (Figure 19), we observe that all folds
identify the same length-1 rule list. For the intermediate value λ = 0.01 (Figure 19), the
folds identify optimal 2-rule or 3-rule lists that contain the nearly same preﬁx rules, up to
permutations. For the smallest value λ = 0.005 (Figure 20), the folds identify optimal 3-rule
or 4-rule lists that contain the nearly same preﬁx rules, up to permutations. Across all three
regularization parameter values and all folds, the preﬁx rules always predict the positive
class label, and the default rule always predicts the negative class label. We note that our
objective is not designed to enforce any of these properties.

67

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Two-year recidivism prediction (λ = 0.02)

if (priors > 3) then predict yes
else predict no

Two-year recidivism prediction (λ = 0.01)

(cid:46) Found by all 10 folds

if (priors > 3) then predict yes
else if (sex = male) and (juvenile crimes > 0) then predict yes
else predict no

(cid:46) Found by 3 folds

if (sex = male) and (juvenile crimes > 0) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (priors > 3) then predict yes
else predict no

if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

(cid:46) Found by 2 folds

(cid:46) Found by 2 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

Figure 19: Example optimal rule lists for the ProPublica data set, found by CORELS with
regularization parameters λ = 0.02 (top), and 0.01 (bottom) across 10 cross-
validation folds.

68

Learning Certifiably Optimal Rule Lists

Two-year recidivism prediction (λ = 0.005)

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (priors > 3) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else predict no

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

(cid:46) Found by 4 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

Figure 20: Example optimal rule lists for the ProPublica data set, found by CORELS with
regularization parameters λ = 0.005, across 10 cross-validation folds.

69

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

F.2 NYPD Stop-and-frisk Data Set

We show example optimal rule lists that predict whether a weapon will be found on a
stopped individual who is frisked or searched, learned from the NYPD data set.

Weapon prediction (λ = 0.01, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else predict no

if (location = transit authority) then predict yes
else if (stop reason = suspicious object) then predict yes
else predict no

Weapon prediction (λ = 0.005, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (location = housing authority) then predict no
else if (location = transit authority) then predict yes
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else if (location = transit authority) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (city = Bronx) then predict no
else if (location = housing authority) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

(cid:46) Found by 8 folds

(cid:46) Found by 2 folds

(cid:46) Found by 7 folds

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

Figure 21: Example optimal rule lists for the NYPD stop-and-frisk data set, found by
CORELS with regularization parameters λ = 0.01 (top) and 0.005 (bottom),
across 10 cross-validation folds.

70

Learning Certifiably Optimal Rule Lists

Weapon prediction (λ = 0.01, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (inside or outside = outside) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (inside or outside = inside) then predict yes
else predict no

Weapon prediction (λ = 0.005, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = f urtive movements) then predict no
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f its description) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f urtive movements) then predict no
else if (stop reason = f its description) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = f its description) then predict no
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = f urtive movements) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = acting as lookout) then predict no
else predict yes

(cid:46) Found by 7 folds

(cid:46) Found by 3 folds

(cid:46) Found by 2 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

Figure 22: Example optimal rule lists for the NYPD stop-and-frisk data set (Feature Set D)
found by CORELS with regularization parameters λ = 0.01 (top) and 0.005 (bot-
tom), across 10 cross-validation folds. For λ = 0.005, we show results from 7 folds;
the remaining 3 folds were equivalent, up to a permutation of the preﬁx rules,
and started with the same ﬁrst preﬁx rule.

71

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

F.3 NYCLU Stop-and-frisk Data Set

We show example optimal rule lists that predict whether a weapon will be found on a
stopped individual who is frisked or searched, learned from the NYCLU data set. Figure 23
shows regularization parameters λ = 0.04 and 0.01, and Figure 24 shows λ = 0.0025. We
showed a representative solution for λ = 0.01 in Figure 5 (§6.3).

For each of the two larger regularization parameters in Figure 23, λ = 0.04 (top) and
0.01 (bottom), we observe that across the folds, all the optimal rule lists contain the same or
equivalent rules, up to a permutation. With the smaller regularization parameter λ = 0.0025
(Figure 24), we observe a greater diversity of longer optimal rule lists, though they share
similar structure.

Weapon prediction (λ = 0.04)

if (stop reason = suspicious object) then predict yes
else if (stop reason (cid:54)= suspicious bulge) then predict no
else predict yes

if (stop reason = suspicious bulge) then predict yes
else if (stop reason (cid:54)= suspicious object) then predict no
else predict yes

Weapon prediction (λ = 0.01)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (stop reason (cid:54)= suspicious bulge) then predict no
else predict yes

if (location = transit authority) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = suspicious object) then predict yes
else predict no

if (location = transit authority) then predict yes
else if (stop reason = suspicious object) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else predict no

if (location = transit authority) then predict yes
else if (stop reason = suspicious object) then predict yes
else if (stop reason (cid:54)= suspicious bulge) then predict no
else predict yes

(cid:46) Found by 7 folds

(cid:46) Found by 3 folds

(cid:46) Found by 4 folds

(cid:46) Found by 3 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

Figure 23: Example optimal rule lists for the NYCLU stop-and-frisk data set, found by
CORELS with regularization parameters λ = 0.04 (top) and 0.01 (bottom),
across 10 cross-validation folds.

72

Learning Certifiably Optimal Rule Lists

Weapon prediction (λ = 0.0025)

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = transit authority) then predict yes
else if (inside or outside = inside) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (location = housing authority) then predict no
else if (stop reason = casing) then predict no
else if (stop reason = f its description) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = drug transaction) then predict no
else if (stop reason = suspicious object) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (location = housing authority) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = casing) then predict no
else if (city = M anhattan) then predict yes
else if (city = Bronx) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = transit authority) then predict yes
else if (inside or outside = inside) then predict no
else if (city = M anhattan) then predict yes
else if (additional circumstances = changed direction) then predict no
else if (city = Bronx) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = actions of violent crime) then predict no
else if (stop reason = f its description) then predict no
else if (location = transit authority) then predict yes
else if (inside or outside = inside) then predict no
else if (city = M anhattan) then predict yes
else if (additional circumstances = evasive response) then predict no
else if (city = Bronx) then predict yes
else predict no

(cid:46) Found by 4 folds (K = 7)

(cid:46) Found by 1 fold (K = 6)

(cid:46) Found by 1 fold (K = 6)

(cid:46) Found by 1 fold (K = 6)

(cid:46) Found by 1 fold (K = 8)

(cid:46) Found by 1 fold (K = 9)

(cid:46) Found by 1 fold (K = 10)

Figure 24: Example optimal rule lists for the NYCLU stop-and-frisk data set λ = 0.0025.

73

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Appendix G. Additional Results on Predictive Performance and Model

Size for CORELS and Other Algorithms

In this appendix, we plot TPR, FPR, and model size for CORELS and three other algo-
rithms, using the NYPD data set (Feature Set D).

Figure 25: TPR (top) and FPR (bottom) for the test set, as a function of model size,
across diﬀerent methods, for weapon prediction with the NYPD stop-and-frisk
data set (Feature Set D). In the legend, numbers in parentheses are algorithm
parameters, as in Figure 12. Legend markers and error bars indicate means and
standard deviations, respectively, across cross-validation folds. C4.5 ﬁnds large
models for all tested parameters.

References

E. Angelino, N. Larus-Stone, D. Alabi, M. Seltzer, and C. Rudin. Learning certiﬁably
optimal rule lists for categorical data. In ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD), 2017.

K. P. Bennett and J. A. Blue. Optimal decision trees. Technical report, R.P.I. Math Report

No. 214, Rensselaer Polytechnic Institute, 1996.

I. Bratko. Machine learning: Between accuracy and interpretability. In Learning, Networks
and Statistics, volume 382 of International Centre for Mechanical Sciences, pages 163–

74

Learning Certifiably Optimal Rule Lists

177. Springer Vienna, 1997.

Trees. Wadsworth, 1984.

2013.

L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and Regression

S. Bushway. Is there any logic to using logit. Criminology & Public Policy, 12(3):563–567,

C. Chen and C. Rudin. An optimization approach to learning falling rule lists. In Interna-

tional Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2018.

H. A. Chipman, E. I. George, and R. E. McCulloch. Bayesian CART model search. Journal

of the American Statistical Association, 93(443):935–948, 1998.

H. A. Chipman, E. I. George, and R. E. McCulloch. Bayesian treed models. Machine

Learning, 48(1):299–320, 2002.

H. A. Chipman, E. I. George, and R. E. McCulloch. BART: Bayesian additive regression

trees. The Annals of Applied Statistics, 4(1):266–298, 2010.

P. Clark and T. Niblett. The CN2 induction algorithm. Machine Learning, 3:261–283, 1989.

W. W. Cohen. Fast eﬀective rule induction.

In International Conference on Machine

Learning (ICML), pages 115–123, 1995.

R. M. Dawes. The robust beauty of improper linear models in decision making. American

Psychologist, 34(7):571–582, 1979.

D. Dension, B. Mallick, and A.F.M. Smith. A Bayesian CART algorithm. Biometrika, 85

(2):363–377, 1998.

trees, 1996.

Advances, 4(1), 2018.

D. Dobkin, T. Fulton, D. Gunopulos, S. Kasif, and S. Salzberg. Induction of shallow decision

J. Dressel and H. Farid. The accuracy, fairness, and limits of predicting recidivism. Science

A. Farhangfar, R. Greiner, and M. Zinkevich. A fast way to produce optimal ﬁxed-depth
decision trees. In International Symposium on Artiﬁcial Intelligence and Mathematics
(ISAIM), 2008.

E. Frank and I. H. Witten. Generating accurate rule sets without global optimization. In

International Conference on Machine Learning (ICML), pages 144–151, 1998.

A. A. Freitas. Comprehensible classiﬁcation models: A position paper. ACM SIGKDD

Explorations Newsletter, 15(1):1–10, 2014.

M. Garofalakis, D. Hyun, R. Rastogi, and K. Shim. Eﬃcient algorithms for constructing
decision trees with constraints. In ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD), pages 335–339, 2000.

75

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

C. Giraud-Carrier. Beyond predictive accuracy: What? In ECML-98 Workshop on Up-
grading Learning to Meta-Level: Model Selection and Data Transformation, pages 78–85,
1998.

S. Goel, J. M. Rao, and R. Shroﬀ. Precinct or prejudice? Understanding racial disparities in
New York City’s stop-and-frisk policy. The Annals of Applied Statistics, 10(1):365–394,
03 2016.

S. T. Goh and C. Rudin. Box drawings for learning with imbalanced data. In ACM SIGKDD

International Conference on Knowledge Discovery and Data Mining (KDD), 2014.

S. T. Goh and C. Rudin. A minimax surrogate loss approach to conditional diﬀerence
estimation. CoRR, abs/1803.03769, 2018. URL https://arxiv.org/abs/1803.03769.

B. Goodman and S. Flaxman. European Union regulations on algorithmic decision-making
and a “right to explanation”. In ICML Workshop on Human Interpretability in Machine
Learning (WHI), 2016.

R. C. Holte. Very simple classiﬁcation rules perform well on most commonly used datasets.

Machine Learning, 11(1):63–91, 1993.

J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, and B. Baesens. An empirical evaluation
of the comprehensibility of decision table, tree and rule based predictive models. Decision
Support Systems, 51(1):141–154, 2011.

V. Kaxiras and A. Saligrama. Building predictive models with rule lists, 2018. URL https:

//corels.eecs.harvard.edu.

H. Lakkaraju and C. Rudin. Cost-sensitive and interpretable dynamic treatment regimes
based on rule lists. In International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS), 2017.

J. Larson, S. Mattu, L. Kirchner, and J. Angwin. How we analyzed the COMPAS recidivism

algorithm. ProPublica, 2016.

N. Larus-Stone, E. Angelino, D. Alabi, M. Seltzer, V. Kaxiras, A. Saligrama, and C. Rudin.
Systems optimizations for learning certiﬁably optimal rule lists. In SysML Conference,
2018.

N. L. Larus-Stone. Learning Certiﬁably Optimal Rule Lists: A Case For Discrete Optimiza-

tion in the 21st Century. 2017. Undergraduate thesis, Harvard College.

B. Letham, C. Rudin, T. H. McCormick, and D. Madigan. Interpretable classiﬁers using
rules and Bayesian analysis: Building a better stroke prediction model. The Annals of
Applied Statistics, 9(3):1350–1371, 2015.

O. Li, H. Liu, C. Chen, and C. Rudin. Deep learning for case-based reasoning through pro-
totypes: A neural network that explains its predictions. In Proceedings of the Association
for the Advancement of Artiﬁcial Intelligence (AAAI), 2018.

76

Learning Certifiably Optimal Rule Lists

W. Li, J. Han, and J. Pei. CMAR: Accurate and eﬃcient classiﬁcation based on multiple
class-association rules. IEEE International Conference on Data Mining (ICDM), pages
369–376, 2001.

J. T. Linderoth and M. W. P. Savelsbergh. A computational study of search strategies for
mixed integer programming. INFORMS Journal on Computing, 11(2):173–187, 1999.

B. Liu, W. Hsu, and Y. Ma. Integrating classiﬁcation and association rule mining. In ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),
pages 80–96, 1998.

M. Marchand and M. Sokolova. Learning with decision lists of data-dependent features.

Journal of Machine Learning Research, 6:427–451, 2005.

R. S. Michalski. On the quasi-minimal solution of the general covering problem. In Inter-

national Symposium on Information Processing, pages 125–128, 1969.

New York Civil Liberties Union. Stop-and-frisk data, 2014. URL http://www.nyclu.org/

content/stop-and-frisk-data.

New York Police Department. Stop, question and frisk data, 2016. URL http://www1.

nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page.

S. Nijssen and E. Fromont. Optimal constraint-based decision tree induction from itemset

lattices. Data Mining and Knowledge Discovery, 21(1):9–51, 2010.

J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1993.

P. R. Rijnbeek and J. A. Kors. Finding a short and accurate decision rule in disjunctive

normal form by exhaustive search. Machine Learning, 80(1):33–62, July 2010.

R. L. Rivest. Learning decision lists. Machine Learning, 2(3):229–246, November 1987.

U. R¨uckert and L. De Raedt. An experimental evaluation of simplicity in rule learning.

Artiﬁcial Intelligence, 172:19–28, 2008.

C. Rudin and S¸. Ertekin. Learning customized and optimized lists of rules with mathemat-

ical programming. Submitted, 2016.

C. Rudin, B. Letham, and D. Madigan. Learning theory analysis for association rules and
sequential event prediction. Journal of Machine Learning Research, 14:3384–3436, 2013.

S. R¨uping. Learning interpretable models. PhD thesis, Universit¨at Dortmund, 2006.

G. Shmueli. To explain or to predict? Statistical Science, 25(3):289–310, August 2010.

M. Sokolova, M. Marchand, N. Japkowicz, and J. Shawe-Taylor. The decision list machine.
In Advances in Neural Information Processing Systems (NIPS), volume 15, pages 921–
928, 2003.

77

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

N. Tollenaar and P. van der Heijden. Which method predicts recidivism best?: A comparison
of statistical, machine learning and data mining predictive models. Journal of the Royal
Statistical Society: Series A (Statistics in Society), 176(2):565–584, 2013.

B. Ustun and C. Rudin. Supersparse linear integer models for optimized medical scoring

systems. Machine Learning, 102(3):349–391, 2016.

B. Ustun and C. Rudin. Optimized risk scores. In ACM SIGKDD International Conference

on Knowledge Discovery and Data Mining (KDD), 2017.

K. Vanhoof and B. Depaire. Structure of association rule classiﬁers: A review. In Inter-
national Conference on Intelligent Systems and Knowledge Engineering (ISKE), pages
9–12, 2010.

A. Vellido, J. D. Mart´ın-Guerrero, and P. J.G. Lisboa. Making machine learning models
In European Symposium on Artiﬁcial Neural Networks, Computational

interpretable.
Intelligence and Machine Learning (ESANN), 2012.

F. Wang and C. Rudin. Falling rule lists. In International Conference on Artiﬁcial Intelli-

gence and Statistics (AISTATS), 2015a.

F. Wang and C. Rudin. Causal falling rule lists. CoRR, abs/1510.05189, 2015b. URL

https://arxiv.org/abs/1510.05189.

T. Wang. Hybrid decision making: When interpretable models collaborate with black-box

models. CoRR, abs/1802.04346, 2018. URL http://arxiv.org/abs/1802.04346.

T. Wang, C. Rudin, F. Doshi-Velez, Y. Liu, E. Klampﬂ, and P. MacNeille. Bayesian or’s
of and’s for interpretable classiﬁcation with application to context aware recommender
systems. In International Conference on Data Mining (ICDM), 2016.

T. Wang, C. Rudin, F. Doshi-Velez, Y. Liu, E. Klampﬂ, and P. MacNeille. A Bayesian frame-
work for learning rule sets for interpretable classiﬁcation. Journal of Machine Learning
Research, 18(70):1–37, 2017.

E. Westervelt.

man’s murder?,
did-a-bail-reform-algorithm-contribute-to-this-san-francisco-man-s-murder.

reform algorithm contribute to this San Francisco
URL https://www.npr.org/2017/08/18/543976003/

Did a bail
2017.

H. Yang, C. Rudin, and M. Seltzer. Scalable Bayesian rule lists. In International Conference

on Machine Learning (ICML), 2017.

X. Yin and J. Han. CPAR: Classiﬁcation based on predictive association rules. In SIAM

International Conference on Data Mining (SDM), pages 331–335, 2003.

J. Zeng, B. Ustun, and C. Rudin. Interpretable classiﬁcation models for recidivism pre-
diction. Journal of the Royal Statistical Society: Series A (Statistics in Society), 180(3):
689–722, 2017.

Y. Zhang, E. B. Laber, A. Tsiatis, and M. Davidian. Using decision lists to construct
interpretable and parsimonious treatment regimes. Biometrics, 71(4):895–904, 2015.

78

8
1
0
2
 
g
u
A
 
3
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
0
7
1
0
.
4
0
7
1
:
v
i
X
r
a

Journal of Machine Learning Research 18 (2018) 1-78

Submitted 11/17; Published 6/18

Learning Certiﬁably Optimal Rule Lists for Categorical Data

Elaine Angelino
Department of Electrical Engineering and Computer Sciences
University of California, Berkeley, Berkeley, CA 94720

elaine@eecs.berkeley.edu

Nicholas Larus-Stone
Daniel Alabi
Margo Seltzer
School of Engineering and Applied Sciences
Harvard University, Cambridge, MA 02138

nlarusstone@alumni.harvard.edu
alabid@g.harvard.edu
margo@eecs.harvard.edu

Cynthia Rudin∗
Department of Computer Science and Department of Electrical and Computer Engineering
Duke University, Durham, NC 27708

cynthia@cs.duke.edu

Editor: Maya Gupta
∗To whom correspondence should be addressed.

Abstract
We present the design and implementation of a custom discrete optimization technique for
building rule lists over a categorical feature space. Our algorithm produces rule lists with
optimal training performance, according to the regularized empirical risk, with a certiﬁcate
of optimality. By leveraging algorithmic bounds, eﬃcient data structures, and computa-
tional reuse, we achieve several orders of magnitude speedup in time and a massive reduc-
tion of memory consumption. We demonstrate that our approach produces optimal rule
lists on practical problems in seconds. Our results indicate that it is possible to construct
optimal sparse rule lists that are approximately as accurate as the COMPAS proprietary
risk prediction tool on data from Broward County, Florida, but that are completely inter-
pretable. This framework is a novel alternative to CART and other decision tree methods
for interpretable modeling.
Keywords: rule lists, decision trees, optimization, interpretable models, criminal justice
applications

1. Introduction

As machine learning continues to gain prominence in socially-important decision-making,
the interpretability of predictive models remains a crucial problem. Our goal is to build
models that are highly predictive, transparent, and easily understood by humans. We use
rule lists, also known as decision lists, to achieve this goal. Rule lists are predictive models
composed of if-then statements; these models are interpretable because the rules provide a
reason for each prediction (Figure 1).

Constructing rule lists, or more generally, decision trees, has been a challenge for more
than 30 years; most approaches use greedy splitting techniques (Rivest, 1987; Breiman
et al., 1984; Quinlan, 1993). Recent approaches use Bayesian analysis, either to ﬁnd a locally
optimal solution (Chipman et al., 1998) or to explore the search space (Letham et al., 2015;

c(cid:13)2018 Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v18/17-716.html.

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 23) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

Figure 1: An example rule list that predicts two-year recidivism for the ProPublica data

set, found by CORELS.

Yang et al., 2017). These approaches achieve high accuracy while also managing to run
reasonably quickly. However, despite the apparent accuracy of the rule lists generated by
these algorithms, there is no way to determine either if the generated rule list is optimal
or how close it is to optimal, where optimality is deﬁned with respect to minimization of a
regularized loss function.

Optimality is important, because there are societal implications for a lack of optimality.
Consider the ProPublica article on the Correctional Oﬀender Management Proﬁling for Al-
ternative Sanctions (COMPAS) recidivism prediction tool (Larson et al., 2016). It highlights
a case where a black box, proprietary predictive model is being used for recidivism predic-
tion. The authors hypothesize that the COMPAS scores are racially biased, but since the
model is not transparent, no one (outside of the creators of COMPAS) can determine the
reason or extent of the bias (Larson et al., 2016), nor can anyone determine the reason for
any particular prediction. By using COMPAS, users implicitly assumed that a transparent
model would not be suﬃciently accurate for recidivism prediction, i.e., they assumed that
a black box model would provide better accuracy. We wondered whether there was indeed
no transparent and suﬃciently accurate model. Answering this question requires solving a
computationally hard problem. Namely, we would like to both ﬁnd a transparent model that
is optimal within a particular pre-determined class of models and produce a certiﬁcate of
its optimality, with respect to the regularized empirical risk. This would enable one to say,
for this problem and model class, with certainty and before resorting to black box methods,
whether there exists a transparent model. While there may be diﬀerences between train-
ing and test performance, ﬁnding the simplest model with optimal training performance is
prescribed by statistical learning theory.

To that end, we consider the class of rule lists assembled from pre-mined frequent item-
sets and search for an optimal rule list that minimizes a regularized risk function, R. This
is a hard discrete optimization problem. Brute force solutions that minimize R are compu-
tationally prohibitive due to the exponential number of possible rule lists. However, this is
a worst case bound that is not realized in practical settings. For realistic cases, it is possible
to solve fairly large cases of this problem to optimality, with the careful use of algorithms,
data structures, and implementation techniques.

We develop specialized tools from the ﬁelds of discrete optimization and artiﬁcial intel-
ligence. Speciﬁcally, we introduce a special branch-and bound algorithm, called Certiﬁably
Optimal RulE ListS (CORELS), that provides the optimal solution according to the train-
ing objective, along with a certiﬁcate of optimality. The certiﬁcate of optimality means that
we can investigate how close other models (e.g., models provided by greedy algorithms) are
to optimal.

2

Learning Certifiably Optimal Rule Lists

Within its branch-and-bound procedure, CORELS maintains a lower bound on the
minimum value of R that each incomplete rule list can achieve. This allows CORELS to
prune an incomplete rule list (and every possible extension) if the bound is larger than
the error of the best rule list that it has already evaluated. The use of careful bounding
techniques leads to massive pruning of the search space of potential rule lists. The algorithm
continues to consider incomplete and complete rule lists until it has either examined or
eliminated every rule list from consideration. Thus, CORELS terminates with the optimal
rule list and a certiﬁcate of optimality.

The eﬃciency of CORELS depends on how much of the search space our bounds allow us
to prune; we seek a tight lower bound on R. The bound we maintain throughout execution is
a maximum of several bounds that come in three categories. The ﬁrst category of bounds are
those intrinsic to the rules themselves. This category includes bounds stating that each rule
must capture suﬃcient data; if not, the rule list is provably non-optimal. The second type of
bound compares a lower bound on the value of R to that of the current best solution. This
allows us to exclude parts of the search space that could never be better than our current
solution. Finally, our last type of bound is based on comparing incomplete rule lists that
capture the same data and allows us to pursue only the most accurate option. This last
class of bounds is especially important—without our use of a novel symmetry-aware map,
we are unable to solve most problems of reasonable scale. This symmetry-aware map keeps
track of the best accuracy over all observed permutations of a given incomplete rule list.

We keep track of these bounds using a modiﬁed preﬁx tree, a data structure also known
as a trie. Each node in the preﬁx tree represents an individual rule; thus, each path in the
tree represents a rule list such that the ﬁnal node in the path contains metrics about that
rule list. This tree structure, together with a search policy and sometimes a queue, enables a
variety of strategies, including breadth-ﬁrst, best-ﬁrst, and stochastic search. In particular,
we can design diﬀerent best-ﬁrst strategies by customizing how we order elements in a
priority queue. In addition, we are able to limit the number of nodes in the trie and thereby
enable tuning of space-time tradeoﬀs in a robust manner. This trie structure is a useful way
of organizing the generation and evaluation of rule lists.

We evaluated CORELS on a number of publicly available data sets. Our metric of
success was 10-fold cross-validated prediction accuracy on a subset of the data. These data
sets involve hundreds of rules and thousands of observations. CORELS is generally able to
ﬁnd an optimal rule list in a matter of seconds and certify its optimality within about 10
minutes. We show that we are able to achieve better or similar out-of-sample accuracy on
these data sets compared to the popular greedy algorithms, CART and C4.5.

CORELS targets large (not massive) problems, where interpretability and certiﬁable
optimality are important. We illustrate the eﬃcacy of our approach using (1) the ProPublica
COMPAS data set (Larson et al., 2016), for the problem of two-year recidivism prediction,
and (2) stop-and-frisk data sets from the NYPD (New York Police Department, 2016) and
the NYCLU (New York Civil Liberties Union, 2014), to predict whether a weapon will
be found on a stopped individual who is frisked or searched. On these data, we produce
certiﬁably optimal, interpretable rule lists that achieve the same accuracy as approaches
such as random forests. This calls into question the need for use of a proprietary, black box
algorithm for recidivism prediction.

3

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Our work overlaps with the thesis of Larus-Stone (2017). We have also written a pre-
liminary conference version of this article (Angelino et al., 2017), and a report highlighting
systems optimizations of our implementation (Larus-Stone et al., 2018); the latter includes
additional empirical measurements not presented here.

Our code is at https://github.com/nlarusstone/corels, where we provide the C++
implementation we used in our experiments (§6). Kaxiras and Saligrama (2018) have also
created an interactive web interface at https://corels.eecs.harvard.edu, where a user
can upload data and run CORELS from a browser.

2. Related Work

Since every rule list is a decision tree and every decision tree can be expressed as an
equivalent rule list, the problem we are solving is a version of the “optimal decision tree”
problem, though regularization changes the nature of the problem (as shown through our
bounds). The optimal decision tree problem is computationally hard, though since the late
1990’s, there has been research on building optimal decision trees using optimization tech-
niques (Bennett and Blue, 1996; Dobkin et al., 1996; Farhangfar et al., 2008). A particularly
interesting paper along these lines is that of Nijssen and Fromont (2010), who created a
“bottom-up” way to form optimal decision trees. Their method performs an expensive search
step, mining all possible leaves (rather than all possible rules), and uses those leaves to form
trees. Their method can lead to memory problems, but it is possible that these memory
issues can be mitigated using the theorems in this paper.1 None of these methods used the
tight bounds and data structures of CORELS.

Because the optimal decision tree problem is hard, there are a huge number of algo-
rithms such as CART (Breiman et al., 1984) and C4.5 (Quinlan, 1993) that do not perform
exploration of the search space beyond greedy splitting. Similarly, there are decision list
and associative classiﬁcation methods that construct rule lists iteratively in a greedy way
(Rivest, 1987; Liu et al., 1998; Li et al., 2001; Yin and Han, 2003; Sokolova et al., 2003;
Marchand and Sokolova, 2005; Vanhoof and Depaire, 2010; Rudin et al., 2013). Some ex-
ploration of the search space is done by Bayesian decision tree methods (Dension et al.,
1998; Chipman et al., 2002, 2010) and Bayesian rule-based methods (Letham et al., 2015;
Yang et al., 2017). The space of trees of a given depth is much larger than the space of rule
lists of that same depth, and the trees within the Bayesian tree algorithms are grown in a
top-down greedy way. Because of this, authors of Bayesian tree algorithms have noted that
their MCMC chains tend to reach only locally optimal solutions. The RIPPER algorithm
(Cohen, 1995) is similar to the Bayesian tree methods in that it grows, prunes, and then
locally optimizes. The space of rule lists is smaller than that of trees, and has simpler struc-
ture. Consequently, Bayesian rule list algorithms tend to be more successful at escaping
local minima and can introduce methods of exploring the search space that exploit this
structure—these properties motivate our focus on lists. That said, the tightest bounds for
the Bayesian lists (namely, those of Yang et al., 2017, upon whose work we build), are not
nearly as tight as those of CORELS.

1. There is no public version of their code for distribution as of this writing.

4

Learning Certifiably Optimal Rule Lists

Tight bounds, on the other hand, have been developed for the (immense) literature on
building disjunctive normal form (DNF) models; a good example of this is the work of Rijn-
beek and Kors (2010). For models of a given size, since the class of DNF’s is a proper subset
of decision lists, our framework can be restricted to learn optimal DNF’s. The ﬁeld of DNF
learning includes work from the ﬁelds of rule learning/induction (e.g., early algorithms by
Michalski, 1969; Clark and Niblett, 1989; Frank and Witten, 1998) and associative classiﬁ-
cation (Vanhoof and Depaire, 2010). Most papers in these ﬁelds aim to carefully guide the
search through the space of models. If we were to place a restriction on our code to learn
DNF’s, which would require restricting predictions within the list to the positive class only,
we could potentially use methods from rule learning and associative classiﬁcation to help
order CORELS’ queue, which would in turn help us eliminate parts of the search space
more quickly.

Some of our bounds, including the minimum support bound (§3.7, Theorem 10), come
from Rudin and Ertekin (2016), who provide ﬂexible mixed-integer programming (MIP)
formulations using the same objective as we use here; MIP solvers in general cannot compete
with the speed of CORELS.

CORELS depends on pre-mined rules, which we obtain here via enumeration. The litera-
ture on association rule mining is huge, and any method for rule mining could be reasonably
substituted.

CORELS’ main use is for producing interpretable predictive models. There is a grow-
ing interest in interpretable (transparent, comprehensible) models because of their societal
importance (see R¨uping, 2006; Bratko, 1997; Dawes, 1979; Vellido et al., 2012; Giraud-
Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans et al., 2011; Freitas, 2014). There are
now regulations on algorithmic decision-making in the European Union on the “right to an
explanation” (Goodman and Flaxman, 2016) that would legally require interpretability of
predictions. There is work in both the DNF literature (R¨uckert and Raedt, 2008) and deci-
sion tree literature (Garofalakis et al., 2000) on building interpretable models. Interpretable
models must be so sparse that they need to be heavily optimized; heuristics tend to produce
either inaccurate or non-sparse models.

Interpretability has many meanings, and it is possible to extend the ideas in this work
to other deﬁnitions of interpretability; these rule lists may have exotic constraints that help
with ease-of-use. For example, Falling Rule Lists (Wang and Rudin, 2015a) are constrained
to have decreasing probabilities down the list, which makes it easier to assess whether an
observation is in a high risk subgroup. In parallel to this paper, we have been working on
an algorithm for Falling Rule Lists (Chen and Rudin, 2018) with bounds similar to those
presented here, but even CORELS’ basic support bounds do not hold for the falling case,
which is much more complicated. One advantage of the approach taken by Chen and Rudin
(2018) is that it can handle class imbalance by weighting the positive and negative classes
diﬀerently; this extension is possible in CORELS but not addressed here.

The models produced by CORELS are predictive only; they cannot be used for policy-
making because they are not causal models, they do not include the costs of true and false
positives, nor the cost of gathering information. It is possible to adapt CORELS’ frame-
work for causal inference (Wang and Rudin, 2015b), dynamic treatment regimes (Zhang
et al., 2015), or cost-sensitive dynamic treatment regimes (Lakkaraju and Rudin, 2017) to

5

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 23) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if p1 then predict q1
else if p2 then predict q2
else if p3 then predict q3
else predict q0

Figure 2: The rule list d = (r1, r2, r3, r0). Each rule is of the form rk = pk → qk, for all
k = 0, . . . , 3. We can also express this rule list as d = (dp, δp, q0, K), where
dp = (p1, p2, p3), δp = (1, 1, 1, 1), q0 = 0, and K = 3. This is the same 3-rule list
as in Figure 1, that predicts two-year recidivism for the ProPublica data set.

help with policy design. CORELS could potentially be adapted to handle these kinds of
interesting problems.

3. Learning Optimal Rule Lists

In this section, we present our framework for learning certiﬁably optimal rule lists. First, we
deﬁne our setting and useful notation (§3.1) and then the objective function we seek to min-
imize (§3.2). Next, we describe the principal structure of our optimization algorithm (§3.3),
which depends on a hierarchically structured objective lower bound (§3.4). We then derive
a series of additional bounds that we incorporate into our algorithm, because they enable
aggressive pruning of our state space.

3.1 Notation

We restrict our setting to binary classiﬁcation, where rule lists are Boolean functions; this
framework is straightforward to generalize to multi-class classiﬁcation. Let {(xn, yn)}N
n=1
denote training data, where xn ∈ {0, 1}J are binary features and yn ∈ {0, 1} are labels.
Let x = {xn}N

n=1, and let xn,j denote the j-th feature of xn.

n=1 and y = {yn}N

A rule list d = (r1, r2, . . . , rK, r0) of length K ≥ 0 is a (K + 1)-tuple consisting of K
distinct association rules, rk = pk → qk, for k = 1, . . . , K, followed by a default rule r0.
Figure 2 illustrates a rule list, d = (r1, r2, r3, r0), which for clarity, we sometimes call a K-
rule list. An association rule r = p → q is an implication corresponding to the conditional
statement, “if p, then q.” In our setting, an antecedent p is a Boolean assertion that evaluates
to either true or false for each datum xn, and a consequent q is a label prediction. For
example, (xn,1 = 0) ∧ (xn,3 = 1) → (yn = 1) is an association rule. The ﬁnal default rule r0
in a rule list can be thought of as a special association rule p0 → q0 whose antecedent p0
simply asserts true.

Let d = (r1, r2, . . . , rK, r0) be a K-rule list, where rk = pk → qk for each k = 0, . . . , K.
We introduce a useful alternate rule list representation: d = (dp, δp, q0, K), where we deﬁne
dp = (p1, . . . , pK) to be d’s preﬁx, δp = (q1, . . . , qK) ∈ {0, 1}K gives the label predictions
associated with dp, and q0 ∈ {0, 1} is the default label prediction. For example, for the
rule list in Figure 1, we would write d = (dp, δp, q0, K), where dp = (p1, p2, p3), δp = (1, 1, 1),
q0 = 0, and K = 3. Note that ((), (), q0, 0) is a well-deﬁned rule list with an empty preﬁx;
it is completely deﬁned by a single default rule.

Let dp = (p1, . . . , pk, . . . , pK) be an antecedent list, then for any k ≤ K, we deﬁne dk

(p1, . . . , pk) to be the k-preﬁx of dp. For any such k-preﬁx dk

p =
p, we say that dp starts with dk
p.

6

Learning Certifiably Optimal Rule Lists

For any given space of rule lists, we deﬁne σ(dp) to be the set of all rule lists whose preﬁxes
start with dp:

σ(dp) = {(d(cid:48)

p, δ(cid:48)

p, q(cid:48)

0, K(cid:48)) : d(cid:48)

p starts with dp}.

(1)

If dp = (p1, . . . , pK) and d(cid:48)
and extends it by a single antecedent, we say that dp is the parent of d(cid:48)
child of dp.

p = (p1, . . . , pK, pK+1) are two preﬁxes such that d(cid:48)

p starts with dp
p is a

p and that d(cid:48)

A rule list d classiﬁes datum xn by providing the label prediction qk of the ﬁrst rule rk
whose antecedent pk is true for xn. We say that an antecedent pk of antecedent list dp
captures xn in the context of dp if pk is the ﬁrst antecedent in dp that evaluates to true
for xn. We also say that a preﬁx captures those data captured by its antecedents; for a rule
list d = (dp, δp, q0, K), data not captured by the preﬁx dp are classiﬁed according to the
default label prediction q0.

Let β be a set of antecedents. We deﬁne cap(xn, β) = 1 if an antecedent in β captures
p starts

p be preﬁxes such that d(cid:48)

datum xn, and 0 otherwise. For example, let dp and d(cid:48)
with dp, then d(cid:48)

p captures all the data that dp captures:

{xn : cap(xn, dp)} ⊆ {xn : cap(xn, d(cid:48)

p)}.

Now let dp be an ordered list of antecedents, and let β be a subset of antecedents in dp.
Let us deﬁne cap(xn, β | dp) = 1 if β captures datum xn in the context of dp, i.e., if the ﬁrst
antecedent in dp that evaluates to true for xn is an antecedent in β, and 0 otherwise. Thus,
cap(xn, β | dp) = 1 only if cap(xn, β) = 1; cap(xn, β | dp) = 0 either if cap(xn, β) = 0, or if
cap(xn, β) = 1 but there is an antecedent α in dp, preceding all antecedents in β, such that
cap(xn, α) = 1. For example, if dp = (p1, . . . , pk, . . . , pK) is a preﬁx, then

cap(xn, pk | dp) =

¬ cap(xn, pk(cid:48))

∧ cap(xn, pk)

(cid:32) k−1
(cid:94)

k(cid:48)=1

(cid:33)

indicates whether antecedent pk captures datum xn in the context of dp. Now, deﬁne
supp(β, x) to be the normalized support of β,

and similarly deﬁne supp(β, x | dp) to be the normalized support of β in the context of dp,

supp(β, x) =

cap(xn, β),

supp(β, x | dp) =

cap(xn, β | dp),

(2)

(3)

Next, we address how empirical data constrains rule lists. Given training data (x, y), an
antecedent list dp = (p1, . . . , pK) implies a rule list d = (dp, δp, q0, K) with preﬁx dp, where
the label predictions δp = (q1, . . . , qK) and q0 are empirically set to minimize the number
of misclassiﬁcation errors made by the rule list on the training data. Thus for 1 ≤ k ≤ K,
label prediction qk corresponds to the majority label of data captured by antecedent pk in

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

7

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

the context of dp, and the default q0 corresponds to the majority label of data not captured
by dp. In the remainder of our presentation, whenever we refer to a rule list with a particular
preﬁx, we implicitly assume these empirically determined label predictions.

Our method is technically an associative classiﬁcation method since it leverages pre-

mined rules.

3.2 Objective Function

We deﬁne a simple objective function for a rule list d = (dp, δp, q0, K):

R(d, x, y) = (cid:96)(d, x, y) + λK.

(4)

This objective function is a regularized empirical risk; it consists of a loss (cid:96)(d, x, y), mea-
suring misclassiﬁcation error, and a regularization term that penalizes longer rule lists.
(cid:96)(d, x, y) is the fraction of training data whose labels are incorrectly predicted by d. In
our setting, the regularization parameter λ ≥ 0 is a small constant; e.g., λ = 0.01 can be
thought of as adding a penalty equivalent to misclassifying 1% of data when increasing a
rule list’s length by one association rule.

3.3 Optimization Framework

Our objective has structure amenable to global optimization via a branch-and-bound frame-
work. In particular, we make a series of important observations, each of which translates
into a useful bound, and that together interact to eliminate large parts of the search space.
We discuss these in depth in what follows:

• Lower bounds on a preﬁx also hold for every extension of that preﬁx. (§3.4, Theorem 1)

• If a rule list is not accurate enough with respect to its length, we can prune all

extensions of it. (§3.4, Lemma 2)

• We can calculate a priori an upper bound on the maximum length of an optimal rule

list. (§3.5, Theorem 6)

• Each rule in an optimal rule list must have support that is suﬃciently large. This allows
us to construct rule lists from frequent itemsets, while preserving the guarantee that
we can ﬁnd a globally optimal rule list from pre-mined rules. (§3.7, Theorem 10)

• Each rule in an optimal rule list must predict accurately. In particular, the number of
observations predicted correctly by each rule in an optimal rule list must be above a
threshold. (§3.7, Theorem 11)

• We need only consider the optimal permutation of antecedents in a preﬁx; we can

omit all other permutations. (§3.10, Theorem 15 and Corollary 16)

• If multiple observations have identical features and opposite labels, we know that any
model will make mistakes. In particular, the number of mistakes on these observations
will be at least the number of observations with the minority label. (§3.14, Theorem 20)

8

Learning Certifiably Optimal Rule Lists

3.4 Hierarchical Objective Lower Bound

We can decompose the misclassiﬁcation error in (4) into two contributions corresponding
to the preﬁx and the default rule:

(cid:96)(d, x, y) ≡ (cid:96)p(dp, δp, x, y) + (cid:96)0(dp, q0, x, y),

where dp = (p1, . . . , pK) and δp = (q1, . . . , qK);

(cid:96)p(dp, δp, x, y) =

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn]

is the fraction of data captured and misclassiﬁed by the preﬁx, and

(cid:96)0(dp, q0, x, y) =

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn]

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

1
N

N
(cid:88)

n=1

is the fraction of data not captured by the preﬁx and misclassiﬁed by the default rule.
Eliminating the latter error term gives a lower bound b(dp, x, y) on the objective,

b(dp, x, y) ≡ (cid:96)p(dp, δp, x, y) + λK ≤ R(d, x, y),

(5)

where we have suppressed the lower bound’s dependence on label predictions δp because
they are fully determined, given (dp, x, y). Furthermore, as we state next in Theorem 1,
b(dp, x, y) gives a lower bound on the objective of any rule list whose preﬁx starts with dp.

Theorem 1 (Hierarchical objective lower bound) Deﬁne b(dp, x, y) as in (5). Also,
deﬁne σ(dp) to be the set of all rule lists whose preﬁxes starts with dp, as in (1). Let d =
(dp, δp, q0, K) be a rule list with preﬁx dp, and let d(cid:48) = (d(cid:48)
0, K(cid:48)) ∈ σ(dp) be any rule
list such that its preﬁx d(cid:48)

p starts with dp and K(cid:48) ≥ K, then b(dp, x, y) ≤ R(d(cid:48), x, y).

p, δ(cid:48)

p, q(cid:48)

Proof Let dp = (p1, . . . , pK) and δp = (q1, . . . , qK); let d(cid:48)
δ(cid:48)
p = (q1, . . . , qK, qK+1, . . . , qK(cid:48)). Notice that d(cid:48)
additional mistakes:

p = (p1, . . . , pK, pK+1, . . . , pK(cid:48)) and
p yields the same mistakes as dp, and possibly

(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) =

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn]

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=1

=

1
N

N
(cid:88)

(cid:32) K
(cid:88)

n=1

k=1

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn] +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn]

(cid:33)

K(cid:48)
(cid:88)

k=K+1

= (cid:96)p(dp, δp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] ≥ (cid:96)p(dp, δp, x, y),

(6)

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K+1

where in the second equality we have used the fact that cap(xn, pk | d(cid:48)
for 1 ≤ k ≤ K. It follows that

p) = cap(xn, pk | dp)

b(dp, x, y) = (cid:96)p(dp, δp, x, y) + λK

≤ (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + λK(cid:48) = b(d(cid:48)

p, x, y) ≤ R(d(cid:48), x, y).

(7)

9

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Algorithm 1 Branch-and-bound for learning rule lists.

m=1, training data (x, y) = {(xn, yn)}N

Input: Objective function R(d, x, y), objective lower bound b(dp, x, y), set of antecedents
n=1, initial best known rule list d0 with
S = {sm}M
objective R0 = R(d0, x, y); d0 could be obtained as output from another (approximate)
algorithm, otherwise, (d0, R0) = (null, 1) provide reasonable default values
Output: Provably optimal rule list d∗ with minimum objective R∗

(dc, Rc) ← (d0, R0)
Q ← queue( [ ( ) ] )
while Q not empty do
dp ← Q.pop( )
d ← (dp, δp, q0, K)
if b(dp, x, y) < Rc then
R ← R(d, x, y)
if R < Rc then

(dc, Rc) ← (d, R)

end if
for s in S do

if s not in dp then
Q.push( (dp, s) )

end if

end for

end if
end while
(d∗, R∗) ← (dc, Rc)

(cid:46) Initialize best rule list and objective
(cid:46) Initialize queue with empty preﬁx
(cid:46) Stop when queue is empty
(cid:46) Remove preﬁx dp from the queue
(cid:46) Set label predictions δp and q0 to minimize training error
(cid:46) Bound: Apply Theorem 1
(cid:46) Compute objective of dp’s rule list d
(cid:46) Update best rule list and objective

(cid:46) Branch: Enqueue dp’s children

(cid:46) Identify provably optimal solution

To generalize, consider a sequence of preﬁxes such that each preﬁx starts with all previ-
ous preﬁxes in the sequence. It follows that the corresponding sequence of objective lower
bounds increases monotonically. This is precisely the structure required and exploited by
branch-and-bound, illustrated in Algorithm 1.

Speciﬁcally, the objective lower bound in Theorem 1 enables us to prune the state
space hierarchically. While executing branch-and-bound, we keep track of the current best
(smallest) objective Rc, thus it is a dynamic, monotonically decreasing quantity. If we
encounter a preﬁx dp with lower bound b(dp, x, y) ≥ Rc, then by Theorem 1, we do not need
to consider any rule list d(cid:48) ∈ σ(dp) whose preﬁx d(cid:48)
p starts with dp. For the objective of such
a rule list, the current best objective provides a lower bound, i.e., R(d(cid:48), x, y) ≥ b(d(cid:48)
p, x, y) ≥
b(dp, x, y) ≥ Rc, and thus d(cid:48) cannot be optimal.

Next, we state an immediate consequence of Theorem 1.

Lemma 2 (Objective lower bound with one-step lookahead) Let dp be a K-preﬁx
and let Rc be the current best objective. If b(dp, x, y) + λ ≥ Rc, then for any K(cid:48)-rule list
d(cid:48) ∈ σ(dp) whose preﬁx d(cid:48)

p starts with dp and K(cid:48) > K, it follows that R(d(cid:48), x, y) ≥ Rc.

10

Learning Certifiably Optimal Rule Lists

Proof By the deﬁnition of the lower bound (5), which includes the penalty for longer
preﬁxes,

R(d(cid:48)

p, x, y) ≥ b(d(cid:48)

p, x, y) = (cid:96)p(d(cid:48)
= (cid:96)p(d(cid:48)
= b(dp, x, y) + λ(K(cid:48) − K) ≥ b(dp, x, y) + λ ≥ Rc.

p, x, y) + λK(cid:48)
p, x, y) + λK + λ(K(cid:48) − K)

p, δ(cid:48)
p, δ(cid:48)

(8)

Therefore, even if we encounter a preﬁx dp with lower bound b(dp, x, y) ≤ Rc, as long
p that start with and are longer

as b(dp, x, y) + λ ≥ Rc, then we can prune all preﬁxes d(cid:48)
than dp.

3.5 Upper Bounds on Preﬁx Length

In this section, we derive several upper bounds on preﬁx length:

• The simplest upper bound on preﬁx length is given by the total number of available

antecedents. (Proposition 3)

• The current best objective Rc implies an upper bound on preﬁx length. (Theorem 4)

• For intuition, we state a version of the above bound that is valid at the start of

execution. (Corollary 5)

length. (Theorem 6)

• By considering speciﬁc families of preﬁxes, we can obtain tighter bounds on preﬁx

In the next section (§3.6), we use these results to derive corresponding upper bounds on the
number of preﬁx evaluations made by Algorithm 1.

Proposition 3 (Trivial upper bound on preﬁx length) Consider a state space of all
rule lists formed from a set of M antecedents, and let L(d) be the length of rule list d.
M provides an upper bound on the length of any optimal rule list d∗ ∈ argmind R(d, x, y),
i.e., L(d) ≤ M .

Proof Rule lists consist of distinct rules by deﬁnition.

At any point during branch-and-bound execution, the current best objective Rc implies

an upper bound on the maximum preﬁx length we might still have to consider.

Theorem 4 (Upper bound on preﬁx length) Consider a state space of all rule lists
formed from a set of M antecedents. Let L(d) be the length of rule list d and let Rc be the
current best objective. For all optimal rule lists d∗ ∈ argmind R(d, x, y)

L(d∗) ≤ min

(cid:23)

(cid:18)(cid:22) Rc
λ

(cid:19)

, M

,

11

(9)

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

where λ is the regularization parameter. Furthermore, if dc is a rule list with objective
R(dc, x, y) = Rc, length K, and zero misclassiﬁcation error, then for every optimal rule
list d∗ ∈ argmind R(d, x, y), if dc ∈ argmind R(d, x, y), then L(d∗) ≤ K, or otherwise if
dc /∈ argmind R(d, x, y), then L(d∗) ≤ K − 1.

Proof For an optimal rule list d∗ with objective R∗,

λL(d∗) ≤ R∗ = R(d∗, x, y) = (cid:96)(d∗, x, y) + λL(d∗) ≤ Rc.

The maximum possible length for d∗ occurs when (cid:96)(d∗, x, y) is minimized; combining with
Proposition 3 gives bound (9).

For the rest of the proof, let K∗ = L(d∗) be the length of d∗. If the current best rule

list dc has zero misclassiﬁcation error, then

λK∗ ≤ (cid:96)(d∗, x, y) + λK∗ = R(d∗, x, y) ≤ Rc = R(dc, x, y) = λK,

and thus K∗ ≤ K. If the current best rule list is suboptimal, i.e., dc /∈ argmind R(d, x, y),
then

λK∗ ≤ (cid:96)(d∗, x, y) + λK∗ = R(d∗, x, y) < Rc = R(dc, x, y) = λK,

in which case K∗ < K, i.e., K∗ ≤ K − 1, since K is an integer.

The latter part of Theorem 4 tells us that if we only need to identify a single instance
of an optimal rule list d∗ ∈ argmind R(d, x, y), and we encounter a perfect K-rule list with
zero misclassiﬁcation error, then we can prune all preﬁxes of length K or greater.

Corollary 5 (Simple upper bound on preﬁx length) Let L(d) be the length of rule
list d. For all optimal rule lists d∗ ∈ argmind R(d, x, y),

L(d∗) ≤ min

(cid:23)

(cid:18)(cid:22) 1
2λ

(cid:19)

, M

.

(10)

Proof Let d = ((), (), q0, 0) be the empty rule list; it has objective R(d, x, y) = (cid:96)(d, x, y) ≤
1/2, which gives an upper bound on Rc. Combining with (9) and Proposition 3 gives (10).

For any particular preﬁx dp, we can obtain potentially tighter upper bounds on preﬁx

length for the family of all preﬁxes that start with dp.

Theorem 6 (Preﬁx-speciﬁc upper bound on preﬁx length) Let d = (dp, δp, q0, K) be
a rule list, let d(cid:48) = (d(cid:48)
p starts with dp, and
let Rc be the current best objective. If d(cid:48)

0, K(cid:48)) ∈ σ(dp) be any rule list such that d(cid:48)
p has lower bound b(d(cid:48)

p, x, y) < Rc, then

p, q(cid:48)

p, δ(cid:48)

(cid:18)

K(cid:48) < min

K +

(cid:22) Rc − b(dp, x, y)
λ

(cid:23)

(cid:19)

, M

.

(11)

12

Learning Certifiably Optimal Rule Lists

Proof First, note that K(cid:48) ≥ K, since d(cid:48)

p starts with dp. Now recall from (7) that

b(dp, x, y) = (cid:96)p(dp, δp, x, y) + λK ≤ (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + λK(cid:48) = b(d(cid:48)

p, x, y),

and from (6) that (cid:96)p(dp, δp, x, y) ≤ (cid:96)p(d(cid:48)
gives

p, δ(cid:48)

p, x, y). Combining these bounds and rearranging

b(d(cid:48)

p, x, y) = (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + λK + λ(K(cid:48) − K)

≥ (cid:96)p(dp, δp, x, y) + λK + λ(K(cid:48) − K) = b(dp, x, y) + λ(K(cid:48) − K).

(12)

Combining (12) with b(d(cid:48)

p, x, y) < Rc and Proposition 3 gives (11).

We can view Theorem 6 as a generalization of our one-step lookahead bound (Lemma 2),
as (11) is equivalently a bound on K(cid:48) − K, an upper bound on the number of remain-
ing ‘steps’ corresponding to an iterative sequence of single-rule extensions of a preﬁx dp.
Notice that when d = ((), (), q0, 0) is the empty rule list, this bound replicates (9), since
b(dp, x, y) = 0.

3.6 Upper Bounds on the Number of Preﬁx Evaluations

In this section, we use our upper bounds on preﬁx length from §3.5 to derive corresponding
upper bounds on the number of preﬁx evaluations made by Algorithm 1. First, we present
Theorem 7, in which we use information about the state of Algorithm 1’s execution to
calculate, for any given execution state, upper bounds on the number of additional preﬁx
evaluations that might be required for the execution to complete. The relevant execution
state depends on the current best objective Rc and information about preﬁxes we are
planning to evaluate, i.e., preﬁxes in the queue Q of Algorithm 1. We deﬁne the number
of remaining preﬁx evaluations as the number of preﬁxes that are currently in or will be
inserted into the queue.

We use Theorem 7 in some of our empirical results (§6, Figure 18) to help illustrate
the dramatic impact of certain algorithm optimizations. The execution trace of this upper
bound on remaining preﬁx evaluations complements the execution traces of other quanti-
ties, e.g., that of the current best objective Rc. After presenting Theorem 7, we also give
two weaker propositions that provide useful intuition. In particular, Proposition 9 is a prac-
tical approximation to Theorem 7 that is signiﬁcantly easier to compute; we use it in our
implementation as a metric of execution progress that we display to the user.

Theorem 7 (Fine-grained upper bound on remaining preﬁx evaluations) Con-
sider the state space of all rule lists formed from a set of M antecedents, and consider Algo-
rithm 1 at a particular instant during execution. Let Rc be the current best objective, let Q
be the queue, and let L(dp) be the length of preﬁx dp. Deﬁne Γ(Rc, Q) to be the number of
remaining preﬁx evaluations, then

Γ(Rc, Q) ≤

(cid:88)

f (dp)
(cid:88)

dp∈Q

k=0

(M − L(dp))!
(M − L(dp) − k)!

,

(13)

13

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

where

f (dp) = min

(cid:18)(cid:22) Rc − b(dp, x, y)

(cid:23)

(cid:19)

, M − L(dp)

.

λ

Proof The number of remaining preﬁx evaluations is equal to the number of preﬁxes that
are currently in or will be inserted into queue Q. For any such preﬁx dp, Theorem 6 gives
an upper bound on the length of any preﬁx d(cid:48)

p that starts with dp:

(cid:18)

L(d(cid:48)

p) ≤ min

L(dp) +

(cid:22) Rc − b(dp, x, y)
λ

(cid:23)

(cid:19)

, M

≡ U (dp).

This gives an upper bound on the number of remaining preﬁx evaluations:

Γ(Rc, Q) ≤

P (M − L(dp), k) =

(cid:88)

U (dp)−L(dp)
(cid:88)

dp∈Q

k=0

(cid:88)

f (dp)
(cid:88)

dp∈Q

k=0

(M − L(dp))!
(M − L(dp) − k)!

,

where P (m, k) denotes the number of k-permutations of m.

Proposition 8 is strictly weaker than Theorem 7 and is the starting point for its deriva-
tion. It is a na¨ıve upper bound on the total number of preﬁx evaluations over the course
of Algorithm 1’s execution. It only depends on the number of rules and the regularization
parameter λ; i.e., unlike Theorem 7, it does not use algorithm execution state to bound the
size of the search space.

Proposition 8 (Upper bound on the total number of preﬁx evaluations) Deﬁne
Γtot(S) to be the total number of preﬁxes evaluated by Algorithm 1, given the state space of
all rule lists formed from a set S of M rules. For any set S of M rules,

Γtot(S) ≤

K
(cid:88)

k=0

M !
(M − k)!

,

where K = min((cid:98)1/2λ(cid:99), M ).

Proof By Corollary 5, K ≡ min((cid:98)1/2λ(cid:99), M ) gives an upper bound on the length of any
optimal rule list. Since we can think of our problem as ﬁnding the optimal selection and
permutation of k out of M rules, over all k ≤ K,

Γtot(S) ≤ 1 +

P (M, k) =

K
(cid:88)

k=1

K
(cid:88)

k=0

M !
(M − k)!

.

Our next upper bound is strictly tighter than the bound in Proposition 8. Like Theo-
rem 7, it uses the current best objective and information about the lengths of preﬁxes in the

14

Learning Certifiably Optimal Rule Lists

queue to constrain the lengths of preﬁxes in the remaining search space. However, Proposi-
tion 9 is weaker than Theorem 7 because it leverages only coarse-grained information from
the queue. Speciﬁcally, Theorem 7 is strictly tighter because it additionally incorporates
preﬁx-speciﬁc objective lower bound information from preﬁxes in the queue, which further
constrains the lengths of preﬁxes in the remaining search space.

Proposition 9 (Coarse-grained upper bound on remaining preﬁx evaluations)
Consider a state space of all rule lists formed from a set of M antecedents, and consider
Algorithm 1 at a particular instant during execution. Let Rc be the current best objective,
let Q be the queue, and let L(dp) be the length of preﬁx dp. Let Qj be the number of preﬁxes
of length j in Q,

Qj = (cid:12)

(cid:12){dp : L(dp) = j, dp ∈ Q}(cid:12)
(cid:12)

and let J = argmaxdp∈Q L(dp) be the length of the longest preﬁx in Q. Deﬁne Γ(Rc, Q) to
be the number of remaining preﬁx evaluations, then

Γ(Rc, Q) ≤

J
(cid:88)

j=1

Qj

(cid:32)K−j
(cid:88)

k=0

(M − j)!
(M − j − k)!

(cid:33)

,

where K = min((cid:98)Rc/λ(cid:99), M ).

Proof The number of remaining preﬁx evaluations is equal to the number of preﬁxes that
are currently in or will be inserted into queue Q. For any such remaining preﬁx dp, Theorem 4
gives an upper bound on its length; deﬁne K to be this bound: L(dp) ≤ min((cid:98)Rc/λ(cid:99), M ) ≡ K.
For any preﬁx dp in queue Q with length L(dp) = j, the maximum number of preﬁxes that
start with dp and remain to be evaluated is:

K−j
(cid:88)

k=0

P (M − j, k) =

K−j
(cid:88)

k=0

(M − j)!
(M − j − k)!

,

where P (T, k) denotes the number of k-permutations of T . This gives an upper bound on
the number of remaining preﬁx evaluations:

Γ(Rc, Q) ≤

J
(cid:88)

j=0

Qj

(cid:32)K−j
(cid:88)

k=0

P (M − j, k)

=

Qj

(cid:33)

J
(cid:88)

j=0

(cid:32)K−j
(cid:88)

k=0

(M − j)!
(M − j − k)!

(cid:33)

.

3.7 Lower Bounds on Antecedent Support

In this section, we give two lower bounds on the normalized support of each antecedent in
any optimal rule list; both are related to the regularization parameter λ.

15

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Theorem 10 (Lower bound on antecedent support) Let d∗ = (dp, δp, q0, K) be any
optimal rule list with objective R∗, i.e., d∗ ∈ argmind R(d, x, y). For each antecedent pk
in preﬁx dp = (p1, . . . , pK), the regularization parameter λ provides a lower bound on the
normalized support of pk,

λ ≤ supp(pk, x | dp).

(14)

Proof Let d∗ = (dp, δp, q0, K) be an optimal rule list with preﬁx dp = (p1, . . . , pK) and
0, K − 1) derived from d∗ by
labels δp = (q1, . . . , qK). Consider the rule list d = (d(cid:48)
deleting a rule pi → qi, therefore d(cid:48)
p = (q1, . . . , qi−1,
i+1, . . . , q(cid:48)
q(cid:48)

k need not be the same as qk, for k > i and k = 0.
The largest possible discrepancy between d∗ and d would occur if d∗ correctly classiﬁed

p, δ(cid:48)
p = (p1, . . . , pi−1, pi+1, . . . , pK) and δ(cid:48)

K), where q(cid:48)

p, q(cid:48)

all the data captured by pi, while d misclassiﬁed these data. This gives an upper bound:
R(d, x, y) = (cid:96)(d, x, y) + λ(K − 1) ≤ (cid:96)(d∗, x, y) + supp(pi, x | dp) + λ(K − 1)

= R(d∗, x, y) + supp(pi, x | dp) − λ
= R∗ + supp(pi, x | dp) − λ

(15)

where supp(pi, x | dp) is the normalized support of pi in the context of dp, deﬁned in (3),
and the regularization ‘bonus’ comes from the fact that d is one rule shorter than d∗.

At the same time, we must have R∗ ≤ R(d, x, y) for d∗ to be optimal. Combining this
with (15) and rearranging gives (14), therefore the regularization parameter λ provides a
lower bound on the support of an antecedent pi in an optimal rule list d∗.

Thus, we can prune a preﬁx dp if any of its antecedents captures less than a fraction λ
of data, even if b(dp, x, y) < R∗. Notice that the bound in Theorem 10 depends on the
antecedents, but not the label predictions, and thus does not account for misclassiﬁcation
error. Theorem 11 gives a tighter bound by leveraging this additional information, which
speciﬁcally tightens the upper bound on R(d, x, y) in (15).

Theorem 11 (Lower bound on accurate antecedent support) Let d∗ be any opti-
mal rule list with objective R∗, i.e., d∗ = (dp, δp, q0, K) ∈ argmind R(d, x, y). Let d∗ have
preﬁx dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK). For each rule pk → qk in d∗, deﬁne ak
to be the fraction of data that are captured by pk and correctly classiﬁed:

ak ≡

cap(xn, pk | dp) ∧ 1[qk = yn].

1
N

N
(cid:88)

n=1

The regularization parameter λ provides a lower bound on ak:

(16)

(17)

0, K − 1) be the rule list derived from d∗ by
Proof As in Theorem 10, let d = (d(cid:48)
deleting a rule pi → qi. Now, let us deﬁne (cid:96)i to be the portion of R∗ due to this rule’s
misclassiﬁcation error,

p, q(cid:48)

p, δ(cid:48)

(cid:96)i ≡

cap(xn, pi | dp) ∧ 1[qi (cid:54)= yn].

1
N

N
(cid:88)

n=1

λ ≤ ak.

16

Learning Certifiably Optimal Rule Lists

The largest discrepancy between d∗ and d would occur if d misclassiﬁed all the data captured
by pi. This gives an upper bound on the diﬀerence between the misclassiﬁcation error of d
and d∗:

(cid:96)(d, x, y) − (cid:96)(d∗, x, y) ≤ supp(pi, x | dp) − (cid:96)i

=

=

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

cap(xn, pi | dp) −

cap(xn, pi | dp) ∧ 1[qi (cid:54)= yn]

1
N

N
(cid:88)

n=1

cap(xn, pi | dp) ∧ 1[qi = yn] = ai,

where we deﬁned ai in (16). Relating this bound to the objectives of d and d∗ gives

R(d, x, y) = (cid:96)(d, x, y) + λ(K − 1) ≤ (cid:96)(d∗, x, y) + ai + λ(K − 1)

= R(d∗, x, y) + ai − λ
= R∗ + ai − λ.

(18)

Combining (18) with the requirement R∗ ≤ R(d, x, y) gives the bound λ ≤ ai.

Thus, we can prune a preﬁx if any of its rules correctly classiﬁes less than a fraction λ
of data. While the lower bound in Theorem 10 is a sub-condition of the lower bound in
Theorem 11, we can still leverage both—since the sub-condition is easier to check, check-
ing it ﬁrst can accelerate pruning. In addition to applying Theorem 10 in the context of
constructing rule lists, we can furthermore apply it in the context of rule mining (§3.1).
Speciﬁcally, it implies that we should only mine rules with normalized support of at least λ;
we need not mine rules with a smaller fraction of observations.2 In contrast, we can only
apply Theorem 11 in the context of constructing rule lists; it depends on the misclassiﬁ-
cation error associated with each rule in a rule list, thus it provides a lower bound on the
number of observations that each such rule must correctly classify.

3.8 Upper Bound on Antecedent Support

In the previous section (§3.7), we proved lower bounds on antecedent support; in Ap-
pendix A, we give an upper bound on antecedent support. Speciﬁcally, Theorem 21 shows
that an antecedent’s support in a rule list cannot be too similar to the set of data not
captured by preceding antecedents in the rule list. In particular, Theorem 21 implies that
we should only mine rules with normalized support less than or equal to 1 − λ; we need
not mine rules with a larger fraction of observations. Note that we do not otherwise use
this bound in our implementation, because we did not observe a meaningful beneﬁt in
preliminary experiments.

3.9 Antecedent Rejection and its Propagation

In this section, we demonstrate further consequences of our lower (§3.7) and upper bounds
(§3.8) on antecedent support, under a uniﬁed framework we refer to as antecedent rejec-
tion. Let dp = (p1, . . . , pK) be a preﬁx, and let pk be an antecedent in dp. Deﬁne pk to have

2. We describe our application of this idea in Appendix E, where we provide details on data processing.

17

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

insuﬃcient support in dp if it does not obey the bound in (14) of Theorem 10. Deﬁne pk
to have insuﬃcient accurate support in dp if it does not obey the bound in (17) of Theo-
rem 11. Deﬁne pk to have excessive support in dp if it does not obey the bound in (37) of
Theorem 21 (Appendix A). If pk in the context of dp has insuﬃcient support, insuﬃcient
accurate support, or excessive support, let us say that preﬁx dp rejects antecedent pK. Next,
in Theorem 12, we describe large classes of related rule lists whose preﬁxes all reject the
same antecedent.

Theorem 12 (Antecedent rejection propagates) For any preﬁx dp = (p1, . . . , pK), let
φ(dp) denote the set of all preﬁxes d(cid:48)
p such that the set of all antecedents in dp is a subset
of the set of all antecedents in d(cid:48)
p, i.e.,

φ(dp) = {d(cid:48)

p = (p(cid:48)

1, . . . , p(cid:48)

K(cid:48)) s.t. {pk : pk ∈ dp} ⊆ {p(cid:48)

κ : p(cid:48)

κ ∈ d(cid:48)

p}, K(cid:48) ≥ K}.

(19)

Let d = (dp, δp, q0, K) be a rule list with preﬁx dp = (p1, . . . , pK−1, pK), such that dp rejects
its last antecedent pK, either because pK in the context of dp has insuﬃcient support, insuf-
ﬁcient accurate support, or excessive support. Let dK−1
= (p1, . . . , pK−1) be the ﬁrst K − 1
antecedents of dp. Let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp = (P1, . . . , PK(cid:48)−1,
PK(cid:48), . . . , Pκ) such that Dp starts with DK(cid:48)−1
) and antecedent
PK(cid:48) = pK. It follows that preﬁx Dp rejects PK(cid:48) for the same reason that dp rejects pK, and
furthermore, D cannot be optimal, i.e., D /∈ argmind† R(d†, x, y).

= (P1, . . . , PK(cid:48)−1) ∈ φ(dK−1

p

p

p

Proof Combine Proposition 13, Proposition 14, and Proposition 22. The ﬁrst two are
found below, and the last in Appendix A.

Theorem 12 implies potentially signiﬁcant computational savings. We know from Theo-
rems 10, 11, and 21 that during branch-and-bound execution, if we ever encounter a preﬁx
dp = (p1, . . . , pK−1, pK) that rejects its last antecedent pK, then we can prune dp. By The-
orem 12, we can also prune any preﬁx d(cid:48)
p whose antecedents contains the set of antecedents
in dp, in almost any order, with the constraint that all antecedents in {p1, . . . , pK−1} pre-
cede pK. These latter antecedents are also rejected directly by the bounds in Theorems 10,
11, and 21; this is how our implementation works in practice. In a preliminary implemen-
tation (not shown), we maintained additional data structures to support the direct use of
Theorem 12. We leave the design of eﬃcient data structures for this task as future work.

Proposition 13 (Insuﬃcient antecedent support propagates) First deﬁne φ(dp) as
in (19), and let dp = (p1, . . . , pK−1, pK) be a preﬁx, such that its last antecedent pK has
insuﬃcient support, i.e., the opposite of the bound in (14): supp(pK, x | dp) < λ. Let dK−1
=
(p1, . . . , pK−1), and let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp = (P1, . . . , PK(cid:48)−1,
PK(cid:48), . . . , Pκ), such that Dp starts with DK(cid:48)−1
) and PK(cid:48) = pK.
It follows that PK(cid:48) has insuﬃcient support in preﬁx Dp, and furthermore, D cannot be
optimal, i.e., D /∈ argmind R(d, x, y).

= (P1, . . . , PK(cid:48)−1) ∈ φ(dK−1

p

p

p

18

Learning Certifiably Optimal Rule Lists

Proof The support of pK in dp depends only on the set of antecedents in dK

p = (p1, . . . , pK):

supp(pK, x | dp) =

cap(xn, pK | dp) =

(cid:0)¬ cap(xn, dK−1

)(cid:1) ∧ cap(xn, pK)

p

N
(cid:88)

n=1
N
(cid:88)

1
N

1
N

=

(cid:32)K−1
(cid:94)

n=1

k=1

(cid:33)

¬ cap(xn, pk)

∧ cap(xn, pK) < λ,

and the support of PK(cid:48) in Dp depends only on the set of antecedents in DK(cid:48)

p = (P1, . . . , PK(cid:48)):

supp(PK(cid:48), x | Dp) =

cap(xn, PK(cid:48) | Dp) =

¬ cap(xn, Pk)

∧ cap(xn, PK(cid:48))

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

1
N

1
N

1
N

N
(cid:88)

(cid:32)K(cid:48)−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

n=1

k=1

≤

=

(cid:33)

(cid:33)

(cid:33)

¬ cap(xn, pk)

∧ cap(xn, PK(cid:48))

¬ cap(xn, pk)

∧ cap(xn, pK)

= supp(pK, x | dp) < λ.

(20)

The ﬁrst inequality reﬂects the condition that DK(cid:48)−1
set of antecedents in DK(cid:48)−1
reﬂects the fact that PK(cid:48) = pK. Thus, P (cid:48)
by Theorem 10, D cannot be optimal, i.e., D /∈ argmind R(d, x, y).

), which implies that the
, and the next equality
K has insuﬃcient support in preﬁx Dp, therefore

contains the set of antecedents in dK−1

∈ φ(dK−1
p

p

p

p

Proposition 14 (Insuﬃcient accurate antecedent support propagates) Let φ(dp)
denote the set of all preﬁxes d(cid:48)
p such that the set of all antecedents in dp is a subset of
the set of all antecedents in d(cid:48)
p, as in (19). Let d = (dp, δp, q0, K) be a rule list with preﬁx
dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK), such that the last antecedent pK has insuﬃ-
cient accurate support, i.e., the opposite of the bound in (17):

1
N

N
(cid:88)

n=1

cap(xn, pK | dp) ∧ 1[qK = yn] < λ.

Let dK−1
p
(P1, . . . , Pκ) and labels ∆p = (Q1, . . . , Qκ), such that Dp starts with DK(cid:48)−1
∈ φ(dK−1
p
and furthermore, D /∈ argmind† R(d†, x, y).

= (p1, . . . , pK−1) and let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp =
= (P1, . . . , PK(cid:48)−1)
) and PK(cid:48) = pK. It follows that PK(cid:48) has insuﬃcient accurate support in preﬁx Dp,

p

19

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Proof The accurate support of PK(cid:48) in Dp is insuﬃcient:

1
N

N
(cid:88)

n=1

cap(xn, PK(cid:48) | Dp) ∧ 1[QK(cid:48) = yn]

=

≤

=

=

≤

1
N

1
N

1
N

1
N

1
N

N
(cid:88)

(cid:32)K(cid:48)−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

k=1

n=1

N
(cid:88)

n=1
N
(cid:88)

n=1

(cid:33)

(cid:33)

(cid:33)

¬ cap(xn, Pk)

∧ cap(xn, PK(cid:48)) ∧ 1[QK(cid:48) = yn]

¬ cap(xn, pk)

∧ cap(xn, PK(cid:48)) ∧ 1[QK(cid:48) = yn]

¬ cap(xn, pk)

∧ cap(xn, pK) ∧ 1[QK(cid:48) = yn]

cap(xn, pK | dp) ∧ 1[QK(cid:48) = yn]

cap(xn, pK | dp) ∧ 1[qK = yn] < λ.

The ﬁrst inequality reﬂects the condition that DK(cid:48)−1
), the next equality reﬂects
the fact that PK(cid:48) = pK. For the following equality, notice that QK(cid:48) is the majority class
label of data captured by PK(cid:48) in Dp, and qK is the majority class label of data captured
by PK in dp, and recall from (20) that supp(PK(cid:48), x | Dp) ≤ supp(pK, x | dp). By Theorem 11,
D /∈ argmind† R(d†, x, y).

∈ φ(dK−1
p

p

Propositions 13 and 14, combined with Proposition 22 (Appendix A), constitute the

proof of Theorem 12.

3.10 Equivalent Support Bound

If two preﬁxes capture the same data, and one is more accurate than the other, then there
is no beneﬁt to considering preﬁxes that start with the less accurate one. Let dp be a
preﬁx, and consider the best possible rule list whose preﬁx starts with dp. If we take its
antecedents in dp and replace them with another preﬁx with the same support (that could
include diﬀerent antecedents), then its objective can only become worse or remain the same.
Formally, let Dp be a preﬁx, and let ξ(Dp) be the set of all preﬁxes that capture exactly
the same data as Dp. Now, let d be a rule list with preﬁx dp in ξ(Dp), such that d has
the minimum objective over all rule lists with preﬁxes in ξ(Dp). Finally, let d(cid:48) be a rule
list whose preﬁx d(cid:48)
p starts with dp, such that d(cid:48) has the minimum objective over all rule
lists whose preﬁxes start with dp. Theorem 15 below implies that d(cid:48) also has the minimum
objective over all rule lists whose preﬁxes start with any preﬁx in ξ(Dp).

Theorem 15 (Equivalent support bound) Deﬁne σ(dp) to be the set of all rule lists
whose preﬁxes start with dp, as in (1). Let d = (dp, δp, q0, K) be a rule list with preﬁx
dp = (p1, . . . , pK), and let D = (Dp, ∆p, Q0, κ) be a rule list with preﬁx Dp = (P1, . . . , Pκ),

20

Learning Certifiably Optimal Rule Lists

such that dp and Dp capture the same data, i.e.,

{xn : cap(xn, dp)} = {xn : cap(xn, Dp)}.

If the objective lower bounds of d and D obey b(dp, x, y) ≤ b(Dp, x, y), then the objective of
the optimal rule list in σ(dp) gives a lower bound on the objective of the optimal rule list
in σ(Dp):

min
d(cid:48)∈σ(dp)

R(d(cid:48), x, y) ≤ min

R(D(cid:48), x, y).

D(cid:48)∈σ(Dp)

(21)

Proof See Appendix B for the proof of Theorem 15.

Thus, if preﬁxes dp and Dp capture the same data, and their objective lower bounds obey
b(dp, x, y) ≤ b(Dp, x, y), Theorem 15 implies that we can prune Dp. Next, in Sections 3.11
and 3.12, we highlight and analyze the special case of preﬁxes that capture the same data
because they contain the same antecedents.

3.11 Permutation Bound

If two preﬁxes are composed of the same antecedents, i.e., they contain the same antecedents
up to a permutation, then they capture the same data, and thus Theorem 15 applies.
Therefore, if one is more accurate than the other, then there is no beneﬁt to considering
preﬁxes that start with the less accurate one. Let dp be a preﬁx, and consider the best
possible rule list whose preﬁx starts with dp. If we permute its antecedents in dp, then its
objective can only become worse or remain the same.

Formally, let P = {pk}K

k=1 be a set of K antecedents, and let Π be the set of all K-preﬁxes
corresponding to permutations of antecedents in P . Let preﬁx dp in Π have the minimum
preﬁx misclassiﬁcation error over all preﬁxes in Π. Also, let d(cid:48) be a rule list whose preﬁx d(cid:48)
p
starts with dp, such that d(cid:48) has the minimum objective over all rule lists whose preﬁxes start
with dp. Corollary 16 below, which can be viewed as special case of Theorem 15, implies
that d(cid:48) also has the minimum objective over all rule lists whose preﬁxes start with any
preﬁx in Π.

p, q(cid:48)

p, δ(cid:48)

0, K(cid:48)) : d(cid:48)

Corollary 16 (Permutation bound) Let π be any permutation of {1, . . . , K}, and de-
ﬁne σ(dp) = {(d(cid:48)
p starts with dp} to be the set of all rule lists whose preﬁxes
start with dp. Let d = (dp, δp, q0, K) and D = (Dp, ∆p, Q0, K) denote rule lists with preﬁxes
dp = (p1, . . . , pK) and Dp = (pπ(1), . . . , pπ(K)), respectively, i.e., the antecedents in Dp cor-
respond to a permutation of the antecedents in dp. If the objective lower bounds of d and D
obey b(dp, x, y) ≤ b(Dp, x, y), then the objective of the optimal rule list in σ(dp) gives a
lower bound on the objective of the optimal rule list in σ(Dp):

min
d(cid:48)∈σ(dp)

R(d(cid:48), x, y) ≤ min

R(D(cid:48), x, y).

D(cid:48)∈σ(Dp)

Proof Since preﬁxes dp and Dp contain the same antecedents, they both capture the same
data. Thus, we can apply Theorem 15.

21

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Thus if preﬁxes dp and Dp have the same antecedents, up to a permutation, and their
objective lower bounds obey b(dp, x, y) ≤ b(Dp, x, y), Corollary 16 implies that we can
prune Dp. We call this symmetry-aware pruning, and we illustrate the subsequent compu-
tational savings next in §3.12.

3.12 Upper Bound on Preﬁx Evaluations with Symmetry-aware Pruning

Here, we present an upper bound on the total number of preﬁx evaluations that accounts for
the eﬀect of symmetry-aware pruning (§3.11). Since every subset of K antecedents generates
an equivalence class of K! preﬁxes equivalent up to permutation, symmetry-aware pruning
dramatically reduces the search space.

First, notice that Algorithm 1 describes a breadth-ﬁrst exploration of the state space of
rule lists. Now suppose we integrate symmetry-aware pruning into our execution of branch-
and-bound, so that after evaluating preﬁxes of length K, we only keep a single best preﬁx
from each set of preﬁxes equivalent up to a permutation.

Theorem 17 (Upper bound on preﬁx evaluations with symmetry-aware pruning)
Consider a state space of all rule lists formed from a set S of M antecedents, and consider
the branch-and-bound algorithm with symmetry-aware pruning. Deﬁne Γtot(S) to be the total
number of preﬁxes evaluated. For any set S of M rules,

Γtot(S) ≤ 1 +

K
(cid:88)

k=1

1
(k − 1)!

·

M !
(M − k)!

,

where K = min((cid:98)1/2λ(cid:99), M ).

Proof By Corollary 5, K ≡ min((cid:98)1/2λ(cid:99), M ) gives an upper bound on the length of any op-
timal rule list. The algorithm begins by evaluating the empty preﬁx, followed by M preﬁxes
of length k = 1, then P (M, 2) preﬁxes of length k = 2, where P (M, 2) is the number of size-2
subsets of {1, . . . , M }. Before proceeding to length k = 3, we keep only C(M, 2) preﬁxes of
length k = 2, where C(M, k) denotes the number of k-combinations of M . Now, the number
of length k = 3 preﬁxes we evaluate is C(M, 2)(M − 2). Propagating this forward gives

Γtot(S) ≤ 1 +

C(M, k − 1)(M − k + 1) = 1 +

K
(cid:88)

k=1

K
(cid:88)

k=1

1
(k − 1)!

·

M !
(M − k)!

.

Pruning based on permutation symmetries thus yields signiﬁcant computational savings.
Let us compare, for example, to the na¨ıve number of preﬁx evaluations given by the upper
bound in Proposition 8. If M = 100 and K = 5, then the na¨ıve number is about 9.1 × 109,
while the reduced number due to symmetry-aware pruning is about 3.9 × 108, which is
smaller by a factor of about 23. If M = 1000 and K = 10, the number of evaluations falls
from about 9.6 × 1029 to about 2.7 × 1024, which is smaller by a factor of about 360,000.

While 1024 seems infeasibly enormous, it does not represent the number of rule lists we
evaluate. As we show in our experiments (§6), our permutation bound in Corollary 16 and

22

Learning Certifiably Optimal Rule Lists

our other bounds together conspire to reduce the search space to a size manageable on a
single computer. The choice of M = 1000 and K = 10 in our example above corresponds to
the state space size our eﬀorts target. K = 10 rules represents a (heuristic) upper limit on
the size of an interpretable rule list, and M = 1000 represents the approximate number of
rules with suﬃciently high support (Theorem 10) we expect to obtain via rule mining (§3.1).

3.13 Similar Support Bound

We now present a relaxation of Theorem 15, our equivalent support bound. Theorem 18
implies that if we know that no extensions of a preﬁx dp are better than the current best
objective, then we can prune all preﬁxes with support similar to dp’s support. Understanding
how to exploit this result in practice represents an exciting direction for future work; our
implementation (§5) does not currently leverage the bound in Theorem 18.

Theorem 18 (Similar support bound) Deﬁne σ(dp) to be the set of all rule lists whose
preﬁxes start with dp, as in (1). Let dp = (p1, . . . , pK) and Dp = (P1, . . . , Pκ) be preﬁxes
that capture nearly the same data. Speciﬁcally, deﬁne ω to be the normalized support of data
captured by dp and not captured by Dp, i.e.,

ω ≡

¬ cap(xn, Dp) ∧ cap(xn, dp).

Similarly, deﬁne Ω to be the normalized support of data captured by Dp and not captured
by dp, i.e.,

Ω ≡

¬ cap(xn, dp) ∧ cap(xn, Dp).

We can bound the diﬀerence between the objectives of the optimal rule lists in σ(dp) and
σ(Dp) as follows:

min
D†∈σ(Dp)

d†∈σ(dp)

R(D†, x, y) − min

R(d†, x, y) ≥ b(Dp, x, y) − b(dp, x, y) − ω − Ω,

(24)

where b(dp, x, y) and b(Dp, x, y) are the objective lower bounds of d and D, respectively.

Proof See Appendix C for the proof of Theorem 18.

Theorem 18 implies that if preﬁxes dp and Dp are similar, and we know the optimal

objective of rule lists starting with dp, then

min
D(cid:48)∈σ(Dp)

d(cid:48)∈σ(dp)

R(D(cid:48), x, y) ≥ min

R(d(cid:48), x, y) + b(Dp, x, y) − b(dp, x, y) − χ

≥ Rc + b(Dp, x, y) − b(dp, x, y) − χ,

where Rc is the current best objective, and χ is the normalized support of the set of data
captured either exclusively by dp or exclusively by Dp. It follows that

min
D(cid:48)∈σ(Dp)

R(D(cid:48), x, y) ≥ Rc + b(Dp, x, y) − b(dp, x, y) − χ ≥ Rc

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

(22)

(23)

23

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if b(Dp, x, y) − b(dp, x, y) ≥ χ. To conclude, we summarize this result and combine it with
our notion of lookahead from Lemma 2. During branch-and-bound execution, if we demon-
strate that mind(cid:48)∈σ(dp) R(d(cid:48), x, y) ≥ Rc, then we can prune all preﬁxes that start with any
preﬁx D(cid:48)

p in the following set:

(cid:40)

(cid:41)

D(cid:48)

p : b(D(cid:48)

p, x, y) + λ − b(dp, x, y) ≥

cap(xn, dp) ⊕ cap(xn, D(cid:48)
p)

,

1
N

N
(cid:88)

n=1

where the symbol ⊕ denotes the logical operation, exclusive or (XOR).

3.14 Equivalent Points Bound

The bounds in this section quantify the following: If multiple observations that are not
captured by a preﬁx dp have identical features and opposite labels, then no rule list that
starts with dp can correctly classify all these observations. For each set of such observations,
the number of mistakes is at least the number of observations with the minority label within
the set.

Consider a data set {(xn, yn)}N

m=1. Deﬁne dis-
tinct observations to be equivalent if they are captured by exactly the same antecedents,
i.e., xi (cid:54)= xj are equivalent if

n=1 and also a set of antecedents {sm}M

1
M

M
(cid:88)

m=1

1[cap(xi, sm) = cap(xj, sm)] = 1.

Notice that we can partition a data set into sets of equivalent points; let {eu}U
u=1 enumerate
these sets. Let eu be the equivalent points set that contains observation xi. Now deﬁne θ(eu)
to be the normalized support of the minority class label with respect to set eu, e.g., let

eu = {xn : ∀m ∈ [M ], 1[cap(xn, sm) = cap(xi, sm)]},

and let qu be the minority class label among points in eu, then

θ(eu) =

1[xn ∈ eu] 1[yn = qu].

(25)

1
N

N
(cid:88)

n=1

The existence of equivalent points sets with non-singleton support yields a tighter ob-
jective lower bound that we can combine with our other bounds; as our experiments demon-
strate (§6), the practical consequences can be dramatic. First, for intuition, we present a
general bound in Proposition 19; next, we explicitly integrate this bound into our framework
in Theorem 20.

Proposition 19 (General equivalent points bound) Let d = (dp, δp, q0, K) be a rule list,
then

R(d, x, y) ≥

θ(eu) + λK.

U
(cid:88)

u=1

24

Learning Certifiably Optimal Rule Lists

Proof Recall that the objective is R(d, x, y) = (cid:96)(d, x, y) + λK, where the misclassiﬁcation
error (cid:96)(d, x, y) is given by

(cid:96)(d, x, y) = (cid:96)0(dp, q0, x, y) + (cid:96)p(dp, δp, x, y)

=

1
N

(cid:32)

N
(cid:88)

n=1

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn] +

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn]

.

(cid:33)

K
(cid:88)

k=1

Any particular rule list uses a speciﬁc rule, and therefore a single class label, to classify
all points within a set of equivalent points. Thus, for a set of equivalent points u, the rule
list d correctly classiﬁes either points that have the majority class label, or points that have
the minority class label. It follows that d misclassiﬁes a number of points in u at least as
great as the number of points with the minority class label. To translate this into a lower
bound on (cid:96)(d, x, y), we ﬁrst sum over all sets of equivalent points, and then for each such
set, count diﬀerences between class labels and the minority class label of the set, instead of
counting mistakes:

(cid:96)(d, x, y)

=

≥

1
N

1
N

U
(cid:88)

N
(cid:88)

(cid:32)

u=1

n=1

U
(cid:88)

N
(cid:88)

(cid:32)

u=1

n=1

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn] +

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn]

1[xn ∈ eu]

¬ cap(xn, dp) ∧ 1[yn = qu] +

cap(xn, pk | dp) ∧ 1[yn = qu]

1[xn ∈ eu].

(cid:33)

(cid:33)

(26)

Next, we factor out the indicator for equivalent point set membership, which yields a term
that sums to one, because every datum is either captured or not captured by preﬁx dp.

(cid:96)(d, x, y) =

¬ cap(xn, dp) +

cap(xn, pk | dp)

∧ 1[xn ∈ eu] 1[yn = qu]

(cid:33)

U
(cid:88)

N
(cid:88)

(cid:32)

1
N

1
N

1
N

=

=

u=1

n=1

U
(cid:88)

N
(cid:88)

u=1
U
(cid:88)

n=1
N
(cid:88)

u=1

n=1

(¬ cap(xn, dp) + cap(xn, dp)) ∧ 1[xn ∈ eu] 1[yn = qu]

1[xn ∈ eu] 1[yn = qu] =

θ(eu),

U
(cid:88)

u=1

where the ﬁnal equality applies the deﬁnition of θ(eu) in (25). Therefore, R(d, x, y) =
(cid:96)(d, x, y) + λK ≥ (cid:80)U

u=1 θ(eu) + λK.

Now, recall that to obtain our lower bound b(dp, x, y) in (5), we simply deleted the
default rule misclassiﬁcation error (cid:96)0(dp, q0, x, y) from the objective R(d, x, y). Theorem 20
obtains a tighter objective lower bound via a tighter lower bound on the default rule mis-
classiﬁcation error, 0 ≤ b0(dp, x, y) ≤ (cid:96)0(dp, q0, x, y).

K
(cid:88)

k=1
K
(cid:88)

k=1

K
(cid:88)

k=1

25

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Theorem 20 (Equivalent points bound) Let d be a rule list with preﬁx dp and lower
bound b(dp, x, y), then for any rule list d(cid:48) ∈ σ(d) whose preﬁx d(cid:48)

p starts with dp,

R(d(cid:48), x, y) ≥ b(dp, x, y) + b0(dp, x, y),

(27)

where

b0(dp, x, y) =

¬ cap(xn, dp) ∧ 1[xn ∈ eu] 1[yn = qu].

(28)

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

Proof See Appendix D for the proof of Theorem 20.

4. Incremental Computation

For every preﬁx dp evaluated during Algorithm 1’s execution, we compute the objective
lower bound b(dp, x, y) and sometimes the objective R(d, x, y) of the corresponding rule
list d. These calculations are the dominant computations with respect to execution time.
This motivates our use of a highly optimized library, designed by Yang et al. (2017), for
representing rule lists and performing operations encountered in evaluating functions of rule
lists. Furthermore, we exploit the hierarchical nature of the objective function and its lower
bound to compute these quantities incrementally throughout branch-and-bound execution.
In this section, we provide explicit expressions for the incremental computations that are
central to our approach. Later, in §5, we describe a cache data structure for supporting our
incremental framework in practice.

For completeness, before presenting our incremental expressions, let us begin by writing
down the objective lower bound and objective of the empty rule list, d = ((), (), q0, 0), the
ﬁrst rule list evaluated in Algorithm 1. Since its preﬁx contains zero rules, it has zero preﬁx
misclassiﬁcation error and also has length zero. Thus, the empty rule list’s objective lower
bound is zero, i.e., b((), x, y) = 0. Since none of the data are captured by the empty preﬁx,
the default rule corresponds to the majority class, and the objective corresponds to the
default rule misclassiﬁcation error, i.e., R(d, x, y) = (cid:96)0((), q0, x, y).

Now, we derive our incremental expressions for the objective function and its lower
p, q(cid:48)
bound. Let d = (dp, δp, q0, K) and d(cid:48) = (d(cid:48)
0, K + 1) be rule lists such that preﬁx dp =
(p1, . . . , pK) is the parent of d(cid:48)
p = (q1, . . . ,
qK, qK+1) be the corresponding labels. The hierarchical structure of Algorithm 1 enforces
that if we ever evaluate d(cid:48), then we will have already evaluated both the objective and ob-
jective lower bound of its parent, d. We would like to reuse as much of these computations as
possible in our evaluation of d(cid:48). We can write the objective lower bound of d(cid:48) incrementally,

p = (p1, . . . , pK, pK+1). Let δp = (q1, . . . , qK) and δ(cid:48)

p, δ(cid:48)

26

Learning Certifiably Optimal Rule Lists

with respect to the objective lower bound of d:

b(d(cid:48)

p, δ(cid:48)
p, x, y) = (cid:96)p(d(cid:48)
N
(cid:88)

p, x, y) + λ(K + 1)
K+1
(cid:88)

cap(xn, pk | d(cid:48)

=

1
N

n=1

k=1

p) ∧ 1[qk (cid:54)= yn] + λ(K + 1)

(29)

= (cid:96)p(dp, δp, x, y) + λK + λ +

cap(xn, pK+1 | d(cid:48)

p) ∧ 1[qK+1 (cid:54)= yn]

1
N

N
(cid:88)

n=1

= b(dp, x, y) + λ +

cap(xn, pK+1 | d(cid:48)

p) ∧ 1[qK+1 (cid:54)= yn]

= b(dp, x, y) + λ +

¬ cap(xn, dp) ∧ cap(xn, pK+1) ∧ 1[qK+1 (cid:54)= yn].

(30)

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

Thus, if we store b(dp, x, y), then we can reuse this quantity when computing b(d(cid:48)
p, x, y).
Transforming (29) into (30) yields a signiﬁcantly simpler expression that is a function of
the stored quantity b(dp, x, y). For the objective of d(cid:48), ﬁrst let us write a na¨ıve expression:

R(d(cid:48), x, y) = (cid:96)(d(cid:48), x, y) + λ(K + 1) = (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + (cid:96)0(d(cid:48)

p, q(cid:48)

0, x, y) + λ(K + 1)

=

1
N

N
(cid:88)

K+1
(cid:88)

n=1

k=1

1
N

N
(cid:88)

n=1

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] +

¬ cap(xn, d(cid:48)

p) ∧ 1[q(cid:48)

0 (cid:54)= yn] + λ(K + 1).

(31)

Instead, we can compute the objective of d(cid:48) incrementally with respect to its objective lower
bound:

R(d(cid:48), x, y) = (cid:96)p(d(cid:48)
= b(d(cid:48)

p, x, y) + (cid:96)0(d(cid:48)
p, δ(cid:48)
p, q(cid:48)
p, x, y) + (cid:96)0(d(cid:48)
N
(cid:88)

p, q(cid:48)
0, x, y)

0, x, y) + λ(K + 1)

= b(d(cid:48)

p, x, y) +

¬ cap(xn, d(cid:48)

p) ∧ 1[q(cid:48)

0 (cid:54)= yn]

1
N

1
N

n=1
N
(cid:88)

n=1

= b(d(cid:48)

p, x, y) +

¬ cap(xn, dp) ∧ (¬ cap(xn, pK+1)) ∧ 1[q(cid:48)

0 (cid:54)= yn].

(32)

The expression in (32) is simpler to compute than that in (31), because the former reuses
p, x, y), which we already computed in (30). Note that instead of computing R(d(cid:48), x, y)
b(d(cid:48)
incrementally from b(d(cid:48)
p, x, y) as in (32), we could have computed it incrementally from
R(d, x, y). However, doing so would in practice require that we store R(d, x, y) in addition
to b(dp, x, y), which we already must store to support (30). We prefer the incremental
approach suggested by (32) since it avoids this additional storage overhead.

We present an incremental branch-and-bound procedure in Algorithm 2, and show the
incremental computations of the objective lower bound (30) and objective (32) as two
separate functions in Algorithms 3 and 4, respectively. In Algorithm 2, we use a cache to

27

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Algorithm 2 Incremental branch-and-bound for learning rule lists, for simplicity, from a
cold start. We explicitly show the incremental objective lower bound and objective functions
in Algorithms 3 and 4, respectively.

Input: Objective function R(d, x, y), objective lower bound b(dp, x, y), set of antecedents
S = {sm}M
Output: Provably optimal rule list d∗ with minimum objective R∗

m=1, training data (x, y) = {(xn, yn)}N

n=1, regularization parameter λ

dc ← ((), (), q0, 0)
(cid:46) Initialize current best rule list with empty rule list
Rc ← R(dc, x, y)
(cid:46) Initialize current best objective
(cid:46) Initialize queue with empty preﬁx
Q ← queue( [ ( ) ] )
C ← cache( [ ( ( ) , 0 ) ] ) (cid:46) Initialize cache with empty preﬁx and its objective lower bound
(cid:46) Optimization complete when the queue is empty
while Q not empty do
(cid:46) Remove a length-K preﬁx dp from the queue
dp ← Q.pop( )
(cid:46) Look up dp’s lower bound in the cache
b(dp, x, y) ← C.ﬁnd(dp)
(cid:46) Bit vector indicating data not captured by dp
u ← ¬ cap(x, dp)
(cid:46) Evaluate all of dp’s children
for s in S do

if s not in dp then
Dp ← (dp, s)
v ← u ∧ cap(x, s)
b(Dp, x, y) ← b(dp, x, y) + λ + IncrementalLowerBound(v, y, N )
if b(Dp, x, y) < Rc then

(cid:46) Branch: Generate child Dp
(cid:46) Bit vector indicating data captured by s in Dp

(cid:46) Bound: Apply bound from Theorem 1

R(D, x, y) ← b(Dp, x, y) + IncrementalObjective(u, v, y, N )
D ← (Dp, ∆p, Q0, K + 1)
if R(D, x, y) < Rc then

(cid:46) ∆p, Q0 are set in the incremental functions

(dc, Rc) ← (D, R(D, x, y)) (cid:46) Update current best rule list and objective

end if
Q.push(Dp)
C.insert(Dp, b(Dp, x, y))

(cid:46) Add Dp to the queue
(cid:46) Add Dp and its lower bound to the cache

end if

end if

end for
end while
(d∗, R∗) ← (dc, Rc)

(cid:46) Identify provably optimal rule list and objective

store preﬁxes and their objective lower bounds. Algorithm 2 additionally reorganizes the
structure of Algorithm 1 to group together the computations associated with all children
of a particular preﬁx. This has two advantages. The ﬁrst is to consolidate cache queries: all
children of the same parent preﬁx compute their objective lower bounds with respect to the
parent’s stored value, and we only require one cache ‘ﬁnd’ operation for the entire group
of children, instead of a separate query for each child. The second is to shrink the queue’s
size: instead of adding all of a preﬁx’s children as separate queue elements, we represent
the entire group of children in the queue by a single element. Since the number of children
associated with each preﬁx is close to the total number of possible antecedents, both of these
eﬀects can yield signiﬁcant savings. For example, if we are trying to optimize over rule lists

28

Learning Certifiably Optimal Rule Lists

Algorithm 3 Incremental objective lower bound (30) used in Algorithm 2.

Input: Bit vector v ∈ {0, 1}N indicating data captured by s, the last antecedent in Dp,
bit vector of class labels y ∈ {0, 1}N , number of observations N
Output: Component of D’s misclassiﬁcation error due to data captured by s

function IncrementalLowerBound(v, y, N )

nv = sum(v)
w ← v ∧ y
nw = sum(w)
if nw/nv > 0.5 then

return (nv − nw)/N

else

return nw/N

end if
end function

(cid:46) Number of data captured by s, the last antecedent in Dp
(cid:46) Bit vector indicating data captured by s with label 1
(cid:46) Number of data captured by s with label 1

(cid:46) Misclassiﬁcation error of the rule s → 1

(cid:46) Misclassiﬁcation error of the rule s → 0

Algorithm 4 Incremental objective function (32) used in Algorithm 2.

Input: Bit vector u ∈ {0, 1}N indicating data not captured by Dp’s parent preﬁx, bit
vector v ∈ {0, 1}N indicating data not captured by s, the last antecedent in Dp, bit
vector of class labels y ∈ {0, 1}N , number of observations N
Output: Component of D’s misclassiﬁcation error due to its default rule

function IncrementalObjective(u, v, y, N )

f ← u ∧ ¬ v
nf = sum(f )
g ← f ∧ y
ng = sum(g)
if ng/nf > 0.5 then

(cid:46) Bit vector indicating data not captured by Dp
(cid:46) Number of data not captured by Dp
(cid:46) Bit vector indicating data not captured by Dp with label 1
(cid:46) Number of data not captued by Dp with label 1

return (nf − ng)/N

(cid:46) Misclassiﬁcation error of the default label prediction 1

return ng/N

(cid:46) Misclassiﬁcation error of the default label prediction 0

else

end if
end function

formed from a set of 1000 antecedents, then the maximum queue size in Algorithm 2 will
be smaller than that in Algorithm 1 by a factor of nearly 1000.

29

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

5. Implementation

We implement our algorithm using a collection of optimized data structures that we describe
in this section. First, we explain how we use a preﬁx tree (§5.1) to support the incremental
computations that we motivated in §4. Second, we describe several queue designs that
implement diﬀerent search policies (§5.2). Third, we introduce a symmetry-aware map (§5.3)
to support symmetry-aware pruning (Corollary 16, §3.11). Next, we summarize how these
data structures interact throughout our model of incremental execution (§5.4). In particular,
Algorithms 5 and 6 illustrate many of the computational details from CORELS’ inner
loop, highlighting each of the bounds from §3 that we use to prune the search space. We
additionally describe how we garbage collect our data structures (§5.5). Finally, we explore
how our queue can be used to support custom scheduling policies designed to improve
performance (§5.6).

5.1 Preﬁx Tree

Our incremental computations (§4) require a cache to keep track of preﬁxes that we have
already evaluated and that are also still under consideration by the algorithm. We implement
this cache as a preﬁx tree, a data structure also known as a trie, which allows us to eﬃciently
represent structure shared between related preﬁxes. Each node in the preﬁx tree encodes
an individual rule rk = pk → qk. Each path starting from the root represents a preﬁx, such
that the ﬁnal node in the path also contains metadata associated with that preﬁx. For a
preﬁx dp = (p1, . . . , pK), let ϕ(dp) denote the corresponding node in the trie. The metadata
at node ϕ(dp) supports the incremental computation and includes:

• An index encoding pK, the last antecedent.

• The objective lower bound b(dp, x, y), deﬁned in (5), the central bound in our frame-

work (Theorem 1).

• The lower bound on the default rule misclassiﬁcation error b0(dp, x, y), deﬁned in (28),

to support our equivalent points bound (Theorem 20).

• An indicator denoting whether this node should be deleted (see §5.5).

• A representation of viable extensions of dp, i.e., length K + 1 preﬁx that start with dp

and have not been pruned.

For evaluation purposes and convenience, we store additional information in the preﬁx tree;
for a preﬁx dp with corresponding rule list d = (dp, δp, q0, K), the node ϕ(dp) also stores:

• The length K; equivalently, node ϕ(dp)’s depth in the trie.

• The label prediction qK corresponding to antecedent pK.

• The default rule label prediction q0.

• Ncap, the number of samples captured by preﬁx dp, as in (34).

• The objective value R(d, x, y), deﬁned in (4).

Finally, we note that we implement the preﬁx tree as a custom C++ class.

30

Learning Certifiably Optimal Rule Lists

5.2 Queue

The queue is a worklist that orders exploration over the search space of possible rule lists;
every queue element corresponds to a leaf in the preﬁx tree, and vice versa. In our imple-
mentation, each queue element points to a leaf; when we pop an element oﬀ the queue, we
use the leaf’s metadata to incrementally evaluate the corresponding preﬁx’s children.

We order entries in the queue to implement several diﬀerent search policies. For exam-
ple, a ﬁrst-in-ﬁrst-out (FIFO) queue implements breadth-ﬁrst search (BFS), and a priority
queue implements best-ﬁrst search. In our experiments (§6), we use the C++ Standard
Template Library (STL) queue and priority queue to implement BFS and best-ﬁrst search,
respectively. For CORELS, priority queue policies of interest include ordering by the lower
bound, the objective, or more generally, any function that maps preﬁxes to real values;
stably ordering by preﬁx length and inverse preﬁx length implement BFS and depth-ﬁrst
search (DFS), respectively. In our released code, we present a uniﬁed implementation, where
we use the STL priority queue to support BFS, DFS, and several best-ﬁrst search policies.
As we demonstrate in our experiments (§6.8), we ﬁnd that using a custom search strategy,
such as ordering by the lower bound, usually leads to a faster runtime than BFS.

We motivate the design of additional custom search strategies in §5.6. In preliminary
work (not shown), we also experimented with stochastic exploration processes that bypass
the need for a queue by instead following random paths from the root to leaves; developing
such methods could be an interesting direction for future work. We note that these search
policies are referred to as node selection strategies in the MIP literature. Strategies such as
best-ﬁrst (best-bound) search and DFS are known as static methods, and the framework
we present in §5.6 has the spirit of estimate-based methods (Linderoth and Savelsbergh,
1999).

5.3 Symmetry-aware Map

The symmetry-aware map supports the symmetry-aware pruning justiﬁed in §3.10. In our
implementation, we speciﬁcally leverage our permutation bound (Corollary 16), though it is
also possible to directly exploit the more general equivalent support bound (Theorem 15).
We use the C++ STL unordered map to keep track of the best known ordering of each
evaluated set of antecedents. The keys of our symmetry-aware map encode antecedents in
canonical order, i.e., antecedent indices in numerically sorted order, and we associate all
permutations of a set of antecedents with a single key. Each key maps to a value that
encodes the best known preﬁx in the permutation group of the key’s antecedents, as well
as the objective lower bound of that preﬁx.

Before we consider adding a preﬁx dp to the trie and queue, we check whether the map
already contains a permutation π(dp) of that preﬁx. If no such permutation exists, then
we insert dp into the map, trie, and queue. Otherwise, if a permutation π(dp) exists and
the lower bound of dp is better than that of π(dp), i.e., b(dp, x, y) < b(π(dp), x, y), then we
update the map and remove π(dp) and its entire subtree from the trie; we also insert dp into
the trie and queue. Otherwise, if there exists a permutation π(dp) such that b(π(dp), x, y) ≤
b(dp, x, y), then we do nothing, i.e., we do not insert dp into any data structures.

31

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

5.4 Incremental Execution

Mapping our algorithm to our data structures produces the following execution strategy,
which we also illustrate in Algorithms 5 and 6. We initialize the current best objective Rc
and rule list dc. While the trie contains unexplored leaves, a scheduling policy selects the
next preﬁx dp to extend; in our implementation, we pop elements from a (priority) queue,
until the queue is empty. Then, for every antecedent s that is not in dp, we construct a new
preﬁx Dp by appending s to dp; we incrementally calculate the lower bound b(Dp, x, y),
the objective R(D, x, y), of the associated rule list D, and other quantities used by our
algorithm, summarized by the metadata ﬁelds of the (potential) preﬁx tree node ϕ(Dp).

If the objective R(D, x, y) is less than the current best objective Rc, then we update Rc
and dc. If the lower bound of the new preﬁx Dp is less than the current best objective,
then as described in §5.3, we query the symmetry-aware map for Dp; if we insert d(cid:48)
p into
the symmetry-aware map, then we also insert it into the trie and queue. Otherwise, then
by our hierarchical lower bound (Theorem 1), no extension of Dp could possibly lead to a
rule list with objective better than Rc, thus we do not insert Dp into the tree or queue. We
also leverage our other bounds from §3 to aggressively prune the search space; we highlight
each of these bounds in Algorithms 5 and 6, which summarize the computations and data
structure operations performed in CORELS’ inner loop. When there are no more leaves
to explore, i.e., the queue is empty, we output the optimal rule list. We can optionally
terminate early according to some alternate condition, e.g., when the size of the preﬁx tree
exceeds some threshold.

5.5 Garbage Collection

During execution, we garbage collect the trie. Each time we update the minimum objective,
we traverse the trie in a depth-ﬁrst manner, deleting all subtrees of any node with lower
bound larger than the current minimum objective. At other times, when we encounter a
node with no children, we prune upwards, deleting that node and recursively traversing
the tree towards the root, deleting any childless nodes. This garbage collection allows us
to constrain the trie’s memory consumption, though in our experiments we observe the
minimum objective to decrease only a small number of times.

In our implementation, we cannot immediately delete preﬁx tree leaves because each
corresponds to a queue element that points to it. The C++ STL priority queue is a wrapper
container that prevents access to the underlying data structure, and thus we cannot access
elements in the middle of the queue, even if we know the relevant identifying information. We
therefore have no way to update the queue without iterating over every element. We address
this by marking preﬁx tree leaves that we wish to delete (see §5.1), deleting the physical
nodes lazily, after they are popped from the queue. Later, in our section on experiments (§6),
we refer to two diﬀerent queues that we deﬁne here: the physical queue corresponds to the
C++ queue, and thus all preﬁx tree leaves, and the logical queue corresponds only to those
preﬁx tree leaves that have not been marked for deletion.

32

Learning Certifiably Optimal Rule Lists

Algorithm 5 The inner loop of CORELS, which evaluates all children of a preﬁx dp.
1[xn ∈ eu][yn = qu]

Deﬁne z ∈ {0, 1}N , s.t. zn = (cid:80)U

u=1

Deﬁne b(dp, x, y) and u = ¬ cap(x, dp)
for s in S if s not in dp then do

(cid:46) eu is the equivalent points set containing xn and qu is the minority class label of eu (§3.14)
(cid:46) u is a bit vector indicating data not captured by dp
(cid:46) Evaluate all of dp’s children
(cid:46) Branch: Generate child Dp
(cid:46) Bit vector indicating data captured by s in Dp
(cid:46) Number of data captured by s, the last antecedent in Dp

(cid:46) Lower bound on antecedent support (Theorem10)

(cid:46) Bit vector indicating data captured by s with label 1
(cid:46) Number of data captured by s with label 1

(cid:46) Number of correct predictions by the new rule s → 1

(cid:46) Number of correct predictions by the new rule s → 0

Dp ← (dp, s)
v ← u ∧ cap(x, s)
nv = sum(v)
if nv/N < λ then
continue

end if
w ← v ∧ y
nw = sum(w)
if nw/nv ≥ 0.5 then

nc ← nw

else

nc ← nv − nw

end if
if nc/N < λ then
continue

(cid:46) Lower bound on accurate antecedent support (Theorem 11)

end if
δb ← (nv − nc)/N
b(Dp, x, y) ← b(dp, x, y) + λ + δb
if b(Dp, x, y) ≥ Rc then

(cid:46) Misclassiﬁcation error of the new rule
(cid:46) Incremental lower bound (30)
(cid:46) Hierarchical objective lower bound (Theorem 1)

continue

end if
f ← u ∧ ¬ v
nf = sum(f )
g ← f ∧ y
ng = sum(g)
if ng/nf ≥ 0.5 then

δR ← (nf − ng)/N

else

δR ← ng/N

end if
R(D, x, y) ← b(Dp, x, y) + δR
D ← (Dp, ∆p, Q0, K + 1)
if R(D, x, y) < Rc then

(cid:46) Bit vector indicating data not captured by Dp
(cid:46) Number of data not captured by Dp
(cid:46) Bit vector indicating data not captured by Dp with label 1
(cid:46) Number of data not captued by Dp with label 1

(cid:46) Misclassiﬁcation error of the default label prediction 1

(cid:46) Misclassiﬁcation error of the default label prediction 0

(cid:46) Incremental objective (32)
(cid:46) ∆p, Q0 are set in the incremental functions

(dc, Rc) ← (D, R(D, x, y))
(cid:46) Update current best rule list and objective
GarbageCollectPrefixTree(Rc) (cid:46) Delete nodes with lower bound ≥ Rc − λ (§5.5),
using the Lookahead bound (Lemma 2)
(cid:46) Lower bound on the default rule misclassiﬁcation
error deﬁned in (28)
(cid:46) Equivalent points bound (Theorem 20)
combined with the Lookahead bound (Lemma 2)

end if
b0(Dp, x, y) ← sum(f ∧ z)/N
b ← b(Dp, x, y) + b0(Dp, x, y)
if b + λ ≥ Rc then

continue

end if
CheckMapAndInsert(Dp, b)

end for

(cid:46) Check the Permutation bound (Corollary 16) and
possibly insert Dp into data structures (Algorithm 6)

33

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Algorithm 6 Possibly insert a preﬁx into CORELS’ data structures, after ﬁrst checking the
symmetry-aware map, which supports search space pruning triggered by the permutation
bound (Corollary 16). For further context, see Algorithm 5.

T is the preﬁx tree (§5.1)
Q is the queue, for concreteness, a priority queue ordered by the lower bound (§5.2)
P is the symmetry-aware map (§5.3)

function CheckMapAndInsert(Dp, b)

π0 ← sort(Dp)
(Dπ, bπ) ← P .ﬁnd(π0)
if Dπ exists then
if b < bπ then

P .update(π0, (Dp, b))
T .delete subtree(Dπ)
T .insert(ϕ(Dp))
Q.push(Dp, b)

else

end if

else

P .insert(π0, (Dp, b))
T .insert(ϕ(Dp))
Q.push(Dp, b)

end if
end function

(cid:46) Dp’s antecedents in canonical order
(cid:46) Look for a permutation of Dp

(cid:46) Dp is better than Dπ
(cid:46) Replace Dπ with Dp in the map
(cid:46) Delete Dπ and its subtree from the preﬁx tree
(cid:46) Add node for Dp to the preﬁx tree
(cid:46) Add Dp to the queue

(cid:46) Add Dp to the map
(cid:46) Add node for Dp to the preﬁx tree
(cid:46) Add Dp to the queue

pass

(cid:46) Dp is inferior to Dπ, thus do not insert it into any data structures

5.6 Custom Scheduling Policies

In our setting, an ideal scheduling policy would immediately identify an optimal rule list,
and then certify its optimality by systematically eliminating the remaining search space.
This motivates trying to design scheduling policies that tend to quickly ﬁnd optimal rule
lists. When we use a priority queue to order the set of preﬁxes to evaluate next, we are free
to implement diﬀerent scheduling policies via the ordering of elements in the queue. This
motivates designing functions that assign higher priorities to ‘better’ preﬁxes that we believe
are more likely to lead to optimal rule lists. We follow the convention that priority queue
elements are ordered by keys, such that keys with smaller values have higher priorities.

We introduce a custom class of functions that we call curiosity functions. Broadly, we
think of the curiosity of a rule list d as the expected objective value of another rule list d(cid:48)
that is related to d; diﬀerent models of the relationship between d and d(cid:48) lead to diﬀerent
curiosity functions. In general, the curiosity of d is, by deﬁnition, equal to the sum of the
expected misclassiﬁcation error and the expected regularization penalty of d(cid:48):

C(dp, x, y) ≡ E[R(d(cid:48), x, y)] = E[(cid:96)(d(cid:48)

p, δ(cid:48)

p, x, y)] + λE[K(cid:48)].

(33)

34

Learning Certifiably Optimal Rule Lists

Next, we describe a simple curiosity function for a rule list d with preﬁx dp. First,

let Ncap denote the number of observations captured by dp, i.e.,

Ncap ≡

cap(xn, dp).

(34)

N
(cid:88)

n=1

We now describe a model that generates another rule list d(cid:48) = (d(cid:48)
sume that preﬁx d(cid:48)
antecedent in d(cid:48)
then, the expected length of d(cid:48)

0, K(cid:48)) from dp. As-
p starts with dp and captures all the data, such that each additional
p captures as many ‘new’ observations as each antecedent in dp, on average;

p, δ(cid:48)

p, q(cid:48)

p is

E[K(cid:48)] =

N
Ncap/K

.

Furthermore, assume that each additional antecedent in d(cid:48)
antecedent in dp, on average, thus the expected misclassiﬁcation error of d(cid:48)

p makes as many mistakes as each

p is

E[(cid:96)(d(cid:48)

p, δ(cid:48)

p, x, y)] = E[(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y)] + E[(cid:96)0(d(cid:48)

= E[(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y)] = E[K(cid:48)]

p, q(cid:48)
0, x, y)]
(cid:18) (cid:96)p(dp, δp, x, y)
K

(cid:19)

.

0, x, y) is zero because we assume
p captures all the data. Inserting (35) and (36) into (33) thus gives curiosity for this

Note that the default rule misclassiﬁcation error (cid:96)0(d(cid:48)
that d(cid:48)
model:

p, q(cid:48)

(35)

(36)

C(dp, x, y) =

(cid:96)p(dp, δp, x, y) + λK

(cid:19)

(cid:19) (cid:18)

(cid:18) N
Ncap

(cid:32)

=

1
N

N
(cid:88)

n=1

(cid:33)−1

cap(xn, dp)

b(dp, x, y) =

b(dp, x, y)
supp(dp, x)

,

where for the second equality, we used the deﬁnitions in (34) of Ncap and in (5) of dp’s lower
bound, and for the last equality, we used the deﬁnition in (2) of dp’s normalized support.

The curiosity for a preﬁx dp is thus also equal to its objective lower bound, scaled by
the inverse of its normalized support. For two preﬁxes with the same lower bound, curiosity
gives higher priority to the one that captures more data. This is a well-motivated scheduling
strategy if we model preﬁxes that extend the preﬁx with smaller support as having more
‘potential’ to make mistakes. We note that using curiosity in practice does not introduce
new bit vector or other expensive computations; during execution, we can calculate curiosity
as a simple function of already derived quantities.

In preliminary experiments, we observe that using a priority queue ordered by curiosity
sometimes yields a dramatic reduction in execution time, compared to using a priority queue
ordered by the objective lower bound. Thus far, we have observed signiﬁcant beneﬁts on
speciﬁc small problems, where the structure of the solutions happen to render curiosity
particularly eﬀective (not shown). Designing and studying other ‘curious’ functions, that
are eﬀective in more general settings, is an exciting direction for future work.

35

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Data set

Prediction problem

N

ProPublica Two-year recidivism
NYPD
Weapon possession
Weapon possession
NYCLU

6,907
325,800
29,595

Positive
fraction
0.46
0.03
0.05

Resample
training set
No
Yes
Yes

Training
set size
6,217
566,839
50,743

Test
set size
692
32,580
2,959

Table 1: Summary of data sets and prediction problems. The last ﬁve columns report the
total number of observations, the fraction of observations with the positive class
label, whether we resampled the training set due to class imbalance, and the sizes
of each training and test set in our 10-fold cross-validation studies.

6. Experiments

Our experimental analysis addresses ﬁve questions: How does CORELS’ predictive perfor-
mance compare to that of COMPAS scores and other algorithms? (§6.4, §6.5, and §6.6)
How does CORELS’ model size compare to that of other algorithms? (§6.6) How rapidly do
the objective value and its lower bound converge, for diﬀerent values of the regularization
parameter λ? (§6.7) How much does each of the implementation optimizations contribute
to CORELS’ performance? (§6.8) How rapidly does CORELS prune the search space? (§6.7
and §6.8) Before proceeding, we ﬁrst describe our computational environment (§6.1), as
well as the data sets and prediction problems we use (§6.2), and then in Section 6.3 show
example optimal rule lists found by CORELS.

6.1 Computational Environment

All timed results ran on a server with an Intel Xeon E5-2699 v4 (55 MB cache, 2.20 GHz)
processor and 264 GB RAM, and we ran each timing measurement separately, on a single
hardware thread, with nothing else running on the server. Except where we mention a
memory constraint, all experiments can run comfortably on smaller machines, e.g., a laptop
with 16 GB RAM.

6.2 Data Sets and Prediction Problems

Our evaluation focuses on two socially-important prediction problems associated with re-
cent, publicly-available data sets. Table 1 summarizes the data sets and prediction problems,
and Table 2 summarizes feature sets extracted from each data set, as well as antecedent sets
we mine from these feature sets. We provide some details next. For further details about
data sets, preprocessing steps, and antecedent mining, see Appendix E.

6.2.1 Recidivism Prediction

For our ﬁrst problem, we predict which individuals in the ProPublica COMPAS data
set (Larson et al., 2016) recidivate within two years. This data set contains records for
all oﬀenders in Broward County, Florida in 2013 and 2014 who were given a COMPAS
score pre-trial. Recidivism is deﬁned as being charged with a new crime within two years

36

Learning Certifiably Optimal Rule Lists

Data set

ProPublica
ProPublica
NYPD
NYPD
NYCLU

Feature Categorical Binary
features
attributes
13
6
17
7
28
5
20
3
28
5

set
A
B
C
D
E

Mined
antecedents
122
189
28
20
46

of clauses
2
2
1
1
1

No
No
No
No
Yes

Max number Negations

Table 2: Summary of feature sets and mined antecedents. The last ﬁve columns report
the number of categorical attributes, the number of binary features, the average
number of mined antecedents, the maximum number of clauses in each antecedent,
and whether antecedents include negated clauses.

after receiving a COMPAS assessment; the article by Larson et al. (2016), and their code,3
provide more details about this deﬁnition. From the original data set of records for 7,214
individuals, we identify a subset of 6,907 records without missing data. For the majority of
our analysis, we extract a set of 13 binary features (Feature Set A), which our antecedent
mining framework combines into M = 122 antecedents, on average (folds ranged from con-
taining 121 to 123 antecedents). We also consider a second, similar antecedent set in §6.3,
derived from a superset of Feature Set A that includes 4 additional binary features (Feature
Set B).

6.2.2 Weapon Prediction

For our second problem, we use New York City stop-and-frisk data to predict whether a
weapon will be found on a stopped individual who is frisked or searched. For experiments
in Sections 6.3 and 6.5 and Appendix G, we compile data from a database maintained by
the New York Police Department (NYPD) (New York Police Department, 2016), from years
2008-2012, following Goel et al. (2016). Starting from 2,941,390 records, each describing an
incident involving a stopped person, we ﬁrst extract 376,488 records where the suspected
crime was criminal possession of a weapon (CPW).4 From these, we next identify a subset
of 325,800 records for which the individual was frisked and/or searched; of these, criminal
possession of a weapon was identiﬁed in only 10,885 instances (about 3.3%). Resampling
due to class imbalance, for 10-fold cross-validation, yields training sets that each contain
566,839 datapoints. (We form corresponding test sets without resampling.) From a set of
5 categorical features, we form a set of 28 single-clause antecedents corresponding to 28
binary features (Feature Set C). We also consider another, similar antecedent set, derived
from a subset of Feature Set C that excludes 8 location-speciﬁc binary features (Feature
Set D).

In Sections 6.3, 6.6, 6.7, and 6.8, we also use a smaller stop-and-frisk data set, derived by
the NYCLU from the NYPD’s 2014 data (New York Civil Liberties Union, 2014). From the

3. Data and code used in the analysis by Larson et al. (2016) can be found at https://github.com/

propublica/compas-analysis.

4. We ﬁlter for records that explicitly match the string ‘CPW’; we note that additional records, after

converting to lowercase, contain strings such as ‘cpw’ or ‘c.p.w.’

37

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

Figure 3: Example optimal rule lists that predict two-year recidivism for the ProPublica
data set (Feature Set B, M = 189), found by CORELS (λ = 0.005), across 10
cross-validation folds. While some input antecedents contain features for race, no
optimal rule list includes such an antecedent. Every optimal rule list is the same
or similar to one of these examples, with preﬁxes containing the same rules, up
to a permutation, and same default rule.

original data set of 45,787 records, each describing an incident involving a stopped person,
we identify a subset of 29,595 records for which the individual was frisked and/or searched.
Of these, criminal possession of a weapon was identiﬁed in about 5% of instances. As with
the larger NYPD data set, we resample the data to form training sets (but not to form
test sets). From the same set of 5 categorical features as in Feature Set C, we form a set of
M = 46 single-clause antecedents, including negations (Feature Set E).

6.3 Example Optimal Rule Lists

To motivate Feature Set A, described in Appendix E, which we used in most of our analysis
of the ProPublica data set, we ﬁrst consider Feature Set B, a larger superset of features.

Figure 3 shows optimal rule lists learned by CORELS, using Feature Set B, which
additionally includes race categories from the ProPublica data set (African American, Cau-
casian, Hispanic, Other5). For Feature Set B, our antecedent mining procedure generated an
average of 189 antecedents, across folds. None of the optimal rule lists contain antecedents
that directly depend on race; this motivated our choice to exclude race, by using Feature
Set A, in our subsequent analysis. For both feature sets, we replaced the original ProPublica
age categories (<25, 25-45, >45) with a set that is more ﬁne-grained for younger individuals
(18-20, 21-22, 23-25, 26-45, >45). Figure 4 shows example optimal rule lists that CORELS
learns for the ProPublica data set (Feature Set A, λ = 0.005), using 10-fold cross validation.
Figures 5 and 6 show example optimal rule lists that CORELS learns for the NYCLU
(λ = 0.01) and NYPD data sets. Figure 6 shows optimal rule lists that CORELS learns for
the larger NYPD data set.

While our goal is to provide illustrative examples, and not to provide a detailed analysis
nor to advocate for the use of these speciﬁc models, we note that these rule lists are short and
easy to understand. For the examples and regularization parameter choices in this section,

5. We grouped the original Native American (<0.003), Asian (<0.005), and Other (<0.06) categories.

38

Learning Certifiably Optimal Rule Lists

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

Figure 4: Example optimal rule lists that predict two-year recidivism for the ProPublica
data set (Feature Set A, M = 122), found by CORELS (λ = 0.005), across 10
cross-validation folds. Feature Set A is a subset of Feature Set B (Figure 3) that
excludes race features. Optimal rule lists found using the two feature sets are
very similar. The upper and lower rule lists are representative of 7 and 3 folds,
respectively. Each of the remaining 8 solutions is the same or similar to one of
these, with preﬁxes containing the same rules, up to a permutation, and the same
default rule. See Figure 20 in Appendix F for a complete listing.

if (location = transit authority) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = suspicious object) then predict yes
else predict no

Figure 5: An example rule list that predicts whether a weapon will be found on a stopped in-
dividual who is frisked or searched, for the NYCLU stop-and-frisk data set. Across
10 cross-validation folds, the other optimal rule lists found by CORELS (λ = 0.01)
contain the same or equivalent rules, up to a permutation. See also Figure 23 in
Appendix F.

the optimal rule lists are relatively robust across cross-validation folds: the rules are nearly
the same, up to permutations of the preﬁx rules. For smaller values of the regularization
parameter, we observe less robustness, as rule lists are allowed to grow in length. For the
sets of optimal rule lists represented in Figures 3, 4, and 5, each set could be equivalently
expressed as a DNF rule; e.g., this is easy to see when the preﬁx rules all predict the
positive class label and the default rule predicts the negative class label. Our objective is
not designed to enforce any of these properties, though some may be seen as desirable.

As we demonstrate in §6.6, optimal rule lists learned by CORELS achieve accuracies
that are competitive with a suite of other models, including black box COMPAS scores. See
Appendix F for additional listings of optimal rule lists found by CORELS, for each of our
prediction problems, across cross-validation folds, for diﬀerent regularization parameters λ.

39

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Weapon prediction (λ = 0.01, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else predict no

Weapon prediction (λ = 0.01, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (inside or outside = outside) then predict no
else predict yes

Weapon prediction (λ = 0.005, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

Weapon prediction (λ = 0.005, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

Figure 6: Example optimal rule lists for the NYPD stop-and-frisk data set, found by
CORELS. Feature Set C contains attributes for ‘location’ and ‘city’, while Fea-
ture Set D does not. For each choice of regularization parameter and feature set,
the rule lists learned by CORELS, across all 10 cross-validation folds, contain
the same or equivalent rules, up to a permutation, with the exception of a single
fold (Feature Set C, λ = 0.005). For a complete listing, see Figures 21 and 22 in
Appendix F.

6.4 Comparison of CORELS to the Black Box COMPAS Algorithm

The accuracies of rule lists learned by CORELS are competitive with scores generated by
the black box COMPAS algorithm at predicting two-year recidivism for the ProPublica
data set (Figure 9). Across 10 cross-validation folds, optimal rule lists learned by CORELS
(Figure 4, λ = 0.005) have a mean test accuracy of 0.665, with standard deviation 0.018. The
COMPAS algorithm outputs scores between 1 and 10, representing low (1-4), medium (5-7),
and high (8-10) risk for recidivism. As in the analysis by Larson et al. (2016), we interpret a
medium or high score as a positive prediction for two-year recidivism, and a low score as a
negative prediction. Across the 10 test sets, the COMPAS algorithm scores obtain a mean
accuracy of 0.660, with standard deviation 0.019.

Figure 7 shows that CORELS and COMPAS perform similarly across both black and
white individuals. Both algorithms have much higher true positive rates (TPR’s) and false
positive rates (FPR’s) for blacks than whites (left), and higher true negative rates (TNR’s)

40

Learning Certifiably Optimal Rule Lists

Figure 7: Comparison of TPR and FPR (left), as well as TNR and FNR (right), for diﬀerent
races in the ProPublica data set, for CORELS and COMPAS, across 10 cross-
validation folds.

and false negative rates (FNR’s) for whites than blacks (right). The fact that COMPAS has
higher FPR’s for blacks and higher FNR’s for whites was a central observation motivating
ProPublica’s claim that COMPAS is racially biased (Larson et al., 2016). The fact that
CORELS’ models are so simple, with almost the same results as COMPAS, and contain
only counts of past crimes, age, and gender, indicates possible explanations for the uneven
predictions of both COMPAS and CORELS among blacks and whites. In particular, blacks
evaluated within Broward County tend to be younger and have longer criminal histories
within the data set, (on average, 4.4 crimes for blacks versus 2.6 crimes for whites) leading
to higher FPR’s for blacks and higher FNR’s for whites. This aspect of the data could help
to explain why ProPublica concluded that COMPAS was racially biased.

Similar observations have been reported for other datasets, namely that complex ma-
chine learning models do not have an advantage over simpler transparent models (Tollenaar
and van der Heijden, 2013; Bushway, 2013; Zeng et al., 2017). There are many deﬁnitions of
fairness, and it is not clear whether CORELS’ models are fair either, but it is much easier to
debate about the fairness of a model when it is transparent. Additional fairness constraints
or transparency constraints can be placed on CORELS’ models if desired, though one would
need to edit our bounds (§3) and implementation (§5) to impose more constraints.

Regardless of whether COMPAS is racially biased (which our analysis does not indicate is
necessarily true as long as criminal history and age are allowed to be considered as features),
COMPAS may have many other fairness defects that might be considered serious. Many of
COMPAS’s survey questions are direct inquiries about socioeconomic status. For instance,
a sample COMPAS survey6 asks: “Is it easy to get drugs in your neighborhood?,” “How
often do you have barely enough money to get by?,” “Do you frequently get jobs that don’t
pay more than minimum wage?,” “How often have you moved in the last 12 months?”
COMPAS’s survey questions also ask about events that were not caused by the person who

6. A sample COMPAS survey contributed by Julia Angwin, ProPublica, can be found at https://www.

documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html.

41

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

is being evaluated, such as: “If you lived with both parents and they later separated, how
old were you at the time?,” “Was one of your parents ever sent to jail or prison?,” “Was
your mother ever arrested, that you know of?”

The fact that COMPAS requires over 130 questions to be answered, many of whose
answers may not be veriﬁable, means that the computation of the COMPAS score is prone
to errors. Even the Arnold Foundation’s “public-safety assessment” (PSA) score—which is
completely transparent, and has only 9 factors—has been miscalculated in serious criminal
trials, leading to a recent lawsuit (Westervelt, 2017). It is substantially more diﬃcult to
obtain the information required to calculate COMPAS scores than PSA scores (with over 14
times the number of survey questions). This signiﬁcant discrepancy suggests that COMPAS
scores are more fallible than PSA scores, as well as even simpler models, like those produced
by CORELS. Some of these problems could be alleviated by using only data within electronic
records that can be automatically calculated, instead of using information entered by hand
and/or collected via subjective surveys.

The United States government pays Northpointe (now called Equivant) to use COMPAS.
In light of our observations that CORELS is as accurate as COMPAS on a real-world data
set where COMPAS is used in practice, CORELS predicts similarly to COMPAS for both
blacks and whites, and CORELS’ models are completely transparent, it is not clear what
value COMPAS scores possess. Our experiments also indicate that the proprietary survey
data required to compute COMPAS scores has not boosted its prediction accuracy above
that of transparent models in practice.

Risk predictions are important for the integrity of the judicial system; judges cannot be
expected to keep entire databases in their heads to calculate risks, whereas models (when
used correctly) can help to ensure equity. Risk prediction models also have the potential
to heavily impact how eﬃcient the judicial system is, in terms of bail and parole decisions;
eﬃciency in this case means that dangerous individuals are not released, whereas non-
dangerous individuals are granted bail or parole. High stakes decisions, such as these, are
ideal applications for machine learning algorithms that produce transparent models from
high dimensional data.

Currently, justice system data does not support highly accurate risk predictions, but
current risk models are useful in practice, and these risk predictions will become more
accurate as more and higher quality data are made available.

6.5 Comparison of CORELS to a Heuristic Model for Weapon Prediction

CORELS generates simple, accurate models for the task of weapon prediction, using the
NYPD stop-and-frisk data set. Our approach oﬀers a principled alternative to heuristic
models proposed by Goel et al. (2016), who develop a series of regression models to analyze
racial disparities in New York City’s stop-and-frisk policy for a related, larger data set. In
particular, the authors arrive at a heuristic that they suggest could potentially help police
oﬃcers more eﬀectively decide when to frisk and/or search stopped individuals, i.e., when
such interventions are likely to discover criminal possession of a weapon (CPW). Starting
from a full regression model with 7,705 variables, the authors reduce this to a smaller model
with 98 variables; from this, they keep three variables with the largest coeﬃcients. This gives

42

Learning Certifiably Optimal Rule Lists

a heuristic model of the form ax + by + cz ≥ T , where

x = 1[stop reason = suspicious object]
y = 1[stop reason = suspicious bulge]
z = 1[additional circumstances = sights and sounds of criminal activity],

and T is a threshold, such that the model predicts CPW when the threshold is met or
exceeded. We focus on their approach that uses a single threshold, rather than precinct-
speciﬁc thresholds. To increase ease-of-use, the authors round the coeﬃcients to the nearest
integers, which gives (a, b, c) = (3, 1, 1); this constrains the threshold to take one of six
values, T ∈ {0, 1, 2, 3, 4, 5}. To employ this heuristic model in the ﬁeld, “. . . oﬃcers simply
need to add at most three small, positive integers . . . and check whether the sum exceeds a
ﬁxed threshold. . . ” (Goel et al., 2016).

Figure 8 directly compares various models learned by CORELS to the heuristic models,
using the same data set as Goel et al. (2016) and 10-fold cross-validation. Recall that we
train on resampled data to correct for class imbalance; we evaluate with respect to test sets
that have been formed without resampling. For CORELS, the models correspond to the
rule lists illustrated in Figure 6 from Section 6.3, and Figures 21 and 22 in Appendix F, we
consider both Feature Sets C and D and both regularization parameters λ = 0.005 and 0.01.
The top panel plots the fraction of weapons recovered as a function of the fraction of stops
where the individual was frisked and/or searched. Goel et al. (2016) target models that
eﬃciently recover a majority of weapons (while also minimizing racial disparities, which
we do not address here). Interestingly, the models learned by CORELS span a signiﬁcant
region that is not available to the heuristic model, which would require larger or non-integer
parameters to access the region. The region is possibly desirable, since it includes models
(λ = 0.005, bright red) that recover a majority (≥ 50%) of weapons (that are known in the
data set). More generally, CORELS’ models all recover at least 40% of weapons on average,
i.e., more weapons than any of the heuristic models with T ≥ 2, which recover less than
25% of weapons on average. At the same time, CORELS’ models all require well under 25%
of stops—signiﬁcantly less than the heuristic model with T = 1, which requires over 30% of
stops to recover a fraction of weapons comparable to the CORELS model that recovers the
most weapons.

The bottom panel in Figure 8 plots both TPR and FPR and labels model size, for each
of the models in the top panel. For the heuristic, we deﬁne model size as the number of
model parameters; for CORELS, we use the number of rules in the rule list, which is equal
to the number of leaves when we view a rule list as a decision tree. The heuristic models all
have 4 parameters, while the diﬀerent CORELS models have either 3 or approximately 5
rules. CORELS’ models are thus approximately as small, interpretable, and transparent as
the heuristic models; furthermore, their predictions are straightforward to compute, without
even requiring arithmetic.

6.6 Predictive Performance and Model Size for CORELS and Other Algorithms

We ran a 10-fold cross validation experiment using CORELS and eight other algorithms:
logistic regression, support vector machines (SVM), AdaBoost, CART, C4.5, random for-

43

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 8: Weapon prediction with the NYPD stop-and-frisk data set, for various models
learned by CORELS and the heuristic model by Goel et al. (2016), across 10
cross-validation folds. Note that the fraction of weapons recovered (top) is equal
to the TPR (bottom, open markers). Markers above the dotted horizontal lines at
the value 0.5 correspond to models that recover a majority of weapons (that are
known in the data set). Top: Fraction of weapons recovered as a function of the
fraction of stops where the individual was frisked and/or searched. In the legend,
entries for CORELS (red markers) indicate the regularization parameter (λ) and
whether or not extra location features were used (“location”); entries for the
heuristic model (blue markers) indicate the threshold value (T ). The results we
report for the heuristic model are our reproduction of the results reported in Figure
9 by Goel et al. (2016) (ﬁrst four open circles in that ﬁgure, from left to right;
we exclude the trivial open circle showing 100% of weapons recovered at 100% of
stops, obtained by setting the threshold at 0). Bottom: Comparison of TPR (open
markers) and FPR (solid markers) for various CORELS and heuristic models.
Models are sorted left-to-right by TPR. Markers and abbreviated horizontal tick
labels correspond to the legend in the top ﬁgure. Numbers in the plot label model
size; there was no variation in model size across folds, except for a single fold for
CORELS (λ = 0.005, Feature Set C), which found a model of size 6.

44

Learning Certifiably Optimal Rule Lists

Figure 9: Two-year recidivism prediction for the ProPublica COMPAS data set. Compari-
son of CORELS and a panel of nine other algorithms: logistic regression (GLM),
support vector machines (SVM), AdaBoost, CART, C4.5, random forests (RF),
RIPPER, scalable Bayesian rule lists (SBRL), and COMPAS. For CORELS, we
use regularization parameter λ = 0.005.

est (RF), RIPPER, and scalable Bayesian rule lists (SBRL).7 We use standard R packages,
with default parameter settings, for the ﬁrst seven algorithms.8 We use the same antecedent
sets as input to the two rule list learning algorithms, CORELS and SBRL; for the other
algorithms, the inputs are binary feature sets corresponding to the single clause antecedents
in the aforementioned antecedent sets (see Appendix E).

Figure 9 shows that for the ProPublica data set, there were no statistically signiﬁcant
diﬀerences in test accuracies across algorithms, the diﬀerence between folds was far larger
than the diﬀerence between algorithms. These algorithms also all perform similarly to the
black box COMPAS algorithm. Figure 10 shows that for the NYCLU data set, logistic
regression, SVM, and AdaBoost have the highest TPR’s and also the highest FPR’s; we show
TPR and FPR due to class imbalance. For this problem, CORELS obtains an intermediate
TPR, compared to other algorithms, while achieving a relatively low FPR. We conclude
that CORELS produces models whose predictive performance is comparable to or better
than those found via other algorithms.

Figures 11 and 12 summarize diﬀerences in predictive performance and model size for
CORELS and other tree (CART, C4.5) and rule list (RIPPER, SBRL) learning algorithms.
Here, we vary diﬀerent algorithm parameters, and increase the number of iterations for
SBRL to 10,000. For two-year recidivism prediction with the ProPublica data set (Fig-
ure 11), we plot both training and test accuracy, as a function of the number of leaves
in the learned model. Due to class imbalance for the weapon prediction problem with the
NYCLU stop-and-frisk data set (Figure 12), we plot both true positive rate (TPR) and
false positive rate (FPR), again as a function of the number of leaves. For both problems,
CORELS can learn short rule lists without sacriﬁcing predictive performance. For listings
of example optimal rule lists that correspond to the results for CORELS summarized here,

7. For SBRL, we use the C implementation at https://github.com/Hongyuy/sbrlmod. By default, SBRL

sets η = 3, λ = 9, the number of chains to 11 and iterations to 1,000.

8. For CART, C4.5 (J48), and RIPPER, we use the R packages rpart, RWeka, and caret, respectively. By
default, CART uses complexity parameter cp = 0.01 and C4.5 uses complexity parameter C = 0.25.

45

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 10: TPR (left) and FPR (right) for the test set, for CORELS and a panel of seven
other algorithms, for the weapon prediction problem with the NYCLU stop-
and-frisk data set. Means (white squares), standard deviations (error bars), and
values (colors correspond to folds), for 10-fold cross-validation experiments. For
CORELS, we use λ = 0.01. Note that we were unable to execute RIPPER for
the NYCLU problem.

Figure 11: Training and test accuracy as a function of model size, across diﬀerent methods,
for two-year recidivism prediction with the ProPublica COMPAS data set. In
the legend, numbers in parentheses are algorithm parameters that we vary for
CORELS (λ), CART (cp), C4.5 (C), and SBRL (η, λ, i), where i is the num-
ber of iterations. Legend markers and error bars indicate means and standard
deviations, respectively, across cross-validation folds. Small circles mark training
accuracy means. None of the models exhibit signiﬁcant overﬁtting; mean training
accuracy never exceeds mean test accuracy by more than about 0.01.

see Appendix F. Also see Figure 25 in Appendix G; it uses the larger NYPD data set and
is similar to Figure 12.

In Figure 4, we used CORELS to identify short rule lists, depending on only three
features—age, prior convictions, and sex—that achieve test accuracy comparable to COM-
PAS (Figure 9, also see Angelino et al., 2017). If we restrict CORELS to search the space
of rule lists formed from only age and prior convictions (λ = 0.005), the optimal rule lists it

46

Learning Certifiably Optimal Rule Lists

Figure 12: TPR (top) and FPR (bottom) for the test set, as a function of model size, across
diﬀerent methods, for weapon prediction with the NYCLU stop-and-frisk data
set. In the legend, numbers in parentheses are algorithm parameters, as in Fig-
ure 11. Legend markers and error bars indicate means and standard deviations,
respectively, across cross-validation folds. C4.5 ﬁnds large models for all tested
parameters.

if (priors > 3) then predict yes
else if (age < 25) and (priors = 2 − 3) then predict yes
else predict no

Figure 13: When restricted to two features (age, priors), CORELS (λ = 0.005) ﬁnds the

same rule list across 10 cross-validation folds.

ﬁnds achieve test accuracy that is again comparable to COMPAS. CORELS identiﬁes the
same rule list across all 10 folds of 10-fold cross-validation experiments (Figure 13). In work
subsequent to ours (Angelino et al., 2017), Dressel and Farid (2018) conﬁrmed this result,
in the sense that they used logistic regression to construct a linear classiﬁer with age and
prior convictions, and also achieved similar accuracy to COMPAS. However, computing a
logistic regression model requires multiplication and addition, and their model cannot easily
be computed, in the sense that it requires a calculator (and thus is potentially error-prone).
Our rule lists require no such computation.

47

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

CORELS with diﬀerent regularization parameters (NYCLU stop-and-frisk data set)

Total
time (s)
.61 (.03)
70 (6)
1600 (100)

Time to
optimum (s)
.002 (.001)
.008 (.002)
56 (74)

Lower bound
evaluations (×106)
.070 (.004)
7.5 (.6)
150 (10)

Max evaluated
preﬁx length
6
11
16-17
Total queue
insertions (×103)
2.2 (.1)
210 (20)
4400 (300)

Optimal
preﬁx length
2
3
6-10
Max queue
size (×103)
.9 (.1)
130 (10)
2500 (170)

λ
.04
.01
.0025

λ
.04
.01
.0025

Table 3: Summary of CORELS executions,

for the NYCLU stop-and-frisk data set
(M = 46), for same three regularization parameter (λ) values as in Figure 12.
The columns report the total execution time, time to optimum, maximum evalu-
ated preﬁx length, optimal preﬁx length, number of times we completely evaluate
a preﬁx dp’s lower bound b(dp, x, y), total number of queue insertions (this number
is equal to the number of cache insertions), and the maximum queue size. For pre-
ﬁx lengths, we report single values or ranges corresponding to the minimum and
maximum observed values; in the other columns, we report means (and standard
deviations) over 10 cross-validation folds. See also Figures 14 and 15.

6.7 CORELS Execution Traces, for Diﬀerent Regularization Parameters

In this section, we illustrate several views of CORELS execution traces, for the NYCLU stop-
and-frisk data set with M = 46 antecedents, for the same three regularization parameters
(λ = .04, .01, .025) as in Figure 12.

Table 3 summarizes execution traces across all 10 cross-validation folds. For each value
of λ, CORELS achieves the optimum in a small fraction of the total execution time. As λ
decreases, these times increase because the search problems become more diﬃcult, as is
summarized by the observation that CORELS must evaluate longer preﬁxes; consequently,
our data structures grow in size. We report the total number of elements inserted into the
queue and the maximum queue size; recall from §5 that the queue elements correspond to
the trie’s leaves, and that the symmetry-aware map elements correspond to the trie’s nodes.
The upper panels in Figure 14 plot example execution traces, from a single cross-
validation fold, of both the current best objective value Rc and the lower bound b(dp, x, y)
of the preﬁx dp being evaluated. These plots illustrate that CORELS certiﬁes optimality
when the lower bound matches the objective value. The lower panels in Figure 14 plot corre-
sponding traces of an upper bound on the size of the remaining search space (Theorem 7),
and illustrate that as λ decreases, it becomes more diﬃcult to eliminate regions of the
search space. For Figure 14, we dynamically and incrementally calculate (cid:98)log10 Γ(Rc, Q)(cid:99),
which adds some computational overhead; we do not calculate this elsewhere unless noted.
Figure 15 visualizes the elements in CORELS’ logical queue, for each of the executions in
Figure 14. Recall from §5.5 that the logical queue corresponds to elements in the (physical)

48

Learning Certifiably Optimal Rule Lists

Figure 14: Example executions of CORELS,

for the NYCLU stop-and-frisk data set
(M = 46). See also Table 3 and Figure 15. Top: Objective value (solid line)
and lower bound (dashed line) for CORELS, as a function of wall clock time (log
scale). Numbered points along the trace of the objective value indicate when the
length of the best known rule list changes and are labeled by the new length.
For each value of λ, a star marks the optimum objective value and time at which
it was achieved. Bottom: (cid:98)log10 Γ(Rc, Q)(cid:99), as a function of wall clock time (log
scale), where Γ(Rc, Q) is the upper bound on remaining search space size (Theo-
rem 7). Rightmost panels: For visual comparison, we overlay the execution traces
from the panels to the left, for the three diﬀerent values of λ.

queue that have not been garbage collected from the trie; these are preﬁxes that CORELS
has already evaluated and whose children the algorithm plans to evaluate next. As an
execution progresses, longer preﬁxes are placed in the queue; as λ decreases, the algorithm
must spend more time evaluating longer and longer preﬁxes.

6.8 Eﬃcacy of CORELS Algorithm Optimizations

This section examines the eﬃcacy of each of our bounds and data structure optimizations.
We remove a single bound or data structure optimization from our ﬁnal implementation
and measure how the performance of our algorithm changes. We examine these performance
traces on both the NYCLU and the ProPublica data sets, and highlight the result that on
diﬀerent problems, the relative performance improvements of our optimizations can vary.

Table 4 provides summary statistics for experiments using the full CORELS implementa-
tion (ﬁrst row) and ﬁve variants (subsequent rows) that each remove a speciﬁc optimization:
(1) Instead of a priority queue (§5.2) ordered by the objective lower bound, we use a queue
that implements breadth-ﬁrst search (BFS). (2) We remove checks that would trigger prun-
ing via our lower bounds on antecedent support (Theorem 10) and accurate antecedent
support (Theorem 11). (3) We remove the eﬀect of our lookahead bound (Lemma 2), which
otherwise tightens the objective lower bound by an amount equal to the regularization pa-
rameter λ. (4) We disable the symmetry-aware map (§5.3), our data structure that enables
pruning triggered by the permutation bound (Corollary 16). (5) We do not identify sets
of equivalent points, which we otherwise use to tighten the objective lower bound via the
equivalent points bound (Theorem 20).

49

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 15: Summary of CORELS’ logical queue, for the NYCLU stop-and-frisk data set
(M = 46), for same three regularization parameters as in Figure 12 and Table 3.
Solid lines plot the numbers of preﬁxes in the logical queue (log scale), colored by
length (legend), as a function of wall clock time (log scale). All plots are generated
using a single, representative cross-validation training set. For each execution,
the gray shading ﬁlls in the area beneath the total number of queue elements,
i.e., the sum over all lengths; we also annotate the total time in seconds, marked
with a dashed vertical line.

Removing any single optimization increases total execution time, by varying amounts
across these optimizations. Similar to our experiments in §6.7, we always encounter the
optimal rule list in far less time than it takes to certify optimality. As in Table 3, we report
metrics that are all proxies for how much computational work our algorithm must perform;
these metrics thus scale with the overall slowdown with respect to CORELS execution time
(Table 4, ﬁrst column).

Figure 16 visualizes execution traces of the elements in CORELS’ logical queue, similar
to Figure 15, for a single, representative cross-validation fold. Panels correspond to diﬀerent
removed optimizations, as in Table 4. These plots demonstrate that our optimizations reduce
the number of evaluated preﬁxes and are especially eﬀective at limiting the number of longer
evaluated preﬁxes. For the ProPublica data set, the most important optimization is the
equivalent points bound—without it, we place preﬁxes of at least length 10 in our queue,
and must terminate these executions before they are complete. In contrast, CORELS and
most other variants evaluate only preﬁxes up to at most length 5, except for the variant
without the lookahead bound, which evaluates preﬁxes up to length 6.

Table 5 and Figure 17 summarize an analogous set of experiments for the NYCLU data
set. Note that while the equivalent points bound proved to be the most important opti-
mization for the ProPublica data set, the symmetry-aware map is the crucial optimization
for the NYCLU data set.

Finally, Figure 18 highlights the most signiﬁcant algorithm optimizations for our pre-
diction problems: the equivalent points bound for the ProPublica data set (left) and the

50

Learning Certifiably Optimal Rule Lists

Per-component performance improvement (ProPublica data set)

Slow-
down
—
1.1×
1.5×
13.3×
8.4×

Total time
(min)
.98 (.6)
1.03 (.6)
1.5 (.9)
12.3 (6.2)
9.1 (6.4)

Time to
optimum (s)
1 (1)
2 (4)
1 (2)
1 (1)
2 (3)

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound* >130 (2.6) >180× >1400 (2000)
Total queue
insertions (×106)
.29 (.2)
.33 (.2)
.40 (.2)
3.6 (1.8)
2.5 (1.7)
>510 (1.1)

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound*

Lower bound
evaluations (×106)
26 (15)
27 (16)
42 (25)
320 (160)
250 (180)

>940 (5)

Max evaluated
preﬁx length
5
5
5
6
5
≥11
Max queue
size (×106)
.24 (.1)
.20 (.1)
.33 (.2)
3.0 (1.5)
2.4 (1.7)
>500 (1.2)

Table 4: Per-component performance improvement, for the ProPublica data set (λ = 0.005,
M = 122). The columns report the total execution time, time to optimum, maxi-
mum evaluated preﬁx length, number of times we completely evaluate a preﬁx dp’s
lower bound b(dp, x, y), total number of queue insertions (which is equal to the
number of cache insertions), and maximum logical queue size. The ﬁrst row shows
CORELS; subsequent rows show variants that each remove a speciﬁc implemen-
tation optimization or bound. (We are not measuring the cumulative eﬀects of
removing a sequence of components.) All rows represent complete executions that
certify optimality, except those labeled ‘No equivalent points bound,’ for which
each execution was terminated due to memory constraints, once the size of the
cache reached 5 × 108 elements, after consuming ∼250GB RAM. In all but the
ﬁnal row and column, we report means (and standard deviations) over 10 cross-
validation folds. We also report the mean slowdown in total execution time, with
respect to CORELS. In the ﬁnal row, we report the mean (and standard deviation)
of the incomplete execution time and corresponding slowdown, and a lower bound
on the mean time to optimum; in the remaining ﬁelds, we report minimum values
across folds. See also Figure 16.
* Only 7 out of 10 folds achieve the optimum before being terminated.

symmetry-aware map for the NYCLU data set (right). For CORELS (thin lines) with
the ProPublica recidivism data set (left), the objective drops quickly, achieving the op-
timal value within a second. CORELS certiﬁes optimality in about a minute—the objective
lower bound steadily converges to the optimal objective (top) as the search space shrinks
(bottom). As in Figure 14, we dynamically and incrementally calculate (cid:98)log10 Γ(Rc, Q)(cid:99),
where Γ(Rc, Q) is the upper bound (13) on remaining search space size (Theorem 7); this
adds some computational overhead. In the same plots (left), we additionally highlight a

51

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 16: Summary of the logical queue’s contents, for full CORELS (top left) and ﬁve
variants that each remove a speciﬁc implementation optimization or bound, for
the ProPublica data set (λ = 0.005, M = 122). See also Table 4. Solid lines plot
the numbers of preﬁxes in the logical queue (log scale), colored by length (legend),
as a function of wall clock time (log scale). All plots are generated using a single,
representative cross-validation training set. The gray shading ﬁlls in the area
beneath the total number of queue elements for CORELS, i.e., the sum over
all lengths in the top left ﬁgure. For comparison, we replicate the same gray
region in the other ﬁve subﬁgures. For each execution, we indicate the total time
in seconds, relative to the full CORELS implementation (T = 21 s), and with a
dashed vertical line. The execution without the equivalent points bound (bottom
right) is incomplete.

separate execution of CORELS without the equivalent points bound (Theorem 20) (thick
lines). After more than 2 hours, the execution is still far from complete; in particular, the
lower bound is far from the optimum objective value (top) and much of the search space
remains unexplored (bottom). For the NYCLU stop-and-frisk data set (right), CORELS
achieves the optimum objective in well under a second, and certiﬁes optimality in a lit-
tle over a minute. CORELS without the permutation bound (Corollary 16), and thus the
symmetry-aware map, requires more than an hour, i.e., orders of magnitude more time, to
complete (thick lines).

52

Learning Certifiably Optimal Rule Lists

Per-component performance improvement (NYCLU stop-and-frisk data set)

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound

Slow-
down
—
2.0×
1.1×
1.6×

Time to
optimum (µs)
8.9 (.1)
110 (10)
8.8 (.8)
7.3 (1.8)

Total
time (min)
1.1 (.1)
2.2 (.2)
1.2 (.1)
1.7 (.2)
> 73 (5)
4 (.3)

3.8×

> 68× > 7.6 (.4)
6.4 (.9)
Total queue
insertions (×105)
2.0 (.2)
4.1 (.4)
2.1 (.2)
3.2 (.3)
> 1000 (0)
9.4 (.7)

Lower bound
evaluations (×106)
7 (1)
14 (1)
8 (1)
11 (1)
> 390 (40)
33 (2)

Max evaluated
preﬁx length
11
11
11
11-12
> 10
14
Max queue
size (×105)
1.3 (.1)
1.4 (.1)
1.3 (.1)
2.1 (.2)
> 900 (10)
6.0 (.4)

Table 5: Per-component performance improvement, as in Table 4, for the NYCLU stop-and-
frisk data set (λ = 0.01, M = 46). All rows except those labeled ‘No symmetry-
aware map’ represent complete executions. A single fold running without a
symmetry-aware map required over 2 days to complete, so in order to run all
10 folds above, we terminated execution after the preﬁx tree (§5.1) reached 108
nodes. See Table 4 for a detailed caption, and also Figure 17.

Algorithmic
approach
CORELS
Brute force
Brute force
CORELS (1984)

Max evaluated Lower bound
evaluations
preﬁx length
2.8 × 107
5
2.5 × 1010
5
5.0 × 1020
10
2.8 × 107
5

Predicted runtime

36 seconds

9.0 hours ≈ 3.3 × 104 s
21 × 106 years ≈ 6.5 × 1014 s
13.5 days ≈ 1.2 × 106 s

Table 6: Algorithmic speedup for the ProPublica data set (λ = 0.005, M = 122). Solving
this problem using brute force is impractical due to the inability to explore rule
lists of reasonable lengths. Removing only the equivalent points bound requires
exploring preﬁxes of up to length 10 (see Table 4), a clearly intractable problem.
Even with all of our improvements, however, it is only recently that processors
have been fast enough for this type of discrete optimization algorithm to succeed.

6.9 Algorithmic Speedup

Table 6 shows the overall speedup of CORELS compared to a na¨ıve implementation and
demonstrates the infeasibility of running our algorithm 30 years ago. Consider an execution
of CORELS for the ProPublica data set, with M = 122 antecedents, that evaluates preﬁxes

53

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 17: Summary of the logical queue’s contents, for full CORELS (top left) and ﬁve
variants that each remove a speciﬁc implementation optimization or bound, for
the NYCLU stop-and-frisk data set (λ = 0.01, M = 46), as in Table 5. The ex-
ecution without the symmetry-aware map (bottom center) is incomplete. See
Figure 16 for a detailed caption.

up to length 5 in order to certify optimality (Table 4). A brute force implementation that
na¨ıvely considers all preﬁxes of up to length 5 would evaluate 2.5 × 1010 preﬁxes. As shown
in Figure 4, the optimal rule list has preﬁx length 3, thus the brute force algorithm would
identify the optimal rule list. However, for this approach to certify optimality, it would
have to consider far longer preﬁxes. Without our equivalent points bound, but with all of
our other optimizations, we evaluate preﬁxes up to at least length 10 (see Table 4 and
Figure 16)—thus a brute force algorithm would have to evaluate preﬁxes of length 10 or
longer. Na¨ıvely evaluating all preﬁxes up to length 10 would require looking at 5.0 × 1020
diﬀerent preﬁxes.

However, CORELS examines only 28 million preﬁxes in total—a reduction of 893×
compared to examining all preﬁxes up to length 5 and a reduction of 1.8 × 1013 for the case
of length 10. On a laptop, we require about 1.3 µs to evaluate a single preﬁx (given by
dividing the number of lower bound evaluations by the total time in Table 4). Our runtime
is only about 36 seconds, but the na¨ıve solutions of examining all preﬁxes up to lengths 5

54

Learning Certifiably Optimal Rule Lists

Figure 18: Execution progress of CORELS and selected variants,

for the ProPublica
(λ = 0.005, M = 122) (left) and NYCLU (λ = 0.01, M = 46) (right) data sets.
Top: Objective value (thin solid lines) and lower bound (dashed lines) for
CORELS, as a function of wall clock time (log scale). Numbered points along the
trace of the objective value indicate when the length of the best known rule list
changes, and are labeled by the new length. CORELS quickly achieves the opti-
mal value (star markers), and certiﬁes optimality when the lower bound matches
the objective value. On the left, a separate and signiﬁcantly longer execution of
CORELS without the equivalent points (Theorem 20) bound remains far from
complete, and its lower bound (thick solid line) far from the optimum. On the
right, a separate execution of CORELS without the permutation bound (Corol-
lary 16), and thus the symmetry-aware map, requires orders of magnitude more
time to complete. Bottom: (cid:98)log10 Γ(Rc, Q)(cid:99), as a function of wall clock time (log
scale), where Γ(Rc, Q) is the upper bound (13) on remaining search space size
(Theorem 7). For these problems, the equivalent points (left) and permutation
(right) bounds are responsible for the ability of CORELS to quickly eliminate
most of the search space (thin solid lines); the remaining search space decays
much more slowly without these bounds (thick solid lines).

and 10 would take 9 hours and 21 million years, respectively. It is clear that brute force
would not scale to larger problems.

We compare our current computing circumstances to those of 1984, the year when CART
was published. Moore’s law holds that computing power doubled every 18 months from 1984
to 2006. This is a period of 264 months, which means computing power has gone up by at
least a factor of 32,000 since 1984. Thus, even with our algorithmic and data structural
improvements, CORELS would have required about 13.5 days in 1984—an unreasonable
amount of time. Our advances are meaningful only because we can run them on a modern

55

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

system. Combining our algorithmic improvements with the increase in modern processor
speeds, our algorithm runs more than 1013 times faster than a na¨ıve implementation would
have in 1984. This helps explain why neither our algorithm, nor other branch-and-bound
variants, had been developed before now.

7. Summary and Future Work on Bounds

Here, we highlight our most signiﬁcant bounds, as well as directions for future work based
on bounds that we have yet to leverage in practice.

In empirical studies, we found our equivalent support (§3.10, Theorem 15) and equiv-
alent points (§3.14, Theorem 20) bounds to yield the most signiﬁcant improvements in
algorithm performance. In fact, they sometimes proved critical for ﬁnding solutions and
proving optimality, even on small problems.

Accordingly, we would hope that our similar support bound (§3.13, Theorem 18) could
be useful; understanding how to eﬃciently exploit this result in practice represents an
important direction for future work. In particular, this type of bound may lead to principled
approximate variants of our approach.

We presented several sets of bounds in which at least one bound was strictly tighter than
the other(s). For example, the lower bound on accurate antecedent support (Theorem 11)
is strictly tighter than the lower bound on accurate support (Theorem 10). It might seem
that we should only use this tighter bound, but in practice, we can use both—the looser
bound can be checked before completing the calculation required to check the tighter bound.
Similarly, the equivalent support bound (Theorem 15) is more general than the special case
of the permutation bound (Corollary 16). We have implemented data structures, which we
call symmetry-aware maps, to support both of these bounds, but have not yet identiﬁed an
eﬃcient approach for supporting the more general equivalent points bound. A good solution
may be related to the challenge of designing an eﬃcient data structure to support the similar
support bound.

We also presented results on antecedent rejection that unify our understanding of our
lower (§3.7) and upper bounds (§3.8) on antecedent support. In a preliminary implemen-
tation (not described here), we experimented with special data structures to support the
direct use of our observation that antecedent rejection propagates (§3.9, Theorem 12). We
leave the design of eﬃcient data structures for this task as future work.

During execution, we ﬁnd it useful to calculate an upper bound on the size of the
remaining search space—e.g., via Theorem 7, or the looser Proposition 9, which incurs
less computational overhead—since these provide meaningful information about algorithm
progress and allow us to estimate the remaining execution time. As we illustrated in Sec-
tion 6.8, these calculations also help us qualify the impact of diﬀerent algorithmic bounds,
e.g., by comparing executions that keep or remove bounds.

When our algorithm terminates, it outputs an optimal solution of the training optimiza-
tion problem, with a certiﬁcate of optimality. On a practical note, our approach can also
provide useful results even for incomplete executions. As shown earlier, we have empirically
observed that our algorithm often identiﬁes the optimal rule list very quickly, compared to
the total time required to prove optimality, e.g., in seconds, versus minutes, respectively.
Furthermore, our objective’s lower bounds allow us to place an upper bound on the size of

56

Learning Certifiably Optimal Rule Lists

the remaining search space, and provides guarantees on the quality of a solution output by
an incomplete execution.

The order in which we evaluate preﬁxes can impact the rate at which we prune the search
space, and thus the total runtime. We think that it is possible to design search policies that
signiﬁcantly improve performance.

8. Conclusion and More Possible Directions for Future Work

Finally, we would like to clarify some limitations of CORELS. As far as we can tell, CORELS
is the current best algorithm for solving a specialized optimal decision tree problem. While
our approach scales well to large numbers of observations, it could have diﬃculty proving
optimality for problems with many possibly relevant features that are highly correlated,
when large regions of the search space might be challenging to exclude.

CORELS is not designed for raw image processing or other problems where the features
themselves are not interpretable. It could instead be used as a ﬁnal classiﬁer for image
processing problems where the features were created beforehand; for instance, one could
create classiﬁers for each part of an image, and use CORELS to create a ﬁnal combined
classiﬁer. The notions of interpretability used in image classiﬁcation tend to be completely
diﬀerent from those for structured data where each feature is separately meaningful (e.g.,
see Li et al., 2018). For structured data, decision trees, along with scoring systems, tend
to be popular forms of transparent models. Scoring systems are sparse linear models with
integer coeﬃcients, and they can also be created from data (Ustun and Rudin, 2017, 2016).

In some of our experiments, CORELS produces a DNF formula by coincidence, but it
might be possible to create a much simpler version of CORELS that only produces DNF
formulae. This could build oﬀ previous algorithms for creating an optimal DNF formula
(Rijnbeek and Kors, 2010; Wang et al., 2016, 2017).

CORELS does not automatically rank the subgroups in order of the likelihood of a
positive outcome; doing so would require an algorithm such as Falling Rule Lists (Wang
and Rudin, 2015a; Chen and Rudin, 2018), which forces the estimated probabilities to de-
crease along the list. Furthermore, while CORELS does not technically produce estimates
of P(Y = 1 | x), one could form such an estimate by computing the empirical proportion
ˆP(Y = 1 | x obeys pk) for each antecedent pk. CORELS is also not designed to assist with
causal inference applications, since it does not estimate the eﬀect of a treatment via the con-
ditional diﬀerence P(Y = 1 | treatment = True, x) − P(Y = 1 | treatment = False, x). Alter-
native algorithms that estimate conditional diﬀerences with interpretable rule lists include
Causal Falling Rule Lists (Wang and Rudin, 2015b), Cost-Eﬀective Interpretable Treatment
Regimes (CITR) (Lakkaraju and Rudin, 2017), and an approach by Zhang et al. (2015) for
constructing interpretable and parsimonious treatment regimes. Alternatively, one could
use a complex machine learning model to predict outcomes for the treatment group and a
separate complex model for the control group that would allow counterfactuals to be esti-
mated for each observation; from there, CORELS could be applied to produce a transparent
model for personalized treatment eﬀects. A similar approach to this was taken by Goh and
Rudin (2018), who use CORELS to understand a black box causal model.

57

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

CORELS could be adapted to handle cost-sensitive learning or weighted regularization.
This would require creating more general versions of our theorems, which would be an
extension of this work.

While CORELS does not directly handle continuous variables, we have found that it is
not diﬃcult in practice to construct a rule set that is suﬃcient for creating a useful model.
It may be possible to use techniques such as Fast Boxes (Goh and Rudin, 2014) to discover
useful and interpretable rules for continuous data that can be used within CORELS.

An interesting direction for future research would be to create a hybrid interpretable/
black box model in the style of Wang (2018), where the rule list would eliminate large
parts of the space away from the decision boundary, and the observations that remain are
evaluated by a black box model rather than a default rule.

Lastly, CORELS does not create generic single-variable-split decision trees. CORELS
optimizes over rule lists, which are one-sided decision trees; in our setting, the leaves of
these ‘trees’ are conjunctions. It may be possible to generalize ideas from our approach to
handle generic decision trees, which could be an interesting project for future work. There
are more symmetries to handle in that case, since there would be many equivalent decision
trees, leading to challenges in developing symmetry-aware data structures.

Acknowledgments

E.A. conducted most of this work while supported by the Miller Institute for Basic Research
in Science, University of California, Berkeley, and hosted by Prof. M.I. Jordan at RISELab.
C.D.R. is supported in part by MIT-Lincoln Labs and the National Science Foundation
under IIS-1053407. E.A. would like to thank E. Jonas, E. Kohler, and S. Tu for early
implementation guidance, A. D’Amour for pointing out the work by Goel et al. (2016),
V. Kanade, S. McCurdy, J. Schleier-Smith and E. Thewalt for helpful conversations, and
members of RISELab, SAIL, and the UC Berkeley Database Group for their support and
feedback. We thank H. Yang and B. Letham for sharing advice and code for processing data
and mining rules, B. Coker for his critical advice on using the ProPublica COMPAS data set,
as well as V. Kaxiras and A. Saligrama for their recent contributions to our implementation
and for creating the CORELS website. We are very grateful to our editor and anonymous
reviewers.

Appendix A. Excessive Antecedent Support

Theorem 21 (Upper bound on antecedent support) Let d∗ = (dp, δp, q0, K) be any op-
timal rule list with objective R∗, i.e., d∗ ∈ argmind R(d, x, y), and let dp = (p1, . . . , pk−1,
pk, . . . , pK) be its preﬁx. For each k ≤ K, antecedent pk in dp has support less than or equal
to the fraction of all data not captured by preceding antecedents, by an amount greater than
the regularization parameter λ:

supp(pk, x | dp) ≤ 1 − supp(dk−1

p

, x) − λ,

(37)

where dk−1
that there also exists a shorter optimal rule list d(cid:48) = (dK−1

p = (p1, . . . , pk−1). For the last antecedent, i.e., when pk = pK, equality implies

0, K − 1) ∈ argmind R(d, x, y).

p, q(cid:48)

, δ(cid:48)

p

58

Learning Certifiably Optimal Rule Lists

Proof First, we focus on the last antecedent pK+1 in a rule list d(cid:48). Let d = (dp, δp, q0, K)
be a rule list with preﬁx dp = (p1, . . . , pK) and objective R(d, x, y) ≥ R∗, where R∗ ≡
minD R(D, x, y) is the optimal objective. Let d(cid:48) = (d(cid:48)
0, K + 1) be a rule list whose
preﬁx d(cid:48)
p = (p1, . . . , pK, pK+1) starts with dp and ends with a new antecedent pK+1. Sup-
pose pK+1 in the context of d(cid:48)
p captures nearly all data not captured by dp, except for a
fraction (cid:15) upper bounded by the regularization parameter λ:

p, δ(cid:48)

p, q(cid:48)

1 − supp(dp, x) − supp(pK+1, x | d(cid:48)

p) ≡ (cid:15) ≤ λ.

Since d(cid:48)
p starts with dp, its preﬁx misclassiﬁcation error is at least as great; the only discrep-
ancy between the misclassiﬁcation errors of d and d(cid:48) can come from the diﬀerence between
the support of the set of data not captured by dp and the support of pK+1:

|(cid:96)(d(cid:48), x, y) − (cid:96)(d, x, y)| ≤ 1 − supp(dp, x) − supp(pK+1, x | d(cid:48)

p) = (cid:15).

The best outcome for d(cid:48) would occur if its misclassiﬁcation error were smaller than that
of d by (cid:15), therefore

R(d(cid:48), x, y) = (cid:96)(d(cid:48), x, y) + λ(K + 1)

≥ (cid:96)(d, x, y) − (cid:15) + λ(K + 1) = R(d, x, y) − (cid:15) + λ ≥ R(d, x, y) ≥ R∗.

d(cid:48) is an optimal rule list, i.e., d(cid:48) ∈ argminD R(D, x, y), if and only if R(d(cid:48), x, y) = R(d, x, y) =
R∗, which requires (cid:15) = λ. Otherwise, (cid:15) < λ, in which case

R(d(cid:48), x, y) ≥ R(d, x, y) − (cid:15) + λ > R(d, x, y) ≥ R∗,

therefore d(cid:48) is not optimal, i.e., d(cid:48) /∈ argminD R(D, x, y). This demonstrates the desired
result for k = K.

In the remainder, we prove the bound in (37) by contradiction, in the context of a rule
list d(cid:48)(cid:48). Let d and d(cid:48) retain their deﬁnitions from above, thus as before, that the data not
captured by d(cid:48)

p has normalized support (cid:15) ≤ λ, i.e.,

1 − supp(d(cid:48)

p, x) = 1 − supp(dp, x) − supp(pK+1, x | d(cid:48)

p) = (cid:15) ≤ λ.

Thus for any rule list d(cid:48)(cid:48) whose preﬁx d(cid:48)(cid:48)
p and ends
with one or more additional rules, each additional rule pk has support supp(pk, x | d(cid:48)(cid:48)
p) ≤
(cid:15) ≤ λ, for all k > K + 1. By Theorem 10, all of the additional rules have insuﬃcient sup-
port, therefore d(cid:48)(cid:48)

p = (p1, . . . , pK+1, . . . , pK(cid:48)) starts with d(cid:48)

p cannot be optimal, i.e., d(cid:48)(cid:48) /∈ argminD R(D, x, y).

Similar to Theorem 10, our lower bound on antecedent support, we can apply Theo-
rem 21 in the contexts of both constructing rule lists and rule mining (§3.1). Theorem 21
implies that if we only seek a single optimal rule list, then during branch-and-bound ex-
ecution, we can prune a preﬁx if we ever add an antecedent with support too similar to
the support of the set of data not captured by the preceding antecedents. One way to view
this result is that if d = (dp, δp, q0, K) and d(cid:48) = (d(cid:48)
0, K + 1) are rule lists such that d(cid:48)
p
starts with dp and ends with an antecedent that captures all or nearly all data not captured
by dp, then the new rule in d(cid:48) behaves similar to the default rule of d. As a result, the
misclassiﬁcation error of d(cid:48) must be similar to that of d, and any reduction may not be
suﬃcient to oﬀset the penalty for longer preﬁxes.

p, δ(cid:48)

p, q(cid:48)

59

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Proposition 22 (Excessive antecedent support propagates) Deﬁne φ(dp) as in (19),
and let dp = (p1, . . . , pK) be a preﬁx, such that its last antecedent pK has excessive support,
i.e., the opposite of the bound in (37):

supp(pK, x | dp) > 1 − supp(dK−1

p

, x) − λ,

p

where dK−1
= (p1, . . . , pK−1). Let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp =
(P1, . . . , Pκ) such that Dp starts with DK(cid:48)−1
) and PK(cid:48) = pK. It
follows that PK(cid:48) has excessive support in preﬁx Dp, and furthermore, D /∈ argmind R(d, x, y).

= (P1, . . . , PK(cid:48)−1) ∈ φ(dK−1

p

p

Proof Since DK(cid:48)

p = (P1, . . . , PK(cid:48)) contains all the antecedents in dp, we have that

supp(DK(cid:48)

p , x) ≥ supp(dp, x).

Expanding these two terms gives

supp(DK(cid:48)

p , x) = supp(DK(cid:48)−1

p

, x) + supp(PK(cid:48), x | Dp)

≥ supp(dp, x) = supp(dK−1

p

, x) + supp(pK, x | dp) > 1 − λ.

Rearranging gives

supp(PK(cid:48), x | Dp) > 1 − supp(DK(cid:48)−1

p

, x) − λ,

thus PK(cid:48) has excessive support in Dp. By Theorem 21, D /∈ argmind R(d, x, y).

Appendix B. Proof of Theorem 15 (Equivalent Support Bound)

We begin by deﬁning four related rule lists. First, let d = (dp, δp, q0, K) be a rule list with
preﬁx dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK). Second, let D = (Dp, ∆p, Q0, κ) be
a rule list with preﬁx Dp = (P1, . . . , Pκ) that captures the same data as dp, and labels
0, K(cid:48)) ∈ σ(dp) be any rule list whose preﬁx
∆p = (Q1, . . . , Qκ). Third, let d(cid:48) = (d(cid:48)
starts with dp, such that K(cid:48) ≥ K. Denote the preﬁx and labels of d(cid:48) by d(cid:48)
p = (p1, . . . , pK,
0, κ(cid:48)) ∈
p, Q(cid:48)
p, ∆(cid:48)
pK+1, . . . , pK(cid:48)) and δp = (q1, . . . , qK(cid:48)), respectively. Finally, deﬁne D(cid:48) = (D(cid:48)
σ(Dp) to be the ‘analogous’ rule list, i.e., whose preﬁx D(cid:48)
p = (P1, . . . , Pκ, Pκ+1, . . . , Pκ(cid:48)) =
(P1, . . . , Pκ, pK+1, . . . , pK(cid:48)) starts with Dp and ends with the same K(cid:48) − K antecedents
as d(cid:48)

p. Let ∆(cid:48)
Next, we claim that the diﬀerence in the objectives of rule lists d(cid:48) and d is the same as
the diﬀerence in the objectives of rule lists D(cid:48) and D. Let us expand the ﬁrst diﬀerence as

p = (Q1, . . . , Qκ(cid:48)) denote the labels of D(cid:48).

p, q(cid:48)

p, δ(cid:48)

R(d(cid:48), x, y) − R(d, x, y) = (cid:96)(d(cid:48), x, y) + λK(cid:48) − (cid:96)(d, x, y) − λK
= (cid:96)p(d(cid:48)

p, x, y) + (cid:96)0(d(cid:48)

0, x, y) − (cid:96)p(dp, δp, x, y) − (cid:96)0(dp, q0, x, y) + λ(K(cid:48) − K).

p, q(cid:48)

p, δ(cid:48)

Similarly, let us expand the second diﬀerence as

R(D(cid:48), x, y) − R(D, x, y) = (cid:96)(D(cid:48), x, y) + λκ(cid:48) − (cid:96)(D, x, y) − λκ
= (cid:96)p(D(cid:48)

p, x, y) + (cid:96)0(D(cid:48)

p, ∆(cid:48)

p, Q(cid:48)

0, x, y) − (cid:96)p(Dp, ∆p, x, y) − (cid:96)0(Dp, Q0, x, y) + λ(K(cid:48) − K),

60

Learning Certifiably Optimal Rule Lists

where we have used the fact that κ(cid:48) − κ = K(cid:48) − K.

The preﬁxes dp and Dp capture the same data. Equivalently, the set of data that is not

captured by dp is the same as the set of data that is not captured by Dp, i.e.,

{xn : ¬ cap(xn, dp)} = {xn : ¬ cap(xn, Dp)}.

Thus, the corresponding rule lists d and D share the same default rule, i.e., q0 = Q0, yielding
the same default rule misclassiﬁcation error:

Similarly, preﬁxes d(cid:48)
same default rule misclassiﬁcation error:

p and D(cid:48)

p capture the same data, and thus rule lists d(cid:48) and D(cid:48) have the

(cid:96)0(dp, q0, x, y) = (cid:96)0(Dp, Q0, x, y).

(cid:96)0(dp, q0, x, y) = (cid:96)0(Dp, Q0, x, y).

At this point, to demonstrate our claim relating the objectives of d, d(cid:48), D, and D(cid:48), what
p and dp is

remains is to show that the diﬀerence in the misclassiﬁcation errors of preﬁxes d(cid:48)
the same as that between D(cid:48)

p and Dp. We can expand the ﬁrst diﬀerence as

(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) − (cid:96)p(dp, δp, x, y) =

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn],

where we have used the fact that since d(cid:48)
p starts with dp, the ﬁrst K rules in d(cid:48)
same mistakes as those in dp. Similarly, we can expand the second diﬀerence as

p make the

(cid:96)p(D(cid:48)

p, ∆(cid:48)

p, x, y) − (cid:96)p(Dp, ∆p, x, y) =

cap(xn, Pk | D(cid:48)

p) ∧ 1[Qk (cid:54)= yn]

cap(xn, pk | D(cid:48)

p) ∧ 1[Qk (cid:54)= yn]

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn]

(38)

n=1
p, δ(cid:48)

k=K+1
p, x, y) − (cid:96)p(dp, δp, x, y).

= (cid:96)p(d(cid:48)

To justify the equality in (38), we observe ﬁrst that preﬁxes D(cid:48)
p start with κ and K
antecedents, respectively, that capture the same data. Second, preﬁxes D(cid:48)
p end with
exactly the same ordered list of K(cid:48) − K antecedents, therefore for any k = 1, . . . , K(cid:48) − K,
antecedent Pκ+k = pK+k in D(cid:48)
p. It follows that
the corresponding labels are all equivalent, i.e., Qκ+k = qK+k, for all k = 1, . . . , K(cid:48) − K, and
consequently, the preﬁx misclassiﬁcation error associated with the last K(cid:48) − K antecedents
of d(cid:48)
p. We have therefore shown that the diﬀerence between the
objectives of d(cid:48) and d is the same as that between D(cid:48) and D, i.e.,

p captures the same data as pK+k captures in d(cid:48)

p is the same as that of D(cid:48)

p and d(cid:48)

p and d(cid:48)

R(d(cid:48), x, y) − R(d, x, y) = R(D(cid:48), x, y) − R(D, x, y).

(39)

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K+1

N
(cid:88)

κ(cid:48)
(cid:88)

n=1

N
(cid:88)

k=κ+1
K(cid:48)
(cid:88)

n=1

N
(cid:88)

k=K+1
K(cid:48)
(cid:88)

1
N

1
N

1
N

=

=

61

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Next, suppose that the objective lower bounds of d and D obey b(dp, x, y) ≤ b(Dp, x, y),

therefore

R(d, x, y) = (cid:96)p(dp, δp, x, y) + (cid:96)0(dp, q0, x, y) + λK
= b(dp, x, y) + (cid:96)0(dp, q0, x, y)
≤ b(Dp, x, y) + (cid:96)0(dp, q0, x, y) = b(Dp, x, y) + (cid:96)0(Dp, Q0, x, y) = R(D, x, y).

Now let d∗ be an optimal rule list with preﬁx constrained to start with dp,

d∗ ∈ argmin
d†∈σ(dp)

R(d†, x, y),

and let K∗ be the length of d∗. Let D∗ be the analogous κ∗-rule list whose preﬁx starts
with Dp and ends with the same K∗ − K antecedents as d∗, where κ∗ = κ + K∗ − K.
By (39),

R(d∗, x, y) − R(d, x, y) = R(D∗, x, y) − R(D, x, y).

Furthermore, we claim that D∗ is an optimal rule list with preﬁx constrained to start
with Dp,

(40)

(41)

(42)

D∗ ∈ argmin
D†∈σ(Dp)

R(D†, x, y).

To demonstrate (42), we consider two separate scenarios. In the ﬁrst scenario, preﬁxes dp
and Dp are composed of the same antecedents, i.e., the two preﬁxes are equivalent up to a
permutation of their antecedents, and as a consequence, κ = K and κ∗ = K∗. Here, every
rule list d(cid:48)(cid:48) ∈ σ(dp) that starts with dp has an analogue D(cid:48)(cid:48) ∈ σ(Dp) that starts with Dp,
such that d(cid:48)(cid:48) and D(cid:48)(cid:48) obey (39), and vice versa, and thus (42) is a direct consequence of (41).
In the second scenario, preﬁxes dp and Dp are not composed of the same antecedents.
Deﬁne φ = {pk : (pk ∈ dp) ∧ (pk /∈ Dp)} to be the set of antecedents in dp that are not in Dp,
and deﬁne Φ = {Pk : (Pk ∈ Dp) ∧ (Pk /∈ dp)} to be the set of antecedents in Dp that are not
in dp; either φ (cid:54)= ∅, or Φ (cid:54)= ∅, or both.

Suppose φ (cid:54)= ∅, and let p ∈ φ be an antecedent in φ. It follows that there exists a subset
of rule lists in σ(Dp) that do not have analogues in σ(dp). Let D(cid:48)(cid:48) ∈ σ(Dp) be such a rule
list, such that its preﬁx D(cid:48)(cid:48)
p = (P1, . . . , Pκ, . . . , p, . . . ) starts with Dp and contains p among
its remaining antecedents. Since p captures a subset of the data that dp captures, and Dp
captures the same data as dp, it follows that p does not capture any data in D(cid:48)(cid:48)

p , i.e.,

1
N

N
(cid:88)

n=1

cap(xn, p | D(cid:48)(cid:48)

p ) = 0 ≤ λ.

By Theorem 10, antecedent p has insuﬃcient support in D(cid:48)(cid:48), and thus D(cid:48)(cid:48) cannot be op-
timal, i.e., D(cid:48)(cid:48) /∈ argminD†∈σ(Dp) R(D†, x, y). By a similar argument, if Φ (cid:54)= ∅ and P ∈ Φ,
and d(cid:48)(cid:48) ∈ σ(dp) is any rule list whose preﬁx starts with dp and contains antecedent P , then d(cid:48)(cid:48)
cannot be optimal, i.e., d(cid:48)(cid:48) /∈ argmind†∈σ(dp) R(d†, x, y).

62

Learning Certifiably Optimal Rule Lists

To ﬁnish justifying claim (42) for the second scenario, ﬁrst deﬁne

τ (dp, Φ) ≡ {d(cid:48)(cid:48) = (d(cid:48)(cid:48)

p, δ(cid:48)(cid:48)

p , q(cid:48)(cid:48)

0 , K(cid:48)(cid:48)) : d(cid:48)(cid:48) ∈ σ(dp) and pk /∈ Φ, ∀pk ∈ d(cid:48)(cid:48)

p} ⊂ σ(dp)

to be the set of all rule lists whose preﬁxes start with dp and do not contain any antecedents
in Φ. Now, recognize that the optimal preﬁxes in τ (dp, Φ) and σ(dp) are the same, i.e.,

argmin
d†∈τ (dp,Φ)

R(d†, x, y) = argmin
d†∈σ(dp)

R(d†, x, y),

and similarly, the optimal preﬁxes in τ (Dp, φ) and σ(Dp) are the same, i.e.,

argmin
D†∈τ (Dp,φ)

R(D†, x, y) = argmin
D†∈σ(Dp)

R(D†, x, y).

Since we have shown that every d(cid:48)(cid:48) ∈ τ (dp, Φ) has a direct analogue D(cid:48)(cid:48) ∈ τ (Dp, φ), such
that d(cid:48)(cid:48) and D(cid:48)(cid:48) obey (39), and vice versa, we again have (42) as a consequence of (41).
We can now ﬁnally combine (40) and (42) to obtain the desired inequality in (21):

min
d(cid:48)∈σ(dp)

R(d(cid:48), x, y) = R(d∗, x, y) ≤ R(D∗, x, y) = min

R(D(cid:48), x, y).

D(cid:48)∈σ(Dp)

Appendix C. Proof of Theorem 18 (Similar Support Bound)

We begin by deﬁning four related rule lists. First, let d = (dp, δp, q0, K) be a rule list with
preﬁx dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK). Second, let D = (Dp, ∆p, Q0, κ) be a
rule list with preﬁx Dp = (P1, . . . , Pκ) and labels ∆p = (Q1, . . . , Qκ). Deﬁne ω as in (22)
and Ω as in (23), and require that ω, Ω ≤ λ. Third, let d(cid:48) = (d(cid:48)
0, K(cid:48)) ∈ σ(dp) be
any rule list whose preﬁx starts with dp, such that K(cid:48) ≥ K. Denote the preﬁx and la-
bels of d(cid:48) by d(cid:48)
p = (p1, . . . , pK, pK+1, . . . , pK(cid:48)) and δp = (q1, . . . , qK(cid:48)), respectively. Finally,
p, ∆(cid:48)
deﬁne D(cid:48) = (D(cid:48)
p =
(P1, . . . , Pκ, Pκ+1, . . . , Pκ(cid:48)) = (P1, . . . , Pκ, pK+1, . . . , pK(cid:48)) starts with Dp and ends with the
same K(cid:48) − K antecedents as d(cid:48)

0, κ(cid:48)) ∈ σ(Dp) to be the ‘analogous’ rule list, i.e., whose preﬁx D(cid:48)

p = (Q1, . . . , Qκ(cid:48)) denote the labels of D(cid:48).

p. Let ∆(cid:48)

p, Q(cid:48)

The smallest possible objective for D(cid:48), in relation to the objective of d(cid:48), reﬂects both
the diﬀerence between the objective lower bounds of D and d and the largest possible
discrepancy between the objectives of d(cid:48) and D(cid:48). The latter would occur if d(cid:48) misclassiﬁed
all the data corresponding to both ω and Ω while D(cid:48) correctly classiﬁed this same data,
thus

p, δ(cid:48)

p, q(cid:48)

R(D(cid:48), x, y) ≥ R(d(cid:48), x, y) + b(Dp, x, y) − b(dp, x, y) − ω − Ω.

(43)

Now let D∗ be an optimal rule list with preﬁx constrained to start with Dp,

and let κ∗ be the length of D∗. Also let d∗ be the analogous K∗-rule list whose preﬁx starts
with dp and ends with the same κ∗ − κ antecedents as D∗, where K∗ = K + κ∗ − κ. By (43),

D∗ ∈ argmin
D†∈σ(Dp)

R(D†, x, y),

63

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

we obtain the desired inequality in (24):

min
D†∈σ(Dp)

R(D†, x, y) = R(D∗, x, y)

≥ R(d∗, x, y) + b(Dp, x, y) − b(dp, x, y) − ω − Ω
≥ min

R(d†, x, y) + b(Dp, x, y) − b(dp, x, y) − ω − Ω.

d†∈σ(dp)

Appendix D. Proof of Theorem 20 (Equivalent Points Bound)

We derive a lower bound on the default rule misclassiﬁcation error (cid:96)0(dp, q0, x, y), analogous
to the lower bound (26) on the misclassiﬁcation error (cid:96)(d, x, y) in the proof of Proposition 19.
As before, we sum over all sets of equivalent points, and then for each such set, we count
diﬀerences between class labels and the minority class label of the set, instead of counting
mistakes made by the default rule:

(cid:96)0(dp, q0, x, y) =

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn]

1
N

1
N

1
N

N
(cid:88)

n=1
U
(cid:88)

N
(cid:88)

u=1
U
(cid:88)

n=1
N
(cid:88)

u=1

n=1

=

≥

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn] 1[xn ∈ eu]

¬ cap(xn, dp) ∧ 1[yn = qu] 1[xn ∈ eu] = b0(dp, x, y),

(44)

where the ﬁnal equality comes from the deﬁnition of b0(dp, x, y) in (28). Since we can write
the objective R(d, x, y) as the sum of the objective lower bound b(dp, x, y) and default rule
misclassiﬁcation error (cid:96)0(dp, q0, x, y), applying (44) gives a lower bound on R(d, x, y):

R(d, x, y) = (cid:96)p(dp, δp, x, y) + (cid:96)0(dp, q0, x, y) + λK = b(dp, x, y) + (cid:96)0(dp, q0, x, y)

≥ b(dp, x, y) + b0(dp, x, y).

(45)

It follows that for any rule list d(cid:48) ∈ σ(d) whose preﬁx d(cid:48)

p starts with dp, we have

R(d(cid:48), x, y) ≥ b(d(cid:48)

p, x, y) + b0(d(cid:48)

p, x, y).

(46)

Finally, we show that the lower bound on R(d, x, y) in (45) is not greater than the lower

bound on R(d(cid:48), x, y) in (46). First, let us deﬁne

Υ(d(cid:48)

p, K, x, y) ≡

cap(xn, pk | d(cid:48)

p) ∧ 1[xn ∈ eu] 1[yn = qu].

(47)

1
N

U
(cid:88)

N
(cid:88)

K(cid:48)
(cid:88)

u=1

n=1

k=K+1

64

Learning Certifiably Optimal Rule Lists

Now, we write a lower bound on b(d(cid:48)

p, x, y) with respect to b(dp, x, y):

b(d(cid:48)

p, x, y) = (cid:96)p(d(cid:48)

p, δp, x, y) + λK(cid:48) =

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] + λK(cid:48)

= (cid:96)p(dp, δp, x, y) + λK +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] + λ(K(cid:48) − K)

= b(dp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] + λ(K(cid:48) − K)

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=1

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K+1

U
(cid:88)

N
(cid:88)

K(cid:48)
(cid:88)

u=1

n=1

U
(cid:88)

N
(cid:88)

k=K+1
K(cid:48)
(cid:88)

1
N

1
N

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

1
N

(cid:32)

= b(dp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] 1[xn ∈ eu] + λ(K(cid:48) − K)

≥ b(dp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[yn = qu] 1[xn ∈ eu] + λ(K(cid:48) − K)

= b(dp, x, y) + Υ(d(cid:48)

n=1

k=K+1

u=1
p, K, x, y) + λ(K(cid:48) − K),

(48)

where the last equality uses (47). Next, we write b0(dp, x, y) with respect to b0(d(cid:48)

p, x, y),

b0(dp, x, y) =

¬ cap(xn, dp) ∧ 1[xn ∈ eu] 1[yn = qu]

=

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

¬ cap(xn, d(cid:48)

p) +

cap(xn, pk | d(cid:48)
p)

∧ 1[xn ∈ eu] 1[yn = qu]

K(cid:48)
(cid:88)

k=K+1

(cid:33)

= b0(d(cid:48)

p, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[xn ∈ eu] 1[yn = qu].

(49)

1
N

U
(cid:88)

N
(cid:88)

K(cid:48)
(cid:88)

u=1

n=1

k=K+1

Rearranging (49) gives

b0(d(cid:48)

p, x, y) = b0(dp, x, y) − Υ(d(cid:48)

p, K, x, y).

(50)

Combining (46) with ﬁrst (50) and then (48) gives the desired inequality in (27):

p, x, y)

p, x, y) + b0(d(cid:48)
p, x, y) + b0(dp, x, y) − Υ(d(cid:48)

R(d(cid:48), x, y) ≥ b(d(cid:48)
= b(d(cid:48)
≥ b(dp, x, y) + Υ(d(cid:48)
= b(dp, x, y) + b0(dp, x, y) + λ(K(cid:48) − K) ≥ b(dp, x, y) + b0(dp, x, y).

p, K, x, y) + λ(K(cid:48) − K) + b0(dp, x, y) − Υ(d(cid:48)

p, K, x, y)

p, K, x, y)

Appendix E. Data Processing Details and Antecedent Mining

In this appendix, we provide details regarding datasets used in our experiments (Section 6).

65

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

E.1 ProPublica Recidivism Data Set

Table 7 shows the 6 attributes and corresponding 17 categorical values that we use for the
ProPublica data set. From these, we construct 17 single-clause antecedents, for example,
(age = 23 − 25). We then combine pairs of these antecedents as conjunctions to form two-
clause antecedents, e.g., (age = 23 − 25) ∧ (priors = 2 − 3). By virtue of our lower bound
on antecedent support, (Theorem 10, §3.7), we eliminate antecedents with support less
than 0.005 or greater than 0.995, since λ = 0.005 is the smallest regularization parameter
value we study for this problem. With this ﬁltering step, we generate between 121 and 123
antecedents for each fold; without it, we would instead generate about 130 antecedents as
input to our algorithm.

Note that we exclude the ‘current charge’ attribute (which has two categorical values,
‘misdemeanor’ and ‘felony’); for individuals in the data set booked on multiple charges, this
attribute does not appear to consistently reﬂect the most serious charge.

Feature
sex
age
juvenile felonies
juvenile misdemeanors
juvenile crimes
priors

Value range
—
18-96
0-20
0-13
0-21
0-38

Categorical values
male, female
18-20, 21-22, 23-25, 26-45, >45
0, >0
0, >0
0, >0
0, 1, 2-3, >3

Count
2
5
2
2
2
4

Table 7: Categorical features (6 attributes, 17 values) from the ProPublica data set. We
construct the feature juvenile crimes from the sum of juvenile felonies, juvenile
misdemeanors, and the number of juvenile crimes that were neither felonies nor
misdemeanors (not shown).

E.2 NYPD Stop-and-frisk Data Set

This data set is larger than, but similar to the NYCLU stop-and-frisk data set, described
next.

E.3 NYCLU Stop-and-frisk Data Set

The original data set contains 45,787 records, each describing an incident involving a stopped
person; the individual was frisked in 30,345 (66.3%) of records and and searched in 7,283
(15.9%). In 30,961 records, the individual was frisked and/or searched (67.6%); of those,
a criminal possession of a weapon was identiﬁed 1,445 times (4.7% of these records). We
remove 1,929 records with missing data, as well as a small number with extreme values
for the individual’s age—we eliminate those with age < 12 or > 89. This yields a set of
29,595 records in which the individual was frisked and/or searched. To address the class
imbalance for this problem, we sample records from the smaller class with replacement. We
generate cross-validation folds ﬁrst, and then resample within each fold. In our 10-fold cross-
validation experiments, each training set contains 50,743 observations. Table 8 shows the 5

66

Learning Certifiably Optimal Rule Lists

categorical attributes that we use, corresponding to a total of 28 values. Our experiments
use these antecedents, as well as negations of the 18 antecedents corresponding to the two
features stop reason and additional circumstances, which gives a total of 46 antecedents.

Feature
stop reason

additional
circumstances

city
location

inside or outside

Values
suspicious object, ﬁts description, casing,
acting as lookout, suspicious clothing,
drug transaction, furtive movements,
actions of violent crime, suspicious bulge
proximity to crime scene, evasive response,
associating with criminals, changed direction,
high crime area, time of day,
sights and sounds of criminal activity,
witness report, ongoing investigation
Queens, Manhattan, Brooklyn, Staten Island, Bronx
housing authority, transit authority,
neither housing nor transit authority
inside, outside

Count
9

9

5
3

2

Table 8: Categorical features (5 attributes, 28 values) from the NYCLU data set.

Appendix F. Example Optimal Rule Lists, for Diﬀerent Values of λ

For each of our prediction problems, we provide listings of optimal rule lists found by
CORELS, across 10 cross-validation folds, for diﬀerent values of the regularization param-
eter λ. These rule lists correspond to the results for CORELS summarized in Figures 11
and 12 (§6.6). Recall that as λ decreases, optimal rule lists tend to grow in length.

F.1 ProPublica Recidivism Data Set

We show example optimal rule lists that predict two-year recidivism. Figure 19 shows exam-
ples for regularization parameters λ = 0.02 and 0.01. Figure 20 shows examples for λ = 0.005;
Figure 4 (§6.3) showed two representative examples.

For the largest regularization parameter λ = 0.02 (Figure 19), we observe that all folds
identify the same length-1 rule list. For the intermediate value λ = 0.01 (Figure 19), the
folds identify optimal 2-rule or 3-rule lists that contain the nearly same preﬁx rules, up to
permutations. For the smallest value λ = 0.005 (Figure 20), the folds identify optimal 3-rule
or 4-rule lists that contain the nearly same preﬁx rules, up to permutations. Across all three
regularization parameter values and all folds, the preﬁx rules always predict the positive
class label, and the default rule always predicts the negative class label. We note that our
objective is not designed to enforce any of these properties.

67

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Two-year recidivism prediction (λ = 0.02)

if (priors > 3) then predict yes
else predict no

Two-year recidivism prediction (λ = 0.01)

(cid:46) Found by all 10 folds

if (priors > 3) then predict yes
else if (sex = male) and (juvenile crimes > 0) then predict yes
else predict no

(cid:46) Found by 3 folds

if (sex = male) and (juvenile crimes > 0) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (priors > 3) then predict yes
else predict no

if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

(cid:46) Found by 2 folds

(cid:46) Found by 2 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

Figure 19: Example optimal rule lists for the ProPublica data set, found by CORELS with
regularization parameters λ = 0.02 (top), and 0.01 (bottom) across 10 cross-
validation folds.

68

Learning Certifiably Optimal Rule Lists

Two-year recidivism prediction (λ = 0.005)

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (priors > 3) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else predict no

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

(cid:46) Found by 4 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

Figure 20: Example optimal rule lists for the ProPublica data set, found by CORELS with
regularization parameters λ = 0.005, across 10 cross-validation folds.

69

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

F.2 NYPD Stop-and-frisk Data Set

We show example optimal rule lists that predict whether a weapon will be found on a
stopped individual who is frisked or searched, learned from the NYPD data set.

Weapon prediction (λ = 0.01, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else predict no

if (location = transit authority) then predict yes
else if (stop reason = suspicious object) then predict yes
else predict no

Weapon prediction (λ = 0.005, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (location = housing authority) then predict no
else if (location = transit authority) then predict yes
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else if (location = transit authority) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (city = Bronx) then predict no
else if (location = housing authority) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

(cid:46) Found by 8 folds

(cid:46) Found by 2 folds

(cid:46) Found by 7 folds

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

Figure 21: Example optimal rule lists for the NYPD stop-and-frisk data set, found by
CORELS with regularization parameters λ = 0.01 (top) and 0.005 (bottom),
across 10 cross-validation folds.

70

Learning Certifiably Optimal Rule Lists

Weapon prediction (λ = 0.01, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (inside or outside = outside) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (inside or outside = inside) then predict yes
else predict no

Weapon prediction (λ = 0.005, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = f urtive movements) then predict no
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f its description) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f urtive movements) then predict no
else if (stop reason = f its description) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = f its description) then predict no
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = f urtive movements) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = acting as lookout) then predict no
else predict yes

(cid:46) Found by 7 folds

(cid:46) Found by 3 folds

(cid:46) Found by 2 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

Figure 22: Example optimal rule lists for the NYPD stop-and-frisk data set (Feature Set D)
found by CORELS with regularization parameters λ = 0.01 (top) and 0.005 (bot-
tom), across 10 cross-validation folds. For λ = 0.005, we show results from 7 folds;
the remaining 3 folds were equivalent, up to a permutation of the preﬁx rules,
and started with the same ﬁrst preﬁx rule.

71

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

F.3 NYCLU Stop-and-frisk Data Set

We show example optimal rule lists that predict whether a weapon will be found on a
stopped individual who is frisked or searched, learned from the NYCLU data set. Figure 23
shows regularization parameters λ = 0.04 and 0.01, and Figure 24 shows λ = 0.0025. We
showed a representative solution for λ = 0.01 in Figure 5 (§6.3).

For each of the two larger regularization parameters in Figure 23, λ = 0.04 (top) and
0.01 (bottom), we observe that across the folds, all the optimal rule lists contain the same or
equivalent rules, up to a permutation. With the smaller regularization parameter λ = 0.0025
(Figure 24), we observe a greater diversity of longer optimal rule lists, though they share
similar structure.

Weapon prediction (λ = 0.04)

if (stop reason = suspicious object) then predict yes
else if (stop reason (cid:54)= suspicious bulge) then predict no
else predict yes

if (stop reason = suspicious bulge) then predict yes
else if (stop reason (cid:54)= suspicious object) then predict no
else predict yes

Weapon prediction (λ = 0.01)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (stop reason (cid:54)= suspicious bulge) then predict no
else predict yes

if (location = transit authority) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = suspicious object) then predict yes
else predict no

if (location = transit authority) then predict yes
else if (stop reason = suspicious object) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else predict no

if (location = transit authority) then predict yes
else if (stop reason = suspicious object) then predict yes
else if (stop reason (cid:54)= suspicious bulge) then predict no
else predict yes

(cid:46) Found by 7 folds

(cid:46) Found by 3 folds

(cid:46) Found by 4 folds

(cid:46) Found by 3 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

Figure 23: Example optimal rule lists for the NYCLU stop-and-frisk data set, found by
CORELS with regularization parameters λ = 0.04 (top) and 0.01 (bottom),
across 10 cross-validation folds.

72

Learning Certifiably Optimal Rule Lists

Weapon prediction (λ = 0.0025)

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = transit authority) then predict yes
else if (inside or outside = inside) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (location = housing authority) then predict no
else if (stop reason = casing) then predict no
else if (stop reason = f its description) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = drug transaction) then predict no
else if (stop reason = suspicious object) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (location = housing authority) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = casing) then predict no
else if (city = M anhattan) then predict yes
else if (city = Bronx) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = transit authority) then predict yes
else if (inside or outside = inside) then predict no
else if (city = M anhattan) then predict yes
else if (additional circumstances = changed direction) then predict no
else if (city = Bronx) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = actions of violent crime) then predict no
else if (stop reason = f its description) then predict no
else if (location = transit authority) then predict yes
else if (inside or outside = inside) then predict no
else if (city = M anhattan) then predict yes
else if (additional circumstances = evasive response) then predict no
else if (city = Bronx) then predict yes
else predict no

(cid:46) Found by 4 folds (K = 7)

(cid:46) Found by 1 fold (K = 6)

(cid:46) Found by 1 fold (K = 6)

(cid:46) Found by 1 fold (K = 6)

(cid:46) Found by 1 fold (K = 8)

(cid:46) Found by 1 fold (K = 9)

(cid:46) Found by 1 fold (K = 10)

Figure 24: Example optimal rule lists for the NYCLU stop-and-frisk data set λ = 0.0025.

73

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Appendix G. Additional Results on Predictive Performance and Model

Size for CORELS and Other Algorithms

In this appendix, we plot TPR, FPR, and model size for CORELS and three other algo-
rithms, using the NYPD data set (Feature Set D).

Figure 25: TPR (top) and FPR (bottom) for the test set, as a function of model size,
across diﬀerent methods, for weapon prediction with the NYPD stop-and-frisk
data set (Feature Set D). In the legend, numbers in parentheses are algorithm
parameters, as in Figure 12. Legend markers and error bars indicate means and
standard deviations, respectively, across cross-validation folds. C4.5 ﬁnds large
models for all tested parameters.

References

E. Angelino, N. Larus-Stone, D. Alabi, M. Seltzer, and C. Rudin. Learning certiﬁably
optimal rule lists for categorical data. In ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD), 2017.

K. P. Bennett and J. A. Blue. Optimal decision trees. Technical report, R.P.I. Math Report

No. 214, Rensselaer Polytechnic Institute, 1996.

I. Bratko. Machine learning: Between accuracy and interpretability. In Learning, Networks
and Statistics, volume 382 of International Centre for Mechanical Sciences, pages 163–

74

Learning Certifiably Optimal Rule Lists

177. Springer Vienna, 1997.

Trees. Wadsworth, 1984.

2013.

L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and Regression

S. Bushway. Is there any logic to using logit. Criminology & Public Policy, 12(3):563–567,

C. Chen and C. Rudin. An optimization approach to learning falling rule lists. In Interna-

tional Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2018.

H. A. Chipman, E. I. George, and R. E. McCulloch. Bayesian CART model search. Journal

of the American Statistical Association, 93(443):935–948, 1998.

H. A. Chipman, E. I. George, and R. E. McCulloch. Bayesian treed models. Machine

Learning, 48(1):299–320, 2002.

H. A. Chipman, E. I. George, and R. E. McCulloch. BART: Bayesian additive regression

trees. The Annals of Applied Statistics, 4(1):266–298, 2010.

P. Clark and T. Niblett. The CN2 induction algorithm. Machine Learning, 3:261–283, 1989.

W. W. Cohen. Fast eﬀective rule induction.

In International Conference on Machine

Learning (ICML), pages 115–123, 1995.

R. M. Dawes. The robust beauty of improper linear models in decision making. American

Psychologist, 34(7):571–582, 1979.

D. Dension, B. Mallick, and A.F.M. Smith. A Bayesian CART algorithm. Biometrika, 85

(2):363–377, 1998.

trees, 1996.

Advances, 4(1), 2018.

D. Dobkin, T. Fulton, D. Gunopulos, S. Kasif, and S. Salzberg. Induction of shallow decision

J. Dressel and H. Farid. The accuracy, fairness, and limits of predicting recidivism. Science

A. Farhangfar, R. Greiner, and M. Zinkevich. A fast way to produce optimal ﬁxed-depth
decision trees. In International Symposium on Artiﬁcial Intelligence and Mathematics
(ISAIM), 2008.

E. Frank and I. H. Witten. Generating accurate rule sets without global optimization. In

International Conference on Machine Learning (ICML), pages 144–151, 1998.

A. A. Freitas. Comprehensible classiﬁcation models: A position paper. ACM SIGKDD

Explorations Newsletter, 15(1):1–10, 2014.

M. Garofalakis, D. Hyun, R. Rastogi, and K. Shim. Eﬃcient algorithms for constructing
decision trees with constraints. In ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD), pages 335–339, 2000.

75

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

C. Giraud-Carrier. Beyond predictive accuracy: What? In ECML-98 Workshop on Up-
grading Learning to Meta-Level: Model Selection and Data Transformation, pages 78–85,
1998.

S. Goel, J. M. Rao, and R. Shroﬀ. Precinct or prejudice? Understanding racial disparities in
New York City’s stop-and-frisk policy. The Annals of Applied Statistics, 10(1):365–394,
03 2016.

S. T. Goh and C. Rudin. Box drawings for learning with imbalanced data. In ACM SIGKDD

International Conference on Knowledge Discovery and Data Mining (KDD), 2014.

S. T. Goh and C. Rudin. A minimax surrogate loss approach to conditional diﬀerence
estimation. CoRR, abs/1803.03769, 2018. URL https://arxiv.org/abs/1803.03769.

B. Goodman and S. Flaxman. European Union regulations on algorithmic decision-making
and a “right to explanation”. In ICML Workshop on Human Interpretability in Machine
Learning (WHI), 2016.

R. C. Holte. Very simple classiﬁcation rules perform well on most commonly used datasets.

Machine Learning, 11(1):63–91, 1993.

J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, and B. Baesens. An empirical evaluation
of the comprehensibility of decision table, tree and rule based predictive models. Decision
Support Systems, 51(1):141–154, 2011.

V. Kaxiras and A. Saligrama. Building predictive models with rule lists, 2018. URL https:

//corels.eecs.harvard.edu.

H. Lakkaraju and C. Rudin. Cost-sensitive and interpretable dynamic treatment regimes
based on rule lists. In International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS), 2017.

J. Larson, S. Mattu, L. Kirchner, and J. Angwin. How we analyzed the COMPAS recidivism

algorithm. ProPublica, 2016.

N. Larus-Stone, E. Angelino, D. Alabi, M. Seltzer, V. Kaxiras, A. Saligrama, and C. Rudin.
Systems optimizations for learning certiﬁably optimal rule lists. In SysML Conference,
2018.

N. L. Larus-Stone. Learning Certiﬁably Optimal Rule Lists: A Case For Discrete Optimiza-

tion in the 21st Century. 2017. Undergraduate thesis, Harvard College.

B. Letham, C. Rudin, T. H. McCormick, and D. Madigan. Interpretable classiﬁers using
rules and Bayesian analysis: Building a better stroke prediction model. The Annals of
Applied Statistics, 9(3):1350–1371, 2015.

O. Li, H. Liu, C. Chen, and C. Rudin. Deep learning for case-based reasoning through pro-
totypes: A neural network that explains its predictions. In Proceedings of the Association
for the Advancement of Artiﬁcial Intelligence (AAAI), 2018.

76

Learning Certifiably Optimal Rule Lists

W. Li, J. Han, and J. Pei. CMAR: Accurate and eﬃcient classiﬁcation based on multiple
class-association rules. IEEE International Conference on Data Mining (ICDM), pages
369–376, 2001.

J. T. Linderoth and M. W. P. Savelsbergh. A computational study of search strategies for
mixed integer programming. INFORMS Journal on Computing, 11(2):173–187, 1999.

B. Liu, W. Hsu, and Y. Ma. Integrating classiﬁcation and association rule mining. In ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),
pages 80–96, 1998.

M. Marchand and M. Sokolova. Learning with decision lists of data-dependent features.

Journal of Machine Learning Research, 6:427–451, 2005.

R. S. Michalski. On the quasi-minimal solution of the general covering problem. In Inter-

national Symposium on Information Processing, pages 125–128, 1969.

New York Civil Liberties Union. Stop-and-frisk data, 2014. URL http://www.nyclu.org/

content/stop-and-frisk-data.

New York Police Department. Stop, question and frisk data, 2016. URL http://www1.

nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page.

S. Nijssen and E. Fromont. Optimal constraint-based decision tree induction from itemset

lattices. Data Mining and Knowledge Discovery, 21(1):9–51, 2010.

J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1993.

P. R. Rijnbeek and J. A. Kors. Finding a short and accurate decision rule in disjunctive

normal form by exhaustive search. Machine Learning, 80(1):33–62, July 2010.

R. L. Rivest. Learning decision lists. Machine Learning, 2(3):229–246, November 1987.

U. R¨uckert and L. De Raedt. An experimental evaluation of simplicity in rule learning.

Artiﬁcial Intelligence, 172:19–28, 2008.

C. Rudin and S¸. Ertekin. Learning customized and optimized lists of rules with mathemat-

ical programming. Submitted, 2016.

C. Rudin, B. Letham, and D. Madigan. Learning theory analysis for association rules and
sequential event prediction. Journal of Machine Learning Research, 14:3384–3436, 2013.

S. R¨uping. Learning interpretable models. PhD thesis, Universit¨at Dortmund, 2006.

G. Shmueli. To explain or to predict? Statistical Science, 25(3):289–310, August 2010.

M. Sokolova, M. Marchand, N. Japkowicz, and J. Shawe-Taylor. The decision list machine.
In Advances in Neural Information Processing Systems (NIPS), volume 15, pages 921–
928, 2003.

77

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

N. Tollenaar and P. van der Heijden. Which method predicts recidivism best?: A comparison
of statistical, machine learning and data mining predictive models. Journal of the Royal
Statistical Society: Series A (Statistics in Society), 176(2):565–584, 2013.

B. Ustun and C. Rudin. Supersparse linear integer models for optimized medical scoring

systems. Machine Learning, 102(3):349–391, 2016.

B. Ustun and C. Rudin. Optimized risk scores. In ACM SIGKDD International Conference

on Knowledge Discovery and Data Mining (KDD), 2017.

K. Vanhoof and B. Depaire. Structure of association rule classiﬁers: A review. In Inter-
national Conference on Intelligent Systems and Knowledge Engineering (ISKE), pages
9–12, 2010.

A. Vellido, J. D. Mart´ın-Guerrero, and P. J.G. Lisboa. Making machine learning models
In European Symposium on Artiﬁcial Neural Networks, Computational

interpretable.
Intelligence and Machine Learning (ESANN), 2012.

F. Wang and C. Rudin. Falling rule lists. In International Conference on Artiﬁcial Intelli-

gence and Statistics (AISTATS), 2015a.

F. Wang and C. Rudin. Causal falling rule lists. CoRR, abs/1510.05189, 2015b. URL

https://arxiv.org/abs/1510.05189.

T. Wang. Hybrid decision making: When interpretable models collaborate with black-box

models. CoRR, abs/1802.04346, 2018. URL http://arxiv.org/abs/1802.04346.

T. Wang, C. Rudin, F. Doshi-Velez, Y. Liu, E. Klampﬂ, and P. MacNeille. Bayesian or’s
of and’s for interpretable classiﬁcation with application to context aware recommender
systems. In International Conference on Data Mining (ICDM), 2016.

T. Wang, C. Rudin, F. Doshi-Velez, Y. Liu, E. Klampﬂ, and P. MacNeille. A Bayesian frame-
work for learning rule sets for interpretable classiﬁcation. Journal of Machine Learning
Research, 18(70):1–37, 2017.

E. Westervelt.

man’s murder?,
did-a-bail-reform-algorithm-contribute-to-this-san-francisco-man-s-murder.

reform algorithm contribute to this San Francisco
URL https://www.npr.org/2017/08/18/543976003/

Did a bail
2017.

H. Yang, C. Rudin, and M. Seltzer. Scalable Bayesian rule lists. In International Conference

on Machine Learning (ICML), 2017.

X. Yin and J. Han. CPAR: Classiﬁcation based on predictive association rules. In SIAM

International Conference on Data Mining (SDM), pages 331–335, 2003.

J. Zeng, B. Ustun, and C. Rudin. Interpretable classiﬁcation models for recidivism pre-
diction. Journal of the Royal Statistical Society: Series A (Statistics in Society), 180(3):
689–722, 2017.

Y. Zhang, E. B. Laber, A. Tsiatis, and M. Davidian. Using decision lists to construct
interpretable and parsimonious treatment regimes. Biometrics, 71(4):895–904, 2015.

78

8
1
0
2
 
g
u
A
 
3
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
0
7
1
0
.
4
0
7
1
:
v
i
X
r
a

Journal of Machine Learning Research 18 (2018) 1-78

Submitted 11/17; Published 6/18

Learning Certiﬁably Optimal Rule Lists for Categorical Data

Elaine Angelino
Department of Electrical Engineering and Computer Sciences
University of California, Berkeley, Berkeley, CA 94720

elaine@eecs.berkeley.edu

Nicholas Larus-Stone
Daniel Alabi
Margo Seltzer
School of Engineering and Applied Sciences
Harvard University, Cambridge, MA 02138

nlarusstone@alumni.harvard.edu
alabid@g.harvard.edu
margo@eecs.harvard.edu

Cynthia Rudin∗
Department of Computer Science and Department of Electrical and Computer Engineering
Duke University, Durham, NC 27708

cynthia@cs.duke.edu

Editor: Maya Gupta
∗To whom correspondence should be addressed.

Abstract
We present the design and implementation of a custom discrete optimization technique for
building rule lists over a categorical feature space. Our algorithm produces rule lists with
optimal training performance, according to the regularized empirical risk, with a certiﬁcate
of optimality. By leveraging algorithmic bounds, eﬃcient data structures, and computa-
tional reuse, we achieve several orders of magnitude speedup in time and a massive reduc-
tion of memory consumption. We demonstrate that our approach produces optimal rule
lists on practical problems in seconds. Our results indicate that it is possible to construct
optimal sparse rule lists that are approximately as accurate as the COMPAS proprietary
risk prediction tool on data from Broward County, Florida, but that are completely inter-
pretable. This framework is a novel alternative to CART and other decision tree methods
for interpretable modeling.
Keywords: rule lists, decision trees, optimization, interpretable models, criminal justice
applications

1. Introduction

As machine learning continues to gain prominence in socially-important decision-making,
the interpretability of predictive models remains a crucial problem. Our goal is to build
models that are highly predictive, transparent, and easily understood by humans. We use
rule lists, also known as decision lists, to achieve this goal. Rule lists are predictive models
composed of if-then statements; these models are interpretable because the rules provide a
reason for each prediction (Figure 1).

Constructing rule lists, or more generally, decision trees, has been a challenge for more
than 30 years; most approaches use greedy splitting techniques (Rivest, 1987; Breiman
et al., 1984; Quinlan, 1993). Recent approaches use Bayesian analysis, either to ﬁnd a locally
optimal solution (Chipman et al., 1998) or to explore the search space (Letham et al., 2015;

c(cid:13)2018 Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v18/17-716.html.

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 23) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

Figure 1: An example rule list that predicts two-year recidivism for the ProPublica data

set, found by CORELS.

Yang et al., 2017). These approaches achieve high accuracy while also managing to run
reasonably quickly. However, despite the apparent accuracy of the rule lists generated by
these algorithms, there is no way to determine either if the generated rule list is optimal
or how close it is to optimal, where optimality is deﬁned with respect to minimization of a
regularized loss function.

Optimality is important, because there are societal implications for a lack of optimality.
Consider the ProPublica article on the Correctional Oﬀender Management Proﬁling for Al-
ternative Sanctions (COMPAS) recidivism prediction tool (Larson et al., 2016). It highlights
a case where a black box, proprietary predictive model is being used for recidivism predic-
tion. The authors hypothesize that the COMPAS scores are racially biased, but since the
model is not transparent, no one (outside of the creators of COMPAS) can determine the
reason or extent of the bias (Larson et al., 2016), nor can anyone determine the reason for
any particular prediction. By using COMPAS, users implicitly assumed that a transparent
model would not be suﬃciently accurate for recidivism prediction, i.e., they assumed that
a black box model would provide better accuracy. We wondered whether there was indeed
no transparent and suﬃciently accurate model. Answering this question requires solving a
computationally hard problem. Namely, we would like to both ﬁnd a transparent model that
is optimal within a particular pre-determined class of models and produce a certiﬁcate of
its optimality, with respect to the regularized empirical risk. This would enable one to say,
for this problem and model class, with certainty and before resorting to black box methods,
whether there exists a transparent model. While there may be diﬀerences between train-
ing and test performance, ﬁnding the simplest model with optimal training performance is
prescribed by statistical learning theory.

To that end, we consider the class of rule lists assembled from pre-mined frequent item-
sets and search for an optimal rule list that minimizes a regularized risk function, R. This
is a hard discrete optimization problem. Brute force solutions that minimize R are compu-
tationally prohibitive due to the exponential number of possible rule lists. However, this is
a worst case bound that is not realized in practical settings. For realistic cases, it is possible
to solve fairly large cases of this problem to optimality, with the careful use of algorithms,
data structures, and implementation techniques.

We develop specialized tools from the ﬁelds of discrete optimization and artiﬁcial intel-
ligence. Speciﬁcally, we introduce a special branch-and bound algorithm, called Certiﬁably
Optimal RulE ListS (CORELS), that provides the optimal solution according to the train-
ing objective, along with a certiﬁcate of optimality. The certiﬁcate of optimality means that
we can investigate how close other models (e.g., models provided by greedy algorithms) are
to optimal.

2

Learning Certifiably Optimal Rule Lists

Within its branch-and-bound procedure, CORELS maintains a lower bound on the
minimum value of R that each incomplete rule list can achieve. This allows CORELS to
prune an incomplete rule list (and every possible extension) if the bound is larger than
the error of the best rule list that it has already evaluated. The use of careful bounding
techniques leads to massive pruning of the search space of potential rule lists. The algorithm
continues to consider incomplete and complete rule lists until it has either examined or
eliminated every rule list from consideration. Thus, CORELS terminates with the optimal
rule list and a certiﬁcate of optimality.

The eﬃciency of CORELS depends on how much of the search space our bounds allow us
to prune; we seek a tight lower bound on R. The bound we maintain throughout execution is
a maximum of several bounds that come in three categories. The ﬁrst category of bounds are
those intrinsic to the rules themselves. This category includes bounds stating that each rule
must capture suﬃcient data; if not, the rule list is provably non-optimal. The second type of
bound compares a lower bound on the value of R to that of the current best solution. This
allows us to exclude parts of the search space that could never be better than our current
solution. Finally, our last type of bound is based on comparing incomplete rule lists that
capture the same data and allows us to pursue only the most accurate option. This last
class of bounds is especially important—without our use of a novel symmetry-aware map,
we are unable to solve most problems of reasonable scale. This symmetry-aware map keeps
track of the best accuracy over all observed permutations of a given incomplete rule list.

We keep track of these bounds using a modiﬁed preﬁx tree, a data structure also known
as a trie. Each node in the preﬁx tree represents an individual rule; thus, each path in the
tree represents a rule list such that the ﬁnal node in the path contains metrics about that
rule list. This tree structure, together with a search policy and sometimes a queue, enables a
variety of strategies, including breadth-ﬁrst, best-ﬁrst, and stochastic search. In particular,
we can design diﬀerent best-ﬁrst strategies by customizing how we order elements in a
priority queue. In addition, we are able to limit the number of nodes in the trie and thereby
enable tuning of space-time tradeoﬀs in a robust manner. This trie structure is a useful way
of organizing the generation and evaluation of rule lists.

We evaluated CORELS on a number of publicly available data sets. Our metric of
success was 10-fold cross-validated prediction accuracy on a subset of the data. These data
sets involve hundreds of rules and thousands of observations. CORELS is generally able to
ﬁnd an optimal rule list in a matter of seconds and certify its optimality within about 10
minutes. We show that we are able to achieve better or similar out-of-sample accuracy on
these data sets compared to the popular greedy algorithms, CART and C4.5.

CORELS targets large (not massive) problems, where interpretability and certiﬁable
optimality are important. We illustrate the eﬃcacy of our approach using (1) the ProPublica
COMPAS data set (Larson et al., 2016), for the problem of two-year recidivism prediction,
and (2) stop-and-frisk data sets from the NYPD (New York Police Department, 2016) and
the NYCLU (New York Civil Liberties Union, 2014), to predict whether a weapon will
be found on a stopped individual who is frisked or searched. On these data, we produce
certiﬁably optimal, interpretable rule lists that achieve the same accuracy as approaches
such as random forests. This calls into question the need for use of a proprietary, black box
algorithm for recidivism prediction.

3

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Our work overlaps with the thesis of Larus-Stone (2017). We have also written a pre-
liminary conference version of this article (Angelino et al., 2017), and a report highlighting
systems optimizations of our implementation (Larus-Stone et al., 2018); the latter includes
additional empirical measurements not presented here.

Our code is at https://github.com/nlarusstone/corels, where we provide the C++
implementation we used in our experiments (§6). Kaxiras and Saligrama (2018) have also
created an interactive web interface at https://corels.eecs.harvard.edu, where a user
can upload data and run CORELS from a browser.

2. Related Work

Since every rule list is a decision tree and every decision tree can be expressed as an
equivalent rule list, the problem we are solving is a version of the “optimal decision tree”
problem, though regularization changes the nature of the problem (as shown through our
bounds). The optimal decision tree problem is computationally hard, though since the late
1990’s, there has been research on building optimal decision trees using optimization tech-
niques (Bennett and Blue, 1996; Dobkin et al., 1996; Farhangfar et al., 2008). A particularly
interesting paper along these lines is that of Nijssen and Fromont (2010), who created a
“bottom-up” way to form optimal decision trees. Their method performs an expensive search
step, mining all possible leaves (rather than all possible rules), and uses those leaves to form
trees. Their method can lead to memory problems, but it is possible that these memory
issues can be mitigated using the theorems in this paper.1 None of these methods used the
tight bounds and data structures of CORELS.

Because the optimal decision tree problem is hard, there are a huge number of algo-
rithms such as CART (Breiman et al., 1984) and C4.5 (Quinlan, 1993) that do not perform
exploration of the search space beyond greedy splitting. Similarly, there are decision list
and associative classiﬁcation methods that construct rule lists iteratively in a greedy way
(Rivest, 1987; Liu et al., 1998; Li et al., 2001; Yin and Han, 2003; Sokolova et al., 2003;
Marchand and Sokolova, 2005; Vanhoof and Depaire, 2010; Rudin et al., 2013). Some ex-
ploration of the search space is done by Bayesian decision tree methods (Dension et al.,
1998; Chipman et al., 2002, 2010) and Bayesian rule-based methods (Letham et al., 2015;
Yang et al., 2017). The space of trees of a given depth is much larger than the space of rule
lists of that same depth, and the trees within the Bayesian tree algorithms are grown in a
top-down greedy way. Because of this, authors of Bayesian tree algorithms have noted that
their MCMC chains tend to reach only locally optimal solutions. The RIPPER algorithm
(Cohen, 1995) is similar to the Bayesian tree methods in that it grows, prunes, and then
locally optimizes. The space of rule lists is smaller than that of trees, and has simpler struc-
ture. Consequently, Bayesian rule list algorithms tend to be more successful at escaping
local minima and can introduce methods of exploring the search space that exploit this
structure—these properties motivate our focus on lists. That said, the tightest bounds for
the Bayesian lists (namely, those of Yang et al., 2017, upon whose work we build), are not
nearly as tight as those of CORELS.

1. There is no public version of their code for distribution as of this writing.

4

Learning Certifiably Optimal Rule Lists

Tight bounds, on the other hand, have been developed for the (immense) literature on
building disjunctive normal form (DNF) models; a good example of this is the work of Rijn-
beek and Kors (2010). For models of a given size, since the class of DNF’s is a proper subset
of decision lists, our framework can be restricted to learn optimal DNF’s. The ﬁeld of DNF
learning includes work from the ﬁelds of rule learning/induction (e.g., early algorithms by
Michalski, 1969; Clark and Niblett, 1989; Frank and Witten, 1998) and associative classiﬁ-
cation (Vanhoof and Depaire, 2010). Most papers in these ﬁelds aim to carefully guide the
search through the space of models. If we were to place a restriction on our code to learn
DNF’s, which would require restricting predictions within the list to the positive class only,
we could potentially use methods from rule learning and associative classiﬁcation to help
order CORELS’ queue, which would in turn help us eliminate parts of the search space
more quickly.

Some of our bounds, including the minimum support bound (§3.7, Theorem 10), come
from Rudin and Ertekin (2016), who provide ﬂexible mixed-integer programming (MIP)
formulations using the same objective as we use here; MIP solvers in general cannot compete
with the speed of CORELS.

CORELS depends on pre-mined rules, which we obtain here via enumeration. The litera-
ture on association rule mining is huge, and any method for rule mining could be reasonably
substituted.

CORELS’ main use is for producing interpretable predictive models. There is a grow-
ing interest in interpretable (transparent, comprehensible) models because of their societal
importance (see R¨uping, 2006; Bratko, 1997; Dawes, 1979; Vellido et al., 2012; Giraud-
Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans et al., 2011; Freitas, 2014). There are
now regulations on algorithmic decision-making in the European Union on the “right to an
explanation” (Goodman and Flaxman, 2016) that would legally require interpretability of
predictions. There is work in both the DNF literature (R¨uckert and Raedt, 2008) and deci-
sion tree literature (Garofalakis et al., 2000) on building interpretable models. Interpretable
models must be so sparse that they need to be heavily optimized; heuristics tend to produce
either inaccurate or non-sparse models.

Interpretability has many meanings, and it is possible to extend the ideas in this work
to other deﬁnitions of interpretability; these rule lists may have exotic constraints that help
with ease-of-use. For example, Falling Rule Lists (Wang and Rudin, 2015a) are constrained
to have decreasing probabilities down the list, which makes it easier to assess whether an
observation is in a high risk subgroup. In parallel to this paper, we have been working on
an algorithm for Falling Rule Lists (Chen and Rudin, 2018) with bounds similar to those
presented here, but even CORELS’ basic support bounds do not hold for the falling case,
which is much more complicated. One advantage of the approach taken by Chen and Rudin
(2018) is that it can handle class imbalance by weighting the positive and negative classes
diﬀerently; this extension is possible in CORELS but not addressed here.

The models produced by CORELS are predictive only; they cannot be used for policy-
making because they are not causal models, they do not include the costs of true and false
positives, nor the cost of gathering information. It is possible to adapt CORELS’ frame-
work for causal inference (Wang and Rudin, 2015b), dynamic treatment regimes (Zhang
et al., 2015), or cost-sensitive dynamic treatment regimes (Lakkaraju and Rudin, 2017) to

5

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 23) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if p1 then predict q1
else if p2 then predict q2
else if p3 then predict q3
else predict q0

Figure 2: The rule list d = (r1, r2, r3, r0). Each rule is of the form rk = pk → qk, for all
k = 0, . . . , 3. We can also express this rule list as d = (dp, δp, q0, K), where
dp = (p1, p2, p3), δp = (1, 1, 1, 1), q0 = 0, and K = 3. This is the same 3-rule list
as in Figure 1, that predicts two-year recidivism for the ProPublica data set.

help with policy design. CORELS could potentially be adapted to handle these kinds of
interesting problems.

3. Learning Optimal Rule Lists

In this section, we present our framework for learning certiﬁably optimal rule lists. First, we
deﬁne our setting and useful notation (§3.1) and then the objective function we seek to min-
imize (§3.2). Next, we describe the principal structure of our optimization algorithm (§3.3),
which depends on a hierarchically structured objective lower bound (§3.4). We then derive
a series of additional bounds that we incorporate into our algorithm, because they enable
aggressive pruning of our state space.

3.1 Notation

We restrict our setting to binary classiﬁcation, where rule lists are Boolean functions; this
framework is straightforward to generalize to multi-class classiﬁcation. Let {(xn, yn)}N
n=1
denote training data, where xn ∈ {0, 1}J are binary features and yn ∈ {0, 1} are labels.
Let x = {xn}N

n=1, and let xn,j denote the j-th feature of xn.

n=1 and y = {yn}N

A rule list d = (r1, r2, . . . , rK, r0) of length K ≥ 0 is a (K + 1)-tuple consisting of K
distinct association rules, rk = pk → qk, for k = 1, . . . , K, followed by a default rule r0.
Figure 2 illustrates a rule list, d = (r1, r2, r3, r0), which for clarity, we sometimes call a K-
rule list. An association rule r = p → q is an implication corresponding to the conditional
statement, “if p, then q.” In our setting, an antecedent p is a Boolean assertion that evaluates
to either true or false for each datum xn, and a consequent q is a label prediction. For
example, (xn,1 = 0) ∧ (xn,3 = 1) → (yn = 1) is an association rule. The ﬁnal default rule r0
in a rule list can be thought of as a special association rule p0 → q0 whose antecedent p0
simply asserts true.

Let d = (r1, r2, . . . , rK, r0) be a K-rule list, where rk = pk → qk for each k = 0, . . . , K.
We introduce a useful alternate rule list representation: d = (dp, δp, q0, K), where we deﬁne
dp = (p1, . . . , pK) to be d’s preﬁx, δp = (q1, . . . , qK) ∈ {0, 1}K gives the label predictions
associated with dp, and q0 ∈ {0, 1} is the default label prediction. For example, for the
rule list in Figure 1, we would write d = (dp, δp, q0, K), where dp = (p1, p2, p3), δp = (1, 1, 1),
q0 = 0, and K = 3. Note that ((), (), q0, 0) is a well-deﬁned rule list with an empty preﬁx;
it is completely deﬁned by a single default rule.

Let dp = (p1, . . . , pk, . . . , pK) be an antecedent list, then for any k ≤ K, we deﬁne dk

(p1, . . . , pk) to be the k-preﬁx of dp. For any such k-preﬁx dk

p =
p, we say that dp starts with dk
p.

6

Learning Certifiably Optimal Rule Lists

For any given space of rule lists, we deﬁne σ(dp) to be the set of all rule lists whose preﬁxes
start with dp:

σ(dp) = {(d(cid:48)

p, δ(cid:48)

p, q(cid:48)

0, K(cid:48)) : d(cid:48)

p starts with dp}.

(1)

If dp = (p1, . . . , pK) and d(cid:48)
and extends it by a single antecedent, we say that dp is the parent of d(cid:48)
child of dp.

p = (p1, . . . , pK, pK+1) are two preﬁxes such that d(cid:48)

p starts with dp
p is a

p and that d(cid:48)

A rule list d classiﬁes datum xn by providing the label prediction qk of the ﬁrst rule rk
whose antecedent pk is true for xn. We say that an antecedent pk of antecedent list dp
captures xn in the context of dp if pk is the ﬁrst antecedent in dp that evaluates to true
for xn. We also say that a preﬁx captures those data captured by its antecedents; for a rule
list d = (dp, δp, q0, K), data not captured by the preﬁx dp are classiﬁed according to the
default label prediction q0.

Let β be a set of antecedents. We deﬁne cap(xn, β) = 1 if an antecedent in β captures
p starts

p be preﬁxes such that d(cid:48)

datum xn, and 0 otherwise. For example, let dp and d(cid:48)
with dp, then d(cid:48)

p captures all the data that dp captures:

{xn : cap(xn, dp)} ⊆ {xn : cap(xn, d(cid:48)

p)}.

Now let dp be an ordered list of antecedents, and let β be a subset of antecedents in dp.
Let us deﬁne cap(xn, β | dp) = 1 if β captures datum xn in the context of dp, i.e., if the ﬁrst
antecedent in dp that evaluates to true for xn is an antecedent in β, and 0 otherwise. Thus,
cap(xn, β | dp) = 1 only if cap(xn, β) = 1; cap(xn, β | dp) = 0 either if cap(xn, β) = 0, or if
cap(xn, β) = 1 but there is an antecedent α in dp, preceding all antecedents in β, such that
cap(xn, α) = 1. For example, if dp = (p1, . . . , pk, . . . , pK) is a preﬁx, then

cap(xn, pk | dp) =

¬ cap(xn, pk(cid:48))

∧ cap(xn, pk)

(cid:32) k−1
(cid:94)

k(cid:48)=1

(cid:33)

indicates whether antecedent pk captures datum xn in the context of dp. Now, deﬁne
supp(β, x) to be the normalized support of β,

and similarly deﬁne supp(β, x | dp) to be the normalized support of β in the context of dp,

supp(β, x) =

cap(xn, β),

supp(β, x | dp) =

cap(xn, β | dp),

(2)

(3)

Next, we address how empirical data constrains rule lists. Given training data (x, y), an
antecedent list dp = (p1, . . . , pK) implies a rule list d = (dp, δp, q0, K) with preﬁx dp, where
the label predictions δp = (q1, . . . , qK) and q0 are empirically set to minimize the number
of misclassiﬁcation errors made by the rule list on the training data. Thus for 1 ≤ k ≤ K,
label prediction qk corresponds to the majority label of data captured by antecedent pk in

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

7

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

the context of dp, and the default q0 corresponds to the majority label of data not captured
by dp. In the remainder of our presentation, whenever we refer to a rule list with a particular
preﬁx, we implicitly assume these empirically determined label predictions.

Our method is technically an associative classiﬁcation method since it leverages pre-

mined rules.

3.2 Objective Function

We deﬁne a simple objective function for a rule list d = (dp, δp, q0, K):

R(d, x, y) = (cid:96)(d, x, y) + λK.

(4)

This objective function is a regularized empirical risk; it consists of a loss (cid:96)(d, x, y), mea-
suring misclassiﬁcation error, and a regularization term that penalizes longer rule lists.
(cid:96)(d, x, y) is the fraction of training data whose labels are incorrectly predicted by d. In
our setting, the regularization parameter λ ≥ 0 is a small constant; e.g., λ = 0.01 can be
thought of as adding a penalty equivalent to misclassifying 1% of data when increasing a
rule list’s length by one association rule.

3.3 Optimization Framework

Our objective has structure amenable to global optimization via a branch-and-bound frame-
work. In particular, we make a series of important observations, each of which translates
into a useful bound, and that together interact to eliminate large parts of the search space.
We discuss these in depth in what follows:

• Lower bounds on a preﬁx also hold for every extension of that preﬁx. (§3.4, Theorem 1)

• If a rule list is not accurate enough with respect to its length, we can prune all

extensions of it. (§3.4, Lemma 2)

• We can calculate a priori an upper bound on the maximum length of an optimal rule

list. (§3.5, Theorem 6)

• Each rule in an optimal rule list must have support that is suﬃciently large. This allows
us to construct rule lists from frequent itemsets, while preserving the guarantee that
we can ﬁnd a globally optimal rule list from pre-mined rules. (§3.7, Theorem 10)

• Each rule in an optimal rule list must predict accurately. In particular, the number of
observations predicted correctly by each rule in an optimal rule list must be above a
threshold. (§3.7, Theorem 11)

• We need only consider the optimal permutation of antecedents in a preﬁx; we can

omit all other permutations. (§3.10, Theorem 15 and Corollary 16)

• If multiple observations have identical features and opposite labels, we know that any
model will make mistakes. In particular, the number of mistakes on these observations
will be at least the number of observations with the minority label. (§3.14, Theorem 20)

8

Learning Certifiably Optimal Rule Lists

3.4 Hierarchical Objective Lower Bound

We can decompose the misclassiﬁcation error in (4) into two contributions corresponding
to the preﬁx and the default rule:

(cid:96)(d, x, y) ≡ (cid:96)p(dp, δp, x, y) + (cid:96)0(dp, q0, x, y),

where dp = (p1, . . . , pK) and δp = (q1, . . . , qK);

(cid:96)p(dp, δp, x, y) =

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn]

is the fraction of data captured and misclassiﬁed by the preﬁx, and

(cid:96)0(dp, q0, x, y) =

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn]

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

1
N

N
(cid:88)

n=1

is the fraction of data not captured by the preﬁx and misclassiﬁed by the default rule.
Eliminating the latter error term gives a lower bound b(dp, x, y) on the objective,

b(dp, x, y) ≡ (cid:96)p(dp, δp, x, y) + λK ≤ R(d, x, y),

(5)

where we have suppressed the lower bound’s dependence on label predictions δp because
they are fully determined, given (dp, x, y). Furthermore, as we state next in Theorem 1,
b(dp, x, y) gives a lower bound on the objective of any rule list whose preﬁx starts with dp.

Theorem 1 (Hierarchical objective lower bound) Deﬁne b(dp, x, y) as in (5). Also,
deﬁne σ(dp) to be the set of all rule lists whose preﬁxes starts with dp, as in (1). Let d =
(dp, δp, q0, K) be a rule list with preﬁx dp, and let d(cid:48) = (d(cid:48)
0, K(cid:48)) ∈ σ(dp) be any rule
list such that its preﬁx d(cid:48)

p starts with dp and K(cid:48) ≥ K, then b(dp, x, y) ≤ R(d(cid:48), x, y).

p, δ(cid:48)

p, q(cid:48)

Proof Let dp = (p1, . . . , pK) and δp = (q1, . . . , qK); let d(cid:48)
δ(cid:48)
p = (q1, . . . , qK, qK+1, . . . , qK(cid:48)). Notice that d(cid:48)
additional mistakes:

p = (p1, . . . , pK, pK+1, . . . , pK(cid:48)) and
p yields the same mistakes as dp, and possibly

(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) =

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn]

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=1

=

1
N

N
(cid:88)

(cid:32) K
(cid:88)

n=1

k=1

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn] +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn]

(cid:33)

K(cid:48)
(cid:88)

k=K+1

= (cid:96)p(dp, δp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] ≥ (cid:96)p(dp, δp, x, y),

(6)

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K+1

where in the second equality we have used the fact that cap(xn, pk | d(cid:48)
for 1 ≤ k ≤ K. It follows that

p) = cap(xn, pk | dp)

b(dp, x, y) = (cid:96)p(dp, δp, x, y) + λK

≤ (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + λK(cid:48) = b(d(cid:48)

p, x, y) ≤ R(d(cid:48), x, y).

(7)

9

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Algorithm 1 Branch-and-bound for learning rule lists.

m=1, training data (x, y) = {(xn, yn)}N

Input: Objective function R(d, x, y), objective lower bound b(dp, x, y), set of antecedents
n=1, initial best known rule list d0 with
S = {sm}M
objective R0 = R(d0, x, y); d0 could be obtained as output from another (approximate)
algorithm, otherwise, (d0, R0) = (null, 1) provide reasonable default values
Output: Provably optimal rule list d∗ with minimum objective R∗

(dc, Rc) ← (d0, R0)
Q ← queue( [ ( ) ] )
while Q not empty do
dp ← Q.pop( )
d ← (dp, δp, q0, K)
if b(dp, x, y) < Rc then
R ← R(d, x, y)
if R < Rc then

(dc, Rc) ← (d, R)

end if
for s in S do

if s not in dp then
Q.push( (dp, s) )

end if

end for

end if
end while
(d∗, R∗) ← (dc, Rc)

(cid:46) Initialize best rule list and objective
(cid:46) Initialize queue with empty preﬁx
(cid:46) Stop when queue is empty
(cid:46) Remove preﬁx dp from the queue
(cid:46) Set label predictions δp and q0 to minimize training error
(cid:46) Bound: Apply Theorem 1
(cid:46) Compute objective of dp’s rule list d
(cid:46) Update best rule list and objective

(cid:46) Branch: Enqueue dp’s children

(cid:46) Identify provably optimal solution

To generalize, consider a sequence of preﬁxes such that each preﬁx starts with all previ-
ous preﬁxes in the sequence. It follows that the corresponding sequence of objective lower
bounds increases monotonically. This is precisely the structure required and exploited by
branch-and-bound, illustrated in Algorithm 1.

Speciﬁcally, the objective lower bound in Theorem 1 enables us to prune the state
space hierarchically. While executing branch-and-bound, we keep track of the current best
(smallest) objective Rc, thus it is a dynamic, monotonically decreasing quantity. If we
encounter a preﬁx dp with lower bound b(dp, x, y) ≥ Rc, then by Theorem 1, we do not need
to consider any rule list d(cid:48) ∈ σ(dp) whose preﬁx d(cid:48)
p starts with dp. For the objective of such
a rule list, the current best objective provides a lower bound, i.e., R(d(cid:48), x, y) ≥ b(d(cid:48)
p, x, y) ≥
b(dp, x, y) ≥ Rc, and thus d(cid:48) cannot be optimal.

Next, we state an immediate consequence of Theorem 1.

Lemma 2 (Objective lower bound with one-step lookahead) Let dp be a K-preﬁx
and let Rc be the current best objective. If b(dp, x, y) + λ ≥ Rc, then for any K(cid:48)-rule list
d(cid:48) ∈ σ(dp) whose preﬁx d(cid:48)

p starts with dp and K(cid:48) > K, it follows that R(d(cid:48), x, y) ≥ Rc.

10

Learning Certifiably Optimal Rule Lists

Proof By the deﬁnition of the lower bound (5), which includes the penalty for longer
preﬁxes,

R(d(cid:48)

p, x, y) ≥ b(d(cid:48)

p, x, y) = (cid:96)p(d(cid:48)
= (cid:96)p(d(cid:48)
= b(dp, x, y) + λ(K(cid:48) − K) ≥ b(dp, x, y) + λ ≥ Rc.

p, x, y) + λK(cid:48)
p, x, y) + λK + λ(K(cid:48) − K)

p, δ(cid:48)
p, δ(cid:48)

(8)

Therefore, even if we encounter a preﬁx dp with lower bound b(dp, x, y) ≤ Rc, as long
p that start with and are longer

as b(dp, x, y) + λ ≥ Rc, then we can prune all preﬁxes d(cid:48)
than dp.

3.5 Upper Bounds on Preﬁx Length

In this section, we derive several upper bounds on preﬁx length:

• The simplest upper bound on preﬁx length is given by the total number of available

antecedents. (Proposition 3)

• The current best objective Rc implies an upper bound on preﬁx length. (Theorem 4)

• For intuition, we state a version of the above bound that is valid at the start of

execution. (Corollary 5)

length. (Theorem 6)

• By considering speciﬁc families of preﬁxes, we can obtain tighter bounds on preﬁx

In the next section (§3.6), we use these results to derive corresponding upper bounds on the
number of preﬁx evaluations made by Algorithm 1.

Proposition 3 (Trivial upper bound on preﬁx length) Consider a state space of all
rule lists formed from a set of M antecedents, and let L(d) be the length of rule list d.
M provides an upper bound on the length of any optimal rule list d∗ ∈ argmind R(d, x, y),
i.e., L(d) ≤ M .

Proof Rule lists consist of distinct rules by deﬁnition.

At any point during branch-and-bound execution, the current best objective Rc implies

an upper bound on the maximum preﬁx length we might still have to consider.

Theorem 4 (Upper bound on preﬁx length) Consider a state space of all rule lists
formed from a set of M antecedents. Let L(d) be the length of rule list d and let Rc be the
current best objective. For all optimal rule lists d∗ ∈ argmind R(d, x, y)

L(d∗) ≤ min

(cid:23)

(cid:18)(cid:22) Rc
λ

(cid:19)

, M

,

11

(9)

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

where λ is the regularization parameter. Furthermore, if dc is a rule list with objective
R(dc, x, y) = Rc, length K, and zero misclassiﬁcation error, then for every optimal rule
list d∗ ∈ argmind R(d, x, y), if dc ∈ argmind R(d, x, y), then L(d∗) ≤ K, or otherwise if
dc /∈ argmind R(d, x, y), then L(d∗) ≤ K − 1.

Proof For an optimal rule list d∗ with objective R∗,

λL(d∗) ≤ R∗ = R(d∗, x, y) = (cid:96)(d∗, x, y) + λL(d∗) ≤ Rc.

The maximum possible length for d∗ occurs when (cid:96)(d∗, x, y) is minimized; combining with
Proposition 3 gives bound (9).

For the rest of the proof, let K∗ = L(d∗) be the length of d∗. If the current best rule

list dc has zero misclassiﬁcation error, then

λK∗ ≤ (cid:96)(d∗, x, y) + λK∗ = R(d∗, x, y) ≤ Rc = R(dc, x, y) = λK,

and thus K∗ ≤ K. If the current best rule list is suboptimal, i.e., dc /∈ argmind R(d, x, y),
then

λK∗ ≤ (cid:96)(d∗, x, y) + λK∗ = R(d∗, x, y) < Rc = R(dc, x, y) = λK,

in which case K∗ < K, i.e., K∗ ≤ K − 1, since K is an integer.

The latter part of Theorem 4 tells us that if we only need to identify a single instance
of an optimal rule list d∗ ∈ argmind R(d, x, y), and we encounter a perfect K-rule list with
zero misclassiﬁcation error, then we can prune all preﬁxes of length K or greater.

Corollary 5 (Simple upper bound on preﬁx length) Let L(d) be the length of rule
list d. For all optimal rule lists d∗ ∈ argmind R(d, x, y),

L(d∗) ≤ min

(cid:23)

(cid:18)(cid:22) 1
2λ

(cid:19)

, M

.

(10)

Proof Let d = ((), (), q0, 0) be the empty rule list; it has objective R(d, x, y) = (cid:96)(d, x, y) ≤
1/2, which gives an upper bound on Rc. Combining with (9) and Proposition 3 gives (10).

For any particular preﬁx dp, we can obtain potentially tighter upper bounds on preﬁx

length for the family of all preﬁxes that start with dp.

Theorem 6 (Preﬁx-speciﬁc upper bound on preﬁx length) Let d = (dp, δp, q0, K) be
a rule list, let d(cid:48) = (d(cid:48)
p starts with dp, and
let Rc be the current best objective. If d(cid:48)

0, K(cid:48)) ∈ σ(dp) be any rule list such that d(cid:48)
p has lower bound b(d(cid:48)

p, x, y) < Rc, then

p, q(cid:48)

p, δ(cid:48)

(cid:18)

K(cid:48) < min

K +

(cid:22) Rc − b(dp, x, y)
λ

(cid:23)

(cid:19)

, M

.

(11)

12

Learning Certifiably Optimal Rule Lists

Proof First, note that K(cid:48) ≥ K, since d(cid:48)

p starts with dp. Now recall from (7) that

b(dp, x, y) = (cid:96)p(dp, δp, x, y) + λK ≤ (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + λK(cid:48) = b(d(cid:48)

p, x, y),

and from (6) that (cid:96)p(dp, δp, x, y) ≤ (cid:96)p(d(cid:48)
gives

p, δ(cid:48)

p, x, y). Combining these bounds and rearranging

b(d(cid:48)

p, x, y) = (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + λK + λ(K(cid:48) − K)

≥ (cid:96)p(dp, δp, x, y) + λK + λ(K(cid:48) − K) = b(dp, x, y) + λ(K(cid:48) − K).

(12)

Combining (12) with b(d(cid:48)

p, x, y) < Rc and Proposition 3 gives (11).

We can view Theorem 6 as a generalization of our one-step lookahead bound (Lemma 2),
as (11) is equivalently a bound on K(cid:48) − K, an upper bound on the number of remain-
ing ‘steps’ corresponding to an iterative sequence of single-rule extensions of a preﬁx dp.
Notice that when d = ((), (), q0, 0) is the empty rule list, this bound replicates (9), since
b(dp, x, y) = 0.

3.6 Upper Bounds on the Number of Preﬁx Evaluations

In this section, we use our upper bounds on preﬁx length from §3.5 to derive corresponding
upper bounds on the number of preﬁx evaluations made by Algorithm 1. First, we present
Theorem 7, in which we use information about the state of Algorithm 1’s execution to
calculate, for any given execution state, upper bounds on the number of additional preﬁx
evaluations that might be required for the execution to complete. The relevant execution
state depends on the current best objective Rc and information about preﬁxes we are
planning to evaluate, i.e., preﬁxes in the queue Q of Algorithm 1. We deﬁne the number
of remaining preﬁx evaluations as the number of preﬁxes that are currently in or will be
inserted into the queue.

We use Theorem 7 in some of our empirical results (§6, Figure 18) to help illustrate
the dramatic impact of certain algorithm optimizations. The execution trace of this upper
bound on remaining preﬁx evaluations complements the execution traces of other quanti-
ties, e.g., that of the current best objective Rc. After presenting Theorem 7, we also give
two weaker propositions that provide useful intuition. In particular, Proposition 9 is a prac-
tical approximation to Theorem 7 that is signiﬁcantly easier to compute; we use it in our
implementation as a metric of execution progress that we display to the user.

Theorem 7 (Fine-grained upper bound on remaining preﬁx evaluations) Con-
sider the state space of all rule lists formed from a set of M antecedents, and consider Algo-
rithm 1 at a particular instant during execution. Let Rc be the current best objective, let Q
be the queue, and let L(dp) be the length of preﬁx dp. Deﬁne Γ(Rc, Q) to be the number of
remaining preﬁx evaluations, then

Γ(Rc, Q) ≤

(cid:88)

f (dp)
(cid:88)

dp∈Q

k=0

(M − L(dp))!
(M − L(dp) − k)!

,

(13)

13

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

where

f (dp) = min

(cid:18)(cid:22) Rc − b(dp, x, y)

(cid:23)

(cid:19)

, M − L(dp)

.

λ

Proof The number of remaining preﬁx evaluations is equal to the number of preﬁxes that
are currently in or will be inserted into queue Q. For any such preﬁx dp, Theorem 6 gives
an upper bound on the length of any preﬁx d(cid:48)

p that starts with dp:

(cid:18)

L(d(cid:48)

p) ≤ min

L(dp) +

(cid:22) Rc − b(dp, x, y)
λ

(cid:23)

(cid:19)

, M

≡ U (dp).

This gives an upper bound on the number of remaining preﬁx evaluations:

Γ(Rc, Q) ≤

P (M − L(dp), k) =

(cid:88)

U (dp)−L(dp)
(cid:88)

dp∈Q

k=0

(cid:88)

f (dp)
(cid:88)

dp∈Q

k=0

(M − L(dp))!
(M − L(dp) − k)!

,

where P (m, k) denotes the number of k-permutations of m.

Proposition 8 is strictly weaker than Theorem 7 and is the starting point for its deriva-
tion. It is a na¨ıve upper bound on the total number of preﬁx evaluations over the course
of Algorithm 1’s execution. It only depends on the number of rules and the regularization
parameter λ; i.e., unlike Theorem 7, it does not use algorithm execution state to bound the
size of the search space.

Proposition 8 (Upper bound on the total number of preﬁx evaluations) Deﬁne
Γtot(S) to be the total number of preﬁxes evaluated by Algorithm 1, given the state space of
all rule lists formed from a set S of M rules. For any set S of M rules,

Γtot(S) ≤

K
(cid:88)

k=0

M !
(M − k)!

,

where K = min((cid:98)1/2λ(cid:99), M ).

Proof By Corollary 5, K ≡ min((cid:98)1/2λ(cid:99), M ) gives an upper bound on the length of any
optimal rule list. Since we can think of our problem as ﬁnding the optimal selection and
permutation of k out of M rules, over all k ≤ K,

Γtot(S) ≤ 1 +

P (M, k) =

K
(cid:88)

k=1

K
(cid:88)

k=0

M !
(M − k)!

.

Our next upper bound is strictly tighter than the bound in Proposition 8. Like Theo-
rem 7, it uses the current best objective and information about the lengths of preﬁxes in the

14

Learning Certifiably Optimal Rule Lists

queue to constrain the lengths of preﬁxes in the remaining search space. However, Proposi-
tion 9 is weaker than Theorem 7 because it leverages only coarse-grained information from
the queue. Speciﬁcally, Theorem 7 is strictly tighter because it additionally incorporates
preﬁx-speciﬁc objective lower bound information from preﬁxes in the queue, which further
constrains the lengths of preﬁxes in the remaining search space.

Proposition 9 (Coarse-grained upper bound on remaining preﬁx evaluations)
Consider a state space of all rule lists formed from a set of M antecedents, and consider
Algorithm 1 at a particular instant during execution. Let Rc be the current best objective,
let Q be the queue, and let L(dp) be the length of preﬁx dp. Let Qj be the number of preﬁxes
of length j in Q,

Qj = (cid:12)

(cid:12){dp : L(dp) = j, dp ∈ Q}(cid:12)
(cid:12)

and let J = argmaxdp∈Q L(dp) be the length of the longest preﬁx in Q. Deﬁne Γ(Rc, Q) to
be the number of remaining preﬁx evaluations, then

Γ(Rc, Q) ≤

J
(cid:88)

j=1

Qj

(cid:32)K−j
(cid:88)

k=0

(M − j)!
(M − j − k)!

(cid:33)

,

where K = min((cid:98)Rc/λ(cid:99), M ).

Proof The number of remaining preﬁx evaluations is equal to the number of preﬁxes that
are currently in or will be inserted into queue Q. For any such remaining preﬁx dp, Theorem 4
gives an upper bound on its length; deﬁne K to be this bound: L(dp) ≤ min((cid:98)Rc/λ(cid:99), M ) ≡ K.
For any preﬁx dp in queue Q with length L(dp) = j, the maximum number of preﬁxes that
start with dp and remain to be evaluated is:

K−j
(cid:88)

k=0

P (M − j, k) =

K−j
(cid:88)

k=0

(M − j)!
(M − j − k)!

,

where P (T, k) denotes the number of k-permutations of T . This gives an upper bound on
the number of remaining preﬁx evaluations:

Γ(Rc, Q) ≤

J
(cid:88)

j=0

Qj

(cid:32)K−j
(cid:88)

k=0

P (M − j, k)

=

Qj

(cid:33)

J
(cid:88)

j=0

(cid:32)K−j
(cid:88)

k=0

(M − j)!
(M − j − k)!

(cid:33)

.

3.7 Lower Bounds on Antecedent Support

In this section, we give two lower bounds on the normalized support of each antecedent in
any optimal rule list; both are related to the regularization parameter λ.

15

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Theorem 10 (Lower bound on antecedent support) Let d∗ = (dp, δp, q0, K) be any
optimal rule list with objective R∗, i.e., d∗ ∈ argmind R(d, x, y). For each antecedent pk
in preﬁx dp = (p1, . . . , pK), the regularization parameter λ provides a lower bound on the
normalized support of pk,

λ ≤ supp(pk, x | dp).

(14)

Proof Let d∗ = (dp, δp, q0, K) be an optimal rule list with preﬁx dp = (p1, . . . , pK) and
0, K − 1) derived from d∗ by
labels δp = (q1, . . . , qK). Consider the rule list d = (d(cid:48)
deleting a rule pi → qi, therefore d(cid:48)
p = (q1, . . . , qi−1,
i+1, . . . , q(cid:48)
q(cid:48)

k need not be the same as qk, for k > i and k = 0.
The largest possible discrepancy between d∗ and d would occur if d∗ correctly classiﬁed

p, δ(cid:48)
p = (p1, . . . , pi−1, pi+1, . . . , pK) and δ(cid:48)

K), where q(cid:48)

p, q(cid:48)

all the data captured by pi, while d misclassiﬁed these data. This gives an upper bound:
R(d, x, y) = (cid:96)(d, x, y) + λ(K − 1) ≤ (cid:96)(d∗, x, y) + supp(pi, x | dp) + λ(K − 1)

= R(d∗, x, y) + supp(pi, x | dp) − λ
= R∗ + supp(pi, x | dp) − λ

(15)

where supp(pi, x | dp) is the normalized support of pi in the context of dp, deﬁned in (3),
and the regularization ‘bonus’ comes from the fact that d is one rule shorter than d∗.

At the same time, we must have R∗ ≤ R(d, x, y) for d∗ to be optimal. Combining this
with (15) and rearranging gives (14), therefore the regularization parameter λ provides a
lower bound on the support of an antecedent pi in an optimal rule list d∗.

Thus, we can prune a preﬁx dp if any of its antecedents captures less than a fraction λ
of data, even if b(dp, x, y) < R∗. Notice that the bound in Theorem 10 depends on the
antecedents, but not the label predictions, and thus does not account for misclassiﬁcation
error. Theorem 11 gives a tighter bound by leveraging this additional information, which
speciﬁcally tightens the upper bound on R(d, x, y) in (15).

Theorem 11 (Lower bound on accurate antecedent support) Let d∗ be any opti-
mal rule list with objective R∗, i.e., d∗ = (dp, δp, q0, K) ∈ argmind R(d, x, y). Let d∗ have
preﬁx dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK). For each rule pk → qk in d∗, deﬁne ak
to be the fraction of data that are captured by pk and correctly classiﬁed:

ak ≡

cap(xn, pk | dp) ∧ 1[qk = yn].

1
N

N
(cid:88)

n=1

The regularization parameter λ provides a lower bound on ak:

(16)

(17)

0, K − 1) be the rule list derived from d∗ by
Proof As in Theorem 10, let d = (d(cid:48)
deleting a rule pi → qi. Now, let us deﬁne (cid:96)i to be the portion of R∗ due to this rule’s
misclassiﬁcation error,

p, q(cid:48)

p, δ(cid:48)

(cid:96)i ≡

cap(xn, pi | dp) ∧ 1[qi (cid:54)= yn].

1
N

N
(cid:88)

n=1

λ ≤ ak.

16

Learning Certifiably Optimal Rule Lists

The largest discrepancy between d∗ and d would occur if d misclassiﬁed all the data captured
by pi. This gives an upper bound on the diﬀerence between the misclassiﬁcation error of d
and d∗:

(cid:96)(d, x, y) − (cid:96)(d∗, x, y) ≤ supp(pi, x | dp) − (cid:96)i

=

=

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

cap(xn, pi | dp) −

cap(xn, pi | dp) ∧ 1[qi (cid:54)= yn]

1
N

N
(cid:88)

n=1

cap(xn, pi | dp) ∧ 1[qi = yn] = ai,

where we deﬁned ai in (16). Relating this bound to the objectives of d and d∗ gives

R(d, x, y) = (cid:96)(d, x, y) + λ(K − 1) ≤ (cid:96)(d∗, x, y) + ai + λ(K − 1)

= R(d∗, x, y) + ai − λ
= R∗ + ai − λ.

(18)

Combining (18) with the requirement R∗ ≤ R(d, x, y) gives the bound λ ≤ ai.

Thus, we can prune a preﬁx if any of its rules correctly classiﬁes less than a fraction λ
of data. While the lower bound in Theorem 10 is a sub-condition of the lower bound in
Theorem 11, we can still leverage both—since the sub-condition is easier to check, check-
ing it ﬁrst can accelerate pruning. In addition to applying Theorem 10 in the context of
constructing rule lists, we can furthermore apply it in the context of rule mining (§3.1).
Speciﬁcally, it implies that we should only mine rules with normalized support of at least λ;
we need not mine rules with a smaller fraction of observations.2 In contrast, we can only
apply Theorem 11 in the context of constructing rule lists; it depends on the misclassiﬁ-
cation error associated with each rule in a rule list, thus it provides a lower bound on the
number of observations that each such rule must correctly classify.

3.8 Upper Bound on Antecedent Support

In the previous section (§3.7), we proved lower bounds on antecedent support; in Ap-
pendix A, we give an upper bound on antecedent support. Speciﬁcally, Theorem 21 shows
that an antecedent’s support in a rule list cannot be too similar to the set of data not
captured by preceding antecedents in the rule list. In particular, Theorem 21 implies that
we should only mine rules with normalized support less than or equal to 1 − λ; we need
not mine rules with a larger fraction of observations. Note that we do not otherwise use
this bound in our implementation, because we did not observe a meaningful beneﬁt in
preliminary experiments.

3.9 Antecedent Rejection and its Propagation

In this section, we demonstrate further consequences of our lower (§3.7) and upper bounds
(§3.8) on antecedent support, under a uniﬁed framework we refer to as antecedent rejec-
tion. Let dp = (p1, . . . , pK) be a preﬁx, and let pk be an antecedent in dp. Deﬁne pk to have

2. We describe our application of this idea in Appendix E, where we provide details on data processing.

17

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

insuﬃcient support in dp if it does not obey the bound in (14) of Theorem 10. Deﬁne pk
to have insuﬃcient accurate support in dp if it does not obey the bound in (17) of Theo-
rem 11. Deﬁne pk to have excessive support in dp if it does not obey the bound in (37) of
Theorem 21 (Appendix A). If pk in the context of dp has insuﬃcient support, insuﬃcient
accurate support, or excessive support, let us say that preﬁx dp rejects antecedent pK. Next,
in Theorem 12, we describe large classes of related rule lists whose preﬁxes all reject the
same antecedent.

Theorem 12 (Antecedent rejection propagates) For any preﬁx dp = (p1, . . . , pK), let
φ(dp) denote the set of all preﬁxes d(cid:48)
p such that the set of all antecedents in dp is a subset
of the set of all antecedents in d(cid:48)
p, i.e.,

φ(dp) = {d(cid:48)

p = (p(cid:48)

1, . . . , p(cid:48)

K(cid:48)) s.t. {pk : pk ∈ dp} ⊆ {p(cid:48)

κ : p(cid:48)

κ ∈ d(cid:48)

p}, K(cid:48) ≥ K}.

(19)

Let d = (dp, δp, q0, K) be a rule list with preﬁx dp = (p1, . . . , pK−1, pK), such that dp rejects
its last antecedent pK, either because pK in the context of dp has insuﬃcient support, insuf-
ﬁcient accurate support, or excessive support. Let dK−1
= (p1, . . . , pK−1) be the ﬁrst K − 1
antecedents of dp. Let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp = (P1, . . . , PK(cid:48)−1,
PK(cid:48), . . . , Pκ) such that Dp starts with DK(cid:48)−1
) and antecedent
PK(cid:48) = pK. It follows that preﬁx Dp rejects PK(cid:48) for the same reason that dp rejects pK, and
furthermore, D cannot be optimal, i.e., D /∈ argmind† R(d†, x, y).

= (P1, . . . , PK(cid:48)−1) ∈ φ(dK−1

p

p

p

Proof Combine Proposition 13, Proposition 14, and Proposition 22. The ﬁrst two are
found below, and the last in Appendix A.

Theorem 12 implies potentially signiﬁcant computational savings. We know from Theo-
rems 10, 11, and 21 that during branch-and-bound execution, if we ever encounter a preﬁx
dp = (p1, . . . , pK−1, pK) that rejects its last antecedent pK, then we can prune dp. By The-
orem 12, we can also prune any preﬁx d(cid:48)
p whose antecedents contains the set of antecedents
in dp, in almost any order, with the constraint that all antecedents in {p1, . . . , pK−1} pre-
cede pK. These latter antecedents are also rejected directly by the bounds in Theorems 10,
11, and 21; this is how our implementation works in practice. In a preliminary implemen-
tation (not shown), we maintained additional data structures to support the direct use of
Theorem 12. We leave the design of eﬃcient data structures for this task as future work.

Proposition 13 (Insuﬃcient antecedent support propagates) First deﬁne φ(dp) as
in (19), and let dp = (p1, . . . , pK−1, pK) be a preﬁx, such that its last antecedent pK has
insuﬃcient support, i.e., the opposite of the bound in (14): supp(pK, x | dp) < λ. Let dK−1
=
(p1, . . . , pK−1), and let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp = (P1, . . . , PK(cid:48)−1,
PK(cid:48), . . . , Pκ), such that Dp starts with DK(cid:48)−1
) and PK(cid:48) = pK.
It follows that PK(cid:48) has insuﬃcient support in preﬁx Dp, and furthermore, D cannot be
optimal, i.e., D /∈ argmind R(d, x, y).

= (P1, . . . , PK(cid:48)−1) ∈ φ(dK−1

p

p

p

18

Learning Certifiably Optimal Rule Lists

Proof The support of pK in dp depends only on the set of antecedents in dK

p = (p1, . . . , pK):

supp(pK, x | dp) =

cap(xn, pK | dp) =

(cid:0)¬ cap(xn, dK−1

)(cid:1) ∧ cap(xn, pK)

p

N
(cid:88)

n=1
N
(cid:88)

1
N

1
N

=

(cid:32)K−1
(cid:94)

n=1

k=1

(cid:33)

¬ cap(xn, pk)

∧ cap(xn, pK) < λ,

and the support of PK(cid:48) in Dp depends only on the set of antecedents in DK(cid:48)

p = (P1, . . . , PK(cid:48)):

supp(PK(cid:48), x | Dp) =

cap(xn, PK(cid:48) | Dp) =

¬ cap(xn, Pk)

∧ cap(xn, PK(cid:48))

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

1
N

1
N

1
N

N
(cid:88)

(cid:32)K(cid:48)−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

n=1

k=1

≤

=

(cid:33)

(cid:33)

(cid:33)

¬ cap(xn, pk)

∧ cap(xn, PK(cid:48))

¬ cap(xn, pk)

∧ cap(xn, pK)

= supp(pK, x | dp) < λ.

(20)

The ﬁrst inequality reﬂects the condition that DK(cid:48)−1
set of antecedents in DK(cid:48)−1
reﬂects the fact that PK(cid:48) = pK. Thus, P (cid:48)
by Theorem 10, D cannot be optimal, i.e., D /∈ argmind R(d, x, y).

), which implies that the
, and the next equality
K has insuﬃcient support in preﬁx Dp, therefore

contains the set of antecedents in dK−1

∈ φ(dK−1
p

p

p

p

Proposition 14 (Insuﬃcient accurate antecedent support propagates) Let φ(dp)
denote the set of all preﬁxes d(cid:48)
p such that the set of all antecedents in dp is a subset of
the set of all antecedents in d(cid:48)
p, as in (19). Let d = (dp, δp, q0, K) be a rule list with preﬁx
dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK), such that the last antecedent pK has insuﬃ-
cient accurate support, i.e., the opposite of the bound in (17):

1
N

N
(cid:88)

n=1

cap(xn, pK | dp) ∧ 1[qK = yn] < λ.

Let dK−1
p
(P1, . . . , Pκ) and labels ∆p = (Q1, . . . , Qκ), such that Dp starts with DK(cid:48)−1
∈ φ(dK−1
p
and furthermore, D /∈ argmind† R(d†, x, y).

= (p1, . . . , pK−1) and let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp =
= (P1, . . . , PK(cid:48)−1)
) and PK(cid:48) = pK. It follows that PK(cid:48) has insuﬃcient accurate support in preﬁx Dp,

p

19

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Proof The accurate support of PK(cid:48) in Dp is insuﬃcient:

1
N

N
(cid:88)

n=1

cap(xn, PK(cid:48) | Dp) ∧ 1[QK(cid:48) = yn]

=

≤

=

=

≤

1
N

1
N

1
N

1
N

1
N

N
(cid:88)

(cid:32)K(cid:48)−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

k=1

n=1

N
(cid:88)

n=1
N
(cid:88)

n=1

(cid:33)

(cid:33)

(cid:33)

¬ cap(xn, Pk)

∧ cap(xn, PK(cid:48)) ∧ 1[QK(cid:48) = yn]

¬ cap(xn, pk)

∧ cap(xn, PK(cid:48)) ∧ 1[QK(cid:48) = yn]

¬ cap(xn, pk)

∧ cap(xn, pK) ∧ 1[QK(cid:48) = yn]

cap(xn, pK | dp) ∧ 1[QK(cid:48) = yn]

cap(xn, pK | dp) ∧ 1[qK = yn] < λ.

The ﬁrst inequality reﬂects the condition that DK(cid:48)−1
), the next equality reﬂects
the fact that PK(cid:48) = pK. For the following equality, notice that QK(cid:48) is the majority class
label of data captured by PK(cid:48) in Dp, and qK is the majority class label of data captured
by PK in dp, and recall from (20) that supp(PK(cid:48), x | Dp) ≤ supp(pK, x | dp). By Theorem 11,
D /∈ argmind† R(d†, x, y).

∈ φ(dK−1
p

p

Propositions 13 and 14, combined with Proposition 22 (Appendix A), constitute the

proof of Theorem 12.

3.10 Equivalent Support Bound

If two preﬁxes capture the same data, and one is more accurate than the other, then there
is no beneﬁt to considering preﬁxes that start with the less accurate one. Let dp be a
preﬁx, and consider the best possible rule list whose preﬁx starts with dp. If we take its
antecedents in dp and replace them with another preﬁx with the same support (that could
include diﬀerent antecedents), then its objective can only become worse or remain the same.
Formally, let Dp be a preﬁx, and let ξ(Dp) be the set of all preﬁxes that capture exactly
the same data as Dp. Now, let d be a rule list with preﬁx dp in ξ(Dp), such that d has
the minimum objective over all rule lists with preﬁxes in ξ(Dp). Finally, let d(cid:48) be a rule
list whose preﬁx d(cid:48)
p starts with dp, such that d(cid:48) has the minimum objective over all rule
lists whose preﬁxes start with dp. Theorem 15 below implies that d(cid:48) also has the minimum
objective over all rule lists whose preﬁxes start with any preﬁx in ξ(Dp).

Theorem 15 (Equivalent support bound) Deﬁne σ(dp) to be the set of all rule lists
whose preﬁxes start with dp, as in (1). Let d = (dp, δp, q0, K) be a rule list with preﬁx
dp = (p1, . . . , pK), and let D = (Dp, ∆p, Q0, κ) be a rule list with preﬁx Dp = (P1, . . . , Pκ),

20

Learning Certifiably Optimal Rule Lists

such that dp and Dp capture the same data, i.e.,

{xn : cap(xn, dp)} = {xn : cap(xn, Dp)}.

If the objective lower bounds of d and D obey b(dp, x, y) ≤ b(Dp, x, y), then the objective of
the optimal rule list in σ(dp) gives a lower bound on the objective of the optimal rule list
in σ(Dp):

min
d(cid:48)∈σ(dp)

R(d(cid:48), x, y) ≤ min

R(D(cid:48), x, y).

D(cid:48)∈σ(Dp)

(21)

Proof See Appendix B for the proof of Theorem 15.

Thus, if preﬁxes dp and Dp capture the same data, and their objective lower bounds obey
b(dp, x, y) ≤ b(Dp, x, y), Theorem 15 implies that we can prune Dp. Next, in Sections 3.11
and 3.12, we highlight and analyze the special case of preﬁxes that capture the same data
because they contain the same antecedents.

3.11 Permutation Bound

If two preﬁxes are composed of the same antecedents, i.e., they contain the same antecedents
up to a permutation, then they capture the same data, and thus Theorem 15 applies.
Therefore, if one is more accurate than the other, then there is no beneﬁt to considering
preﬁxes that start with the less accurate one. Let dp be a preﬁx, and consider the best
possible rule list whose preﬁx starts with dp. If we permute its antecedents in dp, then its
objective can only become worse or remain the same.

Formally, let P = {pk}K

k=1 be a set of K antecedents, and let Π be the set of all K-preﬁxes
corresponding to permutations of antecedents in P . Let preﬁx dp in Π have the minimum
preﬁx misclassiﬁcation error over all preﬁxes in Π. Also, let d(cid:48) be a rule list whose preﬁx d(cid:48)
p
starts with dp, such that d(cid:48) has the minimum objective over all rule lists whose preﬁxes start
with dp. Corollary 16 below, which can be viewed as special case of Theorem 15, implies
that d(cid:48) also has the minimum objective over all rule lists whose preﬁxes start with any
preﬁx in Π.

p, q(cid:48)

p, δ(cid:48)

0, K(cid:48)) : d(cid:48)

Corollary 16 (Permutation bound) Let π be any permutation of {1, . . . , K}, and de-
ﬁne σ(dp) = {(d(cid:48)
p starts with dp} to be the set of all rule lists whose preﬁxes
start with dp. Let d = (dp, δp, q0, K) and D = (Dp, ∆p, Q0, K) denote rule lists with preﬁxes
dp = (p1, . . . , pK) and Dp = (pπ(1), . . . , pπ(K)), respectively, i.e., the antecedents in Dp cor-
respond to a permutation of the antecedents in dp. If the objective lower bounds of d and D
obey b(dp, x, y) ≤ b(Dp, x, y), then the objective of the optimal rule list in σ(dp) gives a
lower bound on the objective of the optimal rule list in σ(Dp):

min
d(cid:48)∈σ(dp)

R(d(cid:48), x, y) ≤ min

R(D(cid:48), x, y).

D(cid:48)∈σ(Dp)

Proof Since preﬁxes dp and Dp contain the same antecedents, they both capture the same
data. Thus, we can apply Theorem 15.

21

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Thus if preﬁxes dp and Dp have the same antecedents, up to a permutation, and their
objective lower bounds obey b(dp, x, y) ≤ b(Dp, x, y), Corollary 16 implies that we can
prune Dp. We call this symmetry-aware pruning, and we illustrate the subsequent compu-
tational savings next in §3.12.

3.12 Upper Bound on Preﬁx Evaluations with Symmetry-aware Pruning

Here, we present an upper bound on the total number of preﬁx evaluations that accounts for
the eﬀect of symmetry-aware pruning (§3.11). Since every subset of K antecedents generates
an equivalence class of K! preﬁxes equivalent up to permutation, symmetry-aware pruning
dramatically reduces the search space.

First, notice that Algorithm 1 describes a breadth-ﬁrst exploration of the state space of
rule lists. Now suppose we integrate symmetry-aware pruning into our execution of branch-
and-bound, so that after evaluating preﬁxes of length K, we only keep a single best preﬁx
from each set of preﬁxes equivalent up to a permutation.

Theorem 17 (Upper bound on preﬁx evaluations with symmetry-aware pruning)
Consider a state space of all rule lists formed from a set S of M antecedents, and consider
the branch-and-bound algorithm with symmetry-aware pruning. Deﬁne Γtot(S) to be the total
number of preﬁxes evaluated. For any set S of M rules,

Γtot(S) ≤ 1 +

K
(cid:88)

k=1

1
(k − 1)!

·

M !
(M − k)!

,

where K = min((cid:98)1/2λ(cid:99), M ).

Proof By Corollary 5, K ≡ min((cid:98)1/2λ(cid:99), M ) gives an upper bound on the length of any op-
timal rule list. The algorithm begins by evaluating the empty preﬁx, followed by M preﬁxes
of length k = 1, then P (M, 2) preﬁxes of length k = 2, where P (M, 2) is the number of size-2
subsets of {1, . . . , M }. Before proceeding to length k = 3, we keep only C(M, 2) preﬁxes of
length k = 2, where C(M, k) denotes the number of k-combinations of M . Now, the number
of length k = 3 preﬁxes we evaluate is C(M, 2)(M − 2). Propagating this forward gives

Γtot(S) ≤ 1 +

C(M, k − 1)(M − k + 1) = 1 +

K
(cid:88)

k=1

K
(cid:88)

k=1

1
(k − 1)!

·

M !
(M − k)!

.

Pruning based on permutation symmetries thus yields signiﬁcant computational savings.
Let us compare, for example, to the na¨ıve number of preﬁx evaluations given by the upper
bound in Proposition 8. If M = 100 and K = 5, then the na¨ıve number is about 9.1 × 109,
while the reduced number due to symmetry-aware pruning is about 3.9 × 108, which is
smaller by a factor of about 23. If M = 1000 and K = 10, the number of evaluations falls
from about 9.6 × 1029 to about 2.7 × 1024, which is smaller by a factor of about 360,000.

While 1024 seems infeasibly enormous, it does not represent the number of rule lists we
evaluate. As we show in our experiments (§6), our permutation bound in Corollary 16 and

22

Learning Certifiably Optimal Rule Lists

our other bounds together conspire to reduce the search space to a size manageable on a
single computer. The choice of M = 1000 and K = 10 in our example above corresponds to
the state space size our eﬀorts target. K = 10 rules represents a (heuristic) upper limit on
the size of an interpretable rule list, and M = 1000 represents the approximate number of
rules with suﬃciently high support (Theorem 10) we expect to obtain via rule mining (§3.1).

3.13 Similar Support Bound

We now present a relaxation of Theorem 15, our equivalent support bound. Theorem 18
implies that if we know that no extensions of a preﬁx dp are better than the current best
objective, then we can prune all preﬁxes with support similar to dp’s support. Understanding
how to exploit this result in practice represents an exciting direction for future work; our
implementation (§5) does not currently leverage the bound in Theorem 18.

Theorem 18 (Similar support bound) Deﬁne σ(dp) to be the set of all rule lists whose
preﬁxes start with dp, as in (1). Let dp = (p1, . . . , pK) and Dp = (P1, . . . , Pκ) be preﬁxes
that capture nearly the same data. Speciﬁcally, deﬁne ω to be the normalized support of data
captured by dp and not captured by Dp, i.e.,

ω ≡

¬ cap(xn, Dp) ∧ cap(xn, dp).

Similarly, deﬁne Ω to be the normalized support of data captured by Dp and not captured
by dp, i.e.,

Ω ≡

¬ cap(xn, dp) ∧ cap(xn, Dp).

We can bound the diﬀerence between the objectives of the optimal rule lists in σ(dp) and
σ(Dp) as follows:

min
D†∈σ(Dp)

d†∈σ(dp)

R(D†, x, y) − min

R(d†, x, y) ≥ b(Dp, x, y) − b(dp, x, y) − ω − Ω,

(24)

where b(dp, x, y) and b(Dp, x, y) are the objective lower bounds of d and D, respectively.

Proof See Appendix C for the proof of Theorem 18.

Theorem 18 implies that if preﬁxes dp and Dp are similar, and we know the optimal

objective of rule lists starting with dp, then

min
D(cid:48)∈σ(Dp)

d(cid:48)∈σ(dp)

R(D(cid:48), x, y) ≥ min

R(d(cid:48), x, y) + b(Dp, x, y) − b(dp, x, y) − χ

≥ Rc + b(Dp, x, y) − b(dp, x, y) − χ,

where Rc is the current best objective, and χ is the normalized support of the set of data
captured either exclusively by dp or exclusively by Dp. It follows that

min
D(cid:48)∈σ(Dp)

R(D(cid:48), x, y) ≥ Rc + b(Dp, x, y) − b(dp, x, y) − χ ≥ Rc

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

(22)

(23)

23

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if b(Dp, x, y) − b(dp, x, y) ≥ χ. To conclude, we summarize this result and combine it with
our notion of lookahead from Lemma 2. During branch-and-bound execution, if we demon-
strate that mind(cid:48)∈σ(dp) R(d(cid:48), x, y) ≥ Rc, then we can prune all preﬁxes that start with any
preﬁx D(cid:48)

p in the following set:

(cid:40)

(cid:41)

D(cid:48)

p : b(D(cid:48)

p, x, y) + λ − b(dp, x, y) ≥

cap(xn, dp) ⊕ cap(xn, D(cid:48)
p)

,

1
N

N
(cid:88)

n=1

where the symbol ⊕ denotes the logical operation, exclusive or (XOR).

3.14 Equivalent Points Bound

The bounds in this section quantify the following: If multiple observations that are not
captured by a preﬁx dp have identical features and opposite labels, then no rule list that
starts with dp can correctly classify all these observations. For each set of such observations,
the number of mistakes is at least the number of observations with the minority label within
the set.

Consider a data set {(xn, yn)}N

m=1. Deﬁne dis-
tinct observations to be equivalent if they are captured by exactly the same antecedents,
i.e., xi (cid:54)= xj are equivalent if

n=1 and also a set of antecedents {sm}M

1
M

M
(cid:88)

m=1

1[cap(xi, sm) = cap(xj, sm)] = 1.

Notice that we can partition a data set into sets of equivalent points; let {eu}U
u=1 enumerate
these sets. Let eu be the equivalent points set that contains observation xi. Now deﬁne θ(eu)
to be the normalized support of the minority class label with respect to set eu, e.g., let

eu = {xn : ∀m ∈ [M ], 1[cap(xn, sm) = cap(xi, sm)]},

and let qu be the minority class label among points in eu, then

θ(eu) =

1[xn ∈ eu] 1[yn = qu].

(25)

1
N

N
(cid:88)

n=1

The existence of equivalent points sets with non-singleton support yields a tighter ob-
jective lower bound that we can combine with our other bounds; as our experiments demon-
strate (§6), the practical consequences can be dramatic. First, for intuition, we present a
general bound in Proposition 19; next, we explicitly integrate this bound into our framework
in Theorem 20.

Proposition 19 (General equivalent points bound) Let d = (dp, δp, q0, K) be a rule list,
then

R(d, x, y) ≥

θ(eu) + λK.

U
(cid:88)

u=1

24

Learning Certifiably Optimal Rule Lists

Proof Recall that the objective is R(d, x, y) = (cid:96)(d, x, y) + λK, where the misclassiﬁcation
error (cid:96)(d, x, y) is given by

(cid:96)(d, x, y) = (cid:96)0(dp, q0, x, y) + (cid:96)p(dp, δp, x, y)

=

1
N

(cid:32)

N
(cid:88)

n=1

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn] +

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn]

.

(cid:33)

K
(cid:88)

k=1

Any particular rule list uses a speciﬁc rule, and therefore a single class label, to classify
all points within a set of equivalent points. Thus, for a set of equivalent points u, the rule
list d correctly classiﬁes either points that have the majority class label, or points that have
the minority class label. It follows that d misclassiﬁes a number of points in u at least as
great as the number of points with the minority class label. To translate this into a lower
bound on (cid:96)(d, x, y), we ﬁrst sum over all sets of equivalent points, and then for each such
set, count diﬀerences between class labels and the minority class label of the set, instead of
counting mistakes:

(cid:96)(d, x, y)

=

≥

1
N

1
N

U
(cid:88)

N
(cid:88)

(cid:32)

u=1

n=1

U
(cid:88)

N
(cid:88)

(cid:32)

u=1

n=1

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn] +

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn]

1[xn ∈ eu]

¬ cap(xn, dp) ∧ 1[yn = qu] +

cap(xn, pk | dp) ∧ 1[yn = qu]

1[xn ∈ eu].

(cid:33)

(cid:33)

(26)

Next, we factor out the indicator for equivalent point set membership, which yields a term
that sums to one, because every datum is either captured or not captured by preﬁx dp.

(cid:96)(d, x, y) =

¬ cap(xn, dp) +

cap(xn, pk | dp)

∧ 1[xn ∈ eu] 1[yn = qu]

(cid:33)

U
(cid:88)

N
(cid:88)

(cid:32)

1
N

1
N

1
N

=

=

u=1

n=1

U
(cid:88)

N
(cid:88)

u=1
U
(cid:88)

n=1
N
(cid:88)

u=1

n=1

(¬ cap(xn, dp) + cap(xn, dp)) ∧ 1[xn ∈ eu] 1[yn = qu]

1[xn ∈ eu] 1[yn = qu] =

θ(eu),

U
(cid:88)

u=1

where the ﬁnal equality applies the deﬁnition of θ(eu) in (25). Therefore, R(d, x, y) =
(cid:96)(d, x, y) + λK ≥ (cid:80)U

u=1 θ(eu) + λK.

Now, recall that to obtain our lower bound b(dp, x, y) in (5), we simply deleted the
default rule misclassiﬁcation error (cid:96)0(dp, q0, x, y) from the objective R(d, x, y). Theorem 20
obtains a tighter objective lower bound via a tighter lower bound on the default rule mis-
classiﬁcation error, 0 ≤ b0(dp, x, y) ≤ (cid:96)0(dp, q0, x, y).

K
(cid:88)

k=1
K
(cid:88)

k=1

K
(cid:88)

k=1

25

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Theorem 20 (Equivalent points bound) Let d be a rule list with preﬁx dp and lower
bound b(dp, x, y), then for any rule list d(cid:48) ∈ σ(d) whose preﬁx d(cid:48)

p starts with dp,

R(d(cid:48), x, y) ≥ b(dp, x, y) + b0(dp, x, y),

(27)

where

b0(dp, x, y) =

¬ cap(xn, dp) ∧ 1[xn ∈ eu] 1[yn = qu].

(28)

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

Proof See Appendix D for the proof of Theorem 20.

4. Incremental Computation

For every preﬁx dp evaluated during Algorithm 1’s execution, we compute the objective
lower bound b(dp, x, y) and sometimes the objective R(d, x, y) of the corresponding rule
list d. These calculations are the dominant computations with respect to execution time.
This motivates our use of a highly optimized library, designed by Yang et al. (2017), for
representing rule lists and performing operations encountered in evaluating functions of rule
lists. Furthermore, we exploit the hierarchical nature of the objective function and its lower
bound to compute these quantities incrementally throughout branch-and-bound execution.
In this section, we provide explicit expressions for the incremental computations that are
central to our approach. Later, in §5, we describe a cache data structure for supporting our
incremental framework in practice.

For completeness, before presenting our incremental expressions, let us begin by writing
down the objective lower bound and objective of the empty rule list, d = ((), (), q0, 0), the
ﬁrst rule list evaluated in Algorithm 1. Since its preﬁx contains zero rules, it has zero preﬁx
misclassiﬁcation error and also has length zero. Thus, the empty rule list’s objective lower
bound is zero, i.e., b((), x, y) = 0. Since none of the data are captured by the empty preﬁx,
the default rule corresponds to the majority class, and the objective corresponds to the
default rule misclassiﬁcation error, i.e., R(d, x, y) = (cid:96)0((), q0, x, y).

Now, we derive our incremental expressions for the objective function and its lower
p, q(cid:48)
bound. Let d = (dp, δp, q0, K) and d(cid:48) = (d(cid:48)
0, K + 1) be rule lists such that preﬁx dp =
(p1, . . . , pK) is the parent of d(cid:48)
p = (q1, . . . ,
qK, qK+1) be the corresponding labels. The hierarchical structure of Algorithm 1 enforces
that if we ever evaluate d(cid:48), then we will have already evaluated both the objective and ob-
jective lower bound of its parent, d. We would like to reuse as much of these computations as
possible in our evaluation of d(cid:48). We can write the objective lower bound of d(cid:48) incrementally,

p = (p1, . . . , pK, pK+1). Let δp = (q1, . . . , qK) and δ(cid:48)

p, δ(cid:48)

26

Learning Certifiably Optimal Rule Lists

with respect to the objective lower bound of d:

b(d(cid:48)

p, δ(cid:48)
p, x, y) = (cid:96)p(d(cid:48)
N
(cid:88)

p, x, y) + λ(K + 1)
K+1
(cid:88)

cap(xn, pk | d(cid:48)

=

1
N

n=1

k=1

p) ∧ 1[qk (cid:54)= yn] + λ(K + 1)

(29)

= (cid:96)p(dp, δp, x, y) + λK + λ +

cap(xn, pK+1 | d(cid:48)

p) ∧ 1[qK+1 (cid:54)= yn]

1
N

N
(cid:88)

n=1

= b(dp, x, y) + λ +

cap(xn, pK+1 | d(cid:48)

p) ∧ 1[qK+1 (cid:54)= yn]

= b(dp, x, y) + λ +

¬ cap(xn, dp) ∧ cap(xn, pK+1) ∧ 1[qK+1 (cid:54)= yn].

(30)

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

Thus, if we store b(dp, x, y), then we can reuse this quantity when computing b(d(cid:48)
p, x, y).
Transforming (29) into (30) yields a signiﬁcantly simpler expression that is a function of
the stored quantity b(dp, x, y). For the objective of d(cid:48), ﬁrst let us write a na¨ıve expression:

R(d(cid:48), x, y) = (cid:96)(d(cid:48), x, y) + λ(K + 1) = (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + (cid:96)0(d(cid:48)

p, q(cid:48)

0, x, y) + λ(K + 1)

=

1
N

N
(cid:88)

K+1
(cid:88)

n=1

k=1

1
N

N
(cid:88)

n=1

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] +

¬ cap(xn, d(cid:48)

p) ∧ 1[q(cid:48)

0 (cid:54)= yn] + λ(K + 1).

(31)

Instead, we can compute the objective of d(cid:48) incrementally with respect to its objective lower
bound:

R(d(cid:48), x, y) = (cid:96)p(d(cid:48)
= b(d(cid:48)

p, x, y) + (cid:96)0(d(cid:48)
p, δ(cid:48)
p, q(cid:48)
p, x, y) + (cid:96)0(d(cid:48)
N
(cid:88)

p, q(cid:48)
0, x, y)

0, x, y) + λ(K + 1)

= b(d(cid:48)

p, x, y) +

¬ cap(xn, d(cid:48)

p) ∧ 1[q(cid:48)

0 (cid:54)= yn]

1
N

1
N

n=1
N
(cid:88)

n=1

= b(d(cid:48)

p, x, y) +

¬ cap(xn, dp) ∧ (¬ cap(xn, pK+1)) ∧ 1[q(cid:48)

0 (cid:54)= yn].

(32)

The expression in (32) is simpler to compute than that in (31), because the former reuses
p, x, y), which we already computed in (30). Note that instead of computing R(d(cid:48), x, y)
b(d(cid:48)
incrementally from b(d(cid:48)
p, x, y) as in (32), we could have computed it incrementally from
R(d, x, y). However, doing so would in practice require that we store R(d, x, y) in addition
to b(dp, x, y), which we already must store to support (30). We prefer the incremental
approach suggested by (32) since it avoids this additional storage overhead.

We present an incremental branch-and-bound procedure in Algorithm 2, and show the
incremental computations of the objective lower bound (30) and objective (32) as two
separate functions in Algorithms 3 and 4, respectively. In Algorithm 2, we use a cache to

27

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Algorithm 2 Incremental branch-and-bound for learning rule lists, for simplicity, from a
cold start. We explicitly show the incremental objective lower bound and objective functions
in Algorithms 3 and 4, respectively.

Input: Objective function R(d, x, y), objective lower bound b(dp, x, y), set of antecedents
S = {sm}M
Output: Provably optimal rule list d∗ with minimum objective R∗

m=1, training data (x, y) = {(xn, yn)}N

n=1, regularization parameter λ

dc ← ((), (), q0, 0)
(cid:46) Initialize current best rule list with empty rule list
Rc ← R(dc, x, y)
(cid:46) Initialize current best objective
(cid:46) Initialize queue with empty preﬁx
Q ← queue( [ ( ) ] )
C ← cache( [ ( ( ) , 0 ) ] ) (cid:46) Initialize cache with empty preﬁx and its objective lower bound
(cid:46) Optimization complete when the queue is empty
while Q not empty do
(cid:46) Remove a length-K preﬁx dp from the queue
dp ← Q.pop( )
(cid:46) Look up dp’s lower bound in the cache
b(dp, x, y) ← C.ﬁnd(dp)
(cid:46) Bit vector indicating data not captured by dp
u ← ¬ cap(x, dp)
(cid:46) Evaluate all of dp’s children
for s in S do

if s not in dp then
Dp ← (dp, s)
v ← u ∧ cap(x, s)
b(Dp, x, y) ← b(dp, x, y) + λ + IncrementalLowerBound(v, y, N )
if b(Dp, x, y) < Rc then

(cid:46) Branch: Generate child Dp
(cid:46) Bit vector indicating data captured by s in Dp

(cid:46) Bound: Apply bound from Theorem 1

R(D, x, y) ← b(Dp, x, y) + IncrementalObjective(u, v, y, N )
D ← (Dp, ∆p, Q0, K + 1)
if R(D, x, y) < Rc then

(cid:46) ∆p, Q0 are set in the incremental functions

(dc, Rc) ← (D, R(D, x, y)) (cid:46) Update current best rule list and objective

end if
Q.push(Dp)
C.insert(Dp, b(Dp, x, y))

(cid:46) Add Dp to the queue
(cid:46) Add Dp and its lower bound to the cache

end if

end if

end for
end while
(d∗, R∗) ← (dc, Rc)

(cid:46) Identify provably optimal rule list and objective

store preﬁxes and their objective lower bounds. Algorithm 2 additionally reorganizes the
structure of Algorithm 1 to group together the computations associated with all children
of a particular preﬁx. This has two advantages. The ﬁrst is to consolidate cache queries: all
children of the same parent preﬁx compute their objective lower bounds with respect to the
parent’s stored value, and we only require one cache ‘ﬁnd’ operation for the entire group
of children, instead of a separate query for each child. The second is to shrink the queue’s
size: instead of adding all of a preﬁx’s children as separate queue elements, we represent
the entire group of children in the queue by a single element. Since the number of children
associated with each preﬁx is close to the total number of possible antecedents, both of these
eﬀects can yield signiﬁcant savings. For example, if we are trying to optimize over rule lists

28

Learning Certifiably Optimal Rule Lists

Algorithm 3 Incremental objective lower bound (30) used in Algorithm 2.

Input: Bit vector v ∈ {0, 1}N indicating data captured by s, the last antecedent in Dp,
bit vector of class labels y ∈ {0, 1}N , number of observations N
Output: Component of D’s misclassiﬁcation error due to data captured by s

function IncrementalLowerBound(v, y, N )

nv = sum(v)
w ← v ∧ y
nw = sum(w)
if nw/nv > 0.5 then

return (nv − nw)/N

else

return nw/N

end if
end function

(cid:46) Number of data captured by s, the last antecedent in Dp
(cid:46) Bit vector indicating data captured by s with label 1
(cid:46) Number of data captured by s with label 1

(cid:46) Misclassiﬁcation error of the rule s → 1

(cid:46) Misclassiﬁcation error of the rule s → 0

Algorithm 4 Incremental objective function (32) used in Algorithm 2.

Input: Bit vector u ∈ {0, 1}N indicating data not captured by Dp’s parent preﬁx, bit
vector v ∈ {0, 1}N indicating data not captured by s, the last antecedent in Dp, bit
vector of class labels y ∈ {0, 1}N , number of observations N
Output: Component of D’s misclassiﬁcation error due to its default rule

function IncrementalObjective(u, v, y, N )

f ← u ∧ ¬ v
nf = sum(f )
g ← f ∧ y
ng = sum(g)
if ng/nf > 0.5 then

(cid:46) Bit vector indicating data not captured by Dp
(cid:46) Number of data not captured by Dp
(cid:46) Bit vector indicating data not captured by Dp with label 1
(cid:46) Number of data not captued by Dp with label 1

return (nf − ng)/N

(cid:46) Misclassiﬁcation error of the default label prediction 1

return ng/N

(cid:46) Misclassiﬁcation error of the default label prediction 0

else

end if
end function

formed from a set of 1000 antecedents, then the maximum queue size in Algorithm 2 will
be smaller than that in Algorithm 1 by a factor of nearly 1000.

29

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

5. Implementation

We implement our algorithm using a collection of optimized data structures that we describe
in this section. First, we explain how we use a preﬁx tree (§5.1) to support the incremental
computations that we motivated in §4. Second, we describe several queue designs that
implement diﬀerent search policies (§5.2). Third, we introduce a symmetry-aware map (§5.3)
to support symmetry-aware pruning (Corollary 16, §3.11). Next, we summarize how these
data structures interact throughout our model of incremental execution (§5.4). In particular,
Algorithms 5 and 6 illustrate many of the computational details from CORELS’ inner
loop, highlighting each of the bounds from §3 that we use to prune the search space. We
additionally describe how we garbage collect our data structures (§5.5). Finally, we explore
how our queue can be used to support custom scheduling policies designed to improve
performance (§5.6).

5.1 Preﬁx Tree

Our incremental computations (§4) require a cache to keep track of preﬁxes that we have
already evaluated and that are also still under consideration by the algorithm. We implement
this cache as a preﬁx tree, a data structure also known as a trie, which allows us to eﬃciently
represent structure shared between related preﬁxes. Each node in the preﬁx tree encodes
an individual rule rk = pk → qk. Each path starting from the root represents a preﬁx, such
that the ﬁnal node in the path also contains metadata associated with that preﬁx. For a
preﬁx dp = (p1, . . . , pK), let ϕ(dp) denote the corresponding node in the trie. The metadata
at node ϕ(dp) supports the incremental computation and includes:

• An index encoding pK, the last antecedent.

• The objective lower bound b(dp, x, y), deﬁned in (5), the central bound in our frame-

work (Theorem 1).

• The lower bound on the default rule misclassiﬁcation error b0(dp, x, y), deﬁned in (28),

to support our equivalent points bound (Theorem 20).

• An indicator denoting whether this node should be deleted (see §5.5).

• A representation of viable extensions of dp, i.e., length K + 1 preﬁx that start with dp

and have not been pruned.

For evaluation purposes and convenience, we store additional information in the preﬁx tree;
for a preﬁx dp with corresponding rule list d = (dp, δp, q0, K), the node ϕ(dp) also stores:

• The length K; equivalently, node ϕ(dp)’s depth in the trie.

• The label prediction qK corresponding to antecedent pK.

• The default rule label prediction q0.

• Ncap, the number of samples captured by preﬁx dp, as in (34).

• The objective value R(d, x, y), deﬁned in (4).

Finally, we note that we implement the preﬁx tree as a custom C++ class.

30

Learning Certifiably Optimal Rule Lists

5.2 Queue

The queue is a worklist that orders exploration over the search space of possible rule lists;
every queue element corresponds to a leaf in the preﬁx tree, and vice versa. In our imple-
mentation, each queue element points to a leaf; when we pop an element oﬀ the queue, we
use the leaf’s metadata to incrementally evaluate the corresponding preﬁx’s children.

We order entries in the queue to implement several diﬀerent search policies. For exam-
ple, a ﬁrst-in-ﬁrst-out (FIFO) queue implements breadth-ﬁrst search (BFS), and a priority
queue implements best-ﬁrst search. In our experiments (§6), we use the C++ Standard
Template Library (STL) queue and priority queue to implement BFS and best-ﬁrst search,
respectively. For CORELS, priority queue policies of interest include ordering by the lower
bound, the objective, or more generally, any function that maps preﬁxes to real values;
stably ordering by preﬁx length and inverse preﬁx length implement BFS and depth-ﬁrst
search (DFS), respectively. In our released code, we present a uniﬁed implementation, where
we use the STL priority queue to support BFS, DFS, and several best-ﬁrst search policies.
As we demonstrate in our experiments (§6.8), we ﬁnd that using a custom search strategy,
such as ordering by the lower bound, usually leads to a faster runtime than BFS.

We motivate the design of additional custom search strategies in §5.6. In preliminary
work (not shown), we also experimented with stochastic exploration processes that bypass
the need for a queue by instead following random paths from the root to leaves; developing
such methods could be an interesting direction for future work. We note that these search
policies are referred to as node selection strategies in the MIP literature. Strategies such as
best-ﬁrst (best-bound) search and DFS are known as static methods, and the framework
we present in §5.6 has the spirit of estimate-based methods (Linderoth and Savelsbergh,
1999).

5.3 Symmetry-aware Map

The symmetry-aware map supports the symmetry-aware pruning justiﬁed in §3.10. In our
implementation, we speciﬁcally leverage our permutation bound (Corollary 16), though it is
also possible to directly exploit the more general equivalent support bound (Theorem 15).
We use the C++ STL unordered map to keep track of the best known ordering of each
evaluated set of antecedents. The keys of our symmetry-aware map encode antecedents in
canonical order, i.e., antecedent indices in numerically sorted order, and we associate all
permutations of a set of antecedents with a single key. Each key maps to a value that
encodes the best known preﬁx in the permutation group of the key’s antecedents, as well
as the objective lower bound of that preﬁx.

Before we consider adding a preﬁx dp to the trie and queue, we check whether the map
already contains a permutation π(dp) of that preﬁx. If no such permutation exists, then
we insert dp into the map, trie, and queue. Otherwise, if a permutation π(dp) exists and
the lower bound of dp is better than that of π(dp), i.e., b(dp, x, y) < b(π(dp), x, y), then we
update the map and remove π(dp) and its entire subtree from the trie; we also insert dp into
the trie and queue. Otherwise, if there exists a permutation π(dp) such that b(π(dp), x, y) ≤
b(dp, x, y), then we do nothing, i.e., we do not insert dp into any data structures.

31

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

5.4 Incremental Execution

Mapping our algorithm to our data structures produces the following execution strategy,
which we also illustrate in Algorithms 5 and 6. We initialize the current best objective Rc
and rule list dc. While the trie contains unexplored leaves, a scheduling policy selects the
next preﬁx dp to extend; in our implementation, we pop elements from a (priority) queue,
until the queue is empty. Then, for every antecedent s that is not in dp, we construct a new
preﬁx Dp by appending s to dp; we incrementally calculate the lower bound b(Dp, x, y),
the objective R(D, x, y), of the associated rule list D, and other quantities used by our
algorithm, summarized by the metadata ﬁelds of the (potential) preﬁx tree node ϕ(Dp).

If the objective R(D, x, y) is less than the current best objective Rc, then we update Rc
and dc. If the lower bound of the new preﬁx Dp is less than the current best objective,
then as described in §5.3, we query the symmetry-aware map for Dp; if we insert d(cid:48)
p into
the symmetry-aware map, then we also insert it into the trie and queue. Otherwise, then
by our hierarchical lower bound (Theorem 1), no extension of Dp could possibly lead to a
rule list with objective better than Rc, thus we do not insert Dp into the tree or queue. We
also leverage our other bounds from §3 to aggressively prune the search space; we highlight
each of these bounds in Algorithms 5 and 6, which summarize the computations and data
structure operations performed in CORELS’ inner loop. When there are no more leaves
to explore, i.e., the queue is empty, we output the optimal rule list. We can optionally
terminate early according to some alternate condition, e.g., when the size of the preﬁx tree
exceeds some threshold.

5.5 Garbage Collection

During execution, we garbage collect the trie. Each time we update the minimum objective,
we traverse the trie in a depth-ﬁrst manner, deleting all subtrees of any node with lower
bound larger than the current minimum objective. At other times, when we encounter a
node with no children, we prune upwards, deleting that node and recursively traversing
the tree towards the root, deleting any childless nodes. This garbage collection allows us
to constrain the trie’s memory consumption, though in our experiments we observe the
minimum objective to decrease only a small number of times.

In our implementation, we cannot immediately delete preﬁx tree leaves because each
corresponds to a queue element that points to it. The C++ STL priority queue is a wrapper
container that prevents access to the underlying data structure, and thus we cannot access
elements in the middle of the queue, even if we know the relevant identifying information. We
therefore have no way to update the queue without iterating over every element. We address
this by marking preﬁx tree leaves that we wish to delete (see §5.1), deleting the physical
nodes lazily, after they are popped from the queue. Later, in our section on experiments (§6),
we refer to two diﬀerent queues that we deﬁne here: the physical queue corresponds to the
C++ queue, and thus all preﬁx tree leaves, and the logical queue corresponds only to those
preﬁx tree leaves that have not been marked for deletion.

32

Learning Certifiably Optimal Rule Lists

Algorithm 5 The inner loop of CORELS, which evaluates all children of a preﬁx dp.
1[xn ∈ eu][yn = qu]

Deﬁne z ∈ {0, 1}N , s.t. zn = (cid:80)U

u=1

Deﬁne b(dp, x, y) and u = ¬ cap(x, dp)
for s in S if s not in dp then do

(cid:46) eu is the equivalent points set containing xn and qu is the minority class label of eu (§3.14)
(cid:46) u is a bit vector indicating data not captured by dp
(cid:46) Evaluate all of dp’s children
(cid:46) Branch: Generate child Dp
(cid:46) Bit vector indicating data captured by s in Dp
(cid:46) Number of data captured by s, the last antecedent in Dp

(cid:46) Lower bound on antecedent support (Theorem10)

(cid:46) Bit vector indicating data captured by s with label 1
(cid:46) Number of data captured by s with label 1

(cid:46) Number of correct predictions by the new rule s → 1

(cid:46) Number of correct predictions by the new rule s → 0

Dp ← (dp, s)
v ← u ∧ cap(x, s)
nv = sum(v)
if nv/N < λ then
continue

end if
w ← v ∧ y
nw = sum(w)
if nw/nv ≥ 0.5 then

nc ← nw

else

nc ← nv − nw

end if
if nc/N < λ then
continue

(cid:46) Lower bound on accurate antecedent support (Theorem 11)

end if
δb ← (nv − nc)/N
b(Dp, x, y) ← b(dp, x, y) + λ + δb
if b(Dp, x, y) ≥ Rc then

(cid:46) Misclassiﬁcation error of the new rule
(cid:46) Incremental lower bound (30)
(cid:46) Hierarchical objective lower bound (Theorem 1)

continue

end if
f ← u ∧ ¬ v
nf = sum(f )
g ← f ∧ y
ng = sum(g)
if ng/nf ≥ 0.5 then

δR ← (nf − ng)/N

else

δR ← ng/N

end if
R(D, x, y) ← b(Dp, x, y) + δR
D ← (Dp, ∆p, Q0, K + 1)
if R(D, x, y) < Rc then

(cid:46) Bit vector indicating data not captured by Dp
(cid:46) Number of data not captured by Dp
(cid:46) Bit vector indicating data not captured by Dp with label 1
(cid:46) Number of data not captued by Dp with label 1

(cid:46) Misclassiﬁcation error of the default label prediction 1

(cid:46) Misclassiﬁcation error of the default label prediction 0

(cid:46) Incremental objective (32)
(cid:46) ∆p, Q0 are set in the incremental functions

(dc, Rc) ← (D, R(D, x, y))
(cid:46) Update current best rule list and objective
GarbageCollectPrefixTree(Rc) (cid:46) Delete nodes with lower bound ≥ Rc − λ (§5.5),
using the Lookahead bound (Lemma 2)
(cid:46) Lower bound on the default rule misclassiﬁcation
error deﬁned in (28)
(cid:46) Equivalent points bound (Theorem 20)
combined with the Lookahead bound (Lemma 2)

end if
b0(Dp, x, y) ← sum(f ∧ z)/N
b ← b(Dp, x, y) + b0(Dp, x, y)
if b + λ ≥ Rc then

continue

end if
CheckMapAndInsert(Dp, b)

end for

(cid:46) Check the Permutation bound (Corollary 16) and
possibly insert Dp into data structures (Algorithm 6)

33

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Algorithm 6 Possibly insert a preﬁx into CORELS’ data structures, after ﬁrst checking the
symmetry-aware map, which supports search space pruning triggered by the permutation
bound (Corollary 16). For further context, see Algorithm 5.

T is the preﬁx tree (§5.1)
Q is the queue, for concreteness, a priority queue ordered by the lower bound (§5.2)
P is the symmetry-aware map (§5.3)

function CheckMapAndInsert(Dp, b)

π0 ← sort(Dp)
(Dπ, bπ) ← P .ﬁnd(π0)
if Dπ exists then
if b < bπ then

P .update(π0, (Dp, b))
T .delete subtree(Dπ)
T .insert(ϕ(Dp))
Q.push(Dp, b)

else

end if

else

P .insert(π0, (Dp, b))
T .insert(ϕ(Dp))
Q.push(Dp, b)

end if
end function

(cid:46) Dp’s antecedents in canonical order
(cid:46) Look for a permutation of Dp

(cid:46) Dp is better than Dπ
(cid:46) Replace Dπ with Dp in the map
(cid:46) Delete Dπ and its subtree from the preﬁx tree
(cid:46) Add node for Dp to the preﬁx tree
(cid:46) Add Dp to the queue

(cid:46) Add Dp to the map
(cid:46) Add node for Dp to the preﬁx tree
(cid:46) Add Dp to the queue

pass

(cid:46) Dp is inferior to Dπ, thus do not insert it into any data structures

5.6 Custom Scheduling Policies

In our setting, an ideal scheduling policy would immediately identify an optimal rule list,
and then certify its optimality by systematically eliminating the remaining search space.
This motivates trying to design scheduling policies that tend to quickly ﬁnd optimal rule
lists. When we use a priority queue to order the set of preﬁxes to evaluate next, we are free
to implement diﬀerent scheduling policies via the ordering of elements in the queue. This
motivates designing functions that assign higher priorities to ‘better’ preﬁxes that we believe
are more likely to lead to optimal rule lists. We follow the convention that priority queue
elements are ordered by keys, such that keys with smaller values have higher priorities.

We introduce a custom class of functions that we call curiosity functions. Broadly, we
think of the curiosity of a rule list d as the expected objective value of another rule list d(cid:48)
that is related to d; diﬀerent models of the relationship between d and d(cid:48) lead to diﬀerent
curiosity functions. In general, the curiosity of d is, by deﬁnition, equal to the sum of the
expected misclassiﬁcation error and the expected regularization penalty of d(cid:48):

C(dp, x, y) ≡ E[R(d(cid:48), x, y)] = E[(cid:96)(d(cid:48)

p, δ(cid:48)

p, x, y)] + λE[K(cid:48)].

(33)

34

Learning Certifiably Optimal Rule Lists

Next, we describe a simple curiosity function for a rule list d with preﬁx dp. First,

let Ncap denote the number of observations captured by dp, i.e.,

Ncap ≡

cap(xn, dp).

(34)

N
(cid:88)

n=1

We now describe a model that generates another rule list d(cid:48) = (d(cid:48)
sume that preﬁx d(cid:48)
antecedent in d(cid:48)
then, the expected length of d(cid:48)

0, K(cid:48)) from dp. As-
p starts with dp and captures all the data, such that each additional
p captures as many ‘new’ observations as each antecedent in dp, on average;

p, δ(cid:48)

p, q(cid:48)

p is

E[K(cid:48)] =

N
Ncap/K

.

Furthermore, assume that each additional antecedent in d(cid:48)
antecedent in dp, on average, thus the expected misclassiﬁcation error of d(cid:48)

p makes as many mistakes as each

p is

E[(cid:96)(d(cid:48)

p, δ(cid:48)

p, x, y)] = E[(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y)] + E[(cid:96)0(d(cid:48)

= E[(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y)] = E[K(cid:48)]

p, q(cid:48)
0, x, y)]
(cid:18) (cid:96)p(dp, δp, x, y)
K

(cid:19)

.

0, x, y) is zero because we assume
p captures all the data. Inserting (35) and (36) into (33) thus gives curiosity for this

Note that the default rule misclassiﬁcation error (cid:96)0(d(cid:48)
that d(cid:48)
model:

p, q(cid:48)

(35)

(36)

C(dp, x, y) =

(cid:96)p(dp, δp, x, y) + λK

(cid:19)

(cid:19) (cid:18)

(cid:18) N
Ncap

(cid:32)

=

1
N

N
(cid:88)

n=1

(cid:33)−1

cap(xn, dp)

b(dp, x, y) =

b(dp, x, y)
supp(dp, x)

,

where for the second equality, we used the deﬁnitions in (34) of Ncap and in (5) of dp’s lower
bound, and for the last equality, we used the deﬁnition in (2) of dp’s normalized support.

The curiosity for a preﬁx dp is thus also equal to its objective lower bound, scaled by
the inverse of its normalized support. For two preﬁxes with the same lower bound, curiosity
gives higher priority to the one that captures more data. This is a well-motivated scheduling
strategy if we model preﬁxes that extend the preﬁx with smaller support as having more
‘potential’ to make mistakes. We note that using curiosity in practice does not introduce
new bit vector or other expensive computations; during execution, we can calculate curiosity
as a simple function of already derived quantities.

In preliminary experiments, we observe that using a priority queue ordered by curiosity
sometimes yields a dramatic reduction in execution time, compared to using a priority queue
ordered by the objective lower bound. Thus far, we have observed signiﬁcant beneﬁts on
speciﬁc small problems, where the structure of the solutions happen to render curiosity
particularly eﬀective (not shown). Designing and studying other ‘curious’ functions, that
are eﬀective in more general settings, is an exciting direction for future work.

35

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Data set

Prediction problem

N

ProPublica Two-year recidivism
NYPD
Weapon possession
Weapon possession
NYCLU

6,907
325,800
29,595

Positive
fraction
0.46
0.03
0.05

Resample
training set
No
Yes
Yes

Training
set size
6,217
566,839
50,743

Test
set size
692
32,580
2,959

Table 1: Summary of data sets and prediction problems. The last ﬁve columns report the
total number of observations, the fraction of observations with the positive class
label, whether we resampled the training set due to class imbalance, and the sizes
of each training and test set in our 10-fold cross-validation studies.

6. Experiments

Our experimental analysis addresses ﬁve questions: How does CORELS’ predictive perfor-
mance compare to that of COMPAS scores and other algorithms? (§6.4, §6.5, and §6.6)
How does CORELS’ model size compare to that of other algorithms? (§6.6) How rapidly do
the objective value and its lower bound converge, for diﬀerent values of the regularization
parameter λ? (§6.7) How much does each of the implementation optimizations contribute
to CORELS’ performance? (§6.8) How rapidly does CORELS prune the search space? (§6.7
and §6.8) Before proceeding, we ﬁrst describe our computational environment (§6.1), as
well as the data sets and prediction problems we use (§6.2), and then in Section 6.3 show
example optimal rule lists found by CORELS.

6.1 Computational Environment

All timed results ran on a server with an Intel Xeon E5-2699 v4 (55 MB cache, 2.20 GHz)
processor and 264 GB RAM, and we ran each timing measurement separately, on a single
hardware thread, with nothing else running on the server. Except where we mention a
memory constraint, all experiments can run comfortably on smaller machines, e.g., a laptop
with 16 GB RAM.

6.2 Data Sets and Prediction Problems

Our evaluation focuses on two socially-important prediction problems associated with re-
cent, publicly-available data sets. Table 1 summarizes the data sets and prediction problems,
and Table 2 summarizes feature sets extracted from each data set, as well as antecedent sets
we mine from these feature sets. We provide some details next. For further details about
data sets, preprocessing steps, and antecedent mining, see Appendix E.

6.2.1 Recidivism Prediction

For our ﬁrst problem, we predict which individuals in the ProPublica COMPAS data
set (Larson et al., 2016) recidivate within two years. This data set contains records for
all oﬀenders in Broward County, Florida in 2013 and 2014 who were given a COMPAS
score pre-trial. Recidivism is deﬁned as being charged with a new crime within two years

36

Learning Certifiably Optimal Rule Lists

Data set

ProPublica
ProPublica
NYPD
NYPD
NYCLU

Feature Categorical Binary
features
attributes
13
6
17
7
28
5
20
3
28
5

set
A
B
C
D
E

Mined
antecedents
122
189
28
20
46

of clauses
2
2
1
1
1

No
No
No
No
Yes

Max number Negations

Table 2: Summary of feature sets and mined antecedents. The last ﬁve columns report
the number of categorical attributes, the number of binary features, the average
number of mined antecedents, the maximum number of clauses in each antecedent,
and whether antecedents include negated clauses.

after receiving a COMPAS assessment; the article by Larson et al. (2016), and their code,3
provide more details about this deﬁnition. From the original data set of records for 7,214
individuals, we identify a subset of 6,907 records without missing data. For the majority of
our analysis, we extract a set of 13 binary features (Feature Set A), which our antecedent
mining framework combines into M = 122 antecedents, on average (folds ranged from con-
taining 121 to 123 antecedents). We also consider a second, similar antecedent set in §6.3,
derived from a superset of Feature Set A that includes 4 additional binary features (Feature
Set B).

6.2.2 Weapon Prediction

For our second problem, we use New York City stop-and-frisk data to predict whether a
weapon will be found on a stopped individual who is frisked or searched. For experiments
in Sections 6.3 and 6.5 and Appendix G, we compile data from a database maintained by
the New York Police Department (NYPD) (New York Police Department, 2016), from years
2008-2012, following Goel et al. (2016). Starting from 2,941,390 records, each describing an
incident involving a stopped person, we ﬁrst extract 376,488 records where the suspected
crime was criminal possession of a weapon (CPW).4 From these, we next identify a subset
of 325,800 records for which the individual was frisked and/or searched; of these, criminal
possession of a weapon was identiﬁed in only 10,885 instances (about 3.3%). Resampling
due to class imbalance, for 10-fold cross-validation, yields training sets that each contain
566,839 datapoints. (We form corresponding test sets without resampling.) From a set of
5 categorical features, we form a set of 28 single-clause antecedents corresponding to 28
binary features (Feature Set C). We also consider another, similar antecedent set, derived
from a subset of Feature Set C that excludes 8 location-speciﬁc binary features (Feature
Set D).

In Sections 6.3, 6.6, 6.7, and 6.8, we also use a smaller stop-and-frisk data set, derived by
the NYCLU from the NYPD’s 2014 data (New York Civil Liberties Union, 2014). From the

3. Data and code used in the analysis by Larson et al. (2016) can be found at https://github.com/

propublica/compas-analysis.

4. We ﬁlter for records that explicitly match the string ‘CPW’; we note that additional records, after

converting to lowercase, contain strings such as ‘cpw’ or ‘c.p.w.’

37

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

Figure 3: Example optimal rule lists that predict two-year recidivism for the ProPublica
data set (Feature Set B, M = 189), found by CORELS (λ = 0.005), across 10
cross-validation folds. While some input antecedents contain features for race, no
optimal rule list includes such an antecedent. Every optimal rule list is the same
or similar to one of these examples, with preﬁxes containing the same rules, up
to a permutation, and same default rule.

original data set of 45,787 records, each describing an incident involving a stopped person,
we identify a subset of 29,595 records for which the individual was frisked and/or searched.
Of these, criminal possession of a weapon was identiﬁed in about 5% of instances. As with
the larger NYPD data set, we resample the data to form training sets (but not to form
test sets). From the same set of 5 categorical features as in Feature Set C, we form a set of
M = 46 single-clause antecedents, including negations (Feature Set E).

6.3 Example Optimal Rule Lists

To motivate Feature Set A, described in Appendix E, which we used in most of our analysis
of the ProPublica data set, we ﬁrst consider Feature Set B, a larger superset of features.

Figure 3 shows optimal rule lists learned by CORELS, using Feature Set B, which
additionally includes race categories from the ProPublica data set (African American, Cau-
casian, Hispanic, Other5). For Feature Set B, our antecedent mining procedure generated an
average of 189 antecedents, across folds. None of the optimal rule lists contain antecedents
that directly depend on race; this motivated our choice to exclude race, by using Feature
Set A, in our subsequent analysis. For both feature sets, we replaced the original ProPublica
age categories (<25, 25-45, >45) with a set that is more ﬁne-grained for younger individuals
(18-20, 21-22, 23-25, 26-45, >45). Figure 4 shows example optimal rule lists that CORELS
learns for the ProPublica data set (Feature Set A, λ = 0.005), using 10-fold cross validation.
Figures 5 and 6 show example optimal rule lists that CORELS learns for the NYCLU
(λ = 0.01) and NYPD data sets. Figure 6 shows optimal rule lists that CORELS learns for
the larger NYPD data set.

While our goal is to provide illustrative examples, and not to provide a detailed analysis
nor to advocate for the use of these speciﬁc models, we note that these rule lists are short and
easy to understand. For the examples and regularization parameter choices in this section,

5. We grouped the original Native American (<0.003), Asian (<0.005), and Other (<0.06) categories.

38

Learning Certifiably Optimal Rule Lists

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

Figure 4: Example optimal rule lists that predict two-year recidivism for the ProPublica
data set (Feature Set A, M = 122), found by CORELS (λ = 0.005), across 10
cross-validation folds. Feature Set A is a subset of Feature Set B (Figure 3) that
excludes race features. Optimal rule lists found using the two feature sets are
very similar. The upper and lower rule lists are representative of 7 and 3 folds,
respectively. Each of the remaining 8 solutions is the same or similar to one of
these, with preﬁxes containing the same rules, up to a permutation, and the same
default rule. See Figure 20 in Appendix F for a complete listing.

if (location = transit authority) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = suspicious object) then predict yes
else predict no

Figure 5: An example rule list that predicts whether a weapon will be found on a stopped in-
dividual who is frisked or searched, for the NYCLU stop-and-frisk data set. Across
10 cross-validation folds, the other optimal rule lists found by CORELS (λ = 0.01)
contain the same or equivalent rules, up to a permutation. See also Figure 23 in
Appendix F.

the optimal rule lists are relatively robust across cross-validation folds: the rules are nearly
the same, up to permutations of the preﬁx rules. For smaller values of the regularization
parameter, we observe less robustness, as rule lists are allowed to grow in length. For the
sets of optimal rule lists represented in Figures 3, 4, and 5, each set could be equivalently
expressed as a DNF rule; e.g., this is easy to see when the preﬁx rules all predict the
positive class label and the default rule predicts the negative class label. Our objective is
not designed to enforce any of these properties, though some may be seen as desirable.

As we demonstrate in §6.6, optimal rule lists learned by CORELS achieve accuracies
that are competitive with a suite of other models, including black box COMPAS scores. See
Appendix F for additional listings of optimal rule lists found by CORELS, for each of our
prediction problems, across cross-validation folds, for diﬀerent regularization parameters λ.

39

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Weapon prediction (λ = 0.01, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else predict no

Weapon prediction (λ = 0.01, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (inside or outside = outside) then predict no
else predict yes

Weapon prediction (λ = 0.005, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

Weapon prediction (λ = 0.005, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

Figure 6: Example optimal rule lists for the NYPD stop-and-frisk data set, found by
CORELS. Feature Set C contains attributes for ‘location’ and ‘city’, while Fea-
ture Set D does not. For each choice of regularization parameter and feature set,
the rule lists learned by CORELS, across all 10 cross-validation folds, contain
the same or equivalent rules, up to a permutation, with the exception of a single
fold (Feature Set C, λ = 0.005). For a complete listing, see Figures 21 and 22 in
Appendix F.

6.4 Comparison of CORELS to the Black Box COMPAS Algorithm

The accuracies of rule lists learned by CORELS are competitive with scores generated by
the black box COMPAS algorithm at predicting two-year recidivism for the ProPublica
data set (Figure 9). Across 10 cross-validation folds, optimal rule lists learned by CORELS
(Figure 4, λ = 0.005) have a mean test accuracy of 0.665, with standard deviation 0.018. The
COMPAS algorithm outputs scores between 1 and 10, representing low (1-4), medium (5-7),
and high (8-10) risk for recidivism. As in the analysis by Larson et al. (2016), we interpret a
medium or high score as a positive prediction for two-year recidivism, and a low score as a
negative prediction. Across the 10 test sets, the COMPAS algorithm scores obtain a mean
accuracy of 0.660, with standard deviation 0.019.

Figure 7 shows that CORELS and COMPAS perform similarly across both black and
white individuals. Both algorithms have much higher true positive rates (TPR’s) and false
positive rates (FPR’s) for blacks than whites (left), and higher true negative rates (TNR’s)

40

Learning Certifiably Optimal Rule Lists

Figure 7: Comparison of TPR and FPR (left), as well as TNR and FNR (right), for diﬀerent
races in the ProPublica data set, for CORELS and COMPAS, across 10 cross-
validation folds.

and false negative rates (FNR’s) for whites than blacks (right). The fact that COMPAS has
higher FPR’s for blacks and higher FNR’s for whites was a central observation motivating
ProPublica’s claim that COMPAS is racially biased (Larson et al., 2016). The fact that
CORELS’ models are so simple, with almost the same results as COMPAS, and contain
only counts of past crimes, age, and gender, indicates possible explanations for the uneven
predictions of both COMPAS and CORELS among blacks and whites. In particular, blacks
evaluated within Broward County tend to be younger and have longer criminal histories
within the data set, (on average, 4.4 crimes for blacks versus 2.6 crimes for whites) leading
to higher FPR’s for blacks and higher FNR’s for whites. This aspect of the data could help
to explain why ProPublica concluded that COMPAS was racially biased.

Similar observations have been reported for other datasets, namely that complex ma-
chine learning models do not have an advantage over simpler transparent models (Tollenaar
and van der Heijden, 2013; Bushway, 2013; Zeng et al., 2017). There are many deﬁnitions of
fairness, and it is not clear whether CORELS’ models are fair either, but it is much easier to
debate about the fairness of a model when it is transparent. Additional fairness constraints
or transparency constraints can be placed on CORELS’ models if desired, though one would
need to edit our bounds (§3) and implementation (§5) to impose more constraints.

Regardless of whether COMPAS is racially biased (which our analysis does not indicate is
necessarily true as long as criminal history and age are allowed to be considered as features),
COMPAS may have many other fairness defects that might be considered serious. Many of
COMPAS’s survey questions are direct inquiries about socioeconomic status. For instance,
a sample COMPAS survey6 asks: “Is it easy to get drugs in your neighborhood?,” “How
often do you have barely enough money to get by?,” “Do you frequently get jobs that don’t
pay more than minimum wage?,” “How often have you moved in the last 12 months?”
COMPAS’s survey questions also ask about events that were not caused by the person who

6. A sample COMPAS survey contributed by Julia Angwin, ProPublica, can be found at https://www.

documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html.

41

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

is being evaluated, such as: “If you lived with both parents and they later separated, how
old were you at the time?,” “Was one of your parents ever sent to jail or prison?,” “Was
your mother ever arrested, that you know of?”

The fact that COMPAS requires over 130 questions to be answered, many of whose
answers may not be veriﬁable, means that the computation of the COMPAS score is prone
to errors. Even the Arnold Foundation’s “public-safety assessment” (PSA) score—which is
completely transparent, and has only 9 factors—has been miscalculated in serious criminal
trials, leading to a recent lawsuit (Westervelt, 2017). It is substantially more diﬃcult to
obtain the information required to calculate COMPAS scores than PSA scores (with over 14
times the number of survey questions). This signiﬁcant discrepancy suggests that COMPAS
scores are more fallible than PSA scores, as well as even simpler models, like those produced
by CORELS. Some of these problems could be alleviated by using only data within electronic
records that can be automatically calculated, instead of using information entered by hand
and/or collected via subjective surveys.

The United States government pays Northpointe (now called Equivant) to use COMPAS.
In light of our observations that CORELS is as accurate as COMPAS on a real-world data
set where COMPAS is used in practice, CORELS predicts similarly to COMPAS for both
blacks and whites, and CORELS’ models are completely transparent, it is not clear what
value COMPAS scores possess. Our experiments also indicate that the proprietary survey
data required to compute COMPAS scores has not boosted its prediction accuracy above
that of transparent models in practice.

Risk predictions are important for the integrity of the judicial system; judges cannot be
expected to keep entire databases in their heads to calculate risks, whereas models (when
used correctly) can help to ensure equity. Risk prediction models also have the potential
to heavily impact how eﬃcient the judicial system is, in terms of bail and parole decisions;
eﬃciency in this case means that dangerous individuals are not released, whereas non-
dangerous individuals are granted bail or parole. High stakes decisions, such as these, are
ideal applications for machine learning algorithms that produce transparent models from
high dimensional data.

Currently, justice system data does not support highly accurate risk predictions, but
current risk models are useful in practice, and these risk predictions will become more
accurate as more and higher quality data are made available.

6.5 Comparison of CORELS to a Heuristic Model for Weapon Prediction

CORELS generates simple, accurate models for the task of weapon prediction, using the
NYPD stop-and-frisk data set. Our approach oﬀers a principled alternative to heuristic
models proposed by Goel et al. (2016), who develop a series of regression models to analyze
racial disparities in New York City’s stop-and-frisk policy for a related, larger data set. In
particular, the authors arrive at a heuristic that they suggest could potentially help police
oﬃcers more eﬀectively decide when to frisk and/or search stopped individuals, i.e., when
such interventions are likely to discover criminal possession of a weapon (CPW). Starting
from a full regression model with 7,705 variables, the authors reduce this to a smaller model
with 98 variables; from this, they keep three variables with the largest coeﬃcients. This gives

42

Learning Certifiably Optimal Rule Lists

a heuristic model of the form ax + by + cz ≥ T , where

x = 1[stop reason = suspicious object]
y = 1[stop reason = suspicious bulge]
z = 1[additional circumstances = sights and sounds of criminal activity],

and T is a threshold, such that the model predicts CPW when the threshold is met or
exceeded. We focus on their approach that uses a single threshold, rather than precinct-
speciﬁc thresholds. To increase ease-of-use, the authors round the coeﬃcients to the nearest
integers, which gives (a, b, c) = (3, 1, 1); this constrains the threshold to take one of six
values, T ∈ {0, 1, 2, 3, 4, 5}. To employ this heuristic model in the ﬁeld, “. . . oﬃcers simply
need to add at most three small, positive integers . . . and check whether the sum exceeds a
ﬁxed threshold. . . ” (Goel et al., 2016).

Figure 8 directly compares various models learned by CORELS to the heuristic models,
using the same data set as Goel et al. (2016) and 10-fold cross-validation. Recall that we
train on resampled data to correct for class imbalance; we evaluate with respect to test sets
that have been formed without resampling. For CORELS, the models correspond to the
rule lists illustrated in Figure 6 from Section 6.3, and Figures 21 and 22 in Appendix F, we
consider both Feature Sets C and D and both regularization parameters λ = 0.005 and 0.01.
The top panel plots the fraction of weapons recovered as a function of the fraction of stops
where the individual was frisked and/or searched. Goel et al. (2016) target models that
eﬃciently recover a majority of weapons (while also minimizing racial disparities, which
we do not address here). Interestingly, the models learned by CORELS span a signiﬁcant
region that is not available to the heuristic model, which would require larger or non-integer
parameters to access the region. The region is possibly desirable, since it includes models
(λ = 0.005, bright red) that recover a majority (≥ 50%) of weapons (that are known in the
data set). More generally, CORELS’ models all recover at least 40% of weapons on average,
i.e., more weapons than any of the heuristic models with T ≥ 2, which recover less than
25% of weapons on average. At the same time, CORELS’ models all require well under 25%
of stops—signiﬁcantly less than the heuristic model with T = 1, which requires over 30% of
stops to recover a fraction of weapons comparable to the CORELS model that recovers the
most weapons.

The bottom panel in Figure 8 plots both TPR and FPR and labels model size, for each
of the models in the top panel. For the heuristic, we deﬁne model size as the number of
model parameters; for CORELS, we use the number of rules in the rule list, which is equal
to the number of leaves when we view a rule list as a decision tree. The heuristic models all
have 4 parameters, while the diﬀerent CORELS models have either 3 or approximately 5
rules. CORELS’ models are thus approximately as small, interpretable, and transparent as
the heuristic models; furthermore, their predictions are straightforward to compute, without
even requiring arithmetic.

6.6 Predictive Performance and Model Size for CORELS and Other Algorithms

We ran a 10-fold cross validation experiment using CORELS and eight other algorithms:
logistic regression, support vector machines (SVM), AdaBoost, CART, C4.5, random for-

43

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 8: Weapon prediction with the NYPD stop-and-frisk data set, for various models
learned by CORELS and the heuristic model by Goel et al. (2016), across 10
cross-validation folds. Note that the fraction of weapons recovered (top) is equal
to the TPR (bottom, open markers). Markers above the dotted horizontal lines at
the value 0.5 correspond to models that recover a majority of weapons (that are
known in the data set). Top: Fraction of weapons recovered as a function of the
fraction of stops where the individual was frisked and/or searched. In the legend,
entries for CORELS (red markers) indicate the regularization parameter (λ) and
whether or not extra location features were used (“location”); entries for the
heuristic model (blue markers) indicate the threshold value (T ). The results we
report for the heuristic model are our reproduction of the results reported in Figure
9 by Goel et al. (2016) (ﬁrst four open circles in that ﬁgure, from left to right;
we exclude the trivial open circle showing 100% of weapons recovered at 100% of
stops, obtained by setting the threshold at 0). Bottom: Comparison of TPR (open
markers) and FPR (solid markers) for various CORELS and heuristic models.
Models are sorted left-to-right by TPR. Markers and abbreviated horizontal tick
labels correspond to the legend in the top ﬁgure. Numbers in the plot label model
size; there was no variation in model size across folds, except for a single fold for
CORELS (λ = 0.005, Feature Set C), which found a model of size 6.

44

Learning Certifiably Optimal Rule Lists

Figure 9: Two-year recidivism prediction for the ProPublica COMPAS data set. Compari-
son of CORELS and a panel of nine other algorithms: logistic regression (GLM),
support vector machines (SVM), AdaBoost, CART, C4.5, random forests (RF),
RIPPER, scalable Bayesian rule lists (SBRL), and COMPAS. For CORELS, we
use regularization parameter λ = 0.005.

est (RF), RIPPER, and scalable Bayesian rule lists (SBRL).7 We use standard R packages,
with default parameter settings, for the ﬁrst seven algorithms.8 We use the same antecedent
sets as input to the two rule list learning algorithms, CORELS and SBRL; for the other
algorithms, the inputs are binary feature sets corresponding to the single clause antecedents
in the aforementioned antecedent sets (see Appendix E).

Figure 9 shows that for the ProPublica data set, there were no statistically signiﬁcant
diﬀerences in test accuracies across algorithms, the diﬀerence between folds was far larger
than the diﬀerence between algorithms. These algorithms also all perform similarly to the
black box COMPAS algorithm. Figure 10 shows that for the NYCLU data set, logistic
regression, SVM, and AdaBoost have the highest TPR’s and also the highest FPR’s; we show
TPR and FPR due to class imbalance. For this problem, CORELS obtains an intermediate
TPR, compared to other algorithms, while achieving a relatively low FPR. We conclude
that CORELS produces models whose predictive performance is comparable to or better
than those found via other algorithms.

Figures 11 and 12 summarize diﬀerences in predictive performance and model size for
CORELS and other tree (CART, C4.5) and rule list (RIPPER, SBRL) learning algorithms.
Here, we vary diﬀerent algorithm parameters, and increase the number of iterations for
SBRL to 10,000. For two-year recidivism prediction with the ProPublica data set (Fig-
ure 11), we plot both training and test accuracy, as a function of the number of leaves
in the learned model. Due to class imbalance for the weapon prediction problem with the
NYCLU stop-and-frisk data set (Figure 12), we plot both true positive rate (TPR) and
false positive rate (FPR), again as a function of the number of leaves. For both problems,
CORELS can learn short rule lists without sacriﬁcing predictive performance. For listings
of example optimal rule lists that correspond to the results for CORELS summarized here,

7. For SBRL, we use the C implementation at https://github.com/Hongyuy/sbrlmod. By default, SBRL

sets η = 3, λ = 9, the number of chains to 11 and iterations to 1,000.

8. For CART, C4.5 (J48), and RIPPER, we use the R packages rpart, RWeka, and caret, respectively. By
default, CART uses complexity parameter cp = 0.01 and C4.5 uses complexity parameter C = 0.25.

45

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 10: TPR (left) and FPR (right) for the test set, for CORELS and a panel of seven
other algorithms, for the weapon prediction problem with the NYCLU stop-
and-frisk data set. Means (white squares), standard deviations (error bars), and
values (colors correspond to folds), for 10-fold cross-validation experiments. For
CORELS, we use λ = 0.01. Note that we were unable to execute RIPPER for
the NYCLU problem.

Figure 11: Training and test accuracy as a function of model size, across diﬀerent methods,
for two-year recidivism prediction with the ProPublica COMPAS data set. In
the legend, numbers in parentheses are algorithm parameters that we vary for
CORELS (λ), CART (cp), C4.5 (C), and SBRL (η, λ, i), where i is the num-
ber of iterations. Legend markers and error bars indicate means and standard
deviations, respectively, across cross-validation folds. Small circles mark training
accuracy means. None of the models exhibit signiﬁcant overﬁtting; mean training
accuracy never exceeds mean test accuracy by more than about 0.01.

see Appendix F. Also see Figure 25 in Appendix G; it uses the larger NYPD data set and
is similar to Figure 12.

In Figure 4, we used CORELS to identify short rule lists, depending on only three
features—age, prior convictions, and sex—that achieve test accuracy comparable to COM-
PAS (Figure 9, also see Angelino et al., 2017). If we restrict CORELS to search the space
of rule lists formed from only age and prior convictions (λ = 0.005), the optimal rule lists it

46

Learning Certifiably Optimal Rule Lists

Figure 12: TPR (top) and FPR (bottom) for the test set, as a function of model size, across
diﬀerent methods, for weapon prediction with the NYCLU stop-and-frisk data
set. In the legend, numbers in parentheses are algorithm parameters, as in Fig-
ure 11. Legend markers and error bars indicate means and standard deviations,
respectively, across cross-validation folds. C4.5 ﬁnds large models for all tested
parameters.

if (priors > 3) then predict yes
else if (age < 25) and (priors = 2 − 3) then predict yes
else predict no

Figure 13: When restricted to two features (age, priors), CORELS (λ = 0.005) ﬁnds the

same rule list across 10 cross-validation folds.

ﬁnds achieve test accuracy that is again comparable to COMPAS. CORELS identiﬁes the
same rule list across all 10 folds of 10-fold cross-validation experiments (Figure 13). In work
subsequent to ours (Angelino et al., 2017), Dressel and Farid (2018) conﬁrmed this result,
in the sense that they used logistic regression to construct a linear classiﬁer with age and
prior convictions, and also achieved similar accuracy to COMPAS. However, computing a
logistic regression model requires multiplication and addition, and their model cannot easily
be computed, in the sense that it requires a calculator (and thus is potentially error-prone).
Our rule lists require no such computation.

47

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

CORELS with diﬀerent regularization parameters (NYCLU stop-and-frisk data set)

Total
time (s)
.61 (.03)
70 (6)
1600 (100)

Time to
optimum (s)
.002 (.001)
.008 (.002)
56 (74)

Lower bound
evaluations (×106)
.070 (.004)
7.5 (.6)
150 (10)

Max evaluated
preﬁx length
6
11
16-17
Total queue
insertions (×103)
2.2 (.1)
210 (20)
4400 (300)

Optimal
preﬁx length
2
3
6-10
Max queue
size (×103)
.9 (.1)
130 (10)
2500 (170)

λ
.04
.01
.0025

λ
.04
.01
.0025

Table 3: Summary of CORELS executions,

for the NYCLU stop-and-frisk data set
(M = 46), for same three regularization parameter (λ) values as in Figure 12.
The columns report the total execution time, time to optimum, maximum evalu-
ated preﬁx length, optimal preﬁx length, number of times we completely evaluate
a preﬁx dp’s lower bound b(dp, x, y), total number of queue insertions (this number
is equal to the number of cache insertions), and the maximum queue size. For pre-
ﬁx lengths, we report single values or ranges corresponding to the minimum and
maximum observed values; in the other columns, we report means (and standard
deviations) over 10 cross-validation folds. See also Figures 14 and 15.

6.7 CORELS Execution Traces, for Diﬀerent Regularization Parameters

In this section, we illustrate several views of CORELS execution traces, for the NYCLU stop-
and-frisk data set with M = 46 antecedents, for the same three regularization parameters
(λ = .04, .01, .025) as in Figure 12.

Table 3 summarizes execution traces across all 10 cross-validation folds. For each value
of λ, CORELS achieves the optimum in a small fraction of the total execution time. As λ
decreases, these times increase because the search problems become more diﬃcult, as is
summarized by the observation that CORELS must evaluate longer preﬁxes; consequently,
our data structures grow in size. We report the total number of elements inserted into the
queue and the maximum queue size; recall from §5 that the queue elements correspond to
the trie’s leaves, and that the symmetry-aware map elements correspond to the trie’s nodes.
The upper panels in Figure 14 plot example execution traces, from a single cross-
validation fold, of both the current best objective value Rc and the lower bound b(dp, x, y)
of the preﬁx dp being evaluated. These plots illustrate that CORELS certiﬁes optimality
when the lower bound matches the objective value. The lower panels in Figure 14 plot corre-
sponding traces of an upper bound on the size of the remaining search space (Theorem 7),
and illustrate that as λ decreases, it becomes more diﬃcult to eliminate regions of the
search space. For Figure 14, we dynamically and incrementally calculate (cid:98)log10 Γ(Rc, Q)(cid:99),
which adds some computational overhead; we do not calculate this elsewhere unless noted.
Figure 15 visualizes the elements in CORELS’ logical queue, for each of the executions in
Figure 14. Recall from §5.5 that the logical queue corresponds to elements in the (physical)

48

Learning Certifiably Optimal Rule Lists

Figure 14: Example executions of CORELS,

for the NYCLU stop-and-frisk data set
(M = 46). See also Table 3 and Figure 15. Top: Objective value (solid line)
and lower bound (dashed line) for CORELS, as a function of wall clock time (log
scale). Numbered points along the trace of the objective value indicate when the
length of the best known rule list changes and are labeled by the new length.
For each value of λ, a star marks the optimum objective value and time at which
it was achieved. Bottom: (cid:98)log10 Γ(Rc, Q)(cid:99), as a function of wall clock time (log
scale), where Γ(Rc, Q) is the upper bound on remaining search space size (Theo-
rem 7). Rightmost panels: For visual comparison, we overlay the execution traces
from the panels to the left, for the three diﬀerent values of λ.

queue that have not been garbage collected from the trie; these are preﬁxes that CORELS
has already evaluated and whose children the algorithm plans to evaluate next. As an
execution progresses, longer preﬁxes are placed in the queue; as λ decreases, the algorithm
must spend more time evaluating longer and longer preﬁxes.

6.8 Eﬃcacy of CORELS Algorithm Optimizations

This section examines the eﬃcacy of each of our bounds and data structure optimizations.
We remove a single bound or data structure optimization from our ﬁnal implementation
and measure how the performance of our algorithm changes. We examine these performance
traces on both the NYCLU and the ProPublica data sets, and highlight the result that on
diﬀerent problems, the relative performance improvements of our optimizations can vary.

Table 4 provides summary statistics for experiments using the full CORELS implementa-
tion (ﬁrst row) and ﬁve variants (subsequent rows) that each remove a speciﬁc optimization:
(1) Instead of a priority queue (§5.2) ordered by the objective lower bound, we use a queue
that implements breadth-ﬁrst search (BFS). (2) We remove checks that would trigger prun-
ing via our lower bounds on antecedent support (Theorem 10) and accurate antecedent
support (Theorem 11). (3) We remove the eﬀect of our lookahead bound (Lemma 2), which
otherwise tightens the objective lower bound by an amount equal to the regularization pa-
rameter λ. (4) We disable the symmetry-aware map (§5.3), our data structure that enables
pruning triggered by the permutation bound (Corollary 16). (5) We do not identify sets
of equivalent points, which we otherwise use to tighten the objective lower bound via the
equivalent points bound (Theorem 20).

49

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 15: Summary of CORELS’ logical queue, for the NYCLU stop-and-frisk data set
(M = 46), for same three regularization parameters as in Figure 12 and Table 3.
Solid lines plot the numbers of preﬁxes in the logical queue (log scale), colored by
length (legend), as a function of wall clock time (log scale). All plots are generated
using a single, representative cross-validation training set. For each execution,
the gray shading ﬁlls in the area beneath the total number of queue elements,
i.e., the sum over all lengths; we also annotate the total time in seconds, marked
with a dashed vertical line.

Removing any single optimization increases total execution time, by varying amounts
across these optimizations. Similar to our experiments in §6.7, we always encounter the
optimal rule list in far less time than it takes to certify optimality. As in Table 3, we report
metrics that are all proxies for how much computational work our algorithm must perform;
these metrics thus scale with the overall slowdown with respect to CORELS execution time
(Table 4, ﬁrst column).

Figure 16 visualizes execution traces of the elements in CORELS’ logical queue, similar
to Figure 15, for a single, representative cross-validation fold. Panels correspond to diﬀerent
removed optimizations, as in Table 4. These plots demonstrate that our optimizations reduce
the number of evaluated preﬁxes and are especially eﬀective at limiting the number of longer
evaluated preﬁxes. For the ProPublica data set, the most important optimization is the
equivalent points bound—without it, we place preﬁxes of at least length 10 in our queue,
and must terminate these executions before they are complete. In contrast, CORELS and
most other variants evaluate only preﬁxes up to at most length 5, except for the variant
without the lookahead bound, which evaluates preﬁxes up to length 6.

Table 5 and Figure 17 summarize an analogous set of experiments for the NYCLU data
set. Note that while the equivalent points bound proved to be the most important opti-
mization for the ProPublica data set, the symmetry-aware map is the crucial optimization
for the NYCLU data set.

Finally, Figure 18 highlights the most signiﬁcant algorithm optimizations for our pre-
diction problems: the equivalent points bound for the ProPublica data set (left) and the

50

Learning Certifiably Optimal Rule Lists

Per-component performance improvement (ProPublica data set)

Slow-
down
—
1.1×
1.5×
13.3×
8.4×

Total time
(min)
.98 (.6)
1.03 (.6)
1.5 (.9)
12.3 (6.2)
9.1 (6.4)

Time to
optimum (s)
1 (1)
2 (4)
1 (2)
1 (1)
2 (3)

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound* >130 (2.6) >180× >1400 (2000)
Total queue
insertions (×106)
.29 (.2)
.33 (.2)
.40 (.2)
3.6 (1.8)
2.5 (1.7)
>510 (1.1)

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound*

Lower bound
evaluations (×106)
26 (15)
27 (16)
42 (25)
320 (160)
250 (180)

>940 (5)

Max evaluated
preﬁx length
5
5
5
6
5
≥11
Max queue
size (×106)
.24 (.1)
.20 (.1)
.33 (.2)
3.0 (1.5)
2.4 (1.7)
>500 (1.2)

Table 4: Per-component performance improvement, for the ProPublica data set (λ = 0.005,
M = 122). The columns report the total execution time, time to optimum, maxi-
mum evaluated preﬁx length, number of times we completely evaluate a preﬁx dp’s
lower bound b(dp, x, y), total number of queue insertions (which is equal to the
number of cache insertions), and maximum logical queue size. The ﬁrst row shows
CORELS; subsequent rows show variants that each remove a speciﬁc implemen-
tation optimization or bound. (We are not measuring the cumulative eﬀects of
removing a sequence of components.) All rows represent complete executions that
certify optimality, except those labeled ‘No equivalent points bound,’ for which
each execution was terminated due to memory constraints, once the size of the
cache reached 5 × 108 elements, after consuming ∼250GB RAM. In all but the
ﬁnal row and column, we report means (and standard deviations) over 10 cross-
validation folds. We also report the mean slowdown in total execution time, with
respect to CORELS. In the ﬁnal row, we report the mean (and standard deviation)
of the incomplete execution time and corresponding slowdown, and a lower bound
on the mean time to optimum; in the remaining ﬁelds, we report minimum values
across folds. See also Figure 16.
* Only 7 out of 10 folds achieve the optimum before being terminated.

symmetry-aware map for the NYCLU data set (right). For CORELS (thin lines) with
the ProPublica recidivism data set (left), the objective drops quickly, achieving the op-
timal value within a second. CORELS certiﬁes optimality in about a minute—the objective
lower bound steadily converges to the optimal objective (top) as the search space shrinks
(bottom). As in Figure 14, we dynamically and incrementally calculate (cid:98)log10 Γ(Rc, Q)(cid:99),
where Γ(Rc, Q) is the upper bound (13) on remaining search space size (Theorem 7); this
adds some computational overhead. In the same plots (left), we additionally highlight a

51

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 16: Summary of the logical queue’s contents, for full CORELS (top left) and ﬁve
variants that each remove a speciﬁc implementation optimization or bound, for
the ProPublica data set (λ = 0.005, M = 122). See also Table 4. Solid lines plot
the numbers of preﬁxes in the logical queue (log scale), colored by length (legend),
as a function of wall clock time (log scale). All plots are generated using a single,
representative cross-validation training set. The gray shading ﬁlls in the area
beneath the total number of queue elements for CORELS, i.e., the sum over
all lengths in the top left ﬁgure. For comparison, we replicate the same gray
region in the other ﬁve subﬁgures. For each execution, we indicate the total time
in seconds, relative to the full CORELS implementation (T = 21 s), and with a
dashed vertical line. The execution without the equivalent points bound (bottom
right) is incomplete.

separate execution of CORELS without the equivalent points bound (Theorem 20) (thick
lines). After more than 2 hours, the execution is still far from complete; in particular, the
lower bound is far from the optimum objective value (top) and much of the search space
remains unexplored (bottom). For the NYCLU stop-and-frisk data set (right), CORELS
achieves the optimum objective in well under a second, and certiﬁes optimality in a lit-
tle over a minute. CORELS without the permutation bound (Corollary 16), and thus the
symmetry-aware map, requires more than an hour, i.e., orders of magnitude more time, to
complete (thick lines).

52

Learning Certifiably Optimal Rule Lists

Per-component performance improvement (NYCLU stop-and-frisk data set)

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound

Slow-
down
—
2.0×
1.1×
1.6×

Time to
optimum (µs)
8.9 (.1)
110 (10)
8.8 (.8)
7.3 (1.8)

Total
time (min)
1.1 (.1)
2.2 (.2)
1.2 (.1)
1.7 (.2)
> 73 (5)
4 (.3)

3.8×

> 68× > 7.6 (.4)
6.4 (.9)
Total queue
insertions (×105)
2.0 (.2)
4.1 (.4)
2.1 (.2)
3.2 (.3)
> 1000 (0)
9.4 (.7)

Lower bound
evaluations (×106)
7 (1)
14 (1)
8 (1)
11 (1)
> 390 (40)
33 (2)

Max evaluated
preﬁx length
11
11
11
11-12
> 10
14
Max queue
size (×105)
1.3 (.1)
1.4 (.1)
1.3 (.1)
2.1 (.2)
> 900 (10)
6.0 (.4)

Table 5: Per-component performance improvement, as in Table 4, for the NYCLU stop-and-
frisk data set (λ = 0.01, M = 46). All rows except those labeled ‘No symmetry-
aware map’ represent complete executions. A single fold running without a
symmetry-aware map required over 2 days to complete, so in order to run all
10 folds above, we terminated execution after the preﬁx tree (§5.1) reached 108
nodes. See Table 4 for a detailed caption, and also Figure 17.

Algorithmic
approach
CORELS
Brute force
Brute force
CORELS (1984)

Max evaluated Lower bound
evaluations
preﬁx length
2.8 × 107
5
2.5 × 1010
5
5.0 × 1020
10
2.8 × 107
5

Predicted runtime

36 seconds

9.0 hours ≈ 3.3 × 104 s
21 × 106 years ≈ 6.5 × 1014 s
13.5 days ≈ 1.2 × 106 s

Table 6: Algorithmic speedup for the ProPublica data set (λ = 0.005, M = 122). Solving
this problem using brute force is impractical due to the inability to explore rule
lists of reasonable lengths. Removing only the equivalent points bound requires
exploring preﬁxes of up to length 10 (see Table 4), a clearly intractable problem.
Even with all of our improvements, however, it is only recently that processors
have been fast enough for this type of discrete optimization algorithm to succeed.

6.9 Algorithmic Speedup

Table 6 shows the overall speedup of CORELS compared to a na¨ıve implementation and
demonstrates the infeasibility of running our algorithm 30 years ago. Consider an execution
of CORELS for the ProPublica data set, with M = 122 antecedents, that evaluates preﬁxes

53

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 17: Summary of the logical queue’s contents, for full CORELS (top left) and ﬁve
variants that each remove a speciﬁc implementation optimization or bound, for
the NYCLU stop-and-frisk data set (λ = 0.01, M = 46), as in Table 5. The ex-
ecution without the symmetry-aware map (bottom center) is incomplete. See
Figure 16 for a detailed caption.

up to length 5 in order to certify optimality (Table 4). A brute force implementation that
na¨ıvely considers all preﬁxes of up to length 5 would evaluate 2.5 × 1010 preﬁxes. As shown
in Figure 4, the optimal rule list has preﬁx length 3, thus the brute force algorithm would
identify the optimal rule list. However, for this approach to certify optimality, it would
have to consider far longer preﬁxes. Without our equivalent points bound, but with all of
our other optimizations, we evaluate preﬁxes up to at least length 10 (see Table 4 and
Figure 16)—thus a brute force algorithm would have to evaluate preﬁxes of length 10 or
longer. Na¨ıvely evaluating all preﬁxes up to length 10 would require looking at 5.0 × 1020
diﬀerent preﬁxes.

However, CORELS examines only 28 million preﬁxes in total—a reduction of 893×
compared to examining all preﬁxes up to length 5 and a reduction of 1.8 × 1013 for the case
of length 10. On a laptop, we require about 1.3 µs to evaluate a single preﬁx (given by
dividing the number of lower bound evaluations by the total time in Table 4). Our runtime
is only about 36 seconds, but the na¨ıve solutions of examining all preﬁxes up to lengths 5

54

Learning Certifiably Optimal Rule Lists

Figure 18: Execution progress of CORELS and selected variants,

for the ProPublica
(λ = 0.005, M = 122) (left) and NYCLU (λ = 0.01, M = 46) (right) data sets.
Top: Objective value (thin solid lines) and lower bound (dashed lines) for
CORELS, as a function of wall clock time (log scale). Numbered points along the
trace of the objective value indicate when the length of the best known rule list
changes, and are labeled by the new length. CORELS quickly achieves the opti-
mal value (star markers), and certiﬁes optimality when the lower bound matches
the objective value. On the left, a separate and signiﬁcantly longer execution of
CORELS without the equivalent points (Theorem 20) bound remains far from
complete, and its lower bound (thick solid line) far from the optimum. On the
right, a separate execution of CORELS without the permutation bound (Corol-
lary 16), and thus the symmetry-aware map, requires orders of magnitude more
time to complete. Bottom: (cid:98)log10 Γ(Rc, Q)(cid:99), as a function of wall clock time (log
scale), where Γ(Rc, Q) is the upper bound (13) on remaining search space size
(Theorem 7). For these problems, the equivalent points (left) and permutation
(right) bounds are responsible for the ability of CORELS to quickly eliminate
most of the search space (thin solid lines); the remaining search space decays
much more slowly without these bounds (thick solid lines).

and 10 would take 9 hours and 21 million years, respectively. It is clear that brute force
would not scale to larger problems.

We compare our current computing circumstances to those of 1984, the year when CART
was published. Moore’s law holds that computing power doubled every 18 months from 1984
to 2006. This is a period of 264 months, which means computing power has gone up by at
least a factor of 32,000 since 1984. Thus, even with our algorithmic and data structural
improvements, CORELS would have required about 13.5 days in 1984—an unreasonable
amount of time. Our advances are meaningful only because we can run them on a modern

55

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

system. Combining our algorithmic improvements with the increase in modern processor
speeds, our algorithm runs more than 1013 times faster than a na¨ıve implementation would
have in 1984. This helps explain why neither our algorithm, nor other branch-and-bound
variants, had been developed before now.

7. Summary and Future Work on Bounds

Here, we highlight our most signiﬁcant bounds, as well as directions for future work based
on bounds that we have yet to leverage in practice.

In empirical studies, we found our equivalent support (§3.10, Theorem 15) and equiv-
alent points (§3.14, Theorem 20) bounds to yield the most signiﬁcant improvements in
algorithm performance. In fact, they sometimes proved critical for ﬁnding solutions and
proving optimality, even on small problems.

Accordingly, we would hope that our similar support bound (§3.13, Theorem 18) could
be useful; understanding how to eﬃciently exploit this result in practice represents an
important direction for future work. In particular, this type of bound may lead to principled
approximate variants of our approach.

We presented several sets of bounds in which at least one bound was strictly tighter than
the other(s). For example, the lower bound on accurate antecedent support (Theorem 11)
is strictly tighter than the lower bound on accurate support (Theorem 10). It might seem
that we should only use this tighter bound, but in practice, we can use both—the looser
bound can be checked before completing the calculation required to check the tighter bound.
Similarly, the equivalent support bound (Theorem 15) is more general than the special case
of the permutation bound (Corollary 16). We have implemented data structures, which we
call symmetry-aware maps, to support both of these bounds, but have not yet identiﬁed an
eﬃcient approach for supporting the more general equivalent points bound. A good solution
may be related to the challenge of designing an eﬃcient data structure to support the similar
support bound.

We also presented results on antecedent rejection that unify our understanding of our
lower (§3.7) and upper bounds (§3.8) on antecedent support. In a preliminary implemen-
tation (not described here), we experimented with special data structures to support the
direct use of our observation that antecedent rejection propagates (§3.9, Theorem 12). We
leave the design of eﬃcient data structures for this task as future work.

During execution, we ﬁnd it useful to calculate an upper bound on the size of the
remaining search space—e.g., via Theorem 7, or the looser Proposition 9, which incurs
less computational overhead—since these provide meaningful information about algorithm
progress and allow us to estimate the remaining execution time. As we illustrated in Sec-
tion 6.8, these calculations also help us qualify the impact of diﬀerent algorithmic bounds,
e.g., by comparing executions that keep or remove bounds.

When our algorithm terminates, it outputs an optimal solution of the training optimiza-
tion problem, with a certiﬁcate of optimality. On a practical note, our approach can also
provide useful results even for incomplete executions. As shown earlier, we have empirically
observed that our algorithm often identiﬁes the optimal rule list very quickly, compared to
the total time required to prove optimality, e.g., in seconds, versus minutes, respectively.
Furthermore, our objective’s lower bounds allow us to place an upper bound on the size of

56

Learning Certifiably Optimal Rule Lists

the remaining search space, and provides guarantees on the quality of a solution output by
an incomplete execution.

The order in which we evaluate preﬁxes can impact the rate at which we prune the search
space, and thus the total runtime. We think that it is possible to design search policies that
signiﬁcantly improve performance.

8. Conclusion and More Possible Directions for Future Work

Finally, we would like to clarify some limitations of CORELS. As far as we can tell, CORELS
is the current best algorithm for solving a specialized optimal decision tree problem. While
our approach scales well to large numbers of observations, it could have diﬃculty proving
optimality for problems with many possibly relevant features that are highly correlated,
when large regions of the search space might be challenging to exclude.

CORELS is not designed for raw image processing or other problems where the features
themselves are not interpretable. It could instead be used as a ﬁnal classiﬁer for image
processing problems where the features were created beforehand; for instance, one could
create classiﬁers for each part of an image, and use CORELS to create a ﬁnal combined
classiﬁer. The notions of interpretability used in image classiﬁcation tend to be completely
diﬀerent from those for structured data where each feature is separately meaningful (e.g.,
see Li et al., 2018). For structured data, decision trees, along with scoring systems, tend
to be popular forms of transparent models. Scoring systems are sparse linear models with
integer coeﬃcients, and they can also be created from data (Ustun and Rudin, 2017, 2016).

In some of our experiments, CORELS produces a DNF formula by coincidence, but it
might be possible to create a much simpler version of CORELS that only produces DNF
formulae. This could build oﬀ previous algorithms for creating an optimal DNF formula
(Rijnbeek and Kors, 2010; Wang et al., 2016, 2017).

CORELS does not automatically rank the subgroups in order of the likelihood of a
positive outcome; doing so would require an algorithm such as Falling Rule Lists (Wang
and Rudin, 2015a; Chen and Rudin, 2018), which forces the estimated probabilities to de-
crease along the list. Furthermore, while CORELS does not technically produce estimates
of P(Y = 1 | x), one could form such an estimate by computing the empirical proportion
ˆP(Y = 1 | x obeys pk) for each antecedent pk. CORELS is also not designed to assist with
causal inference applications, since it does not estimate the eﬀect of a treatment via the con-
ditional diﬀerence P(Y = 1 | treatment = True, x) − P(Y = 1 | treatment = False, x). Alter-
native algorithms that estimate conditional diﬀerences with interpretable rule lists include
Causal Falling Rule Lists (Wang and Rudin, 2015b), Cost-Eﬀective Interpretable Treatment
Regimes (CITR) (Lakkaraju and Rudin, 2017), and an approach by Zhang et al. (2015) for
constructing interpretable and parsimonious treatment regimes. Alternatively, one could
use a complex machine learning model to predict outcomes for the treatment group and a
separate complex model for the control group that would allow counterfactuals to be esti-
mated for each observation; from there, CORELS could be applied to produce a transparent
model for personalized treatment eﬀects. A similar approach to this was taken by Goh and
Rudin (2018), who use CORELS to understand a black box causal model.

57

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

CORELS could be adapted to handle cost-sensitive learning or weighted regularization.
This would require creating more general versions of our theorems, which would be an
extension of this work.

While CORELS does not directly handle continuous variables, we have found that it is
not diﬃcult in practice to construct a rule set that is suﬃcient for creating a useful model.
It may be possible to use techniques such as Fast Boxes (Goh and Rudin, 2014) to discover
useful and interpretable rules for continuous data that can be used within CORELS.

An interesting direction for future research would be to create a hybrid interpretable/
black box model in the style of Wang (2018), where the rule list would eliminate large
parts of the space away from the decision boundary, and the observations that remain are
evaluated by a black box model rather than a default rule.

Lastly, CORELS does not create generic single-variable-split decision trees. CORELS
optimizes over rule lists, which are one-sided decision trees; in our setting, the leaves of
these ‘trees’ are conjunctions. It may be possible to generalize ideas from our approach to
handle generic decision trees, which could be an interesting project for future work. There
are more symmetries to handle in that case, since there would be many equivalent decision
trees, leading to challenges in developing symmetry-aware data structures.

Acknowledgments

E.A. conducted most of this work while supported by the Miller Institute for Basic Research
in Science, University of California, Berkeley, and hosted by Prof. M.I. Jordan at RISELab.
C.D.R. is supported in part by MIT-Lincoln Labs and the National Science Foundation
under IIS-1053407. E.A. would like to thank E. Jonas, E. Kohler, and S. Tu for early
implementation guidance, A. D’Amour for pointing out the work by Goel et al. (2016),
V. Kanade, S. McCurdy, J. Schleier-Smith and E. Thewalt for helpful conversations, and
members of RISELab, SAIL, and the UC Berkeley Database Group for their support and
feedback. We thank H. Yang and B. Letham for sharing advice and code for processing data
and mining rules, B. Coker for his critical advice on using the ProPublica COMPAS data set,
as well as V. Kaxiras and A. Saligrama for their recent contributions to our implementation
and for creating the CORELS website. We are very grateful to our editor and anonymous
reviewers.

Appendix A. Excessive Antecedent Support

Theorem 21 (Upper bound on antecedent support) Let d∗ = (dp, δp, q0, K) be any op-
timal rule list with objective R∗, i.e., d∗ ∈ argmind R(d, x, y), and let dp = (p1, . . . , pk−1,
pk, . . . , pK) be its preﬁx. For each k ≤ K, antecedent pk in dp has support less than or equal
to the fraction of all data not captured by preceding antecedents, by an amount greater than
the regularization parameter λ:

supp(pk, x | dp) ≤ 1 − supp(dk−1

p

, x) − λ,

(37)

where dk−1
that there also exists a shorter optimal rule list d(cid:48) = (dK−1

p = (p1, . . . , pk−1). For the last antecedent, i.e., when pk = pK, equality implies

0, K − 1) ∈ argmind R(d, x, y).

p, q(cid:48)

, δ(cid:48)

p

58

Learning Certifiably Optimal Rule Lists

Proof First, we focus on the last antecedent pK+1 in a rule list d(cid:48). Let d = (dp, δp, q0, K)
be a rule list with preﬁx dp = (p1, . . . , pK) and objective R(d, x, y) ≥ R∗, where R∗ ≡
minD R(D, x, y) is the optimal objective. Let d(cid:48) = (d(cid:48)
0, K + 1) be a rule list whose
preﬁx d(cid:48)
p = (p1, . . . , pK, pK+1) starts with dp and ends with a new antecedent pK+1. Sup-
pose pK+1 in the context of d(cid:48)
p captures nearly all data not captured by dp, except for a
fraction (cid:15) upper bounded by the regularization parameter λ:

p, δ(cid:48)

p, q(cid:48)

1 − supp(dp, x) − supp(pK+1, x | d(cid:48)

p) ≡ (cid:15) ≤ λ.

Since d(cid:48)
p starts with dp, its preﬁx misclassiﬁcation error is at least as great; the only discrep-
ancy between the misclassiﬁcation errors of d and d(cid:48) can come from the diﬀerence between
the support of the set of data not captured by dp and the support of pK+1:

|(cid:96)(d(cid:48), x, y) − (cid:96)(d, x, y)| ≤ 1 − supp(dp, x) − supp(pK+1, x | d(cid:48)

p) = (cid:15).

The best outcome for d(cid:48) would occur if its misclassiﬁcation error were smaller than that
of d by (cid:15), therefore

R(d(cid:48), x, y) = (cid:96)(d(cid:48), x, y) + λ(K + 1)

≥ (cid:96)(d, x, y) − (cid:15) + λ(K + 1) = R(d, x, y) − (cid:15) + λ ≥ R(d, x, y) ≥ R∗.

d(cid:48) is an optimal rule list, i.e., d(cid:48) ∈ argminD R(D, x, y), if and only if R(d(cid:48), x, y) = R(d, x, y) =
R∗, which requires (cid:15) = λ. Otherwise, (cid:15) < λ, in which case

R(d(cid:48), x, y) ≥ R(d, x, y) − (cid:15) + λ > R(d, x, y) ≥ R∗,

therefore d(cid:48) is not optimal, i.e., d(cid:48) /∈ argminD R(D, x, y). This demonstrates the desired
result for k = K.

In the remainder, we prove the bound in (37) by contradiction, in the context of a rule
list d(cid:48)(cid:48). Let d and d(cid:48) retain their deﬁnitions from above, thus as before, that the data not
captured by d(cid:48)

p has normalized support (cid:15) ≤ λ, i.e.,

1 − supp(d(cid:48)

p, x) = 1 − supp(dp, x) − supp(pK+1, x | d(cid:48)

p) = (cid:15) ≤ λ.

Thus for any rule list d(cid:48)(cid:48) whose preﬁx d(cid:48)(cid:48)
p and ends
with one or more additional rules, each additional rule pk has support supp(pk, x | d(cid:48)(cid:48)
p) ≤
(cid:15) ≤ λ, for all k > K + 1. By Theorem 10, all of the additional rules have insuﬃcient sup-
port, therefore d(cid:48)(cid:48)

p = (p1, . . . , pK+1, . . . , pK(cid:48)) starts with d(cid:48)

p cannot be optimal, i.e., d(cid:48)(cid:48) /∈ argminD R(D, x, y).

Similar to Theorem 10, our lower bound on antecedent support, we can apply Theo-
rem 21 in the contexts of both constructing rule lists and rule mining (§3.1). Theorem 21
implies that if we only seek a single optimal rule list, then during branch-and-bound ex-
ecution, we can prune a preﬁx if we ever add an antecedent with support too similar to
the support of the set of data not captured by the preceding antecedents. One way to view
this result is that if d = (dp, δp, q0, K) and d(cid:48) = (d(cid:48)
0, K + 1) are rule lists such that d(cid:48)
p
starts with dp and ends with an antecedent that captures all or nearly all data not captured
by dp, then the new rule in d(cid:48) behaves similar to the default rule of d. As a result, the
misclassiﬁcation error of d(cid:48) must be similar to that of d, and any reduction may not be
suﬃcient to oﬀset the penalty for longer preﬁxes.

p, δ(cid:48)

p, q(cid:48)

59

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Proposition 22 (Excessive antecedent support propagates) Deﬁne φ(dp) as in (19),
and let dp = (p1, . . . , pK) be a preﬁx, such that its last antecedent pK has excessive support,
i.e., the opposite of the bound in (37):

supp(pK, x | dp) > 1 − supp(dK−1

p

, x) − λ,

p

where dK−1
= (p1, . . . , pK−1). Let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp =
(P1, . . . , Pκ) such that Dp starts with DK(cid:48)−1
) and PK(cid:48) = pK. It
follows that PK(cid:48) has excessive support in preﬁx Dp, and furthermore, D /∈ argmind R(d, x, y).

= (P1, . . . , PK(cid:48)−1) ∈ φ(dK−1

p

p

Proof Since DK(cid:48)

p = (P1, . . . , PK(cid:48)) contains all the antecedents in dp, we have that

supp(DK(cid:48)

p , x) ≥ supp(dp, x).

Expanding these two terms gives

supp(DK(cid:48)

p , x) = supp(DK(cid:48)−1

p

, x) + supp(PK(cid:48), x | Dp)

≥ supp(dp, x) = supp(dK−1

p

, x) + supp(pK, x | dp) > 1 − λ.

Rearranging gives

supp(PK(cid:48), x | Dp) > 1 − supp(DK(cid:48)−1

p

, x) − λ,

thus PK(cid:48) has excessive support in Dp. By Theorem 21, D /∈ argmind R(d, x, y).

Appendix B. Proof of Theorem 15 (Equivalent Support Bound)

We begin by deﬁning four related rule lists. First, let d = (dp, δp, q0, K) be a rule list with
preﬁx dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK). Second, let D = (Dp, ∆p, Q0, κ) be
a rule list with preﬁx Dp = (P1, . . . , Pκ) that captures the same data as dp, and labels
0, K(cid:48)) ∈ σ(dp) be any rule list whose preﬁx
∆p = (Q1, . . . , Qκ). Third, let d(cid:48) = (d(cid:48)
starts with dp, such that K(cid:48) ≥ K. Denote the preﬁx and labels of d(cid:48) by d(cid:48)
p = (p1, . . . , pK,
0, κ(cid:48)) ∈
p, Q(cid:48)
p, ∆(cid:48)
pK+1, . . . , pK(cid:48)) and δp = (q1, . . . , qK(cid:48)), respectively. Finally, deﬁne D(cid:48) = (D(cid:48)
σ(Dp) to be the ‘analogous’ rule list, i.e., whose preﬁx D(cid:48)
p = (P1, . . . , Pκ, Pκ+1, . . . , Pκ(cid:48)) =
(P1, . . . , Pκ, pK+1, . . . , pK(cid:48)) starts with Dp and ends with the same K(cid:48) − K antecedents
as d(cid:48)

p. Let ∆(cid:48)
Next, we claim that the diﬀerence in the objectives of rule lists d(cid:48) and d is the same as
the diﬀerence in the objectives of rule lists D(cid:48) and D. Let us expand the ﬁrst diﬀerence as

p = (Q1, . . . , Qκ(cid:48)) denote the labels of D(cid:48).

p, q(cid:48)

p, δ(cid:48)

R(d(cid:48), x, y) − R(d, x, y) = (cid:96)(d(cid:48), x, y) + λK(cid:48) − (cid:96)(d, x, y) − λK
= (cid:96)p(d(cid:48)

p, x, y) + (cid:96)0(d(cid:48)

0, x, y) − (cid:96)p(dp, δp, x, y) − (cid:96)0(dp, q0, x, y) + λ(K(cid:48) − K).

p, q(cid:48)

p, δ(cid:48)

Similarly, let us expand the second diﬀerence as

R(D(cid:48), x, y) − R(D, x, y) = (cid:96)(D(cid:48), x, y) + λκ(cid:48) − (cid:96)(D, x, y) − λκ
= (cid:96)p(D(cid:48)

p, x, y) + (cid:96)0(D(cid:48)

p, ∆(cid:48)

p, Q(cid:48)

0, x, y) − (cid:96)p(Dp, ∆p, x, y) − (cid:96)0(Dp, Q0, x, y) + λ(K(cid:48) − K),

60

Learning Certifiably Optimal Rule Lists

where we have used the fact that κ(cid:48) − κ = K(cid:48) − K.

The preﬁxes dp and Dp capture the same data. Equivalently, the set of data that is not

captured by dp is the same as the set of data that is not captured by Dp, i.e.,

{xn : ¬ cap(xn, dp)} = {xn : ¬ cap(xn, Dp)}.

Thus, the corresponding rule lists d and D share the same default rule, i.e., q0 = Q0, yielding
the same default rule misclassiﬁcation error:

Similarly, preﬁxes d(cid:48)
same default rule misclassiﬁcation error:

p and D(cid:48)

p capture the same data, and thus rule lists d(cid:48) and D(cid:48) have the

(cid:96)0(dp, q0, x, y) = (cid:96)0(Dp, Q0, x, y).

(cid:96)0(dp, q0, x, y) = (cid:96)0(Dp, Q0, x, y).

At this point, to demonstrate our claim relating the objectives of d, d(cid:48), D, and D(cid:48), what
p and dp is

remains is to show that the diﬀerence in the misclassiﬁcation errors of preﬁxes d(cid:48)
the same as that between D(cid:48)

p and Dp. We can expand the ﬁrst diﬀerence as

(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) − (cid:96)p(dp, δp, x, y) =

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn],

where we have used the fact that since d(cid:48)
p starts with dp, the ﬁrst K rules in d(cid:48)
same mistakes as those in dp. Similarly, we can expand the second diﬀerence as

p make the

(cid:96)p(D(cid:48)

p, ∆(cid:48)

p, x, y) − (cid:96)p(Dp, ∆p, x, y) =

cap(xn, Pk | D(cid:48)

p) ∧ 1[Qk (cid:54)= yn]

cap(xn, pk | D(cid:48)

p) ∧ 1[Qk (cid:54)= yn]

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn]

(38)

n=1
p, δ(cid:48)

k=K+1
p, x, y) − (cid:96)p(dp, δp, x, y).

= (cid:96)p(d(cid:48)

To justify the equality in (38), we observe ﬁrst that preﬁxes D(cid:48)
p start with κ and K
antecedents, respectively, that capture the same data. Second, preﬁxes D(cid:48)
p end with
exactly the same ordered list of K(cid:48) − K antecedents, therefore for any k = 1, . . . , K(cid:48) − K,
antecedent Pκ+k = pK+k in D(cid:48)
p. It follows that
the corresponding labels are all equivalent, i.e., Qκ+k = qK+k, for all k = 1, . . . , K(cid:48) − K, and
consequently, the preﬁx misclassiﬁcation error associated with the last K(cid:48) − K antecedents
of d(cid:48)
p. We have therefore shown that the diﬀerence between the
objectives of d(cid:48) and d is the same as that between D(cid:48) and D, i.e.,

p captures the same data as pK+k captures in d(cid:48)

p is the same as that of D(cid:48)

p and d(cid:48)

p and d(cid:48)

R(d(cid:48), x, y) − R(d, x, y) = R(D(cid:48), x, y) − R(D, x, y).

(39)

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K+1

N
(cid:88)

κ(cid:48)
(cid:88)

n=1

N
(cid:88)

k=κ+1
K(cid:48)
(cid:88)

n=1

N
(cid:88)

k=K+1
K(cid:48)
(cid:88)

1
N

1
N

1
N

=

=

61

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Next, suppose that the objective lower bounds of d and D obey b(dp, x, y) ≤ b(Dp, x, y),

therefore

R(d, x, y) = (cid:96)p(dp, δp, x, y) + (cid:96)0(dp, q0, x, y) + λK
= b(dp, x, y) + (cid:96)0(dp, q0, x, y)
≤ b(Dp, x, y) + (cid:96)0(dp, q0, x, y) = b(Dp, x, y) + (cid:96)0(Dp, Q0, x, y) = R(D, x, y).

Now let d∗ be an optimal rule list with preﬁx constrained to start with dp,

d∗ ∈ argmin
d†∈σ(dp)

R(d†, x, y),

and let K∗ be the length of d∗. Let D∗ be the analogous κ∗-rule list whose preﬁx starts
with Dp and ends with the same K∗ − K antecedents as d∗, where κ∗ = κ + K∗ − K.
By (39),

R(d∗, x, y) − R(d, x, y) = R(D∗, x, y) − R(D, x, y).

Furthermore, we claim that D∗ is an optimal rule list with preﬁx constrained to start
with Dp,

(40)

(41)

(42)

D∗ ∈ argmin
D†∈σ(Dp)

R(D†, x, y).

To demonstrate (42), we consider two separate scenarios. In the ﬁrst scenario, preﬁxes dp
and Dp are composed of the same antecedents, i.e., the two preﬁxes are equivalent up to a
permutation of their antecedents, and as a consequence, κ = K and κ∗ = K∗. Here, every
rule list d(cid:48)(cid:48) ∈ σ(dp) that starts with dp has an analogue D(cid:48)(cid:48) ∈ σ(Dp) that starts with Dp,
such that d(cid:48)(cid:48) and D(cid:48)(cid:48) obey (39), and vice versa, and thus (42) is a direct consequence of (41).
In the second scenario, preﬁxes dp and Dp are not composed of the same antecedents.
Deﬁne φ = {pk : (pk ∈ dp) ∧ (pk /∈ Dp)} to be the set of antecedents in dp that are not in Dp,
and deﬁne Φ = {Pk : (Pk ∈ Dp) ∧ (Pk /∈ dp)} to be the set of antecedents in Dp that are not
in dp; either φ (cid:54)= ∅, or Φ (cid:54)= ∅, or both.

Suppose φ (cid:54)= ∅, and let p ∈ φ be an antecedent in φ. It follows that there exists a subset
of rule lists in σ(Dp) that do not have analogues in σ(dp). Let D(cid:48)(cid:48) ∈ σ(Dp) be such a rule
list, such that its preﬁx D(cid:48)(cid:48)
p = (P1, . . . , Pκ, . . . , p, . . . ) starts with Dp and contains p among
its remaining antecedents. Since p captures a subset of the data that dp captures, and Dp
captures the same data as dp, it follows that p does not capture any data in D(cid:48)(cid:48)

p , i.e.,

1
N

N
(cid:88)

n=1

cap(xn, p | D(cid:48)(cid:48)

p ) = 0 ≤ λ.

By Theorem 10, antecedent p has insuﬃcient support in D(cid:48)(cid:48), and thus D(cid:48)(cid:48) cannot be op-
timal, i.e., D(cid:48)(cid:48) /∈ argminD†∈σ(Dp) R(D†, x, y). By a similar argument, if Φ (cid:54)= ∅ and P ∈ Φ,
and d(cid:48)(cid:48) ∈ σ(dp) is any rule list whose preﬁx starts with dp and contains antecedent P , then d(cid:48)(cid:48)
cannot be optimal, i.e., d(cid:48)(cid:48) /∈ argmind†∈σ(dp) R(d†, x, y).

62

Learning Certifiably Optimal Rule Lists

To ﬁnish justifying claim (42) for the second scenario, ﬁrst deﬁne

τ (dp, Φ) ≡ {d(cid:48)(cid:48) = (d(cid:48)(cid:48)

p, δ(cid:48)(cid:48)

p , q(cid:48)(cid:48)

0 , K(cid:48)(cid:48)) : d(cid:48)(cid:48) ∈ σ(dp) and pk /∈ Φ, ∀pk ∈ d(cid:48)(cid:48)

p} ⊂ σ(dp)

to be the set of all rule lists whose preﬁxes start with dp and do not contain any antecedents
in Φ. Now, recognize that the optimal preﬁxes in τ (dp, Φ) and σ(dp) are the same, i.e.,

argmin
d†∈τ (dp,Φ)

R(d†, x, y) = argmin
d†∈σ(dp)

R(d†, x, y),

and similarly, the optimal preﬁxes in τ (Dp, φ) and σ(Dp) are the same, i.e.,

argmin
D†∈τ (Dp,φ)

R(D†, x, y) = argmin
D†∈σ(Dp)

R(D†, x, y).

Since we have shown that every d(cid:48)(cid:48) ∈ τ (dp, Φ) has a direct analogue D(cid:48)(cid:48) ∈ τ (Dp, φ), such
that d(cid:48)(cid:48) and D(cid:48)(cid:48) obey (39), and vice versa, we again have (42) as a consequence of (41).
We can now ﬁnally combine (40) and (42) to obtain the desired inequality in (21):

min
d(cid:48)∈σ(dp)

R(d(cid:48), x, y) = R(d∗, x, y) ≤ R(D∗, x, y) = min

R(D(cid:48), x, y).

D(cid:48)∈σ(Dp)

Appendix C. Proof of Theorem 18 (Similar Support Bound)

We begin by deﬁning four related rule lists. First, let d = (dp, δp, q0, K) be a rule list with
preﬁx dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK). Second, let D = (Dp, ∆p, Q0, κ) be a
rule list with preﬁx Dp = (P1, . . . , Pκ) and labels ∆p = (Q1, . . . , Qκ). Deﬁne ω as in (22)
and Ω as in (23), and require that ω, Ω ≤ λ. Third, let d(cid:48) = (d(cid:48)
0, K(cid:48)) ∈ σ(dp) be
any rule list whose preﬁx starts with dp, such that K(cid:48) ≥ K. Denote the preﬁx and la-
bels of d(cid:48) by d(cid:48)
p = (p1, . . . , pK, pK+1, . . . , pK(cid:48)) and δp = (q1, . . . , qK(cid:48)), respectively. Finally,
p, ∆(cid:48)
deﬁne D(cid:48) = (D(cid:48)
p =
(P1, . . . , Pκ, Pκ+1, . . . , Pκ(cid:48)) = (P1, . . . , Pκ, pK+1, . . . , pK(cid:48)) starts with Dp and ends with the
same K(cid:48) − K antecedents as d(cid:48)

0, κ(cid:48)) ∈ σ(Dp) to be the ‘analogous’ rule list, i.e., whose preﬁx D(cid:48)

p = (Q1, . . . , Qκ(cid:48)) denote the labels of D(cid:48).

p. Let ∆(cid:48)

p, Q(cid:48)

The smallest possible objective for D(cid:48), in relation to the objective of d(cid:48), reﬂects both
the diﬀerence between the objective lower bounds of D and d and the largest possible
discrepancy between the objectives of d(cid:48) and D(cid:48). The latter would occur if d(cid:48) misclassiﬁed
all the data corresponding to both ω and Ω while D(cid:48) correctly classiﬁed this same data,
thus

p, q(cid:48)

p, δ(cid:48)

R(D(cid:48), x, y) ≥ R(d(cid:48), x, y) + b(Dp, x, y) − b(dp, x, y) − ω − Ω.

(43)

Now let D∗ be an optimal rule list with preﬁx constrained to start with Dp,

and let κ∗ be the length of D∗. Also let d∗ be the analogous K∗-rule list whose preﬁx starts
with dp and ends with the same κ∗ − κ antecedents as D∗, where K∗ = K + κ∗ − κ. By (43),

D∗ ∈ argmin
D†∈σ(Dp)

R(D†, x, y),

63

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

we obtain the desired inequality in (24):

min
D†∈σ(Dp)

R(D†, x, y) = R(D∗, x, y)

≥ R(d∗, x, y) + b(Dp, x, y) − b(dp, x, y) − ω − Ω
≥ min

R(d†, x, y) + b(Dp, x, y) − b(dp, x, y) − ω − Ω.

d†∈σ(dp)

Appendix D. Proof of Theorem 20 (Equivalent Points Bound)

We derive a lower bound on the default rule misclassiﬁcation error (cid:96)0(dp, q0, x, y), analogous
to the lower bound (26) on the misclassiﬁcation error (cid:96)(d, x, y) in the proof of Proposition 19.
As before, we sum over all sets of equivalent points, and then for each such set, we count
diﬀerences between class labels and the minority class label of the set, instead of counting
mistakes made by the default rule:

(cid:96)0(dp, q0, x, y) =

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn]

1
N

1
N

1
N

N
(cid:88)

n=1
U
(cid:88)

N
(cid:88)

u=1
U
(cid:88)

n=1
N
(cid:88)

u=1

n=1

=

≥

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn] 1[xn ∈ eu]

¬ cap(xn, dp) ∧ 1[yn = qu] 1[xn ∈ eu] = b0(dp, x, y),

(44)

where the ﬁnal equality comes from the deﬁnition of b0(dp, x, y) in (28). Since we can write
the objective R(d, x, y) as the sum of the objective lower bound b(dp, x, y) and default rule
misclassiﬁcation error (cid:96)0(dp, q0, x, y), applying (44) gives a lower bound on R(d, x, y):

R(d, x, y) = (cid:96)p(dp, δp, x, y) + (cid:96)0(dp, q0, x, y) + λK = b(dp, x, y) + (cid:96)0(dp, q0, x, y)

≥ b(dp, x, y) + b0(dp, x, y).

(45)

It follows that for any rule list d(cid:48) ∈ σ(d) whose preﬁx d(cid:48)

p starts with dp, we have

R(d(cid:48), x, y) ≥ b(d(cid:48)

p, x, y) + b0(d(cid:48)

p, x, y).

(46)

Finally, we show that the lower bound on R(d, x, y) in (45) is not greater than the lower

bound on R(d(cid:48), x, y) in (46). First, let us deﬁne

Υ(d(cid:48)

p, K, x, y) ≡

cap(xn, pk | d(cid:48)

p) ∧ 1[xn ∈ eu] 1[yn = qu].

(47)

1
N

U
(cid:88)

N
(cid:88)

K(cid:48)
(cid:88)

u=1

n=1

k=K+1

64

Learning Certifiably Optimal Rule Lists

Now, we write a lower bound on b(d(cid:48)

p, x, y) with respect to b(dp, x, y):

b(d(cid:48)

p, x, y) = (cid:96)p(d(cid:48)

p, δp, x, y) + λK(cid:48) =

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] + λK(cid:48)

= (cid:96)p(dp, δp, x, y) + λK +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] + λ(K(cid:48) − K)

= b(dp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] + λ(K(cid:48) − K)

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=1

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K+1

U
(cid:88)

N
(cid:88)

K(cid:48)
(cid:88)

u=1

n=1

U
(cid:88)

N
(cid:88)

k=K+1
K(cid:48)
(cid:88)

1
N

1
N

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

1
N

(cid:32)

= b(dp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] 1[xn ∈ eu] + λ(K(cid:48) − K)

≥ b(dp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[yn = qu] 1[xn ∈ eu] + λ(K(cid:48) − K)

= b(dp, x, y) + Υ(d(cid:48)

n=1

k=K+1

u=1
p, K, x, y) + λ(K(cid:48) − K),

(48)

where the last equality uses (47). Next, we write b0(dp, x, y) with respect to b0(d(cid:48)

p, x, y),

b0(dp, x, y) =

¬ cap(xn, dp) ∧ 1[xn ∈ eu] 1[yn = qu]

=

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

¬ cap(xn, d(cid:48)

p) +

cap(xn, pk | d(cid:48)
p)

∧ 1[xn ∈ eu] 1[yn = qu]

K(cid:48)
(cid:88)

k=K+1

(cid:33)

= b0(d(cid:48)

p, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[xn ∈ eu] 1[yn = qu].

(49)

1
N

U
(cid:88)

N
(cid:88)

K(cid:48)
(cid:88)

u=1

n=1

k=K+1

Rearranging (49) gives

b0(d(cid:48)

p, x, y) = b0(dp, x, y) − Υ(d(cid:48)

p, K, x, y).

(50)

Combining (46) with ﬁrst (50) and then (48) gives the desired inequality in (27):

p, x, y)

p, x, y) + b0(d(cid:48)
p, x, y) + b0(dp, x, y) − Υ(d(cid:48)

R(d(cid:48), x, y) ≥ b(d(cid:48)
= b(d(cid:48)
≥ b(dp, x, y) + Υ(d(cid:48)
= b(dp, x, y) + b0(dp, x, y) + λ(K(cid:48) − K) ≥ b(dp, x, y) + b0(dp, x, y).

p, K, x, y) + λ(K(cid:48) − K) + b0(dp, x, y) − Υ(d(cid:48)

p, K, x, y)

p, K, x, y)

Appendix E. Data Processing Details and Antecedent Mining

In this appendix, we provide details regarding datasets used in our experiments (Section 6).

65

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

E.1 ProPublica Recidivism Data Set

Table 7 shows the 6 attributes and corresponding 17 categorical values that we use for the
ProPublica data set. From these, we construct 17 single-clause antecedents, for example,
(age = 23 − 25). We then combine pairs of these antecedents as conjunctions to form two-
clause antecedents, e.g., (age = 23 − 25) ∧ (priors = 2 − 3). By virtue of our lower bound
on antecedent support, (Theorem 10, §3.7), we eliminate antecedents with support less
than 0.005 or greater than 0.995, since λ = 0.005 is the smallest regularization parameter
value we study for this problem. With this ﬁltering step, we generate between 121 and 123
antecedents for each fold; without it, we would instead generate about 130 antecedents as
input to our algorithm.

Note that we exclude the ‘current charge’ attribute (which has two categorical values,
‘misdemeanor’ and ‘felony’); for individuals in the data set booked on multiple charges, this
attribute does not appear to consistently reﬂect the most serious charge.

Feature
sex
age
juvenile felonies
juvenile misdemeanors
juvenile crimes
priors

Value range
—
18-96
0-20
0-13
0-21
0-38

Categorical values
male, female
18-20, 21-22, 23-25, 26-45, >45
0, >0
0, >0
0, >0
0, 1, 2-3, >3

Count
2
5
2
2
2
4

Table 7: Categorical features (6 attributes, 17 values) from the ProPublica data set. We
construct the feature juvenile crimes from the sum of juvenile felonies, juvenile
misdemeanors, and the number of juvenile crimes that were neither felonies nor
misdemeanors (not shown).

E.2 NYPD Stop-and-frisk Data Set

This data set is larger than, but similar to the NYCLU stop-and-frisk data set, described
next.

E.3 NYCLU Stop-and-frisk Data Set

The original data set contains 45,787 records, each describing an incident involving a stopped
person; the individual was frisked in 30,345 (66.3%) of records and and searched in 7,283
(15.9%). In 30,961 records, the individual was frisked and/or searched (67.6%); of those,
a criminal possession of a weapon was identiﬁed 1,445 times (4.7% of these records). We
remove 1,929 records with missing data, as well as a small number with extreme values
for the individual’s age—we eliminate those with age < 12 or > 89. This yields a set of
29,595 records in which the individual was frisked and/or searched. To address the class
imbalance for this problem, we sample records from the smaller class with replacement. We
generate cross-validation folds ﬁrst, and then resample within each fold. In our 10-fold cross-
validation experiments, each training set contains 50,743 observations. Table 8 shows the 5

66

Learning Certifiably Optimal Rule Lists

categorical attributes that we use, corresponding to a total of 28 values. Our experiments
use these antecedents, as well as negations of the 18 antecedents corresponding to the two
features stop reason and additional circumstances, which gives a total of 46 antecedents.

Feature
stop reason

additional
circumstances

city
location

inside or outside

Values
suspicious object, ﬁts description, casing,
acting as lookout, suspicious clothing,
drug transaction, furtive movements,
actions of violent crime, suspicious bulge
proximity to crime scene, evasive response,
associating with criminals, changed direction,
high crime area, time of day,
sights and sounds of criminal activity,
witness report, ongoing investigation
Queens, Manhattan, Brooklyn, Staten Island, Bronx
housing authority, transit authority,
neither housing nor transit authority
inside, outside

Count
9

9

5
3

2

Table 8: Categorical features (5 attributes, 28 values) from the NYCLU data set.

Appendix F. Example Optimal Rule Lists, for Diﬀerent Values of λ

For each of our prediction problems, we provide listings of optimal rule lists found by
CORELS, across 10 cross-validation folds, for diﬀerent values of the regularization param-
eter λ. These rule lists correspond to the results for CORELS summarized in Figures 11
and 12 (§6.6). Recall that as λ decreases, optimal rule lists tend to grow in length.

F.1 ProPublica Recidivism Data Set

We show example optimal rule lists that predict two-year recidivism. Figure 19 shows exam-
ples for regularization parameters λ = 0.02 and 0.01. Figure 20 shows examples for λ = 0.005;
Figure 4 (§6.3) showed two representative examples.

For the largest regularization parameter λ = 0.02 (Figure 19), we observe that all folds
identify the same length-1 rule list. For the intermediate value λ = 0.01 (Figure 19), the
folds identify optimal 2-rule or 3-rule lists that contain the nearly same preﬁx rules, up to
permutations. For the smallest value λ = 0.005 (Figure 20), the folds identify optimal 3-rule
or 4-rule lists that contain the nearly same preﬁx rules, up to permutations. Across all three
regularization parameter values and all folds, the preﬁx rules always predict the positive
class label, and the default rule always predicts the negative class label. We note that our
objective is not designed to enforce any of these properties.

67

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Two-year recidivism prediction (λ = 0.02)

if (priors > 3) then predict yes
else predict no

Two-year recidivism prediction (λ = 0.01)

(cid:46) Found by all 10 folds

if (priors > 3) then predict yes
else if (sex = male) and (juvenile crimes > 0) then predict yes
else predict no

(cid:46) Found by 3 folds

if (sex = male) and (juvenile crimes > 0) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (priors > 3) then predict yes
else predict no

if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

(cid:46) Found by 2 folds

(cid:46) Found by 2 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

Figure 19: Example optimal rule lists for the ProPublica data set, found by CORELS with
regularization parameters λ = 0.02 (top), and 0.01 (bottom) across 10 cross-
validation folds.

68

Learning Certifiably Optimal Rule Lists

Two-year recidivism prediction (λ = 0.005)

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (priors > 3) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else predict no

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

(cid:46) Found by 4 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

Figure 20: Example optimal rule lists for the ProPublica data set, found by CORELS with
regularization parameters λ = 0.005, across 10 cross-validation folds.

69

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

F.2 NYPD Stop-and-frisk Data Set

We show example optimal rule lists that predict whether a weapon will be found on a
stopped individual who is frisked or searched, learned from the NYPD data set.

Weapon prediction (λ = 0.01, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else predict no

if (location = transit authority) then predict yes
else if (stop reason = suspicious object) then predict yes
else predict no

Weapon prediction (λ = 0.005, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (location = housing authority) then predict no
else if (location = transit authority) then predict yes
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else if (location = transit authority) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (city = Bronx) then predict no
else if (location = housing authority) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

(cid:46) Found by 8 folds

(cid:46) Found by 2 folds

(cid:46) Found by 7 folds

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

Figure 21: Example optimal rule lists for the NYPD stop-and-frisk data set, found by
CORELS with regularization parameters λ = 0.01 (top) and 0.005 (bottom),
across 10 cross-validation folds.

70

Learning Certifiably Optimal Rule Lists

Weapon prediction (λ = 0.01, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (inside or outside = outside) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (inside or outside = inside) then predict yes
else predict no

Weapon prediction (λ = 0.005, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = f urtive movements) then predict no
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f its description) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f urtive movements) then predict no
else if (stop reason = f its description) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = f its description) then predict no
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = f urtive movements) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = acting as lookout) then predict no
else predict yes

(cid:46) Found by 7 folds

(cid:46) Found by 3 folds

(cid:46) Found by 2 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

Figure 22: Example optimal rule lists for the NYPD stop-and-frisk data set (Feature Set D)
found by CORELS with regularization parameters λ = 0.01 (top) and 0.005 (bot-
tom), across 10 cross-validation folds. For λ = 0.005, we show results from 7 folds;
the remaining 3 folds were equivalent, up to a permutation of the preﬁx rules,
and started with the same ﬁrst preﬁx rule.

71

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

F.3 NYCLU Stop-and-frisk Data Set

We show example optimal rule lists that predict whether a weapon will be found on a
stopped individual who is frisked or searched, learned from the NYCLU data set. Figure 23
shows regularization parameters λ = 0.04 and 0.01, and Figure 24 shows λ = 0.0025. We
showed a representative solution for λ = 0.01 in Figure 5 (§6.3).

For each of the two larger regularization parameters in Figure 23, λ = 0.04 (top) and
0.01 (bottom), we observe that across the folds, all the optimal rule lists contain the same or
equivalent rules, up to a permutation. With the smaller regularization parameter λ = 0.0025
(Figure 24), we observe a greater diversity of longer optimal rule lists, though they share
similar structure.

Weapon prediction (λ = 0.04)

if (stop reason = suspicious object) then predict yes
else if (stop reason (cid:54)= suspicious bulge) then predict no
else predict yes

if (stop reason = suspicious bulge) then predict yes
else if (stop reason (cid:54)= suspicious object) then predict no
else predict yes

Weapon prediction (λ = 0.01)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (stop reason (cid:54)= suspicious bulge) then predict no
else predict yes

if (location = transit authority) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = suspicious object) then predict yes
else predict no

if (location = transit authority) then predict yes
else if (stop reason = suspicious object) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else predict no

if (location = transit authority) then predict yes
else if (stop reason = suspicious object) then predict yes
else if (stop reason (cid:54)= suspicious bulge) then predict no
else predict yes

(cid:46) Found by 7 folds

(cid:46) Found by 3 folds

(cid:46) Found by 4 folds

(cid:46) Found by 3 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

Figure 23: Example optimal rule lists for the NYCLU stop-and-frisk data set, found by
CORELS with regularization parameters λ = 0.04 (top) and 0.01 (bottom),
across 10 cross-validation folds.

72

Learning Certifiably Optimal Rule Lists

Weapon prediction (λ = 0.0025)

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = transit authority) then predict yes
else if (inside or outside = inside) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (location = housing authority) then predict no
else if (stop reason = casing) then predict no
else if (stop reason = f its description) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = drug transaction) then predict no
else if (stop reason = suspicious object) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (location = housing authority) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = casing) then predict no
else if (city = M anhattan) then predict yes
else if (city = Bronx) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = transit authority) then predict yes
else if (inside or outside = inside) then predict no
else if (city = M anhattan) then predict yes
else if (additional circumstances = changed direction) then predict no
else if (city = Bronx) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = actions of violent crime) then predict no
else if (stop reason = f its description) then predict no
else if (location = transit authority) then predict yes
else if (inside or outside = inside) then predict no
else if (city = M anhattan) then predict yes
else if (additional circumstances = evasive response) then predict no
else if (city = Bronx) then predict yes
else predict no

(cid:46) Found by 4 folds (K = 7)

(cid:46) Found by 1 fold (K = 6)

(cid:46) Found by 1 fold (K = 6)

(cid:46) Found by 1 fold (K = 6)

(cid:46) Found by 1 fold (K = 8)

(cid:46) Found by 1 fold (K = 9)

(cid:46) Found by 1 fold (K = 10)

Figure 24: Example optimal rule lists for the NYCLU stop-and-frisk data set λ = 0.0025.

73

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Appendix G. Additional Results on Predictive Performance and Model

Size for CORELS and Other Algorithms

In this appendix, we plot TPR, FPR, and model size for CORELS and three other algo-
rithms, using the NYPD data set (Feature Set D).

Figure 25: TPR (top) and FPR (bottom) for the test set, as a function of model size,
across diﬀerent methods, for weapon prediction with the NYPD stop-and-frisk
data set (Feature Set D). In the legend, numbers in parentheses are algorithm
parameters, as in Figure 12. Legend markers and error bars indicate means and
standard deviations, respectively, across cross-validation folds. C4.5 ﬁnds large
models for all tested parameters.

References

E. Angelino, N. Larus-Stone, D. Alabi, M. Seltzer, and C. Rudin. Learning certiﬁably
optimal rule lists for categorical data. In ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD), 2017.

K. P. Bennett and J. A. Blue. Optimal decision trees. Technical report, R.P.I. Math Report

No. 214, Rensselaer Polytechnic Institute, 1996.

I. Bratko. Machine learning: Between accuracy and interpretability. In Learning, Networks
and Statistics, volume 382 of International Centre for Mechanical Sciences, pages 163–

74

Learning Certifiably Optimal Rule Lists

177. Springer Vienna, 1997.

Trees. Wadsworth, 1984.

2013.

L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and Regression

S. Bushway. Is there any logic to using logit. Criminology & Public Policy, 12(3):563–567,

C. Chen and C. Rudin. An optimization approach to learning falling rule lists. In Interna-

tional Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2018.

H. A. Chipman, E. I. George, and R. E. McCulloch. Bayesian CART model search. Journal

of the American Statistical Association, 93(443):935–948, 1998.

H. A. Chipman, E. I. George, and R. E. McCulloch. Bayesian treed models. Machine

Learning, 48(1):299–320, 2002.

H. A. Chipman, E. I. George, and R. E. McCulloch. BART: Bayesian additive regression

trees. The Annals of Applied Statistics, 4(1):266–298, 2010.

P. Clark and T. Niblett. The CN2 induction algorithm. Machine Learning, 3:261–283, 1989.

W. W. Cohen. Fast eﬀective rule induction.

In International Conference on Machine

Learning (ICML), pages 115–123, 1995.

R. M. Dawes. The robust beauty of improper linear models in decision making. American

Psychologist, 34(7):571–582, 1979.

D. Dension, B. Mallick, and A.F.M. Smith. A Bayesian CART algorithm. Biometrika, 85

(2):363–377, 1998.

trees, 1996.

Advances, 4(1), 2018.

D. Dobkin, T. Fulton, D. Gunopulos, S. Kasif, and S. Salzberg. Induction of shallow decision

J. Dressel and H. Farid. The accuracy, fairness, and limits of predicting recidivism. Science

A. Farhangfar, R. Greiner, and M. Zinkevich. A fast way to produce optimal ﬁxed-depth
decision trees. In International Symposium on Artiﬁcial Intelligence and Mathematics
(ISAIM), 2008.

E. Frank and I. H. Witten. Generating accurate rule sets without global optimization. In

International Conference on Machine Learning (ICML), pages 144–151, 1998.

A. A. Freitas. Comprehensible classiﬁcation models: A position paper. ACM SIGKDD

Explorations Newsletter, 15(1):1–10, 2014.

M. Garofalakis, D. Hyun, R. Rastogi, and K. Shim. Eﬃcient algorithms for constructing
decision trees with constraints. In ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD), pages 335–339, 2000.

75

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

C. Giraud-Carrier. Beyond predictive accuracy: What? In ECML-98 Workshop on Up-
grading Learning to Meta-Level: Model Selection and Data Transformation, pages 78–85,
1998.

S. Goel, J. M. Rao, and R. Shroﬀ. Precinct or prejudice? Understanding racial disparities in
New York City’s stop-and-frisk policy. The Annals of Applied Statistics, 10(1):365–394,
03 2016.

S. T. Goh and C. Rudin. Box drawings for learning with imbalanced data. In ACM SIGKDD

International Conference on Knowledge Discovery and Data Mining (KDD), 2014.

S. T. Goh and C. Rudin. A minimax surrogate loss approach to conditional diﬀerence
estimation. CoRR, abs/1803.03769, 2018. URL https://arxiv.org/abs/1803.03769.

B. Goodman and S. Flaxman. European Union regulations on algorithmic decision-making
and a “right to explanation”. In ICML Workshop on Human Interpretability in Machine
Learning (WHI), 2016.

R. C. Holte. Very simple classiﬁcation rules perform well on most commonly used datasets.

Machine Learning, 11(1):63–91, 1993.

J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, and B. Baesens. An empirical evaluation
of the comprehensibility of decision table, tree and rule based predictive models. Decision
Support Systems, 51(1):141–154, 2011.

V. Kaxiras and A. Saligrama. Building predictive models with rule lists, 2018. URL https:

//corels.eecs.harvard.edu.

H. Lakkaraju and C. Rudin. Cost-sensitive and interpretable dynamic treatment regimes
based on rule lists. In International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS), 2017.

J. Larson, S. Mattu, L. Kirchner, and J. Angwin. How we analyzed the COMPAS recidivism

algorithm. ProPublica, 2016.

N. Larus-Stone, E. Angelino, D. Alabi, M. Seltzer, V. Kaxiras, A. Saligrama, and C. Rudin.
Systems optimizations for learning certiﬁably optimal rule lists. In SysML Conference,
2018.

N. L. Larus-Stone. Learning Certiﬁably Optimal Rule Lists: A Case For Discrete Optimiza-

tion in the 21st Century. 2017. Undergraduate thesis, Harvard College.

B. Letham, C. Rudin, T. H. McCormick, and D. Madigan. Interpretable classiﬁers using
rules and Bayesian analysis: Building a better stroke prediction model. The Annals of
Applied Statistics, 9(3):1350–1371, 2015.

O. Li, H. Liu, C. Chen, and C. Rudin. Deep learning for case-based reasoning through pro-
totypes: A neural network that explains its predictions. In Proceedings of the Association
for the Advancement of Artiﬁcial Intelligence (AAAI), 2018.

76

Learning Certifiably Optimal Rule Lists

W. Li, J. Han, and J. Pei. CMAR: Accurate and eﬃcient classiﬁcation based on multiple
class-association rules. IEEE International Conference on Data Mining (ICDM), pages
369–376, 2001.

J. T. Linderoth and M. W. P. Savelsbergh. A computational study of search strategies for
mixed integer programming. INFORMS Journal on Computing, 11(2):173–187, 1999.

B. Liu, W. Hsu, and Y. Ma. Integrating classiﬁcation and association rule mining. In ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),
pages 80–96, 1998.

M. Marchand and M. Sokolova. Learning with decision lists of data-dependent features.

Journal of Machine Learning Research, 6:427–451, 2005.

R. S. Michalski. On the quasi-minimal solution of the general covering problem. In Inter-

national Symposium on Information Processing, pages 125–128, 1969.

New York Civil Liberties Union. Stop-and-frisk data, 2014. URL http://www.nyclu.org/

content/stop-and-frisk-data.

New York Police Department. Stop, question and frisk data, 2016. URL http://www1.

nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page.

S. Nijssen and E. Fromont. Optimal constraint-based decision tree induction from itemset

lattices. Data Mining and Knowledge Discovery, 21(1):9–51, 2010.

J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1993.

P. R. Rijnbeek and J. A. Kors. Finding a short and accurate decision rule in disjunctive

normal form by exhaustive search. Machine Learning, 80(1):33–62, July 2010.

R. L. Rivest. Learning decision lists. Machine Learning, 2(3):229–246, November 1987.

U. R¨uckert and L. De Raedt. An experimental evaluation of simplicity in rule learning.

Artiﬁcial Intelligence, 172:19–28, 2008.

C. Rudin and S¸. Ertekin. Learning customized and optimized lists of rules with mathemat-

ical programming. Submitted, 2016.

C. Rudin, B. Letham, and D. Madigan. Learning theory analysis for association rules and
sequential event prediction. Journal of Machine Learning Research, 14:3384–3436, 2013.

S. R¨uping. Learning interpretable models. PhD thesis, Universit¨at Dortmund, 2006.

G. Shmueli. To explain or to predict? Statistical Science, 25(3):289–310, August 2010.

M. Sokolova, M. Marchand, N. Japkowicz, and J. Shawe-Taylor. The decision list machine.
In Advances in Neural Information Processing Systems (NIPS), volume 15, pages 921–
928, 2003.

77

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

N. Tollenaar and P. van der Heijden. Which method predicts recidivism best?: A comparison
of statistical, machine learning and data mining predictive models. Journal of the Royal
Statistical Society: Series A (Statistics in Society), 176(2):565–584, 2013.

B. Ustun and C. Rudin. Supersparse linear integer models for optimized medical scoring

systems. Machine Learning, 102(3):349–391, 2016.

B. Ustun and C. Rudin. Optimized risk scores. In ACM SIGKDD International Conference

on Knowledge Discovery and Data Mining (KDD), 2017.

K. Vanhoof and B. Depaire. Structure of association rule classiﬁers: A review. In Inter-
national Conference on Intelligent Systems and Knowledge Engineering (ISKE), pages
9–12, 2010.

A. Vellido, J. D. Mart´ın-Guerrero, and P. J.G. Lisboa. Making machine learning models
In European Symposium on Artiﬁcial Neural Networks, Computational

interpretable.
Intelligence and Machine Learning (ESANN), 2012.

F. Wang and C. Rudin. Falling rule lists. In International Conference on Artiﬁcial Intelli-

gence and Statistics (AISTATS), 2015a.

F. Wang and C. Rudin. Causal falling rule lists. CoRR, abs/1510.05189, 2015b. URL

https://arxiv.org/abs/1510.05189.

T. Wang. Hybrid decision making: When interpretable models collaborate with black-box

models. CoRR, abs/1802.04346, 2018. URL http://arxiv.org/abs/1802.04346.

T. Wang, C. Rudin, F. Doshi-Velez, Y. Liu, E. Klampﬂ, and P. MacNeille. Bayesian or’s
of and’s for interpretable classiﬁcation with application to context aware recommender
systems. In International Conference on Data Mining (ICDM), 2016.

T. Wang, C. Rudin, F. Doshi-Velez, Y. Liu, E. Klampﬂ, and P. MacNeille. A Bayesian frame-
work for learning rule sets for interpretable classiﬁcation. Journal of Machine Learning
Research, 18(70):1–37, 2017.

E. Westervelt.

man’s murder?,
did-a-bail-reform-algorithm-contribute-to-this-san-francisco-man-s-murder.

reform algorithm contribute to this San Francisco
URL https://www.npr.org/2017/08/18/543976003/

Did a bail
2017.

H. Yang, C. Rudin, and M. Seltzer. Scalable Bayesian rule lists. In International Conference

on Machine Learning (ICML), 2017.

X. Yin and J. Han. CPAR: Classiﬁcation based on predictive association rules. In SIAM

International Conference on Data Mining (SDM), pages 331–335, 2003.

J. Zeng, B. Ustun, and C. Rudin. Interpretable classiﬁcation models for recidivism pre-
diction. Journal of the Royal Statistical Society: Series A (Statistics in Society), 180(3):
689–722, 2017.

Y. Zhang, E. B. Laber, A. Tsiatis, and M. Davidian. Using decision lists to construct
interpretable and parsimonious treatment regimes. Biometrics, 71(4):895–904, 2015.

78

8
1
0
2
 
g
u
A
 
3
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
0
7
1
0
.
4
0
7
1
:
v
i
X
r
a

Journal of Machine Learning Research 18 (2018) 1-78

Submitted 11/17; Published 6/18

Learning Certiﬁably Optimal Rule Lists for Categorical Data

Elaine Angelino
Department of Electrical Engineering and Computer Sciences
University of California, Berkeley, Berkeley, CA 94720

elaine@eecs.berkeley.edu

Nicholas Larus-Stone
Daniel Alabi
Margo Seltzer
School of Engineering and Applied Sciences
Harvard University, Cambridge, MA 02138

nlarusstone@alumni.harvard.edu
alabid@g.harvard.edu
margo@eecs.harvard.edu

Cynthia Rudin∗
Department of Computer Science and Department of Electrical and Computer Engineering
Duke University, Durham, NC 27708

cynthia@cs.duke.edu

Editor: Maya Gupta
∗To whom correspondence should be addressed.

Abstract
We present the design and implementation of a custom discrete optimization technique for
building rule lists over a categorical feature space. Our algorithm produces rule lists with
optimal training performance, according to the regularized empirical risk, with a certiﬁcate
of optimality. By leveraging algorithmic bounds, eﬃcient data structures, and computa-
tional reuse, we achieve several orders of magnitude speedup in time and a massive reduc-
tion of memory consumption. We demonstrate that our approach produces optimal rule
lists on practical problems in seconds. Our results indicate that it is possible to construct
optimal sparse rule lists that are approximately as accurate as the COMPAS proprietary
risk prediction tool on data from Broward County, Florida, but that are completely inter-
pretable. This framework is a novel alternative to CART and other decision tree methods
for interpretable modeling.
Keywords: rule lists, decision trees, optimization, interpretable models, criminal justice
applications

1. Introduction

As machine learning continues to gain prominence in socially-important decision-making,
the interpretability of predictive models remains a crucial problem. Our goal is to build
models that are highly predictive, transparent, and easily understood by humans. We use
rule lists, also known as decision lists, to achieve this goal. Rule lists are predictive models
composed of if-then statements; these models are interpretable because the rules provide a
reason for each prediction (Figure 1).

Constructing rule lists, or more generally, decision trees, has been a challenge for more
than 30 years; most approaches use greedy splitting techniques (Rivest, 1987; Breiman
et al., 1984; Quinlan, 1993). Recent approaches use Bayesian analysis, either to ﬁnd a locally
optimal solution (Chipman et al., 1998) or to explore the search space (Letham et al., 2015;

c(cid:13)2018 Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v18/17-716.html.

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 23) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

Figure 1: An example rule list that predicts two-year recidivism for the ProPublica data

set, found by CORELS.

Yang et al., 2017). These approaches achieve high accuracy while also managing to run
reasonably quickly. However, despite the apparent accuracy of the rule lists generated by
these algorithms, there is no way to determine either if the generated rule list is optimal
or how close it is to optimal, where optimality is deﬁned with respect to minimization of a
regularized loss function.

Optimality is important, because there are societal implications for a lack of optimality.
Consider the ProPublica article on the Correctional Oﬀender Management Proﬁling for Al-
ternative Sanctions (COMPAS) recidivism prediction tool (Larson et al., 2016). It highlights
a case where a black box, proprietary predictive model is being used for recidivism predic-
tion. The authors hypothesize that the COMPAS scores are racially biased, but since the
model is not transparent, no one (outside of the creators of COMPAS) can determine the
reason or extent of the bias (Larson et al., 2016), nor can anyone determine the reason for
any particular prediction. By using COMPAS, users implicitly assumed that a transparent
model would not be suﬃciently accurate for recidivism prediction, i.e., they assumed that
a black box model would provide better accuracy. We wondered whether there was indeed
no transparent and suﬃciently accurate model. Answering this question requires solving a
computationally hard problem. Namely, we would like to both ﬁnd a transparent model that
is optimal within a particular pre-determined class of models and produce a certiﬁcate of
its optimality, with respect to the regularized empirical risk. This would enable one to say,
for this problem and model class, with certainty and before resorting to black box methods,
whether there exists a transparent model. While there may be diﬀerences between train-
ing and test performance, ﬁnding the simplest model with optimal training performance is
prescribed by statistical learning theory.

To that end, we consider the class of rule lists assembled from pre-mined frequent item-
sets and search for an optimal rule list that minimizes a regularized risk function, R. This
is a hard discrete optimization problem. Brute force solutions that minimize R are compu-
tationally prohibitive due to the exponential number of possible rule lists. However, this is
a worst case bound that is not realized in practical settings. For realistic cases, it is possible
to solve fairly large cases of this problem to optimality, with the careful use of algorithms,
data structures, and implementation techniques.

We develop specialized tools from the ﬁelds of discrete optimization and artiﬁcial intel-
ligence. Speciﬁcally, we introduce a special branch-and bound algorithm, called Certiﬁably
Optimal RulE ListS (CORELS), that provides the optimal solution according to the train-
ing objective, along with a certiﬁcate of optimality. The certiﬁcate of optimality means that
we can investigate how close other models (e.g., models provided by greedy algorithms) are
to optimal.

2

Learning Certifiably Optimal Rule Lists

Within its branch-and-bound procedure, CORELS maintains a lower bound on the
minimum value of R that each incomplete rule list can achieve. This allows CORELS to
prune an incomplete rule list (and every possible extension) if the bound is larger than
the error of the best rule list that it has already evaluated. The use of careful bounding
techniques leads to massive pruning of the search space of potential rule lists. The algorithm
continues to consider incomplete and complete rule lists until it has either examined or
eliminated every rule list from consideration. Thus, CORELS terminates with the optimal
rule list and a certiﬁcate of optimality.

The eﬃciency of CORELS depends on how much of the search space our bounds allow us
to prune; we seek a tight lower bound on R. The bound we maintain throughout execution is
a maximum of several bounds that come in three categories. The ﬁrst category of bounds are
those intrinsic to the rules themselves. This category includes bounds stating that each rule
must capture suﬃcient data; if not, the rule list is provably non-optimal. The second type of
bound compares a lower bound on the value of R to that of the current best solution. This
allows us to exclude parts of the search space that could never be better than our current
solution. Finally, our last type of bound is based on comparing incomplete rule lists that
capture the same data and allows us to pursue only the most accurate option. This last
class of bounds is especially important—without our use of a novel symmetry-aware map,
we are unable to solve most problems of reasonable scale. This symmetry-aware map keeps
track of the best accuracy over all observed permutations of a given incomplete rule list.

We keep track of these bounds using a modiﬁed preﬁx tree, a data structure also known
as a trie. Each node in the preﬁx tree represents an individual rule; thus, each path in the
tree represents a rule list such that the ﬁnal node in the path contains metrics about that
rule list. This tree structure, together with a search policy and sometimes a queue, enables a
variety of strategies, including breadth-ﬁrst, best-ﬁrst, and stochastic search. In particular,
we can design diﬀerent best-ﬁrst strategies by customizing how we order elements in a
priority queue. In addition, we are able to limit the number of nodes in the trie and thereby
enable tuning of space-time tradeoﬀs in a robust manner. This trie structure is a useful way
of organizing the generation and evaluation of rule lists.

We evaluated CORELS on a number of publicly available data sets. Our metric of
success was 10-fold cross-validated prediction accuracy on a subset of the data. These data
sets involve hundreds of rules and thousands of observations. CORELS is generally able to
ﬁnd an optimal rule list in a matter of seconds and certify its optimality within about 10
minutes. We show that we are able to achieve better or similar out-of-sample accuracy on
these data sets compared to the popular greedy algorithms, CART and C4.5.

CORELS targets large (not massive) problems, where interpretability and certiﬁable
optimality are important. We illustrate the eﬃcacy of our approach using (1) the ProPublica
COMPAS data set (Larson et al., 2016), for the problem of two-year recidivism prediction,
and (2) stop-and-frisk data sets from the NYPD (New York Police Department, 2016) and
the NYCLU (New York Civil Liberties Union, 2014), to predict whether a weapon will
be found on a stopped individual who is frisked or searched. On these data, we produce
certiﬁably optimal, interpretable rule lists that achieve the same accuracy as approaches
such as random forests. This calls into question the need for use of a proprietary, black box
algorithm for recidivism prediction.

3

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Our work overlaps with the thesis of Larus-Stone (2017). We have also written a pre-
liminary conference version of this article (Angelino et al., 2017), and a report highlighting
systems optimizations of our implementation (Larus-Stone et al., 2018); the latter includes
additional empirical measurements not presented here.

Our code is at https://github.com/nlarusstone/corels, where we provide the C++
implementation we used in our experiments (§6). Kaxiras and Saligrama (2018) have also
created an interactive web interface at https://corels.eecs.harvard.edu, where a user
can upload data and run CORELS from a browser.

2. Related Work

Since every rule list is a decision tree and every decision tree can be expressed as an
equivalent rule list, the problem we are solving is a version of the “optimal decision tree”
problem, though regularization changes the nature of the problem (as shown through our
bounds). The optimal decision tree problem is computationally hard, though since the late
1990’s, there has been research on building optimal decision trees using optimization tech-
niques (Bennett and Blue, 1996; Dobkin et al., 1996; Farhangfar et al., 2008). A particularly
interesting paper along these lines is that of Nijssen and Fromont (2010), who created a
“bottom-up” way to form optimal decision trees. Their method performs an expensive search
step, mining all possible leaves (rather than all possible rules), and uses those leaves to form
trees. Their method can lead to memory problems, but it is possible that these memory
issues can be mitigated using the theorems in this paper.1 None of these methods used the
tight bounds and data structures of CORELS.

Because the optimal decision tree problem is hard, there are a huge number of algo-
rithms such as CART (Breiman et al., 1984) and C4.5 (Quinlan, 1993) that do not perform
exploration of the search space beyond greedy splitting. Similarly, there are decision list
and associative classiﬁcation methods that construct rule lists iteratively in a greedy way
(Rivest, 1987; Liu et al., 1998; Li et al., 2001; Yin and Han, 2003; Sokolova et al., 2003;
Marchand and Sokolova, 2005; Vanhoof and Depaire, 2010; Rudin et al., 2013). Some ex-
ploration of the search space is done by Bayesian decision tree methods (Dension et al.,
1998; Chipman et al., 2002, 2010) and Bayesian rule-based methods (Letham et al., 2015;
Yang et al., 2017). The space of trees of a given depth is much larger than the space of rule
lists of that same depth, and the trees within the Bayesian tree algorithms are grown in a
top-down greedy way. Because of this, authors of Bayesian tree algorithms have noted that
their MCMC chains tend to reach only locally optimal solutions. The RIPPER algorithm
(Cohen, 1995) is similar to the Bayesian tree methods in that it grows, prunes, and then
locally optimizes. The space of rule lists is smaller than that of trees, and has simpler struc-
ture. Consequently, Bayesian rule list algorithms tend to be more successful at escaping
local minima and can introduce methods of exploring the search space that exploit this
structure—these properties motivate our focus on lists. That said, the tightest bounds for
the Bayesian lists (namely, those of Yang et al., 2017, upon whose work we build), are not
nearly as tight as those of CORELS.

1. There is no public version of their code for distribution as of this writing.

4

Learning Certifiably Optimal Rule Lists

Tight bounds, on the other hand, have been developed for the (immense) literature on
building disjunctive normal form (DNF) models; a good example of this is the work of Rijn-
beek and Kors (2010). For models of a given size, since the class of DNF’s is a proper subset
of decision lists, our framework can be restricted to learn optimal DNF’s. The ﬁeld of DNF
learning includes work from the ﬁelds of rule learning/induction (e.g., early algorithms by
Michalski, 1969; Clark and Niblett, 1989; Frank and Witten, 1998) and associative classiﬁ-
cation (Vanhoof and Depaire, 2010). Most papers in these ﬁelds aim to carefully guide the
search through the space of models. If we were to place a restriction on our code to learn
DNF’s, which would require restricting predictions within the list to the positive class only,
we could potentially use methods from rule learning and associative classiﬁcation to help
order CORELS’ queue, which would in turn help us eliminate parts of the search space
more quickly.

Some of our bounds, including the minimum support bound (§3.7, Theorem 10), come
from Rudin and Ertekin (2016), who provide ﬂexible mixed-integer programming (MIP)
formulations using the same objective as we use here; MIP solvers in general cannot compete
with the speed of CORELS.

CORELS depends on pre-mined rules, which we obtain here via enumeration. The litera-
ture on association rule mining is huge, and any method for rule mining could be reasonably
substituted.

CORELS’ main use is for producing interpretable predictive models. There is a grow-
ing interest in interpretable (transparent, comprehensible) models because of their societal
importance (see R¨uping, 2006; Bratko, 1997; Dawes, 1979; Vellido et al., 2012; Giraud-
Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans et al., 2011; Freitas, 2014). There are
now regulations on algorithmic decision-making in the European Union on the “right to an
explanation” (Goodman and Flaxman, 2016) that would legally require interpretability of
predictions. There is work in both the DNF literature (R¨uckert and Raedt, 2008) and deci-
sion tree literature (Garofalakis et al., 2000) on building interpretable models. Interpretable
models must be so sparse that they need to be heavily optimized; heuristics tend to produce
either inaccurate or non-sparse models.

Interpretability has many meanings, and it is possible to extend the ideas in this work
to other deﬁnitions of interpretability; these rule lists may have exotic constraints that help
with ease-of-use. For example, Falling Rule Lists (Wang and Rudin, 2015a) are constrained
to have decreasing probabilities down the list, which makes it easier to assess whether an
observation is in a high risk subgroup. In parallel to this paper, we have been working on
an algorithm for Falling Rule Lists (Chen and Rudin, 2018) with bounds similar to those
presented here, but even CORELS’ basic support bounds do not hold for the falling case,
which is much more complicated. One advantage of the approach taken by Chen and Rudin
(2018) is that it can handle class imbalance by weighting the positive and negative classes
diﬀerently; this extension is possible in CORELS but not addressed here.

The models produced by CORELS are predictive only; they cannot be used for policy-
making because they are not causal models, they do not include the costs of true and false
positives, nor the cost of gathering information. It is possible to adapt CORELS’ frame-
work for causal inference (Wang and Rudin, 2015b), dynamic treatment regimes (Zhang
et al., 2015), or cost-sensitive dynamic treatment regimes (Lakkaraju and Rudin, 2017) to

5

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 23) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if p1 then predict q1
else if p2 then predict q2
else if p3 then predict q3
else predict q0

Figure 2: The rule list d = (r1, r2, r3, r0). Each rule is of the form rk = pk → qk, for all
k = 0, . . . , 3. We can also express this rule list as d = (dp, δp, q0, K), where
dp = (p1, p2, p3), δp = (1, 1, 1, 1), q0 = 0, and K = 3. This is the same 3-rule list
as in Figure 1, that predicts two-year recidivism for the ProPublica data set.

help with policy design. CORELS could potentially be adapted to handle these kinds of
interesting problems.

3. Learning Optimal Rule Lists

In this section, we present our framework for learning certiﬁably optimal rule lists. First, we
deﬁne our setting and useful notation (§3.1) and then the objective function we seek to min-
imize (§3.2). Next, we describe the principal structure of our optimization algorithm (§3.3),
which depends on a hierarchically structured objective lower bound (§3.4). We then derive
a series of additional bounds that we incorporate into our algorithm, because they enable
aggressive pruning of our state space.

3.1 Notation

We restrict our setting to binary classiﬁcation, where rule lists are Boolean functions; this
framework is straightforward to generalize to multi-class classiﬁcation. Let {(xn, yn)}N
n=1
denote training data, where xn ∈ {0, 1}J are binary features and yn ∈ {0, 1} are labels.
Let x = {xn}N

n=1, and let xn,j denote the j-th feature of xn.

n=1 and y = {yn}N

A rule list d = (r1, r2, . . . , rK, r0) of length K ≥ 0 is a (K + 1)-tuple consisting of K
distinct association rules, rk = pk → qk, for k = 1, . . . , K, followed by a default rule r0.
Figure 2 illustrates a rule list, d = (r1, r2, r3, r0), which for clarity, we sometimes call a K-
rule list. An association rule r = p → q is an implication corresponding to the conditional
statement, “if p, then q.” In our setting, an antecedent p is a Boolean assertion that evaluates
to either true or false for each datum xn, and a consequent q is a label prediction. For
example, (xn,1 = 0) ∧ (xn,3 = 1) → (yn = 1) is an association rule. The ﬁnal default rule r0
in a rule list can be thought of as a special association rule p0 → q0 whose antecedent p0
simply asserts true.

Let d = (r1, r2, . . . , rK, r0) be a K-rule list, where rk = pk → qk for each k = 0, . . . , K.
We introduce a useful alternate rule list representation: d = (dp, δp, q0, K), where we deﬁne
dp = (p1, . . . , pK) to be d’s preﬁx, δp = (q1, . . . , qK) ∈ {0, 1}K gives the label predictions
associated with dp, and q0 ∈ {0, 1} is the default label prediction. For example, for the
rule list in Figure 1, we would write d = (dp, δp, q0, K), where dp = (p1, p2, p3), δp = (1, 1, 1),
q0 = 0, and K = 3. Note that ((), (), q0, 0) is a well-deﬁned rule list with an empty preﬁx;
it is completely deﬁned by a single default rule.

Let dp = (p1, . . . , pk, . . . , pK) be an antecedent list, then for any k ≤ K, we deﬁne dk

(p1, . . . , pk) to be the k-preﬁx of dp. For any such k-preﬁx dk

p =
p, we say that dp starts with dk
p.

6

Learning Certifiably Optimal Rule Lists

For any given space of rule lists, we deﬁne σ(dp) to be the set of all rule lists whose preﬁxes
start with dp:

σ(dp) = {(d(cid:48)

p, δ(cid:48)

p, q(cid:48)

0, K(cid:48)) : d(cid:48)

p starts with dp}.

(1)

If dp = (p1, . . . , pK) and d(cid:48)
and extends it by a single antecedent, we say that dp is the parent of d(cid:48)
child of dp.

p = (p1, . . . , pK, pK+1) are two preﬁxes such that d(cid:48)

p starts with dp
p is a

p and that d(cid:48)

A rule list d classiﬁes datum xn by providing the label prediction qk of the ﬁrst rule rk
whose antecedent pk is true for xn. We say that an antecedent pk of antecedent list dp
captures xn in the context of dp if pk is the ﬁrst antecedent in dp that evaluates to true
for xn. We also say that a preﬁx captures those data captured by its antecedents; for a rule
list d = (dp, δp, q0, K), data not captured by the preﬁx dp are classiﬁed according to the
default label prediction q0.

Let β be a set of antecedents. We deﬁne cap(xn, β) = 1 if an antecedent in β captures
p starts

p be preﬁxes such that d(cid:48)

datum xn, and 0 otherwise. For example, let dp and d(cid:48)
with dp, then d(cid:48)

p captures all the data that dp captures:

{xn : cap(xn, dp)} ⊆ {xn : cap(xn, d(cid:48)

p)}.

Now let dp be an ordered list of antecedents, and let β be a subset of antecedents in dp.
Let us deﬁne cap(xn, β | dp) = 1 if β captures datum xn in the context of dp, i.e., if the ﬁrst
antecedent in dp that evaluates to true for xn is an antecedent in β, and 0 otherwise. Thus,
cap(xn, β | dp) = 1 only if cap(xn, β) = 1; cap(xn, β | dp) = 0 either if cap(xn, β) = 0, or if
cap(xn, β) = 1 but there is an antecedent α in dp, preceding all antecedents in β, such that
cap(xn, α) = 1. For example, if dp = (p1, . . . , pk, . . . , pK) is a preﬁx, then

cap(xn, pk | dp) =

¬ cap(xn, pk(cid:48))

∧ cap(xn, pk)

(cid:32) k−1
(cid:94)

k(cid:48)=1

(cid:33)

indicates whether antecedent pk captures datum xn in the context of dp. Now, deﬁne
supp(β, x) to be the normalized support of β,

and similarly deﬁne supp(β, x | dp) to be the normalized support of β in the context of dp,

supp(β, x) =

cap(xn, β),

supp(β, x | dp) =

cap(xn, β | dp),

(2)

(3)

Next, we address how empirical data constrains rule lists. Given training data (x, y), an
antecedent list dp = (p1, . . . , pK) implies a rule list d = (dp, δp, q0, K) with preﬁx dp, where
the label predictions δp = (q1, . . . , qK) and q0 are empirically set to minimize the number
of misclassiﬁcation errors made by the rule list on the training data. Thus for 1 ≤ k ≤ K,
label prediction qk corresponds to the majority label of data captured by antecedent pk in

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

7

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

the context of dp, and the default q0 corresponds to the majority label of data not captured
by dp. In the remainder of our presentation, whenever we refer to a rule list with a particular
preﬁx, we implicitly assume these empirically determined label predictions.

Our method is technically an associative classiﬁcation method since it leverages pre-

mined rules.

3.2 Objective Function

We deﬁne a simple objective function for a rule list d = (dp, δp, q0, K):

R(d, x, y) = (cid:96)(d, x, y) + λK.

(4)

This objective function is a regularized empirical risk; it consists of a loss (cid:96)(d, x, y), mea-
suring misclassiﬁcation error, and a regularization term that penalizes longer rule lists.
(cid:96)(d, x, y) is the fraction of training data whose labels are incorrectly predicted by d. In
our setting, the regularization parameter λ ≥ 0 is a small constant; e.g., λ = 0.01 can be
thought of as adding a penalty equivalent to misclassifying 1% of data when increasing a
rule list’s length by one association rule.

3.3 Optimization Framework

Our objective has structure amenable to global optimization via a branch-and-bound frame-
work. In particular, we make a series of important observations, each of which translates
into a useful bound, and that together interact to eliminate large parts of the search space.
We discuss these in depth in what follows:

• Lower bounds on a preﬁx also hold for every extension of that preﬁx. (§3.4, Theorem 1)

• If a rule list is not accurate enough with respect to its length, we can prune all

extensions of it. (§3.4, Lemma 2)

• We can calculate a priori an upper bound on the maximum length of an optimal rule

list. (§3.5, Theorem 6)

• Each rule in an optimal rule list must have support that is suﬃciently large. This allows
us to construct rule lists from frequent itemsets, while preserving the guarantee that
we can ﬁnd a globally optimal rule list from pre-mined rules. (§3.7, Theorem 10)

• Each rule in an optimal rule list must predict accurately. In particular, the number of
observations predicted correctly by each rule in an optimal rule list must be above a
threshold. (§3.7, Theorem 11)

• We need only consider the optimal permutation of antecedents in a preﬁx; we can

omit all other permutations. (§3.10, Theorem 15 and Corollary 16)

• If multiple observations have identical features and opposite labels, we know that any
model will make mistakes. In particular, the number of mistakes on these observations
will be at least the number of observations with the minority label. (§3.14, Theorem 20)

8

Learning Certifiably Optimal Rule Lists

3.4 Hierarchical Objective Lower Bound

We can decompose the misclassiﬁcation error in (4) into two contributions corresponding
to the preﬁx and the default rule:

(cid:96)(d, x, y) ≡ (cid:96)p(dp, δp, x, y) + (cid:96)0(dp, q0, x, y),

where dp = (p1, . . . , pK) and δp = (q1, . . . , qK);

(cid:96)p(dp, δp, x, y) =

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn]

is the fraction of data captured and misclassiﬁed by the preﬁx, and

(cid:96)0(dp, q0, x, y) =

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn]

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

1
N

N
(cid:88)

n=1

is the fraction of data not captured by the preﬁx and misclassiﬁed by the default rule.
Eliminating the latter error term gives a lower bound b(dp, x, y) on the objective,

b(dp, x, y) ≡ (cid:96)p(dp, δp, x, y) + λK ≤ R(d, x, y),

(5)

where we have suppressed the lower bound’s dependence on label predictions δp because
they are fully determined, given (dp, x, y). Furthermore, as we state next in Theorem 1,
b(dp, x, y) gives a lower bound on the objective of any rule list whose preﬁx starts with dp.

Theorem 1 (Hierarchical objective lower bound) Deﬁne b(dp, x, y) as in (5). Also,
deﬁne σ(dp) to be the set of all rule lists whose preﬁxes starts with dp, as in (1). Let d =
(dp, δp, q0, K) be a rule list with preﬁx dp, and let d(cid:48) = (d(cid:48)
0, K(cid:48)) ∈ σ(dp) be any rule
list such that its preﬁx d(cid:48)

p starts with dp and K(cid:48) ≥ K, then b(dp, x, y) ≤ R(d(cid:48), x, y).

p, δ(cid:48)

p, q(cid:48)

Proof Let dp = (p1, . . . , pK) and δp = (q1, . . . , qK); let d(cid:48)
δ(cid:48)
p = (q1, . . . , qK, qK+1, . . . , qK(cid:48)). Notice that d(cid:48)
additional mistakes:

p = (p1, . . . , pK, pK+1, . . . , pK(cid:48)) and
p yields the same mistakes as dp, and possibly

(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) =

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn]

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=1

=

1
N

N
(cid:88)

(cid:32) K
(cid:88)

n=1

k=1

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn] +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn]

(cid:33)

K(cid:48)
(cid:88)

k=K+1

= (cid:96)p(dp, δp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] ≥ (cid:96)p(dp, δp, x, y),

(6)

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K+1

where in the second equality we have used the fact that cap(xn, pk | d(cid:48)
for 1 ≤ k ≤ K. It follows that

p) = cap(xn, pk | dp)

b(dp, x, y) = (cid:96)p(dp, δp, x, y) + λK

≤ (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + λK(cid:48) = b(d(cid:48)

p, x, y) ≤ R(d(cid:48), x, y).

(7)

9

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Algorithm 1 Branch-and-bound for learning rule lists.

m=1, training data (x, y) = {(xn, yn)}N

Input: Objective function R(d, x, y), objective lower bound b(dp, x, y), set of antecedents
n=1, initial best known rule list d0 with
S = {sm}M
objective R0 = R(d0, x, y); d0 could be obtained as output from another (approximate)
algorithm, otherwise, (d0, R0) = (null, 1) provide reasonable default values
Output: Provably optimal rule list d∗ with minimum objective R∗

(dc, Rc) ← (d0, R0)
Q ← queue( [ ( ) ] )
while Q not empty do
dp ← Q.pop( )
d ← (dp, δp, q0, K)
if b(dp, x, y) < Rc then
R ← R(d, x, y)
if R < Rc then

(dc, Rc) ← (d, R)

end if
for s in S do

if s not in dp then
Q.push( (dp, s) )

end if

end for

end if
end while
(d∗, R∗) ← (dc, Rc)

(cid:46) Initialize best rule list and objective
(cid:46) Initialize queue with empty preﬁx
(cid:46) Stop when queue is empty
(cid:46) Remove preﬁx dp from the queue
(cid:46) Set label predictions δp and q0 to minimize training error
(cid:46) Bound: Apply Theorem 1
(cid:46) Compute objective of dp’s rule list d
(cid:46) Update best rule list and objective

(cid:46) Branch: Enqueue dp’s children

(cid:46) Identify provably optimal solution

To generalize, consider a sequence of preﬁxes such that each preﬁx starts with all previ-
ous preﬁxes in the sequence. It follows that the corresponding sequence of objective lower
bounds increases monotonically. This is precisely the structure required and exploited by
branch-and-bound, illustrated in Algorithm 1.

Speciﬁcally, the objective lower bound in Theorem 1 enables us to prune the state
space hierarchically. While executing branch-and-bound, we keep track of the current best
(smallest) objective Rc, thus it is a dynamic, monotonically decreasing quantity. If we
encounter a preﬁx dp with lower bound b(dp, x, y) ≥ Rc, then by Theorem 1, we do not need
to consider any rule list d(cid:48) ∈ σ(dp) whose preﬁx d(cid:48)
p starts with dp. For the objective of such
a rule list, the current best objective provides a lower bound, i.e., R(d(cid:48), x, y) ≥ b(d(cid:48)
p, x, y) ≥
b(dp, x, y) ≥ Rc, and thus d(cid:48) cannot be optimal.

Next, we state an immediate consequence of Theorem 1.

Lemma 2 (Objective lower bound with one-step lookahead) Let dp be a K-preﬁx
and let Rc be the current best objective. If b(dp, x, y) + λ ≥ Rc, then for any K(cid:48)-rule list
d(cid:48) ∈ σ(dp) whose preﬁx d(cid:48)

p starts with dp and K(cid:48) > K, it follows that R(d(cid:48), x, y) ≥ Rc.

10

Learning Certifiably Optimal Rule Lists

Proof By the deﬁnition of the lower bound (5), which includes the penalty for longer
preﬁxes,

R(d(cid:48)

p, x, y) ≥ b(d(cid:48)

p, x, y) = (cid:96)p(d(cid:48)
= (cid:96)p(d(cid:48)
= b(dp, x, y) + λ(K(cid:48) − K) ≥ b(dp, x, y) + λ ≥ Rc.

p, x, y) + λK(cid:48)
p, x, y) + λK + λ(K(cid:48) − K)

p, δ(cid:48)
p, δ(cid:48)

(8)

Therefore, even if we encounter a preﬁx dp with lower bound b(dp, x, y) ≤ Rc, as long
p that start with and are longer

as b(dp, x, y) + λ ≥ Rc, then we can prune all preﬁxes d(cid:48)
than dp.

3.5 Upper Bounds on Preﬁx Length

In this section, we derive several upper bounds on preﬁx length:

• The simplest upper bound on preﬁx length is given by the total number of available

antecedents. (Proposition 3)

• The current best objective Rc implies an upper bound on preﬁx length. (Theorem 4)

• For intuition, we state a version of the above bound that is valid at the start of

execution. (Corollary 5)

length. (Theorem 6)

• By considering speciﬁc families of preﬁxes, we can obtain tighter bounds on preﬁx

In the next section (§3.6), we use these results to derive corresponding upper bounds on the
number of preﬁx evaluations made by Algorithm 1.

Proposition 3 (Trivial upper bound on preﬁx length) Consider a state space of all
rule lists formed from a set of M antecedents, and let L(d) be the length of rule list d.
M provides an upper bound on the length of any optimal rule list d∗ ∈ argmind R(d, x, y),
i.e., L(d) ≤ M .

Proof Rule lists consist of distinct rules by deﬁnition.

At any point during branch-and-bound execution, the current best objective Rc implies

an upper bound on the maximum preﬁx length we might still have to consider.

Theorem 4 (Upper bound on preﬁx length) Consider a state space of all rule lists
formed from a set of M antecedents. Let L(d) be the length of rule list d and let Rc be the
current best objective. For all optimal rule lists d∗ ∈ argmind R(d, x, y)

L(d∗) ≤ min

(cid:23)

(cid:18)(cid:22) Rc
λ

(cid:19)

, M

,

11

(9)

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

where λ is the regularization parameter. Furthermore, if dc is a rule list with objective
R(dc, x, y) = Rc, length K, and zero misclassiﬁcation error, then for every optimal rule
list d∗ ∈ argmind R(d, x, y), if dc ∈ argmind R(d, x, y), then L(d∗) ≤ K, or otherwise if
dc /∈ argmind R(d, x, y), then L(d∗) ≤ K − 1.

Proof For an optimal rule list d∗ with objective R∗,

λL(d∗) ≤ R∗ = R(d∗, x, y) = (cid:96)(d∗, x, y) + λL(d∗) ≤ Rc.

The maximum possible length for d∗ occurs when (cid:96)(d∗, x, y) is minimized; combining with
Proposition 3 gives bound (9).

For the rest of the proof, let K∗ = L(d∗) be the length of d∗. If the current best rule

list dc has zero misclassiﬁcation error, then

λK∗ ≤ (cid:96)(d∗, x, y) + λK∗ = R(d∗, x, y) ≤ Rc = R(dc, x, y) = λK,

and thus K∗ ≤ K. If the current best rule list is suboptimal, i.e., dc /∈ argmind R(d, x, y),
then

λK∗ ≤ (cid:96)(d∗, x, y) + λK∗ = R(d∗, x, y) < Rc = R(dc, x, y) = λK,

in which case K∗ < K, i.e., K∗ ≤ K − 1, since K is an integer.

The latter part of Theorem 4 tells us that if we only need to identify a single instance
of an optimal rule list d∗ ∈ argmind R(d, x, y), and we encounter a perfect K-rule list with
zero misclassiﬁcation error, then we can prune all preﬁxes of length K or greater.

Corollary 5 (Simple upper bound on preﬁx length) Let L(d) be the length of rule
list d. For all optimal rule lists d∗ ∈ argmind R(d, x, y),

L(d∗) ≤ min

(cid:23)

(cid:18)(cid:22) 1
2λ

(cid:19)

, M

.

(10)

Proof Let d = ((), (), q0, 0) be the empty rule list; it has objective R(d, x, y) = (cid:96)(d, x, y) ≤
1/2, which gives an upper bound on Rc. Combining with (9) and Proposition 3 gives (10).

For any particular preﬁx dp, we can obtain potentially tighter upper bounds on preﬁx

length for the family of all preﬁxes that start with dp.

Theorem 6 (Preﬁx-speciﬁc upper bound on preﬁx length) Let d = (dp, δp, q0, K) be
a rule list, let d(cid:48) = (d(cid:48)
p starts with dp, and
let Rc be the current best objective. If d(cid:48)

0, K(cid:48)) ∈ σ(dp) be any rule list such that d(cid:48)
p has lower bound b(d(cid:48)

p, x, y) < Rc, then

p, q(cid:48)

p, δ(cid:48)

(cid:18)

K(cid:48) < min

K +

(cid:22) Rc − b(dp, x, y)
λ

(cid:23)

(cid:19)

, M

.

(11)

12

Learning Certifiably Optimal Rule Lists

Proof First, note that K(cid:48) ≥ K, since d(cid:48)

p starts with dp. Now recall from (7) that

b(dp, x, y) = (cid:96)p(dp, δp, x, y) + λK ≤ (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + λK(cid:48) = b(d(cid:48)

p, x, y),

and from (6) that (cid:96)p(dp, δp, x, y) ≤ (cid:96)p(d(cid:48)
gives

p, δ(cid:48)

p, x, y). Combining these bounds and rearranging

b(d(cid:48)

p, x, y) = (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + λK + λ(K(cid:48) − K)

≥ (cid:96)p(dp, δp, x, y) + λK + λ(K(cid:48) − K) = b(dp, x, y) + λ(K(cid:48) − K).

(12)

Combining (12) with b(d(cid:48)

p, x, y) < Rc and Proposition 3 gives (11).

We can view Theorem 6 as a generalization of our one-step lookahead bound (Lemma 2),
as (11) is equivalently a bound on K(cid:48) − K, an upper bound on the number of remain-
ing ‘steps’ corresponding to an iterative sequence of single-rule extensions of a preﬁx dp.
Notice that when d = ((), (), q0, 0) is the empty rule list, this bound replicates (9), since
b(dp, x, y) = 0.

3.6 Upper Bounds on the Number of Preﬁx Evaluations

In this section, we use our upper bounds on preﬁx length from §3.5 to derive corresponding
upper bounds on the number of preﬁx evaluations made by Algorithm 1. First, we present
Theorem 7, in which we use information about the state of Algorithm 1’s execution to
calculate, for any given execution state, upper bounds on the number of additional preﬁx
evaluations that might be required for the execution to complete. The relevant execution
state depends on the current best objective Rc and information about preﬁxes we are
planning to evaluate, i.e., preﬁxes in the queue Q of Algorithm 1. We deﬁne the number
of remaining preﬁx evaluations as the number of preﬁxes that are currently in or will be
inserted into the queue.

We use Theorem 7 in some of our empirical results (§6, Figure 18) to help illustrate
the dramatic impact of certain algorithm optimizations. The execution trace of this upper
bound on remaining preﬁx evaluations complements the execution traces of other quanti-
ties, e.g., that of the current best objective Rc. After presenting Theorem 7, we also give
two weaker propositions that provide useful intuition. In particular, Proposition 9 is a prac-
tical approximation to Theorem 7 that is signiﬁcantly easier to compute; we use it in our
implementation as a metric of execution progress that we display to the user.

Theorem 7 (Fine-grained upper bound on remaining preﬁx evaluations) Con-
sider the state space of all rule lists formed from a set of M antecedents, and consider Algo-
rithm 1 at a particular instant during execution. Let Rc be the current best objective, let Q
be the queue, and let L(dp) be the length of preﬁx dp. Deﬁne Γ(Rc, Q) to be the number of
remaining preﬁx evaluations, then

Γ(Rc, Q) ≤

(cid:88)

f (dp)
(cid:88)

dp∈Q

k=0

(M − L(dp))!
(M − L(dp) − k)!

,

(13)

13

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

where

f (dp) = min

(cid:18)(cid:22) Rc − b(dp, x, y)

(cid:23)

(cid:19)

, M − L(dp)

.

λ

Proof The number of remaining preﬁx evaluations is equal to the number of preﬁxes that
are currently in or will be inserted into queue Q. For any such preﬁx dp, Theorem 6 gives
an upper bound on the length of any preﬁx d(cid:48)

p that starts with dp:

(cid:18)

L(d(cid:48)

p) ≤ min

L(dp) +

(cid:22) Rc − b(dp, x, y)
λ

(cid:23)

(cid:19)

, M

≡ U (dp).

This gives an upper bound on the number of remaining preﬁx evaluations:

Γ(Rc, Q) ≤

P (M − L(dp), k) =

(cid:88)

U (dp)−L(dp)
(cid:88)

dp∈Q

k=0

(cid:88)

f (dp)
(cid:88)

dp∈Q

k=0

(M − L(dp))!
(M − L(dp) − k)!

,

where P (m, k) denotes the number of k-permutations of m.

Proposition 8 is strictly weaker than Theorem 7 and is the starting point for its deriva-
tion. It is a na¨ıve upper bound on the total number of preﬁx evaluations over the course
of Algorithm 1’s execution. It only depends on the number of rules and the regularization
parameter λ; i.e., unlike Theorem 7, it does not use algorithm execution state to bound the
size of the search space.

Proposition 8 (Upper bound on the total number of preﬁx evaluations) Deﬁne
Γtot(S) to be the total number of preﬁxes evaluated by Algorithm 1, given the state space of
all rule lists formed from a set S of M rules. For any set S of M rules,

Γtot(S) ≤

K
(cid:88)

k=0

M !
(M − k)!

,

where K = min((cid:98)1/2λ(cid:99), M ).

Proof By Corollary 5, K ≡ min((cid:98)1/2λ(cid:99), M ) gives an upper bound on the length of any
optimal rule list. Since we can think of our problem as ﬁnding the optimal selection and
permutation of k out of M rules, over all k ≤ K,

Γtot(S) ≤ 1 +

P (M, k) =

K
(cid:88)

k=1

K
(cid:88)

k=0

M !
(M − k)!

.

Our next upper bound is strictly tighter than the bound in Proposition 8. Like Theo-
rem 7, it uses the current best objective and information about the lengths of preﬁxes in the

14

Learning Certifiably Optimal Rule Lists

queue to constrain the lengths of preﬁxes in the remaining search space. However, Proposi-
tion 9 is weaker than Theorem 7 because it leverages only coarse-grained information from
the queue. Speciﬁcally, Theorem 7 is strictly tighter because it additionally incorporates
preﬁx-speciﬁc objective lower bound information from preﬁxes in the queue, which further
constrains the lengths of preﬁxes in the remaining search space.

Proposition 9 (Coarse-grained upper bound on remaining preﬁx evaluations)
Consider a state space of all rule lists formed from a set of M antecedents, and consider
Algorithm 1 at a particular instant during execution. Let Rc be the current best objective,
let Q be the queue, and let L(dp) be the length of preﬁx dp. Let Qj be the number of preﬁxes
of length j in Q,

Qj = (cid:12)

(cid:12){dp : L(dp) = j, dp ∈ Q}(cid:12)
(cid:12)

and let J = argmaxdp∈Q L(dp) be the length of the longest preﬁx in Q. Deﬁne Γ(Rc, Q) to
be the number of remaining preﬁx evaluations, then

Γ(Rc, Q) ≤

J
(cid:88)

j=1

Qj

(cid:32)K−j
(cid:88)

k=0

(M − j)!
(M − j − k)!

(cid:33)

,

where K = min((cid:98)Rc/λ(cid:99), M ).

Proof The number of remaining preﬁx evaluations is equal to the number of preﬁxes that
are currently in or will be inserted into queue Q. For any such remaining preﬁx dp, Theorem 4
gives an upper bound on its length; deﬁne K to be this bound: L(dp) ≤ min((cid:98)Rc/λ(cid:99), M ) ≡ K.
For any preﬁx dp in queue Q with length L(dp) = j, the maximum number of preﬁxes that
start with dp and remain to be evaluated is:

K−j
(cid:88)

k=0

P (M − j, k) =

K−j
(cid:88)

k=0

(M − j)!
(M − j − k)!

,

where P (T, k) denotes the number of k-permutations of T . This gives an upper bound on
the number of remaining preﬁx evaluations:

Γ(Rc, Q) ≤

J
(cid:88)

j=0

Qj

(cid:32)K−j
(cid:88)

k=0

P (M − j, k)

=

Qj

(cid:33)

J
(cid:88)

j=0

(cid:32)K−j
(cid:88)

k=0

(M − j)!
(M − j − k)!

(cid:33)

.

3.7 Lower Bounds on Antecedent Support

In this section, we give two lower bounds on the normalized support of each antecedent in
any optimal rule list; both are related to the regularization parameter λ.

15

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Theorem 10 (Lower bound on antecedent support) Let d∗ = (dp, δp, q0, K) be any
optimal rule list with objective R∗, i.e., d∗ ∈ argmind R(d, x, y). For each antecedent pk
in preﬁx dp = (p1, . . . , pK), the regularization parameter λ provides a lower bound on the
normalized support of pk,

λ ≤ supp(pk, x | dp).

(14)

Proof Let d∗ = (dp, δp, q0, K) be an optimal rule list with preﬁx dp = (p1, . . . , pK) and
0, K − 1) derived from d∗ by
labels δp = (q1, . . . , qK). Consider the rule list d = (d(cid:48)
deleting a rule pi → qi, therefore d(cid:48)
p = (q1, . . . , qi−1,
i+1, . . . , q(cid:48)
q(cid:48)

k need not be the same as qk, for k > i and k = 0.
The largest possible discrepancy between d∗ and d would occur if d∗ correctly classiﬁed

p, δ(cid:48)
p = (p1, . . . , pi−1, pi+1, . . . , pK) and δ(cid:48)

K), where q(cid:48)

p, q(cid:48)

all the data captured by pi, while d misclassiﬁed these data. This gives an upper bound:
R(d, x, y) = (cid:96)(d, x, y) + λ(K − 1) ≤ (cid:96)(d∗, x, y) + supp(pi, x | dp) + λ(K − 1)

= R(d∗, x, y) + supp(pi, x | dp) − λ
= R∗ + supp(pi, x | dp) − λ

(15)

where supp(pi, x | dp) is the normalized support of pi in the context of dp, deﬁned in (3),
and the regularization ‘bonus’ comes from the fact that d is one rule shorter than d∗.

At the same time, we must have R∗ ≤ R(d, x, y) for d∗ to be optimal. Combining this
with (15) and rearranging gives (14), therefore the regularization parameter λ provides a
lower bound on the support of an antecedent pi in an optimal rule list d∗.

Thus, we can prune a preﬁx dp if any of its antecedents captures less than a fraction λ
of data, even if b(dp, x, y) < R∗. Notice that the bound in Theorem 10 depends on the
antecedents, but not the label predictions, and thus does not account for misclassiﬁcation
error. Theorem 11 gives a tighter bound by leveraging this additional information, which
speciﬁcally tightens the upper bound on R(d, x, y) in (15).

Theorem 11 (Lower bound on accurate antecedent support) Let d∗ be any opti-
mal rule list with objective R∗, i.e., d∗ = (dp, δp, q0, K) ∈ argmind R(d, x, y). Let d∗ have
preﬁx dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK). For each rule pk → qk in d∗, deﬁne ak
to be the fraction of data that are captured by pk and correctly classiﬁed:

ak ≡

cap(xn, pk | dp) ∧ 1[qk = yn].

1
N

N
(cid:88)

n=1

The regularization parameter λ provides a lower bound on ak:

(16)

(17)

0, K − 1) be the rule list derived from d∗ by
Proof As in Theorem 10, let d = (d(cid:48)
deleting a rule pi → qi. Now, let us deﬁne (cid:96)i to be the portion of R∗ due to this rule’s
misclassiﬁcation error,

p, δ(cid:48)

p, q(cid:48)

(cid:96)i ≡

cap(xn, pi | dp) ∧ 1[qi (cid:54)= yn].

1
N

N
(cid:88)

n=1

λ ≤ ak.

16

Learning Certifiably Optimal Rule Lists

The largest discrepancy between d∗ and d would occur if d misclassiﬁed all the data captured
by pi. This gives an upper bound on the diﬀerence between the misclassiﬁcation error of d
and d∗:

(cid:96)(d, x, y) − (cid:96)(d∗, x, y) ≤ supp(pi, x | dp) − (cid:96)i

=

=

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

cap(xn, pi | dp) −

cap(xn, pi | dp) ∧ 1[qi (cid:54)= yn]

1
N

N
(cid:88)

n=1

cap(xn, pi | dp) ∧ 1[qi = yn] = ai,

where we deﬁned ai in (16). Relating this bound to the objectives of d and d∗ gives

R(d, x, y) = (cid:96)(d, x, y) + λ(K − 1) ≤ (cid:96)(d∗, x, y) + ai + λ(K − 1)

= R(d∗, x, y) + ai − λ
= R∗ + ai − λ.

(18)

Combining (18) with the requirement R∗ ≤ R(d, x, y) gives the bound λ ≤ ai.

Thus, we can prune a preﬁx if any of its rules correctly classiﬁes less than a fraction λ
of data. While the lower bound in Theorem 10 is a sub-condition of the lower bound in
Theorem 11, we can still leverage both—since the sub-condition is easier to check, check-
ing it ﬁrst can accelerate pruning. In addition to applying Theorem 10 in the context of
constructing rule lists, we can furthermore apply it in the context of rule mining (§3.1).
Speciﬁcally, it implies that we should only mine rules with normalized support of at least λ;
we need not mine rules with a smaller fraction of observations.2 In contrast, we can only
apply Theorem 11 in the context of constructing rule lists; it depends on the misclassiﬁ-
cation error associated with each rule in a rule list, thus it provides a lower bound on the
number of observations that each such rule must correctly classify.

3.8 Upper Bound on Antecedent Support

In the previous section (§3.7), we proved lower bounds on antecedent support; in Ap-
pendix A, we give an upper bound on antecedent support. Speciﬁcally, Theorem 21 shows
that an antecedent’s support in a rule list cannot be too similar to the set of data not
captured by preceding antecedents in the rule list. In particular, Theorem 21 implies that
we should only mine rules with normalized support less than or equal to 1 − λ; we need
not mine rules with a larger fraction of observations. Note that we do not otherwise use
this bound in our implementation, because we did not observe a meaningful beneﬁt in
preliminary experiments.

3.9 Antecedent Rejection and its Propagation

In this section, we demonstrate further consequences of our lower (§3.7) and upper bounds
(§3.8) on antecedent support, under a uniﬁed framework we refer to as antecedent rejec-
tion. Let dp = (p1, . . . , pK) be a preﬁx, and let pk be an antecedent in dp. Deﬁne pk to have

2. We describe our application of this idea in Appendix E, where we provide details on data processing.

17

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

insuﬃcient support in dp if it does not obey the bound in (14) of Theorem 10. Deﬁne pk
to have insuﬃcient accurate support in dp if it does not obey the bound in (17) of Theo-
rem 11. Deﬁne pk to have excessive support in dp if it does not obey the bound in (37) of
Theorem 21 (Appendix A). If pk in the context of dp has insuﬃcient support, insuﬃcient
accurate support, or excessive support, let us say that preﬁx dp rejects antecedent pK. Next,
in Theorem 12, we describe large classes of related rule lists whose preﬁxes all reject the
same antecedent.

Theorem 12 (Antecedent rejection propagates) For any preﬁx dp = (p1, . . . , pK), let
φ(dp) denote the set of all preﬁxes d(cid:48)
p such that the set of all antecedents in dp is a subset
of the set of all antecedents in d(cid:48)
p, i.e.,

φ(dp) = {d(cid:48)

p = (p(cid:48)

1, . . . , p(cid:48)

K(cid:48)) s.t. {pk : pk ∈ dp} ⊆ {p(cid:48)

κ : p(cid:48)

κ ∈ d(cid:48)

p}, K(cid:48) ≥ K}.

(19)

Let d = (dp, δp, q0, K) be a rule list with preﬁx dp = (p1, . . . , pK−1, pK), such that dp rejects
its last antecedent pK, either because pK in the context of dp has insuﬃcient support, insuf-
ﬁcient accurate support, or excessive support. Let dK−1
= (p1, . . . , pK−1) be the ﬁrst K − 1
antecedents of dp. Let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp = (P1, . . . , PK(cid:48)−1,
PK(cid:48), . . . , Pκ) such that Dp starts with DK(cid:48)−1
) and antecedent
PK(cid:48) = pK. It follows that preﬁx Dp rejects PK(cid:48) for the same reason that dp rejects pK, and
furthermore, D cannot be optimal, i.e., D /∈ argmind† R(d†, x, y).

= (P1, . . . , PK(cid:48)−1) ∈ φ(dK−1

p

p

p

Proof Combine Proposition 13, Proposition 14, and Proposition 22. The ﬁrst two are
found below, and the last in Appendix A.

Theorem 12 implies potentially signiﬁcant computational savings. We know from Theo-
rems 10, 11, and 21 that during branch-and-bound execution, if we ever encounter a preﬁx
dp = (p1, . . . , pK−1, pK) that rejects its last antecedent pK, then we can prune dp. By The-
orem 12, we can also prune any preﬁx d(cid:48)
p whose antecedents contains the set of antecedents
in dp, in almost any order, with the constraint that all antecedents in {p1, . . . , pK−1} pre-
cede pK. These latter antecedents are also rejected directly by the bounds in Theorems 10,
11, and 21; this is how our implementation works in practice. In a preliminary implemen-
tation (not shown), we maintained additional data structures to support the direct use of
Theorem 12. We leave the design of eﬃcient data structures for this task as future work.

Proposition 13 (Insuﬃcient antecedent support propagates) First deﬁne φ(dp) as
in (19), and let dp = (p1, . . . , pK−1, pK) be a preﬁx, such that its last antecedent pK has
insuﬃcient support, i.e., the opposite of the bound in (14): supp(pK, x | dp) < λ. Let dK−1
=
(p1, . . . , pK−1), and let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp = (P1, . . . , PK(cid:48)−1,
PK(cid:48), . . . , Pκ), such that Dp starts with DK(cid:48)−1
) and PK(cid:48) = pK.
It follows that PK(cid:48) has insuﬃcient support in preﬁx Dp, and furthermore, D cannot be
optimal, i.e., D /∈ argmind R(d, x, y).

= (P1, . . . , PK(cid:48)−1) ∈ φ(dK−1

p

p

p

18

Learning Certifiably Optimal Rule Lists

Proof The support of pK in dp depends only on the set of antecedents in dK

p = (p1, . . . , pK):

supp(pK, x | dp) =

cap(xn, pK | dp) =

(cid:0)¬ cap(xn, dK−1

)(cid:1) ∧ cap(xn, pK)

p

N
(cid:88)

n=1
N
(cid:88)

1
N

1
N

=

(cid:32)K−1
(cid:94)

n=1

k=1

(cid:33)

¬ cap(xn, pk)

∧ cap(xn, pK) < λ,

and the support of PK(cid:48) in Dp depends only on the set of antecedents in DK(cid:48)

p = (P1, . . . , PK(cid:48)):

supp(PK(cid:48), x | Dp) =

cap(xn, PK(cid:48) | Dp) =

¬ cap(xn, Pk)

∧ cap(xn, PK(cid:48))

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

1
N

1
N

1
N

N
(cid:88)

(cid:32)K(cid:48)−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

n=1

k=1

≤

=

(cid:33)

(cid:33)

(cid:33)

¬ cap(xn, pk)

∧ cap(xn, PK(cid:48))

¬ cap(xn, pk)

∧ cap(xn, pK)

= supp(pK, x | dp) < λ.

(20)

The ﬁrst inequality reﬂects the condition that DK(cid:48)−1
set of antecedents in DK(cid:48)−1
reﬂects the fact that PK(cid:48) = pK. Thus, P (cid:48)
by Theorem 10, D cannot be optimal, i.e., D /∈ argmind R(d, x, y).

), which implies that the
, and the next equality
K has insuﬃcient support in preﬁx Dp, therefore

contains the set of antecedents in dK−1

∈ φ(dK−1
p

p

p

p

Proposition 14 (Insuﬃcient accurate antecedent support propagates) Let φ(dp)
denote the set of all preﬁxes d(cid:48)
p such that the set of all antecedents in dp is a subset of
the set of all antecedents in d(cid:48)
p, as in (19). Let d = (dp, δp, q0, K) be a rule list with preﬁx
dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK), such that the last antecedent pK has insuﬃ-
cient accurate support, i.e., the opposite of the bound in (17):

1
N

N
(cid:88)

n=1

cap(xn, pK | dp) ∧ 1[qK = yn] < λ.

Let dK−1
p
(P1, . . . , Pκ) and labels ∆p = (Q1, . . . , Qκ), such that Dp starts with DK(cid:48)−1
∈ φ(dK−1
p
and furthermore, D /∈ argmind† R(d†, x, y).

= (p1, . . . , pK−1) and let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp =
= (P1, . . . , PK(cid:48)−1)
) and PK(cid:48) = pK. It follows that PK(cid:48) has insuﬃcient accurate support in preﬁx Dp,

p

19

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Proof The accurate support of PK(cid:48) in Dp is insuﬃcient:

1
N

N
(cid:88)

n=1

cap(xn, PK(cid:48) | Dp) ∧ 1[QK(cid:48) = yn]

=

≤

=

=

≤

1
N

1
N

1
N

1
N

1
N

N
(cid:88)

(cid:32)K(cid:48)−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

n=1

N
(cid:88)

k=1
(cid:32)K−1
(cid:94)

k=1

n=1

N
(cid:88)

n=1
N
(cid:88)

n=1

(cid:33)

(cid:33)

(cid:33)

¬ cap(xn, Pk)

∧ cap(xn, PK(cid:48)) ∧ 1[QK(cid:48) = yn]

¬ cap(xn, pk)

∧ cap(xn, PK(cid:48)) ∧ 1[QK(cid:48) = yn]

¬ cap(xn, pk)

∧ cap(xn, pK) ∧ 1[QK(cid:48) = yn]

cap(xn, pK | dp) ∧ 1[QK(cid:48) = yn]

cap(xn, pK | dp) ∧ 1[qK = yn] < λ.

The ﬁrst inequality reﬂects the condition that DK(cid:48)−1
), the next equality reﬂects
the fact that PK(cid:48) = pK. For the following equality, notice that QK(cid:48) is the majority class
label of data captured by PK(cid:48) in Dp, and qK is the majority class label of data captured
by PK in dp, and recall from (20) that supp(PK(cid:48), x | Dp) ≤ supp(pK, x | dp). By Theorem 11,
D /∈ argmind† R(d†, x, y).

∈ φ(dK−1
p

p

Propositions 13 and 14, combined with Proposition 22 (Appendix A), constitute the

proof of Theorem 12.

3.10 Equivalent Support Bound

If two preﬁxes capture the same data, and one is more accurate than the other, then there
is no beneﬁt to considering preﬁxes that start with the less accurate one. Let dp be a
preﬁx, and consider the best possible rule list whose preﬁx starts with dp. If we take its
antecedents in dp and replace them with another preﬁx with the same support (that could
include diﬀerent antecedents), then its objective can only become worse or remain the same.
Formally, let Dp be a preﬁx, and let ξ(Dp) be the set of all preﬁxes that capture exactly
the same data as Dp. Now, let d be a rule list with preﬁx dp in ξ(Dp), such that d has
the minimum objective over all rule lists with preﬁxes in ξ(Dp). Finally, let d(cid:48) be a rule
list whose preﬁx d(cid:48)
p starts with dp, such that d(cid:48) has the minimum objective over all rule
lists whose preﬁxes start with dp. Theorem 15 below implies that d(cid:48) also has the minimum
objective over all rule lists whose preﬁxes start with any preﬁx in ξ(Dp).

Theorem 15 (Equivalent support bound) Deﬁne σ(dp) to be the set of all rule lists
whose preﬁxes start with dp, as in (1). Let d = (dp, δp, q0, K) be a rule list with preﬁx
dp = (p1, . . . , pK), and let D = (Dp, ∆p, Q0, κ) be a rule list with preﬁx Dp = (P1, . . . , Pκ),

20

Learning Certifiably Optimal Rule Lists

such that dp and Dp capture the same data, i.e.,

{xn : cap(xn, dp)} = {xn : cap(xn, Dp)}.

If the objective lower bounds of d and D obey b(dp, x, y) ≤ b(Dp, x, y), then the objective of
the optimal rule list in σ(dp) gives a lower bound on the objective of the optimal rule list
in σ(Dp):

min
d(cid:48)∈σ(dp)

R(d(cid:48), x, y) ≤ min

R(D(cid:48), x, y).

D(cid:48)∈σ(Dp)

(21)

Proof See Appendix B for the proof of Theorem 15.

Thus, if preﬁxes dp and Dp capture the same data, and their objective lower bounds obey
b(dp, x, y) ≤ b(Dp, x, y), Theorem 15 implies that we can prune Dp. Next, in Sections 3.11
and 3.12, we highlight and analyze the special case of preﬁxes that capture the same data
because they contain the same antecedents.

3.11 Permutation Bound

If two preﬁxes are composed of the same antecedents, i.e., they contain the same antecedents
up to a permutation, then they capture the same data, and thus Theorem 15 applies.
Therefore, if one is more accurate than the other, then there is no beneﬁt to considering
preﬁxes that start with the less accurate one. Let dp be a preﬁx, and consider the best
possible rule list whose preﬁx starts with dp. If we permute its antecedents in dp, then its
objective can only become worse or remain the same.

Formally, let P = {pk}K

k=1 be a set of K antecedents, and let Π be the set of all K-preﬁxes
corresponding to permutations of antecedents in P . Let preﬁx dp in Π have the minimum
preﬁx misclassiﬁcation error over all preﬁxes in Π. Also, let d(cid:48) be a rule list whose preﬁx d(cid:48)
p
starts with dp, such that d(cid:48) has the minimum objective over all rule lists whose preﬁxes start
with dp. Corollary 16 below, which can be viewed as special case of Theorem 15, implies
that d(cid:48) also has the minimum objective over all rule lists whose preﬁxes start with any
preﬁx in Π.

p, q(cid:48)

p, δ(cid:48)

0, K(cid:48)) : d(cid:48)

Corollary 16 (Permutation bound) Let π be any permutation of {1, . . . , K}, and de-
ﬁne σ(dp) = {(d(cid:48)
p starts with dp} to be the set of all rule lists whose preﬁxes
start with dp. Let d = (dp, δp, q0, K) and D = (Dp, ∆p, Q0, K) denote rule lists with preﬁxes
dp = (p1, . . . , pK) and Dp = (pπ(1), . . . , pπ(K)), respectively, i.e., the antecedents in Dp cor-
respond to a permutation of the antecedents in dp. If the objective lower bounds of d and D
obey b(dp, x, y) ≤ b(Dp, x, y), then the objective of the optimal rule list in σ(dp) gives a
lower bound on the objective of the optimal rule list in σ(Dp):

min
d(cid:48)∈σ(dp)

R(d(cid:48), x, y) ≤ min

R(D(cid:48), x, y).

D(cid:48)∈σ(Dp)

Proof Since preﬁxes dp and Dp contain the same antecedents, they both capture the same
data. Thus, we can apply Theorem 15.

21

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Thus if preﬁxes dp and Dp have the same antecedents, up to a permutation, and their
objective lower bounds obey b(dp, x, y) ≤ b(Dp, x, y), Corollary 16 implies that we can
prune Dp. We call this symmetry-aware pruning, and we illustrate the subsequent compu-
tational savings next in §3.12.

3.12 Upper Bound on Preﬁx Evaluations with Symmetry-aware Pruning

Here, we present an upper bound on the total number of preﬁx evaluations that accounts for
the eﬀect of symmetry-aware pruning (§3.11). Since every subset of K antecedents generates
an equivalence class of K! preﬁxes equivalent up to permutation, symmetry-aware pruning
dramatically reduces the search space.

First, notice that Algorithm 1 describes a breadth-ﬁrst exploration of the state space of
rule lists. Now suppose we integrate symmetry-aware pruning into our execution of branch-
and-bound, so that after evaluating preﬁxes of length K, we only keep a single best preﬁx
from each set of preﬁxes equivalent up to a permutation.

Theorem 17 (Upper bound on preﬁx evaluations with symmetry-aware pruning)
Consider a state space of all rule lists formed from a set S of M antecedents, and consider
the branch-and-bound algorithm with symmetry-aware pruning. Deﬁne Γtot(S) to be the total
number of preﬁxes evaluated. For any set S of M rules,

Γtot(S) ≤ 1 +

K
(cid:88)

k=1

1
(k − 1)!

·

M !
(M − k)!

,

where K = min((cid:98)1/2λ(cid:99), M ).

Proof By Corollary 5, K ≡ min((cid:98)1/2λ(cid:99), M ) gives an upper bound on the length of any op-
timal rule list. The algorithm begins by evaluating the empty preﬁx, followed by M preﬁxes
of length k = 1, then P (M, 2) preﬁxes of length k = 2, where P (M, 2) is the number of size-2
subsets of {1, . . . , M }. Before proceeding to length k = 3, we keep only C(M, 2) preﬁxes of
length k = 2, where C(M, k) denotes the number of k-combinations of M . Now, the number
of length k = 3 preﬁxes we evaluate is C(M, 2)(M − 2). Propagating this forward gives

Γtot(S) ≤ 1 +

C(M, k − 1)(M − k + 1) = 1 +

K
(cid:88)

k=1

K
(cid:88)

k=1

1
(k − 1)!

·

M !
(M − k)!

.

Pruning based on permutation symmetries thus yields signiﬁcant computational savings.
Let us compare, for example, to the na¨ıve number of preﬁx evaluations given by the upper
bound in Proposition 8. If M = 100 and K = 5, then the na¨ıve number is about 9.1 × 109,
while the reduced number due to symmetry-aware pruning is about 3.9 × 108, which is
smaller by a factor of about 23. If M = 1000 and K = 10, the number of evaluations falls
from about 9.6 × 1029 to about 2.7 × 1024, which is smaller by a factor of about 360,000.

While 1024 seems infeasibly enormous, it does not represent the number of rule lists we
evaluate. As we show in our experiments (§6), our permutation bound in Corollary 16 and

22

Learning Certifiably Optimal Rule Lists

our other bounds together conspire to reduce the search space to a size manageable on a
single computer. The choice of M = 1000 and K = 10 in our example above corresponds to
the state space size our eﬀorts target. K = 10 rules represents a (heuristic) upper limit on
the size of an interpretable rule list, and M = 1000 represents the approximate number of
rules with suﬃciently high support (Theorem 10) we expect to obtain via rule mining (§3.1).

3.13 Similar Support Bound

We now present a relaxation of Theorem 15, our equivalent support bound. Theorem 18
implies that if we know that no extensions of a preﬁx dp are better than the current best
objective, then we can prune all preﬁxes with support similar to dp’s support. Understanding
how to exploit this result in practice represents an exciting direction for future work; our
implementation (§5) does not currently leverage the bound in Theorem 18.

Theorem 18 (Similar support bound) Deﬁne σ(dp) to be the set of all rule lists whose
preﬁxes start with dp, as in (1). Let dp = (p1, . . . , pK) and Dp = (P1, . . . , Pκ) be preﬁxes
that capture nearly the same data. Speciﬁcally, deﬁne ω to be the normalized support of data
captured by dp and not captured by Dp, i.e.,

ω ≡

¬ cap(xn, Dp) ∧ cap(xn, dp).

Similarly, deﬁne Ω to be the normalized support of data captured by Dp and not captured
by dp, i.e.,

Ω ≡

¬ cap(xn, dp) ∧ cap(xn, Dp).

We can bound the diﬀerence between the objectives of the optimal rule lists in σ(dp) and
σ(Dp) as follows:

min
D†∈σ(Dp)

d†∈σ(dp)

R(D†, x, y) − min

R(d†, x, y) ≥ b(Dp, x, y) − b(dp, x, y) − ω − Ω,

(24)

where b(dp, x, y) and b(Dp, x, y) are the objective lower bounds of d and D, respectively.

Proof See Appendix C for the proof of Theorem 18.

Theorem 18 implies that if preﬁxes dp and Dp are similar, and we know the optimal

objective of rule lists starting with dp, then

min
D(cid:48)∈σ(Dp)

d(cid:48)∈σ(dp)

R(D(cid:48), x, y) ≥ min

R(d(cid:48), x, y) + b(Dp, x, y) − b(dp, x, y) − χ

≥ Rc + b(Dp, x, y) − b(dp, x, y) − χ,

where Rc is the current best objective, and χ is the normalized support of the set of data
captured either exclusively by dp or exclusively by Dp. It follows that

min
D(cid:48)∈σ(Dp)

R(D(cid:48), x, y) ≥ Rc + b(Dp, x, y) − b(dp, x, y) − χ ≥ Rc

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

(22)

(23)

23

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if b(Dp, x, y) − b(dp, x, y) ≥ χ. To conclude, we summarize this result and combine it with
our notion of lookahead from Lemma 2. During branch-and-bound execution, if we demon-
strate that mind(cid:48)∈σ(dp) R(d(cid:48), x, y) ≥ Rc, then we can prune all preﬁxes that start with any
preﬁx D(cid:48)

p in the following set:

(cid:40)

(cid:41)

D(cid:48)

p : b(D(cid:48)

p, x, y) + λ − b(dp, x, y) ≥

cap(xn, dp) ⊕ cap(xn, D(cid:48)
p)

,

1
N

N
(cid:88)

n=1

where the symbol ⊕ denotes the logical operation, exclusive or (XOR).

3.14 Equivalent Points Bound

The bounds in this section quantify the following: If multiple observations that are not
captured by a preﬁx dp have identical features and opposite labels, then no rule list that
starts with dp can correctly classify all these observations. For each set of such observations,
the number of mistakes is at least the number of observations with the minority label within
the set.

Consider a data set {(xn, yn)}N

m=1. Deﬁne dis-
tinct observations to be equivalent if they are captured by exactly the same antecedents,
i.e., xi (cid:54)= xj are equivalent if

n=1 and also a set of antecedents {sm}M

1
M

M
(cid:88)

m=1

1[cap(xi, sm) = cap(xj, sm)] = 1.

Notice that we can partition a data set into sets of equivalent points; let {eu}U
u=1 enumerate
these sets. Let eu be the equivalent points set that contains observation xi. Now deﬁne θ(eu)
to be the normalized support of the minority class label with respect to set eu, e.g., let

eu = {xn : ∀m ∈ [M ], 1[cap(xn, sm) = cap(xi, sm)]},

and let qu be the minority class label among points in eu, then

θ(eu) =

1[xn ∈ eu] 1[yn = qu].

(25)

1
N

N
(cid:88)

n=1

The existence of equivalent points sets with non-singleton support yields a tighter ob-
jective lower bound that we can combine with our other bounds; as our experiments demon-
strate (§6), the practical consequences can be dramatic. First, for intuition, we present a
general bound in Proposition 19; next, we explicitly integrate this bound into our framework
in Theorem 20.

Proposition 19 (General equivalent points bound) Let d = (dp, δp, q0, K) be a rule list,
then

R(d, x, y) ≥

θ(eu) + λK.

U
(cid:88)

u=1

24

Learning Certifiably Optimal Rule Lists

Proof Recall that the objective is R(d, x, y) = (cid:96)(d, x, y) + λK, where the misclassiﬁcation
error (cid:96)(d, x, y) is given by

(cid:96)(d, x, y) = (cid:96)0(dp, q0, x, y) + (cid:96)p(dp, δp, x, y)

=

1
N

(cid:32)

N
(cid:88)

n=1

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn] +

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn]

.

(cid:33)

K
(cid:88)

k=1

Any particular rule list uses a speciﬁc rule, and therefore a single class label, to classify
all points within a set of equivalent points. Thus, for a set of equivalent points u, the rule
list d correctly classiﬁes either points that have the majority class label, or points that have
the minority class label. It follows that d misclassiﬁes a number of points in u at least as
great as the number of points with the minority class label. To translate this into a lower
bound on (cid:96)(d, x, y), we ﬁrst sum over all sets of equivalent points, and then for each such
set, count diﬀerences between class labels and the minority class label of the set, instead of
counting mistakes:

(cid:96)(d, x, y)

=

≥

1
N

1
N

U
(cid:88)

N
(cid:88)

(cid:32)

u=1

n=1

U
(cid:88)

N
(cid:88)

(cid:32)

u=1

n=1

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn] +

cap(xn, pk | dp) ∧ 1[qk (cid:54)= yn]

1[xn ∈ eu]

¬ cap(xn, dp) ∧ 1[yn = qu] +

cap(xn, pk | dp) ∧ 1[yn = qu]

1[xn ∈ eu].

(cid:33)

(cid:33)

(26)

Next, we factor out the indicator for equivalent point set membership, which yields a term
that sums to one, because every datum is either captured or not captured by preﬁx dp.

(cid:96)(d, x, y) =

¬ cap(xn, dp) +

cap(xn, pk | dp)

∧ 1[xn ∈ eu] 1[yn = qu]

(cid:33)

U
(cid:88)

N
(cid:88)

(cid:32)

1
N

1
N

1
N

=

=

u=1

n=1

U
(cid:88)

N
(cid:88)

u=1
U
(cid:88)

n=1
N
(cid:88)

u=1

n=1

(¬ cap(xn, dp) + cap(xn, dp)) ∧ 1[xn ∈ eu] 1[yn = qu]

1[xn ∈ eu] 1[yn = qu] =

θ(eu),

U
(cid:88)

u=1

where the ﬁnal equality applies the deﬁnition of θ(eu) in (25). Therefore, R(d, x, y) =
(cid:96)(d, x, y) + λK ≥ (cid:80)U

u=1 θ(eu) + λK.

Now, recall that to obtain our lower bound b(dp, x, y) in (5), we simply deleted the
default rule misclassiﬁcation error (cid:96)0(dp, q0, x, y) from the objective R(d, x, y). Theorem 20
obtains a tighter objective lower bound via a tighter lower bound on the default rule mis-
classiﬁcation error, 0 ≤ b0(dp, x, y) ≤ (cid:96)0(dp, q0, x, y).

K
(cid:88)

k=1
K
(cid:88)

k=1

K
(cid:88)

k=1

25

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Theorem 20 (Equivalent points bound) Let d be a rule list with preﬁx dp and lower
bound b(dp, x, y), then for any rule list d(cid:48) ∈ σ(d) whose preﬁx d(cid:48)

p starts with dp,

R(d(cid:48), x, y) ≥ b(dp, x, y) + b0(dp, x, y),

(27)

where

b0(dp, x, y) =

¬ cap(xn, dp) ∧ 1[xn ∈ eu] 1[yn = qu].

(28)

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

Proof See Appendix D for the proof of Theorem 20.

4. Incremental Computation

For every preﬁx dp evaluated during Algorithm 1’s execution, we compute the objective
lower bound b(dp, x, y) and sometimes the objective R(d, x, y) of the corresponding rule
list d. These calculations are the dominant computations with respect to execution time.
This motivates our use of a highly optimized library, designed by Yang et al. (2017), for
representing rule lists and performing operations encountered in evaluating functions of rule
lists. Furthermore, we exploit the hierarchical nature of the objective function and its lower
bound to compute these quantities incrementally throughout branch-and-bound execution.
In this section, we provide explicit expressions for the incremental computations that are
central to our approach. Later, in §5, we describe a cache data structure for supporting our
incremental framework in practice.

For completeness, before presenting our incremental expressions, let us begin by writing
down the objective lower bound and objective of the empty rule list, d = ((), (), q0, 0), the
ﬁrst rule list evaluated in Algorithm 1. Since its preﬁx contains zero rules, it has zero preﬁx
misclassiﬁcation error and also has length zero. Thus, the empty rule list’s objective lower
bound is zero, i.e., b((), x, y) = 0. Since none of the data are captured by the empty preﬁx,
the default rule corresponds to the majority class, and the objective corresponds to the
default rule misclassiﬁcation error, i.e., R(d, x, y) = (cid:96)0((), q0, x, y).

Now, we derive our incremental expressions for the objective function and its lower
p, q(cid:48)
bound. Let d = (dp, δp, q0, K) and d(cid:48) = (d(cid:48)
0, K + 1) be rule lists such that preﬁx dp =
(p1, . . . , pK) is the parent of d(cid:48)
p = (q1, . . . ,
qK, qK+1) be the corresponding labels. The hierarchical structure of Algorithm 1 enforces
that if we ever evaluate d(cid:48), then we will have already evaluated both the objective and ob-
jective lower bound of its parent, d. We would like to reuse as much of these computations as
possible in our evaluation of d(cid:48). We can write the objective lower bound of d(cid:48) incrementally,

p = (p1, . . . , pK, pK+1). Let δp = (q1, . . . , qK) and δ(cid:48)

p, δ(cid:48)

26

Learning Certifiably Optimal Rule Lists

with respect to the objective lower bound of d:

b(d(cid:48)

p, δ(cid:48)
p, x, y) = (cid:96)p(d(cid:48)
N
(cid:88)

p, x, y) + λ(K + 1)
K+1
(cid:88)

cap(xn, pk | d(cid:48)

=

1
N

n=1

k=1

p) ∧ 1[qk (cid:54)= yn] + λ(K + 1)

(29)

= (cid:96)p(dp, δp, x, y) + λK + λ +

cap(xn, pK+1 | d(cid:48)

p) ∧ 1[qK+1 (cid:54)= yn]

1
N

N
(cid:88)

n=1

= b(dp, x, y) + λ +

cap(xn, pK+1 | d(cid:48)

p) ∧ 1[qK+1 (cid:54)= yn]

= b(dp, x, y) + λ +

¬ cap(xn, dp) ∧ cap(xn, pK+1) ∧ 1[qK+1 (cid:54)= yn].

(30)

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

Thus, if we store b(dp, x, y), then we can reuse this quantity when computing b(d(cid:48)
p, x, y).
Transforming (29) into (30) yields a signiﬁcantly simpler expression that is a function of
the stored quantity b(dp, x, y). For the objective of d(cid:48), ﬁrst let us write a na¨ıve expression:

R(d(cid:48), x, y) = (cid:96)(d(cid:48), x, y) + λ(K + 1) = (cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) + (cid:96)0(d(cid:48)

p, q(cid:48)

0, x, y) + λ(K + 1)

=

1
N

N
(cid:88)

K+1
(cid:88)

n=1

k=1

1
N

N
(cid:88)

n=1

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] +

¬ cap(xn, d(cid:48)

p) ∧ 1[q(cid:48)

0 (cid:54)= yn] + λ(K + 1).

(31)

Instead, we can compute the objective of d(cid:48) incrementally with respect to its objective lower
bound:

R(d(cid:48), x, y) = (cid:96)p(d(cid:48)
= b(d(cid:48)

p, x, y) + (cid:96)0(d(cid:48)
p, δ(cid:48)
p, q(cid:48)
p, x, y) + (cid:96)0(d(cid:48)
N
(cid:88)

p, q(cid:48)
0, x, y)

0, x, y) + λ(K + 1)

= b(d(cid:48)

p, x, y) +

¬ cap(xn, d(cid:48)

p) ∧ 1[q(cid:48)

0 (cid:54)= yn]

1
N

1
N

n=1
N
(cid:88)

n=1

= b(d(cid:48)

p, x, y) +

¬ cap(xn, dp) ∧ (¬ cap(xn, pK+1)) ∧ 1[q(cid:48)

0 (cid:54)= yn].

(32)

The expression in (32) is simpler to compute than that in (31), because the former reuses
p, x, y), which we already computed in (30). Note that instead of computing R(d(cid:48), x, y)
b(d(cid:48)
incrementally from b(d(cid:48)
p, x, y) as in (32), we could have computed it incrementally from
R(d, x, y). However, doing so would in practice require that we store R(d, x, y) in addition
to b(dp, x, y), which we already must store to support (30). We prefer the incremental
approach suggested by (32) since it avoids this additional storage overhead.

We present an incremental branch-and-bound procedure in Algorithm 2, and show the
incremental computations of the objective lower bound (30) and objective (32) as two
separate functions in Algorithms 3 and 4, respectively. In Algorithm 2, we use a cache to

27

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Algorithm 2 Incremental branch-and-bound for learning rule lists, for simplicity, from a
cold start. We explicitly show the incremental objective lower bound and objective functions
in Algorithms 3 and 4, respectively.

Input: Objective function R(d, x, y), objective lower bound b(dp, x, y), set of antecedents
S = {sm}M
Output: Provably optimal rule list d∗ with minimum objective R∗

m=1, training data (x, y) = {(xn, yn)}N

n=1, regularization parameter λ

dc ← ((), (), q0, 0)
(cid:46) Initialize current best rule list with empty rule list
Rc ← R(dc, x, y)
(cid:46) Initialize current best objective
(cid:46) Initialize queue with empty preﬁx
Q ← queue( [ ( ) ] )
C ← cache( [ ( ( ) , 0 ) ] ) (cid:46) Initialize cache with empty preﬁx and its objective lower bound
(cid:46) Optimization complete when the queue is empty
while Q not empty do
(cid:46) Remove a length-K preﬁx dp from the queue
dp ← Q.pop( )
(cid:46) Look up dp’s lower bound in the cache
b(dp, x, y) ← C.ﬁnd(dp)
(cid:46) Bit vector indicating data not captured by dp
u ← ¬ cap(x, dp)
(cid:46) Evaluate all of dp’s children
for s in S do

if s not in dp then
Dp ← (dp, s)
v ← u ∧ cap(x, s)
b(Dp, x, y) ← b(dp, x, y) + λ + IncrementalLowerBound(v, y, N )
if b(Dp, x, y) < Rc then

(cid:46) Branch: Generate child Dp
(cid:46) Bit vector indicating data captured by s in Dp

(cid:46) Bound: Apply bound from Theorem 1

R(D, x, y) ← b(Dp, x, y) + IncrementalObjective(u, v, y, N )
D ← (Dp, ∆p, Q0, K + 1)
if R(D, x, y) < Rc then

(cid:46) ∆p, Q0 are set in the incremental functions

(dc, Rc) ← (D, R(D, x, y)) (cid:46) Update current best rule list and objective

end if
Q.push(Dp)
C.insert(Dp, b(Dp, x, y))

(cid:46) Add Dp to the queue
(cid:46) Add Dp and its lower bound to the cache

end if

end if

end for
end while
(d∗, R∗) ← (dc, Rc)

(cid:46) Identify provably optimal rule list and objective

store preﬁxes and their objective lower bounds. Algorithm 2 additionally reorganizes the
structure of Algorithm 1 to group together the computations associated with all children
of a particular preﬁx. This has two advantages. The ﬁrst is to consolidate cache queries: all
children of the same parent preﬁx compute their objective lower bounds with respect to the
parent’s stored value, and we only require one cache ‘ﬁnd’ operation for the entire group
of children, instead of a separate query for each child. The second is to shrink the queue’s
size: instead of adding all of a preﬁx’s children as separate queue elements, we represent
the entire group of children in the queue by a single element. Since the number of children
associated with each preﬁx is close to the total number of possible antecedents, both of these
eﬀects can yield signiﬁcant savings. For example, if we are trying to optimize over rule lists

28

Learning Certifiably Optimal Rule Lists

Algorithm 3 Incremental objective lower bound (30) used in Algorithm 2.

Input: Bit vector v ∈ {0, 1}N indicating data captured by s, the last antecedent in Dp,
bit vector of class labels y ∈ {0, 1}N , number of observations N
Output: Component of D’s misclassiﬁcation error due to data captured by s

function IncrementalLowerBound(v, y, N )

nv = sum(v)
w ← v ∧ y
nw = sum(w)
if nw/nv > 0.5 then

return (nv − nw)/N

else

return nw/N

end if
end function

(cid:46) Number of data captured by s, the last antecedent in Dp
(cid:46) Bit vector indicating data captured by s with label 1
(cid:46) Number of data captured by s with label 1

(cid:46) Misclassiﬁcation error of the rule s → 1

(cid:46) Misclassiﬁcation error of the rule s → 0

Algorithm 4 Incremental objective function (32) used in Algorithm 2.

Input: Bit vector u ∈ {0, 1}N indicating data not captured by Dp’s parent preﬁx, bit
vector v ∈ {0, 1}N indicating data not captured by s, the last antecedent in Dp, bit
vector of class labels y ∈ {0, 1}N , number of observations N
Output: Component of D’s misclassiﬁcation error due to its default rule

function IncrementalObjective(u, v, y, N )

f ← u ∧ ¬ v
nf = sum(f )
g ← f ∧ y
ng = sum(g)
if ng/nf > 0.5 then

(cid:46) Bit vector indicating data not captured by Dp
(cid:46) Number of data not captured by Dp
(cid:46) Bit vector indicating data not captured by Dp with label 1
(cid:46) Number of data not captued by Dp with label 1

return (nf − ng)/N

(cid:46) Misclassiﬁcation error of the default label prediction 1

return ng/N

(cid:46) Misclassiﬁcation error of the default label prediction 0

else

end if
end function

formed from a set of 1000 antecedents, then the maximum queue size in Algorithm 2 will
be smaller than that in Algorithm 1 by a factor of nearly 1000.

29

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

5. Implementation

We implement our algorithm using a collection of optimized data structures that we describe
in this section. First, we explain how we use a preﬁx tree (§5.1) to support the incremental
computations that we motivated in §4. Second, we describe several queue designs that
implement diﬀerent search policies (§5.2). Third, we introduce a symmetry-aware map (§5.3)
to support symmetry-aware pruning (Corollary 16, §3.11). Next, we summarize how these
data structures interact throughout our model of incremental execution (§5.4). In particular,
Algorithms 5 and 6 illustrate many of the computational details from CORELS’ inner
loop, highlighting each of the bounds from §3 that we use to prune the search space. We
additionally describe how we garbage collect our data structures (§5.5). Finally, we explore
how our queue can be used to support custom scheduling policies designed to improve
performance (§5.6).

5.1 Preﬁx Tree

Our incremental computations (§4) require a cache to keep track of preﬁxes that we have
already evaluated and that are also still under consideration by the algorithm. We implement
this cache as a preﬁx tree, a data structure also known as a trie, which allows us to eﬃciently
represent structure shared between related preﬁxes. Each node in the preﬁx tree encodes
an individual rule rk = pk → qk. Each path starting from the root represents a preﬁx, such
that the ﬁnal node in the path also contains metadata associated with that preﬁx. For a
preﬁx dp = (p1, . . . , pK), let ϕ(dp) denote the corresponding node in the trie. The metadata
at node ϕ(dp) supports the incremental computation and includes:

• An index encoding pK, the last antecedent.

• The objective lower bound b(dp, x, y), deﬁned in (5), the central bound in our frame-

work (Theorem 1).

• The lower bound on the default rule misclassiﬁcation error b0(dp, x, y), deﬁned in (28),

to support our equivalent points bound (Theorem 20).

• An indicator denoting whether this node should be deleted (see §5.5).

• A representation of viable extensions of dp, i.e., length K + 1 preﬁx that start with dp

and have not been pruned.

For evaluation purposes and convenience, we store additional information in the preﬁx tree;
for a preﬁx dp with corresponding rule list d = (dp, δp, q0, K), the node ϕ(dp) also stores:

• The length K; equivalently, node ϕ(dp)’s depth in the trie.

• The label prediction qK corresponding to antecedent pK.

• The default rule label prediction q0.

• Ncap, the number of samples captured by preﬁx dp, as in (34).

• The objective value R(d, x, y), deﬁned in (4).

Finally, we note that we implement the preﬁx tree as a custom C++ class.

30

Learning Certifiably Optimal Rule Lists

5.2 Queue

The queue is a worklist that orders exploration over the search space of possible rule lists;
every queue element corresponds to a leaf in the preﬁx tree, and vice versa. In our imple-
mentation, each queue element points to a leaf; when we pop an element oﬀ the queue, we
use the leaf’s metadata to incrementally evaluate the corresponding preﬁx’s children.

We order entries in the queue to implement several diﬀerent search policies. For exam-
ple, a ﬁrst-in-ﬁrst-out (FIFO) queue implements breadth-ﬁrst search (BFS), and a priority
queue implements best-ﬁrst search. In our experiments (§6), we use the C++ Standard
Template Library (STL) queue and priority queue to implement BFS and best-ﬁrst search,
respectively. For CORELS, priority queue policies of interest include ordering by the lower
bound, the objective, or more generally, any function that maps preﬁxes to real values;
stably ordering by preﬁx length and inverse preﬁx length implement BFS and depth-ﬁrst
search (DFS), respectively. In our released code, we present a uniﬁed implementation, where
we use the STL priority queue to support BFS, DFS, and several best-ﬁrst search policies.
As we demonstrate in our experiments (§6.8), we ﬁnd that using a custom search strategy,
such as ordering by the lower bound, usually leads to a faster runtime than BFS.

We motivate the design of additional custom search strategies in §5.6. In preliminary
work (not shown), we also experimented with stochastic exploration processes that bypass
the need for a queue by instead following random paths from the root to leaves; developing
such methods could be an interesting direction for future work. We note that these search
policies are referred to as node selection strategies in the MIP literature. Strategies such as
best-ﬁrst (best-bound) search and DFS are known as static methods, and the framework
we present in §5.6 has the spirit of estimate-based methods (Linderoth and Savelsbergh,
1999).

5.3 Symmetry-aware Map

The symmetry-aware map supports the symmetry-aware pruning justiﬁed in §3.10. In our
implementation, we speciﬁcally leverage our permutation bound (Corollary 16), though it is
also possible to directly exploit the more general equivalent support bound (Theorem 15).
We use the C++ STL unordered map to keep track of the best known ordering of each
evaluated set of antecedents. The keys of our symmetry-aware map encode antecedents in
canonical order, i.e., antecedent indices in numerically sorted order, and we associate all
permutations of a set of antecedents with a single key. Each key maps to a value that
encodes the best known preﬁx in the permutation group of the key’s antecedents, as well
as the objective lower bound of that preﬁx.

Before we consider adding a preﬁx dp to the trie and queue, we check whether the map
already contains a permutation π(dp) of that preﬁx. If no such permutation exists, then
we insert dp into the map, trie, and queue. Otherwise, if a permutation π(dp) exists and
the lower bound of dp is better than that of π(dp), i.e., b(dp, x, y) < b(π(dp), x, y), then we
update the map and remove π(dp) and its entire subtree from the trie; we also insert dp into
the trie and queue. Otherwise, if there exists a permutation π(dp) such that b(π(dp), x, y) ≤
b(dp, x, y), then we do nothing, i.e., we do not insert dp into any data structures.

31

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

5.4 Incremental Execution

Mapping our algorithm to our data structures produces the following execution strategy,
which we also illustrate in Algorithms 5 and 6. We initialize the current best objective Rc
and rule list dc. While the trie contains unexplored leaves, a scheduling policy selects the
next preﬁx dp to extend; in our implementation, we pop elements from a (priority) queue,
until the queue is empty. Then, for every antecedent s that is not in dp, we construct a new
preﬁx Dp by appending s to dp; we incrementally calculate the lower bound b(Dp, x, y),
the objective R(D, x, y), of the associated rule list D, and other quantities used by our
algorithm, summarized by the metadata ﬁelds of the (potential) preﬁx tree node ϕ(Dp).

If the objective R(D, x, y) is less than the current best objective Rc, then we update Rc
and dc. If the lower bound of the new preﬁx Dp is less than the current best objective,
then as described in §5.3, we query the symmetry-aware map for Dp; if we insert d(cid:48)
p into
the symmetry-aware map, then we also insert it into the trie and queue. Otherwise, then
by our hierarchical lower bound (Theorem 1), no extension of Dp could possibly lead to a
rule list with objective better than Rc, thus we do not insert Dp into the tree or queue. We
also leverage our other bounds from §3 to aggressively prune the search space; we highlight
each of these bounds in Algorithms 5 and 6, which summarize the computations and data
structure operations performed in CORELS’ inner loop. When there are no more leaves
to explore, i.e., the queue is empty, we output the optimal rule list. We can optionally
terminate early according to some alternate condition, e.g., when the size of the preﬁx tree
exceeds some threshold.

5.5 Garbage Collection

During execution, we garbage collect the trie. Each time we update the minimum objective,
we traverse the trie in a depth-ﬁrst manner, deleting all subtrees of any node with lower
bound larger than the current minimum objective. At other times, when we encounter a
node with no children, we prune upwards, deleting that node and recursively traversing
the tree towards the root, deleting any childless nodes. This garbage collection allows us
to constrain the trie’s memory consumption, though in our experiments we observe the
minimum objective to decrease only a small number of times.

In our implementation, we cannot immediately delete preﬁx tree leaves because each
corresponds to a queue element that points to it. The C++ STL priority queue is a wrapper
container that prevents access to the underlying data structure, and thus we cannot access
elements in the middle of the queue, even if we know the relevant identifying information. We
therefore have no way to update the queue without iterating over every element. We address
this by marking preﬁx tree leaves that we wish to delete (see §5.1), deleting the physical
nodes lazily, after they are popped from the queue. Later, in our section on experiments (§6),
we refer to two diﬀerent queues that we deﬁne here: the physical queue corresponds to the
C++ queue, and thus all preﬁx tree leaves, and the logical queue corresponds only to those
preﬁx tree leaves that have not been marked for deletion.

32

Learning Certifiably Optimal Rule Lists

Algorithm 5 The inner loop of CORELS, which evaluates all children of a preﬁx dp.
1[xn ∈ eu][yn = qu]

Deﬁne z ∈ {0, 1}N , s.t. zn = (cid:80)U

u=1

Deﬁne b(dp, x, y) and u = ¬ cap(x, dp)
for s in S if s not in dp then do

(cid:46) eu is the equivalent points set containing xn and qu is the minority class label of eu (§3.14)
(cid:46) u is a bit vector indicating data not captured by dp
(cid:46) Evaluate all of dp’s children
(cid:46) Branch: Generate child Dp
(cid:46) Bit vector indicating data captured by s in Dp
(cid:46) Number of data captured by s, the last antecedent in Dp

(cid:46) Lower bound on antecedent support (Theorem10)

(cid:46) Bit vector indicating data captured by s with label 1
(cid:46) Number of data captured by s with label 1

(cid:46) Number of correct predictions by the new rule s → 1

(cid:46) Number of correct predictions by the new rule s → 0

Dp ← (dp, s)
v ← u ∧ cap(x, s)
nv = sum(v)
if nv/N < λ then
continue

end if
w ← v ∧ y
nw = sum(w)
if nw/nv ≥ 0.5 then

nc ← nw

else

nc ← nv − nw

end if
if nc/N < λ then
continue

(cid:46) Lower bound on accurate antecedent support (Theorem 11)

end if
δb ← (nv − nc)/N
b(Dp, x, y) ← b(dp, x, y) + λ + δb
if b(Dp, x, y) ≥ Rc then

(cid:46) Misclassiﬁcation error of the new rule
(cid:46) Incremental lower bound (30)
(cid:46) Hierarchical objective lower bound (Theorem 1)

continue

end if
f ← u ∧ ¬ v
nf = sum(f )
g ← f ∧ y
ng = sum(g)
if ng/nf ≥ 0.5 then

δR ← (nf − ng)/N

else

δR ← ng/N

end if
R(D, x, y) ← b(Dp, x, y) + δR
D ← (Dp, ∆p, Q0, K + 1)
if R(D, x, y) < Rc then

(cid:46) Bit vector indicating data not captured by Dp
(cid:46) Number of data not captured by Dp
(cid:46) Bit vector indicating data not captured by Dp with label 1
(cid:46) Number of data not captued by Dp with label 1

(cid:46) Misclassiﬁcation error of the default label prediction 1

(cid:46) Misclassiﬁcation error of the default label prediction 0

(cid:46) Incremental objective (32)
(cid:46) ∆p, Q0 are set in the incremental functions

(dc, Rc) ← (D, R(D, x, y))
(cid:46) Update current best rule list and objective
GarbageCollectPrefixTree(Rc) (cid:46) Delete nodes with lower bound ≥ Rc − λ (§5.5),
using the Lookahead bound (Lemma 2)
(cid:46) Lower bound on the default rule misclassiﬁcation
error deﬁned in (28)
(cid:46) Equivalent points bound (Theorem 20)
combined with the Lookahead bound (Lemma 2)

end if
b0(Dp, x, y) ← sum(f ∧ z)/N
b ← b(Dp, x, y) + b0(Dp, x, y)
if b + λ ≥ Rc then

continue

end if
CheckMapAndInsert(Dp, b)

end for

(cid:46) Check the Permutation bound (Corollary 16) and
possibly insert Dp into data structures (Algorithm 6)

33

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Algorithm 6 Possibly insert a preﬁx into CORELS’ data structures, after ﬁrst checking the
symmetry-aware map, which supports search space pruning triggered by the permutation
bound (Corollary 16). For further context, see Algorithm 5.

T is the preﬁx tree (§5.1)
Q is the queue, for concreteness, a priority queue ordered by the lower bound (§5.2)
P is the symmetry-aware map (§5.3)

function CheckMapAndInsert(Dp, b)

π0 ← sort(Dp)
(Dπ, bπ) ← P .ﬁnd(π0)
if Dπ exists then
if b < bπ then

P .update(π0, (Dp, b))
T .delete subtree(Dπ)
T .insert(ϕ(Dp))
Q.push(Dp, b)

else

end if

else

P .insert(π0, (Dp, b))
T .insert(ϕ(Dp))
Q.push(Dp, b)

end if
end function

(cid:46) Dp’s antecedents in canonical order
(cid:46) Look for a permutation of Dp

(cid:46) Dp is better than Dπ
(cid:46) Replace Dπ with Dp in the map
(cid:46) Delete Dπ and its subtree from the preﬁx tree
(cid:46) Add node for Dp to the preﬁx tree
(cid:46) Add Dp to the queue

(cid:46) Add Dp to the map
(cid:46) Add node for Dp to the preﬁx tree
(cid:46) Add Dp to the queue

pass

(cid:46) Dp is inferior to Dπ, thus do not insert it into any data structures

5.6 Custom Scheduling Policies

In our setting, an ideal scheduling policy would immediately identify an optimal rule list,
and then certify its optimality by systematically eliminating the remaining search space.
This motivates trying to design scheduling policies that tend to quickly ﬁnd optimal rule
lists. When we use a priority queue to order the set of preﬁxes to evaluate next, we are free
to implement diﬀerent scheduling policies via the ordering of elements in the queue. This
motivates designing functions that assign higher priorities to ‘better’ preﬁxes that we believe
are more likely to lead to optimal rule lists. We follow the convention that priority queue
elements are ordered by keys, such that keys with smaller values have higher priorities.

We introduce a custom class of functions that we call curiosity functions. Broadly, we
think of the curiosity of a rule list d as the expected objective value of another rule list d(cid:48)
that is related to d; diﬀerent models of the relationship between d and d(cid:48) lead to diﬀerent
curiosity functions. In general, the curiosity of d is, by deﬁnition, equal to the sum of the
expected misclassiﬁcation error and the expected regularization penalty of d(cid:48):

C(dp, x, y) ≡ E[R(d(cid:48), x, y)] = E[(cid:96)(d(cid:48)

p, δ(cid:48)

p, x, y)] + λE[K(cid:48)].

(33)

34

Learning Certifiably Optimal Rule Lists

Next, we describe a simple curiosity function for a rule list d with preﬁx dp. First,

let Ncap denote the number of observations captured by dp, i.e.,

Ncap ≡

cap(xn, dp).

(34)

N
(cid:88)

n=1

We now describe a model that generates another rule list d(cid:48) = (d(cid:48)
sume that preﬁx d(cid:48)
antecedent in d(cid:48)
then, the expected length of d(cid:48)

0, K(cid:48)) from dp. As-
p starts with dp and captures all the data, such that each additional
p captures as many ‘new’ observations as each antecedent in dp, on average;

p, q(cid:48)

p, δ(cid:48)

p is

E[K(cid:48)] =

N
Ncap/K

.

Furthermore, assume that each additional antecedent in d(cid:48)
antecedent in dp, on average, thus the expected misclassiﬁcation error of d(cid:48)

p makes as many mistakes as each

p is

E[(cid:96)(d(cid:48)

p, δ(cid:48)

p, x, y)] = E[(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y)] + E[(cid:96)0(d(cid:48)

= E[(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y)] = E[K(cid:48)]

p, q(cid:48)
0, x, y)]
(cid:18) (cid:96)p(dp, δp, x, y)
K

(cid:19)

.

0, x, y) is zero because we assume
p captures all the data. Inserting (35) and (36) into (33) thus gives curiosity for this

Note that the default rule misclassiﬁcation error (cid:96)0(d(cid:48)
that d(cid:48)
model:

p, q(cid:48)

(35)

(36)

C(dp, x, y) =

(cid:96)p(dp, δp, x, y) + λK

(cid:19)

(cid:19) (cid:18)

(cid:18) N
Ncap

(cid:32)

=

1
N

N
(cid:88)

n=1

(cid:33)−1

cap(xn, dp)

b(dp, x, y) =

b(dp, x, y)
supp(dp, x)

,

where for the second equality, we used the deﬁnitions in (34) of Ncap and in (5) of dp’s lower
bound, and for the last equality, we used the deﬁnition in (2) of dp’s normalized support.

The curiosity for a preﬁx dp is thus also equal to its objective lower bound, scaled by
the inverse of its normalized support. For two preﬁxes with the same lower bound, curiosity
gives higher priority to the one that captures more data. This is a well-motivated scheduling
strategy if we model preﬁxes that extend the preﬁx with smaller support as having more
‘potential’ to make mistakes. We note that using curiosity in practice does not introduce
new bit vector or other expensive computations; during execution, we can calculate curiosity
as a simple function of already derived quantities.

In preliminary experiments, we observe that using a priority queue ordered by curiosity
sometimes yields a dramatic reduction in execution time, compared to using a priority queue
ordered by the objective lower bound. Thus far, we have observed signiﬁcant beneﬁts on
speciﬁc small problems, where the structure of the solutions happen to render curiosity
particularly eﬀective (not shown). Designing and studying other ‘curious’ functions, that
are eﬀective in more general settings, is an exciting direction for future work.

35

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Data set

Prediction problem

N

ProPublica Two-year recidivism
NYPD
Weapon possession
Weapon possession
NYCLU

6,907
325,800
29,595

Positive
fraction
0.46
0.03
0.05

Resample
training set
No
Yes
Yes

Training
set size
6,217
566,839
50,743

Test
set size
692
32,580
2,959

Table 1: Summary of data sets and prediction problems. The last ﬁve columns report the
total number of observations, the fraction of observations with the positive class
label, whether we resampled the training set due to class imbalance, and the sizes
of each training and test set in our 10-fold cross-validation studies.

6. Experiments

Our experimental analysis addresses ﬁve questions: How does CORELS’ predictive perfor-
mance compare to that of COMPAS scores and other algorithms? (§6.4, §6.5, and §6.6)
How does CORELS’ model size compare to that of other algorithms? (§6.6) How rapidly do
the objective value and its lower bound converge, for diﬀerent values of the regularization
parameter λ? (§6.7) How much does each of the implementation optimizations contribute
to CORELS’ performance? (§6.8) How rapidly does CORELS prune the search space? (§6.7
and §6.8) Before proceeding, we ﬁrst describe our computational environment (§6.1), as
well as the data sets and prediction problems we use (§6.2), and then in Section 6.3 show
example optimal rule lists found by CORELS.

6.1 Computational Environment

All timed results ran on a server with an Intel Xeon E5-2699 v4 (55 MB cache, 2.20 GHz)
processor and 264 GB RAM, and we ran each timing measurement separately, on a single
hardware thread, with nothing else running on the server. Except where we mention a
memory constraint, all experiments can run comfortably on smaller machines, e.g., a laptop
with 16 GB RAM.

6.2 Data Sets and Prediction Problems

Our evaluation focuses on two socially-important prediction problems associated with re-
cent, publicly-available data sets. Table 1 summarizes the data sets and prediction problems,
and Table 2 summarizes feature sets extracted from each data set, as well as antecedent sets
we mine from these feature sets. We provide some details next. For further details about
data sets, preprocessing steps, and antecedent mining, see Appendix E.

6.2.1 Recidivism Prediction

For our ﬁrst problem, we predict which individuals in the ProPublica COMPAS data
set (Larson et al., 2016) recidivate within two years. This data set contains records for
all oﬀenders in Broward County, Florida in 2013 and 2014 who were given a COMPAS
score pre-trial. Recidivism is deﬁned as being charged with a new crime within two years

36

Learning Certifiably Optimal Rule Lists

Data set

ProPublica
ProPublica
NYPD
NYPD
NYCLU

Feature Categorical Binary
features
attributes
13
6
17
7
28
5
20
3
28
5

set
A
B
C
D
E

Mined
antecedents
122
189
28
20
46

of clauses
2
2
1
1
1

No
No
No
No
Yes

Max number Negations

Table 2: Summary of feature sets and mined antecedents. The last ﬁve columns report
the number of categorical attributes, the number of binary features, the average
number of mined antecedents, the maximum number of clauses in each antecedent,
and whether antecedents include negated clauses.

after receiving a COMPAS assessment; the article by Larson et al. (2016), and their code,3
provide more details about this deﬁnition. From the original data set of records for 7,214
individuals, we identify a subset of 6,907 records without missing data. For the majority of
our analysis, we extract a set of 13 binary features (Feature Set A), which our antecedent
mining framework combines into M = 122 antecedents, on average (folds ranged from con-
taining 121 to 123 antecedents). We also consider a second, similar antecedent set in §6.3,
derived from a superset of Feature Set A that includes 4 additional binary features (Feature
Set B).

6.2.2 Weapon Prediction

For our second problem, we use New York City stop-and-frisk data to predict whether a
weapon will be found on a stopped individual who is frisked or searched. For experiments
in Sections 6.3 and 6.5 and Appendix G, we compile data from a database maintained by
the New York Police Department (NYPD) (New York Police Department, 2016), from years
2008-2012, following Goel et al. (2016). Starting from 2,941,390 records, each describing an
incident involving a stopped person, we ﬁrst extract 376,488 records where the suspected
crime was criminal possession of a weapon (CPW).4 From these, we next identify a subset
of 325,800 records for which the individual was frisked and/or searched; of these, criminal
possession of a weapon was identiﬁed in only 10,885 instances (about 3.3%). Resampling
due to class imbalance, for 10-fold cross-validation, yields training sets that each contain
566,839 datapoints. (We form corresponding test sets without resampling.) From a set of
5 categorical features, we form a set of 28 single-clause antecedents corresponding to 28
binary features (Feature Set C). We also consider another, similar antecedent set, derived
from a subset of Feature Set C that excludes 8 location-speciﬁc binary features (Feature
Set D).

In Sections 6.3, 6.6, 6.7, and 6.8, we also use a smaller stop-and-frisk data set, derived by
the NYCLU from the NYPD’s 2014 data (New York Civil Liberties Union, 2014). From the

3. Data and code used in the analysis by Larson et al. (2016) can be found at https://github.com/

propublica/compas-analysis.

4. We ﬁlter for records that explicitly match the string ‘CPW’; we note that additional records, after

converting to lowercase, contain strings such as ‘cpw’ or ‘c.p.w.’

37

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

Figure 3: Example optimal rule lists that predict two-year recidivism for the ProPublica
data set (Feature Set B, M = 189), found by CORELS (λ = 0.005), across 10
cross-validation folds. While some input antecedents contain features for race, no
optimal rule list includes such an antecedent. Every optimal rule list is the same
or similar to one of these examples, with preﬁxes containing the same rules, up
to a permutation, and same default rule.

original data set of 45,787 records, each describing an incident involving a stopped person,
we identify a subset of 29,595 records for which the individual was frisked and/or searched.
Of these, criminal possession of a weapon was identiﬁed in about 5% of instances. As with
the larger NYPD data set, we resample the data to form training sets (but not to form
test sets). From the same set of 5 categorical features as in Feature Set C, we form a set of
M = 46 single-clause antecedents, including negations (Feature Set E).

6.3 Example Optimal Rule Lists

To motivate Feature Set A, described in Appendix E, which we used in most of our analysis
of the ProPublica data set, we ﬁrst consider Feature Set B, a larger superset of features.

Figure 3 shows optimal rule lists learned by CORELS, using Feature Set B, which
additionally includes race categories from the ProPublica data set (African American, Cau-
casian, Hispanic, Other5). For Feature Set B, our antecedent mining procedure generated an
average of 189 antecedents, across folds. None of the optimal rule lists contain antecedents
that directly depend on race; this motivated our choice to exclude race, by using Feature
Set A, in our subsequent analysis. For both feature sets, we replaced the original ProPublica
age categories (<25, 25-45, >45) with a set that is more ﬁne-grained for younger individuals
(18-20, 21-22, 23-25, 26-45, >45). Figure 4 shows example optimal rule lists that CORELS
learns for the ProPublica data set (Feature Set A, λ = 0.005), using 10-fold cross validation.
Figures 5 and 6 show example optimal rule lists that CORELS learns for the NYCLU
(λ = 0.01) and NYPD data sets. Figure 6 shows optimal rule lists that CORELS learns for
the larger NYPD data set.

While our goal is to provide illustrative examples, and not to provide a detailed analysis
nor to advocate for the use of these speciﬁc models, we note that these rule lists are short and
easy to understand. For the examples and regularization parameter choices in this section,

5. We grouped the original Native American (<0.003), Asian (<0.005), and Other (<0.06) categories.

38

Learning Certifiably Optimal Rule Lists

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

Figure 4: Example optimal rule lists that predict two-year recidivism for the ProPublica
data set (Feature Set A, M = 122), found by CORELS (λ = 0.005), across 10
cross-validation folds. Feature Set A is a subset of Feature Set B (Figure 3) that
excludes race features. Optimal rule lists found using the two feature sets are
very similar. The upper and lower rule lists are representative of 7 and 3 folds,
respectively. Each of the remaining 8 solutions is the same or similar to one of
these, with preﬁxes containing the same rules, up to a permutation, and the same
default rule. See Figure 20 in Appendix F for a complete listing.

if (location = transit authority) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = suspicious object) then predict yes
else predict no

Figure 5: An example rule list that predicts whether a weapon will be found on a stopped in-
dividual who is frisked or searched, for the NYCLU stop-and-frisk data set. Across
10 cross-validation folds, the other optimal rule lists found by CORELS (λ = 0.01)
contain the same or equivalent rules, up to a permutation. See also Figure 23 in
Appendix F.

the optimal rule lists are relatively robust across cross-validation folds: the rules are nearly
the same, up to permutations of the preﬁx rules. For smaller values of the regularization
parameter, we observe less robustness, as rule lists are allowed to grow in length. For the
sets of optimal rule lists represented in Figures 3, 4, and 5, each set could be equivalently
expressed as a DNF rule; e.g., this is easy to see when the preﬁx rules all predict the
positive class label and the default rule predicts the negative class label. Our objective is
not designed to enforce any of these properties, though some may be seen as desirable.

As we demonstrate in §6.6, optimal rule lists learned by CORELS achieve accuracies
that are competitive with a suite of other models, including black box COMPAS scores. See
Appendix F for additional listings of optimal rule lists found by CORELS, for each of our
prediction problems, across cross-validation folds, for diﬀerent regularization parameters λ.

39

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Weapon prediction (λ = 0.01, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else predict no

Weapon prediction (λ = 0.01, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (inside or outside = outside) then predict no
else predict yes

Weapon prediction (λ = 0.005, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

Weapon prediction (λ = 0.005, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

Figure 6: Example optimal rule lists for the NYPD stop-and-frisk data set, found by
CORELS. Feature Set C contains attributes for ‘location’ and ‘city’, while Fea-
ture Set D does not. For each choice of regularization parameter and feature set,
the rule lists learned by CORELS, across all 10 cross-validation folds, contain
the same or equivalent rules, up to a permutation, with the exception of a single
fold (Feature Set C, λ = 0.005). For a complete listing, see Figures 21 and 22 in
Appendix F.

6.4 Comparison of CORELS to the Black Box COMPAS Algorithm

The accuracies of rule lists learned by CORELS are competitive with scores generated by
the black box COMPAS algorithm at predicting two-year recidivism for the ProPublica
data set (Figure 9). Across 10 cross-validation folds, optimal rule lists learned by CORELS
(Figure 4, λ = 0.005) have a mean test accuracy of 0.665, with standard deviation 0.018. The
COMPAS algorithm outputs scores between 1 and 10, representing low (1-4), medium (5-7),
and high (8-10) risk for recidivism. As in the analysis by Larson et al. (2016), we interpret a
medium or high score as a positive prediction for two-year recidivism, and a low score as a
negative prediction. Across the 10 test sets, the COMPAS algorithm scores obtain a mean
accuracy of 0.660, with standard deviation 0.019.

Figure 7 shows that CORELS and COMPAS perform similarly across both black and
white individuals. Both algorithms have much higher true positive rates (TPR’s) and false
positive rates (FPR’s) for blacks than whites (left), and higher true negative rates (TNR’s)

40

Learning Certifiably Optimal Rule Lists

Figure 7: Comparison of TPR and FPR (left), as well as TNR and FNR (right), for diﬀerent
races in the ProPublica data set, for CORELS and COMPAS, across 10 cross-
validation folds.

and false negative rates (FNR’s) for whites than blacks (right). The fact that COMPAS has
higher FPR’s for blacks and higher FNR’s for whites was a central observation motivating
ProPublica’s claim that COMPAS is racially biased (Larson et al., 2016). The fact that
CORELS’ models are so simple, with almost the same results as COMPAS, and contain
only counts of past crimes, age, and gender, indicates possible explanations for the uneven
predictions of both COMPAS and CORELS among blacks and whites. In particular, blacks
evaluated within Broward County tend to be younger and have longer criminal histories
within the data set, (on average, 4.4 crimes for blacks versus 2.6 crimes for whites) leading
to higher FPR’s for blacks and higher FNR’s for whites. This aspect of the data could help
to explain why ProPublica concluded that COMPAS was racially biased.

Similar observations have been reported for other datasets, namely that complex ma-
chine learning models do not have an advantage over simpler transparent models (Tollenaar
and van der Heijden, 2013; Bushway, 2013; Zeng et al., 2017). There are many deﬁnitions of
fairness, and it is not clear whether CORELS’ models are fair either, but it is much easier to
debate about the fairness of a model when it is transparent. Additional fairness constraints
or transparency constraints can be placed on CORELS’ models if desired, though one would
need to edit our bounds (§3) and implementation (§5) to impose more constraints.

Regardless of whether COMPAS is racially biased (which our analysis does not indicate is
necessarily true as long as criminal history and age are allowed to be considered as features),
COMPAS may have many other fairness defects that might be considered serious. Many of
COMPAS’s survey questions are direct inquiries about socioeconomic status. For instance,
a sample COMPAS survey6 asks: “Is it easy to get drugs in your neighborhood?,” “How
often do you have barely enough money to get by?,” “Do you frequently get jobs that don’t
pay more than minimum wage?,” “How often have you moved in the last 12 months?”
COMPAS’s survey questions also ask about events that were not caused by the person who

6. A sample COMPAS survey contributed by Julia Angwin, ProPublica, can be found at https://www.

documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html.

41

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

is being evaluated, such as: “If you lived with both parents and they later separated, how
old were you at the time?,” “Was one of your parents ever sent to jail or prison?,” “Was
your mother ever arrested, that you know of?”

The fact that COMPAS requires over 130 questions to be answered, many of whose
answers may not be veriﬁable, means that the computation of the COMPAS score is prone
to errors. Even the Arnold Foundation’s “public-safety assessment” (PSA) score—which is
completely transparent, and has only 9 factors—has been miscalculated in serious criminal
trials, leading to a recent lawsuit (Westervelt, 2017). It is substantially more diﬃcult to
obtain the information required to calculate COMPAS scores than PSA scores (with over 14
times the number of survey questions). This signiﬁcant discrepancy suggests that COMPAS
scores are more fallible than PSA scores, as well as even simpler models, like those produced
by CORELS. Some of these problems could be alleviated by using only data within electronic
records that can be automatically calculated, instead of using information entered by hand
and/or collected via subjective surveys.

The United States government pays Northpointe (now called Equivant) to use COMPAS.
In light of our observations that CORELS is as accurate as COMPAS on a real-world data
set where COMPAS is used in practice, CORELS predicts similarly to COMPAS for both
blacks and whites, and CORELS’ models are completely transparent, it is not clear what
value COMPAS scores possess. Our experiments also indicate that the proprietary survey
data required to compute COMPAS scores has not boosted its prediction accuracy above
that of transparent models in practice.

Risk predictions are important for the integrity of the judicial system; judges cannot be
expected to keep entire databases in their heads to calculate risks, whereas models (when
used correctly) can help to ensure equity. Risk prediction models also have the potential
to heavily impact how eﬃcient the judicial system is, in terms of bail and parole decisions;
eﬃciency in this case means that dangerous individuals are not released, whereas non-
dangerous individuals are granted bail or parole. High stakes decisions, such as these, are
ideal applications for machine learning algorithms that produce transparent models from
high dimensional data.

Currently, justice system data does not support highly accurate risk predictions, but
current risk models are useful in practice, and these risk predictions will become more
accurate as more and higher quality data are made available.

6.5 Comparison of CORELS to a Heuristic Model for Weapon Prediction

CORELS generates simple, accurate models for the task of weapon prediction, using the
NYPD stop-and-frisk data set. Our approach oﬀers a principled alternative to heuristic
models proposed by Goel et al. (2016), who develop a series of regression models to analyze
racial disparities in New York City’s stop-and-frisk policy for a related, larger data set. In
particular, the authors arrive at a heuristic that they suggest could potentially help police
oﬃcers more eﬀectively decide when to frisk and/or search stopped individuals, i.e., when
such interventions are likely to discover criminal possession of a weapon (CPW). Starting
from a full regression model with 7,705 variables, the authors reduce this to a smaller model
with 98 variables; from this, they keep three variables with the largest coeﬃcients. This gives

42

Learning Certifiably Optimal Rule Lists

a heuristic model of the form ax + by + cz ≥ T , where

x = 1[stop reason = suspicious object]
y = 1[stop reason = suspicious bulge]
z = 1[additional circumstances = sights and sounds of criminal activity],

and T is a threshold, such that the model predicts CPW when the threshold is met or
exceeded. We focus on their approach that uses a single threshold, rather than precinct-
speciﬁc thresholds. To increase ease-of-use, the authors round the coeﬃcients to the nearest
integers, which gives (a, b, c) = (3, 1, 1); this constrains the threshold to take one of six
values, T ∈ {0, 1, 2, 3, 4, 5}. To employ this heuristic model in the ﬁeld, “. . . oﬃcers simply
need to add at most three small, positive integers . . . and check whether the sum exceeds a
ﬁxed threshold. . . ” (Goel et al., 2016).

Figure 8 directly compares various models learned by CORELS to the heuristic models,
using the same data set as Goel et al. (2016) and 10-fold cross-validation. Recall that we
train on resampled data to correct for class imbalance; we evaluate with respect to test sets
that have been formed without resampling. For CORELS, the models correspond to the
rule lists illustrated in Figure 6 from Section 6.3, and Figures 21 and 22 in Appendix F, we
consider both Feature Sets C and D and both regularization parameters λ = 0.005 and 0.01.
The top panel plots the fraction of weapons recovered as a function of the fraction of stops
where the individual was frisked and/or searched. Goel et al. (2016) target models that
eﬃciently recover a majority of weapons (while also minimizing racial disparities, which
we do not address here). Interestingly, the models learned by CORELS span a signiﬁcant
region that is not available to the heuristic model, which would require larger or non-integer
parameters to access the region. The region is possibly desirable, since it includes models
(λ = 0.005, bright red) that recover a majority (≥ 50%) of weapons (that are known in the
data set). More generally, CORELS’ models all recover at least 40% of weapons on average,
i.e., more weapons than any of the heuristic models with T ≥ 2, which recover less than
25% of weapons on average. At the same time, CORELS’ models all require well under 25%
of stops—signiﬁcantly less than the heuristic model with T = 1, which requires over 30% of
stops to recover a fraction of weapons comparable to the CORELS model that recovers the
most weapons.

The bottom panel in Figure 8 plots both TPR and FPR and labels model size, for each
of the models in the top panel. For the heuristic, we deﬁne model size as the number of
model parameters; for CORELS, we use the number of rules in the rule list, which is equal
to the number of leaves when we view a rule list as a decision tree. The heuristic models all
have 4 parameters, while the diﬀerent CORELS models have either 3 or approximately 5
rules. CORELS’ models are thus approximately as small, interpretable, and transparent as
the heuristic models; furthermore, their predictions are straightforward to compute, without
even requiring arithmetic.

6.6 Predictive Performance and Model Size for CORELS and Other Algorithms

We ran a 10-fold cross validation experiment using CORELS and eight other algorithms:
logistic regression, support vector machines (SVM), AdaBoost, CART, C4.5, random for-

43

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 8: Weapon prediction with the NYPD stop-and-frisk data set, for various models
learned by CORELS and the heuristic model by Goel et al. (2016), across 10
cross-validation folds. Note that the fraction of weapons recovered (top) is equal
to the TPR (bottom, open markers). Markers above the dotted horizontal lines at
the value 0.5 correspond to models that recover a majority of weapons (that are
known in the data set). Top: Fraction of weapons recovered as a function of the
fraction of stops where the individual was frisked and/or searched. In the legend,
entries for CORELS (red markers) indicate the regularization parameter (λ) and
whether or not extra location features were used (“location”); entries for the
heuristic model (blue markers) indicate the threshold value (T ). The results we
report for the heuristic model are our reproduction of the results reported in Figure
9 by Goel et al. (2016) (ﬁrst four open circles in that ﬁgure, from left to right;
we exclude the trivial open circle showing 100% of weapons recovered at 100% of
stops, obtained by setting the threshold at 0). Bottom: Comparison of TPR (open
markers) and FPR (solid markers) for various CORELS and heuristic models.
Models are sorted left-to-right by TPR. Markers and abbreviated horizontal tick
labels correspond to the legend in the top ﬁgure. Numbers in the plot label model
size; there was no variation in model size across folds, except for a single fold for
CORELS (λ = 0.005, Feature Set C), which found a model of size 6.

44

Learning Certifiably Optimal Rule Lists

Figure 9: Two-year recidivism prediction for the ProPublica COMPAS data set. Compari-
son of CORELS and a panel of nine other algorithms: logistic regression (GLM),
support vector machines (SVM), AdaBoost, CART, C4.5, random forests (RF),
RIPPER, scalable Bayesian rule lists (SBRL), and COMPAS. For CORELS, we
use regularization parameter λ = 0.005.

est (RF), RIPPER, and scalable Bayesian rule lists (SBRL).7 We use standard R packages,
with default parameter settings, for the ﬁrst seven algorithms.8 We use the same antecedent
sets as input to the two rule list learning algorithms, CORELS and SBRL; for the other
algorithms, the inputs are binary feature sets corresponding to the single clause antecedents
in the aforementioned antecedent sets (see Appendix E).

Figure 9 shows that for the ProPublica data set, there were no statistically signiﬁcant
diﬀerences in test accuracies across algorithms, the diﬀerence between folds was far larger
than the diﬀerence between algorithms. These algorithms also all perform similarly to the
black box COMPAS algorithm. Figure 10 shows that for the NYCLU data set, logistic
regression, SVM, and AdaBoost have the highest TPR’s and also the highest FPR’s; we show
TPR and FPR due to class imbalance. For this problem, CORELS obtains an intermediate
TPR, compared to other algorithms, while achieving a relatively low FPR. We conclude
that CORELS produces models whose predictive performance is comparable to or better
than those found via other algorithms.

Figures 11 and 12 summarize diﬀerences in predictive performance and model size for
CORELS and other tree (CART, C4.5) and rule list (RIPPER, SBRL) learning algorithms.
Here, we vary diﬀerent algorithm parameters, and increase the number of iterations for
SBRL to 10,000. For two-year recidivism prediction with the ProPublica data set (Fig-
ure 11), we plot both training and test accuracy, as a function of the number of leaves
in the learned model. Due to class imbalance for the weapon prediction problem with the
NYCLU stop-and-frisk data set (Figure 12), we plot both true positive rate (TPR) and
false positive rate (FPR), again as a function of the number of leaves. For both problems,
CORELS can learn short rule lists without sacriﬁcing predictive performance. For listings
of example optimal rule lists that correspond to the results for CORELS summarized here,

7. For SBRL, we use the C implementation at https://github.com/Hongyuy/sbrlmod. By default, SBRL

sets η = 3, λ = 9, the number of chains to 11 and iterations to 1,000.

8. For CART, C4.5 (J48), and RIPPER, we use the R packages rpart, RWeka, and caret, respectively. By
default, CART uses complexity parameter cp = 0.01 and C4.5 uses complexity parameter C = 0.25.

45

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 10: TPR (left) and FPR (right) for the test set, for CORELS and a panel of seven
other algorithms, for the weapon prediction problem with the NYCLU stop-
and-frisk data set. Means (white squares), standard deviations (error bars), and
values (colors correspond to folds), for 10-fold cross-validation experiments. For
CORELS, we use λ = 0.01. Note that we were unable to execute RIPPER for
the NYCLU problem.

Figure 11: Training and test accuracy as a function of model size, across diﬀerent methods,
for two-year recidivism prediction with the ProPublica COMPAS data set. In
the legend, numbers in parentheses are algorithm parameters that we vary for
CORELS (λ), CART (cp), C4.5 (C), and SBRL (η, λ, i), where i is the num-
ber of iterations. Legend markers and error bars indicate means and standard
deviations, respectively, across cross-validation folds. Small circles mark training
accuracy means. None of the models exhibit signiﬁcant overﬁtting; mean training
accuracy never exceeds mean test accuracy by more than about 0.01.

see Appendix F. Also see Figure 25 in Appendix G; it uses the larger NYPD data set and
is similar to Figure 12.

In Figure 4, we used CORELS to identify short rule lists, depending on only three
features—age, prior convictions, and sex—that achieve test accuracy comparable to COM-
PAS (Figure 9, also see Angelino et al., 2017). If we restrict CORELS to search the space
of rule lists formed from only age and prior convictions (λ = 0.005), the optimal rule lists it

46

Learning Certifiably Optimal Rule Lists

Figure 12: TPR (top) and FPR (bottom) for the test set, as a function of model size, across
diﬀerent methods, for weapon prediction with the NYCLU stop-and-frisk data
set. In the legend, numbers in parentheses are algorithm parameters, as in Fig-
ure 11. Legend markers and error bars indicate means and standard deviations,
respectively, across cross-validation folds. C4.5 ﬁnds large models for all tested
parameters.

if (priors > 3) then predict yes
else if (age < 25) and (priors = 2 − 3) then predict yes
else predict no

Figure 13: When restricted to two features (age, priors), CORELS (λ = 0.005) ﬁnds the

same rule list across 10 cross-validation folds.

ﬁnds achieve test accuracy that is again comparable to COMPAS. CORELS identiﬁes the
same rule list across all 10 folds of 10-fold cross-validation experiments (Figure 13). In work
subsequent to ours (Angelino et al., 2017), Dressel and Farid (2018) conﬁrmed this result,
in the sense that they used logistic regression to construct a linear classiﬁer with age and
prior convictions, and also achieved similar accuracy to COMPAS. However, computing a
logistic regression model requires multiplication and addition, and their model cannot easily
be computed, in the sense that it requires a calculator (and thus is potentially error-prone).
Our rule lists require no such computation.

47

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

CORELS with diﬀerent regularization parameters (NYCLU stop-and-frisk data set)

Total
time (s)
.61 (.03)
70 (6)
1600 (100)

Time to
optimum (s)
.002 (.001)
.008 (.002)
56 (74)

Lower bound
evaluations (×106)
.070 (.004)
7.5 (.6)
150 (10)

Max evaluated
preﬁx length
6
11
16-17
Total queue
insertions (×103)
2.2 (.1)
210 (20)
4400 (300)

Optimal
preﬁx length
2
3
6-10
Max queue
size (×103)
.9 (.1)
130 (10)
2500 (170)

λ
.04
.01
.0025

λ
.04
.01
.0025

Table 3: Summary of CORELS executions,

for the NYCLU stop-and-frisk data set
(M = 46), for same three regularization parameter (λ) values as in Figure 12.
The columns report the total execution time, time to optimum, maximum evalu-
ated preﬁx length, optimal preﬁx length, number of times we completely evaluate
a preﬁx dp’s lower bound b(dp, x, y), total number of queue insertions (this number
is equal to the number of cache insertions), and the maximum queue size. For pre-
ﬁx lengths, we report single values or ranges corresponding to the minimum and
maximum observed values; in the other columns, we report means (and standard
deviations) over 10 cross-validation folds. See also Figures 14 and 15.

6.7 CORELS Execution Traces, for Diﬀerent Regularization Parameters

In this section, we illustrate several views of CORELS execution traces, for the NYCLU stop-
and-frisk data set with M = 46 antecedents, for the same three regularization parameters
(λ = .04, .01, .025) as in Figure 12.

Table 3 summarizes execution traces across all 10 cross-validation folds. For each value
of λ, CORELS achieves the optimum in a small fraction of the total execution time. As λ
decreases, these times increase because the search problems become more diﬃcult, as is
summarized by the observation that CORELS must evaluate longer preﬁxes; consequently,
our data structures grow in size. We report the total number of elements inserted into the
queue and the maximum queue size; recall from §5 that the queue elements correspond to
the trie’s leaves, and that the symmetry-aware map elements correspond to the trie’s nodes.
The upper panels in Figure 14 plot example execution traces, from a single cross-
validation fold, of both the current best objective value Rc and the lower bound b(dp, x, y)
of the preﬁx dp being evaluated. These plots illustrate that CORELS certiﬁes optimality
when the lower bound matches the objective value. The lower panels in Figure 14 plot corre-
sponding traces of an upper bound on the size of the remaining search space (Theorem 7),
and illustrate that as λ decreases, it becomes more diﬃcult to eliminate regions of the
search space. For Figure 14, we dynamically and incrementally calculate (cid:98)log10 Γ(Rc, Q)(cid:99),
which adds some computational overhead; we do not calculate this elsewhere unless noted.
Figure 15 visualizes the elements in CORELS’ logical queue, for each of the executions in
Figure 14. Recall from §5.5 that the logical queue corresponds to elements in the (physical)

48

Learning Certifiably Optimal Rule Lists

Figure 14: Example executions of CORELS,

for the NYCLU stop-and-frisk data set
(M = 46). See also Table 3 and Figure 15. Top: Objective value (solid line)
and lower bound (dashed line) for CORELS, as a function of wall clock time (log
scale). Numbered points along the trace of the objective value indicate when the
length of the best known rule list changes and are labeled by the new length.
For each value of λ, a star marks the optimum objective value and time at which
it was achieved. Bottom: (cid:98)log10 Γ(Rc, Q)(cid:99), as a function of wall clock time (log
scale), where Γ(Rc, Q) is the upper bound on remaining search space size (Theo-
rem 7). Rightmost panels: For visual comparison, we overlay the execution traces
from the panels to the left, for the three diﬀerent values of λ.

queue that have not been garbage collected from the trie; these are preﬁxes that CORELS
has already evaluated and whose children the algorithm plans to evaluate next. As an
execution progresses, longer preﬁxes are placed in the queue; as λ decreases, the algorithm
must spend more time evaluating longer and longer preﬁxes.

6.8 Eﬃcacy of CORELS Algorithm Optimizations

This section examines the eﬃcacy of each of our bounds and data structure optimizations.
We remove a single bound or data structure optimization from our ﬁnal implementation
and measure how the performance of our algorithm changes. We examine these performance
traces on both the NYCLU and the ProPublica data sets, and highlight the result that on
diﬀerent problems, the relative performance improvements of our optimizations can vary.

Table 4 provides summary statistics for experiments using the full CORELS implementa-
tion (ﬁrst row) and ﬁve variants (subsequent rows) that each remove a speciﬁc optimization:
(1) Instead of a priority queue (§5.2) ordered by the objective lower bound, we use a queue
that implements breadth-ﬁrst search (BFS). (2) We remove checks that would trigger prun-
ing via our lower bounds on antecedent support (Theorem 10) and accurate antecedent
support (Theorem 11). (3) We remove the eﬀect of our lookahead bound (Lemma 2), which
otherwise tightens the objective lower bound by an amount equal to the regularization pa-
rameter λ. (4) We disable the symmetry-aware map (§5.3), our data structure that enables
pruning triggered by the permutation bound (Corollary 16). (5) We do not identify sets
of equivalent points, which we otherwise use to tighten the objective lower bound via the
equivalent points bound (Theorem 20).

49

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 15: Summary of CORELS’ logical queue, for the NYCLU stop-and-frisk data set
(M = 46), for same three regularization parameters as in Figure 12 and Table 3.
Solid lines plot the numbers of preﬁxes in the logical queue (log scale), colored by
length (legend), as a function of wall clock time (log scale). All plots are generated
using a single, representative cross-validation training set. For each execution,
the gray shading ﬁlls in the area beneath the total number of queue elements,
i.e., the sum over all lengths; we also annotate the total time in seconds, marked
with a dashed vertical line.

Removing any single optimization increases total execution time, by varying amounts
across these optimizations. Similar to our experiments in §6.7, we always encounter the
optimal rule list in far less time than it takes to certify optimality. As in Table 3, we report
metrics that are all proxies for how much computational work our algorithm must perform;
these metrics thus scale with the overall slowdown with respect to CORELS execution time
(Table 4, ﬁrst column).

Figure 16 visualizes execution traces of the elements in CORELS’ logical queue, similar
to Figure 15, for a single, representative cross-validation fold. Panels correspond to diﬀerent
removed optimizations, as in Table 4. These plots demonstrate that our optimizations reduce
the number of evaluated preﬁxes and are especially eﬀective at limiting the number of longer
evaluated preﬁxes. For the ProPublica data set, the most important optimization is the
equivalent points bound—without it, we place preﬁxes of at least length 10 in our queue,
and must terminate these executions before they are complete. In contrast, CORELS and
most other variants evaluate only preﬁxes up to at most length 5, except for the variant
without the lookahead bound, which evaluates preﬁxes up to length 6.

Table 5 and Figure 17 summarize an analogous set of experiments for the NYCLU data
set. Note that while the equivalent points bound proved to be the most important opti-
mization for the ProPublica data set, the symmetry-aware map is the crucial optimization
for the NYCLU data set.

Finally, Figure 18 highlights the most signiﬁcant algorithm optimizations for our pre-
diction problems: the equivalent points bound for the ProPublica data set (left) and the

50

Learning Certifiably Optimal Rule Lists

Per-component performance improvement (ProPublica data set)

Slow-
down
—
1.1×
1.5×
13.3×
8.4×

Total time
(min)
.98 (.6)
1.03 (.6)
1.5 (.9)
12.3 (6.2)
9.1 (6.4)

Time to
optimum (s)
1 (1)
2 (4)
1 (2)
1 (1)
2 (3)

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound* >130 (2.6) >180× >1400 (2000)
Total queue
insertions (×106)
.29 (.2)
.33 (.2)
.40 (.2)
3.6 (1.8)
2.5 (1.7)
>510 (1.1)

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound*

Lower bound
evaluations (×106)
26 (15)
27 (16)
42 (25)
320 (160)
250 (180)

>940 (5)

Max evaluated
preﬁx length
5
5
5
6
5
≥11
Max queue
size (×106)
.24 (.1)
.20 (.1)
.33 (.2)
3.0 (1.5)
2.4 (1.7)
>500 (1.2)

Table 4: Per-component performance improvement, for the ProPublica data set (λ = 0.005,
M = 122). The columns report the total execution time, time to optimum, maxi-
mum evaluated preﬁx length, number of times we completely evaluate a preﬁx dp’s
lower bound b(dp, x, y), total number of queue insertions (which is equal to the
number of cache insertions), and maximum logical queue size. The ﬁrst row shows
CORELS; subsequent rows show variants that each remove a speciﬁc implemen-
tation optimization or bound. (We are not measuring the cumulative eﬀects of
removing a sequence of components.) All rows represent complete executions that
certify optimality, except those labeled ‘No equivalent points bound,’ for which
each execution was terminated due to memory constraints, once the size of the
cache reached 5 × 108 elements, after consuming ∼250GB RAM. In all but the
ﬁnal row and column, we report means (and standard deviations) over 10 cross-
validation folds. We also report the mean slowdown in total execution time, with
respect to CORELS. In the ﬁnal row, we report the mean (and standard deviation)
of the incomplete execution time and corresponding slowdown, and a lower bound
on the mean time to optimum; in the remaining ﬁelds, we report minimum values
across folds. See also Figure 16.
* Only 7 out of 10 folds achieve the optimum before being terminated.

symmetry-aware map for the NYCLU data set (right). For CORELS (thin lines) with
the ProPublica recidivism data set (left), the objective drops quickly, achieving the op-
timal value within a second. CORELS certiﬁes optimality in about a minute—the objective
lower bound steadily converges to the optimal objective (top) as the search space shrinks
(bottom). As in Figure 14, we dynamically and incrementally calculate (cid:98)log10 Γ(Rc, Q)(cid:99),
where Γ(Rc, Q) is the upper bound (13) on remaining search space size (Theorem 7); this
adds some computational overhead. In the same plots (left), we additionally highlight a

51

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 16: Summary of the logical queue’s contents, for full CORELS (top left) and ﬁve
variants that each remove a speciﬁc implementation optimization or bound, for
the ProPublica data set (λ = 0.005, M = 122). See also Table 4. Solid lines plot
the numbers of preﬁxes in the logical queue (log scale), colored by length (legend),
as a function of wall clock time (log scale). All plots are generated using a single,
representative cross-validation training set. The gray shading ﬁlls in the area
beneath the total number of queue elements for CORELS, i.e., the sum over
all lengths in the top left ﬁgure. For comparison, we replicate the same gray
region in the other ﬁve subﬁgures. For each execution, we indicate the total time
in seconds, relative to the full CORELS implementation (T = 21 s), and with a
dashed vertical line. The execution without the equivalent points bound (bottom
right) is incomplete.

separate execution of CORELS without the equivalent points bound (Theorem 20) (thick
lines). After more than 2 hours, the execution is still far from complete; in particular, the
lower bound is far from the optimum objective value (top) and much of the search space
remains unexplored (bottom). For the NYCLU stop-and-frisk data set (right), CORELS
achieves the optimum objective in well under a second, and certiﬁes optimality in a lit-
tle over a minute. CORELS without the permutation bound (Corollary 16), and thus the
symmetry-aware map, requires more than an hour, i.e., orders of magnitude more time, to
complete (thick lines).

52

Learning Certifiably Optimal Rule Lists

Per-component performance improvement (NYCLU stop-and-frisk data set)

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound

Algorithm variant
CORELS
No priority queue (BFS)
No support bounds
No lookahead bound
No symmetry-aware map
No equivalent points bound

Slow-
down
—
2.0×
1.1×
1.6×

Time to
optimum (µs)
8.9 (.1)
110 (10)
8.8 (.8)
7.3 (1.8)

Total
time (min)
1.1 (.1)
2.2 (.2)
1.2 (.1)
1.7 (.2)
> 73 (5)
4 (.3)

3.8×

> 68× > 7.6 (.4)
6.4 (.9)
Total queue
insertions (×105)
2.0 (.2)
4.1 (.4)
2.1 (.2)
3.2 (.3)
> 1000 (0)
9.4 (.7)

Lower bound
evaluations (×106)
7 (1)
14 (1)
8 (1)
11 (1)
> 390 (40)
33 (2)

Max evaluated
preﬁx length
11
11
11
11-12
> 10
14
Max queue
size (×105)
1.3 (.1)
1.4 (.1)
1.3 (.1)
2.1 (.2)
> 900 (10)
6.0 (.4)

Table 5: Per-component performance improvement, as in Table 4, for the NYCLU stop-and-
frisk data set (λ = 0.01, M = 46). All rows except those labeled ‘No symmetry-
aware map’ represent complete executions. A single fold running without a
symmetry-aware map required over 2 days to complete, so in order to run all
10 folds above, we terminated execution after the preﬁx tree (§5.1) reached 108
nodes. See Table 4 for a detailed caption, and also Figure 17.

Algorithmic
approach
CORELS
Brute force
Brute force
CORELS (1984)

Max evaluated Lower bound
evaluations
preﬁx length
2.8 × 107
5
2.5 × 1010
5
5.0 × 1020
10
2.8 × 107
5

Predicted runtime

36 seconds

9.0 hours ≈ 3.3 × 104 s
21 × 106 years ≈ 6.5 × 1014 s
13.5 days ≈ 1.2 × 106 s

Table 6: Algorithmic speedup for the ProPublica data set (λ = 0.005, M = 122). Solving
this problem using brute force is impractical due to the inability to explore rule
lists of reasonable lengths. Removing only the equivalent points bound requires
exploring preﬁxes of up to length 10 (see Table 4), a clearly intractable problem.
Even with all of our improvements, however, it is only recently that processors
have been fast enough for this type of discrete optimization algorithm to succeed.

6.9 Algorithmic Speedup

Table 6 shows the overall speedup of CORELS compared to a na¨ıve implementation and
demonstrates the infeasibility of running our algorithm 30 years ago. Consider an execution
of CORELS for the ProPublica data set, with M = 122 antecedents, that evaluates preﬁxes

53

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Figure 17: Summary of the logical queue’s contents, for full CORELS (top left) and ﬁve
variants that each remove a speciﬁc implementation optimization or bound, for
the NYCLU stop-and-frisk data set (λ = 0.01, M = 46), as in Table 5. The ex-
ecution without the symmetry-aware map (bottom center) is incomplete. See
Figure 16 for a detailed caption.

up to length 5 in order to certify optimality (Table 4). A brute force implementation that
na¨ıvely considers all preﬁxes of up to length 5 would evaluate 2.5 × 1010 preﬁxes. As shown
in Figure 4, the optimal rule list has preﬁx length 3, thus the brute force algorithm would
identify the optimal rule list. However, for this approach to certify optimality, it would
have to consider far longer preﬁxes. Without our equivalent points bound, but with all of
our other optimizations, we evaluate preﬁxes up to at least length 10 (see Table 4 and
Figure 16)—thus a brute force algorithm would have to evaluate preﬁxes of length 10 or
longer. Na¨ıvely evaluating all preﬁxes up to length 10 would require looking at 5.0 × 1020
diﬀerent preﬁxes.

However, CORELS examines only 28 million preﬁxes in total—a reduction of 893×
compared to examining all preﬁxes up to length 5 and a reduction of 1.8 × 1013 for the case
of length 10. On a laptop, we require about 1.3 µs to evaluate a single preﬁx (given by
dividing the number of lower bound evaluations by the total time in Table 4). Our runtime
is only about 36 seconds, but the na¨ıve solutions of examining all preﬁxes up to lengths 5

54

Learning Certifiably Optimal Rule Lists

Figure 18: Execution progress of CORELS and selected variants,

for the ProPublica
(λ = 0.005, M = 122) (left) and NYCLU (λ = 0.01, M = 46) (right) data sets.
Top: Objective value (thin solid lines) and lower bound (dashed lines) for
CORELS, as a function of wall clock time (log scale). Numbered points along the
trace of the objective value indicate when the length of the best known rule list
changes, and are labeled by the new length. CORELS quickly achieves the opti-
mal value (star markers), and certiﬁes optimality when the lower bound matches
the objective value. On the left, a separate and signiﬁcantly longer execution of
CORELS without the equivalent points (Theorem 20) bound remains far from
complete, and its lower bound (thick solid line) far from the optimum. On the
right, a separate execution of CORELS without the permutation bound (Corol-
lary 16), and thus the symmetry-aware map, requires orders of magnitude more
time to complete. Bottom: (cid:98)log10 Γ(Rc, Q)(cid:99), as a function of wall clock time (log
scale), where Γ(Rc, Q) is the upper bound (13) on remaining search space size
(Theorem 7). For these problems, the equivalent points (left) and permutation
(right) bounds are responsible for the ability of CORELS to quickly eliminate
most of the search space (thin solid lines); the remaining search space decays
much more slowly without these bounds (thick solid lines).

and 10 would take 9 hours and 21 million years, respectively. It is clear that brute force
would not scale to larger problems.

We compare our current computing circumstances to those of 1984, the year when CART
was published. Moore’s law holds that computing power doubled every 18 months from 1984
to 2006. This is a period of 264 months, which means computing power has gone up by at
least a factor of 32,000 since 1984. Thus, even with our algorithmic and data structural
improvements, CORELS would have required about 13.5 days in 1984—an unreasonable
amount of time. Our advances are meaningful only because we can run them on a modern

55

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

system. Combining our algorithmic improvements with the increase in modern processor
speeds, our algorithm runs more than 1013 times faster than a na¨ıve implementation would
have in 1984. This helps explain why neither our algorithm, nor other branch-and-bound
variants, had been developed before now.

7. Summary and Future Work on Bounds

Here, we highlight our most signiﬁcant bounds, as well as directions for future work based
on bounds that we have yet to leverage in practice.

In empirical studies, we found our equivalent support (§3.10, Theorem 15) and equiv-
alent points (§3.14, Theorem 20) bounds to yield the most signiﬁcant improvements in
algorithm performance. In fact, they sometimes proved critical for ﬁnding solutions and
proving optimality, even on small problems.

Accordingly, we would hope that our similar support bound (§3.13, Theorem 18) could
be useful; understanding how to eﬃciently exploit this result in practice represents an
important direction for future work. In particular, this type of bound may lead to principled
approximate variants of our approach.

We presented several sets of bounds in which at least one bound was strictly tighter than
the other(s). For example, the lower bound on accurate antecedent support (Theorem 11)
is strictly tighter than the lower bound on accurate support (Theorem 10). It might seem
that we should only use this tighter bound, but in practice, we can use both—the looser
bound can be checked before completing the calculation required to check the tighter bound.
Similarly, the equivalent support bound (Theorem 15) is more general than the special case
of the permutation bound (Corollary 16). We have implemented data structures, which we
call symmetry-aware maps, to support both of these bounds, but have not yet identiﬁed an
eﬃcient approach for supporting the more general equivalent points bound. A good solution
may be related to the challenge of designing an eﬃcient data structure to support the similar
support bound.

We also presented results on antecedent rejection that unify our understanding of our
lower (§3.7) and upper bounds (§3.8) on antecedent support. In a preliminary implemen-
tation (not described here), we experimented with special data structures to support the
direct use of our observation that antecedent rejection propagates (§3.9, Theorem 12). We
leave the design of eﬃcient data structures for this task as future work.

During execution, we ﬁnd it useful to calculate an upper bound on the size of the
remaining search space—e.g., via Theorem 7, or the looser Proposition 9, which incurs
less computational overhead—since these provide meaningful information about algorithm
progress and allow us to estimate the remaining execution time. As we illustrated in Sec-
tion 6.8, these calculations also help us qualify the impact of diﬀerent algorithmic bounds,
e.g., by comparing executions that keep or remove bounds.

When our algorithm terminates, it outputs an optimal solution of the training optimiza-
tion problem, with a certiﬁcate of optimality. On a practical note, our approach can also
provide useful results even for incomplete executions. As shown earlier, we have empirically
observed that our algorithm often identiﬁes the optimal rule list very quickly, compared to
the total time required to prove optimality, e.g., in seconds, versus minutes, respectively.
Furthermore, our objective’s lower bounds allow us to place an upper bound on the size of

56

Learning Certifiably Optimal Rule Lists

the remaining search space, and provides guarantees on the quality of a solution output by
an incomplete execution.

The order in which we evaluate preﬁxes can impact the rate at which we prune the search
space, and thus the total runtime. We think that it is possible to design search policies that
signiﬁcantly improve performance.

8. Conclusion and More Possible Directions for Future Work

Finally, we would like to clarify some limitations of CORELS. As far as we can tell, CORELS
is the current best algorithm for solving a specialized optimal decision tree problem. While
our approach scales well to large numbers of observations, it could have diﬃculty proving
optimality for problems with many possibly relevant features that are highly correlated,
when large regions of the search space might be challenging to exclude.

CORELS is not designed for raw image processing or other problems where the features
themselves are not interpretable. It could instead be used as a ﬁnal classiﬁer for image
processing problems where the features were created beforehand; for instance, one could
create classiﬁers for each part of an image, and use CORELS to create a ﬁnal combined
classiﬁer. The notions of interpretability used in image classiﬁcation tend to be completely
diﬀerent from those for structured data where each feature is separately meaningful (e.g.,
see Li et al., 2018). For structured data, decision trees, along with scoring systems, tend
to be popular forms of transparent models. Scoring systems are sparse linear models with
integer coeﬃcients, and they can also be created from data (Ustun and Rudin, 2017, 2016).

In some of our experiments, CORELS produces a DNF formula by coincidence, but it
might be possible to create a much simpler version of CORELS that only produces DNF
formulae. This could build oﬀ previous algorithms for creating an optimal DNF formula
(Rijnbeek and Kors, 2010; Wang et al., 2016, 2017).

CORELS does not automatically rank the subgroups in order of the likelihood of a
positive outcome; doing so would require an algorithm such as Falling Rule Lists (Wang
and Rudin, 2015a; Chen and Rudin, 2018), which forces the estimated probabilities to de-
crease along the list. Furthermore, while CORELS does not technically produce estimates
of P(Y = 1 | x), one could form such an estimate by computing the empirical proportion
ˆP(Y = 1 | x obeys pk) for each antecedent pk. CORELS is also not designed to assist with
causal inference applications, since it does not estimate the eﬀect of a treatment via the con-
ditional diﬀerence P(Y = 1 | treatment = True, x) − P(Y = 1 | treatment = False, x). Alter-
native algorithms that estimate conditional diﬀerences with interpretable rule lists include
Causal Falling Rule Lists (Wang and Rudin, 2015b), Cost-Eﬀective Interpretable Treatment
Regimes (CITR) (Lakkaraju and Rudin, 2017), and an approach by Zhang et al. (2015) for
constructing interpretable and parsimonious treatment regimes. Alternatively, one could
use a complex machine learning model to predict outcomes for the treatment group and a
separate complex model for the control group that would allow counterfactuals to be esti-
mated for each observation; from there, CORELS could be applied to produce a transparent
model for personalized treatment eﬀects. A similar approach to this was taken by Goh and
Rudin (2018), who use CORELS to understand a black box causal model.

57

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

CORELS could be adapted to handle cost-sensitive learning or weighted regularization.
This would require creating more general versions of our theorems, which would be an
extension of this work.

While CORELS does not directly handle continuous variables, we have found that it is
not diﬃcult in practice to construct a rule set that is suﬃcient for creating a useful model.
It may be possible to use techniques such as Fast Boxes (Goh and Rudin, 2014) to discover
useful and interpretable rules for continuous data that can be used within CORELS.

An interesting direction for future research would be to create a hybrid interpretable/
black box model in the style of Wang (2018), where the rule list would eliminate large
parts of the space away from the decision boundary, and the observations that remain are
evaluated by a black box model rather than a default rule.

Lastly, CORELS does not create generic single-variable-split decision trees. CORELS
optimizes over rule lists, which are one-sided decision trees; in our setting, the leaves of
these ‘trees’ are conjunctions. It may be possible to generalize ideas from our approach to
handle generic decision trees, which could be an interesting project for future work. There
are more symmetries to handle in that case, since there would be many equivalent decision
trees, leading to challenges in developing symmetry-aware data structures.

Acknowledgments

E.A. conducted most of this work while supported by the Miller Institute for Basic Research
in Science, University of California, Berkeley, and hosted by Prof. M.I. Jordan at RISELab.
C.D.R. is supported in part by MIT-Lincoln Labs and the National Science Foundation
under IIS-1053407. E.A. would like to thank E. Jonas, E. Kohler, and S. Tu for early
implementation guidance, A. D’Amour for pointing out the work by Goel et al. (2016),
V. Kanade, S. McCurdy, J. Schleier-Smith and E. Thewalt for helpful conversations, and
members of RISELab, SAIL, and the UC Berkeley Database Group for their support and
feedback. We thank H. Yang and B. Letham for sharing advice and code for processing data
and mining rules, B. Coker for his critical advice on using the ProPublica COMPAS data set,
as well as V. Kaxiras and A. Saligrama for their recent contributions to our implementation
and for creating the CORELS website. We are very grateful to our editor and anonymous
reviewers.

Appendix A. Excessive Antecedent Support

Theorem 21 (Upper bound on antecedent support) Let d∗ = (dp, δp, q0, K) be any op-
timal rule list with objective R∗, i.e., d∗ ∈ argmind R(d, x, y), and let dp = (p1, . . . , pk−1,
pk, . . . , pK) be its preﬁx. For each k ≤ K, antecedent pk in dp has support less than or equal
to the fraction of all data not captured by preceding antecedents, by an amount greater than
the regularization parameter λ:

supp(pk, x | dp) ≤ 1 − supp(dk−1

p

, x) − λ,

(37)

where dk−1
that there also exists a shorter optimal rule list d(cid:48) = (dK−1

p = (p1, . . . , pk−1). For the last antecedent, i.e., when pk = pK, equality implies

0, K − 1) ∈ argmind R(d, x, y).

p, q(cid:48)

, δ(cid:48)

p

58

Learning Certifiably Optimal Rule Lists

Proof First, we focus on the last antecedent pK+1 in a rule list d(cid:48). Let d = (dp, δp, q0, K)
be a rule list with preﬁx dp = (p1, . . . , pK) and objective R(d, x, y) ≥ R∗, where R∗ ≡
minD R(D, x, y) is the optimal objective. Let d(cid:48) = (d(cid:48)
0, K + 1) be a rule list whose
preﬁx d(cid:48)
p = (p1, . . . , pK, pK+1) starts with dp and ends with a new antecedent pK+1. Sup-
pose pK+1 in the context of d(cid:48)
p captures nearly all data not captured by dp, except for a
fraction (cid:15) upper bounded by the regularization parameter λ:

p, q(cid:48)

p, δ(cid:48)

1 − supp(dp, x) − supp(pK+1, x | d(cid:48)

p) ≡ (cid:15) ≤ λ.

Since d(cid:48)
p starts with dp, its preﬁx misclassiﬁcation error is at least as great; the only discrep-
ancy between the misclassiﬁcation errors of d and d(cid:48) can come from the diﬀerence between
the support of the set of data not captured by dp and the support of pK+1:

|(cid:96)(d(cid:48), x, y) − (cid:96)(d, x, y)| ≤ 1 − supp(dp, x) − supp(pK+1, x | d(cid:48)

p) = (cid:15).

The best outcome for d(cid:48) would occur if its misclassiﬁcation error were smaller than that
of d by (cid:15), therefore

R(d(cid:48), x, y) = (cid:96)(d(cid:48), x, y) + λ(K + 1)

≥ (cid:96)(d, x, y) − (cid:15) + λ(K + 1) = R(d, x, y) − (cid:15) + λ ≥ R(d, x, y) ≥ R∗.

d(cid:48) is an optimal rule list, i.e., d(cid:48) ∈ argminD R(D, x, y), if and only if R(d(cid:48), x, y) = R(d, x, y) =
R∗, which requires (cid:15) = λ. Otherwise, (cid:15) < λ, in which case

R(d(cid:48), x, y) ≥ R(d, x, y) − (cid:15) + λ > R(d, x, y) ≥ R∗,

therefore d(cid:48) is not optimal, i.e., d(cid:48) /∈ argminD R(D, x, y). This demonstrates the desired
result for k = K.

In the remainder, we prove the bound in (37) by contradiction, in the context of a rule
list d(cid:48)(cid:48). Let d and d(cid:48) retain their deﬁnitions from above, thus as before, that the data not
captured by d(cid:48)

p has normalized support (cid:15) ≤ λ, i.e.,

1 − supp(d(cid:48)

p, x) = 1 − supp(dp, x) − supp(pK+1, x | d(cid:48)

p) = (cid:15) ≤ λ.

Thus for any rule list d(cid:48)(cid:48) whose preﬁx d(cid:48)(cid:48)
p and ends
with one or more additional rules, each additional rule pk has support supp(pk, x | d(cid:48)(cid:48)
p) ≤
(cid:15) ≤ λ, for all k > K + 1. By Theorem 10, all of the additional rules have insuﬃcient sup-
port, therefore d(cid:48)(cid:48)

p = (p1, . . . , pK+1, . . . , pK(cid:48)) starts with d(cid:48)

p cannot be optimal, i.e., d(cid:48)(cid:48) /∈ argminD R(D, x, y).

Similar to Theorem 10, our lower bound on antecedent support, we can apply Theo-
rem 21 in the contexts of both constructing rule lists and rule mining (§3.1). Theorem 21
implies that if we only seek a single optimal rule list, then during branch-and-bound ex-
ecution, we can prune a preﬁx if we ever add an antecedent with support too similar to
the support of the set of data not captured by the preceding antecedents. One way to view
this result is that if d = (dp, δp, q0, K) and d(cid:48) = (d(cid:48)
0, K + 1) are rule lists such that d(cid:48)
p
starts with dp and ends with an antecedent that captures all or nearly all data not captured
by dp, then the new rule in d(cid:48) behaves similar to the default rule of d. As a result, the
misclassiﬁcation error of d(cid:48) must be similar to that of d, and any reduction may not be
suﬃcient to oﬀset the penalty for longer preﬁxes.

p, δ(cid:48)

p, q(cid:48)

59

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Proposition 22 (Excessive antecedent support propagates) Deﬁne φ(dp) as in (19),
and let dp = (p1, . . . , pK) be a preﬁx, such that its last antecedent pK has excessive support,
i.e., the opposite of the bound in (37):

supp(pK, x | dp) > 1 − supp(dK−1

p

, x) − λ,

p

where dK−1
= (p1, . . . , pK−1). Let D = (Dp, ∆p, Q0, κ) be any rule list with preﬁx Dp =
(P1, . . . , Pκ) such that Dp starts with DK(cid:48)−1
) and PK(cid:48) = pK. It
follows that PK(cid:48) has excessive support in preﬁx Dp, and furthermore, D /∈ argmind R(d, x, y).

= (P1, . . . , PK(cid:48)−1) ∈ φ(dK−1

p

p

Proof Since DK(cid:48)

p = (P1, . . . , PK(cid:48)) contains all the antecedents in dp, we have that

supp(DK(cid:48)

p , x) ≥ supp(dp, x).

Expanding these two terms gives

supp(DK(cid:48)

p , x) = supp(DK(cid:48)−1

p

, x) + supp(PK(cid:48), x | Dp)

≥ supp(dp, x) = supp(dK−1

p

, x) + supp(pK, x | dp) > 1 − λ.

Rearranging gives

supp(PK(cid:48), x | Dp) > 1 − supp(DK(cid:48)−1

p

, x) − λ,

thus PK(cid:48) has excessive support in Dp. By Theorem 21, D /∈ argmind R(d, x, y).

Appendix B. Proof of Theorem 15 (Equivalent Support Bound)

We begin by deﬁning four related rule lists. First, let d = (dp, δp, q0, K) be a rule list with
preﬁx dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK). Second, let D = (Dp, ∆p, Q0, κ) be
a rule list with preﬁx Dp = (P1, . . . , Pκ) that captures the same data as dp, and labels
0, K(cid:48)) ∈ σ(dp) be any rule list whose preﬁx
∆p = (Q1, . . . , Qκ). Third, let d(cid:48) = (d(cid:48)
starts with dp, such that K(cid:48) ≥ K. Denote the preﬁx and labels of d(cid:48) by d(cid:48)
p = (p1, . . . , pK,
0, κ(cid:48)) ∈
p, Q(cid:48)
p, ∆(cid:48)
pK+1, . . . , pK(cid:48)) and δp = (q1, . . . , qK(cid:48)), respectively. Finally, deﬁne D(cid:48) = (D(cid:48)
σ(Dp) to be the ‘analogous’ rule list, i.e., whose preﬁx D(cid:48)
p = (P1, . . . , Pκ, Pκ+1, . . . , Pκ(cid:48)) =
(P1, . . . , Pκ, pK+1, . . . , pK(cid:48)) starts with Dp and ends with the same K(cid:48) − K antecedents
as d(cid:48)

p. Let ∆(cid:48)
Next, we claim that the diﬀerence in the objectives of rule lists d(cid:48) and d is the same as
the diﬀerence in the objectives of rule lists D(cid:48) and D. Let us expand the ﬁrst diﬀerence as

p = (Q1, . . . , Qκ(cid:48)) denote the labels of D(cid:48).

p, δ(cid:48)

p, q(cid:48)

R(d(cid:48), x, y) − R(d, x, y) = (cid:96)(d(cid:48), x, y) + λK(cid:48) − (cid:96)(d, x, y) − λK
= (cid:96)p(d(cid:48)

p, x, y) + (cid:96)0(d(cid:48)

0, x, y) − (cid:96)p(dp, δp, x, y) − (cid:96)0(dp, q0, x, y) + λ(K(cid:48) − K).

p, q(cid:48)

p, δ(cid:48)

Similarly, let us expand the second diﬀerence as

R(D(cid:48), x, y) − R(D, x, y) = (cid:96)(D(cid:48), x, y) + λκ(cid:48) − (cid:96)(D, x, y) − λκ
= (cid:96)p(D(cid:48)

p, x, y) + (cid:96)0(D(cid:48)

p, ∆(cid:48)

p, Q(cid:48)

0, x, y) − (cid:96)p(Dp, ∆p, x, y) − (cid:96)0(Dp, Q0, x, y) + λ(K(cid:48) − K),

60

Learning Certifiably Optimal Rule Lists

where we have used the fact that κ(cid:48) − κ = K(cid:48) − K.

The preﬁxes dp and Dp capture the same data. Equivalently, the set of data that is not

captured by dp is the same as the set of data that is not captured by Dp, i.e.,

{xn : ¬ cap(xn, dp)} = {xn : ¬ cap(xn, Dp)}.

Thus, the corresponding rule lists d and D share the same default rule, i.e., q0 = Q0, yielding
the same default rule misclassiﬁcation error:

Similarly, preﬁxes d(cid:48)
same default rule misclassiﬁcation error:

p and D(cid:48)

p capture the same data, and thus rule lists d(cid:48) and D(cid:48) have the

(cid:96)0(dp, q0, x, y) = (cid:96)0(Dp, Q0, x, y).

(cid:96)0(dp, q0, x, y) = (cid:96)0(Dp, Q0, x, y).

At this point, to demonstrate our claim relating the objectives of d, d(cid:48), D, and D(cid:48), what
p and dp is

remains is to show that the diﬀerence in the misclassiﬁcation errors of preﬁxes d(cid:48)
the same as that between D(cid:48)

p and Dp. We can expand the ﬁrst diﬀerence as

(cid:96)p(d(cid:48)

p, δ(cid:48)

p, x, y) − (cid:96)p(dp, δp, x, y) =

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn],

where we have used the fact that since d(cid:48)
p starts with dp, the ﬁrst K rules in d(cid:48)
same mistakes as those in dp. Similarly, we can expand the second diﬀerence as

p make the

(cid:96)p(D(cid:48)

p, ∆(cid:48)

p, x, y) − (cid:96)p(Dp, ∆p, x, y) =

cap(xn, Pk | D(cid:48)

p) ∧ 1[Qk (cid:54)= yn]

cap(xn, pk | D(cid:48)

p) ∧ 1[Qk (cid:54)= yn]

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn]

(38)

n=1
p, δ(cid:48)

k=K+1
p, x, y) − (cid:96)p(dp, δp, x, y).

= (cid:96)p(d(cid:48)

To justify the equality in (38), we observe ﬁrst that preﬁxes D(cid:48)
p start with κ and K
antecedents, respectively, that capture the same data. Second, preﬁxes D(cid:48)
p end with
exactly the same ordered list of K(cid:48) − K antecedents, therefore for any k = 1, . . . , K(cid:48) − K,
antecedent Pκ+k = pK+k in D(cid:48)
p. It follows that
the corresponding labels are all equivalent, i.e., Qκ+k = qK+k, for all k = 1, . . . , K(cid:48) − K, and
consequently, the preﬁx misclassiﬁcation error associated with the last K(cid:48) − K antecedents
of d(cid:48)
p. We have therefore shown that the diﬀerence between the
objectives of d(cid:48) and d is the same as that between D(cid:48) and D, i.e.,

p captures the same data as pK+k captures in d(cid:48)

p is the same as that of D(cid:48)

p and d(cid:48)

p and d(cid:48)

R(d(cid:48), x, y) − R(d, x, y) = R(D(cid:48), x, y) − R(D, x, y).

(39)

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K+1

N
(cid:88)

κ(cid:48)
(cid:88)

n=1

N
(cid:88)

k=κ+1
K(cid:48)
(cid:88)

n=1

N
(cid:88)

k=K+1
K(cid:48)
(cid:88)

1
N

1
N

1
N

=

=

61

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Next, suppose that the objective lower bounds of d and D obey b(dp, x, y) ≤ b(Dp, x, y),

therefore

R(d, x, y) = (cid:96)p(dp, δp, x, y) + (cid:96)0(dp, q0, x, y) + λK
= b(dp, x, y) + (cid:96)0(dp, q0, x, y)
≤ b(Dp, x, y) + (cid:96)0(dp, q0, x, y) = b(Dp, x, y) + (cid:96)0(Dp, Q0, x, y) = R(D, x, y).

Now let d∗ be an optimal rule list with preﬁx constrained to start with dp,

d∗ ∈ argmin
d†∈σ(dp)

R(d†, x, y),

and let K∗ be the length of d∗. Let D∗ be the analogous κ∗-rule list whose preﬁx starts
with Dp and ends with the same K∗ − K antecedents as d∗, where κ∗ = κ + K∗ − K.
By (39),

R(d∗, x, y) − R(d, x, y) = R(D∗, x, y) − R(D, x, y).

Furthermore, we claim that D∗ is an optimal rule list with preﬁx constrained to start
with Dp,

(40)

(41)

(42)

D∗ ∈ argmin
D†∈σ(Dp)

R(D†, x, y).

To demonstrate (42), we consider two separate scenarios. In the ﬁrst scenario, preﬁxes dp
and Dp are composed of the same antecedents, i.e., the two preﬁxes are equivalent up to a
permutation of their antecedents, and as a consequence, κ = K and κ∗ = K∗. Here, every
rule list d(cid:48)(cid:48) ∈ σ(dp) that starts with dp has an analogue D(cid:48)(cid:48) ∈ σ(Dp) that starts with Dp,
such that d(cid:48)(cid:48) and D(cid:48)(cid:48) obey (39), and vice versa, and thus (42) is a direct consequence of (41).
In the second scenario, preﬁxes dp and Dp are not composed of the same antecedents.
Deﬁne φ = {pk : (pk ∈ dp) ∧ (pk /∈ Dp)} to be the set of antecedents in dp that are not in Dp,
and deﬁne Φ = {Pk : (Pk ∈ Dp) ∧ (Pk /∈ dp)} to be the set of antecedents in Dp that are not
in dp; either φ (cid:54)= ∅, or Φ (cid:54)= ∅, or both.

Suppose φ (cid:54)= ∅, and let p ∈ φ be an antecedent in φ. It follows that there exists a subset
of rule lists in σ(Dp) that do not have analogues in σ(dp). Let D(cid:48)(cid:48) ∈ σ(Dp) be such a rule
list, such that its preﬁx D(cid:48)(cid:48)
p = (P1, . . . , Pκ, . . . , p, . . . ) starts with Dp and contains p among
its remaining antecedents. Since p captures a subset of the data that dp captures, and Dp
captures the same data as dp, it follows that p does not capture any data in D(cid:48)(cid:48)

p , i.e.,

1
N

N
(cid:88)

n=1

cap(xn, p | D(cid:48)(cid:48)

p ) = 0 ≤ λ.

By Theorem 10, antecedent p has insuﬃcient support in D(cid:48)(cid:48), and thus D(cid:48)(cid:48) cannot be op-
timal, i.e., D(cid:48)(cid:48) /∈ argminD†∈σ(Dp) R(D†, x, y). By a similar argument, if Φ (cid:54)= ∅ and P ∈ Φ,
and d(cid:48)(cid:48) ∈ σ(dp) is any rule list whose preﬁx starts with dp and contains antecedent P , then d(cid:48)(cid:48)
cannot be optimal, i.e., d(cid:48)(cid:48) /∈ argmind†∈σ(dp) R(d†, x, y).

62

Learning Certifiably Optimal Rule Lists

To ﬁnish justifying claim (42) for the second scenario, ﬁrst deﬁne

τ (dp, Φ) ≡ {d(cid:48)(cid:48) = (d(cid:48)(cid:48)

p, δ(cid:48)(cid:48)

p , q(cid:48)(cid:48)

0 , K(cid:48)(cid:48)) : d(cid:48)(cid:48) ∈ σ(dp) and pk /∈ Φ, ∀pk ∈ d(cid:48)(cid:48)

p} ⊂ σ(dp)

to be the set of all rule lists whose preﬁxes start with dp and do not contain any antecedents
in Φ. Now, recognize that the optimal preﬁxes in τ (dp, Φ) and σ(dp) are the same, i.e.,

argmin
d†∈τ (dp,Φ)

R(d†, x, y) = argmin
d†∈σ(dp)

R(d†, x, y),

and similarly, the optimal preﬁxes in τ (Dp, φ) and σ(Dp) are the same, i.e.,

argmin
D†∈τ (Dp,φ)

R(D†, x, y) = argmin
D†∈σ(Dp)

R(D†, x, y).

Since we have shown that every d(cid:48)(cid:48) ∈ τ (dp, Φ) has a direct analogue D(cid:48)(cid:48) ∈ τ (Dp, φ), such
that d(cid:48)(cid:48) and D(cid:48)(cid:48) obey (39), and vice versa, we again have (42) as a consequence of (41).
We can now ﬁnally combine (40) and (42) to obtain the desired inequality in (21):

min
d(cid:48)∈σ(dp)

R(d(cid:48), x, y) = R(d∗, x, y) ≤ R(D∗, x, y) = min

R(D(cid:48), x, y).

D(cid:48)∈σ(Dp)

Appendix C. Proof of Theorem 18 (Similar Support Bound)

We begin by deﬁning four related rule lists. First, let d = (dp, δp, q0, K) be a rule list with
preﬁx dp = (p1, . . . , pK) and labels δp = (q1, . . . , qK). Second, let D = (Dp, ∆p, Q0, κ) be a
rule list with preﬁx Dp = (P1, . . . , Pκ) and labels ∆p = (Q1, . . . , Qκ). Deﬁne ω as in (22)
and Ω as in (23), and require that ω, Ω ≤ λ. Third, let d(cid:48) = (d(cid:48)
0, K(cid:48)) ∈ σ(dp) be
any rule list whose preﬁx starts with dp, such that K(cid:48) ≥ K. Denote the preﬁx and la-
bels of d(cid:48) by d(cid:48)
p = (p1, . . . , pK, pK+1, . . . , pK(cid:48)) and δp = (q1, . . . , qK(cid:48)), respectively. Finally,
p, ∆(cid:48)
deﬁne D(cid:48) = (D(cid:48)
p =
(P1, . . . , Pκ, Pκ+1, . . . , Pκ(cid:48)) = (P1, . . . , Pκ, pK+1, . . . , pK(cid:48)) starts with Dp and ends with the
same K(cid:48) − K antecedents as d(cid:48)

0, κ(cid:48)) ∈ σ(Dp) to be the ‘analogous’ rule list, i.e., whose preﬁx D(cid:48)

p = (Q1, . . . , Qκ(cid:48)) denote the labels of D(cid:48).

p. Let ∆(cid:48)

p, Q(cid:48)

The smallest possible objective for D(cid:48), in relation to the objective of d(cid:48), reﬂects both
the diﬀerence between the objective lower bounds of D and d and the largest possible
discrepancy between the objectives of d(cid:48) and D(cid:48). The latter would occur if d(cid:48) misclassiﬁed
all the data corresponding to both ω and Ω while D(cid:48) correctly classiﬁed this same data,
thus

p, δ(cid:48)

p, q(cid:48)

R(D(cid:48), x, y) ≥ R(d(cid:48), x, y) + b(Dp, x, y) − b(dp, x, y) − ω − Ω.

(43)

Now let D∗ be an optimal rule list with preﬁx constrained to start with Dp,

and let κ∗ be the length of D∗. Also let d∗ be the analogous K∗-rule list whose preﬁx starts
with dp and ends with the same κ∗ − κ antecedents as D∗, where K∗ = K + κ∗ − κ. By (43),

D∗ ∈ argmin
D†∈σ(Dp)

R(D†, x, y),

63

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

we obtain the desired inequality in (24):

min
D†∈σ(Dp)

R(D†, x, y) = R(D∗, x, y)

≥ R(d∗, x, y) + b(Dp, x, y) − b(dp, x, y) − ω − Ω
≥ min

R(d†, x, y) + b(Dp, x, y) − b(dp, x, y) − ω − Ω.

d†∈σ(dp)

Appendix D. Proof of Theorem 20 (Equivalent Points Bound)

We derive a lower bound on the default rule misclassiﬁcation error (cid:96)0(dp, q0, x, y), analogous
to the lower bound (26) on the misclassiﬁcation error (cid:96)(d, x, y) in the proof of Proposition 19.
As before, we sum over all sets of equivalent points, and then for each such set, we count
diﬀerences between class labels and the minority class label of the set, instead of counting
mistakes made by the default rule:

(cid:96)0(dp, q0, x, y) =

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn]

1
N

1
N

1
N

N
(cid:88)

n=1
U
(cid:88)

N
(cid:88)

u=1
U
(cid:88)

n=1
N
(cid:88)

u=1

n=1

=

≥

¬ cap(xn, dp) ∧ 1[q0 (cid:54)= yn] 1[xn ∈ eu]

¬ cap(xn, dp) ∧ 1[yn = qu] 1[xn ∈ eu] = b0(dp, x, y),

(44)

where the ﬁnal equality comes from the deﬁnition of b0(dp, x, y) in (28). Since we can write
the objective R(d, x, y) as the sum of the objective lower bound b(dp, x, y) and default rule
misclassiﬁcation error (cid:96)0(dp, q0, x, y), applying (44) gives a lower bound on R(d, x, y):

R(d, x, y) = (cid:96)p(dp, δp, x, y) + (cid:96)0(dp, q0, x, y) + λK = b(dp, x, y) + (cid:96)0(dp, q0, x, y)

≥ b(dp, x, y) + b0(dp, x, y).

(45)

It follows that for any rule list d(cid:48) ∈ σ(d) whose preﬁx d(cid:48)

p starts with dp, we have

R(d(cid:48), x, y) ≥ b(d(cid:48)

p, x, y) + b0(d(cid:48)

p, x, y).

(46)

Finally, we show that the lower bound on R(d, x, y) in (45) is not greater than the lower

bound on R(d(cid:48), x, y) in (46). First, let us deﬁne

Υ(d(cid:48)

p, K, x, y) ≡

cap(xn, pk | d(cid:48)

p) ∧ 1[xn ∈ eu] 1[yn = qu].

(47)

1
N

U
(cid:88)

N
(cid:88)

K(cid:48)
(cid:88)

u=1

n=1

k=K+1

64

Learning Certifiably Optimal Rule Lists

Now, we write a lower bound on b(d(cid:48)

p, x, y) with respect to b(dp, x, y):

b(d(cid:48)

p, x, y) = (cid:96)p(d(cid:48)

p, δp, x, y) + λK(cid:48) =

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] + λK(cid:48)

= (cid:96)p(dp, δp, x, y) + λK +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] + λ(K(cid:48) − K)

= b(dp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] + λ(K(cid:48) − K)

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=1

1
N

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K

N
(cid:88)

K(cid:48)
(cid:88)

n=1

k=K+1

U
(cid:88)

N
(cid:88)

K(cid:48)
(cid:88)

u=1

n=1

U
(cid:88)

N
(cid:88)

k=K+1
K(cid:48)
(cid:88)

1
N

1
N

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

1
N

(cid:32)

= b(dp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[qk (cid:54)= yn] 1[xn ∈ eu] + λ(K(cid:48) − K)

≥ b(dp, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[yn = qu] 1[xn ∈ eu] + λ(K(cid:48) − K)

= b(dp, x, y) + Υ(d(cid:48)

n=1

k=K+1

u=1
p, K, x, y) + λ(K(cid:48) − K),

(48)

where the last equality uses (47). Next, we write b0(dp, x, y) with respect to b0(d(cid:48)

p, x, y),

b0(dp, x, y) =

¬ cap(xn, dp) ∧ 1[xn ∈ eu] 1[yn = qu]

=

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

¬ cap(xn, d(cid:48)

p) +

cap(xn, pk | d(cid:48)
p)

∧ 1[xn ∈ eu] 1[yn = qu]

K(cid:48)
(cid:88)

k=K+1

(cid:33)

= b0(d(cid:48)

p, x, y) +

cap(xn, pk | d(cid:48)

p) ∧ 1[xn ∈ eu] 1[yn = qu].

(49)

1
N

U
(cid:88)

N
(cid:88)

K(cid:48)
(cid:88)

u=1

n=1

k=K+1

Rearranging (49) gives

b0(d(cid:48)

p, x, y) = b0(dp, x, y) − Υ(d(cid:48)

p, K, x, y).

(50)

Combining (46) with ﬁrst (50) and then (48) gives the desired inequality in (27):

p, x, y)

p, x, y) + b0(d(cid:48)
p, x, y) + b0(dp, x, y) − Υ(d(cid:48)

R(d(cid:48), x, y) ≥ b(d(cid:48)
= b(d(cid:48)
≥ b(dp, x, y) + Υ(d(cid:48)
= b(dp, x, y) + b0(dp, x, y) + λ(K(cid:48) − K) ≥ b(dp, x, y) + b0(dp, x, y).

p, K, x, y) + λ(K(cid:48) − K) + b0(dp, x, y) − Υ(d(cid:48)

p, K, x, y)

p, K, x, y)

Appendix E. Data Processing Details and Antecedent Mining

In this appendix, we provide details regarding datasets used in our experiments (Section 6).

65

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

E.1 ProPublica Recidivism Data Set

Table 7 shows the 6 attributes and corresponding 17 categorical values that we use for the
ProPublica data set. From these, we construct 17 single-clause antecedents, for example,
(age = 23 − 25). We then combine pairs of these antecedents as conjunctions to form two-
clause antecedents, e.g., (age = 23 − 25) ∧ (priors = 2 − 3). By virtue of our lower bound
on antecedent support, (Theorem 10, §3.7), we eliminate antecedents with support less
than 0.005 or greater than 0.995, since λ = 0.005 is the smallest regularization parameter
value we study for this problem. With this ﬁltering step, we generate between 121 and 123
antecedents for each fold; without it, we would instead generate about 130 antecedents as
input to our algorithm.

Note that we exclude the ‘current charge’ attribute (which has two categorical values,
‘misdemeanor’ and ‘felony’); for individuals in the data set booked on multiple charges, this
attribute does not appear to consistently reﬂect the most serious charge.

Feature
sex
age
juvenile felonies
juvenile misdemeanors
juvenile crimes
priors

Value range
—
18-96
0-20
0-13
0-21
0-38

Categorical values
male, female
18-20, 21-22, 23-25, 26-45, >45
0, >0
0, >0
0, >0
0, 1, 2-3, >3

Count
2
5
2
2
2
4

Table 7: Categorical features (6 attributes, 17 values) from the ProPublica data set. We
construct the feature juvenile crimes from the sum of juvenile felonies, juvenile
misdemeanors, and the number of juvenile crimes that were neither felonies nor
misdemeanors (not shown).

E.2 NYPD Stop-and-frisk Data Set

This data set is larger than, but similar to the NYCLU stop-and-frisk data set, described
next.

E.3 NYCLU Stop-and-frisk Data Set

The original data set contains 45,787 records, each describing an incident involving a stopped
person; the individual was frisked in 30,345 (66.3%) of records and and searched in 7,283
(15.9%). In 30,961 records, the individual was frisked and/or searched (67.6%); of those,
a criminal possession of a weapon was identiﬁed 1,445 times (4.7% of these records). We
remove 1,929 records with missing data, as well as a small number with extreme values
for the individual’s age—we eliminate those with age < 12 or > 89. This yields a set of
29,595 records in which the individual was frisked and/or searched. To address the class
imbalance for this problem, we sample records from the smaller class with replacement. We
generate cross-validation folds ﬁrst, and then resample within each fold. In our 10-fold cross-
validation experiments, each training set contains 50,743 observations. Table 8 shows the 5

66

Learning Certifiably Optimal Rule Lists

categorical attributes that we use, corresponding to a total of 28 values. Our experiments
use these antecedents, as well as negations of the 18 antecedents corresponding to the two
features stop reason and additional circumstances, which gives a total of 46 antecedents.

Feature
stop reason

additional
circumstances

city
location

inside or outside

Values
suspicious object, ﬁts description, casing,
acting as lookout, suspicious clothing,
drug transaction, furtive movements,
actions of violent crime, suspicious bulge
proximity to crime scene, evasive response,
associating with criminals, changed direction,
high crime area, time of day,
sights and sounds of criminal activity,
witness report, ongoing investigation
Queens, Manhattan, Brooklyn, Staten Island, Bronx
housing authority, transit authority,
neither housing nor transit authority
inside, outside

Count
9

9

5
3

2

Table 8: Categorical features (5 attributes, 28 values) from the NYCLU data set.

Appendix F. Example Optimal Rule Lists, for Diﬀerent Values of λ

For each of our prediction problems, we provide listings of optimal rule lists found by
CORELS, across 10 cross-validation folds, for diﬀerent values of the regularization param-
eter λ. These rule lists correspond to the results for CORELS summarized in Figures 11
and 12 (§6.6). Recall that as λ decreases, optimal rule lists tend to grow in length.

F.1 ProPublica Recidivism Data Set

We show example optimal rule lists that predict two-year recidivism. Figure 19 shows exam-
ples for regularization parameters λ = 0.02 and 0.01. Figure 20 shows examples for λ = 0.005;
Figure 4 (§6.3) showed two representative examples.

For the largest regularization parameter λ = 0.02 (Figure 19), we observe that all folds
identify the same length-1 rule list. For the intermediate value λ = 0.01 (Figure 19), the
folds identify optimal 2-rule or 3-rule lists that contain the nearly same preﬁx rules, up to
permutations. For the smallest value λ = 0.005 (Figure 20), the folds identify optimal 3-rule
or 4-rule lists that contain the nearly same preﬁx rules, up to permutations. Across all three
regularization parameter values and all folds, the preﬁx rules always predict the positive
class label, and the default rule always predicts the negative class label. We note that our
objective is not designed to enforce any of these properties.

67

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Two-year recidivism prediction (λ = 0.02)

if (priors > 3) then predict yes
else predict no

Two-year recidivism prediction (λ = 0.01)

(cid:46) Found by all 10 folds

if (priors > 3) then predict yes
else if (sex = male) and (juvenile crimes > 0) then predict yes
else predict no

(cid:46) Found by 3 folds

if (sex = male) and (juvenile crimes > 0) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (priors > 3) then predict yes
else predict no

if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

(cid:46) Found by 2 folds

(cid:46) Found by 2 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

Figure 19: Example optimal rule lists for the ProPublica data set, found by CORELS with
regularization parameters λ = 0.02 (top), and 0.01 (bottom) across 10 cross-
validation folds.

68

Learning Certifiably Optimal Rule Lists

Two-year recidivism prediction (λ = 0.005)

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (priors > 3) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no

if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else predict no

if (age = 21 − 22) and (priors = 2 − 3) then predict yes
else if (age = 23 − 25) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else if (age = 18 − 20) and (sex = male) then predict yes
else predict no

(cid:46) Found by 4 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

Figure 20: Example optimal rule lists for the ProPublica data set, found by CORELS with
regularization parameters λ = 0.005, across 10 cross-validation folds.

69

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

F.2 NYPD Stop-and-frisk Data Set

We show example optimal rule lists that predict whether a weapon will be found on a
stopped individual who is frisked or searched, learned from the NYPD data set.

Weapon prediction (λ = 0.01, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else predict no

if (location = transit authority) then predict yes
else if (stop reason = suspicious object) then predict yes
else predict no

Weapon prediction (λ = 0.005, Feature Set C)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (location = housing authority) then predict no
else if (location = transit authority) then predict yes
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else if (location = transit authority) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (city = Bronx) then predict no
else if (location = housing authority) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

(cid:46) Found by 8 folds

(cid:46) Found by 2 folds

(cid:46) Found by 7 folds

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

Figure 21: Example optimal rule lists for the NYPD stop-and-frisk data set, found by
CORELS with regularization parameters λ = 0.01 (top) and 0.005 (bottom),
across 10 cross-validation folds.

70

Learning Certifiably Optimal Rule Lists

Weapon prediction (λ = 0.01, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (inside or outside = outside) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (inside or outside = inside) then predict yes
else predict no

Weapon prediction (λ = 0.005, Feature Set D)

if (stop reason = suspicious object) then predict yes
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = f urtive movements) then predict no
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f its description) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f urtive movements) then predict no
else if (stop reason = f its description) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = f its description) then predict no
else if (stop reason = acting as lookout) then predict no
else if (stop reason = f urtive movements) then predict no
else predict yes

if (stop reason = suspicious object) then predict yes
else if (stop reason = f urtive movements) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = acting as lookout) then predict no
else predict yes

(cid:46) Found by 7 folds

(cid:46) Found by 3 folds

(cid:46) Found by 2 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

(cid:46) Found by 1 fold

Figure 22: Example optimal rule lists for the NYPD stop-and-frisk data set (Feature Set D)
found by CORELS with regularization parameters λ = 0.01 (top) and 0.005 (bot-
tom), across 10 cross-validation folds. For λ = 0.005, we show results from 7 folds;
the remaining 3 folds were equivalent, up to a permutation of the preﬁx rules,
and started with the same ﬁrst preﬁx rule.

71

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

F.3 NYCLU Stop-and-frisk Data Set

We show example optimal rule lists that predict whether a weapon will be found on a
stopped individual who is frisked or searched, learned from the NYCLU data set. Figure 23
shows regularization parameters λ = 0.04 and 0.01, and Figure 24 shows λ = 0.0025. We
showed a representative solution for λ = 0.01 in Figure 5 (§6.3).

For each of the two larger regularization parameters in Figure 23, λ = 0.04 (top) and
0.01 (bottom), we observe that across the folds, all the optimal rule lists contain the same or
equivalent rules, up to a permutation. With the smaller regularization parameter λ = 0.0025
(Figure 24), we observe a greater diversity of longer optimal rule lists, though they share
similar structure.

Weapon prediction (λ = 0.04)

if (stop reason = suspicious object) then predict yes
else if (stop reason (cid:54)= suspicious bulge) then predict no
else predict yes

if (stop reason = suspicious bulge) then predict yes
else if (stop reason (cid:54)= suspicious object) then predict no
else predict yes

Weapon prediction (λ = 0.01)

if (stop reason = suspicious object) then predict yes
else if (location = transit authority) then predict yes
else if (stop reason (cid:54)= suspicious bulge) then predict no
else predict yes

if (location = transit authority) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = suspicious object) then predict yes
else predict no

if (location = transit authority) then predict yes
else if (stop reason = suspicious object) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else predict no

if (location = transit authority) then predict yes
else if (stop reason = suspicious object) then predict yes
else if (stop reason (cid:54)= suspicious bulge) then predict no
else predict yes

(cid:46) Found by 7 folds

(cid:46) Found by 3 folds

(cid:46) Found by 4 folds

(cid:46) Found by 3 folds

(cid:46) Found by 2 folds

(cid:46) Found by 1 fold

Figure 23: Example optimal rule lists for the NYCLU stop-and-frisk data set, found by
CORELS with regularization parameters λ = 0.04 (top) and 0.01 (bottom),
across 10 cross-validation folds.

72

Learning Certifiably Optimal Rule Lists

Weapon prediction (λ = 0.0025)

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = transit authority) then predict yes
else if (inside or outside = inside) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (location = housing authority) then predict no
else if (stop reason = casing) then predict no
else if (stop reason = f its description) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = housing authority) then predict no
else if (city = M anhattan) then predict yes
else predict no

if (stop reason = drug transaction) then predict no
else if (stop reason = suspicious object) then predict yes
else if (stop reason = suspicious bulge) then predict yes
else if (location = housing authority) then predict no
else if (stop reason = f its description) then predict no
else if (stop reason = casing) then predict no
else if (city = M anhattan) then predict yes
else if (city = Bronx) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = f its description) then predict no
else if (location = transit authority) then predict yes
else if (inside or outside = inside) then predict no
else if (city = M anhattan) then predict yes
else if (additional circumstances = changed direction) then predict no
else if (city = Bronx) then predict yes
else predict no

if (stop reason = suspicious object) then predict yes
else if (stop reason = casing) then predict no
else if (stop reason = suspicious bulge) then predict yes
else if (stop reason = actions of violent crime) then predict no
else if (stop reason = f its description) then predict no
else if (location = transit authority) then predict yes
else if (inside or outside = inside) then predict no
else if (city = M anhattan) then predict yes
else if (additional circumstances = evasive response) then predict no
else if (city = Bronx) then predict yes
else predict no

(cid:46) Found by 4 folds (K = 7)

(cid:46) Found by 1 fold (K = 6)

(cid:46) Found by 1 fold (K = 6)

(cid:46) Found by 1 fold (K = 6)

(cid:46) Found by 1 fold (K = 8)

(cid:46) Found by 1 fold (K = 9)

(cid:46) Found by 1 fold (K = 10)

Figure 24: Example optimal rule lists for the NYCLU stop-and-frisk data set λ = 0.0025.

73

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

Appendix G. Additional Results on Predictive Performance and Model

Size for CORELS and Other Algorithms

In this appendix, we plot TPR, FPR, and model size for CORELS and three other algo-
rithms, using the NYPD data set (Feature Set D).

Figure 25: TPR (top) and FPR (bottom) for the test set, as a function of model size,
across diﬀerent methods, for weapon prediction with the NYPD stop-and-frisk
data set (Feature Set D). In the legend, numbers in parentheses are algorithm
parameters, as in Figure 12. Legend markers and error bars indicate means and
standard deviations, respectively, across cross-validation folds. C4.5 ﬁnds large
models for all tested parameters.

References

E. Angelino, N. Larus-Stone, D. Alabi, M. Seltzer, and C. Rudin. Learning certiﬁably
optimal rule lists for categorical data. In ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD), 2017.

K. P. Bennett and J. A. Blue. Optimal decision trees. Technical report, R.P.I. Math Report

No. 214, Rensselaer Polytechnic Institute, 1996.

I. Bratko. Machine learning: Between accuracy and interpretability. In Learning, Networks
and Statistics, volume 382 of International Centre for Mechanical Sciences, pages 163–

74

Learning Certifiably Optimal Rule Lists

177. Springer Vienna, 1997.

Trees. Wadsworth, 1984.

2013.

L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and Regression

S. Bushway. Is there any logic to using logit. Criminology & Public Policy, 12(3):563–567,

C. Chen and C. Rudin. An optimization approach to learning falling rule lists. In Interna-

tional Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2018.

H. A. Chipman, E. I. George, and R. E. McCulloch. Bayesian CART model search. Journal

of the American Statistical Association, 93(443):935–948, 1998.

H. A. Chipman, E. I. George, and R. E. McCulloch. Bayesian treed models. Machine

Learning, 48(1):299–320, 2002.

H. A. Chipman, E. I. George, and R. E. McCulloch. BART: Bayesian additive regression

trees. The Annals of Applied Statistics, 4(1):266–298, 2010.

P. Clark and T. Niblett. The CN2 induction algorithm. Machine Learning, 3:261–283, 1989.

W. W. Cohen. Fast eﬀective rule induction.

In International Conference on Machine

Learning (ICML), pages 115–123, 1995.

R. M. Dawes. The robust beauty of improper linear models in decision making. American

Psychologist, 34(7):571–582, 1979.

D. Dension, B. Mallick, and A.F.M. Smith. A Bayesian CART algorithm. Biometrika, 85

(2):363–377, 1998.

trees, 1996.

Advances, 4(1), 2018.

D. Dobkin, T. Fulton, D. Gunopulos, S. Kasif, and S. Salzberg. Induction of shallow decision

J. Dressel and H. Farid. The accuracy, fairness, and limits of predicting recidivism. Science

A. Farhangfar, R. Greiner, and M. Zinkevich. A fast way to produce optimal ﬁxed-depth
decision trees. In International Symposium on Artiﬁcial Intelligence and Mathematics
(ISAIM), 2008.

E. Frank and I. H. Witten. Generating accurate rule sets without global optimization. In

International Conference on Machine Learning (ICML), pages 144–151, 1998.

A. A. Freitas. Comprehensible classiﬁcation models: A position paper. ACM SIGKDD

Explorations Newsletter, 15(1):1–10, 2014.

M. Garofalakis, D. Hyun, R. Rastogi, and K. Shim. Eﬃcient algorithms for constructing
decision trees with constraints. In ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD), pages 335–339, 2000.

75

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

C. Giraud-Carrier. Beyond predictive accuracy: What? In ECML-98 Workshop on Up-
grading Learning to Meta-Level: Model Selection and Data Transformation, pages 78–85,
1998.

S. Goel, J. M. Rao, and R. Shroﬀ. Precinct or prejudice? Understanding racial disparities in
New York City’s stop-and-frisk policy. The Annals of Applied Statistics, 10(1):365–394,
03 2016.

S. T. Goh and C. Rudin. Box drawings for learning with imbalanced data. In ACM SIGKDD

International Conference on Knowledge Discovery and Data Mining (KDD), 2014.

S. T. Goh and C. Rudin. A minimax surrogate loss approach to conditional diﬀerence
estimation. CoRR, abs/1803.03769, 2018. URL https://arxiv.org/abs/1803.03769.

B. Goodman and S. Flaxman. European Union regulations on algorithmic decision-making
and a “right to explanation”. In ICML Workshop on Human Interpretability in Machine
Learning (WHI), 2016.

R. C. Holte. Very simple classiﬁcation rules perform well on most commonly used datasets.

Machine Learning, 11(1):63–91, 1993.

J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, and B. Baesens. An empirical evaluation
of the comprehensibility of decision table, tree and rule based predictive models. Decision
Support Systems, 51(1):141–154, 2011.

V. Kaxiras and A. Saligrama. Building predictive models with rule lists, 2018. URL https:

//corels.eecs.harvard.edu.

H. Lakkaraju and C. Rudin. Cost-sensitive and interpretable dynamic treatment regimes
based on rule lists. In International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS), 2017.

J. Larson, S. Mattu, L. Kirchner, and J. Angwin. How we analyzed the COMPAS recidivism

algorithm. ProPublica, 2016.

N. Larus-Stone, E. Angelino, D. Alabi, M. Seltzer, V. Kaxiras, A. Saligrama, and C. Rudin.
Systems optimizations for learning certiﬁably optimal rule lists. In SysML Conference,
2018.

N. L. Larus-Stone. Learning Certiﬁably Optimal Rule Lists: A Case For Discrete Optimiza-

tion in the 21st Century. 2017. Undergraduate thesis, Harvard College.

B. Letham, C. Rudin, T. H. McCormick, and D. Madigan. Interpretable classiﬁers using
rules and Bayesian analysis: Building a better stroke prediction model. The Annals of
Applied Statistics, 9(3):1350–1371, 2015.

O. Li, H. Liu, C. Chen, and C. Rudin. Deep learning for case-based reasoning through pro-
totypes: A neural network that explains its predictions. In Proceedings of the Association
for the Advancement of Artiﬁcial Intelligence (AAAI), 2018.

76

Learning Certifiably Optimal Rule Lists

W. Li, J. Han, and J. Pei. CMAR: Accurate and eﬃcient classiﬁcation based on multiple
class-association rules. IEEE International Conference on Data Mining (ICDM), pages
369–376, 2001.

J. T. Linderoth and M. W. P. Savelsbergh. A computational study of search strategies for
mixed integer programming. INFORMS Journal on Computing, 11(2):173–187, 1999.

B. Liu, W. Hsu, and Y. Ma. Integrating classiﬁcation and association rule mining. In ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),
pages 80–96, 1998.

M. Marchand and M. Sokolova. Learning with decision lists of data-dependent features.

Journal of Machine Learning Research, 6:427–451, 2005.

R. S. Michalski. On the quasi-minimal solution of the general covering problem. In Inter-

national Symposium on Information Processing, pages 125–128, 1969.

New York Civil Liberties Union. Stop-and-frisk data, 2014. URL http://www.nyclu.org/

content/stop-and-frisk-data.

New York Police Department. Stop, question and frisk data, 2016. URL http://www1.

nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page.

S. Nijssen and E. Fromont. Optimal constraint-based decision tree induction from itemset

lattices. Data Mining and Knowledge Discovery, 21(1):9–51, 2010.

J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1993.

P. R. Rijnbeek and J. A. Kors. Finding a short and accurate decision rule in disjunctive

normal form by exhaustive search. Machine Learning, 80(1):33–62, July 2010.

R. L. Rivest. Learning decision lists. Machine Learning, 2(3):229–246, November 1987.

U. R¨uckert and L. De Raedt. An experimental evaluation of simplicity in rule learning.

Artiﬁcial Intelligence, 172:19–28, 2008.

C. Rudin and S¸. Ertekin. Learning customized and optimized lists of rules with mathemat-

ical programming. Submitted, 2016.

C. Rudin, B. Letham, and D. Madigan. Learning theory analysis for association rules and
sequential event prediction. Journal of Machine Learning Research, 14:3384–3436, 2013.

S. R¨uping. Learning interpretable models. PhD thesis, Universit¨at Dortmund, 2006.

G. Shmueli. To explain or to predict? Statistical Science, 25(3):289–310, August 2010.

M. Sokolova, M. Marchand, N. Japkowicz, and J. Shawe-Taylor. The decision list machine.
In Advances in Neural Information Processing Systems (NIPS), volume 15, pages 921–
928, 2003.

77

Angelino, Larus-Stone, Alabi, Seltzer, and Rudin

N. Tollenaar and P. van der Heijden. Which method predicts recidivism best?: A comparison
of statistical, machine learning and data mining predictive models. Journal of the Royal
Statistical Society: Series A (Statistics in Society), 176(2):565–584, 2013.

B. Ustun and C. Rudin. Supersparse linear integer models for optimized medical scoring

systems. Machine Learning, 102(3):349–391, 2016.

B. Ustun and C. Rudin. Optimized risk scores. In ACM SIGKDD International Conference

on Knowledge Discovery and Data Mining (KDD), 2017.

K. Vanhoof and B. Depaire. Structure of association rule classiﬁers: A review. In Inter-
national Conference on Intelligent Systems and Knowledge Engineering (ISKE), pages
9–12, 2010.

A. Vellido, J. D. Mart´ın-Guerrero, and P. J.G. Lisboa. Making machine learning models
In European Symposium on Artiﬁcial Neural Networks, Computational

interpretable.
Intelligence and Machine Learning (ESANN), 2012.

F. Wang and C. Rudin. Falling rule lists. In International Conference on Artiﬁcial Intelli-

gence and Statistics (AISTATS), 2015a.

F. Wang and C. Rudin. Causal falling rule lists. CoRR, abs/1510.05189, 2015b. URL

https://arxiv.org/abs/1510.05189.

T. Wang. Hybrid decision making: When interpretable models collaborate with black-box

models. CoRR, abs/1802.04346, 2018. URL http://arxiv.org/abs/1802.04346.

T. Wang, C. Rudin, F. Doshi-Velez, Y. Liu, E. Klampﬂ, and P. MacNeille. Bayesian or’s
of and’s for interpretable classiﬁcation with application to context aware recommender
systems. In International Conference on Data Mining (ICDM), 2016.

T. Wang, C. Rudin, F. Doshi-Velez, Y. Liu, E. Klampﬂ, and P. MacNeille. A Bayesian frame-
work for learning rule sets for interpretable classiﬁcation. Journal of Machine Learning
Research, 18(70):1–37, 2017.

E. Westervelt.

man’s murder?,
did-a-bail-reform-algorithm-contribute-to-this-san-francisco-man-s-murder.

reform algorithm contribute to this San Francisco
URL https://www.npr.org/2017/08/18/543976003/

Did a bail
2017.

H. Yang, C. Rudin, and M. Seltzer. Scalable Bayesian rule lists. In International Conference

on Machine Learning (ICML), 2017.

X. Yin and J. Han. CPAR: Classiﬁcation based on predictive association rules. In SIAM

International Conference on Data Mining (SDM), pages 331–335, 2003.

J. Zeng, B. Ustun, and C. Rudin. Interpretable classiﬁcation models for recidivism pre-
diction. Journal of the Royal Statistical Society: Series A (Statistics in Society), 180(3):
689–722, 2017.

Y. Zhang, E. B. Laber, A. Tsiatis, and M. Davidian. Using decision lists to construct
interpretable and parsimonious treatment regimes. Biometrics, 71(4):895–904, 2015.

78

