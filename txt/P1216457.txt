Bringing Salary Transparency to the World: Computing
Robust Compensation Insights via LinkedIn Salary

Krishnaram Kenthapadi, Stuart Ambler, Liang Zhang, Deepak Agarwal
LinkedIn Corporation, USA
(kkenthapadi, sambler, lizhang, dagarwal)@linkedin.com

7
1
0
2
 
p
e
S
 
1
 
 
]
I
S
.
s
c
[
 
 
3
v
5
4
8
9
0
.
3
0
7
1
:
v
i
X
r
a

ABSTRACT
The recently launched LinkedIn Salary product has been designed
with the goal of providing compensation insights to the world’s
professionals and thereby helping them optimize their earning po-
tential. We describe the overall design and architecture of the sta-
tistical modeling system underlying this product. We focus on the
unique data mining challenges while designing and implement-
ing the system, and describe the modeling components such as
Bayesian hierarchical smoothing that help to compute and present
robust compensation insights to users. We report on extensive eval-
uation with nearly one year of de-identiﬁed compensation data col-
lected from over one million LinkedIn users, thereby demonstrating
the efﬁcacy of the statistical models. We also highlight the lessons
learned through the deployment of our system at LinkedIn.

1.

INTRODUCTION

Online professional social networks such as LinkedIn have en-
abled job seekers to discover and assess career opportunities, and
job providers to discover and assess potential candidates. For most
job seekers, salary (or broadly, compensation) is a crucial consid-
eration in choosing a new job opportunity1. At the same time, job
seekers face challenges in learning the compensation associated
with different jobs, given the sensitive nature of compensation data
and the dearth of reliable sources containing compensation data.
The recently launched LinkedIn Salary product2 has been designed
with the goal of providing compensation insights to the world’s
professionals, and thereby help them make more informed career
decisions.

With its structured information including the work experience,
educational history, and skills associated with over 500 million
users, LinkedIn is in a unique position to collect compensation data
from its users at scale and provide rich, robust insights covering
different aspects of compensation, while preserving user privacy.
For instance, we can provide insights on the distribution of base
salary, bonus, equity, and other types of compensation for a given
profession, how they vary based on factors such as region, expe-
rience, education, company size, and industry, and which regions,
industries, or companies pay the most.

1More candidates (74%) want to see salary compared to any other
feature in a job posting, according to a survey of over 5000 job
seekers in US and Canada [4]. Job seekers value compensation the
most when looking for new opportunities, according to a US survey
of 2305 adults [5].
2https://www.linkedin.com/salary

.

In addition to helping job seekers understand their economic
value in the marketplace, the compensation data has the potential
to help us better understand the monetary dimensions of the Eco-
nomic Graph [22] (which includes companies, industries, regions,
jobs, skills, educational institutions, etc.).

The availability of compensation insights along dimensions such
as gender, ethnicity, and other demographic factors can lead to
greater transparency, shedding light on the extent of compensation
disparity, and thereby help stakeholders including employers, em-
ployees, and policy makers to take steps to address pay inequality.
Further, products such as LinkedIn Salary can improve efﬁciency
in the labor marketplace by reducing asymmetry of compensation
knowledge, and by serving as market-perfecting tools for workers
and employers [15]. Such tools have the potential to help students
make good career choices, taking expected compensation into ac-
count, and to encourage workers to learn skills needed for obtaining
well paying jobs, thereby helping reduce the skills gap.

We describe the overall design and architecture of the modeling
system underlying LinkedIn’s Salary product. We focus on unique
challenges such as the simultaneous need for user privacy, product
coverage, and robust, reliable compensation insights, and describe
how we addressed them using mechanisms such as outlier detec-
tion and Bayesian hierarchical smoothing. We perform extensive
evaluation with nearly one year of de-identiﬁed compensation data
collected from over one million LinkedIn users, thereby demon-
strating the efﬁcacy of the statistical models. We also highlight the
lessons learned through the production deployment of our system.

2. PROBLEM SETTING

In the publicly launched LinkedIn Salary product [12, 20], users
can explore compensation insights by searching for different titles
and regions (Figure 1). For a given title and region, we present
the quantiles (10th and 90th percentiles, median) and histograms
for base salary, bonus, and other types of compensation. We also
present more granular insights on how the pay varies based on fac-
tors such as region, experience, education, company size, and in-
dustry, and which regions, industries, or companies pay the most.

The compensation insights shown in the product are based on
compensation data that we have been collecting from LinkedIn
users. We designed a give-to-get model based data collection pro-
cess as follows. First, cohorts (such as User Experience Designers
in San Francisco Bay Area) with a sufﬁcient number of LinkedIn
users are selected. Within each cohort, emails are sent to a random
subset of users, requesting them to submit their compensation data
(in return for aggregated compensation insights later). Once we
collect sufﬁcient data, we get back to the responding users with the
compensation insights, and also reach out to the remaining users in

Figure 1: LinkedIn Salary Insights Page

those cohorts, promising corresponding insights immediately upon
submission of their compensation data.

Considering the sensitive nature of compensation data and the
desire for preserving privacy of users, we designed our system such
that there is protection against data breach, and any one individual’s
compensation data cannot be inferred by observing the outputs of
the system. Our methodology for achieving these goals through a
combination of techniques such as encryption, access control, de-
identiﬁcation, aggregation, and thresholding is described in [18].
We next highlight the key data mining and machine learning chal-
lenges for the salary modeling system.

2.1 Modeling Challenges

Modeling on de-identiﬁed data: Due to the privacy requirements,
the salary modeling system has access only to cohort level data
containing de-identiﬁed compensation submissions (e.g., salaries
for UX Designers in San Francisco Bay Area), limited to those co-
horts having at least a minimum number of entries. Each cohort
is deﬁned by a combination of attributes such as title, country, re-
gion, company, and years of experience, and contains de-identiﬁed
compensation entries obtained from individuals having the same
values of those attributes. Within a cohort, each individual entry
consists of values for different compensation types such as base
salary, annual bonus, sign-on bonus, commission, annual monetary
value of vested stocks, and tips, and is available without associated
user name, id, or any attributes other than those that deﬁne the co-
hort. Consequently, our modeling choices are limited since we have
access only to the de-identiﬁed data, and cannot, for instance, build
prediction models that make use of more discriminating features
not available due to de-identiﬁcation.

Evaluation: In contrast to several other user-facing products such
as movie and job recommendations, we face unique evaluation and
data quality challenges. Users themselves may not have a good
perception of the true compensation range, and hence it is not fea-
sible to perform online A/B testing to compare the compensation
insights generated by different models. Further, there are very few
reliable and easily available ground truth datasets in the compensa-
tion domain, and even when available (e.g., BLS OES dataset [2]),
mapping such datasets to LinkedIn’s taxonomy is inevitably noisy.

Figure 2: Salary Modeling Online and Ofﬂine Architecture

Outlier detection: As the quality of the insights depends on the
quality of submitted data, detecting and pruning potential outlier
entries is crucial. Such entries could arise due to either mistakes/mis-
understandings during submission, or intentional falsiﬁcation (such
as someone attempting to game the system). We needed a solution
to this problem that would work even during the early stages of data
collection, when this problem was more challenging, and there may
not be sufﬁcient data across say, related cohorts.

Robustness and stability: While some cohorts may each have a
large sample size, a large number of cohorts typically contain very
few (< 20) data points each. Given the desire to have data for as
many cohorts as possible, we need to ensure that the compensation
insights are robust and stable even when there is data sparsity. That
is, for such cohorts, the insights should be reliable, and not too sen-
sitive to the addition of a new entry. A related challenge is whether
we can reliably infer the insights for cohorts with no data at all.

Our problem can thus be stated as follows: How do we design
the salary modeling system to meet the immediate and future needs
of LinkedIn Salary and other LinkedIn products? How do we com-
pute robust, reliable compensation insights based on de-identiﬁed
compensation data (for preserving privacy of users), while address-
ing the product requirements such as coverage? We address these
questions in §3 and §4 respectively.

3. LINKEDIN SALARY MODELING SYS-
TEM DESIGN AND ARCHITECTURE
We describe the overall design and architecture of the salary
modeling system deployed as part of the recently launched LinkedIn
Salary product. Our system consists of an online component that
uses a service oriented architecture for retrieving compensation in-
sights corresponding to the query from the user facing product,
and an ofﬂine component for processing de-identiﬁed compensa-
tion data and generating compensation insights. Figure 2 presents
the key components of our system, divided into three groups: On-
line, Run Regularly, and Run As Needed. Explanations of the text
in the ﬁgure are in italics in the remainder of the section.

3.1 Online System for Retrieving
Compensation Insights

3.1.1 LinkedIn Salary Platform

The REST Server provides compensation insights on request by
instances of REST Client. The REST API [13] allows retrieval of
individual insights, or lists of them. For each cohort, an insight
includes, when data is available, the quantiles (10th and 90th per-
centiles, median), and histograms for base salary, bonus, and other
compensation types. For robustness of the insights in the face of
small numbers of submissions and changes as data is collected,
we report quantiles such as 10th and 90th percentiles and median,
rather than absolute range and mean.

3.1.2 LinkedIn Salary Use Case

For an eligible user, compensation insights are obtained via a
REST Client from the REST Server implementing the Salary Plat-
form REST API. These are then presented as part of LinkedIn
Salary product. Based on the product and business needs, the eli-
gibility can be deﬁned in terms of criteria such as whether the user
has submitted his/her compensation data within the last one year
(give-to-get model), or whether the user has a premium member-
ship.

Our Salary Platform has four service APIs to give the informa-
tion needed for LinkedIn Salary insight page: (1) a “criteria” ﬁnder
to obtain the core compensation insight for a cohort, (2) a “facets”
ﬁnder to provide information on cohorts with insights for ﬁlters
such as industry and years of experience, (3) a “relatedInsights”
ﬁnder to obtain compensation insights for related titles, regions,
etc., and (4) a “topInsights” ﬁnder to obtain compensation insights
by top paying industries, companies, etc. These ﬁnders were care-
fully designed to be extensible as the product needs evolve over
time. For instance, although we had originally designed the “crite-
ria” ﬁnder to provide insights during the compensation collection
stage, we were able to reuse and extend it for LinkedIn Salary and
Job search use cases.

3.1.3

Job Search Use Cases

The compensation insights can be very valuable for enhancing
other LinkedIn products. For example, we would like to present
the compensation information as part of a webpage providing the
description of a job posting, as part of the job search results page,
and as a facet to ﬁlter jobs based on salary range. Suppose the com-
pensation insights are requested for a combination such as (cid:104)title,
company, region(cid:105), e.g., (cid:104)Software engineer, Slack Technologies,
SF Bay Area(cid:105) for presentation under a corresponding job posting
webpage. We may not have sufﬁcient data or conﬁdence to return
insights at this granularity. Hence, the Salary Platform REST Server
generates possible generalizations such as (cid:104)Software engineer, In-
ternet industry, SF Bay Area(cid:105) or (cid:104)Software engineer, SF Bay Area(cid:105),
and probes the Insights store for the corresponding precomputed
insights.

3.2 Ofﬂine System for Computing
Compensation Insights

The insights are generated using an ofﬂine workﬂow, that con-
sumes the De-identiﬁed Submissions data (corresponding to cohorts
having at least a minimum number of entries) on HDFS, and then
pushes the results to the Insights and Lists Voldemort key-value
stores [21] for use by the REST Server. Analytical Data is gen-
erated at several points for business analysis, modeling research,
development, debugging, and testing. We can also use previously
generated compensation insights data for sanity checks on data
quality, by checking differences in the size or number of cohorts,
or in their insights.

This ofﬂine workﬂow consists of two groups, Run Regularly for
the hadoop ﬂow that runs more than once a day, and Run As Needed

for hadoop and other ﬂows that run, for instance, when externally
supplied data changes.

3.2.1 Hadoop Flow Run Regularly

This consists of components that Process Outliers (see §4.1),
using several methods to detect questionable submissions and re-
move or modify them, Aggregate Submissions to obtain 10th per-
centiles (“low end”), medians, 90th percentiles (“high end”), and
histograms of various kinds of compensation for each cohort, Smooth
Small Cohorts (see §4.2) that uses Bayesian smoothing to help with
some of the problems caused by cohorts with small numbers of sub-
missions, Delete & Add Insights to remove certain insights and add
others such as from trusted reliable sources other than user submis-
sions, and Make Lists to generate lists of insights or their keys.

3.2.2 Flows Run As Needed

These consist of components such as those that Ingest External
Data (see §4.1.1) to map external notions of title and region to
their LinkedIn counterparts, Train Regression Models (see §4.2.3)
to train models used for smoothing and prediction, Choose Smooth-
ing Parameters (see §5.3.3) to optimize tuning parameters for Baye-
sian smoothing, Make Similar Titles to create for each title, a list of
similar ones, and its analog Make Similar Regions.

4. STATISTICAL MODELING FOR
COMPENSATION INSIGHTS

4.1 Outlier Detection

An important goal for the LinkedIn Salary product is accuracy,
which is difﬁcult to evaluate, since there are few reliable public
datasets with comparable information. We use user-submitted data
as the main basis of reported results. Even if submissions were
completely accurate, there would still be selection bias; but ac-
curacy of submissions cannot simply be assumed. Mistakes and
misunderstandings in submission entry occur, and falsiﬁcation is
possible.

As part of the salary modeling ofﬂine workﬂow, de-identiﬁed
submissions are treated with three successive stages of outlier de-
tection to reduce the impact of spurious data. The ﬁrst stage uses
sanity-check limits such as the federal minimum wage as lower
limit, and an ad-hoc upper bound (e.g., $2M /year in US), for base
salary. The second stage uses limits derived from US Bureau of
Labor Statistics (BLS) Occupational Employment Statistics (OES)
data [2], aggregated from federal government mail surveys. This
dataset contains estimates of employment rates and wage percentiles
for different occupations and regions in US. The third stage is based
on the internal characteristics of each cohort of submissions, using
a traditional box-and-whisker method applied to the data remaining
from the second stage.

4.1.1 External Dataset based Outlier Detection

We map the BLS OES compensation dataset into LinkedIn ti-
tles and regions as follows, for outlier detection for US base salary
data. There are about 840 BLS occupations with SOC (Standard
Occupational Classiﬁcation) codes; an example is 13-2051, Finan-
cial Analysts. To map them to the circa 25K LinkedIn standard-
ized (canonical) titles, ﬁrst we expand them to O*NET alternate ti-
tles (look for alternate titles data dictionary at www.onetcenter.org).
Our example becomes 53 alternate titles including Bond Analyst,
Chartered Financial Analyst, Money Manager, Trust Ofﬁcer, etc.
To these, we apply LinkedIn standardization software which maps
an arbitrary title to a canonical title. Bond Analyst maps to Ana-
lyst, and the other three to themselves. In general, one BLS SOC

code corresponds to more than one LinkedIn standardized title. The
mapping from BLS to LinkedIn regions is done using zipcodes. In
general, more than one BLS region corresponds to one LinkedIn
region. Thus, we have a many to many mapping.

For each LinkedIn (title, region) combination, we obtain all BLS
(SOC code, region) rows that map to it, compute associated box-
and-whisker compensation range limits, and aggregate these limits
to derive one lower and one upper bound.

We obtained the limits for 6.5K LinkedIn standardized titles and
about 300 LinkedIn US region codes, resulting in 1.5M (cid:104)title, region(cid:105)
pairs. Submissions with base salary outside these limits are ex-
cluded.

4.1.2 Outlier Detection from User Submitted Data

Outlier detection based on user submitted data itself is done by a
box-and-whisker method [3]3, applied separately to each compen-
sation type.

The box-and-whisker method is as follows. For each compensa-
tion type for each cohort, we compute Q1 and Q3, the ﬁrst and third
quartiles respectively, the interquartile range, IQR = Q3 − Q1,
then compute the lower limit as Q1 − 1.5 · IQR, and the upper
limit as Q3 + 2 · IQR. We chose (and tuned) the different factors
1.5 and 2 to reﬂect the typically skewed compensation distributions
observed in our dataset.

Submissions with base salary outside the calculated range are
excluded. Other compensation type data are instead clipped to the
limits. We do not prune the entire entry, since the base salary is
valid, and given that, do not want to remove outlying values of other
compensation types, since that would have the effect of making
them zero for the total compensation calculation. We also exclude
whole compensation types or even whole cohorts when the fraction
of outliers is too large.

Note that this third stage is different in kind from the second
stage, where the limits do not depend on the submissions received.

4.2 Bayesian Hierarchical Smoothing

There is an inherent trade-off between the quality of compensa-
tion insights and the product coverage. The higher the threshold
for the number of samples for the cohort, the more accurate the
empirical estimates are. The lower the threshold for the number of
samples, the larger the coverage. Since it is critical for the product
to have a good coverage of insights for many different cohorts, ob-
taining accurate estimates of insights (e.g., percentiles) for cohorts
with very few user input samples turns out to be a key challenge.
Due to the sparsity of the data, empirical estimates of the 10th or
90th percentile, or even the median, are not reliable when a cohort
contains very little data. For example, the empirical estimate of the
10th percentile of a cohort’s compensation with only 10 points is
the minimum of the 10, which is known to be a very inaccurate
estimate.

We next describe a Bayesian hierarchical smoothing method-
ology to obtain accurate estimates of compensation insights for
cohorts with very little data. Speciﬁcally, for cohorts with large
enough sample sizes (i.e., number of samples greater than or equal
to a threshold h, say 20), we consider it safe to use the empirical
estimates for median and percentiles (see §6 for a discussion of the
tradeoffs in choosing h). On the other hand, for cohorts with sam-
ple sizes less than h, we ﬁrst assume that the compensation follows
a log-normal distribution [19] (see §5.3.1 for validation of the as-
sumption), then exploit the rich hierarchical structure amongst the
cohorts, and “borrow strength” from the ancestral cohorts that have

3http://www.itl.nist.gov/div898/handbook/
prc/section1/prc16.htm

sufﬁcient data to derive cohort estimates. For example, by suc-
cessively relaxing the conditions, we can associate the cohort “UX
designers in SF Bay Area in Internet industry with 10+ years of
experience” with larger cohorts such as “UX designers in SF Bay
Area in Internet industry”, “UX designers in SF Bay Area with
10+ years of experience”, “UX designers in SF Bay Area”, and so
forth, and pick the “best” ancestral cohort using statistical meth-
ods (§4.2.1). After the best ancestor is selected, we treat the data
collected from the ancestral cohort as the prior to apply a Bayesian
smoothing methodology, and obtain the posterior of the parameters
of the log-normal distribution for the cohort of interest (§4.2.2). We
also describe how to handle the “root” cohorts (a combination of
job title and region in our product) using a prior from a regression
model in §4.2.3.

4.2.1 Finding the Best Ancestral Cohort

The set of all possible cohorts forms a natural hierarchy, where
each cohort in the hierarchy can have multiple ancestors, as in the
example of “UX designers in SF Bay Area in Internet industry with
10+ years of experience” given above. We describe our statistical
approach to ﬁnd the “best” ancestral cohort among all the ances-
tors of the cohort of interest in the hierarchy, where the ancestor
that can “best explain” the observed entries in the cohort of interest
statistically is considered the best. The ancestor can later be used to
provide the prior information to estimate the posterior of parame-
ters for the distribution of compensation insights for the cohort. We
note that for ancestors with sample size greater than h, the empiri-
cal estimates of parameters are used to obtain the prior; otherwise,
the posterior estimates of the parameters for the ancestor will be
used as the prior of the cohort of interest.

We denote the cohort of interest as y, and the observed log-
transformed compensation data that belong to y as yi, i = 1, . . . , n,
with empirical average ¯y. Let P = {z(1), . . . , z(K)} be the set of
y’s ancestral cohorts. Our objective is to pick the “best” ancestral
cohort from P , based on how well the data in y matches the data in
the ancestor statistically.

Assume for cohort z(j), the compensation data follows a log-
normal distribution, with mean µj and variance σ2
j after the log
transformation. We can pick the best z(J) out of the K ancestors of
y, by the following criteria (maximizing log-likelihood, or equiva-
lently minimizing negative log-likelihood, assuming each yi to be
independently drawn at random from the distribution of z(j)), using
the estimates of µj and σ2

j for each z(j), j = 1, . . . , K:

J = arg min

j

log(2πσ2

j ) +







n
2

n
(cid:80)
i=1

(yi − µj)2

2σ2
j







(1)

We then use the corresponding µJ and σ2
J that provides the max-
imum of the log-likelihood as the prior for the smoothing of the
compensation percentiles for cohort y (see §4.2.2 for details). We
also note that in Equation (1), if the number of samples for z(j) is
greater than h, we compute µj and σ2
j from the empirical estimates
from its data directly. Otherwise, µj and σ2
j would be estimated
by the posterior of cohort z(j) smoothed by the prior from z(j)’s
ancestors, again following §4.2.2.

4.2.2 Bayesian Hierarchical Smoothing

For cohort y, suppose we pick cohort z as the best ancestor fol-
lowing §4.2.1. Denote µ and σ2 as the estimated mean and variance
for z. Also, denote number of samples in cohort z as m. We can
assume the following model for the data yi, i = 1, . . . , n, with yi

(2)

(3)

(4)

(6)

(7)

(8)

(9)

(10)

being the log of each compensation sample in cohort y:

• Estimate the posterior mean of τ 2 as

yi ∼ N (ν, τ 2), i = 1, . . . , n

where we assume ν and τ 2 have the conjugate prior as follows:

η + 1
2

(cid:80)
i

ˆτ 2 =

(yi − ¯y)2 + nn0

2(n+n0) (¯y − µ)2

.

n

2 + η
σ2

ν|τ 2 ∼ N (µ,

τ 2
n0

),

˜τ = 1/τ 2 ∼ Gamma(η/σ2, η),

where n0 = m/δ, and δ and η are hyper-parameters that can be
tuned through cross-validation. We note that the prior mean of ν is
the same as the best ancestor’s mean µ, and for the prior distribu-
tion of ˜τ , 1/σ2 is the prior mean (also from the best ancestor) and
1/(ησ2) is the prior variance.

Now we derive the posterior p(ν, ˜τ |y1, . . . , yn). Let ¯y = (cid:80)

yi/n

be the empirical average of observations in y. First, the joint poste-
rior,

i

p(ν, ˜τ |y1, . . . , yn) ∝ p(y1, . . . , yn|ν, ˜τ )p(ν|˜τ )p(˜τ ).

(5)

The marginal posterior,
(cid:90)

p(˜τ |y1, . . . , yn) ∝

p(y1, . . . , yn|ν, ˜τ )p(ν|˜τ )p(˜τ )dν

∼ Gamma

+

(cid:16) n
2

η
σ2 ,

η +

1
2

(cid:88)

i

(yi − ¯y)2 +

nn0
2(n + n0)

(cid:33)

(¯y − µ)2

.

Hence the posterior mean of ˜τ ,

ˆ˜τ =

η + 1
2

(cid:80)
i

n

2 + η
σ2
(yi − ¯y)2 + nn0
2(n+n0) (¯y − µ)2

.

For simplicity, we plug in ˆτ 2 = 1/ˆ˜τ as the estimate of τ 2 for the

rest of the calculation. Given ˜τ , the posterior of ν is

p(ν|˜τ , y1, . . . , yn) ∝ p(y1, . . . , yn|ν, ˜τ )p(ν|˜τ )

∼ N (

n
n + n0

¯y +

n0
n + n0

µ,

1
(n + n0)˜τ

)

• For a new observation added to the population of existing
observations of y, denoted as ynew, the mean and variance
of ynew (post log-transformation) are
n
n + n0

ˆµ = E[ynew|y1, . . . , yn] =

n0
n + n0

¯y +

µ,

ˆσ2 = V ar[ynew|y1, . . . , yn] = (1 +

1
n + n0

)ˆτ 2.

Since normal distribution is symmetric, the median of log(compensation)
is set to ˆµ, 10th percentile to ˆµ−1.282·ˆσ, and 90th percentile
to ˆµ + 1.282 · ˆσ.

• Obtain the ﬁnal estimates by taking exponential transforma-

tion of these three quantities.

4.2.3

Smoothing for Root Cohorts

We note that there can be cases where the root cohorts in the
hierarchy do not have enough samples to estimate percentiles em-
pirically. For LinkedIn Salary product speciﬁcally, we consider a
root cohort as a function of a job title and a geographical region,
and model the compensation for these cohorts using a number of
features. Simply using parent cohorts such as title only or region
only as the prior might not be good enough, as there is a lot of het-
erogeneity for compensation of the same title for different regions
(e.g., New York vs. Fresno), and that of the same region for differ-
ent titles as well (e.g., Software engineers vs. Nurses). Hence, for
these cohorts, we build a feature based regression model to serve
as the prior for Bayesian smoothing.

Suppose there are P root cohorts, with a combined total of N
samples. Denote the value of ith sample for cohort p, after apply-
ing log-transformation as uip, and the vector of all responses as U .
Let the feature vector for cohort p be xp, which can include features
such as title, country, region, skills related to the title, average com-
pensation data for the region determined from external sources, and
so forth, and the entire design matrix be X. The regression model
can then be expressed as

uip ∼ N (x(cid:48)

pβ, γ2),

Therefore, for any new observation ynew,

E[ynew|y1, . . . , yn] = E[ν|˜τ , y1, . . . , yn] =

n
n + n0

¯y+

n0
n + n0

µ,

where β has a Gaussian prior

β ∼ N (0, λI),

V ar[ynew|y1, . . . , yn] = E[V ar[ynew|y1, . . . , yn, ν]]
+ V ar[E[ynew|y1, . . . , yn, ν]]

= ˆτ 2 + V ar[ν|y1, . . . , yn] = (1 +

1
n + n0

)ˆτ 2

To summarize, the median, 10th percentile, and the 90th percentile
of the compensation for cohort y can be estimated as follows:

• Input: Data for cohort y as y1, . . . , yn; For best ancestral co-
hort z of y, let z’s sample size be m, mean be µ and variance
be σ2. Compute the empirical average, ¯y of y1, . . . , yn.

• Tuning parameters: δ and η, which can be optimized via

cross-validation (see §5.3.3).

• Let n0 = m/δ.

and λ is a L2 penalty tuning parameter for the regression model.
The estimate of β is given by

ˆβ = (X (cid:48)X + λI)−1X (cid:48)U ,

and the estimate of γ2 is given by

(cid:80)
i

(cid:80)
p

(uip − x(cid:48)
p

ˆβ)2

ˆγ2 =

N

.

(14)

For any root cohort, to obtain its posterior of ν and ˜τ following
Equation (5), the parameters µ and σ2 for the prior in Equations (3)
ˆβ, and σ2 = ˆγ2. We set the
and (4) can be speciﬁed as: µ = x(cid:48)
p
“sample size” m to be equal to the smoothing sample size threshold
h, with the interpretation that a regression model prior is given the
same weight as an ancestral cohort with h samples.

(11)

(12)

(13)

Table 1: Fractional Change From Adding Spurious Data

5

25

20

15

10

45
0
-5
-1
-1

22
0
-47
-6
-2

29
0
-42
-4
-2

37
0
-23
-2
-1

14
0
-52
-8
-3

Percent Added Data
Adding Minimum Wage Data
Mean Percent of Added, Removed
Mean Percent of Original, Removed
Mean Percent Change 10th Percentile
Mean Percent Change Median
Mean Percent Change 90th Percentile
Adding Data Between 10th and 90th Percentiles
Mean Percent of Added, Removed
Mean Percent of Original, Removed
Mean Percent Change 10th Percentile
Mean Percent Change Median
Mean Percent Change 90th Percentile
Adding $2 Million Data
100
Mean Percent of Added, Removed
0
Mean Percent of Original, Removed
Mean Percent Change 10th Percentile
0
0
Mean Percent Change Median
Mean Percent Change 90th Percentile
0
Notes: percents rounded to integers with two signiﬁcant digits. Means are over all test cohorts.

100
0
0
0
0

100
0
0
0
0

100
0
0
0
0

100
0
0
0
0

0
0
1
0
-1

0
0
2
0
-1

0
0
2
1
-2

0
0
3
1
-2

0
0
3
1
-2

30

5
0
-59
-10
-4

0
0
4
1
-2

92
0
0
1
180

35

0
0
-63
-11
-4

0
0
4
2
-3

0
0
6
13
2300

5. EXPERIMENTS

We next present an evaluation of our system for computing com-
pensation insights as part of LinkedIn Salary, focusing on the statis-
tical models. We study the impact of the outlier detection methods
and evaluate the effectiveness of the regression model and Bayesian
statistical smoothing. We also investigate the performance of the
statistical smoothing techniques for different segments and under
different parameter settings.

5.1 Experimental Setup

As described earlier, we implemented our statistical models as
part of the ofﬂine system for computing compensation insights, and
deployed in production as part of LinkedIn Salary product. Our of-
ﬂine system for computing compensation insights is implemented
in a distributed computing environment using Hadoop.

We performed our experiments over nearly one year of de-identiﬁed

compensation data collected from more than one million LinkedIn
users, across three countries (USA, Canada, and UK). Due to the
privacy requirements, we have access only to cohort level data con-
taining de-identiﬁed compensation submissions (e.g., salaries for
Software Engineers in San Francisco Bay Area), limited to those
cohorts having at least a minimum number of entries. Each cohort
consists of a set of de-identiﬁed compensation data correspond-
ing to individuals, and each individual entry consists of values for
different compensation types. There are approximately 54K title-
country-region cohorts, and 24K title-country-region-company-cohorts,
for which we have insights.

As discussed in §2.1, we cannot compare the performance of
different models using online A/B testing, since users may not nec-
essarily have a good perception of the true compensation range. We
also face additional evaluation challenges since there are very few
reliable and easily available ground truth datasets in the compensa-
tion domain (e.g., BLS OES dataset), and it is challenging to map
such datasets to LinkedIn’s taxonomy due to the noise introduced.

5.2 Studying Impact of Spurious Added Data
Because of the lack of reliable ground truth data, our focus is on
studying the impact of adding spurious data, then winnowing it by
the box-and-whisker method (§4.1.2). We take the actual compen-
sation data submitted, after it has gone through all the outlier de-
tection stages, and consider it as valid. This constitutes about 80%
of the submissions in cohorts. We limit this data to title-country-
region cohorts, to US data of yearly base salary, and to cohorts with
at least 20 valid entries, the last so that statistical smoothing does
not interfere with interpretation. This leaves about 10K cohorts for
the study.

For each cohort, we record selected quantiles (10th percentile,
median, and 90th percentile), then perturb the cohort in three ways
by addition of spurious, synthetic salary entries numbering certain
fractions (5% through 35% in steps of 5%) of the original numbers
of entries. The three methods of perturbation are:

• Add the spurious data at the federal minimum wage, $15080.
• Add the spurious data at $2 million.
• Add the spurious data uniformly at random between the 10th

and 90th percentiles, the low and high ends.

For each method, we examine the following, averaged over all the
cohorts used in the test.

• The fraction of the added entries that are removed as outliers;

• The fraction of the original entries that are removed as out-

a perfect score is 1.

liers; a perfect score is 0.

• The fractional changes in the selected quantiles. This is the
most important of these measures in practice, because these
are what we report to users.

For all the data added between low and high ends, less than 1% of
the added or original entries were removed. The reported quantiles
changed by less than 5%.

For added high entries, about 100% were removed as outliers
until the amount of the added data was sufﬁciently large: at 30%,
92% were removed, and at 35%, none. Less than 1% of the original
data were removed as outliers. The selected quantiles changed by
less than 1% until at 30% added, the median increased by 1% and
the high end increased by 180%, and at 35%, the low end increased
by 6%, median by 13%, and high end by 2300%.

For added low entries, note that the minimum wage is in general
closer to the low end of the original data, than $2 million is to the
high end. One might therefore suspect that outlier detection by
the box-and-whisker method of these added entries, would be less
successful than for added high entries. Little of the original data
were eliminated as outliers; see Table 1 for other results.

Overall, the box-and-whisker outlier detection, aside from very
large amounts of spurious data or when the low end changed from
the addition of more than 5% spurious minimum wage data, did at
least reasonably and often quite well.

5.3 Evaluating Regression Model and

Bayesian Statistical Smoothing

We ﬁrst selected the set of base salaries reported by LinkedIn
users in US, resulting in about 800K entries. We limited to only
title-country-region cohorts so that each user submission is reﬂected
exactly once. We used this dataset for validating the log-normal
distribution assumption and for training and evaluating the regres-
sion model.

5.3.1 Validation of Log-Normal Assumption

We sampled 10K entries at random from the above dataset, and
generated Q-Q plots in both original and logarithmic scale. From
Figure 3, we can visually observe that the logarithmic scale is a
better ﬁt (closer to a straight line), suggesting that the data on the
whole can be ﬁt to a log-normal distribution. Hence, for training
the regression model as well as applying statistical smoothing, we
use the compensation data in the logarithmic scale. However, we
remark that this assumption is an approximation since the observed
salaries for an individual cohort may not follow a log-normal dis-
tribution.

5.3.2 Evaluating Regression Model for Root Cohorts
We trained and validated log-linear regression models for esti-
mating compensation for each title-country-region cohort. A to-
tal of 24 models were trained corresponding to three countries and
eight compensation types. We performed evaluation for all 24 com-
binations, but present results only for base and total compensation
models in US. For the (log) base salary model, we observed that
the mean cross-validation error (0.085) is comparable to the mean
empirical variance within each cohort (0.080). The corresponding
numbers for the total compensation model were 0.097 and 0.092
respectively. Since we used only cohort-level features and not fea-
tures that could discriminate samples within a cohort (due to de-
identiﬁcation), we do not expect the mean error to be less than the
mean variance within each cohort. Thus, these results suggest that
the trained models are performing well.

We next computed the coefﬁcient of determination, R2 for the
(log) base salary model in two different ways. First, we computed
over the entire dataset, where each row represents the de-identiﬁed
(log) base salary reported by some user. There are as many rows
for each title-country-region cohort as the number of users that
provided salary for that combination. We computed R2 = 1 −
the residual sum of squares
the total sum of squares = 0.58. The same value (after rounding) was
observed for the total compensation as well. Due to de-identiﬁcation
however, intuitively, our model attempts to predict the mean log
base salary for a cohort, but not the base salary for each user.
Hence, we also computed a variant of R2 over the aggregated dataset,
with one row per title-country-region cohort. The empirical mean
log base salary for each cohort is treated as the observed value, and
the predicted log base salary is treated as the predicted value. The
coefﬁcient computed in this manner was signiﬁcantly higher: 0.88
for both base and total compensation. We observed similar results
for all 24 models.

5.3.3 Optimization of Statistical Smoothing

Parameters

We next describe how we chose the optimal values of the follow-
ing parameters used as part of statistical smoothing: (1) δ, which
is a discounting factor for the effect of the ancestral cohort, and
(2) η, which is a parameter used in the prior Gamma distribution
for choosing the variance. The key idea is to maximize the likeli-
hood of observing a hold-out set, with respect to the posterior dis-
tribution computed with each choice of smoothing parameters. We
partitioned the set S of all observed entries, restricted to cohorts
with small sample size (< 20), where we applied smoothing, into a
training set, Strain and a hold-out test set, Stest of 10% of the data.
We performed this over all possible cohorts (title-country-region,
title-country-region-company, etc.), randomly partitioning each co-
hort, ensuring that at least one entry is present in the hold-out set.
For different choices of smoothing parameters, we computed the
posterior distribution for each cohort based on the training set, and
computed the (combined) likelihood of observing the entries in the
hold-out set (assuming independence across entries, which is an
approximation). We selected the parameters that maximized this
likelihood. Algorithm 1 provides a formal description.

We performed grid search with ∆ = [1, 1000] and E = {0.01 ·
2r|r ∈ [0, 11]}, and computed the optimal parameters overall as
well as for different segments such as cohorts containing a com-
pany, cohorts containing an industry, and so on. In all cases, the
likelihood was maximized well inside this range (that is, not at
the extremes for either δ or η). We observed that (δ∗ = 5, η∗ =
0.32) was optimal overall, while for cohorts containing a company,
(δ∗ = 250, η∗ = 0.04), implying larger discount for ancestral co-
hort. This observation agrees with our intuition that the salaries at

Figure 3: Validation of Log-normal Distribution Assumption using
Q-Q Plots. In both plots, X-axis corresponds to normal theoretical
quantiles. Y-axis corresponds to the data quantiles for US base
salaries in original and logarithmic scale respectively.

Algorithm 1 Optimizing smoothing parameters

1: Input: The set, S of all observed entries, partioned into the
training set, Strain and the hold-out set, Stest; The candidate
set, ∆ of choices for δ; The candidate set, E of choices for η.

2: Output: Optimal parameters, δ∗, η∗.
3: for all δ ∈ ∆ do
4:
5:

for all η ∈ E do

Compute smoothed posterior log-normal distribution,
Dposterior(c) for each cohort, c based on only Strain.

6:

LL[δ, η] := (cid:80)
s∈Stest
c(s) denotes the cohort containing entry s.
7: Output (δ∗, η∗) = arg maxδ∈∆,η∈E LL[δ, η].

log(pDposterior (c(s))(s)), where

say, title-country-region-company level could deviate signiﬁcantly
from the corresponding title-country-region level (which is likely
to be selected as the best ancestral cohort), and hence we would
like to give relatively lesser weight to the ancestral cohort in such
cases.

5.3.4 Evaluating Statistical Smoothing:
Goodness-of-ﬁt Analysis

In addition to parameter optimization, Algorithm 1 also helps us
study the “goodness-of-ﬁt” of the observed data in the hold-out set,
with no smoothing vs. some extent of smoothing. Here, we com-
pare the likelihood of observing each cohort in the hold-out set with
respect to (1) a distribution derived just from the observed entries
in the corresponding cohort in the training set, vs. (2) the smoothed
posterior distribution that also takes ancestral cohort into account.
As δ → ∞, the weight given to the ancestral cohort tends to zero,
and hence the smoothed distribution converges to a (log-normal)
distribution derived from just the observed entries. However, in
§5.3.3, we observed that a ﬁnite value of δ (= 5 overall), rather
than δ → ∞ (corresponding to the right extreme, 1000 in our com-
putation), leads to the maximum likelihood or the best goodness-
of-ﬁt. A similar observation holds even when limited to different
segments such as cohorts containing a company. We conclude that
combining ancestor and cohort data with statistical smoothing re-
sults in better goodness-of-ﬁt for the observed data in the hold-out
set, compared to using ancestor or cohort alone.

An intuitive explanation is that statistical smoothing provides
better stability and robustness of insights. Inferring a distribution
based on just the observed entries in the training set for a cohort
and using it to ﬁt the corresponding hold-out set is not as robust
as using the smoothed distribution to ﬁt the hold-out set, especially
when the cohort contains very few entries. However, this evalua-
tion approach intrinsically requires a distributional assumption for
computing the likelihood, which can be thought of as a limitation
since (1) any such assumption could be empirically validated on
the whole (§5.3.1), but may not hold for an individual cohort, and
(2) the log-normal distribution assumption is used as part of the
smoothing methodology, and hence can be viewed as “favoring”
it. Consequently, we decided to evaluate using a non-parametric
approach that does not require any distributional assumption.

5.3.5 Evaluating Statistical Smoothing:

Quantile Coverage Test

We next performed a quantile coverage test, wherein we mea-
sured what fraction of a hold-out set lies between two quantiles
(e.g., 10th and 90th percentiles), computed based on the training set
(1) empirically without smoothing, and (2) after applying smooth-
ing. For 0 < α < β < 1, an ideal quantile computation method

should satisfy the following properties (averaged across all cohorts):
(1) α fraction of the hold-out data points in a cohort should be lower
than the corresponding α-quantile, (2) 1−β fraction of the hold-out
data points in a cohort should exceed the corresponding β-quantile,
and (3) β − α fraction is in between, where the quantiles are com-
puted for each cohort based on the training set entries. Since 10th
and 90th percentiles are shown to the users in the product, we set
α = 0.1, β = 0.9. We used the optimal parameters from §5.3.3 for
computing 10th and 90th percentiles using smoothing method.

We partitioned the set of observed entries (limited to cohorts
with small sample size (< 20), where we applied smoothing) into a
training set and a hold-out set as before. For each cohort, we com-
puted 10th and 90th percentiles using the training set data points
using each method (empirical vs. smoothed), and measured what
fraction of the hold-out data points belong to this range. We then
computed the mean fraction across all cohorts (and also for differ-
ent segments of interest).

We observed that the fractions computed using smoothed per-
centiles are signiﬁcantly better than those computed using empir-
ical percentiles. The mean fraction of the hold-out data between
10th and 90th percentiles is 85% with smoothing (close to the ideal
of 80%), but only 54% with empirical approach. The remaining
hold-out data is roughly evenly split below 10th percentile and
above 90th percentile. We observed similar results for various seg-
ments such as cohorts containing a company, an industry, or a de-
gree.

We also investigated the effect of the cohort size. Intuitively, we
expect closer-to-ideal results for larger cohorts, but more deviation
for very small cohorts. Comparing cohorts with at least 5 entries to
those with just 3 or 4 entries, the above mean fraction deviates only
slightly away from the 80% ideal (83% vs. 86%), with smoothing
applied. However, the deviation is signiﬁcantly worse with em-
pirical approach (71% vs. 39%), agreeing with our intuition that
computing empirical percentiles based on very few entries is unre-
liable.

While such a quantile coverage test by itself cannot imply that an
approach is effective, the goodness-of-ﬁt and coverage-based eval-
uations together establish that statistical smoothing leads to sig-
niﬁcantly better and robust compensation insights. Compensation
domain knowledge experts also conﬁrmed this conclusion. By em-
ploying statistical smoothing, we were able to reduce the thresh-
old used for displaying compensation insights in LinkedIn Salary
product, thereby achieving signiﬁcant increase in product coverage,
while simultaneously preserving the quality of the insights.

6. LESSONS LEARNED IN PRACTICE

We next present the challenges encountered and the lessons learned

through the production deployment of both our salary modeling of-
ﬂine workﬂow and REST service as part of the overall LinkedIn
Salary system for more than a year, initially during the compen-
sation collection and later as part of the publicly launched product.
We performed several iterative deployments of the ofﬂine workﬂow
and the REST service, always maintaining a sandbox version (for
testing) and a production version of our system. The service was
deployed to multiple servers across different data centers to ensure
better fault-tolerance and latency.

One of the greatest challenges has been the lack of good public
“ground truth” datasets.
In the United States, the Internal Rev-
enue Service has fairly complete salary data, but it is not public.
The Bureau of Labor Statistics makes available aggregate data, but
that brings with it the challenge of mapping to LinkedIn taxonomy.
Some state governments, for example California, make available

government employee salaries, but government jobs differ from
private-sector jobs.

We did simulations sampling smaller from larger cohorts to get
an idea of how much variation in the reported quantiles might be
expected if we decreased the sample size threshold. This analy-
sis helped us understand the tradeoffs between increasing coverage
and ensuring robust insights, and guided the product team decisions
on whether to reduce the threshold. In fact, this analysis helped
motivate the need for applying Bayesian smoothing, after which
we were able to further reduce the threshold while retaining ro-
bustness of insights. The choice of smoothing threshold (h = 20)
was determined by similar tradeoffs. While applying smoothing
is desirable for even larger sized cohorts from the perspective of
robustness, a practical limitation is that the smoothed histograms
have to be computed based on a parametrized (log-normal) distri-
bution, resulting in all smoothed histograms having identical shape
(truncated log-normal distribution from 10th to 90th percentiles).
As the empirical histogram was considered more valuable from the
product perspective, we decided not to display any histogram for
smoothed cohorts, and chose a relatively low threshold of 20 to
ensure adequate coverage for cohorts with histograms.

7. RELATED WORK

Salary Information Products: There are several commercial ser-
vices offering compensation information. For example, Glassdoor [6]
offers a comparable service, while PayScale [8] collects individual
salary submissions, offers free reports for detailed matches, and
sells compensation information to companies. The US Bureau of
Labor Statistics [7] publishes a variety of statistics on pay and ben-
eﬁts.

Privacy: Preserving user privacy is crucial when collecting com-
pensation data. We encountered unique challenges associated with
privacy and security while designing our system. Our methodol-
ogy for addressing these challenges through a combination of tech-
niques such as encryption, access control, de-identiﬁcation, aggre-
gation, and thresholding is described in [18]. Please refer this paper
for a discussion of different privacy techniques (in contrast with
our approach), and an empirically study of the tradeoffs between
privacy and modeling needs.

Survey Techniques: There is extensive work on traditional statis-
tical survey techniques [14, 17], as well on newer areas such as web
survey methodology [11]. See [1] for a survey of non-response bias
challenges, and [10] for an overview of selection bias.

Statistical Smoothing: The idea of Bayesian hierarchical statisti-
cal smoothing originates from the smoothing of sparse events (e.g.,
CTR) in the context of computational advertising [9, 23], where a
natural hierarchy for the combination of ads category and publisher
category is used for an (ad, publisher) pair with very little data to
borrow strength from its ancestor nodes. However, that is an en-
tirely different context and the models used are hence different.

8. CONCLUSIONS AND FUTURE WORK

We studied the problem of computing robust, reliable compen-
sation insights based on de-identiﬁed compensation data collected
from LinkedIn users. We presented the design and architecture of
the modeling system underlying LinkedIn’s Salary product, which
was launched recently towards the goals of bringing greater trans-
parency and helping professionals make more informed career de-
cisions. We highlighted unique challenges such as modeling on de-
identiﬁed data, lack of good evaluation datasets or measures, and
the simultaneous need for user privacy, data quality, and product
coverage, and described how we addressed them using mechanisms

such as outlier detection and Bayesian hierarchical smoothing. We
showed the effectiveness of our models through extensive experi-
ments on de-identiﬁed data from over one million users. We also
discussed the design decisions and tradeoffs while building our sys-
tem, and the lessons learned from more than one year of production
deployment.

The availability of compensation data, combined with other datasets,

opens several research possibilities to better understand and im-
prove the efﬁciency of career marketplace (as discussed in §1).
There are also several directions to extend this work, which we are
currently pursuing. We would like to improve quality of insights
and product coverage via better data collection and processing, in-
cluding inference of insights for cohorts with no data, improvement
of the statistical smoothing methodology, better estimation of vari-
ance in the regression models, and creation of intermediary levels
such as company clusters between companies and industries. We
also plan to improve outlier detection both at the user-level (using
user proﬁle and behavioral features, during submission), and at the
cohort level (e.g., using the medcouple measure of skewness [16]).
Another direction is to use other datasets (e.g., position transition
graphs, salaries extracted from job postings) to detect/correct in-
consistencies in the insights across cohorts. Finally, mechanisms
can be explored to quantify and address different types of biases
such as sample selection bias and response bias. For example,
models could be developed to predict response propensity based
on user proﬁle and behavioral attributes, which could then be used
to compensate for response bias through techniques such as inverse
probability weighting.

9. ACKNOWLEDGMENTS

The authors would like to thank all other members of LinkedIn
Salary team for their collaboration for deploying our system as
part of the launched product, and Stephanie Chou, Ahsan Chud-
hary, Tim Converse, Tushar Dalvi, Anthony Duerr, David Freeman,
Joseph Florencio, Souvik Ghosh, David Hardtke, Parul Jain, Pra-
teek Janardhan, Santosh Kumar Kancha, Ryan Sandler, Cory Scott,
Ganesh Venkataraman, Ya Xu, and Lu Zheng for insightful feed-
back and discussions.

10. REFERENCES

[1] Encyclopedia of Social Measurement, volume 2, chapter

Non-Response Bias. Academic Press, 2005.

[2] BLS Handbook of Methods, chapter 3, Occupational

Employment Statistics. U.S. Bureau of Labor Statistics,
2008. http://www.bls.gov/opub/hom/pdf/homch3.pdf.
[3] NIST/SEMATECH e-Handbook of Statistical Methods.
National Institute of Standards and Technology, U.S.
Department of Commerce, 2013.

[4] How to rethink the candidate experience and make better
hires. CareerBuilder’s Candidate Behavior Study, 2016.

[5] Job seeker nation study. Jobvite, 2016.
[6] Glassdoor introduces salary estimates in job listings,

[7] Overview of BLS statistics on pay and beneﬁts, February
2017. https://www.bls.gov/bls/wages.htm.

[8] Payscale data & methodology, February 2017.
http://www.payscale.com/docs/
default-source/pdf/data_one_pager.pdf.

February 2017.
https://www.glassdoor.com/press/
glassdoor-introduces-salary-estimates-job-listings-reveals-unfilled-jobs-272-billion/.

supermarket. LinkedIn Pulse, 2016.
https://www.linkedin.com/pulse/
how-make-job-market-work-like-supermarket-seth-harris.

[9] D. Agarwal, R. Agrawal, R. Khanna, and N. Kota.

Estimating rates of rare events with multiple hierarchies
through scalable log-linear models. In KDD, 2010.
[10] J. Bethlehem. Selection bias in web surveys. International

Statistical Review, 78(2), 2010.

[11] M. Callegaro, K. L. Manfreda, and V. Vehovar. Web survey

methodology. Sage, 2015.

[12] A. Duerr and S. K. Kancha. Bringing salary transparency to
the world. LinkedIn Engineering Blog, 2016. https:
//engineering.linkedin.com/blog/2016/10/
bringing-salary-transparency-to-the-world.

[13] R. T. Fielding. Architectural styles and the design of

network-based software architectures. PhD thesis, University
of California, Irvine, 2000.

[14] R. M. Groves, F. J. Fowler Jr, M. P. Couper, J. M.
Lepkowski, E. Singer, and R. Tourangeau. Survey
methodology. John Wiley & Sons, 2011.

[15] S. Harris. How to make the job market work like a

[16] M. Hubert and E. Vandervieren. An adjusted boxplot for

skewed distributions. Computational statistics & data
analysis, 52(12), 2008.

[17] R. J. Jessen. Statistical survey techniques. John Wiley &

Sons, 1978.

[18] K. Kenthapadi, A. Chudhary, and S. Ambler. LinkedIn

Salary: A system for secure collection and presentation of
structured compensation insights to job seekers. In IEEE
PAC, 2017. Available at
https://arxiv.org/abs/1705.06976.

[19] M. Pinkovskiy and X. Sala-i Martin. Parametric estimations

of the world distribution of income, 2009. Working Paper
No. 15433, National Bureau of Economic Research.
[20] R. Sandler. Introducing “LinkedIn Salary”: Unlock your

[21] R. Sumbaly, J. Kreps, L. Gao, A. Feinberg, C. Soman, and
S. Shah. Serving large-scale batch computed data with
project Voldemort. In FAST, 2012.

[22] J. Weiner. The future of LinkedIn and the Economic Graph.

LinkedIn Pulse, 2012.

[23] L. Zhang and D. Agarwal. Fast computation of posterior
mode in multi-level hierarchical models. In NIPS, 2009.

earning potential. LinkedIn Blog, 2016.
https://blog.linkedin.com/2016/11/02/
introducing-linkedin-salary-unlock-your-earning-potential.

