A physical model for eﬃcient ranking in networks

Caterina De Bacco,1, 2, ∗ Daniel B. Larremore,3, 4, 2, † and Cristopher Moore2, ‡
1Data Science Institute, Columbia University, New York, NY 10027, USA
2Santa Fe Institute, Santa Fe, NM 87501, USA
3Department of Computer Science, University of Colorado, Boulder, CO 80309, USA
4BioFrontiers Institute, University of Colorado, Boulder, CO 80303, USA

We present a physically-inspired model and an eﬃcient algorithm to infer hierarchical rankings of
nodes in directed networks. It assigns real-valued ranks to nodes rather than simply ordinal ranks,
and it formalizes the assumption that interactions are more likely to occur between individuals with
similar ranks.
It provides a natural statistical signiﬁcance test for the inferred hierarchy, and it
can be used to perform inference tasks such as predicting the existence or direction of edges. The
ranking is obtained by solving a linear system of equations, which is sparse if the network is; thus the
resulting algorithm is extremely eﬃcient and scalable. We illustrate these ﬁndings by analyzing real
and synthetic data, including datasets from animal behavior, faculty hiring, social support networks,
and sports tournaments. We show that our method often outperforms a variety of others, in both
speed and accuracy, in recovering the underlying ranks and predicting edge directions.

Introduction

In systems of many individual entities, interactions and
their outcomes are often correlated with these entities’
ranks or positions in a hierarchy. While in most cases
these rankings are hidden from us, their presence is nev-
ertheless revealed in the asymmetric patterns of interac-
tions that we observe. For example, some social groups
of birds, primates, and elephants are organized accord-
ing to dominance hierarchies, reﬂected in patterns of re-
peated interactions in which dominant animals tend to
assert themselves over less powerful subordinates [1]. So-
cial positions are not directly visible to researchers, but
we can infer each animal’s position in the hierarchy by
observing the network of pairwise interactions. Similar
latent hierarchies have been hypothesized in systems of
endorsement in which status is due to prestige, reputa-
tion, or social position [2, 3]. For example, in academia,
universities may be more likely to hire faculty candidates
from equally or more prestigious universities [3].

In all these cases, the direction of the interactions is af-
fected by the status, prestige, or social position of the en-
tities involved. But it is often the case that even the exis-
tence of an interaction, rather than its direction, contains
some information about those entities’ relative prestige.
For example, in some species, animals are more likely to
interact with others who are close in dominance rank [4–
8]; human beings tend to claim friendships with others
of similar or slightly higher status [9]; and sports tourna-
ments and league structures are often designed to match
players or teams based on similar skill levels [10, 11]. This
suggests that we can infer the ranks of individuals in a so-
cial hierarchy using both the existence and the direction
of their pairwise interactions. It also suggests assigning

real-valued ranks to entities rather than simply ordinal
rankings, for instance in order to infer clusters of entities
with roughly equal status with gaps between them.

In this work we introduce a physically-inspired model
that addresses the problems of hierarchy inference, edge
prediction, and signiﬁcance testing. The model, which
we call SpringRank, maps each directed edge to a di-
rected spring between the nodes that it connects, and
ﬁnds real-valued positions of the nodes that minimizes
the total energy of these springs. Because this optimiza-
tion problem requires only linear algebra, it can be solved
for networks of millions of nodes and edges in seconds.

We also introduce a generative model for hierarchical
networks in which the existence and direction of edges de-
pend on the relative ranks of the nodes. This model for-
malizes the assumption that individuals tend to interact
with others of similar rank, and it can be used to create
synthetic benchmark networks with tunable levels of hi-
erarchy and noise. It can also predict unobserved edges,
allowing us to use cross-validation as a test of accuracy
and statistical signiﬁcance. Moreover, the maximum like-
lihood estimates of the ranks coincides with SpringRank
asymptotically.

We test SpringRank and its generative model version
on both synthetic and real datasets, including data from
animal behavior, faculty hiring, social support networks,
and sports tournaments. We ﬁnd that it infers accurate
rankings, provides a simple signiﬁcance test for hierarchi-
cal structure, and can predict the existence and direction
of as-yet unobserved edges. In particular, we ﬁnd that
SpringRank often predicts the direction of unobserved
edges more accurately than a variety of existing methods,
including popular spectral techniques, Minimum Viola-
tion Ranking, and the Bradley-Terry-Luce method.

∗ cdebacco@santafe.edu; Contributed equally.
† daniel.larremore@colorado.edu; Contributed equally.
‡ moore@santafe.edu

Ranking entities in a system from pairwise compar-
isons or interactions is a fundamental problem in many

Related work

8
1
0
2
 
n
u
J
 
3
1
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
4
v
2
0
0
9
0
.
9
0
7
1
:
v
i
X
r
a

contexts, and many methods have been proposed. One
family consists of spectral methods like Eigenvector Cen-
trality [12], PageRank [13], Rank Centrality [14], and the
method of Callaghan et al. [15]. These methods propose
various types of random walks on the directed network
and therefore produce real-valued scores. However, by
design these methods tend to give high ranks to a small
number of important nodes, giving us little information
about the lower-ranked nodes.
In addition, they often
require explicit regularization, adding a small term to
every element of the adjacency matrix if the graph of
comparisons is not strongly connected.

A second family focuses on ordinal rankings, i.e., per-
mutations, that minimize various penalty functions. This
family includes Minimum Violation Rank [16–18] and Se-
rialRank [19] and SyncRank [20]. Minimum Violation
Rank (MVR) imposes a uniform penalty for every viola-
tion or “upset,” deﬁned as an edge that has a direction
opposite to the one expected by the rank diﬀerence be-
tween the two nodes. Non-uniform penalties and other
generalizations are often referred to as agony methods
[21]. For common choices of the penalty function, mini-
mization can be computationally diﬃcult [17, 22], forcing
us to use simple heuristics that ﬁnd local minima.

SerialRank constructs a matrix of similarity scores be-
tween each pair of nodes by examining whether they pro-
duce similar outcomes when compared with the other
nodes, thereby relating the ranking problem to a more
general ordering problem called seriation. SyncRank is
a hybrid method which ﬁrst solves a spectral problem
based on synchronization, embeds node positions on a
half-circle in the complex plane, and then chooses among
the circular permutations of those ranks by minimizing
the number of violations as in MVR.

Random Utility Models [23], such as the Bradley-
Terry-Luce (BTL) model [24, 25], are designed to in-
fer real-valued ranks from data on pairwise preferences.
These models assign a probability to the direction of an
edge conditioned on its existence, but they do not assign
a probability to the existence of an edge. They are ap-
propriate, for instance, when an experimenter presents
subjects with choices between pairs of items, and asks
them which they prefer.

Methods like David’s Score [26] and the Colley ma-
trix [27] compute rankings from proportions of wins and
losses. The latter, which was originally developed by
making mathematical adjustments to winning percent-
ages, is equivalent to a particular case of the general
method we introduce below. Elo score [28], Go Rank [29],
and TrueSkill [30] are also widely used win-loss methods,
but these schemes update the ranks after each match
rather than taking all previous interactions into account.
This specialization makes them useful when ranks evolve
over sequential matches, but less useful otherwise.

Finally, there are fully generative models such the
Probabilistic Niche Model of ecology [31–33], models of
friendship based on social status [9], and more generally
latent space models [34] which assign probabilities to the

2

existence and direction of edges based on real-valued po-
sitions in social space. However, inference of these models
tends to be diﬃcult, with many local optima. Our gen-
erative model can be viewed as a special case of these
models for which inference is especially easy.

In the absence of ground-truth rankings, we can
compare the accuracy of these methods using cross-
validation, computing the ranks using a subset of the
edges in the network and then using those ranks to pre-
dict the direction of the remaining edges. Equivalently,
we can ask them to predict unobserved edges, such as
which of two sports teams will win a game. However,
these methods do not all make the same kinds of pre-
dictions, requiring us to use diﬀerent kinds of cross-
validation. Methods such as BTL produce probabilis-
tic predictions about the direction of an edge, i.e., they
estimate the probability one item will be preferred to
another. Fully generative models also predict the proba-
bility that an edge exists, i.e., that a given pair of nodes
in the network interact. On the other hand, ordinal rank-
ing methods such as MVR do not make probabilistic pre-
dictions, but we can interpret their ranking as a coarse
prediction that an edge is more likely to point in one
direction than another.

The SpringRank model

We represent interactions between N entities as a
weighted directed network, where Aij is the number of
interactions i → j suggesting that i is ranked above j.
This allows both ordinal and cardinal input, including
where pairs interact multiple times. For instance, Aij
could be the number of ﬁghts between i and j that i has
won, or the number of times that j has endorsed i.

Given the adjacency matrix A, our goal is to ﬁnd a
ranking of the nodes. To do so, the SpringRank model
computes the optimal location of nodes in a hierarchy by
imagining the network as a physical system. Speciﬁcally,
each node i is embedded at a real-valued position or rank
si, and each directed edge i → j becomes an oriented
spring with a nonzero resting length and displacement
si − sj. Since we are free to rescale the latent space
and the energy scale, we set the spring constant and the
resting length to 1. Thus, the spring corresponding to an
edge i → j has energy

Hij =

(si − sj − 1)2 ,

(1)

1
2

which is minimized when si − sj = 1.

This version of the model has no tunable parameters.
Alternately, we could allow each edge to have its own
rest length or spring constant, based on the strength of
each edge. However, this would create a large number of
parameters, which we would have to infer from the data
or choose a priori. We do not explore this here.

According to this model, the optimal rankings of the
N ) which minimize

nodes are the ranks s∗ = (s∗

1, . . . , s∗

the total energy of the system given by the Hamiltonian

H(s) =

AijHij =

Aij (si − sj − 1)2 .

(2)

N
(cid:88)

i,j=1

1
2

(cid:88)

i,j

Since this Hamiltonian is convex in s, we can ﬁnd s∗ by
setting ∇H(s) = 0, yielding the linear system

(cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) s∗ = (cid:2)Dout − Din(cid:3) 1 ,

(3)

where 1 is the all-ones vector and Dout and Din are di-
agonal matrices whose entries are the weighted in- and
out-degrees, Dout
j Aji. See SI
Text S1 for detailed derivations.

j Aij and Din

ii = (cid:80)

ii = (cid:80)

The matrix on the left side of Eq. (3) is not invertible.
This is because H is translation-invariant:
it depends
only on the relative ranks si − sj, so that if s∗ = {si}
minimizes H(s) then so does {si + a} for any constant a.
One way to break this symmetry is to invert the matrix
in the subspace orthogonal to its nullspace by comput-
ing a Moore-Penrose pseudoinverse. If the network con-
sists of a single component, the nullspace is spanned by
the eigenvector 1, in which case this method ﬁnds the
s∗ where the average rank (1/N ) (cid:80)
i si = (1/N )s∗ · 1 is
zero. This is related to the random walk method of [15]:
if a random walk moves along each directed edge with
rate 1
2 − ε, then
s∗ is proportional to the perturbation to the stationary
distribution to ﬁrst order in ε.

2 + ε and against each one with rate 1

In practice, it is more eﬃcient and accurate to ﬁx the
rank of one of the nodes and solve the resulting equation
using a sparse iterative solver (see SI Text S1). Faster
still, because this matrix is a Laplacian, recent results [35,
36] allow us to solve Eq. (3) in nearly linear time in M ,
the number of non-zero edges in A.

Another way to break translation invariance is to in-
i aﬀecting each

troduce an “external ﬁeld” H0(si) = 1
2 αs2
node, so that the combined Hamiltonian is

Hα(s) = H(s) +

(4)

α
2

N
(cid:88)

i=1

s2
i .

The ﬁeld H0 corresponds to a spring that attracts every
node to the origin. We can think of this as imposing a
Gaussian prior on the ranks, or as a regularization term
that quadratically penalizes ranks with large absolute
values. This version of the model has a single tunable
parameter, namely the spring constant α. Since H(s)
scales with the total edge weight M = (cid:80)
i,j Aij while
H0(s) scales with N , for a ﬁxed value of α this regular-
ization becomes less relevant as networks become more
dense and the average (weighted) degree M/N increases.
For α > 0 there is a unique s∗ that minimizes Hα,

given by

(cid:2)Dout + Din − (cid:0)A + AT (cid:1) + αI(cid:3) s∗ = (cid:2)Dout − Din(cid:3) 1 ,

3

where I is the identity matrix. The matrix on the left
side is now invertible, since the eigenvector 1 has eigen-
In the limit α → 0, we recover
values α instead of 0.
Eq. (3); the value α = 2 corresponds to the Colley ma-
trix method [27].

Minimizing H(s), or the regularized version Hα(s),
corresponds to ﬁnding the “ground state” s∗ of the
model.
In the next section we show that this corre-
sponds to a maximum-likelihood estimate of the ranks
in a generative model. However, we can use SpringRank
not just to maximize the likelihood, but to compute a
joint distribution of the ranks as a Boltzmann distribu-
tion with Hamiltonian Eq. (4), and thus estimate the
uncertainty and correlations between the ranks. In par-
ticular, the ranks si are random variables following an
N -dimensional Gaussian distribution with mean s∗ and
covariance matrix (SI Text S4)

Σ =

(cid:2)Dout + Din − (cid:0)A + AT + αI(cid:1)(cid:3)−1

.

(6)

1
β

Here β is an inverse temperature controlling the amount
of noise in the model. In the limit β → ∞, the rankings
are sharply peaked around the ground state s∗, while
for β → 0 they are noisy. As we discuss below, we can
estimate β from the observed data in various ways.

The rankings given by SpringRank Eq. (3) and its reg-
ularized form Eq. (5) are easily and rapidly computed by
standard linear solvers.
In particular, iterative solvers
that take advantage of the sparsity of the system can
ﬁnd s∗ for networks with millions of nodes and edges
in seconds. However, as deﬁned above, SpringRank is
not a fully generative model that assigns probabilities to
the data and allows for Bayesian inference. In the next
section we introduce a generative model for hierarchical
networks and show that it converges to SpringRank in
the limit of strong hierarchy.

A generative model

In this section we propose a probabilistic genera-
tive model that takes as its input a set of node ranks
s1, . . . , sN and produces a weighted directed network.
The model also has a temperature or noise parameter
β and a density parameter c. Edges between each pair
of nodes i, j are generated independently of other pairs,
conditioned on the ranks. The expected number of edges
from i to j is proportional to the Boltzmann weight of
the corresponding term in the Hamiltonian Eq. (2),

E[Aij] = c exp(−βHij) = c exp

−

(si − sj − 1)2

,

(cid:21)

(cid:20)

β
2

where the actual edge weight Aij is drawn from a Poisson
distribution with this mean. The parameter c controls
the overall density of the network, giving an expected

(5)

4

number of edges

Predicting edge directions

E[M ] =

E[Aij] = c

exp

−

(si − sj − 1)2

,

(cid:88)

i,j

(cid:88)

i,j

(cid:20)

β
2

(cid:21)

while the inverse temperature β controls the extent to
which edges respect (or defy) the ranks s. For smaller
β, edges are more likely to violate the hierarchy or to
connect distant nodes, decreasing the correlation between
the ranks and the directions of the interactions: for β = 0
the model generates a directed Erd˝os-R´enyi graph, while
in the limit β → ∞ edges only exist between nodes i, j
with si − sj = 1, and only in the direction i → j.

The Poisson distribution may generate multiple edges
between a pair of nodes, so this model generates directed
multigraphs. This is consistent with the interpretation
that Aij is the number, or total weight, of edges from i
to j. However, in the limit as E[Aij] → 0, the Poisson
distribution approaches a Bernoulli distribution, gener-
ating binary networks with Aij ∈ {0, 1}.

The likelihood of observing a network A given ranks s,

inverse temperature β, and density c is

(cid:104)

ce− β

2 (si−sj −1)2(cid:105)Aij

P (A | s, β, c) =

(cid:89)

i,j

Aij!

(cid:104)

exp

−ce− β

2 (si−sj −1)2 (cid:105)

(7)
Taking logs, substituting the maximum-likelihood value
of c, and discarding constants that do not depend on s
or β yields a log-likelihood (see Supplemental Text S2)

If hierarchical structure plays an important role in a
system, it should allow us to predict the direction of pre-
viously unobserved interactions, such as the winner of an
upcoming match, or which direction social support will
ﬂow between two individuals. This is a kind of cross-
validation, which lets us test the statistical signiﬁcance
of hierarchical structure. It is also a principled way of
comparing the accuracy of various ranking methods for
datasets where no ground-truth ranks are known.

We formulate the edge prediction question as follows:
given a set of known interactions, and given that there is
an edge between i and j, in which direction does it point?
In one sense, any ranking method provides an answer to
this question, since we can predict the direction according
to which of i or j is ranked higher based on the known
interactions. When comparing SpringRank to methods
such as SyncRank, SerialRank, and MVR, we use these
“bitwise” predictions, and deﬁne the accuracy σb as the
fraction of edges whose direction is consistent with the
inferred ranking.

But we want to know the odds on each game, not just
the likely winner—that is, we want to estimate the prob-
ability that an edge goes in each direction. A priori,
.
a ranking algorithm does not provide these probabili-
ties unless we make further assumptions about how they
depend on the relative ranks. Such assumptions yield
generative models like the one deﬁned above, where the
conditional probability of an edge i → j is

L(A | s, β) = −βH(s) − M log

(cid:20) (cid:88)

2 (si−sj −1)2 (cid:21)

e− β

, (8)

i,j

Pij(β) =

e−βHij
e−βHij + e−βHji

=

1
1 + e−2β(si−sj )

.

(9)

where H(s) is the SpringRank energy deﬁned in Eq. (2).
In the limit of large β where the hierarchical structure
is strong, the ˆs that maximizes Eq. (8), approaches the
solution s∗ of Eq. (3) that minimizes H(s). Thus the
maximum likelihood estimate ˆs of the rankings in this
model approaches the SpringRank ground state.

As discussed above, we can break translational sym-
metry by adding a ﬁeld H0 that attracts the ranks to
the origin. This is is equivalent to imposing a prior
P (s) ∝ (cid:81)N
. The maximum a posteriori
estimate ˆs then approaches the ground state s∗ of the
Hamiltonian in Eq. (4), given by Eq. (5).

i=1 e− αβ

2 (si−1)2

This model belongs to a larger family of genera-
tive models considered in ecology and network the-
ory [9, 31, 32], and more generally the class of latent space
models [34], where an edge points from i to j with proba-
bility f (si − sj) for some function f . These models typi-
cally have complicated posterior distributions with many
local optima, requiring Monte Carlo methods (e.g. [33])
In our
that do not scale eﬃciently to large networks.
case, f (si − sj) is a Gaussian centered at 1, and the pos-
terior converges to the multivariate Gaussian Eq. (6) in
the limit of strong structure.

The density parameter c aﬀects the probability that an
edge exists, but not its direction. Thus our probabilistic
prediction method has a single tunable parameter β.

Note that Pij is a logistic curve, is monotonic in the
rank diﬀerence si − sj, and has width determined by the
inverse temperature β. SpringRank has this in common
with two other ranking methods: setting γi = e2βsi re-
covers the Bradley-Terry-Luce model [24, 25] for which
Pij = γi/(γi + γj), and setting k = 2β recovers the
probability that i beats j in the Go rank [29], where
Pij = 1/(1 + e−k(si−sj )). However, SpringRank diﬀers
from these methods in how it infers the ranks from ob-
served interactions, so SpringRank and BTL make diﬀer-
ent probabilistic predictions.

In our experiments below, we test various ranking
methods for edge prediction by giving them access to
80% of the edges in the network (the training data) and
then asking them to predict the direction of the remain-
ing edges (the test data). We consider two measures of
accuracy: σa is the average probability assigned to the
correct direction of an edge, and σL is the log-likelihood
of generating the directed edges given their existence. For
simple directed graphs where Aij +Aji ∈ {0, 1}, these are

σa =

AijPij

and σL =

Aij log Pij .

(10)

(cid:88)

i,j

(cid:88)

i,j

In the multigraph case, we ask how well Pij approximates
the fraction of interactions between i and j that point
from i to j [see Eqs. (12) and (13)]. For a discussion of
other performance measures, see Supplemental Text S9.
We perform our probabilistic prediction experiments
as follows. Given the training data, we infer the ranks
using Eq. (5). We then choose the temperature parame-
ter β by maximizing either σa or σL on the training data
while holding the ranks ﬁxed. The resulting values of β,
which we denote ˆβa and ˆβL respectively, are generally
distinct (Supplemental Table S2 and Text S7). This is
intuitive, since a single severe mistake where Aij = 1 but
Pij ≈ 0 reduces the likelihood by a large amount, while
only reducing the accuracy by one edge. As a result,
predictions using ˆβa produce fewer incorrectly oriented
edges, achieving a higher σa on the test set, while predic-
tions using ˆβL will produce fewer dramatically incorrect
predictions where Pij is very low, and thus achieve higher
σL on the test set.

Statistical signiﬁcance using the ground state energy

We can measure statistical signiﬁcance using any test
statistic, by asking whether its value on a given dataset
would be highly improbable in a null model. One such
statistic is the accuracy of edge prediction using a method
such as the one described above. However, this may
become computationally expensive for cross-validation
studies with many replicates, since each fold of each
replicate requires inference of the parameter ˆβa. Here
we propose a test statistic which is very easy to com-
pute, inspired by the physical model behind SpringRank:
namely, the ground state energy. For the unregularized
version Eq. (2), the energy per edge is (see SI Text S3)

H(s∗)
M

=

1
2M

(cid:88)

i

(din

i − dout

i

) s∗

i +

(11)

1
2

.

Since the ground state energy depends on many aspects
of the network structure, and since hierarchical structure
is statistically signiﬁcant if it helps us predict edge direc-
tions, like [37] we focus on the following null model: we
randomize the direction of each edge while preserving the
total number ¯Aij = Aij + Aji of edges between each pair
of vertices. If the real network has a ground state energy
which is much lower than typical networks drawn from
this null model, we can conclude that the hierarchical
structure is statistically signiﬁcant.

This test correctly concludes that directed Erd˝os-R´enyi
graphs have no signiﬁcant structure. It also ﬁnds no sig-
niﬁcant structure for networks created using the genera-
tive model Eq. (7) with β = 0.1, i.e., when the tempera-
ture or noise level 1/β is suﬃciently large the ranks are no

5

longer relevant to edge existence or direction (Fig. S2).
However, we see in the next section that it shows sta-
tistically signiﬁcant hierarchy for a variety of real-world
datasets, showing that H(s∗) is both useful and compu-
tationally eﬃcient as a test statistic.

FIG. 1. Performance on synthetic data. (A) A synthetic
network of N = 100 nodes, with ranks drawn from a standard
Gaussian and edges drawn via the generative model Eq. (7)
for two diﬀerent values of β and average degree 5. Blue edges
point down the hierarchy and red edges point up, indicated
by arrows. (B) The accuracy of the inferred ordering deﬁned
as the Spearman correlation averaged over 100 indendepently
generated networks; error bars indicate one standard devia-
tion. (C, D) Identical to A and B but with ranks drawn from
a mixture of three Gaussians so that the nodes cluster into
three tiers (Materials and Methods). See Fig. S1 for perfor-
mance curves for Pearson correlation r.

Results on real and synthetic data

Having introduced SpringRank, an eﬃcient procedure
for inferring real-valued ranks, a corresponding gener-
ative model, a method for edge prediction, and a test
for the statistical signiﬁcance of hierarchical structure,
we now demonstrate it by applying it to both real and
synthetic data. For synthetic datasets where the ground-
truth ranks are known, our goal is to see to what extent
SpringRank and other algorithms can recover the actual
ranks. For real-world datasets, in most cases we have
no ground-truth ranking, so we apply the statistical sig-
niﬁcance test deﬁned above, and compare the ability of
SpringRank and other algorithms to predict edge direc-

tions given a subset of the interactions.

We compare SpringRank to other widely used meth-
ods: the spectral methods PageRank [13], Eigenvector
Centrality [12] and Rank Centrality [14]; Minimum Vi-
olation Ranking (MVR) [16, 17], SerialRank [19] and
SyncRank [20], which produce ordinal rankings; David’s
score [26]; and the BTL random utility model [24, 25]
using the algorithm proposed in [38], which like our
generative model makes probabilistic predictions. We
also compare unregularized SpringRank with the regu-
larized version α = 2, corresponding to the Colley ma-
trix method [27]. Unfortunately, Eigenvector Centrality,
Rank Centrality, David’s score, and BTL are undeﬁned
when the network is not strongly connected, e.g. when
there are nodes with zero in- or out-degree. In such cases
we follow the common regularization procedure of adding
low-weight edges between every pair of nodes (see Sup-
plemental Text S10).

Performance for synthetic networks

We study two types of synthetic networks, generated
by the model described above. Of course, since the log-
likelihood in this model corresponds to the SpringRank
energy in the limit of large β, we expect SpringRank to
do well on these networks, and its performance should be
viewed largely as a consistency check. But by varying the
distribution of ranks and the noise level, we can illustrate
types of structure that may exist in real-world data, and
test each algorithm’s ability to identify them.

In the ﬁrst type, the ranks are normally distributed
with mean zero and variance one (Fig. 1A). In the second
type, the ranks are drawn from an equal mixture of three
Gaussians with diﬀerent means and variances, so that
nodes cluster into high, middle, and low tiers (Fig. 1C).
This second type is intended to focus on the importance
of real-valued ranks, and to measure the performance of
algorithms that (implicitly or explicitly) impose strong
priors on the ranks when the data defy their expectations.
In both cases, we vary the amount of noise by changing
β while keeping the total number of edges constant (see
Materials and Methods).

Since we wish to compare SpringRank both to meth-
ods such as MVR that only produce ordinal rankings,
and to those like PageRank and David’s Score that pro-
duce real-valued ranks, we measure the accuracy of each
algorithm according to the Spearman correlation ρ be-
tween its inferred rank order and the true one. Results
for the Pearson correlation, where we measure the algo-
rithms’ ability to infer the real-valued ranks as opposed
to just their ordering, are shown in Fig. S1.

We ﬁnd that all the algorithms do well on the ﬁrst
type of synthetic network. As β increases so that the net-
work becomes more structured, with fewer edges (shown
in red in Fig. 1A) pointing in the “wrong” direction,
all algorithms infer ranks that are more correlated with
the ground truth. SpringRank and SyncRank have the

6

FIG. 2. Ranking the History faculty hiring network [3].
(A) Linear hierarchy diagram with nodes embedded at their
inferred SpringRank scores. Blue edges point down the hier-
archy and red edges point up. (B) Histogram of the empirical
distribution of ranks, with a vertical axis of ranks matched to
panel A. (C) Histogram of ground-state energies from 10, 000
randomizations of the network according to the null model
where edge directions are random; the dashed red line shows
the ground state energy of the empirical network depicted in
panels A and B. The fact that the ground state energy is so
far below the tail of the null model is overwhelming evidence
that the hierarchical structure is statistically signiﬁcant, with
a p-value < 10−4

.

highest accuracy, followed closely by the Colley matrix
method and BTL (Fig. 1B). Presumably the Colley ma-
trix works well here because the ranks are in fact drawn
from a Gaussian prior, as it implicitly assumes.

Results for the second type of network are more nu-
anced. The accuracy of SpringRank and SyncRank in-
creases rapidly with β with exact recovery around β = 1.
Inter-
SerialRank also performs quite well on average.

7

FIG. 3. Edge prediction accuracy over BTL. Distribution of diﬀerences in performance of edge prediction of SpringRank
compared to BTL on real and synthetic networks deﬁned as (A) edge-prediction accuracy σa Eq. (12) and (B) the conditional
log-likelihood σL Eq. (13). Error bars indicate quartiles and markers show medians, corresponding to 50 independent trials of
5-fold cross-validation, for a total of 250 test sets for each network. The two synthetic networks are generated with N = 100,
average degree 5, and Gaussian-distributed ranks as in Fig. 1A, with inverse temperatures β = 1 and β = 5. For each
experiment shown, the fractions of trials in which each method performed equal to or better than BTL are shown in Table I.
These diﬀerences correspond to prediction of an additional 1 to 12 more correct edge directions, on average.

estingly, the other methods do not improve as β in-
creases, and many of them decrease beyond a certain
point (Fig. 1D). This suggests that these algorithms be-
come confused when the nodes are clustered into tiers,
even when the noise is small enough that most edges have
directions consistent with the hierarchy.
SpringRank
takes advantage of the fact that edges are more likely
between nodes in the same tier (Fig. 1C), so the mere
existence of edges helps it cluster the ranks.

These synthetic tests suggest that real-valued ranks
capture information that ordinal ranks do not, and that
many ranking methods perform poorly when there are
substructures in the data such as tiered groups. Of
course, in most real-world scenarios, the ground-truth
ranks are not known, and thus edge prediction and other
forms of cross-validation should be used instead. We turn
to edge prediction in the next section.

Performance for real-world networks

As discussed above, in most real-world networks, we
have no ground truth for the ranks. Thus we focus on
our ability to predict edge directions from a subset of
the data, and measure the statistical signiﬁcance of the
inferred hierarchy.

We apply our methods to datasets from a diverse set
of ﬁelds, with sizes ranging up to N = 415 nodes and
up to 7000 edges (see Table S2): three North Ameri-
can academic hiring networks where Aij is the number
of faculty at university j who received their doctorate
from university i, for History (illustrated in Figs. 2A and
B), Business, and Computer Science departments [3]; two

networks of animal dominance among captive monk para-
keets [5] and one among Asian elephants [37] where Aij
is the number of dominating acts by animal i toward an-
imal j; and social support networks from two villages
in Tamil Nadu referred to (for privacy reasons) by the
pseudonyms “Tenpat.t.i” and “Alak¯apuram,” where Aij
is the number of distinct social relationships (up to ﬁve)
through which person i supports person j [2]; and 53 net-
works of NCAA Women’s and Men’s college basketball
matches during the regular season, spanning 1985-2017
(Men) and 1998-2017 (Women), where Aij = 1 if team i
beat team j. Each year’s network comprises a diﬀerent
number of matches, ranging from 747 to 1079 [39].

Together, these examples cover prestige, dominance,
and social hierarchies. In each of these domains, infer-
ring ranks from interactions is key to further analysis.
Prestige hierarchies play an unequivocal role in the dy-
namics of academic labor markets [40]; in behavioral ecol-
ogy, higher-ranked individuals in dominance hierarchies
are believed to have higher ﬁtness [1, 41]; and patterns
of aggression are believed to reveal animal strategies and
cognitive capacities [4–8]. Finally, in social support net-
works, higher ranked individuals have greater social capi-
tal and reputational standing [42, 43], particularly in set-
tings in which social support is a primary way to express
and gain respect and prestige [44].

We ﬁrst applied our ground state energy test for the
presence of statistically signiﬁcant hierarchy, rejecting
the null hypothesis with p < 10−4 in almost all cases
(e.g., for History faculty hiring, see Fig. 2C). The one ex-
ception is the Asian Elephants network for which p > 0.4.
This corroborates the original study of this network [37],
which found that counting triad motifs shows no signif-

8

FIG. 4. Probabilistic edge prediction accuracy σa of
SpringRank vs. BTL. For 50 independent trials of 5-fold
cross-validation (250 total folds per network), the values of σa
for SpringRank and BTL are shown on the vertical axis and
the horizontal axis respectively. Points above the diagonal,
shown in blue, are trials where SpringRank is more accurate
than BTL. The fractions for which each method is superior
are shown in plot legends, matching Table I.

icant hierarchy [45]. This is despite the fact that one
can ﬁnd an appealing ordering of the elephants using the
Minimum Violation Rank method, with just a few vio-
lating edges (SI Fig. S9). Thus the hierarchy found by
MVR may well be illusory.

As described above, we performed edge prediction ex-
periments using 5-fold cross-validation, where 80% of the
edges are available to the algorithm as training data, and
a test set consisting of 20% of the edges is held out (see
Materials and Methods). To test SpringRank’s ability to
make probabilistic predictions, we compare it to BTL.

We found that SpringRank outperforms BTL, both in
terms of the accuracy σa (Fig. 3A) and, for most net-
works, the log-likelihood σL (Fig. 3B). The accuracy of
both methods has a fairly broad distribution over the tri-
als of cross-validation, since in each network some subsets
of the edges are harder to predict than others when they
are held out. However, as shown in Fig. 4, in most tri-
als SpringRank was more accurate than BTL. Fig. 3A
and Table I show that SpringRank predicts edge direc-
tions more accurately in the majority of trials of cross-
validation for all nine real-world networks, where this
majority ranges from 62% for the parakeet networks to
100% for the Computer Science hiring network.

Table I shows that SpringRank also obtained a higher
log-likelihood σL than BTL for 6 of the 9 real-world net-
works. Regularizing SpringRank with α = 2 does not ap-
pear to signiﬁcantly improve either measure of accuracy
(Fig. 3). We did not attempt to tune the regularization
parameter α.

To compare SpringRank with methods that do not
make probabilistic predictions, including those that pro-

FIG. 5. Bitwise prediction accuracy σb of SpringRank
vs. SyncRank. For 50 independent trials of 5-fold cross-
validation (250 total folds per NCAA season), the fraction
of correctly predicted game outcomes σb for SpringRank and
Syncrank are shown on the vertical axis and the horizontal
axis respectively. Points above the equal performance line,
shown in blue, are trials where SpringRank is more accurate
than SyncRank; the fractions for which each method is supe-
rior are shown in plot legends.

duce ordinal rankings, we measured the accuracy σb of
“bitwise” predictions, i.e., the fraction of edges consis-
tent with the infered ranking. We found that spectral
methods perform poorly here, as does SerialRank.
In-
terestingly, BTL does better on the NCAA networks in
terms of bitwise prediction than it does for probabilistic
predictions, suggesting that it is better at rank-ordering
teams than determining their real-valued position.

We found that SyncRank is the strongest of the or-
dinal methods, matching SpringRank’s accuracy on the
parakeet and business school networks, but SpringRank
outperforms SyncRank on the social support and NCAA
networks (see Fig. S4). We show a trial-by-trial compari-
son of SpringRank and SyncRank in Fig. 5, showing that
in most trials of cross-validation SpringRank makes more
accurate predictions for the NCAA networks.

To check whether our results were dependent on the
choice of holding out 20% of the data, we repeated our
experiments using 2-fold cross-validation, i.e., using 50%
of network edges as training data and trying to predict
the other 50%. We show these results in Fig. S5. While
all algorithms are less accurate in this setting, the com-
parison between algorithms is similar to that for 5-fold
cross-validation.

Finally, the real-valued ranks found by SpringRank
shed light on the organization and assembly of real-world
networks (see Figs. S6, S7, S8, S12, and S13). For exam-
ple, we found that ranks in the faculty hiring networks
have a long tail at the top, suggesting that the most pres-
tigious universities are more separated from those below
them than an ordinal ranking would reveal. In contrast,
ranks in the social support networks have a long tail
at the bottom, suggesting a set of people who do not
have suﬃcient social status to provide support to oth-
ers. SpringRank’s ability to ﬁnd real-valued ranks makes
these distributions amenable to statistical analysis, and

9

% trials higher σa vs BTL % trials higher σL vs BTL
Type SpringRank +regularization SpringRank +regularization

Dataset
Comp. Sci. [3]
Alak¯apuram [2]
Synthetic β = 5
History [3]
NCAA Women (1998-2017) [39]
Tenpat.t.i [2]
Synthetic β = 1
NCAA Men (1985-2017) [39]

Faculty Hiring
Social Support
Synthetic
Faculty Hiring
Basketball
Social Support
Synthetic
Basketball
Parakeet G1 [5] Animal Dominance
Faculty Hiring
Parakeet G2 [5] Animal Dominance

Business [3]

100.0
99.2†
98.4
97.6†
94.4†
88.8
83.2
76.0†
71.2†
66.8†
62.0

97.2
99.6
63.2
96.8
87.0
93.6
65.2
62.3
56.8
59.2
51.6

100.0
100.0
76.4
98.8
69.1
100.0
98.4
68.5
41.2
39.2
47.6

99.6
100.0
46.4
98.8
51.0
100.0
98.4
52.4
37.2
36.8
47.2

TABLE I. Edge prediction with BTL as a benchmark. During 50 independent trials of 5-fold cross-validation (250 total
folds per network), columns show the the percentages of instances in which SpringRank Eq. (3) and regularized SpringRank
Eq. (5) with α = 2 produced probabilistic predictions with equal or higher accuracy than BTL. Distributions of accuracy
improvements are shown in Fig. 3. Center columns show accuracy σa and right columns show σL (Materials and Methods).
Italics indicate where BTL outperformed SpringRank for more than 50% of tests. † Dagger symbols indicate tests that are
shown in detail in Fig. 4. NCAA Basketball datasets were analyzed one year at a time

.

we suggest this as a direction for future work.

Conclusions

SpringRank is a mathematically principled, physics-
inspired model for hierarchical structure in networks
It yields a simple and highly
of directed interactions.
scalable algorithm, requiring only sparse linear algebra,
which enables analysis of networks with millions of nodes
and edges in seconds. Its ground state energy provides
a natural test statistic for the statistical signiﬁcance of
hierarchical structure.

While the basic SpringRank algorithm is nonparamet-
ric, a parameterized regularization term can be included
as well, corresponding to a Gaussian prior. While reg-
ularization is often required for BTL, Eigenvector Cen-
trality, and other commonly used methods (Supplemen-
tal Text S10) it is not necessary for SpringRank and our
tests indicate that its eﬀects are mixed.

We also presented a generative model that allows one
to create synthetic networks with tunable levels of hierar-
chy and noise, whose posterior coincides with SpringRank
in the limit where the eﬀect of the hierarchy is strong.
By tuning a single temperature parameter, we can use
this model to make probabilistic predictions of edge di-
rections, generalizing from observed to unobserved inter-
actions. Therefore, after conﬁrming its ability to infer
ranks in synthetic networks where ground truth ranks
are known, we measured SpringRank’s ability to to pre-
dict edge directions in real networks. We found that
in networks of faculty hiring, animal interactions, social
support, and NCAA basketball, SpringRank often makes
better probabilistic predictions of edge predictions than
the popular Bradley-Terry-Luce model, and performs as
well or better than SyncRank and a variety of other meth-
ods that produce ordinal rankings.

SpringRank is based on springs with quadratic poten-
tials, but other potentials may be of interest. For in-
stance, to make the system more tolerant to outliers while
remaining convex, one might consider a piecewise poten-
tial that is quadratic for small displacements and linear
otherwise. We leave this investigation of alternative po-
tentials to future work.

Given its simplicity, speed, and high performance, we
believe that SpringRank will be useful in a wide vari-
ety of ﬁelds where hierarchical structure appears due to
dominance, social status, or prestige.

Materials and methods

Synthetic network generation

Networks were generated in three steps. First, node
ranks splanted were drawn from a chosen distribution. For
Test 1, N = 100 ranks were drawn from a standard nor-
mal distribution, while for Test 2, 34 ranks were drawn
from each of three Gaussians, N (−4, 2), N (0, 1
2 ), and
N (4, 1) for a total of N = 102. Second, an average de-
gree (cid:104)k(cid:105) and a value of the inverse temperature β were
chosen. Third, edges were drawn generated to Eq. (7)
with c = (cid:104)k(cid:105)N/ (cid:80)
i,j exp [−(β/2)(si − sj − 1)2] so that
the expected mean degree is (cid:104)k(cid:105) (see SI Text S6).

This procedure resulted in directed networks with the
desired hierarchical structure, mean degree, and noise
level. Tests were conducted for (cid:104)k(cid:105) ∈ [5, 15], β ∈ [0.1, 5],
and all performance plots show mean and standard devi-
ations for 100 replicates.

Performance measures for edge prediction

For multigraphs, we deﬁne the accuracy of probabilis-
tic edge prediction as the extent to which Pij is a good

estimate of the fraction of interactions between i and j
that point from i to j, given that there are any edges to
predict at all, i.e., assuming ¯Aij = Aij + Aji > 0. If this
prediction were perfect, we would have Aij = ¯AijPij. We
deﬁne σA as 1 minus the sum of the absolute values of
the diﬀerence between Aij and this estimate,

σa = 1 −

1
2M

(cid:88)

i,j

(cid:12)
(cid:12)Aij − ¯Aij Pij

(cid:12)
(cid:12) ,

(12)

where M is the number of directed edges in the subset
of the network under consideration, e.g., the training or
test set. Then σa = 1 if Pij = Aij/ ¯Aij for all i, j, and
σa = 0 if for each i, j all the edges go from i to j (say)
but Pij = 0 and Pji = 1.

To measure accuracy via the conditional log-likelihood,
we ask with what probability we would get the directed
network A from the undirected network ¯A if each edge
between i and j points from i → j with probability Pij
and from j → i with probability Pji = 1−Pij. This gives

σL = log Pr[A | ¯A]
(cid:88)

=

log

(cid:19)

(cid:18)Aij + Aji
Aij

i,j

+ log

(cid:104)

P Aij
ij

[1 − Pij]Aji(cid:105)

,

(13)

y

where (cid:0)x
(cid:1) is the binomial coeﬃcient. We disregard the
ﬁrst term of this sum since it does not depend on P .
If we wish to compare networks of diﬀerent sizes as in
Fig. 3, we can normalize σL by the number of edges.
For an extensive discussion of performance metrics see
Supplementary Text S9.

Statistical signiﬁcance of ranks

We compute a standard left-tailed p-value for the sta-
tistical signiﬁcance of the ranks s∗ by comparing the
ground state energy Eq. (11) of the real network A with
the null distribution of ground state energies of an ensem-
ble of networks ˜A drawn from the null model where ¯Aij is

10

kept ﬁxed, but the direction of each edge is randomized.

p-value = Pr[H(s∗; A) ≤ H(˜s∗; ˜A)] .

(14)

In practice, this p-value is estimated by drawing many
samples from the null distribution by randomizing the
edge directions of A to produce ˜A, computing the ranks
˜s∗ from Eq. (3), and then computing the ground state
energy Eq. (11) of each.

Cross-validation tests

We performed edge prediction using 5-fold cross-
validation. In each realization, we divide the interacting
pairs i, j, i.e., those with nonzero ¯Aij = Aij + Aji, into
ﬁve equal groups. We use four of these groups as a train-
ing set, inferring the ranks and setting β to maximize σa
or σL (on the left and right of Fig. 3 respectively). We
then use the ﬁfth group as a test set, asking the algorithm
for Pij for each pair i, j in that group, and report σa or
σL on that test set. By varying which group we use as the
test set, we get 5 trials per realization: for instance, 50
realizations give us 250 trials of cross-validation. Results
for 2-fold cross-validation are reported in SI.

Acknowledgements

CDB and CM were supported by the John Tem-
pleton Foundation. CM was also supported by the
Army Research Oﬃce under grant W911NF-12-R-0012.
DBL was supported by NSF award SMA-1633747 and
the Santa Fe Institute Omidyar Fellowship. We thank
Aaron Clauset and Johan Ugander for helpful com-
ments.
Competing Interests: The authors declare
that they have no competing interests. All authors
derived the model, analyzed results, and wrote the
manuscript. CDB wrote Python implementations and
DBL wrote MATLAB implementations. Open-source
code in Python, MATLAB, and SAS/IML available at
https://github.com/cdebacco/SpringRank.

[1] C. Drews, The concept and deﬁnition of dominance in

animal behaviour, Behaviour 125, 283 (1993).

[2] E. A. Power, Social support networks and religiosity in
rural South India, Nature Human Behaviour 1, 0057
(2017).

[3] A. Clauset, S. Arbesman, D. B. Larremore, Systematic
inequality and hierarchy in faculty hiring networks, Sci-
ence Advances 1, e1400005 (2015).

[4] S. D. Cˆot´e, M. Festa-Bianchet, Reproductive success in
female mountain goats: the inﬂuence of age and social
rank, Animal Behaviour 62, 173 (2001).

[5] E. A. Hobson, S. DeDeo, Social feedback and the emer-
gence of rank in animal society, PLoS Computational Bi-
ology 11, e1004411 (2015).

[6] C. J. Dey, J. S. Quinn, Individual attributes and self-
organizational processes aﬀect dominance network struc-
ture in pukeko, Behavioral Ecology 25, 1402 (2014).
[7] C. J. Dey, A. R. Reddon, C. M. O’Connor, S. Balshine,
Network structure is related to social conﬂict in a cooper-
atively breeding ﬁsh, Animal Behaviour 85, 395 (2013).
[8] M. A. Cant, J. B. Llop, J. Field, Individual variation
in social aggression and the probability of inheritance:
theory and a ﬁeld test, The American Naturalist 167,
837 (2006).

[9] B. Ball, M. E. Newman, Friendship networks and social

status, Network Science 1, 16 (2013).

[10] S. Szymanski, The economic design of sporting contests,

Journal of Economic Literature 41, 1137 (2003).

11

[33] A. Z. Jacobs, J. A. Dunne, C. Moore, A. Clauset, Un-
tangling the roles of parasites in food webs with gener-
ative network models, arXiv preprint arXiv:1505.04741
(2015).

[34] P. D. Hoﬀ, A. E. Raftery, M. S. Handcock, Latent space
approaches to social network analysis, Journal of the
American Statistical Association 97, 1090 (2001).
[35] D. A. Spielman, S.-H. Teng, Nearly linear time algo-
rithms for preconditioning and solving symmetric, diago-
nally dominant linear systems, SIAM Journal on Matrix
Analysis and Applications 35, 835 (2014).

[36] I. Koutis, G. L. Miller, R. Peng, A nearly-m log n time
solver for SDD linear systems, Proc. 52nd Foundations
of Computer Science (FOCS) (IEEE Presss, 2011), pp.
590–598.

[37] S. de Silva, V. Schmid, G. Wittemyer, Fission–fusion pro-
cesses weaken dominance networks of female asian ele-
phants in a productive habitat, Behavioral Ecology 28,
243 (2016).

[38] D. R. Hunter, MM algorithms for generalized Bradley-

Terry models, Annals of Statistics pp. 384–406 (2004).

[39] NCAA, http://www.ncaa.org/championships/statistics

(2018).

[40] S. F. Way, D. B. Larremore, A. Clauset, Gender, pro-
ductivity, and prestige in computer science faculty hir-
ing networks, Proc. 25th Intl Conf on World Wide Web
(2016), pp. 1169–1179.

[41] B. Majolo, J. Lehmann, A. de Bortoli Vizioli, G. Schino,
Fitness-related beneﬁts of dominance in primates, Amer-
ican Journal of Physical Anthropology 147, 652 (2012).
[42] N. Lin, Social Capital: A Theory of Social Structure and
Action, vol. 19 (Cambridge University Press, 2002).
[43] K. S. Cook, M. Levi, R. Hardin, Whom can we trust?:
How groups, networks, and institutions make trust possi-
ble (Russell Sage Foundation, 2009).

[44] M. Mines, Public faces, private lives: Community and in-
dividuality in South India (University of California Press,
1994).

[45] D. Shizuka, D. B. McDonald, A social network perspec-
tive on measurements of dominance hierarchies, Animal
Behaviour 83, 925 (2012).

[46] L. Rosasco, E. D. Vito, A. Caponnetto, M. Piana,
A. Verri, Are loss functions all the same?, Neural Com-
putation 16, 1063 (2004).

[11] R. Baumann, V. A. Matheson, C. A. Howe, Anomalies
in tournament design: the madness of march madness,
Journal of Quantitative Analysis in Sports 6 (2010).
[12] P. Bonacich, Power and centrality: A family of measures,

American Journal of Sociology 92, 1170 (1987).

[13] L. Page, S. Brin, R. Motwani, T. Winograd, The PageR-
ank citation ranking: Bringing order to the web., Tech.
rep., Stanford InfoLab (1999).

[14] S. Negahban, S. Oh, D. Shah, Rank centrality: Ranking
from pairwise comparisons, Operations Research (2016).
[15] T. Callaghan, P. J. Mucha, M. A. Porter, Random walker
ranking for NCAA division IA football, American Math-
ematical Monthly 114, 761 (2007).

[16] I. Ali, W. D. Cook, M. Kress, On the minimum violations
ranking of a tournament, Management Science 32, 660
(1986).

[17] P. Slater, Inconsistencies in a schedule of paired compar-

isons, Biometrika 48, 303 (1961).

[18] M. Gupte, P. Shankar, J. Li, S. Muthukrishnan, L. Iftode,
Finding hierarchy in directed online social networks,
Proc. 20th Intl. Conf. on the World Wide Web (ACM,
2011), pp. 557–566.

[19] F. Fogel, A. d’Aspremont, M. Vojnovic, Serialrank: Spec-
tral ranking using seriation, Advances in Neural Informa-
tion Processing Systems (2014), pp. 900–908.

[20] M. Cucuringu, Sync-rank: Robust ranking, constrained
ranking and rank aggregation via eigenvector and sdp
synchronization, IEEE Transactions on Network Science
and Engineering 3, 58 (2016).

[21] E. Letizia, P. Barucca, F. Lillo, Resolution of ranking
hierarchies in directed networks, PloS one 13, e0191604
(2018).

[22] N. Tatti, Tiers for peers: a practical algorithm for discov-
ering hierarchy in weighted networks, Data Mining and
Knowledge Discovery 31, 702 (2017).

[23] K. E. Train, Discrete Choice Methods with Simulation

(Cambridge University Press, 2009).

[24] R. A. Bradley, M. E. Terry, Rank analysis of incom-
plete block designs: I. the method of paired comparisons,
Biometrika 39, 324 (1952).

[25] R. D. Luce, On the possible psychophysical laws., Psy-

chological Review 66, 81 (1959).

[26] H. A. David, Ranking from unbalanced paired-

comparison data, Biometrika 74, 432 (1987).

[27] W. N. Colley, Colley’s bias free college football rank-
ing method: The Colley matrix explained (2002).
Http://www.colleyrankings.com/matrate.pdf.

[28] A. E. Elo, The Rating of Chessplayers, Past and Present

(Arco Pub., 1978).

[29] R. Coulom, Whole-history rating: A Bayesian rating sys-
tem for players of time-varying strength, International
Conference on Computers and Games (Springer, 2008),
pp. 113–124.

[30] R. Herbrich, T. Minka, T. Graepel, Trueskill: a Bayesian
skill rating system, Advances in Neural Information Pro-
cessing Systems (2007), pp. 569–576.

[31] R. J. Williams, A. Anandanadesan, D. Purves, The prob-
abilistic niche model reveals the niche structure and role
of body size in a complex food web, PLoS ONE 5, e12092
(2010).

[32] R. J. Williams, D. W. Purves, The probabilistic niche
model reveals substantial variation in the niche structure
of empirical food webs, Ecology 92, 1849 (2011).

Supporting Information (SI)

S1. Deriving the linear system minimizing the Hamiltonian

The SpringRank Hamiltonian Eq. (2) is convex in s and we set its gradient ∇H(s) = 0 to obtain the global

minimum:

Let the weighted out-degree and in-degree be dout
written as

i = (cid:80)

j Aij and din

i = (cid:80)

j Aji, respectively. Then Eq. (S1) can be

∂H
∂si

(cid:88)

=

j

[Aij (si − sj − 1) − Aji (sj − si − 1)] = 0 .

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i − din
i

(cid:1) −

[Aij + Aji] sj = 0 .

(cid:88)

j

We now write the system of N equations together by introducing the following matrix notation. Let Dout =
diag(dout
N ) be diagonal matrices, let 1 be the N -dimensional vector of all ones.
Then Eq. (S2) becomes

N ) and Din = diag(din

1 , . . . , din

, . . . , dout

1

(cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) s = (cid:2)Dout − Din(cid:3) 1 .

This is a linear system of the type B s = b, where B = (cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) and b = (cid:2)Dout − Din(cid:3) 1. The rank
of B is at most N − 1 and more generally, if the network represented by A consists of C disconnected components, B
will have rank N − C. In fact, B has an eigenvalue 0 with multiplicity C, and the eigenvector 1 is in the nullspace.
B is not invertible, but we can only invert in the N − C-dimensional subspace orthogonal to the nullspace of B. The
family of translation-invariant solutions s∗ is therefore deﬁned by

s∗ = (cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3)−1 (cid:2)Dout − Din(cid:3) 1 ,

in which the notation [·]−1 should be taken as the Moore-Penrose pseudoinverse.

In practice, rather than constructing the pseudo-inverse, it will be more computationally eﬃcient (and for large
systems, more accurate) to solve the linear system in an iterative fashion. Since we know that solutions may be
translated up or down by an arbitrary constant, the system can be made full-rank by ﬁxing the position of an
arbitrary node 0. Without loss of generality, let sN = 0. In this case, terms that involve sN can be dropped from
Eq. (S2), yielding

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i − din
i

(cid:1) −

[Aij + Aji] sj = 0 ,

i (cid:54)= N

− (cid:0)dout

N − din
N

(cid:1) −

[AN j + AjN ] sj = 0 .

N −1
(cid:88)

j=1

N −1
(cid:88)

j=1

N −1
(cid:88)

j=1

Adding Eq. (S5) to Eq. (S6) yields

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i + dout

N − din

i − din
N

(cid:1) −

[Aij + AN j + Aji + AjN ] sj = 0 ,

(S7)

which can be written in matrix notation as

(cid:105)
(cid:104)
Dout + Din − ˚A

s = (cid:2)Dout − Din(cid:3) 1 + (cid:0)dout

N − din
N

(cid:1) 1 ,

where

˚Aij = Aij + AN j + Aji + AjN .

In this formulation, Eq. (S8) can be solved to arbitrary precision using iterative methods that take advantage of the
sparsity of ˚A. The resulting solution may then be translated by an arbitrary amount as desired.

12

(S1)

(S2)

(S3)

(S4)

(S5)

(S6)

(S8)

(S9)

13

(S12)

(S13)

S2. Poisson generative model

The expected number of edges from node i to node j is c exp

and therefore the likelihood of

(cid:104)

2 (si − sj − 1)2(cid:105)
− β

observing a network A, given parameters β, s, and c is

(cid:104)

c e− β

2 (si−sj −1)2(cid:105)Aij

P (A | s, β, c) =

(cid:89)

i,j

Aij!

(cid:104)
exp

−c e− β

2 (si−sj −1)2(cid:105)

.

(S10)

Taking logs yields

log P (A | s, β, c) =

Aij log c −

Aij (si − sj − 1)2 − log [Aij!] − ce− β

2 (si−sj −1)2

.

(S11)

(cid:88)

i,j

β
2

Discarding the constant term log [Aij!], and recognizing the appearance of the SpringRank Hamiltonian H(s), yields

L(A | s, β, c) = −βH(s) +

Aij log c −

ce− β

2 (si−sj −1)2

.

(cid:88)

i,j

(cid:88)

i,j

Taking ∂L/∂c and setting it equal to zero yields

ˆc =

(cid:80)

i,j Aij
2 (si−sj −1)2 ,

(cid:80)

i,j e− β

which has the straightforward interpretation of being the ratio between the number of observed edges and the expected
number of edges created in the generative process for c = 1. Substituting in this solution and letting M = (cid:80)
i,j Aij
yields

L(A | s, β) = −βH(s) + M log ˆc −

ˆc e− β

2 (si−sj −1)2

(cid:88)

i,j





(cid:88)

i,j

M
β





(cid:88)

i,j





 .

= −βH(s) + M log M − M log

e− β

2 (si−sj −1)2

 − M .

(S14)

The terms M log M and M may be neglected since they do not depend on the parameters, and we divide by β,
yielding a log-likelihood of

L(A | s, β) = −H(s) −

log

e− β

2 (si−sj −1)2

(S15)

(cid:3) where (cid:104)·(cid:105)E denotes
Note that the SpringRank Hamiltonian may be rewritten as H(s) = M (cid:2) 1
the average over elements in the edge set E. In other words, H(s) scales with M and the square of the average spring
length. This substitution for H(s) allows us to analyze the behavior of the log-likelihood

2 (cid:104)Aij(si − sj − 1)2(cid:105)E

L(A | s, β) = −M

(cid:68)

Aij (si − sj − 1)2(cid:69)

+

log

1
β

E

e− β

2 (si−sj −1)2





(cid:88)

i,j










.






1
2

(S16)

Inside the logarithm there are N 2 terms of ﬁnite value, so that the logarithm term is of order O( log N
β ). Thus, for well-
resolved hierarchies, i.e. when β is large enough that the sampled edges consistently agree with the score diﬀerence
between nodes, the maximum likelihood ranks ˆs approach the ranks s∗ found by minimizing the Hamiltonian. In
practice, exactly maximizing the likelihood would require extensive computation, e.g. by using local search heuristic
or Markov chain Monte Carlo sampling.

S3. Rewriting the energy

The Hamiltonian Eq. (2) can be rewritten as

2 H(s) =

Aij (si − sj − 1)2

N
(cid:88)

i,j=1

N
(cid:88)

i,j=1

N
(cid:88)

=

=

N
(cid:88)

=

i

Aij

(cid:0)s2

i + s2

j − 2sisj + 1 − 2si + 2sj

(cid:1)

N
(cid:88)

s2
i

Aij +

N
(cid:88)

N
(cid:88)

s2
j

j

i=1

Aij − 2

(cid:88)

(cid:88)

si

Aijsj + M

i

j

i

j=1

−2

N
(cid:88)

N
(cid:88)

si

i=1

j=1

Aij + 2

N
(cid:88)

N
(cid:88)

sj

j=1

i=1

Aij

s2
i

(cid:0)dout

i + din
i

(cid:1) − 2

si

(cid:0)dout

i − din
i

(cid:1) + M − 2

(cid:88)

(cid:88)

si

Aijsj .

(cid:88)

i

i

j

From Eq. (S2) we have

(cid:88)

s2
i

(cid:0)dout

i + din
i

(cid:1) −

(cid:88)

si

(cid:0)dout

i − din
i

(cid:1) =

(cid:88)

(cid:88)

si

[Aij + Aji] sj .

j

i

i

j

We can substitute this into Eq. (S17)
(cid:88)

(cid:88)

2H(s) =

si

[Aij + Aji] sj −

si

(cid:0)dout

i − din
i

(cid:1) + M − 2

(cid:88)

(cid:88)

si

Aijsj

(cid:88)

i

i

j

j

si

(cid:0)din

i − dout

i

(cid:1) + M

hisi + M ,

=

=

i
(cid:88)

i
(cid:88)

i

where hi ≡ din

i − dout

i

.

S4. Ranks distributed as a multivariate Gaussian distribution

Assuming that the ranks are random variables distributed as a multivariate Gaussian distribution of average ¯s and

covariance matrix Σ, we have:

We can obtain this formulation by considering a Boltzman distribution with the Hamiltonian Eq. (2) as the energy
term and inverse temperature β so that

P (s) ∝ exp

−

(s − ¯s)(cid:124)Σ−1(s − ¯s)

.

(cid:18)

1
2

(cid:19)



P (s) ∝ exp

−

Aij (si − sj − 1)2

 .



N
(cid:88)

i,j=1

β
2

1
2

Manipulating the exponent of Eq. (S20) yields

(s − ¯s)T Σ−1(s − ¯s) =

(cid:0)sT Σ−1s − 2sT Σ−1¯s + ¯sT Σ−1¯s(cid:1) ,

1
2

whereas the parallel manipulation of Eq. (S21) yields

β
2

N
(cid:88)

i,j=1

β
2

Aij (si − sj − 1)2 =

(cid:2)sT (cid:0)Dout + Din − AT − A(cid:1) s + 2 sT (Din − Dout)1 + M (cid:3) ,

(S23)

14

(S17)

(S18)

(S19)

(S20)

(S21)

(S22)

where 1 is a vector of ones and Din are diagonal matrices whose entries are the in- and out-degrees, Dout
and Din
on s because irrelevant when accounting for normalization, we obtain:

j Aij
i,j Aij. Comparing these last two expressions and removing terms that do not depend

j Aji and M = (cid:80)

ii = (cid:80)

ii = (cid:80)

Σ = 1
β

(cid:0)Dout + Din − AT − A(cid:1)−1

and ¯s = β Σ (cid:0)Dout − Din(cid:1) 1 = s∗ .

(S24)

S5. Bayesian SpringRank

Adopting a Bayesian approach with a factorized Gaussian prior for the ranks, we obtain that the s that maximizes
the posterior distribution is the one that minimizes the regularized SpringRank Hamiltonian Eq. (4), i.e. the s that
solves the linear system Eq. (5). In fact, deﬁning:

P (s) = Z −1(β, α)

e−β α

2 (si−1)2

= Z −1(β, α)

e−β αH0(si) ,

(S25)

(cid:89)

i∈V

is a normalization constant that depends on α and β, and following the same steps as

where Z(β, α) =
before we get:

(cid:105)N/2

(cid:104) 2π
β α

log P (s | A) =

log P (Aij | s) − βα

H0(si) + log (Z (β, α) )

(cid:89)

i∈V

(cid:88)

i,j

(cid:34)

(cid:88)

i∈V

(cid:35)

(cid:88)

i∈V

= − β

H(s) + α

H0(si)

+ C ,

(S26)

where C is a constant that does not depend on the parameters, and thus may be ignored when maximizing log P (s | A).

The parameter c included in the generative model (7) controls for network’s sparsity. We can indeed ﬁx it so to

obtain a network with a desired expected number of edges (cid:104)M (cid:105) as follows:

For a given vector of ranks s and inverse temperature β, the c realizing the desired sparsity will then be:

S6. Fixing c to control for sparsity

(cid:104)M (cid:105) ≡

(cid:104)Aij(cid:105) = c

e− β

2 (si−sj −1)2

.

(cid:88)

i,j

(cid:88)

i,j

c =

(cid:80)

i,j e− β

(cid:104)M (cid:105)
2 (si−sj −1)2 =

(cid:104)k(cid:105)N
2 (si−sj −1)2 ,

(cid:80)

i,j e− β

where (cid:104)k(cid:105) is the expected node degree (cid:104)k(cid:105) = (cid:80)N
i=1
model with Bernoulli distribution.

(cid:2)din

i + dout

i

(cid:3). Similar arguments apply when considering a generative

S7. Comparing optimal β for predicting edge directions

In the main text, (12) and Eq. (13) deﬁne the accuracy of edge prediction, in terms of the number of edges predicted
correctly in each direction and the log-likelihood conditioned on the undirected graph. Here we compute the optimal
values of β for both notions of accuracy. In both computations that follow, the following two facts will be used:

and

ij(β) = 2 (si − sj) e−2β(si−sj ) P 2
P (cid:48)

ij(β) ,

1 =

Pij(β)
1 − Pij(β)

e−2β(si−sj ) .

15

(S27)

(S28)

(S29)

A. Choosing β to optimize edge direction accuracy

We take the derivative of Eq. (12) with respect to β, set it equal to zero, and solve as follows.

0 ≡

∂σa(β)
∂β

=

∂
∂β



1 −

1
2m

(cid:88)

i,j

|Aij − (Aij + Aji) Pij(β)|

 .

(S30)



In preparation to take the derivatives above, note that P (cid:48)
takes one sign, the (j, i) term takes the opposite sign,

ij(β) = −P (cid:48)

ji(β) and that whenever the (i, j) term of σa(β)

Without loss of generality, assume that the (i, j) term is positive and the (j, i) term is negative. This implies that

Aij − (Aij + Aji) Pij(β) = − [Aji − (Aij + Aji) Pji(β)] .

∂
∂β

∂
∂β

|Aij − (Aij + Aji) Pij(β)| = − (Aij + Aji) P (cid:48)

ij(β) ,

|Aji − (Aij + Aji) Pji(β)| = − (Aij + Aji) P (cid:48)

ij(β) .

and

∂
∂β

In other words, the derivatives of the (i, j) and (j, i) terms are identical, and the sign of both depends on whether the
quantity [Aij − (Aij + Aji)Pij(β)] is positive or negative. We can make this more precise by directly including the
sign of the (i, j) term, and by using Eq. (S28), to ﬁnd that

|Aij − (Aij + Aji) Pij(β)| = −2 (Aij + Aji) (si − sj) e−2β(si−sj ) P 2

ij × sign(cid:8)Aij − (Aij + Aji) Pij(β)(cid:9) .

(S34)

Expanding P 2

ij and reorganizing yields

∂
∂β

|Aij − (Aij + Aji) Pij(β)| = −2

(Aij + Aji) (si − sj)
2 cosh [2β (si − sj)] + 2

× sign(cid:8)Aij − (Aij + Aji) Pij(β)(cid:9) .

(S35)

Combining terms (i, j) and (j, i), the optimal inverse temperature for local accuracy ˆβa is that which satisﬁes

N
(cid:88)

0 =

(i,j)∈U (E)

(Aij + Aji) (si − sj)
(cid:105)
2 ˆβa (si − sj)

(cid:104)

cosh

+ 1

× sign(cid:8)Aij − (Aij + Aji) Pij( ˆβa)(cid:9) ,

(S36)

which may be found using standard root-ﬁnding methods.

B. Choosing β to optimize the conditional log likelihood

We take the derivative of Eq. (13) with respect to β, set it equal to zero, and partially solve as follows.

0 ≡

∂σL(β)
∂β

=

∂
∂β





(cid:88)

log

i,j

(cid:19)

(cid:18)Aij + Aji
Aij

+ log

(cid:104)

Pij(β)Aij [1 − Pij(β)]Aji(cid:105)



 .

Combining the (i, j) and (j, i) terms, we get
(cid:18)Aij + Aji
Aij

∂
∂β

0 ≡

(cid:88)

log

(cid:19)

(i,j)∈U (E)

+ log

(cid:19)

(cid:18)Aij + Aji
Aji

(cid:20) Aij
Pij(β)

−

Aji
1 − Pij(β)

(cid:21) ∂Pij(β)
∂β

(i,j)∈U (E)

(cid:88)

(cid:88)

=

=

(i,j)∈U (E)

2 (si − sj) [Aij − (Aij + Aji) Pij(β)]

Pij(β)
1 − Pij(β)

e−2β(si−sj ) .

+ [ Aij log Pij(β) + Aji log [1 − Pij(β)] ]

16

(S31)

(S32)

(S33)

(S37)

(S38)

Applying both Eq. (S28) and Eq. (S29), the optimal inverse temperature for the conditional log likelihood ˆβL is that
which satisﬁes

0 =

(cid:88)

2 (si − sj)

(cid:104)

(cid:105)
Aij − (Aij + Aji) Pij( ˆβL)

,

(i,j)∈U (E)

which, like Eq. (S36) may be found using standard root-ﬁnding methods. Comparing equations Eq. (S36) and
Eq. (S39), we can see that the values of β that maximize the two measures may, in general, be diﬀerent. Table S2
shows for optimal values for ˆβL and ˆβa for various real-world datasets.

17

(S39)

S8. Bitwise accuracy σb

Some methods provide rankings but do not provide a model to estimate Pij, meaning that Eq. (12) and Eq. (13)
cannot be used. Nevertheless, such methods still estimate one bit of information about each pair (i, j): whether the
majority of the edges are from i to j or vice versa. This motivates the use of a bitwise version of σa, which we call
σb,

σb = 1 −

Θ (si − sj) Θ(Aji − Aij) ,

(S40)

1
N 2 − t

(cid:88)

i,j

where Θ(x) = 1 if x > 0 and Θ(x) = 0 otherwise, and N is the number of nodes and t is the number of instances in
which Aij = Aji; there are N 2 − t total bits to predict. Results in terms of this measure on the networks considered
in the main text are shown in Figure S4. In the special case that the network is unweighted (A is a binary adjacency
matrix) and there are no bi-directional edges (if Aij = 1, then Aji = 0), then 1 − σb is the fraction of edges that
violate the rankings in s. In other words, for this particular type of network, 1 − σb is the minimum violations rank
penalty normalized by the total number of edges in the network, i.e., 1
M

i,j Θ(si − sj) Aji.

(cid:80)

S9. Performance metrics

When evaluating the performance of a ranking algorithm in general one could consider a variety of diﬀerent measures.
One possibility is to focus on the ranks themselves, rather than the outcomes of pairwise interactions, and calculate
correlation coeﬃcients as in Fig. 1; this is a valid strategy when using synthetic data thanks to the presence of ground
truth ranks, but can only assess the performance with respect of the speciﬁc generative process used to generate
the pairwise comparisons, as we point out in the main text. This strategy can also be applied for comparisons with
observed real world ranks, as we did in Table S11 and it has been done for instance in [19, 20] to compare the ranks
with those observed in real data in sports. However, the observed ranks might have been derived from a diﬀerent
process than the one implied by the ranking algorithm considered. For instance, in the faculty hiring networks, popular
ranking methods proposed by domain experts for evaluating the prestige of universities do not consider interactions
between institutions, but instead rely on a combination of performance indicators such as ﬁrst-year student retention
or graduation rates. The correlation between observed and inferred ranks should thus be treated as a qualitative
indicator of how well the two capture similar features of the system, such as prestige, but should not be used to
evaluate the performance of a ranking algorithm.

Alternatively, one can look at the outcomes of the pairwise comparisons and relate them to the rankings of the
nodes involved as in Eqs. (12) and (13) for testing prediction performance. A popular metric of this type is the number
of violations (also called upsets), i.e., outcomes where a higher ranked node is defeated by a lower ranked one. This
is very similar to the bitwise accuracy deﬁned in (S40), indeed when there are no ties and two nodes are compared
only once, then they are equivalent. These can be seen as low-resolution or coarse-grained measures of performance:
for each comparison predict a winner, but do not distinguish between cases where the winner is easy to predict and
cases where there is almost a tie. In particular, an upset between two nodes ranked nearby counts as much as an
upset between two nodes that are far away in the ranking. The latter case signals a much less likely scenario. In order
to distinguish these two situations, one can penalize each upset by the nodes’ rank diﬀerence elevated to a certain
power d. This is what the agony function does [18] with the exponent d treated as a parameter to tune based on the
application. When d = 0 we recover the standard number of unweighted upsets.

Note that optimization of agony is often used as a non-parametric approach to detect hierarchies [21], in particular
for ordinal ranks. For ordinal ranks, rank diﬀerences are integer-valued and equal to one for adjacent-ranked nodes,
yet for real-valued scores this is not the case. Therefore the result of the agony minimization problem can vary

18

widely between ordinal and real valued ranking algorithms. (We note that the SpringRank objective function, i.e.,
the Hamiltonian in Eq. (2), can be considered a kind of agony. However, since we assume that nearby pairs are more
likely to interact, it is large for a edge from i to j if i is ranked far above or far below j, and more speciﬁcally whenever
si is far from sj + 1.)

In contrast to the coarse prediction above—which competitor is more likely to win?—we require, when possible,
more precise predictions in Eqs. (12) and (13), which ask how much more likely is one competitor to win? This,
however, requires the ranking algorithm to provide an estimate of Pij, the probability that i wins over j, which is
provided only by BTL and SpringRank; all other methods compared in this study provide orderings or embeddings
without probabilistic predictions.

The conditional log-likelihood σL as deﬁned in Eq. (13) can be seen as a Log Loss often used as a classiﬁcation loss
function [46] in statistical learning. This type of function heavily penalizes ranking algorithms that are very conﬁdent
about an incorrect outcome, e.g. when the predicted Pij is close to 1, i very likely to win over j, but the observed
outcome is that j wins over i. For this reason, this metric is more sensitive to outliers, as when in sports a very
strong team loses against one at the bottom of the league. The accuracy σb deﬁned in Eq. (12) focuses instead in
predicting the correct proportions of wins/losses between two nodes that are matched in several comparisons. This
is less sensitive to outliers, and in fact if Pij is close but not exactly equal to 1, for a large number of comparisons
between i and j, we would expect that j should indeed win few times, e.g. if Pij = 0.99 and i, j are compared 100
times, σa is maximized when i wins 99 times and j wins once.

S10. Parameters used for regularizing ranking methods

When comparing SpringRank to other methods, we need to deal with the fact that certain network structures cause
other methods to fail to return any output. Eigenvector Centrality cannot, for example, be applied to directed trees,
yet this is precisely the sort of structure that one might expect when hierarchy becomes extreme.

More generally, many spectral techniques fail on networks that are not strongly connected, i.e., where it is not the
case that one can reach any node from any other by moving along a path consistent with the edge directions, since
in that case the adjacency matrix is not irreducible and the Perron-Frobenius theorem does not apply. In particular,
nodes with zero out-degree—sometimes called “dangling nodes” in the literature [13]—cause issues for many spectral
methods since the adjacency matrix annihilates any vector supported on such nodes. In contrast, the SpringRank
optimum given by Eq. (3) is unique up to translation whenever the network is connected in the undirected sense, i.e.,
whenever we can reach any node from any other by moving with or against directed edges.

A diﬀerent issue occurs in the case of SyncRank. When edges are reciprocal in the sense that an equal number
of edges point in each direction, they eﬀectively cancel out. That is, if Aij = Aji, the corresponding entries in the
SyncRank comparison matrix will be zero, Cij = Cji = 0, as if i and j were never compared at all. As a result, there
can be nodes i such that Cij = Cji = 0 for all j. While rare, these pathological cases exist in real data and during
cross-validation tests, causing the output of SyncRank to be undeﬁned.

In all these cases, regularization is required. Our regularized implementations of ﬁve ranking methods are described

below:

• Regularized Bradley-Terry-Luce (BTL). If there exist dangling nodes, the Minimization-Maximization
algorithm to ﬁt the BTL model to real data proposed in [38] requires a regularization. In this case we set the
total number of out-edges dout
i = 10−6 for nodes that would have di = 0 otherwise. This corresponds to Wi in
Eq.(3) of [38].

• Regularized PageRank. If there exist dangling nodes, we add an edge of weight 1/N from each dangling
node to every other node in the network. For each dataset we tried three diﬀerent values of the teleportation
parameter, α ∈ {0.4, 0.6, 0.8}, and reported the best results of these three.

• Regularized Rank Centrality. If there exist dangling nodes, we use the regularized version of the algorithm

presented in Eq. (5) of [14] with (cid:15) = 1.

• Regularized SyncRank. If there are nodes whose entries in the comparison matrix C are zero, we add a

small constant (cid:15) = 0.001 to the entries of H in Eq. (13) of Ref. [20], so that D is invertible.

• Regularized Eigenvector Centrality. If the network is not strongly connected, we add a weight of 1/N to

every entry in A and then diagonalize.

19

S11. Supplemental Tables

Comp. Sci. SpringRank MVR US News NRC Eig. C. PageRank
0.57
SpringRank
0.48
MVR
0.41
US News
0.41
NRC
0.74
Eig. C.
-
PageRank

0.80 0.72
0.81 0.73
- 0.73
0.73
-
0.69 0.68
0.41 0.41

0.84
0.80
0.69
0.68
-
0.74

0.96
-
0.81
0.73
0.80
0.48

-
0.96
0.80
0.72
0.84
0.57

Business
SpringRank
MVR
US News
NRC
Eig. C.
PageRank

History
SpringRank
MVR
US News
NRC
Eig. C.
PageRank

SpringRank MVR US News NRC Eig. C. PageRank
0.75
0.74
0.69
0.72
0.60
-
-
-
0.72
0.68
-
0.60

0.98
-
0.72
-
0.92
0.69

0.92
0.92
0.68
-
-
0.72

-
0.98
0.74
-
0.92
0.75

-
-
-
-
-
-

SpringRank MVR US News NRC Eig. C. PageRank
0.69
0.57
0.51
0.44
0.88
-

0.86 0.66
0.86 0.65
- 0.66
0.66
-
0.72 0.59
0.51 0.44

0.86
0.77
0.72
0.59
-
0.88

0.95
-
0.86
0.65
0.77
0.57

-
0.95
0.86
0.66
0.86
0.69

TABLE S1. Pearson correlation coeﬃcients between various rankings of faculty hiring networks. All coeﬃcients are statistically
signiﬁcant (p < 10−9). SpringRank is most highly correlated with Minimum Violations Ranks across all three faculty hiring
networks. Among US News and NRC rankings, SpringRank is more similar to US News. Values for US News and NRC were
[3] for comparison to the ranks available at the same time that the faculty hiring data were collected. The
drawn from Ref.
NRC does not rank business departments.

N M H/M Acc. σa ˆβL

ˆβa

Type
DataSet
Anim. Dom. 21 838 0.174 0.930
Parakeet G1 [5]
Anim. Dom. 19 961 0.193 0.932
Parakeet G2 [5]
0.078 0.923
Asian Elephants [37] Anim. Dom. 20 23
112 7353 0.251 0.881
Business [3]
Fac. Hiring
205 4033 0.220 0.882
Computer Science [3] Fac. Hiring
Fac. Hiring
History [3]
144 3921 0.186 0.909
Soc. Support 415 2497 0.222 0.867
Alak¯apuram [2]
Soc. Support 361 1809 0.241 0.858
Tenpat.t.i [2]

0.008 (0.089)
2.70 6.03 76 (9.1%) / 42
0.011 (0.139)
2.78 18.12 75 (7.8%) / 36
2.33 3.44 2 (8.7%) / 0
0.001 (0.040)
2.04 3.14 1171 (15.9%) / 808 0.019 (0.119)
2.23 8.74 516 (12.8%) / 255 0.013 (0.105)
2.39 5.74 397 (10.1%) / 227 0.012 (0.119)
1.98 7.95 347 (13.9%) / 120 0.011 (0.079)
1.89 8.20 262 (14.5%) / 120 0.012 (0.082)

Viol. (%) / Bound Wt. viol. (per viol.) Depth p-value
2.604 < 10−4
1.879 < 10−4
3.000 0.4466
2.125 < 10−4
2.423 < 10−4
2.234 < 10−4
3.618 < 10−4
3.749 < 10−4

TABLE S2. Statistics for SpringRank applied to real-world networks. Column details are as follows: N is the number
of nodes; M is the number of edges; H/m is the ground state energy per edge; Accuracy σa refers to accuracy in 5-fold
cross-validation tests using temperature ˆβa; ˆβL and ˆβa are temperatures optimizing edge prediction accuracies σL and σa
respectively; Violations refers to the number of edges that violate the direction of the hierarchy as a number, as a percentage
of all edges, with a lower bound provided for reference, computed as the number of unavoidable violations due to reciprocated
edges; Weighted violations are the sum of each violation weighted by the diﬀerence in ranks between the oﬀending nodes; Depth
is smax − smin; p-value refers to the null model described in the Materials and Methods. Relevant performance statistics for
NCAA datasets (53 networks) are reported elsewhere; see Fig. S3.

S12. Supplemental Figures

20

FIG. S1. Performance (Pearson correlation) on synthetic data. Tests were performed as in Fig. 1, but here performance
is measured using Pearson correlation. This favors algorithms like SpringRank and BTL, that produce real-valued ranks, over
ordinal ranking schemes like Minimum Violation Ranking which are not expected to recover latent positions.
(A) Linear
hierarchy diagrams show latent ranks splanted of 100 nodes, drawn from a standard normal distribution, with edges drawn
via the generative model Eq. (7) for indicated β (noise) values. Blue edges point down the hierarchy and red edges point
up, indicated by arrows. (B) Mean accuracies ± one standard deviation (symbols ± shading) are measured as the Pearson
correlation between method output and splanted for 100 replicates. (C, D) Identical to A and B but for hierarchies of N = 102
nodes divided into three tiers. All plots have mean degree 5; see Fig. 1 for performance curves for Spearman correlation r. See
Materials and Methods for synthetic network generation.

(a) US HS

(b) US BS

(c) US CS

(d) Tenpat.t.i

(e) Alak¯apuram

(f) parakeet G1

(g) parakeet G2

(h) planted β = 5.0

(i) planted β = 0.1

(j) Asian elephant

(k) ER
N = 100, (cid:104)k(cid:105) = 3

FIG. S2. Statistical signiﬁcance testing using the null model distribution of energies. Results are 1000 realizations
of the null model where edge directions are randomized while keeping the total number of interactions between each pair ﬁxed,
for real and synthetic networks: a-c) US History (HS), Business (BS) and Computer Science (CS) faculty hiring networks [3];
d-e) social support networks of two Indian villages [2] considering 5 types of interactions (see main manuscript); f,g) aggression
network of parakeet Group 1 and 2 (as in [5]); h,i) planted network using SpringRank generative model with N = 100 and
mean degree (cid:104)k(cid:105) = 5, Gaussian prior for the ranks with average µ = 0.5 and variance 1 (α = 1/β) and two noise levels β = 5.0
and β = 0.1; j) dominance network of asian elephants [37]; k) Erd˝os-R´enyi directed random network with N = 100 and (cid:104)k(cid:105) = 3.
The vertical line is the energy obtained on the real network. In all but the last two cases we reject the null hypothesis that
edge directions are independent of the ranks, and conclude that the hierarchy is statistically signiﬁcant.

21

FIG. S3. Edge prediction accuracy over BTL for NCAA basketball datasets. Distribution of diﬀerences in performance
of edge prediction of SpringRank compared to BTL on NCAA College Basketball regular season matches for (top) Women and
(middle) Men, deﬁned as (left) the probabilistic edge-prediction accuracy σa Eq. (12) and (right) the conditional log-likelihood
σL Eq. (13). Error bars indicate quartiles and markers show medians, corresponding to 50 independent trials of 5-fold cross-
validation, for a total of 250 test sets for each dataset. The bottom plot is obtained by considering the distributions over all
the seasons together. In terms of number of correctly predicted outcomes, SpringRank correctly predicts on average 8 to 16
more outcomes than BTL for each of the 20 Women NCAA seasons and up to 12 more outcomes for each of the 33 Men NCAA
seasons; for the latter dataset, BTL has an average better prediction in 3 out of the 33 seasons. The number of matches played
per season in the test set varies from the past to the most recents years from 747 to 1079.

22

FIG. S4. Bitwise edge direction prediction. Symbols show medians of bitwise edge prediction accuracies Eq. (S40) over
50 realization of 5-fold cross-validation (for a total of 250 trials) compared with the median accuracy for SyncRank; error bars
indicate quartiles. Thus, points above the dashed line at zero indicate better predictions than SyncRank, while values below
indicate that SyncRank performed better.

23

FIG. S5. Edge prediction accuracy with 2-fold cross-validation. Top: the accuracy of probabilistic edge prediction
of SpringRank compared to the median accuracy of BTL on real and synthetic networks deﬁned as (top left) edge-prediction
accuracy σa Eq. (12) and (top right) the conditional log-likelihood σL Eq. (13); (bottom) bitwise edge prediction accuracies σb
Eq. (S40) of SpringRank and other algorithms compared with the median accuracy of SyncRank. Error bars indicate quartiles
and markers show medians, corresponding to 50 independent trials of 2-fold cross-validation, for a total of 100 test sets for
each network. The two synthetic networks are generated with N = 100, average degree 5, and Gaussian-distributed ranks as
in Fig. 1A, with inverse temperatures β = 1 and β = 5. Notice that these results are similar those of Fig. 3, obtained using
5-fold cross-validation.

Computer Science

24

FIG. S6. Summary of SpringRank applied to Computer Science faculty hiring network [3]. (top-left) A linear
hierarchy diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and
red edges point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned)
and the horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency
matrix; blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of
statistical signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized
samples: the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted,
ordered by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means
algorithm.

History

25

FIG. S7. Summary of SpringRank applied to History faculty hiring network [3]. (top-left) A linear hierarchy diagram
showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up.
(top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal
axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red
dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance
test with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted
line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank,
from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Business

26

FIG. S8. Summary of SpringRank applied to Business faculty hiring network [3]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Asian Elephants

27

FIG. S9. Summary of SpringRank applied to Asian Elephants network [37]. (top-left) A linear hierarchy diagram
showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up.
(top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal
axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red
dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance
test with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted
line is the ground state energy obtained on the observed real network.

Parakeet G1

28

FIG. S10. Summary of SpringRank applied to Parakeet G1 network [5]. (top-left) A linear hierarchy diagram showing
inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up. (top-
middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal axis
is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red dots
represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance test
with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted line
is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank, from
top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Parakeet G2

29

FIG. S11. Summary of SpringRank applied to Parakeet G2 network [5]. (top-left) A linear hierarchy diagram showing
inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up. (top-
middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal axis
is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red dots
represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance test
with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted line
is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank, from
top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Tenpat.t.i

30

FIG. S12. Summary of SpringRank applied to Tenpat.t.i social support network [2]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Alak¯apuram

31

FIG. S13. Summary of SpringRank applied to Alak¯apuram social support network [2]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

A physical model for eﬃcient ranking in networks

Caterina De Bacco,1, 2, ∗ Daniel B. Larremore,3, 4, 2, † and Cristopher Moore2, ‡
1Data Science Institute, Columbia University, New York, NY 10027, USA
2Santa Fe Institute, Santa Fe, NM 87501, USA
3Department of Computer Science, University of Colorado, Boulder, CO 80309, USA
4BioFrontiers Institute, University of Colorado, Boulder, CO 80303, USA

We present a physically-inspired model and an eﬃcient algorithm to infer hierarchical rankings of
nodes in directed networks. It assigns real-valued ranks to nodes rather than simply ordinal ranks,
and it formalizes the assumption that interactions are more likely to occur between individuals with
similar ranks.
It provides a natural statistical signiﬁcance test for the inferred hierarchy, and it
can be used to perform inference tasks such as predicting the existence or direction of edges. The
ranking is obtained by solving a linear system of equations, which is sparse if the network is; thus the
resulting algorithm is extremely eﬃcient and scalable. We illustrate these ﬁndings by analyzing real
and synthetic data, including datasets from animal behavior, faculty hiring, social support networks,
and sports tournaments. We show that our method often outperforms a variety of others, in both
speed and accuracy, in recovering the underlying ranks and predicting edge directions.

Introduction

In systems of many individual entities, interactions and
their outcomes are often correlated with these entities’
ranks or positions in a hierarchy. While in most cases
these rankings are hidden from us, their presence is nev-
ertheless revealed in the asymmetric patterns of interac-
tions that we observe. For example, some social groups
of birds, primates, and elephants are organized accord-
ing to dominance hierarchies, reﬂected in patterns of re-
peated interactions in which dominant animals tend to
assert themselves over less powerful subordinates [1]. So-
cial positions are not directly visible to researchers, but
we can infer each animal’s position in the hierarchy by
observing the network of pairwise interactions. Similar
latent hierarchies have been hypothesized in systems of
endorsement in which status is due to prestige, reputa-
tion, or social position [2, 3]. For example, in academia,
universities may be more likely to hire faculty candidates
from equally or more prestigious universities [3].

In all these cases, the direction of the interactions is af-
fected by the status, prestige, or social position of the en-
tities involved. But it is often the case that even the exis-
tence of an interaction, rather than its direction, contains
some information about those entities’ relative prestige.
For example, in some species, animals are more likely to
interact with others who are close in dominance rank [4–
8]; human beings tend to claim friendships with others
of similar or slightly higher status [9]; and sports tourna-
ments and league structures are often designed to match
players or teams based on similar skill levels [10, 11]. This
suggests that we can infer the ranks of individuals in a so-
cial hierarchy using both the existence and the direction
of their pairwise interactions. It also suggests assigning

real-valued ranks to entities rather than simply ordinal
rankings, for instance in order to infer clusters of entities
with roughly equal status with gaps between them.

In this work we introduce a physically-inspired model
that addresses the problems of hierarchy inference, edge
prediction, and signiﬁcance testing. The model, which
we call SpringRank, maps each directed edge to a di-
rected spring between the nodes that it connects, and
ﬁnds real-valued positions of the nodes that minimizes
the total energy of these springs. Because this optimiza-
tion problem requires only linear algebra, it can be solved
for networks of millions of nodes and edges in seconds.

We also introduce a generative model for hierarchical
networks in which the existence and direction of edges de-
pend on the relative ranks of the nodes. This model for-
malizes the assumption that individuals tend to interact
with others of similar rank, and it can be used to create
synthetic benchmark networks with tunable levels of hi-
erarchy and noise. It can also predict unobserved edges,
allowing us to use cross-validation as a test of accuracy
and statistical signiﬁcance. Moreover, the maximum like-
lihood estimates of the ranks coincides with SpringRank
asymptotically.

We test SpringRank and its generative model version
on both synthetic and real datasets, including data from
animal behavior, faculty hiring, social support networks,
and sports tournaments. We ﬁnd that it infers accurate
rankings, provides a simple signiﬁcance test for hierarchi-
cal structure, and can predict the existence and direction
of as-yet unobserved edges. In particular, we ﬁnd that
SpringRank often predicts the direction of unobserved
edges more accurately than a variety of existing methods,
including popular spectral techniques, Minimum Viola-
tion Ranking, and the Bradley-Terry-Luce method.

∗ cdebacco@santafe.edu; Contributed equally.
† daniel.larremore@colorado.edu; Contributed equally.
‡ moore@santafe.edu

Ranking entities in a system from pairwise compar-
isons or interactions is a fundamental problem in many

Related work

8
1
0
2
 
n
u
J
 
3
1
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
4
v
2
0
0
9
0
.
9
0
7
1
:
v
i
X
r
a

contexts, and many methods have been proposed. One
family consists of spectral methods like Eigenvector Cen-
trality [12], PageRank [13], Rank Centrality [14], and the
method of Callaghan et al. [15]. These methods propose
various types of random walks on the directed network
and therefore produce real-valued scores. However, by
design these methods tend to give high ranks to a small
number of important nodes, giving us little information
about the lower-ranked nodes.
In addition, they often
require explicit regularization, adding a small term to
every element of the adjacency matrix if the graph of
comparisons is not strongly connected.

A second family focuses on ordinal rankings, i.e., per-
mutations, that minimize various penalty functions. This
family includes Minimum Violation Rank [16–18] and Se-
rialRank [19] and SyncRank [20]. Minimum Violation
Rank (MVR) imposes a uniform penalty for every viola-
tion or “upset,” deﬁned as an edge that has a direction
opposite to the one expected by the rank diﬀerence be-
tween the two nodes. Non-uniform penalties and other
generalizations are often referred to as agony methods
[21]. For common choices of the penalty function, mini-
mization can be computationally diﬃcult [17, 22], forcing
us to use simple heuristics that ﬁnd local minima.

SerialRank constructs a matrix of similarity scores be-
tween each pair of nodes by examining whether they pro-
duce similar outcomes when compared with the other
nodes, thereby relating the ranking problem to a more
general ordering problem called seriation. SyncRank is
a hybrid method which ﬁrst solves a spectral problem
based on synchronization, embeds node positions on a
half-circle in the complex plane, and then chooses among
the circular permutations of those ranks by minimizing
the number of violations as in MVR.

Random Utility Models [23], such as the Bradley-
Terry-Luce (BTL) model [24, 25], are designed to in-
fer real-valued ranks from data on pairwise preferences.
These models assign a probability to the direction of an
edge conditioned on its existence, but they do not assign
a probability to the existence of an edge. They are ap-
propriate, for instance, when an experimenter presents
subjects with choices between pairs of items, and asks
them which they prefer.

Methods like David’s Score [26] and the Colley ma-
trix [27] compute rankings from proportions of wins and
losses. The latter, which was originally developed by
making mathematical adjustments to winning percent-
ages, is equivalent to a particular case of the general
method we introduce below. Elo score [28], Go Rank [29],
and TrueSkill [30] are also widely used win-loss methods,
but these schemes update the ranks after each match
rather than taking all previous interactions into account.
This specialization makes them useful when ranks evolve
over sequential matches, but less useful otherwise.

Finally, there are fully generative models such the
Probabilistic Niche Model of ecology [31–33], models of
friendship based on social status [9], and more generally
latent space models [34] which assign probabilities to the

2

existence and direction of edges based on real-valued po-
sitions in social space. However, inference of these models
tends to be diﬃcult, with many local optima. Our gen-
erative model can be viewed as a special case of these
models for which inference is especially easy.

In the absence of ground-truth rankings, we can
compare the accuracy of these methods using cross-
validation, computing the ranks using a subset of the
edges in the network and then using those ranks to pre-
dict the direction of the remaining edges. Equivalently,
we can ask them to predict unobserved edges, such as
which of two sports teams will win a game. However,
these methods do not all make the same kinds of pre-
dictions, requiring us to use diﬀerent kinds of cross-
validation. Methods such as BTL produce probabilis-
tic predictions about the direction of an edge, i.e., they
estimate the probability one item will be preferred to
another. Fully generative models also predict the proba-
bility that an edge exists, i.e., that a given pair of nodes
in the network interact. On the other hand, ordinal rank-
ing methods such as MVR do not make probabilistic pre-
dictions, but we can interpret their ranking as a coarse
prediction that an edge is more likely to point in one
direction than another.

The SpringRank model

We represent interactions between N entities as a
weighted directed network, where Aij is the number of
interactions i → j suggesting that i is ranked above j.
This allows both ordinal and cardinal input, including
where pairs interact multiple times. For instance, Aij
could be the number of ﬁghts between i and j that i has
won, or the number of times that j has endorsed i.

Given the adjacency matrix A, our goal is to ﬁnd a
ranking of the nodes. To do so, the SpringRank model
computes the optimal location of nodes in a hierarchy by
imagining the network as a physical system. Speciﬁcally,
each node i is embedded at a real-valued position or rank
si, and each directed edge i → j becomes an oriented
spring with a nonzero resting length and displacement
si − sj. Since we are free to rescale the latent space
and the energy scale, we set the spring constant and the
resting length to 1. Thus, the spring corresponding to an
edge i → j has energy

Hij =

(si − sj − 1)2 ,

(1)

1
2

which is minimized when si − sj = 1.

This version of the model has no tunable parameters.
Alternately, we could allow each edge to have its own
rest length or spring constant, based on the strength of
each edge. However, this would create a large number of
parameters, which we would have to infer from the data
or choose a priori. We do not explore this here.

According to this model, the optimal rankings of the
N ) which minimize

nodes are the ranks s∗ = (s∗

1, . . . , s∗

the total energy of the system given by the Hamiltonian

H(s) =

AijHij =

Aij (si − sj − 1)2 .

(2)

N
(cid:88)

i,j=1

1
2

(cid:88)

i,j

Since this Hamiltonian is convex in s, we can ﬁnd s∗ by
setting ∇H(s) = 0, yielding the linear system

(cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) s∗ = (cid:2)Dout − Din(cid:3) 1 ,

(3)

where 1 is the all-ones vector and Dout and Din are di-
agonal matrices whose entries are the weighted in- and
out-degrees, Dout
j Aji. See SI
Text S1 for detailed derivations.

j Aij and Din

ii = (cid:80)

ii = (cid:80)

The matrix on the left side of Eq. (3) is not invertible.
This is because H is translation-invariant:
it depends
only on the relative ranks si − sj, so that if s∗ = {si}
minimizes H(s) then so does {si + a} for any constant a.
One way to break this symmetry is to invert the matrix
in the subspace orthogonal to its nullspace by comput-
ing a Moore-Penrose pseudoinverse. If the network con-
sists of a single component, the nullspace is spanned by
the eigenvector 1, in which case this method ﬁnds the
s∗ where the average rank (1/N ) (cid:80)
i si = (1/N )s∗ · 1 is
zero. This is related to the random walk method of [15]:
if a random walk moves along each directed edge with
rate 1
2 − ε, then
s∗ is proportional to the perturbation to the stationary
distribution to ﬁrst order in ε.

2 + ε and against each one with rate 1

In practice, it is more eﬃcient and accurate to ﬁx the
rank of one of the nodes and solve the resulting equation
using a sparse iterative solver (see SI Text S1). Faster
still, because this matrix is a Laplacian, recent results [35,
36] allow us to solve Eq. (3) in nearly linear time in M ,
the number of non-zero edges in A.

Another way to break translation invariance is to in-
i aﬀecting each

troduce an “external ﬁeld” H0(si) = 1
2 αs2
node, so that the combined Hamiltonian is

Hα(s) = H(s) +

(4)

α
2

N
(cid:88)

i=1

s2
i .

The ﬁeld H0 corresponds to a spring that attracts every
node to the origin. We can think of this as imposing a
Gaussian prior on the ranks, or as a regularization term
that quadratically penalizes ranks with large absolute
values. This version of the model has a single tunable
parameter, namely the spring constant α. Since H(s)
scales with the total edge weight M = (cid:80)
i,j Aij while
H0(s) scales with N , for a ﬁxed value of α this regular-
ization becomes less relevant as networks become more
dense and the average (weighted) degree M/N increases.
For α > 0 there is a unique s∗ that minimizes Hα,

given by

(cid:2)Dout + Din − (cid:0)A + AT (cid:1) + αI(cid:3) s∗ = (cid:2)Dout − Din(cid:3) 1 ,

3

where I is the identity matrix. The matrix on the left
side is now invertible, since the eigenvector 1 has eigen-
In the limit α → 0, we recover
values α instead of 0.
Eq. (3); the value α = 2 corresponds to the Colley ma-
trix method [27].

Minimizing H(s), or the regularized version Hα(s),
corresponds to ﬁnding the “ground state” s∗ of the
model.
In the next section we show that this corre-
sponds to a maximum-likelihood estimate of the ranks
in a generative model. However, we can use SpringRank
not just to maximize the likelihood, but to compute a
joint distribution of the ranks as a Boltzmann distribu-
tion with Hamiltonian Eq. (4), and thus estimate the
uncertainty and correlations between the ranks. In par-
ticular, the ranks si are random variables following an
N -dimensional Gaussian distribution with mean s∗ and
covariance matrix (SI Text S4)

Σ =

(cid:2)Dout + Din − (cid:0)A + AT + αI(cid:1)(cid:3)−1

.

(6)

1
β

Here β is an inverse temperature controlling the amount
of noise in the model. In the limit β → ∞, the rankings
are sharply peaked around the ground state s∗, while
for β → 0 they are noisy. As we discuss below, we can
estimate β from the observed data in various ways.

The rankings given by SpringRank Eq. (3) and its reg-
ularized form Eq. (5) are easily and rapidly computed by
standard linear solvers.
In particular, iterative solvers
that take advantage of the sparsity of the system can
ﬁnd s∗ for networks with millions of nodes and edges
in seconds. However, as deﬁned above, SpringRank is
not a fully generative model that assigns probabilities to
the data and allows for Bayesian inference. In the next
section we introduce a generative model for hierarchical
networks and show that it converges to SpringRank in
the limit of strong hierarchy.

A generative model

In this section we propose a probabilistic genera-
tive model that takes as its input a set of node ranks
s1, . . . , sN and produces a weighted directed network.
The model also has a temperature or noise parameter
β and a density parameter c. Edges between each pair
of nodes i, j are generated independently of other pairs,
conditioned on the ranks. The expected number of edges
from i to j is proportional to the Boltzmann weight of
the corresponding term in the Hamiltonian Eq. (2),

E[Aij] = c exp(−βHij) = c exp

−

(si − sj − 1)2

,

(cid:21)

(cid:20)

β
2

where the actual edge weight Aij is drawn from a Poisson
distribution with this mean. The parameter c controls
the overall density of the network, giving an expected

(5)

4

number of edges

Predicting edge directions

E[M ] =

E[Aij] = c

exp

−

(si − sj − 1)2

,

(cid:88)

i,j

(cid:88)

i,j

(cid:20)

β
2

(cid:21)

while the inverse temperature β controls the extent to
which edges respect (or defy) the ranks s. For smaller
β, edges are more likely to violate the hierarchy or to
connect distant nodes, decreasing the correlation between
the ranks and the directions of the interactions: for β = 0
the model generates a directed Erd˝os-R´enyi graph, while
in the limit β → ∞ edges only exist between nodes i, j
with si − sj = 1, and only in the direction i → j.

The Poisson distribution may generate multiple edges
between a pair of nodes, so this model generates directed
multigraphs. This is consistent with the interpretation
that Aij is the number, or total weight, of edges from i
to j. However, in the limit as E[Aij] → 0, the Poisson
distribution approaches a Bernoulli distribution, gener-
ating binary networks with Aij ∈ {0, 1}.

The likelihood of observing a network A given ranks s,

inverse temperature β, and density c is

(cid:104)

ce− β

2 (si−sj −1)2(cid:105)Aij

P (A | s, β, c) =

(cid:89)

i,j

Aij!

(cid:104)

exp

−ce− β

2 (si−sj −1)2 (cid:105)

(7)
Taking logs, substituting the maximum-likelihood value
of c, and discarding constants that do not depend on s
or β yields a log-likelihood (see Supplemental Text S2)

If hierarchical structure plays an important role in a
system, it should allow us to predict the direction of pre-
viously unobserved interactions, such as the winner of an
upcoming match, or which direction social support will
ﬂow between two individuals. This is a kind of cross-
validation, which lets us test the statistical signiﬁcance
of hierarchical structure. It is also a principled way of
comparing the accuracy of various ranking methods for
datasets where no ground-truth ranks are known.

We formulate the edge prediction question as follows:
given a set of known interactions, and given that there is
an edge between i and j, in which direction does it point?
In one sense, any ranking method provides an answer to
this question, since we can predict the direction according
to which of i or j is ranked higher based on the known
interactions. When comparing SpringRank to methods
such as SyncRank, SerialRank, and MVR, we use these
“bitwise” predictions, and deﬁne the accuracy σb as the
fraction of edges whose direction is consistent with the
inferred ranking.

But we want to know the odds on each game, not just
the likely winner—that is, we want to estimate the prob-
ability that an edge goes in each direction. A priori,
.
a ranking algorithm does not provide these probabili-
ties unless we make further assumptions about how they
depend on the relative ranks. Such assumptions yield
generative models like the one deﬁned above, where the
conditional probability of an edge i → j is

L(A | s, β) = −βH(s) − M log

(cid:20) (cid:88)

2 (si−sj −1)2 (cid:21)

e− β

, (8)

i,j

Pij(β) =

e−βHij
e−βHij + e−βHji

=

1
1 + e−2β(si−sj )

.

(9)

where H(s) is the SpringRank energy deﬁned in Eq. (2).
In the limit of large β where the hierarchical structure
is strong, the ˆs that maximizes Eq. (8), approaches the
solution s∗ of Eq. (3) that minimizes H(s). Thus the
maximum likelihood estimate ˆs of the rankings in this
model approaches the SpringRank ground state.

As discussed above, we can break translational sym-
metry by adding a ﬁeld H0 that attracts the ranks to
the origin. This is is equivalent to imposing a prior
P (s) ∝ (cid:81)N
. The maximum a posteriori
estimate ˆs then approaches the ground state s∗ of the
Hamiltonian in Eq. (4), given by Eq. (5).

i=1 e− αβ

2 (si−1)2

This model belongs to a larger family of genera-
tive models considered in ecology and network the-
ory [9, 31, 32], and more generally the class of latent space
models [34], where an edge points from i to j with proba-
bility f (si − sj) for some function f . These models typi-
cally have complicated posterior distributions with many
local optima, requiring Monte Carlo methods (e.g. [33])
In our
that do not scale eﬃciently to large networks.
case, f (si − sj) is a Gaussian centered at 1, and the pos-
terior converges to the multivariate Gaussian Eq. (6) in
the limit of strong structure.

The density parameter c aﬀects the probability that an
edge exists, but not its direction. Thus our probabilistic
prediction method has a single tunable parameter β.

Note that Pij is a logistic curve, is monotonic in the
rank diﬀerence si − sj, and has width determined by the
inverse temperature β. SpringRank has this in common
with two other ranking methods: setting γi = e2βsi re-
covers the Bradley-Terry-Luce model [24, 25] for which
Pij = γi/(γi + γj), and setting k = 2β recovers the
probability that i beats j in the Go rank [29], where
Pij = 1/(1 + e−k(si−sj )). However, SpringRank diﬀers
from these methods in how it infers the ranks from ob-
served interactions, so SpringRank and BTL make diﬀer-
ent probabilistic predictions.

In our experiments below, we test various ranking
methods for edge prediction by giving them access to
80% of the edges in the network (the training data) and
then asking them to predict the direction of the remain-
ing edges (the test data). We consider two measures of
accuracy: σa is the average probability assigned to the
correct direction of an edge, and σL is the log-likelihood
of generating the directed edges given their existence. For
simple directed graphs where Aij +Aji ∈ {0, 1}, these are

σa =

AijPij

and σL =

Aij log Pij .

(10)

(cid:88)

i,j

(cid:88)

i,j

In the multigraph case, we ask how well Pij approximates
the fraction of interactions between i and j that point
from i to j [see Eqs. (12) and (13)]. For a discussion of
other performance measures, see Supplemental Text S9.
We perform our probabilistic prediction experiments
as follows. Given the training data, we infer the ranks
using Eq. (5). We then choose the temperature parame-
ter β by maximizing either σa or σL on the training data
while holding the ranks ﬁxed. The resulting values of β,
which we denote ˆβa and ˆβL respectively, are generally
distinct (Supplemental Table S2 and Text S7). This is
intuitive, since a single severe mistake where Aij = 1 but
Pij ≈ 0 reduces the likelihood by a large amount, while
only reducing the accuracy by one edge. As a result,
predictions using ˆβa produce fewer incorrectly oriented
edges, achieving a higher σa on the test set, while predic-
tions using ˆβL will produce fewer dramatically incorrect
predictions where Pij is very low, and thus achieve higher
σL on the test set.

Statistical signiﬁcance using the ground state energy

We can measure statistical signiﬁcance using any test
statistic, by asking whether its value on a given dataset
would be highly improbable in a null model. One such
statistic is the accuracy of edge prediction using a method
such as the one described above. However, this may
become computationally expensive for cross-validation
studies with many replicates, since each fold of each
replicate requires inference of the parameter ˆβa. Here
we propose a test statistic which is very easy to com-
pute, inspired by the physical model behind SpringRank:
namely, the ground state energy. For the unregularized
version Eq. (2), the energy per edge is (see SI Text S3)

H(s∗)
M

=

1
2M

(cid:88)

i

(din

i − dout

i

) s∗

i +

(11)

1
2

.

Since the ground state energy depends on many aspects
of the network structure, and since hierarchical structure
is statistically signiﬁcant if it helps us predict edge direc-
tions, like [37] we focus on the following null model: we
randomize the direction of each edge while preserving the
total number ¯Aij = Aij + Aji of edges between each pair
of vertices. If the real network has a ground state energy
which is much lower than typical networks drawn from
this null model, we can conclude that the hierarchical
structure is statistically signiﬁcant.

This test correctly concludes that directed Erd˝os-R´enyi
graphs have no signiﬁcant structure. It also ﬁnds no sig-
niﬁcant structure for networks created using the genera-
tive model Eq. (7) with β = 0.1, i.e., when the tempera-
ture or noise level 1/β is suﬃciently large the ranks are no

5

longer relevant to edge existence or direction (Fig. S2).
However, we see in the next section that it shows sta-
tistically signiﬁcant hierarchy for a variety of real-world
datasets, showing that H(s∗) is both useful and compu-
tationally eﬃcient as a test statistic.

FIG. 1. Performance on synthetic data. (A) A synthetic
network of N = 100 nodes, with ranks drawn from a standard
Gaussian and edges drawn via the generative model Eq. (7)
for two diﬀerent values of β and average degree 5. Blue edges
point down the hierarchy and red edges point up, indicated
by arrows. (B) The accuracy of the inferred ordering deﬁned
as the Spearman correlation averaged over 100 indendepently
generated networks; error bars indicate one standard devia-
tion. (C, D) Identical to A and B but with ranks drawn from
a mixture of three Gaussians so that the nodes cluster into
three tiers (Materials and Methods). See Fig. S1 for perfor-
mance curves for Pearson correlation r.

Results on real and synthetic data

Having introduced SpringRank, an eﬃcient procedure
for inferring real-valued ranks, a corresponding gener-
ative model, a method for edge prediction, and a test
for the statistical signiﬁcance of hierarchical structure,
we now demonstrate it by applying it to both real and
synthetic data. For synthetic datasets where the ground-
truth ranks are known, our goal is to see to what extent
SpringRank and other algorithms can recover the actual
ranks. For real-world datasets, in most cases we have
no ground-truth ranking, so we apply the statistical sig-
niﬁcance test deﬁned above, and compare the ability of
SpringRank and other algorithms to predict edge direc-

tions given a subset of the interactions.

We compare SpringRank to other widely used meth-
ods: the spectral methods PageRank [13], Eigenvector
Centrality [12] and Rank Centrality [14]; Minimum Vi-
olation Ranking (MVR) [16, 17], SerialRank [19] and
SyncRank [20], which produce ordinal rankings; David’s
score [26]; and the BTL random utility model [24, 25]
using the algorithm proposed in [38], which like our
generative model makes probabilistic predictions. We
also compare unregularized SpringRank with the regu-
larized version α = 2, corresponding to the Colley ma-
trix method [27]. Unfortunately, Eigenvector Centrality,
Rank Centrality, David’s score, and BTL are undeﬁned
when the network is not strongly connected, e.g. when
there are nodes with zero in- or out-degree. In such cases
we follow the common regularization procedure of adding
low-weight edges between every pair of nodes (see Sup-
plemental Text S10).

Performance for synthetic networks

We study two types of synthetic networks, generated
by the model described above. Of course, since the log-
likelihood in this model corresponds to the SpringRank
energy in the limit of large β, we expect SpringRank to
do well on these networks, and its performance should be
viewed largely as a consistency check. But by varying the
distribution of ranks and the noise level, we can illustrate
types of structure that may exist in real-world data, and
test each algorithm’s ability to identify them.

In the ﬁrst type, the ranks are normally distributed
with mean zero and variance one (Fig. 1A). In the second
type, the ranks are drawn from an equal mixture of three
Gaussians with diﬀerent means and variances, so that
nodes cluster into high, middle, and low tiers (Fig. 1C).
This second type is intended to focus on the importance
of real-valued ranks, and to measure the performance of
algorithms that (implicitly or explicitly) impose strong
priors on the ranks when the data defy their expectations.
In both cases, we vary the amount of noise by changing
β while keeping the total number of edges constant (see
Materials and Methods).

Since we wish to compare SpringRank both to meth-
ods such as MVR that only produce ordinal rankings,
and to those like PageRank and David’s Score that pro-
duce real-valued ranks, we measure the accuracy of each
algorithm according to the Spearman correlation ρ be-
tween its inferred rank order and the true one. Results
for the Pearson correlation, where we measure the algo-
rithms’ ability to infer the real-valued ranks as opposed
to just their ordering, are shown in Fig. S1.

We ﬁnd that all the algorithms do well on the ﬁrst
type of synthetic network. As β increases so that the net-
work becomes more structured, with fewer edges (shown
in red in Fig. 1A) pointing in the “wrong” direction,
all algorithms infer ranks that are more correlated with
the ground truth. SpringRank and SyncRank have the

6

FIG. 2. Ranking the History faculty hiring network [3].
(A) Linear hierarchy diagram with nodes embedded at their
inferred SpringRank scores. Blue edges point down the hier-
archy and red edges point up. (B) Histogram of the empirical
distribution of ranks, with a vertical axis of ranks matched to
panel A. (C) Histogram of ground-state energies from 10, 000
randomizations of the network according to the null model
where edge directions are random; the dashed red line shows
the ground state energy of the empirical network depicted in
panels A and B. The fact that the ground state energy is so
far below the tail of the null model is overwhelming evidence
that the hierarchical structure is statistically signiﬁcant, with
a p-value < 10−4

.

highest accuracy, followed closely by the Colley matrix
method and BTL (Fig. 1B). Presumably the Colley ma-
trix works well here because the ranks are in fact drawn
from a Gaussian prior, as it implicitly assumes.

Results for the second type of network are more nu-
anced. The accuracy of SpringRank and SyncRank in-
creases rapidly with β with exact recovery around β = 1.
Inter-
SerialRank also performs quite well on average.

7

FIG. 3. Edge prediction accuracy over BTL. Distribution of diﬀerences in performance of edge prediction of SpringRank
compared to BTL on real and synthetic networks deﬁned as (A) edge-prediction accuracy σa Eq. (12) and (B) the conditional
log-likelihood σL Eq. (13). Error bars indicate quartiles and markers show medians, corresponding to 50 independent trials of
5-fold cross-validation, for a total of 250 test sets for each network. The two synthetic networks are generated with N = 100,
average degree 5, and Gaussian-distributed ranks as in Fig. 1A, with inverse temperatures β = 1 and β = 5. For each
experiment shown, the fractions of trials in which each method performed equal to or better than BTL are shown in Table I.
These diﬀerences correspond to prediction of an additional 1 to 12 more correct edge directions, on average.

estingly, the other methods do not improve as β in-
creases, and many of them decrease beyond a certain
point (Fig. 1D). This suggests that these algorithms be-
come confused when the nodes are clustered into tiers,
even when the noise is small enough that most edges have
directions consistent with the hierarchy.
SpringRank
takes advantage of the fact that edges are more likely
between nodes in the same tier (Fig. 1C), so the mere
existence of edges helps it cluster the ranks.

These synthetic tests suggest that real-valued ranks
capture information that ordinal ranks do not, and that
many ranking methods perform poorly when there are
substructures in the data such as tiered groups. Of
course, in most real-world scenarios, the ground-truth
ranks are not known, and thus edge prediction and other
forms of cross-validation should be used instead. We turn
to edge prediction in the next section.

Performance for real-world networks

As discussed above, in most real-world networks, we
have no ground truth for the ranks. Thus we focus on
our ability to predict edge directions from a subset of
the data, and measure the statistical signiﬁcance of the
inferred hierarchy.

We apply our methods to datasets from a diverse set
of ﬁelds, with sizes ranging up to N = 415 nodes and
up to 7000 edges (see Table S2): three North Ameri-
can academic hiring networks where Aij is the number
of faculty at university j who received their doctorate
from university i, for History (illustrated in Figs. 2A and
B), Business, and Computer Science departments [3]; two

networks of animal dominance among captive monk para-
keets [5] and one among Asian elephants [37] where Aij
is the number of dominating acts by animal i toward an-
imal j; and social support networks from two villages
in Tamil Nadu referred to (for privacy reasons) by the
pseudonyms “Tenpat.t.i” and “Alak¯apuram,” where Aij
is the number of distinct social relationships (up to ﬁve)
through which person i supports person j [2]; and 53 net-
works of NCAA Women’s and Men’s college basketball
matches during the regular season, spanning 1985-2017
(Men) and 1998-2017 (Women), where Aij = 1 if team i
beat team j. Each year’s network comprises a diﬀerent
number of matches, ranging from 747 to 1079 [39].

Together, these examples cover prestige, dominance,
and social hierarchies. In each of these domains, infer-
ring ranks from interactions is key to further analysis.
Prestige hierarchies play an unequivocal role in the dy-
namics of academic labor markets [40]; in behavioral ecol-
ogy, higher-ranked individuals in dominance hierarchies
are believed to have higher ﬁtness [1, 41]; and patterns
of aggression are believed to reveal animal strategies and
cognitive capacities [4–8]. Finally, in social support net-
works, higher ranked individuals have greater social capi-
tal and reputational standing [42, 43], particularly in set-
tings in which social support is a primary way to express
and gain respect and prestige [44].

We ﬁrst applied our ground state energy test for the
presence of statistically signiﬁcant hierarchy, rejecting
the null hypothesis with p < 10−4 in almost all cases
(e.g., for History faculty hiring, see Fig. 2C). The one ex-
ception is the Asian Elephants network for which p > 0.4.
This corroborates the original study of this network [37],
which found that counting triad motifs shows no signif-

8

FIG. 4. Probabilistic edge prediction accuracy σa of
SpringRank vs. BTL. For 50 independent trials of 5-fold
cross-validation (250 total folds per network), the values of σa
for SpringRank and BTL are shown on the vertical axis and
the horizontal axis respectively. Points above the diagonal,
shown in blue, are trials where SpringRank is more accurate
than BTL. The fractions for which each method is superior
are shown in plot legends, matching Table I.

icant hierarchy [45]. This is despite the fact that one
can ﬁnd an appealing ordering of the elephants using the
Minimum Violation Rank method, with just a few vio-
lating edges (SI Fig. S9). Thus the hierarchy found by
MVR may well be illusory.

As described above, we performed edge prediction ex-
periments using 5-fold cross-validation, where 80% of the
edges are available to the algorithm as training data, and
a test set consisting of 20% of the edges is held out (see
Materials and Methods). To test SpringRank’s ability to
make probabilistic predictions, we compare it to BTL.

We found that SpringRank outperforms BTL, both in
terms of the accuracy σa (Fig. 3A) and, for most net-
works, the log-likelihood σL (Fig. 3B). The accuracy of
both methods has a fairly broad distribution over the tri-
als of cross-validation, since in each network some subsets
of the edges are harder to predict than others when they
are held out. However, as shown in Fig. 4, in most tri-
als SpringRank was more accurate than BTL. Fig. 3A
and Table I show that SpringRank predicts edge direc-
tions more accurately in the majority of trials of cross-
validation for all nine real-world networks, where this
majority ranges from 62% for the parakeet networks to
100% for the Computer Science hiring network.

Table I shows that SpringRank also obtained a higher
log-likelihood σL than BTL for 6 of the 9 real-world net-
works. Regularizing SpringRank with α = 2 does not ap-
pear to signiﬁcantly improve either measure of accuracy
(Fig. 3). We did not attempt to tune the regularization
parameter α.

To compare SpringRank with methods that do not
make probabilistic predictions, including those that pro-

FIG. 5. Bitwise prediction accuracy σb of SpringRank
vs. SyncRank. For 50 independent trials of 5-fold cross-
validation (250 total folds per NCAA season), the fraction
of correctly predicted game outcomes σb for SpringRank and
Syncrank are shown on the vertical axis and the horizontal
axis respectively. Points above the equal performance line,
shown in blue, are trials where SpringRank is more accurate
than SyncRank; the fractions for which each method is supe-
rior are shown in plot legends.

duce ordinal rankings, we measured the accuracy σb of
“bitwise” predictions, i.e., the fraction of edges consis-
tent with the infered ranking. We found that spectral
methods perform poorly here, as does SerialRank.
In-
terestingly, BTL does better on the NCAA networks in
terms of bitwise prediction than it does for probabilistic
predictions, suggesting that it is better at rank-ordering
teams than determining their real-valued position.

We found that SyncRank is the strongest of the or-
dinal methods, matching SpringRank’s accuracy on the
parakeet and business school networks, but SpringRank
outperforms SyncRank on the social support and NCAA
networks (see Fig. S4). We show a trial-by-trial compari-
son of SpringRank and SyncRank in Fig. 5, showing that
in most trials of cross-validation SpringRank makes more
accurate predictions for the NCAA networks.

To check whether our results were dependent on the
choice of holding out 20% of the data, we repeated our
experiments using 2-fold cross-validation, i.e., using 50%
of network edges as training data and trying to predict
the other 50%. We show these results in Fig. S5. While
all algorithms are less accurate in this setting, the com-
parison between algorithms is similar to that for 5-fold
cross-validation.

Finally, the real-valued ranks found by SpringRank
shed light on the organization and assembly of real-world
networks (see Figs. S6, S7, S8, S12, and S13). For exam-
ple, we found that ranks in the faculty hiring networks
have a long tail at the top, suggesting that the most pres-
tigious universities are more separated from those below
them than an ordinal ranking would reveal. In contrast,
ranks in the social support networks have a long tail
at the bottom, suggesting a set of people who do not
have suﬃcient social status to provide support to oth-
ers. SpringRank’s ability to ﬁnd real-valued ranks makes
these distributions amenable to statistical analysis, and

9

% trials higher σa vs BTL % trials higher σL vs BTL
Type SpringRank +regularization SpringRank +regularization

Dataset
Comp. Sci. [3]
Alak¯apuram [2]
Synthetic β = 5
History [3]
NCAA Women (1998-2017) [39]
Tenpat.t.i [2]
Synthetic β = 1
NCAA Men (1985-2017) [39]

Faculty Hiring
Social Support
Synthetic
Faculty Hiring
Basketball
Social Support
Synthetic
Basketball
Parakeet G1 [5] Animal Dominance
Faculty Hiring
Parakeet G2 [5] Animal Dominance

Business [3]

100.0
99.2†
98.4
97.6†
94.4†
88.8
83.2
76.0†
71.2†
66.8†
62.0

97.2
99.6
63.2
96.8
87.0
93.6
65.2
62.3
56.8
59.2
51.6

100.0
100.0
76.4
98.8
69.1
100.0
98.4
68.5
41.2
39.2
47.6

99.6
100.0
46.4
98.8
51.0
100.0
98.4
52.4
37.2
36.8
47.2

TABLE I. Edge prediction with BTL as a benchmark. During 50 independent trials of 5-fold cross-validation (250 total
folds per network), columns show the the percentages of instances in which SpringRank Eq. (3) and regularized SpringRank
Eq. (5) with α = 2 produced probabilistic predictions with equal or higher accuracy than BTL. Distributions of accuracy
improvements are shown in Fig. 3. Center columns show accuracy σa and right columns show σL (Materials and Methods).
Italics indicate where BTL outperformed SpringRank for more than 50% of tests. † Dagger symbols indicate tests that are
shown in detail in Fig. 4. NCAA Basketball datasets were analyzed one year at a time

.

we suggest this as a direction for future work.

Conclusions

SpringRank is a mathematically principled, physics-
inspired model for hierarchical structure in networks
It yields a simple and highly
of directed interactions.
scalable algorithm, requiring only sparse linear algebra,
which enables analysis of networks with millions of nodes
and edges in seconds. Its ground state energy provides
a natural test statistic for the statistical signiﬁcance of
hierarchical structure.

While the basic SpringRank algorithm is nonparamet-
ric, a parameterized regularization term can be included
as well, corresponding to a Gaussian prior. While reg-
ularization is often required for BTL, Eigenvector Cen-
trality, and other commonly used methods (Supplemen-
tal Text S10) it is not necessary for SpringRank and our
tests indicate that its eﬀects are mixed.

We also presented a generative model that allows one
to create synthetic networks with tunable levels of hierar-
chy and noise, whose posterior coincides with SpringRank
in the limit where the eﬀect of the hierarchy is strong.
By tuning a single temperature parameter, we can use
this model to make probabilistic predictions of edge di-
rections, generalizing from observed to unobserved inter-
actions. Therefore, after conﬁrming its ability to infer
ranks in synthetic networks where ground truth ranks
are known, we measured SpringRank’s ability to to pre-
dict edge directions in real networks. We found that
in networks of faculty hiring, animal interactions, social
support, and NCAA basketball, SpringRank often makes
better probabilistic predictions of edge predictions than
the popular Bradley-Terry-Luce model, and performs as
well or better than SyncRank and a variety of other meth-
ods that produce ordinal rankings.

SpringRank is based on springs with quadratic poten-
tials, but other potentials may be of interest. For in-
stance, to make the system more tolerant to outliers while
remaining convex, one might consider a piecewise poten-
tial that is quadratic for small displacements and linear
otherwise. We leave this investigation of alternative po-
tentials to future work.

Given its simplicity, speed, and high performance, we
believe that SpringRank will be useful in a wide vari-
ety of ﬁelds where hierarchical structure appears due to
dominance, social status, or prestige.

Materials and methods

Synthetic network generation

Networks were generated in three steps. First, node
ranks splanted were drawn from a chosen distribution. For
Test 1, N = 100 ranks were drawn from a standard nor-
mal distribution, while for Test 2, 34 ranks were drawn
from each of three Gaussians, N (−4, 2), N (0, 1
2 ), and
N (4, 1) for a total of N = 102. Second, an average de-
gree (cid:104)k(cid:105) and a value of the inverse temperature β were
chosen. Third, edges were drawn generated to Eq. (7)
with c = (cid:104)k(cid:105)N/ (cid:80)
i,j exp [−(β/2)(si − sj − 1)2] so that
the expected mean degree is (cid:104)k(cid:105) (see SI Text S6).

This procedure resulted in directed networks with the
desired hierarchical structure, mean degree, and noise
level. Tests were conducted for (cid:104)k(cid:105) ∈ [5, 15], β ∈ [0.1, 5],
and all performance plots show mean and standard devi-
ations for 100 replicates.

Performance measures for edge prediction

For multigraphs, we deﬁne the accuracy of probabilis-
tic edge prediction as the extent to which Pij is a good

estimate of the fraction of interactions between i and j
that point from i to j, given that there are any edges to
predict at all, i.e., assuming ¯Aij = Aij + Aji > 0. If this
prediction were perfect, we would have Aij = ¯AijPij. We
deﬁne σA as 1 minus the sum of the absolute values of
the diﬀerence between Aij and this estimate,

σa = 1 −

1
2M

(cid:88)

i,j

(cid:12)
(cid:12)Aij − ¯Aij Pij

(cid:12)
(cid:12) ,

(12)

where M is the number of directed edges in the subset
of the network under consideration, e.g., the training or
test set. Then σa = 1 if Pij = Aij/ ¯Aij for all i, j, and
σa = 0 if for each i, j all the edges go from i to j (say)
but Pij = 0 and Pji = 1.

To measure accuracy via the conditional log-likelihood,
we ask with what probability we would get the directed
network A from the undirected network ¯A if each edge
between i and j points from i → j with probability Pij
and from j → i with probability Pji = 1−Pij. This gives

σL = log Pr[A | ¯A]
(cid:88)

=

log

(cid:19)

(cid:18)Aij + Aji
Aij

i,j

+ log

(cid:104)

P Aij
ij

[1 − Pij]Aji(cid:105)

,

(13)

y

where (cid:0)x
(cid:1) is the binomial coeﬃcient. We disregard the
ﬁrst term of this sum since it does not depend on P .
If we wish to compare networks of diﬀerent sizes as in
Fig. 3, we can normalize σL by the number of edges.
For an extensive discussion of performance metrics see
Supplementary Text S9.

Statistical signiﬁcance of ranks

We compute a standard left-tailed p-value for the sta-
tistical signiﬁcance of the ranks s∗ by comparing the
ground state energy Eq. (11) of the real network A with
the null distribution of ground state energies of an ensem-
ble of networks ˜A drawn from the null model where ¯Aij is

10

kept ﬁxed, but the direction of each edge is randomized.

p-value = Pr[H(s∗; A) ≤ H(˜s∗; ˜A)] .

(14)

In practice, this p-value is estimated by drawing many
samples from the null distribution by randomizing the
edge directions of A to produce ˜A, computing the ranks
˜s∗ from Eq. (3), and then computing the ground state
energy Eq. (11) of each.

Cross-validation tests

We performed edge prediction using 5-fold cross-
validation. In each realization, we divide the interacting
pairs i, j, i.e., those with nonzero ¯Aij = Aij + Aji, into
ﬁve equal groups. We use four of these groups as a train-
ing set, inferring the ranks and setting β to maximize σa
or σL (on the left and right of Fig. 3 respectively). We
then use the ﬁfth group as a test set, asking the algorithm
for Pij for each pair i, j in that group, and report σa or
σL on that test set. By varying which group we use as the
test set, we get 5 trials per realization: for instance, 50
realizations give us 250 trials of cross-validation. Results
for 2-fold cross-validation are reported in SI.

Acknowledgements

CDB and CM were supported by the John Tem-
pleton Foundation. CM was also supported by the
Army Research Oﬃce under grant W911NF-12-R-0012.
DBL was supported by NSF award SMA-1633747 and
the Santa Fe Institute Omidyar Fellowship. We thank
Aaron Clauset and Johan Ugander for helpful com-
ments.
Competing Interests: The authors declare
that they have no competing interests. All authors
derived the model, analyzed results, and wrote the
manuscript. CDB wrote Python implementations and
DBL wrote MATLAB implementations. Open-source
code in Python, MATLAB, and SAS/IML available at
https://github.com/cdebacco/SpringRank.

[1] C. Drews, The concept and deﬁnition of dominance in

animal behaviour, Behaviour 125, 283 (1993).

[2] E. A. Power, Social support networks and religiosity in
rural South India, Nature Human Behaviour 1, 0057
(2017).

[3] A. Clauset, S. Arbesman, D. B. Larremore, Systematic
inequality and hierarchy in faculty hiring networks, Sci-
ence Advances 1, e1400005 (2015).

[4] S. D. Cˆot´e, M. Festa-Bianchet, Reproductive success in
female mountain goats: the inﬂuence of age and social
rank, Animal Behaviour 62, 173 (2001).

[5] E. A. Hobson, S. DeDeo, Social feedback and the emer-
gence of rank in animal society, PLoS Computational Bi-
ology 11, e1004411 (2015).

[6] C. J. Dey, J. S. Quinn, Individual attributes and self-
organizational processes aﬀect dominance network struc-
ture in pukeko, Behavioral Ecology 25, 1402 (2014).
[7] C. J. Dey, A. R. Reddon, C. M. O’Connor, S. Balshine,
Network structure is related to social conﬂict in a cooper-
atively breeding ﬁsh, Animal Behaviour 85, 395 (2013).
[8] M. A. Cant, J. B. Llop, J. Field, Individual variation
in social aggression and the probability of inheritance:
theory and a ﬁeld test, The American Naturalist 167,
837 (2006).

[9] B. Ball, M. E. Newman, Friendship networks and social

status, Network Science 1, 16 (2013).

[10] S. Szymanski, The economic design of sporting contests,

Journal of Economic Literature 41, 1137 (2003).

11

[33] A. Z. Jacobs, J. A. Dunne, C. Moore, A. Clauset, Un-
tangling the roles of parasites in food webs with gener-
ative network models, arXiv preprint arXiv:1505.04741
(2015).

[34] P. D. Hoﬀ, A. E. Raftery, M. S. Handcock, Latent space
approaches to social network analysis, Journal of the
American Statistical Association 97, 1090 (2001).
[35] D. A. Spielman, S.-H. Teng, Nearly linear time algo-
rithms for preconditioning and solving symmetric, diago-
nally dominant linear systems, SIAM Journal on Matrix
Analysis and Applications 35, 835 (2014).

[36] I. Koutis, G. L. Miller, R. Peng, A nearly-m log n time
solver for SDD linear systems, Proc. 52nd Foundations
of Computer Science (FOCS) (IEEE Presss, 2011), pp.
590–598.

[37] S. de Silva, V. Schmid, G. Wittemyer, Fission–fusion pro-
cesses weaken dominance networks of female asian ele-
phants in a productive habitat, Behavioral Ecology 28,
243 (2016).

[38] D. R. Hunter, MM algorithms for generalized Bradley-

Terry models, Annals of Statistics pp. 384–406 (2004).

[39] NCAA, http://www.ncaa.org/championships/statistics

(2018).

[40] S. F. Way, D. B. Larremore, A. Clauset, Gender, pro-
ductivity, and prestige in computer science faculty hir-
ing networks, Proc. 25th Intl Conf on World Wide Web
(2016), pp. 1169–1179.

[41] B. Majolo, J. Lehmann, A. de Bortoli Vizioli, G. Schino,
Fitness-related beneﬁts of dominance in primates, Amer-
ican Journal of Physical Anthropology 147, 652 (2012).
[42] N. Lin, Social Capital: A Theory of Social Structure and
Action, vol. 19 (Cambridge University Press, 2002).
[43] K. S. Cook, M. Levi, R. Hardin, Whom can we trust?:
How groups, networks, and institutions make trust possi-
ble (Russell Sage Foundation, 2009).

[44] M. Mines, Public faces, private lives: Community and in-
dividuality in South India (University of California Press,
1994).

[45] D. Shizuka, D. B. McDonald, A social network perspec-
tive on measurements of dominance hierarchies, Animal
Behaviour 83, 925 (2012).

[46] L. Rosasco, E. D. Vito, A. Caponnetto, M. Piana,
A. Verri, Are loss functions all the same?, Neural Com-
putation 16, 1063 (2004).

[11] R. Baumann, V. A. Matheson, C. A. Howe, Anomalies
in tournament design: the madness of march madness,
Journal of Quantitative Analysis in Sports 6 (2010).
[12] P. Bonacich, Power and centrality: A family of measures,

American Journal of Sociology 92, 1170 (1987).

[13] L. Page, S. Brin, R. Motwani, T. Winograd, The PageR-
ank citation ranking: Bringing order to the web., Tech.
rep., Stanford InfoLab (1999).

[14] S. Negahban, S. Oh, D. Shah, Rank centrality: Ranking
from pairwise comparisons, Operations Research (2016).
[15] T. Callaghan, P. J. Mucha, M. A. Porter, Random walker
ranking for NCAA division IA football, American Math-
ematical Monthly 114, 761 (2007).

[16] I. Ali, W. D. Cook, M. Kress, On the minimum violations
ranking of a tournament, Management Science 32, 660
(1986).

[17] P. Slater, Inconsistencies in a schedule of paired compar-

isons, Biometrika 48, 303 (1961).

[18] M. Gupte, P. Shankar, J. Li, S. Muthukrishnan, L. Iftode,
Finding hierarchy in directed online social networks,
Proc. 20th Intl. Conf. on the World Wide Web (ACM,
2011), pp. 557–566.

[19] F. Fogel, A. d’Aspremont, M. Vojnovic, Serialrank: Spec-
tral ranking using seriation, Advances in Neural Informa-
tion Processing Systems (2014), pp. 900–908.

[20] M. Cucuringu, Sync-rank: Robust ranking, constrained
ranking and rank aggregation via eigenvector and sdp
synchronization, IEEE Transactions on Network Science
and Engineering 3, 58 (2016).

[21] E. Letizia, P. Barucca, F. Lillo, Resolution of ranking
hierarchies in directed networks, PloS one 13, e0191604
(2018).

[22] N. Tatti, Tiers for peers: a practical algorithm for discov-
ering hierarchy in weighted networks, Data Mining and
Knowledge Discovery 31, 702 (2017).

[23] K. E. Train, Discrete Choice Methods with Simulation

(Cambridge University Press, 2009).

[24] R. A. Bradley, M. E. Terry, Rank analysis of incom-
plete block designs: I. the method of paired comparisons,
Biometrika 39, 324 (1952).

[25] R. D. Luce, On the possible psychophysical laws., Psy-

chological Review 66, 81 (1959).

[26] H. A. David, Ranking from unbalanced paired-

comparison data, Biometrika 74, 432 (1987).

[27] W. N. Colley, Colley’s bias free college football rank-
ing method: The Colley matrix explained (2002).
Http://www.colleyrankings.com/matrate.pdf.

[28] A. E. Elo, The Rating of Chessplayers, Past and Present

(Arco Pub., 1978).

[29] R. Coulom, Whole-history rating: A Bayesian rating sys-
tem for players of time-varying strength, International
Conference on Computers and Games (Springer, 2008),
pp. 113–124.

[30] R. Herbrich, T. Minka, T. Graepel, Trueskill: a Bayesian
skill rating system, Advances in Neural Information Pro-
cessing Systems (2007), pp. 569–576.

[31] R. J. Williams, A. Anandanadesan, D. Purves, The prob-
abilistic niche model reveals the niche structure and role
of body size in a complex food web, PLoS ONE 5, e12092
(2010).

[32] R. J. Williams, D. W. Purves, The probabilistic niche
model reveals substantial variation in the niche structure
of empirical food webs, Ecology 92, 1849 (2011).

Supporting Information (SI)

S1. Deriving the linear system minimizing the Hamiltonian

The SpringRank Hamiltonian Eq. (2) is convex in s and we set its gradient ∇H(s) = 0 to obtain the global

minimum:

Let the weighted out-degree and in-degree be dout
written as

i = (cid:80)

j Aij and din

i = (cid:80)

j Aji, respectively. Then Eq. (S1) can be

∂H
∂si

(cid:88)

=

j

[Aij (si − sj − 1) − Aji (sj − si − 1)] = 0 .

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i − din
i

(cid:1) −

[Aij + Aji] sj = 0 .

(cid:88)

j

We now write the system of N equations together by introducing the following matrix notation. Let Dout =
diag(dout
N ) be diagonal matrices, let 1 be the N -dimensional vector of all ones.
Then Eq. (S2) becomes

N ) and Din = diag(din

1 , . . . , din

, . . . , dout

1

(cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) s = (cid:2)Dout − Din(cid:3) 1 .

This is a linear system of the type B s = b, where B = (cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) and b = (cid:2)Dout − Din(cid:3) 1. The rank
of B is at most N − 1 and more generally, if the network represented by A consists of C disconnected components, B
will have rank N − C. In fact, B has an eigenvalue 0 with multiplicity C, and the eigenvector 1 is in the nullspace.
B is not invertible, but we can only invert in the N − C-dimensional subspace orthogonal to the nullspace of B. The
family of translation-invariant solutions s∗ is therefore deﬁned by

s∗ = (cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3)−1 (cid:2)Dout − Din(cid:3) 1 ,

in which the notation [·]−1 should be taken as the Moore-Penrose pseudoinverse.

In practice, rather than constructing the pseudo-inverse, it will be more computationally eﬃcient (and for large
systems, more accurate) to solve the linear system in an iterative fashion. Since we know that solutions may be
translated up or down by an arbitrary constant, the system can be made full-rank by ﬁxing the position of an
arbitrary node 0. Without loss of generality, let sN = 0. In this case, terms that involve sN can be dropped from
Eq. (S2), yielding

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i − din
i

(cid:1) −

[Aij + Aji] sj = 0 ,

i (cid:54)= N

− (cid:0)dout

N − din
N

(cid:1) −

[AN j + AjN ] sj = 0 .

N −1
(cid:88)

j=1

N −1
(cid:88)

j=1

N −1
(cid:88)

j=1

Adding Eq. (S5) to Eq. (S6) yields

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i + dout

N − din

i − din
N

(cid:1) −

[Aij + AN j + Aji + AjN ] sj = 0 ,

(S7)

which can be written in matrix notation as

(cid:105)
(cid:104)
Dout + Din − ˚A

s = (cid:2)Dout − Din(cid:3) 1 + (cid:0)dout

N − din
N

(cid:1) 1 ,

where

˚Aij = Aij + AN j + Aji + AjN .

In this formulation, Eq. (S8) can be solved to arbitrary precision using iterative methods that take advantage of the
sparsity of ˚A. The resulting solution may then be translated by an arbitrary amount as desired.

12

(S1)

(S2)

(S3)

(S4)

(S5)

(S6)

(S8)

(S9)

13

(S12)

(S13)

S2. Poisson generative model

The expected number of edges from node i to node j is c exp

and therefore the likelihood of

(cid:104)

2 (si − sj − 1)2(cid:105)
− β

observing a network A, given parameters β, s, and c is

(cid:104)

c e− β

2 (si−sj −1)2(cid:105)Aij

P (A | s, β, c) =

(cid:89)

i,j

Aij!

(cid:104)
exp

−c e− β

2 (si−sj −1)2(cid:105)

.

(S10)

Taking logs yields

log P (A | s, β, c) =

Aij log c −

Aij (si − sj − 1)2 − log [Aij!] − ce− β

2 (si−sj −1)2

.

(S11)

(cid:88)

i,j

β
2

Discarding the constant term log [Aij!], and recognizing the appearance of the SpringRank Hamiltonian H(s), yields

L(A | s, β, c) = −βH(s) +

Aij log c −

ce− β

2 (si−sj −1)2

.

(cid:88)

i,j

(cid:88)

i,j

Taking ∂L/∂c and setting it equal to zero yields

ˆc =

(cid:80)

i,j Aij
2 (si−sj −1)2 ,

(cid:80)

i,j e− β

which has the straightforward interpretation of being the ratio between the number of observed edges and the expected
number of edges created in the generative process for c = 1. Substituting in this solution and letting M = (cid:80)
i,j Aij
yields

L(A | s, β) = −βH(s) + M log ˆc −

ˆc e− β

2 (si−sj −1)2

(cid:88)

i,j





(cid:88)

i,j

M
β





(cid:88)

i,j





 .

= −βH(s) + M log M − M log

e− β

2 (si−sj −1)2

 − M .

(S14)

The terms M log M and M may be neglected since they do not depend on the parameters, and we divide by β,
yielding a log-likelihood of

L(A | s, β) = −H(s) −

log

e− β

2 (si−sj −1)2

(S15)

(cid:3) where (cid:104)·(cid:105)E denotes
Note that the SpringRank Hamiltonian may be rewritten as H(s) = M (cid:2) 1
the average over elements in the edge set E. In other words, H(s) scales with M and the square of the average spring
length. This substitution for H(s) allows us to analyze the behavior of the log-likelihood

2 (cid:104)Aij(si − sj − 1)2(cid:105)E

L(A | s, β) = −M

(cid:68)

Aij (si − sj − 1)2(cid:69)

+

log

1
β

E

e− β

2 (si−sj −1)2





(cid:88)

i,j










.






1
2

(S16)

Inside the logarithm there are N 2 terms of ﬁnite value, so that the logarithm term is of order O( log N
β ). Thus, for well-
resolved hierarchies, i.e. when β is large enough that the sampled edges consistently agree with the score diﬀerence
between nodes, the maximum likelihood ranks ˆs approach the ranks s∗ found by minimizing the Hamiltonian. In
practice, exactly maximizing the likelihood would require extensive computation, e.g. by using local search heuristic
or Markov chain Monte Carlo sampling.

S3. Rewriting the energy

The Hamiltonian Eq. (2) can be rewritten as

2 H(s) =

Aij (si − sj − 1)2

N
(cid:88)

i,j=1

N
(cid:88)

i,j=1

N
(cid:88)

=

=

N
(cid:88)

=

i

Aij

(cid:0)s2

i + s2

j − 2sisj + 1 − 2si + 2sj

(cid:1)

N
(cid:88)

s2
i

Aij +

N
(cid:88)

N
(cid:88)

s2
j

j

i=1

Aij − 2

(cid:88)

(cid:88)

si

Aijsj + M

i

j

i

j=1

−2

N
(cid:88)

N
(cid:88)

si

i=1

j=1

Aij + 2

N
(cid:88)

N
(cid:88)

sj

j=1

i=1

Aij

s2
i

(cid:0)dout

i + din
i

(cid:1) − 2

si

(cid:0)dout

i − din
i

(cid:1) + M − 2

(cid:88)

(cid:88)

si

Aijsj .

(cid:88)

i

i

j

From Eq. (S2) we have

(cid:88)

s2
i

(cid:0)dout

i + din
i

(cid:1) −

(cid:88)

si

(cid:0)dout

i − din
i

(cid:1) =

(cid:88)

(cid:88)

si

[Aij + Aji] sj .

j

i

i

j

We can substitute this into Eq. (S17)
(cid:88)

(cid:88)

2H(s) =

si

[Aij + Aji] sj −

si

(cid:0)dout

i − din
i

(cid:1) + M − 2

(cid:88)

(cid:88)

si

Aijsj

(cid:88)

i

i

j

j

si

(cid:0)din

i − dout

i

(cid:1) + M

hisi + M ,

=

=

i
(cid:88)

i
(cid:88)

i

where hi ≡ din

i − dout

i

.

S4. Ranks distributed as a multivariate Gaussian distribution

Assuming that the ranks are random variables distributed as a multivariate Gaussian distribution of average ¯s and

covariance matrix Σ, we have:

We can obtain this formulation by considering a Boltzman distribution with the Hamiltonian Eq. (2) as the energy
term and inverse temperature β so that

P (s) ∝ exp

−

(s − ¯s)(cid:124)Σ−1(s − ¯s)

.

(cid:18)

1
2

(cid:19)



P (s) ∝ exp

−

Aij (si − sj − 1)2

 .



N
(cid:88)

i,j=1

β
2

1
2

Manipulating the exponent of Eq. (S20) yields

(s − ¯s)T Σ−1(s − ¯s) =

(cid:0)sT Σ−1s − 2sT Σ−1¯s + ¯sT Σ−1¯s(cid:1) ,

1
2

whereas the parallel manipulation of Eq. (S21) yields

β
2

N
(cid:88)

i,j=1

β
2

Aij (si − sj − 1)2 =

(cid:2)sT (cid:0)Dout + Din − AT − A(cid:1) s + 2 sT (Din − Dout)1 + M (cid:3) ,

(S23)

14

(S17)

(S18)

(S19)

(S20)

(S21)

(S22)

where 1 is a vector of ones and Din are diagonal matrices whose entries are the in- and out-degrees, Dout
and Din
on s because irrelevant when accounting for normalization, we obtain:

j Aij
i,j Aij. Comparing these last two expressions and removing terms that do not depend

j Aji and M = (cid:80)

ii = (cid:80)

ii = (cid:80)

Σ = 1
β

(cid:0)Dout + Din − AT − A(cid:1)−1

and ¯s = β Σ (cid:0)Dout − Din(cid:1) 1 = s∗ .

(S24)

S5. Bayesian SpringRank

Adopting a Bayesian approach with a factorized Gaussian prior for the ranks, we obtain that the s that maximizes
the posterior distribution is the one that minimizes the regularized SpringRank Hamiltonian Eq. (4), i.e. the s that
solves the linear system Eq. (5). In fact, deﬁning:

P (s) = Z −1(β, α)

e−β α

2 (si−1)2

= Z −1(β, α)

e−β αH0(si) ,

(S25)

(cid:89)

i∈V

is a normalization constant that depends on α and β, and following the same steps as

where Z(β, α) =
before we get:

(cid:105)N/2

(cid:104) 2π
β α

log P (s | A) =

log P (Aij | s) − βα

H0(si) + log (Z (β, α) )

(cid:89)

i∈V

(cid:88)

i,j

(cid:34)

(cid:88)

i∈V

(cid:35)

(cid:88)

i∈V

= − β

H(s) + α

H0(si)

+ C ,

(S26)

where C is a constant that does not depend on the parameters, and thus may be ignored when maximizing log P (s | A).

The parameter c included in the generative model (7) controls for network’s sparsity. We can indeed ﬁx it so to

obtain a network with a desired expected number of edges (cid:104)M (cid:105) as follows:

For a given vector of ranks s and inverse temperature β, the c realizing the desired sparsity will then be:

S6. Fixing c to control for sparsity

(cid:104)M (cid:105) ≡

(cid:104)Aij(cid:105) = c

e− β

2 (si−sj −1)2

.

(cid:88)

i,j

(cid:88)

i,j

c =

(cid:80)

i,j e− β

(cid:104)M (cid:105)
2 (si−sj −1)2 =

(cid:104)k(cid:105)N
2 (si−sj −1)2 ,

(cid:80)

i,j e− β

where (cid:104)k(cid:105) is the expected node degree (cid:104)k(cid:105) = (cid:80)N
i=1
model with Bernoulli distribution.

(cid:2)din

i + dout

i

(cid:3). Similar arguments apply when considering a generative

S7. Comparing optimal β for predicting edge directions

In the main text, (12) and Eq. (13) deﬁne the accuracy of edge prediction, in terms of the number of edges predicted
correctly in each direction and the log-likelihood conditioned on the undirected graph. Here we compute the optimal
values of β for both notions of accuracy. In both computations that follow, the following two facts will be used:

and

ij(β) = 2 (si − sj) e−2β(si−sj ) P 2
P (cid:48)

ij(β) ,

1 =

Pij(β)
1 − Pij(β)

e−2β(si−sj ) .

15

(S27)

(S28)

(S29)

A. Choosing β to optimize edge direction accuracy

We take the derivative of Eq. (12) with respect to β, set it equal to zero, and solve as follows.

0 ≡

∂σa(β)
∂β

=

∂
∂β



1 −

1
2m

(cid:88)

i,j

|Aij − (Aij + Aji) Pij(β)|

 .

(S30)



In preparation to take the derivatives above, note that P (cid:48)
takes one sign, the (j, i) term takes the opposite sign,

ij(β) = −P (cid:48)

ji(β) and that whenever the (i, j) term of σa(β)

Without loss of generality, assume that the (i, j) term is positive and the (j, i) term is negative. This implies that

Aij − (Aij + Aji) Pij(β) = − [Aji − (Aij + Aji) Pji(β)] .

∂
∂β

∂
∂β

|Aij − (Aij + Aji) Pij(β)| = − (Aij + Aji) P (cid:48)

ij(β) ,

|Aji − (Aij + Aji) Pji(β)| = − (Aij + Aji) P (cid:48)

ij(β) .

and

∂
∂β

In other words, the derivatives of the (i, j) and (j, i) terms are identical, and the sign of both depends on whether the
quantity [Aij − (Aij + Aji)Pij(β)] is positive or negative. We can make this more precise by directly including the
sign of the (i, j) term, and by using Eq. (S28), to ﬁnd that

|Aij − (Aij + Aji) Pij(β)| = −2 (Aij + Aji) (si − sj) e−2β(si−sj ) P 2

ij × sign(cid:8)Aij − (Aij + Aji) Pij(β)(cid:9) .

(S34)

Expanding P 2

ij and reorganizing yields

∂
∂β

|Aij − (Aij + Aji) Pij(β)| = −2

(Aij + Aji) (si − sj)
2 cosh [2β (si − sj)] + 2

× sign(cid:8)Aij − (Aij + Aji) Pij(β)(cid:9) .

(S35)

Combining terms (i, j) and (j, i), the optimal inverse temperature for local accuracy ˆβa is that which satisﬁes

N
(cid:88)

0 =

(i,j)∈U (E)

(Aij + Aji) (si − sj)
(cid:105)
2 ˆβa (si − sj)

(cid:104)

cosh

+ 1

× sign(cid:8)Aij − (Aij + Aji) Pij( ˆβa)(cid:9) ,

(S36)

which may be found using standard root-ﬁnding methods.

B. Choosing β to optimize the conditional log likelihood

We take the derivative of Eq. (13) with respect to β, set it equal to zero, and partially solve as follows.

0 ≡

∂σL(β)
∂β

=

∂
∂β





(cid:88)

log

i,j

(cid:19)

(cid:18)Aij + Aji
Aij

+ log

(cid:104)

Pij(β)Aij [1 − Pij(β)]Aji(cid:105)



 .

Combining the (i, j) and (j, i) terms, we get
(cid:18)Aij + Aji
Aij

∂
∂β

0 ≡

(cid:88)

log

(cid:19)

(i,j)∈U (E)

+ log

(cid:19)

(cid:18)Aij + Aji
Aji

(cid:20) Aij
Pij(β)

−

Aji
1 − Pij(β)

(cid:21) ∂Pij(β)
∂β

(i,j)∈U (E)

(cid:88)

(cid:88)

=

=

(i,j)∈U (E)

2 (si − sj) [Aij − (Aij + Aji) Pij(β)]

Pij(β)
1 − Pij(β)

e−2β(si−sj ) .

+ [ Aij log Pij(β) + Aji log [1 − Pij(β)] ]

16

(S31)

(S32)

(S33)

(S37)

(S38)

Applying both Eq. (S28) and Eq. (S29), the optimal inverse temperature for the conditional log likelihood ˆβL is that
which satisﬁes

0 =

(cid:88)

2 (si − sj)

(cid:104)

(cid:105)
Aij − (Aij + Aji) Pij( ˆβL)

,

(i,j)∈U (E)

which, like Eq. (S36) may be found using standard root-ﬁnding methods. Comparing equations Eq. (S36) and
Eq. (S39), we can see that the values of β that maximize the two measures may, in general, be diﬀerent. Table S2
shows for optimal values for ˆβL and ˆβa for various real-world datasets.

17

(S39)

S8. Bitwise accuracy σb

Some methods provide rankings but do not provide a model to estimate Pij, meaning that Eq. (12) and Eq. (13)
cannot be used. Nevertheless, such methods still estimate one bit of information about each pair (i, j): whether the
majority of the edges are from i to j or vice versa. This motivates the use of a bitwise version of σa, which we call
σb,

σb = 1 −

Θ (si − sj) Θ(Aji − Aij) ,

(S40)

1
N 2 − t

(cid:88)

i,j

where Θ(x) = 1 if x > 0 and Θ(x) = 0 otherwise, and N is the number of nodes and t is the number of instances in
which Aij = Aji; there are N 2 − t total bits to predict. Results in terms of this measure on the networks considered
in the main text are shown in Figure S4. In the special case that the network is unweighted (A is a binary adjacency
matrix) and there are no bi-directional edges (if Aij = 1, then Aji = 0), then 1 − σb is the fraction of edges that
violate the rankings in s. In other words, for this particular type of network, 1 − σb is the minimum violations rank
penalty normalized by the total number of edges in the network, i.e., 1
M

i,j Θ(si − sj) Aji.

(cid:80)

S9. Performance metrics

When evaluating the performance of a ranking algorithm in general one could consider a variety of diﬀerent measures.
One possibility is to focus on the ranks themselves, rather than the outcomes of pairwise interactions, and calculate
correlation coeﬃcients as in Fig. 1; this is a valid strategy when using synthetic data thanks to the presence of ground
truth ranks, but can only assess the performance with respect of the speciﬁc generative process used to generate
the pairwise comparisons, as we point out in the main text. This strategy can also be applied for comparisons with
observed real world ranks, as we did in Table S11 and it has been done for instance in [19, 20] to compare the ranks
with those observed in real data in sports. However, the observed ranks might have been derived from a diﬀerent
process than the one implied by the ranking algorithm considered. For instance, in the faculty hiring networks, popular
ranking methods proposed by domain experts for evaluating the prestige of universities do not consider interactions
between institutions, but instead rely on a combination of performance indicators such as ﬁrst-year student retention
or graduation rates. The correlation between observed and inferred ranks should thus be treated as a qualitative
indicator of how well the two capture similar features of the system, such as prestige, but should not be used to
evaluate the performance of a ranking algorithm.

Alternatively, one can look at the outcomes of the pairwise comparisons and relate them to the rankings of the
nodes involved as in Eqs. (12) and (13) for testing prediction performance. A popular metric of this type is the number
of violations (also called upsets), i.e., outcomes where a higher ranked node is defeated by a lower ranked one. This
is very similar to the bitwise accuracy deﬁned in (S40), indeed when there are no ties and two nodes are compared
only once, then they are equivalent. These can be seen as low-resolution or coarse-grained measures of performance:
for each comparison predict a winner, but do not distinguish between cases where the winner is easy to predict and
cases where there is almost a tie. In particular, an upset between two nodes ranked nearby counts as much as an
upset between two nodes that are far away in the ranking. The latter case signals a much less likely scenario. In order
to distinguish these two situations, one can penalize each upset by the nodes’ rank diﬀerence elevated to a certain
power d. This is what the agony function does [18] with the exponent d treated as a parameter to tune based on the
application. When d = 0 we recover the standard number of unweighted upsets.

Note that optimization of agony is often used as a non-parametric approach to detect hierarchies [21], in particular
for ordinal ranks. For ordinal ranks, rank diﬀerences are integer-valued and equal to one for adjacent-ranked nodes,
yet for real-valued scores this is not the case. Therefore the result of the agony minimization problem can vary

18

widely between ordinal and real valued ranking algorithms. (We note that the SpringRank objective function, i.e.,
the Hamiltonian in Eq. (2), can be considered a kind of agony. However, since we assume that nearby pairs are more
likely to interact, it is large for a edge from i to j if i is ranked far above or far below j, and more speciﬁcally whenever
si is far from sj + 1.)

In contrast to the coarse prediction above—which competitor is more likely to win?—we require, when possible,
more precise predictions in Eqs. (12) and (13), which ask how much more likely is one competitor to win? This,
however, requires the ranking algorithm to provide an estimate of Pij, the probability that i wins over j, which is
provided only by BTL and SpringRank; all other methods compared in this study provide orderings or embeddings
without probabilistic predictions.

The conditional log-likelihood σL as deﬁned in Eq. (13) can be seen as a Log Loss often used as a classiﬁcation loss
function [46] in statistical learning. This type of function heavily penalizes ranking algorithms that are very conﬁdent
about an incorrect outcome, e.g. when the predicted Pij is close to 1, i very likely to win over j, but the observed
outcome is that j wins over i. For this reason, this metric is more sensitive to outliers, as when in sports a very
strong team loses against one at the bottom of the league. The accuracy σb deﬁned in Eq. (12) focuses instead in
predicting the correct proportions of wins/losses between two nodes that are matched in several comparisons. This
is less sensitive to outliers, and in fact if Pij is close but not exactly equal to 1, for a large number of comparisons
between i and j, we would expect that j should indeed win few times, e.g. if Pij = 0.99 and i, j are compared 100
times, σa is maximized when i wins 99 times and j wins once.

S10. Parameters used for regularizing ranking methods

When comparing SpringRank to other methods, we need to deal with the fact that certain network structures cause
other methods to fail to return any output. Eigenvector Centrality cannot, for example, be applied to directed trees,
yet this is precisely the sort of structure that one might expect when hierarchy becomes extreme.

More generally, many spectral techniques fail on networks that are not strongly connected, i.e., where it is not the
case that one can reach any node from any other by moving along a path consistent with the edge directions, since
in that case the adjacency matrix is not irreducible and the Perron-Frobenius theorem does not apply. In particular,
nodes with zero out-degree—sometimes called “dangling nodes” in the literature [13]—cause issues for many spectral
methods since the adjacency matrix annihilates any vector supported on such nodes. In contrast, the SpringRank
optimum given by Eq. (3) is unique up to translation whenever the network is connected in the undirected sense, i.e.,
whenever we can reach any node from any other by moving with or against directed edges.

A diﬀerent issue occurs in the case of SyncRank. When edges are reciprocal in the sense that an equal number
of edges point in each direction, they eﬀectively cancel out. That is, if Aij = Aji, the corresponding entries in the
SyncRank comparison matrix will be zero, Cij = Cji = 0, as if i and j were never compared at all. As a result, there
can be nodes i such that Cij = Cji = 0 for all j. While rare, these pathological cases exist in real data and during
cross-validation tests, causing the output of SyncRank to be undeﬁned.

In all these cases, regularization is required. Our regularized implementations of ﬁve ranking methods are described

below:

• Regularized Bradley-Terry-Luce (BTL). If there exist dangling nodes, the Minimization-Maximization
algorithm to ﬁt the BTL model to real data proposed in [38] requires a regularization. In this case we set the
total number of out-edges dout
i = 10−6 for nodes that would have di = 0 otherwise. This corresponds to Wi in
Eq.(3) of [38].

• Regularized PageRank. If there exist dangling nodes, we add an edge of weight 1/N from each dangling
node to every other node in the network. For each dataset we tried three diﬀerent values of the teleportation
parameter, α ∈ {0.4, 0.6, 0.8}, and reported the best results of these three.

• Regularized Rank Centrality. If there exist dangling nodes, we use the regularized version of the algorithm

presented in Eq. (5) of [14] with (cid:15) = 1.

• Regularized SyncRank. If there are nodes whose entries in the comparison matrix C are zero, we add a

small constant (cid:15) = 0.001 to the entries of H in Eq. (13) of Ref. [20], so that D is invertible.

• Regularized Eigenvector Centrality. If the network is not strongly connected, we add a weight of 1/N to

every entry in A and then diagonalize.

19

S11. Supplemental Tables

Comp. Sci. SpringRank MVR US News NRC Eig. C. PageRank
0.57
SpringRank
0.48
MVR
0.41
US News
0.41
NRC
0.74
Eig. C.
-
PageRank

0.80 0.72
0.81 0.73
- 0.73
0.73
-
0.69 0.68
0.41 0.41

0.84
0.80
0.69
0.68
-
0.74

0.96
-
0.81
0.73
0.80
0.48

-
0.96
0.80
0.72
0.84
0.57

Business
SpringRank
MVR
US News
NRC
Eig. C.
PageRank

History
SpringRank
MVR
US News
NRC
Eig. C.
PageRank

SpringRank MVR US News NRC Eig. C. PageRank
0.75
0.74
0.69
0.72
0.60
-
-
-
0.72
0.68
-
0.60

0.98
-
0.72
-
0.92
0.69

0.92
0.92
0.68
-
-
0.72

-
0.98
0.74
-
0.92
0.75

-
-
-
-
-
-

SpringRank MVR US News NRC Eig. C. PageRank
0.69
0.57
0.51
0.44
0.88
-

0.86 0.66
0.86 0.65
- 0.66
0.66
-
0.72 0.59
0.51 0.44

0.86
0.77
0.72
0.59
-
0.88

0.95
-
0.86
0.65
0.77
0.57

-
0.95
0.86
0.66
0.86
0.69

TABLE S1. Pearson correlation coeﬃcients between various rankings of faculty hiring networks. All coeﬃcients are statistically
signiﬁcant (p < 10−9). SpringRank is most highly correlated with Minimum Violations Ranks across all three faculty hiring
networks. Among US News and NRC rankings, SpringRank is more similar to US News. Values for US News and NRC were
[3] for comparison to the ranks available at the same time that the faculty hiring data were collected. The
drawn from Ref.
NRC does not rank business departments.

N M H/M Acc. σa ˆβL

ˆβa

Type
DataSet
Anim. Dom. 21 838 0.174 0.930
Parakeet G1 [5]
Anim. Dom. 19 961 0.193 0.932
Parakeet G2 [5]
0.078 0.923
Asian Elephants [37] Anim. Dom. 20 23
112 7353 0.251 0.881
Business [3]
Fac. Hiring
205 4033 0.220 0.882
Computer Science [3] Fac. Hiring
Fac. Hiring
History [3]
144 3921 0.186 0.909
Soc. Support 415 2497 0.222 0.867
Alak¯apuram [2]
Soc. Support 361 1809 0.241 0.858
Tenpat.t.i [2]

0.008 (0.089)
2.70 6.03 76 (9.1%) / 42
0.011 (0.139)
2.78 18.12 75 (7.8%) / 36
2.33 3.44 2 (8.7%) / 0
0.001 (0.040)
2.04 3.14 1171 (15.9%) / 808 0.019 (0.119)
2.23 8.74 516 (12.8%) / 255 0.013 (0.105)
2.39 5.74 397 (10.1%) / 227 0.012 (0.119)
1.98 7.95 347 (13.9%) / 120 0.011 (0.079)
1.89 8.20 262 (14.5%) / 120 0.012 (0.082)

Viol. (%) / Bound Wt. viol. (per viol.) Depth p-value
2.604 < 10−4
1.879 < 10−4
3.000 0.4466
2.125 < 10−4
2.423 < 10−4
2.234 < 10−4
3.618 < 10−4
3.749 < 10−4

TABLE S2. Statistics for SpringRank applied to real-world networks. Column details are as follows: N is the number
of nodes; M is the number of edges; H/m is the ground state energy per edge; Accuracy σa refers to accuracy in 5-fold
cross-validation tests using temperature ˆβa; ˆβL and ˆβa are temperatures optimizing edge prediction accuracies σL and σa
respectively; Violations refers to the number of edges that violate the direction of the hierarchy as a number, as a percentage
of all edges, with a lower bound provided for reference, computed as the number of unavoidable violations due to reciprocated
edges; Weighted violations are the sum of each violation weighted by the diﬀerence in ranks between the oﬀending nodes; Depth
is smax − smin; p-value refers to the null model described in the Materials and Methods. Relevant performance statistics for
NCAA datasets (53 networks) are reported elsewhere; see Fig. S3.

S12. Supplemental Figures

20

FIG. S1. Performance (Pearson correlation) on synthetic data. Tests were performed as in Fig. 1, but here performance
is measured using Pearson correlation. This favors algorithms like SpringRank and BTL, that produce real-valued ranks, over
ordinal ranking schemes like Minimum Violation Ranking which are not expected to recover latent positions.
(A) Linear
hierarchy diagrams show latent ranks splanted of 100 nodes, drawn from a standard normal distribution, with edges drawn
via the generative model Eq. (7) for indicated β (noise) values. Blue edges point down the hierarchy and red edges point
up, indicated by arrows. (B) Mean accuracies ± one standard deviation (symbols ± shading) are measured as the Pearson
correlation between method output and splanted for 100 replicates. (C, D) Identical to A and B but for hierarchies of N = 102
nodes divided into three tiers. All plots have mean degree 5; see Fig. 1 for performance curves for Spearman correlation r. See
Materials and Methods for synthetic network generation.

(a) US HS

(b) US BS

(c) US CS

(d) Tenpat.t.i

(e) Alak¯apuram

(f) parakeet G1

(g) parakeet G2

(h) planted β = 5.0

(i) planted β = 0.1

(j) Asian elephant

(k) ER
N = 100, (cid:104)k(cid:105) = 3

FIG. S2. Statistical signiﬁcance testing using the null model distribution of energies. Results are 1000 realizations
of the null model where edge directions are randomized while keeping the total number of interactions between each pair ﬁxed,
for real and synthetic networks: a-c) US History (HS), Business (BS) and Computer Science (CS) faculty hiring networks [3];
d-e) social support networks of two Indian villages [2] considering 5 types of interactions (see main manuscript); f,g) aggression
network of parakeet Group 1 and 2 (as in [5]); h,i) planted network using SpringRank generative model with N = 100 and
mean degree (cid:104)k(cid:105) = 5, Gaussian prior for the ranks with average µ = 0.5 and variance 1 (α = 1/β) and two noise levels β = 5.0
and β = 0.1; j) dominance network of asian elephants [37]; k) Erd˝os-R´enyi directed random network with N = 100 and (cid:104)k(cid:105) = 3.
The vertical line is the energy obtained on the real network. In all but the last two cases we reject the null hypothesis that
edge directions are independent of the ranks, and conclude that the hierarchy is statistically signiﬁcant.

21

FIG. S3. Edge prediction accuracy over BTL for NCAA basketball datasets. Distribution of diﬀerences in performance
of edge prediction of SpringRank compared to BTL on NCAA College Basketball regular season matches for (top) Women and
(middle) Men, deﬁned as (left) the probabilistic edge-prediction accuracy σa Eq. (12) and (right) the conditional log-likelihood
σL Eq. (13). Error bars indicate quartiles and markers show medians, corresponding to 50 independent trials of 5-fold cross-
validation, for a total of 250 test sets for each dataset. The bottom plot is obtained by considering the distributions over all
the seasons together. In terms of number of correctly predicted outcomes, SpringRank correctly predicts on average 8 to 16
more outcomes than BTL for each of the 20 Women NCAA seasons and up to 12 more outcomes for each of the 33 Men NCAA
seasons; for the latter dataset, BTL has an average better prediction in 3 out of the 33 seasons. The number of matches played
per season in the test set varies from the past to the most recents years from 747 to 1079.

22

FIG. S4. Bitwise edge direction prediction. Symbols show medians of bitwise edge prediction accuracies Eq. (S40) over
50 realization of 5-fold cross-validation (for a total of 250 trials) compared with the median accuracy for SyncRank; error bars
indicate quartiles. Thus, points above the dashed line at zero indicate better predictions than SyncRank, while values below
indicate that SyncRank performed better.

23

FIG. S5. Edge prediction accuracy with 2-fold cross-validation. Top: the accuracy of probabilistic edge prediction
of SpringRank compared to the median accuracy of BTL on real and synthetic networks deﬁned as (top left) edge-prediction
accuracy σa Eq. (12) and (top right) the conditional log-likelihood σL Eq. (13); (bottom) bitwise edge prediction accuracies σb
Eq. (S40) of SpringRank and other algorithms compared with the median accuracy of SyncRank. Error bars indicate quartiles
and markers show medians, corresponding to 50 independent trials of 2-fold cross-validation, for a total of 100 test sets for
each network. The two synthetic networks are generated with N = 100, average degree 5, and Gaussian-distributed ranks as
in Fig. 1A, with inverse temperatures β = 1 and β = 5. Notice that these results are similar those of Fig. 3, obtained using
5-fold cross-validation.

Computer Science

24

FIG. S6. Summary of SpringRank applied to Computer Science faculty hiring network [3]. (top-left) A linear
hierarchy diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and
red edges point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned)
and the horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency
matrix; blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of
statistical signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized
samples: the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted,
ordered by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means
algorithm.

History

25

FIG. S7. Summary of SpringRank applied to History faculty hiring network [3]. (top-left) A linear hierarchy diagram
showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up.
(top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal
axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red
dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance
test with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted
line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank,
from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Business

26

FIG. S8. Summary of SpringRank applied to Business faculty hiring network [3]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Asian Elephants

27

FIG. S9. Summary of SpringRank applied to Asian Elephants network [37]. (top-left) A linear hierarchy diagram
showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up.
(top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal
axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red
dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance
test with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted
line is the ground state energy obtained on the observed real network.

Parakeet G1

28

FIG. S10. Summary of SpringRank applied to Parakeet G1 network [5]. (top-left) A linear hierarchy diagram showing
inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up. (top-
middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal axis
is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red dots
represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance test
with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted line
is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank, from
top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Parakeet G2

29

FIG. S11. Summary of SpringRank applied to Parakeet G2 network [5]. (top-left) A linear hierarchy diagram showing
inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up. (top-
middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal axis
is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red dots
represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance test
with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted line
is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank, from
top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Tenpat.t.i

30

FIG. S12. Summary of SpringRank applied to Tenpat.t.i social support network [2]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Alak¯apuram

31

FIG. S13. Summary of SpringRank applied to Alak¯apuram social support network [2]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

A physical model for eﬃcient ranking in networks

Caterina De Bacco,1, 2, ∗ Daniel B. Larremore,3, 4, 2, † and Cristopher Moore2, ‡
1Data Science Institute, Columbia University, New York, NY 10027, USA
2Santa Fe Institute, Santa Fe, NM 87501, USA
3Department of Computer Science, University of Colorado, Boulder, CO 80309, USA
4BioFrontiers Institute, University of Colorado, Boulder, CO 80303, USA

We present a physically-inspired model and an eﬃcient algorithm to infer hierarchical rankings of
nodes in directed networks. It assigns real-valued ranks to nodes rather than simply ordinal ranks,
and it formalizes the assumption that interactions are more likely to occur between individuals with
similar ranks.
It provides a natural statistical signiﬁcance test for the inferred hierarchy, and it
can be used to perform inference tasks such as predicting the existence or direction of edges. The
ranking is obtained by solving a linear system of equations, which is sparse if the network is; thus the
resulting algorithm is extremely eﬃcient and scalable. We illustrate these ﬁndings by analyzing real
and synthetic data, including datasets from animal behavior, faculty hiring, social support networks,
and sports tournaments. We show that our method often outperforms a variety of others, in both
speed and accuracy, in recovering the underlying ranks and predicting edge directions.

Introduction

In systems of many individual entities, interactions and
their outcomes are often correlated with these entities’
ranks or positions in a hierarchy. While in most cases
these rankings are hidden from us, their presence is nev-
ertheless revealed in the asymmetric patterns of interac-
tions that we observe. For example, some social groups
of birds, primates, and elephants are organized accord-
ing to dominance hierarchies, reﬂected in patterns of re-
peated interactions in which dominant animals tend to
assert themselves over less powerful subordinates [1]. So-
cial positions are not directly visible to researchers, but
we can infer each animal’s position in the hierarchy by
observing the network of pairwise interactions. Similar
latent hierarchies have been hypothesized in systems of
endorsement in which status is due to prestige, reputa-
tion, or social position [2, 3]. For example, in academia,
universities may be more likely to hire faculty candidates
from equally or more prestigious universities [3].

In all these cases, the direction of the interactions is af-
fected by the status, prestige, or social position of the en-
tities involved. But it is often the case that even the exis-
tence of an interaction, rather than its direction, contains
some information about those entities’ relative prestige.
For example, in some species, animals are more likely to
interact with others who are close in dominance rank [4–
8]; human beings tend to claim friendships with others
of similar or slightly higher status [9]; and sports tourna-
ments and league structures are often designed to match
players or teams based on similar skill levels [10, 11]. This
suggests that we can infer the ranks of individuals in a so-
cial hierarchy using both the existence and the direction
of their pairwise interactions. It also suggests assigning

real-valued ranks to entities rather than simply ordinal
rankings, for instance in order to infer clusters of entities
with roughly equal status with gaps between them.

In this work we introduce a physically-inspired model
that addresses the problems of hierarchy inference, edge
prediction, and signiﬁcance testing. The model, which
we call SpringRank, maps each directed edge to a di-
rected spring between the nodes that it connects, and
ﬁnds real-valued positions of the nodes that minimizes
the total energy of these springs. Because this optimiza-
tion problem requires only linear algebra, it can be solved
for networks of millions of nodes and edges in seconds.

We also introduce a generative model for hierarchical
networks in which the existence and direction of edges de-
pend on the relative ranks of the nodes. This model for-
malizes the assumption that individuals tend to interact
with others of similar rank, and it can be used to create
synthetic benchmark networks with tunable levels of hi-
erarchy and noise. It can also predict unobserved edges,
allowing us to use cross-validation as a test of accuracy
and statistical signiﬁcance. Moreover, the maximum like-
lihood estimates of the ranks coincides with SpringRank
asymptotically.

We test SpringRank and its generative model version
on both synthetic and real datasets, including data from
animal behavior, faculty hiring, social support networks,
and sports tournaments. We ﬁnd that it infers accurate
rankings, provides a simple signiﬁcance test for hierarchi-
cal structure, and can predict the existence and direction
of as-yet unobserved edges. In particular, we ﬁnd that
SpringRank often predicts the direction of unobserved
edges more accurately than a variety of existing methods,
including popular spectral techniques, Minimum Viola-
tion Ranking, and the Bradley-Terry-Luce method.

∗ cdebacco@santafe.edu; Contributed equally.
† daniel.larremore@colorado.edu; Contributed equally.
‡ moore@santafe.edu

Ranking entities in a system from pairwise compar-
isons or interactions is a fundamental problem in many

Related work

8
1
0
2
 
n
u
J
 
3
1
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
4
v
2
0
0
9
0
.
9
0
7
1
:
v
i
X
r
a

contexts, and many methods have been proposed. One
family consists of spectral methods like Eigenvector Cen-
trality [12], PageRank [13], Rank Centrality [14], and the
method of Callaghan et al. [15]. These methods propose
various types of random walks on the directed network
and therefore produce real-valued scores. However, by
design these methods tend to give high ranks to a small
number of important nodes, giving us little information
about the lower-ranked nodes.
In addition, they often
require explicit regularization, adding a small term to
every element of the adjacency matrix if the graph of
comparisons is not strongly connected.

A second family focuses on ordinal rankings, i.e., per-
mutations, that minimize various penalty functions. This
family includes Minimum Violation Rank [16–18] and Se-
rialRank [19] and SyncRank [20]. Minimum Violation
Rank (MVR) imposes a uniform penalty for every viola-
tion or “upset,” deﬁned as an edge that has a direction
opposite to the one expected by the rank diﬀerence be-
tween the two nodes. Non-uniform penalties and other
generalizations are often referred to as agony methods
[21]. For common choices of the penalty function, mini-
mization can be computationally diﬃcult [17, 22], forcing
us to use simple heuristics that ﬁnd local minima.

SerialRank constructs a matrix of similarity scores be-
tween each pair of nodes by examining whether they pro-
duce similar outcomes when compared with the other
nodes, thereby relating the ranking problem to a more
general ordering problem called seriation. SyncRank is
a hybrid method which ﬁrst solves a spectral problem
based on synchronization, embeds node positions on a
half-circle in the complex plane, and then chooses among
the circular permutations of those ranks by minimizing
the number of violations as in MVR.

Random Utility Models [23], such as the Bradley-
Terry-Luce (BTL) model [24, 25], are designed to in-
fer real-valued ranks from data on pairwise preferences.
These models assign a probability to the direction of an
edge conditioned on its existence, but they do not assign
a probability to the existence of an edge. They are ap-
propriate, for instance, when an experimenter presents
subjects with choices between pairs of items, and asks
them which they prefer.

Methods like David’s Score [26] and the Colley ma-
trix [27] compute rankings from proportions of wins and
losses. The latter, which was originally developed by
making mathematical adjustments to winning percent-
ages, is equivalent to a particular case of the general
method we introduce below. Elo score [28], Go Rank [29],
and TrueSkill [30] are also widely used win-loss methods,
but these schemes update the ranks after each match
rather than taking all previous interactions into account.
This specialization makes them useful when ranks evolve
over sequential matches, but less useful otherwise.

Finally, there are fully generative models such the
Probabilistic Niche Model of ecology [31–33], models of
friendship based on social status [9], and more generally
latent space models [34] which assign probabilities to the

2

existence and direction of edges based on real-valued po-
sitions in social space. However, inference of these models
tends to be diﬃcult, with many local optima. Our gen-
erative model can be viewed as a special case of these
models for which inference is especially easy.

In the absence of ground-truth rankings, we can
compare the accuracy of these methods using cross-
validation, computing the ranks using a subset of the
edges in the network and then using those ranks to pre-
dict the direction of the remaining edges. Equivalently,
we can ask them to predict unobserved edges, such as
which of two sports teams will win a game. However,
these methods do not all make the same kinds of pre-
dictions, requiring us to use diﬀerent kinds of cross-
validation. Methods such as BTL produce probabilis-
tic predictions about the direction of an edge, i.e., they
estimate the probability one item will be preferred to
another. Fully generative models also predict the proba-
bility that an edge exists, i.e., that a given pair of nodes
in the network interact. On the other hand, ordinal rank-
ing methods such as MVR do not make probabilistic pre-
dictions, but we can interpret their ranking as a coarse
prediction that an edge is more likely to point in one
direction than another.

The SpringRank model

We represent interactions between N entities as a
weighted directed network, where Aij is the number of
interactions i → j suggesting that i is ranked above j.
This allows both ordinal and cardinal input, including
where pairs interact multiple times. For instance, Aij
could be the number of ﬁghts between i and j that i has
won, or the number of times that j has endorsed i.

Given the adjacency matrix A, our goal is to ﬁnd a
ranking of the nodes. To do so, the SpringRank model
computes the optimal location of nodes in a hierarchy by
imagining the network as a physical system. Speciﬁcally,
each node i is embedded at a real-valued position or rank
si, and each directed edge i → j becomes an oriented
spring with a nonzero resting length and displacement
si − sj. Since we are free to rescale the latent space
and the energy scale, we set the spring constant and the
resting length to 1. Thus, the spring corresponding to an
edge i → j has energy

Hij =

(si − sj − 1)2 ,

(1)

1
2

which is minimized when si − sj = 1.

This version of the model has no tunable parameters.
Alternately, we could allow each edge to have its own
rest length or spring constant, based on the strength of
each edge. However, this would create a large number of
parameters, which we would have to infer from the data
or choose a priori. We do not explore this here.

According to this model, the optimal rankings of the
N ) which minimize

nodes are the ranks s∗ = (s∗

1, . . . , s∗

the total energy of the system given by the Hamiltonian

H(s) =

AijHij =

Aij (si − sj − 1)2 .

(2)

N
(cid:88)

i,j=1

1
2

(cid:88)

i,j

Since this Hamiltonian is convex in s, we can ﬁnd s∗ by
setting ∇H(s) = 0, yielding the linear system

(cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) s∗ = (cid:2)Dout − Din(cid:3) 1 ,

(3)

where 1 is the all-ones vector and Dout and Din are di-
agonal matrices whose entries are the weighted in- and
out-degrees, Dout
j Aji. See SI
Text S1 for detailed derivations.

j Aij and Din

ii = (cid:80)

ii = (cid:80)

The matrix on the left side of Eq. (3) is not invertible.
This is because H is translation-invariant:
it depends
only on the relative ranks si − sj, so that if s∗ = {si}
minimizes H(s) then so does {si + a} for any constant a.
One way to break this symmetry is to invert the matrix
in the subspace orthogonal to its nullspace by comput-
ing a Moore-Penrose pseudoinverse. If the network con-
sists of a single component, the nullspace is spanned by
the eigenvector 1, in which case this method ﬁnds the
s∗ where the average rank (1/N ) (cid:80)
i si = (1/N )s∗ · 1 is
zero. This is related to the random walk method of [15]:
if a random walk moves along each directed edge with
rate 1
2 − ε, then
s∗ is proportional to the perturbation to the stationary
distribution to ﬁrst order in ε.

2 + ε and against each one with rate 1

In practice, it is more eﬃcient and accurate to ﬁx the
rank of one of the nodes and solve the resulting equation
using a sparse iterative solver (see SI Text S1). Faster
still, because this matrix is a Laplacian, recent results [35,
36] allow us to solve Eq. (3) in nearly linear time in M ,
the number of non-zero edges in A.

Another way to break translation invariance is to in-
i aﬀecting each

troduce an “external ﬁeld” H0(si) = 1
2 αs2
node, so that the combined Hamiltonian is

Hα(s) = H(s) +

(4)

α
2

N
(cid:88)

i=1

s2
i .

The ﬁeld H0 corresponds to a spring that attracts every
node to the origin. We can think of this as imposing a
Gaussian prior on the ranks, or as a regularization term
that quadratically penalizes ranks with large absolute
values. This version of the model has a single tunable
parameter, namely the spring constant α. Since H(s)
scales with the total edge weight M = (cid:80)
i,j Aij while
H0(s) scales with N , for a ﬁxed value of α this regular-
ization becomes less relevant as networks become more
dense and the average (weighted) degree M/N increases.
For α > 0 there is a unique s∗ that minimizes Hα,

given by

(cid:2)Dout + Din − (cid:0)A + AT (cid:1) + αI(cid:3) s∗ = (cid:2)Dout − Din(cid:3) 1 ,

3

where I is the identity matrix. The matrix on the left
side is now invertible, since the eigenvector 1 has eigen-
In the limit α → 0, we recover
values α instead of 0.
Eq. (3); the value α = 2 corresponds to the Colley ma-
trix method [27].

Minimizing H(s), or the regularized version Hα(s),
corresponds to ﬁnding the “ground state” s∗ of the
model.
In the next section we show that this corre-
sponds to a maximum-likelihood estimate of the ranks
in a generative model. However, we can use SpringRank
not just to maximize the likelihood, but to compute a
joint distribution of the ranks as a Boltzmann distribu-
tion with Hamiltonian Eq. (4), and thus estimate the
uncertainty and correlations between the ranks. In par-
ticular, the ranks si are random variables following an
N -dimensional Gaussian distribution with mean s∗ and
covariance matrix (SI Text S4)

Σ =

(cid:2)Dout + Din − (cid:0)A + AT + αI(cid:1)(cid:3)−1

.

(6)

1
β

Here β is an inverse temperature controlling the amount
of noise in the model. In the limit β → ∞, the rankings
are sharply peaked around the ground state s∗, while
for β → 0 they are noisy. As we discuss below, we can
estimate β from the observed data in various ways.

The rankings given by SpringRank Eq. (3) and its reg-
ularized form Eq. (5) are easily and rapidly computed by
standard linear solvers.
In particular, iterative solvers
that take advantage of the sparsity of the system can
ﬁnd s∗ for networks with millions of nodes and edges
in seconds. However, as deﬁned above, SpringRank is
not a fully generative model that assigns probabilities to
the data and allows for Bayesian inference. In the next
section we introduce a generative model for hierarchical
networks and show that it converges to SpringRank in
the limit of strong hierarchy.

A generative model

In this section we propose a probabilistic genera-
tive model that takes as its input a set of node ranks
s1, . . . , sN and produces a weighted directed network.
The model also has a temperature or noise parameter
β and a density parameter c. Edges between each pair
of nodes i, j are generated independently of other pairs,
conditioned on the ranks. The expected number of edges
from i to j is proportional to the Boltzmann weight of
the corresponding term in the Hamiltonian Eq. (2),

E[Aij] = c exp(−βHij) = c exp

−

(si − sj − 1)2

,

(cid:21)

(cid:20)

β
2

where the actual edge weight Aij is drawn from a Poisson
distribution with this mean. The parameter c controls
the overall density of the network, giving an expected

(5)

4

number of edges

Predicting edge directions

E[M ] =

E[Aij] = c

exp

−

(si − sj − 1)2

,

(cid:88)

i,j

(cid:88)

i,j

(cid:20)

β
2

(cid:21)

while the inverse temperature β controls the extent to
which edges respect (or defy) the ranks s. For smaller
β, edges are more likely to violate the hierarchy or to
connect distant nodes, decreasing the correlation between
the ranks and the directions of the interactions: for β = 0
the model generates a directed Erd˝os-R´enyi graph, while
in the limit β → ∞ edges only exist between nodes i, j
with si − sj = 1, and only in the direction i → j.

The Poisson distribution may generate multiple edges
between a pair of nodes, so this model generates directed
multigraphs. This is consistent with the interpretation
that Aij is the number, or total weight, of edges from i
to j. However, in the limit as E[Aij] → 0, the Poisson
distribution approaches a Bernoulli distribution, gener-
ating binary networks with Aij ∈ {0, 1}.

The likelihood of observing a network A given ranks s,

inverse temperature β, and density c is

(cid:104)

ce− β

2 (si−sj −1)2(cid:105)Aij

P (A | s, β, c) =

(cid:89)

i,j

Aij!

(cid:104)

exp

−ce− β

2 (si−sj −1)2 (cid:105)

(7)
Taking logs, substituting the maximum-likelihood value
of c, and discarding constants that do not depend on s
or β yields a log-likelihood (see Supplemental Text S2)

If hierarchical structure plays an important role in a
system, it should allow us to predict the direction of pre-
viously unobserved interactions, such as the winner of an
upcoming match, or which direction social support will
ﬂow between two individuals. This is a kind of cross-
validation, which lets us test the statistical signiﬁcance
of hierarchical structure. It is also a principled way of
comparing the accuracy of various ranking methods for
datasets where no ground-truth ranks are known.

We formulate the edge prediction question as follows:
given a set of known interactions, and given that there is
an edge between i and j, in which direction does it point?
In one sense, any ranking method provides an answer to
this question, since we can predict the direction according
to which of i or j is ranked higher based on the known
interactions. When comparing SpringRank to methods
such as SyncRank, SerialRank, and MVR, we use these
“bitwise” predictions, and deﬁne the accuracy σb as the
fraction of edges whose direction is consistent with the
inferred ranking.

But we want to know the odds on each game, not just
the likely winner—that is, we want to estimate the prob-
ability that an edge goes in each direction. A priori,
.
a ranking algorithm does not provide these probabili-
ties unless we make further assumptions about how they
depend on the relative ranks. Such assumptions yield
generative models like the one deﬁned above, where the
conditional probability of an edge i → j is

L(A | s, β) = −βH(s) − M log

(cid:20) (cid:88)

2 (si−sj −1)2 (cid:21)

e− β

, (8)

i,j

Pij(β) =

e−βHij
e−βHij + e−βHji

=

1
1 + e−2β(si−sj )

.

(9)

where H(s) is the SpringRank energy deﬁned in Eq. (2).
In the limit of large β where the hierarchical structure
is strong, the ˆs that maximizes Eq. (8), approaches the
solution s∗ of Eq. (3) that minimizes H(s). Thus the
maximum likelihood estimate ˆs of the rankings in this
model approaches the SpringRank ground state.

As discussed above, we can break translational sym-
metry by adding a ﬁeld H0 that attracts the ranks to
the origin. This is is equivalent to imposing a prior
P (s) ∝ (cid:81)N
. The maximum a posteriori
estimate ˆs then approaches the ground state s∗ of the
Hamiltonian in Eq. (4), given by Eq. (5).

i=1 e− αβ

2 (si−1)2

This model belongs to a larger family of genera-
tive models considered in ecology and network the-
ory [9, 31, 32], and more generally the class of latent space
models [34], where an edge points from i to j with proba-
bility f (si − sj) for some function f . These models typi-
cally have complicated posterior distributions with many
local optima, requiring Monte Carlo methods (e.g. [33])
In our
that do not scale eﬃciently to large networks.
case, f (si − sj) is a Gaussian centered at 1, and the pos-
terior converges to the multivariate Gaussian Eq. (6) in
the limit of strong structure.

The density parameter c aﬀects the probability that an
edge exists, but not its direction. Thus our probabilistic
prediction method has a single tunable parameter β.

Note that Pij is a logistic curve, is monotonic in the
rank diﬀerence si − sj, and has width determined by the
inverse temperature β. SpringRank has this in common
with two other ranking methods: setting γi = e2βsi re-
covers the Bradley-Terry-Luce model [24, 25] for which
Pij = γi/(γi + γj), and setting k = 2β recovers the
probability that i beats j in the Go rank [29], where
Pij = 1/(1 + e−k(si−sj )). However, SpringRank diﬀers
from these methods in how it infers the ranks from ob-
served interactions, so SpringRank and BTL make diﬀer-
ent probabilistic predictions.

In our experiments below, we test various ranking
methods for edge prediction by giving them access to
80% of the edges in the network (the training data) and
then asking them to predict the direction of the remain-
ing edges (the test data). We consider two measures of
accuracy: σa is the average probability assigned to the
correct direction of an edge, and σL is the log-likelihood
of generating the directed edges given their existence. For
simple directed graphs where Aij +Aji ∈ {0, 1}, these are

σa =

AijPij

and σL =

Aij log Pij .

(10)

(cid:88)

i,j

(cid:88)

i,j

In the multigraph case, we ask how well Pij approximates
the fraction of interactions between i and j that point
from i to j [see Eqs. (12) and (13)]. For a discussion of
other performance measures, see Supplemental Text S9.
We perform our probabilistic prediction experiments
as follows. Given the training data, we infer the ranks
using Eq. (5). We then choose the temperature parame-
ter β by maximizing either σa or σL on the training data
while holding the ranks ﬁxed. The resulting values of β,
which we denote ˆβa and ˆβL respectively, are generally
distinct (Supplemental Table S2 and Text S7). This is
intuitive, since a single severe mistake where Aij = 1 but
Pij ≈ 0 reduces the likelihood by a large amount, while
only reducing the accuracy by one edge. As a result,
predictions using ˆβa produce fewer incorrectly oriented
edges, achieving a higher σa on the test set, while predic-
tions using ˆβL will produce fewer dramatically incorrect
predictions where Pij is very low, and thus achieve higher
σL on the test set.

Statistical signiﬁcance using the ground state energy

We can measure statistical signiﬁcance using any test
statistic, by asking whether its value on a given dataset
would be highly improbable in a null model. One such
statistic is the accuracy of edge prediction using a method
such as the one described above. However, this may
become computationally expensive for cross-validation
studies with many replicates, since each fold of each
replicate requires inference of the parameter ˆβa. Here
we propose a test statistic which is very easy to com-
pute, inspired by the physical model behind SpringRank:
namely, the ground state energy. For the unregularized
version Eq. (2), the energy per edge is (see SI Text S3)

H(s∗)
M

=

1
2M

(cid:88)

i

(din

i − dout

i

) s∗

i +

(11)

1
2

.

Since the ground state energy depends on many aspects
of the network structure, and since hierarchical structure
is statistically signiﬁcant if it helps us predict edge direc-
tions, like [37] we focus on the following null model: we
randomize the direction of each edge while preserving the
total number ¯Aij = Aij + Aji of edges between each pair
of vertices. If the real network has a ground state energy
which is much lower than typical networks drawn from
this null model, we can conclude that the hierarchical
structure is statistically signiﬁcant.

This test correctly concludes that directed Erd˝os-R´enyi
graphs have no signiﬁcant structure. It also ﬁnds no sig-
niﬁcant structure for networks created using the genera-
tive model Eq. (7) with β = 0.1, i.e., when the tempera-
ture or noise level 1/β is suﬃciently large the ranks are no

5

longer relevant to edge existence or direction (Fig. S2).
However, we see in the next section that it shows sta-
tistically signiﬁcant hierarchy for a variety of real-world
datasets, showing that H(s∗) is both useful and compu-
tationally eﬃcient as a test statistic.

FIG. 1. Performance on synthetic data. (A) A synthetic
network of N = 100 nodes, with ranks drawn from a standard
Gaussian and edges drawn via the generative model Eq. (7)
for two diﬀerent values of β and average degree 5. Blue edges
point down the hierarchy and red edges point up, indicated
by arrows. (B) The accuracy of the inferred ordering deﬁned
as the Spearman correlation averaged over 100 indendepently
generated networks; error bars indicate one standard devia-
tion. (C, D) Identical to A and B but with ranks drawn from
a mixture of three Gaussians so that the nodes cluster into
three tiers (Materials and Methods). See Fig. S1 for perfor-
mance curves for Pearson correlation r.

Results on real and synthetic data

Having introduced SpringRank, an eﬃcient procedure
for inferring real-valued ranks, a corresponding gener-
ative model, a method for edge prediction, and a test
for the statistical signiﬁcance of hierarchical structure,
we now demonstrate it by applying it to both real and
synthetic data. For synthetic datasets where the ground-
truth ranks are known, our goal is to see to what extent
SpringRank and other algorithms can recover the actual
ranks. For real-world datasets, in most cases we have
no ground-truth ranking, so we apply the statistical sig-
niﬁcance test deﬁned above, and compare the ability of
SpringRank and other algorithms to predict edge direc-

tions given a subset of the interactions.

We compare SpringRank to other widely used meth-
ods: the spectral methods PageRank [13], Eigenvector
Centrality [12] and Rank Centrality [14]; Minimum Vi-
olation Ranking (MVR) [16, 17], SerialRank [19] and
SyncRank [20], which produce ordinal rankings; David’s
score [26]; and the BTL random utility model [24, 25]
using the algorithm proposed in [38], which like our
generative model makes probabilistic predictions. We
also compare unregularized SpringRank with the regu-
larized version α = 2, corresponding to the Colley ma-
trix method [27]. Unfortunately, Eigenvector Centrality,
Rank Centrality, David’s score, and BTL are undeﬁned
when the network is not strongly connected, e.g. when
there are nodes with zero in- or out-degree. In such cases
we follow the common regularization procedure of adding
low-weight edges between every pair of nodes (see Sup-
plemental Text S10).

Performance for synthetic networks

We study two types of synthetic networks, generated
by the model described above. Of course, since the log-
likelihood in this model corresponds to the SpringRank
energy in the limit of large β, we expect SpringRank to
do well on these networks, and its performance should be
viewed largely as a consistency check. But by varying the
distribution of ranks and the noise level, we can illustrate
types of structure that may exist in real-world data, and
test each algorithm’s ability to identify them.

In the ﬁrst type, the ranks are normally distributed
with mean zero and variance one (Fig. 1A). In the second
type, the ranks are drawn from an equal mixture of three
Gaussians with diﬀerent means and variances, so that
nodes cluster into high, middle, and low tiers (Fig. 1C).
This second type is intended to focus on the importance
of real-valued ranks, and to measure the performance of
algorithms that (implicitly or explicitly) impose strong
priors on the ranks when the data defy their expectations.
In both cases, we vary the amount of noise by changing
β while keeping the total number of edges constant (see
Materials and Methods).

Since we wish to compare SpringRank both to meth-
ods such as MVR that only produce ordinal rankings,
and to those like PageRank and David’s Score that pro-
duce real-valued ranks, we measure the accuracy of each
algorithm according to the Spearman correlation ρ be-
tween its inferred rank order and the true one. Results
for the Pearson correlation, where we measure the algo-
rithms’ ability to infer the real-valued ranks as opposed
to just their ordering, are shown in Fig. S1.

We ﬁnd that all the algorithms do well on the ﬁrst
type of synthetic network. As β increases so that the net-
work becomes more structured, with fewer edges (shown
in red in Fig. 1A) pointing in the “wrong” direction,
all algorithms infer ranks that are more correlated with
the ground truth. SpringRank and SyncRank have the

6

FIG. 2. Ranking the History faculty hiring network [3].
(A) Linear hierarchy diagram with nodes embedded at their
inferred SpringRank scores. Blue edges point down the hier-
archy and red edges point up. (B) Histogram of the empirical
distribution of ranks, with a vertical axis of ranks matched to
panel A. (C) Histogram of ground-state energies from 10, 000
randomizations of the network according to the null model
where edge directions are random; the dashed red line shows
the ground state energy of the empirical network depicted in
panels A and B. The fact that the ground state energy is so
far below the tail of the null model is overwhelming evidence
that the hierarchical structure is statistically signiﬁcant, with
a p-value < 10−4

.

highest accuracy, followed closely by the Colley matrix
method and BTL (Fig. 1B). Presumably the Colley ma-
trix works well here because the ranks are in fact drawn
from a Gaussian prior, as it implicitly assumes.

Results for the second type of network are more nu-
anced. The accuracy of SpringRank and SyncRank in-
creases rapidly with β with exact recovery around β = 1.
Inter-
SerialRank also performs quite well on average.

7

FIG. 3. Edge prediction accuracy over BTL. Distribution of diﬀerences in performance of edge prediction of SpringRank
compared to BTL on real and synthetic networks deﬁned as (A) edge-prediction accuracy σa Eq. (12) and (B) the conditional
log-likelihood σL Eq. (13). Error bars indicate quartiles and markers show medians, corresponding to 50 independent trials of
5-fold cross-validation, for a total of 250 test sets for each network. The two synthetic networks are generated with N = 100,
average degree 5, and Gaussian-distributed ranks as in Fig. 1A, with inverse temperatures β = 1 and β = 5. For each
experiment shown, the fractions of trials in which each method performed equal to or better than BTL are shown in Table I.
These diﬀerences correspond to prediction of an additional 1 to 12 more correct edge directions, on average.

estingly, the other methods do not improve as β in-
creases, and many of them decrease beyond a certain
point (Fig. 1D). This suggests that these algorithms be-
come confused when the nodes are clustered into tiers,
even when the noise is small enough that most edges have
directions consistent with the hierarchy.
SpringRank
takes advantage of the fact that edges are more likely
between nodes in the same tier (Fig. 1C), so the mere
existence of edges helps it cluster the ranks.

These synthetic tests suggest that real-valued ranks
capture information that ordinal ranks do not, and that
many ranking methods perform poorly when there are
substructures in the data such as tiered groups. Of
course, in most real-world scenarios, the ground-truth
ranks are not known, and thus edge prediction and other
forms of cross-validation should be used instead. We turn
to edge prediction in the next section.

Performance for real-world networks

As discussed above, in most real-world networks, we
have no ground truth for the ranks. Thus we focus on
our ability to predict edge directions from a subset of
the data, and measure the statistical signiﬁcance of the
inferred hierarchy.

We apply our methods to datasets from a diverse set
of ﬁelds, with sizes ranging up to N = 415 nodes and
up to 7000 edges (see Table S2): three North Ameri-
can academic hiring networks where Aij is the number
of faculty at university j who received their doctorate
from university i, for History (illustrated in Figs. 2A and
B), Business, and Computer Science departments [3]; two

networks of animal dominance among captive monk para-
keets [5] and one among Asian elephants [37] where Aij
is the number of dominating acts by animal i toward an-
imal j; and social support networks from two villages
in Tamil Nadu referred to (for privacy reasons) by the
pseudonyms “Tenpat.t.i” and “Alak¯apuram,” where Aij
is the number of distinct social relationships (up to ﬁve)
through which person i supports person j [2]; and 53 net-
works of NCAA Women’s and Men’s college basketball
matches during the regular season, spanning 1985-2017
(Men) and 1998-2017 (Women), where Aij = 1 if team i
beat team j. Each year’s network comprises a diﬀerent
number of matches, ranging from 747 to 1079 [39].

Together, these examples cover prestige, dominance,
and social hierarchies. In each of these domains, infer-
ring ranks from interactions is key to further analysis.
Prestige hierarchies play an unequivocal role in the dy-
namics of academic labor markets [40]; in behavioral ecol-
ogy, higher-ranked individuals in dominance hierarchies
are believed to have higher ﬁtness [1, 41]; and patterns
of aggression are believed to reveal animal strategies and
cognitive capacities [4–8]. Finally, in social support net-
works, higher ranked individuals have greater social capi-
tal and reputational standing [42, 43], particularly in set-
tings in which social support is a primary way to express
and gain respect and prestige [44].

We ﬁrst applied our ground state energy test for the
presence of statistically signiﬁcant hierarchy, rejecting
the null hypothesis with p < 10−4 in almost all cases
(e.g., for History faculty hiring, see Fig. 2C). The one ex-
ception is the Asian Elephants network for which p > 0.4.
This corroborates the original study of this network [37],
which found that counting triad motifs shows no signif-

8

FIG. 4. Probabilistic edge prediction accuracy σa of
SpringRank vs. BTL. For 50 independent trials of 5-fold
cross-validation (250 total folds per network), the values of σa
for SpringRank and BTL are shown on the vertical axis and
the horizontal axis respectively. Points above the diagonal,
shown in blue, are trials where SpringRank is more accurate
than BTL. The fractions for which each method is superior
are shown in plot legends, matching Table I.

icant hierarchy [45]. This is despite the fact that one
can ﬁnd an appealing ordering of the elephants using the
Minimum Violation Rank method, with just a few vio-
lating edges (SI Fig. S9). Thus the hierarchy found by
MVR may well be illusory.

As described above, we performed edge prediction ex-
periments using 5-fold cross-validation, where 80% of the
edges are available to the algorithm as training data, and
a test set consisting of 20% of the edges is held out (see
Materials and Methods). To test SpringRank’s ability to
make probabilistic predictions, we compare it to BTL.

We found that SpringRank outperforms BTL, both in
terms of the accuracy σa (Fig. 3A) and, for most net-
works, the log-likelihood σL (Fig. 3B). The accuracy of
both methods has a fairly broad distribution over the tri-
als of cross-validation, since in each network some subsets
of the edges are harder to predict than others when they
are held out. However, as shown in Fig. 4, in most tri-
als SpringRank was more accurate than BTL. Fig. 3A
and Table I show that SpringRank predicts edge direc-
tions more accurately in the majority of trials of cross-
validation for all nine real-world networks, where this
majority ranges from 62% for the parakeet networks to
100% for the Computer Science hiring network.

Table I shows that SpringRank also obtained a higher
log-likelihood σL than BTL for 6 of the 9 real-world net-
works. Regularizing SpringRank with α = 2 does not ap-
pear to signiﬁcantly improve either measure of accuracy
(Fig. 3). We did not attempt to tune the regularization
parameter α.

To compare SpringRank with methods that do not
make probabilistic predictions, including those that pro-

FIG. 5. Bitwise prediction accuracy σb of SpringRank
vs. SyncRank. For 50 independent trials of 5-fold cross-
validation (250 total folds per NCAA season), the fraction
of correctly predicted game outcomes σb for SpringRank and
Syncrank are shown on the vertical axis and the horizontal
axis respectively. Points above the equal performance line,
shown in blue, are trials where SpringRank is more accurate
than SyncRank; the fractions for which each method is supe-
rior are shown in plot legends.

duce ordinal rankings, we measured the accuracy σb of
“bitwise” predictions, i.e., the fraction of edges consis-
tent with the infered ranking. We found that spectral
methods perform poorly here, as does SerialRank.
In-
terestingly, BTL does better on the NCAA networks in
terms of bitwise prediction than it does for probabilistic
predictions, suggesting that it is better at rank-ordering
teams than determining their real-valued position.

We found that SyncRank is the strongest of the or-
dinal methods, matching SpringRank’s accuracy on the
parakeet and business school networks, but SpringRank
outperforms SyncRank on the social support and NCAA
networks (see Fig. S4). We show a trial-by-trial compari-
son of SpringRank and SyncRank in Fig. 5, showing that
in most trials of cross-validation SpringRank makes more
accurate predictions for the NCAA networks.

To check whether our results were dependent on the
choice of holding out 20% of the data, we repeated our
experiments using 2-fold cross-validation, i.e., using 50%
of network edges as training data and trying to predict
the other 50%. We show these results in Fig. S5. While
all algorithms are less accurate in this setting, the com-
parison between algorithms is similar to that for 5-fold
cross-validation.

Finally, the real-valued ranks found by SpringRank
shed light on the organization and assembly of real-world
networks (see Figs. S6, S7, S8, S12, and S13). For exam-
ple, we found that ranks in the faculty hiring networks
have a long tail at the top, suggesting that the most pres-
tigious universities are more separated from those below
them than an ordinal ranking would reveal. In contrast,
ranks in the social support networks have a long tail
at the bottom, suggesting a set of people who do not
have suﬃcient social status to provide support to oth-
ers. SpringRank’s ability to ﬁnd real-valued ranks makes
these distributions amenable to statistical analysis, and

9

% trials higher σa vs BTL % trials higher σL vs BTL
Type SpringRank +regularization SpringRank +regularization

Dataset
Comp. Sci. [3]
Alak¯apuram [2]
Synthetic β = 5
History [3]
NCAA Women (1998-2017) [39]
Tenpat.t.i [2]
Synthetic β = 1
NCAA Men (1985-2017) [39]

Faculty Hiring
Social Support
Synthetic
Faculty Hiring
Basketball
Social Support
Synthetic
Basketball
Parakeet G1 [5] Animal Dominance
Faculty Hiring
Parakeet G2 [5] Animal Dominance

Business [3]

100.0
99.2†
98.4
97.6†
94.4†
88.8
83.2
76.0†
71.2†
66.8†
62.0

97.2
99.6
63.2
96.8
87.0
93.6
65.2
62.3
56.8
59.2
51.6

100.0
100.0
76.4
98.8
69.1
100.0
98.4
68.5
41.2
39.2
47.6

99.6
100.0
46.4
98.8
51.0
100.0
98.4
52.4
37.2
36.8
47.2

TABLE I. Edge prediction with BTL as a benchmark. During 50 independent trials of 5-fold cross-validation (250 total
folds per network), columns show the the percentages of instances in which SpringRank Eq. (3) and regularized SpringRank
Eq. (5) with α = 2 produced probabilistic predictions with equal or higher accuracy than BTL. Distributions of accuracy
improvements are shown in Fig. 3. Center columns show accuracy σa and right columns show σL (Materials and Methods).
Italics indicate where BTL outperformed SpringRank for more than 50% of tests. † Dagger symbols indicate tests that are
shown in detail in Fig. 4. NCAA Basketball datasets were analyzed one year at a time

.

we suggest this as a direction for future work.

Conclusions

SpringRank is a mathematically principled, physics-
inspired model for hierarchical structure in networks
It yields a simple and highly
of directed interactions.
scalable algorithm, requiring only sparse linear algebra,
which enables analysis of networks with millions of nodes
and edges in seconds. Its ground state energy provides
a natural test statistic for the statistical signiﬁcance of
hierarchical structure.

While the basic SpringRank algorithm is nonparamet-
ric, a parameterized regularization term can be included
as well, corresponding to a Gaussian prior. While reg-
ularization is often required for BTL, Eigenvector Cen-
trality, and other commonly used methods (Supplemen-
tal Text S10) it is not necessary for SpringRank and our
tests indicate that its eﬀects are mixed.

We also presented a generative model that allows one
to create synthetic networks with tunable levels of hierar-
chy and noise, whose posterior coincides with SpringRank
in the limit where the eﬀect of the hierarchy is strong.
By tuning a single temperature parameter, we can use
this model to make probabilistic predictions of edge di-
rections, generalizing from observed to unobserved inter-
actions. Therefore, after conﬁrming its ability to infer
ranks in synthetic networks where ground truth ranks
are known, we measured SpringRank’s ability to to pre-
dict edge directions in real networks. We found that
in networks of faculty hiring, animal interactions, social
support, and NCAA basketball, SpringRank often makes
better probabilistic predictions of edge predictions than
the popular Bradley-Terry-Luce model, and performs as
well or better than SyncRank and a variety of other meth-
ods that produce ordinal rankings.

SpringRank is based on springs with quadratic poten-
tials, but other potentials may be of interest. For in-
stance, to make the system more tolerant to outliers while
remaining convex, one might consider a piecewise poten-
tial that is quadratic for small displacements and linear
otherwise. We leave this investigation of alternative po-
tentials to future work.

Given its simplicity, speed, and high performance, we
believe that SpringRank will be useful in a wide vari-
ety of ﬁelds where hierarchical structure appears due to
dominance, social status, or prestige.

Materials and methods

Synthetic network generation

Networks were generated in three steps. First, node
ranks splanted were drawn from a chosen distribution. For
Test 1, N = 100 ranks were drawn from a standard nor-
mal distribution, while for Test 2, 34 ranks were drawn
from each of three Gaussians, N (−4, 2), N (0, 1
2 ), and
N (4, 1) for a total of N = 102. Second, an average de-
gree (cid:104)k(cid:105) and a value of the inverse temperature β were
chosen. Third, edges were drawn generated to Eq. (7)
with c = (cid:104)k(cid:105)N/ (cid:80)
i,j exp [−(β/2)(si − sj − 1)2] so that
the expected mean degree is (cid:104)k(cid:105) (see SI Text S6).

This procedure resulted in directed networks with the
desired hierarchical structure, mean degree, and noise
level. Tests were conducted for (cid:104)k(cid:105) ∈ [5, 15], β ∈ [0.1, 5],
and all performance plots show mean and standard devi-
ations for 100 replicates.

Performance measures for edge prediction

For multigraphs, we deﬁne the accuracy of probabilis-
tic edge prediction as the extent to which Pij is a good

estimate of the fraction of interactions between i and j
that point from i to j, given that there are any edges to
predict at all, i.e., assuming ¯Aij = Aij + Aji > 0. If this
prediction were perfect, we would have Aij = ¯AijPij. We
deﬁne σA as 1 minus the sum of the absolute values of
the diﬀerence between Aij and this estimate,

σa = 1 −

1
2M

(cid:88)

i,j

(cid:12)
(cid:12)Aij − ¯Aij Pij

(cid:12)
(cid:12) ,

(12)

where M is the number of directed edges in the subset
of the network under consideration, e.g., the training or
test set. Then σa = 1 if Pij = Aij/ ¯Aij for all i, j, and
σa = 0 if for each i, j all the edges go from i to j (say)
but Pij = 0 and Pji = 1.

To measure accuracy via the conditional log-likelihood,
we ask with what probability we would get the directed
network A from the undirected network ¯A if each edge
between i and j points from i → j with probability Pij
and from j → i with probability Pji = 1−Pij. This gives

σL = log Pr[A | ¯A]
(cid:88)

=

log

(cid:19)

(cid:18)Aij + Aji
Aij

i,j

+ log

(cid:104)

P Aij
ij

[1 − Pij]Aji(cid:105)

,

(13)

y

where (cid:0)x
(cid:1) is the binomial coeﬃcient. We disregard the
ﬁrst term of this sum since it does not depend on P .
If we wish to compare networks of diﬀerent sizes as in
Fig. 3, we can normalize σL by the number of edges.
For an extensive discussion of performance metrics see
Supplementary Text S9.

Statistical signiﬁcance of ranks

We compute a standard left-tailed p-value for the sta-
tistical signiﬁcance of the ranks s∗ by comparing the
ground state energy Eq. (11) of the real network A with
the null distribution of ground state energies of an ensem-
ble of networks ˜A drawn from the null model where ¯Aij is

10

kept ﬁxed, but the direction of each edge is randomized.

p-value = Pr[H(s∗; A) ≤ H(˜s∗; ˜A)] .

(14)

In practice, this p-value is estimated by drawing many
samples from the null distribution by randomizing the
edge directions of A to produce ˜A, computing the ranks
˜s∗ from Eq. (3), and then computing the ground state
energy Eq. (11) of each.

Cross-validation tests

We performed edge prediction using 5-fold cross-
validation. In each realization, we divide the interacting
pairs i, j, i.e., those with nonzero ¯Aij = Aij + Aji, into
ﬁve equal groups. We use four of these groups as a train-
ing set, inferring the ranks and setting β to maximize σa
or σL (on the left and right of Fig. 3 respectively). We
then use the ﬁfth group as a test set, asking the algorithm
for Pij for each pair i, j in that group, and report σa or
σL on that test set. By varying which group we use as the
test set, we get 5 trials per realization: for instance, 50
realizations give us 250 trials of cross-validation. Results
for 2-fold cross-validation are reported in SI.

Acknowledgements

CDB and CM were supported by the John Tem-
pleton Foundation. CM was also supported by the
Army Research Oﬃce under grant W911NF-12-R-0012.
DBL was supported by NSF award SMA-1633747 and
the Santa Fe Institute Omidyar Fellowship. We thank
Aaron Clauset and Johan Ugander for helpful com-
ments.
Competing Interests: The authors declare
that they have no competing interests. All authors
derived the model, analyzed results, and wrote the
manuscript. CDB wrote Python implementations and
DBL wrote MATLAB implementations. Open-source
code in Python, MATLAB, and SAS/IML available at
https://github.com/cdebacco/SpringRank.

[1] C. Drews, The concept and deﬁnition of dominance in

animal behaviour, Behaviour 125, 283 (1993).

[2] E. A. Power, Social support networks and religiosity in
rural South India, Nature Human Behaviour 1, 0057
(2017).

[3] A. Clauset, S. Arbesman, D. B. Larremore, Systematic
inequality and hierarchy in faculty hiring networks, Sci-
ence Advances 1, e1400005 (2015).

[4] S. D. Cˆot´e, M. Festa-Bianchet, Reproductive success in
female mountain goats: the inﬂuence of age and social
rank, Animal Behaviour 62, 173 (2001).

[5] E. A. Hobson, S. DeDeo, Social feedback and the emer-
gence of rank in animal society, PLoS Computational Bi-
ology 11, e1004411 (2015).

[6] C. J. Dey, J. S. Quinn, Individual attributes and self-
organizational processes aﬀect dominance network struc-
ture in pukeko, Behavioral Ecology 25, 1402 (2014).
[7] C. J. Dey, A. R. Reddon, C. M. O’Connor, S. Balshine,
Network structure is related to social conﬂict in a cooper-
atively breeding ﬁsh, Animal Behaviour 85, 395 (2013).
[8] M. A. Cant, J. B. Llop, J. Field, Individual variation
in social aggression and the probability of inheritance:
theory and a ﬁeld test, The American Naturalist 167,
837 (2006).

[9] B. Ball, M. E. Newman, Friendship networks and social

status, Network Science 1, 16 (2013).

[10] S. Szymanski, The economic design of sporting contests,

Journal of Economic Literature 41, 1137 (2003).

11

[33] A. Z. Jacobs, J. A. Dunne, C. Moore, A. Clauset, Un-
tangling the roles of parasites in food webs with gener-
ative network models, arXiv preprint arXiv:1505.04741
(2015).

[34] P. D. Hoﬀ, A. E. Raftery, M. S. Handcock, Latent space
approaches to social network analysis, Journal of the
American Statistical Association 97, 1090 (2001).
[35] D. A. Spielman, S.-H. Teng, Nearly linear time algo-
rithms for preconditioning and solving symmetric, diago-
nally dominant linear systems, SIAM Journal on Matrix
Analysis and Applications 35, 835 (2014).

[36] I. Koutis, G. L. Miller, R. Peng, A nearly-m log n time
solver for SDD linear systems, Proc. 52nd Foundations
of Computer Science (FOCS) (IEEE Presss, 2011), pp.
590–598.

[37] S. de Silva, V. Schmid, G. Wittemyer, Fission–fusion pro-
cesses weaken dominance networks of female asian ele-
phants in a productive habitat, Behavioral Ecology 28,
243 (2016).

[38] D. R. Hunter, MM algorithms for generalized Bradley-

Terry models, Annals of Statistics pp. 384–406 (2004).

[39] NCAA, http://www.ncaa.org/championships/statistics

(2018).

[40] S. F. Way, D. B. Larremore, A. Clauset, Gender, pro-
ductivity, and prestige in computer science faculty hir-
ing networks, Proc. 25th Intl Conf on World Wide Web
(2016), pp. 1169–1179.

[41] B. Majolo, J. Lehmann, A. de Bortoli Vizioli, G. Schino,
Fitness-related beneﬁts of dominance in primates, Amer-
ican Journal of Physical Anthropology 147, 652 (2012).
[42] N. Lin, Social Capital: A Theory of Social Structure and
Action, vol. 19 (Cambridge University Press, 2002).
[43] K. S. Cook, M. Levi, R. Hardin, Whom can we trust?:
How groups, networks, and institutions make trust possi-
ble (Russell Sage Foundation, 2009).

[44] M. Mines, Public faces, private lives: Community and in-
dividuality in South India (University of California Press,
1994).

[45] D. Shizuka, D. B. McDonald, A social network perspec-
tive on measurements of dominance hierarchies, Animal
Behaviour 83, 925 (2012).

[46] L. Rosasco, E. D. Vito, A. Caponnetto, M. Piana,
A. Verri, Are loss functions all the same?, Neural Com-
putation 16, 1063 (2004).

[11] R. Baumann, V. A. Matheson, C. A. Howe, Anomalies
in tournament design: the madness of march madness,
Journal of Quantitative Analysis in Sports 6 (2010).
[12] P. Bonacich, Power and centrality: A family of measures,

American Journal of Sociology 92, 1170 (1987).

[13] L. Page, S. Brin, R. Motwani, T. Winograd, The PageR-
ank citation ranking: Bringing order to the web., Tech.
rep., Stanford InfoLab (1999).

[14] S. Negahban, S. Oh, D. Shah, Rank centrality: Ranking
from pairwise comparisons, Operations Research (2016).
[15] T. Callaghan, P. J. Mucha, M. A. Porter, Random walker
ranking for NCAA division IA football, American Math-
ematical Monthly 114, 761 (2007).

[16] I. Ali, W. D. Cook, M. Kress, On the minimum violations
ranking of a tournament, Management Science 32, 660
(1986).

[17] P. Slater, Inconsistencies in a schedule of paired compar-

isons, Biometrika 48, 303 (1961).

[18] M. Gupte, P. Shankar, J. Li, S. Muthukrishnan, L. Iftode,
Finding hierarchy in directed online social networks,
Proc. 20th Intl. Conf. on the World Wide Web (ACM,
2011), pp. 557–566.

[19] F. Fogel, A. d’Aspremont, M. Vojnovic, Serialrank: Spec-
tral ranking using seriation, Advances in Neural Informa-
tion Processing Systems (2014), pp. 900–908.

[20] M. Cucuringu, Sync-rank: Robust ranking, constrained
ranking and rank aggregation via eigenvector and sdp
synchronization, IEEE Transactions on Network Science
and Engineering 3, 58 (2016).

[21] E. Letizia, P. Barucca, F. Lillo, Resolution of ranking
hierarchies in directed networks, PloS one 13, e0191604
(2018).

[22] N. Tatti, Tiers for peers: a practical algorithm for discov-
ering hierarchy in weighted networks, Data Mining and
Knowledge Discovery 31, 702 (2017).

[23] K. E. Train, Discrete Choice Methods with Simulation

(Cambridge University Press, 2009).

[24] R. A. Bradley, M. E. Terry, Rank analysis of incom-
plete block designs: I. the method of paired comparisons,
Biometrika 39, 324 (1952).

[25] R. D. Luce, On the possible psychophysical laws., Psy-

chological Review 66, 81 (1959).

[26] H. A. David, Ranking from unbalanced paired-

comparison data, Biometrika 74, 432 (1987).

[27] W. N. Colley, Colley’s bias free college football rank-
ing method: The Colley matrix explained (2002).
Http://www.colleyrankings.com/matrate.pdf.

[28] A. E. Elo, The Rating of Chessplayers, Past and Present

(Arco Pub., 1978).

[29] R. Coulom, Whole-history rating: A Bayesian rating sys-
tem for players of time-varying strength, International
Conference on Computers and Games (Springer, 2008),
pp. 113–124.

[30] R. Herbrich, T. Minka, T. Graepel, Trueskill: a Bayesian
skill rating system, Advances in Neural Information Pro-
cessing Systems (2007), pp. 569–576.

[31] R. J. Williams, A. Anandanadesan, D. Purves, The prob-
abilistic niche model reveals the niche structure and role
of body size in a complex food web, PLoS ONE 5, e12092
(2010).

[32] R. J. Williams, D. W. Purves, The probabilistic niche
model reveals substantial variation in the niche structure
of empirical food webs, Ecology 92, 1849 (2011).

Supporting Information (SI)

S1. Deriving the linear system minimizing the Hamiltonian

The SpringRank Hamiltonian Eq. (2) is convex in s and we set its gradient ∇H(s) = 0 to obtain the global

minimum:

Let the weighted out-degree and in-degree be dout
written as

i = (cid:80)

j Aij and din

i = (cid:80)

j Aji, respectively. Then Eq. (S1) can be

∂H
∂si

(cid:88)

=

j

[Aij (si − sj − 1) − Aji (sj − si − 1)] = 0 .

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i − din
i

(cid:1) −

[Aij + Aji] sj = 0 .

(cid:88)

j

We now write the system of N equations together by introducing the following matrix notation. Let Dout =
diag(dout
N ) be diagonal matrices, let 1 be the N -dimensional vector of all ones.
Then Eq. (S2) becomes

N ) and Din = diag(din

1 , . . . , din

, . . . , dout

1

(cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) s = (cid:2)Dout − Din(cid:3) 1 .

This is a linear system of the type B s = b, where B = (cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) and b = (cid:2)Dout − Din(cid:3) 1. The rank
of B is at most N − 1 and more generally, if the network represented by A consists of C disconnected components, B
will have rank N − C. In fact, B has an eigenvalue 0 with multiplicity C, and the eigenvector 1 is in the nullspace.
B is not invertible, but we can only invert in the N − C-dimensional subspace orthogonal to the nullspace of B. The
family of translation-invariant solutions s∗ is therefore deﬁned by

s∗ = (cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3)−1 (cid:2)Dout − Din(cid:3) 1 ,

in which the notation [·]−1 should be taken as the Moore-Penrose pseudoinverse.

In practice, rather than constructing the pseudo-inverse, it will be more computationally eﬃcient (and for large
systems, more accurate) to solve the linear system in an iterative fashion. Since we know that solutions may be
translated up or down by an arbitrary constant, the system can be made full-rank by ﬁxing the position of an
arbitrary node 0. Without loss of generality, let sN = 0. In this case, terms that involve sN can be dropped from
Eq. (S2), yielding

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i − din
i

(cid:1) −

[Aij + Aji] sj = 0 ,

i (cid:54)= N

− (cid:0)dout

N − din
N

(cid:1) −

[AN j + AjN ] sj = 0 .

N −1
(cid:88)

j=1

N −1
(cid:88)

j=1

N −1
(cid:88)

j=1

Adding Eq. (S5) to Eq. (S6) yields

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i + dout

N − din

i − din
N

(cid:1) −

[Aij + AN j + Aji + AjN ] sj = 0 ,

(S7)

which can be written in matrix notation as

(cid:105)
(cid:104)
Dout + Din − ˚A

s = (cid:2)Dout − Din(cid:3) 1 + (cid:0)dout

N − din
N

(cid:1) 1 ,

where

˚Aij = Aij + AN j + Aji + AjN .

In this formulation, Eq. (S8) can be solved to arbitrary precision using iterative methods that take advantage of the
sparsity of ˚A. The resulting solution may then be translated by an arbitrary amount as desired.

12

(S1)

(S2)

(S3)

(S4)

(S5)

(S6)

(S8)

(S9)

13

(S12)

(S13)

S2. Poisson generative model

The expected number of edges from node i to node j is c exp

and therefore the likelihood of

(cid:104)

2 (si − sj − 1)2(cid:105)
− β

observing a network A, given parameters β, s, and c is

(cid:104)

c e− β

2 (si−sj −1)2(cid:105)Aij

P (A | s, β, c) =

(cid:89)

i,j

Aij!

(cid:104)
exp

−c e− β

2 (si−sj −1)2(cid:105)

.

(S10)

Taking logs yields

log P (A | s, β, c) =

Aij log c −

Aij (si − sj − 1)2 − log [Aij!] − ce− β

2 (si−sj −1)2

.

(S11)

(cid:88)

i,j

β
2

Discarding the constant term log [Aij!], and recognizing the appearance of the SpringRank Hamiltonian H(s), yields

L(A | s, β, c) = −βH(s) +

Aij log c −

ce− β

2 (si−sj −1)2

.

(cid:88)

i,j

(cid:88)

i,j

Taking ∂L/∂c and setting it equal to zero yields

ˆc =

(cid:80)

i,j Aij
2 (si−sj −1)2 ,

(cid:80)

i,j e− β

which has the straightforward interpretation of being the ratio between the number of observed edges and the expected
number of edges created in the generative process for c = 1. Substituting in this solution and letting M = (cid:80)
i,j Aij
yields

L(A | s, β) = −βH(s) + M log ˆc −

ˆc e− β

2 (si−sj −1)2

(cid:88)

i,j





(cid:88)

i,j

M
β





(cid:88)

i,j





 .

= −βH(s) + M log M − M log

e− β

2 (si−sj −1)2

 − M .

(S14)

The terms M log M and M may be neglected since they do not depend on the parameters, and we divide by β,
yielding a log-likelihood of

L(A | s, β) = −H(s) −

log

e− β

2 (si−sj −1)2

(S15)

(cid:3) where (cid:104)·(cid:105)E denotes
Note that the SpringRank Hamiltonian may be rewritten as H(s) = M (cid:2) 1
the average over elements in the edge set E. In other words, H(s) scales with M and the square of the average spring
length. This substitution for H(s) allows us to analyze the behavior of the log-likelihood

2 (cid:104)Aij(si − sj − 1)2(cid:105)E

L(A | s, β) = −M

(cid:68)

Aij (si − sj − 1)2(cid:69)

+

log

1
β

E

e− β

2 (si−sj −1)2





(cid:88)

i,j










.






1
2

(S16)

Inside the logarithm there are N 2 terms of ﬁnite value, so that the logarithm term is of order O( log N
β ). Thus, for well-
resolved hierarchies, i.e. when β is large enough that the sampled edges consistently agree with the score diﬀerence
between nodes, the maximum likelihood ranks ˆs approach the ranks s∗ found by minimizing the Hamiltonian. In
practice, exactly maximizing the likelihood would require extensive computation, e.g. by using local search heuristic
or Markov chain Monte Carlo sampling.

S3. Rewriting the energy

The Hamiltonian Eq. (2) can be rewritten as

2 H(s) =

Aij (si − sj − 1)2

N
(cid:88)

i,j=1

N
(cid:88)

i,j=1

N
(cid:88)

=

=

N
(cid:88)

=

i

Aij

(cid:0)s2

i + s2

j − 2sisj + 1 − 2si + 2sj

(cid:1)

N
(cid:88)

s2
i

Aij +

N
(cid:88)

N
(cid:88)

s2
j

j

i=1

Aij − 2

(cid:88)

(cid:88)

si

Aijsj + M

i

j

i

j=1

−2

N
(cid:88)

N
(cid:88)

si

i=1

j=1

Aij + 2

N
(cid:88)

N
(cid:88)

sj

j=1

i=1

Aij

s2
i

(cid:0)dout

i + din
i

(cid:1) − 2

si

(cid:0)dout

i − din
i

(cid:1) + M − 2

(cid:88)

(cid:88)

si

Aijsj .

(cid:88)

i

i

j

From Eq. (S2) we have

(cid:88)

s2
i

(cid:0)dout

i + din
i

(cid:1) −

(cid:88)

si

(cid:0)dout

i − din
i

(cid:1) =

(cid:88)

(cid:88)

si

[Aij + Aji] sj .

j

i

i

j

We can substitute this into Eq. (S17)
(cid:88)

(cid:88)

2H(s) =

si

[Aij + Aji] sj −

si

(cid:0)dout

i − din
i

(cid:1) + M − 2

(cid:88)

(cid:88)

si

Aijsj

(cid:88)

i

i

j

j

si

(cid:0)din

i − dout

i

(cid:1) + M

hisi + M ,

=

=

i
(cid:88)

i
(cid:88)

i

where hi ≡ din

i − dout

i

.

S4. Ranks distributed as a multivariate Gaussian distribution

Assuming that the ranks are random variables distributed as a multivariate Gaussian distribution of average ¯s and

covariance matrix Σ, we have:

We can obtain this formulation by considering a Boltzman distribution with the Hamiltonian Eq. (2) as the energy
term and inverse temperature β so that

P (s) ∝ exp

−

(s − ¯s)(cid:124)Σ−1(s − ¯s)

.

(cid:18)

1
2

(cid:19)



P (s) ∝ exp

−

Aij (si − sj − 1)2

 .



N
(cid:88)

i,j=1

β
2

1
2

Manipulating the exponent of Eq. (S20) yields

(s − ¯s)T Σ−1(s − ¯s) =

(cid:0)sT Σ−1s − 2sT Σ−1¯s + ¯sT Σ−1¯s(cid:1) ,

1
2

whereas the parallel manipulation of Eq. (S21) yields

β
2

N
(cid:88)

i,j=1

β
2

Aij (si − sj − 1)2 =

(cid:2)sT (cid:0)Dout + Din − AT − A(cid:1) s + 2 sT (Din − Dout)1 + M (cid:3) ,

(S23)

14

(S17)

(S18)

(S19)

(S20)

(S21)

(S22)

where 1 is a vector of ones and Din are diagonal matrices whose entries are the in- and out-degrees, Dout
and Din
on s because irrelevant when accounting for normalization, we obtain:

j Aij
i,j Aij. Comparing these last two expressions and removing terms that do not depend

j Aji and M = (cid:80)

ii = (cid:80)

ii = (cid:80)

Σ = 1
β

(cid:0)Dout + Din − AT − A(cid:1)−1

and ¯s = β Σ (cid:0)Dout − Din(cid:1) 1 = s∗ .

(S24)

S5. Bayesian SpringRank

Adopting a Bayesian approach with a factorized Gaussian prior for the ranks, we obtain that the s that maximizes
the posterior distribution is the one that minimizes the regularized SpringRank Hamiltonian Eq. (4), i.e. the s that
solves the linear system Eq. (5). In fact, deﬁning:

P (s) = Z −1(β, α)

e−β α

2 (si−1)2

= Z −1(β, α)

e−β αH0(si) ,

(S25)

(cid:89)

i∈V

is a normalization constant that depends on α and β, and following the same steps as

where Z(β, α) =
before we get:

(cid:105)N/2

(cid:104) 2π
β α

log P (s | A) =

log P (Aij | s) − βα

H0(si) + log (Z (β, α) )

(cid:89)

i∈V

(cid:88)

i,j

(cid:34)

(cid:88)

i∈V

(cid:35)

(cid:88)

i∈V

= − β

H(s) + α

H0(si)

+ C ,

(S26)

where C is a constant that does not depend on the parameters, and thus may be ignored when maximizing log P (s | A).

The parameter c included in the generative model (7) controls for network’s sparsity. We can indeed ﬁx it so to

obtain a network with a desired expected number of edges (cid:104)M (cid:105) as follows:

For a given vector of ranks s and inverse temperature β, the c realizing the desired sparsity will then be:

S6. Fixing c to control for sparsity

(cid:104)M (cid:105) ≡

(cid:104)Aij(cid:105) = c

e− β

2 (si−sj −1)2

.

(cid:88)

i,j

(cid:88)

i,j

c =

(cid:80)

i,j e− β

(cid:104)M (cid:105)
2 (si−sj −1)2 =

(cid:104)k(cid:105)N
2 (si−sj −1)2 ,

(cid:80)

i,j e− β

where (cid:104)k(cid:105) is the expected node degree (cid:104)k(cid:105) = (cid:80)N
i=1
model with Bernoulli distribution.

(cid:2)din

i + dout

i

(cid:3). Similar arguments apply when considering a generative

S7. Comparing optimal β for predicting edge directions

In the main text, (12) and Eq. (13) deﬁne the accuracy of edge prediction, in terms of the number of edges predicted
correctly in each direction and the log-likelihood conditioned on the undirected graph. Here we compute the optimal
values of β for both notions of accuracy. In both computations that follow, the following two facts will be used:

and

ij(β) = 2 (si − sj) e−2β(si−sj ) P 2
P (cid:48)

ij(β) ,

1 =

Pij(β)
1 − Pij(β)

e−2β(si−sj ) .

15

(S27)

(S28)

(S29)

A. Choosing β to optimize edge direction accuracy

We take the derivative of Eq. (12) with respect to β, set it equal to zero, and solve as follows.

0 ≡

∂σa(β)
∂β

=

∂
∂β



1 −

1
2m

(cid:88)

i,j

|Aij − (Aij + Aji) Pij(β)|

 .

(S30)



In preparation to take the derivatives above, note that P (cid:48)
takes one sign, the (j, i) term takes the opposite sign,

ij(β) = −P (cid:48)

ji(β) and that whenever the (i, j) term of σa(β)

Without loss of generality, assume that the (i, j) term is positive and the (j, i) term is negative. This implies that

Aij − (Aij + Aji) Pij(β) = − [Aji − (Aij + Aji) Pji(β)] .

∂
∂β

∂
∂β

|Aij − (Aij + Aji) Pij(β)| = − (Aij + Aji) P (cid:48)

ij(β) ,

|Aji − (Aij + Aji) Pji(β)| = − (Aij + Aji) P (cid:48)

ij(β) .

and

∂
∂β

In other words, the derivatives of the (i, j) and (j, i) terms are identical, and the sign of both depends on whether the
quantity [Aij − (Aij + Aji)Pij(β)] is positive or negative. We can make this more precise by directly including the
sign of the (i, j) term, and by using Eq. (S28), to ﬁnd that

|Aij − (Aij + Aji) Pij(β)| = −2 (Aij + Aji) (si − sj) e−2β(si−sj ) P 2

ij × sign(cid:8)Aij − (Aij + Aji) Pij(β)(cid:9) .

(S34)

Expanding P 2

ij and reorganizing yields

∂
∂β

|Aij − (Aij + Aji) Pij(β)| = −2

(Aij + Aji) (si − sj)
2 cosh [2β (si − sj)] + 2

× sign(cid:8)Aij − (Aij + Aji) Pij(β)(cid:9) .

(S35)

Combining terms (i, j) and (j, i), the optimal inverse temperature for local accuracy ˆβa is that which satisﬁes

N
(cid:88)

0 =

(i,j)∈U (E)

(Aij + Aji) (si − sj)
(cid:105)
2 ˆβa (si − sj)

(cid:104)

cosh

+ 1

× sign(cid:8)Aij − (Aij + Aji) Pij( ˆβa)(cid:9) ,

(S36)

which may be found using standard root-ﬁnding methods.

B. Choosing β to optimize the conditional log likelihood

We take the derivative of Eq. (13) with respect to β, set it equal to zero, and partially solve as follows.

0 ≡

∂σL(β)
∂β

=

∂
∂β





(cid:88)

log

i,j

(cid:19)

(cid:18)Aij + Aji
Aij

+ log

(cid:104)

Pij(β)Aij [1 − Pij(β)]Aji(cid:105)



 .

Combining the (i, j) and (j, i) terms, we get
(cid:18)Aij + Aji
Aij

∂
∂β

0 ≡

(cid:88)

log

(cid:19)

(i,j)∈U (E)

+ log

(cid:19)

(cid:18)Aij + Aji
Aji

(cid:20) Aij
Pij(β)

−

Aji
1 − Pij(β)

(cid:21) ∂Pij(β)
∂β

(i,j)∈U (E)

(cid:88)

(cid:88)

=

=

(i,j)∈U (E)

2 (si − sj) [Aij − (Aij + Aji) Pij(β)]

Pij(β)
1 − Pij(β)

e−2β(si−sj ) .

+ [ Aij log Pij(β) + Aji log [1 − Pij(β)] ]

16

(S31)

(S32)

(S33)

(S37)

(S38)

Applying both Eq. (S28) and Eq. (S29), the optimal inverse temperature for the conditional log likelihood ˆβL is that
which satisﬁes

0 =

(cid:88)

2 (si − sj)

(cid:104)

(cid:105)
Aij − (Aij + Aji) Pij( ˆβL)

,

(i,j)∈U (E)

which, like Eq. (S36) may be found using standard root-ﬁnding methods. Comparing equations Eq. (S36) and
Eq. (S39), we can see that the values of β that maximize the two measures may, in general, be diﬀerent. Table S2
shows for optimal values for ˆβL and ˆβa for various real-world datasets.

17

(S39)

S8. Bitwise accuracy σb

Some methods provide rankings but do not provide a model to estimate Pij, meaning that Eq. (12) and Eq. (13)
cannot be used. Nevertheless, such methods still estimate one bit of information about each pair (i, j): whether the
majority of the edges are from i to j or vice versa. This motivates the use of a bitwise version of σa, which we call
σb,

σb = 1 −

Θ (si − sj) Θ(Aji − Aij) ,

(S40)

1
N 2 − t

(cid:88)

i,j

where Θ(x) = 1 if x > 0 and Θ(x) = 0 otherwise, and N is the number of nodes and t is the number of instances in
which Aij = Aji; there are N 2 − t total bits to predict. Results in terms of this measure on the networks considered
in the main text are shown in Figure S4. In the special case that the network is unweighted (A is a binary adjacency
matrix) and there are no bi-directional edges (if Aij = 1, then Aji = 0), then 1 − σb is the fraction of edges that
violate the rankings in s. In other words, for this particular type of network, 1 − σb is the minimum violations rank
penalty normalized by the total number of edges in the network, i.e., 1
M

i,j Θ(si − sj) Aji.

(cid:80)

S9. Performance metrics

When evaluating the performance of a ranking algorithm in general one could consider a variety of diﬀerent measures.
One possibility is to focus on the ranks themselves, rather than the outcomes of pairwise interactions, and calculate
correlation coeﬃcients as in Fig. 1; this is a valid strategy when using synthetic data thanks to the presence of ground
truth ranks, but can only assess the performance with respect of the speciﬁc generative process used to generate
the pairwise comparisons, as we point out in the main text. This strategy can also be applied for comparisons with
observed real world ranks, as we did in Table S11 and it has been done for instance in [19, 20] to compare the ranks
with those observed in real data in sports. However, the observed ranks might have been derived from a diﬀerent
process than the one implied by the ranking algorithm considered. For instance, in the faculty hiring networks, popular
ranking methods proposed by domain experts for evaluating the prestige of universities do not consider interactions
between institutions, but instead rely on a combination of performance indicators such as ﬁrst-year student retention
or graduation rates. The correlation between observed and inferred ranks should thus be treated as a qualitative
indicator of how well the two capture similar features of the system, such as prestige, but should not be used to
evaluate the performance of a ranking algorithm.

Alternatively, one can look at the outcomes of the pairwise comparisons and relate them to the rankings of the
nodes involved as in Eqs. (12) and (13) for testing prediction performance. A popular metric of this type is the number
of violations (also called upsets), i.e., outcomes where a higher ranked node is defeated by a lower ranked one. This
is very similar to the bitwise accuracy deﬁned in (S40), indeed when there are no ties and two nodes are compared
only once, then they are equivalent. These can be seen as low-resolution or coarse-grained measures of performance:
for each comparison predict a winner, but do not distinguish between cases where the winner is easy to predict and
cases where there is almost a tie. In particular, an upset between two nodes ranked nearby counts as much as an
upset between two nodes that are far away in the ranking. The latter case signals a much less likely scenario. In order
to distinguish these two situations, one can penalize each upset by the nodes’ rank diﬀerence elevated to a certain
power d. This is what the agony function does [18] with the exponent d treated as a parameter to tune based on the
application. When d = 0 we recover the standard number of unweighted upsets.

Note that optimization of agony is often used as a non-parametric approach to detect hierarchies [21], in particular
for ordinal ranks. For ordinal ranks, rank diﬀerences are integer-valued and equal to one for adjacent-ranked nodes,
yet for real-valued scores this is not the case. Therefore the result of the agony minimization problem can vary

18

widely between ordinal and real valued ranking algorithms. (We note that the SpringRank objective function, i.e.,
the Hamiltonian in Eq. (2), can be considered a kind of agony. However, since we assume that nearby pairs are more
likely to interact, it is large for a edge from i to j if i is ranked far above or far below j, and more speciﬁcally whenever
si is far from sj + 1.)

In contrast to the coarse prediction above—which competitor is more likely to win?—we require, when possible,
more precise predictions in Eqs. (12) and (13), which ask how much more likely is one competitor to win? This,
however, requires the ranking algorithm to provide an estimate of Pij, the probability that i wins over j, which is
provided only by BTL and SpringRank; all other methods compared in this study provide orderings or embeddings
without probabilistic predictions.

The conditional log-likelihood σL as deﬁned in Eq. (13) can be seen as a Log Loss often used as a classiﬁcation loss
function [46] in statistical learning. This type of function heavily penalizes ranking algorithms that are very conﬁdent
about an incorrect outcome, e.g. when the predicted Pij is close to 1, i very likely to win over j, but the observed
outcome is that j wins over i. For this reason, this metric is more sensitive to outliers, as when in sports a very
strong team loses against one at the bottom of the league. The accuracy σb deﬁned in Eq. (12) focuses instead in
predicting the correct proportions of wins/losses between two nodes that are matched in several comparisons. This
is less sensitive to outliers, and in fact if Pij is close but not exactly equal to 1, for a large number of comparisons
between i and j, we would expect that j should indeed win few times, e.g. if Pij = 0.99 and i, j are compared 100
times, σa is maximized when i wins 99 times and j wins once.

S10. Parameters used for regularizing ranking methods

When comparing SpringRank to other methods, we need to deal with the fact that certain network structures cause
other methods to fail to return any output. Eigenvector Centrality cannot, for example, be applied to directed trees,
yet this is precisely the sort of structure that one might expect when hierarchy becomes extreme.

More generally, many spectral techniques fail on networks that are not strongly connected, i.e., where it is not the
case that one can reach any node from any other by moving along a path consistent with the edge directions, since
in that case the adjacency matrix is not irreducible and the Perron-Frobenius theorem does not apply. In particular,
nodes with zero out-degree—sometimes called “dangling nodes” in the literature [13]—cause issues for many spectral
methods since the adjacency matrix annihilates any vector supported on such nodes. In contrast, the SpringRank
optimum given by Eq. (3) is unique up to translation whenever the network is connected in the undirected sense, i.e.,
whenever we can reach any node from any other by moving with or against directed edges.

A diﬀerent issue occurs in the case of SyncRank. When edges are reciprocal in the sense that an equal number
of edges point in each direction, they eﬀectively cancel out. That is, if Aij = Aji, the corresponding entries in the
SyncRank comparison matrix will be zero, Cij = Cji = 0, as if i and j were never compared at all. As a result, there
can be nodes i such that Cij = Cji = 0 for all j. While rare, these pathological cases exist in real data and during
cross-validation tests, causing the output of SyncRank to be undeﬁned.

In all these cases, regularization is required. Our regularized implementations of ﬁve ranking methods are described

below:

• Regularized Bradley-Terry-Luce (BTL). If there exist dangling nodes, the Minimization-Maximization
algorithm to ﬁt the BTL model to real data proposed in [38] requires a regularization. In this case we set the
total number of out-edges dout
i = 10−6 for nodes that would have di = 0 otherwise. This corresponds to Wi in
Eq.(3) of [38].

• Regularized PageRank. If there exist dangling nodes, we add an edge of weight 1/N from each dangling
node to every other node in the network. For each dataset we tried three diﬀerent values of the teleportation
parameter, α ∈ {0.4, 0.6, 0.8}, and reported the best results of these three.

• Regularized Rank Centrality. If there exist dangling nodes, we use the regularized version of the algorithm

presented in Eq. (5) of [14] with (cid:15) = 1.

• Regularized SyncRank. If there are nodes whose entries in the comparison matrix C are zero, we add a

small constant (cid:15) = 0.001 to the entries of H in Eq. (13) of Ref. [20], so that D is invertible.

• Regularized Eigenvector Centrality. If the network is not strongly connected, we add a weight of 1/N to

every entry in A and then diagonalize.

19

S11. Supplemental Tables

Comp. Sci. SpringRank MVR US News NRC Eig. C. PageRank
0.57
SpringRank
0.48
MVR
0.41
US News
0.41
NRC
0.74
Eig. C.
-
PageRank

0.80 0.72
0.81 0.73
- 0.73
0.73
-
0.69 0.68
0.41 0.41

0.84
0.80
0.69
0.68
-
0.74

0.96
-
0.81
0.73
0.80
0.48

-
0.96
0.80
0.72
0.84
0.57

Business
SpringRank
MVR
US News
NRC
Eig. C.
PageRank

History
SpringRank
MVR
US News
NRC
Eig. C.
PageRank

SpringRank MVR US News NRC Eig. C. PageRank
0.75
0.74
0.69
0.72
0.60
-
-
-
0.72
0.68
-
0.60

0.98
-
0.72
-
0.92
0.69

0.92
0.92
0.68
-
-
0.72

-
0.98
0.74
-
0.92
0.75

-
-
-
-
-
-

SpringRank MVR US News NRC Eig. C. PageRank
0.69
0.57
0.51
0.44
0.88
-

0.86 0.66
0.86 0.65
- 0.66
0.66
-
0.72 0.59
0.51 0.44

0.86
0.77
0.72
0.59
-
0.88

0.95
-
0.86
0.65
0.77
0.57

-
0.95
0.86
0.66
0.86
0.69

TABLE S1. Pearson correlation coeﬃcients between various rankings of faculty hiring networks. All coeﬃcients are statistically
signiﬁcant (p < 10−9). SpringRank is most highly correlated with Minimum Violations Ranks across all three faculty hiring
networks. Among US News and NRC rankings, SpringRank is more similar to US News. Values for US News and NRC were
[3] for comparison to the ranks available at the same time that the faculty hiring data were collected. The
drawn from Ref.
NRC does not rank business departments.

N M H/M Acc. σa ˆβL

ˆβa

Type
DataSet
Anim. Dom. 21 838 0.174 0.930
Parakeet G1 [5]
Anim. Dom. 19 961 0.193 0.932
Parakeet G2 [5]
0.078 0.923
Asian Elephants [37] Anim. Dom. 20 23
112 7353 0.251 0.881
Business [3]
Fac. Hiring
205 4033 0.220 0.882
Computer Science [3] Fac. Hiring
Fac. Hiring
History [3]
144 3921 0.186 0.909
Soc. Support 415 2497 0.222 0.867
Alak¯apuram [2]
Soc. Support 361 1809 0.241 0.858
Tenpat.t.i [2]

0.008 (0.089)
2.70 6.03 76 (9.1%) / 42
0.011 (0.139)
2.78 18.12 75 (7.8%) / 36
2.33 3.44 2 (8.7%) / 0
0.001 (0.040)
2.04 3.14 1171 (15.9%) / 808 0.019 (0.119)
2.23 8.74 516 (12.8%) / 255 0.013 (0.105)
2.39 5.74 397 (10.1%) / 227 0.012 (0.119)
1.98 7.95 347 (13.9%) / 120 0.011 (0.079)
1.89 8.20 262 (14.5%) / 120 0.012 (0.082)

Viol. (%) / Bound Wt. viol. (per viol.) Depth p-value
2.604 < 10−4
1.879 < 10−4
3.000 0.4466
2.125 < 10−4
2.423 < 10−4
2.234 < 10−4
3.618 < 10−4
3.749 < 10−4

TABLE S2. Statistics for SpringRank applied to real-world networks. Column details are as follows: N is the number
of nodes; M is the number of edges; H/m is the ground state energy per edge; Accuracy σa refers to accuracy in 5-fold
cross-validation tests using temperature ˆβa; ˆβL and ˆβa are temperatures optimizing edge prediction accuracies σL and σa
respectively; Violations refers to the number of edges that violate the direction of the hierarchy as a number, as a percentage
of all edges, with a lower bound provided for reference, computed as the number of unavoidable violations due to reciprocated
edges; Weighted violations are the sum of each violation weighted by the diﬀerence in ranks between the oﬀending nodes; Depth
is smax − smin; p-value refers to the null model described in the Materials and Methods. Relevant performance statistics for
NCAA datasets (53 networks) are reported elsewhere; see Fig. S3.

S12. Supplemental Figures

20

FIG. S1. Performance (Pearson correlation) on synthetic data. Tests were performed as in Fig. 1, but here performance
is measured using Pearson correlation. This favors algorithms like SpringRank and BTL, that produce real-valued ranks, over
ordinal ranking schemes like Minimum Violation Ranking which are not expected to recover latent positions.
(A) Linear
hierarchy diagrams show latent ranks splanted of 100 nodes, drawn from a standard normal distribution, with edges drawn
via the generative model Eq. (7) for indicated β (noise) values. Blue edges point down the hierarchy and red edges point
up, indicated by arrows. (B) Mean accuracies ± one standard deviation (symbols ± shading) are measured as the Pearson
correlation between method output and splanted for 100 replicates. (C, D) Identical to A and B but for hierarchies of N = 102
nodes divided into three tiers. All plots have mean degree 5; see Fig. 1 for performance curves for Spearman correlation r. See
Materials and Methods for synthetic network generation.

(a) US HS

(b) US BS

(c) US CS

(d) Tenpat.t.i

(e) Alak¯apuram

(f) parakeet G1

(g) parakeet G2

(h) planted β = 5.0

(i) planted β = 0.1

(j) Asian elephant

(k) ER
N = 100, (cid:104)k(cid:105) = 3

FIG. S2. Statistical signiﬁcance testing using the null model distribution of energies. Results are 1000 realizations
of the null model where edge directions are randomized while keeping the total number of interactions between each pair ﬁxed,
for real and synthetic networks: a-c) US History (HS), Business (BS) and Computer Science (CS) faculty hiring networks [3];
d-e) social support networks of two Indian villages [2] considering 5 types of interactions (see main manuscript); f,g) aggression
network of parakeet Group 1 and 2 (as in [5]); h,i) planted network using SpringRank generative model with N = 100 and
mean degree (cid:104)k(cid:105) = 5, Gaussian prior for the ranks with average µ = 0.5 and variance 1 (α = 1/β) and two noise levels β = 5.0
and β = 0.1; j) dominance network of asian elephants [37]; k) Erd˝os-R´enyi directed random network with N = 100 and (cid:104)k(cid:105) = 3.
The vertical line is the energy obtained on the real network. In all but the last two cases we reject the null hypothesis that
edge directions are independent of the ranks, and conclude that the hierarchy is statistically signiﬁcant.

21

FIG. S3. Edge prediction accuracy over BTL for NCAA basketball datasets. Distribution of diﬀerences in performance
of edge prediction of SpringRank compared to BTL on NCAA College Basketball regular season matches for (top) Women and
(middle) Men, deﬁned as (left) the probabilistic edge-prediction accuracy σa Eq. (12) and (right) the conditional log-likelihood
σL Eq. (13). Error bars indicate quartiles and markers show medians, corresponding to 50 independent trials of 5-fold cross-
validation, for a total of 250 test sets for each dataset. The bottom plot is obtained by considering the distributions over all
the seasons together. In terms of number of correctly predicted outcomes, SpringRank correctly predicts on average 8 to 16
more outcomes than BTL for each of the 20 Women NCAA seasons and up to 12 more outcomes for each of the 33 Men NCAA
seasons; for the latter dataset, BTL has an average better prediction in 3 out of the 33 seasons. The number of matches played
per season in the test set varies from the past to the most recents years from 747 to 1079.

22

FIG. S4. Bitwise edge direction prediction. Symbols show medians of bitwise edge prediction accuracies Eq. (S40) over
50 realization of 5-fold cross-validation (for a total of 250 trials) compared with the median accuracy for SyncRank; error bars
indicate quartiles. Thus, points above the dashed line at zero indicate better predictions than SyncRank, while values below
indicate that SyncRank performed better.

23

FIG. S5. Edge prediction accuracy with 2-fold cross-validation. Top: the accuracy of probabilistic edge prediction
of SpringRank compared to the median accuracy of BTL on real and synthetic networks deﬁned as (top left) edge-prediction
accuracy σa Eq. (12) and (top right) the conditional log-likelihood σL Eq. (13); (bottom) bitwise edge prediction accuracies σb
Eq. (S40) of SpringRank and other algorithms compared with the median accuracy of SyncRank. Error bars indicate quartiles
and markers show medians, corresponding to 50 independent trials of 2-fold cross-validation, for a total of 100 test sets for
each network. The two synthetic networks are generated with N = 100, average degree 5, and Gaussian-distributed ranks as
in Fig. 1A, with inverse temperatures β = 1 and β = 5. Notice that these results are similar those of Fig. 3, obtained using
5-fold cross-validation.

Computer Science

24

FIG. S6. Summary of SpringRank applied to Computer Science faculty hiring network [3]. (top-left) A linear
hierarchy diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and
red edges point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned)
and the horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency
matrix; blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of
statistical signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized
samples: the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted,
ordered by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means
algorithm.

History

25

FIG. S7. Summary of SpringRank applied to History faculty hiring network [3]. (top-left) A linear hierarchy diagram
showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up.
(top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal
axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red
dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance
test with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted
line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank,
from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Business

26

FIG. S8. Summary of SpringRank applied to Business faculty hiring network [3]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Asian Elephants

27

FIG. S9. Summary of SpringRank applied to Asian Elephants network [37]. (top-left) A linear hierarchy diagram
showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up.
(top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal
axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red
dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance
test with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted
line is the ground state energy obtained on the observed real network.

Parakeet G1

28

FIG. S10. Summary of SpringRank applied to Parakeet G1 network [5]. (top-left) A linear hierarchy diagram showing
inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up. (top-
middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal axis
is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red dots
represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance test
with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted line
is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank, from
top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Parakeet G2

29

FIG. S11. Summary of SpringRank applied to Parakeet G2 network [5]. (top-left) A linear hierarchy diagram showing
inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up. (top-
middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal axis
is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red dots
represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance test
with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted line
is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank, from
top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Tenpat.t.i

30

FIG. S12. Summary of SpringRank applied to Tenpat.t.i social support network [2]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Alak¯apuram

31

FIG. S13. Summary of SpringRank applied to Alak¯apuram social support network [2]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

A physical model for eﬃcient ranking in networks

Caterina De Bacco,1, 2, ∗ Daniel B. Larremore,3, 4, 2, † and Cristopher Moore2, ‡
1Data Science Institute, Columbia University, New York, NY 10027, USA
2Santa Fe Institute, Santa Fe, NM 87501, USA
3Department of Computer Science, University of Colorado, Boulder, CO 80309, USA
4BioFrontiers Institute, University of Colorado, Boulder, CO 80303, USA

We present a physically-inspired model and an eﬃcient algorithm to infer hierarchical rankings of
nodes in directed networks. It assigns real-valued ranks to nodes rather than simply ordinal ranks,
and it formalizes the assumption that interactions are more likely to occur between individuals with
similar ranks.
It provides a natural statistical signiﬁcance test for the inferred hierarchy, and it
can be used to perform inference tasks such as predicting the existence or direction of edges. The
ranking is obtained by solving a linear system of equations, which is sparse if the network is; thus the
resulting algorithm is extremely eﬃcient and scalable. We illustrate these ﬁndings by analyzing real
and synthetic data, including datasets from animal behavior, faculty hiring, social support networks,
and sports tournaments. We show that our method often outperforms a variety of others, in both
speed and accuracy, in recovering the underlying ranks and predicting edge directions.

Introduction

In systems of many individual entities, interactions and
their outcomes are often correlated with these entities’
ranks or positions in a hierarchy. While in most cases
these rankings are hidden from us, their presence is nev-
ertheless revealed in the asymmetric patterns of interac-
tions that we observe. For example, some social groups
of birds, primates, and elephants are organized accord-
ing to dominance hierarchies, reﬂected in patterns of re-
peated interactions in which dominant animals tend to
assert themselves over less powerful subordinates [1]. So-
cial positions are not directly visible to researchers, but
we can infer each animal’s position in the hierarchy by
observing the network of pairwise interactions. Similar
latent hierarchies have been hypothesized in systems of
endorsement in which status is due to prestige, reputa-
tion, or social position [2, 3]. For example, in academia,
universities may be more likely to hire faculty candidates
from equally or more prestigious universities [3].

In all these cases, the direction of the interactions is af-
fected by the status, prestige, or social position of the en-
tities involved. But it is often the case that even the exis-
tence of an interaction, rather than its direction, contains
some information about those entities’ relative prestige.
For example, in some species, animals are more likely to
interact with others who are close in dominance rank [4–
8]; human beings tend to claim friendships with others
of similar or slightly higher status [9]; and sports tourna-
ments and league structures are often designed to match
players or teams based on similar skill levels [10, 11]. This
suggests that we can infer the ranks of individuals in a so-
cial hierarchy using both the existence and the direction
of their pairwise interactions. It also suggests assigning

real-valued ranks to entities rather than simply ordinal
rankings, for instance in order to infer clusters of entities
with roughly equal status with gaps between them.

In this work we introduce a physically-inspired model
that addresses the problems of hierarchy inference, edge
prediction, and signiﬁcance testing. The model, which
we call SpringRank, maps each directed edge to a di-
rected spring between the nodes that it connects, and
ﬁnds real-valued positions of the nodes that minimizes
the total energy of these springs. Because this optimiza-
tion problem requires only linear algebra, it can be solved
for networks of millions of nodes and edges in seconds.

We also introduce a generative model for hierarchical
networks in which the existence and direction of edges de-
pend on the relative ranks of the nodes. This model for-
malizes the assumption that individuals tend to interact
with others of similar rank, and it can be used to create
synthetic benchmark networks with tunable levels of hi-
erarchy and noise. It can also predict unobserved edges,
allowing us to use cross-validation as a test of accuracy
and statistical signiﬁcance. Moreover, the maximum like-
lihood estimates of the ranks coincides with SpringRank
asymptotically.

We test SpringRank and its generative model version
on both synthetic and real datasets, including data from
animal behavior, faculty hiring, social support networks,
and sports tournaments. We ﬁnd that it infers accurate
rankings, provides a simple signiﬁcance test for hierarchi-
cal structure, and can predict the existence and direction
of as-yet unobserved edges. In particular, we ﬁnd that
SpringRank often predicts the direction of unobserved
edges more accurately than a variety of existing methods,
including popular spectral techniques, Minimum Viola-
tion Ranking, and the Bradley-Terry-Luce method.

∗ cdebacco@santafe.edu; Contributed equally.
† daniel.larremore@colorado.edu; Contributed equally.
‡ moore@santafe.edu

Ranking entities in a system from pairwise compar-
isons or interactions is a fundamental problem in many

Related work

8
1
0
2
 
n
u
J
 
3
1
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
4
v
2
0
0
9
0
.
9
0
7
1
:
v
i
X
r
a

contexts, and many methods have been proposed. One
family consists of spectral methods like Eigenvector Cen-
trality [12], PageRank [13], Rank Centrality [14], and the
method of Callaghan et al. [15]. These methods propose
various types of random walks on the directed network
and therefore produce real-valued scores. However, by
design these methods tend to give high ranks to a small
number of important nodes, giving us little information
about the lower-ranked nodes.
In addition, they often
require explicit regularization, adding a small term to
every element of the adjacency matrix if the graph of
comparisons is not strongly connected.

A second family focuses on ordinal rankings, i.e., per-
mutations, that minimize various penalty functions. This
family includes Minimum Violation Rank [16–18] and Se-
rialRank [19] and SyncRank [20]. Minimum Violation
Rank (MVR) imposes a uniform penalty for every viola-
tion or “upset,” deﬁned as an edge that has a direction
opposite to the one expected by the rank diﬀerence be-
tween the two nodes. Non-uniform penalties and other
generalizations are often referred to as agony methods
[21]. For common choices of the penalty function, mini-
mization can be computationally diﬃcult [17, 22], forcing
us to use simple heuristics that ﬁnd local minima.

SerialRank constructs a matrix of similarity scores be-
tween each pair of nodes by examining whether they pro-
duce similar outcomes when compared with the other
nodes, thereby relating the ranking problem to a more
general ordering problem called seriation. SyncRank is
a hybrid method which ﬁrst solves a spectral problem
based on synchronization, embeds node positions on a
half-circle in the complex plane, and then chooses among
the circular permutations of those ranks by minimizing
the number of violations as in MVR.

Random Utility Models [23], such as the Bradley-
Terry-Luce (BTL) model [24, 25], are designed to in-
fer real-valued ranks from data on pairwise preferences.
These models assign a probability to the direction of an
edge conditioned on its existence, but they do not assign
a probability to the existence of an edge. They are ap-
propriate, for instance, when an experimenter presents
subjects with choices between pairs of items, and asks
them which they prefer.

Methods like David’s Score [26] and the Colley ma-
trix [27] compute rankings from proportions of wins and
losses. The latter, which was originally developed by
making mathematical adjustments to winning percent-
ages, is equivalent to a particular case of the general
method we introduce below. Elo score [28], Go Rank [29],
and TrueSkill [30] are also widely used win-loss methods,
but these schemes update the ranks after each match
rather than taking all previous interactions into account.
This specialization makes them useful when ranks evolve
over sequential matches, but less useful otherwise.

Finally, there are fully generative models such the
Probabilistic Niche Model of ecology [31–33], models of
friendship based on social status [9], and more generally
latent space models [34] which assign probabilities to the

2

existence and direction of edges based on real-valued po-
sitions in social space. However, inference of these models
tends to be diﬃcult, with many local optima. Our gen-
erative model can be viewed as a special case of these
models for which inference is especially easy.

In the absence of ground-truth rankings, we can
compare the accuracy of these methods using cross-
validation, computing the ranks using a subset of the
edges in the network and then using those ranks to pre-
dict the direction of the remaining edges. Equivalently,
we can ask them to predict unobserved edges, such as
which of two sports teams will win a game. However,
these methods do not all make the same kinds of pre-
dictions, requiring us to use diﬀerent kinds of cross-
validation. Methods such as BTL produce probabilis-
tic predictions about the direction of an edge, i.e., they
estimate the probability one item will be preferred to
another. Fully generative models also predict the proba-
bility that an edge exists, i.e., that a given pair of nodes
in the network interact. On the other hand, ordinal rank-
ing methods such as MVR do not make probabilistic pre-
dictions, but we can interpret their ranking as a coarse
prediction that an edge is more likely to point in one
direction than another.

The SpringRank model

We represent interactions between N entities as a
weighted directed network, where Aij is the number of
interactions i → j suggesting that i is ranked above j.
This allows both ordinal and cardinal input, including
where pairs interact multiple times. For instance, Aij
could be the number of ﬁghts between i and j that i has
won, or the number of times that j has endorsed i.

Given the adjacency matrix A, our goal is to ﬁnd a
ranking of the nodes. To do so, the SpringRank model
computes the optimal location of nodes in a hierarchy by
imagining the network as a physical system. Speciﬁcally,
each node i is embedded at a real-valued position or rank
si, and each directed edge i → j becomes an oriented
spring with a nonzero resting length and displacement
si − sj. Since we are free to rescale the latent space
and the energy scale, we set the spring constant and the
resting length to 1. Thus, the spring corresponding to an
edge i → j has energy

Hij =

(si − sj − 1)2 ,

(1)

1
2

which is minimized when si − sj = 1.

This version of the model has no tunable parameters.
Alternately, we could allow each edge to have its own
rest length or spring constant, based on the strength of
each edge. However, this would create a large number of
parameters, which we would have to infer from the data
or choose a priori. We do not explore this here.

According to this model, the optimal rankings of the
N ) which minimize

nodes are the ranks s∗ = (s∗

1, . . . , s∗

the total energy of the system given by the Hamiltonian

H(s) =

AijHij =

Aij (si − sj − 1)2 .

(2)

N
(cid:88)

i,j=1

1
2

(cid:88)

i,j

Since this Hamiltonian is convex in s, we can ﬁnd s∗ by
setting ∇H(s) = 0, yielding the linear system

(cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) s∗ = (cid:2)Dout − Din(cid:3) 1 ,

(3)

where 1 is the all-ones vector and Dout and Din are di-
agonal matrices whose entries are the weighted in- and
out-degrees, Dout
j Aji. See SI
Text S1 for detailed derivations.

j Aij and Din

ii = (cid:80)

ii = (cid:80)

The matrix on the left side of Eq. (3) is not invertible.
This is because H is translation-invariant:
it depends
only on the relative ranks si − sj, so that if s∗ = {si}
minimizes H(s) then so does {si + a} for any constant a.
One way to break this symmetry is to invert the matrix
in the subspace orthogonal to its nullspace by comput-
ing a Moore-Penrose pseudoinverse. If the network con-
sists of a single component, the nullspace is spanned by
the eigenvector 1, in which case this method ﬁnds the
s∗ where the average rank (1/N ) (cid:80)
i si = (1/N )s∗ · 1 is
zero. This is related to the random walk method of [15]:
if a random walk moves along each directed edge with
rate 1
2 − ε, then
s∗ is proportional to the perturbation to the stationary
distribution to ﬁrst order in ε.

2 + ε and against each one with rate 1

In practice, it is more eﬃcient and accurate to ﬁx the
rank of one of the nodes and solve the resulting equation
using a sparse iterative solver (see SI Text S1). Faster
still, because this matrix is a Laplacian, recent results [35,
36] allow us to solve Eq. (3) in nearly linear time in M ,
the number of non-zero edges in A.

Another way to break translation invariance is to in-
i aﬀecting each

troduce an “external ﬁeld” H0(si) = 1
2 αs2
node, so that the combined Hamiltonian is

Hα(s) = H(s) +

(4)

α
2

N
(cid:88)

i=1

s2
i .

The ﬁeld H0 corresponds to a spring that attracts every
node to the origin. We can think of this as imposing a
Gaussian prior on the ranks, or as a regularization term
that quadratically penalizes ranks with large absolute
values. This version of the model has a single tunable
parameter, namely the spring constant α. Since H(s)
scales with the total edge weight M = (cid:80)
i,j Aij while
H0(s) scales with N , for a ﬁxed value of α this regular-
ization becomes less relevant as networks become more
dense and the average (weighted) degree M/N increases.
For α > 0 there is a unique s∗ that minimizes Hα,

given by

(cid:2)Dout + Din − (cid:0)A + AT (cid:1) + αI(cid:3) s∗ = (cid:2)Dout − Din(cid:3) 1 ,

3

where I is the identity matrix. The matrix on the left
side is now invertible, since the eigenvector 1 has eigen-
In the limit α → 0, we recover
values α instead of 0.
Eq. (3); the value α = 2 corresponds to the Colley ma-
trix method [27].

Minimizing H(s), or the regularized version Hα(s),
corresponds to ﬁnding the “ground state” s∗ of the
model.
In the next section we show that this corre-
sponds to a maximum-likelihood estimate of the ranks
in a generative model. However, we can use SpringRank
not just to maximize the likelihood, but to compute a
joint distribution of the ranks as a Boltzmann distribu-
tion with Hamiltonian Eq. (4), and thus estimate the
uncertainty and correlations between the ranks. In par-
ticular, the ranks si are random variables following an
N -dimensional Gaussian distribution with mean s∗ and
covariance matrix (SI Text S4)

Σ =

(cid:2)Dout + Din − (cid:0)A + AT + αI(cid:1)(cid:3)−1

.

(6)

1
β

Here β is an inverse temperature controlling the amount
of noise in the model. In the limit β → ∞, the rankings
are sharply peaked around the ground state s∗, while
for β → 0 they are noisy. As we discuss below, we can
estimate β from the observed data in various ways.

The rankings given by SpringRank Eq. (3) and its reg-
ularized form Eq. (5) are easily and rapidly computed by
standard linear solvers.
In particular, iterative solvers
that take advantage of the sparsity of the system can
ﬁnd s∗ for networks with millions of nodes and edges
in seconds. However, as deﬁned above, SpringRank is
not a fully generative model that assigns probabilities to
the data and allows for Bayesian inference. In the next
section we introduce a generative model for hierarchical
networks and show that it converges to SpringRank in
the limit of strong hierarchy.

A generative model

In this section we propose a probabilistic genera-
tive model that takes as its input a set of node ranks
s1, . . . , sN and produces a weighted directed network.
The model also has a temperature or noise parameter
β and a density parameter c. Edges between each pair
of nodes i, j are generated independently of other pairs,
conditioned on the ranks. The expected number of edges
from i to j is proportional to the Boltzmann weight of
the corresponding term in the Hamiltonian Eq. (2),

E[Aij] = c exp(−βHij) = c exp

−

(si − sj − 1)2

,

(cid:21)

(cid:20)

β
2

where the actual edge weight Aij is drawn from a Poisson
distribution with this mean. The parameter c controls
the overall density of the network, giving an expected

(5)

4

number of edges

Predicting edge directions

E[M ] =

E[Aij] = c

exp

−

(si − sj − 1)2

,

(cid:88)

i,j

(cid:88)

i,j

(cid:20)

β
2

(cid:21)

while the inverse temperature β controls the extent to
which edges respect (or defy) the ranks s. For smaller
β, edges are more likely to violate the hierarchy or to
connect distant nodes, decreasing the correlation between
the ranks and the directions of the interactions: for β = 0
the model generates a directed Erd˝os-R´enyi graph, while
in the limit β → ∞ edges only exist between nodes i, j
with si − sj = 1, and only in the direction i → j.

The Poisson distribution may generate multiple edges
between a pair of nodes, so this model generates directed
multigraphs. This is consistent with the interpretation
that Aij is the number, or total weight, of edges from i
to j. However, in the limit as E[Aij] → 0, the Poisson
distribution approaches a Bernoulli distribution, gener-
ating binary networks with Aij ∈ {0, 1}.

The likelihood of observing a network A given ranks s,

inverse temperature β, and density c is

(cid:104)

ce− β

2 (si−sj −1)2(cid:105)Aij

P (A | s, β, c) =

(cid:89)

i,j

Aij!

(cid:104)

exp

−ce− β

2 (si−sj −1)2 (cid:105)

(7)
Taking logs, substituting the maximum-likelihood value
of c, and discarding constants that do not depend on s
or β yields a log-likelihood (see Supplemental Text S2)

If hierarchical structure plays an important role in a
system, it should allow us to predict the direction of pre-
viously unobserved interactions, such as the winner of an
upcoming match, or which direction social support will
ﬂow between two individuals. This is a kind of cross-
validation, which lets us test the statistical signiﬁcance
of hierarchical structure. It is also a principled way of
comparing the accuracy of various ranking methods for
datasets where no ground-truth ranks are known.

We formulate the edge prediction question as follows:
given a set of known interactions, and given that there is
an edge between i and j, in which direction does it point?
In one sense, any ranking method provides an answer to
this question, since we can predict the direction according
to which of i or j is ranked higher based on the known
interactions. When comparing SpringRank to methods
such as SyncRank, SerialRank, and MVR, we use these
“bitwise” predictions, and deﬁne the accuracy σb as the
fraction of edges whose direction is consistent with the
inferred ranking.

But we want to know the odds on each game, not just
the likely winner—that is, we want to estimate the prob-
ability that an edge goes in each direction. A priori,
.
a ranking algorithm does not provide these probabili-
ties unless we make further assumptions about how they
depend on the relative ranks. Such assumptions yield
generative models like the one deﬁned above, where the
conditional probability of an edge i → j is

L(A | s, β) = −βH(s) − M log

(cid:20) (cid:88)

2 (si−sj −1)2 (cid:21)

e− β

, (8)

i,j

Pij(β) =

e−βHij
e−βHij + e−βHji

=

1
1 + e−2β(si−sj )

.

(9)

where H(s) is the SpringRank energy deﬁned in Eq. (2).
In the limit of large β where the hierarchical structure
is strong, the ˆs that maximizes Eq. (8), approaches the
solution s∗ of Eq. (3) that minimizes H(s). Thus the
maximum likelihood estimate ˆs of the rankings in this
model approaches the SpringRank ground state.

As discussed above, we can break translational sym-
metry by adding a ﬁeld H0 that attracts the ranks to
the origin. This is is equivalent to imposing a prior
P (s) ∝ (cid:81)N
. The maximum a posteriori
estimate ˆs then approaches the ground state s∗ of the
Hamiltonian in Eq. (4), given by Eq. (5).

i=1 e− αβ

2 (si−1)2

This model belongs to a larger family of genera-
tive models considered in ecology and network the-
ory [9, 31, 32], and more generally the class of latent space
models [34], where an edge points from i to j with proba-
bility f (si − sj) for some function f . These models typi-
cally have complicated posterior distributions with many
local optima, requiring Monte Carlo methods (e.g. [33])
In our
that do not scale eﬃciently to large networks.
case, f (si − sj) is a Gaussian centered at 1, and the pos-
terior converges to the multivariate Gaussian Eq. (6) in
the limit of strong structure.

The density parameter c aﬀects the probability that an
edge exists, but not its direction. Thus our probabilistic
prediction method has a single tunable parameter β.

Note that Pij is a logistic curve, is monotonic in the
rank diﬀerence si − sj, and has width determined by the
inverse temperature β. SpringRank has this in common
with two other ranking methods: setting γi = e2βsi re-
covers the Bradley-Terry-Luce model [24, 25] for which
Pij = γi/(γi + γj), and setting k = 2β recovers the
probability that i beats j in the Go rank [29], where
Pij = 1/(1 + e−k(si−sj )). However, SpringRank diﬀers
from these methods in how it infers the ranks from ob-
served interactions, so SpringRank and BTL make diﬀer-
ent probabilistic predictions.

In our experiments below, we test various ranking
methods for edge prediction by giving them access to
80% of the edges in the network (the training data) and
then asking them to predict the direction of the remain-
ing edges (the test data). We consider two measures of
accuracy: σa is the average probability assigned to the
correct direction of an edge, and σL is the log-likelihood
of generating the directed edges given their existence. For
simple directed graphs where Aij +Aji ∈ {0, 1}, these are

σa =

AijPij

and σL =

Aij log Pij .

(10)

(cid:88)

i,j

(cid:88)

i,j

In the multigraph case, we ask how well Pij approximates
the fraction of interactions between i and j that point
from i to j [see Eqs. (12) and (13)]. For a discussion of
other performance measures, see Supplemental Text S9.
We perform our probabilistic prediction experiments
as follows. Given the training data, we infer the ranks
using Eq. (5). We then choose the temperature parame-
ter β by maximizing either σa or σL on the training data
while holding the ranks ﬁxed. The resulting values of β,
which we denote ˆβa and ˆβL respectively, are generally
distinct (Supplemental Table S2 and Text S7). This is
intuitive, since a single severe mistake where Aij = 1 but
Pij ≈ 0 reduces the likelihood by a large amount, while
only reducing the accuracy by one edge. As a result,
predictions using ˆβa produce fewer incorrectly oriented
edges, achieving a higher σa on the test set, while predic-
tions using ˆβL will produce fewer dramatically incorrect
predictions where Pij is very low, and thus achieve higher
σL on the test set.

Statistical signiﬁcance using the ground state energy

We can measure statistical signiﬁcance using any test
statistic, by asking whether its value on a given dataset
would be highly improbable in a null model. One such
statistic is the accuracy of edge prediction using a method
such as the one described above. However, this may
become computationally expensive for cross-validation
studies with many replicates, since each fold of each
replicate requires inference of the parameter ˆβa. Here
we propose a test statistic which is very easy to com-
pute, inspired by the physical model behind SpringRank:
namely, the ground state energy. For the unregularized
version Eq. (2), the energy per edge is (see SI Text S3)

H(s∗)
M

=

1
2M

(cid:88)

i

(din

i − dout

i

) s∗

i +

(11)

1
2

.

Since the ground state energy depends on many aspects
of the network structure, and since hierarchical structure
is statistically signiﬁcant if it helps us predict edge direc-
tions, like [37] we focus on the following null model: we
randomize the direction of each edge while preserving the
total number ¯Aij = Aij + Aji of edges between each pair
of vertices. If the real network has a ground state energy
which is much lower than typical networks drawn from
this null model, we can conclude that the hierarchical
structure is statistically signiﬁcant.

This test correctly concludes that directed Erd˝os-R´enyi
graphs have no signiﬁcant structure. It also ﬁnds no sig-
niﬁcant structure for networks created using the genera-
tive model Eq. (7) with β = 0.1, i.e., when the tempera-
ture or noise level 1/β is suﬃciently large the ranks are no

5

longer relevant to edge existence or direction (Fig. S2).
However, we see in the next section that it shows sta-
tistically signiﬁcant hierarchy for a variety of real-world
datasets, showing that H(s∗) is both useful and compu-
tationally eﬃcient as a test statistic.

FIG. 1. Performance on synthetic data. (A) A synthetic
network of N = 100 nodes, with ranks drawn from a standard
Gaussian and edges drawn via the generative model Eq. (7)
for two diﬀerent values of β and average degree 5. Blue edges
point down the hierarchy and red edges point up, indicated
by arrows. (B) The accuracy of the inferred ordering deﬁned
as the Spearman correlation averaged over 100 indendepently
generated networks; error bars indicate one standard devia-
tion. (C, D) Identical to A and B but with ranks drawn from
a mixture of three Gaussians so that the nodes cluster into
three tiers (Materials and Methods). See Fig. S1 for perfor-
mance curves for Pearson correlation r.

Results on real and synthetic data

Having introduced SpringRank, an eﬃcient procedure
for inferring real-valued ranks, a corresponding gener-
ative model, a method for edge prediction, and a test
for the statistical signiﬁcance of hierarchical structure,
we now demonstrate it by applying it to both real and
synthetic data. For synthetic datasets where the ground-
truth ranks are known, our goal is to see to what extent
SpringRank and other algorithms can recover the actual
ranks. For real-world datasets, in most cases we have
no ground-truth ranking, so we apply the statistical sig-
niﬁcance test deﬁned above, and compare the ability of
SpringRank and other algorithms to predict edge direc-

tions given a subset of the interactions.

We compare SpringRank to other widely used meth-
ods: the spectral methods PageRank [13], Eigenvector
Centrality [12] and Rank Centrality [14]; Minimum Vi-
olation Ranking (MVR) [16, 17], SerialRank [19] and
SyncRank [20], which produce ordinal rankings; David’s
score [26]; and the BTL random utility model [24, 25]
using the algorithm proposed in [38], which like our
generative model makes probabilistic predictions. We
also compare unregularized SpringRank with the regu-
larized version α = 2, corresponding to the Colley ma-
trix method [27]. Unfortunately, Eigenvector Centrality,
Rank Centrality, David’s score, and BTL are undeﬁned
when the network is not strongly connected, e.g. when
there are nodes with zero in- or out-degree. In such cases
we follow the common regularization procedure of adding
low-weight edges between every pair of nodes (see Sup-
plemental Text S10).

Performance for synthetic networks

We study two types of synthetic networks, generated
by the model described above. Of course, since the log-
likelihood in this model corresponds to the SpringRank
energy in the limit of large β, we expect SpringRank to
do well on these networks, and its performance should be
viewed largely as a consistency check. But by varying the
distribution of ranks and the noise level, we can illustrate
types of structure that may exist in real-world data, and
test each algorithm’s ability to identify them.

In the ﬁrst type, the ranks are normally distributed
with mean zero and variance one (Fig. 1A). In the second
type, the ranks are drawn from an equal mixture of three
Gaussians with diﬀerent means and variances, so that
nodes cluster into high, middle, and low tiers (Fig. 1C).
This second type is intended to focus on the importance
of real-valued ranks, and to measure the performance of
algorithms that (implicitly or explicitly) impose strong
priors on the ranks when the data defy their expectations.
In both cases, we vary the amount of noise by changing
β while keeping the total number of edges constant (see
Materials and Methods).

Since we wish to compare SpringRank both to meth-
ods such as MVR that only produce ordinal rankings,
and to those like PageRank and David’s Score that pro-
duce real-valued ranks, we measure the accuracy of each
algorithm according to the Spearman correlation ρ be-
tween its inferred rank order and the true one. Results
for the Pearson correlation, where we measure the algo-
rithms’ ability to infer the real-valued ranks as opposed
to just their ordering, are shown in Fig. S1.

We ﬁnd that all the algorithms do well on the ﬁrst
type of synthetic network. As β increases so that the net-
work becomes more structured, with fewer edges (shown
in red in Fig. 1A) pointing in the “wrong” direction,
all algorithms infer ranks that are more correlated with
the ground truth. SpringRank and SyncRank have the

6

FIG. 2. Ranking the History faculty hiring network [3].
(A) Linear hierarchy diagram with nodes embedded at their
inferred SpringRank scores. Blue edges point down the hier-
archy and red edges point up. (B) Histogram of the empirical
distribution of ranks, with a vertical axis of ranks matched to
panel A. (C) Histogram of ground-state energies from 10, 000
randomizations of the network according to the null model
where edge directions are random; the dashed red line shows
the ground state energy of the empirical network depicted in
panels A and B. The fact that the ground state energy is so
far below the tail of the null model is overwhelming evidence
that the hierarchical structure is statistically signiﬁcant, with
a p-value < 10−4

.

highest accuracy, followed closely by the Colley matrix
method and BTL (Fig. 1B). Presumably the Colley ma-
trix works well here because the ranks are in fact drawn
from a Gaussian prior, as it implicitly assumes.

Results for the second type of network are more nu-
anced. The accuracy of SpringRank and SyncRank in-
creases rapidly with β with exact recovery around β = 1.
Inter-
SerialRank also performs quite well on average.

7

FIG. 3. Edge prediction accuracy over BTL. Distribution of diﬀerences in performance of edge prediction of SpringRank
compared to BTL on real and synthetic networks deﬁned as (A) edge-prediction accuracy σa Eq. (12) and (B) the conditional
log-likelihood σL Eq. (13). Error bars indicate quartiles and markers show medians, corresponding to 50 independent trials of
5-fold cross-validation, for a total of 250 test sets for each network. The two synthetic networks are generated with N = 100,
average degree 5, and Gaussian-distributed ranks as in Fig. 1A, with inverse temperatures β = 1 and β = 5. For each
experiment shown, the fractions of trials in which each method performed equal to or better than BTL are shown in Table I.
These diﬀerences correspond to prediction of an additional 1 to 12 more correct edge directions, on average.

estingly, the other methods do not improve as β in-
creases, and many of them decrease beyond a certain
point (Fig. 1D). This suggests that these algorithms be-
come confused when the nodes are clustered into tiers,
even when the noise is small enough that most edges have
directions consistent with the hierarchy.
SpringRank
takes advantage of the fact that edges are more likely
between nodes in the same tier (Fig. 1C), so the mere
existence of edges helps it cluster the ranks.

These synthetic tests suggest that real-valued ranks
capture information that ordinal ranks do not, and that
many ranking methods perform poorly when there are
substructures in the data such as tiered groups. Of
course, in most real-world scenarios, the ground-truth
ranks are not known, and thus edge prediction and other
forms of cross-validation should be used instead. We turn
to edge prediction in the next section.

Performance for real-world networks

As discussed above, in most real-world networks, we
have no ground truth for the ranks. Thus we focus on
our ability to predict edge directions from a subset of
the data, and measure the statistical signiﬁcance of the
inferred hierarchy.

We apply our methods to datasets from a diverse set
of ﬁelds, with sizes ranging up to N = 415 nodes and
up to 7000 edges (see Table S2): three North Ameri-
can academic hiring networks where Aij is the number
of faculty at university j who received their doctorate
from university i, for History (illustrated in Figs. 2A and
B), Business, and Computer Science departments [3]; two

networks of animal dominance among captive monk para-
keets [5] and one among Asian elephants [37] where Aij
is the number of dominating acts by animal i toward an-
imal j; and social support networks from two villages
in Tamil Nadu referred to (for privacy reasons) by the
pseudonyms “Tenpat.t.i” and “Alak¯apuram,” where Aij
is the number of distinct social relationships (up to ﬁve)
through which person i supports person j [2]; and 53 net-
works of NCAA Women’s and Men’s college basketball
matches during the regular season, spanning 1985-2017
(Men) and 1998-2017 (Women), where Aij = 1 if team i
beat team j. Each year’s network comprises a diﬀerent
number of matches, ranging from 747 to 1079 [39].

Together, these examples cover prestige, dominance,
and social hierarchies. In each of these domains, infer-
ring ranks from interactions is key to further analysis.
Prestige hierarchies play an unequivocal role in the dy-
namics of academic labor markets [40]; in behavioral ecol-
ogy, higher-ranked individuals in dominance hierarchies
are believed to have higher ﬁtness [1, 41]; and patterns
of aggression are believed to reveal animal strategies and
cognitive capacities [4–8]. Finally, in social support net-
works, higher ranked individuals have greater social capi-
tal and reputational standing [42, 43], particularly in set-
tings in which social support is a primary way to express
and gain respect and prestige [44].

We ﬁrst applied our ground state energy test for the
presence of statistically signiﬁcant hierarchy, rejecting
the null hypothesis with p < 10−4 in almost all cases
(e.g., for History faculty hiring, see Fig. 2C). The one ex-
ception is the Asian Elephants network for which p > 0.4.
This corroborates the original study of this network [37],
which found that counting triad motifs shows no signif-

8

FIG. 4. Probabilistic edge prediction accuracy σa of
SpringRank vs. BTL. For 50 independent trials of 5-fold
cross-validation (250 total folds per network), the values of σa
for SpringRank and BTL are shown on the vertical axis and
the horizontal axis respectively. Points above the diagonal,
shown in blue, are trials where SpringRank is more accurate
than BTL. The fractions for which each method is superior
are shown in plot legends, matching Table I.

icant hierarchy [45]. This is despite the fact that one
can ﬁnd an appealing ordering of the elephants using the
Minimum Violation Rank method, with just a few vio-
lating edges (SI Fig. S9). Thus the hierarchy found by
MVR may well be illusory.

As described above, we performed edge prediction ex-
periments using 5-fold cross-validation, where 80% of the
edges are available to the algorithm as training data, and
a test set consisting of 20% of the edges is held out (see
Materials and Methods). To test SpringRank’s ability to
make probabilistic predictions, we compare it to BTL.

We found that SpringRank outperforms BTL, both in
terms of the accuracy σa (Fig. 3A) and, for most net-
works, the log-likelihood σL (Fig. 3B). The accuracy of
both methods has a fairly broad distribution over the tri-
als of cross-validation, since in each network some subsets
of the edges are harder to predict than others when they
are held out. However, as shown in Fig. 4, in most tri-
als SpringRank was more accurate than BTL. Fig. 3A
and Table I show that SpringRank predicts edge direc-
tions more accurately in the majority of trials of cross-
validation for all nine real-world networks, where this
majority ranges from 62% for the parakeet networks to
100% for the Computer Science hiring network.

Table I shows that SpringRank also obtained a higher
log-likelihood σL than BTL for 6 of the 9 real-world net-
works. Regularizing SpringRank with α = 2 does not ap-
pear to signiﬁcantly improve either measure of accuracy
(Fig. 3). We did not attempt to tune the regularization
parameter α.

To compare SpringRank with methods that do not
make probabilistic predictions, including those that pro-

FIG. 5. Bitwise prediction accuracy σb of SpringRank
vs. SyncRank. For 50 independent trials of 5-fold cross-
validation (250 total folds per NCAA season), the fraction
of correctly predicted game outcomes σb for SpringRank and
Syncrank are shown on the vertical axis and the horizontal
axis respectively. Points above the equal performance line,
shown in blue, are trials where SpringRank is more accurate
than SyncRank; the fractions for which each method is supe-
rior are shown in plot legends.

duce ordinal rankings, we measured the accuracy σb of
“bitwise” predictions, i.e., the fraction of edges consis-
tent with the infered ranking. We found that spectral
methods perform poorly here, as does SerialRank.
In-
terestingly, BTL does better on the NCAA networks in
terms of bitwise prediction than it does for probabilistic
predictions, suggesting that it is better at rank-ordering
teams than determining their real-valued position.

We found that SyncRank is the strongest of the or-
dinal methods, matching SpringRank’s accuracy on the
parakeet and business school networks, but SpringRank
outperforms SyncRank on the social support and NCAA
networks (see Fig. S4). We show a trial-by-trial compari-
son of SpringRank and SyncRank in Fig. 5, showing that
in most trials of cross-validation SpringRank makes more
accurate predictions for the NCAA networks.

To check whether our results were dependent on the
choice of holding out 20% of the data, we repeated our
experiments using 2-fold cross-validation, i.e., using 50%
of network edges as training data and trying to predict
the other 50%. We show these results in Fig. S5. While
all algorithms are less accurate in this setting, the com-
parison between algorithms is similar to that for 5-fold
cross-validation.

Finally, the real-valued ranks found by SpringRank
shed light on the organization and assembly of real-world
networks (see Figs. S6, S7, S8, S12, and S13). For exam-
ple, we found that ranks in the faculty hiring networks
have a long tail at the top, suggesting that the most pres-
tigious universities are more separated from those below
them than an ordinal ranking would reveal. In contrast,
ranks in the social support networks have a long tail
at the bottom, suggesting a set of people who do not
have suﬃcient social status to provide support to oth-
ers. SpringRank’s ability to ﬁnd real-valued ranks makes
these distributions amenable to statistical analysis, and

9

% trials higher σa vs BTL % trials higher σL vs BTL
Type SpringRank +regularization SpringRank +regularization

Dataset
Comp. Sci. [3]
Alak¯apuram [2]
Synthetic β = 5
History [3]
NCAA Women (1998-2017) [39]
Tenpat.t.i [2]
Synthetic β = 1
NCAA Men (1985-2017) [39]

Faculty Hiring
Social Support
Synthetic
Faculty Hiring
Basketball
Social Support
Synthetic
Basketball
Parakeet G1 [5] Animal Dominance
Faculty Hiring
Parakeet G2 [5] Animal Dominance

Business [3]

100.0
99.2†
98.4
97.6†
94.4†
88.8
83.2
76.0†
71.2†
66.8†
62.0

97.2
99.6
63.2
96.8
87.0
93.6
65.2
62.3
56.8
59.2
51.6

100.0
100.0
76.4
98.8
69.1
100.0
98.4
68.5
41.2
39.2
47.6

99.6
100.0
46.4
98.8
51.0
100.0
98.4
52.4
37.2
36.8
47.2

TABLE I. Edge prediction with BTL as a benchmark. During 50 independent trials of 5-fold cross-validation (250 total
folds per network), columns show the the percentages of instances in which SpringRank Eq. (3) and regularized SpringRank
Eq. (5) with α = 2 produced probabilistic predictions with equal or higher accuracy than BTL. Distributions of accuracy
improvements are shown in Fig. 3. Center columns show accuracy σa and right columns show σL (Materials and Methods).
Italics indicate where BTL outperformed SpringRank for more than 50% of tests. † Dagger symbols indicate tests that are
shown in detail in Fig. 4. NCAA Basketball datasets were analyzed one year at a time

.

we suggest this as a direction for future work.

Conclusions

SpringRank is a mathematically principled, physics-
inspired model for hierarchical structure in networks
It yields a simple and highly
of directed interactions.
scalable algorithm, requiring only sparse linear algebra,
which enables analysis of networks with millions of nodes
and edges in seconds. Its ground state energy provides
a natural test statistic for the statistical signiﬁcance of
hierarchical structure.

While the basic SpringRank algorithm is nonparamet-
ric, a parameterized regularization term can be included
as well, corresponding to a Gaussian prior. While reg-
ularization is often required for BTL, Eigenvector Cen-
trality, and other commonly used methods (Supplemen-
tal Text S10) it is not necessary for SpringRank and our
tests indicate that its eﬀects are mixed.

We also presented a generative model that allows one
to create synthetic networks with tunable levels of hierar-
chy and noise, whose posterior coincides with SpringRank
in the limit where the eﬀect of the hierarchy is strong.
By tuning a single temperature parameter, we can use
this model to make probabilistic predictions of edge di-
rections, generalizing from observed to unobserved inter-
actions. Therefore, after conﬁrming its ability to infer
ranks in synthetic networks where ground truth ranks
are known, we measured SpringRank’s ability to to pre-
dict edge directions in real networks. We found that
in networks of faculty hiring, animal interactions, social
support, and NCAA basketball, SpringRank often makes
better probabilistic predictions of edge predictions than
the popular Bradley-Terry-Luce model, and performs as
well or better than SyncRank and a variety of other meth-
ods that produce ordinal rankings.

SpringRank is based on springs with quadratic poten-
tials, but other potentials may be of interest. For in-
stance, to make the system more tolerant to outliers while
remaining convex, one might consider a piecewise poten-
tial that is quadratic for small displacements and linear
otherwise. We leave this investigation of alternative po-
tentials to future work.

Given its simplicity, speed, and high performance, we
believe that SpringRank will be useful in a wide vari-
ety of ﬁelds where hierarchical structure appears due to
dominance, social status, or prestige.

Materials and methods

Synthetic network generation

Networks were generated in three steps. First, node
ranks splanted were drawn from a chosen distribution. For
Test 1, N = 100 ranks were drawn from a standard nor-
mal distribution, while for Test 2, 34 ranks were drawn
from each of three Gaussians, N (−4, 2), N (0, 1
2 ), and
N (4, 1) for a total of N = 102. Second, an average de-
gree (cid:104)k(cid:105) and a value of the inverse temperature β were
chosen. Third, edges were drawn generated to Eq. (7)
with c = (cid:104)k(cid:105)N/ (cid:80)
i,j exp [−(β/2)(si − sj − 1)2] so that
the expected mean degree is (cid:104)k(cid:105) (see SI Text S6).

This procedure resulted in directed networks with the
desired hierarchical structure, mean degree, and noise
level. Tests were conducted for (cid:104)k(cid:105) ∈ [5, 15], β ∈ [0.1, 5],
and all performance plots show mean and standard devi-
ations for 100 replicates.

Performance measures for edge prediction

For multigraphs, we deﬁne the accuracy of probabilis-
tic edge prediction as the extent to which Pij is a good

estimate of the fraction of interactions between i and j
that point from i to j, given that there are any edges to
predict at all, i.e., assuming ¯Aij = Aij + Aji > 0. If this
prediction were perfect, we would have Aij = ¯AijPij. We
deﬁne σA as 1 minus the sum of the absolute values of
the diﬀerence between Aij and this estimate,

σa = 1 −

1
2M

(cid:88)

i,j

(cid:12)
(cid:12)Aij − ¯Aij Pij

(cid:12)
(cid:12) ,

(12)

where M is the number of directed edges in the subset
of the network under consideration, e.g., the training or
test set. Then σa = 1 if Pij = Aij/ ¯Aij for all i, j, and
σa = 0 if for each i, j all the edges go from i to j (say)
but Pij = 0 and Pji = 1.

To measure accuracy via the conditional log-likelihood,
we ask with what probability we would get the directed
network A from the undirected network ¯A if each edge
between i and j points from i → j with probability Pij
and from j → i with probability Pji = 1−Pij. This gives

σL = log Pr[A | ¯A]
(cid:88)

=

log

(cid:19)

(cid:18)Aij + Aji
Aij

i,j

+ log

(cid:104)

P Aij
ij

[1 − Pij]Aji(cid:105)

,

(13)

y

where (cid:0)x
(cid:1) is the binomial coeﬃcient. We disregard the
ﬁrst term of this sum since it does not depend on P .
If we wish to compare networks of diﬀerent sizes as in
Fig. 3, we can normalize σL by the number of edges.
For an extensive discussion of performance metrics see
Supplementary Text S9.

Statistical signiﬁcance of ranks

We compute a standard left-tailed p-value for the sta-
tistical signiﬁcance of the ranks s∗ by comparing the
ground state energy Eq. (11) of the real network A with
the null distribution of ground state energies of an ensem-
ble of networks ˜A drawn from the null model where ¯Aij is

10

kept ﬁxed, but the direction of each edge is randomized.

p-value = Pr[H(s∗; A) ≤ H(˜s∗; ˜A)] .

(14)

In practice, this p-value is estimated by drawing many
samples from the null distribution by randomizing the
edge directions of A to produce ˜A, computing the ranks
˜s∗ from Eq. (3), and then computing the ground state
energy Eq. (11) of each.

Cross-validation tests

We performed edge prediction using 5-fold cross-
validation. In each realization, we divide the interacting
pairs i, j, i.e., those with nonzero ¯Aij = Aij + Aji, into
ﬁve equal groups. We use four of these groups as a train-
ing set, inferring the ranks and setting β to maximize σa
or σL (on the left and right of Fig. 3 respectively). We
then use the ﬁfth group as a test set, asking the algorithm
for Pij for each pair i, j in that group, and report σa or
σL on that test set. By varying which group we use as the
test set, we get 5 trials per realization: for instance, 50
realizations give us 250 trials of cross-validation. Results
for 2-fold cross-validation are reported in SI.

Acknowledgements

CDB and CM were supported by the John Tem-
pleton Foundation. CM was also supported by the
Army Research Oﬃce under grant W911NF-12-R-0012.
DBL was supported by NSF award SMA-1633747 and
the Santa Fe Institute Omidyar Fellowship. We thank
Aaron Clauset and Johan Ugander for helpful com-
ments.
Competing Interests: The authors declare
that they have no competing interests. All authors
derived the model, analyzed results, and wrote the
manuscript. CDB wrote Python implementations and
DBL wrote MATLAB implementations. Open-source
code in Python, MATLAB, and SAS/IML available at
https://github.com/cdebacco/SpringRank.

[1] C. Drews, The concept and deﬁnition of dominance in

animal behaviour, Behaviour 125, 283 (1993).

[2] E. A. Power, Social support networks and religiosity in
rural South India, Nature Human Behaviour 1, 0057
(2017).

[3] A. Clauset, S. Arbesman, D. B. Larremore, Systematic
inequality and hierarchy in faculty hiring networks, Sci-
ence Advances 1, e1400005 (2015).

[4] S. D. Cˆot´e, M. Festa-Bianchet, Reproductive success in
female mountain goats: the inﬂuence of age and social
rank, Animal Behaviour 62, 173 (2001).

[5] E. A. Hobson, S. DeDeo, Social feedback and the emer-
gence of rank in animal society, PLoS Computational Bi-
ology 11, e1004411 (2015).

[6] C. J. Dey, J. S. Quinn, Individual attributes and self-
organizational processes aﬀect dominance network struc-
ture in pukeko, Behavioral Ecology 25, 1402 (2014).
[7] C. J. Dey, A. R. Reddon, C. M. O’Connor, S. Balshine,
Network structure is related to social conﬂict in a cooper-
atively breeding ﬁsh, Animal Behaviour 85, 395 (2013).
[8] M. A. Cant, J. B. Llop, J. Field, Individual variation
in social aggression and the probability of inheritance:
theory and a ﬁeld test, The American Naturalist 167,
837 (2006).

[9] B. Ball, M. E. Newman, Friendship networks and social

status, Network Science 1, 16 (2013).

[10] S. Szymanski, The economic design of sporting contests,

Journal of Economic Literature 41, 1137 (2003).

11

[33] A. Z. Jacobs, J. A. Dunne, C. Moore, A. Clauset, Un-
tangling the roles of parasites in food webs with gener-
ative network models, arXiv preprint arXiv:1505.04741
(2015).

[34] P. D. Hoﬀ, A. E. Raftery, M. S. Handcock, Latent space
approaches to social network analysis, Journal of the
American Statistical Association 97, 1090 (2001).
[35] D. A. Spielman, S.-H. Teng, Nearly linear time algo-
rithms for preconditioning and solving symmetric, diago-
nally dominant linear systems, SIAM Journal on Matrix
Analysis and Applications 35, 835 (2014).

[36] I. Koutis, G. L. Miller, R. Peng, A nearly-m log n time
solver for SDD linear systems, Proc. 52nd Foundations
of Computer Science (FOCS) (IEEE Presss, 2011), pp.
590–598.

[37] S. de Silva, V. Schmid, G. Wittemyer, Fission–fusion pro-
cesses weaken dominance networks of female asian ele-
phants in a productive habitat, Behavioral Ecology 28,
243 (2016).

[38] D. R. Hunter, MM algorithms for generalized Bradley-

Terry models, Annals of Statistics pp. 384–406 (2004).

[39] NCAA, http://www.ncaa.org/championships/statistics

(2018).

[40] S. F. Way, D. B. Larremore, A. Clauset, Gender, pro-
ductivity, and prestige in computer science faculty hir-
ing networks, Proc. 25th Intl Conf on World Wide Web
(2016), pp. 1169–1179.

[41] B. Majolo, J. Lehmann, A. de Bortoli Vizioli, G. Schino,
Fitness-related beneﬁts of dominance in primates, Amer-
ican Journal of Physical Anthropology 147, 652 (2012).
[42] N. Lin, Social Capital: A Theory of Social Structure and
Action, vol. 19 (Cambridge University Press, 2002).
[43] K. S. Cook, M. Levi, R. Hardin, Whom can we trust?:
How groups, networks, and institutions make trust possi-
ble (Russell Sage Foundation, 2009).

[44] M. Mines, Public faces, private lives: Community and in-
dividuality in South India (University of California Press,
1994).

[45] D. Shizuka, D. B. McDonald, A social network perspec-
tive on measurements of dominance hierarchies, Animal
Behaviour 83, 925 (2012).

[46] L. Rosasco, E. D. Vito, A. Caponnetto, M. Piana,
A. Verri, Are loss functions all the same?, Neural Com-
putation 16, 1063 (2004).

[11] R. Baumann, V. A. Matheson, C. A. Howe, Anomalies
in tournament design: the madness of march madness,
Journal of Quantitative Analysis in Sports 6 (2010).
[12] P. Bonacich, Power and centrality: A family of measures,

American Journal of Sociology 92, 1170 (1987).

[13] L. Page, S. Brin, R. Motwani, T. Winograd, The PageR-
ank citation ranking: Bringing order to the web., Tech.
rep., Stanford InfoLab (1999).

[14] S. Negahban, S. Oh, D. Shah, Rank centrality: Ranking
from pairwise comparisons, Operations Research (2016).
[15] T. Callaghan, P. J. Mucha, M. A. Porter, Random walker
ranking for NCAA division IA football, American Math-
ematical Monthly 114, 761 (2007).

[16] I. Ali, W. D. Cook, M. Kress, On the minimum violations
ranking of a tournament, Management Science 32, 660
(1986).

[17] P. Slater, Inconsistencies in a schedule of paired compar-

isons, Biometrika 48, 303 (1961).

[18] M. Gupte, P. Shankar, J. Li, S. Muthukrishnan, L. Iftode,
Finding hierarchy in directed online social networks,
Proc. 20th Intl. Conf. on the World Wide Web (ACM,
2011), pp. 557–566.

[19] F. Fogel, A. d’Aspremont, M. Vojnovic, Serialrank: Spec-
tral ranking using seriation, Advances in Neural Informa-
tion Processing Systems (2014), pp. 900–908.

[20] M. Cucuringu, Sync-rank: Robust ranking, constrained
ranking and rank aggregation via eigenvector and sdp
synchronization, IEEE Transactions on Network Science
and Engineering 3, 58 (2016).

[21] E. Letizia, P. Barucca, F. Lillo, Resolution of ranking
hierarchies in directed networks, PloS one 13, e0191604
(2018).

[22] N. Tatti, Tiers for peers: a practical algorithm for discov-
ering hierarchy in weighted networks, Data Mining and
Knowledge Discovery 31, 702 (2017).

[23] K. E. Train, Discrete Choice Methods with Simulation

(Cambridge University Press, 2009).

[24] R. A. Bradley, M. E. Terry, Rank analysis of incom-
plete block designs: I. the method of paired comparisons,
Biometrika 39, 324 (1952).

[25] R. D. Luce, On the possible psychophysical laws., Psy-

chological Review 66, 81 (1959).

[26] H. A. David, Ranking from unbalanced paired-

comparison data, Biometrika 74, 432 (1987).

[27] W. N. Colley, Colley’s bias free college football rank-
ing method: The Colley matrix explained (2002).
Http://www.colleyrankings.com/matrate.pdf.

[28] A. E. Elo, The Rating of Chessplayers, Past and Present

(Arco Pub., 1978).

[29] R. Coulom, Whole-history rating: A Bayesian rating sys-
tem for players of time-varying strength, International
Conference on Computers and Games (Springer, 2008),
pp. 113–124.

[30] R. Herbrich, T. Minka, T. Graepel, Trueskill: a Bayesian
skill rating system, Advances in Neural Information Pro-
cessing Systems (2007), pp. 569–576.

[31] R. J. Williams, A. Anandanadesan, D. Purves, The prob-
abilistic niche model reveals the niche structure and role
of body size in a complex food web, PLoS ONE 5, e12092
(2010).

[32] R. J. Williams, D. W. Purves, The probabilistic niche
model reveals substantial variation in the niche structure
of empirical food webs, Ecology 92, 1849 (2011).

Supporting Information (SI)

S1. Deriving the linear system minimizing the Hamiltonian

The SpringRank Hamiltonian Eq. (2) is convex in s and we set its gradient ∇H(s) = 0 to obtain the global

minimum:

Let the weighted out-degree and in-degree be dout
written as

i = (cid:80)

j Aij and din

i = (cid:80)

j Aji, respectively. Then Eq. (S1) can be

∂H
∂si

(cid:88)

=

j

[Aij (si − sj − 1) − Aji (sj − si − 1)] = 0 .

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i − din
i

(cid:1) −

[Aij + Aji] sj = 0 .

(cid:88)

j

We now write the system of N equations together by introducing the following matrix notation. Let Dout =
diag(dout
N ) be diagonal matrices, let 1 be the N -dimensional vector of all ones.
Then Eq. (S2) becomes

N ) and Din = diag(din

1 , . . . , din

, . . . , dout

1

(cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) s = (cid:2)Dout − Din(cid:3) 1 .

This is a linear system of the type B s = b, where B = (cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) and b = (cid:2)Dout − Din(cid:3) 1. The rank
of B is at most N − 1 and more generally, if the network represented by A consists of C disconnected components, B
will have rank N − C. In fact, B has an eigenvalue 0 with multiplicity C, and the eigenvector 1 is in the nullspace.
B is not invertible, but we can only invert in the N − C-dimensional subspace orthogonal to the nullspace of B. The
family of translation-invariant solutions s∗ is therefore deﬁned by

s∗ = (cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3)−1 (cid:2)Dout − Din(cid:3) 1 ,

in which the notation [·]−1 should be taken as the Moore-Penrose pseudoinverse.

In practice, rather than constructing the pseudo-inverse, it will be more computationally eﬃcient (and for large
systems, more accurate) to solve the linear system in an iterative fashion. Since we know that solutions may be
translated up or down by an arbitrary constant, the system can be made full-rank by ﬁxing the position of an
arbitrary node 0. Without loss of generality, let sN = 0. In this case, terms that involve sN can be dropped from
Eq. (S2), yielding

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i − din
i

(cid:1) −

[Aij + Aji] sj = 0 ,

i (cid:54)= N

− (cid:0)dout

N − din
N

(cid:1) −

[AN j + AjN ] sj = 0 .

N −1
(cid:88)

j=1

N −1
(cid:88)

j=1

N −1
(cid:88)

j=1

Adding Eq. (S5) to Eq. (S6) yields

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i + dout

N − din

i − din
N

(cid:1) −

[Aij + AN j + Aji + AjN ] sj = 0 ,

(S7)

which can be written in matrix notation as

(cid:105)
(cid:104)
Dout + Din − ˚A

s = (cid:2)Dout − Din(cid:3) 1 + (cid:0)dout

N − din
N

(cid:1) 1 ,

where

˚Aij = Aij + AN j + Aji + AjN .

In this formulation, Eq. (S8) can be solved to arbitrary precision using iterative methods that take advantage of the
sparsity of ˚A. The resulting solution may then be translated by an arbitrary amount as desired.

12

(S1)

(S2)

(S3)

(S4)

(S5)

(S6)

(S8)

(S9)

13

(S12)

(S13)

S2. Poisson generative model

The expected number of edges from node i to node j is c exp

and therefore the likelihood of

(cid:104)

2 (si − sj − 1)2(cid:105)
− β

observing a network A, given parameters β, s, and c is

(cid:104)

c e− β

2 (si−sj −1)2(cid:105)Aij

P (A | s, β, c) =

(cid:89)

i,j

Aij!

(cid:104)
exp

−c e− β

2 (si−sj −1)2(cid:105)

.

(S10)

Taking logs yields

log P (A | s, β, c) =

Aij log c −

Aij (si − sj − 1)2 − log [Aij!] − ce− β

2 (si−sj −1)2

.

(S11)

(cid:88)

i,j

β
2

Discarding the constant term log [Aij!], and recognizing the appearance of the SpringRank Hamiltonian H(s), yields

L(A | s, β, c) = −βH(s) +

Aij log c −

ce− β

2 (si−sj −1)2

.

(cid:88)

i,j

(cid:88)

i,j

Taking ∂L/∂c and setting it equal to zero yields

ˆc =

(cid:80)

i,j Aij
2 (si−sj −1)2 ,

(cid:80)

i,j e− β

which has the straightforward interpretation of being the ratio between the number of observed edges and the expected
number of edges created in the generative process for c = 1. Substituting in this solution and letting M = (cid:80)
i,j Aij
yields

L(A | s, β) = −βH(s) + M log ˆc −

ˆc e− β

2 (si−sj −1)2

(cid:88)

i,j





(cid:88)

i,j

M
β





(cid:88)

i,j





 .

= −βH(s) + M log M − M log

e− β

2 (si−sj −1)2

 − M .

(S14)

The terms M log M and M may be neglected since they do not depend on the parameters, and we divide by β,
yielding a log-likelihood of

L(A | s, β) = −H(s) −

log

e− β

2 (si−sj −1)2

(S15)

(cid:3) where (cid:104)·(cid:105)E denotes
Note that the SpringRank Hamiltonian may be rewritten as H(s) = M (cid:2) 1
the average over elements in the edge set E. In other words, H(s) scales with M and the square of the average spring
length. This substitution for H(s) allows us to analyze the behavior of the log-likelihood

2 (cid:104)Aij(si − sj − 1)2(cid:105)E

L(A | s, β) = −M

(cid:68)

Aij (si − sj − 1)2(cid:69)

+

log

1
β

E

e− β

2 (si−sj −1)2





(cid:88)

i,j










.






1
2

(S16)

Inside the logarithm there are N 2 terms of ﬁnite value, so that the logarithm term is of order O( log N
β ). Thus, for well-
resolved hierarchies, i.e. when β is large enough that the sampled edges consistently agree with the score diﬀerence
between nodes, the maximum likelihood ranks ˆs approach the ranks s∗ found by minimizing the Hamiltonian. In
practice, exactly maximizing the likelihood would require extensive computation, e.g. by using local search heuristic
or Markov chain Monte Carlo sampling.

S3. Rewriting the energy

The Hamiltonian Eq. (2) can be rewritten as

2 H(s) =

Aij (si − sj − 1)2

N
(cid:88)

i,j=1

N
(cid:88)

i,j=1

N
(cid:88)

=

=

N
(cid:88)

=

i

Aij

(cid:0)s2

i + s2

j − 2sisj + 1 − 2si + 2sj

(cid:1)

N
(cid:88)

s2
i

Aij +

N
(cid:88)

N
(cid:88)

s2
j

j

i=1

Aij − 2

(cid:88)

(cid:88)

si

Aijsj + M

i

j

i

j=1

−2

N
(cid:88)

N
(cid:88)

si

i=1

j=1

Aij + 2

N
(cid:88)

N
(cid:88)

sj

j=1

i=1

Aij

s2
i

(cid:0)dout

i + din
i

(cid:1) − 2

si

(cid:0)dout

i − din
i

(cid:1) + M − 2

(cid:88)

(cid:88)

si

Aijsj .

(cid:88)

i

i

j

From Eq. (S2) we have

(cid:88)

s2
i

(cid:0)dout

i + din
i

(cid:1) −

(cid:88)

si

(cid:0)dout

i − din
i

(cid:1) =

(cid:88)

(cid:88)

si

[Aij + Aji] sj .

j

i

i

j

We can substitute this into Eq. (S17)
(cid:88)

(cid:88)

2H(s) =

si

[Aij + Aji] sj −

si

(cid:0)dout

i − din
i

(cid:1) + M − 2

(cid:88)

(cid:88)

si

Aijsj

(cid:88)

i

i

j

j

si

(cid:0)din

i − dout

i

(cid:1) + M

hisi + M ,

=

=

i
(cid:88)

i
(cid:88)

i

where hi ≡ din

i − dout

i

.

S4. Ranks distributed as a multivariate Gaussian distribution

Assuming that the ranks are random variables distributed as a multivariate Gaussian distribution of average ¯s and

covariance matrix Σ, we have:

We can obtain this formulation by considering a Boltzman distribution with the Hamiltonian Eq. (2) as the energy
term and inverse temperature β so that

P (s) ∝ exp

−

(s − ¯s)(cid:124)Σ−1(s − ¯s)

.

(cid:18)

1
2

(cid:19)



P (s) ∝ exp

−

Aij (si − sj − 1)2

 .



N
(cid:88)

i,j=1

β
2

1
2

Manipulating the exponent of Eq. (S20) yields

(s − ¯s)T Σ−1(s − ¯s) =

(cid:0)sT Σ−1s − 2sT Σ−1¯s + ¯sT Σ−1¯s(cid:1) ,

1
2

whereas the parallel manipulation of Eq. (S21) yields

β
2

N
(cid:88)

i,j=1

β
2

Aij (si − sj − 1)2 =

(cid:2)sT (cid:0)Dout + Din − AT − A(cid:1) s + 2 sT (Din − Dout)1 + M (cid:3) ,

(S23)

14

(S17)

(S18)

(S19)

(S20)

(S21)

(S22)

where 1 is a vector of ones and Din are diagonal matrices whose entries are the in- and out-degrees, Dout
and Din
on s because irrelevant when accounting for normalization, we obtain:

j Aij
i,j Aij. Comparing these last two expressions and removing terms that do not depend

j Aji and M = (cid:80)

ii = (cid:80)

ii = (cid:80)

Σ = 1
β

(cid:0)Dout + Din − AT − A(cid:1)−1

and ¯s = β Σ (cid:0)Dout − Din(cid:1) 1 = s∗ .

(S24)

S5. Bayesian SpringRank

Adopting a Bayesian approach with a factorized Gaussian prior for the ranks, we obtain that the s that maximizes
the posterior distribution is the one that minimizes the regularized SpringRank Hamiltonian Eq. (4), i.e. the s that
solves the linear system Eq. (5). In fact, deﬁning:

P (s) = Z −1(β, α)

e−β α

2 (si−1)2

= Z −1(β, α)

e−β αH0(si) ,

(S25)

(cid:89)

i∈V

is a normalization constant that depends on α and β, and following the same steps as

where Z(β, α) =
before we get:

(cid:105)N/2

(cid:104) 2π
β α

log P (s | A) =

log P (Aij | s) − βα

H0(si) + log (Z (β, α) )

(cid:89)

i∈V

(cid:88)

i,j

(cid:34)

(cid:88)

i∈V

(cid:35)

(cid:88)

i∈V

= − β

H(s) + α

H0(si)

+ C ,

(S26)

where C is a constant that does not depend on the parameters, and thus may be ignored when maximizing log P (s | A).

The parameter c included in the generative model (7) controls for network’s sparsity. We can indeed ﬁx it so to

obtain a network with a desired expected number of edges (cid:104)M (cid:105) as follows:

For a given vector of ranks s and inverse temperature β, the c realizing the desired sparsity will then be:

S6. Fixing c to control for sparsity

(cid:104)M (cid:105) ≡

(cid:104)Aij(cid:105) = c

e− β

2 (si−sj −1)2

.

(cid:88)

i,j

(cid:88)

i,j

c =

(cid:80)

i,j e− β

(cid:104)M (cid:105)
2 (si−sj −1)2 =

(cid:104)k(cid:105)N
2 (si−sj −1)2 ,

(cid:80)

i,j e− β

where (cid:104)k(cid:105) is the expected node degree (cid:104)k(cid:105) = (cid:80)N
i=1
model with Bernoulli distribution.

(cid:2)din

i + dout

i

(cid:3). Similar arguments apply when considering a generative

S7. Comparing optimal β for predicting edge directions

In the main text, (12) and Eq. (13) deﬁne the accuracy of edge prediction, in terms of the number of edges predicted
correctly in each direction and the log-likelihood conditioned on the undirected graph. Here we compute the optimal
values of β for both notions of accuracy. In both computations that follow, the following two facts will be used:

and

ij(β) = 2 (si − sj) e−2β(si−sj ) P 2
P (cid:48)

ij(β) ,

1 =

Pij(β)
1 − Pij(β)

e−2β(si−sj ) .

15

(S27)

(S28)

(S29)

A. Choosing β to optimize edge direction accuracy

We take the derivative of Eq. (12) with respect to β, set it equal to zero, and solve as follows.

0 ≡

∂σa(β)
∂β

=

∂
∂β



1 −

1
2m

(cid:88)

i,j

|Aij − (Aij + Aji) Pij(β)|

 .

(S30)



In preparation to take the derivatives above, note that P (cid:48)
takes one sign, the (j, i) term takes the opposite sign,

ij(β) = −P (cid:48)

ji(β) and that whenever the (i, j) term of σa(β)

Without loss of generality, assume that the (i, j) term is positive and the (j, i) term is negative. This implies that

Aij − (Aij + Aji) Pij(β) = − [Aji − (Aij + Aji) Pji(β)] .

∂
∂β

∂
∂β

|Aij − (Aij + Aji) Pij(β)| = − (Aij + Aji) P (cid:48)

ij(β) ,

|Aji − (Aij + Aji) Pji(β)| = − (Aij + Aji) P (cid:48)

ij(β) .

and

∂
∂β

In other words, the derivatives of the (i, j) and (j, i) terms are identical, and the sign of both depends on whether the
quantity [Aij − (Aij + Aji)Pij(β)] is positive or negative. We can make this more precise by directly including the
sign of the (i, j) term, and by using Eq. (S28), to ﬁnd that

|Aij − (Aij + Aji) Pij(β)| = −2 (Aij + Aji) (si − sj) e−2β(si−sj ) P 2

ij × sign(cid:8)Aij − (Aij + Aji) Pij(β)(cid:9) .

(S34)

Expanding P 2

ij and reorganizing yields

∂
∂β

|Aij − (Aij + Aji) Pij(β)| = −2

(Aij + Aji) (si − sj)
2 cosh [2β (si − sj)] + 2

× sign(cid:8)Aij − (Aij + Aji) Pij(β)(cid:9) .

(S35)

Combining terms (i, j) and (j, i), the optimal inverse temperature for local accuracy ˆβa is that which satisﬁes

N
(cid:88)

0 =

(i,j)∈U (E)

(Aij + Aji) (si − sj)
(cid:105)
2 ˆβa (si − sj)

(cid:104)

cosh

+ 1

× sign(cid:8)Aij − (Aij + Aji) Pij( ˆβa)(cid:9) ,

(S36)

which may be found using standard root-ﬁnding methods.

B. Choosing β to optimize the conditional log likelihood

We take the derivative of Eq. (13) with respect to β, set it equal to zero, and partially solve as follows.

0 ≡

∂σL(β)
∂β

=

∂
∂β





(cid:88)

log

i,j

(cid:19)

(cid:18)Aij + Aji
Aij

+ log

(cid:104)

Pij(β)Aij [1 − Pij(β)]Aji(cid:105)



 .

Combining the (i, j) and (j, i) terms, we get
(cid:18)Aij + Aji
Aij

∂
∂β

0 ≡

(cid:88)

log

(cid:19)

(i,j)∈U (E)

+ log

(cid:19)

(cid:18)Aij + Aji
Aji

(cid:20) Aij
Pij(β)

−

Aji
1 − Pij(β)

(cid:21) ∂Pij(β)
∂β

(i,j)∈U (E)

(cid:88)

(cid:88)

=

=

(i,j)∈U (E)

2 (si − sj) [Aij − (Aij + Aji) Pij(β)]

Pij(β)
1 − Pij(β)

e−2β(si−sj ) .

+ [ Aij log Pij(β) + Aji log [1 − Pij(β)] ]

16

(S31)

(S32)

(S33)

(S37)

(S38)

Applying both Eq. (S28) and Eq. (S29), the optimal inverse temperature for the conditional log likelihood ˆβL is that
which satisﬁes

0 =

(cid:88)

2 (si − sj)

(cid:104)

(cid:105)
Aij − (Aij + Aji) Pij( ˆβL)

,

(i,j)∈U (E)

which, like Eq. (S36) may be found using standard root-ﬁnding methods. Comparing equations Eq. (S36) and
Eq. (S39), we can see that the values of β that maximize the two measures may, in general, be diﬀerent. Table S2
shows for optimal values for ˆβL and ˆβa for various real-world datasets.

17

(S39)

S8. Bitwise accuracy σb

Some methods provide rankings but do not provide a model to estimate Pij, meaning that Eq. (12) and Eq. (13)
cannot be used. Nevertheless, such methods still estimate one bit of information about each pair (i, j): whether the
majority of the edges are from i to j or vice versa. This motivates the use of a bitwise version of σa, which we call
σb,

σb = 1 −

Θ (si − sj) Θ(Aji − Aij) ,

(S40)

1
N 2 − t

(cid:88)

i,j

where Θ(x) = 1 if x > 0 and Θ(x) = 0 otherwise, and N is the number of nodes and t is the number of instances in
which Aij = Aji; there are N 2 − t total bits to predict. Results in terms of this measure on the networks considered
in the main text are shown in Figure S4. In the special case that the network is unweighted (A is a binary adjacency
matrix) and there are no bi-directional edges (if Aij = 1, then Aji = 0), then 1 − σb is the fraction of edges that
violate the rankings in s. In other words, for this particular type of network, 1 − σb is the minimum violations rank
penalty normalized by the total number of edges in the network, i.e., 1
M

i,j Θ(si − sj) Aji.

(cid:80)

S9. Performance metrics

When evaluating the performance of a ranking algorithm in general one could consider a variety of diﬀerent measures.
One possibility is to focus on the ranks themselves, rather than the outcomes of pairwise interactions, and calculate
correlation coeﬃcients as in Fig. 1; this is a valid strategy when using synthetic data thanks to the presence of ground
truth ranks, but can only assess the performance with respect of the speciﬁc generative process used to generate
the pairwise comparisons, as we point out in the main text. This strategy can also be applied for comparisons with
observed real world ranks, as we did in Table S11 and it has been done for instance in [19, 20] to compare the ranks
with those observed in real data in sports. However, the observed ranks might have been derived from a diﬀerent
process than the one implied by the ranking algorithm considered. For instance, in the faculty hiring networks, popular
ranking methods proposed by domain experts for evaluating the prestige of universities do not consider interactions
between institutions, but instead rely on a combination of performance indicators such as ﬁrst-year student retention
or graduation rates. The correlation between observed and inferred ranks should thus be treated as a qualitative
indicator of how well the two capture similar features of the system, such as prestige, but should not be used to
evaluate the performance of a ranking algorithm.

Alternatively, one can look at the outcomes of the pairwise comparisons and relate them to the rankings of the
nodes involved as in Eqs. (12) and (13) for testing prediction performance. A popular metric of this type is the number
of violations (also called upsets), i.e., outcomes where a higher ranked node is defeated by a lower ranked one. This
is very similar to the bitwise accuracy deﬁned in (S40), indeed when there are no ties and two nodes are compared
only once, then they are equivalent. These can be seen as low-resolution or coarse-grained measures of performance:
for each comparison predict a winner, but do not distinguish between cases where the winner is easy to predict and
cases where there is almost a tie. In particular, an upset between two nodes ranked nearby counts as much as an
upset between two nodes that are far away in the ranking. The latter case signals a much less likely scenario. In order
to distinguish these two situations, one can penalize each upset by the nodes’ rank diﬀerence elevated to a certain
power d. This is what the agony function does [18] with the exponent d treated as a parameter to tune based on the
application. When d = 0 we recover the standard number of unweighted upsets.

Note that optimization of agony is often used as a non-parametric approach to detect hierarchies [21], in particular
for ordinal ranks. For ordinal ranks, rank diﬀerences are integer-valued and equal to one for adjacent-ranked nodes,
yet for real-valued scores this is not the case. Therefore the result of the agony minimization problem can vary

18

widely between ordinal and real valued ranking algorithms. (We note that the SpringRank objective function, i.e.,
the Hamiltonian in Eq. (2), can be considered a kind of agony. However, since we assume that nearby pairs are more
likely to interact, it is large for a edge from i to j if i is ranked far above or far below j, and more speciﬁcally whenever
si is far from sj + 1.)

In contrast to the coarse prediction above—which competitor is more likely to win?—we require, when possible,
more precise predictions in Eqs. (12) and (13), which ask how much more likely is one competitor to win? This,
however, requires the ranking algorithm to provide an estimate of Pij, the probability that i wins over j, which is
provided only by BTL and SpringRank; all other methods compared in this study provide orderings or embeddings
without probabilistic predictions.

The conditional log-likelihood σL as deﬁned in Eq. (13) can be seen as a Log Loss often used as a classiﬁcation loss
function [46] in statistical learning. This type of function heavily penalizes ranking algorithms that are very conﬁdent
about an incorrect outcome, e.g. when the predicted Pij is close to 1, i very likely to win over j, but the observed
outcome is that j wins over i. For this reason, this metric is more sensitive to outliers, as when in sports a very
strong team loses against one at the bottom of the league. The accuracy σb deﬁned in Eq. (12) focuses instead in
predicting the correct proportions of wins/losses between two nodes that are matched in several comparisons. This
is less sensitive to outliers, and in fact if Pij is close but not exactly equal to 1, for a large number of comparisons
between i and j, we would expect that j should indeed win few times, e.g. if Pij = 0.99 and i, j are compared 100
times, σa is maximized when i wins 99 times and j wins once.

S10. Parameters used for regularizing ranking methods

When comparing SpringRank to other methods, we need to deal with the fact that certain network structures cause
other methods to fail to return any output. Eigenvector Centrality cannot, for example, be applied to directed trees,
yet this is precisely the sort of structure that one might expect when hierarchy becomes extreme.

More generally, many spectral techniques fail on networks that are not strongly connected, i.e., where it is not the
case that one can reach any node from any other by moving along a path consistent with the edge directions, since
in that case the adjacency matrix is not irreducible and the Perron-Frobenius theorem does not apply. In particular,
nodes with zero out-degree—sometimes called “dangling nodes” in the literature [13]—cause issues for many spectral
methods since the adjacency matrix annihilates any vector supported on such nodes. In contrast, the SpringRank
optimum given by Eq. (3) is unique up to translation whenever the network is connected in the undirected sense, i.e.,
whenever we can reach any node from any other by moving with or against directed edges.

A diﬀerent issue occurs in the case of SyncRank. When edges are reciprocal in the sense that an equal number
of edges point in each direction, they eﬀectively cancel out. That is, if Aij = Aji, the corresponding entries in the
SyncRank comparison matrix will be zero, Cij = Cji = 0, as if i and j were never compared at all. As a result, there
can be nodes i such that Cij = Cji = 0 for all j. While rare, these pathological cases exist in real data and during
cross-validation tests, causing the output of SyncRank to be undeﬁned.

In all these cases, regularization is required. Our regularized implementations of ﬁve ranking methods are described

below:

• Regularized Bradley-Terry-Luce (BTL). If there exist dangling nodes, the Minimization-Maximization
algorithm to ﬁt the BTL model to real data proposed in [38] requires a regularization. In this case we set the
total number of out-edges dout
i = 10−6 for nodes that would have di = 0 otherwise. This corresponds to Wi in
Eq.(3) of [38].

• Regularized PageRank. If there exist dangling nodes, we add an edge of weight 1/N from each dangling
node to every other node in the network. For each dataset we tried three diﬀerent values of the teleportation
parameter, α ∈ {0.4, 0.6, 0.8}, and reported the best results of these three.

• Regularized Rank Centrality. If there exist dangling nodes, we use the regularized version of the algorithm

presented in Eq. (5) of [14] with (cid:15) = 1.

• Regularized SyncRank. If there are nodes whose entries in the comparison matrix C are zero, we add a

small constant (cid:15) = 0.001 to the entries of H in Eq. (13) of Ref. [20], so that D is invertible.

• Regularized Eigenvector Centrality. If the network is not strongly connected, we add a weight of 1/N to

every entry in A and then diagonalize.

19

S11. Supplemental Tables

Comp. Sci. SpringRank MVR US News NRC Eig. C. PageRank
0.57
SpringRank
0.48
MVR
0.41
US News
0.41
NRC
0.74
Eig. C.
-
PageRank

0.80 0.72
0.81 0.73
- 0.73
0.73
-
0.69 0.68
0.41 0.41

0.84
0.80
0.69
0.68
-
0.74

0.96
-
0.81
0.73
0.80
0.48

-
0.96
0.80
0.72
0.84
0.57

Business
SpringRank
MVR
US News
NRC
Eig. C.
PageRank

History
SpringRank
MVR
US News
NRC
Eig. C.
PageRank

SpringRank MVR US News NRC Eig. C. PageRank
0.75
0.74
0.69
0.72
0.60
-
-
-
0.72
0.68
-
0.60

0.98
-
0.72
-
0.92
0.69

0.92
0.92
0.68
-
-
0.72

-
0.98
0.74
-
0.92
0.75

-
-
-
-
-
-

SpringRank MVR US News NRC Eig. C. PageRank
0.69
0.57
0.51
0.44
0.88
-

0.86 0.66
0.86 0.65
- 0.66
0.66
-
0.72 0.59
0.51 0.44

0.86
0.77
0.72
0.59
-
0.88

0.95
-
0.86
0.65
0.77
0.57

-
0.95
0.86
0.66
0.86
0.69

TABLE S1. Pearson correlation coeﬃcients between various rankings of faculty hiring networks. All coeﬃcients are statistically
signiﬁcant (p < 10−9). SpringRank is most highly correlated with Minimum Violations Ranks across all three faculty hiring
networks. Among US News and NRC rankings, SpringRank is more similar to US News. Values for US News and NRC were
[3] for comparison to the ranks available at the same time that the faculty hiring data were collected. The
drawn from Ref.
NRC does not rank business departments.

N M H/M Acc. σa ˆβL

ˆβa

Type
DataSet
Anim. Dom. 21 838 0.174 0.930
Parakeet G1 [5]
Anim. Dom. 19 961 0.193 0.932
Parakeet G2 [5]
0.078 0.923
Asian Elephants [37] Anim. Dom. 20 23
112 7353 0.251 0.881
Business [3]
Fac. Hiring
205 4033 0.220 0.882
Computer Science [3] Fac. Hiring
Fac. Hiring
History [3]
144 3921 0.186 0.909
Soc. Support 415 2497 0.222 0.867
Alak¯apuram [2]
Soc. Support 361 1809 0.241 0.858
Tenpat.t.i [2]

0.008 (0.089)
2.70 6.03 76 (9.1%) / 42
0.011 (0.139)
2.78 18.12 75 (7.8%) / 36
2.33 3.44 2 (8.7%) / 0
0.001 (0.040)
2.04 3.14 1171 (15.9%) / 808 0.019 (0.119)
2.23 8.74 516 (12.8%) / 255 0.013 (0.105)
2.39 5.74 397 (10.1%) / 227 0.012 (0.119)
1.98 7.95 347 (13.9%) / 120 0.011 (0.079)
1.89 8.20 262 (14.5%) / 120 0.012 (0.082)

Viol. (%) / Bound Wt. viol. (per viol.) Depth p-value
2.604 < 10−4
1.879 < 10−4
3.000 0.4466
2.125 < 10−4
2.423 < 10−4
2.234 < 10−4
3.618 < 10−4
3.749 < 10−4

TABLE S2. Statistics for SpringRank applied to real-world networks. Column details are as follows: N is the number
of nodes; M is the number of edges; H/m is the ground state energy per edge; Accuracy σa refers to accuracy in 5-fold
cross-validation tests using temperature ˆβa; ˆβL and ˆβa are temperatures optimizing edge prediction accuracies σL and σa
respectively; Violations refers to the number of edges that violate the direction of the hierarchy as a number, as a percentage
of all edges, with a lower bound provided for reference, computed as the number of unavoidable violations due to reciprocated
edges; Weighted violations are the sum of each violation weighted by the diﬀerence in ranks between the oﬀending nodes; Depth
is smax − smin; p-value refers to the null model described in the Materials and Methods. Relevant performance statistics for
NCAA datasets (53 networks) are reported elsewhere; see Fig. S3.

S12. Supplemental Figures

20

FIG. S1. Performance (Pearson correlation) on synthetic data. Tests were performed as in Fig. 1, but here performance
is measured using Pearson correlation. This favors algorithms like SpringRank and BTL, that produce real-valued ranks, over
ordinal ranking schemes like Minimum Violation Ranking which are not expected to recover latent positions.
(A) Linear
hierarchy diagrams show latent ranks splanted of 100 nodes, drawn from a standard normal distribution, with edges drawn
via the generative model Eq. (7) for indicated β (noise) values. Blue edges point down the hierarchy and red edges point
up, indicated by arrows. (B) Mean accuracies ± one standard deviation (symbols ± shading) are measured as the Pearson
correlation between method output and splanted for 100 replicates. (C, D) Identical to A and B but for hierarchies of N = 102
nodes divided into three tiers. All plots have mean degree 5; see Fig. 1 for performance curves for Spearman correlation r. See
Materials and Methods for synthetic network generation.

(a) US HS

(b) US BS

(c) US CS

(d) Tenpat.t.i

(e) Alak¯apuram

(f) parakeet G1

(g) parakeet G2

(h) planted β = 5.0

(i) planted β = 0.1

(j) Asian elephant

(k) ER
N = 100, (cid:104)k(cid:105) = 3

FIG. S2. Statistical signiﬁcance testing using the null model distribution of energies. Results are 1000 realizations
of the null model where edge directions are randomized while keeping the total number of interactions between each pair ﬁxed,
for real and synthetic networks: a-c) US History (HS), Business (BS) and Computer Science (CS) faculty hiring networks [3];
d-e) social support networks of two Indian villages [2] considering 5 types of interactions (see main manuscript); f,g) aggression
network of parakeet Group 1 and 2 (as in [5]); h,i) planted network using SpringRank generative model with N = 100 and
mean degree (cid:104)k(cid:105) = 5, Gaussian prior for the ranks with average µ = 0.5 and variance 1 (α = 1/β) and two noise levels β = 5.0
and β = 0.1; j) dominance network of asian elephants [37]; k) Erd˝os-R´enyi directed random network with N = 100 and (cid:104)k(cid:105) = 3.
The vertical line is the energy obtained on the real network. In all but the last two cases we reject the null hypothesis that
edge directions are independent of the ranks, and conclude that the hierarchy is statistically signiﬁcant.

21

FIG. S3. Edge prediction accuracy over BTL for NCAA basketball datasets. Distribution of diﬀerences in performance
of edge prediction of SpringRank compared to BTL on NCAA College Basketball regular season matches for (top) Women and
(middle) Men, deﬁned as (left) the probabilistic edge-prediction accuracy σa Eq. (12) and (right) the conditional log-likelihood
σL Eq. (13). Error bars indicate quartiles and markers show medians, corresponding to 50 independent trials of 5-fold cross-
validation, for a total of 250 test sets for each dataset. The bottom plot is obtained by considering the distributions over all
the seasons together. In terms of number of correctly predicted outcomes, SpringRank correctly predicts on average 8 to 16
more outcomes than BTL for each of the 20 Women NCAA seasons and up to 12 more outcomes for each of the 33 Men NCAA
seasons; for the latter dataset, BTL has an average better prediction in 3 out of the 33 seasons. The number of matches played
per season in the test set varies from the past to the most recents years from 747 to 1079.

22

FIG. S4. Bitwise edge direction prediction. Symbols show medians of bitwise edge prediction accuracies Eq. (S40) over
50 realization of 5-fold cross-validation (for a total of 250 trials) compared with the median accuracy for SyncRank; error bars
indicate quartiles. Thus, points above the dashed line at zero indicate better predictions than SyncRank, while values below
indicate that SyncRank performed better.

23

FIG. S5. Edge prediction accuracy with 2-fold cross-validation. Top: the accuracy of probabilistic edge prediction
of SpringRank compared to the median accuracy of BTL on real and synthetic networks deﬁned as (top left) edge-prediction
accuracy σa Eq. (12) and (top right) the conditional log-likelihood σL Eq. (13); (bottom) bitwise edge prediction accuracies σb
Eq. (S40) of SpringRank and other algorithms compared with the median accuracy of SyncRank. Error bars indicate quartiles
and markers show medians, corresponding to 50 independent trials of 2-fold cross-validation, for a total of 100 test sets for
each network. The two synthetic networks are generated with N = 100, average degree 5, and Gaussian-distributed ranks as
in Fig. 1A, with inverse temperatures β = 1 and β = 5. Notice that these results are similar those of Fig. 3, obtained using
5-fold cross-validation.

Computer Science

24

FIG. S6. Summary of SpringRank applied to Computer Science faculty hiring network [3]. (top-left) A linear
hierarchy diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and
red edges point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned)
and the horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency
matrix; blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of
statistical signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized
samples: the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted,
ordered by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means
algorithm.

History

25

FIG. S7. Summary of SpringRank applied to History faculty hiring network [3]. (top-left) A linear hierarchy diagram
showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up.
(top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal
axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red
dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance
test with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted
line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank,
from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Business

26

FIG. S8. Summary of SpringRank applied to Business faculty hiring network [3]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Asian Elephants

27

FIG. S9. Summary of SpringRank applied to Asian Elephants network [37]. (top-left) A linear hierarchy diagram
showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up.
(top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal
axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red
dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance
test with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted
line is the ground state energy obtained on the observed real network.

Parakeet G1

28

FIG. S10. Summary of SpringRank applied to Parakeet G1 network [5]. (top-left) A linear hierarchy diagram showing
inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up. (top-
middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal axis
is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red dots
represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance test
with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted line
is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank, from
top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Parakeet G2

29

FIG. S11. Summary of SpringRank applied to Parakeet G2 network [5]. (top-left) A linear hierarchy diagram showing
inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up. (top-
middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal axis
is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red dots
represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance test
with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted line
is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank, from
top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Tenpat.t.i

30

FIG. S12. Summary of SpringRank applied to Tenpat.t.i social support network [2]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Alak¯apuram

31

FIG. S13. Summary of SpringRank applied to Alak¯apuram social support network [2]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

A physical model for eﬃcient ranking in networks

Caterina De Bacco,1, 2, ∗ Daniel B. Larremore,3, 4, 2, † and Cristopher Moore2, ‡
1Data Science Institute, Columbia University, New York, NY 10027, USA
2Santa Fe Institute, Santa Fe, NM 87501, USA
3Department of Computer Science, University of Colorado, Boulder, CO 80309, USA
4BioFrontiers Institute, University of Colorado, Boulder, CO 80303, USA

We present a physically-inspired model and an eﬃcient algorithm to infer hierarchical rankings of
nodes in directed networks. It assigns real-valued ranks to nodes rather than simply ordinal ranks,
and it formalizes the assumption that interactions are more likely to occur between individuals with
similar ranks.
It provides a natural statistical signiﬁcance test for the inferred hierarchy, and it
can be used to perform inference tasks such as predicting the existence or direction of edges. The
ranking is obtained by solving a linear system of equations, which is sparse if the network is; thus the
resulting algorithm is extremely eﬃcient and scalable. We illustrate these ﬁndings by analyzing real
and synthetic data, including datasets from animal behavior, faculty hiring, social support networks,
and sports tournaments. We show that our method often outperforms a variety of others, in both
speed and accuracy, in recovering the underlying ranks and predicting edge directions.

Introduction

In systems of many individual entities, interactions and
their outcomes are often correlated with these entities’
ranks or positions in a hierarchy. While in most cases
these rankings are hidden from us, their presence is nev-
ertheless revealed in the asymmetric patterns of interac-
tions that we observe. For example, some social groups
of birds, primates, and elephants are organized accord-
ing to dominance hierarchies, reﬂected in patterns of re-
peated interactions in which dominant animals tend to
assert themselves over less powerful subordinates [1]. So-
cial positions are not directly visible to researchers, but
we can infer each animal’s position in the hierarchy by
observing the network of pairwise interactions. Similar
latent hierarchies have been hypothesized in systems of
endorsement in which status is due to prestige, reputa-
tion, or social position [2, 3]. For example, in academia,
universities may be more likely to hire faculty candidates
from equally or more prestigious universities [3].

In all these cases, the direction of the interactions is af-
fected by the status, prestige, or social position of the en-
tities involved. But it is often the case that even the exis-
tence of an interaction, rather than its direction, contains
some information about those entities’ relative prestige.
For example, in some species, animals are more likely to
interact with others who are close in dominance rank [4–
8]; human beings tend to claim friendships with others
of similar or slightly higher status [9]; and sports tourna-
ments and league structures are often designed to match
players or teams based on similar skill levels [10, 11]. This
suggests that we can infer the ranks of individuals in a so-
cial hierarchy using both the existence and the direction
of their pairwise interactions. It also suggests assigning

real-valued ranks to entities rather than simply ordinal
rankings, for instance in order to infer clusters of entities
with roughly equal status with gaps between them.

In this work we introduce a physically-inspired model
that addresses the problems of hierarchy inference, edge
prediction, and signiﬁcance testing. The model, which
we call SpringRank, maps each directed edge to a di-
rected spring between the nodes that it connects, and
ﬁnds real-valued positions of the nodes that minimizes
the total energy of these springs. Because this optimiza-
tion problem requires only linear algebra, it can be solved
for networks of millions of nodes and edges in seconds.

We also introduce a generative model for hierarchical
networks in which the existence and direction of edges de-
pend on the relative ranks of the nodes. This model for-
malizes the assumption that individuals tend to interact
with others of similar rank, and it can be used to create
synthetic benchmark networks with tunable levels of hi-
erarchy and noise. It can also predict unobserved edges,
allowing us to use cross-validation as a test of accuracy
and statistical signiﬁcance. Moreover, the maximum like-
lihood estimates of the ranks coincides with SpringRank
asymptotically.

We test SpringRank and its generative model version
on both synthetic and real datasets, including data from
animal behavior, faculty hiring, social support networks,
and sports tournaments. We ﬁnd that it infers accurate
rankings, provides a simple signiﬁcance test for hierarchi-
cal structure, and can predict the existence and direction
of as-yet unobserved edges. In particular, we ﬁnd that
SpringRank often predicts the direction of unobserved
edges more accurately than a variety of existing methods,
including popular spectral techniques, Minimum Viola-
tion Ranking, and the Bradley-Terry-Luce method.

∗ cdebacco@santafe.edu; Contributed equally.
† daniel.larremore@colorado.edu; Contributed equally.
‡ moore@santafe.edu

Ranking entities in a system from pairwise compar-
isons or interactions is a fundamental problem in many

Related work

8
1
0
2
 
n
u
J
 
3
1
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
4
v
2
0
0
9
0
.
9
0
7
1
:
v
i
X
r
a

contexts, and many methods have been proposed. One
family consists of spectral methods like Eigenvector Cen-
trality [12], PageRank [13], Rank Centrality [14], and the
method of Callaghan et al. [15]. These methods propose
various types of random walks on the directed network
and therefore produce real-valued scores. However, by
design these methods tend to give high ranks to a small
number of important nodes, giving us little information
about the lower-ranked nodes.
In addition, they often
require explicit regularization, adding a small term to
every element of the adjacency matrix if the graph of
comparisons is not strongly connected.

A second family focuses on ordinal rankings, i.e., per-
mutations, that minimize various penalty functions. This
family includes Minimum Violation Rank [16–18] and Se-
rialRank [19] and SyncRank [20]. Minimum Violation
Rank (MVR) imposes a uniform penalty for every viola-
tion or “upset,” deﬁned as an edge that has a direction
opposite to the one expected by the rank diﬀerence be-
tween the two nodes. Non-uniform penalties and other
generalizations are often referred to as agony methods
[21]. For common choices of the penalty function, mini-
mization can be computationally diﬃcult [17, 22], forcing
us to use simple heuristics that ﬁnd local minima.

SerialRank constructs a matrix of similarity scores be-
tween each pair of nodes by examining whether they pro-
duce similar outcomes when compared with the other
nodes, thereby relating the ranking problem to a more
general ordering problem called seriation. SyncRank is
a hybrid method which ﬁrst solves a spectral problem
based on synchronization, embeds node positions on a
half-circle in the complex plane, and then chooses among
the circular permutations of those ranks by minimizing
the number of violations as in MVR.

Random Utility Models [23], such as the Bradley-
Terry-Luce (BTL) model [24, 25], are designed to in-
fer real-valued ranks from data on pairwise preferences.
These models assign a probability to the direction of an
edge conditioned on its existence, but they do not assign
a probability to the existence of an edge. They are ap-
propriate, for instance, when an experimenter presents
subjects with choices between pairs of items, and asks
them which they prefer.

Methods like David’s Score [26] and the Colley ma-
trix [27] compute rankings from proportions of wins and
losses. The latter, which was originally developed by
making mathematical adjustments to winning percent-
ages, is equivalent to a particular case of the general
method we introduce below. Elo score [28], Go Rank [29],
and TrueSkill [30] are also widely used win-loss methods,
but these schemes update the ranks after each match
rather than taking all previous interactions into account.
This specialization makes them useful when ranks evolve
over sequential matches, but less useful otherwise.

Finally, there are fully generative models such the
Probabilistic Niche Model of ecology [31–33], models of
friendship based on social status [9], and more generally
latent space models [34] which assign probabilities to the

2

existence and direction of edges based on real-valued po-
sitions in social space. However, inference of these models
tends to be diﬃcult, with many local optima. Our gen-
erative model can be viewed as a special case of these
models for which inference is especially easy.

In the absence of ground-truth rankings, we can
compare the accuracy of these methods using cross-
validation, computing the ranks using a subset of the
edges in the network and then using those ranks to pre-
dict the direction of the remaining edges. Equivalently,
we can ask them to predict unobserved edges, such as
which of two sports teams will win a game. However,
these methods do not all make the same kinds of pre-
dictions, requiring us to use diﬀerent kinds of cross-
validation. Methods such as BTL produce probabilis-
tic predictions about the direction of an edge, i.e., they
estimate the probability one item will be preferred to
another. Fully generative models also predict the proba-
bility that an edge exists, i.e., that a given pair of nodes
in the network interact. On the other hand, ordinal rank-
ing methods such as MVR do not make probabilistic pre-
dictions, but we can interpret their ranking as a coarse
prediction that an edge is more likely to point in one
direction than another.

The SpringRank model

We represent interactions between N entities as a
weighted directed network, where Aij is the number of
interactions i → j suggesting that i is ranked above j.
This allows both ordinal and cardinal input, including
where pairs interact multiple times. For instance, Aij
could be the number of ﬁghts between i and j that i has
won, or the number of times that j has endorsed i.

Given the adjacency matrix A, our goal is to ﬁnd a
ranking of the nodes. To do so, the SpringRank model
computes the optimal location of nodes in a hierarchy by
imagining the network as a physical system. Speciﬁcally,
each node i is embedded at a real-valued position or rank
si, and each directed edge i → j becomes an oriented
spring with a nonzero resting length and displacement
si − sj. Since we are free to rescale the latent space
and the energy scale, we set the spring constant and the
resting length to 1. Thus, the spring corresponding to an
edge i → j has energy

Hij =

(si − sj − 1)2 ,

(1)

1
2

which is minimized when si − sj = 1.

This version of the model has no tunable parameters.
Alternately, we could allow each edge to have its own
rest length or spring constant, based on the strength of
each edge. However, this would create a large number of
parameters, which we would have to infer from the data
or choose a priori. We do not explore this here.

According to this model, the optimal rankings of the
N ) which minimize

nodes are the ranks s∗ = (s∗

1, . . . , s∗

the total energy of the system given by the Hamiltonian

H(s) =

AijHij =

Aij (si − sj − 1)2 .

(2)

N
(cid:88)

i,j=1

1
2

(cid:88)

i,j

Since this Hamiltonian is convex in s, we can ﬁnd s∗ by
setting ∇H(s) = 0, yielding the linear system

(cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) s∗ = (cid:2)Dout − Din(cid:3) 1 ,

(3)

where 1 is the all-ones vector and Dout and Din are di-
agonal matrices whose entries are the weighted in- and
out-degrees, Dout
j Aji. See SI
Text S1 for detailed derivations.

j Aij and Din

ii = (cid:80)

ii = (cid:80)

The matrix on the left side of Eq. (3) is not invertible.
This is because H is translation-invariant:
it depends
only on the relative ranks si − sj, so that if s∗ = {si}
minimizes H(s) then so does {si + a} for any constant a.
One way to break this symmetry is to invert the matrix
in the subspace orthogonal to its nullspace by comput-
ing a Moore-Penrose pseudoinverse. If the network con-
sists of a single component, the nullspace is spanned by
the eigenvector 1, in which case this method ﬁnds the
s∗ where the average rank (1/N ) (cid:80)
i si = (1/N )s∗ · 1 is
zero. This is related to the random walk method of [15]:
if a random walk moves along each directed edge with
rate 1
2 − ε, then
s∗ is proportional to the perturbation to the stationary
distribution to ﬁrst order in ε.

2 + ε and against each one with rate 1

In practice, it is more eﬃcient and accurate to ﬁx the
rank of one of the nodes and solve the resulting equation
using a sparse iterative solver (see SI Text S1). Faster
still, because this matrix is a Laplacian, recent results [35,
36] allow us to solve Eq. (3) in nearly linear time in M ,
the number of non-zero edges in A.

Another way to break translation invariance is to in-
i aﬀecting each

troduce an “external ﬁeld” H0(si) = 1
2 αs2
node, so that the combined Hamiltonian is

Hα(s) = H(s) +

(4)

α
2

N
(cid:88)

i=1

s2
i .

The ﬁeld H0 corresponds to a spring that attracts every
node to the origin. We can think of this as imposing a
Gaussian prior on the ranks, or as a regularization term
that quadratically penalizes ranks with large absolute
values. This version of the model has a single tunable
parameter, namely the spring constant α. Since H(s)
scales with the total edge weight M = (cid:80)
i,j Aij while
H0(s) scales with N , for a ﬁxed value of α this regular-
ization becomes less relevant as networks become more
dense and the average (weighted) degree M/N increases.
For α > 0 there is a unique s∗ that minimizes Hα,

given by

(cid:2)Dout + Din − (cid:0)A + AT (cid:1) + αI(cid:3) s∗ = (cid:2)Dout − Din(cid:3) 1 ,

3

where I is the identity matrix. The matrix on the left
side is now invertible, since the eigenvector 1 has eigen-
In the limit α → 0, we recover
values α instead of 0.
Eq. (3); the value α = 2 corresponds to the Colley ma-
trix method [27].

Minimizing H(s), or the regularized version Hα(s),
corresponds to ﬁnding the “ground state” s∗ of the
model.
In the next section we show that this corre-
sponds to a maximum-likelihood estimate of the ranks
in a generative model. However, we can use SpringRank
not just to maximize the likelihood, but to compute a
joint distribution of the ranks as a Boltzmann distribu-
tion with Hamiltonian Eq. (4), and thus estimate the
uncertainty and correlations between the ranks. In par-
ticular, the ranks si are random variables following an
N -dimensional Gaussian distribution with mean s∗ and
covariance matrix (SI Text S4)

Σ =

(cid:2)Dout + Din − (cid:0)A + AT + αI(cid:1)(cid:3)−1

.

(6)

1
β

Here β is an inverse temperature controlling the amount
of noise in the model. In the limit β → ∞, the rankings
are sharply peaked around the ground state s∗, while
for β → 0 they are noisy. As we discuss below, we can
estimate β from the observed data in various ways.

The rankings given by SpringRank Eq. (3) and its reg-
ularized form Eq. (5) are easily and rapidly computed by
standard linear solvers.
In particular, iterative solvers
that take advantage of the sparsity of the system can
ﬁnd s∗ for networks with millions of nodes and edges
in seconds. However, as deﬁned above, SpringRank is
not a fully generative model that assigns probabilities to
the data and allows for Bayesian inference. In the next
section we introduce a generative model for hierarchical
networks and show that it converges to SpringRank in
the limit of strong hierarchy.

A generative model

In this section we propose a probabilistic genera-
tive model that takes as its input a set of node ranks
s1, . . . , sN and produces a weighted directed network.
The model also has a temperature or noise parameter
β and a density parameter c. Edges between each pair
of nodes i, j are generated independently of other pairs,
conditioned on the ranks. The expected number of edges
from i to j is proportional to the Boltzmann weight of
the corresponding term in the Hamiltonian Eq. (2),

E[Aij] = c exp(−βHij) = c exp

−

(si − sj − 1)2

,

(cid:21)

(cid:20)

β
2

where the actual edge weight Aij is drawn from a Poisson
distribution with this mean. The parameter c controls
the overall density of the network, giving an expected

(5)

4

number of edges

Predicting edge directions

E[M ] =

E[Aij] = c

exp

−

(si − sj − 1)2

,

(cid:88)

i,j

(cid:88)

i,j

(cid:20)

β
2

(cid:21)

while the inverse temperature β controls the extent to
which edges respect (or defy) the ranks s. For smaller
β, edges are more likely to violate the hierarchy or to
connect distant nodes, decreasing the correlation between
the ranks and the directions of the interactions: for β = 0
the model generates a directed Erd˝os-R´enyi graph, while
in the limit β → ∞ edges only exist between nodes i, j
with si − sj = 1, and only in the direction i → j.

The Poisson distribution may generate multiple edges
between a pair of nodes, so this model generates directed
multigraphs. This is consistent with the interpretation
that Aij is the number, or total weight, of edges from i
to j. However, in the limit as E[Aij] → 0, the Poisson
distribution approaches a Bernoulli distribution, gener-
ating binary networks with Aij ∈ {0, 1}.

The likelihood of observing a network A given ranks s,

inverse temperature β, and density c is

(cid:104)

ce− β

2 (si−sj −1)2(cid:105)Aij

P (A | s, β, c) =

(cid:89)

i,j

Aij!

(cid:104)

exp

−ce− β

2 (si−sj −1)2 (cid:105)

(7)
Taking logs, substituting the maximum-likelihood value
of c, and discarding constants that do not depend on s
or β yields a log-likelihood (see Supplemental Text S2)

If hierarchical structure plays an important role in a
system, it should allow us to predict the direction of pre-
viously unobserved interactions, such as the winner of an
upcoming match, or which direction social support will
ﬂow between two individuals. This is a kind of cross-
validation, which lets us test the statistical signiﬁcance
of hierarchical structure. It is also a principled way of
comparing the accuracy of various ranking methods for
datasets where no ground-truth ranks are known.

We formulate the edge prediction question as follows:
given a set of known interactions, and given that there is
an edge between i and j, in which direction does it point?
In one sense, any ranking method provides an answer to
this question, since we can predict the direction according
to which of i or j is ranked higher based on the known
interactions. When comparing SpringRank to methods
such as SyncRank, SerialRank, and MVR, we use these
“bitwise” predictions, and deﬁne the accuracy σb as the
fraction of edges whose direction is consistent with the
inferred ranking.

But we want to know the odds on each game, not just
the likely winner—that is, we want to estimate the prob-
ability that an edge goes in each direction. A priori,
.
a ranking algorithm does not provide these probabili-
ties unless we make further assumptions about how they
depend on the relative ranks. Such assumptions yield
generative models like the one deﬁned above, where the
conditional probability of an edge i → j is

L(A | s, β) = −βH(s) − M log

(cid:20) (cid:88)

2 (si−sj −1)2 (cid:21)

e− β

, (8)

i,j

Pij(β) =

e−βHij
e−βHij + e−βHji

=

1
1 + e−2β(si−sj )

.

(9)

where H(s) is the SpringRank energy deﬁned in Eq. (2).
In the limit of large β where the hierarchical structure
is strong, the ˆs that maximizes Eq. (8), approaches the
solution s∗ of Eq. (3) that minimizes H(s). Thus the
maximum likelihood estimate ˆs of the rankings in this
model approaches the SpringRank ground state.

As discussed above, we can break translational sym-
metry by adding a ﬁeld H0 that attracts the ranks to
the origin. This is is equivalent to imposing a prior
P (s) ∝ (cid:81)N
. The maximum a posteriori
estimate ˆs then approaches the ground state s∗ of the
Hamiltonian in Eq. (4), given by Eq. (5).

i=1 e− αβ

2 (si−1)2

This model belongs to a larger family of genera-
tive models considered in ecology and network the-
ory [9, 31, 32], and more generally the class of latent space
models [34], where an edge points from i to j with proba-
bility f (si − sj) for some function f . These models typi-
cally have complicated posterior distributions with many
local optima, requiring Monte Carlo methods (e.g. [33])
In our
that do not scale eﬃciently to large networks.
case, f (si − sj) is a Gaussian centered at 1, and the pos-
terior converges to the multivariate Gaussian Eq. (6) in
the limit of strong structure.

The density parameter c aﬀects the probability that an
edge exists, but not its direction. Thus our probabilistic
prediction method has a single tunable parameter β.

Note that Pij is a logistic curve, is monotonic in the
rank diﬀerence si − sj, and has width determined by the
inverse temperature β. SpringRank has this in common
with two other ranking methods: setting γi = e2βsi re-
covers the Bradley-Terry-Luce model [24, 25] for which
Pij = γi/(γi + γj), and setting k = 2β recovers the
probability that i beats j in the Go rank [29], where
Pij = 1/(1 + e−k(si−sj )). However, SpringRank diﬀers
from these methods in how it infers the ranks from ob-
served interactions, so SpringRank and BTL make diﬀer-
ent probabilistic predictions.

In our experiments below, we test various ranking
methods for edge prediction by giving them access to
80% of the edges in the network (the training data) and
then asking them to predict the direction of the remain-
ing edges (the test data). We consider two measures of
accuracy: σa is the average probability assigned to the
correct direction of an edge, and σL is the log-likelihood
of generating the directed edges given their existence. For
simple directed graphs where Aij +Aji ∈ {0, 1}, these are

σa =

AijPij

and σL =

Aij log Pij .

(10)

(cid:88)

i,j

(cid:88)

i,j

In the multigraph case, we ask how well Pij approximates
the fraction of interactions between i and j that point
from i to j [see Eqs. (12) and (13)]. For a discussion of
other performance measures, see Supplemental Text S9.
We perform our probabilistic prediction experiments
as follows. Given the training data, we infer the ranks
using Eq. (5). We then choose the temperature parame-
ter β by maximizing either σa or σL on the training data
while holding the ranks ﬁxed. The resulting values of β,
which we denote ˆβa and ˆβL respectively, are generally
distinct (Supplemental Table S2 and Text S7). This is
intuitive, since a single severe mistake where Aij = 1 but
Pij ≈ 0 reduces the likelihood by a large amount, while
only reducing the accuracy by one edge. As a result,
predictions using ˆβa produce fewer incorrectly oriented
edges, achieving a higher σa on the test set, while predic-
tions using ˆβL will produce fewer dramatically incorrect
predictions where Pij is very low, and thus achieve higher
σL on the test set.

Statistical signiﬁcance using the ground state energy

We can measure statistical signiﬁcance using any test
statistic, by asking whether its value on a given dataset
would be highly improbable in a null model. One such
statistic is the accuracy of edge prediction using a method
such as the one described above. However, this may
become computationally expensive for cross-validation
studies with many replicates, since each fold of each
replicate requires inference of the parameter ˆβa. Here
we propose a test statistic which is very easy to com-
pute, inspired by the physical model behind SpringRank:
namely, the ground state energy. For the unregularized
version Eq. (2), the energy per edge is (see SI Text S3)

H(s∗)
M

=

1
2M

(cid:88)

i

(din

i − dout

i

) s∗

i +

(11)

1
2

.

Since the ground state energy depends on many aspects
of the network structure, and since hierarchical structure
is statistically signiﬁcant if it helps us predict edge direc-
tions, like [37] we focus on the following null model: we
randomize the direction of each edge while preserving the
total number ¯Aij = Aij + Aji of edges between each pair
of vertices. If the real network has a ground state energy
which is much lower than typical networks drawn from
this null model, we can conclude that the hierarchical
structure is statistically signiﬁcant.

This test correctly concludes that directed Erd˝os-R´enyi
graphs have no signiﬁcant structure. It also ﬁnds no sig-
niﬁcant structure for networks created using the genera-
tive model Eq. (7) with β = 0.1, i.e., when the tempera-
ture or noise level 1/β is suﬃciently large the ranks are no

5

longer relevant to edge existence or direction (Fig. S2).
However, we see in the next section that it shows sta-
tistically signiﬁcant hierarchy for a variety of real-world
datasets, showing that H(s∗) is both useful and compu-
tationally eﬃcient as a test statistic.

FIG. 1. Performance on synthetic data. (A) A synthetic
network of N = 100 nodes, with ranks drawn from a standard
Gaussian and edges drawn via the generative model Eq. (7)
for two diﬀerent values of β and average degree 5. Blue edges
point down the hierarchy and red edges point up, indicated
by arrows. (B) The accuracy of the inferred ordering deﬁned
as the Spearman correlation averaged over 100 indendepently
generated networks; error bars indicate one standard devia-
tion. (C, D) Identical to A and B but with ranks drawn from
a mixture of three Gaussians so that the nodes cluster into
three tiers (Materials and Methods). See Fig. S1 for perfor-
mance curves for Pearson correlation r.

Results on real and synthetic data

Having introduced SpringRank, an eﬃcient procedure
for inferring real-valued ranks, a corresponding gener-
ative model, a method for edge prediction, and a test
for the statistical signiﬁcance of hierarchical structure,
we now demonstrate it by applying it to both real and
synthetic data. For synthetic datasets where the ground-
truth ranks are known, our goal is to see to what extent
SpringRank and other algorithms can recover the actual
ranks. For real-world datasets, in most cases we have
no ground-truth ranking, so we apply the statistical sig-
niﬁcance test deﬁned above, and compare the ability of
SpringRank and other algorithms to predict edge direc-

tions given a subset of the interactions.

We compare SpringRank to other widely used meth-
ods: the spectral methods PageRank [13], Eigenvector
Centrality [12] and Rank Centrality [14]; Minimum Vi-
olation Ranking (MVR) [16, 17], SerialRank [19] and
SyncRank [20], which produce ordinal rankings; David’s
score [26]; and the BTL random utility model [24, 25]
using the algorithm proposed in [38], which like our
generative model makes probabilistic predictions. We
also compare unregularized SpringRank with the regu-
larized version α = 2, corresponding to the Colley ma-
trix method [27]. Unfortunately, Eigenvector Centrality,
Rank Centrality, David’s score, and BTL are undeﬁned
when the network is not strongly connected, e.g. when
there are nodes with zero in- or out-degree. In such cases
we follow the common regularization procedure of adding
low-weight edges between every pair of nodes (see Sup-
plemental Text S10).

Performance for synthetic networks

We study two types of synthetic networks, generated
by the model described above. Of course, since the log-
likelihood in this model corresponds to the SpringRank
energy in the limit of large β, we expect SpringRank to
do well on these networks, and its performance should be
viewed largely as a consistency check. But by varying the
distribution of ranks and the noise level, we can illustrate
types of structure that may exist in real-world data, and
test each algorithm’s ability to identify them.

In the ﬁrst type, the ranks are normally distributed
with mean zero and variance one (Fig. 1A). In the second
type, the ranks are drawn from an equal mixture of three
Gaussians with diﬀerent means and variances, so that
nodes cluster into high, middle, and low tiers (Fig. 1C).
This second type is intended to focus on the importance
of real-valued ranks, and to measure the performance of
algorithms that (implicitly or explicitly) impose strong
priors on the ranks when the data defy their expectations.
In both cases, we vary the amount of noise by changing
β while keeping the total number of edges constant (see
Materials and Methods).

Since we wish to compare SpringRank both to meth-
ods such as MVR that only produce ordinal rankings,
and to those like PageRank and David’s Score that pro-
duce real-valued ranks, we measure the accuracy of each
algorithm according to the Spearman correlation ρ be-
tween its inferred rank order and the true one. Results
for the Pearson correlation, where we measure the algo-
rithms’ ability to infer the real-valued ranks as opposed
to just their ordering, are shown in Fig. S1.

We ﬁnd that all the algorithms do well on the ﬁrst
type of synthetic network. As β increases so that the net-
work becomes more structured, with fewer edges (shown
in red in Fig. 1A) pointing in the “wrong” direction,
all algorithms infer ranks that are more correlated with
the ground truth. SpringRank and SyncRank have the

6

FIG. 2. Ranking the History faculty hiring network [3].
(A) Linear hierarchy diagram with nodes embedded at their
inferred SpringRank scores. Blue edges point down the hier-
archy and red edges point up. (B) Histogram of the empirical
distribution of ranks, with a vertical axis of ranks matched to
panel A. (C) Histogram of ground-state energies from 10, 000
randomizations of the network according to the null model
where edge directions are random; the dashed red line shows
the ground state energy of the empirical network depicted in
panels A and B. The fact that the ground state energy is so
far below the tail of the null model is overwhelming evidence
that the hierarchical structure is statistically signiﬁcant, with
a p-value < 10−4

.

highest accuracy, followed closely by the Colley matrix
method and BTL (Fig. 1B). Presumably the Colley ma-
trix works well here because the ranks are in fact drawn
from a Gaussian prior, as it implicitly assumes.

Results for the second type of network are more nu-
anced. The accuracy of SpringRank and SyncRank in-
creases rapidly with β with exact recovery around β = 1.
Inter-
SerialRank also performs quite well on average.

7

FIG. 3. Edge prediction accuracy over BTL. Distribution of diﬀerences in performance of edge prediction of SpringRank
compared to BTL on real and synthetic networks deﬁned as (A) edge-prediction accuracy σa Eq. (12) and (B) the conditional
log-likelihood σL Eq. (13). Error bars indicate quartiles and markers show medians, corresponding to 50 independent trials of
5-fold cross-validation, for a total of 250 test sets for each network. The two synthetic networks are generated with N = 100,
average degree 5, and Gaussian-distributed ranks as in Fig. 1A, with inverse temperatures β = 1 and β = 5. For each
experiment shown, the fractions of trials in which each method performed equal to or better than BTL are shown in Table I.
These diﬀerences correspond to prediction of an additional 1 to 12 more correct edge directions, on average.

estingly, the other methods do not improve as β in-
creases, and many of them decrease beyond a certain
point (Fig. 1D). This suggests that these algorithms be-
come confused when the nodes are clustered into tiers,
even when the noise is small enough that most edges have
directions consistent with the hierarchy.
SpringRank
takes advantage of the fact that edges are more likely
between nodes in the same tier (Fig. 1C), so the mere
existence of edges helps it cluster the ranks.

These synthetic tests suggest that real-valued ranks
capture information that ordinal ranks do not, and that
many ranking methods perform poorly when there are
substructures in the data such as tiered groups. Of
course, in most real-world scenarios, the ground-truth
ranks are not known, and thus edge prediction and other
forms of cross-validation should be used instead. We turn
to edge prediction in the next section.

Performance for real-world networks

As discussed above, in most real-world networks, we
have no ground truth for the ranks. Thus we focus on
our ability to predict edge directions from a subset of
the data, and measure the statistical signiﬁcance of the
inferred hierarchy.

We apply our methods to datasets from a diverse set
of ﬁelds, with sizes ranging up to N = 415 nodes and
up to 7000 edges (see Table S2): three North Ameri-
can academic hiring networks where Aij is the number
of faculty at university j who received their doctorate
from university i, for History (illustrated in Figs. 2A and
B), Business, and Computer Science departments [3]; two

networks of animal dominance among captive monk para-
keets [5] and one among Asian elephants [37] where Aij
is the number of dominating acts by animal i toward an-
imal j; and social support networks from two villages
in Tamil Nadu referred to (for privacy reasons) by the
pseudonyms “Tenpat.t.i” and “Alak¯apuram,” where Aij
is the number of distinct social relationships (up to ﬁve)
through which person i supports person j [2]; and 53 net-
works of NCAA Women’s and Men’s college basketball
matches during the regular season, spanning 1985-2017
(Men) and 1998-2017 (Women), where Aij = 1 if team i
beat team j. Each year’s network comprises a diﬀerent
number of matches, ranging from 747 to 1079 [39].

Together, these examples cover prestige, dominance,
and social hierarchies. In each of these domains, infer-
ring ranks from interactions is key to further analysis.
Prestige hierarchies play an unequivocal role in the dy-
namics of academic labor markets [40]; in behavioral ecol-
ogy, higher-ranked individuals in dominance hierarchies
are believed to have higher ﬁtness [1, 41]; and patterns
of aggression are believed to reveal animal strategies and
cognitive capacities [4–8]. Finally, in social support net-
works, higher ranked individuals have greater social capi-
tal and reputational standing [42, 43], particularly in set-
tings in which social support is a primary way to express
and gain respect and prestige [44].

We ﬁrst applied our ground state energy test for the
presence of statistically signiﬁcant hierarchy, rejecting
the null hypothesis with p < 10−4 in almost all cases
(e.g., for History faculty hiring, see Fig. 2C). The one ex-
ception is the Asian Elephants network for which p > 0.4.
This corroborates the original study of this network [37],
which found that counting triad motifs shows no signif-

8

FIG. 4. Probabilistic edge prediction accuracy σa of
SpringRank vs. BTL. For 50 independent trials of 5-fold
cross-validation (250 total folds per network), the values of σa
for SpringRank and BTL are shown on the vertical axis and
the horizontal axis respectively. Points above the diagonal,
shown in blue, are trials where SpringRank is more accurate
than BTL. The fractions for which each method is superior
are shown in plot legends, matching Table I.

icant hierarchy [45]. This is despite the fact that one
can ﬁnd an appealing ordering of the elephants using the
Minimum Violation Rank method, with just a few vio-
lating edges (SI Fig. S9). Thus the hierarchy found by
MVR may well be illusory.

As described above, we performed edge prediction ex-
periments using 5-fold cross-validation, where 80% of the
edges are available to the algorithm as training data, and
a test set consisting of 20% of the edges is held out (see
Materials and Methods). To test SpringRank’s ability to
make probabilistic predictions, we compare it to BTL.

We found that SpringRank outperforms BTL, both in
terms of the accuracy σa (Fig. 3A) and, for most net-
works, the log-likelihood σL (Fig. 3B). The accuracy of
both methods has a fairly broad distribution over the tri-
als of cross-validation, since in each network some subsets
of the edges are harder to predict than others when they
are held out. However, as shown in Fig. 4, in most tri-
als SpringRank was more accurate than BTL. Fig. 3A
and Table I show that SpringRank predicts edge direc-
tions more accurately in the majority of trials of cross-
validation for all nine real-world networks, where this
majority ranges from 62% for the parakeet networks to
100% for the Computer Science hiring network.

Table I shows that SpringRank also obtained a higher
log-likelihood σL than BTL for 6 of the 9 real-world net-
works. Regularizing SpringRank with α = 2 does not ap-
pear to signiﬁcantly improve either measure of accuracy
(Fig. 3). We did not attempt to tune the regularization
parameter α.

To compare SpringRank with methods that do not
make probabilistic predictions, including those that pro-

FIG. 5. Bitwise prediction accuracy σb of SpringRank
vs. SyncRank. For 50 independent trials of 5-fold cross-
validation (250 total folds per NCAA season), the fraction
of correctly predicted game outcomes σb for SpringRank and
Syncrank are shown on the vertical axis and the horizontal
axis respectively. Points above the equal performance line,
shown in blue, are trials where SpringRank is more accurate
than SyncRank; the fractions for which each method is supe-
rior are shown in plot legends.

duce ordinal rankings, we measured the accuracy σb of
“bitwise” predictions, i.e., the fraction of edges consis-
tent with the infered ranking. We found that spectral
methods perform poorly here, as does SerialRank.
In-
terestingly, BTL does better on the NCAA networks in
terms of bitwise prediction than it does for probabilistic
predictions, suggesting that it is better at rank-ordering
teams than determining their real-valued position.

We found that SyncRank is the strongest of the or-
dinal methods, matching SpringRank’s accuracy on the
parakeet and business school networks, but SpringRank
outperforms SyncRank on the social support and NCAA
networks (see Fig. S4). We show a trial-by-trial compari-
son of SpringRank and SyncRank in Fig. 5, showing that
in most trials of cross-validation SpringRank makes more
accurate predictions for the NCAA networks.

To check whether our results were dependent on the
choice of holding out 20% of the data, we repeated our
experiments using 2-fold cross-validation, i.e., using 50%
of network edges as training data and trying to predict
the other 50%. We show these results in Fig. S5. While
all algorithms are less accurate in this setting, the com-
parison between algorithms is similar to that for 5-fold
cross-validation.

Finally, the real-valued ranks found by SpringRank
shed light on the organization and assembly of real-world
networks (see Figs. S6, S7, S8, S12, and S13). For exam-
ple, we found that ranks in the faculty hiring networks
have a long tail at the top, suggesting that the most pres-
tigious universities are more separated from those below
them than an ordinal ranking would reveal. In contrast,
ranks in the social support networks have a long tail
at the bottom, suggesting a set of people who do not
have suﬃcient social status to provide support to oth-
ers. SpringRank’s ability to ﬁnd real-valued ranks makes
these distributions amenable to statistical analysis, and

9

% trials higher σa vs BTL % trials higher σL vs BTL
Type SpringRank +regularization SpringRank +regularization

Dataset
Comp. Sci. [3]
Alak¯apuram [2]
Synthetic β = 5
History [3]
NCAA Women (1998-2017) [39]
Tenpat.t.i [2]
Synthetic β = 1
NCAA Men (1985-2017) [39]

Faculty Hiring
Social Support
Synthetic
Faculty Hiring
Basketball
Social Support
Synthetic
Basketball
Parakeet G1 [5] Animal Dominance
Faculty Hiring
Parakeet G2 [5] Animal Dominance

Business [3]

100.0
99.2†
98.4
97.6†
94.4†
88.8
83.2
76.0†
71.2†
66.8†
62.0

97.2
99.6
63.2
96.8
87.0
93.6
65.2
62.3
56.8
59.2
51.6

100.0
100.0
76.4
98.8
69.1
100.0
98.4
68.5
41.2
39.2
47.6

99.6
100.0
46.4
98.8
51.0
100.0
98.4
52.4
37.2
36.8
47.2

TABLE I. Edge prediction with BTL as a benchmark. During 50 independent trials of 5-fold cross-validation (250 total
folds per network), columns show the the percentages of instances in which SpringRank Eq. (3) and regularized SpringRank
Eq. (5) with α = 2 produced probabilistic predictions with equal or higher accuracy than BTL. Distributions of accuracy
improvements are shown in Fig. 3. Center columns show accuracy σa and right columns show σL (Materials and Methods).
Italics indicate where BTL outperformed SpringRank for more than 50% of tests. † Dagger symbols indicate tests that are
shown in detail in Fig. 4. NCAA Basketball datasets were analyzed one year at a time

.

we suggest this as a direction for future work.

Conclusions

SpringRank is a mathematically principled, physics-
inspired model for hierarchical structure in networks
It yields a simple and highly
of directed interactions.
scalable algorithm, requiring only sparse linear algebra,
which enables analysis of networks with millions of nodes
and edges in seconds. Its ground state energy provides
a natural test statistic for the statistical signiﬁcance of
hierarchical structure.

While the basic SpringRank algorithm is nonparamet-
ric, a parameterized regularization term can be included
as well, corresponding to a Gaussian prior. While reg-
ularization is often required for BTL, Eigenvector Cen-
trality, and other commonly used methods (Supplemen-
tal Text S10) it is not necessary for SpringRank and our
tests indicate that its eﬀects are mixed.

We also presented a generative model that allows one
to create synthetic networks with tunable levels of hierar-
chy and noise, whose posterior coincides with SpringRank
in the limit where the eﬀect of the hierarchy is strong.
By tuning a single temperature parameter, we can use
this model to make probabilistic predictions of edge di-
rections, generalizing from observed to unobserved inter-
actions. Therefore, after conﬁrming its ability to infer
ranks in synthetic networks where ground truth ranks
are known, we measured SpringRank’s ability to to pre-
dict edge directions in real networks. We found that
in networks of faculty hiring, animal interactions, social
support, and NCAA basketball, SpringRank often makes
better probabilistic predictions of edge predictions than
the popular Bradley-Terry-Luce model, and performs as
well or better than SyncRank and a variety of other meth-
ods that produce ordinal rankings.

SpringRank is based on springs with quadratic poten-
tials, but other potentials may be of interest. For in-
stance, to make the system more tolerant to outliers while
remaining convex, one might consider a piecewise poten-
tial that is quadratic for small displacements and linear
otherwise. We leave this investigation of alternative po-
tentials to future work.

Given its simplicity, speed, and high performance, we
believe that SpringRank will be useful in a wide vari-
ety of ﬁelds where hierarchical structure appears due to
dominance, social status, or prestige.

Materials and methods

Synthetic network generation

Networks were generated in three steps. First, node
ranks splanted were drawn from a chosen distribution. For
Test 1, N = 100 ranks were drawn from a standard nor-
mal distribution, while for Test 2, 34 ranks were drawn
from each of three Gaussians, N (−4, 2), N (0, 1
2 ), and
N (4, 1) for a total of N = 102. Second, an average de-
gree (cid:104)k(cid:105) and a value of the inverse temperature β were
chosen. Third, edges were drawn generated to Eq. (7)
with c = (cid:104)k(cid:105)N/ (cid:80)
i,j exp [−(β/2)(si − sj − 1)2] so that
the expected mean degree is (cid:104)k(cid:105) (see SI Text S6).

This procedure resulted in directed networks with the
desired hierarchical structure, mean degree, and noise
level. Tests were conducted for (cid:104)k(cid:105) ∈ [5, 15], β ∈ [0.1, 5],
and all performance plots show mean and standard devi-
ations for 100 replicates.

Performance measures for edge prediction

For multigraphs, we deﬁne the accuracy of probabilis-
tic edge prediction as the extent to which Pij is a good

estimate of the fraction of interactions between i and j
that point from i to j, given that there are any edges to
predict at all, i.e., assuming ¯Aij = Aij + Aji > 0. If this
prediction were perfect, we would have Aij = ¯AijPij. We
deﬁne σA as 1 minus the sum of the absolute values of
the diﬀerence between Aij and this estimate,

σa = 1 −

1
2M

(cid:88)

i,j

(cid:12)
(cid:12)Aij − ¯Aij Pij

(cid:12)
(cid:12) ,

(12)

where M is the number of directed edges in the subset
of the network under consideration, e.g., the training or
test set. Then σa = 1 if Pij = Aij/ ¯Aij for all i, j, and
σa = 0 if for each i, j all the edges go from i to j (say)
but Pij = 0 and Pji = 1.

To measure accuracy via the conditional log-likelihood,
we ask with what probability we would get the directed
network A from the undirected network ¯A if each edge
between i and j points from i → j with probability Pij
and from j → i with probability Pji = 1−Pij. This gives

σL = log Pr[A | ¯A]
(cid:88)

=

log

(cid:19)

(cid:18)Aij + Aji
Aij

i,j

+ log

(cid:104)

P Aij
ij

[1 − Pij]Aji(cid:105)

,

(13)

y

where (cid:0)x
(cid:1) is the binomial coeﬃcient. We disregard the
ﬁrst term of this sum since it does not depend on P .
If we wish to compare networks of diﬀerent sizes as in
Fig. 3, we can normalize σL by the number of edges.
For an extensive discussion of performance metrics see
Supplementary Text S9.

Statistical signiﬁcance of ranks

We compute a standard left-tailed p-value for the sta-
tistical signiﬁcance of the ranks s∗ by comparing the
ground state energy Eq. (11) of the real network A with
the null distribution of ground state energies of an ensem-
ble of networks ˜A drawn from the null model where ¯Aij is

10

kept ﬁxed, but the direction of each edge is randomized.

p-value = Pr[H(s∗; A) ≤ H(˜s∗; ˜A)] .

(14)

In practice, this p-value is estimated by drawing many
samples from the null distribution by randomizing the
edge directions of A to produce ˜A, computing the ranks
˜s∗ from Eq. (3), and then computing the ground state
energy Eq. (11) of each.

Cross-validation tests

We performed edge prediction using 5-fold cross-
validation. In each realization, we divide the interacting
pairs i, j, i.e., those with nonzero ¯Aij = Aij + Aji, into
ﬁve equal groups. We use four of these groups as a train-
ing set, inferring the ranks and setting β to maximize σa
or σL (on the left and right of Fig. 3 respectively). We
then use the ﬁfth group as a test set, asking the algorithm
for Pij for each pair i, j in that group, and report σa or
σL on that test set. By varying which group we use as the
test set, we get 5 trials per realization: for instance, 50
realizations give us 250 trials of cross-validation. Results
for 2-fold cross-validation are reported in SI.

Acknowledgements

CDB and CM were supported by the John Tem-
pleton Foundation. CM was also supported by the
Army Research Oﬃce under grant W911NF-12-R-0012.
DBL was supported by NSF award SMA-1633747 and
the Santa Fe Institute Omidyar Fellowship. We thank
Aaron Clauset and Johan Ugander for helpful com-
ments.
Competing Interests: The authors declare
that they have no competing interests. All authors
derived the model, analyzed results, and wrote the
manuscript. CDB wrote Python implementations and
DBL wrote MATLAB implementations. Open-source
code in Python, MATLAB, and SAS/IML available at
https://github.com/cdebacco/SpringRank.

[1] C. Drews, The concept and deﬁnition of dominance in

animal behaviour, Behaviour 125, 283 (1993).

[2] E. A. Power, Social support networks and religiosity in
rural South India, Nature Human Behaviour 1, 0057
(2017).

[3] A. Clauset, S. Arbesman, D. B. Larremore, Systematic
inequality and hierarchy in faculty hiring networks, Sci-
ence Advances 1, e1400005 (2015).

[4] S. D. Cˆot´e, M. Festa-Bianchet, Reproductive success in
female mountain goats: the inﬂuence of age and social
rank, Animal Behaviour 62, 173 (2001).

[5] E. A. Hobson, S. DeDeo, Social feedback and the emer-
gence of rank in animal society, PLoS Computational Bi-
ology 11, e1004411 (2015).

[6] C. J. Dey, J. S. Quinn, Individual attributes and self-
organizational processes aﬀect dominance network struc-
ture in pukeko, Behavioral Ecology 25, 1402 (2014).
[7] C. J. Dey, A. R. Reddon, C. M. O’Connor, S. Balshine,
Network structure is related to social conﬂict in a cooper-
atively breeding ﬁsh, Animal Behaviour 85, 395 (2013).
[8] M. A. Cant, J. B. Llop, J. Field, Individual variation
in social aggression and the probability of inheritance:
theory and a ﬁeld test, The American Naturalist 167,
837 (2006).

[9] B. Ball, M. E. Newman, Friendship networks and social

status, Network Science 1, 16 (2013).

[10] S. Szymanski, The economic design of sporting contests,

Journal of Economic Literature 41, 1137 (2003).

11

[33] A. Z. Jacobs, J. A. Dunne, C. Moore, A. Clauset, Un-
tangling the roles of parasites in food webs with gener-
ative network models, arXiv preprint arXiv:1505.04741
(2015).

[34] P. D. Hoﬀ, A. E. Raftery, M. S. Handcock, Latent space
approaches to social network analysis, Journal of the
American Statistical Association 97, 1090 (2001).
[35] D. A. Spielman, S.-H. Teng, Nearly linear time algo-
rithms for preconditioning and solving symmetric, diago-
nally dominant linear systems, SIAM Journal on Matrix
Analysis and Applications 35, 835 (2014).

[36] I. Koutis, G. L. Miller, R. Peng, A nearly-m log n time
solver for SDD linear systems, Proc. 52nd Foundations
of Computer Science (FOCS) (IEEE Presss, 2011), pp.
590–598.

[37] S. de Silva, V. Schmid, G. Wittemyer, Fission–fusion pro-
cesses weaken dominance networks of female asian ele-
phants in a productive habitat, Behavioral Ecology 28,
243 (2016).

[38] D. R. Hunter, MM algorithms for generalized Bradley-

Terry models, Annals of Statistics pp. 384–406 (2004).

[39] NCAA, http://www.ncaa.org/championships/statistics

(2018).

[40] S. F. Way, D. B. Larremore, A. Clauset, Gender, pro-
ductivity, and prestige in computer science faculty hir-
ing networks, Proc. 25th Intl Conf on World Wide Web
(2016), pp. 1169–1179.

[41] B. Majolo, J. Lehmann, A. de Bortoli Vizioli, G. Schino,
Fitness-related beneﬁts of dominance in primates, Amer-
ican Journal of Physical Anthropology 147, 652 (2012).
[42] N. Lin, Social Capital: A Theory of Social Structure and
Action, vol. 19 (Cambridge University Press, 2002).
[43] K. S. Cook, M. Levi, R. Hardin, Whom can we trust?:
How groups, networks, and institutions make trust possi-
ble (Russell Sage Foundation, 2009).

[44] M. Mines, Public faces, private lives: Community and in-
dividuality in South India (University of California Press,
1994).

[45] D. Shizuka, D. B. McDonald, A social network perspec-
tive on measurements of dominance hierarchies, Animal
Behaviour 83, 925 (2012).

[46] L. Rosasco, E. D. Vito, A. Caponnetto, M. Piana,
A. Verri, Are loss functions all the same?, Neural Com-
putation 16, 1063 (2004).

[11] R. Baumann, V. A. Matheson, C. A. Howe, Anomalies
in tournament design: the madness of march madness,
Journal of Quantitative Analysis in Sports 6 (2010).
[12] P. Bonacich, Power and centrality: A family of measures,

American Journal of Sociology 92, 1170 (1987).

[13] L. Page, S. Brin, R. Motwani, T. Winograd, The PageR-
ank citation ranking: Bringing order to the web., Tech.
rep., Stanford InfoLab (1999).

[14] S. Negahban, S. Oh, D. Shah, Rank centrality: Ranking
from pairwise comparisons, Operations Research (2016).
[15] T. Callaghan, P. J. Mucha, M. A. Porter, Random walker
ranking for NCAA division IA football, American Math-
ematical Monthly 114, 761 (2007).

[16] I. Ali, W. D. Cook, M. Kress, On the minimum violations
ranking of a tournament, Management Science 32, 660
(1986).

[17] P. Slater, Inconsistencies in a schedule of paired compar-

isons, Biometrika 48, 303 (1961).

[18] M. Gupte, P. Shankar, J. Li, S. Muthukrishnan, L. Iftode,
Finding hierarchy in directed online social networks,
Proc. 20th Intl. Conf. on the World Wide Web (ACM,
2011), pp. 557–566.

[19] F. Fogel, A. d’Aspremont, M. Vojnovic, Serialrank: Spec-
tral ranking using seriation, Advances in Neural Informa-
tion Processing Systems (2014), pp. 900–908.

[20] M. Cucuringu, Sync-rank: Robust ranking, constrained
ranking and rank aggregation via eigenvector and sdp
synchronization, IEEE Transactions on Network Science
and Engineering 3, 58 (2016).

[21] E. Letizia, P. Barucca, F. Lillo, Resolution of ranking
hierarchies in directed networks, PloS one 13, e0191604
(2018).

[22] N. Tatti, Tiers for peers: a practical algorithm for discov-
ering hierarchy in weighted networks, Data Mining and
Knowledge Discovery 31, 702 (2017).

[23] K. E. Train, Discrete Choice Methods with Simulation

(Cambridge University Press, 2009).

[24] R. A. Bradley, M. E. Terry, Rank analysis of incom-
plete block designs: I. the method of paired comparisons,
Biometrika 39, 324 (1952).

[25] R. D. Luce, On the possible psychophysical laws., Psy-

chological Review 66, 81 (1959).

[26] H. A. David, Ranking from unbalanced paired-

comparison data, Biometrika 74, 432 (1987).

[27] W. N. Colley, Colley’s bias free college football rank-
ing method: The Colley matrix explained (2002).
Http://www.colleyrankings.com/matrate.pdf.

[28] A. E. Elo, The Rating of Chessplayers, Past and Present

(Arco Pub., 1978).

[29] R. Coulom, Whole-history rating: A Bayesian rating sys-
tem for players of time-varying strength, International
Conference on Computers and Games (Springer, 2008),
pp. 113–124.

[30] R. Herbrich, T. Minka, T. Graepel, Trueskill: a Bayesian
skill rating system, Advances in Neural Information Pro-
cessing Systems (2007), pp. 569–576.

[31] R. J. Williams, A. Anandanadesan, D. Purves, The prob-
abilistic niche model reveals the niche structure and role
of body size in a complex food web, PLoS ONE 5, e12092
(2010).

[32] R. J. Williams, D. W. Purves, The probabilistic niche
model reveals substantial variation in the niche structure
of empirical food webs, Ecology 92, 1849 (2011).

Supporting Information (SI)

S1. Deriving the linear system minimizing the Hamiltonian

The SpringRank Hamiltonian Eq. (2) is convex in s and we set its gradient ∇H(s) = 0 to obtain the global

minimum:

Let the weighted out-degree and in-degree be dout
written as

i = (cid:80)

j Aij and din

i = (cid:80)

j Aji, respectively. Then Eq. (S1) can be

∂H
∂si

(cid:88)

=

j

[Aij (si − sj − 1) − Aji (sj − si − 1)] = 0 .

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i − din
i

(cid:1) −

[Aij + Aji] sj = 0 .

(cid:88)

j

We now write the system of N equations together by introducing the following matrix notation. Let Dout =
diag(dout
N ) be diagonal matrices, let 1 be the N -dimensional vector of all ones.
Then Eq. (S2) becomes

N ) and Din = diag(din

1 , . . . , din

, . . . , dout

1

(cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) s = (cid:2)Dout − Din(cid:3) 1 .

This is a linear system of the type B s = b, where B = (cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3) and b = (cid:2)Dout − Din(cid:3) 1. The rank
of B is at most N − 1 and more generally, if the network represented by A consists of C disconnected components, B
will have rank N − C. In fact, B has an eigenvalue 0 with multiplicity C, and the eigenvector 1 is in the nullspace.
B is not invertible, but we can only invert in the N − C-dimensional subspace orthogonal to the nullspace of B. The
family of translation-invariant solutions s∗ is therefore deﬁned by

s∗ = (cid:2)Dout + Din − (cid:0)A + AT (cid:1)(cid:3)−1 (cid:2)Dout − Din(cid:3) 1 ,

in which the notation [·]−1 should be taken as the Moore-Penrose pseudoinverse.

In practice, rather than constructing the pseudo-inverse, it will be more computationally eﬃcient (and for large
systems, more accurate) to solve the linear system in an iterative fashion. Since we know that solutions may be
translated up or down by an arbitrary constant, the system can be made full-rank by ﬁxing the position of an
arbitrary node 0. Without loss of generality, let sN = 0. In this case, terms that involve sN can be dropped from
Eq. (S2), yielding

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i − din
i

(cid:1) −

[Aij + Aji] sj = 0 ,

i (cid:54)= N

− (cid:0)dout

N − din
N

(cid:1) −

[AN j + AjN ] sj = 0 .

N −1
(cid:88)

j=1

N −1
(cid:88)

j=1

N −1
(cid:88)

j=1

Adding Eq. (S5) to Eq. (S6) yields

(cid:0)dout

i + din
i

(cid:1) si − (cid:0)dout

i + dout

N − din

i − din
N

(cid:1) −

[Aij + AN j + Aji + AjN ] sj = 0 ,

(S7)

which can be written in matrix notation as

(cid:105)
(cid:104)
Dout + Din − ˚A

s = (cid:2)Dout − Din(cid:3) 1 + (cid:0)dout

N − din
N

(cid:1) 1 ,

where

˚Aij = Aij + AN j + Aji + AjN .

In this formulation, Eq. (S8) can be solved to arbitrary precision using iterative methods that take advantage of the
sparsity of ˚A. The resulting solution may then be translated by an arbitrary amount as desired.

12

(S1)

(S2)

(S3)

(S4)

(S5)

(S6)

(S8)

(S9)

13

(S12)

(S13)

S2. Poisson generative model

The expected number of edges from node i to node j is c exp

and therefore the likelihood of

(cid:104)

2 (si − sj − 1)2(cid:105)
− β

observing a network A, given parameters β, s, and c is

(cid:104)

c e− β

2 (si−sj −1)2(cid:105)Aij

P (A | s, β, c) =

(cid:89)

i,j

Aij!

(cid:104)
exp

−c e− β

2 (si−sj −1)2(cid:105)

.

(S10)

Taking logs yields

log P (A | s, β, c) =

Aij log c −

Aij (si − sj − 1)2 − log [Aij!] − ce− β

2 (si−sj −1)2

.

(S11)

(cid:88)

i,j

β
2

Discarding the constant term log [Aij!], and recognizing the appearance of the SpringRank Hamiltonian H(s), yields

L(A | s, β, c) = −βH(s) +

Aij log c −

ce− β

2 (si−sj −1)2

.

(cid:88)

i,j

(cid:88)

i,j

Taking ∂L/∂c and setting it equal to zero yields

ˆc =

(cid:80)

i,j Aij
2 (si−sj −1)2 ,

(cid:80)

i,j e− β

which has the straightforward interpretation of being the ratio between the number of observed edges and the expected
number of edges created in the generative process for c = 1. Substituting in this solution and letting M = (cid:80)
i,j Aij
yields

L(A | s, β) = −βH(s) + M log ˆc −

ˆc e− β

2 (si−sj −1)2

(cid:88)

i,j





(cid:88)

i,j

M
β





(cid:88)

i,j





 .

= −βH(s) + M log M − M log

e− β

2 (si−sj −1)2

 − M .

(S14)

The terms M log M and M may be neglected since they do not depend on the parameters, and we divide by β,
yielding a log-likelihood of

L(A | s, β) = −H(s) −

log

e− β

2 (si−sj −1)2

(S15)

(cid:3) where (cid:104)·(cid:105)E denotes
Note that the SpringRank Hamiltonian may be rewritten as H(s) = M (cid:2) 1
the average over elements in the edge set E. In other words, H(s) scales with M and the square of the average spring
length. This substitution for H(s) allows us to analyze the behavior of the log-likelihood

2 (cid:104)Aij(si − sj − 1)2(cid:105)E

L(A | s, β) = −M

(cid:68)

Aij (si − sj − 1)2(cid:69)

+

log

1
β

E

e− β

2 (si−sj −1)2





(cid:88)

i,j










.






1
2

(S16)

Inside the logarithm there are N 2 terms of ﬁnite value, so that the logarithm term is of order O( log N
β ). Thus, for well-
resolved hierarchies, i.e. when β is large enough that the sampled edges consistently agree with the score diﬀerence
between nodes, the maximum likelihood ranks ˆs approach the ranks s∗ found by minimizing the Hamiltonian. In
practice, exactly maximizing the likelihood would require extensive computation, e.g. by using local search heuristic
or Markov chain Monte Carlo sampling.

S3. Rewriting the energy

The Hamiltonian Eq. (2) can be rewritten as

2 H(s) =

Aij (si − sj − 1)2

N
(cid:88)

i,j=1

N
(cid:88)

i,j=1

N
(cid:88)

=

=

N
(cid:88)

=

i

Aij

(cid:0)s2

i + s2

j − 2sisj + 1 − 2si + 2sj

(cid:1)

N
(cid:88)

s2
i

Aij +

N
(cid:88)

N
(cid:88)

s2
j

j

i=1

Aij − 2

(cid:88)

(cid:88)

si

Aijsj + M

i

j

i

j=1

−2

N
(cid:88)

N
(cid:88)

si

i=1

j=1

Aij + 2

N
(cid:88)

N
(cid:88)

sj

j=1

i=1

Aij

s2
i

(cid:0)dout

i + din
i

(cid:1) − 2

si

(cid:0)dout

i − din
i

(cid:1) + M − 2

(cid:88)

(cid:88)

si

Aijsj .

(cid:88)

i

i

j

From Eq. (S2) we have

(cid:88)

s2
i

(cid:0)dout

i + din
i

(cid:1) −

(cid:88)

si

(cid:0)dout

i − din
i

(cid:1) =

(cid:88)

(cid:88)

si

[Aij + Aji] sj .

j

i

i

j

We can substitute this into Eq. (S17)
(cid:88)

(cid:88)

2H(s) =

si

[Aij + Aji] sj −

si

(cid:0)dout

i − din
i

(cid:1) + M − 2

(cid:88)

(cid:88)

si

Aijsj

(cid:88)

i

i

j

j

si

(cid:0)din

i − dout

i

(cid:1) + M

hisi + M ,

=

=

i
(cid:88)

i
(cid:88)

i

where hi ≡ din

i − dout

i

.

S4. Ranks distributed as a multivariate Gaussian distribution

Assuming that the ranks are random variables distributed as a multivariate Gaussian distribution of average ¯s and

covariance matrix Σ, we have:

We can obtain this formulation by considering a Boltzman distribution with the Hamiltonian Eq. (2) as the energy
term and inverse temperature β so that

P (s) ∝ exp

−

(s − ¯s)(cid:124)Σ−1(s − ¯s)

.

(cid:18)

1
2

(cid:19)



P (s) ∝ exp

−

Aij (si − sj − 1)2

 .



N
(cid:88)

i,j=1

β
2

1
2

Manipulating the exponent of Eq. (S20) yields

(s − ¯s)T Σ−1(s − ¯s) =

(cid:0)sT Σ−1s − 2sT Σ−1¯s + ¯sT Σ−1¯s(cid:1) ,

1
2

whereas the parallel manipulation of Eq. (S21) yields

β
2

N
(cid:88)

i,j=1

β
2

Aij (si − sj − 1)2 =

(cid:2)sT (cid:0)Dout + Din − AT − A(cid:1) s + 2 sT (Din − Dout)1 + M (cid:3) ,

(S23)

14

(S17)

(S18)

(S19)

(S20)

(S21)

(S22)

where 1 is a vector of ones and Din are diagonal matrices whose entries are the in- and out-degrees, Dout
and Din
on s because irrelevant when accounting for normalization, we obtain:

j Aij
i,j Aij. Comparing these last two expressions and removing terms that do not depend

j Aji and M = (cid:80)

ii = (cid:80)

ii = (cid:80)

Σ = 1
β

(cid:0)Dout + Din − AT − A(cid:1)−1

and ¯s = β Σ (cid:0)Dout − Din(cid:1) 1 = s∗ .

(S24)

S5. Bayesian SpringRank

Adopting a Bayesian approach with a factorized Gaussian prior for the ranks, we obtain that the s that maximizes
the posterior distribution is the one that minimizes the regularized SpringRank Hamiltonian Eq. (4), i.e. the s that
solves the linear system Eq. (5). In fact, deﬁning:

P (s) = Z −1(β, α)

e−β α

2 (si−1)2

= Z −1(β, α)

e−β αH0(si) ,

(S25)

(cid:89)

i∈V

is a normalization constant that depends on α and β, and following the same steps as

where Z(β, α) =
before we get:

(cid:105)N/2

(cid:104) 2π
β α

log P (s | A) =

log P (Aij | s) − βα

H0(si) + log (Z (β, α) )

(cid:89)

i∈V

(cid:88)

i,j

(cid:34)

(cid:88)

i∈V

(cid:35)

(cid:88)

i∈V

= − β

H(s) + α

H0(si)

+ C ,

(S26)

where C is a constant that does not depend on the parameters, and thus may be ignored when maximizing log P (s | A).

The parameter c included in the generative model (7) controls for network’s sparsity. We can indeed ﬁx it so to

obtain a network with a desired expected number of edges (cid:104)M (cid:105) as follows:

For a given vector of ranks s and inverse temperature β, the c realizing the desired sparsity will then be:

S6. Fixing c to control for sparsity

(cid:104)M (cid:105) ≡

(cid:104)Aij(cid:105) = c

e− β

2 (si−sj −1)2

.

(cid:88)

i,j

(cid:88)

i,j

c =

(cid:80)

i,j e− β

(cid:104)M (cid:105)
2 (si−sj −1)2 =

(cid:104)k(cid:105)N
2 (si−sj −1)2 ,

(cid:80)

i,j e− β

where (cid:104)k(cid:105) is the expected node degree (cid:104)k(cid:105) = (cid:80)N
i=1
model with Bernoulli distribution.

(cid:2)din

i + dout

i

(cid:3). Similar arguments apply when considering a generative

S7. Comparing optimal β for predicting edge directions

In the main text, (12) and Eq. (13) deﬁne the accuracy of edge prediction, in terms of the number of edges predicted
correctly in each direction and the log-likelihood conditioned on the undirected graph. Here we compute the optimal
values of β for both notions of accuracy. In both computations that follow, the following two facts will be used:

and

ij(β) = 2 (si − sj) e−2β(si−sj ) P 2
P (cid:48)

ij(β) ,

1 =

Pij(β)
1 − Pij(β)

e−2β(si−sj ) .

15

(S27)

(S28)

(S29)

A. Choosing β to optimize edge direction accuracy

We take the derivative of Eq. (12) with respect to β, set it equal to zero, and solve as follows.

0 ≡

∂σa(β)
∂β

=

∂
∂β



1 −

1
2m

(cid:88)

i,j

|Aij − (Aij + Aji) Pij(β)|

 .

(S30)



In preparation to take the derivatives above, note that P (cid:48)
takes one sign, the (j, i) term takes the opposite sign,

ij(β) = −P (cid:48)

ji(β) and that whenever the (i, j) term of σa(β)

Without loss of generality, assume that the (i, j) term is positive and the (j, i) term is negative. This implies that

Aij − (Aij + Aji) Pij(β) = − [Aji − (Aij + Aji) Pji(β)] .

∂
∂β

∂
∂β

|Aij − (Aij + Aji) Pij(β)| = − (Aij + Aji) P (cid:48)

ij(β) ,

|Aji − (Aij + Aji) Pji(β)| = − (Aij + Aji) P (cid:48)

ij(β) .

and

∂
∂β

In other words, the derivatives of the (i, j) and (j, i) terms are identical, and the sign of both depends on whether the
quantity [Aij − (Aij + Aji)Pij(β)] is positive or negative. We can make this more precise by directly including the
sign of the (i, j) term, and by using Eq. (S28), to ﬁnd that

|Aij − (Aij + Aji) Pij(β)| = −2 (Aij + Aji) (si − sj) e−2β(si−sj ) P 2

ij × sign(cid:8)Aij − (Aij + Aji) Pij(β)(cid:9) .

(S34)

Expanding P 2

ij and reorganizing yields

∂
∂β

|Aij − (Aij + Aji) Pij(β)| = −2

(Aij + Aji) (si − sj)
2 cosh [2β (si − sj)] + 2

× sign(cid:8)Aij − (Aij + Aji) Pij(β)(cid:9) .

(S35)

Combining terms (i, j) and (j, i), the optimal inverse temperature for local accuracy ˆβa is that which satisﬁes

N
(cid:88)

0 =

(i,j)∈U (E)

(Aij + Aji) (si − sj)
(cid:105)
2 ˆβa (si − sj)

(cid:104)

cosh

+ 1

× sign(cid:8)Aij − (Aij + Aji) Pij( ˆβa)(cid:9) ,

(S36)

which may be found using standard root-ﬁnding methods.

B. Choosing β to optimize the conditional log likelihood

We take the derivative of Eq. (13) with respect to β, set it equal to zero, and partially solve as follows.

0 ≡

∂σL(β)
∂β

=

∂
∂β





(cid:88)

log

i,j

(cid:19)

(cid:18)Aij + Aji
Aij

+ log

(cid:104)

Pij(β)Aij [1 − Pij(β)]Aji(cid:105)



 .

Combining the (i, j) and (j, i) terms, we get
(cid:18)Aij + Aji
Aij

∂
∂β

0 ≡

(cid:88)

log

(cid:19)

(i,j)∈U (E)

+ log

(cid:19)

(cid:18)Aij + Aji
Aji

(cid:20) Aij
Pij(β)

−

Aji
1 − Pij(β)

(cid:21) ∂Pij(β)
∂β

(i,j)∈U (E)

(cid:88)

(cid:88)

=

=

(i,j)∈U (E)

2 (si − sj) [Aij − (Aij + Aji) Pij(β)]

Pij(β)
1 − Pij(β)

e−2β(si−sj ) .

+ [ Aij log Pij(β) + Aji log [1 − Pij(β)] ]

16

(S31)

(S32)

(S33)

(S37)

(S38)

Applying both Eq. (S28) and Eq. (S29), the optimal inverse temperature for the conditional log likelihood ˆβL is that
which satisﬁes

0 =

(cid:88)

2 (si − sj)

(cid:104)

(cid:105)
Aij − (Aij + Aji) Pij( ˆβL)

,

(i,j)∈U (E)

which, like Eq. (S36) may be found using standard root-ﬁnding methods. Comparing equations Eq. (S36) and
Eq. (S39), we can see that the values of β that maximize the two measures may, in general, be diﬀerent. Table S2
shows for optimal values for ˆβL and ˆβa for various real-world datasets.

17

(S39)

S8. Bitwise accuracy σb

Some methods provide rankings but do not provide a model to estimate Pij, meaning that Eq. (12) and Eq. (13)
cannot be used. Nevertheless, such methods still estimate one bit of information about each pair (i, j): whether the
majority of the edges are from i to j or vice versa. This motivates the use of a bitwise version of σa, which we call
σb,

σb = 1 −

Θ (si − sj) Θ(Aji − Aij) ,

(S40)

1
N 2 − t

(cid:88)

i,j

where Θ(x) = 1 if x > 0 and Θ(x) = 0 otherwise, and N is the number of nodes and t is the number of instances in
which Aij = Aji; there are N 2 − t total bits to predict. Results in terms of this measure on the networks considered
in the main text are shown in Figure S4. In the special case that the network is unweighted (A is a binary adjacency
matrix) and there are no bi-directional edges (if Aij = 1, then Aji = 0), then 1 − σb is the fraction of edges that
violate the rankings in s. In other words, for this particular type of network, 1 − σb is the minimum violations rank
penalty normalized by the total number of edges in the network, i.e., 1
M

i,j Θ(si − sj) Aji.

(cid:80)

S9. Performance metrics

When evaluating the performance of a ranking algorithm in general one could consider a variety of diﬀerent measures.
One possibility is to focus on the ranks themselves, rather than the outcomes of pairwise interactions, and calculate
correlation coeﬃcients as in Fig. 1; this is a valid strategy when using synthetic data thanks to the presence of ground
truth ranks, but can only assess the performance with respect of the speciﬁc generative process used to generate
the pairwise comparisons, as we point out in the main text. This strategy can also be applied for comparisons with
observed real world ranks, as we did in Table S11 and it has been done for instance in [19, 20] to compare the ranks
with those observed in real data in sports. However, the observed ranks might have been derived from a diﬀerent
process than the one implied by the ranking algorithm considered. For instance, in the faculty hiring networks, popular
ranking methods proposed by domain experts for evaluating the prestige of universities do not consider interactions
between institutions, but instead rely on a combination of performance indicators such as ﬁrst-year student retention
or graduation rates. The correlation between observed and inferred ranks should thus be treated as a qualitative
indicator of how well the two capture similar features of the system, such as prestige, but should not be used to
evaluate the performance of a ranking algorithm.

Alternatively, one can look at the outcomes of the pairwise comparisons and relate them to the rankings of the
nodes involved as in Eqs. (12) and (13) for testing prediction performance. A popular metric of this type is the number
of violations (also called upsets), i.e., outcomes where a higher ranked node is defeated by a lower ranked one. This
is very similar to the bitwise accuracy deﬁned in (S40), indeed when there are no ties and two nodes are compared
only once, then they are equivalent. These can be seen as low-resolution or coarse-grained measures of performance:
for each comparison predict a winner, but do not distinguish between cases where the winner is easy to predict and
cases where there is almost a tie. In particular, an upset between two nodes ranked nearby counts as much as an
upset between two nodes that are far away in the ranking. The latter case signals a much less likely scenario. In order
to distinguish these two situations, one can penalize each upset by the nodes’ rank diﬀerence elevated to a certain
power d. This is what the agony function does [18] with the exponent d treated as a parameter to tune based on the
application. When d = 0 we recover the standard number of unweighted upsets.

Note that optimization of agony is often used as a non-parametric approach to detect hierarchies [21], in particular
for ordinal ranks. For ordinal ranks, rank diﬀerences are integer-valued and equal to one for adjacent-ranked nodes,
yet for real-valued scores this is not the case. Therefore the result of the agony minimization problem can vary

18

widely between ordinal and real valued ranking algorithms. (We note that the SpringRank objective function, i.e.,
the Hamiltonian in Eq. (2), can be considered a kind of agony. However, since we assume that nearby pairs are more
likely to interact, it is large for a edge from i to j if i is ranked far above or far below j, and more speciﬁcally whenever
si is far from sj + 1.)

In contrast to the coarse prediction above—which competitor is more likely to win?—we require, when possible,
more precise predictions in Eqs. (12) and (13), which ask how much more likely is one competitor to win? This,
however, requires the ranking algorithm to provide an estimate of Pij, the probability that i wins over j, which is
provided only by BTL and SpringRank; all other methods compared in this study provide orderings or embeddings
without probabilistic predictions.

The conditional log-likelihood σL as deﬁned in Eq. (13) can be seen as a Log Loss often used as a classiﬁcation loss
function [46] in statistical learning. This type of function heavily penalizes ranking algorithms that are very conﬁdent
about an incorrect outcome, e.g. when the predicted Pij is close to 1, i very likely to win over j, but the observed
outcome is that j wins over i. For this reason, this metric is more sensitive to outliers, as when in sports a very
strong team loses against one at the bottom of the league. The accuracy σb deﬁned in Eq. (12) focuses instead in
predicting the correct proportions of wins/losses between two nodes that are matched in several comparisons. This
is less sensitive to outliers, and in fact if Pij is close but not exactly equal to 1, for a large number of comparisons
between i and j, we would expect that j should indeed win few times, e.g. if Pij = 0.99 and i, j are compared 100
times, σa is maximized when i wins 99 times and j wins once.

S10. Parameters used for regularizing ranking methods

When comparing SpringRank to other methods, we need to deal with the fact that certain network structures cause
other methods to fail to return any output. Eigenvector Centrality cannot, for example, be applied to directed trees,
yet this is precisely the sort of structure that one might expect when hierarchy becomes extreme.

More generally, many spectral techniques fail on networks that are not strongly connected, i.e., where it is not the
case that one can reach any node from any other by moving along a path consistent with the edge directions, since
in that case the adjacency matrix is not irreducible and the Perron-Frobenius theorem does not apply. In particular,
nodes with zero out-degree—sometimes called “dangling nodes” in the literature [13]—cause issues for many spectral
methods since the adjacency matrix annihilates any vector supported on such nodes. In contrast, the SpringRank
optimum given by Eq. (3) is unique up to translation whenever the network is connected in the undirected sense, i.e.,
whenever we can reach any node from any other by moving with or against directed edges.

A diﬀerent issue occurs in the case of SyncRank. When edges are reciprocal in the sense that an equal number
of edges point in each direction, they eﬀectively cancel out. That is, if Aij = Aji, the corresponding entries in the
SyncRank comparison matrix will be zero, Cij = Cji = 0, as if i and j were never compared at all. As a result, there
can be nodes i such that Cij = Cji = 0 for all j. While rare, these pathological cases exist in real data and during
cross-validation tests, causing the output of SyncRank to be undeﬁned.

In all these cases, regularization is required. Our regularized implementations of ﬁve ranking methods are described

below:

• Regularized Bradley-Terry-Luce (BTL). If there exist dangling nodes, the Minimization-Maximization
algorithm to ﬁt the BTL model to real data proposed in [38] requires a regularization. In this case we set the
total number of out-edges dout
i = 10−6 for nodes that would have di = 0 otherwise. This corresponds to Wi in
Eq.(3) of [38].

• Regularized PageRank. If there exist dangling nodes, we add an edge of weight 1/N from each dangling
node to every other node in the network. For each dataset we tried three diﬀerent values of the teleportation
parameter, α ∈ {0.4, 0.6, 0.8}, and reported the best results of these three.

• Regularized Rank Centrality. If there exist dangling nodes, we use the regularized version of the algorithm

presented in Eq. (5) of [14] with (cid:15) = 1.

• Regularized SyncRank. If there are nodes whose entries in the comparison matrix C are zero, we add a

small constant (cid:15) = 0.001 to the entries of H in Eq. (13) of Ref. [20], so that D is invertible.

• Regularized Eigenvector Centrality. If the network is not strongly connected, we add a weight of 1/N to

every entry in A and then diagonalize.

19

S11. Supplemental Tables

Comp. Sci. SpringRank MVR US News NRC Eig. C. PageRank
0.57
SpringRank
0.48
MVR
0.41
US News
0.41
NRC
0.74
Eig. C.
-
PageRank

0.80 0.72
0.81 0.73
- 0.73
0.73
-
0.69 0.68
0.41 0.41

0.84
0.80
0.69
0.68
-
0.74

0.96
-
0.81
0.73
0.80
0.48

-
0.96
0.80
0.72
0.84
0.57

Business
SpringRank
MVR
US News
NRC
Eig. C.
PageRank

History
SpringRank
MVR
US News
NRC
Eig. C.
PageRank

SpringRank MVR US News NRC Eig. C. PageRank
0.75
0.74
0.69
0.72
0.60
-
-
-
0.72
0.68
-
0.60

0.98
-
0.72
-
0.92
0.69

0.92
0.92
0.68
-
-
0.72

-
0.98
0.74
-
0.92
0.75

-
-
-
-
-
-

SpringRank MVR US News NRC Eig. C. PageRank
0.69
0.57
0.51
0.44
0.88
-

0.86 0.66
0.86 0.65
- 0.66
0.66
-
0.72 0.59
0.51 0.44

0.86
0.77
0.72
0.59
-
0.88

0.95
-
0.86
0.65
0.77
0.57

-
0.95
0.86
0.66
0.86
0.69

TABLE S1. Pearson correlation coeﬃcients between various rankings of faculty hiring networks. All coeﬃcients are statistically
signiﬁcant (p < 10−9). SpringRank is most highly correlated with Minimum Violations Ranks across all three faculty hiring
networks. Among US News and NRC rankings, SpringRank is more similar to US News. Values for US News and NRC were
[3] for comparison to the ranks available at the same time that the faculty hiring data were collected. The
drawn from Ref.
NRC does not rank business departments.

N M H/M Acc. σa ˆβL

ˆβa

Type
DataSet
Anim. Dom. 21 838 0.174 0.930
Parakeet G1 [5]
Anim. Dom. 19 961 0.193 0.932
Parakeet G2 [5]
0.078 0.923
Asian Elephants [37] Anim. Dom. 20 23
112 7353 0.251 0.881
Business [3]
Fac. Hiring
205 4033 0.220 0.882
Computer Science [3] Fac. Hiring
Fac. Hiring
History [3]
144 3921 0.186 0.909
Soc. Support 415 2497 0.222 0.867
Alak¯apuram [2]
Soc. Support 361 1809 0.241 0.858
Tenpat.t.i [2]

0.008 (0.089)
2.70 6.03 76 (9.1%) / 42
0.011 (0.139)
2.78 18.12 75 (7.8%) / 36
2.33 3.44 2 (8.7%) / 0
0.001 (0.040)
2.04 3.14 1171 (15.9%) / 808 0.019 (0.119)
2.23 8.74 516 (12.8%) / 255 0.013 (0.105)
2.39 5.74 397 (10.1%) / 227 0.012 (0.119)
1.98 7.95 347 (13.9%) / 120 0.011 (0.079)
1.89 8.20 262 (14.5%) / 120 0.012 (0.082)

Viol. (%) / Bound Wt. viol. (per viol.) Depth p-value
2.604 < 10−4
1.879 < 10−4
3.000 0.4466
2.125 < 10−4
2.423 < 10−4
2.234 < 10−4
3.618 < 10−4
3.749 < 10−4

TABLE S2. Statistics for SpringRank applied to real-world networks. Column details are as follows: N is the number
of nodes; M is the number of edges; H/m is the ground state energy per edge; Accuracy σa refers to accuracy in 5-fold
cross-validation tests using temperature ˆβa; ˆβL and ˆβa are temperatures optimizing edge prediction accuracies σL and σa
respectively; Violations refers to the number of edges that violate the direction of the hierarchy as a number, as a percentage
of all edges, with a lower bound provided for reference, computed as the number of unavoidable violations due to reciprocated
edges; Weighted violations are the sum of each violation weighted by the diﬀerence in ranks between the oﬀending nodes; Depth
is smax − smin; p-value refers to the null model described in the Materials and Methods. Relevant performance statistics for
NCAA datasets (53 networks) are reported elsewhere; see Fig. S3.

S12. Supplemental Figures

20

FIG. S1. Performance (Pearson correlation) on synthetic data. Tests were performed as in Fig. 1, but here performance
is measured using Pearson correlation. This favors algorithms like SpringRank and BTL, that produce real-valued ranks, over
ordinal ranking schemes like Minimum Violation Ranking which are not expected to recover latent positions.
(A) Linear
hierarchy diagrams show latent ranks splanted of 100 nodes, drawn from a standard normal distribution, with edges drawn
via the generative model Eq. (7) for indicated β (noise) values. Blue edges point down the hierarchy and red edges point
up, indicated by arrows. (B) Mean accuracies ± one standard deviation (symbols ± shading) are measured as the Pearson
correlation between method output and splanted for 100 replicates. (C, D) Identical to A and B but for hierarchies of N = 102
nodes divided into three tiers. All plots have mean degree 5; see Fig. 1 for performance curves for Spearman correlation r. See
Materials and Methods for synthetic network generation.

(a) US HS

(b) US BS

(c) US CS

(d) Tenpat.t.i

(e) Alak¯apuram

(f) parakeet G1

(g) parakeet G2

(h) planted β = 5.0

(i) planted β = 0.1

(j) Asian elephant

(k) ER
N = 100, (cid:104)k(cid:105) = 3

FIG. S2. Statistical signiﬁcance testing using the null model distribution of energies. Results are 1000 realizations
of the null model where edge directions are randomized while keeping the total number of interactions between each pair ﬁxed,
for real and synthetic networks: a-c) US History (HS), Business (BS) and Computer Science (CS) faculty hiring networks [3];
d-e) social support networks of two Indian villages [2] considering 5 types of interactions (see main manuscript); f,g) aggression
network of parakeet Group 1 and 2 (as in [5]); h,i) planted network using SpringRank generative model with N = 100 and
mean degree (cid:104)k(cid:105) = 5, Gaussian prior for the ranks with average µ = 0.5 and variance 1 (α = 1/β) and two noise levels β = 5.0
and β = 0.1; j) dominance network of asian elephants [37]; k) Erd˝os-R´enyi directed random network with N = 100 and (cid:104)k(cid:105) = 3.
The vertical line is the energy obtained on the real network. In all but the last two cases we reject the null hypothesis that
edge directions are independent of the ranks, and conclude that the hierarchy is statistically signiﬁcant.

21

FIG. S3. Edge prediction accuracy over BTL for NCAA basketball datasets. Distribution of diﬀerences in performance
of edge prediction of SpringRank compared to BTL on NCAA College Basketball regular season matches for (top) Women and
(middle) Men, deﬁned as (left) the probabilistic edge-prediction accuracy σa Eq. (12) and (right) the conditional log-likelihood
σL Eq. (13). Error bars indicate quartiles and markers show medians, corresponding to 50 independent trials of 5-fold cross-
validation, for a total of 250 test sets for each dataset. The bottom plot is obtained by considering the distributions over all
the seasons together. In terms of number of correctly predicted outcomes, SpringRank correctly predicts on average 8 to 16
more outcomes than BTL for each of the 20 Women NCAA seasons and up to 12 more outcomes for each of the 33 Men NCAA
seasons; for the latter dataset, BTL has an average better prediction in 3 out of the 33 seasons. The number of matches played
per season in the test set varies from the past to the most recents years from 747 to 1079.

22

FIG. S4. Bitwise edge direction prediction. Symbols show medians of bitwise edge prediction accuracies Eq. (S40) over
50 realization of 5-fold cross-validation (for a total of 250 trials) compared with the median accuracy for SyncRank; error bars
indicate quartiles. Thus, points above the dashed line at zero indicate better predictions than SyncRank, while values below
indicate that SyncRank performed better.

23

FIG. S5. Edge prediction accuracy with 2-fold cross-validation. Top: the accuracy of probabilistic edge prediction
of SpringRank compared to the median accuracy of BTL on real and synthetic networks deﬁned as (top left) edge-prediction
accuracy σa Eq. (12) and (top right) the conditional log-likelihood σL Eq. (13); (bottom) bitwise edge prediction accuracies σb
Eq. (S40) of SpringRank and other algorithms compared with the median accuracy of SyncRank. Error bars indicate quartiles
and markers show medians, corresponding to 50 independent trials of 2-fold cross-validation, for a total of 100 test sets for
each network. The two synthetic networks are generated with N = 100, average degree 5, and Gaussian-distributed ranks as
in Fig. 1A, with inverse temperatures β = 1 and β = 5. Notice that these results are similar those of Fig. 3, obtained using
5-fold cross-validation.

Computer Science

24

FIG. S6. Summary of SpringRank applied to Computer Science faculty hiring network [3]. (top-left) A linear
hierarchy diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and
red edges point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned)
and the horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency
matrix; blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of
statistical signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized
samples: the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted,
ordered by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means
algorithm.

History

25

FIG. S7. Summary of SpringRank applied to History faculty hiring network [3]. (top-left) A linear hierarchy diagram
showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up.
(top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal
axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red
dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance
test with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted
line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank,
from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Business

26

FIG. S8. Summary of SpringRank applied to Business faculty hiring network [3]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Asian Elephants

27

FIG. S9. Summary of SpringRank applied to Asian Elephants network [37]. (top-left) A linear hierarchy diagram
showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up.
(top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal
axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red
dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance
test with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted
line is the ground state energy obtained on the observed real network.

Parakeet G1

28

FIG. S10. Summary of SpringRank applied to Parakeet G1 network [5]. (top-left) A linear hierarchy diagram showing
inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up. (top-
middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal axis
is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red dots
represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance test
with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted line
is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank, from
top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Parakeet G2

29

FIG. S11. Summary of SpringRank applied to Parakeet G2 network [5]. (top-left) A linear hierarchy diagram showing
inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges point up. (top-
middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the horizontal axis
is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix; blue and red dots
represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical signiﬁcance test
with randomized edge directions. The histogram represents the energies obtained in the randomized samples: the dotted line
is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered by rank, from
top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Tenpat.t.i

30

FIG. S12. Summary of SpringRank applied to Tenpat.t.i social support network [2]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

Alak¯apuram

31

FIG. S13. Summary of SpringRank applied to Alak¯apuram social support network [2]. (top-left) A linear hierarchy
diagram showing inferred SpringRank scores. Circles correspond to nodes; blue edges point down the hierarchy and red edges
point up. (top-middle) A histogram shows the empirical distribution of ranks: the vertical axis is the rank si (binned) and the
horizontal axis is the count of nodes having a rank in that bin. (top-right) A sparsity plot of rank-ordered adjacency matrix;
blue and red dots represent non-zero entries going down and up the hierarchy, respectively. (middle-right) Results of statistical
signiﬁcance test with randomized edge directions. The histogram represents the energies obtained in the randomized samples:
the dotted line is the ground state energy obtained on the observed real network. (bottom) Nodes’ ranks are plotted, ordered
by rank, from top rank (left) to bottom rank (right), and shaded by tier. The tiers are calculated by the k-means algorithm.

