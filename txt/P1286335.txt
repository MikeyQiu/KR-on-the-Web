Lipschitz Continuity in Model-based Reinforcement Learning

Kavosh Asadi * 1 Dipendra Misra * 2 Michael L. Littman 1

8
1
0
2
 
l
u
J
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
3
9
1
7
0
.
4
0
8
1
:
v
i
X
r
a

Abstract
We examine the impact of learning Lipschitz
continuous models in the context of model-based
reinforcement learning. We provide a novel bound
on multi-step prediction error of Lipschitz models
where we quantify the error using the Wasserstein
metric. We go on to prove an error bound for
the value-function estimate arising from Lipschitz
models and show that the estimated value function
is itself Lipschitz. We conclude with empirical
results that show the beneﬁts of controlling the
Lipschitz constant of neural-network models.

1. Introduction

The model-based approach to reinforcement learning (RL)
focuses on predicting the dynamics of the environment
to plan and make high-quality decisions (Kaelbling et al.,
1996; Sutton & Barto, 1998). Although the behavior of
model-based algorithms in tabular environments is well
understood and can be effective (Sutton & Barto, 1998),
scaling up to the approximate setting can cause instabilities.
Even small model errors can be magniﬁed by the planning
process resulting in poor performance (Talvitie, 2014).

In this paper, we study model-based RL through the lens of
Lipschitz continuity, intuitively related to the smoothness
of a function. We show that the ability of a model to make
accurate multi-step predictions is related to the model’s
one-step accuracy, but also to the magnitude of the Lipschitz
constant (smoothness) of the model. We further show that
the dependence on the Lipschitz constant carries over to the
value-prediction problem, ultimately inﬂuencing the quality
of the policy found by planning.

We consider a setting with continuous state spaces and
stochastic transitions where we quantify the distance
between distributions using the Wasserstein metric. We

*Equal contribution

1Department of Computer Science,
Brown University, Providence, USA 2Department of Computer
Science and Cornell Tech, Cornell University, New York, USA.
Correspondence to: Kavosh Asadi <kavosh@brown.edu>.

introduce a novel characterization of models, referred
to as a Lipschitz model class, that represents stochastic
dynamics using a set of component deterministic functions.
This allows us to study any stochastic dynamic using
the Lipschitz continuity of its component deterministic
functions. To learn a Lipschitz model class in continuous
state spaces, we provide an Expectation-Maximization
algorithm (Dempster et al., 1977).

the learned models or

One promising direction for mitigating the effects of
inaccurate models is the idea of limiting the complexity
of
reducing the horizon of
planning (Jiang et al., 2015). Doing so can sometimes
make models more useful, much as regularization in
supervised learning can improve generalization performance
(Tibshirani, 1996). In this work, we also examine a type
of regularization that comes from controlling the Lipschitz
constant of models. This regularization technique can be
applied efﬁciently, as we will show, when we represent the
transition model by neural networks.

2. Background

,

A

, R, T, γ

We consider the Markov decision process (MDP) setting
in which the RL problem is formulated by the tuple
we mean a continuous state
. Here, by
(cid:105)
(cid:104)S
we mean a discrete action set. The functions
space and by
A
R and T :
) denote the reward
Pr(
R : S
S
S × A →
and transition dynamics. Finally, γ
[0, 1) is the discount
∈
rate. If
= 1, the setting is called a Markov reward
process (MRP).

|A|

→

×

A

S

2.1. Lipschitz Continuity

Our analyses leverage the “smoothness” of various
functions, quantiﬁed as follows.

Deﬁnition 1. Given two metric spaces (M1, d1) and
(M2, d2) consisting of a space and a distance metric, a
M2 is Lipschitz continuous (sometimes
function f : M1 (cid:55)→
simply Lipschitz) if the Lipschitz constant, deﬁned as

Kd1,d2(f ) :=

sup
M1,s2

s1

∈

d2

f (s1), f (s2)
d1(s1, s2)

,

(cid:1)

M1

∈

(cid:0)

(1)

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

is ﬁnite.

Lipschitz Continuity in Model-based Reinforcement Learning

Sometimes referred to as “Earth Mover’s distance”,
Wasserstein is the minimum expected distance between
pairs of points where the joint distribution j is constrained
to match the marginals µ1 and µ2. New applications of
this metric are discovered in machine learning, namely in
the context of generative adversarial networks (Arjovsky
et al., 2017) and value distributions in reinforcement
learning (Bellemare et al., 2017).

Wasserstein is linked to Lipschitz continuity using duality:

W (µ1, µ2) =

sup

f (s)µ1(s)

f (s)µ2(s)

ds .

f :Kd,dR (f )

1 (cid:90)

≤

(cid:0)

−

(cid:1)

(4)

This equivalence, known as Kantorovich-Rubinstein duality
(Villani, 2008), lets us compute Wasserstein by maximizing
R, a relatively
over a Lipschitz set of functions f :
In our theory, we utilize both
easier problem to solve.
deﬁnitions, namely the primal deﬁnition (3) and the dual
deﬁnition (4).

S (cid:55)→

3. Lipschitz Model Class

We introduce a novel representation of stochastic MDP
transitions in terms of a distribution over a set of
deterministic components.
Deﬁnition 4. Given a metric state space (
action space
Fg =
a

) and an
, we deﬁne Fg as a collection of functions:
a) where

f :
{
. We say that Fg is a Lipschitz model class if
∈ A

distributed according to g(f

A
S (cid:55)→ S}

, d
S

S

|

KF := sup
Fg

f

∈

KdS ,dS (f ) ,

Our deﬁnition captures a subset of stochastic transitions,
namely ones that can be represented as a state-independent
distribution over deterministic transitions. An example is
provided in Figure 2. We further prove in the appendix (see
Claim 1) that any ﬁnite MDP transition probabilities can be
decomposed into a state-independent distribution g over a
ﬁnite set of deterministic functions f .

Associated with a Lipschitz model class is a transition
function given by:

T (s(cid:48)

s, a) =

|

(cid:98)

f
(cid:88)

(cid:0)

1

f (s) = s(cid:48)

g(f

a) .

|

(cid:1)

Given a state distribution µ(s), we also deﬁne a generalized
notion of transition function
T
G

µ, a) given by:

(
· |

Figure 1. An illustration of Lipschitz continuity.
Pictorially,
Lipschitz continuity ensures that f lies in between the two afﬁne
functions (colored in blue) with slopes K and −K.

Equivalently, for a Lipschitz f ,

s1,

s2

d2

f (s1), f (s2)

Kd1,d2(f ) d1(s1, s2) .

∀

∀

≤

(cid:1)
The concept of Lipschitz continuity is visualized in Figure 1.

(cid:0)

A Lipschitz function f is called a non-expansion when
Kd1,d2(f ) = 1 and a contraction when Kd1,d2(f ) < 1.
Lipschitz continuity, in one form or another, has been a
key tool in the theory of reinforcement learning (Bertsekas,
1975; Bertsekas & Tsitsiklis, 1995; Littman & Szepesv´ari,
1996; M¨uller, 1996; Ferns et al., 2004; Hinderer, 2005;
Rachelson & Lagoudakis, 2010; Szepesv´ari, 2010; Pazis
& Parr, 2013; Pirotta et al., 2015; Pires & Szepesv´ari,
2016; Berkenkamp et al., 2017; Bellemare et al., 2017) and
bandits (Kleinberg et al., 2008; Bubeck et al., 2011). Below,
we also deﬁne Lipschitz continuity over a subset of inputs.
Deﬁnition 2. A function f : M1 × A (cid:55)→
Lipschitz continuous in

M2 is uniformly

if

A

is ﬁnite.

K Ad1,d2(f ) := sup
∈A

a

sup
s1,s2

d2

f (s1, a), f (s2, a)
d1(s1, s2)

(cid:0)

(cid:1)

,

(2)

is ﬁnite.

Note that the metric d1 is deﬁned only on M1.

2.2. Wasserstein Metric

We quantify the distance between two distributions using
the following metric:

Deﬁnition 3. Given a metric space (M, d) and the set
P(M ) of all probability measures on M , the Wasserstein
metric (or the 1st Kantorovic metric) between two
probability distributions µ1 and µ2 in P(M ) is deﬁned as

W (µ1, µ2) := inf
Λ
∈

j

(cid:90) (cid:90)

j(s1, s2)d(s1, s2)ds2 ds1 , (3)

where Λ denotes the collection of all joint distributions j on
M with marginals µ1 and µ2 (Vaserstein, 1969).
M

(cid:98)

×

(s(cid:48)

T
G

|

µ, a) =

1

f (s) = s(cid:48)
(cid:98)

g(f

a)

µ(s)ds .

(cid:90)s

f
(cid:88)

(cid:0)

(cid:124)

(cid:98)T (s(cid:48)

(cid:1)
s,a)
|
(cid:123)(cid:122)

|

(cid:125)

Lipschitz Continuity in Model-based Reinforcement Learning

Figure 2. An example of a Lipschitz model class in a gridworld
environment (Russell & Norvig, 1995). The dynamics are such
that any action choice results in an attempted transition in the
corresponding direction with probability 0.8 and in the neighboring
directions with probabilities 0.1 and 0.1. We can deﬁne Fg =
{f up, f right, f down, f left} where each f outputs a deterministic
next position in the grid (factoring in obstacles). For a = up,
we have: g(f up | a = up) = 0.8, g(f right
| a = up) =
g(f left | a = up) = 0.1, and g(f down | a = up) = 0. Deﬁning
distances between states as their Manhattan distance in the grid,
(cid:0)d(f (s1), f (s2)(cid:1)/d(s1, s2) = 2, and so KF =
then ∀f sups1,s2
2. So, the four functions and g comprise a Lipschitz model class.

T
G

. However, since

), the Lipschitz
We are primarily interested in K Ad,d(
T
G
constant of
takes as input a
T
G
(cid:98)
probability distribution and also outputs a probability
distribution, we require a notion of distance between two
distributions. This notion is quantiﬁed using Wasserstein
and is justiﬁed in the next section.

(cid:98)

(cid:98)

4. On the Choice of Probability Metric

We consider the stochastic model-based setting and show
through an example that
the Wasserstein metric is a
reasonable choice compared to other common options.

Consider a uniform distribution over states µ(s) as shown
in black in Figure 3 (top). Take a transition function T
in
G
the environment that, given an action a, uniformly randomly
adds or subtracts a scalar c1. The distribution of states
after one transition is shown in red in Figure 3 (middle).
Now, consider a transition model
that approximates T
G
by uniformly randomly adding or subtracting the scalar
c2. The distribution over states after one transition using
this imperfect model is shown in blue in Figure 3 (bottom).
We desire a metric that captures the similarity between the
output of the two transition functions. We ﬁrst consider
Kullback-Leibler (KL) divergence and observe that:

T
G

(cid:98)

KL

T
G

:=

(cid:0)

(cid:90)

(
· |
(s(cid:48)

T
G

µ, a),

(

T
G

· |
T
µ, a) log
G
(cid:98)
T
G

µ, a)
(s(cid:48)
(cid:1)
|
(s(cid:48) |

|

µ, a)

µ, a)

ds(cid:48) =

,

∞

unless the two constants are exactly the same.

(cid:98)

Figure 3. A state distribution µ(s) (top), a stochastic environment
that randomly adds or subtracts c1 (middle), and an approximate
transition model that randomly adds or subtracts a second scalar
c2 (bottom).

The next possible choice is Total Variation (TV) deﬁned as:

µ, a),

µ, a)

T
G

T V

:=

1
(cid:0)
2

(
· |
T
G

(s(cid:48)

|

(
· |

T
G
µ, a)
(cid:98)

(cid:1)
(s(cid:48)

T
G

−

|

µ, a)

ds(cid:48) = 1 ,

if the two distributions have disjoint supports regardless of
how far the supports are from each other.

(cid:98)

(cid:12)
(cid:12)

(cid:90)

(cid:12)
(cid:12)

In contrast, Wasserstein is sensitive to how far the constants
are as:

µ, a)

=

c1 −
|

c2|

.

W

T
G

(
· |

µ, a),

(

T
G

(cid:0)

· |
It is clear that, of the three, Wasserstein corresponds best
to the intuitive sense of how closely T
approximates
G
. This is particularly important in high-dimensional
T
G
spaces where the true distribution is known to usually lie in
low-dimensional manifolds. (Narayanan & Mitter, 2010)
(cid:98)

(cid:98)

(cid:1)

5. Understanding the Compounding Error

Phenomenon

−

To extract a prediction with a horizon n > 1, model-based
algorithms typically apply the model for n steps by taking
the state input in step t to be the state output from
the step t
1. Previous work has shown that model
error can result in poor long-horizon predictions and
ineffective planning (Talvitie, 2014; 2017). Observed even
beyond reinforcement learning (Lorenz, 1972; Venkatraman
et al., 2015), this is referred to as the compounding error
phenomenon. The goal of this section is to provide a bound
on multi-step prediction error of a model. We formalize the
notion of model accuracy below:

Deﬁnition 5. Given an MDP with a transition function
T , we identify a Lipschitz model Fg as ∆-accurate if its
induced

T satisﬁes:

s
∀

a W
(cid:98)
∀

· |

T (

s, a), T (

s, a)

∆ .

· |

≤

(cid:1)

(cid:0)

(cid:98)

Lipschitz Continuity in Model-based Reinforcement Learning

We want to express the multi-step Wasserstein error in
terms of the single-step Wasserstein error and the Lipschitz
constant of the transition function
. We provide a bound
using the following lemma:
on the Lipschitz constant of

T
G

Lemma 1. A generalized transition function
(cid:98)
a Lipschitz model class Fg is Lipschitz with a constant:

T
G

induced by

T
G

(cid:98)

(cid:98)

K AW,W (

T
G

) := sup

sup
µ1,µ2

a

(cid:98)

W

(

T
G

(
µ1, a),
T
·|
·|
G
W (µ1, µ2)

µ2, a)

KF

≤

(cid:1)

(cid:0)

(cid:98)

(cid:98)

the two input
Intuitively, Lemma 1 states that,
distributions are similar, then for any action the output
distributions given by
are also similar up to a KF factor.
We prove this lemma, as well as the subsequent lemmas, in
the appendix.

T
G

if

(cid:98)

Given the one-step error (Deﬁnition 5), a start state
distribution µ and a ﬁxed sequence of actions a0, ..., an
1,
we desire a bound on n-step error:

−

δ(n) := W

T n
G

(
· |

µ), T n
G

(

· |

µ)

,

where

T n
G

(
·|

µ) :=

(cid:0)
(cid:98)
T
G

T
G

(
·|

(cid:1)

...

(
·|

µ, a0)..., an

(
·|

T
G
n recursive calls

−

2), an

1)

−

(cid:98)
(cid:98)
and T n
(
· |
G
lemma followed by the theorem.

µ) is deﬁned similarly. We provide a useful
(cid:125)

(cid:98)
(cid:124)

(cid:123)(cid:122)

(cid:98)

Lemma 2. (Composition Lemma) Deﬁne three metric
spaces (M1, d1), (M2, d2), and (M3, d3). Deﬁne Lipschitz
M2 with constants
M3 and g : M1 (cid:55)→
functions f : M2 (cid:55)→
Kd2,d3(f ) and Kd1,d2 (g). Then, h : f
M3 is
g : M1 (cid:55)→
Kd2,d3 (f )Kd1,d2(g).
Lipschitz with constant Kd1,d3 (h)

◦

≤

Similar to composition, we can show that summation
preserves Lipschitz continuity with a constant bounded by
the sum of the Lipschitz constants of the two functions. We
omitted this result due to brevity.

Theorem 1. Deﬁne a ∆-accurate
with the Lipschitz
constant KF and an MDP with a Lipschitz transition
with constant KT . Let ¯K = min
function T
.
KF , KT
}
{
G
1:
Then
≥

T
G

n

∀

(cid:98)

δ(n) := W

T n
G

(
· |

µ), T n
G

(

· |

µ)

∆

≤

( ¯K)i .

n

1

−

i=0
(cid:88)

(cid:1)

(cid:0)

(cid:98)

(cid:1)
s, a0)

f (s(cid:48))µ(s) ds ds(cid:48)

(cid:1)
f (s(cid:48)) ds(cid:48)

µ(s) ds

δ(1) := W

:= sup

µ, a0)

(

T
· |
G
T (s(cid:48)
(cid:98)

|

(

µ, a0), T
· |
G
T (s(cid:48)

s, a0)

−

|

(cid:0)
f (cid:90) (cid:90)
sup
f (cid:90)

(cid:0)
=W

(cid:98)

(cid:0)
(cid:98)
T (s(cid:48)

s, a0)
|

−

T (s(cid:48)

s, a0)
|

(cid:1)
due to duality (4)

(cid:1)
s, a0)

µ(s) ds

(cid:125)

(cid:124)
W

(cid:98)T (

s,a0),T (

·|

s,a0)
·|
(cid:123)(cid:122)

s, a0), T (

(cid:0)
T (
· |
∆ due to Deﬁnition 5

· |

(cid:0)

≤

(cid:98)

(cid:124)
∆ µ(s) ds = ∆ .

(cid:123)(cid:122)

(cid:1)

(cid:125)

≤

(cid:90)

=

(cid:90)

≤

(cid:90)

1

−

We now prove the inductive step. Assuming δ(n
T n
W
(
G
write:
(cid:0)
(cid:98)
δ(n) := W

µ), T n
G

(
· |

(cid:1)
µ)

(cid:80)

µ)

· |

∆

≤

−

(

n

1

2

1) :=
i=0 (KF )i we can

−

−

T n
(
· |
G
T
µ),
G

µ), T n
· |
G
T n
−
· |
G
µ), an

−

1
(
(cid:98)

(cid:0)
· |

1

−

(
· |

1

(

(cid:1)
· |

µ), an

1

−

1

−

, T n
G

(
· |

(cid:1)
1),

T
G

· |

W

≤
+W

T n
G

(cid:16)
T
(cid:98)
G

= W

+W

(cid:16)

T
(cid:98)
G

(cid:0)
(
· |

(cid:16)
T
(cid:98)
G
(cid:0)

· |

· |

(cid:0)
(
(cid:98)
· |
T n
G
T n
G
T n
(cid:98)
G

µ), an

−

1

1

−

(
· |

µ), an

, T
(cid:98)
G

(cid:0)
(
· |

(cid:17)
T n
−
G
T n
G

−

1

1

(

· |

(

· |

µ), an

1

−

µ), an

1)

(cid:16)

−
We now use Lemma 1 and Deﬁnition 5 to upper bound the
ﬁrst and the second term of the last line respectively.

(cid:98)

(cid:1)

−

(cid:1)(cid:17)

(cid:17)

(cid:1)(cid:17)

µ)

(Triangle ineq)

δ(n)

KF W

≤

(cid:0)
= KF δ(n

1

−

T n
G

(
· |

1

µ), T n
G
n

−

(cid:98)
−

1) + ∆

∆

≤

+ ∆

µ)

(
· |
1
(KF )i .

(cid:1)

−

(5)

i=0
(cid:88)

in the triangle inequality, we may replace
T n
1
and follow
G

(
· |

µ)

µ)

· |

Note that
T n
with T
(
T
· |
G
G
G
the same basic steps to get:
(cid:98)

· |

(cid:98)

(cid:0)

(cid:0)

(cid:1)

(cid:1)

−

−

1

W

(

T n
G

· |

µ), T n
G

(
· |

µ)

(cid:0)

(cid:98)

n

−

1
(KT )i .

∆

≤

(cid:1)

i=0
(cid:88)

Combining (5) and (6) allows us to write:

(6)

δ(n) = W

T n
G

(
· |
n

(cid:0)
min

∆

(
· |

µ), T n
G
1
(KT )i, ∆

µ)
n
(cid:1)
−

−

1
(KF )i

(cid:41)

i=0
(cid:88)

≤

= ∆

(cid:98)
(cid:40)
1

n

−

i=0
(cid:88)

i=0
(cid:88)
( ¯K)i ,

which concludes the proof.

Proof. We construct a proof by induction.
Using
Kantarovich-Rubinstein duality (Lipschitz property of f
not shown for brevity) we ﬁrst prove the base of induction:

There exist similar
results in the literature relating
one-step transition error to multi-step transition error and
sub-optimality bounds for planning with an approximate

Lipschitz Continuity in Model-based Reinforcement Learning

model. The Simulation Lemma (Kearns & Singh, 2002;
Strehl et al., 2009) is for discrete state MDPs and relates
error in the one-step model to the value obtained by
using it for planning. A related result for continuous
state-spaces (Kakade et al., 2003) bounds the error in
estimating the probability of a trajectory using total
variation. A second related result (Venkatraman et al.,
2015) provides a slightly looser bound for prediction error
in the deterministic case—our result can be thought of as a
generalization of their result to the probabilistic case.

6. Value Error with Lipschitz Models

,

A

(cid:104)S

, T, R, γ

We next investigate the error in the state-value function
induced by a Lipschitz model class. To answer this question,
we consider an MRP M1 denoted by
and
a second MRP M2 that only differs from the ﬁrst in its
be the
transition function
T , R, γ
action set with a single action a. We further assume that
the reward function is only dependent upon state. We ﬁrst
express the state-value function for a start state s with
respect to the two transition functions. By δs below, we
mean a Dirac delta function denoting a distribution with
probability 1 at state s.

. Let
(cid:105)

,
A

a
}

(cid:104)S

A

=

(cid:98)

{

(cid:105)

,

VT (s) :=

(s(cid:48)

δs)R(s(cid:48)) ds(cid:48) ,

∞

γn

n=0
(cid:88)

(cid:90)

T n
G

∞

γn

n=0
(cid:88)

(cid:90)

T n
G

(cid:98)

|

|

V (cid:98)T (s) :=

(s(cid:48)

δs)R(s(cid:48)) ds(cid:48) .

Let

=

h : KdS ,R(h)

F

{

1

. Then given f
}

≤

:

∈ F

KR

∞

γn

f (s(cid:48))

T n
G

(s(cid:48)

δs)

|

T n
G

(s(cid:48)

δs)
|

−

ds(cid:48)

n=0
(cid:88)

KR

(cid:90)
∞

n=0
(cid:88)

(cid:0)
γn sup
f

∈F (cid:90)

≤

= KR

∞

(cid:124)
γn W

f (s(cid:48))

δs)

(s(cid:48)

δs)

ds(cid:48)

(cid:98)
T n
(s(cid:48)
G

|

(cid:1)

−

T n
G

(cid:98)

|

(cid:1)

(cid:125)

:=W

δs), (cid:98)T n

due to duality (4)

(cid:0)
T n
G (.
|

δs)
G (.
|
(cid:123)(cid:122)
δs)

(cid:1)

(cid:0)
|

(.

(.

δs),

T n
G

T n
G
(cid:80)n−1
(cid:0)
(cid:1)
i=0 ∆( ¯K)i due to Theorem 1
(cid:98)
1
(cid:125)

(cid:123)(cid:122)

|

≤
n
(cid:124)

−

n=0
(cid:88)

n=0
(cid:88)

∞

KR

∞

γn

∆( ¯K)i

= KR∆

≤

=

i=0
(cid:88)
γn 1
−
1
−

¯K n
¯K

.

n=0
(cid:88)
γKR∆
γ)(1

−

−

(1

γ ¯K)
We can derive the same bound for V (cid:98)T (s)
VT (s) using
the fact that Wasserstein distance is a metric, and therefore
symmetric, thereby completing the proof.

−

Regarding the tightness of our bounds, we can show that
when the transition model is deterministic and linear then
Theorem 1 provides a tight bound. Moreover, if the reward
function is linear, the bound provided by Theorem 2 is tight.
(See Claim 2 in the appendix.) Notice also that our proof
does not require a bounded reward function.

7. Lipschitz Generalized Value Iteration

Next we derive a bound on

VT (s)

s.

∀

−

V (cid:98)T (s)
(cid:12)
(cid:12)

Theorem 2. Assume a Lipschitz model class Fg with a
(cid:12)
(cid:12)
T with ¯K = min
∆-accurate
. Further, assume
KF , KT
}
{
a Lipschitz reward function with constant KR = KdS ,R(R).
Then

and ¯K
(cid:98)

[0, 1
γ )

s
∀

∈ S

∈

We next show that, given a Lipschitz transition model,
solving for the ﬁxed point of a class of Bellman equations
yields a Lipschitz state-action value function. Our proof is in
the context of Generalized Value Iteration (GVI) (Littman &
Szepesv´ari, 1996), which deﬁnes Value Iteration (Bellman,
1957) for planning with arbitrary backup operators.

VT (s)

V (cid:98)T (s)

−

≤

(1

(cid:12)
(cid:12)

(cid:12)
(cid:12)

γKR∆
γ)(1

−

−

.

γ ¯K)

Proof. We ﬁrst deﬁne the function f (s) = R(s)
KR
observed that KdS ,R(f ) = 1. We now write:

. It can be

VT (s)

V (cid:98)T (s)

−
γn

∞

=

n=0
(cid:88)

(cid:90)

= KR

∞

γn

T n
G

(cid:0)
f (s(cid:48))

n=0
(cid:88)

(cid:90)

(cid:0)

R(s(cid:48))

(s(cid:48)

δs)

(s(cid:48)

δs)

ds(cid:48)

|

|

T n
G

−

(cid:98)
δs)

(cid:98)

(s(cid:48)

T n
G

|

(s(cid:48)

T n
G

|

−

(cid:1)
δs)

ds(cid:48)

(cid:1)

Algorithm 1 GVI algorithm

Input: initial
repeat

Q(s, a), δ, and choose an operator f

for each s, a
(cid:98)
∈ S × A
R(s, a)+γ
Q(s, a)

do

←

end for
(cid:98)

until convergence

|

(cid:82)

(cid:98)

T (s(cid:48)

s, a)f

Q(s(cid:48),

ds(cid:48)

)
·

(cid:1)

(cid:0)

(cid:98)

To prove the result, we make use of the following lemmas.
Lemma 3. Given a Lipschitz function f :
R with
constant KdS ,dR(f ):

S (cid:55)→

K AdS ,dR

T (s(cid:48)

s, a)f (s(cid:48))ds(cid:48)
|

KdS ,dR(f )K AdS ,W

T

.

≤

(cid:17)

(cid:16) (cid:90)

(cid:98)

(cid:0)

(cid:1)

(cid:98)

Lipschitz Continuity in Model-based Reinforcement Learning

Lemma 4. The following operators (Asadi & Littman,
2017) are Lipschitz with constants:

(cid:107)(cid:107)∞,dR

mean(x)

=

(cid:0)

(cid:1)

1. K
K

2. K

3. K

(cid:107)(cid:107)∞,dR (max(x)) = K
(cid:107)(cid:107)∞,dR ((cid:15)-greedy(x)) = 1
(cid:107)(cid:107)∞,dR (mmβ(x) := log
(cid:107)(cid:107)∞,dR (boltzβ(x)
:=
A
βVmax|
|

(cid:80)n

β

(cid:80)

i eβxi
n

) = 1

i=1 xieβxi
(cid:80)n
i=1eβ xi

)

≤

+

A
|

|

(cid:112)

Theorem 3. For any non-expansion backup operator f
outlined in Lemma 4, GVI computes a value function

with a Lipschitz constant bounded by
γK AdS ,W (T ) < 1.

(R)

KA
dS ,dR
γKdS ,W (T )

1

−

if

Proof. From Algorithm 1, in the nth round of GVI updates:

·

(cid:1)

)
·

Qn+1(s, a)

R(s, a) + γ

T (s(cid:48)

s, a)f

Qn(s(cid:48),

)

ds(cid:48).

←

|

(cid:90)

(cid:0)

(cid:98)

(cid:98)
Now observe that:

K AdS ,dR (

Qn+1)

≤

≤

K AdS ,dR(R)+γK AdS ,dR

(cid:98)

T (s(cid:48)

s, a)f

Qn(s(cid:48),

ds(cid:48)

K AdS ,dR (R) + γK AdS ,W (T ) KdS,R

(cid:90)

(cid:0)

(cid:1)

(cid:0)
Qn(s,
(cid:98)

·

f

(cid:16)

)
(cid:1)(cid:17)
(cid:0)
(cid:107)·(cid:107)∞,dR(f )K AdS ,dR (
(cid:98)
Qn)

(cid:1)

Qn)

K AdS ,dR (R) + γK AdS ,W (T )K

≤
= K AdS ,dR (R) + γK AdS ,W (T )K AdS ,dR (

(cid:98)
Where we used Lemmas 3, 2, and 4 for the second, third,
and fourth inequality respectively. Equivalently:

(cid:98)

|

n

K AdS ,dR (

Qn+1)

K AdS ,dR(R)

γK AdS ,W (T )

i

≤

+

(cid:98)

i=0
(cid:88)
(cid:0)
n
γK AdS ,W (T )

K AdS ,dR (

(cid:1)
Q0) .

By computing the limit of both sides, we get:

(cid:1)

(cid:0)

K AdS ,dR (

Qn+1)

K AdS ,dR (R)

γK AdS ,W (T )

lim
n
→∞

(cid:98)

(cid:98)

n

i=0
(cid:88)
(cid:0)
γK AdS ,W (T )

n

K AdS ,dR (

(cid:1)
Q0)

K AdS ,dR (R)

(cid:0)
γKdS ,W (T )

(cid:1)
+ 0 ,

i

(cid:98)

lim
n
→∞

≤

+ lim
n
→∞

=

1

−

This concludes the proof.

Two implications of this result: First, PAC exploration in
continuous state spaces is shown assuming a Lipschitz value
function (Pazis & Parr, 2013). However, the theorem shows

that it is sufﬁcient to have a Lipschitz model, an assumption
perhaps easier to conﬁrm. The second implication relates to
value-aware model learning (VAML) objective (Farahmand
et al., 2017). Using the above theorem, we can show that
minimizing Wasserstein is equivalent to minimizing the
VAML objective (Asadi et al., 2018).

8. Experiments

|S|

Our ﬁrst goal in this section1 is to compare TV, KL, and
Wasserstein in terms of the ability to best quantify error of
an imperfect model. To this end, we built ﬁnite MRPs with
random transitions,
= 10 states, and γ = 0.95. In the
ﬁrst case the reward signal is randomly sampled from [0, 10],
and in the second case the reward of an state is the index of
that state, so small Euclidean norm between two states is
an indication of similar values. For 105 trials, we generated
an MRP and a random model, and then computed model
error and planning error (Figure 4). We understand a good
metric as the one that computes a model error with a high
correlation with value error. We show these correlations for
different values of γ in Figure 5.

Figure 4. Value error (x axis) and model error (y axis). When
the reward is the index of the state (right), correlation between
Wasserstein error and value-prediction error is high.
This
highlights the fact that when closeness in the state-space is an
indication of similar values, Wasserstein can be a powerful metric
for model-based RL. Note that Wasserstein provides no advantage
given random rewards (left).

Figure 5. Correlation between value-prediction error and model
error for the three metrics using random rewards (left) and index
rewards (right). Given a useful notion of state similarities, low
Wasserstein error is a better indication of planning error.

1We release the code here: github.com/kavosh8/Lip

Lipschitz Continuity in Model-based Reinforcement Learning

Function f

Deﬁnition

ReLu : Rn

+b : Rn

Rn,

W : Rn

×

→
Rm,

→

Rn
→
b
∀
W

∈

∀

∈

Rn
Rm

n

×

ReLu(x)i := max
{
+b(x) := x + b

0, xi

}

W (x) := W x

×

Lipschitz constant K

p,

(cid:107)(cid:107)

(cid:107)(cid:107)

p = 1
1
1

p = 2
1
1

Wj

j (cid:107)

(cid:107)∞

Wj

2
2

(cid:107)

j (cid:107)

(cid:80)

(cid:113)(cid:80)

p (f )
p =
1
1
supj (cid:107)

∞

Wj

(cid:107)1

Table 1. Lipschitz constant for various functions used in a neural network. Here, Wj denotes the jth row of a weight matrix W .

It is known that controlling the Lipschitz constant of neural
nets can help in terms of improving generalization error due
to a lower bound on Rademacher complexity (Neyshabur
et al., 2015; Bartlett & Mendelson, 2002). It then follows
from Theorems 1 and 2 that controlling the Lipschitz
constant of a learned transition model can achieve better
error bounds for multi-step and value predictions. To
enforce this constraint during learning, we bound the
Lipschitz constant of various operations used in building
neural network. The bound on the constant of the entire
neural network then follows from Lemma 2. In Table 1, we
provide Lipschitz constant for operations (see Appendix for
proof) used in our experiments. We quantify these results
for different p-norms

(cid:107)·(cid:107)p.

speciﬁcally when the transition model

Given these simple methods for enforcing Lipschitz
continuity, we performed empirical evaluations
to
understand the impact of Lipschitz continuity of transition
models,
is
used to perform multi-step state-predictions and policy
improvements. We chose two standard domains: Cart Pole
and Pendulum. In Cart Pole, we trained a network on a
dataset of 15
. During training, we
(cid:105)
ensured that the weights of the network are smaller than k.
For each k, we performed 20 independent model estimation,
and chose the model with median cross-validation error.

s, a, s(cid:48)
(cid:104)

103 tuples

∗

Using the learned model, along with the actual reward
signal of the environment, we then performed stochastic
actor-critic RL. (Barto et al., 1983; Sutton et al., 2000)
This required an interaction between the policy and the
learned model for relatively long trajectories. To measure
the usefulness of the model, we then tested the learned
policy on the actual domain. We repeated this experiment
on Pendulum. To train the neural transition model for
this domain we used 104 samples. Notably, we used
deterministic policy gradient (Silver et al., 2014) for training
the policy network with the hyper parameters suggested by
Lillicrap et al. (2015). We report these results in Figure 6.

Observe that an intermediate Lipschitz constant yields the
best result. Consistent with the theory, controlling the
Lipschitz constant in practice can combat the compounding
errors and can help in the value estimation problem. This
ultimately results in learning a better policy.

We next examined if the beneﬁts carry over to stochastic

Figure 6. Impact of Lipschitz constant of learned models in Cart
Pole (left) and Pendulum (right). An intermediate value of k
(Lipschitz constant) yields the best performance.

Fg

θf : f
{

settings. To capture stochasticity we need an algorithm to
learn a Lipschitz model class (Deﬁnition 4). We used an EM
algorithm to joinly learn a set of functions f , parameterized
by θ =
, and a distribution over functions
}
g. Note that in practice our dataset only consists of a set
of samples
and does not include the function the
sample is drawn from. Hence, we consider this as our
latent variable z. As is standard with EM, we start with the
log-likelihood objective (for simplicity of presentation we
assume a single action in the derivation):

s, a, s(cid:48)
(cid:104)

∈

(cid:105)

L(θ) =

log p(si, si(cid:48); θ)

N

i=1
(cid:88)
N

i=1
(cid:88)
N

i=1
(cid:88)
N

=

=

≥

log

p(zi = f, si, si(cid:48); θ)

f
(cid:88)

f
(cid:88)

log

q(zi = f

si, si(cid:48))

|

q(zi = f

si, si(cid:48))log

|

i=1
(cid:88)

f
(cid:88)

p(zi = f, si, si(cid:48); θ)
si, si(cid:48))
|

q(zi = f

p(zi = f, si, si(cid:48); θ)
si, si(cid:48))
|

q(zi = f

,

where we used Jensen’s inequality and concavity of log in
the last line. This derivation leads to the following EM
algorithm.

Lipschitz Continuity in Model-based Reinforcement Learning

Figure 7. A stochastic problem solved by training a Lipschitz
model class using EM. The top left ﬁgure shows the functions
before any training (iteration 0), and the bottom right ﬁgure shows
the ﬁnal results (iteration 50).

In the M step, ﬁnd θt by solving for:

N

argmax

θ

i=1
(cid:88)

f
(cid:88)

qt

1(zi = f

−

si, si(cid:48))log
|

p(zi = f, si, si(cid:48); θ)
si, si(cid:48))
qt

1(zi = f

−

|

In the E step, compute posteriors:

qt(zi = f

si, si(cid:48)) =
|

p(si, si(cid:48)
f p(si, si(cid:48)|

|

zi = f ; θt

f )g(zi = f ; θt)

zi = f ; θt

f )g(zi = f ; θt)

.

Note that we assume each point is drawn from a neural
network f with probability:

(cid:80)

p

si, si(cid:48)

zi = f ; θt
|

N
and with a ﬁxed variance σ2 tuned as a hyper-parameter.

−

(cid:17)

(cid:0)

(cid:1)

(cid:16)(cid:12)
(cid:12)

(cid:12)
(cid:12)

f

=

si(cid:48)

f (si, θt

f )

, σ2

,

We used a supervised-learning domain to evaluate the EM
algorithm. We generated 30 points from 5 functions (written
at the end of Appendix) and trained 5 neural networks to ﬁt
these points. Iterations of a single run is shown in Figure 7
and the summary of results is presented in Figure 8. Observe
that the EM algorithm is effective, and that controlling the
Lipschitz constant is again useful.

We next applied EM to train a transition model for an RL
setting, namely the gridworld domain from Moerland et al.
(2017). Here a useful model needs to capture the stochastic
behavior of the two ghosts. We modify the reward to be
-1 whenever the agent is in the same cell as either one of
the ghosts and 0 otherwise. We performed environmental
interactions for 1000 time-steps and measured the return.
We compared against standard tabular methods(Sutton
& Barto, 1998), and a deterministic model that predicts
expected next state (Sutton et al., 2008; Parr et al., 2008). In
all cases we used value iteration for planning.

Figure 8. Impact of controlling the Lipschitz constant in the
supervised-learning domain.
Notice the U-shape of ﬁnal
Wasserstein loss with respect to Lipschitz constant k.

Figure 9. Performance of a Lipschitz model class on the gridworld
domain. We show model test accuracy (left) and quality of the
policy found using the model (right). Notice the poor performance
of tabular and expected models.

Results in Figure 9 show that tabular models fail due to no
generalization, and expected models fail since the ghosts
do not move on expectation, a prediction not useful for
planner. Performing value iteration with a Lipschitz model
class outperforms the baselines.

9. Conclusion

We took an important
step towards understanding
model-based RL with function approximation. We showed
that Lipschitz continuity of an estimated model plays
a central role in multi-step prediction error, and in
value-estimation error. We also showed the beneﬁts of
employing Wasserstein for model-based RL. An important
future work is to apply these ideas to larger problems.

10. Acknowledgements

The authors recognize the assistance of Eli Upfal, John
Langford, George Konidaris, and members of Brown’s Rlab
speciﬁcally Cameron Allen, David Abel, and Evan Cater.
The authors also thank anonymous ICML reviewer 1 for
insights on a value-aware interpretation of Wasserstein.

Lipschitz Continuity in Model-based Reinforcement Learning

References

Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein
In International

generative adversarial networks.
Conference on Machine Learning, pp. 214–223, 2017.

Asadi, K. and Littman, M. L. An alternative softmax
operator for reinforcement learning. In Proceedings of
the 34th International Conference on Machine Learning,
pp. 243–252, 2017.

Asadi, K., Cater, E., Misra, D., and Littman, M. L.
Equivalence between wasserstein and value-aware
model-based reinforcement learning. arXiv preprint
arXiv:1806.01265, 2018.

Bartlett, P. L. and Mendelson, S. Rademacher and gaussian
complexities: Risk bounds and structural results. Journal
of Machine Learning Research, 3(Nov):463–482, 2002.

Barto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike
adaptive elements that can solve difﬁcult learning control
IEEE transactions on systems, man, and
problems.
cybernetics, pp. 834–846, 1983.

Bellemare, M. G., Dabney, W., and Munos, R. A
distributional perspective on reinforcement learning. In
International Conference on Machine Learning, pp.
449–458, 2017.

Bellman, R. A markovian decision process. Journal of

Mathematics and Mechanics, pp. 679–684, 1957.

Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause,
A.
Safe model-based reinforcement learning with
stability guarantees. In Advances in Neural Information
Processing Systems, pp. 908–919, 2017.

Bertsekas, D. Convergence of discretization procedures in
dynamic programming. IEEE Transactions on Automatic
Control, 20(3):415–419, 1975.

Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-dynamic
programming: an overview. In Decision and Control,
1995, Proceedings of the 34th IEEE Conference on,
volume 1, pp. 560–564. IEEE, 1995.

Bubeck, S., Munos, R., Stoltz, G., and Szepesv´ari, C.
X-armed bandits. Journal of Machine Learning Research,
12(May):1655–1695, 2011.

Dempster, A. P., Laird, N. M., and Rubin, D. B.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society. Series
B (methodological), pp. 1–38, 1977.

Farahmand, A.-M., Barreto, A., and Nikovski, D.
for Model-based
the

Value-Aware
Reinforcement Learning.

In Proceedings of

Function

Loss

20th International Conference on Artiﬁcial Intelligence
and Statistics, pp. 1486–1494, 2017.

Ferns, N., Panangaden, P., and Precup, D. Metrics for ﬁnite
markov decision processes. In Proceedings of the 20th
conference on Uncertainty in artiﬁcial intelligence, pp.
162–169. AUAI Press, 2004.

Fox, R., Pakman, A., and Tishby, N. G-learning: Taming
the noise in reinforcement learning via soft updates.
Uncertainty in Artiﬁcal Intelligence, 2016.

Gao, B. and Pavel, L.

On the properties of the
softmax function with application in game theory and
reinforcement learning. arXiv preprint arXiv:1704.00805,
2017.

Hinderer, K. Lipschitz continuity of value functions in
Markovian decision processes. Mathematical Methods of
Operations Research, 62(1):3–22, 2005.

Jiang, N., Kulesza, A., Singh, S., and Lewis, R. The
dependence of effective planning horizon on model
In Proceedings of AAMAS, pp. 1181–1189,
accuracy.
2015.

Kaelbling, L. P., Littman, M. L., and Moore, A. W.
Reinforcement learning: A survey. Journal of artiﬁcial
intelligence research, 4:237–285, 1996.

Kakade, S., Kearns, M. J., and Langford, J. Exploration
the
in metric state spaces.
20th International Conference on Machine Learning
(ICML-03), pp. 306–312, 2003.

In Proceedings of

Kearns, M. and Singh, S. Near-optimal reinforcement
learning in polynomial time. Machine Learning, 49(2-3):
209–232, 2002.

Kleinberg, R., Slivkins, A., and Upfal, E. Multi-armed
bandits in metric spaces. In Proceedings of the Fortieth
Annual ACM Symposium on Theory of Computing, pp.
681–690. ACM, 2008.

Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.

Littman, M. L. and Szepesv´ari, C.
A generalized
reinforcement-learning model:
and
applications. In Proceedings of the 13th International
Conference on Machine Learning, pp. 310–318, 1996.

Convergence

Lorenz, E. Predictability: does the ﬂap of a butterﬂy’s wing

in Brazil set off a tornado in Texas? na, 1972.

Lipschitz Continuity in Model-based Reinforcement Learning

Moerland, T. M., Broekens, J., and Jonker, C. M. Learning
for model-based
multimodal
reinforcement learning. arXiv preprint arXiv:1705.00470,
2017.

transition dynamics

M¨uller, A. Optimal selection from distributions with
unknown parameters: Robustness of bayesian models.
Mathematical Methods of Operations Research, 44(3):
371–386, 1996.

Nachum, O., Norouzi, M., Xu, K., and Schuurmans,
D. Bridging the gap between value and policy based
reinforcement learning. arXiv preprint arXiv:1702.08892,
2017.

Narayanan, H. and Mitter, S.

Sample complexity of
testing the manifold hypothesis. In Advances in Neural
Information Processing Systems, pp. 1786–1794, 2010.

Strehl, A. L., Li, L., and Littman, M. L. Reinforcement
learning in ﬁnite mdps: Pac analysis. Journal of Machine
Learning Research, 10(Nov):2413–2444, 2009.

Sutton, R. S. and Barto, A. G. Reinforcement Learning: An

Introduction. The MIT Press, 1998.

Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour,
Y. Policy gradient methods for reinforcement learning
In Advances in Neural
with function approximation.
Information Processing Systems, pp. 1057–1063, 2000.

Sutton, R. S., Szepesv´ari, C., Geramifard, A., and Bowling,
M. H.
Dyna-style planning with linear function
approximation and prioritized sweeping. In UAI 2008,
Proceedings of the 24th Conference in Uncertainty in
Artiﬁcial Intelligence, Helsinki, Finland, July 9-12, 2008,
pp. 528–536, 2008.

Neu, G., Jonsson, A., and G´omez, V. A uniﬁed view of
entropy-regularized Markov decision processes. arXiv
preprint arXiv:1705.07798, 2017.

Szepesv´ari, C. Algorithms for reinforcement learning.
Synthesis Lectures on Artiﬁcial Intelligence and Machine
Learning, 4(1):1–103, 2010.

Talvitie, E. Model regularization for stable sample
rollouts. In Proceedings of the Thirtieth Conference on
Uncertainty in Artiﬁcial Intelligence, pp. 780–789. AUAI
Press, 2014.

Talvitie, E.

Self-correcting models for model-based
reinforcement learning. In Proceedings of the Thirty-First
AAAI Conference on Artiﬁcial Intelligence, February 4-9,
2017, San Francisco, California, USA., 2017.

Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society. Series B
(Methodological), pp. 267–288, 1996.

Vaserstein, L. N. Markov processes over denumerable
products of spaces, describing large systems of automata.
Problemy Peredachi Informatsii, 5(3):64–72, 1969.

Venkatraman, A., Hebert, M., and Bagnell, J. A. Improving
multi-step prediction of learned time series models. In
Proceedings of the Twenty-Ninth AAAI Conference on
Artiﬁcial Intelligence, January 25-30, 2015, Austin, Texas,
USA., 2015.

Villani, C. Optimal transport: old and new, volume 338.

Springer Science & Business Media, 2008.

Neyshabur, B., Tomioka, R., and Srebro, N. Norm-based
capacity control in neural networks. In Proceedings of
The 28th Conference on Learning Theory, pp. 1376–1401,
2015.

Parr, R., Li, L., Taylor, G., Painter-Wakeﬁeld, C., and
Littman, M. L. An analysis of linear models, linear
value-function approximation, and feature selection
In Proceedings of the
for reinforcement learning.
25th international conference on Machine learning, pp.
752–759. ACM, 2008.

Pazis, J. and Parr, R. Pac optimal exploration in continuous

space markov decision processes. In AAAI, 2013.

Pires, B. ´A. and Szepesv´ari, C. Policy error bounds for
model-based reinforcement learning with factored linear
models. In Conference on Learning Theory, pp. 121–151,
2016.

Pirotta, M., Restelli, M., and Bascetta, L. Policy gradient in
lipschitz Markov decision processes. Machine Learning,
100(2-3):255–283, 2015.

Rachelson, E. and Lagoudakis, M. G. On the locality of
action domination in sequential decision making.
In
International Symposium on Artiﬁcial Intelligence and
Mathematics, ISAIM 2010, Fort Lauderdale, Florida,
USA, January 6-8, 2010, 2010.

Russell, S. J. and Norvig, P. Artiﬁcial intelligence: A

modern approach, 1995.

Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and
Riedmiller, M. Deterministic policy gradient algorithms.
In ICML, 2014.

Lipschitz Continuity in Model-based Reinforcement Learning

Appendix

Claim 1. In a ﬁnite MDP, transition probabilities can be expressed using a ﬁnite set of deterministic functions and a
distribution over the functions.

Proof. Let P r(s, a, s(cid:48)) denote the probability of a transiton from s to s(cid:48) when executing the action a. Deﬁne an ordering
over states s1, ..., sn with an additional unreachable state s0. Now deﬁne the cumulative probability distribution:

Further deﬁne L as the set of distinct entries in C:

C(s, a, si) :=

P r(s, a, sj) .

i

j=0
(cid:88)

L :=

C(s, a, si)
|

s

, i

[0, n]

.

∈ S

∈

(cid:110)

(cid:111)

Note that, since the MDP is assumed to be ﬁnite, then
value of the set. Note that c0 = 0 and c
L
|

|

i = 1 to

and

j = 1 to n, deﬁne fi(s) = sj if and only if:

∀

L
|

|

∀

= 1. We now build determinstic set of functions f1, ..., f

is ﬁnite. We sort the values of L and denote, by ci, ith smallest
as follows:

L
|

|

L
|

|

We also deﬁne the probability distribution g over f as follows:

C(s, a, sj

1) < ci

C(s, a, sj) .

−

≤

g(fi

a) := ci
|

−

ci

1 .

−

Given the functions f1, ..., f
executing action a:

L
|

|

and the distribution g, we can now compute the probability of a transition to sj from s after

1(fi(s) = sj) g(fi

a)
|

i
(cid:88)
=

1

i
(cid:88)

(cid:0)

= C(s, a, sj)
−
= P r(s, a, sj) ,

C(s, a, sj

1) < ci

C(s, a, sj)

(ci

−

≤

ci

1)

−

−

C(s, a, sj

1)

−

(cid:1)

where 1 is a binary function that outputs one if and only if its condition holds. We reconstructed the transition probabilities
using distribution g and deterministic functions f1, ..., f

.

L
|

|

Claim 2. Given a deterministic and linear transition model, and a linear reward signal, the bounds provided in Theorems 1
and 2 are both tight.

Assume a linear transition function T deﬁned as:

Assume our learned transition function ˆT :

Note that:

and that:

T (s) = Ks

ˆT (s) := Ks + ∆

max
s

T (s)

−

ˆT (s)

= ∆

(cid:12)
(cid:12)
KT , K ˆT }
min
{

(cid:12)
(cid:12)
= K

First observe that the bound in Theorem 2 is tight for n = 2:

T

T (s)

ˆT

ˆT (s)

=

K 2s

s
∀

(cid:12)
(cid:12)
(cid:12)

−

(cid:0)

(cid:1)

(cid:0)

(cid:1)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

−

K 2s + ∆(1 + K)
(cid:12)
(cid:12)
(cid:12)

= ∆

K i

1

i=0
(cid:88)

Lipschitz Continuity in Model-based Reinforcement Learning

and more generally and after n compositions of the models, denoted by T n and ˆT n, the following equality holds:

Lets further assume that the reward is linear:

s
∀

T n(s)

−

(cid:12)
(cid:12)
(cid:12)

= ∆

K i

n

1

−

i=0
(cid:88)

ˆT n(s)
(cid:12)
(cid:12)
(cid:12)

R(s) = KRs

Consider the state s = 0. Note that clearly v(0) = 0. We now compute the value predicted using ˆT , denoted by ˆv(0):

ˆv(0) = R(0) + γR(0 + ∆

K i) + γ2R(0 + ∆

K i) + γ3R(0 + ∆

K i) + ...

0

i=0
(cid:88)

2

i=0
(cid:88)

1

i=0
(cid:88)

2

i=0
(cid:88)

= 0 + γKR∆

K i + γ2KR∆

K i) + γ3KR∆

K i + ...

0

1

= γKR∆

K i =

i=0
(cid:88)
n
γn

1

−

∞

n=0
(cid:88)

i=0
(cid:88)

i=0
(cid:88)
γKR∆
γ)(1

−

(1

−

,

γ ¯K)

and so:

v(0)
|

ˆv(0)
|

−

=

(1

γKR∆
γ)(1

−

−

γ ¯K)

Note that this exactly matches the bound derived in our Theorem 2.

Lemma 1. A generalized transition function

induced by a Lipschitz model class Fg is Lipschitz with a constant:

T
G

Proof.

W

T (

µ1, a),

T (

µ2, a)

· |

(cid:0)

(cid:98)

· |

(cid:98)

(cid:1)

K AW,W (

T
G

(cid:98)
) := sup

sup
µ1,µ2

a

W

T
G

(
µ1, a),
(
T
·|
G
W (µ1, µ2)

·|

µ2, a)

(cid:0)

(cid:98)

(cid:98)

KF

≤

(cid:1)

(cid:98)

:= inf
j

= inf
j

= inf
j

j(s(cid:48)1, s(cid:48)2)d(s(cid:48)1, s(cid:48)2)ds(cid:48)1ds(cid:48)2

(cid:90)s(cid:48)

1 (cid:90)s(cid:48)

2

1

f (s1) = s(cid:48)1 ∧

(cid:90)s1 (cid:90)s2 (cid:90)s(cid:48)

1 (cid:90)s(cid:48)

2

f
(cid:88)
j(s1, s2, f )d

(cid:0)

(cid:90)s1 (cid:90)s2

f
(cid:88)

(cid:1)

f (s1), f (s2)

ds1ds2

(cid:0)

(cid:1)

KF inf
j

≤

g(f

a)j(s1, s2)d(s1, s2)ds1ds2
|

= KF

g(f

j(s1, s2)d(s1, s2)ds1ds2

(cid:90)s1 (cid:90)s2

f
(cid:88)
a) inf
j

(cid:90)s1 (cid:90)s2

|

|

f
(cid:88)

f
(cid:88)

= KF

g(f

a)W (µ1, µ2) = KF W (µ1, µ2)

Dividing by W (µ1, µ2) and taking sup over a, µ1, and µ2, we conclude:

K AW,W (

T ) = sup

sup
µ1,µ2

a

(cid:98)

W

T (

· |

µ1, a),
T (
· |
W (µ1, µ2)

µ2, a)

(cid:0)

(cid:98)

(cid:98)

KF .

≤

(cid:1)

We can also prove this using the Kantarovich-Rubinstein duality theorem:

f (s2) = s(cid:48)2

j(s1, s2, f )d(s(cid:48)1, s(cid:48)2)ds(cid:48)1ds(cid:48)2ds1ds2

Lipschitz Continuity in Model-based Reinforcement Learning

For every µ1, µ2, and a

we have:

∈ A

W

T
G

(
· |

µ1, a),

T
G

(
· |

µ2, a)

(cid:0)

(cid:98)

(cid:98)

(cid:1)

=

=

=

=

=

=

≤

≤

sup

sup

sup

sup

f :KdS ,R(f )

1 (cid:90)s

≤

(cid:0)

(cid:98)
1 (cid:90)s (cid:90)s0 (cid:16)

f :KdS ,R(f )

≤

T
G

(s
|

µ1, a)

T
G

µ2, a)
(s
|

−

f (s)ds

(cid:98)
s0, a)µ1(s0)
T (s
|

−

(cid:1)
s0, a)µ2(s0)
T (s
|

f (s)dsds0

(cid:98)
s0, a)
T (s
|

µ1(s0)

−

(cid:98)
µ2(s0)

(cid:17)
f (s)dsds0

f :KdS ,R(f )

1 (cid:90)s (cid:90)s0

≤

f :KdS ,R(f )

1 (cid:90)s (cid:90)s0

≤

sup

f :KdS ,R(f )

sup

f :KdS ,R(f )

1

≤

t
(cid:88)

1

≤

t
(cid:88)

g(t

a)

|

(cid:98)

|

|

t
(cid:88)
a)

g(t

g(t

a)

(cid:16)
a)1

g(t

|

(cid:0)
1

(cid:90)s0 (cid:90)s

(cid:90)s0

(cid:0)

−

−

(cid:17)
µ1(s0)

(cid:1)(cid:16)

−

−

t(s0) = s

µ1(s0)

µ2(s0)

(cid:0)
µ1(s0)

(cid:1)(cid:0)
µ2(s0)

f

t(s0)

ds0

(cid:1)

sup

µ1(s0)

(cid:1)
µ2(s0)

f

(cid:0)

(cid:1)

t(s0)

ds0

t
(cid:88)
composition of f, t is Lipschitz with constant upper bounded by KF .

(cid:1)

(cid:0)

(cid:1)

(cid:0)

f :KdS ,R(f )

1 (cid:90)s0
≤

t(s0) = s

µ2(s0)

f (s)dsds0

(cid:17)
f (s)dsds0

= KF

g(t

a)

sup

µ1(s0)

µ2(s0)

f :KdS ,R(f )

1 (cid:90)s0
≤

KF

g(t

a)

sup

= KF

g(t

h:KdS ,R(h)

1 (cid:90)s0
≤
a)W (µ1, µ2) = KF W (µ1, µ2)

(cid:0)

(cid:1)

f (t(s0))
KF

ds0

(cid:0)
µ1(s0)

(cid:1)
h(s0))ds0

µ2(s0)

−

−

|

|

|

t
(cid:88)

t
(cid:88)

t
(cid:88)

Again we conclude by dividing by W (µ1, µ2) and taking sup over a, µ1, and µ2.

Lemma 2. (Composition Lemma) Deﬁne three metric spaces (M1, d1), (M2, d2), and (M3, d3). Deﬁne Lipschitz functions
M3 is Lipschitz with
f : M2 (cid:55)→
constant Kd1,d3 (h)

M2 with constants Kd2,d3 (f ) and Kd1,d2 (g). Then, h : f

Kd2,d3 (f )Kd1,d2(g).

g : M1 (cid:55)→

◦

M3 and g : M1 (cid:55)→
≤

Proof.

Kd1,d3 (h) = sup
s1,s2

d3

f

g(s1)

, f

g(s2)

(cid:16)

(cid:0)

d1(s1, s2)
(cid:0)
(cid:1)

(cid:1)(cid:17)

= sup
s1,s2

d2

g(s1), g(s2)
d1(s1, s2)
(cid:0)

d2

(cid:1)
g(s1), g(s2)
d1(s1, s2)
≤
(cid:0)
(cid:1)
= Kd1,d2(g)Kd2,d3 (f ).

sup
s1,s2

d3

f

g(s1)

, f

g(s2)

(cid:1)(cid:17)

(cid:16)

sup
s1,s2

d2
(cid:0)
d3
(cid:0)

g(s1), g(s2)
(cid:1)
(cid:0)
f (s1), f (s2)
(cid:1)
d2(s1, s2)

(cid:0)

(cid:1)

Lemma 3. Given a Lipschitz function f :

R with constant KdS ,dR(f ):

S (cid:55)→

K AdS ,dR

T (s(cid:48)

s, a)f (s(cid:48))ds(cid:48)

KdS ,dR (f )K AdS ,W

T

.

|

(cid:16) (cid:90)

(cid:98)

≤

(cid:17)

(cid:0)

(cid:1)

(cid:98)

Proof.

K AdS ,dR

T (s(cid:48)

s, a)f (s(cid:48))ds(cid:48)

= sup

|

(cid:16) (cid:90)s(cid:48)

(cid:98)

Lipschitz Continuity in Model-based Reinforcement Learning

(cid:17)

sup
s1,s2

(cid:82)

(cid:0)

s(cid:48)

s(cid:48)

|

|

T (s(cid:48)

s1, a)
|

(cid:98)
T (s(cid:48)

s1, a)
|

a

a

= sup

sup
s1,s2

(cid:82)

(cid:0)

(cid:98)

= KdS ,dR(f ) sup
a

sup
s1,s2

s2, a)
T (s(cid:48)
−
|
d(s1, s2)
(cid:98)
T (s(cid:48)

s2, a)
|
d(s1, s2)
(cid:98)
T (s(cid:48)

s1, a)

−

(cid:1)

(cid:1)

f (s(cid:48))ds(cid:48)

|

f (s(cid:48))

KdS ,dR (f )
KdS ,dR (f ) ds(cid:48)

|

s(cid:48)

|

(cid:0)

(cid:82)
(cid:98)
supg:KdS ,dR (g)
≤

1

|

|

supg:KdS ,dR (g)
≤

1

s2, a)
|

T (s(cid:48)

−
d(s1, s2)

(cid:98)
s(cid:48)

(cid:82)

T (s(cid:48)

(cid:1)
s1, a)
|
d(s1, s2)
(cid:0)
(cid:98)
T (s(cid:48)

s(cid:48)

s1, a)
|
d(s1, s2)
(cid:0)
(cid:98)
s2, a)

(cid:82)

·|

(cid:1)

f (s(cid:48))
KdS ,dR (f ) ds(cid:48)

|

T (s(cid:48)

s2, a)
|

−

g(s(cid:48))ds(cid:48)

|

(cid:1)
g(s(cid:48))ds(cid:48)

(cid:98)
T (s(cid:48)

−

s2, a)
|

(cid:98)

(cid:1)

KdS ,dR(f ) sup
a

sup
s1,s2

≤

= KdS ,dR(f ) sup
a

sup
s1,s2

= KdS ,dR(f ) sup
a

sup
s1,s2

= KdS ,dR(f )K AdS ,W (

T ) .

W

T (

·|

s1, a),
T (
d(s1, s2)
(cid:98)

(cid:0)

(cid:98)

(cid:98)

Lemma 4. The following operators (Asadi & Littman, 2017) are Lipschitz with constants:

(cid:107)(cid:107)∞,dR

mean(x)

= K

(cid:107)(cid:107)∞,dR ((cid:15)-greedy(x)) = 1

1. K

2. K

(cid:107)(cid:107)∞,dR (max(x)) = K
(cid:107)(cid:107)∞,dR (mmβ(x) := log

(cid:80)

(cid:0)
i eβxi
n

β

) = 1

(cid:1)

3. K

(cid:107)(cid:107)∞,dR (boltzβ(x) :=

(cid:80)n

i=1 xieβxi
(cid:80)n
i=1eβ xi

)

A
|

|

A
+ βVmax|

|

≤

(cid:112)

and observe that boltzβ(x) = x(cid:62)ρ(x). Gao & Pavel (2017) showed that ρ is Lipschitz:

(cid:80)

ρ(x)i =

eβxi
n
i=1 eβxi

,

ρ(x2)

ρ(x1)
(cid:107)

x2(cid:107)2
Using their result, we can further show:

x1 −
(cid:107)

(cid:107)2 ≤

−

β

Proof. 1 was proven by Littman & Szepesv´ari (1996), and 2 is proven several times (Fox et al., 2016; Asadi & Littman,
2017; Nachum et al., 2017; Neu et al., 2017). We focus on proving 3. Deﬁne

(7)

ρ(x1)(cid:62)x2 −

|

ρ(x2)(cid:62)x2|

(Cauchy-Shwartz)

ρ(x2)(cid:62)x2|

|

ρ(x1)(cid:62)x1 −
≤ |

+

ρ(x1)(cid:62)x1 −
ρ(x1)
(cid:107)2 (cid:107)
ρ(x1)
x2(cid:107)2 (cid:107)
(cid:107)
ρ(x1)
x2(cid:107)2 β
(cid:107)
(1 + βVmax

x1 −
−
x1 −
(cid:107)2 (cid:107)
x1 −
(cid:107)

ρ(x1)(cid:62)x2|
x2(cid:107)2
ρ(x2)
x2(cid:107)2
x2(cid:107)2
x1 −
)
A
(cid:0)
|
(cid:107)
|
x1 −
)
A
+ βVmax|
(cid:112)
(cid:107)
|

A
|

(cid:107)2

(

≤ (cid:107)
+

≤ (cid:107)
+

≤

≤
|
(cid:112)
leads to 3.

from Eqn 7)
x2(cid:107)2
x2(cid:107)∞

,

(cid:1)

dividing both sides by

x1 −
(cid:107)

x2(cid:107)∞

Lipschitz Continuity in Model-based Reinforcement Learning

Below, we derive the Lipschitz constant for various functions mentioned in Table 1.
Rn has Lipschitz constant 1 for p.
ReLu non-linearity We show that ReLu : Rn

→

K
(cid:107)

.
(cid:107)

p,

.
(cid:107)

(cid:107)

p (ReLu) = sup
x1,x2

= sup
x1,x2
(cid:107)
(We can show that

(cid:80)

1
p

p)
|

p

p

p

(

i |

ReLu(x1)
(cid:107)

ReLu(x2)
−
(cid:107)
x2(cid:107)
x1 −
(cid:107)
ReLu(x1)i
ReLu(x2)i
−
x2(cid:107)
x1 −
ReLu(x1)i
|
x1,i
x2,i
i |
−
x1 −
x2(cid:107)
(cid:80)
(cid:107)
x2(cid:107)
x1 −
= 1
(cid:107)
x2(cid:107)
x1 −
(cid:107)

p)
|

−

(

1
p

p

p

p

sup
x1,x2

≤

= sup
x1,x2

ReLu(x2)i

x1,i

x2,i

and so) :

| ≤ |

−

|

Matrix multiplication Let W

Rn

×

m. We derive the Lipschitz continuity for the function

W (x) = W x.

×

For p =

we have:

∞

∈

where Wj refers to jth row of the weight matrix W . Similarly, for p = 1 we have:

K

(cid:107)(cid:107)∞,

= sup
x1,x2

= sup
x1,x2

sup
≤
x1,x2
= sup

j (cid:107)

W (x1)

(cid:107)(cid:107)∞

×
W (x1)
(cid:0)
(cid:107)×

W (x2)
(cid:1)
− ×
x2(cid:107)∞
x2)

|

x1 −
(cid:107)
Wj(x1 −
supj |
x2(cid:107)∞
x1 −
(cid:107)
x1 −
Wj
supj (cid:107)
(cid:107) (cid:107)
x2(cid:107)∞
x1 −
(cid:107)
(cid:107)1 ,

Wj

x2(cid:107)∞

W (x2)
x2(cid:107)1
x2)
|

K

(cid:107)(cid:107)1,

(cid:107)(cid:107)1

= sup
x1,x2

(cid:0)
(cid:107)×

= sup

x1,x2 (cid:80)

≤

sup
x1,x2 (cid:80)

W (x1)

×
W (x1)

(cid:1)
− ×
x1 −
(cid:107)
Wj(x1 −
j |
x2(cid:107)1
x1 −
(cid:107)
Wj
j (cid:107)
(cid:107)∞ (cid:107)
x1 −
(cid:107)

K

(cid:107)(cid:107)2,

(cid:107)(cid:107)2

= sup
x1,x2

(cid:0)
(cid:107)×

W (x1)

×
W (x1)

2
|

x2)

W (x2)
(cid:1)
− ×
x2(cid:107)2
x1 −
(cid:107)
Wj(x1 −
j |
x2(cid:107)2
x1 −
(cid:107)
2
Wj
x1 −
j (cid:107)
2 (cid:107)
(cid:107)
x2(cid:107)2
x1 −
(cid:107)

= sup

x1,x2 (cid:113)(cid:80)

sup
x1,x2 (cid:113)(cid:80)

≤

(cid:107)∞

= sup
x1,x2

W x1 −
(cid:107)
x1 −
(cid:107)

W x2(cid:107)∞
x2(cid:107)∞

= sup
x1,x2

W (x1 −
(cid:107)
x1 −
(cid:107)

x2)
x2(cid:107)∞

(cid:107)∞

(H¨older’s inequality)

(cid:107)1

= sup
x1,x2

(cid:107)

W x1 −
x1 −
(cid:107)

W x2(cid:107)1
x2(cid:107)1

= sup
x1,x2

(cid:107)

W (x1 −
x1 −
(cid:107)

x2)
x2(cid:107)1

(cid:107)1

(cid:107)2

= sup
x1,x2

(cid:107)

W x1 −
x1 −
(cid:107)

W x2(cid:107)2
x2(cid:107)2

= sup
x1,x2

(cid:107)

W (x1 −
x1 −
(cid:107)

x2)
x2(cid:107)2

(cid:107)2

2
2

x2(cid:107)

=

Wj
(cid:107)

2
2
(cid:107)

.

j

(cid:115)(cid:88)

and ﬁnally for p = 2:

x2(cid:107)1

=

x1 −
x2(cid:107)1

j
(cid:88)

Wj
(cid:107)

(cid:107)∞

,

Lipschitz Continuity in Model-based Reinforcement Learning

Vector addition We show that +b : Rn

Rn has Lipschitz constant 1 for p = 0, 1,

for all b

Rn.

→

∞

∈

K
(cid:107)

.
(cid:107)

p,

.
(cid:107)

(cid:107)

p (ReLu) = sup
x1,x2

(cid:107)

+ b(x1)
−
x1 −
(cid:107)
(x1 + b)
−
(cid:107)
x1 −
(cid:107)

+b(x2)
p
(cid:107)
x2(cid:107)
p
(x2 + b)
p
(cid:107)
x2(cid:107)

p

= sup
x1,x2

x1 −
= (cid:107)
x1 −
(cid:107)

x2(cid:107)
x2(cid:107)

p

p

= 1

Supervised-learning domain We used the following 5 functions to generate the dataset:

f0(x) = tanh(x) + 3
x
f1(x) = x
f2(x) = sin(x)
f3(x) = sin(x)
f4(x) = sin(x)

−

−

5

∗

3
sin(x)

∗

We sampled each function 30 times, where the input was chosen uniformly randomly from [

2, 2] each time.

−

Lipschitz Continuity in Model-based Reinforcement Learning

Kavosh Asadi * 1 Dipendra Misra * 2 Michael L. Littman 1

8
1
0
2
 
l
u
J
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
3
9
1
7
0
.
4
0
8
1
:
v
i
X
r
a

Abstract
We examine the impact of learning Lipschitz
continuous models in the context of model-based
reinforcement learning. We provide a novel bound
on multi-step prediction error of Lipschitz models
where we quantify the error using the Wasserstein
metric. We go on to prove an error bound for
the value-function estimate arising from Lipschitz
models and show that the estimated value function
is itself Lipschitz. We conclude with empirical
results that show the beneﬁts of controlling the
Lipschitz constant of neural-network models.

1. Introduction

The model-based approach to reinforcement learning (RL)
focuses on predicting the dynamics of the environment
to plan and make high-quality decisions (Kaelbling et al.,
1996; Sutton & Barto, 1998). Although the behavior of
model-based algorithms in tabular environments is well
understood and can be effective (Sutton & Barto, 1998),
scaling up to the approximate setting can cause instabilities.
Even small model errors can be magniﬁed by the planning
process resulting in poor performance (Talvitie, 2014).

In this paper, we study model-based RL through the lens of
Lipschitz continuity, intuitively related to the smoothness
of a function. We show that the ability of a model to make
accurate multi-step predictions is related to the model’s
one-step accuracy, but also to the magnitude of the Lipschitz
constant (smoothness) of the model. We further show that
the dependence on the Lipschitz constant carries over to the
value-prediction problem, ultimately inﬂuencing the quality
of the policy found by planning.

We consider a setting with continuous state spaces and
stochastic transitions where we quantify the distance
between distributions using the Wasserstein metric. We

*Equal contribution

1Department of Computer Science,
Brown University, Providence, USA 2Department of Computer
Science and Cornell Tech, Cornell University, New York, USA.
Correspondence to: Kavosh Asadi <kavosh@brown.edu>.

introduce a novel characterization of models, referred
to as a Lipschitz model class, that represents stochastic
dynamics using a set of component deterministic functions.
This allows us to study any stochastic dynamic using
the Lipschitz continuity of its component deterministic
functions. To learn a Lipschitz model class in continuous
state spaces, we provide an Expectation-Maximization
algorithm (Dempster et al., 1977).

the learned models or

One promising direction for mitigating the effects of
inaccurate models is the idea of limiting the complexity
of
reducing the horizon of
planning (Jiang et al., 2015). Doing so can sometimes
make models more useful, much as regularization in
supervised learning can improve generalization performance
(Tibshirani, 1996). In this work, we also examine a type
of regularization that comes from controlling the Lipschitz
constant of models. This regularization technique can be
applied efﬁciently, as we will show, when we represent the
transition model by neural networks.

2. Background

,

A

, R, T, γ

We consider the Markov decision process (MDP) setting
in which the RL problem is formulated by the tuple
we mean a continuous state
. Here, by
(cid:105)
(cid:104)S
we mean a discrete action set. The functions
space and by
A
R and T :
) denote the reward
Pr(
R : S
S
S × A →
and transition dynamics. Finally, γ
[0, 1) is the discount
∈
rate. If
= 1, the setting is called a Markov reward
process (MRP).

|A|

→

×

A

S

2.1. Lipschitz Continuity

Our analyses leverage the “smoothness” of various
functions, quantiﬁed as follows.

Deﬁnition 1. Given two metric spaces (M1, d1) and
(M2, d2) consisting of a space and a distance metric, a
M2 is Lipschitz continuous (sometimes
function f : M1 (cid:55)→
simply Lipschitz) if the Lipschitz constant, deﬁned as

Kd1,d2(f ) :=

sup
M1,s2

s1

∈

d2

f (s1), f (s2)
d1(s1, s2)

,

(cid:1)

M1

∈

(cid:0)

(1)

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

is ﬁnite.

Lipschitz Continuity in Model-based Reinforcement Learning

Sometimes referred to as “Earth Mover’s distance”,
Wasserstein is the minimum expected distance between
pairs of points where the joint distribution j is constrained
to match the marginals µ1 and µ2. New applications of
this metric are discovered in machine learning, namely in
the context of generative adversarial networks (Arjovsky
et al., 2017) and value distributions in reinforcement
learning (Bellemare et al., 2017).

Wasserstein is linked to Lipschitz continuity using duality:

W (µ1, µ2) =

sup

f (s)µ1(s)

f (s)µ2(s)

ds .

f :Kd,dR (f )

1 (cid:90)

≤

(cid:0)

−

(cid:1)

(4)

This equivalence, known as Kantorovich-Rubinstein duality
(Villani, 2008), lets us compute Wasserstein by maximizing
R, a relatively
over a Lipschitz set of functions f :
In our theory, we utilize both
easier problem to solve.
deﬁnitions, namely the primal deﬁnition (3) and the dual
deﬁnition (4).

S (cid:55)→

3. Lipschitz Model Class

We introduce a novel representation of stochastic MDP
transitions in terms of a distribution over a set of
deterministic components.
Deﬁnition 4. Given a metric state space (
action space
Fg =
a

) and an
, we deﬁne Fg as a collection of functions:
a) where

f :
{
. We say that Fg is a Lipschitz model class if
∈ A

distributed according to g(f

A
S (cid:55)→ S}

, d
S

S

|

KF := sup
Fg

f

∈

KdS ,dS (f ) ,

Our deﬁnition captures a subset of stochastic transitions,
namely ones that can be represented as a state-independent
distribution over deterministic transitions. An example is
provided in Figure 2. We further prove in the appendix (see
Claim 1) that any ﬁnite MDP transition probabilities can be
decomposed into a state-independent distribution g over a
ﬁnite set of deterministic functions f .

Associated with a Lipschitz model class is a transition
function given by:

T (s(cid:48)

s, a) =

|

(cid:98)

f
(cid:88)

(cid:0)

1

f (s) = s(cid:48)

g(f

a) .

|

(cid:1)

Given a state distribution µ(s), we also deﬁne a generalized
notion of transition function
T
G

µ, a) given by:

(
· |

Figure 1. An illustration of Lipschitz continuity.
Pictorially,
Lipschitz continuity ensures that f lies in between the two afﬁne
functions (colored in blue) with slopes K and −K.

Equivalently, for a Lipschitz f ,

s1,

s2

d2

f (s1), f (s2)

Kd1,d2(f ) d1(s1, s2) .

∀

∀

≤

(cid:1)
The concept of Lipschitz continuity is visualized in Figure 1.

(cid:0)

A Lipschitz function f is called a non-expansion when
Kd1,d2(f ) = 1 and a contraction when Kd1,d2(f ) < 1.
Lipschitz continuity, in one form or another, has been a
key tool in the theory of reinforcement learning (Bertsekas,
1975; Bertsekas & Tsitsiklis, 1995; Littman & Szepesv´ari,
1996; M¨uller, 1996; Ferns et al., 2004; Hinderer, 2005;
Rachelson & Lagoudakis, 2010; Szepesv´ari, 2010; Pazis
& Parr, 2013; Pirotta et al., 2015; Pires & Szepesv´ari,
2016; Berkenkamp et al., 2017; Bellemare et al., 2017) and
bandits (Kleinberg et al., 2008; Bubeck et al., 2011). Below,
we also deﬁne Lipschitz continuity over a subset of inputs.
Deﬁnition 2. A function f : M1 × A (cid:55)→
Lipschitz continuous in

M2 is uniformly

if

A

is ﬁnite.

K Ad1,d2(f ) := sup
∈A

a

sup
s1,s2

d2

f (s1, a), f (s2, a)
d1(s1, s2)

(cid:0)

(cid:1)

,

(2)

is ﬁnite.

Note that the metric d1 is deﬁned only on M1.

2.2. Wasserstein Metric

We quantify the distance between two distributions using
the following metric:

Deﬁnition 3. Given a metric space (M, d) and the set
P(M ) of all probability measures on M , the Wasserstein
metric (or the 1st Kantorovic metric) between two
probability distributions µ1 and µ2 in P(M ) is deﬁned as

W (µ1, µ2) := inf
Λ
∈

j

(cid:90) (cid:90)

j(s1, s2)d(s1, s2)ds2 ds1 , (3)

where Λ denotes the collection of all joint distributions j on
M with marginals µ1 and µ2 (Vaserstein, 1969).
M

(cid:98)

×

(s(cid:48)

T
G

|

µ, a) =

1

f (s) = s(cid:48)
(cid:98)

g(f

a)

µ(s)ds .

(cid:90)s

f
(cid:88)

(cid:0)

(cid:124)

(cid:98)T (s(cid:48)

(cid:1)
s,a)
|
(cid:123)(cid:122)

|

(cid:125)

Lipschitz Continuity in Model-based Reinforcement Learning

Figure 2. An example of a Lipschitz model class in a gridworld
environment (Russell & Norvig, 1995). The dynamics are such
that any action choice results in an attempted transition in the
corresponding direction with probability 0.8 and in the neighboring
directions with probabilities 0.1 and 0.1. We can deﬁne Fg =
{f up, f right, f down, f left} where each f outputs a deterministic
next position in the grid (factoring in obstacles). For a = up,
we have: g(f up | a = up) = 0.8, g(f right
| a = up) =
g(f left | a = up) = 0.1, and g(f down | a = up) = 0. Deﬁning
distances between states as their Manhattan distance in the grid,
(cid:0)d(f (s1), f (s2)(cid:1)/d(s1, s2) = 2, and so KF =
then ∀f sups1,s2
2. So, the four functions and g comprise a Lipschitz model class.

T
G

. However, since

), the Lipschitz
We are primarily interested in K Ad,d(
T
G
constant of
takes as input a
T
G
(cid:98)
probability distribution and also outputs a probability
distribution, we require a notion of distance between two
distributions. This notion is quantiﬁed using Wasserstein
and is justiﬁed in the next section.

(cid:98)

(cid:98)

4. On the Choice of Probability Metric

We consider the stochastic model-based setting and show
through an example that
the Wasserstein metric is a
reasonable choice compared to other common options.

Consider a uniform distribution over states µ(s) as shown
in black in Figure 3 (top). Take a transition function T
in
G
the environment that, given an action a, uniformly randomly
adds or subtracts a scalar c1. The distribution of states
after one transition is shown in red in Figure 3 (middle).
Now, consider a transition model
that approximates T
G
by uniformly randomly adding or subtracting the scalar
c2. The distribution over states after one transition using
this imperfect model is shown in blue in Figure 3 (bottom).
We desire a metric that captures the similarity between the
output of the two transition functions. We ﬁrst consider
Kullback-Leibler (KL) divergence and observe that:

T
G

(cid:98)

KL

T
G

:=

(cid:0)

(cid:90)

(
· |
(s(cid:48)

T
G

µ, a),

(

T
G

· |
T
µ, a) log
G
(cid:98)
T
G

µ, a)
(s(cid:48)
(cid:1)
|
(s(cid:48) |

|

µ, a)

µ, a)

ds(cid:48) =

,

∞

unless the two constants are exactly the same.

(cid:98)

Figure 3. A state distribution µ(s) (top), a stochastic environment
that randomly adds or subtracts c1 (middle), and an approximate
transition model that randomly adds or subtracts a second scalar
c2 (bottom).

The next possible choice is Total Variation (TV) deﬁned as:

µ, a),

µ, a)

T
G

T V

:=

1
(cid:0)
2

(
· |
T
G

(s(cid:48)

|

(
· |

T
G
µ, a)
(cid:98)

(cid:1)
(s(cid:48)

T
G

−

|

µ, a)

ds(cid:48) = 1 ,

if the two distributions have disjoint supports regardless of
how far the supports are from each other.

(cid:98)

(cid:12)
(cid:12)

(cid:90)

(cid:12)
(cid:12)

In contrast, Wasserstein is sensitive to how far the constants
are as:

µ, a)

=

c1 −
|

c2|

.

W

T
G

(
· |

µ, a),

(

T
G

(cid:0)

· |
It is clear that, of the three, Wasserstein corresponds best
to the intuitive sense of how closely T
approximates
G
. This is particularly important in high-dimensional
T
G
spaces where the true distribution is known to usually lie in
low-dimensional manifolds. (Narayanan & Mitter, 2010)
(cid:98)

(cid:98)

(cid:1)

5. Understanding the Compounding Error

Phenomenon

−

To extract a prediction with a horizon n > 1, model-based
algorithms typically apply the model for n steps by taking
the state input in step t to be the state output from
the step t
1. Previous work has shown that model
error can result in poor long-horizon predictions and
ineffective planning (Talvitie, 2014; 2017). Observed even
beyond reinforcement learning (Lorenz, 1972; Venkatraman
et al., 2015), this is referred to as the compounding error
phenomenon. The goal of this section is to provide a bound
on multi-step prediction error of a model. We formalize the
notion of model accuracy below:

Deﬁnition 5. Given an MDP with a transition function
T , we identify a Lipschitz model Fg as ∆-accurate if its
induced

T satisﬁes:

s
∀

a W
(cid:98)
∀

· |

T (

s, a), T (

s, a)

∆ .

· |

≤

(cid:1)

(cid:0)

(cid:98)

Lipschitz Continuity in Model-based Reinforcement Learning

We want to express the multi-step Wasserstein error in
terms of the single-step Wasserstein error and the Lipschitz
constant of the transition function
. We provide a bound
using the following lemma:
on the Lipschitz constant of

T
G

Lemma 1. A generalized transition function
(cid:98)
a Lipschitz model class Fg is Lipschitz with a constant:

T
G

induced by

T
G

(cid:98)

(cid:98)

K AW,W (

T
G

) := sup

sup
µ1,µ2

a

(cid:98)

W

(

T
G

(
µ1, a),
T
·|
·|
G
W (µ1, µ2)

µ2, a)

KF

≤

(cid:1)

(cid:0)

(cid:98)

(cid:98)

the two input
Intuitively, Lemma 1 states that,
distributions are similar, then for any action the output
distributions given by
are also similar up to a KF factor.
We prove this lemma, as well as the subsequent lemmas, in
the appendix.

T
G

if

(cid:98)

Given the one-step error (Deﬁnition 5), a start state
distribution µ and a ﬁxed sequence of actions a0, ..., an
1,
we desire a bound on n-step error:

−

δ(n) := W

T n
G

(
· |

µ), T n
G

(

· |

µ)

,

where

T n
G

(
·|

µ) :=

(cid:0)
(cid:98)
T
G

T
G

(
·|

(cid:1)

...

(
·|

µ, a0)..., an

(
·|

T
G
n recursive calls

−

2), an

1)

−

(cid:98)
(cid:98)
and T n
(
· |
G
lemma followed by the theorem.

µ) is deﬁned similarly. We provide a useful
(cid:125)

(cid:98)
(cid:124)

(cid:123)(cid:122)

(cid:98)

Lemma 2. (Composition Lemma) Deﬁne three metric
spaces (M1, d1), (M2, d2), and (M3, d3). Deﬁne Lipschitz
M2 with constants
M3 and g : M1 (cid:55)→
functions f : M2 (cid:55)→
Kd2,d3(f ) and Kd1,d2 (g). Then, h : f
M3 is
g : M1 (cid:55)→
Kd2,d3 (f )Kd1,d2(g).
Lipschitz with constant Kd1,d3 (h)

◦

≤

Similar to composition, we can show that summation
preserves Lipschitz continuity with a constant bounded by
the sum of the Lipschitz constants of the two functions. We
omitted this result due to brevity.

Theorem 1. Deﬁne a ∆-accurate
with the Lipschitz
constant KF and an MDP with a Lipschitz transition
with constant KT . Let ¯K = min
function T
.
KF , KT
}
{
G
1:
Then
≥

T
G

n

∀

(cid:98)

δ(n) := W

T n
G

(
· |

µ), T n
G

(

· |

µ)

∆

≤

( ¯K)i .

n

1

−

i=0
(cid:88)

(cid:1)

(cid:0)

(cid:98)

(cid:1)
s, a0)

f (s(cid:48))µ(s) ds ds(cid:48)

(cid:1)
f (s(cid:48)) ds(cid:48)

µ(s) ds

δ(1) := W

:= sup

µ, a0)

(

T
· |
G
T (s(cid:48)
(cid:98)

|

(

µ, a0), T
· |
G
T (s(cid:48)

s, a0)

−

|

(cid:0)
f (cid:90) (cid:90)
sup
f (cid:90)

(cid:0)
=W

(cid:98)

(cid:0)
(cid:98)
T (s(cid:48)

s, a0)
|

−

T (s(cid:48)

s, a0)
|

(cid:1)
due to duality (4)

(cid:1)
s, a0)

µ(s) ds

(cid:125)

(cid:124)
W

(cid:98)T (

s,a0),T (

·|

s,a0)
·|
(cid:123)(cid:122)

s, a0), T (

(cid:0)
T (
· |
∆ due to Deﬁnition 5

· |

(cid:0)

≤

(cid:98)

(cid:124)
∆ µ(s) ds = ∆ .

(cid:123)(cid:122)

(cid:1)

(cid:125)

≤

(cid:90)

=

(cid:90)

≤

(cid:90)

1

−

We now prove the inductive step. Assuming δ(n
T n
W
(
G
write:
(cid:0)
(cid:98)
δ(n) := W

µ), T n
G

(
· |

(cid:1)
µ)

(cid:80)

µ)

· |

∆

≤

−

1

1) :=
i=0 (KF )i we can

−

−

n

2

T n
(
· |
G
T
µ),
G

(

µ), T n
· |
G
T n
−
· |
G
µ), an

−

1
(
(cid:98)

(cid:0)
· |

1

−

(
· |

1

(

(cid:1)
· |

µ), an

1

−

1

−

, T n
G

(
· |

(cid:1)
1),

T
G

· |

W

≤
+W

T n
G

(cid:16)
T
(cid:98)
G

= W

+W

(cid:16)

T
(cid:98)
G

(cid:0)
(
· |

(cid:16)
T
(cid:98)
G
(cid:0)

· |

· |

(cid:0)
(
(cid:98)
· |
T n
G
T n
G
T n
(cid:98)
G

µ), an

−

1

1

−

(
· |

µ), an

, T
(cid:98)
G

(cid:0)
(
· |

(cid:17)
T n
−
G
T n
G

−

1

1

(

· |

(

· |

µ), an

1

−

µ), an

1)

(cid:16)

−
We now use Lemma 1 and Deﬁnition 5 to upper bound the
ﬁrst and the second term of the last line respectively.

(cid:98)

(cid:1)

−

(cid:1)(cid:17)

(cid:17)

(cid:1)(cid:17)

µ)

(Triangle ineq)

δ(n)

KF W

≤

(cid:0)
= KF δ(n

1

−

T n
G

(
· |

1

µ), T n
G
n

−

(cid:98)
−

1) + ∆

∆

≤

+ ∆

µ)

(
· |
1
(KF )i .

(cid:1)

−

(5)

i=0
(cid:88)

in the triangle inequality, we may replace
T n
1
and follow
G

(
· |

µ)

µ)

· |

Note that
T n
with T
(
T
· |
G
G
G
the same basic steps to get:
(cid:98)

· |

(cid:98)

(cid:0)

(cid:0)

(cid:1)

(cid:1)

−

−

1

W

(

T n
G

· |

µ), T n
G

(
· |

µ)

(cid:0)

(cid:98)

n

−

1
(KT )i .

∆

≤

(cid:1)

i=0
(cid:88)

Combining (5) and (6) allows us to write:

(6)

δ(n) = W

T n
G

(
· |
n

(cid:0)
min

∆

(
· |

µ), T n
G
1
(KT )i, ∆

µ)
n
(cid:1)
−

−

1
(KF )i

(cid:41)

i=0
(cid:88)

≤

= ∆

(cid:98)
(cid:40)
1

n

−

i=0
(cid:88)

i=0
(cid:88)
( ¯K)i ,

which concludes the proof.

Proof. We construct a proof by induction.
Using
Kantarovich-Rubinstein duality (Lipschitz property of f
not shown for brevity) we ﬁrst prove the base of induction:

There exist similar
results in the literature relating
one-step transition error to multi-step transition error and
sub-optimality bounds for planning with an approximate

Lipschitz Continuity in Model-based Reinforcement Learning

model. The Simulation Lemma (Kearns & Singh, 2002;
Strehl et al., 2009) is for discrete state MDPs and relates
error in the one-step model to the value obtained by
using it for planning. A related result for continuous
state-spaces (Kakade et al., 2003) bounds the error in
estimating the probability of a trajectory using total
variation. A second related result (Venkatraman et al.,
2015) provides a slightly looser bound for prediction error
in the deterministic case—our result can be thought of as a
generalization of their result to the probabilistic case.

6. Value Error with Lipschitz Models

,

A

(cid:104)S

, T, R, γ

We next investigate the error in the state-value function
induced by a Lipschitz model class. To answer this question,
we consider an MRP M1 denoted by
and
a second MRP M2 that only differs from the ﬁrst in its
be the
transition function
T , R, γ
action set with a single action a. We further assume that
the reward function is only dependent upon state. We ﬁrst
express the state-value function for a start state s with
respect to the two transition functions. By δs below, we
mean a Dirac delta function denoting a distribution with
probability 1 at state s.

. Let
(cid:105)

,
A

a
}

(cid:104)S

A

=

(cid:98)

{

(cid:105)

,

VT (s) :=

(s(cid:48)

δs)R(s(cid:48)) ds(cid:48) ,

∞

γn

n=0
(cid:88)

(cid:90)

T n
G

∞

γn

n=0
(cid:88)

(cid:90)

T n
G

(cid:98)

|

|

V (cid:98)T (s) :=

(s(cid:48)

δs)R(s(cid:48)) ds(cid:48) .

Let

=

h : KdS ,R(h)

F

{

1

. Then given f
}

≤

:

∈ F

KR

∞

γn

f (s(cid:48))

T n
G

(s(cid:48)

δs)

|

T n
G

(s(cid:48)

δs)
|

−

ds(cid:48)

n=0
(cid:88)

KR

(cid:90)
∞

n=0
(cid:88)

(cid:0)
γn sup
f

∈F (cid:90)

≤

= KR

∞

(cid:124)
γn W

f (s(cid:48))

δs)

(s(cid:48)

δs)

ds(cid:48)

(cid:98)
T n
(s(cid:48)
G

|

(cid:1)

−

T n
G

(cid:98)

|

(cid:1)

(cid:125)

:=W

δs), (cid:98)T n

due to duality (4)

(cid:0)
T n
G (.
|

δs)
G (.
|
(cid:123)(cid:122)
δs)

(cid:1)

(cid:0)
|

(.

(.

δs),

T n
G

T n
G
(cid:80)n−1
(cid:0)
(cid:1)
i=0 ∆( ¯K)i due to Theorem 1
(cid:98)
1
(cid:125)

(cid:123)(cid:122)

|

≤
n
(cid:124)

−

n=0
(cid:88)

n=0
(cid:88)

∞

KR

∞

γn

∆( ¯K)i

= KR∆

≤

=

i=0
(cid:88)
γn 1
−
1
−

¯K n
¯K

.

n=0
(cid:88)
γKR∆
γ)(1

−

−

(1

γ ¯K)
We can derive the same bound for V (cid:98)T (s)
VT (s) using
the fact that Wasserstein distance is a metric, and therefore
symmetric, thereby completing the proof.

−

Regarding the tightness of our bounds, we can show that
when the transition model is deterministic and linear then
Theorem 1 provides a tight bound. Moreover, if the reward
function is linear, the bound provided by Theorem 2 is tight.
(See Claim 2 in the appendix.) Notice also that our proof
does not require a bounded reward function.

7. Lipschitz Generalized Value Iteration

Next we derive a bound on

VT (s)

s.

∀

−

V (cid:98)T (s)
(cid:12)
(cid:12)

Theorem 2. Assume a Lipschitz model class Fg with a
(cid:12)
(cid:12)
T with ¯K = min
∆-accurate
. Further, assume
KF , KT
}
{
a Lipschitz reward function with constant KR = KdS ,R(R).
Then

and ¯K
(cid:98)

[0, 1
γ )

s
∀

∈ S

∈

We next show that, given a Lipschitz transition model,
solving for the ﬁxed point of a class of Bellman equations
yields a Lipschitz state-action value function. Our proof is in
the context of Generalized Value Iteration (GVI) (Littman &
Szepesv´ari, 1996), which deﬁnes Value Iteration (Bellman,
1957) for planning with arbitrary backup operators.

VT (s)

V (cid:98)T (s)

−

≤

(1

(cid:12)
(cid:12)

(cid:12)
(cid:12)

γKR∆
γ)(1

−

−

.

γ ¯K)

Proof. We ﬁrst deﬁne the function f (s) = R(s)
KR
observed that KdS ,R(f ) = 1. We now write:

. It can be

VT (s)

V (cid:98)T (s)

−
γn

∞

=

n=0
(cid:88)

(cid:90)

= KR

∞

γn

T n
G

(cid:0)
f (s(cid:48))

n=0
(cid:88)

(cid:90)

(cid:0)

R(s(cid:48))

(s(cid:48)

δs)

(s(cid:48)

δs)

ds(cid:48)

|

|

T n
G

−

(cid:98)
δs)

(cid:98)

(s(cid:48)

T n
G

|

(s(cid:48)

T n
G

|

−

(cid:1)
δs)

ds(cid:48)

(cid:1)

Algorithm 1 GVI algorithm

Input: initial
repeat

Q(s, a), δ, and choose an operator f

for each s, a
(cid:98)
∈ S × A
R(s, a)+γ
Q(s, a)

do

←

end for
(cid:98)

until convergence

|

(cid:82)

(cid:98)

T (s(cid:48)

s, a)f

Q(s(cid:48),

ds(cid:48)

)
·

(cid:1)

(cid:0)

(cid:98)

To prove the result, we make use of the following lemmas.
Lemma 3. Given a Lipschitz function f :
R with
constant KdS ,dR(f ):

S (cid:55)→

K AdS ,dR

T (s(cid:48)

s, a)f (s(cid:48))ds(cid:48)
|

KdS ,dR(f )K AdS ,W

T

.

≤

(cid:17)

(cid:16) (cid:90)

(cid:98)

(cid:0)

(cid:1)

(cid:98)

Lipschitz Continuity in Model-based Reinforcement Learning

Lemma 4. The following operators (Asadi & Littman,
2017) are Lipschitz with constants:

(cid:107)(cid:107)∞,dR

mean(x)

=

(cid:0)

(cid:1)

1. K
K

2. K

3. K

(cid:107)(cid:107)∞,dR (max(x)) = K
(cid:107)(cid:107)∞,dR ((cid:15)-greedy(x)) = 1
(cid:107)(cid:107)∞,dR (mmβ(x) := log
(cid:107)(cid:107)∞,dR (boltzβ(x)
:=
A
βVmax|
|

(cid:80)n

β

(cid:80)

i eβxi
n

) = 1

i=1 xieβxi
(cid:80)n
i=1eβ xi

)

≤

+

A
|

|

(cid:112)

Theorem 3. For any non-expansion backup operator f
outlined in Lemma 4, GVI computes a value function

with a Lipschitz constant bounded by
γK AdS ,W (T ) < 1.

(R)

KA
dS ,dR
γKdS ,W (T )

1

−

if

Proof. From Algorithm 1, in the nth round of GVI updates:

·

(cid:1)

)
·

Qn+1(s, a)

R(s, a) + γ

T (s(cid:48)

s, a)f

Qn(s(cid:48),

)

ds(cid:48).

←

|

(cid:90)

(cid:0)

(cid:98)

(cid:98)
Now observe that:

K AdS ,dR (

Qn+1)

≤

≤

K AdS ,dR(R)+γK AdS ,dR

(cid:98)

T (s(cid:48)

s, a)f

Qn(s(cid:48),

ds(cid:48)

K AdS ,dR (R) + γK AdS ,W (T ) KdS,R

(cid:90)

(cid:0)

(cid:1)

(cid:0)
Qn(s,
(cid:98)

·

f

(cid:16)

)
(cid:1)(cid:17)
(cid:0)
(cid:107)·(cid:107)∞,dR(f )K AdS ,dR (
(cid:98)
Qn)

(cid:1)

Qn)

K AdS ,dR (R) + γK AdS ,W (T )K

≤
= K AdS ,dR (R) + γK AdS ,W (T )K AdS ,dR (

(cid:98)
Where we used Lemmas 3, 2, and 4 for the second, third,
and fourth inequality respectively. Equivalently:

(cid:98)

|

n

K AdS ,dR (

Qn+1)

K AdS ,dR(R)

γK AdS ,W (T )

i

≤

+

(cid:98)

i=0
(cid:88)
(cid:0)
n
γK AdS ,W (T )

K AdS ,dR (

(cid:1)
Q0) .

By computing the limit of both sides, we get:

(cid:1)

(cid:0)

K AdS ,dR (

Qn+1)

K AdS ,dR (R)

γK AdS ,W (T )

lim
n
→∞

(cid:98)

(cid:98)

n

i=0
(cid:88)
(cid:0)
γK AdS ,W (T )

n

K AdS ,dR (

(cid:1)
Q0)

K AdS ,dR (R)

(cid:0)
γKdS ,W (T )

(cid:1)
+ 0 ,

i

(cid:98)

lim
n
→∞

≤

+ lim
n
→∞

=

1

−

This concludes the proof.

Two implications of this result: First, PAC exploration in
continuous state spaces is shown assuming a Lipschitz value
function (Pazis & Parr, 2013). However, the theorem shows

that it is sufﬁcient to have a Lipschitz model, an assumption
perhaps easier to conﬁrm. The second implication relates to
value-aware model learning (VAML) objective (Farahmand
et al., 2017). Using the above theorem, we can show that
minimizing Wasserstein is equivalent to minimizing the
VAML objective (Asadi et al., 2018).

8. Experiments

|S|

Our ﬁrst goal in this section1 is to compare TV, KL, and
Wasserstein in terms of the ability to best quantify error of
an imperfect model. To this end, we built ﬁnite MRPs with
random transitions,
= 10 states, and γ = 0.95. In the
ﬁrst case the reward signal is randomly sampled from [0, 10],
and in the second case the reward of an state is the index of
that state, so small Euclidean norm between two states is
an indication of similar values. For 105 trials, we generated
an MRP and a random model, and then computed model
error and planning error (Figure 4). We understand a good
metric as the one that computes a model error with a high
correlation with value error. We show these correlations for
different values of γ in Figure 5.

Figure 4. Value error (x axis) and model error (y axis). When
the reward is the index of the state (right), correlation between
Wasserstein error and value-prediction error is high.
This
highlights the fact that when closeness in the state-space is an
indication of similar values, Wasserstein can be a powerful metric
for model-based RL. Note that Wasserstein provides no advantage
given random rewards (left).

Figure 5. Correlation between value-prediction error and model
error for the three metrics using random rewards (left) and index
rewards (right). Given a useful notion of state similarities, low
Wasserstein error is a better indication of planning error.

1We release the code here: github.com/kavosh8/Lip

Lipschitz Continuity in Model-based Reinforcement Learning

Function f

Deﬁnition

ReLu : Rn

+b : Rn

Rn,

W : Rn

×

→
Rm,

→

Rn
→
b
∀
W

∈

∀

∈

Rn
Rm

n

×

ReLu(x)i := max
{
+b(x) := x + b

0, xi

}

W (x) := W x

×

Lipschitz constant K

p,

(cid:107)(cid:107)

(cid:107)(cid:107)

p = 1
1
1

p = 2
1
1

Wj

j (cid:107)

(cid:107)∞

Wj

2
2

(cid:107)

j (cid:107)

(cid:80)

(cid:113)(cid:80)

p (f )
p =
1
1
supj (cid:107)

∞

Wj

(cid:107)1

Table 1. Lipschitz constant for various functions used in a neural network. Here, Wj denotes the jth row of a weight matrix W .

It is known that controlling the Lipschitz constant of neural
nets can help in terms of improving generalization error due
to a lower bound on Rademacher complexity (Neyshabur
et al., 2015; Bartlett & Mendelson, 2002). It then follows
from Theorems 1 and 2 that controlling the Lipschitz
constant of a learned transition model can achieve better
error bounds for multi-step and value predictions. To
enforce this constraint during learning, we bound the
Lipschitz constant of various operations used in building
neural network. The bound on the constant of the entire
neural network then follows from Lemma 2. In Table 1, we
provide Lipschitz constant for operations (see Appendix for
proof) used in our experiments. We quantify these results
for different p-norms

(cid:107)·(cid:107)p.

speciﬁcally when the transition model

Given these simple methods for enforcing Lipschitz
continuity, we performed empirical evaluations
to
understand the impact of Lipschitz continuity of transition
models,
is
used to perform multi-step state-predictions and policy
improvements. We chose two standard domains: Cart Pole
and Pendulum. In Cart Pole, we trained a network on a
dataset of 15
. During training, we
(cid:105)
ensured that the weights of the network are smaller than k.
For each k, we performed 20 independent model estimation,
and chose the model with median cross-validation error.

s, a, s(cid:48)
(cid:104)

103 tuples

∗

Using the learned model, along with the actual reward
signal of the environment, we then performed stochastic
actor-critic RL. (Barto et al., 1983; Sutton et al., 2000)
This required an interaction between the policy and the
learned model for relatively long trajectories. To measure
the usefulness of the model, we then tested the learned
policy on the actual domain. We repeated this experiment
on Pendulum. To train the neural transition model for
this domain we used 104 samples. Notably, we used
deterministic policy gradient (Silver et al., 2014) for training
the policy network with the hyper parameters suggested by
Lillicrap et al. (2015). We report these results in Figure 6.

Observe that an intermediate Lipschitz constant yields the
best result. Consistent with the theory, controlling the
Lipschitz constant in practice can combat the compounding
errors and can help in the value estimation problem. This
ultimately results in learning a better policy.

We next examined if the beneﬁts carry over to stochastic

Figure 6. Impact of Lipschitz constant of learned models in Cart
Pole (left) and Pendulum (right). An intermediate value of k
(Lipschitz constant) yields the best performance.

Fg

θf : f
{

settings. To capture stochasticity we need an algorithm to
learn a Lipschitz model class (Deﬁnition 4). We used an EM
algorithm to joinly learn a set of functions f , parameterized
by θ =
, and a distribution over functions
}
g. Note that in practice our dataset only consists of a set
of samples
and does not include the function the
sample is drawn from. Hence, we consider this as our
latent variable z. As is standard with EM, we start with the
log-likelihood objective (for simplicity of presentation we
assume a single action in the derivation):

s, a, s(cid:48)
(cid:104)

∈

(cid:105)

L(θ) =

log p(si, si(cid:48); θ)

N

i=1
(cid:88)
N

i=1
(cid:88)
N

i=1
(cid:88)
N

=

=

≥

log

p(zi = f, si, si(cid:48); θ)

f
(cid:88)

f
(cid:88)

log

q(zi = f

si, si(cid:48))

|

q(zi = f

si, si(cid:48))log

|

i=1
(cid:88)

f
(cid:88)

p(zi = f, si, si(cid:48); θ)
si, si(cid:48))
|

q(zi = f

p(zi = f, si, si(cid:48); θ)
si, si(cid:48))
|

q(zi = f

,

where we used Jensen’s inequality and concavity of log in
the last line. This derivation leads to the following EM
algorithm.

Lipschitz Continuity in Model-based Reinforcement Learning

Figure 7. A stochastic problem solved by training a Lipschitz
model class using EM. The top left ﬁgure shows the functions
before any training (iteration 0), and the bottom right ﬁgure shows
the ﬁnal results (iteration 50).

In the M step, ﬁnd θt by solving for:

N

argmax

θ

i=1
(cid:88)

f
(cid:88)

qt

1(zi = f

−

si, si(cid:48))log
|

p(zi = f, si, si(cid:48); θ)
si, si(cid:48))
qt

1(zi = f

−

|

In the E step, compute posteriors:

qt(zi = f

si, si(cid:48)) =
|

p(si, si(cid:48)
f p(si, si(cid:48)|

|

zi = f ; θt

f )g(zi = f ; θt)

zi = f ; θt

f )g(zi = f ; θt)

.

Note that we assume each point is drawn from a neural
network f with probability:

(cid:80)

p

si, si(cid:48)

zi = f ; θt
|

N
and with a ﬁxed variance σ2 tuned as a hyper-parameter.

−

(cid:17)

(cid:0)

(cid:1)

(cid:16)(cid:12)
(cid:12)

(cid:12)
(cid:12)

f

=

si(cid:48)

f (si, θt

f )

, σ2

,

We used a supervised-learning domain to evaluate the EM
algorithm. We generated 30 points from 5 functions (written
at the end of Appendix) and trained 5 neural networks to ﬁt
these points. Iterations of a single run is shown in Figure 7
and the summary of results is presented in Figure 8. Observe
that the EM algorithm is effective, and that controlling the
Lipschitz constant is again useful.

We next applied EM to train a transition model for an RL
setting, namely the gridworld domain from Moerland et al.
(2017). Here a useful model needs to capture the stochastic
behavior of the two ghosts. We modify the reward to be
-1 whenever the agent is in the same cell as either one of
the ghosts and 0 otherwise. We performed environmental
interactions for 1000 time-steps and measured the return.
We compared against standard tabular methods(Sutton
& Barto, 1998), and a deterministic model that predicts
expected next state (Sutton et al., 2008; Parr et al., 2008). In
all cases we used value iteration for planning.

Figure 8. Impact of controlling the Lipschitz constant in the
supervised-learning domain.
Notice the U-shape of ﬁnal
Wasserstein loss with respect to Lipschitz constant k.

Figure 9. Performance of a Lipschitz model class on the gridworld
domain. We show model test accuracy (left) and quality of the
policy found using the model (right). Notice the poor performance
of tabular and expected models.

Results in Figure 9 show that tabular models fail due to no
generalization, and expected models fail since the ghosts
do not move on expectation, a prediction not useful for
planner. Performing value iteration with a Lipschitz model
class outperforms the baselines.

9. Conclusion

We took an important
step towards understanding
model-based RL with function approximation. We showed
that Lipschitz continuity of an estimated model plays
a central role in multi-step prediction error, and in
value-estimation error. We also showed the beneﬁts of
employing Wasserstein for model-based RL. An important
future work is to apply these ideas to larger problems.

10. Acknowledgements

The authors recognize the assistance of Eli Upfal, John
Langford, George Konidaris, and members of Brown’s Rlab
speciﬁcally Cameron Allen, David Abel, and Evan Cater.
The authors also thank anonymous ICML reviewer 1 for
insights on a value-aware interpretation of Wasserstein.

Lipschitz Continuity in Model-based Reinforcement Learning

References

Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein
In International

generative adversarial networks.
Conference on Machine Learning, pp. 214–223, 2017.

Asadi, K. and Littman, M. L. An alternative softmax
operator for reinforcement learning. In Proceedings of
the 34th International Conference on Machine Learning,
pp. 243–252, 2017.

Asadi, K., Cater, E., Misra, D., and Littman, M. L.
Equivalence between wasserstein and value-aware
model-based reinforcement learning. arXiv preprint
arXiv:1806.01265, 2018.

Bartlett, P. L. and Mendelson, S. Rademacher and gaussian
complexities: Risk bounds and structural results. Journal
of Machine Learning Research, 3(Nov):463–482, 2002.

Barto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike
adaptive elements that can solve difﬁcult learning control
IEEE transactions on systems, man, and
problems.
cybernetics, pp. 834–846, 1983.

Bellemare, M. G., Dabney, W., and Munos, R. A
distributional perspective on reinforcement learning. In
International Conference on Machine Learning, pp.
449–458, 2017.

Bellman, R. A markovian decision process. Journal of

Mathematics and Mechanics, pp. 679–684, 1957.

Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause,
A.
Safe model-based reinforcement learning with
stability guarantees. In Advances in Neural Information
Processing Systems, pp. 908–919, 2017.

Bertsekas, D. Convergence of discretization procedures in
dynamic programming. IEEE Transactions on Automatic
Control, 20(3):415–419, 1975.

Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-dynamic
programming: an overview. In Decision and Control,
1995, Proceedings of the 34th IEEE Conference on,
volume 1, pp. 560–564. IEEE, 1995.

Bubeck, S., Munos, R., Stoltz, G., and Szepesv´ari, C.
X-armed bandits. Journal of Machine Learning Research,
12(May):1655–1695, 2011.

Dempster, A. P., Laird, N. M., and Rubin, D. B.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society. Series
B (methodological), pp. 1–38, 1977.

Farahmand, A.-M., Barreto, A., and Nikovski, D.
for Model-based
the

Value-Aware
Reinforcement Learning.

In Proceedings of

Function

Loss

20th International Conference on Artiﬁcial Intelligence
and Statistics, pp. 1486–1494, 2017.

Ferns, N., Panangaden, P., and Precup, D. Metrics for ﬁnite
markov decision processes. In Proceedings of the 20th
conference on Uncertainty in artiﬁcial intelligence, pp.
162–169. AUAI Press, 2004.

Fox, R., Pakman, A., and Tishby, N. G-learning: Taming
the noise in reinforcement learning via soft updates.
Uncertainty in Artiﬁcal Intelligence, 2016.

Gao, B. and Pavel, L.

On the properties of the
softmax function with application in game theory and
reinforcement learning. arXiv preprint arXiv:1704.00805,
2017.

Hinderer, K. Lipschitz continuity of value functions in
Markovian decision processes. Mathematical Methods of
Operations Research, 62(1):3–22, 2005.

Jiang, N., Kulesza, A., Singh, S., and Lewis, R. The
dependence of effective planning horizon on model
In Proceedings of AAMAS, pp. 1181–1189,
accuracy.
2015.

Kaelbling, L. P., Littman, M. L., and Moore, A. W.
Reinforcement learning: A survey. Journal of artiﬁcial
intelligence research, 4:237–285, 1996.

Kakade, S., Kearns, M. J., and Langford, J. Exploration
the
in metric state spaces.
20th International Conference on Machine Learning
(ICML-03), pp. 306–312, 2003.

In Proceedings of

Kearns, M. and Singh, S. Near-optimal reinforcement
learning in polynomial time. Machine Learning, 49(2-3):
209–232, 2002.

Kleinberg, R., Slivkins, A., and Upfal, E. Multi-armed
bandits in metric spaces. In Proceedings of the Fortieth
Annual ACM Symposium on Theory of Computing, pp.
681–690. ACM, 2008.

Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.

Littman, M. L. and Szepesv´ari, C.
A generalized
reinforcement-learning model:
and
applications. In Proceedings of the 13th International
Conference on Machine Learning, pp. 310–318, 1996.

Convergence

Lorenz, E. Predictability: does the ﬂap of a butterﬂy’s wing

in Brazil set off a tornado in Texas? na, 1972.

Lipschitz Continuity in Model-based Reinforcement Learning

Moerland, T. M., Broekens, J., and Jonker, C. M. Learning
for model-based
multimodal
reinforcement learning. arXiv preprint arXiv:1705.00470,
2017.

transition dynamics

M¨uller, A. Optimal selection from distributions with
unknown parameters: Robustness of bayesian models.
Mathematical Methods of Operations Research, 44(3):
371–386, 1996.

Nachum, O., Norouzi, M., Xu, K., and Schuurmans,
D. Bridging the gap between value and policy based
reinforcement learning. arXiv preprint arXiv:1702.08892,
2017.

Narayanan, H. and Mitter, S.

Sample complexity of
testing the manifold hypothesis. In Advances in Neural
Information Processing Systems, pp. 1786–1794, 2010.

Strehl, A. L., Li, L., and Littman, M. L. Reinforcement
learning in ﬁnite mdps: Pac analysis. Journal of Machine
Learning Research, 10(Nov):2413–2444, 2009.

Sutton, R. S. and Barto, A. G. Reinforcement Learning: An

Introduction. The MIT Press, 1998.

Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour,
Y. Policy gradient methods for reinforcement learning
In Advances in Neural
with function approximation.
Information Processing Systems, pp. 1057–1063, 2000.

Sutton, R. S., Szepesv´ari, C., Geramifard, A., and Bowling,
M. H.
Dyna-style planning with linear function
approximation and prioritized sweeping. In UAI 2008,
Proceedings of the 24th Conference in Uncertainty in
Artiﬁcial Intelligence, Helsinki, Finland, July 9-12, 2008,
pp. 528–536, 2008.

Neu, G., Jonsson, A., and G´omez, V. A uniﬁed view of
entropy-regularized Markov decision processes. arXiv
preprint arXiv:1705.07798, 2017.

Szepesv´ari, C. Algorithms for reinforcement learning.
Synthesis Lectures on Artiﬁcial Intelligence and Machine
Learning, 4(1):1–103, 2010.

Talvitie, E. Model regularization for stable sample
rollouts. In Proceedings of the Thirtieth Conference on
Uncertainty in Artiﬁcial Intelligence, pp. 780–789. AUAI
Press, 2014.

Talvitie, E.

Self-correcting models for model-based
reinforcement learning. In Proceedings of the Thirty-First
AAAI Conference on Artiﬁcial Intelligence, February 4-9,
2017, San Francisco, California, USA., 2017.

Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society. Series B
(Methodological), pp. 267–288, 1996.

Vaserstein, L. N. Markov processes over denumerable
products of spaces, describing large systems of automata.
Problemy Peredachi Informatsii, 5(3):64–72, 1969.

Venkatraman, A., Hebert, M., and Bagnell, J. A. Improving
multi-step prediction of learned time series models. In
Proceedings of the Twenty-Ninth AAAI Conference on
Artiﬁcial Intelligence, January 25-30, 2015, Austin, Texas,
USA., 2015.

Villani, C. Optimal transport: old and new, volume 338.

Springer Science & Business Media, 2008.

Neyshabur, B., Tomioka, R., and Srebro, N. Norm-based
capacity control in neural networks. In Proceedings of
The 28th Conference on Learning Theory, pp. 1376–1401,
2015.

Parr, R., Li, L., Taylor, G., Painter-Wakeﬁeld, C., and
Littman, M. L. An analysis of linear models, linear
value-function approximation, and feature selection
In Proceedings of the
for reinforcement learning.
25th international conference on Machine learning, pp.
752–759. ACM, 2008.

Pazis, J. and Parr, R. Pac optimal exploration in continuous

space markov decision processes. In AAAI, 2013.

Pires, B. ´A. and Szepesv´ari, C. Policy error bounds for
model-based reinforcement learning with factored linear
models. In Conference on Learning Theory, pp. 121–151,
2016.

Pirotta, M., Restelli, M., and Bascetta, L. Policy gradient in
lipschitz Markov decision processes. Machine Learning,
100(2-3):255–283, 2015.

Rachelson, E. and Lagoudakis, M. G. On the locality of
action domination in sequential decision making.
In
International Symposium on Artiﬁcial Intelligence and
Mathematics, ISAIM 2010, Fort Lauderdale, Florida,
USA, January 6-8, 2010, 2010.

Russell, S. J. and Norvig, P. Artiﬁcial intelligence: A

modern approach, 1995.

Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and
Riedmiller, M. Deterministic policy gradient algorithms.
In ICML, 2014.

Lipschitz Continuity in Model-based Reinforcement Learning

Appendix

Claim 1. In a ﬁnite MDP, transition probabilities can be expressed using a ﬁnite set of deterministic functions and a
distribution over the functions.

Proof. Let P r(s, a, s(cid:48)) denote the probability of a transiton from s to s(cid:48) when executing the action a. Deﬁne an ordering
over states s1, ..., sn with an additional unreachable state s0. Now deﬁne the cumulative probability distribution:

Further deﬁne L as the set of distinct entries in C:

C(s, a, si) :=

P r(s, a, sj) .

i

j=0
(cid:88)

L :=

C(s, a, si)
|

s

, i

[0, n]

.

∈ S

∈

(cid:110)

(cid:111)

Note that, since the MDP is assumed to be ﬁnite, then
value of the set. Note that c0 = 0 and c
L
|

|

i = 1 to

and

j = 1 to n, deﬁne fi(s) = sj if and only if:

∀

L
|

|

∀

= 1. We now build determinstic set of functions f1, ..., f

is ﬁnite. We sort the values of L and denote, by ci, ith smallest
as follows:

L
|

|

L
|

|

We also deﬁne the probability distribution g over f as follows:

C(s, a, sj

1) < ci

C(s, a, sj) .

−

≤

g(fi

a) := ci
|

−

ci

1 .

−

Given the functions f1, ..., f
executing action a:

L
|

|

and the distribution g, we can now compute the probability of a transition to sj from s after

1(fi(s) = sj) g(fi

a)
|

i
(cid:88)
=

1

i
(cid:88)

(cid:0)

= C(s, a, sj)
−
= P r(s, a, sj) ,

C(s, a, sj

1) < ci

C(s, a, sj)

(ci

−

≤

ci

1)

−

−

C(s, a, sj

1)

−

(cid:1)

where 1 is a binary function that outputs one if and only if its condition holds. We reconstructed the transition probabilities
using distribution g and deterministic functions f1, ..., f

.

L
|

|

Claim 2. Given a deterministic and linear transition model, and a linear reward signal, the bounds provided in Theorems 1
and 2 are both tight.

Assume a linear transition function T deﬁned as:

Assume our learned transition function ˆT :

Note that:

and that:

T (s) = Ks

ˆT (s) := Ks + ∆

max
s

T (s)

−

ˆT (s)

= ∆

(cid:12)
(cid:12)
KT , K ˆT }
min
{

(cid:12)
(cid:12)
= K

First observe that the bound in Theorem 2 is tight for n = 2:

T

T (s)

ˆT

ˆT (s)

=

K 2s

s
∀

(cid:12)
(cid:12)
(cid:12)

−

(cid:0)

(cid:1)

(cid:0)

(cid:1)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

−

K 2s + ∆(1 + K)
(cid:12)
(cid:12)
(cid:12)

= ∆

K i

1

i=0
(cid:88)

Lipschitz Continuity in Model-based Reinforcement Learning

and more generally and after n compositions of the models, denoted by T n and ˆT n, the following equality holds:

Lets further assume that the reward is linear:

s
∀

T n(s)

−

(cid:12)
(cid:12)
(cid:12)

= ∆

K i

n

1

−

i=0
(cid:88)

ˆT n(s)
(cid:12)
(cid:12)
(cid:12)

R(s) = KRs

Consider the state s = 0. Note that clearly v(0) = 0. We now compute the value predicted using ˆT , denoted by ˆv(0):

ˆv(0) = R(0) + γR(0 + ∆

K i) + γ2R(0 + ∆

K i) + γ3R(0 + ∆

K i) + ...

0

i=0
(cid:88)

2

i=0
(cid:88)

1

i=0
(cid:88)

2

i=0
(cid:88)

= 0 + γKR∆

K i + γ2KR∆

K i) + γ3KR∆

K i + ...

0

1

= γKR∆

K i =

i=0
(cid:88)
n
γn

1

−

∞

n=0
(cid:88)

i=0
(cid:88)

i=0
(cid:88)
γKR∆
γ)(1

−

(1

−

,

γ ¯K)

and so:

v(0)
|

ˆv(0)
|

−

=

(1

γKR∆
γ)(1

−

−

γ ¯K)

Note that this exactly matches the bound derived in our Theorem 2.

Lemma 1. A generalized transition function

induced by a Lipschitz model class Fg is Lipschitz with a constant:

T
G

Proof.

W

T (

µ1, a),

T (

µ2, a)

· |

(cid:0)

(cid:98)

· |

(cid:98)

(cid:1)

K AW,W (

T
G

(cid:98)
) := sup

sup
µ1,µ2

a

W

T
G

(
µ1, a),
(
T
·|
G
W (µ1, µ2)

·|

µ2, a)

(cid:0)

(cid:98)

(cid:98)

KF

≤

(cid:1)

(cid:98)

:= inf
j

= inf
j

= inf
j

j(s(cid:48)1, s(cid:48)2)d(s(cid:48)1, s(cid:48)2)ds(cid:48)1ds(cid:48)2

(cid:90)s(cid:48)

1 (cid:90)s(cid:48)

2

1

f (s1) = s(cid:48)1 ∧

(cid:90)s1 (cid:90)s2 (cid:90)s(cid:48)

1 (cid:90)s(cid:48)

2

f
(cid:88)
j(s1, s2, f )d

(cid:0)

(cid:90)s1 (cid:90)s2

f
(cid:88)

(cid:1)

f (s1), f (s2)

ds1ds2

(cid:0)

(cid:1)

KF inf
j

≤

g(f

a)j(s1, s2)d(s1, s2)ds1ds2
|

= KF

g(f

j(s1, s2)d(s1, s2)ds1ds2

(cid:90)s1 (cid:90)s2

f
(cid:88)
a) inf
j

(cid:90)s1 (cid:90)s2

|

|

f
(cid:88)

f
(cid:88)

= KF

g(f

a)W (µ1, µ2) = KF W (µ1, µ2)

Dividing by W (µ1, µ2) and taking sup over a, µ1, and µ2, we conclude:

K AW,W (

T ) = sup

sup
µ1,µ2

a

(cid:98)

W

T (

· |

µ1, a),
T (
· |
W (µ1, µ2)

µ2, a)

(cid:0)

(cid:98)

(cid:98)

KF .

≤

(cid:1)

We can also prove this using the Kantarovich-Rubinstein duality theorem:

f (s2) = s(cid:48)2

j(s1, s2, f )d(s(cid:48)1, s(cid:48)2)ds(cid:48)1ds(cid:48)2ds1ds2

Lipschitz Continuity in Model-based Reinforcement Learning

For every µ1, µ2, and a

we have:

∈ A

W

T
G

(
· |

µ1, a),

T
G

(
· |

µ2, a)

(cid:0)

(cid:98)

(cid:98)

(cid:1)

=

=

=

=

=

=

≤

≤

sup

sup

sup

sup

f :KdS ,R(f )

1 (cid:90)s

≤

(cid:0)

(cid:98)
1 (cid:90)s (cid:90)s0 (cid:16)

f :KdS ,R(f )

≤

T
G

(s
|

µ1, a)

T
G

µ2, a)
(s
|

−

f (s)ds

(cid:98)
s0, a)µ1(s0)
T (s
|

−

(cid:1)
s0, a)µ2(s0)
T (s
|

f (s)dsds0

(cid:98)
s0, a)
T (s
|

µ1(s0)

−

(cid:98)
µ2(s0)

(cid:17)
f (s)dsds0

f :KdS ,R(f )

1 (cid:90)s (cid:90)s0

≤

f :KdS ,R(f )

1 (cid:90)s (cid:90)s0

≤

sup

f :KdS ,R(f )

sup

f :KdS ,R(f )

1

≤

t
(cid:88)

1

≤

t
(cid:88)

g(t

a)

|

(cid:98)

|

|

t
(cid:88)
a)

g(t

g(t

a)

(cid:16)
a)1

g(t

|

(cid:0)
1

(cid:90)s0 (cid:90)s

(cid:90)s0

(cid:0)

−

−

(cid:17)
µ1(s0)

(cid:1)(cid:16)

−

−

t(s0) = s

µ1(s0)

µ2(s0)

(cid:0)
µ1(s0)

(cid:1)(cid:0)
µ2(s0)

f

t(s0)

ds0

(cid:1)

sup

µ1(s0)

(cid:1)
µ2(s0)

f

(cid:0)

(cid:1)

t(s0)

ds0

t
(cid:88)
composition of f, t is Lipschitz with constant upper bounded by KF .

(cid:0)

(cid:1)

(cid:1)

(cid:0)

f :KdS ,R(f )

1 (cid:90)s0
≤

t(s0) = s

µ2(s0)

f (s)dsds0

(cid:17)
f (s)dsds0

= KF

g(t

a)

sup

µ1(s0)

µ2(s0)

f :KdS ,R(f )

1 (cid:90)s0
≤

KF

g(t

a)

sup

= KF

g(t

h:KdS ,R(h)

1 (cid:90)s0
≤
a)W (µ1, µ2) = KF W (µ1, µ2)

(cid:0)

(cid:1)

f (t(s0))
KF

ds0

(cid:0)
µ1(s0)

(cid:1)
h(s0))ds0

µ2(s0)

−

−

|

|

|

t
(cid:88)

t
(cid:88)

t
(cid:88)

Again we conclude by dividing by W (µ1, µ2) and taking sup over a, µ1, and µ2.

Lemma 2. (Composition Lemma) Deﬁne three metric spaces (M1, d1), (M2, d2), and (M3, d3). Deﬁne Lipschitz functions
M3 is Lipschitz with
f : M2 (cid:55)→
constant Kd1,d3 (h)

M2 with constants Kd2,d3 (f ) and Kd1,d2 (g). Then, h : f

Kd2,d3 (f )Kd1,d2(g).

g : M1 (cid:55)→

◦

M3 and g : M1 (cid:55)→
≤

Proof.

Kd1,d3 (h) = sup
s1,s2

d3

f

g(s1)

, f

g(s2)

(cid:16)

(cid:0)

d1(s1, s2)
(cid:0)
(cid:1)

(cid:1)(cid:17)

= sup
s1,s2

d2

g(s1), g(s2)
d1(s1, s2)
(cid:0)

d2

(cid:1)
g(s1), g(s2)
d1(s1, s2)
≤
(cid:0)
(cid:1)
= Kd1,d2(g)Kd2,d3 (f ).

sup
s1,s2

d3

f

g(s1)

, f

g(s2)

(cid:1)(cid:17)

(cid:16)

sup
s1,s2

d2
(cid:0)
d3
(cid:0)

g(s1), g(s2)
(cid:1)
(cid:0)
f (s1), f (s2)
(cid:1)
d2(s1, s2)

(cid:0)

(cid:1)

Lemma 3. Given a Lipschitz function f :

R with constant KdS ,dR(f ):

S (cid:55)→

K AdS ,dR

T (s(cid:48)

s, a)f (s(cid:48))ds(cid:48)

KdS ,dR (f )K AdS ,W

T

.

|

(cid:16) (cid:90)

(cid:98)

≤

(cid:17)

(cid:0)

(cid:1)

(cid:98)

Proof.

K AdS ,dR

T (s(cid:48)

s, a)f (s(cid:48))ds(cid:48)

= sup

|

(cid:16) (cid:90)s(cid:48)

(cid:98)

Lipschitz Continuity in Model-based Reinforcement Learning

(cid:17)

sup
s1,s2

(cid:82)

(cid:0)

s(cid:48)

s(cid:48)

|

|

T (s(cid:48)

s1, a)
|

(cid:98)
T (s(cid:48)

s1, a)
|

a

a

= sup

sup
s1,s2

(cid:82)

(cid:0)

(cid:98)

= KdS ,dR(f ) sup
a

sup
s1,s2

s2, a)
T (s(cid:48)
−
|
d(s1, s2)
(cid:98)
T (s(cid:48)

s2, a)
|
d(s1, s2)
(cid:98)
T (s(cid:48)

s1, a)

−

(cid:1)

(cid:1)

f (s(cid:48))ds(cid:48)

|

f (s(cid:48))

KdS ,dR (f )
KdS ,dR (f ) ds(cid:48)

|

s(cid:48)

|

(cid:0)

(cid:82)
(cid:98)
supg:KdS ,dR (g)
≤

1

|

|

supg:KdS ,dR (g)
≤

1

s2, a)
|

T (s(cid:48)

−
d(s1, s2)

(cid:98)
s(cid:48)

(cid:82)

T (s(cid:48)

(cid:1)
s1, a)
|
d(s1, s2)
(cid:0)
(cid:98)
T (s(cid:48)

s(cid:48)

s1, a)
|
d(s1, s2)
(cid:0)
(cid:98)
s2, a)

(cid:82)

·|

(cid:1)

f (s(cid:48))
KdS ,dR (f ) ds(cid:48)

|

T (s(cid:48)

s2, a)
|

−

g(s(cid:48))ds(cid:48)

|

(cid:1)
g(s(cid:48))ds(cid:48)

(cid:98)
T (s(cid:48)

−

s2, a)
|

(cid:98)

(cid:1)

KdS ,dR(f ) sup
a

sup
s1,s2

≤

= KdS ,dR(f ) sup
a

sup
s1,s2

= KdS ,dR(f ) sup
a

sup
s1,s2

= KdS ,dR(f )K AdS ,W (

T ) .

W

T (

·|

s1, a),
T (
d(s1, s2)
(cid:98)

(cid:0)

(cid:98)

(cid:98)

Lemma 4. The following operators (Asadi & Littman, 2017) are Lipschitz with constants:

(cid:107)(cid:107)∞,dR

mean(x)

= K

(cid:107)(cid:107)∞,dR ((cid:15)-greedy(x)) = 1

1. K

2. K

(cid:107)(cid:107)∞,dR (max(x)) = K
(cid:107)(cid:107)∞,dR (mmβ(x) := log

(cid:80)

(cid:0)
i eβxi
n

β

) = 1

(cid:1)

3. K

(cid:107)(cid:107)∞,dR (boltzβ(x) :=

(cid:80)n

i=1 xieβxi
(cid:80)n
i=1eβ xi

)

A
|

|

A
+ βVmax|

|

≤

(cid:112)

and observe that boltzβ(x) = x(cid:62)ρ(x). Gao & Pavel (2017) showed that ρ is Lipschitz:

(cid:80)

ρ(x)i =

eβxi
n
i=1 eβxi

,

ρ(x2)

ρ(x1)
(cid:107)

x2(cid:107)2
Using their result, we can further show:

x1 −
(cid:107)

(cid:107)2 ≤

−

β

Proof. 1 was proven by Littman & Szepesv´ari (1996), and 2 is proven several times (Fox et al., 2016; Asadi & Littman,
2017; Nachum et al., 2017; Neu et al., 2017). We focus on proving 3. Deﬁne

(7)

ρ(x1)(cid:62)x2 −

|

ρ(x2)(cid:62)x2|

(Cauchy-Shwartz)

ρ(x2)(cid:62)x2|

|

ρ(x1)(cid:62)x1 −
≤ |

+

ρ(x1)(cid:62)x1 −
ρ(x1)
(cid:107)2 (cid:107)
ρ(x1)
x2(cid:107)2 (cid:107)
(cid:107)
ρ(x1)
x2(cid:107)2 β
(cid:107)
(1 + βVmax

x1 −
−
x1 −
(cid:107)2 (cid:107)
x1 −
(cid:107)

ρ(x1)(cid:62)x2|
x2(cid:107)2
ρ(x2)
x2(cid:107)2
x2(cid:107)2
x1 −
)
A
(cid:0)
|
(cid:107)
|
x1 −
)
A
+ βVmax|
(cid:112)
(cid:107)
|

A
|

(cid:107)2

(

≤ (cid:107)
+

≤ (cid:107)
+

≤

≤
|
(cid:112)
leads to 3.

from Eqn 7)
x2(cid:107)2
x2(cid:107)∞

,

(cid:1)

dividing both sides by

x1 −
(cid:107)

x2(cid:107)∞

Lipschitz Continuity in Model-based Reinforcement Learning

Below, we derive the Lipschitz constant for various functions mentioned in Table 1.
Rn has Lipschitz constant 1 for p.
ReLu non-linearity We show that ReLu : Rn

→

K
(cid:107)

.
(cid:107)

p,

.
(cid:107)

(cid:107)

p (ReLu) = sup
x1,x2

= sup
x1,x2
(cid:107)
(We can show that

(cid:80)

1
p

p)
|

p

p

p

(

i |

ReLu(x1)
(cid:107)

ReLu(x2)
−
(cid:107)
x2(cid:107)
x1 −
(cid:107)
ReLu(x1)i
ReLu(x2)i
−
x2(cid:107)
x1 −
ReLu(x1)i
|
x1,i
x2,i
i |
−
x1 −
x2(cid:107)
(cid:80)
(cid:107)
x2(cid:107)
x1 −
= 1
(cid:107)
x2(cid:107)
x1 −
(cid:107)

p)
|

−

(

1
p

p

p

p

sup
x1,x2

≤

= sup
x1,x2

ReLu(x2)i

x1,i

x2,i

and so) :

| ≤ |

−

|

Matrix multiplication Let W

Rn

×

m. We derive the Lipschitz continuity for the function

W (x) = W x.

×

For p =

we have:

∞

∈

where Wj refers to jth row of the weight matrix W . Similarly, for p = 1 we have:

K

(cid:107)(cid:107)∞,

= sup
x1,x2

= sup
x1,x2

sup
≤
x1,x2
= sup

j (cid:107)

W (x1)

(cid:107)(cid:107)∞

×
W (x1)
(cid:0)
(cid:107)×

W (x2)
(cid:1)
− ×
x2(cid:107)∞
x2)

|

x1 −
(cid:107)
Wj(x1 −
supj |
x2(cid:107)∞
x1 −
(cid:107)
x1 −
Wj
supj (cid:107)
(cid:107) (cid:107)
x2(cid:107)∞
x1 −
(cid:107)
(cid:107)1 ,

Wj

x2(cid:107)∞

W (x2)
x2(cid:107)1
x2)
|

K

(cid:107)(cid:107)1,

(cid:107)(cid:107)1

= sup
x1,x2

(cid:0)
(cid:107)×

= sup

x1,x2 (cid:80)

≤

sup
x1,x2 (cid:80)

W (x1)

×
W (x1)

(cid:1)
− ×
x1 −
(cid:107)
Wj(x1 −
j |
x2(cid:107)1
x1 −
(cid:107)
Wj
j (cid:107)
(cid:107)∞ (cid:107)
x1 −
(cid:107)

K

(cid:107)(cid:107)2,

(cid:107)(cid:107)2

= sup
x1,x2

(cid:0)
(cid:107)×

W (x1)

×
W (x1)

2
|

x2)

W (x2)
(cid:1)
− ×
x2(cid:107)2
x1 −
(cid:107)
Wj(x1 −
j |
x2(cid:107)2
x1 −
(cid:107)
2
Wj
x1 −
j (cid:107)
2 (cid:107)
(cid:107)
x2(cid:107)2
x1 −
(cid:107)

= sup

x1,x2 (cid:113)(cid:80)

sup
x1,x2 (cid:113)(cid:80)

≤

(cid:107)∞

= sup
x1,x2

W x1 −
(cid:107)
x1 −
(cid:107)

W x2(cid:107)∞
x2(cid:107)∞

= sup
x1,x2

W (x1 −
(cid:107)
x1 −
(cid:107)

x2)
x2(cid:107)∞

(cid:107)∞

(H¨older’s inequality)

(cid:107)1

= sup
x1,x2

(cid:107)

W x1 −
x1 −
(cid:107)

W x2(cid:107)1
x2(cid:107)1

= sup
x1,x2

(cid:107)

W (x1 −
x1 −
(cid:107)

x2)
x2(cid:107)1

(cid:107)1

(cid:107)2

= sup
x1,x2

(cid:107)

W x1 −
x1 −
(cid:107)

W x2(cid:107)2
x2(cid:107)2

= sup
x1,x2

(cid:107)

W (x1 −
x1 −
(cid:107)

x2)
x2(cid:107)2

(cid:107)2

2
2

x2(cid:107)

=

Wj
(cid:107)

2
2
(cid:107)

.

j

(cid:115)(cid:88)

and ﬁnally for p = 2:

x2(cid:107)1

=

x1 −
x2(cid:107)1

j
(cid:88)

Wj
(cid:107)

(cid:107)∞

,

Lipschitz Continuity in Model-based Reinforcement Learning

Vector addition We show that +b : Rn

Rn has Lipschitz constant 1 for p = 0, 1,

for all b

Rn.

→

∞

∈

K
(cid:107)

.
(cid:107)

p,

.
(cid:107)

(cid:107)

p (ReLu) = sup
x1,x2

(cid:107)

+ b(x1)
−
x1 −
(cid:107)
(x1 + b)
−
(cid:107)
x1 −
(cid:107)

+b(x2)
p
(cid:107)
x2(cid:107)
p
(x2 + b)
p
(cid:107)
x2(cid:107)

p

= sup
x1,x2

x1 −
= (cid:107)
x1 −
(cid:107)

x2(cid:107)
x2(cid:107)

p

p

= 1

Supervised-learning domain We used the following 5 functions to generate the dataset:

f0(x) = tanh(x) + 3
x
f1(x) = x
f2(x) = sin(x)
f3(x) = sin(x)
f4(x) = sin(x)

−

−

5

∗

3
sin(x)

∗

We sampled each function 30 times, where the input was chosen uniformly randomly from [

2, 2] each time.

−

Lipschitz Continuity in Model-based Reinforcement Learning

Kavosh Asadi * 1 Dipendra Misra * 2 Michael L. Littman 1

8
1
0
2
 
l
u
J
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
3
9
1
7
0
.
4
0
8
1
:
v
i
X
r
a

Abstract
We examine the impact of learning Lipschitz
continuous models in the context of model-based
reinforcement learning. We provide a novel bound
on multi-step prediction error of Lipschitz models
where we quantify the error using the Wasserstein
metric. We go on to prove an error bound for
the value-function estimate arising from Lipschitz
models and show that the estimated value function
is itself Lipschitz. We conclude with empirical
results that show the beneﬁts of controlling the
Lipschitz constant of neural-network models.

1. Introduction

The model-based approach to reinforcement learning (RL)
focuses on predicting the dynamics of the environment
to plan and make high-quality decisions (Kaelbling et al.,
1996; Sutton & Barto, 1998). Although the behavior of
model-based algorithms in tabular environments is well
understood and can be effective (Sutton & Barto, 1998),
scaling up to the approximate setting can cause instabilities.
Even small model errors can be magniﬁed by the planning
process resulting in poor performance (Talvitie, 2014).

In this paper, we study model-based RL through the lens of
Lipschitz continuity, intuitively related to the smoothness
of a function. We show that the ability of a model to make
accurate multi-step predictions is related to the model’s
one-step accuracy, but also to the magnitude of the Lipschitz
constant (smoothness) of the model. We further show that
the dependence on the Lipschitz constant carries over to the
value-prediction problem, ultimately inﬂuencing the quality
of the policy found by planning.

We consider a setting with continuous state spaces and
stochastic transitions where we quantify the distance
between distributions using the Wasserstein metric. We

*Equal contribution

1Department of Computer Science,
Brown University, Providence, USA 2Department of Computer
Science and Cornell Tech, Cornell University, New York, USA.
Correspondence to: Kavosh Asadi <kavosh@brown.edu>.

introduce a novel characterization of models, referred
to as a Lipschitz model class, that represents stochastic
dynamics using a set of component deterministic functions.
This allows us to study any stochastic dynamic using
the Lipschitz continuity of its component deterministic
functions. To learn a Lipschitz model class in continuous
state spaces, we provide an Expectation-Maximization
algorithm (Dempster et al., 1977).

the learned models or

One promising direction for mitigating the effects of
inaccurate models is the idea of limiting the complexity
of
reducing the horizon of
planning (Jiang et al., 2015). Doing so can sometimes
make models more useful, much as regularization in
supervised learning can improve generalization performance
(Tibshirani, 1996). In this work, we also examine a type
of regularization that comes from controlling the Lipschitz
constant of models. This regularization technique can be
applied efﬁciently, as we will show, when we represent the
transition model by neural networks.

2. Background

,

A

, R, T, γ

We consider the Markov decision process (MDP) setting
in which the RL problem is formulated by the tuple
we mean a continuous state
. Here, by
(cid:105)
(cid:104)S
we mean a discrete action set. The functions
space and by
A
R and T :
) denote the reward
Pr(
R : S
S
S × A →
and transition dynamics. Finally, γ
[0, 1) is the discount
∈
rate. If
= 1, the setting is called a Markov reward
process (MRP).

|A|

→

×

A

S

2.1. Lipschitz Continuity

Our analyses leverage the “smoothness” of various
functions, quantiﬁed as follows.

Deﬁnition 1. Given two metric spaces (M1, d1) and
(M2, d2) consisting of a space and a distance metric, a
M2 is Lipschitz continuous (sometimes
function f : M1 (cid:55)→
simply Lipschitz) if the Lipschitz constant, deﬁned as

Kd1,d2(f ) :=

sup
M1,s2

s1

∈

d2

f (s1), f (s2)
d1(s1, s2)

,

(cid:1)

M1

∈

(cid:0)

(1)

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

is ﬁnite.

Lipschitz Continuity in Model-based Reinforcement Learning

Sometimes referred to as “Earth Mover’s distance”,
Wasserstein is the minimum expected distance between
pairs of points where the joint distribution j is constrained
to match the marginals µ1 and µ2. New applications of
this metric are discovered in machine learning, namely in
the context of generative adversarial networks (Arjovsky
et al., 2017) and value distributions in reinforcement
learning (Bellemare et al., 2017).

Wasserstein is linked to Lipschitz continuity using duality:

W (µ1, µ2) =

sup

f (s)µ1(s)

f (s)µ2(s)

ds .

f :Kd,dR (f )

1 (cid:90)

≤

(cid:0)

−

(cid:1)

(4)

This equivalence, known as Kantorovich-Rubinstein duality
(Villani, 2008), lets us compute Wasserstein by maximizing
R, a relatively
over a Lipschitz set of functions f :
In our theory, we utilize both
easier problem to solve.
deﬁnitions, namely the primal deﬁnition (3) and the dual
deﬁnition (4).

S (cid:55)→

3. Lipschitz Model Class

We introduce a novel representation of stochastic MDP
transitions in terms of a distribution over a set of
deterministic components.
Deﬁnition 4. Given a metric state space (
action space
Fg =
a

) and an
, we deﬁne Fg as a collection of functions:
a) where

f :
{
. We say that Fg is a Lipschitz model class if
∈ A

distributed according to g(f

A
S (cid:55)→ S}

, d
S

S

|

KF := sup
Fg

f

∈

KdS ,dS (f ) ,

Our deﬁnition captures a subset of stochastic transitions,
namely ones that can be represented as a state-independent
distribution over deterministic transitions. An example is
provided in Figure 2. We further prove in the appendix (see
Claim 1) that any ﬁnite MDP transition probabilities can be
decomposed into a state-independent distribution g over a
ﬁnite set of deterministic functions f .

Associated with a Lipschitz model class is a transition
function given by:

T (s(cid:48)

s, a) =

|

(cid:98)

f
(cid:88)

(cid:0)

1

f (s) = s(cid:48)

g(f

a) .

|

(cid:1)

Given a state distribution µ(s), we also deﬁne a generalized
notion of transition function
T
G

µ, a) given by:

(
· |

Figure 1. An illustration of Lipschitz continuity.
Pictorially,
Lipschitz continuity ensures that f lies in between the two afﬁne
functions (colored in blue) with slopes K and −K.

Equivalently, for a Lipschitz f ,

s1,

s2

d2

f (s1), f (s2)

Kd1,d2(f ) d1(s1, s2) .

∀

∀

≤

(cid:1)
The concept of Lipschitz continuity is visualized in Figure 1.

(cid:0)

A Lipschitz function f is called a non-expansion when
Kd1,d2(f ) = 1 and a contraction when Kd1,d2(f ) < 1.
Lipschitz continuity, in one form or another, has been a
key tool in the theory of reinforcement learning (Bertsekas,
1975; Bertsekas & Tsitsiklis, 1995; Littman & Szepesv´ari,
1996; M¨uller, 1996; Ferns et al., 2004; Hinderer, 2005;
Rachelson & Lagoudakis, 2010; Szepesv´ari, 2010; Pazis
& Parr, 2013; Pirotta et al., 2015; Pires & Szepesv´ari,
2016; Berkenkamp et al., 2017; Bellemare et al., 2017) and
bandits (Kleinberg et al., 2008; Bubeck et al., 2011). Below,
we also deﬁne Lipschitz continuity over a subset of inputs.
Deﬁnition 2. A function f : M1 × A (cid:55)→
Lipschitz continuous in

M2 is uniformly

if

A

is ﬁnite.

K Ad1,d2(f ) := sup
∈A

a

sup
s1,s2

d2

f (s1, a), f (s2, a)
d1(s1, s2)

(cid:0)

(cid:1)

,

(2)

is ﬁnite.

Note that the metric d1 is deﬁned only on M1.

2.2. Wasserstein Metric

We quantify the distance between two distributions using
the following metric:

Deﬁnition 3. Given a metric space (M, d) and the set
P(M ) of all probability measures on M , the Wasserstein
metric (or the 1st Kantorovic metric) between two
probability distributions µ1 and µ2 in P(M ) is deﬁned as

W (µ1, µ2) := inf
Λ
∈

j

(cid:90) (cid:90)

j(s1, s2)d(s1, s2)ds2 ds1 , (3)

where Λ denotes the collection of all joint distributions j on
M with marginals µ1 and µ2 (Vaserstein, 1969).
M

(cid:98)

×

(s(cid:48)

T
G

|

µ, a) =

1

f (s) = s(cid:48)
(cid:98)

g(f

a)

µ(s)ds .

(cid:90)s

f
(cid:88)

(cid:0)

(cid:124)

(cid:98)T (s(cid:48)

(cid:1)
s,a)
|
(cid:123)(cid:122)

|

(cid:125)

Lipschitz Continuity in Model-based Reinforcement Learning

Figure 2. An example of a Lipschitz model class in a gridworld
environment (Russell & Norvig, 1995). The dynamics are such
that any action choice results in an attempted transition in the
corresponding direction with probability 0.8 and in the neighboring
directions with probabilities 0.1 and 0.1. We can deﬁne Fg =
{f up, f right, f down, f left} where each f outputs a deterministic
next position in the grid (factoring in obstacles). For a = up,
we have: g(f up | a = up) = 0.8, g(f right
| a = up) =
g(f left | a = up) = 0.1, and g(f down | a = up) = 0. Deﬁning
distances between states as their Manhattan distance in the grid,
(cid:0)d(f (s1), f (s2)(cid:1)/d(s1, s2) = 2, and so KF =
then ∀f sups1,s2
2. So, the four functions and g comprise a Lipschitz model class.

T
G

. However, since

), the Lipschitz
We are primarily interested in K Ad,d(
T
G
constant of
takes as input a
T
G
(cid:98)
probability distribution and also outputs a probability
distribution, we require a notion of distance between two
distributions. This notion is quantiﬁed using Wasserstein
and is justiﬁed in the next section.

(cid:98)

(cid:98)

4. On the Choice of Probability Metric

We consider the stochastic model-based setting and show
through an example that
the Wasserstein metric is a
reasonable choice compared to other common options.

Consider a uniform distribution over states µ(s) as shown
in black in Figure 3 (top). Take a transition function T
in
G
the environment that, given an action a, uniformly randomly
adds or subtracts a scalar c1. The distribution of states
after one transition is shown in red in Figure 3 (middle).
Now, consider a transition model
that approximates T
G
by uniformly randomly adding or subtracting the scalar
c2. The distribution over states after one transition using
this imperfect model is shown in blue in Figure 3 (bottom).
We desire a metric that captures the similarity between the
output of the two transition functions. We ﬁrst consider
Kullback-Leibler (KL) divergence and observe that:

T
G

(cid:98)

KL

T
G

:=

(cid:0)

(cid:90)

(
· |
(s(cid:48)

T
G

µ, a),

(

T
G

· |
T
µ, a) log
G
(cid:98)
T
G

µ, a)
(s(cid:48)
(cid:1)
|
(s(cid:48) |

|

µ, a)

µ, a)

ds(cid:48) =

,

∞

unless the two constants are exactly the same.

(cid:98)

Figure 3. A state distribution µ(s) (top), a stochastic environment
that randomly adds or subtracts c1 (middle), and an approximate
transition model that randomly adds or subtracts a second scalar
c2 (bottom).

The next possible choice is Total Variation (TV) deﬁned as:

µ, a),

µ, a)

T
G

T V

:=

1
(cid:0)
2

(
· |
T
G

(s(cid:48)

|

(
· |

T
G
µ, a)
(cid:98)

(cid:1)
(s(cid:48)

T
G

−

|

µ, a)

ds(cid:48) = 1 ,

if the two distributions have disjoint supports regardless of
how far the supports are from each other.

(cid:98)

(cid:12)
(cid:12)

(cid:90)

(cid:12)
(cid:12)

In contrast, Wasserstein is sensitive to how far the constants
are as:

µ, a)

=

c1 −
|

c2|

.

W

T
G

(
· |

µ, a),

(

T
G

(cid:0)

· |
It is clear that, of the three, Wasserstein corresponds best
to the intuitive sense of how closely T
approximates
G
. This is particularly important in high-dimensional
T
G
spaces where the true distribution is known to usually lie in
low-dimensional manifolds. (Narayanan & Mitter, 2010)
(cid:98)

(cid:98)

(cid:1)

5. Understanding the Compounding Error

Phenomenon

−

To extract a prediction with a horizon n > 1, model-based
algorithms typically apply the model for n steps by taking
the state input in step t to be the state output from
the step t
1. Previous work has shown that model
error can result in poor long-horizon predictions and
ineffective planning (Talvitie, 2014; 2017). Observed even
beyond reinforcement learning (Lorenz, 1972; Venkatraman
et al., 2015), this is referred to as the compounding error
phenomenon. The goal of this section is to provide a bound
on multi-step prediction error of a model. We formalize the
notion of model accuracy below:

Deﬁnition 5. Given an MDP with a transition function
T , we identify a Lipschitz model Fg as ∆-accurate if its
induced

T satisﬁes:

s
∀

a W
(cid:98)
∀

· |

T (

s, a), T (

s, a)

∆ .

· |

≤

(cid:1)

(cid:0)

(cid:98)

Lipschitz Continuity in Model-based Reinforcement Learning

We want to express the multi-step Wasserstein error in
terms of the single-step Wasserstein error and the Lipschitz
constant of the transition function
. We provide a bound
using the following lemma:
on the Lipschitz constant of

T
G

Lemma 1. A generalized transition function
(cid:98)
a Lipschitz model class Fg is Lipschitz with a constant:

T
G

induced by

T
G

(cid:98)

(cid:98)

K AW,W (

T
G

) := sup

sup
µ1,µ2

a

(cid:98)

W

(

T
G

(
µ1, a),
T
·|
·|
G
W (µ1, µ2)

µ2, a)

KF

≤

(cid:1)

(cid:0)

(cid:98)

(cid:98)

the two input
Intuitively, Lemma 1 states that,
distributions are similar, then for any action the output
distributions given by
are also similar up to a KF factor.
We prove this lemma, as well as the subsequent lemmas, in
the appendix.

T
G

if

(cid:98)

Given the one-step error (Deﬁnition 5), a start state
distribution µ and a ﬁxed sequence of actions a0, ..., an
1,
we desire a bound on n-step error:

−

δ(n) := W

T n
G

(
· |

µ), T n
G

(

· |

µ)

,

where

T n
G

(
·|

µ) :=

(cid:0)
(cid:98)
T
G

T
G

(
·|

(cid:1)

...

(
·|

µ, a0)..., an

(
·|

T
G
n recursive calls

−

2), an

1)

−

(cid:98)
(cid:98)
and T n
(
· |
G
lemma followed by the theorem.

µ) is deﬁned similarly. We provide a useful
(cid:125)

(cid:98)
(cid:124)

(cid:123)(cid:122)

(cid:98)

Lemma 2. (Composition Lemma) Deﬁne three metric
spaces (M1, d1), (M2, d2), and (M3, d3). Deﬁne Lipschitz
M2 with constants
M3 and g : M1 (cid:55)→
functions f : M2 (cid:55)→
Kd2,d3(f ) and Kd1,d2 (g). Then, h : f
M3 is
g : M1 (cid:55)→
Kd2,d3 (f )Kd1,d2(g).
Lipschitz with constant Kd1,d3 (h)

◦

≤

Similar to composition, we can show that summation
preserves Lipschitz continuity with a constant bounded by
the sum of the Lipschitz constants of the two functions. We
omitted this result due to brevity.

Theorem 1. Deﬁne a ∆-accurate
with the Lipschitz
constant KF and an MDP with a Lipschitz transition
with constant KT . Let ¯K = min
function T
.
KF , KT
}
{
G
1:
Then
≥

T
G

n

∀

(cid:98)

δ(n) := W

T n
G

(
· |

µ), T n
G

(

· |

µ)

∆

≤

( ¯K)i .

n

1

−

i=0
(cid:88)

(cid:1)

(cid:0)

(cid:98)

(cid:1)
s, a0)

f (s(cid:48))µ(s) ds ds(cid:48)

(cid:1)
f (s(cid:48)) ds(cid:48)

µ(s) ds

δ(1) := W

:= sup

µ, a0)

(

T
· |
G
T (s(cid:48)
(cid:98)

|

(

µ, a0), T
· |
G
T (s(cid:48)

s, a0)

−

|

(cid:0)
f (cid:90) (cid:90)
sup
f (cid:90)

(cid:0)
=W

(cid:98)

(cid:0)
(cid:98)
T (s(cid:48)

s, a0)
|

−

T (s(cid:48)

s, a0)
|

(cid:1)
due to duality (4)

(cid:1)
s, a0)

µ(s) ds

(cid:125)

(cid:124)
W

(cid:98)T (

s,a0),T (

·|

s,a0)
·|
(cid:123)(cid:122)

s, a0), T (

(cid:0)
T (
· |
∆ due to Deﬁnition 5

· |

(cid:0)

≤

(cid:98)

(cid:124)
∆ µ(s) ds = ∆ .

(cid:123)(cid:122)

(cid:1)

(cid:125)

≤

(cid:90)

=

(cid:90)

≤

(cid:90)

1

−

We now prove the inductive step. Assuming δ(n
T n
W
(
G
write:
(cid:0)
(cid:98)
δ(n) := W

µ), T n
G

(
· |

(cid:1)
µ)

(cid:80)

µ)

· |

∆

≤

−

(

1

1) :=
i=0 (KF )i we can

−

−

n

2

T n
(
· |
G
T
µ),
G

µ), T n
· |
G
T n
−
· |
G
µ), an

−

1
(
(cid:98)

(cid:0)
· |

1

−

(
· |

1

(

(cid:1)
· |

µ), an

1

−

1

−

, T n
G

(
· |

(cid:1)
1),

T
G

· |

W

≤
+W

T n
G

(cid:16)
T
(cid:98)
G

= W

+W

(cid:16)

T
(cid:98)
G

(cid:0)
(
· |

(cid:16)
T
(cid:98)
G
(cid:0)

· |

· |

(cid:0)
(
(cid:98)
· |
T n
G
T n
G
T n
(cid:98)
G

µ), an

−

1

1

−

(
· |

µ), an

, T
(cid:98)
G

(cid:0)
(
· |

(cid:17)
T n
−
G
T n
G

−

1

1

(

· |

(

· |

µ), an

1

−

µ), an

1)

(cid:16)

−
We now use Lemma 1 and Deﬁnition 5 to upper bound the
ﬁrst and the second term of the last line respectively.

(cid:98)

(cid:1)

−

(cid:1)(cid:17)

(cid:17)

(cid:1)(cid:17)

µ)

(Triangle ineq)

δ(n)

KF W

≤

(cid:0)
= KF δ(n

1

−

T n
G

(
· |

1

µ), T n
G
n

−

(cid:98)
−

1) + ∆

∆

≤

+ ∆

µ)

(
· |
1
(KF )i .

(cid:1)

−

(5)

i=0
(cid:88)

in the triangle inequality, we may replace
T n
1
and follow
G

(
· |

µ)

µ)

· |

Note that
T n
with T
(
T
· |
G
G
G
the same basic steps to get:
(cid:98)

· |

(cid:98)

(cid:1)

(cid:0)

(cid:0)

(cid:1)

−

−

1

W

(

T n
G

· |

µ), T n
G

(
· |

µ)

(cid:0)

(cid:98)

n

−

1
(KT )i .

∆

≤

(cid:1)

i=0
(cid:88)

Combining (5) and (6) allows us to write:

(6)

δ(n) = W

T n
G

(
· |
n

(cid:0)
min

∆

(
· |

µ), T n
G
1
(KT )i, ∆

µ)
n
(cid:1)
−

−

1
(KF )i

(cid:41)

i=0
(cid:88)

≤

= ∆

(cid:98)
(cid:40)
1

n

−

i=0
(cid:88)

i=0
(cid:88)
( ¯K)i ,

which concludes the proof.

Proof. We construct a proof by induction.
Using
Kantarovich-Rubinstein duality (Lipschitz property of f
not shown for brevity) we ﬁrst prove the base of induction:

There exist similar
results in the literature relating
one-step transition error to multi-step transition error and
sub-optimality bounds for planning with an approximate

Lipschitz Continuity in Model-based Reinforcement Learning

model. The Simulation Lemma (Kearns & Singh, 2002;
Strehl et al., 2009) is for discrete state MDPs and relates
error in the one-step model to the value obtained by
using it for planning. A related result for continuous
state-spaces (Kakade et al., 2003) bounds the error in
estimating the probability of a trajectory using total
variation. A second related result (Venkatraman et al.,
2015) provides a slightly looser bound for prediction error
in the deterministic case—our result can be thought of as a
generalization of their result to the probabilistic case.

6. Value Error with Lipschitz Models

,

A

(cid:104)S

, T, R, γ

We next investigate the error in the state-value function
induced by a Lipschitz model class. To answer this question,
we consider an MRP M1 denoted by
and
a second MRP M2 that only differs from the ﬁrst in its
be the
transition function
T , R, γ
action set with a single action a. We further assume that
the reward function is only dependent upon state. We ﬁrst
express the state-value function for a start state s with
respect to the two transition functions. By δs below, we
mean a Dirac delta function denoting a distribution with
probability 1 at state s.

. Let
(cid:105)

,
A

a
}

(cid:104)S

A

=

(cid:98)

{

(cid:105)

,

VT (s) :=

(s(cid:48)

δs)R(s(cid:48)) ds(cid:48) ,

∞

γn

n=0
(cid:88)

(cid:90)

T n
G

∞

γn

n=0
(cid:88)

(cid:90)

T n
G

(cid:98)

|

|

V (cid:98)T (s) :=

(s(cid:48)

δs)R(s(cid:48)) ds(cid:48) .

Let

=

h : KdS ,R(h)

F

{

1

. Then given f
}

≤

:

∈ F

KR

∞

γn

f (s(cid:48))

T n
G

(s(cid:48)

δs)

|

T n
G

(s(cid:48)

δs)
|

−

ds(cid:48)

n=0
(cid:88)

KR

(cid:90)
∞

n=0
(cid:88)

(cid:0)
γn sup
f

∈F (cid:90)

≤

= KR

∞

(cid:124)
γn W

f (s(cid:48))

δs)

(s(cid:48)

δs)

ds(cid:48)

(cid:98)
T n
(s(cid:48)
G

|

(cid:1)

−

T n
G

(cid:98)

|

(cid:1)

(cid:125)

:=W

δs), (cid:98)T n

due to duality (4)

(cid:0)
T n
G (.
|

δs)
G (.
|
(cid:123)(cid:122)
δs)

(cid:1)

(cid:0)
|

(.

(.

δs),

T n
G

T n
G
(cid:80)n−1
(cid:0)
(cid:1)
i=0 ∆( ¯K)i due to Theorem 1
(cid:98)
1
(cid:125)

(cid:123)(cid:122)

|

≤
n
(cid:124)

−

n=0
(cid:88)

n=0
(cid:88)

∞

KR

∞

γn

∆( ¯K)i

= KR∆

≤

=

i=0
(cid:88)
γn 1
−
1
−

¯K n
¯K

.

n=0
(cid:88)
γKR∆
γ)(1

−

−

(1

γ ¯K)
We can derive the same bound for V (cid:98)T (s)
VT (s) using
the fact that Wasserstein distance is a metric, and therefore
symmetric, thereby completing the proof.

−

Regarding the tightness of our bounds, we can show that
when the transition model is deterministic and linear then
Theorem 1 provides a tight bound. Moreover, if the reward
function is linear, the bound provided by Theorem 2 is tight.
(See Claim 2 in the appendix.) Notice also that our proof
does not require a bounded reward function.

7. Lipschitz Generalized Value Iteration

Next we derive a bound on

VT (s)

s.

∀

−

V (cid:98)T (s)
(cid:12)
(cid:12)

Theorem 2. Assume a Lipschitz model class Fg with a
(cid:12)
(cid:12)
T with ¯K = min
∆-accurate
. Further, assume
KF , KT
}
{
a Lipschitz reward function with constant KR = KdS ,R(R).
Then

and ¯K
(cid:98)

[0, 1
γ )

s
∀

∈ S

∈

We next show that, given a Lipschitz transition model,
solving for the ﬁxed point of a class of Bellman equations
yields a Lipschitz state-action value function. Our proof is in
the context of Generalized Value Iteration (GVI) (Littman &
Szepesv´ari, 1996), which deﬁnes Value Iteration (Bellman,
1957) for planning with arbitrary backup operators.

VT (s)

V (cid:98)T (s)

−

≤

(1

(cid:12)
(cid:12)

(cid:12)
(cid:12)

γKR∆
γ)(1

−

−

.

γ ¯K)

Proof. We ﬁrst deﬁne the function f (s) = R(s)
KR
observed that KdS ,R(f ) = 1. We now write:

. It can be

VT (s)

V (cid:98)T (s)

−
γn

∞

=

n=0
(cid:88)

(cid:90)

= KR

∞

γn

T n
G

(cid:0)
f (s(cid:48))

n=0
(cid:88)

(cid:90)

(cid:0)

R(s(cid:48))

(s(cid:48)

δs)

(s(cid:48)

δs)

ds(cid:48)

|

|

T n
G

−

(cid:98)
δs)

(cid:98)

(s(cid:48)

T n
G

|

(s(cid:48)

T n
G

|

−

(cid:1)
δs)

ds(cid:48)

(cid:1)

Algorithm 1 GVI algorithm

Input: initial
repeat

Q(s, a), δ, and choose an operator f

for each s, a
(cid:98)
∈ S × A
R(s, a)+γ
Q(s, a)

do

←

end for
(cid:98)

until convergence

|

(cid:82)

(cid:98)

T (s(cid:48)

s, a)f

Q(s(cid:48),

ds(cid:48)

)
·

(cid:1)

(cid:0)

(cid:98)

To prove the result, we make use of the following lemmas.
Lemma 3. Given a Lipschitz function f :
R with
constant KdS ,dR(f ):

S (cid:55)→

K AdS ,dR

T (s(cid:48)

s, a)f (s(cid:48))ds(cid:48)
|

KdS ,dR(f )K AdS ,W

T

.

≤

(cid:17)

(cid:16) (cid:90)

(cid:98)

(cid:0)

(cid:1)

(cid:98)

Lipschitz Continuity in Model-based Reinforcement Learning

Lemma 4. The following operators (Asadi & Littman,
2017) are Lipschitz with constants:

(cid:107)(cid:107)∞,dR

mean(x)

=

(cid:0)

(cid:1)

1. K
K

2. K

3. K

(cid:107)(cid:107)∞,dR (max(x)) = K
(cid:107)(cid:107)∞,dR ((cid:15)-greedy(x)) = 1
(cid:107)(cid:107)∞,dR (mmβ(x) := log
(cid:107)(cid:107)∞,dR (boltzβ(x)
:=
A
βVmax|
|

(cid:80)n

β

(cid:80)

i eβxi
n

) = 1

i=1 xieβxi
(cid:80)n
i=1eβ xi

)

≤

+

A
|

|

(cid:112)

Theorem 3. For any non-expansion backup operator f
outlined in Lemma 4, GVI computes a value function

with a Lipschitz constant bounded by
γK AdS ,W (T ) < 1.

(R)

KA
dS ,dR
γKdS ,W (T )

1

−

if

Proof. From Algorithm 1, in the nth round of GVI updates:

·

(cid:1)

)
·

Qn+1(s, a)

R(s, a) + γ

T (s(cid:48)

s, a)f

Qn(s(cid:48),

)

ds(cid:48).

←

|

(cid:90)

(cid:0)

(cid:98)

(cid:98)
Now observe that:

K AdS ,dR (

Qn+1)

≤

≤

K AdS ,dR(R)+γK AdS ,dR

(cid:98)

T (s(cid:48)

s, a)f

Qn(s(cid:48),

ds(cid:48)

K AdS ,dR (R) + γK AdS ,W (T ) KdS,R

(cid:90)

(cid:0)

(cid:1)

(cid:0)
Qn(s,
(cid:98)

·

f

(cid:16)

)
(cid:1)(cid:17)
(cid:0)
(cid:107)·(cid:107)∞,dR(f )K AdS ,dR (
(cid:98)
Qn)

(cid:1)

Qn)

K AdS ,dR (R) + γK AdS ,W (T )K

≤
= K AdS ,dR (R) + γK AdS ,W (T )K AdS ,dR (

(cid:98)
Where we used Lemmas 3, 2, and 4 for the second, third,
and fourth inequality respectively. Equivalently:

(cid:98)

|

n

K AdS ,dR (

Qn+1)

K AdS ,dR(R)

γK AdS ,W (T )

i

≤

+

(cid:98)

i=0
(cid:88)
(cid:0)
n
γK AdS ,W (T )

K AdS ,dR (

(cid:1)
Q0) .

By computing the limit of both sides, we get:

(cid:1)

(cid:0)

K AdS ,dR (

Qn+1)

K AdS ,dR (R)

γK AdS ,W (T )

lim
n
→∞

(cid:98)

(cid:98)

n

i=0
(cid:88)
(cid:0)
γK AdS ,W (T )

n

K AdS ,dR (

(cid:1)
Q0)

K AdS ,dR (R)

(cid:0)
γKdS ,W (T )

(cid:1)
+ 0 ,

i

(cid:98)

lim
n
→∞

≤

+ lim
n
→∞

=

1

−

This concludes the proof.

Two implications of this result: First, PAC exploration in
continuous state spaces is shown assuming a Lipschitz value
function (Pazis & Parr, 2013). However, the theorem shows

that it is sufﬁcient to have a Lipschitz model, an assumption
perhaps easier to conﬁrm. The second implication relates to
value-aware model learning (VAML) objective (Farahmand
et al., 2017). Using the above theorem, we can show that
minimizing Wasserstein is equivalent to minimizing the
VAML objective (Asadi et al., 2018).

8. Experiments

|S|

Our ﬁrst goal in this section1 is to compare TV, KL, and
Wasserstein in terms of the ability to best quantify error of
an imperfect model. To this end, we built ﬁnite MRPs with
random transitions,
= 10 states, and γ = 0.95. In the
ﬁrst case the reward signal is randomly sampled from [0, 10],
and in the second case the reward of an state is the index of
that state, so small Euclidean norm between two states is
an indication of similar values. For 105 trials, we generated
an MRP and a random model, and then computed model
error and planning error (Figure 4). We understand a good
metric as the one that computes a model error with a high
correlation with value error. We show these correlations for
different values of γ in Figure 5.

Figure 4. Value error (x axis) and model error (y axis). When
the reward is the index of the state (right), correlation between
Wasserstein error and value-prediction error is high.
This
highlights the fact that when closeness in the state-space is an
indication of similar values, Wasserstein can be a powerful metric
for model-based RL. Note that Wasserstein provides no advantage
given random rewards (left).

Figure 5. Correlation between value-prediction error and model
error for the three metrics using random rewards (left) and index
rewards (right). Given a useful notion of state similarities, low
Wasserstein error is a better indication of planning error.

1We release the code here: github.com/kavosh8/Lip

Lipschitz Continuity in Model-based Reinforcement Learning

Function f

Deﬁnition

ReLu : Rn

+b : Rn

Rn,

W : Rn

×

→
Rm,

→

Rn
→
b
∀
W

∈

∀

∈

Rn
Rm

n

×

ReLu(x)i := max
{
+b(x) := x + b

0, xi

}

W (x) := W x

×

Lipschitz constant K

p,

(cid:107)(cid:107)

(cid:107)(cid:107)

p = 1
1
1

p = 2
1
1

Wj

j (cid:107)

(cid:107)∞

Wj

2
2

(cid:107)

j (cid:107)

(cid:80)

(cid:113)(cid:80)

p (f )
p =
1
1
supj (cid:107)

∞

Wj

(cid:107)1

Table 1. Lipschitz constant for various functions used in a neural network. Here, Wj denotes the jth row of a weight matrix W .

It is known that controlling the Lipschitz constant of neural
nets can help in terms of improving generalization error due
to a lower bound on Rademacher complexity (Neyshabur
et al., 2015; Bartlett & Mendelson, 2002). It then follows
from Theorems 1 and 2 that controlling the Lipschitz
constant of a learned transition model can achieve better
error bounds for multi-step and value predictions. To
enforce this constraint during learning, we bound the
Lipschitz constant of various operations used in building
neural network. The bound on the constant of the entire
neural network then follows from Lemma 2. In Table 1, we
provide Lipschitz constant for operations (see Appendix for
proof) used in our experiments. We quantify these results
for different p-norms

(cid:107)·(cid:107)p.

speciﬁcally when the transition model

Given these simple methods for enforcing Lipschitz
continuity, we performed empirical evaluations
to
understand the impact of Lipschitz continuity of transition
models,
is
used to perform multi-step state-predictions and policy
improvements. We chose two standard domains: Cart Pole
and Pendulum. In Cart Pole, we trained a network on a
dataset of 15
. During training, we
(cid:105)
ensured that the weights of the network are smaller than k.
For each k, we performed 20 independent model estimation,
and chose the model with median cross-validation error.

s, a, s(cid:48)
(cid:104)

103 tuples

∗

Using the learned model, along with the actual reward
signal of the environment, we then performed stochastic
actor-critic RL. (Barto et al., 1983; Sutton et al., 2000)
This required an interaction between the policy and the
learned model for relatively long trajectories. To measure
the usefulness of the model, we then tested the learned
policy on the actual domain. We repeated this experiment
on Pendulum. To train the neural transition model for
this domain we used 104 samples. Notably, we used
deterministic policy gradient (Silver et al., 2014) for training
the policy network with the hyper parameters suggested by
Lillicrap et al. (2015). We report these results in Figure 6.

Observe that an intermediate Lipschitz constant yields the
best result. Consistent with the theory, controlling the
Lipschitz constant in practice can combat the compounding
errors and can help in the value estimation problem. This
ultimately results in learning a better policy.

We next examined if the beneﬁts carry over to stochastic

Figure 6. Impact of Lipschitz constant of learned models in Cart
Pole (left) and Pendulum (right). An intermediate value of k
(Lipschitz constant) yields the best performance.

Fg

θf : f
{

settings. To capture stochasticity we need an algorithm to
learn a Lipschitz model class (Deﬁnition 4). We used an EM
algorithm to joinly learn a set of functions f , parameterized
by θ =
, and a distribution over functions
}
g. Note that in practice our dataset only consists of a set
of samples
and does not include the function the
sample is drawn from. Hence, we consider this as our
latent variable z. As is standard with EM, we start with the
log-likelihood objective (for simplicity of presentation we
assume a single action in the derivation):

s, a, s(cid:48)
(cid:104)

∈

(cid:105)

L(θ) =

log p(si, si(cid:48); θ)

N

i=1
(cid:88)
N

i=1
(cid:88)
N

i=1
(cid:88)
N

=

=

≥

log

p(zi = f, si, si(cid:48); θ)

f
(cid:88)

f
(cid:88)

log

q(zi = f

si, si(cid:48))

|

q(zi = f

si, si(cid:48))log

|

i=1
(cid:88)

f
(cid:88)

p(zi = f, si, si(cid:48); θ)
si, si(cid:48))
|

q(zi = f

p(zi = f, si, si(cid:48); θ)
si, si(cid:48))
|

q(zi = f

,

where we used Jensen’s inequality and concavity of log in
the last line. This derivation leads to the following EM
algorithm.

Lipschitz Continuity in Model-based Reinforcement Learning

Figure 7. A stochastic problem solved by training a Lipschitz
model class using EM. The top left ﬁgure shows the functions
before any training (iteration 0), and the bottom right ﬁgure shows
the ﬁnal results (iteration 50).

In the M step, ﬁnd θt by solving for:

N

argmax

θ

i=1
(cid:88)

f
(cid:88)

qt

1(zi = f

−

si, si(cid:48))log
|

p(zi = f, si, si(cid:48); θ)
si, si(cid:48))
qt

1(zi = f

−

|

In the E step, compute posteriors:

qt(zi = f

si, si(cid:48)) =
|

p(si, si(cid:48)
f p(si, si(cid:48)|

|

zi = f ; θt

f )g(zi = f ; θt)

zi = f ; θt

f )g(zi = f ; θt)

.

Note that we assume each point is drawn from a neural
network f with probability:

(cid:80)

p

si, si(cid:48)

zi = f ; θt
|

N
and with a ﬁxed variance σ2 tuned as a hyper-parameter.

−

(cid:17)

(cid:0)

(cid:1)

(cid:16)(cid:12)
(cid:12)

(cid:12)
(cid:12)

f

=

si(cid:48)

f (si, θt

f )

, σ2

,

We used a supervised-learning domain to evaluate the EM
algorithm. We generated 30 points from 5 functions (written
at the end of Appendix) and trained 5 neural networks to ﬁt
these points. Iterations of a single run is shown in Figure 7
and the summary of results is presented in Figure 8. Observe
that the EM algorithm is effective, and that controlling the
Lipschitz constant is again useful.

We next applied EM to train a transition model for an RL
setting, namely the gridworld domain from Moerland et al.
(2017). Here a useful model needs to capture the stochastic
behavior of the two ghosts. We modify the reward to be
-1 whenever the agent is in the same cell as either one of
the ghosts and 0 otherwise. We performed environmental
interactions for 1000 time-steps and measured the return.
We compared against standard tabular methods(Sutton
& Barto, 1998), and a deterministic model that predicts
expected next state (Sutton et al., 2008; Parr et al., 2008). In
all cases we used value iteration for planning.

Figure 8. Impact of controlling the Lipschitz constant in the
supervised-learning domain.
Notice the U-shape of ﬁnal
Wasserstein loss with respect to Lipschitz constant k.

Figure 9. Performance of a Lipschitz model class on the gridworld
domain. We show model test accuracy (left) and quality of the
policy found using the model (right). Notice the poor performance
of tabular and expected models.

Results in Figure 9 show that tabular models fail due to no
generalization, and expected models fail since the ghosts
do not move on expectation, a prediction not useful for
planner. Performing value iteration with a Lipschitz model
class outperforms the baselines.

9. Conclusion

We took an important
step towards understanding
model-based RL with function approximation. We showed
that Lipschitz continuity of an estimated model plays
a central role in multi-step prediction error, and in
value-estimation error. We also showed the beneﬁts of
employing Wasserstein for model-based RL. An important
future work is to apply these ideas to larger problems.

10. Acknowledgements

The authors recognize the assistance of Eli Upfal, John
Langford, George Konidaris, and members of Brown’s Rlab
speciﬁcally Cameron Allen, David Abel, and Evan Cater.
The authors also thank anonymous ICML reviewer 1 for
insights on a value-aware interpretation of Wasserstein.

Lipschitz Continuity in Model-based Reinforcement Learning

References

Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein
In International

generative adversarial networks.
Conference on Machine Learning, pp. 214–223, 2017.

Asadi, K. and Littman, M. L. An alternative softmax
operator for reinforcement learning. In Proceedings of
the 34th International Conference on Machine Learning,
pp. 243–252, 2017.

Asadi, K., Cater, E., Misra, D., and Littman, M. L.
Equivalence between wasserstein and value-aware
model-based reinforcement learning. arXiv preprint
arXiv:1806.01265, 2018.

Bartlett, P. L. and Mendelson, S. Rademacher and gaussian
complexities: Risk bounds and structural results. Journal
of Machine Learning Research, 3(Nov):463–482, 2002.

Barto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike
adaptive elements that can solve difﬁcult learning control
IEEE transactions on systems, man, and
problems.
cybernetics, pp. 834–846, 1983.

Bellemare, M. G., Dabney, W., and Munos, R. A
distributional perspective on reinforcement learning. In
International Conference on Machine Learning, pp.
449–458, 2017.

Bellman, R. A markovian decision process. Journal of

Mathematics and Mechanics, pp. 679–684, 1957.

Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause,
A.
Safe model-based reinforcement learning with
stability guarantees. In Advances in Neural Information
Processing Systems, pp. 908–919, 2017.

Bertsekas, D. Convergence of discretization procedures in
dynamic programming. IEEE Transactions on Automatic
Control, 20(3):415–419, 1975.

Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-dynamic
programming: an overview. In Decision and Control,
1995, Proceedings of the 34th IEEE Conference on,
volume 1, pp. 560–564. IEEE, 1995.

Bubeck, S., Munos, R., Stoltz, G., and Szepesv´ari, C.
X-armed bandits. Journal of Machine Learning Research,
12(May):1655–1695, 2011.

Dempster, A. P., Laird, N. M., and Rubin, D. B.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society. Series
B (methodological), pp. 1–38, 1977.

Farahmand, A.-M., Barreto, A., and Nikovski, D.
for Model-based
the

Value-Aware
Reinforcement Learning.

In Proceedings of

Function

Loss

20th International Conference on Artiﬁcial Intelligence
and Statistics, pp. 1486–1494, 2017.

Ferns, N., Panangaden, P., and Precup, D. Metrics for ﬁnite
markov decision processes. In Proceedings of the 20th
conference on Uncertainty in artiﬁcial intelligence, pp.
162–169. AUAI Press, 2004.

Fox, R., Pakman, A., and Tishby, N. G-learning: Taming
the noise in reinforcement learning via soft updates.
Uncertainty in Artiﬁcal Intelligence, 2016.

Gao, B. and Pavel, L.

On the properties of the
softmax function with application in game theory and
reinforcement learning. arXiv preprint arXiv:1704.00805,
2017.

Hinderer, K. Lipschitz continuity of value functions in
Markovian decision processes. Mathematical Methods of
Operations Research, 62(1):3–22, 2005.

Jiang, N., Kulesza, A., Singh, S., and Lewis, R. The
dependence of effective planning horizon on model
In Proceedings of AAMAS, pp. 1181–1189,
accuracy.
2015.

Kaelbling, L. P., Littman, M. L., and Moore, A. W.
Reinforcement learning: A survey. Journal of artiﬁcial
intelligence research, 4:237–285, 1996.

Kakade, S., Kearns, M. J., and Langford, J. Exploration
the
in metric state spaces.
20th International Conference on Machine Learning
(ICML-03), pp. 306–312, 2003.

In Proceedings of

Kearns, M. and Singh, S. Near-optimal reinforcement
learning in polynomial time. Machine Learning, 49(2-3):
209–232, 2002.

Kleinberg, R., Slivkins, A., and Upfal, E. Multi-armed
bandits in metric spaces. In Proceedings of the Fortieth
Annual ACM Symposium on Theory of Computing, pp.
681–690. ACM, 2008.

Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.

Littman, M. L. and Szepesv´ari, C.
A generalized
reinforcement-learning model:
and
applications. In Proceedings of the 13th International
Conference on Machine Learning, pp. 310–318, 1996.

Convergence

Lorenz, E. Predictability: does the ﬂap of a butterﬂy’s wing

in Brazil set off a tornado in Texas? na, 1972.

Lipschitz Continuity in Model-based Reinforcement Learning

Moerland, T. M., Broekens, J., and Jonker, C. M. Learning
for model-based
multimodal
reinforcement learning. arXiv preprint arXiv:1705.00470,
2017.

transition dynamics

M¨uller, A. Optimal selection from distributions with
unknown parameters: Robustness of bayesian models.
Mathematical Methods of Operations Research, 44(3):
371–386, 1996.

Nachum, O., Norouzi, M., Xu, K., and Schuurmans,
D. Bridging the gap between value and policy based
reinforcement learning. arXiv preprint arXiv:1702.08892,
2017.

Narayanan, H. and Mitter, S.

Sample complexity of
testing the manifold hypothesis. In Advances in Neural
Information Processing Systems, pp. 1786–1794, 2010.

Strehl, A. L., Li, L., and Littman, M. L. Reinforcement
learning in ﬁnite mdps: Pac analysis. Journal of Machine
Learning Research, 10(Nov):2413–2444, 2009.

Sutton, R. S. and Barto, A. G. Reinforcement Learning: An

Introduction. The MIT Press, 1998.

Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour,
Y. Policy gradient methods for reinforcement learning
In Advances in Neural
with function approximation.
Information Processing Systems, pp. 1057–1063, 2000.

Sutton, R. S., Szepesv´ari, C., Geramifard, A., and Bowling,
M. H.
Dyna-style planning with linear function
approximation and prioritized sweeping. In UAI 2008,
Proceedings of the 24th Conference in Uncertainty in
Artiﬁcial Intelligence, Helsinki, Finland, July 9-12, 2008,
pp. 528–536, 2008.

Neu, G., Jonsson, A., and G´omez, V. A uniﬁed view of
entropy-regularized Markov decision processes. arXiv
preprint arXiv:1705.07798, 2017.

Szepesv´ari, C. Algorithms for reinforcement learning.
Synthesis Lectures on Artiﬁcial Intelligence and Machine
Learning, 4(1):1–103, 2010.

Talvitie, E. Model regularization for stable sample
rollouts. In Proceedings of the Thirtieth Conference on
Uncertainty in Artiﬁcial Intelligence, pp. 780–789. AUAI
Press, 2014.

Talvitie, E.

Self-correcting models for model-based
reinforcement learning. In Proceedings of the Thirty-First
AAAI Conference on Artiﬁcial Intelligence, February 4-9,
2017, San Francisco, California, USA., 2017.

Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society. Series B
(Methodological), pp. 267–288, 1996.

Vaserstein, L. N. Markov processes over denumerable
products of spaces, describing large systems of automata.
Problemy Peredachi Informatsii, 5(3):64–72, 1969.

Venkatraman, A., Hebert, M., and Bagnell, J. A. Improving
multi-step prediction of learned time series models. In
Proceedings of the Twenty-Ninth AAAI Conference on
Artiﬁcial Intelligence, January 25-30, 2015, Austin, Texas,
USA., 2015.

Villani, C. Optimal transport: old and new, volume 338.

Springer Science & Business Media, 2008.

Neyshabur, B., Tomioka, R., and Srebro, N. Norm-based
capacity control in neural networks. In Proceedings of
The 28th Conference on Learning Theory, pp. 1376–1401,
2015.

Parr, R., Li, L., Taylor, G., Painter-Wakeﬁeld, C., and
Littman, M. L. An analysis of linear models, linear
value-function approximation, and feature selection
In Proceedings of the
for reinforcement learning.
25th international conference on Machine learning, pp.
752–759. ACM, 2008.

Pazis, J. and Parr, R. Pac optimal exploration in continuous

space markov decision processes. In AAAI, 2013.

Pires, B. ´A. and Szepesv´ari, C. Policy error bounds for
model-based reinforcement learning with factored linear
models. In Conference on Learning Theory, pp. 121–151,
2016.

Pirotta, M., Restelli, M., and Bascetta, L. Policy gradient in
lipschitz Markov decision processes. Machine Learning,
100(2-3):255–283, 2015.

Rachelson, E. and Lagoudakis, M. G. On the locality of
action domination in sequential decision making.
In
International Symposium on Artiﬁcial Intelligence and
Mathematics, ISAIM 2010, Fort Lauderdale, Florida,
USA, January 6-8, 2010, 2010.

Russell, S. J. and Norvig, P. Artiﬁcial intelligence: A

modern approach, 1995.

Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and
Riedmiller, M. Deterministic policy gradient algorithms.
In ICML, 2014.

Lipschitz Continuity in Model-based Reinforcement Learning

Appendix

Claim 1. In a ﬁnite MDP, transition probabilities can be expressed using a ﬁnite set of deterministic functions and a
distribution over the functions.

Proof. Let P r(s, a, s(cid:48)) denote the probability of a transiton from s to s(cid:48) when executing the action a. Deﬁne an ordering
over states s1, ..., sn with an additional unreachable state s0. Now deﬁne the cumulative probability distribution:

Further deﬁne L as the set of distinct entries in C:

C(s, a, si) :=

P r(s, a, sj) .

i

j=0
(cid:88)

L :=

C(s, a, si)
|

s

, i

[0, n]

.

∈ S

∈

(cid:110)

(cid:111)

Note that, since the MDP is assumed to be ﬁnite, then
value of the set. Note that c0 = 0 and c
L
|

|

i = 1 to

and

j = 1 to n, deﬁne fi(s) = sj if and only if:

∀

L
|

|

∀

= 1. We now build determinstic set of functions f1, ..., f

is ﬁnite. We sort the values of L and denote, by ci, ith smallest
as follows:

L
|

|

L
|

|

We also deﬁne the probability distribution g over f as follows:

C(s, a, sj

1) < ci

C(s, a, sj) .

−

≤

g(fi

a) := ci
|

−

ci

1 .

−

Given the functions f1, ..., f
executing action a:

L
|

|

and the distribution g, we can now compute the probability of a transition to sj from s after

1(fi(s) = sj) g(fi

a)
|

i
(cid:88)
=

1

i
(cid:88)

(cid:0)

= C(s, a, sj)
−
= P r(s, a, sj) ,

C(s, a, sj

1) < ci

C(s, a, sj)

(ci

−

≤

ci

1)

−

−

C(s, a, sj

1)

−

(cid:1)

where 1 is a binary function that outputs one if and only if its condition holds. We reconstructed the transition probabilities
using distribution g and deterministic functions f1, ..., f

.

L
|

|

Claim 2. Given a deterministic and linear transition model, and a linear reward signal, the bounds provided in Theorems 1
and 2 are both tight.

Assume a linear transition function T deﬁned as:

Assume our learned transition function ˆT :

Note that:

and that:

T (s) = Ks

ˆT (s) := Ks + ∆

max
s

T (s)

−

ˆT (s)

= ∆

(cid:12)
(cid:12)
KT , K ˆT }
min
{

(cid:12)
(cid:12)
= K

First observe that the bound in Theorem 2 is tight for n = 2:

T

T (s)

ˆT

ˆT (s)

=

K 2s

s
∀

(cid:12)
(cid:12)
(cid:12)

−

(cid:0)

(cid:1)

(cid:0)

(cid:1)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

−

K 2s + ∆(1 + K)
(cid:12)
(cid:12)
(cid:12)

= ∆

K i

1

i=0
(cid:88)

Lipschitz Continuity in Model-based Reinforcement Learning

and more generally and after n compositions of the models, denoted by T n and ˆT n, the following equality holds:

Lets further assume that the reward is linear:

s
∀

T n(s)

−

(cid:12)
(cid:12)
(cid:12)

= ∆

K i

n

1

−

i=0
(cid:88)

ˆT n(s)
(cid:12)
(cid:12)
(cid:12)

R(s) = KRs

Consider the state s = 0. Note that clearly v(0) = 0. We now compute the value predicted using ˆT , denoted by ˆv(0):

ˆv(0) = R(0) + γR(0 + ∆

K i) + γ2R(0 + ∆

K i) + γ3R(0 + ∆

K i) + ...

0

i=0
(cid:88)

2

i=0
(cid:88)

1

i=0
(cid:88)

2

i=0
(cid:88)

= 0 + γKR∆

K i + γ2KR∆

K i) + γ3KR∆

K i + ...

0

1

= γKR∆

K i =

i=0
(cid:88)
n
γn

1

−

∞

n=0
(cid:88)

i=0
(cid:88)

i=0
(cid:88)
γKR∆
γ)(1

−

(1

−

,

γ ¯K)

and so:

v(0)
|

ˆv(0)
|

−

=

(1

γKR∆
γ)(1

−

−

γ ¯K)

Note that this exactly matches the bound derived in our Theorem 2.

Lemma 1. A generalized transition function

induced by a Lipschitz model class Fg is Lipschitz with a constant:

T
G

Proof.

W

T (

µ1, a),

T (

µ2, a)

· |

(cid:0)

(cid:98)

· |

(cid:98)

(cid:1)

K AW,W (

T
G

(cid:98)
) := sup

sup
µ1,µ2

a

W

T
G

(
µ1, a),
(
T
·|
G
W (µ1, µ2)

·|

µ2, a)

(cid:0)

(cid:98)

(cid:98)

KF

≤

(cid:1)

(cid:98)

:= inf
j

= inf
j

= inf
j

j(s(cid:48)1, s(cid:48)2)d(s(cid:48)1, s(cid:48)2)ds(cid:48)1ds(cid:48)2

(cid:90)s(cid:48)

1 (cid:90)s(cid:48)

2

1

f (s1) = s(cid:48)1 ∧

(cid:90)s1 (cid:90)s2 (cid:90)s(cid:48)

1 (cid:90)s(cid:48)

2

f
(cid:88)
j(s1, s2, f )d

(cid:0)

(cid:90)s1 (cid:90)s2

f
(cid:88)

(cid:1)

f (s1), f (s2)

ds1ds2

(cid:0)

(cid:1)

KF inf
j

≤

g(f

a)j(s1, s2)d(s1, s2)ds1ds2
|

= KF

g(f

j(s1, s2)d(s1, s2)ds1ds2

(cid:90)s1 (cid:90)s2

f
(cid:88)
a) inf
j

(cid:90)s1 (cid:90)s2

|

|

f
(cid:88)

f
(cid:88)

= KF

g(f

a)W (µ1, µ2) = KF W (µ1, µ2)

Dividing by W (µ1, µ2) and taking sup over a, µ1, and µ2, we conclude:

K AW,W (

T ) = sup

sup
µ1,µ2

a

(cid:98)

W

T (

· |

µ1, a),
T (
· |
W (µ1, µ2)

µ2, a)

(cid:0)

(cid:98)

(cid:98)

KF .

≤

(cid:1)

We can also prove this using the Kantarovich-Rubinstein duality theorem:

f (s2) = s(cid:48)2

j(s1, s2, f )d(s(cid:48)1, s(cid:48)2)ds(cid:48)1ds(cid:48)2ds1ds2

Lipschitz Continuity in Model-based Reinforcement Learning

For every µ1, µ2, and a

we have:

∈ A

W

T
G

(
· |

µ1, a),

T
G

(
· |

µ2, a)

(cid:0)

(cid:98)

(cid:98)

(cid:1)

=

=

=

=

=

=

≤

≤

sup

sup

sup

sup

f :KdS ,R(f )

1 (cid:90)s

≤

(cid:0)

(cid:98)
1 (cid:90)s (cid:90)s0 (cid:16)

f :KdS ,R(f )

≤

T
G

(s
|

µ1, a)

T
G

µ2, a)
(s
|

−

f (s)ds

(cid:98)
s0, a)µ1(s0)
T (s
|

−

(cid:1)
s0, a)µ2(s0)
T (s
|

f (s)dsds0

(cid:98)
s0, a)
T (s
|

µ1(s0)

−

(cid:98)
µ2(s0)

(cid:17)
f (s)dsds0

f :KdS ,R(f )

1 (cid:90)s (cid:90)s0

≤

f :KdS ,R(f )

1 (cid:90)s (cid:90)s0

≤

sup

f :KdS ,R(f )

sup

f :KdS ,R(f )

1

≤

t
(cid:88)

1

≤

t
(cid:88)

g(t

a)

|

(cid:98)

|

|

t
(cid:88)
a)

g(t

g(t

a)

(cid:16)
a)1

g(t

|

(cid:0)
1

(cid:90)s0 (cid:90)s

(cid:90)s0

(cid:0)

−

−

(cid:17)
µ1(s0)

(cid:1)(cid:16)

−

−

t(s0) = s

µ1(s0)

µ2(s0)

(cid:0)
µ1(s0)

(cid:1)(cid:0)
µ2(s0)

f

t(s0)

ds0

(cid:1)

sup

µ1(s0)

(cid:1)
µ2(s0)

f

(cid:0)

(cid:1)

t(s0)

ds0

t
(cid:88)
composition of f, t is Lipschitz with constant upper bounded by KF .

(cid:1)

(cid:1)

(cid:0)

(cid:0)

f :KdS ,R(f )

1 (cid:90)s0
≤

t(s0) = s

µ2(s0)

f (s)dsds0

(cid:17)
f (s)dsds0

= KF

g(t

a)

sup

µ1(s0)

µ2(s0)

f :KdS ,R(f )

1 (cid:90)s0
≤

KF

g(t

a)

sup

= KF

g(t

h:KdS ,R(h)

1 (cid:90)s0
≤
a)W (µ1, µ2) = KF W (µ1, µ2)

(cid:0)

(cid:1)

f (t(s0))
KF

ds0

(cid:0)
µ1(s0)

(cid:1)
h(s0))ds0

µ2(s0)

−

−

|

|

|

t
(cid:88)

t
(cid:88)

t
(cid:88)

Again we conclude by dividing by W (µ1, µ2) and taking sup over a, µ1, and µ2.

Lemma 2. (Composition Lemma) Deﬁne three metric spaces (M1, d1), (M2, d2), and (M3, d3). Deﬁne Lipschitz functions
M3 is Lipschitz with
f : M2 (cid:55)→
constant Kd1,d3 (h)

M2 with constants Kd2,d3 (f ) and Kd1,d2 (g). Then, h : f

Kd2,d3 (f )Kd1,d2(g).

g : M1 (cid:55)→

◦

M3 and g : M1 (cid:55)→
≤

Proof.

Kd1,d3 (h) = sup
s1,s2

d3

f

g(s1)

, f

g(s2)

(cid:16)

(cid:0)

d1(s1, s2)
(cid:0)
(cid:1)

(cid:1)(cid:17)

= sup
s1,s2

d2

g(s1), g(s2)
d1(s1, s2)
(cid:0)

d2

(cid:1)
g(s1), g(s2)
d1(s1, s2)
≤
(cid:0)
(cid:1)
= Kd1,d2(g)Kd2,d3 (f ).

sup
s1,s2

d3

f

g(s1)

, f

g(s2)

(cid:1)(cid:17)

(cid:16)

sup
s1,s2

d2
(cid:0)
d3
(cid:0)

g(s1), g(s2)
(cid:1)
(cid:0)
f (s1), f (s2)
(cid:1)
d2(s1, s2)

(cid:0)

(cid:1)

Lemma 3. Given a Lipschitz function f :

R with constant KdS ,dR(f ):

S (cid:55)→

K AdS ,dR

T (s(cid:48)

s, a)f (s(cid:48))ds(cid:48)

KdS ,dR (f )K AdS ,W

T

.

|

(cid:16) (cid:90)

(cid:98)

≤

(cid:17)

(cid:0)

(cid:1)

(cid:98)

Proof.

K AdS ,dR

T (s(cid:48)

s, a)f (s(cid:48))ds(cid:48)

= sup

|

(cid:16) (cid:90)s(cid:48)

(cid:98)

Lipschitz Continuity in Model-based Reinforcement Learning

(cid:17)

sup
s1,s2

(cid:82)

(cid:0)

s(cid:48)

s(cid:48)

|

|

T (s(cid:48)

s1, a)
|

(cid:98)
T (s(cid:48)

s1, a)
|

a

a

= sup

sup
s1,s2

(cid:82)

(cid:0)

(cid:98)

= KdS ,dR(f ) sup
a

sup
s1,s2

s2, a)
T (s(cid:48)
−
|
d(s1, s2)
(cid:98)
T (s(cid:48)

s2, a)
|
d(s1, s2)
(cid:98)
T (s(cid:48)

s1, a)

−

(cid:1)

(cid:1)

f (s(cid:48))ds(cid:48)

|

f (s(cid:48))

KdS ,dR (f )
KdS ,dR (f ) ds(cid:48)

|

s(cid:48)

|

(cid:0)

(cid:82)
(cid:98)
supg:KdS ,dR (g)
≤

1

|

|

supg:KdS ,dR (g)
≤

1

s2, a)
|

T (s(cid:48)

−
d(s1, s2)

(cid:98)
s(cid:48)

(cid:82)

T (s(cid:48)

(cid:1)
s1, a)
|
d(s1, s2)
(cid:0)
(cid:98)
T (s(cid:48)

s(cid:48)

s1, a)
|
d(s1, s2)
(cid:0)
(cid:98)
s2, a)

(cid:82)

·|

(cid:1)

f (s(cid:48))
KdS ,dR (f ) ds(cid:48)

|

T (s(cid:48)

s2, a)
|

−

g(s(cid:48))ds(cid:48)

|

(cid:1)
g(s(cid:48))ds(cid:48)

(cid:98)
T (s(cid:48)

−

s2, a)
|

(cid:98)

(cid:1)

KdS ,dR(f ) sup
a

sup
s1,s2

≤

= KdS ,dR(f ) sup
a

sup
s1,s2

= KdS ,dR(f ) sup
a

sup
s1,s2

= KdS ,dR(f )K AdS ,W (

T ) .

W

T (

·|

s1, a),
T (
d(s1, s2)
(cid:98)

(cid:0)

(cid:98)

(cid:98)

Lemma 4. The following operators (Asadi & Littman, 2017) are Lipschitz with constants:

(cid:107)(cid:107)∞,dR

mean(x)

= K

(cid:107)(cid:107)∞,dR ((cid:15)-greedy(x)) = 1

1. K

2. K

(cid:107)(cid:107)∞,dR (max(x)) = K
(cid:107)(cid:107)∞,dR (mmβ(x) := log

(cid:80)

(cid:0)
i eβxi
n

β

) = 1

(cid:1)

3. K

(cid:107)(cid:107)∞,dR (boltzβ(x) :=

(cid:80)n

i=1 xieβxi
(cid:80)n
i=1eβ xi

)

A
|

|

A
+ βVmax|

|

≤

(cid:112)

and observe that boltzβ(x) = x(cid:62)ρ(x). Gao & Pavel (2017) showed that ρ is Lipschitz:

(cid:80)

ρ(x)i =

eβxi
n
i=1 eβxi

,

ρ(x2)

ρ(x1)
(cid:107)

x2(cid:107)2
Using their result, we can further show:

x1 −
(cid:107)

(cid:107)2 ≤

−

β

Proof. 1 was proven by Littman & Szepesv´ari (1996), and 2 is proven several times (Fox et al., 2016; Asadi & Littman,
2017; Nachum et al., 2017; Neu et al., 2017). We focus on proving 3. Deﬁne

(7)

ρ(x1)(cid:62)x2 −

|

ρ(x2)(cid:62)x2|

(Cauchy-Shwartz)

ρ(x2)(cid:62)x2|

|

ρ(x1)(cid:62)x1 −
≤ |

+

ρ(x1)(cid:62)x1 −
ρ(x1)
(cid:107)2 (cid:107)
ρ(x1)
x2(cid:107)2 (cid:107)
(cid:107)
ρ(x1)
x2(cid:107)2 β
(cid:107)
(1 + βVmax

x1 −
−
x1 −
(cid:107)2 (cid:107)
x1 −
(cid:107)

ρ(x1)(cid:62)x2|
x2(cid:107)2
ρ(x2)
x2(cid:107)2
x2(cid:107)2
x1 −
)
A
(cid:0)
|
(cid:107)
|
x1 −
)
A
+ βVmax|
(cid:112)
(cid:107)
|

A
|

(cid:107)2

(

≤ (cid:107)
+

≤ (cid:107)
+

≤

≤
|
(cid:112)
leads to 3.

from Eqn 7)
x2(cid:107)2
x2(cid:107)∞

,

(cid:1)

dividing both sides by

x1 −
(cid:107)

x2(cid:107)∞

Lipschitz Continuity in Model-based Reinforcement Learning

Below, we derive the Lipschitz constant for various functions mentioned in Table 1.
Rn has Lipschitz constant 1 for p.
ReLu non-linearity We show that ReLu : Rn

→

K
(cid:107)

.
(cid:107)

p,

.
(cid:107)

(cid:107)

p (ReLu) = sup
x1,x2

= sup
x1,x2
(cid:107)
(We can show that

1
p

p)
|

p

p

p

(

i |

(cid:80)

ReLu(x1)
(cid:107)

ReLu(x2)
−
(cid:107)
x2(cid:107)
x1 −
(cid:107)
ReLu(x1)i
ReLu(x2)i
−
x2(cid:107)
x1 −
ReLu(x1)i
|
x1,i
x2,i
i |
−
x1 −
x2(cid:107)
(cid:80)
(cid:107)
x2(cid:107)
x1 −
= 1
(cid:107)
x2(cid:107)
x1 −
(cid:107)

p)
|

−

(

1
p

p

p

p

sup
x1,x2

≤

= sup
x1,x2

ReLu(x2)i

x1,i

x2,i

and so) :

| ≤ |

−

|

Matrix multiplication Let W

Rn

×

m. We derive the Lipschitz continuity for the function

W (x) = W x.

×

For p =

we have:

∞

∈

where Wj refers to jth row of the weight matrix W . Similarly, for p = 1 we have:

K

(cid:107)(cid:107)∞,

= sup
x1,x2

= sup
x1,x2

sup
≤
x1,x2
= sup

j (cid:107)

W (x1)

(cid:107)(cid:107)∞

×
W (x1)
(cid:0)
(cid:107)×

W (x2)
(cid:1)
− ×
x2(cid:107)∞
x2)

|

x1 −
(cid:107)
Wj(x1 −
supj |
x2(cid:107)∞
x1 −
(cid:107)
x1 −
Wj
supj (cid:107)
(cid:107) (cid:107)
x2(cid:107)∞
x1 −
(cid:107)
(cid:107)1 ,

Wj

x2(cid:107)∞

W (x2)
x2(cid:107)1
x2)
|

K

(cid:107)(cid:107)1,

(cid:107)(cid:107)1

= sup
x1,x2

(cid:0)
(cid:107)×

= sup

x1,x2 (cid:80)

≤

sup
x1,x2 (cid:80)

W (x1)

×
W (x1)

(cid:1)
− ×
x1 −
(cid:107)
Wj(x1 −
j |
x2(cid:107)1
x1 −
(cid:107)
Wj
j (cid:107)
(cid:107)∞ (cid:107)
x1 −
(cid:107)

K

(cid:107)(cid:107)2,

(cid:107)(cid:107)2

= sup
x1,x2

(cid:0)
(cid:107)×

W (x1)

×
W (x1)

2
|

x2)

W (x2)
(cid:1)
− ×
x2(cid:107)2
x1 −
(cid:107)
Wj(x1 −
j |
x2(cid:107)2
x1 −
(cid:107)
2
Wj
x1 −
j (cid:107)
2 (cid:107)
(cid:107)
x2(cid:107)2
x1 −
(cid:107)

= sup

x1,x2 (cid:113)(cid:80)

sup
x1,x2 (cid:113)(cid:80)

≤

(cid:107)∞

= sup
x1,x2

W x1 −
(cid:107)
x1 −
(cid:107)

W x2(cid:107)∞
x2(cid:107)∞

= sup
x1,x2

W (x1 −
(cid:107)
x1 −
(cid:107)

x2)
x2(cid:107)∞

(cid:107)∞

(H¨older’s inequality)

(cid:107)1

= sup
x1,x2

(cid:107)

W x1 −
x1 −
(cid:107)

W x2(cid:107)1
x2(cid:107)1

= sup
x1,x2

(cid:107)

W (x1 −
x1 −
(cid:107)

x2)
x2(cid:107)1

(cid:107)1

(cid:107)2

= sup
x1,x2

(cid:107)

W x1 −
x1 −
(cid:107)

W x2(cid:107)2
x2(cid:107)2

= sup
x1,x2

(cid:107)

W (x1 −
x1 −
(cid:107)

x2)
x2(cid:107)2

(cid:107)2

2
2

x2(cid:107)

=

Wj
(cid:107)

2
2
(cid:107)

.

j

(cid:115)(cid:88)

and ﬁnally for p = 2:

x2(cid:107)1

=

x1 −
x2(cid:107)1

j
(cid:88)

Wj
(cid:107)

(cid:107)∞

,

Lipschitz Continuity in Model-based Reinforcement Learning

Vector addition We show that +b : Rn

Rn has Lipschitz constant 1 for p = 0, 1,

for all b

Rn.

→

∞

∈

K
(cid:107)

.
(cid:107)

p,

.
(cid:107)

(cid:107)

p (ReLu) = sup
x1,x2

(cid:107)

+ b(x1)
−
x1 −
(cid:107)
(x1 + b)
−
(cid:107)
x1 −
(cid:107)

+b(x2)
p
(cid:107)
x2(cid:107)
p
(x2 + b)
p
(cid:107)
x2(cid:107)

p

= sup
x1,x2

x1 −
= (cid:107)
x1 −
(cid:107)

x2(cid:107)
x2(cid:107)

p

p

= 1

Supervised-learning domain We used the following 5 functions to generate the dataset:

f0(x) = tanh(x) + 3
x
f1(x) = x
f2(x) = sin(x)
f3(x) = sin(x)
f4(x) = sin(x)

−

−

∗

5

3
sin(x)

∗

We sampled each function 30 times, where the input was chosen uniformly randomly from [

2, 2] each time.

−

Lipschitz Continuity in Model-based Reinforcement Learning

Kavosh Asadi * 1 Dipendra Misra * 2 Michael L. Littman 1

8
1
0
2
 
l
u
J
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
3
9
1
7
0
.
4
0
8
1
:
v
i
X
r
a

Abstract
We examine the impact of learning Lipschitz
continuous models in the context of model-based
reinforcement learning. We provide a novel bound
on multi-step prediction error of Lipschitz models
where we quantify the error using the Wasserstein
metric. We go on to prove an error bound for
the value-function estimate arising from Lipschitz
models and show that the estimated value function
is itself Lipschitz. We conclude with empirical
results that show the beneﬁts of controlling the
Lipschitz constant of neural-network models.

1. Introduction

The model-based approach to reinforcement learning (RL)
focuses on predicting the dynamics of the environment
to plan and make high-quality decisions (Kaelbling et al.,
1996; Sutton & Barto, 1998). Although the behavior of
model-based algorithms in tabular environments is well
understood and can be effective (Sutton & Barto, 1998),
scaling up to the approximate setting can cause instabilities.
Even small model errors can be magniﬁed by the planning
process resulting in poor performance (Talvitie, 2014).

In this paper, we study model-based RL through the lens of
Lipschitz continuity, intuitively related to the smoothness
of a function. We show that the ability of a model to make
accurate multi-step predictions is related to the model’s
one-step accuracy, but also to the magnitude of the Lipschitz
constant (smoothness) of the model. We further show that
the dependence on the Lipschitz constant carries over to the
value-prediction problem, ultimately inﬂuencing the quality
of the policy found by planning.

We consider a setting with continuous state spaces and
stochastic transitions where we quantify the distance
between distributions using the Wasserstein metric. We

*Equal contribution

1Department of Computer Science,
Brown University, Providence, USA 2Department of Computer
Science and Cornell Tech, Cornell University, New York, USA.
Correspondence to: Kavosh Asadi <kavosh@brown.edu>.

introduce a novel characterization of models, referred
to as a Lipschitz model class, that represents stochastic
dynamics using a set of component deterministic functions.
This allows us to study any stochastic dynamic using
the Lipschitz continuity of its component deterministic
functions. To learn a Lipschitz model class in continuous
state spaces, we provide an Expectation-Maximization
algorithm (Dempster et al., 1977).

the learned models or

One promising direction for mitigating the effects of
inaccurate models is the idea of limiting the complexity
of
reducing the horizon of
planning (Jiang et al., 2015). Doing so can sometimes
make models more useful, much as regularization in
supervised learning can improve generalization performance
(Tibshirani, 1996). In this work, we also examine a type
of regularization that comes from controlling the Lipschitz
constant of models. This regularization technique can be
applied efﬁciently, as we will show, when we represent the
transition model by neural networks.

2. Background

,

A

, R, T, γ

We consider the Markov decision process (MDP) setting
in which the RL problem is formulated by the tuple
we mean a continuous state
. Here, by
(cid:105)
(cid:104)S
we mean a discrete action set. The functions
space and by
A
R and T :
) denote the reward
Pr(
R : S
S
S × A →
and transition dynamics. Finally, γ
[0, 1) is the discount
∈
rate. If
= 1, the setting is called a Markov reward
process (MRP).

|A|

→

×

A

S

2.1. Lipschitz Continuity

Our analyses leverage the “smoothness” of various
functions, quantiﬁed as follows.

Deﬁnition 1. Given two metric spaces (M1, d1) and
(M2, d2) consisting of a space and a distance metric, a
M2 is Lipschitz continuous (sometimes
function f : M1 (cid:55)→
simply Lipschitz) if the Lipschitz constant, deﬁned as

Kd1,d2(f ) :=

sup
M1,s2

s1

∈

d2

f (s1), f (s2)
d1(s1, s2)

,

(cid:1)

M1

∈

(cid:0)

(1)

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

is ﬁnite.

Lipschitz Continuity in Model-based Reinforcement Learning

Sometimes referred to as “Earth Mover’s distance”,
Wasserstein is the minimum expected distance between
pairs of points where the joint distribution j is constrained
to match the marginals µ1 and µ2. New applications of
this metric are discovered in machine learning, namely in
the context of generative adversarial networks (Arjovsky
et al., 2017) and value distributions in reinforcement
learning (Bellemare et al., 2017).

Wasserstein is linked to Lipschitz continuity using duality:

W (µ1, µ2) =

sup

f (s)µ1(s)

f (s)µ2(s)

ds .

f :Kd,dR (f )

1 (cid:90)

≤

(cid:0)

−

(cid:1)

(4)

This equivalence, known as Kantorovich-Rubinstein duality
(Villani, 2008), lets us compute Wasserstein by maximizing
R, a relatively
over a Lipschitz set of functions f :
In our theory, we utilize both
easier problem to solve.
deﬁnitions, namely the primal deﬁnition (3) and the dual
deﬁnition (4).

S (cid:55)→

3. Lipschitz Model Class

We introduce a novel representation of stochastic MDP
transitions in terms of a distribution over a set of
deterministic components.
Deﬁnition 4. Given a metric state space (
action space
Fg =
a

) and an
, we deﬁne Fg as a collection of functions:
a) where

f :
{
. We say that Fg is a Lipschitz model class if
∈ A

distributed according to g(f

A
S (cid:55)→ S}

, d
S

S

|

KF := sup
Fg

f

∈

KdS ,dS (f ) ,

Our deﬁnition captures a subset of stochastic transitions,
namely ones that can be represented as a state-independent
distribution over deterministic transitions. An example is
provided in Figure 2. We further prove in the appendix (see
Claim 1) that any ﬁnite MDP transition probabilities can be
decomposed into a state-independent distribution g over a
ﬁnite set of deterministic functions f .

Associated with a Lipschitz model class is a transition
function given by:

T (s(cid:48)

s, a) =

|

(cid:98)

f
(cid:88)

(cid:0)

1

f (s) = s(cid:48)

g(f

a) .

|

(cid:1)

Given a state distribution µ(s), we also deﬁne a generalized
notion of transition function
T
G

µ, a) given by:

(
· |

Figure 1. An illustration of Lipschitz continuity.
Pictorially,
Lipschitz continuity ensures that f lies in between the two afﬁne
functions (colored in blue) with slopes K and −K.

Equivalently, for a Lipschitz f ,

s1,

s2

d2

f (s1), f (s2)

Kd1,d2(f ) d1(s1, s2) .

∀

∀

≤

(cid:1)
The concept of Lipschitz continuity is visualized in Figure 1.

(cid:0)

A Lipschitz function f is called a non-expansion when
Kd1,d2(f ) = 1 and a contraction when Kd1,d2(f ) < 1.
Lipschitz continuity, in one form or another, has been a
key tool in the theory of reinforcement learning (Bertsekas,
1975; Bertsekas & Tsitsiklis, 1995; Littman & Szepesv´ari,
1996; M¨uller, 1996; Ferns et al., 2004; Hinderer, 2005;
Rachelson & Lagoudakis, 2010; Szepesv´ari, 2010; Pazis
& Parr, 2013; Pirotta et al., 2015; Pires & Szepesv´ari,
2016; Berkenkamp et al., 2017; Bellemare et al., 2017) and
bandits (Kleinberg et al., 2008; Bubeck et al., 2011). Below,
we also deﬁne Lipschitz continuity over a subset of inputs.
Deﬁnition 2. A function f : M1 × A (cid:55)→
Lipschitz continuous in

M2 is uniformly

if

A

is ﬁnite.

K Ad1,d2(f ) := sup
∈A

a

sup
s1,s2

d2

f (s1, a), f (s2, a)
d1(s1, s2)

(cid:0)

(cid:1)

,

(2)

is ﬁnite.

Note that the metric d1 is deﬁned only on M1.

2.2. Wasserstein Metric

We quantify the distance between two distributions using
the following metric:

Deﬁnition 3. Given a metric space (M, d) and the set
P(M ) of all probability measures on M , the Wasserstein
metric (or the 1st Kantorovic metric) between two
probability distributions µ1 and µ2 in P(M ) is deﬁned as

W (µ1, µ2) := inf
Λ
∈

j

(cid:90) (cid:90)

j(s1, s2)d(s1, s2)ds2 ds1 , (3)

where Λ denotes the collection of all joint distributions j on
M with marginals µ1 and µ2 (Vaserstein, 1969).
M

(cid:98)

×

(s(cid:48)

T
G

|

µ, a) =

1

f (s) = s(cid:48)
(cid:98)

g(f

a)

µ(s)ds .

(cid:90)s

f
(cid:88)

(cid:0)

(cid:124)

(cid:98)T (s(cid:48)

(cid:1)
s,a)
|
(cid:123)(cid:122)

|

(cid:125)

Lipschitz Continuity in Model-based Reinforcement Learning

Figure 2. An example of a Lipschitz model class in a gridworld
environment (Russell & Norvig, 1995). The dynamics are such
that any action choice results in an attempted transition in the
corresponding direction with probability 0.8 and in the neighboring
directions with probabilities 0.1 and 0.1. We can deﬁne Fg =
{f up, f right, f down, f left} where each f outputs a deterministic
next position in the grid (factoring in obstacles). For a = up,
we have: g(f up | a = up) = 0.8, g(f right
| a = up) =
g(f left | a = up) = 0.1, and g(f down | a = up) = 0. Deﬁning
distances between states as their Manhattan distance in the grid,
(cid:0)d(f (s1), f (s2)(cid:1)/d(s1, s2) = 2, and so KF =
then ∀f sups1,s2
2. So, the four functions and g comprise a Lipschitz model class.

T
G

. However, since

), the Lipschitz
We are primarily interested in K Ad,d(
T
G
constant of
takes as input a
T
G
(cid:98)
probability distribution and also outputs a probability
distribution, we require a notion of distance between two
distributions. This notion is quantiﬁed using Wasserstein
and is justiﬁed in the next section.

(cid:98)

(cid:98)

4. On the Choice of Probability Metric

We consider the stochastic model-based setting and show
through an example that
the Wasserstein metric is a
reasonable choice compared to other common options.

Consider a uniform distribution over states µ(s) as shown
in black in Figure 3 (top). Take a transition function T
in
G
the environment that, given an action a, uniformly randomly
adds or subtracts a scalar c1. The distribution of states
after one transition is shown in red in Figure 3 (middle).
Now, consider a transition model
that approximates T
G
by uniformly randomly adding or subtracting the scalar
c2. The distribution over states after one transition using
this imperfect model is shown in blue in Figure 3 (bottom).
We desire a metric that captures the similarity between the
output of the two transition functions. We ﬁrst consider
Kullback-Leibler (KL) divergence and observe that:

T
G

(cid:98)

KL

T
G

:=

(cid:0)

(cid:90)

(
· |
(s(cid:48)

T
G

µ, a),

(

T
G

· |
T
µ, a) log
G
(cid:98)
T
G

µ, a)
(s(cid:48)
(cid:1)
|
(s(cid:48) |

|

µ, a)

µ, a)

ds(cid:48) =

,

∞

unless the two constants are exactly the same.

(cid:98)

Figure 3. A state distribution µ(s) (top), a stochastic environment
that randomly adds or subtracts c1 (middle), and an approximate
transition model that randomly adds or subtracts a second scalar
c2 (bottom).

The next possible choice is Total Variation (TV) deﬁned as:

µ, a),

µ, a)

T
G

T V

:=

1
(cid:0)
2

(
· |
T
G

(s(cid:48)

|

(
· |

T
G
µ, a)
(cid:98)

(cid:1)
(s(cid:48)

T
G

−

|

µ, a)

ds(cid:48) = 1 ,

if the two distributions have disjoint supports regardless of
how far the supports are from each other.

(cid:98)

(cid:12)
(cid:12)

(cid:90)

(cid:12)
(cid:12)

In contrast, Wasserstein is sensitive to how far the constants
are as:

µ, a)

=

c1 −
|

c2|

.

W

T
G

(
· |

µ, a),

(

T
G

(cid:0)

· |
It is clear that, of the three, Wasserstein corresponds best
to the intuitive sense of how closely T
approximates
G
. This is particularly important in high-dimensional
T
G
spaces where the true distribution is known to usually lie in
low-dimensional manifolds. (Narayanan & Mitter, 2010)
(cid:98)

(cid:98)

(cid:1)

5. Understanding the Compounding Error

Phenomenon

−

To extract a prediction with a horizon n > 1, model-based
algorithms typically apply the model for n steps by taking
the state input in step t to be the state output from
the step t
1. Previous work has shown that model
error can result in poor long-horizon predictions and
ineffective planning (Talvitie, 2014; 2017). Observed even
beyond reinforcement learning (Lorenz, 1972; Venkatraman
et al., 2015), this is referred to as the compounding error
phenomenon. The goal of this section is to provide a bound
on multi-step prediction error of a model. We formalize the
notion of model accuracy below:

Deﬁnition 5. Given an MDP with a transition function
T , we identify a Lipschitz model Fg as ∆-accurate if its
induced

T satisﬁes:

s
∀

a W
(cid:98)
∀

· |

T (

s, a), T (

s, a)

∆ .

· |

≤

(cid:1)

(cid:0)

(cid:98)

Lipschitz Continuity in Model-based Reinforcement Learning

We want to express the multi-step Wasserstein error in
terms of the single-step Wasserstein error and the Lipschitz
constant of the transition function
. We provide a bound
using the following lemma:
on the Lipschitz constant of

T
G

Lemma 1. A generalized transition function
(cid:98)
a Lipschitz model class Fg is Lipschitz with a constant:

T
G

induced by

T
G

(cid:98)

(cid:98)

K AW,W (

T
G

) := sup

sup
µ1,µ2

a

(cid:98)

W

(

T
G

(
µ1, a),
T
·|
·|
G
W (µ1, µ2)

µ2, a)

KF

≤

(cid:1)

(cid:0)

(cid:98)

(cid:98)

the two input
Intuitively, Lemma 1 states that,
distributions are similar, then for any action the output
distributions given by
are also similar up to a KF factor.
We prove this lemma, as well as the subsequent lemmas, in
the appendix.

T
G

if

(cid:98)

Given the one-step error (Deﬁnition 5), a start state
distribution µ and a ﬁxed sequence of actions a0, ..., an
1,
we desire a bound on n-step error:

−

δ(n) := W

T n
G

(
· |

µ), T n
G

(

· |

µ)

,

where

T n
G

(
·|

µ) :=

(cid:0)
(cid:98)
T
G

T
G

(
·|

(cid:1)

...

(
·|

µ, a0)..., an

(
·|

T
G
n recursive calls

−

2), an

1)

−

(cid:98)
(cid:98)
and T n
(
· |
G
lemma followed by the theorem.

µ) is deﬁned similarly. We provide a useful
(cid:125)

(cid:98)
(cid:124)

(cid:123)(cid:122)

(cid:98)

Lemma 2. (Composition Lemma) Deﬁne three metric
spaces (M1, d1), (M2, d2), and (M3, d3). Deﬁne Lipschitz
M2 with constants
M3 and g : M1 (cid:55)→
functions f : M2 (cid:55)→
Kd2,d3(f ) and Kd1,d2 (g). Then, h : f
M3 is
g : M1 (cid:55)→
Kd2,d3 (f )Kd1,d2(g).
Lipschitz with constant Kd1,d3 (h)

◦

≤

Similar to composition, we can show that summation
preserves Lipschitz continuity with a constant bounded by
the sum of the Lipschitz constants of the two functions. We
omitted this result due to brevity.

Theorem 1. Deﬁne a ∆-accurate
with the Lipschitz
constant KF and an MDP with a Lipschitz transition
with constant KT . Let ¯K = min
function T
.
KF , KT
}
{
G
1:
Then
≥

T
G

n

∀

(cid:98)

δ(n) := W

T n
G

(
· |

µ), T n
G

(

· |

µ)

∆

≤

( ¯K)i .

n

1

−

i=0
(cid:88)

(cid:1)

(cid:0)

(cid:98)

(cid:1)
s, a0)

f (s(cid:48))µ(s) ds ds(cid:48)

(cid:1)
f (s(cid:48)) ds(cid:48)

µ(s) ds

δ(1) := W

:= sup

µ, a0)

(

T
· |
G
T (s(cid:48)
(cid:98)

|

(

µ, a0), T
· |
G
T (s(cid:48)

s, a0)

−

|

(cid:0)
f (cid:90) (cid:90)
sup
f (cid:90)

(cid:0)
=W

(cid:98)

(cid:0)
(cid:98)
T (s(cid:48)

s, a0)
|

−

T (s(cid:48)

s, a0)
|

(cid:1)
due to duality (4)

(cid:1)
s, a0)

µ(s) ds

(cid:125)

(cid:124)
W

(cid:98)T (

s,a0),T (

·|

s,a0)
·|
(cid:123)(cid:122)

s, a0), T (

(cid:0)
T (
· |
∆ due to Deﬁnition 5

· |

(cid:0)

≤

(cid:98)

(cid:124)
∆ µ(s) ds = ∆ .

(cid:123)(cid:122)

(cid:1)

(cid:125)

≤

(cid:90)

=

(cid:90)

≤

(cid:90)

1

−

We now prove the inductive step. Assuming δ(n
T n
W
(
G
write:
(cid:0)
(cid:98)
δ(n) := W

µ), T n
G

(
· |

(cid:1)
µ)

(cid:80)

µ)

· |

∆

≤

−

(

1

2

1) :=
i=0 (KF )i we can

−

−

n

T n
(
· |
G
T
µ),
G

µ), T n
· |
G
T n
−
· |
G
µ), an

−

1
(
(cid:98)

(cid:0)
· |

1

−

(
· |

1

(

(cid:1)
· |

µ), an

1

−

1

−

, T n
G

(
· |

(cid:1)
1),

T
G

· |

W

≤
+W

T n
G

(cid:16)
T
(cid:98)
G

= W

+W

(cid:16)

T
(cid:98)
G

(cid:0)
(
· |

(cid:16)
T
(cid:98)
G
(cid:0)

· |

· |

(cid:0)
(
(cid:98)
· |
T n
G
T n
G
T n
(cid:98)
G

µ), an

−

1

1

−

(
· |

µ), an

, T
(cid:98)
G

(cid:0)
(
· |

(cid:17)
T n
−
G
T n
G

−

1

1

(

· |

(

· |

µ), an

1

−

µ), an

1)

(cid:16)

−
We now use Lemma 1 and Deﬁnition 5 to upper bound the
ﬁrst and the second term of the last line respectively.

(cid:98)

(cid:1)

−

(cid:1)(cid:17)

(cid:17)

(cid:1)(cid:17)

µ)

(Triangle ineq)

δ(n)

KF W

≤

(cid:0)
= KF δ(n

1

−

T n
G

(
· |

1

µ), T n
G
n

−

(cid:98)
−

1) + ∆

∆

≤

+ ∆

µ)

(
· |
1
(KF )i .

(cid:1)

−

(5)

i=0
(cid:88)

in the triangle inequality, we may replace
T n
1
and follow
G

(
· |

µ)

µ)

· |

Note that
T n
with T
(
T
· |
G
G
G
the same basic steps to get:
(cid:98)

· |

(cid:98)

(cid:0)

(cid:0)

(cid:1)

(cid:1)

−

−

1

W

(

T n
G

· |

µ), T n
G

(
· |

µ)

(cid:0)

(cid:98)

n

−

1
(KT )i .

∆

≤

(cid:1)

i=0
(cid:88)

Combining (5) and (6) allows us to write:

(6)

δ(n) = W

T n
G

(
· |
n

(cid:0)
min

∆

(
· |

µ), T n
G
1
(KT )i, ∆

µ)
n
(cid:1)
−

−

1
(KF )i

(cid:41)

i=0
(cid:88)

≤

= ∆

(cid:98)
(cid:40)
1

n

−

i=0
(cid:88)

i=0
(cid:88)
( ¯K)i ,

which concludes the proof.

Proof. We construct a proof by induction.
Using
Kantarovich-Rubinstein duality (Lipschitz property of f
not shown for brevity) we ﬁrst prove the base of induction:

There exist similar
results in the literature relating
one-step transition error to multi-step transition error and
sub-optimality bounds for planning with an approximate

Lipschitz Continuity in Model-based Reinforcement Learning

model. The Simulation Lemma (Kearns & Singh, 2002;
Strehl et al., 2009) is for discrete state MDPs and relates
error in the one-step model to the value obtained by
using it for planning. A related result for continuous
state-spaces (Kakade et al., 2003) bounds the error in
estimating the probability of a trajectory using total
variation. A second related result (Venkatraman et al.,
2015) provides a slightly looser bound for prediction error
in the deterministic case—our result can be thought of as a
generalization of their result to the probabilistic case.

6. Value Error with Lipschitz Models

,

A

(cid:104)S

, T, R, γ

We next investigate the error in the state-value function
induced by a Lipschitz model class. To answer this question,
we consider an MRP M1 denoted by
and
a second MRP M2 that only differs from the ﬁrst in its
be the
transition function
T , R, γ
action set with a single action a. We further assume that
the reward function is only dependent upon state. We ﬁrst
express the state-value function for a start state s with
respect to the two transition functions. By δs below, we
mean a Dirac delta function denoting a distribution with
probability 1 at state s.

. Let
(cid:105)

,
A

a
}

(cid:104)S

A

=

(cid:98)

{

(cid:105)

,

VT (s) :=

(s(cid:48)

δs)R(s(cid:48)) ds(cid:48) ,

∞

γn

n=0
(cid:88)

(cid:90)

T n
G

∞

γn

n=0
(cid:88)

(cid:90)

T n
G

(cid:98)

|

|

V (cid:98)T (s) :=

(s(cid:48)

δs)R(s(cid:48)) ds(cid:48) .

Let

=

h : KdS ,R(h)

F

{

1

. Then given f
}

≤

:

∈ F

KR

∞

γn

f (s(cid:48))

T n
G

(s(cid:48)

δs)

|

T n
G

(s(cid:48)

δs)
|

−

ds(cid:48)

n=0
(cid:88)

KR

(cid:90)
∞

n=0
(cid:88)

(cid:0)
γn sup
f

∈F (cid:90)

≤

= KR

∞

(cid:124)
γn W

f (s(cid:48))

δs)

(s(cid:48)

δs)

ds(cid:48)

(cid:98)
T n
(s(cid:48)
G

|

(cid:1)

−

T n
G

(cid:98)

|

(cid:1)

(cid:125)

:=W

δs), (cid:98)T n

due to duality (4)

(cid:0)
T n
G (.
|

δs)
G (.
|
(cid:123)(cid:122)
δs)

(cid:1)

(cid:0)
|

(.

(.

δs),

T n
G

T n
G
(cid:80)n−1
(cid:0)
(cid:1)
i=0 ∆( ¯K)i due to Theorem 1
(cid:98)
1
(cid:125)

(cid:123)(cid:122)

|

≤
n
(cid:124)

−

n=0
(cid:88)

n=0
(cid:88)

∞

KR

∞

γn

∆( ¯K)i

= KR∆

≤

=

i=0
(cid:88)
γn 1
−
1
−

¯K n
¯K

.

n=0
(cid:88)
γKR∆
γ)(1

−

−

(1

γ ¯K)
We can derive the same bound for V (cid:98)T (s)
VT (s) using
the fact that Wasserstein distance is a metric, and therefore
symmetric, thereby completing the proof.

−

Regarding the tightness of our bounds, we can show that
when the transition model is deterministic and linear then
Theorem 1 provides a tight bound. Moreover, if the reward
function is linear, the bound provided by Theorem 2 is tight.
(See Claim 2 in the appendix.) Notice also that our proof
does not require a bounded reward function.

7. Lipschitz Generalized Value Iteration

Next we derive a bound on

VT (s)

s.

∀

−

V (cid:98)T (s)
(cid:12)
(cid:12)

Theorem 2. Assume a Lipschitz model class Fg with a
(cid:12)
(cid:12)
T with ¯K = min
∆-accurate
. Further, assume
KF , KT
}
{
a Lipschitz reward function with constant KR = KdS ,R(R).
Then

and ¯K
(cid:98)

[0, 1
γ )

s
∀

∈ S

∈

We next show that, given a Lipschitz transition model,
solving for the ﬁxed point of a class of Bellman equations
yields a Lipschitz state-action value function. Our proof is in
the context of Generalized Value Iteration (GVI) (Littman &
Szepesv´ari, 1996), which deﬁnes Value Iteration (Bellman,
1957) for planning with arbitrary backup operators.

VT (s)

V (cid:98)T (s)

−

≤

(1

(cid:12)
(cid:12)

(cid:12)
(cid:12)

γKR∆
γ)(1

−

−

.

γ ¯K)

Proof. We ﬁrst deﬁne the function f (s) = R(s)
KR
observed that KdS ,R(f ) = 1. We now write:

. It can be

VT (s)

V (cid:98)T (s)

−
γn

∞

=

n=0
(cid:88)

(cid:90)

= KR

∞

γn

T n
G

(cid:0)
f (s(cid:48))

n=0
(cid:88)

(cid:90)

(cid:0)

R(s(cid:48))

(s(cid:48)

δs)

(s(cid:48)

δs)

ds(cid:48)

|

|

T n
G

−

(cid:98)
δs)

(cid:98)

(s(cid:48)

T n
G

|

(s(cid:48)

T n
G

|

−

(cid:1)
δs)

ds(cid:48)

(cid:1)

Algorithm 1 GVI algorithm

Input: initial
repeat

Q(s, a), δ, and choose an operator f

for each s, a
(cid:98)
∈ S × A
R(s, a)+γ
Q(s, a)

do

←

end for
(cid:98)

until convergence

|

(cid:82)

(cid:98)

T (s(cid:48)

s, a)f

Q(s(cid:48),

ds(cid:48)

)
·

(cid:1)

(cid:0)

(cid:98)

To prove the result, we make use of the following lemmas.
Lemma 3. Given a Lipschitz function f :
R with
constant KdS ,dR(f ):

S (cid:55)→

K AdS ,dR

T (s(cid:48)

s, a)f (s(cid:48))ds(cid:48)
|

KdS ,dR(f )K AdS ,W

T

.

≤

(cid:17)

(cid:16) (cid:90)

(cid:98)

(cid:0)

(cid:1)

(cid:98)

Lipschitz Continuity in Model-based Reinforcement Learning

Lemma 4. The following operators (Asadi & Littman,
2017) are Lipschitz with constants:

(cid:107)(cid:107)∞,dR

mean(x)

=

(cid:0)

(cid:1)

1. K
K

2. K

3. K

(cid:107)(cid:107)∞,dR (max(x)) = K
(cid:107)(cid:107)∞,dR ((cid:15)-greedy(x)) = 1
(cid:107)(cid:107)∞,dR (mmβ(x) := log
(cid:107)(cid:107)∞,dR (boltzβ(x)
:=
A
βVmax|
|

(cid:80)n

β

(cid:80)

i eβxi
n

) = 1

i=1 xieβxi
(cid:80)n
i=1eβ xi

)

≤

+

A
|

|

(cid:112)

Theorem 3. For any non-expansion backup operator f
outlined in Lemma 4, GVI computes a value function

with a Lipschitz constant bounded by
γK AdS ,W (T ) < 1.

(R)

KA
dS ,dR
γKdS ,W (T )

1

−

if

Proof. From Algorithm 1, in the nth round of GVI updates:

·

(cid:1)

)
·

Qn+1(s, a)

R(s, a) + γ

T (s(cid:48)

s, a)f

Qn(s(cid:48),

)

ds(cid:48).

←

|

(cid:90)

(cid:0)

(cid:98)

(cid:98)
Now observe that:

K AdS ,dR (

Qn+1)

≤

≤

K AdS ,dR(R)+γK AdS ,dR

(cid:98)

T (s(cid:48)

s, a)f

Qn(s(cid:48),

ds(cid:48)

K AdS ,dR (R) + γK AdS ,W (T ) KdS,R

(cid:90)

(cid:0)

(cid:1)

(cid:0)
Qn(s,
(cid:98)

·

f

(cid:16)

)
(cid:1)(cid:17)
(cid:0)
(cid:107)·(cid:107)∞,dR(f )K AdS ,dR (
(cid:98)
Qn)

(cid:1)

Qn)

K AdS ,dR (R) + γK AdS ,W (T )K

≤
= K AdS ,dR (R) + γK AdS ,W (T )K AdS ,dR (

(cid:98)
Where we used Lemmas 3, 2, and 4 for the second, third,
and fourth inequality respectively. Equivalently:

(cid:98)

|

n

K AdS ,dR (

Qn+1)

K AdS ,dR(R)

γK AdS ,W (T )

i

≤

+

(cid:98)

i=0
(cid:88)
(cid:0)
n
γK AdS ,W (T )

K AdS ,dR (

(cid:1)
Q0) .

By computing the limit of both sides, we get:

(cid:1)

(cid:0)

K AdS ,dR (

Qn+1)

K AdS ,dR (R)

γK AdS ,W (T )

lim
n
→∞

(cid:98)

(cid:98)

n

i=0
(cid:88)
(cid:0)
γK AdS ,W (T )

n

K AdS ,dR (

(cid:1)
Q0)

K AdS ,dR (R)

(cid:0)
γKdS ,W (T )

(cid:1)
+ 0 ,

i

(cid:98)

lim
n
→∞

≤

+ lim
n
→∞

=

1

−

This concludes the proof.

Two implications of this result: First, PAC exploration in
continuous state spaces is shown assuming a Lipschitz value
function (Pazis & Parr, 2013). However, the theorem shows

that it is sufﬁcient to have a Lipschitz model, an assumption
perhaps easier to conﬁrm. The second implication relates to
value-aware model learning (VAML) objective (Farahmand
et al., 2017). Using the above theorem, we can show that
minimizing Wasserstein is equivalent to minimizing the
VAML objective (Asadi et al., 2018).

8. Experiments

|S|

Our ﬁrst goal in this section1 is to compare TV, KL, and
Wasserstein in terms of the ability to best quantify error of
an imperfect model. To this end, we built ﬁnite MRPs with
random transitions,
= 10 states, and γ = 0.95. In the
ﬁrst case the reward signal is randomly sampled from [0, 10],
and in the second case the reward of an state is the index of
that state, so small Euclidean norm between two states is
an indication of similar values. For 105 trials, we generated
an MRP and a random model, and then computed model
error and planning error (Figure 4). We understand a good
metric as the one that computes a model error with a high
correlation with value error. We show these correlations for
different values of γ in Figure 5.

Figure 4. Value error (x axis) and model error (y axis). When
the reward is the index of the state (right), correlation between
Wasserstein error and value-prediction error is high.
This
highlights the fact that when closeness in the state-space is an
indication of similar values, Wasserstein can be a powerful metric
for model-based RL. Note that Wasserstein provides no advantage
given random rewards (left).

Figure 5. Correlation between value-prediction error and model
error for the three metrics using random rewards (left) and index
rewards (right). Given a useful notion of state similarities, low
Wasserstein error is a better indication of planning error.

1We release the code here: github.com/kavosh8/Lip

Lipschitz Continuity in Model-based Reinforcement Learning

Function f

Deﬁnition

ReLu : Rn

+b : Rn

Rn,

W : Rn

×

→
Rm,

→

Rn
→
b
∀
W

∈

∀

∈

Rn
Rm

n

×

ReLu(x)i := max
{
+b(x) := x + b

0, xi

}

W (x) := W x

×

Lipschitz constant K

p,

(cid:107)(cid:107)

(cid:107)(cid:107)

p = 1
1
1

p = 2
1
1

Wj

j (cid:107)

(cid:107)∞

Wj

2
2

(cid:107)

j (cid:107)

(cid:80)

(cid:113)(cid:80)

p (f )
p =
1
1
supj (cid:107)

∞

Wj

(cid:107)1

Table 1. Lipschitz constant for various functions used in a neural network. Here, Wj denotes the jth row of a weight matrix W .

It is known that controlling the Lipschitz constant of neural
nets can help in terms of improving generalization error due
to a lower bound on Rademacher complexity (Neyshabur
et al., 2015; Bartlett & Mendelson, 2002). It then follows
from Theorems 1 and 2 that controlling the Lipschitz
constant of a learned transition model can achieve better
error bounds for multi-step and value predictions. To
enforce this constraint during learning, we bound the
Lipschitz constant of various operations used in building
neural network. The bound on the constant of the entire
neural network then follows from Lemma 2. In Table 1, we
provide Lipschitz constant for operations (see Appendix for
proof) used in our experiments. We quantify these results
for different p-norms

(cid:107)·(cid:107)p.

speciﬁcally when the transition model

Given these simple methods for enforcing Lipschitz
continuity, we performed empirical evaluations
to
understand the impact of Lipschitz continuity of transition
models,
is
used to perform multi-step state-predictions and policy
improvements. We chose two standard domains: Cart Pole
and Pendulum. In Cart Pole, we trained a network on a
dataset of 15
. During training, we
(cid:105)
ensured that the weights of the network are smaller than k.
For each k, we performed 20 independent model estimation,
and chose the model with median cross-validation error.

s, a, s(cid:48)
(cid:104)

103 tuples

∗

Using the learned model, along with the actual reward
signal of the environment, we then performed stochastic
actor-critic RL. (Barto et al., 1983; Sutton et al., 2000)
This required an interaction between the policy and the
learned model for relatively long trajectories. To measure
the usefulness of the model, we then tested the learned
policy on the actual domain. We repeated this experiment
on Pendulum. To train the neural transition model for
this domain we used 104 samples. Notably, we used
deterministic policy gradient (Silver et al., 2014) for training
the policy network with the hyper parameters suggested by
Lillicrap et al. (2015). We report these results in Figure 6.

Observe that an intermediate Lipschitz constant yields the
best result. Consistent with the theory, controlling the
Lipschitz constant in practice can combat the compounding
errors and can help in the value estimation problem. This
ultimately results in learning a better policy.

We next examined if the beneﬁts carry over to stochastic

Figure 6. Impact of Lipschitz constant of learned models in Cart
Pole (left) and Pendulum (right). An intermediate value of k
(Lipschitz constant) yields the best performance.

Fg

θf : f
{

settings. To capture stochasticity we need an algorithm to
learn a Lipschitz model class (Deﬁnition 4). We used an EM
algorithm to joinly learn a set of functions f , parameterized
by θ =
, and a distribution over functions
}
g. Note that in practice our dataset only consists of a set
of samples
and does not include the function the
sample is drawn from. Hence, we consider this as our
latent variable z. As is standard with EM, we start with the
log-likelihood objective (for simplicity of presentation we
assume a single action in the derivation):

s, a, s(cid:48)
(cid:104)

∈

(cid:105)

L(θ) =

log p(si, si(cid:48); θ)

N

i=1
(cid:88)
N

i=1
(cid:88)
N

i=1
(cid:88)
N

=

=

≥

log

p(zi = f, si, si(cid:48); θ)

f
(cid:88)

f
(cid:88)

log

q(zi = f

si, si(cid:48))

|

q(zi = f

si, si(cid:48))log

|

i=1
(cid:88)

f
(cid:88)

p(zi = f, si, si(cid:48); θ)
si, si(cid:48))
|

q(zi = f

p(zi = f, si, si(cid:48); θ)
si, si(cid:48))
|

q(zi = f

,

where we used Jensen’s inequality and concavity of log in
the last line. This derivation leads to the following EM
algorithm.

Lipschitz Continuity in Model-based Reinforcement Learning

Figure 7. A stochastic problem solved by training a Lipschitz
model class using EM. The top left ﬁgure shows the functions
before any training (iteration 0), and the bottom right ﬁgure shows
the ﬁnal results (iteration 50).

In the M step, ﬁnd θt by solving for:

N

argmax

θ

i=1
(cid:88)

f
(cid:88)

qt

1(zi = f

−

si, si(cid:48))log
|

p(zi = f, si, si(cid:48); θ)
si, si(cid:48))
qt

1(zi = f

−

|

In the E step, compute posteriors:

qt(zi = f

si, si(cid:48)) =
|

p(si, si(cid:48)
f p(si, si(cid:48)|

|

zi = f ; θt

f )g(zi = f ; θt)

zi = f ; θt

f )g(zi = f ; θt)

.

Note that we assume each point is drawn from a neural
network f with probability:

(cid:80)

p

si, si(cid:48)

zi = f ; θt
|

N
and with a ﬁxed variance σ2 tuned as a hyper-parameter.

−

(cid:17)

(cid:0)

(cid:1)

(cid:16)(cid:12)
(cid:12)

(cid:12)
(cid:12)

f

=

si(cid:48)

f (si, θt

f )

, σ2

,

We used a supervised-learning domain to evaluate the EM
algorithm. We generated 30 points from 5 functions (written
at the end of Appendix) and trained 5 neural networks to ﬁt
these points. Iterations of a single run is shown in Figure 7
and the summary of results is presented in Figure 8. Observe
that the EM algorithm is effective, and that controlling the
Lipschitz constant is again useful.

We next applied EM to train a transition model for an RL
setting, namely the gridworld domain from Moerland et al.
(2017). Here a useful model needs to capture the stochastic
behavior of the two ghosts. We modify the reward to be
-1 whenever the agent is in the same cell as either one of
the ghosts and 0 otherwise. We performed environmental
interactions for 1000 time-steps and measured the return.
We compared against standard tabular methods(Sutton
& Barto, 1998), and a deterministic model that predicts
expected next state (Sutton et al., 2008; Parr et al., 2008). In
all cases we used value iteration for planning.

Figure 8. Impact of controlling the Lipschitz constant in the
supervised-learning domain.
Notice the U-shape of ﬁnal
Wasserstein loss with respect to Lipschitz constant k.

Figure 9. Performance of a Lipschitz model class on the gridworld
domain. We show model test accuracy (left) and quality of the
policy found using the model (right). Notice the poor performance
of tabular and expected models.

Results in Figure 9 show that tabular models fail due to no
generalization, and expected models fail since the ghosts
do not move on expectation, a prediction not useful for
planner. Performing value iteration with a Lipschitz model
class outperforms the baselines.

9. Conclusion

We took an important
step towards understanding
model-based RL with function approximation. We showed
that Lipschitz continuity of an estimated model plays
a central role in multi-step prediction error, and in
value-estimation error. We also showed the beneﬁts of
employing Wasserstein for model-based RL. An important
future work is to apply these ideas to larger problems.

10. Acknowledgements

The authors recognize the assistance of Eli Upfal, John
Langford, George Konidaris, and members of Brown’s Rlab
speciﬁcally Cameron Allen, David Abel, and Evan Cater.
The authors also thank anonymous ICML reviewer 1 for
insights on a value-aware interpretation of Wasserstein.

Lipschitz Continuity in Model-based Reinforcement Learning

References

Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein
In International

generative adversarial networks.
Conference on Machine Learning, pp. 214–223, 2017.

Asadi, K. and Littman, M. L. An alternative softmax
operator for reinforcement learning. In Proceedings of
the 34th International Conference on Machine Learning,
pp. 243–252, 2017.

Asadi, K., Cater, E., Misra, D., and Littman, M. L.
Equivalence between wasserstein and value-aware
model-based reinforcement learning. arXiv preprint
arXiv:1806.01265, 2018.

Bartlett, P. L. and Mendelson, S. Rademacher and gaussian
complexities: Risk bounds and structural results. Journal
of Machine Learning Research, 3(Nov):463–482, 2002.

Barto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike
adaptive elements that can solve difﬁcult learning control
IEEE transactions on systems, man, and
problems.
cybernetics, pp. 834–846, 1983.

Bellemare, M. G., Dabney, W., and Munos, R. A
distributional perspective on reinforcement learning. In
International Conference on Machine Learning, pp.
449–458, 2017.

Bellman, R. A markovian decision process. Journal of

Mathematics and Mechanics, pp. 679–684, 1957.

Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause,
A.
Safe model-based reinforcement learning with
stability guarantees. In Advances in Neural Information
Processing Systems, pp. 908–919, 2017.

Bertsekas, D. Convergence of discretization procedures in
dynamic programming. IEEE Transactions on Automatic
Control, 20(3):415–419, 1975.

Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-dynamic
programming: an overview. In Decision and Control,
1995, Proceedings of the 34th IEEE Conference on,
volume 1, pp. 560–564. IEEE, 1995.

Bubeck, S., Munos, R., Stoltz, G., and Szepesv´ari, C.
X-armed bandits. Journal of Machine Learning Research,
12(May):1655–1695, 2011.

Dempster, A. P., Laird, N. M., and Rubin, D. B.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society. Series
B (methodological), pp. 1–38, 1977.

Farahmand, A.-M., Barreto, A., and Nikovski, D.
for Model-based
the

Value-Aware
Reinforcement Learning.

In Proceedings of

Function

Loss

20th International Conference on Artiﬁcial Intelligence
and Statistics, pp. 1486–1494, 2017.

Ferns, N., Panangaden, P., and Precup, D. Metrics for ﬁnite
markov decision processes. In Proceedings of the 20th
conference on Uncertainty in artiﬁcial intelligence, pp.
162–169. AUAI Press, 2004.

Fox, R., Pakman, A., and Tishby, N. G-learning: Taming
the noise in reinforcement learning via soft updates.
Uncertainty in Artiﬁcal Intelligence, 2016.

Gao, B. and Pavel, L.

On the properties of the
softmax function with application in game theory and
reinforcement learning. arXiv preprint arXiv:1704.00805,
2017.

Hinderer, K. Lipschitz continuity of value functions in
Markovian decision processes. Mathematical Methods of
Operations Research, 62(1):3–22, 2005.

Jiang, N., Kulesza, A., Singh, S., and Lewis, R. The
dependence of effective planning horizon on model
In Proceedings of AAMAS, pp. 1181–1189,
accuracy.
2015.

Kaelbling, L. P., Littman, M. L., and Moore, A. W.
Reinforcement learning: A survey. Journal of artiﬁcial
intelligence research, 4:237–285, 1996.

Kakade, S., Kearns, M. J., and Langford, J. Exploration
the
in metric state spaces.
20th International Conference on Machine Learning
(ICML-03), pp. 306–312, 2003.

In Proceedings of

Kearns, M. and Singh, S. Near-optimal reinforcement
learning in polynomial time. Machine Learning, 49(2-3):
209–232, 2002.

Kleinberg, R., Slivkins, A., and Upfal, E. Multi-armed
bandits in metric spaces. In Proceedings of the Fortieth
Annual ACM Symposium on Theory of Computing, pp.
681–690. ACM, 2008.

Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.

Littman, M. L. and Szepesv´ari, C.
A generalized
reinforcement-learning model:
and
applications. In Proceedings of the 13th International
Conference on Machine Learning, pp. 310–318, 1996.

Convergence

Lorenz, E. Predictability: does the ﬂap of a butterﬂy’s wing

in Brazil set off a tornado in Texas? na, 1972.

Lipschitz Continuity in Model-based Reinforcement Learning

Moerland, T. M., Broekens, J., and Jonker, C. M. Learning
for model-based
multimodal
reinforcement learning. arXiv preprint arXiv:1705.00470,
2017.

transition dynamics

M¨uller, A. Optimal selection from distributions with
unknown parameters: Robustness of bayesian models.
Mathematical Methods of Operations Research, 44(3):
371–386, 1996.

Nachum, O., Norouzi, M., Xu, K., and Schuurmans,
D. Bridging the gap between value and policy based
reinforcement learning. arXiv preprint arXiv:1702.08892,
2017.

Narayanan, H. and Mitter, S.

Sample complexity of
testing the manifold hypothesis. In Advances in Neural
Information Processing Systems, pp. 1786–1794, 2010.

Strehl, A. L., Li, L., and Littman, M. L. Reinforcement
learning in ﬁnite mdps: Pac analysis. Journal of Machine
Learning Research, 10(Nov):2413–2444, 2009.

Sutton, R. S. and Barto, A. G. Reinforcement Learning: An

Introduction. The MIT Press, 1998.

Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour,
Y. Policy gradient methods for reinforcement learning
In Advances in Neural
with function approximation.
Information Processing Systems, pp. 1057–1063, 2000.

Sutton, R. S., Szepesv´ari, C., Geramifard, A., and Bowling,
M. H.
Dyna-style planning with linear function
approximation and prioritized sweeping. In UAI 2008,
Proceedings of the 24th Conference in Uncertainty in
Artiﬁcial Intelligence, Helsinki, Finland, July 9-12, 2008,
pp. 528–536, 2008.

Neu, G., Jonsson, A., and G´omez, V. A uniﬁed view of
entropy-regularized Markov decision processes. arXiv
preprint arXiv:1705.07798, 2017.

Szepesv´ari, C. Algorithms for reinforcement learning.
Synthesis Lectures on Artiﬁcial Intelligence and Machine
Learning, 4(1):1–103, 2010.

Talvitie, E. Model regularization for stable sample
rollouts. In Proceedings of the Thirtieth Conference on
Uncertainty in Artiﬁcial Intelligence, pp. 780–789. AUAI
Press, 2014.

Talvitie, E.

Self-correcting models for model-based
reinforcement learning. In Proceedings of the Thirty-First
AAAI Conference on Artiﬁcial Intelligence, February 4-9,
2017, San Francisco, California, USA., 2017.

Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society. Series B
(Methodological), pp. 267–288, 1996.

Vaserstein, L. N. Markov processes over denumerable
products of spaces, describing large systems of automata.
Problemy Peredachi Informatsii, 5(3):64–72, 1969.

Venkatraman, A., Hebert, M., and Bagnell, J. A. Improving
multi-step prediction of learned time series models. In
Proceedings of the Twenty-Ninth AAAI Conference on
Artiﬁcial Intelligence, January 25-30, 2015, Austin, Texas,
USA., 2015.

Villani, C. Optimal transport: old and new, volume 338.

Springer Science & Business Media, 2008.

Neyshabur, B., Tomioka, R., and Srebro, N. Norm-based
capacity control in neural networks. In Proceedings of
The 28th Conference on Learning Theory, pp. 1376–1401,
2015.

Parr, R., Li, L., Taylor, G., Painter-Wakeﬁeld, C., and
Littman, M. L. An analysis of linear models, linear
value-function approximation, and feature selection
In Proceedings of the
for reinforcement learning.
25th international conference on Machine learning, pp.
752–759. ACM, 2008.

Pazis, J. and Parr, R. Pac optimal exploration in continuous

space markov decision processes. In AAAI, 2013.

Pires, B. ´A. and Szepesv´ari, C. Policy error bounds for
model-based reinforcement learning with factored linear
models. In Conference on Learning Theory, pp. 121–151,
2016.

Pirotta, M., Restelli, M., and Bascetta, L. Policy gradient in
lipschitz Markov decision processes. Machine Learning,
100(2-3):255–283, 2015.

Rachelson, E. and Lagoudakis, M. G. On the locality of
action domination in sequential decision making.
In
International Symposium on Artiﬁcial Intelligence and
Mathematics, ISAIM 2010, Fort Lauderdale, Florida,
USA, January 6-8, 2010, 2010.

Russell, S. J. and Norvig, P. Artiﬁcial intelligence: A

modern approach, 1995.

Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and
Riedmiller, M. Deterministic policy gradient algorithms.
In ICML, 2014.

Lipschitz Continuity in Model-based Reinforcement Learning

Appendix

Claim 1. In a ﬁnite MDP, transition probabilities can be expressed using a ﬁnite set of deterministic functions and a
distribution over the functions.

Proof. Let P r(s, a, s(cid:48)) denote the probability of a transiton from s to s(cid:48) when executing the action a. Deﬁne an ordering
over states s1, ..., sn with an additional unreachable state s0. Now deﬁne the cumulative probability distribution:

Further deﬁne L as the set of distinct entries in C:

C(s, a, si) :=

P r(s, a, sj) .

i

j=0
(cid:88)

L :=

C(s, a, si)
|

s

, i

[0, n]

.

∈ S

∈

(cid:110)

(cid:111)

Note that, since the MDP is assumed to be ﬁnite, then
value of the set. Note that c0 = 0 and c
L
|

|

i = 1 to

and

j = 1 to n, deﬁne fi(s) = sj if and only if:

∀

L
|

|

∀

= 1. We now build determinstic set of functions f1, ..., f

is ﬁnite. We sort the values of L and denote, by ci, ith smallest
as follows:

L
|

|

L
|

|

We also deﬁne the probability distribution g over f as follows:

C(s, a, sj

1) < ci

C(s, a, sj) .

−

≤

g(fi

a) := ci
|

−

ci

1 .

−

Given the functions f1, ..., f
executing action a:

L
|

|

and the distribution g, we can now compute the probability of a transition to sj from s after

1(fi(s) = sj) g(fi

a)
|

i
(cid:88)
=

1

i
(cid:88)

(cid:0)

= C(s, a, sj)
−
= P r(s, a, sj) ,

C(s, a, sj

1) < ci

C(s, a, sj)

(ci

−

≤

ci

1)

−

−

C(s, a, sj

1)

−

(cid:1)

where 1 is a binary function that outputs one if and only if its condition holds. We reconstructed the transition probabilities
using distribution g and deterministic functions f1, ..., f

.

L
|

|

Claim 2. Given a deterministic and linear transition model, and a linear reward signal, the bounds provided in Theorems 1
and 2 are both tight.

Assume a linear transition function T deﬁned as:

Assume our learned transition function ˆT :

Note that:

and that:

T (s) = Ks

ˆT (s) := Ks + ∆

max
s

T (s)

−

ˆT (s)

= ∆

(cid:12)
(cid:12)
KT , K ˆT }
min
{

(cid:12)
(cid:12)
= K

First observe that the bound in Theorem 2 is tight for n = 2:

T

T (s)

ˆT

ˆT (s)

=

K 2s

s
∀

(cid:12)
(cid:12)
(cid:12)

−

(cid:0)

(cid:1)

(cid:0)

(cid:1)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

−

K 2s + ∆(1 + K)
(cid:12)
(cid:12)
(cid:12)

= ∆

K i

1

i=0
(cid:88)

Lipschitz Continuity in Model-based Reinforcement Learning

and more generally and after n compositions of the models, denoted by T n and ˆT n, the following equality holds:

Lets further assume that the reward is linear:

s
∀

T n(s)

−

(cid:12)
(cid:12)
(cid:12)

= ∆

K i

n

1

−

i=0
(cid:88)

ˆT n(s)
(cid:12)
(cid:12)
(cid:12)

R(s) = KRs

Consider the state s = 0. Note that clearly v(0) = 0. We now compute the value predicted using ˆT , denoted by ˆv(0):

ˆv(0) = R(0) + γR(0 + ∆

K i) + γ2R(0 + ∆

K i) + γ3R(0 + ∆

K i) + ...

0

i=0
(cid:88)

2

i=0
(cid:88)

1

i=0
(cid:88)

2

i=0
(cid:88)

= 0 + γKR∆

K i + γ2KR∆

K i) + γ3KR∆

K i + ...

0

1

= γKR∆

K i =

i=0
(cid:88)
n
γn

1

−

∞

n=0
(cid:88)

i=0
(cid:88)

i=0
(cid:88)
γKR∆
γ)(1

−

(1

−

,

γ ¯K)

and so:

v(0)
|

ˆv(0)
|

−

=

(1

γKR∆
γ)(1

−

−

γ ¯K)

Note that this exactly matches the bound derived in our Theorem 2.

Lemma 1. A generalized transition function

induced by a Lipschitz model class Fg is Lipschitz with a constant:

T
G

Proof.

W

T (

µ1, a),

T (

µ2, a)

· |

(cid:0)

(cid:98)

· |

(cid:98)

(cid:1)

K AW,W (

T
G

(cid:98)
) := sup

sup
µ1,µ2

a

W

T
G

(
µ1, a),
(
T
·|
G
W (µ1, µ2)

·|

µ2, a)

(cid:0)

(cid:98)

(cid:98)

KF

≤

(cid:1)

(cid:98)

:= inf
j

= inf
j

= inf
j

j(s(cid:48)1, s(cid:48)2)d(s(cid:48)1, s(cid:48)2)ds(cid:48)1ds(cid:48)2

(cid:90)s(cid:48)

1 (cid:90)s(cid:48)

2

1

f (s1) = s(cid:48)1 ∧

(cid:90)s1 (cid:90)s2 (cid:90)s(cid:48)

1 (cid:90)s(cid:48)

2

f
(cid:88)
j(s1, s2, f )d

(cid:0)

(cid:90)s1 (cid:90)s2

f
(cid:88)

(cid:1)

f (s1), f (s2)

ds1ds2

(cid:0)

(cid:1)

KF inf
j

≤

g(f

a)j(s1, s2)d(s1, s2)ds1ds2
|

= KF

g(f

j(s1, s2)d(s1, s2)ds1ds2

(cid:90)s1 (cid:90)s2

f
(cid:88)
a) inf
j

(cid:90)s1 (cid:90)s2

|

|

f
(cid:88)

f
(cid:88)

= KF

g(f

a)W (µ1, µ2) = KF W (µ1, µ2)

Dividing by W (µ1, µ2) and taking sup over a, µ1, and µ2, we conclude:

K AW,W (

T ) = sup

sup
µ1,µ2

a

(cid:98)

W

T (

· |

µ1, a),
T (
· |
W (µ1, µ2)

µ2, a)

(cid:0)

(cid:98)

(cid:98)

KF .

≤

(cid:1)

We can also prove this using the Kantarovich-Rubinstein duality theorem:

f (s2) = s(cid:48)2

j(s1, s2, f )d(s(cid:48)1, s(cid:48)2)ds(cid:48)1ds(cid:48)2ds1ds2

Lipschitz Continuity in Model-based Reinforcement Learning

For every µ1, µ2, and a

we have:

∈ A

W

T
G

(
· |

µ1, a),

T
G

(
· |

µ2, a)

(cid:0)

(cid:98)

(cid:98)

(cid:1)

=

=

=

=

=

=

≤

≤

sup

sup

sup

sup

f :KdS ,R(f )

1 (cid:90)s

≤

(cid:0)

(cid:98)
1 (cid:90)s (cid:90)s0 (cid:16)

f :KdS ,R(f )

≤

T
G

(s
|

µ1, a)

T
G

µ2, a)
(s
|

−

f (s)ds

(cid:98)
s0, a)µ1(s0)
T (s
|

−

(cid:1)
s0, a)µ2(s0)
T (s
|

f (s)dsds0

(cid:98)
s0, a)
T (s
|

µ1(s0)

−

(cid:98)
µ2(s0)

(cid:17)
f (s)dsds0

f :KdS ,R(f )

1 (cid:90)s (cid:90)s0

≤

f :KdS ,R(f )

1 (cid:90)s (cid:90)s0

≤

sup

f :KdS ,R(f )

sup

f :KdS ,R(f )

1

≤

t
(cid:88)

1

≤

t
(cid:88)

g(t

a)

|

(cid:98)

|

|

t
(cid:88)
a)

g(t

g(t

a)

(cid:16)
a)1

g(t

|

(cid:0)
1

(cid:90)s0 (cid:90)s

(cid:90)s0

(cid:0)

−

−

(cid:17)
µ1(s0)

(cid:1)(cid:16)

−

−

t(s0) = s

µ1(s0)

µ2(s0)

(cid:0)
µ1(s0)

(cid:1)(cid:0)
µ2(s0)

f

t(s0)

ds0

(cid:1)

sup

µ1(s0)

(cid:1)
µ2(s0)

f

(cid:0)

(cid:1)

t(s0)

ds0

t
(cid:88)
composition of f, t is Lipschitz with constant upper bounded by KF .

(cid:1)

(cid:0)

(cid:1)

(cid:0)

f :KdS ,R(f )

1 (cid:90)s0
≤

t(s0) = s

µ2(s0)

f (s)dsds0

(cid:17)
f (s)dsds0

= KF

g(t

a)

sup

µ1(s0)

µ2(s0)

f :KdS ,R(f )

1 (cid:90)s0
≤

KF

g(t

a)

sup

= KF

g(t

h:KdS ,R(h)

1 (cid:90)s0
≤
a)W (µ1, µ2) = KF W (µ1, µ2)

(cid:0)

(cid:1)

f (t(s0))
KF

ds0

(cid:0)
µ1(s0)

(cid:1)
h(s0))ds0

µ2(s0)

−

−

|

|

|

t
(cid:88)

t
(cid:88)

t
(cid:88)

Again we conclude by dividing by W (µ1, µ2) and taking sup over a, µ1, and µ2.

Lemma 2. (Composition Lemma) Deﬁne three metric spaces (M1, d1), (M2, d2), and (M3, d3). Deﬁne Lipschitz functions
M3 is Lipschitz with
f : M2 (cid:55)→
constant Kd1,d3 (h)

M2 with constants Kd2,d3 (f ) and Kd1,d2 (g). Then, h : f

Kd2,d3 (f )Kd1,d2(g).

g : M1 (cid:55)→

◦

M3 and g : M1 (cid:55)→
≤

Proof.

Kd1,d3 (h) = sup
s1,s2

d3

f

g(s1)

, f

g(s2)

(cid:16)

(cid:0)

d1(s1, s2)
(cid:0)
(cid:1)

(cid:1)(cid:17)

= sup
s1,s2

d2

g(s1), g(s2)
d1(s1, s2)
(cid:0)

d2

(cid:1)
g(s1), g(s2)
d1(s1, s2)
≤
(cid:0)
(cid:1)
= Kd1,d2(g)Kd2,d3 (f ).

sup
s1,s2

d3

f

g(s1)

, f

g(s2)

(cid:1)(cid:17)

(cid:16)

sup
s1,s2

d2
(cid:0)
d3
(cid:0)

g(s1), g(s2)
(cid:1)
(cid:0)
f (s1), f (s2)
(cid:1)
d2(s1, s2)

(cid:0)

(cid:1)

Lemma 3. Given a Lipschitz function f :

R with constant KdS ,dR(f ):

S (cid:55)→

K AdS ,dR

T (s(cid:48)

s, a)f (s(cid:48))ds(cid:48)

KdS ,dR (f )K AdS ,W

T

.

|

(cid:16) (cid:90)

(cid:98)

≤

(cid:17)

(cid:0)

(cid:1)

(cid:98)

Proof.

K AdS ,dR

T (s(cid:48)

s, a)f (s(cid:48))ds(cid:48)

= sup

|

(cid:16) (cid:90)s(cid:48)

(cid:98)

Lipschitz Continuity in Model-based Reinforcement Learning

(cid:17)

sup
s1,s2

(cid:82)

(cid:0)

s(cid:48)

s(cid:48)

|

|

T (s(cid:48)

s1, a)
|

(cid:98)
T (s(cid:48)

s1, a)
|

a

a

= sup

sup
s1,s2

(cid:82)

(cid:0)

(cid:98)

= KdS ,dR(f ) sup
a

sup
s1,s2

s2, a)
T (s(cid:48)
−
|
d(s1, s2)
(cid:98)
T (s(cid:48)

s2, a)
|
d(s1, s2)
(cid:98)
T (s(cid:48)

s1, a)

−

(cid:1)

(cid:1)

f (s(cid:48))ds(cid:48)

|

f (s(cid:48))

KdS ,dR (f )
KdS ,dR (f ) ds(cid:48)

|

s(cid:48)

|

(cid:0)

(cid:82)
(cid:98)
supg:KdS ,dR (g)
≤

1

|

|

supg:KdS ,dR (g)
≤

1

s2, a)
|

T (s(cid:48)

−
d(s1, s2)

(cid:98)
s(cid:48)

(cid:82)

T (s(cid:48)

(cid:1)
s1, a)
|
d(s1, s2)
(cid:0)
(cid:98)
T (s(cid:48)

s(cid:48)

s1, a)
|
d(s1, s2)
(cid:0)
(cid:98)
s2, a)

(cid:82)

·|

(cid:1)

f (s(cid:48))
KdS ,dR (f ) ds(cid:48)

|

T (s(cid:48)

s2, a)
|

−

g(s(cid:48))ds(cid:48)

|

(cid:1)
g(s(cid:48))ds(cid:48)

(cid:98)
T (s(cid:48)

−

s2, a)
|

(cid:98)

(cid:1)

KdS ,dR(f ) sup
a

sup
s1,s2

≤

= KdS ,dR(f ) sup
a

sup
s1,s2

= KdS ,dR(f ) sup
a

sup
s1,s2

= KdS ,dR(f )K AdS ,W (

T ) .

W

T (

·|

s1, a),
T (
d(s1, s2)
(cid:98)

(cid:0)

(cid:98)

(cid:98)

Lemma 4. The following operators (Asadi & Littman, 2017) are Lipschitz with constants:

(cid:107)(cid:107)∞,dR

mean(x)

= K

(cid:107)(cid:107)∞,dR ((cid:15)-greedy(x)) = 1

1. K

2. K

(cid:107)(cid:107)∞,dR (max(x)) = K
(cid:107)(cid:107)∞,dR (mmβ(x) := log

(cid:80)

(cid:0)
i eβxi
n

β

) = 1

(cid:1)

3. K

(cid:107)(cid:107)∞,dR (boltzβ(x) :=

(cid:80)n

i=1 xieβxi
(cid:80)n
i=1eβ xi

)

A
|

|

A
+ βVmax|

|

≤

(cid:112)

and observe that boltzβ(x) = x(cid:62)ρ(x). Gao & Pavel (2017) showed that ρ is Lipschitz:

(cid:80)

ρ(x)i =

eβxi
n
i=1 eβxi

,

ρ(x2)

ρ(x1)
(cid:107)

x2(cid:107)2
Using their result, we can further show:

x1 −
(cid:107)

(cid:107)2 ≤

−

β

Proof. 1 was proven by Littman & Szepesv´ari (1996), and 2 is proven several times (Fox et al., 2016; Asadi & Littman,
2017; Nachum et al., 2017; Neu et al., 2017). We focus on proving 3. Deﬁne

(7)

ρ(x1)(cid:62)x2 −

|

ρ(x2)(cid:62)x2|

(Cauchy-Shwartz)

ρ(x2)(cid:62)x2|

|

ρ(x1)(cid:62)x1 −
≤ |

+

ρ(x1)(cid:62)x1 −
ρ(x1)
(cid:107)2 (cid:107)
ρ(x1)
x2(cid:107)2 (cid:107)
(cid:107)
ρ(x1)
x2(cid:107)2 β
(cid:107)
(1 + βVmax

x1 −
−
x1 −
(cid:107)2 (cid:107)
x1 −
(cid:107)

ρ(x1)(cid:62)x2|
x2(cid:107)2
ρ(x2)
x2(cid:107)2
x2(cid:107)2
x1 −
)
A
(cid:0)
|
(cid:107)
|
x1 −
)
A
+ βVmax|
(cid:112)
(cid:107)
|

A
|

(cid:107)2

(

≤ (cid:107)
+

≤ (cid:107)
+

≤

≤
|
(cid:112)
leads to 3.

from Eqn 7)
x2(cid:107)2
x2(cid:107)∞

,

(cid:1)

dividing both sides by

x1 −
(cid:107)

x2(cid:107)∞

Lipschitz Continuity in Model-based Reinforcement Learning

Below, we derive the Lipschitz constant for various functions mentioned in Table 1.
Rn has Lipschitz constant 1 for p.
ReLu non-linearity We show that ReLu : Rn

→

K
(cid:107)

.
(cid:107)

p,

.
(cid:107)

(cid:107)

p (ReLu) = sup
x1,x2

= sup
x1,x2
(cid:107)
(We can show that

1
p

p)
|

p

p

p

(

i |

(cid:80)

ReLu(x1)
(cid:107)

ReLu(x2)
−
(cid:107)
x2(cid:107)
x1 −
(cid:107)
ReLu(x1)i
ReLu(x2)i
−
x2(cid:107)
x1 −
ReLu(x1)i
|
x1,i
x2,i
i |
−
x1 −
x2(cid:107)
(cid:80)
(cid:107)
x2(cid:107)
x1 −
= 1
(cid:107)
x2(cid:107)
x1 −
(cid:107)

p)
|

−

(

1
p

p

p

p

sup
x1,x2

≤

= sup
x1,x2

ReLu(x2)i

x1,i

x2,i

and so) :

| ≤ |

−

|

Matrix multiplication Let W

Rn

×

m. We derive the Lipschitz continuity for the function

W (x) = W x.

×

For p =

we have:

∞

∈

where Wj refers to jth row of the weight matrix W . Similarly, for p = 1 we have:

K

(cid:107)(cid:107)∞,

= sup
x1,x2

= sup
x1,x2

sup
≤
x1,x2
= sup

j (cid:107)

W (x1)

(cid:107)(cid:107)∞

×
W (x1)
(cid:0)
(cid:107)×

W (x2)
(cid:1)
− ×
x2(cid:107)∞
x2)

|

x1 −
(cid:107)
Wj(x1 −
supj |
x2(cid:107)∞
x1 −
(cid:107)
x1 −
Wj
supj (cid:107)
(cid:107) (cid:107)
x2(cid:107)∞
x1 −
(cid:107)
(cid:107)1 ,

Wj

x2(cid:107)∞

W (x2)
x2(cid:107)1
x2)
|

K

(cid:107)(cid:107)1,

(cid:107)(cid:107)1

= sup
x1,x2

(cid:0)
(cid:107)×

= sup

x1,x2 (cid:80)

≤

sup
x1,x2 (cid:80)

W (x1)

×
W (x1)

(cid:1)
− ×
x1 −
(cid:107)
Wj(x1 −
j |
x2(cid:107)1
x1 −
(cid:107)
Wj
j (cid:107)
(cid:107)∞ (cid:107)
x1 −
(cid:107)

K

(cid:107)(cid:107)2,

(cid:107)(cid:107)2

= sup
x1,x2

(cid:0)
(cid:107)×

W (x1)

×
W (x1)

2
|

x2)

W (x2)
(cid:1)
− ×
x2(cid:107)2
x1 −
(cid:107)
Wj(x1 −
j |
x2(cid:107)2
x1 −
(cid:107)
2
Wj
x1 −
j (cid:107)
2 (cid:107)
(cid:107)
x2(cid:107)2
x1 −
(cid:107)

= sup

x1,x2 (cid:113)(cid:80)

sup
x1,x2 (cid:113)(cid:80)

≤

(cid:107)∞

= sup
x1,x2

W x1 −
(cid:107)
x1 −
(cid:107)

W x2(cid:107)∞
x2(cid:107)∞

= sup
x1,x2

W (x1 −
(cid:107)
x1 −
(cid:107)

x2)
x2(cid:107)∞

(cid:107)∞

(H¨older’s inequality)

(cid:107)1

= sup
x1,x2

(cid:107)

W x1 −
x1 −
(cid:107)

W x2(cid:107)1
x2(cid:107)1

= sup
x1,x2

(cid:107)

W (x1 −
x1 −
(cid:107)

x2)
x2(cid:107)1

(cid:107)1

(cid:107)2

= sup
x1,x2

(cid:107)

W x1 −
x1 −
(cid:107)

W x2(cid:107)2
x2(cid:107)2

= sup
x1,x2

(cid:107)

W (x1 −
x1 −
(cid:107)

x2)
x2(cid:107)2

(cid:107)2

2
2

x2(cid:107)

=

Wj
(cid:107)

2
2
(cid:107)

.

j

(cid:115)(cid:88)

and ﬁnally for p = 2:

x2(cid:107)1

=

x1 −
x2(cid:107)1

j
(cid:88)

Wj
(cid:107)

(cid:107)∞

,

Lipschitz Continuity in Model-based Reinforcement Learning

Vector addition We show that +b : Rn

Rn has Lipschitz constant 1 for p = 0, 1,

for all b

Rn.

→

∞

∈

K
(cid:107)

.
(cid:107)

p,

.
(cid:107)

(cid:107)

p (ReLu) = sup
x1,x2

(cid:107)

+ b(x1)
−
x1 −
(cid:107)
(x1 + b)
−
(cid:107)
x1 −
(cid:107)

+b(x2)
p
(cid:107)
x2(cid:107)
p
(x2 + b)
p
(cid:107)
x2(cid:107)

p

= sup
x1,x2

x1 −
= (cid:107)
x1 −
(cid:107)

x2(cid:107)
x2(cid:107)

p

p

= 1

Supervised-learning domain We used the following 5 functions to generate the dataset:

f0(x) = tanh(x) + 3
x
f1(x) = x
f2(x) = sin(x)
f3(x) = sin(x)
f4(x) = sin(x)

−

−

5

∗

3
sin(x)

∗

We sampled each function 30 times, where the input was chosen uniformly randomly from [

2, 2] each time.

−

