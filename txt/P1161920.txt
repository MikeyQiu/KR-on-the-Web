Tree Structured Dirichlet Processes for
Hierarchical Morphological Segmentation

Burcu Can
Hacettepe University
Department of Computer Engineering
burcucan@cs.hacettepe.edu.tr

Suresh Manandhar
University of York
Department of Computer Science
suresh@cs.york.ac.uk

This article presents a probabilistic hierarchical clustering model for morphological segmenta-
tion. In contrast to existing approaches to morphology learning, our method allows learning
hierarchical organization of word morphology as a collection of tree structured paradigms. The
model is fully unsupervised and based on the hierarchical Dirichlet process. Tree hierarchies are
learned along with the corresponding morphological paradigms simultaneously. Our model is
evaluated on Morpho Challenge and shows competitive performance when compared to state-
of-the-art unsupervised morphological segmentation systems. Although we apply this model for
morphological segmentation, the model itself can also be used for hierarchical clustering of other
types of data.

1. Introduction

Unsupervised learning of morphology has been an important task because of the bene-
ﬁts it provides to many other natural language processing applications such as machine
translation, information retrieval, question answering, and so forth. Morphological
paradigms provide a natural way to capture the internal morphological structure of
a group of morphologically related words. Following Goldsmith (2001) and Monson
(2008), we use the term paradigm as consisting of a set of stems and a set of sufﬁxes
where each combination of a stem and a sufﬁx leads to a valid word form, for example,
{walk,talk,order,yawn}{s,ed,ing} generating the surface forms walk+ed, walk+s, walk+ing,
talk+ed, talk+s, talk+ing, order+s, order+ed, order+ing, yawn+ed, yawn+s, yawn+ing. A sam-
ple paradigm is given in Figure 1.

Recently, we introduced a probabilistic hierarchical clustering model for learning
hierarchical morphological paradigms (Can and Manandhar 2012). Each node in the
hierarchical tree corresponds to a morphological paradigm and each leaf node consists
of a word. A single tree is learned, where different branches on the hierarchical tree

Submission received: 29 June 2016; revised version received: 30 July 2017; accepted for publication: 1 March
2018.

doi:10.1162/COLI a 00318

© 2018 Association for Computational Linguistics
Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International
(CC BY-NC-ND 4.0) license

Computational Linguistics

Volume 44, Number 2

Figure 1
An example paradigm.

correspond to different morphological forms. Well-deﬁned paradigms in the lower
levels of trees were learned. However, merging of the paradigms at the upper levels
led to undersegmentation in that model. This problem led us to search for ways to
learn multiple trees. In our current approach, we learn a forest of paradigms spread
over several hierarchical trees. Our evaluation on Morpho Challenge data sets provides
better results when compared to the previous method (Can and Manandhar 2012). Our
results are comparable to current state-of-the-art results while having the additional
beneﬁt of inferring the hierarchical structure of morphemes for which no comparable
systems exist.

The article is organized as follows: Section 2 introduces the related work in the ﬁeld.
Section 3 describes the probabilistic hierarchical clustering model with its mathematical
model deﬁnition and how it is applied for morphological segmentation; the same
section explains the inference and the morphological segmentation. Section 4 presents
the experimental setting and the obtained evaluation scores from each experiment,
and Section 5 concludes and addresses the potential future work following the model
presented in this article.

2. Related Work

There have been many unsupervised approaches to morphology learning that focus
solely on segmentation (Creutz and Lagus 2005a, 2007; Snyder and Barzilay 2008; Poon,
Cherry, and Toutanova 2009; Narasimhan, Barzilay, and Jaakkola 2015). Others, such
as Monson et al. (2008), Can and Manandhar (2010), Chan (2006), and Dreyer and Eisner
(2011), learn morphological paradigms that permit additional generalization.

A popular paradigmatic model is Linguistica (Goldsmith 2001), which uses the
Minimum Description Length principle to minimize the description length of a corpus
based on paradigm-like structures called signatures. A signature consists of a list of
sufﬁxes that are seen with a particular stem—for example, order-{ed, ing, s} denotes a
signature for the stem order.

Snover, Jarosz, and Brent (2002) propose a generative probabilistic model that
deﬁnes a probability distribution over different segmentations of the lexicon into
paradigms. Paradigms are learned with a directed search algorithm that examines

350

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 2
Examples of hierarchical morphological paradigms.

subsets of the lexicon, ranks them, and incrementally combines them in order to ﬁnd
the best segmentation of the lexicon. The proposed model addresses both inﬂectional
and derivational morphology in a language independent framework. However, their
model does not allow multiple sufﬁxation (e.g., having multiple sufﬁxes added to a
single stem) whereas Linguistica allows this.

Monson et al. (2008) induce morphological paradigms in a deterministic framework
named ParaMor. Their search algorithm begins with a set of candidate sufﬁxes and
collects candidate stems that attach to the sufﬁxes (see Figure 2(b)). The algorithm
gradually develops paradigms following search paths in a lattice-like structure. Proba-
bilistic ParaMor, involving a statistical natural language tagger to mimic ParaMor, was
introduced in Morpho Challenge 2009 (Kurimo et al. 2009). The system outperforms
other unsupervised morphological segmentation systems that competed in Morpho
Challenge 2009 (Kurimo et al. 2009) for the languages Finnish, Turkish, and German.

Can and Manandhar (2010) exploit syntactic categories to capture morphological
paradigms. In a deterministic scheme, morphological paradigms are learned by pairing
syntactic categories and identifying common sufﬁxes between them. The paradigms
compete to acquire more word pairs.

Chan (2006) describes a supervised procedure to ﬁnd morphological paradigms
within a hierarchical structure by applying latent Dirichlet allocation. Each paradigm
is placed in a node on the tree (see Figure 2(a)). The results show that each paradigm
corresponds to a part-of-speech such as adjective, noun, verb, or adverb. However, as
the method is supervised, the true sufﬁxes and segmentations are known in advance.
Learning hierarchical paradigms helps not only in learning morphological segmenta-
tion, but also in learning syntactic categories. This linguistic motivation led us toward
learning the hierarchical organization of morphological paradigms.

Dreyer and Eisner (2011) propose a Dirichlet process mixture model to learn
paradigms. The model uses 50–100 seed paradigms to infer the remaining paradigms,
which makes it semi-supervised. The model is similar to ours in the sense that it also
uses Dirichlet processes (DPs); however the model does not learn hierarchies between
the paradigms.

351

Computational Linguistics

Volume 44, Number 2

Luo, Narasimhan, and Barzilay (2017) learn morphological families that share the
same stem, such as faithful, faithfully, unfaithful, faithless, and so on, that are all derived
from faith. Those morphological families are learned as a graph and called morpho-
logical forests, which deviates from the meaning of the term forest we refer in this
article. Although learning morphological families has been studied as a graph learning
problem in Luo, Narasimhan, and Barzilay (2017), in this work, we learn paradigms that
generalize morphological families within a collection of hierarchical structures.

Narasimhan, Barzilay, and Jaakkola (2015) model the word formation with mor-
phological chains in terms of parent-child relations. For example, play and playful have
a parent–child relationship as a result of adding the morpheme ful at the end of play.
These relations are modeled by using log-linear models in order to predict the parent
relations. Semantic features as given by word2vec (Mikolov et al. 2013) are used in their
model in addition to orthographic features for the prediction of parent–child relations.
Narasimhan, Barzilay, and Jaakkola use contrastive estimation and generate corrupted
examples as pseudo negative examples within their approach.

Our model is an extension of our previous hierarchical clustering algorithm (Can
and Manandhar 2012). In that algorithm, a single tree is learned that corresponds to
a hierarchical organization of morphological paradigms. The parent nodes merge the
paradigms from the child nodes. But such merging of paradigms into a single structure
causes unrelated paradigms to be merged resulting in lower segmentation accuracy. The
current model addresses this issue by learning a forest of tree structures. Within each
tree structure the parent nodes merge the paradigms from the child nodes. Multiple
trees ensure that paradigms that should not be merged are kept separated. Additionally,
in single tree hierarchical clustering, a manually deﬁned context free grammar was
employed to generate the segmentation of a word. In the current model, we predict
the segmentation of a word without using any manually deﬁned grammar rules.

3. Probabilistic Hierarchical Clustering

Chan (2006) showed that learning the hierarchy between morphological paradigms
can help reveal latent relations in data. In the latent class model of Chan (2006), mor-
phological paradigms in a tree structure can be linked to syntactic categories (i.e., part-
of-speech tags). An example output of the model is given in Figure 2(a). Furthermore,
tokens can be assigned to allomorphs or gender/conjugational variants in each
paradigm.

Monson et al. (2008) showed that learning paradigms within a hierarchical model
gives a strong performance on morphological segmentation. In their model, each
paradigm is part of another paradigm, implemented within a lattice structure (see
Figure 2(b)).

Motivated by these works, we aim to learn paradigms within a hierarchical tree
structure. We propose a novel hierarchical clustering model that deviates from the
current hierarchical clustering models in two aspects:

It is a generative model based on a hierarchical Dirichlet process (HDP)
that simultaneously infers the hierarchical structure and morphological
segmentation.

Our model infers multiple trees (i.e., a forest of trees) instead of a single
tree.

1.

2.

352

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 3
A portion of a tree rooted at i with child nodes k, j with corresponding data points they generate.
i, j, k refer to the tree nodes; Ti, Tj, Tk refer to corresponding trees; and Di, Dj, Dk refer to data
contained in trees.

Although not covered in this work, additional work can be aimed at discovering other
types of latent information such as part-of-speech and allomorphs.

3.1 Model Overview

Let D = {x1, x2, . . . , xN} denote the input data where each xj is a word. Let Di ⊆ D
denote the subset of the data generated by tree rooted at i, then (see Figure 3):

where Di = {x1

i , x2

i , . . . , xNi

i }.

θ and hyperparameters β is given by:

The marginal probability of the data items in a given node in a tree i with parameters

D = D1 ∪ D2 ∪ · · · ∪ Dk

(cid:90)

p(Di) =

p(Di|θ)p(θ|β)dθ

Given tree Ti the data likelihood is given recursively by:

p(Di|Ti) =

1
Z p(Di)p(Dl|Tl)p(Dr|Tr) if i is an internal node with child nodes
l and r, and Z is the partition function

(3)






p(Di) if i is a leaf node

The term 1
Z (Di)p(Dl|Tl)p(Dr|Tr) corresponds to a product of experts model (Hinton
2002) comprising two competing hypotheses.1 Hypothesis H1 is given by p(Di) and
assigns all the data points in Di into a single cluster. Hypothesis H2 is given by

1 The review version of this paper and our previous work (Can and Manandhar 2012) missed this

connection to the product of experts model and did not include the 1/Z term. We thank our reviewers for
spotting this error.

(1)

(2)

353

Computational Linguistics

Volume 44, Number 2

p(Dl|Tl)p(Dr|Tr) and recursively splits the data in Di into two partitions (i.e., subtrees)
Dl and Dr. The factor Z is the partition function or the normalizing constant given by:

Z =

(cid:88)

Di,Dl,Dr:Di=Dl∪Dr

p(Di)p(Dl|Tl)p(Dr|Tr)

(4)

The recursive partitioning scheme we use is similar to that of Heller and
Ghahramani (2005). The product of experts scheme used in this paper contrasts with
the more conventional sum of experts mixture model of Heller and Ghahramani (2005)
that would have resulted in the mixture π p(Di) + (1 − π) p(Dl|Tl)p(Dr|Tr), where π
denotes the mixing proportion.

Finally, given the collection of trees T = {T1, T2, . . . , Tn}, the total likelihood of data

in all trees is deﬁned as follows:

(5)

(6)

(7)

(8)

(9)

Trees are generated from a DP. Let T = {T1, T2, . . . , Tn} be a set of trees. Ti is

sampled from a DP as follows:

p(D|T) =

p(Di|Ti)

|T|
(cid:89)

i

F ∼ DP(α, U)

Ti ∼ F

p(Tk|T, α, U) =

Nk
N + α

p(T|T|+1|T, α, U) =

α/k
N + α

where α denotes the concentration parameter of the DP. U is a uniform base distribution
that assigns equal probability to each tree. We integrate out F, which is a distribution
over trees, instead of estimating it. Hence, the conditional probability of sampling an
existing tree is computed as follows:

where Nk denotes the number of words in Tk and N denotes the total number of words
in the model. A new tree is generated with the following:

3.2 Modeling Morphology with Probabilistic Hierarchical Clustering

In our model, data points are the words and each tree node corresponds to a morpho-
logical paradigm (see Figure 4). Each word is part of all the paradigms on the path from
the leaf node having that word until the root node. The word can share either its stem or
sufﬁx with other words in the same paradigm. Hence, a considerable number of words
can be generated through this approach that may not be seen in the corpus. Our model
will prefer words that share stems or sufﬁxes to be close to each other within the tree.

354

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 4
A sample hierarchical tree structure that illustrates the clusters in each node (i.e., paradigms).
Each node corresponds to a cluster (i.e., morphological paradigm) and the leaf nodes correspond
to input data. The ﬁgure shows the ideal forest of trees that one expects given the input data.

The plate diagram of the generative model is given in Figure 5(a). Given a child
i ) and a

node i, we deﬁne a Dirichlet process to generate stems (denoted by s1
i , . . . , mNi
separate Dirichlet process to generate sufﬁxes (denoted by m1
i ):

i , . . . , sNi

Gs

i ∼ DP(βs, Ps)
sj
i ∼ Gs
i
i ∼ DP(βm, Pm)
Gm
mj
i ∼ Gm
i
(cid:89)

p(c)

Ps(sj

i) =

Pm(mj

i) =

p(c)

c∈sj
i
(cid:89)

c∈mj
i

where DP(βs, Ps) is a Dirichlet process that generates stems, βs denotes the concentra-
i is a distribution over the stems sj
tion parameter, and Ps is the base distribution. Gs
i
in node i. Correspondingly, DP(βm, Pm) is a Dirichlet process that generates sufﬁxes
with analogous parameters. Gm
i in node i. For
i
smaller values of the concentration parameter, it is less likely to generate new types.
Thus, sparse multinomials can be generated by the Dirichlet process yielding a skewed
distribution. We set βs < 1 and βm < 1 in order to generate a small number of stem and
sufﬁx types. sj

i are the jth stem and sufﬁx instance in the ith node, respectively.

is a distribution over the sufﬁxes mj

i and mj

The base distributions for stems and sufﬁxes are given by Equation (14) and Equa-
tion (15). Here, c denotes a single letter or character. We assume that letters are dis-
tributed uniformly (Creutz and Lagus 2005b), where p(c) = 1/A for an alphabet having
A letters. Our model will favor shorter morphemes because they have less factors in the
joint probability given by Equations (14) and (15).

For the root nodes we use HDPs that share global DPs (denoted by Hs for stems
and Hm for sufﬁxes). These introduce dependencies between stems/sufﬁxes in distinct

(10)

(11)

(12)

(13)

(14)

(15)

355

Computational Linguistics

Volume 44, Number 2

Figure 5
The DP model for the child nodes (on the left) illustrates the generation of words talking, cooked,
yelling. Each child node maintains its own DP that is independent of DPs from other nodes. The
HDP model for the root nodes (on the right) illustrates the generation of words yelling, talking,
repairs. In contrast to the DPs in the child nodes, in the HDP model, the stems/sufﬁxes are
shared across all root HDPs.

trees. The model will favor stems and sufﬁxes that are already generated in one of the
trees. The HDP for a root node i is deﬁned as follows:

Fs
i ∼ DP(βs, Hs)
Hs ∼ DP(αs, Ps)
sj
i ∼ Fs
i
ψz
i ∼ Hs
Fm
i ∼ DP(βm, Hm)
Hm ∼ DP(αm, Pm)
mj
i ∼ Fm
i
i ∼ Hm
φz

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

where the base distributions Hs and Hm are drawn from the global DPs DP(αs, Ps) and
DP(αm, Pm). Here, ψz
i denotes the sufﬁx type z
in node i, which are drawn from Hs and Hm (i.e., the global DPs), respectively. The plate
diagram of the HDP model is given in Figure 5(b).

i denotes the stem type z in node i, and φz

From the Chinese restaurant process (CRP) metaphor perspective, the global DPs
generate the global sets of dishes (i.e., stems and sufﬁxes) that constitute the menu for
all trees in the model. At each tree node, there are two restaurants: one for stems and
one for sufﬁxes. At each table, a different type of dish (i.e., stem or sufﬁx type) is served

356

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 6
A depiction of the Chinese restaurant franchise (i.e., global vs. local CRPs).
S1 = {walk, order, sleep, etc.}, M1 = {s, ing}, S2 = {pen, book}, M2 = {0, s}, S3 = {walk, order},
M3 = {0, s}, where 0 denotes an empty sufﬁx. For each stem type in the distinct trees, a customer
is inserted in the global restaurant. For example, there are two stem customers that are being
served the stem type walk because walk exists in two different trees.

i or φz

and for each stem/sufﬁx type there exists only one table in each node (i.e., restaurant).
Customers are the stem or sufﬁx tokens. Whenever a new customer, sj
i, enters a
restaurant, if the table, ψz
i , serving that dish already exists, the new customer sits
at that table. Otherwise, a new table is generated in the restaurant. A change in one of
the restaurants in the leaf nodes leads to the update in each restaurant all the way to
the root node. If the dish is not available in the root node, a new table is created for that
root node and a global customer is also added to the global restaurant. If no global table
exists for that dish, a new global table serving the dish is also created. This can be seen
as a Chinese restaurant franchise where each node is a restaurant itself (see Figure 6).

i or mj

In order to calculate the joint likelihood of the model, we need to consider both trees
and global stem/sufﬁx sets (i.e., local and global restaurants). The model is exchange-
able because it is a CRP—which means that the order the words are segmented does not
alter the joint probability. The joint likelihood of the entire model for a given collection
of trees T = {T1, T2, . . . , Tn} is computed as follows:

p(D|T) =

p(Di|Ti)

|T|
(cid:89)

|T|
(cid:89)

i

i

=

p(Si|Ti)p(Mi|Ti)

= p(Sroot)p(Mroot)

p(Si|Ti)p(Mi|Ti)

|T|
(cid:89)

i=1
i(cid:54)=root

(24)

(25)

(26)

357

Computational Linguistics

Volume 44, Number 2

where p(Si|Ti) and p(Mi|Ti) are computed recursively:

(cid:26) 1

p(Si|Ti) =

Z p(Si)p(Sl|Tl)p(Sr|Tr) if i is an internal node with child nodes l and r
p(Si) if i is a leaf node

where Z is the normalization constant. The same also applies for Mi.

Following the CRP, the joint probability of the stems in each root node Ti, Srooti
i , . . . , sNi

i }, is:

{s1

i , s2

p(Sroot) =

p(s1

i , s2

i , . . . , sNi
i )

|T|
(cid:89)

i

|T|
(cid:89)

i






=

p(s1

i )p(s2

i |s1

i ) . . . p(sNi

i

i , . . . , sNi−1
|s1

i

)

=



|T|
(cid:89)





i

Γ(βs)
Γ(Ni + βs)

i −1

βLs
s





(ns

ij − 1)!





Γ((cid:80)

Γ(αs)
sj∈Ks ks

j + αs)

αKs−1
s



(ks

j − 1)!Ps(sj)



Ls
i(cid:89)

j=1

Ks
(cid:89)

j=1

(27)

=

(28)

(29)

(30)

where the ﬁrst line in Equation (30) corresponds to the stem CRPs in the root nodes
of the trees (see Equation (2.22) in Can [2011] and Equation (A.1) in Appendix A). The
second line in Equation (30) corresponds to the global CRP of stems. The second factor in
the ﬁrst line of Equation (30) corresponds to the case where Ls
i stem types are generated
the ﬁrst time, and the third factor in the ﬁrst line corresponds to the case, where for each
of the Ls
ij stem tokens of type j. The ﬁrst factor accounts
for all denominators from both cases. Similarly, the second and the fourth factor in the
second line of Equation (30) corresponds to the case where Ks stem types are generated
globally (i.e., stem tables in the global restaurant), the third factor corresponds to the
case where, for each of the Ks stem types, there are ks
j stems of type j in distinct trees.
A sample hierarchical structure is given in Figure 7.

i stem types at node i, there are ns

The joint probability of stems in a child node Ti, Si = {s1

i , s2

i , . . . , sNi

i }, which belong

to stem tables {ψ1

i , ψ2

i , . . . } that index the items on the global menu is reduced to:

p(Si) =

Γ(βs)
Γ(Ni + βs)

i −1

βLs
s

(ns

ij − 1)!Ps(ψj
i)

(cid:17)

Ls
i(cid:89)

(cid:16)

j=1

(31)

Whenever a new stem is added to a node i (i.e., new customer enters one of the
i that belongs to type z

local restaurants), the conditional probability of the new stem sj

358

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 7
A sample hierarchical structure that contains D = {walk + ed, talk + ed, order + ed, walk + ing,
talk + ing} and the corresponding global tables. Here s1
node k with frequency 1.

k = walk/1 denotes the ﬁrst stem walk in

(i.e., customer sj
2006):

i sitting at table ψz

i ) is computed as follows (see Teh 2010 and Teh et al.

p(s j

i = z|Si

−s j

i , βs, Ps) =

otherwise (for an internal node)

(32)

if ψz

i ∈ Ψi






n

−s j
i
iz
Ni − 1 + βs
βsPs(s j
i)
Ni − 1 + βs
βsHs(ψz

i |αs, Ps)

Ni − 1 + βs

otherwise (for a root node)

where Ψi denotes the table indicators in node i, n
that belong to type z in node i when the last instance sj

i is excluded.

−sj
i
iz denotes the number of stem tokens

If the stem (customer) does not exist in the root node (i.e., chooses a non-existing

dish type in the root node), the new stem’s probability is calculated as follows:

Hs(ψz

i |αs, Ps) =

j + αs





(cid:80)

(cid:80)

ks
z
sj∈Ks ks
αsPs(sz)
sj∈Ks ks

j + αs

if j ∈ Ks

otherwise

If the stem type (dish) exists in the global clusters (i.e., global menu), it is chosen with the
probability proportional to the number of trees that contain the stem type (i.e., number
of global stem customers that are served the dish). Otherwise, the new stem (dish type)
is chosen based on the base distribution Ps.

(33)

359

Computational Linguistics

Volume 44, Number 2

Analogously to Equations (30)–(33) that apply to stems, the corresponding equa-

tions for sufﬁxes are given by Equations (34)–(37).




p(Mroot) =





Γ(βm)
Γ(Ni + βm)

βLm
i −1
m





(nm

ij − 1)!





Lm
i(cid:89)

j=1

|T|
(cid:89)

i





Γ((cid:80)

Γ(αm)
mj∈Km km

j + αm)

αKm−1
m

Km
(cid:89)

j=1



(km

j − 1)!Pm(mj)



p(Mi) =

Γ(βm)
Γ(Ni + βm)

βLm
i −1
m

(nm

ij − 1)!Pm(φj
i)

(cid:17)

Lm
i(cid:89)

(cid:16)

j=1

p(mj

i = z|Mi

−mj

i , βm, Pm) =

if φz

i ∈ Φi

n

−mj
i
iz
Ni − 1 + βm
βmPm(mj
i)
Ni − 1 + βm
βmHm(φz






i |αm, Pm)

Ni − 1 + βm

otherwise (for an internal node)

otherwise (for a root node)

Hm(φz

i |αm, Pm) =





(cid:80)

(cid:80)

km
z
mj∈Km km
αmPm(mz)
mj∈Km km

j + αm

j + αm

if j ∈ Km

otherwise

(34)

(35)

(36)

(37)

i , . . . , mNi

i , m1
i , . . . }; Nm
i

where Mi = {m1
types {φ1
tokens of type j in node i; Km is the total number of sufﬁx types; and km
j
trees that contain sufﬁxes of type j.

i } is the set of sufﬁxes in node i belonging to global sufﬁx
ij is the number of sufﬁx
is the number of

is the number of local sufﬁx types; nm

i , φ2

3.3 Metropolis-Hastings Sampling for Inferring Trees

Trees are learned along with the segmentations of words via inference. Learning trees
involves two steps: 1) constructing initial trees; 2) Metropolis-Hastings sampling for
inferring trees.

3.3.1 Constructing Initial Trees. Initially, all words are split at random points with uniform
probability. We use an approximation to construct the intial trees. Instead of computing
the full likelihood of the model, for each tree, we only compute the likelihood of a single
DP and assume that all words belong to this DP. The conditional probability of inserting
word wj = s + m in tree Tk is given by:

p(Tk, w j = s + m|D, T, α, βs, βm, Ps, Pm, U)
= p(Tk|T, α, U)p(wj = s + m|Dk)
= p(Tk|T, α, U)p(s|Sk, βs, Ps)p(m|Mk, βm, Pm)

(38)

360

Can and Manandhar

Tree Structured Morphological Segmentation

We use Equation (8) and (9) for computing the conditional probability p(Tk|T, α, U) of
choosing a particular tree. Once a tree is chosen, a branch to insert is selected at random.
The algorithm for constructing the initial trees is given in Algorithm 1.

3.3.2 Metropolis-Hastings Sampling. Once the initial trees are constructed, the hierar-
chical structures yielding a global maximum likelihood are inferred by using the
Metropolis Hastings algorithm (Hastings 1970). The inference is performed by itera-
tively removing a word from a leaf node from a tree and subsequently sampling a new
tree, a position within the tree, and a new segmentation (see Algorithm 2). Trees are
sampled with the conditional probability given in Equations (8) and (9). Hence, trees
with more words attract more words, and new trees are created proportional to the
hyperparameter α.

Once a tree is sampled, we draw a new position and a new segmentation. The word
is inserted at the sampled position with the sampled segmentation. The new model is
either accepted or rejected with the Metropolis-Hastings accept-reject criteria. We also
use a simulated annealing cooling schedule by assigning an initial temperature γ to the
system and decreasing the temperature at each iteration. The accept probability we use
is given by:

PAcc =

(cid:19) 1
γ

(cid:18) pnext(D|T)
pcur(D|T)

(39)

where pnext(D|T) denotes the likelihood of the data under the altered model and
pcur(D|T) denotes the likelihood of data under the current model before sampling. The
normalization constant cancels out because it is the same for the numerator and the
1
denominator. If pnext(D|T)
γ , then the new sample is accepted: otherwise,

γ > pcur(D|T)

1

Algorithm 1 Construction of the initial trees.

1: input: data D := {w1 = s1 + m1, . . . , wN = sN + mN}
2: initialize: The trees in the model: T := ∅
3: For j = 1 . . . N
4: Choose wj from the corpus.
5: Sample, Tk as a new empty tree or existing tree from T, and, a split point wj = s + m

from the joint distribution (see Equation 38):
p(Tk, wj = s + m|D, T, α, βs, βm, Ps, Pm, U)

6: if Tk ∈ T then
7: Draw a node s from Tk with uniform probability
8: Assign Tk as a sibling of Ts: Ts := Ts + Tk (i.e., Ts is new tree with Tk and old Ts

as children)

Tk is an empty tree. Add Tk into T: T := T ∪ Tk

9: else
10:
11: Dk := {wj = s + m} (i.e., word wj = s + m is assigned into Tk)
12: end if
13: Remove wj from the corpus
14: End For
15: output: T

361

Computational Linguistics

Volume 44, Number 2

Algorithm 2 Learning the trees with Metropolis-Hastings algorithm

1: input: data D := {w1 = s1 + m1, . . . , wN = sN + mN}, initial trees T, initial

temperature γ, the target temperature κ, temperature decrement η

Choose the leaf node j from all trees with uniform probability
Let Dj := {wj = sold + mold}

2: initialize: pcur(D|T) := p(D|T)
3: while γ > κ do
4:
5:
6: Draw a split point wj = snew + mnew with uniform probability
7: Draw a tree Tk with probability p(Tk|T, α, U) (see Equations 8 and 9)
8:

if Tk ∈ T then

Draw a sibling node s from Tk with uniform probability
Ts := Ts + Tk (see Figure 8)

9:
10:
11:
12:
13:
14:
15:

16:
17:

18:

else

Create a new tree Tk
T := T ∪ Tk
Dk := {wj = snew + mnew}

end if
rand ∼ Normal(0, 1)
pnext := p(D|T) (see Equation 24)

if pnext(D|T) >= pcur(D|T) or rand <
Accept the new tree structure
pcur(D|T) := pnext(D|T)

19:
20:
21:
22:
23: end while
24: output: T

end if
γ := γ − η

(cid:16) pnext(D|T)
pcur(D|T)

(cid:17) 1
γ

then

the new model is still accepted with a probability pAcc. The system is cooled in each
iteration with decrements η. We refer to Section 4 for details of parameter settings.

γ ← γ − η

(40)

3.4 Morphological Segmentation

Once the model is learned, it can be used for the segmentation of novel words. We
use only root nodes for the segmentation. Viterbi decoding is used in order to ﬁnd the
morphological segmentation of each word having the maximum probability:

arg

max
s1,··· ,sa,m1,...,mb

p(wk = s1, · · · , sa, m1, · · · , mb|D, αs, βs, Hs, Ps, αm, βm, Hm, Pm)

a
(cid:89)

|T|
(cid:88)

=

j=1

b
(cid:89)

i=1
i=root
|T|
(cid:88)

j=1

i=1
i=root

p(sj

i|Si, αs, βs, Hs, Ps)

p(mj

i|Mi, αm, βm, Hm, Pm)

(41)

where wk denotes the kth word to be segmented in the test set.

362

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 8
A sampling step in Metropolis-Hastings algorithm.

3.5 Example Paradigms

A sample of root paradigms learned by our model for English is given in Table 1. The
model can ﬁnd similar word forms (i.e., separat+ists, medal+ists, hygien+ists) that are
grouped in the neighbor branches in the tree structure (see Figures 9, B.1, and B.2 for
sample paradigms learned in English, Turkish, and German).

Paradigms are captured based on the similarity of either stems or sufﬁxes. Having
the same stem such as co-chair (co-chairman, co-chairmen) or trades (trades+man,
trades+men) allows us to ﬁnd segmentations such as co-chair+man vs. co-chair+men
and trades+man vs. trades+men. Although we assume a stem+sufﬁx segmentation,
other types of segmentation, such as preﬁx+stem, are also covered. However, stem
alterations and inﬁxation are not covered in our model.

4. Evaluation

We used publicly available Morpho Challenge data sets for English, German, and
Turkish for training. The English data set consists of 878,034 words, the German data set
consists of 2,338,323 words, and the Turkish data set consists of 617,298 words. Although
frequency of each word was available in the training set, we did not make use of this

363

Computational Linguistics

Volume 44, Number 2

Table 1
Example paradigms in English.

{ﬁnal, ungod, frequent, pensive} {ly}

{kind, kind, kind} {est, er, 0}

{underrepresent, modul} {ation}

{plebe, hawai, muslim-croat} {ian}

{compuls, shredd, compuls, shredd, compuls} {ion, er, ively, ers, ory}

{zion, modern, modern, dynam} {ism, ists}

{reclaim, chas, pleas, fell} {ing}

{mov, engrav, stray, engrav, fantasiz, reischau,
decilit, suspect} {ing, er, e}

{measur, measur, incontest, transport, unplay, reput}
{e, able}

{housewar, decorat, entitl, cuss, decorat, entitl, materi, toss, ﬂay, unconﬁrm
linse, equipp} {es, ing, alise, ed}

{fair, norw, soon, smooth, narrow, sadd, steep, noisi, statesw, narrow}
{est, ing}

{rest, wit, name, top, odor, bay, odor, sleep} {less, s}

{umpir, absorb, regard, embellish, freez, gnash} {ing}

{nutrition, manicur, separat, medal, hygien, nutrition, genetic, preservation}
{0, ists}

information. In other words, we use only the word types (not tokens) in training. We do
not address the ambiguity of words in this work and leave this as future research.

In all experiments, the initial temperature of the system is set γ = 2 and it is
reduced to γ = 0.01 with decrements η = 0.0001 (see Equation (39)). Figure 10 shows
the time required for the log likelihoods of the trees of sizes 10K, 16K, and 22K to
converge. We ﬁxed αs = αm = βs = βm = 0.01 and α = 0.0005 in all our experiments.
The hyperparameters are set manually as a result of several experiments. These are the
optimum values obtained from a number of experiments.2

Precision, recall, and F-score values against training set sizes are given in Figures 11

and 12 for English and Turkish, respectively.

4.1 Morpho Challenge Evaluation

Although we experimented with different sizes of training sets, we used a randomly
chosen 600K words from the English and 200K words from the Turkish and German
data sets for evaluation purposes.

Evaluation is performed according to the method proposed in Morpho Challenge
(Kurimo et al. 2010a), which in turn is based on evaluation used by Creutz and Lagus
(2007). The gold standard evaluation data set utilized within the Morpho Challenge is
a hidden set that is not available publicly. This makes the Morpho Challenge evaluation
different from other evaluations that provide test data. In this evaluation, word pairs

2 The source code of the model is accessible at: https://github.com/burcu-can/TreeStructuredDP.

364

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 9
Sample hierarchies in English, Turkish, and German.

sharing at least one common morpheme are sampled. For precision, word pairs are
sampled from the system results and checked against the gold standard segmentations.
For recall, word pairs are sampled from the gold standard and checked against the
system results. For each matching morpheme, 1 point is given. Precision and recall are
calculated by normalizing the total obtained scores.

We compare our results with other unsupervised systems from Morpho Challenge
2010 (Kurimo et al. 2010b) for English, German, and Turkish. More speciﬁcally, we com-
pare our model with all competing unsupervised systems: Morfessor Baseline (Creutz
and Lagus 2002), Morfessor CATMAP (Creutz and Lagus 2005a), Base Inference (Lignos
2010), Iterative Compounding (Lignos 2010), Aggressive Compounding (Lignos 2010),
and Nicolas, Farr´e, and Molinero (2010). Additionally, we compare our system with the
Morpho Chain model of Narasimhan, Barzilay, and Jaakkola (2015) by re-training their
model on exactly the same training sets as ours. All evaluation was carried out by the
Morpho Challenge organizers based on the hidden gold data sets.

365

Computational Linguistics

Volume 44, Number 2

Figure 10
The likelihood convergence in time (in minutes) for data sets of size 16K and 22K.

Figure 11
English results for different size of data.

English results are given in Table 2. For English, the Base Inference algorithm of
Lignos (2010) obtained the highest F-measure in Morpho Challenge 2010 among other
competing unsupervised systems. Our model is ranked fourth among all unsupervised
systems with a F-measure 60.27%.

German results are given in Table 3. Our model outperforms other unsupervised

models in Morpho Challenge 2010 with a F-measure 50.71%.

Turkish results are given in Table 4. Again, our model outperforms other unsuper-

vised participants, achieving a F-measure 56.41%.

Our model also outperforms the model from Can and Manandhar (2012) (which
we refer to as Single Tree Probabilistic Clustering), although the results are not directly
comparable because the largest data set we were able to train that model on was 22K
words due to the training time required. The full training set provided by Morpho
Challenge was not used in Can and Manandhar (2012). Our current approach is more
efﬁcient as the training cost is divided across multiple tree structures with each tree
being shallower compared with our previous model.

366

Can and Manandhar

Tree Structured Morphological Segmentation

Table 2
Morpho Challenge 2010 experimental results for English.

System

Precision (%) Recall (%)

F-measure (%)

Figure 12
Turkish results for different size of data.

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)
Base Inference (Lignos 2010)
Iterative Compounding (Lignos 2010)
Aggressive Compounding (Lignos 2010)
Nicolas (Nicolas, Farr´e, and Molinero 2010)
Morfessor Baseline (Creutz and Lagus 2002)
Morpho Chain (Narasimhan, Barzilay, and
Jaakkola 2015)
Morfessor CatMAP (Creutz and Lagus 2005a)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)
Base Inference (Lignos 2010)
Iterative Compounding (Lignos 2010)
Aggressive Compounding (Lignos 2010)
Morfessor Baseline (Creutz and Lagus 2002)
Morfessor CatMAP (Creutz and Lagus 2005a)

55.60
55.60

80.77
80.27
71.45
67.83
81.39
74.87

86.84

47.92
57.79

66.38
62.13
59.41
82.80
72.70

65.80
57.58

53.76
52.76
52.31
53.43
41.70
39.01

30.03

53.84
32.42

35.36
34.70
37.21
19.77
35.43

Table 3
Morpho Challenge 2010 experimental results for German.

System

Precision (%) Recall (%)

F-measure (%)

In order to draw a substantive empirical comparison, we performed another set
of experiments by running the current approach on only 22K words as the Single Tree
Probabilistic Clustering (Can and Manandhar 2012). Results are given in Tables 5, 6, and
7. As shown in the tables, the current model outperforms the previous model even on
the smaller data set.

60.27
57.33

64.55
63.67
60.40
59.78
55.14
50.42

44.63

50.71
41.54

46.14
44.53
45.76
31.92
47.64

367

Computational Linguistics

Volume 44, Number 2

Table 4
Morpho Challenge 2010 experimental results for Turkish.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)
Base Inference (Lignos 2010)
Iterative Compounding (Lignos 2010)
Aggressive Compounding (Lignos 2010)
Nicolas (Nicolas, Farr´e, and Molinero 2010)
Morfessor Baseline (Creutz and Lagus 2002)
Morpho Chain (Narasimhan, Barzilay, and
Jaakkola 2015)
Morfessor CatMAP (Creutz and Lagus 2005a)

57.70
72.36

72.81
68.69
55.51
79.02
89.68
69.25

79.38

55.18
25.81

16.11
21.44
34.36
19.78
17.78
31.51

31.88

56.41
38.04

26.38
32.68
42.45
31.64
29.67
43.32

45.49

Table 5
Comparison with Single Tree Probabilistic Clustering for English.

System

Precision (%) Recall (%) F-measure (%)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)

67.75
55.60

53.93
57.58

60.06
57.33

Table 6
Comparison with Single Tree Probabilistic Clustering for German.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)

33.93
57.79

65.31
32.42

44.66
41.54

Table 7
Comparison with Single Tree Probabilistic Clustering for Turkish.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)

64.39
72.36

42.99
25.81

51.56
38.04

4.2 Additional Evaluation

For additional experiments, we compare our model with Morpho Chain (Narasimhan,
Barzilay, and Jaakkola 2015) based on their evaluation method that differs from the
Morpho Challenge evaluation method. Their evaluation method is based on counting
the correct segmentation points. For example, if the result segmentation is booking+s
and the gold segmentation is book+ing+s, 1 point is counted. Precision and recall are

368

Can and Manandhar

Tree Structured Morphological Segmentation

Table 8
Comparison with Morpho Chain model for English based on Morpho Chain evaluation.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Morpho Chain

67.41
72.63

62.5
78.72

64.86
75.55

Table 9
Comparison with Morpho Chain model for Turkish based on Morpho Chain evaluation.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Morpho Chain

89.30
70.49

48.22
63.27

62.63
66.66

calculated based on these matching segmentation points. In addition, this evaluation
does not use the hidden gold data sets. Instead, the test sets are created by aggregating
the test data from Morpho Challenge 2005 and Morpho Challenge 2010 (as reported in
Narasimhan, Barzilay, and Jaakkola [2015]) that provide segmentation points.3

We used the same trained models as in our Morpho Challenge evaluation. The

English test set contains 2,218 words and the Turkish test set contains 2,534 words.4
The English results are given in Table 8 and Turkish results are given in Table 9.
For all systems, Morpho Chain evaluation scores are comparably higher than the
Morpho Challenge scores. There are several reasons for this. In the Morpho Challenge
evaluation, the morpheme labels are considered rather than the surface forms of the
morphemes. For example, pantolon+u+yla [with his trousers] and emel+ler+i+yle [with
his desires] have got both possessive morpheme (u and i) that is labeled with POS and
relational morpheme (yla and yle) labeled with REL in common. This increases the total
number of points that is computed over all word pairs, and therefore lowers the scores.
Secondly, in the Morpho Chain evaluation, only the gold segmentation that has the
maximum match with the result segmentation is chosen for each word (e.g., yazımıza
has two gold segmentations: yaz+ı+mız+a [to our summer] and yazı+mız+a; [to our
writing]). In contrast, in the Morpho Challenge evaluation all segmentations in the
gold segmentation are evaluated. This is another factor that increases the scores in
Morpho Chain evaluation. Thus, the Morpho Chain evaluation favors precision over
recall. Indeed, in the Morpho Challenge evaluation, the Morpho Chain system has high
precision but their model suffers from low recall due to undersegmentation (see Tables 2
and 4).

It should be noted that the output of our system is not only the segmentation points,
but also the hierarchical organization of morphological paradigms that we believe is
novel in this work. However, because of the difﬁculty in measuring the quality of hier-
archical paradigms, which will require a corresponding hierarchically organized gold
data set, we are unable to provide an objective measure of the quality of hierarchical

3 In addition to the hidden test data, Morpho Challenge also provides separate publicly available test data.
4 Because of the unavailability of word2vec word embeddings for German, we were unable to perform

Morpho Chain evaluation on this language.

369

Computational Linguistics

Volume 44, Number 2

structures learned. We present different portions from the obtained trees in Appendix B
(see Figures B.1 and B.2).5 It can be seen that words sharing the same sufﬁxes are gath-
ered closer to each other, such as reestablish+ed, reclassiﬁ+ed, circl+ed, uncloth+ed, and so
forth. Secondly, related morphological families gather closer to each other, such as
impress+ively, impress+ionist, impress+ions, impress+ion.

5. Conclusions and Future Work

In this article, we introduce a tree structured Dirichlet process model for hierarchical
morphological segmentation. The method is different compared with existing hierar-
chical and non-hierarchical methods for learning paradigms. Our model learns mor-
phological paradigms that are clustered hierarchically within a forest of trees.

Although our goal in this work is on hierarchical learning, our model shows com-
petitive performance against other unsupervised morphological segmentation systems
that are designed primarily for segmentation only. The system outperforms other un-
supervised systems in Morpho Challenge 2010 for German and Turkish. It also out-
performs the more recent Morpho Chain (Narasimhan, Barzilay, and Jaakkola 2015)
system on the Morpho Challenge evaluation for German and Turkish.

The sample paradigms learned show that these can be linked to other types of latent
information, such as part-of-speech tags. Combining morphology and syntax as a joint
learning problem within the same model can be a fruitful direction for future work.

The hierarchical structure is beneﬁcial because we can have both compact and more
general paradigms at the same time. In this article, we use the paradigms only for the
segmentation task, and applications of hierarchy learned is left as future work.

Appendix A. Derivation of the Full Joint Distribution

Let D = {s1, s2, . . . , sN} denote the input data where each sj is a data item. A particular
setting of a table with N customers has a joint probability of:

p(s1, . . . , sN|α, P) = p(s1|α, P)p(s2|s1, α, P)p(s3|s1, s2, α, P) . . . p(sN|s1, . . . , sN−1, α, P)

=

αL
α(1 + α)(2 + α)(N − 1 + α)

P(si)

(nsi − 1)!

L
(cid:89)

i=1

=

Γ(α)
Γ(N + α)

αL−1

L
(cid:89)

i=1

(nsi − 1)!

P(si)

L
(cid:89)

i=1

L
(cid:89)

i=1

(A.1)

For each customer, either a new table is created or the customer sits at an occupied table.
For each table, at least one table creation is performed, which forms the second and
the fourth factor in the last equation. Once the table is created, factors that represent the
number of customers sitting at each table are chosen accordingly, which refers to the
third factor in the last equation. All factors with α are aggregated in the ﬁrst factor in
terms of a Gamma function in the last equation.

5 Some of the full trees are given at http://web.cs.hacettepe.edu.tr/~burcucan/TreeStructuredDP.

370

Can and Manandhar

Tree Structured Morphological Segmentation

Appendix B. Sample Tree Structures

Figure B.1
Sample hierarchies in English.

371

Computational Linguistics

Volume 44, Number 2

Figure B.2
Sample hierarchies in Turkish.

Acknowledgments
This research was supported by TUBITAK
(The Scientiﬁc and Technological Research
Council of Turkey) grant number 115E464.
We thank Karthik Narasimhan for providing
their data sets and code. We are grateful to
Sami Virpioja for the evaluation of our
results on the hidden gold data provided by
Morpho Challenge. We thank our reviewers
for critical feedback and spotting an error in
our previous version of the article. Their
comments have immensely helped improve
the article.

References
Can, Burcu. 2011. Statistical Models for

Unsupervised Learning of Morphology and
POS tagging. Ph.D. thesis, Department of

Computer Science, The University
of York.

Can, Burcu and Suresh Manandhar. 2010.
Clustering morphological paradigms
using syntactic categories. In Multilingual
Information Access Evaluation I. Text
Retrieval Experiments: 10th Workshop of the
Cross-Language Evaluation Forum, CLEF
2009, Corfu, Greece, September 30 -
October 2, 2009, Revised Selected Papers,
pages 641–648, Corfu.

Can, Burcu and Suresh Manandhar. 2012.
Probabilistic hierarchical clustering of
morphological paradigms. In Proceedings of
the 13th Conference of the European Chapter of
the Association for Computational Linguistics,
EACL ’12, pages 654–663, Avignon.
Chan, Erwin. 2006. Learning probabilistic
paradigms for morphology in a latent
class model. In Proceedings of the Eighth

372

Can and Manandhar

Tree Structured Morphological Segmentation

Meeting of the ACL Special Interest
Group on Computational Phonology and
Morphology, SIGPHON ’06, pages 69–78,
New York, NY.

Creutz, Mathias and Krista Lagus. 2002.

Unsupervised discovery of morphemes.
In Proceedings of the ACL-02 Workshop on
Morphological and Phonological Learning -
Volume 6, MPL ’02, pages 21–30,
Philadelphia, PA.

Creutz, Mathias and Krista Lagus. 2005a.

Inducing the morphological lexicon of a
natural language from unannotated text.
In Proceedings of the International and
Interdisciplinary Conference on Adaptive
Knowledge Representation and Reasoning
(AKRR 2005), pages 106–113, Espoo.
Creutz, Mathias and Krista Lagus. 2005b.
Unsupervised morpheme segmentation
and morphology induction from text
corpora using Morfessor 1.0. Technical
Report A81. Helsinki University of
Technology.

Creutz, Mathias and Krista Lagus. 2007.
Unsupervised models for morpheme
segmentation and morphology learning.
ACM Transactions Speech Language
Processing, 43:1–3:34.

Dreyer, Markus and Jason Eisner. 2011.

Discovering morphological paradigms
from plain text using a Dirichlet Process
mixture model. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 616–627,
Edinburgh.

Goldsmith, John. 2001. Unsupervised

learning of the morphology of a natural
language. Computational Linguistics,
27(2):153–198.

Hastings, W. K. 1970. Monte Carlo sampling
methods using Markov chains and their
applications. Biometrika, 57:97–109.

Heller, Katherine A. and Zoubin

Ghahramani. 2005. Bayesian hierarchical
clustering. In Proceedings of the 22nd
International Conference on Machine
Learning, ICML ’05, pages 297–304,
Bonn.

Hinton, Geoff. 2002. Training products of

experts by minimizing contrastive
divergence. Neural Computation,
14(8):1771–1800.

Kurimo, Mikko, Sami Virpioja, Ville

Turunen, and Krista Lagus. 2010a. Morpho
Challenge Competition 2005–2010:
Evaluations and results. In Proceedings of
the 11th Meeting of the ACL Special Interest
Group on Computational Morphology and
Phonology, SIGMORPHON ’10,
pages 87–95, Uppsala.

Kurimo, Mikko, Sami Virpioja, Ville T.

Turunen, Graeme W. Blackwood, and
William Byrne. 2009. Overview and results
of Morpho Challenge 2009. In Proceedings
of the 10th Cross-Language Evaluation Forum
Conference on Multilingual Information Access
Evaluation: Text Retrieval Experiments,
CLEF’09, pages 578–597, Corfu.
Kurimo, Mikko, Sami Virpioja, Ville T.
Turunen, Bruno G ´olenia, Sebastian
Spiegler, Oliver Ray, Peter Flach, Oskar
Kohonen, Laura Lepp¨anen, Krista Lagus,
Constantine Lignos, Lionel Nicolas,
Jacques Farr´e, and Miguel Molinero.
2010b. Proceedings of the Morpho
Challenge 2010 Workshop. Technical
Report TKK-ICS-R37. Aalto University.
Lignos, Constantine. 2010. Learning from

unseen data. In Proceedings of the Morpho
Challenge 2010 Workshop, pages 35–38,
Espoo.

Luo, Jiaming, Karthik Narasimhan, and
Regina Barzilay. 2017. Unsupervised
learning of morphological forests.
Transactions of the Association of
Computational Linguistics, 5:353–364.
Mikolov, Tomas, Kai Chen, Greg Corrado,

and Jeffrey Dean. 2013. Efﬁcient estimation
of word representations in vector space.
CoRR, abs/1301.3781.

Monson, Christian. 2008. Paramor: From
Paradigm Structure to Natural Language
Morphology Induction. Ph.D. thesis,
Language Technologies Institute, School of
Computer Science, Carnegie Mellon
University.

Monson, Christian, Jaime Carbonell, Alon
Lavie, and Lori Levin. 2008. Paramor:
Finding paradigms across morphology.
In Lecture Notes in Computer Science -
Advances in Multilingual and Multimodal
Information Retrieval; Cross-Language
Evaluation Forum, CLEF 2007,
pages 900–907, Springer, Berlin.

Narasimhan, Karthik, Regina Barzilay, and

Tommi S. Jaakkola. 2015. An unsupervised
method for uncovering morphological
chains. Transactions of the Association for
Computational Linguistics, 3:157–167.

Nicolas, Lionel, Jacques Farr´e, and Miguel A.
Molinero. 2010. Unsupervised learning of
concatenative morphology based on
frequency-related form occurrence. In
Proceedings of the Morpho Challenge 2010
Workshop, pages 39–43, Espoo.

Poon, Hoifung, Colin Cherry, and Kristina

Toutanova. 2009. Unsupervised
morphological segmentation with
log-linear models. In Proceedings of Human
Language Technologies: The 2009 Annual

373

Computational Linguistics

Volume 44, Number 2

Conference of the North American
Chapter of the Association for Computational
Linguistics, NAACL ’09, pages 209–217,
Boulder, CO.

Snover, Matthew G., Gaja E. Jarosz, and
Michael R. Brent. 2002. Unsupervised
learning of morphology using a novel
directed search algorithm: Taking the ﬁrst
step. In Proceedings of the ACL-02 Workshop
on Morphological and Phonological Learning,
pages 11–20, Philadelphia, PA.

Snyder, Benjamin and Regina Barzilay. 2008.
Unsupervised multilingual learning for
morphological segmentation. In
Proceedings of ACL-08: HLT, pages 737–745,
Columbus, OH.

Teh, Y. W. 2010. In Dirichlet processes, In

Encyclopedia of Machine Learning, Springer.
Teh, Y. W., M. I. Jordan, M. J. Beal, and D. M.
Blei. 2006. Hierarchical Dirichlet Processes.
Journal of the American Statistical
Association, 101(476):1566–1581.

374

Tree Structured Dirichlet Processes for
Hierarchical Morphological Segmentation

Burcu Can
Hacettepe University
Department of Computer Engineering
burcucan@cs.hacettepe.edu.tr

Suresh Manandhar
University of York
Department of Computer Science
suresh@cs.york.ac.uk

This article presents a probabilistic hierarchical clustering model for morphological segmenta-
tion. In contrast to existing approaches to morphology learning, our method allows learning
hierarchical organization of word morphology as a collection of tree structured paradigms. The
model is fully unsupervised and based on the hierarchical Dirichlet process. Tree hierarchies are
learned along with the corresponding morphological paradigms simultaneously. Our model is
evaluated on Morpho Challenge and shows competitive performance when compared to state-
of-the-art unsupervised morphological segmentation systems. Although we apply this model for
morphological segmentation, the model itself can also be used for hierarchical clustering of other
types of data.

1. Introduction

Unsupervised learning of morphology has been an important task because of the bene-
ﬁts it provides to many other natural language processing applications such as machine
translation, information retrieval, question answering, and so forth. Morphological
paradigms provide a natural way to capture the internal morphological structure of
a group of morphologically related words. Following Goldsmith (2001) and Monson
(2008), we use the term paradigm as consisting of a set of stems and a set of sufﬁxes
where each combination of a stem and a sufﬁx leads to a valid word form, for example,
{walk,talk,order,yawn}{s,ed,ing} generating the surface forms walk+ed, walk+s, walk+ing,
talk+ed, talk+s, talk+ing, order+s, order+ed, order+ing, yawn+ed, yawn+s, yawn+ing. A sam-
ple paradigm is given in Figure 1.

Recently, we introduced a probabilistic hierarchical clustering model for learning
hierarchical morphological paradigms (Can and Manandhar 2012). Each node in the
hierarchical tree corresponds to a morphological paradigm and each leaf node consists
of a word. A single tree is learned, where different branches on the hierarchical tree

Submission received: 29 June 2016; revised version received: 30 July 2017; accepted for publication: 1 March
2018.

doi:10.1162/COLI a 00318

© 2018 Association for Computational Linguistics
Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International
(CC BY-NC-ND 4.0) license

Computational Linguistics

Volume 44, Number 2

Figure 1
An example paradigm.

correspond to different morphological forms. Well-deﬁned paradigms in the lower
levels of trees were learned. However, merging of the paradigms at the upper levels
led to undersegmentation in that model. This problem led us to search for ways to
learn multiple trees. In our current approach, we learn a forest of paradigms spread
over several hierarchical trees. Our evaluation on Morpho Challenge data sets provides
better results when compared to the previous method (Can and Manandhar 2012). Our
results are comparable to current state-of-the-art results while having the additional
beneﬁt of inferring the hierarchical structure of morphemes for which no comparable
systems exist.

The article is organized as follows: Section 2 introduces the related work in the ﬁeld.
Section 3 describes the probabilistic hierarchical clustering model with its mathematical
model deﬁnition and how it is applied for morphological segmentation; the same
section explains the inference and the morphological segmentation. Section 4 presents
the experimental setting and the obtained evaluation scores from each experiment,
and Section 5 concludes and addresses the potential future work following the model
presented in this article.

2. Related Work

There have been many unsupervised approaches to morphology learning that focus
solely on segmentation (Creutz and Lagus 2005a, 2007; Snyder and Barzilay 2008; Poon,
Cherry, and Toutanova 2009; Narasimhan, Barzilay, and Jaakkola 2015). Others, such
as Monson et al. (2008), Can and Manandhar (2010), Chan (2006), and Dreyer and Eisner
(2011), learn morphological paradigms that permit additional generalization.

A popular paradigmatic model is Linguistica (Goldsmith 2001), which uses the
Minimum Description Length principle to minimize the description length of a corpus
based on paradigm-like structures called signatures. A signature consists of a list of
sufﬁxes that are seen with a particular stem—for example, order-{ed, ing, s} denotes a
signature for the stem order.

Snover, Jarosz, and Brent (2002) propose a generative probabilistic model that
deﬁnes a probability distribution over different segmentations of the lexicon into
paradigms. Paradigms are learned with a directed search algorithm that examines

350

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 2
Examples of hierarchical morphological paradigms.

subsets of the lexicon, ranks them, and incrementally combines them in order to ﬁnd
the best segmentation of the lexicon. The proposed model addresses both inﬂectional
and derivational morphology in a language independent framework. However, their
model does not allow multiple sufﬁxation (e.g., having multiple sufﬁxes added to a
single stem) whereas Linguistica allows this.

Monson et al. (2008) induce morphological paradigms in a deterministic framework
named ParaMor. Their search algorithm begins with a set of candidate sufﬁxes and
collects candidate stems that attach to the sufﬁxes (see Figure 2(b)). The algorithm
gradually develops paradigms following search paths in a lattice-like structure. Proba-
bilistic ParaMor, involving a statistical natural language tagger to mimic ParaMor, was
introduced in Morpho Challenge 2009 (Kurimo et al. 2009). The system outperforms
other unsupervised morphological segmentation systems that competed in Morpho
Challenge 2009 (Kurimo et al. 2009) for the languages Finnish, Turkish, and German.

Can and Manandhar (2010) exploit syntactic categories to capture morphological
paradigms. In a deterministic scheme, morphological paradigms are learned by pairing
syntactic categories and identifying common sufﬁxes between them. The paradigms
compete to acquire more word pairs.

Chan (2006) describes a supervised procedure to ﬁnd morphological paradigms
within a hierarchical structure by applying latent Dirichlet allocation. Each paradigm
is placed in a node on the tree (see Figure 2(a)). The results show that each paradigm
corresponds to a part-of-speech such as adjective, noun, verb, or adverb. However, as
the method is supervised, the true sufﬁxes and segmentations are known in advance.
Learning hierarchical paradigms helps not only in learning morphological segmenta-
tion, but also in learning syntactic categories. This linguistic motivation led us toward
learning the hierarchical organization of morphological paradigms.

Dreyer and Eisner (2011) propose a Dirichlet process mixture model to learn
paradigms. The model uses 50–100 seed paradigms to infer the remaining paradigms,
which makes it semi-supervised. The model is similar to ours in the sense that it also
uses Dirichlet processes (DPs); however the model does not learn hierarchies between
the paradigms.

351

Computational Linguistics

Volume 44, Number 2

Luo, Narasimhan, and Barzilay (2017) learn morphological families that share the
same stem, such as faithful, faithfully, unfaithful, faithless, and so on, that are all derived
from faith. Those morphological families are learned as a graph and called morpho-
logical forests, which deviates from the meaning of the term forest we refer in this
article. Although learning morphological families has been studied as a graph learning
problem in Luo, Narasimhan, and Barzilay (2017), in this work, we learn paradigms that
generalize morphological families within a collection of hierarchical structures.

Narasimhan, Barzilay, and Jaakkola (2015) model the word formation with mor-
phological chains in terms of parent-child relations. For example, play and playful have
a parent–child relationship as a result of adding the morpheme ful at the end of play.
These relations are modeled by using log-linear models in order to predict the parent
relations. Semantic features as given by word2vec (Mikolov et al. 2013) are used in their
model in addition to orthographic features for the prediction of parent–child relations.
Narasimhan, Barzilay, and Jaakkola use contrastive estimation and generate corrupted
examples as pseudo negative examples within their approach.

Our model is an extension of our previous hierarchical clustering algorithm (Can
and Manandhar 2012). In that algorithm, a single tree is learned that corresponds to
a hierarchical organization of morphological paradigms. The parent nodes merge the
paradigms from the child nodes. But such merging of paradigms into a single structure
causes unrelated paradigms to be merged resulting in lower segmentation accuracy. The
current model addresses this issue by learning a forest of tree structures. Within each
tree structure the parent nodes merge the paradigms from the child nodes. Multiple
trees ensure that paradigms that should not be merged are kept separated. Additionally,
in single tree hierarchical clustering, a manually deﬁned context free grammar was
employed to generate the segmentation of a word. In the current model, we predict
the segmentation of a word without using any manually deﬁned grammar rules.

3. Probabilistic Hierarchical Clustering

Chan (2006) showed that learning the hierarchy between morphological paradigms
can help reveal latent relations in data. In the latent class model of Chan (2006), mor-
phological paradigms in a tree structure can be linked to syntactic categories (i.e., part-
of-speech tags). An example output of the model is given in Figure 2(a). Furthermore,
tokens can be assigned to allomorphs or gender/conjugational variants in each
paradigm.

Monson et al. (2008) showed that learning paradigms within a hierarchical model
gives a strong performance on morphological segmentation. In their model, each
paradigm is part of another paradigm, implemented within a lattice structure (see
Figure 2(b)).

Motivated by these works, we aim to learn paradigms within a hierarchical tree
structure. We propose a novel hierarchical clustering model that deviates from the
current hierarchical clustering models in two aspects:

It is a generative model based on a hierarchical Dirichlet process (HDP)
that simultaneously infers the hierarchical structure and morphological
segmentation.

Our model infers multiple trees (i.e., a forest of trees) instead of a single
tree.

1.

2.

352

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 3
A portion of a tree rooted at i with child nodes k, j with corresponding data points they generate.
i, j, k refer to the tree nodes; Ti, Tj, Tk refer to corresponding trees; and Di, Dj, Dk refer to data
contained in trees.

Although not covered in this work, additional work can be aimed at discovering other
types of latent information such as part-of-speech and allomorphs.

3.1 Model Overview

Let D = {x1, x2, . . . , xN} denote the input data where each xj is a word. Let Di ⊆ D
denote the subset of the data generated by tree rooted at i, then (see Figure 3):

where Di = {x1

i , x2

i , . . . , xNi

i }.

θ and hyperparameters β is given by:

The marginal probability of the data items in a given node in a tree i with parameters

D = D1 ∪ D2 ∪ · · · ∪ Dk

(cid:90)

p(Di) =

p(Di|θ)p(θ|β)dθ

Given tree Ti the data likelihood is given recursively by:

p(Di|Ti) =

1
Z p(Di)p(Dl|Tl)p(Dr|Tr) if i is an internal node with child nodes
l and r, and Z is the partition function

(3)






p(Di) if i is a leaf node

The term 1
Z (Di)p(Dl|Tl)p(Dr|Tr) corresponds to a product of experts model (Hinton
2002) comprising two competing hypotheses.1 Hypothesis H1 is given by p(Di) and
assigns all the data points in Di into a single cluster. Hypothesis H2 is given by

1 The review version of this paper and our previous work (Can and Manandhar 2012) missed this

connection to the product of experts model and did not include the 1/Z term. We thank our reviewers for
spotting this error.

(1)

(2)

353

Computational Linguistics

Volume 44, Number 2

p(Dl|Tl)p(Dr|Tr) and recursively splits the data in Di into two partitions (i.e., subtrees)
Dl and Dr. The factor Z is the partition function or the normalizing constant given by:

Z =

(cid:88)

Di,Dl,Dr:Di=Dl∪Dr

p(Di)p(Dl|Tl)p(Dr|Tr)

(4)

The recursive partitioning scheme we use is similar to that of Heller and
Ghahramani (2005). The product of experts scheme used in this paper contrasts with
the more conventional sum of experts mixture model of Heller and Ghahramani (2005)
that would have resulted in the mixture π p(Di) + (1 − π) p(Dl|Tl)p(Dr|Tr), where π
denotes the mixing proportion.

Finally, given the collection of trees T = {T1, T2, . . . , Tn}, the total likelihood of data

in all trees is deﬁned as follows:

(5)

(6)

(7)

(8)

(9)

Trees are generated from a DP. Let T = {T1, T2, . . . , Tn} be a set of trees. Ti is

sampled from a DP as follows:

p(D|T) =

p(Di|Ti)

|T|
(cid:89)

i

F ∼ DP(α, U)

Ti ∼ F

p(Tk|T, α, U) =

Nk
N + α

p(T|T|+1|T, α, U) =

α/k
N + α

where α denotes the concentration parameter of the DP. U is a uniform base distribution
that assigns equal probability to each tree. We integrate out F, which is a distribution
over trees, instead of estimating it. Hence, the conditional probability of sampling an
existing tree is computed as follows:

where Nk denotes the number of words in Tk and N denotes the total number of words
in the model. A new tree is generated with the following:

3.2 Modeling Morphology with Probabilistic Hierarchical Clustering

In our model, data points are the words and each tree node corresponds to a morpho-
logical paradigm (see Figure 4). Each word is part of all the paradigms on the path from
the leaf node having that word until the root node. The word can share either its stem or
sufﬁx with other words in the same paradigm. Hence, a considerable number of words
can be generated through this approach that may not be seen in the corpus. Our model
will prefer words that share stems or sufﬁxes to be close to each other within the tree.

354

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 4
A sample hierarchical tree structure that illustrates the clusters in each node (i.e., paradigms).
Each node corresponds to a cluster (i.e., morphological paradigm) and the leaf nodes correspond
to input data. The ﬁgure shows the ideal forest of trees that one expects given the input data.

The plate diagram of the generative model is given in Figure 5(a). Given a child
i ) and a

node i, we deﬁne a Dirichlet process to generate stems (denoted by s1
i , . . . , mNi
separate Dirichlet process to generate sufﬁxes (denoted by m1
i ):

i , . . . , sNi

Gs

i ∼ DP(βs, Ps)
sj
i ∼ Gs
i
i ∼ DP(βm, Pm)
Gm
mj
i ∼ Gm
i
(cid:89)

p(c)

Ps(sj

i) =

Pm(mj

i) =

p(c)

c∈sj
i
(cid:89)

c∈mj
i

where DP(βs, Ps) is a Dirichlet process that generates stems, βs denotes the concentra-
i is a distribution over the stems sj
tion parameter, and Ps is the base distribution. Gs
i
in node i. Correspondingly, DP(βm, Pm) is a Dirichlet process that generates sufﬁxes
with analogous parameters. Gm
i in node i. For
i
smaller values of the concentration parameter, it is less likely to generate new types.
Thus, sparse multinomials can be generated by the Dirichlet process yielding a skewed
distribution. We set βs < 1 and βm < 1 in order to generate a small number of stem and
sufﬁx types. sj

i are the jth stem and sufﬁx instance in the ith node, respectively.

is a distribution over the sufﬁxes mj

i and mj

The base distributions for stems and sufﬁxes are given by Equation (14) and Equa-
tion (15). Here, c denotes a single letter or character. We assume that letters are dis-
tributed uniformly (Creutz and Lagus 2005b), where p(c) = 1/A for an alphabet having
A letters. Our model will favor shorter morphemes because they have less factors in the
joint probability given by Equations (14) and (15).

For the root nodes we use HDPs that share global DPs (denoted by Hs for stems
and Hm for sufﬁxes). These introduce dependencies between stems/sufﬁxes in distinct

(10)

(11)

(12)

(13)

(14)

(15)

355

Computational Linguistics

Volume 44, Number 2

Figure 5
The DP model for the child nodes (on the left) illustrates the generation of words talking, cooked,
yelling. Each child node maintains its own DP that is independent of DPs from other nodes. The
HDP model for the root nodes (on the right) illustrates the generation of words yelling, talking,
repairs. In contrast to the DPs in the child nodes, in the HDP model, the stems/sufﬁxes are
shared across all root HDPs.

trees. The model will favor stems and sufﬁxes that are already generated in one of the
trees. The HDP for a root node i is deﬁned as follows:

Fs
i ∼ DP(βs, Hs)
Hs ∼ DP(αs, Ps)
sj
i ∼ Fs
i
ψz
i ∼ Hs
Fm
i ∼ DP(βm, Hm)
Hm ∼ DP(αm, Pm)
mj
i ∼ Fm
i
i ∼ Hm
φz

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

where the base distributions Hs and Hm are drawn from the global DPs DP(αs, Ps) and
DP(αm, Pm). Here, ψz
i denotes the sufﬁx type z
in node i, which are drawn from Hs and Hm (i.e., the global DPs), respectively. The plate
diagram of the HDP model is given in Figure 5(b).

i denotes the stem type z in node i, and φz

From the Chinese restaurant process (CRP) metaphor perspective, the global DPs
generate the global sets of dishes (i.e., stems and sufﬁxes) that constitute the menu for
all trees in the model. At each tree node, there are two restaurants: one for stems and
one for sufﬁxes. At each table, a different type of dish (i.e., stem or sufﬁx type) is served

356

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 6
A depiction of the Chinese restaurant franchise (i.e., global vs. local CRPs).
S1 = {walk, order, sleep, etc.}, M1 = {s, ing}, S2 = {pen, book}, M2 = {0, s}, S3 = {walk, order},
M3 = {0, s}, where 0 denotes an empty sufﬁx. For each stem type in the distinct trees, a customer
is inserted in the global restaurant. For example, there are two stem customers that are being
served the stem type walk because walk exists in two different trees.

i or φz

and for each stem/sufﬁx type there exists only one table in each node (i.e., restaurant).
Customers are the stem or sufﬁx tokens. Whenever a new customer, sj
i, enters a
restaurant, if the table, ψz
i , serving that dish already exists, the new customer sits
at that table. Otherwise, a new table is generated in the restaurant. A change in one of
the restaurants in the leaf nodes leads to the update in each restaurant all the way to
the root node. If the dish is not available in the root node, a new table is created for that
root node and a global customer is also added to the global restaurant. If no global table
exists for that dish, a new global table serving the dish is also created. This can be seen
as a Chinese restaurant franchise where each node is a restaurant itself (see Figure 6).

i or mj

In order to calculate the joint likelihood of the model, we need to consider both trees
and global stem/sufﬁx sets (i.e., local and global restaurants). The model is exchange-
able because it is a CRP—which means that the order the words are segmented does not
alter the joint probability. The joint likelihood of the entire model for a given collection
of trees T = {T1, T2, . . . , Tn} is computed as follows:

p(D|T) =

p(Di|Ti)

|T|
(cid:89)

|T|
(cid:89)

i

i

=

p(Si|Ti)p(Mi|Ti)

= p(Sroot)p(Mroot)

p(Si|Ti)p(Mi|Ti)

|T|
(cid:89)

i=1
i(cid:54)=root

(24)

(25)

(26)

357

Computational Linguistics

Volume 44, Number 2

where p(Si|Ti) and p(Mi|Ti) are computed recursively:

(cid:26) 1

p(Si|Ti) =

Z p(Si)p(Sl|Tl)p(Sr|Tr) if i is an internal node with child nodes l and r
p(Si) if i is a leaf node

where Z is the normalization constant. The same also applies for Mi.

Following the CRP, the joint probability of the stems in each root node Ti, Srooti
i , . . . , sNi

i }, is:

{s1

i , s2

p(Sroot) =

p(s1

i , s2

i , . . . , sNi
i )

|T|
(cid:89)

i

|T|
(cid:89)

i






=

p(s1

i )p(s2

i |s1

i ) . . . p(sNi

i

i , . . . , sNi−1
|s1

i

)

=



|T|
(cid:89)





i

Γ(βs)
Γ(Ni + βs)

i −1

βLs
s





(ns

ij − 1)!





Γ((cid:80)

Γ(αs)
sj∈Ks ks

j + αs)

αKs−1
s



(ks

j − 1)!Ps(sj)



Ls
i(cid:89)

j=1

Ks
(cid:89)

j=1

(27)

=

(28)

(29)

(30)

where the ﬁrst line in Equation (30) corresponds to the stem CRPs in the root nodes
of the trees (see Equation (2.22) in Can [2011] and Equation (A.1) in Appendix A). The
second line in Equation (30) corresponds to the global CRP of stems. The second factor in
the ﬁrst line of Equation (30) corresponds to the case where Ls
i stem types are generated
the ﬁrst time, and the third factor in the ﬁrst line corresponds to the case, where for each
of the Ls
ij stem tokens of type j. The ﬁrst factor accounts
for all denominators from both cases. Similarly, the second and the fourth factor in the
second line of Equation (30) corresponds to the case where Ks stem types are generated
globally (i.e., stem tables in the global restaurant), the third factor corresponds to the
case where, for each of the Ks stem types, there are ks
j stems of type j in distinct trees.
A sample hierarchical structure is given in Figure 7.

i stem types at node i, there are ns

The joint probability of stems in a child node Ti, Si = {s1

i , s2

i , . . . , sNi

i }, which belong

to stem tables {ψ1

i , ψ2

i , . . . } that index the items on the global menu is reduced to:

p(Si) =

Γ(βs)
Γ(Ni + βs)

i −1

βLs
s

(ns

ij − 1)!Ps(ψj
i)

(cid:17)

Ls
i(cid:89)

(cid:16)

j=1

(31)

Whenever a new stem is added to a node i (i.e., new customer enters one of the
i that belongs to type z

local restaurants), the conditional probability of the new stem sj

358

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 7
A sample hierarchical structure that contains D = {walk + ed, talk + ed, order + ed, walk + ing,
talk + ing} and the corresponding global tables. Here s1
node k with frequency 1.

k = walk/1 denotes the ﬁrst stem walk in

(i.e., customer sj
2006):

i sitting at table ψz

i ) is computed as follows (see Teh 2010 and Teh et al.

p(s j

i = z|Si

−s j

i , βs, Ps) =

otherwise (for an internal node)

(32)

if ψz

i ∈ Ψi






n

−s j
i
iz
Ni − 1 + βs
βsPs(s j
i)
Ni − 1 + βs
βsHs(ψz

i |αs, Ps)

Ni − 1 + βs

otherwise (for a root node)

where Ψi denotes the table indicators in node i, n
that belong to type z in node i when the last instance sj

i is excluded.

−sj
i
iz denotes the number of stem tokens

If the stem (customer) does not exist in the root node (i.e., chooses a non-existing

dish type in the root node), the new stem’s probability is calculated as follows:

Hs(ψz

i |αs, Ps) =

j + αs





(cid:80)

(cid:80)

ks
z
sj∈Ks ks
αsPs(sz)
sj∈Ks ks

j + αs

if j ∈ Ks

otherwise

If the stem type (dish) exists in the global clusters (i.e., global menu), it is chosen with the
probability proportional to the number of trees that contain the stem type (i.e., number
of global stem customers that are served the dish). Otherwise, the new stem (dish type)
is chosen based on the base distribution Ps.

(33)

359

Computational Linguistics

Volume 44, Number 2

Analogously to Equations (30)–(33) that apply to stems, the corresponding equa-

tions for sufﬁxes are given by Equations (34)–(37).




p(Mroot) =





Γ(βm)
Γ(Ni + βm)

βLm
i −1
m





(nm

ij − 1)!





Lm
i(cid:89)

j=1

|T|
(cid:89)

i





Γ((cid:80)

Γ(αm)
mj∈Km km

j + αm)

αKm−1
m

Km
(cid:89)

j=1



(km

j − 1)!Pm(mj)



p(Mi) =

Γ(βm)
Γ(Ni + βm)

βLm
i −1
m

(nm

ij − 1)!Pm(φj
i)

(cid:17)

Lm
i(cid:89)

(cid:16)

j=1

p(mj

i = z|Mi

−mj

i , βm, Pm) =

if φz

i ∈ Φi

n

−mj
i
iz
Ni − 1 + βm
βmPm(mj
i)
Ni − 1 + βm
βmHm(φz






i |αm, Pm)

Ni − 1 + βm

otherwise (for an internal node)

otherwise (for a root node)

Hm(φz

i |αm, Pm) =





(cid:80)

(cid:80)

km
z
mj∈Km km
αmPm(mz)
mj∈Km km

j + αm

j + αm

if j ∈ Km

otherwise

(34)

(35)

(36)

(37)

i , . . . , mNi

i , m1
i , . . . }; Nm
i

where Mi = {m1
types {φ1
tokens of type j in node i; Km is the total number of sufﬁx types; and km
j
trees that contain sufﬁxes of type j.

i } is the set of sufﬁxes in node i belonging to global sufﬁx
ij is the number of sufﬁx
is the number of

is the number of local sufﬁx types; nm

i , φ2

3.3 Metropolis-Hastings Sampling for Inferring Trees

Trees are learned along with the segmentations of words via inference. Learning trees
involves two steps: 1) constructing initial trees; 2) Metropolis-Hastings sampling for
inferring trees.

3.3.1 Constructing Initial Trees. Initially, all words are split at random points with uniform
probability. We use an approximation to construct the intial trees. Instead of computing
the full likelihood of the model, for each tree, we only compute the likelihood of a single
DP and assume that all words belong to this DP. The conditional probability of inserting
word wj = s + m in tree Tk is given by:

p(Tk, w j = s + m|D, T, α, βs, βm, Ps, Pm, U)
= p(Tk|T, α, U)p(wj = s + m|Dk)
= p(Tk|T, α, U)p(s|Sk, βs, Ps)p(m|Mk, βm, Pm)

(38)

360

Can and Manandhar

Tree Structured Morphological Segmentation

We use Equation (8) and (9) for computing the conditional probability p(Tk|T, α, U) of
choosing a particular tree. Once a tree is chosen, a branch to insert is selected at random.
The algorithm for constructing the initial trees is given in Algorithm 1.

3.3.2 Metropolis-Hastings Sampling. Once the initial trees are constructed, the hierar-
chical structures yielding a global maximum likelihood are inferred by using the
Metropolis Hastings algorithm (Hastings 1970). The inference is performed by itera-
tively removing a word from a leaf node from a tree and subsequently sampling a new
tree, a position within the tree, and a new segmentation (see Algorithm 2). Trees are
sampled with the conditional probability given in Equations (8) and (9). Hence, trees
with more words attract more words, and new trees are created proportional to the
hyperparameter α.

Once a tree is sampled, we draw a new position and a new segmentation. The word
is inserted at the sampled position with the sampled segmentation. The new model is
either accepted or rejected with the Metropolis-Hastings accept-reject criteria. We also
use a simulated annealing cooling schedule by assigning an initial temperature γ to the
system and decreasing the temperature at each iteration. The accept probability we use
is given by:

PAcc =

(cid:19) 1
γ

(cid:18) pnext(D|T)
pcur(D|T)

(39)

where pnext(D|T) denotes the likelihood of the data under the altered model and
pcur(D|T) denotes the likelihood of data under the current model before sampling. The
normalization constant cancels out because it is the same for the numerator and the
1
denominator. If pnext(D|T)
γ , then the new sample is accepted: otherwise,

γ > pcur(D|T)

1

Algorithm 1 Construction of the initial trees.

1: input: data D := {w1 = s1 + m1, . . . , wN = sN + mN}
2: initialize: The trees in the model: T := ∅
3: For j = 1 . . . N
4: Choose wj from the corpus.
5: Sample, Tk as a new empty tree or existing tree from T, and, a split point wj = s + m

from the joint distribution (see Equation 38):
p(Tk, wj = s + m|D, T, α, βs, βm, Ps, Pm, U)

6: if Tk ∈ T then
7: Draw a node s from Tk with uniform probability
8: Assign Tk as a sibling of Ts: Ts := Ts + Tk (i.e., Ts is new tree with Tk and old Ts

as children)

Tk is an empty tree. Add Tk into T: T := T ∪ Tk

9: else
10:
11: Dk := {wj = s + m} (i.e., word wj = s + m is assigned into Tk)
12: end if
13: Remove wj from the corpus
14: End For
15: output: T

361

Computational Linguistics

Volume 44, Number 2

Algorithm 2 Learning the trees with Metropolis-Hastings algorithm

1: input: data D := {w1 = s1 + m1, . . . , wN = sN + mN}, initial trees T, initial

temperature γ, the target temperature κ, temperature decrement η

Choose the leaf node j from all trees with uniform probability
Let Dj := {wj = sold + mold}

2: initialize: pcur(D|T) := p(D|T)
3: while γ > κ do
4:
5:
6: Draw a split point wj = snew + mnew with uniform probability
7: Draw a tree Tk with probability p(Tk|T, α, U) (see Equations 8 and 9)
8:

if Tk ∈ T then

Draw a sibling node s from Tk with uniform probability
Ts := Ts + Tk (see Figure 8)

9:
10:
11:
12:
13:
14:
15:

16:
17:

18:

else

Create a new tree Tk
T := T ∪ Tk
Dk := {wj = snew + mnew}

end if
rand ∼ Normal(0, 1)
pnext := p(D|T) (see Equation 24)

if pnext(D|T) >= pcur(D|T) or rand <
Accept the new tree structure
pcur(D|T) := pnext(D|T)

19:
20:
21:
22:
23: end while
24: output: T

end if
γ := γ − η

(cid:16) pnext(D|T)
pcur(D|T)

(cid:17) 1
γ

then

the new model is still accepted with a probability pAcc. The system is cooled in each
iteration with decrements η. We refer to Section 4 for details of parameter settings.

γ ← γ − η

(40)

3.4 Morphological Segmentation

Once the model is learned, it can be used for the segmentation of novel words. We
use only root nodes for the segmentation. Viterbi decoding is used in order to ﬁnd the
morphological segmentation of each word having the maximum probability:

arg

max
s1,··· ,sa,m1,...,mb

p(wk = s1, · · · , sa, m1, · · · , mb|D, αs, βs, Hs, Ps, αm, βm, Hm, Pm)

a
(cid:89)

|T|
(cid:88)

=

j=1

b
(cid:89)

i=1
i=root
|T|
(cid:88)

j=1

i=1
i=root

p(sj

i|Si, αs, βs, Hs, Ps)

p(mj

i|Mi, αm, βm, Hm, Pm)

(41)

where wk denotes the kth word to be segmented in the test set.

362

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 8
A sampling step in Metropolis-Hastings algorithm.

3.5 Example Paradigms

A sample of root paradigms learned by our model for English is given in Table 1. The
model can ﬁnd similar word forms (i.e., separat+ists, medal+ists, hygien+ists) that are
grouped in the neighbor branches in the tree structure (see Figures 9, B.1, and B.2 for
sample paradigms learned in English, Turkish, and German).

Paradigms are captured based on the similarity of either stems or sufﬁxes. Having
the same stem such as co-chair (co-chairman, co-chairmen) or trades (trades+man,
trades+men) allows us to ﬁnd segmentations such as co-chair+man vs. co-chair+men
and trades+man vs. trades+men. Although we assume a stem+sufﬁx segmentation,
other types of segmentation, such as preﬁx+stem, are also covered. However, stem
alterations and inﬁxation are not covered in our model.

4. Evaluation

We used publicly available Morpho Challenge data sets for English, German, and
Turkish for training. The English data set consists of 878,034 words, the German data set
consists of 2,338,323 words, and the Turkish data set consists of 617,298 words. Although
frequency of each word was available in the training set, we did not make use of this

363

Computational Linguistics

Volume 44, Number 2

Table 1
Example paradigms in English.

{ﬁnal, ungod, frequent, pensive} {ly}

{kind, kind, kind} {est, er, 0}

{underrepresent, modul} {ation}

{plebe, hawai, muslim-croat} {ian}

{compuls, shredd, compuls, shredd, compuls} {ion, er, ively, ers, ory}

{zion, modern, modern, dynam} {ism, ists}

{reclaim, chas, pleas, fell} {ing}

{mov, engrav, stray, engrav, fantasiz, reischau,
decilit, suspect} {ing, er, e}

{measur, measur, incontest, transport, unplay, reput}
{e, able}

{housewar, decorat, entitl, cuss, decorat, entitl, materi, toss, ﬂay, unconﬁrm
linse, equipp} {es, ing, alise, ed}

{fair, norw, soon, smooth, narrow, sadd, steep, noisi, statesw, narrow}
{est, ing}

{rest, wit, name, top, odor, bay, odor, sleep} {less, s}

{umpir, absorb, regard, embellish, freez, gnash} {ing}

{nutrition, manicur, separat, medal, hygien, nutrition, genetic, preservation}
{0, ists}

information. In other words, we use only the word types (not tokens) in training. We do
not address the ambiguity of words in this work and leave this as future research.

In all experiments, the initial temperature of the system is set γ = 2 and it is
reduced to γ = 0.01 with decrements η = 0.0001 (see Equation (39)). Figure 10 shows
the time required for the log likelihoods of the trees of sizes 10K, 16K, and 22K to
converge. We ﬁxed αs = αm = βs = βm = 0.01 and α = 0.0005 in all our experiments.
The hyperparameters are set manually as a result of several experiments. These are the
optimum values obtained from a number of experiments.2

Precision, recall, and F-score values against training set sizes are given in Figures 11

and 12 for English and Turkish, respectively.

4.1 Morpho Challenge Evaluation

Although we experimented with different sizes of training sets, we used a randomly
chosen 600K words from the English and 200K words from the Turkish and German
data sets for evaluation purposes.

Evaluation is performed according to the method proposed in Morpho Challenge
(Kurimo et al. 2010a), which in turn is based on evaluation used by Creutz and Lagus
(2007). The gold standard evaluation data set utilized within the Morpho Challenge is
a hidden set that is not available publicly. This makes the Morpho Challenge evaluation
different from other evaluations that provide test data. In this evaluation, word pairs

2 The source code of the model is accessible at: https://github.com/burcu-can/TreeStructuredDP.

364

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 9
Sample hierarchies in English, Turkish, and German.

sharing at least one common morpheme are sampled. For precision, word pairs are
sampled from the system results and checked against the gold standard segmentations.
For recall, word pairs are sampled from the gold standard and checked against the
system results. For each matching morpheme, 1 point is given. Precision and recall are
calculated by normalizing the total obtained scores.

We compare our results with other unsupervised systems from Morpho Challenge
2010 (Kurimo et al. 2010b) for English, German, and Turkish. More speciﬁcally, we com-
pare our model with all competing unsupervised systems: Morfessor Baseline (Creutz
and Lagus 2002), Morfessor CATMAP (Creutz and Lagus 2005a), Base Inference (Lignos
2010), Iterative Compounding (Lignos 2010), Aggressive Compounding (Lignos 2010),
and Nicolas, Farr´e, and Molinero (2010). Additionally, we compare our system with the
Morpho Chain model of Narasimhan, Barzilay, and Jaakkola (2015) by re-training their
model on exactly the same training sets as ours. All evaluation was carried out by the
Morpho Challenge organizers based on the hidden gold data sets.

365

Computational Linguistics

Volume 44, Number 2

Figure 10
The likelihood convergence in time (in minutes) for data sets of size 16K and 22K.

Figure 11
English results for different size of data.

English results are given in Table 2. For English, the Base Inference algorithm of
Lignos (2010) obtained the highest F-measure in Morpho Challenge 2010 among other
competing unsupervised systems. Our model is ranked fourth among all unsupervised
systems with a F-measure 60.27%.

German results are given in Table 3. Our model outperforms other unsupervised

models in Morpho Challenge 2010 with a F-measure 50.71%.

Turkish results are given in Table 4. Again, our model outperforms other unsuper-

vised participants, achieving a F-measure 56.41%.

Our model also outperforms the model from Can and Manandhar (2012) (which
we refer to as Single Tree Probabilistic Clustering), although the results are not directly
comparable because the largest data set we were able to train that model on was 22K
words due to the training time required. The full training set provided by Morpho
Challenge was not used in Can and Manandhar (2012). Our current approach is more
efﬁcient as the training cost is divided across multiple tree structures with each tree
being shallower compared with our previous model.

366

Can and Manandhar

Tree Structured Morphological Segmentation

Table 2
Morpho Challenge 2010 experimental results for English.

System

Precision (%) Recall (%)

F-measure (%)

Figure 12
Turkish results for different size of data.

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)
Base Inference (Lignos 2010)
Iterative Compounding (Lignos 2010)
Aggressive Compounding (Lignos 2010)
Nicolas (Nicolas, Farr´e, and Molinero 2010)
Morfessor Baseline (Creutz and Lagus 2002)
Morpho Chain (Narasimhan, Barzilay, and
Jaakkola 2015)
Morfessor CatMAP (Creutz and Lagus 2005a)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)
Base Inference (Lignos 2010)
Iterative Compounding (Lignos 2010)
Aggressive Compounding (Lignos 2010)
Morfessor Baseline (Creutz and Lagus 2002)
Morfessor CatMAP (Creutz and Lagus 2005a)

55.60
55.60

80.77
80.27
71.45
67.83
81.39
74.87

86.84

47.92
57.79

66.38
62.13
59.41
82.80
72.70

65.80
57.58

53.76
52.76
52.31
53.43
41.70
39.01

30.03

53.84
32.42

35.36
34.70
37.21
19.77
35.43

Table 3
Morpho Challenge 2010 experimental results for German.

System

Precision (%) Recall (%)

F-measure (%)

In order to draw a substantive empirical comparison, we performed another set
of experiments by running the current approach on only 22K words as the Single Tree
Probabilistic Clustering (Can and Manandhar 2012). Results are given in Tables 5, 6, and
7. As shown in the tables, the current model outperforms the previous model even on
the smaller data set.

60.27
57.33

64.55
63.67
60.40
59.78
55.14
50.42

44.63

50.71
41.54

46.14
44.53
45.76
31.92
47.64

367

Computational Linguistics

Volume 44, Number 2

Table 4
Morpho Challenge 2010 experimental results for Turkish.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)
Base Inference (Lignos 2010)
Iterative Compounding (Lignos 2010)
Aggressive Compounding (Lignos 2010)
Nicolas (Nicolas, Farr´e, and Molinero 2010)
Morfessor Baseline (Creutz and Lagus 2002)
Morpho Chain (Narasimhan, Barzilay, and
Jaakkola 2015)
Morfessor CatMAP (Creutz and Lagus 2005a)

57.70
72.36

72.81
68.69
55.51
79.02
89.68
69.25

79.38

55.18
25.81

16.11
21.44
34.36
19.78
17.78
31.51

31.88

56.41
38.04

26.38
32.68
42.45
31.64
29.67
43.32

45.49

Table 5
Comparison with Single Tree Probabilistic Clustering for English.

System

Precision (%) Recall (%) F-measure (%)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)

67.75
55.60

53.93
57.58

60.06
57.33

Table 6
Comparison with Single Tree Probabilistic Clustering for German.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)

33.93
57.79

65.31
32.42

44.66
41.54

Table 7
Comparison with Single Tree Probabilistic Clustering for Turkish.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)

64.39
72.36

42.99
25.81

51.56
38.04

4.2 Additional Evaluation

For additional experiments, we compare our model with Morpho Chain (Narasimhan,
Barzilay, and Jaakkola 2015) based on their evaluation method that differs from the
Morpho Challenge evaluation method. Their evaluation method is based on counting
the correct segmentation points. For example, if the result segmentation is booking+s
and the gold segmentation is book+ing+s, 1 point is counted. Precision and recall are

368

Can and Manandhar

Tree Structured Morphological Segmentation

Table 8
Comparison with Morpho Chain model for English based on Morpho Chain evaluation.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Morpho Chain

67.41
72.63

62.5
78.72

64.86
75.55

Table 9
Comparison with Morpho Chain model for Turkish based on Morpho Chain evaluation.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Morpho Chain

89.30
70.49

48.22
63.27

62.63
66.66

calculated based on these matching segmentation points. In addition, this evaluation
does not use the hidden gold data sets. Instead, the test sets are created by aggregating
the test data from Morpho Challenge 2005 and Morpho Challenge 2010 (as reported in
Narasimhan, Barzilay, and Jaakkola [2015]) that provide segmentation points.3

We used the same trained models as in our Morpho Challenge evaluation. The

English test set contains 2,218 words and the Turkish test set contains 2,534 words.4
The English results are given in Table 8 and Turkish results are given in Table 9.
For all systems, Morpho Chain evaluation scores are comparably higher than the
Morpho Challenge scores. There are several reasons for this. In the Morpho Challenge
evaluation, the morpheme labels are considered rather than the surface forms of the
morphemes. For example, pantolon+u+yla [with his trousers] and emel+ler+i+yle [with
his desires] have got both possessive morpheme (u and i) that is labeled with POS and
relational morpheme (yla and yle) labeled with REL in common. This increases the total
number of points that is computed over all word pairs, and therefore lowers the scores.
Secondly, in the Morpho Chain evaluation, only the gold segmentation that has the
maximum match with the result segmentation is chosen for each word (e.g., yazımıza
has two gold segmentations: yaz+ı+mız+a [to our summer] and yazı+mız+a; [to our
writing]). In contrast, in the Morpho Challenge evaluation all segmentations in the
gold segmentation are evaluated. This is another factor that increases the scores in
Morpho Chain evaluation. Thus, the Morpho Chain evaluation favors precision over
recall. Indeed, in the Morpho Challenge evaluation, the Morpho Chain system has high
precision but their model suffers from low recall due to undersegmentation (see Tables 2
and 4).

It should be noted that the output of our system is not only the segmentation points,
but also the hierarchical organization of morphological paradigms that we believe is
novel in this work. However, because of the difﬁculty in measuring the quality of hier-
archical paradigms, which will require a corresponding hierarchically organized gold
data set, we are unable to provide an objective measure of the quality of hierarchical

3 In addition to the hidden test data, Morpho Challenge also provides separate publicly available test data.
4 Because of the unavailability of word2vec word embeddings for German, we were unable to perform

Morpho Chain evaluation on this language.

369

Computational Linguistics

Volume 44, Number 2

structures learned. We present different portions from the obtained trees in Appendix B
(see Figures B.1 and B.2).5 It can be seen that words sharing the same sufﬁxes are gath-
ered closer to each other, such as reestablish+ed, reclassiﬁ+ed, circl+ed, uncloth+ed, and so
forth. Secondly, related morphological families gather closer to each other, such as
impress+ively, impress+ionist, impress+ions, impress+ion.

5. Conclusions and Future Work

In this article, we introduce a tree structured Dirichlet process model for hierarchical
morphological segmentation. The method is different compared with existing hierar-
chical and non-hierarchical methods for learning paradigms. Our model learns mor-
phological paradigms that are clustered hierarchically within a forest of trees.

Although our goal in this work is on hierarchical learning, our model shows com-
petitive performance against other unsupervised morphological segmentation systems
that are designed primarily for segmentation only. The system outperforms other un-
supervised systems in Morpho Challenge 2010 for German and Turkish. It also out-
performs the more recent Morpho Chain (Narasimhan, Barzilay, and Jaakkola 2015)
system on the Morpho Challenge evaluation for German and Turkish.

The sample paradigms learned show that these can be linked to other types of latent
information, such as part-of-speech tags. Combining morphology and syntax as a joint
learning problem within the same model can be a fruitful direction for future work.

The hierarchical structure is beneﬁcial because we can have both compact and more
general paradigms at the same time. In this article, we use the paradigms only for the
segmentation task, and applications of hierarchy learned is left as future work.

Appendix A. Derivation of the Full Joint Distribution

Let D = {s1, s2, . . . , sN} denote the input data where each sj is a data item. A particular
setting of a table with N customers has a joint probability of:

p(s1, . . . , sN|α, P) = p(s1|α, P)p(s2|s1, α, P)p(s3|s1, s2, α, P) . . . p(sN|s1, . . . , sN−1, α, P)

=

αL
α(1 + α)(2 + α)(N − 1 + α)

P(si)

(nsi − 1)!

L
(cid:89)

i=1

=

Γ(α)
Γ(N + α)

αL−1

L
(cid:89)

i=1

(nsi − 1)!

P(si)

L
(cid:89)

i=1

L
(cid:89)

i=1

(A.1)

For each customer, either a new table is created or the customer sits at an occupied table.
For each table, at least one table creation is performed, which forms the second and
the fourth factor in the last equation. Once the table is created, factors that represent the
number of customers sitting at each table are chosen accordingly, which refers to the
third factor in the last equation. All factors with α are aggregated in the ﬁrst factor in
terms of a Gamma function in the last equation.

5 Some of the full trees are given at http://web.cs.hacettepe.edu.tr/~burcucan/TreeStructuredDP.

370

Can and Manandhar

Tree Structured Morphological Segmentation

Appendix B. Sample Tree Structures

Figure B.1
Sample hierarchies in English.

371

Computational Linguistics

Volume 44, Number 2

Figure B.2
Sample hierarchies in Turkish.

Acknowledgments
This research was supported by TUBITAK
(The Scientiﬁc and Technological Research
Council of Turkey) grant number 115E464.
We thank Karthik Narasimhan for providing
their data sets and code. We are grateful to
Sami Virpioja for the evaluation of our
results on the hidden gold data provided by
Morpho Challenge. We thank our reviewers
for critical feedback and spotting an error in
our previous version of the article. Their
comments have immensely helped improve
the article.

References
Can, Burcu. 2011. Statistical Models for

Unsupervised Learning of Morphology and
POS tagging. Ph.D. thesis, Department of

Computer Science, The University
of York.

Can, Burcu and Suresh Manandhar. 2010.
Clustering morphological paradigms
using syntactic categories. In Multilingual
Information Access Evaluation I. Text
Retrieval Experiments: 10th Workshop of the
Cross-Language Evaluation Forum, CLEF
2009, Corfu, Greece, September 30 -
October 2, 2009, Revised Selected Papers,
pages 641–648, Corfu.

Can, Burcu and Suresh Manandhar. 2012.
Probabilistic hierarchical clustering of
morphological paradigms. In Proceedings of
the 13th Conference of the European Chapter of
the Association for Computational Linguistics,
EACL ’12, pages 654–663, Avignon.
Chan, Erwin. 2006. Learning probabilistic
paradigms for morphology in a latent
class model. In Proceedings of the Eighth

372

Can and Manandhar

Tree Structured Morphological Segmentation

Meeting of the ACL Special Interest
Group on Computational Phonology and
Morphology, SIGPHON ’06, pages 69–78,
New York, NY.

Creutz, Mathias and Krista Lagus. 2002.

Unsupervised discovery of morphemes.
In Proceedings of the ACL-02 Workshop on
Morphological and Phonological Learning -
Volume 6, MPL ’02, pages 21–30,
Philadelphia, PA.

Creutz, Mathias and Krista Lagus. 2005a.

Inducing the morphological lexicon of a
natural language from unannotated text.
In Proceedings of the International and
Interdisciplinary Conference on Adaptive
Knowledge Representation and Reasoning
(AKRR 2005), pages 106–113, Espoo.
Creutz, Mathias and Krista Lagus. 2005b.
Unsupervised morpheme segmentation
and morphology induction from text
corpora using Morfessor 1.0. Technical
Report A81. Helsinki University of
Technology.

Creutz, Mathias and Krista Lagus. 2007.
Unsupervised models for morpheme
segmentation and morphology learning.
ACM Transactions Speech Language
Processing, 43:1–3:34.

Dreyer, Markus and Jason Eisner. 2011.

Discovering morphological paradigms
from plain text using a Dirichlet Process
mixture model. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 616–627,
Edinburgh.

Goldsmith, John. 2001. Unsupervised

learning of the morphology of a natural
language. Computational Linguistics,
27(2):153–198.

Hastings, W. K. 1970. Monte Carlo sampling
methods using Markov chains and their
applications. Biometrika, 57:97–109.

Heller, Katherine A. and Zoubin

Ghahramani. 2005. Bayesian hierarchical
clustering. In Proceedings of the 22nd
International Conference on Machine
Learning, ICML ’05, pages 297–304,
Bonn.

Hinton, Geoff. 2002. Training products of

experts by minimizing contrastive
divergence. Neural Computation,
14(8):1771–1800.

Kurimo, Mikko, Sami Virpioja, Ville

Turunen, and Krista Lagus. 2010a. Morpho
Challenge Competition 2005–2010:
Evaluations and results. In Proceedings of
the 11th Meeting of the ACL Special Interest
Group on Computational Morphology and
Phonology, SIGMORPHON ’10,
pages 87–95, Uppsala.

Kurimo, Mikko, Sami Virpioja, Ville T.

Turunen, Graeme W. Blackwood, and
William Byrne. 2009. Overview and results
of Morpho Challenge 2009. In Proceedings
of the 10th Cross-Language Evaluation Forum
Conference on Multilingual Information Access
Evaluation: Text Retrieval Experiments,
CLEF’09, pages 578–597, Corfu.
Kurimo, Mikko, Sami Virpioja, Ville T.
Turunen, Bruno G ´olenia, Sebastian
Spiegler, Oliver Ray, Peter Flach, Oskar
Kohonen, Laura Lepp¨anen, Krista Lagus,
Constantine Lignos, Lionel Nicolas,
Jacques Farr´e, and Miguel Molinero.
2010b. Proceedings of the Morpho
Challenge 2010 Workshop. Technical
Report TKK-ICS-R37. Aalto University.
Lignos, Constantine. 2010. Learning from

unseen data. In Proceedings of the Morpho
Challenge 2010 Workshop, pages 35–38,
Espoo.

Luo, Jiaming, Karthik Narasimhan, and
Regina Barzilay. 2017. Unsupervised
learning of morphological forests.
Transactions of the Association of
Computational Linguistics, 5:353–364.
Mikolov, Tomas, Kai Chen, Greg Corrado,

and Jeffrey Dean. 2013. Efﬁcient estimation
of word representations in vector space.
CoRR, abs/1301.3781.

Monson, Christian. 2008. Paramor: From
Paradigm Structure to Natural Language
Morphology Induction. Ph.D. thesis,
Language Technologies Institute, School of
Computer Science, Carnegie Mellon
University.

Monson, Christian, Jaime Carbonell, Alon
Lavie, and Lori Levin. 2008. Paramor:
Finding paradigms across morphology.
In Lecture Notes in Computer Science -
Advances in Multilingual and Multimodal
Information Retrieval; Cross-Language
Evaluation Forum, CLEF 2007,
pages 900–907, Springer, Berlin.

Narasimhan, Karthik, Regina Barzilay, and

Tommi S. Jaakkola. 2015. An unsupervised
method for uncovering morphological
chains. Transactions of the Association for
Computational Linguistics, 3:157–167.

Nicolas, Lionel, Jacques Farr´e, and Miguel A.
Molinero. 2010. Unsupervised learning of
concatenative morphology based on
frequency-related form occurrence. In
Proceedings of the Morpho Challenge 2010
Workshop, pages 39–43, Espoo.

Poon, Hoifung, Colin Cherry, and Kristina

Toutanova. 2009. Unsupervised
morphological segmentation with
log-linear models. In Proceedings of Human
Language Technologies: The 2009 Annual

373

Computational Linguistics

Volume 44, Number 2

Conference of the North American
Chapter of the Association for Computational
Linguistics, NAACL ’09, pages 209–217,
Boulder, CO.

Snover, Matthew G., Gaja E. Jarosz, and
Michael R. Brent. 2002. Unsupervised
learning of morphology using a novel
directed search algorithm: Taking the ﬁrst
step. In Proceedings of the ACL-02 Workshop
on Morphological and Phonological Learning,
pages 11–20, Philadelphia, PA.

Snyder, Benjamin and Regina Barzilay. 2008.
Unsupervised multilingual learning for
morphological segmentation. In
Proceedings of ACL-08: HLT, pages 737–745,
Columbus, OH.

Teh, Y. W. 2010. In Dirichlet processes, In

Encyclopedia of Machine Learning, Springer.
Teh, Y. W., M. I. Jordan, M. J. Beal, and D. M.
Blei. 2006. Hierarchical Dirichlet Processes.
Journal of the American Statistical
Association, 101(476):1566–1581.

374

Tree Structured Dirichlet Processes for
Hierarchical Morphological Segmentation

Burcu Can
Hacettepe University
Department of Computer Engineering
burcucan@cs.hacettepe.edu.tr

Suresh Manandhar
University of York
Department of Computer Science
suresh@cs.york.ac.uk

This article presents a probabilistic hierarchical clustering model for morphological segmenta-
tion. In contrast to existing approaches to morphology learning, our method allows learning
hierarchical organization of word morphology as a collection of tree structured paradigms. The
model is fully unsupervised and based on the hierarchical Dirichlet process. Tree hierarchies are
learned along with the corresponding morphological paradigms simultaneously. Our model is
evaluated on Morpho Challenge and shows competitive performance when compared to state-
of-the-art unsupervised morphological segmentation systems. Although we apply this model for
morphological segmentation, the model itself can also be used for hierarchical clustering of other
types of data.

1. Introduction

Unsupervised learning of morphology has been an important task because of the bene-
ﬁts it provides to many other natural language processing applications such as machine
translation, information retrieval, question answering, and so forth. Morphological
paradigms provide a natural way to capture the internal morphological structure of
a group of morphologically related words. Following Goldsmith (2001) and Monson
(2008), we use the term paradigm as consisting of a set of stems and a set of sufﬁxes
where each combination of a stem and a sufﬁx leads to a valid word form, for example,
{walk,talk,order,yawn}{s,ed,ing} generating the surface forms walk+ed, walk+s, walk+ing,
talk+ed, talk+s, talk+ing, order+s, order+ed, order+ing, yawn+ed, yawn+s, yawn+ing. A sam-
ple paradigm is given in Figure 1.

Recently, we introduced a probabilistic hierarchical clustering model for learning
hierarchical morphological paradigms (Can and Manandhar 2012). Each node in the
hierarchical tree corresponds to a morphological paradigm and each leaf node consists
of a word. A single tree is learned, where different branches on the hierarchical tree

Submission received: 29 June 2016; revised version received: 30 July 2017; accepted for publication: 1 March
2018.

doi:10.1162/COLI a 00318

© 2018 Association for Computational Linguistics
Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International
(CC BY-NC-ND 4.0) license

Computational Linguistics

Volume 44, Number 2

Figure 1
An example paradigm.

correspond to different morphological forms. Well-deﬁned paradigms in the lower
levels of trees were learned. However, merging of the paradigms at the upper levels
led to undersegmentation in that model. This problem led us to search for ways to
learn multiple trees. In our current approach, we learn a forest of paradigms spread
over several hierarchical trees. Our evaluation on Morpho Challenge data sets provides
better results when compared to the previous method (Can and Manandhar 2012). Our
results are comparable to current state-of-the-art results while having the additional
beneﬁt of inferring the hierarchical structure of morphemes for which no comparable
systems exist.

The article is organized as follows: Section 2 introduces the related work in the ﬁeld.
Section 3 describes the probabilistic hierarchical clustering model with its mathematical
model deﬁnition and how it is applied for morphological segmentation; the same
section explains the inference and the morphological segmentation. Section 4 presents
the experimental setting and the obtained evaluation scores from each experiment,
and Section 5 concludes and addresses the potential future work following the model
presented in this article.

2. Related Work

There have been many unsupervised approaches to morphology learning that focus
solely on segmentation (Creutz and Lagus 2005a, 2007; Snyder and Barzilay 2008; Poon,
Cherry, and Toutanova 2009; Narasimhan, Barzilay, and Jaakkola 2015). Others, such
as Monson et al. (2008), Can and Manandhar (2010), Chan (2006), and Dreyer and Eisner
(2011), learn morphological paradigms that permit additional generalization.

A popular paradigmatic model is Linguistica (Goldsmith 2001), which uses the
Minimum Description Length principle to minimize the description length of a corpus
based on paradigm-like structures called signatures. A signature consists of a list of
sufﬁxes that are seen with a particular stem—for example, order-{ed, ing, s} denotes a
signature for the stem order.

Snover, Jarosz, and Brent (2002) propose a generative probabilistic model that
deﬁnes a probability distribution over different segmentations of the lexicon into
paradigms. Paradigms are learned with a directed search algorithm that examines

350

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 2
Examples of hierarchical morphological paradigms.

subsets of the lexicon, ranks them, and incrementally combines them in order to ﬁnd
the best segmentation of the lexicon. The proposed model addresses both inﬂectional
and derivational morphology in a language independent framework. However, their
model does not allow multiple sufﬁxation (e.g., having multiple sufﬁxes added to a
single stem) whereas Linguistica allows this.

Monson et al. (2008) induce morphological paradigms in a deterministic framework
named ParaMor. Their search algorithm begins with a set of candidate sufﬁxes and
collects candidate stems that attach to the sufﬁxes (see Figure 2(b)). The algorithm
gradually develops paradigms following search paths in a lattice-like structure. Proba-
bilistic ParaMor, involving a statistical natural language tagger to mimic ParaMor, was
introduced in Morpho Challenge 2009 (Kurimo et al. 2009). The system outperforms
other unsupervised morphological segmentation systems that competed in Morpho
Challenge 2009 (Kurimo et al. 2009) for the languages Finnish, Turkish, and German.

Can and Manandhar (2010) exploit syntactic categories to capture morphological
paradigms. In a deterministic scheme, morphological paradigms are learned by pairing
syntactic categories and identifying common sufﬁxes between them. The paradigms
compete to acquire more word pairs.

Chan (2006) describes a supervised procedure to ﬁnd morphological paradigms
within a hierarchical structure by applying latent Dirichlet allocation. Each paradigm
is placed in a node on the tree (see Figure 2(a)). The results show that each paradigm
corresponds to a part-of-speech such as adjective, noun, verb, or adverb. However, as
the method is supervised, the true sufﬁxes and segmentations are known in advance.
Learning hierarchical paradigms helps not only in learning morphological segmenta-
tion, but also in learning syntactic categories. This linguistic motivation led us toward
learning the hierarchical organization of morphological paradigms.

Dreyer and Eisner (2011) propose a Dirichlet process mixture model to learn
paradigms. The model uses 50–100 seed paradigms to infer the remaining paradigms,
which makes it semi-supervised. The model is similar to ours in the sense that it also
uses Dirichlet processes (DPs); however the model does not learn hierarchies between
the paradigms.

351

Computational Linguistics

Volume 44, Number 2

Luo, Narasimhan, and Barzilay (2017) learn morphological families that share the
same stem, such as faithful, faithfully, unfaithful, faithless, and so on, that are all derived
from faith. Those morphological families are learned as a graph and called morpho-
logical forests, which deviates from the meaning of the term forest we refer in this
article. Although learning morphological families has been studied as a graph learning
problem in Luo, Narasimhan, and Barzilay (2017), in this work, we learn paradigms that
generalize morphological families within a collection of hierarchical structures.

Narasimhan, Barzilay, and Jaakkola (2015) model the word formation with mor-
phological chains in terms of parent-child relations. For example, play and playful have
a parent–child relationship as a result of adding the morpheme ful at the end of play.
These relations are modeled by using log-linear models in order to predict the parent
relations. Semantic features as given by word2vec (Mikolov et al. 2013) are used in their
model in addition to orthographic features for the prediction of parent–child relations.
Narasimhan, Barzilay, and Jaakkola use contrastive estimation and generate corrupted
examples as pseudo negative examples within their approach.

Our model is an extension of our previous hierarchical clustering algorithm (Can
and Manandhar 2012). In that algorithm, a single tree is learned that corresponds to
a hierarchical organization of morphological paradigms. The parent nodes merge the
paradigms from the child nodes. But such merging of paradigms into a single structure
causes unrelated paradigms to be merged resulting in lower segmentation accuracy. The
current model addresses this issue by learning a forest of tree structures. Within each
tree structure the parent nodes merge the paradigms from the child nodes. Multiple
trees ensure that paradigms that should not be merged are kept separated. Additionally,
in single tree hierarchical clustering, a manually deﬁned context free grammar was
employed to generate the segmentation of a word. In the current model, we predict
the segmentation of a word without using any manually deﬁned grammar rules.

3. Probabilistic Hierarchical Clustering

Chan (2006) showed that learning the hierarchy between morphological paradigms
can help reveal latent relations in data. In the latent class model of Chan (2006), mor-
phological paradigms in a tree structure can be linked to syntactic categories (i.e., part-
of-speech tags). An example output of the model is given in Figure 2(a). Furthermore,
tokens can be assigned to allomorphs or gender/conjugational variants in each
paradigm.

Monson et al. (2008) showed that learning paradigms within a hierarchical model
gives a strong performance on morphological segmentation. In their model, each
paradigm is part of another paradigm, implemented within a lattice structure (see
Figure 2(b)).

Motivated by these works, we aim to learn paradigms within a hierarchical tree
structure. We propose a novel hierarchical clustering model that deviates from the
current hierarchical clustering models in two aspects:

It is a generative model based on a hierarchical Dirichlet process (HDP)
that simultaneously infers the hierarchical structure and morphological
segmentation.

Our model infers multiple trees (i.e., a forest of trees) instead of a single
tree.

1.

2.

352

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 3
A portion of a tree rooted at i with child nodes k, j with corresponding data points they generate.
i, j, k refer to the tree nodes; Ti, Tj, Tk refer to corresponding trees; and Di, Dj, Dk refer to data
contained in trees.

Although not covered in this work, additional work can be aimed at discovering other
types of latent information such as part-of-speech and allomorphs.

3.1 Model Overview

Let D = {x1, x2, . . . , xN} denote the input data where each xj is a word. Let Di ⊆ D
denote the subset of the data generated by tree rooted at i, then (see Figure 3):

where Di = {x1

i , x2

i , . . . , xNi

i }.

θ and hyperparameters β is given by:

The marginal probability of the data items in a given node in a tree i with parameters

D = D1 ∪ D2 ∪ · · · ∪ Dk

(cid:90)

p(Di) =

p(Di|θ)p(θ|β)dθ

Given tree Ti the data likelihood is given recursively by:

p(Di|Ti) =

1
Z p(Di)p(Dl|Tl)p(Dr|Tr) if i is an internal node with child nodes
l and r, and Z is the partition function

(3)






p(Di) if i is a leaf node

The term 1
Z (Di)p(Dl|Tl)p(Dr|Tr) corresponds to a product of experts model (Hinton
2002) comprising two competing hypotheses.1 Hypothesis H1 is given by p(Di) and
assigns all the data points in Di into a single cluster. Hypothesis H2 is given by

1 The review version of this paper and our previous work (Can and Manandhar 2012) missed this

connection to the product of experts model and did not include the 1/Z term. We thank our reviewers for
spotting this error.

(1)

(2)

353

Computational Linguistics

Volume 44, Number 2

p(Dl|Tl)p(Dr|Tr) and recursively splits the data in Di into two partitions (i.e., subtrees)
Dl and Dr. The factor Z is the partition function or the normalizing constant given by:

Z =

(cid:88)

Di,Dl,Dr:Di=Dl∪Dr

p(Di)p(Dl|Tl)p(Dr|Tr)

(4)

The recursive partitioning scheme we use is similar to that of Heller and
Ghahramani (2005). The product of experts scheme used in this paper contrasts with
the more conventional sum of experts mixture model of Heller and Ghahramani (2005)
that would have resulted in the mixture π p(Di) + (1 − π) p(Dl|Tl)p(Dr|Tr), where π
denotes the mixing proportion.

Finally, given the collection of trees T = {T1, T2, . . . , Tn}, the total likelihood of data

in all trees is deﬁned as follows:

(5)

(6)

(7)

(8)

(9)

Trees are generated from a DP. Let T = {T1, T2, . . . , Tn} be a set of trees. Ti is

sampled from a DP as follows:

p(D|T) =

p(Di|Ti)

|T|
(cid:89)

i

F ∼ DP(α, U)

Ti ∼ F

p(Tk|T, α, U) =

Nk
N + α

p(T|T|+1|T, α, U) =

α/k
N + α

where α denotes the concentration parameter of the DP. U is a uniform base distribution
that assigns equal probability to each tree. We integrate out F, which is a distribution
over trees, instead of estimating it. Hence, the conditional probability of sampling an
existing tree is computed as follows:

where Nk denotes the number of words in Tk and N denotes the total number of words
in the model. A new tree is generated with the following:

3.2 Modeling Morphology with Probabilistic Hierarchical Clustering

In our model, data points are the words and each tree node corresponds to a morpho-
logical paradigm (see Figure 4). Each word is part of all the paradigms on the path from
the leaf node having that word until the root node. The word can share either its stem or
sufﬁx with other words in the same paradigm. Hence, a considerable number of words
can be generated through this approach that may not be seen in the corpus. Our model
will prefer words that share stems or sufﬁxes to be close to each other within the tree.

354

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 4
A sample hierarchical tree structure that illustrates the clusters in each node (i.e., paradigms).
Each node corresponds to a cluster (i.e., morphological paradigm) and the leaf nodes correspond
to input data. The ﬁgure shows the ideal forest of trees that one expects given the input data.

The plate diagram of the generative model is given in Figure 5(a). Given a child
i ) and a

node i, we deﬁne a Dirichlet process to generate stems (denoted by s1
i , . . . , mNi
separate Dirichlet process to generate sufﬁxes (denoted by m1
i ):

i , . . . , sNi

Gs

i ∼ DP(βs, Ps)
sj
i ∼ Gs
i
i ∼ DP(βm, Pm)
Gm
mj
i ∼ Gm
i
(cid:89)

p(c)

Ps(sj

i) =

Pm(mj

i) =

p(c)

c∈sj
i
(cid:89)

c∈mj
i

where DP(βs, Ps) is a Dirichlet process that generates stems, βs denotes the concentra-
i is a distribution over the stems sj
tion parameter, and Ps is the base distribution. Gs
i
in node i. Correspondingly, DP(βm, Pm) is a Dirichlet process that generates sufﬁxes
with analogous parameters. Gm
i in node i. For
i
smaller values of the concentration parameter, it is less likely to generate new types.
Thus, sparse multinomials can be generated by the Dirichlet process yielding a skewed
distribution. We set βs < 1 and βm < 1 in order to generate a small number of stem and
sufﬁx types. sj

i are the jth stem and sufﬁx instance in the ith node, respectively.

is a distribution over the sufﬁxes mj

i and mj

The base distributions for stems and sufﬁxes are given by Equation (14) and Equa-
tion (15). Here, c denotes a single letter or character. We assume that letters are dis-
tributed uniformly (Creutz and Lagus 2005b), where p(c) = 1/A for an alphabet having
A letters. Our model will favor shorter morphemes because they have less factors in the
joint probability given by Equations (14) and (15).

For the root nodes we use HDPs that share global DPs (denoted by Hs for stems
and Hm for sufﬁxes). These introduce dependencies between stems/sufﬁxes in distinct

(10)

(11)

(12)

(13)

(14)

(15)

355

Computational Linguistics

Volume 44, Number 2

Figure 5
The DP model for the child nodes (on the left) illustrates the generation of words talking, cooked,
yelling. Each child node maintains its own DP that is independent of DPs from other nodes. The
HDP model for the root nodes (on the right) illustrates the generation of words yelling, talking,
repairs. In contrast to the DPs in the child nodes, in the HDP model, the stems/sufﬁxes are
shared across all root HDPs.

trees. The model will favor stems and sufﬁxes that are already generated in one of the
trees. The HDP for a root node i is deﬁned as follows:

Fs
i ∼ DP(βs, Hs)
Hs ∼ DP(αs, Ps)
sj
i ∼ Fs
i
ψz
i ∼ Hs
Fm
i ∼ DP(βm, Hm)
Hm ∼ DP(αm, Pm)
mj
i ∼ Fm
i
i ∼ Hm
φz

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

where the base distributions Hs and Hm are drawn from the global DPs DP(αs, Ps) and
DP(αm, Pm). Here, ψz
i denotes the sufﬁx type z
in node i, which are drawn from Hs and Hm (i.e., the global DPs), respectively. The plate
diagram of the HDP model is given in Figure 5(b).

i denotes the stem type z in node i, and φz

From the Chinese restaurant process (CRP) metaphor perspective, the global DPs
generate the global sets of dishes (i.e., stems and sufﬁxes) that constitute the menu for
all trees in the model. At each tree node, there are two restaurants: one for stems and
one for sufﬁxes. At each table, a different type of dish (i.e., stem or sufﬁx type) is served

356

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 6
A depiction of the Chinese restaurant franchise (i.e., global vs. local CRPs).
S1 = {walk, order, sleep, etc.}, M1 = {s, ing}, S2 = {pen, book}, M2 = {0, s}, S3 = {walk, order},
M3 = {0, s}, where 0 denotes an empty sufﬁx. For each stem type in the distinct trees, a customer
is inserted in the global restaurant. For example, there are two stem customers that are being
served the stem type walk because walk exists in two different trees.

i or φz

and for each stem/sufﬁx type there exists only one table in each node (i.e., restaurant).
Customers are the stem or sufﬁx tokens. Whenever a new customer, sj
i, enters a
restaurant, if the table, ψz
i , serving that dish already exists, the new customer sits
at that table. Otherwise, a new table is generated in the restaurant. A change in one of
the restaurants in the leaf nodes leads to the update in each restaurant all the way to
the root node. If the dish is not available in the root node, a new table is created for that
root node and a global customer is also added to the global restaurant. If no global table
exists for that dish, a new global table serving the dish is also created. This can be seen
as a Chinese restaurant franchise where each node is a restaurant itself (see Figure 6).

i or mj

In order to calculate the joint likelihood of the model, we need to consider both trees
and global stem/sufﬁx sets (i.e., local and global restaurants). The model is exchange-
able because it is a CRP—which means that the order the words are segmented does not
alter the joint probability. The joint likelihood of the entire model for a given collection
of trees T = {T1, T2, . . . , Tn} is computed as follows:

p(D|T) =

p(Di|Ti)

|T|
(cid:89)

|T|
(cid:89)

i

i

=

p(Si|Ti)p(Mi|Ti)

= p(Sroot)p(Mroot)

p(Si|Ti)p(Mi|Ti)

|T|
(cid:89)

i=1
i(cid:54)=root

(24)

(25)

(26)

357

Computational Linguistics

Volume 44, Number 2

where p(Si|Ti) and p(Mi|Ti) are computed recursively:

(cid:26) 1

p(Si|Ti) =

Z p(Si)p(Sl|Tl)p(Sr|Tr) if i is an internal node with child nodes l and r
p(Si) if i is a leaf node

where Z is the normalization constant. The same also applies for Mi.

Following the CRP, the joint probability of the stems in each root node Ti, Srooti
i , . . . , sNi

i }, is:

{s1

i , s2

p(Sroot) =

p(s1

i , s2

i , . . . , sNi
i )

|T|
(cid:89)

i

|T|
(cid:89)

i






=

p(s1

i )p(s2

i |s1

i ) . . . p(sNi

i

i , . . . , sNi−1
|s1

i

)

=



|T|
(cid:89)





i

Γ(βs)
Γ(Ni + βs)

i −1

βLs
s





(ns

ij − 1)!





Γ((cid:80)

Γ(αs)
sj∈Ks ks

j + αs)

αKs−1
s



(ks

j − 1)!Ps(sj)



Ls
i(cid:89)

j=1

Ks
(cid:89)

j=1

(27)

=

(28)

(29)

(30)

where the ﬁrst line in Equation (30) corresponds to the stem CRPs in the root nodes
of the trees (see Equation (2.22) in Can [2011] and Equation (A.1) in Appendix A). The
second line in Equation (30) corresponds to the global CRP of stems. The second factor in
the ﬁrst line of Equation (30) corresponds to the case where Ls
i stem types are generated
the ﬁrst time, and the third factor in the ﬁrst line corresponds to the case, where for each
of the Ls
ij stem tokens of type j. The ﬁrst factor accounts
for all denominators from both cases. Similarly, the second and the fourth factor in the
second line of Equation (30) corresponds to the case where Ks stem types are generated
globally (i.e., stem tables in the global restaurant), the third factor corresponds to the
case where, for each of the Ks stem types, there are ks
j stems of type j in distinct trees.
A sample hierarchical structure is given in Figure 7.

i stem types at node i, there are ns

The joint probability of stems in a child node Ti, Si = {s1

i , s2

i , . . . , sNi

i }, which belong

to stem tables {ψ1

i , ψ2

i , . . . } that index the items on the global menu is reduced to:

p(Si) =

Γ(βs)
Γ(Ni + βs)

i −1

βLs
s

(ns

ij − 1)!Ps(ψj
i)

(cid:17)

Ls
i(cid:89)

(cid:16)

j=1

(31)

Whenever a new stem is added to a node i (i.e., new customer enters one of the
i that belongs to type z

local restaurants), the conditional probability of the new stem sj

358

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 7
A sample hierarchical structure that contains D = {walk + ed, talk + ed, order + ed, walk + ing,
talk + ing} and the corresponding global tables. Here s1
node k with frequency 1.

k = walk/1 denotes the ﬁrst stem walk in

(i.e., customer sj
2006):

i sitting at table ψz

i ) is computed as follows (see Teh 2010 and Teh et al.

p(s j

i = z|Si

−s j

i , βs, Ps) =

otherwise (for an internal node)

(32)

if ψz

i ∈ Ψi






n

−s j
i
iz
Ni − 1 + βs
βsPs(s j
i)
Ni − 1 + βs
βsHs(ψz

i |αs, Ps)

Ni − 1 + βs

otherwise (for a root node)

where Ψi denotes the table indicators in node i, n
that belong to type z in node i when the last instance sj

i is excluded.

−sj
i
iz denotes the number of stem tokens

If the stem (customer) does not exist in the root node (i.e., chooses a non-existing

dish type in the root node), the new stem’s probability is calculated as follows:

Hs(ψz

i |αs, Ps) =

j + αs





(cid:80)

(cid:80)

ks
z
sj∈Ks ks
αsPs(sz)
sj∈Ks ks

j + αs

if j ∈ Ks

otherwise

If the stem type (dish) exists in the global clusters (i.e., global menu), it is chosen with the
probability proportional to the number of trees that contain the stem type (i.e., number
of global stem customers that are served the dish). Otherwise, the new stem (dish type)
is chosen based on the base distribution Ps.

(33)

359

Computational Linguistics

Volume 44, Number 2

Analogously to Equations (30)–(33) that apply to stems, the corresponding equa-

tions for sufﬁxes are given by Equations (34)–(37).




p(Mroot) =





Γ(βm)
Γ(Ni + βm)

βLm
i −1
m





(nm

ij − 1)!





Lm
i(cid:89)

j=1

|T|
(cid:89)

i





Γ((cid:80)

Γ(αm)
mj∈Km km

j + αm)

αKm−1
m

Km
(cid:89)

j=1



(km

j − 1)!Pm(mj)



p(Mi) =

Γ(βm)
Γ(Ni + βm)

βLm
i −1
m

(nm

ij − 1)!Pm(φj
i)

(cid:17)

Lm
i(cid:89)

(cid:16)

j=1

p(mj

i = z|Mi

−mj

i , βm, Pm) =

if φz

i ∈ Φi

n

−mj
i
iz
Ni − 1 + βm
βmPm(mj
i)
Ni − 1 + βm
βmHm(φz






i |αm, Pm)

Ni − 1 + βm

otherwise (for an internal node)

otherwise (for a root node)

Hm(φz

i |αm, Pm) =





(cid:80)

(cid:80)

km
z
mj∈Km km
αmPm(mz)
mj∈Km km

j + αm

j + αm

if j ∈ Km

otherwise

(34)

(35)

(36)

(37)

i , . . . , mNi

i , m1
i , . . . }; Nm
i

where Mi = {m1
types {φ1
tokens of type j in node i; Km is the total number of sufﬁx types; and km
j
trees that contain sufﬁxes of type j.

i } is the set of sufﬁxes in node i belonging to global sufﬁx
ij is the number of sufﬁx
is the number of

is the number of local sufﬁx types; nm

i , φ2

3.3 Metropolis-Hastings Sampling for Inferring Trees

Trees are learned along with the segmentations of words via inference. Learning trees
involves two steps: 1) constructing initial trees; 2) Metropolis-Hastings sampling for
inferring trees.

3.3.1 Constructing Initial Trees. Initially, all words are split at random points with uniform
probability. We use an approximation to construct the intial trees. Instead of computing
the full likelihood of the model, for each tree, we only compute the likelihood of a single
DP and assume that all words belong to this DP. The conditional probability of inserting
word wj = s + m in tree Tk is given by:

p(Tk, w j = s + m|D, T, α, βs, βm, Ps, Pm, U)
= p(Tk|T, α, U)p(wj = s + m|Dk)
= p(Tk|T, α, U)p(s|Sk, βs, Ps)p(m|Mk, βm, Pm)

(38)

360

Can and Manandhar

Tree Structured Morphological Segmentation

We use Equation (8) and (9) for computing the conditional probability p(Tk|T, α, U) of
choosing a particular tree. Once a tree is chosen, a branch to insert is selected at random.
The algorithm for constructing the initial trees is given in Algorithm 1.

3.3.2 Metropolis-Hastings Sampling. Once the initial trees are constructed, the hierar-
chical structures yielding a global maximum likelihood are inferred by using the
Metropolis Hastings algorithm (Hastings 1970). The inference is performed by itera-
tively removing a word from a leaf node from a tree and subsequently sampling a new
tree, a position within the tree, and a new segmentation (see Algorithm 2). Trees are
sampled with the conditional probability given in Equations (8) and (9). Hence, trees
with more words attract more words, and new trees are created proportional to the
hyperparameter α.

Once a tree is sampled, we draw a new position and a new segmentation. The word
is inserted at the sampled position with the sampled segmentation. The new model is
either accepted or rejected with the Metropolis-Hastings accept-reject criteria. We also
use a simulated annealing cooling schedule by assigning an initial temperature γ to the
system and decreasing the temperature at each iteration. The accept probability we use
is given by:

PAcc =

(cid:19) 1
γ

(cid:18) pnext(D|T)
pcur(D|T)

(39)

where pnext(D|T) denotes the likelihood of the data under the altered model and
pcur(D|T) denotes the likelihood of data under the current model before sampling. The
normalization constant cancels out because it is the same for the numerator and the
1
denominator. If pnext(D|T)
γ , then the new sample is accepted: otherwise,

γ > pcur(D|T)

1

Algorithm 1 Construction of the initial trees.

1: input: data D := {w1 = s1 + m1, . . . , wN = sN + mN}
2: initialize: The trees in the model: T := ∅
3: For j = 1 . . . N
4: Choose wj from the corpus.
5: Sample, Tk as a new empty tree or existing tree from T, and, a split point wj = s + m

from the joint distribution (see Equation 38):
p(Tk, wj = s + m|D, T, α, βs, βm, Ps, Pm, U)

6: if Tk ∈ T then
7: Draw a node s from Tk with uniform probability
8: Assign Tk as a sibling of Ts: Ts := Ts + Tk (i.e., Ts is new tree with Tk and old Ts

as children)

Tk is an empty tree. Add Tk into T: T := T ∪ Tk

9: else
10:
11: Dk := {wj = s + m} (i.e., word wj = s + m is assigned into Tk)
12: end if
13: Remove wj from the corpus
14: End For
15: output: T

361

Computational Linguistics

Volume 44, Number 2

Algorithm 2 Learning the trees with Metropolis-Hastings algorithm

1: input: data D := {w1 = s1 + m1, . . . , wN = sN + mN}, initial trees T, initial

temperature γ, the target temperature κ, temperature decrement η

Choose the leaf node j from all trees with uniform probability
Let Dj := {wj = sold + mold}

2: initialize: pcur(D|T) := p(D|T)
3: while γ > κ do
4:
5:
6: Draw a split point wj = snew + mnew with uniform probability
7: Draw a tree Tk with probability p(Tk|T, α, U) (see Equations 8 and 9)
8:

if Tk ∈ T then

Draw a sibling node s from Tk with uniform probability
Ts := Ts + Tk (see Figure 8)

9:
10:
11:
12:
13:
14:
15:

16:
17:

18:

else

Create a new tree Tk
T := T ∪ Tk
Dk := {wj = snew + mnew}

end if
rand ∼ Normal(0, 1)
pnext := p(D|T) (see Equation 24)

if pnext(D|T) >= pcur(D|T) or rand <
Accept the new tree structure
pcur(D|T) := pnext(D|T)

19:
20:
21:
22:
23: end while
24: output: T

end if
γ := γ − η

(cid:16) pnext(D|T)
pcur(D|T)

(cid:17) 1
γ

then

the new model is still accepted with a probability pAcc. The system is cooled in each
iteration with decrements η. We refer to Section 4 for details of parameter settings.

γ ← γ − η

(40)

3.4 Morphological Segmentation

Once the model is learned, it can be used for the segmentation of novel words. We
use only root nodes for the segmentation. Viterbi decoding is used in order to ﬁnd the
morphological segmentation of each word having the maximum probability:

arg

max
s1,··· ,sa,m1,...,mb

p(wk = s1, · · · , sa, m1, · · · , mb|D, αs, βs, Hs, Ps, αm, βm, Hm, Pm)

a
(cid:89)

|T|
(cid:88)

=

j=1

b
(cid:89)

i=1
i=root
|T|
(cid:88)

j=1

i=1
i=root

p(sj

i|Si, αs, βs, Hs, Ps)

p(mj

i|Mi, αm, βm, Hm, Pm)

(41)

where wk denotes the kth word to be segmented in the test set.

362

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 8
A sampling step in Metropolis-Hastings algorithm.

3.5 Example Paradigms

A sample of root paradigms learned by our model for English is given in Table 1. The
model can ﬁnd similar word forms (i.e., separat+ists, medal+ists, hygien+ists) that are
grouped in the neighbor branches in the tree structure (see Figures 9, B.1, and B.2 for
sample paradigms learned in English, Turkish, and German).

Paradigms are captured based on the similarity of either stems or sufﬁxes. Having
the same stem such as co-chair (co-chairman, co-chairmen) or trades (trades+man,
trades+men) allows us to ﬁnd segmentations such as co-chair+man vs. co-chair+men
and trades+man vs. trades+men. Although we assume a stem+sufﬁx segmentation,
other types of segmentation, such as preﬁx+stem, are also covered. However, stem
alterations and inﬁxation are not covered in our model.

4. Evaluation

We used publicly available Morpho Challenge data sets for English, German, and
Turkish for training. The English data set consists of 878,034 words, the German data set
consists of 2,338,323 words, and the Turkish data set consists of 617,298 words. Although
frequency of each word was available in the training set, we did not make use of this

363

Computational Linguistics

Volume 44, Number 2

Table 1
Example paradigms in English.

{ﬁnal, ungod, frequent, pensive} {ly}

{kind, kind, kind} {est, er, 0}

{underrepresent, modul} {ation}

{plebe, hawai, muslim-croat} {ian}

{compuls, shredd, compuls, shredd, compuls} {ion, er, ively, ers, ory}

{zion, modern, modern, dynam} {ism, ists}

{reclaim, chas, pleas, fell} {ing}

{mov, engrav, stray, engrav, fantasiz, reischau,
decilit, suspect} {ing, er, e}

{measur, measur, incontest, transport, unplay, reput}
{e, able}

{housewar, decorat, entitl, cuss, decorat, entitl, materi, toss, ﬂay, unconﬁrm
linse, equipp} {es, ing, alise, ed}

{fair, norw, soon, smooth, narrow, sadd, steep, noisi, statesw, narrow}
{est, ing}

{rest, wit, name, top, odor, bay, odor, sleep} {less, s}

{umpir, absorb, regard, embellish, freez, gnash} {ing}

{nutrition, manicur, separat, medal, hygien, nutrition, genetic, preservation}
{0, ists}

information. In other words, we use only the word types (not tokens) in training. We do
not address the ambiguity of words in this work and leave this as future research.

In all experiments, the initial temperature of the system is set γ = 2 and it is
reduced to γ = 0.01 with decrements η = 0.0001 (see Equation (39)). Figure 10 shows
the time required for the log likelihoods of the trees of sizes 10K, 16K, and 22K to
converge. We ﬁxed αs = αm = βs = βm = 0.01 and α = 0.0005 in all our experiments.
The hyperparameters are set manually as a result of several experiments. These are the
optimum values obtained from a number of experiments.2

Precision, recall, and F-score values against training set sizes are given in Figures 11

and 12 for English and Turkish, respectively.

4.1 Morpho Challenge Evaluation

Although we experimented with different sizes of training sets, we used a randomly
chosen 600K words from the English and 200K words from the Turkish and German
data sets for evaluation purposes.

Evaluation is performed according to the method proposed in Morpho Challenge
(Kurimo et al. 2010a), which in turn is based on evaluation used by Creutz and Lagus
(2007). The gold standard evaluation data set utilized within the Morpho Challenge is
a hidden set that is not available publicly. This makes the Morpho Challenge evaluation
different from other evaluations that provide test data. In this evaluation, word pairs

2 The source code of the model is accessible at: https://github.com/burcu-can/TreeStructuredDP.

364

Can and Manandhar

Tree Structured Morphological Segmentation

Figure 9
Sample hierarchies in English, Turkish, and German.

sharing at least one common morpheme are sampled. For precision, word pairs are
sampled from the system results and checked against the gold standard segmentations.
For recall, word pairs are sampled from the gold standard and checked against the
system results. For each matching morpheme, 1 point is given. Precision and recall are
calculated by normalizing the total obtained scores.

We compare our results with other unsupervised systems from Morpho Challenge
2010 (Kurimo et al. 2010b) for English, German, and Turkish. More speciﬁcally, we com-
pare our model with all competing unsupervised systems: Morfessor Baseline (Creutz
and Lagus 2002), Morfessor CATMAP (Creutz and Lagus 2005a), Base Inference (Lignos
2010), Iterative Compounding (Lignos 2010), Aggressive Compounding (Lignos 2010),
and Nicolas, Farr´e, and Molinero (2010). Additionally, we compare our system with the
Morpho Chain model of Narasimhan, Barzilay, and Jaakkola (2015) by re-training their
model on exactly the same training sets as ours. All evaluation was carried out by the
Morpho Challenge organizers based on the hidden gold data sets.

365

Computational Linguistics

Volume 44, Number 2

Figure 10
The likelihood convergence in time (in minutes) for data sets of size 16K and 22K.

Figure 11
English results for different size of data.

English results are given in Table 2. For English, the Base Inference algorithm of
Lignos (2010) obtained the highest F-measure in Morpho Challenge 2010 among other
competing unsupervised systems. Our model is ranked fourth among all unsupervised
systems with a F-measure 60.27%.

German results are given in Table 3. Our model outperforms other unsupervised

models in Morpho Challenge 2010 with a F-measure 50.71%.

Turkish results are given in Table 4. Again, our model outperforms other unsuper-

vised participants, achieving a F-measure 56.41%.

Our model also outperforms the model from Can and Manandhar (2012) (which
we refer to as Single Tree Probabilistic Clustering), although the results are not directly
comparable because the largest data set we were able to train that model on was 22K
words due to the training time required. The full training set provided by Morpho
Challenge was not used in Can and Manandhar (2012). Our current approach is more
efﬁcient as the training cost is divided across multiple tree structures with each tree
being shallower compared with our previous model.

366

Can and Manandhar

Tree Structured Morphological Segmentation

Table 2
Morpho Challenge 2010 experimental results for English.

System

Precision (%) Recall (%)

F-measure (%)

Figure 12
Turkish results for different size of data.

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)
Base Inference (Lignos 2010)
Iterative Compounding (Lignos 2010)
Aggressive Compounding (Lignos 2010)
Nicolas (Nicolas, Farr´e, and Molinero 2010)
Morfessor Baseline (Creutz and Lagus 2002)
Morpho Chain (Narasimhan, Barzilay, and
Jaakkola 2015)
Morfessor CatMAP (Creutz and Lagus 2005a)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)
Base Inference (Lignos 2010)
Iterative Compounding (Lignos 2010)
Aggressive Compounding (Lignos 2010)
Morfessor Baseline (Creutz and Lagus 2002)
Morfessor CatMAP (Creutz and Lagus 2005a)

55.60
55.60

80.77
80.27
71.45
67.83
81.39
74.87

86.84

47.92
57.79

66.38
62.13
59.41
82.80
72.70

65.80
57.58

53.76
52.76
52.31
53.43
41.70
39.01

30.03

53.84
32.42

35.36
34.70
37.21
19.77
35.43

Table 3
Morpho Challenge 2010 experimental results for German.

System

Precision (%) Recall (%)

F-measure (%)

In order to draw a substantive empirical comparison, we performed another set
of experiments by running the current approach on only 22K words as the Single Tree
Probabilistic Clustering (Can and Manandhar 2012). Results are given in Tables 5, 6, and
7. As shown in the tables, the current model outperforms the previous model even on
the smaller data set.

60.27
57.33

64.55
63.67
60.40
59.78
55.14
50.42

44.63

50.71
41.54

46.14
44.53
45.76
31.92
47.64

367

Computational Linguistics

Volume 44, Number 2

Table 4
Morpho Challenge 2010 experimental results for Turkish.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)
Base Inference (Lignos 2010)
Iterative Compounding (Lignos 2010)
Aggressive Compounding (Lignos 2010)
Nicolas (Nicolas, Farr´e, and Molinero 2010)
Morfessor Baseline (Creutz and Lagus 2002)
Morpho Chain (Narasimhan, Barzilay, and
Jaakkola 2015)
Morfessor CatMAP (Creutz and Lagus 2005a)

57.70
72.36

72.81
68.69
55.51
79.02
89.68
69.25

79.38

55.18
25.81

16.11
21.44
34.36
19.78
17.78
31.51

31.88

56.41
38.04

26.38
32.68
42.45
31.64
29.67
43.32

45.49

Table 5
Comparison with Single Tree Probabilistic Clustering for English.

System

Precision (%) Recall (%) F-measure (%)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)

67.75
55.60

53.93
57.58

60.06
57.33

Table 6
Comparison with Single Tree Probabilistic Clustering for German.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)

33.93
57.79

65.31
32.42

44.66
41.54

Table 7
Comparison with Single Tree Probabilistic Clustering for Turkish.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Single Tree Prob. Clustering (Can and Manandhar
2012)

64.39
72.36

42.99
25.81

51.56
38.04

4.2 Additional Evaluation

For additional experiments, we compare our model with Morpho Chain (Narasimhan,
Barzilay, and Jaakkola 2015) based on their evaluation method that differs from the
Morpho Challenge evaluation method. Their evaluation method is based on counting
the correct segmentation points. For example, if the result segmentation is booking+s
and the gold segmentation is book+ing+s, 1 point is counted. Precision and recall are

368

Can and Manandhar

Tree Structured Morphological Segmentation

Table 8
Comparison with Morpho Chain model for English based on Morpho Chain evaluation.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Morpho Chain

67.41
72.63

62.5
78.72

64.86
75.55

Table 9
Comparison with Morpho Chain model for Turkish based on Morpho Chain evaluation.

System

Precision (%) Recall (%)

F-measure (%)

Hierarchical Morphological Segmentation
Morpho Chain

89.30
70.49

48.22
63.27

62.63
66.66

calculated based on these matching segmentation points. In addition, this evaluation
does not use the hidden gold data sets. Instead, the test sets are created by aggregating
the test data from Morpho Challenge 2005 and Morpho Challenge 2010 (as reported in
Narasimhan, Barzilay, and Jaakkola [2015]) that provide segmentation points.3

We used the same trained models as in our Morpho Challenge evaluation. The

English test set contains 2,218 words and the Turkish test set contains 2,534 words.4
The English results are given in Table 8 and Turkish results are given in Table 9.
For all systems, Morpho Chain evaluation scores are comparably higher than the
Morpho Challenge scores. There are several reasons for this. In the Morpho Challenge
evaluation, the morpheme labels are considered rather than the surface forms of the
morphemes. For example, pantolon+u+yla [with his trousers] and emel+ler+i+yle [with
his desires] have got both possessive morpheme (u and i) that is labeled with POS and
relational morpheme (yla and yle) labeled with REL in common. This increases the total
number of points that is computed over all word pairs, and therefore lowers the scores.
Secondly, in the Morpho Chain evaluation, only the gold segmentation that has the
maximum match with the result segmentation is chosen for each word (e.g., yazımıza
has two gold segmentations: yaz+ı+mız+a [to our summer] and yazı+mız+a; [to our
writing]). In contrast, in the Morpho Challenge evaluation all segmentations in the
gold segmentation are evaluated. This is another factor that increases the scores in
Morpho Chain evaluation. Thus, the Morpho Chain evaluation favors precision over
recall. Indeed, in the Morpho Challenge evaluation, the Morpho Chain system has high
precision but their model suffers from low recall due to undersegmentation (see Tables 2
and 4).

It should be noted that the output of our system is not only the segmentation points,
but also the hierarchical organization of morphological paradigms that we believe is
novel in this work. However, because of the difﬁculty in measuring the quality of hier-
archical paradigms, which will require a corresponding hierarchically organized gold
data set, we are unable to provide an objective measure of the quality of hierarchical

3 In addition to the hidden test data, Morpho Challenge also provides separate publicly available test data.
4 Because of the unavailability of word2vec word embeddings for German, we were unable to perform

Morpho Chain evaluation on this language.

369

Computational Linguistics

Volume 44, Number 2

structures learned. We present different portions from the obtained trees in Appendix B
(see Figures B.1 and B.2).5 It can be seen that words sharing the same sufﬁxes are gath-
ered closer to each other, such as reestablish+ed, reclassiﬁ+ed, circl+ed, uncloth+ed, and so
forth. Secondly, related morphological families gather closer to each other, such as
impress+ively, impress+ionist, impress+ions, impress+ion.

5. Conclusions and Future Work

In this article, we introduce a tree structured Dirichlet process model for hierarchical
morphological segmentation. The method is different compared with existing hierar-
chical and non-hierarchical methods for learning paradigms. Our model learns mor-
phological paradigms that are clustered hierarchically within a forest of trees.

Although our goal in this work is on hierarchical learning, our model shows com-
petitive performance against other unsupervised morphological segmentation systems
that are designed primarily for segmentation only. The system outperforms other un-
supervised systems in Morpho Challenge 2010 for German and Turkish. It also out-
performs the more recent Morpho Chain (Narasimhan, Barzilay, and Jaakkola 2015)
system on the Morpho Challenge evaluation for German and Turkish.

The sample paradigms learned show that these can be linked to other types of latent
information, such as part-of-speech tags. Combining morphology and syntax as a joint
learning problem within the same model can be a fruitful direction for future work.

The hierarchical structure is beneﬁcial because we can have both compact and more
general paradigms at the same time. In this article, we use the paradigms only for the
segmentation task, and applications of hierarchy learned is left as future work.

Appendix A. Derivation of the Full Joint Distribution

Let D = {s1, s2, . . . , sN} denote the input data where each sj is a data item. A particular
setting of a table with N customers has a joint probability of:

p(s1, . . . , sN|α, P) = p(s1|α, P)p(s2|s1, α, P)p(s3|s1, s2, α, P) . . . p(sN|s1, . . . , sN−1, α, P)

=

αL
α(1 + α)(2 + α)(N − 1 + α)

P(si)

(nsi − 1)!

L
(cid:89)

i=1

=

Γ(α)
Γ(N + α)

αL−1

L
(cid:89)

i=1

(nsi − 1)!

P(si)

L
(cid:89)

i=1

L
(cid:89)

i=1

(A.1)

For each customer, either a new table is created or the customer sits at an occupied table.
For each table, at least one table creation is performed, which forms the second and
the fourth factor in the last equation. Once the table is created, factors that represent the
number of customers sitting at each table are chosen accordingly, which refers to the
third factor in the last equation. All factors with α are aggregated in the ﬁrst factor in
terms of a Gamma function in the last equation.

5 Some of the full trees are given at http://web.cs.hacettepe.edu.tr/~burcucan/TreeStructuredDP.

370

Can and Manandhar

Tree Structured Morphological Segmentation

Appendix B. Sample Tree Structures

Figure B.1
Sample hierarchies in English.

371

Computational Linguistics

Volume 44, Number 2

Figure B.2
Sample hierarchies in Turkish.

Acknowledgments
This research was supported by TUBITAK
(The Scientiﬁc and Technological Research
Council of Turkey) grant number 115E464.
We thank Karthik Narasimhan for providing
their data sets and code. We are grateful to
Sami Virpioja for the evaluation of our
results on the hidden gold data provided by
Morpho Challenge. We thank our reviewers
for critical feedback and spotting an error in
our previous version of the article. Their
comments have immensely helped improve
the article.

References
Can, Burcu. 2011. Statistical Models for

Unsupervised Learning of Morphology and
POS tagging. Ph.D. thesis, Department of

Computer Science, The University
of York.

Can, Burcu and Suresh Manandhar. 2010.
Clustering morphological paradigms
using syntactic categories. In Multilingual
Information Access Evaluation I. Text
Retrieval Experiments: 10th Workshop of the
Cross-Language Evaluation Forum, CLEF
2009, Corfu, Greece, September 30 -
October 2, 2009, Revised Selected Papers,
pages 641–648, Corfu.

Can, Burcu and Suresh Manandhar. 2012.
Probabilistic hierarchical clustering of
morphological paradigms. In Proceedings of
the 13th Conference of the European Chapter of
the Association for Computational Linguistics,
EACL ’12, pages 654–663, Avignon.
Chan, Erwin. 2006. Learning probabilistic
paradigms for morphology in a latent
class model. In Proceedings of the Eighth

372

Can and Manandhar

Tree Structured Morphological Segmentation

Meeting of the ACL Special Interest
Group on Computational Phonology and
Morphology, SIGPHON ’06, pages 69–78,
New York, NY.

Creutz, Mathias and Krista Lagus. 2002.

Unsupervised discovery of morphemes.
In Proceedings of the ACL-02 Workshop on
Morphological and Phonological Learning -
Volume 6, MPL ’02, pages 21–30,
Philadelphia, PA.

Creutz, Mathias and Krista Lagus. 2005a.

Inducing the morphological lexicon of a
natural language from unannotated text.
In Proceedings of the International and
Interdisciplinary Conference on Adaptive
Knowledge Representation and Reasoning
(AKRR 2005), pages 106–113, Espoo.
Creutz, Mathias and Krista Lagus. 2005b.
Unsupervised morpheme segmentation
and morphology induction from text
corpora using Morfessor 1.0. Technical
Report A81. Helsinki University of
Technology.

Creutz, Mathias and Krista Lagus. 2007.
Unsupervised models for morpheme
segmentation and morphology learning.
ACM Transactions Speech Language
Processing, 43:1–3:34.

Dreyer, Markus and Jason Eisner. 2011.

Discovering morphological paradigms
from plain text using a Dirichlet Process
mixture model. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 616–627,
Edinburgh.

Goldsmith, John. 2001. Unsupervised

learning of the morphology of a natural
language. Computational Linguistics,
27(2):153–198.

Hastings, W. K. 1970. Monte Carlo sampling
methods using Markov chains and their
applications. Biometrika, 57:97–109.

Heller, Katherine A. and Zoubin

Ghahramani. 2005. Bayesian hierarchical
clustering. In Proceedings of the 22nd
International Conference on Machine
Learning, ICML ’05, pages 297–304,
Bonn.

Hinton, Geoff. 2002. Training products of

experts by minimizing contrastive
divergence. Neural Computation,
14(8):1771–1800.

Kurimo, Mikko, Sami Virpioja, Ville

Turunen, and Krista Lagus. 2010a. Morpho
Challenge Competition 2005–2010:
Evaluations and results. In Proceedings of
the 11th Meeting of the ACL Special Interest
Group on Computational Morphology and
Phonology, SIGMORPHON ’10,
pages 87–95, Uppsala.

Kurimo, Mikko, Sami Virpioja, Ville T.

Turunen, Graeme W. Blackwood, and
William Byrne. 2009. Overview and results
of Morpho Challenge 2009. In Proceedings
of the 10th Cross-Language Evaluation Forum
Conference on Multilingual Information Access
Evaluation: Text Retrieval Experiments,
CLEF’09, pages 578–597, Corfu.
Kurimo, Mikko, Sami Virpioja, Ville T.
Turunen, Bruno G ´olenia, Sebastian
Spiegler, Oliver Ray, Peter Flach, Oskar
Kohonen, Laura Lepp¨anen, Krista Lagus,
Constantine Lignos, Lionel Nicolas,
Jacques Farr´e, and Miguel Molinero.
2010b. Proceedings of the Morpho
Challenge 2010 Workshop. Technical
Report TKK-ICS-R37. Aalto University.
Lignos, Constantine. 2010. Learning from

unseen data. In Proceedings of the Morpho
Challenge 2010 Workshop, pages 35–38,
Espoo.

Luo, Jiaming, Karthik Narasimhan, and
Regina Barzilay. 2017. Unsupervised
learning of morphological forests.
Transactions of the Association of
Computational Linguistics, 5:353–364.
Mikolov, Tomas, Kai Chen, Greg Corrado,

and Jeffrey Dean. 2013. Efﬁcient estimation
of word representations in vector space.
CoRR, abs/1301.3781.

Monson, Christian. 2008. Paramor: From
Paradigm Structure to Natural Language
Morphology Induction. Ph.D. thesis,
Language Technologies Institute, School of
Computer Science, Carnegie Mellon
University.

Monson, Christian, Jaime Carbonell, Alon
Lavie, and Lori Levin. 2008. Paramor:
Finding paradigms across morphology.
In Lecture Notes in Computer Science -
Advances in Multilingual and Multimodal
Information Retrieval; Cross-Language
Evaluation Forum, CLEF 2007,
pages 900–907, Springer, Berlin.

Narasimhan, Karthik, Regina Barzilay, and

Tommi S. Jaakkola. 2015. An unsupervised
method for uncovering morphological
chains. Transactions of the Association for
Computational Linguistics, 3:157–167.

Nicolas, Lionel, Jacques Farr´e, and Miguel A.
Molinero. 2010. Unsupervised learning of
concatenative morphology based on
frequency-related form occurrence. In
Proceedings of the Morpho Challenge 2010
Workshop, pages 39–43, Espoo.

Poon, Hoifung, Colin Cherry, and Kristina

Toutanova. 2009. Unsupervised
morphological segmentation with
log-linear models. In Proceedings of Human
Language Technologies: The 2009 Annual

373

Computational Linguistics

Volume 44, Number 2

Conference of the North American
Chapter of the Association for Computational
Linguistics, NAACL ’09, pages 209–217,
Boulder, CO.

Snover, Matthew G., Gaja E. Jarosz, and
Michael R. Brent. 2002. Unsupervised
learning of morphology using a novel
directed search algorithm: Taking the ﬁrst
step. In Proceedings of the ACL-02 Workshop
on Morphological and Phonological Learning,
pages 11–20, Philadelphia, PA.

Snyder, Benjamin and Regina Barzilay. 2008.
Unsupervised multilingual learning for
morphological segmentation. In
Proceedings of ACL-08: HLT, pages 737–745,
Columbus, OH.

Teh, Y. W. 2010. In Dirichlet processes, In

Encyclopedia of Machine Learning, Springer.
Teh, Y. W., M. I. Jordan, M. J. Beal, and D. M.
Blei. 2006. Hierarchical Dirichlet Processes.
Journal of the American Statistical
Association, 101(476):1566–1581.

374

