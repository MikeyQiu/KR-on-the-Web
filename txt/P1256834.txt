8
1
0
2
 
g
u
A
 
5
1
 
 
]

V
C
.
s
c
[
 
 
1
v
5
8
2
5
0
.
8
0
8
1
:
v
i
X
r
a

DNN Feature Map Compression using Learned
Representation over GF(2)

Denis A. Gudovskiy, Alec Hodgkinson, Luca Rigazio

Panasonic Beta Research Lab, Mountain View, CA, 94043, USA
{denis.gudovskiy,alec.hodgkinson,luca.rigazio}@us.panasonic.com

Abstract. In this paper, we introduce a method to compress interme-
diate feature maps of deep neural networks (DNNs) to decrease memory
storage and bandwidth requirements during inference. Unlike previous
works, the proposed method is based on converting ﬁxed-point activa-
tions into vectors over the smallest GF(2) ﬁnite ﬁeld followed by nonlin-
ear dimensionality reduction (NDR) layers embedded into a DNN. Such
an end-to-end learned representation ﬁnds more compact feature maps by
exploiting quantization redundancies within the ﬁxed-point activations
along the channel or spatial dimensions. We apply the proposed net-
work architectures derived from modiﬁed SqueezeNet and MobileNetV2
to the tasks of ImageNet classiﬁcation and PASCAL VOC object detec-
tion. Compared to prior approaches, the conducted experiments show a
factor of 2 decrease in memory requirements with minor degradation in
accuracy while adding only bitwise computations.

Keywords: feature map compression; dimensionality reduction; net-
work quantization; memory-eﬃcient inference

1 Introduction

Recent achievements of deep neural networks (DNNs) make them an attractive
choice in many computer vision applications including image classiﬁcation [1]
and object detection [2]. The memory and computations required for DNNs can
be excessive for low-power deployments. In this paper, we explore the task of
minimizing the memory footprint of DNN feature maps during inference and,
more speciﬁcally, ﬁnding a network architecture that uses minimal storage with-
out introducing a considerable amount of additional computations or on-the-ﬂy
heuristic encoding-decoding schemes. In general, the task of feature map com-
pression is tightly connected to an input sparsity. The input sparsity can de-
termine several diﬀerent usage scenarios. This may lead to substantial decrease
in memory requirements and overall inference complexity. First, a pen sketches
are spatially sparse and can be processed eﬃciently by recently introduced sub-
manifold sparse CNNs [3]. Second, surveillance cameras with mostly static input
contain temporal sparsity that can be addressed by Sigma-Delta networks [4].
A more general scenario presumes a dense input e.g. video frames from a high-
resolution camera mounted on a moving autonomous car. In this work, we ad-
dress the latter scenario and concentrate on feature map compression in order to

2

Denis Gudovskiy et al.

minimize memory footprint and bandwidth during DNN inference which might
be prohibitive for high-resolution cameras.

We propose a method to convert intermediate ﬁxed-point feature map acti-
vations into vectors over the smallest ﬁnite ﬁeld called the Galois ﬁeld of two ele-
ments (GF(2)) or, simply, binary vectors followed by compression convolutional
layers using a nonlinear dimensionality reduction (NDR) technique embedded
into DNN architecture. The compressed feature maps can then be projected
back to a higher cardinality representation over a ﬁxed-point (integer) ﬁeld us-
ing decompression convolutional layers. A layer fusion method allows to keep
only the compressed feature maps for inference while adding only computation-
ally inexpensive bitwise operations. Compression and decompression layers over
GF(2) can be repeated within the proposed network architecture and trained
in an end-to-end fashion. In brief, the proposed method resembles autoencoder-
type [5] structures embedded into a base network that work over GF(2). Bi-
nary conversion and compression-decompression layers are implemented in the
Caﬀe [6] framework and publicly available1.

The rest of the paper is organized as follows. Section 2 reviews related work.
Section 3 gives notation for convolutional layers, describes conventional fusion
and NDR methods, and explains the proposed method including details about
network training and the derived architectures from SqueezeNet [7] and Mo-
bileNetV2 [8]. Section 4 presents experimental results on ImageNet classiﬁcation
and PASCAL VOC object detection using SSD [9], memory requirements, and
obtained compression rates.

2 Related Work

Feature map compression using quantization. Unlike a weight compres-
sion, surprisingly few papers consider feature map compression. This can most
likely be explained by the fact that feature maps have to be compressed for
every network input as opposed to oﬄine weight compression. Previous fea-
ture map compression methods are primarily developed around the idea of
representation approximation using a certain quantization scheme: ﬁxed-point
quantization [10,11], binary quantization [12,13,14,15], and power-of-two quan-
tization [16]. The base ﬂoating-point network is converted to the approximate
quantized representation and, then, the quantized network is retrained to re-
store accuracy. Such methods are inherently limited in ﬁnding more compact
representations since the base architecture remains unchanged. For example,
the dynamic ﬁxed-point scheme typically requires around 8-bits of resolution
to achieve baseline accuracy for state-of-the-art network architectures. At the
same time, binary networks experience signiﬁcant accuracy drops for large-scale
datasets or compact (not over-parametrized) network architectures. Instead, our
method can be considered in a narrow sense as a learned quantization using
binary representation.

1 https://github.com/gudovskiy/fmap compression

DNN Feature Map Compression over GF(2)

3

Embedded NDR and linear layers. Another interesting approach is im-
plicitly proposed by Iandola et al. [7]. Although the authors emphasized weight
compression rather than feature map compression, they introduced NDR-type
layers into network architecture that allowed to decrease not only the number
of weights but also feature map sizes by a factor of 8, if one keeps only the
outputs of so-called squeeze layers. The latter is possible because such network
architecture does not introduce any additional convolution recomputations since
squeeze layer computations with a 1×1 kernel can be fused with the preceding
expand layers.

Recently, similar method was proposed for MobileNet architecture [8] with
the embedded bottleneck compression layers, which are, unlike SqueezeNet, lin-
ear. Authors view compression task aside from the rest of the network and argue
that linear layers are more suitable for compression because no information is
lost. While a small accuracy gain achieved for such layers compared to NDR
layers in ﬂoating-point according to their experiments, we believe that it is due
to larger set of numbers (R vs. R≥0 for NDR with rectiﬁed linear unit (ReLU)).
This is justiﬁed by our experiments using quantized models with limited set of
available values. We consider linear compression approach as a subset of nonlin-
ear. Our work goes beyond [7,8] approaches by extending compression layers to
work over GF(2) to ﬁnd a more compact feature map representation.

Hardware accelerator architectures. Horowitz [17] estimated that oﬀ-
chip DRAM access requires approximately 100× more power than local on-chip
cache access. Therefore, currently proposed DNN accelerator architectures pro-
pose various schemes to decrease memory footprint and bandwidth. One obvious
solution is to keep only a subset of intermediate feature maps at the expense
of recomputing convolutions [18]. The presented fusion approach seems to be
oversimpliﬁed but eﬀective due to high memory access cost. Our approach is
complementary to this work but proposes to keep only compressed feature maps
with minimum additional computations.

Another recent work [19] exploits weight and feature map sparsity using a
more eﬃcient encoding for zeros. While this approach targets similar goals, it
requires having high sparsity, which is often unavailable in the ﬁrst and the
largest feature maps. In addition, a special control and encoding-decoding logic
decrease the beneﬁts of this approach. In our work, compressed feature maps
are stored in a dense form without the need of special control and enconding-
decoding logic.

3 Feature Map Compression Methods

3.1 Model and Notation

The input feature map of lth convolutional layer in commonly used DNNs can be
represented by a tensor Xl−1 ∈ R ´C×H×W , where ´C, H and W are the number
of input channels, the height and the width, respectively. The input Xl−1 is
convolved with a weight tensor Wl ∈ RC× ´C×Hf ×Wf , where C is the number

4

Denis Gudovskiy et al.

of output channels, Hf and Wf are the height and the width of ﬁlter kernel,
respectively. A bias vector b ∈ RC is added to the result of convolution operation.
Once all C channels are computed, an element-wise nonlinear function is applied
to the result of the convolution operations. Then, the cth channel of the output
tensor Xl ∈ RC×H×W can be computed as

(cid:16)

Xl

c = g

Wl

c ∗ Xl−1 + b c

(cid:17)

,

(1)

where ∗ denotes convolution operation and g() is some nonlinear function. In
this paper, we assume g() is the most commonly used ReLU deﬁned as g(x) =
max (0, x) such that all activations are non-negative.

3.2 Conventional Methods

Fig. 1. The uniﬁed model of conventional methods: fusion allows to keep only bottleneck
feature maps and quantization compresses each activation.

We formally describe previously proposed methods brieﬂy reviewed in Sec-
tion 2 using the uniﬁed model illustrated in Figure 1. To simplify notation, biases
are not shown. Consider a network built using multiple convolutional layers and
processed according to (1). Similar to Alwani et al. [18], calculation of N se-
quential layers can be fused together without storing intermediate feature maps
Xl−N +1, . . . , Xl−1. For example, fusion can be done in a channel-wise fashion us-
ing memory buﬀers which are much smaller than the whole feature map. Then,
feature map Xl ∈ R can be quantized into ˆX
∈ Q using a nonlinear quantiza-
tion function q() where Q is a ﬁnite ﬁeld over integers. The quantization step
may introduce a drop in accuracy due to imperfect approximation. The network
can be further ﬁnetuned to restore some of the original accuracy [10,11]. The
network architecture is not changed after quantization and feature maps can be
compressed only up to a certain suboptimal bitwidth resolution.

l

The next step implicitly introduced by SqueezeNet [7] is to perform NDR

using an additional convolutional layer. A mapping ˆX

∈ QC×H×W → ˆY

∈

l

l

DNN Feature Map Compression over GF(2)

5

l

Q ˜C×H×W can be performed using projection weights Pl ∈ R ˜C×C×Hf ×Wf , where
the output channel dimension ˜C < C. Then, only compressed bottleneck feature
map ˆY
needs to be stored in the memory buﬀer. During the inverse steps,
the compressed feature map can be projected back onto the higher-dimensional
∈ Q using weights Rl ∈ RC× ˜C×Hf ×Wf and, lastly, converted back
tensor ˆX
to Xl+1 ∈ R using an inverse quantization function q−1(). In the case of a fully
quantized network, the inverse quantization can be omitted.

l+1

In practice, the number of bits for the feature map quantization step de-
pends on the dataset, network architecture and desired accuracy. For example,
over-parameterized architecture like AlexNet may require only 1 or 2 bits for
small-scale datasets (CIFAR-10, MNIST, SVHN), but experience signiﬁcant ac-
curacy drops for large-scale datasets like ImageNet. In particular, the modiﬁed
AlexNet (with the ﬁrst and last layers kept in full-precision) top-1 accuracy is de-
graded by 12.4% and 6.8% for 1-bit XNOR-Net [13] and 2-bit DoReFa-Net [14],
respectively. At the same time, eﬃcient network architectures e.g. [7] using NDR
layers require 6-8 bits for the ﬁxed-point quantization scheme on ImageNet and
fail to work with lower precision activations. In this paper, we follow the path to
select an eﬃcient base network architecture and then introduce additional com-
pression layers to obtain smaller feature maps as opposed to initially selecting
an over-parametrized network architecture for quantization.

3.3 Proposed Method

Fig. 2. Scheme of the proposed method: binarization is added and compression happens
in GF(2) followed by inverse operations.

Representation over GF(2). Consider a scalar x from Xl ∈ R. A con-
ventional feature map quantization step can be represented as a scalar-to-scalar
mapping or a nonlinear function ˆx = q(x) such that

x ∈ R1×1 q()

−−→ ˆx ∈ Q1×1 : min(cid:107)x − ˆx(cid:107)2,

(2)

where ˆx is the quantized scalar, Q is the GF(2B) ﬁnite ﬁeld for ﬁxed-point
representation and B is the number of bits.

6

Denis Gudovskiy et al.

We can introduce a new ˆx representation by a linear binarization function

b() deﬁned by

ˆx ∈ Q1×1 b()

−−→ ˜x ∈ BB×1 : ˜x = b ⊗ ˆx,

(3)

where ⊗ is a bitwise AND operation, vector b = [20, 21, . . . , 2B−1]T and B is
GF(2) ﬁnite ﬁeld.

An inverse linear function b−1() can be written as

˜x ∈ BB×1 b−1()

−−−→ ˆx ∈ Q1×1 : ˆx = b T ˜x = b T b ⊗ ˆx = (2B − 1) ⊗ ˆx.

(4)

l

Equations (3)-(4) show that a scalar over a higher cardinality ﬁnite ﬁeld can
be linearly converted to and from a vector over a ﬁnite ﬁeld with two elements.
Based on these derivations, we propose a feature map compression method shown
in Figure 2. Similar to [10], we quantize activations to obtain ˆX
and, then, ap-
ply transformation (3). The resulting feature map can be represented as ˜X
∈
BB×C×H×W . For implementation convenience, a new bit dimension can be con-
catenated along channel dimension resulting in the feature map ˜X
∈ BBC×H×W .
Next, a single convolutional layer using weights Pl or a sequence of layers with
Pl
i weights can be applied to obtain a compressed representation over GF(2).
Using the fusion technique, only the compressed feature maps ˜Y
∈ B need to
be stored in memory during inference. Non-compressed feature maps can be
processed using small buﬀers e.g. in a sequential channel-wise fashion. Lastly,
the inverse function b−1() from (4) using convolutional layers Rl
i and inverse of
quantization q−1() undo the compression and quantization steps.

l

l

l

Learning over GF(2). The graph model shown in Figure 3 explains de-
tails about the inference (forward pass) and backpropagation (backward pass)
phases of the newly introduced functions. The inference pass represents (3)-(4)
as explained above.

Fig. 3. Forward and backward passes during inference and backpropagation.

DNN Feature Map Compression over GF(2)

7

Clearly, the backpropagation pass may seem not obvious at a ﬁrst glance. One
diﬃculty related to the quantized network is that quantization function itself is
not diﬀerentiable. But many studies e.g. [10] show that a mini-batch-averaged
ﬂoating-point gradient practically works well assuming quantized forward pass.
The new functions b() and b−1() can be represented as gates that make hard
decisions similar to ReLU. The gradient of b−1() can then be calculated using
results of Bengio et al. [20] as

ˆ∇ ∈ R1×1 b−1()

−−−→ ˜∇ ∈ RB×1 : ˜∇ = 1˜x >0∇.

(5)

Lastly, the gradient of b() is just a scaled sum of the gradient vector calculated

by

˜∇ ∈ RB×1 b()

−−→ ˆ∇ ∈ R1×1 : ˆ∇ = 1T ˜∇ = 1T 1˜x >0∇ = (cid:107)˜x (cid:107)0∇,

(6)

where (cid:107)˜x (cid:107)0 is a gradient scaling factor that represents the number of nonzero
elements in ˜x . Practically, the scaling factor can be calculated based on statis-
tical information only once and used as a static hyperparameter for gradient
normalization.

Since the purpose of the network is to learn and keep only the smallest ˜Y
, the
choice of Pl and Rl initialization is important. Therefore, we can initialize these
weight tensors by an identity function that maps the non-compressed feature
map to a truncated compressed feature map and vice versa. That provides a good
starting point for training. At the same time, other initializations are possible
e.g. noise sampled from some distribution studied by [20] can be added as well.

l

Fig. 4. SqueezeNet architecture example: ﬁre module is extended by the proposed
method.

Network Architecture. A base network architecture can be selected among
existing networks with the embedded bottleneck layers e.g. SqueezeNet [7] or

8

Denis Gudovskiy et al.

MobileNetV2 [8]. We explain how a base network architecture can be modiﬁed
according to Section 3.3 using SqueezeNet example.

The latter network architecture consists of a sequence of ﬁre modules where
each module contains two concatenated expand layers and a squeeze layer illus-
trated in Figure 4. The squeeze layers perform NDR over the ﬁeld of real or,
in case of the quantized model, integer numbers. Speciﬁcally, the size of con-
catenated expand 1×1 and expand 3×3 layers is compressed by a factor of 8
along channel dimension by squeeze 1×1 layer. Activations of only the former
one can be stored during inference using the fusion method. According to the
analysis presented in Gysel et al. [11], activations quantized to 8-bit integers do
not experience signiﬁcant accuracy drop.

The quantized squeeze layer feature map can be converted to its binary rep-
resentation following Figure 2. Then, the additional compression rate is deﬁned
by selecting parameters of Pl
i. In the simplest case, only a single NDR layer can
be introduced with the weights Pl. In general, a number of NDR layers can be
added with 1×1, 3×3 and other kernels with or without pooling at the expense
of increased computational cost. For example, 1×1 kernels allow to learn opti-
mal quantization and to compensate redundancies along channel dimension only.
But 3×3 kernels can address spatial redundancies and, while being implemented
with stride 2 using convolutional-deconvolutional layers, decrease feature map
size along spatial dimensions.

MobileNetV2 architecture can be modiﬁed using the proposed method with
few remarks. First, its bottleneck layers compress feature maps by a 1×1 kernel
with variable compression factor from 2 to 6 unlike ﬁxed factor in SqueezeNet.
Second, linear compression layers either have to be turned into NDR layers
by adding ReLUs or implementation of compression-decompression layers needs
to support negative integers. In practice, the former approach might be less
cumbersome.

4 Experiments

4.1

ImageNet Classiﬁcation

We implemented the new binarization layers from Section 3 as well as quan-
tization layers using modiﬁed [11] code in the Caﬀe [6] framework. The latter
code is modiﬁed to accurately support binary quantization during inference and
training. SqueezeNetV1.1 and MobilenetV2 are selected as a base ﬂoating-point
network architectures, and their pretrained weights were downloaded from the
publicly available sources23.

SqueezeNet architecture. We compress the ﬁre2/squeeze and ﬁre3/squeeze
layers which consume 80% of total network memory footprint when fusion is ap-
plied due to high spatial dimensions. The input to the network has a resolution
of 227×227, and the weights are all ﬂoating-point.

2 https://github.com/DeepScale/SqueezeNet
3 https://github.com/shicai/MobileNet-Caﬀe

DNN Feature Map Compression over GF(2)

9

Table 1. SqueezeNet ImageNet accuracy: A - ﬁre2,3/squeeze feature maps and W -
weights.

Model W size, MB A size, KB Top-1 Accuracy, % Top-5 Accuracy, %

392.0

58.4

81.0

fp32

uint8
uint6
uint4

uint6
uint4

uint8
uint6

4.7

4.7
4.7
4.7

5.0
4.9

7.6
6.9

Quantized

58.6(58.3)
57.8(55.5)
54.9(18.0)

Proposed: b() →1×1→1×1→ b−1()

Proposed: b() →3×3/2→3×3*2→ b−1()

98.0
73.5
49.0

73.5
49.0

24.5
18.4

58.8
57.3

54.1
53.8

81.1(81.0)
80.7(78.7)
78.3(34.2)

81.3
80.0

77.4
77.2

The quantized and compressed models are retrained for 100,000 iterations
with a mini-batch size of 1024 on the ImageNet [21] (ILSVRC2012) training
dataset, and SGD solver with a step-policy learning rate starting from 1e-3
divided by 10 every 20,000 iterations. Although this large mini-batch size was
used by the original model, it helps the quantized and compressed models to
estimate gradients as well. The compressed models were derived and retrained
iteratively from the 8-bit quantized model. Table 1 reports top-1 and top-5
inference accuracies of 50,000 images from ImageNet validation dataset.

According to Table 1, the retrained quantized models experience -0.2%, 0.6%
and 3.5% top-1 accuracy drops for 8-bit, 6-bit and 4-bit quantization, respec-
tively. For comparison, the quantized models without retraining are shown in
parentheses. The proposed compression method using 1 × 1 kernels allows us
to restore corresponding top-1 accuracy by 1.0% and 2.4% for 6-bit and 4-bit
versions at the expense of a small increase in the number of weights and bit-
wise convolutions. Moreover, we evaluated a model with a convolutional layer
followed by a deconvolutional layer both with a 3 × 3 stride 2 kernel at the ex-
pense of a 47% increase in weight size for 6-bit activations. That allowed us to
decrease feature maps in spatial dimensions by exploiting local spatial quantiza-
tion redundancies. Then, the feature map size is further reduced by a factor of 4,
while top-1 accuracy dropped by 4.3% and 4.6% for 8-bit and 6-bit activations,
respectively. A comprehensive comparison for fully quantized models with the
state-of-the-art binary and ternary networks is given below.

MobileNetV2 architecture. We compress the conv2 1/linear feature map
which size is nearly 3× more than any other feature map among other bottleneck
layers. The same training hyperparameters are used as in previous experiment
setup with few diﬀerences. The number of iterations is 50,000 with proportional
change in learning rate policy. Second, we add ReLU layer after conv2 1/linear to

10

Denis Gudovskiy et al.

Table 2. MobileNetV2 ImageNet accuracy: A - conv2 1/linear feature maps and W -
weights.

Model W size, MB A size, KB Top-1 Accuracy, % Top-5 Accuracy, %

fp32

13.5

784.0

71.2

90.2

Quantized

71.5(71.2)
71.5(68.5)
70.9(7.3)

89.9(90.2)
89.8(88.4)
89.4(17.8)

uint8

13.5

196.0

Modiﬁed: ReLU nonlinearity added

Proposed: b() →1×1→1×1→ b−1()

int9
int7
int5

uint6
uint4

uint8
uint6

13.5
13.5
13.5

13.7
13.6

14.2
14.0

220.5
171.5
122.5

147.0
98.0

49.0
36.8

71.6

70.9
69.5

66.6
66.7

Proposed: b() →2×2/2→2×2*2→ b−1()

90.0

89.4
88.5

86.9
86.9

be compatible with the current implementation of compression method. Hence,
conv2 1/linear feature map contains signed integers in the original model and
unsigned integers in the modiﬁed one. Lastly, we found that batch normalization
layers cause some instability to training process. Therefore, normalization and
scaling parameters are ﬁxed and merged into weights and biases of convolutional
layers. Then, the modiﬁed model was retrained from the original one.

According to Table 2, the original (without ReLU) quantized models after
retraining experience -0.3%, -0.3% and 0.3% top-1 accuracy drops for 9-bit, 7-bit
and 5-bit quantization, respectively. For comparison, the quantized models with-
out retraining are shown in parentheses. Surprisingly, quantized MobileNetV2 is
resilient to smaller bitwidths with only 0.6% degradation for 5-bit model com-
pared to 9-bit one. The modiﬁed (with ReLU nonlinearity) 8-bit model outper-
forms all the original quantized model by 0.1%, even the one with more bits,
unlike results reported by [8] for ﬂoating-point models. Hence, the conclusions
about advantages of linear compression layers could be reconsidered in ﬁnite (in-
teger) ﬁeld. Accuracies of the proposed models using 1×1 kernels are on par with
the conventional quantization approaches. Most likely, lack of batch normaliza-
tion layers does not allow to increase accuracy which should be investigated. The
proposed models with a convolutional-deconvolutional layers and 2 × 2 stride 2
kernel compress feature maps by another factor of 2 with around 4.5% accuracy
degradation and 5% increase in weight size. A comparison in object detection
section further compares 2 × 2 and 3 × 3 stride 2 kernels and concludes that the
former one is preferable due to accuracy and size.

Comparison to binary and ternary state-of-the-art. We compare re-
cently reported ImageNet results for low-precision networks as well as several

DNN Feature Map Compression over GF(2)

11

Table 3. ImageNet accuracy: W - weights, A - feature maps, F - fusion, Q - quantiza-
tion, C - compression.

Model

AlexNet
AlexNet
XNOR-Net
XNOR-Net
DoReFa-Net
DoReFa-Net
DoReFa-Net
Tang’17
Tang’17

Base
Network

W,
bits

W size,
MB

A,
bits

A size,
KB

Top-1
Acc., %

Top-5
Acc., %

-
-
AlexNet
ResNet-18
AlexNet
AlexNet
AlexNet
AlexNet
NIN-Net

32
32
11
11
11
11
11
13
13

232
232
22.6
3.34
22.6
22.6
22.6
7.43
1.23

32
6
1
1
1
2
4
2
2

3053.7
572.6
344.42
1033.02
95.4
190.9
381.7
190.9
498.6

56.6
55.8
44.2
51.2
43.6
49.8
53.0
46.6
51.4

79.8
79.2
69.2
73.2
-
-
-
71.1
75.6

12165.4
-
SqueezeNet
189.9
SqueezeNet
F+Q
6(8)4 165.4
SqueezeNet
F+Q+C(1×1)
4(8)4 140.9
SqueezeNet
F+Q+C(1×1)
8(8)4 116.4
F+Q+C(3×3s2) SqueezeNet
6(8)4 110.3
F+Q+C(3×3s2) SqueezeNet
1 Weights are not binarized for the ﬁrst and the last layer.
2 Activation size estimates are based on 8-bit assumption since it is not clear from [13]

58.3(58.8)5 81.0(81.3)5
56.6(57.3)5 79.7(80.0)5
53.5(54.1)5 76.7(77.4)5
53.0(53.8)5 76.8(77.2)5

The proposed models
4.7
1.2
1.2
1.2
1.9
1.7

32
8
8
8
8
8

81.0
80.8

58.4
58.3

32
8

whether the activations were binarized or not for the ﬁrst and the last layer.

3 Weights are not binarized for the ﬁrst layer.
4 Number of bits for the compressed ﬁre2,3/squeeze layers and, in parentheses, for

5 For comparison, accuracy in parentheses represents result for the corresponding

the rest of layers.

model in Table 1.

conﬁgurations of the proposed approach for which, unlike previous experiments,
all weights and activations are quantized. Most of the works use the over-
parametrized AlexNet architecture while ours is based on SqueezeNet architec-
ture in this comparison. Table 3 shows accuracy results for base networks as well
as their quantized versions. Binary XNOR-Net [13] estimates based on AlexNet
as well as ResNet-18. DoReFa-Net [14] is more ﬂexible and can adjust the number
of bits for weights and activations. Since its accuracy is limited by the number of
activation bits, we present three cases with 1-bit, 2-bit, and 4-bit activations. The
most recent work [15] solves the problem of binarizing the last layer weights, but
weights of the ﬁrst layer are full-precision. Overall, AlexNet-based low-precision
networks achieve 43.6%, 49.8%, 53.0% top-1 accuracy for 1-bit, 2-bit and 4-bit
activations, respectively. Around 70% of the memory footprint is deﬁned by the
ﬁrst two layers of AlexNet. The fusion technique is diﬃcult in such architectures
due to large kernel sizes (11×11 and 5×5 for AlexNet) which can cause ex-

12

Denis Gudovskiy et al.

tra recomputations. Thus, activations require 95.4KB, 190.0KB and 381.7KB of
memory for 1-bit, 2-bit and 4-bit models, respectively. The NIN-based network
from [15] with 2-bit activations achieves 51.4% top-1 accuracy, but its activation
memory is larger than AlexNet due to late pooling layers.

The SqueezeNet-based models in Table 3 are ﬁnetuned from the correspond-
ing models in Table 1 for 40,000 iterations with a mini-batch size of 1024, and
SGD solver with a step-policy learning rate starting from 1e-4 divided by 10
every 10,000 iterations. The model with fusion and 8-bit quantized weights and
activations, while having an accuracy similar to ﬂoating-point model, outper-
forms the state-of-the-art networks in terms of weight and activation memory.
The proposed four models from Table 1 further decrease activation memory by
adding compression-decompression layers to ﬁre2,3 modules. This step allowed
to shrink memory from 189.9KB to 165.4KB, 140.9KB, 116.4KB and 110.3KB
depending on the compression conﬁguration. More compression is possible, if
apply the proposed approach to other squeeze layers.

4.2 PASCAL VOC Object Detection using SSD

Accuracy experiments. We evaluate object detection using Pascal VOC [22]
dataset which is a more realistic application for autonomous cars where the high-
resolution cameras emphasize feature map compression beneﬁts. The VOC2007
test dataset contains 4,952 images and a training dataset of 16,551 images is a
union of VOC2007 and VOC2012. We adopted SSD512 model [9] for the pro-
posed architecture. SqueezeNet pretrained on ImageNet is used as a feature
extractor instead of the original VGG-16 network. This reduces number of pa-
rameters and overall inference time by a factor of 4 and 3, respectively. The
original VOC images are rescaled to 512×512 resolution. As with ImageNet ex-
periments, we generated several models for comparisons: a base ﬂoating-point
model, quantized models, and compressed models. We apply quantization and
compression to the ﬁre2/squeeze and ﬁre3/squeeze layers which represent, if the
fusion technique is applied, more than 80% of total feature map memory due to
their large spatial dimensions. Typically, spatial dimensions decrease quadrat-
ically because of max pooling layers compared to linear growth in the depth
dimension. The compressed models are derived from the 8-bit quantized model,
and both are retrained for 10,000 mini-batch-256 iterations using SGD solver
with a step-policy learning rate starting from 1e-3 divided by 10 every 2,500
iterations.

Table 4 presents mean average precision (mAP) results for SqueezeNet-based
models as well as size of the weights and feature maps to compress. The 8-bit
quantized model with retraining drops accuracy by less than 0.04%, while 6-bit,
4-bit and 2-bit models decrease accuracy by 0.5%, 2.2% and 12.3%, respectively.
For reference, mAPs for the quantized models without retraining are shown in
parentheses. Using the proposed compression-decompression layers with a 1×1
kernel, mAP for the 6-bit model is increased by 0.5% and mAP for the 4-bit is
decreased by 0.5%. We conclude that compression along channel dimension is not
beneﬁcial for SSD unlike ImageNet classiﬁcation either due to low quantization

DNN Feature Map Compression over GF(2)

13

Table 4. VOC2007 SSD512 accuracy: A - ﬁre2,3/squeeze feature maps and W -
weights.

Model W size, MB A size, KB

mAP, %

fp32

23.7

2048

68.12

uint8
uint6
uint4
uint2

Quantized
512
384
256
128

23.7
23.7
23.7
23.7

68.08(68.04)
67.66(67.14)
65.92(44.13)
55.86(0.0)

Proposed b() →1×1→1×1→ b−1()

uint6
uint4

uint8
uint6

384
256

23.9
23.8

68.17
65.42
Proposed: b() →3×3/2→3×3*2→ b−1()
63.53
62.22
Proposed: b() →2×2/2→2×2*2→ b−1()
64.39
62.09

24.9
24.6

26.5
25.9

128
96

128
96

uint8
uint6

redundancy in that dimension or the choice of hyperparameters e.g. mini-batch
size. Then, we evaluate the models with spatial-dimension compression which
is intuitively appealing for high-resolution images. Empirically, we found that
a 2×2 kernel with stride 2 performs better than a corresponding 3×3 kernel
while requiring less parameters and computations. According to Table 4, an 8-
bit model with 2×2 kernel and downsampling-upsampling layers achieves 1%
higher mAP than a model with 3×3 kernel and only 3.7% lower than the base
ﬂoating-point model.

Memory requirements. Table 5 summarizes memory footprint beneﬁts for
the evaluated SSD models. Similar to the previous section, we consider only the
largest feature maps that represent more than 80% of total activation memory.
Assuming that the input frame is stored separately, the fusion technique allows to
compress feature maps by a factor of 19. Note that no additional recomputations
are needed. Second, conventional 8-bit and 4-bit ﬁxed-point models decrease the
size of feature maps by a factor of 4 and 8, respectively. Third, the proposed
model with 2×2 stride 2 kernel gains another factor of 2 compression compared
to 4-bit ﬁxed-point model with only 1.5% degradation in mAP. This result is
similar to ImageNet experiments which showed relatively limited compression
gain along channel dimension only. At the same time, learned quantization along
combined channel and spatial dimensions pushes further compression gain. In
total, the memory footprint for this feature extractor is reduced by two orders
of magnitude.

14

Denis Gudovskiy et al.

Table 5. SSD512 memory requirements: A - feature map, F - fusion, Q - quantization,
C - compression (2×2s2).

A size, KB

Base, fp32 F, fp32 F+Q, uint8 F+Q, uint4 F+Q+C, uint8

input (int8)

conv1
mpool1
ﬁre2,3/squeeze
ﬁre2,3/expand

Total
mAP, %
Compression

768

16384
4096
2048
16384

38912
68.12
-

768

0
0
2048
0

2048
68.12
19×

768

0
0
512
0

512
68.08
76×

768

0
0
256
0

256
65.92
152×

768

0
0
128
0

128
64.39
304×

5 Conclusions

We introduced a method to decrease memory storage and bandwidth require-
ments for DNNs. Complementary to conventional approaches that use layer fu-
sion and quantization, we presented an end-to-end method for learning feature
map representations over GF(2) within DNNs. Such a binary representation al-
lowed us to compress network feature maps in a higher-dimensional space using
autoencoder-inspired layers embedded into a DNN along channel and spatial
dimensions. These compression-decompression layers can be implemented using
conventional convolutional layers with bitwise operations. To be more precise,
the proposed representation traded cardinality of the ﬁnite ﬁeld with the di-
mensionality of the vector space which makes possible to learn features at the
binary level. The evaluated compression strategy for inference can be adopted
for GPUs, CPUs or custom accelerators. Alternatively, existing binary networks
can be extended to achieve higher accuracy for emerging applications such as
object detection and others.

References

1. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

arXiv preprint arXiv:1512.03385 (2015)

2. Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I.,
Wojna, Z., Song, Y., Guadarrama, S., Murphy, K.: Speed/accuracy trade-oﬀs for
modern convolutional object detectors. In: CVPR. (July 2017)

3. Graham, B., van der Maaten, L.: Submanifold sparse convolutional networks.

arXiv preprint arXiv:1706.01307 (2017)

4. O’Connor, P., Welling, M.: Sigma delta quantized networks.

In: ICLR. (April

2017)

5. Hinton, G., Salakhutdinov, R.: Reducing the dimensionality of data with neural

networks. Science 313(5786) (July 2006) 504–507

DNN Feature Map Compression over GF(2)

15

6. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093 (2014)

7. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:
Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <0.5MB model
size. arXiv preprint arXiv:1602.07360 (2016)

8. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-

verted residuals and linear bottlenecks. In: CVPR. (June 2018)

9. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.:

SSD: Single shot multibox detector. In: ECCV. (October 2016)

10. Courbariaux, M., Bengio, Y., David, J.: Training deep neural networks with low

precision multiplications. In: ICLR. (May 2015)

11. Gysel, P., Motamedi, M., Ghiasi, S.: Hardware-oriented approximation of convo-

lutional neural networks. In: ICLR. (May 2016)

12. Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., Bengio, Y.: Binarized neural

networks. In: NIPS. (2016) 4107–4115

13. Rastegari, M., Ordonez, V., Redmon, J., Farhadi, A.:

Ima-
genet classiﬁcation using binary convolutional neural networks. arXiv preprint
arXiv:1603.05279 (2016)

XNOR-net:

14. Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., Zou, Y.: DoReFa-Net: Training low
bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160 (2016)

15. Tang, W., Hua, G., Wang, L.: How to train a compact binary neural network with

high accuracy? In: AAAI. (2017)

16. Miyashita, D., Lee, E.H., Murmann, B.: Convolutional neural networks using

logarithmic data representation. arXiv preprint arXiv:1603.01025 (2016)

17. Horowitz, M.: Computing’s energy problem (and what we can do about it). In:

18. Alwani, M., Chen, H., Ferdman, M., Milder, P.A.: Fused-layer CNN accelerators.

ISSCC. (February 2014) 10–14

In: MICRO. (October 2016) 1–12

19. Parashar, A., Rhu, M., Mukkara, A., Puglielli, A., Venkatesan, R., Khailany, B.,
Emer, J., Keckler, S.W., Dally, W.J.: SCNN: An accelerator for compressed-sparse
convolutional neural networks. In: ISCA. (2017) 27–40

20. Bengio, Y., Lonard, N., Courville, A.: Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432
(2013)

21. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Challenge. IJCV 115(3) (2015) 211–252

22. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The
pascal visual object classes (VOC) challenge. IJCV 88(2) (June 2010) 303–338

8
1
0
2
 
g
u
A
 
5
1
 
 
]

V
C
.
s
c
[
 
 
1
v
5
8
2
5
0
.
8
0
8
1
:
v
i
X
r
a

DNN Feature Map Compression using Learned
Representation over GF(2)

Denis A. Gudovskiy, Alec Hodgkinson, Luca Rigazio

Panasonic Beta Research Lab, Mountain View, CA, 94043, USA
{denis.gudovskiy,alec.hodgkinson,luca.rigazio}@us.panasonic.com

Abstract. In this paper, we introduce a method to compress interme-
diate feature maps of deep neural networks (DNNs) to decrease memory
storage and bandwidth requirements during inference. Unlike previous
works, the proposed method is based on converting ﬁxed-point activa-
tions into vectors over the smallest GF(2) ﬁnite ﬁeld followed by nonlin-
ear dimensionality reduction (NDR) layers embedded into a DNN. Such
an end-to-end learned representation ﬁnds more compact feature maps by
exploiting quantization redundancies within the ﬁxed-point activations
along the channel or spatial dimensions. We apply the proposed net-
work architectures derived from modiﬁed SqueezeNet and MobileNetV2
to the tasks of ImageNet classiﬁcation and PASCAL VOC object detec-
tion. Compared to prior approaches, the conducted experiments show a
factor of 2 decrease in memory requirements with minor degradation in
accuracy while adding only bitwise computations.

Keywords: feature map compression; dimensionality reduction; net-
work quantization; memory-eﬃcient inference

1 Introduction

Recent achievements of deep neural networks (DNNs) make them an attractive
choice in many computer vision applications including image classiﬁcation [1]
and object detection [2]. The memory and computations required for DNNs can
be excessive for low-power deployments. In this paper, we explore the task of
minimizing the memory footprint of DNN feature maps during inference and,
more speciﬁcally, ﬁnding a network architecture that uses minimal storage with-
out introducing a considerable amount of additional computations or on-the-ﬂy
heuristic encoding-decoding schemes. In general, the task of feature map com-
pression is tightly connected to an input sparsity. The input sparsity can de-
termine several diﬀerent usage scenarios. This may lead to substantial decrease
in memory requirements and overall inference complexity. First, a pen sketches
are spatially sparse and can be processed eﬃciently by recently introduced sub-
manifold sparse CNNs [3]. Second, surveillance cameras with mostly static input
contain temporal sparsity that can be addressed by Sigma-Delta networks [4].
A more general scenario presumes a dense input e.g. video frames from a high-
resolution camera mounted on a moving autonomous car. In this work, we ad-
dress the latter scenario and concentrate on feature map compression in order to

2

Denis Gudovskiy et al.

minimize memory footprint and bandwidth during DNN inference which might
be prohibitive for high-resolution cameras.

We propose a method to convert intermediate ﬁxed-point feature map acti-
vations into vectors over the smallest ﬁnite ﬁeld called the Galois ﬁeld of two ele-
ments (GF(2)) or, simply, binary vectors followed by compression convolutional
layers using a nonlinear dimensionality reduction (NDR) technique embedded
into DNN architecture. The compressed feature maps can then be projected
back to a higher cardinality representation over a ﬁxed-point (integer) ﬁeld us-
ing decompression convolutional layers. A layer fusion method allows to keep
only the compressed feature maps for inference while adding only computation-
ally inexpensive bitwise operations. Compression and decompression layers over
GF(2) can be repeated within the proposed network architecture and trained
in an end-to-end fashion. In brief, the proposed method resembles autoencoder-
type [5] structures embedded into a base network that work over GF(2). Bi-
nary conversion and compression-decompression layers are implemented in the
Caﬀe [6] framework and publicly available1.

The rest of the paper is organized as follows. Section 2 reviews related work.
Section 3 gives notation for convolutional layers, describes conventional fusion
and NDR methods, and explains the proposed method including details about
network training and the derived architectures from SqueezeNet [7] and Mo-
bileNetV2 [8]. Section 4 presents experimental results on ImageNet classiﬁcation
and PASCAL VOC object detection using SSD [9], memory requirements, and
obtained compression rates.

2 Related Work

Feature map compression using quantization. Unlike a weight compres-
sion, surprisingly few papers consider feature map compression. This can most
likely be explained by the fact that feature maps have to be compressed for
every network input as opposed to oﬄine weight compression. Previous fea-
ture map compression methods are primarily developed around the idea of
representation approximation using a certain quantization scheme: ﬁxed-point
quantization [10,11], binary quantization [12,13,14,15], and power-of-two quan-
tization [16]. The base ﬂoating-point network is converted to the approximate
quantized representation and, then, the quantized network is retrained to re-
store accuracy. Such methods are inherently limited in ﬁnding more compact
representations since the base architecture remains unchanged. For example,
the dynamic ﬁxed-point scheme typically requires around 8-bits of resolution
to achieve baseline accuracy for state-of-the-art network architectures. At the
same time, binary networks experience signiﬁcant accuracy drops for large-scale
datasets or compact (not over-parametrized) network architectures. Instead, our
method can be considered in a narrow sense as a learned quantization using
binary representation.

1 https://github.com/gudovskiy/fmap compression

DNN Feature Map Compression over GF(2)

3

Embedded NDR and linear layers. Another interesting approach is im-
plicitly proposed by Iandola et al. [7]. Although the authors emphasized weight
compression rather than feature map compression, they introduced NDR-type
layers into network architecture that allowed to decrease not only the number
of weights but also feature map sizes by a factor of 8, if one keeps only the
outputs of so-called squeeze layers. The latter is possible because such network
architecture does not introduce any additional convolution recomputations since
squeeze layer computations with a 1×1 kernel can be fused with the preceding
expand layers.

Recently, similar method was proposed for MobileNet architecture [8] with
the embedded bottleneck compression layers, which are, unlike SqueezeNet, lin-
ear. Authors view compression task aside from the rest of the network and argue
that linear layers are more suitable for compression because no information is
lost. While a small accuracy gain achieved for such layers compared to NDR
layers in ﬂoating-point according to their experiments, we believe that it is due
to larger set of numbers (R vs. R≥0 for NDR with rectiﬁed linear unit (ReLU)).
This is justiﬁed by our experiments using quantized models with limited set of
available values. We consider linear compression approach as a subset of nonlin-
ear. Our work goes beyond [7,8] approaches by extending compression layers to
work over GF(2) to ﬁnd a more compact feature map representation.

Hardware accelerator architectures. Horowitz [17] estimated that oﬀ-
chip DRAM access requires approximately 100× more power than local on-chip
cache access. Therefore, currently proposed DNN accelerator architectures pro-
pose various schemes to decrease memory footprint and bandwidth. One obvious
solution is to keep only a subset of intermediate feature maps at the expense
of recomputing convolutions [18]. The presented fusion approach seems to be
oversimpliﬁed but eﬀective due to high memory access cost. Our approach is
complementary to this work but proposes to keep only compressed feature maps
with minimum additional computations.

Another recent work [19] exploits weight and feature map sparsity using a
more eﬃcient encoding for zeros. While this approach targets similar goals, it
requires having high sparsity, which is often unavailable in the ﬁrst and the
largest feature maps. In addition, a special control and encoding-decoding logic
decrease the beneﬁts of this approach. In our work, compressed feature maps
are stored in a dense form without the need of special control and enconding-
decoding logic.

3 Feature Map Compression Methods

3.1 Model and Notation

The input feature map of lth convolutional layer in commonly used DNNs can be
represented by a tensor Xl−1 ∈ R ´C×H×W , where ´C, H and W are the number
of input channels, the height and the width, respectively. The input Xl−1 is
convolved with a weight tensor Wl ∈ RC× ´C×Hf ×Wf , where C is the number

4

Denis Gudovskiy et al.

of output channels, Hf and Wf are the height and the width of ﬁlter kernel,
respectively. A bias vector b ∈ RC is added to the result of convolution operation.
Once all C channels are computed, an element-wise nonlinear function is applied
to the result of the convolution operations. Then, the cth channel of the output
tensor Xl ∈ RC×H×W can be computed as

(cid:16)

Xl

c = g

Wl

c ∗ Xl−1 + b c

(cid:17)

,

(1)

where ∗ denotes convolution operation and g() is some nonlinear function. In
this paper, we assume g() is the most commonly used ReLU deﬁned as g(x) =
max (0, x) such that all activations are non-negative.

3.2 Conventional Methods

Fig. 1. The uniﬁed model of conventional methods: fusion allows to keep only bottleneck
feature maps and quantization compresses each activation.

We formally describe previously proposed methods brieﬂy reviewed in Sec-
tion 2 using the uniﬁed model illustrated in Figure 1. To simplify notation, biases
are not shown. Consider a network built using multiple convolutional layers and
processed according to (1). Similar to Alwani et al. [18], calculation of N se-
quential layers can be fused together without storing intermediate feature maps
Xl−N +1, . . . , Xl−1. For example, fusion can be done in a channel-wise fashion us-
ing memory buﬀers which are much smaller than the whole feature map. Then,
feature map Xl ∈ R can be quantized into ˆX
∈ Q using a nonlinear quantiza-
tion function q() where Q is a ﬁnite ﬁeld over integers. The quantization step
may introduce a drop in accuracy due to imperfect approximation. The network
can be further ﬁnetuned to restore some of the original accuracy [10,11]. The
network architecture is not changed after quantization and feature maps can be
compressed only up to a certain suboptimal bitwidth resolution.

l

The next step implicitly introduced by SqueezeNet [7] is to perform NDR

using an additional convolutional layer. A mapping ˆX

∈ QC×H×W → ˆY

∈

l

l

DNN Feature Map Compression over GF(2)

5

l

Q ˜C×H×W can be performed using projection weights Pl ∈ R ˜C×C×Hf ×Wf , where
the output channel dimension ˜C < C. Then, only compressed bottleneck feature
map ˆY
needs to be stored in the memory buﬀer. During the inverse steps,
the compressed feature map can be projected back onto the higher-dimensional
∈ Q using weights Rl ∈ RC× ˜C×Hf ×Wf and, lastly, converted back
tensor ˆX
to Xl+1 ∈ R using an inverse quantization function q−1(). In the case of a fully
quantized network, the inverse quantization can be omitted.

l+1

In practice, the number of bits for the feature map quantization step de-
pends on the dataset, network architecture and desired accuracy. For example,
over-parameterized architecture like AlexNet may require only 1 or 2 bits for
small-scale datasets (CIFAR-10, MNIST, SVHN), but experience signiﬁcant ac-
curacy drops for large-scale datasets like ImageNet. In particular, the modiﬁed
AlexNet (with the ﬁrst and last layers kept in full-precision) top-1 accuracy is de-
graded by 12.4% and 6.8% for 1-bit XNOR-Net [13] and 2-bit DoReFa-Net [14],
respectively. At the same time, eﬃcient network architectures e.g. [7] using NDR
layers require 6-8 bits for the ﬁxed-point quantization scheme on ImageNet and
fail to work with lower precision activations. In this paper, we follow the path to
select an eﬃcient base network architecture and then introduce additional com-
pression layers to obtain smaller feature maps as opposed to initially selecting
an over-parametrized network architecture for quantization.

3.3 Proposed Method

Fig. 2. Scheme of the proposed method: binarization is added and compression happens
in GF(2) followed by inverse operations.

Representation over GF(2). Consider a scalar x from Xl ∈ R. A con-
ventional feature map quantization step can be represented as a scalar-to-scalar
mapping or a nonlinear function ˆx = q(x) such that

x ∈ R1×1 q()

−−→ ˆx ∈ Q1×1 : min(cid:107)x − ˆx(cid:107)2,

(2)

where ˆx is the quantized scalar, Q is the GF(2B) ﬁnite ﬁeld for ﬁxed-point
representation and B is the number of bits.

6

Denis Gudovskiy et al.

We can introduce a new ˆx representation by a linear binarization function

b() deﬁned by

ˆx ∈ Q1×1 b()

−−→ ˜x ∈ BB×1 : ˜x = b ⊗ ˆx,

(3)

where ⊗ is a bitwise AND operation, vector b = [20, 21, . . . , 2B−1]T and B is
GF(2) ﬁnite ﬁeld.

An inverse linear function b−1() can be written as

˜x ∈ BB×1 b−1()

−−−→ ˆx ∈ Q1×1 : ˆx = b T ˜x = b T b ⊗ ˆx = (2B − 1) ⊗ ˆx.

(4)

l

Equations (3)-(4) show that a scalar over a higher cardinality ﬁnite ﬁeld can
be linearly converted to and from a vector over a ﬁnite ﬁeld with two elements.
Based on these derivations, we propose a feature map compression method shown
in Figure 2. Similar to [10], we quantize activations to obtain ˆX
and, then, ap-
ply transformation (3). The resulting feature map can be represented as ˜X
∈
BB×C×H×W . For implementation convenience, a new bit dimension can be con-
catenated along channel dimension resulting in the feature map ˜X
∈ BBC×H×W .
Next, a single convolutional layer using weights Pl or a sequence of layers with
Pl
i weights can be applied to obtain a compressed representation over GF(2).
Using the fusion technique, only the compressed feature maps ˜Y
∈ B need to
be stored in memory during inference. Non-compressed feature maps can be
processed using small buﬀers e.g. in a sequential channel-wise fashion. Lastly,
the inverse function b−1() from (4) using convolutional layers Rl
i and inverse of
quantization q−1() undo the compression and quantization steps.

l

l

l

Learning over GF(2). The graph model shown in Figure 3 explains de-
tails about the inference (forward pass) and backpropagation (backward pass)
phases of the newly introduced functions. The inference pass represents (3)-(4)
as explained above.

Fig. 3. Forward and backward passes during inference and backpropagation.

DNN Feature Map Compression over GF(2)

7

Clearly, the backpropagation pass may seem not obvious at a ﬁrst glance. One
diﬃculty related to the quantized network is that quantization function itself is
not diﬀerentiable. But many studies e.g. [10] show that a mini-batch-averaged
ﬂoating-point gradient practically works well assuming quantized forward pass.
The new functions b() and b−1() can be represented as gates that make hard
decisions similar to ReLU. The gradient of b−1() can then be calculated using
results of Bengio et al. [20] as

ˆ∇ ∈ R1×1 b−1()

−−−→ ˜∇ ∈ RB×1 : ˜∇ = 1˜x >0∇.

(5)

Lastly, the gradient of b() is just a scaled sum of the gradient vector calculated

by

˜∇ ∈ RB×1 b()

−−→ ˆ∇ ∈ R1×1 : ˆ∇ = 1T ˜∇ = 1T 1˜x >0∇ = (cid:107)˜x (cid:107)0∇,

(6)

where (cid:107)˜x (cid:107)0 is a gradient scaling factor that represents the number of nonzero
elements in ˜x . Practically, the scaling factor can be calculated based on statis-
tical information only once and used as a static hyperparameter for gradient
normalization.

Since the purpose of the network is to learn and keep only the smallest ˜Y
, the
choice of Pl and Rl initialization is important. Therefore, we can initialize these
weight tensors by an identity function that maps the non-compressed feature
map to a truncated compressed feature map and vice versa. That provides a good
starting point for training. At the same time, other initializations are possible
e.g. noise sampled from some distribution studied by [20] can be added as well.

l

Fig. 4. SqueezeNet architecture example: ﬁre module is extended by the proposed
method.

Network Architecture. A base network architecture can be selected among
existing networks with the embedded bottleneck layers e.g. SqueezeNet [7] or

8

Denis Gudovskiy et al.

MobileNetV2 [8]. We explain how a base network architecture can be modiﬁed
according to Section 3.3 using SqueezeNet example.

The latter network architecture consists of a sequence of ﬁre modules where
each module contains two concatenated expand layers and a squeeze layer illus-
trated in Figure 4. The squeeze layers perform NDR over the ﬁeld of real or,
in case of the quantized model, integer numbers. Speciﬁcally, the size of con-
catenated expand 1×1 and expand 3×3 layers is compressed by a factor of 8
along channel dimension by squeeze 1×1 layer. Activations of only the former
one can be stored during inference using the fusion method. According to the
analysis presented in Gysel et al. [11], activations quantized to 8-bit integers do
not experience signiﬁcant accuracy drop.

The quantized squeeze layer feature map can be converted to its binary rep-
resentation following Figure 2. Then, the additional compression rate is deﬁned
by selecting parameters of Pl
i. In the simplest case, only a single NDR layer can
be introduced with the weights Pl. In general, a number of NDR layers can be
added with 1×1, 3×3 and other kernels with or without pooling at the expense
of increased computational cost. For example, 1×1 kernels allow to learn opti-
mal quantization and to compensate redundancies along channel dimension only.
But 3×3 kernels can address spatial redundancies and, while being implemented
with stride 2 using convolutional-deconvolutional layers, decrease feature map
size along spatial dimensions.

MobileNetV2 architecture can be modiﬁed using the proposed method with
few remarks. First, its bottleneck layers compress feature maps by a 1×1 kernel
with variable compression factor from 2 to 6 unlike ﬁxed factor in SqueezeNet.
Second, linear compression layers either have to be turned into NDR layers
by adding ReLUs or implementation of compression-decompression layers needs
to support negative integers. In practice, the former approach might be less
cumbersome.

4 Experiments

4.1

ImageNet Classiﬁcation

We implemented the new binarization layers from Section 3 as well as quan-
tization layers using modiﬁed [11] code in the Caﬀe [6] framework. The latter
code is modiﬁed to accurately support binary quantization during inference and
training. SqueezeNetV1.1 and MobilenetV2 are selected as a base ﬂoating-point
network architectures, and their pretrained weights were downloaded from the
publicly available sources23.

SqueezeNet architecture. We compress the ﬁre2/squeeze and ﬁre3/squeeze
layers which consume 80% of total network memory footprint when fusion is ap-
plied due to high spatial dimensions. The input to the network has a resolution
of 227×227, and the weights are all ﬂoating-point.

2 https://github.com/DeepScale/SqueezeNet
3 https://github.com/shicai/MobileNet-Caﬀe

DNN Feature Map Compression over GF(2)

9

Table 1. SqueezeNet ImageNet accuracy: A - ﬁre2,3/squeeze feature maps and W -
weights.

Model W size, MB A size, KB Top-1 Accuracy, % Top-5 Accuracy, %

392.0

58.4

81.0

fp32

uint8
uint6
uint4

uint6
uint4

uint8
uint6

4.7

4.7
4.7
4.7

5.0
4.9

7.6
6.9

Quantized

58.6(58.3)
57.8(55.5)
54.9(18.0)

Proposed: b() →1×1→1×1→ b−1()

Proposed: b() →3×3/2→3×3*2→ b−1()

98.0
73.5
49.0

73.5
49.0

24.5
18.4

58.8
57.3

54.1
53.8

81.1(81.0)
80.7(78.7)
78.3(34.2)

81.3
80.0

77.4
77.2

The quantized and compressed models are retrained for 100,000 iterations
with a mini-batch size of 1024 on the ImageNet [21] (ILSVRC2012) training
dataset, and SGD solver with a step-policy learning rate starting from 1e-3
divided by 10 every 20,000 iterations. Although this large mini-batch size was
used by the original model, it helps the quantized and compressed models to
estimate gradients as well. The compressed models were derived and retrained
iteratively from the 8-bit quantized model. Table 1 reports top-1 and top-5
inference accuracies of 50,000 images from ImageNet validation dataset.

According to Table 1, the retrained quantized models experience -0.2%, 0.6%
and 3.5% top-1 accuracy drops for 8-bit, 6-bit and 4-bit quantization, respec-
tively. For comparison, the quantized models without retraining are shown in
parentheses. The proposed compression method using 1 × 1 kernels allows us
to restore corresponding top-1 accuracy by 1.0% and 2.4% for 6-bit and 4-bit
versions at the expense of a small increase in the number of weights and bit-
wise convolutions. Moreover, we evaluated a model with a convolutional layer
followed by a deconvolutional layer both with a 3 × 3 stride 2 kernel at the ex-
pense of a 47% increase in weight size for 6-bit activations. That allowed us to
decrease feature maps in spatial dimensions by exploiting local spatial quantiza-
tion redundancies. Then, the feature map size is further reduced by a factor of 4,
while top-1 accuracy dropped by 4.3% and 4.6% for 8-bit and 6-bit activations,
respectively. A comprehensive comparison for fully quantized models with the
state-of-the-art binary and ternary networks is given below.

MobileNetV2 architecture. We compress the conv2 1/linear feature map
which size is nearly 3× more than any other feature map among other bottleneck
layers. The same training hyperparameters are used as in previous experiment
setup with few diﬀerences. The number of iterations is 50,000 with proportional
change in learning rate policy. Second, we add ReLU layer after conv2 1/linear to

10

Denis Gudovskiy et al.

Table 2. MobileNetV2 ImageNet accuracy: A - conv2 1/linear feature maps and W -
weights.

Model W size, MB A size, KB Top-1 Accuracy, % Top-5 Accuracy, %

fp32

13.5

784.0

71.2

90.2

Quantized

71.5(71.2)
71.5(68.5)
70.9(7.3)

89.9(90.2)
89.8(88.4)
89.4(17.8)

uint8

13.5

196.0

Modiﬁed: ReLU nonlinearity added

Proposed: b() →1×1→1×1→ b−1()

int9
int7
int5

uint6
uint4

uint8
uint6

13.5
13.5
13.5

13.7
13.6

14.2
14.0

220.5
171.5
122.5

147.0
98.0

49.0
36.8

71.6

70.9
69.5

66.6
66.7

Proposed: b() →2×2/2→2×2*2→ b−1()

90.0

89.4
88.5

86.9
86.9

be compatible with the current implementation of compression method. Hence,
conv2 1/linear feature map contains signed integers in the original model and
unsigned integers in the modiﬁed one. Lastly, we found that batch normalization
layers cause some instability to training process. Therefore, normalization and
scaling parameters are ﬁxed and merged into weights and biases of convolutional
layers. Then, the modiﬁed model was retrained from the original one.

According to Table 2, the original (without ReLU) quantized models after
retraining experience -0.3%, -0.3% and 0.3% top-1 accuracy drops for 9-bit, 7-bit
and 5-bit quantization, respectively. For comparison, the quantized models with-
out retraining are shown in parentheses. Surprisingly, quantized MobileNetV2 is
resilient to smaller bitwidths with only 0.6% degradation for 5-bit model com-
pared to 9-bit one. The modiﬁed (with ReLU nonlinearity) 8-bit model outper-
forms all the original quantized model by 0.1%, even the one with more bits,
unlike results reported by [8] for ﬂoating-point models. Hence, the conclusions
about advantages of linear compression layers could be reconsidered in ﬁnite (in-
teger) ﬁeld. Accuracies of the proposed models using 1×1 kernels are on par with
the conventional quantization approaches. Most likely, lack of batch normaliza-
tion layers does not allow to increase accuracy which should be investigated. The
proposed models with a convolutional-deconvolutional layers and 2 × 2 stride 2
kernel compress feature maps by another factor of 2 with around 4.5% accuracy
degradation and 5% increase in weight size. A comparison in object detection
section further compares 2 × 2 and 3 × 3 stride 2 kernels and concludes that the
former one is preferable due to accuracy and size.

Comparison to binary and ternary state-of-the-art. We compare re-
cently reported ImageNet results for low-precision networks as well as several

DNN Feature Map Compression over GF(2)

11

Table 3. ImageNet accuracy: W - weights, A - feature maps, F - fusion, Q - quantiza-
tion, C - compression.

Model

AlexNet
AlexNet
XNOR-Net
XNOR-Net
DoReFa-Net
DoReFa-Net
DoReFa-Net
Tang’17
Tang’17

Base
Network

W,
bits

W size,
MB

A,
bits

A size,
KB

Top-1
Acc., %

Top-5
Acc., %

-
-
AlexNet
ResNet-18
AlexNet
AlexNet
AlexNet
AlexNet
NIN-Net

32
32
11
11
11
11
11
13
13

232
232
22.6
3.34
22.6
22.6
22.6
7.43
1.23

32
6
1
1
1
2
4
2
2

3053.7
572.6
344.42
1033.02
95.4
190.9
381.7
190.9
498.6

56.6
55.8
44.2
51.2
43.6
49.8
53.0
46.6
51.4

79.8
79.2
69.2
73.2
-
-
-
71.1
75.6

12165.4
-
SqueezeNet
189.9
SqueezeNet
F+Q
6(8)4 165.4
SqueezeNet
F+Q+C(1×1)
4(8)4 140.9
SqueezeNet
F+Q+C(1×1)
8(8)4 116.4
F+Q+C(3×3s2) SqueezeNet
6(8)4 110.3
F+Q+C(3×3s2) SqueezeNet
1 Weights are not binarized for the ﬁrst and the last layer.
2 Activation size estimates are based on 8-bit assumption since it is not clear from [13]

58.3(58.8)5 81.0(81.3)5
56.6(57.3)5 79.7(80.0)5
53.5(54.1)5 76.7(77.4)5
53.0(53.8)5 76.8(77.2)5

The proposed models
4.7
1.2
1.2
1.2
1.9
1.7

32
8
8
8
8
8

81.0
80.8

58.4
58.3

32
8

whether the activations were binarized or not for the ﬁrst and the last layer.

3 Weights are not binarized for the ﬁrst layer.
4 Number of bits for the compressed ﬁre2,3/squeeze layers and, in parentheses, for

5 For comparison, accuracy in parentheses represents result for the corresponding

the rest of layers.

model in Table 1.

conﬁgurations of the proposed approach for which, unlike previous experiments,
all weights and activations are quantized. Most of the works use the over-
parametrized AlexNet architecture while ours is based on SqueezeNet architec-
ture in this comparison. Table 3 shows accuracy results for base networks as well
as their quantized versions. Binary XNOR-Net [13] estimates based on AlexNet
as well as ResNet-18. DoReFa-Net [14] is more ﬂexible and can adjust the number
of bits for weights and activations. Since its accuracy is limited by the number of
activation bits, we present three cases with 1-bit, 2-bit, and 4-bit activations. The
most recent work [15] solves the problem of binarizing the last layer weights, but
weights of the ﬁrst layer are full-precision. Overall, AlexNet-based low-precision
networks achieve 43.6%, 49.8%, 53.0% top-1 accuracy for 1-bit, 2-bit and 4-bit
activations, respectively. Around 70% of the memory footprint is deﬁned by the
ﬁrst two layers of AlexNet. The fusion technique is diﬃcult in such architectures
due to large kernel sizes (11×11 and 5×5 for AlexNet) which can cause ex-

12

Denis Gudovskiy et al.

tra recomputations. Thus, activations require 95.4KB, 190.0KB and 381.7KB of
memory for 1-bit, 2-bit and 4-bit models, respectively. The NIN-based network
from [15] with 2-bit activations achieves 51.4% top-1 accuracy, but its activation
memory is larger than AlexNet due to late pooling layers.

The SqueezeNet-based models in Table 3 are ﬁnetuned from the correspond-
ing models in Table 1 for 40,000 iterations with a mini-batch size of 1024, and
SGD solver with a step-policy learning rate starting from 1e-4 divided by 10
every 10,000 iterations. The model with fusion and 8-bit quantized weights and
activations, while having an accuracy similar to ﬂoating-point model, outper-
forms the state-of-the-art networks in terms of weight and activation memory.
The proposed four models from Table 1 further decrease activation memory by
adding compression-decompression layers to ﬁre2,3 modules. This step allowed
to shrink memory from 189.9KB to 165.4KB, 140.9KB, 116.4KB and 110.3KB
depending on the compression conﬁguration. More compression is possible, if
apply the proposed approach to other squeeze layers.

4.2 PASCAL VOC Object Detection using SSD

Accuracy experiments. We evaluate object detection using Pascal VOC [22]
dataset which is a more realistic application for autonomous cars where the high-
resolution cameras emphasize feature map compression beneﬁts. The VOC2007
test dataset contains 4,952 images and a training dataset of 16,551 images is a
union of VOC2007 and VOC2012. We adopted SSD512 model [9] for the pro-
posed architecture. SqueezeNet pretrained on ImageNet is used as a feature
extractor instead of the original VGG-16 network. This reduces number of pa-
rameters and overall inference time by a factor of 4 and 3, respectively. The
original VOC images are rescaled to 512×512 resolution. As with ImageNet ex-
periments, we generated several models for comparisons: a base ﬂoating-point
model, quantized models, and compressed models. We apply quantization and
compression to the ﬁre2/squeeze and ﬁre3/squeeze layers which represent, if the
fusion technique is applied, more than 80% of total feature map memory due to
their large spatial dimensions. Typically, spatial dimensions decrease quadrat-
ically because of max pooling layers compared to linear growth in the depth
dimension. The compressed models are derived from the 8-bit quantized model,
and both are retrained for 10,000 mini-batch-256 iterations using SGD solver
with a step-policy learning rate starting from 1e-3 divided by 10 every 2,500
iterations.

Table 4 presents mean average precision (mAP) results for SqueezeNet-based
models as well as size of the weights and feature maps to compress. The 8-bit
quantized model with retraining drops accuracy by less than 0.04%, while 6-bit,
4-bit and 2-bit models decrease accuracy by 0.5%, 2.2% and 12.3%, respectively.
For reference, mAPs for the quantized models without retraining are shown in
parentheses. Using the proposed compression-decompression layers with a 1×1
kernel, mAP for the 6-bit model is increased by 0.5% and mAP for the 4-bit is
decreased by 0.5%. We conclude that compression along channel dimension is not
beneﬁcial for SSD unlike ImageNet classiﬁcation either due to low quantization

DNN Feature Map Compression over GF(2)

13

Table 4. VOC2007 SSD512 accuracy: A - ﬁre2,3/squeeze feature maps and W -
weights.

Model W size, MB A size, KB

mAP, %

fp32

23.7

2048

68.12

uint8
uint6
uint4
uint2

Quantized
512
384
256
128

23.7
23.7
23.7
23.7

68.08(68.04)
67.66(67.14)
65.92(44.13)
55.86(0.0)

Proposed b() →1×1→1×1→ b−1()

uint6
uint4

384
256

23.9
23.8

68.17
65.42
Proposed: b() →3×3/2→3×3*2→ b−1()
63.53
62.22
Proposed: b() →2×2/2→2×2*2→ b−1()
64.39
62.09

26.5
25.9

24.9
24.6

128
96

128
96

uint8
uint6

uint8
uint6

redundancy in that dimension or the choice of hyperparameters e.g. mini-batch
size. Then, we evaluate the models with spatial-dimension compression which
is intuitively appealing for high-resolution images. Empirically, we found that
a 2×2 kernel with stride 2 performs better than a corresponding 3×3 kernel
while requiring less parameters and computations. According to Table 4, an 8-
bit model with 2×2 kernel and downsampling-upsampling layers achieves 1%
higher mAP than a model with 3×3 kernel and only 3.7% lower than the base
ﬂoating-point model.

Memory requirements. Table 5 summarizes memory footprint beneﬁts for
the evaluated SSD models. Similar to the previous section, we consider only the
largest feature maps that represent more than 80% of total activation memory.
Assuming that the input frame is stored separately, the fusion technique allows to
compress feature maps by a factor of 19. Note that no additional recomputations
are needed. Second, conventional 8-bit and 4-bit ﬁxed-point models decrease the
size of feature maps by a factor of 4 and 8, respectively. Third, the proposed
model with 2×2 stride 2 kernel gains another factor of 2 compression compared
to 4-bit ﬁxed-point model with only 1.5% degradation in mAP. This result is
similar to ImageNet experiments which showed relatively limited compression
gain along channel dimension only. At the same time, learned quantization along
combined channel and spatial dimensions pushes further compression gain. In
total, the memory footprint for this feature extractor is reduced by two orders
of magnitude.

14

Denis Gudovskiy et al.

Table 5. SSD512 memory requirements: A - feature map, F - fusion, Q - quantization,
C - compression (2×2s2).

A size, KB

Base, fp32 F, fp32 F+Q, uint8 F+Q, uint4 F+Q+C, uint8

input (int8)

conv1
mpool1
ﬁre2,3/squeeze
ﬁre2,3/expand

Total
mAP, %
Compression

768

16384
4096
2048
16384

38912
68.12
-

768

0
0
2048
0

2048
68.12
19×

768

0
0
512
0

512
68.08
76×

768

0
0
256
0

256
65.92
152×

768

0
0
128
0

128
64.39
304×

5 Conclusions

We introduced a method to decrease memory storage and bandwidth require-
ments for DNNs. Complementary to conventional approaches that use layer fu-
sion and quantization, we presented an end-to-end method for learning feature
map representations over GF(2) within DNNs. Such a binary representation al-
lowed us to compress network feature maps in a higher-dimensional space using
autoencoder-inspired layers embedded into a DNN along channel and spatial
dimensions. These compression-decompression layers can be implemented using
conventional convolutional layers with bitwise operations. To be more precise,
the proposed representation traded cardinality of the ﬁnite ﬁeld with the di-
mensionality of the vector space which makes possible to learn features at the
binary level. The evaluated compression strategy for inference can be adopted
for GPUs, CPUs or custom accelerators. Alternatively, existing binary networks
can be extended to achieve higher accuracy for emerging applications such as
object detection and others.

References

1. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

arXiv preprint arXiv:1512.03385 (2015)

2. Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I.,
Wojna, Z., Song, Y., Guadarrama, S., Murphy, K.: Speed/accuracy trade-oﬀs for
modern convolutional object detectors. In: CVPR. (July 2017)

3. Graham, B., van der Maaten, L.: Submanifold sparse convolutional networks.

arXiv preprint arXiv:1706.01307 (2017)

4. O’Connor, P., Welling, M.: Sigma delta quantized networks.

In: ICLR. (April

2017)

5. Hinton, G., Salakhutdinov, R.: Reducing the dimensionality of data with neural

networks. Science 313(5786) (July 2006) 504–507

DNN Feature Map Compression over GF(2)

15

6. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093 (2014)

7. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:
Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <0.5MB model
size. arXiv preprint arXiv:1602.07360 (2016)

8. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-

verted residuals and linear bottlenecks. In: CVPR. (June 2018)

9. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.:

SSD: Single shot multibox detector. In: ECCV. (October 2016)

10. Courbariaux, M., Bengio, Y., David, J.: Training deep neural networks with low

precision multiplications. In: ICLR. (May 2015)

11. Gysel, P., Motamedi, M., Ghiasi, S.: Hardware-oriented approximation of convo-

lutional neural networks. In: ICLR. (May 2016)

12. Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., Bengio, Y.: Binarized neural

networks. In: NIPS. (2016) 4107–4115

13. Rastegari, M., Ordonez, V., Redmon, J., Farhadi, A.:

Ima-
genet classiﬁcation using binary convolutional neural networks. arXiv preprint
arXiv:1603.05279 (2016)

XNOR-net:

14. Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., Zou, Y.: DoReFa-Net: Training low
bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160 (2016)

15. Tang, W., Hua, G., Wang, L.: How to train a compact binary neural network with

high accuracy? In: AAAI. (2017)

16. Miyashita, D., Lee, E.H., Murmann, B.: Convolutional neural networks using

logarithmic data representation. arXiv preprint arXiv:1603.01025 (2016)

17. Horowitz, M.: Computing’s energy problem (and what we can do about it). In:

18. Alwani, M., Chen, H., Ferdman, M., Milder, P.A.: Fused-layer CNN accelerators.

ISSCC. (February 2014) 10–14

In: MICRO. (October 2016) 1–12

19. Parashar, A., Rhu, M., Mukkara, A., Puglielli, A., Venkatesan, R., Khailany, B.,
Emer, J., Keckler, S.W., Dally, W.J.: SCNN: An accelerator for compressed-sparse
convolutional neural networks. In: ISCA. (2017) 27–40

20. Bengio, Y., Lonard, N., Courville, A.: Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432
(2013)

21. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Challenge. IJCV 115(3) (2015) 211–252

22. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The
pascal visual object classes (VOC) challenge. IJCV 88(2) (June 2010) 303–338

