Deep Dyna-Q: Integrating Planning for
Task-Completion Dialogue Policy Learning

Baolin Peng(cid:63) Xiujun Li†

Jianfeng Gao†

Jingjing Liu† Kam-Fai Wong(cid:63)‡ Shang-Yu Su§

†Microsoft Research, Redmond, WA, USA
(cid:63)The Chinese University of Hong Kong, Hong Kong
‡MoE Key Lab of High Conﬁdence Software Technologies, China
§National Taiwan University, Taipei, Taiwan
{xiul,jfgao,jingjl}@microsoft.com
{blpeng,kfwong}@se.cuhk.edu.hk shangyusu.tw@gmail.com

Abstract

Training a task-completion dialogue agent
via reinforcement learning (RL) is costly
because it requires many interactions with
real users. One common alternative is to
use a user simulator. However, a user
simulator usually lacks the language com-
plexity of human interlocutors and the bi-
ases in its design may tend to degrade the
agent. To address these issues, we present
Deep Dyna-Q, which to our knowledge
is the ﬁrst deep RL framework that inte-
grates planning for task-completion dia-
logue policy learning. We incorporate into
the dialogue agent a model of the envi-
ronment, referred to as the world model,
to mimic real user response and gener-
ate simulated experience. During dialogue
policy learning, the world model is con-
stantly updated with real user experience
to approach real user behavior, and in
turn, the dialogue agent is optimized using
both real experience and simulated expe-
rience. The effectiveness of our approach
is demonstrated on a movie-ticket booking
task in both simulated and human-in-the-
loop settings1.

1

Introduction

Learning policies for task-completion dialogue is
often formulated as a reinforcement learning (RL)
problem (Young et al., 2013; Levin et al., 1997).
However, applying RL to real-world dialogue sys-
tems can be challenging, due to the constraint that
an RL learner needs an environment to operate
in.
In the dialogue setting, this requires a dia-
logue agent to interact with real users and adjust

1The source code of this work is available at https://

github.com/MiuLab/DDQ

its policy in an online fashion, as illustrated in Fig-
ure 1(a). Unlike simulation-based games such as
Atari games (Mnih et al., 2015) and AlphaGo (Sil-
ver et al., 2016a, 2017) where RL has made its
greatest strides, task-completion dialogue systems
may incur signiﬁcant real-world cost in case of
failure. Thus, except for very simple tasks (Singh
et al., 2002; Gaˇsi´c et al., 2010, 2011; Pietquin
et al., 2011; Li et al., 2016a; Su et al., 2016b), RL
is too expensive to be applied to real users to train
dialogue agents from scratch.

One strategy is to convert human-interacting di-
alogue to a simulation problem (similar to Atari
games), by building a user simulator using human
conversational data (Schatzmann et al., 2007; Li
et al., 2016b). In this way, the dialogue agent can
learn its policy by interacting with the simulator
instead of real users (Figure 1(b)). The simulator,
in theory, does not incur any real-world cost and
can provide unlimited simulated experience for re-
inforcement learning. The dialogue agent trained
with such a user simulator can then be deployed to
real users and further enhanced by only a small
number of human interactions. Most of recent
studies in this area have adopted this strategy (Su
et al., 2016a; Lipton et al., 2016; Zhao and Eske-
nazi, 2016; Williams et al., 2017; Dhingra et al.,
2017; Li et al., 2017; Liu and Lane, 2017; Peng
et al., 2017b; Budzianowski et al., 2017; Peng
et al., 2017a).

However, user simulators usually lack the con-
versational complexity of human interlocutors,
and the trained agent is inevitably affected by bi-
ases in the design of the simulator. Dhingra et al.
(2017) demonstrated a signiﬁcant discrepancy in
a simulator-trained dialogue agent when evalu-
ated with simulators and with real users. Even
more challenging is the fact that there is no uni-
versally accepted metric to evaluate a user simula-
tor (Pietquin and Hastie, 2013). Thus, it remains

8
1
0
2
 
y
a
M
 
3
2
 
 
]
L
C
.
s
c
[
 
 
3
v
6
7
1
6
0
.
1
0
8
1
:
v
i
X
r
a

(a) Learning with real users

(b) Learning with user simulators

(c) Learning with real users via DDQ

Figure 1: Three strategies of learning task-completion dialogue policies via RL.

controversial whether training task-completion di-
alogue agent via simulated users is a valid ap-
proach.

We propose a new strategy of learning dialogue
policy by interacting with real users. Compared
to previous works (Singh et al., 2002; Li et al.,
2016a; Su et al., 2016b; Papangelis, 2012), our di-
alogue agent learns in a much more efﬁcient way,
using only a small number of real user interac-
tions, which amounts to an affordable cost in many
nontrivial dialogue tasks.

Our approach is based on the Dyna-Q frame-
work (Sutton, 1990) where planning is integrated
into policy learning for task-completion dialogue.
Speciﬁcally, we incorporate a model of the envi-
ronment, referred to as the world model, into the
dialogue agent, which simulates the environment
and generates simulated user experience. During
the dialogue policy learning, real user experience
plays two pivotal roles: ﬁrst, it can be used to im-
prove the world model and make it behave more
like real users, via supervised learning; second, it
can also be used to directly improve the dialogue
policy via RL. The former is referred to as world
model learning, and the latter direct reinforcement
learning. Dialogue policy can be improved ei-
ther using real experience directly (i.e., direct re-
inforcement learning) or via the world model in-
directly (referred to as planning or indirect re-
inforcement learning). The interaction between
world model learning, direct reinforcement learn-
ing and planning is illustrated in Figure 1(c), fol-
lowing the Dyna-Q framework (Sutton, 1990).

The original papers on Dyna-Q and most its
early extensions used tabular methods for both
planning and learning (Singh, 1992; Peng and
Williams, 1993; Moore and Atkeson, 1993; Ku-
vayev and Sutton, 1996). This table-lookup repre-
sentation limits its application to small problems

only. Sutton et al. (2012) extends the Dyna ar-
chitecture to linear function approximation, mak-
ing it applicable to larger problems.
In the dia-
logue setting, we are dealing with a much larger
action-state space. Inspired by Mnih et al. (2015),
we propose Deep Dyna-Q (DDQ) by combining
Dyna-Q with deep learning approaches to repre-
senting the state-action space by neural networks
(NN).

By employing the world model for planning, the
DDQ method can be viewed as a model-based RL
approach, which has drawn growing interest in the
research community. However, most model-based
RL methods (Tamar et al., 2016; Silver et al.,
2016b; Gu et al., 2016; Racani`ere et al., 2017) are
developed for simulation-based, synthetic prob-
lems (e.g., games), but not for human-in-the-loop,
real-world problems. To these ends, our main con-
tributions in this work are two-fold:

• We present Deep Dyna-Q, which to the best
of our knowledge is the ﬁrst deep RL frame-
incorporates planning for task-
work that
completion dialogue policy learning.

• We demonstrate that a task-completion dia-
logue agent can efﬁciently adapt its policy on
the ﬂy, by interacting with real users via RL.
This results in a signiﬁcant improvement in
success rate on a nontrivial task.

2 Dialogue Policy Learning via Deep

Dyna-Q (DDQ)

Our DDQ dialogue agent is illustrated in Fig-
ure 2, consisting of ﬁve modules: (1) an LSTM-
based natural
language understanding (NLU)
module (Hakkani-T¨ur et al., 2016) for identifying
user intents and extracting associated slots; (2) a
state tracker (Mrkˇsi´c et al., 2016) for tracking the
dialogue states; (3) a dialogue policy which selects

acts with a user in a sequence of actions to ac-
complish a user goal. In each step, the agent ob-
serves the dialogue state s, and chooses the action
a to execute, using an (cid:15)-greedy policy that selects a
random action with probability (cid:15) or otherwise fol-
lows the greedy policy a = argmaxa(cid:48)Q(s, a(cid:48); θQ).
Q(s, a; θQ) which is the approximated value func-
tion, implemented as a Multi-Layer Perceptron
(MLP) parameterized by θQ. The agent then re-
ceives reward3 r, observes next user response au,
and updates the state to s(cid:48). Finally, we store the
experience (s, a, r, au, s(cid:48)) in the replay buffer Du.
The cycle continues until the dialogue terminates.
We improve the value function Q(s, a; θQ) by
adjusting θQ to minimize the mean-squared loss
function, deﬁned as follows:

L(θQ) = E(s,a,r,s(cid:48))∼Du[(yi − Q(s, a; θQ))2]
(1)

yi = r + γ max

Q(cid:48)(s(cid:48), a(cid:48); θQ(cid:48))

a(cid:48)

where γ ∈ [0, 1] is a discount factor, and Q(cid:48)(.) is
the target value function that is only periodically
updated (line 42 in Algorithm 1). By differentiat-
ing the loss function with respect to θQ, we arrive
at the following gradient:

∇θQL(θQ) = E(s,a,r,s(cid:48))∼Du[(r+
γ max
a(cid:48)

Q(cid:48)(s(cid:48), a(cid:48); θQ(cid:48)) − Q(s, a; θQ))

(2)

∇θQQ(s, a; θQ)]

As shown in lines 16-17 in Algorithm 1, in each
iteration, we improve Q(.) using minibatch Deep
Q-learning.

2.2 Planning

In the planning process (lines 23-41 in Algo-
rithm 1), the world model is employed to generate
simulated experience that can be used to improve
dialogue policy. K in line 24 is the number of
planning steps that the agent performs per step of
direct reinforcement learning. If the world model
is able to accurately simulate the environment, a
big K can be used to speed up the policy learn-
ing. In DDQ, we use two replay buffers, Du for
storing real experience and Ds for simulated ex-
perience. Learning and planning are accomplished

3In the dialogue scenario, reward is deﬁned to measure
the degree of success of a dialogue. In our experiment, for
example, success corresponds to a reward of 80, failure to a
reward of −40, and the agent receives a reward of −1 at each
turn so as to encourage shorter dialogues.

Figure 2: Illustration of the task-completion DDQ
dialogue agent.

the next action2 based on the current state; (4) a
model-based natural language generation (NLG)
module for converting dialogue actions to natural
language response (Wen et al., 2015); and (5) a
world model for generating simulated user actions
and simulated rewards.

As illustrated in Figure 1(c), starting with an
initial dialogue policy and an initial world model
(both trained with pre-collected human conversa-
tional data), the training of the DDQ agent con-
sists of three processes: (1) direct reinforcement
learning, where the agent interacts with a real user,
collects real experience and improves the dialogue
policy; (2) world model learning, where the world
model is learned and reﬁned using real experience;
and (3) planning, where the agent improves the di-
alogue policy using simulated experience.

Although these three processes conceptually
can occur simultaneously in the DDQ agent,
we implement an iterative training procedure, as
shown in Algorithm 1, where we specify the or-
der in which they occur within each iteration. In
what follows, we will describe these processes in
details.

2.1 Direct Reinforcement Learning

In this process (lines 5-18 in Algorithm 1) we use
the DQN method (Mnih et al., 2015) to improve
the dialogue policy based on real experience. We
consider task-completion dialogue as a Markov
Decision Process (MDP), where the agent inter-

2In the dialogue scenario, actions are dialogue-acts, con-
sisting of a single act and a (possibly empty) collection of
(slot = value) pairs (Schatzmann et al., 2007).

Algorithm 1 Deep Dyna-Q for Dialogue Policy
Learning
Require: N , (cid:15), K, L, C, Z
Ensure: Q(s, a; θQ), M (s, a; θM )
1: initialize Q(s, a; θQ) and M (s, a; θM ) via pre-training

on human conversational data

2: initialize Q(cid:48)(s, a; θQ(cid:48) ) with θQ(cid:48) = θQ
3: initialize real experience replay buffer Du using Reply
Buffer Spiking (RBS), and simulated experience replay
buffer Ds as empty

4: for n=1:N do
5:
6:
7:
8:
9:
10:
11:

# Direct Reinforcement Learning starts
user starts a dialogue with user action au
generate an initial dialogue state s
while s is not a terminal state do

with probability (cid:15) select a random action a
otherwise select a = argmaxa(cid:48) Q(s, a(cid:48); θQ)
execute a, and observe user response au and re-
ward r
update dialogue state to s(cid:48)
store (s, a, r, au, s(cid:48)) to Du
s = s(cid:48)
end while
sample random minibatches of (s, a, r, s(cid:48)) from Du
update θQ via Z-step minibatch Q-learning according
to Equation (2)
# Direct Reinforcement Learning ends
# World Model Learning starts
sample random minibatches of
(s, a, r, au, s(cid:48)) from Du
update θM via Z-step minibatch SGD of multi-task
learning
# World Model Learning ends
# Planning starts
for k=1:K do

training samples

t = FALSE, l = 0
sample a user goal G
sample user action au from G
generate an initial dialogue state s
while t is FALSE ∧ l ≤ L do

with probability (cid:15) select a random action a
otherwise select a = argmaxa(cid:48) Q(s, a(cid:48); θQ)
execute a
world model responds with au, r and t
update dialogue state to s(cid:48)
store (s, a, r, s(cid:48)) to Ds
l = l + 1, s = s(cid:48)

end while
sample random minibatches of (s, a, r, s(cid:48)) from
Ds
update θQ via Z-step minibatch Q-learning ac-
cording to Equation (2)

12:
13:
14:
15:
16:
17:

18:
19:
20:

21:

22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:

39:

end for
# Planning ends
every C steps reset θQ(cid:48) = θQ

40:
41:
42:
43: end for

by the same DQN algorithm, operating on real ex-
perience in Du for learning and on simulated ex-
perience in Ds for planning. Thus, here we only
describe the way the simulated experience is gen-
erated.

Similar to Schatzmann et al. (2007), at the be-
ginning of each dialogue, we uniformly draw a
user goal G = (C, R), where C is a set of con-

straints and R is a set of requests (line 26 in Al-
gorithm 1). For movie-ticket booking dialogues,
constraints are typically the name and the date
of the movie, the number of tickets to buy, etc.
Requests can contain these slots as well as the
location of the theater, its start time, etc. Ta-
ble 3 presents some sampled user goals and di-
alogues generated by simulated and real users,
respectively. The ﬁrst user action au (line 27)
can be either a request or an inform dialogue-
act. A request, such as request(theater;
moviename=batman), consists of a request
slot and multiple ((cid:62) 1) constraint slots, uni-
formly sampled from R and C,
respectively.
An inform contains constraint slots only. The
user action can also be converted to natural lan-
guage via NLG, e.g., "which theater will
show batman?"

In each dialogue turn, the world model takes
as input the current dialogue state s and the last
agent action a (represented as an one-hot vector),
and generates user response au, reward r, and a
binary variable t, which indicates whether the di-
alogue terminates (line 33). The generation is ac-
complished using the world model M (s, a; θM ), a
MLP shown in Figure 3, as follows:

h = tanh(Wh(s, a) + bh)
r = Wrh + br
au = softmax(Wah + ba)
t = sigmoid(Wth + bt)

where (s, a) is the concatenation of s and a, and
W and b are parameter matrices and vectors, re-
spectively.

Figure 3: The world model architecture.

2.3 World Model Learning

In this process (lines 19-22 in Algorithm 1),
M (s, a; θM ) is reﬁned via minibatch SGD using
real experience in the replay buffer Du. As shown
in Figure 3, M (s, a; θM ) is a multi-task neural net-
work (Liu et al., 2015) that combines two classi-
ﬁcation tasks of simulating au and t, respectively,
and one regression task of simulating r. The lower
layers are shared across all tasks, while the top lay-
ers are task-speciﬁc.

3 Experiments and Results

We evaluate the DDQ method on a movie-ticket
booking task in both simulation and human-in-the-
loop settings.

3.1 Dataset

Raw conversational data in the movie-ticket book-
ing scenario was collected via Amazon Mechani-
cal Turk. The dataset has been manually labeled
based on a schema deﬁned by domain experts, as
shown in Table 4, which consists of 11 dialogue
acts and 16 slots. In total, the dataset contains 280
annotated dialogues, the average length of which
is approximately 11 turns.

3.2 Dialogue Agents for Comparison

To benchmark the performance of DDQ, we have
developed different versions of task-completion
dialogue agents, using variations of Algorithm 1.
• A DQN agent is learned by standard DQN,
implemented with direct reinforcement learn-
ing only (lines 5-18 in Algorithm 1) in each
epoch.

• The DDQ(K) agents are learned by DDQ of
Algorithm 1, with an initial world model pre-
trained on human conversational data, as de-
scribed in Section 3.1. K is the number of
planning steps. We trained different versions
of DDQ(K) with different K’s.

• The DDQ(K, rand-init θM ) agents are
learned by the DDQ method with a randomly
initialized world model.

• The DDQ(K, ﬁxed θM ) agents are learned
by DDQ with an initial world model pre-
trained on human conversational data. But
the world model is not updated afterwards.
That is, the world model learning part in Al-
gorithm 1 (lines 19-22) is removed. The
DDQ(K, ﬁxed θM ) agents are evaluated in
the simulation setting only.

• The DQN(K) agents are learned by DQN,
but with K times more real experiences than
the DQN agent. DQN(K) is evaluated in the
simulation setting only. Its performance can
be viewed as the upper bound of its DDQ(K)
counterpart, assuming that the world model
in DDQ(K) perfectly matches real users.

Implementation Details All the models in these
agents (Q(s, a; θQ), M (s, a; θM )) are MLPs with
tanh activations. Each policy network Q(.) has
one hidden layer with 80 hidden nodes. As shown
in Figure 3, the world model M (.) contains two
shared hidden layers and three task-speciﬁc hid-
den layers, with 80 nodes in each. All the agents
are trained by Algorithm 1 with the same set of
hyper-parameters. (cid:15)-greedy is always applied for
exploration. We set the discount factor γ = 0.95.
The buffer sizes of both Du and Ds are set to
5000. The target value function is updated at
the end of each epoch. In each epoch, Q(.) and
M (.) are reﬁned using one-step (Z = 1) 16-tuple-
minibatch update. 4
In planning, the maximum
length of a simulated dialogue is 40 (L = 40).
In addition, to make the dialogue training efﬁ-
cient, we also applied a variant of imitation learn-
ing, called Reply Buffer Spiking (RBS) (Lipton
et al., 2016). We built a naive but occasionally suc-
cessful rule-based agent based on human conver-
sational dataset (line 1 in Algorithm 1), and pre-
ﬁlled the real experience replay buffer Du with
100 dialogues of experience (line 2) before train-
ing for all the variants of agents.

3.3 Simulated User Evaluation

In this setting the dialogue agents are optimized
by interacting with user simulators, instead of real
users. Thus, the world model is learned to mimic
user simulators. Although the simulator-trained
agents are sub-optimal when applied to real users
due to the discrepancy between simulators and real
users, the simulation setting allows us to perform
a detailed analysis of DDQ without much cost and
to reproduce the experimental results easily.

4We found in our experiments that setting Z > 1 im-
proves the performance of all agents, but does not change
the conclusion of this study: DDQ consistently outperforms
DQN by a statistically signiﬁcant margin. Conceptually, the
optimal value of Z used in planning is different from that in
direct reinforcement learning, and should vary according to
the quality of the world model. The better the world model
is, the more aggressive update (thus bigger Z) is being used
in planning. We leave it to future work to investigate how to
optimize Z for planning in DDQ.

Agent

DQN
DDQ(5)
DDQ(5, rand-init θM )
DDQ(5, ﬁxed θM )
DQN(5)
DDQ(10)
DDQ(10, rand-init θM )
DDQ(10, ﬁxed θM )
DQN(10)

Epoch = 100

Epoch = 200

Epoch = 300

Success Reward
-3.84
.4260
20.35
.6056
18.75
.5904
14.54
.5540
29.38
.6560
.6624
28.18
21.50
.6132
18.41
.5884
48.61
.7944

Turns
31.93
26.65
26.21
25.89
21.76
24.62
26.16
26.41
15.43

Success Reward
10.78
.5308
36.76
.7128
33.47
.6888
29.72
.6660
41.09
.7344
.7664
42.46
32.43
.6864
24.17
.6196
54.00
.8296

Turns
22.72
19.55
20.36
22.39
16.07
21.01
21.86
22.36
13.09

Success Reward
27.66
.6480
39.97
.7372
36.06
.7032
33.58
.6860
43.97
.7576
.7840
45.11
42.37
.7628
26.70
.6412
54.89
.8356

Turns
22.21
18.99
18.64
19.49
15.88
19.94
20.32
22.49
12.77

Table 1: Results of different agents at training epoch = {100, 200, 300}. Each number is averaged
over 5 runs, each run tested on 2000 dialogues. Excluding DQN(5) and DQN(10) which serve as the
upper bounds, any two groups of success rate (except three groups: at epoch 100, DDQ(5, rand-init θM )
and DDQ(10, ﬁxed θM ), at epoch 200, DDQ(5, rand-init θM ) and DDQ(10, rand-init θM ), at epoch
300, DQN and DDQ(10, ﬁxed θM )) evaluated at the same epoch is statistically signiﬁcant in mean with
p < 0.01. (Success: success rate)

Figure 4: Learning curves of the DDQ(K) agents
with K = 2, 5, 10, 20. The DQN agent is identical
to a DDQ(K) agent with K = 0.

Figure 5: Learning curves of DQN, DDQ(10),
DDQ(10, rand-init θM ), DDQ(10, ﬁxed θM ), and
DQN(10).

User Simulator We adapted a publicly avail-
able user simulator (Li et al., 2016b) to the task-
completion dialogue setting. During training, the
simulator provides the agent with a simulated user
response in each dialogue turn and a reward sig-
nal at the end of the dialogue. A dialogue is
considered successful only when a movie ticket
is booked successfully and when the information
provided by the agent satisﬁes all the user’s con-
straints. At the end of each dialogue, the agent
receives a positive reward of 2 ∗ L for success, or
a negative reward of −L for failure, where L is
the maximum number of turns in each dialogue,
and is set to 40 in our experiments. Furthermore,
in each turn, the agent receives a reward of −1,
so that shorter dialogues are encouraged. Read-
ers can refer to Appendix B for details on the user
simulator.

Results The main simulation results are reported
in Table 1 and Figures 4 and 5. For each agent,
we report its results in terms of success rate, av-
erage reward, and average number of turns (aver-
aged over 5 repetitions of the experiments). Re-
sults show that the DDQ agents consistently out-
perform DQN with a statistically signiﬁcant mar-
gin. Figure 4 shows the learning curves of differ-
ent DDQ agents trained using different planning
steps. Since the training of all RL agents started
with RBS using the same rule-based agent, their
performance in the ﬁrst few epochs is very close.
After that, performance improved for all values of
K, but much more rapidly for larger values. Re-
call that the DDQ(K) agent with K=0 is identical
to the DQN agent, which does no planning but re-
lies on direct reinforcement learning only. Without
planning, the DQN agent took about 180 epochs
(real dialogues) to reach the success rate of 50%,

Agent

DQN
DDQ(5)
DDQ(5, rand-init θM )
DDQ(10)
DDQ(10, rand-init θM )

Epoch = 100

Epoch = 150

Epoch = 200

Success Reward
-58.69
.0000
00.78
.4620
-11.67
.3600
.5555
14.69
6.27
.5010

Turns
39.38
31.33
31.74
25.92
29.70

Success Reward
-5.730
.4080
15.05
.5637
13.71
.5500
.6416
25.85
22.11
.6055

Turns
30.38
26.17
26.58
24.28
23.11

Success Reward
0.350
.4545
19.84
.6000
16.84
.5752
.7332
38.88
36.90
.7023

Turns
30.38
26.32
26.37
20.21
21.20

Table 2: The performance of different agents at training epoch = {100, 150, 200} in the human-in-the-
loop experiments. The difference between the results of all agent pairs evaluated at the same epoch is
statistically signiﬁcant (p < 0.01). (Success: success rate)

and DDQ(10) took only 50 epochs.

Intuitively, the optimal value of K needs to be
determined by seeking the best trade-off between
the quality of the world model and the amount
of simulated experience that is useful for improv-
ing the dialogue agent. This is a non-trivial opti-
mization problem because both the dialogue agent
and the world model are updated constantly during
training and the optimal K needs to be adjusted
accordingly. For example, we ﬁnd in our experi-
ments that at the early stages of training, it is ﬁne
to perform planning aggressively by using large
amounts of simulated experience even though they
are of low quality, but in the late stages of train-
ing where the dialogue agent has been signif-
icantly improved, low-quality simulated experi-
ence is likely to hurt the performance. Thus, in our
implementation of Algorithm 1, we use a heuris-
tic5 to reduce the value of K in the late stages of
training (e.g., after 150 epochs in Figure 4) to mit-
igate the negative impact of low-qualify simulated
experience. We leave it to future work how to op-
timize the planning step size during DDQ training
in a principled way.

Figure 5 shows that the quality of the world
model has a signiﬁcant impact on the agent’s
performance. The learning curve of DQN(10)
indicates the best performance we can expect
with a perfect world model. With a pre-trained
world model, the performance of the DDQ agent
improves more rapidly, although eventually, the
DDQ and DDQ(rand-init θM ) agents reach the
same success rate after many epochs. The world
model learning process is crucial to both the ef-
ﬁciency of dialogue policy learning and the ﬁnal
performance of the agent. For example, in the
early stages (before 60 epochs), the performances
of DDQ and DDQ(ﬁxed θM ) remain very close to
each other, but DDQ reaches a success rate almost

5The heuristic is not presented in Algorithm 1. Readers

can refer to the released source code for details.

Figure 6: Human-in-the-loop dialogue policy
learning curves in four different agents.

10% better than DDQ(ﬁxed θM ) after 400 epochs.

3.4 Human-in-the-Loop Evaluation

In this setting, ﬁve dialogue agents (i.e., DQN,
DDQ(10), DDQ(10, rand-init θM ), DDQ(5), and
DDQ(5, rand-init θM )) are trained via RL by in-
teracting with real human users. In each dialogue
session, one of the agents was randomly picked to
converse with a user. The user was presented with
a user goal sampled from the corpus, and was in-
structed to converse with the agent to complete the
task. The user had the choice of abandoning the
task and ending the dialogue at any time, if she or
he believed that the dialogue was unlikely to suc-
ceed or simply because the dialogue dragged on
for too many turns. In such cases, the dialogue ses-
sion is considered failed. At the end of each ses-
sion, the user was asked to give explicit feedback
whether the dialogue succeeded (i.e., whether the
movie tickets were booked with all the user con-
straints satisﬁed). Each learning curve is trained
with two runs, with each run generating 150 dia-
logues (and K∗150 additional simulated dialogues
when planning is applied). In total, we collected
1500 dialogue sessions for training all ﬁve agents.
The main results are presented in Table 2 and

Simulation Sample

movie-ticket booking user goal:
{

Real User Sample

movie-ticket booking user goal:
{

“constraint slots”: {

“numberofpeople”:“2”
“moviename”: “deadpool”
“city”: “seattle”
}

“request slots”: {
“ticket”: “?”
“theater”: “?”
“starttime”: “?”

“constraint slots”: {

“date”:“this weekend”
“numberofpeople”: “1”
“moviename”: “batman”
}

“request slots”: {
“ticket”: “?”
“theater”: “?”
“starttime”: “?”
“date”: “?”

}

}

}

}

DQN agent

usr: Hi, what date can I book 2 tickets for deadpool?
1
agt: Which city would you like?
2
usr: I want to watch at seattle.
3
agt: What time would you like to see it?
4
usr: What start time is available?
5
agt: Which theater would you like?
6
usr: Which theater is available?
7
agt: What time would you like to see it?
8
usr: What start time is available?
9
10 agt: Which theater would you like?
11 usr: Which theater is available?
12 agt: How many tickets do you need?
13 usr: I want 2 tickets please!
14 agt: Which theater would you like?
15 usr: Which theater is available?
16 agt: What time would you like to see it?

usr: Hi, which theater will show batman? I want 1 ticket.
agt: Which city would you like?
usr: Boston.
agt: What time would you like to see it?
usr: What start time is available?
agt: What date would you like to see it?
usr: This weekend.
agt:Which theater would you like?
usr: Which theater is available?
agt: Which theater would you like?
usr: Which theater is available?
agt: What time would you like to see it?
usr: What start time is available?
agt: Which theater would you like?
usr: Thanks, I do not need the ticket.
agt: Thank you.

...

39 agt: Which theater would you like?
40 usr: Which theater is available?

Failure:

Agent does not book the right ticket within the
max turns.

Failure:

User terminates the dialogue before reaching the
max turns because the agent repeatly asks the same
question.

DDQ(10) agent

1
2
3
4
5
6
7

8

usr: Hi, when is deadpool playing at seattle?
agt: Tonight.
usr: What is the start time?
agt: Which theater would you like?
usr: I want to watch it at amc paciﬁc place 11.
agt: 10pm is available.
usr: I want 2 tickets please!

usr: Hi, which theater will show batman? I want 1 ticket.
agt: Century Eastport 16.
usr: What is the start time?
agt: 9PM.
usr: Can I buy a ticket here?
agt: This weekend?
usr: Yes, one ticket please!

agt:

Great, here are 2 deadpool tickets for you
at amc paciﬁc place 11 in seattle tonight 10 pm.

agt:

Sure, here it is - batman at Century Eastport 16,
9PM this weekend.

usr: Thank you.
9
10 agt: Thank you.

usr: Thank you.
agt: Thank you.

Success

Success

Table 3: Two sample dialogue sessions by DQN and DDQ(10) agents trained at epoch 100: Left: simu-
lated user experiments; Right: human-in-the-loop experiments. (agt: agent, usr: user)

Figure 6, with each agent averaged over two in-
dependent runs. The results conﬁrm what we ob-
served in the simulation experiments. The conclu-
sions are summarized as below:

• The DDQ agent signiﬁcantly outperforms
DQN, as demonstrated by the comparison be-
tween DDQ(10) and DQN. Table 3 presents
four example dialogues produced by two di-
alogue agents interacting with simulated and
human users, respectively. The DQN agent,
after being trained with 100 dialogues, still
behaved like a naive rule-based agent that re-

quested information bit by bit in a ﬁxed or-
der. When the user did not answer the request
which theater
explicitly (e.g., usr:
is available?), the agent failed to re-
spond properly. On the other hand, with plan-
ning, the DDQ agent trained with 100 real
dialogues is much more robust and can com-
plete 50% of user tasks successfully.

• A larger K leads to more aggressive planning
and better results, as shown by DDQ(10) vs.
DDQ(5).

• Pre-training world model with human con-

versational data improves the learning efﬁ-
ciency and the agent’s performance, as shown
by DDQ(5) vs. DDQ(5, rand-init θM ), and
DDQ(10) vs. DDQ(10, rand-init θM ).

4 Conclusion

We propose a new strategy for a task-completion
dialogue agent to learn its policy by interacting
with real users. Compared to previous work, our
agent learns in a much more efﬁcient way, us-
ing only a small number of real user interactions,
which amounts to an affordable cost in many non-
trivial domains. Our strategy is based on the Deep
Dyna-Q (DDQ) framework where planning is in-
tegrated into dialogue policy learning. The ef-
fectiveness of DDQ is validated by human-in-the-
loop experiments, demonstrating that a dialogue
agent can efﬁciently adapt its policy on the ﬂy by
interacting with real users via deep RL.

One interesting topic for future research is ex-
ploration in planning. We need to deal with the
challenge of adapting the world model in a chang-
ing environment, as exempliﬁed by the domain ex-
tension problem (Lipton et al., 2016). As pointed
out by Sutton and Barto (1998), the general prob-
lem here is a particular manifestation of the con-
In a
ﬂict between exploration and exploitation.
planning context, exploration means trying actions
that may improve the world model, whereas ex-
ploitation means trying to behave in the optimal
way given the current model. To this end, we want
the agent to explore in the environment, but not so
much that the performance would be greatly de-
graded.

Acknowledgments

We would like to thank Chris Brockett, Yun-Nung
Chen, Michel Galley and Lihong Li for their in-
sightful comments on the paper. We would like
to acknowledge the volunteers from Microsoft Re-
search for helping us with the human-in-the-loop
experiments. This work was done when Baolin
Peng and Shang-Yu Su were visiting Microsoft.
Baolin Peng is in part supported by Innovation
and Technology Fund (6904333), and General Re-
search Fund of Hong Kong (12183516).

References

Pawel Budzianowski, Stefan Ultes, Pei-Hao Su, Nikola
Mrksic, Tsung-Hsien Wen, Inigo Casanueva, Lina

Rojas-Barahona, and Milica Gasic. 2017.
Sub-
domain modelling for dialogue management with
hierarchical reinforcement learning. arXiv preprint
arXiv:1706.06210 .

Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao,
Yun-Nung Chen, Faisal Ahmed, and Li Deng. 2017.
Towards end-to-end reinforcement learning of dia-
In Proceed-
logue agents for information access.
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). volume 1, pages 484–495.

Milica Gaˇsi´c, Filip Jurˇc´ıˇcek, Simon Keizer, Franc¸ois
Mairesse, Blaise Thomson, Kai Yu, and Steve
Young. 2010. Gaussian processes for fast policy op-
timisation of pomdp-based dialogue managers.
In
Proceedings of the 11th Annual Meeting of the Spe-
cial Interest Group on Discourse and Dialogue. As-
sociation for Computational Linguistics, pages 201–
204.

Milica Gaˇsi´c, Filip Jurˇc´ıˇcek, Blaise Thomson, Kai Yu,
and Steve Young. 2011. On-line policy optimisation
of spoken dialogue systems via live interaction with
In Automatic Speech Recognition
human subjects.
and Understanding (ASRU), 2011 IEEE Workshop
on. IEEE, pages 312–317.

Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and
Sergey Levine. 2016. Continuous deep q-learning
In International
with model-based acceleration.
Conference on Machine Learning. pages 2829–
2838.

Dilek Hakkani-T¨ur, Gokhan Tur, Asli Celikyilmaz,
Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-
Yi Wang. 2016. Multi-domain joint semantic frame
In Pro-
parsing using bi-directional RNN-LSTM.
ceedings of The 17th Annual Meeting of the Interna-
tional Speech Communication Association.

Leonid Kuvayev and Richard S Sutton. 1996. Model-
based reinforcement learning with an approximate,
learned model. In in Proceedings of the Ninth Yale
Workshop on Adaptive and Learning Systems. Cite-
seer.

Esther Levin, Roberto Pieraccini, and Wieland Eck-
ert. 1997. Learning dialogue strategies within the
markov decision process framework. In Automatic
Speech Recognition and Understanding, 1997. Pro-
ceedings., 1997 IEEE Workshop on. IEEE, pages
72–79.

Jiwei Li, Alexander H Miller, Sumit Chopra,
Marc’Aurelio Ranzato, and Jason Weston. 2016a.
Dialogue learning with human-in-the-loop. arXiv
preprint arXiv:1611.09823 .

Xiujun Li, Zachary C Lipton, Bhuwan Dhingra, Lihong
Li, Jianfeng Gao, and Yun-Nung Chen. 2016b. A
user simulator for task-completion dialogues. arXiv
preprint arXiv:1612.05688 .

Xuijun Li, Yun-Nung Chen, Lihong Li, Jianfeng Gao,
End-to-end task-
and Asli Celikyilmaz. 2017.
In Proceed-
completion neural dialogue systems.
ings of the The 8th International Joint Conference
on Natural Language Processing. pages 733–743.

Zachary C Lipton, Jianfeng Gao, Lihong Li, Xiujun
Li, Faisal Ahmed, and Li Deng. 2016. Efﬁcient
exploration for dialogue policy learning with bbq
networks & replay buffer spiking. arXiv preprint
arXiv:1608.05081 .

Bing Liu and Ian Lane. 2017. Iterative policy learning
in end-to-end trainable task-oriented neural dialog
models. In Proceedings of 2017 IEEE Workshop on
Automatic Speech Recognition and Understanding.

Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,
Kevin Duh, and Ye-Yi Wang. 2015. Representation
learning using multi-task deep neural networks for
semantic classiﬁcation and information retrieval .

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. Na-
ture 518(7540):529–533.

Andrew W Moore and Christopher G Atkeson. 1993.
Prioritized sweeping: Reinforcement learning with
Machine learning
less data and less time.
13(1):103–130.

Nikola Mrkˇsi´c, Diarmuid O S´eaghdha, Tsung-Hsien
Wen, Blaise Thomson, and Steve Young. 2016.
Neural belief tracker: Data-driven dialogue state
tracking. arXiv preprint arXiv:1606.03777 .

Alexandros Papangelis. 2012. A comparative study of
reinforcement learning techniques on dialogue man-
In Proceedings of the Student Research
agement.
Workshop at the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
pages 22–31.

Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu,
Yun-Nung Chen, and Kam-Fai Wong. 2017a. Ad-
versarial advantage actor-critic model
task-
completion dialogue policy learning. arXiv preprint
arXiv:1710.11277 .

for

Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao,
Asli Celikyilmaz, Sungjin Lee, and Kam-Fai Wong.
2017b. Composite task-completion dialogue policy
learning via hierarchical deep reinforcement learn-
ing. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing.
pages 2221–2230.

Olivier Pietquin, Matthieu Geist, Senthilkumar Chan-
Sample efﬁcient on-
dramohan, et al. 2011.
line learning of optimal dialogue policies with
kalman temporal differences. In IJCAI Proceedings-
International Joint Conference on Artiﬁcial Intelli-
gence. volume 22, page 1878.

Olivier Pietquin and Helen Hastie. 2013. A survey on
metrics for the evaluation of user simulations. The
knowledge engineering review .

S´ebastien Racani`ere, Th´eophane Weber, David Re-
ichert, Lars Buesing, Arthur Guez, Danilo Jimenez
Rezende, Adri`a Puigdom`enech Badia, Oriol
Vinyals, Nicolas Heess, Yujia Li, et al. 2017.
Imagination-augmented agents for deep reinforce-
ment learning. In Advances in Neural Information
Processing Systems. pages 5694–5705.

Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based user
simulation for bootstrapping a pomdp dialogue sys-
In NAACL 2007; Companion Volume, Short
tem.
Papers. Association for Computational Linguistics,
pages 149–152.

David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George Van Den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, et al. 2016a. Mastering
the game of go with deep neural networks and tree
search. Nature 529(7587):484–489.

David Silver, Julian Schrittwieser, Karen Simonyan,
Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian
Bolton, et al. 2017. Mastering the game of go with-
out human knowledge. Nature 550(7676):354.

David Silver, Hado van Hasselt, Matteo Hessel, Tom
Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-
Arnold, David Reichert, Neil Rabinowitz, Andre
The predictron: End-
Barreto, et al. 2016b.
arXiv preprint
to-end learning and planning.
arXiv:1612.08810 .

Satinder Singh, Diane Litman, Michael Kearns, and
Marilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experiments
with the njfun system. Journal of Artiﬁcial Intelli-
gence Research 16:105–133.

Satinder P Singh. 1992. Reinforcement learning with
In Proceedings of
a hierarchy of abstract models.
the National Conference on Artiﬁcial Intelligence.
JOHN WILEY & SONS LTD, 10, page 202.

Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-
Barahona, Stefan Ultes, David Vandyke, Tsung-
Hsien Wen, and Steve Young. 2016a. Continu-
ously learning neural dialogue management. arXiv
preprint arXiv:1606.02689 .

Jing Peng and Ronald J Williams. 1993. Efﬁcient
learning and planning within the dyna framework.
Adaptive Behavior 1(4):437–454.

Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-
Barahona, Stefan Ultes, David Vandyke, Tsung-
Hsien Wen, and Steve Young. 2016b. On-line active

reward learning for policy optimisation in spoken di-
alogue systems. arXiv preprint arXiv:1605.07669 .

Richard S Sutton. 1990.

Integrated architectures for
learning, planning, and reacting based on approx-
In Proceedings
imating dynamic programming.
of the seventh international conference on machine
learning. pages 216–224.

Richard S Sutton and Andrew G Barto. 1998. Introduc-
tion to reinforcement learning, volume 135. MIT
press Cambridge.

Richard S Sutton, Csaba Szepesv´ari, Alborz Geram-
ifard, and Michael P Bowling. 2012. Dyna-style
planning with linear function approximation and pri-
oritized sweeping. arXiv preprint arXiv:1206.3285
.

Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine,
and Pieter Abbeel. 2016. Value iteration networks.
In Advances in Neural Information Processing Sys-
tems. pages 2154–2162.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-
Hao Su, David Vandyke, and Steve Young. 2015.
Semantically conditioned lstm-based natural lan-
guage generation for spoken dialogue systems.
arXiv preprint arXiv:1508.01745 .

Jason D Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid code networks: Practical and efﬁcient
end-to-end dialog control with supervised and rein-
forcement learning. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics.

Steve Young, Milica Gaˇsi´c, Blaise Thomson, and Ja-
son D Williams. 2013. Pomdp-based statistical spo-
ken dialog systems: A review. Proceedings of the
IEEE 101(5):1160–1179.

Tiancheng Zhao and Maxine Eskenazi. 2016. To-
wards end-to-end learning for dialog state tracking
and management using deep reinforcement learning.
arXiv preprint arXiv:1606.02560 .

A Dataset Annotation Schema

Table 4 lists all annotated dialogue acts and slots
in details.

Annotations

Intent

Slot

request, inform, deny, conﬁrm question,
conﬁrm answer, greeting, closing, not sure,
multiple choice, thanks, welcome
city, closing, date, distanceconstraints,
greeting, moviename, numberofpeople,
price, starttime, state, taskcomplete, theater,
theater chain, ticket, video format, zip

Table 4: The data annotation schema

B User Simulator

In the task-completion dialogue setting, the entire
conversation is around a user goal implicitly, but
the agent knows nothing about the user goal ex-
plicitly and its objective is to help the user to ac-
complish this goal. Generally, the deﬁnition of
user goal contains two parts:

• inform slots contain a number of slot-value
pairs which serve as constraints from the user.
• request slots contain a set of slots that user
has no information about
the values, but
wants to get the values from the agent dur-
ticket is a default slot
ing the conversation.
which always appears in the request slots
part of user goal.

To make the user goal more realistic, we add
some constraints in the user goal: slots are split
into two groups. Some of slots must appear in the
user goal, we called these elements as Required
slots. In the movie-booking scenario, it includes
moviename,
theater, starttime, date, num-
berofpeople; the rest slots are Optional slots, for
example, theater chain, video format etc.

We generated the user goals from the labeled
dataset mentioned in Section 3.1, using two mech-
anisms. One mechanism is to extract all the slots
(known and unknown) from the ﬁrst user turns (ex-
cluding the greeting user turn) in the data, since
usually the ﬁrst turn contains some or all the re-
quired information from user. The other mech-
anism is to extract all the slots (known and un-
known) that ﬁrst appear in all the user turns,
and then aggregate them into one user goal. We
dump these user goals into a ﬁle as the user-goal
database. Every time when running a dialogue, we
randomly sample one user goal from this user goal
database.

