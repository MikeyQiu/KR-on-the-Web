JATE 2.0: Java Automatic Term Extraction with Apache Solr

Ziqi Zhang, Jie Gao, Fabio Ciravegna
Regent Court, 211 Portobello, Shefﬁeld, UK, S1 4DP
ziqi.zhang@shefﬁeld.ac.uk, j.gao@shefﬁeld.ac.uk, f.ciravegna@shefﬁeld.ac.uk

Abstract
Automatic Term Extraction (ATE) or Recognition (ATR) is a fundamental processing step preceding many complex knowledge
engineering tasks. However, few methods have been implemented as public tools and in particular, available as open-source freeware.
Further, little effort is made to develop an adaptable and scalable framework that enables customization, development, and comparison
of algorithms under a uniform environment. This paper introduces JATE 2.0, a complete remake of the free Java Automatic Term
Extraction Toolkit (Zhang et al., 2008) delivering new features including: (1) highly modular, adaptable and scalable ATE thanks to
integration with Apache Solr, the open source free-text indexing and search platform; (2) an extended collection of state-of-the-art
algorithms. We carry out experiments on two well-known benchmarking datasets and compare the algorithms along the dimensions of
effectiveness (precision) and efﬁciency (speed and memory consumption). To the best of our knowledge, this is by far the only free ATE
library offering a ﬂexible architecture and the most comprehensive collection of algorithms.

Keywords: term extraction, term recognition, NLP, text mining, Solr, search, indexing

1.

Introduction

Automatic Term Extraction (or Recognition) is an impor-
tant Natural Language Processing (NLP) task that deals
with the extraction of terminologies from domain-speciﬁc
textual corpora. ATE is widely used by both industries
and researchers in many complex tasks, such as Informa-
tion Retrieval (IR), machine translation, ontology engineer-
ing and text summarization (Bowker, 2003; Brewster et
al., 2007; Maynard et al., 2007). Over the years, there
has been constant effort on researching new algorithms,
adapting to different domains and/or languages, and creat-
ing benchmarking datasets (Ananiadou, 1994; Church and
Gale, 1995; Ahmad et al., 1999; Frantzi et al., 2000; Park
et al., 2002; Matsuo and Ishizuka, 2003; Sclano and Ve-
lardi, 2007; Rose et al., 2010; Spasi´c et al., 2013; Zadeh
and Handschuh, 2014).
Given the fundamental role of ATE in many tasks, a real
beneﬁt to the community would be releasing the tools to
facilitate the development of downstream applications, also
encouraging code-reuse, comparative studies, and fostering
further research. Unfortunately, effort in this direction has
been extremely limited. First, only very few tools have been
published with readily available source code (Spasi´c et al.,
2013) and some are proprietary (labs.translated.net, 2015;
Yahoo!, 2015), or no longer maintained (Sclano and Ve-
lardi, 2007)1. Second, different tools have been developed
under different scenarios and evaluated in different domains
using proprietary language resources, making it difﬁcult for
comparison. Third, it is unclear whether and how well these
tools can adapt to different domain tasks and scale up to
large data.
To the best of our knowledge, the only effort towards ad-
dressing these issues is the Java Automatic Term Extrac-
tion Toolkit (JATE)2, where we originally implemented
ﬁve state-of-the-art algorithms under a uniform framework
(Zhang et al., 2008). This work makes one step further

1Original link: http://lcl2.di.uniroma1.it/
2https://code.google.com/p/jatetoolkit.

Google Code has been shut down since Jan 2016.

by completely re-designing and re-implementing JATE to
fulﬁll three goals: adaptability, scalability, and extended
collections of algorithms. The new library, named JATE
2.03, is built on the Apache Solr4 free-text indexing and
search platform and can be either used as a separate mod-
ule, or a Solr plugin during document processing to en-
rich the indexed documents with terms. JATE 2.0 deliv-
ers domain-speciﬁc adaptability and scalability by seam-
lessly integrating with the Solr text analysis capability in
the form of ‘analyzers’5, allowing users to directly exploit
the highly modular and scalable Solr text analysis to facili-
tate term extraction for large data from various content for-
mats. Last but not least, JATE 2.0 expands its collection
of state-of-the-art algorithms6 to double that in the origi-
nal JATE release. We also evaluate these algorithms under
uniform settings on two publicly available benchmarking
datasets. Released as open source software, we believe that
JATE 2.0 offers signiﬁcant value to both researchers and
practitioners.
The remainder of this paper is structured as follows. Sec-
tion 2. discusses related work. Section 3. describes JATE
2.0 in details. Section 5. describes experiments and dis-
cusses results. Section 6. concludes this paper.

2. Related Work
We describe related work from three persectives: ATE
methods in general, ATE tools and software, and text min-
ing plugins based on Solr.
ATE methods typically start with linguistic processors
as the ﬁrst stage to extract candidate terms; followed by

3https://github.com/ziqizhang/jate
4http://lucene.apache.org/solr/
5https://cwiki.apache.org/confluence/

display/solr/Analyzers

6By this we mean the mathematical formulation of term candi-
date scores. We do not claim identical replication of state-of-the-
art methods, as they often differ in pre- and post-processing, such
as term candidate generation, term variant handling and the use of
dictionary.

2262

statistics-based term ranking algorithms to perform can-
didate ﬁltering. Linguistic processors are domain spe-
ciﬁc, and designed to capture term formation and collo-
cation patterns. They often take two forms: ‘closed ﬁl-
ters’ (Arora et al., 2014) focus on precision and are usu-
ally restricted to nouns or noun sequences. ‘Open ﬁlters’
(Frantzi et al., 2000) are more permissive and often al-
low adjectives, adverbs, etc. For both, widely used tech-
niques include Part-of-Speech (PoS) tag sequence match-
ing, N-gram extraction, Noun Phrase (NP) Chunking, and
dictionary lookup. The selection of linguistic processors is
often domain-speciﬁc and important for the trade-off be-
tween precision and recall. The ranking algorithms are of-
ten based on: ‘unithood’ indicating the collocation strength
of units that comprise a single term (Matsuo and Ishizuka,
2003); and ‘termhood’ indicating the association strength
of a term to domain concepts (Ahmad et al., 1999). Most
methods combine both termhood and unithood measures
(Ananiadou, 1994; Frantzi et al., 2000; Park et al., 2002;
Sclano and Velardi, 2007; Spasi´c et al., 2013). Two unique
features of JATE 2.0 are (1) customization of linguistic pro-
cessors of the two forms hence making it adaptable to many
domains and languages, and (2) a wide selection of imple-
mented ranking algorithms, which are not available in any
other tools.
Existing ATE tools and software are extremely limited
for several reasons.
First, many are proprietary soft-
ware (labs.translated.net, 2015), quota-limited (e.g., Open-
Calais7), or restricted to academic usage only (e.g., Ter-
Mine8). Second, all tools, including the original JATE
(Zhang et al., 2008), are built in a monolithic architec-
ture which allows very limited customization and scala-
bility (Spasi´c et al., 2013). Third, most offer very limited
(typically a single choice) conﬁgurability, e.g., in terms of
term candidate extraction and/or ranking algorithms (e.g.,
TermRaider (Maynard et al., 2008), TOPIA9, and Flex-
iTerm (Spasi´c et al., 2013). JATE 2.0 is a free, open-source
library. It is highly modular, allowing customization and
conﬁguration of many components. Meanwhile, to our best
knowledge, no available ATE tools support automatic in-
dexing.
Solr plugins for text mining Solr is an open source en-
terprise search platform written in Java. It is highly mod-
ular, customizable, and scalable, hence widely used as an
industry standard. A core component of Solr is its power-
ful, extensible text processing library covering a wide range
of functionality and languages. Many text mining and NLP
tools have integrated with Solr to beneﬁt from these fea-
tures as well as contributing additional support for doc-
ument indexing (e.g., OpenNLP plugins10, SolrTextTag-
ger11, and UIMA12). However, no plugin is available for

7http://new.opencalais.com/
8http://www.nactem.ac.uk/software/

9https://pypi.python.org/pypi/topia.

termine/

termextract

10https://wiki.apache.org/solr/OpenNLP
11https://github.com/OpenSextant/

SolrTextTagger

12https://uima.apache.org

ATE.

3.

JATE 2.0 Architecture

Figure 1 shows the general architecture of JATE 2.0 and
its workﬂow, which consists of four phases: (1) data pre-
processing; (2) term candidate extraction and indexing; (3)
candidate scoring and ﬁltering and (4) ﬁnal term indexing
and export.
Data pre-processing (Section 3.1.) parses input docu-
ments to raw text content and performs text normaliza-
tion in order to reduce ‘noise’ in irregular data. The pre-
processed text content then passes through the candidate
extraction (Section 3.2.) component that extracts and nor-
malizes term candidates from each document. Candidate
extraction and data pre-processing are embedded as part
of the Solr document indexing process, largely realized by
Solr’s ‘analyzers’. An analyzer is composed of a series
of processors (‘tokenizers’ and ‘ﬁlters’) to form a pipeline,
or an analysis ‘chain’ that examines the text content from
each document, generates a token stream and records sta-
tistical information. Solr already implements a very large
text processing library for this purpose. JATE 2.0 further
extends these. Depending on individual needs, users may
assemble customized analyzers for term candidate genera-
tion. Deﬁning analyzers is achieved by conﬁguration in the
Solr schema, such as that shown in Figure 2.
Next, the candidates are processed by the subsequent ﬁlter-
ing component (Section 3.3.), where different ATE algo-
rithms can be conﬁgured to score and rank the candidates,
and making the ﬁnal selection. The implementation of the
algorithms are largely parallelized. The resulting terms can
either be exported for further validation, or written back to
the index to annotate documents.
JATE 2.0 can be run in two modes: (1) as a separate mod-
ule to extract terms from a corpus using an embedded Solr
instance (Section 4.1.), or (2) as a Solr plugin that performs
ATE as part of the document indexing process to anno-
tate documents with domain-speciﬁc terminologies (Sec-
tion 4.2.). Both modes undergo the same workﬂow de-
scribed above.
JATE 2.0 offers adaptability and scalability thanks to its
seamless integration with Solr and parallelized implemen-
tation. Access to Solr’s very large collection of multi-
lingual text processing libraries that can be conﬁgured in
a plug-and-play way enables JATE 2.0 to be adapted to dif-
ferent languages, document formats, and domains. The rich
collection of algorithms also give users options to better
tailor the tool to speciﬁc use cases. Parallelization enables
JATE 2.0 to scale up to large datasets. Parallelization for
term candidate generation can be achieved via Solr’s dis-
tributed indexing capability under SolrCloud13. Users sim-
ply follow Solr’s guide on conﬁguring distributed indexing.
Parallelization for scoring and ranking algorithms comes
as standard in JATE 2.0 and is also conﬁgurable. Detailed
guidelines on deﬁning analyzers for ATE and conﬁgura-
tions can be found on the JATE 2.0 homepage.

13https://cwiki.apache.org/confluence/

display/solr/SolrCloud

2263

tokenization, such as line 3 of schema.xml shown in Figure
2.

3.2. Candidate Extraction
Term candidate extraction is realized as part of the Solr in-
dexing process. A dedicated ‘ﬁeld’19 is deﬁned within the
Solr indexing schema to hold candidates for each document
(candidate ﬁeld). The ﬁeld is bound to a ‘ﬁeldType’, which
uses a customized analyzer to break content into term can-
didates and normalize them to conﬂate the variants that are
dispersed throughout the corpus. The Solr indexing pro-
cess also generates basic statistical information about can-
didates, such as a candidate’s frequency within each docu-
ment, and the number of documents it appears in20.
The analyzer is highly customizable and adaptable. All ex-
isting Solr text analysis libraries can be used, such as tok-
enization, case folding, stop words removal, token N-gram
extraction, stemming, etc. It is also easy to implement new
components to replace or complement existing ones in a
plug-and-play way. Currently, Solr 5.3 supports nearly 20
implementations of tokenization and over 100 text normal-
ization and ﬁltering methods21, covering a wide range of
languages and use cases. Thus by selecting different com-
ponents and assembling them in different orders, one can
create different analyzers to achieve different extraction and
normalization results.
To support ATE, JATE 2.0 extends Solr’s text analysis li-
braries by supporting three types of linguistic ﬁlters for
term candidate extraction. These include: (1) a PoS pattern
based chunker that extracts candidates based on user spec-
iﬁed patterns (e.g., line 9 of schema.xml in Figure 2); (2)
a token N-gram extractor that extends the built-in one; (3)
a noun phrase chunker. All of these are implemented with
the capability to normalize noisy terms, such as removing
leading and trailing stop words, and non-alphanumeric to-
kens.
In addition, JATE 2.0 also implements an English
lemmatizer, which is a recommended replacement of stem-
mers that can sometimes be too aggressive for ATE. These
offer great ﬂexibility enabling almost any customized term
candidate extraction. Figure 2 shows a standard conﬁgu-
ration of an text analyzer for PoS pattern based candidate
extraction for English.

3.3. Filtering
Extracted term candidates are further processed to make a
ﬁnal decision to separate terms from non-terms.

3.3.1. Pre-ﬁltering
It is often a common practice to apply a ﬁltering process to
term candidates to reduce both noise (e.g., due to irregular
textual data or erroneous PoS tagging) and computation be-
fore the subsequent step of term scoring and ranking. The

19https://cwiki.apache.org/confluence/

display/solr/Solr+Glossary

20For practical reasons, we rely on a separate ﬁeld that indexes
the same content as token N-grams and stored term vectors which
provides the lookup of statistics about term candidates. Details on
this can be found on the JATE website and Solr documentation for
Inverted Index.

21See Solr API documentation.

Figure 1: General Architecture of JATE 2.0

support multi-lingual

3.1. Data pre-processing
With the standard JATE 2.0 conﬁguration, data pre-
processing begins with Solr Content Extraction Library
(also known as SolrCell14) to extract textual content from
It integrates the popular Apache Tika framework15
ﬁles.
that supports detecting and extracting metadata and text
from a large range of ﬁle formats such as plain text, HTML,
PDF, and Microsoft Ofﬁce. To use this, users simply con-
ﬁgure an instance of ExtractingRequestHandler16
for their data in Solr.
To
also use Solr’s
of
speciﬁc analyzers
UpdateRequestProcessor17.
Next, a recommended practice for pre-processing irregu-
lar textual data (commonly found in corporate datasets)
is applying the Solr char ﬁlter component18
to per-
form character-level
For example,
HTMLStripCharFilterFactory detects HTML en-
tities (e.g., ‘&#65;’, ‘&nbsp;’) with corresponding charac-
ters. This can reduce errors in downstream processors (e.g.,
PoS tagger) in the analyzer. This can be conﬁgured as part
of the analyzer in the Solr schema, usually placed before

users may
capability for detecting languages
to language-
indexing using the langid

in documents and map text

text normalization.

documents,

texts

for

14https://cwiki.apache.org/confluence/

display/solr/Uploading+Data+with+Solr+Cell+
using+Apache+Tika

15https://tika.apache.org
16https://wiki.apache.org/solr/

ExtractingRequestHandler

17https://cwiki.apache.org/confluence/
display/solr/Detecting+Languages+During+
Indexing

18https://cwiki.apache.org/confluence/

display/solr/CharFilterFactories

2264

Figure 2: Example of PoS pattern based candidate extraction

following strategies have been implemented in JATE 2.0
and can be conﬁgured according to user needs to balance
precision and recall.
Minimal and maximal character length restricts candi-
dates to a ﬁxed range of character length. Minimal and
maximal token number requires a valid candidate to con-
tain certain number of tokens (e.g., to limit multi-word ex-
pressions). Minimal stop words removal removes lead-
ing and/or trailing stop words in a multi-word expression
(e.g.,‘the cat’ becomes ‘cat’, but ‘Tower of London’ re-
mains intact). All these are implemented as part of the
three term candidate extraction methods described before
and can be conﬁgured in the analyzer.
Frequency threshold allows candidates to be ﬁltered by a
minimum total frequency in the corpus. This is often used
in typical ATE methods, and is conﬁgurable via individual
scoring and ranking algorithms that follow.

3.3.2. Scoring and ranking
In this step, term candidates that pass the pre-ﬁlter are
scored and ranked. Their basic statistical information is
gathered from the Solr index, to create complex features
required by different algorithms.
JATE 2.0 has implemented 10 algorithms for scoring can-
didates, listed in Table 1. TTF is the total frequency of a
candidate in the corpus. It’s usage in ATE was ﬁrstly docu-
mented in (Justeson and Katz, 1995). ATTF divides TTF by
the number of documents a candidate appears in. TTF-IDF
adapts the classic document-speciﬁc TF-IDF used in IR to
work at corpus level, by replacing TF with TTF.
RIDF was initially proposed by (Church and Gale, 1995)
as an enhancement to IDF to identify keywords in a doc-
ument collection. It measures the deviation of the actual
IDF score of a word from its ‘expected’ IDF score, which
is predicted based on a Poisson distribution. The hypothe-
sis is based on the fact that Poisson model ﬁts poorly with
such keywords. Thus a prediction of IDF based on Poisson
can deviate from its actual IDF observed based on a cor-

pus. Empirically, it is shown that all words have real IDF
scores that deviate from the expected value under a Poisson
distribution. However, keywords tend to have larger devi-
ations than non-keywords. We adapt it to work with term
candidates that can be either single words or multi-word
expressions.
CValue observes that real terms in technical domains are
often long, multi-word expressions and usually not nested
in other terms (i.e., as part of the longer terms). Frequency-
based methods are not effective for such terms as (1) nested
term candidates will have at least the same and often higher
frequency, and (2) the fact that a longer string appears n
times is a lot more important than that of a shorter string
appearing n times. Thus CValue computes a score that is
based on the frequency of a candidate and its length, then
adjusted by the frequency of longer candidates that contain
it.
Similarly, RAKE is designed to favour longer multi-word
expressions. It ﬁrstly computes a score for individual words
based on two components: one that favours words oc-
curring often and in longer term candidates, and one that
favours words occurring frequently regardless of the words
which they co-occur with. Then it adds up the scores of
composing words for a candidate.
χ2 promotes term candidates that co-occur very often with
‘frequent’ candidates in the corpus. First, candidates are
ranked by frequency in the corpus and a subset (typically
top n%) is selected - to be called ‘frequent terms’. Next,
candidates are scored based on the degree to which their
co-occurrence with these frequent terms are biased. These
biases can be due to semantic, lexical, or other relations of
two terms. Thus, a candidate showing strong co-occurrence
biases with frequent terms may have an important mean-
ing. To evaluate the statistical signiﬁcance of the biases,
χ2 is used. For each candidate, co-occurrence frequency
with the frequent terms is regarded as a sample value. The
null hypothesis that we expect to reject is that ‘occurrence
of a candidate is independent from occurrence of frequent

2265

3.3.3. Threshold cutoff
Next, a cutoff decision is to be made to separate real terms
from non-terms based on their scores. Three options are
supported: (1) hard threshold, where term candidates with
scores no less than the threshold are chosen; (2) top K,
where top ranked K candidates are chosen; (3) and top K%,
where the top ranked K percentage of candidates are cho-
sen.

4. Using JATE 2.0 in Two Modes

4.1. Embedded mode

The embedded mode is recommended when users need a
list of domain-speciﬁc terms from a corpus to be used in
subsequent knowledge engineering tasks. Users conﬁgure
a Solr instance and in particular, a text analysis chain that
deﬁnes how term candidates are extracted and normalized.
The Solr instance is instantiated as an embedded24 module
that interacts with other components. Users then explicitly
start an indexing process on a corpus to trigger term candi-
date extraction, then use speciﬁc ATE utility classes25, each
wrapping an individual ATE algorithm, to perform candi-
date scoring, ranking, ﬁltering and export.

4.2. Plugin mode

The plugin mode is recommended when users need to in-
dex new or enrich existing index using extracted terms,
which can, e.g., support faceted search. ATE is per-
formed as a Solr plugin. A SolrRequestHandler 26
is implemented so that term extraction can be triggered by
HTTP request. Users conﬁgure their Solr instance in the
same way as above, then start the instance as a back-end
server. To trigger ATE, users send an HTTP request to the
SolrRequestHandler, passing parameters specifying
the input data and the ATE algorithms. Candidate extrac-
tion is optional if the process is performed with document
indexing. ATE then begins and when ﬁnished, updates doc-
uments in the index with extracted terms.

Table 1: ATE algorithms implemented in JATE 2.0

Name
TTF

ATTF
TTF-IDF

Full name
Term Total
Frequency
Average TTF
TTF and Inverse
Doc Frequency
Residual IDF
C-Value
Chi-Square
Rapid Keyword
Extraction
Weirdness Weirdness
GlossEx
TermEx

RIDF
CValue
χ2
RAKE

Glossry Extraction
Term Extraction

Ref.
(Justeson and Katz, 1995),
FiveFilters.org
-
-

(Church and Gale, 1995)
(Ananiadou, 1994)
(Matsuo and Ishizuka, 2003)
(Rose et al., 2010)

(Ahmad et al., 1999)
(Park et al., 2002)
(Sclano and Velardi, 2007)

terms’.
Both RAKE and χ2 were initially developed for extract-
ing document-speciﬁc keywords. We adapt
them for
ATE from document collections. This is done by replac-
ing document-level frequency with corpus-level frequency,
wherever needed.
Weirdness is a contrastive approach (Drouin, 2003) which
is particularly interesting when trying to identify low-
frequency terms. The method compares normalized fre-
quency of a term candidate in a domain-speciﬁc corpus
with a reference corpus, such as the general-purpose British
National Corpus22. The idea is that candidates appearing
more often in the target corpus are more speciﬁc to that
corpus and therefore, more likely to be real terms. To cope
with out-of-vocabulary candidates, we modify this by tak-
ing the sum of the Weirdness scores of composing words
for a candidate.
Both GlossEx and TermEx extend Weirdness. GlossEx lin-
early combines ‘domain speciﬁcity’, which normalizes the
Weirdness score by the length (number of words) of a term
candidate, with ‘term cohesion’ that measures the degree to
which the composing words tend to occur together as a can-
didate other than appearing individually. This is computed
based on comparing the frequency of a candidate against its
composing words. TermEx, in a very similar form, further
extends GlossEx by linearly combining a third component
that promotes candidates with an even probability distribu-
tion across the documents in the corpus (in an analogy, the
candidate ‘gains consensus’ among the documents).
Essentially both GlossEx and TermEx combine termhood
with unithood, which exploits a reference corpus. For these
we introduce a scalar to balance the contribution of unit-
hood, which tends to dominate the ﬁnal score in case of
largely disproportionate sizes of the domain and reference
corpora (e.g., orders of magnitude difference). The scalar
adjusts the normalized frequency of words in both the tar-
get and reference corpora to be in the same orders of mag-
nitude23.

Figure 3: Term Extraction by HTTP request (Plugin mode)

22http://www.natcorp.ox.ac.uk
23Details can be found in the implementation of the two algo-

25See details in the app package of JATE 2.0
26https://wiki.apache.org/solr/

rithms in JATE 2.0.

SolrRequestHandler

24https://wiki.apache.org/solr/

EmbeddedSolr

2266

5. Experiment
We evaluate JATE 2.0 on two datasets, the GENIA dataset
(Kim et al., 2003), a semantically annotated corpus for bio-
textmining previously used by (Zhang et al., 2008); and
the ACL RD-TEC dataset (Zadeh and Handschuh, 2014),
containing publications in the domain of computational lin-
guistics and a list of manually annotated domain-speciﬁc
terms.
GENIA contains 1,999 Medline abstracts, selected using a
PubMed query for the terms ‘human’, ‘blood cells’, and
‘transcription factors’. The corpus is annotated with var-
ious levels of linguistic and semantic information. We
extract any text annotated as ‘cons’ (concept) to compile
a gold standard list of terms. ACL RD-TEC contains
over 10,900 scientiﬁc publications. In (Zadeh and Hand-
schuh, 2014), the corpus is automatically segmented and
PoS tagged. Candidate terms are extracted by applying
a list of patterns based on PoS sequence. These are then
ranked by several ATE algorithms, and the top set of over
82,000 candidates are manually annotated as valid or in-
valid. We use the valid candidates as gold standard terms.
We notice three different versions of corpus available27:
one only contains the sentences where at least one of the
annotated candidate terms must be present (under ‘anno-
tation’), one contains complete raw text ﬁles in XML for-
mat (under ‘cleansed text’), and one contains plain text ﬁles
from the ACL ARC (under ‘external resource’). We use the
XML version and only extract content from ‘<title>’ and
‘<paragraph>’ elements.
We ran all experiments on a server with 16 CPUs. All algo-
rithms follow the same candidate extraction process, unless
otherwise noted. Detailed conﬁgurations are as follows:

• three different analyzers have been tested, each using
PoS sequence pattern based (PoS based) , noun phrase
chunking based (NP Chunking based), and the N-gram
based term candidate extraction. The pipelines are
slightly different for each analyzer and each dataset.
Detailed conﬁguration can be found in the example
section of the JATE website;

• for PoS-based term candidate extraction, we use: for
GENIA, a list of PoS sequence patterns deﬁned in
(Ananiadou, 1994); for ACL RD-TEC, a list of PoS
sequence patterns deﬁned in (Zadeh and Handschuh,
2014);

• min. character length of 2; max. of 40;
• min. tokens of 1, max. of 5;
• leading and trailing stop words and non-alphanumeric
character removal. (for N-gram based removal of any
non-alphanumeric characters);

• stop words removal;
• min. frequency threshold of 2;
• for χ2, the ‘frequent terms’ are selected as top 10%.
Moreover, only candidates that appear in at least two
sentences are considered;

• for Weirdness, GlossEx and TermEx, the BNC corpus

is used as reference;

27http://atmykitchen.info/datasets/acl_rd_

tec/. Accessed: 10 March 2016

Table 2: Comparision of candidate extraction on GENIA
(run in a single thread)

Method
PoS-based
NP-based
N-gram

Term candidates Recall CPU time (millsec)
10,582/38,850
35,799/44,479
48,945/440,974

68,914
118,753
33,721

10%
23%
16%

Table 3: Comparision of candidate extraction on ACL RD-
TEC (run in a single thread)
Term candidates
524,662/1,569,877
585,012/2,014,916
887,316/7,776,457

Recall CPU time (min)
133.84
74%
367
66%
189.20
41%

Method
PoS-based
NP-based
N-gram

The total number of term candidates extracted for each
dataset under each analyzer setting after/before applying
the minimum frequency threshold with associated overall
recall and CPU time are shown in Tables 2 and 3. The low
recall for GENIA may be due to several reasons. First, a
substantial part of the gold standard terms are ‘irregular’, as
they contain numbers, punctuations and symbols. The pat-
terns in our experiment are mainly based on adjectives and
nouns, and will not match irregular terms. Second, we use
a general purpose PoS tagger which does not perform well
for the biomedical text.
In addition, GENIA consists of
very short abstracts and as a result, many legitimate terms
may be removed due to the frequency threshold and lexical
pruning. However, this can be easily rectiﬁed by relaxing
the pre-ﬁlters.
We then show precision of top K terms ranked by each al-
gorithm, commonly used in ATE. Figure 4 shows results for
the GENIA dataset and Figure 5 shows results for the ACL
RD-TEC dataset. First, all algorithms achieve very high
precision on the GENIA dataset. This is partly because the
gold standard terms are very densely distributed. Our anal-
ysis shows that 49% of words are annotated as part of a
term. Second, TFIDF and CValue appear to be the most
reliable methods as they obtain consistently good results
on both datasets. Third, Weirdness, GlossEx and TermEx
are very sensitive to the choice of reference corpus. For
example, on the ACL RD-TEC dataset, phrases containing
‘treebank’ are very highly ranked. However, many of them
are not valid terms. Finally, RAKE is the worst perform-
ing algorithm on both datasets, possibly because it is very
speciﬁcally tailored to document-level (other than corpus-
level) keyword extraction.
Next we compare the efﬁciency of each algorithm for scor-
ing and ranking term candidates, by measuring the max-
imum memory footprint and CPU time for computation.
We found consistent patterns among different algorithms,
despite what term candidate extractor is used (which only
affects absolute ﬁgures as it changes the number of candi-
dates generated, see Tables 2 and 3). Using the PoS-based
term candidate extractor as example, we show the statistics
in Table 4 and Table 5 for the two datasets. As it is shown,
χ2 is the most memory intensive due to the in-memory stor-
age of co-occurrence. This largely depends on the number
of term candidates and frequent terms. In terms of speed,

2267

Figure 4: Comparison of Top K precisions on GENIA

Figure 5: Comparison of Top K precisions on ACL RD-TEC

χ2, CValue and RAKE are among the slowest. While χ2
spends signiﬁcant time on co-occurrence related computa-
tion, CValue and RAKE spend substantial time on comput-
ing the containment relations among candidates.

6. Conclusion
This paper describes JATE 2.0, a highly modular, adaptable
and scalable ATE library with 10 implemented algorithms,
developed within the Apache Solr framework. It advances
existing ATE tools mainly by enabling a signiﬁcant degree
of customization and adaptation thanks to the ﬂexibility un-
der the Solr framework; and making available a large col-
lection of state-of-the-art algorithms. It is expected that the
tool will bring both academia and industries under a uni-
form development and benchmarking framework that will

Table 4: Running time and memory usage on GENIA

Name
TTF
ATTF
TTF-IDF
RIDF
χ2
CValue
Weirdness
GlossEx
TermEx
RAKE

CPU time (sec.) Max vmem (gb)
19
20
20
20
30
47
22
25
27
26

0.81
0.83
0.83
0.78
1.12
1.41
1.07
1.01
1.06
1.42

encourage collaborative effort in this area of research, to

2268

Table 5: Running time and memory usage on ACL RD-
TEC

Name
TTF
ATTF
TTF-IDF
RIDF
χ2
CValue
Weirdness
GlossEx
TermEx
RAKE

CPU time (min:sec) Max vmem (gb)
5:46
5:56
5:58
5:59
21:47
20:59
7:11
6:40
9:52
22:59

4.2
4.18
4.24
4.38
16.05
5.01
4.89
4.88
6.12
5.45

foster further contributions in terms of novel algorithms,
benchmarkings, support for new languages, new text pro-
cessing capabilities, and so on. Future work will look into
the implementation of additional ATE algorithms, particu-
larly machine learning based methods.

7. Acknowledgement
Part of this research has been sponsored by the EU funded
project WeSenseIt under grant agreement number 308429;
and the SPEEAK-PC collaboration agreement 101947 of
the Innovative UK.

8. Bibliographical References
Ahmad, K., Gillam, L., and Tostevin, L. (1999). University
of surrey participation in trec 8: Weirdness indexing for
logical document extrapolation and retrieval (wilder). In
Proceedings of the 8th Text REtrieval Conference.

Ananiadou, S. (1994). A methodology for automatic term
recognition. In Proceedings of the 15th Conference on
Computational Linguistics, pages 1034–1038, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.

Arora, C., Sabetzadeh, M., Briand, L., and Zimmer, F.
Improving requirements glossary construction
(2014).
via clustering: approach and industrial case studies. In
Proceedings of the 8th ACM/IEEE International Sympo-
sium on Empirical Software Engineering and Measure-
ment, page 18. ACM.

Bowker, L.

(2003). Terminology tools for translators.

BENJAMINS TRANSLATION LIBRARY, 35:49–66.

Brewster, C., Iria, J., Zhang, Z., Ciravegna, F., Guthrie, L.,
and Wilks, Y. (2007). Dynamic iterative ontology learn-
ing. In Proceedings of the 6th International Conference
on Recent Advances in Natural Language Processing,
Borovets, Bulgaria, September.

Church, K. W. and Gale, W. A. (1995). Inverse document
frequency (idf): A measure of deviations from poisson.
In Proceedings of the ACL 3rd Workshop on Very Large
Corpora, pages 121–130, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

Drouin, P.

(2003). Term extraction using non-technical
corpora as a point of leverage. Terminology, 9(1):99–
115.

Frantzi, K. T., Ananiadou, S., and Mima, H. (2000). Auto-
matic recognition of multi-word terms:. the c-value/nc-

value method. Natural Language Processing For Digital
Libraries, 3(2):115–130.

Justeson, J. and Katz, S. M. (1995). Technical terminol-
ogy: some linguistic properties and an algorithm for
identiﬁcation in text. Natural Language Engineering,
1(1):9–27.

Kim, J.-D., Ohta, T., Tateisi, Y., and ichi Tsujii, J. (2003).
GENIA corpus - a semantically annotated corpus for bio-
In ISMB (Supplement of Bioinformatics),
textmining.
pages 180–182.
labs.translated.net.

(2015).
https://developer.yahoo.com/contentanalysis/
accessed 9-October-2015].

Terminology extraction.
[Online;

Matsuo, Y. and Ishizuka, M. (2003). Keyword extraction
from a single document using word co-occurrence sta-
tistical information. International Journal on Artiﬁcial
Intelligence Tools, 13(1):157–169.

Maynard, D., Saggion, H., Yankova, M., Bontcheva, K.,
and Peters, W. (2007). Natural language technology for
information integration in business intelligence. In Busi-
ness Information Systems, pages 366–380. Springer.
Maynard, D., Li, Y., and Peters, W. (2008). Nlp techniques
for term extraction and ontology population. In Proceed-
ings of the 2008 Conference on Ontology Learning and
Population: Bridging the Gap Between Text and Knowl-
edge, pages 107–127, Amsterdam, The Netherlands, The
Netherlands. IOS Press.

Park, Y., Byrd, R. J., and Boguraev, B. K. (2002). Auto-
matic glossary extraction: Beyond terminology identiﬁ-
cation. In Proceedings of the 19th International Confer-
ence on Computational Linguistics, pages 1–7, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.

Rose, S., Engel, D., Cramer, N., and Cowley, W., (2010).
Automatic keyword extraction from individual docu-
ments. John Wiley and Sons.

Sclano, F. and Velardi, P. (2007). Termextractor: a web
application to learn the shared terminology of emer-
gent web communities. In Proceedings of the 3rd Inter-
national Conference on Interoperability for Enterprise
Software and Applications.

Spasi´c, I., Greenwood, M., Preece, A., Francis, N., and El-
wyn, G. (2013). Flexiterm: a ﬂexible term recognition
method. Journal of Biomedical Semantics, 4(27).

Yahoo!

(2015). Yahoo!

content analysis api.
[Online;

https://developer.yahoo.com/contentanalysis/
accessed 9-October-2015].
Zadeh, B. and Handschuh, S.

(2014). The acl rd-tec:
A dataset for benchmarking terminology extraction and
classiﬁcation in computational linguistics. In Proceed-
ings of the 4th International Workshop on Computational
Terminology (Computerm), pages 52–63, Dublin, Ire-
land, August. Association for Computational Linguistics
and Dublin City University.

Zhang, Z., Iria, J., Brewster, C., and Ciravegna, F. (2008).
A comparative evaluation of term recognition algo-
In Proceedings of The 6th international con-
rithms.
ference on Language Resources and Evaluation, Mar-
rakech, Morocco, May.

2269

JATE 2.0: Java Automatic Term Extraction with Apache Solr

Ziqi Zhang, Jie Gao, Fabio Ciravegna
Regent Court, 211 Portobello, Shefﬁeld, UK, S1 4DP
ziqi.zhang@shefﬁeld.ac.uk, j.gao@shefﬁeld.ac.uk, f.ciravegna@shefﬁeld.ac.uk

Abstract
Automatic Term Extraction (ATE) or Recognition (ATR) is a fundamental processing step preceding many complex knowledge
engineering tasks. However, few methods have been implemented as public tools and in particular, available as open-source freeware.
Further, little effort is made to develop an adaptable and scalable framework that enables customization, development, and comparison
of algorithms under a uniform environment. This paper introduces JATE 2.0, a complete remake of the free Java Automatic Term
Extraction Toolkit (Zhang et al., 2008) delivering new features including: (1) highly modular, adaptable and scalable ATE thanks to
integration with Apache Solr, the open source free-text indexing and search platform; (2) an extended collection of state-of-the-art
algorithms. We carry out experiments on two well-known benchmarking datasets and compare the algorithms along the dimensions of
effectiveness (precision) and efﬁciency (speed and memory consumption). To the best of our knowledge, this is by far the only free ATE
library offering a ﬂexible architecture and the most comprehensive collection of algorithms.

Keywords: term extraction, term recognition, NLP, text mining, Solr, search, indexing

1.

Introduction

Automatic Term Extraction (or Recognition) is an impor-
tant Natural Language Processing (NLP) task that deals
with the extraction of terminologies from domain-speciﬁc
textual corpora. ATE is widely used by both industries
and researchers in many complex tasks, such as Informa-
tion Retrieval (IR), machine translation, ontology engineer-
ing and text summarization (Bowker, 2003; Brewster et
al., 2007; Maynard et al., 2007). Over the years, there
has been constant effort on researching new algorithms,
adapting to different domains and/or languages, and creat-
ing benchmarking datasets (Ananiadou, 1994; Church and
Gale, 1995; Ahmad et al., 1999; Frantzi et al., 2000; Park
et al., 2002; Matsuo and Ishizuka, 2003; Sclano and Ve-
lardi, 2007; Rose et al., 2010; Spasi´c et al., 2013; Zadeh
and Handschuh, 2014).
Given the fundamental role of ATE in many tasks, a real
beneﬁt to the community would be releasing the tools to
facilitate the development of downstream applications, also
encouraging code-reuse, comparative studies, and fostering
further research. Unfortunately, effort in this direction has
been extremely limited. First, only very few tools have been
published with readily available source code (Spasi´c et al.,
2013) and some are proprietary (labs.translated.net, 2015;
Yahoo!, 2015), or no longer maintained (Sclano and Ve-
lardi, 2007)1. Second, different tools have been developed
under different scenarios and evaluated in different domains
using proprietary language resources, making it difﬁcult for
comparison. Third, it is unclear whether and how well these
tools can adapt to different domain tasks and scale up to
large data.
To the best of our knowledge, the only effort towards ad-
dressing these issues is the Java Automatic Term Extrac-
tion Toolkit (JATE)2, where we originally implemented
ﬁve state-of-the-art algorithms under a uniform framework
(Zhang et al., 2008). This work makes one step further

1Original link: http://lcl2.di.uniroma1.it/
2https://code.google.com/p/jatetoolkit.

Google Code has been shut down since Jan 2016.

by completely re-designing and re-implementing JATE to
fulﬁll three goals: adaptability, scalability, and extended
collections of algorithms. The new library, named JATE
2.03, is built on the Apache Solr4 free-text indexing and
search platform and can be either used as a separate mod-
ule, or a Solr plugin during document processing to en-
rich the indexed documents with terms. JATE 2.0 deliv-
ers domain-speciﬁc adaptability and scalability by seam-
lessly integrating with the Solr text analysis capability in
the form of ‘analyzers’5, allowing users to directly exploit
the highly modular and scalable Solr text analysis to facili-
tate term extraction for large data from various content for-
mats. Last but not least, JATE 2.0 expands its collection
of state-of-the-art algorithms6 to double that in the origi-
nal JATE release. We also evaluate these algorithms under
uniform settings on two publicly available benchmarking
datasets. Released as open source software, we believe that
JATE 2.0 offers signiﬁcant value to both researchers and
practitioners.
The remainder of this paper is structured as follows. Sec-
tion 2. discusses related work. Section 3. describes JATE
2.0 in details. Section 5. describes experiments and dis-
cusses results. Section 6. concludes this paper.

2. Related Work
We describe related work from three persectives: ATE
methods in general, ATE tools and software, and text min-
ing plugins based on Solr.
ATE methods typically start with linguistic processors
as the ﬁrst stage to extract candidate terms; followed by

3https://github.com/ziqizhang/jate
4http://lucene.apache.org/solr/
5https://cwiki.apache.org/confluence/

display/solr/Analyzers

6By this we mean the mathematical formulation of term candi-
date scores. We do not claim identical replication of state-of-the-
art methods, as they often differ in pre- and post-processing, such
as term candidate generation, term variant handling and the use of
dictionary.

2262

statistics-based term ranking algorithms to perform can-
didate ﬁltering. Linguistic processors are domain spe-
ciﬁc, and designed to capture term formation and collo-
cation patterns. They often take two forms: ‘closed ﬁl-
ters’ (Arora et al., 2014) focus on precision and are usu-
ally restricted to nouns or noun sequences. ‘Open ﬁlters’
(Frantzi et al., 2000) are more permissive and often al-
low adjectives, adverbs, etc. For both, widely used tech-
niques include Part-of-Speech (PoS) tag sequence match-
ing, N-gram extraction, Noun Phrase (NP) Chunking, and
dictionary lookup. The selection of linguistic processors is
often domain-speciﬁc and important for the trade-off be-
tween precision and recall. The ranking algorithms are of-
ten based on: ‘unithood’ indicating the collocation strength
of units that comprise a single term (Matsuo and Ishizuka,
2003); and ‘termhood’ indicating the association strength
of a term to domain concepts (Ahmad et al., 1999). Most
methods combine both termhood and unithood measures
(Ananiadou, 1994; Frantzi et al., 2000; Park et al., 2002;
Sclano and Velardi, 2007; Spasi´c et al., 2013). Two unique
features of JATE 2.0 are (1) customization of linguistic pro-
cessors of the two forms hence making it adaptable to many
domains and languages, and (2) a wide selection of imple-
mented ranking algorithms, which are not available in any
other tools.
Existing ATE tools and software are extremely limited
for several reasons.
First, many are proprietary soft-
ware (labs.translated.net, 2015), quota-limited (e.g., Open-
Calais7), or restricted to academic usage only (e.g., Ter-
Mine8). Second, all tools, including the original JATE
(Zhang et al., 2008), are built in a monolithic architec-
ture which allows very limited customization and scala-
bility (Spasi´c et al., 2013). Third, most offer very limited
(typically a single choice) conﬁgurability, e.g., in terms of
term candidate extraction and/or ranking algorithms (e.g.,
TermRaider (Maynard et al., 2008), TOPIA9, and Flex-
iTerm (Spasi´c et al., 2013). JATE 2.0 is a free, open-source
library. It is highly modular, allowing customization and
conﬁguration of many components. Meanwhile, to our best
knowledge, no available ATE tools support automatic in-
dexing.
Solr plugins for text mining Solr is an open source en-
terprise search platform written in Java. It is highly mod-
ular, customizable, and scalable, hence widely used as an
industry standard. A core component of Solr is its power-
ful, extensible text processing library covering a wide range
of functionality and languages. Many text mining and NLP
tools have integrated with Solr to beneﬁt from these fea-
tures as well as contributing additional support for doc-
ument indexing (e.g., OpenNLP plugins10, SolrTextTag-
ger11, and UIMA12). However, no plugin is available for

7http://new.opencalais.com/
8http://www.nactem.ac.uk/software/

9https://pypi.python.org/pypi/topia.

termine/

termextract

10https://wiki.apache.org/solr/OpenNLP
11https://github.com/OpenSextant/

SolrTextTagger

12https://uima.apache.org

ATE.

3.

JATE 2.0 Architecture

Figure 1 shows the general architecture of JATE 2.0 and
its workﬂow, which consists of four phases: (1) data pre-
processing; (2) term candidate extraction and indexing; (3)
candidate scoring and ﬁltering and (4) ﬁnal term indexing
and export.
Data pre-processing (Section 3.1.) parses input docu-
ments to raw text content and performs text normaliza-
tion in order to reduce ‘noise’ in irregular data. The pre-
processed text content then passes through the candidate
extraction (Section 3.2.) component that extracts and nor-
malizes term candidates from each document. Candidate
extraction and data pre-processing are embedded as part
of the Solr document indexing process, largely realized by
Solr’s ‘analyzers’. An analyzer is composed of a series
of processors (‘tokenizers’ and ‘ﬁlters’) to form a pipeline,
or an analysis ‘chain’ that examines the text content from
each document, generates a token stream and records sta-
tistical information. Solr already implements a very large
text processing library for this purpose. JATE 2.0 further
extends these. Depending on individual needs, users may
assemble customized analyzers for term candidate genera-
tion. Deﬁning analyzers is achieved by conﬁguration in the
Solr schema, such as that shown in Figure 2.
Next, the candidates are processed by the subsequent ﬁlter-
ing component (Section 3.3.), where different ATE algo-
rithms can be conﬁgured to score and rank the candidates,
and making the ﬁnal selection. The implementation of the
algorithms are largely parallelized. The resulting terms can
either be exported for further validation, or written back to
the index to annotate documents.
JATE 2.0 can be run in two modes: (1) as a separate mod-
ule to extract terms from a corpus using an embedded Solr
instance (Section 4.1.), or (2) as a Solr plugin that performs
ATE as part of the document indexing process to anno-
tate documents with domain-speciﬁc terminologies (Sec-
tion 4.2.). Both modes undergo the same workﬂow de-
scribed above.
JATE 2.0 offers adaptability and scalability thanks to its
seamless integration with Solr and parallelized implemen-
tation. Access to Solr’s very large collection of multi-
lingual text processing libraries that can be conﬁgured in
a plug-and-play way enables JATE 2.0 to be adapted to dif-
ferent languages, document formats, and domains. The rich
collection of algorithms also give users options to better
tailor the tool to speciﬁc use cases. Parallelization enables
JATE 2.0 to scale up to large datasets. Parallelization for
term candidate generation can be achieved via Solr’s dis-
tributed indexing capability under SolrCloud13. Users sim-
ply follow Solr’s guide on conﬁguring distributed indexing.
Parallelization for scoring and ranking algorithms comes
as standard in JATE 2.0 and is also conﬁgurable. Detailed
guidelines on deﬁning analyzers for ATE and conﬁgura-
tions can be found on the JATE 2.0 homepage.

13https://cwiki.apache.org/confluence/

display/solr/SolrCloud

2263

tokenization, such as line 3 of schema.xml shown in Figure
2.

3.2. Candidate Extraction
Term candidate extraction is realized as part of the Solr in-
dexing process. A dedicated ‘ﬁeld’19 is deﬁned within the
Solr indexing schema to hold candidates for each document
(candidate ﬁeld). The ﬁeld is bound to a ‘ﬁeldType’, which
uses a customized analyzer to break content into term can-
didates and normalize them to conﬂate the variants that are
dispersed throughout the corpus. The Solr indexing pro-
cess also generates basic statistical information about can-
didates, such as a candidate’s frequency within each docu-
ment, and the number of documents it appears in20.
The analyzer is highly customizable and adaptable. All ex-
isting Solr text analysis libraries can be used, such as tok-
enization, case folding, stop words removal, token N-gram
extraction, stemming, etc. It is also easy to implement new
components to replace or complement existing ones in a
plug-and-play way. Currently, Solr 5.3 supports nearly 20
implementations of tokenization and over 100 text normal-
ization and ﬁltering methods21, covering a wide range of
languages and use cases. Thus by selecting different com-
ponents and assembling them in different orders, one can
create different analyzers to achieve different extraction and
normalization results.
To support ATE, JATE 2.0 extends Solr’s text analysis li-
braries by supporting three types of linguistic ﬁlters for
term candidate extraction. These include: (1) a PoS pattern
based chunker that extracts candidates based on user spec-
iﬁed patterns (e.g., line 9 of schema.xml in Figure 2); (2)
a token N-gram extractor that extends the built-in one; (3)
a noun phrase chunker. All of these are implemented with
the capability to normalize noisy terms, such as removing
leading and trailing stop words, and non-alphanumeric to-
kens.
In addition, JATE 2.0 also implements an English
lemmatizer, which is a recommended replacement of stem-
mers that can sometimes be too aggressive for ATE. These
offer great ﬂexibility enabling almost any customized term
candidate extraction. Figure 2 shows a standard conﬁgu-
ration of an text analyzer for PoS pattern based candidate
extraction for English.

3.3. Filtering
Extracted term candidates are further processed to make a
ﬁnal decision to separate terms from non-terms.

3.3.1. Pre-ﬁltering
It is often a common practice to apply a ﬁltering process to
term candidates to reduce both noise (e.g., due to irregular
textual data or erroneous PoS tagging) and computation be-
fore the subsequent step of term scoring and ranking. The

19https://cwiki.apache.org/confluence/

display/solr/Solr+Glossary

20For practical reasons, we rely on a separate ﬁeld that indexes
the same content as token N-grams and stored term vectors which
provides the lookup of statistics about term candidates. Details on
this can be found on the JATE website and Solr documentation for
Inverted Index.

21See Solr API documentation.

Figure 1: General Architecture of JATE 2.0

support multi-lingual

3.1. Data pre-processing
With the standard JATE 2.0 conﬁguration, data pre-
processing begins with Solr Content Extraction Library
(also known as SolrCell14) to extract textual content from
It integrates the popular Apache Tika framework15
ﬁles.
that supports detecting and extracting metadata and text
from a large range of ﬁle formats such as plain text, HTML,
PDF, and Microsoft Ofﬁce. To use this, users simply con-
ﬁgure an instance of ExtractingRequestHandler16
for their data in Solr.
To
also use Solr’s
of
speciﬁc analyzers
UpdateRequestProcessor17.
Next, a recommended practice for pre-processing irregu-
lar textual data (commonly found in corporate datasets)
is applying the Solr char ﬁlter component18
to per-
form character-level
For example,
HTMLStripCharFilterFactory detects HTML en-
tities (e.g., ‘&#65;’, ‘&nbsp;’) with corresponding charac-
ters. This can reduce errors in downstream processors (e.g.,
PoS tagger) in the analyzer. This can be conﬁgured as part
of the analyzer in the Solr schema, usually placed before

users may
capability for detecting languages
to language-
indexing using the langid

in documents and map text

text normalization.

documents,

texts

for

14https://cwiki.apache.org/confluence/

display/solr/Uploading+Data+with+Solr+Cell+
using+Apache+Tika

15https://tika.apache.org
16https://wiki.apache.org/solr/

ExtractingRequestHandler

17https://cwiki.apache.org/confluence/
display/solr/Detecting+Languages+During+
Indexing

18https://cwiki.apache.org/confluence/

display/solr/CharFilterFactories

2264

Figure 2: Example of PoS pattern based candidate extraction

following strategies have been implemented in JATE 2.0
and can be conﬁgured according to user needs to balance
precision and recall.
Minimal and maximal character length restricts candi-
dates to a ﬁxed range of character length. Minimal and
maximal token number requires a valid candidate to con-
tain certain number of tokens (e.g., to limit multi-word ex-
pressions). Minimal stop words removal removes lead-
ing and/or trailing stop words in a multi-word expression
(e.g.,‘the cat’ becomes ‘cat’, but ‘Tower of London’ re-
mains intact). All these are implemented as part of the
three term candidate extraction methods described before
and can be conﬁgured in the analyzer.
Frequency threshold allows candidates to be ﬁltered by a
minimum total frequency in the corpus. This is often used
in typical ATE methods, and is conﬁgurable via individual
scoring and ranking algorithms that follow.

3.3.2. Scoring and ranking
In this step, term candidates that pass the pre-ﬁlter are
scored and ranked. Their basic statistical information is
gathered from the Solr index, to create complex features
required by different algorithms.
JATE 2.0 has implemented 10 algorithms for scoring can-
didates, listed in Table 1. TTF is the total frequency of a
candidate in the corpus. It’s usage in ATE was ﬁrstly docu-
mented in (Justeson and Katz, 1995). ATTF divides TTF by
the number of documents a candidate appears in. TTF-IDF
adapts the classic document-speciﬁc TF-IDF used in IR to
work at corpus level, by replacing TF with TTF.
RIDF was initially proposed by (Church and Gale, 1995)
as an enhancement to IDF to identify keywords in a doc-
ument collection. It measures the deviation of the actual
IDF score of a word from its ‘expected’ IDF score, which
is predicted based on a Poisson distribution. The hypothe-
sis is based on the fact that Poisson model ﬁts poorly with
such keywords. Thus a prediction of IDF based on Poisson
can deviate from its actual IDF observed based on a cor-

pus. Empirically, it is shown that all words have real IDF
scores that deviate from the expected value under a Poisson
distribution. However, keywords tend to have larger devi-
ations than non-keywords. We adapt it to work with term
candidates that can be either single words or multi-word
expressions.
CValue observes that real terms in technical domains are
often long, multi-word expressions and usually not nested
in other terms (i.e., as part of the longer terms). Frequency-
based methods are not effective for such terms as (1) nested
term candidates will have at least the same and often higher
frequency, and (2) the fact that a longer string appears n
times is a lot more important than that of a shorter string
appearing n times. Thus CValue computes a score that is
based on the frequency of a candidate and its length, then
adjusted by the frequency of longer candidates that contain
it.
Similarly, RAKE is designed to favour longer multi-word
expressions. It ﬁrstly computes a score for individual words
based on two components: one that favours words oc-
curring often and in longer term candidates, and one that
favours words occurring frequently regardless of the words
which they co-occur with. Then it adds up the scores of
composing words for a candidate.
χ2 promotes term candidates that co-occur very often with
‘frequent’ candidates in the corpus. First, candidates are
ranked by frequency in the corpus and a subset (typically
top n%) is selected - to be called ‘frequent terms’. Next,
candidates are scored based on the degree to which their
co-occurrence with these frequent terms are biased. These
biases can be due to semantic, lexical, or other relations of
two terms. Thus, a candidate showing strong co-occurrence
biases with frequent terms may have an important mean-
ing. To evaluate the statistical signiﬁcance of the biases,
χ2 is used. For each candidate, co-occurrence frequency
with the frequent terms is regarded as a sample value. The
null hypothesis that we expect to reject is that ‘occurrence
of a candidate is independent from occurrence of frequent

2265

3.3.3. Threshold cutoff
Next, a cutoff decision is to be made to separate real terms
from non-terms based on their scores. Three options are
supported: (1) hard threshold, where term candidates with
scores no less than the threshold are chosen; (2) top K,
where top ranked K candidates are chosen; (3) and top K%,
where the top ranked K percentage of candidates are cho-
sen.

4. Using JATE 2.0 in Two Modes

4.1. Embedded mode

The embedded mode is recommended when users need a
list of domain-speciﬁc terms from a corpus to be used in
subsequent knowledge engineering tasks. Users conﬁgure
a Solr instance and in particular, a text analysis chain that
deﬁnes how term candidates are extracted and normalized.
The Solr instance is instantiated as an embedded24 module
that interacts with other components. Users then explicitly
start an indexing process on a corpus to trigger term candi-
date extraction, then use speciﬁc ATE utility classes25, each
wrapping an individual ATE algorithm, to perform candi-
date scoring, ranking, ﬁltering and export.

4.2. Plugin mode

The plugin mode is recommended when users need to in-
dex new or enrich existing index using extracted terms,
which can, e.g., support faceted search. ATE is per-
formed as a Solr plugin. A SolrRequestHandler 26
is implemented so that term extraction can be triggered by
HTTP request. Users conﬁgure their Solr instance in the
same way as above, then start the instance as a back-end
server. To trigger ATE, users send an HTTP request to the
SolrRequestHandler, passing parameters specifying
the input data and the ATE algorithms. Candidate extrac-
tion is optional if the process is performed with document
indexing. ATE then begins and when ﬁnished, updates doc-
uments in the index with extracted terms.

Table 1: ATE algorithms implemented in JATE 2.0

Name
TTF

ATTF
TTF-IDF

Full name
Term Total
Frequency
Average TTF
TTF and Inverse
Doc Frequency
Residual IDF
C-Value
Chi-Square
Rapid Keyword
Extraction
Weirdness Weirdness
GlossEx
TermEx

RIDF
CValue
χ2
RAKE

Glossry Extraction
Term Extraction

Ref.
(Justeson and Katz, 1995),
FiveFilters.org
-
-

(Church and Gale, 1995)
(Ananiadou, 1994)
(Matsuo and Ishizuka, 2003)
(Rose et al., 2010)

(Ahmad et al., 1999)
(Park et al., 2002)
(Sclano and Velardi, 2007)

terms’.
Both RAKE and χ2 were initially developed for extract-
ing document-speciﬁc keywords. We adapt
them for
ATE from document collections. This is done by replac-
ing document-level frequency with corpus-level frequency,
wherever needed.
Weirdness is a contrastive approach (Drouin, 2003) which
is particularly interesting when trying to identify low-
frequency terms. The method compares normalized fre-
quency of a term candidate in a domain-speciﬁc corpus
with a reference corpus, such as the general-purpose British
National Corpus22. The idea is that candidates appearing
more often in the target corpus are more speciﬁc to that
corpus and therefore, more likely to be real terms. To cope
with out-of-vocabulary candidates, we modify this by tak-
ing the sum of the Weirdness scores of composing words
for a candidate.
Both GlossEx and TermEx extend Weirdness. GlossEx lin-
early combines ‘domain speciﬁcity’, which normalizes the
Weirdness score by the length (number of words) of a term
candidate, with ‘term cohesion’ that measures the degree to
which the composing words tend to occur together as a can-
didate other than appearing individually. This is computed
based on comparing the frequency of a candidate against its
composing words. TermEx, in a very similar form, further
extends GlossEx by linearly combining a third component
that promotes candidates with an even probability distribu-
tion across the documents in the corpus (in an analogy, the
candidate ‘gains consensus’ among the documents).
Essentially both GlossEx and TermEx combine termhood
with unithood, which exploits a reference corpus. For these
we introduce a scalar to balance the contribution of unit-
hood, which tends to dominate the ﬁnal score in case of
largely disproportionate sizes of the domain and reference
corpora (e.g., orders of magnitude difference). The scalar
adjusts the normalized frequency of words in both the tar-
get and reference corpora to be in the same orders of mag-
nitude23.

Figure 3: Term Extraction by HTTP request (Plugin mode)

22http://www.natcorp.ox.ac.uk
23Details can be found in the implementation of the two algo-

25See details in the app package of JATE 2.0
26https://wiki.apache.org/solr/

rithms in JATE 2.0.

SolrRequestHandler

24https://wiki.apache.org/solr/

EmbeddedSolr

2266

5. Experiment
We evaluate JATE 2.0 on two datasets, the GENIA dataset
(Kim et al., 2003), a semantically annotated corpus for bio-
textmining previously used by (Zhang et al., 2008); and
the ACL RD-TEC dataset (Zadeh and Handschuh, 2014),
containing publications in the domain of computational lin-
guistics and a list of manually annotated domain-speciﬁc
terms.
GENIA contains 1,999 Medline abstracts, selected using a
PubMed query for the terms ‘human’, ‘blood cells’, and
‘transcription factors’. The corpus is annotated with var-
ious levels of linguistic and semantic information. We
extract any text annotated as ‘cons’ (concept) to compile
a gold standard list of terms. ACL RD-TEC contains
over 10,900 scientiﬁc publications. In (Zadeh and Hand-
schuh, 2014), the corpus is automatically segmented and
PoS tagged. Candidate terms are extracted by applying
a list of patterns based on PoS sequence. These are then
ranked by several ATE algorithms, and the top set of over
82,000 candidates are manually annotated as valid or in-
valid. We use the valid candidates as gold standard terms.
We notice three different versions of corpus available27:
one only contains the sentences where at least one of the
annotated candidate terms must be present (under ‘anno-
tation’), one contains complete raw text ﬁles in XML for-
mat (under ‘cleansed text’), and one contains plain text ﬁles
from the ACL ARC (under ‘external resource’). We use the
XML version and only extract content from ‘<title>’ and
‘<paragraph>’ elements.
We ran all experiments on a server with 16 CPUs. All algo-
rithms follow the same candidate extraction process, unless
otherwise noted. Detailed conﬁgurations are as follows:

• three different analyzers have been tested, each using
PoS sequence pattern based (PoS based) , noun phrase
chunking based (NP Chunking based), and the N-gram
based term candidate extraction. The pipelines are
slightly different for each analyzer and each dataset.
Detailed conﬁguration can be found in the example
section of the JATE website;

• for PoS-based term candidate extraction, we use: for
GENIA, a list of PoS sequence patterns deﬁned in
(Ananiadou, 1994); for ACL RD-TEC, a list of PoS
sequence patterns deﬁned in (Zadeh and Handschuh,
2014);

• min. character length of 2; max. of 40;
• min. tokens of 1, max. of 5;
• leading and trailing stop words and non-alphanumeric
character removal. (for N-gram based removal of any
non-alphanumeric characters);

• stop words removal;
• min. frequency threshold of 2;
• for χ2, the ‘frequent terms’ are selected as top 10%.
Moreover, only candidates that appear in at least two
sentences are considered;

• for Weirdness, GlossEx and TermEx, the BNC corpus

is used as reference;

27http://atmykitchen.info/datasets/acl_rd_

tec/. Accessed: 10 March 2016

Table 2: Comparision of candidate extraction on GENIA
(run in a single thread)

Method
PoS-based
NP-based
N-gram

Term candidates Recall CPU time (millsec)
10,582/38,850
35,799/44,479
48,945/440,974

68,914
118,753
33,721

10%
23%
16%

Table 3: Comparision of candidate extraction on ACL RD-
TEC (run in a single thread)
Term candidates
524,662/1,569,877
585,012/2,014,916
887,316/7,776,457

Recall CPU time (min)
133.84
74%
367
66%
189.20
41%

Method
PoS-based
NP-based
N-gram

The total number of term candidates extracted for each
dataset under each analyzer setting after/before applying
the minimum frequency threshold with associated overall
recall and CPU time are shown in Tables 2 and 3. The low
recall for GENIA may be due to several reasons. First, a
substantial part of the gold standard terms are ‘irregular’, as
they contain numbers, punctuations and symbols. The pat-
terns in our experiment are mainly based on adjectives and
nouns, and will not match irregular terms. Second, we use
a general purpose PoS tagger which does not perform well
for the biomedical text.
In addition, GENIA consists of
very short abstracts and as a result, many legitimate terms
may be removed due to the frequency threshold and lexical
pruning. However, this can be easily rectiﬁed by relaxing
the pre-ﬁlters.
We then show precision of top K terms ranked by each al-
gorithm, commonly used in ATE. Figure 4 shows results for
the GENIA dataset and Figure 5 shows results for the ACL
RD-TEC dataset. First, all algorithms achieve very high
precision on the GENIA dataset. This is partly because the
gold standard terms are very densely distributed. Our anal-
ysis shows that 49% of words are annotated as part of a
term. Second, TFIDF and CValue appear to be the most
reliable methods as they obtain consistently good results
on both datasets. Third, Weirdness, GlossEx and TermEx
are very sensitive to the choice of reference corpus. For
example, on the ACL RD-TEC dataset, phrases containing
‘treebank’ are very highly ranked. However, many of them
are not valid terms. Finally, RAKE is the worst perform-
ing algorithm on both datasets, possibly because it is very
speciﬁcally tailored to document-level (other than corpus-
level) keyword extraction.
Next we compare the efﬁciency of each algorithm for scor-
ing and ranking term candidates, by measuring the max-
imum memory footprint and CPU time for computation.
We found consistent patterns among different algorithms,
despite what term candidate extractor is used (which only
affects absolute ﬁgures as it changes the number of candi-
dates generated, see Tables 2 and 3). Using the PoS-based
term candidate extractor as example, we show the statistics
in Table 4 and Table 5 for the two datasets. As it is shown,
χ2 is the most memory intensive due to the in-memory stor-
age of co-occurrence. This largely depends on the number
of term candidates and frequent terms. In terms of speed,

2267

Figure 4: Comparison of Top K precisions on GENIA

Figure 5: Comparison of Top K precisions on ACL RD-TEC

χ2, CValue and RAKE are among the slowest. While χ2
spends signiﬁcant time on co-occurrence related computa-
tion, CValue and RAKE spend substantial time on comput-
ing the containment relations among candidates.

6. Conclusion
This paper describes JATE 2.0, a highly modular, adaptable
and scalable ATE library with 10 implemented algorithms,
developed within the Apache Solr framework. It advances
existing ATE tools mainly by enabling a signiﬁcant degree
of customization and adaptation thanks to the ﬂexibility un-
der the Solr framework; and making available a large col-
lection of state-of-the-art algorithms. It is expected that the
tool will bring both academia and industries under a uni-
form development and benchmarking framework that will

Table 4: Running time and memory usage on GENIA

Name
TTF
ATTF
TTF-IDF
RIDF
χ2
CValue
Weirdness
GlossEx
TermEx
RAKE

CPU time (sec.) Max vmem (gb)
19
20
20
20
30
47
22
25
27
26

0.81
0.83
0.83
0.78
1.12
1.41
1.07
1.01
1.06
1.42

encourage collaborative effort in this area of research, to

2268

Table 5: Running time and memory usage on ACL RD-
TEC

Name
TTF
ATTF
TTF-IDF
RIDF
χ2
CValue
Weirdness
GlossEx
TermEx
RAKE

CPU time (min:sec) Max vmem (gb)
5:46
5:56
5:58
5:59
21:47
20:59
7:11
6:40
9:52
22:59

4.2
4.18
4.24
4.38
16.05
5.01
4.89
4.88
6.12
5.45

foster further contributions in terms of novel algorithms,
benchmarkings, support for new languages, new text pro-
cessing capabilities, and so on. Future work will look into
the implementation of additional ATE algorithms, particu-
larly machine learning based methods.

7. Acknowledgement
Part of this research has been sponsored by the EU funded
project WeSenseIt under grant agreement number 308429;
and the SPEEAK-PC collaboration agreement 101947 of
the Innovative UK.

8. Bibliographical References
Ahmad, K., Gillam, L., and Tostevin, L. (1999). University
of surrey participation in trec 8: Weirdness indexing for
logical document extrapolation and retrieval (wilder). In
Proceedings of the 8th Text REtrieval Conference.

Ananiadou, S. (1994). A methodology for automatic term
recognition. In Proceedings of the 15th Conference on
Computational Linguistics, pages 1034–1038, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.

Arora, C., Sabetzadeh, M., Briand, L., and Zimmer, F.
Improving requirements glossary construction
(2014).
via clustering: approach and industrial case studies. In
Proceedings of the 8th ACM/IEEE International Sympo-
sium on Empirical Software Engineering and Measure-
ment, page 18. ACM.

Bowker, L.

(2003). Terminology tools for translators.

BENJAMINS TRANSLATION LIBRARY, 35:49–66.

Brewster, C., Iria, J., Zhang, Z., Ciravegna, F., Guthrie, L.,
and Wilks, Y. (2007). Dynamic iterative ontology learn-
ing. In Proceedings of the 6th International Conference
on Recent Advances in Natural Language Processing,
Borovets, Bulgaria, September.

Church, K. W. and Gale, W. A. (1995). Inverse document
frequency (idf): A measure of deviations from poisson.
In Proceedings of the ACL 3rd Workshop on Very Large
Corpora, pages 121–130, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

Drouin, P.

(2003). Term extraction using non-technical
corpora as a point of leverage. Terminology, 9(1):99–
115.

Frantzi, K. T., Ananiadou, S., and Mima, H. (2000). Auto-
matic recognition of multi-word terms:. the c-value/nc-

value method. Natural Language Processing For Digital
Libraries, 3(2):115–130.

Justeson, J. and Katz, S. M. (1995). Technical terminol-
ogy: some linguistic properties and an algorithm for
identiﬁcation in text. Natural Language Engineering,
1(1):9–27.

Kim, J.-D., Ohta, T., Tateisi, Y., and ichi Tsujii, J. (2003).
GENIA corpus - a semantically annotated corpus for bio-
In ISMB (Supplement of Bioinformatics),
textmining.
pages 180–182.
labs.translated.net.

(2015).
https://developer.yahoo.com/contentanalysis/
accessed 9-October-2015].

Terminology extraction.
[Online;

Matsuo, Y. and Ishizuka, M. (2003). Keyword extraction
from a single document using word co-occurrence sta-
tistical information. International Journal on Artiﬁcial
Intelligence Tools, 13(1):157–169.

Maynard, D., Saggion, H., Yankova, M., Bontcheva, K.,
and Peters, W. (2007). Natural language technology for
information integration in business intelligence. In Busi-
ness Information Systems, pages 366–380. Springer.
Maynard, D., Li, Y., and Peters, W. (2008). Nlp techniques
for term extraction and ontology population. In Proceed-
ings of the 2008 Conference on Ontology Learning and
Population: Bridging the Gap Between Text and Knowl-
edge, pages 107–127, Amsterdam, The Netherlands, The
Netherlands. IOS Press.

Park, Y., Byrd, R. J., and Boguraev, B. K. (2002). Auto-
matic glossary extraction: Beyond terminology identiﬁ-
cation. In Proceedings of the 19th International Confer-
ence on Computational Linguistics, pages 1–7, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.

Rose, S., Engel, D., Cramer, N., and Cowley, W., (2010).
Automatic keyword extraction from individual docu-
ments. John Wiley and Sons.

Sclano, F. and Velardi, P. (2007). Termextractor: a web
application to learn the shared terminology of emer-
gent web communities. In Proceedings of the 3rd Inter-
national Conference on Interoperability for Enterprise
Software and Applications.

Spasi´c, I., Greenwood, M., Preece, A., Francis, N., and El-
wyn, G. (2013). Flexiterm: a ﬂexible term recognition
method. Journal of Biomedical Semantics, 4(27).

Yahoo!

(2015). Yahoo!

content analysis api.
[Online;

https://developer.yahoo.com/contentanalysis/
accessed 9-October-2015].
Zadeh, B. and Handschuh, S.

(2014). The acl rd-tec:
A dataset for benchmarking terminology extraction and
classiﬁcation in computational linguistics. In Proceed-
ings of the 4th International Workshop on Computational
Terminology (Computerm), pages 52–63, Dublin, Ire-
land, August. Association for Computational Linguistics
and Dublin City University.

Zhang, Z., Iria, J., Brewster, C., and Ciravegna, F. (2008).
A comparative evaluation of term recognition algo-
In Proceedings of The 6th international con-
rithms.
ference on Language Resources and Evaluation, Mar-
rakech, Morocco, May.

2269

JATE 2.0: Java Automatic Term Extraction with Apache Solr

Ziqi Zhang, Jie Gao, Fabio Ciravegna
Regent Court, 211 Portobello, Shefﬁeld, UK, S1 4DP
ziqi.zhang@shefﬁeld.ac.uk, j.gao@shefﬁeld.ac.uk, f.ciravegna@shefﬁeld.ac.uk

Abstract
Automatic Term Extraction (ATE) or Recognition (ATR) is a fundamental processing step preceding many complex knowledge
engineering tasks. However, few methods have been implemented as public tools and in particular, available as open-source freeware.
Further, little effort is made to develop an adaptable and scalable framework that enables customization, development, and comparison
of algorithms under a uniform environment. This paper introduces JATE 2.0, a complete remake of the free Java Automatic Term
Extraction Toolkit (Zhang et al., 2008) delivering new features including: (1) highly modular, adaptable and scalable ATE thanks to
integration with Apache Solr, the open source free-text indexing and search platform; (2) an extended collection of state-of-the-art
algorithms. We carry out experiments on two well-known benchmarking datasets and compare the algorithms along the dimensions of
effectiveness (precision) and efﬁciency (speed and memory consumption). To the best of our knowledge, this is by far the only free ATE
library offering a ﬂexible architecture and the most comprehensive collection of algorithms.

Keywords: term extraction, term recognition, NLP, text mining, Solr, search, indexing

1.

Introduction

Automatic Term Extraction (or Recognition) is an impor-
tant Natural Language Processing (NLP) task that deals
with the extraction of terminologies from domain-speciﬁc
textual corpora. ATE is widely used by both industries
and researchers in many complex tasks, such as Informa-
tion Retrieval (IR), machine translation, ontology engineer-
ing and text summarization (Bowker, 2003; Brewster et
al., 2007; Maynard et al., 2007). Over the years, there
has been constant effort on researching new algorithms,
adapting to different domains and/or languages, and creat-
ing benchmarking datasets (Ananiadou, 1994; Church and
Gale, 1995; Ahmad et al., 1999; Frantzi et al., 2000; Park
et al., 2002; Matsuo and Ishizuka, 2003; Sclano and Ve-
lardi, 2007; Rose et al., 2010; Spasi´c et al., 2013; Zadeh
and Handschuh, 2014).
Given the fundamental role of ATE in many tasks, a real
beneﬁt to the community would be releasing the tools to
facilitate the development of downstream applications, also
encouraging code-reuse, comparative studies, and fostering
further research. Unfortunately, effort in this direction has
been extremely limited. First, only very few tools have been
published with readily available source code (Spasi´c et al.,
2013) and some are proprietary (labs.translated.net, 2015;
Yahoo!, 2015), or no longer maintained (Sclano and Ve-
lardi, 2007)1. Second, different tools have been developed
under different scenarios and evaluated in different domains
using proprietary language resources, making it difﬁcult for
comparison. Third, it is unclear whether and how well these
tools can adapt to different domain tasks and scale up to
large data.
To the best of our knowledge, the only effort towards ad-
dressing these issues is the Java Automatic Term Extrac-
tion Toolkit (JATE)2, where we originally implemented
ﬁve state-of-the-art algorithms under a uniform framework
(Zhang et al., 2008). This work makes one step further

1Original link: http://lcl2.di.uniroma1.it/
2https://code.google.com/p/jatetoolkit.

Google Code has been shut down since Jan 2016.

by completely re-designing and re-implementing JATE to
fulﬁll three goals: adaptability, scalability, and extended
collections of algorithms. The new library, named JATE
2.03, is built on the Apache Solr4 free-text indexing and
search platform and can be either used as a separate mod-
ule, or a Solr plugin during document processing to en-
rich the indexed documents with terms. JATE 2.0 deliv-
ers domain-speciﬁc adaptability and scalability by seam-
lessly integrating with the Solr text analysis capability in
the form of ‘analyzers’5, allowing users to directly exploit
the highly modular and scalable Solr text analysis to facili-
tate term extraction for large data from various content for-
mats. Last but not least, JATE 2.0 expands its collection
of state-of-the-art algorithms6 to double that in the origi-
nal JATE release. We also evaluate these algorithms under
uniform settings on two publicly available benchmarking
datasets. Released as open source software, we believe that
JATE 2.0 offers signiﬁcant value to both researchers and
practitioners.
The remainder of this paper is structured as follows. Sec-
tion 2. discusses related work. Section 3. describes JATE
2.0 in details. Section 5. describes experiments and dis-
cusses results. Section 6. concludes this paper.

2. Related Work
We describe related work from three persectives: ATE
methods in general, ATE tools and software, and text min-
ing plugins based on Solr.
ATE methods typically start with linguistic processors
as the ﬁrst stage to extract candidate terms; followed by

3https://github.com/ziqizhang/jate
4http://lucene.apache.org/solr/
5https://cwiki.apache.org/confluence/

display/solr/Analyzers

6By this we mean the mathematical formulation of term candi-
date scores. We do not claim identical replication of state-of-the-
art methods, as they often differ in pre- and post-processing, such
as term candidate generation, term variant handling and the use of
dictionary.

2262

statistics-based term ranking algorithms to perform can-
didate ﬁltering. Linguistic processors are domain spe-
ciﬁc, and designed to capture term formation and collo-
cation patterns. They often take two forms: ‘closed ﬁl-
ters’ (Arora et al., 2014) focus on precision and are usu-
ally restricted to nouns or noun sequences. ‘Open ﬁlters’
(Frantzi et al., 2000) are more permissive and often al-
low adjectives, adverbs, etc. For both, widely used tech-
niques include Part-of-Speech (PoS) tag sequence match-
ing, N-gram extraction, Noun Phrase (NP) Chunking, and
dictionary lookup. The selection of linguistic processors is
often domain-speciﬁc and important for the trade-off be-
tween precision and recall. The ranking algorithms are of-
ten based on: ‘unithood’ indicating the collocation strength
of units that comprise a single term (Matsuo and Ishizuka,
2003); and ‘termhood’ indicating the association strength
of a term to domain concepts (Ahmad et al., 1999). Most
methods combine both termhood and unithood measures
(Ananiadou, 1994; Frantzi et al., 2000; Park et al., 2002;
Sclano and Velardi, 2007; Spasi´c et al., 2013). Two unique
features of JATE 2.0 are (1) customization of linguistic pro-
cessors of the two forms hence making it adaptable to many
domains and languages, and (2) a wide selection of imple-
mented ranking algorithms, which are not available in any
other tools.
Existing ATE tools and software are extremely limited
for several reasons.
First, many are proprietary soft-
ware (labs.translated.net, 2015), quota-limited (e.g., Open-
Calais7), or restricted to academic usage only (e.g., Ter-
Mine8). Second, all tools, including the original JATE
(Zhang et al., 2008), are built in a monolithic architec-
ture which allows very limited customization and scala-
bility (Spasi´c et al., 2013). Third, most offer very limited
(typically a single choice) conﬁgurability, e.g., in terms of
term candidate extraction and/or ranking algorithms (e.g.,
TermRaider (Maynard et al., 2008), TOPIA9, and Flex-
iTerm (Spasi´c et al., 2013). JATE 2.0 is a free, open-source
library. It is highly modular, allowing customization and
conﬁguration of many components. Meanwhile, to our best
knowledge, no available ATE tools support automatic in-
dexing.
Solr plugins for text mining Solr is an open source en-
terprise search platform written in Java. It is highly mod-
ular, customizable, and scalable, hence widely used as an
industry standard. A core component of Solr is its power-
ful, extensible text processing library covering a wide range
of functionality and languages. Many text mining and NLP
tools have integrated with Solr to beneﬁt from these fea-
tures as well as contributing additional support for doc-
ument indexing (e.g., OpenNLP plugins10, SolrTextTag-
ger11, and UIMA12). However, no plugin is available for

7http://new.opencalais.com/
8http://www.nactem.ac.uk/software/

9https://pypi.python.org/pypi/topia.

termine/

termextract

10https://wiki.apache.org/solr/OpenNLP
11https://github.com/OpenSextant/

SolrTextTagger

12https://uima.apache.org

ATE.

3.

JATE 2.0 Architecture

Figure 1 shows the general architecture of JATE 2.0 and
its workﬂow, which consists of four phases: (1) data pre-
processing; (2) term candidate extraction and indexing; (3)
candidate scoring and ﬁltering and (4) ﬁnal term indexing
and export.
Data pre-processing (Section 3.1.) parses input docu-
ments to raw text content and performs text normaliza-
tion in order to reduce ‘noise’ in irregular data. The pre-
processed text content then passes through the candidate
extraction (Section 3.2.) component that extracts and nor-
malizes term candidates from each document. Candidate
extraction and data pre-processing are embedded as part
of the Solr document indexing process, largely realized by
Solr’s ‘analyzers’. An analyzer is composed of a series
of processors (‘tokenizers’ and ‘ﬁlters’) to form a pipeline,
or an analysis ‘chain’ that examines the text content from
each document, generates a token stream and records sta-
tistical information. Solr already implements a very large
text processing library for this purpose. JATE 2.0 further
extends these. Depending on individual needs, users may
assemble customized analyzers for term candidate genera-
tion. Deﬁning analyzers is achieved by conﬁguration in the
Solr schema, such as that shown in Figure 2.
Next, the candidates are processed by the subsequent ﬁlter-
ing component (Section 3.3.), where different ATE algo-
rithms can be conﬁgured to score and rank the candidates,
and making the ﬁnal selection. The implementation of the
algorithms are largely parallelized. The resulting terms can
either be exported for further validation, or written back to
the index to annotate documents.
JATE 2.0 can be run in two modes: (1) as a separate mod-
ule to extract terms from a corpus using an embedded Solr
instance (Section 4.1.), or (2) as a Solr plugin that performs
ATE as part of the document indexing process to anno-
tate documents with domain-speciﬁc terminologies (Sec-
tion 4.2.). Both modes undergo the same workﬂow de-
scribed above.
JATE 2.0 offers adaptability and scalability thanks to its
seamless integration with Solr and parallelized implemen-
tation. Access to Solr’s very large collection of multi-
lingual text processing libraries that can be conﬁgured in
a plug-and-play way enables JATE 2.0 to be adapted to dif-
ferent languages, document formats, and domains. The rich
collection of algorithms also give users options to better
tailor the tool to speciﬁc use cases. Parallelization enables
JATE 2.0 to scale up to large datasets. Parallelization for
term candidate generation can be achieved via Solr’s dis-
tributed indexing capability under SolrCloud13. Users sim-
ply follow Solr’s guide on conﬁguring distributed indexing.
Parallelization for scoring and ranking algorithms comes
as standard in JATE 2.0 and is also conﬁgurable. Detailed
guidelines on deﬁning analyzers for ATE and conﬁgura-
tions can be found on the JATE 2.0 homepage.

13https://cwiki.apache.org/confluence/

display/solr/SolrCloud

2263

tokenization, such as line 3 of schema.xml shown in Figure
2.

3.2. Candidate Extraction
Term candidate extraction is realized as part of the Solr in-
dexing process. A dedicated ‘ﬁeld’19 is deﬁned within the
Solr indexing schema to hold candidates for each document
(candidate ﬁeld). The ﬁeld is bound to a ‘ﬁeldType’, which
uses a customized analyzer to break content into term can-
didates and normalize them to conﬂate the variants that are
dispersed throughout the corpus. The Solr indexing pro-
cess also generates basic statistical information about can-
didates, such as a candidate’s frequency within each docu-
ment, and the number of documents it appears in20.
The analyzer is highly customizable and adaptable. All ex-
isting Solr text analysis libraries can be used, such as tok-
enization, case folding, stop words removal, token N-gram
extraction, stemming, etc. It is also easy to implement new
components to replace or complement existing ones in a
plug-and-play way. Currently, Solr 5.3 supports nearly 20
implementations of tokenization and over 100 text normal-
ization and ﬁltering methods21, covering a wide range of
languages and use cases. Thus by selecting different com-
ponents and assembling them in different orders, one can
create different analyzers to achieve different extraction and
normalization results.
To support ATE, JATE 2.0 extends Solr’s text analysis li-
braries by supporting three types of linguistic ﬁlters for
term candidate extraction. These include: (1) a PoS pattern
based chunker that extracts candidates based on user spec-
iﬁed patterns (e.g., line 9 of schema.xml in Figure 2); (2)
a token N-gram extractor that extends the built-in one; (3)
a noun phrase chunker. All of these are implemented with
the capability to normalize noisy terms, such as removing
leading and trailing stop words, and non-alphanumeric to-
kens.
In addition, JATE 2.0 also implements an English
lemmatizer, which is a recommended replacement of stem-
mers that can sometimes be too aggressive for ATE. These
offer great ﬂexibility enabling almost any customized term
candidate extraction. Figure 2 shows a standard conﬁgu-
ration of an text analyzer for PoS pattern based candidate
extraction for English.

3.3. Filtering
Extracted term candidates are further processed to make a
ﬁnal decision to separate terms from non-terms.

3.3.1. Pre-ﬁltering
It is often a common practice to apply a ﬁltering process to
term candidates to reduce both noise (e.g., due to irregular
textual data or erroneous PoS tagging) and computation be-
fore the subsequent step of term scoring and ranking. The

19https://cwiki.apache.org/confluence/

display/solr/Solr+Glossary

20For practical reasons, we rely on a separate ﬁeld that indexes
the same content as token N-grams and stored term vectors which
provides the lookup of statistics about term candidates. Details on
this can be found on the JATE website and Solr documentation for
Inverted Index.

21See Solr API documentation.

Figure 1: General Architecture of JATE 2.0

support multi-lingual

3.1. Data pre-processing
With the standard JATE 2.0 conﬁguration, data pre-
processing begins with Solr Content Extraction Library
(also known as SolrCell14) to extract textual content from
It integrates the popular Apache Tika framework15
ﬁles.
that supports detecting and extracting metadata and text
from a large range of ﬁle formats such as plain text, HTML,
PDF, and Microsoft Ofﬁce. To use this, users simply con-
ﬁgure an instance of ExtractingRequestHandler16
for their data in Solr.
To
also use Solr’s
of
speciﬁc analyzers
UpdateRequestProcessor17.
Next, a recommended practice for pre-processing irregu-
lar textual data (commonly found in corporate datasets)
is applying the Solr char ﬁlter component18
to per-
form character-level
For example,
HTMLStripCharFilterFactory detects HTML en-
tities (e.g., ‘&#65;’, ‘&nbsp;’) with corresponding charac-
ters. This can reduce errors in downstream processors (e.g.,
PoS tagger) in the analyzer. This can be conﬁgured as part
of the analyzer in the Solr schema, usually placed before

users may
capability for detecting languages
to language-
indexing using the langid

in documents and map text

text normalization.

documents,

texts

for

14https://cwiki.apache.org/confluence/

display/solr/Uploading+Data+with+Solr+Cell+
using+Apache+Tika

15https://tika.apache.org
16https://wiki.apache.org/solr/

ExtractingRequestHandler

17https://cwiki.apache.org/confluence/
display/solr/Detecting+Languages+During+
Indexing

18https://cwiki.apache.org/confluence/

display/solr/CharFilterFactories

2264

Figure 2: Example of PoS pattern based candidate extraction

following strategies have been implemented in JATE 2.0
and can be conﬁgured according to user needs to balance
precision and recall.
Minimal and maximal character length restricts candi-
dates to a ﬁxed range of character length. Minimal and
maximal token number requires a valid candidate to con-
tain certain number of tokens (e.g., to limit multi-word ex-
pressions). Minimal stop words removal removes lead-
ing and/or trailing stop words in a multi-word expression
(e.g.,‘the cat’ becomes ‘cat’, but ‘Tower of London’ re-
mains intact). All these are implemented as part of the
three term candidate extraction methods described before
and can be conﬁgured in the analyzer.
Frequency threshold allows candidates to be ﬁltered by a
minimum total frequency in the corpus. This is often used
in typical ATE methods, and is conﬁgurable via individual
scoring and ranking algorithms that follow.

3.3.2. Scoring and ranking
In this step, term candidates that pass the pre-ﬁlter are
scored and ranked. Their basic statistical information is
gathered from the Solr index, to create complex features
required by different algorithms.
JATE 2.0 has implemented 10 algorithms for scoring can-
didates, listed in Table 1. TTF is the total frequency of a
candidate in the corpus. It’s usage in ATE was ﬁrstly docu-
mented in (Justeson and Katz, 1995). ATTF divides TTF by
the number of documents a candidate appears in. TTF-IDF
adapts the classic document-speciﬁc TF-IDF used in IR to
work at corpus level, by replacing TF with TTF.
RIDF was initially proposed by (Church and Gale, 1995)
as an enhancement to IDF to identify keywords in a doc-
ument collection. It measures the deviation of the actual
IDF score of a word from its ‘expected’ IDF score, which
is predicted based on a Poisson distribution. The hypothe-
sis is based on the fact that Poisson model ﬁts poorly with
such keywords. Thus a prediction of IDF based on Poisson
can deviate from its actual IDF observed based on a cor-

pus. Empirically, it is shown that all words have real IDF
scores that deviate from the expected value under a Poisson
distribution. However, keywords tend to have larger devi-
ations than non-keywords. We adapt it to work with term
candidates that can be either single words or multi-word
expressions.
CValue observes that real terms in technical domains are
often long, multi-word expressions and usually not nested
in other terms (i.e., as part of the longer terms). Frequency-
based methods are not effective for such terms as (1) nested
term candidates will have at least the same and often higher
frequency, and (2) the fact that a longer string appears n
times is a lot more important than that of a shorter string
appearing n times. Thus CValue computes a score that is
based on the frequency of a candidate and its length, then
adjusted by the frequency of longer candidates that contain
it.
Similarly, RAKE is designed to favour longer multi-word
expressions. It ﬁrstly computes a score for individual words
based on two components: one that favours words oc-
curring often and in longer term candidates, and one that
favours words occurring frequently regardless of the words
which they co-occur with. Then it adds up the scores of
composing words for a candidate.
χ2 promotes term candidates that co-occur very often with
‘frequent’ candidates in the corpus. First, candidates are
ranked by frequency in the corpus and a subset (typically
top n%) is selected - to be called ‘frequent terms’. Next,
candidates are scored based on the degree to which their
co-occurrence with these frequent terms are biased. These
biases can be due to semantic, lexical, or other relations of
two terms. Thus, a candidate showing strong co-occurrence
biases with frequent terms may have an important mean-
ing. To evaluate the statistical signiﬁcance of the biases,
χ2 is used. For each candidate, co-occurrence frequency
with the frequent terms is regarded as a sample value. The
null hypothesis that we expect to reject is that ‘occurrence
of a candidate is independent from occurrence of frequent

2265

3.3.3. Threshold cutoff
Next, a cutoff decision is to be made to separate real terms
from non-terms based on their scores. Three options are
supported: (1) hard threshold, where term candidates with
scores no less than the threshold are chosen; (2) top K,
where top ranked K candidates are chosen; (3) and top K%,
where the top ranked K percentage of candidates are cho-
sen.

4. Using JATE 2.0 in Two Modes

4.1. Embedded mode

The embedded mode is recommended when users need a
list of domain-speciﬁc terms from a corpus to be used in
subsequent knowledge engineering tasks. Users conﬁgure
a Solr instance and in particular, a text analysis chain that
deﬁnes how term candidates are extracted and normalized.
The Solr instance is instantiated as an embedded24 module
that interacts with other components. Users then explicitly
start an indexing process on a corpus to trigger term candi-
date extraction, then use speciﬁc ATE utility classes25, each
wrapping an individual ATE algorithm, to perform candi-
date scoring, ranking, ﬁltering and export.

4.2. Plugin mode

The plugin mode is recommended when users need to in-
dex new or enrich existing index using extracted terms,
which can, e.g., support faceted search. ATE is per-
formed as a Solr plugin. A SolrRequestHandler 26
is implemented so that term extraction can be triggered by
HTTP request. Users conﬁgure their Solr instance in the
same way as above, then start the instance as a back-end
server. To trigger ATE, users send an HTTP request to the
SolrRequestHandler, passing parameters specifying
the input data and the ATE algorithms. Candidate extrac-
tion is optional if the process is performed with document
indexing. ATE then begins and when ﬁnished, updates doc-
uments in the index with extracted terms.

Table 1: ATE algorithms implemented in JATE 2.0

Name
TTF

ATTF
TTF-IDF

Full name
Term Total
Frequency
Average TTF
TTF and Inverse
Doc Frequency
Residual IDF
C-Value
Chi-Square
Rapid Keyword
Extraction
Weirdness Weirdness
GlossEx
TermEx

RIDF
CValue
χ2
RAKE

Glossry Extraction
Term Extraction

Ref.
(Justeson and Katz, 1995),
FiveFilters.org
-
-

(Church and Gale, 1995)
(Ananiadou, 1994)
(Matsuo and Ishizuka, 2003)
(Rose et al., 2010)

(Ahmad et al., 1999)
(Park et al., 2002)
(Sclano and Velardi, 2007)

terms’.
Both RAKE and χ2 were initially developed for extract-
ing document-speciﬁc keywords. We adapt
them for
ATE from document collections. This is done by replac-
ing document-level frequency with corpus-level frequency,
wherever needed.
Weirdness is a contrastive approach (Drouin, 2003) which
is particularly interesting when trying to identify low-
frequency terms. The method compares normalized fre-
quency of a term candidate in a domain-speciﬁc corpus
with a reference corpus, such as the general-purpose British
National Corpus22. The idea is that candidates appearing
more often in the target corpus are more speciﬁc to that
corpus and therefore, more likely to be real terms. To cope
with out-of-vocabulary candidates, we modify this by tak-
ing the sum of the Weirdness scores of composing words
for a candidate.
Both GlossEx and TermEx extend Weirdness. GlossEx lin-
early combines ‘domain speciﬁcity’, which normalizes the
Weirdness score by the length (number of words) of a term
candidate, with ‘term cohesion’ that measures the degree to
which the composing words tend to occur together as a can-
didate other than appearing individually. This is computed
based on comparing the frequency of a candidate against its
composing words. TermEx, in a very similar form, further
extends GlossEx by linearly combining a third component
that promotes candidates with an even probability distribu-
tion across the documents in the corpus (in an analogy, the
candidate ‘gains consensus’ among the documents).
Essentially both GlossEx and TermEx combine termhood
with unithood, which exploits a reference corpus. For these
we introduce a scalar to balance the contribution of unit-
hood, which tends to dominate the ﬁnal score in case of
largely disproportionate sizes of the domain and reference
corpora (e.g., orders of magnitude difference). The scalar
adjusts the normalized frequency of words in both the tar-
get and reference corpora to be in the same orders of mag-
nitude23.

Figure 3: Term Extraction by HTTP request (Plugin mode)

22http://www.natcorp.ox.ac.uk
23Details can be found in the implementation of the two algo-

25See details in the app package of JATE 2.0
26https://wiki.apache.org/solr/

rithms in JATE 2.0.

SolrRequestHandler

24https://wiki.apache.org/solr/

EmbeddedSolr

2266

5. Experiment
We evaluate JATE 2.0 on two datasets, the GENIA dataset
(Kim et al., 2003), a semantically annotated corpus for bio-
textmining previously used by (Zhang et al., 2008); and
the ACL RD-TEC dataset (Zadeh and Handschuh, 2014),
containing publications in the domain of computational lin-
guistics and a list of manually annotated domain-speciﬁc
terms.
GENIA contains 1,999 Medline abstracts, selected using a
PubMed query for the terms ‘human’, ‘blood cells’, and
‘transcription factors’. The corpus is annotated with var-
ious levels of linguistic and semantic information. We
extract any text annotated as ‘cons’ (concept) to compile
a gold standard list of terms. ACL RD-TEC contains
over 10,900 scientiﬁc publications. In (Zadeh and Hand-
schuh, 2014), the corpus is automatically segmented and
PoS tagged. Candidate terms are extracted by applying
a list of patterns based on PoS sequence. These are then
ranked by several ATE algorithms, and the top set of over
82,000 candidates are manually annotated as valid or in-
valid. We use the valid candidates as gold standard terms.
We notice three different versions of corpus available27:
one only contains the sentences where at least one of the
annotated candidate terms must be present (under ‘anno-
tation’), one contains complete raw text ﬁles in XML for-
mat (under ‘cleansed text’), and one contains plain text ﬁles
from the ACL ARC (under ‘external resource’). We use the
XML version and only extract content from ‘<title>’ and
‘<paragraph>’ elements.
We ran all experiments on a server with 16 CPUs. All algo-
rithms follow the same candidate extraction process, unless
otherwise noted. Detailed conﬁgurations are as follows:

• three different analyzers have been tested, each using
PoS sequence pattern based (PoS based) , noun phrase
chunking based (NP Chunking based), and the N-gram
based term candidate extraction. The pipelines are
slightly different for each analyzer and each dataset.
Detailed conﬁguration can be found in the example
section of the JATE website;

• for PoS-based term candidate extraction, we use: for
GENIA, a list of PoS sequence patterns deﬁned in
(Ananiadou, 1994); for ACL RD-TEC, a list of PoS
sequence patterns deﬁned in (Zadeh and Handschuh,
2014);

• min. character length of 2; max. of 40;
• min. tokens of 1, max. of 5;
• leading and trailing stop words and non-alphanumeric
character removal. (for N-gram based removal of any
non-alphanumeric characters);

• stop words removal;
• min. frequency threshold of 2;
• for χ2, the ‘frequent terms’ are selected as top 10%.
Moreover, only candidates that appear in at least two
sentences are considered;

• for Weirdness, GlossEx and TermEx, the BNC corpus

is used as reference;

27http://atmykitchen.info/datasets/acl_rd_

tec/. Accessed: 10 March 2016

Table 2: Comparision of candidate extraction on GENIA
(run in a single thread)

Method
PoS-based
NP-based
N-gram

Term candidates Recall CPU time (millsec)
10,582/38,850
35,799/44,479
48,945/440,974

68,914
118,753
33,721

10%
23%
16%

Table 3: Comparision of candidate extraction on ACL RD-
TEC (run in a single thread)
Term candidates
524,662/1,569,877
585,012/2,014,916
887,316/7,776,457

Recall CPU time (min)
133.84
74%
367
66%
189.20
41%

Method
PoS-based
NP-based
N-gram

The total number of term candidates extracted for each
dataset under each analyzer setting after/before applying
the minimum frequency threshold with associated overall
recall and CPU time are shown in Tables 2 and 3. The low
recall for GENIA may be due to several reasons. First, a
substantial part of the gold standard terms are ‘irregular’, as
they contain numbers, punctuations and symbols. The pat-
terns in our experiment are mainly based on adjectives and
nouns, and will not match irregular terms. Second, we use
a general purpose PoS tagger which does not perform well
for the biomedical text.
In addition, GENIA consists of
very short abstracts and as a result, many legitimate terms
may be removed due to the frequency threshold and lexical
pruning. However, this can be easily rectiﬁed by relaxing
the pre-ﬁlters.
We then show precision of top K terms ranked by each al-
gorithm, commonly used in ATE. Figure 4 shows results for
the GENIA dataset and Figure 5 shows results for the ACL
RD-TEC dataset. First, all algorithms achieve very high
precision on the GENIA dataset. This is partly because the
gold standard terms are very densely distributed. Our anal-
ysis shows that 49% of words are annotated as part of a
term. Second, TFIDF and CValue appear to be the most
reliable methods as they obtain consistently good results
on both datasets. Third, Weirdness, GlossEx and TermEx
are very sensitive to the choice of reference corpus. For
example, on the ACL RD-TEC dataset, phrases containing
‘treebank’ are very highly ranked. However, many of them
are not valid terms. Finally, RAKE is the worst perform-
ing algorithm on both datasets, possibly because it is very
speciﬁcally tailored to document-level (other than corpus-
level) keyword extraction.
Next we compare the efﬁciency of each algorithm for scor-
ing and ranking term candidates, by measuring the max-
imum memory footprint and CPU time for computation.
We found consistent patterns among different algorithms,
despite what term candidate extractor is used (which only
affects absolute ﬁgures as it changes the number of candi-
dates generated, see Tables 2 and 3). Using the PoS-based
term candidate extractor as example, we show the statistics
in Table 4 and Table 5 for the two datasets. As it is shown,
χ2 is the most memory intensive due to the in-memory stor-
age of co-occurrence. This largely depends on the number
of term candidates and frequent terms. In terms of speed,

2267

Figure 4: Comparison of Top K precisions on GENIA

Figure 5: Comparison of Top K precisions on ACL RD-TEC

χ2, CValue and RAKE are among the slowest. While χ2
spends signiﬁcant time on co-occurrence related computa-
tion, CValue and RAKE spend substantial time on comput-
ing the containment relations among candidates.

6. Conclusion
This paper describes JATE 2.0, a highly modular, adaptable
and scalable ATE library with 10 implemented algorithms,
developed within the Apache Solr framework. It advances
existing ATE tools mainly by enabling a signiﬁcant degree
of customization and adaptation thanks to the ﬂexibility un-
der the Solr framework; and making available a large col-
lection of state-of-the-art algorithms. It is expected that the
tool will bring both academia and industries under a uni-
form development and benchmarking framework that will

Table 4: Running time and memory usage on GENIA

Name
TTF
ATTF
TTF-IDF
RIDF
χ2
CValue
Weirdness
GlossEx
TermEx
RAKE

CPU time (sec.) Max vmem (gb)
19
20
20
20
30
47
22
25
27
26

0.81
0.83
0.83
0.78
1.12
1.41
1.07
1.01
1.06
1.42

encourage collaborative effort in this area of research, to

2268

Table 5: Running time and memory usage on ACL RD-
TEC

Name
TTF
ATTF
TTF-IDF
RIDF
χ2
CValue
Weirdness
GlossEx
TermEx
RAKE

CPU time (min:sec) Max vmem (gb)
5:46
5:56
5:58
5:59
21:47
20:59
7:11
6:40
9:52
22:59

4.2
4.18
4.24
4.38
16.05
5.01
4.89
4.88
6.12
5.45

foster further contributions in terms of novel algorithms,
benchmarkings, support for new languages, new text pro-
cessing capabilities, and so on. Future work will look into
the implementation of additional ATE algorithms, particu-
larly machine learning based methods.

7. Acknowledgement
Part of this research has been sponsored by the EU funded
project WeSenseIt under grant agreement number 308429;
and the SPEEAK-PC collaboration agreement 101947 of
the Innovative UK.

8. Bibliographical References
Ahmad, K., Gillam, L., and Tostevin, L. (1999). University
of surrey participation in trec 8: Weirdness indexing for
logical document extrapolation and retrieval (wilder). In
Proceedings of the 8th Text REtrieval Conference.

Ananiadou, S. (1994). A methodology for automatic term
recognition. In Proceedings of the 15th Conference on
Computational Linguistics, pages 1034–1038, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.

Arora, C., Sabetzadeh, M., Briand, L., and Zimmer, F.
Improving requirements glossary construction
(2014).
via clustering: approach and industrial case studies. In
Proceedings of the 8th ACM/IEEE International Sympo-
sium on Empirical Software Engineering and Measure-
ment, page 18. ACM.

Bowker, L.

(2003). Terminology tools for translators.

BENJAMINS TRANSLATION LIBRARY, 35:49–66.

Brewster, C., Iria, J., Zhang, Z., Ciravegna, F., Guthrie, L.,
and Wilks, Y. (2007). Dynamic iterative ontology learn-
ing. In Proceedings of the 6th International Conference
on Recent Advances in Natural Language Processing,
Borovets, Bulgaria, September.

Church, K. W. and Gale, W. A. (1995). Inverse document
frequency (idf): A measure of deviations from poisson.
In Proceedings of the ACL 3rd Workshop on Very Large
Corpora, pages 121–130, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

Drouin, P.

(2003). Term extraction using non-technical
corpora as a point of leverage. Terminology, 9(1):99–
115.

Frantzi, K. T., Ananiadou, S., and Mima, H. (2000). Auto-
matic recognition of multi-word terms:. the c-value/nc-

value method. Natural Language Processing For Digital
Libraries, 3(2):115–130.

Justeson, J. and Katz, S. M. (1995). Technical terminol-
ogy: some linguistic properties and an algorithm for
identiﬁcation in text. Natural Language Engineering,
1(1):9–27.

Kim, J.-D., Ohta, T., Tateisi, Y., and ichi Tsujii, J. (2003).
GENIA corpus - a semantically annotated corpus for bio-
In ISMB (Supplement of Bioinformatics),
textmining.
pages 180–182.
labs.translated.net.

(2015).
https://developer.yahoo.com/contentanalysis/
accessed 9-October-2015].

Terminology extraction.
[Online;

Matsuo, Y. and Ishizuka, M. (2003). Keyword extraction
from a single document using word co-occurrence sta-
tistical information. International Journal on Artiﬁcial
Intelligence Tools, 13(1):157–169.

Maynard, D., Saggion, H., Yankova, M., Bontcheva, K.,
and Peters, W. (2007). Natural language technology for
information integration in business intelligence. In Busi-
ness Information Systems, pages 366–380. Springer.
Maynard, D., Li, Y., and Peters, W. (2008). Nlp techniques
for term extraction and ontology population. In Proceed-
ings of the 2008 Conference on Ontology Learning and
Population: Bridging the Gap Between Text and Knowl-
edge, pages 107–127, Amsterdam, The Netherlands, The
Netherlands. IOS Press.

Park, Y., Byrd, R. J., and Boguraev, B. K. (2002). Auto-
matic glossary extraction: Beyond terminology identiﬁ-
cation. In Proceedings of the 19th International Confer-
ence on Computational Linguistics, pages 1–7, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.

Rose, S., Engel, D., Cramer, N., and Cowley, W., (2010).
Automatic keyword extraction from individual docu-
ments. John Wiley and Sons.

Sclano, F. and Velardi, P. (2007). Termextractor: a web
application to learn the shared terminology of emer-
gent web communities. In Proceedings of the 3rd Inter-
national Conference on Interoperability for Enterprise
Software and Applications.

Spasi´c, I., Greenwood, M., Preece, A., Francis, N., and El-
wyn, G. (2013). Flexiterm: a ﬂexible term recognition
method. Journal of Biomedical Semantics, 4(27).

Yahoo!

(2015). Yahoo!

content analysis api.
[Online;

https://developer.yahoo.com/contentanalysis/
accessed 9-October-2015].
Zadeh, B. and Handschuh, S.

(2014). The acl rd-tec:
A dataset for benchmarking terminology extraction and
classiﬁcation in computational linguistics. In Proceed-
ings of the 4th International Workshop on Computational
Terminology (Computerm), pages 52–63, Dublin, Ire-
land, August. Association for Computational Linguistics
and Dublin City University.

Zhang, Z., Iria, J., Brewster, C., and Ciravegna, F. (2008).
A comparative evaluation of term recognition algo-
In Proceedings of The 6th international con-
rithms.
ference on Language Resources and Evaluation, Mar-
rakech, Morocco, May.

2269

JATE 2.0: Java Automatic Term Extraction with Apache Solr

Ziqi Zhang, Jie Gao, Fabio Ciravegna
Regent Court, 211 Portobello, Shefﬁeld, UK, S1 4DP
ziqi.zhang@shefﬁeld.ac.uk, j.gao@shefﬁeld.ac.uk, f.ciravegna@shefﬁeld.ac.uk

Abstract
Automatic Term Extraction (ATE) or Recognition (ATR) is a fundamental processing step preceding many complex knowledge
engineering tasks. However, few methods have been implemented as public tools and in particular, available as open-source freeware.
Further, little effort is made to develop an adaptable and scalable framework that enables customization, development, and comparison
of algorithms under a uniform environment. This paper introduces JATE 2.0, a complete remake of the free Java Automatic Term
Extraction Toolkit (Zhang et al., 2008) delivering new features including: (1) highly modular, adaptable and scalable ATE thanks to
integration with Apache Solr, the open source free-text indexing and search platform; (2) an extended collection of state-of-the-art
algorithms. We carry out experiments on two well-known benchmarking datasets and compare the algorithms along the dimensions of
effectiveness (precision) and efﬁciency (speed and memory consumption). To the best of our knowledge, this is by far the only free ATE
library offering a ﬂexible architecture and the most comprehensive collection of algorithms.

Keywords: term extraction, term recognition, NLP, text mining, Solr, search, indexing

1.

Introduction

Automatic Term Extraction (or Recognition) is an impor-
tant Natural Language Processing (NLP) task that deals
with the extraction of terminologies from domain-speciﬁc
textual corpora. ATE is widely used by both industries
and researchers in many complex tasks, such as Informa-
tion Retrieval (IR), machine translation, ontology engineer-
ing and text summarization (Bowker, 2003; Brewster et
al., 2007; Maynard et al., 2007). Over the years, there
has been constant effort on researching new algorithms,
adapting to different domains and/or languages, and creat-
ing benchmarking datasets (Ananiadou, 1994; Church and
Gale, 1995; Ahmad et al., 1999; Frantzi et al., 2000; Park
et al., 2002; Matsuo and Ishizuka, 2003; Sclano and Ve-
lardi, 2007; Rose et al., 2010; Spasi´c et al., 2013; Zadeh
and Handschuh, 2014).
Given the fundamental role of ATE in many tasks, a real
beneﬁt to the community would be releasing the tools to
facilitate the development of downstream applications, also
encouraging code-reuse, comparative studies, and fostering
further research. Unfortunately, effort in this direction has
been extremely limited. First, only very few tools have been
published with readily available source code (Spasi´c et al.,
2013) and some are proprietary (labs.translated.net, 2015;
Yahoo!, 2015), or no longer maintained (Sclano and Ve-
lardi, 2007)1. Second, different tools have been developed
under different scenarios and evaluated in different domains
using proprietary language resources, making it difﬁcult for
comparison. Third, it is unclear whether and how well these
tools can adapt to different domain tasks and scale up to
large data.
To the best of our knowledge, the only effort towards ad-
dressing these issues is the Java Automatic Term Extrac-
tion Toolkit (JATE)2, where we originally implemented
ﬁve state-of-the-art algorithms under a uniform framework
(Zhang et al., 2008). This work makes one step further

1Original link: http://lcl2.di.uniroma1.it/
2https://code.google.com/p/jatetoolkit.

Google Code has been shut down since Jan 2016.

by completely re-designing and re-implementing JATE to
fulﬁll three goals: adaptability, scalability, and extended
collections of algorithms. The new library, named JATE
2.03, is built on the Apache Solr4 free-text indexing and
search platform and can be either used as a separate mod-
ule, or a Solr plugin during document processing to en-
rich the indexed documents with terms. JATE 2.0 deliv-
ers domain-speciﬁc adaptability and scalability by seam-
lessly integrating with the Solr text analysis capability in
the form of ‘analyzers’5, allowing users to directly exploit
the highly modular and scalable Solr text analysis to facili-
tate term extraction for large data from various content for-
mats. Last but not least, JATE 2.0 expands its collection
of state-of-the-art algorithms6 to double that in the origi-
nal JATE release. We also evaluate these algorithms under
uniform settings on two publicly available benchmarking
datasets. Released as open source software, we believe that
JATE 2.0 offers signiﬁcant value to both researchers and
practitioners.
The remainder of this paper is structured as follows. Sec-
tion 2. discusses related work. Section 3. describes JATE
2.0 in details. Section 5. describes experiments and dis-
cusses results. Section 6. concludes this paper.

2. Related Work
We describe related work from three persectives: ATE
methods in general, ATE tools and software, and text min-
ing plugins based on Solr.
ATE methods typically start with linguistic processors
as the ﬁrst stage to extract candidate terms; followed by

3https://github.com/ziqizhang/jate
4http://lucene.apache.org/solr/
5https://cwiki.apache.org/confluence/

display/solr/Analyzers

6By this we mean the mathematical formulation of term candi-
date scores. We do not claim identical replication of state-of-the-
art methods, as they often differ in pre- and post-processing, such
as term candidate generation, term variant handling and the use of
dictionary.

2262

statistics-based term ranking algorithms to perform can-
didate ﬁltering. Linguistic processors are domain spe-
ciﬁc, and designed to capture term formation and collo-
cation patterns. They often take two forms: ‘closed ﬁl-
ters’ (Arora et al., 2014) focus on precision and are usu-
ally restricted to nouns or noun sequences. ‘Open ﬁlters’
(Frantzi et al., 2000) are more permissive and often al-
low adjectives, adverbs, etc. For both, widely used tech-
niques include Part-of-Speech (PoS) tag sequence match-
ing, N-gram extraction, Noun Phrase (NP) Chunking, and
dictionary lookup. The selection of linguistic processors is
often domain-speciﬁc and important for the trade-off be-
tween precision and recall. The ranking algorithms are of-
ten based on: ‘unithood’ indicating the collocation strength
of units that comprise a single term (Matsuo and Ishizuka,
2003); and ‘termhood’ indicating the association strength
of a term to domain concepts (Ahmad et al., 1999). Most
methods combine both termhood and unithood measures
(Ananiadou, 1994; Frantzi et al., 2000; Park et al., 2002;
Sclano and Velardi, 2007; Spasi´c et al., 2013). Two unique
features of JATE 2.0 are (1) customization of linguistic pro-
cessors of the two forms hence making it adaptable to many
domains and languages, and (2) a wide selection of imple-
mented ranking algorithms, which are not available in any
other tools.
Existing ATE tools and software are extremely limited
for several reasons.
First, many are proprietary soft-
ware (labs.translated.net, 2015), quota-limited (e.g., Open-
Calais7), or restricted to academic usage only (e.g., Ter-
Mine8). Second, all tools, including the original JATE
(Zhang et al., 2008), are built in a monolithic architec-
ture which allows very limited customization and scala-
bility (Spasi´c et al., 2013). Third, most offer very limited
(typically a single choice) conﬁgurability, e.g., in terms of
term candidate extraction and/or ranking algorithms (e.g.,
TermRaider (Maynard et al., 2008), TOPIA9, and Flex-
iTerm (Spasi´c et al., 2013). JATE 2.0 is a free, open-source
library. It is highly modular, allowing customization and
conﬁguration of many components. Meanwhile, to our best
knowledge, no available ATE tools support automatic in-
dexing.
Solr plugins for text mining Solr is an open source en-
terprise search platform written in Java. It is highly mod-
ular, customizable, and scalable, hence widely used as an
industry standard. A core component of Solr is its power-
ful, extensible text processing library covering a wide range
of functionality and languages. Many text mining and NLP
tools have integrated with Solr to beneﬁt from these fea-
tures as well as contributing additional support for doc-
ument indexing (e.g., OpenNLP plugins10, SolrTextTag-
ger11, and UIMA12). However, no plugin is available for

7http://new.opencalais.com/
8http://www.nactem.ac.uk/software/

9https://pypi.python.org/pypi/topia.

termine/

termextract

10https://wiki.apache.org/solr/OpenNLP
11https://github.com/OpenSextant/

SolrTextTagger

12https://uima.apache.org

ATE.

3.

JATE 2.0 Architecture

Figure 1 shows the general architecture of JATE 2.0 and
its workﬂow, which consists of four phases: (1) data pre-
processing; (2) term candidate extraction and indexing; (3)
candidate scoring and ﬁltering and (4) ﬁnal term indexing
and export.
Data pre-processing (Section 3.1.) parses input docu-
ments to raw text content and performs text normaliza-
tion in order to reduce ‘noise’ in irregular data. The pre-
processed text content then passes through the candidate
extraction (Section 3.2.) component that extracts and nor-
malizes term candidates from each document. Candidate
extraction and data pre-processing are embedded as part
of the Solr document indexing process, largely realized by
Solr’s ‘analyzers’. An analyzer is composed of a series
of processors (‘tokenizers’ and ‘ﬁlters’) to form a pipeline,
or an analysis ‘chain’ that examines the text content from
each document, generates a token stream and records sta-
tistical information. Solr already implements a very large
text processing library for this purpose. JATE 2.0 further
extends these. Depending on individual needs, users may
assemble customized analyzers for term candidate genera-
tion. Deﬁning analyzers is achieved by conﬁguration in the
Solr schema, such as that shown in Figure 2.
Next, the candidates are processed by the subsequent ﬁlter-
ing component (Section 3.3.), where different ATE algo-
rithms can be conﬁgured to score and rank the candidates,
and making the ﬁnal selection. The implementation of the
algorithms are largely parallelized. The resulting terms can
either be exported for further validation, or written back to
the index to annotate documents.
JATE 2.0 can be run in two modes: (1) as a separate mod-
ule to extract terms from a corpus using an embedded Solr
instance (Section 4.1.), or (2) as a Solr plugin that performs
ATE as part of the document indexing process to anno-
tate documents with domain-speciﬁc terminologies (Sec-
tion 4.2.). Both modes undergo the same workﬂow de-
scribed above.
JATE 2.0 offers adaptability and scalability thanks to its
seamless integration with Solr and parallelized implemen-
tation. Access to Solr’s very large collection of multi-
lingual text processing libraries that can be conﬁgured in
a plug-and-play way enables JATE 2.0 to be adapted to dif-
ferent languages, document formats, and domains. The rich
collection of algorithms also give users options to better
tailor the tool to speciﬁc use cases. Parallelization enables
JATE 2.0 to scale up to large datasets. Parallelization for
term candidate generation can be achieved via Solr’s dis-
tributed indexing capability under SolrCloud13. Users sim-
ply follow Solr’s guide on conﬁguring distributed indexing.
Parallelization for scoring and ranking algorithms comes
as standard in JATE 2.0 and is also conﬁgurable. Detailed
guidelines on deﬁning analyzers for ATE and conﬁgura-
tions can be found on the JATE 2.0 homepage.

13https://cwiki.apache.org/confluence/

display/solr/SolrCloud

2263

tokenization, such as line 3 of schema.xml shown in Figure
2.

3.2. Candidate Extraction
Term candidate extraction is realized as part of the Solr in-
dexing process. A dedicated ‘ﬁeld’19 is deﬁned within the
Solr indexing schema to hold candidates for each document
(candidate ﬁeld). The ﬁeld is bound to a ‘ﬁeldType’, which
uses a customized analyzer to break content into term can-
didates and normalize them to conﬂate the variants that are
dispersed throughout the corpus. The Solr indexing pro-
cess also generates basic statistical information about can-
didates, such as a candidate’s frequency within each docu-
ment, and the number of documents it appears in20.
The analyzer is highly customizable and adaptable. All ex-
isting Solr text analysis libraries can be used, such as tok-
enization, case folding, stop words removal, token N-gram
extraction, stemming, etc. It is also easy to implement new
components to replace or complement existing ones in a
plug-and-play way. Currently, Solr 5.3 supports nearly 20
implementations of tokenization and over 100 text normal-
ization and ﬁltering methods21, covering a wide range of
languages and use cases. Thus by selecting different com-
ponents and assembling them in different orders, one can
create different analyzers to achieve different extraction and
normalization results.
To support ATE, JATE 2.0 extends Solr’s text analysis li-
braries by supporting three types of linguistic ﬁlters for
term candidate extraction. These include: (1) a PoS pattern
based chunker that extracts candidates based on user spec-
iﬁed patterns (e.g., line 9 of schema.xml in Figure 2); (2)
a token N-gram extractor that extends the built-in one; (3)
a noun phrase chunker. All of these are implemented with
the capability to normalize noisy terms, such as removing
leading and trailing stop words, and non-alphanumeric to-
kens.
In addition, JATE 2.0 also implements an English
lemmatizer, which is a recommended replacement of stem-
mers that can sometimes be too aggressive for ATE. These
offer great ﬂexibility enabling almost any customized term
candidate extraction. Figure 2 shows a standard conﬁgu-
ration of an text analyzer for PoS pattern based candidate
extraction for English.

3.3. Filtering
Extracted term candidates are further processed to make a
ﬁnal decision to separate terms from non-terms.

3.3.1. Pre-ﬁltering
It is often a common practice to apply a ﬁltering process to
term candidates to reduce both noise (e.g., due to irregular
textual data or erroneous PoS tagging) and computation be-
fore the subsequent step of term scoring and ranking. The

19https://cwiki.apache.org/confluence/

display/solr/Solr+Glossary

20For practical reasons, we rely on a separate ﬁeld that indexes
the same content as token N-grams and stored term vectors which
provides the lookup of statistics about term candidates. Details on
this can be found on the JATE website and Solr documentation for
Inverted Index.

21See Solr API documentation.

Figure 1: General Architecture of JATE 2.0

support multi-lingual

3.1. Data pre-processing
With the standard JATE 2.0 conﬁguration, data pre-
processing begins with Solr Content Extraction Library
(also known as SolrCell14) to extract textual content from
It integrates the popular Apache Tika framework15
ﬁles.
that supports detecting and extracting metadata and text
from a large range of ﬁle formats such as plain text, HTML,
PDF, and Microsoft Ofﬁce. To use this, users simply con-
ﬁgure an instance of ExtractingRequestHandler16
for their data in Solr.
To
also use Solr’s
of
speciﬁc analyzers
UpdateRequestProcessor17.
Next, a recommended practice for pre-processing irregu-
lar textual data (commonly found in corporate datasets)
is applying the Solr char ﬁlter component18
to per-
form character-level
For example,
HTMLStripCharFilterFactory detects HTML en-
tities (e.g., ‘&#65;’, ‘&nbsp;’) with corresponding charac-
ters. This can reduce errors in downstream processors (e.g.,
PoS tagger) in the analyzer. This can be conﬁgured as part
of the analyzer in the Solr schema, usually placed before

users may
capability for detecting languages
to language-
indexing using the langid

in documents and map text

text normalization.

documents,

texts

for

14https://cwiki.apache.org/confluence/

display/solr/Uploading+Data+with+Solr+Cell+
using+Apache+Tika

15https://tika.apache.org
16https://wiki.apache.org/solr/

ExtractingRequestHandler

17https://cwiki.apache.org/confluence/
display/solr/Detecting+Languages+During+
Indexing

18https://cwiki.apache.org/confluence/

display/solr/CharFilterFactories

2264

Figure 2: Example of PoS pattern based candidate extraction

following strategies have been implemented in JATE 2.0
and can be conﬁgured according to user needs to balance
precision and recall.
Minimal and maximal character length restricts candi-
dates to a ﬁxed range of character length. Minimal and
maximal token number requires a valid candidate to con-
tain certain number of tokens (e.g., to limit multi-word ex-
pressions). Minimal stop words removal removes lead-
ing and/or trailing stop words in a multi-word expression
(e.g.,‘the cat’ becomes ‘cat’, but ‘Tower of London’ re-
mains intact). All these are implemented as part of the
three term candidate extraction methods described before
and can be conﬁgured in the analyzer.
Frequency threshold allows candidates to be ﬁltered by a
minimum total frequency in the corpus. This is often used
in typical ATE methods, and is conﬁgurable via individual
scoring and ranking algorithms that follow.

3.3.2. Scoring and ranking
In this step, term candidates that pass the pre-ﬁlter are
scored and ranked. Their basic statistical information is
gathered from the Solr index, to create complex features
required by different algorithms.
JATE 2.0 has implemented 10 algorithms for scoring can-
didates, listed in Table 1. TTF is the total frequency of a
candidate in the corpus. It’s usage in ATE was ﬁrstly docu-
mented in (Justeson and Katz, 1995). ATTF divides TTF by
the number of documents a candidate appears in. TTF-IDF
adapts the classic document-speciﬁc TF-IDF used in IR to
work at corpus level, by replacing TF with TTF.
RIDF was initially proposed by (Church and Gale, 1995)
as an enhancement to IDF to identify keywords in a doc-
ument collection. It measures the deviation of the actual
IDF score of a word from its ‘expected’ IDF score, which
is predicted based on a Poisson distribution. The hypothe-
sis is based on the fact that Poisson model ﬁts poorly with
such keywords. Thus a prediction of IDF based on Poisson
can deviate from its actual IDF observed based on a cor-

pus. Empirically, it is shown that all words have real IDF
scores that deviate from the expected value under a Poisson
distribution. However, keywords tend to have larger devi-
ations than non-keywords. We adapt it to work with term
candidates that can be either single words or multi-word
expressions.
CValue observes that real terms in technical domains are
often long, multi-word expressions and usually not nested
in other terms (i.e., as part of the longer terms). Frequency-
based methods are not effective for such terms as (1) nested
term candidates will have at least the same and often higher
frequency, and (2) the fact that a longer string appears n
times is a lot more important than that of a shorter string
appearing n times. Thus CValue computes a score that is
based on the frequency of a candidate and its length, then
adjusted by the frequency of longer candidates that contain
it.
Similarly, RAKE is designed to favour longer multi-word
expressions. It ﬁrstly computes a score for individual words
based on two components: one that favours words oc-
curring often and in longer term candidates, and one that
favours words occurring frequently regardless of the words
which they co-occur with. Then it adds up the scores of
composing words for a candidate.
χ2 promotes term candidates that co-occur very often with
‘frequent’ candidates in the corpus. First, candidates are
ranked by frequency in the corpus and a subset (typically
top n%) is selected - to be called ‘frequent terms’. Next,
candidates are scored based on the degree to which their
co-occurrence with these frequent terms are biased. These
biases can be due to semantic, lexical, or other relations of
two terms. Thus, a candidate showing strong co-occurrence
biases with frequent terms may have an important mean-
ing. To evaluate the statistical signiﬁcance of the biases,
χ2 is used. For each candidate, co-occurrence frequency
with the frequent terms is regarded as a sample value. The
null hypothesis that we expect to reject is that ‘occurrence
of a candidate is independent from occurrence of frequent

2265

3.3.3. Threshold cutoff
Next, a cutoff decision is to be made to separate real terms
from non-terms based on their scores. Three options are
supported: (1) hard threshold, where term candidates with
scores no less than the threshold are chosen; (2) top K,
where top ranked K candidates are chosen; (3) and top K%,
where the top ranked K percentage of candidates are cho-
sen.

4. Using JATE 2.0 in Two Modes

4.1. Embedded mode

The embedded mode is recommended when users need a
list of domain-speciﬁc terms from a corpus to be used in
subsequent knowledge engineering tasks. Users conﬁgure
a Solr instance and in particular, a text analysis chain that
deﬁnes how term candidates are extracted and normalized.
The Solr instance is instantiated as an embedded24 module
that interacts with other components. Users then explicitly
start an indexing process on a corpus to trigger term candi-
date extraction, then use speciﬁc ATE utility classes25, each
wrapping an individual ATE algorithm, to perform candi-
date scoring, ranking, ﬁltering and export.

4.2. Plugin mode

The plugin mode is recommended when users need to in-
dex new or enrich existing index using extracted terms,
which can, e.g., support faceted search. ATE is per-
formed as a Solr plugin. A SolrRequestHandler 26
is implemented so that term extraction can be triggered by
HTTP request. Users conﬁgure their Solr instance in the
same way as above, then start the instance as a back-end
server. To trigger ATE, users send an HTTP request to the
SolrRequestHandler, passing parameters specifying
the input data and the ATE algorithms. Candidate extrac-
tion is optional if the process is performed with document
indexing. ATE then begins and when ﬁnished, updates doc-
uments in the index with extracted terms.

Table 1: ATE algorithms implemented in JATE 2.0

Name
TTF

ATTF
TTF-IDF

Full name
Term Total
Frequency
Average TTF
TTF and Inverse
Doc Frequency
Residual IDF
C-Value
Chi-Square
Rapid Keyword
Extraction
Weirdness Weirdness
GlossEx
TermEx

RIDF
CValue
χ2
RAKE

Glossry Extraction
Term Extraction

Ref.
(Justeson and Katz, 1995),
FiveFilters.org
-
-

(Church and Gale, 1995)
(Ananiadou, 1994)
(Matsuo and Ishizuka, 2003)
(Rose et al., 2010)

(Ahmad et al., 1999)
(Park et al., 2002)
(Sclano and Velardi, 2007)

terms’.
Both RAKE and χ2 were initially developed for extract-
ing document-speciﬁc keywords. We adapt
them for
ATE from document collections. This is done by replac-
ing document-level frequency with corpus-level frequency,
wherever needed.
Weirdness is a contrastive approach (Drouin, 2003) which
is particularly interesting when trying to identify low-
frequency terms. The method compares normalized fre-
quency of a term candidate in a domain-speciﬁc corpus
with a reference corpus, such as the general-purpose British
National Corpus22. The idea is that candidates appearing
more often in the target corpus are more speciﬁc to that
corpus and therefore, more likely to be real terms. To cope
with out-of-vocabulary candidates, we modify this by tak-
ing the sum of the Weirdness scores of composing words
for a candidate.
Both GlossEx and TermEx extend Weirdness. GlossEx lin-
early combines ‘domain speciﬁcity’, which normalizes the
Weirdness score by the length (number of words) of a term
candidate, with ‘term cohesion’ that measures the degree to
which the composing words tend to occur together as a can-
didate other than appearing individually. This is computed
based on comparing the frequency of a candidate against its
composing words. TermEx, in a very similar form, further
extends GlossEx by linearly combining a third component
that promotes candidates with an even probability distribu-
tion across the documents in the corpus (in an analogy, the
candidate ‘gains consensus’ among the documents).
Essentially both GlossEx and TermEx combine termhood
with unithood, which exploits a reference corpus. For these
we introduce a scalar to balance the contribution of unit-
hood, which tends to dominate the ﬁnal score in case of
largely disproportionate sizes of the domain and reference
corpora (e.g., orders of magnitude difference). The scalar
adjusts the normalized frequency of words in both the tar-
get and reference corpora to be in the same orders of mag-
nitude23.

Figure 3: Term Extraction by HTTP request (Plugin mode)

22http://www.natcorp.ox.ac.uk
23Details can be found in the implementation of the two algo-

25See details in the app package of JATE 2.0
26https://wiki.apache.org/solr/

rithms in JATE 2.0.

SolrRequestHandler

24https://wiki.apache.org/solr/

EmbeddedSolr

2266

5. Experiment
We evaluate JATE 2.0 on two datasets, the GENIA dataset
(Kim et al., 2003), a semantically annotated corpus for bio-
textmining previously used by (Zhang et al., 2008); and
the ACL RD-TEC dataset (Zadeh and Handschuh, 2014),
containing publications in the domain of computational lin-
guistics and a list of manually annotated domain-speciﬁc
terms.
GENIA contains 1,999 Medline abstracts, selected using a
PubMed query for the terms ‘human’, ‘blood cells’, and
‘transcription factors’. The corpus is annotated with var-
ious levels of linguistic and semantic information. We
extract any text annotated as ‘cons’ (concept) to compile
a gold standard list of terms. ACL RD-TEC contains
over 10,900 scientiﬁc publications. In (Zadeh and Hand-
schuh, 2014), the corpus is automatically segmented and
PoS tagged. Candidate terms are extracted by applying
a list of patterns based on PoS sequence. These are then
ranked by several ATE algorithms, and the top set of over
82,000 candidates are manually annotated as valid or in-
valid. We use the valid candidates as gold standard terms.
We notice three different versions of corpus available27:
one only contains the sentences where at least one of the
annotated candidate terms must be present (under ‘anno-
tation’), one contains complete raw text ﬁles in XML for-
mat (under ‘cleansed text’), and one contains plain text ﬁles
from the ACL ARC (under ‘external resource’). We use the
XML version and only extract content from ‘<title>’ and
‘<paragraph>’ elements.
We ran all experiments on a server with 16 CPUs. All algo-
rithms follow the same candidate extraction process, unless
otherwise noted. Detailed conﬁgurations are as follows:

• three different analyzers have been tested, each using
PoS sequence pattern based (PoS based) , noun phrase
chunking based (NP Chunking based), and the N-gram
based term candidate extraction. The pipelines are
slightly different for each analyzer and each dataset.
Detailed conﬁguration can be found in the example
section of the JATE website;

• for PoS-based term candidate extraction, we use: for
GENIA, a list of PoS sequence patterns deﬁned in
(Ananiadou, 1994); for ACL RD-TEC, a list of PoS
sequence patterns deﬁned in (Zadeh and Handschuh,
2014);

• min. character length of 2; max. of 40;
• min. tokens of 1, max. of 5;
• leading and trailing stop words and non-alphanumeric
character removal. (for N-gram based removal of any
non-alphanumeric characters);

• stop words removal;
• min. frequency threshold of 2;
• for χ2, the ‘frequent terms’ are selected as top 10%.
Moreover, only candidates that appear in at least two
sentences are considered;

• for Weirdness, GlossEx and TermEx, the BNC corpus

is used as reference;

27http://atmykitchen.info/datasets/acl_rd_

tec/. Accessed: 10 March 2016

Table 2: Comparision of candidate extraction on GENIA
(run in a single thread)

Method
PoS-based
NP-based
N-gram

Term candidates Recall CPU time (millsec)
10,582/38,850
35,799/44,479
48,945/440,974

68,914
118,753
33,721

10%
23%
16%

Table 3: Comparision of candidate extraction on ACL RD-
TEC (run in a single thread)
Term candidates
524,662/1,569,877
585,012/2,014,916
887,316/7,776,457

Recall CPU time (min)
133.84
74%
367
66%
189.20
41%

Method
PoS-based
NP-based
N-gram

The total number of term candidates extracted for each
dataset under each analyzer setting after/before applying
the minimum frequency threshold with associated overall
recall and CPU time are shown in Tables 2 and 3. The low
recall for GENIA may be due to several reasons. First, a
substantial part of the gold standard terms are ‘irregular’, as
they contain numbers, punctuations and symbols. The pat-
terns in our experiment are mainly based on adjectives and
nouns, and will not match irregular terms. Second, we use
a general purpose PoS tagger which does not perform well
for the biomedical text.
In addition, GENIA consists of
very short abstracts and as a result, many legitimate terms
may be removed due to the frequency threshold and lexical
pruning. However, this can be easily rectiﬁed by relaxing
the pre-ﬁlters.
We then show precision of top K terms ranked by each al-
gorithm, commonly used in ATE. Figure 4 shows results for
the GENIA dataset and Figure 5 shows results for the ACL
RD-TEC dataset. First, all algorithms achieve very high
precision on the GENIA dataset. This is partly because the
gold standard terms are very densely distributed. Our anal-
ysis shows that 49% of words are annotated as part of a
term. Second, TFIDF and CValue appear to be the most
reliable methods as they obtain consistently good results
on both datasets. Third, Weirdness, GlossEx and TermEx
are very sensitive to the choice of reference corpus. For
example, on the ACL RD-TEC dataset, phrases containing
‘treebank’ are very highly ranked. However, many of them
are not valid terms. Finally, RAKE is the worst perform-
ing algorithm on both datasets, possibly because it is very
speciﬁcally tailored to document-level (other than corpus-
level) keyword extraction.
Next we compare the efﬁciency of each algorithm for scor-
ing and ranking term candidates, by measuring the max-
imum memory footprint and CPU time for computation.
We found consistent patterns among different algorithms,
despite what term candidate extractor is used (which only
affects absolute ﬁgures as it changes the number of candi-
dates generated, see Tables 2 and 3). Using the PoS-based
term candidate extractor as example, we show the statistics
in Table 4 and Table 5 for the two datasets. As it is shown,
χ2 is the most memory intensive due to the in-memory stor-
age of co-occurrence. This largely depends on the number
of term candidates and frequent terms. In terms of speed,

2267

Figure 4: Comparison of Top K precisions on GENIA

Figure 5: Comparison of Top K precisions on ACL RD-TEC

χ2, CValue and RAKE are among the slowest. While χ2
spends signiﬁcant time on co-occurrence related computa-
tion, CValue and RAKE spend substantial time on comput-
ing the containment relations among candidates.

6. Conclusion
This paper describes JATE 2.0, a highly modular, adaptable
and scalable ATE library with 10 implemented algorithms,
developed within the Apache Solr framework. It advances
existing ATE tools mainly by enabling a signiﬁcant degree
of customization and adaptation thanks to the ﬂexibility un-
der the Solr framework; and making available a large col-
lection of state-of-the-art algorithms. It is expected that the
tool will bring both academia and industries under a uni-
form development and benchmarking framework that will

Table 4: Running time and memory usage on GENIA

Name
TTF
ATTF
TTF-IDF
RIDF
χ2
CValue
Weirdness
GlossEx
TermEx
RAKE

CPU time (sec.) Max vmem (gb)
19
20
20
20
30
47
22
25
27
26

0.81
0.83
0.83
0.78
1.12
1.41
1.07
1.01
1.06
1.42

encourage collaborative effort in this area of research, to

2268

Table 5: Running time and memory usage on ACL RD-
TEC

Name
TTF
ATTF
TTF-IDF
RIDF
χ2
CValue
Weirdness
GlossEx
TermEx
RAKE

CPU time (min:sec) Max vmem (gb)
5:46
5:56
5:58
5:59
21:47
20:59
7:11
6:40
9:52
22:59

4.2
4.18
4.24
4.38
16.05
5.01
4.89
4.88
6.12
5.45

foster further contributions in terms of novel algorithms,
benchmarkings, support for new languages, new text pro-
cessing capabilities, and so on. Future work will look into
the implementation of additional ATE algorithms, particu-
larly machine learning based methods.

7. Acknowledgement
Part of this research has been sponsored by the EU funded
project WeSenseIt under grant agreement number 308429;
and the SPEEAK-PC collaboration agreement 101947 of
the Innovative UK.

8. Bibliographical References
Ahmad, K., Gillam, L., and Tostevin, L. (1999). University
of surrey participation in trec 8: Weirdness indexing for
logical document extrapolation and retrieval (wilder). In
Proceedings of the 8th Text REtrieval Conference.

Ananiadou, S. (1994). A methodology for automatic term
recognition. In Proceedings of the 15th Conference on
Computational Linguistics, pages 1034–1038, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.

Arora, C., Sabetzadeh, M., Briand, L., and Zimmer, F.
Improving requirements glossary construction
(2014).
via clustering: approach and industrial case studies. In
Proceedings of the 8th ACM/IEEE International Sympo-
sium on Empirical Software Engineering and Measure-
ment, page 18. ACM.

Bowker, L.

(2003). Terminology tools for translators.

BENJAMINS TRANSLATION LIBRARY, 35:49–66.

Brewster, C., Iria, J., Zhang, Z., Ciravegna, F., Guthrie, L.,
and Wilks, Y. (2007). Dynamic iterative ontology learn-
ing. In Proceedings of the 6th International Conference
on Recent Advances in Natural Language Processing,
Borovets, Bulgaria, September.

Church, K. W. and Gale, W. A. (1995). Inverse document
frequency (idf): A measure of deviations from poisson.
In Proceedings of the ACL 3rd Workshop on Very Large
Corpora, pages 121–130, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

Drouin, P.

(2003). Term extraction using non-technical
corpora as a point of leverage. Terminology, 9(1):99–
115.

Frantzi, K. T., Ananiadou, S., and Mima, H. (2000). Auto-
matic recognition of multi-word terms:. the c-value/nc-

value method. Natural Language Processing For Digital
Libraries, 3(2):115–130.

Justeson, J. and Katz, S. M. (1995). Technical terminol-
ogy: some linguistic properties and an algorithm for
identiﬁcation in text. Natural Language Engineering,
1(1):9–27.

Kim, J.-D., Ohta, T., Tateisi, Y., and ichi Tsujii, J. (2003).
GENIA corpus - a semantically annotated corpus for bio-
In ISMB (Supplement of Bioinformatics),
textmining.
pages 180–182.
labs.translated.net.

(2015).
https://developer.yahoo.com/contentanalysis/
accessed 9-October-2015].

Terminology extraction.
[Online;

Matsuo, Y. and Ishizuka, M. (2003). Keyword extraction
from a single document using word co-occurrence sta-
tistical information. International Journal on Artiﬁcial
Intelligence Tools, 13(1):157–169.

Maynard, D., Saggion, H., Yankova, M., Bontcheva, K.,
and Peters, W. (2007). Natural language technology for
information integration in business intelligence. In Busi-
ness Information Systems, pages 366–380. Springer.
Maynard, D., Li, Y., and Peters, W. (2008). Nlp techniques
for term extraction and ontology population. In Proceed-
ings of the 2008 Conference on Ontology Learning and
Population: Bridging the Gap Between Text and Knowl-
edge, pages 107–127, Amsterdam, The Netherlands, The
Netherlands. IOS Press.

Park, Y., Byrd, R. J., and Boguraev, B. K. (2002). Auto-
matic glossary extraction: Beyond terminology identiﬁ-
cation. In Proceedings of the 19th International Confer-
ence on Computational Linguistics, pages 1–7, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.

Rose, S., Engel, D., Cramer, N., and Cowley, W., (2010).
Automatic keyword extraction from individual docu-
ments. John Wiley and Sons.

Sclano, F. and Velardi, P. (2007). Termextractor: a web
application to learn the shared terminology of emer-
gent web communities. In Proceedings of the 3rd Inter-
national Conference on Interoperability for Enterprise
Software and Applications.

Spasi´c, I., Greenwood, M., Preece, A., Francis, N., and El-
wyn, G. (2013). Flexiterm: a ﬂexible term recognition
method. Journal of Biomedical Semantics, 4(27).

Yahoo!

(2015). Yahoo!

content analysis api.
[Online;

https://developer.yahoo.com/contentanalysis/
accessed 9-October-2015].
Zadeh, B. and Handschuh, S.

(2014). The acl rd-tec:
A dataset for benchmarking terminology extraction and
classiﬁcation in computational linguistics. In Proceed-
ings of the 4th International Workshop on Computational
Terminology (Computerm), pages 52–63, Dublin, Ire-
land, August. Association for Computational Linguistics
and Dublin City University.

Zhang, Z., Iria, J., Brewster, C., and Ciravegna, F. (2008).
A comparative evaluation of term recognition algo-
In Proceedings of The 6th international con-
rithms.
ference on Language Resources and Evaluation, Mar-
rakech, Morocco, May.

2269

