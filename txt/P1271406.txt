Discovering and Removing Exogenous State Variables and Rewards for
Reinforcement Learning

Thomas Dietterich 1 George Trimponias 2 Zhitang Chen 2

8
1
0
2
 
n
u
J
 
5
 
 
]

G
L
.
s
c
[
 
 
1
v
4
8
5
1
0
.
6
0
8
1
:
v
i
X
r
a

Abstract

Exogenous state variables and rewards can slow
down reinforcement learning by injecting uncon-
trolled variation into the reward signal. We for-
malize exogenous state variables and rewards and
identify conditions under which an MDP with
exogenous state can be decomposed into an ex-
ogenous Markov Reward Process involving only
the exogenous state+reward and an endogenous
Markov Decision Process deﬁned with respect to
only the endogenous rewards. We also derive a
variance-covariance condition under which Monte
Carlo policy evaluation on the endogenous MDP
is accelerated compared to using the full MDP.
Similar speedups are likely to carry over to all
RL algorithms. We develop two algorithms for
discovering the exogenous variables and test them
on several MDPs. Results show that the algo-
rithms are practical and can signiﬁcantly speed up
reinforcement learning.

1. Introduction

In many practical settings, the actions of an agent have only
a limited effect on the environment. For example, in a wire-
less cellular network, system performance is determined
by a number of parameters that must be dynamically con-
trolled to optimize performance. We can formulate this as a
Markov Decision Process (MDP) in which the reward func-
tion is the negative of the number of users who are suffering
from low bandwidth. However, the reward is heavily inﬂu-
enced by exogenous factors such as the number, location,
and behavior of the cellular network customers. Customer
demand varies stochastically as a function of latent factors
(news, special events, trafﬁc accidents). In addition, atmo-
spheric conditions can affect the capacity of each wireless

1School of EECS, Oregon State University, Corvallis, OR,
USA 2Huawei Noah’s Ark Lab, Hong Kong. Correspondence to:
Thomas Dietterich <tgd@cs.orst.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

channel. This high degree of stochasticity can confuse rein-
forcement learning algorithms, because during exploration,
the expected beneﬁt of trying action a in state s is hard to
determine. Many trials are required to average away the
exogenous component of the reward so that the effect of
the action can be measured. For temporal difference algo-
rithms, such as Q Learning, the learning rate will need to
be very small. For policy gradient algorithms, the number
of Monte Carlo trials required to estimate the gradient will
be very large (or equivalently, the step size will need to be
very small). In this paper, we analyze this setting and de-
velop algorithms for automatically detecting and removing
the effects of exogenous state variables. This accelerates
reinforcement learning (RL).

This paper begins by deﬁning exogenous variables
and rewards and presenting the exogenous-endogenous
(Exo/Endo) MDP decomposition. We show that, under
the assumption that the reward function decomposes addi-
tively into exogenous and endogenous components, the Bell-
man equation for the original MDP decomposes into two
equations: one for the exogenous Markov reward process
(Exo-MRP) and the other for the endogenous MDP (Endo-
MDP). Importantly, every optimal policy for the Endo-MDP
is an optimal policy for the full MDP. Next we study con-
ditions under which solving the Endo-MDP is faster (in
sample complexity) than solving the full MDP. To do this,
we derive dynamic programming updates for the covariance
between the H-horizon returns of the Exo-MRP and the
Endo-MDP, which may be of independent interest. The
third part of the paper presents an abstract algorithm for
automatically identifying the Exo/Endo decomposition. We
develop two approximations to this general algorithm. One
is a global scheme that computes the entire decomposition
at once; the second is a faster stepwise method that can scale
to large problems. Finally, we present experimental results
to illustrate cases where the Exo/Endo decomposition yields
large speedups and other cases where it does not.

2. MDPs with Exogenous Variables and

Rewards

We study discrete time MDPs with stochastic rewards (Put-
erman, 1994; Sutton & Barto, 1998); the state and ac-

Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning

tion spaces may be either discrete or continuous. Nota-
tion: state space S, action space A, reward distribution
R : S × A (cid:55)→ D((cid:60)) (where D((cid:60)) is the space of proba-
bility densities over the real numbers), transition function
P : S × A (cid:55)→ D(S) (where D(S) is the space of prob-
ability densities or distributions over S), starting state s0,
and discount factor γ ∈ (0, 1). We assume that for all
(s, a) ∈ S × A, R(s, a) has expected value m(s, a) and
ﬁnite variance σ2(s, a).

Suppose the state space can be decomposed into two sub-
spaces X and E according to S = E × X . We will say that
the subspace X is exogenous if the transition function can
be decomposed as P (e(cid:48), x(cid:48)|e, x, a) = P (x(cid:48)|x)P (e(cid:48)|x, e, a)
∀e, e(cid:48) ∈ E, x, x(cid:48) ∈ X , a ∈ A, where s(cid:48) = (e(cid:48), x(cid:48)) is the
state that results from executing action a in state s = (e, x).
We will say that an MDP is an Exogenous State MDP if its
transition function can be decomposed in this way. We will
say it is an Additively Decomposable Exogenous State MDP
if its reward function can be decomposed as the sum of two
terms as follows. Let Rexo : X (cid:55)→ D((cid:60)) be the exogenous
reward distribution and Rend : E × X (cid:55)→ D((cid:60)) be the en-
dogenous reward distribution such that r = rexo + rend
where rexo ∼ Rexo(·|x) with mean mexo(x) and vari-
ance σ2
exo(x) < ∞ and rend ∼ Rend(·|e, x, a) with mean
mend(e, x, a) and variance σ2

end(e, x, a) < ∞.

Theorem 1 For any Additively Decomposable Exogenous
State MDP with exogenous state space X , the H-step ﬁnite-
horizon Bellman optimality equation can be decomposed
into two separate equations, one for a Markov Reward Pro-
cess involving only X and Rexo and the other for an MDP
(the endo-MDP) involving only Rend:
V (e, x; h) = Vexo(x; h) + Vend(e, x; h)
Vexo(x; h) = mexo(x; h) +

(1)
(2)

γEx(cid:48)∼P (x(cid:48)|x)[Vexo(x(cid:48); h − 1)]

a

Vend(e, x; h) = max

mend(e, x, a) +
γEx(cid:48)∼P (x(cid:48)|x);e(cid:48)∼P (e(cid:48)|e,x,a)[Vend(e(cid:48), x(cid:48); h − 1)].
Proof. Proof by induction on the horizon H. Note that
the expectations could be either sums (if S is discrete) or
integrals (if S is continuous).

(3)

Base case: H = 1; we take one action and terminate.

V (e, x; 1) = mexo(x) + max

mend(x, a).

a

The base case is established by setting Vexo(x; 1) =
mexo(x) and Vend(e, x; 1) = maxa mend(e, x, a).

Recursive case: H = h.

V (e, x; h) = mexo(x) + max

{mend(e, x, a) +
γEx(cid:48)∼P (x(cid:48)|x);e(cid:48)∼P (e(cid:48)|e,x,a)[Vexo(x(cid:48); h − 1) +

a

Vend(e(cid:48), x(cid:48); h − 1)]}

Distribute the expectation over the sum in brackets and
simplify. We obtain

V (e, x; h) = mexo(x) + γEx(cid:48)∼P (x(cid:48)|x)[Vexo(x(cid:48); h − 1)] +

max
a

{mend(e, x, a) +
γEx(cid:48)∼P (x(cid:48)|x);e(cid:48)∼P (e(cid:48)|e,x,a)[Vend(e(cid:48), x(cid:48); h − 1)]}

The result is established by setting
Vexo(x; h) = mexo(x) + γEx(cid:48)∼P (x(cid:48)|x)[Vexo(x(cid:48); h − 1)]
mend(e, x, a) +
Vend(e, x; h) = max
γEx(cid:48)∼P (x(cid:48)|x);e(cid:48)∼P (e(cid:48)|e,x,a)[Vend(e(cid:48), x(cid:48); h − 1)].

a

QED.

Lemma 1 Any optimal policy for the endo-MDP of Equa-
tion 3 is an optimal policy for the full exogenous state MDP.
Proof. Because Vexo(s; H) does not depend on the policy,
the optimal policy can be computed simply by solving the
endo-MDP. QED.

We will refer to Equations 1, 2 and 3 as the Exo/Endo
Decomposition of the full MDP.

In an unpublished manuscript, Bray (2017) proves a simi-
lar result. He also identiﬁes conditions under which value
iteration and policy iteration on the Endo-MDP can be accel-
erated by computing the eigenvector decomposition of the
endogenous transition matrix. While such techniques are
useful for MDP planning with a known transition matrix, we
do not know how to exploit them in reinforcement learning
where the MDP is unknown.

In other related work, McGregor et al. (2017) show how
to remove known exogenous state variables in order to ac-
celerate an algorithm known as Model Free Monte Carlo
(Fonteneau et al., 2012). Their experiments obtain substan-
tial improvements in policy evaluation and reinforcement
learning.

3. Analysis of the Exo/Endo Decomposition

Suppose we are given the decomposition of the state space
into exogenous and endogenous subspaces. Under what
conditions would reinforcement learning on the endoge-
nous MDP be more efﬁcient than on the original MDP? To
explore this question, let us consider the problem of esti-
mating the value of a ﬁxed policy π in a given start state
s0 via Monte Carlo trials of length H. We will compare
the sample complexity of estimating V π(s0; H) on the full
MDP to the sample complexity of estimating V π
end(s0; H)
on the Endogenous MDP. Of course most RL algorithms
must do more than simply estimate V π(s0; H) for ﬁxed π,
but the difﬁculty of estimating V π(s0; H) is closely related
to the difﬁculty of ﬁtting a value function approximator or
estimating the gradient in a policy gradient method.

Deﬁne Bπ(s0; H) to be a random variable for the H-step
cumulative discounted return of starting in state s0 and
choosing actions according to π for H steps. To compute
a Monte Carlo estimate of V π(s0; H), we will generate

Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning

N realizations b1, . . . , bN of Bπ(s0; H) by executing N
H-step trajectories in the MDP, each time starting in s0.

Theorem 2 For any (cid:15) > 0 and any 0 < δ < 1, let
ˆV π(s0; H) = 1/N (cid:80)N
i=0 bi be the Monte Carlo estimate of
the expected H-step return of policy π starting in state s0.
If

N ≥

Var[Bπ(s0; H)]
δ(cid:15)2

then

P [| ˆV π(s0; H) − V π(s0; H)| > (cid:15)] ≤ δ.

Proof. This is a simple application of the Chebychev In-
equality,

P (|X − E[X]| > (cid:15)) ≤

Var[X]
(cid:15)2

,

with X = ˆV π(s0; H).
The variance of the mean
of N iid random variables is the variance of any sin-
gle variable divided by N . Hence Var[ ˆV π(s0; H)] =
Var[Bπ(s0; H)]/N . To obtain the result, plug this into
the Chebychev inequality, set the rhs equal to δ, and solve
for N . QED.

Now let us consider the Exo/Endo decomposition of the
MDP. Let Bπ
x (s0; H) denote the H-step return of the ex-
ogenous MRP and Bπ
e (s0; H) denote the return of the en-
dogenous MDP. Then Bπ
e (s0; H) is a random
variable denoting the cumulative H-horizon discounted re-
turn of the original, full MDP. Let Var[Bπ(s0; H))] be the
variance of Bπ(s0; H) and Cov[Bπ
e (s0; H)] be
the covariance between them.

x (s0; H) + Bπ

x (s0; H), Bπ

Theorem 3 The Chebychev upper bound on the number of
Monte Carlo trials required to estimate V π(s0; H) using the
endogenous MDP will be reduced compared to the full MDP
iff Var[Bπ

x (s0; H)] > −2 Cov[Bπ

x (s0; H), Bπ

e (s0; H)].

Proof.
From Theorem 2, we know that the sample
size bound using the endogenous MDP will be less
than the required sample size using the full MDP when
Var[Bπ
e (s0; H)]. The
variance of the sum of two random variables is

e (s0; H)] < Var[Bπ

x (s0; H) + Bπ

Var[Bπ

x (s0; H) + Bπ

e (s0; H)] =

Var[Bπ

x (s0; H)] + Var[Bπ
x (s0; H), Bπ

2Cov[Bπ

e (s0; H)].

e (s0; H)] +

Hence, Var[Bπ
iff Var[Bπ
QED.

e (s0; H)] < Var[Bπ
x (s0; H)] > −2Cov[Bπ

x (s0; H) + Bπ
x (s0; H), Bπ

e (s0; H)]
e (s0; H)].

To evaluate this covariance condition, we need to compute
the variance and covariance of the H-step returns. We derive
dynamic programming formulas for these.

Theorem 4 The variance of the H-step return Bπ(s; H)
can be computed as the solution to the following dynamic
program:

V π(s; 0) := 0; Var[Bπ(s; 0)] := 0
V π(s; h) := m(s, π(s)) +

γEs(cid:48)∼P (s(cid:48)|s,π(s))[V π(s(cid:48); h − 1)]

Var[Bπ(s; h)] := σ2(s, π(s)) − V π(s; h)2 +
γ2Es(cid:48)∼P (s(cid:48)|s,π(s))[Var[Bπ(s(cid:48); h − 1)]] +
Es(cid:48)∼P (s(cid:48)|s,π(s)[m(s, π(s)) + γV π(s(cid:48); h − 1)]2

Equations 5 and 6 apply for all h > 0.

(4)

(5)

(6)

Proof. Sobel (1982) analyzed the variance of inﬁnite hori-
zon discounted MDPs with deterministic rewards. We mod-
ify his proof to handle a ﬁxed horizon and stochastic rewards.
We proceed by induction on h. To simplify notation, we
omit the dependence on π.

Base Case: H = 0. This is established by Equations 4.
Inductive Step: H = h. Write

B(s; h) = R(s) + γB(s(cid:48); h − 1),

where the rhs involves the three random variables R(s),
s(cid:48), and B(s(cid:48); h − 1). To obtain Equation 5, compute the
expected value
V (s; h) = EB(s;h)[B(s; h)] =

Es(cid:48),R(s),B(s(cid:48);h−1)[R(s) + γB(s(cid:48); h − 1)],

and take each expectation in turn.

To obtain the formula for the variance, write the standard
formula for the variance:
Var[B(s; h)] = EB(s;h)[B(s; h)2] − EB(s;h)[B(s; h)]2
Substitute R(s)+γB(s(cid:48); h−1) in the ﬁrst term and simplify
the second term to obtain

= Es(cid:48),R(s),B(s(cid:48);h−1)
Expand the square in the ﬁrst term:
= Es(cid:48),R(s),B(s(cid:48);h−1)[R(s)2 + 2R(s)γB(s(cid:48); h − 1) +

(cid:2){R(s) + γB(s(cid:48); h − 1)}2(cid:3) − V (s; h)2

γ2B(s(cid:48); h − 1)2] − V (s; h)2

Distribute the two innermost expectations over the sum:
= Es(cid:48)[ER(s)[R(s)2] + 2m(s)γV (s(cid:48); h − 1) +
γ2EB(s(cid:48);h−1)[B(s(cid:48); h − 1)2]] − V (s; h)2

Apply the deﬁnition of variance in reverse to terms 1 and 3
in brackets:
= Es(cid:48)[Var[R(s)] + m(s)2 + 2m(s)γV (s(cid:48); h − 1) +

γ2Var[B(s; h − 1)] + γ2V (s(cid:48); h − 1)2] − V (s; h)2

Factor the quadratic involving terms 1, 3, and 4:
= Es(cid:48)[σ2(s) + [m(s) + γV (s(cid:48); h − 1)]2 +
γ2Var[B(s(cid:48); h − 1)]] − V (s; h)2

Finally, distribute the expectation with respect to s(cid:48) to obtain
Equation 6. QED.

Theorem 5 The covariance between the exogenous H-
step return Bπ
x (x; H) and the endogenous H-step return

Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning

e (e, x; H) can be computed via the following dynamic

Bπ
program:

Cov[Bπ
Cov[Bπ

x (x; 0), Bπ
x (x; h), Bπ

e (e, x; 0)] := 0
e (e, x; h)] :=

γ2Ee(cid:48)∼P (e(cid:48)|e,x),x(cid:48)∼P (x(cid:48)|x)

x (x(cid:48); h − 1), Bπ

(cid:0)Cov[Bπ
[mx(x) + γVx(x(cid:48); h − 1)] ×
[me(e, x, π(e, x)) + γV π
e (e, x; h).

x (x; h)V π

−V π

Equations 7 and 8 apply for all h > 0.

e (e(cid:48), x(cid:48); h − 1)] +

e (e(cid:48), x(cid:48); h − 1)](cid:1)

(7)

(8)

Proof. By induction. The base case is established by Equa-
tion 7. For the inductive case, we begin with the formula for
non-centered covariance:

Cov[Bx(x; h), Be(e, x; h)] =

EBx(x;h),Be(e,x;h)[Bx(x; h)Be(e, x; h)] −

Vx(x; h)Ve(e, x; h)

Replace Bx(x; h) by Rx(x) + γBx(x(cid:48); h − 1) and
Be(e, x; h) by Re(e, x) + γBe(e(cid:48), x(cid:48); h − 1) and replace the
expectations wrt Bx(x; h) and Be(e, x; h) by expectations
wrt the six variables {e(cid:48), x(cid:48), Rx(x), Re(e, x), Bx(x(cid:48); h −
1), Be(e(cid:48), x(cid:48); h − 1)}. We will use the following ab-
breviations for these variables: s(cid:48) = {e(cid:48), x(cid:48)}, r =
{Re(x), Re(e, x)}, and B = {Bx(x(cid:48); h−1), Be(e(cid:48), x(cid:48); h−
1)}.

Cov[Bx(x; h), Be(e, x; h)] = −Vx(x; h)Ve(e, x; h) +

Es(cid:48),r,B[(Rx(x) + γBx(x(cid:48); h − 1)) ×
(Re(e, x) + γBe(e(cid:48), x(cid:48); h − 1))]

We will focus on the expectation term. Multiply out the two
terms and distribute the expectations wrt r and B:
Es(cid:48)[mx(x)me(e, x) + γmx(x)Ve(e(cid:48), x(cid:48); h − 1) +

γme(e, x)Vx(x(cid:48); h − 1) +
γ2EB[Bx(x(cid:48); h − 1)Be(e(cid:48), x(cid:48); h − 1)]]

Apply the non-centered covariance formula “in reverse” to
term 4.
Es(cid:48)[mx(x)me(e, x) + γmx(x)Ve(e(cid:48), x(cid:48); h − 1) +

γme(e, x)Vx(x(cid:48); h − 1) +
γ2Vx(x(cid:48); h − 1)Ve(e(cid:48), x(cid:48); h − 1) +
γ2Cov[Bx(x(cid:48); h − 1)Be(e(cid:48), x(cid:48); h − 1)]]

Distribute expectation with respect to s(cid:48):
mx(x)me(e, x) + γmx(x)Es(cid:48)[Ve(e(cid:48), x(cid:48); h − 1)] +

γme(e, x)Es(cid:48)[Vx(x(cid:48); h − 1)] +
γ2Es(cid:48)[Vx(x(cid:48); h − 1)]Es(cid:48)[Ve(e(cid:48), x(cid:48); h − 1)] +
γ2Cov[Bx(x(cid:48); h − 1)Be(e(cid:48), x(cid:48); h − 1)]]

Obtain Equation 8 by factoring the ﬁrst four terms,
writing the expectations explicitly, and including the
−Vx(x; h)Ve(e, x; h) term. QED.

To gain some intuition for this theorem, examine the three
terms on the right-hand side of Equation 8. The ﬁrst is the

“recursive” covariance for h − 1. The second is the one-step
non-centered covariance, which is the expected value of
e and V π
the product of the backed-up values for V π
x . The
third term is the product of V π
x for the current state,
which re-centers the covariance.

e and V π

Theorems 4 and 5 allow us to check the covariance condition
of Theorem 3 in every state, including the start state s0, so
that we can decide whether to solve the original MDP or
the endo-MDP. Some special cases are easy to verify. For
example, if the mean exogenous reward mx(s) = 0, for all
states, then the covariance condition reduces to σ2
x(s) > 0.

4. Algorithms for Decomposing an MDP into
Exogenous and Endogenous Components

i)}N

In some applications,
it is easy to specify the exoge-
nous state variables, but in others, we must discover them
from training data. Suppose we are given a database of
{(si, ai, ri, s(cid:48)
i=1 sample transitions obtained by execut-
ing one or more exploration policies in the full MDP. As-
sume each si, s(cid:48)
i ∈ S is a d-dimensional real-valued vector
and ai ∈ A is a c-dimensional real-valued vector (possibly
a one-hot encoding of c discrete actions). In the following,
we center si and s(cid:48)

i by subtracting off the sample mean.

We seek to learn three functions Fexo, Fend, and G param-
eterized by wx, we, and wG, such that x = Fexo(s; wx) is
the exogenous state, e = Fend(s; we) is the endogenous
state, and s = G(Fexo(s), Fexo(s); wG) recovers the orig-
inal state from the exogenous and endogenous parts. We
want to capture as much exogenous state as possible subject
to the constraint that we (1) satisfy the conditional indepen-
dence relationship P (s(cid:48)|s, a) = P (x(cid:48)|x)P (e(cid:48), x(cid:48)|e, x, a),
and (2) we can recover the original state from the exoge-
nous and endogenous parts. We formulate the decomposi-
tion problem as the following abstract optimization problem:

E[(cid:107)Fexo(s(cid:48); wx)(cid:107)]

arg max
wx,we,wG
subject to
ˆI(Fexo(s(cid:48); wx); [Fend(s; we), a]|Fexo(s; wx)) < (cid:15)
E[(cid:107)G(Fexo(s(cid:48); wx), Fend(s(cid:48); we); wG) − s(cid:48)(cid:107)] < (cid:15)(cid:48)

(9)

We treat the (si, ai, ri, s(cid:48)
i) observations as samples from
the random variables s, a, r, s(cid:48) and the expectations are es-
timated from these samples. The objective is to maximize
the expected “size” of Fexo; below we will instantiate this
abstract norm. In the ﬁrst constraint, ˆI(·; ·|·) denotes the
estimated conditional mutual information. Ideally, it should
be 0, which implies that P (x(cid:48)|x, e, a) = P (x(cid:48)|x). We only
require it to be smaller than (cid:15). The second constraint en-
codes the requirement that the average reconstruction error
of the state should be small. We make the usual assumption
that x(cid:48) and s(cid:48) are independent conditioned in x, s, and a.

Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning

We now instantiate this abstract schema by making speciﬁc
choices for the norm, Fexo, Fend, and G. Instantiate the
norm as the squared L2 norm, so the objective is to maxi-
mize the variance of the exogenous state. Deﬁne Fexo as a
linear projection from the full d-dimensional state space to
a smaller dx-dimensional space, deﬁned by the matrix Wx.
The projected version of state s is W (cid:62)
x s. Deﬁne the endoge-
nous state e to contain the components of s not contained
within x. From linear decomposition theory (Jolliffe, 2002),
we know that for a ﬁxed Wx consisting of orthonormal com-
x Wx = Idx ), the exogenous state using all d
ponents (W (cid:62)
dimensions is x = Fexo(s) = WxW (cid:62)
x s, and the endoge-
nous state e = Fend(s) = s − x = s − WxW (cid:62)
x s. Under
this approach, G(x, e) = Fexo(s) + Fend(s) = x + e = s,
so the reconstruction error is 0, and the second constraint in
(9) is trivially satisﬁed. The endo-exo optimization problem
becomes

E[(cid:107)W (cid:62)

x s(cid:48)(cid:107)2
2]

max
0≤dx≤d,Wx∈Rd×dx
subject to W (cid:62)
ˆI(W (cid:62)

x Wx = Idx

x s(cid:48); [s − WxW (cid:62)

x s, a]|W (cid:62)

x s) < (cid:15).

(10)

Formulation (10) involves simultaneous optimization over
the unknown dimensionality dx and the projection ma-
trix Wx.
It is hard to solve, because of the conditional
mutual information constraint. To tackle this, we ap-
proximate ˆI(X; Y |Z) by the partial correlation coefﬁcient
P CC(X, Y |Z), deﬁned as the Frobenius norm of the nor-
malized partial covariance matrix V (X, Y, Z) (Baba et al.,
2004; Fukumizu et al., 2008; 2004):

P CC(X; Y |Z) = tr(V (cid:62)(X, Y, Z)V (X, Y, Z))

where

ZZΣZY )Σ−1/2
Y Y ,

V (X, Y, Z) = Σ−1/2

XX (ΣXY − ΣXZΣ−1
and tr is the trace. If the set of random variables (X, Y, Z)
follow a multivariate Gaussian distribution, then the partial
correlation coefﬁcient of a pair of variables given all the
other variables is equal to the conditional correlation coefﬁ-
cient (Baba et al., 2004). With this change, we can express
our optimization problem in matrix form as follows. Ar-
range the data into matrices S, S(cid:48), X, X (cid:48), E, E(cid:48), each with
n rows and d columns, where the ith row refers to the ith
instance. Let A be the action matrix with n rows and c
columns:

x S(cid:48)(cid:62)S(cid:48)W )

tr(W (cid:62)

max
0≤dx≤d,Wx∈Rd×dx
subject to W (cid:62)
P CC(S(cid:48)Wx; [S − SWxW (cid:62)

x Wx = Idx

x , A]|SWx) < (cid:15).

(11)

4.1. Global Algorithm

Formulation (11) is challenging to solve directly because of
the second constraint, so we seek an approximate solution.

We simplify (11) as

P CC(S(cid:48)Wx; [S − SWxW (cid:62)

x , A]|SWx)

min
Wx∈Rd×dx
subject to W (cid:62)

x Wx = Idx

(12)

This formulation assumes a ﬁxed exogenous dimensionality
dx and computes the projection with the minimal PCC. The
orthonormality condition constrains Wx to lie on a Stiefel
manifold (Stiefel, 1935). Several optimization algorithms
exist for optimizing on Stiefel manifolds (Jiang & Dai, 2015;
Absil et al., 2007; Edelman et al., 1999). We employ the
algorithms implemented in the Manopt package (Boumal
et al., 2014).

For a ﬁxed dimensionality dx of the exogenous state, we
can solve (12) to minimize the PCC. Given that our true
objective is (11) and the optimal value of dx is unknown,
we must try all values dx = 0, . . . , d, and pick the dx that
achieves the maximal variance, provided that the PCC from
optimization problem (12) is less than (cid:15). This is costly since
it requires solving O(d) manifold optimization problems.
We can speed this up somewhat by iterating dx from d down
to 1 and stopping with the ﬁrst projection Wx that satis-
ﬁes the PCC constraint. If no such projection exists, then
the exogenous projection is empty, and the state is fully
endogenous. The justiﬁcation for this early-stopping ap-
proach is that the exogenous decomposition that maximizes
dx will contain any component that appears in exogenous
decompositions of lower dimensionality. Hence, the Wx
that attains maximal variance is achieved by the largest dx
satisfying the PCC constraint. Algorithm 1 describes this
procedure. Unfortunately, this scheme must still solve O(d)
optimization problems in the worst case. We now introduce
an efﬁcient stepwise algorithm.

Algorithm 1 Global Exo/Endo State Decomposition
1: Inputs: A database of transitions {(si, ai, ri, s(cid:48)
i)}N
i=1
2: Output: The exogenous state projection matrix Wx
3: for dx = d down to 1 do
4:

Solve the following optimization problem:

Wx :=

arg min
W ∈Rd×dx
subject to W (cid:62)W = Idx

P CC(S(cid:48)W ; [S − SW W (cid:62), A]|SW )

x , A]|SWx)

pcc ← P CC(S(cid:48)Wx; [S − SWxW (cid:62)
if pcc < (cid:15) then
return Wx

5:
6:
7:
end if
8:
9: end for
10: return empty projection [ ]

Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning

4.2. Stepwise Algorithm

Our stepwise algorithm extracts the components (vectors)
of the exogenous projection matrix Wx one at a time by
solving a sequence of small optimization problems. Sup-
pose we have already identiﬁed the ﬁrst k − 1 compo-
nents of Wx, w1, . . . , wk−1 and we seek to identify wk.
These k − 1 components deﬁne k − 1 exogenous state
variables x1, x2, . . . , xk−1. Recall that the original objec-
tive P CC(X (cid:48); [E, A]|X) seeks to uncover the conditional
independence P (x(cid:48)|x, e, a) = P (x(cid:48)|x). This requires us
to know x and e, whereas we only know a portion of x,
and we therefore do not know e at all. To circumvent this
problem, we make two approximations. First, we elim-
inate e from the conditional independence. Second, we
assume that P (X (cid:48)|X) has a lower-triangular form, so that
P (x(cid:48)
k|x, a) = P (x(cid:48)
k|x1, . . . , xk−1, xk, a). This yields the
simpler objective P CC(X (cid:48)

k; A|X1, . . . , Xk).

What damage is done by eliminating E? Components with
low values for P CC(X (cid:48); [E, A]|X) will also have low val-
ues for P CC(X (cid:48); A|X), so this objective will not miss true
exogenous components. On the other hand, it may ﬁnd
components that are not exogenous, because they depend on
E. To address this, after discovering a new component we
add it to Wx, only if it satisﬁes the original PCC constraint
conditioned on both E and A. What damage is done by
assuming a triangular dependence structure? We do not
have a mathematical characterization of this case, so we will
assess it experimentally.

Algorithm 2 Stepwise Exo/Endo State Decomposition
1: Inputs: A database of transitions {(si, ai, ri, s(cid:48)
i)}N
i=1
2: Output: The exogenous state projection matrix Wx
3: Initialize projection matrix Wx ← [ ], Cx ← [ ], k ← 0

4: repeat
5: N ← orthonormal basis for null space of Cx
6:

Solve the following optimization problem:
ˆw :=

P CC(S(cid:48)[Wx, N w(cid:62)]; A|S[Wx, N w(cid:62)])

arg min
w∈R1×(d−k)
subject to w(cid:62)w = 1

wk+1 ← N ˆw(cid:62)
Cx ← Cx ∪ {wk+1}
E = S − S[Wx, wk+1][Wx, wk+1](cid:62)
pcc ← P CC(S(cid:48)[Wx, wk+1]; [E, A]|S[Wx, wk+1])
if pcc < (cid:15) then

Wx ← Wx ∪ {wk+1}

7:
8:
9:
10:
11:
12:
13:
14:
15: until k = d
16: return Wx

end if
k ← k + 1

To ﬁnish the derivation of the stepwise algorithm, we
must ensure that each new component is orthogonal to the
previously-discovered components. Consider the matrix
Cx = [w1, w2, . . . , wk]. To ensure that the new component
wk+1 will be orthogonal to the k previously-discovered
vectors, we restrict wk to lie in the null space of Cx. We
compute an orthonormal basis N for this null space. The ma-
trix C = [Cx N ] is then an orthonormal basis for Rd. Since
we want wk+1 to be orthogonal to the components in Cx, it
must have the form [Cx N ] · [01×k w](cid:62) = C · [01×k w](cid:62) =
N w(cid:62), where w ∈ R1×(d−k). The unit norm constraint
(cid:107)C · [01×k w](cid:62)(cid:107)2 = 1 is satisﬁed if and only if (cid:107)w(cid:107)2 = 1.

Algorithm 2 incorporates the aforementioned observations.
Line 5 computes an orthonormal basis for the null space
N of all components Cx that have been discovered so far.
Lines 6-8 compute the component in N that minimizes the
PCC and add it to Cx. Line 9 computes the endogenous
space E assuming the new component was added to the
current exogenous projection Wx. In Lines 10-13, if the
PCC, conditioned on both the endogenous state and the
action, is lower than (cid:15), then we add the newly discovered
component to Wx. The algorithm terminates when the entire
state space of dimensionality d has been decomposed.

As in the global algorithm, Line 6 involves a manifold opti-
mization problem; however, the Stiefel manifold correspond-
ing to the constraint w(cid:62)w = 1 is just the unit sphere, which
has a simpler form than the general manifold in formulation
(12) in the Global algorithm 1. Note that the variance of the
exogenous state can only increase as we add components
to Wx. The stepwise scheme has the added beneﬁt that it
can terminate early, for instance once it has discovered a
sufﬁcient number of components or once it has exceeded a
certain time threshold.

5. Experimental Studies

We report three experiments. (Several additional experi-
ments are described in the Supplementary Materials.) In
each experiment, we compare Q Learning (Watkins, 1989)
on the full MDP (“Full MDP”) to Q Learning on the de-
composed MDPs discovered by the Global (“Endo MDP
Global”) and Stepwise (“Endo MDP Stepwise”) algorithms.
Where possible, we also compare the performance of Q
Learning on the true endogenous MDP (“Endo MDP Ora-
cle”). The Q function is represented as a neural network
with a single hidden layer of 20 tanh units and a linear output
layer, except for Problem 1 where 2 layers of 40 units each
are used. Q-learning updates are implemented with stochas-
tic gradient descent. Exploration is achieved via Boltzmann
exploration with temperature parameter β. Given the current
Q values, the action at is selected according to

at ∼ π(a|st) =

exp(Q(st, a)/β)
i exp(Q(st, ai)/β)

.

(cid:80)

(13)

Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning

period. The state vector St consists of ﬁve features: (1) the
number of active users during the observation period; (2)
the average number of users registered with the network
during the period; (3) the channel quality index; (4) the ratio
of small packets to total packets; and (5) the ratio of total
bytes in small packets to total bytes from all packets. The
discount factor is λ = 0.95. The overall reward depends
on the number of active users, and this in turn is driven by
exogenous factors that are not controlled by the policy. This
observation motivated this research.

We obtained 5 days of hourly data collected from a cell
network with 105 cells. To create a simulator, we applied
Model-Free Monte Carlo (Fonteneau et al., 2012) to synthe-
size new trajectories by stitching together segments from
the historical trajectories. We applied Q learning to the full
MDP and to the endogenous MDP computed by both of our
methods. Instead of measuring instantaneous performance,
which is very noisy, we captured the Q function Qτ at each
iteration τ and ran independent evaluation trials starting in
10 selected states {s1, . . . , s10}. Trial i always starts in state
si, applies the policy for H = log 0.01
log λ steps, and returns
i (si) = (cid:80)H
the cumulative discounted reward V H
j=0 λjRij.
The estimated value of the policy πτ (deﬁned by Qτ ) is
ˆV πτ = 1/10 (cid:80)10

i=1 V H

i (si).

Figure 1 presents the results. We observe that the policies
learned from the Global and Stepwise Endo MDPs converge
in 165 steps. At that point, the full MDP has only achieved
73% of the improvement achieved by the Endo MDPs. An
additional 90 steps (54%) are required for the full MDP to
obtain the remaining 27% points.

5.2. Problem 2: Two-dimensional Linear System with
Anti-correlated Exogenous and Endogenous
Rewards

The second problem is designed to test how the methods
behave when the covariance condition is violated. Denote
by Xt the exogenous state and by Et the endogenous state
at time t. The state transition function is

Xt+1 = 0.9Xt + (cid:15)x; Et+1 = 0.9Et + At + 0.1Xt + (cid:15)e,

where (cid:15)x ∼ N (0, 0.16) and (cid:15)e ∼ N (0, 0.04).

The observed state vector St is a linear mixture of the hid-
den exogenous and endogenous states deﬁned as St =
M [Xt, Et](cid:62), where M =

. The reward at

(cid:21)

(cid:20)0.4
0.7

0.6
0.3

time t is Rt = Rx,t + Re,t, where Rx,t is the exoge-
nous reward Rx,t = exp[−|Xt + 3|/5] and Re,t is the
endogenous reward Re,t = exp[−|Et − 3|/5]. The opti-
mal policy is to drive Et to 3. Because the reward func-
tions are anti-correlated, the condition of Theorem 3 is
violated. The variance Var[Bx] = 0.235, which is less than
−2Cov[Bx, Be] = 0.388.

Figure 1. Performance of Q Learning applied to the Full MDP and
to the Endogenous MDP as estimated by the Global and Step-
wise algorithms on the wireless network task (N = 20, T = 30).
Performance is % improvement over the performance of the con-
ﬁguration recommended by the manufacturer (shown as 0%).

In each experiment, all Q learners observe the entire current
state st, but the full Q learner is trained on the full reward,
while the endogenous reward Q learners are trained on the
(estimated) endogenous reward. All learners are initialized
identically and employ the same random seed. For the ﬁrst L
steps, the full reward is employed, and we collect a database
of (s, a, s(cid:48), r) transitions. We then apply Algorithm 1 and
Algorithm 2 to this database to estimate Wx and We. The
algorithms then ﬁt a linear regression model Rexo(W (cid:62)
x s) to
predict the reward r as a function of the exogenous state x =
W (cid:62)
x s. The endogenous reward is deﬁned as the residuals
of this model: Rend(s) = r − Rexo(x). The endogenous Q
learner then employs this endogenous reward for steps L + 1
onward. Each experiment is repeated N times. A learning
curve is created by plotting one value every T steps, which
consists of the mean of N × T immediate rewards, along
with a 95% conﬁdence interval for that mean.

In all Q Learners, we set the discount factor to be 0.9. The
learning rates are set to 0.001 for Problem 1, and 0.02 for
Problem 2, and 0.05 for Problem 3. The temperature of
Boltzmann exploration is set to 5.0 for Problem 1 and 1.0
for Problems 2 and 3. We employ steepest descent solving
in Manopt. For the PCC constraint, (cid:15) is set to 0.05.

5.1. Problem 1: Wireless Network Parameter

Conﬁguration

We begin by applying our developed algorithm to the wire-
less network problem described in Section 1. The parameter
at to be conﬁgured in the time period [t, t+∆t] is the thresh-
old of monitored signal power that determines when a user
can switch to other frequency bands with stronger signals.
The reward is Rt = −Pt, where Pt is the percentage of
the users that suffer from low bandwidth during the time

Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning

endogenous variables. The state transition function is

Xt+1 = Mx · Xt + Ex; Et+1 = Me ·

 + Ee,







Et
Xt
At

where Mx ∈ R15×15 is the transition function for the ex-
ogenous MRP; Me ∈ R15×31 is the transition function
for the endogenous MDP and involves all Et, Xt, and At;
Ex ∈ R15×1 is the exogenous noise, whose elements are
distributed according to N (0, 0.09); and Ee ∈ R15×1 is the
endogenous noise, whose elements are distributed according
to N (0, 0.04). The observed state vector St is a linear mix-
ture of the hidden exogenous and endogenous states deﬁned
(cid:21)
, where M ∈ R30×30. The elements in

as St = M ·

(cid:20)Et
Xt

Mx, Me, and M are generated according to N (0, 1) and
then each row of each matrix is normalized to sum to 0.99
for stability. The starting state is the zero vector. The reward
at time t is Rt = Rx,t + Re,t, where Rx,t is the exogenous
reward Rx,t = −3 · avg(Xt) and Re,t is the endogenous
reward Re,t = exp[−|avg(Et) − 1|], where avg denotes the
average over a vector’s elements.

Figure 3 shows the true endogenous reward attained by
four Q learning conﬁgurations. For the ﬁrst 1000 steps, all
methods are optimizing the full reward (not shown) which
causes the endogenous reward to drop. Then the Endo-Exo
decompositions are computed, and performance begins to
improve. The Global and Stepwise methods substantially
out-perform Q learning on the Full MDP. Endo Global is
the quickest learner and even beats Endo Oracle, hence it
may have discovered additional exogenous state.

We must note that in Problems 2 and 3, Q learning needs
to run much longer to discover the optimal policy (0.97 in
Problem 2 and 0.92 in Problem 3).

6. Concluding Remarks

This paper developed a theory of exogenous-state MDPs.
It showed that if the reward decomposes additively, then
the MDP can be decomposed into an exogenous Markov
reward process and an endogenous MDP such that any op-
timal policy for the endogenous MDP is an optimal policy
for the original MDP. We derived a covariance criterion that
elucidates the conditions under which solving the endoge-
nous MDP can be expected to be faster than solving the full
MDP. We then presented two practical algorithms (Global
and Stepwise) for decomposing an MDP based on observed
trajectories. Experiments on synthetic problems and on a
cellular network management problem conﬁrmed that these
algorithms can signiﬁcantly speed up Q learning. An im-
portant open question is how best to explore the MDP for
purposes of learning the endo-exo decomposition.

Figure 2. Comparison of Q Learning applied to the Full MDP, to
the Endogenous MDP, and to Endogenous Oracle on a 2-d linear
MDP that fails the covariance condition of Theorem 3 (N =
200, T = 100).

Figure 3. Comparison of methods on the 30-dimensional linear
system, N = 1500, T = 1.

Our theory predicts that the Endo-Q learners will learn more
slowly than Q learning on the full MDP. The results (see
Figure 2) provide partial support for this. As expected, the
Oracle Endo MDP exhibits slower learning. But we were
surprised to see that the Stepwise and Global methods dis-
covered additional exogenous state that we did not realize
was present. This reduced the negative correlation and al-
lowed them to match the performance of Q learning on the
full MDP. Hence, this experiment both conﬁrms our theory
and also shows that there can be “hidden” exogenous state
that can be exploited to speed up learning.

5.3. Problem 3: High dimensional linear system

The ﬁnal experiment tests how well our algorithms can
handle high-dimensional problems. We designed a 30-
dimensional state MDP with 15 exogenous and 15 endoge-
nous state variables. Let Xt = [X1,t, . . . , X15,t](cid:62) be the
exogenous variables and Et = [E1,t, . . . , E15,t](cid:62) be the

Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning

Sobel, M. J. The Variance of Discounted Markov Decision
Processes. Journal of Applied Probability, 19(4):394–
802, 1982.

Stiefel, E. Richtungsfelder und fernparallelismus in n-
dimensionalen mannigfaltigkeiten. Commentarii Mathe-
matici Helvetici, 8(1):305–353, 1935.

Sutton, R. S. and Barto, A. G. Introduction to Reinforcement
Learning. MIT Press, Cambridge, MA, USA, 1st edition,
1998. ISBN 0262193981.

Watkins, C. J. C. H. Learning from delayed rewards. PhD

thesis, King’s College, Cambridge, 1989.

Acknowledgments

Dietterich was supported by a gift from Huawei, Inc.

References

Absil, P.-A., Mahony, R., and Sepulchre, R. Optimization
Algorithms on Matrix Manifolds. Princeton University
Press, Princeton, NJ, USA, 2007. ISBN 0691132984,
9780691132983.

Baba, K., Shibata, R., and Sibuya, M. Partial correlation
and conditional correlation as measures of conditional
independence. Australian & New Zealand Journal of
Statistics, 46(4):657–664, 2004.

Boumal, N., Mishra, B., Absil, P.-A., and Sepulchre, R.
Manopt, a matlab toolbox for optimization on manifolds.
J. Mach. Learn. Res., 15(1):1455–1459, January 2014.

Bray, R. Markov Decision Processes with Exogenous Vari-

ables. Working Paper, pp. 1–17, 2017.

Edelman, A., Arias, T. A., and Smith, S. T. The geometry
of algorithms with orthogonality constraints. SIAM J.
Matrix Anal. Appl., 20(2):303–353, 1999.

Fonteneau, R., Murphy, S. A., Wehenkel, L., and Ernst,
D. Batch mode reinforcement learning based on the
synthesis of artiﬁcial trajectories. Annals of Opera-
tions Research, 208(1):383–416, 2012. doi: 10.1007/
s10479-012-1248-5.

Fukumizu, K., Bach, F. R., and Jordan, M. I. Dimensionality
reduction for supervised learning with reproducing kernel
hilbert spaces. Journal of Machine Learning Research, 5
(Jan):73–99, 2004.

Fukumizu, K., Gretton, A., Sun, X., and Sch¨olkopf, B. Ker-
nel measures of conditional dependence. In Advances
in neural information processing systems, pp. 489–496,
2008.

Jiang, B. and Dai, Y.-H. A framework of constraint preserv-
ing update schemes for optimization on stiefel manifold.
Math. Program., 153(2):535–575, 2015.

Jolliffe, I. Principal component analysis. Springer Verlag,

New York, 2002.

McGregor, S., Houtman, R., Montgomery, C., Metoyer,
R., and Dietterich, T. G. Factoring Exogenous State for
Model-Free Monte Carlo. arXiv 1703.09390, 2017.

Puterman, M. L. Markov Decision Processes: Discrete
Stochastic Dynamic Programming. John Wiley & Sons,
Inc., New York, NY, USA, 1st edition, 1994.
ISBN
0471619779.

Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning

Figure 4. A trafﬁc network. A car starts in s0 and seeks to reach
sg by the quickest path. The time to traverse each edge under ideal
conditions is shown. Exogenous trafﬁc can add delays to these
times.

Figure 5. Comparison of Q Learning applied to the Full MDP and
to the Endogenous MDP for the trafﬁc network problem (N =
200, T = 400).

A. Supplementary Materials

We experimented with several additional test problems that
did not ﬁt into the main body of the paper.

reward and that they are able to match the performance of
the oracle.

A.2. Problem 2: Linear system with 2-d exogenous

A.1. Problem 1: Route Planning with Trafﬁc

state

Consider training a self-driving car to minimize the time
it takes to get to the ofﬁce every morning. It is natural
to reward the car for minimizing the travel time, but the
primary factor determining the reward is the exogenous
trafﬁc of all the other drivers on the road. This is similar in
many ways to the cellular network management problem.
The purpose of this ﬁrst problem is to see if our methods
work on a simple version of this problem. Figure 4 shows
a road network MDP. The endogenous states are the nodes
of the network. The exogenous state Xt is the level of
trafﬁc in the network. It evolves according to the linear
system Xt+1 = 0.9Xt + (cid:15), where (cid:15) ∼ N (0, 1). The reward
function is:

Let X1,t and X2,t be the exogenous state variables and Et
be the endogenous state. The state transition function is
deﬁned as

X1,t+1 = 0.9X1,t + (cid:15)1,
X2,t+1 = 0.7X2,t + (cid:15)2,
Et+1 = 0.4Et + At + 0.1X1,t + 0.1X2,t + (cid:15)3,

(14)

where (cid:15)1 ∼ N (0, 0.16) and (cid:15)2 ∼ N (0, 0.04) and (cid:15)3 ∼
N (0, 0.04).

The observed state vector St
is a linear mixture of
the hidden exogenous and endogenous states St =
M [X1,t, X2,t, Et](cid:62),where

rt =

1
cost(st → st+1)

+ Xt.

The actions at each node consist of choosing one of the
outbound edges to traverse. To make the task easier, we
restrict the set of actions to move only rightward (i.e., toward
states with higher subscripts) except that sg can return to s0.
The cost of traversing an edge is shown by the weights in
Figure 4. For example, if the agent moves from s0 to s4,
the cost(s0 → s4) = 3.

The Q function is represented as a neural network with a
1-hot encoding of the 9 states plus a tenth input unit for
Xt and an eleventh input unit for At. The PCC threshold
(cid:15) = 0.05.

M =





0.7
0.6,
0.3,
0.3, −0.7, 0.2
0.2
0.3,
0.6,





The reward at time t is deﬁned as Rt = Rx,t + Re,t, where
Rx,t is the exogenous reward Rx,t = −X1,t − X2,t and
Re,t is the endogenous reward Re,t == exp[|Et − 3|/4].
Figure 7 shows that with the exception of a few extreme
states, the learned Wx successfully reconstructs the values
of X1 and X2.

A.3. Problem 3: 5-d linear system with 3-d exogenous

state

Figure 5 conﬁrms that both Endo-Q learners are able to learn
much faster than the Q-learner that is given the full MDP

Let X1,t, X2,t, X3,t be the exogenous state variables and
E1,t, E2,t be the endogenous state variables. The state

Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning

Figure 6. Comparison of Q Learning applied to the Full MDP and
to the Endogenous MDP for a 3-d linear system with two coupled
exogenous dimensions

Figure 7. Comparison of Q Learning applied to the Full MDP and
to the Endogenous MDP for a 3-d linear system with two coupled
exogenous dimensions

the estimated endogenous reward after applying either the
stepwise or the global optimization methods to estimate Wx,
and “full MDP” is trained on the original MDP. Q learning
on “full MDP” is very slow, whereas both “endo stepwise”
and “endo global” are able to learn nearly as quickly as
“endo oracle”. There is no apparent difference between the
two optimization methods.

transition function is deﬁned as:

X1,t+1 =

X1,t +

X2,t +

X3,t + (cid:15)1,

X2,t+1 = 7/15X2,t +

X3,t +

X1,t + (cid:15)2,

X3,t+1 =

X3,t +

X1,t +

X2,t + (cid:15)3,

3
10

7
30

7
30

3
5

8
15
13
20
13
20

9
50

7
50

8
50
13
40
13
40

E1,t+1 =

E1,t +

E2,t + At + 0.1X1,t + 0.1X2,t + (cid:15)4,

E2,t+1 =

E2,t +

E1,t + At + 0.1X2,t + 0.1X3,t + (cid:15)5,

where (cid:15)1 ∼ N (0, 0.16), (cid:15)2 ∼ N (0, 0.04), (cid:15)3 ∼
N (0, 0.09), (cid:15)4 ∼ N (0, 0.04), and (cid:15)5 ∼ N (0, 0.04).

The observed state vector St is a linear mixture of the hidden
exogenous and endogenous states:

St =









0.6
0.3
0.3
0.3
0.6 −0.7
0.7
0.2 −0.8
0.2
0.4 −0.2 −0.1 −0.2
0.3 −0.2
0.9

0.2 −0.4
0.5 −0.3
0.6
0.9
0.7 −0.2









·

















X3,t
X2,t
X1,t
E2,t
E1,t

.

The reward at time t is deﬁned as Rt = Rx,t + Re,t,
where Rx,t is the exogenous reward Rx,t = −1.4X1,t −
1.7X2,t − 1.8X3,t and Re,t is the endogenous reward
Re,t = exp[− |E1,t+1.5E2.t−1|
]. The action At can take
5
the discrete values {−1.0, −0.9, . . . , 0.9, 1.0}.

The PCC threshold was set to 0.1 for this problem.

Figure 8 shows the performance of four Q Learning algo-
rithms: “endo oracle” is trained on the true endogenous
reward, “endo stepwise” and “endo global” are trained on

Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning

Figure 8. Comparison of Q Learning applied to the Full MDP and
to the Endogenous MDP for a 5-d linear system with three coupled
exogenous dimensions (T = 50, N = 1000).

