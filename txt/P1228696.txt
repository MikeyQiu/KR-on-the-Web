Structure-Infused Copy Mechanisms for Abstractive Summarization

Kaiqiang Song
Computer Science Dept.
University of Central Florida
Orlando, FL 32816, USA
kqsong@knights.ucf.edu

Lin Zhao
Research and Tech. Center
Robert Bosch LLC
Sunnyvale, CA 94085, USA
lin.zhao@us.bosch.com

Fei Liu
Computer Science Dept.
University of Central Florida
Orlando, FL 32816, USA
feiliu@cs.ucf.edu

Abstract

Seq2seq learning has produced promising results on summarization. However, in many cases,
system summaries still struggle to keep the meaning of the original intact. They may miss out im-
portant words or relations that play critical roles in the syntactic structure of source sentences. In
this paper, we present structure-infused copy mechanisms to facilitate copying important words
and relations from the source sentence to summary sentence. The approach naturally combines
source dependency structure with the copy mechanism of an abstractive sentence summarizer.
Experimental results demonstrate the effectiveness of incorporating source-side syntactic infor-
mation in the system, and our proposed approach compares favorably to state-of-the-art methods.

1

Introduction

Recent years have witnessed increasing interest in abstractive summarization. The systems seek to con-
dense source texts to summaries that are concise, grammatical, and preserve the important meaning of
the original texts (Nenkova and McKeown, 2011). The task encompasses a number of high-level text
operations, e.g., paraphrasing, generalization, text reduction and reordering (Jing and McKeown, 1999),
posing a considerable challenge to natural language understanding.

Src A Mozambican man suspect of murdering Jorge Microsse, director of Maputo central prison,

has escaped from the city’s police headquarters, local media reported on Tuesday.

Ref Mozambican suspected of killing Maputo prison director escapes
Sys mozambican man arrested for murder
Src An Alaska father who was too drunk to drive had his 11-year-old son take the wheel, authorities said.
Ref Drunk Alaska dad has 11 year old drive home
Sys
alaska father who was too drunk to drive

Table 1: Example source sentences, reference and system summaries produced by a neural attentive seq-to-seq model. System
summaries fail to preserve summary-worthy content of the source (e.g., main verbs) despite their syntactic importance.

The sequence-to-sequence learning paradigm has achieved remarkable success on abstractive sum-
marization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017). While the
results are impressive, individual system summaries can appear unreliable and fail to preserve the mean-
ing of the source texts. Table 1 presents two examples. In these cases, the syntactic structure of source
sentences is relatively rare but perfectly normal. The ﬁrst sentence contains two appositional phrases
(“suspect of murdering Jorge Microsse,” “director of Maputo central prison”) and the second sentence
has a relative clause (“who was too drunk to drive”), both located between the subject and the main verb.
The system, however, fails to identify the main verb in both cases; it instead chooses to focus on the
ﬁrst few words of the source sentences. We observe that rare syntactic constructions of the source can
pose problems for neural summarization systems, possibly for two reasons. First, similar to rare words,
certain syntactic constructions do not occur frequently enough in the training data to allow the system
to learn the patterns. Second, neural summarization systems are not explicitly informed of the syntactic
structure of the source sentences and they tend to bias towards sequential recency.

This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/.

8
1
0
2
 
n
u
J
 
4
2
 
 
]
L
C
.
s
c
[
 
 
2
v
8
5
6
5
0
.
6
0
8
1
:
v
i
X
r
a

Figure 1: An example dependency parse tree created for the source sentence in Table 1. If important dependency edges such as
“father ← had” can be preserved in the summary, the system summary is likely to preserve the meaning of the original.

In this paper we seek to address this problem by incorporating source syntactic structure in neural
sentence summarization to help the system identify summary-worthy content and compose summaries
that preserve the important meaning of the source texts. We present structure-infused copy mechanisms
to facilitate copying source words and relations to the summary based on their semantic and structural
importance in the source sentences. For example, if important parts of the source syntactic structure,
“had,” shown in Figure 1),
such as a dependency edge from the main verb to the subject (“father”
can be preserved in the summary, the “missing verb” issue in Table 1 can be effectively alleviated. Our
model therefore learns to recognize important source words and source dependency relations and strives
to preserve them in the summaries. Our research contributions include the following:

←

•

•

•

we introduce novel neural architectures that encourage salient source words/relations to be preserved
in summaries. The framework naturally combines the dependency parse tree structure with the copy
mechanism of an abstractive summarization system. To the best of our knowledge, this is the ﬁrst
attempt at comparing various neural architectures for this purpose;

we study the effectiveness of several important components, including the vocabulary size, a coverage-
based regularizer (See et al., 2017), and a beam search with reference mechanism (Tan et al., 2017);

through extensive experiments we demonstrate that incorporating syntactic information in neural sen-
tence summarization is effective. Our approach surpasses state-of-the-art published systems on the
benchmark dataset.1

2 Related Work

Prior to the deep learning era, sentence syntactic structure has been utilized to generate summaries with
an “extract-and-compress” framework. Compressed summaries are generated using a joint model to
extract sentences and drop non-important syntactic constituents (Daume III and Marcu, 2002; Berg-
Kirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al., 2016), or a pipeline approach that
combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al.,
2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androut-
sopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is
helpful for summarization, there has been little prior work investigating how best to combine sentence
syntactic structure with the neural abstractive summarization systems.

Existing neural summarization systems handle syntactic structure only implicitly (Kikuchi et al., 2016;
Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017; Paulus et al., 2017). Most systems adopt a “cut-and-
stitch” scheme that picks words either from the vocabulary or the source text and stitch them together
using a recurrent language model. However, there lacks a mechanism to ensure structurally salient words
and relations in source sentences are preserved in the summaries. The resulting summary sentences can
contain misleading information (e.g., “mozambican man arrested for murder” ﬂips the meaning of the
original) or grammatical errors (e.g., verbless, as in “alaska father who was too drunk to drive”).

Natural language generation (NLG)-based abstractive summarization (Carenini and Cheung, 2008;
Gerani et al., 2014; Fabbrizio et al., 2014; Liu et al., 2015; Takase et al., 2016) also makes extensive
use of structural information, including syntactic/semantic parse trees, discourse structures, and domain-
speciﬁc templates built using a text planner or an OpenIE system (Pighin et al., 2014). In particular, Cao
et al. (2018) leverage OpenIE and dependency parsing to extract fact tuples from the source text and use
those to improve the faithfulness of summaries.

1We made our system publicly available at: https://github.com/KaiQiangSong/struct_infused_summ

Different from the above approaches, this paper seeks to directly incorporate source-side syntactic
structure in the copy mechanism of an abstractive sentence summarization system. It learns to recognize
important source words and relations during training, while striving to preserve them in the summaries at
test time to aid reproduction of factual details. Our intent of incorporating source syntax in summariza-
tion is different from that of neural machine translation (NMT) (Li et al., 2017a; Chen et al., 2017), in
part because NMT does not handle the information loss from source to target. In contrast, a summariza-
tion system must selectively preserve source content to render concise and grammatical summaries. We
speciﬁcally focus on sentence summarization, where the goal is to reduce the ﬁrst sentence of an article
to a title-like summary. We believe even for this reasonably simple task there remains issues unsolved.

3 Our Approach

We seek to transform a source sentence x to a summary sentence y that is concise, grammatical, and
preserves the meaning of the source sentence. A source word is replaced by its Glove embedding (Pen-
nington et al., 2014) before it is fed to the system; the vector is denoted by xi (i
[S]; ‘S’ for source).
[T ]; ‘T’ for target). If a word does not appear in the
Similarly, a summary word is denoted by yt (t
∈
’ token. We begin this section by describing the basic
input vocabulary, it is replaced by a special ‘
unk
(cid:105)
(cid:104)
summarization framework, followed by our new copy mechanisms used to encourage source words and
dependency relations to be preserved in the summary.

∈

3.1 The Basic Framework

We build an encoder-decoder architecture for this work. An encoder condenses the entire source text to
a continuous vector; it also learns a vector representation for each unit of the source text (e.g., words as
units). In this work we use a two-layer stacked bi-directional Long Short-Term Memory (Hochreiter and
Schmidhuber, 1997) networks as the encoder, where the input to the second layer is the concatenation of
hidden states from the forward and backward passes of the ﬁrst layer. We obtain the hidden states of the
second layer; they are denoted by he
i and
passing the vector through a feedforward layer with tanh activation to convert from the encoder hidden
states to an initial decoder hidden state (hd

i . The source text vector is constructed by averaging over all he

0). This process is illustrated in Eq. (2).

i−1, xi) hd
S

i = fe(he
he
0 = tanh(Wh0 1
hd
S

i + bh0)
he

t = fd(hd

t−1, yt−1)

i=1
(cid:80)
A decoder unrolls the summary by predicting one word at a time. During training, the decoder takes
as input the embeddings of ground truth summary words, denoted by yt, while at test time yt are embed-
dings of system predicted summary words (i.e., teacher forcing). We implement an LSTM decoder with
the attention mechanism. A context vector ct is used to encode the source words that the system attends
] denotes the concatena-
to for generating the next summary word. It is deﬁned in Eqs (3-5), where [
tion of two vectors. The α matrix measures the strength of interaction between the decoder hidden states
hd
t are con-
t }
{
hd
catenated and used as input to build a new vector
t is a surrogate for semantic meanings
carried at time step t of the decoder. It is subsequently used to compute a probability distribution over
(cid:101)
the output vocabulary (Eq. (7)).

. To predict the next word, the context vector ct and hd
hd

and encoder hidden states

t (Eq. (6)).

he
i }

·||·

(cid:101)

{

et,i = v(cid:62) tanh(We[hd
t ||

i ] + be)
he

αt,i =

exp(et,i)
S
i(cid:48)=1 exp(et,i(cid:48))

S
i=1 αt,ihe
ct =
(cid:80)
i
t = tanh(Wh[hd
hd
t ||
Pvocab(w) = softmax(Wy
(cid:101)

(cid:80)

ct] + bh)
t + by)
hd

(cid:101)

(1)

(2)

(3)

(4)

(5)

(6)

(7)

Figure 2: System architectures for ‘Struct+Input’ (left) and ‘Struct+Hidden’ (right). A critical question we seek to answer is
whether the structural embeddings (se
i ) should be supplied as input to the encoder (left) or be exempted from encoding and
directly concatenated with the encoder hidden states (right).

The copy mechanism (Gulcehre et al., 2016; See et al., 2017) allows words in the source sequence to
be selectively copied to the target sequence. It expands the search space for summary words to include
both the output vocabulary and the source text. The copy mechanism can effectively reduce out-of-
vocabulary tokens in the generated text, potentially aiding a number of applications such as MT (Luong
et al., 2015b) and text summarization (Gu et al., 2016; Cheng and Lapata, 2016; Zeng et al., 2017).

Our copy mechanism employs a ‘switch’ to estimate the likelihood of generating a word from the
vocabulary (pgen) vs. copying it from the source text (1
pgen). The basic model is similar to that of the
pointer-generator networks (See et al., 2017). The switch is a feedforward layer with sigmoid activation
(Eq. (8)). At time step t, its input is a concatenation of the decoder hidden state hd
t , context vector ct,
and the embedding of the previously generated word yt−1. For predicting the next word, we combine
the generation and copy probabilities, shown in Eq. (9). If a word w appears once or more in the input
i:wi=w αt,i) is the sum of the attention weights over all its occurrences. If w
text, its copy probability (
appears in both the vocabulary and source text, P (w) is a weighted sum of the two probabilities.

−

(cid:80)

pgen=σ(Wz[hd
t ||

ct

yt−1])+bz)
||

P (w)=pgenPvocab(w)+(1

pgen)

−

αt,i

i:wi=w
(cid:88)

(8)

(9)

3.2 Structure-Infused Copy Mechanisms

The aforementioned copy mechanism attends to source words based on their “semantic” importance en-
, which measures the semantic relatedness of the encoder hidden state he
i and the decoder
coded in
αt,i
}
{
hidden state hd
t (Eq. (4)). However, the source syntactic structure is ignored. This is problematic, because
it hurts the system’s ability to effectively identify summary-worthy source words that are syntactically
important. We next propose three strategies to inject source syntactic structure to the copy mechanism.

3.2.1 Shallow Combination

Inspired by compressive summarization via
structured prediction (Berg-Kirkpatrick et al.,
2011; Almeida and Martins, 2013), we hypoth-
esize that structural labels, such as the incoming
dependency arc and the depth in a dependency
parse tree, can be helpful to predict word impor-
tance. We consider six categories of structural
labels in this work; they are presented in Table 2.
Each structural label is mapped to a ﬁxed-length,
trainable structural embedding. However, a crit-
ical question remains as to where the structural
embeddings should be injected in the existing neural architecture. This problem has not yet been sys-
tematically investigated. In this work, we compare two settings:

Structural info
(1) depth in the dependency parse tree
(2) label of the incoming edge
(3) number of outgoing edges
(4) part-of-speech tag
(5) absolution position in the source text
(6) relative position in the source text

Table 2: Six categories of structural labels. Example labels are
generated for word ‘had’ in Figure 1. Relative word positions
are discretized into ten buckets.

Example
0
‘root’
3
‘VBD’
9
(0.5, 0.6]

•

Struct+Input concatenates structural embeddings of position i (ﬂattened into one vector se
i ) with
se
i ];
the source word embedding xi and uses them as a new form of input to the encoder: xi
[xi
||

⇒

Figure 3: System architectures for ‘Struct+2Way+Word’ (left) and ‘Struct+2Way+Relation’ (right). βt,i (left) measures the
structural importance of the i-th source word; βt,i (right) measures the saliency of the dependency edge pointing to the i-th
source word. ge
p,i is the structural embedding of the parent. In both cases δt,i replaces αt,i to become the new attention value
used to estimate the context vector ct.

•

Struct+Hidden concatenates structural embeddings of position i (ﬂattened) with the encoder hid-
den state he

i and uses them as a new form of hidden states: he

se
i ].

i ⇒

[he
i ||

The architectural difference is illustrated in Figure 2. Structural embeddings are important comple-
ments to existing neural architectures. However, it is unclear whether they should be supplied as input
to the encoder or be left out of the encoding process and directly concatenated with the encoder hidden
states. This is a critical question we seek to answer by comparing the two settings. Note that an alter-
native setting is to separately encode words and structural labels using two RNN encoders, we consider
this as a subproblem of the “Struct+Input” case.

The above models complement state-of-the-art by combining semantic and structural signals to deter-
mine summary-worthy content. Intuitively, a source word is copied to the summary for two reasons: it
contains salient semantic content, or it serves a critical syntactic role in the source sentence. Without
explicitly modeling the two factors, ‘semantics’ can outweigh ‘structure,’ resulting in summaries that
fail to keep the original meaning intact. In the following we propose a two-way mechanism to separately
model the “semantic” and “structural” importance of source words.

3.2.2

2-Way Combination (+Word)

i = [se
i ||
We deﬁne δt,i

Our new architecture involves two attention matrices that are parallel to each other, denoted by α and β.
αt,i is deﬁned as previously in Eq. (3-4). It represents the “semantic” aspect, calculated as the strength
of interaction between the encoder hidden state he
t . In contrast, βt,i
measures the “structural” importance of the i-th input word to generating the t-th output word, calculated
by comparing the structure-enhanced embedding ge
t (Eq. (10-11)). We
use ge

xi] as a primitive (unencoded) representation of the i-th source word.

i and the decoder hidden state hd

i with the decoder hidden state hd

∝

αt,i + (cid:15)βt,i as a weighted sum of αt,i and βt,i, where a trainable coefﬁcient (cid:15) is
introduced to balance the contribution from both sides (Eq. (12)). Merging semantic and structural
salience at this stage allows us to acquire an accurate estimate of how important the i-th source word
is to predicting the t-th output word. δt,i replaces αt,i to become the new attention value. It is used to
calculate the context vector ct (Eq. (13)). A reliable estimate of ct is crucial as it is used to estimate the
generation probability over the vocabulary (Pvocab(w), Eq. (6-7)), the switch value (pgen, Eq. (8)), and
ultimately used to predict the next word (P (w), Eq. (9)).

ft,i = u(cid:62) tanh(Wf [ge
i ||

t ] + bf )
hd

βt,i =

δt,i =

exp(ft,i)
S
i(cid:48)=1 exp(ft,i(cid:48))
αt,i + (cid:15)βt,i
(cid:80)
S
i(cid:48)=1(αt,i(cid:48) + (cid:15)βt,i(cid:48))

ct =

δt,ihe
i

S
(cid:80)

i=1
(cid:88)

(10)

(11)

(12)

(13)

2-Way Combination (+Relation)

3.2.3
We observe that salient source relations also play a critical role in predicting the next word. For example,
if a dependency edge (“father” nsubj
“had”) is salient and “father” is selected to be included in the
←−−
summary, it is likely that “had” will be selected next such that a salient source relation (“nsubj”) is
preserved in the summary. Because summary words tend to follow the word order of the original, we
assume selecting a source word and including it in the summary has an impact on its subsequent source
words, but not the reverse.

In this formulation we use βt,i to capture the saliency of the dependency edge pointing to the i-th

wi has its salience score saved in βt,j; and conversely, an edge wj

source word. Thus, an edge wj
→
wi has its salience score in βt,i. β is calculated in the same way as described in Eq. (10-11). However,
i with [ge
we replace ge
ge
p,i] so that a dependency edge is characterized by the embeddings of its two
i ||
endpoints (ge
p,i is the parent embedding). The architectural difference between “Struct+2Way+Word”
and “Struct+2Way+Relation” is illustrated in Figure 3.

←

←

To obtain the likelihood of wj being selected to the summary prior to time step t, we deﬁne
αt,j =
t−1
t(cid:48)=0 αt(cid:48),j that sums up the individual probabilities up to time step t-1. Assume there is a dependency
αt,jβt,i (or
wi (j<i) whose salience score is denoted by βt,i. At time step t, we calculate
edge wj
(cid:101)
→
(cid:80)
wi) as the probability of wi being selected to the summary, given that one of its
αt,jβt,j for edge wj
prior words wj (j<i) is included in the summary and there is a dependency edge connecting the two. By
summing the impact over all its previous words, we obtain the likelihood of the i-th source word being
(cid:101)
included to the summary at time step t in order to preserve salient source relations; this is denoted by
αt,i + (cid:15)γt,i as a weighted combination of semantic and structural
γt,i (Eq. (15)). Next, we deﬁne δt,i
salience (Eq. (16)). δt,i replace αt,i to become the new attention values used to estimate the context
vector ct (Eq. (13)). Finally, the calculation of generation probabilities Pvocab(w), switch value pgen,
and probabilities for predicting the next word P (w) remains the same as previously (Eq. (6-9)).

∝

(cid:101)

αt,j =

t−1
t(cid:48)=0 αt(cid:48),j

γt,i =
(cid:101)

δt,i =

if wj
if wj

wi
wi

→
←

(cid:80)
j:j<i (cid:40)
(cid:88)

αt,jβt,i
αt,jβt,j
(cid:101)
(cid:101)

αt,i + (cid:15)γt,i
S
i(cid:48)=1(αt,i(cid:48) + (cid:15)γt,i(cid:48))

(cid:80)

(14)

(15)

(16)

3.3 Learning Objective and Beam Search

We next describe our learning objective, including a coverage-based regularizer (See et al., 2017), and
a beam search with reference mechanism (Tan et al., 2017). We want to investigate the effectiveness of
these techniques on sentence summarization, which has not been explored in previous work.

Learning objective. Our training proceeds by minimizing a per-target-word cross-entropy loss function.
[0, 1] measures the interaction strength
A regularization term is applied to the α matrix. Recall that αt,i
between the t-th output word and the i-th input word. Naturally, we expect a 1-to-1 mapping between the
two words. The coverage-based regularizer, proposed by See et al., (2017), encourages this behavior by
tracking the historical attention values attributed to the i-th input word (up to time step t-1), denoted by
αt,i and αt,i, which has the practical
αt,i =
t) to be close to either 0 or 1, otherwise a penalty will be applied. The regularizer
effect of forcing αt,i (
Ω is deﬁned in Eq. (17), where M is the size of the mini-batch, S and T are the lengths of the source
(cid:101)

t−1
t(cid:48)=0 αt(cid:48),i. The approach then takes the minimum between

(cid:80)

∈

(cid:101)

∀

and target sequences. For two-way copy mechanisms, δ replaces α to become the new attention values,
we therefore apply regularization to δ instead of α. When the regularizer applies, the objective becomes
minimizing (

+ Ω).

L

Ω=λ

M

m=1
(cid:88)

1
T (m)S(m)

T (m)

S(m)

t=1
(cid:88)

i=1 (cid:16)
(cid:88)

min(

αt,i,αt,i)

(cid:17)

(cid:101)

(17)

Beam search with reference. During testing, we employ greedy search to generate system summary
sequences. For the task of summarization, the ground truth summary sequences are usually close to the
source texts. This property can be leveraged in beam search. Tan et al., (2017) describe a beam search
with reference mechanism that rewards system summaries that have a high degree of bigram overlap with
the source texts. We describe it in Eq. (18), where where
(y<t, x)
measures the number of bigrams shared by the system summary (up to time step t-1) and the source text;
adds a word w to the end of the system summary. The shorter the source text (measured by
y<t, w
{
length S), the more weight a shared bigram will add to the score of the current word w. A hyperparameter
η controls the degree of closeness between the system summary and the source text.

(w) denotes the score of word w.

B

S

}

(w)=logP (w)+η B

S

y<t ,w

(
{

,x)
}
S

−B

(y<t ,x)

(18)

4 Experiments

4.1 Data Sets

We evaluate the proposed structure-infused copy mechanisms for summarization in this section. We
describe the dataset, experimental settings, baselines, and ﬁnally, evaluation results and analysis.

We evaluate our proposed models on the Gigaword summarization dataset (Parker, 2011; Rush et al.,
2015). The task is to reduce the ﬁrst sentence of an article to a title-like summary. We obtain dependency
parse trees for source sentences using the Stanford neural network parser (Chen and Manning, 2014). We
also use the standard train/valid/test data splits. Following (Rush et al., 2015), the train and valid splits
are pruned2 to improve the data quality. Spurious pairs that are repetitive, overly long/short, and pairs
whose source and summary sequences have little word overlap are removed. No pruning is performed
for instances in the test set. The processed corpus contains 4,018K training instances. We construct
two (non-overlapped) validation sets: “valid-4096” contains 4,096 randomly sampled instances from the
valid split; it is used for hyperparameter tuning and early stopping. “valid-2000” is used for evaluation;
it allows the models to be trained and evaluated on pruned instances. Finally, we report results on the
standard Gigaword test set (Rush et al., 2015) containing 1,951 instances (“test-1951”).

4.2 Experimental Setup

∼ N

(0, σ), σ =

2
nin+nout

We use the Xavier scheme (Glorot and Bengio, 2010) for parameter initialization, where weights are
initialized using a Gaussian distribution Wi,j
; nin and nout are numbers of
the input and output units of the network; biases are set to be 0. We further implement two techniques to
accelerate mini-batch training. First, all training instances are sorted by the source sequence length and
partitioned into mini-batches. The shorter sequences are padded to have the same length as the longest
sequence in the batch. All batches are shufﬂed at the beginning of each epoch. Second, we introduce a
variable-length batch vocabulary containing only source words of the current mini-batch and words of
the output vocabulary. P (w) in Eq. (9) only needs to be calculated for words in the batch vocabulary.
It is magnitudes smaller than a direct combination of the input and output vocabularies. Finally, our
input vocabulary contains the most frequent 70K words in the source texts and summaries. The output
vocabulary contains 5K words by default. More network parameters are presented in Table 3.

(cid:113)

2https://github.com/facebookarchive/NAMAS/blob/master/dataset/filter.py

Input vocabulary size
Output vocabulary size
Dim. of word embeddings
Dim. of structural embeddings
Num. of encoder/decoder hidden units
Adam optimizer (Kingma and Ba, 2015)
Coeff. for coverage-based regularizer
Coeff. for beam search with reference
Beam size
Minibatch size
Early stopping criterion (max 20 epochs)
Gradient clipping (Pascanu et al., 2013)

70K
5K (default)
100
16
256
lr = 1e-4
λ = 1
η ≈ 13.5
K = 5
M = 64
valid. loss
g ∈ [-5, 5]

Table 3: Parameter settings of our summarization system.

System

Baseline
Struct+Input
Struct+Hidden
Struct+2Way+Word
Struct+2Way+Relation

Gigaword Valid-2000
R-L
R-2
R-1

42.48
42.44
42.88
43.21
42.83

21.34
21.75
21.81
21.84
21.85

40.18
40.46
40.63
40.86
40.60

Table 4: Results on the Gigaword valid-2000 set (full-length
F1). Models implementing the structure-infused copy mech-
anisms (“Struct+*”) outperform the baseline.

S:

the government ﬁled another round of criminal charges in a
widening stock options scandal

T: options scandal widens

another round of criminal charges in stock options scandal

B: government ﬁles more charges in stock options scandal
I:
H: charges ﬁled in stock options scandal
W: another round of criminal charges in stock options scandal
R: government ﬁles another round of criminal charges in options

scandal

Table 5: Example system summaries. ‘S:’ source; ‘T:’ target;
‘B:’ baseline; ‘I:’ Struct+Input; ‘H:’ Struct+Hidden; ‘W:’
2Way+Word; “R:” 2Way+Relation. “2Way+Relation” is able
to preserve important source relations in the summary, e.g.,
nsubj←−−− ﬁles,” “ﬁles dobj−−→ round,” and “round
“government
nmod−−−→ charges.”

S:

red cross negotiators from rivals north korea and south korea held
talks wednesday on emergency food shipments to starving north
koreans and agreed to meet again thursday

T: koreas meet in beijing to discuss food aid from south eds

north korea , south korea meet again

B: north korea , south korea agree to meet again
I:
H: north korea , south korea meet on emergency food shipments
W: north korea , south korea hold talks on food shipments
R: north korea , south korea hold talks on emergency food shipments

Table 6: Example system summaries. “Struct+Hidden” and
“2Way+Relation” successfully preserve salient source words
(“emergency food shipments”), which are missed out by
other systems. We observe that copying “hold talks” from the
source also makes the resulting summaries more informative
than using the word “meet.”

4.3 Results

ROUGE results on valid set. We ﬁrst report results on the Gigaword valid-2000 dataset in Table 4. We
present R-1, R-2, and R-L scores (Lin, 2004) that respectively measures the overlapped unigrams, bi-
grams, and longest common subsequences between the system and reference summaries3. Our baseline
system (“Baseline”) implements the seq2seq architecture with the basic copy mechanism (Eq. (1-9)). It is
a strong baseline that resembles the pointer-generator networks described in (See et al., 2017). The struc-
tural models (“Struct+*”) differ from the baseline only on the structure-infused copy mechanisms. All
models are evaluated without the coverage regularizer or beam search (
3.3) to ensure fair comparison.
§
Overall, we observe that models equipped with the structure-infused copy mechanisms are superior to
the baseline, suggesting that combining source syntactic structure with the copy mechanism is effective.
We found that the “Struct+Hidden” architecture, which directly concatenates structural embeddings with
the encoder hidden states, outperforms “Struct+Input” despite that the latter requires more parameters.
“Struct+2Way+Word” also demonstrates strong performance, achieving 43.21%, 21.84%, and 40.86%
F1 scores, for R-1, R-2, and R-L respectively.

ROUGE results on test set. We compare our proposed approach with a range of state-of-the-art neural
summarization systems. Results on the standard Gigaword test set (“test-1951”) are presented in Table 7.
Details about these systems are provided in Table 8. Overall, our proposed approach with structure-
infused pointer networks perform strongly, yielding ROUGE scores that are on-par with or surpassing
state-of-the-art published systems. Notice that the scores on the valid-2000 dataset are generally higher
than those of test-1951. This is because the (source, summary) pairs in the Gigaword test set are not
4.1). In some cases, none (or very few) of the summary words appear in the source. This
pruned (see
§
may cause difﬁculties to the systems equipped with the copy mechanism. The “Struct+2Way+Word”
architecture that respectively models the semantic and syntactic importance of source words achieves the
highest scores. It outperforms its counterpart of “Struct+2Way+Relation,” which seeks to preserve source
dependency relations in summaries. We conjecture that the imperfect dependency parse trees generated

3w/ ROUGE options: -n 2 -m -w 1.2 -c 95 -r 1000

System

ABS (Rush et al., 2015)
ABS+ (Rush et al., 2015)
Luong-NMT (Chopra et al., 2016)
RAS-LSTM (Chopra et al., 2016)
RAS-Elman (Chopra et al., 2016)
ASC+FSC1 (Miao and Blunsom, 2016)
lvt2k-1sent (Nallapati et al., 2016)
lvt5k-1sent (Nallapati et al., 2016)
Multi-Task (Pasunuru et al., 2017)
DRGD (Li et al., 2017b)

Gigaword Test-1951
R-L
R-2
R-1

29.55 11.32 26.42
29.76 11.88 26.96
33.10 14.45 30.71
32.55 14.70 30.03
33.78 15.97 31.15
34.17 15.94 31.92
32.67 15.59 30.64
35.30 16.64 32.62
32.75 15.35 30.82
36.27 17.57 33.62

35.43 17.49 33.39
Baseline (this paper)
Struct+Input (this paper)
35.32 17.50 33.25
Struct+2Way+Relation (this paper) 35.46 17.51 33.28
35.49 17.61 33.33
Struct+Hidden (this paper)
35.47 17.66 33.52
Struct+2Way+Word (this paper)

Table 7: Results on the Gigaword test-1951 set (full-
length F1). Models with structure-infused copy mechanisms
(“Struct+*”) perform well. Their R-2 F-scores are on-par
with or outperform state-of-the-art published systems.

ABS and ABS+ (Rush et al., 2015) are the ﬁrst work introduc-
ing an encoder-decoder architecture for summarization.
Luong-NMT (Chopra et al., 2016) is a re-implementation of
the attentive stacked LSTM encoder-decoder of Luong
et al. (2015a).

RAS-LSTM and RAS-Elman (Chopra et al., 2016) describe a
convolutional attentive encoder that ensures the decoder
focuses on appropriate words at each step of generation.
ASC+FSC1 (Miao and Blunsom, 2016) presents a genera-
tive auto-encoding sentence compression model jointly
trained on labelled/unlabelled data.

lvt2k-1sent and lvt5k-1sent (Nallapati et al., 2016) address
issues in the attentive encoder-decoder framework,
including modeling keywords, capturing sentence-to-
word structure, and handling rare words.

Multi-Task w/ Entailment (Pasunuru et al., 2017) combines
entailment with summarization in a multi-task setting.
DRGD (Li et al., 2017b) describes a deep recurrent generative
decoder learning latent structure of summary sequences
via variational inference.

Table 8: Existing summarization methods.

by the parser may affect the “Struct+2Way+Relation” results. However, because the Gigaword dataset
does not provide gold-standard annotations for parse trees, we could not easily verify this and will leave
it for future work. In Table 5 and 6, we present system summaries produced by various models.

System
Struct+Input
Struct+2Way+Relation
Ground-truth Summ.

Linguistic quality. To further gauge the summary
quality, we hire human workers from the Amazon
Mechanical Turk platform to rate summaries on a Lik-
ert scale of 1 to 5 according to three criteria (Zhang
and Lapata, 2017): ﬂuency (is the summary grammat-
ical and well-formed?), informativeness (to what ex-
tent is the meaning of the original sentence preserved
in the summary?), and faithfulness (is the summary
accurate and faithful to the original?). We sample 100
instances from the test set and employ 5 turkers to rate
each summary; their averaged scores are presented in Table 9. We found that “Struct+2Way+Relation”
outperforms “Struct+Input” on all three criteria. It also compares favorably to ground-truth summaries
on “ﬂuency” and “faithfulness.” On the other hand, the ground-truth summaries, corresponding to article
titles, are judged as less satisfying according to human raters.

Table 9: Informativeness, ﬂuency, and faithfulness scores
of summaries. They are rated by Amazon turkers on a
Likert scale of 1 (worst) to 5 (best). We choose to eval-
uate Struct+2Way+Relation (as oppose to 2Way+Word)
because it focuses on preserving source relations in the
summaries.

Info. Fluency Faithful.
2.9
3.0
3.2

3.0
3.1
3.1

3.3
3.4
3.5

Dependency relations. We investigate the source dependency relations preserved in the summaries in
Table 10. A source relation is considered preserved if both its words appear in the summary. We observe
that the models implementing structure-infused copy mechanisms (e.g., “Struct+2Way+Word”) are more
likely to preserve important dependency relations in the summaries, including nsubj, dobj, amod, nmod,
and nmod:poss. Dependency relations that are less important (mark, case, conj, cc, det) are less likely to
be preserved. These results show that our structure-infused copy mechanisms can learn to recognize the
importance of dependency relations and selectively preserve them in the summaries.

Coverage and reference beam. In Figure 11, we investigate the effect of applying the coverage regu-
larizer (“coverage”) and reference-based beam search (“ref beam”) (
3.3) to our models. The coverage
§
regularizer is applied in a second training stage, where the system is trained for an extra 5 epochs with
coverage and the model yielding the lowest validation loss is selected. Both coverage and ref beam
can improve the system performance. Our observation suggests that ref beam is an effective addition to
shorten the gap between different systems.

System

nsubj

dobj

amod

nmod

nmod:poss mark

case

Baseline
Struct+Input
Struct+Hidden
Struct+2Way+Word
Struct+2Way+Relation

7.23
7.03
7.78↑
7.46↑
7.35↑

12.07
11.72
12.34↑
12.69↑
12.07↑

20.45
19.72
21.11↑
20.59↑
20.59↑

8.73
9.17↑
9.18↑
9.03↑
8.68

12.46
12.46
14.86↑
13.00↑
13.47↑

15.83
15.35
14.93
15.83
15.41

14.84
14.69
15.84↑
14.43
14.39

conj

9.72
9.55
9.47
8.86
9.12

cc

5.03
4.67
3.93
3.48
4.30

det

2.22
1.97
2.65↑
1.91
1.89

Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.

|V |
R-2
13.99
1K
15.35
2K
5K
17.25
10K 17.62

Train Speed
2.5h/epoch
2.7h/epoch
3.2h/epoch
3.8h/epoch

InVcb
60.57
69.71
79.98
88.26

InVcb+Src
76.04
80.72
86.51
92.18

Table 12: Results of the “Struct+2Way+Relation” system
trained using output vocabularies of various sizes (|V |), eval-
uated on test-1951 w/o coverage or ref beam. The training
speed is calculated as the elapsed time (hours) per epoch,
tested on a GTX 1080Ti GPU card.

Table 11: Effects of applying the coverage regularizer and
the reference beam search to structural models, evaluated on
test-1951. Combining both yields the highest scores.

Output vocabulary size. Finally, we investigate the impact of the output vocabulary size on the sum-
marization performance in Table 12. All our models by default use an output vocabulary of 5K words in
order to make the results comparable to state-of-the-art-systems. However, we observe that there is a po-
tential to further boost the system performance (17.25
17.62 R-2 F1-score, w/o coverage or ref beam)
if we had chosen to use a larger vocabulary (10K) and can endure a slightly longer training time (1.2x).
In Table 12, we further report the percentages of reference summary words covered by the output vocab-
ulary (“InVcb”) and covered by either the output vocabulary or the source text (“InVcb+Src”). The gap
between the two conditions shortens as the size of the output vocabulary is increased.

→

In this paper, we investigated structure-infused copy mechanisms that combine source syntactic struc-
ture with the copy mechanism of an abstractive summarization system. We compared various system
architectures and showed that our models can effectively preserve salient source relations in summaries.
Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-the-
art published systems.

5 Conclusion

References

Miguel B. Almeida and Andre F. T. Martins. 2013. Fast and robust compressive summarization with dual decom-

position and multi-task learning. In Proceedings of ACL.

Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceed-

ings of the Annual Meeting of the Association for Computational Linguistics (ACL).

Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive

summarization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI).

Giuseppe Carenini and Jackie Chi Kit Cheung. 2008. Extractive vs. NLG-based abstractive summarization of eval-
uative text: The effect of corpus controversiality. In Proceedings of the Fifth International Natural Language
Generation Conference (INLG).

Danqi Chen and Christopher D. Manning. 2014. A fast and accurate dependency parser using neural networks. In

Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, and Hui Jiang. 2016. Distraction-based neural networks
for document summarization. In Proceedings of the Twenty-Fifth International Joint Conference on Artiﬁcial
Intelligence (IJCAI).

Huadong Chen, Shujian Huang, David Chiang, and Jiajun Chen. 2017. Improved neural machine translation with a
syntax-aware encoder and decoder. In Proceedings of the Annual Meeting of the Association for Computational
Linguistics (ACL).

Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. In Proceed-

ings of ACL.

Sumit Chopra, Michael Auli, and Alexander M. Rush. 2016. Abstractive sentence summarization with attentive

recurrent neural networks. In Proceedings of NAACL.

James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear program-

ming approach. Journal of Artiﬁcial Intelligence Research.

Hal Daume III and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of

the Annual Meeting of the Association for Computational Linguistics (ACL).

Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein. 2016. Learning-based single-document summarization
with compression and anaphoricity constraints. In Proceedings of the Association for Computational Linguistics
(ACL).

Giuseppe Di Fabbrizio, Amanda J. Stent, and Robert Gaizauskas. 2014. A hybrid approach to multi-document
summarization of opinions in reviews. Proceedings of the 8th International Natural Language Generation
Conference (INLG).

Katja Filippova, Enrique Alfonseca, Carlos Colmenares, Lukasz Kaiser, and Oriol Vinyals. 2015. Sentence com-
pression by deletion with lstms. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing (EMNLP).

Dimitrios Galanis and Ion Androutsopoulos. 2010. An extractive supervised two-stage method for sentence

compression. In Proceedings of NAACL-HLT.

Shima Gerani, Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng, and Bita Nejat. 2014. Abstractive summa-
rization of product reviews using discourse structure. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).

Xavier Glorot and Yoshua Bengio. 2010. Understanding the difﬁculty of training deep feedforward neural net-
works. In Proceedings of the 13th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS).

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016. Incorporating copying mechanism in sequence-to-

sequence learning. In Proceedings of ACL.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown
words. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL).

Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735–

1780.

Hongyan Jing and Kathleen McKeown. 1999. The decomposition of human-written summary sentences. In Pro-
ceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR).

Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya Takamura, and Manabu Okumura. 2016. Controlling

output length in neural encoder-decoders. In Proceedings of EMNLP.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of the

International Conference on Learning Representations (ICLR).

Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013. Document summarization via guided sentence compression.

In Proceedings of EMNLP.

Chen Li, Yang Liu, Fei Liu, Lin Zhao, and Fuliang Weng. 2014. Improving multi-documents summarization by

sentence compression based on expanded constituent parse tree. In Proceedings of EMNLP.

Junhui Li, Deyi Xiong, Zhaopeng Tu, Muhua Zhu, Min Zhang, and Guodong Zhou. 2017a. Modeling source syn-
tax for neural machine translation. In Proceedings of the Annual Meeting of the Association for Computational
Linguistics (ACL).

Piji Li, Wai Lam, Lidong Bing, and Zihao Wang. 2017b. Deep recurrent generative decoder for abstractive
text summarization. In Proceedings of the Conference on Empirical Methods in Natural Language Processing
(EMNLP).

Chin-Yew Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In Proceedings of ACL Work-

shop on Text Summarization Branches Out.

Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman Sadeh, and Noah A. Smith. 2015. Toward abstractive summa-
rization using semantic representations. In Proceedings of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies (NAACL).

Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015a. Effective approaches to attention-based
In Proceedings of the Conference on Empirical Methods in Natural Language

neural machine translation.
Processing (EMNLP).

Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. 2015b. Addressing the
rare word problem in neural machine translation. In Proceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL).

Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In Proceedings of

EACL.

Retrieval.

Yishu Miao and Phil Blunsom. 2016. Language as a latent variable: Discrete generative models for sentence com-
pression. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive text
summarization using sequence-to-sequence RNNs and beyond. In Proceedings of the 20th SIGNLL Conference
on Computational Natural Language Learning (CoNLL).

Ani Nenkova and Kathleen McKeown. 2011. Automatic summarization. Foundations and Trends in Information

Robert Parker. 2011. English Gigaword ﬁfth edition LDC2011T07. Philadelphia: Linguistic Data Consortium.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difﬁculty of training recurrent neural net-

works. In Proceedings of the International Conference on Machine Learning (ICML).

Ramakanth Pasunuru, Han Guo, and Mohit Bansal. 2017. Towards improving abstractive summarization via

entailment generation. In Proceedings of the Workshop on New Frontiers in Summarization.

Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summariza-
tion. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).

Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word repre-
sentation. In Proceedings of the Conference Empirical Methods in Natural Language Processing (EMNLP).

Daniele Pighin, Marco Cornolti, Enrique Alfonseca, and Katja Filippova. 2014. Modelling events through
memory-based, open-ie patterns for abstractive summarization. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).

Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for sentence summariza-
tion. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).

Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the Annual Meeting of the Association for Computational Linguistics
(ACL).

Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu Hirao, and Masaaki Nagata. 2016. Neural headline genera-
tion on abstract meaning representation. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).

Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017. Abstractive document summarization with a graph-based at-
tentional neural model. In Proceedings of the Annual Meeting of the Association for Computational Linguistics
(ACL).

Kapil Thadani and Kathleen McKeown. 2013. Sentence compression with joint structural inference. In Proceed-

ings of CoNLL.

Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Florian, and Claire Cardie. 2013. A sentence compression

based framework to query-focused multi-document summarization. In Proceedings of ACL.

David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard Schwartz. 2007. Multi-candidate reduction: Sentence

compression as a tool for document summarization tasks. Information Processing and Management.

Wenyuan Zeng, Wenjie Luo, Sanja Fidler, and Raquel Urtasun. 2017. Efﬁcient summarization with read-again
and copy mechanism. In Proceedings of the International Conference on Learning Representations (ICLR).

Xingxing Zhang and Mirella Lapata. 2017. Sentence simpliﬁcation with deep reinforcement learning. In Pro-

ceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).

Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou. 2017. Selective encoding for abstractive sentence summa-

rization. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).

Structure-Infused Copy Mechanisms for Abstractive Summarization

Kaiqiang Song
Computer Science Dept.
University of Central Florida
Orlando, FL 32816, USA
kqsong@knights.ucf.edu

Lin Zhao
Research and Tech. Center
Robert Bosch LLC
Sunnyvale, CA 94085, USA
lin.zhao@us.bosch.com

Fei Liu
Computer Science Dept.
University of Central Florida
Orlando, FL 32816, USA
feiliu@cs.ucf.edu

Abstract

Seq2seq learning has produced promising results on summarization. However, in many cases,
system summaries still struggle to keep the meaning of the original intact. They may miss out im-
portant words or relations that play critical roles in the syntactic structure of source sentences. In
this paper, we present structure-infused copy mechanisms to facilitate copying important words
and relations from the source sentence to summary sentence. The approach naturally combines
source dependency structure with the copy mechanism of an abstractive sentence summarizer.
Experimental results demonstrate the effectiveness of incorporating source-side syntactic infor-
mation in the system, and our proposed approach compares favorably to state-of-the-art methods.

1

Introduction

Recent years have witnessed increasing interest in abstractive summarization. The systems seek to con-
dense source texts to summaries that are concise, grammatical, and preserve the important meaning of
the original texts (Nenkova and McKeown, 2011). The task encompasses a number of high-level text
operations, e.g., paraphrasing, generalization, text reduction and reordering (Jing and McKeown, 1999),
posing a considerable challenge to natural language understanding.

Src A Mozambican man suspect of murdering Jorge Microsse, director of Maputo central prison,

has escaped from the city’s police headquarters, local media reported on Tuesday.

Ref Mozambican suspected of killing Maputo prison director escapes
Sys mozambican man arrested for murder
Src An Alaska father who was too drunk to drive had his 11-year-old son take the wheel, authorities said.
Ref Drunk Alaska dad has 11 year old drive home
Sys
alaska father who was too drunk to drive

Table 1: Example source sentences, reference and system summaries produced by a neural attentive seq-to-seq model. System
summaries fail to preserve summary-worthy content of the source (e.g., main verbs) despite their syntactic importance.

The sequence-to-sequence learning paradigm has achieved remarkable success on abstractive sum-
marization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017). While the
results are impressive, individual system summaries can appear unreliable and fail to preserve the mean-
ing of the source texts. Table 1 presents two examples. In these cases, the syntactic structure of source
sentences is relatively rare but perfectly normal. The ﬁrst sentence contains two appositional phrases
(“suspect of murdering Jorge Microsse,” “director of Maputo central prison”) and the second sentence
has a relative clause (“who was too drunk to drive”), both located between the subject and the main verb.
The system, however, fails to identify the main verb in both cases; it instead chooses to focus on the
ﬁrst few words of the source sentences. We observe that rare syntactic constructions of the source can
pose problems for neural summarization systems, possibly for two reasons. First, similar to rare words,
certain syntactic constructions do not occur frequently enough in the training data to allow the system
to learn the patterns. Second, neural summarization systems are not explicitly informed of the syntactic
structure of the source sentences and they tend to bias towards sequential recency.

This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/.

8
1
0
2
 
n
u
J
 
4
2
 
 
]
L
C
.
s
c
[
 
 
2
v
8
5
6
5
0
.
6
0
8
1
:
v
i
X
r
a

Figure 1: An example dependency parse tree created for the source sentence in Table 1. If important dependency edges such as
“father ← had” can be preserved in the summary, the system summary is likely to preserve the meaning of the original.

In this paper we seek to address this problem by incorporating source syntactic structure in neural
sentence summarization to help the system identify summary-worthy content and compose summaries
that preserve the important meaning of the source texts. We present structure-infused copy mechanisms
to facilitate copying source words and relations to the summary based on their semantic and structural
importance in the source sentences. For example, if important parts of the source syntactic structure,
“had,” shown in Figure 1),
such as a dependency edge from the main verb to the subject (“father”
can be preserved in the summary, the “missing verb” issue in Table 1 can be effectively alleviated. Our
model therefore learns to recognize important source words and source dependency relations and strives
to preserve them in the summaries. Our research contributions include the following:

←

•

•

•

we introduce novel neural architectures that encourage salient source words/relations to be preserved
in summaries. The framework naturally combines the dependency parse tree structure with the copy
mechanism of an abstractive summarization system. To the best of our knowledge, this is the ﬁrst
attempt at comparing various neural architectures for this purpose;

we study the effectiveness of several important components, including the vocabulary size, a coverage-
based regularizer (See et al., 2017), and a beam search with reference mechanism (Tan et al., 2017);

through extensive experiments we demonstrate that incorporating syntactic information in neural sen-
tence summarization is effective. Our approach surpasses state-of-the-art published systems on the
benchmark dataset.1

2 Related Work

Prior to the deep learning era, sentence syntactic structure has been utilized to generate summaries with
an “extract-and-compress” framework. Compressed summaries are generated using a joint model to
extract sentences and drop non-important syntactic constituents (Daume III and Marcu, 2002; Berg-
Kirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al., 2016), or a pipeline approach that
combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al.,
2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androut-
sopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is
helpful for summarization, there has been little prior work investigating how best to combine sentence
syntactic structure with the neural abstractive summarization systems.

Existing neural summarization systems handle syntactic structure only implicitly (Kikuchi et al., 2016;
Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017; Paulus et al., 2017). Most systems adopt a “cut-and-
stitch” scheme that picks words either from the vocabulary or the source text and stitch them together
using a recurrent language model. However, there lacks a mechanism to ensure structurally salient words
and relations in source sentences are preserved in the summaries. The resulting summary sentences can
contain misleading information (e.g., “mozambican man arrested for murder” ﬂips the meaning of the
original) or grammatical errors (e.g., verbless, as in “alaska father who was too drunk to drive”).

Natural language generation (NLG)-based abstractive summarization (Carenini and Cheung, 2008;
Gerani et al., 2014; Fabbrizio et al., 2014; Liu et al., 2015; Takase et al., 2016) also makes extensive
use of structural information, including syntactic/semantic parse trees, discourse structures, and domain-
speciﬁc templates built using a text planner or an OpenIE system (Pighin et al., 2014). In particular, Cao
et al. (2018) leverage OpenIE and dependency parsing to extract fact tuples from the source text and use
those to improve the faithfulness of summaries.

1We made our system publicly available at: https://github.com/KaiQiangSong/struct_infused_summ

Different from the above approaches, this paper seeks to directly incorporate source-side syntactic
structure in the copy mechanism of an abstractive sentence summarization system. It learns to recognize
important source words and relations during training, while striving to preserve them in the summaries at
test time to aid reproduction of factual details. Our intent of incorporating source syntax in summariza-
tion is different from that of neural machine translation (NMT) (Li et al., 2017a; Chen et al., 2017), in
part because NMT does not handle the information loss from source to target. In contrast, a summariza-
tion system must selectively preserve source content to render concise and grammatical summaries. We
speciﬁcally focus on sentence summarization, where the goal is to reduce the ﬁrst sentence of an article
to a title-like summary. We believe even for this reasonably simple task there remains issues unsolved.

3 Our Approach

We seek to transform a source sentence x to a summary sentence y that is concise, grammatical, and
preserves the meaning of the source sentence. A source word is replaced by its Glove embedding (Pen-
nington et al., 2014) before it is fed to the system; the vector is denoted by xi (i
[S]; ‘S’ for source).
[T ]; ‘T’ for target). If a word does not appear in the
Similarly, a summary word is denoted by yt (t
∈
’ token. We begin this section by describing the basic
input vocabulary, it is replaced by a special ‘
unk
(cid:105)
(cid:104)
summarization framework, followed by our new copy mechanisms used to encourage source words and
dependency relations to be preserved in the summary.

∈

3.1 The Basic Framework

We build an encoder-decoder architecture for this work. An encoder condenses the entire source text to
a continuous vector; it also learns a vector representation for each unit of the source text (e.g., words as
units). In this work we use a two-layer stacked bi-directional Long Short-Term Memory (Hochreiter and
Schmidhuber, 1997) networks as the encoder, where the input to the second layer is the concatenation of
hidden states from the forward and backward passes of the ﬁrst layer. We obtain the hidden states of the
second layer; they are denoted by he
i and
passing the vector through a feedforward layer with tanh activation to convert from the encoder hidden
states to an initial decoder hidden state (hd

i . The source text vector is constructed by averaging over all he

0). This process is illustrated in Eq. (2).

i−1, xi) hd
S

i = fe(he
he
0 = tanh(Wh0 1
hd
S

i + bh0)
he

t = fd(hd

t−1, yt−1)

i=1
(cid:80)
A decoder unrolls the summary by predicting one word at a time. During training, the decoder takes
as input the embeddings of ground truth summary words, denoted by yt, while at test time yt are embed-
dings of system predicted summary words (i.e., teacher forcing). We implement an LSTM decoder with
the attention mechanism. A context vector ct is used to encode the source words that the system attends
] denotes the concatena-
to for generating the next summary word. It is deﬁned in Eqs (3-5), where [
tion of two vectors. The α matrix measures the strength of interaction between the decoder hidden states
hd
t are con-
t }
{
hd
catenated and used as input to build a new vector
t is a surrogate for semantic meanings
carried at time step t of the decoder. It is subsequently used to compute a probability distribution over
(cid:101)
the output vocabulary (Eq. (7)).

. To predict the next word, the context vector ct and hd
hd

and encoder hidden states

t (Eq. (6)).

he
i }

·||·

(cid:101)

{

et,i = v(cid:62) tanh(We[hd
t ||

i ] + be)
he

αt,i =

exp(et,i)
S
i(cid:48)=1 exp(et,i(cid:48))

S
i=1 αt,ihe
ct =
(cid:80)
i
t = tanh(Wh[hd
hd
t ||
Pvocab(w) = softmax(Wy
(cid:101)

(cid:80)

ct] + bh)
t + by)
hd

(cid:101)

(1)

(2)

(3)

(4)

(5)

(6)

(7)

Figure 2: System architectures for ‘Struct+Input’ (left) and ‘Struct+Hidden’ (right). A critical question we seek to answer is
whether the structural embeddings (se
i ) should be supplied as input to the encoder (left) or be exempted from encoding and
directly concatenated with the encoder hidden states (right).

The copy mechanism (Gulcehre et al., 2016; See et al., 2017) allows words in the source sequence to
be selectively copied to the target sequence. It expands the search space for summary words to include
both the output vocabulary and the source text. The copy mechanism can effectively reduce out-of-
vocabulary tokens in the generated text, potentially aiding a number of applications such as MT (Luong
et al., 2015b) and text summarization (Gu et al., 2016; Cheng and Lapata, 2016; Zeng et al., 2017).

Our copy mechanism employs a ‘switch’ to estimate the likelihood of generating a word from the
vocabulary (pgen) vs. copying it from the source text (1
pgen). The basic model is similar to that of the
pointer-generator networks (See et al., 2017). The switch is a feedforward layer with sigmoid activation
(Eq. (8)). At time step t, its input is a concatenation of the decoder hidden state hd
t , context vector ct,
and the embedding of the previously generated word yt−1. For predicting the next word, we combine
the generation and copy probabilities, shown in Eq. (9). If a word w appears once or more in the input
i:wi=w αt,i) is the sum of the attention weights over all its occurrences. If w
text, its copy probability (
appears in both the vocabulary and source text, P (w) is a weighted sum of the two probabilities.

−

(cid:80)

pgen=σ(Wz[hd
t ||

ct

yt−1])+bz)
||

P (w)=pgenPvocab(w)+(1

pgen)

−

αt,i

i:wi=w
(cid:88)

(8)

(9)

3.2 Structure-Infused Copy Mechanisms

The aforementioned copy mechanism attends to source words based on their “semantic” importance en-
, which measures the semantic relatedness of the encoder hidden state he
i and the decoder
coded in
αt,i
}
{
hidden state hd
t (Eq. (4)). However, the source syntactic structure is ignored. This is problematic, because
it hurts the system’s ability to effectively identify summary-worthy source words that are syntactically
important. We next propose three strategies to inject source syntactic structure to the copy mechanism.

3.2.1 Shallow Combination

Inspired by compressive summarization via
structured prediction (Berg-Kirkpatrick et al.,
2011; Almeida and Martins, 2013), we hypoth-
esize that structural labels, such as the incoming
dependency arc and the depth in a dependency
parse tree, can be helpful to predict word impor-
tance. We consider six categories of structural
labels in this work; they are presented in Table 2.
Each structural label is mapped to a ﬁxed-length,
trainable structural embedding. However, a crit-
ical question remains as to where the structural
embeddings should be injected in the existing neural architecture. This problem has not yet been sys-
tematically investigated. In this work, we compare two settings:

Structural info
(1) depth in the dependency parse tree
(2) label of the incoming edge
(3) number of outgoing edges
(4) part-of-speech tag
(5) absolution position in the source text
(6) relative position in the source text

Table 2: Six categories of structural labels. Example labels are
generated for word ‘had’ in Figure 1. Relative word positions
are discretized into ten buckets.

Example
0
‘root’
3
‘VBD’
9
(0.5, 0.6]

•

Struct+Input concatenates structural embeddings of position i (ﬂattened into one vector se
i ) with
se
i ];
the source word embedding xi and uses them as a new form of input to the encoder: xi
[xi
||

⇒

Figure 3: System architectures for ‘Struct+2Way+Word’ (left) and ‘Struct+2Way+Relation’ (right). βt,i (left) measures the
structural importance of the i-th source word; βt,i (right) measures the saliency of the dependency edge pointing to the i-th
source word. ge
p,i is the structural embedding of the parent. In both cases δt,i replaces αt,i to become the new attention value
used to estimate the context vector ct.

•

Struct+Hidden concatenates structural embeddings of position i (ﬂattened) with the encoder hid-
den state he

i and uses them as a new form of hidden states: he

se
i ].

i ⇒

[he
i ||

The architectural difference is illustrated in Figure 2. Structural embeddings are important comple-
ments to existing neural architectures. However, it is unclear whether they should be supplied as input
to the encoder or be left out of the encoding process and directly concatenated with the encoder hidden
states. This is a critical question we seek to answer by comparing the two settings. Note that an alter-
native setting is to separately encode words and structural labels using two RNN encoders, we consider
this as a subproblem of the “Struct+Input” case.

The above models complement state-of-the-art by combining semantic and structural signals to deter-
mine summary-worthy content. Intuitively, a source word is copied to the summary for two reasons: it
contains salient semantic content, or it serves a critical syntactic role in the source sentence. Without
explicitly modeling the two factors, ‘semantics’ can outweigh ‘structure,’ resulting in summaries that
fail to keep the original meaning intact. In the following we propose a two-way mechanism to separately
model the “semantic” and “structural” importance of source words.

3.2.2

2-Way Combination (+Word)

i = [se
i ||
We deﬁne δt,i

Our new architecture involves two attention matrices that are parallel to each other, denoted by α and β.
αt,i is deﬁned as previously in Eq. (3-4). It represents the “semantic” aspect, calculated as the strength
of interaction between the encoder hidden state he
t . In contrast, βt,i
measures the “structural” importance of the i-th input word to generating the t-th output word, calculated
by comparing the structure-enhanced embedding ge
t (Eq. (10-11)). We
use ge

xi] as a primitive (unencoded) representation of the i-th source word.

i and the decoder hidden state hd

i with the decoder hidden state hd

∝

αt,i + (cid:15)βt,i as a weighted sum of αt,i and βt,i, where a trainable coefﬁcient (cid:15) is
introduced to balance the contribution from both sides (Eq. (12)). Merging semantic and structural
salience at this stage allows us to acquire an accurate estimate of how important the i-th source word
is to predicting the t-th output word. δt,i replaces αt,i to become the new attention value. It is used to
calculate the context vector ct (Eq. (13)). A reliable estimate of ct is crucial as it is used to estimate the
generation probability over the vocabulary (Pvocab(w), Eq. (6-7)), the switch value (pgen, Eq. (8)), and
ultimately used to predict the next word (P (w), Eq. (9)).

ft,i = u(cid:62) tanh(Wf [ge
i ||

t ] + bf )
hd

βt,i =

δt,i =

exp(ft,i)
S
i(cid:48)=1 exp(ft,i(cid:48))
αt,i + (cid:15)βt,i
(cid:80)
S
i(cid:48)=1(αt,i(cid:48) + (cid:15)βt,i(cid:48))

ct =

δt,ihe
i

S
(cid:80)

i=1
(cid:88)

(10)

(11)

(12)

(13)

2-Way Combination (+Relation)

3.2.3
We observe that salient source relations also play a critical role in predicting the next word. For example,
if a dependency edge (“father” nsubj
“had”) is salient and “father” is selected to be included in the
←−−
summary, it is likely that “had” will be selected next such that a salient source relation (“nsubj”) is
preserved in the summary. Because summary words tend to follow the word order of the original, we
assume selecting a source word and including it in the summary has an impact on its subsequent source
words, but not the reverse.

In this formulation we use βt,i to capture the saliency of the dependency edge pointing to the i-th

wi has its salience score saved in βt,j; and conversely, an edge wj

source word. Thus, an edge wj
→
wi has its salience score in βt,i. β is calculated in the same way as described in Eq. (10-11). However,
i with [ge
we replace ge
ge
p,i] so that a dependency edge is characterized by the embeddings of its two
i ||
endpoints (ge
p,i is the parent embedding). The architectural difference between “Struct+2Way+Word”
and “Struct+2Way+Relation” is illustrated in Figure 3.

←

←

To obtain the likelihood of wj being selected to the summary prior to time step t, we deﬁne
αt,j =
t−1
t(cid:48)=0 αt(cid:48),j that sums up the individual probabilities up to time step t-1. Assume there is a dependency
αt,jβt,i (or
wi (j<i) whose salience score is denoted by βt,i. At time step t, we calculate
edge wj
(cid:101)
→
(cid:80)
wi) as the probability of wi being selected to the summary, given that one of its
αt,jβt,j for edge wj
prior words wj (j<i) is included in the summary and there is a dependency edge connecting the two. By
summing the impact over all its previous words, we obtain the likelihood of the i-th source word being
(cid:101)
included to the summary at time step t in order to preserve salient source relations; this is denoted by
αt,i + (cid:15)γt,i as a weighted combination of semantic and structural
γt,i (Eq. (15)). Next, we deﬁne δt,i
salience (Eq. (16)). δt,i replace αt,i to become the new attention values used to estimate the context
vector ct (Eq. (13)). Finally, the calculation of generation probabilities Pvocab(w), switch value pgen,
and probabilities for predicting the next word P (w) remains the same as previously (Eq. (6-9)).

∝

(cid:101)

αt,j =

t−1
t(cid:48)=0 αt(cid:48),j

γt,i =
(cid:101)

δt,i =

if wj
if wj

wi
wi

→
←

(cid:80)
j:j<i (cid:40)
(cid:88)

αt,jβt,i
αt,jβt,j
(cid:101)
(cid:101)

αt,i + (cid:15)γt,i
S
i(cid:48)=1(αt,i(cid:48) + (cid:15)γt,i(cid:48))

(cid:80)

(14)

(15)

(16)

3.3 Learning Objective and Beam Search

We next describe our learning objective, including a coverage-based regularizer (See et al., 2017), and
a beam search with reference mechanism (Tan et al., 2017). We want to investigate the effectiveness of
these techniques on sentence summarization, which has not been explored in previous work.

Learning objective. Our training proceeds by minimizing a per-target-word cross-entropy loss function.
[0, 1] measures the interaction strength
A regularization term is applied to the α matrix. Recall that αt,i
between the t-th output word and the i-th input word. Naturally, we expect a 1-to-1 mapping between the
two words. The coverage-based regularizer, proposed by See et al., (2017), encourages this behavior by
tracking the historical attention values attributed to the i-th input word (up to time step t-1), denoted by
αt,i and αt,i, which has the practical
αt,i =
t) to be close to either 0 or 1, otherwise a penalty will be applied. The regularizer
effect of forcing αt,i (
Ω is deﬁned in Eq. (17), where M is the size of the mini-batch, S and T are the lengths of the source
(cid:101)

t−1
t(cid:48)=0 αt(cid:48),i. The approach then takes the minimum between

(cid:80)

∈

(cid:101)

∀

and target sequences. For two-way copy mechanisms, δ replaces α to become the new attention values,
we therefore apply regularization to δ instead of α. When the regularizer applies, the objective becomes
minimizing (

+ Ω).

L

Ω=λ

M

m=1
(cid:88)

1
T (m)S(m)

T (m)

S(m)

t=1
(cid:88)

i=1 (cid:16)
(cid:88)

min(

αt,i,αt,i)

(cid:17)

(cid:101)

(17)

Beam search with reference. During testing, we employ greedy search to generate system summary
sequences. For the task of summarization, the ground truth summary sequences are usually close to the
source texts. This property can be leveraged in beam search. Tan et al., (2017) describe a beam search
with reference mechanism that rewards system summaries that have a high degree of bigram overlap with
the source texts. We describe it in Eq. (18), where where
(y<t, x)
measures the number of bigrams shared by the system summary (up to time step t-1) and the source text;
adds a word w to the end of the system summary. The shorter the source text (measured by
y<t, w
{
length S), the more weight a shared bigram will add to the score of the current word w. A hyperparameter
η controls the degree of closeness between the system summary and the source text.

(w) denotes the score of word w.

B

S

}

(w)=logP (w)+η B

S

y<t ,w

(
{

,x)
}
S

−B

(y<t ,x)

(18)

4 Experiments

4.1 Data Sets

We evaluate the proposed structure-infused copy mechanisms for summarization in this section. We
describe the dataset, experimental settings, baselines, and ﬁnally, evaluation results and analysis.

We evaluate our proposed models on the Gigaword summarization dataset (Parker, 2011; Rush et al.,
2015). The task is to reduce the ﬁrst sentence of an article to a title-like summary. We obtain dependency
parse trees for source sentences using the Stanford neural network parser (Chen and Manning, 2014). We
also use the standard train/valid/test data splits. Following (Rush et al., 2015), the train and valid splits
are pruned2 to improve the data quality. Spurious pairs that are repetitive, overly long/short, and pairs
whose source and summary sequences have little word overlap are removed. No pruning is performed
for instances in the test set. The processed corpus contains 4,018K training instances. We construct
two (non-overlapped) validation sets: “valid-4096” contains 4,096 randomly sampled instances from the
valid split; it is used for hyperparameter tuning and early stopping. “valid-2000” is used for evaluation;
it allows the models to be trained and evaluated on pruned instances. Finally, we report results on the
standard Gigaword test set (Rush et al., 2015) containing 1,951 instances (“test-1951”).

4.2 Experimental Setup

∼ N

(0, σ), σ =

2
nin+nout

We use the Xavier scheme (Glorot and Bengio, 2010) for parameter initialization, where weights are
initialized using a Gaussian distribution Wi,j
; nin and nout are numbers of
the input and output units of the network; biases are set to be 0. We further implement two techniques to
accelerate mini-batch training. First, all training instances are sorted by the source sequence length and
partitioned into mini-batches. The shorter sequences are padded to have the same length as the longest
sequence in the batch. All batches are shufﬂed at the beginning of each epoch. Second, we introduce a
variable-length batch vocabulary containing only source words of the current mini-batch and words of
the output vocabulary. P (w) in Eq. (9) only needs to be calculated for words in the batch vocabulary.
It is magnitudes smaller than a direct combination of the input and output vocabularies. Finally, our
input vocabulary contains the most frequent 70K words in the source texts and summaries. The output
vocabulary contains 5K words by default. More network parameters are presented in Table 3.

(cid:113)

2https://github.com/facebookarchive/NAMAS/blob/master/dataset/filter.py

Input vocabulary size
Output vocabulary size
Dim. of word embeddings
Dim. of structural embeddings
Num. of encoder/decoder hidden units
Adam optimizer (Kingma and Ba, 2015)
Coeff. for coverage-based regularizer
Coeff. for beam search with reference
Beam size
Minibatch size
Early stopping criterion (max 20 epochs)
Gradient clipping (Pascanu et al., 2013)

70K
5K (default)
100
16
256
lr = 1e-4
λ = 1
η ≈ 13.5
K = 5
M = 64
valid. loss
g ∈ [-5, 5]

Table 3: Parameter settings of our summarization system.

System

Baseline
Struct+Input
Struct+Hidden
Struct+2Way+Word
Struct+2Way+Relation

Gigaword Valid-2000
R-L
R-2
R-1

42.48
42.44
42.88
43.21
42.83

21.34
21.75
21.81
21.84
21.85

40.18
40.46
40.63
40.86
40.60

Table 4: Results on the Gigaword valid-2000 set (full-length
F1). Models implementing the structure-infused copy mech-
anisms (“Struct+*”) outperform the baseline.

S:

the government ﬁled another round of criminal charges in a
widening stock options scandal

T: options scandal widens

another round of criminal charges in stock options scandal

B: government ﬁles more charges in stock options scandal
I:
H: charges ﬁled in stock options scandal
W: another round of criminal charges in stock options scandal
R: government ﬁles another round of criminal charges in options

scandal

Table 5: Example system summaries. ‘S:’ source; ‘T:’ target;
‘B:’ baseline; ‘I:’ Struct+Input; ‘H:’ Struct+Hidden; ‘W:’
2Way+Word; “R:” 2Way+Relation. “2Way+Relation” is able
to preserve important source relations in the summary, e.g.,
nsubj←−−− ﬁles,” “ﬁles dobj−−→ round,” and “round
“government
nmod−−−→ charges.”

S:

red cross negotiators from rivals north korea and south korea held
talks wednesday on emergency food shipments to starving north
koreans and agreed to meet again thursday

T: koreas meet in beijing to discuss food aid from south eds

north korea , south korea meet again

B: north korea , south korea agree to meet again
I:
H: north korea , south korea meet on emergency food shipments
W: north korea , south korea hold talks on food shipments
R: north korea , south korea hold talks on emergency food shipments

Table 6: Example system summaries. “Struct+Hidden” and
“2Way+Relation” successfully preserve salient source words
(“emergency food shipments”), which are missed out by
other systems. We observe that copying “hold talks” from the
source also makes the resulting summaries more informative
than using the word “meet.”

4.3 Results

ROUGE results on valid set. We ﬁrst report results on the Gigaword valid-2000 dataset in Table 4. We
present R-1, R-2, and R-L scores (Lin, 2004) that respectively measures the overlapped unigrams, bi-
grams, and longest common subsequences between the system and reference summaries3. Our baseline
system (“Baseline”) implements the seq2seq architecture with the basic copy mechanism (Eq. (1-9)). It is
a strong baseline that resembles the pointer-generator networks described in (See et al., 2017). The struc-
tural models (“Struct+*”) differ from the baseline only on the structure-infused copy mechanisms. All
models are evaluated without the coverage regularizer or beam search (
3.3) to ensure fair comparison.
§
Overall, we observe that models equipped with the structure-infused copy mechanisms are superior to
the baseline, suggesting that combining source syntactic structure with the copy mechanism is effective.
We found that the “Struct+Hidden” architecture, which directly concatenates structural embeddings with
the encoder hidden states, outperforms “Struct+Input” despite that the latter requires more parameters.
“Struct+2Way+Word” also demonstrates strong performance, achieving 43.21%, 21.84%, and 40.86%
F1 scores, for R-1, R-2, and R-L respectively.

ROUGE results on test set. We compare our proposed approach with a range of state-of-the-art neural
summarization systems. Results on the standard Gigaword test set (“test-1951”) are presented in Table 7.
Details about these systems are provided in Table 8. Overall, our proposed approach with structure-
infused pointer networks perform strongly, yielding ROUGE scores that are on-par with or surpassing
state-of-the-art published systems. Notice that the scores on the valid-2000 dataset are generally higher
than those of test-1951. This is because the (source, summary) pairs in the Gigaword test set are not
4.1). In some cases, none (or very few) of the summary words appear in the source. This
pruned (see
§
may cause difﬁculties to the systems equipped with the copy mechanism. The “Struct+2Way+Word”
architecture that respectively models the semantic and syntactic importance of source words achieves the
highest scores. It outperforms its counterpart of “Struct+2Way+Relation,” which seeks to preserve source
dependency relations in summaries. We conjecture that the imperfect dependency parse trees generated

3w/ ROUGE options: -n 2 -m -w 1.2 -c 95 -r 1000

System

ABS (Rush et al., 2015)
ABS+ (Rush et al., 2015)
Luong-NMT (Chopra et al., 2016)
RAS-LSTM (Chopra et al., 2016)
RAS-Elman (Chopra et al., 2016)
ASC+FSC1 (Miao and Blunsom, 2016)
lvt2k-1sent (Nallapati et al., 2016)
lvt5k-1sent (Nallapati et al., 2016)
Multi-Task (Pasunuru et al., 2017)
DRGD (Li et al., 2017b)

Gigaword Test-1951
R-L
R-2
R-1

29.55 11.32 26.42
29.76 11.88 26.96
33.10 14.45 30.71
32.55 14.70 30.03
33.78 15.97 31.15
34.17 15.94 31.92
32.67 15.59 30.64
35.30 16.64 32.62
32.75 15.35 30.82
36.27 17.57 33.62

35.43 17.49 33.39
Baseline (this paper)
Struct+Input (this paper)
35.32 17.50 33.25
Struct+2Way+Relation (this paper) 35.46 17.51 33.28
35.49 17.61 33.33
Struct+Hidden (this paper)
35.47 17.66 33.52
Struct+2Way+Word (this paper)

Table 7: Results on the Gigaword test-1951 set (full-
length F1). Models with structure-infused copy mechanisms
(“Struct+*”) perform well. Their R-2 F-scores are on-par
with or outperform state-of-the-art published systems.

ABS and ABS+ (Rush et al., 2015) are the ﬁrst work introduc-
ing an encoder-decoder architecture for summarization.
Luong-NMT (Chopra et al., 2016) is a re-implementation of
the attentive stacked LSTM encoder-decoder of Luong
et al. (2015a).

RAS-LSTM and RAS-Elman (Chopra et al., 2016) describe a
convolutional attentive encoder that ensures the decoder
focuses on appropriate words at each step of generation.
ASC+FSC1 (Miao and Blunsom, 2016) presents a genera-
tive auto-encoding sentence compression model jointly
trained on labelled/unlabelled data.

lvt2k-1sent and lvt5k-1sent (Nallapati et al., 2016) address
issues in the attentive encoder-decoder framework,
including modeling keywords, capturing sentence-to-
word structure, and handling rare words.

Multi-Task w/ Entailment (Pasunuru et al., 2017) combines
entailment with summarization in a multi-task setting.
DRGD (Li et al., 2017b) describes a deep recurrent generative
decoder learning latent structure of summary sequences
via variational inference.

Table 8: Existing summarization methods.

by the parser may affect the “Struct+2Way+Relation” results. However, because the Gigaword dataset
does not provide gold-standard annotations for parse trees, we could not easily verify this and will leave
it for future work. In Table 5 and 6, we present system summaries produced by various models.

System
Struct+Input
Struct+2Way+Relation
Ground-truth Summ.

Linguistic quality. To further gauge the summary
quality, we hire human workers from the Amazon
Mechanical Turk platform to rate summaries on a Lik-
ert scale of 1 to 5 according to three criteria (Zhang
and Lapata, 2017): ﬂuency (is the summary grammat-
ical and well-formed?), informativeness (to what ex-
tent is the meaning of the original sentence preserved
in the summary?), and faithfulness (is the summary
accurate and faithful to the original?). We sample 100
instances from the test set and employ 5 turkers to rate
each summary; their averaged scores are presented in Table 9. We found that “Struct+2Way+Relation”
outperforms “Struct+Input” on all three criteria. It also compares favorably to ground-truth summaries
on “ﬂuency” and “faithfulness.” On the other hand, the ground-truth summaries, corresponding to article
titles, are judged as less satisfying according to human raters.

Table 9: Informativeness, ﬂuency, and faithfulness scores
of summaries. They are rated by Amazon turkers on a
Likert scale of 1 (worst) to 5 (best). We choose to eval-
uate Struct+2Way+Relation (as oppose to 2Way+Word)
because it focuses on preserving source relations in the
summaries.

Info. Fluency Faithful.
2.9
3.0
3.2

3.0
3.1
3.1

3.3
3.4
3.5

Dependency relations. We investigate the source dependency relations preserved in the summaries in
Table 10. A source relation is considered preserved if both its words appear in the summary. We observe
that the models implementing structure-infused copy mechanisms (e.g., “Struct+2Way+Word”) are more
likely to preserve important dependency relations in the summaries, including nsubj, dobj, amod, nmod,
and nmod:poss. Dependency relations that are less important (mark, case, conj, cc, det) are less likely to
be preserved. These results show that our structure-infused copy mechanisms can learn to recognize the
importance of dependency relations and selectively preserve them in the summaries.

Coverage and reference beam. In Figure 11, we investigate the effect of applying the coverage regu-
larizer (“coverage”) and reference-based beam search (“ref beam”) (
3.3) to our models. The coverage
§
regularizer is applied in a second training stage, where the system is trained for an extra 5 epochs with
coverage and the model yielding the lowest validation loss is selected. Both coverage and ref beam
can improve the system performance. Our observation suggests that ref beam is an effective addition to
shorten the gap between different systems.

System

nsubj

dobj

amod

nmod

nmod:poss mark

case

Baseline
Struct+Input
Struct+Hidden
Struct+2Way+Word
Struct+2Way+Relation

7.23
7.03
7.78↑
7.46↑
7.35↑

12.07
11.72
12.34↑
12.69↑
12.07↑

20.45
19.72
21.11↑
20.59↑
20.59↑

8.73
9.17↑
9.18↑
9.03↑
8.68

12.46
12.46
14.86↑
13.00↑
13.47↑

15.83
15.35
14.93
15.83
15.41

14.84
14.69
15.84↑
14.43
14.39

conj

9.72
9.55
9.47
8.86
9.12

cc

5.03
4.67
3.93
3.48
4.30

det

2.22
1.97
2.65↑
1.91
1.89

Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.

|V |
R-2
13.99
1K
15.35
2K
5K
17.25
10K 17.62

Train Speed
2.5h/epoch
2.7h/epoch
3.2h/epoch
3.8h/epoch

InVcb
60.57
69.71
79.98
88.26

InVcb+Src
76.04
80.72
86.51
92.18

Table 12: Results of the “Struct+2Way+Relation” system
trained using output vocabularies of various sizes (|V |), eval-
uated on test-1951 w/o coverage or ref beam. The training
speed is calculated as the elapsed time (hours) per epoch,
tested on a GTX 1080Ti GPU card.

Table 11: Effects of applying the coverage regularizer and
the reference beam search to structural models, evaluated on
test-1951. Combining both yields the highest scores.

Output vocabulary size. Finally, we investigate the impact of the output vocabulary size on the sum-
marization performance in Table 12. All our models by default use an output vocabulary of 5K words in
order to make the results comparable to state-of-the-art-systems. However, we observe that there is a po-
tential to further boost the system performance (17.25
17.62 R-2 F1-score, w/o coverage or ref beam)
if we had chosen to use a larger vocabulary (10K) and can endure a slightly longer training time (1.2x).
In Table 12, we further report the percentages of reference summary words covered by the output vocab-
ulary (“InVcb”) and covered by either the output vocabulary or the source text (“InVcb+Src”). The gap
between the two conditions shortens as the size of the output vocabulary is increased.

→

In this paper, we investigated structure-infused copy mechanisms that combine source syntactic struc-
ture with the copy mechanism of an abstractive summarization system. We compared various system
architectures and showed that our models can effectively preserve salient source relations in summaries.
Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-the-
art published systems.

5 Conclusion

References

Miguel B. Almeida and Andre F. T. Martins. 2013. Fast and robust compressive summarization with dual decom-

position and multi-task learning. In Proceedings of ACL.

Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceed-

ings of the Annual Meeting of the Association for Computational Linguistics (ACL).

Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive

summarization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI).

Giuseppe Carenini and Jackie Chi Kit Cheung. 2008. Extractive vs. NLG-based abstractive summarization of eval-
uative text: The effect of corpus controversiality. In Proceedings of the Fifth International Natural Language
Generation Conference (INLG).

Danqi Chen and Christopher D. Manning. 2014. A fast and accurate dependency parser using neural networks. In

Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, and Hui Jiang. 2016. Distraction-based neural networks
for document summarization. In Proceedings of the Twenty-Fifth International Joint Conference on Artiﬁcial
Intelligence (IJCAI).

Huadong Chen, Shujian Huang, David Chiang, and Jiajun Chen. 2017. Improved neural machine translation with a
syntax-aware encoder and decoder. In Proceedings of the Annual Meeting of the Association for Computational
Linguistics (ACL).

Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. In Proceed-

ings of ACL.

Sumit Chopra, Michael Auli, and Alexander M. Rush. 2016. Abstractive sentence summarization with attentive

recurrent neural networks. In Proceedings of NAACL.

James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear program-

ming approach. Journal of Artiﬁcial Intelligence Research.

Hal Daume III and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of

the Annual Meeting of the Association for Computational Linguistics (ACL).

Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein. 2016. Learning-based single-document summarization
with compression and anaphoricity constraints. In Proceedings of the Association for Computational Linguistics
(ACL).

Giuseppe Di Fabbrizio, Amanda J. Stent, and Robert Gaizauskas. 2014. A hybrid approach to multi-document
summarization of opinions in reviews. Proceedings of the 8th International Natural Language Generation
Conference (INLG).

Katja Filippova, Enrique Alfonseca, Carlos Colmenares, Lukasz Kaiser, and Oriol Vinyals. 2015. Sentence com-
pression by deletion with lstms. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing (EMNLP).

Dimitrios Galanis and Ion Androutsopoulos. 2010. An extractive supervised two-stage method for sentence

compression. In Proceedings of NAACL-HLT.

Shima Gerani, Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng, and Bita Nejat. 2014. Abstractive summa-
rization of product reviews using discourse structure. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).

Xavier Glorot and Yoshua Bengio. 2010. Understanding the difﬁculty of training deep feedforward neural net-
works. In Proceedings of the 13th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS).

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016. Incorporating copying mechanism in sequence-to-

sequence learning. In Proceedings of ACL.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown
words. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL).

Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735–

1780.

Hongyan Jing and Kathleen McKeown. 1999. The decomposition of human-written summary sentences. In Pro-
ceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR).

Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya Takamura, and Manabu Okumura. 2016. Controlling

output length in neural encoder-decoders. In Proceedings of EMNLP.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of the

International Conference on Learning Representations (ICLR).

Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013. Document summarization via guided sentence compression.

In Proceedings of EMNLP.

Chen Li, Yang Liu, Fei Liu, Lin Zhao, and Fuliang Weng. 2014. Improving multi-documents summarization by

sentence compression based on expanded constituent parse tree. In Proceedings of EMNLP.

Junhui Li, Deyi Xiong, Zhaopeng Tu, Muhua Zhu, Min Zhang, and Guodong Zhou. 2017a. Modeling source syn-
tax for neural machine translation. In Proceedings of the Annual Meeting of the Association for Computational
Linguistics (ACL).

Piji Li, Wai Lam, Lidong Bing, and Zihao Wang. 2017b. Deep recurrent generative decoder for abstractive
text summarization. In Proceedings of the Conference on Empirical Methods in Natural Language Processing
(EMNLP).

Chin-Yew Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In Proceedings of ACL Work-

shop on Text Summarization Branches Out.

Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman Sadeh, and Noah A. Smith. 2015. Toward abstractive summa-
rization using semantic representations. In Proceedings of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies (NAACL).

Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015a. Effective approaches to attention-based
In Proceedings of the Conference on Empirical Methods in Natural Language

neural machine translation.
Processing (EMNLP).

Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. 2015b. Addressing the
rare word problem in neural machine translation. In Proceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL).

Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In Proceedings of

EACL.

Retrieval.

Yishu Miao and Phil Blunsom. 2016. Language as a latent variable: Discrete generative models for sentence com-
pression. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive text
summarization using sequence-to-sequence RNNs and beyond. In Proceedings of the 20th SIGNLL Conference
on Computational Natural Language Learning (CoNLL).

Ani Nenkova and Kathleen McKeown. 2011. Automatic summarization. Foundations and Trends in Information

Robert Parker. 2011. English Gigaword ﬁfth edition LDC2011T07. Philadelphia: Linguistic Data Consortium.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difﬁculty of training recurrent neural net-

works. In Proceedings of the International Conference on Machine Learning (ICML).

Ramakanth Pasunuru, Han Guo, and Mohit Bansal. 2017. Towards improving abstractive summarization via

entailment generation. In Proceedings of the Workshop on New Frontiers in Summarization.

Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summariza-
tion. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).

Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word repre-
sentation. In Proceedings of the Conference Empirical Methods in Natural Language Processing (EMNLP).

Daniele Pighin, Marco Cornolti, Enrique Alfonseca, and Katja Filippova. 2014. Modelling events through
memory-based, open-ie patterns for abstractive summarization. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).

Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for sentence summariza-
tion. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).

Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the Annual Meeting of the Association for Computational Linguistics
(ACL).

Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu Hirao, and Masaaki Nagata. 2016. Neural headline genera-
tion on abstract meaning representation. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).

Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017. Abstractive document summarization with a graph-based at-
tentional neural model. In Proceedings of the Annual Meeting of the Association for Computational Linguistics
(ACL).

Kapil Thadani and Kathleen McKeown. 2013. Sentence compression with joint structural inference. In Proceed-

ings of CoNLL.

Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Florian, and Claire Cardie. 2013. A sentence compression

based framework to query-focused multi-document summarization. In Proceedings of ACL.

David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard Schwartz. 2007. Multi-candidate reduction: Sentence

compression as a tool for document summarization tasks. Information Processing and Management.

Wenyuan Zeng, Wenjie Luo, Sanja Fidler, and Raquel Urtasun. 2017. Efﬁcient summarization with read-again
and copy mechanism. In Proceedings of the International Conference on Learning Representations (ICLR).

Xingxing Zhang and Mirella Lapata. 2017. Sentence simpliﬁcation with deep reinforcement learning. In Pro-

ceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).

Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou. 2017. Selective encoding for abstractive sentence summa-

rization. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).

