Optical Flow augmented Semantic Segmentation networks
for Automated Driving

Hazem Rashed1, Senthil Yogamani2, Ahmad El-Sallab1, Pavel Kˇr´ıˇzek3 and Mohamed El-Helw4
1CDV AI Research, Cairo, Egypt
2Valeo Vision Systems, Ireland
3Valeo R&D DVS, Prague, Czech Republic
4Nile University, Cairo, Egypt
{hazem.rashed, senthil.yogamani, ahmad.el-sallab@valeo.com}@valeo.com, melhelw@nu.edu.eg

9
1
0
2
 
n
a
J
 
1
1
 
 
]

V
C
.
s
c
[
 
 
1
v
5
5
3
7
0
.
1
0
9
1
:
v
i
X
r
a

Keywords:

Semantic Segmentation, Visual Perception, Dense Optical Flow, Automated Driving.

Abstract:

Motion is a dominant cue in automated driving systems. Optical ﬂow is typically computed to detect moving
objects and to estimate depth using triangulation. In this paper, our motivation is to leverage the existing
dense optical ﬂow to improve the performance of semantic segmentation. To provide a systematic study, we
construct four different architectures which use RGB only, ﬂow only, RGBF concatenated and two-stream
RGB + ﬂow. We evaluate these networks on two automotive datasets namely Virtual KITTI and Cityscapes
using the state-of-the-art ﬂow estimator FlowNet v2. We also make use of the ground truth optical ﬂow in
Virtual KITTI to serve as an ideal estimator and a standard Farneback optical ﬂow algorithm to study the
effect of noise. Using the ﬂow ground truth in Virtual KITTI, two-stream architecture achieves the best results
with an improvement of 4% IoU. As expected, there is a large improvement for moving objects like trucks,
vans and cars with 38%, 28% and 6% increase in IoU. FlowNet produces an improvement of 2.4% in average
IoU with larger improvement in the moving objects corresponding to 26%, 11% and 5% in trucks, vans and
cars. In Cityscapes, ﬂow augmentation provided an improvement for moving objects like motorcycle and train
with an increase of 17% and 7% in IoU.

1 Introduction

Semantic image segmentation has witnessed
tremendous progress recently with deep learning.
It provides dense pixel-wise labeling of the image
which leads to scene understanding. Automated driv-
ing is one of the main application areas where it is
commonly used (Horgan et al., 2015). The level of
maturity in this domain has rapidly grown recently
and the computational power of embedded systems
have increased as well to enable commercial deploy-
ment. Currently, the main challenge is the cost of
constructing large datasets as pixel-wise annotation is
very labor intensive. It is also difﬁcult to perform cor-
ner case mining as it is a uniﬁed model to detect all
the objects in the scene. Thus there is a lot of research
to reduce the sample complexity of segmentation net-
works by incorporating domain knowledge and other
cues where-ever possible. In this work, we explore
the usage of motion cues via dense optical ﬂow to im-
prove the accuracy.

Majority of semantic segmentation algorithms

mainly rely on appearance cues based on a single im-
age and do not exploit motion cues from two con-
secutive images.
In this paper, we address the us-
age of dense optical ﬂow as a motion cue in seman-
tic segmentation networks. In particular for automo-
tive scenes, the scene is typically split into static in-
frastructure and set of independently moving objects.
Motion cues which needs two consecutive frames
could particularly improve the segmentation of mov-
ing objects.

The contributions of this work include:

• Construction of four CNN architectures to sys-
tematically study the effect of optical ﬂow aug-
mentation to semantic segmentation.

• Experimentation on two automotive datasets

namely Virtual KITTI and Cityscapes.

• Ablation study on using different ﬂow estimators

and different ﬂow representations.

The rest of the paper is organized as follows: Sec-
tion 2 reviews the related work in segmentation, com-
putation of ﬂow and role of ﬂow in semantic segmen-

tation. Section 3 details the construction of four archi-
tectures to systematically study the effect of augment-
ing ﬂow to semantic segmentation networks. Section
4 discusses the experimental results in Virtual KITTI
and Cityscapes. Finally, section 5 provides conclud-
ing remarks.

2 Related Work

2.1 Semantic Segmentation

A detailed survey of semantic segmentation for au-
tomated driving is presented in (Siam et al., 2017a).
We brieﬂy summarize the relevant parts focused on
CNN based methods which are split into mainly three
subcategories. The ﬁrst (Farabet et al., 2013) used
patch-wise training to yield the ﬁnal classiﬁcation.
In(Farabet et al., 2013) an image is fed into a Lapla-
cian pyramid, each scale is forwarded through a 3-
stage network to extract hierarchical features and
patch-wise classiﬁcation is used. The output is post
processed with a graph based classical segmentation
method. In (Grangier et al., 2009) a deep network was
used for the ﬁnal pixel-wise classiﬁcation to alleviate
any post processing needed. However, it still utilized
patch-wise training.

The second subcategory (Long et al., 2015)(Noh
et al., 2015)(Badrinarayanan et al., 2015) was focused
on end-to-end learning of pixel-wise classiﬁcation.
It started with the work in (Long et al., 2015) that
developed fully convolutional networks (FCN). The
network learned heatmaps that was then upsampled
with-in the network using deconvolution to get dense
predictions. Unlike patch-wise training methods this
method uses the full image to infer dense predictions.
In (Noh et al., 2015) a deeper deconvolution network
was developed, in which stacked deconvolution and
unpooling layers are used. In Segnet (Badrinarayanan
et al., 2015) a similar approach was used where an
encoder-decoder architecture was deployed. The de-
coder network upsampled the feature maps by keep-
ing the maxpooling indices from the corresponding
encoder layer.

2.2 Optical Flow in Automated Driving

Systems

Flow estimation is very critical for automated driv-
ing and it has been a standard module for more than
ten years. Due to computational restriction, sparse
optical ﬂow was used and it is replaced by Dense
Optical Flow (DOF) in recent times. As ﬂow is

already computed, it can be leveraged for seman-
tic segmentation. Motion estimation is a challeng-
ing problem because of the continuous camera mo-
tion along with the motion of independent objects.
Moving objects are the most critical in terms of
avoiding fatalities and enabling smooth maneuver-
ing and braking of the car. Motion cues can also
enable generic object detection as it is not possi-
ble to train for all possible object categories be-
forehand. Classical approaches in motion detec-
tion were focused on geometry based approaches
(Torr, 1998)(Papazoglou and Ferrari, 2013)(Ochs
et al., 2014)(Menze and Geiger, 2015)(Wehrwein and
Szeliski, 2017). However, pure geometry based ap-
proaches have many limitations, motion parallax is-
sue is one such example. A recent trend (Tok-
makov et al., 2016)(Jain et al., 2017)(Drayer and
Brox, 2016)(Vijayanarasimhan et al., 2017)(Fragki-
adaki et al., 2015) for learning motion in videos has
emerged. CNN based optical ﬂow has produced state
of the art results. FlowNet (Ilg et al., 2016) was a
simple two stream structure which was trained on syn-
thetic data.

There has been many attempts to combine appear-
ance and motion cues for various tasks. Jain et. al.
presented a method for appearance and motion fu-
sion in (Jain et al., 2017) for generic foreground ob-
ject segmentation. Laura et. al. (Sevilla-Lara et al.,
2016) leverages semantic segmentation for customiz-
ing motion model for various objects. This has been
commonly used in scene ﬂow models.
Junhwa et.
al (Hur and Roth, 2016) uses optical ﬂow to pro-
vide temporally consistent semantic segmentation via
post processing. MODNet (Siam et al., 2017b) fuses
optical ﬂow and RGB images for moving object de-
tection. FuseNet (Hazirbas et al., 2016) is the clos-
est to the work in this paper where they augmented
semantic segmentation networks with depth. They
show that concatenation of RGBD slightly reduces
mean IoU and two-stream network with cross links
show an improvement of 3.65 % IoU in SUN RBG-
D dataset. Motion is a complementary cue of color
and its role is relatively less explored for semantic
segmentation. Our motivation is to construct sim-
ple baseline architectures which can be built on top
of current CNN based semantic segmentation. More
sophisticated ﬂow augmentation architectures were
proposed in (Nilsson and Sminchisescu, 2016) and
(Gadde et al., 2017), however they are computation-
ally more intensive.

3 Semantic Segmentation Models

In this section, we discuss the details of the four
different semantic segmentation networks used in this
paper. We construct two ﬂow augmented architec-
tures namely RGBF network (Figure 1 (c)) which
does concatenation and two-stream RGB+F network
(Figure 1 (d)). RGB only and ﬂow only architectures
serve as baselines for comparative analysis.

(a) Input RGB

(b) Input Flow

(c) Input RGBF

(d) Two Stream RGB+F

Figure 1: Four types of architectures constructed and tested
in the paper. (a) and (b) are baselines using RGB and Flow
only. (c) and (d) are ﬂow augmented semantic segmentation
architectures.

Table 1: Quantitative evaluation on Virtual KITTI data for
our four segmentation networks.

Network Type
RGB
F
RGBF (concat)
RGB + F (2-stream add)

IoU
66.47
42
65.33
70.52

Precision
78.23
63.92
82.37
83

Recall
75.6
55.28
73.77
76.4

F-Score
73.7
55.76
75.85
78.9

3.1 One-stream networks

An encoder-decoder architecture is used for perform-
ing semantic segmentation. Our network is based on
FCN8s (Long et al., 2015) architecture, after which
the remaining fully connected layers of the VGG16
are transformed to a fully convolutional network. The
ﬁrst 15 convolutional layers are used for feature ex-
traction. The segmentation decoder follows the FCN
architecture. 1x1 convolutional layer is used followed
by three transposed convolution layers to perform up-
sampling. Skip connections are utilized to extract

high resolution features from the lower layers and
then the extracted features are added to the partially
upsampled results. The exact same one stream archi-
tecture is used for both RGB-only and ﬂow-only ex-
periments. We use the cross-entropy loss function as
shown below. where q denotes predictions and p de-
notes ground-truth. CDataset is the set of classes for
the used dataset.

3.2 RGBF network

The input to this network is a 3D volume containing
the original RGB image concatenated with the ﬂow
map. Several optical ﬂow representations have been
studied experimentally to ﬁnd the best performing in-
put, namely Color wheel representation in 3 channels,
magnitude and direction in 2 channels, and magni-
tude only in 1 channel. It was found that an input of
4 channels containing RGB image concatenated with
optical ﬂow magnitude performs the best (Refer to Ta-
ble 2 and 3). The ﬂow map is normalized from 0 to
255 to have the same value range as the RGB. The
ﬁrst layer of the VGG is adapted to use the input of
four channels and the corresponding weights are ini-
tialized randomly. The rest of the network utilizes the
VGG pre-trained weights. In case of Virtual KITTI,
we make use of ﬂow ground truth as one of the in-
puts. This is done to simulate a near-perfect ﬂow esti-
mator which can be achieved by state-of-the-art CNN
based algorithms and eliminate the variability due to
estimation errors. In case of cityscapes, we make use
of OpenCV Farneback function where the magnitude
u2 + v2 where
of the ﬂow vectors are computed as
u and v are the horizontal and vertical ﬂow vectors
output from the function. The magnitude is then nor-
malized to 0-255. In both the datasets, we also make
use of the state-of-the-art ﬂow estimator FlowNet v2
(Ilg et al., 2017).

√

3.3 Two stream (RGB+F) network

Inspired from (Simonyan and Zisserman, 2014)(Jain
et al., 2017)(Siam et al., 2017b), we construct a two
stream network which utilizes two parallel VGG16
encoders to extract appearance and ﬂow features sep-
arately. The feature maps from both streams are
fused together using summation junction producing
encoded features of same dimensions. Then the same
decoder described in the one-stream network is used
to perform upsampling. Following the same approach
as the one-stream network, skip connections are used
to beneﬁt from the high resolution feature maps. This
network produces the best performance among the
four as discussed in the experiments section. How-

Table 2: Semantic Segmentation Results on Virtual Kitti (Mean IoU) for different DOF (GT) representations

Type
RGBF (GT-Color Wheel)
RGBF (GT-Mag & Dir)
RGBF (GT-Mag only)
RGB+F (GT-3 layers Mag only)
RGB+F (GT-Color Wheel)

Mean
59.88
58.82
65.32
67.88
70.52

Truck
41.7
45.12
70.73
35.7
71.79

Car
84.44
82.3
80.16
91.02
91.4

Van
40.74
30.04
48.33
24.78
56.8

Road
93.76
90.25
93.59
96.47
96.19

Sky
93.6
94.1
93.3
94.06
93.5

Vegetation
66.3
60.97
70.79
88.72
83.4

Building
49.43
56.48
62.04
74.4
66.53

Guardrail
52.18
51.48
67.86
84.5
82.6

TrafﬁcSign
61.21
58.74
55.13
69.48
64.69

TrafﬁcLight
49.61
49.7
55.48
68.95
64.65

Pole
21.52
26.01
31.92
34.28
26.6

Table 3: Semantic Segmentation Results (Mean IoU) on CityScapes for different DOF (Farneback) representations

Type
RGBF (Mag only)
RGBF (Mag & Dir)
RGBF (Color Wheel)
RGB+F (3 layers Mag only)
RGB+F (Color Wheel)

Mean
47.8
54.6
57.2
62.1
62.56

Bicycle
52.63
57.28
61.47
65.15
63.65

Person
55.82
58.63
62.18
65.44
66.3

Rider
31.08
33.56
35.13
32.59
39.65

Motorcycle
22.38
18.49
22.68
33.19
47.22

Bus
39.34
56.44
54.87
63.07
66.24

Car
82.75
87.6
87.45
89.48
89.63

Train
22.8
41.15
36.69
43.6
51.02

Building
80.43
84.41
86.28
87.88
87.13

Road
92.24
95.4
95.94
96.17
96.4

Truck
20.7
31.8
40.2
57.2
36.11

Sky
81.87
87.86
90.07
91.48
90.64

TrafﬁcSign
44.08
44.26
51.64
55.76
60.68

Table 4: Semantic Segmentation Results (Mean IoU) on Virtual KITTI dataset

Type
RGB
F (GT)
RGBF (GT)
RGB+F (GT)
F (Flownet)
RGB+F (Flownet)

Mean
66.47
42
65.328
70.52
28.6
68.84

Truck
33.66
36.2
70.74
71.79
24.6
60.05

Van
29.04
20.7
48.34
56.8
14.3
40.54

Road
95.91
62.6
93.6
96.19
57.9
96.05

Sky
93.91
93.9
93.3
93.5
68
91.73

Vegetation
80.92
19.54
70.79
83.4
13.4
84.54

Building
68.15
34
62.05
66.53
4.9
68.52

Guardrail
81.82
15.23
67.86
82.6
0.8
82.43

TrafﬁcSign
66.01
51.5
55.14
64.69
31.8
65.2

TrafﬁcLight
65.07
33.2
55.48
64.65
18.5
63.54

Pole
40.91
29.3
31.9
26.6
6.6
26.54

Table 5: Semantic Segmentation Results (Mean IoU) on Cityscapes dataset

Type
RGB
F (Farneback)
RGBF (Farneback)
RGB+F (Farneback)
F (Flownet)
RGBF (Flownet)
RGB+F (Flownet)

Mean
62.47
34.7
47.8
62.56
36.8
52.3
62.43

Bicycle
63.52
34.48
52.6
63.65
32.9
54.9
64.2

Rider
40.49
12.7
31.1
39.65
26.8
34.8
40.9

Motorcycle
29.96
7.39
22.4
47.22
5.12
26.1
40.76

Bus
62.13
31.4
39.34
66.24
25.99
53.7
66.05

Car
89.16
74.3
82.75
89.63
75.29
83.6
90.03

Train
44.19
11.35
22.8
51.02
15.1
40.7
41.3

Building
87.86
72.77
80.43
87.13
65.16
79.4
87.3

Road
96.22
91.2
92.24
96.4
90.75
94
95.8

Truck
48.54
19.42
20.7
36.1
25.46
28.1
54.7

Sky
89.79
79.6
81.87
90.64
50.16
79.4
91.07

TrafﬁcSign
59.88
11.4
44.08
60.68
29.14
45.5
58.21

Car
85.69
55.2
80.2
91.4
47.8
90.87

Person
67.93
37.9
55.8
66.3
50.9
58.9
66.32

ever, the two stream network is computationally more
complex with more parameters relative to the RGBF
network. The main difference is the fusion at the en-
coder stage rather than early fusion in RGBF network.
RGB+F network also has the advantage of being able
to re-use pre-trained image encoders as it is decoupled
from ﬂow.

4 Experiments

In this section, we present the datasets used, ex-

perimental setup and results.

4.1 Datasets

We had the following criteria for choosing the seman-
tic segmentation datasets for our experiments. Firstly,
the dataset has to be for automotive road scenes as
our application is focused on automated driving. Sec-
ondly, we needed a mechanism to have the previous
image for computational of optical ﬂow. We chose
two datasets namely Virtual KITTI (Gaidon et al.,
2016) and Cityscapes (Cordts et al., 2016) datasets
which satisfy the above criteria. Cityscapes is a pop-
ular automotive semantic segmentation dataset devel-

oped by Daimler. It comprises of 5000 images hav-
ing full semantic segmentation annotation and 20,000
images with coarse annotations. We only use the ﬁne
annotations in our experiment and evaluated perfor-
mance on the validation set that comprises of 500 im-
ages. We also chose Virtual KITTI dataset for ob-
taining ground truth for ﬂow which can be used as a
proxy for a perfect ﬂow estimator.
It is a synthetic
dataset that consists of 21,260 frames generated using
Unity game engine in urban environment under differ-
ent weather conditions. Virtual KITTI provides many
annotations of which two are utilized in our approach
namely dense ﬂow and semantic segmentation. We
split the dataset into 80% training data and 20% test-
ing data. The semantic information is given for 14
classes including static and moving objects.

4.2 Experimental Setup

For all experiments, Adam optimizer is used with a
learning rate of 1e−5. L2 regularization is used in the
loss function with a factor of value of 5e−4 to avoid
over-ﬁtting the data. The encoder is initialized with
VGG pre-trained weights on ImageNet. Dropout with
probability 0.5 is utilized for 1x1 convolutional lay-
ers. Input image resolution of 375x1242 is used for
Virtual KITTI and 1024x2048 is used for Cityscapes

which is downsized to 512x1024 during training. In-
tersection over Union (IoU) is used as the perfor-
mance metric for each class separately and an aver-
age IoU is computed over all the classes. In addition
to IoU, precision, recall and F-score were calculated
for Virtual KITTI dataset.

4.3 Experimental Results

We provide several tables of qualitative evaluation on
the two datasets and discuss the impact of various
classes due to ﬂow augmentation. We also provide
video links on outputs of the four architectures on test
sequences in both datasets. In case of Virtual KITTI,
we use the synthetic ﬂow annotation as input. This is
used to simulate a perfect ﬂow estimator so that it can
act as a best case baseline. Then it is compared with
FlowNet v2 algorithm. In case of Cityscapes, we use
a commonly used dense optical ﬂow estimation algo-
rithm namely Farneback’s algorithm in OpenCV 3.1
with default parameters. We intentionally use this al-
gorithm to understand the effects of relatively noisy
ﬂow estimations and compare it with FlowNet v2.

Table 1 shows the evaluation of our four architec-
tures on Virtual KITTI dataset with different metrics.
Flow augmentation consistently provides improve-
ment in all four metrics. There is an improvement of
4% in IoU, 4.3% in Precision, 4.36% in Recall and
5.89% in F-score. Tables 2 and 3 shows a quantitative
comparison of different DOF representations in
RGBF and RGB+F architectures for both VKITTI
and Cityscapes datasets. Results show that Color
Wheel based representation in RGB+F architecture
provides the best capability of capturing the motion
cues and thus the best semantic segmentation accu-
racy. For all the following experiments, we make use
of Color Wheel representation format for ﬂow.

Table 4 shows detailed evaluation for each class
separately. Although the overall improvement is in-
cremental, there is a large improvement for certain
classes, for example, trucks, van, car show an im-
provement of 38.14%, 27.8% and 5.8% respectively.
Table 5 shows the evaluation of our proposed algo-
rithm on Cityscapes dataset and shows a marginal im-
provement of 0.1% in overall IoU. However there is
a signiﬁcant improvement in moving object classes
like motorcycle and train by 17% and 7% even us-
ing noisy ﬂow maps. It is important to note here that
the average IoU is dominated by road and sky pix-
els as there is no pixel frequency based normaliza-
tion and the obtained improvement in moving object
classes is still signiﬁcant. Figure 2 and Figure 3 il-
lustrates qualitative results of the four architectures

on Virtual KITTI and Cityscapes respectively. Fig-
ure 2 (f) shows better detection of the van which has
a uniform texture and ﬂow cue has helped to detect
it better. Figure 2 (h) shows that ﬂownet provides
good segmentation for the moving van, however fu-
sion in Figure 2 (i) still needs to be improved. Fig-
ure 3 (f) and (h) illustrate better detection of the bi-
cycle and the cyclist after augmenting DOF. These
examples visually verify the accuracy improvement
obtained as shown in Table 4 and Table 5. Unexpect-
edly, Table 5 shows that Farneback and FlowNet v2
provides similar results, however FlowNet shows bet-
ter results for some classes like Truck. One of the
design goals in this work is to have a computation-
ally efﬁcient model, our most complex architecture
namely RGB+F runs at 4 fps on a TitanX Maxwell
architecture GPU. Autonomous driving systems need
real-time performance and better run-time can be ob-
tained by using a more efﬁcient encoder instead of the
standard encoder we used in our experiments.

Semantic segmentation results in test sequences of
both datasets for all the four architectures along with
ground truth and original images are shared publicly
in YouTube. The Virtual KITTI results are available
in 1 and Cityscapes results are available in 2. We in-
cluded ﬂow only network to conduct an ablation study
to understand what performance ﬂow alone can pro-
duce. To our surprise, it produces good results espe-
cially for road, vegetation, vehicles and pedestrians.
We noticed that there is degradation of accuracy rela-
tive to RGB baseline whenever there is noisy ﬂow. It
would be interesting to incorporate a graceful degra-
dation via a loose coupling of ﬂow in the network.
In spite of ﬂow only network showing good perfor-
mance, the joint network only shows incremental im-
provement. It is likely that the simple networks we
constructed do not effectively combine ﬂow and RGB
as they are completely different modalities with dif-
ferent properties.
It could also be possible that the
RGB only network is implicitly learning geometric
ﬂow cues. In our future work, we plan to understand
this better and construct better multi-modal architec-
tures to effectively combine color and ﬂow.

4.4 Future Work

The scope of this work is to construct simple archi-
tectures to demonstrate the beneﬁt of ﬂow augmen-
tation to standard CNN based semantic segmentation
networks. The improvement in accuracy obtained and
visual veriﬁcation of test sequences shows that there
is still a lot of scope for improvement. Flow and

1https://youtu.be/4M1lS2-2w5U
2https://youtu.be/VtFLpklatrQ

(a) Input Image

(b) Input DOF (Ground Truth)

(c) RGB only

(d) Flow only (Ground Truth)

(e) RGBF (Ground Truth)

(f) RGB + F (Ground Truth)

(g) Input DOF ( FlowNet)

(h) Flow only ( FlowNet)

(i) RGB+F ( FlowNet)

(j) Ground Truth

Figure 2: Qualitative comparison of semantic segmentation outputs from four architectures on Virtual KITTI dataset

color are different modalities and an explicit synergis-
tic model would probably produce better results com-
pared to learning their relationships from data. We
summarize the list of future work below:
(1) Understand the effect of different encoders of
varying complexity like Resnet-10, Resnet-101, etc.
(2) Evaluation of state-of-the-art CNN based ﬂow es-
timators and joint multi-task learning.
(3) Construction of better multi-modal fusion archi-
tectures.
(4) Auxiliary loss induced ﬂow feature learning in se-
mantic segmentation architecture.
(5) Incorporating sparsity invariant metrics to handle
missing ﬂow estimates.

5 Conclusion

In this paper, we explored the problem of leverag-
ing ﬂow in semantic segmentation networks. In appli-
cations like automated driving, ﬂow is already com-
puted in geometric vision pipeline and can be utilized.
We constructed four variants of semantic segmenta-
tion networks which use RGB only, ﬂow only, RGBF
concatenated and two-stream RGB and ﬂow. We
evaluated these networks on two automotive datasets
namely Virtual KITTI and Cityscapes. We provided
class-wise accuracy scores and discussed them quali-
tatively. The simple ﬂow augmentation architectures
demonstrate a good improvement for moving object

(a) Input Image

(b) Input DOF (Farneback)

(c) RGB only output

(d) Flow only (Farneback)

(e) RGBF output (Farneback)

(f) RGB + F (Farneback)

(g) Input DOF ( FlowNet)

(h) RGB + F ( FlowNet)

(i) DOF only (FlowNet)

(j) Ground Truth

Figure 3: Qualitative comparison of semantic segmentation outputs from four architectures on Cityscapes dataset

classes which are important for automated driving.
We hope that this study encourages further research
in construction of better ﬂow-aware networks to fully
utilize its complementary nature.

REFERENCES

Badrinarayanan, V., Kendall, A., and Cipolla, R. (2015).
Segnet: A deep convolutional encoder-decoder ar-
arXiv preprint
chitecture for image segmentation.
arXiv:1511.00561.

Cordts, M., Omran, M., Ramos, S., Rehfeld, T., En-
zweiler, M., Benenson, R., Franke, U., Roth, S., and
Schiele, B. (2016). The cityscapes dataset for se-
arXiv preprint
mantic urban scene understanding.
arXiv:1604.01685.

Drayer, B. and Brox, T. (2016). Object detection, tracking,
and motion segmentation for object-level video seg-
mentation. arXiv preprint arXiv:1608.03066.

Farabet, C., Couprie, C., Najman, L., and LeCun, Y.
(2013). Learning hierarchical features for scene label-
ing. IEEE transactions on pattern analysis and ma-
chine intelligence, 35(8):1915–1929.

Fragkiadaki, K., Arbelaez, P., Felsen, P., and Malik, J.
(2015).
Learning to segment moving objects in
In Proceedings of the IEEE Conference
videos.
on Computer Vision and Pattern Recognition, pages
4083–4090.

Gadde, R., Jampani, V., and Gehler, P. V. (2017). Seman-
tic video cnns through representation warping. CoRR,
abs/1708.03088, 8:9.

Gaidon, A., Wang, Q., Cabon, Y., and Vig, E. (2016). Vir-
tual worlds as proxy for multi-object tracking analy-
sis. In CVPR.

Grangier, D., Bottou, L., and Collobert, R. (2009). Deep
In ICML

convolutional networks for scene parsing.
2009 Deep Learning Workshop, volume 3. Citeseer.

Hazirbas, C., Ma, L., Domokos, C., and Cremers, D.
(2016). Fusenet: Incorporating depth into semantic
In
segmentation via fusion-based cnn architecture.
Asian Conference on Computer Vision, pages 213–
228. Springer.

Horgan, J., Hughes, C., McDonald, J., and Yogamani, S.
(2015). Vision-based driver assistance systems: Sur-
In Intelligent Trans-
vey, taxonomy and advances.
portation Systems (ITSC), 2015 IEEE 18th Interna-
tional Conference on, pages 2032–2039. IEEE.
Hur, J. and Roth, S. (2016). Joint optical ﬂow and tem-
In Euro-
porally consistent semantic segmentation.
pean Conference on Computer Vision, pages 163–177.
Springer.

Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A.,
and Brox, T. (2016). Flownet 2.0: Evolution of optical
ﬂow estimation with deep networks. arXiv preprint
arXiv:1612.01925.

Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A.,
and Brox, T. (2017). Flownet 2.0: Evolution of op-
In IEEE
tical ﬂow estimation with deep networks.

conference on computer vision and pattern recogni-
tion (CVPR), volume 2, page 6.

Jain, S. D., Xiong, B., and Grauman, K. (2017). Fusionseg:
Learning to combine motion and appearance for fully
automatic segmention of generic objects in videos.
arXiv preprint arXiv:1701.05384.

Long, J., Shelhamer, E., and Darrell, T. (2015). Fully con-
volutional networks for semantic segmentation.
In
Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, pages 3431–3440.
Menze, M. and Geiger, A. (2015). Object scene ﬂow for au-
tonomous vehicles. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition,
pages 3061–3070.

Nilsson, D. and Sminchisescu, C. (2016). Semantic video
segmentation by gated recurrent ﬂow propagation.
arXiv preprint arXiv:1612.08871, 2.

Noh, H., Hong, S., and Han, B. (2015). Learning de-
convolution network for semantic segmentation.
In
Proceedings of the IEEE International Conference on
Computer Vision, pages 1520–1528.

Ochs, P., Malik, J., and Brox, T. (2014). Segmentation of
IEEE
moving objects by long term video analysis.
transactions on pattern analysis and machine intelli-
gence, 36(6):1187–1200.

Papazoglou, A. and Ferrari, V. (2013). Fast object segmen-
tation in unconstrained video. In Proceedings of the
IEEE International Conference on Computer Vision,
pages 1777–1784.

Sevilla-Lara, L., Sun, D., Jampani, V., and Black, M. J.
(2016). Optical ﬂow with semantic segmentation and
In Proceedings of the IEEE Con-
localized layers.
ference on Computer Vision and Pattern Recognition,
pages 3889–3898.

Siam, M., Elkerdawy, S., Jagersand, M., and Yogamani, S.
(2017a). Deep semantic segmentation for automated
driving: Taxonomy, roadmap and challenges. arXiv
preprint arXiv:1707.02432.

Siam, M., Mahgoub, H., Zahran, M., Yogamani, S., Jager-
sand, M., and El-Sallab, A. (2017b). Modnet: Mov-
ing object detection network with motion and ap-
arXiv preprint
pearance for autonomous driving.
arXiv:1709.04821.

Simonyan, K. and Zisserman, A. (2014). Two-stream con-
volutional networks for action recognition in videos.
In Advances in neural information processing sys-
tems, pages 568–576.

Tokmakov, P., Alahari, K., and Schmid, C. (2016). Learn-
arXiv preprint

ing motion patterns in videos.
arXiv:1612.07217.

Torr, P. H. (1998). Geometric motion segmentation and
model selection. Philosophical Transactions of the
Royal Society of London A: Mathematical, Physical
and Engineering Sciences, 356(1740):1321–1340.
Vijayanarasimhan, S., Ricco, S., Schmid, C., Sukthankar,
R., and Fragkiadaki, K. (2017). Sfm-net: Learning
of structure and motion from video. arXiv preprint
arXiv:1704.07804.

Wehrwein, S. and Szeliski, R. (2017). Video segmentation

with background motion models. In BMVC.

