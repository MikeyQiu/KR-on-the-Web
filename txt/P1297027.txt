Multimodal Task-Driven Dictionary Learning
for Image Classiﬁcation

Soheil Bahrampour, Member, IEEE, Nasser M. Nasrabadi, Fellow, IEEE, Asok Ray, Fellow, IEEE,
and W. Kenneth Jenkins, Life Fellow, IEEE

1

5
1
0
2
 
t
c
O
 
7
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
9
0
1
0
.
2
0
5
1
:
v
i
X
r
a

Abstract—Dictionary learning algorithms have been suc-
cessfully used for both reconstructive and discriminative tasks,
where an input signal is represented with a sparse linear
combination of dictionary atoms. While these methods are
mostly developed for single-modality scenarios, recent studies
have demonstrated the advantages of feature-level fusion
based on the joint sparse representation of the multimodal
inputs. In this paper, we propose a multimodal task-driven
dictionary learning algorithm under the joint sparsity con-
straint (prior) to enforce collaborations among multiple ho-
mogeneous/heterogeneous sources of information. In this task-
driven formulation, the multimodal dictionaries are learned
simultaneously with their corresponding classiﬁers. The re-
sulting multimodal dictionaries can generate discriminative
latent features (sparse codes) from the data that are optimized
for a given task such as binary or multiclass classiﬁca-
tion. Moreover, we present an extension of the proposed
formulation using a mixed joint and independent sparsity
prior which facilitates more ﬂexible fusion of the modalities
at feature level. The efﬁcacy of the proposed algorithms
for multimodal classiﬁcation is illustrated on four different
applications – multimodal face recognition, multi-view face
recognition, multi-view action recognition, and multimodal
biometric recognition. It is also shown that, compared to
the counterpart reconstructive-based dictionary learning algo-
rithms, the task-driven formulations are more computationally
efﬁcient in the sense that they can be equipped with more
compact dictionaries and still achieve superior performance.

Index Terms—Dictionary learning, Multimodal classiﬁca-

tion, Sparse representation, Feature fusion

I. INTRODUCTION
It is well established that information fusion using multi-
ple sensors can generally result in an improved recognition
performance [1]. It provides a framework to combine
local information from different perspectives which is more
tolerant to the errors of individual sources [2], [3]. Fusion
methods for classiﬁcation are generally categorized into
feature fusion [4] and classiﬁer fusion [5] algorithms.
Feature fusion methods aggregate extracted features from
different sources into a single feature set which is then used
for classiﬁcation. On the other hand, classiﬁer fusions algo-
rithms combine decisions from individual classiﬁers, each

When this work was achieved, S. Bahrampour, A. Ray, and W. K.
Jenkins were with the Department of Electrical Engineering, Pennsylvania
State University, University Park, PA 16802, USA; N. M. Nasrabadi was
with the Army Research Laboratory, Adelphi, MD 20783. S. Bahrampour
is now with Bosch Research and Technology Center, Palo Alto, CA; N. M.
Nasrabadi is now with the Computer Science and Electrical Engineering
Department at the West Virginia University, WV.

soheil.bahrampour@us.bosch.com, nassernasrabadi@mail.wvu.edu,

{axr2, wkj1}@psu.edu.

of which is trained using separate sources. While classiﬁer
fusion is a well-studied topic, fewer studies have been done
for feature fusion, mainly due to the incompatibility of the
feature sets [6]. A naive way of feature fusion is to stack
the features into a longer one [7]. However this approach
usually suffers from the curse of dimensionality due to the
limited number of training samples [4]. Even in scenarios
with abundant training samples, concatenation of feature
vectors does not take into account the relationship among
the different sources and it may contain noisy or redundant
data, which degrade the performance of the classiﬁer [6].
However, if these limitations are mitigated, feature fusion
can potentially result
in improved classiﬁcation perfor-
mance [8], [9].

Sparse representation classiﬁcation has recently attracted
the interest of many researchers in which the input sig-
nal is approximated with a linear combination of a few
dictionary atoms [10] and has been successfully applied
to several problems such as robust face recognition [10],
visual tracking [11], and transient acoustic signal classi-
ﬁcation [12]. In this approach, a structured dictionary is
usually constructed by stacking all the training samples
from the different classes. The method has also been
expanded for efﬁcient feature-level fusion which is usually
referred to as multi-task learning [13], [14], [15], [16].
Among different proposed sparsity constraints (priors), joint
sparse representation has shown signiﬁcant performance
improvement
in several multi-task learning applications
such as target classiﬁcation, biometric recognitions, and
multiview face recognition [12], [14], [17], [18]. The un-
derlying assumption is that the multimodal test input can
be simultaneously represented by a few dictionary atoms,
or training samples, from a multimodal dictionary, that
represents all the modalities and, therefore, the resulting
sparse coefﬁcients should have the same sparsity pattern.
However, the dictionary constructed by the collection of
the training samples suffer from two limitations. First, as
the resulting
the number of training samples increases,
optimization problem becomes more computationally de-
manding. Second, the dictionary that is constructed this way
is not optimal neither for the reconstructive tasks [19] nor
the discriminative tasks [20].

Recently it has been shown that learning the dictionary
can overcome the above limitations and signiﬁcantly im-
prove the performance in several applications including
image restoration [21], face recognition [22] and object
recognition [23], [24]. The learned dictionaries are usu-

2

ally more compact and have fewer dictionary atoms than
the number of training samples [25], [26]. Dictionary
learning algorithms can generally be categorized into two
groups: unsupervised and supervised. Unsupervised dictio-
nary learning algorithms such as the method of optimal
direction [27] and K-SVD [25] are aimed at ﬁnding a
dictionary that yields minimum errors when adapted to
reconstruction tasks such as signal denoising [28] and im-
age inpainting [19]. Although, the unsupervised dictionary
learning has also been used for classiﬁcation [22], it has
been shown that better performance can be achieved by
learning the dictionaries that are adapted to an speciﬁc
task rather than just the data set [29], [30]. These methods
are called supervised, or task-driven, dictionary learning
algorithms. For the classiﬁcation task, for example, it is
more meaningful to utilize the labeled data to minimize
the misclassiﬁcation error rather than the reconstruction
error [31]. Adding a discriminative term to the recon-
struction error and minimizing a trade-off between them
has been proposed in several formulations [20], [24],
[32], [33]. The incoherent dictionary learning algorithm
proposed in [34] is another supervised formulation which
trains class-speciﬁc dictionaries to minimize atom sharing
between different classes and uses sparse representation
for classiﬁcation. In [35], a Fisher criterion is proposed to
learn structured dictionaries such that the sparse coefﬁcients
have small within-class and large between-class scatters.
While unsupervised dictionary learning can be reformulated
as a large scale matrix factorization problem and solved
efﬁciently [19], supervised dictionary learning is usually
more difﬁcult to optimize. More recently, it has been shown
that better optimization tool can be used to tackle the
supervised dictionary learning [30], [36]. This is achieved
by formulating it as a bilevel optimization problem [37],
[38]. In particular, a stochastic gradient descent algorithm
has been proposed in [29] which efﬁciently solves the
dictionary learning problem in a uniﬁed framework for
different tasks, such as classiﬁcation, nonlinear image map-
ping, and compressive sensing.

The majority of the existing dictionary learning algo-
rithms, including the task-driven dictionary learning [29],
are only applicable to single source of data. In [39], a set
of view-speciﬁc dictionaries and a common dictionary are
learned for the application of multi-view action recogni-
tion. The view-speciﬁc dictionaries are trained to exploit
view-level correspondence while the common dictionary is
trained to capture common patterns shared among the dif-
ferent views. The proposed formulation belongs to the class
of dictionary learning algorithms that leverages the labeled
samples to learn class-speciﬁc atoms while minimizing
the reconstruction error. Moreover, it cannot be used for
fusion of the heterogeneous modalities. In [40], a generative
multimodal dictionary learning algorithm is proposed to
extract typical templates of multimodal features. The tem-
plates represent synchronous transient structures between
modalities which can be used for localization applications.
More recently, a multimodal dictionary learning algorithm
with joint sparsity prior is proposed in [41] for multimodal

Fig. 1: Multimodal task-driven dictionary learning scheme.

retrieval where the task is to ﬁnd relevant samples from
other modalities for a given unimodal query. However,
the proposed formulation cannot be readily applied for
information fusion in which the task is to ﬁnd label of a
given multimodal query. Moreover, the joint sparsity prior
is used in [41] to couple similarly labeled samples within
each modality and is not utilized to extract cross-modality
information which is essential for information fusion [12].
Furthermore,
the dictionaries in [41] are learned to be
generative by minimizing the reconstruction error of data
across modalities and, therefore, are not necessary optimal
for discriminative tasks [31].

This paper focuses on learning discriminative multimodal
dictionaries. The major contributions of the paper are as
follows:

• Formulation of

the multimodal dictionary learning
algorithms: A multimodal task-driven dictionary learn-
ing algorithm is proposed for classiﬁcation using ho-
mogeneous or heterogeneous sources of information.
Information from different modalities are fused both
at the feature level, by using the joint sparse repre-
sentation, and at the decision level, by combining the
scores of the modal-based classiﬁers. The proposed
formulation simultaneously trains the multimodal dic-
tionaries and classiﬁers under the joint sparsity prior in
order to enforce collaborations among the modalities
and obtain the latent sparse codes as the optimized
features for different tasks such as binary and mul-
ticlass classiﬁcation. Fig. 1 presents an overview of
the proposed framework. An unsupervised multimodal
dictionary learning algorithm is also presented as a by-
product of the supervised version.

• Differentiability of the bi-level optimization problem:
The main difﬁculty in proposing such a formulation
is that the solution of the corresponding joint sparse
coding problem is not differentiable with respect to
the dictionaries. While the joint sparse coding has
a non-smooth cost function,
is shown here that
it is locally differentiable and the resulting bi-level
optimization for task-driven multimodal dictionary
learning is smooth and can be solved using a stochastic

it

gradient descent algorithm. 1

• Flexible feature-level

fusion: An extension of the
proposed framework is presented which facilitates
more ﬂexible fusion of the modalities at the feature
level by allowing the modalities to have different
sparsity patterns. This extension provides a frame-
work to tune the trade-off between independent sparse
representation and joint sparse representation among
the modalities. Improved performance for multimodal
classiﬁcation: The proposed methods achieve the state-
of-the-art performance in a range of different multi-
modal classiﬁcation tasks.
In particular, we have
provided extensive performance comparison between
the proposed algorithms and some of the competing
tasks of
methods from literature for four different
multimodal face recognition, multi-view face recog-
nition, multimodal biometric recognition, and multi-
view action recognition. The experimental results on
these datasets have demonstrated the usefulness of
the proposed formulation, showing that the proposed
algorithm can be readily applied to several different
application domains.

• Improved efﬁciency for sparse-representation based
classiﬁcation: It is shown here that, compared to the
counterpart sparse representation classiﬁcation algo-
rithms, the proposed algorithms are more computa-
tionally efﬁcient in the sense that they can be equipped
with more compact dictionaries and still achieve su-
perior performance.

A. Paper organization

The rest of the paper is organized as follows. In Sec-
tion II, unsupervised and supervised dictionary learning
algorithms for single source of information are reviewed.
Joint sparse representation for multimodal classiﬁcation is
also reviewed in this section. Section III proposes the task-
driven multimodal dictionary learning algorithms. Compar-
ative studies on several benchmarks and concluding results
are presented in Section IV and Section V, respectively.

B. Notation

Vectors are denoted by bold lower case letters and
matrices by bold upper case letters. For a given vector x,
xi is its ith element. For a given ﬁnite set of indices γ,
xγ is the vector formed with those elements of x indexed
in γ. Symbol → is used to distinguish the row vectors
from column vectors, i.e. for a given matrix X, the ith row
and jth column of matrix are represented as xi→ and xj,
respectively. For a given ﬁnite set of indices γ, Xγ is the
matrix formed with those columns of X indexed in γ and
Xγ→ is the matrix formed with those rows of X indexed
in γ. Similarly, for given ﬁnite sets of indices γ and ψ,
Xγ→,ψ is the matrix formed with those rows and columns
of X indexed in γ and ψ, respectively. xij is the element

1The source code of the proposed algorithm is released here: https:

//github.com/soheilb/multimodal dictionary learning

3

of X at row i and column j. The lq norm, q ≥ 1, of a
vector x ∈ Rm is deﬁned as (cid:107)x(cid:107)(cid:96)q = ((cid:80)m
j=1 |xj|q)1/q.
The Frobenius norm and (cid:96)1q norm, q ≥ 1, of matrix
(cid:17)1/2
X ∈ Rm×n is deﬁned as (cid:107)X(cid:107)F =
and (cid:107)X(cid:107)(cid:96)1q = (cid:80)m
{xi|i ∈ γ} is shortly denoted as {xi}.

i=1 (cid:107)xi→(cid:107)(cid:96)q , respectively. The collection

(cid:16)(cid:80)m
i=1

j=1 x2
ij

(cid:80)n

II. BACKGROUND

A. Dictionary learning

Dictionary learning has been widely used in various tasks
such as reconstruction, classiﬁcation, and compressive sens-
ing [29], [33], [42], [43]. In contrast to principal component
analysis (PCA) and its variants, dictionary learning algo-
rithms generally do not impose orthogonality condition and
are more ﬂexible allowing to be well-tuned to the training
data. Let X = [x1, x2, . . . , xN ] ∈ Rn×N be the collection
of N (normalized) training samples that are assumed to be
statistically independent. Dictionary D ∈ Rn×d can then
be obtained as the minimizer of the following empirical
cost [22]:

gN (D) (cid:44) 1
N

N
(cid:88)

i=1

lu (xi, D)

(1)

the

regularizing convex set D (cid:44) {D ∈
over
Rn×d|(cid:107)dk(cid:107)(cid:96)2 ≤ 1, ∀k = 1, . . . , d}, where dk is the kth
column, or atom, in the dictionary and the unsupervised
loss lu is deﬁned as
lu (x, D) (cid:44) min
α∈Rd

+λ1(cid:107)α(cid:107)(cid:96)1 +λ2(cid:107)α(cid:107)2
(cid:96)2

(cid:107)x−Dα(cid:107)2
(cid:96)2

, (2)

which is the optimal value of the sparse coding problem
with λ1 and λ2 being the regularizing parameters. While
λ2 is usually set to zero to exploit sparsity, using λ2 > 0
makes the optimization problem in Eq. (2) strongly convex
resulting in a differentiable cost function [29]. The index u
of lu is used to emphasize that the above dictionary learning
formulation is an unsupervised method. It is well-known
that one is often interested in minimizing an expected
risk, rather than the perfect minimization of the empirical
cost [44]. An efﬁcient online algorithm is proposed in [19]
to ﬁnd the dictionary D as the minimizer of the following
stochastic cost over the convex set D:

g (D) (cid:44) Ex [lu (x, D)] ,

(3)

where it is assumed that the data x is drawn from a ﬁnite
probability distribution p(x) which is usually unknown
and Ex [.] is the expectation operator with respect to the
distribution p(x).

The trained dictionary can then be used to (sparsely)
reconstruct the input. The reconstruction error has been
shown to be a robust measure for classiﬁcation tasks [10],
[45]. Another use of a given trained dictionary is for feature
extraction where the sparse code α(cid:63)(x, D), obtained as a
solution of (2), is used as a feature vector representing the
input signal x in the classical expected risk optimization
for training a classiﬁer [29]:

min
w∈W

Ey,x [l (y, w, α(cid:63)(x, D))] +

ν
2

(cid:107)w(cid:107)2
(cid:96)2

,

(4)

4

where y is the ground truth class label associated with
the input x, w is model (classiﬁer) parameters, ν is a
regularizing parameter, and l is a convex loss function that
measures how well one can predict y given the feature
vector α(cid:63) and classiﬁer parameters w. The expectation
Ey,x is taken with respect to the probability distribution
p(y, x) of the labeled data. Note that in Eq. 4, the dictionary
D is ﬁxed and independent of the given task and class label
y. In task-driven dictionary learning, on the other hand,
a supervised formulation is used which ﬁnds the optimal
dictionary and classiﬁer parameters jointly by solving the
following optimization problem [29]:

min
D∈D,w∈W

Ey,x [lsu (y, w, α(cid:63)(x, D))] +

(cid:107)w(cid:107)2
(cid:96)2

.

(5)

ν
2

The index su of convex loss function lsu is used to
emphasize that the above dictionary learning formulation
is supervised. The learned task-driven dictionary has been
shown to result in a superior performance compared to the
unsupervised setting [29]. In this setting, the sparse codes
are indeed the optimized latent features for the classiﬁer.

B. Multimodal joint sparse representation

Joint sparse representation provides an efﬁcient tool for
feature-level fusion of sources of information [12], [14],
[46]. Let S (cid:44) {1, . . . , S} be a ﬁnite set of available
modalities and let xs ∈ Rns
, s ∈ S, be the feature
vector for the sth modality. Also let Ds ∈ Rns×d be
the corresponding dictionary for the sth modality. For
now, it is assumed that the multimodal dictionaries are
constructed by collections of the training samples from
i.e. jth atom of dictionary Ds is
different modalities,
the jth training sample from the sth modality. Given a
multimodal input {xs|s ∈ S}, shortly denoted as {xs}, an
optimal sparse matrix A(cid:63) ∈ Rd×S is obtained by solving
the following (cid:96)12-regularized reconstruction problem:

argmin
A=[α1...αS ]

1
2

S
(cid:88)

s=1

(cid:107)xs − Dsαs(cid:107)2
(cid:96)2

+ λ(cid:107)A(cid:107)(cid:96)12 ,

(6)

where λ is a regularization parameter. Here αs is the sth-
column of A which corresponds to the sparse representa-
tion for the sth modality. Different algorithms have been
proposed to solve the above optimization problem [47],
[48]. We use the efﬁcient alternating direction method
of multipliers (ADMM) [49] to ﬁnd A(cid:63). The (cid:96)12 prior
encourages row sparsity in A(cid:63), i.e. it encourages collab-
oration among all the modalities by enforcing the same
dictionary atoms from different modalities that present the
same event, to be used for reconstructing the inputs {xs}.
An (cid:96)11 term can also be added to the above cost function
to extend it to a more general framework where sparsity
can also be sought within the rows, as will be discussed
in Section III-D. It has been shown that
joint sparse
representation can result in a superior performance in fusing
multimodal sources of information compared to other infor-
mation fusion techniques [45]. We are interested in learning
multimodal dictionaries under the joint sparsity prior. This

has several advantages over a ﬁxed dictionary consisting of
training data. Most importantly, it can potentially remove
the redundant and noisy information by representing the
training data in a more compact form. Also using the
supervised formulation, one expects to ﬁnd dictionaries that
are well-adapted to the discriminative tasks.

III. MULTIMODAL DICTIONARY LEARNING

In this section, online algorithms for unsupervised and

supervised multimodal dictionary learning are proposed.

A. Multimodal unsupervised dictionary learning

Unsupervised multimodal dictionary learning is derived
by extending the optimization problem characterized in
Eq. (3) and using the joint sparse representation of (6) to
enforce collaborations among modalities. Let the minimum
u ({xs, Ds}) of the joint sparse coding be deﬁned as
cost l(cid:48)

min
A

1
2

S
(cid:88)

s=1

(cid:107)xs − Dsαs(cid:107)2
(cid:96)2

+ λ1(cid:107)A(cid:107)(cid:96)12 +

(cid:107)A(cid:107)2

F , (7)

λ2
2

where λ1 and λ2 are the regularizing parameters. The addi-
tional Frobenius norm (cid:107).(cid:107)F compared to Eq. (6) guarantees
a unique solution for the joint sparse optimization problem.
In the special case when S = 1, optimization (7) reduces
to the well-studied elastic-net optimization [50]. By natural
extension of the optimization problem (3), the unsupervised
multimodal dictionaries are obtained by:

Ds(cid:63) = argmin
Ds∈Ds

Exs [l(cid:48)

u ({xs, Ds})] , ∀s ∈ S,

(8)

where the convex set Ds is deﬁned as

Ds (cid:44) {D ∈ Rns×d|(cid:107)dk(cid:107)(cid:96)2 ≤ 1, ∀k = 1, . . . , d}.

(9)

It is assumed that data xs is drawn from a ﬁnite (un-
known) probability distribution p(xs). The above optimiza-
tion problem can be solved using the classical projected
stochastic gradient algorithm [51] which consists of a
sequence of updates as follows:

Ds ← ΠDs [Ds − ρt∇Ds l(cid:48)

u ({xs

t , Ds})] ,

(10)

where ρt is the gradient step at time t and ΠD is the
orthogonal projector onto set D. The algorithm converges
to a stationary point for a decreasing sequence of ρt [51],
[52]. A typical choice of ρt is shown in the next section.
This problem can also be solved using online matrix
factorization algorithm [26]. It should be noted that the
while the stochastic gradient descent does converge, it is
not guaranteed to converge to a global minimum due to
the non-convexity of the optimization problem [26], [44].
However, such stationary point is empirically found to be
sufﬁciently good for practical applications [21], [28].

B. Multimodal task-driven dictionary learning

As discussed in Section II, the unsupervised setting does
not take into account the label of the training data, and
the dictionaries are obtained by minimizing the reconstruc-
tion error. However, for classiﬁcation tasks, the minimum
reconstruction error does not necessarily result in discrimi-
native dictionaries. In this section, a multimodal task-driven
dictionary learning algorithm is proposed that enforces
collaboration among the modalities both at the feature level
using joint sparse representation and the decision level
using a sum of the decision scores. We propose to learn
the dictionaries Ds(cid:63), ∀s ∈ S, and the classiﬁer parameters
ws(cid:63), ∀s ∈ S, shortly denoted as the set {Ds(cid:63), ws(cid:63)}, jointly
as the solution of the following optimization problem:

min
{Ds∈Ds,ws∈W s}

f ({Ds, ws}) +

(cid:107)ws(cid:107)2
(cid:96)2

,

(11)

ν
2

S
(cid:88)

s=1

where f is deﬁned as the expected cumulative cost:

f ({Ds, ws}) = E

lsu(y, ws, αs(cid:63)),

(12)

S
(cid:88)

s=1

is

the

sth

column of

where αs(cid:63)
the minimizer
A(cid:63)({xs, Ds}) of
the optimization problem (7) and
lsu(y, w, α) is a convex loss function that measures how
well the classiﬁer parametrized by w can predict y by
observing α. The expectation is taken with respect to the
joint probability distribution of the multimodal inputs {xs}
and label y. Note that αs(cid:63) acts as a hidden/latent feature
vector, corresponding to the input xs, which is generated by
the learned discriminative dictionary Ds(cid:63). In general, lsu
can be chosen as any convex function such that lsu(y, ., .)
is twice continuously differentiable for all possible values
of y. A few examples are given below for binary and
multiclass classiﬁcation tasks.

1) Binary classiﬁcation: In a binary classiﬁcation task
where the label y belongs to the set {−1, 1}, lsu can be
naturally chosen as the logistic regression loss

lsu(y, w, α(cid:63)) = log(1 + e−ywT α(cid:63)

),

(13)

where w ∈ Rd is the classiﬁer parameters. Once the
optimal {Ds, ws} are obtained, a new multimodal sample
{xs} is classiﬁed according to sign of (cid:80)S
s=1 wsT α(cid:63) due
to the uniform monotonicity of (cid:80)S
s=1 lsu. For simplicity,
the intercept term for the linear model is omitted here,
but it can be easily added. One can also use a bilinear
model where, instead of a set of vectors {ws}, a set of
matrices {W s} are learned and a new multimodal sample
is classiﬁed according to the sign of (cid:80)S
s=1 xsT W sα(cid:63).
Accordingly, the (cid:96)2-norm regularization of Eq. (11) needs
to be replaced with the matrix Frobenius norm. The bilinear
model is richer than the linear model and can sometimes
result in better classiﬁcation performance but needs more
careful training to avoid over-ﬁtting.

5

2) Multiclass classiﬁcation: Multiclass classiﬁcation can
be formulated using a collections of (independently learned)
binary classiﬁers in a one-vs-one or one-vs-all setting.
Multiclass classiﬁcation can also be handled in an all-vs-all
setting using the softmax regression loss function. In this
scheme, the label y belongs to the set {1, . . . , K} and the
softmax regression loss is deﬁned as

lsu(y, W , α(cid:63)) = −

1{y=k} log

K
(cid:88)

k=1

(cid:32)

ewT
k α(cid:63)
l=1 ewT

(cid:80)K

l α(cid:63)

(cid:33)

,

(14)
where W = [w1 . . . wK] ∈ Rd×K, and 1{.} is the indicator
function. Once the optimal {Ds, W s} are obtained, a new
multimodal sample {xs} is classiﬁed as

argmaxk∈{1,...,K}

(cid:32)

S
(cid:88)

s=1

T αs(cid:63)

k

ews
l=1 ews

l

(cid:80)K

T αs(cid:63)

(cid:33)

.

(15)

In yet another all-vs-all setting, the multiclass classiﬁcation
task can be turned into a regression task in which the scaler
label y is changed to a binary vector y ∈ RK, where the
kth coordinate corresponding to the label of {xs} is set to
one and the rest of the coordinates are set to zero. In this
setting, lsu is deﬁned as

lsu(y, W , α(cid:63)) =

1
2
where W ∈ RK×d. Having obtained the optimal
{Ds, W s}, the test sample {xs} is then classiﬁed as

(cid:107)y − W α(cid:63)(cid:107)2
(cid:96)2

(16)

,

S
(cid:88)

argmink∈{1,...,K}

(cid:107)qk − W sαs(cid:63)(cid:107)2
(cid:96)2

,

(17)

s=1
where qk is a binary vector in which its kth coordinate is
one and its remaining coordinates are zero.

In choosing between the one-vs-all setting, in which
independent multimodal dictionaries are trained for each
class, and the multiclass formulation, in which multimodal
dictionaries are shared between classes, a few points should
be considered. In the one-vs-all setting, the total number of
dictionary atoms is equal to dSK in the K-class classiﬁ-
cation while in the multiclass setting the number is equal
to dS. It should be noted that in the multiclass setting a
larger dictionary is generally required to achieve the same
level of performance to capture the variations among all
classes. However, it is generally observed that the size
of the dictionaries in multiclass setting is not required to
grow linearly as the number of classes increases due to
atom sharing among the different classes. Another point to
consider is that the class-speciﬁc dictionaries of the one-
vs-all approach are independent and can be obtained in
parallel. In this paper, the multiclass formulation is used
to allow feature sharing among the classes.

C. Optimization

The main challenge in optimizing (11) is the non-
differentiability of A(cid:63)({xs, Ds}). However,
it can be
shown that although the sparse coefﬁcients A(cid:63) are obtained
by solving a non-differentiable optimization problem, the

6

function f ({Ds, ws}), deﬁned in Eq. (12), is differen-
tiable on D1 × · · · DS × W 1 × · · · W S, and therefore its
gradients are computable. To ﬁnd the gradient of f with
respect to Ds, one can ﬁnd the optimality condition of the
optimization (7) or use the ﬁxed point differentiation [36],
[38] and show that A(cid:63) is differentiable over its non-zero
label
rows. Without
y admits a ﬁnite set of values such as those deﬁned in
Eqs. (13) and (14). The same algorithm can be derived for
the scenario when y belongs to a compact subset of a ﬁnite-
dimensional real vector space as in Eq. (16). A couple of
mild assumptions are required to prove the differentiability
of f which are direct generalizations of those required for
the single modal scenario [29] and are listed below:

loss of generality, we assume that

Assumption (A). The multimodal data (y, {xs}) admit a

probability density p with compact support.

Assumption (B). For all possible values of y, p(y, .)
is continuous and lsu(y, .) is twice continuously differen-
tiable.

The ﬁrst assumption is reasonable when dealing with
the signal/image processing applications where the acquired
values obtained by the sensors are bounded. Also all the
given examples for lsu in the previous section satisfy the
second assumption. Before stating the main proposition of
this paper below, the term active set is deﬁned.

Deﬁnition 3.1 (Active set): The active set Λ of the solu-
tion A(cid:63) of the joint sparse coding problem (7) is deﬁned
to be

Λ = {j ∈ {1, . . . , d} : (cid:107)a(cid:63)

j→(cid:107)(cid:96)2 (cid:54)= 0},

(18)

where a(cid:63)

j→ is the jth row of A(cid:63).

Proposition 3.1 (Differentiability and gradients of f ):
Let λ2 > 0 and the assumptions (A) and (B) hold. Let
Υ = ∪j∈ΛΥj where Υj = {j, j + d, . . . , j + (S − 1)d}.
Let the matrix ˆD ∈ Rn×|Υ| be deﬁned as
ˆD =

(cid:104) ˆD1 . . . ˆD|Λ|

(19)

(cid:105)

,

where ˆDj = blkdiag(d1
j ) ∈ Rn×S, ∀j ∈ Λ, is
j , . . . , dS
the collection of the jth active atoms of the multimodal
j is the jth active atom of Ds, blkdiag is
dictionaries, ds
the block diagonalization operator, and n = (cid:80)
s∈S ns. Also
let matrix ∆ ∈ R|Υ|×|Υ| be deﬁned as

∆ = blkdiag(∆1, . . . , ∆|Λ|),

(20)

j→ ∈
I −
where ∆j =
RS×S, ∀j ∈ Λ, and I is the identity matrix. Then, the
function f deﬁned in Eq. (12) is differentiable and ∀s ∈ S,

(cid:107)a(cid:63)

(cid:107)a(cid:63)

j→

1
j→(cid:107)(cid:96)2

1
j→(cid:107)(cid:96)2

3 a(cid:63)

T a(cid:63)

∇wsf = E [∇ws lsu (y, ws, αs(cid:63))] ,
∇Dsf = E

(xs − Dsαs(cid:63)) βT

(cid:104)

˜s − Dsβ˜sαs(cid:63)T (cid:105)

,

(21)

where ˜s = {s, s + S, . . . , s + (d − 1)S} and β ∈ RdS is
deﬁned as

βΥc = 0, βΥ = ( ˆDT ˆD + λ1∆ + λ2I)−1g,

(22)
T (cid:80)S
s=1 lsu(y, ws, αs(cid:63))), Υc =
in which g = vec(∇A(cid:63)
{1, . . . , dS} \Υ, βΥ ∈ R|Υ| is formed of those rows of β
indexed by Υ, and vec(.) is the vectorization operator.

Λ→

The proof of this proposition is given in the Appendix.
A stochastic gradient descent algorithm to ﬁnd the optimal
dictionaries {Ds(cid:63)} and classiﬁers {ws(cid:63)} is described in
Algorithm 1. The stochastic gradient descent algorithm is
guaranteed to converge under a few assumptions that are
mildly stricter than those in this paper (requires three-times
differentiability) [53]. To further improve the convergence
of the proposed stochastic gradient descent algorithm, a
classic mini-batch strategy is used in which a small batch
of the training data are sampled in each batch, instead of 1
sample, and the parameters are updated using the averaged
updates of the batch. This has additional advantage in which
ˆDT ˆD and the corresponding factorization of the ADMM
for solving the sparse coding problem can be computed
once for the whole batch. For the special case when S = 1,
the proposed algorithm reduces to the single-modal task-
driven dictionary learning algorithm in [29]. Selecting λ2 in
Eq. (7) to be strictly positive guarantees the linear equations
of (22) to have a unique solution. In other words, it is easy
to show that the matrix ( ˆDT ˆD + λ1∆ + λ2I) is positive
deﬁnite given λ1 ≥ 0, λ2 > 0. However, in practice it is
observed that the solution of the joint sparse representation
problem is numerically stable since ˆD becomes full-column
rank when sparsity is sought with a sufﬁciently large λ1,
and λ2 can be set to zero. It should be noted that the
assumption of ˆD being a full column rank matrix is a
common assumption in sparse linear regression [26]. As
in any non-convex optimization algorithm, if the algorithm
is not initialized properly, it may yield poor performance.
Similar to [29], the dictionaries {Ds} are initialized by the
solution of the unsupervised multimodal dictionary learning
algorithm. Upon assignment of the initial dictionaries,
parameters {ws} of the classiﬁers are set by solving (11)
only with respect to {ws} which is a convex optimization
problem.

D. Extension

We now present an extension of the proposed algorithm
with a more ﬂexible structure on the sparse codes. Joint
sparse representation relies on the fact that all the modalities
share the same sparsity pattern in which, if a multimodal
training sample is selected to reconstruct the input, then
all the modalities within that training sample are active.
However, this group sparsity constraint, imposed by the
(cid:96)12 norm, may be too stringent for some applications [45],
[54], for example in the scenarios where the modalities
have different noise levels or when the heterogeneity of
the modalities imposes different sparsity levels for the
reconstruction task. A natural relaxation to the joint sparsity
prior is to let the multimodal inputs not share the full
active set which can be achieved by replacing the (cid:96)12 norm
with a combination of the (cid:96)12 and (cid:96)11 norms ((cid:96)12 − (cid:96)11
norm). Following the same formulation as in Section III-B,
let A(cid:63)({xs, Ds}) in Eq. (11) be the minimizer of the

Algorithm 1 Stochastic gradient descent algorithm for multi-
modal task-driven dictionary learning.

Input: Regularization parameters λ1, λ2, ν,

learning rate parameters
ρ, t0, number of iterations T , initial dictionaries {Ds ∈ Ds}s∈S ,
initial model parameters {ws ∈ W s}s∈S .

Output: Learned {Ds, ws}
1: for t = 1, . . . , T do
2:
3:

t , . . . , xS
Draw a random sample (x1
t , yt) from the training data.
Find solution A(cid:63) = (cid:2)α(cid:63)1 . . . α(cid:63)S (cid:3) ∈ Rd×S of the joint sparse
coding problem

argmin
A=[α1...αS]

1
2

S
(cid:88)

s=1

(cid:107)xs

t − Dsαs(cid:107)2
(cid:96)2

+ λ1(cid:107)A(cid:107)(cid:96)12 +

(cid:107)A(cid:107)2
F .

λ2
2

4:
5:
6:
7:

8:
9:

Compute set of active rows Λ of A(cid:63) using (18).
Compute ˆD ∈ Rn×|Υ| using (19).
Compute ∆ ∈ R|Υ|×|Υ| using (20).
Compute β ∈ RdS as:

βΥc = 0, βΥ = ( ˆDT ˆD + λ1∆ + λ2I)−1g,

T

Λ→

(cid:80)S

where Υ = ∪j∈Λ{j, j + d, . . . , j + (S − 1)d} and g =
s=1 lsu(yt, ws, αs(cid:63))).
vec(∇A(cid:63)
Choose the learning rate ρt ← min(ρ, ρ t0
Update the parameters by a projected gradient step:
ws ← ΠW s [ws − ρt (∇ws lsu (yt, ws, αs(cid:63)) + νws)] ,
Ds ← ΠDs

˜s − Dsβ˜sαs(cid:63)T (cid:17)(cid:105)

t − Dsαs(cid:63)) βT

Ds − ρt

t ).

(xs

(cid:16)

(cid:104)

,

∀s ∈ S, where ˜s = {s, s + S, . . . , s + (d − 1)S}.

10: end for

following optimization problem:

min
A

1
2

S
(cid:88)

s=1

(cid:107)xs − Dsαs(cid:107)2
(cid:96)2

+ λ1(cid:107)A(cid:107)(cid:96)12 + λ(cid:48)

1(cid:107)A(cid:107)(cid:96)11 +

(cid:107)A(cid:107)2
F ,

λ2
2

(23)

where λ(cid:48)
1 is the regularization parameter for the added (cid:96)11
norm and other terms are the same as those in Eq. (7). The
selection of λ1 and λ(cid:48)
1 inﬂuences the sparsity pattern of
A(cid:63). Intuitively, as λ1/λ(cid:48)
1 increases, the group constraint be-
comes dominant and more collaboration is enforced among
the modalities. On the other hand, small values of λ1/λ(cid:48)
1
encourage independent reconstructions across modalities.
In the extreme case of λ1 being set to zero, the above
optimization problem is separable across the modalities.
The above formulation brings added ﬂexibility with the cost
of one additional design parameter which is obtained in this
paper using cross-validation.

Here we present how the Algorithm 1 should be modiﬁed
to solve the supervised multimodal dictionary learning
problem under the mixed (cid:96)12 − (cid:96)11 constraint. The proof
for obtaining the algorithm is similar to the one for the
(cid:96)12 norm and is brieﬂy discussed in the appendix. In
Algorithm 1, let A(cid:63) be the solution of the optimization
problem (23) and let Λ be the set of its active rows. Let
Ψ ⊆ {1, . . . , S|Λ|} be the set of indices with non-zero
T ); i.e. it consists of non-zero entries
entries in vec(A(cid:63)
of the active rows of A(cid:63). Let ˆD, ∆, and g be the same as
those deﬁned in algorithm 1. Then, β ∈ RdS is updated as

Λ→

βΥc = 0, βΥ = ( ˆDT
Ψ

ˆDΨ + λ1∆Ψ→,Ψ + λ2I)−1gΨ,

7

Fig. 2: Extracted modalities from a sample in AR dataset.

where Υ is the set of indices with non-zero entries in
vec(A(cid:63)T
) and Υc = {1, . . . , dS} \Υ. Note that Υ is
deﬁned over the entire matrix A(cid:63) while Ψ is deﬁned over its
active rows. The rest of the algorithm remains unchanged.

IV. RESULTS AND DISCUSSION

The performance of the proposed multimodal dictio-
nary learning algorithms are evaluated on the AR face
database [55], the CMU Multi-PIE dataset [56], the IXMAS
action recognition dataset [57] and the WVU multimodal
ls is chosen to be
dataset [58]. For these algorithms,
the quadratic loss of Eq. (16) to handle the multiclass
classiﬁcation. In our experiments, it is observed that us-
ing the multiclass formulation achieves similar classiﬁ-
cation performance compared to using the logistic loss
formulation of Eq. (13) in the one-vs-all setting. Regu-
larization parameters λ1 and ν are selected using cross-
validation in the sets {0.01 + 0.005k|k ∈ {−3, 3}} and
{10−2, ..., 10−9}, respectively. It is observed that when the
number of dictionary atoms is kept small compared to the
number of training samples, ν can be arbitrarily set to a
small value, e.g. ν = 10−8, for the normalized inputs.
When the mixed (cid:96)12 − (cid:96)11 norm is used, the regularization
parameters λ1 and λ(cid:48)
1 are selected by cross-validation
in the set {0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05}. The
parameter λ2 is set to zero in most of the experiments
except when using the (cid:96)11 prior in Section IV-B1 where
a small positive value for λ2 was required for convergence.
The learning parameter ρt
is selected according to the
(cid:1) where
heuristic proposed in [29], i.e. ρt = min (cid:0)ρ, ρ t0
ρ and t0 are constants. This results in a constant learning
rate during the ﬁrst t0 iterations and an annealing strategy
of 1/t for the rest of the iterations. It is observed that
choosing t0 = T /10, where T is the total number of
iterations over the whole training set, works well for all
of our experiments. Different values of ρ are tried during
the ﬁrst few iterations and the one that results in minimum
error on a small validation set is retained. T is set equal
to be 20 in all the experiments. We observed empirically
that the selection of these parameters is quite robust and
small variations in their values do not affect considerably
the obtained results. We also used a mini-batch size of 100
in all our experiments. It should also be noted that design
parameters for the competitive algorithms are also selected
using cross-validation for a fair comparison.

t

A. AR face recognition

The AR dataset consists of faces under different poses,
illumination and expression conditions, captured in two
sessions. A set of 100 users are used, each consisting of

8

TABLE III: Multimodal classiﬁcation results obtained for the AR datasets

SVM-Maj

SVM-Sum LR-Maj

LR-Sum MKL

JSRC [14]

JDSRC [7]

85.57

92.14

85.00

91.14

91.14

96.14

96.14

SMDL(cid:96)11
95.86

SMDL(cid:96)12
96.86

SMDL(cid:96)12−(cid:96)11
97.14

TABLE I: Correct classiﬁcation rates obtained using the
whole face modality for the AR database.

SVM MKL [59]

LR

SRC [10] UDL

SDL [29]

86.43

82.86

81.00

88.86

89.58

90.57

TABLE IV: Comparison of the reconstructive-based (JSRC
and JSRC-UDL) and the proposed discriminative-based
(SMDL(cid:96)12 ) classiﬁcation algorithms obtained using the
joint sparsity prior for different numbers of dictionary
atoms per class on the AR dataset.

TABLE II: Comparison of the (cid:96)11 and (cid:96)12 priors for mul-
timodal classiﬁcation. Modalities include 1. left periocular,
2. right periocular, 3. nose, 4. mouth, and 5. face.

Modalities
UMDL(cid:96)11
UMDL(cid:96)12
SMDL(cid:96)11
SMDL(cid:96)12

{1, 2}

{1, 2, 3}

{1, 2, 3, 4}

{1, 2, 3, 4, 5}

81.9
82.6
83.86
86.43

87.57
87.86
89.86
89.86

90.14
92.00
92.42
93.57

95.57
96.29
95.86
96.86

seven images from the ﬁrst session as training samples and
seven images from the second session as test samples. A
small randomly selected portion of the training set, 50 out
of 700, is used as validation set for optimizing the design
parameters. Fusion is taken on ﬁve modalities which are
the left and right periocular, nose, mouth, and the whole
face modalities, similar to the setup in [14], [45]. A test
sample from the AR dataset and the extracted modalities
are shown in Fig. 2. Raw pixels are ﬁrst PCA-transformed
and then normalized to have zero mean and unit l2 norm.
The dictionary size for the dictionary learning algorithms
is chosen to be four per class, resulting in dictionaries of
overall 400 atoms.

Classiﬁcation using the whole face modality: The classi-
ﬁcation results using the whole face modality are shown in
Table I. The results are obtained using linear support vector
machine (SVM) [60], multiple kernel learning (MKL) [59],
logistic regression (LR) [60], sparse representation classi-
ﬁcation (SRC) [10], and unsupervised and supervised dic-
tionary learning algorithms (UDL and SDL) [29]. For the
MKL algorithm, linear, polynomial, and RBF kernels are
used. The UDL and SDL are equipped with the quadratic
classiﬁer (16). The SDL results in the best performance.

(cid:96)11 vs (cid:96)12 sparse priors for multimodal classiﬁcation: A
straightforward way of utilizing the single-modal dictionary
learning algorithms, namely UDL and SDL, for multimodal
classiﬁcation is to train independent dictionaries and clas-
siﬁers for each modality and then combine the individual
scores for a fused decision. This way of fusion is equivalent
to using the (cid:96)11 norm on A,
in
Eq. (7) (or setting λ1 to zero in Eq. (23)) which does not
enforce row sparsity in the sparse coefﬁcients. We denote
the corresponding unsupervised and supervised multimodal
dictionary learning algorithms using only the (cid:96)11 norm as
UMDL(cid:96)11 and SMDL(cid:96)11, respectively. Similarly, the pro-
posed unsupervised and supervised multimodal dictionary
learning algorithms using the (cid:96)12 norm are denoted as

instead of (cid:96)12 norm,

atoms/class

JSRC JSRC-UDL

1
2
3
4
5
6
7

46.14
69.00
79.57
88.14
91.00
94.43
96.14

71.71
78.86
83.57
91.14
94.85
96.28
96.14

SMDL(cid:96)12
91.28
95.00
95.71
96.86
97.14
96.71
96.00

UMDL(cid:96)12 and SMDL(cid:96)12. Table II compares the perfor-
mance of the multimodal dictionary learning algorithms
under the two priors. As shown, the proposed algorithms
with (cid:96)12 prior, which enforces collaborations among the
modalities, have better fusion performances than those with
(cid:96)11 prior. In particular, SMDL(cid:96)12 has signiﬁcantly better
performance than the SMDL(cid:96)11 for fusion of the ﬁrst and
second (left and right periocular) modalities. This agrees
with the intuition that these modalities are highly correlated
and learning the multimodal dictionaries jointly indeed
improves the recognition performance.

Comparison with other fusion methods: The perfor-
mances of the proposed fusion algorithms under different
sparsity priors are compared with those of the several state-
of-the-art decision-level and feature-level fusion algorithms.
In addition to (cid:96)11 and (cid:96)12 priors, we evaluate the proposed
supervised multimodal dictionary learning algorithm with
the mixed (cid:96)12−(cid:96)11 norm which is denoted as SMDL(cid:96)12−(cid:96)11.
One way to achieve decision-level fusion is to train in-
dependent classiﬁers for each modality and aggregate the
outputs by either adding the corresponding scores of each
modality to come up with the fused decision, or using the
majority voting among the independent decisions obtained
from different modalities. These approaches are abbrevi-
ated with Sum and Maj, respectively, and are used with
SVM and LR classiﬁers for decision-level fusion. The pro-
posed methods are also compared with feature-level fusion
methods including the joint sparse representation classiﬁer
(JSRC) [14], joint dynamic sparse representation classiﬁer
(JDSRC) [7], and MKL. For the JSRC and JDSRC, the
dictionary consists of all the training samples. Table III
compares the performance of our proposed algorithms with
the other fusion algorithms for the AR dataset. As expected,
the multimodal fusion results in signiﬁcant performance
improvement compared to using only the whole face modal-
ity. Moreover, the proposed SMDL(cid:96)12 and SMDL(cid:96)12−(cid:96)11
achieve the superior performances.

Reconstructive vs discriminative formulation with joint

9

Fig. 3: Computational time required to solve the optimiza-
tion problem (7) for a given test sample.

TABLE V: Comparison of the supervised multimodal dic-
tionary learning algorithms with different sparsity priors for
face recognition under occlusion on the AR dataset.

SMDL(cid:96)12
89.00

SMDL(cid:96)11
90.54

SMDL(cid:96)12−(cid:96)11
91.15

sparsity prior: Comparison of the algorithms with joint
sparsity priors in Table III indicates that
the proposed
SMDL(cid:96)12 algorithm equipped with dictionaries of size 400
achieves relatively better results than the JSRC that uses
dictionaries of size 700. The results conﬁrm the idea that
by using the supervised formulation, compared to using the
reconstruction error, one can achieve better classiﬁcation
performance even with more compact dictionaries. For
further comparison, an experiment is performed in which
the correct classiﬁcation rates of the reconsturtive and
discriminative formulations are compared when the their
dictionary sizes are kept equal. For a given number of
dictionary atoms per class d, dictionaries of JSRC are
thus constructed by random selection of d train samples
from different classes. This is different from the standard
JSRC, utilized for the results in Table III, in which all the
training samples are used to construct the dictionaries [14].
Moreover, to utilize all the available training samples for
the reconstructive approach and make a more meaningful
comparison, we use the unsupervised multimodal dictionary
learning algorithm of Eq. (8) to train class-speciﬁc sub-
dictionaries which minimizes the reconstruction error in
approximating the training samples for a given class. These
sub-dictionaries are then stacked to construct
the ﬁnal
dictionaries, similar to the approach in [22]. We call this
algorithm as JSRC-UDL to indicate that the dictionaries are
indeed learned by the reconstructive formulation. Table IV
summarizes the recognition performance of JSRC and
JSRC-UDL in comparison to the proposed SMDL(cid:96)12, which
enjoys a discriminative formulation, for different number of
dictionary atoms per class. As seen, SMDL(cid:96)12 outperforms
the reconstructive approaches, especially when the number
of dictionary is chosen to be relatively small. This is the
main advantage of SMDL(cid:96)12 compared to the reconstructive
approaches in which more compact dictionaries can be

Fig. 4: Conﬁgurations of the cameras and sample multi-
view images from CMU Multi-Pie dataset.

used for the recognition task that is important for the real-
time applications. It is clear that reconstructive model can
only result in comparable performance when the dictionary
size is chosen to be relatively large. On the other hand,
the SMDL(cid:96)12 algorithm may get over-ﬁtted with the large
number of dictionary atoms. In terms of computational
expense at test time, as discussed in [14], the time required
to solve the optimization problem (7) is expected to be
linear in the dictionary size using the efﬁcient ADMM if the
required matrix factorization is cashed beforehand. Typical
computational time to solve (7) for a given multimodal test
sample is shown in Fig. 3 for different dictionary sizes. As
expected, it increases linearly as the size of the dictionary
increases. This illustrates the advantage of the SMDL(cid:96)12
algorithm that results in the state-of-the-art performance
with more compact dictionaries.

Classiﬁcation in presence of disguise: The AR dataset
also contains 600 occluded samples per session, overall
1200 images, where the faces are disguised using sun
glasses or scarf. Here we use these additional images to
evaluate the robustness of the proposed algorithms. Similar
to previous experiments, images from session 1 are used
as training samples and images from session 2 are used
as test data. Classiﬁcation performance under different
sparsity priors are shown in Table V and as expected, the
SMDL(cid:96)12−(cid:96)11 achieves the best performance. In presence
of occlusion, some of the modalities are less coupled and
the joint sparsity prior among all the modalities may be too
stringent as is also reﬂected in the results.

B. Multi-view recognition

In this section,

1) Multi-view face recognition:

the
performance of the proposed algorithm is evaluated for
multi-view face recognition using the CMU Multi-PIE
[56]. The dataset consists of a large num-
dataset
ber of face images under different
illuminations, view-
points, and expressions which are recorded in four
several months. Subjects
sessions over
were imaged using 13 cameras at different view-angles
of {0◦, ±15◦, ±30◦, ±45◦, ±60◦, ±75◦, ±90◦} at head
height. Illustrations for the multiple camera conﬁgurations,
as well as sample multi-view images are shown in Fig. 4.
We use the multi-view face images for 129 subjects that are

the span of

10

TABLE VI: Correct classiﬁcation rates obtained using
individual modalities in the CMU Multi-PIE database.

View

SVM MKL

LR

Left
Frontal
Right

47.30
41.15
47.30

52.85
54.10
51.85

43.65
45.40
42.85

SRC

49.85
54.25
52.55

UDL

47.80
52.10
43.10

SDL

50.45
56.10
48.50

Fig. 5: Sample frames of the IXMAS dataset from 5
different views.

TABLE VII: Correct classiﬁcation rates (CCR) obtained
using multi-view images on the CMU Multi-PIE database.

TABLE VIII: Correct classiﬁcation rates (CCR) obtained
for multi-view action recognition on the IXMAS database.

Algorithm CCR

Algorithm

SVM-Maj
62.95
SVM-Sum 69.30
72.40
70.20
77.25
76.10

MKL
JDSRC
SMDL(cid:96)11
SMDL(cid:96)12

LR-Maj
LR-Sum
JSRC
UMDL(cid:96)11
UMDL(cid:96)12
SMDL(cid:96)12−(cid:96)11

CCR

69.40
71.10
73.30
74.80
70.50
81.30

Algorithm

Junejo et al. [65]
Wu et al. [66]
Wang et al. 2 [63]
UMDL(cid:96)11
UMDL(cid:96)12

CCR

79.6
88.2
93.6
90.3
90.6

Algorithm

Tran and Sorokin [62]
Wang et al. 1 [63]
JSRC
SMDL(cid:96)11
SMDL(cid:96)12

CCR

80.2
87.8
93.6
93.9
94.8

present in all sessions. The face regions for all the poses
are extracted manually and resized to 10 × 8. Similar to
the protocol used in [61], images from session 1 at views
{0◦, ±30◦, ±60◦, ±90◦} are used as training samples. Test
images are obtained from all available view angles from
session 2 to have a more realistic scenario in which not
all
the testing poses are available in the training set.
To handle multi-view recognition using the multi-modal
formulation, we divide the available views into three sets of
{−90◦, −75◦, −60◦, −45◦}, {−30◦, −15◦, 0◦, 15◦, 30◦, },
{45◦, 60◦, 75◦, 90◦}, each of which forms a modality. A
test sample is then constructed by randomly selecting an
image from each modality. Two thousand test samples are
generated in this way. The dictionary size for the dictionary
learning algorithms is chosen to have two atoms per class.
The classiﬁcation results obtained using individual
modalities are shown in Table VI. As expected, better
classiﬁcation performance is obtained using the frontal
view. Results of the multi-view face recognition is shown
in Table VII. The proposed supervised dictionary learn-
ing algorithms outperform the corresponding unsupervised
methods and other fusion algorithms. The SMDL(cid:96)12−(cid:96)11
results in the state-of-the-art performance. It is consistently
observed in all the studied applications that the multimodal
dictionary learning algorithm with the mixed prior results
in better performance than those with individual (cid:96)12 or
(cid:96)12 prior. However, it requires one additional regularizing
parameter to be tuned. For the rest of the paper,
the
performance of the proposed dictionary learning algorithms
are only reported under the individual priors.

the proposed algorithm for

2) Multi-view action recognition: This section presents
the results of
the pur-
pose of multi-view action recognition using the IXMAS
dataset [57]. Each action is recorded simultaneously by
cameras from ﬁve different viewpoints, which are con-
sidered as modalities in this experiment. A multimodal
sample of the IXMAS dataset is shown in Fig. 5. The
dataset contains 11 action classes where each action is
repeated three times by each of the ten actors, resulting
in 330 sequences per view. The dataset include actions

such as check watch, cross arms, and scratch head. Similar
to the work in [57], [62], [63], leave-one-actor-out cross-
validation is performed and samples from all ﬁve views are
used for training and testing.

We use dense trajectories as features which are generated
using the publicly available code [63] in which a 2000
word codebook is generated by a random subset of these
trajectories and the k-means clustering as in [64]. Note that
Wang et al. [63] used HOG, HOF, and MBH descriptors in
addition to the dense trajectories. However, here only dense
trajectory descriptors are used. The number of dictionary
atoms for the proposed dictionary learning algorithms are
chosen to be 4 atoms per class, resulting in a dictionary of
44 atoms per view. The ﬁve dictionaries for JSRC are con-
structed using all the training samples, thus each dictionary,
corresponding to a different view, has 297 atoms.

Table VIII shows average accuracies over all classes
obtained using the existing algorithms and the state of
the art algorithms. The Wang et al. 1 [63] algorithm uses
only the dense trajectories as feature, similar to our setup.
The Wang et al. 2 [63] algorithm, however, uses HOG,
HOF, MBH descriptors and the spatio-temporal pyramids
in addition to the trajectory descriptor. The results show
that the proposed SMDL(cid:96)12 algorithm achieves the superior
performance while the SMDL(cid:96)11 algorithm achieves the
second best performance. This indicates that sparse coefﬁ-
cients generated by the trained dictionaries are indeed more
discriminative than the engineered features. The resulting
confusion matrix of the SMDL(cid:96)12 algorithm is shown in
Fig. 6.

C. Multimodal biometric recognition

The WVU dataset consists of different biometric modali-
ties such as ﬁngerprint, iris, palmprint, hand geometry, and
voice from subjects of different age, gender, and ethnicity.
It
is a challenging data set, as many of the samples
are corrupted with blur, occlusion, and sensor noise. In
this paper, two irises (left and right) and four ﬁngerprint
modalities are used. The evaluation is done on a subset
of 202 subjects which have more than four samples in all

TABLE IX: Correct classiﬁcation rates obtained using individual modalities in the WVU database.

Finger 1

Finger 2

Finger 3

Finger 4

Iris 1

Iris 2

SVM 56.77 ± 0.72
61.81 ± 1.39
MKL
55.64 ± 1.89
LR
67.66 ± 1.86
SRC
64.68 ± 2.11
UDL
66.29 ± 1.81
SDL

82.95 ± 2.15
82.55 ± 1.47
81.10 ± 1.85
88.68 ± 1.59
87.35 ± 2.23
88.84 ± 2.31

55.83 ± 2.03
63.50 ± 1.75
55.21 ± 2.21
69.29 ± 0.77
67.35 ± 1.22
68.61 ± 1.30

80.47 ± 0.91
81.85 ± 0.74
78.82 ± 0.66
88.68 ± 1.03
86.40 ± 0.70
87.50 ± 0.82

60.67 ± 1.78
56.31 ± 2.20
55.25 ± 1.48
65.43 ± 1.24
64.36 ± 1.37
66.05 ± 0.75

57.52 ± 1.95
54.49 ± 0.79
56.86 ± 1.70
67.78 ± 1.76
65.23 ± 2.02
67.31 ± 1.38

11

TABLE X: Multimodal classiﬁcation results obtained for
the WVU dataset.

Algorithm 4 Fingerprints

2 Irises

All modalities

90.14 ± 0.70
SVM-Maj
SVM-Sum 93.56 ± 1.26
89.23 ± 1.63
93.60 ± 0.96
93.28 ± 1.52
97.64 ± 0.44
97.17 ± 0.26
97.09 ± 0.56
97.41 ± 0.71
96.78 ± 0.57
97.56 ± 0.41

LR-Maj
LR-Sum
MKL
JSRC
JDSRC
UMDL(cid:96)11
SMDL(cid:96)11
UMDL(cid:96)12
SMDL(cid:96)12

65.30 ± 1.92
74.03 ± 1.89
63.73 ± 1.29
71.43 ± 1.91
67.23 ± 0.70
82.94 ± 0.78
79.61 ± 0.70
80.90 ± 0.61
82.83 ± 0.87
81.53 ± 2.18
83.77 ± 0.89

95.24 ± 0.92
97.09 ± 0.83
94.18 ± 1.13
98.51 ± 0.18
94.46 ± 0.87
98.89 ± 0.30
97.80 ± 0.51
98.62 ± 0.46
98.66 ± 0.43
98.78 ± 0.43
99.10 ± 0.30

nally proposed for biometric recognition systems [67]. As
seen, the SMDL(cid:96)12 algorithm outperforms the competitive
algorithms and achieves the state-of-the-art performance
using the Irises and all modalities with the rank one
recognition rate of 83.77% and 99.10%, respectively. Using
the ﬁngerprints, the performance of the SMDL(cid:96)12 is close to
the best performing algorithm, which is JSRC. The results
suggest that using joint sparsity prior indeed improves the
multimodal classiﬁcation performance by extracting the
coupled information among the modalities.

Comparison of the algorithms with the joint sparsity
the proposed SMDL(cid:96)12 algorithm
priors indicates that
equipped with dictionaries of size 404 achieves comparable,
and mostly better, results than the JSRC that uses dictionary
of size 808. Similar to the experiment in Section IV-A,
we compared the reconstructive and discriminating algo-
rithms that are based on the joint sparsity prior when the
number of dictionary atoms per class is kept equal. Fig. 8
summarizes the results of the different fusion scenarios.
As seen, SMDL(cid:96)12 signiﬁcantly outperforms JSRC and
JSRC-UDL when the number of dictionary atoms per class
is chosen to be 1 or 2. The results are consistent with
that of Table IV for the AR dataset indicating that the
proposed supervised formulation equipped with more com-
pact dictionaries achieves superior performance than that
of the reconstructive formulation for the studied biometric
recognition applications.

V. CONCLUSIONS AND FUTURE WORKS

The problem of multimodal classiﬁcation using sparsity
models was studied and a task-driven formulation was
proposed to jointly ﬁnd the optimal dictionaries and clas-
siﬁers under the joint sparsity prior. It was shown that the

Fig. 6: The confusion matrix obtained by the SMDL(cid:96)12
algorithm on the IXMAS dataset. The actions are 1: check
watch, 2: cross arms, 3: scratch head, 4: sit down, 5: get
up, 6: turn around, 7: walk, 8: wave, 9: punch, 10: kick and
11: pick up.

modalities. Samples from different modalities are shown in
Fig. 1. The training set is formed by randomly selecting
four samples from each subject, overall 808 samples. The
remaining 509 samples are used for testing. The features
used here are those described in [14] which are further
PCA-transformed. The dimension of the input data after
preprocessing are 178 and 550 for the ﬁngerprint and iris
modalities, respectively. All inputs are normalized to have
zero mean and unit l2 norm. The number of dictionary
atoms for the dictionary learning algorithms are chosen to
be 2 per class, resulting in dictionaries of overall 404 atoms.
The dictionaries for JSRC and JDSRC are constructed using
all the training samples.

The classiﬁcation results obtained using individual
modalities on 5 different splits of the data into training and
test samples are shown in Table IX. As shown, ﬁnger 2 is
the strongest modality for the recognition task. The SRC
and SDL algorithms achieve the best results. It should be
noted that dictionary size of SRC is twice of that in SDL.
For multimodal classiﬁcation, we consider fusion of
ﬁngerprints, fusion of Irises, and fusion of all the modali-
ties. Table X summarizes the correct classiﬁcation rates of
several fusion algorithms using 4 ﬁngerprints, 2 Irises, and
all the modalities, obtained on 5 different training and test
splits. Fig. 7 shows the corresponding cumulative matched
score curves (CMC) for the competitive methods. CMC is
a performance measure, similar to ROC, which is origi-

12

Fig. 8: Comparison of the reconstructive-based (JSRC
and JSRC-UDL) and the proposed discriminative-based
(SMDL(cid:96)12 ) classiﬁcation algorithms obtained using the
joint sparsity prior for different numbers of dictionary
atoms per class on the WVU dataset.

utilizes the stochastic gradient algorithm, the learning rate
should be carefully chosen for convergence of the algo-
rithm. In out experiments, a heuristic was used to control
the learning rate. Topics of future research include develop-
ing of better optimization tools for fast convergence guaran-
tee in this non-convex setting. Moreover, developing task-
driven dictionary learning algorithms under other proposed
structured sparsity priors for multimodal fusion such as the
tree-structured sparsity prior [45], [68] is another future
research topic. Future research will also include adapting of
the proposed algorithms for other multimodal tasks such as
multimodal retrieval, multimodal action recognition using
Kinect data, and image super-resolution.

APPENDIX

The proof of Proposition 3.1 is presented using the

following two results.

(cid:104)

Lemma A.1 (Optimality condition): The matrix A(cid:63) =
α1(cid:63) . . . αS (cid:63)(cid:105)
∈ Rd×S is a min-
1→
imizer of (7) if and only if , ∀j ∈ {1, . . . , d},

T . . . a(cid:63)

T (cid:105)T

a(cid:63)

d→

=

(cid:104)






T (cid:16)

(cid:104)

d1
j

. . . dS
j

x1 − D1α1(cid:63)(cid:17)
a(cid:63)
j→
j→ = λ1
(cid:107)a(cid:63)
j→(cid:107)(cid:96)2
x1 − D1α1(cid:63)(cid:17)
j→(cid:107)(cid:96)2 ≤ λ1, otherwise.

. . . dS
j

, if (cid:107)a(cid:63)
T (cid:16)

− λ2a(cid:63)
T (cid:16)
(cid:104)
d1
(cid:107)
j
− λ2a(cid:63)

T (cid:16)

xS − DSαS (cid:63)(cid:17)(cid:105)

j→(cid:107)(cid:96)2 (cid:54)= 0,
xS − DSαS (cid:63)(cid:17)(cid:105)

(24)
Proof: . The proof follows directly from the subgradi-

ent optimality condition of (7), i.e.

0 ∈ {

D1α1(cid:63)

. . . DS T (cid:16)
− x1(cid:17)
D1T (cid:16)
(cid:104)
+ λ2A(cid:63) + λ1P : P ∈ ∂(cid:107)A(cid:63)(cid:107)(cid:96)12 },

DSαS (cid:63)

− xS(cid:17)(cid:105)

where ∂(cid:107)A(cid:63)(cid:107)(cid:96)12 denotes the subgradient of the (cid:96)12 norm
evaluated at A(cid:63). As shown in [69], the subgradient is
characterized, for all j ∈ {1, . . . , d}, as pj→ = aj→
if (cid:107)aj→(cid:107)(cid:96)2 > 0, and (cid:107)pj→(cid:107)(cid:96)2 ≤ 1 otherwise.

(cid:107)aj→(cid:107)(cid:96)2

Fig. 7: CMC plots obtained by fusing the Irises (top),
ﬁngerprints (middle), and all modalities (below) on the
WVU dataset.

resulting bi-level optimization problem is smooth and an
stochastic gradient descent algorithm was proposed to solve
the corresponding optimization problem. The algorithm
was then extended for a more general scenario where
the sparsity prior was the combination of the joint and
independent sparsity constraints. The simulation results on
the studied image classiﬁcation applications suggest that
while the unsupervised dictionaries can be used for feature
learning, the sparse coefﬁcients generated by the proposed
multimodal task-driven dictionary learning algorithms are
usually more discriminative and therefore can result
in
improved multimodal classiﬁcation performance. It was
also shown that, compared to the sparse-representation
classiﬁcation algorithms (JSRC, JDSRC, and JSRC-UDL),
the proposed algorithms can achieve signiﬁcantly better
performance when compact dictionaries are utilized.

In the proposed dictionary learning framework which

Before proceeding to the next proposition, we need to
deﬁne the term transition point. For a given {xs}, let Λλ
be the active set of the solution A(cid:63) of (7) when λ1 = λ.
Then λ is deﬁned to be a transition point of {xs} if Λλ+(cid:15) (cid:54)=
Λλ−(cid:15), ∀(cid:15) > 0.

Proposition A.1 (Regularity of A(cid:63)): Let λ2 > 0 and

assumption (A) be hold. Then,

Part 1. A(cid:63)({xs, Ds}) is a continuous function of {xs}

and {Ds}.

Part 2. If λ1 is not a transition point of {xs}, then
the active set Λ of A(cid:63)({xs, Ds}) is locally constant with
respect to both {xs} and {Ds}. Moreover, A(cid:63)({xs, Ds})
is locally differentiable with respect to {Ds}.

Part 3. ∀λ1 > 0, ∃ a set Nλ1 of measure zero in which
∀{xs} ∈ {Rns}\Nλ1 , λ1 is not any of the transition points
of {xs}.

Proof: . Part 1. In the special case of S = 1, which
is equivalent to an elastic net problem, this has already
been shown [19], [70]. Our proof follows similar steps.
Assumption (A) guarantees that A(cid:63) is bounded. Therefore,
we can restrict the optimization problem (7) to a compact
subset of Rd×S. Since A(cid:63) is unique (imposed by λ2 > 0)
and the cost function of (7) is continuous in A and each
element of the set {xs, Ds} is deﬁned over a compact set,
A(cid:63)({xs, Ds}) is a continuous function of {xs} and {Ds}.
Part 2 and Part 3. These statements are proved here by
converting the optimization problem (7) into an equivalent
group lasso problem [71] and using some recent results
j ) ∈
on it. Let
Rn×S, ∀j ∈ {1, . . . , d}, be the block-diagnoal collection
of the jth atoms of the dictionaries. Also let D(cid:48) =
∈ Rn, and
[D(cid:48)
a(cid:48) = [a1→ . . . ad→]T ∈ RSd . Then (7) can be rewritten as

d] ∈ Rn×Sd, x(cid:48) =

x1T . . . xS T (cid:105)T

j = blkdiag(d1

the matrix D(cid:48)

j , . . . , dS

1 . . . D(cid:48)

(cid:104)

min
A

1
2

(cid:107)x(cid:48) − D(cid:48)a(cid:48)(cid:107)2
(cid:96)2

+ λ1

(cid:107)aj→(cid:107)(cid:96)2 +

(cid:107)A(cid:107)2

F . (25)

d
(cid:88)

j=1

λ2
2

This can be further converted into the standard group lasso:

min
A

1
2

(cid:107)x(cid:48)(cid:48) − D(cid:48)(cid:48)a(cid:48)(cid:107)2
(cid:96)2

+ λ1

(cid:107)aj→(cid:107)(cid:96)2,

(26)

d
(cid:88)

j=1

(cid:104)

x(cid:48)T 0T (cid:105)T

(cid:105)T

where x(cid:48)(cid:48) =
D(cid:48)T √
(cid:104)
D(cid:48)(cid:48)
directly from the results in [72].

λ2I

∈ R(n+Sd)×Sd. It is clear that the matrix
is full column rank. The rest of the proof follows

∈ Rn+Sd and D(cid:48)(cid:48) =

Proof of Proposition 3.1: The above proposition
implies that A(cid:63) is differentiable almost everywhere. We
know prove the proposition 3.1. It is easy to show that f
is differentiable with respect to ws due to the assumption
(A) and the fact
lsu is twice differentiable. f is
also differentiable with respect to Ds given assumption
(A), twice differentiability of lsu, and the fact that A(cid:63)
is differentiable everywhere except on a set of measure
zero (Prop A.1). We obtain the derivative of f with respect
to Ds using the chain rule. The steps are similar to those

that

13

taken for (cid:96)1-related optimization in [36], though a bit more
involved. Since the active set is locally constant, using the
optimality condition (24), we can implicitly differentiate
A(cid:63)({xs, Ds}) with respect to Ds. For the non-active rows
of A(cid:63), the differential is zero. On the active set Λ, (24) can
be rewritten as

T (cid:16)

(cid:104)
D1
Λ

x1 − D1α1(cid:63)(cid:17)
(cid:34)

− λ2A(cid:63)

Λ→ = λ1

T (cid:16)

. . . DS
Λ

xS − DSαS (cid:63)(cid:17)(cid:105)
(cid:35)T

T

a(cid:63)
1→
(cid:107)a(cid:63)
1→(cid:107)(cid:96)2

. . .

T

a(cid:63)
N→
(cid:107)a(cid:63)
N→(cid:107)(cid:96)2

,

(27)

where N is the cardinality of Λ and DΛ and A(cid:63)
Λ→ are the
matrices consisting of active columns of D and active rows
of A(cid:63), respectively. For the rest of the proof, we only work
on the active set and the symbols Λ and (cid:63) are dropped for
the ease of notation. Taking the partial derivative from both
sides of (27) with respect to ds
ij, the element in the ith-row
and jth-column of Ds, and taking its transpose we have:





+

λ2

∂AT
∂ds
ij

0
(Dsαs − xs)T Es
ij + αsT Es
ij
0



T Ds

 +








∂α1T
∂ds
ij

∂αS T
∂ds
ij

D1T D1
...
DS T

DS








= −λ1

∆1

. . . ∆N

(cid:34)

1→

∂aT
∂ds
ij

(cid:35)

,

N→

∂aT
∂ds
ij

ij ∈ Rns×N is a matrix with zero elements except
where Es
the element in the ith row and jth column which is one
and

∆k =

1
(cid:107)ak→(cid:107)(cid:96)2

(cid:32)

I −

1
(cid:107)ak→(cid:107)2
(cid:96)2

(cid:33)

aT

k→ak→

∈ RS×S,

∀k ∈ {1, . . . , N }. It
Vectorizing the both sides and factorizing results in

is easy to check that ∆k ≥ 0.

vec

(cid:32)

(cid:33)

∂AT
∂ds
ij

=











P

0
(xs − Dsαs)T es
...
(xs − Dsαs)T es
ij N
0

ij 1

− αsT Es
ij

T ds
1

(28)

− αsT Es
ij

T ds
N











,

is

ij k

the kth
(cid:17)−1

column of Es

where es
=
(cid:16) ˆDT ˆD + λ1∆ + λ2I
, and ˆD and ∆ are deﬁned
in Eqs. (19) and (20), respectively. Further simplifying
Eq. (28) yields
(cid:33)
(cid:32)

ij, P

= P˜sEs
ij

T (xs − Dsαs) − P˜sds

T αs
j ,

i→

vec

∂AT
∂ds
ij

where ˜s is deﬁned in Eq. (21). Using the chain rule, we
have

(cid:34)

(cid:32)

(cid:33)(cid:35)

∂f
∂ds
ij

= E

gT vec

∂AT
∂ds
ij

,

14

where g = vec
respective to the active columns of dictionary Ds is

. Therefore, derivative with

(cid:16) ∂ (cid:80)S

s=1 lsu
∂AT

(cid:17)

∂f
∂Ds = E








. . .

gT P˜s

(cid:16)

gT P˜s

Es
11

(cid:16)

Es

ns1

gT P˜s
(cid:16)

Es

1N

T (xs − Dsαs) − ds
...
T (xs − Dsαs) − ds

(cid:17)

T αs
1

1→

(cid:17)

T αs
1


ns→
(cid:17)

T αs
N

1→

T (xs − Dsαs) − ds
...
T (xs − Dsαs) − ds
˜s gαsT (cid:105)

.

T αs
N

ns→






(cid:17)

(cid:16)

. . . gT P˜s

Es

nsN

(cid:104)

= E

(xs − Dsαs) gT P˜s − DsP T

Setting β = P T g ∈ RN S and noting that β˜s = P T
complete the proof.

˜s g

Derivation of the algorithm with the mixed (cid:96)12 − (cid:96)11
prior can be obtained similarly. For each active row j ∈ Λ
of A(cid:63), the solution of the optimization problem (23) with
the mixed prior, let Πj ⊆ S be the set of active modalities
which have non-zeros entries. Then the optimality condition
for the active row j is

(cid:104)
d1
j

T (cid:16)

x1 − D1α1(cid:63)(cid:17)

. . . dS
j

T (cid:16)

xS − DSαS (cid:63)(cid:17)(cid:105)

Πj

− λ2a(cid:63)

j→ = λ1

a(cid:63)
(cid:107)a(cid:63)

j→,Πj
j→(cid:107)(cid:96)2

+ λ(cid:48)

1 sign

(cid:16)

a(cid:63)

j→,Πj

(cid:17)

.

Then, the algorithm for the mixed prior can be obtained by
differentiating the optimality condition, following similar
steps as was shown for the (cid:96)12 prior.

REFERENCES

[1] D. L. Hall and J. Llinas,

“An introduction to multisensor data

fusion,” Proc. IEEE, vol. 85, no. 1, pp. 6–23, January 1997.
[2] P. K. Varshney, “Multisensor data fusion,” in Intell. Problem Solving.
Methodologies and Approaches. Springer Berlin Heidelberg, 2000.
[3] H. Wu, M. Siegel, R. Stiefelhagen, and J. Yang, “Sensor fusion
using Dempster-Shafer theory,” in Proc. 19th IEEE Instrum. and
Meas. Technol. Conf. (IMTC), 2002, pp. 7–12.

[4] A. Ross and R. Govindarajan, “Feature level fusion using hand and

face biometrics,” in SPIE proc. series, 2005, pp. 196–204.

[5] D. Ruta and B. Gabrys, “An overview of classiﬁer fusion methods,”

Comput. and Inf. syst., vol. 7, no. 1, pp. 1–10, 2000.

[6] A. Rattani, D. R. Kisku, M. Bicego, and M. Tistarelli, “Feature
level fusion of face and ﬁngerprint biometrics,” in Proc. 1st IEEE
Int. Conf. Biometrics: Theory, Applicat., and Syst., 2007, pp. 1–6.

[7] H. Zhang, N. M. Nasrabadi, Y. Zhang, and T. S. Huang, “Multi-
observation visual recognition via joint dynamic sparse representa-
tion,” in Proc. IEEE Conf. Comput. Vision (ICCV), 2011, pp. 595–
602.

[8] A. Klausne, A. Tengg, and B. Rinner, “Vehicle classiﬁcation on
multi-sensor smart cameras using feature- and decision-fusion,” in
Proc. 1st ACM/IEEE Int. Conf. Distributed Smart Cameras, 2007,
pp. 67–74.

[9] A. Rattani and M. Tistarelli, “Robust multi-modal and multi-unit
feature level fusion of face and iris biometrics,” in Advances in
Biometrics, pp. 960–969. Springer, 2009.

[10] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, “Robust
IEEE Trans. Pattern

face recognition via sparse representation,”
Anal. Mach. Intell, vol. 31, no. 2, pp. 210–227, Feb. 2009.

[11] X. Mei and H. Ling, “Robust visual tracking and vehicle classiﬁ-
cation via sparse representation,” IEEE Trans. Pattern Anal. Mach.
Intell, vol. 33, no. 11, pp. 2259–2272, Nov. 2011.

[12] H. Zhang, Y. Zhang, N. M. Nasrabadi, and T. S. Huang, “Joint-
structured-sparsity-based classiﬁcation for multiple-measurement
transient acoustic signals,” IEEE Trans. Syst., Man, Cybern., vol.
42, no. 6, pp. 1586–98, Dec. 2012.

[13] R. Caruana, Multitask learning, Springer, 1998.
[14] S. Shekhar, V. Patel, N. M. Nasrabadi, and R. Chellappa, “Joint
sparse representation for robust multimodal biometrics recognition,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 1, pp. 113–126,
Jan. 2013.

[15] U. Srinivas, H. Mousavi, C. Jeon, V. Monga, A. Hattel, and B. Ja-
yarao, “Simultaneous sparsity model for histopathological image
representation and classiﬁcation,” IEEE Trans. Med. Imag., vol. 33,
no. 5, pp. 1163 – 1179, May 2014.

[16] H. S. Mousavi, V. Srinivas, U.and Monga, Y. Suo, M. Dao, and T. D.
Tran, “Multi-task image classiﬁcation via collaborative, hierarchical
spike-and-slab priors,” in IEEE Intl. Conf. Image Processing (ICIP),
Oct 2014, pp. 4236–4240.

[17] N. H. Nguyen, N. M. Nasrabadi, and T. D. Tran, “Robust multi-
sensor classiﬁcation via joint sparse representation,” in Proc. 14th
Int. Conf. Information Fusion (FUSION), 2011.

[18] M. Yang, L. Zhang, D. Zhang, and S. Wang, “Relaxed collaborative
in Proc. IEEE Conf.
representation for pattern classiﬁcation,”
Comput. Vision and Pattern Recognition (CVPR), 2012, pp. 2224–
2231.

[19] J. Mairal, F. Bach, J. Ponce, and G. Sapiro,

“Online dictionary
learning for sparse coding,” Proc. 26th Annu. Int. Conf. Mach.
Learning (ICML), pp. 689–696, 2009.

[20] J. Mairal, F. Bach, A. Zisserman, and G. Sapiro,

“Supervised
in Advances Neural Inform. Process. Syst.

dictionary learning,”
(NIPS), 2008, pp. 1033–1040.

[21] J. Mairal, M. Elad, and G. Sapiro, “Sparse representation for color
image restoration,” IEEE Trans. Image Process., vol. 17, no. 1, pp.
53–69, Jan. 2008.

[22] M. Yang, L. Zhang, J. Yang, and D. Zhang, “Metaface learning for
sparse representation based face recognition,” in Proc. IEEE Conf.
Image Process. (ICIP), 2010, pp. 1601–1604.

[23] Y. L. Boureau, F. Bach, Y. LeCun, and J. Ponce, “Learning mid-
level features for recognition,” in Proc. IEEE Conf. Comput. Vision
and Pattern Recognition (CVPR), 2010, pp. 2559–2566.

[24] Z. Jiang, Z. Lin, and L. S. Davis, “Label consistent K-SVD: Learning
a discriminative dictionary for recognition,” IEEE Trans. Pattern
Anal. Mach. Intell, vol. 35, no. 11, pp. 2651–2664, Nov. 2013.
[25] M. Aharon, M. Elad, and A. Bruckstein, “K-SVD : An algorithm
for designing overcomplete dictionaries for sparse representation,”
IEEE Trans. Signal Process., vol. 54, no. 11, pp. 4311–4322, Nov.
2006.

[26] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online learning for
matrix factorization and sparse coding,” The J. of Mach. Learning
Research, vol. 11, pp. 19–60, 2010.

[27] K. Engan, S. O. Aase, and J. H. Husoy,

“Method of optimal
directions for frame design,” in Proc. IEEE Int. Conf. Acoustics,
Speech, and Signal Process., 1999, vol. 5, pp. 2443–2446.

[28] M. Elad and M. Aharon, “Image denoising via saprse and redundant
IEEE Trans. Image

representations over learned dictionaries,”
Process., vol. 15, no. 12, pp. 3736–3745, Dec. 2006.

[29] J. Mairal, F. Bach, and J. Ponce, “Task-driven dictionary learning,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 4, pp. 791–804,
Apr. 2012.

[30] J. Yang, Z. Wang, Z. Lin, X. Shu, and T. Huang, “Bilevel sparse
coding for coupled feature spaces,” in IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2012, pp. 2360–2367.
[31] S. Kong and D. Wang, “A brief summary of dictionary learning
based approach for classiﬁcation,” arxivId:1205.6544, 2012.
[32] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman, “Dis-
criminative learned dictionaries for local image analysis,” in Proc.
IEEE Conf. Comput. Vision and Pattern Recognition (CVPR), 2008,
pp. 1–8.

[33] Q. Zhang and B. Li, “Discriminative K-SVD for dictionary learning
in face recognition,” in Proc. IEEE Conf. Comput. Vision and Pattern
Recognition (CVPR), 2010, pp. 2691–2698.

[34] I. Ramirez, P. Sprechmann, and G. Sapiro,

“Classiﬁcation and
clustering via dictionary learning with structured incoherence and
shared features,” in Proc. IEEE Conf. Comput. Vision and Pattern
Recognition (CVPR), 2010, pp. 3501–3508.

[35] M. Yang, L. Zhang, X. Feng, and D. Zhang, “Fisher discrimination
dictionary learning for sparse representation,” in Proc. IEEE Int.
Conf. Comput. Vision (ICCV), 2011, pp. 543–550.

[36] J. Yang, K. Yu, and T. Huang,

“Supervised translation-invariant
sparse coding,” in Proc. IEEE Conf. Comput. Vision and Pattern
Recognition (CVPR), 2010, pp. 3517–3524.

[37] B. Colson, P. Marcotte, and G. Savard, “An overview of bilevel
optimization,” Ann. Operations Research, vol. 153, no. 1, pp. 235–
256, Apr. 2007.

[38] D. M. Bradley and J. A. Bagnell, “Differentiable sparse coding,” in

Advances Neural Inform. Process. Syst. (NIPS), 2008.

[39] J. Zheng and Z. Jiang, “Learning view-invariant sparse representa-
tions for cross-view action recognition,” in Proc. IEEE Intel. Conf.
Computer Vision (ICCV), 2013, pp. 3176–3183.

[40] G. Monaci, P. Jost, P. Vandergheynst, B. Mailh´e, S. Lesage, and
IEEE Trans.

R. Gribonval,
Image Process, vol. 16, no. 9, pp. 2272–2283, Sep. 2007.

“Learning multimodal dictionaries,”

[41] Y. Zhuang, Y. Wang, F. Wu, Y. Zhang, and W. Lu, “Supervised
coupled dictionary learning with group structures for multi-modal
retrieval,” in Proc. 27th Conf. Artiﬁcial Intell., 2013, pp. 1070–1076.
[42] H. Zhang, Y. Zhang, and T. S. Huang, “Simultaneous discriminative
projection and dictionary learning for sparse representation based
classiﬁcation,” Pattern Recognition, vol. 46, no. 1, pp. 346–354,
Jan. 2013.

[43] B. A. Olshausen and D. J. Field, “Sparse coding with an overcom-
plete basis set: A strategy employed by v1?,” Vision Research, vol.
37, no. 23, pp. 3311–3325, Dec. 1997.

[44] L. Bottou and O. Bousquet, “The trade-offs of large scale learning,”
in Advances Neural Inform. Process. Syst. (NIPS), 2007, pp. 161–
168.

[45] S. Bahrampour, A. Ray, N. M. Nasrabadi, and W. K. Jenkins,
“Quality-based multimodal classiﬁcation using tree-structured spar-
sity,” in Proc. IEEE Conf. Comput. Vision and Pattern Recognition
(CVPR), 2014, pp. 4114–4121.

[46] S. F. Cotter, B. D. Rao, K. Engan, and K. Kreutz-Delgado, “Sparse
solutions to linear inverse problems with multiple measurement
vectors,” IEEE Trans. Signal Process., vol. 53, no. 7, pp. 2477–
2488, Jul. 2005.

[47] J. A. Tropp, “Algorithms for simultaneous sparse approximation. part
ii: Convex relaxation,” Signal Process., vol. 86, no. 3, pp. 589–602,
Mar. 2006.

[48] A. Rakotomamonjy, “Surveying and comparing simultaneous sparse
approximation (or group-lasso) algorithms,” Signal Process., vol.
91, no. 7, pp. 1505–1526, Jul. 2011.

[49] N. Parikh and S. Boyd, “Proximal algorithms,” Foundations and

Trends in Optimization, pp. 1–96, 2013.

[50] H. Zou and T. Hastie, “Regularization and variable selection via the
elastic net,” J. Royal Statistical Soc. Series B, vol. 67, no. 2, pp.
301–320, 2005.

[51] M. Aharon and M. Elad, “Sparse and redundant modeling of image
content using an image-signature-dictionary,” SIAM J.l on Imaging
Sciences, vol. 1, no. 3, pp. 228–247, 2008.

“Large-scale machine learning with stochastic gradi-
in Proceedings of COMPSTAT’2010, pp. 177–186.

[52] L. Bottou,

ent descent,”
Springer, 2010.

[53] L. Bottou, “Online learning and stochastic approximations,” On-line

learning in neural networks, vol. 17, pp. 9.

[54] P. Sprechmann, I. Ramirez, G. Sapiro, and Y. C. Eldar, “C-hilasso: A
collaborative hierarchical sparse modeling framework,” IEEE Trans.
Signal Process., vol. 59, no. 9, pp. 4183–4198, Sep. 2011.

[55] A. M. Martinez and R. Benavente, “The AR face database,” CVC

Technical Rep., vol. 24, 1998.

[56] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, “Multi-
pie,” Image and Vision Computing, vol. 28, no. 5, pp. 807–813,
2010.

[57] D. Weinland, E. Boyer, and R. Ronfard, “Action recognition from
in IEEE 11th Intel. Conf.

arbitrary views using 3d exemplars,”
Computer Vision (ICCV), 2007, pp. 1–7.

[58] S. S. S. Crihalmeanu, A. Ross, and L. Hornak, “A protocol for
multibiometric data acquisition, storage and dissemination,” Tech.
Rep., Lane Dept. of Comput. Sci. and Elect. Eng., West Virginia
Univ., 2007.

[59] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet, “Sim-
plemkl.,” J. of Mach. Learning Research, vol. 9, no. 11, 2008.
[60] C. M. Bishop, Pattern Recognition and Machine Learning, Springer,

2006.

[61] H. Zhang, N. M. Nasrabadi, Y. Zhang, and T. S. Huang, “Joint
dynamic sparse representation for multi-view face recognition,”
Pattern Recognition, vol. 45, pp. 1290–1298, 2012.

15

[62] D. Tran and A. Sorokin, “Human activity recognition with metric
learning,” in Computer Vision–ECCV 2008, pp. 548–561. Springer,
2008.

[63] A. Wang, H.and Kl¨aser, C. Schmid, and C.-L. Liu, “Dense trajec-
tories and motion boundary descriptors for action recognition,” Intl.
J. Computer Vision, vol. 103, no. 1, pp. 60–79, 2013.

[64] A. Gupta, A. Shafaei, J. J. Little, and R. J. Woodham, “Unlabelled 3d
motion examples improve cross-view action recognition,” in Proc.
British Machine Vision Conf., 2014.

[65] I. N. Junejo, E. Dexter, and P. Laptev, I.and Perez,

“View-
independent action recognition from temporal self-similarities,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 1, pp. 172–
185, 2011.

[66] X. Wu, D. Xu, L. Duan, and J. Luo, “Action recognition using
context and appearance distribution features,” in IEEE Conf. Comput.
Vision Pattern Recognition (CVPR), 2011, pp. 489–496.

[67] R. M. Bolle, J. H. Connell, S. Pankanti, N. K. Ratha, and A. W.
Senior, “The relation between the roc curve and the cmc,” in Proc.
4th IEEE Workshop Automat. Identiﬁcation Advanced Technologies,
2005, pp. 15–20.

[68] Julien Mairal, Rodolphe Jenatton, Guillaume Obozinski, and Francis
“Learning hierarchical and topographic dictionaries with

Bach,
structured sparsity,” in SPIE, 2011.

[69] E. V. D. Berg and M. P. Friedlander, “Theoretical and empirical
results for recovery from multiple measurements,” IEEE Trans. Inf.
Theory, vol. 56, no. 5, pp. 2516–2527, Apr. 2010.

[70] H. Zou, T. Hastie, and R. Tibshirani, “On the degrees of freedom
of the lasso,” The Ann. of Statistics, vol. 35, no. 5, pp. 2173–2192,
2007.

[71] M. Yuan and Y. Lin, “Model selection and estimation in regression
with grouped variables,” J. Royal Statistical Soc.: Series B (Statis-
tical Methodology), vol. 68, no. 1, pp. 49–67, 2006.

[72] S. Vaiter, C. Deledalle, G. Peyre, J. Fadili, and C. Dossal, “Degrees

of freedom of the group lasso,” arXiv:1205.1481, 2012.

Soheil Bahrampour received the M.Sc. degree
in electrical engineering from the University of
Tehran, Iran,
in 2009. He then received the
M.Sc. degree in Mechanical Engineering and
PhD degree in Electrical Engineering under the
supervision of A. Ray and W.K. Jenkins from
The Pennsylvania State University, University
Park, PA, in 2013 and 2015, respectively. Dr.
Bahrampour is currently a Research Scientist
with Bosch Research and Technology Center,
Palo Alto, CA. His research interests include

machine learning, data mining, signal processing, and computer vision.

16

Nasser M. Nasrabadi
(S’80-M’84-SM’92-
FM’01) received the B.Sc. (Eng.) and Ph.D.
degrees in Electrical Engineering from Imperial
College of Science and Technology (University
of London), London, England, in 1980 and 1984,
respectively. From October 1984 to December
1984 he worked for IBM (UK) as a senior
programmer. During 1985 to 1986 he worked
with Philips research laboratory in NY as a
member of technical staff. From 1986 to 1991 he
was an assistant professor in the Department of
Electrical Engineering at Worcester Polytechnic Institute, Worcester, MA.
From 1991 to 1996 he was an associate professor with the Department
of Electrical and Computer Engineering at State University of New York
at Buffalo, Buffalo, NY. Since September From 1996 to 2015 he was a
Senior Research Scientist (ST) with the US Army Research Laboratory
(ARL). Since August 2015 he has been a professor at Lane Dept. of
Computer Science and Elecrical Engineering. Dr. Nasrabadi has served
as an associate editor for the IEEE Transactions on Image Processing,
the IEEE Transactions on Circuits, Systems and Video Technology, and
the IEEE Transactions on Neural Networks. His current research interests
are in image processing, computer vision, biometrics, statistical machine
learning theory, sparsity, robotics, and neural networks applications to
image processing. He is also a Fellow of ARL, SPIE and IEEE.

Asok Ray (SM’83F’02) received the Ph.D. de-
gree in Mechanical Engineering from Northeast-
ern University, Boston, MA, and the graduate de-
grees in the disciplines of Electrical Engineering,
Mathematics, and Computer Science. He joined
The Pennsylvania State University (Penn State),
University Park, PA, in July 1985, and is cur-
rently a Distinguished Professor of Mechanical
Engineering and Mathematics, a Graduate Fac-
ulty of Electrical Engineering, and a Graduate
Faculty of Nuclear Engineering. Prior to joining
Penn State, he held research and academic positions with Massachusetts
Institute of Technology, Cambridge, MA, and Carnegie-Mellon University,
Pittsburgh, PA, as well as management and research positions with GTE
Strategic Systems Division, Westborough, MA, Charles Stark Draper
Laboratory, Cambridge, MA, and MITRE Corporation, Bedford, MA. Dr.
Ray has authored or coauthored over 550 research publications, including
over 285 scholarly articles in refereed journals and research monographs.
Dr. Ray is also a Fellow of the American Society of Mechanical Engineers
(ASME) and a Fellow of World Innovative Foundation (WIF). Dr. Ray had
been a Senior Research Fellow at NASA Glenn Research Center under a
National Academy of Sciences award.

W. Kenneth Jenkins received the B.S.E.E. de-
gree from Lehigh University and the M.S.E.E.
and Ph.D. degrees from Purdue University. From
1974 to 1977 he was a Research Scientist Asso-
ciate in the Communication Sciences Laboratory
at the Lockheed Research Laboratory, Palo Alto,
CA. In 1977 he joined the University of Illinois
at Urbana-Champaign where he was a faculty
member in Electrical and Computer Engineering
from 1977 until 1999. From 1986-1999 Dr. Jenk-
ins was the Director of the Coordinated Science
Laboratory. From 1999 through 2011 he served Professor and Head of
Electrical Engineering at Penn State University, and in 2011 he returned
to the rank of Professor of Electrical Engineering. Dr. Jenkins current
research interests include fault
tolerant DSP for highly scaled VLSI
systems, adaptive signal processing, multidimensional array processing,
computer imaging, bio-inspired optimization algorithms for intelligent
signal processing, and fault tolerant digital signal processing. He co-
authored the book Advanced Concepts in Adaptive Signal Processing,
published by Kluwer in 1996. He is a past Associate Editor for the
IEEE Transaction on Circuits and Systems, and a past President (1985) of
the CAS Society. He served as General Chairman of the 1988 Midwest
Symposium on Circuits and Systems and as the General Chairman of
the Thirty Second Annual Asilomar Conference on Signals and Systems.
From 2002 to 2007 he served on the Board of Directors of the Electrical
and Computer Engineering Department Heads Association (ECEDHA)
and as President of ECEDHA in 2005. Since January 2011 he has been
serving as a Member of the IEEE-HKN Board of Governors. Dr. Jenkins
is a Life Fellow of the IEEE and a recipient of the 1990 Distinguished
Service Award of the IEEE Circuits and Systems Society. In 2000 he
received a Golden Jubilee Medal from the IEEE Circuits and Systems
Society and a 2000 Millennium Award from the IEEE. In 2000 was named
a co-winner of the 2000 International Award of theGeorge Monteﬁore
Foundation (Belgium) for outstanding career contributions to the ﬁeld of
electrical engineering and electrical science, in 2002 he was awarded the
Shaler Area High School Distinguished Alumnus Award, in 2007 he was
honored with an IEEE Midwest Symposium on Circuits and Systems 50th
Anniversary Award, and in 2013 he received the ECEDHA Robert M.
Janowiak Outstanding Leadership and Service Award.

Multimodal Task-Driven Dictionary Learning
for Image Classiﬁcation

Soheil Bahrampour, Member, IEEE, Nasser M. Nasrabadi, Fellow, IEEE, Asok Ray, Fellow, IEEE,
and W. Kenneth Jenkins, Life Fellow, IEEE

1

5
1
0
2
 
t
c
O
 
7
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
9
0
1
0
.
2
0
5
1
:
v
i
X
r
a

Abstract—Dictionary learning algorithms have been suc-
cessfully used for both reconstructive and discriminative tasks,
where an input signal is represented with a sparse linear
combination of dictionary atoms. While these methods are
mostly developed for single-modality scenarios, recent studies
have demonstrated the advantages of feature-level fusion
based on the joint sparse representation of the multimodal
inputs. In this paper, we propose a multimodal task-driven
dictionary learning algorithm under the joint sparsity con-
straint (prior) to enforce collaborations among multiple ho-
mogeneous/heterogeneous sources of information. In this task-
driven formulation, the multimodal dictionaries are learned
simultaneously with their corresponding classiﬁers. The re-
sulting multimodal dictionaries can generate discriminative
latent features (sparse codes) from the data that are optimized
for a given task such as binary or multiclass classiﬁca-
tion. Moreover, we present an extension of the proposed
formulation using a mixed joint and independent sparsity
prior which facilitates more ﬂexible fusion of the modalities
at feature level. The efﬁcacy of the proposed algorithms
for multimodal classiﬁcation is illustrated on four different
applications – multimodal face recognition, multi-view face
recognition, multi-view action recognition, and multimodal
biometric recognition. It is also shown that, compared to
the counterpart reconstructive-based dictionary learning algo-
rithms, the task-driven formulations are more computationally
efﬁcient in the sense that they can be equipped with more
compact dictionaries and still achieve superior performance.

Index Terms—Dictionary learning, Multimodal classiﬁca-

tion, Sparse representation, Feature fusion

I. INTRODUCTION
It is well established that information fusion using multi-
ple sensors can generally result in an improved recognition
performance [1]. It provides a framework to combine
local information from different perspectives which is more
tolerant to the errors of individual sources [2], [3]. Fusion
methods for classiﬁcation are generally categorized into
feature fusion [4] and classiﬁer fusion [5] algorithms.
Feature fusion methods aggregate extracted features from
different sources into a single feature set which is then used
for classiﬁcation. On the other hand, classiﬁer fusions algo-
rithms combine decisions from individual classiﬁers, each

When this work was achieved, S. Bahrampour, A. Ray, and W. K.
Jenkins were with the Department of Electrical Engineering, Pennsylvania
State University, University Park, PA 16802, USA; N. M. Nasrabadi was
with the Army Research Laboratory, Adelphi, MD 20783. S. Bahrampour
is now with Bosch Research and Technology Center, Palo Alto, CA; N. M.
Nasrabadi is now with the Computer Science and Electrical Engineering
Department at the West Virginia University, WV.

soheil.bahrampour@us.bosch.com, nassernasrabadi@mail.wvu.edu,

{axr2, wkj1}@psu.edu.

of which is trained using separate sources. While classiﬁer
fusion is a well-studied topic, fewer studies have been done
for feature fusion, mainly due to the incompatibility of the
feature sets [6]. A naive way of feature fusion is to stack
the features into a longer one [7]. However this approach
usually suffers from the curse of dimensionality due to the
limited number of training samples [4]. Even in scenarios
with abundant training samples, concatenation of feature
vectors does not take into account the relationship among
the different sources and it may contain noisy or redundant
data, which degrade the performance of the classiﬁer [6].
However, if these limitations are mitigated, feature fusion
can potentially result
in improved classiﬁcation perfor-
mance [8], [9].

Sparse representation classiﬁcation has recently attracted
the interest of many researchers in which the input sig-
nal is approximated with a linear combination of a few
dictionary atoms [10] and has been successfully applied
to several problems such as robust face recognition [10],
visual tracking [11], and transient acoustic signal classi-
ﬁcation [12]. In this approach, a structured dictionary is
usually constructed by stacking all the training samples
from the different classes. The method has also been
expanded for efﬁcient feature-level fusion which is usually
referred to as multi-task learning [13], [14], [15], [16].
Among different proposed sparsity constraints (priors), joint
sparse representation has shown signiﬁcant performance
improvement
in several multi-task learning applications
such as target classiﬁcation, biometric recognitions, and
multiview face recognition [12], [14], [17], [18]. The un-
derlying assumption is that the multimodal test input can
be simultaneously represented by a few dictionary atoms,
or training samples, from a multimodal dictionary, that
represents all the modalities and, therefore, the resulting
sparse coefﬁcients should have the same sparsity pattern.
However, the dictionary constructed by the collection of
the training samples suffer from two limitations. First, as
the resulting
the number of training samples increases,
optimization problem becomes more computationally de-
manding. Second, the dictionary that is constructed this way
is not optimal neither for the reconstructive tasks [19] nor
the discriminative tasks [20].

Recently it has been shown that learning the dictionary
can overcome the above limitations and signiﬁcantly im-
prove the performance in several applications including
image restoration [21], face recognition [22] and object
recognition [23], [24]. The learned dictionaries are usu-

2

ally more compact and have fewer dictionary atoms than
the number of training samples [25], [26]. Dictionary
learning algorithms can generally be categorized into two
groups: unsupervised and supervised. Unsupervised dictio-
nary learning algorithms such as the method of optimal
direction [27] and K-SVD [25] are aimed at ﬁnding a
dictionary that yields minimum errors when adapted to
reconstruction tasks such as signal denoising [28] and im-
age inpainting [19]. Although, the unsupervised dictionary
learning has also been used for classiﬁcation [22], it has
been shown that better performance can be achieved by
learning the dictionaries that are adapted to an speciﬁc
task rather than just the data set [29], [30]. These methods
are called supervised, or task-driven, dictionary learning
algorithms. For the classiﬁcation task, for example, it is
more meaningful to utilize the labeled data to minimize
the misclassiﬁcation error rather than the reconstruction
error [31]. Adding a discriminative term to the recon-
struction error and minimizing a trade-off between them
has been proposed in several formulations [20], [24],
[32], [33]. The incoherent dictionary learning algorithm
proposed in [34] is another supervised formulation which
trains class-speciﬁc dictionaries to minimize atom sharing
between different classes and uses sparse representation
for classiﬁcation. In [35], a Fisher criterion is proposed to
learn structured dictionaries such that the sparse coefﬁcients
have small within-class and large between-class scatters.
While unsupervised dictionary learning can be reformulated
as a large scale matrix factorization problem and solved
efﬁciently [19], supervised dictionary learning is usually
more difﬁcult to optimize. More recently, it has been shown
that better optimization tool can be used to tackle the
supervised dictionary learning [30], [36]. This is achieved
by formulating it as a bilevel optimization problem [37],
[38]. In particular, a stochastic gradient descent algorithm
has been proposed in [29] which efﬁciently solves the
dictionary learning problem in a uniﬁed framework for
different tasks, such as classiﬁcation, nonlinear image map-
ping, and compressive sensing.

The majority of the existing dictionary learning algo-
rithms, including the task-driven dictionary learning [29],
are only applicable to single source of data. In [39], a set
of view-speciﬁc dictionaries and a common dictionary are
learned for the application of multi-view action recogni-
tion. The view-speciﬁc dictionaries are trained to exploit
view-level correspondence while the common dictionary is
trained to capture common patterns shared among the dif-
ferent views. The proposed formulation belongs to the class
of dictionary learning algorithms that leverages the labeled
samples to learn class-speciﬁc atoms while minimizing
the reconstruction error. Moreover, it cannot be used for
fusion of the heterogeneous modalities. In [40], a generative
multimodal dictionary learning algorithm is proposed to
extract typical templates of multimodal features. The tem-
plates represent synchronous transient structures between
modalities which can be used for localization applications.
More recently, a multimodal dictionary learning algorithm
with joint sparsity prior is proposed in [41] for multimodal

Fig. 1: Multimodal task-driven dictionary learning scheme.

retrieval where the task is to ﬁnd relevant samples from
other modalities for a given unimodal query. However,
the proposed formulation cannot be readily applied for
information fusion in which the task is to ﬁnd label of a
given multimodal query. Moreover, the joint sparsity prior
is used in [41] to couple similarly labeled samples within
each modality and is not utilized to extract cross-modality
information which is essential for information fusion [12].
Furthermore,
the dictionaries in [41] are learned to be
generative by minimizing the reconstruction error of data
across modalities and, therefore, are not necessary optimal
for discriminative tasks [31].

This paper focuses on learning discriminative multimodal
dictionaries. The major contributions of the paper are as
follows:

• Formulation of

the multimodal dictionary learning
algorithms: A multimodal task-driven dictionary learn-
ing algorithm is proposed for classiﬁcation using ho-
mogeneous or heterogeneous sources of information.
Information from different modalities are fused both
at the feature level, by using the joint sparse repre-
sentation, and at the decision level, by combining the
scores of the modal-based classiﬁers. The proposed
formulation simultaneously trains the multimodal dic-
tionaries and classiﬁers under the joint sparsity prior in
order to enforce collaborations among the modalities
and obtain the latent sparse codes as the optimized
features for different tasks such as binary and mul-
ticlass classiﬁcation. Fig. 1 presents an overview of
the proposed framework. An unsupervised multimodal
dictionary learning algorithm is also presented as a by-
product of the supervised version.

• Differentiability of the bi-level optimization problem:
The main difﬁculty in proposing such a formulation
is that the solution of the corresponding joint sparse
coding problem is not differentiable with respect to
the dictionaries. While the joint sparse coding has
a non-smooth cost function,
is shown here that
it is locally differentiable and the resulting bi-level
optimization for task-driven multimodal dictionary
learning is smooth and can be solved using a stochastic

it

gradient descent algorithm. 1

• Flexible feature-level

fusion: An extension of the
proposed framework is presented which facilitates
more ﬂexible fusion of the modalities at the feature
level by allowing the modalities to have different
sparsity patterns. This extension provides a frame-
work to tune the trade-off between independent sparse
representation and joint sparse representation among
the modalities. Improved performance for multimodal
classiﬁcation: The proposed methods achieve the state-
of-the-art performance in a range of different multi-
modal classiﬁcation tasks.
In particular, we have
provided extensive performance comparison between
the proposed algorithms and some of the competing
tasks of
methods from literature for four different
multimodal face recognition, multi-view face recog-
nition, multimodal biometric recognition, and multi-
view action recognition. The experimental results on
these datasets have demonstrated the usefulness of
the proposed formulation, showing that the proposed
algorithm can be readily applied to several different
application domains.

• Improved efﬁciency for sparse-representation based
classiﬁcation: It is shown here that, compared to the
counterpart sparse representation classiﬁcation algo-
rithms, the proposed algorithms are more computa-
tionally efﬁcient in the sense that they can be equipped
with more compact dictionaries and still achieve su-
perior performance.

A. Paper organization

The rest of the paper is organized as follows. In Sec-
tion II, unsupervised and supervised dictionary learning
algorithms for single source of information are reviewed.
Joint sparse representation for multimodal classiﬁcation is
also reviewed in this section. Section III proposes the task-
driven multimodal dictionary learning algorithms. Compar-
ative studies on several benchmarks and concluding results
are presented in Section IV and Section V, respectively.

B. Notation

Vectors are denoted by bold lower case letters and
matrices by bold upper case letters. For a given vector x,
xi is its ith element. For a given ﬁnite set of indices γ,
xγ is the vector formed with those elements of x indexed
in γ. Symbol → is used to distinguish the row vectors
from column vectors, i.e. for a given matrix X, the ith row
and jth column of matrix are represented as xi→ and xj,
respectively. For a given ﬁnite set of indices γ, Xγ is the
matrix formed with those columns of X indexed in γ and
Xγ→ is the matrix formed with those rows of X indexed
in γ. Similarly, for given ﬁnite sets of indices γ and ψ,
Xγ→,ψ is the matrix formed with those rows and columns
of X indexed in γ and ψ, respectively. xij is the element

1The source code of the proposed algorithm is released here: https:

//github.com/soheilb/multimodal dictionary learning

3

of X at row i and column j. The lq norm, q ≥ 1, of a
vector x ∈ Rm is deﬁned as (cid:107)x(cid:107)(cid:96)q = ((cid:80)m
j=1 |xj|q)1/q.
The Frobenius norm and (cid:96)1q norm, q ≥ 1, of matrix
(cid:17)1/2
X ∈ Rm×n is deﬁned as (cid:107)X(cid:107)F =
and (cid:107)X(cid:107)(cid:96)1q = (cid:80)m
{xi|i ∈ γ} is shortly denoted as {xi}.

i=1 (cid:107)xi→(cid:107)(cid:96)q , respectively. The collection

(cid:16)(cid:80)m
i=1

j=1 x2
ij

(cid:80)n

II. BACKGROUND

A. Dictionary learning

Dictionary learning has been widely used in various tasks
such as reconstruction, classiﬁcation, and compressive sens-
ing [29], [33], [42], [43]. In contrast to principal component
analysis (PCA) and its variants, dictionary learning algo-
rithms generally do not impose orthogonality condition and
are more ﬂexible allowing to be well-tuned to the training
data. Let X = [x1, x2, . . . , xN ] ∈ Rn×N be the collection
of N (normalized) training samples that are assumed to be
statistically independent. Dictionary D ∈ Rn×d can then
be obtained as the minimizer of the following empirical
cost [22]:

gN (D) (cid:44) 1
N

N
(cid:88)

i=1

lu (xi, D)

(1)

the

regularizing convex set D (cid:44) {D ∈
over
Rn×d|(cid:107)dk(cid:107)(cid:96)2 ≤ 1, ∀k = 1, . . . , d}, where dk is the kth
column, or atom, in the dictionary and the unsupervised
loss lu is deﬁned as
lu (x, D) (cid:44) min
α∈Rd

+λ1(cid:107)α(cid:107)(cid:96)1 +λ2(cid:107)α(cid:107)2
(cid:96)2

(cid:107)x−Dα(cid:107)2
(cid:96)2

, (2)

which is the optimal value of the sparse coding problem
with λ1 and λ2 being the regularizing parameters. While
λ2 is usually set to zero to exploit sparsity, using λ2 > 0
makes the optimization problem in Eq. (2) strongly convex
resulting in a differentiable cost function [29]. The index u
of lu is used to emphasize that the above dictionary learning
formulation is an unsupervised method. It is well-known
that one is often interested in minimizing an expected
risk, rather than the perfect minimization of the empirical
cost [44]. An efﬁcient online algorithm is proposed in [19]
to ﬁnd the dictionary D as the minimizer of the following
stochastic cost over the convex set D:

g (D) (cid:44) Ex [lu (x, D)] ,

(3)

where it is assumed that the data x is drawn from a ﬁnite
probability distribution p(x) which is usually unknown
and Ex [.] is the expectation operator with respect to the
distribution p(x).

The trained dictionary can then be used to (sparsely)
reconstruct the input. The reconstruction error has been
shown to be a robust measure for classiﬁcation tasks [10],
[45]. Another use of a given trained dictionary is for feature
extraction where the sparse code α(cid:63)(x, D), obtained as a
solution of (2), is used as a feature vector representing the
input signal x in the classical expected risk optimization
for training a classiﬁer [29]:

min
w∈W

Ey,x [l (y, w, α(cid:63)(x, D))] +

ν
2

(cid:107)w(cid:107)2
(cid:96)2

,

(4)

4

where y is the ground truth class label associated with
the input x, w is model (classiﬁer) parameters, ν is a
regularizing parameter, and l is a convex loss function that
measures how well one can predict y given the feature
vector α(cid:63) and classiﬁer parameters w. The expectation
Ey,x is taken with respect to the probability distribution
p(y, x) of the labeled data. Note that in Eq. 4, the dictionary
D is ﬁxed and independent of the given task and class label
y. In task-driven dictionary learning, on the other hand,
a supervised formulation is used which ﬁnds the optimal
dictionary and classiﬁer parameters jointly by solving the
following optimization problem [29]:

min
D∈D,w∈W

Ey,x [lsu (y, w, α(cid:63)(x, D))] +

(cid:107)w(cid:107)2
(cid:96)2

.

(5)

ν
2

The index su of convex loss function lsu is used to
emphasize that the above dictionary learning formulation
is supervised. The learned task-driven dictionary has been
shown to result in a superior performance compared to the
unsupervised setting [29]. In this setting, the sparse codes
are indeed the optimized latent features for the classiﬁer.

B. Multimodal joint sparse representation

Joint sparse representation provides an efﬁcient tool for
feature-level fusion of sources of information [12], [14],
[46]. Let S (cid:44) {1, . . . , S} be a ﬁnite set of available
modalities and let xs ∈ Rns
, s ∈ S, be the feature
vector for the sth modality. Also let Ds ∈ Rns×d be
the corresponding dictionary for the sth modality. For
now, it is assumed that the multimodal dictionaries are
constructed by collections of the training samples from
i.e. jth atom of dictionary Ds is
different modalities,
the jth training sample from the sth modality. Given a
multimodal input {xs|s ∈ S}, shortly denoted as {xs}, an
optimal sparse matrix A(cid:63) ∈ Rd×S is obtained by solving
the following (cid:96)12-regularized reconstruction problem:

argmin
A=[α1...αS ]

1
2

S
(cid:88)

s=1

(cid:107)xs − Dsαs(cid:107)2
(cid:96)2

+ λ(cid:107)A(cid:107)(cid:96)12 ,

(6)

where λ is a regularization parameter. Here αs is the sth-
column of A which corresponds to the sparse representa-
tion for the sth modality. Different algorithms have been
proposed to solve the above optimization problem [47],
[48]. We use the efﬁcient alternating direction method
of multipliers (ADMM) [49] to ﬁnd A(cid:63). The (cid:96)12 prior
encourages row sparsity in A(cid:63), i.e. it encourages collab-
oration among all the modalities by enforcing the same
dictionary atoms from different modalities that present the
same event, to be used for reconstructing the inputs {xs}.
An (cid:96)11 term can also be added to the above cost function
to extend it to a more general framework where sparsity
can also be sought within the rows, as will be discussed
in Section III-D. It has been shown that
joint sparse
representation can result in a superior performance in fusing
multimodal sources of information compared to other infor-
mation fusion techniques [45]. We are interested in learning
multimodal dictionaries under the joint sparsity prior. This

has several advantages over a ﬁxed dictionary consisting of
training data. Most importantly, it can potentially remove
the redundant and noisy information by representing the
training data in a more compact form. Also using the
supervised formulation, one expects to ﬁnd dictionaries that
are well-adapted to the discriminative tasks.

III. MULTIMODAL DICTIONARY LEARNING

In this section, online algorithms for unsupervised and

supervised multimodal dictionary learning are proposed.

A. Multimodal unsupervised dictionary learning

Unsupervised multimodal dictionary learning is derived
by extending the optimization problem characterized in
Eq. (3) and using the joint sparse representation of (6) to
enforce collaborations among modalities. Let the minimum
u ({xs, Ds}) of the joint sparse coding be deﬁned as
cost l(cid:48)

min
A

1
2

S
(cid:88)

s=1

(cid:107)xs − Dsαs(cid:107)2
(cid:96)2

+ λ1(cid:107)A(cid:107)(cid:96)12 +

(cid:107)A(cid:107)2

F , (7)

λ2
2

where λ1 and λ2 are the regularizing parameters. The addi-
tional Frobenius norm (cid:107).(cid:107)F compared to Eq. (6) guarantees
a unique solution for the joint sparse optimization problem.
In the special case when S = 1, optimization (7) reduces
to the well-studied elastic-net optimization [50]. By natural
extension of the optimization problem (3), the unsupervised
multimodal dictionaries are obtained by:

Ds(cid:63) = argmin
Ds∈Ds

Exs [l(cid:48)

u ({xs, Ds})] , ∀s ∈ S,

(8)

where the convex set Ds is deﬁned as

Ds (cid:44) {D ∈ Rns×d|(cid:107)dk(cid:107)(cid:96)2 ≤ 1, ∀k = 1, . . . , d}.

(9)

It is assumed that data xs is drawn from a ﬁnite (un-
known) probability distribution p(xs). The above optimiza-
tion problem can be solved using the classical projected
stochastic gradient algorithm [51] which consists of a
sequence of updates as follows:

Ds ← ΠDs [Ds − ρt∇Ds l(cid:48)

u ({xs

t , Ds})] ,

(10)

where ρt is the gradient step at time t and ΠD is the
orthogonal projector onto set D. The algorithm converges
to a stationary point for a decreasing sequence of ρt [51],
[52]. A typical choice of ρt is shown in the next section.
This problem can also be solved using online matrix
factorization algorithm [26]. It should be noted that the
while the stochastic gradient descent does converge, it is
not guaranteed to converge to a global minimum due to
the non-convexity of the optimization problem [26], [44].
However, such stationary point is empirically found to be
sufﬁciently good for practical applications [21], [28].

B. Multimodal task-driven dictionary learning

As discussed in Section II, the unsupervised setting does
not take into account the label of the training data, and
the dictionaries are obtained by minimizing the reconstruc-
tion error. However, for classiﬁcation tasks, the minimum
reconstruction error does not necessarily result in discrimi-
native dictionaries. In this section, a multimodal task-driven
dictionary learning algorithm is proposed that enforces
collaboration among the modalities both at the feature level
using joint sparse representation and the decision level
using a sum of the decision scores. We propose to learn
the dictionaries Ds(cid:63), ∀s ∈ S, and the classiﬁer parameters
ws(cid:63), ∀s ∈ S, shortly denoted as the set {Ds(cid:63), ws(cid:63)}, jointly
as the solution of the following optimization problem:

min
{Ds∈Ds,ws∈W s}

f ({Ds, ws}) +

(cid:107)ws(cid:107)2
(cid:96)2

,

(11)

ν
2

S
(cid:88)

s=1

where f is deﬁned as the expected cumulative cost:

f ({Ds, ws}) = E

lsu(y, ws, αs(cid:63)),

(12)

S
(cid:88)

s=1

is

the

sth

column of

where αs(cid:63)
the minimizer
A(cid:63)({xs, Ds}) of
the optimization problem (7) and
lsu(y, w, α) is a convex loss function that measures how
well the classiﬁer parametrized by w can predict y by
observing α. The expectation is taken with respect to the
joint probability distribution of the multimodal inputs {xs}
and label y. Note that αs(cid:63) acts as a hidden/latent feature
vector, corresponding to the input xs, which is generated by
the learned discriminative dictionary Ds(cid:63). In general, lsu
can be chosen as any convex function such that lsu(y, ., .)
is twice continuously differentiable for all possible values
of y. A few examples are given below for binary and
multiclass classiﬁcation tasks.

1) Binary classiﬁcation: In a binary classiﬁcation task
where the label y belongs to the set {−1, 1}, lsu can be
naturally chosen as the logistic regression loss

lsu(y, w, α(cid:63)) = log(1 + e−ywT α(cid:63)

),

(13)

where w ∈ Rd is the classiﬁer parameters. Once the
optimal {Ds, ws} are obtained, a new multimodal sample
{xs} is classiﬁed according to sign of (cid:80)S
s=1 wsT α(cid:63) due
to the uniform monotonicity of (cid:80)S
s=1 lsu. For simplicity,
the intercept term for the linear model is omitted here,
but it can be easily added. One can also use a bilinear
model where, instead of a set of vectors {ws}, a set of
matrices {W s} are learned and a new multimodal sample
is classiﬁed according to the sign of (cid:80)S
s=1 xsT W sα(cid:63).
Accordingly, the (cid:96)2-norm regularization of Eq. (11) needs
to be replaced with the matrix Frobenius norm. The bilinear
model is richer than the linear model and can sometimes
result in better classiﬁcation performance but needs more
careful training to avoid over-ﬁtting.

5

2) Multiclass classiﬁcation: Multiclass classiﬁcation can
be formulated using a collections of (independently learned)
binary classiﬁers in a one-vs-one or one-vs-all setting.
Multiclass classiﬁcation can also be handled in an all-vs-all
setting using the softmax regression loss function. In this
scheme, the label y belongs to the set {1, . . . , K} and the
softmax regression loss is deﬁned as

lsu(y, W , α(cid:63)) = −

1{y=k} log

K
(cid:88)

k=1

(cid:32)

ewT
k α(cid:63)
l=1 ewT

(cid:80)K

l α(cid:63)

(cid:33)

,

(14)
where W = [w1 . . . wK] ∈ Rd×K, and 1{.} is the indicator
function. Once the optimal {Ds, W s} are obtained, a new
multimodal sample {xs} is classiﬁed as

argmaxk∈{1,...,K}

(cid:32)

S
(cid:88)

s=1

T αs(cid:63)

k

ews
l=1 ews

l

(cid:80)K

T αs(cid:63)

(cid:33)

.

(15)

In yet another all-vs-all setting, the multiclass classiﬁcation
task can be turned into a regression task in which the scaler
label y is changed to a binary vector y ∈ RK, where the
kth coordinate corresponding to the label of {xs} is set to
one and the rest of the coordinates are set to zero. In this
setting, lsu is deﬁned as

lsu(y, W , α(cid:63)) =

1
2
where W ∈ RK×d. Having obtained the optimal
{Ds, W s}, the test sample {xs} is then classiﬁed as

(cid:107)y − W α(cid:63)(cid:107)2
(cid:96)2

(16)

,

S
(cid:88)

argmink∈{1,...,K}

(cid:107)qk − W sαs(cid:63)(cid:107)2
(cid:96)2

,

(17)

s=1
where qk is a binary vector in which its kth coordinate is
one and its remaining coordinates are zero.

In choosing between the one-vs-all setting, in which
independent multimodal dictionaries are trained for each
class, and the multiclass formulation, in which multimodal
dictionaries are shared between classes, a few points should
be considered. In the one-vs-all setting, the total number of
dictionary atoms is equal to dSK in the K-class classiﬁ-
cation while in the multiclass setting the number is equal
to dS. It should be noted that in the multiclass setting a
larger dictionary is generally required to achieve the same
level of performance to capture the variations among all
classes. However, it is generally observed that the size
of the dictionaries in multiclass setting is not required to
grow linearly as the number of classes increases due to
atom sharing among the different classes. Another point to
consider is that the class-speciﬁc dictionaries of the one-
vs-all approach are independent and can be obtained in
parallel. In this paper, the multiclass formulation is used
to allow feature sharing among the classes.

C. Optimization

The main challenge in optimizing (11) is the non-
differentiability of A(cid:63)({xs, Ds}). However,
it can be
shown that although the sparse coefﬁcients A(cid:63) are obtained
by solving a non-differentiable optimization problem, the

6

function f ({Ds, ws}), deﬁned in Eq. (12), is differen-
tiable on D1 × · · · DS × W 1 × · · · W S, and therefore its
gradients are computable. To ﬁnd the gradient of f with
respect to Ds, one can ﬁnd the optimality condition of the
optimization (7) or use the ﬁxed point differentiation [36],
[38] and show that A(cid:63) is differentiable over its non-zero
label
rows. Without
y admits a ﬁnite set of values such as those deﬁned in
Eqs. (13) and (14). The same algorithm can be derived for
the scenario when y belongs to a compact subset of a ﬁnite-
dimensional real vector space as in Eq. (16). A couple of
mild assumptions are required to prove the differentiability
of f which are direct generalizations of those required for
the single modal scenario [29] and are listed below:

loss of generality, we assume that

Assumption (A). The multimodal data (y, {xs}) admit a

probability density p with compact support.

Assumption (B). For all possible values of y, p(y, .)
is continuous and lsu(y, .) is twice continuously differen-
tiable.

The ﬁrst assumption is reasonable when dealing with
the signal/image processing applications where the acquired
values obtained by the sensors are bounded. Also all the
given examples for lsu in the previous section satisfy the
second assumption. Before stating the main proposition of
this paper below, the term active set is deﬁned.

Deﬁnition 3.1 (Active set): The active set Λ of the solu-
tion A(cid:63) of the joint sparse coding problem (7) is deﬁned
to be

Λ = {j ∈ {1, . . . , d} : (cid:107)a(cid:63)

j→(cid:107)(cid:96)2 (cid:54)= 0},

(18)

where a(cid:63)

j→ is the jth row of A(cid:63).

Proposition 3.1 (Differentiability and gradients of f ):
Let λ2 > 0 and the assumptions (A) and (B) hold. Let
Υ = ∪j∈ΛΥj where Υj = {j, j + d, . . . , j + (S − 1)d}.
Let the matrix ˆD ∈ Rn×|Υ| be deﬁned as
ˆD =

(cid:104) ˆD1 . . . ˆD|Λ|

(19)

(cid:105)

,

where ˆDj = blkdiag(d1
j ) ∈ Rn×S, ∀j ∈ Λ, is
j , . . . , dS
the collection of the jth active atoms of the multimodal
j is the jth active atom of Ds, blkdiag is
dictionaries, ds
the block diagonalization operator, and n = (cid:80)
s∈S ns. Also
let matrix ∆ ∈ R|Υ|×|Υ| be deﬁned as

∆ = blkdiag(∆1, . . . , ∆|Λ|),

(20)

j→ ∈
I −
where ∆j =
RS×S, ∀j ∈ Λ, and I is the identity matrix. Then, the
function f deﬁned in Eq. (12) is differentiable and ∀s ∈ S,

(cid:107)a(cid:63)

(cid:107)a(cid:63)

j→

1
j→(cid:107)(cid:96)2

1
j→(cid:107)(cid:96)2

3 a(cid:63)

T a(cid:63)

∇wsf = E [∇ws lsu (y, ws, αs(cid:63))] ,
∇Dsf = E

(xs − Dsαs(cid:63)) βT

(cid:104)

˜s − Dsβ˜sαs(cid:63)T (cid:105)

,

(21)

where ˜s = {s, s + S, . . . , s + (d − 1)S} and β ∈ RdS is
deﬁned as

βΥc = 0, βΥ = ( ˆDT ˆD + λ1∆ + λ2I)−1g,

(22)
T (cid:80)S
s=1 lsu(y, ws, αs(cid:63))), Υc =
in which g = vec(∇A(cid:63)
{1, . . . , dS} \Υ, βΥ ∈ R|Υ| is formed of those rows of β
indexed by Υ, and vec(.) is the vectorization operator.

Λ→

The proof of this proposition is given in the Appendix.
A stochastic gradient descent algorithm to ﬁnd the optimal
dictionaries {Ds(cid:63)} and classiﬁers {ws(cid:63)} is described in
Algorithm 1. The stochastic gradient descent algorithm is
guaranteed to converge under a few assumptions that are
mildly stricter than those in this paper (requires three-times
differentiability) [53]. To further improve the convergence
of the proposed stochastic gradient descent algorithm, a
classic mini-batch strategy is used in which a small batch
of the training data are sampled in each batch, instead of 1
sample, and the parameters are updated using the averaged
updates of the batch. This has additional advantage in which
ˆDT ˆD and the corresponding factorization of the ADMM
for solving the sparse coding problem can be computed
once for the whole batch. For the special case when S = 1,
the proposed algorithm reduces to the single-modal task-
driven dictionary learning algorithm in [29]. Selecting λ2 in
Eq. (7) to be strictly positive guarantees the linear equations
of (22) to have a unique solution. In other words, it is easy
to show that the matrix ( ˆDT ˆD + λ1∆ + λ2I) is positive
deﬁnite given λ1 ≥ 0, λ2 > 0. However, in practice it is
observed that the solution of the joint sparse representation
problem is numerically stable since ˆD becomes full-column
rank when sparsity is sought with a sufﬁciently large λ1,
and λ2 can be set to zero. It should be noted that the
assumption of ˆD being a full column rank matrix is a
common assumption in sparse linear regression [26]. As
in any non-convex optimization algorithm, if the algorithm
is not initialized properly, it may yield poor performance.
Similar to [29], the dictionaries {Ds} are initialized by the
solution of the unsupervised multimodal dictionary learning
algorithm. Upon assignment of the initial dictionaries,
parameters {ws} of the classiﬁers are set by solving (11)
only with respect to {ws} which is a convex optimization
problem.

D. Extension

We now present an extension of the proposed algorithm
with a more ﬂexible structure on the sparse codes. Joint
sparse representation relies on the fact that all the modalities
share the same sparsity pattern in which, if a multimodal
training sample is selected to reconstruct the input, then
all the modalities within that training sample are active.
However, this group sparsity constraint, imposed by the
(cid:96)12 norm, may be too stringent for some applications [45],
[54], for example in the scenarios where the modalities
have different noise levels or when the heterogeneity of
the modalities imposes different sparsity levels for the
reconstruction task. A natural relaxation to the joint sparsity
prior is to let the multimodal inputs not share the full
active set which can be achieved by replacing the (cid:96)12 norm
with a combination of the (cid:96)12 and (cid:96)11 norms ((cid:96)12 − (cid:96)11
norm). Following the same formulation as in Section III-B,
let A(cid:63)({xs, Ds}) in Eq. (11) be the minimizer of the

Algorithm 1 Stochastic gradient descent algorithm for multi-
modal task-driven dictionary learning.

Input: Regularization parameters λ1, λ2, ν,

learning rate parameters
ρ, t0, number of iterations T , initial dictionaries {Ds ∈ Ds}s∈S ,
initial model parameters {ws ∈ W s}s∈S .

Output: Learned {Ds, ws}
1: for t = 1, . . . , T do
2:
3:

t , . . . , xS
Draw a random sample (x1
t , yt) from the training data.
Find solution A(cid:63) = (cid:2)α(cid:63)1 . . . α(cid:63)S (cid:3) ∈ Rd×S of the joint sparse
coding problem

argmin
A=[α1...αS]

1
2

S
(cid:88)

s=1

(cid:107)xs

t − Dsαs(cid:107)2
(cid:96)2

+ λ1(cid:107)A(cid:107)(cid:96)12 +

(cid:107)A(cid:107)2
F .

λ2
2

4:
5:
6:
7:

8:
9:

Compute set of active rows Λ of A(cid:63) using (18).
Compute ˆD ∈ Rn×|Υ| using (19).
Compute ∆ ∈ R|Υ|×|Υ| using (20).
Compute β ∈ RdS as:

βΥc = 0, βΥ = ( ˆDT ˆD + λ1∆ + λ2I)−1g,

T

Λ→

(cid:80)S

where Υ = ∪j∈Λ{j, j + d, . . . , j + (S − 1)d} and g =
s=1 lsu(yt, ws, αs(cid:63))).
vec(∇A(cid:63)
Choose the learning rate ρt ← min(ρ, ρ t0
Update the parameters by a projected gradient step:
ws ← ΠW s [ws − ρt (∇ws lsu (yt, ws, αs(cid:63)) + νws)] ,
Ds ← ΠDs

˜s − Dsβ˜sαs(cid:63)T (cid:17)(cid:105)

t − Dsαs(cid:63)) βT

Ds − ρt

t ).

(xs

(cid:16)

(cid:104)

,

∀s ∈ S, where ˜s = {s, s + S, . . . , s + (d − 1)S}.

10: end for

following optimization problem:

min
A

1
2

S
(cid:88)

s=1

(cid:107)xs − Dsαs(cid:107)2
(cid:96)2

+ λ1(cid:107)A(cid:107)(cid:96)12 + λ(cid:48)

1(cid:107)A(cid:107)(cid:96)11 +

(cid:107)A(cid:107)2
F ,

λ2
2

(23)

where λ(cid:48)
1 is the regularization parameter for the added (cid:96)11
norm and other terms are the same as those in Eq. (7). The
selection of λ1 and λ(cid:48)
1 inﬂuences the sparsity pattern of
A(cid:63). Intuitively, as λ1/λ(cid:48)
1 increases, the group constraint be-
comes dominant and more collaboration is enforced among
the modalities. On the other hand, small values of λ1/λ(cid:48)
1
encourage independent reconstructions across modalities.
In the extreme case of λ1 being set to zero, the above
optimization problem is separable across the modalities.
The above formulation brings added ﬂexibility with the cost
of one additional design parameter which is obtained in this
paper using cross-validation.

Here we present how the Algorithm 1 should be modiﬁed
to solve the supervised multimodal dictionary learning
problem under the mixed (cid:96)12 − (cid:96)11 constraint. The proof
for obtaining the algorithm is similar to the one for the
(cid:96)12 norm and is brieﬂy discussed in the appendix. In
Algorithm 1, let A(cid:63) be the solution of the optimization
problem (23) and let Λ be the set of its active rows. Let
Ψ ⊆ {1, . . . , S|Λ|} be the set of indices with non-zero
T ); i.e. it consists of non-zero entries
entries in vec(A(cid:63)
of the active rows of A(cid:63). Let ˆD, ∆, and g be the same as
those deﬁned in algorithm 1. Then, β ∈ RdS is updated as

Λ→

βΥc = 0, βΥ = ( ˆDT
Ψ

ˆDΨ + λ1∆Ψ→,Ψ + λ2I)−1gΨ,

7

Fig. 2: Extracted modalities from a sample in AR dataset.

where Υ is the set of indices with non-zero entries in
vec(A(cid:63)T
) and Υc = {1, . . . , dS} \Υ. Note that Υ is
deﬁned over the entire matrix A(cid:63) while Ψ is deﬁned over its
active rows. The rest of the algorithm remains unchanged.

IV. RESULTS AND DISCUSSION

The performance of the proposed multimodal dictio-
nary learning algorithms are evaluated on the AR face
database [55], the CMU Multi-PIE dataset [56], the IXMAS
action recognition dataset [57] and the WVU multimodal
ls is chosen to be
dataset [58]. For these algorithms,
the quadratic loss of Eq. (16) to handle the multiclass
classiﬁcation. In our experiments, it is observed that us-
ing the multiclass formulation achieves similar classiﬁ-
cation performance compared to using the logistic loss
formulation of Eq. (13) in the one-vs-all setting. Regu-
larization parameters λ1 and ν are selected using cross-
validation in the sets {0.01 + 0.005k|k ∈ {−3, 3}} and
{10−2, ..., 10−9}, respectively. It is observed that when the
number of dictionary atoms is kept small compared to the
number of training samples, ν can be arbitrarily set to a
small value, e.g. ν = 10−8, for the normalized inputs.
When the mixed (cid:96)12 − (cid:96)11 norm is used, the regularization
parameters λ1 and λ(cid:48)
1 are selected by cross-validation
in the set {0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05}. The
parameter λ2 is set to zero in most of the experiments
except when using the (cid:96)11 prior in Section IV-B1 where
a small positive value for λ2 was required for convergence.
The learning parameter ρt
is selected according to the
(cid:1) where
heuristic proposed in [29], i.e. ρt = min (cid:0)ρ, ρ t0
ρ and t0 are constants. This results in a constant learning
rate during the ﬁrst t0 iterations and an annealing strategy
of 1/t for the rest of the iterations. It is observed that
choosing t0 = T /10, where T is the total number of
iterations over the whole training set, works well for all
of our experiments. Different values of ρ are tried during
the ﬁrst few iterations and the one that results in minimum
error on a small validation set is retained. T is set equal
to be 20 in all the experiments. We observed empirically
that the selection of these parameters is quite robust and
small variations in their values do not affect considerably
the obtained results. We also used a mini-batch size of 100
in all our experiments. It should also be noted that design
parameters for the competitive algorithms are also selected
using cross-validation for a fair comparison.

t

A. AR face recognition

The AR dataset consists of faces under different poses,
illumination and expression conditions, captured in two
sessions. A set of 100 users are used, each consisting of

8

TABLE III: Multimodal classiﬁcation results obtained for the AR datasets

SVM-Maj

SVM-Sum LR-Maj

LR-Sum MKL

JSRC [14]

JDSRC [7]

85.57

92.14

85.00

91.14

91.14

96.14

96.14

SMDL(cid:96)11
95.86

SMDL(cid:96)12
96.86

SMDL(cid:96)12−(cid:96)11
97.14

TABLE I: Correct classiﬁcation rates obtained using the
whole face modality for the AR database.

SVM MKL [59]

LR

SRC [10] UDL

SDL [29]

86.43

82.86

81.00

88.86

89.58

90.57

TABLE IV: Comparison of the reconstructive-based (JSRC
and JSRC-UDL) and the proposed discriminative-based
(SMDL(cid:96)12 ) classiﬁcation algorithms obtained using the
joint sparsity prior for different numbers of dictionary
atoms per class on the AR dataset.

TABLE II: Comparison of the (cid:96)11 and (cid:96)12 priors for mul-
timodal classiﬁcation. Modalities include 1. left periocular,
2. right periocular, 3. nose, 4. mouth, and 5. face.

Modalities
UMDL(cid:96)11
UMDL(cid:96)12
SMDL(cid:96)11
SMDL(cid:96)12

{1, 2}

{1, 2, 3}

{1, 2, 3, 4}

{1, 2, 3, 4, 5}

81.9
82.6
83.86
86.43

87.57
87.86
89.86
89.86

90.14
92.00
92.42
93.57

95.57
96.29
95.86
96.86

seven images from the ﬁrst session as training samples and
seven images from the second session as test samples. A
small randomly selected portion of the training set, 50 out
of 700, is used as validation set for optimizing the design
parameters. Fusion is taken on ﬁve modalities which are
the left and right periocular, nose, mouth, and the whole
face modalities, similar to the setup in [14], [45]. A test
sample from the AR dataset and the extracted modalities
are shown in Fig. 2. Raw pixels are ﬁrst PCA-transformed
and then normalized to have zero mean and unit l2 norm.
The dictionary size for the dictionary learning algorithms
is chosen to be four per class, resulting in dictionaries of
overall 400 atoms.

Classiﬁcation using the whole face modality: The classi-
ﬁcation results using the whole face modality are shown in
Table I. The results are obtained using linear support vector
machine (SVM) [60], multiple kernel learning (MKL) [59],
logistic regression (LR) [60], sparse representation classi-
ﬁcation (SRC) [10], and unsupervised and supervised dic-
tionary learning algorithms (UDL and SDL) [29]. For the
MKL algorithm, linear, polynomial, and RBF kernels are
used. The UDL and SDL are equipped with the quadratic
classiﬁer (16). The SDL results in the best performance.

(cid:96)11 vs (cid:96)12 sparse priors for multimodal classiﬁcation: A
straightforward way of utilizing the single-modal dictionary
learning algorithms, namely UDL and SDL, for multimodal
classiﬁcation is to train independent dictionaries and clas-
siﬁers for each modality and then combine the individual
scores for a fused decision. This way of fusion is equivalent
to using the (cid:96)11 norm on A,
in
Eq. (7) (or setting λ1 to zero in Eq. (23)) which does not
enforce row sparsity in the sparse coefﬁcients. We denote
the corresponding unsupervised and supervised multimodal
dictionary learning algorithms using only the (cid:96)11 norm as
UMDL(cid:96)11 and SMDL(cid:96)11, respectively. Similarly, the pro-
posed unsupervised and supervised multimodal dictionary
learning algorithms using the (cid:96)12 norm are denoted as

instead of (cid:96)12 norm,

atoms/class

JSRC JSRC-UDL

1
2
3
4
5
6
7

46.14
69.00
79.57
88.14
91.00
94.43
96.14

71.71
78.86
83.57
91.14
94.85
96.28
96.14

SMDL(cid:96)12
91.28
95.00
95.71
96.86
97.14
96.71
96.00

UMDL(cid:96)12 and SMDL(cid:96)12. Table II compares the perfor-
mance of the multimodal dictionary learning algorithms
under the two priors. As shown, the proposed algorithms
with (cid:96)12 prior, which enforces collaborations among the
modalities, have better fusion performances than those with
(cid:96)11 prior. In particular, SMDL(cid:96)12 has signiﬁcantly better
performance than the SMDL(cid:96)11 for fusion of the ﬁrst and
second (left and right periocular) modalities. This agrees
with the intuition that these modalities are highly correlated
and learning the multimodal dictionaries jointly indeed
improves the recognition performance.

Comparison with other fusion methods: The perfor-
mances of the proposed fusion algorithms under different
sparsity priors are compared with those of the several state-
of-the-art decision-level and feature-level fusion algorithms.
In addition to (cid:96)11 and (cid:96)12 priors, we evaluate the proposed
supervised multimodal dictionary learning algorithm with
the mixed (cid:96)12−(cid:96)11 norm which is denoted as SMDL(cid:96)12−(cid:96)11.
One way to achieve decision-level fusion is to train in-
dependent classiﬁers for each modality and aggregate the
outputs by either adding the corresponding scores of each
modality to come up with the fused decision, or using the
majority voting among the independent decisions obtained
from different modalities. These approaches are abbrevi-
ated with Sum and Maj, respectively, and are used with
SVM and LR classiﬁers for decision-level fusion. The pro-
posed methods are also compared with feature-level fusion
methods including the joint sparse representation classiﬁer
(JSRC) [14], joint dynamic sparse representation classiﬁer
(JDSRC) [7], and MKL. For the JSRC and JDSRC, the
dictionary consists of all the training samples. Table III
compares the performance of our proposed algorithms with
the other fusion algorithms for the AR dataset. As expected,
the multimodal fusion results in signiﬁcant performance
improvement compared to using only the whole face modal-
ity. Moreover, the proposed SMDL(cid:96)12 and SMDL(cid:96)12−(cid:96)11
achieve the superior performances.

Reconstructive vs discriminative formulation with joint

9

Fig. 3: Computational time required to solve the optimiza-
tion problem (7) for a given test sample.

TABLE V: Comparison of the supervised multimodal dic-
tionary learning algorithms with different sparsity priors for
face recognition under occlusion on the AR dataset.

SMDL(cid:96)12
89.00

SMDL(cid:96)11
90.54

SMDL(cid:96)12−(cid:96)11
91.15

sparsity prior: Comparison of the algorithms with joint
sparsity priors in Table III indicates that
the proposed
SMDL(cid:96)12 algorithm equipped with dictionaries of size 400
achieves relatively better results than the JSRC that uses
dictionaries of size 700. The results conﬁrm the idea that
by using the supervised formulation, compared to using the
reconstruction error, one can achieve better classiﬁcation
performance even with more compact dictionaries. For
further comparison, an experiment is performed in which
the correct classiﬁcation rates of the reconsturtive and
discriminative formulations are compared when the their
dictionary sizes are kept equal. For a given number of
dictionary atoms per class d, dictionaries of JSRC are
thus constructed by random selection of d train samples
from different classes. This is different from the standard
JSRC, utilized for the results in Table III, in which all the
training samples are used to construct the dictionaries [14].
Moreover, to utilize all the available training samples for
the reconstructive approach and make a more meaningful
comparison, we use the unsupervised multimodal dictionary
learning algorithm of Eq. (8) to train class-speciﬁc sub-
dictionaries which minimizes the reconstruction error in
approximating the training samples for a given class. These
sub-dictionaries are then stacked to construct
the ﬁnal
dictionaries, similar to the approach in [22]. We call this
algorithm as JSRC-UDL to indicate that the dictionaries are
indeed learned by the reconstructive formulation. Table IV
summarizes the recognition performance of JSRC and
JSRC-UDL in comparison to the proposed SMDL(cid:96)12, which
enjoys a discriminative formulation, for different number of
dictionary atoms per class. As seen, SMDL(cid:96)12 outperforms
the reconstructive approaches, especially when the number
of dictionary is chosen to be relatively small. This is the
main advantage of SMDL(cid:96)12 compared to the reconstructive
approaches in which more compact dictionaries can be

Fig. 4: Conﬁgurations of the cameras and sample multi-
view images from CMU Multi-Pie dataset.

used for the recognition task that is important for the real-
time applications. It is clear that reconstructive model can
only result in comparable performance when the dictionary
size is chosen to be relatively large. On the other hand,
the SMDL(cid:96)12 algorithm may get over-ﬁtted with the large
number of dictionary atoms. In terms of computational
expense at test time, as discussed in [14], the time required
to solve the optimization problem (7) is expected to be
linear in the dictionary size using the efﬁcient ADMM if the
required matrix factorization is cashed beforehand. Typical
computational time to solve (7) for a given multimodal test
sample is shown in Fig. 3 for different dictionary sizes. As
expected, it increases linearly as the size of the dictionary
increases. This illustrates the advantage of the SMDL(cid:96)12
algorithm that results in the state-of-the-art performance
with more compact dictionaries.

Classiﬁcation in presence of disguise: The AR dataset
also contains 600 occluded samples per session, overall
1200 images, where the faces are disguised using sun
glasses or scarf. Here we use these additional images to
evaluate the robustness of the proposed algorithms. Similar
to previous experiments, images from session 1 are used
as training samples and images from session 2 are used
as test data. Classiﬁcation performance under different
sparsity priors are shown in Table V and as expected, the
SMDL(cid:96)12−(cid:96)11 achieves the best performance. In presence
of occlusion, some of the modalities are less coupled and
the joint sparsity prior among all the modalities may be too
stringent as is also reﬂected in the results.

B. Multi-view recognition

In this section,

1) Multi-view face recognition:

the
performance of the proposed algorithm is evaluated for
multi-view face recognition using the CMU Multi-PIE
[56]. The dataset consists of a large num-
dataset
ber of face images under different
illuminations, view-
points, and expressions which are recorded in four
several months. Subjects
sessions over
were imaged using 13 cameras at different view-angles
of {0◦, ±15◦, ±30◦, ±45◦, ±60◦, ±75◦, ±90◦} at head
height. Illustrations for the multiple camera conﬁgurations,
as well as sample multi-view images are shown in Fig. 4.
We use the multi-view face images for 129 subjects that are

the span of

10

TABLE VI: Correct classiﬁcation rates obtained using
individual modalities in the CMU Multi-PIE database.

View

SVM MKL

LR

Left
Frontal
Right

47.30
41.15
47.30

52.85
54.10
51.85

43.65
45.40
42.85

SRC

49.85
54.25
52.55

UDL

47.80
52.10
43.10

SDL

50.45
56.10
48.50

Fig. 5: Sample frames of the IXMAS dataset from 5
different views.

TABLE VII: Correct classiﬁcation rates (CCR) obtained
using multi-view images on the CMU Multi-PIE database.

TABLE VIII: Correct classiﬁcation rates (CCR) obtained
for multi-view action recognition on the IXMAS database.

Algorithm CCR

Algorithm

SVM-Maj
62.95
SVM-Sum 69.30
72.40
70.20
77.25
76.10

MKL
JDSRC
SMDL(cid:96)11
SMDL(cid:96)12

LR-Maj
LR-Sum
JSRC
UMDL(cid:96)11
UMDL(cid:96)12
SMDL(cid:96)12−(cid:96)11

CCR

69.40
71.10
73.30
74.80
70.50
81.30

Algorithm

Junejo et al. [65]
Wu et al. [66]
Wang et al. 2 [63]
UMDL(cid:96)11
UMDL(cid:96)12

CCR

79.6
88.2
93.6
90.3
90.6

Algorithm

Tran and Sorokin [62]
Wang et al. 1 [63]
JSRC
SMDL(cid:96)11
SMDL(cid:96)12

CCR

80.2
87.8
93.6
93.9
94.8

present in all sessions. The face regions for all the poses
are extracted manually and resized to 10 × 8. Similar to
the protocol used in [61], images from session 1 at views
{0◦, ±30◦, ±60◦, ±90◦} are used as training samples. Test
images are obtained from all available view angles from
session 2 to have a more realistic scenario in which not
all
the testing poses are available in the training set.
To handle multi-view recognition using the multi-modal
formulation, we divide the available views into three sets of
{−90◦, −75◦, −60◦, −45◦}, {−30◦, −15◦, 0◦, 15◦, 30◦, },
{45◦, 60◦, 75◦, 90◦}, each of which forms a modality. A
test sample is then constructed by randomly selecting an
image from each modality. Two thousand test samples are
generated in this way. The dictionary size for the dictionary
learning algorithms is chosen to have two atoms per class.
The classiﬁcation results obtained using individual
modalities are shown in Table VI. As expected, better
classiﬁcation performance is obtained using the frontal
view. Results of the multi-view face recognition is shown
in Table VII. The proposed supervised dictionary learn-
ing algorithms outperform the corresponding unsupervised
methods and other fusion algorithms. The SMDL(cid:96)12−(cid:96)11
results in the state-of-the-art performance. It is consistently
observed in all the studied applications that the multimodal
dictionary learning algorithm with the mixed prior results
in better performance than those with individual (cid:96)12 or
(cid:96)12 prior. However, it requires one additional regularizing
parameter to be tuned. For the rest of the paper,
the
performance of the proposed dictionary learning algorithms
are only reported under the individual priors.

the proposed algorithm for

2) Multi-view action recognition: This section presents
the results of
the pur-
pose of multi-view action recognition using the IXMAS
dataset [57]. Each action is recorded simultaneously by
cameras from ﬁve different viewpoints, which are con-
sidered as modalities in this experiment. A multimodal
sample of the IXMAS dataset is shown in Fig. 5. The
dataset contains 11 action classes where each action is
repeated three times by each of the ten actors, resulting
in 330 sequences per view. The dataset include actions

such as check watch, cross arms, and scratch head. Similar
to the work in [57], [62], [63], leave-one-actor-out cross-
validation is performed and samples from all ﬁve views are
used for training and testing.

We use dense trajectories as features which are generated
using the publicly available code [63] in which a 2000
word codebook is generated by a random subset of these
trajectories and the k-means clustering as in [64]. Note that
Wang et al. [63] used HOG, HOF, and MBH descriptors in
addition to the dense trajectories. However, here only dense
trajectory descriptors are used. The number of dictionary
atoms for the proposed dictionary learning algorithms are
chosen to be 4 atoms per class, resulting in a dictionary of
44 atoms per view. The ﬁve dictionaries for JSRC are con-
structed using all the training samples, thus each dictionary,
corresponding to a different view, has 297 atoms.

Table VIII shows average accuracies over all classes
obtained using the existing algorithms and the state of
the art algorithms. The Wang et al. 1 [63] algorithm uses
only the dense trajectories as feature, similar to our setup.
The Wang et al. 2 [63] algorithm, however, uses HOG,
HOF, MBH descriptors and the spatio-temporal pyramids
in addition to the trajectory descriptor. The results show
that the proposed SMDL(cid:96)12 algorithm achieves the superior
performance while the SMDL(cid:96)11 algorithm achieves the
second best performance. This indicates that sparse coefﬁ-
cients generated by the trained dictionaries are indeed more
discriminative than the engineered features. The resulting
confusion matrix of the SMDL(cid:96)12 algorithm is shown in
Fig. 6.

C. Multimodal biometric recognition

The WVU dataset consists of different biometric modali-
ties such as ﬁngerprint, iris, palmprint, hand geometry, and
voice from subjects of different age, gender, and ethnicity.
It
is a challenging data set, as many of the samples
are corrupted with blur, occlusion, and sensor noise. In
this paper, two irises (left and right) and four ﬁngerprint
modalities are used. The evaluation is done on a subset
of 202 subjects which have more than four samples in all

TABLE IX: Correct classiﬁcation rates obtained using individual modalities in the WVU database.

Finger 1

Finger 2

Finger 3

Finger 4

Iris 1

Iris 2

SVM 56.77 ± 0.72
61.81 ± 1.39
MKL
55.64 ± 1.89
LR
67.66 ± 1.86
SRC
64.68 ± 2.11
UDL
66.29 ± 1.81
SDL

82.95 ± 2.15
82.55 ± 1.47
81.10 ± 1.85
88.68 ± 1.59
87.35 ± 2.23
88.84 ± 2.31

55.83 ± 2.03
63.50 ± 1.75
55.21 ± 2.21
69.29 ± 0.77
67.35 ± 1.22
68.61 ± 1.30

80.47 ± 0.91
81.85 ± 0.74
78.82 ± 0.66
88.68 ± 1.03
86.40 ± 0.70
87.50 ± 0.82

60.67 ± 1.78
56.31 ± 2.20
55.25 ± 1.48
65.43 ± 1.24
64.36 ± 1.37
66.05 ± 0.75

57.52 ± 1.95
54.49 ± 0.79
56.86 ± 1.70
67.78 ± 1.76
65.23 ± 2.02
67.31 ± 1.38

11

TABLE X: Multimodal classiﬁcation results obtained for
the WVU dataset.

Algorithm 4 Fingerprints

2 Irises

All modalities

90.14 ± 0.70
SVM-Maj
SVM-Sum 93.56 ± 1.26
89.23 ± 1.63
93.60 ± 0.96
93.28 ± 1.52
97.64 ± 0.44
97.17 ± 0.26
97.09 ± 0.56
97.41 ± 0.71
96.78 ± 0.57
97.56 ± 0.41

LR-Maj
LR-Sum
MKL
JSRC
JDSRC
UMDL(cid:96)11
SMDL(cid:96)11
UMDL(cid:96)12
SMDL(cid:96)12

65.30 ± 1.92
74.03 ± 1.89
63.73 ± 1.29
71.43 ± 1.91
67.23 ± 0.70
82.94 ± 0.78
79.61 ± 0.70
80.90 ± 0.61
82.83 ± 0.87
81.53 ± 2.18
83.77 ± 0.89

95.24 ± 0.92
97.09 ± 0.83
94.18 ± 1.13
98.51 ± 0.18
94.46 ± 0.87
98.89 ± 0.30
97.80 ± 0.51
98.62 ± 0.46
98.66 ± 0.43
98.78 ± 0.43
99.10 ± 0.30

nally proposed for biometric recognition systems [67]. As
seen, the SMDL(cid:96)12 algorithm outperforms the competitive
algorithms and achieves the state-of-the-art performance
using the Irises and all modalities with the rank one
recognition rate of 83.77% and 99.10%, respectively. Using
the ﬁngerprints, the performance of the SMDL(cid:96)12 is close to
the best performing algorithm, which is JSRC. The results
suggest that using joint sparsity prior indeed improves the
multimodal classiﬁcation performance by extracting the
coupled information among the modalities.

Comparison of the algorithms with the joint sparsity
the proposed SMDL(cid:96)12 algorithm
priors indicates that
equipped with dictionaries of size 404 achieves comparable,
and mostly better, results than the JSRC that uses dictionary
of size 808. Similar to the experiment in Section IV-A,
we compared the reconstructive and discriminating algo-
rithms that are based on the joint sparsity prior when the
number of dictionary atoms per class is kept equal. Fig. 8
summarizes the results of the different fusion scenarios.
As seen, SMDL(cid:96)12 signiﬁcantly outperforms JSRC and
JSRC-UDL when the number of dictionary atoms per class
is chosen to be 1 or 2. The results are consistent with
that of Table IV for the AR dataset indicating that the
proposed supervised formulation equipped with more com-
pact dictionaries achieves superior performance than that
of the reconstructive formulation for the studied biometric
recognition applications.

V. CONCLUSIONS AND FUTURE WORKS

The problem of multimodal classiﬁcation using sparsity
models was studied and a task-driven formulation was
proposed to jointly ﬁnd the optimal dictionaries and clas-
siﬁers under the joint sparsity prior. It was shown that the

Fig. 6: The confusion matrix obtained by the SMDL(cid:96)12
algorithm on the IXMAS dataset. The actions are 1: check
watch, 2: cross arms, 3: scratch head, 4: sit down, 5: get
up, 6: turn around, 7: walk, 8: wave, 9: punch, 10: kick and
11: pick up.

modalities. Samples from different modalities are shown in
Fig. 1. The training set is formed by randomly selecting
four samples from each subject, overall 808 samples. The
remaining 509 samples are used for testing. The features
used here are those described in [14] which are further
PCA-transformed. The dimension of the input data after
preprocessing are 178 and 550 for the ﬁngerprint and iris
modalities, respectively. All inputs are normalized to have
zero mean and unit l2 norm. The number of dictionary
atoms for the dictionary learning algorithms are chosen to
be 2 per class, resulting in dictionaries of overall 404 atoms.
The dictionaries for JSRC and JDSRC are constructed using
all the training samples.

The classiﬁcation results obtained using individual
modalities on 5 different splits of the data into training and
test samples are shown in Table IX. As shown, ﬁnger 2 is
the strongest modality for the recognition task. The SRC
and SDL algorithms achieve the best results. It should be
noted that dictionary size of SRC is twice of that in SDL.
For multimodal classiﬁcation, we consider fusion of
ﬁngerprints, fusion of Irises, and fusion of all the modali-
ties. Table X summarizes the correct classiﬁcation rates of
several fusion algorithms using 4 ﬁngerprints, 2 Irises, and
all the modalities, obtained on 5 different training and test
splits. Fig. 7 shows the corresponding cumulative matched
score curves (CMC) for the competitive methods. CMC is
a performance measure, similar to ROC, which is origi-

12

Fig. 8: Comparison of the reconstructive-based (JSRC
and JSRC-UDL) and the proposed discriminative-based
(SMDL(cid:96)12 ) classiﬁcation algorithms obtained using the
joint sparsity prior for different numbers of dictionary
atoms per class on the WVU dataset.

utilizes the stochastic gradient algorithm, the learning rate
should be carefully chosen for convergence of the algo-
rithm. In out experiments, a heuristic was used to control
the learning rate. Topics of future research include develop-
ing of better optimization tools for fast convergence guaran-
tee in this non-convex setting. Moreover, developing task-
driven dictionary learning algorithms under other proposed
structured sparsity priors for multimodal fusion such as the
tree-structured sparsity prior [45], [68] is another future
research topic. Future research will also include adapting of
the proposed algorithms for other multimodal tasks such as
multimodal retrieval, multimodal action recognition using
Kinect data, and image super-resolution.

APPENDIX

The proof of Proposition 3.1 is presented using the

following two results.

(cid:104)

Lemma A.1 (Optimality condition): The matrix A(cid:63) =
α1(cid:63) . . . αS (cid:63)(cid:105)
∈ Rd×S is a min-
1→
imizer of (7) if and only if , ∀j ∈ {1, . . . , d},

T . . . a(cid:63)

T (cid:105)T

a(cid:63)

d→

=

(cid:104)






T (cid:16)

(cid:104)

d1
j

. . . dS
j

x1 − D1α1(cid:63)(cid:17)
a(cid:63)
j→
j→ = λ1
(cid:107)a(cid:63)
j→(cid:107)(cid:96)2
x1 − D1α1(cid:63)(cid:17)
j→(cid:107)(cid:96)2 ≤ λ1, otherwise.

. . . dS
j

, if (cid:107)a(cid:63)
T (cid:16)

− λ2a(cid:63)
T (cid:16)
(cid:104)
d1
(cid:107)
j
− λ2a(cid:63)

T (cid:16)

xS − DSαS (cid:63)(cid:17)(cid:105)

j→(cid:107)(cid:96)2 (cid:54)= 0,
xS − DSαS (cid:63)(cid:17)(cid:105)

(24)
Proof: . The proof follows directly from the subgradi-

ent optimality condition of (7), i.e.

0 ∈ {

D1α1(cid:63)

. . . DS T (cid:16)
− x1(cid:17)
D1T (cid:16)
(cid:104)
+ λ2A(cid:63) + λ1P : P ∈ ∂(cid:107)A(cid:63)(cid:107)(cid:96)12 },

DSαS (cid:63)

− xS(cid:17)(cid:105)

where ∂(cid:107)A(cid:63)(cid:107)(cid:96)12 denotes the subgradient of the (cid:96)12 norm
evaluated at A(cid:63). As shown in [69], the subgradient is
characterized, for all j ∈ {1, . . . , d}, as pj→ = aj→
if (cid:107)aj→(cid:107)(cid:96)2 > 0, and (cid:107)pj→(cid:107)(cid:96)2 ≤ 1 otherwise.

(cid:107)aj→(cid:107)(cid:96)2

Fig. 7: CMC plots obtained by fusing the Irises (top),
ﬁngerprints (middle), and all modalities (below) on the
WVU dataset.

resulting bi-level optimization problem is smooth and an
stochastic gradient descent algorithm was proposed to solve
the corresponding optimization problem. The algorithm
was then extended for a more general scenario where
the sparsity prior was the combination of the joint and
independent sparsity constraints. The simulation results on
the studied image classiﬁcation applications suggest that
while the unsupervised dictionaries can be used for feature
learning, the sparse coefﬁcients generated by the proposed
multimodal task-driven dictionary learning algorithms are
usually more discriminative and therefore can result
in
improved multimodal classiﬁcation performance. It was
also shown that, compared to the sparse-representation
classiﬁcation algorithms (JSRC, JDSRC, and JSRC-UDL),
the proposed algorithms can achieve signiﬁcantly better
performance when compact dictionaries are utilized.

In the proposed dictionary learning framework which

Before proceeding to the next proposition, we need to
deﬁne the term transition point. For a given {xs}, let Λλ
be the active set of the solution A(cid:63) of (7) when λ1 = λ.
Then λ is deﬁned to be a transition point of {xs} if Λλ+(cid:15) (cid:54)=
Λλ−(cid:15), ∀(cid:15) > 0.

Proposition A.1 (Regularity of A(cid:63)): Let λ2 > 0 and

assumption (A) be hold. Then,

Part 1. A(cid:63)({xs, Ds}) is a continuous function of {xs}

and {Ds}.

Part 2. If λ1 is not a transition point of {xs}, then
the active set Λ of A(cid:63)({xs, Ds}) is locally constant with
respect to both {xs} and {Ds}. Moreover, A(cid:63)({xs, Ds})
is locally differentiable with respect to {Ds}.

Part 3. ∀λ1 > 0, ∃ a set Nλ1 of measure zero in which
∀{xs} ∈ {Rns}\Nλ1 , λ1 is not any of the transition points
of {xs}.

Proof: . Part 1. In the special case of S = 1, which
is equivalent to an elastic net problem, this has already
been shown [19], [70]. Our proof follows similar steps.
Assumption (A) guarantees that A(cid:63) is bounded. Therefore,
we can restrict the optimization problem (7) to a compact
subset of Rd×S. Since A(cid:63) is unique (imposed by λ2 > 0)
and the cost function of (7) is continuous in A and each
element of the set {xs, Ds} is deﬁned over a compact set,
A(cid:63)({xs, Ds}) is a continuous function of {xs} and {Ds}.
Part 2 and Part 3. These statements are proved here by
converting the optimization problem (7) into an equivalent
group lasso problem [71] and using some recent results
j ) ∈
on it. Let
Rn×S, ∀j ∈ {1, . . . , d}, be the block-diagnoal collection
of the jth atoms of the dictionaries. Also let D(cid:48) =
∈ Rn, and
[D(cid:48)
a(cid:48) = [a1→ . . . ad→]T ∈ RSd . Then (7) can be rewritten as

d] ∈ Rn×Sd, x(cid:48) =

x1T . . . xS T (cid:105)T

j = blkdiag(d1

the matrix D(cid:48)

j , . . . , dS

1 . . . D(cid:48)

(cid:104)

min
A

1
2

(cid:107)x(cid:48) − D(cid:48)a(cid:48)(cid:107)2
(cid:96)2

+ λ1

(cid:107)aj→(cid:107)(cid:96)2 +

(cid:107)A(cid:107)2

F . (25)

d
(cid:88)

j=1

λ2
2

This can be further converted into the standard group lasso:

min
A

1
2

(cid:107)x(cid:48)(cid:48) − D(cid:48)(cid:48)a(cid:48)(cid:107)2
(cid:96)2

+ λ1

(cid:107)aj→(cid:107)(cid:96)2,

(26)

d
(cid:88)

j=1

(cid:104)

x(cid:48)T 0T (cid:105)T

(cid:105)T

where x(cid:48)(cid:48) =
D(cid:48)T √
(cid:104)
D(cid:48)(cid:48)
directly from the results in [72].

λ2I

∈ R(n+Sd)×Sd. It is clear that the matrix
is full column rank. The rest of the proof follows

∈ Rn+Sd and D(cid:48)(cid:48) =

Proof of Proposition 3.1: The above proposition
implies that A(cid:63) is differentiable almost everywhere. We
know prove the proposition 3.1. It is easy to show that f
is differentiable with respect to ws due to the assumption
(A) and the fact
lsu is twice differentiable. f is
also differentiable with respect to Ds given assumption
(A), twice differentiability of lsu, and the fact that A(cid:63)
is differentiable everywhere except on a set of measure
zero (Prop A.1). We obtain the derivative of f with respect
to Ds using the chain rule. The steps are similar to those

that

13

taken for (cid:96)1-related optimization in [36], though a bit more
involved. Since the active set is locally constant, using the
optimality condition (24), we can implicitly differentiate
A(cid:63)({xs, Ds}) with respect to Ds. For the non-active rows
of A(cid:63), the differential is zero. On the active set Λ, (24) can
be rewritten as

T (cid:16)

(cid:104)
D1
Λ

x1 − D1α1(cid:63)(cid:17)
(cid:34)

− λ2A(cid:63)

Λ→ = λ1

T (cid:16)

. . . DS
Λ

xS − DSαS (cid:63)(cid:17)(cid:105)
(cid:35)T

T

a(cid:63)
1→
(cid:107)a(cid:63)
1→(cid:107)(cid:96)2

. . .

T

a(cid:63)
N→
(cid:107)a(cid:63)
N→(cid:107)(cid:96)2

,

(27)

where N is the cardinality of Λ and DΛ and A(cid:63)
Λ→ are the
matrices consisting of active columns of D and active rows
of A(cid:63), respectively. For the rest of the proof, we only work
on the active set and the symbols Λ and (cid:63) are dropped for
the ease of notation. Taking the partial derivative from both
sides of (27) with respect to ds
ij, the element in the ith-row
and jth-column of Ds, and taking its transpose we have:





+

λ2

∂AT
∂ds
ij

0
(Dsαs − xs)T Es
ij + αsT Es
ij
0



T Ds

 +








∂α1T
∂ds
ij

∂αS T
∂ds
ij

D1T D1
...
DS T

DS








= −λ1

∆1

. . . ∆N

(cid:34)

1→

∂aT
∂ds
ij

(cid:35)

,

N→

∂aT
∂ds
ij

ij ∈ Rns×N is a matrix with zero elements except
where Es
the element in the ith row and jth column which is one
and

∆k =

1
(cid:107)ak→(cid:107)(cid:96)2

(cid:32)

I −

1
(cid:107)ak→(cid:107)2
(cid:96)2

(cid:33)

aT

k→ak→

∈ RS×S,

∀k ∈ {1, . . . , N }. It
Vectorizing the both sides and factorizing results in

is easy to check that ∆k ≥ 0.

vec

(cid:32)

(cid:33)

∂AT
∂ds
ij

=











P

0
(xs − Dsαs)T es
...
(xs − Dsαs)T es
ij N
0

ij 1

− αsT Es
ij

T ds
1

(28)

− αsT Es
ij

T ds
N











,

is

ij k

the kth
(cid:17)−1

column of Es

where es
=
(cid:16) ˆDT ˆD + λ1∆ + λ2I
, and ˆD and ∆ are deﬁned
in Eqs. (19) and (20), respectively. Further simplifying
Eq. (28) yields
(cid:33)
(cid:32)

ij, P

= P˜sEs
ij

T (xs − Dsαs) − P˜sds

T αs
j ,

i→

vec

∂AT
∂ds
ij

where ˜s is deﬁned in Eq. (21). Using the chain rule, we
have

(cid:34)

(cid:32)

(cid:33)(cid:35)

∂f
∂ds
ij

= E

gT vec

∂AT
∂ds
ij

,

14

where g = vec
respective to the active columns of dictionary Ds is

. Therefore, derivative with

(cid:16) ∂ (cid:80)S

s=1 lsu
∂AT

(cid:17)

∂f
∂Ds = E








. . .

gT P˜s

(cid:16)

gT P˜s

Es
11

(cid:16)

Es

ns1

gT P˜s
(cid:16)

Es

1N

T (xs − Dsαs) − ds
...
T (xs − Dsαs) − ds

(cid:17)

T αs
1

1→

(cid:17)

T αs
1


ns→
(cid:17)

T αs
N

1→

T (xs − Dsαs) − ds
...
T (xs − Dsαs) − ds
˜s gαsT (cid:105)

.

T αs
N

ns→






(cid:17)

(cid:16)

. . . gT P˜s

Es

nsN

(cid:104)

= E

(xs − Dsαs) gT P˜s − DsP T

Setting β = P T g ∈ RN S and noting that β˜s = P T
complete the proof.

˜s g

Derivation of the algorithm with the mixed (cid:96)12 − (cid:96)11
prior can be obtained similarly. For each active row j ∈ Λ
of A(cid:63), the solution of the optimization problem (23) with
the mixed prior, let Πj ⊆ S be the set of active modalities
which have non-zeros entries. Then the optimality condition
for the active row j is

(cid:104)
d1
j

T (cid:16)

x1 − D1α1(cid:63)(cid:17)

. . . dS
j

T (cid:16)

xS − DSαS (cid:63)(cid:17)(cid:105)

Πj

− λ2a(cid:63)

j→ = λ1

a(cid:63)
(cid:107)a(cid:63)

j→,Πj
j→(cid:107)(cid:96)2

+ λ(cid:48)

1 sign

(cid:16)

a(cid:63)

j→,Πj

(cid:17)

.

Then, the algorithm for the mixed prior can be obtained by
differentiating the optimality condition, following similar
steps as was shown for the (cid:96)12 prior.

REFERENCES

[1] D. L. Hall and J. Llinas,

“An introduction to multisensor data

fusion,” Proc. IEEE, vol. 85, no. 1, pp. 6–23, January 1997.
[2] P. K. Varshney, “Multisensor data fusion,” in Intell. Problem Solving.
Methodologies and Approaches. Springer Berlin Heidelberg, 2000.
[3] H. Wu, M. Siegel, R. Stiefelhagen, and J. Yang, “Sensor fusion
using Dempster-Shafer theory,” in Proc. 19th IEEE Instrum. and
Meas. Technol. Conf. (IMTC), 2002, pp. 7–12.

[4] A. Ross and R. Govindarajan, “Feature level fusion using hand and

face biometrics,” in SPIE proc. series, 2005, pp. 196–204.

[5] D. Ruta and B. Gabrys, “An overview of classiﬁer fusion methods,”

Comput. and Inf. syst., vol. 7, no. 1, pp. 1–10, 2000.

[6] A. Rattani, D. R. Kisku, M. Bicego, and M. Tistarelli, “Feature
level fusion of face and ﬁngerprint biometrics,” in Proc. 1st IEEE
Int. Conf. Biometrics: Theory, Applicat., and Syst., 2007, pp. 1–6.

[7] H. Zhang, N. M. Nasrabadi, Y. Zhang, and T. S. Huang, “Multi-
observation visual recognition via joint dynamic sparse representa-
tion,” in Proc. IEEE Conf. Comput. Vision (ICCV), 2011, pp. 595–
602.

[8] A. Klausne, A. Tengg, and B. Rinner, “Vehicle classiﬁcation on
multi-sensor smart cameras using feature- and decision-fusion,” in
Proc. 1st ACM/IEEE Int. Conf. Distributed Smart Cameras, 2007,
pp. 67–74.

[9] A. Rattani and M. Tistarelli, “Robust multi-modal and multi-unit
feature level fusion of face and iris biometrics,” in Advances in
Biometrics, pp. 960–969. Springer, 2009.

[10] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, “Robust
IEEE Trans. Pattern

face recognition via sparse representation,”
Anal. Mach. Intell, vol. 31, no. 2, pp. 210–227, Feb. 2009.

[11] X. Mei and H. Ling, “Robust visual tracking and vehicle classiﬁ-
cation via sparse representation,” IEEE Trans. Pattern Anal. Mach.
Intell, vol. 33, no. 11, pp. 2259–2272, Nov. 2011.

[12] H. Zhang, Y. Zhang, N. M. Nasrabadi, and T. S. Huang, “Joint-
structured-sparsity-based classiﬁcation for multiple-measurement
transient acoustic signals,” IEEE Trans. Syst., Man, Cybern., vol.
42, no. 6, pp. 1586–98, Dec. 2012.

[13] R. Caruana, Multitask learning, Springer, 1998.
[14] S. Shekhar, V. Patel, N. M. Nasrabadi, and R. Chellappa, “Joint
sparse representation for robust multimodal biometrics recognition,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 1, pp. 113–126,
Jan. 2013.

[15] U. Srinivas, H. Mousavi, C. Jeon, V. Monga, A. Hattel, and B. Ja-
yarao, “Simultaneous sparsity model for histopathological image
representation and classiﬁcation,” IEEE Trans. Med. Imag., vol. 33,
no. 5, pp. 1163 – 1179, May 2014.

[16] H. S. Mousavi, V. Srinivas, U.and Monga, Y. Suo, M. Dao, and T. D.
Tran, “Multi-task image classiﬁcation via collaborative, hierarchical
spike-and-slab priors,” in IEEE Intl. Conf. Image Processing (ICIP),
Oct 2014, pp. 4236–4240.

[17] N. H. Nguyen, N. M. Nasrabadi, and T. D. Tran, “Robust multi-
sensor classiﬁcation via joint sparse representation,” in Proc. 14th
Int. Conf. Information Fusion (FUSION), 2011.

[18] M. Yang, L. Zhang, D. Zhang, and S. Wang, “Relaxed collaborative
in Proc. IEEE Conf.
representation for pattern classiﬁcation,”
Comput. Vision and Pattern Recognition (CVPR), 2012, pp. 2224–
2231.

[19] J. Mairal, F. Bach, J. Ponce, and G. Sapiro,

“Online dictionary
learning for sparse coding,” Proc. 26th Annu. Int. Conf. Mach.
Learning (ICML), pp. 689–696, 2009.

[20] J. Mairal, F. Bach, A. Zisserman, and G. Sapiro,

“Supervised
in Advances Neural Inform. Process. Syst.

dictionary learning,”
(NIPS), 2008, pp. 1033–1040.

[21] J. Mairal, M. Elad, and G. Sapiro, “Sparse representation for color
image restoration,” IEEE Trans. Image Process., vol. 17, no. 1, pp.
53–69, Jan. 2008.

[22] M. Yang, L. Zhang, J. Yang, and D. Zhang, “Metaface learning for
sparse representation based face recognition,” in Proc. IEEE Conf.
Image Process. (ICIP), 2010, pp. 1601–1604.

[23] Y. L. Boureau, F. Bach, Y. LeCun, and J. Ponce, “Learning mid-
level features for recognition,” in Proc. IEEE Conf. Comput. Vision
and Pattern Recognition (CVPR), 2010, pp. 2559–2566.

[24] Z. Jiang, Z. Lin, and L. S. Davis, “Label consistent K-SVD: Learning
a discriminative dictionary for recognition,” IEEE Trans. Pattern
Anal. Mach. Intell, vol. 35, no. 11, pp. 2651–2664, Nov. 2013.
[25] M. Aharon, M. Elad, and A. Bruckstein, “K-SVD : An algorithm
for designing overcomplete dictionaries for sparse representation,”
IEEE Trans. Signal Process., vol. 54, no. 11, pp. 4311–4322, Nov.
2006.

[26] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online learning for
matrix factorization and sparse coding,” The J. of Mach. Learning
Research, vol. 11, pp. 19–60, 2010.

[27] K. Engan, S. O. Aase, and J. H. Husoy,

“Method of optimal
directions for frame design,” in Proc. IEEE Int. Conf. Acoustics,
Speech, and Signal Process., 1999, vol. 5, pp. 2443–2446.

[28] M. Elad and M. Aharon, “Image denoising via saprse and redundant
IEEE Trans. Image

representations over learned dictionaries,”
Process., vol. 15, no. 12, pp. 3736–3745, Dec. 2006.

[29] J. Mairal, F. Bach, and J. Ponce, “Task-driven dictionary learning,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 4, pp. 791–804,
Apr. 2012.

[30] J. Yang, Z. Wang, Z. Lin, X. Shu, and T. Huang, “Bilevel sparse
coding for coupled feature spaces,” in IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2012, pp. 2360–2367.
[31] S. Kong and D. Wang, “A brief summary of dictionary learning
based approach for classiﬁcation,” arxivId:1205.6544, 2012.
[32] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman, “Dis-
criminative learned dictionaries for local image analysis,” in Proc.
IEEE Conf. Comput. Vision and Pattern Recognition (CVPR), 2008,
pp. 1–8.

[33] Q. Zhang and B. Li, “Discriminative K-SVD for dictionary learning
in face recognition,” in Proc. IEEE Conf. Comput. Vision and Pattern
Recognition (CVPR), 2010, pp. 2691–2698.

[34] I. Ramirez, P. Sprechmann, and G. Sapiro,

“Classiﬁcation and
clustering via dictionary learning with structured incoherence and
shared features,” in Proc. IEEE Conf. Comput. Vision and Pattern
Recognition (CVPR), 2010, pp. 3501–3508.

[35] M. Yang, L. Zhang, X. Feng, and D. Zhang, “Fisher discrimination
dictionary learning for sparse representation,” in Proc. IEEE Int.
Conf. Comput. Vision (ICCV), 2011, pp. 543–550.

[36] J. Yang, K. Yu, and T. Huang,

“Supervised translation-invariant
sparse coding,” in Proc. IEEE Conf. Comput. Vision and Pattern
Recognition (CVPR), 2010, pp. 3517–3524.

[37] B. Colson, P. Marcotte, and G. Savard, “An overview of bilevel
optimization,” Ann. Operations Research, vol. 153, no. 1, pp. 235–
256, Apr. 2007.

[38] D. M. Bradley and J. A. Bagnell, “Differentiable sparse coding,” in

Advances Neural Inform. Process. Syst. (NIPS), 2008.

[39] J. Zheng and Z. Jiang, “Learning view-invariant sparse representa-
tions for cross-view action recognition,” in Proc. IEEE Intel. Conf.
Computer Vision (ICCV), 2013, pp. 3176–3183.

[40] G. Monaci, P. Jost, P. Vandergheynst, B. Mailh´e, S. Lesage, and
IEEE Trans.

R. Gribonval,
Image Process, vol. 16, no. 9, pp. 2272–2283, Sep. 2007.

“Learning multimodal dictionaries,”

[41] Y. Zhuang, Y. Wang, F. Wu, Y. Zhang, and W. Lu, “Supervised
coupled dictionary learning with group structures for multi-modal
retrieval,” in Proc. 27th Conf. Artiﬁcial Intell., 2013, pp. 1070–1076.
[42] H. Zhang, Y. Zhang, and T. S. Huang, “Simultaneous discriminative
projection and dictionary learning for sparse representation based
classiﬁcation,” Pattern Recognition, vol. 46, no. 1, pp. 346–354,
Jan. 2013.

[43] B. A. Olshausen and D. J. Field, “Sparse coding with an overcom-
plete basis set: A strategy employed by v1?,” Vision Research, vol.
37, no. 23, pp. 3311–3325, Dec. 1997.

[44] L. Bottou and O. Bousquet, “The trade-offs of large scale learning,”
in Advances Neural Inform. Process. Syst. (NIPS), 2007, pp. 161–
168.

[45] S. Bahrampour, A. Ray, N. M. Nasrabadi, and W. K. Jenkins,
“Quality-based multimodal classiﬁcation using tree-structured spar-
sity,” in Proc. IEEE Conf. Comput. Vision and Pattern Recognition
(CVPR), 2014, pp. 4114–4121.

[46] S. F. Cotter, B. D. Rao, K. Engan, and K. Kreutz-Delgado, “Sparse
solutions to linear inverse problems with multiple measurement
vectors,” IEEE Trans. Signal Process., vol. 53, no. 7, pp. 2477–
2488, Jul. 2005.

[47] J. A. Tropp, “Algorithms for simultaneous sparse approximation. part
ii: Convex relaxation,” Signal Process., vol. 86, no. 3, pp. 589–602,
Mar. 2006.

[48] A. Rakotomamonjy, “Surveying and comparing simultaneous sparse
approximation (or group-lasso) algorithms,” Signal Process., vol.
91, no. 7, pp. 1505–1526, Jul. 2011.

[49] N. Parikh and S. Boyd, “Proximal algorithms,” Foundations and

Trends in Optimization, pp. 1–96, 2013.

[50] H. Zou and T. Hastie, “Regularization and variable selection via the
elastic net,” J. Royal Statistical Soc. Series B, vol. 67, no. 2, pp.
301–320, 2005.

[51] M. Aharon and M. Elad, “Sparse and redundant modeling of image
content using an image-signature-dictionary,” SIAM J.l on Imaging
Sciences, vol. 1, no. 3, pp. 228–247, 2008.

“Large-scale machine learning with stochastic gradi-
in Proceedings of COMPSTAT’2010, pp. 177–186.

[52] L. Bottou,

ent descent,”
Springer, 2010.

[53] L. Bottou, “Online learning and stochastic approximations,” On-line

learning in neural networks, vol. 17, pp. 9.

[54] P. Sprechmann, I. Ramirez, G. Sapiro, and Y. C. Eldar, “C-hilasso: A
collaborative hierarchical sparse modeling framework,” IEEE Trans.
Signal Process., vol. 59, no. 9, pp. 4183–4198, Sep. 2011.

[55] A. M. Martinez and R. Benavente, “The AR face database,” CVC

Technical Rep., vol. 24, 1998.

[56] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, “Multi-
pie,” Image and Vision Computing, vol. 28, no. 5, pp. 807–813,
2010.

[57] D. Weinland, E. Boyer, and R. Ronfard, “Action recognition from
in IEEE 11th Intel. Conf.

arbitrary views using 3d exemplars,”
Computer Vision (ICCV), 2007, pp. 1–7.

[58] S. S. S. Crihalmeanu, A. Ross, and L. Hornak, “A protocol for
multibiometric data acquisition, storage and dissemination,” Tech.
Rep., Lane Dept. of Comput. Sci. and Elect. Eng., West Virginia
Univ., 2007.

[59] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet, “Sim-
plemkl.,” J. of Mach. Learning Research, vol. 9, no. 11, 2008.
[60] C. M. Bishop, Pattern Recognition and Machine Learning, Springer,

2006.

[61] H. Zhang, N. M. Nasrabadi, Y. Zhang, and T. S. Huang, “Joint
dynamic sparse representation for multi-view face recognition,”
Pattern Recognition, vol. 45, pp. 1290–1298, 2012.

15

[62] D. Tran and A. Sorokin, “Human activity recognition with metric
learning,” in Computer Vision–ECCV 2008, pp. 548–561. Springer,
2008.

[63] A. Wang, H.and Kl¨aser, C. Schmid, and C.-L. Liu, “Dense trajec-
tories and motion boundary descriptors for action recognition,” Intl.
J. Computer Vision, vol. 103, no. 1, pp. 60–79, 2013.

[64] A. Gupta, A. Shafaei, J. J. Little, and R. J. Woodham, “Unlabelled 3d
motion examples improve cross-view action recognition,” in Proc.
British Machine Vision Conf., 2014.

[65] I. N. Junejo, E. Dexter, and P. Laptev, I.and Perez,

“View-
independent action recognition from temporal self-similarities,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 1, pp. 172–
185, 2011.

[66] X. Wu, D. Xu, L. Duan, and J. Luo, “Action recognition using
context and appearance distribution features,” in IEEE Conf. Comput.
Vision Pattern Recognition (CVPR), 2011, pp. 489–496.

[67] R. M. Bolle, J. H. Connell, S. Pankanti, N. K. Ratha, and A. W.
Senior, “The relation between the roc curve and the cmc,” in Proc.
4th IEEE Workshop Automat. Identiﬁcation Advanced Technologies,
2005, pp. 15–20.

[68] Julien Mairal, Rodolphe Jenatton, Guillaume Obozinski, and Francis
“Learning hierarchical and topographic dictionaries with

Bach,
structured sparsity,” in SPIE, 2011.

[69] E. V. D. Berg and M. P. Friedlander, “Theoretical and empirical
results for recovery from multiple measurements,” IEEE Trans. Inf.
Theory, vol. 56, no. 5, pp. 2516–2527, Apr. 2010.

[70] H. Zou, T. Hastie, and R. Tibshirani, “On the degrees of freedom
of the lasso,” The Ann. of Statistics, vol. 35, no. 5, pp. 2173–2192,
2007.

[71] M. Yuan and Y. Lin, “Model selection and estimation in regression
with grouped variables,” J. Royal Statistical Soc.: Series B (Statis-
tical Methodology), vol. 68, no. 1, pp. 49–67, 2006.

[72] S. Vaiter, C. Deledalle, G. Peyre, J. Fadili, and C. Dossal, “Degrees

of freedom of the group lasso,” arXiv:1205.1481, 2012.

Soheil Bahrampour received the M.Sc. degree
in electrical engineering from the University of
Tehran, Iran,
in 2009. He then received the
M.Sc. degree in Mechanical Engineering and
PhD degree in Electrical Engineering under the
supervision of A. Ray and W.K. Jenkins from
The Pennsylvania State University, University
Park, PA, in 2013 and 2015, respectively. Dr.
Bahrampour is currently a Research Scientist
with Bosch Research and Technology Center,
Palo Alto, CA. His research interests include

machine learning, data mining, signal processing, and computer vision.

16

Nasser M. Nasrabadi
(S’80-M’84-SM’92-
FM’01) received the B.Sc. (Eng.) and Ph.D.
degrees in Electrical Engineering from Imperial
College of Science and Technology (University
of London), London, England, in 1980 and 1984,
respectively. From October 1984 to December
1984 he worked for IBM (UK) as a senior
programmer. During 1985 to 1986 he worked
with Philips research laboratory in NY as a
member of technical staff. From 1986 to 1991 he
was an assistant professor in the Department of
Electrical Engineering at Worcester Polytechnic Institute, Worcester, MA.
From 1991 to 1996 he was an associate professor with the Department
of Electrical and Computer Engineering at State University of New York
at Buffalo, Buffalo, NY. Since September From 1996 to 2015 he was a
Senior Research Scientist (ST) with the US Army Research Laboratory
(ARL). Since August 2015 he has been a professor at Lane Dept. of
Computer Science and Elecrical Engineering. Dr. Nasrabadi has served
as an associate editor for the IEEE Transactions on Image Processing,
the IEEE Transactions on Circuits, Systems and Video Technology, and
the IEEE Transactions on Neural Networks. His current research interests
are in image processing, computer vision, biometrics, statistical machine
learning theory, sparsity, robotics, and neural networks applications to
image processing. He is also a Fellow of ARL, SPIE and IEEE.

Asok Ray (SM’83F’02) received the Ph.D. de-
gree in Mechanical Engineering from Northeast-
ern University, Boston, MA, and the graduate de-
grees in the disciplines of Electrical Engineering,
Mathematics, and Computer Science. He joined
The Pennsylvania State University (Penn State),
University Park, PA, in July 1985, and is cur-
rently a Distinguished Professor of Mechanical
Engineering and Mathematics, a Graduate Fac-
ulty of Electrical Engineering, and a Graduate
Faculty of Nuclear Engineering. Prior to joining
Penn State, he held research and academic positions with Massachusetts
Institute of Technology, Cambridge, MA, and Carnegie-Mellon University,
Pittsburgh, PA, as well as management and research positions with GTE
Strategic Systems Division, Westborough, MA, Charles Stark Draper
Laboratory, Cambridge, MA, and MITRE Corporation, Bedford, MA. Dr.
Ray has authored or coauthored over 550 research publications, including
over 285 scholarly articles in refereed journals and research monographs.
Dr. Ray is also a Fellow of the American Society of Mechanical Engineers
(ASME) and a Fellow of World Innovative Foundation (WIF). Dr. Ray had
been a Senior Research Fellow at NASA Glenn Research Center under a
National Academy of Sciences award.

W. Kenneth Jenkins received the B.S.E.E. de-
gree from Lehigh University and the M.S.E.E.
and Ph.D. degrees from Purdue University. From
1974 to 1977 he was a Research Scientist Asso-
ciate in the Communication Sciences Laboratory
at the Lockheed Research Laboratory, Palo Alto,
CA. In 1977 he joined the University of Illinois
at Urbana-Champaign where he was a faculty
member in Electrical and Computer Engineering
from 1977 until 1999. From 1986-1999 Dr. Jenk-
ins was the Director of the Coordinated Science
Laboratory. From 1999 through 2011 he served Professor and Head of
Electrical Engineering at Penn State University, and in 2011 he returned
to the rank of Professor of Electrical Engineering. Dr. Jenkins current
research interests include fault
tolerant DSP for highly scaled VLSI
systems, adaptive signal processing, multidimensional array processing,
computer imaging, bio-inspired optimization algorithms for intelligent
signal processing, and fault tolerant digital signal processing. He co-
authored the book Advanced Concepts in Adaptive Signal Processing,
published by Kluwer in 1996. He is a past Associate Editor for the
IEEE Transaction on Circuits and Systems, and a past President (1985) of
the CAS Society. He served as General Chairman of the 1988 Midwest
Symposium on Circuits and Systems and as the General Chairman of
the Thirty Second Annual Asilomar Conference on Signals and Systems.
From 2002 to 2007 he served on the Board of Directors of the Electrical
and Computer Engineering Department Heads Association (ECEDHA)
and as President of ECEDHA in 2005. Since January 2011 he has been
serving as a Member of the IEEE-HKN Board of Governors. Dr. Jenkins
is a Life Fellow of the IEEE and a recipient of the 1990 Distinguished
Service Award of the IEEE Circuits and Systems Society. In 2000 he
received a Golden Jubilee Medal from the IEEE Circuits and Systems
Society and a 2000 Millennium Award from the IEEE. In 2000 was named
a co-winner of the 2000 International Award of theGeorge Monteﬁore
Foundation (Belgium) for outstanding career contributions to the ﬁeld of
electrical engineering and electrical science, in 2002 he was awarded the
Shaler Area High School Distinguished Alumnus Award, in 2007 he was
honored with an IEEE Midwest Symposium on Circuits and Systems 50th
Anniversary Award, and in 2013 he received the ECEDHA Robert M.
Janowiak Outstanding Leadership and Service Award.

Multimodal Task-Driven Dictionary Learning
for Image Classiﬁcation

Soheil Bahrampour, Member, IEEE, Nasser M. Nasrabadi, Fellow, IEEE, Asok Ray, Fellow, IEEE,
and W. Kenneth Jenkins, Life Fellow, IEEE

1

5
1
0
2
 
t
c
O
 
7
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
9
0
1
0
.
2
0
5
1
:
v
i
X
r
a

Abstract—Dictionary learning algorithms have been suc-
cessfully used for both reconstructive and discriminative tasks,
where an input signal is represented with a sparse linear
combination of dictionary atoms. While these methods are
mostly developed for single-modality scenarios, recent studies
have demonstrated the advantages of feature-level fusion
based on the joint sparse representation of the multimodal
inputs. In this paper, we propose a multimodal task-driven
dictionary learning algorithm under the joint sparsity con-
straint (prior) to enforce collaborations among multiple ho-
mogeneous/heterogeneous sources of information. In this task-
driven formulation, the multimodal dictionaries are learned
simultaneously with their corresponding classiﬁers. The re-
sulting multimodal dictionaries can generate discriminative
latent features (sparse codes) from the data that are optimized
for a given task such as binary or multiclass classiﬁca-
tion. Moreover, we present an extension of the proposed
formulation using a mixed joint and independent sparsity
prior which facilitates more ﬂexible fusion of the modalities
at feature level. The efﬁcacy of the proposed algorithms
for multimodal classiﬁcation is illustrated on four different
applications – multimodal face recognition, multi-view face
recognition, multi-view action recognition, and multimodal
biometric recognition. It is also shown that, compared to
the counterpart reconstructive-based dictionary learning algo-
rithms, the task-driven formulations are more computationally
efﬁcient in the sense that they can be equipped with more
compact dictionaries and still achieve superior performance.

Index Terms—Dictionary learning, Multimodal classiﬁca-

tion, Sparse representation, Feature fusion

I. INTRODUCTION
It is well established that information fusion using multi-
ple sensors can generally result in an improved recognition
performance [1]. It provides a framework to combine
local information from different perspectives which is more
tolerant to the errors of individual sources [2], [3]. Fusion
methods for classiﬁcation are generally categorized into
feature fusion [4] and classiﬁer fusion [5] algorithms.
Feature fusion methods aggregate extracted features from
different sources into a single feature set which is then used
for classiﬁcation. On the other hand, classiﬁer fusions algo-
rithms combine decisions from individual classiﬁers, each

When this work was achieved, S. Bahrampour, A. Ray, and W. K.
Jenkins were with the Department of Electrical Engineering, Pennsylvania
State University, University Park, PA 16802, USA; N. M. Nasrabadi was
with the Army Research Laboratory, Adelphi, MD 20783. S. Bahrampour
is now with Bosch Research and Technology Center, Palo Alto, CA; N. M.
Nasrabadi is now with the Computer Science and Electrical Engineering
Department at the West Virginia University, WV.

soheil.bahrampour@us.bosch.com, nassernasrabadi@mail.wvu.edu,

{axr2, wkj1}@psu.edu.

of which is trained using separate sources. While classiﬁer
fusion is a well-studied topic, fewer studies have been done
for feature fusion, mainly due to the incompatibility of the
feature sets [6]. A naive way of feature fusion is to stack
the features into a longer one [7]. However this approach
usually suffers from the curse of dimensionality due to the
limited number of training samples [4]. Even in scenarios
with abundant training samples, concatenation of feature
vectors does not take into account the relationship among
the different sources and it may contain noisy or redundant
data, which degrade the performance of the classiﬁer [6].
However, if these limitations are mitigated, feature fusion
can potentially result
in improved classiﬁcation perfor-
mance [8], [9].

Sparse representation classiﬁcation has recently attracted
the interest of many researchers in which the input sig-
nal is approximated with a linear combination of a few
dictionary atoms [10] and has been successfully applied
to several problems such as robust face recognition [10],
visual tracking [11], and transient acoustic signal classi-
ﬁcation [12]. In this approach, a structured dictionary is
usually constructed by stacking all the training samples
from the different classes. The method has also been
expanded for efﬁcient feature-level fusion which is usually
referred to as multi-task learning [13], [14], [15], [16].
Among different proposed sparsity constraints (priors), joint
sparse representation has shown signiﬁcant performance
improvement
in several multi-task learning applications
such as target classiﬁcation, biometric recognitions, and
multiview face recognition [12], [14], [17], [18]. The un-
derlying assumption is that the multimodal test input can
be simultaneously represented by a few dictionary atoms,
or training samples, from a multimodal dictionary, that
represents all the modalities and, therefore, the resulting
sparse coefﬁcients should have the same sparsity pattern.
However, the dictionary constructed by the collection of
the training samples suffer from two limitations. First, as
the resulting
the number of training samples increases,
optimization problem becomes more computationally de-
manding. Second, the dictionary that is constructed this way
is not optimal neither for the reconstructive tasks [19] nor
the discriminative tasks [20].

Recently it has been shown that learning the dictionary
can overcome the above limitations and signiﬁcantly im-
prove the performance in several applications including
image restoration [21], face recognition [22] and object
recognition [23], [24]. The learned dictionaries are usu-

2

ally more compact and have fewer dictionary atoms than
the number of training samples [25], [26]. Dictionary
learning algorithms can generally be categorized into two
groups: unsupervised and supervised. Unsupervised dictio-
nary learning algorithms such as the method of optimal
direction [27] and K-SVD [25] are aimed at ﬁnding a
dictionary that yields minimum errors when adapted to
reconstruction tasks such as signal denoising [28] and im-
age inpainting [19]. Although, the unsupervised dictionary
learning has also been used for classiﬁcation [22], it has
been shown that better performance can be achieved by
learning the dictionaries that are adapted to an speciﬁc
task rather than just the data set [29], [30]. These methods
are called supervised, or task-driven, dictionary learning
algorithms. For the classiﬁcation task, for example, it is
more meaningful to utilize the labeled data to minimize
the misclassiﬁcation error rather than the reconstruction
error [31]. Adding a discriminative term to the recon-
struction error and minimizing a trade-off between them
has been proposed in several formulations [20], [24],
[32], [33]. The incoherent dictionary learning algorithm
proposed in [34] is another supervised formulation which
trains class-speciﬁc dictionaries to minimize atom sharing
between different classes and uses sparse representation
for classiﬁcation. In [35], a Fisher criterion is proposed to
learn structured dictionaries such that the sparse coefﬁcients
have small within-class and large between-class scatters.
While unsupervised dictionary learning can be reformulated
as a large scale matrix factorization problem and solved
efﬁciently [19], supervised dictionary learning is usually
more difﬁcult to optimize. More recently, it has been shown
that better optimization tool can be used to tackle the
supervised dictionary learning [30], [36]. This is achieved
by formulating it as a bilevel optimization problem [37],
[38]. In particular, a stochastic gradient descent algorithm
has been proposed in [29] which efﬁciently solves the
dictionary learning problem in a uniﬁed framework for
different tasks, such as classiﬁcation, nonlinear image map-
ping, and compressive sensing.

The majority of the existing dictionary learning algo-
rithms, including the task-driven dictionary learning [29],
are only applicable to single source of data. In [39], a set
of view-speciﬁc dictionaries and a common dictionary are
learned for the application of multi-view action recogni-
tion. The view-speciﬁc dictionaries are trained to exploit
view-level correspondence while the common dictionary is
trained to capture common patterns shared among the dif-
ferent views. The proposed formulation belongs to the class
of dictionary learning algorithms that leverages the labeled
samples to learn class-speciﬁc atoms while minimizing
the reconstruction error. Moreover, it cannot be used for
fusion of the heterogeneous modalities. In [40], a generative
multimodal dictionary learning algorithm is proposed to
extract typical templates of multimodal features. The tem-
plates represent synchronous transient structures between
modalities which can be used for localization applications.
More recently, a multimodal dictionary learning algorithm
with joint sparsity prior is proposed in [41] for multimodal

Fig. 1: Multimodal task-driven dictionary learning scheme.

retrieval where the task is to ﬁnd relevant samples from
other modalities for a given unimodal query. However,
the proposed formulation cannot be readily applied for
information fusion in which the task is to ﬁnd label of a
given multimodal query. Moreover, the joint sparsity prior
is used in [41] to couple similarly labeled samples within
each modality and is not utilized to extract cross-modality
information which is essential for information fusion [12].
Furthermore,
the dictionaries in [41] are learned to be
generative by minimizing the reconstruction error of data
across modalities and, therefore, are not necessary optimal
for discriminative tasks [31].

This paper focuses on learning discriminative multimodal
dictionaries. The major contributions of the paper are as
follows:

• Formulation of

the multimodal dictionary learning
algorithms: A multimodal task-driven dictionary learn-
ing algorithm is proposed for classiﬁcation using ho-
mogeneous or heterogeneous sources of information.
Information from different modalities are fused both
at the feature level, by using the joint sparse repre-
sentation, and at the decision level, by combining the
scores of the modal-based classiﬁers. The proposed
formulation simultaneously trains the multimodal dic-
tionaries and classiﬁers under the joint sparsity prior in
order to enforce collaborations among the modalities
and obtain the latent sparse codes as the optimized
features for different tasks such as binary and mul-
ticlass classiﬁcation. Fig. 1 presents an overview of
the proposed framework. An unsupervised multimodal
dictionary learning algorithm is also presented as a by-
product of the supervised version.

• Differentiability of the bi-level optimization problem:
The main difﬁculty in proposing such a formulation
is that the solution of the corresponding joint sparse
coding problem is not differentiable with respect to
the dictionaries. While the joint sparse coding has
a non-smooth cost function,
is shown here that
it is locally differentiable and the resulting bi-level
optimization for task-driven multimodal dictionary
learning is smooth and can be solved using a stochastic

it

gradient descent algorithm. 1

• Flexible feature-level

fusion: An extension of the
proposed framework is presented which facilitates
more ﬂexible fusion of the modalities at the feature
level by allowing the modalities to have different
sparsity patterns. This extension provides a frame-
work to tune the trade-off between independent sparse
representation and joint sparse representation among
the modalities. Improved performance for multimodal
classiﬁcation: The proposed methods achieve the state-
of-the-art performance in a range of different multi-
modal classiﬁcation tasks.
In particular, we have
provided extensive performance comparison between
the proposed algorithms and some of the competing
tasks of
methods from literature for four different
multimodal face recognition, multi-view face recog-
nition, multimodal biometric recognition, and multi-
view action recognition. The experimental results on
these datasets have demonstrated the usefulness of
the proposed formulation, showing that the proposed
algorithm can be readily applied to several different
application domains.

• Improved efﬁciency for sparse-representation based
classiﬁcation: It is shown here that, compared to the
counterpart sparse representation classiﬁcation algo-
rithms, the proposed algorithms are more computa-
tionally efﬁcient in the sense that they can be equipped
with more compact dictionaries and still achieve su-
perior performance.

A. Paper organization

The rest of the paper is organized as follows. In Sec-
tion II, unsupervised and supervised dictionary learning
algorithms for single source of information are reviewed.
Joint sparse representation for multimodal classiﬁcation is
also reviewed in this section. Section III proposes the task-
driven multimodal dictionary learning algorithms. Compar-
ative studies on several benchmarks and concluding results
are presented in Section IV and Section V, respectively.

B. Notation

Vectors are denoted by bold lower case letters and
matrices by bold upper case letters. For a given vector x,
xi is its ith element. For a given ﬁnite set of indices γ,
xγ is the vector formed with those elements of x indexed
in γ. Symbol → is used to distinguish the row vectors
from column vectors, i.e. for a given matrix X, the ith row
and jth column of matrix are represented as xi→ and xj,
respectively. For a given ﬁnite set of indices γ, Xγ is the
matrix formed with those columns of X indexed in γ and
Xγ→ is the matrix formed with those rows of X indexed
in γ. Similarly, for given ﬁnite sets of indices γ and ψ,
Xγ→,ψ is the matrix formed with those rows and columns
of X indexed in γ and ψ, respectively. xij is the element

1The source code of the proposed algorithm is released here: https:

//github.com/soheilb/multimodal dictionary learning

3

of X at row i and column j. The lq norm, q ≥ 1, of a
vector x ∈ Rm is deﬁned as (cid:107)x(cid:107)(cid:96)q = ((cid:80)m
j=1 |xj|q)1/q.
The Frobenius norm and (cid:96)1q norm, q ≥ 1, of matrix
(cid:17)1/2
X ∈ Rm×n is deﬁned as (cid:107)X(cid:107)F =
and (cid:107)X(cid:107)(cid:96)1q = (cid:80)m
{xi|i ∈ γ} is shortly denoted as {xi}.

i=1 (cid:107)xi→(cid:107)(cid:96)q , respectively. The collection

(cid:16)(cid:80)m
i=1

j=1 x2
ij

(cid:80)n

II. BACKGROUND

A. Dictionary learning

Dictionary learning has been widely used in various tasks
such as reconstruction, classiﬁcation, and compressive sens-
ing [29], [33], [42], [43]. In contrast to principal component
analysis (PCA) and its variants, dictionary learning algo-
rithms generally do not impose orthogonality condition and
are more ﬂexible allowing to be well-tuned to the training
data. Let X = [x1, x2, . . . , xN ] ∈ Rn×N be the collection
of N (normalized) training samples that are assumed to be
statistically independent. Dictionary D ∈ Rn×d can then
be obtained as the minimizer of the following empirical
cost [22]:

gN (D) (cid:44) 1
N

N
(cid:88)

i=1

lu (xi, D)

(1)

the

regularizing convex set D (cid:44) {D ∈
over
Rn×d|(cid:107)dk(cid:107)(cid:96)2 ≤ 1, ∀k = 1, . . . , d}, where dk is the kth
column, or atom, in the dictionary and the unsupervised
loss lu is deﬁned as
lu (x, D) (cid:44) min
α∈Rd

+λ1(cid:107)α(cid:107)(cid:96)1 +λ2(cid:107)α(cid:107)2
(cid:96)2

(cid:107)x−Dα(cid:107)2
(cid:96)2

, (2)

which is the optimal value of the sparse coding problem
with λ1 and λ2 being the regularizing parameters. While
λ2 is usually set to zero to exploit sparsity, using λ2 > 0
makes the optimization problem in Eq. (2) strongly convex
resulting in a differentiable cost function [29]. The index u
of lu is used to emphasize that the above dictionary learning
formulation is an unsupervised method. It is well-known
that one is often interested in minimizing an expected
risk, rather than the perfect minimization of the empirical
cost [44]. An efﬁcient online algorithm is proposed in [19]
to ﬁnd the dictionary D as the minimizer of the following
stochastic cost over the convex set D:

g (D) (cid:44) Ex [lu (x, D)] ,

(3)

where it is assumed that the data x is drawn from a ﬁnite
probability distribution p(x) which is usually unknown
and Ex [.] is the expectation operator with respect to the
distribution p(x).

The trained dictionary can then be used to (sparsely)
reconstruct the input. The reconstruction error has been
shown to be a robust measure for classiﬁcation tasks [10],
[45]. Another use of a given trained dictionary is for feature
extraction where the sparse code α(cid:63)(x, D), obtained as a
solution of (2), is used as a feature vector representing the
input signal x in the classical expected risk optimization
for training a classiﬁer [29]:

min
w∈W

Ey,x [l (y, w, α(cid:63)(x, D))] +

ν
2

(cid:107)w(cid:107)2
(cid:96)2

,

(4)

4

where y is the ground truth class label associated with
the input x, w is model (classiﬁer) parameters, ν is a
regularizing parameter, and l is a convex loss function that
measures how well one can predict y given the feature
vector α(cid:63) and classiﬁer parameters w. The expectation
Ey,x is taken with respect to the probability distribution
p(y, x) of the labeled data. Note that in Eq. 4, the dictionary
D is ﬁxed and independent of the given task and class label
y. In task-driven dictionary learning, on the other hand,
a supervised formulation is used which ﬁnds the optimal
dictionary and classiﬁer parameters jointly by solving the
following optimization problem [29]:

min
D∈D,w∈W

Ey,x [lsu (y, w, α(cid:63)(x, D))] +

(cid:107)w(cid:107)2
(cid:96)2

.

(5)

ν
2

The index su of convex loss function lsu is used to
emphasize that the above dictionary learning formulation
is supervised. The learned task-driven dictionary has been
shown to result in a superior performance compared to the
unsupervised setting [29]. In this setting, the sparse codes
are indeed the optimized latent features for the classiﬁer.

B. Multimodal joint sparse representation

Joint sparse representation provides an efﬁcient tool for
feature-level fusion of sources of information [12], [14],
[46]. Let S (cid:44) {1, . . . , S} be a ﬁnite set of available
modalities and let xs ∈ Rns
, s ∈ S, be the feature
vector for the sth modality. Also let Ds ∈ Rns×d be
the corresponding dictionary for the sth modality. For
now, it is assumed that the multimodal dictionaries are
constructed by collections of the training samples from
i.e. jth atom of dictionary Ds is
different modalities,
the jth training sample from the sth modality. Given a
multimodal input {xs|s ∈ S}, shortly denoted as {xs}, an
optimal sparse matrix A(cid:63) ∈ Rd×S is obtained by solving
the following (cid:96)12-regularized reconstruction problem:

argmin
A=[α1...αS ]

1
2

S
(cid:88)

s=1

(cid:107)xs − Dsαs(cid:107)2
(cid:96)2

+ λ(cid:107)A(cid:107)(cid:96)12 ,

(6)

where λ is a regularization parameter. Here αs is the sth-
column of A which corresponds to the sparse representa-
tion for the sth modality. Different algorithms have been
proposed to solve the above optimization problem [47],
[48]. We use the efﬁcient alternating direction method
of multipliers (ADMM) [49] to ﬁnd A(cid:63). The (cid:96)12 prior
encourages row sparsity in A(cid:63), i.e. it encourages collab-
oration among all the modalities by enforcing the same
dictionary atoms from different modalities that present the
same event, to be used for reconstructing the inputs {xs}.
An (cid:96)11 term can also be added to the above cost function
to extend it to a more general framework where sparsity
can also be sought within the rows, as will be discussed
in Section III-D. It has been shown that
joint sparse
representation can result in a superior performance in fusing
multimodal sources of information compared to other infor-
mation fusion techniques [45]. We are interested in learning
multimodal dictionaries under the joint sparsity prior. This

has several advantages over a ﬁxed dictionary consisting of
training data. Most importantly, it can potentially remove
the redundant and noisy information by representing the
training data in a more compact form. Also using the
supervised formulation, one expects to ﬁnd dictionaries that
are well-adapted to the discriminative tasks.

III. MULTIMODAL DICTIONARY LEARNING

In this section, online algorithms for unsupervised and

supervised multimodal dictionary learning are proposed.

A. Multimodal unsupervised dictionary learning

Unsupervised multimodal dictionary learning is derived
by extending the optimization problem characterized in
Eq. (3) and using the joint sparse representation of (6) to
enforce collaborations among modalities. Let the minimum
u ({xs, Ds}) of the joint sparse coding be deﬁned as
cost l(cid:48)

min
A

1
2

S
(cid:88)

s=1

(cid:107)xs − Dsαs(cid:107)2
(cid:96)2

+ λ1(cid:107)A(cid:107)(cid:96)12 +

(cid:107)A(cid:107)2

F , (7)

λ2
2

where λ1 and λ2 are the regularizing parameters. The addi-
tional Frobenius norm (cid:107).(cid:107)F compared to Eq. (6) guarantees
a unique solution for the joint sparse optimization problem.
In the special case when S = 1, optimization (7) reduces
to the well-studied elastic-net optimization [50]. By natural
extension of the optimization problem (3), the unsupervised
multimodal dictionaries are obtained by:

Ds(cid:63) = argmin
Ds∈Ds

Exs [l(cid:48)

u ({xs, Ds})] , ∀s ∈ S,

(8)

where the convex set Ds is deﬁned as

Ds (cid:44) {D ∈ Rns×d|(cid:107)dk(cid:107)(cid:96)2 ≤ 1, ∀k = 1, . . . , d}.

(9)

It is assumed that data xs is drawn from a ﬁnite (un-
known) probability distribution p(xs). The above optimiza-
tion problem can be solved using the classical projected
stochastic gradient algorithm [51] which consists of a
sequence of updates as follows:

Ds ← ΠDs [Ds − ρt∇Ds l(cid:48)

u ({xs

t , Ds})] ,

(10)

where ρt is the gradient step at time t and ΠD is the
orthogonal projector onto set D. The algorithm converges
to a stationary point for a decreasing sequence of ρt [51],
[52]. A typical choice of ρt is shown in the next section.
This problem can also be solved using online matrix
factorization algorithm [26]. It should be noted that the
while the stochastic gradient descent does converge, it is
not guaranteed to converge to a global minimum due to
the non-convexity of the optimization problem [26], [44].
However, such stationary point is empirically found to be
sufﬁciently good for practical applications [21], [28].

B. Multimodal task-driven dictionary learning

As discussed in Section II, the unsupervised setting does
not take into account the label of the training data, and
the dictionaries are obtained by minimizing the reconstruc-
tion error. However, for classiﬁcation tasks, the minimum
reconstruction error does not necessarily result in discrimi-
native dictionaries. In this section, a multimodal task-driven
dictionary learning algorithm is proposed that enforces
collaboration among the modalities both at the feature level
using joint sparse representation and the decision level
using a sum of the decision scores. We propose to learn
the dictionaries Ds(cid:63), ∀s ∈ S, and the classiﬁer parameters
ws(cid:63), ∀s ∈ S, shortly denoted as the set {Ds(cid:63), ws(cid:63)}, jointly
as the solution of the following optimization problem:

min
{Ds∈Ds,ws∈W s}

f ({Ds, ws}) +

(cid:107)ws(cid:107)2
(cid:96)2

,

(11)

ν
2

S
(cid:88)

s=1

where f is deﬁned as the expected cumulative cost:

f ({Ds, ws}) = E

lsu(y, ws, αs(cid:63)),

(12)

S
(cid:88)

s=1

is

the

sth

column of

where αs(cid:63)
the minimizer
A(cid:63)({xs, Ds}) of
the optimization problem (7) and
lsu(y, w, α) is a convex loss function that measures how
well the classiﬁer parametrized by w can predict y by
observing α. The expectation is taken with respect to the
joint probability distribution of the multimodal inputs {xs}
and label y. Note that αs(cid:63) acts as a hidden/latent feature
vector, corresponding to the input xs, which is generated by
the learned discriminative dictionary Ds(cid:63). In general, lsu
can be chosen as any convex function such that lsu(y, ., .)
is twice continuously differentiable for all possible values
of y. A few examples are given below for binary and
multiclass classiﬁcation tasks.

1) Binary classiﬁcation: In a binary classiﬁcation task
where the label y belongs to the set {−1, 1}, lsu can be
naturally chosen as the logistic regression loss

lsu(y, w, α(cid:63)) = log(1 + e−ywT α(cid:63)

),

(13)

where w ∈ Rd is the classiﬁer parameters. Once the
optimal {Ds, ws} are obtained, a new multimodal sample
{xs} is classiﬁed according to sign of (cid:80)S
s=1 wsT α(cid:63) due
to the uniform monotonicity of (cid:80)S
s=1 lsu. For simplicity,
the intercept term for the linear model is omitted here,
but it can be easily added. One can also use a bilinear
model where, instead of a set of vectors {ws}, a set of
matrices {W s} are learned and a new multimodal sample
is classiﬁed according to the sign of (cid:80)S
s=1 xsT W sα(cid:63).
Accordingly, the (cid:96)2-norm regularization of Eq. (11) needs
to be replaced with the matrix Frobenius norm. The bilinear
model is richer than the linear model and can sometimes
result in better classiﬁcation performance but needs more
careful training to avoid over-ﬁtting.

5

2) Multiclass classiﬁcation: Multiclass classiﬁcation can
be formulated using a collections of (independently learned)
binary classiﬁers in a one-vs-one or one-vs-all setting.
Multiclass classiﬁcation can also be handled in an all-vs-all
setting using the softmax regression loss function. In this
scheme, the label y belongs to the set {1, . . . , K} and the
softmax regression loss is deﬁned as

lsu(y, W , α(cid:63)) = −

1{y=k} log

K
(cid:88)

k=1

(cid:32)

ewT
k α(cid:63)
l=1 ewT

(cid:80)K

l α(cid:63)

(cid:33)

,

(14)
where W = [w1 . . . wK] ∈ Rd×K, and 1{.} is the indicator
function. Once the optimal {Ds, W s} are obtained, a new
multimodal sample {xs} is classiﬁed as

argmaxk∈{1,...,K}

(cid:32)

S
(cid:88)

s=1

T αs(cid:63)

k

ews
l=1 ews

l

(cid:80)K

T αs(cid:63)

(cid:33)

.

(15)

In yet another all-vs-all setting, the multiclass classiﬁcation
task can be turned into a regression task in which the scaler
label y is changed to a binary vector y ∈ RK, where the
kth coordinate corresponding to the label of {xs} is set to
one and the rest of the coordinates are set to zero. In this
setting, lsu is deﬁned as

lsu(y, W , α(cid:63)) =

1
2
where W ∈ RK×d. Having obtained the optimal
{Ds, W s}, the test sample {xs} is then classiﬁed as

(cid:107)y − W α(cid:63)(cid:107)2
(cid:96)2

(16)

,

S
(cid:88)

argmink∈{1,...,K}

(cid:107)qk − W sαs(cid:63)(cid:107)2
(cid:96)2

,

(17)

s=1
where qk is a binary vector in which its kth coordinate is
one and its remaining coordinates are zero.

In choosing between the one-vs-all setting, in which
independent multimodal dictionaries are trained for each
class, and the multiclass formulation, in which multimodal
dictionaries are shared between classes, a few points should
be considered. In the one-vs-all setting, the total number of
dictionary atoms is equal to dSK in the K-class classiﬁ-
cation while in the multiclass setting the number is equal
to dS. It should be noted that in the multiclass setting a
larger dictionary is generally required to achieve the same
level of performance to capture the variations among all
classes. However, it is generally observed that the size
of the dictionaries in multiclass setting is not required to
grow linearly as the number of classes increases due to
atom sharing among the different classes. Another point to
consider is that the class-speciﬁc dictionaries of the one-
vs-all approach are independent and can be obtained in
parallel. In this paper, the multiclass formulation is used
to allow feature sharing among the classes.

C. Optimization

The main challenge in optimizing (11) is the non-
differentiability of A(cid:63)({xs, Ds}). However,
it can be
shown that although the sparse coefﬁcients A(cid:63) are obtained
by solving a non-differentiable optimization problem, the

6

function f ({Ds, ws}), deﬁned in Eq. (12), is differen-
tiable on D1 × · · · DS × W 1 × · · · W S, and therefore its
gradients are computable. To ﬁnd the gradient of f with
respect to Ds, one can ﬁnd the optimality condition of the
optimization (7) or use the ﬁxed point differentiation [36],
[38] and show that A(cid:63) is differentiable over its non-zero
label
rows. Without
y admits a ﬁnite set of values such as those deﬁned in
Eqs. (13) and (14). The same algorithm can be derived for
the scenario when y belongs to a compact subset of a ﬁnite-
dimensional real vector space as in Eq. (16). A couple of
mild assumptions are required to prove the differentiability
of f which are direct generalizations of those required for
the single modal scenario [29] and are listed below:

loss of generality, we assume that

Assumption (A). The multimodal data (y, {xs}) admit a

probability density p with compact support.

Assumption (B). For all possible values of y, p(y, .)
is continuous and lsu(y, .) is twice continuously differen-
tiable.

The ﬁrst assumption is reasonable when dealing with
the signal/image processing applications where the acquired
values obtained by the sensors are bounded. Also all the
given examples for lsu in the previous section satisfy the
second assumption. Before stating the main proposition of
this paper below, the term active set is deﬁned.

Deﬁnition 3.1 (Active set): The active set Λ of the solu-
tion A(cid:63) of the joint sparse coding problem (7) is deﬁned
to be

Λ = {j ∈ {1, . . . , d} : (cid:107)a(cid:63)

j→(cid:107)(cid:96)2 (cid:54)= 0},

(18)

where a(cid:63)

j→ is the jth row of A(cid:63).

Proposition 3.1 (Differentiability and gradients of f ):
Let λ2 > 0 and the assumptions (A) and (B) hold. Let
Υ = ∪j∈ΛΥj where Υj = {j, j + d, . . . , j + (S − 1)d}.
Let the matrix ˆD ∈ Rn×|Υ| be deﬁned as
ˆD =

(cid:104) ˆD1 . . . ˆD|Λ|

(19)

(cid:105)

,

where ˆDj = blkdiag(d1
j ) ∈ Rn×S, ∀j ∈ Λ, is
j , . . . , dS
the collection of the jth active atoms of the multimodal
j is the jth active atom of Ds, blkdiag is
dictionaries, ds
the block diagonalization operator, and n = (cid:80)
s∈S ns. Also
let matrix ∆ ∈ R|Υ|×|Υ| be deﬁned as

∆ = blkdiag(∆1, . . . , ∆|Λ|),

(20)

j→ ∈
I −
where ∆j =
RS×S, ∀j ∈ Λ, and I is the identity matrix. Then, the
function f deﬁned in Eq. (12) is differentiable and ∀s ∈ S,

(cid:107)a(cid:63)

(cid:107)a(cid:63)

j→

1
j→(cid:107)(cid:96)2

1
j→(cid:107)(cid:96)2

3 a(cid:63)

T a(cid:63)

∇wsf = E [∇ws lsu (y, ws, αs(cid:63))] ,
∇Dsf = E

(xs − Dsαs(cid:63)) βT

(cid:104)

˜s − Dsβ˜sαs(cid:63)T (cid:105)

,

(21)

where ˜s = {s, s + S, . . . , s + (d − 1)S} and β ∈ RdS is
deﬁned as

βΥc = 0, βΥ = ( ˆDT ˆD + λ1∆ + λ2I)−1g,

(22)
T (cid:80)S
s=1 lsu(y, ws, αs(cid:63))), Υc =
in which g = vec(∇A(cid:63)
{1, . . . , dS} \Υ, βΥ ∈ R|Υ| is formed of those rows of β
indexed by Υ, and vec(.) is the vectorization operator.

Λ→

The proof of this proposition is given in the Appendix.
A stochastic gradient descent algorithm to ﬁnd the optimal
dictionaries {Ds(cid:63)} and classiﬁers {ws(cid:63)} is described in
Algorithm 1. The stochastic gradient descent algorithm is
guaranteed to converge under a few assumptions that are
mildly stricter than those in this paper (requires three-times
differentiability) [53]. To further improve the convergence
of the proposed stochastic gradient descent algorithm, a
classic mini-batch strategy is used in which a small batch
of the training data are sampled in each batch, instead of 1
sample, and the parameters are updated using the averaged
updates of the batch. This has additional advantage in which
ˆDT ˆD and the corresponding factorization of the ADMM
for solving the sparse coding problem can be computed
once for the whole batch. For the special case when S = 1,
the proposed algorithm reduces to the single-modal task-
driven dictionary learning algorithm in [29]. Selecting λ2 in
Eq. (7) to be strictly positive guarantees the linear equations
of (22) to have a unique solution. In other words, it is easy
to show that the matrix ( ˆDT ˆD + λ1∆ + λ2I) is positive
deﬁnite given λ1 ≥ 0, λ2 > 0. However, in practice it is
observed that the solution of the joint sparse representation
problem is numerically stable since ˆD becomes full-column
rank when sparsity is sought with a sufﬁciently large λ1,
and λ2 can be set to zero. It should be noted that the
assumption of ˆD being a full column rank matrix is a
common assumption in sparse linear regression [26]. As
in any non-convex optimization algorithm, if the algorithm
is not initialized properly, it may yield poor performance.
Similar to [29], the dictionaries {Ds} are initialized by the
solution of the unsupervised multimodal dictionary learning
algorithm. Upon assignment of the initial dictionaries,
parameters {ws} of the classiﬁers are set by solving (11)
only with respect to {ws} which is a convex optimization
problem.

D. Extension

We now present an extension of the proposed algorithm
with a more ﬂexible structure on the sparse codes. Joint
sparse representation relies on the fact that all the modalities
share the same sparsity pattern in which, if a multimodal
training sample is selected to reconstruct the input, then
all the modalities within that training sample are active.
However, this group sparsity constraint, imposed by the
(cid:96)12 norm, may be too stringent for some applications [45],
[54], for example in the scenarios where the modalities
have different noise levels or when the heterogeneity of
the modalities imposes different sparsity levels for the
reconstruction task. A natural relaxation to the joint sparsity
prior is to let the multimodal inputs not share the full
active set which can be achieved by replacing the (cid:96)12 norm
with a combination of the (cid:96)12 and (cid:96)11 norms ((cid:96)12 − (cid:96)11
norm). Following the same formulation as in Section III-B,
let A(cid:63)({xs, Ds}) in Eq. (11) be the minimizer of the

Algorithm 1 Stochastic gradient descent algorithm for multi-
modal task-driven dictionary learning.

Input: Regularization parameters λ1, λ2, ν,

learning rate parameters
ρ, t0, number of iterations T , initial dictionaries {Ds ∈ Ds}s∈S ,
initial model parameters {ws ∈ W s}s∈S .

Output: Learned {Ds, ws}
1: for t = 1, . . . , T do
2:
3:

t , . . . , xS
Draw a random sample (x1
t , yt) from the training data.
Find solution A(cid:63) = (cid:2)α(cid:63)1 . . . α(cid:63)S (cid:3) ∈ Rd×S of the joint sparse
coding problem

argmin
A=[α1...αS]

1
2

S
(cid:88)

s=1

(cid:107)xs

t − Dsαs(cid:107)2
(cid:96)2

+ λ1(cid:107)A(cid:107)(cid:96)12 +

(cid:107)A(cid:107)2
F .

λ2
2

4:
5:
6:
7:

8:
9:

Compute set of active rows Λ of A(cid:63) using (18).
Compute ˆD ∈ Rn×|Υ| using (19).
Compute ∆ ∈ R|Υ|×|Υ| using (20).
Compute β ∈ RdS as:

βΥc = 0, βΥ = ( ˆDT ˆD + λ1∆ + λ2I)−1g,

T

Λ→

(cid:80)S

where Υ = ∪j∈Λ{j, j + d, . . . , j + (S − 1)d} and g =
s=1 lsu(yt, ws, αs(cid:63))).
vec(∇A(cid:63)
Choose the learning rate ρt ← min(ρ, ρ t0
Update the parameters by a projected gradient step:
ws ← ΠW s [ws − ρt (∇ws lsu (yt, ws, αs(cid:63)) + νws)] ,
Ds ← ΠDs

˜s − Dsβ˜sαs(cid:63)T (cid:17)(cid:105)

t − Dsαs(cid:63)) βT

Ds − ρt

t ).

(xs

(cid:16)

(cid:104)

,

∀s ∈ S, where ˜s = {s, s + S, . . . , s + (d − 1)S}.

10: end for

following optimization problem:

min
A

1
2

S
(cid:88)

s=1

(cid:107)xs − Dsαs(cid:107)2
(cid:96)2

+ λ1(cid:107)A(cid:107)(cid:96)12 + λ(cid:48)

1(cid:107)A(cid:107)(cid:96)11 +

(cid:107)A(cid:107)2
F ,

λ2
2

(23)

where λ(cid:48)
1 is the regularization parameter for the added (cid:96)11
norm and other terms are the same as those in Eq. (7). The
selection of λ1 and λ(cid:48)
1 inﬂuences the sparsity pattern of
A(cid:63). Intuitively, as λ1/λ(cid:48)
1 increases, the group constraint be-
comes dominant and more collaboration is enforced among
the modalities. On the other hand, small values of λ1/λ(cid:48)
1
encourage independent reconstructions across modalities.
In the extreme case of λ1 being set to zero, the above
optimization problem is separable across the modalities.
The above formulation brings added ﬂexibility with the cost
of one additional design parameter which is obtained in this
paper using cross-validation.

Here we present how the Algorithm 1 should be modiﬁed
to solve the supervised multimodal dictionary learning
problem under the mixed (cid:96)12 − (cid:96)11 constraint. The proof
for obtaining the algorithm is similar to the one for the
(cid:96)12 norm and is brieﬂy discussed in the appendix. In
Algorithm 1, let A(cid:63) be the solution of the optimization
problem (23) and let Λ be the set of its active rows. Let
Ψ ⊆ {1, . . . , S|Λ|} be the set of indices with non-zero
T ); i.e. it consists of non-zero entries
entries in vec(A(cid:63)
of the active rows of A(cid:63). Let ˆD, ∆, and g be the same as
those deﬁned in algorithm 1. Then, β ∈ RdS is updated as

Λ→

βΥc = 0, βΥ = ( ˆDT
Ψ

ˆDΨ + λ1∆Ψ→,Ψ + λ2I)−1gΨ,

7

Fig. 2: Extracted modalities from a sample in AR dataset.

where Υ is the set of indices with non-zero entries in
vec(A(cid:63)T
) and Υc = {1, . . . , dS} \Υ. Note that Υ is
deﬁned over the entire matrix A(cid:63) while Ψ is deﬁned over its
active rows. The rest of the algorithm remains unchanged.

IV. RESULTS AND DISCUSSION

The performance of the proposed multimodal dictio-
nary learning algorithms are evaluated on the AR face
database [55], the CMU Multi-PIE dataset [56], the IXMAS
action recognition dataset [57] and the WVU multimodal
ls is chosen to be
dataset [58]. For these algorithms,
the quadratic loss of Eq. (16) to handle the multiclass
classiﬁcation. In our experiments, it is observed that us-
ing the multiclass formulation achieves similar classiﬁ-
cation performance compared to using the logistic loss
formulation of Eq. (13) in the one-vs-all setting. Regu-
larization parameters λ1 and ν are selected using cross-
validation in the sets {0.01 + 0.005k|k ∈ {−3, 3}} and
{10−2, ..., 10−9}, respectively. It is observed that when the
number of dictionary atoms is kept small compared to the
number of training samples, ν can be arbitrarily set to a
small value, e.g. ν = 10−8, for the normalized inputs.
When the mixed (cid:96)12 − (cid:96)11 norm is used, the regularization
parameters λ1 and λ(cid:48)
1 are selected by cross-validation
in the set {0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05}. The
parameter λ2 is set to zero in most of the experiments
except when using the (cid:96)11 prior in Section IV-B1 where
a small positive value for λ2 was required for convergence.
The learning parameter ρt
is selected according to the
(cid:1) where
heuristic proposed in [29], i.e. ρt = min (cid:0)ρ, ρ t0
ρ and t0 are constants. This results in a constant learning
rate during the ﬁrst t0 iterations and an annealing strategy
of 1/t for the rest of the iterations. It is observed that
choosing t0 = T /10, where T is the total number of
iterations over the whole training set, works well for all
of our experiments. Different values of ρ are tried during
the ﬁrst few iterations and the one that results in minimum
error on a small validation set is retained. T is set equal
to be 20 in all the experiments. We observed empirically
that the selection of these parameters is quite robust and
small variations in their values do not affect considerably
the obtained results. We also used a mini-batch size of 100
in all our experiments. It should also be noted that design
parameters for the competitive algorithms are also selected
using cross-validation for a fair comparison.

t

A. AR face recognition

The AR dataset consists of faces under different poses,
illumination and expression conditions, captured in two
sessions. A set of 100 users are used, each consisting of

8

TABLE III: Multimodal classiﬁcation results obtained for the AR datasets

SVM-Maj

SVM-Sum LR-Maj

LR-Sum MKL

JSRC [14]

JDSRC [7]

85.57

92.14

85.00

91.14

91.14

96.14

96.14

SMDL(cid:96)11
95.86

SMDL(cid:96)12
96.86

SMDL(cid:96)12−(cid:96)11
97.14

TABLE I: Correct classiﬁcation rates obtained using the
whole face modality for the AR database.

SVM MKL [59]

LR

SRC [10] UDL

SDL [29]

86.43

82.86

81.00

88.86

89.58

90.57

TABLE IV: Comparison of the reconstructive-based (JSRC
and JSRC-UDL) and the proposed discriminative-based
(SMDL(cid:96)12 ) classiﬁcation algorithms obtained using the
joint sparsity prior for different numbers of dictionary
atoms per class on the AR dataset.

TABLE II: Comparison of the (cid:96)11 and (cid:96)12 priors for mul-
timodal classiﬁcation. Modalities include 1. left periocular,
2. right periocular, 3. nose, 4. mouth, and 5. face.

Modalities
UMDL(cid:96)11
UMDL(cid:96)12
SMDL(cid:96)11
SMDL(cid:96)12

{1, 2}

{1, 2, 3}

{1, 2, 3, 4}

{1, 2, 3, 4, 5}

81.9
82.6
83.86
86.43

87.57
87.86
89.86
89.86

90.14
92.00
92.42
93.57

95.57
96.29
95.86
96.86

seven images from the ﬁrst session as training samples and
seven images from the second session as test samples. A
small randomly selected portion of the training set, 50 out
of 700, is used as validation set for optimizing the design
parameters. Fusion is taken on ﬁve modalities which are
the left and right periocular, nose, mouth, and the whole
face modalities, similar to the setup in [14], [45]. A test
sample from the AR dataset and the extracted modalities
are shown in Fig. 2. Raw pixels are ﬁrst PCA-transformed
and then normalized to have zero mean and unit l2 norm.
The dictionary size for the dictionary learning algorithms
is chosen to be four per class, resulting in dictionaries of
overall 400 atoms.

Classiﬁcation using the whole face modality: The classi-
ﬁcation results using the whole face modality are shown in
Table I. The results are obtained using linear support vector
machine (SVM) [60], multiple kernel learning (MKL) [59],
logistic regression (LR) [60], sparse representation classi-
ﬁcation (SRC) [10], and unsupervised and supervised dic-
tionary learning algorithms (UDL and SDL) [29]. For the
MKL algorithm, linear, polynomial, and RBF kernels are
used. The UDL and SDL are equipped with the quadratic
classiﬁer (16). The SDL results in the best performance.

(cid:96)11 vs (cid:96)12 sparse priors for multimodal classiﬁcation: A
straightforward way of utilizing the single-modal dictionary
learning algorithms, namely UDL and SDL, for multimodal
classiﬁcation is to train independent dictionaries and clas-
siﬁers for each modality and then combine the individual
scores for a fused decision. This way of fusion is equivalent
to using the (cid:96)11 norm on A,
in
Eq. (7) (or setting λ1 to zero in Eq. (23)) which does not
enforce row sparsity in the sparse coefﬁcients. We denote
the corresponding unsupervised and supervised multimodal
dictionary learning algorithms using only the (cid:96)11 norm as
UMDL(cid:96)11 and SMDL(cid:96)11, respectively. Similarly, the pro-
posed unsupervised and supervised multimodal dictionary
learning algorithms using the (cid:96)12 norm are denoted as

instead of (cid:96)12 norm,

atoms/class

JSRC JSRC-UDL

1
2
3
4
5
6
7

46.14
69.00
79.57
88.14
91.00
94.43
96.14

71.71
78.86
83.57
91.14
94.85
96.28
96.14

SMDL(cid:96)12
91.28
95.00
95.71
96.86
97.14
96.71
96.00

UMDL(cid:96)12 and SMDL(cid:96)12. Table II compares the perfor-
mance of the multimodal dictionary learning algorithms
under the two priors. As shown, the proposed algorithms
with (cid:96)12 prior, which enforces collaborations among the
modalities, have better fusion performances than those with
(cid:96)11 prior. In particular, SMDL(cid:96)12 has signiﬁcantly better
performance than the SMDL(cid:96)11 for fusion of the ﬁrst and
second (left and right periocular) modalities. This agrees
with the intuition that these modalities are highly correlated
and learning the multimodal dictionaries jointly indeed
improves the recognition performance.

Comparison with other fusion methods: The perfor-
mances of the proposed fusion algorithms under different
sparsity priors are compared with those of the several state-
of-the-art decision-level and feature-level fusion algorithms.
In addition to (cid:96)11 and (cid:96)12 priors, we evaluate the proposed
supervised multimodal dictionary learning algorithm with
the mixed (cid:96)12−(cid:96)11 norm which is denoted as SMDL(cid:96)12−(cid:96)11.
One way to achieve decision-level fusion is to train in-
dependent classiﬁers for each modality and aggregate the
outputs by either adding the corresponding scores of each
modality to come up with the fused decision, or using the
majority voting among the independent decisions obtained
from different modalities. These approaches are abbrevi-
ated with Sum and Maj, respectively, and are used with
SVM and LR classiﬁers for decision-level fusion. The pro-
posed methods are also compared with feature-level fusion
methods including the joint sparse representation classiﬁer
(JSRC) [14], joint dynamic sparse representation classiﬁer
(JDSRC) [7], and MKL. For the JSRC and JDSRC, the
dictionary consists of all the training samples. Table III
compares the performance of our proposed algorithms with
the other fusion algorithms for the AR dataset. As expected,
the multimodal fusion results in signiﬁcant performance
improvement compared to using only the whole face modal-
ity. Moreover, the proposed SMDL(cid:96)12 and SMDL(cid:96)12−(cid:96)11
achieve the superior performances.

Reconstructive vs discriminative formulation with joint

9

Fig. 3: Computational time required to solve the optimiza-
tion problem (7) for a given test sample.

TABLE V: Comparison of the supervised multimodal dic-
tionary learning algorithms with different sparsity priors for
face recognition under occlusion on the AR dataset.

SMDL(cid:96)12
89.00

SMDL(cid:96)11
90.54

SMDL(cid:96)12−(cid:96)11
91.15

sparsity prior: Comparison of the algorithms with joint
sparsity priors in Table III indicates that
the proposed
SMDL(cid:96)12 algorithm equipped with dictionaries of size 400
achieves relatively better results than the JSRC that uses
dictionaries of size 700. The results conﬁrm the idea that
by using the supervised formulation, compared to using the
reconstruction error, one can achieve better classiﬁcation
performance even with more compact dictionaries. For
further comparison, an experiment is performed in which
the correct classiﬁcation rates of the reconsturtive and
discriminative formulations are compared when the their
dictionary sizes are kept equal. For a given number of
dictionary atoms per class d, dictionaries of JSRC are
thus constructed by random selection of d train samples
from different classes. This is different from the standard
JSRC, utilized for the results in Table III, in which all the
training samples are used to construct the dictionaries [14].
Moreover, to utilize all the available training samples for
the reconstructive approach and make a more meaningful
comparison, we use the unsupervised multimodal dictionary
learning algorithm of Eq. (8) to train class-speciﬁc sub-
dictionaries which minimizes the reconstruction error in
approximating the training samples for a given class. These
sub-dictionaries are then stacked to construct
the ﬁnal
dictionaries, similar to the approach in [22]. We call this
algorithm as JSRC-UDL to indicate that the dictionaries are
indeed learned by the reconstructive formulation. Table IV
summarizes the recognition performance of JSRC and
JSRC-UDL in comparison to the proposed SMDL(cid:96)12, which
enjoys a discriminative formulation, for different number of
dictionary atoms per class. As seen, SMDL(cid:96)12 outperforms
the reconstructive approaches, especially when the number
of dictionary is chosen to be relatively small. This is the
main advantage of SMDL(cid:96)12 compared to the reconstructive
approaches in which more compact dictionaries can be

Fig. 4: Conﬁgurations of the cameras and sample multi-
view images from CMU Multi-Pie dataset.

used for the recognition task that is important for the real-
time applications. It is clear that reconstructive model can
only result in comparable performance when the dictionary
size is chosen to be relatively large. On the other hand,
the SMDL(cid:96)12 algorithm may get over-ﬁtted with the large
number of dictionary atoms. In terms of computational
expense at test time, as discussed in [14], the time required
to solve the optimization problem (7) is expected to be
linear in the dictionary size using the efﬁcient ADMM if the
required matrix factorization is cashed beforehand. Typical
computational time to solve (7) for a given multimodal test
sample is shown in Fig. 3 for different dictionary sizes. As
expected, it increases linearly as the size of the dictionary
increases. This illustrates the advantage of the SMDL(cid:96)12
algorithm that results in the state-of-the-art performance
with more compact dictionaries.

Classiﬁcation in presence of disguise: The AR dataset
also contains 600 occluded samples per session, overall
1200 images, where the faces are disguised using sun
glasses or scarf. Here we use these additional images to
evaluate the robustness of the proposed algorithms. Similar
to previous experiments, images from session 1 are used
as training samples and images from session 2 are used
as test data. Classiﬁcation performance under different
sparsity priors are shown in Table V and as expected, the
SMDL(cid:96)12−(cid:96)11 achieves the best performance. In presence
of occlusion, some of the modalities are less coupled and
the joint sparsity prior among all the modalities may be too
stringent as is also reﬂected in the results.

B. Multi-view recognition

In this section,

1) Multi-view face recognition:

the
performance of the proposed algorithm is evaluated for
multi-view face recognition using the CMU Multi-PIE
[56]. The dataset consists of a large num-
dataset
ber of face images under different
illuminations, view-
points, and expressions which are recorded in four
several months. Subjects
sessions over
were imaged using 13 cameras at different view-angles
of {0◦, ±15◦, ±30◦, ±45◦, ±60◦, ±75◦, ±90◦} at head
height. Illustrations for the multiple camera conﬁgurations,
as well as sample multi-view images are shown in Fig. 4.
We use the multi-view face images for 129 subjects that are

the span of

10

TABLE VI: Correct classiﬁcation rates obtained using
individual modalities in the CMU Multi-PIE database.

View

SVM MKL

LR

Left
Frontal
Right

47.30
41.15
47.30

52.85
54.10
51.85

43.65
45.40
42.85

SRC

49.85
54.25
52.55

UDL

47.80
52.10
43.10

SDL

50.45
56.10
48.50

Fig. 5: Sample frames of the IXMAS dataset from 5
different views.

TABLE VII: Correct classiﬁcation rates (CCR) obtained
using multi-view images on the CMU Multi-PIE database.

TABLE VIII: Correct classiﬁcation rates (CCR) obtained
for multi-view action recognition on the IXMAS database.

Algorithm CCR

Algorithm

SVM-Maj
62.95
SVM-Sum 69.30
72.40
70.20
77.25
76.10

MKL
JDSRC
SMDL(cid:96)11
SMDL(cid:96)12

LR-Maj
LR-Sum
JSRC
UMDL(cid:96)11
UMDL(cid:96)12
SMDL(cid:96)12−(cid:96)11

CCR

69.40
71.10
73.30
74.80
70.50
81.30

Algorithm

Junejo et al. [65]
Wu et al. [66]
Wang et al. 2 [63]
UMDL(cid:96)11
UMDL(cid:96)12

CCR

79.6
88.2
93.6
90.3
90.6

Algorithm

Tran and Sorokin [62]
Wang et al. 1 [63]
JSRC
SMDL(cid:96)11
SMDL(cid:96)12

CCR

80.2
87.8
93.6
93.9
94.8

present in all sessions. The face regions for all the poses
are extracted manually and resized to 10 × 8. Similar to
the protocol used in [61], images from session 1 at views
{0◦, ±30◦, ±60◦, ±90◦} are used as training samples. Test
images are obtained from all available view angles from
session 2 to have a more realistic scenario in which not
all
the testing poses are available in the training set.
To handle multi-view recognition using the multi-modal
formulation, we divide the available views into three sets of
{−90◦, −75◦, −60◦, −45◦}, {−30◦, −15◦, 0◦, 15◦, 30◦, },
{45◦, 60◦, 75◦, 90◦}, each of which forms a modality. A
test sample is then constructed by randomly selecting an
image from each modality. Two thousand test samples are
generated in this way. The dictionary size for the dictionary
learning algorithms is chosen to have two atoms per class.
The classiﬁcation results obtained using individual
modalities are shown in Table VI. As expected, better
classiﬁcation performance is obtained using the frontal
view. Results of the multi-view face recognition is shown
in Table VII. The proposed supervised dictionary learn-
ing algorithms outperform the corresponding unsupervised
methods and other fusion algorithms. The SMDL(cid:96)12−(cid:96)11
results in the state-of-the-art performance. It is consistently
observed in all the studied applications that the multimodal
dictionary learning algorithm with the mixed prior results
in better performance than those with individual (cid:96)12 or
(cid:96)12 prior. However, it requires one additional regularizing
parameter to be tuned. For the rest of the paper,
the
performance of the proposed dictionary learning algorithms
are only reported under the individual priors.

the proposed algorithm for

2) Multi-view action recognition: This section presents
the results of
the pur-
pose of multi-view action recognition using the IXMAS
dataset [57]. Each action is recorded simultaneously by
cameras from ﬁve different viewpoints, which are con-
sidered as modalities in this experiment. A multimodal
sample of the IXMAS dataset is shown in Fig. 5. The
dataset contains 11 action classes where each action is
repeated three times by each of the ten actors, resulting
in 330 sequences per view. The dataset include actions

such as check watch, cross arms, and scratch head. Similar
to the work in [57], [62], [63], leave-one-actor-out cross-
validation is performed and samples from all ﬁve views are
used for training and testing.

We use dense trajectories as features which are generated
using the publicly available code [63] in which a 2000
word codebook is generated by a random subset of these
trajectories and the k-means clustering as in [64]. Note that
Wang et al. [63] used HOG, HOF, and MBH descriptors in
addition to the dense trajectories. However, here only dense
trajectory descriptors are used. The number of dictionary
atoms for the proposed dictionary learning algorithms are
chosen to be 4 atoms per class, resulting in a dictionary of
44 atoms per view. The ﬁve dictionaries for JSRC are con-
structed using all the training samples, thus each dictionary,
corresponding to a different view, has 297 atoms.

Table VIII shows average accuracies over all classes
obtained using the existing algorithms and the state of
the art algorithms. The Wang et al. 1 [63] algorithm uses
only the dense trajectories as feature, similar to our setup.
The Wang et al. 2 [63] algorithm, however, uses HOG,
HOF, MBH descriptors and the spatio-temporal pyramids
in addition to the trajectory descriptor. The results show
that the proposed SMDL(cid:96)12 algorithm achieves the superior
performance while the SMDL(cid:96)11 algorithm achieves the
second best performance. This indicates that sparse coefﬁ-
cients generated by the trained dictionaries are indeed more
discriminative than the engineered features. The resulting
confusion matrix of the SMDL(cid:96)12 algorithm is shown in
Fig. 6.

C. Multimodal biometric recognition

The WVU dataset consists of different biometric modali-
ties such as ﬁngerprint, iris, palmprint, hand geometry, and
voice from subjects of different age, gender, and ethnicity.
It
is a challenging data set, as many of the samples
are corrupted with blur, occlusion, and sensor noise. In
this paper, two irises (left and right) and four ﬁngerprint
modalities are used. The evaluation is done on a subset
of 202 subjects which have more than four samples in all

TABLE IX: Correct classiﬁcation rates obtained using individual modalities in the WVU database.

Finger 1

Finger 2

Finger 3

Finger 4

Iris 1

Iris 2

SVM 56.77 ± 0.72
61.81 ± 1.39
MKL
55.64 ± 1.89
LR
67.66 ± 1.86
SRC
64.68 ± 2.11
UDL
66.29 ± 1.81
SDL

82.95 ± 2.15
82.55 ± 1.47
81.10 ± 1.85
88.68 ± 1.59
87.35 ± 2.23
88.84 ± 2.31

55.83 ± 2.03
63.50 ± 1.75
55.21 ± 2.21
69.29 ± 0.77
67.35 ± 1.22
68.61 ± 1.30

80.47 ± 0.91
81.85 ± 0.74
78.82 ± 0.66
88.68 ± 1.03
86.40 ± 0.70
87.50 ± 0.82

60.67 ± 1.78
56.31 ± 2.20
55.25 ± 1.48
65.43 ± 1.24
64.36 ± 1.37
66.05 ± 0.75

57.52 ± 1.95
54.49 ± 0.79
56.86 ± 1.70
67.78 ± 1.76
65.23 ± 2.02
67.31 ± 1.38

11

TABLE X: Multimodal classiﬁcation results obtained for
the WVU dataset.

Algorithm 4 Fingerprints

2 Irises

All modalities

90.14 ± 0.70
SVM-Maj
SVM-Sum 93.56 ± 1.26
89.23 ± 1.63
93.60 ± 0.96
93.28 ± 1.52
97.64 ± 0.44
97.17 ± 0.26
97.09 ± 0.56
97.41 ± 0.71
96.78 ± 0.57
97.56 ± 0.41

LR-Maj
LR-Sum
MKL
JSRC
JDSRC
UMDL(cid:96)11
SMDL(cid:96)11
UMDL(cid:96)12
SMDL(cid:96)12

65.30 ± 1.92
74.03 ± 1.89
63.73 ± 1.29
71.43 ± 1.91
67.23 ± 0.70
82.94 ± 0.78
79.61 ± 0.70
80.90 ± 0.61
82.83 ± 0.87
81.53 ± 2.18
83.77 ± 0.89

95.24 ± 0.92
97.09 ± 0.83
94.18 ± 1.13
98.51 ± 0.18
94.46 ± 0.87
98.89 ± 0.30
97.80 ± 0.51
98.62 ± 0.46
98.66 ± 0.43
98.78 ± 0.43
99.10 ± 0.30

nally proposed for biometric recognition systems [67]. As
seen, the SMDL(cid:96)12 algorithm outperforms the competitive
algorithms and achieves the state-of-the-art performance
using the Irises and all modalities with the rank one
recognition rate of 83.77% and 99.10%, respectively. Using
the ﬁngerprints, the performance of the SMDL(cid:96)12 is close to
the best performing algorithm, which is JSRC. The results
suggest that using joint sparsity prior indeed improves the
multimodal classiﬁcation performance by extracting the
coupled information among the modalities.

Comparison of the algorithms with the joint sparsity
the proposed SMDL(cid:96)12 algorithm
priors indicates that
equipped with dictionaries of size 404 achieves comparable,
and mostly better, results than the JSRC that uses dictionary
of size 808. Similar to the experiment in Section IV-A,
we compared the reconstructive and discriminating algo-
rithms that are based on the joint sparsity prior when the
number of dictionary atoms per class is kept equal. Fig. 8
summarizes the results of the different fusion scenarios.
As seen, SMDL(cid:96)12 signiﬁcantly outperforms JSRC and
JSRC-UDL when the number of dictionary atoms per class
is chosen to be 1 or 2. The results are consistent with
that of Table IV for the AR dataset indicating that the
proposed supervised formulation equipped with more com-
pact dictionaries achieves superior performance than that
of the reconstructive formulation for the studied biometric
recognition applications.

V. CONCLUSIONS AND FUTURE WORKS

The problem of multimodal classiﬁcation using sparsity
models was studied and a task-driven formulation was
proposed to jointly ﬁnd the optimal dictionaries and clas-
siﬁers under the joint sparsity prior. It was shown that the

Fig. 6: The confusion matrix obtained by the SMDL(cid:96)12
algorithm on the IXMAS dataset. The actions are 1: check
watch, 2: cross arms, 3: scratch head, 4: sit down, 5: get
up, 6: turn around, 7: walk, 8: wave, 9: punch, 10: kick and
11: pick up.

modalities. Samples from different modalities are shown in
Fig. 1. The training set is formed by randomly selecting
four samples from each subject, overall 808 samples. The
remaining 509 samples are used for testing. The features
used here are those described in [14] which are further
PCA-transformed. The dimension of the input data after
preprocessing are 178 and 550 for the ﬁngerprint and iris
modalities, respectively. All inputs are normalized to have
zero mean and unit l2 norm. The number of dictionary
atoms for the dictionary learning algorithms are chosen to
be 2 per class, resulting in dictionaries of overall 404 atoms.
The dictionaries for JSRC and JDSRC are constructed using
all the training samples.

The classiﬁcation results obtained using individual
modalities on 5 different splits of the data into training and
test samples are shown in Table IX. As shown, ﬁnger 2 is
the strongest modality for the recognition task. The SRC
and SDL algorithms achieve the best results. It should be
noted that dictionary size of SRC is twice of that in SDL.
For multimodal classiﬁcation, we consider fusion of
ﬁngerprints, fusion of Irises, and fusion of all the modali-
ties. Table X summarizes the correct classiﬁcation rates of
several fusion algorithms using 4 ﬁngerprints, 2 Irises, and
all the modalities, obtained on 5 different training and test
splits. Fig. 7 shows the corresponding cumulative matched
score curves (CMC) for the competitive methods. CMC is
a performance measure, similar to ROC, which is origi-

12

Fig. 8: Comparison of the reconstructive-based (JSRC
and JSRC-UDL) and the proposed discriminative-based
(SMDL(cid:96)12 ) classiﬁcation algorithms obtained using the
joint sparsity prior for different numbers of dictionary
atoms per class on the WVU dataset.

utilizes the stochastic gradient algorithm, the learning rate
should be carefully chosen for convergence of the algo-
rithm. In out experiments, a heuristic was used to control
the learning rate. Topics of future research include develop-
ing of better optimization tools for fast convergence guaran-
tee in this non-convex setting. Moreover, developing task-
driven dictionary learning algorithms under other proposed
structured sparsity priors for multimodal fusion such as the
tree-structured sparsity prior [45], [68] is another future
research topic. Future research will also include adapting of
the proposed algorithms for other multimodal tasks such as
multimodal retrieval, multimodal action recognition using
Kinect data, and image super-resolution.

APPENDIX

The proof of Proposition 3.1 is presented using the

following two results.

(cid:104)

Lemma A.1 (Optimality condition): The matrix A(cid:63) =
α1(cid:63) . . . αS (cid:63)(cid:105)
∈ Rd×S is a min-
1→
imizer of (7) if and only if , ∀j ∈ {1, . . . , d},

T . . . a(cid:63)

T (cid:105)T

a(cid:63)

d→

=

(cid:104)






T (cid:16)

(cid:104)

d1
j

. . . dS
j

x1 − D1α1(cid:63)(cid:17)
a(cid:63)
j→
j→ = λ1
(cid:107)a(cid:63)
j→(cid:107)(cid:96)2
x1 − D1α1(cid:63)(cid:17)
j→(cid:107)(cid:96)2 ≤ λ1, otherwise.

. . . dS
j

, if (cid:107)a(cid:63)
T (cid:16)

− λ2a(cid:63)
T (cid:16)
(cid:104)
d1
(cid:107)
j
− λ2a(cid:63)

T (cid:16)

xS − DSαS (cid:63)(cid:17)(cid:105)

j→(cid:107)(cid:96)2 (cid:54)= 0,
xS − DSαS (cid:63)(cid:17)(cid:105)

(24)
Proof: . The proof follows directly from the subgradi-

ent optimality condition of (7), i.e.

0 ∈ {

D1α1(cid:63)

. . . DS T (cid:16)
− x1(cid:17)
D1T (cid:16)
(cid:104)
+ λ2A(cid:63) + λ1P : P ∈ ∂(cid:107)A(cid:63)(cid:107)(cid:96)12 },

DSαS (cid:63)

− xS(cid:17)(cid:105)

where ∂(cid:107)A(cid:63)(cid:107)(cid:96)12 denotes the subgradient of the (cid:96)12 norm
evaluated at A(cid:63). As shown in [69], the subgradient is
characterized, for all j ∈ {1, . . . , d}, as pj→ = aj→
if (cid:107)aj→(cid:107)(cid:96)2 > 0, and (cid:107)pj→(cid:107)(cid:96)2 ≤ 1 otherwise.

(cid:107)aj→(cid:107)(cid:96)2

Fig. 7: CMC plots obtained by fusing the Irises (top),
ﬁngerprints (middle), and all modalities (below) on the
WVU dataset.

resulting bi-level optimization problem is smooth and an
stochastic gradient descent algorithm was proposed to solve
the corresponding optimization problem. The algorithm
was then extended for a more general scenario where
the sparsity prior was the combination of the joint and
independent sparsity constraints. The simulation results on
the studied image classiﬁcation applications suggest that
while the unsupervised dictionaries can be used for feature
learning, the sparse coefﬁcients generated by the proposed
multimodal task-driven dictionary learning algorithms are
usually more discriminative and therefore can result
in
improved multimodal classiﬁcation performance. It was
also shown that, compared to the sparse-representation
classiﬁcation algorithms (JSRC, JDSRC, and JSRC-UDL),
the proposed algorithms can achieve signiﬁcantly better
performance when compact dictionaries are utilized.

In the proposed dictionary learning framework which

Before proceeding to the next proposition, we need to
deﬁne the term transition point. For a given {xs}, let Λλ
be the active set of the solution A(cid:63) of (7) when λ1 = λ.
Then λ is deﬁned to be a transition point of {xs} if Λλ+(cid:15) (cid:54)=
Λλ−(cid:15), ∀(cid:15) > 0.

Proposition A.1 (Regularity of A(cid:63)): Let λ2 > 0 and

assumption (A) be hold. Then,

Part 1. A(cid:63)({xs, Ds}) is a continuous function of {xs}

and {Ds}.

Part 2. If λ1 is not a transition point of {xs}, then
the active set Λ of A(cid:63)({xs, Ds}) is locally constant with
respect to both {xs} and {Ds}. Moreover, A(cid:63)({xs, Ds})
is locally differentiable with respect to {Ds}.

Part 3. ∀λ1 > 0, ∃ a set Nλ1 of measure zero in which
∀{xs} ∈ {Rns}\Nλ1 , λ1 is not any of the transition points
of {xs}.

Proof: . Part 1. In the special case of S = 1, which
is equivalent to an elastic net problem, this has already
been shown [19], [70]. Our proof follows similar steps.
Assumption (A) guarantees that A(cid:63) is bounded. Therefore,
we can restrict the optimization problem (7) to a compact
subset of Rd×S. Since A(cid:63) is unique (imposed by λ2 > 0)
and the cost function of (7) is continuous in A and each
element of the set {xs, Ds} is deﬁned over a compact set,
A(cid:63)({xs, Ds}) is a continuous function of {xs} and {Ds}.
Part 2 and Part 3. These statements are proved here by
converting the optimization problem (7) into an equivalent
group lasso problem [71] and using some recent results
j ) ∈
on it. Let
Rn×S, ∀j ∈ {1, . . . , d}, be the block-diagnoal collection
of the jth atoms of the dictionaries. Also let D(cid:48) =
∈ Rn, and
[D(cid:48)
a(cid:48) = [a1→ . . . ad→]T ∈ RSd . Then (7) can be rewritten as

d] ∈ Rn×Sd, x(cid:48) =

x1T . . . xS T (cid:105)T

j = blkdiag(d1

the matrix D(cid:48)

j , . . . , dS

1 . . . D(cid:48)

(cid:104)

min
A

1
2

(cid:107)x(cid:48) − D(cid:48)a(cid:48)(cid:107)2
(cid:96)2

+ λ1

(cid:107)aj→(cid:107)(cid:96)2 +

(cid:107)A(cid:107)2

F . (25)

d
(cid:88)

j=1

λ2
2

This can be further converted into the standard group lasso:

min
A

1
2

(cid:107)x(cid:48)(cid:48) − D(cid:48)(cid:48)a(cid:48)(cid:107)2
(cid:96)2

+ λ1

(cid:107)aj→(cid:107)(cid:96)2,

(26)

d
(cid:88)

j=1

(cid:104)

x(cid:48)T 0T (cid:105)T

(cid:105)T

where x(cid:48)(cid:48) =
D(cid:48)T √
(cid:104)
D(cid:48)(cid:48)
directly from the results in [72].

λ2I

∈ R(n+Sd)×Sd. It is clear that the matrix
is full column rank. The rest of the proof follows

∈ Rn+Sd and D(cid:48)(cid:48) =

Proof of Proposition 3.1: The above proposition
implies that A(cid:63) is differentiable almost everywhere. We
know prove the proposition 3.1. It is easy to show that f
is differentiable with respect to ws due to the assumption
(A) and the fact
lsu is twice differentiable. f is
also differentiable with respect to Ds given assumption
(A), twice differentiability of lsu, and the fact that A(cid:63)
is differentiable everywhere except on a set of measure
zero (Prop A.1). We obtain the derivative of f with respect
to Ds using the chain rule. The steps are similar to those

that

13

taken for (cid:96)1-related optimization in [36], though a bit more
involved. Since the active set is locally constant, using the
optimality condition (24), we can implicitly differentiate
A(cid:63)({xs, Ds}) with respect to Ds. For the non-active rows
of A(cid:63), the differential is zero. On the active set Λ, (24) can
be rewritten as

T (cid:16)

(cid:104)
D1
Λ

x1 − D1α1(cid:63)(cid:17)
(cid:34)

− λ2A(cid:63)

Λ→ = λ1

T (cid:16)

. . . DS
Λ

xS − DSαS (cid:63)(cid:17)(cid:105)
(cid:35)T

T

a(cid:63)
1→
(cid:107)a(cid:63)
1→(cid:107)(cid:96)2

. . .

T

a(cid:63)
N→
(cid:107)a(cid:63)
N→(cid:107)(cid:96)2

,

(27)

where N is the cardinality of Λ and DΛ and A(cid:63)
Λ→ are the
matrices consisting of active columns of D and active rows
of A(cid:63), respectively. For the rest of the proof, we only work
on the active set and the symbols Λ and (cid:63) are dropped for
the ease of notation. Taking the partial derivative from both
sides of (27) with respect to ds
ij, the element in the ith-row
and jth-column of Ds, and taking its transpose we have:





+

λ2

∂AT
∂ds
ij

0
(Dsαs − xs)T Es
ij + αsT Es
ij
0



T Ds

 +








∂α1T
∂ds
ij

∂αS T
∂ds
ij

D1T D1
...
DS T

DS








= −λ1

∆1

. . . ∆N

(cid:34)

1→

∂aT
∂ds
ij

(cid:35)

,

N→

∂aT
∂ds
ij

ij ∈ Rns×N is a matrix with zero elements except
where Es
the element in the ith row and jth column which is one
and

∆k =

1
(cid:107)ak→(cid:107)(cid:96)2

(cid:32)

I −

1
(cid:107)ak→(cid:107)2
(cid:96)2

(cid:33)

aT

k→ak→

∈ RS×S,

∀k ∈ {1, . . . , N }. It
Vectorizing the both sides and factorizing results in

is easy to check that ∆k ≥ 0.

vec

(cid:32)

(cid:33)

∂AT
∂ds
ij

=











P

0
(xs − Dsαs)T es
...
(xs − Dsαs)T es
ij N
0

ij 1

− αsT Es
ij

T ds
1

(28)

− αsT Es
ij

T ds
N











,

is

ij k

the kth
(cid:17)−1

column of Es

where es
=
(cid:16) ˆDT ˆD + λ1∆ + λ2I
, and ˆD and ∆ are deﬁned
in Eqs. (19) and (20), respectively. Further simplifying
Eq. (28) yields
(cid:33)
(cid:32)

ij, P

= P˜sEs
ij

T (xs − Dsαs) − P˜sds

T αs
j ,

i→

vec

∂AT
∂ds
ij

where ˜s is deﬁned in Eq. (21). Using the chain rule, we
have

(cid:34)

(cid:32)

(cid:33)(cid:35)

∂f
∂ds
ij

= E

gT vec

∂AT
∂ds
ij

,

14

where g = vec
respective to the active columns of dictionary Ds is

. Therefore, derivative with

(cid:16) ∂ (cid:80)S

s=1 lsu
∂AT

(cid:17)

∂f
∂Ds = E








. . .

gT P˜s

(cid:16)

gT P˜s

Es
11

(cid:16)

Es

ns1

gT P˜s
(cid:16)

Es

1N

T (xs − Dsαs) − ds
...
T (xs − Dsαs) − ds

(cid:17)

T αs
1

1→

(cid:17)

T αs
1


ns→
(cid:17)

T αs
N

1→

T (xs − Dsαs) − ds
...
T (xs − Dsαs) − ds
˜s gαsT (cid:105)

.

T αs
N

ns→






(cid:17)

(cid:16)

. . . gT P˜s

Es

nsN

(cid:104)

= E

(xs − Dsαs) gT P˜s − DsP T

Setting β = P T g ∈ RN S and noting that β˜s = P T
complete the proof.

˜s g

Derivation of the algorithm with the mixed (cid:96)12 − (cid:96)11
prior can be obtained similarly. For each active row j ∈ Λ
of A(cid:63), the solution of the optimization problem (23) with
the mixed prior, let Πj ⊆ S be the set of active modalities
which have non-zeros entries. Then the optimality condition
for the active row j is

(cid:104)
d1
j

T (cid:16)

x1 − D1α1(cid:63)(cid:17)

. . . dS
j

T (cid:16)

xS − DSαS (cid:63)(cid:17)(cid:105)

Πj

− λ2a(cid:63)

j→ = λ1

a(cid:63)
(cid:107)a(cid:63)

j→,Πj
j→(cid:107)(cid:96)2

+ λ(cid:48)

1 sign

(cid:16)

a(cid:63)

j→,Πj

(cid:17)

.

Then, the algorithm for the mixed prior can be obtained by
differentiating the optimality condition, following similar
steps as was shown for the (cid:96)12 prior.

REFERENCES

[1] D. L. Hall and J. Llinas,

“An introduction to multisensor data

fusion,” Proc. IEEE, vol. 85, no. 1, pp. 6–23, January 1997.
[2] P. K. Varshney, “Multisensor data fusion,” in Intell. Problem Solving.
Methodologies and Approaches. Springer Berlin Heidelberg, 2000.
[3] H. Wu, M. Siegel, R. Stiefelhagen, and J. Yang, “Sensor fusion
using Dempster-Shafer theory,” in Proc. 19th IEEE Instrum. and
Meas. Technol. Conf. (IMTC), 2002, pp. 7–12.

[4] A. Ross and R. Govindarajan, “Feature level fusion using hand and

face biometrics,” in SPIE proc. series, 2005, pp. 196–204.

[5] D. Ruta and B. Gabrys, “An overview of classiﬁer fusion methods,”

Comput. and Inf. syst., vol. 7, no. 1, pp. 1–10, 2000.

[6] A. Rattani, D. R. Kisku, M. Bicego, and M. Tistarelli, “Feature
level fusion of face and ﬁngerprint biometrics,” in Proc. 1st IEEE
Int. Conf. Biometrics: Theory, Applicat., and Syst., 2007, pp. 1–6.

[7] H. Zhang, N. M. Nasrabadi, Y. Zhang, and T. S. Huang, “Multi-
observation visual recognition via joint dynamic sparse representa-
tion,” in Proc. IEEE Conf. Comput. Vision (ICCV), 2011, pp. 595–
602.

[8] A. Klausne, A. Tengg, and B. Rinner, “Vehicle classiﬁcation on
multi-sensor smart cameras using feature- and decision-fusion,” in
Proc. 1st ACM/IEEE Int. Conf. Distributed Smart Cameras, 2007,
pp. 67–74.

[9] A. Rattani and M. Tistarelli, “Robust multi-modal and multi-unit
feature level fusion of face and iris biometrics,” in Advances in
Biometrics, pp. 960–969. Springer, 2009.

[10] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, “Robust
IEEE Trans. Pattern

face recognition via sparse representation,”
Anal. Mach. Intell, vol. 31, no. 2, pp. 210–227, Feb. 2009.

[11] X. Mei and H. Ling, “Robust visual tracking and vehicle classiﬁ-
cation via sparse representation,” IEEE Trans. Pattern Anal. Mach.
Intell, vol. 33, no. 11, pp. 2259–2272, Nov. 2011.

[12] H. Zhang, Y. Zhang, N. M. Nasrabadi, and T. S. Huang, “Joint-
structured-sparsity-based classiﬁcation for multiple-measurement
transient acoustic signals,” IEEE Trans. Syst., Man, Cybern., vol.
42, no. 6, pp. 1586–98, Dec. 2012.

[13] R. Caruana, Multitask learning, Springer, 1998.
[14] S. Shekhar, V. Patel, N. M. Nasrabadi, and R. Chellappa, “Joint
sparse representation for robust multimodal biometrics recognition,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 1, pp. 113–126,
Jan. 2013.

[15] U. Srinivas, H. Mousavi, C. Jeon, V. Monga, A. Hattel, and B. Ja-
yarao, “Simultaneous sparsity model for histopathological image
representation and classiﬁcation,” IEEE Trans. Med. Imag., vol. 33,
no. 5, pp. 1163 – 1179, May 2014.

[16] H. S. Mousavi, V. Srinivas, U.and Monga, Y. Suo, M. Dao, and T. D.
Tran, “Multi-task image classiﬁcation via collaborative, hierarchical
spike-and-slab priors,” in IEEE Intl. Conf. Image Processing (ICIP),
Oct 2014, pp. 4236–4240.

[17] N. H. Nguyen, N. M. Nasrabadi, and T. D. Tran, “Robust multi-
sensor classiﬁcation via joint sparse representation,” in Proc. 14th
Int. Conf. Information Fusion (FUSION), 2011.

[18] M. Yang, L. Zhang, D. Zhang, and S. Wang, “Relaxed collaborative
in Proc. IEEE Conf.
representation for pattern classiﬁcation,”
Comput. Vision and Pattern Recognition (CVPR), 2012, pp. 2224–
2231.

[19] J. Mairal, F. Bach, J. Ponce, and G. Sapiro,

“Online dictionary
learning for sparse coding,” Proc. 26th Annu. Int. Conf. Mach.
Learning (ICML), pp. 689–696, 2009.

[20] J. Mairal, F. Bach, A. Zisserman, and G. Sapiro,

“Supervised
in Advances Neural Inform. Process. Syst.

dictionary learning,”
(NIPS), 2008, pp. 1033–1040.

[21] J. Mairal, M. Elad, and G. Sapiro, “Sparse representation for color
image restoration,” IEEE Trans. Image Process., vol. 17, no. 1, pp.
53–69, Jan. 2008.

[22] M. Yang, L. Zhang, J. Yang, and D. Zhang, “Metaface learning for
sparse representation based face recognition,” in Proc. IEEE Conf.
Image Process. (ICIP), 2010, pp. 1601–1604.

[23] Y. L. Boureau, F. Bach, Y. LeCun, and J. Ponce, “Learning mid-
level features for recognition,” in Proc. IEEE Conf. Comput. Vision
and Pattern Recognition (CVPR), 2010, pp. 2559–2566.

[24] Z. Jiang, Z. Lin, and L. S. Davis, “Label consistent K-SVD: Learning
a discriminative dictionary for recognition,” IEEE Trans. Pattern
Anal. Mach. Intell, vol. 35, no. 11, pp. 2651–2664, Nov. 2013.
[25] M. Aharon, M. Elad, and A. Bruckstein, “K-SVD : An algorithm
for designing overcomplete dictionaries for sparse representation,”
IEEE Trans. Signal Process., vol. 54, no. 11, pp. 4311–4322, Nov.
2006.

[26] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online learning for
matrix factorization and sparse coding,” The J. of Mach. Learning
Research, vol. 11, pp. 19–60, 2010.

[27] K. Engan, S. O. Aase, and J. H. Husoy,

“Method of optimal
directions for frame design,” in Proc. IEEE Int. Conf. Acoustics,
Speech, and Signal Process., 1999, vol. 5, pp. 2443–2446.

[28] M. Elad and M. Aharon, “Image denoising via saprse and redundant
IEEE Trans. Image

representations over learned dictionaries,”
Process., vol. 15, no. 12, pp. 3736–3745, Dec. 2006.

[29] J. Mairal, F. Bach, and J. Ponce, “Task-driven dictionary learning,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 4, pp. 791–804,
Apr. 2012.

[30] J. Yang, Z. Wang, Z. Lin, X. Shu, and T. Huang, “Bilevel sparse
coding for coupled feature spaces,” in IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2012, pp. 2360–2367.
[31] S. Kong and D. Wang, “A brief summary of dictionary learning
based approach for classiﬁcation,” arxivId:1205.6544, 2012.
[32] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman, “Dis-
criminative learned dictionaries for local image analysis,” in Proc.
IEEE Conf. Comput. Vision and Pattern Recognition (CVPR), 2008,
pp. 1–8.

[33] Q. Zhang and B. Li, “Discriminative K-SVD for dictionary learning
in face recognition,” in Proc. IEEE Conf. Comput. Vision and Pattern
Recognition (CVPR), 2010, pp. 2691–2698.

[34] I. Ramirez, P. Sprechmann, and G. Sapiro,

“Classiﬁcation and
clustering via dictionary learning with structured incoherence and
shared features,” in Proc. IEEE Conf. Comput. Vision and Pattern
Recognition (CVPR), 2010, pp. 3501–3508.

[35] M. Yang, L. Zhang, X. Feng, and D. Zhang, “Fisher discrimination
dictionary learning for sparse representation,” in Proc. IEEE Int.
Conf. Comput. Vision (ICCV), 2011, pp. 543–550.

[36] J. Yang, K. Yu, and T. Huang,

“Supervised translation-invariant
sparse coding,” in Proc. IEEE Conf. Comput. Vision and Pattern
Recognition (CVPR), 2010, pp. 3517–3524.

[37] B. Colson, P. Marcotte, and G. Savard, “An overview of bilevel
optimization,” Ann. Operations Research, vol. 153, no. 1, pp. 235–
256, Apr. 2007.

[38] D. M. Bradley and J. A. Bagnell, “Differentiable sparse coding,” in

Advances Neural Inform. Process. Syst. (NIPS), 2008.

[39] J. Zheng and Z. Jiang, “Learning view-invariant sparse representa-
tions for cross-view action recognition,” in Proc. IEEE Intel. Conf.
Computer Vision (ICCV), 2013, pp. 3176–3183.

[40] G. Monaci, P. Jost, P. Vandergheynst, B. Mailh´e, S. Lesage, and
IEEE Trans.

R. Gribonval,
Image Process, vol. 16, no. 9, pp. 2272–2283, Sep. 2007.

“Learning multimodal dictionaries,”

[41] Y. Zhuang, Y. Wang, F. Wu, Y. Zhang, and W. Lu, “Supervised
coupled dictionary learning with group structures for multi-modal
retrieval,” in Proc. 27th Conf. Artiﬁcial Intell., 2013, pp. 1070–1076.
[42] H. Zhang, Y. Zhang, and T. S. Huang, “Simultaneous discriminative
projection and dictionary learning for sparse representation based
classiﬁcation,” Pattern Recognition, vol. 46, no. 1, pp. 346–354,
Jan. 2013.

[43] B. A. Olshausen and D. J. Field, “Sparse coding with an overcom-
plete basis set: A strategy employed by v1?,” Vision Research, vol.
37, no. 23, pp. 3311–3325, Dec. 1997.

[44] L. Bottou and O. Bousquet, “The trade-offs of large scale learning,”
in Advances Neural Inform. Process. Syst. (NIPS), 2007, pp. 161–
168.

[45] S. Bahrampour, A. Ray, N. M. Nasrabadi, and W. K. Jenkins,
“Quality-based multimodal classiﬁcation using tree-structured spar-
sity,” in Proc. IEEE Conf. Comput. Vision and Pattern Recognition
(CVPR), 2014, pp. 4114–4121.

[46] S. F. Cotter, B. D. Rao, K. Engan, and K. Kreutz-Delgado, “Sparse
solutions to linear inverse problems with multiple measurement
vectors,” IEEE Trans. Signal Process., vol. 53, no. 7, pp. 2477–
2488, Jul. 2005.

[47] J. A. Tropp, “Algorithms for simultaneous sparse approximation. part
ii: Convex relaxation,” Signal Process., vol. 86, no. 3, pp. 589–602,
Mar. 2006.

[48] A. Rakotomamonjy, “Surveying and comparing simultaneous sparse
approximation (or group-lasso) algorithms,” Signal Process., vol.
91, no. 7, pp. 1505–1526, Jul. 2011.

[49] N. Parikh and S. Boyd, “Proximal algorithms,” Foundations and

Trends in Optimization, pp. 1–96, 2013.

[50] H. Zou and T. Hastie, “Regularization and variable selection via the
elastic net,” J. Royal Statistical Soc. Series B, vol. 67, no. 2, pp.
301–320, 2005.

[51] M. Aharon and M. Elad, “Sparse and redundant modeling of image
content using an image-signature-dictionary,” SIAM J.l on Imaging
Sciences, vol. 1, no. 3, pp. 228–247, 2008.

“Large-scale machine learning with stochastic gradi-
in Proceedings of COMPSTAT’2010, pp. 177–186.

[52] L. Bottou,

ent descent,”
Springer, 2010.

[53] L. Bottou, “Online learning and stochastic approximations,” On-line

learning in neural networks, vol. 17, pp. 9.

[54] P. Sprechmann, I. Ramirez, G. Sapiro, and Y. C. Eldar, “C-hilasso: A
collaborative hierarchical sparse modeling framework,” IEEE Trans.
Signal Process., vol. 59, no. 9, pp. 4183–4198, Sep. 2011.

[55] A. M. Martinez and R. Benavente, “The AR face database,” CVC

Technical Rep., vol. 24, 1998.

[56] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, “Multi-
pie,” Image and Vision Computing, vol. 28, no. 5, pp. 807–813,
2010.

[57] D. Weinland, E. Boyer, and R. Ronfard, “Action recognition from
in IEEE 11th Intel. Conf.

arbitrary views using 3d exemplars,”
Computer Vision (ICCV), 2007, pp. 1–7.

[58] S. S. S. Crihalmeanu, A. Ross, and L. Hornak, “A protocol for
multibiometric data acquisition, storage and dissemination,” Tech.
Rep., Lane Dept. of Comput. Sci. and Elect. Eng., West Virginia
Univ., 2007.

[59] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet, “Sim-
plemkl.,” J. of Mach. Learning Research, vol. 9, no. 11, 2008.
[60] C. M. Bishop, Pattern Recognition and Machine Learning, Springer,

2006.

[61] H. Zhang, N. M. Nasrabadi, Y. Zhang, and T. S. Huang, “Joint
dynamic sparse representation for multi-view face recognition,”
Pattern Recognition, vol. 45, pp. 1290–1298, 2012.

15

[62] D. Tran and A. Sorokin, “Human activity recognition with metric
learning,” in Computer Vision–ECCV 2008, pp. 548–561. Springer,
2008.

[63] A. Wang, H.and Kl¨aser, C. Schmid, and C.-L. Liu, “Dense trajec-
tories and motion boundary descriptors for action recognition,” Intl.
J. Computer Vision, vol. 103, no. 1, pp. 60–79, 2013.

[64] A. Gupta, A. Shafaei, J. J. Little, and R. J. Woodham, “Unlabelled 3d
motion examples improve cross-view action recognition,” in Proc.
British Machine Vision Conf., 2014.

[65] I. N. Junejo, E. Dexter, and P. Laptev, I.and Perez,

“View-
independent action recognition from temporal self-similarities,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 1, pp. 172–
185, 2011.

[66] X. Wu, D. Xu, L. Duan, and J. Luo, “Action recognition using
context and appearance distribution features,” in IEEE Conf. Comput.
Vision Pattern Recognition (CVPR), 2011, pp. 489–496.

[67] R. M. Bolle, J. H. Connell, S. Pankanti, N. K. Ratha, and A. W.
Senior, “The relation between the roc curve and the cmc,” in Proc.
4th IEEE Workshop Automat. Identiﬁcation Advanced Technologies,
2005, pp. 15–20.

[68] Julien Mairal, Rodolphe Jenatton, Guillaume Obozinski, and Francis
“Learning hierarchical and topographic dictionaries with

Bach,
structured sparsity,” in SPIE, 2011.

[69] E. V. D. Berg and M. P. Friedlander, “Theoretical and empirical
results for recovery from multiple measurements,” IEEE Trans. Inf.
Theory, vol. 56, no. 5, pp. 2516–2527, Apr. 2010.

[70] H. Zou, T. Hastie, and R. Tibshirani, “On the degrees of freedom
of the lasso,” The Ann. of Statistics, vol. 35, no. 5, pp. 2173–2192,
2007.

[71] M. Yuan and Y. Lin, “Model selection and estimation in regression
with grouped variables,” J. Royal Statistical Soc.: Series B (Statis-
tical Methodology), vol. 68, no. 1, pp. 49–67, 2006.

[72] S. Vaiter, C. Deledalle, G. Peyre, J. Fadili, and C. Dossal, “Degrees

of freedom of the group lasso,” arXiv:1205.1481, 2012.

Soheil Bahrampour received the M.Sc. degree
in electrical engineering from the University of
Tehran, Iran,
in 2009. He then received the
M.Sc. degree in Mechanical Engineering and
PhD degree in Electrical Engineering under the
supervision of A. Ray and W.K. Jenkins from
The Pennsylvania State University, University
Park, PA, in 2013 and 2015, respectively. Dr.
Bahrampour is currently a Research Scientist
with Bosch Research and Technology Center,
Palo Alto, CA. His research interests include

machine learning, data mining, signal processing, and computer vision.

16

Nasser M. Nasrabadi
(S’80-M’84-SM’92-
FM’01) received the B.Sc. (Eng.) and Ph.D.
degrees in Electrical Engineering from Imperial
College of Science and Technology (University
of London), London, England, in 1980 and 1984,
respectively. From October 1984 to December
1984 he worked for IBM (UK) as a senior
programmer. During 1985 to 1986 he worked
with Philips research laboratory in NY as a
member of technical staff. From 1986 to 1991 he
was an assistant professor in the Department of
Electrical Engineering at Worcester Polytechnic Institute, Worcester, MA.
From 1991 to 1996 he was an associate professor with the Department
of Electrical and Computer Engineering at State University of New York
at Buffalo, Buffalo, NY. Since September From 1996 to 2015 he was a
Senior Research Scientist (ST) with the US Army Research Laboratory
(ARL). Since August 2015 he has been a professor at Lane Dept. of
Computer Science and Elecrical Engineering. Dr. Nasrabadi has served
as an associate editor for the IEEE Transactions on Image Processing,
the IEEE Transactions on Circuits, Systems and Video Technology, and
the IEEE Transactions on Neural Networks. His current research interests
are in image processing, computer vision, biometrics, statistical machine
learning theory, sparsity, robotics, and neural networks applications to
image processing. He is also a Fellow of ARL, SPIE and IEEE.

Asok Ray (SM’83F’02) received the Ph.D. de-
gree in Mechanical Engineering from Northeast-
ern University, Boston, MA, and the graduate de-
grees in the disciplines of Electrical Engineering,
Mathematics, and Computer Science. He joined
The Pennsylvania State University (Penn State),
University Park, PA, in July 1985, and is cur-
rently a Distinguished Professor of Mechanical
Engineering and Mathematics, a Graduate Fac-
ulty of Electrical Engineering, and a Graduate
Faculty of Nuclear Engineering. Prior to joining
Penn State, he held research and academic positions with Massachusetts
Institute of Technology, Cambridge, MA, and Carnegie-Mellon University,
Pittsburgh, PA, as well as management and research positions with GTE
Strategic Systems Division, Westborough, MA, Charles Stark Draper
Laboratory, Cambridge, MA, and MITRE Corporation, Bedford, MA. Dr.
Ray has authored or coauthored over 550 research publications, including
over 285 scholarly articles in refereed journals and research monographs.
Dr. Ray is also a Fellow of the American Society of Mechanical Engineers
(ASME) and a Fellow of World Innovative Foundation (WIF). Dr. Ray had
been a Senior Research Fellow at NASA Glenn Research Center under a
National Academy of Sciences award.

W. Kenneth Jenkins received the B.S.E.E. de-
gree from Lehigh University and the M.S.E.E.
and Ph.D. degrees from Purdue University. From
1974 to 1977 he was a Research Scientist Asso-
ciate in the Communication Sciences Laboratory
at the Lockheed Research Laboratory, Palo Alto,
CA. In 1977 he joined the University of Illinois
at Urbana-Champaign where he was a faculty
member in Electrical and Computer Engineering
from 1977 until 1999. From 1986-1999 Dr. Jenk-
ins was the Director of the Coordinated Science
Laboratory. From 1999 through 2011 he served Professor and Head of
Electrical Engineering at Penn State University, and in 2011 he returned
to the rank of Professor of Electrical Engineering. Dr. Jenkins current
research interests include fault
tolerant DSP for highly scaled VLSI
systems, adaptive signal processing, multidimensional array processing,
computer imaging, bio-inspired optimization algorithms for intelligent
signal processing, and fault tolerant digital signal processing. He co-
authored the book Advanced Concepts in Adaptive Signal Processing,
published by Kluwer in 1996. He is a past Associate Editor for the
IEEE Transaction on Circuits and Systems, and a past President (1985) of
the CAS Society. He served as General Chairman of the 1988 Midwest
Symposium on Circuits and Systems and as the General Chairman of
the Thirty Second Annual Asilomar Conference on Signals and Systems.
From 2002 to 2007 he served on the Board of Directors of the Electrical
and Computer Engineering Department Heads Association (ECEDHA)
and as President of ECEDHA in 2005. Since January 2011 he has been
serving as a Member of the IEEE-HKN Board of Governors. Dr. Jenkins
is a Life Fellow of the IEEE and a recipient of the 1990 Distinguished
Service Award of the IEEE Circuits and Systems Society. In 2000 he
received a Golden Jubilee Medal from the IEEE Circuits and Systems
Society and a 2000 Millennium Award from the IEEE. In 2000 was named
a co-winner of the 2000 International Award of theGeorge Monteﬁore
Foundation (Belgium) for outstanding career contributions to the ﬁeld of
electrical engineering and electrical science, in 2002 he was awarded the
Shaler Area High School Distinguished Alumnus Award, in 2007 he was
honored with an IEEE Midwest Symposium on Circuits and Systems 50th
Anniversary Award, and in 2013 he received the ECEDHA Robert M.
Janowiak Outstanding Leadership and Service Award.

Multimodal Task-Driven Dictionary Learning
for Image Classiﬁcation

Soheil Bahrampour, Member, IEEE, Nasser M. Nasrabadi, Fellow, IEEE, Asok Ray, Fellow, IEEE,
and W. Kenneth Jenkins, Life Fellow, IEEE

1

5
1
0
2
 
t
c
O
 
7
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
9
0
1
0
.
2
0
5
1
:
v
i
X
r
a

Abstract—Dictionary learning algorithms have been suc-
cessfully used for both reconstructive and discriminative tasks,
where an input signal is represented with a sparse linear
combination of dictionary atoms. While these methods are
mostly developed for single-modality scenarios, recent studies
have demonstrated the advantages of feature-level fusion
based on the joint sparse representation of the multimodal
inputs. In this paper, we propose a multimodal task-driven
dictionary learning algorithm under the joint sparsity con-
straint (prior) to enforce collaborations among multiple ho-
mogeneous/heterogeneous sources of information. In this task-
driven formulation, the multimodal dictionaries are learned
simultaneously with their corresponding classiﬁers. The re-
sulting multimodal dictionaries can generate discriminative
latent features (sparse codes) from the data that are optimized
for a given task such as binary or multiclass classiﬁca-
tion. Moreover, we present an extension of the proposed
formulation using a mixed joint and independent sparsity
prior which facilitates more ﬂexible fusion of the modalities
at feature level. The efﬁcacy of the proposed algorithms
for multimodal classiﬁcation is illustrated on four different
applications – multimodal face recognition, multi-view face
recognition, multi-view action recognition, and multimodal
biometric recognition. It is also shown that, compared to
the counterpart reconstructive-based dictionary learning algo-
rithms, the task-driven formulations are more computationally
efﬁcient in the sense that they can be equipped with more
compact dictionaries and still achieve superior performance.

Index Terms—Dictionary learning, Multimodal classiﬁca-

tion, Sparse representation, Feature fusion

I. INTRODUCTION
It is well established that information fusion using multi-
ple sensors can generally result in an improved recognition
performance [1]. It provides a framework to combine
local information from different perspectives which is more
tolerant to the errors of individual sources [2], [3]. Fusion
methods for classiﬁcation are generally categorized into
feature fusion [4] and classiﬁer fusion [5] algorithms.
Feature fusion methods aggregate extracted features from
different sources into a single feature set which is then used
for classiﬁcation. On the other hand, classiﬁer fusions algo-
rithms combine decisions from individual classiﬁers, each

When this work was achieved, S. Bahrampour, A. Ray, and W. K.
Jenkins were with the Department of Electrical Engineering, Pennsylvania
State University, University Park, PA 16802, USA; N. M. Nasrabadi was
with the Army Research Laboratory, Adelphi, MD 20783. S. Bahrampour
is now with Bosch Research and Technology Center, Palo Alto, CA; N. M.
Nasrabadi is now with the Computer Science and Electrical Engineering
Department at the West Virginia University, WV.

soheil.bahrampour@us.bosch.com, nassernasrabadi@mail.wvu.edu,

{axr2, wkj1}@psu.edu.

of which is trained using separate sources. While classiﬁer
fusion is a well-studied topic, fewer studies have been done
for feature fusion, mainly due to the incompatibility of the
feature sets [6]. A naive way of feature fusion is to stack
the features into a longer one [7]. However this approach
usually suffers from the curse of dimensionality due to the
limited number of training samples [4]. Even in scenarios
with abundant training samples, concatenation of feature
vectors does not take into account the relationship among
the different sources and it may contain noisy or redundant
data, which degrade the performance of the classiﬁer [6].
However, if these limitations are mitigated, feature fusion
can potentially result
in improved classiﬁcation perfor-
mance [8], [9].

Sparse representation classiﬁcation has recently attracted
the interest of many researchers in which the input sig-
nal is approximated with a linear combination of a few
dictionary atoms [10] and has been successfully applied
to several problems such as robust face recognition [10],
visual tracking [11], and transient acoustic signal classi-
ﬁcation [12]. In this approach, a structured dictionary is
usually constructed by stacking all the training samples
from the different classes. The method has also been
expanded for efﬁcient feature-level fusion which is usually
referred to as multi-task learning [13], [14], [15], [16].
Among different proposed sparsity constraints (priors), joint
sparse representation has shown signiﬁcant performance
improvement
in several multi-task learning applications
such as target classiﬁcation, biometric recognitions, and
multiview face recognition [12], [14], [17], [18]. The un-
derlying assumption is that the multimodal test input can
be simultaneously represented by a few dictionary atoms,
or training samples, from a multimodal dictionary, that
represents all the modalities and, therefore, the resulting
sparse coefﬁcients should have the same sparsity pattern.
However, the dictionary constructed by the collection of
the training samples suffer from two limitations. First, as
the resulting
the number of training samples increases,
optimization problem becomes more computationally de-
manding. Second, the dictionary that is constructed this way
is not optimal neither for the reconstructive tasks [19] nor
the discriminative tasks [20].

Recently it has been shown that learning the dictionary
can overcome the above limitations and signiﬁcantly im-
prove the performance in several applications including
image restoration [21], face recognition [22] and object
recognition [23], [24]. The learned dictionaries are usu-

2

ally more compact and have fewer dictionary atoms than
the number of training samples [25], [26]. Dictionary
learning algorithms can generally be categorized into two
groups: unsupervised and supervised. Unsupervised dictio-
nary learning algorithms such as the method of optimal
direction [27] and K-SVD [25] are aimed at ﬁnding a
dictionary that yields minimum errors when adapted to
reconstruction tasks such as signal denoising [28] and im-
age inpainting [19]. Although, the unsupervised dictionary
learning has also been used for classiﬁcation [22], it has
been shown that better performance can be achieved by
learning the dictionaries that are adapted to an speciﬁc
task rather than just the data set [29], [30]. These methods
are called supervised, or task-driven, dictionary learning
algorithms. For the classiﬁcation task, for example, it is
more meaningful to utilize the labeled data to minimize
the misclassiﬁcation error rather than the reconstruction
error [31]. Adding a discriminative term to the recon-
struction error and minimizing a trade-off between them
has been proposed in several formulations [20], [24],
[32], [33]. The incoherent dictionary learning algorithm
proposed in [34] is another supervised formulation which
trains class-speciﬁc dictionaries to minimize atom sharing
between different classes and uses sparse representation
for classiﬁcation. In [35], a Fisher criterion is proposed to
learn structured dictionaries such that the sparse coefﬁcients
have small within-class and large between-class scatters.
While unsupervised dictionary learning can be reformulated
as a large scale matrix factorization problem and solved
efﬁciently [19], supervised dictionary learning is usually
more difﬁcult to optimize. More recently, it has been shown
that better optimization tool can be used to tackle the
supervised dictionary learning [30], [36]. This is achieved
by formulating it as a bilevel optimization problem [37],
[38]. In particular, a stochastic gradient descent algorithm
has been proposed in [29] which efﬁciently solves the
dictionary learning problem in a uniﬁed framework for
different tasks, such as classiﬁcation, nonlinear image map-
ping, and compressive sensing.

The majority of the existing dictionary learning algo-
rithms, including the task-driven dictionary learning [29],
are only applicable to single source of data. In [39], a set
of view-speciﬁc dictionaries and a common dictionary are
learned for the application of multi-view action recogni-
tion. The view-speciﬁc dictionaries are trained to exploit
view-level correspondence while the common dictionary is
trained to capture common patterns shared among the dif-
ferent views. The proposed formulation belongs to the class
of dictionary learning algorithms that leverages the labeled
samples to learn class-speciﬁc atoms while minimizing
the reconstruction error. Moreover, it cannot be used for
fusion of the heterogeneous modalities. In [40], a generative
multimodal dictionary learning algorithm is proposed to
extract typical templates of multimodal features. The tem-
plates represent synchronous transient structures between
modalities which can be used for localization applications.
More recently, a multimodal dictionary learning algorithm
with joint sparsity prior is proposed in [41] for multimodal

Fig. 1: Multimodal task-driven dictionary learning scheme.

retrieval where the task is to ﬁnd relevant samples from
other modalities for a given unimodal query. However,
the proposed formulation cannot be readily applied for
information fusion in which the task is to ﬁnd label of a
given multimodal query. Moreover, the joint sparsity prior
is used in [41] to couple similarly labeled samples within
each modality and is not utilized to extract cross-modality
information which is essential for information fusion [12].
Furthermore,
the dictionaries in [41] are learned to be
generative by minimizing the reconstruction error of data
across modalities and, therefore, are not necessary optimal
for discriminative tasks [31].

This paper focuses on learning discriminative multimodal
dictionaries. The major contributions of the paper are as
follows:

• Formulation of

the multimodal dictionary learning
algorithms: A multimodal task-driven dictionary learn-
ing algorithm is proposed for classiﬁcation using ho-
mogeneous or heterogeneous sources of information.
Information from different modalities are fused both
at the feature level, by using the joint sparse repre-
sentation, and at the decision level, by combining the
scores of the modal-based classiﬁers. The proposed
formulation simultaneously trains the multimodal dic-
tionaries and classiﬁers under the joint sparsity prior in
order to enforce collaborations among the modalities
and obtain the latent sparse codes as the optimized
features for different tasks such as binary and mul-
ticlass classiﬁcation. Fig. 1 presents an overview of
the proposed framework. An unsupervised multimodal
dictionary learning algorithm is also presented as a by-
product of the supervised version.

• Differentiability of the bi-level optimization problem:
The main difﬁculty in proposing such a formulation
is that the solution of the corresponding joint sparse
coding problem is not differentiable with respect to
the dictionaries. While the joint sparse coding has
a non-smooth cost function,
is shown here that
it is locally differentiable and the resulting bi-level
optimization for task-driven multimodal dictionary
learning is smooth and can be solved using a stochastic

it

gradient descent algorithm. 1

• Flexible feature-level

fusion: An extension of the
proposed framework is presented which facilitates
more ﬂexible fusion of the modalities at the feature
level by allowing the modalities to have different
sparsity patterns. This extension provides a frame-
work to tune the trade-off between independent sparse
representation and joint sparse representation among
the modalities. Improved performance for multimodal
classiﬁcation: The proposed methods achieve the state-
of-the-art performance in a range of different multi-
modal classiﬁcation tasks.
In particular, we have
provided extensive performance comparison between
the proposed algorithms and some of the competing
tasks of
methods from literature for four different
multimodal face recognition, multi-view face recog-
nition, multimodal biometric recognition, and multi-
view action recognition. The experimental results on
these datasets have demonstrated the usefulness of
the proposed formulation, showing that the proposed
algorithm can be readily applied to several different
application domains.

• Improved efﬁciency for sparse-representation based
classiﬁcation: It is shown here that, compared to the
counterpart sparse representation classiﬁcation algo-
rithms, the proposed algorithms are more computa-
tionally efﬁcient in the sense that they can be equipped
with more compact dictionaries and still achieve su-
perior performance.

A. Paper organization

The rest of the paper is organized as follows. In Sec-
tion II, unsupervised and supervised dictionary learning
algorithms for single source of information are reviewed.
Joint sparse representation for multimodal classiﬁcation is
also reviewed in this section. Section III proposes the task-
driven multimodal dictionary learning algorithms. Compar-
ative studies on several benchmarks and concluding results
are presented in Section IV and Section V, respectively.

B. Notation

Vectors are denoted by bold lower case letters and
matrices by bold upper case letters. For a given vector x,
xi is its ith element. For a given ﬁnite set of indices γ,
xγ is the vector formed with those elements of x indexed
in γ. Symbol → is used to distinguish the row vectors
from column vectors, i.e. for a given matrix X, the ith row
and jth column of matrix are represented as xi→ and xj,
respectively. For a given ﬁnite set of indices γ, Xγ is the
matrix formed with those columns of X indexed in γ and
Xγ→ is the matrix formed with those rows of X indexed
in γ. Similarly, for given ﬁnite sets of indices γ and ψ,
Xγ→,ψ is the matrix formed with those rows and columns
of X indexed in γ and ψ, respectively. xij is the element

1The source code of the proposed algorithm is released here: https:

//github.com/soheilb/multimodal dictionary learning

3

of X at row i and column j. The lq norm, q ≥ 1, of a
vector x ∈ Rm is deﬁned as (cid:107)x(cid:107)(cid:96)q = ((cid:80)m
j=1 |xj|q)1/q.
The Frobenius norm and (cid:96)1q norm, q ≥ 1, of matrix
(cid:17)1/2
X ∈ Rm×n is deﬁned as (cid:107)X(cid:107)F =
and (cid:107)X(cid:107)(cid:96)1q = (cid:80)m
{xi|i ∈ γ} is shortly denoted as {xi}.

i=1 (cid:107)xi→(cid:107)(cid:96)q , respectively. The collection

(cid:16)(cid:80)m
i=1

j=1 x2
ij

(cid:80)n

II. BACKGROUND

A. Dictionary learning

Dictionary learning has been widely used in various tasks
such as reconstruction, classiﬁcation, and compressive sens-
ing [29], [33], [42], [43]. In contrast to principal component
analysis (PCA) and its variants, dictionary learning algo-
rithms generally do not impose orthogonality condition and
are more ﬂexible allowing to be well-tuned to the training
data. Let X = [x1, x2, . . . , xN ] ∈ Rn×N be the collection
of N (normalized) training samples that are assumed to be
statistically independent. Dictionary D ∈ Rn×d can then
be obtained as the minimizer of the following empirical
cost [22]:

gN (D) (cid:44) 1
N

N
(cid:88)

i=1

lu (xi, D)

(1)

the

regularizing convex set D (cid:44) {D ∈
over
Rn×d|(cid:107)dk(cid:107)(cid:96)2 ≤ 1, ∀k = 1, . . . , d}, where dk is the kth
column, or atom, in the dictionary and the unsupervised
loss lu is deﬁned as
lu (x, D) (cid:44) min
α∈Rd

+λ1(cid:107)α(cid:107)(cid:96)1 +λ2(cid:107)α(cid:107)2
(cid:96)2

(cid:107)x−Dα(cid:107)2
(cid:96)2

, (2)

which is the optimal value of the sparse coding problem
with λ1 and λ2 being the regularizing parameters. While
λ2 is usually set to zero to exploit sparsity, using λ2 > 0
makes the optimization problem in Eq. (2) strongly convex
resulting in a differentiable cost function [29]. The index u
of lu is used to emphasize that the above dictionary learning
formulation is an unsupervised method. It is well-known
that one is often interested in minimizing an expected
risk, rather than the perfect minimization of the empirical
cost [44]. An efﬁcient online algorithm is proposed in [19]
to ﬁnd the dictionary D as the minimizer of the following
stochastic cost over the convex set D:

g (D) (cid:44) Ex [lu (x, D)] ,

(3)

where it is assumed that the data x is drawn from a ﬁnite
probability distribution p(x) which is usually unknown
and Ex [.] is the expectation operator with respect to the
distribution p(x).

The trained dictionary can then be used to (sparsely)
reconstruct the input. The reconstruction error has been
shown to be a robust measure for classiﬁcation tasks [10],
[45]. Another use of a given trained dictionary is for feature
extraction where the sparse code α(cid:63)(x, D), obtained as a
solution of (2), is used as a feature vector representing the
input signal x in the classical expected risk optimization
for training a classiﬁer [29]:

min
w∈W

Ey,x [l (y, w, α(cid:63)(x, D))] +

ν
2

(cid:107)w(cid:107)2
(cid:96)2

,

(4)

4

where y is the ground truth class label associated with
the input x, w is model (classiﬁer) parameters, ν is a
regularizing parameter, and l is a convex loss function that
measures how well one can predict y given the feature
vector α(cid:63) and classiﬁer parameters w. The expectation
Ey,x is taken with respect to the probability distribution
p(y, x) of the labeled data. Note that in Eq. 4, the dictionary
D is ﬁxed and independent of the given task and class label
y. In task-driven dictionary learning, on the other hand,
a supervised formulation is used which ﬁnds the optimal
dictionary and classiﬁer parameters jointly by solving the
following optimization problem [29]:

min
D∈D,w∈W

Ey,x [lsu (y, w, α(cid:63)(x, D))] +

(cid:107)w(cid:107)2
(cid:96)2

.

(5)

ν
2

The index su of convex loss function lsu is used to
emphasize that the above dictionary learning formulation
is supervised. The learned task-driven dictionary has been
shown to result in a superior performance compared to the
unsupervised setting [29]. In this setting, the sparse codes
are indeed the optimized latent features for the classiﬁer.

B. Multimodal joint sparse representation

Joint sparse representation provides an efﬁcient tool for
feature-level fusion of sources of information [12], [14],
[46]. Let S (cid:44) {1, . . . , S} be a ﬁnite set of available
modalities and let xs ∈ Rns
, s ∈ S, be the feature
vector for the sth modality. Also let Ds ∈ Rns×d be
the corresponding dictionary for the sth modality. For
now, it is assumed that the multimodal dictionaries are
constructed by collections of the training samples from
i.e. jth atom of dictionary Ds is
different modalities,
the jth training sample from the sth modality. Given a
multimodal input {xs|s ∈ S}, shortly denoted as {xs}, an
optimal sparse matrix A(cid:63) ∈ Rd×S is obtained by solving
the following (cid:96)12-regularized reconstruction problem:

argmin
A=[α1...αS ]

1
2

S
(cid:88)

s=1

(cid:107)xs − Dsαs(cid:107)2
(cid:96)2

+ λ(cid:107)A(cid:107)(cid:96)12 ,

(6)

where λ is a regularization parameter. Here αs is the sth-
column of A which corresponds to the sparse representa-
tion for the sth modality. Different algorithms have been
proposed to solve the above optimization problem [47],
[48]. We use the efﬁcient alternating direction method
of multipliers (ADMM) [49] to ﬁnd A(cid:63). The (cid:96)12 prior
encourages row sparsity in A(cid:63), i.e. it encourages collab-
oration among all the modalities by enforcing the same
dictionary atoms from different modalities that present the
same event, to be used for reconstructing the inputs {xs}.
An (cid:96)11 term can also be added to the above cost function
to extend it to a more general framework where sparsity
can also be sought within the rows, as will be discussed
in Section III-D. It has been shown that
joint sparse
representation can result in a superior performance in fusing
multimodal sources of information compared to other infor-
mation fusion techniques [45]. We are interested in learning
multimodal dictionaries under the joint sparsity prior. This

has several advantages over a ﬁxed dictionary consisting of
training data. Most importantly, it can potentially remove
the redundant and noisy information by representing the
training data in a more compact form. Also using the
supervised formulation, one expects to ﬁnd dictionaries that
are well-adapted to the discriminative tasks.

III. MULTIMODAL DICTIONARY LEARNING

In this section, online algorithms for unsupervised and

supervised multimodal dictionary learning are proposed.

A. Multimodal unsupervised dictionary learning

Unsupervised multimodal dictionary learning is derived
by extending the optimization problem characterized in
Eq. (3) and using the joint sparse representation of (6) to
enforce collaborations among modalities. Let the minimum
u ({xs, Ds}) of the joint sparse coding be deﬁned as
cost l(cid:48)

min
A

1
2

S
(cid:88)

s=1

(cid:107)xs − Dsαs(cid:107)2
(cid:96)2

+ λ1(cid:107)A(cid:107)(cid:96)12 +

(cid:107)A(cid:107)2

F , (7)

λ2
2

where λ1 and λ2 are the regularizing parameters. The addi-
tional Frobenius norm (cid:107).(cid:107)F compared to Eq. (6) guarantees
a unique solution for the joint sparse optimization problem.
In the special case when S = 1, optimization (7) reduces
to the well-studied elastic-net optimization [50]. By natural
extension of the optimization problem (3), the unsupervised
multimodal dictionaries are obtained by:

Ds(cid:63) = argmin
Ds∈Ds

Exs [l(cid:48)

u ({xs, Ds})] , ∀s ∈ S,

(8)

where the convex set Ds is deﬁned as

Ds (cid:44) {D ∈ Rns×d|(cid:107)dk(cid:107)(cid:96)2 ≤ 1, ∀k = 1, . . . , d}.

(9)

It is assumed that data xs is drawn from a ﬁnite (un-
known) probability distribution p(xs). The above optimiza-
tion problem can be solved using the classical projected
stochastic gradient algorithm [51] which consists of a
sequence of updates as follows:

Ds ← ΠDs [Ds − ρt∇Ds l(cid:48)

u ({xs

t , Ds})] ,

(10)

where ρt is the gradient step at time t and ΠD is the
orthogonal projector onto set D. The algorithm converges
to a stationary point for a decreasing sequence of ρt [51],
[52]. A typical choice of ρt is shown in the next section.
This problem can also be solved using online matrix
factorization algorithm [26]. It should be noted that the
while the stochastic gradient descent does converge, it is
not guaranteed to converge to a global minimum due to
the non-convexity of the optimization problem [26], [44].
However, such stationary point is empirically found to be
sufﬁciently good for practical applications [21], [28].

B. Multimodal task-driven dictionary learning

As discussed in Section II, the unsupervised setting does
not take into account the label of the training data, and
the dictionaries are obtained by minimizing the reconstruc-
tion error. However, for classiﬁcation tasks, the minimum
reconstruction error does not necessarily result in discrimi-
native dictionaries. In this section, a multimodal task-driven
dictionary learning algorithm is proposed that enforces
collaboration among the modalities both at the feature level
using joint sparse representation and the decision level
using a sum of the decision scores. We propose to learn
the dictionaries Ds(cid:63), ∀s ∈ S, and the classiﬁer parameters
ws(cid:63), ∀s ∈ S, shortly denoted as the set {Ds(cid:63), ws(cid:63)}, jointly
as the solution of the following optimization problem:

min
{Ds∈Ds,ws∈W s}

f ({Ds, ws}) +

(cid:107)ws(cid:107)2
(cid:96)2

,

(11)

ν
2

S
(cid:88)

s=1

where f is deﬁned as the expected cumulative cost:

f ({Ds, ws}) = E

lsu(y, ws, αs(cid:63)),

(12)

S
(cid:88)

s=1

is

the

sth

column of

where αs(cid:63)
the minimizer
A(cid:63)({xs, Ds}) of
the optimization problem (7) and
lsu(y, w, α) is a convex loss function that measures how
well the classiﬁer parametrized by w can predict y by
observing α. The expectation is taken with respect to the
joint probability distribution of the multimodal inputs {xs}
and label y. Note that αs(cid:63) acts as a hidden/latent feature
vector, corresponding to the input xs, which is generated by
the learned discriminative dictionary Ds(cid:63). In general, lsu
can be chosen as any convex function such that lsu(y, ., .)
is twice continuously differentiable for all possible values
of y. A few examples are given below for binary and
multiclass classiﬁcation tasks.

1) Binary classiﬁcation: In a binary classiﬁcation task
where the label y belongs to the set {−1, 1}, lsu can be
naturally chosen as the logistic regression loss

lsu(y, w, α(cid:63)) = log(1 + e−ywT α(cid:63)

),

(13)

where w ∈ Rd is the classiﬁer parameters. Once the
optimal {Ds, ws} are obtained, a new multimodal sample
{xs} is classiﬁed according to sign of (cid:80)S
s=1 wsT α(cid:63) due
to the uniform monotonicity of (cid:80)S
s=1 lsu. For simplicity,
the intercept term for the linear model is omitted here,
but it can be easily added. One can also use a bilinear
model where, instead of a set of vectors {ws}, a set of
matrices {W s} are learned and a new multimodal sample
is classiﬁed according to the sign of (cid:80)S
s=1 xsT W sα(cid:63).
Accordingly, the (cid:96)2-norm regularization of Eq. (11) needs
to be replaced with the matrix Frobenius norm. The bilinear
model is richer than the linear model and can sometimes
result in better classiﬁcation performance but needs more
careful training to avoid over-ﬁtting.

5

2) Multiclass classiﬁcation: Multiclass classiﬁcation can
be formulated using a collections of (independently learned)
binary classiﬁers in a one-vs-one or one-vs-all setting.
Multiclass classiﬁcation can also be handled in an all-vs-all
setting using the softmax regression loss function. In this
scheme, the label y belongs to the set {1, . . . , K} and the
softmax regression loss is deﬁned as

lsu(y, W , α(cid:63)) = −

1{y=k} log

K
(cid:88)

k=1

(cid:32)

ewT
k α(cid:63)
l=1 ewT

(cid:80)K

l α(cid:63)

(cid:33)

,

(14)
where W = [w1 . . . wK] ∈ Rd×K, and 1{.} is the indicator
function. Once the optimal {Ds, W s} are obtained, a new
multimodal sample {xs} is classiﬁed as

argmaxk∈{1,...,K}

(cid:32)

S
(cid:88)

s=1

T αs(cid:63)

k

ews
l=1 ews

l

(cid:80)K

T αs(cid:63)

(cid:33)

.

(15)

In yet another all-vs-all setting, the multiclass classiﬁcation
task can be turned into a regression task in which the scaler
label y is changed to a binary vector y ∈ RK, where the
kth coordinate corresponding to the label of {xs} is set to
one and the rest of the coordinates are set to zero. In this
setting, lsu is deﬁned as

lsu(y, W , α(cid:63)) =

1
2
where W ∈ RK×d. Having obtained the optimal
{Ds, W s}, the test sample {xs} is then classiﬁed as

(cid:107)y − W α(cid:63)(cid:107)2
(cid:96)2

(16)

,

S
(cid:88)

argmink∈{1,...,K}

(cid:107)qk − W sαs(cid:63)(cid:107)2
(cid:96)2

,

(17)

s=1
where qk is a binary vector in which its kth coordinate is
one and its remaining coordinates are zero.

In choosing between the one-vs-all setting, in which
independent multimodal dictionaries are trained for each
class, and the multiclass formulation, in which multimodal
dictionaries are shared between classes, a few points should
be considered. In the one-vs-all setting, the total number of
dictionary atoms is equal to dSK in the K-class classiﬁ-
cation while in the multiclass setting the number is equal
to dS. It should be noted that in the multiclass setting a
larger dictionary is generally required to achieve the same
level of performance to capture the variations among all
classes. However, it is generally observed that the size
of the dictionaries in multiclass setting is not required to
grow linearly as the number of classes increases due to
atom sharing among the different classes. Another point to
consider is that the class-speciﬁc dictionaries of the one-
vs-all approach are independent and can be obtained in
parallel. In this paper, the multiclass formulation is used
to allow feature sharing among the classes.

C. Optimization

The main challenge in optimizing (11) is the non-
differentiability of A(cid:63)({xs, Ds}). However,
it can be
shown that although the sparse coefﬁcients A(cid:63) are obtained
by solving a non-differentiable optimization problem, the

6

function f ({Ds, ws}), deﬁned in Eq. (12), is differen-
tiable on D1 × · · · DS × W 1 × · · · W S, and therefore its
gradients are computable. To ﬁnd the gradient of f with
respect to Ds, one can ﬁnd the optimality condition of the
optimization (7) or use the ﬁxed point differentiation [36],
[38] and show that A(cid:63) is differentiable over its non-zero
label
rows. Without
y admits a ﬁnite set of values such as those deﬁned in
Eqs. (13) and (14). The same algorithm can be derived for
the scenario when y belongs to a compact subset of a ﬁnite-
dimensional real vector space as in Eq. (16). A couple of
mild assumptions are required to prove the differentiability
of f which are direct generalizations of those required for
the single modal scenario [29] and are listed below:

loss of generality, we assume that

Assumption (A). The multimodal data (y, {xs}) admit a

probability density p with compact support.

Assumption (B). For all possible values of y, p(y, .)
is continuous and lsu(y, .) is twice continuously differen-
tiable.

The ﬁrst assumption is reasonable when dealing with
the signal/image processing applications where the acquired
values obtained by the sensors are bounded. Also all the
given examples for lsu in the previous section satisfy the
second assumption. Before stating the main proposition of
this paper below, the term active set is deﬁned.

Deﬁnition 3.1 (Active set): The active set Λ of the solu-
tion A(cid:63) of the joint sparse coding problem (7) is deﬁned
to be

Λ = {j ∈ {1, . . . , d} : (cid:107)a(cid:63)

j→(cid:107)(cid:96)2 (cid:54)= 0},

(18)

where a(cid:63)

j→ is the jth row of A(cid:63).

Proposition 3.1 (Differentiability and gradients of f ):
Let λ2 > 0 and the assumptions (A) and (B) hold. Let
Υ = ∪j∈ΛΥj where Υj = {j, j + d, . . . , j + (S − 1)d}.
Let the matrix ˆD ∈ Rn×|Υ| be deﬁned as
ˆD =

(cid:104) ˆD1 . . . ˆD|Λ|

(19)

(cid:105)

,

where ˆDj = blkdiag(d1
j ) ∈ Rn×S, ∀j ∈ Λ, is
j , . . . , dS
the collection of the jth active atoms of the multimodal
j is the jth active atom of Ds, blkdiag is
dictionaries, ds
the block diagonalization operator, and n = (cid:80)
s∈S ns. Also
let matrix ∆ ∈ R|Υ|×|Υ| be deﬁned as

∆ = blkdiag(∆1, . . . , ∆|Λ|),

(20)

j→ ∈
I −
where ∆j =
RS×S, ∀j ∈ Λ, and I is the identity matrix. Then, the
function f deﬁned in Eq. (12) is differentiable and ∀s ∈ S,

(cid:107)a(cid:63)

(cid:107)a(cid:63)

j→

1
j→(cid:107)(cid:96)2

1
j→(cid:107)(cid:96)2

3 a(cid:63)

T a(cid:63)

∇wsf = E [∇ws lsu (y, ws, αs(cid:63))] ,
∇Dsf = E

(xs − Dsαs(cid:63)) βT

(cid:104)

˜s − Dsβ˜sαs(cid:63)T (cid:105)

,

(21)

where ˜s = {s, s + S, . . . , s + (d − 1)S} and β ∈ RdS is
deﬁned as

βΥc = 0, βΥ = ( ˆDT ˆD + λ1∆ + λ2I)−1g,

(22)
T (cid:80)S
s=1 lsu(y, ws, αs(cid:63))), Υc =
in which g = vec(∇A(cid:63)
{1, . . . , dS} \Υ, βΥ ∈ R|Υ| is formed of those rows of β
indexed by Υ, and vec(.) is the vectorization operator.

Λ→

The proof of this proposition is given in the Appendix.
A stochastic gradient descent algorithm to ﬁnd the optimal
dictionaries {Ds(cid:63)} and classiﬁers {ws(cid:63)} is described in
Algorithm 1. The stochastic gradient descent algorithm is
guaranteed to converge under a few assumptions that are
mildly stricter than those in this paper (requires three-times
differentiability) [53]. To further improve the convergence
of the proposed stochastic gradient descent algorithm, a
classic mini-batch strategy is used in which a small batch
of the training data are sampled in each batch, instead of 1
sample, and the parameters are updated using the averaged
updates of the batch. This has additional advantage in which
ˆDT ˆD and the corresponding factorization of the ADMM
for solving the sparse coding problem can be computed
once for the whole batch. For the special case when S = 1,
the proposed algorithm reduces to the single-modal task-
driven dictionary learning algorithm in [29]. Selecting λ2 in
Eq. (7) to be strictly positive guarantees the linear equations
of (22) to have a unique solution. In other words, it is easy
to show that the matrix ( ˆDT ˆD + λ1∆ + λ2I) is positive
deﬁnite given λ1 ≥ 0, λ2 > 0. However, in practice it is
observed that the solution of the joint sparse representation
problem is numerically stable since ˆD becomes full-column
rank when sparsity is sought with a sufﬁciently large λ1,
and λ2 can be set to zero. It should be noted that the
assumption of ˆD being a full column rank matrix is a
common assumption in sparse linear regression [26]. As
in any non-convex optimization algorithm, if the algorithm
is not initialized properly, it may yield poor performance.
Similar to [29], the dictionaries {Ds} are initialized by the
solution of the unsupervised multimodal dictionary learning
algorithm. Upon assignment of the initial dictionaries,
parameters {ws} of the classiﬁers are set by solving (11)
only with respect to {ws} which is a convex optimization
problem.

D. Extension

We now present an extension of the proposed algorithm
with a more ﬂexible structure on the sparse codes. Joint
sparse representation relies on the fact that all the modalities
share the same sparsity pattern in which, if a multimodal
training sample is selected to reconstruct the input, then
all the modalities within that training sample are active.
However, this group sparsity constraint, imposed by the
(cid:96)12 norm, may be too stringent for some applications [45],
[54], for example in the scenarios where the modalities
have different noise levels or when the heterogeneity of
the modalities imposes different sparsity levels for the
reconstruction task. A natural relaxation to the joint sparsity
prior is to let the multimodal inputs not share the full
active set which can be achieved by replacing the (cid:96)12 norm
with a combination of the (cid:96)12 and (cid:96)11 norms ((cid:96)12 − (cid:96)11
norm). Following the same formulation as in Section III-B,
let A(cid:63)({xs, Ds}) in Eq. (11) be the minimizer of the

Algorithm 1 Stochastic gradient descent algorithm for multi-
modal task-driven dictionary learning.

Input: Regularization parameters λ1, λ2, ν,

learning rate parameters
ρ, t0, number of iterations T , initial dictionaries {Ds ∈ Ds}s∈S ,
initial model parameters {ws ∈ W s}s∈S .

Output: Learned {Ds, ws}
1: for t = 1, . . . , T do
2:
3:

t , . . . , xS
Draw a random sample (x1
t , yt) from the training data.
Find solution A(cid:63) = (cid:2)α(cid:63)1 . . . α(cid:63)S (cid:3) ∈ Rd×S of the joint sparse
coding problem

argmin
A=[α1...αS]

1
2

S
(cid:88)

s=1

(cid:107)xs

t − Dsαs(cid:107)2
(cid:96)2

+ λ1(cid:107)A(cid:107)(cid:96)12 +

(cid:107)A(cid:107)2
F .

λ2
2

4:
5:
6:
7:

8:
9:

Compute set of active rows Λ of A(cid:63) using (18).
Compute ˆD ∈ Rn×|Υ| using (19).
Compute ∆ ∈ R|Υ|×|Υ| using (20).
Compute β ∈ RdS as:

βΥc = 0, βΥ = ( ˆDT ˆD + λ1∆ + λ2I)−1g,

T

Λ→

(cid:80)S

where Υ = ∪j∈Λ{j, j + d, . . . , j + (S − 1)d} and g =
s=1 lsu(yt, ws, αs(cid:63))).
vec(∇A(cid:63)
Choose the learning rate ρt ← min(ρ, ρ t0
Update the parameters by a projected gradient step:
ws ← ΠW s [ws − ρt (∇ws lsu (yt, ws, αs(cid:63)) + νws)] ,
Ds ← ΠDs

˜s − Dsβ˜sαs(cid:63)T (cid:17)(cid:105)

t − Dsαs(cid:63)) βT

Ds − ρt

t ).

(xs

(cid:16)

(cid:104)

,

∀s ∈ S, where ˜s = {s, s + S, . . . , s + (d − 1)S}.

10: end for

following optimization problem:

min
A

1
2

S
(cid:88)

s=1

(cid:107)xs − Dsαs(cid:107)2
(cid:96)2

+ λ1(cid:107)A(cid:107)(cid:96)12 + λ(cid:48)

1(cid:107)A(cid:107)(cid:96)11 +

(cid:107)A(cid:107)2
F ,

λ2
2

(23)

where λ(cid:48)
1 is the regularization parameter for the added (cid:96)11
norm and other terms are the same as those in Eq. (7). The
selection of λ1 and λ(cid:48)
1 inﬂuences the sparsity pattern of
A(cid:63). Intuitively, as λ1/λ(cid:48)
1 increases, the group constraint be-
comes dominant and more collaboration is enforced among
the modalities. On the other hand, small values of λ1/λ(cid:48)
1
encourage independent reconstructions across modalities.
In the extreme case of λ1 being set to zero, the above
optimization problem is separable across the modalities.
The above formulation brings added ﬂexibility with the cost
of one additional design parameter which is obtained in this
paper using cross-validation.

Here we present how the Algorithm 1 should be modiﬁed
to solve the supervised multimodal dictionary learning
problem under the mixed (cid:96)12 − (cid:96)11 constraint. The proof
for obtaining the algorithm is similar to the one for the
(cid:96)12 norm and is brieﬂy discussed in the appendix. In
Algorithm 1, let A(cid:63) be the solution of the optimization
problem (23) and let Λ be the set of its active rows. Let
Ψ ⊆ {1, . . . , S|Λ|} be the set of indices with non-zero
T ); i.e. it consists of non-zero entries
entries in vec(A(cid:63)
of the active rows of A(cid:63). Let ˆD, ∆, and g be the same as
those deﬁned in algorithm 1. Then, β ∈ RdS is updated as

Λ→

βΥc = 0, βΥ = ( ˆDT
Ψ

ˆDΨ + λ1∆Ψ→,Ψ + λ2I)−1gΨ,

7

Fig. 2: Extracted modalities from a sample in AR dataset.

where Υ is the set of indices with non-zero entries in
vec(A(cid:63)T
) and Υc = {1, . . . , dS} \Υ. Note that Υ is
deﬁned over the entire matrix A(cid:63) while Ψ is deﬁned over its
active rows. The rest of the algorithm remains unchanged.

IV. RESULTS AND DISCUSSION

The performance of the proposed multimodal dictio-
nary learning algorithms are evaluated on the AR face
database [55], the CMU Multi-PIE dataset [56], the IXMAS
action recognition dataset [57] and the WVU multimodal
ls is chosen to be
dataset [58]. For these algorithms,
the quadratic loss of Eq. (16) to handle the multiclass
classiﬁcation. In our experiments, it is observed that us-
ing the multiclass formulation achieves similar classiﬁ-
cation performance compared to using the logistic loss
formulation of Eq. (13) in the one-vs-all setting. Regu-
larization parameters λ1 and ν are selected using cross-
validation in the sets {0.01 + 0.005k|k ∈ {−3, 3}} and
{10−2, ..., 10−9}, respectively. It is observed that when the
number of dictionary atoms is kept small compared to the
number of training samples, ν can be arbitrarily set to a
small value, e.g. ν = 10−8, for the normalized inputs.
When the mixed (cid:96)12 − (cid:96)11 norm is used, the regularization
parameters λ1 and λ(cid:48)
1 are selected by cross-validation
in the set {0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05}. The
parameter λ2 is set to zero in most of the experiments
except when using the (cid:96)11 prior in Section IV-B1 where
a small positive value for λ2 was required for convergence.
The learning parameter ρt
is selected according to the
(cid:1) where
heuristic proposed in [29], i.e. ρt = min (cid:0)ρ, ρ t0
ρ and t0 are constants. This results in a constant learning
rate during the ﬁrst t0 iterations and an annealing strategy
of 1/t for the rest of the iterations. It is observed that
choosing t0 = T /10, where T is the total number of
iterations over the whole training set, works well for all
of our experiments. Different values of ρ are tried during
the ﬁrst few iterations and the one that results in minimum
error on a small validation set is retained. T is set equal
to be 20 in all the experiments. We observed empirically
that the selection of these parameters is quite robust and
small variations in their values do not affect considerably
the obtained results. We also used a mini-batch size of 100
in all our experiments. It should also be noted that design
parameters for the competitive algorithms are also selected
using cross-validation for a fair comparison.

t

A. AR face recognition

The AR dataset consists of faces under different poses,
illumination and expression conditions, captured in two
sessions. A set of 100 users are used, each consisting of

8

TABLE III: Multimodal classiﬁcation results obtained for the AR datasets

SVM-Maj

SVM-Sum LR-Maj

LR-Sum MKL

JSRC [14]

JDSRC [7]

85.57

92.14

85.00

91.14

91.14

96.14

96.14

SMDL(cid:96)11
95.86

SMDL(cid:96)12
96.86

SMDL(cid:96)12−(cid:96)11
97.14

TABLE I: Correct classiﬁcation rates obtained using the
whole face modality for the AR database.

SVM MKL [59]

LR

SRC [10] UDL

SDL [29]

86.43

82.86

81.00

88.86

89.58

90.57

TABLE IV: Comparison of the reconstructive-based (JSRC
and JSRC-UDL) and the proposed discriminative-based
(SMDL(cid:96)12 ) classiﬁcation algorithms obtained using the
joint sparsity prior for different numbers of dictionary
atoms per class on the AR dataset.

TABLE II: Comparison of the (cid:96)11 and (cid:96)12 priors for mul-
timodal classiﬁcation. Modalities include 1. left periocular,
2. right periocular, 3. nose, 4. mouth, and 5. face.

Modalities
UMDL(cid:96)11
UMDL(cid:96)12
SMDL(cid:96)11
SMDL(cid:96)12

{1, 2}

{1, 2, 3}

{1, 2, 3, 4}

{1, 2, 3, 4, 5}

81.9
82.6
83.86
86.43

87.57
87.86
89.86
89.86

90.14
92.00
92.42
93.57

95.57
96.29
95.86
96.86

seven images from the ﬁrst session as training samples and
seven images from the second session as test samples. A
small randomly selected portion of the training set, 50 out
of 700, is used as validation set for optimizing the design
parameters. Fusion is taken on ﬁve modalities which are
the left and right periocular, nose, mouth, and the whole
face modalities, similar to the setup in [14], [45]. A test
sample from the AR dataset and the extracted modalities
are shown in Fig. 2. Raw pixels are ﬁrst PCA-transformed
and then normalized to have zero mean and unit l2 norm.
The dictionary size for the dictionary learning algorithms
is chosen to be four per class, resulting in dictionaries of
overall 400 atoms.

Classiﬁcation using the whole face modality: The classi-
ﬁcation results using the whole face modality are shown in
Table I. The results are obtained using linear support vector
machine (SVM) [60], multiple kernel learning (MKL) [59],
logistic regression (LR) [60], sparse representation classi-
ﬁcation (SRC) [10], and unsupervised and supervised dic-
tionary learning algorithms (UDL and SDL) [29]. For the
MKL algorithm, linear, polynomial, and RBF kernels are
used. The UDL and SDL are equipped with the quadratic
classiﬁer (16). The SDL results in the best performance.

(cid:96)11 vs (cid:96)12 sparse priors for multimodal classiﬁcation: A
straightforward way of utilizing the single-modal dictionary
learning algorithms, namely UDL and SDL, for multimodal
classiﬁcation is to train independent dictionaries and clas-
siﬁers for each modality and then combine the individual
scores for a fused decision. This way of fusion is equivalent
to using the (cid:96)11 norm on A,
in
Eq. (7) (or setting λ1 to zero in Eq. (23)) which does not
enforce row sparsity in the sparse coefﬁcients. We denote
the corresponding unsupervised and supervised multimodal
dictionary learning algorithms using only the (cid:96)11 norm as
UMDL(cid:96)11 and SMDL(cid:96)11, respectively. Similarly, the pro-
posed unsupervised and supervised multimodal dictionary
learning algorithms using the (cid:96)12 norm are denoted as

instead of (cid:96)12 norm,

atoms/class

JSRC JSRC-UDL

1
2
3
4
5
6
7

46.14
69.00
79.57
88.14
91.00
94.43
96.14

71.71
78.86
83.57
91.14
94.85
96.28
96.14

SMDL(cid:96)12
91.28
95.00
95.71
96.86
97.14
96.71
96.00

UMDL(cid:96)12 and SMDL(cid:96)12. Table II compares the perfor-
mance of the multimodal dictionary learning algorithms
under the two priors. As shown, the proposed algorithms
with (cid:96)12 prior, which enforces collaborations among the
modalities, have better fusion performances than those with
(cid:96)11 prior. In particular, SMDL(cid:96)12 has signiﬁcantly better
performance than the SMDL(cid:96)11 for fusion of the ﬁrst and
second (left and right periocular) modalities. This agrees
with the intuition that these modalities are highly correlated
and learning the multimodal dictionaries jointly indeed
improves the recognition performance.

Comparison with other fusion methods: The perfor-
mances of the proposed fusion algorithms under different
sparsity priors are compared with those of the several state-
of-the-art decision-level and feature-level fusion algorithms.
In addition to (cid:96)11 and (cid:96)12 priors, we evaluate the proposed
supervised multimodal dictionary learning algorithm with
the mixed (cid:96)12−(cid:96)11 norm which is denoted as SMDL(cid:96)12−(cid:96)11.
One way to achieve decision-level fusion is to train in-
dependent classiﬁers for each modality and aggregate the
outputs by either adding the corresponding scores of each
modality to come up with the fused decision, or using the
majority voting among the independent decisions obtained
from different modalities. These approaches are abbrevi-
ated with Sum and Maj, respectively, and are used with
SVM and LR classiﬁers for decision-level fusion. The pro-
posed methods are also compared with feature-level fusion
methods including the joint sparse representation classiﬁer
(JSRC) [14], joint dynamic sparse representation classiﬁer
(JDSRC) [7], and MKL. For the JSRC and JDSRC, the
dictionary consists of all the training samples. Table III
compares the performance of our proposed algorithms with
the other fusion algorithms for the AR dataset. As expected,
the multimodal fusion results in signiﬁcant performance
improvement compared to using only the whole face modal-
ity. Moreover, the proposed SMDL(cid:96)12 and SMDL(cid:96)12−(cid:96)11
achieve the superior performances.

Reconstructive vs discriminative formulation with joint

9

Fig. 3: Computational time required to solve the optimiza-
tion problem (7) for a given test sample.

TABLE V: Comparison of the supervised multimodal dic-
tionary learning algorithms with different sparsity priors for
face recognition under occlusion on the AR dataset.

SMDL(cid:96)12
89.00

SMDL(cid:96)11
90.54

SMDL(cid:96)12−(cid:96)11
91.15

sparsity prior: Comparison of the algorithms with joint
sparsity priors in Table III indicates that
the proposed
SMDL(cid:96)12 algorithm equipped with dictionaries of size 400
achieves relatively better results than the JSRC that uses
dictionaries of size 700. The results conﬁrm the idea that
by using the supervised formulation, compared to using the
reconstruction error, one can achieve better classiﬁcation
performance even with more compact dictionaries. For
further comparison, an experiment is performed in which
the correct classiﬁcation rates of the reconsturtive and
discriminative formulations are compared when the their
dictionary sizes are kept equal. For a given number of
dictionary atoms per class d, dictionaries of JSRC are
thus constructed by random selection of d train samples
from different classes. This is different from the standard
JSRC, utilized for the results in Table III, in which all the
training samples are used to construct the dictionaries [14].
Moreover, to utilize all the available training samples for
the reconstructive approach and make a more meaningful
comparison, we use the unsupervised multimodal dictionary
learning algorithm of Eq. (8) to train class-speciﬁc sub-
dictionaries which minimizes the reconstruction error in
approximating the training samples for a given class. These
sub-dictionaries are then stacked to construct
the ﬁnal
dictionaries, similar to the approach in [22]. We call this
algorithm as JSRC-UDL to indicate that the dictionaries are
indeed learned by the reconstructive formulation. Table IV
summarizes the recognition performance of JSRC and
JSRC-UDL in comparison to the proposed SMDL(cid:96)12, which
enjoys a discriminative formulation, for different number of
dictionary atoms per class. As seen, SMDL(cid:96)12 outperforms
the reconstructive approaches, especially when the number
of dictionary is chosen to be relatively small. This is the
main advantage of SMDL(cid:96)12 compared to the reconstructive
approaches in which more compact dictionaries can be

Fig. 4: Conﬁgurations of the cameras and sample multi-
view images from CMU Multi-Pie dataset.

used for the recognition task that is important for the real-
time applications. It is clear that reconstructive model can
only result in comparable performance when the dictionary
size is chosen to be relatively large. On the other hand,
the SMDL(cid:96)12 algorithm may get over-ﬁtted with the large
number of dictionary atoms. In terms of computational
expense at test time, as discussed in [14], the time required
to solve the optimization problem (7) is expected to be
linear in the dictionary size using the efﬁcient ADMM if the
required matrix factorization is cashed beforehand. Typical
computational time to solve (7) for a given multimodal test
sample is shown in Fig. 3 for different dictionary sizes. As
expected, it increases linearly as the size of the dictionary
increases. This illustrates the advantage of the SMDL(cid:96)12
algorithm that results in the state-of-the-art performance
with more compact dictionaries.

Classiﬁcation in presence of disguise: The AR dataset
also contains 600 occluded samples per session, overall
1200 images, where the faces are disguised using sun
glasses or scarf. Here we use these additional images to
evaluate the robustness of the proposed algorithms. Similar
to previous experiments, images from session 1 are used
as training samples and images from session 2 are used
as test data. Classiﬁcation performance under different
sparsity priors are shown in Table V and as expected, the
SMDL(cid:96)12−(cid:96)11 achieves the best performance. In presence
of occlusion, some of the modalities are less coupled and
the joint sparsity prior among all the modalities may be too
stringent as is also reﬂected in the results.

B. Multi-view recognition

In this section,

1) Multi-view face recognition:

the
performance of the proposed algorithm is evaluated for
multi-view face recognition using the CMU Multi-PIE
[56]. The dataset consists of a large num-
dataset
ber of face images under different
illuminations, view-
points, and expressions which are recorded in four
several months. Subjects
sessions over
were imaged using 13 cameras at different view-angles
of {0◦, ±15◦, ±30◦, ±45◦, ±60◦, ±75◦, ±90◦} at head
height. Illustrations for the multiple camera conﬁgurations,
as well as sample multi-view images are shown in Fig. 4.
We use the multi-view face images for 129 subjects that are

the span of

10

TABLE VI: Correct classiﬁcation rates obtained using
individual modalities in the CMU Multi-PIE database.

View

SVM MKL

LR

Left
Frontal
Right

47.30
41.15
47.30

52.85
54.10
51.85

43.65
45.40
42.85

SRC

49.85
54.25
52.55

UDL

47.80
52.10
43.10

SDL

50.45
56.10
48.50

Fig. 5: Sample frames of the IXMAS dataset from 5
different views.

TABLE VII: Correct classiﬁcation rates (CCR) obtained
using multi-view images on the CMU Multi-PIE database.

TABLE VIII: Correct classiﬁcation rates (CCR) obtained
for multi-view action recognition on the IXMAS database.

Algorithm CCR

Algorithm

SVM-Maj
62.95
SVM-Sum 69.30
72.40
70.20
77.25
76.10

MKL
JDSRC
SMDL(cid:96)11
SMDL(cid:96)12

LR-Maj
LR-Sum
JSRC
UMDL(cid:96)11
UMDL(cid:96)12
SMDL(cid:96)12−(cid:96)11

CCR

69.40
71.10
73.30
74.80
70.50
81.30

Algorithm

Junejo et al. [65]
Wu et al. [66]
Wang et al. 2 [63]
UMDL(cid:96)11
UMDL(cid:96)12

CCR

79.6
88.2
93.6
90.3
90.6

Algorithm

Tran and Sorokin [62]
Wang et al. 1 [63]
JSRC
SMDL(cid:96)11
SMDL(cid:96)12

CCR

80.2
87.8
93.6
93.9
94.8

present in all sessions. The face regions for all the poses
are extracted manually and resized to 10 × 8. Similar to
the protocol used in [61], images from session 1 at views
{0◦, ±30◦, ±60◦, ±90◦} are used as training samples. Test
images are obtained from all available view angles from
session 2 to have a more realistic scenario in which not
all
the testing poses are available in the training set.
To handle multi-view recognition using the multi-modal
formulation, we divide the available views into three sets of
{−90◦, −75◦, −60◦, −45◦}, {−30◦, −15◦, 0◦, 15◦, 30◦, },
{45◦, 60◦, 75◦, 90◦}, each of which forms a modality. A
test sample is then constructed by randomly selecting an
image from each modality. Two thousand test samples are
generated in this way. The dictionary size for the dictionary
learning algorithms is chosen to have two atoms per class.
The classiﬁcation results obtained using individual
modalities are shown in Table VI. As expected, better
classiﬁcation performance is obtained using the frontal
view. Results of the multi-view face recognition is shown
in Table VII. The proposed supervised dictionary learn-
ing algorithms outperform the corresponding unsupervised
methods and other fusion algorithms. The SMDL(cid:96)12−(cid:96)11
results in the state-of-the-art performance. It is consistently
observed in all the studied applications that the multimodal
dictionary learning algorithm with the mixed prior results
in better performance than those with individual (cid:96)12 or
(cid:96)12 prior. However, it requires one additional regularizing
parameter to be tuned. For the rest of the paper,
the
performance of the proposed dictionary learning algorithms
are only reported under the individual priors.

the proposed algorithm for

2) Multi-view action recognition: This section presents
the results of
the pur-
pose of multi-view action recognition using the IXMAS
dataset [57]. Each action is recorded simultaneously by
cameras from ﬁve different viewpoints, which are con-
sidered as modalities in this experiment. A multimodal
sample of the IXMAS dataset is shown in Fig. 5. The
dataset contains 11 action classes where each action is
repeated three times by each of the ten actors, resulting
in 330 sequences per view. The dataset include actions

such as check watch, cross arms, and scratch head. Similar
to the work in [57], [62], [63], leave-one-actor-out cross-
validation is performed and samples from all ﬁve views are
used for training and testing.

We use dense trajectories as features which are generated
using the publicly available code [63] in which a 2000
word codebook is generated by a random subset of these
trajectories and the k-means clustering as in [64]. Note that
Wang et al. [63] used HOG, HOF, and MBH descriptors in
addition to the dense trajectories. However, here only dense
trajectory descriptors are used. The number of dictionary
atoms for the proposed dictionary learning algorithms are
chosen to be 4 atoms per class, resulting in a dictionary of
44 atoms per view. The ﬁve dictionaries for JSRC are con-
structed using all the training samples, thus each dictionary,
corresponding to a different view, has 297 atoms.

Table VIII shows average accuracies over all classes
obtained using the existing algorithms and the state of
the art algorithms. The Wang et al. 1 [63] algorithm uses
only the dense trajectories as feature, similar to our setup.
The Wang et al. 2 [63] algorithm, however, uses HOG,
HOF, MBH descriptors and the spatio-temporal pyramids
in addition to the trajectory descriptor. The results show
that the proposed SMDL(cid:96)12 algorithm achieves the superior
performance while the SMDL(cid:96)11 algorithm achieves the
second best performance. This indicates that sparse coefﬁ-
cients generated by the trained dictionaries are indeed more
discriminative than the engineered features. The resulting
confusion matrix of the SMDL(cid:96)12 algorithm is shown in
Fig. 6.

C. Multimodal biometric recognition

The WVU dataset consists of different biometric modali-
ties such as ﬁngerprint, iris, palmprint, hand geometry, and
voice from subjects of different age, gender, and ethnicity.
It
is a challenging data set, as many of the samples
are corrupted with blur, occlusion, and sensor noise. In
this paper, two irises (left and right) and four ﬁngerprint
modalities are used. The evaluation is done on a subset
of 202 subjects which have more than four samples in all

TABLE IX: Correct classiﬁcation rates obtained using individual modalities in the WVU database.

Finger 1

Finger 2

Finger 3

Finger 4

Iris 1

Iris 2

SVM 56.77 ± 0.72
61.81 ± 1.39
MKL
55.64 ± 1.89
LR
67.66 ± 1.86
SRC
64.68 ± 2.11
UDL
66.29 ± 1.81
SDL

82.95 ± 2.15
82.55 ± 1.47
81.10 ± 1.85
88.68 ± 1.59
87.35 ± 2.23
88.84 ± 2.31

55.83 ± 2.03
63.50 ± 1.75
55.21 ± 2.21
69.29 ± 0.77
67.35 ± 1.22
68.61 ± 1.30

80.47 ± 0.91
81.85 ± 0.74
78.82 ± 0.66
88.68 ± 1.03
86.40 ± 0.70
87.50 ± 0.82

60.67 ± 1.78
56.31 ± 2.20
55.25 ± 1.48
65.43 ± 1.24
64.36 ± 1.37
66.05 ± 0.75

57.52 ± 1.95
54.49 ± 0.79
56.86 ± 1.70
67.78 ± 1.76
65.23 ± 2.02
67.31 ± 1.38

11

TABLE X: Multimodal classiﬁcation results obtained for
the WVU dataset.

Algorithm 4 Fingerprints

2 Irises

All modalities

90.14 ± 0.70
SVM-Maj
SVM-Sum 93.56 ± 1.26
89.23 ± 1.63
93.60 ± 0.96
93.28 ± 1.52
97.64 ± 0.44
97.17 ± 0.26
97.09 ± 0.56
97.41 ± 0.71
96.78 ± 0.57
97.56 ± 0.41

LR-Maj
LR-Sum
MKL
JSRC
JDSRC
UMDL(cid:96)11
SMDL(cid:96)11
UMDL(cid:96)12
SMDL(cid:96)12

65.30 ± 1.92
74.03 ± 1.89
63.73 ± 1.29
71.43 ± 1.91
67.23 ± 0.70
82.94 ± 0.78
79.61 ± 0.70
80.90 ± 0.61
82.83 ± 0.87
81.53 ± 2.18
83.77 ± 0.89

95.24 ± 0.92
97.09 ± 0.83
94.18 ± 1.13
98.51 ± 0.18
94.46 ± 0.87
98.89 ± 0.30
97.80 ± 0.51
98.62 ± 0.46
98.66 ± 0.43
98.78 ± 0.43
99.10 ± 0.30

nally proposed for biometric recognition systems [67]. As
seen, the SMDL(cid:96)12 algorithm outperforms the competitive
algorithms and achieves the state-of-the-art performance
using the Irises and all modalities with the rank one
recognition rate of 83.77% and 99.10%, respectively. Using
the ﬁngerprints, the performance of the SMDL(cid:96)12 is close to
the best performing algorithm, which is JSRC. The results
suggest that using joint sparsity prior indeed improves the
multimodal classiﬁcation performance by extracting the
coupled information among the modalities.

Comparison of the algorithms with the joint sparsity
the proposed SMDL(cid:96)12 algorithm
priors indicates that
equipped with dictionaries of size 404 achieves comparable,
and mostly better, results than the JSRC that uses dictionary
of size 808. Similar to the experiment in Section IV-A,
we compared the reconstructive and discriminating algo-
rithms that are based on the joint sparsity prior when the
number of dictionary atoms per class is kept equal. Fig. 8
summarizes the results of the different fusion scenarios.
As seen, SMDL(cid:96)12 signiﬁcantly outperforms JSRC and
JSRC-UDL when the number of dictionary atoms per class
is chosen to be 1 or 2. The results are consistent with
that of Table IV for the AR dataset indicating that the
proposed supervised formulation equipped with more com-
pact dictionaries achieves superior performance than that
of the reconstructive formulation for the studied biometric
recognition applications.

V. CONCLUSIONS AND FUTURE WORKS

The problem of multimodal classiﬁcation using sparsity
models was studied and a task-driven formulation was
proposed to jointly ﬁnd the optimal dictionaries and clas-
siﬁers under the joint sparsity prior. It was shown that the

Fig. 6: The confusion matrix obtained by the SMDL(cid:96)12
algorithm on the IXMAS dataset. The actions are 1: check
watch, 2: cross arms, 3: scratch head, 4: sit down, 5: get
up, 6: turn around, 7: walk, 8: wave, 9: punch, 10: kick and
11: pick up.

modalities. Samples from different modalities are shown in
Fig. 1. The training set is formed by randomly selecting
four samples from each subject, overall 808 samples. The
remaining 509 samples are used for testing. The features
used here are those described in [14] which are further
PCA-transformed. The dimension of the input data after
preprocessing are 178 and 550 for the ﬁngerprint and iris
modalities, respectively. All inputs are normalized to have
zero mean and unit l2 norm. The number of dictionary
atoms for the dictionary learning algorithms are chosen to
be 2 per class, resulting in dictionaries of overall 404 atoms.
The dictionaries for JSRC and JDSRC are constructed using
all the training samples.

The classiﬁcation results obtained using individual
modalities on 5 different splits of the data into training and
test samples are shown in Table IX. As shown, ﬁnger 2 is
the strongest modality for the recognition task. The SRC
and SDL algorithms achieve the best results. It should be
noted that dictionary size of SRC is twice of that in SDL.
For multimodal classiﬁcation, we consider fusion of
ﬁngerprints, fusion of Irises, and fusion of all the modali-
ties. Table X summarizes the correct classiﬁcation rates of
several fusion algorithms using 4 ﬁngerprints, 2 Irises, and
all the modalities, obtained on 5 different training and test
splits. Fig. 7 shows the corresponding cumulative matched
score curves (CMC) for the competitive methods. CMC is
a performance measure, similar to ROC, which is origi-

12

Fig. 8: Comparison of the reconstructive-based (JSRC
and JSRC-UDL) and the proposed discriminative-based
(SMDL(cid:96)12 ) classiﬁcation algorithms obtained using the
joint sparsity prior for different numbers of dictionary
atoms per class on the WVU dataset.

utilizes the stochastic gradient algorithm, the learning rate
should be carefully chosen for convergence of the algo-
rithm. In out experiments, a heuristic was used to control
the learning rate. Topics of future research include develop-
ing of better optimization tools for fast convergence guaran-
tee in this non-convex setting. Moreover, developing task-
driven dictionary learning algorithms under other proposed
structured sparsity priors for multimodal fusion such as the
tree-structured sparsity prior [45], [68] is another future
research topic. Future research will also include adapting of
the proposed algorithms for other multimodal tasks such as
multimodal retrieval, multimodal action recognition using
Kinect data, and image super-resolution.

APPENDIX

The proof of Proposition 3.1 is presented using the

following two results.

(cid:104)

Lemma A.1 (Optimality condition): The matrix A(cid:63) =
α1(cid:63) . . . αS (cid:63)(cid:105)
∈ Rd×S is a min-
1→
imizer of (7) if and only if , ∀j ∈ {1, . . . , d},

T . . . a(cid:63)

T (cid:105)T

a(cid:63)

d→

=

(cid:104)






T (cid:16)

(cid:104)

d1
j

. . . dS
j

x1 − D1α1(cid:63)(cid:17)
a(cid:63)
j→
j→ = λ1
(cid:107)a(cid:63)
j→(cid:107)(cid:96)2
x1 − D1α1(cid:63)(cid:17)
j→(cid:107)(cid:96)2 ≤ λ1, otherwise.

. . . dS
j

, if (cid:107)a(cid:63)
T (cid:16)

− λ2a(cid:63)
T (cid:16)
(cid:104)
d1
(cid:107)
j
− λ2a(cid:63)

T (cid:16)

xS − DSαS (cid:63)(cid:17)(cid:105)

j→(cid:107)(cid:96)2 (cid:54)= 0,
xS − DSαS (cid:63)(cid:17)(cid:105)

(24)
Proof: . The proof follows directly from the subgradi-

ent optimality condition of (7), i.e.

0 ∈ {

D1α1(cid:63)

. . . DS T (cid:16)
− x1(cid:17)
D1T (cid:16)
(cid:104)
+ λ2A(cid:63) + λ1P : P ∈ ∂(cid:107)A(cid:63)(cid:107)(cid:96)12 },

DSαS (cid:63)

− xS(cid:17)(cid:105)

where ∂(cid:107)A(cid:63)(cid:107)(cid:96)12 denotes the subgradient of the (cid:96)12 norm
evaluated at A(cid:63). As shown in [69], the subgradient is
characterized, for all j ∈ {1, . . . , d}, as pj→ = aj→
if (cid:107)aj→(cid:107)(cid:96)2 > 0, and (cid:107)pj→(cid:107)(cid:96)2 ≤ 1 otherwise.

(cid:107)aj→(cid:107)(cid:96)2

Fig. 7: CMC plots obtained by fusing the Irises (top),
ﬁngerprints (middle), and all modalities (below) on the
WVU dataset.

resulting bi-level optimization problem is smooth and an
stochastic gradient descent algorithm was proposed to solve
the corresponding optimization problem. The algorithm
was then extended for a more general scenario where
the sparsity prior was the combination of the joint and
independent sparsity constraints. The simulation results on
the studied image classiﬁcation applications suggest that
while the unsupervised dictionaries can be used for feature
learning, the sparse coefﬁcients generated by the proposed
multimodal task-driven dictionary learning algorithms are
usually more discriminative and therefore can result
in
improved multimodal classiﬁcation performance. It was
also shown that, compared to the sparse-representation
classiﬁcation algorithms (JSRC, JDSRC, and JSRC-UDL),
the proposed algorithms can achieve signiﬁcantly better
performance when compact dictionaries are utilized.

In the proposed dictionary learning framework which

Before proceeding to the next proposition, we need to
deﬁne the term transition point. For a given {xs}, let Λλ
be the active set of the solution A(cid:63) of (7) when λ1 = λ.
Then λ is deﬁned to be a transition point of {xs} if Λλ+(cid:15) (cid:54)=
Λλ−(cid:15), ∀(cid:15) > 0.

Proposition A.1 (Regularity of A(cid:63)): Let λ2 > 0 and

assumption (A) be hold. Then,

Part 1. A(cid:63)({xs, Ds}) is a continuous function of {xs}

and {Ds}.

Part 2. If λ1 is not a transition point of {xs}, then
the active set Λ of A(cid:63)({xs, Ds}) is locally constant with
respect to both {xs} and {Ds}. Moreover, A(cid:63)({xs, Ds})
is locally differentiable with respect to {Ds}.

Part 3. ∀λ1 > 0, ∃ a set Nλ1 of measure zero in which
∀{xs} ∈ {Rns}\Nλ1 , λ1 is not any of the transition points
of {xs}.

Proof: . Part 1. In the special case of S = 1, which
is equivalent to an elastic net problem, this has already
been shown [19], [70]. Our proof follows similar steps.
Assumption (A) guarantees that A(cid:63) is bounded. Therefore,
we can restrict the optimization problem (7) to a compact
subset of Rd×S. Since A(cid:63) is unique (imposed by λ2 > 0)
and the cost function of (7) is continuous in A and each
element of the set {xs, Ds} is deﬁned over a compact set,
A(cid:63)({xs, Ds}) is a continuous function of {xs} and {Ds}.
Part 2 and Part 3. These statements are proved here by
converting the optimization problem (7) into an equivalent
group lasso problem [71] and using some recent results
j ) ∈
on it. Let
Rn×S, ∀j ∈ {1, . . . , d}, be the block-diagnoal collection
of the jth atoms of the dictionaries. Also let D(cid:48) =
∈ Rn, and
[D(cid:48)
a(cid:48) = [a1→ . . . ad→]T ∈ RSd . Then (7) can be rewritten as

d] ∈ Rn×Sd, x(cid:48) =

x1T . . . xS T (cid:105)T

j = blkdiag(d1

the matrix D(cid:48)

j , . . . , dS

1 . . . D(cid:48)

(cid:104)

min
A

1
2

(cid:107)x(cid:48) − D(cid:48)a(cid:48)(cid:107)2
(cid:96)2

+ λ1

(cid:107)aj→(cid:107)(cid:96)2 +

(cid:107)A(cid:107)2

F . (25)

d
(cid:88)

j=1

λ2
2

This can be further converted into the standard group lasso:

min
A

1
2

(cid:107)x(cid:48)(cid:48) − D(cid:48)(cid:48)a(cid:48)(cid:107)2
(cid:96)2

+ λ1

(cid:107)aj→(cid:107)(cid:96)2,

(26)

d
(cid:88)

j=1

(cid:104)

x(cid:48)T 0T (cid:105)T

(cid:105)T

where x(cid:48)(cid:48) =
D(cid:48)T √
(cid:104)
D(cid:48)(cid:48)
directly from the results in [72].

λ2I

∈ R(n+Sd)×Sd. It is clear that the matrix
is full column rank. The rest of the proof follows

∈ Rn+Sd and D(cid:48)(cid:48) =

Proof of Proposition 3.1: The above proposition
implies that A(cid:63) is differentiable almost everywhere. We
know prove the proposition 3.1. It is easy to show that f
is differentiable with respect to ws due to the assumption
(A) and the fact
lsu is twice differentiable. f is
also differentiable with respect to Ds given assumption
(A), twice differentiability of lsu, and the fact that A(cid:63)
is differentiable everywhere except on a set of measure
zero (Prop A.1). We obtain the derivative of f with respect
to Ds using the chain rule. The steps are similar to those

that

13

taken for (cid:96)1-related optimization in [36], though a bit more
involved. Since the active set is locally constant, using the
optimality condition (24), we can implicitly differentiate
A(cid:63)({xs, Ds}) with respect to Ds. For the non-active rows
of A(cid:63), the differential is zero. On the active set Λ, (24) can
be rewritten as

T (cid:16)

(cid:104)
D1
Λ

x1 − D1α1(cid:63)(cid:17)
(cid:34)

− λ2A(cid:63)

Λ→ = λ1

T (cid:16)

. . . DS
Λ

xS − DSαS (cid:63)(cid:17)(cid:105)
(cid:35)T

T

a(cid:63)
1→
(cid:107)a(cid:63)
1→(cid:107)(cid:96)2

. . .

T

a(cid:63)
N→
(cid:107)a(cid:63)
N→(cid:107)(cid:96)2

,

(27)

where N is the cardinality of Λ and DΛ and A(cid:63)
Λ→ are the
matrices consisting of active columns of D and active rows
of A(cid:63), respectively. For the rest of the proof, we only work
on the active set and the symbols Λ and (cid:63) are dropped for
the ease of notation. Taking the partial derivative from both
sides of (27) with respect to ds
ij, the element in the ith-row
and jth-column of Ds, and taking its transpose we have:





+

λ2

∂AT
∂ds
ij

0
(Dsαs − xs)T Es
ij + αsT Es
ij
0



T Ds

 +








∂α1T
∂ds
ij

∂αS T
∂ds
ij

D1T D1
...
DS T

DS








= −λ1

∆1

. . . ∆N

(cid:34)

1→

∂aT
∂ds
ij

(cid:35)

,

N→

∂aT
∂ds
ij

ij ∈ Rns×N is a matrix with zero elements except
where Es
the element in the ith row and jth column which is one
and

∆k =

1
(cid:107)ak→(cid:107)(cid:96)2

(cid:32)

I −

1
(cid:107)ak→(cid:107)2
(cid:96)2

(cid:33)

aT

k→ak→

∈ RS×S,

∀k ∈ {1, . . . , N }. It
Vectorizing the both sides and factorizing results in

is easy to check that ∆k ≥ 0.

vec

(cid:32)

(cid:33)

∂AT
∂ds
ij

=











P

0
(xs − Dsαs)T es
...
(xs − Dsαs)T es
ij N
0

ij 1

− αsT Es
ij

T ds
1

(28)

− αsT Es
ij

T ds
N











,

is

ij k

the kth
(cid:17)−1

column of Es

where es
=
(cid:16) ˆDT ˆD + λ1∆ + λ2I
, and ˆD and ∆ are deﬁned
in Eqs. (19) and (20), respectively. Further simplifying
Eq. (28) yields
(cid:33)
(cid:32)

ij, P

= P˜sEs
ij

T (xs − Dsαs) − P˜sds

T αs
j ,

i→

vec

∂AT
∂ds
ij

where ˜s is deﬁned in Eq. (21). Using the chain rule, we
have

(cid:34)

(cid:32)

(cid:33)(cid:35)

∂f
∂ds
ij

= E

gT vec

∂AT
∂ds
ij

,

14

where g = vec
respective to the active columns of dictionary Ds is

. Therefore, derivative with

(cid:16) ∂ (cid:80)S

s=1 lsu
∂AT

(cid:17)

∂f
∂Ds = E








. . .

gT P˜s

(cid:16)

gT P˜s

Es
11

(cid:16)

Es

ns1

gT P˜s
(cid:16)

Es

1N

T (xs − Dsαs) − ds
...
T (xs − Dsαs) − ds

(cid:17)

T αs
1

1→

(cid:17)

T αs
1


ns→
(cid:17)

T αs
N

1→

T (xs − Dsαs) − ds
...
T (xs − Dsαs) − ds
˜s gαsT (cid:105)

.

T αs
N

ns→






(cid:17)

(cid:16)

. . . gT P˜s

Es

nsN

(cid:104)

= E

(xs − Dsαs) gT P˜s − DsP T

Setting β = P T g ∈ RN S and noting that β˜s = P T
complete the proof.

˜s g

Derivation of the algorithm with the mixed (cid:96)12 − (cid:96)11
prior can be obtained similarly. For each active row j ∈ Λ
of A(cid:63), the solution of the optimization problem (23) with
the mixed prior, let Πj ⊆ S be the set of active modalities
which have non-zeros entries. Then the optimality condition
for the active row j is

(cid:104)
d1
j

T (cid:16)

x1 − D1α1(cid:63)(cid:17)

. . . dS
j

T (cid:16)

xS − DSαS (cid:63)(cid:17)(cid:105)

Πj

− λ2a(cid:63)

j→ = λ1

a(cid:63)
(cid:107)a(cid:63)

j→,Πj
j→(cid:107)(cid:96)2

+ λ(cid:48)

1 sign

(cid:16)

a(cid:63)

j→,Πj

(cid:17)

.

Then, the algorithm for the mixed prior can be obtained by
differentiating the optimality condition, following similar
steps as was shown for the (cid:96)12 prior.

REFERENCES

[1] D. L. Hall and J. Llinas,

“An introduction to multisensor data

fusion,” Proc. IEEE, vol. 85, no. 1, pp. 6–23, January 1997.
[2] P. K. Varshney, “Multisensor data fusion,” in Intell. Problem Solving.
Methodologies and Approaches. Springer Berlin Heidelberg, 2000.
[3] H. Wu, M. Siegel, R. Stiefelhagen, and J. Yang, “Sensor fusion
using Dempster-Shafer theory,” in Proc. 19th IEEE Instrum. and
Meas. Technol. Conf. (IMTC), 2002, pp. 7–12.

[4] A. Ross and R. Govindarajan, “Feature level fusion using hand and

face biometrics,” in SPIE proc. series, 2005, pp. 196–204.

[5] D. Ruta and B. Gabrys, “An overview of classiﬁer fusion methods,”

Comput. and Inf. syst., vol. 7, no. 1, pp. 1–10, 2000.

[6] A. Rattani, D. R. Kisku, M. Bicego, and M. Tistarelli, “Feature
level fusion of face and ﬁngerprint biometrics,” in Proc. 1st IEEE
Int. Conf. Biometrics: Theory, Applicat., and Syst., 2007, pp. 1–6.

[7] H. Zhang, N. M. Nasrabadi, Y. Zhang, and T. S. Huang, “Multi-
observation visual recognition via joint dynamic sparse representa-
tion,” in Proc. IEEE Conf. Comput. Vision (ICCV), 2011, pp. 595–
602.

[8] A. Klausne, A. Tengg, and B. Rinner, “Vehicle classiﬁcation on
multi-sensor smart cameras using feature- and decision-fusion,” in
Proc. 1st ACM/IEEE Int. Conf. Distributed Smart Cameras, 2007,
pp. 67–74.

[9] A. Rattani and M. Tistarelli, “Robust multi-modal and multi-unit
feature level fusion of face and iris biometrics,” in Advances in
Biometrics, pp. 960–969. Springer, 2009.

[10] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, “Robust
IEEE Trans. Pattern

face recognition via sparse representation,”
Anal. Mach. Intell, vol. 31, no. 2, pp. 210–227, Feb. 2009.

[11] X. Mei and H. Ling, “Robust visual tracking and vehicle classiﬁ-
cation via sparse representation,” IEEE Trans. Pattern Anal. Mach.
Intell, vol. 33, no. 11, pp. 2259–2272, Nov. 2011.

[12] H. Zhang, Y. Zhang, N. M. Nasrabadi, and T. S. Huang, “Joint-
structured-sparsity-based classiﬁcation for multiple-measurement
transient acoustic signals,” IEEE Trans. Syst., Man, Cybern., vol.
42, no. 6, pp. 1586–98, Dec. 2012.

[13] R. Caruana, Multitask learning, Springer, 1998.
[14] S. Shekhar, V. Patel, N. M. Nasrabadi, and R. Chellappa, “Joint
sparse representation for robust multimodal biometrics recognition,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 1, pp. 113–126,
Jan. 2013.

[15] U. Srinivas, H. Mousavi, C. Jeon, V. Monga, A. Hattel, and B. Ja-
yarao, “Simultaneous sparsity model for histopathological image
representation and classiﬁcation,” IEEE Trans. Med. Imag., vol. 33,
no. 5, pp. 1163 – 1179, May 2014.

[16] H. S. Mousavi, V. Srinivas, U.and Monga, Y. Suo, M. Dao, and T. D.
Tran, “Multi-task image classiﬁcation via collaborative, hierarchical
spike-and-slab priors,” in IEEE Intl. Conf. Image Processing (ICIP),
Oct 2014, pp. 4236–4240.

[17] N. H. Nguyen, N. M. Nasrabadi, and T. D. Tran, “Robust multi-
sensor classiﬁcation via joint sparse representation,” in Proc. 14th
Int. Conf. Information Fusion (FUSION), 2011.

[18] M. Yang, L. Zhang, D. Zhang, and S. Wang, “Relaxed collaborative
in Proc. IEEE Conf.
representation for pattern classiﬁcation,”
Comput. Vision and Pattern Recognition (CVPR), 2012, pp. 2224–
2231.

[19] J. Mairal, F. Bach, J. Ponce, and G. Sapiro,

“Online dictionary
learning for sparse coding,” Proc. 26th Annu. Int. Conf. Mach.
Learning (ICML), pp. 689–696, 2009.

[20] J. Mairal, F. Bach, A. Zisserman, and G. Sapiro,

“Supervised
in Advances Neural Inform. Process. Syst.

dictionary learning,”
(NIPS), 2008, pp. 1033–1040.

[21] J. Mairal, M. Elad, and G. Sapiro, “Sparse representation for color
image restoration,” IEEE Trans. Image Process., vol. 17, no. 1, pp.
53–69, Jan. 2008.

[22] M. Yang, L. Zhang, J. Yang, and D. Zhang, “Metaface learning for
sparse representation based face recognition,” in Proc. IEEE Conf.
Image Process. (ICIP), 2010, pp. 1601–1604.

[23] Y. L. Boureau, F. Bach, Y. LeCun, and J. Ponce, “Learning mid-
level features for recognition,” in Proc. IEEE Conf. Comput. Vision
and Pattern Recognition (CVPR), 2010, pp. 2559–2566.

[24] Z. Jiang, Z. Lin, and L. S. Davis, “Label consistent K-SVD: Learning
a discriminative dictionary for recognition,” IEEE Trans. Pattern
Anal. Mach. Intell, vol. 35, no. 11, pp. 2651–2664, Nov. 2013.
[25] M. Aharon, M. Elad, and A. Bruckstein, “K-SVD : An algorithm
for designing overcomplete dictionaries for sparse representation,”
IEEE Trans. Signal Process., vol. 54, no. 11, pp. 4311–4322, Nov.
2006.

[26] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online learning for
matrix factorization and sparse coding,” The J. of Mach. Learning
Research, vol. 11, pp. 19–60, 2010.

[27] K. Engan, S. O. Aase, and J. H. Husoy,

“Method of optimal
directions for frame design,” in Proc. IEEE Int. Conf. Acoustics,
Speech, and Signal Process., 1999, vol. 5, pp. 2443–2446.

[28] M. Elad and M. Aharon, “Image denoising via saprse and redundant
IEEE Trans. Image

representations over learned dictionaries,”
Process., vol. 15, no. 12, pp. 3736–3745, Dec. 2006.

[29] J. Mairal, F. Bach, and J. Ponce, “Task-driven dictionary learning,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 4, pp. 791–804,
Apr. 2012.

[30] J. Yang, Z. Wang, Z. Lin, X. Shu, and T. Huang, “Bilevel sparse
coding for coupled feature spaces,” in IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2012, pp. 2360–2367.
[31] S. Kong and D. Wang, “A brief summary of dictionary learning
based approach for classiﬁcation,” arxivId:1205.6544, 2012.
[32] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman, “Dis-
criminative learned dictionaries for local image analysis,” in Proc.
IEEE Conf. Comput. Vision and Pattern Recognition (CVPR), 2008,
pp. 1–8.

[33] Q. Zhang and B. Li, “Discriminative K-SVD for dictionary learning
in face recognition,” in Proc. IEEE Conf. Comput. Vision and Pattern
Recognition (CVPR), 2010, pp. 2691–2698.

[34] I. Ramirez, P. Sprechmann, and G. Sapiro,

“Classiﬁcation and
clustering via dictionary learning with structured incoherence and
shared features,” in Proc. IEEE Conf. Comput. Vision and Pattern
Recognition (CVPR), 2010, pp. 3501–3508.

[35] M. Yang, L. Zhang, X. Feng, and D. Zhang, “Fisher discrimination
dictionary learning for sparse representation,” in Proc. IEEE Int.
Conf. Comput. Vision (ICCV), 2011, pp. 543–550.

[36] J. Yang, K. Yu, and T. Huang,

“Supervised translation-invariant
sparse coding,” in Proc. IEEE Conf. Comput. Vision and Pattern
Recognition (CVPR), 2010, pp. 3517–3524.

[37] B. Colson, P. Marcotte, and G. Savard, “An overview of bilevel
optimization,” Ann. Operations Research, vol. 153, no. 1, pp. 235–
256, Apr. 2007.

[38] D. M. Bradley and J. A. Bagnell, “Differentiable sparse coding,” in

Advances Neural Inform. Process. Syst. (NIPS), 2008.

[39] J. Zheng and Z. Jiang, “Learning view-invariant sparse representa-
tions for cross-view action recognition,” in Proc. IEEE Intel. Conf.
Computer Vision (ICCV), 2013, pp. 3176–3183.

[40] G. Monaci, P. Jost, P. Vandergheynst, B. Mailh´e, S. Lesage, and
IEEE Trans.

R. Gribonval,
Image Process, vol. 16, no. 9, pp. 2272–2283, Sep. 2007.

“Learning multimodal dictionaries,”

[41] Y. Zhuang, Y. Wang, F. Wu, Y. Zhang, and W. Lu, “Supervised
coupled dictionary learning with group structures for multi-modal
retrieval,” in Proc. 27th Conf. Artiﬁcial Intell., 2013, pp. 1070–1076.
[42] H. Zhang, Y. Zhang, and T. S. Huang, “Simultaneous discriminative
projection and dictionary learning for sparse representation based
classiﬁcation,” Pattern Recognition, vol. 46, no. 1, pp. 346–354,
Jan. 2013.

[43] B. A. Olshausen and D. J. Field, “Sparse coding with an overcom-
plete basis set: A strategy employed by v1?,” Vision Research, vol.
37, no. 23, pp. 3311–3325, Dec. 1997.

[44] L. Bottou and O. Bousquet, “The trade-offs of large scale learning,”
in Advances Neural Inform. Process. Syst. (NIPS), 2007, pp. 161–
168.

[45] S. Bahrampour, A. Ray, N. M. Nasrabadi, and W. K. Jenkins,
“Quality-based multimodal classiﬁcation using tree-structured spar-
sity,” in Proc. IEEE Conf. Comput. Vision and Pattern Recognition
(CVPR), 2014, pp. 4114–4121.

[46] S. F. Cotter, B. D. Rao, K. Engan, and K. Kreutz-Delgado, “Sparse
solutions to linear inverse problems with multiple measurement
vectors,” IEEE Trans. Signal Process., vol. 53, no. 7, pp. 2477–
2488, Jul. 2005.

[47] J. A. Tropp, “Algorithms for simultaneous sparse approximation. part
ii: Convex relaxation,” Signal Process., vol. 86, no. 3, pp. 589–602,
Mar. 2006.

[48] A. Rakotomamonjy, “Surveying and comparing simultaneous sparse
approximation (or group-lasso) algorithms,” Signal Process., vol.
91, no. 7, pp. 1505–1526, Jul. 2011.

[49] N. Parikh and S. Boyd, “Proximal algorithms,” Foundations and

Trends in Optimization, pp. 1–96, 2013.

[50] H. Zou and T. Hastie, “Regularization and variable selection via the
elastic net,” J. Royal Statistical Soc. Series B, vol. 67, no. 2, pp.
301–320, 2005.

[51] M. Aharon and M. Elad, “Sparse and redundant modeling of image
content using an image-signature-dictionary,” SIAM J.l on Imaging
Sciences, vol. 1, no. 3, pp. 228–247, 2008.

“Large-scale machine learning with stochastic gradi-
in Proceedings of COMPSTAT’2010, pp. 177–186.

[52] L. Bottou,

ent descent,”
Springer, 2010.

[53] L. Bottou, “Online learning and stochastic approximations,” On-line

learning in neural networks, vol. 17, pp. 9.

[54] P. Sprechmann, I. Ramirez, G. Sapiro, and Y. C. Eldar, “C-hilasso: A
collaborative hierarchical sparse modeling framework,” IEEE Trans.
Signal Process., vol. 59, no. 9, pp. 4183–4198, Sep. 2011.

[55] A. M. Martinez and R. Benavente, “The AR face database,” CVC

Technical Rep., vol. 24, 1998.

[56] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, “Multi-
pie,” Image and Vision Computing, vol. 28, no. 5, pp. 807–813,
2010.

[57] D. Weinland, E. Boyer, and R. Ronfard, “Action recognition from
in IEEE 11th Intel. Conf.

arbitrary views using 3d exemplars,”
Computer Vision (ICCV), 2007, pp. 1–7.

[58] S. S. S. Crihalmeanu, A. Ross, and L. Hornak, “A protocol for
multibiometric data acquisition, storage and dissemination,” Tech.
Rep., Lane Dept. of Comput. Sci. and Elect. Eng., West Virginia
Univ., 2007.

[59] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet, “Sim-
plemkl.,” J. of Mach. Learning Research, vol. 9, no. 11, 2008.
[60] C. M. Bishop, Pattern Recognition and Machine Learning, Springer,

2006.

[61] H. Zhang, N. M. Nasrabadi, Y. Zhang, and T. S. Huang, “Joint
dynamic sparse representation for multi-view face recognition,”
Pattern Recognition, vol. 45, pp. 1290–1298, 2012.

15

[62] D. Tran and A. Sorokin, “Human activity recognition with metric
learning,” in Computer Vision–ECCV 2008, pp. 548–561. Springer,
2008.

[63] A. Wang, H.and Kl¨aser, C. Schmid, and C.-L. Liu, “Dense trajec-
tories and motion boundary descriptors for action recognition,” Intl.
J. Computer Vision, vol. 103, no. 1, pp. 60–79, 2013.

[64] A. Gupta, A. Shafaei, J. J. Little, and R. J. Woodham, “Unlabelled 3d
motion examples improve cross-view action recognition,” in Proc.
British Machine Vision Conf., 2014.

[65] I. N. Junejo, E. Dexter, and P. Laptev, I.and Perez,

“View-
independent action recognition from temporal self-similarities,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 1, pp. 172–
185, 2011.

[66] X. Wu, D. Xu, L. Duan, and J. Luo, “Action recognition using
context and appearance distribution features,” in IEEE Conf. Comput.
Vision Pattern Recognition (CVPR), 2011, pp. 489–496.

[67] R. M. Bolle, J. H. Connell, S. Pankanti, N. K. Ratha, and A. W.
Senior, “The relation between the roc curve and the cmc,” in Proc.
4th IEEE Workshop Automat. Identiﬁcation Advanced Technologies,
2005, pp. 15–20.

[68] Julien Mairal, Rodolphe Jenatton, Guillaume Obozinski, and Francis
“Learning hierarchical and topographic dictionaries with

Bach,
structured sparsity,” in SPIE, 2011.

[69] E. V. D. Berg and M. P. Friedlander, “Theoretical and empirical
results for recovery from multiple measurements,” IEEE Trans. Inf.
Theory, vol. 56, no. 5, pp. 2516–2527, Apr. 2010.

[70] H. Zou, T. Hastie, and R. Tibshirani, “On the degrees of freedom
of the lasso,” The Ann. of Statistics, vol. 35, no. 5, pp. 2173–2192,
2007.

[71] M. Yuan and Y. Lin, “Model selection and estimation in regression
with grouped variables,” J. Royal Statistical Soc.: Series B (Statis-
tical Methodology), vol. 68, no. 1, pp. 49–67, 2006.

[72] S. Vaiter, C. Deledalle, G. Peyre, J. Fadili, and C. Dossal, “Degrees

of freedom of the group lasso,” arXiv:1205.1481, 2012.

Soheil Bahrampour received the M.Sc. degree
in electrical engineering from the University of
Tehran, Iran,
in 2009. He then received the
M.Sc. degree in Mechanical Engineering and
PhD degree in Electrical Engineering under the
supervision of A. Ray and W.K. Jenkins from
The Pennsylvania State University, University
Park, PA, in 2013 and 2015, respectively. Dr.
Bahrampour is currently a Research Scientist
with Bosch Research and Technology Center,
Palo Alto, CA. His research interests include

machine learning, data mining, signal processing, and computer vision.

16

Nasser M. Nasrabadi
(S’80-M’84-SM’92-
FM’01) received the B.Sc. (Eng.) and Ph.D.
degrees in Electrical Engineering from Imperial
College of Science and Technology (University
of London), London, England, in 1980 and 1984,
respectively. From October 1984 to December
1984 he worked for IBM (UK) as a senior
programmer. During 1985 to 1986 he worked
with Philips research laboratory in NY as a
member of technical staff. From 1986 to 1991 he
was an assistant professor in the Department of
Electrical Engineering at Worcester Polytechnic Institute, Worcester, MA.
From 1991 to 1996 he was an associate professor with the Department
of Electrical and Computer Engineering at State University of New York
at Buffalo, Buffalo, NY. Since September From 1996 to 2015 he was a
Senior Research Scientist (ST) with the US Army Research Laboratory
(ARL). Since August 2015 he has been a professor at Lane Dept. of
Computer Science and Elecrical Engineering. Dr. Nasrabadi has served
as an associate editor for the IEEE Transactions on Image Processing,
the IEEE Transactions on Circuits, Systems and Video Technology, and
the IEEE Transactions on Neural Networks. His current research interests
are in image processing, computer vision, biometrics, statistical machine
learning theory, sparsity, robotics, and neural networks applications to
image processing. He is also a Fellow of ARL, SPIE and IEEE.

Asok Ray (SM’83F’02) received the Ph.D. de-
gree in Mechanical Engineering from Northeast-
ern University, Boston, MA, and the graduate de-
grees in the disciplines of Electrical Engineering,
Mathematics, and Computer Science. He joined
The Pennsylvania State University (Penn State),
University Park, PA, in July 1985, and is cur-
rently a Distinguished Professor of Mechanical
Engineering and Mathematics, a Graduate Fac-
ulty of Electrical Engineering, and a Graduate
Faculty of Nuclear Engineering. Prior to joining
Penn State, he held research and academic positions with Massachusetts
Institute of Technology, Cambridge, MA, and Carnegie-Mellon University,
Pittsburgh, PA, as well as management and research positions with GTE
Strategic Systems Division, Westborough, MA, Charles Stark Draper
Laboratory, Cambridge, MA, and MITRE Corporation, Bedford, MA. Dr.
Ray has authored or coauthored over 550 research publications, including
over 285 scholarly articles in refereed journals and research monographs.
Dr. Ray is also a Fellow of the American Society of Mechanical Engineers
(ASME) and a Fellow of World Innovative Foundation (WIF). Dr. Ray had
been a Senior Research Fellow at NASA Glenn Research Center under a
National Academy of Sciences award.

W. Kenneth Jenkins received the B.S.E.E. de-
gree from Lehigh University and the M.S.E.E.
and Ph.D. degrees from Purdue University. From
1974 to 1977 he was a Research Scientist Asso-
ciate in the Communication Sciences Laboratory
at the Lockheed Research Laboratory, Palo Alto,
CA. In 1977 he joined the University of Illinois
at Urbana-Champaign where he was a faculty
member in Electrical and Computer Engineering
from 1977 until 1999. From 1986-1999 Dr. Jenk-
ins was the Director of the Coordinated Science
Laboratory. From 1999 through 2011 he served Professor and Head of
Electrical Engineering at Penn State University, and in 2011 he returned
to the rank of Professor of Electrical Engineering. Dr. Jenkins current
research interests include fault
tolerant DSP for highly scaled VLSI
systems, adaptive signal processing, multidimensional array processing,
computer imaging, bio-inspired optimization algorithms for intelligent
signal processing, and fault tolerant digital signal processing. He co-
authored the book Advanced Concepts in Adaptive Signal Processing,
published by Kluwer in 1996. He is a past Associate Editor for the
IEEE Transaction on Circuits and Systems, and a past President (1985) of
the CAS Society. He served as General Chairman of the 1988 Midwest
Symposium on Circuits and Systems and as the General Chairman of
the Thirty Second Annual Asilomar Conference on Signals and Systems.
From 2002 to 2007 he served on the Board of Directors of the Electrical
and Computer Engineering Department Heads Association (ECEDHA)
and as President of ECEDHA in 2005. Since January 2011 he has been
serving as a Member of the IEEE-HKN Board of Governors. Dr. Jenkins
is a Life Fellow of the IEEE and a recipient of the 1990 Distinguished
Service Award of the IEEE Circuits and Systems Society. In 2000 he
received a Golden Jubilee Medal from the IEEE Circuits and Systems
Society and a 2000 Millennium Award from the IEEE. In 2000 was named
a co-winner of the 2000 International Award of theGeorge Monteﬁore
Foundation (Belgium) for outstanding career contributions to the ﬁeld of
electrical engineering and electrical science, in 2002 he was awarded the
Shaler Area High School Distinguished Alumnus Award, in 2007 he was
honored with an IEEE Midwest Symposium on Circuits and Systems 50th
Anniversary Award, and in 2013 he received the ECEDHA Robert M.
Janowiak Outstanding Leadership and Service Award.

