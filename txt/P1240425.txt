Semantic Segmentation of Pathological Lung Tissue
with Dilated Fully Convolutional Networks

Marios Anthimopoulos, Member, IEEE, Stergios Christodoulidis, Member, IEEE, Lukas Ebner, Thomas Geiser,
Andreas Christe, and Stavroula Mougiakakou*, Member, IEEE

1

8
1
0
2
 
r
a

M
 
6
1
 
 
]

V
C
.
s
c
[
 
 
1
v
7
6
1
6
0
.
3
0
8
1
:
v
i
X
r
a

Abstract—Early and accurate diagnosis of interstitial

lung
diseases (ILDs) is crucial for making treatment decisions, but
can be challenging even for experienced radiologists. The diag-
nostic procedure is based on the detection and recognition of
the different ILD pathologies in thoracic CT scans, yet their
manifestation often appears similar. In this study, we propose
the use of a deep purely convolutional neural network for the
semantic segmentation of ILD patterns, as the basic component of
a computer aided diagnosis (CAD) system for ILDs. The proposed
CNN, which consists of convolutional layers with dilated ﬁlters,
takes as input a lung CT image of arbitrary size and outputs
the corresponding label map. We trained and tested the network
on a dataset of 172 sparsely annotated CT scans, within a cross-
validation scheme. The training was performed in an end-to-
end and semi-supervised fashion, utilizing both labeled and non-
labeled image regions. The experimental results show signiﬁcant
performance improvement with respect to the state of the art.

Index Terms—Interstitial

lung disease, Fully convolutional
neural networks, Dilated convolutions, Texture segmentation,
Semi-supervised learning

I. INTRODUCTION

I NTERSTITIAL lung disease (ILD) is a group of more than

200 chronic lung disorders characterized by inﬂammation
and scarring of the lung tissue that leads to respiratory failure.
ILD accounts for 15 percent of all cases seen by pulmonolo-
gists and can be caused by autoimmune disease, genetic abnor-
malities, infections, drugs or long-term exposure to hazardous
materials. In many cases the cause remains unknown and the

Manuscript received March 19, 2018
This research was carried out within the framework of the IntACT research
project, supported by Bern University Hospital,“Inselspital” and the Swiss
National Science Foundation (SNSF) under Grant 156511.

M. Anthimopoulos and S. Christodoulidis contributed equally to this work.

The asterisk indicates the corresponding author.

M. Anthimopoulos is with the ARTORG Center for Biomedical Engineering
Research, University of Bern, 3008 Bern, Switzerland, and the Department
of Emergency Medicine, Bern University Hospital “Inselspital”, 3010 Bern,
Switzerland (e-mail: marios.anthimopoulos@artorg.unibe.ch).

S. Christodoulidis is with the ARTORG Center for Biomedical Engi-
neering Research, University of Bern, 3008 Bern, Switzerland (e-mail: ster-
gios.christodoulidis@artorg.unibe.ch).

T. Geiser

is with the University Clinic for Pneumonology, Bern
(e-mail:

3010 Bern, Switzerland

“Inselspital”,

University Hospital
thomas.geiser@insel.ch)

S. Mougiakakou* is with the Department of Diagnostic,

L. Ebner and A. Christe are with the Department of Diagnostic, Interven-
tional and Pediatric Radiology, Bern University Hospital “Inselspital”, 3010
Bern, Switzerland (e-mails: lukas.ebner@insel.ch; andreas.christe@insel.ch).
Interven-
tional and Pediatric Radiology, Bern University Hospital “Inselspital”,
3010 Bern, Switzerland, and the ARTORG Center for Biomedical En-
gineering Research, University of Bern, 3008 Bern Switzerland (e-mail:
stavroula.mougiakakou@artorg.unibe.ch).

Source code available at: https://github.com/intact-project/LungNet

disease is described as idiopathic. The diagnosis of ILD is
mostly performed by radiologists and is usually based on the
assessment of the different ILD pathologies in high resolution
computed tomography (HRCT) thoracic scans. Early diagnosis
is crucial for making treatment decisions, while misdiagnosis
may lead to life-threatening complications [1]. Extensive re-
search has been conducted on the development of computer-
aided diagnosis (CAD) systems, which are able to support
clinicians and improve their diagnostic performance. The basic
characteristics of such a system are the automatic detection and
recognition of the pathological lung tissue. Pathological tissue
is usually manifested as various textural patterns in the CT
scan. This ILD pattern recognition procedure is traditionally
performed by a local texture classiﬁcation scheme that slides
across the images and outputs a map of pathologies, which is
later used to reach a ﬁnal diagnosis. For texture recognition,
a great variety of handcrafted image features and machine
learning classiﬁers have been utilized.

Recently, deep artiﬁcial neural networks (ANN) and, in
particular, deep convolutional neural networks (CNNs) have
gained a lot of attention after their impressive results in the
ImageNet Large Scale Visual Recognition Competition in
2012 [2]. Networks of this kind have existed for decades [3],
but have only recently managed to achieve adequate perfor-
mance mainly due to the large volumes of available annotated
data, the massive parallelization capabilities of GPUs and a
few design tricks. The potential beneﬁts of deep learning tech-
niques in medical image analysis have also been investigated
recently and the ﬁrst results have been promising [4]. In [5] ,
we designed, trained and tested a deep CNN as a ﬁxed-scale,
local texture classiﬁer that outperformed traditional methods
in ILD pattern classiﬁcation. However, training large networks
on medical images can often be challenging due to the lack
of databases that are adequately sized to satisfy the needs of
these models. Medical data are scarce and collecting them is
a difﬁcult and time consuming process, while their annotation
has to be performed by multiple specialists to ensure its
validity. To this end, in [6], we investigated the potential
use of general
in a multi-source
transfer learning scheme that yielded signiﬁcant improvements
in performance. One remaining limitation in these works is the
local nature of the classiﬁer that requires rigorous scanning
of the input image with a sliding window, and simultaneous
aggregation of the results in the output. This classiﬁcation
scheme can be signiﬁcantly time-consuming while it may also
ignore less local but useful information, if the input size is not
appropriately conﬁgured.

texture image databases,

In this study, we propose the use of a deep fully-
convolutional network for the problem of ILD pattern recog-
nition that uses dilated convolutions and is trained in an end-
to-end and semi-supervised manner. The proposed CNN takes
as input a lung HRCT image of arbitrary size and outputs
the corresponding label map, thus avoiding the limitations of
a sliding window model. Additionally, the utilization of non-
labeled image regions in the learning procedure, permits robust
training of larger models and proves to be particularly useful
when using databases with sparse annotations.

II. RELATED WORK

In this section, we provide a short review of the recent
advances in deep learning for computer vision, followed by a
brief overview of previous studies on ILD pattern classiﬁcation

A. Deep CNNs for Computer Vision

Although CNNs have existed for decades [3], they only
became widely popular after their remarkable success in the
ImageNet challenge of 2012 [2]. The winning approach of the
competition [7], also known as AlexNet, was a deep CNN
with ﬁve convolutional and three dense layers, each followed
by a rectiﬁed linear unit (ReLU), while increased strides
were used for the max pooling and convolution operations to
gradually down-sample the feature maps. Dropout [8] and data
augmentation were also utilized, in order to prevent overﬁtting.
Since then, the proposed deep CNNs have led to continuous
improvements in the results on ImageNet and other datasets,
mainly by enhancing their architecture and by increasing their
depth and width. The VGG network [9] reduced the size of
the kernels to 3×3, while increasing the number of layers
to 19. GoogleNet [10] used consecutive inception modules,
where different convolutional and pooling operations are per-
formed in parallel with their outputs merged. This approach
drastically reduced the number of parameters while improving
the results. ResNet [11] introduced skip connections between
layers which permitted the training of networks with hundreds
of layers and pushed the limits of deep CNNs even further. The
batch normalization (BN) technique [12] also supported these
developments, by regularizing and accelerating the training
procedure.

Many of the already proposed CNNs have recently been
adapted to perform semantic segmentation, rather than just
image classiﬁcation. The term semantic segmentation refers
to the task of assigning a class label to every pixel of an
image. A simple approach to this task is to use any ﬁxed-
scale classiﬁcation method under a sliding window scheme and
then to aggregate the results to build a label map. However,
this could be highly inefﬁcient as local image features would
have to be recalculated multiple times for adjacent positions of
the input window. Luckily, the convolutional layers (with the
appropriate padding) in a typical CNN produce feature maps
that maintain spatial correspondence with the input image.
Therefore, input images of any size can be fed to the network
and each pixel can be classiﬁed, on the basis of the values of
the respective feature map position. This can be achieved by
utilizing convolutional layers of size 1 × 1 that serve as local

2

(a) D = 1

(b) D = 2

(c) D = 3

Fig. 1: Dilated convolution kernels. D denotes the dilation rate

dense layers and, these networks are therefore often referred
to as fully convolutional (FCNs).

However, the spatial correspondence between the input and
output of a CNN, can be disrupted by the use of down-
sampling operations such, as strided pooling and convolution.
Down-sampling of the feature maps is often used to increase
the receptive ﬁeld of the network with respect to the input,
as well as to reduce the amount of computational load. In
order to restore the original size of the input, researchers
have used encoder-decoder architectures, where the encoder
usually adopts a well-known architecture such as VGG [9]
and the decoder reverses the process by mapping the feature
representation back to the input data space. To this end,
upsampling operations and transposed convolution [13] (also
known as fractionally strided convolution or “deconvolution”)
have been used for semantic segmentation [14]. Alternatively,
in [15] and [16], max unpooling has been used as the inverse
operation of each max pooling layer, where the pairs of
pooling/unpooling layers are coupled by transferring the max
indices from the encoder to the decoder. In [17], a similar
architecture was proposed for biomedical image segmentation,
with additional skip connections that concatenate the feature
maps of an encoding layer to the feature maps of the same-
scale decoding layer.

Recently, some CNNs for semantic segmentation have been
proposed that use dilated convolutions to increase the receptive
ﬁeld,
instead of downsampling the feature maps. Dilated
convolution, also called `atrous, is the convolution with kernels
that have been dilated by inserting zero holes (`atrous in
French) between the non-zero values of a kernel. This was
originally proposed for efﬁcient wavelet decomposition in a
scheme also known as “algorithme `atrous” [18]. Figure 1
shows examples of kernels with different dilation rates. Dilated
convolution can increase the receptive ﬁeld without increasing
the number of parameters, as opposed to normal convolution.
Moreover, feature maps are densely computed on the original
image resolution without the need for downsampling. In [19],
a CNN module with dilated convolutions was designed to ag-
gregate multiscale contextual information and improve the per-
formance of state-of-the-art semantic segmentation systems.
The module has eight convolutional layers with exponentially
increasing dilation rates (i.e. 1, 1, 2, 4, 8, 16), resulting in an
exponential increase in the receptive ﬁeld, while the number
of parameters is only grown linearly. Similarly, expansion
of the receptive ﬁeld was achieved in [20] by integrating
dilated convolutions in a bottleneck module that was designed
the `atrous spatial pyramid pooling
for efﬁciency. In [21],
(ASPP) scheme is proposed that uses multiple parallel dilated

convolutional layers, in order to capture information from
multiple scales.

B. ILD Pattern Classiﬁcation

Over the last twenty years, numerous approaches have been
proposed for the problem of ILD pattern recognition, which
is generally regarded as a texture classiﬁcation problem. Most
of the proposed methods involve hand-crafted texture features
which are fed to machine learning classiﬁers, and locally rec-
ognize lung tissue within a sliding window framework. In one
of the early studies [22], the adaptive multiple feature method
(AMFM) was proposed, which utilizes a combination of gray-
level histograms, co-occurrence and run-length matrices, as
well as fractal analysis parameters. For the classiﬁcation, a
Bayesian classiﬁer was used. In [23], a ﬁlter bank of Gaussian
and Laplacian kernels was applied on the input images and
the histogram moments of the responses were fed to a linear
discriminant classiﬁer. The simple, yet powerful, Local Binary
Pattern (LBP) descriptor has also been proposed [24], com-
bined with a k-nearest neighbors classiﬁer. In [25], a random
forest classiﬁer was utilized that was trained on local DCT
features. More recently, some proposed methods have adopted
unsupervised feature extraction techniques, such as bag of
features [26], [27] and sparse representation models [28], [29].
Lately, a few methods have been proposed that utilize CNNs
for lung pattern classiﬁcation. Most of these are still designed
under a patch-wise scheme where a square patch is fed to the
CNN, while the output consists of the probabilities for this
patch to belong to each class. Although the strong descriptive
capabilities of modern CNNs are commonly attributed to their
depth, the ﬁrst studies utilized rather shallow architectures.
A modiﬁed RBM that resembles a convolutional layer was
used in [30], whereas in [31], a CNN was proposed with
one convolutional and three fully-connected layers. More
recently, some attempts have also been made to utilize deeper
architectures. In our previous work [5], a CNN with ﬁve
convolutional and three dense layers was designed and trained
on ILD data, while in [6] its performance was improved using
knowledge transfer from other domains. In another study [32],
a CNN with three convolutional and one dense layer was fed
with rotational invariant Gabor-LBP representations of lung
tissue patches. Finally, in [33] and [34], the authors utilized
well established pretrained CNN architectures such as AlexNet
and GoogleNet which were further ﬁnetuned for detecting
possible “presence/absence” of pathologies at a slice level.
However, these architectures were designed to classify natural
color images with size 224 × 224, so the authors had to resize
the images and artiﬁcially generate three channels by applying
different Hounsﬁeld unit (HU) windows.

III. MATERIALS AND METHODS

This section presents the proposed fully convolutional neu-
ral network for semantic lung tissue segmentation. Prior to
this, we describe the materials used for training and testing
the network.

3

TABLE I: Data statistics across the considered classes i.e.
Healthy (H), Ground Glass Opacity (GGO), Micronodules
(MN), Consolidation (Cons), Reticulation (Ret) and Honey-
combing (HC).

H

GGO MN

Cons

Ret

HC

Totals

#Pixels×105
#Cases

92.5
66

27.7
82

35.8
15

7.08
46

28.2
81

20.1
47

211.4
172

A. Materials

For the purposes of this study, we compiled a dataset of 172
HRCT scans, each corresponding to a unique ILD or healthy
subject. The dataset contains 109 cases from the publicly
available multimedia database of interstitial lung diseases [35]
by the Geneva University Hospital (HUG), along with 63 cases
from Bern University Hospital - “Inselspital” (INSEL), as col-
lected by the authors. The scans were acquired between 2003
and 2015 using different scanners and acquisition protocols.
The INSEL scans are volumetric, while the HUG scans have
a 10-15mm spacing. The slice thickness is 1-2mm for both
datasets.

Two experienced radiologists from INSEL annotated or re-
annotated ILD typical pathological patterns, as well as healthy
tissue in both databases1. A lung ﬁeld segmentation mask
was also provided for each case. In total six types of tissue
were considered: normal, ground glass opacity, micronodules,
consolidation, reticulation and honeycombing. It should be
emphasized that these annotations do not cover the entire lung
ﬁeld, but only the most typical manifestations of the listed ILD
patterns. This protocol was followed in both databases since
it permits the annotation of more scans for the same effort
and thus increases data diversity. On the other hand, sparse
annotations also introduce challenges. Non-annotated lung
areas have to be excluded from both supervised training and
evaluation. Another challenging characteristic of the databases
is the uneven distribution of the considered classes across the
cases. Table I provides statistics for the entire dataset, while
ﬁgure 2 presents a sample CT lung slice along with the given
annotations.

B. Methods

In this study, we propose the use of a deep purely con-
volutional network for the problem of lung tissue semantic
segmentation. The network is inspired by [19] and consists of
solely convolutional layers that use dilated kernels to increase
the receptive ﬁeld, instead of downsampling the feature maps.
This kind of network has been shown to be suitable for
similar dense prediction problems that require high resolution
precision. The proposed network (Fig. 3) has 13 convolutional
layers and a total receptive ﬁeld of 287×287. Speciﬁcally, each
of the ﬁrst ten layers has 32 kernels of size 3×3 and dilation
rates 1, 1, 2, 3, 5, 8, 13, 21, 34 and 55, respectively. We
chose not to increase the dilation rates exponentially, as is
commonly done, in order to avoid extreme gridding problems
that have been reported in several studies [36], [37]. Instead,

1ITK-snap was used for the annotation process, http://www.itksnap.org

4

ReLU function (Fig. 4) substantially improves the results. This
instance normalization skip connection cancels the mean nor-
malization of activations (when the trainable parameters have
not been trained), while it performs a kind of feature contrast
enhancement which reduces the importance of variance shift
without providing complete invariance to the latter.

The network was trained by minimizing the categorical
cross entropy using the Adam optimizer [40] with a learning
rate of 0.0001. The dense nature of the considered classiﬁca-
tion problem combined with the sparse available annotations,
resulted in two issues. Firstly, large parts of the dataset were
not annotated, and so could not be used for either supervised
training or testing. Secondly, the distribution of the considered
classes in the dataset was highly imbalanced, a fact
that
can be challenging for any classiﬁcation method. We tackled
both problems by scaling the considered loss and accuracy
with appropriate weighting schemes computed for each set.
All pixels corresponding to annotated areas were assigned a
weight inversely proportional to the number of samples of its
class in the speciﬁc set. In this way, all classes contributed
equally to the considered metrics. Furthermore, we employed
a semi-supervised learning technique to additionally exploit
non-labeled areas of the data. We added an extra term to the
supervised loss function, which corresponds to the entropy of
the network’s output on the areas that do not participate in
the supervised learning. This entropy minimization technique
has been used in different applications such as in
[41]
yielding signiﬁcant improvements in performance. Similarly in
[42] the technique of pseudo-labelling was introduced, where
the network classiﬁes non-annotated regions and then uses
them as ground truth for ﬁne-tuning. Semi-supervised learning
techniques of this kind are based on the cluster assumption i.e.
samples from the same class tend to form compact clusters. By
minimizing the entropy of the network’s output, the decision
boundaries are driven away from areas densely populated by
learning samples. If the cluster assumption holds and there is
no large overlap between the classes this method may increase
the network’s generalization ability. It acts equivalently to
manifold learning and includes self-learning as a special case,
as it increases the conﬁdence of the classiﬁer. The inﬂuence
of the semi-supervised term is controlled by an appropriate
weight, which is scaled relatively with the proportion of the
unlabeled regions versus the annotated ones. Hence, the loss
for a pixel x with output ˆy is:

L(x, ˆy) =






− (cid:80)C

i=1 wi

syi log( ˆyi), when y is given

(1)

(cid:80)C

−αwu

i=1 ˆyi log( ˆyi),
where y is the true label in one-hot encoding, C is the
number of classes, wi
s is the supervised weight for class i
(which is inversely proportional to the number of samples of
the class), α is a scaler and wu the unsupervised weight.

otherwise

The training procedure stops when the network does not
signiﬁcantly improve its performance on the validation set for
50 epochs. The performance is assessed in terms of weighted
is considered
(balanced) accuracy, while an improvement
signiﬁcant if the relative increase in performance is at least

Fig. 2: A typical slice with annotations. The white border line
denotes the lung ﬁeld segmentation, the blue denotes healthy
tissue, the purple micronodules and the red the honeycombing
pattern.

we use the ﬁrst terms of the Fibonacci sequence as dilation
rates; this mitigates the gridding problem by providing a less
steep dilation rate increase and thus denser sampling.

The output of the ﬁrst 10 layers, as well as the input
of the network, are concatenated, thus leading to 1+10×32
= 321 feature maps, which are passed through a dropout
layer with a rate of 0.5 and fed to the rest of the network.
This concatenation is allowed by the lack of pooling layers
and the appropriate zero padding for each convolution and
brings several beneﬁts. It permits the aggregation of features
from all different scales and levels of abstraction, while it
also facilitates the ﬂow of gradients thought the network and
therefore allows faster training. The last three layers have 1×1
kernels and play the role of locally dense layers that reduce
the feature dimensionality for each pixel from 321 to 128,
32 and ﬁnally 6, which is the number of classes considered.
The output is converted into a probability distribution by the
softmax function.

A BN layer follows each convolution and is based on the
batch statistics in both training and test time. This is permitted,
as the batch size is one, so there is always a full batch during
inference. This approach has been proposed before, under
the term instance normalization (InstanceNorm) [38], and
has exhibited good performance in texture synthesis, image
stylization and image to image translation [39]. InstanceNorm
provides invariance to intensity and contrast shifts, which
makes the features adaptive for each slice and could mitigate
problems caused by different CT scanners and reconstruction
kernels. We also found that adding the normalized activations
to the non-normalized ones, before passing them through the

InstanceNorm

Input

+

(3x3), D@1

(3x3), D@1

(3x3), D@2

(3x3), D@3

(3x3), D@5

(3x3), D@8

(3x3), D@13

(3x3), D@21

(3x3), D@34

(3x3), D@55

Concatenate

Dropout

(1x1), D@1

(1x1), D@1

(1x1), D@1

Softmax Output

Fig. 3: The architecture of the proposed network. Each gray
box corresponds to a block like the one presented in Fig. 4

0.5%. In order to artiﬁcially increase the volume of training
data and avoid overﬁtting, we transformed the images using
ﬂips and rotations, which are considered label-preserving in
this domain. The augmentation was performed online i.e. for
each training image in each epoch, one operation out of all
eight combinations of ﬂip and rotate is randomly selected and
applied.

5

Dilated Convolution
(k,k), D@x

InstanceNorm

+

ReLU

Fig. 4: The block function of the proposed architecture. (k, k)
is the size of the convolution kernel and x is the dilation rate.

IV. EXPERIMENTAL SETUP AND RESULTS

In this section, we ﬁrst present the setup of the experiments
conducted, followed by the corresponding results that justify
the algorithmic choices of the proposed method and compare
it to the state of the art.

A. Experimental Setup

Given the relatively small size of the dataset with respect
to the diversity of the problem, we adopted a 5-fold cross
validation (CV) scheme to ensure the validity of the results.
The data splitting was performed per scan, so tissue from one
case was never present in more than one set. Speciﬁcally, the
172 scans of the dataset were divided into ﬁve non-overlapping
sets, with one of them having 36 and the rest 34 scans. Every
time a model was tested on a speciﬁc set, the rest of the data
were used for training. On average over all folds, the number
of slices was 2060 for training and 515 for testing. As principal
performance metric, we used the balanced accuracy (Eq. 2),
averaged over the ﬁve folds.

BACC =

1
N

N
(cid:88)

i=1

ci
ni

(2)

where N is the number of classes, ci is the number of
correctly classiﬁed samples of class i and ni
is the total
number of samples of class i. Since the slices were only
sparsely annotated, the accuracy was calculated over the areas
of the scans where a ground truth was available.

In order to avoid extreme class imbalances between the
different sets, data splitting was performed using a simple hill
climbing technique that maximizes the entropy of the class
distribution for the ﬁve sets. The methods started from an
arbitrary split and then randomly swapped two cases between
two sets in an attempt to ﬁnd a more balanced solution. If the
new solution has a higher class distribution entropy (averaged
over the 5 sets), we retained it and repeated the procedure
until no further improvement was possible.

To minimize the number of computations and memory re-
quirements, we discarded part of the data that lack annotations.
Hence, we cropped the left and right lung on each slice and
used only the ones with relevant annotations as inputs of the

TABLE II: Comparison of the different network conﬁgurations

Network
conﬁguration

Number of
parameters
×105

Average
inference time
ms

CV balanced
accuracy
%

w/o dilated convolutions
w/o concatenation
w/o InstanceNorm
w/o InstanceNorm skip
16 kernels/layer
Exponential dilation [19]
Purely supervised
9 dilated layers
Proposed
64 kernels/layer

1.30
0.93
1.29
1.30
0.47
1.03
1.30
1.18
1.30
4.23

51
53
38
57
51
48
58
53
58
82

68.0
72.6
77.9
78.6
79.2
79.5
80.6
81.3
81.8
82.1

networks. This is permitted by the fully-convolutional nature
of the tested networks that do not require a ﬁxed input size.
For the cropping, we utilized the available lung mask, while a
margin of 32 pixels was added on each side to provide context
that could be useful to the networks.

The proposed method was implemented in Python2 using
the Keras framework3 with the Theano [43] back-end. All
experiments were performed under Linux OS on a machine
with CPU Intel Core i7-5960X @ 3.50GHz, GPU NVIDIA
GeForce Titan X, and 128 GB of RAM.

B. Results

Table II presents a comparison between different network
conﬁgurations. The bold line corresponds to the proposed
CNN, while the rest correspond to models that differ from
the proposed in only one aspect, as speciﬁed in the ﬁrst
column. The rest of the columns provide the number of model
parameters, the average inference time per (single-lung) slice,
and the average balanced accuracy across the ﬁve validation
sets.

The proposed model achieved top performance with accu-
racy nearly equal to 82% and inference time 58ms. The use
of 64 kernels per layer instead of 32 did indeed improve the
results, yet not signiﬁcantly enough and with higher inference
times, whereas the network with 16 kernels performed notably
worse. On reducing the dilated convolutional layers from 10
to 9, we observed a relatively small reduction in the accuracy.
However, we chose to keep 10 layers, since the difference in
memory and time requirements was also small and because
the resulting receptive ﬁeld was comparable with that of the
state of the art networks used for comparison. The use of semi-
supervised learning yielded an improvement of nearly 1.5%,
with no additional requirements in computational resources.
We also performed an experiment with exponential increase in
the dilation rates of the consecutive layers, similarly to [19]
i.e. 1, 1, 2, 4, 8, 16, 32 and 64. The resulting model was
smaller and faster, since 2 fewer layers were required to
achieve similar receptive ﬁeld, however the accuracy decreased
by almost 3%. In the case where the convolutions were not
dilated, the network performed poorly, because of the radical
decrease of the receptive ﬁeld. The accuracy of the proposed

2https://github.com/intact-project/LungNet
3https://github.com/fchollet/keras

6

Fig. 5: Accuracy curves for different values of wu.

TABLE III: Comparison with previous studies

Network

Number of
parameters
×105

Average
inference time
ms

CV balanced
accuracy
%

ILD-CNN [5]
Segnet [15]
U-net [17]

Proposed

0.9
335
310

1.3

237
111
88

58

72.2
73.6
77.5

81.8

model without any normalization was substantially poorer,
probably because it could not properly handle the contrast
differences among the scans caused by different CT scanners
and reconstruction kernels. The use of instance normaliza-
tion improved the performance by adaptively normalizing
the feature contrast for each input. However, this kind of
normalization also normalizes the mean intensity that could be
a useful feature. By adding the InstanceNorm skip connection
(Fig. 4), the accuracy improved even further. We speculate this
is because the mean normalization is diminished, while the
resulting variance normalization is only partially invariant to
contrast shifts. Finally, omitting the concatenation of the ﬁrst
10 layers also resulted in signiﬁcant impairment of the results,
which was expected since only 32 features are considered.

In Fig. 5 the accuracy curves for different values of wu
are presented. These curves are generated by averaging over
the ﬁve folds the best accuracies achieved this far by each
model in each epoch. The curve for the model without the
unsupervised learning was also included for comparison. The
best performing conﬁguration proved to be the one with wu =
0.1, which we utilized for the training of the proposed model.
Table III presents a comparison between the proposed
network and three previous studies. It has to be noted that all
models used the same unsupervised weight (wu = 0.1) and
whenever batch normalization was performed, this was based
on batch statistics (instance normalization) since this yielded
the best results. Fig. 7 illustrates a few segmentation results
for each of the models in Table III.

The ﬁrst line of the table refers to our previous work [5],
which has been converted into a fully convolutional network
so it can accept arbitrarily sized images for input. Its low
accuracy is probably due to the small receptive ﬁeld (33×33)
and the extensive pooling. This architecture was sufﬁcient to
describe the local texture of the 32×32 single-class patches
in [5], but could not capture higher level structure that is
present in the whole-lung dataset of this study. The results
of the model in Fig. 7 show its noisy output near the lung
boundaries or between patterns, where context information
could be useful. Segnet [15] and U-net [17] yielded better
results, with the latter being slightly faster and substantially
more accurate. Both models have a very high number of
parameters and large enough receptive ﬁelds to capture any
relevant information. The superior performance of U-net could
be attributed to its skip connections that allow features from
the lower scales to directly contribute to its output. Indeed,
Fig. 7 illustrates the more detailed results of U-net as opposed
to the overly smoothed areas produced by Segnet. Finally, the
proposed network yielded the best results, while being faster
and having far fewer parameters. The output examples in Fig. 7
indicate that the proposed model manages to keep a better
balance between ﬁne details and smooth border among the
different classes. Even thought it is really difﬁcult to visually
assess the performance of the system for the different classes,
there are a few examples in Fig. 7 with wrong classiﬁcations
on which we can comment. Firstly, parts of the broncho-
vascular tree in the third row were recognized as consolidation
because of their similar densities, while accentuated terminal
bronchial parts, that might be physiological as well, caused
the erroneous classiﬁcation of healthy areas into reticulation,
in the ﬁrst row. Some mistakes however are also attributed in
the limited number of annotated classes. For example in row
6, there are emphysematic areas (dark area in the center of the
lung) that have been annotated as healthy due to their similar
density. Figure 6 shows the confusion matrix of the proposed
model. As expected, many of the misclassiﬁcations occur
between reticulation and honeycombing due to their similar
textural appearance. Moreover, healthy tissue is often confused
with reticulation probably because of the 2D sections of the
bronchovascular tree that could resemble reticular patterns.

V. CONCLUSIONS

In this study, we proposed and evaluated a deep CNN
for the semantic segmentation of pathological lung tissue on
HRCT slices. The CNN is designed under a fully convo-
lutional scheme and thus can handle variable input sizes,
while it was trained in an end-to-end and semi-supervised
fashion. The main characteristic of the proposed network is
the use of dilated convolutions along with an instance variance
normalization scheme, and multi-scale feature fusion. The
training and testing of the network was performed using a
cross validation scheme on a dataset of 172 cases, whereas
the split of the dataset into folds was performed per case. The
proposed network surpassed the highest performance in pre-
vious studies, and is much more efﬁcient in terms of memory
and computation. Future work includes the modiﬁcation of

7

Fig. 6: Confusion matrix of the proposed model as calculated
over the cross validation scheme. The numbers represent
percentages of pixels across all validation images.

the model to consider the 3D nature of lung patterns, and
to account for the bronchovascular tree. The former could
be achieved by a direct extension of the architecture to 3D,
similarly to 3D U-Net [44] and V-Net [45] or by employing
a multi-planar view aggregation scheme, also referred to as
2.5D, [46]. Alternatively, a 3D post processing scheme could
be used to reﬁne the 2D segmentation output using conditional
random ﬁelds or deformation models [47], [48], [49]. Finally,
the result of a bronchovascular segmentation method could be
utilized by the network to reduce false alarms.

REFERENCES

[1] B. SOCIETY, “The diagnosis, assessment and treatment of diffuse
parenchymal lung disease in adults,” Thorax, vol. 54, no. Suppl 1, p. S1,
1999.

[2] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp.
211–252, 2015.

[3] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, Nov 1998.

[4] H. Greenspan, B. van Ginneken, and R. M. Summers, “Guest editorial
deep learning in medical imaging: Overview and future promise of
an exciting new technique,” IEEE Transactions on Medical Imaging,
vol. 35, no. 5, pp. 1153–1159, 2016.

[5] M. Anthimopoulos, S. Christodoulidis, L. Ebner, A. Christe, and
S. Mougiakakou, “Lung pattern classiﬁcation for interstitial lung dis-
eases using a deep convolutional neural network,” IEEE Transactions
on Medical Imaging, vol. 35, no. 5, pp. 1207–1216, May 2016.

[6] S. Christodoulidis, M. Anthimopoulos, L. Ebner, A. Christe, and
S. Mougiakakou, “Multisource transfer learning with convolutional
neural networks for lung pattern analysis,” IEEE Journal of Biomedical
and Health Informatics, vol. 21, no. 1, pp. 76–84, Jan 2017.

[7] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.
[8] N. Srivastava, G. Hinton, A. Krizhevsky,

and
R. Salakhutdinov, “Dropout: A simple way to prevent neural
networks from overﬁtting,” J. Mach. Learn. Res., vol. 15, no. 1, pp.
1929–1958, Jan. 2014.

I. Sutskever,

[9] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.

8

Fig. 7: Output examples for the models of Table III. From left to right: Ground Truth, ILD-CNN, Segnet, U-net, Proposed. Each
example has a different pattern annotated. From top to bottom: Healthy (Blue), Ground Glass Opacity (Purple), Micronodules
(Green), Consolidation (Yellow), Reticulation (Orange) and Honeycombing (Red).

9

[32] Q. Wang, Y. Zheng, G. Yang, W. Jin, X. Chen, and Y. Yin, “Multi-
scale rotation-invariant convolutional neural networks for lung texture
classiﬁcation,” IEEE Journal of Biomedical and Health Informatics,
vol. PP, no. 99, pp. 1–1, 2017.

[33] H. C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao,
D. Mollura, and R. M. Summers, “Deep convolutional neural networks
for computer-aided detection: Cnn architectures, dataset characteristics
and transfer learning,” IEEE Transactions on Medical Imaging, vol. 35,
no. 5, pp. 1285–1298, May 2016.

[34] M. Gao, Z. Xu, L. Lu, A. P. Harrison, R. M. Summers, and D. J. Mollura,
“Holistic interstitial lung disease detection using deep convolutional
neural networks: Multi-label learning and unordered pooling,” arXiv
preprint arXiv:1701.05616, 2017.

[35] A. Depeursinge, A. Vargas, A. Platon, A. Geissbuhler, P.-A. Poletti, and
H. M¨uller, “Building a reference multimedia database for interstitial lung
diseases,” Computerized medical imaging and graphics, vol. 36, no. 3,
pp. 227–238, 2012.

[36] P. Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, and
G. W. Cottrell, “Understanding convolution for semantic segmentation,”
CoRR, vol. abs/1702.08502, 2017.

[37] F. Yu, V. Koltun, and T. A. Funkhouser, “Dilated residual networks,”

CoRR, vol. abs/1705.09914, 2017.

[38] D. Ulyanov, A. Vedaldi, and V. S. Lempitsky, “Improved texture
networks: Maximizing quality and diversity in feed-forward stylization
and texture synthesis,” CoRR, vol. abs/1701.02096, 2017.

[39] P. Isola, J. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
with conditional adversarial networks,” CoRR, vol. abs/1611.07004,
2016.

[40] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

CoRR, vol. abs/1412.6980, 2014.

[41] Y. Grandvalet and Y. Bengio, “Semi-supervised learning by entropy
minimization,” in Proceedings of the 17th International Conference
on Neural Information Processing Systems, ser. NIPS’04. Cambridge,
MA, USA: MIT Press, 2004, pp. 529–536.

[42] D.-H. Lee, “Pseudo-label: The simple and efﬁcient semi-supervised
learning method for deep neural networks,” in Workshop on Challenges
in Representation Learning, ICML, vol. 3, 2013, p. 2.

[44]

[43] Theano Development Team, “Theano: A Python framework for
fast computation of mathematical expressions,” arXiv e-prints, vol.
abs/1605.02688, May 2016.
¨O. C¸ ic¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,
“3d u-net: learning dense volumetric segmentation from sparse anno-
tation,” in International Conference on Medical Image Computing and
Computer-Assisted Intervention. Springer, 2016, pp. 424–432.
[45] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional
neural networks for volumetric medical image segmentation,” in 3D
Vision (3DV), 2016 Fourth International Conference on.
IEEE, 2016,
pp. 565–571.

[46] H. R. Roth, L. Lu, J. Liu, J. Yao, A. Seff, K. Cherry, L. Kim, and R. M.
Summers, “Improving computer-aided detection using convolutional
neural networks and random view aggregation,” IEEE transactions on
medical imaging, vol. 35, no. 5, pp. 1170–1181, 2016.

[47] F. Liu, Z. Zhou, H. Jang, A. Samsonov, G. Zhao, and R. Kijowski,
“Deep convolutional neural network and 3d deformable approach for
tissue segmentation in musculoskeletal magnetic resonance imaging,”
Magnetic resonance in medicine, vol. 79, no. 4, pp. 2379–2391, 2018.
[48] P. F. Christ, M. E. A. Elshaer, F. Ettlinger, S. Tatavarty, M. Bickel,
P. Bilic, M. Rempﬂer, M. Armbruster, F. Hofmann, M. DAnastasi et al.,
“Automatic liver and lesion segmentation in ct using cascaded fully
convolutional neural networks and 3d conditional random ﬁelds,” in
International Conference on Medical Image Computing and Computer-
Assisted Intervention. Springer, 2016, pp. 415–423.

[49] K. Kamnitsas, C. Ledig, V. F. Newcombe, J. P. Simpson, A. D. Kane,
D. K. Menon, D. Rueckert, and B. Glocker, “Efﬁcient multi-scale 3d
cnn with fully connected crf for accurate brain lesion segmentation,”
Medical image analysis, vol. 36, pp. 61–78, 2017.

[10] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2015, pp. 1–9.

[11] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2016, pp. 770–778.

[12] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” CoRR, vol.
abs/1502.03167, 2015.

[13] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-
tional networks,” in European conference on computer vision. Springer,
2014, pp. 818–833.

[14] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2015, pp. 3431–3440.
[15] V. Badrinarayanan, A. Handa, and R. Cipolla, “Segnet: A deep con-
volutional encoder-decoder architecture for robust semantic pixel-wise
labelling,” arXiv preprint arXiv:1505.07293, 2015.

[16] H. Noh, S. Hong, and B. Han, “Learning deconvolution network
for semantic segmentation,” in Proceedings of the IEEE International
Conference on Computer Vision, 2015, pp. 1520–1528.

[17] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” CoRR, vol. abs/1505.04597, 2015.
[18] M. Holschneider, R. Kronland-Martinet, J. Morlet, and P. Tchamitchian,
“A real-time algorithm for signal analysis with the help of the wavelet
transform,” in Wavelets. Springer, 1990, pp. 286–297.

[19] F. Yu and V. Koltun, “Multi-scale context aggregation by dilated

convolutions,” CoRR, vol. abs/1511.07122, 2015.

[20] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A deep
neural network architecture for real-time semantic segmentation,” CoRR,
vol. abs/1606.02147, 2016.

[21] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille, “Deeplab: Semantic image segmentation with deep convolutional
nets, atrous convolution, and fully connected crfs,” arXiv preprint
arXiv:1606.00915, 2016.

[22] R. Uppaluri, E. A. Hoffman, M. Sonka, P. G. Hartley, G. W. Hunning-
hake, and G. McLennan, “Computer recognition of regional lung disease
patterns,” American journal of respiratory and critical care medicine,
vol. 160, no. 2, pp. 648–654, 1999.

[23] I. C. Sluimer, P. F. van Waes, M. A. Viergever, and B. van Ginneken,
“Computer-aided diagnosis in high resolution ct of the lungs,” Medical
physics, vol. 30, no. 12, pp. 3081–3090, 2003.

[24] L. Sorensen, S. B. Shaker, and M. De Bruijne, “Quantitative analysis of
pulmonary emphysema using local binary patterns,” IEEE transactions
on medical imaging, vol. 29, no. 2, pp. 559–569, 2010.

[25] M. Anthimopoulos, S. Christodoulidis, A. Christe, and S. Mougiakakou,
“Classiﬁcation of interstitial lung disease patterns using local dct features
and random forest,” in 2014 36th Annual International Conference of
the IEEE Engineering in Medicine and Biology Society, Aug 2014, pp.
6040–6043.

[26] M. Gangeh, L. Sørensen, S. Shaker, M. Kamel, M. De Bruijne, and
M. Loog, “A texton-based approach for the classiﬁcation of lung
parenchyma in ct images,” Medical Image Computing and Computer-
Assisted Intervention–MICCAI 2010, pp. 595–602, 2010.

[27] A. Foncubierta-Rodr´ıguez, A. Depeursinge, and H. M¨uller, “Using
multiscale visual words for lung texture classiﬁcation and retrieval,” in
MICCAI International Workshop on Medical Content-Based Retrieval
for Clinical Decision Support. Springer, 2011, pp. 69–79.

[28] W. Zhao, R. Xu, Y. Hirano, R. Tachibana, and S. Kido, “Classiﬁcation of
diffuse lung diseases patterns by a sparse representation based method on
hrct images,” in Engineering in Medicine and Biology Society (EMBC),
2013 35th Annual International Conference of the IEEE.
IEEE, 2013,
pp. 5457–5460.

[29] K. T. Vo and A. Sowmya, “Multiscale sparse representation of high-
resolution computed tomography (hrct) lung images for diffuse lung
disease classiﬁcation,” in Image Processing (ICIP), 2011 18th IEEE
International Conference on.

IEEE, 2011, pp. 441–444.

[30] G. van Tulder and M. de Bruijne, “Learning features for tissue classi-
ﬁcation with the classiﬁcation restricted boltzmann machine,” in Inter-
national MICCAI Workshop on Medical Computer Vision.
Springer,
2014, pp. 47–58.

[31] Q. Li, W. Cai, X. Wang, Y. Zhou, D. D. Feng, and M. Chen, “Med-
ical image classiﬁcation with convolutional neural network,” in 2014
13th International Conference on Control Automation Robotics Vision
(ICARCV), Dec 2014, pp. 844–848.

Semantic Segmentation of Pathological Lung Tissue
with Dilated Fully Convolutional Networks

Marios Anthimopoulos, Member, IEEE, Stergios Christodoulidis, Member, IEEE, Lukas Ebner, Thomas Geiser,
Andreas Christe, and Stavroula Mougiakakou*, Member, IEEE

1

8
1
0
2
 
r
a

M
 
6
1
 
 
]

V
C
.
s
c
[
 
 
1
v
7
6
1
6
0
.
3
0
8
1
:
v
i
X
r
a

Abstract—Early and accurate diagnosis of interstitial

lung
diseases (ILDs) is crucial for making treatment decisions, but
can be challenging even for experienced radiologists. The diag-
nostic procedure is based on the detection and recognition of
the different ILD pathologies in thoracic CT scans, yet their
manifestation often appears similar. In this study, we propose
the use of a deep purely convolutional neural network for the
semantic segmentation of ILD patterns, as the basic component of
a computer aided diagnosis (CAD) system for ILDs. The proposed
CNN, which consists of convolutional layers with dilated ﬁlters,
takes as input a lung CT image of arbitrary size and outputs
the corresponding label map. We trained and tested the network
on a dataset of 172 sparsely annotated CT scans, within a cross-
validation scheme. The training was performed in an end-to-
end and semi-supervised fashion, utilizing both labeled and non-
labeled image regions. The experimental results show signiﬁcant
performance improvement with respect to the state of the art.

Index Terms—Interstitial

lung disease, Fully convolutional
neural networks, Dilated convolutions, Texture segmentation,
Semi-supervised learning

I. INTRODUCTION

I NTERSTITIAL lung disease (ILD) is a group of more than

200 chronic lung disorders characterized by inﬂammation
and scarring of the lung tissue that leads to respiratory failure.
ILD accounts for 15 percent of all cases seen by pulmonolo-
gists and can be caused by autoimmune disease, genetic abnor-
malities, infections, drugs or long-term exposure to hazardous
materials. In many cases the cause remains unknown and the

Manuscript received March 19, 2018
This research was carried out within the framework of the IntACT research
project, supported by Bern University Hospital,“Inselspital” and the Swiss
National Science Foundation (SNSF) under Grant 156511.

M. Anthimopoulos and S. Christodoulidis contributed equally to this work.

The asterisk indicates the corresponding author.

M. Anthimopoulos is with the ARTORG Center for Biomedical Engineering
Research, University of Bern, 3008 Bern, Switzerland, and the Department
of Emergency Medicine, Bern University Hospital “Inselspital”, 3010 Bern,
Switzerland (e-mail: marios.anthimopoulos@artorg.unibe.ch).

S. Christodoulidis is with the ARTORG Center for Biomedical Engi-
neering Research, University of Bern, 3008 Bern, Switzerland (e-mail: ster-
gios.christodoulidis@artorg.unibe.ch).

T. Geiser

is with the University Clinic for Pneumonology, Bern
(e-mail:

3010 Bern, Switzerland

“Inselspital”,

University Hospital
thomas.geiser@insel.ch)

S. Mougiakakou* is with the Department of Diagnostic,

L. Ebner and A. Christe are with the Department of Diagnostic, Interven-
tional and Pediatric Radiology, Bern University Hospital “Inselspital”, 3010
Bern, Switzerland (e-mails: lukas.ebner@insel.ch; andreas.christe@insel.ch).
Interven-
tional and Pediatric Radiology, Bern University Hospital “Inselspital”,
3010 Bern, Switzerland, and the ARTORG Center for Biomedical En-
gineering Research, University of Bern, 3008 Bern Switzerland (e-mail:
stavroula.mougiakakou@artorg.unibe.ch).

Source code available at: https://github.com/intact-project/LungNet

disease is described as idiopathic. The diagnosis of ILD is
mostly performed by radiologists and is usually based on the
assessment of the different ILD pathologies in high resolution
computed tomography (HRCT) thoracic scans. Early diagnosis
is crucial for making treatment decisions, while misdiagnosis
may lead to life-threatening complications [1]. Extensive re-
search has been conducted on the development of computer-
aided diagnosis (CAD) systems, which are able to support
clinicians and improve their diagnostic performance. The basic
characteristics of such a system are the automatic detection and
recognition of the pathological lung tissue. Pathological tissue
is usually manifested as various textural patterns in the CT
scan. This ILD pattern recognition procedure is traditionally
performed by a local texture classiﬁcation scheme that slides
across the images and outputs a map of pathologies, which is
later used to reach a ﬁnal diagnosis. For texture recognition,
a great variety of handcrafted image features and machine
learning classiﬁers have been utilized.

Recently, deep artiﬁcial neural networks (ANN) and, in
particular, deep convolutional neural networks (CNNs) have
gained a lot of attention after their impressive results in the
ImageNet Large Scale Visual Recognition Competition in
2012 [2]. Networks of this kind have existed for decades [3],
but have only recently managed to achieve adequate perfor-
mance mainly due to the large volumes of available annotated
data, the massive parallelization capabilities of GPUs and a
few design tricks. The potential beneﬁts of deep learning tech-
niques in medical image analysis have also been investigated
recently and the ﬁrst results have been promising [4]. In [5] ,
we designed, trained and tested a deep CNN as a ﬁxed-scale,
local texture classiﬁer that outperformed traditional methods
in ILD pattern classiﬁcation. However, training large networks
on medical images can often be challenging due to the lack
of databases that are adequately sized to satisfy the needs of
these models. Medical data are scarce and collecting them is
a difﬁcult and time consuming process, while their annotation
has to be performed by multiple specialists to ensure its
validity. To this end, in [6], we investigated the potential
use of general
in a multi-source
transfer learning scheme that yielded signiﬁcant improvements
in performance. One remaining limitation in these works is the
local nature of the classiﬁer that requires rigorous scanning
of the input image with a sliding window, and simultaneous
aggregation of the results in the output. This classiﬁcation
scheme can be signiﬁcantly time-consuming while it may also
ignore less local but useful information, if the input size is not
appropriately conﬁgured.

texture image databases,

In this study, we propose the use of a deep fully-
convolutional network for the problem of ILD pattern recog-
nition that uses dilated convolutions and is trained in an end-
to-end and semi-supervised manner. The proposed CNN takes
as input a lung HRCT image of arbitrary size and outputs
the corresponding label map, thus avoiding the limitations of
a sliding window model. Additionally, the utilization of non-
labeled image regions in the learning procedure, permits robust
training of larger models and proves to be particularly useful
when using databases with sparse annotations.

II. RELATED WORK

In this section, we provide a short review of the recent
advances in deep learning for computer vision, followed by a
brief overview of previous studies on ILD pattern classiﬁcation

A. Deep CNNs for Computer Vision

Although CNNs have existed for decades [3], they only
became widely popular after their remarkable success in the
ImageNet challenge of 2012 [2]. The winning approach of the
competition [7], also known as AlexNet, was a deep CNN
with ﬁve convolutional and three dense layers, each followed
by a rectiﬁed linear unit (ReLU), while increased strides
were used for the max pooling and convolution operations to
gradually down-sample the feature maps. Dropout [8] and data
augmentation were also utilized, in order to prevent overﬁtting.
Since then, the proposed deep CNNs have led to continuous
improvements in the results on ImageNet and other datasets,
mainly by enhancing their architecture and by increasing their
depth and width. The VGG network [9] reduced the size of
the kernels to 3×3, while increasing the number of layers
to 19. GoogleNet [10] used consecutive inception modules,
where different convolutional and pooling operations are per-
formed in parallel with their outputs merged. This approach
drastically reduced the number of parameters while improving
the results. ResNet [11] introduced skip connections between
layers which permitted the training of networks with hundreds
of layers and pushed the limits of deep CNNs even further. The
batch normalization (BN) technique [12] also supported these
developments, by regularizing and accelerating the training
procedure.

Many of the already proposed CNNs have recently been
adapted to perform semantic segmentation, rather than just
image classiﬁcation. The term semantic segmentation refers
to the task of assigning a class label to every pixel of an
image. A simple approach to this task is to use any ﬁxed-
scale classiﬁcation method under a sliding window scheme and
then to aggregate the results to build a label map. However,
this could be highly inefﬁcient as local image features would
have to be recalculated multiple times for adjacent positions of
the input window. Luckily, the convolutional layers (with the
appropriate padding) in a typical CNN produce feature maps
that maintain spatial correspondence with the input image.
Therefore, input images of any size can be fed to the network
and each pixel can be classiﬁed, on the basis of the values of
the respective feature map position. This can be achieved by
utilizing convolutional layers of size 1 × 1 that serve as local

2

(a) D = 1

(b) D = 2

(c) D = 3

Fig. 1: Dilated convolution kernels. D denotes the dilation rate

dense layers and, these networks are therefore often referred
to as fully convolutional (FCNs).

However, the spatial correspondence between the input and
output of a CNN, can be disrupted by the use of down-
sampling operations such, as strided pooling and convolution.
Down-sampling of the feature maps is often used to increase
the receptive ﬁeld of the network with respect to the input,
as well as to reduce the amount of computational load. In
order to restore the original size of the input, researchers
have used encoder-decoder architectures, where the encoder
usually adopts a well-known architecture such as VGG [9]
and the decoder reverses the process by mapping the feature
representation back to the input data space. To this end,
upsampling operations and transposed convolution [13] (also
known as fractionally strided convolution or “deconvolution”)
have been used for semantic segmentation [14]. Alternatively,
in [15] and [16], max unpooling has been used as the inverse
operation of each max pooling layer, where the pairs of
pooling/unpooling layers are coupled by transferring the max
indices from the encoder to the decoder. In [17], a similar
architecture was proposed for biomedical image segmentation,
with additional skip connections that concatenate the feature
maps of an encoding layer to the feature maps of the same-
scale decoding layer.

Recently, some CNNs for semantic segmentation have been
proposed that use dilated convolutions to increase the receptive
ﬁeld,
instead of downsampling the feature maps. Dilated
convolution, also called `atrous, is the convolution with kernels
that have been dilated by inserting zero holes (`atrous in
French) between the non-zero values of a kernel. This was
originally proposed for efﬁcient wavelet decomposition in a
scheme also known as “algorithme `atrous” [18]. Figure 1
shows examples of kernels with different dilation rates. Dilated
convolution can increase the receptive ﬁeld without increasing
the number of parameters, as opposed to normal convolution.
Moreover, feature maps are densely computed on the original
image resolution without the need for downsampling. In [19],
a CNN module with dilated convolutions was designed to ag-
gregate multiscale contextual information and improve the per-
formance of state-of-the-art semantic segmentation systems.
The module has eight convolutional layers with exponentially
increasing dilation rates (i.e. 1, 1, 2, 4, 8, 16), resulting in an
exponential increase in the receptive ﬁeld, while the number
of parameters is only grown linearly. Similarly, expansion
of the receptive ﬁeld was achieved in [20] by integrating
dilated convolutions in a bottleneck module that was designed
the `atrous spatial pyramid pooling
for efﬁciency. In [21],
(ASPP) scheme is proposed that uses multiple parallel dilated

convolutional layers, in order to capture information from
multiple scales.

B. ILD Pattern Classiﬁcation

Over the last twenty years, numerous approaches have been
proposed for the problem of ILD pattern recognition, which
is generally regarded as a texture classiﬁcation problem. Most
of the proposed methods involve hand-crafted texture features
which are fed to machine learning classiﬁers, and locally rec-
ognize lung tissue within a sliding window framework. In one
of the early studies [22], the adaptive multiple feature method
(AMFM) was proposed, which utilizes a combination of gray-
level histograms, co-occurrence and run-length matrices, as
well as fractal analysis parameters. For the classiﬁcation, a
Bayesian classiﬁer was used. In [23], a ﬁlter bank of Gaussian
and Laplacian kernels was applied on the input images and
the histogram moments of the responses were fed to a linear
discriminant classiﬁer. The simple, yet powerful, Local Binary
Pattern (LBP) descriptor has also been proposed [24], com-
bined with a k-nearest neighbors classiﬁer. In [25], a random
forest classiﬁer was utilized that was trained on local DCT
features. More recently, some proposed methods have adopted
unsupervised feature extraction techniques, such as bag of
features [26], [27] and sparse representation models [28], [29].
Lately, a few methods have been proposed that utilize CNNs
for lung pattern classiﬁcation. Most of these are still designed
under a patch-wise scheme where a square patch is fed to the
CNN, while the output consists of the probabilities for this
patch to belong to each class. Although the strong descriptive
capabilities of modern CNNs are commonly attributed to their
depth, the ﬁrst studies utilized rather shallow architectures.
A modiﬁed RBM that resembles a convolutional layer was
used in [30], whereas in [31], a CNN was proposed with
one convolutional and three fully-connected layers. More
recently, some attempts have also been made to utilize deeper
architectures. In our previous work [5], a CNN with ﬁve
convolutional and three dense layers was designed and trained
on ILD data, while in [6] its performance was improved using
knowledge transfer from other domains. In another study [32],
a CNN with three convolutional and one dense layer was fed
with rotational invariant Gabor-LBP representations of lung
tissue patches. Finally, in [33] and [34], the authors utilized
well established pretrained CNN architectures such as AlexNet
and GoogleNet which were further ﬁnetuned for detecting
possible “presence/absence” of pathologies at a slice level.
However, these architectures were designed to classify natural
color images with size 224 × 224, so the authors had to resize
the images and artiﬁcially generate three channels by applying
different Hounsﬁeld unit (HU) windows.

III. MATERIALS AND METHODS

This section presents the proposed fully convolutional neu-
ral network for semantic lung tissue segmentation. Prior to
this, we describe the materials used for training and testing
the network.

3

TABLE I: Data statistics across the considered classes i.e.
Healthy (H), Ground Glass Opacity (GGO), Micronodules
(MN), Consolidation (Cons), Reticulation (Ret) and Honey-
combing (HC).

H

GGO MN

Cons

Ret

HC

Totals

#Pixels×105
#Cases

92.5
66

27.7
82

35.8
15

7.08
46

28.2
81

20.1
47

211.4
172

A. Materials

For the purposes of this study, we compiled a dataset of 172
HRCT scans, each corresponding to a unique ILD or healthy
subject. The dataset contains 109 cases from the publicly
available multimedia database of interstitial lung diseases [35]
by the Geneva University Hospital (HUG), along with 63 cases
from Bern University Hospital - “Inselspital” (INSEL), as col-
lected by the authors. The scans were acquired between 2003
and 2015 using different scanners and acquisition protocols.
The INSEL scans are volumetric, while the HUG scans have
a 10-15mm spacing. The slice thickness is 1-2mm for both
datasets.

Two experienced radiologists from INSEL annotated or re-
annotated ILD typical pathological patterns, as well as healthy
tissue in both databases1. A lung ﬁeld segmentation mask
was also provided for each case. In total six types of tissue
were considered: normal, ground glass opacity, micronodules,
consolidation, reticulation and honeycombing. It should be
emphasized that these annotations do not cover the entire lung
ﬁeld, but only the most typical manifestations of the listed ILD
patterns. This protocol was followed in both databases since
it permits the annotation of more scans for the same effort
and thus increases data diversity. On the other hand, sparse
annotations also introduce challenges. Non-annotated lung
areas have to be excluded from both supervised training and
evaluation. Another challenging characteristic of the databases
is the uneven distribution of the considered classes across the
cases. Table I provides statistics for the entire dataset, while
ﬁgure 2 presents a sample CT lung slice along with the given
annotations.

B. Methods

In this study, we propose the use of a deep purely con-
volutional network for the problem of lung tissue semantic
segmentation. The network is inspired by [19] and consists of
solely convolutional layers that use dilated kernels to increase
the receptive ﬁeld, instead of downsampling the feature maps.
This kind of network has been shown to be suitable for
similar dense prediction problems that require high resolution
precision. The proposed network (Fig. 3) has 13 convolutional
layers and a total receptive ﬁeld of 287×287. Speciﬁcally, each
of the ﬁrst ten layers has 32 kernels of size 3×3 and dilation
rates 1, 1, 2, 3, 5, 8, 13, 21, 34 and 55, respectively. We
chose not to increase the dilation rates exponentially, as is
commonly done, in order to avoid extreme gridding problems
that have been reported in several studies [36], [37]. Instead,

1ITK-snap was used for the annotation process, http://www.itksnap.org

4

ReLU function (Fig. 4) substantially improves the results. This
instance normalization skip connection cancels the mean nor-
malization of activations (when the trainable parameters have
not been trained), while it performs a kind of feature contrast
enhancement which reduces the importance of variance shift
without providing complete invariance to the latter.

The network was trained by minimizing the categorical
cross entropy using the Adam optimizer [40] with a learning
rate of 0.0001. The dense nature of the considered classiﬁca-
tion problem combined with the sparse available annotations,
resulted in two issues. Firstly, large parts of the dataset were
not annotated, and so could not be used for either supervised
training or testing. Secondly, the distribution of the considered
classes in the dataset was highly imbalanced, a fact
that
can be challenging for any classiﬁcation method. We tackled
both problems by scaling the considered loss and accuracy
with appropriate weighting schemes computed for each set.
All pixels corresponding to annotated areas were assigned a
weight inversely proportional to the number of samples of its
class in the speciﬁc set. In this way, all classes contributed
equally to the considered metrics. Furthermore, we employed
a semi-supervised learning technique to additionally exploit
non-labeled areas of the data. We added an extra term to the
supervised loss function, which corresponds to the entropy of
the network’s output on the areas that do not participate in
the supervised learning. This entropy minimization technique
has been used in different applications such as in
[41]
yielding signiﬁcant improvements in performance. Similarly in
[42] the technique of pseudo-labelling was introduced, where
the network classiﬁes non-annotated regions and then uses
them as ground truth for ﬁne-tuning. Semi-supervised learning
techniques of this kind are based on the cluster assumption i.e.
samples from the same class tend to form compact clusters. By
minimizing the entropy of the network’s output, the decision
boundaries are driven away from areas densely populated by
learning samples. If the cluster assumption holds and there is
no large overlap between the classes this method may increase
the network’s generalization ability. It acts equivalently to
manifold learning and includes self-learning as a special case,
as it increases the conﬁdence of the classiﬁer. The inﬂuence
of the semi-supervised term is controlled by an appropriate
weight, which is scaled relatively with the proportion of the
unlabeled regions versus the annotated ones. Hence, the loss
for a pixel x with output ˆy is:

L(x, ˆy) =






− (cid:80)C

i=1 wi

syi log( ˆyi), when y is given

(1)

(cid:80)C

−αwu

i=1 ˆyi log( ˆyi),
where y is the true label in one-hot encoding, C is the
number of classes, wi
s is the supervised weight for class i
(which is inversely proportional to the number of samples of
the class), α is a scaler and wu the unsupervised weight.

otherwise

The training procedure stops when the network does not
signiﬁcantly improve its performance on the validation set for
50 epochs. The performance is assessed in terms of weighted
is considered
(balanced) accuracy, while an improvement
signiﬁcant if the relative increase in performance is at least

Fig. 2: A typical slice with annotations. The white border line
denotes the lung ﬁeld segmentation, the blue denotes healthy
tissue, the purple micronodules and the red the honeycombing
pattern.

we use the ﬁrst terms of the Fibonacci sequence as dilation
rates; this mitigates the gridding problem by providing a less
steep dilation rate increase and thus denser sampling.

The output of the ﬁrst 10 layers, as well as the input
of the network, are concatenated, thus leading to 1+10×32
= 321 feature maps, which are passed through a dropout
layer with a rate of 0.5 and fed to the rest of the network.
This concatenation is allowed by the lack of pooling layers
and the appropriate zero padding for each convolution and
brings several beneﬁts. It permits the aggregation of features
from all different scales and levels of abstraction, while it
also facilitates the ﬂow of gradients thought the network and
therefore allows faster training. The last three layers have 1×1
kernels and play the role of locally dense layers that reduce
the feature dimensionality for each pixel from 321 to 128,
32 and ﬁnally 6, which is the number of classes considered.
The output is converted into a probability distribution by the
softmax function.

A BN layer follows each convolution and is based on the
batch statistics in both training and test time. This is permitted,
as the batch size is one, so there is always a full batch during
inference. This approach has been proposed before, under
the term instance normalization (InstanceNorm) [38], and
has exhibited good performance in texture synthesis, image
stylization and image to image translation [39]. InstanceNorm
provides invariance to intensity and contrast shifts, which
makes the features adaptive for each slice and could mitigate
problems caused by different CT scanners and reconstruction
kernels. We also found that adding the normalized activations
to the non-normalized ones, before passing them through the

InstanceNorm

Input

+

(3x3), D@1

(3x3), D@1

(3x3), D@2

(3x3), D@3

(3x3), D@5

(3x3), D@8

(3x3), D@13

(3x3), D@21

(3x3), D@34

(3x3), D@55

Concatenate

Dropout

(1x1), D@1

(1x1), D@1

(1x1), D@1

Softmax Output

Fig. 3: The architecture of the proposed network. Each gray
box corresponds to a block like the one presented in Fig. 4

0.5%. In order to artiﬁcially increase the volume of training
data and avoid overﬁtting, we transformed the images using
ﬂips and rotations, which are considered label-preserving in
this domain. The augmentation was performed online i.e. for
each training image in each epoch, one operation out of all
eight combinations of ﬂip and rotate is randomly selected and
applied.

5

Dilated Convolution
(k,k), D@x

InstanceNorm

+

ReLU

Fig. 4: The block function of the proposed architecture. (k, k)
is the size of the convolution kernel and x is the dilation rate.

IV. EXPERIMENTAL SETUP AND RESULTS

In this section, we ﬁrst present the setup of the experiments
conducted, followed by the corresponding results that justify
the algorithmic choices of the proposed method and compare
it to the state of the art.

A. Experimental Setup

Given the relatively small size of the dataset with respect
to the diversity of the problem, we adopted a 5-fold cross
validation (CV) scheme to ensure the validity of the results.
The data splitting was performed per scan, so tissue from one
case was never present in more than one set. Speciﬁcally, the
172 scans of the dataset were divided into ﬁve non-overlapping
sets, with one of them having 36 and the rest 34 scans. Every
time a model was tested on a speciﬁc set, the rest of the data
were used for training. On average over all folds, the number
of slices was 2060 for training and 515 for testing. As principal
performance metric, we used the balanced accuracy (Eq. 2),
averaged over the ﬁve folds.

BACC =

1
N

N
(cid:88)

i=1

ci
ni

(2)

where N is the number of classes, ci is the number of
correctly classiﬁed samples of class i and ni
is the total
number of samples of class i. Since the slices were only
sparsely annotated, the accuracy was calculated over the areas
of the scans where a ground truth was available.

In order to avoid extreme class imbalances between the
different sets, data splitting was performed using a simple hill
climbing technique that maximizes the entropy of the class
distribution for the ﬁve sets. The methods started from an
arbitrary split and then randomly swapped two cases between
two sets in an attempt to ﬁnd a more balanced solution. If the
new solution has a higher class distribution entropy (averaged
over the 5 sets), we retained it and repeated the procedure
until no further improvement was possible.

To minimize the number of computations and memory re-
quirements, we discarded part of the data that lack annotations.
Hence, we cropped the left and right lung on each slice and
used only the ones with relevant annotations as inputs of the

TABLE II: Comparison of the different network conﬁgurations

Network
conﬁguration

Number of
parameters
×105

Average
inference time
ms

CV balanced
accuracy
%

w/o dilated convolutions
w/o concatenation
w/o InstanceNorm
w/o InstanceNorm skip
16 kernels/layer
Exponential dilation [19]
Purely supervised
9 dilated layers
Proposed
64 kernels/layer

1.30
0.93
1.29
1.30
0.47
1.03
1.30
1.18
1.30
4.23

51
53
38
57
51
48
58
53
58
82

68.0
72.6
77.9
78.6
79.2
79.5
80.6
81.3
81.8
82.1

networks. This is permitted by the fully-convolutional nature
of the tested networks that do not require a ﬁxed input size.
For the cropping, we utilized the available lung mask, while a
margin of 32 pixels was added on each side to provide context
that could be useful to the networks.

The proposed method was implemented in Python2 using
the Keras framework3 with the Theano [43] back-end. All
experiments were performed under Linux OS on a machine
with CPU Intel Core i7-5960X @ 3.50GHz, GPU NVIDIA
GeForce Titan X, and 128 GB of RAM.

B. Results

Table II presents a comparison between different network
conﬁgurations. The bold line corresponds to the proposed
CNN, while the rest correspond to models that differ from
the proposed in only one aspect, as speciﬁed in the ﬁrst
column. The rest of the columns provide the number of model
parameters, the average inference time per (single-lung) slice,
and the average balanced accuracy across the ﬁve validation
sets.

The proposed model achieved top performance with accu-
racy nearly equal to 82% and inference time 58ms. The use
of 64 kernels per layer instead of 32 did indeed improve the
results, yet not signiﬁcantly enough and with higher inference
times, whereas the network with 16 kernels performed notably
worse. On reducing the dilated convolutional layers from 10
to 9, we observed a relatively small reduction in the accuracy.
However, we chose to keep 10 layers, since the difference in
memory and time requirements was also small and because
the resulting receptive ﬁeld was comparable with that of the
state of the art networks used for comparison. The use of semi-
supervised learning yielded an improvement of nearly 1.5%,
with no additional requirements in computational resources.
We also performed an experiment with exponential increase in
the dilation rates of the consecutive layers, similarly to [19]
i.e. 1, 1, 2, 4, 8, 16, 32 and 64. The resulting model was
smaller and faster, since 2 fewer layers were required to
achieve similar receptive ﬁeld, however the accuracy decreased
by almost 3%. In the case where the convolutions were not
dilated, the network performed poorly, because of the radical
decrease of the receptive ﬁeld. The accuracy of the proposed

2https://github.com/intact-project/LungNet
3https://github.com/fchollet/keras

6

Fig. 5: Accuracy curves for different values of wu.

TABLE III: Comparison with previous studies

Network

Number of
parameters
×105

Average
inference time
ms

CV balanced
accuracy
%

ILD-CNN [5]
Segnet [15]
U-net [17]

Proposed

0.9
335
310

1.3

237
111
88

58

72.2
73.6
77.5

81.8

model without any normalization was substantially poorer,
probably because it could not properly handle the contrast
differences among the scans caused by different CT scanners
and reconstruction kernels. The use of instance normaliza-
tion improved the performance by adaptively normalizing
the feature contrast for each input. However, this kind of
normalization also normalizes the mean intensity that could be
a useful feature. By adding the InstanceNorm skip connection
(Fig. 4), the accuracy improved even further. We speculate this
is because the mean normalization is diminished, while the
resulting variance normalization is only partially invariant to
contrast shifts. Finally, omitting the concatenation of the ﬁrst
10 layers also resulted in signiﬁcant impairment of the results,
which was expected since only 32 features are considered.

In Fig. 5 the accuracy curves for different values of wu
are presented. These curves are generated by averaging over
the ﬁve folds the best accuracies achieved this far by each
model in each epoch. The curve for the model without the
unsupervised learning was also included for comparison. The
best performing conﬁguration proved to be the one with wu =
0.1, which we utilized for the training of the proposed model.
Table III presents a comparison between the proposed
network and three previous studies. It has to be noted that all
models used the same unsupervised weight (wu = 0.1) and
whenever batch normalization was performed, this was based
on batch statistics (instance normalization) since this yielded
the best results. Fig. 7 illustrates a few segmentation results
for each of the models in Table III.

The ﬁrst line of the table refers to our previous work [5],
which has been converted into a fully convolutional network
so it can accept arbitrarily sized images for input. Its low
accuracy is probably due to the small receptive ﬁeld (33×33)
and the extensive pooling. This architecture was sufﬁcient to
describe the local texture of the 32×32 single-class patches
in [5], but could not capture higher level structure that is
present in the whole-lung dataset of this study. The results
of the model in Fig. 7 show its noisy output near the lung
boundaries or between patterns, where context information
could be useful. Segnet [15] and U-net [17] yielded better
results, with the latter being slightly faster and substantially
more accurate. Both models have a very high number of
parameters and large enough receptive ﬁelds to capture any
relevant information. The superior performance of U-net could
be attributed to its skip connections that allow features from
the lower scales to directly contribute to its output. Indeed,
Fig. 7 illustrates the more detailed results of U-net as opposed
to the overly smoothed areas produced by Segnet. Finally, the
proposed network yielded the best results, while being faster
and having far fewer parameters. The output examples in Fig. 7
indicate that the proposed model manages to keep a better
balance between ﬁne details and smooth border among the
different classes. Even thought it is really difﬁcult to visually
assess the performance of the system for the different classes,
there are a few examples in Fig. 7 with wrong classiﬁcations
on which we can comment. Firstly, parts of the broncho-
vascular tree in the third row were recognized as consolidation
because of their similar densities, while accentuated terminal
bronchial parts, that might be physiological as well, caused
the erroneous classiﬁcation of healthy areas into reticulation,
in the ﬁrst row. Some mistakes however are also attributed in
the limited number of annotated classes. For example in row
6, there are emphysematic areas (dark area in the center of the
lung) that have been annotated as healthy due to their similar
density. Figure 6 shows the confusion matrix of the proposed
model. As expected, many of the misclassiﬁcations occur
between reticulation and honeycombing due to their similar
textural appearance. Moreover, healthy tissue is often confused
with reticulation probably because of the 2D sections of the
bronchovascular tree that could resemble reticular patterns.

V. CONCLUSIONS

In this study, we proposed and evaluated a deep CNN
for the semantic segmentation of pathological lung tissue on
HRCT slices. The CNN is designed under a fully convo-
lutional scheme and thus can handle variable input sizes,
while it was trained in an end-to-end and semi-supervised
fashion. The main characteristic of the proposed network is
the use of dilated convolutions along with an instance variance
normalization scheme, and multi-scale feature fusion. The
training and testing of the network was performed using a
cross validation scheme on a dataset of 172 cases, whereas
the split of the dataset into folds was performed per case. The
proposed network surpassed the highest performance in pre-
vious studies, and is much more efﬁcient in terms of memory
and computation. Future work includes the modiﬁcation of

7

Fig. 6: Confusion matrix of the proposed model as calculated
over the cross validation scheme. The numbers represent
percentages of pixels across all validation images.

the model to consider the 3D nature of lung patterns, and
to account for the bronchovascular tree. The former could
be achieved by a direct extension of the architecture to 3D,
similarly to 3D U-Net [44] and V-Net [45] or by employing
a multi-planar view aggregation scheme, also referred to as
2.5D, [46]. Alternatively, a 3D post processing scheme could
be used to reﬁne the 2D segmentation output using conditional
random ﬁelds or deformation models [47], [48], [49]. Finally,
the result of a bronchovascular segmentation method could be
utilized by the network to reduce false alarms.

REFERENCES

[1] B. SOCIETY, “The diagnosis, assessment and treatment of diffuse
parenchymal lung disease in adults,” Thorax, vol. 54, no. Suppl 1, p. S1,
1999.

[2] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp.
211–252, 2015.

[3] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, Nov 1998.

[4] H. Greenspan, B. van Ginneken, and R. M. Summers, “Guest editorial
deep learning in medical imaging: Overview and future promise of
an exciting new technique,” IEEE Transactions on Medical Imaging,
vol. 35, no. 5, pp. 1153–1159, 2016.

[5] M. Anthimopoulos, S. Christodoulidis, L. Ebner, A. Christe, and
S. Mougiakakou, “Lung pattern classiﬁcation for interstitial lung dis-
eases using a deep convolutional neural network,” IEEE Transactions
on Medical Imaging, vol. 35, no. 5, pp. 1207–1216, May 2016.

[6] S. Christodoulidis, M. Anthimopoulos, L. Ebner, A. Christe, and
S. Mougiakakou, “Multisource transfer learning with convolutional
neural networks for lung pattern analysis,” IEEE Journal of Biomedical
and Health Informatics, vol. 21, no. 1, pp. 76–84, Jan 2017.

[7] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.
[8] N. Srivastava, G. Hinton, A. Krizhevsky,

and
R. Salakhutdinov, “Dropout: A simple way to prevent neural
networks from overﬁtting,” J. Mach. Learn. Res., vol. 15, no. 1, pp.
1929–1958, Jan. 2014.

I. Sutskever,

[9] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.

8

Fig. 7: Output examples for the models of Table III. From left to right: Ground Truth, ILD-CNN, Segnet, U-net, Proposed. Each
example has a different pattern annotated. From top to bottom: Healthy (Blue), Ground Glass Opacity (Purple), Micronodules
(Green), Consolidation (Yellow), Reticulation (Orange) and Honeycombing (Red).

9

[32] Q. Wang, Y. Zheng, G. Yang, W. Jin, X. Chen, and Y. Yin, “Multi-
scale rotation-invariant convolutional neural networks for lung texture
classiﬁcation,” IEEE Journal of Biomedical and Health Informatics,
vol. PP, no. 99, pp. 1–1, 2017.

[33] H. C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao,
D. Mollura, and R. M. Summers, “Deep convolutional neural networks
for computer-aided detection: Cnn architectures, dataset characteristics
and transfer learning,” IEEE Transactions on Medical Imaging, vol. 35,
no. 5, pp. 1285–1298, May 2016.

[34] M. Gao, Z. Xu, L. Lu, A. P. Harrison, R. M. Summers, and D. J. Mollura,
“Holistic interstitial lung disease detection using deep convolutional
neural networks: Multi-label learning and unordered pooling,” arXiv
preprint arXiv:1701.05616, 2017.

[35] A. Depeursinge, A. Vargas, A. Platon, A. Geissbuhler, P.-A. Poletti, and
H. M¨uller, “Building a reference multimedia database for interstitial lung
diseases,” Computerized medical imaging and graphics, vol. 36, no. 3,
pp. 227–238, 2012.

[36] P. Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, and
G. W. Cottrell, “Understanding convolution for semantic segmentation,”
CoRR, vol. abs/1702.08502, 2017.

[37] F. Yu, V. Koltun, and T. A. Funkhouser, “Dilated residual networks,”

CoRR, vol. abs/1705.09914, 2017.

[38] D. Ulyanov, A. Vedaldi, and V. S. Lempitsky, “Improved texture
networks: Maximizing quality and diversity in feed-forward stylization
and texture synthesis,” CoRR, vol. abs/1701.02096, 2017.

[39] P. Isola, J. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
with conditional adversarial networks,” CoRR, vol. abs/1611.07004,
2016.

[40] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

CoRR, vol. abs/1412.6980, 2014.

[41] Y. Grandvalet and Y. Bengio, “Semi-supervised learning by entropy
minimization,” in Proceedings of the 17th International Conference
on Neural Information Processing Systems, ser. NIPS’04. Cambridge,
MA, USA: MIT Press, 2004, pp. 529–536.

[42] D.-H. Lee, “Pseudo-label: The simple and efﬁcient semi-supervised
learning method for deep neural networks,” in Workshop on Challenges
in Representation Learning, ICML, vol. 3, 2013, p. 2.

[44]

[43] Theano Development Team, “Theano: A Python framework for
fast computation of mathematical expressions,” arXiv e-prints, vol.
abs/1605.02688, May 2016.
¨O. C¸ ic¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,
“3d u-net: learning dense volumetric segmentation from sparse anno-
tation,” in International Conference on Medical Image Computing and
Computer-Assisted Intervention. Springer, 2016, pp. 424–432.
[45] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional
neural networks for volumetric medical image segmentation,” in 3D
Vision (3DV), 2016 Fourth International Conference on.
IEEE, 2016,
pp. 565–571.

[46] H. R. Roth, L. Lu, J. Liu, J. Yao, A. Seff, K. Cherry, L. Kim, and R. M.
Summers, “Improving computer-aided detection using convolutional
neural networks and random view aggregation,” IEEE transactions on
medical imaging, vol. 35, no. 5, pp. 1170–1181, 2016.

[47] F. Liu, Z. Zhou, H. Jang, A. Samsonov, G. Zhao, and R. Kijowski,
“Deep convolutional neural network and 3d deformable approach for
tissue segmentation in musculoskeletal magnetic resonance imaging,”
Magnetic resonance in medicine, vol. 79, no. 4, pp. 2379–2391, 2018.
[48] P. F. Christ, M. E. A. Elshaer, F. Ettlinger, S. Tatavarty, M. Bickel,
P. Bilic, M. Rempﬂer, M. Armbruster, F. Hofmann, M. DAnastasi et al.,
“Automatic liver and lesion segmentation in ct using cascaded fully
convolutional neural networks and 3d conditional random ﬁelds,” in
International Conference on Medical Image Computing and Computer-
Assisted Intervention. Springer, 2016, pp. 415–423.

[49] K. Kamnitsas, C. Ledig, V. F. Newcombe, J. P. Simpson, A. D. Kane,
D. K. Menon, D. Rueckert, and B. Glocker, “Efﬁcient multi-scale 3d
cnn with fully connected crf for accurate brain lesion segmentation,”
Medical image analysis, vol. 36, pp. 61–78, 2017.

[10] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2015, pp. 1–9.

[11] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2016, pp. 770–778.

[12] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” CoRR, vol.
abs/1502.03167, 2015.

[13] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-
tional networks,” in European conference on computer vision. Springer,
2014, pp. 818–833.

[14] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2015, pp. 3431–3440.
[15] V. Badrinarayanan, A. Handa, and R. Cipolla, “Segnet: A deep con-
volutional encoder-decoder architecture for robust semantic pixel-wise
labelling,” arXiv preprint arXiv:1505.07293, 2015.

[16] H. Noh, S. Hong, and B. Han, “Learning deconvolution network
for semantic segmentation,” in Proceedings of the IEEE International
Conference on Computer Vision, 2015, pp. 1520–1528.

[17] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” CoRR, vol. abs/1505.04597, 2015.
[18] M. Holschneider, R. Kronland-Martinet, J. Morlet, and P. Tchamitchian,
“A real-time algorithm for signal analysis with the help of the wavelet
transform,” in Wavelets. Springer, 1990, pp. 286–297.

[19] F. Yu and V. Koltun, “Multi-scale context aggregation by dilated

convolutions,” CoRR, vol. abs/1511.07122, 2015.

[20] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A deep
neural network architecture for real-time semantic segmentation,” CoRR,
vol. abs/1606.02147, 2016.

[21] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille, “Deeplab: Semantic image segmentation with deep convolutional
nets, atrous convolution, and fully connected crfs,” arXiv preprint
arXiv:1606.00915, 2016.

[22] R. Uppaluri, E. A. Hoffman, M. Sonka, P. G. Hartley, G. W. Hunning-
hake, and G. McLennan, “Computer recognition of regional lung disease
patterns,” American journal of respiratory and critical care medicine,
vol. 160, no. 2, pp. 648–654, 1999.

[23] I. C. Sluimer, P. F. van Waes, M. A. Viergever, and B. van Ginneken,
“Computer-aided diagnosis in high resolution ct of the lungs,” Medical
physics, vol. 30, no. 12, pp. 3081–3090, 2003.

[24] L. Sorensen, S. B. Shaker, and M. De Bruijne, “Quantitative analysis of
pulmonary emphysema using local binary patterns,” IEEE transactions
on medical imaging, vol. 29, no. 2, pp. 559–569, 2010.

[25] M. Anthimopoulos, S. Christodoulidis, A. Christe, and S. Mougiakakou,
“Classiﬁcation of interstitial lung disease patterns using local dct features
and random forest,” in 2014 36th Annual International Conference of
the IEEE Engineering in Medicine and Biology Society, Aug 2014, pp.
6040–6043.

[26] M. Gangeh, L. Sørensen, S. Shaker, M. Kamel, M. De Bruijne, and
M. Loog, “A texton-based approach for the classiﬁcation of lung
parenchyma in ct images,” Medical Image Computing and Computer-
Assisted Intervention–MICCAI 2010, pp. 595–602, 2010.

[27] A. Foncubierta-Rodr´ıguez, A. Depeursinge, and H. M¨uller, “Using
multiscale visual words for lung texture classiﬁcation and retrieval,” in
MICCAI International Workshop on Medical Content-Based Retrieval
for Clinical Decision Support. Springer, 2011, pp. 69–79.

[28] W. Zhao, R. Xu, Y. Hirano, R. Tachibana, and S. Kido, “Classiﬁcation of
diffuse lung diseases patterns by a sparse representation based method on
hrct images,” in Engineering in Medicine and Biology Society (EMBC),
2013 35th Annual International Conference of the IEEE.
IEEE, 2013,
pp. 5457–5460.

[29] K. T. Vo and A. Sowmya, “Multiscale sparse representation of high-
resolution computed tomography (hrct) lung images for diffuse lung
disease classiﬁcation,” in Image Processing (ICIP), 2011 18th IEEE
International Conference on.

IEEE, 2011, pp. 441–444.

[30] G. van Tulder and M. de Bruijne, “Learning features for tissue classi-
ﬁcation with the classiﬁcation restricted boltzmann machine,” in Inter-
national MICCAI Workshop on Medical Computer Vision.
Springer,
2014, pp. 47–58.

[31] Q. Li, W. Cai, X. Wang, Y. Zhou, D. D. Feng, and M. Chen, “Med-
ical image classiﬁcation with convolutional neural network,” in 2014
13th International Conference on Control Automation Robotics Vision
(ICARCV), Dec 2014, pp. 844–848.

Semantic Segmentation of Pathological Lung Tissue
with Dilated Fully Convolutional Networks

Marios Anthimopoulos, Member, IEEE, Stergios Christodoulidis, Member, IEEE, Lukas Ebner, Thomas Geiser,
Andreas Christe, and Stavroula Mougiakakou*, Member, IEEE

1

8
1
0
2
 
r
a

M
 
6
1
 
 
]

V
C
.
s
c
[
 
 
1
v
7
6
1
6
0
.
3
0
8
1
:
v
i
X
r
a

Abstract—Early and accurate diagnosis of interstitial

lung
diseases (ILDs) is crucial for making treatment decisions, but
can be challenging even for experienced radiologists. The diag-
nostic procedure is based on the detection and recognition of
the different ILD pathologies in thoracic CT scans, yet their
manifestation often appears similar. In this study, we propose
the use of a deep purely convolutional neural network for the
semantic segmentation of ILD patterns, as the basic component of
a computer aided diagnosis (CAD) system for ILDs. The proposed
CNN, which consists of convolutional layers with dilated ﬁlters,
takes as input a lung CT image of arbitrary size and outputs
the corresponding label map. We trained and tested the network
on a dataset of 172 sparsely annotated CT scans, within a cross-
validation scheme. The training was performed in an end-to-
end and semi-supervised fashion, utilizing both labeled and non-
labeled image regions. The experimental results show signiﬁcant
performance improvement with respect to the state of the art.

Index Terms—Interstitial

lung disease, Fully convolutional
neural networks, Dilated convolutions, Texture segmentation,
Semi-supervised learning

I. INTRODUCTION

I NTERSTITIAL lung disease (ILD) is a group of more than

200 chronic lung disorders characterized by inﬂammation
and scarring of the lung tissue that leads to respiratory failure.
ILD accounts for 15 percent of all cases seen by pulmonolo-
gists and can be caused by autoimmune disease, genetic abnor-
malities, infections, drugs or long-term exposure to hazardous
materials. In many cases the cause remains unknown and the

Manuscript received March 19, 2018
This research was carried out within the framework of the IntACT research
project, supported by Bern University Hospital,“Inselspital” and the Swiss
National Science Foundation (SNSF) under Grant 156511.

M. Anthimopoulos and S. Christodoulidis contributed equally to this work.

The asterisk indicates the corresponding author.

M. Anthimopoulos is with the ARTORG Center for Biomedical Engineering
Research, University of Bern, 3008 Bern, Switzerland, and the Department
of Emergency Medicine, Bern University Hospital “Inselspital”, 3010 Bern,
Switzerland (e-mail: marios.anthimopoulos@artorg.unibe.ch).

S. Christodoulidis is with the ARTORG Center for Biomedical Engi-
neering Research, University of Bern, 3008 Bern, Switzerland (e-mail: ster-
gios.christodoulidis@artorg.unibe.ch).

T. Geiser

is with the University Clinic for Pneumonology, Bern
(e-mail:

3010 Bern, Switzerland

“Inselspital”,

University Hospital
thomas.geiser@insel.ch)

S. Mougiakakou* is with the Department of Diagnostic,

L. Ebner and A. Christe are with the Department of Diagnostic, Interven-
tional and Pediatric Radiology, Bern University Hospital “Inselspital”, 3010
Bern, Switzerland (e-mails: lukas.ebner@insel.ch; andreas.christe@insel.ch).
Interven-
tional and Pediatric Radiology, Bern University Hospital “Inselspital”,
3010 Bern, Switzerland, and the ARTORG Center for Biomedical En-
gineering Research, University of Bern, 3008 Bern Switzerland (e-mail:
stavroula.mougiakakou@artorg.unibe.ch).

Source code available at: https://github.com/intact-project/LungNet

disease is described as idiopathic. The diagnosis of ILD is
mostly performed by radiologists and is usually based on the
assessment of the different ILD pathologies in high resolution
computed tomography (HRCT) thoracic scans. Early diagnosis
is crucial for making treatment decisions, while misdiagnosis
may lead to life-threatening complications [1]. Extensive re-
search has been conducted on the development of computer-
aided diagnosis (CAD) systems, which are able to support
clinicians and improve their diagnostic performance. The basic
characteristics of such a system are the automatic detection and
recognition of the pathological lung tissue. Pathological tissue
is usually manifested as various textural patterns in the CT
scan. This ILD pattern recognition procedure is traditionally
performed by a local texture classiﬁcation scheme that slides
across the images and outputs a map of pathologies, which is
later used to reach a ﬁnal diagnosis. For texture recognition,
a great variety of handcrafted image features and machine
learning classiﬁers have been utilized.

Recently, deep artiﬁcial neural networks (ANN) and, in
particular, deep convolutional neural networks (CNNs) have
gained a lot of attention after their impressive results in the
ImageNet Large Scale Visual Recognition Competition in
2012 [2]. Networks of this kind have existed for decades [3],
but have only recently managed to achieve adequate perfor-
mance mainly due to the large volumes of available annotated
data, the massive parallelization capabilities of GPUs and a
few design tricks. The potential beneﬁts of deep learning tech-
niques in medical image analysis have also been investigated
recently and the ﬁrst results have been promising [4]. In [5] ,
we designed, trained and tested a deep CNN as a ﬁxed-scale,
local texture classiﬁer that outperformed traditional methods
in ILD pattern classiﬁcation. However, training large networks
on medical images can often be challenging due to the lack
of databases that are adequately sized to satisfy the needs of
these models. Medical data are scarce and collecting them is
a difﬁcult and time consuming process, while their annotation
has to be performed by multiple specialists to ensure its
validity. To this end, in [6], we investigated the potential
use of general
in a multi-source
transfer learning scheme that yielded signiﬁcant improvements
in performance. One remaining limitation in these works is the
local nature of the classiﬁer that requires rigorous scanning
of the input image with a sliding window, and simultaneous
aggregation of the results in the output. This classiﬁcation
scheme can be signiﬁcantly time-consuming while it may also
ignore less local but useful information, if the input size is not
appropriately conﬁgured.

texture image databases,

In this study, we propose the use of a deep fully-
convolutional network for the problem of ILD pattern recog-
nition that uses dilated convolutions and is trained in an end-
to-end and semi-supervised manner. The proposed CNN takes
as input a lung HRCT image of arbitrary size and outputs
the corresponding label map, thus avoiding the limitations of
a sliding window model. Additionally, the utilization of non-
labeled image regions in the learning procedure, permits robust
training of larger models and proves to be particularly useful
when using databases with sparse annotations.

II. RELATED WORK

In this section, we provide a short review of the recent
advances in deep learning for computer vision, followed by a
brief overview of previous studies on ILD pattern classiﬁcation

A. Deep CNNs for Computer Vision

Although CNNs have existed for decades [3], they only
became widely popular after their remarkable success in the
ImageNet challenge of 2012 [2]. The winning approach of the
competition [7], also known as AlexNet, was a deep CNN
with ﬁve convolutional and three dense layers, each followed
by a rectiﬁed linear unit (ReLU), while increased strides
were used for the max pooling and convolution operations to
gradually down-sample the feature maps. Dropout [8] and data
augmentation were also utilized, in order to prevent overﬁtting.
Since then, the proposed deep CNNs have led to continuous
improvements in the results on ImageNet and other datasets,
mainly by enhancing their architecture and by increasing their
depth and width. The VGG network [9] reduced the size of
the kernels to 3×3, while increasing the number of layers
to 19. GoogleNet [10] used consecutive inception modules,
where different convolutional and pooling operations are per-
formed in parallel with their outputs merged. This approach
drastically reduced the number of parameters while improving
the results. ResNet [11] introduced skip connections between
layers which permitted the training of networks with hundreds
of layers and pushed the limits of deep CNNs even further. The
batch normalization (BN) technique [12] also supported these
developments, by regularizing and accelerating the training
procedure.

Many of the already proposed CNNs have recently been
adapted to perform semantic segmentation, rather than just
image classiﬁcation. The term semantic segmentation refers
to the task of assigning a class label to every pixel of an
image. A simple approach to this task is to use any ﬁxed-
scale classiﬁcation method under a sliding window scheme and
then to aggregate the results to build a label map. However,
this could be highly inefﬁcient as local image features would
have to be recalculated multiple times for adjacent positions of
the input window. Luckily, the convolutional layers (with the
appropriate padding) in a typical CNN produce feature maps
that maintain spatial correspondence with the input image.
Therefore, input images of any size can be fed to the network
and each pixel can be classiﬁed, on the basis of the values of
the respective feature map position. This can be achieved by
utilizing convolutional layers of size 1 × 1 that serve as local

2

(a) D = 1

(b) D = 2

(c) D = 3

Fig. 1: Dilated convolution kernels. D denotes the dilation rate

dense layers and, these networks are therefore often referred
to as fully convolutional (FCNs).

However, the spatial correspondence between the input and
output of a CNN, can be disrupted by the use of down-
sampling operations such, as strided pooling and convolution.
Down-sampling of the feature maps is often used to increase
the receptive ﬁeld of the network with respect to the input,
as well as to reduce the amount of computational load. In
order to restore the original size of the input, researchers
have used encoder-decoder architectures, where the encoder
usually adopts a well-known architecture such as VGG [9]
and the decoder reverses the process by mapping the feature
representation back to the input data space. To this end,
upsampling operations and transposed convolution [13] (also
known as fractionally strided convolution or “deconvolution”)
have been used for semantic segmentation [14]. Alternatively,
in [15] and [16], max unpooling has been used as the inverse
operation of each max pooling layer, where the pairs of
pooling/unpooling layers are coupled by transferring the max
indices from the encoder to the decoder. In [17], a similar
architecture was proposed for biomedical image segmentation,
with additional skip connections that concatenate the feature
maps of an encoding layer to the feature maps of the same-
scale decoding layer.

Recently, some CNNs for semantic segmentation have been
proposed that use dilated convolutions to increase the receptive
ﬁeld,
instead of downsampling the feature maps. Dilated
convolution, also called `atrous, is the convolution with kernels
that have been dilated by inserting zero holes (`atrous in
French) between the non-zero values of a kernel. This was
originally proposed for efﬁcient wavelet decomposition in a
scheme also known as “algorithme `atrous” [18]. Figure 1
shows examples of kernels with different dilation rates. Dilated
convolution can increase the receptive ﬁeld without increasing
the number of parameters, as opposed to normal convolution.
Moreover, feature maps are densely computed on the original
image resolution without the need for downsampling. In [19],
a CNN module with dilated convolutions was designed to ag-
gregate multiscale contextual information and improve the per-
formance of state-of-the-art semantic segmentation systems.
The module has eight convolutional layers with exponentially
increasing dilation rates (i.e. 1, 1, 2, 4, 8, 16), resulting in an
exponential increase in the receptive ﬁeld, while the number
of parameters is only grown linearly. Similarly, expansion
of the receptive ﬁeld was achieved in [20] by integrating
dilated convolutions in a bottleneck module that was designed
the `atrous spatial pyramid pooling
for efﬁciency. In [21],
(ASPP) scheme is proposed that uses multiple parallel dilated

convolutional layers, in order to capture information from
multiple scales.

B. ILD Pattern Classiﬁcation

Over the last twenty years, numerous approaches have been
proposed for the problem of ILD pattern recognition, which
is generally regarded as a texture classiﬁcation problem. Most
of the proposed methods involve hand-crafted texture features
which are fed to machine learning classiﬁers, and locally rec-
ognize lung tissue within a sliding window framework. In one
of the early studies [22], the adaptive multiple feature method
(AMFM) was proposed, which utilizes a combination of gray-
level histograms, co-occurrence and run-length matrices, as
well as fractal analysis parameters. For the classiﬁcation, a
Bayesian classiﬁer was used. In [23], a ﬁlter bank of Gaussian
and Laplacian kernels was applied on the input images and
the histogram moments of the responses were fed to a linear
discriminant classiﬁer. The simple, yet powerful, Local Binary
Pattern (LBP) descriptor has also been proposed [24], com-
bined with a k-nearest neighbors classiﬁer. In [25], a random
forest classiﬁer was utilized that was trained on local DCT
features. More recently, some proposed methods have adopted
unsupervised feature extraction techniques, such as bag of
features [26], [27] and sparse representation models [28], [29].
Lately, a few methods have been proposed that utilize CNNs
for lung pattern classiﬁcation. Most of these are still designed
under a patch-wise scheme where a square patch is fed to the
CNN, while the output consists of the probabilities for this
patch to belong to each class. Although the strong descriptive
capabilities of modern CNNs are commonly attributed to their
depth, the ﬁrst studies utilized rather shallow architectures.
A modiﬁed RBM that resembles a convolutional layer was
used in [30], whereas in [31], a CNN was proposed with
one convolutional and three fully-connected layers. More
recently, some attempts have also been made to utilize deeper
architectures. In our previous work [5], a CNN with ﬁve
convolutional and three dense layers was designed and trained
on ILD data, while in [6] its performance was improved using
knowledge transfer from other domains. In another study [32],
a CNN with three convolutional and one dense layer was fed
with rotational invariant Gabor-LBP representations of lung
tissue patches. Finally, in [33] and [34], the authors utilized
well established pretrained CNN architectures such as AlexNet
and GoogleNet which were further ﬁnetuned for detecting
possible “presence/absence” of pathologies at a slice level.
However, these architectures were designed to classify natural
color images with size 224 × 224, so the authors had to resize
the images and artiﬁcially generate three channels by applying
different Hounsﬁeld unit (HU) windows.

III. MATERIALS AND METHODS

This section presents the proposed fully convolutional neu-
ral network for semantic lung tissue segmentation. Prior to
this, we describe the materials used for training and testing
the network.

3

TABLE I: Data statistics across the considered classes i.e.
Healthy (H), Ground Glass Opacity (GGO), Micronodules
(MN), Consolidation (Cons), Reticulation (Ret) and Honey-
combing (HC).

H

GGO MN

Cons

Ret

HC

Totals

#Pixels×105
#Cases

92.5
66

27.7
82

35.8
15

7.08
46

28.2
81

20.1
47

211.4
172

A. Materials

For the purposes of this study, we compiled a dataset of 172
HRCT scans, each corresponding to a unique ILD or healthy
subject. The dataset contains 109 cases from the publicly
available multimedia database of interstitial lung diseases [35]
by the Geneva University Hospital (HUG), along with 63 cases
from Bern University Hospital - “Inselspital” (INSEL), as col-
lected by the authors. The scans were acquired between 2003
and 2015 using different scanners and acquisition protocols.
The INSEL scans are volumetric, while the HUG scans have
a 10-15mm spacing. The slice thickness is 1-2mm for both
datasets.

Two experienced radiologists from INSEL annotated or re-
annotated ILD typical pathological patterns, as well as healthy
tissue in both databases1. A lung ﬁeld segmentation mask
was also provided for each case. In total six types of tissue
were considered: normal, ground glass opacity, micronodules,
consolidation, reticulation and honeycombing. It should be
emphasized that these annotations do not cover the entire lung
ﬁeld, but only the most typical manifestations of the listed ILD
patterns. This protocol was followed in both databases since
it permits the annotation of more scans for the same effort
and thus increases data diversity. On the other hand, sparse
annotations also introduce challenges. Non-annotated lung
areas have to be excluded from both supervised training and
evaluation. Another challenging characteristic of the databases
is the uneven distribution of the considered classes across the
cases. Table I provides statistics for the entire dataset, while
ﬁgure 2 presents a sample CT lung slice along with the given
annotations.

B. Methods

In this study, we propose the use of a deep purely con-
volutional network for the problem of lung tissue semantic
segmentation. The network is inspired by [19] and consists of
solely convolutional layers that use dilated kernels to increase
the receptive ﬁeld, instead of downsampling the feature maps.
This kind of network has been shown to be suitable for
similar dense prediction problems that require high resolution
precision. The proposed network (Fig. 3) has 13 convolutional
layers and a total receptive ﬁeld of 287×287. Speciﬁcally, each
of the ﬁrst ten layers has 32 kernels of size 3×3 and dilation
rates 1, 1, 2, 3, 5, 8, 13, 21, 34 and 55, respectively. We
chose not to increase the dilation rates exponentially, as is
commonly done, in order to avoid extreme gridding problems
that have been reported in several studies [36], [37]. Instead,

1ITK-snap was used for the annotation process, http://www.itksnap.org

4

ReLU function (Fig. 4) substantially improves the results. This
instance normalization skip connection cancels the mean nor-
malization of activations (when the trainable parameters have
not been trained), while it performs a kind of feature contrast
enhancement which reduces the importance of variance shift
without providing complete invariance to the latter.

The network was trained by minimizing the categorical
cross entropy using the Adam optimizer [40] with a learning
rate of 0.0001. The dense nature of the considered classiﬁca-
tion problem combined with the sparse available annotations,
resulted in two issues. Firstly, large parts of the dataset were
not annotated, and so could not be used for either supervised
training or testing. Secondly, the distribution of the considered
classes in the dataset was highly imbalanced, a fact
that
can be challenging for any classiﬁcation method. We tackled
both problems by scaling the considered loss and accuracy
with appropriate weighting schemes computed for each set.
All pixels corresponding to annotated areas were assigned a
weight inversely proportional to the number of samples of its
class in the speciﬁc set. In this way, all classes contributed
equally to the considered metrics. Furthermore, we employed
a semi-supervised learning technique to additionally exploit
non-labeled areas of the data. We added an extra term to the
supervised loss function, which corresponds to the entropy of
the network’s output on the areas that do not participate in
the supervised learning. This entropy minimization technique
has been used in different applications such as in
[41]
yielding signiﬁcant improvements in performance. Similarly in
[42] the technique of pseudo-labelling was introduced, where
the network classiﬁes non-annotated regions and then uses
them as ground truth for ﬁne-tuning. Semi-supervised learning
techniques of this kind are based on the cluster assumption i.e.
samples from the same class tend to form compact clusters. By
minimizing the entropy of the network’s output, the decision
boundaries are driven away from areas densely populated by
learning samples. If the cluster assumption holds and there is
no large overlap between the classes this method may increase
the network’s generalization ability. It acts equivalently to
manifold learning and includes self-learning as a special case,
as it increases the conﬁdence of the classiﬁer. The inﬂuence
of the semi-supervised term is controlled by an appropriate
weight, which is scaled relatively with the proportion of the
unlabeled regions versus the annotated ones. Hence, the loss
for a pixel x with output ˆy is:

L(x, ˆy) =






− (cid:80)C

i=1 wi

syi log( ˆyi), when y is given

(1)

(cid:80)C

−αwu

i=1 ˆyi log( ˆyi),
where y is the true label in one-hot encoding, C is the
number of classes, wi
s is the supervised weight for class i
(which is inversely proportional to the number of samples of
the class), α is a scaler and wu the unsupervised weight.

otherwise

The training procedure stops when the network does not
signiﬁcantly improve its performance on the validation set for
50 epochs. The performance is assessed in terms of weighted
is considered
(balanced) accuracy, while an improvement
signiﬁcant if the relative increase in performance is at least

Fig. 2: A typical slice with annotations. The white border line
denotes the lung ﬁeld segmentation, the blue denotes healthy
tissue, the purple micronodules and the red the honeycombing
pattern.

we use the ﬁrst terms of the Fibonacci sequence as dilation
rates; this mitigates the gridding problem by providing a less
steep dilation rate increase and thus denser sampling.

The output of the ﬁrst 10 layers, as well as the input
of the network, are concatenated, thus leading to 1+10×32
= 321 feature maps, which are passed through a dropout
layer with a rate of 0.5 and fed to the rest of the network.
This concatenation is allowed by the lack of pooling layers
and the appropriate zero padding for each convolution and
brings several beneﬁts. It permits the aggregation of features
from all different scales and levels of abstraction, while it
also facilitates the ﬂow of gradients thought the network and
therefore allows faster training. The last three layers have 1×1
kernels and play the role of locally dense layers that reduce
the feature dimensionality for each pixel from 321 to 128,
32 and ﬁnally 6, which is the number of classes considered.
The output is converted into a probability distribution by the
softmax function.

A BN layer follows each convolution and is based on the
batch statistics in both training and test time. This is permitted,
as the batch size is one, so there is always a full batch during
inference. This approach has been proposed before, under
the term instance normalization (InstanceNorm) [38], and
has exhibited good performance in texture synthesis, image
stylization and image to image translation [39]. InstanceNorm
provides invariance to intensity and contrast shifts, which
makes the features adaptive for each slice and could mitigate
problems caused by different CT scanners and reconstruction
kernels. We also found that adding the normalized activations
to the non-normalized ones, before passing them through the

InstanceNorm

Input

+

(3x3), D@1

(3x3), D@1

(3x3), D@2

(3x3), D@3

(3x3), D@5

(3x3), D@8

(3x3), D@13

(3x3), D@21

(3x3), D@34

(3x3), D@55

Concatenate

Dropout

(1x1), D@1

(1x1), D@1

(1x1), D@1

Softmax Output

Fig. 3: The architecture of the proposed network. Each gray
box corresponds to a block like the one presented in Fig. 4

0.5%. In order to artiﬁcially increase the volume of training
data and avoid overﬁtting, we transformed the images using
ﬂips and rotations, which are considered label-preserving in
this domain. The augmentation was performed online i.e. for
each training image in each epoch, one operation out of all
eight combinations of ﬂip and rotate is randomly selected and
applied.

5

Dilated Convolution
(k,k), D@x

InstanceNorm

+

ReLU

Fig. 4: The block function of the proposed architecture. (k, k)
is the size of the convolution kernel and x is the dilation rate.

IV. EXPERIMENTAL SETUP AND RESULTS

In this section, we ﬁrst present the setup of the experiments
conducted, followed by the corresponding results that justify
the algorithmic choices of the proposed method and compare
it to the state of the art.

A. Experimental Setup

Given the relatively small size of the dataset with respect
to the diversity of the problem, we adopted a 5-fold cross
validation (CV) scheme to ensure the validity of the results.
The data splitting was performed per scan, so tissue from one
case was never present in more than one set. Speciﬁcally, the
172 scans of the dataset were divided into ﬁve non-overlapping
sets, with one of them having 36 and the rest 34 scans. Every
time a model was tested on a speciﬁc set, the rest of the data
were used for training. On average over all folds, the number
of slices was 2060 for training and 515 for testing. As principal
performance metric, we used the balanced accuracy (Eq. 2),
averaged over the ﬁve folds.

BACC =

1
N

N
(cid:88)

i=1

ci
ni

(2)

where N is the number of classes, ci is the number of
correctly classiﬁed samples of class i and ni
is the total
number of samples of class i. Since the slices were only
sparsely annotated, the accuracy was calculated over the areas
of the scans where a ground truth was available.

In order to avoid extreme class imbalances between the
different sets, data splitting was performed using a simple hill
climbing technique that maximizes the entropy of the class
distribution for the ﬁve sets. The methods started from an
arbitrary split and then randomly swapped two cases between
two sets in an attempt to ﬁnd a more balanced solution. If the
new solution has a higher class distribution entropy (averaged
over the 5 sets), we retained it and repeated the procedure
until no further improvement was possible.

To minimize the number of computations and memory re-
quirements, we discarded part of the data that lack annotations.
Hence, we cropped the left and right lung on each slice and
used only the ones with relevant annotations as inputs of the

TABLE II: Comparison of the different network conﬁgurations

Network
conﬁguration

Number of
parameters
×105

Average
inference time
ms

CV balanced
accuracy
%

w/o dilated convolutions
w/o concatenation
w/o InstanceNorm
w/o InstanceNorm skip
16 kernels/layer
Exponential dilation [19]
Purely supervised
9 dilated layers
Proposed
64 kernels/layer

1.30
0.93
1.29
1.30
0.47
1.03
1.30
1.18
1.30
4.23

51
53
38
57
51
48
58
53
58
82

68.0
72.6
77.9
78.6
79.2
79.5
80.6
81.3
81.8
82.1

networks. This is permitted by the fully-convolutional nature
of the tested networks that do not require a ﬁxed input size.
For the cropping, we utilized the available lung mask, while a
margin of 32 pixels was added on each side to provide context
that could be useful to the networks.

The proposed method was implemented in Python2 using
the Keras framework3 with the Theano [43] back-end. All
experiments were performed under Linux OS on a machine
with CPU Intel Core i7-5960X @ 3.50GHz, GPU NVIDIA
GeForce Titan X, and 128 GB of RAM.

B. Results

Table II presents a comparison between different network
conﬁgurations. The bold line corresponds to the proposed
CNN, while the rest correspond to models that differ from
the proposed in only one aspect, as speciﬁed in the ﬁrst
column. The rest of the columns provide the number of model
parameters, the average inference time per (single-lung) slice,
and the average balanced accuracy across the ﬁve validation
sets.

The proposed model achieved top performance with accu-
racy nearly equal to 82% and inference time 58ms. The use
of 64 kernels per layer instead of 32 did indeed improve the
results, yet not signiﬁcantly enough and with higher inference
times, whereas the network with 16 kernels performed notably
worse. On reducing the dilated convolutional layers from 10
to 9, we observed a relatively small reduction in the accuracy.
However, we chose to keep 10 layers, since the difference in
memory and time requirements was also small and because
the resulting receptive ﬁeld was comparable with that of the
state of the art networks used for comparison. The use of semi-
supervised learning yielded an improvement of nearly 1.5%,
with no additional requirements in computational resources.
We also performed an experiment with exponential increase in
the dilation rates of the consecutive layers, similarly to [19]
i.e. 1, 1, 2, 4, 8, 16, 32 and 64. The resulting model was
smaller and faster, since 2 fewer layers were required to
achieve similar receptive ﬁeld, however the accuracy decreased
by almost 3%. In the case where the convolutions were not
dilated, the network performed poorly, because of the radical
decrease of the receptive ﬁeld. The accuracy of the proposed

2https://github.com/intact-project/LungNet
3https://github.com/fchollet/keras

6

Fig. 5: Accuracy curves for different values of wu.

TABLE III: Comparison with previous studies

Network

Number of
parameters
×105

Average
inference time
ms

CV balanced
accuracy
%

ILD-CNN [5]
Segnet [15]
U-net [17]

Proposed

0.9
335
310

1.3

237
111
88

58

72.2
73.6
77.5

81.8

model without any normalization was substantially poorer,
probably because it could not properly handle the contrast
differences among the scans caused by different CT scanners
and reconstruction kernels. The use of instance normaliza-
tion improved the performance by adaptively normalizing
the feature contrast for each input. However, this kind of
normalization also normalizes the mean intensity that could be
a useful feature. By adding the InstanceNorm skip connection
(Fig. 4), the accuracy improved even further. We speculate this
is because the mean normalization is diminished, while the
resulting variance normalization is only partially invariant to
contrast shifts. Finally, omitting the concatenation of the ﬁrst
10 layers also resulted in signiﬁcant impairment of the results,
which was expected since only 32 features are considered.

In Fig. 5 the accuracy curves for different values of wu
are presented. These curves are generated by averaging over
the ﬁve folds the best accuracies achieved this far by each
model in each epoch. The curve for the model without the
unsupervised learning was also included for comparison. The
best performing conﬁguration proved to be the one with wu =
0.1, which we utilized for the training of the proposed model.
Table III presents a comparison between the proposed
network and three previous studies. It has to be noted that all
models used the same unsupervised weight (wu = 0.1) and
whenever batch normalization was performed, this was based
on batch statistics (instance normalization) since this yielded
the best results. Fig. 7 illustrates a few segmentation results
for each of the models in Table III.

The ﬁrst line of the table refers to our previous work [5],
which has been converted into a fully convolutional network
so it can accept arbitrarily sized images for input. Its low
accuracy is probably due to the small receptive ﬁeld (33×33)
and the extensive pooling. This architecture was sufﬁcient to
describe the local texture of the 32×32 single-class patches
in [5], but could not capture higher level structure that is
present in the whole-lung dataset of this study. The results
of the model in Fig. 7 show its noisy output near the lung
boundaries or between patterns, where context information
could be useful. Segnet [15] and U-net [17] yielded better
results, with the latter being slightly faster and substantially
more accurate. Both models have a very high number of
parameters and large enough receptive ﬁelds to capture any
relevant information. The superior performance of U-net could
be attributed to its skip connections that allow features from
the lower scales to directly contribute to its output. Indeed,
Fig. 7 illustrates the more detailed results of U-net as opposed
to the overly smoothed areas produced by Segnet. Finally, the
proposed network yielded the best results, while being faster
and having far fewer parameters. The output examples in Fig. 7
indicate that the proposed model manages to keep a better
balance between ﬁne details and smooth border among the
different classes. Even thought it is really difﬁcult to visually
assess the performance of the system for the different classes,
there are a few examples in Fig. 7 with wrong classiﬁcations
on which we can comment. Firstly, parts of the broncho-
vascular tree in the third row were recognized as consolidation
because of their similar densities, while accentuated terminal
bronchial parts, that might be physiological as well, caused
the erroneous classiﬁcation of healthy areas into reticulation,
in the ﬁrst row. Some mistakes however are also attributed in
the limited number of annotated classes. For example in row
6, there are emphysematic areas (dark area in the center of the
lung) that have been annotated as healthy due to their similar
density. Figure 6 shows the confusion matrix of the proposed
model. As expected, many of the misclassiﬁcations occur
between reticulation and honeycombing due to their similar
textural appearance. Moreover, healthy tissue is often confused
with reticulation probably because of the 2D sections of the
bronchovascular tree that could resemble reticular patterns.

V. CONCLUSIONS

In this study, we proposed and evaluated a deep CNN
for the semantic segmentation of pathological lung tissue on
HRCT slices. The CNN is designed under a fully convo-
lutional scheme and thus can handle variable input sizes,
while it was trained in an end-to-end and semi-supervised
fashion. The main characteristic of the proposed network is
the use of dilated convolutions along with an instance variance
normalization scheme, and multi-scale feature fusion. The
training and testing of the network was performed using a
cross validation scheme on a dataset of 172 cases, whereas
the split of the dataset into folds was performed per case. The
proposed network surpassed the highest performance in pre-
vious studies, and is much more efﬁcient in terms of memory
and computation. Future work includes the modiﬁcation of

7

Fig. 6: Confusion matrix of the proposed model as calculated
over the cross validation scheme. The numbers represent
percentages of pixels across all validation images.

the model to consider the 3D nature of lung patterns, and
to account for the bronchovascular tree. The former could
be achieved by a direct extension of the architecture to 3D,
similarly to 3D U-Net [44] and V-Net [45] or by employing
a multi-planar view aggregation scheme, also referred to as
2.5D, [46]. Alternatively, a 3D post processing scheme could
be used to reﬁne the 2D segmentation output using conditional
random ﬁelds or deformation models [47], [48], [49]. Finally,
the result of a bronchovascular segmentation method could be
utilized by the network to reduce false alarms.

REFERENCES

[1] B. SOCIETY, “The diagnosis, assessment and treatment of diffuse
parenchymal lung disease in adults,” Thorax, vol. 54, no. Suppl 1, p. S1,
1999.

[2] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp.
211–252, 2015.

[3] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, Nov 1998.

[4] H. Greenspan, B. van Ginneken, and R. M. Summers, “Guest editorial
deep learning in medical imaging: Overview and future promise of
an exciting new technique,” IEEE Transactions on Medical Imaging,
vol. 35, no. 5, pp. 1153–1159, 2016.

[5] M. Anthimopoulos, S. Christodoulidis, L. Ebner, A. Christe, and
S. Mougiakakou, “Lung pattern classiﬁcation for interstitial lung dis-
eases using a deep convolutional neural network,” IEEE Transactions
on Medical Imaging, vol. 35, no. 5, pp. 1207–1216, May 2016.

[6] S. Christodoulidis, M. Anthimopoulos, L. Ebner, A. Christe, and
S. Mougiakakou, “Multisource transfer learning with convolutional
neural networks for lung pattern analysis,” IEEE Journal of Biomedical
and Health Informatics, vol. 21, no. 1, pp. 76–84, Jan 2017.

[7] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.
[8] N. Srivastava, G. Hinton, A. Krizhevsky,

and
R. Salakhutdinov, “Dropout: A simple way to prevent neural
networks from overﬁtting,” J. Mach. Learn. Res., vol. 15, no. 1, pp.
1929–1958, Jan. 2014.

I. Sutskever,

[9] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.

8

Fig. 7: Output examples for the models of Table III. From left to right: Ground Truth, ILD-CNN, Segnet, U-net, Proposed. Each
example has a different pattern annotated. From top to bottom: Healthy (Blue), Ground Glass Opacity (Purple), Micronodules
(Green), Consolidation (Yellow), Reticulation (Orange) and Honeycombing (Red).

9

[32] Q. Wang, Y. Zheng, G. Yang, W. Jin, X. Chen, and Y. Yin, “Multi-
scale rotation-invariant convolutional neural networks for lung texture
classiﬁcation,” IEEE Journal of Biomedical and Health Informatics,
vol. PP, no. 99, pp. 1–1, 2017.

[33] H. C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao,
D. Mollura, and R. M. Summers, “Deep convolutional neural networks
for computer-aided detection: Cnn architectures, dataset characteristics
and transfer learning,” IEEE Transactions on Medical Imaging, vol. 35,
no. 5, pp. 1285–1298, May 2016.

[34] M. Gao, Z. Xu, L. Lu, A. P. Harrison, R. M. Summers, and D. J. Mollura,
“Holistic interstitial lung disease detection using deep convolutional
neural networks: Multi-label learning and unordered pooling,” arXiv
preprint arXiv:1701.05616, 2017.

[35] A. Depeursinge, A. Vargas, A. Platon, A. Geissbuhler, P.-A. Poletti, and
H. M¨uller, “Building a reference multimedia database for interstitial lung
diseases,” Computerized medical imaging and graphics, vol. 36, no. 3,
pp. 227–238, 2012.

[36] P. Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, and
G. W. Cottrell, “Understanding convolution for semantic segmentation,”
CoRR, vol. abs/1702.08502, 2017.

[37] F. Yu, V. Koltun, and T. A. Funkhouser, “Dilated residual networks,”

CoRR, vol. abs/1705.09914, 2017.

[38] D. Ulyanov, A. Vedaldi, and V. S. Lempitsky, “Improved texture
networks: Maximizing quality and diversity in feed-forward stylization
and texture synthesis,” CoRR, vol. abs/1701.02096, 2017.

[39] P. Isola, J. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
with conditional adversarial networks,” CoRR, vol. abs/1611.07004,
2016.

[40] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

CoRR, vol. abs/1412.6980, 2014.

[41] Y. Grandvalet and Y. Bengio, “Semi-supervised learning by entropy
minimization,” in Proceedings of the 17th International Conference
on Neural Information Processing Systems, ser. NIPS’04. Cambridge,
MA, USA: MIT Press, 2004, pp. 529–536.

[42] D.-H. Lee, “Pseudo-label: The simple and efﬁcient semi-supervised
learning method for deep neural networks,” in Workshop on Challenges
in Representation Learning, ICML, vol. 3, 2013, p. 2.

[44]

[43] Theano Development Team, “Theano: A Python framework for
fast computation of mathematical expressions,” arXiv e-prints, vol.
abs/1605.02688, May 2016.
¨O. C¸ ic¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,
“3d u-net: learning dense volumetric segmentation from sparse anno-
tation,” in International Conference on Medical Image Computing and
Computer-Assisted Intervention. Springer, 2016, pp. 424–432.
[45] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional
neural networks for volumetric medical image segmentation,” in 3D
Vision (3DV), 2016 Fourth International Conference on.
IEEE, 2016,
pp. 565–571.

[46] H. R. Roth, L. Lu, J. Liu, J. Yao, A. Seff, K. Cherry, L. Kim, and R. M.
Summers, “Improving computer-aided detection using convolutional
neural networks and random view aggregation,” IEEE transactions on
medical imaging, vol. 35, no. 5, pp. 1170–1181, 2016.

[47] F. Liu, Z. Zhou, H. Jang, A. Samsonov, G. Zhao, and R. Kijowski,
“Deep convolutional neural network and 3d deformable approach for
tissue segmentation in musculoskeletal magnetic resonance imaging,”
Magnetic resonance in medicine, vol. 79, no. 4, pp. 2379–2391, 2018.
[48] P. F. Christ, M. E. A. Elshaer, F. Ettlinger, S. Tatavarty, M. Bickel,
P. Bilic, M. Rempﬂer, M. Armbruster, F. Hofmann, M. DAnastasi et al.,
“Automatic liver and lesion segmentation in ct using cascaded fully
convolutional neural networks and 3d conditional random ﬁelds,” in
International Conference on Medical Image Computing and Computer-
Assisted Intervention. Springer, 2016, pp. 415–423.

[49] K. Kamnitsas, C. Ledig, V. F. Newcombe, J. P. Simpson, A. D. Kane,
D. K. Menon, D. Rueckert, and B. Glocker, “Efﬁcient multi-scale 3d
cnn with fully connected crf for accurate brain lesion segmentation,”
Medical image analysis, vol. 36, pp. 61–78, 2017.

[10] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2015, pp. 1–9.

[11] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2016, pp. 770–778.

[12] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” CoRR, vol.
abs/1502.03167, 2015.

[13] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-
tional networks,” in European conference on computer vision. Springer,
2014, pp. 818–833.

[14] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2015, pp. 3431–3440.
[15] V. Badrinarayanan, A. Handa, and R. Cipolla, “Segnet: A deep con-
volutional encoder-decoder architecture for robust semantic pixel-wise
labelling,” arXiv preprint arXiv:1505.07293, 2015.

[16] H. Noh, S. Hong, and B. Han, “Learning deconvolution network
for semantic segmentation,” in Proceedings of the IEEE International
Conference on Computer Vision, 2015, pp. 1520–1528.

[17] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” CoRR, vol. abs/1505.04597, 2015.
[18] M. Holschneider, R. Kronland-Martinet, J. Morlet, and P. Tchamitchian,
“A real-time algorithm for signal analysis with the help of the wavelet
transform,” in Wavelets. Springer, 1990, pp. 286–297.

[19] F. Yu and V. Koltun, “Multi-scale context aggregation by dilated

convolutions,” CoRR, vol. abs/1511.07122, 2015.

[20] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A deep
neural network architecture for real-time semantic segmentation,” CoRR,
vol. abs/1606.02147, 2016.

[21] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille, “Deeplab: Semantic image segmentation with deep convolutional
nets, atrous convolution, and fully connected crfs,” arXiv preprint
arXiv:1606.00915, 2016.

[22] R. Uppaluri, E. A. Hoffman, M. Sonka, P. G. Hartley, G. W. Hunning-
hake, and G. McLennan, “Computer recognition of regional lung disease
patterns,” American journal of respiratory and critical care medicine,
vol. 160, no. 2, pp. 648–654, 1999.

[23] I. C. Sluimer, P. F. van Waes, M. A. Viergever, and B. van Ginneken,
“Computer-aided diagnosis in high resolution ct of the lungs,” Medical
physics, vol. 30, no. 12, pp. 3081–3090, 2003.

[24] L. Sorensen, S. B. Shaker, and M. De Bruijne, “Quantitative analysis of
pulmonary emphysema using local binary patterns,” IEEE transactions
on medical imaging, vol. 29, no. 2, pp. 559–569, 2010.

[25] M. Anthimopoulos, S. Christodoulidis, A. Christe, and S. Mougiakakou,
“Classiﬁcation of interstitial lung disease patterns using local dct features
and random forest,” in 2014 36th Annual International Conference of
the IEEE Engineering in Medicine and Biology Society, Aug 2014, pp.
6040–6043.

[26] M. Gangeh, L. Sørensen, S. Shaker, M. Kamel, M. De Bruijne, and
M. Loog, “A texton-based approach for the classiﬁcation of lung
parenchyma in ct images,” Medical Image Computing and Computer-
Assisted Intervention–MICCAI 2010, pp. 595–602, 2010.

[27] A. Foncubierta-Rodr´ıguez, A. Depeursinge, and H. M¨uller, “Using
multiscale visual words for lung texture classiﬁcation and retrieval,” in
MICCAI International Workshop on Medical Content-Based Retrieval
for Clinical Decision Support. Springer, 2011, pp. 69–79.

[28] W. Zhao, R. Xu, Y. Hirano, R. Tachibana, and S. Kido, “Classiﬁcation of
diffuse lung diseases patterns by a sparse representation based method on
hrct images,” in Engineering in Medicine and Biology Society (EMBC),
2013 35th Annual International Conference of the IEEE.
IEEE, 2013,
pp. 5457–5460.

[29] K. T. Vo and A. Sowmya, “Multiscale sparse representation of high-
resolution computed tomography (hrct) lung images for diffuse lung
disease classiﬁcation,” in Image Processing (ICIP), 2011 18th IEEE
International Conference on.

IEEE, 2011, pp. 441–444.

[30] G. van Tulder and M. de Bruijne, “Learning features for tissue classi-
ﬁcation with the classiﬁcation restricted boltzmann machine,” in Inter-
national MICCAI Workshop on Medical Computer Vision.
Springer,
2014, pp. 47–58.

[31] Q. Li, W. Cai, X. Wang, Y. Zhou, D. D. Feng, and M. Chen, “Med-
ical image classiﬁcation with convolutional neural network,” in 2014
13th International Conference on Control Automation Robotics Vision
(ICARCV), Dec 2014, pp. 844–848.

Semantic Segmentation of Pathological Lung Tissue
with Dilated Fully Convolutional Networks

Marios Anthimopoulos, Member, IEEE, Stergios Christodoulidis, Member, IEEE, Lukas Ebner, Thomas Geiser,
Andreas Christe, and Stavroula Mougiakakou*, Member, IEEE

1

8
1
0
2
 
r
a

M
 
6
1
 
 
]

V
C
.
s
c
[
 
 
1
v
7
6
1
6
0
.
3
0
8
1
:
v
i
X
r
a

Abstract—Early and accurate diagnosis of interstitial

lung
diseases (ILDs) is crucial for making treatment decisions, but
can be challenging even for experienced radiologists. The diag-
nostic procedure is based on the detection and recognition of
the different ILD pathologies in thoracic CT scans, yet their
manifestation often appears similar. In this study, we propose
the use of a deep purely convolutional neural network for the
semantic segmentation of ILD patterns, as the basic component of
a computer aided diagnosis (CAD) system for ILDs. The proposed
CNN, which consists of convolutional layers with dilated ﬁlters,
takes as input a lung CT image of arbitrary size and outputs
the corresponding label map. We trained and tested the network
on a dataset of 172 sparsely annotated CT scans, within a cross-
validation scheme. The training was performed in an end-to-
end and semi-supervised fashion, utilizing both labeled and non-
labeled image regions. The experimental results show signiﬁcant
performance improvement with respect to the state of the art.

Index Terms—Interstitial

lung disease, Fully convolutional
neural networks, Dilated convolutions, Texture segmentation,
Semi-supervised learning

I. INTRODUCTION

I NTERSTITIAL lung disease (ILD) is a group of more than

200 chronic lung disorders characterized by inﬂammation
and scarring of the lung tissue that leads to respiratory failure.
ILD accounts for 15 percent of all cases seen by pulmonolo-
gists and can be caused by autoimmune disease, genetic abnor-
malities, infections, drugs or long-term exposure to hazardous
materials. In many cases the cause remains unknown and the

Manuscript received March 19, 2018
This research was carried out within the framework of the IntACT research
project, supported by Bern University Hospital,“Inselspital” and the Swiss
National Science Foundation (SNSF) under Grant 156511.

M. Anthimopoulos and S. Christodoulidis contributed equally to this work.

The asterisk indicates the corresponding author.

M. Anthimopoulos is with the ARTORG Center for Biomedical Engineering
Research, University of Bern, 3008 Bern, Switzerland, and the Department
of Emergency Medicine, Bern University Hospital “Inselspital”, 3010 Bern,
Switzerland (e-mail: marios.anthimopoulos@artorg.unibe.ch).

S. Christodoulidis is with the ARTORG Center for Biomedical Engi-
neering Research, University of Bern, 3008 Bern, Switzerland (e-mail: ster-
gios.christodoulidis@artorg.unibe.ch).

T. Geiser

is with the University Clinic for Pneumonology, Bern
(e-mail:

3010 Bern, Switzerland

“Inselspital”,

University Hospital
thomas.geiser@insel.ch)

S. Mougiakakou* is with the Department of Diagnostic,

L. Ebner and A. Christe are with the Department of Diagnostic, Interven-
tional and Pediatric Radiology, Bern University Hospital “Inselspital”, 3010
Bern, Switzerland (e-mails: lukas.ebner@insel.ch; andreas.christe@insel.ch).
Interven-
tional and Pediatric Radiology, Bern University Hospital “Inselspital”,
3010 Bern, Switzerland, and the ARTORG Center for Biomedical En-
gineering Research, University of Bern, 3008 Bern Switzerland (e-mail:
stavroula.mougiakakou@artorg.unibe.ch).

Source code available at: https://github.com/intact-project/LungNet

disease is described as idiopathic. The diagnosis of ILD is
mostly performed by radiologists and is usually based on the
assessment of the different ILD pathologies in high resolution
computed tomography (HRCT) thoracic scans. Early diagnosis
is crucial for making treatment decisions, while misdiagnosis
may lead to life-threatening complications [1]. Extensive re-
search has been conducted on the development of computer-
aided diagnosis (CAD) systems, which are able to support
clinicians and improve their diagnostic performance. The basic
characteristics of such a system are the automatic detection and
recognition of the pathological lung tissue. Pathological tissue
is usually manifested as various textural patterns in the CT
scan. This ILD pattern recognition procedure is traditionally
performed by a local texture classiﬁcation scheme that slides
across the images and outputs a map of pathologies, which is
later used to reach a ﬁnal diagnosis. For texture recognition,
a great variety of handcrafted image features and machine
learning classiﬁers have been utilized.

Recently, deep artiﬁcial neural networks (ANN) and, in
particular, deep convolutional neural networks (CNNs) have
gained a lot of attention after their impressive results in the
ImageNet Large Scale Visual Recognition Competition in
2012 [2]. Networks of this kind have existed for decades [3],
but have only recently managed to achieve adequate perfor-
mance mainly due to the large volumes of available annotated
data, the massive parallelization capabilities of GPUs and a
few design tricks. The potential beneﬁts of deep learning tech-
niques in medical image analysis have also been investigated
recently and the ﬁrst results have been promising [4]. In [5] ,
we designed, trained and tested a deep CNN as a ﬁxed-scale,
local texture classiﬁer that outperformed traditional methods
in ILD pattern classiﬁcation. However, training large networks
on medical images can often be challenging due to the lack
of databases that are adequately sized to satisfy the needs of
these models. Medical data are scarce and collecting them is
a difﬁcult and time consuming process, while their annotation
has to be performed by multiple specialists to ensure its
validity. To this end, in [6], we investigated the potential
use of general
in a multi-source
transfer learning scheme that yielded signiﬁcant improvements
in performance. One remaining limitation in these works is the
local nature of the classiﬁer that requires rigorous scanning
of the input image with a sliding window, and simultaneous
aggregation of the results in the output. This classiﬁcation
scheme can be signiﬁcantly time-consuming while it may also
ignore less local but useful information, if the input size is not
appropriately conﬁgured.

texture image databases,

In this study, we propose the use of a deep fully-
convolutional network for the problem of ILD pattern recog-
nition that uses dilated convolutions and is trained in an end-
to-end and semi-supervised manner. The proposed CNN takes
as input a lung HRCT image of arbitrary size and outputs
the corresponding label map, thus avoiding the limitations of
a sliding window model. Additionally, the utilization of non-
labeled image regions in the learning procedure, permits robust
training of larger models and proves to be particularly useful
when using databases with sparse annotations.

II. RELATED WORK

In this section, we provide a short review of the recent
advances in deep learning for computer vision, followed by a
brief overview of previous studies on ILD pattern classiﬁcation

A. Deep CNNs for Computer Vision

Although CNNs have existed for decades [3], they only
became widely popular after their remarkable success in the
ImageNet challenge of 2012 [2]. The winning approach of the
competition [7], also known as AlexNet, was a deep CNN
with ﬁve convolutional and three dense layers, each followed
by a rectiﬁed linear unit (ReLU), while increased strides
were used for the max pooling and convolution operations to
gradually down-sample the feature maps. Dropout [8] and data
augmentation were also utilized, in order to prevent overﬁtting.
Since then, the proposed deep CNNs have led to continuous
improvements in the results on ImageNet and other datasets,
mainly by enhancing their architecture and by increasing their
depth and width. The VGG network [9] reduced the size of
the kernels to 3×3, while increasing the number of layers
to 19. GoogleNet [10] used consecutive inception modules,
where different convolutional and pooling operations are per-
formed in parallel with their outputs merged. This approach
drastically reduced the number of parameters while improving
the results. ResNet [11] introduced skip connections between
layers which permitted the training of networks with hundreds
of layers and pushed the limits of deep CNNs even further. The
batch normalization (BN) technique [12] also supported these
developments, by regularizing and accelerating the training
procedure.

Many of the already proposed CNNs have recently been
adapted to perform semantic segmentation, rather than just
image classiﬁcation. The term semantic segmentation refers
to the task of assigning a class label to every pixel of an
image. A simple approach to this task is to use any ﬁxed-
scale classiﬁcation method under a sliding window scheme and
then to aggregate the results to build a label map. However,
this could be highly inefﬁcient as local image features would
have to be recalculated multiple times for adjacent positions of
the input window. Luckily, the convolutional layers (with the
appropriate padding) in a typical CNN produce feature maps
that maintain spatial correspondence with the input image.
Therefore, input images of any size can be fed to the network
and each pixel can be classiﬁed, on the basis of the values of
the respective feature map position. This can be achieved by
utilizing convolutional layers of size 1 × 1 that serve as local

2

(a) D = 1

(b) D = 2

(c) D = 3

Fig. 1: Dilated convolution kernels. D denotes the dilation rate

dense layers and, these networks are therefore often referred
to as fully convolutional (FCNs).

However, the spatial correspondence between the input and
output of a CNN, can be disrupted by the use of down-
sampling operations such, as strided pooling and convolution.
Down-sampling of the feature maps is often used to increase
the receptive ﬁeld of the network with respect to the input,
as well as to reduce the amount of computational load. In
order to restore the original size of the input, researchers
have used encoder-decoder architectures, where the encoder
usually adopts a well-known architecture such as VGG [9]
and the decoder reverses the process by mapping the feature
representation back to the input data space. To this end,
upsampling operations and transposed convolution [13] (also
known as fractionally strided convolution or “deconvolution”)
have been used for semantic segmentation [14]. Alternatively,
in [15] and [16], max unpooling has been used as the inverse
operation of each max pooling layer, where the pairs of
pooling/unpooling layers are coupled by transferring the max
indices from the encoder to the decoder. In [17], a similar
architecture was proposed for biomedical image segmentation,
with additional skip connections that concatenate the feature
maps of an encoding layer to the feature maps of the same-
scale decoding layer.

Recently, some CNNs for semantic segmentation have been
proposed that use dilated convolutions to increase the receptive
ﬁeld,
instead of downsampling the feature maps. Dilated
convolution, also called `atrous, is the convolution with kernels
that have been dilated by inserting zero holes (`atrous in
French) between the non-zero values of a kernel. This was
originally proposed for efﬁcient wavelet decomposition in a
scheme also known as “algorithme `atrous” [18]. Figure 1
shows examples of kernels with different dilation rates. Dilated
convolution can increase the receptive ﬁeld without increasing
the number of parameters, as opposed to normal convolution.
Moreover, feature maps are densely computed on the original
image resolution without the need for downsampling. In [19],
a CNN module with dilated convolutions was designed to ag-
gregate multiscale contextual information and improve the per-
formance of state-of-the-art semantic segmentation systems.
The module has eight convolutional layers with exponentially
increasing dilation rates (i.e. 1, 1, 2, 4, 8, 16), resulting in an
exponential increase in the receptive ﬁeld, while the number
of parameters is only grown linearly. Similarly, expansion
of the receptive ﬁeld was achieved in [20] by integrating
dilated convolutions in a bottleneck module that was designed
the `atrous spatial pyramid pooling
for efﬁciency. In [21],
(ASPP) scheme is proposed that uses multiple parallel dilated

convolutional layers, in order to capture information from
multiple scales.

B. ILD Pattern Classiﬁcation

Over the last twenty years, numerous approaches have been
proposed for the problem of ILD pattern recognition, which
is generally regarded as a texture classiﬁcation problem. Most
of the proposed methods involve hand-crafted texture features
which are fed to machine learning classiﬁers, and locally rec-
ognize lung tissue within a sliding window framework. In one
of the early studies [22], the adaptive multiple feature method
(AMFM) was proposed, which utilizes a combination of gray-
level histograms, co-occurrence and run-length matrices, as
well as fractal analysis parameters. For the classiﬁcation, a
Bayesian classiﬁer was used. In [23], a ﬁlter bank of Gaussian
and Laplacian kernels was applied on the input images and
the histogram moments of the responses were fed to a linear
discriminant classiﬁer. The simple, yet powerful, Local Binary
Pattern (LBP) descriptor has also been proposed [24], com-
bined with a k-nearest neighbors classiﬁer. In [25], a random
forest classiﬁer was utilized that was trained on local DCT
features. More recently, some proposed methods have adopted
unsupervised feature extraction techniques, such as bag of
features [26], [27] and sparse representation models [28], [29].
Lately, a few methods have been proposed that utilize CNNs
for lung pattern classiﬁcation. Most of these are still designed
under a patch-wise scheme where a square patch is fed to the
CNN, while the output consists of the probabilities for this
patch to belong to each class. Although the strong descriptive
capabilities of modern CNNs are commonly attributed to their
depth, the ﬁrst studies utilized rather shallow architectures.
A modiﬁed RBM that resembles a convolutional layer was
used in [30], whereas in [31], a CNN was proposed with
one convolutional and three fully-connected layers. More
recently, some attempts have also been made to utilize deeper
architectures. In our previous work [5], a CNN with ﬁve
convolutional and three dense layers was designed and trained
on ILD data, while in [6] its performance was improved using
knowledge transfer from other domains. In another study [32],
a CNN with three convolutional and one dense layer was fed
with rotational invariant Gabor-LBP representations of lung
tissue patches. Finally, in [33] and [34], the authors utilized
well established pretrained CNN architectures such as AlexNet
and GoogleNet which were further ﬁnetuned for detecting
possible “presence/absence” of pathologies at a slice level.
However, these architectures were designed to classify natural
color images with size 224 × 224, so the authors had to resize
the images and artiﬁcially generate three channels by applying
different Hounsﬁeld unit (HU) windows.

III. MATERIALS AND METHODS

This section presents the proposed fully convolutional neu-
ral network for semantic lung tissue segmentation. Prior to
this, we describe the materials used for training and testing
the network.

3

TABLE I: Data statistics across the considered classes i.e.
Healthy (H), Ground Glass Opacity (GGO), Micronodules
(MN), Consolidation (Cons), Reticulation (Ret) and Honey-
combing (HC).

H

GGO MN

Cons

Ret

HC

Totals

#Pixels×105
#Cases

92.5
66

27.7
82

35.8
15

7.08
46

28.2
81

20.1
47

211.4
172

A. Materials

For the purposes of this study, we compiled a dataset of 172
HRCT scans, each corresponding to a unique ILD or healthy
subject. The dataset contains 109 cases from the publicly
available multimedia database of interstitial lung diseases [35]
by the Geneva University Hospital (HUG), along with 63 cases
from Bern University Hospital - “Inselspital” (INSEL), as col-
lected by the authors. The scans were acquired between 2003
and 2015 using different scanners and acquisition protocols.
The INSEL scans are volumetric, while the HUG scans have
a 10-15mm spacing. The slice thickness is 1-2mm for both
datasets.

Two experienced radiologists from INSEL annotated or re-
annotated ILD typical pathological patterns, as well as healthy
tissue in both databases1. A lung ﬁeld segmentation mask
was also provided for each case. In total six types of tissue
were considered: normal, ground glass opacity, micronodules,
consolidation, reticulation and honeycombing. It should be
emphasized that these annotations do not cover the entire lung
ﬁeld, but only the most typical manifestations of the listed ILD
patterns. This protocol was followed in both databases since
it permits the annotation of more scans for the same effort
and thus increases data diversity. On the other hand, sparse
annotations also introduce challenges. Non-annotated lung
areas have to be excluded from both supervised training and
evaluation. Another challenging characteristic of the databases
is the uneven distribution of the considered classes across the
cases. Table I provides statistics for the entire dataset, while
ﬁgure 2 presents a sample CT lung slice along with the given
annotations.

B. Methods

In this study, we propose the use of a deep purely con-
volutional network for the problem of lung tissue semantic
segmentation. The network is inspired by [19] and consists of
solely convolutional layers that use dilated kernels to increase
the receptive ﬁeld, instead of downsampling the feature maps.
This kind of network has been shown to be suitable for
similar dense prediction problems that require high resolution
precision. The proposed network (Fig. 3) has 13 convolutional
layers and a total receptive ﬁeld of 287×287. Speciﬁcally, each
of the ﬁrst ten layers has 32 kernels of size 3×3 and dilation
rates 1, 1, 2, 3, 5, 8, 13, 21, 34 and 55, respectively. We
chose not to increase the dilation rates exponentially, as is
commonly done, in order to avoid extreme gridding problems
that have been reported in several studies [36], [37]. Instead,

1ITK-snap was used for the annotation process, http://www.itksnap.org

4

ReLU function (Fig. 4) substantially improves the results. This
instance normalization skip connection cancels the mean nor-
malization of activations (when the trainable parameters have
not been trained), while it performs a kind of feature contrast
enhancement which reduces the importance of variance shift
without providing complete invariance to the latter.

The network was trained by minimizing the categorical
cross entropy using the Adam optimizer [40] with a learning
rate of 0.0001. The dense nature of the considered classiﬁca-
tion problem combined with the sparse available annotations,
resulted in two issues. Firstly, large parts of the dataset were
not annotated, and so could not be used for either supervised
training or testing. Secondly, the distribution of the considered
classes in the dataset was highly imbalanced, a fact
that
can be challenging for any classiﬁcation method. We tackled
both problems by scaling the considered loss and accuracy
with appropriate weighting schemes computed for each set.
All pixels corresponding to annotated areas were assigned a
weight inversely proportional to the number of samples of its
class in the speciﬁc set. In this way, all classes contributed
equally to the considered metrics. Furthermore, we employed
a semi-supervised learning technique to additionally exploit
non-labeled areas of the data. We added an extra term to the
supervised loss function, which corresponds to the entropy of
the network’s output on the areas that do not participate in
the supervised learning. This entropy minimization technique
has been used in different applications such as in
[41]
yielding signiﬁcant improvements in performance. Similarly in
[42] the technique of pseudo-labelling was introduced, where
the network classiﬁes non-annotated regions and then uses
them as ground truth for ﬁne-tuning. Semi-supervised learning
techniques of this kind are based on the cluster assumption i.e.
samples from the same class tend to form compact clusters. By
minimizing the entropy of the network’s output, the decision
boundaries are driven away from areas densely populated by
learning samples. If the cluster assumption holds and there is
no large overlap between the classes this method may increase
the network’s generalization ability. It acts equivalently to
manifold learning and includes self-learning as a special case,
as it increases the conﬁdence of the classiﬁer. The inﬂuence
of the semi-supervised term is controlled by an appropriate
weight, which is scaled relatively with the proportion of the
unlabeled regions versus the annotated ones. Hence, the loss
for a pixel x with output ˆy is:

L(x, ˆy) =






− (cid:80)C

i=1 wi

syi log( ˆyi), when y is given

(1)

(cid:80)C

−αwu

i=1 ˆyi log( ˆyi),
where y is the true label in one-hot encoding, C is the
number of classes, wi
s is the supervised weight for class i
(which is inversely proportional to the number of samples of
the class), α is a scaler and wu the unsupervised weight.

otherwise

The training procedure stops when the network does not
signiﬁcantly improve its performance on the validation set for
50 epochs. The performance is assessed in terms of weighted
is considered
(balanced) accuracy, while an improvement
signiﬁcant if the relative increase in performance is at least

Fig. 2: A typical slice with annotations. The white border line
denotes the lung ﬁeld segmentation, the blue denotes healthy
tissue, the purple micronodules and the red the honeycombing
pattern.

we use the ﬁrst terms of the Fibonacci sequence as dilation
rates; this mitigates the gridding problem by providing a less
steep dilation rate increase and thus denser sampling.

The output of the ﬁrst 10 layers, as well as the input
of the network, are concatenated, thus leading to 1+10×32
= 321 feature maps, which are passed through a dropout
layer with a rate of 0.5 and fed to the rest of the network.
This concatenation is allowed by the lack of pooling layers
and the appropriate zero padding for each convolution and
brings several beneﬁts. It permits the aggregation of features
from all different scales and levels of abstraction, while it
also facilitates the ﬂow of gradients thought the network and
therefore allows faster training. The last three layers have 1×1
kernels and play the role of locally dense layers that reduce
the feature dimensionality for each pixel from 321 to 128,
32 and ﬁnally 6, which is the number of classes considered.
The output is converted into a probability distribution by the
softmax function.

A BN layer follows each convolution and is based on the
batch statistics in both training and test time. This is permitted,
as the batch size is one, so there is always a full batch during
inference. This approach has been proposed before, under
the term instance normalization (InstanceNorm) [38], and
has exhibited good performance in texture synthesis, image
stylization and image to image translation [39]. InstanceNorm
provides invariance to intensity and contrast shifts, which
makes the features adaptive for each slice and could mitigate
problems caused by different CT scanners and reconstruction
kernels. We also found that adding the normalized activations
to the non-normalized ones, before passing them through the

InstanceNorm

Input

+

(3x3), D@1

(3x3), D@1

(3x3), D@2

(3x3), D@3

(3x3), D@5

(3x3), D@8

(3x3), D@13

(3x3), D@21

(3x3), D@34

(3x3), D@55

Concatenate

Dropout

(1x1), D@1

(1x1), D@1

(1x1), D@1

Softmax Output

Fig. 3: The architecture of the proposed network. Each gray
box corresponds to a block like the one presented in Fig. 4

0.5%. In order to artiﬁcially increase the volume of training
data and avoid overﬁtting, we transformed the images using
ﬂips and rotations, which are considered label-preserving in
this domain. The augmentation was performed online i.e. for
each training image in each epoch, one operation out of all
eight combinations of ﬂip and rotate is randomly selected and
applied.

5

Dilated Convolution
(k,k), D@x

InstanceNorm

+

ReLU

Fig. 4: The block function of the proposed architecture. (k, k)
is the size of the convolution kernel and x is the dilation rate.

IV. EXPERIMENTAL SETUP AND RESULTS

In this section, we ﬁrst present the setup of the experiments
conducted, followed by the corresponding results that justify
the algorithmic choices of the proposed method and compare
it to the state of the art.

A. Experimental Setup

Given the relatively small size of the dataset with respect
to the diversity of the problem, we adopted a 5-fold cross
validation (CV) scheme to ensure the validity of the results.
The data splitting was performed per scan, so tissue from one
case was never present in more than one set. Speciﬁcally, the
172 scans of the dataset were divided into ﬁve non-overlapping
sets, with one of them having 36 and the rest 34 scans. Every
time a model was tested on a speciﬁc set, the rest of the data
were used for training. On average over all folds, the number
of slices was 2060 for training and 515 for testing. As principal
performance metric, we used the balanced accuracy (Eq. 2),
averaged over the ﬁve folds.

BACC =

1
N

N
(cid:88)

i=1

ci
ni

(2)

where N is the number of classes, ci is the number of
correctly classiﬁed samples of class i and ni
is the total
number of samples of class i. Since the slices were only
sparsely annotated, the accuracy was calculated over the areas
of the scans where a ground truth was available.

In order to avoid extreme class imbalances between the
different sets, data splitting was performed using a simple hill
climbing technique that maximizes the entropy of the class
distribution for the ﬁve sets. The methods started from an
arbitrary split and then randomly swapped two cases between
two sets in an attempt to ﬁnd a more balanced solution. If the
new solution has a higher class distribution entropy (averaged
over the 5 sets), we retained it and repeated the procedure
until no further improvement was possible.

To minimize the number of computations and memory re-
quirements, we discarded part of the data that lack annotations.
Hence, we cropped the left and right lung on each slice and
used only the ones with relevant annotations as inputs of the

TABLE II: Comparison of the different network conﬁgurations

Network
conﬁguration

Number of
parameters
×105

Average
inference time
ms

CV balanced
accuracy
%

w/o dilated convolutions
w/o concatenation
w/o InstanceNorm
w/o InstanceNorm skip
16 kernels/layer
Exponential dilation [19]
Purely supervised
9 dilated layers
Proposed
64 kernels/layer

1.30
0.93
1.29
1.30
0.47
1.03
1.30
1.18
1.30
4.23

51
53
38
57
51
48
58
53
58
82

68.0
72.6
77.9
78.6
79.2
79.5
80.6
81.3
81.8
82.1

networks. This is permitted by the fully-convolutional nature
of the tested networks that do not require a ﬁxed input size.
For the cropping, we utilized the available lung mask, while a
margin of 32 pixels was added on each side to provide context
that could be useful to the networks.

The proposed method was implemented in Python2 using
the Keras framework3 with the Theano [43] back-end. All
experiments were performed under Linux OS on a machine
with CPU Intel Core i7-5960X @ 3.50GHz, GPU NVIDIA
GeForce Titan X, and 128 GB of RAM.

B. Results

Table II presents a comparison between different network
conﬁgurations. The bold line corresponds to the proposed
CNN, while the rest correspond to models that differ from
the proposed in only one aspect, as speciﬁed in the ﬁrst
column. The rest of the columns provide the number of model
parameters, the average inference time per (single-lung) slice,
and the average balanced accuracy across the ﬁve validation
sets.

The proposed model achieved top performance with accu-
racy nearly equal to 82% and inference time 58ms. The use
of 64 kernels per layer instead of 32 did indeed improve the
results, yet not signiﬁcantly enough and with higher inference
times, whereas the network with 16 kernels performed notably
worse. On reducing the dilated convolutional layers from 10
to 9, we observed a relatively small reduction in the accuracy.
However, we chose to keep 10 layers, since the difference in
memory and time requirements was also small and because
the resulting receptive ﬁeld was comparable with that of the
state of the art networks used for comparison. The use of semi-
supervised learning yielded an improvement of nearly 1.5%,
with no additional requirements in computational resources.
We also performed an experiment with exponential increase in
the dilation rates of the consecutive layers, similarly to [19]
i.e. 1, 1, 2, 4, 8, 16, 32 and 64. The resulting model was
smaller and faster, since 2 fewer layers were required to
achieve similar receptive ﬁeld, however the accuracy decreased
by almost 3%. In the case where the convolutions were not
dilated, the network performed poorly, because of the radical
decrease of the receptive ﬁeld. The accuracy of the proposed

2https://github.com/intact-project/LungNet
3https://github.com/fchollet/keras

6

Fig. 5: Accuracy curves for different values of wu.

TABLE III: Comparison with previous studies

Network

Number of
parameters
×105

Average
inference time
ms

CV balanced
accuracy
%

ILD-CNN [5]
Segnet [15]
U-net [17]

Proposed

0.9
335
310

1.3

237
111
88

58

72.2
73.6
77.5

81.8

model without any normalization was substantially poorer,
probably because it could not properly handle the contrast
differences among the scans caused by different CT scanners
and reconstruction kernels. The use of instance normaliza-
tion improved the performance by adaptively normalizing
the feature contrast for each input. However, this kind of
normalization also normalizes the mean intensity that could be
a useful feature. By adding the InstanceNorm skip connection
(Fig. 4), the accuracy improved even further. We speculate this
is because the mean normalization is diminished, while the
resulting variance normalization is only partially invariant to
contrast shifts. Finally, omitting the concatenation of the ﬁrst
10 layers also resulted in signiﬁcant impairment of the results,
which was expected since only 32 features are considered.

In Fig. 5 the accuracy curves for different values of wu
are presented. These curves are generated by averaging over
the ﬁve folds the best accuracies achieved this far by each
model in each epoch. The curve for the model without the
unsupervised learning was also included for comparison. The
best performing conﬁguration proved to be the one with wu =
0.1, which we utilized for the training of the proposed model.
Table III presents a comparison between the proposed
network and three previous studies. It has to be noted that all
models used the same unsupervised weight (wu = 0.1) and
whenever batch normalization was performed, this was based
on batch statistics (instance normalization) since this yielded
the best results. Fig. 7 illustrates a few segmentation results
for each of the models in Table III.

The ﬁrst line of the table refers to our previous work [5],
which has been converted into a fully convolutional network
so it can accept arbitrarily sized images for input. Its low
accuracy is probably due to the small receptive ﬁeld (33×33)
and the extensive pooling. This architecture was sufﬁcient to
describe the local texture of the 32×32 single-class patches
in [5], but could not capture higher level structure that is
present in the whole-lung dataset of this study. The results
of the model in Fig. 7 show its noisy output near the lung
boundaries or between patterns, where context information
could be useful. Segnet [15] and U-net [17] yielded better
results, with the latter being slightly faster and substantially
more accurate. Both models have a very high number of
parameters and large enough receptive ﬁelds to capture any
relevant information. The superior performance of U-net could
be attributed to its skip connections that allow features from
the lower scales to directly contribute to its output. Indeed,
Fig. 7 illustrates the more detailed results of U-net as opposed
to the overly smoothed areas produced by Segnet. Finally, the
proposed network yielded the best results, while being faster
and having far fewer parameters. The output examples in Fig. 7
indicate that the proposed model manages to keep a better
balance between ﬁne details and smooth border among the
different classes. Even thought it is really difﬁcult to visually
assess the performance of the system for the different classes,
there are a few examples in Fig. 7 with wrong classiﬁcations
on which we can comment. Firstly, parts of the broncho-
vascular tree in the third row were recognized as consolidation
because of their similar densities, while accentuated terminal
bronchial parts, that might be physiological as well, caused
the erroneous classiﬁcation of healthy areas into reticulation,
in the ﬁrst row. Some mistakes however are also attributed in
the limited number of annotated classes. For example in row
6, there are emphysematic areas (dark area in the center of the
lung) that have been annotated as healthy due to their similar
density. Figure 6 shows the confusion matrix of the proposed
model. As expected, many of the misclassiﬁcations occur
between reticulation and honeycombing due to their similar
textural appearance. Moreover, healthy tissue is often confused
with reticulation probably because of the 2D sections of the
bronchovascular tree that could resemble reticular patterns.

V. CONCLUSIONS

In this study, we proposed and evaluated a deep CNN
for the semantic segmentation of pathological lung tissue on
HRCT slices. The CNN is designed under a fully convo-
lutional scheme and thus can handle variable input sizes,
while it was trained in an end-to-end and semi-supervised
fashion. The main characteristic of the proposed network is
the use of dilated convolutions along with an instance variance
normalization scheme, and multi-scale feature fusion. The
training and testing of the network was performed using a
cross validation scheme on a dataset of 172 cases, whereas
the split of the dataset into folds was performed per case. The
proposed network surpassed the highest performance in pre-
vious studies, and is much more efﬁcient in terms of memory
and computation. Future work includes the modiﬁcation of

7

Fig. 6: Confusion matrix of the proposed model as calculated
over the cross validation scheme. The numbers represent
percentages of pixels across all validation images.

the model to consider the 3D nature of lung patterns, and
to account for the bronchovascular tree. The former could
be achieved by a direct extension of the architecture to 3D,
similarly to 3D U-Net [44] and V-Net [45] or by employing
a multi-planar view aggregation scheme, also referred to as
2.5D, [46]. Alternatively, a 3D post processing scheme could
be used to reﬁne the 2D segmentation output using conditional
random ﬁelds or deformation models [47], [48], [49]. Finally,
the result of a bronchovascular segmentation method could be
utilized by the network to reduce false alarms.

REFERENCES

[1] B. SOCIETY, “The diagnosis, assessment and treatment of diffuse
parenchymal lung disease in adults,” Thorax, vol. 54, no. Suppl 1, p. S1,
1999.

[2] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp.
211–252, 2015.

[3] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, Nov 1998.

[4] H. Greenspan, B. van Ginneken, and R. M. Summers, “Guest editorial
deep learning in medical imaging: Overview and future promise of
an exciting new technique,” IEEE Transactions on Medical Imaging,
vol. 35, no. 5, pp. 1153–1159, 2016.

[5] M. Anthimopoulos, S. Christodoulidis, L. Ebner, A. Christe, and
S. Mougiakakou, “Lung pattern classiﬁcation for interstitial lung dis-
eases using a deep convolutional neural network,” IEEE Transactions
on Medical Imaging, vol. 35, no. 5, pp. 1207–1216, May 2016.

[6] S. Christodoulidis, M. Anthimopoulos, L. Ebner, A. Christe, and
S. Mougiakakou, “Multisource transfer learning with convolutional
neural networks for lung pattern analysis,” IEEE Journal of Biomedical
and Health Informatics, vol. 21, no. 1, pp. 76–84, Jan 2017.

[7] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.
[8] N. Srivastava, G. Hinton, A. Krizhevsky,

and
R. Salakhutdinov, “Dropout: A simple way to prevent neural
networks from overﬁtting,” J. Mach. Learn. Res., vol. 15, no. 1, pp.
1929–1958, Jan. 2014.

I. Sutskever,

[9] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.

8

Fig. 7: Output examples for the models of Table III. From left to right: Ground Truth, ILD-CNN, Segnet, U-net, Proposed. Each
example has a different pattern annotated. From top to bottom: Healthy (Blue), Ground Glass Opacity (Purple), Micronodules
(Green), Consolidation (Yellow), Reticulation (Orange) and Honeycombing (Red).

9

[32] Q. Wang, Y. Zheng, G. Yang, W. Jin, X. Chen, and Y. Yin, “Multi-
scale rotation-invariant convolutional neural networks for lung texture
classiﬁcation,” IEEE Journal of Biomedical and Health Informatics,
vol. PP, no. 99, pp. 1–1, 2017.

[33] H. C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao,
D. Mollura, and R. M. Summers, “Deep convolutional neural networks
for computer-aided detection: Cnn architectures, dataset characteristics
and transfer learning,” IEEE Transactions on Medical Imaging, vol. 35,
no. 5, pp. 1285–1298, May 2016.

[34] M. Gao, Z. Xu, L. Lu, A. P. Harrison, R. M. Summers, and D. J. Mollura,
“Holistic interstitial lung disease detection using deep convolutional
neural networks: Multi-label learning and unordered pooling,” arXiv
preprint arXiv:1701.05616, 2017.

[35] A. Depeursinge, A. Vargas, A. Platon, A. Geissbuhler, P.-A. Poletti, and
H. M¨uller, “Building a reference multimedia database for interstitial lung
diseases,” Computerized medical imaging and graphics, vol. 36, no. 3,
pp. 227–238, 2012.

[36] P. Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, and
G. W. Cottrell, “Understanding convolution for semantic segmentation,”
CoRR, vol. abs/1702.08502, 2017.

[37] F. Yu, V. Koltun, and T. A. Funkhouser, “Dilated residual networks,”

CoRR, vol. abs/1705.09914, 2017.

[38] D. Ulyanov, A. Vedaldi, and V. S. Lempitsky, “Improved texture
networks: Maximizing quality and diversity in feed-forward stylization
and texture synthesis,” CoRR, vol. abs/1701.02096, 2017.

[39] P. Isola, J. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
with conditional adversarial networks,” CoRR, vol. abs/1611.07004,
2016.

[40] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

CoRR, vol. abs/1412.6980, 2014.

[41] Y. Grandvalet and Y. Bengio, “Semi-supervised learning by entropy
minimization,” in Proceedings of the 17th International Conference
on Neural Information Processing Systems, ser. NIPS’04. Cambridge,
MA, USA: MIT Press, 2004, pp. 529–536.

[42] D.-H. Lee, “Pseudo-label: The simple and efﬁcient semi-supervised
learning method for deep neural networks,” in Workshop on Challenges
in Representation Learning, ICML, vol. 3, 2013, p. 2.

[44]

[43] Theano Development Team, “Theano: A Python framework for
fast computation of mathematical expressions,” arXiv e-prints, vol.
abs/1605.02688, May 2016.
¨O. C¸ ic¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,
“3d u-net: learning dense volumetric segmentation from sparse anno-
tation,” in International Conference on Medical Image Computing and
Computer-Assisted Intervention. Springer, 2016, pp. 424–432.
[45] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional
neural networks for volumetric medical image segmentation,” in 3D
Vision (3DV), 2016 Fourth International Conference on.
IEEE, 2016,
pp. 565–571.

[46] H. R. Roth, L. Lu, J. Liu, J. Yao, A. Seff, K. Cherry, L. Kim, and R. M.
Summers, “Improving computer-aided detection using convolutional
neural networks and random view aggregation,” IEEE transactions on
medical imaging, vol. 35, no. 5, pp. 1170–1181, 2016.

[47] F. Liu, Z. Zhou, H. Jang, A. Samsonov, G. Zhao, and R. Kijowski,
“Deep convolutional neural network and 3d deformable approach for
tissue segmentation in musculoskeletal magnetic resonance imaging,”
Magnetic resonance in medicine, vol. 79, no. 4, pp. 2379–2391, 2018.
[48] P. F. Christ, M. E. A. Elshaer, F. Ettlinger, S. Tatavarty, M. Bickel,
P. Bilic, M. Rempﬂer, M. Armbruster, F. Hofmann, M. DAnastasi et al.,
“Automatic liver and lesion segmentation in ct using cascaded fully
convolutional neural networks and 3d conditional random ﬁelds,” in
International Conference on Medical Image Computing and Computer-
Assisted Intervention. Springer, 2016, pp. 415–423.

[49] K. Kamnitsas, C. Ledig, V. F. Newcombe, J. P. Simpson, A. D. Kane,
D. K. Menon, D. Rueckert, and B. Glocker, “Efﬁcient multi-scale 3d
cnn with fully connected crf for accurate brain lesion segmentation,”
Medical image analysis, vol. 36, pp. 61–78, 2017.

[10] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2015, pp. 1–9.

[11] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2016, pp. 770–778.

[12] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” CoRR, vol.
abs/1502.03167, 2015.

[13] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-
tional networks,” in European conference on computer vision. Springer,
2014, pp. 818–833.

[14] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2015, pp. 3431–3440.
[15] V. Badrinarayanan, A. Handa, and R. Cipolla, “Segnet: A deep con-
volutional encoder-decoder architecture for robust semantic pixel-wise
labelling,” arXiv preprint arXiv:1505.07293, 2015.

[16] H. Noh, S. Hong, and B. Han, “Learning deconvolution network
for semantic segmentation,” in Proceedings of the IEEE International
Conference on Computer Vision, 2015, pp. 1520–1528.

[17] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” CoRR, vol. abs/1505.04597, 2015.
[18] M. Holschneider, R. Kronland-Martinet, J. Morlet, and P. Tchamitchian,
“A real-time algorithm for signal analysis with the help of the wavelet
transform,” in Wavelets. Springer, 1990, pp. 286–297.

[19] F. Yu and V. Koltun, “Multi-scale context aggregation by dilated

convolutions,” CoRR, vol. abs/1511.07122, 2015.

[20] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A deep
neural network architecture for real-time semantic segmentation,” CoRR,
vol. abs/1606.02147, 2016.

[21] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille, “Deeplab: Semantic image segmentation with deep convolutional
nets, atrous convolution, and fully connected crfs,” arXiv preprint
arXiv:1606.00915, 2016.

[22] R. Uppaluri, E. A. Hoffman, M. Sonka, P. G. Hartley, G. W. Hunning-
hake, and G. McLennan, “Computer recognition of regional lung disease
patterns,” American journal of respiratory and critical care medicine,
vol. 160, no. 2, pp. 648–654, 1999.

[23] I. C. Sluimer, P. F. van Waes, M. A. Viergever, and B. van Ginneken,
“Computer-aided diagnosis in high resolution ct of the lungs,” Medical
physics, vol. 30, no. 12, pp. 3081–3090, 2003.

[24] L. Sorensen, S. B. Shaker, and M. De Bruijne, “Quantitative analysis of
pulmonary emphysema using local binary patterns,” IEEE transactions
on medical imaging, vol. 29, no. 2, pp. 559–569, 2010.

[25] M. Anthimopoulos, S. Christodoulidis, A. Christe, and S. Mougiakakou,
“Classiﬁcation of interstitial lung disease patterns using local dct features
and random forest,” in 2014 36th Annual International Conference of
the IEEE Engineering in Medicine and Biology Society, Aug 2014, pp.
6040–6043.

[26] M. Gangeh, L. Sørensen, S. Shaker, M. Kamel, M. De Bruijne, and
M. Loog, “A texton-based approach for the classiﬁcation of lung
parenchyma in ct images,” Medical Image Computing and Computer-
Assisted Intervention–MICCAI 2010, pp. 595–602, 2010.

[27] A. Foncubierta-Rodr´ıguez, A. Depeursinge, and H. M¨uller, “Using
multiscale visual words for lung texture classiﬁcation and retrieval,” in
MICCAI International Workshop on Medical Content-Based Retrieval
for Clinical Decision Support. Springer, 2011, pp. 69–79.

[28] W. Zhao, R. Xu, Y. Hirano, R. Tachibana, and S. Kido, “Classiﬁcation of
diffuse lung diseases patterns by a sparse representation based method on
hrct images,” in Engineering in Medicine and Biology Society (EMBC),
2013 35th Annual International Conference of the IEEE.
IEEE, 2013,
pp. 5457–5460.

[29] K. T. Vo and A. Sowmya, “Multiscale sparse representation of high-
resolution computed tomography (hrct) lung images for diffuse lung
disease classiﬁcation,” in Image Processing (ICIP), 2011 18th IEEE
International Conference on.

IEEE, 2011, pp. 441–444.

[30] G. van Tulder and M. de Bruijne, “Learning features for tissue classi-
ﬁcation with the classiﬁcation restricted boltzmann machine,” in Inter-
national MICCAI Workshop on Medical Computer Vision.
Springer,
2014, pp. 47–58.

[31] Q. Li, W. Cai, X. Wang, Y. Zhou, D. D. Feng, and M. Chen, “Med-
ical image classiﬁcation with convolutional neural network,” in 2014
13th International Conference on Control Automation Robotics Vision
(ICARCV), Dec 2014, pp. 844–848.

