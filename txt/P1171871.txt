6
1
0
2
 
v
o
N
 
4
2
 
 
]
E
N
.
s
c
[
 
 
1
v
7
0
3
8
0
.
1
1
6
1
:
v
i
X
r
a

Under review as a conference paper at ICLR 2017

LEARNING PYTHON CODE SUGGESTION WITH A
SPARSE POINTER NETWORK

Avishkar Bhoopchand, Tim Rockt¨aschel, Earl Barr & Sebastian Riedel
Department of Computer Science
University College London
avishkar.bhoopchand.15@ucl.ac.uk,
{t.rocktaschel,e.barr,s.riedel}@cs.ucl.ac.uk

ABSTRACT

To enhance developer productivity, all modern integrated development environ-
ments (IDEs) include code suggestion functionality that proposes likely next tokens
at the cursor. While current IDEs work well for statically-typed languages, their re-
liance on type annotations means that they do not provide the same level of support
for dynamic programming languages as for statically-typed languages. Moreover,
suggestion engines in modern IDEs do not propose expressions or multi-statement
idiomatic code. Recent work has shown that language models can improve code
suggestion systems by learning from software repositories. This paper introduces a
neural language model with a sparse pointer network aimed at capturing very long-
range dependencies. We release a large-scale code suggestion corpus of 41M lines
of Python code crawled from GitHub. On this corpus, we found standard neural
language models to perform well at suggesting local phenomena, but struggle to
refer to identiﬁers that are introduced many tokens in the past. By augmenting a
neural language model with a pointer network specialized in referring to predeﬁned
classes of identiﬁers, we obtain a much lower perplexity and a 5 percentage points
increase in accuracy for code suggestion compared to an LSTM baseline. In fact,
this increase in code suggestion accuracy is due to a 13 times more accurate pre-
diction of identiﬁers. Furthermore, a qualitative analysis shows this model indeed
captures interesting long-range dependencies, like referring to a class member
deﬁned over 60 tokens in the past.

1

INTRODUCTION

Integrated development environments (IDEs) are essential tools for programmers. Especially when a
developer is new to a codebase, one of their most useful features is code suggestion: given a piece of
code as context, suggest a likely sequence of next tokens. Typically, the IDE suggests an identiﬁer or
a function call, including API calls. While extensive support exists for statically-typed languages
such as Java, code suggestion for dynamic languages like Python is harder and less well supported
because of the lack of type annotations. Moreover, suggestion engines in modern IDEs do not propose
expressions or multi-statement idiomatic code.

Recently, methods from statistical natural language processing (NLP) have been used to train code
suggestion systems from code usage in large code repositories (Hindle et al., 2012; Allamanis &
Sutton, 2013; Tu et al., 2014). To this end, usually an n-gram language model is trained to score
possible completions. Neural language models for code suggestion (White et al., 2015; Das &
Shah, 2015) have extended this line of work to capture more long-range dependencies. Yet, these
standard neural language models are limited by the so-called hidden state bottleneck, i.e., all context
information has to be stored in a ﬁxed-dimensional internal vector representation. This limitation
restricts such models to local phenomena and does not capture very long-range semantic relationships
like suggesting calling a function that has been deﬁned many tokens before.

To address these issues, we create a large corpus of 41M lines of Python code by using a heuristic for
crawling high-quality code repositories from GitHub. We investigate, for the ﬁrst time, the use of
attention (Bahdanau et al., 2014) for code suggestion and ﬁnd that, despite a substantial improvement

1

Under review as a conference paper at ICLR 2017

in accuracy, it still makes avoidable mistakes. Hence, we introduce a model that leverages long-range
Python dependencies by selectively attending over the introduction of identiﬁers as determined
by examining the Abstract Syntax Tree. The model is a form of pointer network (Vinyals et al.,
2015a), and learns to dynamically choose between syntax-aware pointing for modeling long-range
dependencies and free form generation to deal with local phenomena, based on the current context.

Our contributions are threefold: (i) We release a code suggestion corpus of 41M lines of Python code
crawled from GitHub, (ii) We introduce a sparse attention mechanism that captures very long-range
dependencies for code suggestion of this dynamic programming language efﬁciently, and (iii) We
provide a qualitative analysis demonstrating that this model is indeed able to learn such long-range
dependencies.

2 METHODS

We ﬁrst revisit neural language models, before brieﬂy describing how to extend such a language
model with an attention mechanism. Then we introduce a sparse attention mechanism for a pointer
network that can exploit the Python abstract syntax tree of the current context for code suggestion.

2.1 NEURAL LANGUAGE MODEL

Code suggestion can be approached by a language model that measures the probability of observing
a sequence of tokens in a Python program. For example, for the sequence S = a1, . . . , aN , the joint
probability of S factorizes according to

Pθ(S) = Pθ(a1) ·

Pθ(at | at−1, . . . , a1)

N
(cid:89)

t=2

(1)

(2)

(3)

where the parameters θ are estimated from a training corpus. Given a sequence of Python tokens, we
seek to predict the next M tokens at+1, . . . , at+M that maximize Equation 1

arg max
at+1, ..., at+M

Pθ(a1, . . . , at, at+1, . . . , at+M ).

In this work, we build upon neural language models using Recurrent Neural Networks (RNNs) and
Long Short-Term Memory (LSTM, Hochreiter & Schmidhuber, 1997). This neural language model
estimates the probabilities in Equation 1 using the output vector of an LSTM at time step t (denoted
ht here) according to

Pθ(at = τ | at−1, . . . , a1) =

τ ht + bτ )

exp (vT
τ (cid:48) exp (vT

(cid:80)

τ (cid:48)ht + bτ (cid:48))

where vτ is a parameter vector associated with token τ in the vocabulary.

Neural language models can, in theory, capture long-term dependencies in token sequences through
their internal memory. However, as this internal memory has ﬁxed dimension and can be updated at
every time step, such models often only capture local phenomena. In contrast, we are interested in
very long-range dependencies like referring to a function identiﬁer introduced many tokens in the
past. For example, a function identiﬁer may be introduced at the top of a ﬁle and only used near
the bottom. In the following, we investigate various external memory architectures for neural code
suggestion.

2.2 ATTENTION

A straight-forward approach to capturing long-range dependencies is to use a neural attention mech-
anism (Bahdanau et al., 2014) on the previous K output vectors of the language model. Attention
mechanisms have been successfully applied to sequence-to-sequence tasks such as machine transla-
tion (Bahdanau et al., 2014), question-answering (Hermann et al., 2015), syntactic parsing (Vinyals
et al., 2015b), as well as dual-sequence modeling like recognizing textual entailment (Rockt¨aschel
et al., 2016). The idea is to overcome the hidden-state bottleneck by allowing referral back to previous
output vectors. Recently, these mechanisms were applied to language modelling by Cheng et al.
(2016) and Tran et al. (2016).

2

Under review as a conference paper at ICLR 2017

Formally, an attention mechanism with a ﬁxed memory Mt ∈ Rk×K of K vectors mi ∈ Rk for
i ∈ [1, K], produces an attention distribution αt ∈ RK and context vector ct ∈ Rk at each time
step t according to Equations 4 to 7. Furthermore, W M , W h ∈ Rk×k and w ∈ Rk are trainable
parameters. Finally, note that 1K represents a K-dimensional vector of ones.

Mt = [m1 . . . mK]
Gt = tanh(W M Mt + 1T
αt = softmax(wT Gt)
ct = MtαT
t

K(W hht))

∈ Rk×K
∈ Rk×K
∈ R1×K
∈ Rk

For language modeling, we populate Mt with a ﬁxed window of the previous K LSTM output
vectors. To obtain a distribution over the next token we combine the context vector ct of the attention
mechanism with the output vector ht of the LSTM using a trainable projection matrix W A ∈ Rk×2k.
The resulting ﬁnal output vector nt ∈ Rk encodes the next-word distribution and is projected to the
size of the vocabulary |V |. Subsequently, we apply a softmax to arrive at a probability distribution
yt ∈ R|V | over the next token. This process is presented in Equation 9 where WV ∈ R|V |×k and
bV ∈ R|V | are trainable parameters.

(cid:18)

nt = tanh

W A

(cid:21)(cid:19)

(cid:20)ht
ct

yt = softmax(W V nt + bV )

∈ Rk

∈ R|V |

The problem of the attention mechanism above is that it quickly becomes computationally expensive
for large K. Moreover, attending over many memories can make training hard as a lot of noise is
introduced in early stages of optimization where the LSTM outputs (and thus the memory Mt) are
more or less random. To alleviate these problems we now turn to pointer networks and a simple
heuristic for populating Mt that permits the efﬁcient retrieval of identiﬁers in a large history of
Python code.

2.3 SPARSE POINTER NETWORK

We develop an attention mechanism that provides a ﬁltered view of a large history of Python tokens.
At any given time step, the memory consists of context representations of the previous K identiﬁers
introduced in the history. This allows us to model long-range dependencies found in identiﬁer usage.
For instance, a class identiﬁer may be declared hundreds of lines of code before it is used. Given a
history of Python tokens, we obtain a next-word distribution from a weighed average of the sparse
pointer network for identiﬁer reference and a standard neural language model. The weighting of the
two is determined by a controller.
Formally, at time-step t, the sparse pointer network operates on a memory Mt ∈ Rk×K of only the
K previous identiﬁer representations (e.g. function identiﬁers, class identiﬁers and so on). In addition,
we maintain a vector mt = [id1, . . . , idK] ∈ NK of symbol ids for these identiﬁer representations
(i.e. pointers into the large global vocabulary).

As before, we calculate a context vector ct using the attention mechanism (Equation 7), but on a
memory Mt only containing representations of identiﬁers that were declared in the history. Next, we
obtain a pseudo-sparse distribution over the global vocabulary from

st[i] =

(cid:26)αt[j]
−C

if mt[j] = i
otherwise

it = softmax(st)

∈ R|V |

where −C is a large negative constant (e.g. −1000). In addition, we calculate a next-word distribution
from a standard neural language model

yt = softmax(W V ht + bV )

∈ R|V |

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

3

Under review as a conference paper at ICLR 2017

Figure 1: Sparse pointer network for code suggestion on a Python code snippet, showing the next-
word distributions of the language model and identiﬁer attention and their weighted combination
through λ

and we use a controller to calculate a distribution λt ∈ R2 over the language model and pointer
network for the ﬁnal weighted next-word distribution y∗
(cid:34) ht
xt
ct

∈ R3k

t via

t =

(13)

hλ

(cid:35)

λt = softmax(W λhλ
y∗
t = [yt it] λt

t + bλ)

∈ R2
∈ R|V |

(14)

(15)
Here, xt is the representation of the input token, and W λ ∈ R2×3k and bλ ∈ R2 a trainable
weight matrix and bias respectively. This controller is conditioned on the input, output and context
representations. This means for deciding whether to refer to an identiﬁer or generate from the global
vocabulary, the controller has access to information from the encoded next-word distribution ht of
the standard neural language model, as well as the attention-weighted identiﬁer representations ct
from the current history.

Figure 1 overviews this process. In it, the identiﬁer base_path appears twice, once as an argument
to a function and once as a member of a class (denoted by *). Each appearance has a different id
in the vocabulary and obtains a different probability from the model. In the example, the model
correctly chooses to refer to the member of the class instead of the out-of-scope function argument,
although, from a user point-of-view, the suggestion would be the same in both cases.

3 LARGE-SCALE PYTHON CORPUS

Previous work on code suggestion either focused on statically-typed languages (particularly Java)
or trained on very small corpora. Thus, we decided to collect a new large-scale corpus of the
dynamic programming language Python. According to the programming language popularity website
Pypl (Carbonnelle, 2016), Python is the second most popular language after Java. It is also the
3rd most common language in terms of number of repositories on the open-source code repository
GitHub, after JavaScript and Java (Zapponi, 2016).

We collected a corpus of 41M lines of Python code from GitHub projects. Ideally, we would like this
corpus to only contain high-quality Python code, as our language model learns to suggest code from
how users write code. However, it is difﬁcult to automatically assess what constitutes high-quality
code. Thus, we resort to the heuristic that popular code projects tend to be of good quality, There are

4

Under review as a conference paper at ICLR 2017

Table 1: Python corpus statistics.

Dataset

#Projects

#Files

#Lines

#Tokens Vocabulary Size

Train
Dev
Test

Total

489
179
281

949

118 298
26 466
43 062

26 868 583
5 804 826
8 398 100

88 935 698
18 147 341
30 178 356

187 826

41 071 509

137 261 395

2 323 819

Figure 2: Example of the Python code normalization. Original ﬁle on the left and normalized version
on the right.

two metrics on GitHub that we can use for this purpose, namely stars (similar to bookmarks) and
forks (copies of a repository that allow users to freely experiment with changes without affecting the
original repository). Similar to Allamanis & Sutton (2013) and Allamanis et al. (2014), we select
Python projects with more than 100 stars, sort by the number of forks descending, and take the top
1000 projects. We then removed projects that did not compile with Python3, leaving us with 949
projects. We split the corpus on the project level into train, dev, and test. Table 1 presents the corpus
statistics.

3.1 NORMALIZATION OF IDENTIFIERS

Unsurprisingly, the long tail of words in the vocabulary consists of rare identiﬁers. To improve
generalization, we normalize identiﬁers before feeding the resulting token stream to our models.
That is, we replace every identiﬁer name with an anonymous identiﬁer indicating the identiﬁer group
(class, variable, argument, attribute or function) concatenated with a random number that makes
the identiﬁer unique in its scope. Note that we only replace novel identiﬁers deﬁned within a ﬁle.
Identiﬁer references to external APIs and libraries are left untouched. Consistent with previous corpus
creation for code suggestion (e.g. Khanh Dam et al., 2016; White et al., 2015), we replace numerical
constant tokens with $NUM$, remove comments, reformat the code, and replace tokens appearing
less than ﬁve times with an $OOV$ (out of vocabulary) token.

4 EXPERIMENTS

Although previous work by White et al. (2015) already established that a simple neural language
model outperforms an n-gram model for code suggestion, we include a number of n-gram baselines
to conﬁrm this observation. Speciﬁcally, we use n-gram models for n ∈ {3, 4, 5, 6} with Modiﬁed
Kneser-Ney smoothing (Kneser & Ney, 1995) from the Kyoto Language Modelling Toolkit (Neubig,
2012).

We train the sparse pointer network using mini-batch SGD with a batch size of 30 and truncated
backpropagation through time (Werbos, 1990) with a history of 20 identiﬁer representations. We use

5

Under review as a conference paper at ICLR 2017

Table 2: Perplexity (PP), Accuracy (Acc) and Accuarcy among top 5 predictions (Acc@5).

Model

3-gram
4-gram
5-gram
6-gram

LSTM
LSTM w/ Attention 20
LSTM w/ Attention 50

Sparse Pointer Network

Train PP Dev PP Test PP

Acc [%]

Acc@5 [%]

12.90
7.60
4.52
3.37

9.29
7.30
7.09

6.41

24.19
21.07
19.33
18.73

13.08
11.07
9.83

9.40

All

IDs Other

All

IDs Other

26.90
23.85
21.22
20.17

14.01
11.74
10.05

13.19
13.68
13.90
14.51

57.91
61.30
63.21

9.18

62.97

–
–
–
–

2.1
21.4
30.2

27.3

–
–
–
–

62.8
64.8
65.3

64.9

50.81
51.26
51.49
51.76

76.30
79.32
81.69

82.62

–
–
–
–

4.5
29.9
41.3

43.6

–
–
–
–

82.6
83.7
84.1

84.5

an initial learning rate of 0.7 and decay it by 0.9 after every epoch. As additional baselines, we test
a neural language model with LSTM units with and without attention. For the attention language
models, we experiment with a ﬁxed-window attention memory of the previous 20 and 50 tokens
respectively, and a batch size of 75.

All neural language models were developed in TensorFlow (Abadi et al., 2016) and trained using
cross-entropy loss. While processing a Python source code ﬁle, the last recurrent state of the RNN
is fed as the initial state of the subsequent sequence of the same ﬁle and reset between ﬁles. All
models use an input and hidden size of 200, an LSTM forget gate bias of 1 (Jozefowicz et al., 2015),
gradient norm clipping of 5 (Pascanu et al., 2013), and randomly initialized parameters in the interval
(−0.05, 0.05). As regularizer, we use a dropout of 0.1 on the input representations. Furthermore, we
use a sampled softmax (Jean et al., 2015) with a log-uniform sampling distribution and a sample size
of 1000.

5 RESULTS

We evaluate all models using perplexity (PP), as well as accuracy of the top prediction (Acc) and the
top ﬁve predictions (Acc@5). The results are summarized in Table 2.

We can conﬁrm that for code suggestion neural models outperform n-gram language models by a
large margin. Furthermore, adding attention improves the results substantially (2.3 lower perplexity
and 3.4 percentage points increased accuracy). Interestingly, this increase can be attributed to a
superior prediction of identiﬁers, which increased from an accuracy of 2.1% to 21.4%. An LSTM
with an attention window of 50 gives us the best accuracy for the top prediction. We achieve further
improvements for perplexity and accuracy of the top ﬁve predictions by using a sparse pointer network
that uses a smaller memory of the past 20 identiﬁer representations.

5.1 QUALITATIVE ANALYSIS

Figures 3a-d show a code suggestion example involving an identiﬁer usage. While the LSTM baseline
is uncertain about the next token, we get a sensible prediction by using attention or the sparse pointer
network. The sparse pointer network provides more reasonable alternative suggestions beyond the
correct top suggestion.

Figures 3e-h show the use-case referring to a class attribute declared 67 tokens in the past. Only
the Sparse Pointer Network makes a good suggestion. Furthermore, the attention weights in 3i
demonstrate that this model distinguished attributes from other groups of identiﬁers. We give a full
example of a token-by-token suggestion of the Sparse Pointer Network in Figure 4 in the Appendix.

6 RELATED WORK

Previous code suggestion work using methods from statistical NLP has mostly focused on n-gram
models. Much of this work is inspired by Hindle et al. (2012) who argued that real programs fall

6

Under review as a conference paper at ICLR 2017

(a) Code snippet for referencing
variable.

(b) LSTM Model.

(c) LSTM w/ Attention
50.

(d) Sparse Pointer Net-
work.

(e) Code snippet for referencing class
member.

(f) LSTM Model.

(g) LSTM w/ Attention
50.

(h) Sparse Pointer Net-
work.

(i) Sparse Pointer Network attention over memory of identiﬁer representations.

Figure 3: Code suggestion example involving a reference to a variable (a-d), a long-range dependency
(e-h), and the attention weights of the Sparse Pointer Network (i).

into a much smaller space than the ﬂexibility of programming languages allows. They were able
to capture the repetitiveness and predictable statistical properties of real programs using language
models. Subsequently, Tu et al. (2014) improved upon Hindle et al.’s work by adding a cache
mechanism that allowed them to exploit locality stemming from the specialisation and decoupling of
program modules. Tu et al.’s idea of adding a cache mechanism to the language model is speciﬁcally
designed to exploit the properties of source code, and thus follows the same aim as the sparse attention
mechanism introduced in this paper.

While the majority of preceding work trained on small corpora, Allamanis & Sutton (2013) created a
corpus of 352M lines of Java code which they analysed with n-gram language models. The size of
the corpus allowed them to train a single language model that was effective across multiple different
project domains. White et al. (2015) later demonstrated that neural language models outperform
n-gram models for code suggestion. They compared various n-gram models (up to nine grams),
including Tu et al.’s cache model, with a basic RNN neural language model. Khanh Dam et al.
(2016) compared White et al.’s basic RNN with LSTMs and found that the latter are better at code
suggestion due to their improved ability to learn long-range dependencies found in source code. Our
paper extends this line of work by introducing a sparse attention model that captures even longer
dependencies.

The combination of lagged attention mechanisms with language modelling is inspired by Cheng et al.
(2016) who equipped LSTM cells with a ﬁxed-length memory tape rather than a single memory cell.
They achieved promising results on the standard Penn Treebank benchmark corpus (Marcus et al.,
1993). Similarly, Tran et al. added a memory block to LSTMs for language modelling of English,
German and Italian and outperformed both n-gram and neural language models. Their memory
encompasses representations of all possible words in the vocabulary rather than providing a sparse
view as we do.

An alternative to our purely lexical approach to code suggestion involves the use of probabilistic
context-free grammars (PCFGs) which exploit the formal grammar speciﬁcations and well-deﬁned,
deterministic parsers available for source code. These were used by Allamanis & Sutton (2014)
to extract idiomatic patterns from source code. A weakness of PCFGs is their inability to model
context-dependent rules of programming languages such as that variables need to be declared before

7

Under review as a conference paper at ICLR 2017

being used. Maddison & Tarlow (2014) added context-aware variables to their PCFG model in order
to capture such rules.

Ling et al. (2016) recently used a pointer network to generate code from natural language descriptions.
Our use of a controller for deciding whether to generate from a language model or copy an identiﬁer
using a sparse pointer network is inspired by their latent code predictor. However, their inputs (textual
descriptions) are short whereas code suggestion requires capturing very long-range dependencies that
we addressed by a ﬁltered view on the memory of previous identiﬁer representations.

7 CONCLUSIONS AND FUTURE WORK

In this paper, we investigated neural language models for code suggestion of the dynamically-typed
programming language Python. We released a corpus of 41M lines of Python crawled from GitHub
and compared n-gram, standard neural language models, and attention. By using attention, we
observed an order of magnitude more accurate prediction of identiﬁers. Furthermore, we proposed
a sparse pointer network that can efﬁciently capture long-range dependencies by only operating
on a ﬁltered view of a memory of previous identiﬁer representations. This model achieves the
lowest perplexity and best accuracy among the top ﬁve predictions. The Python corpus and code for
replicating our experiment is released at https://github.com/uclmr/pycodesuggest.

The presented methods were only tested for code suggestion within the same Python ﬁle. We are
interested in scaling the approach to the level of entire code projects and collections thereof, as well
as integrating a trained code suggestion model into an existing IDE. Furthermore, we plan to work on
code completion, i.e., models that provide a likely continuation of a partial token, using character
language models (Graves, 2013).

This work was supported by Microsoft Research through its PhD Scholarship Programme, an Allen
Distinguished Investigator Award, and a Marie Curie Career Integration Award.

ACKNOWLEDGMENTS

REFERENCES

Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat
Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan,
Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zhang. Tensorﬂow: A system for large-scale
machine learning. CoRR, abs/1605.08695, 2016. URL http://arxiv.org/abs/1605.
08695.

Miltiadis Allamanis and Charles Sutton. Mining idioms from source code.

In Proceedings of
the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 472–483, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635901. URL http://doi.acm.org/10.1145/2635868.2635901.

Miltiadis Allamanis and Charles A. Sutton. Mining source code repositories at massive scale
using language modeling. In Thomas Zimmermann, Massimiliano Di Penta, and Sunghun Kim
(eds.), MSR, pp. 207–216. IEEE Computer Society, 2013.
ISBN 978-1-4673-2936-1. URL
http://dblp.uni-trier.de/db/conf/msr/msr2013.html#AllamanisS13a.

Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Learning natural coding
conventions. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations
of Software Engineering, FSE 2014, pp. 281–293, New York, NY, USA, 2014. ACM. ISBN 978-
1-4503-3056-5. doi: 10.1145/2635868.2635883. URL http://doi.acm.org/10.1145/
2635868.2635883.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/abs/
1409.0473.

8

Under review as a conference paper at ICLR 2017

Pierre Carbonnelle. Pypl popularity of programming language. http://pypl.github.io/
PYPL.html, 2016. URL http://pypl.github.io/PYPL.html. [Online; accessed 30-
August-2016].

Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing, pp. 551–561. Association for Computational Linguistics, 2016. URL http://
aclweb.org/anthology/D16-1053.

Subhasis Das and Chinmayee Shah. Contextual code completion using machine learning. 2015.

Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.

URL http://arxiv.org/abs/1308.0850.

Karl Moritz Hermann, Tom´as Kocisk´y, Edward Grefenstette, Lasse Espeholt, Will Kay,
Teaching machines to read and compre-
Mustafa Suleyman,
In Advances in Neural Information Processing Systems 28: Annual Confer-
hend.
ence on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal,
URL http://papers.nips.cc/paper/
Quebec, Canada, pp. 1693–1701, 2015.
5945-teaching-machines-to-read-and-comprehend.

and Phil Blunsom.

Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the naturalness
of software. In Proceedings of the 34th International Conference on Software Engineering, ICSE
’12, pp. 837–847, Piscataway, NJ, USA, 2012. IEEE Press. ISBN 978-1-4673-1067-3. URL
http://dl.acm.org/citation.cfm?id=2337223.2337322.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target
vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), pp. 1–10, Beijing, China, July 2015. Association
for Computational Linguistics. URL http://www.aclweb.org/anthology/P15-1001.

Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent
network architectures. In David Blei and Francis Bach (eds.), Proceedings of the 32nd Inter-
national Conference on Machine Learning (ICML-15), pp. 2342–2350. JMLR Workshop and
Conference Proceedings, 2015. URL http://jmlr.org/proceedings/papers/v37/
jozefowicz15.pdf.

H. Khanh Dam, T. Tran, and T. Pham. A deep language model for software code. ArXiv e-prints,

August 2016.

R. Kneser and H. Ney. Improved backing-off for m-gram language modeling. In Acoustics, Speech,
and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pp.
181–184 vol.1, May 1995. doi: 10.1109/ICASSP.1995.479394.

Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, Andrew Senior, Fumin
Wang, and Phil Blunsom. Latent predictor networks for code generation. arXiv preprint
arXiv:1603.06744, 2016.

Chris J Maddison and Daniel Tarlow. Structured generative models of natural source code. In

International Conference on Machine Learning, 2014.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313–330, 1993.

Graham Neubig. Kylm - the kyoto language modeling toolkit. http://www.phontron.com/
[Online; accessed 23-July-

kylm/, 2012. URL http://www.phontron.com/kylm/.
2016].

9

Under review as a conference paper at ICLR 2017

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
networks. In Proceedings of the 30th International Conference on Machine Learning, ICML
2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1310–1318, 2013. URL http://jmlr.org/
proceedings/papers/v28/pascanu13.html.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom.

Reasoning about entailment with neural attention. In ICLR, 2016.

Ke M. Tran, Arianna Bisazza, and Christof Monz. Recurrent memory networks for language
modeling. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, San Diego California,
USA, June 12-17, 2016, pp. 321–331, 2016. URL http://aclweb.org/anthology/N/
N16/N16-1036.pdf.

Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. On the localness of software. In Proceedings
of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 269–280, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635875. URL http://doi.acm.org/10.1145/2635868.2635875.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural

Information Processing Systems, pp. 2692–2700, 2015a.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton.
Grammar as a foreign language. In Advances in Neural Information Processing Systems 28:
Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon-
treal, Quebec, Canada, pp. 2773–2781, 2015b. URL http://papers.nips.cc/paper/
5635-grammar-as-a-foreign-language.

Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the

IEEE, 78(10):1550–1560, 1990.

Martin White, Christopher Vendome, Mario Linares-V´asquez, and Denys Poshyvanyk. Toward
deep learning software repositories. In Proceedings of the 12th Working Conference on Mining
Software Repositories, MSR ’15, pp. 334–345, Piscataway, NJ, USA, 2015. IEEE Press. URL
http://dl.acm.org/citation.cfm?id=2820518.2820559.

Carlo Zapponi. Githut - programming languages and github. http://githut.info/, 2016.

URL http://githut.info/. [Online; accessed 19-August-2016].

10

Under review as a conference paper at ICLR 2017

APPENDIX

Figure 4: Full example of code suggestion with a Sparse Pointer Network. Boldface tokens on the left
show the ﬁrst declaration of an identiﬁer. The middle part visualizes the memory of representations of
these identiﬁers. The right part visualizes the output λ of the controller, which is used for interpolating
between the language model (LM) and the attention of the pointer network (Att).

11

6
1
0
2
 
v
o
N
 
4
2
 
 
]
E
N
.
s
c
[
 
 
1
v
7
0
3
8
0
.
1
1
6
1
:
v
i
X
r
a

Under review as a conference paper at ICLR 2017

LEARNING PYTHON CODE SUGGESTION WITH A
SPARSE POINTER NETWORK

Avishkar Bhoopchand, Tim Rockt¨aschel, Earl Barr & Sebastian Riedel
Department of Computer Science
University College London
avishkar.bhoopchand.15@ucl.ac.uk,
{t.rocktaschel,e.barr,s.riedel}@cs.ucl.ac.uk

ABSTRACT

To enhance developer productivity, all modern integrated development environ-
ments (IDEs) include code suggestion functionality that proposes likely next tokens
at the cursor. While current IDEs work well for statically-typed languages, their re-
liance on type annotations means that they do not provide the same level of support
for dynamic programming languages as for statically-typed languages. Moreover,
suggestion engines in modern IDEs do not propose expressions or multi-statement
idiomatic code. Recent work has shown that language models can improve code
suggestion systems by learning from software repositories. This paper introduces a
neural language model with a sparse pointer network aimed at capturing very long-
range dependencies. We release a large-scale code suggestion corpus of 41M lines
of Python code crawled from GitHub. On this corpus, we found standard neural
language models to perform well at suggesting local phenomena, but struggle to
refer to identiﬁers that are introduced many tokens in the past. By augmenting a
neural language model with a pointer network specialized in referring to predeﬁned
classes of identiﬁers, we obtain a much lower perplexity and a 5 percentage points
increase in accuracy for code suggestion compared to an LSTM baseline. In fact,
this increase in code suggestion accuracy is due to a 13 times more accurate pre-
diction of identiﬁers. Furthermore, a qualitative analysis shows this model indeed
captures interesting long-range dependencies, like referring to a class member
deﬁned over 60 tokens in the past.

1

INTRODUCTION

Integrated development environments (IDEs) are essential tools for programmers. Especially when a
developer is new to a codebase, one of their most useful features is code suggestion: given a piece of
code as context, suggest a likely sequence of next tokens. Typically, the IDE suggests an identiﬁer or
a function call, including API calls. While extensive support exists for statically-typed languages
such as Java, code suggestion for dynamic languages like Python is harder and less well supported
because of the lack of type annotations. Moreover, suggestion engines in modern IDEs do not propose
expressions or multi-statement idiomatic code.

Recently, methods from statistical natural language processing (NLP) have been used to train code
suggestion systems from code usage in large code repositories (Hindle et al., 2012; Allamanis &
Sutton, 2013; Tu et al., 2014). To this end, usually an n-gram language model is trained to score
possible completions. Neural language models for code suggestion (White et al., 2015; Das &
Shah, 2015) have extended this line of work to capture more long-range dependencies. Yet, these
standard neural language models are limited by the so-called hidden state bottleneck, i.e., all context
information has to be stored in a ﬁxed-dimensional internal vector representation. This limitation
restricts such models to local phenomena and does not capture very long-range semantic relationships
like suggesting calling a function that has been deﬁned many tokens before.

To address these issues, we create a large corpus of 41M lines of Python code by using a heuristic for
crawling high-quality code repositories from GitHub. We investigate, for the ﬁrst time, the use of
attention (Bahdanau et al., 2014) for code suggestion and ﬁnd that, despite a substantial improvement

1

Under review as a conference paper at ICLR 2017

in accuracy, it still makes avoidable mistakes. Hence, we introduce a model that leverages long-range
Python dependencies by selectively attending over the introduction of identiﬁers as determined
by examining the Abstract Syntax Tree. The model is a form of pointer network (Vinyals et al.,
2015a), and learns to dynamically choose between syntax-aware pointing for modeling long-range
dependencies and free form generation to deal with local phenomena, based on the current context.

Our contributions are threefold: (i) We release a code suggestion corpus of 41M lines of Python code
crawled from GitHub, (ii) We introduce a sparse attention mechanism that captures very long-range
dependencies for code suggestion of this dynamic programming language efﬁciently, and (iii) We
provide a qualitative analysis demonstrating that this model is indeed able to learn such long-range
dependencies.

2 METHODS

We ﬁrst revisit neural language models, before brieﬂy describing how to extend such a language
model with an attention mechanism. Then we introduce a sparse attention mechanism for a pointer
network that can exploit the Python abstract syntax tree of the current context for code suggestion.

2.1 NEURAL LANGUAGE MODEL

Code suggestion can be approached by a language model that measures the probability of observing
a sequence of tokens in a Python program. For example, for the sequence S = a1, . . . , aN , the joint
probability of S factorizes according to

Pθ(S) = Pθ(a1) ·

Pθ(at | at−1, . . . , a1)

N
(cid:89)

t=2

(1)

(2)

(3)

where the parameters θ are estimated from a training corpus. Given a sequence of Python tokens, we
seek to predict the next M tokens at+1, . . . , at+M that maximize Equation 1

arg max
at+1, ..., at+M

Pθ(a1, . . . , at, at+1, . . . , at+M ).

In this work, we build upon neural language models using Recurrent Neural Networks (RNNs) and
Long Short-Term Memory (LSTM, Hochreiter & Schmidhuber, 1997). This neural language model
estimates the probabilities in Equation 1 using the output vector of an LSTM at time step t (denoted
ht here) according to

Pθ(at = τ | at−1, . . . , a1) =

τ ht + bτ )

exp (vT
τ (cid:48) exp (vT

(cid:80)

τ (cid:48)ht + bτ (cid:48))

where vτ is a parameter vector associated with token τ in the vocabulary.

Neural language models can, in theory, capture long-term dependencies in token sequences through
their internal memory. However, as this internal memory has ﬁxed dimension and can be updated at
every time step, such models often only capture local phenomena. In contrast, we are interested in
very long-range dependencies like referring to a function identiﬁer introduced many tokens in the
past. For example, a function identiﬁer may be introduced at the top of a ﬁle and only used near
the bottom. In the following, we investigate various external memory architectures for neural code
suggestion.

2.2 ATTENTION

A straight-forward approach to capturing long-range dependencies is to use a neural attention mech-
anism (Bahdanau et al., 2014) on the previous K output vectors of the language model. Attention
mechanisms have been successfully applied to sequence-to-sequence tasks such as machine transla-
tion (Bahdanau et al., 2014), question-answering (Hermann et al., 2015), syntactic parsing (Vinyals
et al., 2015b), as well as dual-sequence modeling like recognizing textual entailment (Rockt¨aschel
et al., 2016). The idea is to overcome the hidden-state bottleneck by allowing referral back to previous
output vectors. Recently, these mechanisms were applied to language modelling by Cheng et al.
(2016) and Tran et al. (2016).

2

Under review as a conference paper at ICLR 2017

Formally, an attention mechanism with a ﬁxed memory Mt ∈ Rk×K of K vectors mi ∈ Rk for
i ∈ [1, K], produces an attention distribution αt ∈ RK and context vector ct ∈ Rk at each time
step t according to Equations 4 to 7. Furthermore, W M , W h ∈ Rk×k and w ∈ Rk are trainable
parameters. Finally, note that 1K represents a K-dimensional vector of ones.

Mt = [m1 . . . mK]
Gt = tanh(W M Mt + 1T
αt = softmax(wT Gt)
ct = MtαT
t

K(W hht))

∈ Rk×K
∈ Rk×K
∈ R1×K
∈ Rk

For language modeling, we populate Mt with a ﬁxed window of the previous K LSTM output
vectors. To obtain a distribution over the next token we combine the context vector ct of the attention
mechanism with the output vector ht of the LSTM using a trainable projection matrix W A ∈ Rk×2k.
The resulting ﬁnal output vector nt ∈ Rk encodes the next-word distribution and is projected to the
size of the vocabulary |V |. Subsequently, we apply a softmax to arrive at a probability distribution
yt ∈ R|V | over the next token. This process is presented in Equation 9 where WV ∈ R|V |×k and
bV ∈ R|V | are trainable parameters.

(cid:18)

nt = tanh

W A

(cid:21)(cid:19)

(cid:20)ht
ct

yt = softmax(W V nt + bV )

∈ Rk

∈ R|V |

The problem of the attention mechanism above is that it quickly becomes computationally expensive
for large K. Moreover, attending over many memories can make training hard as a lot of noise is
introduced in early stages of optimization where the LSTM outputs (and thus the memory Mt) are
more or less random. To alleviate these problems we now turn to pointer networks and a simple
heuristic for populating Mt that permits the efﬁcient retrieval of identiﬁers in a large history of
Python code.

2.3 SPARSE POINTER NETWORK

We develop an attention mechanism that provides a ﬁltered view of a large history of Python tokens.
At any given time step, the memory consists of context representations of the previous K identiﬁers
introduced in the history. This allows us to model long-range dependencies found in identiﬁer usage.
For instance, a class identiﬁer may be declared hundreds of lines of code before it is used. Given a
history of Python tokens, we obtain a next-word distribution from a weighed average of the sparse
pointer network for identiﬁer reference and a standard neural language model. The weighting of the
two is determined by a controller.
Formally, at time-step t, the sparse pointer network operates on a memory Mt ∈ Rk×K of only the
K previous identiﬁer representations (e.g. function identiﬁers, class identiﬁers and so on). In addition,
we maintain a vector mt = [id1, . . . , idK] ∈ NK of symbol ids for these identiﬁer representations
(i.e. pointers into the large global vocabulary).

As before, we calculate a context vector ct using the attention mechanism (Equation 7), but on a
memory Mt only containing representations of identiﬁers that were declared in the history. Next, we
obtain a pseudo-sparse distribution over the global vocabulary from

st[i] =

(cid:26)αt[j]
−C

if mt[j] = i
otherwise

it = softmax(st)

∈ R|V |

where −C is a large negative constant (e.g. −1000). In addition, we calculate a next-word distribution
from a standard neural language model

yt = softmax(W V ht + bV )

∈ R|V |

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

3

Under review as a conference paper at ICLR 2017

Figure 1: Sparse pointer network for code suggestion on a Python code snippet, showing the next-
word distributions of the language model and identiﬁer attention and their weighted combination
through λ

and we use a controller to calculate a distribution λt ∈ R2 over the language model and pointer
network for the ﬁnal weighted next-word distribution y∗
(cid:34) ht
xt
ct

∈ R3k

t via

t =

(13)

hλ

(cid:35)

λt = softmax(W λhλ
y∗
t = [yt it] λt

t + bλ)

∈ R2
∈ R|V |

(14)

(15)
Here, xt is the representation of the input token, and W λ ∈ R2×3k and bλ ∈ R2 a trainable
weight matrix and bias respectively. This controller is conditioned on the input, output and context
representations. This means for deciding whether to refer to an identiﬁer or generate from the global
vocabulary, the controller has access to information from the encoded next-word distribution ht of
the standard neural language model, as well as the attention-weighted identiﬁer representations ct
from the current history.

Figure 1 overviews this process. In it, the identiﬁer base_path appears twice, once as an argument
to a function and once as a member of a class (denoted by *). Each appearance has a different id
in the vocabulary and obtains a different probability from the model. In the example, the model
correctly chooses to refer to the member of the class instead of the out-of-scope function argument,
although, from a user point-of-view, the suggestion would be the same in both cases.

3 LARGE-SCALE PYTHON CORPUS

Previous work on code suggestion either focused on statically-typed languages (particularly Java)
or trained on very small corpora. Thus, we decided to collect a new large-scale corpus of the
dynamic programming language Python. According to the programming language popularity website
Pypl (Carbonnelle, 2016), Python is the second most popular language after Java. It is also the
3rd most common language in terms of number of repositories on the open-source code repository
GitHub, after JavaScript and Java (Zapponi, 2016).

We collected a corpus of 41M lines of Python code from GitHub projects. Ideally, we would like this
corpus to only contain high-quality Python code, as our language model learns to suggest code from
how users write code. However, it is difﬁcult to automatically assess what constitutes high-quality
code. Thus, we resort to the heuristic that popular code projects tend to be of good quality, There are

4

Under review as a conference paper at ICLR 2017

Table 1: Python corpus statistics.

Dataset

#Projects

#Files

#Lines

#Tokens Vocabulary Size

Train
Dev
Test

Total

489
179
281

949

118 298
26 466
43 062

26 868 583
5 804 826
8 398 100

88 935 698
18 147 341
30 178 356

187 826

41 071 509

137 261 395

2 323 819

Figure 2: Example of the Python code normalization. Original ﬁle on the left and normalized version
on the right.

two metrics on GitHub that we can use for this purpose, namely stars (similar to bookmarks) and
forks (copies of a repository that allow users to freely experiment with changes without affecting the
original repository). Similar to Allamanis & Sutton (2013) and Allamanis et al. (2014), we select
Python projects with more than 100 stars, sort by the number of forks descending, and take the top
1000 projects. We then removed projects that did not compile with Python3, leaving us with 949
projects. We split the corpus on the project level into train, dev, and test. Table 1 presents the corpus
statistics.

3.1 NORMALIZATION OF IDENTIFIERS

Unsurprisingly, the long tail of words in the vocabulary consists of rare identiﬁers. To improve
generalization, we normalize identiﬁers before feeding the resulting token stream to our models.
That is, we replace every identiﬁer name with an anonymous identiﬁer indicating the identiﬁer group
(class, variable, argument, attribute or function) concatenated with a random number that makes
the identiﬁer unique in its scope. Note that we only replace novel identiﬁers deﬁned within a ﬁle.
Identiﬁer references to external APIs and libraries are left untouched. Consistent with previous corpus
creation for code suggestion (e.g. Khanh Dam et al., 2016; White et al., 2015), we replace numerical
constant tokens with $NUM$, remove comments, reformat the code, and replace tokens appearing
less than ﬁve times with an $OOV$ (out of vocabulary) token.

4 EXPERIMENTS

Although previous work by White et al. (2015) already established that a simple neural language
model outperforms an n-gram model for code suggestion, we include a number of n-gram baselines
to conﬁrm this observation. Speciﬁcally, we use n-gram models for n ∈ {3, 4, 5, 6} with Modiﬁed
Kneser-Ney smoothing (Kneser & Ney, 1995) from the Kyoto Language Modelling Toolkit (Neubig,
2012).

We train the sparse pointer network using mini-batch SGD with a batch size of 30 and truncated
backpropagation through time (Werbos, 1990) with a history of 20 identiﬁer representations. We use

5

Under review as a conference paper at ICLR 2017

Table 2: Perplexity (PP), Accuracy (Acc) and Accuarcy among top 5 predictions (Acc@5).

Model

3-gram
4-gram
5-gram
6-gram

LSTM
LSTM w/ Attention 20
LSTM w/ Attention 50

Sparse Pointer Network

Train PP Dev PP Test PP

Acc [%]

Acc@5 [%]

12.90
7.60
4.52
3.37

9.29
7.30
7.09

6.41

24.19
21.07
19.33
18.73

13.08
11.07
9.83

9.40

All

IDs Other

All

IDs Other

26.90
23.85
21.22
20.17

14.01
11.74
10.05

13.19
13.68
13.90
14.51

57.91
61.30
63.21

9.18

62.97

–
–
–
–

2.1
21.4
30.2

27.3

–
–
–
–

62.8
64.8
65.3

64.9

50.81
51.26
51.49
51.76

76.30
79.32
81.69

82.62

–
–
–
–

4.5
29.9
41.3

43.6

–
–
–
–

82.6
83.7
84.1

84.5

an initial learning rate of 0.7 and decay it by 0.9 after every epoch. As additional baselines, we test
a neural language model with LSTM units with and without attention. For the attention language
models, we experiment with a ﬁxed-window attention memory of the previous 20 and 50 tokens
respectively, and a batch size of 75.

All neural language models were developed in TensorFlow (Abadi et al., 2016) and trained using
cross-entropy loss. While processing a Python source code ﬁle, the last recurrent state of the RNN
is fed as the initial state of the subsequent sequence of the same ﬁle and reset between ﬁles. All
models use an input and hidden size of 200, an LSTM forget gate bias of 1 (Jozefowicz et al., 2015),
gradient norm clipping of 5 (Pascanu et al., 2013), and randomly initialized parameters in the interval
(−0.05, 0.05). As regularizer, we use a dropout of 0.1 on the input representations. Furthermore, we
use a sampled softmax (Jean et al., 2015) with a log-uniform sampling distribution and a sample size
of 1000.

5 RESULTS

We evaluate all models using perplexity (PP), as well as accuracy of the top prediction (Acc) and the
top ﬁve predictions (Acc@5). The results are summarized in Table 2.

We can conﬁrm that for code suggestion neural models outperform n-gram language models by a
large margin. Furthermore, adding attention improves the results substantially (2.3 lower perplexity
and 3.4 percentage points increased accuracy). Interestingly, this increase can be attributed to a
superior prediction of identiﬁers, which increased from an accuracy of 2.1% to 21.4%. An LSTM
with an attention window of 50 gives us the best accuracy for the top prediction. We achieve further
improvements for perplexity and accuracy of the top ﬁve predictions by using a sparse pointer network
that uses a smaller memory of the past 20 identiﬁer representations.

5.1 QUALITATIVE ANALYSIS

Figures 3a-d show a code suggestion example involving an identiﬁer usage. While the LSTM baseline
is uncertain about the next token, we get a sensible prediction by using attention or the sparse pointer
network. The sparse pointer network provides more reasonable alternative suggestions beyond the
correct top suggestion.

Figures 3e-h show the use-case referring to a class attribute declared 67 tokens in the past. Only
the Sparse Pointer Network makes a good suggestion. Furthermore, the attention weights in 3i
demonstrate that this model distinguished attributes from other groups of identiﬁers. We give a full
example of a token-by-token suggestion of the Sparse Pointer Network in Figure 4 in the Appendix.

6 RELATED WORK

Previous code suggestion work using methods from statistical NLP has mostly focused on n-gram
models. Much of this work is inspired by Hindle et al. (2012) who argued that real programs fall

6

Under review as a conference paper at ICLR 2017

(a) Code snippet for referencing
variable.

(b) LSTM Model.

(c) LSTM w/ Attention
50.

(d) Sparse Pointer Net-
work.

(e) Code snippet for referencing class
member.

(f) LSTM Model.

(g) LSTM w/ Attention
50.

(h) Sparse Pointer Net-
work.

(i) Sparse Pointer Network attention over memory of identiﬁer representations.

Figure 3: Code suggestion example involving a reference to a variable (a-d), a long-range dependency
(e-h), and the attention weights of the Sparse Pointer Network (i).

into a much smaller space than the ﬂexibility of programming languages allows. They were able
to capture the repetitiveness and predictable statistical properties of real programs using language
models. Subsequently, Tu et al. (2014) improved upon Hindle et al.’s work by adding a cache
mechanism that allowed them to exploit locality stemming from the specialisation and decoupling of
program modules. Tu et al.’s idea of adding a cache mechanism to the language model is speciﬁcally
designed to exploit the properties of source code, and thus follows the same aim as the sparse attention
mechanism introduced in this paper.

While the majority of preceding work trained on small corpora, Allamanis & Sutton (2013) created a
corpus of 352M lines of Java code which they analysed with n-gram language models. The size of
the corpus allowed them to train a single language model that was effective across multiple different
project domains. White et al. (2015) later demonstrated that neural language models outperform
n-gram models for code suggestion. They compared various n-gram models (up to nine grams),
including Tu et al.’s cache model, with a basic RNN neural language model. Khanh Dam et al.
(2016) compared White et al.’s basic RNN with LSTMs and found that the latter are better at code
suggestion due to their improved ability to learn long-range dependencies found in source code. Our
paper extends this line of work by introducing a sparse attention model that captures even longer
dependencies.

The combination of lagged attention mechanisms with language modelling is inspired by Cheng et al.
(2016) who equipped LSTM cells with a ﬁxed-length memory tape rather than a single memory cell.
They achieved promising results on the standard Penn Treebank benchmark corpus (Marcus et al.,
1993). Similarly, Tran et al. added a memory block to LSTMs for language modelling of English,
German and Italian and outperformed both n-gram and neural language models. Their memory
encompasses representations of all possible words in the vocabulary rather than providing a sparse
view as we do.

An alternative to our purely lexical approach to code suggestion involves the use of probabilistic
context-free grammars (PCFGs) which exploit the formal grammar speciﬁcations and well-deﬁned,
deterministic parsers available for source code. These were used by Allamanis & Sutton (2014)
to extract idiomatic patterns from source code. A weakness of PCFGs is their inability to model
context-dependent rules of programming languages such as that variables need to be declared before

7

Under review as a conference paper at ICLR 2017

being used. Maddison & Tarlow (2014) added context-aware variables to their PCFG model in order
to capture such rules.

Ling et al. (2016) recently used a pointer network to generate code from natural language descriptions.
Our use of a controller for deciding whether to generate from a language model or copy an identiﬁer
using a sparse pointer network is inspired by their latent code predictor. However, their inputs (textual
descriptions) are short whereas code suggestion requires capturing very long-range dependencies that
we addressed by a ﬁltered view on the memory of previous identiﬁer representations.

7 CONCLUSIONS AND FUTURE WORK

In this paper, we investigated neural language models for code suggestion of the dynamically-typed
programming language Python. We released a corpus of 41M lines of Python crawled from GitHub
and compared n-gram, standard neural language models, and attention. By using attention, we
observed an order of magnitude more accurate prediction of identiﬁers. Furthermore, we proposed
a sparse pointer network that can efﬁciently capture long-range dependencies by only operating
on a ﬁltered view of a memory of previous identiﬁer representations. This model achieves the
lowest perplexity and best accuracy among the top ﬁve predictions. The Python corpus and code for
replicating our experiment is released at https://github.com/uclmr/pycodesuggest.

The presented methods were only tested for code suggestion within the same Python ﬁle. We are
interested in scaling the approach to the level of entire code projects and collections thereof, as well
as integrating a trained code suggestion model into an existing IDE. Furthermore, we plan to work on
code completion, i.e., models that provide a likely continuation of a partial token, using character
language models (Graves, 2013).

This work was supported by Microsoft Research through its PhD Scholarship Programme, an Allen
Distinguished Investigator Award, and a Marie Curie Career Integration Award.

ACKNOWLEDGMENTS

REFERENCES

Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat
Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan,
Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zhang. Tensorﬂow: A system for large-scale
machine learning. CoRR, abs/1605.08695, 2016. URL http://arxiv.org/abs/1605.
08695.

Miltiadis Allamanis and Charles Sutton. Mining idioms from source code.

In Proceedings of
the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 472–483, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635901. URL http://doi.acm.org/10.1145/2635868.2635901.

Miltiadis Allamanis and Charles A. Sutton. Mining source code repositories at massive scale
using language modeling. In Thomas Zimmermann, Massimiliano Di Penta, and Sunghun Kim
(eds.), MSR, pp. 207–216. IEEE Computer Society, 2013.
ISBN 978-1-4673-2936-1. URL
http://dblp.uni-trier.de/db/conf/msr/msr2013.html#AllamanisS13a.

Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Learning natural coding
conventions. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations
of Software Engineering, FSE 2014, pp. 281–293, New York, NY, USA, 2014. ACM. ISBN 978-
1-4503-3056-5. doi: 10.1145/2635868.2635883. URL http://doi.acm.org/10.1145/
2635868.2635883.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/abs/
1409.0473.

8

Under review as a conference paper at ICLR 2017

Pierre Carbonnelle. Pypl popularity of programming language. http://pypl.github.io/
PYPL.html, 2016. URL http://pypl.github.io/PYPL.html. [Online; accessed 30-
August-2016].

Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing, pp. 551–561. Association for Computational Linguistics, 2016. URL http://
aclweb.org/anthology/D16-1053.

Subhasis Das and Chinmayee Shah. Contextual code completion using machine learning. 2015.

Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.

URL http://arxiv.org/abs/1308.0850.

Karl Moritz Hermann, Tom´as Kocisk´y, Edward Grefenstette, Lasse Espeholt, Will Kay,
Teaching machines to read and compre-
Mustafa Suleyman,
In Advances in Neural Information Processing Systems 28: Annual Confer-
hend.
ence on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal,
URL http://papers.nips.cc/paper/
Quebec, Canada, pp. 1693–1701, 2015.
5945-teaching-machines-to-read-and-comprehend.

and Phil Blunsom.

Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the naturalness
of software. In Proceedings of the 34th International Conference on Software Engineering, ICSE
’12, pp. 837–847, Piscataway, NJ, USA, 2012. IEEE Press. ISBN 978-1-4673-1067-3. URL
http://dl.acm.org/citation.cfm?id=2337223.2337322.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target
vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), pp. 1–10, Beijing, China, July 2015. Association
for Computational Linguistics. URL http://www.aclweb.org/anthology/P15-1001.

Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent
network architectures. In David Blei and Francis Bach (eds.), Proceedings of the 32nd Inter-
national Conference on Machine Learning (ICML-15), pp. 2342–2350. JMLR Workshop and
Conference Proceedings, 2015. URL http://jmlr.org/proceedings/papers/v37/
jozefowicz15.pdf.

H. Khanh Dam, T. Tran, and T. Pham. A deep language model for software code. ArXiv e-prints,

August 2016.

R. Kneser and H. Ney. Improved backing-off for m-gram language modeling. In Acoustics, Speech,
and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pp.
181–184 vol.1, May 1995. doi: 10.1109/ICASSP.1995.479394.

Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, Andrew Senior, Fumin
Wang, and Phil Blunsom. Latent predictor networks for code generation. arXiv preprint
arXiv:1603.06744, 2016.

Chris J Maddison and Daniel Tarlow. Structured generative models of natural source code. In

International Conference on Machine Learning, 2014.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313–330, 1993.

Graham Neubig. Kylm - the kyoto language modeling toolkit. http://www.phontron.com/
[Online; accessed 23-July-

kylm/, 2012. URL http://www.phontron.com/kylm/.
2016].

9

Under review as a conference paper at ICLR 2017

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
networks. In Proceedings of the 30th International Conference on Machine Learning, ICML
2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1310–1318, 2013. URL http://jmlr.org/
proceedings/papers/v28/pascanu13.html.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom.

Reasoning about entailment with neural attention. In ICLR, 2016.

Ke M. Tran, Arianna Bisazza, and Christof Monz. Recurrent memory networks for language
modeling. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, San Diego California,
USA, June 12-17, 2016, pp. 321–331, 2016. URL http://aclweb.org/anthology/N/
N16/N16-1036.pdf.

Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. On the localness of software. In Proceedings
of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 269–280, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635875. URL http://doi.acm.org/10.1145/2635868.2635875.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural

Information Processing Systems, pp. 2692–2700, 2015a.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton.
Grammar as a foreign language. In Advances in Neural Information Processing Systems 28:
Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon-
treal, Quebec, Canada, pp. 2773–2781, 2015b. URL http://papers.nips.cc/paper/
5635-grammar-as-a-foreign-language.

Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the

IEEE, 78(10):1550–1560, 1990.

Martin White, Christopher Vendome, Mario Linares-V´asquez, and Denys Poshyvanyk. Toward
deep learning software repositories. In Proceedings of the 12th Working Conference on Mining
Software Repositories, MSR ’15, pp. 334–345, Piscataway, NJ, USA, 2015. IEEE Press. URL
http://dl.acm.org/citation.cfm?id=2820518.2820559.

Carlo Zapponi. Githut - programming languages and github. http://githut.info/, 2016.

URL http://githut.info/. [Online; accessed 19-August-2016].

10

Under review as a conference paper at ICLR 2017

APPENDIX

Figure 4: Full example of code suggestion with a Sparse Pointer Network. Boldface tokens on the left
show the ﬁrst declaration of an identiﬁer. The middle part visualizes the memory of representations of
these identiﬁers. The right part visualizes the output λ of the controller, which is used for interpolating
between the language model (LM) and the attention of the pointer network (Att).

11

6
1
0
2
 
v
o
N
 
4
2
 
 
]
E
N
.
s
c
[
 
 
1
v
7
0
3
8
0
.
1
1
6
1
:
v
i
X
r
a

Under review as a conference paper at ICLR 2017

LEARNING PYTHON CODE SUGGESTION WITH A
SPARSE POINTER NETWORK

Avishkar Bhoopchand, Tim Rockt¨aschel, Earl Barr & Sebastian Riedel
Department of Computer Science
University College London
avishkar.bhoopchand.15@ucl.ac.uk,
{t.rocktaschel,e.barr,s.riedel}@cs.ucl.ac.uk

ABSTRACT

To enhance developer productivity, all modern integrated development environ-
ments (IDEs) include code suggestion functionality that proposes likely next tokens
at the cursor. While current IDEs work well for statically-typed languages, their re-
liance on type annotations means that they do not provide the same level of support
for dynamic programming languages as for statically-typed languages. Moreover,
suggestion engines in modern IDEs do not propose expressions or multi-statement
idiomatic code. Recent work has shown that language models can improve code
suggestion systems by learning from software repositories. This paper introduces a
neural language model with a sparse pointer network aimed at capturing very long-
range dependencies. We release a large-scale code suggestion corpus of 41M lines
of Python code crawled from GitHub. On this corpus, we found standard neural
language models to perform well at suggesting local phenomena, but struggle to
refer to identiﬁers that are introduced many tokens in the past. By augmenting a
neural language model with a pointer network specialized in referring to predeﬁned
classes of identiﬁers, we obtain a much lower perplexity and a 5 percentage points
increase in accuracy for code suggestion compared to an LSTM baseline. In fact,
this increase in code suggestion accuracy is due to a 13 times more accurate pre-
diction of identiﬁers. Furthermore, a qualitative analysis shows this model indeed
captures interesting long-range dependencies, like referring to a class member
deﬁned over 60 tokens in the past.

1

INTRODUCTION

Integrated development environments (IDEs) are essential tools for programmers. Especially when a
developer is new to a codebase, one of their most useful features is code suggestion: given a piece of
code as context, suggest a likely sequence of next tokens. Typically, the IDE suggests an identiﬁer or
a function call, including API calls. While extensive support exists for statically-typed languages
such as Java, code suggestion for dynamic languages like Python is harder and less well supported
because of the lack of type annotations. Moreover, suggestion engines in modern IDEs do not propose
expressions or multi-statement idiomatic code.

Recently, methods from statistical natural language processing (NLP) have been used to train code
suggestion systems from code usage in large code repositories (Hindle et al., 2012; Allamanis &
Sutton, 2013; Tu et al., 2014). To this end, usually an n-gram language model is trained to score
possible completions. Neural language models for code suggestion (White et al., 2015; Das &
Shah, 2015) have extended this line of work to capture more long-range dependencies. Yet, these
standard neural language models are limited by the so-called hidden state bottleneck, i.e., all context
information has to be stored in a ﬁxed-dimensional internal vector representation. This limitation
restricts such models to local phenomena and does not capture very long-range semantic relationships
like suggesting calling a function that has been deﬁned many tokens before.

To address these issues, we create a large corpus of 41M lines of Python code by using a heuristic for
crawling high-quality code repositories from GitHub. We investigate, for the ﬁrst time, the use of
attention (Bahdanau et al., 2014) for code suggestion and ﬁnd that, despite a substantial improvement

1

Under review as a conference paper at ICLR 2017

in accuracy, it still makes avoidable mistakes. Hence, we introduce a model that leverages long-range
Python dependencies by selectively attending over the introduction of identiﬁers as determined
by examining the Abstract Syntax Tree. The model is a form of pointer network (Vinyals et al.,
2015a), and learns to dynamically choose between syntax-aware pointing for modeling long-range
dependencies and free form generation to deal with local phenomena, based on the current context.

Our contributions are threefold: (i) We release a code suggestion corpus of 41M lines of Python code
crawled from GitHub, (ii) We introduce a sparse attention mechanism that captures very long-range
dependencies for code suggestion of this dynamic programming language efﬁciently, and (iii) We
provide a qualitative analysis demonstrating that this model is indeed able to learn such long-range
dependencies.

2 METHODS

We ﬁrst revisit neural language models, before brieﬂy describing how to extend such a language
model with an attention mechanism. Then we introduce a sparse attention mechanism for a pointer
network that can exploit the Python abstract syntax tree of the current context for code suggestion.

2.1 NEURAL LANGUAGE MODEL

Code suggestion can be approached by a language model that measures the probability of observing
a sequence of tokens in a Python program. For example, for the sequence S = a1, . . . , aN , the joint
probability of S factorizes according to

Pθ(S) = Pθ(a1) ·

Pθ(at | at−1, . . . , a1)

N
(cid:89)

t=2

(1)

(2)

(3)

where the parameters θ are estimated from a training corpus. Given a sequence of Python tokens, we
seek to predict the next M tokens at+1, . . . , at+M that maximize Equation 1

arg max
at+1, ..., at+M

Pθ(a1, . . . , at, at+1, . . . , at+M ).

In this work, we build upon neural language models using Recurrent Neural Networks (RNNs) and
Long Short-Term Memory (LSTM, Hochreiter & Schmidhuber, 1997). This neural language model
estimates the probabilities in Equation 1 using the output vector of an LSTM at time step t (denoted
ht here) according to

Pθ(at = τ | at−1, . . . , a1) =

τ ht + bτ )

exp (vT
τ (cid:48) exp (vT

(cid:80)

τ (cid:48)ht + bτ (cid:48))

where vτ is a parameter vector associated with token τ in the vocabulary.

Neural language models can, in theory, capture long-term dependencies in token sequences through
their internal memory. However, as this internal memory has ﬁxed dimension and can be updated at
every time step, such models often only capture local phenomena. In contrast, we are interested in
very long-range dependencies like referring to a function identiﬁer introduced many tokens in the
past. For example, a function identiﬁer may be introduced at the top of a ﬁle and only used near
the bottom. In the following, we investigate various external memory architectures for neural code
suggestion.

2.2 ATTENTION

A straight-forward approach to capturing long-range dependencies is to use a neural attention mech-
anism (Bahdanau et al., 2014) on the previous K output vectors of the language model. Attention
mechanisms have been successfully applied to sequence-to-sequence tasks such as machine transla-
tion (Bahdanau et al., 2014), question-answering (Hermann et al., 2015), syntactic parsing (Vinyals
et al., 2015b), as well as dual-sequence modeling like recognizing textual entailment (Rockt¨aschel
et al., 2016). The idea is to overcome the hidden-state bottleneck by allowing referral back to previous
output vectors. Recently, these mechanisms were applied to language modelling by Cheng et al.
(2016) and Tran et al. (2016).

2

Under review as a conference paper at ICLR 2017

Formally, an attention mechanism with a ﬁxed memory Mt ∈ Rk×K of K vectors mi ∈ Rk for
i ∈ [1, K], produces an attention distribution αt ∈ RK and context vector ct ∈ Rk at each time
step t according to Equations 4 to 7. Furthermore, W M , W h ∈ Rk×k and w ∈ Rk are trainable
parameters. Finally, note that 1K represents a K-dimensional vector of ones.

Mt = [m1 . . . mK]
Gt = tanh(W M Mt + 1T
αt = softmax(wT Gt)
ct = MtαT
t

K(W hht))

∈ Rk×K
∈ Rk×K
∈ R1×K
∈ Rk

For language modeling, we populate Mt with a ﬁxed window of the previous K LSTM output
vectors. To obtain a distribution over the next token we combine the context vector ct of the attention
mechanism with the output vector ht of the LSTM using a trainable projection matrix W A ∈ Rk×2k.
The resulting ﬁnal output vector nt ∈ Rk encodes the next-word distribution and is projected to the
size of the vocabulary |V |. Subsequently, we apply a softmax to arrive at a probability distribution
yt ∈ R|V | over the next token. This process is presented in Equation 9 where WV ∈ R|V |×k and
bV ∈ R|V | are trainable parameters.

(cid:18)

nt = tanh

W A

(cid:21)(cid:19)

(cid:20)ht
ct

yt = softmax(W V nt + bV )

∈ Rk

∈ R|V |

The problem of the attention mechanism above is that it quickly becomes computationally expensive
for large K. Moreover, attending over many memories can make training hard as a lot of noise is
introduced in early stages of optimization where the LSTM outputs (and thus the memory Mt) are
more or less random. To alleviate these problems we now turn to pointer networks and a simple
heuristic for populating Mt that permits the efﬁcient retrieval of identiﬁers in a large history of
Python code.

2.3 SPARSE POINTER NETWORK

We develop an attention mechanism that provides a ﬁltered view of a large history of Python tokens.
At any given time step, the memory consists of context representations of the previous K identiﬁers
introduced in the history. This allows us to model long-range dependencies found in identiﬁer usage.
For instance, a class identiﬁer may be declared hundreds of lines of code before it is used. Given a
history of Python tokens, we obtain a next-word distribution from a weighed average of the sparse
pointer network for identiﬁer reference and a standard neural language model. The weighting of the
two is determined by a controller.
Formally, at time-step t, the sparse pointer network operates on a memory Mt ∈ Rk×K of only the
K previous identiﬁer representations (e.g. function identiﬁers, class identiﬁers and so on). In addition,
we maintain a vector mt = [id1, . . . , idK] ∈ NK of symbol ids for these identiﬁer representations
(i.e. pointers into the large global vocabulary).

As before, we calculate a context vector ct using the attention mechanism (Equation 7), but on a
memory Mt only containing representations of identiﬁers that were declared in the history. Next, we
obtain a pseudo-sparse distribution over the global vocabulary from

st[i] =

(cid:26)αt[j]
−C

if mt[j] = i
otherwise

it = softmax(st)

∈ R|V |

where −C is a large negative constant (e.g. −1000). In addition, we calculate a next-word distribution
from a standard neural language model

yt = softmax(W V ht + bV )

∈ R|V |

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

3

Under review as a conference paper at ICLR 2017

Figure 1: Sparse pointer network for code suggestion on a Python code snippet, showing the next-
word distributions of the language model and identiﬁer attention and their weighted combination
through λ

and we use a controller to calculate a distribution λt ∈ R2 over the language model and pointer
network for the ﬁnal weighted next-word distribution y∗
(cid:34) ht
xt
ct

∈ R3k

t via

t =

(13)

hλ

(cid:35)

λt = softmax(W λhλ
y∗
t = [yt it] λt

t + bλ)

∈ R2
∈ R|V |

(14)

(15)
Here, xt is the representation of the input token, and W λ ∈ R2×3k and bλ ∈ R2 a trainable
weight matrix and bias respectively. This controller is conditioned on the input, output and context
representations. This means for deciding whether to refer to an identiﬁer or generate from the global
vocabulary, the controller has access to information from the encoded next-word distribution ht of
the standard neural language model, as well as the attention-weighted identiﬁer representations ct
from the current history.

Figure 1 overviews this process. In it, the identiﬁer base_path appears twice, once as an argument
to a function and once as a member of a class (denoted by *). Each appearance has a different id
in the vocabulary and obtains a different probability from the model. In the example, the model
correctly chooses to refer to the member of the class instead of the out-of-scope function argument,
although, from a user point-of-view, the suggestion would be the same in both cases.

3 LARGE-SCALE PYTHON CORPUS

Previous work on code suggestion either focused on statically-typed languages (particularly Java)
or trained on very small corpora. Thus, we decided to collect a new large-scale corpus of the
dynamic programming language Python. According to the programming language popularity website
Pypl (Carbonnelle, 2016), Python is the second most popular language after Java. It is also the
3rd most common language in terms of number of repositories on the open-source code repository
GitHub, after JavaScript and Java (Zapponi, 2016).

We collected a corpus of 41M lines of Python code from GitHub projects. Ideally, we would like this
corpus to only contain high-quality Python code, as our language model learns to suggest code from
how users write code. However, it is difﬁcult to automatically assess what constitutes high-quality
code. Thus, we resort to the heuristic that popular code projects tend to be of good quality, There are

4

Under review as a conference paper at ICLR 2017

Table 1: Python corpus statistics.

Dataset

#Projects

#Files

#Lines

#Tokens Vocabulary Size

Train
Dev
Test

Total

489
179
281

949

118 298
26 466
43 062

26 868 583
5 804 826
8 398 100

88 935 698
18 147 341
30 178 356

187 826

41 071 509

137 261 395

2 323 819

Figure 2: Example of the Python code normalization. Original ﬁle on the left and normalized version
on the right.

two metrics on GitHub that we can use for this purpose, namely stars (similar to bookmarks) and
forks (copies of a repository that allow users to freely experiment with changes without affecting the
original repository). Similar to Allamanis & Sutton (2013) and Allamanis et al. (2014), we select
Python projects with more than 100 stars, sort by the number of forks descending, and take the top
1000 projects. We then removed projects that did not compile with Python3, leaving us with 949
projects. We split the corpus on the project level into train, dev, and test. Table 1 presents the corpus
statistics.

3.1 NORMALIZATION OF IDENTIFIERS

Unsurprisingly, the long tail of words in the vocabulary consists of rare identiﬁers. To improve
generalization, we normalize identiﬁers before feeding the resulting token stream to our models.
That is, we replace every identiﬁer name with an anonymous identiﬁer indicating the identiﬁer group
(class, variable, argument, attribute or function) concatenated with a random number that makes
the identiﬁer unique in its scope. Note that we only replace novel identiﬁers deﬁned within a ﬁle.
Identiﬁer references to external APIs and libraries are left untouched. Consistent with previous corpus
creation for code suggestion (e.g. Khanh Dam et al., 2016; White et al., 2015), we replace numerical
constant tokens with $NUM$, remove comments, reformat the code, and replace tokens appearing
less than ﬁve times with an $OOV$ (out of vocabulary) token.

4 EXPERIMENTS

Although previous work by White et al. (2015) already established that a simple neural language
model outperforms an n-gram model for code suggestion, we include a number of n-gram baselines
to conﬁrm this observation. Speciﬁcally, we use n-gram models for n ∈ {3, 4, 5, 6} with Modiﬁed
Kneser-Ney smoothing (Kneser & Ney, 1995) from the Kyoto Language Modelling Toolkit (Neubig,
2012).

We train the sparse pointer network using mini-batch SGD with a batch size of 30 and truncated
backpropagation through time (Werbos, 1990) with a history of 20 identiﬁer representations. We use

5

Under review as a conference paper at ICLR 2017

Table 2: Perplexity (PP), Accuracy (Acc) and Accuarcy among top 5 predictions (Acc@5).

Model

3-gram
4-gram
5-gram
6-gram

LSTM
LSTM w/ Attention 20
LSTM w/ Attention 50

Sparse Pointer Network

Train PP Dev PP Test PP

Acc [%]

Acc@5 [%]

12.90
7.60
4.52
3.37

9.29
7.30
7.09

6.41

24.19
21.07
19.33
18.73

13.08
11.07
9.83

9.40

All

IDs Other

All

IDs Other

26.90
23.85
21.22
20.17

14.01
11.74
10.05

13.19
13.68
13.90
14.51

57.91
61.30
63.21

9.18

62.97

–
–
–
–

2.1
21.4
30.2

27.3

–
–
–
–

62.8
64.8
65.3

64.9

50.81
51.26
51.49
51.76

76.30
79.32
81.69

82.62

–
–
–
–

4.5
29.9
41.3

43.6

–
–
–
–

82.6
83.7
84.1

84.5

an initial learning rate of 0.7 and decay it by 0.9 after every epoch. As additional baselines, we test
a neural language model with LSTM units with and without attention. For the attention language
models, we experiment with a ﬁxed-window attention memory of the previous 20 and 50 tokens
respectively, and a batch size of 75.

All neural language models were developed in TensorFlow (Abadi et al., 2016) and trained using
cross-entropy loss. While processing a Python source code ﬁle, the last recurrent state of the RNN
is fed as the initial state of the subsequent sequence of the same ﬁle and reset between ﬁles. All
models use an input and hidden size of 200, an LSTM forget gate bias of 1 (Jozefowicz et al., 2015),
gradient norm clipping of 5 (Pascanu et al., 2013), and randomly initialized parameters in the interval
(−0.05, 0.05). As regularizer, we use a dropout of 0.1 on the input representations. Furthermore, we
use a sampled softmax (Jean et al., 2015) with a log-uniform sampling distribution and a sample size
of 1000.

5 RESULTS

We evaluate all models using perplexity (PP), as well as accuracy of the top prediction (Acc) and the
top ﬁve predictions (Acc@5). The results are summarized in Table 2.

We can conﬁrm that for code suggestion neural models outperform n-gram language models by a
large margin. Furthermore, adding attention improves the results substantially (2.3 lower perplexity
and 3.4 percentage points increased accuracy). Interestingly, this increase can be attributed to a
superior prediction of identiﬁers, which increased from an accuracy of 2.1% to 21.4%. An LSTM
with an attention window of 50 gives us the best accuracy for the top prediction. We achieve further
improvements for perplexity and accuracy of the top ﬁve predictions by using a sparse pointer network
that uses a smaller memory of the past 20 identiﬁer representations.

5.1 QUALITATIVE ANALYSIS

Figures 3a-d show a code suggestion example involving an identiﬁer usage. While the LSTM baseline
is uncertain about the next token, we get a sensible prediction by using attention or the sparse pointer
network. The sparse pointer network provides more reasonable alternative suggestions beyond the
correct top suggestion.

Figures 3e-h show the use-case referring to a class attribute declared 67 tokens in the past. Only
the Sparse Pointer Network makes a good suggestion. Furthermore, the attention weights in 3i
demonstrate that this model distinguished attributes from other groups of identiﬁers. We give a full
example of a token-by-token suggestion of the Sparse Pointer Network in Figure 4 in the Appendix.

6 RELATED WORK

Previous code suggestion work using methods from statistical NLP has mostly focused on n-gram
models. Much of this work is inspired by Hindle et al. (2012) who argued that real programs fall

6

Under review as a conference paper at ICLR 2017

(a) Code snippet for referencing
variable.

(b) LSTM Model.

(c) LSTM w/ Attention
50.

(d) Sparse Pointer Net-
work.

(e) Code snippet for referencing class
member.

(f) LSTM Model.

(g) LSTM w/ Attention
50.

(h) Sparse Pointer Net-
work.

(i) Sparse Pointer Network attention over memory of identiﬁer representations.

Figure 3: Code suggestion example involving a reference to a variable (a-d), a long-range dependency
(e-h), and the attention weights of the Sparse Pointer Network (i).

into a much smaller space than the ﬂexibility of programming languages allows. They were able
to capture the repetitiveness and predictable statistical properties of real programs using language
models. Subsequently, Tu et al. (2014) improved upon Hindle et al.’s work by adding a cache
mechanism that allowed them to exploit locality stemming from the specialisation and decoupling of
program modules. Tu et al.’s idea of adding a cache mechanism to the language model is speciﬁcally
designed to exploit the properties of source code, and thus follows the same aim as the sparse attention
mechanism introduced in this paper.

While the majority of preceding work trained on small corpora, Allamanis & Sutton (2013) created a
corpus of 352M lines of Java code which they analysed with n-gram language models. The size of
the corpus allowed them to train a single language model that was effective across multiple different
project domains. White et al. (2015) later demonstrated that neural language models outperform
n-gram models for code suggestion. They compared various n-gram models (up to nine grams),
including Tu et al.’s cache model, with a basic RNN neural language model. Khanh Dam et al.
(2016) compared White et al.’s basic RNN with LSTMs and found that the latter are better at code
suggestion due to their improved ability to learn long-range dependencies found in source code. Our
paper extends this line of work by introducing a sparse attention model that captures even longer
dependencies.

The combination of lagged attention mechanisms with language modelling is inspired by Cheng et al.
(2016) who equipped LSTM cells with a ﬁxed-length memory tape rather than a single memory cell.
They achieved promising results on the standard Penn Treebank benchmark corpus (Marcus et al.,
1993). Similarly, Tran et al. added a memory block to LSTMs for language modelling of English,
German and Italian and outperformed both n-gram and neural language models. Their memory
encompasses representations of all possible words in the vocabulary rather than providing a sparse
view as we do.

An alternative to our purely lexical approach to code suggestion involves the use of probabilistic
context-free grammars (PCFGs) which exploit the formal grammar speciﬁcations and well-deﬁned,
deterministic parsers available for source code. These were used by Allamanis & Sutton (2014)
to extract idiomatic patterns from source code. A weakness of PCFGs is their inability to model
context-dependent rules of programming languages such as that variables need to be declared before

7

Under review as a conference paper at ICLR 2017

being used. Maddison & Tarlow (2014) added context-aware variables to their PCFG model in order
to capture such rules.

Ling et al. (2016) recently used a pointer network to generate code from natural language descriptions.
Our use of a controller for deciding whether to generate from a language model or copy an identiﬁer
using a sparse pointer network is inspired by their latent code predictor. However, their inputs (textual
descriptions) are short whereas code suggestion requires capturing very long-range dependencies that
we addressed by a ﬁltered view on the memory of previous identiﬁer representations.

7 CONCLUSIONS AND FUTURE WORK

In this paper, we investigated neural language models for code suggestion of the dynamically-typed
programming language Python. We released a corpus of 41M lines of Python crawled from GitHub
and compared n-gram, standard neural language models, and attention. By using attention, we
observed an order of magnitude more accurate prediction of identiﬁers. Furthermore, we proposed
a sparse pointer network that can efﬁciently capture long-range dependencies by only operating
on a ﬁltered view of a memory of previous identiﬁer representations. This model achieves the
lowest perplexity and best accuracy among the top ﬁve predictions. The Python corpus and code for
replicating our experiment is released at https://github.com/uclmr/pycodesuggest.

The presented methods were only tested for code suggestion within the same Python ﬁle. We are
interested in scaling the approach to the level of entire code projects and collections thereof, as well
as integrating a trained code suggestion model into an existing IDE. Furthermore, we plan to work on
code completion, i.e., models that provide a likely continuation of a partial token, using character
language models (Graves, 2013).

This work was supported by Microsoft Research through its PhD Scholarship Programme, an Allen
Distinguished Investigator Award, and a Marie Curie Career Integration Award.

ACKNOWLEDGMENTS

REFERENCES

Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat
Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan,
Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zhang. Tensorﬂow: A system for large-scale
machine learning. CoRR, abs/1605.08695, 2016. URL http://arxiv.org/abs/1605.
08695.

Miltiadis Allamanis and Charles Sutton. Mining idioms from source code.

In Proceedings of
the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 472–483, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635901. URL http://doi.acm.org/10.1145/2635868.2635901.

Miltiadis Allamanis and Charles A. Sutton. Mining source code repositories at massive scale
using language modeling. In Thomas Zimmermann, Massimiliano Di Penta, and Sunghun Kim
(eds.), MSR, pp. 207–216. IEEE Computer Society, 2013.
ISBN 978-1-4673-2936-1. URL
http://dblp.uni-trier.de/db/conf/msr/msr2013.html#AllamanisS13a.

Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Learning natural coding
conventions. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations
of Software Engineering, FSE 2014, pp. 281–293, New York, NY, USA, 2014. ACM. ISBN 978-
1-4503-3056-5. doi: 10.1145/2635868.2635883. URL http://doi.acm.org/10.1145/
2635868.2635883.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/abs/
1409.0473.

8

Under review as a conference paper at ICLR 2017

Pierre Carbonnelle. Pypl popularity of programming language. http://pypl.github.io/
PYPL.html, 2016. URL http://pypl.github.io/PYPL.html. [Online; accessed 30-
August-2016].

Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing, pp. 551–561. Association for Computational Linguistics, 2016. URL http://
aclweb.org/anthology/D16-1053.

Subhasis Das and Chinmayee Shah. Contextual code completion using machine learning. 2015.

Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.

URL http://arxiv.org/abs/1308.0850.

Karl Moritz Hermann, Tom´as Kocisk´y, Edward Grefenstette, Lasse Espeholt, Will Kay,
Teaching machines to read and compre-
Mustafa Suleyman,
In Advances in Neural Information Processing Systems 28: Annual Confer-
hend.
ence on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal,
URL http://papers.nips.cc/paper/
Quebec, Canada, pp. 1693–1701, 2015.
5945-teaching-machines-to-read-and-comprehend.

and Phil Blunsom.

Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the naturalness
of software. In Proceedings of the 34th International Conference on Software Engineering, ICSE
’12, pp. 837–847, Piscataway, NJ, USA, 2012. IEEE Press. ISBN 978-1-4673-1067-3. URL
http://dl.acm.org/citation.cfm?id=2337223.2337322.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target
vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), pp. 1–10, Beijing, China, July 2015. Association
for Computational Linguistics. URL http://www.aclweb.org/anthology/P15-1001.

Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent
network architectures. In David Blei and Francis Bach (eds.), Proceedings of the 32nd Inter-
national Conference on Machine Learning (ICML-15), pp. 2342–2350. JMLR Workshop and
Conference Proceedings, 2015. URL http://jmlr.org/proceedings/papers/v37/
jozefowicz15.pdf.

H. Khanh Dam, T. Tran, and T. Pham. A deep language model for software code. ArXiv e-prints,

August 2016.

R. Kneser and H. Ney. Improved backing-off for m-gram language modeling. In Acoustics, Speech,
and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pp.
181–184 vol.1, May 1995. doi: 10.1109/ICASSP.1995.479394.

Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, Andrew Senior, Fumin
Wang, and Phil Blunsom. Latent predictor networks for code generation. arXiv preprint
arXiv:1603.06744, 2016.

Chris J Maddison and Daniel Tarlow. Structured generative models of natural source code. In

International Conference on Machine Learning, 2014.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313–330, 1993.

Graham Neubig. Kylm - the kyoto language modeling toolkit. http://www.phontron.com/
[Online; accessed 23-July-

kylm/, 2012. URL http://www.phontron.com/kylm/.
2016].

9

Under review as a conference paper at ICLR 2017

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
networks. In Proceedings of the 30th International Conference on Machine Learning, ICML
2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1310–1318, 2013. URL http://jmlr.org/
proceedings/papers/v28/pascanu13.html.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom.

Reasoning about entailment with neural attention. In ICLR, 2016.

Ke M. Tran, Arianna Bisazza, and Christof Monz. Recurrent memory networks for language
modeling. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, San Diego California,
USA, June 12-17, 2016, pp. 321–331, 2016. URL http://aclweb.org/anthology/N/
N16/N16-1036.pdf.

Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. On the localness of software. In Proceedings
of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 269–280, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635875. URL http://doi.acm.org/10.1145/2635868.2635875.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural

Information Processing Systems, pp. 2692–2700, 2015a.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton.
Grammar as a foreign language. In Advances in Neural Information Processing Systems 28:
Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon-
treal, Quebec, Canada, pp. 2773–2781, 2015b. URL http://papers.nips.cc/paper/
5635-grammar-as-a-foreign-language.

Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the

IEEE, 78(10):1550–1560, 1990.

Martin White, Christopher Vendome, Mario Linares-V´asquez, and Denys Poshyvanyk. Toward
deep learning software repositories. In Proceedings of the 12th Working Conference on Mining
Software Repositories, MSR ’15, pp. 334–345, Piscataway, NJ, USA, 2015. IEEE Press. URL
http://dl.acm.org/citation.cfm?id=2820518.2820559.

Carlo Zapponi. Githut - programming languages and github. http://githut.info/, 2016.

URL http://githut.info/. [Online; accessed 19-August-2016].

10

Under review as a conference paper at ICLR 2017

APPENDIX

Figure 4: Full example of code suggestion with a Sparse Pointer Network. Boldface tokens on the left
show the ﬁrst declaration of an identiﬁer. The middle part visualizes the memory of representations of
these identiﬁers. The right part visualizes the output λ of the controller, which is used for interpolating
between the language model (LM) and the attention of the pointer network (Att).

11

6
1
0
2
 
v
o
N
 
4
2
 
 
]
E
N
.
s
c
[
 
 
1
v
7
0
3
8
0
.
1
1
6
1
:
v
i
X
r
a

Under review as a conference paper at ICLR 2017

LEARNING PYTHON CODE SUGGESTION WITH A
SPARSE POINTER NETWORK

Avishkar Bhoopchand, Tim Rockt¨aschel, Earl Barr & Sebastian Riedel
Department of Computer Science
University College London
avishkar.bhoopchand.15@ucl.ac.uk,
{t.rocktaschel,e.barr,s.riedel}@cs.ucl.ac.uk

ABSTRACT

To enhance developer productivity, all modern integrated development environ-
ments (IDEs) include code suggestion functionality that proposes likely next tokens
at the cursor. While current IDEs work well for statically-typed languages, their re-
liance on type annotations means that they do not provide the same level of support
for dynamic programming languages as for statically-typed languages. Moreover,
suggestion engines in modern IDEs do not propose expressions or multi-statement
idiomatic code. Recent work has shown that language models can improve code
suggestion systems by learning from software repositories. This paper introduces a
neural language model with a sparse pointer network aimed at capturing very long-
range dependencies. We release a large-scale code suggestion corpus of 41M lines
of Python code crawled from GitHub. On this corpus, we found standard neural
language models to perform well at suggesting local phenomena, but struggle to
refer to identiﬁers that are introduced many tokens in the past. By augmenting a
neural language model with a pointer network specialized in referring to predeﬁned
classes of identiﬁers, we obtain a much lower perplexity and a 5 percentage points
increase in accuracy for code suggestion compared to an LSTM baseline. In fact,
this increase in code suggestion accuracy is due to a 13 times more accurate pre-
diction of identiﬁers. Furthermore, a qualitative analysis shows this model indeed
captures interesting long-range dependencies, like referring to a class member
deﬁned over 60 tokens in the past.

1

INTRODUCTION

Integrated development environments (IDEs) are essential tools for programmers. Especially when a
developer is new to a codebase, one of their most useful features is code suggestion: given a piece of
code as context, suggest a likely sequence of next tokens. Typically, the IDE suggests an identiﬁer or
a function call, including API calls. While extensive support exists for statically-typed languages
such as Java, code suggestion for dynamic languages like Python is harder and less well supported
because of the lack of type annotations. Moreover, suggestion engines in modern IDEs do not propose
expressions or multi-statement idiomatic code.

Recently, methods from statistical natural language processing (NLP) have been used to train code
suggestion systems from code usage in large code repositories (Hindle et al., 2012; Allamanis &
Sutton, 2013; Tu et al., 2014). To this end, usually an n-gram language model is trained to score
possible completions. Neural language models for code suggestion (White et al., 2015; Das &
Shah, 2015) have extended this line of work to capture more long-range dependencies. Yet, these
standard neural language models are limited by the so-called hidden state bottleneck, i.e., all context
information has to be stored in a ﬁxed-dimensional internal vector representation. This limitation
restricts such models to local phenomena and does not capture very long-range semantic relationships
like suggesting calling a function that has been deﬁned many tokens before.

To address these issues, we create a large corpus of 41M lines of Python code by using a heuristic for
crawling high-quality code repositories from GitHub. We investigate, for the ﬁrst time, the use of
attention (Bahdanau et al., 2014) for code suggestion and ﬁnd that, despite a substantial improvement

1

Under review as a conference paper at ICLR 2017

in accuracy, it still makes avoidable mistakes. Hence, we introduce a model that leverages long-range
Python dependencies by selectively attending over the introduction of identiﬁers as determined
by examining the Abstract Syntax Tree. The model is a form of pointer network (Vinyals et al.,
2015a), and learns to dynamically choose between syntax-aware pointing for modeling long-range
dependencies and free form generation to deal with local phenomena, based on the current context.

Our contributions are threefold: (i) We release a code suggestion corpus of 41M lines of Python code
crawled from GitHub, (ii) We introduce a sparse attention mechanism that captures very long-range
dependencies for code suggestion of this dynamic programming language efﬁciently, and (iii) We
provide a qualitative analysis demonstrating that this model is indeed able to learn such long-range
dependencies.

2 METHODS

We ﬁrst revisit neural language models, before brieﬂy describing how to extend such a language
model with an attention mechanism. Then we introduce a sparse attention mechanism for a pointer
network that can exploit the Python abstract syntax tree of the current context for code suggestion.

2.1 NEURAL LANGUAGE MODEL

Code suggestion can be approached by a language model that measures the probability of observing
a sequence of tokens in a Python program. For example, for the sequence S = a1, . . . , aN , the joint
probability of S factorizes according to

Pθ(S) = Pθ(a1) ·

Pθ(at | at−1, . . . , a1)

N
(cid:89)

t=2

(1)

(2)

(3)

where the parameters θ are estimated from a training corpus. Given a sequence of Python tokens, we
seek to predict the next M tokens at+1, . . . , at+M that maximize Equation 1

arg max
at+1, ..., at+M

Pθ(a1, . . . , at, at+1, . . . , at+M ).

In this work, we build upon neural language models using Recurrent Neural Networks (RNNs) and
Long Short-Term Memory (LSTM, Hochreiter & Schmidhuber, 1997). This neural language model
estimates the probabilities in Equation 1 using the output vector of an LSTM at time step t (denoted
ht here) according to

Pθ(at = τ | at−1, . . . , a1) =

τ ht + bτ )

exp (vT
τ (cid:48) exp (vT

(cid:80)

τ (cid:48)ht + bτ (cid:48))

where vτ is a parameter vector associated with token τ in the vocabulary.

Neural language models can, in theory, capture long-term dependencies in token sequences through
their internal memory. However, as this internal memory has ﬁxed dimension and can be updated at
every time step, such models often only capture local phenomena. In contrast, we are interested in
very long-range dependencies like referring to a function identiﬁer introduced many tokens in the
past. For example, a function identiﬁer may be introduced at the top of a ﬁle and only used near
the bottom. In the following, we investigate various external memory architectures for neural code
suggestion.

2.2 ATTENTION

A straight-forward approach to capturing long-range dependencies is to use a neural attention mech-
anism (Bahdanau et al., 2014) on the previous K output vectors of the language model. Attention
mechanisms have been successfully applied to sequence-to-sequence tasks such as machine transla-
tion (Bahdanau et al., 2014), question-answering (Hermann et al., 2015), syntactic parsing (Vinyals
et al., 2015b), as well as dual-sequence modeling like recognizing textual entailment (Rockt¨aschel
et al., 2016). The idea is to overcome the hidden-state bottleneck by allowing referral back to previous
output vectors. Recently, these mechanisms were applied to language modelling by Cheng et al.
(2016) and Tran et al. (2016).

2

Under review as a conference paper at ICLR 2017

Formally, an attention mechanism with a ﬁxed memory Mt ∈ Rk×K of K vectors mi ∈ Rk for
i ∈ [1, K], produces an attention distribution αt ∈ RK and context vector ct ∈ Rk at each time
step t according to Equations 4 to 7. Furthermore, W M , W h ∈ Rk×k and w ∈ Rk are trainable
parameters. Finally, note that 1K represents a K-dimensional vector of ones.

Mt = [m1 . . . mK]
Gt = tanh(W M Mt + 1T
αt = softmax(wT Gt)
ct = MtαT
t

K(W hht))

∈ Rk×K
∈ Rk×K
∈ R1×K
∈ Rk

For language modeling, we populate Mt with a ﬁxed window of the previous K LSTM output
vectors. To obtain a distribution over the next token we combine the context vector ct of the attention
mechanism with the output vector ht of the LSTM using a trainable projection matrix W A ∈ Rk×2k.
The resulting ﬁnal output vector nt ∈ Rk encodes the next-word distribution and is projected to the
size of the vocabulary |V |. Subsequently, we apply a softmax to arrive at a probability distribution
yt ∈ R|V | over the next token. This process is presented in Equation 9 where WV ∈ R|V |×k and
bV ∈ R|V | are trainable parameters.

(cid:18)

nt = tanh

W A

(cid:21)(cid:19)

(cid:20)ht
ct

yt = softmax(W V nt + bV )

∈ Rk

∈ R|V |

The problem of the attention mechanism above is that it quickly becomes computationally expensive
for large K. Moreover, attending over many memories can make training hard as a lot of noise is
introduced in early stages of optimization where the LSTM outputs (and thus the memory Mt) are
more or less random. To alleviate these problems we now turn to pointer networks and a simple
heuristic for populating Mt that permits the efﬁcient retrieval of identiﬁers in a large history of
Python code.

2.3 SPARSE POINTER NETWORK

We develop an attention mechanism that provides a ﬁltered view of a large history of Python tokens.
At any given time step, the memory consists of context representations of the previous K identiﬁers
introduced in the history. This allows us to model long-range dependencies found in identiﬁer usage.
For instance, a class identiﬁer may be declared hundreds of lines of code before it is used. Given a
history of Python tokens, we obtain a next-word distribution from a weighed average of the sparse
pointer network for identiﬁer reference and a standard neural language model. The weighting of the
two is determined by a controller.
Formally, at time-step t, the sparse pointer network operates on a memory Mt ∈ Rk×K of only the
K previous identiﬁer representations (e.g. function identiﬁers, class identiﬁers and so on). In addition,
we maintain a vector mt = [id1, . . . , idK] ∈ NK of symbol ids for these identiﬁer representations
(i.e. pointers into the large global vocabulary).

As before, we calculate a context vector ct using the attention mechanism (Equation 7), but on a
memory Mt only containing representations of identiﬁers that were declared in the history. Next, we
obtain a pseudo-sparse distribution over the global vocabulary from

st[i] =

(cid:26)αt[j]
−C

if mt[j] = i
otherwise

it = softmax(st)

∈ R|V |

where −C is a large negative constant (e.g. −1000). In addition, we calculate a next-word distribution
from a standard neural language model

yt = softmax(W V ht + bV )

∈ R|V |

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

3

Under review as a conference paper at ICLR 2017

Figure 1: Sparse pointer network for code suggestion on a Python code snippet, showing the next-
word distributions of the language model and identiﬁer attention and their weighted combination
through λ

and we use a controller to calculate a distribution λt ∈ R2 over the language model and pointer
network for the ﬁnal weighted next-word distribution y∗
(cid:34) ht
xt
ct

∈ R3k

t via

t =

(13)

hλ

(cid:35)

λt = softmax(W λhλ
y∗
t = [yt it] λt

t + bλ)

∈ R2
∈ R|V |

(14)

(15)
Here, xt is the representation of the input token, and W λ ∈ R2×3k and bλ ∈ R2 a trainable
weight matrix and bias respectively. This controller is conditioned on the input, output and context
representations. This means for deciding whether to refer to an identiﬁer or generate from the global
vocabulary, the controller has access to information from the encoded next-word distribution ht of
the standard neural language model, as well as the attention-weighted identiﬁer representations ct
from the current history.

Figure 1 overviews this process. In it, the identiﬁer base_path appears twice, once as an argument
to a function and once as a member of a class (denoted by *). Each appearance has a different id
in the vocabulary and obtains a different probability from the model. In the example, the model
correctly chooses to refer to the member of the class instead of the out-of-scope function argument,
although, from a user point-of-view, the suggestion would be the same in both cases.

3 LARGE-SCALE PYTHON CORPUS

Previous work on code suggestion either focused on statically-typed languages (particularly Java)
or trained on very small corpora. Thus, we decided to collect a new large-scale corpus of the
dynamic programming language Python. According to the programming language popularity website
Pypl (Carbonnelle, 2016), Python is the second most popular language after Java. It is also the
3rd most common language in terms of number of repositories on the open-source code repository
GitHub, after JavaScript and Java (Zapponi, 2016).

We collected a corpus of 41M lines of Python code from GitHub projects. Ideally, we would like this
corpus to only contain high-quality Python code, as our language model learns to suggest code from
how users write code. However, it is difﬁcult to automatically assess what constitutes high-quality
code. Thus, we resort to the heuristic that popular code projects tend to be of good quality, There are

4

Under review as a conference paper at ICLR 2017

Table 1: Python corpus statistics.

Dataset

#Projects

#Files

#Lines

#Tokens Vocabulary Size

Train
Dev
Test

Total

489
179
281

949

118 298
26 466
43 062

26 868 583
5 804 826
8 398 100

88 935 698
18 147 341
30 178 356

187 826

41 071 509

137 261 395

2 323 819

Figure 2: Example of the Python code normalization. Original ﬁle on the left and normalized version
on the right.

two metrics on GitHub that we can use for this purpose, namely stars (similar to bookmarks) and
forks (copies of a repository that allow users to freely experiment with changes without affecting the
original repository). Similar to Allamanis & Sutton (2013) and Allamanis et al. (2014), we select
Python projects with more than 100 stars, sort by the number of forks descending, and take the top
1000 projects. We then removed projects that did not compile with Python3, leaving us with 949
projects. We split the corpus on the project level into train, dev, and test. Table 1 presents the corpus
statistics.

3.1 NORMALIZATION OF IDENTIFIERS

Unsurprisingly, the long tail of words in the vocabulary consists of rare identiﬁers. To improve
generalization, we normalize identiﬁers before feeding the resulting token stream to our models.
That is, we replace every identiﬁer name with an anonymous identiﬁer indicating the identiﬁer group
(class, variable, argument, attribute or function) concatenated with a random number that makes
the identiﬁer unique in its scope. Note that we only replace novel identiﬁers deﬁned within a ﬁle.
Identiﬁer references to external APIs and libraries are left untouched. Consistent with previous corpus
creation for code suggestion (e.g. Khanh Dam et al., 2016; White et al., 2015), we replace numerical
constant tokens with $NUM$, remove comments, reformat the code, and replace tokens appearing
less than ﬁve times with an $OOV$ (out of vocabulary) token.

4 EXPERIMENTS

Although previous work by White et al. (2015) already established that a simple neural language
model outperforms an n-gram model for code suggestion, we include a number of n-gram baselines
to conﬁrm this observation. Speciﬁcally, we use n-gram models for n ∈ {3, 4, 5, 6} with Modiﬁed
Kneser-Ney smoothing (Kneser & Ney, 1995) from the Kyoto Language Modelling Toolkit (Neubig,
2012).

We train the sparse pointer network using mini-batch SGD with a batch size of 30 and truncated
backpropagation through time (Werbos, 1990) with a history of 20 identiﬁer representations. We use

5

Under review as a conference paper at ICLR 2017

Table 2: Perplexity (PP), Accuracy (Acc) and Accuarcy among top 5 predictions (Acc@5).

Model

3-gram
4-gram
5-gram
6-gram

LSTM
LSTM w/ Attention 20
LSTM w/ Attention 50

Sparse Pointer Network

Train PP Dev PP Test PP

Acc [%]

Acc@5 [%]

12.90
7.60
4.52
3.37

9.29
7.30
7.09

6.41

24.19
21.07
19.33
18.73

13.08
11.07
9.83

9.40

All

IDs Other

All

IDs Other

26.90
23.85
21.22
20.17

14.01
11.74
10.05

13.19
13.68
13.90
14.51

57.91
61.30
63.21

9.18

62.97

–
–
–
–

2.1
21.4
30.2

27.3

–
–
–
–

62.8
64.8
65.3

64.9

50.81
51.26
51.49
51.76

76.30
79.32
81.69

82.62

–
–
–
–

4.5
29.9
41.3

43.6

–
–
–
–

82.6
83.7
84.1

84.5

an initial learning rate of 0.7 and decay it by 0.9 after every epoch. As additional baselines, we test
a neural language model with LSTM units with and without attention. For the attention language
models, we experiment with a ﬁxed-window attention memory of the previous 20 and 50 tokens
respectively, and a batch size of 75.

All neural language models were developed in TensorFlow (Abadi et al., 2016) and trained using
cross-entropy loss. While processing a Python source code ﬁle, the last recurrent state of the RNN
is fed as the initial state of the subsequent sequence of the same ﬁle and reset between ﬁles. All
models use an input and hidden size of 200, an LSTM forget gate bias of 1 (Jozefowicz et al., 2015),
gradient norm clipping of 5 (Pascanu et al., 2013), and randomly initialized parameters in the interval
(−0.05, 0.05). As regularizer, we use a dropout of 0.1 on the input representations. Furthermore, we
use a sampled softmax (Jean et al., 2015) with a log-uniform sampling distribution and a sample size
of 1000.

5 RESULTS

We evaluate all models using perplexity (PP), as well as accuracy of the top prediction (Acc) and the
top ﬁve predictions (Acc@5). The results are summarized in Table 2.

We can conﬁrm that for code suggestion neural models outperform n-gram language models by a
large margin. Furthermore, adding attention improves the results substantially (2.3 lower perplexity
and 3.4 percentage points increased accuracy). Interestingly, this increase can be attributed to a
superior prediction of identiﬁers, which increased from an accuracy of 2.1% to 21.4%. An LSTM
with an attention window of 50 gives us the best accuracy for the top prediction. We achieve further
improvements for perplexity and accuracy of the top ﬁve predictions by using a sparse pointer network
that uses a smaller memory of the past 20 identiﬁer representations.

5.1 QUALITATIVE ANALYSIS

Figures 3a-d show a code suggestion example involving an identiﬁer usage. While the LSTM baseline
is uncertain about the next token, we get a sensible prediction by using attention or the sparse pointer
network. The sparse pointer network provides more reasonable alternative suggestions beyond the
correct top suggestion.

Figures 3e-h show the use-case referring to a class attribute declared 67 tokens in the past. Only
the Sparse Pointer Network makes a good suggestion. Furthermore, the attention weights in 3i
demonstrate that this model distinguished attributes from other groups of identiﬁers. We give a full
example of a token-by-token suggestion of the Sparse Pointer Network in Figure 4 in the Appendix.

6 RELATED WORK

Previous code suggestion work using methods from statistical NLP has mostly focused on n-gram
models. Much of this work is inspired by Hindle et al. (2012) who argued that real programs fall

6

Under review as a conference paper at ICLR 2017

(a) Code snippet for referencing
variable.

(b) LSTM Model.

(c) LSTM w/ Attention
50.

(d) Sparse Pointer Net-
work.

(e) Code snippet for referencing class
member.

(f) LSTM Model.

(g) LSTM w/ Attention
50.

(h) Sparse Pointer Net-
work.

(i) Sparse Pointer Network attention over memory of identiﬁer representations.

Figure 3: Code suggestion example involving a reference to a variable (a-d), a long-range dependency
(e-h), and the attention weights of the Sparse Pointer Network (i).

into a much smaller space than the ﬂexibility of programming languages allows. They were able
to capture the repetitiveness and predictable statistical properties of real programs using language
models. Subsequently, Tu et al. (2014) improved upon Hindle et al.’s work by adding a cache
mechanism that allowed them to exploit locality stemming from the specialisation and decoupling of
program modules. Tu et al.’s idea of adding a cache mechanism to the language model is speciﬁcally
designed to exploit the properties of source code, and thus follows the same aim as the sparse attention
mechanism introduced in this paper.

While the majority of preceding work trained on small corpora, Allamanis & Sutton (2013) created a
corpus of 352M lines of Java code which they analysed with n-gram language models. The size of
the corpus allowed them to train a single language model that was effective across multiple different
project domains. White et al. (2015) later demonstrated that neural language models outperform
n-gram models for code suggestion. They compared various n-gram models (up to nine grams),
including Tu et al.’s cache model, with a basic RNN neural language model. Khanh Dam et al.
(2016) compared White et al.’s basic RNN with LSTMs and found that the latter are better at code
suggestion due to their improved ability to learn long-range dependencies found in source code. Our
paper extends this line of work by introducing a sparse attention model that captures even longer
dependencies.

The combination of lagged attention mechanisms with language modelling is inspired by Cheng et al.
(2016) who equipped LSTM cells with a ﬁxed-length memory tape rather than a single memory cell.
They achieved promising results on the standard Penn Treebank benchmark corpus (Marcus et al.,
1993). Similarly, Tran et al. added a memory block to LSTMs for language modelling of English,
German and Italian and outperformed both n-gram and neural language models. Their memory
encompasses representations of all possible words in the vocabulary rather than providing a sparse
view as we do.

An alternative to our purely lexical approach to code suggestion involves the use of probabilistic
context-free grammars (PCFGs) which exploit the formal grammar speciﬁcations and well-deﬁned,
deterministic parsers available for source code. These were used by Allamanis & Sutton (2014)
to extract idiomatic patterns from source code. A weakness of PCFGs is their inability to model
context-dependent rules of programming languages such as that variables need to be declared before

7

Under review as a conference paper at ICLR 2017

being used. Maddison & Tarlow (2014) added context-aware variables to their PCFG model in order
to capture such rules.

Ling et al. (2016) recently used a pointer network to generate code from natural language descriptions.
Our use of a controller for deciding whether to generate from a language model or copy an identiﬁer
using a sparse pointer network is inspired by their latent code predictor. However, their inputs (textual
descriptions) are short whereas code suggestion requires capturing very long-range dependencies that
we addressed by a ﬁltered view on the memory of previous identiﬁer representations.

7 CONCLUSIONS AND FUTURE WORK

In this paper, we investigated neural language models for code suggestion of the dynamically-typed
programming language Python. We released a corpus of 41M lines of Python crawled from GitHub
and compared n-gram, standard neural language models, and attention. By using attention, we
observed an order of magnitude more accurate prediction of identiﬁers. Furthermore, we proposed
a sparse pointer network that can efﬁciently capture long-range dependencies by only operating
on a ﬁltered view of a memory of previous identiﬁer representations. This model achieves the
lowest perplexity and best accuracy among the top ﬁve predictions. The Python corpus and code for
replicating our experiment is released at https://github.com/uclmr/pycodesuggest.

The presented methods were only tested for code suggestion within the same Python ﬁle. We are
interested in scaling the approach to the level of entire code projects and collections thereof, as well
as integrating a trained code suggestion model into an existing IDE. Furthermore, we plan to work on
code completion, i.e., models that provide a likely continuation of a partial token, using character
language models (Graves, 2013).

This work was supported by Microsoft Research through its PhD Scholarship Programme, an Allen
Distinguished Investigator Award, and a Marie Curie Career Integration Award.

ACKNOWLEDGMENTS

REFERENCES

Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat
Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan,
Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zhang. Tensorﬂow: A system for large-scale
machine learning. CoRR, abs/1605.08695, 2016. URL http://arxiv.org/abs/1605.
08695.

Miltiadis Allamanis and Charles Sutton. Mining idioms from source code.

In Proceedings of
the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 472–483, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635901. URL http://doi.acm.org/10.1145/2635868.2635901.

Miltiadis Allamanis and Charles A. Sutton. Mining source code repositories at massive scale
using language modeling. In Thomas Zimmermann, Massimiliano Di Penta, and Sunghun Kim
(eds.), MSR, pp. 207–216. IEEE Computer Society, 2013.
ISBN 978-1-4673-2936-1. URL
http://dblp.uni-trier.de/db/conf/msr/msr2013.html#AllamanisS13a.

Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Learning natural coding
conventions. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations
of Software Engineering, FSE 2014, pp. 281–293, New York, NY, USA, 2014. ACM. ISBN 978-
1-4503-3056-5. doi: 10.1145/2635868.2635883. URL http://doi.acm.org/10.1145/
2635868.2635883.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/abs/
1409.0473.

8

Under review as a conference paper at ICLR 2017

Pierre Carbonnelle. Pypl popularity of programming language. http://pypl.github.io/
PYPL.html, 2016. URL http://pypl.github.io/PYPL.html. [Online; accessed 30-
August-2016].

Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing, pp. 551–561. Association for Computational Linguistics, 2016. URL http://
aclweb.org/anthology/D16-1053.

Subhasis Das and Chinmayee Shah. Contextual code completion using machine learning. 2015.

Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.

URL http://arxiv.org/abs/1308.0850.

Karl Moritz Hermann, Tom´as Kocisk´y, Edward Grefenstette, Lasse Espeholt, Will Kay,
Teaching machines to read and compre-
Mustafa Suleyman,
In Advances in Neural Information Processing Systems 28: Annual Confer-
hend.
ence on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal,
URL http://papers.nips.cc/paper/
Quebec, Canada, pp. 1693–1701, 2015.
5945-teaching-machines-to-read-and-comprehend.

and Phil Blunsom.

Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the naturalness
of software. In Proceedings of the 34th International Conference on Software Engineering, ICSE
’12, pp. 837–847, Piscataway, NJ, USA, 2012. IEEE Press. ISBN 978-1-4673-1067-3. URL
http://dl.acm.org/citation.cfm?id=2337223.2337322.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target
vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), pp. 1–10, Beijing, China, July 2015. Association
for Computational Linguistics. URL http://www.aclweb.org/anthology/P15-1001.

Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent
network architectures. In David Blei and Francis Bach (eds.), Proceedings of the 32nd Inter-
national Conference on Machine Learning (ICML-15), pp. 2342–2350. JMLR Workshop and
Conference Proceedings, 2015. URL http://jmlr.org/proceedings/papers/v37/
jozefowicz15.pdf.

H. Khanh Dam, T. Tran, and T. Pham. A deep language model for software code. ArXiv e-prints,

August 2016.

R. Kneser and H. Ney. Improved backing-off for m-gram language modeling. In Acoustics, Speech,
and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pp.
181–184 vol.1, May 1995. doi: 10.1109/ICASSP.1995.479394.

Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, Andrew Senior, Fumin
Wang, and Phil Blunsom. Latent predictor networks for code generation. arXiv preprint
arXiv:1603.06744, 2016.

Chris J Maddison and Daniel Tarlow. Structured generative models of natural source code. In

International Conference on Machine Learning, 2014.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313–330, 1993.

Graham Neubig. Kylm - the kyoto language modeling toolkit. http://www.phontron.com/
[Online; accessed 23-July-

kylm/, 2012. URL http://www.phontron.com/kylm/.
2016].

9

Under review as a conference paper at ICLR 2017

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
networks. In Proceedings of the 30th International Conference on Machine Learning, ICML
2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1310–1318, 2013. URL http://jmlr.org/
proceedings/papers/v28/pascanu13.html.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom.

Reasoning about entailment with neural attention. In ICLR, 2016.

Ke M. Tran, Arianna Bisazza, and Christof Monz. Recurrent memory networks for language
modeling. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, San Diego California,
USA, June 12-17, 2016, pp. 321–331, 2016. URL http://aclweb.org/anthology/N/
N16/N16-1036.pdf.

Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. On the localness of software. In Proceedings
of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 269–280, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635875. URL http://doi.acm.org/10.1145/2635868.2635875.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural

Information Processing Systems, pp. 2692–2700, 2015a.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton.
Grammar as a foreign language. In Advances in Neural Information Processing Systems 28:
Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon-
treal, Quebec, Canada, pp. 2773–2781, 2015b. URL http://papers.nips.cc/paper/
5635-grammar-as-a-foreign-language.

Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the

IEEE, 78(10):1550–1560, 1990.

Martin White, Christopher Vendome, Mario Linares-V´asquez, and Denys Poshyvanyk. Toward
deep learning software repositories. In Proceedings of the 12th Working Conference on Mining
Software Repositories, MSR ’15, pp. 334–345, Piscataway, NJ, USA, 2015. IEEE Press. URL
http://dl.acm.org/citation.cfm?id=2820518.2820559.

Carlo Zapponi. Githut - programming languages and github. http://githut.info/, 2016.

URL http://githut.info/. [Online; accessed 19-August-2016].

10

Under review as a conference paper at ICLR 2017

APPENDIX

Figure 4: Full example of code suggestion with a Sparse Pointer Network. Boldface tokens on the left
show the ﬁrst declaration of an identiﬁer. The middle part visualizes the memory of representations of
these identiﬁers. The right part visualizes the output λ of the controller, which is used for interpolating
between the language model (LM) and the attention of the pointer network (Att).

11

6
1
0
2
 
v
o
N
 
4
2
 
 
]
E
N
.
s
c
[
 
 
1
v
7
0
3
8
0
.
1
1
6
1
:
v
i
X
r
a

Under review as a conference paper at ICLR 2017

LEARNING PYTHON CODE SUGGESTION WITH A
SPARSE POINTER NETWORK

Avishkar Bhoopchand, Tim Rockt¨aschel, Earl Barr & Sebastian Riedel
Department of Computer Science
University College London
avishkar.bhoopchand.15@ucl.ac.uk,
{t.rocktaschel,e.barr,s.riedel}@cs.ucl.ac.uk

ABSTRACT

To enhance developer productivity, all modern integrated development environ-
ments (IDEs) include code suggestion functionality that proposes likely next tokens
at the cursor. While current IDEs work well for statically-typed languages, their re-
liance on type annotations means that they do not provide the same level of support
for dynamic programming languages as for statically-typed languages. Moreover,
suggestion engines in modern IDEs do not propose expressions or multi-statement
idiomatic code. Recent work has shown that language models can improve code
suggestion systems by learning from software repositories. This paper introduces a
neural language model with a sparse pointer network aimed at capturing very long-
range dependencies. We release a large-scale code suggestion corpus of 41M lines
of Python code crawled from GitHub. On this corpus, we found standard neural
language models to perform well at suggesting local phenomena, but struggle to
refer to identiﬁers that are introduced many tokens in the past. By augmenting a
neural language model with a pointer network specialized in referring to predeﬁned
classes of identiﬁers, we obtain a much lower perplexity and a 5 percentage points
increase in accuracy for code suggestion compared to an LSTM baseline. In fact,
this increase in code suggestion accuracy is due to a 13 times more accurate pre-
diction of identiﬁers. Furthermore, a qualitative analysis shows this model indeed
captures interesting long-range dependencies, like referring to a class member
deﬁned over 60 tokens in the past.

1

INTRODUCTION

Integrated development environments (IDEs) are essential tools for programmers. Especially when a
developer is new to a codebase, one of their most useful features is code suggestion: given a piece of
code as context, suggest a likely sequence of next tokens. Typically, the IDE suggests an identiﬁer or
a function call, including API calls. While extensive support exists for statically-typed languages
such as Java, code suggestion for dynamic languages like Python is harder and less well supported
because of the lack of type annotations. Moreover, suggestion engines in modern IDEs do not propose
expressions or multi-statement idiomatic code.

Recently, methods from statistical natural language processing (NLP) have been used to train code
suggestion systems from code usage in large code repositories (Hindle et al., 2012; Allamanis &
Sutton, 2013; Tu et al., 2014). To this end, usually an n-gram language model is trained to score
possible completions. Neural language models for code suggestion (White et al., 2015; Das &
Shah, 2015) have extended this line of work to capture more long-range dependencies. Yet, these
standard neural language models are limited by the so-called hidden state bottleneck, i.e., all context
information has to be stored in a ﬁxed-dimensional internal vector representation. This limitation
restricts such models to local phenomena and does not capture very long-range semantic relationships
like suggesting calling a function that has been deﬁned many tokens before.

To address these issues, we create a large corpus of 41M lines of Python code by using a heuristic for
crawling high-quality code repositories from GitHub. We investigate, for the ﬁrst time, the use of
attention (Bahdanau et al., 2014) for code suggestion and ﬁnd that, despite a substantial improvement

1

Under review as a conference paper at ICLR 2017

in accuracy, it still makes avoidable mistakes. Hence, we introduce a model that leverages long-range
Python dependencies by selectively attending over the introduction of identiﬁers as determined
by examining the Abstract Syntax Tree. The model is a form of pointer network (Vinyals et al.,
2015a), and learns to dynamically choose between syntax-aware pointing for modeling long-range
dependencies and free form generation to deal with local phenomena, based on the current context.

Our contributions are threefold: (i) We release a code suggestion corpus of 41M lines of Python code
crawled from GitHub, (ii) We introduce a sparse attention mechanism that captures very long-range
dependencies for code suggestion of this dynamic programming language efﬁciently, and (iii) We
provide a qualitative analysis demonstrating that this model is indeed able to learn such long-range
dependencies.

2 METHODS

We ﬁrst revisit neural language models, before brieﬂy describing how to extend such a language
model with an attention mechanism. Then we introduce a sparse attention mechanism for a pointer
network that can exploit the Python abstract syntax tree of the current context for code suggestion.

2.1 NEURAL LANGUAGE MODEL

Code suggestion can be approached by a language model that measures the probability of observing
a sequence of tokens in a Python program. For example, for the sequence S = a1, . . . , aN , the joint
probability of S factorizes according to

Pθ(S) = Pθ(a1) ·

Pθ(at | at−1, . . . , a1)

N
(cid:89)

t=2

(1)

(2)

(3)

where the parameters θ are estimated from a training corpus. Given a sequence of Python tokens, we
seek to predict the next M tokens at+1, . . . , at+M that maximize Equation 1

arg max
at+1, ..., at+M

Pθ(a1, . . . , at, at+1, . . . , at+M ).

In this work, we build upon neural language models using Recurrent Neural Networks (RNNs) and
Long Short-Term Memory (LSTM, Hochreiter & Schmidhuber, 1997). This neural language model
estimates the probabilities in Equation 1 using the output vector of an LSTM at time step t (denoted
ht here) according to

Pθ(at = τ | at−1, . . . , a1) =

τ ht + bτ )

exp (vT
τ (cid:48) exp (vT

(cid:80)

τ (cid:48)ht + bτ (cid:48))

where vτ is a parameter vector associated with token τ in the vocabulary.

Neural language models can, in theory, capture long-term dependencies in token sequences through
their internal memory. However, as this internal memory has ﬁxed dimension and can be updated at
every time step, such models often only capture local phenomena. In contrast, we are interested in
very long-range dependencies like referring to a function identiﬁer introduced many tokens in the
past. For example, a function identiﬁer may be introduced at the top of a ﬁle and only used near
the bottom. In the following, we investigate various external memory architectures for neural code
suggestion.

2.2 ATTENTION

A straight-forward approach to capturing long-range dependencies is to use a neural attention mech-
anism (Bahdanau et al., 2014) on the previous K output vectors of the language model. Attention
mechanisms have been successfully applied to sequence-to-sequence tasks such as machine transla-
tion (Bahdanau et al., 2014), question-answering (Hermann et al., 2015), syntactic parsing (Vinyals
et al., 2015b), as well as dual-sequence modeling like recognizing textual entailment (Rockt¨aschel
et al., 2016). The idea is to overcome the hidden-state bottleneck by allowing referral back to previous
output vectors. Recently, these mechanisms were applied to language modelling by Cheng et al.
(2016) and Tran et al. (2016).

2

Under review as a conference paper at ICLR 2017

Formally, an attention mechanism with a ﬁxed memory Mt ∈ Rk×K of K vectors mi ∈ Rk for
i ∈ [1, K], produces an attention distribution αt ∈ RK and context vector ct ∈ Rk at each time
step t according to Equations 4 to 7. Furthermore, W M , W h ∈ Rk×k and w ∈ Rk are trainable
parameters. Finally, note that 1K represents a K-dimensional vector of ones.

Mt = [m1 . . . mK]
Gt = tanh(W M Mt + 1T
αt = softmax(wT Gt)
ct = MtαT
t

K(W hht))

∈ Rk×K
∈ Rk×K
∈ R1×K
∈ Rk

For language modeling, we populate Mt with a ﬁxed window of the previous K LSTM output
vectors. To obtain a distribution over the next token we combine the context vector ct of the attention
mechanism with the output vector ht of the LSTM using a trainable projection matrix W A ∈ Rk×2k.
The resulting ﬁnal output vector nt ∈ Rk encodes the next-word distribution and is projected to the
size of the vocabulary |V |. Subsequently, we apply a softmax to arrive at a probability distribution
yt ∈ R|V | over the next token. This process is presented in Equation 9 where WV ∈ R|V |×k and
bV ∈ R|V | are trainable parameters.

(cid:18)

nt = tanh

W A

(cid:21)(cid:19)

(cid:20)ht
ct

yt = softmax(W V nt + bV )

∈ Rk

∈ R|V |

The problem of the attention mechanism above is that it quickly becomes computationally expensive
for large K. Moreover, attending over many memories can make training hard as a lot of noise is
introduced in early stages of optimization where the LSTM outputs (and thus the memory Mt) are
more or less random. To alleviate these problems we now turn to pointer networks and a simple
heuristic for populating Mt that permits the efﬁcient retrieval of identiﬁers in a large history of
Python code.

2.3 SPARSE POINTER NETWORK

We develop an attention mechanism that provides a ﬁltered view of a large history of Python tokens.
At any given time step, the memory consists of context representations of the previous K identiﬁers
introduced in the history. This allows us to model long-range dependencies found in identiﬁer usage.
For instance, a class identiﬁer may be declared hundreds of lines of code before it is used. Given a
history of Python tokens, we obtain a next-word distribution from a weighed average of the sparse
pointer network for identiﬁer reference and a standard neural language model. The weighting of the
two is determined by a controller.
Formally, at time-step t, the sparse pointer network operates on a memory Mt ∈ Rk×K of only the
K previous identiﬁer representations (e.g. function identiﬁers, class identiﬁers and so on). In addition,
we maintain a vector mt = [id1, . . . , idK] ∈ NK of symbol ids for these identiﬁer representations
(i.e. pointers into the large global vocabulary).

As before, we calculate a context vector ct using the attention mechanism (Equation 7), but on a
memory Mt only containing representations of identiﬁers that were declared in the history. Next, we
obtain a pseudo-sparse distribution over the global vocabulary from

st[i] =

(cid:26)αt[j]
−C

if mt[j] = i
otherwise

it = softmax(st)

∈ R|V |

where −C is a large negative constant (e.g. −1000). In addition, we calculate a next-word distribution
from a standard neural language model

yt = softmax(W V ht + bV )

∈ R|V |

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

3

Under review as a conference paper at ICLR 2017

Figure 1: Sparse pointer network for code suggestion on a Python code snippet, showing the next-
word distributions of the language model and identiﬁer attention and their weighted combination
through λ

and we use a controller to calculate a distribution λt ∈ R2 over the language model and pointer
network for the ﬁnal weighted next-word distribution y∗
(cid:34) ht
xt
ct

∈ R3k

t via

t =

(13)

hλ

(cid:35)

λt = softmax(W λhλ
y∗
t = [yt it] λt

t + bλ)

∈ R2
∈ R|V |

(14)

(15)
Here, xt is the representation of the input token, and W λ ∈ R2×3k and bλ ∈ R2 a trainable
weight matrix and bias respectively. This controller is conditioned on the input, output and context
representations. This means for deciding whether to refer to an identiﬁer or generate from the global
vocabulary, the controller has access to information from the encoded next-word distribution ht of
the standard neural language model, as well as the attention-weighted identiﬁer representations ct
from the current history.

Figure 1 overviews this process. In it, the identiﬁer base_path appears twice, once as an argument
to a function and once as a member of a class (denoted by *). Each appearance has a different id
in the vocabulary and obtains a different probability from the model. In the example, the model
correctly chooses to refer to the member of the class instead of the out-of-scope function argument,
although, from a user point-of-view, the suggestion would be the same in both cases.

3 LARGE-SCALE PYTHON CORPUS

Previous work on code suggestion either focused on statically-typed languages (particularly Java)
or trained on very small corpora. Thus, we decided to collect a new large-scale corpus of the
dynamic programming language Python. According to the programming language popularity website
Pypl (Carbonnelle, 2016), Python is the second most popular language after Java. It is also the
3rd most common language in terms of number of repositories on the open-source code repository
GitHub, after JavaScript and Java (Zapponi, 2016).

We collected a corpus of 41M lines of Python code from GitHub projects. Ideally, we would like this
corpus to only contain high-quality Python code, as our language model learns to suggest code from
how users write code. However, it is difﬁcult to automatically assess what constitutes high-quality
code. Thus, we resort to the heuristic that popular code projects tend to be of good quality, There are

4

Under review as a conference paper at ICLR 2017

Table 1: Python corpus statistics.

Dataset

#Projects

#Files

#Lines

#Tokens Vocabulary Size

Train
Dev
Test

Total

489
179
281

949

118 298
26 466
43 062

26 868 583
5 804 826
8 398 100

88 935 698
18 147 341
30 178 356

187 826

41 071 509

137 261 395

2 323 819

Figure 2: Example of the Python code normalization. Original ﬁle on the left and normalized version
on the right.

two metrics on GitHub that we can use for this purpose, namely stars (similar to bookmarks) and
forks (copies of a repository that allow users to freely experiment with changes without affecting the
original repository). Similar to Allamanis & Sutton (2013) and Allamanis et al. (2014), we select
Python projects with more than 100 stars, sort by the number of forks descending, and take the top
1000 projects. We then removed projects that did not compile with Python3, leaving us with 949
projects. We split the corpus on the project level into train, dev, and test. Table 1 presents the corpus
statistics.

3.1 NORMALIZATION OF IDENTIFIERS

Unsurprisingly, the long tail of words in the vocabulary consists of rare identiﬁers. To improve
generalization, we normalize identiﬁers before feeding the resulting token stream to our models.
That is, we replace every identiﬁer name with an anonymous identiﬁer indicating the identiﬁer group
(class, variable, argument, attribute or function) concatenated with a random number that makes
the identiﬁer unique in its scope. Note that we only replace novel identiﬁers deﬁned within a ﬁle.
Identiﬁer references to external APIs and libraries are left untouched. Consistent with previous corpus
creation for code suggestion (e.g. Khanh Dam et al., 2016; White et al., 2015), we replace numerical
constant tokens with $NUM$, remove comments, reformat the code, and replace tokens appearing
less than ﬁve times with an $OOV$ (out of vocabulary) token.

4 EXPERIMENTS

Although previous work by White et al. (2015) already established that a simple neural language
model outperforms an n-gram model for code suggestion, we include a number of n-gram baselines
to conﬁrm this observation. Speciﬁcally, we use n-gram models for n ∈ {3, 4, 5, 6} with Modiﬁed
Kneser-Ney smoothing (Kneser & Ney, 1995) from the Kyoto Language Modelling Toolkit (Neubig,
2012).

We train the sparse pointer network using mini-batch SGD with a batch size of 30 and truncated
backpropagation through time (Werbos, 1990) with a history of 20 identiﬁer representations. We use

5

Under review as a conference paper at ICLR 2017

Table 2: Perplexity (PP), Accuracy (Acc) and Accuarcy among top 5 predictions (Acc@5).

Model

3-gram
4-gram
5-gram
6-gram

LSTM
LSTM w/ Attention 20
LSTM w/ Attention 50

Sparse Pointer Network

Train PP Dev PP Test PP

Acc [%]

Acc@5 [%]

12.90
7.60
4.52
3.37

9.29
7.30
7.09

6.41

24.19
21.07
19.33
18.73

13.08
11.07
9.83

9.40

All

IDs Other

All

IDs Other

26.90
23.85
21.22
20.17

14.01
11.74
10.05

13.19
13.68
13.90
14.51

57.91
61.30
63.21

9.18

62.97

–
–
–
–

2.1
21.4
30.2

27.3

–
–
–
–

62.8
64.8
65.3

64.9

50.81
51.26
51.49
51.76

76.30
79.32
81.69

82.62

–
–
–
–

4.5
29.9
41.3

43.6

–
–
–
–

82.6
83.7
84.1

84.5

an initial learning rate of 0.7 and decay it by 0.9 after every epoch. As additional baselines, we test
a neural language model with LSTM units with and without attention. For the attention language
models, we experiment with a ﬁxed-window attention memory of the previous 20 and 50 tokens
respectively, and a batch size of 75.

All neural language models were developed in TensorFlow (Abadi et al., 2016) and trained using
cross-entropy loss. While processing a Python source code ﬁle, the last recurrent state of the RNN
is fed as the initial state of the subsequent sequence of the same ﬁle and reset between ﬁles. All
models use an input and hidden size of 200, an LSTM forget gate bias of 1 (Jozefowicz et al., 2015),
gradient norm clipping of 5 (Pascanu et al., 2013), and randomly initialized parameters in the interval
(−0.05, 0.05). As regularizer, we use a dropout of 0.1 on the input representations. Furthermore, we
use a sampled softmax (Jean et al., 2015) with a log-uniform sampling distribution and a sample size
of 1000.

5 RESULTS

We evaluate all models using perplexity (PP), as well as accuracy of the top prediction (Acc) and the
top ﬁve predictions (Acc@5). The results are summarized in Table 2.

We can conﬁrm that for code suggestion neural models outperform n-gram language models by a
large margin. Furthermore, adding attention improves the results substantially (2.3 lower perplexity
and 3.4 percentage points increased accuracy). Interestingly, this increase can be attributed to a
superior prediction of identiﬁers, which increased from an accuracy of 2.1% to 21.4%. An LSTM
with an attention window of 50 gives us the best accuracy for the top prediction. We achieve further
improvements for perplexity and accuracy of the top ﬁve predictions by using a sparse pointer network
that uses a smaller memory of the past 20 identiﬁer representations.

5.1 QUALITATIVE ANALYSIS

Figures 3a-d show a code suggestion example involving an identiﬁer usage. While the LSTM baseline
is uncertain about the next token, we get a sensible prediction by using attention or the sparse pointer
network. The sparse pointer network provides more reasonable alternative suggestions beyond the
correct top suggestion.

Figures 3e-h show the use-case referring to a class attribute declared 67 tokens in the past. Only
the Sparse Pointer Network makes a good suggestion. Furthermore, the attention weights in 3i
demonstrate that this model distinguished attributes from other groups of identiﬁers. We give a full
example of a token-by-token suggestion of the Sparse Pointer Network in Figure 4 in the Appendix.

6 RELATED WORK

Previous code suggestion work using methods from statistical NLP has mostly focused on n-gram
models. Much of this work is inspired by Hindle et al. (2012) who argued that real programs fall

6

Under review as a conference paper at ICLR 2017

(a) Code snippet for referencing
variable.

(b) LSTM Model.

(c) LSTM w/ Attention
50.

(d) Sparse Pointer Net-
work.

(e) Code snippet for referencing class
member.

(f) LSTM Model.

(g) LSTM w/ Attention
50.

(h) Sparse Pointer Net-
work.

(i) Sparse Pointer Network attention over memory of identiﬁer representations.

Figure 3: Code suggestion example involving a reference to a variable (a-d), a long-range dependency
(e-h), and the attention weights of the Sparse Pointer Network (i).

into a much smaller space than the ﬂexibility of programming languages allows. They were able
to capture the repetitiveness and predictable statistical properties of real programs using language
models. Subsequently, Tu et al. (2014) improved upon Hindle et al.’s work by adding a cache
mechanism that allowed them to exploit locality stemming from the specialisation and decoupling of
program modules. Tu et al.’s idea of adding a cache mechanism to the language model is speciﬁcally
designed to exploit the properties of source code, and thus follows the same aim as the sparse attention
mechanism introduced in this paper.

While the majority of preceding work trained on small corpora, Allamanis & Sutton (2013) created a
corpus of 352M lines of Java code which they analysed with n-gram language models. The size of
the corpus allowed them to train a single language model that was effective across multiple different
project domains. White et al. (2015) later demonstrated that neural language models outperform
n-gram models for code suggestion. They compared various n-gram models (up to nine grams),
including Tu et al.’s cache model, with a basic RNN neural language model. Khanh Dam et al.
(2016) compared White et al.’s basic RNN with LSTMs and found that the latter are better at code
suggestion due to their improved ability to learn long-range dependencies found in source code. Our
paper extends this line of work by introducing a sparse attention model that captures even longer
dependencies.

The combination of lagged attention mechanisms with language modelling is inspired by Cheng et al.
(2016) who equipped LSTM cells with a ﬁxed-length memory tape rather than a single memory cell.
They achieved promising results on the standard Penn Treebank benchmark corpus (Marcus et al.,
1993). Similarly, Tran et al. added a memory block to LSTMs for language modelling of English,
German and Italian and outperformed both n-gram and neural language models. Their memory
encompasses representations of all possible words in the vocabulary rather than providing a sparse
view as we do.

An alternative to our purely lexical approach to code suggestion involves the use of probabilistic
context-free grammars (PCFGs) which exploit the formal grammar speciﬁcations and well-deﬁned,
deterministic parsers available for source code. These were used by Allamanis & Sutton (2014)
to extract idiomatic patterns from source code. A weakness of PCFGs is their inability to model
context-dependent rules of programming languages such as that variables need to be declared before

7

Under review as a conference paper at ICLR 2017

being used. Maddison & Tarlow (2014) added context-aware variables to their PCFG model in order
to capture such rules.

Ling et al. (2016) recently used a pointer network to generate code from natural language descriptions.
Our use of a controller for deciding whether to generate from a language model or copy an identiﬁer
using a sparse pointer network is inspired by their latent code predictor. However, their inputs (textual
descriptions) are short whereas code suggestion requires capturing very long-range dependencies that
we addressed by a ﬁltered view on the memory of previous identiﬁer representations.

7 CONCLUSIONS AND FUTURE WORK

In this paper, we investigated neural language models for code suggestion of the dynamically-typed
programming language Python. We released a corpus of 41M lines of Python crawled from GitHub
and compared n-gram, standard neural language models, and attention. By using attention, we
observed an order of magnitude more accurate prediction of identiﬁers. Furthermore, we proposed
a sparse pointer network that can efﬁciently capture long-range dependencies by only operating
on a ﬁltered view of a memory of previous identiﬁer representations. This model achieves the
lowest perplexity and best accuracy among the top ﬁve predictions. The Python corpus and code for
replicating our experiment is released at https://github.com/uclmr/pycodesuggest.

The presented methods were only tested for code suggestion within the same Python ﬁle. We are
interested in scaling the approach to the level of entire code projects and collections thereof, as well
as integrating a trained code suggestion model into an existing IDE. Furthermore, we plan to work on
code completion, i.e., models that provide a likely continuation of a partial token, using character
language models (Graves, 2013).

This work was supported by Microsoft Research through its PhD Scholarship Programme, an Allen
Distinguished Investigator Award, and a Marie Curie Career Integration Award.

ACKNOWLEDGMENTS

REFERENCES

Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat
Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan,
Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zhang. Tensorﬂow: A system for large-scale
machine learning. CoRR, abs/1605.08695, 2016. URL http://arxiv.org/abs/1605.
08695.

Miltiadis Allamanis and Charles Sutton. Mining idioms from source code.

In Proceedings of
the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 472–483, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635901. URL http://doi.acm.org/10.1145/2635868.2635901.

Miltiadis Allamanis and Charles A. Sutton. Mining source code repositories at massive scale
using language modeling. In Thomas Zimmermann, Massimiliano Di Penta, and Sunghun Kim
(eds.), MSR, pp. 207–216. IEEE Computer Society, 2013.
ISBN 978-1-4673-2936-1. URL
http://dblp.uni-trier.de/db/conf/msr/msr2013.html#AllamanisS13a.

Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Learning natural coding
conventions. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations
of Software Engineering, FSE 2014, pp. 281–293, New York, NY, USA, 2014. ACM. ISBN 978-
1-4503-3056-5. doi: 10.1145/2635868.2635883. URL http://doi.acm.org/10.1145/
2635868.2635883.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/abs/
1409.0473.

8

Under review as a conference paper at ICLR 2017

Pierre Carbonnelle. Pypl popularity of programming language. http://pypl.github.io/
PYPL.html, 2016. URL http://pypl.github.io/PYPL.html. [Online; accessed 30-
August-2016].

Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing, pp. 551–561. Association for Computational Linguistics, 2016. URL http://
aclweb.org/anthology/D16-1053.

Subhasis Das and Chinmayee Shah. Contextual code completion using machine learning. 2015.

Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.

URL http://arxiv.org/abs/1308.0850.

Karl Moritz Hermann, Tom´as Kocisk´y, Edward Grefenstette, Lasse Espeholt, Will Kay,
Teaching machines to read and compre-
Mustafa Suleyman,
In Advances in Neural Information Processing Systems 28: Annual Confer-
hend.
ence on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal,
URL http://papers.nips.cc/paper/
Quebec, Canada, pp. 1693–1701, 2015.
5945-teaching-machines-to-read-and-comprehend.

and Phil Blunsom.

Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the naturalness
of software. In Proceedings of the 34th International Conference on Software Engineering, ICSE
’12, pp. 837–847, Piscataway, NJ, USA, 2012. IEEE Press. ISBN 978-1-4673-1067-3. URL
http://dl.acm.org/citation.cfm?id=2337223.2337322.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target
vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), pp. 1–10, Beijing, China, July 2015. Association
for Computational Linguistics. URL http://www.aclweb.org/anthology/P15-1001.

Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent
network architectures. In David Blei and Francis Bach (eds.), Proceedings of the 32nd Inter-
national Conference on Machine Learning (ICML-15), pp. 2342–2350. JMLR Workshop and
Conference Proceedings, 2015. URL http://jmlr.org/proceedings/papers/v37/
jozefowicz15.pdf.

H. Khanh Dam, T. Tran, and T. Pham. A deep language model for software code. ArXiv e-prints,

August 2016.

R. Kneser and H. Ney. Improved backing-off for m-gram language modeling. In Acoustics, Speech,
and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pp.
181–184 vol.1, May 1995. doi: 10.1109/ICASSP.1995.479394.

Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, Andrew Senior, Fumin
Wang, and Phil Blunsom. Latent predictor networks for code generation. arXiv preprint
arXiv:1603.06744, 2016.

Chris J Maddison and Daniel Tarlow. Structured generative models of natural source code. In

International Conference on Machine Learning, 2014.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313–330, 1993.

Graham Neubig. Kylm - the kyoto language modeling toolkit. http://www.phontron.com/
[Online; accessed 23-July-

kylm/, 2012. URL http://www.phontron.com/kylm/.
2016].

9

Under review as a conference paper at ICLR 2017

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
networks. In Proceedings of the 30th International Conference on Machine Learning, ICML
2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1310–1318, 2013. URL http://jmlr.org/
proceedings/papers/v28/pascanu13.html.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom.

Reasoning about entailment with neural attention. In ICLR, 2016.

Ke M. Tran, Arianna Bisazza, and Christof Monz. Recurrent memory networks for language
modeling. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, San Diego California,
USA, June 12-17, 2016, pp. 321–331, 2016. URL http://aclweb.org/anthology/N/
N16/N16-1036.pdf.

Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. On the localness of software. In Proceedings
of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 269–280, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635875. URL http://doi.acm.org/10.1145/2635868.2635875.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural

Information Processing Systems, pp. 2692–2700, 2015a.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton.
Grammar as a foreign language. In Advances in Neural Information Processing Systems 28:
Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon-
treal, Quebec, Canada, pp. 2773–2781, 2015b. URL http://papers.nips.cc/paper/
5635-grammar-as-a-foreign-language.

Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the

IEEE, 78(10):1550–1560, 1990.

Martin White, Christopher Vendome, Mario Linares-V´asquez, and Denys Poshyvanyk. Toward
deep learning software repositories. In Proceedings of the 12th Working Conference on Mining
Software Repositories, MSR ’15, pp. 334–345, Piscataway, NJ, USA, 2015. IEEE Press. URL
http://dl.acm.org/citation.cfm?id=2820518.2820559.

Carlo Zapponi. Githut - programming languages and github. http://githut.info/, 2016.

URL http://githut.info/. [Online; accessed 19-August-2016].

10

Under review as a conference paper at ICLR 2017

APPENDIX

Figure 4: Full example of code suggestion with a Sparse Pointer Network. Boldface tokens on the left
show the ﬁrst declaration of an identiﬁer. The middle part visualizes the memory of representations of
these identiﬁers. The right part visualizes the output λ of the controller, which is used for interpolating
between the language model (LM) and the attention of the pointer network (Att).

11

6
1
0
2
 
v
o
N
 
4
2
 
 
]
E
N
.
s
c
[
 
 
1
v
7
0
3
8
0
.
1
1
6
1
:
v
i
X
r
a

Under review as a conference paper at ICLR 2017

LEARNING PYTHON CODE SUGGESTION WITH A
SPARSE POINTER NETWORK

Avishkar Bhoopchand, Tim Rockt¨aschel, Earl Barr & Sebastian Riedel
Department of Computer Science
University College London
avishkar.bhoopchand.15@ucl.ac.uk,
{t.rocktaschel,e.barr,s.riedel}@cs.ucl.ac.uk

ABSTRACT

To enhance developer productivity, all modern integrated development environ-
ments (IDEs) include code suggestion functionality that proposes likely next tokens
at the cursor. While current IDEs work well for statically-typed languages, their re-
liance on type annotations means that they do not provide the same level of support
for dynamic programming languages as for statically-typed languages. Moreover,
suggestion engines in modern IDEs do not propose expressions or multi-statement
idiomatic code. Recent work has shown that language models can improve code
suggestion systems by learning from software repositories. This paper introduces a
neural language model with a sparse pointer network aimed at capturing very long-
range dependencies. We release a large-scale code suggestion corpus of 41M lines
of Python code crawled from GitHub. On this corpus, we found standard neural
language models to perform well at suggesting local phenomena, but struggle to
refer to identiﬁers that are introduced many tokens in the past. By augmenting a
neural language model with a pointer network specialized in referring to predeﬁned
classes of identiﬁers, we obtain a much lower perplexity and a 5 percentage points
increase in accuracy for code suggestion compared to an LSTM baseline. In fact,
this increase in code suggestion accuracy is due to a 13 times more accurate pre-
diction of identiﬁers. Furthermore, a qualitative analysis shows this model indeed
captures interesting long-range dependencies, like referring to a class member
deﬁned over 60 tokens in the past.

1

INTRODUCTION

Integrated development environments (IDEs) are essential tools for programmers. Especially when a
developer is new to a codebase, one of their most useful features is code suggestion: given a piece of
code as context, suggest a likely sequence of next tokens. Typically, the IDE suggests an identiﬁer or
a function call, including API calls. While extensive support exists for statically-typed languages
such as Java, code suggestion for dynamic languages like Python is harder and less well supported
because of the lack of type annotations. Moreover, suggestion engines in modern IDEs do not propose
expressions or multi-statement idiomatic code.

Recently, methods from statistical natural language processing (NLP) have been used to train code
suggestion systems from code usage in large code repositories (Hindle et al., 2012; Allamanis &
Sutton, 2013; Tu et al., 2014). To this end, usually an n-gram language model is trained to score
possible completions. Neural language models for code suggestion (White et al., 2015; Das &
Shah, 2015) have extended this line of work to capture more long-range dependencies. Yet, these
standard neural language models are limited by the so-called hidden state bottleneck, i.e., all context
information has to be stored in a ﬁxed-dimensional internal vector representation. This limitation
restricts such models to local phenomena and does not capture very long-range semantic relationships
like suggesting calling a function that has been deﬁned many tokens before.

To address these issues, we create a large corpus of 41M lines of Python code by using a heuristic for
crawling high-quality code repositories from GitHub. We investigate, for the ﬁrst time, the use of
attention (Bahdanau et al., 2014) for code suggestion and ﬁnd that, despite a substantial improvement

1

Under review as a conference paper at ICLR 2017

in accuracy, it still makes avoidable mistakes. Hence, we introduce a model that leverages long-range
Python dependencies by selectively attending over the introduction of identiﬁers as determined
by examining the Abstract Syntax Tree. The model is a form of pointer network (Vinyals et al.,
2015a), and learns to dynamically choose between syntax-aware pointing for modeling long-range
dependencies and free form generation to deal with local phenomena, based on the current context.

Our contributions are threefold: (i) We release a code suggestion corpus of 41M lines of Python code
crawled from GitHub, (ii) We introduce a sparse attention mechanism that captures very long-range
dependencies for code suggestion of this dynamic programming language efﬁciently, and (iii) We
provide a qualitative analysis demonstrating that this model is indeed able to learn such long-range
dependencies.

2 METHODS

We ﬁrst revisit neural language models, before brieﬂy describing how to extend such a language
model with an attention mechanism. Then we introduce a sparse attention mechanism for a pointer
network that can exploit the Python abstract syntax tree of the current context for code suggestion.

2.1 NEURAL LANGUAGE MODEL

Code suggestion can be approached by a language model that measures the probability of observing
a sequence of tokens in a Python program. For example, for the sequence S = a1, . . . , aN , the joint
probability of S factorizes according to

Pθ(S) = Pθ(a1) ·

Pθ(at | at−1, . . . , a1)

N
(cid:89)

t=2

(1)

(2)

(3)

where the parameters θ are estimated from a training corpus. Given a sequence of Python tokens, we
seek to predict the next M tokens at+1, . . . , at+M that maximize Equation 1

arg max
at+1, ..., at+M

Pθ(a1, . . . , at, at+1, . . . , at+M ).

In this work, we build upon neural language models using Recurrent Neural Networks (RNNs) and
Long Short-Term Memory (LSTM, Hochreiter & Schmidhuber, 1997). This neural language model
estimates the probabilities in Equation 1 using the output vector of an LSTM at time step t (denoted
ht here) according to

Pθ(at = τ | at−1, . . . , a1) =

τ ht + bτ )

exp (vT
τ (cid:48) exp (vT

(cid:80)

τ (cid:48)ht + bτ (cid:48))

where vτ is a parameter vector associated with token τ in the vocabulary.

Neural language models can, in theory, capture long-term dependencies in token sequences through
their internal memory. However, as this internal memory has ﬁxed dimension and can be updated at
every time step, such models often only capture local phenomena. In contrast, we are interested in
very long-range dependencies like referring to a function identiﬁer introduced many tokens in the
past. For example, a function identiﬁer may be introduced at the top of a ﬁle and only used near
the bottom. In the following, we investigate various external memory architectures for neural code
suggestion.

2.2 ATTENTION

A straight-forward approach to capturing long-range dependencies is to use a neural attention mech-
anism (Bahdanau et al., 2014) on the previous K output vectors of the language model. Attention
mechanisms have been successfully applied to sequence-to-sequence tasks such as machine transla-
tion (Bahdanau et al., 2014), question-answering (Hermann et al., 2015), syntactic parsing (Vinyals
et al., 2015b), as well as dual-sequence modeling like recognizing textual entailment (Rockt¨aschel
et al., 2016). The idea is to overcome the hidden-state bottleneck by allowing referral back to previous
output vectors. Recently, these mechanisms were applied to language modelling by Cheng et al.
(2016) and Tran et al. (2016).

2

Under review as a conference paper at ICLR 2017

Formally, an attention mechanism with a ﬁxed memory Mt ∈ Rk×K of K vectors mi ∈ Rk for
i ∈ [1, K], produces an attention distribution αt ∈ RK and context vector ct ∈ Rk at each time
step t according to Equations 4 to 7. Furthermore, W M , W h ∈ Rk×k and w ∈ Rk are trainable
parameters. Finally, note that 1K represents a K-dimensional vector of ones.

Mt = [m1 . . . mK]
Gt = tanh(W M Mt + 1T
αt = softmax(wT Gt)
ct = MtαT
t

K(W hht))

∈ Rk×K
∈ Rk×K
∈ R1×K
∈ Rk

For language modeling, we populate Mt with a ﬁxed window of the previous K LSTM output
vectors. To obtain a distribution over the next token we combine the context vector ct of the attention
mechanism with the output vector ht of the LSTM using a trainable projection matrix W A ∈ Rk×2k.
The resulting ﬁnal output vector nt ∈ Rk encodes the next-word distribution and is projected to the
size of the vocabulary |V |. Subsequently, we apply a softmax to arrive at a probability distribution
yt ∈ R|V | over the next token. This process is presented in Equation 9 where WV ∈ R|V |×k and
bV ∈ R|V | are trainable parameters.

(cid:18)

nt = tanh

W A

(cid:21)(cid:19)

(cid:20)ht
ct

yt = softmax(W V nt + bV )

∈ Rk

∈ R|V |

The problem of the attention mechanism above is that it quickly becomes computationally expensive
for large K. Moreover, attending over many memories can make training hard as a lot of noise is
introduced in early stages of optimization where the LSTM outputs (and thus the memory Mt) are
more or less random. To alleviate these problems we now turn to pointer networks and a simple
heuristic for populating Mt that permits the efﬁcient retrieval of identiﬁers in a large history of
Python code.

2.3 SPARSE POINTER NETWORK

We develop an attention mechanism that provides a ﬁltered view of a large history of Python tokens.
At any given time step, the memory consists of context representations of the previous K identiﬁers
introduced in the history. This allows us to model long-range dependencies found in identiﬁer usage.
For instance, a class identiﬁer may be declared hundreds of lines of code before it is used. Given a
history of Python tokens, we obtain a next-word distribution from a weighed average of the sparse
pointer network for identiﬁer reference and a standard neural language model. The weighting of the
two is determined by a controller.
Formally, at time-step t, the sparse pointer network operates on a memory Mt ∈ Rk×K of only the
K previous identiﬁer representations (e.g. function identiﬁers, class identiﬁers and so on). In addition,
we maintain a vector mt = [id1, . . . , idK] ∈ NK of symbol ids for these identiﬁer representations
(i.e. pointers into the large global vocabulary).

As before, we calculate a context vector ct using the attention mechanism (Equation 7), but on a
memory Mt only containing representations of identiﬁers that were declared in the history. Next, we
obtain a pseudo-sparse distribution over the global vocabulary from

st[i] =

(cid:26)αt[j]
−C

if mt[j] = i
otherwise

it = softmax(st)

∈ R|V |

where −C is a large negative constant (e.g. −1000). In addition, we calculate a next-word distribution
from a standard neural language model

yt = softmax(W V ht + bV )

∈ R|V |

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

3

Under review as a conference paper at ICLR 2017

Figure 1: Sparse pointer network for code suggestion on a Python code snippet, showing the next-
word distributions of the language model and identiﬁer attention and their weighted combination
through λ

and we use a controller to calculate a distribution λt ∈ R2 over the language model and pointer
network for the ﬁnal weighted next-word distribution y∗
(cid:34) ht
xt
ct

∈ R3k

t via

t =

(13)

hλ

(cid:35)

λt = softmax(W λhλ
y∗
t = [yt it] λt

t + bλ)

∈ R2
∈ R|V |

(14)

(15)
Here, xt is the representation of the input token, and W λ ∈ R2×3k and bλ ∈ R2 a trainable
weight matrix and bias respectively. This controller is conditioned on the input, output and context
representations. This means for deciding whether to refer to an identiﬁer or generate from the global
vocabulary, the controller has access to information from the encoded next-word distribution ht of
the standard neural language model, as well as the attention-weighted identiﬁer representations ct
from the current history.

Figure 1 overviews this process. In it, the identiﬁer base_path appears twice, once as an argument
to a function and once as a member of a class (denoted by *). Each appearance has a different id
in the vocabulary and obtains a different probability from the model. In the example, the model
correctly chooses to refer to the member of the class instead of the out-of-scope function argument,
although, from a user point-of-view, the suggestion would be the same in both cases.

3 LARGE-SCALE PYTHON CORPUS

Previous work on code suggestion either focused on statically-typed languages (particularly Java)
or trained on very small corpora. Thus, we decided to collect a new large-scale corpus of the
dynamic programming language Python. According to the programming language popularity website
Pypl (Carbonnelle, 2016), Python is the second most popular language after Java. It is also the
3rd most common language in terms of number of repositories on the open-source code repository
GitHub, after JavaScript and Java (Zapponi, 2016).

We collected a corpus of 41M lines of Python code from GitHub projects. Ideally, we would like this
corpus to only contain high-quality Python code, as our language model learns to suggest code from
how users write code. However, it is difﬁcult to automatically assess what constitutes high-quality
code. Thus, we resort to the heuristic that popular code projects tend to be of good quality, There are

4

Under review as a conference paper at ICLR 2017

Table 1: Python corpus statistics.

Dataset

#Projects

#Files

#Lines

#Tokens Vocabulary Size

Train
Dev
Test

Total

489
179
281

949

118 298
26 466
43 062

26 868 583
5 804 826
8 398 100

88 935 698
18 147 341
30 178 356

187 826

41 071 509

137 261 395

2 323 819

Figure 2: Example of the Python code normalization. Original ﬁle on the left and normalized version
on the right.

two metrics on GitHub that we can use for this purpose, namely stars (similar to bookmarks) and
forks (copies of a repository that allow users to freely experiment with changes without affecting the
original repository). Similar to Allamanis & Sutton (2013) and Allamanis et al. (2014), we select
Python projects with more than 100 stars, sort by the number of forks descending, and take the top
1000 projects. We then removed projects that did not compile with Python3, leaving us with 949
projects. We split the corpus on the project level into train, dev, and test. Table 1 presents the corpus
statistics.

3.1 NORMALIZATION OF IDENTIFIERS

Unsurprisingly, the long tail of words in the vocabulary consists of rare identiﬁers. To improve
generalization, we normalize identiﬁers before feeding the resulting token stream to our models.
That is, we replace every identiﬁer name with an anonymous identiﬁer indicating the identiﬁer group
(class, variable, argument, attribute or function) concatenated with a random number that makes
the identiﬁer unique in its scope. Note that we only replace novel identiﬁers deﬁned within a ﬁle.
Identiﬁer references to external APIs and libraries are left untouched. Consistent with previous corpus
creation for code suggestion (e.g. Khanh Dam et al., 2016; White et al., 2015), we replace numerical
constant tokens with $NUM$, remove comments, reformat the code, and replace tokens appearing
less than ﬁve times with an $OOV$ (out of vocabulary) token.

4 EXPERIMENTS

Although previous work by White et al. (2015) already established that a simple neural language
model outperforms an n-gram model for code suggestion, we include a number of n-gram baselines
to conﬁrm this observation. Speciﬁcally, we use n-gram models for n ∈ {3, 4, 5, 6} with Modiﬁed
Kneser-Ney smoothing (Kneser & Ney, 1995) from the Kyoto Language Modelling Toolkit (Neubig,
2012).

We train the sparse pointer network using mini-batch SGD with a batch size of 30 and truncated
backpropagation through time (Werbos, 1990) with a history of 20 identiﬁer representations. We use

5

Under review as a conference paper at ICLR 2017

Table 2: Perplexity (PP), Accuracy (Acc) and Accuarcy among top 5 predictions (Acc@5).

Model

3-gram
4-gram
5-gram
6-gram

LSTM
LSTM w/ Attention 20
LSTM w/ Attention 50

Sparse Pointer Network

Train PP Dev PP Test PP

Acc [%]

Acc@5 [%]

12.90
7.60
4.52
3.37

9.29
7.30
7.09

6.41

24.19
21.07
19.33
18.73

13.08
11.07
9.83

9.40

All

IDs Other

All

IDs Other

26.90
23.85
21.22
20.17

14.01
11.74
10.05

13.19
13.68
13.90
14.51

57.91
61.30
63.21

9.18

62.97

–
–
–
–

2.1
21.4
30.2

27.3

–
–
–
–

62.8
64.8
65.3

64.9

50.81
51.26
51.49
51.76

76.30
79.32
81.69

82.62

–
–
–
–

4.5
29.9
41.3

43.6

–
–
–
–

82.6
83.7
84.1

84.5

an initial learning rate of 0.7 and decay it by 0.9 after every epoch. As additional baselines, we test
a neural language model with LSTM units with and without attention. For the attention language
models, we experiment with a ﬁxed-window attention memory of the previous 20 and 50 tokens
respectively, and a batch size of 75.

All neural language models were developed in TensorFlow (Abadi et al., 2016) and trained using
cross-entropy loss. While processing a Python source code ﬁle, the last recurrent state of the RNN
is fed as the initial state of the subsequent sequence of the same ﬁle and reset between ﬁles. All
models use an input and hidden size of 200, an LSTM forget gate bias of 1 (Jozefowicz et al., 2015),
gradient norm clipping of 5 (Pascanu et al., 2013), and randomly initialized parameters in the interval
(−0.05, 0.05). As regularizer, we use a dropout of 0.1 on the input representations. Furthermore, we
use a sampled softmax (Jean et al., 2015) with a log-uniform sampling distribution and a sample size
of 1000.

5 RESULTS

We evaluate all models using perplexity (PP), as well as accuracy of the top prediction (Acc) and the
top ﬁve predictions (Acc@5). The results are summarized in Table 2.

We can conﬁrm that for code suggestion neural models outperform n-gram language models by a
large margin. Furthermore, adding attention improves the results substantially (2.3 lower perplexity
and 3.4 percentage points increased accuracy). Interestingly, this increase can be attributed to a
superior prediction of identiﬁers, which increased from an accuracy of 2.1% to 21.4%. An LSTM
with an attention window of 50 gives us the best accuracy for the top prediction. We achieve further
improvements for perplexity and accuracy of the top ﬁve predictions by using a sparse pointer network
that uses a smaller memory of the past 20 identiﬁer representations.

5.1 QUALITATIVE ANALYSIS

Figures 3a-d show a code suggestion example involving an identiﬁer usage. While the LSTM baseline
is uncertain about the next token, we get a sensible prediction by using attention or the sparse pointer
network. The sparse pointer network provides more reasonable alternative suggestions beyond the
correct top suggestion.

Figures 3e-h show the use-case referring to a class attribute declared 67 tokens in the past. Only
the Sparse Pointer Network makes a good suggestion. Furthermore, the attention weights in 3i
demonstrate that this model distinguished attributes from other groups of identiﬁers. We give a full
example of a token-by-token suggestion of the Sparse Pointer Network in Figure 4 in the Appendix.

6 RELATED WORK

Previous code suggestion work using methods from statistical NLP has mostly focused on n-gram
models. Much of this work is inspired by Hindle et al. (2012) who argued that real programs fall

6

Under review as a conference paper at ICLR 2017

(a) Code snippet for referencing
variable.

(b) LSTM Model.

(c) LSTM w/ Attention
50.

(d) Sparse Pointer Net-
work.

(e) Code snippet for referencing class
member.

(f) LSTM Model.

(g) LSTM w/ Attention
50.

(h) Sparse Pointer Net-
work.

(i) Sparse Pointer Network attention over memory of identiﬁer representations.

Figure 3: Code suggestion example involving a reference to a variable (a-d), a long-range dependency
(e-h), and the attention weights of the Sparse Pointer Network (i).

into a much smaller space than the ﬂexibility of programming languages allows. They were able
to capture the repetitiveness and predictable statistical properties of real programs using language
models. Subsequently, Tu et al. (2014) improved upon Hindle et al.’s work by adding a cache
mechanism that allowed them to exploit locality stemming from the specialisation and decoupling of
program modules. Tu et al.’s idea of adding a cache mechanism to the language model is speciﬁcally
designed to exploit the properties of source code, and thus follows the same aim as the sparse attention
mechanism introduced in this paper.

While the majority of preceding work trained on small corpora, Allamanis & Sutton (2013) created a
corpus of 352M lines of Java code which they analysed with n-gram language models. The size of
the corpus allowed them to train a single language model that was effective across multiple different
project domains. White et al. (2015) later demonstrated that neural language models outperform
n-gram models for code suggestion. They compared various n-gram models (up to nine grams),
including Tu et al.’s cache model, with a basic RNN neural language model. Khanh Dam et al.
(2016) compared White et al.’s basic RNN with LSTMs and found that the latter are better at code
suggestion due to their improved ability to learn long-range dependencies found in source code. Our
paper extends this line of work by introducing a sparse attention model that captures even longer
dependencies.

The combination of lagged attention mechanisms with language modelling is inspired by Cheng et al.
(2016) who equipped LSTM cells with a ﬁxed-length memory tape rather than a single memory cell.
They achieved promising results on the standard Penn Treebank benchmark corpus (Marcus et al.,
1993). Similarly, Tran et al. added a memory block to LSTMs for language modelling of English,
German and Italian and outperformed both n-gram and neural language models. Their memory
encompasses representations of all possible words in the vocabulary rather than providing a sparse
view as we do.

An alternative to our purely lexical approach to code suggestion involves the use of probabilistic
context-free grammars (PCFGs) which exploit the formal grammar speciﬁcations and well-deﬁned,
deterministic parsers available for source code. These were used by Allamanis & Sutton (2014)
to extract idiomatic patterns from source code. A weakness of PCFGs is their inability to model
context-dependent rules of programming languages such as that variables need to be declared before

7

Under review as a conference paper at ICLR 2017

being used. Maddison & Tarlow (2014) added context-aware variables to their PCFG model in order
to capture such rules.

Ling et al. (2016) recently used a pointer network to generate code from natural language descriptions.
Our use of a controller for deciding whether to generate from a language model or copy an identiﬁer
using a sparse pointer network is inspired by their latent code predictor. However, their inputs (textual
descriptions) are short whereas code suggestion requires capturing very long-range dependencies that
we addressed by a ﬁltered view on the memory of previous identiﬁer representations.

7 CONCLUSIONS AND FUTURE WORK

In this paper, we investigated neural language models for code suggestion of the dynamically-typed
programming language Python. We released a corpus of 41M lines of Python crawled from GitHub
and compared n-gram, standard neural language models, and attention. By using attention, we
observed an order of magnitude more accurate prediction of identiﬁers. Furthermore, we proposed
a sparse pointer network that can efﬁciently capture long-range dependencies by only operating
on a ﬁltered view of a memory of previous identiﬁer representations. This model achieves the
lowest perplexity and best accuracy among the top ﬁve predictions. The Python corpus and code for
replicating our experiment is released at https://github.com/uclmr/pycodesuggest.

The presented methods were only tested for code suggestion within the same Python ﬁle. We are
interested in scaling the approach to the level of entire code projects and collections thereof, as well
as integrating a trained code suggestion model into an existing IDE. Furthermore, we plan to work on
code completion, i.e., models that provide a likely continuation of a partial token, using character
language models (Graves, 2013).

This work was supported by Microsoft Research through its PhD Scholarship Programme, an Allen
Distinguished Investigator Award, and a Marie Curie Career Integration Award.

ACKNOWLEDGMENTS

REFERENCES

Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat
Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan,
Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zhang. Tensorﬂow: A system for large-scale
machine learning. CoRR, abs/1605.08695, 2016. URL http://arxiv.org/abs/1605.
08695.

Miltiadis Allamanis and Charles Sutton. Mining idioms from source code.

In Proceedings of
the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 472–483, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635901. URL http://doi.acm.org/10.1145/2635868.2635901.

Miltiadis Allamanis and Charles A. Sutton. Mining source code repositories at massive scale
using language modeling. In Thomas Zimmermann, Massimiliano Di Penta, and Sunghun Kim
(eds.), MSR, pp. 207–216. IEEE Computer Society, 2013.
ISBN 978-1-4673-2936-1. URL
http://dblp.uni-trier.de/db/conf/msr/msr2013.html#AllamanisS13a.

Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Learning natural coding
conventions. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations
of Software Engineering, FSE 2014, pp. 281–293, New York, NY, USA, 2014. ACM. ISBN 978-
1-4503-3056-5. doi: 10.1145/2635868.2635883. URL http://doi.acm.org/10.1145/
2635868.2635883.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/abs/
1409.0473.

8

Under review as a conference paper at ICLR 2017

Pierre Carbonnelle. Pypl popularity of programming language. http://pypl.github.io/
PYPL.html, 2016. URL http://pypl.github.io/PYPL.html. [Online; accessed 30-
August-2016].

Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing, pp. 551–561. Association for Computational Linguistics, 2016. URL http://
aclweb.org/anthology/D16-1053.

Subhasis Das and Chinmayee Shah. Contextual code completion using machine learning. 2015.

Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.

URL http://arxiv.org/abs/1308.0850.

Karl Moritz Hermann, Tom´as Kocisk´y, Edward Grefenstette, Lasse Espeholt, Will Kay,
Teaching machines to read and compre-
Mustafa Suleyman,
In Advances in Neural Information Processing Systems 28: Annual Confer-
hend.
ence on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal,
URL http://papers.nips.cc/paper/
Quebec, Canada, pp. 1693–1701, 2015.
5945-teaching-machines-to-read-and-comprehend.

and Phil Blunsom.

Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the naturalness
of software. In Proceedings of the 34th International Conference on Software Engineering, ICSE
’12, pp. 837–847, Piscataway, NJ, USA, 2012. IEEE Press. ISBN 978-1-4673-1067-3. URL
http://dl.acm.org/citation.cfm?id=2337223.2337322.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target
vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), pp. 1–10, Beijing, China, July 2015. Association
for Computational Linguistics. URL http://www.aclweb.org/anthology/P15-1001.

Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent
network architectures. In David Blei and Francis Bach (eds.), Proceedings of the 32nd Inter-
national Conference on Machine Learning (ICML-15), pp. 2342–2350. JMLR Workshop and
Conference Proceedings, 2015. URL http://jmlr.org/proceedings/papers/v37/
jozefowicz15.pdf.

H. Khanh Dam, T. Tran, and T. Pham. A deep language model for software code. ArXiv e-prints,

August 2016.

R. Kneser and H. Ney. Improved backing-off for m-gram language modeling. In Acoustics, Speech,
and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pp.
181–184 vol.1, May 1995. doi: 10.1109/ICASSP.1995.479394.

Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, Andrew Senior, Fumin
Wang, and Phil Blunsom. Latent predictor networks for code generation. arXiv preprint
arXiv:1603.06744, 2016.

Chris J Maddison and Daniel Tarlow. Structured generative models of natural source code. In

International Conference on Machine Learning, 2014.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313–330, 1993.

Graham Neubig. Kylm - the kyoto language modeling toolkit. http://www.phontron.com/
[Online; accessed 23-July-

kylm/, 2012. URL http://www.phontron.com/kylm/.
2016].

9

Under review as a conference paper at ICLR 2017

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
networks. In Proceedings of the 30th International Conference on Machine Learning, ICML
2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1310–1318, 2013. URL http://jmlr.org/
proceedings/papers/v28/pascanu13.html.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom.

Reasoning about entailment with neural attention. In ICLR, 2016.

Ke M. Tran, Arianna Bisazza, and Christof Monz. Recurrent memory networks for language
modeling. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, San Diego California,
USA, June 12-17, 2016, pp. 321–331, 2016. URL http://aclweb.org/anthology/N/
N16/N16-1036.pdf.

Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. On the localness of software. In Proceedings
of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 269–280, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635875. URL http://doi.acm.org/10.1145/2635868.2635875.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural

Information Processing Systems, pp. 2692–2700, 2015a.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton.
Grammar as a foreign language. In Advances in Neural Information Processing Systems 28:
Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon-
treal, Quebec, Canada, pp. 2773–2781, 2015b. URL http://papers.nips.cc/paper/
5635-grammar-as-a-foreign-language.

Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the

IEEE, 78(10):1550–1560, 1990.

Martin White, Christopher Vendome, Mario Linares-V´asquez, and Denys Poshyvanyk. Toward
deep learning software repositories. In Proceedings of the 12th Working Conference on Mining
Software Repositories, MSR ’15, pp. 334–345, Piscataway, NJ, USA, 2015. IEEE Press. URL
http://dl.acm.org/citation.cfm?id=2820518.2820559.

Carlo Zapponi. Githut - programming languages and github. http://githut.info/, 2016.

URL http://githut.info/. [Online; accessed 19-August-2016].

10

Under review as a conference paper at ICLR 2017

APPENDIX

Figure 4: Full example of code suggestion with a Sparse Pointer Network. Boldface tokens on the left
show the ﬁrst declaration of an identiﬁer. The middle part visualizes the memory of representations of
these identiﬁers. The right part visualizes the output λ of the controller, which is used for interpolating
between the language model (LM) and the attention of the pointer network (Att).

11

6
1
0
2
 
v
o
N
 
4
2
 
 
]
E
N
.
s
c
[
 
 
1
v
7
0
3
8
0
.
1
1
6
1
:
v
i
X
r
a

Under review as a conference paper at ICLR 2017

LEARNING PYTHON CODE SUGGESTION WITH A
SPARSE POINTER NETWORK

Avishkar Bhoopchand, Tim Rockt¨aschel, Earl Barr & Sebastian Riedel
Department of Computer Science
University College London
avishkar.bhoopchand.15@ucl.ac.uk,
{t.rocktaschel,e.barr,s.riedel}@cs.ucl.ac.uk

ABSTRACT

To enhance developer productivity, all modern integrated development environ-
ments (IDEs) include code suggestion functionality that proposes likely next tokens
at the cursor. While current IDEs work well for statically-typed languages, their re-
liance on type annotations means that they do not provide the same level of support
for dynamic programming languages as for statically-typed languages. Moreover,
suggestion engines in modern IDEs do not propose expressions or multi-statement
idiomatic code. Recent work has shown that language models can improve code
suggestion systems by learning from software repositories. This paper introduces a
neural language model with a sparse pointer network aimed at capturing very long-
range dependencies. We release a large-scale code suggestion corpus of 41M lines
of Python code crawled from GitHub. On this corpus, we found standard neural
language models to perform well at suggesting local phenomena, but struggle to
refer to identiﬁers that are introduced many tokens in the past. By augmenting a
neural language model with a pointer network specialized in referring to predeﬁned
classes of identiﬁers, we obtain a much lower perplexity and a 5 percentage points
increase in accuracy for code suggestion compared to an LSTM baseline. In fact,
this increase in code suggestion accuracy is due to a 13 times more accurate pre-
diction of identiﬁers. Furthermore, a qualitative analysis shows this model indeed
captures interesting long-range dependencies, like referring to a class member
deﬁned over 60 tokens in the past.

1

INTRODUCTION

Integrated development environments (IDEs) are essential tools for programmers. Especially when a
developer is new to a codebase, one of their most useful features is code suggestion: given a piece of
code as context, suggest a likely sequence of next tokens. Typically, the IDE suggests an identiﬁer or
a function call, including API calls. While extensive support exists for statically-typed languages
such as Java, code suggestion for dynamic languages like Python is harder and less well supported
because of the lack of type annotations. Moreover, suggestion engines in modern IDEs do not propose
expressions or multi-statement idiomatic code.

Recently, methods from statistical natural language processing (NLP) have been used to train code
suggestion systems from code usage in large code repositories (Hindle et al., 2012; Allamanis &
Sutton, 2013; Tu et al., 2014). To this end, usually an n-gram language model is trained to score
possible completions. Neural language models for code suggestion (White et al., 2015; Das &
Shah, 2015) have extended this line of work to capture more long-range dependencies. Yet, these
standard neural language models are limited by the so-called hidden state bottleneck, i.e., all context
information has to be stored in a ﬁxed-dimensional internal vector representation. This limitation
restricts such models to local phenomena and does not capture very long-range semantic relationships
like suggesting calling a function that has been deﬁned many tokens before.

To address these issues, we create a large corpus of 41M lines of Python code by using a heuristic for
crawling high-quality code repositories from GitHub. We investigate, for the ﬁrst time, the use of
attention (Bahdanau et al., 2014) for code suggestion and ﬁnd that, despite a substantial improvement

1

Under review as a conference paper at ICLR 2017

in accuracy, it still makes avoidable mistakes. Hence, we introduce a model that leverages long-range
Python dependencies by selectively attending over the introduction of identiﬁers as determined
by examining the Abstract Syntax Tree. The model is a form of pointer network (Vinyals et al.,
2015a), and learns to dynamically choose between syntax-aware pointing for modeling long-range
dependencies and free form generation to deal with local phenomena, based on the current context.

Our contributions are threefold: (i) We release a code suggestion corpus of 41M lines of Python code
crawled from GitHub, (ii) We introduce a sparse attention mechanism that captures very long-range
dependencies for code suggestion of this dynamic programming language efﬁciently, and (iii) We
provide a qualitative analysis demonstrating that this model is indeed able to learn such long-range
dependencies.

2 METHODS

We ﬁrst revisit neural language models, before brieﬂy describing how to extend such a language
model with an attention mechanism. Then we introduce a sparse attention mechanism for a pointer
network that can exploit the Python abstract syntax tree of the current context for code suggestion.

2.1 NEURAL LANGUAGE MODEL

Code suggestion can be approached by a language model that measures the probability of observing
a sequence of tokens in a Python program. For example, for the sequence S = a1, . . . , aN , the joint
probability of S factorizes according to

Pθ(S) = Pθ(a1) ·

Pθ(at | at−1, . . . , a1)

N
(cid:89)

t=2

(1)

(2)

(3)

where the parameters θ are estimated from a training corpus. Given a sequence of Python tokens, we
seek to predict the next M tokens at+1, . . . , at+M that maximize Equation 1

arg max
at+1, ..., at+M

Pθ(a1, . . . , at, at+1, . . . , at+M ).

In this work, we build upon neural language models using Recurrent Neural Networks (RNNs) and
Long Short-Term Memory (LSTM, Hochreiter & Schmidhuber, 1997). This neural language model
estimates the probabilities in Equation 1 using the output vector of an LSTM at time step t (denoted
ht here) according to

Pθ(at = τ | at−1, . . . , a1) =

τ ht + bτ )

exp (vT
τ (cid:48) exp (vT

(cid:80)

τ (cid:48)ht + bτ (cid:48))

where vτ is a parameter vector associated with token τ in the vocabulary.

Neural language models can, in theory, capture long-term dependencies in token sequences through
their internal memory. However, as this internal memory has ﬁxed dimension and can be updated at
every time step, such models often only capture local phenomena. In contrast, we are interested in
very long-range dependencies like referring to a function identiﬁer introduced many tokens in the
past. For example, a function identiﬁer may be introduced at the top of a ﬁle and only used near
the bottom. In the following, we investigate various external memory architectures for neural code
suggestion.

2.2 ATTENTION

A straight-forward approach to capturing long-range dependencies is to use a neural attention mech-
anism (Bahdanau et al., 2014) on the previous K output vectors of the language model. Attention
mechanisms have been successfully applied to sequence-to-sequence tasks such as machine transla-
tion (Bahdanau et al., 2014), question-answering (Hermann et al., 2015), syntactic parsing (Vinyals
et al., 2015b), as well as dual-sequence modeling like recognizing textual entailment (Rockt¨aschel
et al., 2016). The idea is to overcome the hidden-state bottleneck by allowing referral back to previous
output vectors. Recently, these mechanisms were applied to language modelling by Cheng et al.
(2016) and Tran et al. (2016).

2

Under review as a conference paper at ICLR 2017

Formally, an attention mechanism with a ﬁxed memory Mt ∈ Rk×K of K vectors mi ∈ Rk for
i ∈ [1, K], produces an attention distribution αt ∈ RK and context vector ct ∈ Rk at each time
step t according to Equations 4 to 7. Furthermore, W M , W h ∈ Rk×k and w ∈ Rk are trainable
parameters. Finally, note that 1K represents a K-dimensional vector of ones.

Mt = [m1 . . . mK]
Gt = tanh(W M Mt + 1T
αt = softmax(wT Gt)
ct = MtαT
t

K(W hht))

∈ Rk×K
∈ Rk×K
∈ R1×K
∈ Rk

For language modeling, we populate Mt with a ﬁxed window of the previous K LSTM output
vectors. To obtain a distribution over the next token we combine the context vector ct of the attention
mechanism with the output vector ht of the LSTM using a trainable projection matrix W A ∈ Rk×2k.
The resulting ﬁnal output vector nt ∈ Rk encodes the next-word distribution and is projected to the
size of the vocabulary |V |. Subsequently, we apply a softmax to arrive at a probability distribution
yt ∈ R|V | over the next token. This process is presented in Equation 9 where WV ∈ R|V |×k and
bV ∈ R|V | are trainable parameters.

(cid:18)

nt = tanh

W A

(cid:21)(cid:19)

(cid:20)ht
ct

yt = softmax(W V nt + bV )

∈ Rk

∈ R|V |

The problem of the attention mechanism above is that it quickly becomes computationally expensive
for large K. Moreover, attending over many memories can make training hard as a lot of noise is
introduced in early stages of optimization where the LSTM outputs (and thus the memory Mt) are
more or less random. To alleviate these problems we now turn to pointer networks and a simple
heuristic for populating Mt that permits the efﬁcient retrieval of identiﬁers in a large history of
Python code.

2.3 SPARSE POINTER NETWORK

We develop an attention mechanism that provides a ﬁltered view of a large history of Python tokens.
At any given time step, the memory consists of context representations of the previous K identiﬁers
introduced in the history. This allows us to model long-range dependencies found in identiﬁer usage.
For instance, a class identiﬁer may be declared hundreds of lines of code before it is used. Given a
history of Python tokens, we obtain a next-word distribution from a weighed average of the sparse
pointer network for identiﬁer reference and a standard neural language model. The weighting of the
two is determined by a controller.
Formally, at time-step t, the sparse pointer network operates on a memory Mt ∈ Rk×K of only the
K previous identiﬁer representations (e.g. function identiﬁers, class identiﬁers and so on). In addition,
we maintain a vector mt = [id1, . . . , idK] ∈ NK of symbol ids for these identiﬁer representations
(i.e. pointers into the large global vocabulary).

As before, we calculate a context vector ct using the attention mechanism (Equation 7), but on a
memory Mt only containing representations of identiﬁers that were declared in the history. Next, we
obtain a pseudo-sparse distribution over the global vocabulary from

st[i] =

(cid:26)αt[j]
−C

if mt[j] = i
otherwise

it = softmax(st)

∈ R|V |

where −C is a large negative constant (e.g. −1000). In addition, we calculate a next-word distribution
from a standard neural language model

yt = softmax(W V ht + bV )

∈ R|V |

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

3

Under review as a conference paper at ICLR 2017

Figure 1: Sparse pointer network for code suggestion on a Python code snippet, showing the next-
word distributions of the language model and identiﬁer attention and their weighted combination
through λ

and we use a controller to calculate a distribution λt ∈ R2 over the language model and pointer
network for the ﬁnal weighted next-word distribution y∗
(cid:34) ht
xt
ct

∈ R3k

t via

t =

(13)

hλ

(cid:35)

λt = softmax(W λhλ
y∗
t = [yt it] λt

t + bλ)

∈ R2
∈ R|V |

(14)

(15)
Here, xt is the representation of the input token, and W λ ∈ R2×3k and bλ ∈ R2 a trainable
weight matrix and bias respectively. This controller is conditioned on the input, output and context
representations. This means for deciding whether to refer to an identiﬁer or generate from the global
vocabulary, the controller has access to information from the encoded next-word distribution ht of
the standard neural language model, as well as the attention-weighted identiﬁer representations ct
from the current history.

Figure 1 overviews this process. In it, the identiﬁer base_path appears twice, once as an argument
to a function and once as a member of a class (denoted by *). Each appearance has a different id
in the vocabulary and obtains a different probability from the model. In the example, the model
correctly chooses to refer to the member of the class instead of the out-of-scope function argument,
although, from a user point-of-view, the suggestion would be the same in both cases.

3 LARGE-SCALE PYTHON CORPUS

Previous work on code suggestion either focused on statically-typed languages (particularly Java)
or trained on very small corpora. Thus, we decided to collect a new large-scale corpus of the
dynamic programming language Python. According to the programming language popularity website
Pypl (Carbonnelle, 2016), Python is the second most popular language after Java. It is also the
3rd most common language in terms of number of repositories on the open-source code repository
GitHub, after JavaScript and Java (Zapponi, 2016).

We collected a corpus of 41M lines of Python code from GitHub projects. Ideally, we would like this
corpus to only contain high-quality Python code, as our language model learns to suggest code from
how users write code. However, it is difﬁcult to automatically assess what constitutes high-quality
code. Thus, we resort to the heuristic that popular code projects tend to be of good quality, There are

4

Under review as a conference paper at ICLR 2017

Table 1: Python corpus statistics.

Dataset

#Projects

#Files

#Lines

#Tokens Vocabulary Size

Train
Dev
Test

Total

489
179
281

949

118 298
26 466
43 062

26 868 583
5 804 826
8 398 100

88 935 698
18 147 341
30 178 356

187 826

41 071 509

137 261 395

2 323 819

Figure 2: Example of the Python code normalization. Original ﬁle on the left and normalized version
on the right.

two metrics on GitHub that we can use for this purpose, namely stars (similar to bookmarks) and
forks (copies of a repository that allow users to freely experiment with changes without affecting the
original repository). Similar to Allamanis & Sutton (2013) and Allamanis et al. (2014), we select
Python projects with more than 100 stars, sort by the number of forks descending, and take the top
1000 projects. We then removed projects that did not compile with Python3, leaving us with 949
projects. We split the corpus on the project level into train, dev, and test. Table 1 presents the corpus
statistics.

3.1 NORMALIZATION OF IDENTIFIERS

Unsurprisingly, the long tail of words in the vocabulary consists of rare identiﬁers. To improve
generalization, we normalize identiﬁers before feeding the resulting token stream to our models.
That is, we replace every identiﬁer name with an anonymous identiﬁer indicating the identiﬁer group
(class, variable, argument, attribute or function) concatenated with a random number that makes
the identiﬁer unique in its scope. Note that we only replace novel identiﬁers deﬁned within a ﬁle.
Identiﬁer references to external APIs and libraries are left untouched. Consistent with previous corpus
creation for code suggestion (e.g. Khanh Dam et al., 2016; White et al., 2015), we replace numerical
constant tokens with $NUM$, remove comments, reformat the code, and replace tokens appearing
less than ﬁve times with an $OOV$ (out of vocabulary) token.

4 EXPERIMENTS

Although previous work by White et al. (2015) already established that a simple neural language
model outperforms an n-gram model for code suggestion, we include a number of n-gram baselines
to conﬁrm this observation. Speciﬁcally, we use n-gram models for n ∈ {3, 4, 5, 6} with Modiﬁed
Kneser-Ney smoothing (Kneser & Ney, 1995) from the Kyoto Language Modelling Toolkit (Neubig,
2012).

We train the sparse pointer network using mini-batch SGD with a batch size of 30 and truncated
backpropagation through time (Werbos, 1990) with a history of 20 identiﬁer representations. We use

5

Under review as a conference paper at ICLR 2017

Table 2: Perplexity (PP), Accuracy (Acc) and Accuarcy among top 5 predictions (Acc@5).

Model

3-gram
4-gram
5-gram
6-gram

LSTM
LSTM w/ Attention 20
LSTM w/ Attention 50

Sparse Pointer Network

Train PP Dev PP Test PP

Acc [%]

Acc@5 [%]

12.90
7.60
4.52
3.37

9.29
7.30
7.09

6.41

24.19
21.07
19.33
18.73

13.08
11.07
9.83

9.40

All

IDs Other

All

IDs Other

26.90
23.85
21.22
20.17

14.01
11.74
10.05

13.19
13.68
13.90
14.51

57.91
61.30
63.21

9.18

62.97

–
–
–
–

2.1
21.4
30.2

27.3

–
–
–
–

62.8
64.8
65.3

64.9

50.81
51.26
51.49
51.76

76.30
79.32
81.69

82.62

–
–
–
–

4.5
29.9
41.3

43.6

–
–
–
–

82.6
83.7
84.1

84.5

an initial learning rate of 0.7 and decay it by 0.9 after every epoch. As additional baselines, we test
a neural language model with LSTM units with and without attention. For the attention language
models, we experiment with a ﬁxed-window attention memory of the previous 20 and 50 tokens
respectively, and a batch size of 75.

All neural language models were developed in TensorFlow (Abadi et al., 2016) and trained using
cross-entropy loss. While processing a Python source code ﬁle, the last recurrent state of the RNN
is fed as the initial state of the subsequent sequence of the same ﬁle and reset between ﬁles. All
models use an input and hidden size of 200, an LSTM forget gate bias of 1 (Jozefowicz et al., 2015),
gradient norm clipping of 5 (Pascanu et al., 2013), and randomly initialized parameters in the interval
(−0.05, 0.05). As regularizer, we use a dropout of 0.1 on the input representations. Furthermore, we
use a sampled softmax (Jean et al., 2015) with a log-uniform sampling distribution and a sample size
of 1000.

5 RESULTS

We evaluate all models using perplexity (PP), as well as accuracy of the top prediction (Acc) and the
top ﬁve predictions (Acc@5). The results are summarized in Table 2.

We can conﬁrm that for code suggestion neural models outperform n-gram language models by a
large margin. Furthermore, adding attention improves the results substantially (2.3 lower perplexity
and 3.4 percentage points increased accuracy). Interestingly, this increase can be attributed to a
superior prediction of identiﬁers, which increased from an accuracy of 2.1% to 21.4%. An LSTM
with an attention window of 50 gives us the best accuracy for the top prediction. We achieve further
improvements for perplexity and accuracy of the top ﬁve predictions by using a sparse pointer network
that uses a smaller memory of the past 20 identiﬁer representations.

5.1 QUALITATIVE ANALYSIS

Figures 3a-d show a code suggestion example involving an identiﬁer usage. While the LSTM baseline
is uncertain about the next token, we get a sensible prediction by using attention or the sparse pointer
network. The sparse pointer network provides more reasonable alternative suggestions beyond the
correct top suggestion.

Figures 3e-h show the use-case referring to a class attribute declared 67 tokens in the past. Only
the Sparse Pointer Network makes a good suggestion. Furthermore, the attention weights in 3i
demonstrate that this model distinguished attributes from other groups of identiﬁers. We give a full
example of a token-by-token suggestion of the Sparse Pointer Network in Figure 4 in the Appendix.

6 RELATED WORK

Previous code suggestion work using methods from statistical NLP has mostly focused on n-gram
models. Much of this work is inspired by Hindle et al. (2012) who argued that real programs fall

6

Under review as a conference paper at ICLR 2017

(a) Code snippet for referencing
variable.

(b) LSTM Model.

(c) LSTM w/ Attention
50.

(d) Sparse Pointer Net-
work.

(e) Code snippet for referencing class
member.

(f) LSTM Model.

(g) LSTM w/ Attention
50.

(h) Sparse Pointer Net-
work.

(i) Sparse Pointer Network attention over memory of identiﬁer representations.

Figure 3: Code suggestion example involving a reference to a variable (a-d), a long-range dependency
(e-h), and the attention weights of the Sparse Pointer Network (i).

into a much smaller space than the ﬂexibility of programming languages allows. They were able
to capture the repetitiveness and predictable statistical properties of real programs using language
models. Subsequently, Tu et al. (2014) improved upon Hindle et al.’s work by adding a cache
mechanism that allowed them to exploit locality stemming from the specialisation and decoupling of
program modules. Tu et al.’s idea of adding a cache mechanism to the language model is speciﬁcally
designed to exploit the properties of source code, and thus follows the same aim as the sparse attention
mechanism introduced in this paper.

While the majority of preceding work trained on small corpora, Allamanis & Sutton (2013) created a
corpus of 352M lines of Java code which they analysed with n-gram language models. The size of
the corpus allowed them to train a single language model that was effective across multiple different
project domains. White et al. (2015) later demonstrated that neural language models outperform
n-gram models for code suggestion. They compared various n-gram models (up to nine grams),
including Tu et al.’s cache model, with a basic RNN neural language model. Khanh Dam et al.
(2016) compared White et al.’s basic RNN with LSTMs and found that the latter are better at code
suggestion due to their improved ability to learn long-range dependencies found in source code. Our
paper extends this line of work by introducing a sparse attention model that captures even longer
dependencies.

The combination of lagged attention mechanisms with language modelling is inspired by Cheng et al.
(2016) who equipped LSTM cells with a ﬁxed-length memory tape rather than a single memory cell.
They achieved promising results on the standard Penn Treebank benchmark corpus (Marcus et al.,
1993). Similarly, Tran et al. added a memory block to LSTMs for language modelling of English,
German and Italian and outperformed both n-gram and neural language models. Their memory
encompasses representations of all possible words in the vocabulary rather than providing a sparse
view as we do.

An alternative to our purely lexical approach to code suggestion involves the use of probabilistic
context-free grammars (PCFGs) which exploit the formal grammar speciﬁcations and well-deﬁned,
deterministic parsers available for source code. These were used by Allamanis & Sutton (2014)
to extract idiomatic patterns from source code. A weakness of PCFGs is their inability to model
context-dependent rules of programming languages such as that variables need to be declared before

7

Under review as a conference paper at ICLR 2017

being used. Maddison & Tarlow (2014) added context-aware variables to their PCFG model in order
to capture such rules.

Ling et al. (2016) recently used a pointer network to generate code from natural language descriptions.
Our use of a controller for deciding whether to generate from a language model or copy an identiﬁer
using a sparse pointer network is inspired by their latent code predictor. However, their inputs (textual
descriptions) are short whereas code suggestion requires capturing very long-range dependencies that
we addressed by a ﬁltered view on the memory of previous identiﬁer representations.

7 CONCLUSIONS AND FUTURE WORK

In this paper, we investigated neural language models for code suggestion of the dynamically-typed
programming language Python. We released a corpus of 41M lines of Python crawled from GitHub
and compared n-gram, standard neural language models, and attention. By using attention, we
observed an order of magnitude more accurate prediction of identiﬁers. Furthermore, we proposed
a sparse pointer network that can efﬁciently capture long-range dependencies by only operating
on a ﬁltered view of a memory of previous identiﬁer representations. This model achieves the
lowest perplexity and best accuracy among the top ﬁve predictions. The Python corpus and code for
replicating our experiment is released at https://github.com/uclmr/pycodesuggest.

The presented methods were only tested for code suggestion within the same Python ﬁle. We are
interested in scaling the approach to the level of entire code projects and collections thereof, as well
as integrating a trained code suggestion model into an existing IDE. Furthermore, we plan to work on
code completion, i.e., models that provide a likely continuation of a partial token, using character
language models (Graves, 2013).

This work was supported by Microsoft Research through its PhD Scholarship Programme, an Allen
Distinguished Investigator Award, and a Marie Curie Career Integration Award.

ACKNOWLEDGMENTS

REFERENCES

Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat
Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan,
Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zhang. Tensorﬂow: A system for large-scale
machine learning. CoRR, abs/1605.08695, 2016. URL http://arxiv.org/abs/1605.
08695.

Miltiadis Allamanis and Charles Sutton. Mining idioms from source code.

In Proceedings of
the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 472–483, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635901. URL http://doi.acm.org/10.1145/2635868.2635901.

Miltiadis Allamanis and Charles A. Sutton. Mining source code repositories at massive scale
using language modeling. In Thomas Zimmermann, Massimiliano Di Penta, and Sunghun Kim
(eds.), MSR, pp. 207–216. IEEE Computer Society, 2013.
ISBN 978-1-4673-2936-1. URL
http://dblp.uni-trier.de/db/conf/msr/msr2013.html#AllamanisS13a.

Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Learning natural coding
conventions. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations
of Software Engineering, FSE 2014, pp. 281–293, New York, NY, USA, 2014. ACM. ISBN 978-
1-4503-3056-5. doi: 10.1145/2635868.2635883. URL http://doi.acm.org/10.1145/
2635868.2635883.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/abs/
1409.0473.

8

Under review as a conference paper at ICLR 2017

Pierre Carbonnelle. Pypl popularity of programming language. http://pypl.github.io/
PYPL.html, 2016. URL http://pypl.github.io/PYPL.html. [Online; accessed 30-
August-2016].

Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing, pp. 551–561. Association for Computational Linguistics, 2016. URL http://
aclweb.org/anthology/D16-1053.

Subhasis Das and Chinmayee Shah. Contextual code completion using machine learning. 2015.

Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.

URL http://arxiv.org/abs/1308.0850.

Karl Moritz Hermann, Tom´as Kocisk´y, Edward Grefenstette, Lasse Espeholt, Will Kay,
Teaching machines to read and compre-
Mustafa Suleyman,
In Advances in Neural Information Processing Systems 28: Annual Confer-
hend.
ence on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal,
URL http://papers.nips.cc/paper/
Quebec, Canada, pp. 1693–1701, 2015.
5945-teaching-machines-to-read-and-comprehend.

and Phil Blunsom.

Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the naturalness
of software. In Proceedings of the 34th International Conference on Software Engineering, ICSE
’12, pp. 837–847, Piscataway, NJ, USA, 2012. IEEE Press. ISBN 978-1-4673-1067-3. URL
http://dl.acm.org/citation.cfm?id=2337223.2337322.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target
vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), pp. 1–10, Beijing, China, July 2015. Association
for Computational Linguistics. URL http://www.aclweb.org/anthology/P15-1001.

Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent
network architectures. In David Blei and Francis Bach (eds.), Proceedings of the 32nd Inter-
national Conference on Machine Learning (ICML-15), pp. 2342–2350. JMLR Workshop and
Conference Proceedings, 2015. URL http://jmlr.org/proceedings/papers/v37/
jozefowicz15.pdf.

H. Khanh Dam, T. Tran, and T. Pham. A deep language model for software code. ArXiv e-prints,

August 2016.

R. Kneser and H. Ney. Improved backing-off for m-gram language modeling. In Acoustics, Speech,
and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pp.
181–184 vol.1, May 1995. doi: 10.1109/ICASSP.1995.479394.

Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, Andrew Senior, Fumin
Wang, and Phil Blunsom. Latent predictor networks for code generation. arXiv preprint
arXiv:1603.06744, 2016.

Chris J Maddison and Daniel Tarlow. Structured generative models of natural source code. In

International Conference on Machine Learning, 2014.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313–330, 1993.

Graham Neubig. Kylm - the kyoto language modeling toolkit. http://www.phontron.com/
[Online; accessed 23-July-

kylm/, 2012. URL http://www.phontron.com/kylm/.
2016].

9

Under review as a conference paper at ICLR 2017

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
networks. In Proceedings of the 30th International Conference on Machine Learning, ICML
2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1310–1318, 2013. URL http://jmlr.org/
proceedings/papers/v28/pascanu13.html.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom.

Reasoning about entailment with neural attention. In ICLR, 2016.

Ke M. Tran, Arianna Bisazza, and Christof Monz. Recurrent memory networks for language
modeling. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, San Diego California,
USA, June 12-17, 2016, pp. 321–331, 2016. URL http://aclweb.org/anthology/N/
N16/N16-1036.pdf.

Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. On the localness of software. In Proceedings
of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
FSE 2014, pp. 269–280, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi:
10.1145/2635868.2635875. URL http://doi.acm.org/10.1145/2635868.2635875.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural

Information Processing Systems, pp. 2692–2700, 2015a.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton.
Grammar as a foreign language. In Advances in Neural Information Processing Systems 28:
Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon-
treal, Quebec, Canada, pp. 2773–2781, 2015b. URL http://papers.nips.cc/paper/
5635-grammar-as-a-foreign-language.

Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the

IEEE, 78(10):1550–1560, 1990.

Martin White, Christopher Vendome, Mario Linares-V´asquez, and Denys Poshyvanyk. Toward
deep learning software repositories. In Proceedings of the 12th Working Conference on Mining
Software Repositories, MSR ’15, pp. 334–345, Piscataway, NJ, USA, 2015. IEEE Press. URL
http://dl.acm.org/citation.cfm?id=2820518.2820559.

Carlo Zapponi. Githut - programming languages and github. http://githut.info/, 2016.

URL http://githut.info/. [Online; accessed 19-August-2016].

10

Under review as a conference paper at ICLR 2017

APPENDIX

Figure 4: Full example of code suggestion with a Sparse Pointer Network. Boldface tokens on the left
show the ﬁrst declaration of an identiﬁer. The middle part visualizes the memory of representations of
these identiﬁers. The right part visualizes the output λ of the controller, which is used for interpolating
between the language model (LM) and the attention of the pointer network (Att).

11

