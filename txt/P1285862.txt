Robust Gyroscope-Aided Camera Self-Calibration

Santiago Cort´es Reina
Department of Computer Science
Aalto University
Helsinki, Finland
santiago.cortesreina@aalto.ﬁ

Arno Solin
Department of Computer Science
Aalto University
Helsinki, Finland
arno.solin@aalto.ﬁ

Juho Kannala
Department of Computer Science
Aalto University
Helsinki, Finland
juho.kannala@aalto.ﬁ

8
1
0
2
 
y
a
M
 
1
3
 
 
]

V
C
.
s
c
[
 
 
1
v
6
0
5
2
1
.
5
0
8
1
:
v
i
X
r
a

Abstract—Camera calibration for estimating the intrinsic
parameters and lens distortion is a prerequisite for various
monocular vision applications including feature tracking and
video stabilization. This application paper proposes a model for
estimating the parameters on the ﬂy by fusing gyroscope and
camera data, both readily available in modern day smartphones.
The model is based on joint estimation of visual feature positions,
camera parameters, and the camera pose, the movement of
which is assumed to follow the movement predicted by the
gyroscope. Our model assumes the camera movement to be free,
but continuous and differentiable, and individual features are
assumed to stay stationary. The estimation is performed online
using an extended Kalman ﬁlter, and it is shown to outperform
existing methods in robustness and insensitivity to initialization.
We demonstrate the method using simulated data and empirical
data from an iPad.

I. INTRODUCTION

The growth in the market of smartphones and tablets has
brought monocular cameras to even the cheapest of smart-
devices. Simultaneously, the improved computational capa-
bilities have made it possible to expand their use into new
ﬁelds and use cases, such as video calls, payment veriﬁcation,
and augmented reality. However, employing the device camera
in pose estimation, video stabilization, or feature tracking
requires the camera calibration parameters to be known or
estimated.

On smart-devices, the use cases often provide an explicit
calibration step that includes capturing a pre-deﬁned calibra-
tion pattern from different positions. This procedure is the
base for traditional camera calibration (see, e.g., [9] for an
overview). Self-calibration is the problem of modeling the in-
ternal parameters of a camera (projection matrix and distortion
coefﬁcients) without using any known pattern. Luckily there
are usually additional sensors available on the devices, typi-
cally a low-cost MEMS inertial measurement unit (IMU). The
IMU typically provides fast-sampled (up to some hundreds of
Hz) readings of the speciﬁc force (accelerometer) and turn-
rate (gyroscope) in the device’s coordinate frame (see, e.g.,
[26] for a more thorough introduction).

In this work we propose a method for camera self-
calibration using the information from a gyroscope rigidly
attached to the camera. We use a structure from motion (SfM)
approach, where the camera model minimizes the reprojection
error over a set of images. The proposed method jointly esti-
mates the camera pose, tracked feature positions, and camera

Phone camera
position p
orientation q

Phone
gyroscope
with turn rate
ω [rad/s]

Camera parameters
(fx/y and cx/y)

Radial distortion
(k1 and k2)

Fig. 1. Illustration of the model setup: A pinhole camera with position p and
orientation quaternion q, auxiliary turn rate ω from the gyroscope. A set of
(unknown) feature locations {zi} are observed through a camera model with
linear and non-linear distortion parameters.

parameters. The method works online employing a state-space
formulation, where the fast-sampled gyroscope data is used for
forward-predicting the relative camera and feature movement,
and the visual data tracking results are then matched to the
predictions in a probabilistic fashion. The inference itself is
solved by an extended Kalman ﬁlter.

Due to its practical importance, camera self-calibration—
or auto-calibration—has been studied extensively during the
past decades. Outside smartphones, practical applications are
found, for example, in trafﬁc surveillance [1], projector camera
calibration [18, 29] and robotics for autonomous vehicles
[5, 7, 8, 10, 21, 28]. The seminal paper by Faugeras et al. [6]
introduced the concept of calibrating the intrinsic and extrinsic
parameters without a known calibration object or pattern.
Since then many methods have been introduced, building upon
special kind of motion (e.g. purely rotating or planar), special
scene geometry (e.g. planar scenes or special depth structure,
see [11] for discussion), or auxiliary sensors.

For example, Civera et al. [4] proposed a sum-of-Gaussians
ﬁlter approach building upon a SfM approach where there
has to be sufﬁcient translation. Additional sensors ease the
motion constraints. Using both a gyroscope and an accelerom-
eter for calibration, information can be acquired about the

relative rotation, absolute scale, and the world coordinate
frame orientation (see, e.g., [19]). Similar setups are often
employed in visual-inertial odometry methods (see, e.g., the
discussion in [24, 27]), where the same set of sensors are
used and the camera calibration is estimated as a part of the
inference. Gyroscopes are also used in video stabilization, and
[22] proposed a model for scaling, time offset, and relative
pose calibration using the gyroscope. They, however, do not
estimate the camera calibration parameters.

There are also other methods which only use the camera
and a gyroscope. Karpenko et al. [16] proposed a method for
calibration which uses a gyroscope together with a quick shake
when pointing to far-away objects, while Hwangbo et al. [12]
constrain the estimation by assuming pure rotation.

Within methods designed for fusing camera and gyroscope
data, we are only aware of one previously published approach,
which works in the same setting as the proposed method.
The method by Jia and Evans was ﬁrst published in [14] and
later reﬁned in [15]. Their method together with ours does
not assume special movement and only require visual and
gyroscope data.

The proposed model is based on the following assumptions:
(i) The camera movement is free, but continuous and differ-
entiable (rather smooth), (ii) The camera rotations follow the
gyroscope turn rate observations, (iii) The world coordinates
of individual features are unknown, but assumed constant
(individual features assumed to stay stationary). In comaprison
to [15], this method aims at providing higher robustness to
initialization and feature-poor enivironments with only a few
visual features being tracked. Where [15] uses a lot of short
(two-frame) connections of features, our method uses a few
long chains of connected features.

This paper is structured as follows. Section II presents the
theoretical methodology in detail starting from the camera
calibration model, the proposed state-space model, and ﬁnally
how to jointly infer all the unknown parameters. The Results
section shows the method employed both in a simulation
study and a on real-world data. Finally, the assumptions and
modeling problems and some future work are discussed.

II. METHODS

We start by deﬁning the notation used in the camera calibra-
tion model, which will later be used in speciﬁcation of the
measurement model. Then we proceed to setting up the state-
space model for the gyroscope-aided dynamics of the camera
pose, feature positions, and camera parameters. Finally, we
couple the forward dynamics with the image data in a visual
measurement model. A sketch of the information ﬂow in the
method is shown in Figure 2.

A. Camera Model

We build upon the well-established theory of monocular
camera calibration models. A detailed and extensive descrip-
tion of camera models can be found in [9]. Through the
camera model, points (x, y, z) in three-dimensional space are
projected to image plane coordinates (u, v) using a pinhole

Initial state

Gyroscope
prediction

k ← k+1

Wait for
gyro data

no

Is frame
available?
yes

Feature
tracking

Visual
update

Fig. 2. The information ﬂow in the self-calibration method. The gyroscope
prediction loop runs at 100 Hz, and visual updates occur once a new frame
is acquired (typically at 10 Hz).

camera model. First the points are rigidly transformed into
the camera reference frame, that is origin at the optical center,
the z-axis perpendicular to the image plane and y-axis parallel
to the vertical axis of the image. Then the points are projected
into the image plane using the pinhole projection. In matrix
form





u
v
 = K E
1











,





x
y
z
1

(1)

where the intrinsic matrix K ∈ R3×3 and extrinsic matrix
E ∈ R3×4 are parametrized as follows:

K =





fx
0
0

0
fy
0

cx
cy
1


 and E = (cid:0)RT −RTp(cid:1) .

(2)

length (fx and fy, separate
The parameters are the focal
between dimensions to account for non-square pixels), the
origin of the image plane (cx, cy), and the position p ∈ R3
and orientation R ∈ R3×3 of the camera in the global frame
of reference.

The projection operation is entirely linear (in homogeneous
space), but real-world lenses usually introduce non-linear
mappings. These are known as distortions, the most common
distortion is rotationally symmetric around the image center
and is modeled by so-called radial distortion coefﬁcients k1
and k2 as follows:
(cid:19)
(cid:18)u(cid:48)
v(cid:48)

(1 + k1 r2 + k2 r4),

(cid:18)u
v

(3)

=

(cid:19)

where the radial component is determined by

(cid:115)

r =

(cid:17)2

(cid:16) u − cx
fx

+

(cid:16) v − cy
fy

(cid:17)2

.

(4)

This camera model is shown in the illustrative sketch in
Figure 1, where also the resulting distortion effect is visible.
We refer to the non-linear distortion function as ‘distort’ later
in the paper.

B. State Estimation

In order to include time-dependency between observed frames
and rotational
information from the gyroscope, we deﬁne
a state-space model. The model describes a device with a

monocular camera and a rigidly attached gyroscope with a
known relative orientation between them (see Fig. 1). An
example of such a device is a modern smartphone. The
following section shows the non-linear ﬁler designed for
information fusion, the non-linear ﬁlter allows for estimation
and fusion while keeping track of the uncertainty and depen-
dencies between the non-deterministic processes describing
the evolution of the calibration parameters, camera pose, and
feature locations.

We deﬁne a state-space model (see, e.g., [23]), where the

state vector is

x = (cid:0)cT pT vT qT

zT(cid:1)T

.

(5)

The variable c = (fx, fy, cx, cy, k1, k2) contains the internal
camera parameters, p ∈ R3 and v ∈ R3 contain the position
and velocity of the camera, q contains the quaternion encoding
the orientation of the camera, and z ∈ R3d contains the
locations of the features (d is the number of features being
tracked).

The non-linear state-space model with the auxiliary gyro-
scope control signal ωk is given as follows. The control can
be embedded into the time-varying dynamical model such that
fk(x, εk) := f (x, ωk, εk). The state-space model takes the
canonical form:

xk = fk(xk−1, εk),
yk = hk(xk) + γk,

(6)

where xk ∈ Rn is the state at time step tk, k = 1, 2, . . ., yk ∈
Rm is a measurement, εk ∼ N(0, Qk) is the Gaussian process
noise, and γk ∼ N(0, Σk) is the Gaussian measurement noise.
The dynamics and measurements are speciﬁed in terms of the
dynamical model function f (·, ·, ·) and the measurement model
function hk(·).

In this work we employ the extended Kalman ﬁlter (EKF,
[2, 13]) which provides a means of approximating the state
distributions p(x | y1:k) with Gaussians:

p(xk | y1:k) (cid:39) N(xk | mk|k, Pk|k).

(7)

In the EKF, these approximations are formed by ﬁrst-order
linearizations of the non-linearities. The extended Kalman
ﬁltering recursion can be written as follows (see [23] for
detailed presentation). The dynamics are incorporated into the
prediction step:

mk|k−1 = fk(mk−1|k−1, 0),
Pk|k−1 = Fx(mk−1|k−1) Pk−1|k−1 FT

Fε(mk−1|k−1) Qk FT

ε (mk−1|k−1),

prediction step is entirely given by the standard Kalman ﬁlter
preduction step:

mk|k−1 = Ak mk−1|k−1,
Pk|k−1 = Ak Pk−1|k−1 AT

k + Qk.

(9)

Measurement data providing observations of the system
state at given time steps are combined with the model in the
update step:

vk = yk − hk(mk|k−1),
Sk = Hx(mk|k−1) Pk|k−1 HT
x(mk|k−1) + Σk,
x(mk|k−1) S−1
Kk = Pk|k−1 HT
k ,

mk|k = mk|k−1 + Kk vk,
Pk|k = [I − Kk Hx(mk|k−1)] Pk|k−1

[I − Kk Hx(mk|k−1)]T + Kk Σk KT
k ,

(10)

where Hx(·) denotes the Jacobian of the measurement model
hk(·) with respect
to the state variables x. The slightly
unorthodox form of the last line is known as the Joseph’s
formula, which both numerically stabilizes updating the co-
variance and preserves symmetry.

The linearizations inside the extended Kalman ﬁlter cause
some errors in the estimation. Most notably the estima-
tion scheme does not preserve the norm of the orientation
quaternions. Therefore after each update an extra quaternion
normalization step is added to the estimation scheme.

C. Propagation by Gyroscope Prediction

The state holds the internal and external parameters of the
camera and the 3D position of the features. The internal
parameters and the 3D coordinates of the features are assumed
to stay constant (but are still unknown), so their propagation
functions are identities. The external parameters (position,
orientation) of the camera are not constant and are treated
differently.

The position of the camera is modeled as a Wiener velocity
process, a commonly used model
in tracking and control
literature (see, e.g., [23] for details). In order to keep track of
the full inertial state the estimate contains both the position and
velocity vectors x = (cid:0)p v(cid:1)T
and the acceleration is modeled
as a white noise process d2x(t)
dt2 = w(t), or in state space
form as a linear time-invariant stochastic differential equation
(independently for each spatial dimension)
(cid:19)
(cid:18)0
1

(cid:18)0
0

(cid:19)
1
0

dx(t)
dt

x(t) +

w(t),

(11)

=

x(mk−1|k−1)+

(8)

where w(t) is a realization of the white noise process. In
discrete time the system is

where the dynamic model
is evaluated with the outcome
from the previous step and zero noise, and Fx(·) denotes the
Jacobian matrix of fk(·, ·) with respect to x and Fε(·) with
respect to the process noise ε.

We will also address the special case, where the dynamics
are entirely linear, that is fk(x) = Ak. In that case the ﬁlter

xk+1 = Axk + εk

(12)

where εk ∼ N(0, Qt), and xk := x(tk).

The orientation of the camera is encoded in a unit quater-
nion, unit quaternions are a direct representation of an axis-
angle rotation and can be converted into a rotation matrix using
Rodriguez formula.

where the linear dynamics are given by

III. EXPERIMENTS

Given a unit quaternion q that represents a rotation and
a known angular rate ω in the same frame of reference, its
derivative can be expressed as

where

dq(t)
dt

1
2

=

Ω(ω) q(t),

Ω(ω) =

(cid:18) 0 −ωT
ω [ω]×

(cid:19)

.

The notation [w]× is the 3×3 cross-product matrix (for further
details on quaternion modeling, see, e.g., [17]).

Assuming constant rotation rate (during ∆t), the discrete-

time system is

qk+1 = exp

(cid:18) ∆tk Ω(ωk)
2

(cid:19)

qk.

Since the gyroscope produces rotational rate measurements
with known accuracy, it can be propagated into an uncertainty
for the quaternion. The gyroscope data is used directly in the
prediction as a control signal in a linear Kalman ﬁlter.

Putting it all together, the state dynamics are described by

fk(xk, εk) = Akx + εk,

(16)

(13)

(14)

(15)

Ak =

I3

I3∆tk
I3

I3









exp( ∆tk

2 Ω(ωk − ωb))

and

εk ∼ N(0, Q), Q = blkdiag (cid:0)03 Qt Qq 03

(cid:1) ,

(18)

where ωb is the gyroscope bias (estimated off-line) and the Qt
and Qq matrices model the process noise of the translation and
rotation, respectively.

The process noise of the translation is derived from the
wiener velocity model described above, the process noise of
the rotation is propagated from the rotational rate.

D. Visual Update

The visual update couples all of the state variables. In Figure 2
the visual update occurs every time a new frame has been
acquired. The frame is ﬁrst processed by the feature tracker
and inlier detection, and then the feature pixel coordinates are
passed to the visual update model which processes an extended
Kalman ﬁlter update step.

The features are chosen by the Shi–Tomasi Good features
to track method [25] which determines strong corners in the
image. These features are tracked across frames by a pyramidal
Lucas–Kanade tracker [3, 20]. The Seven-point algorithm [9]
is used for inlier detection. If a feature is recognized as an
inlier, the visual update directly proceeds for the feature. If the
feature is not an inlier, the feature is replaced in the state by
re-initializing its current position z(i) estimate (both in terms
of state mean and covariance) to uninformative a priori values.

TABLE I
RMSE ERRORS PER METHOD AND PARAMETER.

Variable

Initial

Batch opt.

Jia and Evans [15]

Proposed

fx (px)
fy (px)
cx (px)
cy (px)
k1
k2

75
75
0.5
0.5
0.01
0.01

0.05
0.05
0.02
0.10
0.0001
0.0095

15.19
15.19
1.91
2.09
—
—

0.36
0.38
0.27
0.34
0.0001
0.0095

The measurement model function h(·) is a function of all
state variables. The measurement vector y ∈ R2d contains the
pixel coordinates, and feature-wise the model can be written
as

y(i) = h(i)(x) = distort

K(c) E(p, q)

(cid:20)

(cid:18)z(i)
1

(cid:19)

(cid:21)

, c

,

(19)

for i = 1, 2, . . . , d, where d is the number of features being
tracked. The measurement model is fully determined by the
the
camera model
Jacobian matrix of Eq. (19) must be formed as well. The
measurement noise is set to Σ = (2.5 px)2 I.

in Eqs. (1)–(4). For the EKF update,









I3

(17)

We demonstrate our self-calibration method both in a simula-
tion setup with known ground-truth values, and empirically on
real data. All the experiments runs were performed on the data
off-line using a Mathworks MATLAB implementation1, and
benchmarked against the publicly available MATLAB imple-
mentation by Jia and Evans [15]. Furthermore, we compared
our approach to a bundle-adjustment setup, which was used
for checking how well the other methods actually were able
to perform.

A. Simulation Study

A simulation similar to the one described in [15] was per-
formed to compare the results. To evaluate the methods, 100
camera movement paths around a three-dimensional regular
point structure were simulated in a Monte Carlo setup. The
point coordinates were projected into a virtual camera that
followed the paths and continuously turned to look at the
point structure. The ground-truth camera model was a zero-
skew square pixel camera with a focal length of 575 pixels, an
image size of 480 × 640 pixels, with the origin at the center
of the frame. The distortion parameters were set to be zero in
the simulation.

The simulation setup produced tracks for the visual features
and gyroscope readings, at 10 Hz and 100 Hz, respectively.
The initial state estimates corresponding to the camera pa-
rameters were set as follows. The initial guess for the origin
of the image plane and focal length were the image center
coordinates and 700 px, respectively.

Table I shows the average RMSE results over the 100
simulations for the initial state and three methods run on

1Code available online:

https://github.com/AaltoVision/camera-gyro-calibration

)
x
p
(

x
f

)
x
p
(

y
f

)
x
p
(

x
c

)
x
p
(

y
c

1000

800

600

400

200
1000

800

600

400

200

400

350

300

250

200

400

350

300

250

200

1
k

2
k

0.4
0.2
0
−0.2
−0.4

0.4
0.2
0
−0.2
−0.4

0

10

20

40

50

60

30
Time (s)

Fig. 3. Evolution of the camera calibration when started from distant
initial values in one Monte Carlo simulation. Initial convergence is reached
immediately after the ﬁrst image pair has been observed, and ﬁnal convergence
once the camera has moved sufﬁciently. The shaded regions depict the 95%
uncertainty interval given by the state-space estimation.

the simulated data. The ﬁrst column shows the initial RMSE
from the initialized parameters. We compare the calibration
parameters acquired from three models. The ﬁrst is the model
proposed, which was initialized and run as described earlier
in the Methods section.

To analyze the observability of the parameters in the simu-
lated data, we provide results also for a bundle-adjusted batch
solution (‘Batch opt.’ in Table I), which acts as a brute-force
baseline for the methods. For this baseline the camera pa-
rameters and motion track estimated by our proposed method
were used for initializing the bundle adjustment (non-linear
minimization of the reprojection error). The bundle adjustment
problem was solved by constrained minimization using the
MATLAB fmincon Interior Point Algorithm. This bundle
adjustment approach was computationally very intensive, but

TABLE II
PARAMETER VALUES IN EMPIRICAL EXPERIMENTS.

Variable

Initial

Final

Ground-truth

fx (px)
fy (px)
cx (px)
cy (px)
k1
k2

700
700
240
320
0
0

568.12
570.64
238.11
325.09
0.0429
−0.0040

570.56
569.72
238.03
323.87
0.1134
−0.0634

the results show that the data is informative about the param-
eter values, as the optimization converges to the ground-truth
values.

The third method was the method by Jia and Evans [15]. We
used their autocalibration toolbox2 implementation for solving
the self-calibration problem. As can be seen in the results,
there were issues in running these results. As the method by Jia
and Evans assumes there to be a lot of features available, we
re-ran the results by including almost ten time more features
in the simulation. The method keeps oscillating around the
correct camera parameter values, but fails to converge. We
suspect that the issues are partly related to the use case not
being optimal for their method, and partly because of issues
in their implementation of the method.

Figure 3 shows the evolution of the camera calibration,
when started from distant initial values. The shaded regions
depict the 95% uncertainty interval given by the state-space
estimation. The model reaches initial convergence immediately
after the ﬁrst image pair has been observed, and ﬁnal conver-
gence once the camera has moved sufﬁciently.Note that k2
does not show the convergence of the other parameters. The
main reason is that for this case the features are around the
center of the image at the beginning, where the r from 4 is
small and thus the gradient with respect to k2 is small. For the
synthetic case, the distortion parameters are an over-model.

The results show that the proposed method has an RMSE
less than one in pixel units for the intrinsic parameters,
not far from the results given brute-force calculated bundle-
adjustment based calibration results. The method by Jia and
Evans is clearly off more, but still did not diverge.

B. Empirical Tests

In order to demonstrate the proposed online calibration ap-
proach on empirical data, we acquired test data in various sit-
uations using an Apple iPad Pro 12.9-inch model. Hardware-
wise the iPad can be seen as a representative example of
a modern-day smart-device. Similar sensors are available in
most Andoroid and iOS smartphones and tablets.

The data acquisition was conducted using a custom data
capture app implemented in Objective-C. The capture tool
app stored the three-axis gyroscope together with associated
timestamps to a ﬁle in the device. The gyroscope sampling rate
was set to 100 Hz. The gyroscope was pre-calibrated before

2Online Camera-Gyroscope Auto-Calibration for Cellphones:
http://users.ece.utexas.edu/∼bevans/papers/2015/autocalibration/

(a) Cards—A dark indoor data set

(b) Cups—A feature-poor data set

1000

)
x
p
(

x
f

800

600

400
1000

)
x
p
(

y
f

)
x
p
(

x
c

)
x
p
(

y
c

800

600

400

500

400

300

200

100

500

400

300

200

100

4
2
0
−2
−4

2
k

4
2
0
−2
−4

(c) Signs—Overexposed outdoor data set

Fig. 4. Example frames from the empirical tests: (a) is underexposed and with
uneven concentration of features, (b) is from a feature-poor indoor scene, and
(c) is an overexposed outdoor scene.

1
k

the data acquisition for estimating the additive gyroscope bias
ωb.

Simultaneously to the gyroscope capture, device camera
frames were read time-locked to the gyroscope observations.
The experiments use the rear-facing camera with a resolution
of 480 × 640 (portrait orientation), grayscale images, exposure
time 1/60, ﬁxed aperture, sensitivity (ISO value) 125, and
locked focus at inﬁnity. The camera refresh rate was 10 fps
(Hz). The camera frames were stored as H.264 packed video
sequences on the device, with exact frame timestamps stored
separately for use in the ofﬂine run.

Furthermore, camera images of the canonical OpenCV
checkerboard pattern were captured for conducting batch cali-
bration of the tablet camera. This calibration was only used for
obtaining ground-truth camera parameters to compare against.
For obtaining a set of verstaile use cases, we recorded short
sequences of a few controlled static scenes. In these data sets
the motion of the camera was smooth in the sense of avoiding
hard stops, in order to comply with the constrains of the model.
Figure 4 shows example frames from the empirical data sets.
They include (a) ill-lit (underexposed) indoor scenes (we name
this data set ‘cards’), (b) visual feature poor scenes (typical in
ofﬁce environments), and (c) overexposed outdoor scenes.

The evolution of the proposed calibration method is shown
in Figure 5, where the shaded area represents the 95% uncer-
tainty interval. The corresponding calibration result is shown
in Table II together with the initial values and the checkerboard
calibrated ground-truth for the device camera.

0

5

10

15

20

25

30

35

Time (s)

Fig. 5. Evolution of the camera calibration in the ‘Cards’ data set, when
the 95%
started from distant
uncertainty interval given by the state-space estimation. The convergence
occurs once sufﬁcient excitation movement has occurred.

initial values. The shaded regions depict

For this run, the evolution of the position estimates and
the orientation (transformed into Euler angles) is visualized in
Figure 6. Subﬁgure (a) shows the latent (unobserved) camera
position track estimates (the shaded area represents the 95%
credible interval) over the time-span of the data. The absolute
scale of the movement remains unobserved, which is due
to the gyroscope only observing the rotation rate and the
camera data being agnostic to the true distance of any feature
movement. Subﬁgure (b) shows the corresponding orientation
states, which coincide more clearly with the behavior in
Figure 5. The linear parameters appear to reach the right
regime after the camera has been rotated sufﬁciently (i.e. the
data features sufﬁcient excitation). The non-linear distortion
parameters take longer to stabilize.

Subﬁgure 6(c) shows the corresponding input frames in the
cards data set with challenging lighting conditions. The green

)
.
u
.
a
(

n
o
i
t
i
s
o
P

5

0

5
−

s
e
l
g
n
a

n
o
i
t
a
t
n
e
i
r

O

4
/
π
3
2
/
π
4
/
π

0

4
/
π
−

0

x

y

z

0

5

10

15

20

30

35

40

45

50

25
Time (seconds)

(a) Latent camera position state estimates

5

10

15

20

30

35

40

45

50

25
Time (seconds)

(b) Camera orientation estimates

t = 0 s

t = 5 s

t = 10 s

t = 15 s

t = 20 s

t = 25 s

t = 30 s

t = 35 s

t = 40 s

t = 45 s

(c) Associated frames at different time points

Fig. 6. State estimates and example input frames for the parameter estimate track results in Fig. 5. Subﬁgure (a) shows the latent (unobserved) camera position
track estimates (the shaded area represents the 95% credible interval) over the time-span of the data. Note that the absolute scale of the movement remains
unobserved. (b) The corresponding orientation states, which coincide more clearly with the behavior in Fig. 5. (c) Example input frames in the cards data set
with green markers showing the tracked feature positions. Observation times correspond to the tick marks in the above plots.

markers show the current feature point locations in the frames,
and their ‘tails’ (green line) show the point movement since
the previous frame (frames sampled at 10 Hz).

For the intrinsic parameters, the estimated parameter values
converge within a few pixels to the checkerboard-calibrated
ground-truth values. For the non-linear radial distortion pa-
rameters the values are also similar. In case of the distortion
parameters, the identiﬁcation might suffer from the low feature
concentration towards the edges of the visual frame data.

IV. DISCUSSION AND CONCLUSION

In this paper we have proposed a method for estimating the
intrinsic parameters and lens distortion coefﬁcients for camera
calibration. This paper proposed a model for estimating the
parameters on the ﬂy by fusing gyroscope and camera data,
where the model is based on joint estimation of visual feature
positions, camera parameters, and the camera pose, the rotation
of which is assumed to follow the movement predicted by the
gyroscope.

The estimation procedure is lightweight and performs on-
line using an extended Kalman ﬁlter. The strengths of the
method is in robustness to feature-poor visual environments
and insensitivity to initialization, the aspects which were also

demonstrated in the experiments on simulated and empirical
data.

The experiments showed that the method performs well
against the current state-of-the-art in gyroscope-aided self-
calibration. The results showed that after sufﬁcient motion the
convergence is both quick, and it is easy to capture the moment
of convergence by monitoring the state variance estimates for
the camera parameters.

The empirical tests demonstrated the method using data
captured using an Apple iPad Pro. The empirical data the
parameter estimates converged rapidly after the system had
experienced sufﬁcient motion required for jointly estimating
both the camera poses and feature world coordinates.

Figure 4 shows three distinctive data sets; one is underex-
posed and with uneven concentration of features, one is from
a feature-poor indoor scene, and the third is from an overex-
posed outdoor scene. The proposed method is not sensitive to
feature-poor environments as it can rely on a relatively low
number of features being tracked—in the simulations only 27
features were used.

This is a clear difference to previous methods, which
have been mostly inspired by building on purely machine
vision methods. Requiring hundreds of tracked features to

be available works well in controlled environments, in good
lighting conditions, and using high-quality camera hardware.
Our method focuses on the opposite—low quality data and
a low number of features sufﬁces. On the other hand, this
comes with a requirement of the features remaining visible
for a sufﬁciently long time period.

The method could be extended to include further parame-
ters. Natural directions of extension would be to also estimate
the additive three-axis gyroscope bias, or include further
camera parameters such as rolling-shutter timing parameters.
As the method jointly estimates both camera poses, param-
eters, and feature positions, it can be sensitive to certain use
cases and environments. For example, in Figure 4(b) the cups
are round and their corresponding features are often lost and
picked up again during movement. Short feature tracks inject
uncertainty into the estimation scheme in the proposed method,
which slows down convergence and misleads the estimation.
This could be improved by tuning the method parameters, or
introducing visual loop-closures which would reuse the same
features when they appear again.

Code implementing the method is available online:

https://aaltovision.github.io/camera-gyro-calibration/

ACKNOWLEDGMENTS

This research was supported by the Academy of Finland grants
308640, 277685, and 295081.

REFERENCES
[1] S. ´Alvarez, D. Llorca, and M. Sotelo. Hierarchical camera auto-
calibration for trafﬁc surveillance systems. Expert Systems with
Applications, 41(4):1532–1542, 2014.

[2] Y. Bar-Shalom, X.-R. Li, and T. Kirubarajan. Estimation with
Applications to Tracking and Navigation. Wiley-Interscience,
New York, 2001.

[3] J.-Y. Bouguet. Pyramidal implementation of the afﬁne Lucas
Kanade feature tracker: Description of the algorithm. Technical
report, Intel Corporation, 2001.

[4] J. Civera, D. R. Bueno, A. J. Davison, and J. Montiel. Camera
self-calibration for sequential Bayesian structure from mo-
tion. In International Conference on Robotics and Automation
(ICRA), pages 403–408. IEEE, 2009.

[5] F. Faion, P. Ruoff, A. Zea, and U. D. Hanebeck. Recursive
bayesian calibration of depth sensors with non-overlapping
In Proceedings of the 15th International Conference
views.
on Information Fusion (FUSION), pages 757–762, July 2012.
[6] O. D. Faugeras, Q.-T. Luong, and S. J. Maybank. Camera self-
calibration: Theory and experiments. In European Conference
on Computer Vision (ECCV), pages 321–334, 1992.

[7] M. Goldshtein, Y. Oshman, and T. Efrati. Seeker gyro cali-
bration via model-based fusion of visual and inertial data. In
Proceedings of the 10th International Conference on Informa-
tion Fusion (FUSION), pages 1–8, July 2007.

[8] N. S. Gopaul, J. Wang, and B. Hu. Camera auto-calibration
in GPS/INS/stereo camera integrated kinematic positioning and
Journal of Global Positioning Systems,
navigation system.
14(1):3, 2016.

[9] R. I. Hartley and A. Zisserman. Multiple View Geometry in
Computer Vision. Cambridge University Press, second edition,
2004.

[10] A. Heidari, I. Alaei-Novin, and P. Aarabi. Fusion of spatial and
visual information for object tracking on iPhone. In Proceedings
of the 16th International Conference on Information Fusion
(FUSION), pages 630–637, July 2013.

[11] D. Herrera, C. J. Kannala, and J. Heikkila. Forget the checker-
board: Practical self-calibration using a planar scene. In Winter
Conference on Applications of Computer Vision (WACV), pages
1–9, 2016.

[12] M. Hwangbo, J.-S. Kim, and T. Kanade. Gyro-aided feature
tracking for a moving camera: Fusion, auto-calibration and
GPU implementation. The International Journal of Robotics
Research, 30(14):1755–1774, 2011.

[13] A. H. Jazwinski. Stochastic Processes and Filtering Theory.

Academic Press, New York, 1970.

[14] C. Jia and B. L. Evans. Online calibration and synchroniza-
In Proceedings of
tion of cellphone camera and gyroscope.
the Global Conference on Signal and Information Processing
(GlobalSIP), pages 731–734. IEEE, 2013.

[15] C. Jia and B. L. Evans. Online camera-gyroscope autocalibra-
tion for cell phones. IEEE Transactions on Image Processing,
23(12):5070–5081, 2014.

[16] A. Karpenko, D. Jacobs, J. Baek, and M. Levoy. Digital video
stabilization and rolling shutter correction using gyroscopes.
Technical Report 2011-3, Stanford University, 2011.

[17] M. Kok. Probabilistic Modeling for Positioning Applications
PhD thesis, Link¨oping University,

Using Inertial Sensors.
Link¨oping, Sweden, 2014.

[18] F. Li, H. Sekkati, J. Deglint, C. Scharfenberger, M. Lamm,
D. Clausi, J. Zelek, and A. Wong. Simultaneous projector-
camera self-calibration for three-dimensional reconstruction and
IEEE Transactions on Computational
projection mapping.
Imaging, 3(1):74–83, March 2017.

[19] M. Li, H. Yu, X. Zheng, and A. I. Mourikis. High-ﬁdelity sensor
modeling and self-calibration in vision-aided inertial naviga-
tion. In International Conference on Robotics and Automation
(ICRA), pages 409–416. IEEE, 2014.

[20] B. D. Lucas and T. Kanade. An iterative image registration
technique with an application to stereo vision. In International
Conference on Artiﬁcial Intelligence (IJCAI), pages 674–679.
Vancouver, BC, Canada, 1981.

[21] J. Maye, P. Furgale, and R. Siegwart. Self-supervised calibration
In IEEE Intelligent Vehicles Symposium

for robotic systems.
(IV), pages 473–480, 2013.

[22] H. Ovr´en and P.-E. Forss´en. Gyroscope-based video stabili-
In International Conference on
sation with auto-calibration.
Robotics and Automation (ICRA), pages 2090–2097. IEEE,
2015.

[23] S. S¨arkk¨a. Bayesian Filtering and Smoothing. Cambridge

University Press, 2013.

[24] M. A. Shelley. Monocular visual

inertial odometry on a
mobile device. Master’s thesis, Technical University of Munich,
Germany, 2014.

[25] J. Shi and C. Tomasi. Good features to track. In Proceedings
of the IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR), pages 593–600, 1994.
[26] A. Solin, S. Cortes, E. Rahtu, and J. Kannala. Inertial odometry
on handheld smartphones. In Proceedings of the International
Conference on Information Fusion (FUSION), 2018.

[27] A. Solin, S. Cortes, E. Rahtu, and J. Kannala. PIVO: Proba-
bilistic inertial-visual odometry for occlusion-robust navigation.
In Proceedings of the IEEE Winter Conference on Applications
of Computer Vision (WACV), pages 616–625, 2018.

[28] J. Sun, P. Wang, Z. Qin, and H. Qiao. Effective self-calibration
for camera parameters and hand-eye geometry based on two
IEEE/CAA Journal of Automatica
feature points motions.
Sinica, 4(2):370–380, 2017.

[29] S. Willi and A. Grundh¨ofer. Robust geometric self-calibration
In International
of generic multi-projector camera systems.
Symposium on Mixed and Augmented Reality (ISMAR), pages
42–51, 2017.

Robust Gyroscope-Aided Camera Self-Calibration

Santiago Cort´es Reina
Department of Computer Science
Aalto University
Helsinki, Finland
santiago.cortesreina@aalto.ﬁ

Arno Solin
Department of Computer Science
Aalto University
Helsinki, Finland
arno.solin@aalto.ﬁ

Juho Kannala
Department of Computer Science
Aalto University
Helsinki, Finland
juho.kannala@aalto.ﬁ

8
1
0
2
 
y
a
M
 
1
3
 
 
]

V
C
.
s
c
[
 
 
1
v
6
0
5
2
1
.
5
0
8
1
:
v
i
X
r
a

Abstract—Camera calibration for estimating the intrinsic
parameters and lens distortion is a prerequisite for various
monocular vision applications including feature tracking and
video stabilization. This application paper proposes a model for
estimating the parameters on the ﬂy by fusing gyroscope and
camera data, both readily available in modern day smartphones.
The model is based on joint estimation of visual feature positions,
camera parameters, and the camera pose, the movement of
which is assumed to follow the movement predicted by the
gyroscope. Our model assumes the camera movement to be free,
but continuous and differentiable, and individual features are
assumed to stay stationary. The estimation is performed online
using an extended Kalman ﬁlter, and it is shown to outperform
existing methods in robustness and insensitivity to initialization.
We demonstrate the method using simulated data and empirical
data from an iPad.

I. INTRODUCTION

The growth in the market of smartphones and tablets has
brought monocular cameras to even the cheapest of smart-
devices. Simultaneously, the improved computational capa-
bilities have made it possible to expand their use into new
ﬁelds and use cases, such as video calls, payment veriﬁcation,
and augmented reality. However, employing the device camera
in pose estimation, video stabilization, or feature tracking
requires the camera calibration parameters to be known or
estimated.

On smart-devices, the use cases often provide an explicit
calibration step that includes capturing a pre-deﬁned calibra-
tion pattern from different positions. This procedure is the
base for traditional camera calibration (see, e.g., [9] for an
overview). Self-calibration is the problem of modeling the in-
ternal parameters of a camera (projection matrix and distortion
coefﬁcients) without using any known pattern. Luckily there
are usually additional sensors available on the devices, typi-
cally a low-cost MEMS inertial measurement unit (IMU). The
IMU typically provides fast-sampled (up to some hundreds of
Hz) readings of the speciﬁc force (accelerometer) and turn-
rate (gyroscope) in the device’s coordinate frame (see, e.g.,
[26] for a more thorough introduction).

In this work we propose a method for camera self-
calibration using the information from a gyroscope rigidly
attached to the camera. We use a structure from motion (SfM)
approach, where the camera model minimizes the reprojection
error over a set of images. The proposed method jointly esti-
mates the camera pose, tracked feature positions, and camera

Phone camera
position p
orientation q

Phone
gyroscope
with turn rate
ω [rad/s]

Camera parameters
(fx/y and cx/y)

Radial distortion
(k1 and k2)

Fig. 1. Illustration of the model setup: A pinhole camera with position p and
orientation quaternion q, auxiliary turn rate ω from the gyroscope. A set of
(unknown) feature locations {zi} are observed through a camera model with
linear and non-linear distortion parameters.

parameters. The method works online employing a state-space
formulation, where the fast-sampled gyroscope data is used for
forward-predicting the relative camera and feature movement,
and the visual data tracking results are then matched to the
predictions in a probabilistic fashion. The inference itself is
solved by an extended Kalman ﬁlter.

Due to its practical importance, camera self-calibration—
or auto-calibration—has been studied extensively during the
past decades. Outside smartphones, practical applications are
found, for example, in trafﬁc surveillance [1], projector camera
calibration [18, 29] and robotics for autonomous vehicles
[5, 7, 8, 10, 21, 28]. The seminal paper by Faugeras et al. [6]
introduced the concept of calibrating the intrinsic and extrinsic
parameters without a known calibration object or pattern.
Since then many methods have been introduced, building upon
special kind of motion (e.g. purely rotating or planar), special
scene geometry (e.g. planar scenes or special depth structure,
see [11] for discussion), or auxiliary sensors.

For example, Civera et al. [4] proposed a sum-of-Gaussians
ﬁlter approach building upon a SfM approach where there
has to be sufﬁcient translation. Additional sensors ease the
motion constraints. Using both a gyroscope and an accelerom-
eter for calibration, information can be acquired about the

relative rotation, absolute scale, and the world coordinate
frame orientation (see, e.g., [19]). Similar setups are often
employed in visual-inertial odometry methods (see, e.g., the
discussion in [24, 27]), where the same set of sensors are
used and the camera calibration is estimated as a part of the
inference. Gyroscopes are also used in video stabilization, and
[22] proposed a model for scaling, time offset, and relative
pose calibration using the gyroscope. They, however, do not
estimate the camera calibration parameters.

There are also other methods which only use the camera
and a gyroscope. Karpenko et al. [16] proposed a method for
calibration which uses a gyroscope together with a quick shake
when pointing to far-away objects, while Hwangbo et al. [12]
constrain the estimation by assuming pure rotation.

Within methods designed for fusing camera and gyroscope
data, we are only aware of one previously published approach,
which works in the same setting as the proposed method.
The method by Jia and Evans was ﬁrst published in [14] and
later reﬁned in [15]. Their method together with ours does
not assume special movement and only require visual and
gyroscope data.

The proposed model is based on the following assumptions:
(i) The camera movement is free, but continuous and differ-
entiable (rather smooth), (ii) The camera rotations follow the
gyroscope turn rate observations, (iii) The world coordinates
of individual features are unknown, but assumed constant
(individual features assumed to stay stationary). In comaprison
to [15], this method aims at providing higher robustness to
initialization and feature-poor enivironments with only a few
visual features being tracked. Where [15] uses a lot of short
(two-frame) connections of features, our method uses a few
long chains of connected features.

This paper is structured as follows. Section II presents the
theoretical methodology in detail starting from the camera
calibration model, the proposed state-space model, and ﬁnally
how to jointly infer all the unknown parameters. The Results
section shows the method employed both in a simulation
study and a on real-world data. Finally, the assumptions and
modeling problems and some future work are discussed.

II. METHODS

We start by deﬁning the notation used in the camera calibra-
tion model, which will later be used in speciﬁcation of the
measurement model. Then we proceed to setting up the state-
space model for the gyroscope-aided dynamics of the camera
pose, feature positions, and camera parameters. Finally, we
couple the forward dynamics with the image data in a visual
measurement model. A sketch of the information ﬂow in the
method is shown in Figure 2.

A. Camera Model

We build upon the well-established theory of monocular
camera calibration models. A detailed and extensive descrip-
tion of camera models can be found in [9]. Through the
camera model, points (x, y, z) in three-dimensional space are
projected to image plane coordinates (u, v) using a pinhole

Initial state

Gyroscope
prediction

k ← k+1

Wait for
gyro data

no

Is frame
available?
yes

Feature
tracking

Visual
update

Fig. 2. The information ﬂow in the self-calibration method. The gyroscope
prediction loop runs at 100 Hz, and visual updates occur once a new frame
is acquired (typically at 10 Hz).

camera model. First the points are rigidly transformed into
the camera reference frame, that is origin at the optical center,
the z-axis perpendicular to the image plane and y-axis parallel
to the vertical axis of the image. Then the points are projected
into the image plane using the pinhole projection. In matrix
form





u
v
 = K E
1











,





x
y
z
1

(1)

where the intrinsic matrix K ∈ R3×3 and extrinsic matrix
E ∈ R3×4 are parametrized as follows:

K =





fx
0
0

0
fy
0

cx
cy
1


 and E = (cid:0)RT −RTp(cid:1) .

(2)

length (fx and fy, separate
The parameters are the focal
between dimensions to account for non-square pixels), the
origin of the image plane (cx, cy), and the position p ∈ R3
and orientation R ∈ R3×3 of the camera in the global frame
of reference.

The projection operation is entirely linear (in homogeneous
space), but real-world lenses usually introduce non-linear
mappings. These are known as distortions, the most common
distortion is rotationally symmetric around the image center
and is modeled by so-called radial distortion coefﬁcients k1
and k2 as follows:
(cid:19)
(cid:18)u(cid:48)
v(cid:48)

(1 + k1 r2 + k2 r4),

(cid:18)u
v

(3)

=

(cid:19)

where the radial component is determined by

(cid:115)

r =

(cid:17)2

(cid:16) u − cx
fx

+

(cid:16) v − cy
fy

(cid:17)2

.

(4)

This camera model is shown in the illustrative sketch in
Figure 1, where also the resulting distortion effect is visible.
We refer to the non-linear distortion function as ‘distort’ later
in the paper.

B. State Estimation

In order to include time-dependency between observed frames
and rotational
information from the gyroscope, we deﬁne
a state-space model. The model describes a device with a

monocular camera and a rigidly attached gyroscope with a
known relative orientation between them (see Fig. 1). An
example of such a device is a modern smartphone. The
following section shows the non-linear ﬁler designed for
information fusion, the non-linear ﬁlter allows for estimation
and fusion while keeping track of the uncertainty and depen-
dencies between the non-deterministic processes describing
the evolution of the calibration parameters, camera pose, and
feature locations.

We deﬁne a state-space model (see, e.g., [23]), where the

state vector is

x = (cid:0)cT pT vT qT

zT(cid:1)T

.

(5)

The variable c = (fx, fy, cx, cy, k1, k2) contains the internal
camera parameters, p ∈ R3 and v ∈ R3 contain the position
and velocity of the camera, q contains the quaternion encoding
the orientation of the camera, and z ∈ R3d contains the
locations of the features (d is the number of features being
tracked).

The non-linear state-space model with the auxiliary gyro-
scope control signal ωk is given as follows. The control can
be embedded into the time-varying dynamical model such that
fk(x, εk) := f (x, ωk, εk). The state-space model takes the
canonical form:

xk = fk(xk−1, εk),
yk = hk(xk) + γk,

(6)

where xk ∈ Rn is the state at time step tk, k = 1, 2, . . ., yk ∈
Rm is a measurement, εk ∼ N(0, Qk) is the Gaussian process
noise, and γk ∼ N(0, Σk) is the Gaussian measurement noise.
The dynamics and measurements are speciﬁed in terms of the
dynamical model function f (·, ·, ·) and the measurement model
function hk(·).

In this work we employ the extended Kalman ﬁlter (EKF,
[2, 13]) which provides a means of approximating the state
distributions p(x | y1:k) with Gaussians:

p(xk | y1:k) (cid:39) N(xk | mk|k, Pk|k).

(7)

In the EKF, these approximations are formed by ﬁrst-order
linearizations of the non-linearities. The extended Kalman
ﬁltering recursion can be written as follows (see [23] for
detailed presentation). The dynamics are incorporated into the
prediction step:

mk|k−1 = fk(mk−1|k−1, 0),
Pk|k−1 = Fx(mk−1|k−1) Pk−1|k−1 FT

Fε(mk−1|k−1) Qk FT

ε (mk−1|k−1),

prediction step is entirely given by the standard Kalman ﬁlter
preduction step:

mk|k−1 = Ak mk−1|k−1,
Pk|k−1 = Ak Pk−1|k−1 AT

k + Qk.

(9)

Measurement data providing observations of the system
state at given time steps are combined with the model in the
update step:

vk = yk − hk(mk|k−1),
Sk = Hx(mk|k−1) Pk|k−1 HT
x(mk|k−1) + Σk,
x(mk|k−1) S−1
Kk = Pk|k−1 HT
k ,

mk|k = mk|k−1 + Kk vk,
Pk|k = [I − Kk Hx(mk|k−1)] Pk|k−1

[I − Kk Hx(mk|k−1)]T + Kk Σk KT
k ,

(10)

where Hx(·) denotes the Jacobian of the measurement model
hk(·) with respect
to the state variables x. The slightly
unorthodox form of the last line is known as the Joseph’s
formula, which both numerically stabilizes updating the co-
variance and preserves symmetry.

The linearizations inside the extended Kalman ﬁlter cause
some errors in the estimation. Most notably the estima-
tion scheme does not preserve the norm of the orientation
quaternions. Therefore after each update an extra quaternion
normalization step is added to the estimation scheme.

C. Propagation by Gyroscope Prediction

The state holds the internal and external parameters of the
camera and the 3D position of the features. The internal
parameters and the 3D coordinates of the features are assumed
to stay constant (but are still unknown), so their propagation
functions are identities. The external parameters (position,
orientation) of the camera are not constant and are treated
differently.

The position of the camera is modeled as a Wiener velocity
process, a commonly used model
in tracking and control
literature (see, e.g., [23] for details). In order to keep track of
the full inertial state the estimate contains both the position and
velocity vectors x = (cid:0)p v(cid:1)T
and the acceleration is modeled
as a white noise process d2x(t)
dt2 = w(t), or in state space
form as a linear time-invariant stochastic differential equation
(independently for each spatial dimension)
(cid:19)
(cid:18)0
1

(cid:18)0
0

(cid:19)
1
0

dx(t)
dt

x(t) +

w(t),

(11)

=

x(mk−1|k−1)+

(8)

where w(t) is a realization of the white noise process. In
discrete time the system is

where the dynamic model
is evaluated with the outcome
from the previous step and zero noise, and Fx(·) denotes the
Jacobian matrix of fk(·, ·) with respect to x and Fε(·) with
respect to the process noise ε.

We will also address the special case, where the dynamics
are entirely linear, that is fk(x) = Ak. In that case the ﬁlter

xk+1 = Axk + εk

(12)

where εk ∼ N(0, Qt), and xk := x(tk).

The orientation of the camera is encoded in a unit quater-
nion, unit quaternions are a direct representation of an axis-
angle rotation and can be converted into a rotation matrix using
Rodriguez formula.

where the linear dynamics are given by

III. EXPERIMENTS

Given a unit quaternion q that represents a rotation and
a known angular rate ω in the same frame of reference, its
derivative can be expressed as

where

dq(t)
dt

1
2

=

Ω(ω) q(t),

Ω(ω) =

(cid:18) 0 −ωT
ω [ω]×

(cid:19)

.

The notation [w]× is the 3×3 cross-product matrix (for further
details on quaternion modeling, see, e.g., [17]).

Assuming constant rotation rate (during ∆t), the discrete-

time system is

qk+1 = exp

(cid:18) ∆tk Ω(ωk)
2

(cid:19)

qk.

Since the gyroscope produces rotational rate measurements
with known accuracy, it can be propagated into an uncertainty
for the quaternion. The gyroscope data is used directly in the
prediction as a control signal in a linear Kalman ﬁlter.

Putting it all together, the state dynamics are described by

fk(xk, εk) = Akx + εk,

(16)

(13)

(14)

(15)

Ak =

I3

I3∆tk
I3

I3









exp( ∆tk

2 Ω(ωk − ωb))

and

εk ∼ N(0, Q), Q = blkdiag (cid:0)03 Qt Qq 03

(cid:1) ,

(18)

where ωb is the gyroscope bias (estimated off-line) and the Qt
and Qq matrices model the process noise of the translation and
rotation, respectively.

The process noise of the translation is derived from the
wiener velocity model described above, the process noise of
the rotation is propagated from the rotational rate.

D. Visual Update

The visual update couples all of the state variables. In Figure 2
the visual update occurs every time a new frame has been
acquired. The frame is ﬁrst processed by the feature tracker
and inlier detection, and then the feature pixel coordinates are
passed to the visual update model which processes an extended
Kalman ﬁlter update step.

The features are chosen by the Shi–Tomasi Good features
to track method [25] which determines strong corners in the
image. These features are tracked across frames by a pyramidal
Lucas–Kanade tracker [3, 20]. The Seven-point algorithm [9]
is used for inlier detection. If a feature is recognized as an
inlier, the visual update directly proceeds for the feature. If the
feature is not an inlier, the feature is replaced in the state by
re-initializing its current position z(i) estimate (both in terms
of state mean and covariance) to uninformative a priori values.

TABLE I
RMSE ERRORS PER METHOD AND PARAMETER.

Variable

Initial

Batch opt.

Jia and Evans [15]

Proposed

fx (px)
fy (px)
cx (px)
cy (px)
k1
k2

75
75
0.5
0.5
0.01
0.01

0.05
0.05
0.02
0.10
0.0001
0.0095

15.19
15.19
1.91
2.09
—
—

0.36
0.38
0.27
0.34
0.0001
0.0095

The measurement model function h(·) is a function of all
state variables. The measurement vector y ∈ R2d contains the
pixel coordinates, and feature-wise the model can be written
as

y(i) = h(i)(x) = distort

K(c) E(p, q)

(cid:20)

(cid:18)z(i)
1

(cid:19)

(cid:21)

, c

,

(19)

for i = 1, 2, . . . , d, where d is the number of features being
tracked. The measurement model is fully determined by the
the
camera model
Jacobian matrix of Eq. (19) must be formed as well. The
measurement noise is set to Σ = (2.5 px)2 I.

in Eqs. (1)–(4). For the EKF update,









I3

(17)

We demonstrate our self-calibration method both in a simula-
tion setup with known ground-truth values, and empirically on
real data. All the experiments runs were performed on the data
off-line using a Mathworks MATLAB implementation1, and
benchmarked against the publicly available MATLAB imple-
mentation by Jia and Evans [15]. Furthermore, we compared
our approach to a bundle-adjustment setup, which was used
for checking how well the other methods actually were able
to perform.

A. Simulation Study

A simulation similar to the one described in [15] was per-
formed to compare the results. To evaluate the methods, 100
camera movement paths around a three-dimensional regular
point structure were simulated in a Monte Carlo setup. The
point coordinates were projected into a virtual camera that
followed the paths and continuously turned to look at the
point structure. The ground-truth camera model was a zero-
skew square pixel camera with a focal length of 575 pixels, an
image size of 480 × 640 pixels, with the origin at the center
of the frame. The distortion parameters were set to be zero in
the simulation.

The simulation setup produced tracks for the visual features
and gyroscope readings, at 10 Hz and 100 Hz, respectively.
The initial state estimates corresponding to the camera pa-
rameters were set as follows. The initial guess for the origin
of the image plane and focal length were the image center
coordinates and 700 px, respectively.

Table I shows the average RMSE results over the 100
simulations for the initial state and three methods run on

1Code available online:

https://github.com/AaltoVision/camera-gyro-calibration

)
x
p
(

x
f

)
x
p
(

y
f

)
x
p
(

x
c

)
x
p
(

y
c

1000

800

600

400

200
1000

800

600

400

200

400

350

300

250

200

400

350

300

250

200

1
k

2
k

0.4
0.2
0
−0.2
−0.4

0.4
0.2
0
−0.2
−0.4

0

10

20

40

50

60

30
Time (s)

Fig. 3. Evolution of the camera calibration when started from distant
initial values in one Monte Carlo simulation. Initial convergence is reached
immediately after the ﬁrst image pair has been observed, and ﬁnal convergence
once the camera has moved sufﬁciently. The shaded regions depict the 95%
uncertainty interval given by the state-space estimation.

the simulated data. The ﬁrst column shows the initial RMSE
from the initialized parameters. We compare the calibration
parameters acquired from three models. The ﬁrst is the model
proposed, which was initialized and run as described earlier
in the Methods section.

To analyze the observability of the parameters in the simu-
lated data, we provide results also for a bundle-adjusted batch
solution (‘Batch opt.’ in Table I), which acts as a brute-force
baseline for the methods. For this baseline the camera pa-
rameters and motion track estimated by our proposed method
were used for initializing the bundle adjustment (non-linear
minimization of the reprojection error). The bundle adjustment
problem was solved by constrained minimization using the
MATLAB fmincon Interior Point Algorithm. This bundle
adjustment approach was computationally very intensive, but

TABLE II
PARAMETER VALUES IN EMPIRICAL EXPERIMENTS.

Variable

Initial

Final

Ground-truth

fx (px)
fy (px)
cx (px)
cy (px)
k1
k2

700
700
240
320
0
0

568.12
570.64
238.11
325.09
0.0429
−0.0040

570.56
569.72
238.03
323.87
0.1134
−0.0634

the results show that the data is informative about the param-
eter values, as the optimization converges to the ground-truth
values.

The third method was the method by Jia and Evans [15]. We
used their autocalibration toolbox2 implementation for solving
the self-calibration problem. As can be seen in the results,
there were issues in running these results. As the method by Jia
and Evans assumes there to be a lot of features available, we
re-ran the results by including almost ten time more features
in the simulation. The method keeps oscillating around the
correct camera parameter values, but fails to converge. We
suspect that the issues are partly related to the use case not
being optimal for their method, and partly because of issues
in their implementation of the method.

Figure 3 shows the evolution of the camera calibration,
when started from distant initial values. The shaded regions
depict the 95% uncertainty interval given by the state-space
estimation. The model reaches initial convergence immediately
after the ﬁrst image pair has been observed, and ﬁnal conver-
gence once the camera has moved sufﬁciently.Note that k2
does not show the convergence of the other parameters. The
main reason is that for this case the features are around the
center of the image at the beginning, where the r from 4 is
small and thus the gradient with respect to k2 is small. For the
synthetic case, the distortion parameters are an over-model.

The results show that the proposed method has an RMSE
less than one in pixel units for the intrinsic parameters,
not far from the results given brute-force calculated bundle-
adjustment based calibration results. The method by Jia and
Evans is clearly off more, but still did not diverge.

B. Empirical Tests

In order to demonstrate the proposed online calibration ap-
proach on empirical data, we acquired test data in various sit-
uations using an Apple iPad Pro 12.9-inch model. Hardware-
wise the iPad can be seen as a representative example of
a modern-day smart-device. Similar sensors are available in
most Andoroid and iOS smartphones and tablets.

The data acquisition was conducted using a custom data
capture app implemented in Objective-C. The capture tool
app stored the three-axis gyroscope together with associated
timestamps to a ﬁle in the device. The gyroscope sampling rate
was set to 100 Hz. The gyroscope was pre-calibrated before

2Online Camera-Gyroscope Auto-Calibration for Cellphones:
http://users.ece.utexas.edu/∼bevans/papers/2015/autocalibration/

(a) Cards—A dark indoor data set

(b) Cups—A feature-poor data set

1000

)
x
p
(

x
f

800

600

400
1000

)
x
p
(

y
f

)
x
p
(

x
c

)
x
p
(

y
c

800

600

400

500

400

300

200

100

500

400

300

200

100

4
2
0
−2
−4

2
k

4
2
0
−2
−4

(c) Signs—Overexposed outdoor data set

Fig. 4. Example frames from the empirical tests: (a) is underexposed and with
uneven concentration of features, (b) is from a feature-poor indoor scene, and
(c) is an overexposed outdoor scene.

1
k

the data acquisition for estimating the additive gyroscope bias
ωb.

Simultaneously to the gyroscope capture, device camera
frames were read time-locked to the gyroscope observations.
The experiments use the rear-facing camera with a resolution
of 480 × 640 (portrait orientation), grayscale images, exposure
time 1/60, ﬁxed aperture, sensitivity (ISO value) 125, and
locked focus at inﬁnity. The camera refresh rate was 10 fps
(Hz). The camera frames were stored as H.264 packed video
sequences on the device, with exact frame timestamps stored
separately for use in the ofﬂine run.

Furthermore, camera images of the canonical OpenCV
checkerboard pattern were captured for conducting batch cali-
bration of the tablet camera. This calibration was only used for
obtaining ground-truth camera parameters to compare against.
For obtaining a set of verstaile use cases, we recorded short
sequences of a few controlled static scenes. In these data sets
the motion of the camera was smooth in the sense of avoiding
hard stops, in order to comply with the constrains of the model.
Figure 4 shows example frames from the empirical data sets.
They include (a) ill-lit (underexposed) indoor scenes (we name
this data set ‘cards’), (b) visual feature poor scenes (typical in
ofﬁce environments), and (c) overexposed outdoor scenes.

The evolution of the proposed calibration method is shown
in Figure 5, where the shaded area represents the 95% uncer-
tainty interval. The corresponding calibration result is shown
in Table II together with the initial values and the checkerboard
calibrated ground-truth for the device camera.

0

5

10

15

20

25

30

35

Time (s)

Fig. 5. Evolution of the camera calibration in the ‘Cards’ data set, when
the 95%
started from distant
uncertainty interval given by the state-space estimation. The convergence
occurs once sufﬁcient excitation movement has occurred.

initial values. The shaded regions depict

For this run, the evolution of the position estimates and
the orientation (transformed into Euler angles) is visualized in
Figure 6. Subﬁgure (a) shows the latent (unobserved) camera
position track estimates (the shaded area represents the 95%
credible interval) over the time-span of the data. The absolute
scale of the movement remains unobserved, which is due
to the gyroscope only observing the rotation rate and the
camera data being agnostic to the true distance of any feature
movement. Subﬁgure (b) shows the corresponding orientation
states, which coincide more clearly with the behavior in
Figure 5. The linear parameters appear to reach the right
regime after the camera has been rotated sufﬁciently (i.e. the
data features sufﬁcient excitation). The non-linear distortion
parameters take longer to stabilize.

Subﬁgure 6(c) shows the corresponding input frames in the
cards data set with challenging lighting conditions. The green

)
.
u
.
a
(

n
o
i
t
i
s
o
P

5

0

5
−

s
e
l
g
n
a

n
o
i
t
a
t
n
e
i
r

O

4
/
π
3
2
/
π
4
/
π

0

4
/
π
−

0

x

y

z

0

5

10

15

20

30

35

40

45

50

25
Time (seconds)

(a) Latent camera position state estimates

5

10

15

20

30

35

40

45

50

25
Time (seconds)

(b) Camera orientation estimates

t = 0 s

t = 5 s

t = 10 s

t = 15 s

t = 20 s

t = 25 s

t = 30 s

t = 35 s

t = 40 s

t = 45 s

(c) Associated frames at different time points

Fig. 6. State estimates and example input frames for the parameter estimate track results in Fig. 5. Subﬁgure (a) shows the latent (unobserved) camera position
track estimates (the shaded area represents the 95% credible interval) over the time-span of the data. Note that the absolute scale of the movement remains
unobserved. (b) The corresponding orientation states, which coincide more clearly with the behavior in Fig. 5. (c) Example input frames in the cards data set
with green markers showing the tracked feature positions. Observation times correspond to the tick marks in the above plots.

markers show the current feature point locations in the frames,
and their ‘tails’ (green line) show the point movement since
the previous frame (frames sampled at 10 Hz).

For the intrinsic parameters, the estimated parameter values
converge within a few pixels to the checkerboard-calibrated
ground-truth values. For the non-linear radial distortion pa-
rameters the values are also similar. In case of the distortion
parameters, the identiﬁcation might suffer from the low feature
concentration towards the edges of the visual frame data.

IV. DISCUSSION AND CONCLUSION

In this paper we have proposed a method for estimating the
intrinsic parameters and lens distortion coefﬁcients for camera
calibration. This paper proposed a model for estimating the
parameters on the ﬂy by fusing gyroscope and camera data,
where the model is based on joint estimation of visual feature
positions, camera parameters, and the camera pose, the rotation
of which is assumed to follow the movement predicted by the
gyroscope.

The estimation procedure is lightweight and performs on-
line using an extended Kalman ﬁlter. The strengths of the
method is in robustness to feature-poor visual environments
and insensitivity to initialization, the aspects which were also

demonstrated in the experiments on simulated and empirical
data.

The experiments showed that the method performs well
against the current state-of-the-art in gyroscope-aided self-
calibration. The results showed that after sufﬁcient motion the
convergence is both quick, and it is easy to capture the moment
of convergence by monitoring the state variance estimates for
the camera parameters.

The empirical tests demonstrated the method using data
captured using an Apple iPad Pro. The empirical data the
parameter estimates converged rapidly after the system had
experienced sufﬁcient motion required for jointly estimating
both the camera poses and feature world coordinates.

Figure 4 shows three distinctive data sets; one is underex-
posed and with uneven concentration of features, one is from
a feature-poor indoor scene, and the third is from an overex-
posed outdoor scene. The proposed method is not sensitive to
feature-poor environments as it can rely on a relatively low
number of features being tracked—in the simulations only 27
features were used.

This is a clear difference to previous methods, which
have been mostly inspired by building on purely machine
vision methods. Requiring hundreds of tracked features to

be available works well in controlled environments, in good
lighting conditions, and using high-quality camera hardware.
Our method focuses on the opposite—low quality data and
a low number of features sufﬁces. On the other hand, this
comes with a requirement of the features remaining visible
for a sufﬁciently long time period.

The method could be extended to include further parame-
ters. Natural directions of extension would be to also estimate
the additive three-axis gyroscope bias, or include further
camera parameters such as rolling-shutter timing parameters.
As the method jointly estimates both camera poses, param-
eters, and feature positions, it can be sensitive to certain use
cases and environments. For example, in Figure 4(b) the cups
are round and their corresponding features are often lost and
picked up again during movement. Short feature tracks inject
uncertainty into the estimation scheme in the proposed method,
which slows down convergence and misleads the estimation.
This could be improved by tuning the method parameters, or
introducing visual loop-closures which would reuse the same
features when they appear again.

Code implementing the method is available online:

https://aaltovision.github.io/camera-gyro-calibration/

ACKNOWLEDGMENTS

This research was supported by the Academy of Finland grants
308640, 277685, and 295081.

REFERENCES
[1] S. ´Alvarez, D. Llorca, and M. Sotelo. Hierarchical camera auto-
calibration for trafﬁc surveillance systems. Expert Systems with
Applications, 41(4):1532–1542, 2014.

[2] Y. Bar-Shalom, X.-R. Li, and T. Kirubarajan. Estimation with
Applications to Tracking and Navigation. Wiley-Interscience,
New York, 2001.

[3] J.-Y. Bouguet. Pyramidal implementation of the afﬁne Lucas
Kanade feature tracker: Description of the algorithm. Technical
report, Intel Corporation, 2001.

[4] J. Civera, D. R. Bueno, A. J. Davison, and J. Montiel. Camera
self-calibration for sequential Bayesian structure from mo-
tion. In International Conference on Robotics and Automation
(ICRA), pages 403–408. IEEE, 2009.

[5] F. Faion, P. Ruoff, A. Zea, and U. D. Hanebeck. Recursive
bayesian calibration of depth sensors with non-overlapping
In Proceedings of the 15th International Conference
views.
on Information Fusion (FUSION), pages 757–762, July 2012.
[6] O. D. Faugeras, Q.-T. Luong, and S. J. Maybank. Camera self-
calibration: Theory and experiments. In European Conference
on Computer Vision (ECCV), pages 321–334, 1992.

[7] M. Goldshtein, Y. Oshman, and T. Efrati. Seeker gyro cali-
bration via model-based fusion of visual and inertial data. In
Proceedings of the 10th International Conference on Informa-
tion Fusion (FUSION), pages 1–8, July 2007.

[8] N. S. Gopaul, J. Wang, and B. Hu. Camera auto-calibration
in GPS/INS/stereo camera integrated kinematic positioning and
Journal of Global Positioning Systems,
navigation system.
14(1):3, 2016.

[9] R. I. Hartley and A. Zisserman. Multiple View Geometry in
Computer Vision. Cambridge University Press, second edition,
2004.

[10] A. Heidari, I. Alaei-Novin, and P. Aarabi. Fusion of spatial and
visual information for object tracking on iPhone. In Proceedings
of the 16th International Conference on Information Fusion
(FUSION), pages 630–637, July 2013.

[11] D. Herrera, C. J. Kannala, and J. Heikkila. Forget the checker-
board: Practical self-calibration using a planar scene. In Winter
Conference on Applications of Computer Vision (WACV), pages
1–9, 2016.

[12] M. Hwangbo, J.-S. Kim, and T. Kanade. Gyro-aided feature
tracking for a moving camera: Fusion, auto-calibration and
GPU implementation. The International Journal of Robotics
Research, 30(14):1755–1774, 2011.

[13] A. H. Jazwinski. Stochastic Processes and Filtering Theory.

Academic Press, New York, 1970.

[14] C. Jia and B. L. Evans. Online calibration and synchroniza-
In Proceedings of
tion of cellphone camera and gyroscope.
the Global Conference on Signal and Information Processing
(GlobalSIP), pages 731–734. IEEE, 2013.

[15] C. Jia and B. L. Evans. Online camera-gyroscope autocalibra-
tion for cell phones. IEEE Transactions on Image Processing,
23(12):5070–5081, 2014.

[16] A. Karpenko, D. Jacobs, J. Baek, and M. Levoy. Digital video
stabilization and rolling shutter correction using gyroscopes.
Technical Report 2011-3, Stanford University, 2011.

[17] M. Kok. Probabilistic Modeling for Positioning Applications
PhD thesis, Link¨oping University,

Using Inertial Sensors.
Link¨oping, Sweden, 2014.

[18] F. Li, H. Sekkati, J. Deglint, C. Scharfenberger, M. Lamm,
D. Clausi, J. Zelek, and A. Wong. Simultaneous projector-
camera self-calibration for three-dimensional reconstruction and
IEEE Transactions on Computational
projection mapping.
Imaging, 3(1):74–83, March 2017.

[19] M. Li, H. Yu, X. Zheng, and A. I. Mourikis. High-ﬁdelity sensor
modeling and self-calibration in vision-aided inertial naviga-
tion. In International Conference on Robotics and Automation
(ICRA), pages 409–416. IEEE, 2014.

[20] B. D. Lucas and T. Kanade. An iterative image registration
technique with an application to stereo vision. In International
Conference on Artiﬁcial Intelligence (IJCAI), pages 674–679.
Vancouver, BC, Canada, 1981.

[21] J. Maye, P. Furgale, and R. Siegwart. Self-supervised calibration
In IEEE Intelligent Vehicles Symposium

for robotic systems.
(IV), pages 473–480, 2013.

[22] H. Ovr´en and P.-E. Forss´en. Gyroscope-based video stabili-
In International Conference on
sation with auto-calibration.
Robotics and Automation (ICRA), pages 2090–2097. IEEE,
2015.

[23] S. S¨arkk¨a. Bayesian Filtering and Smoothing. Cambridge

University Press, 2013.

[24] M. A. Shelley. Monocular visual

inertial odometry on a
mobile device. Master’s thesis, Technical University of Munich,
Germany, 2014.

[25] J. Shi and C. Tomasi. Good features to track. In Proceedings
of the IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR), pages 593–600, 1994.
[26] A. Solin, S. Cortes, E. Rahtu, and J. Kannala. Inertial odometry
on handheld smartphones. In Proceedings of the International
Conference on Information Fusion (FUSION), 2018.

[27] A. Solin, S. Cortes, E. Rahtu, and J. Kannala. PIVO: Proba-
bilistic inertial-visual odometry for occlusion-robust navigation.
In Proceedings of the IEEE Winter Conference on Applications
of Computer Vision (WACV), pages 616–625, 2018.

[28] J. Sun, P. Wang, Z. Qin, and H. Qiao. Effective self-calibration
for camera parameters and hand-eye geometry based on two
IEEE/CAA Journal of Automatica
feature points motions.
Sinica, 4(2):370–380, 2017.

[29] S. Willi and A. Grundh¨ofer. Robust geometric self-calibration
In International
of generic multi-projector camera systems.
Symposium on Mixed and Augmented Reality (ISMAR), pages
42–51, 2017.

Robust Gyroscope-Aided Camera Self-Calibration

Santiago Cort´es Reina
Department of Computer Science
Aalto University
Helsinki, Finland
santiago.cortesreina@aalto.ﬁ

Arno Solin
Department of Computer Science
Aalto University
Helsinki, Finland
arno.solin@aalto.ﬁ

Juho Kannala
Department of Computer Science
Aalto University
Helsinki, Finland
juho.kannala@aalto.ﬁ

8
1
0
2
 
y
a
M
 
1
3
 
 
]

V
C
.
s
c
[
 
 
1
v
6
0
5
2
1
.
5
0
8
1
:
v
i
X
r
a

Abstract—Camera calibration for estimating the intrinsic
parameters and lens distortion is a prerequisite for various
monocular vision applications including feature tracking and
video stabilization. This application paper proposes a model for
estimating the parameters on the ﬂy by fusing gyroscope and
camera data, both readily available in modern day smartphones.
The model is based on joint estimation of visual feature positions,
camera parameters, and the camera pose, the movement of
which is assumed to follow the movement predicted by the
gyroscope. Our model assumes the camera movement to be free,
but continuous and differentiable, and individual features are
assumed to stay stationary. The estimation is performed online
using an extended Kalman ﬁlter, and it is shown to outperform
existing methods in robustness and insensitivity to initialization.
We demonstrate the method using simulated data and empirical
data from an iPad.

I. INTRODUCTION

The growth in the market of smartphones and tablets has
brought monocular cameras to even the cheapest of smart-
devices. Simultaneously, the improved computational capa-
bilities have made it possible to expand their use into new
ﬁelds and use cases, such as video calls, payment veriﬁcation,
and augmented reality. However, employing the device camera
in pose estimation, video stabilization, or feature tracking
requires the camera calibration parameters to be known or
estimated.

On smart-devices, the use cases often provide an explicit
calibration step that includes capturing a pre-deﬁned calibra-
tion pattern from different positions. This procedure is the
base for traditional camera calibration (see, e.g., [9] for an
overview). Self-calibration is the problem of modeling the in-
ternal parameters of a camera (projection matrix and distortion
coefﬁcients) without using any known pattern. Luckily there
are usually additional sensors available on the devices, typi-
cally a low-cost MEMS inertial measurement unit (IMU). The
IMU typically provides fast-sampled (up to some hundreds of
Hz) readings of the speciﬁc force (accelerometer) and turn-
rate (gyroscope) in the device’s coordinate frame (see, e.g.,
[26] for a more thorough introduction).

In this work we propose a method for camera self-
calibration using the information from a gyroscope rigidly
attached to the camera. We use a structure from motion (SfM)
approach, where the camera model minimizes the reprojection
error over a set of images. The proposed method jointly esti-
mates the camera pose, tracked feature positions, and camera

Phone camera
position p
orientation q

Phone
gyroscope
with turn rate
ω [rad/s]

Camera parameters
(fx/y and cx/y)

Radial distortion
(k1 and k2)

Fig. 1. Illustration of the model setup: A pinhole camera with position p and
orientation quaternion q, auxiliary turn rate ω from the gyroscope. A set of
(unknown) feature locations {zi} are observed through a camera model with
linear and non-linear distortion parameters.

parameters. The method works online employing a state-space
formulation, where the fast-sampled gyroscope data is used for
forward-predicting the relative camera and feature movement,
and the visual data tracking results are then matched to the
predictions in a probabilistic fashion. The inference itself is
solved by an extended Kalman ﬁlter.

Due to its practical importance, camera self-calibration—
or auto-calibration—has been studied extensively during the
past decades. Outside smartphones, practical applications are
found, for example, in trafﬁc surveillance [1], projector camera
calibration [18, 29] and robotics for autonomous vehicles
[5, 7, 8, 10, 21, 28]. The seminal paper by Faugeras et al. [6]
introduced the concept of calibrating the intrinsic and extrinsic
parameters without a known calibration object or pattern.
Since then many methods have been introduced, building upon
special kind of motion (e.g. purely rotating or planar), special
scene geometry (e.g. planar scenes or special depth structure,
see [11] for discussion), or auxiliary sensors.

For example, Civera et al. [4] proposed a sum-of-Gaussians
ﬁlter approach building upon a SfM approach where there
has to be sufﬁcient translation. Additional sensors ease the
motion constraints. Using both a gyroscope and an accelerom-
eter for calibration, information can be acquired about the

relative rotation, absolute scale, and the world coordinate
frame orientation (see, e.g., [19]). Similar setups are often
employed in visual-inertial odometry methods (see, e.g., the
discussion in [24, 27]), where the same set of sensors are
used and the camera calibration is estimated as a part of the
inference. Gyroscopes are also used in video stabilization, and
[22] proposed a model for scaling, time offset, and relative
pose calibration using the gyroscope. They, however, do not
estimate the camera calibration parameters.

There are also other methods which only use the camera
and a gyroscope. Karpenko et al. [16] proposed a method for
calibration which uses a gyroscope together with a quick shake
when pointing to far-away objects, while Hwangbo et al. [12]
constrain the estimation by assuming pure rotation.

Within methods designed for fusing camera and gyroscope
data, we are only aware of one previously published approach,
which works in the same setting as the proposed method.
The method by Jia and Evans was ﬁrst published in [14] and
later reﬁned in [15]. Their method together with ours does
not assume special movement and only require visual and
gyroscope data.

The proposed model is based on the following assumptions:
(i) The camera movement is free, but continuous and differ-
entiable (rather smooth), (ii) The camera rotations follow the
gyroscope turn rate observations, (iii) The world coordinates
of individual features are unknown, but assumed constant
(individual features assumed to stay stationary). In comaprison
to [15], this method aims at providing higher robustness to
initialization and feature-poor enivironments with only a few
visual features being tracked. Where [15] uses a lot of short
(two-frame) connections of features, our method uses a few
long chains of connected features.

This paper is structured as follows. Section II presents the
theoretical methodology in detail starting from the camera
calibration model, the proposed state-space model, and ﬁnally
how to jointly infer all the unknown parameters. The Results
section shows the method employed both in a simulation
study and a on real-world data. Finally, the assumptions and
modeling problems and some future work are discussed.

II. METHODS

We start by deﬁning the notation used in the camera calibra-
tion model, which will later be used in speciﬁcation of the
measurement model. Then we proceed to setting up the state-
space model for the gyroscope-aided dynamics of the camera
pose, feature positions, and camera parameters. Finally, we
couple the forward dynamics with the image data in a visual
measurement model. A sketch of the information ﬂow in the
method is shown in Figure 2.

A. Camera Model

We build upon the well-established theory of monocular
camera calibration models. A detailed and extensive descrip-
tion of camera models can be found in [9]. Through the
camera model, points (x, y, z) in three-dimensional space are
projected to image plane coordinates (u, v) using a pinhole

Initial state

Gyroscope
prediction

k ← k+1

Wait for
gyro data

no

Is frame
available?
yes

Feature
tracking

Visual
update

Fig. 2. The information ﬂow in the self-calibration method. The gyroscope
prediction loop runs at 100 Hz, and visual updates occur once a new frame
is acquired (typically at 10 Hz).

camera model. First the points are rigidly transformed into
the camera reference frame, that is origin at the optical center,
the z-axis perpendicular to the image plane and y-axis parallel
to the vertical axis of the image. Then the points are projected
into the image plane using the pinhole projection. In matrix
form





u
v
 = K E
1











,





x
y
z
1

(1)

where the intrinsic matrix K ∈ R3×3 and extrinsic matrix
E ∈ R3×4 are parametrized as follows:

K =





fx
0
0

0
fy
0

cx
cy
1


 and E = (cid:0)RT −RTp(cid:1) .

(2)

length (fx and fy, separate
The parameters are the focal
between dimensions to account for non-square pixels), the
origin of the image plane (cx, cy), and the position p ∈ R3
and orientation R ∈ R3×3 of the camera in the global frame
of reference.

The projection operation is entirely linear (in homogeneous
space), but real-world lenses usually introduce non-linear
mappings. These are known as distortions, the most common
distortion is rotationally symmetric around the image center
and is modeled by so-called radial distortion coefﬁcients k1
and k2 as follows:
(cid:19)
(cid:18)u(cid:48)
v(cid:48)

(1 + k1 r2 + k2 r4),

(cid:18)u
v

(3)

=

(cid:19)

where the radial component is determined by

(cid:115)

r =

(cid:17)2

(cid:16) u − cx
fx

+

(cid:16) v − cy
fy

(cid:17)2

.

(4)

This camera model is shown in the illustrative sketch in
Figure 1, where also the resulting distortion effect is visible.
We refer to the non-linear distortion function as ‘distort’ later
in the paper.

B. State Estimation

In order to include time-dependency between observed frames
and rotational
information from the gyroscope, we deﬁne
a state-space model. The model describes a device with a

monocular camera and a rigidly attached gyroscope with a
known relative orientation between them (see Fig. 1). An
example of such a device is a modern smartphone. The
following section shows the non-linear ﬁler designed for
information fusion, the non-linear ﬁlter allows for estimation
and fusion while keeping track of the uncertainty and depen-
dencies between the non-deterministic processes describing
the evolution of the calibration parameters, camera pose, and
feature locations.

We deﬁne a state-space model (see, e.g., [23]), where the

state vector is

x = (cid:0)cT pT vT qT

zT(cid:1)T

.

(5)

The variable c = (fx, fy, cx, cy, k1, k2) contains the internal
camera parameters, p ∈ R3 and v ∈ R3 contain the position
and velocity of the camera, q contains the quaternion encoding
the orientation of the camera, and z ∈ R3d contains the
locations of the features (d is the number of features being
tracked).

The non-linear state-space model with the auxiliary gyro-
scope control signal ωk is given as follows. The control can
be embedded into the time-varying dynamical model such that
fk(x, εk) := f (x, ωk, εk). The state-space model takes the
canonical form:

xk = fk(xk−1, εk),
yk = hk(xk) + γk,

(6)

where xk ∈ Rn is the state at time step tk, k = 1, 2, . . ., yk ∈
Rm is a measurement, εk ∼ N(0, Qk) is the Gaussian process
noise, and γk ∼ N(0, Σk) is the Gaussian measurement noise.
The dynamics and measurements are speciﬁed in terms of the
dynamical model function f (·, ·, ·) and the measurement model
function hk(·).

In this work we employ the extended Kalman ﬁlter (EKF,
[2, 13]) which provides a means of approximating the state
distributions p(x | y1:k) with Gaussians:

p(xk | y1:k) (cid:39) N(xk | mk|k, Pk|k).

(7)

In the EKF, these approximations are formed by ﬁrst-order
linearizations of the non-linearities. The extended Kalman
ﬁltering recursion can be written as follows (see [23] for
detailed presentation). The dynamics are incorporated into the
prediction step:

mk|k−1 = fk(mk−1|k−1, 0),
Pk|k−1 = Fx(mk−1|k−1) Pk−1|k−1 FT

Fε(mk−1|k−1) Qk FT

ε (mk−1|k−1),

prediction step is entirely given by the standard Kalman ﬁlter
preduction step:

mk|k−1 = Ak mk−1|k−1,
Pk|k−1 = Ak Pk−1|k−1 AT

k + Qk.

(9)

Measurement data providing observations of the system
state at given time steps are combined with the model in the
update step:

vk = yk − hk(mk|k−1),
Sk = Hx(mk|k−1) Pk|k−1 HT
x(mk|k−1) + Σk,
x(mk|k−1) S−1
Kk = Pk|k−1 HT
k ,

mk|k = mk|k−1 + Kk vk,
Pk|k = [I − Kk Hx(mk|k−1)] Pk|k−1

[I − Kk Hx(mk|k−1)]T + Kk Σk KT
k ,

(10)

where Hx(·) denotes the Jacobian of the measurement model
hk(·) with respect
to the state variables x. The slightly
unorthodox form of the last line is known as the Joseph’s
formula, which both numerically stabilizes updating the co-
variance and preserves symmetry.

The linearizations inside the extended Kalman ﬁlter cause
some errors in the estimation. Most notably the estima-
tion scheme does not preserve the norm of the orientation
quaternions. Therefore after each update an extra quaternion
normalization step is added to the estimation scheme.

C. Propagation by Gyroscope Prediction

The state holds the internal and external parameters of the
camera and the 3D position of the features. The internal
parameters and the 3D coordinates of the features are assumed
to stay constant (but are still unknown), so their propagation
functions are identities. The external parameters (position,
orientation) of the camera are not constant and are treated
differently.

The position of the camera is modeled as a Wiener velocity
process, a commonly used model
in tracking and control
literature (see, e.g., [23] for details). In order to keep track of
the full inertial state the estimate contains both the position and
velocity vectors x = (cid:0)p v(cid:1)T
and the acceleration is modeled
as a white noise process d2x(t)
dt2 = w(t), or in state space
form as a linear time-invariant stochastic differential equation
(independently for each spatial dimension)
(cid:19)
(cid:18)0
1

(cid:18)0
0

(cid:19)
1
0

dx(t)
dt

x(t) +

w(t),

(11)

=

x(mk−1|k−1)+

(8)

where w(t) is a realization of the white noise process. In
discrete time the system is

where the dynamic model
is evaluated with the outcome
from the previous step and zero noise, and Fx(·) denotes the
Jacobian matrix of fk(·, ·) with respect to x and Fε(·) with
respect to the process noise ε.

We will also address the special case, where the dynamics
are entirely linear, that is fk(x) = Ak. In that case the ﬁlter

xk+1 = Axk + εk

(12)

where εk ∼ N(0, Qt), and xk := x(tk).

The orientation of the camera is encoded in a unit quater-
nion, unit quaternions are a direct representation of an axis-
angle rotation and can be converted into a rotation matrix using
Rodriguez formula.

where the linear dynamics are given by

III. EXPERIMENTS

Given a unit quaternion q that represents a rotation and
a known angular rate ω in the same frame of reference, its
derivative can be expressed as

where

dq(t)
dt

1
2

=

Ω(ω) q(t),

Ω(ω) =

(cid:18) 0 −ωT
ω [ω]×

(cid:19)

.

The notation [w]× is the 3×3 cross-product matrix (for further
details on quaternion modeling, see, e.g., [17]).

Assuming constant rotation rate (during ∆t), the discrete-

time system is

qk+1 = exp

(cid:18) ∆tk Ω(ωk)
2

(cid:19)

qk.

Since the gyroscope produces rotational rate measurements
with known accuracy, it can be propagated into an uncertainty
for the quaternion. The gyroscope data is used directly in the
prediction as a control signal in a linear Kalman ﬁlter.

Putting it all together, the state dynamics are described by

fk(xk, εk) = Akx + εk,

(16)

(13)

(14)

(15)

Ak =

I3

I3∆tk
I3

I3









exp( ∆tk

2 Ω(ωk − ωb))

and

εk ∼ N(0, Q), Q = blkdiag (cid:0)03 Qt Qq 03

(cid:1) ,

(18)

where ωb is the gyroscope bias (estimated off-line) and the Qt
and Qq matrices model the process noise of the translation and
rotation, respectively.

The process noise of the translation is derived from the
wiener velocity model described above, the process noise of
the rotation is propagated from the rotational rate.

D. Visual Update

The visual update couples all of the state variables. In Figure 2
the visual update occurs every time a new frame has been
acquired. The frame is ﬁrst processed by the feature tracker
and inlier detection, and then the feature pixel coordinates are
passed to the visual update model which processes an extended
Kalman ﬁlter update step.

The features are chosen by the Shi–Tomasi Good features
to track method [25] which determines strong corners in the
image. These features are tracked across frames by a pyramidal
Lucas–Kanade tracker [3, 20]. The Seven-point algorithm [9]
is used for inlier detection. If a feature is recognized as an
inlier, the visual update directly proceeds for the feature. If the
feature is not an inlier, the feature is replaced in the state by
re-initializing its current position z(i) estimate (both in terms
of state mean and covariance) to uninformative a priori values.

TABLE I
RMSE ERRORS PER METHOD AND PARAMETER.

Variable

Initial

Batch opt.

Jia and Evans [15]

Proposed

fx (px)
fy (px)
cx (px)
cy (px)
k1
k2

75
75
0.5
0.5
0.01
0.01

0.05
0.05
0.02
0.10
0.0001
0.0095

15.19
15.19
1.91
2.09
—
—

0.36
0.38
0.27
0.34
0.0001
0.0095

The measurement model function h(·) is a function of all
state variables. The measurement vector y ∈ R2d contains the
pixel coordinates, and feature-wise the model can be written
as

y(i) = h(i)(x) = distort

K(c) E(p, q)

(cid:20)

(cid:18)z(i)
1

(cid:19)

(cid:21)

, c

,

(19)

for i = 1, 2, . . . , d, where d is the number of features being
tracked. The measurement model is fully determined by the
the
camera model
Jacobian matrix of Eq. (19) must be formed as well. The
measurement noise is set to Σ = (2.5 px)2 I.

in Eqs. (1)–(4). For the EKF update,









I3

(17)

We demonstrate our self-calibration method both in a simula-
tion setup with known ground-truth values, and empirically on
real data. All the experiments runs were performed on the data
off-line using a Mathworks MATLAB implementation1, and
benchmarked against the publicly available MATLAB imple-
mentation by Jia and Evans [15]. Furthermore, we compared
our approach to a bundle-adjustment setup, which was used
for checking how well the other methods actually were able
to perform.

A. Simulation Study

A simulation similar to the one described in [15] was per-
formed to compare the results. To evaluate the methods, 100
camera movement paths around a three-dimensional regular
point structure were simulated in a Monte Carlo setup. The
point coordinates were projected into a virtual camera that
followed the paths and continuously turned to look at the
point structure. The ground-truth camera model was a zero-
skew square pixel camera with a focal length of 575 pixels, an
image size of 480 × 640 pixels, with the origin at the center
of the frame. The distortion parameters were set to be zero in
the simulation.

The simulation setup produced tracks for the visual features
and gyroscope readings, at 10 Hz and 100 Hz, respectively.
The initial state estimates corresponding to the camera pa-
rameters were set as follows. The initial guess for the origin
of the image plane and focal length were the image center
coordinates and 700 px, respectively.

Table I shows the average RMSE results over the 100
simulations for the initial state and three methods run on

1Code available online:

https://github.com/AaltoVision/camera-gyro-calibration

)
x
p
(

x
f

)
x
p
(

y
f

)
x
p
(

x
c

)
x
p
(

y
c

1000

800

600

400

200
1000

800

600

400

200

400

350

300

250

200

400

350

300

250

200

1
k

2
k

0.4
0.2
0
−0.2
−0.4

0.4
0.2
0
−0.2
−0.4

0

10

20

40

50

60

30
Time (s)

Fig. 3. Evolution of the camera calibration when started from distant
initial values in one Monte Carlo simulation. Initial convergence is reached
immediately after the ﬁrst image pair has been observed, and ﬁnal convergence
once the camera has moved sufﬁciently. The shaded regions depict the 95%
uncertainty interval given by the state-space estimation.

the simulated data. The ﬁrst column shows the initial RMSE
from the initialized parameters. We compare the calibration
parameters acquired from three models. The ﬁrst is the model
proposed, which was initialized and run as described earlier
in the Methods section.

To analyze the observability of the parameters in the simu-
lated data, we provide results also for a bundle-adjusted batch
solution (‘Batch opt.’ in Table I), which acts as a brute-force
baseline for the methods. For this baseline the camera pa-
rameters and motion track estimated by our proposed method
were used for initializing the bundle adjustment (non-linear
minimization of the reprojection error). The bundle adjustment
problem was solved by constrained minimization using the
MATLAB fmincon Interior Point Algorithm. This bundle
adjustment approach was computationally very intensive, but

TABLE II
PARAMETER VALUES IN EMPIRICAL EXPERIMENTS.

Variable

Initial

Final

Ground-truth

fx (px)
fy (px)
cx (px)
cy (px)
k1
k2

700
700
240
320
0
0

568.12
570.64
238.11
325.09
0.0429
−0.0040

570.56
569.72
238.03
323.87
0.1134
−0.0634

the results show that the data is informative about the param-
eter values, as the optimization converges to the ground-truth
values.

The third method was the method by Jia and Evans [15]. We
used their autocalibration toolbox2 implementation for solving
the self-calibration problem. As can be seen in the results,
there were issues in running these results. As the method by Jia
and Evans assumes there to be a lot of features available, we
re-ran the results by including almost ten time more features
in the simulation. The method keeps oscillating around the
correct camera parameter values, but fails to converge. We
suspect that the issues are partly related to the use case not
being optimal for their method, and partly because of issues
in their implementation of the method.

Figure 3 shows the evolution of the camera calibration,
when started from distant initial values. The shaded regions
depict the 95% uncertainty interval given by the state-space
estimation. The model reaches initial convergence immediately
after the ﬁrst image pair has been observed, and ﬁnal conver-
gence once the camera has moved sufﬁciently.Note that k2
does not show the convergence of the other parameters. The
main reason is that for this case the features are around the
center of the image at the beginning, where the r from 4 is
small and thus the gradient with respect to k2 is small. For the
synthetic case, the distortion parameters are an over-model.

The results show that the proposed method has an RMSE
less than one in pixel units for the intrinsic parameters,
not far from the results given brute-force calculated bundle-
adjustment based calibration results. The method by Jia and
Evans is clearly off more, but still did not diverge.

B. Empirical Tests

In order to demonstrate the proposed online calibration ap-
proach on empirical data, we acquired test data in various sit-
uations using an Apple iPad Pro 12.9-inch model. Hardware-
wise the iPad can be seen as a representative example of
a modern-day smart-device. Similar sensors are available in
most Andoroid and iOS smartphones and tablets.

The data acquisition was conducted using a custom data
capture app implemented in Objective-C. The capture tool
app stored the three-axis gyroscope together with associated
timestamps to a ﬁle in the device. The gyroscope sampling rate
was set to 100 Hz. The gyroscope was pre-calibrated before

2Online Camera-Gyroscope Auto-Calibration for Cellphones:
http://users.ece.utexas.edu/∼bevans/papers/2015/autocalibration/

(a) Cards—A dark indoor data set

(b) Cups—A feature-poor data set

1000

)
x
p
(

x
f

800

600

400
1000

)
x
p
(

y
f

)
x
p
(

x
c

)
x
p
(

y
c

800

600

400

500

400

300

200

100

500

400

300

200

100

4
2
0
−2
−4

2
k

4
2
0
−2
−4

(c) Signs—Overexposed outdoor data set

Fig. 4. Example frames from the empirical tests: (a) is underexposed and with
uneven concentration of features, (b) is from a feature-poor indoor scene, and
(c) is an overexposed outdoor scene.

1
k

the data acquisition for estimating the additive gyroscope bias
ωb.

Simultaneously to the gyroscope capture, device camera
frames were read time-locked to the gyroscope observations.
The experiments use the rear-facing camera with a resolution
of 480 × 640 (portrait orientation), grayscale images, exposure
time 1/60, ﬁxed aperture, sensitivity (ISO value) 125, and
locked focus at inﬁnity. The camera refresh rate was 10 fps
(Hz). The camera frames were stored as H.264 packed video
sequences on the device, with exact frame timestamps stored
separately for use in the ofﬂine run.

Furthermore, camera images of the canonical OpenCV
checkerboard pattern were captured for conducting batch cali-
bration of the tablet camera. This calibration was only used for
obtaining ground-truth camera parameters to compare against.
For obtaining a set of verstaile use cases, we recorded short
sequences of a few controlled static scenes. In these data sets
the motion of the camera was smooth in the sense of avoiding
hard stops, in order to comply with the constrains of the model.
Figure 4 shows example frames from the empirical data sets.
They include (a) ill-lit (underexposed) indoor scenes (we name
this data set ‘cards’), (b) visual feature poor scenes (typical in
ofﬁce environments), and (c) overexposed outdoor scenes.

The evolution of the proposed calibration method is shown
in Figure 5, where the shaded area represents the 95% uncer-
tainty interval. The corresponding calibration result is shown
in Table II together with the initial values and the checkerboard
calibrated ground-truth for the device camera.

0

5

10

15

20

25

30

35

Time (s)

Fig. 5. Evolution of the camera calibration in the ‘Cards’ data set, when
the 95%
started from distant
uncertainty interval given by the state-space estimation. The convergence
occurs once sufﬁcient excitation movement has occurred.

initial values. The shaded regions depict

For this run, the evolution of the position estimates and
the orientation (transformed into Euler angles) is visualized in
Figure 6. Subﬁgure (a) shows the latent (unobserved) camera
position track estimates (the shaded area represents the 95%
credible interval) over the time-span of the data. The absolute
scale of the movement remains unobserved, which is due
to the gyroscope only observing the rotation rate and the
camera data being agnostic to the true distance of any feature
movement. Subﬁgure (b) shows the corresponding orientation
states, which coincide more clearly with the behavior in
Figure 5. The linear parameters appear to reach the right
regime after the camera has been rotated sufﬁciently (i.e. the
data features sufﬁcient excitation). The non-linear distortion
parameters take longer to stabilize.

Subﬁgure 6(c) shows the corresponding input frames in the
cards data set with challenging lighting conditions. The green

)
.
u
.
a
(

n
o
i
t
i
s
o
P

5

0

5
−

s
e
l
g
n
a

n
o
i
t
a
t
n
e
i
r

O

4
/
π
3
2
/
π
4
/
π

0

4
/
π
−

0

x

y

z

0

5

10

15

20

30

35

40

45

50

25
Time (seconds)

(a) Latent camera position state estimates

5

10

15

20

30

35

40

45

50

25
Time (seconds)

(b) Camera orientation estimates

t = 0 s

t = 5 s

t = 10 s

t = 15 s

t = 20 s

t = 25 s

t = 30 s

t = 35 s

t = 40 s

t = 45 s

(c) Associated frames at different time points

Fig. 6. State estimates and example input frames for the parameter estimate track results in Fig. 5. Subﬁgure (a) shows the latent (unobserved) camera position
track estimates (the shaded area represents the 95% credible interval) over the time-span of the data. Note that the absolute scale of the movement remains
unobserved. (b) The corresponding orientation states, which coincide more clearly with the behavior in Fig. 5. (c) Example input frames in the cards data set
with green markers showing the tracked feature positions. Observation times correspond to the tick marks in the above plots.

markers show the current feature point locations in the frames,
and their ‘tails’ (green line) show the point movement since
the previous frame (frames sampled at 10 Hz).

For the intrinsic parameters, the estimated parameter values
converge within a few pixels to the checkerboard-calibrated
ground-truth values. For the non-linear radial distortion pa-
rameters the values are also similar. In case of the distortion
parameters, the identiﬁcation might suffer from the low feature
concentration towards the edges of the visual frame data.

IV. DISCUSSION AND CONCLUSION

In this paper we have proposed a method for estimating the
intrinsic parameters and lens distortion coefﬁcients for camera
calibration. This paper proposed a model for estimating the
parameters on the ﬂy by fusing gyroscope and camera data,
where the model is based on joint estimation of visual feature
positions, camera parameters, and the camera pose, the rotation
of which is assumed to follow the movement predicted by the
gyroscope.

The estimation procedure is lightweight and performs on-
line using an extended Kalman ﬁlter. The strengths of the
method is in robustness to feature-poor visual environments
and insensitivity to initialization, the aspects which were also

demonstrated in the experiments on simulated and empirical
data.

The experiments showed that the method performs well
against the current state-of-the-art in gyroscope-aided self-
calibration. The results showed that after sufﬁcient motion the
convergence is both quick, and it is easy to capture the moment
of convergence by monitoring the state variance estimates for
the camera parameters.

The empirical tests demonstrated the method using data
captured using an Apple iPad Pro. The empirical data the
parameter estimates converged rapidly after the system had
experienced sufﬁcient motion required for jointly estimating
both the camera poses and feature world coordinates.

Figure 4 shows three distinctive data sets; one is underex-
posed and with uneven concentration of features, one is from
a feature-poor indoor scene, and the third is from an overex-
posed outdoor scene. The proposed method is not sensitive to
feature-poor environments as it can rely on a relatively low
number of features being tracked—in the simulations only 27
features were used.

This is a clear difference to previous methods, which
have been mostly inspired by building on purely machine
vision methods. Requiring hundreds of tracked features to

be available works well in controlled environments, in good
lighting conditions, and using high-quality camera hardware.
Our method focuses on the opposite—low quality data and
a low number of features sufﬁces. On the other hand, this
comes with a requirement of the features remaining visible
for a sufﬁciently long time period.

The method could be extended to include further parame-
ters. Natural directions of extension would be to also estimate
the additive three-axis gyroscope bias, or include further
camera parameters such as rolling-shutter timing parameters.
As the method jointly estimates both camera poses, param-
eters, and feature positions, it can be sensitive to certain use
cases and environments. For example, in Figure 4(b) the cups
are round and their corresponding features are often lost and
picked up again during movement. Short feature tracks inject
uncertainty into the estimation scheme in the proposed method,
which slows down convergence and misleads the estimation.
This could be improved by tuning the method parameters, or
introducing visual loop-closures which would reuse the same
features when they appear again.

Code implementing the method is available online:

https://aaltovision.github.io/camera-gyro-calibration/

ACKNOWLEDGMENTS

This research was supported by the Academy of Finland grants
308640, 277685, and 295081.

REFERENCES
[1] S. ´Alvarez, D. Llorca, and M. Sotelo. Hierarchical camera auto-
calibration for trafﬁc surveillance systems. Expert Systems with
Applications, 41(4):1532–1542, 2014.

[2] Y. Bar-Shalom, X.-R. Li, and T. Kirubarajan. Estimation with
Applications to Tracking and Navigation. Wiley-Interscience,
New York, 2001.

[3] J.-Y. Bouguet. Pyramidal implementation of the afﬁne Lucas
Kanade feature tracker: Description of the algorithm. Technical
report, Intel Corporation, 2001.

[4] J. Civera, D. R. Bueno, A. J. Davison, and J. Montiel. Camera
self-calibration for sequential Bayesian structure from mo-
tion. In International Conference on Robotics and Automation
(ICRA), pages 403–408. IEEE, 2009.

[5] F. Faion, P. Ruoff, A. Zea, and U. D. Hanebeck. Recursive
bayesian calibration of depth sensors with non-overlapping
In Proceedings of the 15th International Conference
views.
on Information Fusion (FUSION), pages 757–762, July 2012.
[6] O. D. Faugeras, Q.-T. Luong, and S. J. Maybank. Camera self-
calibration: Theory and experiments. In European Conference
on Computer Vision (ECCV), pages 321–334, 1992.

[7] M. Goldshtein, Y. Oshman, and T. Efrati. Seeker gyro cali-
bration via model-based fusion of visual and inertial data. In
Proceedings of the 10th International Conference on Informa-
tion Fusion (FUSION), pages 1–8, July 2007.

[8] N. S. Gopaul, J. Wang, and B. Hu. Camera auto-calibration
in GPS/INS/stereo camera integrated kinematic positioning and
Journal of Global Positioning Systems,
navigation system.
14(1):3, 2016.

[9] R. I. Hartley and A. Zisserman. Multiple View Geometry in
Computer Vision. Cambridge University Press, second edition,
2004.

[10] A. Heidari, I. Alaei-Novin, and P. Aarabi. Fusion of spatial and
visual information for object tracking on iPhone. In Proceedings
of the 16th International Conference on Information Fusion
(FUSION), pages 630–637, July 2013.

[11] D. Herrera, C. J. Kannala, and J. Heikkila. Forget the checker-
board: Practical self-calibration using a planar scene. In Winter
Conference on Applications of Computer Vision (WACV), pages
1–9, 2016.

[12] M. Hwangbo, J.-S. Kim, and T. Kanade. Gyro-aided feature
tracking for a moving camera: Fusion, auto-calibration and
GPU implementation. The International Journal of Robotics
Research, 30(14):1755–1774, 2011.

[13] A. H. Jazwinski. Stochastic Processes and Filtering Theory.

Academic Press, New York, 1970.

[14] C. Jia and B. L. Evans. Online calibration and synchroniza-
In Proceedings of
tion of cellphone camera and gyroscope.
the Global Conference on Signal and Information Processing
(GlobalSIP), pages 731–734. IEEE, 2013.

[15] C. Jia and B. L. Evans. Online camera-gyroscope autocalibra-
tion for cell phones. IEEE Transactions on Image Processing,
23(12):5070–5081, 2014.

[16] A. Karpenko, D. Jacobs, J. Baek, and M. Levoy. Digital video
stabilization and rolling shutter correction using gyroscopes.
Technical Report 2011-3, Stanford University, 2011.

[17] M. Kok. Probabilistic Modeling for Positioning Applications
PhD thesis, Link¨oping University,

Using Inertial Sensors.
Link¨oping, Sweden, 2014.

[18] F. Li, H. Sekkati, J. Deglint, C. Scharfenberger, M. Lamm,
D. Clausi, J. Zelek, and A. Wong. Simultaneous projector-
camera self-calibration for three-dimensional reconstruction and
IEEE Transactions on Computational
projection mapping.
Imaging, 3(1):74–83, March 2017.

[19] M. Li, H. Yu, X. Zheng, and A. I. Mourikis. High-ﬁdelity sensor
modeling and self-calibration in vision-aided inertial naviga-
tion. In International Conference on Robotics and Automation
(ICRA), pages 409–416. IEEE, 2014.

[20] B. D. Lucas and T. Kanade. An iterative image registration
technique with an application to stereo vision. In International
Conference on Artiﬁcial Intelligence (IJCAI), pages 674–679.
Vancouver, BC, Canada, 1981.

[21] J. Maye, P. Furgale, and R. Siegwart. Self-supervised calibration
In IEEE Intelligent Vehicles Symposium

for robotic systems.
(IV), pages 473–480, 2013.

[22] H. Ovr´en and P.-E. Forss´en. Gyroscope-based video stabili-
In International Conference on
sation with auto-calibration.
Robotics and Automation (ICRA), pages 2090–2097. IEEE,
2015.

[23] S. S¨arkk¨a. Bayesian Filtering and Smoothing. Cambridge

University Press, 2013.

[24] M. A. Shelley. Monocular visual

inertial odometry on a
mobile device. Master’s thesis, Technical University of Munich,
Germany, 2014.

[25] J. Shi and C. Tomasi. Good features to track. In Proceedings
of the IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR), pages 593–600, 1994.
[26] A. Solin, S. Cortes, E. Rahtu, and J. Kannala. Inertial odometry
on handheld smartphones. In Proceedings of the International
Conference on Information Fusion (FUSION), 2018.

[27] A. Solin, S. Cortes, E. Rahtu, and J. Kannala. PIVO: Proba-
bilistic inertial-visual odometry for occlusion-robust navigation.
In Proceedings of the IEEE Winter Conference on Applications
of Computer Vision (WACV), pages 616–625, 2018.

[28] J. Sun, P. Wang, Z. Qin, and H. Qiao. Effective self-calibration
for camera parameters and hand-eye geometry based on two
IEEE/CAA Journal of Automatica
feature points motions.
Sinica, 4(2):370–380, 2017.

[29] S. Willi and A. Grundh¨ofer. Robust geometric self-calibration
In International
of generic multi-projector camera systems.
Symposium on Mixed and Augmented Reality (ISMAR), pages
42–51, 2017.

Robust Gyroscope-Aided Camera Self-Calibration

Santiago Cort´es Reina
Department of Computer Science
Aalto University
Helsinki, Finland
santiago.cortesreina@aalto.ﬁ

Arno Solin
Department of Computer Science
Aalto University
Helsinki, Finland
arno.solin@aalto.ﬁ

Juho Kannala
Department of Computer Science
Aalto University
Helsinki, Finland
juho.kannala@aalto.ﬁ

8
1
0
2
 
y
a
M
 
1
3
 
 
]

V
C
.
s
c
[
 
 
1
v
6
0
5
2
1
.
5
0
8
1
:
v
i
X
r
a

Abstract—Camera calibration for estimating the intrinsic
parameters and lens distortion is a prerequisite for various
monocular vision applications including feature tracking and
video stabilization. This application paper proposes a model for
estimating the parameters on the ﬂy by fusing gyroscope and
camera data, both readily available in modern day smartphones.
The model is based on joint estimation of visual feature positions,
camera parameters, and the camera pose, the movement of
which is assumed to follow the movement predicted by the
gyroscope. Our model assumes the camera movement to be free,
but continuous and differentiable, and individual features are
assumed to stay stationary. The estimation is performed online
using an extended Kalman ﬁlter, and it is shown to outperform
existing methods in robustness and insensitivity to initialization.
We demonstrate the method using simulated data and empirical
data from an iPad.

I. INTRODUCTION

The growth in the market of smartphones and tablets has
brought monocular cameras to even the cheapest of smart-
devices. Simultaneously, the improved computational capa-
bilities have made it possible to expand their use into new
ﬁelds and use cases, such as video calls, payment veriﬁcation,
and augmented reality. However, employing the device camera
in pose estimation, video stabilization, or feature tracking
requires the camera calibration parameters to be known or
estimated.

On smart-devices, the use cases often provide an explicit
calibration step that includes capturing a pre-deﬁned calibra-
tion pattern from different positions. This procedure is the
base for traditional camera calibration (see, e.g., [9] for an
overview). Self-calibration is the problem of modeling the in-
ternal parameters of a camera (projection matrix and distortion
coefﬁcients) without using any known pattern. Luckily there
are usually additional sensors available on the devices, typi-
cally a low-cost MEMS inertial measurement unit (IMU). The
IMU typically provides fast-sampled (up to some hundreds of
Hz) readings of the speciﬁc force (accelerometer) and turn-
rate (gyroscope) in the device’s coordinate frame (see, e.g.,
[26] for a more thorough introduction).

In this work we propose a method for camera self-
calibration using the information from a gyroscope rigidly
attached to the camera. We use a structure from motion (SfM)
approach, where the camera model minimizes the reprojection
error over a set of images. The proposed method jointly esti-
mates the camera pose, tracked feature positions, and camera

Phone camera
position p
orientation q

Phone
gyroscope
with turn rate
ω [rad/s]

Camera parameters
(fx/y and cx/y)

Radial distortion
(k1 and k2)

Fig. 1. Illustration of the model setup: A pinhole camera with position p and
orientation quaternion q, auxiliary turn rate ω from the gyroscope. A set of
(unknown) feature locations {zi} are observed through a camera model with
linear and non-linear distortion parameters.

parameters. The method works online employing a state-space
formulation, where the fast-sampled gyroscope data is used for
forward-predicting the relative camera and feature movement,
and the visual data tracking results are then matched to the
predictions in a probabilistic fashion. The inference itself is
solved by an extended Kalman ﬁlter.

Due to its practical importance, camera self-calibration—
or auto-calibration—has been studied extensively during the
past decades. Outside smartphones, practical applications are
found, for example, in trafﬁc surveillance [1], projector camera
calibration [18, 29] and robotics for autonomous vehicles
[5, 7, 8, 10, 21, 28]. The seminal paper by Faugeras et al. [6]
introduced the concept of calibrating the intrinsic and extrinsic
parameters without a known calibration object or pattern.
Since then many methods have been introduced, building upon
special kind of motion (e.g. purely rotating or planar), special
scene geometry (e.g. planar scenes or special depth structure,
see [11] for discussion), or auxiliary sensors.

For example, Civera et al. [4] proposed a sum-of-Gaussians
ﬁlter approach building upon a SfM approach where there
has to be sufﬁcient translation. Additional sensors ease the
motion constraints. Using both a gyroscope and an accelerom-
eter for calibration, information can be acquired about the

relative rotation, absolute scale, and the world coordinate
frame orientation (see, e.g., [19]). Similar setups are often
employed in visual-inertial odometry methods (see, e.g., the
discussion in [24, 27]), where the same set of sensors are
used and the camera calibration is estimated as a part of the
inference. Gyroscopes are also used in video stabilization, and
[22] proposed a model for scaling, time offset, and relative
pose calibration using the gyroscope. They, however, do not
estimate the camera calibration parameters.

There are also other methods which only use the camera
and a gyroscope. Karpenko et al. [16] proposed a method for
calibration which uses a gyroscope together with a quick shake
when pointing to far-away objects, while Hwangbo et al. [12]
constrain the estimation by assuming pure rotation.

Within methods designed for fusing camera and gyroscope
data, we are only aware of one previously published approach,
which works in the same setting as the proposed method.
The method by Jia and Evans was ﬁrst published in [14] and
later reﬁned in [15]. Their method together with ours does
not assume special movement and only require visual and
gyroscope data.

The proposed model is based on the following assumptions:
(i) The camera movement is free, but continuous and differ-
entiable (rather smooth), (ii) The camera rotations follow the
gyroscope turn rate observations, (iii) The world coordinates
of individual features are unknown, but assumed constant
(individual features assumed to stay stationary). In comaprison
to [15], this method aims at providing higher robustness to
initialization and feature-poor enivironments with only a few
visual features being tracked. Where [15] uses a lot of short
(two-frame) connections of features, our method uses a few
long chains of connected features.

This paper is structured as follows. Section II presents the
theoretical methodology in detail starting from the camera
calibration model, the proposed state-space model, and ﬁnally
how to jointly infer all the unknown parameters. The Results
section shows the method employed both in a simulation
study and a on real-world data. Finally, the assumptions and
modeling problems and some future work are discussed.

II. METHODS

We start by deﬁning the notation used in the camera calibra-
tion model, which will later be used in speciﬁcation of the
measurement model. Then we proceed to setting up the state-
space model for the gyroscope-aided dynamics of the camera
pose, feature positions, and camera parameters. Finally, we
couple the forward dynamics with the image data in a visual
measurement model. A sketch of the information ﬂow in the
method is shown in Figure 2.

A. Camera Model

We build upon the well-established theory of monocular
camera calibration models. A detailed and extensive descrip-
tion of camera models can be found in [9]. Through the
camera model, points (x, y, z) in three-dimensional space are
projected to image plane coordinates (u, v) using a pinhole

Initial state

Gyroscope
prediction

k ← k+1

Wait for
gyro data

no

Is frame
available?
yes

Feature
tracking

Visual
update

Fig. 2. The information ﬂow in the self-calibration method. The gyroscope
prediction loop runs at 100 Hz, and visual updates occur once a new frame
is acquired (typically at 10 Hz).

camera model. First the points are rigidly transformed into
the camera reference frame, that is origin at the optical center,
the z-axis perpendicular to the image plane and y-axis parallel
to the vertical axis of the image. Then the points are projected
into the image plane using the pinhole projection. In matrix
form





u
v
 = K E
1











,





x
y
z
1

(1)

where the intrinsic matrix K ∈ R3×3 and extrinsic matrix
E ∈ R3×4 are parametrized as follows:

K =





fx
0
0

0
fy
0

cx
cy
1


 and E = (cid:0)RT −RTp(cid:1) .

(2)

length (fx and fy, separate
The parameters are the focal
between dimensions to account for non-square pixels), the
origin of the image plane (cx, cy), and the position p ∈ R3
and orientation R ∈ R3×3 of the camera in the global frame
of reference.

The projection operation is entirely linear (in homogeneous
space), but real-world lenses usually introduce non-linear
mappings. These are known as distortions, the most common
distortion is rotationally symmetric around the image center
and is modeled by so-called radial distortion coefﬁcients k1
and k2 as follows:
(cid:19)
(cid:18)u(cid:48)
v(cid:48)

(1 + k1 r2 + k2 r4),

(cid:18)u
v

(3)

=

(cid:19)

where the radial component is determined by

(cid:115)

r =

(cid:17)2

(cid:16) u − cx
fx

+

(cid:16) v − cy
fy

(cid:17)2

.

(4)

This camera model is shown in the illustrative sketch in
Figure 1, where also the resulting distortion effect is visible.
We refer to the non-linear distortion function as ‘distort’ later
in the paper.

B. State Estimation

In order to include time-dependency between observed frames
and rotational
information from the gyroscope, we deﬁne
a state-space model. The model describes a device with a

monocular camera and a rigidly attached gyroscope with a
known relative orientation between them (see Fig. 1). An
example of such a device is a modern smartphone. The
following section shows the non-linear ﬁler designed for
information fusion, the non-linear ﬁlter allows for estimation
and fusion while keeping track of the uncertainty and depen-
dencies between the non-deterministic processes describing
the evolution of the calibration parameters, camera pose, and
feature locations.

We deﬁne a state-space model (see, e.g., [23]), where the

state vector is

x = (cid:0)cT pT vT qT

zT(cid:1)T

.

(5)

The variable c = (fx, fy, cx, cy, k1, k2) contains the internal
camera parameters, p ∈ R3 and v ∈ R3 contain the position
and velocity of the camera, q contains the quaternion encoding
the orientation of the camera, and z ∈ R3d contains the
locations of the features (d is the number of features being
tracked).

The non-linear state-space model with the auxiliary gyro-
scope control signal ωk is given as follows. The control can
be embedded into the time-varying dynamical model such that
fk(x, εk) := f (x, ωk, εk). The state-space model takes the
canonical form:

xk = fk(xk−1, εk),
yk = hk(xk) + γk,

(6)

where xk ∈ Rn is the state at time step tk, k = 1, 2, . . ., yk ∈
Rm is a measurement, εk ∼ N(0, Qk) is the Gaussian process
noise, and γk ∼ N(0, Σk) is the Gaussian measurement noise.
The dynamics and measurements are speciﬁed in terms of the
dynamical model function f (·, ·, ·) and the measurement model
function hk(·).

In this work we employ the extended Kalman ﬁlter (EKF,
[2, 13]) which provides a means of approximating the state
distributions p(x | y1:k) with Gaussians:

p(xk | y1:k) (cid:39) N(xk | mk|k, Pk|k).

(7)

In the EKF, these approximations are formed by ﬁrst-order
linearizations of the non-linearities. The extended Kalman
ﬁltering recursion can be written as follows (see [23] for
detailed presentation). The dynamics are incorporated into the
prediction step:

mk|k−1 = fk(mk−1|k−1, 0),
Pk|k−1 = Fx(mk−1|k−1) Pk−1|k−1 FT

Fε(mk−1|k−1) Qk FT

ε (mk−1|k−1),

prediction step is entirely given by the standard Kalman ﬁlter
preduction step:

mk|k−1 = Ak mk−1|k−1,
Pk|k−1 = Ak Pk−1|k−1 AT

k + Qk.

(9)

Measurement data providing observations of the system
state at given time steps are combined with the model in the
update step:

vk = yk − hk(mk|k−1),
Sk = Hx(mk|k−1) Pk|k−1 HT
x(mk|k−1) + Σk,
x(mk|k−1) S−1
Kk = Pk|k−1 HT
k ,

mk|k = mk|k−1 + Kk vk,
Pk|k = [I − Kk Hx(mk|k−1)] Pk|k−1

[I − Kk Hx(mk|k−1)]T + Kk Σk KT
k ,

(10)

where Hx(·) denotes the Jacobian of the measurement model
hk(·) with respect
to the state variables x. The slightly
unorthodox form of the last line is known as the Joseph’s
formula, which both numerically stabilizes updating the co-
variance and preserves symmetry.

The linearizations inside the extended Kalman ﬁlter cause
some errors in the estimation. Most notably the estima-
tion scheme does not preserve the norm of the orientation
quaternions. Therefore after each update an extra quaternion
normalization step is added to the estimation scheme.

C. Propagation by Gyroscope Prediction

The state holds the internal and external parameters of the
camera and the 3D position of the features. The internal
parameters and the 3D coordinates of the features are assumed
to stay constant (but are still unknown), so their propagation
functions are identities. The external parameters (position,
orientation) of the camera are not constant and are treated
differently.

The position of the camera is modeled as a Wiener velocity
process, a commonly used model
in tracking and control
literature (see, e.g., [23] for details). In order to keep track of
the full inertial state the estimate contains both the position and
velocity vectors x = (cid:0)p v(cid:1)T
and the acceleration is modeled
as a white noise process d2x(t)
dt2 = w(t), or in state space
form as a linear time-invariant stochastic differential equation
(independently for each spatial dimension)
(cid:19)
(cid:18)0
1

(cid:18)0
0

(cid:19)
1
0

dx(t)
dt

x(t) +

w(t),

(11)

=

x(mk−1|k−1)+

(8)

where w(t) is a realization of the white noise process. In
discrete time the system is

where the dynamic model
is evaluated with the outcome
from the previous step and zero noise, and Fx(·) denotes the
Jacobian matrix of fk(·, ·) with respect to x and Fε(·) with
respect to the process noise ε.

We will also address the special case, where the dynamics
are entirely linear, that is fk(x) = Ak. In that case the ﬁlter

xk+1 = Axk + εk

(12)

where εk ∼ N(0, Qt), and xk := x(tk).

The orientation of the camera is encoded in a unit quater-
nion, unit quaternions are a direct representation of an axis-
angle rotation and can be converted into a rotation matrix using
Rodriguez formula.

where the linear dynamics are given by

III. EXPERIMENTS

Given a unit quaternion q that represents a rotation and
a known angular rate ω in the same frame of reference, its
derivative can be expressed as

where

dq(t)
dt

1
2

=

Ω(ω) q(t),

Ω(ω) =

(cid:18) 0 −ωT
ω [ω]×

(cid:19)

.

The notation [w]× is the 3×3 cross-product matrix (for further
details on quaternion modeling, see, e.g., [17]).

Assuming constant rotation rate (during ∆t), the discrete-

time system is

qk+1 = exp

(cid:18) ∆tk Ω(ωk)
2

(cid:19)

qk.

Since the gyroscope produces rotational rate measurements
with known accuracy, it can be propagated into an uncertainty
for the quaternion. The gyroscope data is used directly in the
prediction as a control signal in a linear Kalman ﬁlter.

Putting it all together, the state dynamics are described by

fk(xk, εk) = Akx + εk,

(16)

(13)

(14)

(15)

Ak =

I3

I3∆tk
I3

I3









exp( ∆tk

2 Ω(ωk − ωb))

and

εk ∼ N(0, Q), Q = blkdiag (cid:0)03 Qt Qq 03

(cid:1) ,

(18)

where ωb is the gyroscope bias (estimated off-line) and the Qt
and Qq matrices model the process noise of the translation and
rotation, respectively.

The process noise of the translation is derived from the
wiener velocity model described above, the process noise of
the rotation is propagated from the rotational rate.

D. Visual Update

The visual update couples all of the state variables. In Figure 2
the visual update occurs every time a new frame has been
acquired. The frame is ﬁrst processed by the feature tracker
and inlier detection, and then the feature pixel coordinates are
passed to the visual update model which processes an extended
Kalman ﬁlter update step.

The features are chosen by the Shi–Tomasi Good features
to track method [25] which determines strong corners in the
image. These features are tracked across frames by a pyramidal
Lucas–Kanade tracker [3, 20]. The Seven-point algorithm [9]
is used for inlier detection. If a feature is recognized as an
inlier, the visual update directly proceeds for the feature. If the
feature is not an inlier, the feature is replaced in the state by
re-initializing its current position z(i) estimate (both in terms
of state mean and covariance) to uninformative a priori values.

TABLE I
RMSE ERRORS PER METHOD AND PARAMETER.

Variable

Initial

Batch opt.

Jia and Evans [15]

Proposed

fx (px)
fy (px)
cx (px)
cy (px)
k1
k2

75
75
0.5
0.5
0.01
0.01

0.05
0.05
0.02
0.10
0.0001
0.0095

15.19
15.19
1.91
2.09
—
—

0.36
0.38
0.27
0.34
0.0001
0.0095

The measurement model function h(·) is a function of all
state variables. The measurement vector y ∈ R2d contains the
pixel coordinates, and feature-wise the model can be written
as

y(i) = h(i)(x) = distort

K(c) E(p, q)

(cid:20)

(cid:18)z(i)
1

(cid:19)

(cid:21)

, c

,

(19)

for i = 1, 2, . . . , d, where d is the number of features being
tracked. The measurement model is fully determined by the
the
camera model
Jacobian matrix of Eq. (19) must be formed as well. The
measurement noise is set to Σ = (2.5 px)2 I.

in Eqs. (1)–(4). For the EKF update,









I3

(17)

We demonstrate our self-calibration method both in a simula-
tion setup with known ground-truth values, and empirically on
real data. All the experiments runs were performed on the data
off-line using a Mathworks MATLAB implementation1, and
benchmarked against the publicly available MATLAB imple-
mentation by Jia and Evans [15]. Furthermore, we compared
our approach to a bundle-adjustment setup, which was used
for checking how well the other methods actually were able
to perform.

A. Simulation Study

A simulation similar to the one described in [15] was per-
formed to compare the results. To evaluate the methods, 100
camera movement paths around a three-dimensional regular
point structure were simulated in a Monte Carlo setup. The
point coordinates were projected into a virtual camera that
followed the paths and continuously turned to look at the
point structure. The ground-truth camera model was a zero-
skew square pixel camera with a focal length of 575 pixels, an
image size of 480 × 640 pixels, with the origin at the center
of the frame. The distortion parameters were set to be zero in
the simulation.

The simulation setup produced tracks for the visual features
and gyroscope readings, at 10 Hz and 100 Hz, respectively.
The initial state estimates corresponding to the camera pa-
rameters were set as follows. The initial guess for the origin
of the image plane and focal length were the image center
coordinates and 700 px, respectively.

Table I shows the average RMSE results over the 100
simulations for the initial state and three methods run on

1Code available online:

https://github.com/AaltoVision/camera-gyro-calibration

)
x
p
(

x
f

)
x
p
(

y
f

)
x
p
(

x
c

)
x
p
(

y
c

1000

800

600

400

200
1000

800

600

400

200

400

350

300

250

200

400

350

300

250

200

1
k

2
k

0.4
0.2
0
−0.2
−0.4

0.4
0.2
0
−0.2
−0.4

0

10

20

40

50

60

30
Time (s)

Fig. 3. Evolution of the camera calibration when started from distant
initial values in one Monte Carlo simulation. Initial convergence is reached
immediately after the ﬁrst image pair has been observed, and ﬁnal convergence
once the camera has moved sufﬁciently. The shaded regions depict the 95%
uncertainty interval given by the state-space estimation.

the simulated data. The ﬁrst column shows the initial RMSE
from the initialized parameters. We compare the calibration
parameters acquired from three models. The ﬁrst is the model
proposed, which was initialized and run as described earlier
in the Methods section.

To analyze the observability of the parameters in the simu-
lated data, we provide results also for a bundle-adjusted batch
solution (‘Batch opt.’ in Table I), which acts as a brute-force
baseline for the methods. For this baseline the camera pa-
rameters and motion track estimated by our proposed method
were used for initializing the bundle adjustment (non-linear
minimization of the reprojection error). The bundle adjustment
problem was solved by constrained minimization using the
MATLAB fmincon Interior Point Algorithm. This bundle
adjustment approach was computationally very intensive, but

TABLE II
PARAMETER VALUES IN EMPIRICAL EXPERIMENTS.

Variable

Initial

Final

Ground-truth

fx (px)
fy (px)
cx (px)
cy (px)
k1
k2

700
700
240
320
0
0

568.12
570.64
238.11
325.09
0.0429
−0.0040

570.56
569.72
238.03
323.87
0.1134
−0.0634

the results show that the data is informative about the param-
eter values, as the optimization converges to the ground-truth
values.

The third method was the method by Jia and Evans [15]. We
used their autocalibration toolbox2 implementation for solving
the self-calibration problem. As can be seen in the results,
there were issues in running these results. As the method by Jia
and Evans assumes there to be a lot of features available, we
re-ran the results by including almost ten time more features
in the simulation. The method keeps oscillating around the
correct camera parameter values, but fails to converge. We
suspect that the issues are partly related to the use case not
being optimal for their method, and partly because of issues
in their implementation of the method.

Figure 3 shows the evolution of the camera calibration,
when started from distant initial values. The shaded regions
depict the 95% uncertainty interval given by the state-space
estimation. The model reaches initial convergence immediately
after the ﬁrst image pair has been observed, and ﬁnal conver-
gence once the camera has moved sufﬁciently.Note that k2
does not show the convergence of the other parameters. The
main reason is that for this case the features are around the
center of the image at the beginning, where the r from 4 is
small and thus the gradient with respect to k2 is small. For the
synthetic case, the distortion parameters are an over-model.

The results show that the proposed method has an RMSE
less than one in pixel units for the intrinsic parameters,
not far from the results given brute-force calculated bundle-
adjustment based calibration results. The method by Jia and
Evans is clearly off more, but still did not diverge.

B. Empirical Tests

In order to demonstrate the proposed online calibration ap-
proach on empirical data, we acquired test data in various sit-
uations using an Apple iPad Pro 12.9-inch model. Hardware-
wise the iPad can be seen as a representative example of
a modern-day smart-device. Similar sensors are available in
most Andoroid and iOS smartphones and tablets.

The data acquisition was conducted using a custom data
capture app implemented in Objective-C. The capture tool
app stored the three-axis gyroscope together with associated
timestamps to a ﬁle in the device. The gyroscope sampling rate
was set to 100 Hz. The gyroscope was pre-calibrated before

2Online Camera-Gyroscope Auto-Calibration for Cellphones:
http://users.ece.utexas.edu/∼bevans/papers/2015/autocalibration/

(a) Cards—A dark indoor data set

(b) Cups—A feature-poor data set

1000

)
x
p
(

x
f

800

600

400
1000

)
x
p
(

y
f

)
x
p
(

x
c

)
x
p
(

y
c

800

600

400

500

400

300

200

100

500

400

300

200

100

4
2
0
−2
−4

2
k

4
2
0
−2
−4

(c) Signs—Overexposed outdoor data set

Fig. 4. Example frames from the empirical tests: (a) is underexposed and with
uneven concentration of features, (b) is from a feature-poor indoor scene, and
(c) is an overexposed outdoor scene.

1
k

the data acquisition for estimating the additive gyroscope bias
ωb.

Simultaneously to the gyroscope capture, device camera
frames were read time-locked to the gyroscope observations.
The experiments use the rear-facing camera with a resolution
of 480 × 640 (portrait orientation), grayscale images, exposure
time 1/60, ﬁxed aperture, sensitivity (ISO value) 125, and
locked focus at inﬁnity. The camera refresh rate was 10 fps
(Hz). The camera frames were stored as H.264 packed video
sequences on the device, with exact frame timestamps stored
separately for use in the ofﬂine run.

Furthermore, camera images of the canonical OpenCV
checkerboard pattern were captured for conducting batch cali-
bration of the tablet camera. This calibration was only used for
obtaining ground-truth camera parameters to compare against.
For obtaining a set of verstaile use cases, we recorded short
sequences of a few controlled static scenes. In these data sets
the motion of the camera was smooth in the sense of avoiding
hard stops, in order to comply with the constrains of the model.
Figure 4 shows example frames from the empirical data sets.
They include (a) ill-lit (underexposed) indoor scenes (we name
this data set ‘cards’), (b) visual feature poor scenes (typical in
ofﬁce environments), and (c) overexposed outdoor scenes.

The evolution of the proposed calibration method is shown
in Figure 5, where the shaded area represents the 95% uncer-
tainty interval. The corresponding calibration result is shown
in Table II together with the initial values and the checkerboard
calibrated ground-truth for the device camera.

0

5

10

15

20

25

30

35

Time (s)

Fig. 5. Evolution of the camera calibration in the ‘Cards’ data set, when
the 95%
started from distant
uncertainty interval given by the state-space estimation. The convergence
occurs once sufﬁcient excitation movement has occurred.

initial values. The shaded regions depict

For this run, the evolution of the position estimates and
the orientation (transformed into Euler angles) is visualized in
Figure 6. Subﬁgure (a) shows the latent (unobserved) camera
position track estimates (the shaded area represents the 95%
credible interval) over the time-span of the data. The absolute
scale of the movement remains unobserved, which is due
to the gyroscope only observing the rotation rate and the
camera data being agnostic to the true distance of any feature
movement. Subﬁgure (b) shows the corresponding orientation
states, which coincide more clearly with the behavior in
Figure 5. The linear parameters appear to reach the right
regime after the camera has been rotated sufﬁciently (i.e. the
data features sufﬁcient excitation). The non-linear distortion
parameters take longer to stabilize.

Subﬁgure 6(c) shows the corresponding input frames in the
cards data set with challenging lighting conditions. The green

)
.
u
.
a
(

n
o
i
t
i
s
o
P

5

0

5
−

s
e
l
g
n
a

n
o
i
t
a
t
n
e
i
r

O

4
/
π
3
2
/
π
4
/
π

0

4
/
π
−

0

x

y

z

0

5

10

15

20

30

35

40

45

50

25
Time (seconds)

(a) Latent camera position state estimates

5

10

15

20

30

35

40

45

50

25
Time (seconds)

(b) Camera orientation estimates

t = 0 s

t = 5 s

t = 10 s

t = 15 s

t = 20 s

t = 25 s

t = 30 s

t = 35 s

t = 40 s

t = 45 s

(c) Associated frames at different time points

Fig. 6. State estimates and example input frames for the parameter estimate track results in Fig. 5. Subﬁgure (a) shows the latent (unobserved) camera position
track estimates (the shaded area represents the 95% credible interval) over the time-span of the data. Note that the absolute scale of the movement remains
unobserved. (b) The corresponding orientation states, which coincide more clearly with the behavior in Fig. 5. (c) Example input frames in the cards data set
with green markers showing the tracked feature positions. Observation times correspond to the tick marks in the above plots.

markers show the current feature point locations in the frames,
and their ‘tails’ (green line) show the point movement since
the previous frame (frames sampled at 10 Hz).

For the intrinsic parameters, the estimated parameter values
converge within a few pixels to the checkerboard-calibrated
ground-truth values. For the non-linear radial distortion pa-
rameters the values are also similar. In case of the distortion
parameters, the identiﬁcation might suffer from the low feature
concentration towards the edges of the visual frame data.

IV. DISCUSSION AND CONCLUSION

In this paper we have proposed a method for estimating the
intrinsic parameters and lens distortion coefﬁcients for camera
calibration. This paper proposed a model for estimating the
parameters on the ﬂy by fusing gyroscope and camera data,
where the model is based on joint estimation of visual feature
positions, camera parameters, and the camera pose, the rotation
of which is assumed to follow the movement predicted by the
gyroscope.

The estimation procedure is lightweight and performs on-
line using an extended Kalman ﬁlter. The strengths of the
method is in robustness to feature-poor visual environments
and insensitivity to initialization, the aspects which were also

demonstrated in the experiments on simulated and empirical
data.

The experiments showed that the method performs well
against the current state-of-the-art in gyroscope-aided self-
calibration. The results showed that after sufﬁcient motion the
convergence is both quick, and it is easy to capture the moment
of convergence by monitoring the state variance estimates for
the camera parameters.

The empirical tests demonstrated the method using data
captured using an Apple iPad Pro. The empirical data the
parameter estimates converged rapidly after the system had
experienced sufﬁcient motion required for jointly estimating
both the camera poses and feature world coordinates.

Figure 4 shows three distinctive data sets; one is underex-
posed and with uneven concentration of features, one is from
a feature-poor indoor scene, and the third is from an overex-
posed outdoor scene. The proposed method is not sensitive to
feature-poor environments as it can rely on a relatively low
number of features being tracked—in the simulations only 27
features were used.

This is a clear difference to previous methods, which
have been mostly inspired by building on purely machine
vision methods. Requiring hundreds of tracked features to

be available works well in controlled environments, in good
lighting conditions, and using high-quality camera hardware.
Our method focuses on the opposite—low quality data and
a low number of features sufﬁces. On the other hand, this
comes with a requirement of the features remaining visible
for a sufﬁciently long time period.

The method could be extended to include further parame-
ters. Natural directions of extension would be to also estimate
the additive three-axis gyroscope bias, or include further
camera parameters such as rolling-shutter timing parameters.
As the method jointly estimates both camera poses, param-
eters, and feature positions, it can be sensitive to certain use
cases and environments. For example, in Figure 4(b) the cups
are round and their corresponding features are often lost and
picked up again during movement. Short feature tracks inject
uncertainty into the estimation scheme in the proposed method,
which slows down convergence and misleads the estimation.
This could be improved by tuning the method parameters, or
introducing visual loop-closures which would reuse the same
features when they appear again.

Code implementing the method is available online:

https://aaltovision.github.io/camera-gyro-calibration/

ACKNOWLEDGMENTS

This research was supported by the Academy of Finland grants
308640, 277685, and 295081.

REFERENCES
[1] S. ´Alvarez, D. Llorca, and M. Sotelo. Hierarchical camera auto-
calibration for trafﬁc surveillance systems. Expert Systems with
Applications, 41(4):1532–1542, 2014.

[2] Y. Bar-Shalom, X.-R. Li, and T. Kirubarajan. Estimation with
Applications to Tracking and Navigation. Wiley-Interscience,
New York, 2001.

[3] J.-Y. Bouguet. Pyramidal implementation of the afﬁne Lucas
Kanade feature tracker: Description of the algorithm. Technical
report, Intel Corporation, 2001.

[4] J. Civera, D. R. Bueno, A. J. Davison, and J. Montiel. Camera
self-calibration for sequential Bayesian structure from mo-
tion. In International Conference on Robotics and Automation
(ICRA), pages 403–408. IEEE, 2009.

[5] F. Faion, P. Ruoff, A. Zea, and U. D. Hanebeck. Recursive
bayesian calibration of depth sensors with non-overlapping
In Proceedings of the 15th International Conference
views.
on Information Fusion (FUSION), pages 757–762, July 2012.
[6] O. D. Faugeras, Q.-T. Luong, and S. J. Maybank. Camera self-
calibration: Theory and experiments. In European Conference
on Computer Vision (ECCV), pages 321–334, 1992.

[7] M. Goldshtein, Y. Oshman, and T. Efrati. Seeker gyro cali-
bration via model-based fusion of visual and inertial data. In
Proceedings of the 10th International Conference on Informa-
tion Fusion (FUSION), pages 1–8, July 2007.

[8] N. S. Gopaul, J. Wang, and B. Hu. Camera auto-calibration
in GPS/INS/stereo camera integrated kinematic positioning and
Journal of Global Positioning Systems,
navigation system.
14(1):3, 2016.

[9] R. I. Hartley and A. Zisserman. Multiple View Geometry in
Computer Vision. Cambridge University Press, second edition,
2004.

[10] A. Heidari, I. Alaei-Novin, and P. Aarabi. Fusion of spatial and
visual information for object tracking on iPhone. In Proceedings
of the 16th International Conference on Information Fusion
(FUSION), pages 630–637, July 2013.

[11] D. Herrera, C. J. Kannala, and J. Heikkila. Forget the checker-
board: Practical self-calibration using a planar scene. In Winter
Conference on Applications of Computer Vision (WACV), pages
1–9, 2016.

[12] M. Hwangbo, J.-S. Kim, and T. Kanade. Gyro-aided feature
tracking for a moving camera: Fusion, auto-calibration and
GPU implementation. The International Journal of Robotics
Research, 30(14):1755–1774, 2011.

[13] A. H. Jazwinski. Stochastic Processes and Filtering Theory.

Academic Press, New York, 1970.

[14] C. Jia and B. L. Evans. Online calibration and synchroniza-
In Proceedings of
tion of cellphone camera and gyroscope.
the Global Conference on Signal and Information Processing
(GlobalSIP), pages 731–734. IEEE, 2013.

[15] C. Jia and B. L. Evans. Online camera-gyroscope autocalibra-
tion for cell phones. IEEE Transactions on Image Processing,
23(12):5070–5081, 2014.

[16] A. Karpenko, D. Jacobs, J. Baek, and M. Levoy. Digital video
stabilization and rolling shutter correction using gyroscopes.
Technical Report 2011-3, Stanford University, 2011.

[17] M. Kok. Probabilistic Modeling for Positioning Applications
PhD thesis, Link¨oping University,

Using Inertial Sensors.
Link¨oping, Sweden, 2014.

[18] F. Li, H. Sekkati, J. Deglint, C. Scharfenberger, M. Lamm,
D. Clausi, J. Zelek, and A. Wong. Simultaneous projector-
camera self-calibration for three-dimensional reconstruction and
IEEE Transactions on Computational
projection mapping.
Imaging, 3(1):74–83, March 2017.

[19] M. Li, H. Yu, X. Zheng, and A. I. Mourikis. High-ﬁdelity sensor
modeling and self-calibration in vision-aided inertial naviga-
tion. In International Conference on Robotics and Automation
(ICRA), pages 409–416. IEEE, 2014.

[20] B. D. Lucas and T. Kanade. An iterative image registration
technique with an application to stereo vision. In International
Conference on Artiﬁcial Intelligence (IJCAI), pages 674–679.
Vancouver, BC, Canada, 1981.

[21] J. Maye, P. Furgale, and R. Siegwart. Self-supervised calibration
In IEEE Intelligent Vehicles Symposium

for robotic systems.
(IV), pages 473–480, 2013.

[22] H. Ovr´en and P.-E. Forss´en. Gyroscope-based video stabili-
In International Conference on
sation with auto-calibration.
Robotics and Automation (ICRA), pages 2090–2097. IEEE,
2015.

[23] S. S¨arkk¨a. Bayesian Filtering and Smoothing. Cambridge

University Press, 2013.

[24] M. A. Shelley. Monocular visual

inertial odometry on a
mobile device. Master’s thesis, Technical University of Munich,
Germany, 2014.

[25] J. Shi and C. Tomasi. Good features to track. In Proceedings
of the IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR), pages 593–600, 1994.
[26] A. Solin, S. Cortes, E. Rahtu, and J. Kannala. Inertial odometry
on handheld smartphones. In Proceedings of the International
Conference on Information Fusion (FUSION), 2018.

[27] A. Solin, S. Cortes, E. Rahtu, and J. Kannala. PIVO: Proba-
bilistic inertial-visual odometry for occlusion-robust navigation.
In Proceedings of the IEEE Winter Conference on Applications
of Computer Vision (WACV), pages 616–625, 2018.

[28] J. Sun, P. Wang, Z. Qin, and H. Qiao. Effective self-calibration
for camera parameters and hand-eye geometry based on two
IEEE/CAA Journal of Automatica
feature points motions.
Sinica, 4(2):370–380, 2017.

[29] S. Willi and A. Grundh¨ofer. Robust geometric self-calibration
In International
of generic multi-projector camera systems.
Symposium on Mixed and Augmented Reality (ISMAR), pages
42–51, 2017.

