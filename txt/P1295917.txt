7
1
0
2
 
l
u
J
 
9
 
 
]

G
L
.
s
c
[
 
 
3
v
9
4
5
9
0
.
6
0
7
1
:
v
i
X
r
a

Distributional Adversarial Networks

Chengtao Li ∗
David Alvarez-Melis ∗
Keyulu Xu
Stefanie Jegelka
Suvrit Sra
Massachusetts Institute of Technology

ctli@mit.edu
dalvmel@mit.edu
keyulu@mit.edu
stefje@csail.mit.edu
suvrit@mit.edu

Abstract

We propose a framework for adversarial training that relies on a sample rather than a single
sample point as the fundamental unit of discrimination. Inspired by discrepancy measures and two-
sample tests between probability distributions, we propose two such distributional adversaries
that operate and predict on samples, and show how they can be easily implemented on top of
existing models. Various experimental results show that generators trained with our distributional
adversaries are much more stable and are remarkably less prone to mode collapse than traditional
models trained with pointwise prediction discriminators. The application of our framework to
domain adaptation also results in considerable improvement over recent state-of-the-art.

1 Introduction

Adversarial training of neural networks, especially Generative Adversarial Networks (GANs) [10],
have proven to be a powerful tool for learning rich models, leading to outstanding results in various
tasks such as realistic image generation, text to image synthesis, 3D object generation, and video
prediction [22, 29, 30]. Despite their success, GANs are known to be difﬁcult to train. The generator
and discriminator oscillate signiﬁcantly from iteration to iteration, and slight imbalances in their
capacity frequently causes the training to diverge. Another common problem suffered by GANs is
mode collapse, where the distribution learned by the generator concentrates on a few modes of the
true data distribution, ignoring the rest of the space. In the case of images, this failure results in
generated images that albeit realistic, lack diversity and reduce to a handful of prototypes.

There has been a ﬂurry of research aimed at understanding and addressing the causes behind
the instability and mode collapse of adversarially-trained models. The ﬁrst insights came from
Goodfellow et al. [10], who noted that one of the leading causes of training instability was saturation
of the discriminator. Arjovsky and Bottou [2] formalized this idea by showing that if the two
distributions have supports that are disjoint or concentrated on low-dimensional manifolds that do
not perfectly align, then there exists an optimal discriminator with perfect classiﬁcation accuracy
almost everywhere, and for which usual divergences (KL, Jensen-Shannon) will max-out.
In
follow-up work [3], the authors propose an alternative training scheme based on an adversary that
estimates the Wasserstein distance, instead of the Jensen-Shannon divergence, between real and
generated distributions.

In this work, we take a novel approach at understanding the limitations of adversarial training,
and propose a framework that brings the discriminator closer to a truly distributional adversary.
Through the lens of comparisons of distributions, the objective of the original GAN can be
1
(disguised as
understood as a Jensen-Shannon divergence test that uses a single sample point
batches, but operated on independently by the discriminator). We show that this approach is far

∗Authors contributed equally.
1
Throughout this paper we will use the term sample to refer to a set of instances generated from a distribution, sample

point to refer to an element of that sample.

1

from ideal from the statistical perspective and might partially explain the mode-collapsing behavior
of GANs. Thus, we posit that it is desirable instead to consider discriminators that operate on
samples. There is a vast literature on two-sample tests and population comparisons from which one
can draw inspiration. We propose methods based on two such approaches. Both of these methods
operate on samples and base their predictions upon their aggregated information. Furthermore,
our approach can be understood as a variant of classical kernel two-sample tests, except that now
the kernels are parametrized as deep neural networks and fully learned from data.

A very appealing characteristic of our framework is that it is surprisingly easy to impose on
current methods. We show how off-the-shelf discriminator networks can be made distribution-aware
through simple modiﬁcations to their architecture, yielding an easy plug-and-play setup. We put
our framework to test in various experimental settings, from benchmark image generation tasks to
adversarial domain adaptation. Our experimental results show that this novel approach leads to
more stable training, to remarkably higher performances, and most importantly, to signiﬁcantly
better mode coverage.

2

Contributions. The main contributions of this work are as follows:

We introduce a new distributional framework for adversarial training of neural networks, which

•
operates on genuine samples rather than a sample points.

We propose two types of adversarial networks based on this distributional approach, and show

•
how existing models can seamlessly be adapted to ﬁt within this framework.

We empirically show that our distributional adversarial framework leads to more stable training
•
and signiﬁcantly better mode coverage compared to single-point-sample methods. The direct
application of our framework to domain adaptation results in considerably higher performances
over state-of-the-art.

2 Distributional Approaches to Adversarial Training

2.1 A case against single sample point discriminators

To motivate our distributional approaches, we take the original GAN setting to illustrate intuitively
how training with sample point-based adversaries might lead to mode collapse in the generator.
Recall the GAN objective function:

min
G

max

D {

∼

E

x

Px [log D(x)] + E

z

Pz [log(1

D(G(z)))]

∼

−

}

(2.1)

where D : Rn
→
Rn maps a noise vector z
Px, and G : Rm
original data space. This in turn deﬁnes an implicit distribution P

[0, 1] maps a sample point to the probability that it comes from data distribution
Rm, drawn from a simple distribution Pz, to the

G for G’s generated outputs.

→

∈

To prevent loss saturation early in the training, G is trained to maximize log D(G(z)) in-
stead. As shown by Goodfellow et al. [10], the discriminator converges in the limit to D∗(x) =
Px(x)/(Px(x) + P
G(x)) for a ﬁxed G. In this limiting scenario, it can be shown that given sample
drawn from the noise distribution Pz, the gradient of G’s loss with respect to
z(1), . . . , z(B)
Z =
its parameters θG is given by

}

{

∇θG loss(Z) =

1
B

B
∑
i=1

1
D(G(z(i))) ∇

D(G(z(i)))

G(z(i))

(cid:21)

(cid:20) d
dθ

(2.2)

2

Code is available at https://github.com/ChengtaoLi/dan

2

Figure 1:
Simple setting to
explain the intuition behind
mode-collapse behavior in sam-
ple point discriminators with
logistic loss. Gradients with re-
spect to generated points x are
dD
weighted by the term
dx
(cyan dashed line), so gradients
corresponding to points close to
the second mode will be domi-
nated by those coming from the
ﬁrst mode.

1
D

−

D(x

, where x

(j)
G )
D(x
(j)
G )

where we slightly abuse the notation d

respect to each sample point is weighted by terms of the form ∇

dθ G to denote G(cid:48)s Jacobian matrix. Thus, the gradient with
(j)
G := G(z(j)).
These terms can be interpreted as relative slopes of the discriminator’s conﬁdence function.
Their magnitude depends on the relation between ∂D
(the slope of D around xG) and D(xG),
∂xG
the conﬁdence on xG being drawn from Px. Although the magnitude and sign of this ratio
depend on the relation between these two opposing terms, there is one notable case in which it
is unambiguously low: values of x where the discriminator has high conﬁdence of real samples
and ﬂat slope. In other words, this weighting term will vanish in regions of the space where
the generator’s distribution has constant and low probability compared to the real distribution,
such as neighborhoods of the support of Px where P
G is missing a mode. Figure 1 exempliﬁes
this situation for a simple case in 1D where the real distribution is bimodal and the Generator’s
distribution is currently concentrated around one of the modes. The cyan dashed line corresponds
D(xG)/D(xG), conﬁrming our analysis above that gradients for points
to the weighting term
around the second mode will vanish.

∇

The effect of this catastrophic averaging-out of mode-seeking gradients during training is that
it hampers G’s ability to recover from mode collapse. Whenever G does generate a point in a
region where it misses a mode (which, by deﬁnition, already occurs with low probability), this
example’s gradient (which would update G’s parameters to move mass to this region), will be
heavily down-weighted and therefore dominated by high-weighted gradients of other examples
in the batch, such as those in spiked high-density regions. At a high level, this phenomenon is a
consequence of a myopic discriminator that bases its predictions on a single sample point, leading
to gradients that are not harmonized by global information. Discriminators that predict based on a
whole sample are a natural way to address this, as we will show in the next section.

2.2 Distributional Adversaries

To mitigate the aforementioned failure case, we would like the discriminator to take a full sample
instead of a single sample point into account for each prediction. More concretely, we would like
R that operates on a sample
our discriminator to be a set function M : 2
of
{
potentially varying sizes. We construct the discriminator step-by-step as follows.

x(1), . . . , x(n)

→

Rd

}

Deep Mean Encoder Despite its simple deﬁnition, the mean is a surprisingly useful statistic for
discerning between distributions, and it is central to Maximum Mean Discrepancy (MMD) [8, 11, 24],
a powerful discrepancy measure between distributions that enjoys strong theoretical guarantees.

3

Instead of designing a mapping explicitly, we propose to learn this function in a fully data-driven
way, parametrizing φ as a neural network. Thus, a deep mean encoder (DME) η would have the form

In practice, η only has access to P through samples of ﬁnite size, thus effectively takes the form

η(P) = E

P[φ(x)]

x

∼

η(

x(1), . . . , x(1)

) =

φ(x(i))

{

}

1
n

n
∑
i=1

(2.3)

(2.4)

This distributional encoder forms the basis of our proposed adversarial learning framework. In
what follows, we propose two alternative adversary models that build upon this encoder to
discriminate between samples, and thus generate rich training signal for the generator.

Sample Classiﬁer The most obvious way to use (2.4) as part of a discriminator in adversarial
training is through a classiﬁer. That is, given a vector encoding a sample (drawn either from the
data Px or generated distributions P
G), the classiﬁer ψS outputs 1 to indicate the sample was drawn
from Px and 0 otherwise. The discrepancy between two distributions could then be quantiﬁed as
the conﬁdence of this classiﬁer. Using a logistic loss, this yields the objective function

dS(P

0, P

1) = log(ψS(η(P

1))) + log(1

ψS(η(P

0))),

(2.5)

−

This is analogous to the original GAN objective (2.1), but differs from it in a crucial aspect: here
the expectation is inside of ψS. In other words, whereas in (2.1) the loss of a sample is deﬁned as
the expected loss of the sample points, here the sample loss is a single value, based on the deep
mean embedding of its expected value. Therefore, (2.5) can be thought of an extension of (2.1) to
non-trivial sample sizes. Henceforth, we use MS := ψS ◦
η to denote the full-model (mean encoder
and classiﬁer), and we refer to it as the sample classiﬁer.

Two-sample Discriminator The sample classiﬁer proposed above has one potential drawback:
the nonlinearities of the discriminator function are separable across P
1, restricting the
interactions between distributions to be additive. To address this potential issue, we propose
to shift from a classiﬁcation to a discrepancy objective, that is, given two samples drawn inde-
pendently, the two-sample discriminator must predict whether they were drawn from the same
or different distributions. Concretely, given two encoded representations η(P
1), the
two-sample discriminator ψ2S uses the absolute difference of their deep mean encodings, and
outputs ψ2S(
[0, 1], reﬂecting its conﬁdence on whether the two samples were
indeed drawn from the same distribution. Again with a binary classiﬁcation criterion, the loss of
this prediction would be given by

0) and η(P

0 and P

η(P

η(P

1)

0)

−

∈

)

|

|

d2S(P

0, P

1) =

P

P

0 = P
1
= P
1

(cid:75)

log(ψ2S(
(1

η(P
|
log(ψ2S(

0)
−
η(P

η(P
0)

−

1)
|
η(P

))+
1)

))

|

−

(cid:75)
to denote the full two-sample discriminator model.

|

(2.6)

0 (cid:54)

(cid:74)

(cid:74)
)

·

As before, we use M2S := ψ2S ◦ |

η(

η(

)

·

|

−

3 Distributional Adversarial Network

Now, we use the above distributional adversaries in a new training framework which we call
Distributional Adversarial Network (DAN). This framework can easily be combined with existing
adversarial training algorithms by a simple modiﬁcation in their adversaries. In this section, we

4

examine in detail an example application of DAN to the generative adversarial setting. In the
experiments section we provide an additional application to adversarial domain adaptation.

To integrate a GAN into the DAN framework, we simply add the distributional adversary:

min
G

max
D,Mξ

V(G, D, Mξ ) =

λ1

E

x

Px,z

Pz [log D(x) + log(1

D(G(z)))] + λ2dξ (Px, P

G),

(3.1)

∼

∼

−

where ξ

S, 2S

indicates whether we use the sample classiﬁer or two-sample discriminator.

∈ {

}

A Note on Gradient Weight Sharing In the case of the sample classiﬁer (ξ = S) and with only
the distributional adversary (λ1 = 0 and λ2 > 0 above), a derivation as in Section 2.1 shows that
the gradient of a sample of B points generated by G is

∇θG loss(Z) =

1
ψS(ηB) ∇

ψS(ηB)

(cid:32)

1
B

B
∑
i=1 ∇

ψS(G(z(i)))

G(z(i))

(3.2)

(cid:21)(cid:33)

(cid:20) d
dθ

}

{

G(z(1)), . . . , G(x(B))

) for ease of notation. Note that, as opposed to (2.2),
where we use ηB := η(
the gradient for each z(i) is weighted by the same left-most discriminator conﬁdence term. This
has the effect of sharing information across samples when updating gradients: whether a sample
(encoded as a vector ηB) can fool the discriminator or not will have an effect on every sample
point’s gradient. In addition, this helps prevent the vanishing gradient for points generated in
regions of mode collapse, since other sample points in the batch will come, with high probability,
ψS(ηB), will
from regions of high density, and thus the weight vector that this sample point gets,
be less likely to be zero than in the single-sample-point discriminator case. Several interesting
observations arise from this analysis. First, (3.2) suggests that the true power of this sample-based
setting lies in choosing a discriminator ψξ that, through non-linearities, enforces interaction between
the points in the sample. Second, the notion of sharing information across examples occurs also
in batch normalization (BN) [13], although the mechanism to achieve this and the underlying
motivation for doing it are different. This connection, which might provide additional theoretical
explanation for BN’s empirical success, is supported by our experimental results, in which we
show that the improvements brought by our framework are much more pronounced when BN
is not used. While the analysis here is certainly not a rigorous one, the intuitive justiﬁcation for
sample-based aggregation is clear, and is conﬁrmed by our experimental results.

∇

Training During training, all expectations of over data/noise distributions will be approximated
via ﬁnite-sample averages. In each training iteration, we use minibatches (i.e. samples) from the
data/noise distributions. We train by alternating between updates of the generator and adversaries.
While for DAN-S the training procedure is similar to that of GAN, DAN-2S requires a modiﬁed
training scheme. Due to the form of the two-sample discriminator, we want a balanced exposure to
pairs of samples drawn from the same and different distributions. Thus, every time we update
Pz from data and
M2S, we draw minibatches X =
x(i)
noise distributions. We then split each sample into two parts, X1 :=
,

2 +1
Z1 :=
and use the discriminator M2S to predict on each pair of
(X1, G(Z2)), (G(Z1), X2), (X1, X2) and (G(Z1), G(Z2)) with target outputs 0, 0, 1 and 1, respectively.
A visualization of this procedure is shown in Figure 2 and the detailed training procedure is
described in Appendix A.

z(1), . . . , z(B)
x(i)

} ∼
B
i=1, X2 =
2

Px and Z =

x(1), . . . , x(B)

i=1, Z2 =

B
i= B

B
i= B

} ∼

z(i)

z(i)

2 +1

}

{

}

{

{

}

{

{

{

}

B
2

5

Figure 2: DAN-S and DAN-2S models and corresponding losses, where X =

Z =

z(i)

B
i=1 ∼

}

{

Pz, X1 :=

x(i)

{

B
2

}

i=1, X2 =

x(i)

B
i= B

}

2 +1

{

, Z1 :=

z(i)

{

B
2

}

i=1 and Z2 =

Px,

B
i=1 ∼

x(i)
}
{
z(i)
B
i= B

}

{

2 +1

3.1 Related Work

Discrepancy Measures The distributional adversary bears close resemblance to two-sample
tests [15], where the model takes two samples drawn from potentially distinct distributions as
input and produces a discrepancy value quantifying how different two distributions are. A popular
kernel-based variant is the Maximum Mean Discrepancy (MMD) [8, 11, 24]:

MMD2(U, V) =

1

n ∑n

i=1 φ(ui)

1

m ∑m

j=1 φ(vj)

2
2

(cid:107)

−

(cid:107)

where φ(
) is some feature mapping, and k(u, v) = φ(u)(cid:62)φ(v) is the corresponding kernel function.
An identity function for φ corresponds to computing the distance of the sample means. More
complex kernels result in distances of higher-order statistics between two samples.

·

In adversarial models, MMD and its variants (e.g., central moment discrepancy (CMD)) have
been used either as a direct objective for the target model or as a form of adversary [7, 16, 25, 32, 32].
However, such discrepancy measure requires hand-picking the kernel. An adaptive feature function,
in contrast, may be able to adapt to the given distributions and thereby discriminate better.

Our distributional adversary tackles the above drawbacks and, at the same time, generalizes
many discrepancy measures given that a neural network is a universal approximator. Since it is
trainable, it can evolve as training proceeds. Speciﬁcally, it can be of simpler form when target

6

model is weak at the beginning of training, and becomes more complex as target model becomes
stronger.

Minibatch Discrimination Another relevant line of work involves training generative models by
looking at statistics at minibatch level, known as minibatch discrimination, initially developed in
[23] to stabilize GAN training. Batch normalization [13] can be seen as performing some form of
minibatch discrimination and it has been shown helpful for GAN training [20]. Zhao et al. [34]
proposed a repelling regularizer that operates on a minibatch and orthogonalizes the pairwise
sample representation, keeping the model from concentrating on only a few modes.

Our distributional adversaries can be seen as learning a minibatch discriminator that adapts as
the target model gets stronger. It not only enjoys the beneﬁts of minibatch discrimination, leading
to better mode coverage and stabler training (as shall be seem in Section 4), but also eliminates the
need of hand-crafting various forms of minibatch discrimination objectives.

Permutation Invariant Networks The deep mean encoder takes an (unordered) sample to pro-
duce a mean encoding. A network whose output is invariant to the order of inputs is called
permutation invariant network. Formally, let f be the network mapping, we have f (x1, x2, . . . , xn) =
f (xσ(1), xσ(2), . . . , xσ(n)), where n could be of varying sizes and σ(

) is any permutation over [n].

The problem of dealing with set inputs is being studied very recently. Vinyals et al. [28] consider
dealing with unordered variable-length inputs with content attention mechanism. In later work
of [21, 31], the authors consider the way of ﬁrst embedding all samples into a ﬁxed-dimensional
latent space, and then adding them up to form a vector of the same dimension, which is further
fed into another neural network. In [17], the authors proposed a similar network for embedding a
set of images into a latent space. They use a weighted summation over latent vectors, where the
weights are learned through another network. The structure of our deep mean encoder resembles
these networks in that it is permutation-invariant, but differs in its motivation—mean discrepancy
measures—as well as its usage within discriminators in adversarial training settings.

·

Other Related Work Extensive work has been devoted to resolving the instability and mode
collapse problems in GANs. One common approach is to train with more complex network
architecture or better-behaving objectives [3, 4, 12, 20, 33, 34]. Another common approach is to add
more discriminators or generators [6, 26] in the hope that training signals from multiple sources
could lead to more stable training and better coverage of modes.

4 Empirical Results

3

We demonstrate the effectiveness of DAN training by applying it to generative models and do-
main adaptation.
In generative models, we observe remarkably better mode recovery than
non-distributional models on synthetic and real datasets, through both qualitative and quantitative
evaluation of generated sample diversity. In domain adaptation we leverage distributional adver-
saries to align latent spaces of source and target domains, yielding considerable improvements
over state-of-the-art.

4.1 Synthetic Data: Mode Recovery in Multimodal Distributions

We ﬁrst apply DAN to generative models, where the data is a mixture of 8 two-dimensional
Gaussian distributions, with means aligned on a circle (bottom right of Figure 3). The goal is for

3

Please refer to the appendix for all details on dataset, network architecture and training procedures.

7

Figure 3: Results for mode recovery on data generated from 8 Gaussian mixtures. The rightmost
distribution is the true data distribution. While with GAN training the generator is only able to
capture 1 of 8 modes, in both DAN-S and DAN-2S training we are able to recover all 8 modes.

the generator to recover 8 modes on the circle. A similar task has been considered as a proof of
concept for mode recovery in [4, 19]. All architectures are simple feed-forward networks with ReLU
activations. To test the power of mode-recovery for DAN, we set λ1 = 0 and λ2 = 1, so only the
distributional adversary is included in training. The results are shown in Figure 3.

While the GAN generator produces samples oscillating between different modes, our distri-
butional methods consistently and stably recover all eight modes of the true data distribution.
Among these two, DAN-2S takes slightly longer to converge. We observe this multi-modal seeking
behavior from early in the process, where the generator ﬁrst distributes mass broadly covering all
modes, and then subsequently sharpens the generated distribution in later iterations.

Note that here we set the architecture of DAN to be the same as GAN, with the only difference
that the DAN adversary takes one extra step of averaging latent representations in the middle,
which does not increase the number of parameters. Thus, DAN achieves a signiﬁcantly better
mode recovery but with the same number of network parameters.

4.2 MNIST: Recovering Mode Frequencies

Mode recovery entails not only capturing a mode, i.e., generating samples that lie in the correspond-
ing mode, but also recovering the true probability mass of the mode. Next, we evaluate our model
on this criterion. To do so, we train DAN on MNIST, which has a 10-class balanced distribution over
digits, and compare the frequencies of generated classes against this target uniform distribution.
Since the generated samples are unlabeled, we train an external classiﬁer on MNIST to label the
generated data.

Besides the original GAN, we also compared to two recently proposed generative models:
RegGAN [4] and EBGAN [34]. To keep the approaches comparable, we use a similar neural
network architecture in all cases, which we did not tailor to any particular model. We trained
models without Batch Normalization (BN), except for RegGAN and EBGAN, which we observed
to consistently beneﬁt from it. We found that in tasks beyond simple data generation in low-
dimensional spaces, it is beneﬁcial to set λ1, λ2 > 0 for DAN. In this case, the generator will receive
training signals from generated samples both locally and globally, since both local (single-sample)
and distributional adversaries are used. Throughout the experiment we set λ1 = 1 and λ2 = 0.2.
We show the results in Figure 4 and 5. Training with the original GAN leads to generators
that place too much mass on some modes and ignore some others. While RegGAN and EBGAN

8

Figure 4: Class distribution of samples generated by various models on MNIST. Note that we
showcase the best one out of 10 random runs for GAN, RegGAN and EBGAN in terms of distribution
entropy to give them an unfair advantage. For DAN we simply showcase a random run (the
variance of the performance in FIgure 5 indicates that DAN-ξ is stable across all runs). The best
runs for RegGAN and EBGN also recover mode frequencies to some extent, but the performance
varies a lot across different runs as in Figure 5.

Figure 5: Performances of mode fre-
quency recovery under 2 different mea-
sures: entropy of generated mode distri-
bution and total variation distances be-
tween generated mode distribution and
uniform one. DAN achieves the best and
most stable mode frequency recovery.

sometimes generate more uniform distributions (such as the best-of-ten shown in Figure 4), this
varies signiﬁcantly across repetitions (c.f. Figure 5), an indication of instability. DAN, on the other
hand, consistently recovers the true frequencies across repetitions.

4.3

Image Generation: Sample Diversity as Evidence of Mode Coverage

We also test DAN on a harder image generation problem: generating faces. We train DCGAN
and DAN with λ1 = 1, λ2 = 0.2 on the unlabeled CelebA [18] dataset and compare the generated
samples. We use DCGAN with BN, and DAN-S/2S without BN. Figure 6 visualizes half a minibatch
(32 samples).

As discussed in Section 3.1, both BN and DAN can be seen as techniques applying some form
of normalization across a sample (minibatch). As seen in Figure 6, both DCGAN with BN and
DAN-ξ promotes diversity in generated samples as compared to DCGAN without BN. Such diverse
samples indicate a better coverage of modes in the original dataset.

4.4 Domain Adaptation

For the last set of experiments, we test our DAN framework in the context of unsupervised domain
adaptation, where the target domain data is fully unlabeled. This problem has been recently
approached from the adversarial training perspective by using a domain classiﬁer adversary to
enforce domain-invariant representations [9, 27]. Following this approach, we use two neural
network encoders (one for each domain) and a shared classiﬁer. Enforcing the outputs of these
encoders to be indistinguishable allows for a classiﬁer trained on the source domain to be used in
combination with the target encoder. For the adversary we use either DAN-S or DAN-2S to discern

9

(a) DCGAN, without BN

(b) DCGAN, with BN

(c) DAN-S, without BN (d) DAN-2S, without BN

Figure 6: GAN, DAN-S and DAN-2S trained on CelebA dataset to generate faces.

between samples of encoded representations from the two domains.

We ﬁrst compare algorithms on the Amazon reviews dataset preprocessed by Chen et al. [5]. It
consists of four domains: books, dvd, electronics and kitchen appliances, each of which contains
reviews encoded in 5, 000 dimensional feature vectors and binary labels indicating if the review
is positive or negative. We compare against the domain-adversarial neural network (DANN) of
Ganin et al. [9], which uses a similar framework but a single-sample-point discriminator. To make
the comparison meaningful, we use the same architectures and parameters on all models, with
the only difference being the representation averaging layer of our sample-based adversaries. We
did not extensively tune parameters in any model. The results are shown in Table 1. Our models
consistently outperform GAN-based DANN on most source-target pairs, with a considerable
average improvement in accuracy of 1.41% for DAN-S and 0.92% for DAN-2S.

Lastly, we test the effectiveness of our proposed framework on a different domain adaptation
task, this time for image label prediction. Speciﬁcally, we tackle the task of adapting from MNIST to
MNIST-M, which is obtained by blending digits over patches randomly extracted from color photos
from BSDS500 [1] as in [9]. The results are shown in Table 2. DAN-2S yields a strong improvement
3%. Again, the DAN-S and DAN-2S architectures consist of a simple plug-
over DANN by
and-play adaptation of DANN to our distributional framework with little modiﬁcations, which
demonstrates the ease of use of the proposed DAN criteria on top of existing models.

∼

5 Discussion and Future Work

The distributional adversarial framework we propose here is another step in an emerging trend
of shifting from traditional classiﬁcation losses to richer criteria better suited to ﬁnite-sample
distribution discrimination. The experimental results obtained with this new approach offer a
promising glimpse of the advantages of genuine sample-based discriminators over sample point
alternatives, while the simplicity and ease of implementation makes this approach an appealing
plug-in addition to existing models.

Our framework is fairly general and opens the door to various possible extensions. The two

10

Source

Target

dvd
electronics
kitchen
books
electronics
kitchen
books

books
books
books
dvd
dvd
dvd
electronics
electronics dvd
electronics
kitchen
kitchen
kitchen

kitchen
books
dvd
electronics

avg. imp.

DANN
77.14(1.42)
74.38(1.12)
77.23(1.09)
75.01(0.83)
75.62(0.85)
79.41(0.96)
70.34(1.01)
69.42(2.60)
83.64(0.48)
69.54(0.88)
69.36(2.24)
82.68(0.47)
0.00

DAN-S
77.79(0.51)
75.68(0.42)
79.01(0.41)
75.38(0.88)
75.98(0.91)
80.37(0.91)
72.41(0.77)
72.39(1.99)
85.02(0.37)
70.39(0.85)
73.85(0.82)
82.47(0.16)

1.41

DAN-2S
78.22(0.78)
74.78(1.08)
76.90(0.58)
74.36(2.16)
76.28(1.37)
79.96(0.89)
72.02(1.92)
72.34(1.65)
84.66(0.68)
70.45(1.41)
71.50(2.93)
83.38(0.28)
0.92

Table 1: Domain adaptation results on the Amazon review dataset. The numbers correspond to
mean accuracies (plus one standard deviation) over 5 runs.

Source Only Target Only
95.60(0.16)
52.20(1.94)

DANN
76.13(1.63)

DAN-S
77.07(1.94)

DAN-2S

79.214(2.19)

Table 2: Results of domain adaptation from MNIST to MNIST-M, averaged over 5 different runs.

types of discriminators proposed here are by no means the only options. There are many other
approaches in the distributional discrepancy literature to draw inspiration from. One aspect that
warrants additional investigation is the effect of sample size on training stability and mode coverage.
It is sensible to expect that in order to maintain global discrimination power in settings with highly
multimodal distributions, the size of samples fed to the discriminators should grow, at least with
the number of modes. Formalizing this relationship is an interesting avenue for future work.

References

[1] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image

segmentation. IEEE Trans. on Pattern Analysis and Machine Intelligence, 2011.

[2] M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial
networks. In NIPS 2016 Workshop on Adversarial Training. In review for ICLR, volume 2016, 2017.

[3] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.

[4] T. Che, Y. Li, A. P. Jacob, Y. Bengio, and W. Li. Mode regularized generative adversarial

networks. International Conference on Learning Representations, 2017.

[5] M. Chen, Z. Xu, K. Weinberger, and F. Sha. Marginalized denoising autoencoders for domain

adaptation. IEEE Trans. on Pattern Analysis and Machine Intelligence, 2012.

11

[6] I. Durugkar, I. Gemp, and S. Mahadevan. Generative multi-adversarial networks. International

Conference on Learning Representations, 2017.

[7] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training generative neural networks via
maximum mean discrepancy optimization. In Uncertainty in Artiﬁcial Intelligence (UAI), 2015.

[8] K. Fukumizu, A. Gretton, X. Sun, and B. Sch ¨olkopf. Kernel measures of conditional depen-

dence. In Advances in neural information processing systems, pages 489–496, 2008.

[9] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and
V. Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning
Research, 2016.

[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems,
pages 2672–2680, 2014.

[11] A. Gretton, O. Bousquet, A. Smola, and B. Sch ¨olkopf. Measuring statistical dependence with
hilbert-schmidt norms. In International conference on algorithmic learning theory, pages 63–77.
Springer, 2005.

[12] X. Huang, Y. Li, O. Poursaeed, J. Hopcroft, and S. Belongie. Stacked generative adversarial

networks. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

[13] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

[14] D. Kingma and J. Ba. Adam: A method for stochastic optimization.

arXiv preprint

arXiv:1412.6980, 2014.

Media, 2006.

Machine Learning (ICML), 2015.

arXiv:1704.03373, 2017.

[15] E. L. Lehmann and J. P. Romano. Testing statistical hypotheses. Springer Science & Business

[16] Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In Int. Conference on

[17] Y. Liu, J. Yan, and W. Ouyang. Quality aware network for set to set recognition. arXiv preprint

[18] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild.

In Int.

Conference on Computer Vision (ICCV), 2015.

[19] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks.

International Conference on Learning Representations, 2017.

[20] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep

convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

[21] S. Ravanbakhsh, J. Schneider, and B. Poczos. Deep learning with sets and point clouds. In

International Conference on Learning Representations, Workshop Track, 2017.

[22] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial
text to image synthesis. In Proceedings of The 33rd International Conference on Machine Learning,
volume 3, 2016.

[23] T. Salimans, I. Good fellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved

techniques for training gans. arXiv preprint arXiv:1606.03498, 2016.

12

[24] A. Smola, A. Gretton, L. Song, and B. Sch ¨olkopf. A hilbert space embedding for distributions.

In International Conference on Algorithmic Learning Theory, pages 13–31. Springer, 2007.

[25] D. J. Sutherland, H.-Y. Tung, H. Strathmann, S. De, A. Ramdas, A. Smola, and A. Gretton.
Generative models and model criticism via optimized maximum mean discrepancy.
In
International Conference on Learning Representations, 2017.

[26] I. Tolstikhin, S. Gelly, O. Bousquet, C.-J. Simon-Gabriel, and B. Sch ¨olkopf. Adagan: Boosting

generative models. arXiv preprint arXiv:1701.02386, 2017.

[27] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation.

arXiv preprint arXiv:1702.05464, 2017.

[28] O. Vinyals, S. Bengio, and M. Kudlur. Order matters: Sequence to sequence for sets. Interna-

tional Conference on Learning Representations, 2016.

[29] C. Vondrick, H. Pirsiavash, and A. Torralba. Generating videos with scene dynamics. In

Advances In Neural Information Processing Systems, pages 613–621, 2016.

[30] J. Wu, C. Zhang, T. Xue, W. T. Freeman, and J. B. Tenenbaum. Learning a probabilistic
latent space of object shapes via 3d generative-adversarial modeling. In Advances in Neural
Information Processing Systems, pages 82–90, 2016.

[31] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. Salakhutdinov, and A. Smola. Deep sets.

arXiv preprint arXiv:1703.06114, 2016.

[32] W. Zellinger, T. Grubinger, E. Lughofer, T. Natschl¨ager, and S. Saminger-Platz. Central moment
discrepancy (CMD) for domain-invariant representation learning. In International Conference
on Learning Representations, 2017.

[33] H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas. Stackgan: Text to
photo-realistic image synthesis with stacked generative adversarial networks. arXiv preprint
arXiv:1612.03242, 2016.

[34] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. 2017.

13

A Training Algorithm for DAN

We show the whole training procedure for DAN in Algorithm 1.

B 2D Gaussian Mixture Experiment Details

Following [19], we generate a mixture of 8 Gaussian distribution with mean equally spaced on a
circle of radius 2 and variances of 0.01.

The generator consists of a fully connected network with 3 hidden layers of size 128 with ReLU
activations, followed by a linear projection to 2 dimensions. The discriminator consists of a fully
connected network with 3 hidden layers of size 32 with ReLU activations, followed by a linear
projection to 1 dimension. Latent vectors are sampled uniformly from [
1, 1]256. Distributional
adversaries starts with 2 hidden layers of size 32, then the latent representations are averaged
across the batch. The mean representation is then fed to 1 hidden layer of size 32 and a linear
projection to 1 dimension. For DAN we set λ1 = 0 and λ2 = 1.

−

Note that the number of parameters in GAN is the same as that in DAN.
We use Adam [14] with learning rate of 1E-4 and β1 = 0.5 as training algorithm. Minibatch
size is ﬁxed to 512 and we train the network for 25000 iterations by alternating between updates of
generator and adversaries.

C MNIST Experiment Details

We use the same architecture for generator across all models, which consists of 3 hidden layers
with 256, 512 and 1,024 neurons each and with ReLU activations, followed by a fully connected
linear layer to map it to 784 dimensional vector and ﬁnally a sigmoid activation for output. Latent
1, 1]100. Decoder in EBGAN has the same architecture and
vectors are uniform samples from [
encoders in RegGAN and EBGAN are the reverse of the decoder. For GAN, RegGAN and DAN
the discriminator is the reverse of the generator except that the ﬁnal linear function maps the vector
of dimension 256 to 1. Distributional adversary in DAN has the same structure as the adversary in
GAN but with an averaging of vectors across the batch before it is mapped to vector of dimension
256. And ﬁnally for RegGAN we set the two hyperparameters (λ1 and λ2 in the original paper [4])
to be 0.2 and 0.4, and for EBGAN we set the margin (m in original paper [34]) to be 5.

−

(a) GAN

(b) DAN-S

(c) DAN-2S

Figure 7: GAN, DAN-S and DAN-2S trained on MNIST dataset to generate digits. DAN-S and
DAN-2S clearly demonstrate better mode coverage and frequency recovery compared to GAN.

14

Algorithm 1 Training Procedure for DAN-S/2S.

Input:total number of iterations T, size of minibatch B, step number k, model mode ξ
for i = 1 to T do

S, 2S

}

∈ {

Sample minibatch X =
Update discriminator by optimizing one step:

x(1), . . . , x(B)

} ∼

{

Px, Z =

{

z(1), . . . , z(B)

Pz

} ∼

λ1
B

(cid:104)

B
∑
i=1

log(D(x(i))) + log(1

D(G(z(i))))

(cid:105)

−

if mod(i,k)=0 then

Sample minibatch X and Z
if ξ = S then

Update MS by optimizing one step:

λ2(log(MS(η(X))) + log(1

MS(η(G(Z)))))

−

else if ξ = 2S then

Divide X and Z into X1, X2 and Z1, Z2
Update M2S by optimizing one step:

λ2
2 {

))+

|

η(X2)

log(M2S(

η(X1)

|

log(M2S(
(1
(1

|
log(M2S(
log(M2S(

−
η(G(Z1))
−
η(X1)
−
η(G(Z1))

−
−

|
|

η(G(Z2))
|
η(G(Z2))
η(X2)

))+
))+
))

|
|

}

−

end if

end if
Sample minibatch X, Z
if ξ = S then

Update G by optimizing one step:

λ1
B

B
∑
i=1

−

log(1

D(G(z(i)))) + λ2 log(1

MS(η(G(Z)))))

−

else if ξ = 2S then

Divide X and Z into X1, X2 and Z1, Z2
Update G by optimizing one step:

λ1
B

B
∑
i=1
λ2(1
(1

−

log(1

D(G(z(i))))+

−

log(M2S(

−
log(M2S(

|

η(X1)
−
η(G(Z1))
−

η(G(Z2))
|
))
η(X2)
}

|

|

))+

end if

end for

15

We use Adam with learning rate of 5E-4 and β1 = 0.5 for training. Minibatch size is ﬁxed to
256 and we train the network for 50 epochs. For DAN we update the distributional adversaries
every 5 iterations. For all other models we train by alternating between generator and adversaries
(and potentially encoders and decoders).

We showcase the generated digits by GAN, DAN-S and DAN-2S in Figure 7, where we clearly

see a better mode coverage of DAN-S and DAN-2S over GAN.

D CelebA Experiment Details

4

−

We use public available code
for DCGAN. We set the network architecture the same as default
in the code. The generator consists of a fully connected linear layer mapping from latent space
1, 1]100 to dimension 8,192, followed by 4 layers of deconvolution layers with ReLU activation
of [
except the last layer which is followed by tanh. The discriminator is the “reverse” of generator
except that the activation function is Leaky ReLU and for the last layer, which is a linear mapping
to 1 dimension and a sigmoid activation. DAN has the same architecture as DCGAN, except for
the distributional adversary where it has one more layer of linear mapping with ReLU with 1,024
unites before the last linear mapping. We set λ1 = 1 and λ2 = 0.2.

All images are cropped to be 64

64. We use Adam with learning rate of 2E-4 and β1 = 0.5 for
training. Batch size is ﬁxed to 64 and we train the network for 20 epochs. We update the network
by alternating between generator and adversaries.

×

More samples from different models are shown in Figure 8.

E Domain Adaptation Experiment Details

5

We adapt public available code
for DANN to train on Amazon and MNIST-M datasets. For the
former dataset, we let the encoder for source/target domains consists of 3 fully connected linear
layer mapping with ReLU activation with numbers of hidden units being 1000, 500, 100 and is then
mapped to 2-dimensional vector for both adversary and classiﬁer. For the latter dataset we set the
parameters to be the default values in the code. We set λ1 = λ2 = 0.1 and λ1 = λ2 = 1 for the two
datasets respectively.

4https://github.com/carpedm20/DCGAN-tensorflow
5https://github.com/pumpikano/tf-dann

16

(a) DCGAN, trained without BN

(b) DCGAN, trained with BN

(c) DAN-S, trained without BN

(d) DAN-2S, trained without BN

Figure 8: GAN, DAN-S and DAN-2S trained on CelebA dataset to generate faces. DAN-S and
DAN-2S demonstrate more diversity in generated samples compared to GAN trained without BN,
and generates more realistic samples compared to that trained with BN.

17

7
1
0
2
 
l
u
J
 
9
 
 
]

G
L
.
s
c
[
 
 
3
v
9
4
5
9
0
.
6
0
7
1
:
v
i
X
r
a

Distributional Adversarial Networks

Chengtao Li ∗
David Alvarez-Melis ∗
Keyulu Xu
Stefanie Jegelka
Suvrit Sra
Massachusetts Institute of Technology

ctli@mit.edu
dalvmel@mit.edu
keyulu@mit.edu
stefje@csail.mit.edu
suvrit@mit.edu

Abstract

We propose a framework for adversarial training that relies on a sample rather than a single
sample point as the fundamental unit of discrimination. Inspired by discrepancy measures and two-
sample tests between probability distributions, we propose two such distributional adversaries
that operate and predict on samples, and show how they can be easily implemented on top of
existing models. Various experimental results show that generators trained with our distributional
adversaries are much more stable and are remarkably less prone to mode collapse than traditional
models trained with pointwise prediction discriminators. The application of our framework to
domain adaptation also results in considerable improvement over recent state-of-the-art.

1 Introduction

Adversarial training of neural networks, especially Generative Adversarial Networks (GANs) [10],
have proven to be a powerful tool for learning rich models, leading to outstanding results in various
tasks such as realistic image generation, text to image synthesis, 3D object generation, and video
prediction [22, 29, 30]. Despite their success, GANs are known to be difﬁcult to train. The generator
and discriminator oscillate signiﬁcantly from iteration to iteration, and slight imbalances in their
capacity frequently causes the training to diverge. Another common problem suffered by GANs is
mode collapse, where the distribution learned by the generator concentrates on a few modes of the
true data distribution, ignoring the rest of the space. In the case of images, this failure results in
generated images that albeit realistic, lack diversity and reduce to a handful of prototypes.

There has been a ﬂurry of research aimed at understanding and addressing the causes behind
the instability and mode collapse of adversarially-trained models. The ﬁrst insights came from
Goodfellow et al. [10], who noted that one of the leading causes of training instability was saturation
of the discriminator. Arjovsky and Bottou [2] formalized this idea by showing that if the two
distributions have supports that are disjoint or concentrated on low-dimensional manifolds that do
not perfectly align, then there exists an optimal discriminator with perfect classiﬁcation accuracy
almost everywhere, and for which usual divergences (KL, Jensen-Shannon) will max-out.
In
follow-up work [3], the authors propose an alternative training scheme based on an adversary that
estimates the Wasserstein distance, instead of the Jensen-Shannon divergence, between real and
generated distributions.

In this work, we take a novel approach at understanding the limitations of adversarial training,
and propose a framework that brings the discriminator closer to a truly distributional adversary.
Through the lens of comparisons of distributions, the objective of the original GAN can be
1
(disguised as
understood as a Jensen-Shannon divergence test that uses a single sample point
batches, but operated on independently by the discriminator). We show that this approach is far

∗Authors contributed equally.
1
Throughout this paper we will use the term sample to refer to a set of instances generated from a distribution, sample

point to refer to an element of that sample.

1

from ideal from the statistical perspective and might partially explain the mode-collapsing behavior
of GANs. Thus, we posit that it is desirable instead to consider discriminators that operate on
samples. There is a vast literature on two-sample tests and population comparisons from which one
can draw inspiration. We propose methods based on two such approaches. Both of these methods
operate on samples and base their predictions upon their aggregated information. Furthermore,
our approach can be understood as a variant of classical kernel two-sample tests, except that now
the kernels are parametrized as deep neural networks and fully learned from data.

A very appealing characteristic of our framework is that it is surprisingly easy to impose on
current methods. We show how off-the-shelf discriminator networks can be made distribution-aware
through simple modiﬁcations to their architecture, yielding an easy plug-and-play setup. We put
our framework to test in various experimental settings, from benchmark image generation tasks to
adversarial domain adaptation. Our experimental results show that this novel approach leads to
more stable training, to remarkably higher performances, and most importantly, to signiﬁcantly
better mode coverage.

2

Contributions. The main contributions of this work are as follows:

We introduce a new distributional framework for adversarial training of neural networks, which

•
operates on genuine samples rather than a sample points.

We propose two types of adversarial networks based on this distributional approach, and show

•
how existing models can seamlessly be adapted to ﬁt within this framework.

We empirically show that our distributional adversarial framework leads to more stable training
•
and signiﬁcantly better mode coverage compared to single-point-sample methods. The direct
application of our framework to domain adaptation results in considerably higher performances
over state-of-the-art.

2 Distributional Approaches to Adversarial Training

2.1 A case against single sample point discriminators

To motivate our distributional approaches, we take the original GAN setting to illustrate intuitively
how training with sample point-based adversaries might lead to mode collapse in the generator.
Recall the GAN objective function:

min
G

max

D {

∼

E

x

Px [log D(x)] + E

z

Pz [log(1

D(G(z)))]

∼

−

}

(2.1)

where D : Rn
→
Rn maps a noise vector z
Px, and G : Rm
original data space. This in turn deﬁnes an implicit distribution P

[0, 1] maps a sample point to the probability that it comes from data distribution
Rm, drawn from a simple distribution Pz, to the

G for G’s generated outputs.

→

∈

To prevent loss saturation early in the training, G is trained to maximize log D(G(z)) in-
stead. As shown by Goodfellow et al. [10], the discriminator converges in the limit to D∗(x) =
Px(x)/(Px(x) + P
G(x)) for a ﬁxed G. In this limiting scenario, it can be shown that given sample
drawn from the noise distribution Pz, the gradient of G’s loss with respect to
z(1), . . . , z(B)
Z =
its parameters θG is given by

}

{

∇θG loss(Z) =

1
B

B
∑
i=1

1
D(G(z(i))) ∇

D(G(z(i)))

G(z(i))

(cid:21)

(cid:20) d
dθ

(2.2)

2

Code is available at https://github.com/ChengtaoLi/dan

2

Figure 1:
Simple setting to
explain the intuition behind
mode-collapse behavior in sam-
ple point discriminators with
logistic loss. Gradients with re-
spect to generated points x are
dD
weighted by the term
dx
(cyan dashed line), so gradients
corresponding to points close to
the second mode will be domi-
nated by those coming from the
ﬁrst mode.

1
D

−

D(x

, where x

(j)
G )
D(x
(j)
G )

where we slightly abuse the notation d

respect to each sample point is weighted by terms of the form ∇

dθ G to denote G(cid:48)s Jacobian matrix. Thus, the gradient with
(j)
G := G(z(j)).
These terms can be interpreted as relative slopes of the discriminator’s conﬁdence function.
Their magnitude depends on the relation between ∂D
(the slope of D around xG) and D(xG),
∂xG
the conﬁdence on xG being drawn from Px. Although the magnitude and sign of this ratio
depend on the relation between these two opposing terms, there is one notable case in which it
is unambiguously low: values of x where the discriminator has high conﬁdence of real samples
and ﬂat slope. In other words, this weighting term will vanish in regions of the space where
the generator’s distribution has constant and low probability compared to the real distribution,
such as neighborhoods of the support of Px where P
G is missing a mode. Figure 1 exempliﬁes
this situation for a simple case in 1D where the real distribution is bimodal and the Generator’s
distribution is currently concentrated around one of the modes. The cyan dashed line corresponds
D(xG)/D(xG), conﬁrming our analysis above that gradients for points
to the weighting term
around the second mode will vanish.

∇

The effect of this catastrophic averaging-out of mode-seeking gradients during training is that
it hampers G’s ability to recover from mode collapse. Whenever G does generate a point in a
region where it misses a mode (which, by deﬁnition, already occurs with low probability), this
example’s gradient (which would update G’s parameters to move mass to this region), will be
heavily down-weighted and therefore dominated by high-weighted gradients of other examples
in the batch, such as those in spiked high-density regions. At a high level, this phenomenon is a
consequence of a myopic discriminator that bases its predictions on a single sample point, leading
to gradients that are not harmonized by global information. Discriminators that predict based on a
whole sample are a natural way to address this, as we will show in the next section.

2.2 Distributional Adversaries

To mitigate the aforementioned failure case, we would like the discriminator to take a full sample
instead of a single sample point into account for each prediction. More concretely, we would like
R that operates on a sample
our discriminator to be a set function M : 2
of
{
potentially varying sizes. We construct the discriminator step-by-step as follows.

x(1), . . . , x(n)

→

Rd

}

Deep Mean Encoder Despite its simple deﬁnition, the mean is a surprisingly useful statistic for
discerning between distributions, and it is central to Maximum Mean Discrepancy (MMD) [8, 11, 24],
a powerful discrepancy measure between distributions that enjoys strong theoretical guarantees.

3

Instead of designing a mapping explicitly, we propose to learn this function in a fully data-driven
way, parametrizing φ as a neural network. Thus, a deep mean encoder (DME) η would have the form

In practice, η only has access to P through samples of ﬁnite size, thus effectively takes the form

η(P) = E

P[φ(x)]

x

∼

η(

x(1), . . . , x(1)

) =

φ(x(i))

{

}

1
n

n
∑
i=1

(2.3)

(2.4)

This distributional encoder forms the basis of our proposed adversarial learning framework. In
what follows, we propose two alternative adversary models that build upon this encoder to
discriminate between samples, and thus generate rich training signal for the generator.

Sample Classiﬁer The most obvious way to use (2.4) as part of a discriminator in adversarial
training is through a classiﬁer. That is, given a vector encoding a sample (drawn either from the
data Px or generated distributions P
G), the classiﬁer ψS outputs 1 to indicate the sample was drawn
from Px and 0 otherwise. The discrepancy between two distributions could then be quantiﬁed as
the conﬁdence of this classiﬁer. Using a logistic loss, this yields the objective function

dS(P

0, P

1) = log(ψS(η(P

1))) + log(1

ψS(η(P

0))),

(2.5)

−

This is analogous to the original GAN objective (2.1), but differs from it in a crucial aspect: here
the expectation is inside of ψS. In other words, whereas in (2.1) the loss of a sample is deﬁned as
the expected loss of the sample points, here the sample loss is a single value, based on the deep
mean embedding of its expected value. Therefore, (2.5) can be thought of an extension of (2.1) to
non-trivial sample sizes. Henceforth, we use MS := ψS ◦
η to denote the full-model (mean encoder
and classiﬁer), and we refer to it as the sample classiﬁer.

Two-sample Discriminator The sample classiﬁer proposed above has one potential drawback:
the nonlinearities of the discriminator function are separable across P
1, restricting the
interactions between distributions to be additive. To address this potential issue, we propose
to shift from a classiﬁcation to a discrepancy objective, that is, given two samples drawn inde-
pendently, the two-sample discriminator must predict whether they were drawn from the same
or different distributions. Concretely, given two encoded representations η(P
1), the
two-sample discriminator ψ2S uses the absolute difference of their deep mean encodings, and
outputs ψ2S(
[0, 1], reﬂecting its conﬁdence on whether the two samples were
indeed drawn from the same distribution. Again with a binary classiﬁcation criterion, the loss of
this prediction would be given by

0) and η(P

0 and P

η(P

η(P

1)

0)

−

∈

)

|

|

d2S(P

0, P

1) =

P

P

0 = P
1
= P
1

(cid:75)

log(ψ2S(
(1

η(P
|
log(ψ2S(

0)
−
η(P

η(P
0)

−

1)
|
η(P

))+
1)

))

|

−

(cid:75)
to denote the full two-sample discriminator model.

|

(2.6)

0 (cid:54)

(cid:74)

(cid:74)
)

·

As before, we use M2S := ψ2S ◦ |

η(

η(

)

·

|

−

3 Distributional Adversarial Network

Now, we use the above distributional adversaries in a new training framework which we call
Distributional Adversarial Network (DAN). This framework can easily be combined with existing
adversarial training algorithms by a simple modiﬁcation in their adversaries. In this section, we

4

examine in detail an example application of DAN to the generative adversarial setting. In the
experiments section we provide an additional application to adversarial domain adaptation.

To integrate a GAN into the DAN framework, we simply add the distributional adversary:

min
G

max
D,Mξ

V(G, D, Mξ ) =

λ1

E

x

Px,z

Pz [log D(x) + log(1

D(G(z)))] + λ2dξ (Px, P

G),

(3.1)

∼

∼

−

where ξ

S, 2S

indicates whether we use the sample classiﬁer or two-sample discriminator.

∈ {

}

A Note on Gradient Weight Sharing In the case of the sample classiﬁer (ξ = S) and with only
the distributional adversary (λ1 = 0 and λ2 > 0 above), a derivation as in Section 2.1 shows that
the gradient of a sample of B points generated by G is

∇θG loss(Z) =

1
ψS(ηB) ∇

ψS(ηB)

(cid:32)

1
B

B
∑
i=1 ∇

ψS(G(z(i)))

G(z(i))

(3.2)

(cid:21)(cid:33)

(cid:20) d
dθ

}

{

G(z(1)), . . . , G(x(B))

) for ease of notation. Note that, as opposed to (2.2),
where we use ηB := η(
the gradient for each z(i) is weighted by the same left-most discriminator conﬁdence term. This
has the effect of sharing information across samples when updating gradients: whether a sample
(encoded as a vector ηB) can fool the discriminator or not will have an effect on every sample
point’s gradient. In addition, this helps prevent the vanishing gradient for points generated in
regions of mode collapse, since other sample points in the batch will come, with high probability,
ψS(ηB), will
from regions of high density, and thus the weight vector that this sample point gets,
be less likely to be zero than in the single-sample-point discriminator case. Several interesting
observations arise from this analysis. First, (3.2) suggests that the true power of this sample-based
setting lies in choosing a discriminator ψξ that, through non-linearities, enforces interaction between
the points in the sample. Second, the notion of sharing information across examples occurs also
in batch normalization (BN) [13], although the mechanism to achieve this and the underlying
motivation for doing it are different. This connection, which might provide additional theoretical
explanation for BN’s empirical success, is supported by our experimental results, in which we
show that the improvements brought by our framework are much more pronounced when BN
is not used. While the analysis here is certainly not a rigorous one, the intuitive justiﬁcation for
sample-based aggregation is clear, and is conﬁrmed by our experimental results.

∇

Training During training, all expectations of over data/noise distributions will be approximated
via ﬁnite-sample averages. In each training iteration, we use minibatches (i.e. samples) from the
data/noise distributions. We train by alternating between updates of the generator and adversaries.
While for DAN-S the training procedure is similar to that of GAN, DAN-2S requires a modiﬁed
training scheme. Due to the form of the two-sample discriminator, we want a balanced exposure to
pairs of samples drawn from the same and different distributions. Thus, every time we update
Pz from data and
M2S, we draw minibatches X =
x(i)
noise distributions. We then split each sample into two parts, X1 :=
,

2 +1
Z1 :=
and use the discriminator M2S to predict on each pair of
(X1, G(Z2)), (G(Z1), X2), (X1, X2) and (G(Z1), G(Z2)) with target outputs 0, 0, 1 and 1, respectively.
A visualization of this procedure is shown in Figure 2 and the detailed training procedure is
described in Appendix A.

z(1), . . . , z(B)
x(i)

} ∼
B
i=1, X2 =
2

Px and Z =

x(1), . . . , x(B)

i=1, Z2 =

B
i= B

B
i= B

} ∼

z(i)

z(i)

2 +1

{

}

}

{

{

{

}

}

{

{

B
2

5

Figure 2: DAN-S and DAN-2S models and corresponding losses, where X =

Z =

z(i)

B
i=1 ∼

}

{

Pz, X1 :=

x(i)

{

B
2

}

i=1, X2 =

x(i)

B
i= B

}

2 +1

{

, Z1 :=

z(i)

{

B
2

}

i=1 and Z2 =

Px,

B
i=1 ∼

x(i)
}
{
z(i)
B
i= B

}

{

2 +1

3.1 Related Work

Discrepancy Measures The distributional adversary bears close resemblance to two-sample
tests [15], where the model takes two samples drawn from potentially distinct distributions as
input and produces a discrepancy value quantifying how different two distributions are. A popular
kernel-based variant is the Maximum Mean Discrepancy (MMD) [8, 11, 24]:

MMD2(U, V) =

1

n ∑n

i=1 φ(ui)

1

m ∑m

j=1 φ(vj)

2
2

(cid:107)

−

(cid:107)

where φ(
) is some feature mapping, and k(u, v) = φ(u)(cid:62)φ(v) is the corresponding kernel function.
An identity function for φ corresponds to computing the distance of the sample means. More
complex kernels result in distances of higher-order statistics between two samples.

·

In adversarial models, MMD and its variants (e.g., central moment discrepancy (CMD)) have
been used either as a direct objective for the target model or as a form of adversary [7, 16, 25, 32, 32].
However, such discrepancy measure requires hand-picking the kernel. An adaptive feature function,
in contrast, may be able to adapt to the given distributions and thereby discriminate better.

Our distributional adversary tackles the above drawbacks and, at the same time, generalizes
many discrepancy measures given that a neural network is a universal approximator. Since it is
trainable, it can evolve as training proceeds. Speciﬁcally, it can be of simpler form when target

6

model is weak at the beginning of training, and becomes more complex as target model becomes
stronger.

Minibatch Discrimination Another relevant line of work involves training generative models by
looking at statistics at minibatch level, known as minibatch discrimination, initially developed in
[23] to stabilize GAN training. Batch normalization [13] can be seen as performing some form of
minibatch discrimination and it has been shown helpful for GAN training [20]. Zhao et al. [34]
proposed a repelling regularizer that operates on a minibatch and orthogonalizes the pairwise
sample representation, keeping the model from concentrating on only a few modes.

Our distributional adversaries can be seen as learning a minibatch discriminator that adapts as
the target model gets stronger. It not only enjoys the beneﬁts of minibatch discrimination, leading
to better mode coverage and stabler training (as shall be seem in Section 4), but also eliminates the
need of hand-crafting various forms of minibatch discrimination objectives.

Permutation Invariant Networks The deep mean encoder takes an (unordered) sample to pro-
duce a mean encoding. A network whose output is invariant to the order of inputs is called
permutation invariant network. Formally, let f be the network mapping, we have f (x1, x2, . . . , xn) =
f (xσ(1), xσ(2), . . . , xσ(n)), where n could be of varying sizes and σ(

) is any permutation over [n].

The problem of dealing with set inputs is being studied very recently. Vinyals et al. [28] consider
dealing with unordered variable-length inputs with content attention mechanism. In later work
of [21, 31], the authors consider the way of ﬁrst embedding all samples into a ﬁxed-dimensional
latent space, and then adding them up to form a vector of the same dimension, which is further
fed into another neural network. In [17], the authors proposed a similar network for embedding a
set of images into a latent space. They use a weighted summation over latent vectors, where the
weights are learned through another network. The structure of our deep mean encoder resembles
these networks in that it is permutation-invariant, but differs in its motivation—mean discrepancy
measures—as well as its usage within discriminators in adversarial training settings.

·

Other Related Work Extensive work has been devoted to resolving the instability and mode
collapse problems in GANs. One common approach is to train with more complex network
architecture or better-behaving objectives [3, 4, 12, 20, 33, 34]. Another common approach is to add
more discriminators or generators [6, 26] in the hope that training signals from multiple sources
could lead to more stable training and better coverage of modes.

4 Empirical Results

3

We demonstrate the effectiveness of DAN training by applying it to generative models and do-
main adaptation.
In generative models, we observe remarkably better mode recovery than
non-distributional models on synthetic and real datasets, through both qualitative and quantitative
evaluation of generated sample diversity. In domain adaptation we leverage distributional adver-
saries to align latent spaces of source and target domains, yielding considerable improvements
over state-of-the-art.

4.1 Synthetic Data: Mode Recovery in Multimodal Distributions

We ﬁrst apply DAN to generative models, where the data is a mixture of 8 two-dimensional
Gaussian distributions, with means aligned on a circle (bottom right of Figure 3). The goal is for

3

Please refer to the appendix for all details on dataset, network architecture and training procedures.

7

Figure 3: Results for mode recovery on data generated from 8 Gaussian mixtures. The rightmost
distribution is the true data distribution. While with GAN training the generator is only able to
capture 1 of 8 modes, in both DAN-S and DAN-2S training we are able to recover all 8 modes.

the generator to recover 8 modes on the circle. A similar task has been considered as a proof of
concept for mode recovery in [4, 19]. All architectures are simple feed-forward networks with ReLU
activations. To test the power of mode-recovery for DAN, we set λ1 = 0 and λ2 = 1, so only the
distributional adversary is included in training. The results are shown in Figure 3.

While the GAN generator produces samples oscillating between different modes, our distri-
butional methods consistently and stably recover all eight modes of the true data distribution.
Among these two, DAN-2S takes slightly longer to converge. We observe this multi-modal seeking
behavior from early in the process, where the generator ﬁrst distributes mass broadly covering all
modes, and then subsequently sharpens the generated distribution in later iterations.

Note that here we set the architecture of DAN to be the same as GAN, with the only difference
that the DAN adversary takes one extra step of averaging latent representations in the middle,
which does not increase the number of parameters. Thus, DAN achieves a signiﬁcantly better
mode recovery but with the same number of network parameters.

4.2 MNIST: Recovering Mode Frequencies

Mode recovery entails not only capturing a mode, i.e., generating samples that lie in the correspond-
ing mode, but also recovering the true probability mass of the mode. Next, we evaluate our model
on this criterion. To do so, we train DAN on MNIST, which has a 10-class balanced distribution over
digits, and compare the frequencies of generated classes against this target uniform distribution.
Since the generated samples are unlabeled, we train an external classiﬁer on MNIST to label the
generated data.

Besides the original GAN, we also compared to two recently proposed generative models:
RegGAN [4] and EBGAN [34]. To keep the approaches comparable, we use a similar neural
network architecture in all cases, which we did not tailor to any particular model. We trained
models without Batch Normalization (BN), except for RegGAN and EBGAN, which we observed
to consistently beneﬁt from it. We found that in tasks beyond simple data generation in low-
dimensional spaces, it is beneﬁcial to set λ1, λ2 > 0 for DAN. In this case, the generator will receive
training signals from generated samples both locally and globally, since both local (single-sample)
and distributional adversaries are used. Throughout the experiment we set λ1 = 1 and λ2 = 0.2.
We show the results in Figure 4 and 5. Training with the original GAN leads to generators
that place too much mass on some modes and ignore some others. While RegGAN and EBGAN

8

Figure 4: Class distribution of samples generated by various models on MNIST. Note that we
showcase the best one out of 10 random runs for GAN, RegGAN and EBGAN in terms of distribution
entropy to give them an unfair advantage. For DAN we simply showcase a random run (the
variance of the performance in FIgure 5 indicates that DAN-ξ is stable across all runs). The best
runs for RegGAN and EBGN also recover mode frequencies to some extent, but the performance
varies a lot across different runs as in Figure 5.

Figure 5: Performances of mode fre-
quency recovery under 2 different mea-
sures: entropy of generated mode distri-
bution and total variation distances be-
tween generated mode distribution and
uniform one. DAN achieves the best and
most stable mode frequency recovery.

sometimes generate more uniform distributions (such as the best-of-ten shown in Figure 4), this
varies signiﬁcantly across repetitions (c.f. Figure 5), an indication of instability. DAN, on the other
hand, consistently recovers the true frequencies across repetitions.

4.3

Image Generation: Sample Diversity as Evidence of Mode Coverage

We also test DAN on a harder image generation problem: generating faces. We train DCGAN
and DAN with λ1 = 1, λ2 = 0.2 on the unlabeled CelebA [18] dataset and compare the generated
samples. We use DCGAN with BN, and DAN-S/2S without BN. Figure 6 visualizes half a minibatch
(32 samples).

As discussed in Section 3.1, both BN and DAN can be seen as techniques applying some form
of normalization across a sample (minibatch). As seen in Figure 6, both DCGAN with BN and
DAN-ξ promotes diversity in generated samples as compared to DCGAN without BN. Such diverse
samples indicate a better coverage of modes in the original dataset.

4.4 Domain Adaptation

For the last set of experiments, we test our DAN framework in the context of unsupervised domain
adaptation, where the target domain data is fully unlabeled. This problem has been recently
approached from the adversarial training perspective by using a domain classiﬁer adversary to
enforce domain-invariant representations [9, 27]. Following this approach, we use two neural
network encoders (one for each domain) and a shared classiﬁer. Enforcing the outputs of these
encoders to be indistinguishable allows for a classiﬁer trained on the source domain to be used in
combination with the target encoder. For the adversary we use either DAN-S or DAN-2S to discern

9

(a) DCGAN, without BN

(b) DCGAN, with BN

(c) DAN-S, without BN (d) DAN-2S, without BN

Figure 6: GAN, DAN-S and DAN-2S trained on CelebA dataset to generate faces.

between samples of encoded representations from the two domains.

We ﬁrst compare algorithms on the Amazon reviews dataset preprocessed by Chen et al. [5]. It
consists of four domains: books, dvd, electronics and kitchen appliances, each of which contains
reviews encoded in 5, 000 dimensional feature vectors and binary labels indicating if the review
is positive or negative. We compare against the domain-adversarial neural network (DANN) of
Ganin et al. [9], which uses a similar framework but a single-sample-point discriminator. To make
the comparison meaningful, we use the same architectures and parameters on all models, with
the only difference being the representation averaging layer of our sample-based adversaries. We
did not extensively tune parameters in any model. The results are shown in Table 1. Our models
consistently outperform GAN-based DANN on most source-target pairs, with a considerable
average improvement in accuracy of 1.41% for DAN-S and 0.92% for DAN-2S.

Lastly, we test the effectiveness of our proposed framework on a different domain adaptation
task, this time for image label prediction. Speciﬁcally, we tackle the task of adapting from MNIST to
MNIST-M, which is obtained by blending digits over patches randomly extracted from color photos
from BSDS500 [1] as in [9]. The results are shown in Table 2. DAN-2S yields a strong improvement
3%. Again, the DAN-S and DAN-2S architectures consist of a simple plug-
over DANN by
and-play adaptation of DANN to our distributional framework with little modiﬁcations, which
demonstrates the ease of use of the proposed DAN criteria on top of existing models.

∼

5 Discussion and Future Work

The distributional adversarial framework we propose here is another step in an emerging trend
of shifting from traditional classiﬁcation losses to richer criteria better suited to ﬁnite-sample
distribution discrimination. The experimental results obtained with this new approach offer a
promising glimpse of the advantages of genuine sample-based discriminators over sample point
alternatives, while the simplicity and ease of implementation makes this approach an appealing
plug-in addition to existing models.

Our framework is fairly general and opens the door to various possible extensions. The two

10

Source

Target

dvd
electronics
kitchen
books
electronics
kitchen
books

books
books
books
dvd
dvd
dvd
electronics
electronics dvd
electronics
kitchen
kitchen
kitchen

kitchen
books
dvd
electronics

avg. imp.

DANN
77.14(1.42)
74.38(1.12)
77.23(1.09)
75.01(0.83)
75.62(0.85)
79.41(0.96)
70.34(1.01)
69.42(2.60)
83.64(0.48)
69.54(0.88)
69.36(2.24)
82.68(0.47)
0.00

DAN-S
77.79(0.51)
75.68(0.42)
79.01(0.41)
75.38(0.88)
75.98(0.91)
80.37(0.91)
72.41(0.77)
72.39(1.99)
85.02(0.37)
70.39(0.85)
73.85(0.82)
82.47(0.16)

1.41

DAN-2S
78.22(0.78)
74.78(1.08)
76.90(0.58)
74.36(2.16)
76.28(1.37)
79.96(0.89)
72.02(1.92)
72.34(1.65)
84.66(0.68)
70.45(1.41)
71.50(2.93)
83.38(0.28)
0.92

Table 1: Domain adaptation results on the Amazon review dataset. The numbers correspond to
mean accuracies (plus one standard deviation) over 5 runs.

Source Only Target Only
95.60(0.16)
52.20(1.94)

DANN
76.13(1.63)

DAN-S
77.07(1.94)

DAN-2S

79.214(2.19)

Table 2: Results of domain adaptation from MNIST to MNIST-M, averaged over 5 different runs.

types of discriminators proposed here are by no means the only options. There are many other
approaches in the distributional discrepancy literature to draw inspiration from. One aspect that
warrants additional investigation is the effect of sample size on training stability and mode coverage.
It is sensible to expect that in order to maintain global discrimination power in settings with highly
multimodal distributions, the size of samples fed to the discriminators should grow, at least with
the number of modes. Formalizing this relationship is an interesting avenue for future work.

References

[1] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image

segmentation. IEEE Trans. on Pattern Analysis and Machine Intelligence, 2011.

[2] M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial
networks. In NIPS 2016 Workshop on Adversarial Training. In review for ICLR, volume 2016, 2017.

[3] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.

[4] T. Che, Y. Li, A. P. Jacob, Y. Bengio, and W. Li. Mode regularized generative adversarial

networks. International Conference on Learning Representations, 2017.

[5] M. Chen, Z. Xu, K. Weinberger, and F. Sha. Marginalized denoising autoencoders for domain

adaptation. IEEE Trans. on Pattern Analysis and Machine Intelligence, 2012.

11

[6] I. Durugkar, I. Gemp, and S. Mahadevan. Generative multi-adversarial networks. International

Conference on Learning Representations, 2017.

[7] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training generative neural networks via
maximum mean discrepancy optimization. In Uncertainty in Artiﬁcial Intelligence (UAI), 2015.

[8] K. Fukumizu, A. Gretton, X. Sun, and B. Sch ¨olkopf. Kernel measures of conditional depen-

dence. In Advances in neural information processing systems, pages 489–496, 2008.

[9] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and
V. Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning
Research, 2016.

[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems,
pages 2672–2680, 2014.

[11] A. Gretton, O. Bousquet, A. Smola, and B. Sch ¨olkopf. Measuring statistical dependence with
hilbert-schmidt norms. In International conference on algorithmic learning theory, pages 63–77.
Springer, 2005.

[12] X. Huang, Y. Li, O. Poursaeed, J. Hopcroft, and S. Belongie. Stacked generative adversarial

networks. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

[13] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

[14] D. Kingma and J. Ba. Adam: A method for stochastic optimization.

arXiv preprint

arXiv:1412.6980, 2014.

Media, 2006.

Machine Learning (ICML), 2015.

arXiv:1704.03373, 2017.

[15] E. L. Lehmann and J. P. Romano. Testing statistical hypotheses. Springer Science & Business

[16] Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In Int. Conference on

[17] Y. Liu, J. Yan, and W. Ouyang. Quality aware network for set to set recognition. arXiv preprint

[18] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild.

In Int.

Conference on Computer Vision (ICCV), 2015.

[19] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks.

International Conference on Learning Representations, 2017.

[20] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep

convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

[21] S. Ravanbakhsh, J. Schneider, and B. Poczos. Deep learning with sets and point clouds. In

International Conference on Learning Representations, Workshop Track, 2017.

[22] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial
text to image synthesis. In Proceedings of The 33rd International Conference on Machine Learning,
volume 3, 2016.

[23] T. Salimans, I. Good fellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved

techniques for training gans. arXiv preprint arXiv:1606.03498, 2016.

12

[24] A. Smola, A. Gretton, L. Song, and B. Sch ¨olkopf. A hilbert space embedding for distributions.

In International Conference on Algorithmic Learning Theory, pages 13–31. Springer, 2007.

[25] D. J. Sutherland, H.-Y. Tung, H. Strathmann, S. De, A. Ramdas, A. Smola, and A. Gretton.
Generative models and model criticism via optimized maximum mean discrepancy.
In
International Conference on Learning Representations, 2017.

[26] I. Tolstikhin, S. Gelly, O. Bousquet, C.-J. Simon-Gabriel, and B. Sch ¨olkopf. Adagan: Boosting

generative models. arXiv preprint arXiv:1701.02386, 2017.

[27] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation.

arXiv preprint arXiv:1702.05464, 2017.

[28] O. Vinyals, S. Bengio, and M. Kudlur. Order matters: Sequence to sequence for sets. Interna-

tional Conference on Learning Representations, 2016.

[29] C. Vondrick, H. Pirsiavash, and A. Torralba. Generating videos with scene dynamics. In

Advances In Neural Information Processing Systems, pages 613–621, 2016.

[30] J. Wu, C. Zhang, T. Xue, W. T. Freeman, and J. B. Tenenbaum. Learning a probabilistic
latent space of object shapes via 3d generative-adversarial modeling. In Advances in Neural
Information Processing Systems, pages 82–90, 2016.

[31] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. Salakhutdinov, and A. Smola. Deep sets.

arXiv preprint arXiv:1703.06114, 2016.

[32] W. Zellinger, T. Grubinger, E. Lughofer, T. Natschl¨ager, and S. Saminger-Platz. Central moment
discrepancy (CMD) for domain-invariant representation learning. In International Conference
on Learning Representations, 2017.

[33] H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas. Stackgan: Text to
photo-realistic image synthesis with stacked generative adversarial networks. arXiv preprint
arXiv:1612.03242, 2016.

[34] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. 2017.

13

A Training Algorithm for DAN

We show the whole training procedure for DAN in Algorithm 1.

B 2D Gaussian Mixture Experiment Details

Following [19], we generate a mixture of 8 Gaussian distribution with mean equally spaced on a
circle of radius 2 and variances of 0.01.

The generator consists of a fully connected network with 3 hidden layers of size 128 with ReLU
activations, followed by a linear projection to 2 dimensions. The discriminator consists of a fully
connected network with 3 hidden layers of size 32 with ReLU activations, followed by a linear
projection to 1 dimension. Latent vectors are sampled uniformly from [
1, 1]256. Distributional
adversaries starts with 2 hidden layers of size 32, then the latent representations are averaged
across the batch. The mean representation is then fed to 1 hidden layer of size 32 and a linear
projection to 1 dimension. For DAN we set λ1 = 0 and λ2 = 1.

−

Note that the number of parameters in GAN is the same as that in DAN.
We use Adam [14] with learning rate of 1E-4 and β1 = 0.5 as training algorithm. Minibatch
size is ﬁxed to 512 and we train the network for 25000 iterations by alternating between updates of
generator and adversaries.

C MNIST Experiment Details

We use the same architecture for generator across all models, which consists of 3 hidden layers
with 256, 512 and 1,024 neurons each and with ReLU activations, followed by a fully connected
linear layer to map it to 784 dimensional vector and ﬁnally a sigmoid activation for output. Latent
1, 1]100. Decoder in EBGAN has the same architecture and
vectors are uniform samples from [
encoders in RegGAN and EBGAN are the reverse of the decoder. For GAN, RegGAN and DAN
the discriminator is the reverse of the generator except that the ﬁnal linear function maps the vector
of dimension 256 to 1. Distributional adversary in DAN has the same structure as the adversary in
GAN but with an averaging of vectors across the batch before it is mapped to vector of dimension
256. And ﬁnally for RegGAN we set the two hyperparameters (λ1 and λ2 in the original paper [4])
to be 0.2 and 0.4, and for EBGAN we set the margin (m in original paper [34]) to be 5.

−

(a) GAN

(b) DAN-S

(c) DAN-2S

Figure 7: GAN, DAN-S and DAN-2S trained on MNIST dataset to generate digits. DAN-S and
DAN-2S clearly demonstrate better mode coverage and frequency recovery compared to GAN.

14

Algorithm 1 Training Procedure for DAN-S/2S.

Input:total number of iterations T, size of minibatch B, step number k, model mode ξ
for i = 1 to T do

S, 2S

}

∈ {

Sample minibatch X =
Update discriminator by optimizing one step:

x(1), . . . , x(B)

} ∼

{

Px, Z =

{

z(1), . . . , z(B)

Pz

} ∼

λ1
B

(cid:104)

B
∑
i=1

log(D(x(i))) + log(1

D(G(z(i))))

(cid:105)

−

if mod(i,k)=0 then

Sample minibatch X and Z
if ξ = S then

Update MS by optimizing one step:

λ2(log(MS(η(X))) + log(1

MS(η(G(Z)))))

−

else if ξ = 2S then

Divide X and Z into X1, X2 and Z1, Z2
Update M2S by optimizing one step:

λ2
2 {

))+

|

η(X2)

log(M2S(

η(X1)

|

log(M2S(
(1
(1

|
log(M2S(
log(M2S(

−
η(G(Z1))
−
η(X1)
−
η(G(Z1))

−
−

|
|

η(G(Z2))
|
η(G(Z2))
η(X2)

))+
))+
))

|
|

}

−

end if

end if
Sample minibatch X, Z
if ξ = S then

Update G by optimizing one step:

λ1
B

B
∑
i=1

−

log(1

D(G(z(i)))) + λ2 log(1

MS(η(G(Z)))))

−

else if ξ = 2S then

Divide X and Z into X1, X2 and Z1, Z2
Update G by optimizing one step:

λ1
B

B
∑
i=1
λ2(1
(1

−

log(1

D(G(z(i))))+

−

log(M2S(

−
log(M2S(

|

η(X1)
−
η(G(Z1))
−

η(G(Z2))
|
))
η(X2)
}

|

|

))+

end if

end for

15

We use Adam with learning rate of 5E-4 and β1 = 0.5 for training. Minibatch size is ﬁxed to
256 and we train the network for 50 epochs. For DAN we update the distributional adversaries
every 5 iterations. For all other models we train by alternating between generator and adversaries
(and potentially encoders and decoders).

We showcase the generated digits by GAN, DAN-S and DAN-2S in Figure 7, where we clearly

see a better mode coverage of DAN-S and DAN-2S over GAN.

D CelebA Experiment Details

4

−

We use public available code
for DCGAN. We set the network architecture the same as default
in the code. The generator consists of a fully connected linear layer mapping from latent space
1, 1]100 to dimension 8,192, followed by 4 layers of deconvolution layers with ReLU activation
of [
except the last layer which is followed by tanh. The discriminator is the “reverse” of generator
except that the activation function is Leaky ReLU and for the last layer, which is a linear mapping
to 1 dimension and a sigmoid activation. DAN has the same architecture as DCGAN, except for
the distributional adversary where it has one more layer of linear mapping with ReLU with 1,024
unites before the last linear mapping. We set λ1 = 1 and λ2 = 0.2.

All images are cropped to be 64

64. We use Adam with learning rate of 2E-4 and β1 = 0.5 for
training. Batch size is ﬁxed to 64 and we train the network for 20 epochs. We update the network
by alternating between generator and adversaries.

×

More samples from different models are shown in Figure 8.

E Domain Adaptation Experiment Details

5

We adapt public available code
for DANN to train on Amazon and MNIST-M datasets. For the
former dataset, we let the encoder for source/target domains consists of 3 fully connected linear
layer mapping with ReLU activation with numbers of hidden units being 1000, 500, 100 and is then
mapped to 2-dimensional vector for both adversary and classiﬁer. For the latter dataset we set the
parameters to be the default values in the code. We set λ1 = λ2 = 0.1 and λ1 = λ2 = 1 for the two
datasets respectively.

4https://github.com/carpedm20/DCGAN-tensorflow
5https://github.com/pumpikano/tf-dann

16

(a) DCGAN, trained without BN

(b) DCGAN, trained with BN

(c) DAN-S, trained without BN

(d) DAN-2S, trained without BN

Figure 8: GAN, DAN-S and DAN-2S trained on CelebA dataset to generate faces. DAN-S and
DAN-2S demonstrate more diversity in generated samples compared to GAN trained without BN,
and generates more realistic samples compared to that trained with BN.

17

