U-Net: Machine Reading Comprehension with Unanswerable Questions

Fu Sun†, Linyang Li†, Xipeng Qiu†∗, Yang Liu‡
† Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
† School of Computer Science, Fudan University
‡ Liulishuo Silicon Valley AI Lab
{fsun17,lyli15,xpqiu}@fudan.edu.cn, yang.liu@liulishuo.com

8
1
0
2
 
t
c
O
 
2
1
 
 
]
L
C
.
s
c
[
 
 
1
v
8
3
6
6
0
.
0
1
8
1
:
v
i
X
r
a

Abstract

Machine reading comprehension with unanswerable ques-
tions is a new challenging task for natural language process-
ing. A key subtask is to reliably predict whether the ques-
tion is unanswerable. In this paper, we propose a uniﬁed
model, called U-Net, with three important components: an-
swer pointer, no-answer pointer, and answer veriﬁer. We in-
troduce a universal node and thus process the question and
its context passage as a single contiguous sequence of tokens.
The universal node encodes the fused information from both
the question and passage, and plays an important role to pre-
dict whether the question is answerable and also greatly im-
proves the conciseness of the U-Net. Different from the state-
of-art pipeline models, U-Net can be learned in an end-to-end
fashion. The experimental results on the SQuAD 2.0 dataset
show that U-Net can effectively predict the unanswerability
of questions and achieves an F1 score of 71.7 on SQuAD 2.0.

Introduction
Machine reading comprehension (MRC) is a challenging
task in natural language processing, which requires that ma-
chine can read, understand, and answer questions about a
text. Beneﬁting from the rapid development of deep learn-
ing techniques and large-scale benchmarks (Hermann et al.
2015; Hill et al. 2015; Rajpurkar et al. 2016), the end-to-end
neural methods have achieved promising results on MRC
task (Seo et al. 2016; Huang et al. 2017; Chen et al. 2017;
Clark and Gardner 2017; Hu et al. 2017). The best sys-
tems have even surpassed human performance on the Stan-
ford Question Answering Dataset (SQuAD) (Rajpurkar et al.
2016), one of the most widely used MRC benchmarks. How-
ever, one of the limitations of the SQuAD task is that each
question has a correct answer in the context passage, there-
fore most models just need to select the most relevant text
span as the answer, without necessarily checking whether it
is indeed the answer to the question.

To remedy the deﬁciency of SQuAD, Rajpurkar, Jia, and
Liang (2018) developed SQuAD 2.0 that combines SQuAD
with new unanswerable questions. Table 1 shows two ex-
amples of unanswerable questions. The new dataset requires
the MRC systems to know what they don’t know.

To do well on MRC with unanswerable questions, the
model needs to comprehend the question, reason among the

∗ Corresponding Author.

Article: Endangered Species Act
Paragraph: “... Other legislation followed, including
the Migratory Bird Conservation Act of 1929, a 1937
treaty prohibiting the hunting of right and gray whales,
and the Bald Eagle Protection Act of 1940. These later
laws had a low cost to societythe species were rela-
tively rareand little opposition was raised.
Question 1: Which laws faced signiﬁcant opposition?
Plausible Answer: later laws
Question 2: What was the name of the 1937 treaty?
Plausible Answer: Bald Eagle Protection Act

Table 1: Unanswerable Questions from SQUAD 2.0 (Ra-
jpurkar, Jia, and Liang 2018).

passage, judge the unanswerability and then identify the an-
swer span. Since extensive work has been done on how to
correctly predict the answer span when the question is an-
swerable (e.g., SQuAD 1.1), the main challenge of this task
lies in how to reliably determine whether a question is not
answerable from the passage.

There are two kinds of approaches to model the answer-
ability of a question. One approach is to directly extend
previous MRC models by introducing a no-answer score
to the score vector of the answer span (Levy et al. 2017;
Clark and Gardner 2017). But this kind of approaches is rela-
tively simple and cannot effectively model the answerability
of a question. Another approach introduces an answer veri-
ﬁer to determine whether the question is unanswerable (Hu
et al. 2018; Tan et al. 2018). However, this kind of ap-
proaches usually has a pipeline structure. The answer pointer
and answer veriﬁer have their respective models, which are
trained separately. Intuitively, it is unnecessary since the un-
derlying comprehension and reasoning of language for these
components is the same.

In this paper, we decompose the problem of MRC with
unanswerable questions into three sub-tasks: answer pointer,
no-answer pointer, and answer veriﬁer. Since these three
sub-tasks are highly related, we regard the MRC with unan-
swerable questions as a multi-task learning problem (Caru-
ana 1997) by sharing some meta-knowledge.

We propose the U-Net to incorporate these three sub-tasks
into a uniﬁed model: 1) an answer pointer to predict a can-

didate answer span for a question; 2) a no-answer pointer to
avoid selecting any text span when a question has no answer;
and 3) an answer veriﬁer to determine the probability of the
“unanswerability” of a question with candidate answer in-
formation. Additionally, we also introduce a universal node
and process the question and its context passage as a single
contiguous sequence of tokens, which greatly improves the
conciseness of U-Net. The universal node acts on both ques-
tion and passage to learn whether the question is answerable.
Different from the previous pipeline models, U-Net can be
learned in an end-to-end fashion. Our experimental results
on the SQuAD 2.0 dataset show that U-Net effectively pre-
dicts the unanswerability of questions and achieves an F1
score of 72.6.

The contributions of this paper can be summarized as fol-

lows.
• We decompose the problem of MRC with unanswerable
questions into three sub-tasks and combine them into a
uniﬁed model, which uses the shared encoding and inter-
action layers. Thus, the three-tasks can be trained simul-
taneously in an end-to-end fashion.

• We introduce a universal node to encode the common in-
formation of the question and passage. Thus, we can use a
uniﬁed representation to model the question and passage,
which makes our model more condensed.
• U-Net is very easy to implement yet effective.

Proposed Model
Formally, we can represent the MRC problem as: given a
set of tuples (Q, P, A), where Q = (q1, q2, · · · , qm) is the
question with m words, P = (p1, p2, · · · , pn) is the context
passage with n words, and A = prs:re is the answer with
rs and re indicating the start and end points, the task is to
estimate the conditional probability P (A|Q, P ).

The architecture of our proposed U-Net is illustrated in

Figure 1.

U-Net consists of four major blocks: Uniﬁed Encod-
ing, Multi-Level Attention, Final Fusion, and Prediction. As
shown in Figure 1, we ﬁrst combine the embedded repre-
sentation of the question and passage with a universal node
u and pass them through a BiLSTM to encode the whole
text. We then use the encoded representation to do the infor-
mation interaction. Then we use the encoded and interacted
representation to fuse the full representation and feed them
into the ﬁnal prediction layers to do the multi-task training.
We will describe our model in details in the following.

(A) Uniﬁed Encoding
Embedding Following the successful models on SQuAD
1.1, we ﬁrst embed both the question and the passage
with the following features. Glove embedding (Pennington,
Socher, and Manning 2014) and Elmo embedding (Peters et
al. 2018) are used as basic embeddings. Besides, we use POS
embedding, NER embedding, and a feature embedding that
includes the exact match, lower-case match, lemma match,
and a TF-IDF feature (Chen et al. 2017).

Now we get the question representation Q = qm

i=1 and
i=1, where each word is

the passage representation P = pn

Figure 1: Architecture of the U-Net.

represented as a d-dim embedding by combining the fea-
tures/embedding described above.

Universal Node We create a universal node u, which is a
key factor in our model and has several roles in predicting
the unanswerability of question Q.

We expect this node to learn universal information from
both passage and question. This universal node is added and
connects the passage and question at the phase of embed-
ding, and then goes along with the whole representation, so
it is a key factor in information representation. Since the uni-
versal node is in between and later shared between passage
and question, it has an abstract semantic meaning rather than
just a word embedding.

Also, the universal node is later shared in the attention
interaction mechanism and used in both the answer bound-
ary detection and classiﬁcation tasks, so this node carries
massive information and has several important roles in our
whole model construction.

The universal node u is ﬁrst represented by a d-dim
randomly-initialized vector. We concatenated question rep-
resentation, universal node representation, passage represen-
tation together as:

V = [Q, u, P ] = [q1, q2 . . . qm, u, p1, p2, · · · , pn],

(1)

V ∈ Rd×(m+n+1) is a joint representation of question and
passage.

Word-level Fusion Then we ﬁrst use two-layer bidi-
rectional LSTM (BiLSTM) (Hochreiter and Schmidhuber
1997) to fuse the joint representation of question, universal
node, and passage.

previous work FusionNet (Huang et al. 2017) to construct
their iterations on three levels.

Take the ﬁrst level as an example. We ﬁrst compute the

afﬁne matrix of H l

q and H l

p by

(cid:16)

(cid:17)T

S =

ReLU(W1H l
q)

ReLU(W2H l

p),

(7)

H l = BiLSTM(V ),
H h = BiLSTM(H l),

where S ∈ R(m+1)×(n+1); W1 and W2 are learnable param-
eters. Next, a bi-directional attention is used to compute the
interacted representation (cid:99)H l

q and (cid:99)H l
p.

(2)

(3)

where H l is the hidden states of the ﬁrst BiLSTM, repre-
senting the low-level semantic information, and H h is the
hidden states of the second BiLSTM, representing the high-
level semantic information.

Finally, we concatenate H l and H h together and pass
them through the third BiLSTM and obtain a full represen-
tation H f .

H f = BiLSTM([H l; H h]).

(4)

Thus, H = [H l; H h; H f ] represents the deep fusion in-
formation of the question and passage on word-level. When
a BiLSTM is applied to encode representations, it learns
the semantic information bi-directionally. Since the univer-
sal node u is between the question and passage, its hidden
states hm+1 can learn both question and passage informa-
tion. When the passage-question pair was encoded as a uni-
ﬁed representation and information ﬂows via the BiLSTM,
the universal node has an important role in information rep-
resentation.

(B) Multi-Level Attention
To fully fuse the semantic representation of the question and
passage, we use the attention mechanism (Bahdanau, Cho,
and Bengio 2014) to capture their interactions on different
levels.

We expected that we could simply use self-attention on
the encoded representation H for interaction between ques-
tion and passage, which contains both bi-attention (Seo et
al. 2016) and self-attention (Wang et al. 2017) of the ques-
tion and passage. But we found that it performed slightly
worse than the traditional bi-directional attention with the
universal node included. Therefore, we use a bi-directional
attention between the question and passage.

We ﬁrst divide H into two representations: attached pas-
sage Hq and attached question Hp, and let the universal node
representation hm+1 attached to both the passage and ques-
tion, i.e.,

(cid:99)H l

(cid:99)H l

q = H l
p = H l

p × softmax(S T),

q × softmax(S),

(8)

(9)

where softmax(·) is column-wise normalized function.

We use the same attention layer to model the interactions
for all the three levels, and get the ﬁnal fused representation
(cid:99)H l, (cid:99)H h, (cid:99)H f for the question and passage respectively.

Note that while dealing with the attention output of the
universal node, we added two outputs from passage-to-
question attention and question-to-passage attention. So af-
ter the interaction, the fused representation (cid:99)H l, (cid:99)H h, (cid:99)H f still
have the same length as the encoded representation H l, H h
and H f .

(C) Final Fusion
After the three-level attentive interaction, we generate the ﬁ-
nal fused information for the question and passage. We con-
catenate all the history information: we ﬁrst concatenate the
encoded representation H and the representation after atten-
tion (cid:98)H (again, we use H l, H h, H f , and (cid:99)H l, (cid:99)H h, (cid:99)H f to rep-
resent 3 different levels of representation for the two previ-
ous steps respectively).

Following the success of DenseNet (Huang, Liu, and
Weinberger 2016), we concatenate the input and output of
each layer as the input of the next layer.

First, we pass the concatenated representation H through

a BiLSTM to get H A.
(cid:16)

H A = BiLSTM

(cid:17)
[H l; H h; H f ; (cid:99)H l; (cid:99)H h; (cid:99)H f ]

,

(10)

where the representation H A is a fusion of information from
different levels.

Then we concatenate the original embedded representa-
tion V and H A for better representation of the fused infor-
mation of passage, universal node, and question.

A = [V ; H A].

(11)

Hq = [h1, h2, · · · , hm+1],
Hp = [hm+1, hm+2, · · · , hm+n+1],

(5)
(6)

Finally, we use a self-attention layer to get the attention
information within the fused information. The self-attention
layer is constructed the same way as (Vaswani et al. 2017):

Note hm+1 is shared by Hq and Hp. Here the universal node
works as a special information carrier, and both passage and
question can focus attention information on this node so that
the connection between them is closer than a traditional bi-
attention interaction.

Since both Hq = [H l

q ; H f
p ; H f
p ]
are concatenated by three-level representations, we followed

q ] and Hp = [H l

p; H h

q; H h

(cid:98)A = A × softmax(ATA),

(12)

where (cid:98)A is the representation after self-attention of the fused
information A. Next we concatenated representation H A
and (cid:98)A and pass them through another BiLSTM layer:

O = BiLSTM[H A; (cid:98)A].

(13)

Now O is the ﬁnal fused representation of all the infor-
mation. At this point, we divide O into two parts: OP , OQ,
representing the fused information of the question and pas-
sage respectively.

OP = [o1, o2, · · · , om],
OQ = [om+1, om+2, · · · , om+n+1],

(14)

(15)

Note for the ﬁnal representation, we attach the universal
node only in the passage representation OP . This is because
we need the universal node as a focus for the pointer when
the question is unanswerable. These will be fed into the next
decoder prediction layer.

(D) Prediction
The prediction layer receives fused information of passage
OP and question OQ, and tackles three prediction tasks: (1)
answer pointer, (2) no-answer pointer and (3) answer veri-
ﬁer.

First, we use a function shown below to summarize the
question information OQ into a ﬁxed-dim representation cq.

cq =

(cid:88)

i

q oQ
exp(W (cid:62)
i )
j exp(W (cid:62)oQ
j )

(cid:80)

oQ
i ,

where Wq is a learnable weight matrix and oQ
represents
i
the ith word in the question representation. Then we feed cq
into the answer pointer to ﬁnd boundaries of answers (Wang
and Jiang 2016), and the classiﬁcation layer to distinguish
whether the question is answerable.

(i) Answer Pointer We use this answer pointer to detect
the answer boundaries from the passage when the question
is answerable (i.e., the answer is a span in the passage). This
layer is a classic pointer net structure (Vinyals, Fortunato,
and Jaitly 2015). We use two trainable matrices Ws and
We to estimate the probability of the answer start and end
boundaries of the ith word in the passage, αi and βi.

αi ∝ exp(cqWsoP
βi ∝ exp(cqWeoP

i ),
i ),

(17)

(18)

Note here when the question is answerable, we do not
consider the universal node in answer boundary detection,
so we have i > 0 (i = 0 is the universal node in the pas-
sage representation). The loss function for the answerable
question pairs is:

LA = −(cid:0) log αa + log βb
where a and b are the ground-truth of the start and end
boundary of the answer.

(19)

(cid:1),

(ii) No-Answer Pointer Then we use the same pointer for
questions that are not answerable. Here the loss LN A is:

α0 and β0 correspond to the position of the universal node,
which is at the front of the passage representation Op. For
this scenario, the loss is calculated for the universal node.

Additionally, since there exits a plausible answer for each
unanswerable question in SQuAD 2.0, we introduce an aux-
iliary plausible answer pointer to predict the boundaries of
the plausible answers. The plausible answer pointer has the
same structure as the answer pointer, but with different pa-
rameters. Thus, the total loss function is:
(cid:1), (21)
LN A = −(cid:0) log α0 + log β0
(cid:1) − (cid:0) log α(cid:48)
where α(cid:48) and β(cid:48) are the output of the plausible answer
pointer; a∗ and b∗ are the start and end boundary of the unan-
swerable answer.

a∗ + log β(cid:48)
b∗

The no-answer pointer and plausible answer pointer are

removed at test phase.

(iii) Answer Veriﬁer We use the answer veriﬁer to distin-
guish whether the question is answerable.

Answer veriﬁer applies a weighted summary layer to
summarize the passage information into a ﬁxed-dim repre-
sentation cq (as shown in Eq.(16)).

And we use the weight matrix obtained from the answer

(16)

pointer to get two representations of the passage.

cs =

αi · oP
i

ce =

βi · oP
i

(cid:88)

i
(cid:88)

i

(22)

(23)

Then we use the universal node om+1 and concatenate it
with the summary of question and passage to make a ﬁxed
vector

F = [cq; om+1; cs; ce].

(24)

This ﬁxed F includes the representation cq representing
the question information, and cs and ce representing the
passage information. Since these representations are highly
summarized specially for classiﬁcation, we believe that this
passage-question pair contains information to distinguish
whether this question is answerable. In addition, we include
the universal node as a supplement. Since the universal node
is pointed at when the question is unanswerable and this
node itself already contains information collected from both
the passage and question during encoding and information
interaction, we believe that this node is important in distin-
guishing whether the question is answerable.

Finally, we pass this ﬁxed vector F through a linear layer
to obtain the prediction whether the question is answerable.
pc = σ(W T

(25)

f F )

where σ is a sigmoid function, Wf is a learnable weight ma-
trix.

Here we use the cross-entropy loss in training.

LAV = −

δ · log pc + (1 − δ) · (log (1 − pc))

(26)

(cid:17)
,

(cid:16)

LN A = −(cid:0) log α0 + log β0

(cid:1),

(20)

where δ ∈ {0, 1} indicates whether the question has an an-
swer in the passage.

Compared with other relatively complex structures devel-
opped for this MRC task, our U-Net model passes the orig-
inal question and passage pair through embedding and en-
coding, which then interacts with each other, yielding fused
information merged from all the levels. The entire architec-
ture is very easy to construct. After we have the fused repre-
sentation of the question and passage, we pass them through
the pointer layer and a fused information classiﬁcation layer
in a multi-task setup.

Training
We jointly train the three tasks by combining the three loss
functions. The ﬁnal loss function is:

L = δLA + (1 − δ)LN A + LAV ,

(27)

where δ ∈ {0, 1} indicates whether the question has an an-
swer in the passage, LA, LN A and LAV are the three loss
functions of the answer pointer, no-answer pointer, and an-
swer veriﬁer.

Although the three tasks could have different weights in
the ﬁnal loss function and be further ﬁne-tuned after joint
training, here we just consider them in the same weight and
do not ﬁne-tune them individually.

At the test phase, we ﬁrst use the answer pointer to ﬁnd
a potential answer to the question, while the veriﬁer layer
judges whether the question is answerable. If the classiﬁer
predicts the question is unanswerable, we consider the an-
swer extracted by the answer pointer as plausible. In this
way, we get the system result.

Experiment

Datasets
Recently, machine reading comprehension and question an-
swering have progressed rapidly, owing to the computation
ability and publicly available high-quality datasets such as
SQuAD. Now new research efforts have been devoted to
the newly released answer extraction test with unanswerable
questions, SQuAD 2.0 (Rajpurkar, Jia, and Liang 2018). It
is constructed by combining question-answer pairs selected
from SQuAD 1.0 and newly crafted unanswerable questions.
These unanswerable questions are created by workers that
were asked to pose questions that cannot be answered based
on the paragraph alone but are similar to the answerable
questions. It is very difﬁcult to distinguish these questions
from the answerable ones. We evaluate our model using
this data set. It contains over 100,000+ questions on 500+
wikipedia articles.

Implementation Details
We use Spacy to process each question and passage to obtain
tokens, POS tags, NER tags and lemmas tags of each text.
We use 12 dimensions to embed POS tags, 8 for NER tags
(Chen et al. 2017). We use 3 binary features: exact match,
lower-case match and lemma match between the question
and passage (Lee et al. 2016). We use 100-dim Glove pre-
trained word embeddings and 1024-dim Elmo embeddings.
All the LSTM blocks are bi-directional with one single layer.

We set the hidden layer dimension as 125, attention layer di-
mension as 250. We added a dropout layer over all the mod-
eling layers, including the embedding layer, at a dropout rate
of 0.3 (Srivastava et al. 2014). We use Adam optimizer with
a learning rate of 0.002 (Kingma and Ba 2014).

During training, we omit passage with over 400 words
and question with more than 50 words. For testing, when
the passage has over 600 words and the question is over 100
words, we simply label these questions as unanswerable.

Main Results
Our model achieves an F1 score of 74.0 and an EM score
of 70.3 on the development set, and an F1 score of 72.6
and an EM score of 69.2 on Test set1, as shown in Table
2. Our model outperforms most of the previous approaches.
Comparing to the best-performing systems, our model has
a simple architecture and is an end-to-end model. In fact,
among all the end-to-end models, we achieve the best F1
scores. We believe that the performance of the U-Net can
be boosted with an additional post-processing step to verify
answers using approaches such as (Hu et al. 2018).

Ablation Study
We also do an ablation study on the SQuAD 2.0 develop-
ment set to further test the effectiveness of different com-
ponents in our model. In Table 3, we show four different
conﬁgurations.

First, we remove the universal node U . We let the nega-
tive examples focus on the plausible answer spans instead
of focusing on the universal node U . This results in a loss
of 2.6% F1 score on the development set, showing that the
universal node U indeed learns information about whether
the question is answerable.

We also tried to make the universal node U only attached
to the passage representation when passing the attention
layer. Our results showed that when node U is shared, as
it is called ‘universal’, it learns information interaction be-
tween the question and passage, and when it is not shared,
the performance slightly degraded.

As for the approaches to encode the representations, we
pass both the question and passage through a shared BiL-
STM. To test the effectiveness of this, we ran the experiment
using separate BiLSTMs on embedded question and passage
representations. Results show that the performance dropped
slightly, suggesting sharing BiLSTM is an effective method
to improve the quality of the encoder.

After removing the plausible answer pointer, the perfor-
mance also dropped, indicating the plausible answers are
useful to improve the model even though they are incorrect.
the performance

After removing the answer veriﬁer,

dropped greatly, indicating it is vital for our model.

Lastly, we run a test using a more concise conﬁguration.
In the second block (multi-level attention) of the U-Net, we
do not split the output of the encoded presentation and let it
pass through a self-attention layer. The bidirectional atten-
tion is removed. In this way, our model uses only one uniﬁed

1https://rajpurkar.github.io/

SQuAD-explorer/

Model

End-to-end Model
BNA(cid:63) (Rajpurkar, Jia, and Liang 2018)
DocQA (Rajpurkar, Jia, and Liang 2018)
FusionNet++ (Huang et al. 2017)
SAN (Liu et al. 2017)
VS3-Net
U-Net

Ensemble Model
FusionNet++ (ensemble)
SAN (ensemble)
U-Net (ensemble)

Pipeline Model
RMR+ELMo+Veriﬁer (Hu et al. 2018)

Human

Dev

Test

EM

F1

EM

F1

59.8
65.1
-
-
-
70.3

-
-
-

62.6
67.6
-
-
-
74.0

-
-
-

72.3

86.3

74.8

89.0

59.2
63.4
66.6
68.6
68.4
69.2

70.3
71.3
71.5

71.7

86.9

62.1
66.3
69.6
71.4
71.3
72.6

72.6
73.7
75.0

74.2

89.5

Table 2: Evaluation results on the SQuAD 2.0 (extracted on Sep 9, 2018). (cid:63) means the BiDAF (Seo et al. 2016) with No Answer.

Conﬁguration

EM F1 ∆EM ∆ F1

U-Net

no node U
no share U
no concatenate P & Q
no plausible answer pointer
no classiﬁcation

Self-Attn Only

70.3

67.9
69.7
69.0
69.6
63.5

69.7

74.0

71.4
73.5
72.8
72.9
68.5

73.5

-

-2.4
-0.6
-1.3
-0.7
-6.8

-0.5

-

-2.6
-0.5
-1.2
-1.1
-5.5

-0.5

Table 3: Comparison of different conﬁgurations for our U-
Net model.

representation of the question and passage at all time. We
simply pass this representation layer by layer to get the ﬁ-
nal result. Compared to the bi-attention model, the F1-score
decreases 0.5%.

Multi-task Study
We also run an experiment to test the performance of our
multi-task model. We select different losses that participate
in the training procedure to observe the performance af-
fected by answer boundary detect or classiﬁcation.

Table 4 shows the performance. Here we use EM ∗ and
F 1∗ to represent the EM and F1 score when the classiﬁca-
tion is not part of the task, which makes it very much like
the task in SQuAD 1.1.

Loss EM∗

F1∗ Classiﬁcation Acc.

L
LAV
LA

75.3
-
77.2

84.8
-
85.1

80.2
67.1
-

Table 4: Multi-task performance on the development set.

To test our classiﬁer performance, we do not use back-
ward propagation over the loss of answer boundary detec-
tion and simply run a classiﬁcation task. Results (the ﬁrst
two rows in Table 4) show that there is a large gain when
using the multi-task model. The answer boundary detection
task helps the encoder learn information between the pas-
sage and question and also feed information into the univer-
sal node, therefore we can use a summarized representation
of the passage and question as well as the universal node
to distinguish whether the question is answerable, i.e., help
improve classiﬁcation.

For the answer boundary detection task, we ﬁnd that the
multi-task setup (i.e., the classiﬁcation layer participates in
the training process) does not help its performance. Since the
classiﬁer and pointer layer shared the encoding process, we
originally expected that classiﬁcation information can help
detect answer boundaries. But this is not the case. We think
this is also reasonable since distinguishing whether the ques-
tion is answerable is mainly focusing on the interactions be-
tween the passage-question pair, so once the question is pre-
dicted as answerable or not, it has nothing to do with the an-
swer boundaries. This is consistent with how human-beings
do this classiﬁcation task.

We also run the test over SQuAD 1.1 development test
to evaluate the performance. Due to a condensed structure,
our model achieves an F 1∗ score of less than 86%, which
is not a very competitive score on SQuAD 1.1 test. But as
shown above, our model achieves a good score in SQuAD
2.0 test, which shows this model has the potential to achieve
higher performance by making progress on both the answer
detection and classiﬁcation tasks.

Overall, we can conclude that our multi-task model works
well since the performance of unanswerability classiﬁcation
improves signiﬁcantly when the answer pointer and answer
veriﬁer work simultaneously.

Study on the Different Thresholds of
Unanswerability Classiﬁcation
The output b of the answer veriﬁer is the probability of a
question being unanswerable. The smaller the output, the
lower the probability of unanswerability is. In SQuAD 2.0,
the proportions of unanswerable questions are different in
the training and test sets. The default threshold 0.5 is opti-
mized on the training set, but not suitable for the test set.
Therefore, it is reasonable to set a proper threshold to man-
ually adapt to the test set.

As mentioned in SQuAD 2.0 paper (Rajpurkar, Jia, and
Liang 2018), different thresholds for answerability predic-
tion result in ﬂuctuated scores between answerable and
unanswerable questions. Here we show the variation of the
F1 score with different thresholds in Figure . The threshold
between [0, 1] is used to decide whether a question can be
answered. When the threshold is set to 0, all questions are
considered as answerable.

Avg F1

NoAns F1

HasAns F1

e
r
o
c
S
1
F

80

75

70

65

0.5

0.55

0.6

0.65

0.7

0.75

Threshold t

Figure 2: F1 score variation with different
“NoAns F1” is the recall of unanswerable questions.

thresholds.

As we can see, when the threshold is set to 0.5, F1 score
of answerable questions is similar to that of unanswerable
questions. When we increase the threshold (i.e., more likely
to predict the question as unanswerable), performance for
answerable questions degrades, and improves for unanswer-
able questions. This is as expected. We can see that the over-
all F 1 score is slightly better, which is consistent with the
idea from SQuAD 2.0. In addition, we ﬁnd that for larger
thresholds, the variance between EM and F 1 is narrowed
since EM and F 1 scores for unanswerable questions are
the same.

Finally, we set the threshold to be 0.7 for the submission

system to SQuAD evaluation.

Related Work

End-to-end Models for MRC
Currently, end-to-end neural network models have achieved
great successes for machine reading comprehension (Seo et

al. 2016; Kumar et al. 2015; Sukhbaatar et al. 2015; Cui et al.
2016; Xiong, Zhong, and Socher 2016; Dhingra et al. 2016;
Shen et al. 2016; Hu et al. 2017; Wang, Yan, and Wu 2018).
Most of these models consist of three components: encoder,
interaction, and pointer. The BiLSTM is widely used for
encoding the embedded representation. For the interaction,
bidirectional attention mechanism is very effective to fuse
information of the question and passage. Finally, a pointer
network (Vinyals, Fortunato, and Jaitly 2015) is used to
predict the span boundaries of the answer. Speciﬁcally, in
SQuAD test (Rajpurkar et al. 2016), there are approaches
to combine match-LSTM and pointer networks to produce
boundaries of the answer and employ variant bidirectional
attention mechanism to match the question and passage mu-
tually.

In our model, we learn from previous work and develop a
condensed end-to-end model for the SQuAD 2.0 task. Dif-
ferent from the previous models, we use a uniﬁed representa-
tion to encode the question and passage simultaneously, and
introduce a universal node to encode the fused information
of the question and passage, which also plays an important
role to predict the unanswerability of a question.

MRC with Unanswerable Questions
MRC with unanswerable questions is a more challenging
task. Previous work Levy et al.; Clark and Gardner (2017;
2017) has attempted to normalize a no-answer score depend-
ing on the probability of all answer spans and still detect
boundaries at the same time. But the scores of the answer
span predictions are not very discriminative in distinguish-
ing whether the question is answerable. Therefore, this kind
of approaches, though relatively simple, cannot effectively
deal with the answerability of a question.

Hu et al.; Tan et al. (2018; 2018) introduced an answer
veriﬁer idea to construct a classiﬁcation layer. However, this
kind of approaches usually has a pipeline structure. The an-
swer pointer and answer veriﬁer have their respective mod-
els that are trained separately.

Multi-task models Different from existing work, we re-
gard the MRC with unanswerable questions as a multi-task
learning problem (Caruana 1997) by sharing some meta-
knowledge. Intuitively, answer prediction and answer veri-
ﬁcation are related tasks since the underlying comprehen-
sion and reasoning of language for these components is the
same. Therefore, we construct a multi-task model to solve
three sub-tasks: answer pointer, no-answer pointer, and an-
swer veriﬁer.

Conclusion and Future Work
In this paper, we regard the MRC with unanswerable ques-
tions as multi-task learning problems and propose the U-Net,
a simple end-to-end model for MRC challenges. U-Net has
good performance on SQuAD 2.0. We ﬁrst add a universal
node to learn a fused representation from both the question
and passage, then use a concatenated representation to pass
through encoding layers. We only treat question and passage
differently during attention interactions. In the rest blocks of

U-Net, we still use the uniﬁed representation containing both
the question and passage representation. Finally, we train the
U-Net as a multi-task framework to determine the ﬁnal an-
swer boundaries as well as whether the question is answer-
able. Our model has very simple structure yet achieves good
results on SQuAD 2.0 test.

Our future work is to reconstruct the structure of U-Net by
replacing the current multi-level attention block with a sim-
pler self-attention mechanism, which we believe can cap-
ture the question and passage information, and intuitively
is also coherent with the rest of our U-Net model. In addi-
tion, we will improve the answer boundary detection per-
formance based on some of the previous successful models.
Since our model actually does not achieve very competitive
performance in the boundary detection task yet still has a
good overall performance on SQuAD 2.0 test, we are opti-
mistic that our U-Net model is potentially capable of achiev-
ing better performance. Furthermore, our model has a simple
structure and is easy to implement, therefore we believe that
our model can be easily modiﬁed for various datasets.

Acknowledgement
We would like to thank Robin Jia, Pranav Rajpurkar for their
help with SQuAD 2.0 submissions.

2016.

References
[Bahdanau, Cho, and Bengio 2014] Bahdanau, D.; Cho, K.;
and Bengio, Y. 2014. Neural machine translation by jointly
learning to align and translate. ArXiv e-prints.
[Caruana 1997] Caruana, R. 1997. Multitask learning. Ma-
chine Learning 28(1):41–75.
[Chen et al. 2017] Chen, D.; Fisch, A.; Weston, J.; and Bor-
des, A. 2017. Reading wikipedia to answer open-domain
questions. CoRR abs/1704.00051.
[Clark and Gardner 2017] Clark, C., and Gardner, M. 2017.
Simple and effective multi-paragraph reading comprehen-
sion. arXiv preprint arXiv:1710.10723.
[Cui et al. 2016] Cui, Y.; Chen, Z.; Wei, S.; Wang, S.; Liu,
T.; and Hu, G.
Attention-over-attention neu-
ral networks for reading comprehension. arXiv preprint
arXiv:1607.04423.
[Dhingra et al. 2016] Dhingra, B.; Liu, H.; Cohen, W. W.;
and Salakhutdinov, R. 2016. Gated-attention readers for
text comprehension. arXiv preprint arXiv:1606.01549.
[Hermann et al. 2015] Hermann, K. M.; Kocisky, T.; Grefen-
stette, E.; Espeholt, L.; Kay, W.; Suleyman, M.; and Blun-
som, P. 2015. Teaching machines to read and compre-
hend. In Advances in Neural Information Processing Sys-
tems, 1684–1692.
[Hill et al. 2015] Hill, F.; Bordes, A.; Chopra, S.; and We-
ston, J. 2015. The goldilocks principle: Reading children’s
books with explicit memory representations. arXiv preprint
arXiv:1511.02301.
and
[Hochreiter and Schmidhuber 1997] Hochreiter,
Schmidhuber, J. 1997. Long short-term memory. Neural
computation 9(8):1735–1780.

S.,

2017.

2014.
CoRR

[Hu et al. 2017] Hu, M.; Peng, Y.; Huang, Z.; Qiu, X.;
Wei, F.; and Zhou, M.
2017. Reinforced mnemonic
reader for machine reading comprehension. arXiv preprint
arXiv:1705.02798.
[Hu et al. 2018] Hu, M.; Peng, Y.; Huang, Z.; Yang, N.;
Zhou, M.; et al. 2018. Read+ verify: Machine reading
comprehension with unanswerable questions. arXiv preprint
arXiv:1808.05759.
[Huang et al. 2017] Huang, H.; Zhu, C.; Shen, Y.; and Chen,
W.
Fusionnet: Fusing via fully-aware atten-
tion with application to machine comprehension. CoRR
abs/1711.07341.
[Huang, Liu, and Weinberger 2016] Huang, G.; Liu, Z.; and
Weinberger, K. Q. 2016. Densely connected convolutional
networks. CoRR abs/1608.06993.
[Kingma and Ba 2014] Kingma, D. P., and Ba, J.
Adam: A method for stochastic optimization.
abs/1412.6980.
[Kumar et al. 2015] Kumar, A.; Irsoy, O.; Su, J.; Bradbury,
J.; English, R.; Pierce, B.; Ondruska, P.; Gulrajani, I.; and
Socher, R. 2015. Ask me anything: Dynamic memory
networks for natural language processing. arXiv preprint
arXiv:1506.07285.
[Lee et al. 2016] Lee, K.; Kwiatkowski, T.; Parikh, A. P.; and
Das, D. 2016. Learning recurrent span representations for
extractive question answering. CoRR abs/1611.01436.
[Levy et al. 2017] Levy, O.; Seo, M.; Choi, E.; and Zettle-
moyer, L. 2017. Zero-shot relation extraction via reading
comprehension. arXiv preprint arXiv:1706.04115.
[Liu et al. 2017] Liu, X.; Shen, Y.; Duh, K.; and Gao, J.
2017. Stochastic answer networks for machine reading com-
prehension. CoRR abs/1712.03556.
J.;
[Pennington, Socher, and Manning 2014] Pennington,
Socher, R.; and Manning, C. D. 2014. Glove: Global vectors
for word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP 2014)
12:1532–1543.
[Peters et al. 2018] Peters, M. E.; Neumann, M.; Iyyer, M.;
Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018.
In Proc. of
Deep contextualized word representations.
NAACL.
[Rajpurkar et al. 2016] Rajpurkar, P.; Zhang, J.; Lopyrev,
K.; and Liang, P.
SQuAD: 100,000+ ques-
tions for machine comprehension of text. arXiv preprint
arXiv:1606.05250.
[Rajpurkar, Jia, and Liang 2018] Rajpurkar, P.; Jia, R.; and
Liang, P. 2018. Know What You Don’t Know: Unanswer-
able Questions for SQuAD. ArXiv e-prints.
[Seo et al. 2016] Seo, M.; Kembhavi, A.; Farhadi, A.; and
Hajishirzi, H. 2016. Bidirectional attention ﬂow for ma-
chine comprehension. arXiv preprint arXiv:1611.01603.
[Shen et al. 2016] Shen, Y.; Huang, P.-S.; Gao, J.; and Chen,
W. 2016. Reasonet: Learning to stop reading in machine
comprehension. arXiv preprint arXiv:1609.05284.
[Srivastava et al. 2014] Srivastava, N.; Hinton, G. E.;
I.; and Salakhutdinov, R.
Krizhevsky, A.; Sutskever,

2016.

2014. Dropout: a simple way to prevent neural networks
from overﬁtting. Journal of Machine Learning Research
15(1):1929–1958.
[Sukhbaatar et al. 2015] Sukhbaatar, S.; Weston, J.; Fergus,
R.; et al. 2015. End-to-end memory networks. In Advances
in Neural Information Processing Systems, 2431–2439.
[Tan et al. 2018] Tan, C.; Wei, F.; Zhou, Q.; Yang, N.; Lv,
W.; and Zhou, M. 2018. I know there is no answer: Model-
ing answer validation for machine reading comprehension.
In CCF International Conference on Natural Language Pro-
cessing and Chinese Computing, 85–97. Springer.
[Vaswani et al. 2017] Vaswani, A.; Shazeer, N.; Parmar, N.;
Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and
Polosukhin, I. 2017. Attention is all you need. CoRR
abs/1706.03762.
[Vinyals, Fortunato, and Jaitly 2015] Vinyals, O.; Fortunato,
M.; and Jaitly, N. 2015. Pointer Networks. ArXiv e-prints.
[Wang and Jiang 2016] Wang, S., and Jiang, J. 2016. Ma-
chine comprehension using match-lstm and answer pointer.
CoRR abs/1608.07905.
[Wang et al. 2017] Wang, W.; Yang, N.; Wei, F.; Chang, B.;
and Zhou, M. 2017. Gated self-matching networks for read-
ing comprehension and question answering. In Proceedings
of the 55th Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2017, Vancouver, Canada, July 30 -
August 4, Volume 1: Long Papers, 189–198.
[Wang, Yan, and Wu 2018] Wang, W.; Yan, M.; and Wu, C.
2018. Multi-granularity hierarchical attention fusion net-
works for reading comprehension and question answering.
In Proceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Papers),
volume 1, 1705–1714.
[Xiong, Zhong, and Socher 2016] Xiong, C.; Zhong, V.; and
Socher, R. 2016. Dynamic coattention networks for question
answering. arXiv preprint arXiv:1611.01604.

