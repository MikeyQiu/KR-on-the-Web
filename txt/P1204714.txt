The Ubuntu Dialogue Corpus: A Large Dataset for Research in
Unstructured Multi-Turn Dialogue Systems

Ryan Lowe∗*, Nissan Pow*, Iulian V. Serban† and Joelle Pineau*

*School of Computer Science, McGill University, Montreal, Canada
†Department of Computer Science and Operations Research, Universié de Montréal, Montreal, Canada

6
1
0
2
 
b
e
F
 
4
 
 
]
L
C
.
s
c
[
 
 
3
v
9
0
9
8
0
.
6
0
5
1
:
v
i
X
r
a

Abstract

This paper introduces the Ubuntu Dia-
logue Corpus, a dataset containing almost
1 million multi-turn dialogues, with a to-
tal of over 7 million utterances and 100
million words. This provides a unique re-
source for research into building dialogue
managers based on neural language mod-
els that can make use of large amounts
of unlabeled data. The dataset has both
the multi-turn property of conversations
in the Dialog State Tracking Challenge
datasets, and the unstructured nature of in-
teractions from microblog services such
as Twitter. We also describe two neural
learning architectures suitable for analyz-
ing this dataset, and provide benchmark
performance on the task of selecting the
best next response.

1

Introduction

The ability for a computer to converse in a nat-
ural and coherent manner with a human has long
been held as one of the primary objectives of artiﬁ-
cial intelligence (AI). In this paper we consider the
problem of building dialogue agents that have the
ability to interact in one-on-one multi-turn con-
versations on a diverse set of topics. We primar-
ily target unstructured dialogues, where there is
no a priori logical representation for the informa-
tion exchanged during the conversation. This is in
contrast to recent systems which focus on struc-
tured dialogue tasks, using a slot-ﬁlling represen-
tation [10, 27, 32].

We observe that in several subﬁelds of AI—
computer vision, speech recognition, machine
translation—fundamental break-throughs were
achieved in recent years using machine learning

methods, more speciﬁcally with neural architec-
tures [1]; however, it is worth noting that many
of the most successful approaches, in particular
convolutional and recurrent neural networks, were
known for many years prior.
It is therefore rea-
sonable to attribute this progress to three major
factors: 1) the public distribution of very large
rich datasets [5], 2) the availability of substantial
computing power, and 3) the development of new
training methods for neural architectures, in par-
ticular leveraging unlabeled data. Similar progress
has not yet been observed in the development of
dialogue systems. We hypothesize that this is due
to the lack of sufﬁciently large datasets, and aim
to overcome this barrier by providing a new large
corpus for research in multi-turn conversation.

The new Ubuntu Dialogue Corpus consists of
almost one million two-person conversations ex-
tracted from the Ubuntu chat logs1, used to receive
technical support for various Ubuntu-related prob-
lems. The conversations have an average of 8 turns
each, with a minimum of 3 turns. All conversa-
tions are carried out in text form (not audio). The
dataset is orders of magnitude larger than struc-
tured corpuses such as those of the Dialogue State
Tracking Challenge [32]. It is on the same scale as
recent datasets for solving problems such as ques-
tion answering and analysis of microblog services,
such as Twitter [22, 25, 28, 33], but each conversa-
tion in our dataset includes several more turns, as
well as longer utterances. Furthermore, because
it targets a speciﬁc domain, namely technical sup-
port, it can be used as a case study for the devel-
opment of AI agents in targeted applications, in
contrast to chatbox agents that often lack a well-
deﬁned goal [26].

In addition to the corpus, we present learning
architectures suitable for analyzing this dataset,
ranging from the simple frequency-inverse docu-

1These logs are available from 2004 to 2015 at http:

∗The ﬁrst two authors contributed equally.

//irclogs.ubuntu.com/

ment frequency (TF-IDF) approach, to more so-
phisticated neural models including a Recurrent
Neural Network (RNN) and a Long Short-Term
Memory (LSTM) architecture. We provide bench-
trained
mark performance of these algorithms,
with our new corpus, on the task of selecting the
best next response, which can be achieved with-
out requiring any human labeling. The dataset is
ready for public release2. The code developed for
the empirical results is also available3.

2 Related Work

We brieﬂy review existing dialogue datasets, and
some of the more recent learning architectures
used for both structured and unstructured dia-
logues. This is by no means an exhaustive list
(due to space constraints), but surveys resources
most related to our contribution. A list of datasets
discussed is provided in Table 1.

2.1 Dialogue Datasets

The Switchboard dataset [8], and the Dialogue
State Tracking Challenge (DSTC) datasets [32]
have been used to train and validate dialogue man-
agement systems for interactive information re-
trieval. The problem is typically formalized as a
slot ﬁlling task, where agents attempt to predict
the goal of a user during the conversation. These
datasets have been signiﬁcant resources for struc-
tured dialogues, and have allowed major progress
in this ﬁeld, though they are quite small compared
to datasets currently used for training neural archi-
tectures.

Recently, a few datasets have been used con-
taining unstructured dialogues extracted from
Twitter4. Ritter et al. [21] collected 1.3 million
conversations; this was extended in [28] to take ad-
vantage of longer contexts by using A-B-A triples.
Shang et al. [25] used data from a similar Chinese
website called Weibo5. However to our knowl-
edge, these datasets have not been made public,
and furthermore, the post-reply format of such mi-
croblogging services is perhaps not as represen-
tative of natural dialogue between humans as the
continuous stream of messages in a chat room. In

2Note that a new version of

is now
https://github.com/rkadlec/
available:
ubuntu-ranking-dataset-creator.
This ver-
sion makes some adjustments and ﬁxes some bugs from the
ﬁrst version.

the dataset

3http://github.com/npow/ubottu
4https://twitter.com/
5http://www.weibo.com/

fact, Ritter et al. estimate that only 37% of posts
on Twitter are ‘conversational in nature’, and 69%
of their collected data contained exchanges of only
length 2 [21]. We hypothesize that chat-room style
messaging is more closely correlated to human-to-
human dialogue than micro-blogging websites, or
forum-based sites such as Reddit.

Part of the Ubuntu chat logs have previously
been aggregated into a dataset, called the Ubuntu
Chat Corpus [30]. However that resource pre-
serves the multi-participant structure and thus is
less amenable to the investigation of more tradi-
tional two-party conversations.

Also weakly related to our contribution is the
problem of question-answer systems.
Several
datasets of question-answer pairs are available [3],
however these interactions are much shorter than
what we seek to study.

2.2 Learning Architectures

Most dialogue research has historically focused
on structured slot-ﬁlling tasks [24]. Various ap-
proaches were proposed, yet few attempts lever-
age more recent developments in neural learning
architectures. A notable exception is the work of
Henderson et al. [11], which proposes an RNN
structure, initialized with a denoising autoencoder,
to tackle the DSTC 3 domain.

Work on unstructured dialogues, recently pi-
oneered by Ritter et al. [22], proposed a re-
sponse generation model for Twitter data based on
ideas from Statistical Machine Translation. This
is shown to give superior performance to previ-
ous information retrieval (e.g. nearest neighbour)
approaches [14]. This idea was further devel-
oped by Sordoni et al. [28] to exploit information
from a longer context, using a structure similar to
the Recurrent Neural Network Encoder-Decoder
model [4]. This achieves rather poor performance
on A-B-A Twitter triples when measured by the
BLEU score (a standard for machine translation),
yet performs comparatively better than the model
of Ritter et al. [22]. Their results are also veriﬁed
with a human-subject study. A similar encoder-
decoder framework is presented in [25]. This
model uses one RNN to transform the input to
some vector representation, and another RNN to
‘decode’ this representation to a response by gen-
erating one word at a time. This model is also eval-
uated in a human-subject study, although much
smaller in size than in [28]. Overall, these models

Dataset

Switchboard [8]

DSTC1 [32]

DSTC2 [10]

DSTC3 [9]

DSTC4[13]

Twitter
Corpus [21]
Twitter Triple
Corpus [28]
Sina Weibo [25]

Ubuntu Dialogue
Corpus

Type

Human-human
spoken
Human-computer
spoken
Human-computer
spoken
Human-computer
spoken
Human-human
spoken
Human-human
micro-blog
Human-human
micro-blog
Human-human
micro-blog
Human-human
chat

Task

Various

State
tracking
State
tracking
State
tracking
State
tracking
Next utterance
generation
Next utterance
generation
Next utterance
generation
Next utterance
classiﬁcation

# Dialogues

# Utterances

# Words

Description

2,400

—

3,000,000

15,000

210,000

3,000

2,265

35

24,000

15,000

—

1,300,000

3,000,000

29,000,000

87,000,000

4,435,959

8,871,918

—

—

—

—

—

—

930,000

7,100,000

100,000,000

Telephone conversations
on pre-speciﬁed topics
Bus ride information
system
Restaurant booking
system
Tourist information
system
21 hours of tourist info
exchange over Skype
Post/ replies extracted
from Twitter
A-B-A triples from
Twitter replies
Post/ reply pairs extracted
from Weibo
Extracted from Ubuntu
Chat Logs

Table 1: A selection of structured and unstructured large-scale datasets applicable to dialogue systems.
Faded datasets are not publicly available. The last entry is our contribution.

highlight the potential of neural learning architec-
tures for interactive systems, yet so far they have
been limited to very short conversations.

3 The Ubuntu Dialogue Corpus

We seek a large dataset for research in dialogue
systems with the following properties:

• Two-way (or dyadic) conversation, as op-
posed to multi-participant chat, preferably
human-human.

• Large number of conversations; 105 − 106
is typical of datasets used for neural-network
learning in other areas of AI.

• Many conversations with several turns (more

• Task-speciﬁc domain, as opposed to chatbot

than 3).

systems.

All of these requirements are satisﬁed by the
Ubuntu Dialogue Corpus presented in this paper.

3.1 Ubuntu Chat Logs

The Ubuntu Chat Logs refer to a collection of logs
from Ubuntu-related chat rooms on the Freenode
Internet Relay Chat (IRC) network. This protocol
allows for real-time chat between a large number
of participants. Each chat room, or channel, has
a particular topic, and every channel participant
can see all the messages posted in a given chan-
nel. Many of these channels are used for obtaining
technical support with various Ubuntu issues.

As the contents of each channel are moderated,
most interactions follow a similar pattern. A new
user joins the channel, and asks a general ques-
tion about a problem they are having with Ubuntu.
Then, another more experienced user replies with

a potential solution, after ﬁrst addressing the ’user-
name’ of the ﬁrst user. This is called a name men-
tion [29], and is done to avoid confusion in the
channel — at any given time during the day, there
can be between 1 and 20 simultaneous conversa-
tions happening in some channels.
In the most
popular channels, there is almost never a time
when only one conversation is occurring; this ren-
ders it particularly problematic to extract dyadic
dialogues. A conversation between a pair of users
generally stops when the problem has been solved,
though some users occasionally continue to dis-
cuss a topic not related to Ubuntu.

Despite the nature of the chat room being a con-
stant stream of messages from multiple users, it is
through the fairly rigid structure in the messages
that we can extract the dialogues between users.
Figure 4 shows an example chat room conversa-
tion from the #ubuntu channel as well as the ex-
tracted dialogues, which illustrates how users usu-
ally state the username of the intended message
recipient before writing their reply (we refer to all
replies and initial questions as ‘utterances’). For
example, it is clear that users ‘Taru’ and ‘kuja’
are engaged in a dialogue, as are users ‘Old’ and
‘bur[n]er’, while user ‘_pm’ is asking an initial
question, and ‘LiveCD’ is perhaps elaborating on
a previous comment.

3.2 Dataset Creation

In order to create the Ubuntu Dialogue Corpus,
ﬁrst a method had to be devised to extract dyadic
dialogues from the chat room multi-party conver-
sations. The ﬁrst step was to separate every mes-
sage into 4-tuples of (time, sender, recipient, utter-
ance). Given these 4-tuples, it is straightforward to

group all tuples where there is a matching sender
and recipient. Although it is easy to separate the
time and the sender from the rest, ﬁnding the in-
tended recipient of the message is not always triv-
ial.

3.2.1 Recipient Identiﬁcation
While in most cases the recipient is the ﬁrst word
of the utterance, it is sometimes located at the end,
or not at all in the case of initial questions. Fur-
thermore, some users choose names correspond-
ing to common English words, such as ‘the’ or
‘stop’, which could lead to many false positives.
In order to solve this issue, we create a dictionary
of usernames from the current and previous days,
and compare the ﬁrst word of each utterance to its
If a match is found, and the word does
entries.
not correspond to a very common English word6,
it is assumed that this user was the intended recip-
ient of the message. If no matches are found, it is
assumed that the message was an initial question,
and the recipient value is left empty.

3.2.2 Utterance Creation
The dialogue extraction algorithm works back-
wards from the ﬁrst response to ﬁnd the initial
question that was replied to, within a time frame
of 3 minutes. A ﬁrst response is identiﬁed by the
presence of a recipient name (someone from the
recent conversation history). The initial question
is identiﬁed to be the most recent utterance by the
recipient identiﬁed in the ﬁrst response.

All utterances that do not qualify as a ﬁrst re-
sponse or an initial question are discarded; initial
questions that do not generate any response are
also discarded. We additionally discard conversa-
tions longer than ﬁve utterances where one user
says more than 80% of the utterances, as these are
typically not representative of real chat dialogues.
Finally, we consider only extracted dialogues that
consist of 3 turns or more to encourage the model-
ing of longer-term dependencies.

To alleviate the problem of ‘holes’ in the dia-
logue, where one user does not address the other
explicitly, as in Figure 5, we check whether each
user talks to someone else for the duration of their
conversation. If not, all non-addressed utterances
are added to the dialogue. An example conversa-
tion along with the extracted dialogues is shown
in Figure 5. Note that we also concatenate all con-
secutive utterances from a given user.

6We use the GNU Aspell spell checking dictionary.

Figure 1: Plot of number of conversations with a
given number of turns. Both axes use a log scale.

# dialogues (human-human)
# utterances (in total)
# words (in total)
Min. # turns per dialogue
Avg. # turns per dialogue
Avg. # words per utterance
Median conversation length (min)

930,000
7,100,000
100,000,000
3
7.71
10.34
6

Table 2: Properties of Ubuntu Dialogue Corpus.

We do not apply any further pre-processing (e.g.
tokenization, stemming) to the data as released in
the Ubuntu Dialogue Corpus. However the use of
pre-processing is standard for most NLP systems,
and was also used in our analysis (see Section 4.)

3.2.3 Special Cases and Limitations

It is often the case that a user will post an ini-
tial question, and multiple people will respond to
it with different answers.
In this instance, each
conversation between the ﬁrst user and the user
who replied is treated as a separate dialogue. This
has the unfortunate side-effect of having the ini-
tial question appear multiple times in several dia-
logues. However the number of such cases is suf-
ﬁciently small compared to the size of the dataset.
Another issue to note is that the utterance post-
ing time is not considered for segmenting conver-
sations between two users. Even if two users have
a conversation that spans multiple hours, or even
days, this is treated as a single dialogue. However,
such dialogues are rare. We include the posting
time in the corpus so that other researchers may
ﬁlter as desired.

3.3 Dataset Statistics

Table 2 summarizes properties of the Ubuntu Dia-
logue Corpus. One of the most important features

of the Ubuntu chat logs is its size. This is cru-
cial for research into building dialogue managers
based on neural architectures. Another important
characteristic is the number of turns in these dia-
logues. The distribution of the number of turns is
shown in Figure 1. It can be seen that the num-
ber of dialogues and turns per dialogue follow an
approximate power law relationship.

3.4 Test Set Generation

We set aside 2% of the Ubuntu Dialogue Corpus
conversations (randomly selected) to form a test
set that can be used for evaluation of response se-
lection algorithms. Compared to the rest of the
corpus, this test set has been further processed to
extract a pair of (context, response, ﬂag) triples
from each dialogue. The ﬂag is a Boolean vari-
able indicating whether or not the response was the
actual next utterance after the given context. The
response is a target (output) utterance which we
aim to correctly identify. The context consists of
the sequence of utterances appearing in dialogue
prior to the response. We create a pair of triples,
where one triple contains the correct response (i.e.
the actual next utterance in the dialogue), and the
other triple contains a false response, sampled ran-
domly from elsewhere within the test set. The ﬂag
is set to 1 in the ﬁrst case and to 0 in the second
case. An example pair is shown in Table 3. To
make the task harder, we can move from pairs of
responses (one correct, one incorrect) to a larger
set of wrong responses (all with ﬂag=0). In our
experiments below, we consider both the case of 1
wrong response and 10 wrong responses.

Context
well, can I move the drives?
__EOS__ ah not like that

well, can I move the drives?
__EOS__ ah not like that

Response
I guess I could just
get an enclosure and
copy via USB
you can use "ps ax"
and "kill (PID #)"

Flag
1

0

Table 3: Test set example with (context, reply,
ﬂag) format. The ’__EOS__’ tag is used to denote
the end of an utterance within the context.

Since we want to learn to predict all parts of a
conversation, as opposed to only the closing state-
ment, we consider various portions of context for
the conversations in the test set. The context size is
determined stochastically using a simple formula:

c = min(t − 1, n − 1),

Here, C denotes the maximum desired context
size, which we set to C = 20. The last term is
the desired minimum context size, which we set
to be 2. Parameter t is the actual length of that
dialogue (thus the constraint that c ≤ t − 1), and
n is a random number corresponding to the ran-
domly sampled context length, that is selected to
be inversely proportional to C.

In practice, this leads to short test dialogues
having short contexts, while longer dialogues are
often broken into short or medium-length seg-
ments, with the occasional long context of 10 or
more turns.

3.5 Evaluation Metric

We consider the task of best response selection.
This can be achieved by processing the data as de-
scribed in Section 3.4, without requiring any hu-
man labels. This classiﬁcation task is an adapta-
tion of the recall and precision metrics previously
applied to dialogue datasets [24].

A family of metrics often used in language tasks
is Recall@k (denoted R@1 R@2, R@5 below).
Here the agent is asked to select the k most likely
responses, and it is correct if the true response is
among these k candidates. Only the R@1 metric
is relevant in the case of binary classiﬁcation (as
in the Table 3 example).

Although a language model that performs well
on response classiﬁcation is not a gauge of good
performance on next utterance generation, we hy-
pothesize that improvements on a model with re-
gards to the classiﬁcation task will eventually lead
to improvements for the generation task. See Sec-
tion 6 for further discussion of this point.

4 Learning Architectures for
Unstructured Dialogues

To provide further evidence of the value of
our dataset for research into neural architectures
for dialogue managers, we provide performance
benchmarks for two neural learning algorithms, as
well as one naive baseline. The approaches con-
sidered are: TF-IDF, Recurrent Neural networks
(RNN), and Long Short-Term Memory (LSTM).
Prior to applying each method, we perform stan-
dard pre-processing of the data using the NLTK7
library and Twitter tokenizer8 to parse each utter-
ance. We use generic tags for various word cat-

where n =

+ 2, η ∼ U nif (C/2, 10C)

10C
η

7www.nltk.org/
8http://www.ark.cs.cmu.edu/TweetNLP/

egories, such as names, locations, organizations,
URLs, and system paths.

To train the RNN and LSTM architectures, we
process the full training Ubuntu Dialogue Corpus
into the same format as the test set described in
Section 3.4, extracting (context, response, ﬂag)
triples from dialogues. For the training set, we
do not sample the context length, but instead con-
sider each utterance (starting at the 3rd one) as a
potential response, with the previous utterances as
its context. So a dialogue of length 10 yields 8
training examples. Since these are overlapping,
they are clearly not independent, but we consider
this a minor issue given the size of the dataset (we
further alleviate the issue by shufﬂing the training
examples). Negative responses are selected at ran-
dom from the rest of the training data.

4.1 TF-IDF

Term frequency-inverse document frequency is a
statistic that intends to capture how important a
given word is to some document, which in our case
is the context [20]. It is a technique often used in
document classiﬁcation and information retrieval.
The ‘term-frequency’ term is simply a count of the
number of times a word appears in a given context,
while the ‘inverse document frequency’ term puts
a penalty on how often the word appears elsewhere
in the corpus. The ﬁnal score is calculated as the
product of these two terms, and has the form:

tﬁdf(w, d, D) = f (w, d)×log

N
|{d ∈ D : w ∈ d}|

,

where f (w, d) indicates the number of times word
w appeared in context d, N is the total number
of dialogues, and the denominator represents the
number of dialogues in which the word w appears.
For classiﬁcation, the TF-IDF vectors are ﬁrst
calculated for the context and each of the candi-
date responses. Given a set of candidate response
vectors, the one with the highest cosine similarity
to the context vector is selected as the output. For
Recall@k, the top k responses are returned.

4.2 RNN

Recurrent neural networks are a variant of neural
networks that allows for time-delayed directed cy-
cles between units [17]. This leads to the forma-
tion of an internal state of the network, ht, which
allows it to model time-dependent data. The in-
ternal state is updated at each time step as some

Figure 2: Diagram of our model. The RNNs have
tied weights. c, r are the last hidden states from
the RNNs. ci, ri are word vectors for the context
and response, i < t. We consider contexts up to a
maximum of t = 160.

function of the observed variables xt, and the hid-
den state at the previous time step ht−1. Wx and
Wh are matrices associated with the input and hid-
den state.

ht = f (Whht−1 + Wxxt).

A diagram of an RNN can be seen in Figure 2.
RNNs have been the primary building block of
many current neural language models [22, 28],
which use RNNs for an encoder and decoder. The
ﬁrst RNN is used to encode the given context,
and the second RNN generates a response by us-
ing beam-search, where its initial hidden state is
biased using the ﬁnal hidden state from the ﬁrst
RNN. In our work, we are concerned with classi-
ﬁcation of responses, instead of generation. We
build upon the approach in [2], which has also
been recently applied to the problem of question
answering [33].

We utilize a siamese network consisting of two
RNNs with tied weights to produce the embed-
dings for the context and response. Given some
input context and response, we compute their em-
beddings — c, r ∈ Rd, respectively — by feeding
the word embeddings one at a time into its respec-
tive RNN. Word embeddings are initialized using
the pre-trained vectors (Common Crawl, 840B to-
kens from [19]), and ﬁne-tuned during training.
The hidden state of the RNN is updated at each
step, and the ﬁnal hidden state represents a sum-
mary of the input utterance. Using the ﬁnal hid-
den states from both RNNs, we then calculate the
probability that this is a valid pair:

p(ﬂag = 1|c, r, M ) = σ(cT M r + b),

where the bias b and the matrix M ∈ Rd×d are
learned model parameters. This can be thought
of as a generative approach; given some input re-
sponse, we generate a context with the product
c(cid:48) = M r, and measure the similarity to the actual
context using the dot product. This is converted
to a probability with the sigmoid function. The
model is trained by minimizing the cross entropy
of all labeled (context, response) pairs [33]:

examples. Of course, the Recall@2 and Recall@5
are not relevant in the binary classiﬁcation case9.

Method
1 in 2 R@1
1 in 10 R@1
1 in 10 R@2
1 in 10 R@5

TF-IDF
RNN LSTM
65.9% 76.8% 87.8%
41.0% 40.3% 60.4%
54.5% 54.7% 74.5%
70.8% 81.9% 92.6%

L = −

log p(ﬂagn|cn, rn, M ) +

(cid:88)

n

λ
2

||θ = ||2
F

Table 4: Results for the three algorithms using var-
ious recall measures for binary (1 in 2) and 1 in 10
(1 in 10) next utterance classiﬁcation %.

where ||θ||2
F is the Frobenius norm of θ = {M, b}.
In our experiments, we use λ = 0 for computa-
tional simplicity.

For training, we used a 1:1 ratio between true re-
sponses (ﬂag = 1), and negative responses (ﬂag=0)
drawn randomly from elsewhere in the training
set. The RNN architecture is set to 1 hidden layer
with 50 neurons. The Wh matrix is initialized us-
ing orthogonal weights [23], while Wx is initial-
ized using a uniform distribution with values be-
tween -0.01 and 0.01. We use Adam as our opti-
mizer [15], with gradients clipped to 10. We found
that weight initialization as well as the choice of
optimizer were critical for training the RNNs.

4.3 LSTM

In addition to the RNN model, we consider the
same architecture but changed the hidden units
to long-short term memory (LSTM) units [12].
LSTMs were introduced in order to model longer-
term dependencies. This is accomplished using a
series of gates that determine whether a new in-
put should be remembered, forgotten (and the old
value retained), or used as output. The error sig-
nal can now be fed back indeﬁnitely into the gates
of the LSTM unit. This helps overcome the van-
ishing and exploding gradient problems in stan-
dard RNNs, where the error gradients would oth-
erwise decrease or increase at an exponential rate.
In training, we used 1 hidden layer with 200 neu-
rons. The hyper-parameter conﬁguration (includ-
ing number of neurons) was optimized indepen-
dently for RNNs and LSTMs using a validation
set extracted from the training data.

5 Empirical Results

The results for the TF-IDF, RNN, and LSTM mod-
els are shown in Table 4. The models were eval-
uated using both 1 (1 in 2) and 9 (1 in 10) false

We observe that the LSTM outperforms both
the RNN and TF-IDF on all evaluation metrics.
It is interesting to note that TF-IDF actually out-
performs the RNN on the Recall@1 case for the
1 in 10 classiﬁcation. This is most likely due to
the limited ability of the RNN to take into account
long contexts, which can be overcome by using the
LSTM. An example output of the LSTM where the
response is correctly classiﬁed is shown in Table 5.
We also show, in Figure 3, the increase in per-
formance of the LSTM as the amount of data used
for training increases. This conﬁrms the impor-
tance of having a large training set.

Context
""any apache hax around ? i just deleted all of
__path__ - which package provides it ?",
"reconﬁguring apache do n’t solve it ?"

Ranked Responses
1. "does n’t seem to, no"
2. "you can log in but not transfer ﬁles ?"

Flag
1
0

Table 5: Example showing the ranked responses
from the LSTM. Each utterance is shown after pre-
processing steps.

6 Discussion

This paper presents the Ubuntu Dialogue Corpus,
a large dataset for research in unstructured multi-
turn dialogue systems. We describe the construc-
tion of the dataset and its properties. The availabil-
ity of a dataset of this size opens up several inter-
esting possibilities for research into dialogue sys-
tems based on rich neural-network architectures.
We present preliminary results demonstrating use
of this dataset to train an RNN and an LSTM for
the task of selecting the next best response in a

9Note that these results are on the original dataset. Results
on the new dataset should not be compared to the old dataset;
baselines on the new dataset will be released shortly.

6.3 State Tracking and Utterance Generation

The work described here focuses on the task of re-
sponse selection. This can be seen as an interme-
diate step between slot ﬁlling and utterance gener-
ation. In slot ﬁlling, the set of candidate outputs
(states) is identiﬁed a priori through knowledge
engineering, and is typically smaller than the set
of responses considered in our work. When the
set of candidate responses is close to the size of
the dataset (e.g. all utterances ever recorded), then
we are quite close to the response generation case.
There are several reasons not to proceed directly
to response generation. First, it is likely that cur-
rent algorithms are not yet able to generate good
results for this task, and it is preferable to tackle
metrics for which we can make progress. Second,
we do not yet have a suitable metric for evaluat-
ing performance in the response generation case.
One option is to use the BLEU [18] or METEOR
[16] scores from machine translation. However,
using BLEU to evaluate dialogue systems has been
shown to give extremely low scores [28], due to
the large space of potential sensible responses [7].
Further, since the BLEU score is calculated us-
ing N-grams [18], it would provide a very low
score for reasonable responses that do not have
any words in common with the ground-truth next
utterance.

Alternatively, one could measure the difference
between the generated utterance and the actual
sentence by comparing their representations in
some embedding (or semantic) space. However,
different models inevitably use different embed-
dings, necessitating a standardized embedding for
evaluation purposes. Such a standardized embed-
dings has yet to be created.

Another possibility is to use human subjects to
score automatically generated responses, but time
and expense make this a highly impractical option.
In summary, while it is possible that current lan-
guage models have outgrown the use of slot ﬁll-
ing as a metric, we are currently unable to mea-
sure their ability in next utterance generation in
a standardized, meaningful and inexpensive way.
This motivates our choice of response selection as
a useful metric for the time being.

Acknowledgments

The authors gratefully acknowledge ﬁnancial sup-
port for this work by the Samsung Advanced
Institute of Technology (SAIT) and the Natural

Figure 3: The LSTM (with 200 hidden units),
showing Recall@1 for the 1 in 10 classiﬁcation,
with increasing dataset sizes.

conversation; we obtain signiﬁcantly better results
with the LSTM architecture. There are several in-
teresting directions for future work.

6.1 Conversation Disentanglement

Our approach to conversation disentanglement
consists of a small set of rules. More sophisticated
techniques have been proposed, such as training a
maximum-entropy classiﬁer to cluster utterances
into separate dialogues [6]. However, since we
are not trying to replicate the exact conversation
between two users, but only to retrieve plausible
natural dialogues, the heuristic method presented
in this paper may be sufﬁcient. This seems sup-
ported through qualitative examination of the data,
but could be the subject of more formal evaluation.

6.2 Altering Test Set Difﬁculty

One of the interesting properties of the response
selection task is the ability to alter the task dif-
ﬁculty in a controlled manner. We demonstrated
this by moving from 1 to 9 false responses, and
by varying the Recall@k parameter. In the future,
instead of choosing false responses randomly, we
will consider selecting false responses that are
similar to the actual response (e.g. as measured by
cosine similarity). A dialogue model that performs
well on this more difﬁcult task should also manage
to capture a more ﬁne-grained semantic meaning
of sentences, as compared to a model that naively
picks replies with the most words in common with
the context such as TF-IDF.

Sciences and Engineering Research Council of
Canada (NSERC). We would like to thank Lau-
rent Charlin for his input into this paper, as well as
Gabriel Forgues and Eric Crawford for interesting
discussions.

References

[1] Y. Bengio, A. Courville, and P. Vincent.
Representation learning: A review and new
Pattern Analysis and Ma-
perspectives.
chine Intelligence, IEEE Transactions on,
35(8):1798–1828, 2013.

[2] A. Bordes, J. Weston, and N. Usunier. Open
question answering with weakly supervised
embedding models. In MLKDD, pages 165–
180. Springer, 2014.
J. Boyd-Graber, B. Satinoff, H. He, and
H. Daume. Besting the quiz master: Crowd-
sourcing incremental classiﬁcation games. In
EMNLP, 2012.

[3]

[5]

[4] K. Cho, B. van Merrienboer, C. Gulcehre,
F. Bougares, H. Schwenk, and Y. Ben-
gio. Learning phrase representations using
rnn encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078,
2014.
J. Deng, W. Dong, R. Socher, L.J. Li, K. Li,
and L. Fei-Fei. Imagenet: A large-scale hier-
archical image database. In CVPR, 2009.
[6] M. Elsner and E. Charniak. You talking to
me? a corpus and algorithm for conversa-
In ACL, pages 834–
tion disentanglement.
842, 2008.

[7] M. Galley, C. Brockett, A. Sordoni, Y. Ji,
M. Auli, C. Quirk, M. Mitchell, J. Gao, and
B. Dolan. deltableu: A discriminative metric
for generation tasks with intrinsically diverse
arXiv preprint arXiv:1506.06863,
targets.
2015.
J.J. Godfrey, E.C. Holliman, and J. Mc-
Switchboard: Telephone speech
Daniel.
corpus for research and development.
In
ICASSP, 1992.

[8]

[9] M. Henderson, B. Thomson, and J. Williams.
Dialog state tracking challenge 2 & 3, 2014.
[10] M. Henderson, B. Thomson, and J. Williams.
The second dialog state tracking challenge.
In SIGDIAL, page 263, 2014.

[11] M. Henderson, B. Thomson, and S. Young.
Word-based dialog state tracking with recur-

rent neural networks. In SIGDIAL, page 292,
2014.

[12] S. Hochreiter and J. Schmidhuber. Long
short-term memory. Neural computation,
9(8):1735–1780, 1997.

[13] Dialog state tracking challenge 4.
[14] S. Jafarpour, C. Burges, and A. Ritter. Filter,
rank, and transfer the knowledge: Learning
to chat. Advances in Ranking, 10, 2010.

[15] D.P. Kingma and J. Ba.

Adam: A
method for stochastic optimization. CoRR,
abs/1412.6980, 2014.

[16] A. Lavie and M.J. Denkowski. The ME-
TEOR metric for automatic evaluation of
Machine Translation. Machine Translation,
23(2-3):105–115, 2009.

[17] L.R. Medsker and L.C. Jain. Recurrent neu-
ral networks. Design and Applications, 2001.
[18] K. Papineni, S. Roukos, T. Ward, and W.J.
Zhu. Bleu: a method for automatic evalua-
tion of machine translation. In ACL, 2002.

[19] J. Pennington, R. Socher, and C.D. Manning.
GloVe: Global Vectors for Word Representa-
tion. In EMNLP, 2014.

[20] J. Ramos. Using tf-idf to determine word rel-
evance in document queries. In ICML, 2003.
[21] A. Ritter, C. Cherry, and W. Dolan. Unsu-
pervised modeling of twitter conversations.
2010.

[22] A. Ritter, C. Cherry, and W. Dolan. Data-
driven response generation in social media.
In EMNLP, pages 583–593, 2011.

[23] A.M. Saxe, J.L. McClelland, and S. Ganguli.
Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks.
arXiv preprint arXiv:1312.6120, 2013.
[24] J. Schatzmann, K. Georgila, and S. Young.
Quantitative evaluation of user simulation
techniques for spoken dialogue systems. In
SIGDIAL, 2005.

[25] L. Shang, Z. Lu, and H. Li.

Neural
responding machine for short-text conver-
arXiv preprint arXiv:1503.02364,
sation.
2015.

[26] B. A. Shawar and E. Atwell. Chatbots: are
In LDV Forum, vol-

they really useful?
ume 22, pages 29–49, 2007.

[27] S. Singh, D. Litman, M. Kearns, and
M. Walker. Optimizing dialogue manage-
ment with reinforcement learning: Experi-
ments with the NJFun system. Journal of

Artiﬁcial Intelligence Research, 16:105–133,
2002.

[28] A. Sordoni, M. Galley, M. Auli, C. Brock-
ett, Y. Ji, M. Mitchell, J.Y. Nie, J. Gao,
and W. Dolan. A neural network approach
to context-sensitive generation of conversa-
tional responses. 2015.

[29] D.C. Uthus and D.W. Aha. Extending word
highlighting in multiparticipant chat. Tech-
nical report, DTIC Document, 2013.

[30] D.C. Uthus and D.W Aha. The ubuntu chat
corpus for multiparticipant chat analysis. In
AAAI Spring Symposium on Analyzing Mi-
crotext, pages 99–102, 2013.

[31] H. Wang, Z. Lu, H. Li, and E. Chen. A
dataset for research on short-text conversa-
tions. In EMNLP, 2013.

[32] J. Williams, A. Raux, D. Ramachandran, and
A. Black. The dialog state tracking chal-
lenge. In SIGDIAL, pages 404–413, 2013.

[33] L. Yu, K. M. Hermann, P. Blunsom,
Deep learning for an-
arXiv preprint

and S. Pulman.
swer sentence selection.
arXiv:1412.1632, 2014.

[34] M.D. Zeiler.

Adadelta:

an adaptive
arXiv preprint

learning rate method.
arXiv:1212.5701, 2012.

Appendix A: Dialogue excerpts

Figure 4: Example chat room conversation from
the #ubuntu channel of the Ubuntu Chat Logs
(top), with the disentangled conversations for the
Ubuntu Dialogue Corpus (bottom).

Time

03:44

03:45
03:45
03:45

User

Old

kuja
Taru
bur[n]er

03:45

kuja

03:45
03:45
03:45

Taru
LiveCD
kuja

03:46

_pm

03:46
Sender

Old

Taru
Recipient

bur[n]er

Old

kuja
Taru
kuja

Taru
kuja

Taru

Taru
Kuja
Taru

Kuja
Taru

Kuja

Utterance

I dont run graphical ubuntu,
I run ubuntu server.
Taru: Haha sucker.
Kuja: ?
Old: you can use "ps ax"
and "kill (PID#)"
Taru: Anyways, you made
the changes right?
Kuja: Yes.
or killall speedlink
Taru: Then from the terminal
type: sudo apt-get update
if i install the beta version,
how can i update it when
the ﬁnal version comes out?
Kuja: I did.
Utterance

I dont run graphical ubuntu,
I run ubuntu server.
you can use "ps ax" and
"kill (PID#)"

Haha sucker.
?
Anyways, you made the
changes right?
Yes.
Then from the terminal type:
sudo apt-get update
I did.

Time

[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:22]

User

dell
cucho
RC
RC
dell
dell
RC
dell
dell

[12:22]

cucho

Utterance

well, can I move the drives?
dell: ah not like that
dell: you can’t move the drives
dell: deﬁnitely not
ok
lol
this is the problem with RAID:)
RC haha yeah
cucho, I guess I could
just get an enclosure
and copy via USB...
dell: i would advise you to get
the disk

Sender

Recipient

Utterance

dell
cucho
dell

dell
cucho

cucho

dell

dell
RC

dell

dell

RC

well, can I move the drives?
ah not like that
I guess I could just get an
enclosure and copy via USB
i would advise you to get the
disk

well, can I move the drives?
you can’t move the drives.
deﬁnitely not. this is
the problem with RAID :)
haha yeah

Figure 5: Example of before (top box) and after
(bottom box) the algorithm adds and concatenates
utterances in dialogue extraction. Since RC only
addresses dell, all of his utterances are added,
however this is not done for dell as he addresses
both RC and cucho.

The Ubuntu Dialogue Corpus: A Large Dataset for Research in
Unstructured Multi-Turn Dialogue Systems

Ryan Lowe∗*, Nissan Pow*, Iulian V. Serban† and Joelle Pineau*

*School of Computer Science, McGill University, Montreal, Canada
†Department of Computer Science and Operations Research, Universié de Montréal, Montreal, Canada

6
1
0
2
 
b
e
F
 
4
 
 
]
L
C
.
s
c
[
 
 
3
v
9
0
9
8
0
.
6
0
5
1
:
v
i
X
r
a

Abstract

This paper introduces the Ubuntu Dia-
logue Corpus, a dataset containing almost
1 million multi-turn dialogues, with a to-
tal of over 7 million utterances and 100
million words. This provides a unique re-
source for research into building dialogue
managers based on neural language mod-
els that can make use of large amounts
of unlabeled data. The dataset has both
the multi-turn property of conversations
in the Dialog State Tracking Challenge
datasets, and the unstructured nature of in-
teractions from microblog services such
as Twitter. We also describe two neural
learning architectures suitable for analyz-
ing this dataset, and provide benchmark
performance on the task of selecting the
best next response.

1

Introduction

The ability for a computer to converse in a nat-
ural and coherent manner with a human has long
been held as one of the primary objectives of artiﬁ-
cial intelligence (AI). In this paper we consider the
problem of building dialogue agents that have the
ability to interact in one-on-one multi-turn con-
versations on a diverse set of topics. We primar-
ily target unstructured dialogues, where there is
no a priori logical representation for the informa-
tion exchanged during the conversation. This is in
contrast to recent systems which focus on struc-
tured dialogue tasks, using a slot-ﬁlling represen-
tation [10, 27, 32].

We observe that in several subﬁelds of AI—
computer vision, speech recognition, machine
translation—fundamental break-throughs were
achieved in recent years using machine learning

methods, more speciﬁcally with neural architec-
tures [1]; however, it is worth noting that many
of the most successful approaches, in particular
convolutional and recurrent neural networks, were
known for many years prior.
It is therefore rea-
sonable to attribute this progress to three major
factors: 1) the public distribution of very large
rich datasets [5], 2) the availability of substantial
computing power, and 3) the development of new
training methods for neural architectures, in par-
ticular leveraging unlabeled data. Similar progress
has not yet been observed in the development of
dialogue systems. We hypothesize that this is due
to the lack of sufﬁciently large datasets, and aim
to overcome this barrier by providing a new large
corpus for research in multi-turn conversation.

The new Ubuntu Dialogue Corpus consists of
almost one million two-person conversations ex-
tracted from the Ubuntu chat logs1, used to receive
technical support for various Ubuntu-related prob-
lems. The conversations have an average of 8 turns
each, with a minimum of 3 turns. All conversa-
tions are carried out in text form (not audio). The
dataset is orders of magnitude larger than struc-
tured corpuses such as those of the Dialogue State
Tracking Challenge [32]. It is on the same scale as
recent datasets for solving problems such as ques-
tion answering and analysis of microblog services,
such as Twitter [22, 25, 28, 33], but each conversa-
tion in our dataset includes several more turns, as
well as longer utterances. Furthermore, because
it targets a speciﬁc domain, namely technical sup-
port, it can be used as a case study for the devel-
opment of AI agents in targeted applications, in
contrast to chatbox agents that often lack a well-
deﬁned goal [26].

In addition to the corpus, we present learning
architectures suitable for analyzing this dataset,
ranging from the simple frequency-inverse docu-

1These logs are available from 2004 to 2015 at http:

∗The ﬁrst two authors contributed equally.

//irclogs.ubuntu.com/

ment frequency (TF-IDF) approach, to more so-
phisticated neural models including a Recurrent
Neural Network (RNN) and a Long Short-Term
Memory (LSTM) architecture. We provide bench-
trained
mark performance of these algorithms,
with our new corpus, on the task of selecting the
best next response, which can be achieved with-
out requiring any human labeling. The dataset is
ready for public release2. The code developed for
the empirical results is also available3.

2 Related Work

We brieﬂy review existing dialogue datasets, and
some of the more recent learning architectures
used for both structured and unstructured dia-
logues. This is by no means an exhaustive list
(due to space constraints), but surveys resources
most related to our contribution. A list of datasets
discussed is provided in Table 1.

2.1 Dialogue Datasets

The Switchboard dataset [8], and the Dialogue
State Tracking Challenge (DSTC) datasets [32]
have been used to train and validate dialogue man-
agement systems for interactive information re-
trieval. The problem is typically formalized as a
slot ﬁlling task, where agents attempt to predict
the goal of a user during the conversation. These
datasets have been signiﬁcant resources for struc-
tured dialogues, and have allowed major progress
in this ﬁeld, though they are quite small compared
to datasets currently used for training neural archi-
tectures.

Recently, a few datasets have been used con-
taining unstructured dialogues extracted from
Twitter4. Ritter et al. [21] collected 1.3 million
conversations; this was extended in [28] to take ad-
vantage of longer contexts by using A-B-A triples.
Shang et al. [25] used data from a similar Chinese
website called Weibo5. However to our knowl-
edge, these datasets have not been made public,
and furthermore, the post-reply format of such mi-
croblogging services is perhaps not as represen-
tative of natural dialogue between humans as the
continuous stream of messages in a chat room. In

2Note that a new version of

is now
https://github.com/rkadlec/
available:
ubuntu-ranking-dataset-creator.
This ver-
sion makes some adjustments and ﬁxes some bugs from the
ﬁrst version.

the dataset

3http://github.com/npow/ubottu
4https://twitter.com/
5http://www.weibo.com/

fact, Ritter et al. estimate that only 37% of posts
on Twitter are ‘conversational in nature’, and 69%
of their collected data contained exchanges of only
length 2 [21]. We hypothesize that chat-room style
messaging is more closely correlated to human-to-
human dialogue than micro-blogging websites, or
forum-based sites such as Reddit.

Part of the Ubuntu chat logs have previously
been aggregated into a dataset, called the Ubuntu
Chat Corpus [30]. However that resource pre-
serves the multi-participant structure and thus is
less amenable to the investigation of more tradi-
tional two-party conversations.

Also weakly related to our contribution is the
problem of question-answer systems.
Several
datasets of question-answer pairs are available [3],
however these interactions are much shorter than
what we seek to study.

2.2 Learning Architectures

Most dialogue research has historically focused
on structured slot-ﬁlling tasks [24]. Various ap-
proaches were proposed, yet few attempts lever-
age more recent developments in neural learning
architectures. A notable exception is the work of
Henderson et al. [11], which proposes an RNN
structure, initialized with a denoising autoencoder,
to tackle the DSTC 3 domain.

Work on unstructured dialogues, recently pi-
oneered by Ritter et al. [22], proposed a re-
sponse generation model for Twitter data based on
ideas from Statistical Machine Translation. This
is shown to give superior performance to previ-
ous information retrieval (e.g. nearest neighbour)
approaches [14]. This idea was further devel-
oped by Sordoni et al. [28] to exploit information
from a longer context, using a structure similar to
the Recurrent Neural Network Encoder-Decoder
model [4]. This achieves rather poor performance
on A-B-A Twitter triples when measured by the
BLEU score (a standard for machine translation),
yet performs comparatively better than the model
of Ritter et al. [22]. Their results are also veriﬁed
with a human-subject study. A similar encoder-
decoder framework is presented in [25]. This
model uses one RNN to transform the input to
some vector representation, and another RNN to
‘decode’ this representation to a response by gen-
erating one word at a time. This model is also eval-
uated in a human-subject study, although much
smaller in size than in [28]. Overall, these models

Dataset

Switchboard [8]

DSTC1 [32]

DSTC2 [10]

DSTC3 [9]

DSTC4[13]

Twitter
Corpus [21]
Twitter Triple
Corpus [28]
Sina Weibo [25]

Ubuntu Dialogue
Corpus

Type

Human-human
spoken
Human-computer
spoken
Human-computer
spoken
Human-computer
spoken
Human-human
spoken
Human-human
micro-blog
Human-human
micro-blog
Human-human
micro-blog
Human-human
chat

Task

Various

State
tracking
State
tracking
State
tracking
State
tracking
Next utterance
generation
Next utterance
generation
Next utterance
generation
Next utterance
classiﬁcation

# Dialogues

# Utterances

# Words

Description

2,400

—

3,000,000

15,000

210,000

3,000

2,265

35

24,000

15,000

—

1,300,000

3,000,000

29,000,000

87,000,000

4,435,959

8,871,918

—

—

—

—

—

—

930,000

7,100,000

100,000,000

Telephone conversations
on pre-speciﬁed topics
Bus ride information
system
Restaurant booking
system
Tourist information
system
21 hours of tourist info
exchange over Skype
Post/ replies extracted
from Twitter
A-B-A triples from
Twitter replies
Post/ reply pairs extracted
from Weibo
Extracted from Ubuntu
Chat Logs

Table 1: A selection of structured and unstructured large-scale datasets applicable to dialogue systems.
Faded datasets are not publicly available. The last entry is our contribution.

highlight the potential of neural learning architec-
tures for interactive systems, yet so far they have
been limited to very short conversations.

3 The Ubuntu Dialogue Corpus

We seek a large dataset for research in dialogue
systems with the following properties:

• Two-way (or dyadic) conversation, as op-
posed to multi-participant chat, preferably
human-human.

• Large number of conversations; 105 − 106
is typical of datasets used for neural-network
learning in other areas of AI.

• Many conversations with several turns (more

• Task-speciﬁc domain, as opposed to chatbot

than 3).

systems.

All of these requirements are satisﬁed by the
Ubuntu Dialogue Corpus presented in this paper.

3.1 Ubuntu Chat Logs

The Ubuntu Chat Logs refer to a collection of logs
from Ubuntu-related chat rooms on the Freenode
Internet Relay Chat (IRC) network. This protocol
allows for real-time chat between a large number
of participants. Each chat room, or channel, has
a particular topic, and every channel participant
can see all the messages posted in a given chan-
nel. Many of these channels are used for obtaining
technical support with various Ubuntu issues.

As the contents of each channel are moderated,
most interactions follow a similar pattern. A new
user joins the channel, and asks a general ques-
tion about a problem they are having with Ubuntu.
Then, another more experienced user replies with

a potential solution, after ﬁrst addressing the ’user-
name’ of the ﬁrst user. This is called a name men-
tion [29], and is done to avoid confusion in the
channel — at any given time during the day, there
can be between 1 and 20 simultaneous conversa-
tions happening in some channels.
In the most
popular channels, there is almost never a time
when only one conversation is occurring; this ren-
ders it particularly problematic to extract dyadic
dialogues. A conversation between a pair of users
generally stops when the problem has been solved,
though some users occasionally continue to dis-
cuss a topic not related to Ubuntu.

Despite the nature of the chat room being a con-
stant stream of messages from multiple users, it is
through the fairly rigid structure in the messages
that we can extract the dialogues between users.
Figure 4 shows an example chat room conversa-
tion from the #ubuntu channel as well as the ex-
tracted dialogues, which illustrates how users usu-
ally state the username of the intended message
recipient before writing their reply (we refer to all
replies and initial questions as ‘utterances’). For
example, it is clear that users ‘Taru’ and ‘kuja’
are engaged in a dialogue, as are users ‘Old’ and
‘bur[n]er’, while user ‘_pm’ is asking an initial
question, and ‘LiveCD’ is perhaps elaborating on
a previous comment.

3.2 Dataset Creation

In order to create the Ubuntu Dialogue Corpus,
ﬁrst a method had to be devised to extract dyadic
dialogues from the chat room multi-party conver-
sations. The ﬁrst step was to separate every mes-
sage into 4-tuples of (time, sender, recipient, utter-
ance). Given these 4-tuples, it is straightforward to

group all tuples where there is a matching sender
and recipient. Although it is easy to separate the
time and the sender from the rest, ﬁnding the in-
tended recipient of the message is not always triv-
ial.

3.2.1 Recipient Identiﬁcation
While in most cases the recipient is the ﬁrst word
of the utterance, it is sometimes located at the end,
or not at all in the case of initial questions. Fur-
thermore, some users choose names correspond-
ing to common English words, such as ‘the’ or
‘stop’, which could lead to many false positives.
In order to solve this issue, we create a dictionary
of usernames from the current and previous days,
and compare the ﬁrst word of each utterance to its
If a match is found, and the word does
entries.
not correspond to a very common English word6,
it is assumed that this user was the intended recip-
ient of the message. If no matches are found, it is
assumed that the message was an initial question,
and the recipient value is left empty.

3.2.2 Utterance Creation
The dialogue extraction algorithm works back-
wards from the ﬁrst response to ﬁnd the initial
question that was replied to, within a time frame
of 3 minutes. A ﬁrst response is identiﬁed by the
presence of a recipient name (someone from the
recent conversation history). The initial question
is identiﬁed to be the most recent utterance by the
recipient identiﬁed in the ﬁrst response.

All utterances that do not qualify as a ﬁrst re-
sponse or an initial question are discarded; initial
questions that do not generate any response are
also discarded. We additionally discard conversa-
tions longer than ﬁve utterances where one user
says more than 80% of the utterances, as these are
typically not representative of real chat dialogues.
Finally, we consider only extracted dialogues that
consist of 3 turns or more to encourage the model-
ing of longer-term dependencies.

To alleviate the problem of ‘holes’ in the dia-
logue, where one user does not address the other
explicitly, as in Figure 5, we check whether each
user talks to someone else for the duration of their
conversation. If not, all non-addressed utterances
are added to the dialogue. An example conversa-
tion along with the extracted dialogues is shown
in Figure 5. Note that we also concatenate all con-
secutive utterances from a given user.

6We use the GNU Aspell spell checking dictionary.

Figure 1: Plot of number of conversations with a
given number of turns. Both axes use a log scale.

# dialogues (human-human)
# utterances (in total)
# words (in total)
Min. # turns per dialogue
Avg. # turns per dialogue
Avg. # words per utterance
Median conversation length (min)

930,000
7,100,000
100,000,000
3
7.71
10.34
6

Table 2: Properties of Ubuntu Dialogue Corpus.

We do not apply any further pre-processing (e.g.
tokenization, stemming) to the data as released in
the Ubuntu Dialogue Corpus. However the use of
pre-processing is standard for most NLP systems,
and was also used in our analysis (see Section 4.)

3.2.3 Special Cases and Limitations

It is often the case that a user will post an ini-
tial question, and multiple people will respond to
it with different answers.
In this instance, each
conversation between the ﬁrst user and the user
who replied is treated as a separate dialogue. This
has the unfortunate side-effect of having the ini-
tial question appear multiple times in several dia-
logues. However the number of such cases is suf-
ﬁciently small compared to the size of the dataset.
Another issue to note is that the utterance post-
ing time is not considered for segmenting conver-
sations between two users. Even if two users have
a conversation that spans multiple hours, or even
days, this is treated as a single dialogue. However,
such dialogues are rare. We include the posting
time in the corpus so that other researchers may
ﬁlter as desired.

3.3 Dataset Statistics

Table 2 summarizes properties of the Ubuntu Dia-
logue Corpus. One of the most important features

of the Ubuntu chat logs is its size. This is cru-
cial for research into building dialogue managers
based on neural architectures. Another important
characteristic is the number of turns in these dia-
logues. The distribution of the number of turns is
shown in Figure 1. It can be seen that the num-
ber of dialogues and turns per dialogue follow an
approximate power law relationship.

3.4 Test Set Generation

We set aside 2% of the Ubuntu Dialogue Corpus
conversations (randomly selected) to form a test
set that can be used for evaluation of response se-
lection algorithms. Compared to the rest of the
corpus, this test set has been further processed to
extract a pair of (context, response, ﬂag) triples
from each dialogue. The ﬂag is a Boolean vari-
able indicating whether or not the response was the
actual next utterance after the given context. The
response is a target (output) utterance which we
aim to correctly identify. The context consists of
the sequence of utterances appearing in dialogue
prior to the response. We create a pair of triples,
where one triple contains the correct response (i.e.
the actual next utterance in the dialogue), and the
other triple contains a false response, sampled ran-
domly from elsewhere within the test set. The ﬂag
is set to 1 in the ﬁrst case and to 0 in the second
case. An example pair is shown in Table 3. To
make the task harder, we can move from pairs of
responses (one correct, one incorrect) to a larger
set of wrong responses (all with ﬂag=0). In our
experiments below, we consider both the case of 1
wrong response and 10 wrong responses.

Context
well, can I move the drives?
__EOS__ ah not like that

well, can I move the drives?
__EOS__ ah not like that

Response
I guess I could just
get an enclosure and
copy via USB
you can use "ps ax"
and "kill (PID #)"

Flag
1

0

Table 3: Test set example with (context, reply,
ﬂag) format. The ’__EOS__’ tag is used to denote
the end of an utterance within the context.

Since we want to learn to predict all parts of a
conversation, as opposed to only the closing state-
ment, we consider various portions of context for
the conversations in the test set. The context size is
determined stochastically using a simple formula:

c = min(t − 1, n − 1),

Here, C denotes the maximum desired context
size, which we set to C = 20. The last term is
the desired minimum context size, which we set
to be 2. Parameter t is the actual length of that
dialogue (thus the constraint that c ≤ t − 1), and
n is a random number corresponding to the ran-
domly sampled context length, that is selected to
be inversely proportional to C.

In practice, this leads to short test dialogues
having short contexts, while longer dialogues are
often broken into short or medium-length seg-
ments, with the occasional long context of 10 or
more turns.

3.5 Evaluation Metric

We consider the task of best response selection.
This can be achieved by processing the data as de-
scribed in Section 3.4, without requiring any hu-
man labels. This classiﬁcation task is an adapta-
tion of the recall and precision metrics previously
applied to dialogue datasets [24].

A family of metrics often used in language tasks
is Recall@k (denoted R@1 R@2, R@5 below).
Here the agent is asked to select the k most likely
responses, and it is correct if the true response is
among these k candidates. Only the R@1 metric
is relevant in the case of binary classiﬁcation (as
in the Table 3 example).

Although a language model that performs well
on response classiﬁcation is not a gauge of good
performance on next utterance generation, we hy-
pothesize that improvements on a model with re-
gards to the classiﬁcation task will eventually lead
to improvements for the generation task. See Sec-
tion 6 for further discussion of this point.

4 Learning Architectures for
Unstructured Dialogues

To provide further evidence of the value of
our dataset for research into neural architectures
for dialogue managers, we provide performance
benchmarks for two neural learning algorithms, as
well as one naive baseline. The approaches con-
sidered are: TF-IDF, Recurrent Neural networks
(RNN), and Long Short-Term Memory (LSTM).
Prior to applying each method, we perform stan-
dard pre-processing of the data using the NLTK7
library and Twitter tokenizer8 to parse each utter-
ance. We use generic tags for various word cat-

where n =

+ 2, η ∼ U nif (C/2, 10C)

10C
η

7www.nltk.org/
8http://www.ark.cs.cmu.edu/TweetNLP/

egories, such as names, locations, organizations,
URLs, and system paths.

To train the RNN and LSTM architectures, we
process the full training Ubuntu Dialogue Corpus
into the same format as the test set described in
Section 3.4, extracting (context, response, ﬂag)
triples from dialogues. For the training set, we
do not sample the context length, but instead con-
sider each utterance (starting at the 3rd one) as a
potential response, with the previous utterances as
its context. So a dialogue of length 10 yields 8
training examples. Since these are overlapping,
they are clearly not independent, but we consider
this a minor issue given the size of the dataset (we
further alleviate the issue by shufﬂing the training
examples). Negative responses are selected at ran-
dom from the rest of the training data.

4.1 TF-IDF

Term frequency-inverse document frequency is a
statistic that intends to capture how important a
given word is to some document, which in our case
is the context [20]. It is a technique often used in
document classiﬁcation and information retrieval.
The ‘term-frequency’ term is simply a count of the
number of times a word appears in a given context,
while the ‘inverse document frequency’ term puts
a penalty on how often the word appears elsewhere
in the corpus. The ﬁnal score is calculated as the
product of these two terms, and has the form:

tﬁdf(w, d, D) = f (w, d)×log

N
|{d ∈ D : w ∈ d}|

,

where f (w, d) indicates the number of times word
w appeared in context d, N is the total number
of dialogues, and the denominator represents the
number of dialogues in which the word w appears.
For classiﬁcation, the TF-IDF vectors are ﬁrst
calculated for the context and each of the candi-
date responses. Given a set of candidate response
vectors, the one with the highest cosine similarity
to the context vector is selected as the output. For
Recall@k, the top k responses are returned.

4.2 RNN

Recurrent neural networks are a variant of neural
networks that allows for time-delayed directed cy-
cles between units [17]. This leads to the forma-
tion of an internal state of the network, ht, which
allows it to model time-dependent data. The in-
ternal state is updated at each time step as some

Figure 2: Diagram of our model. The RNNs have
tied weights. c, r are the last hidden states from
the RNNs. ci, ri are word vectors for the context
and response, i < t. We consider contexts up to a
maximum of t = 160.

function of the observed variables xt, and the hid-
den state at the previous time step ht−1. Wx and
Wh are matrices associated with the input and hid-
den state.

ht = f (Whht−1 + Wxxt).

A diagram of an RNN can be seen in Figure 2.
RNNs have been the primary building block of
many current neural language models [22, 28],
which use RNNs for an encoder and decoder. The
ﬁrst RNN is used to encode the given context,
and the second RNN generates a response by us-
ing beam-search, where its initial hidden state is
biased using the ﬁnal hidden state from the ﬁrst
RNN. In our work, we are concerned with classi-
ﬁcation of responses, instead of generation. We
build upon the approach in [2], which has also
been recently applied to the problem of question
answering [33].

We utilize a siamese network consisting of two
RNNs with tied weights to produce the embed-
dings for the context and response. Given some
input context and response, we compute their em-
beddings — c, r ∈ Rd, respectively — by feeding
the word embeddings one at a time into its respec-
tive RNN. Word embeddings are initialized using
the pre-trained vectors (Common Crawl, 840B to-
kens from [19]), and ﬁne-tuned during training.
The hidden state of the RNN is updated at each
step, and the ﬁnal hidden state represents a sum-
mary of the input utterance. Using the ﬁnal hid-
den states from both RNNs, we then calculate the
probability that this is a valid pair:

p(ﬂag = 1|c, r, M ) = σ(cT M r + b),

where the bias b and the matrix M ∈ Rd×d are
learned model parameters. This can be thought
of as a generative approach; given some input re-
sponse, we generate a context with the product
c(cid:48) = M r, and measure the similarity to the actual
context using the dot product. This is converted
to a probability with the sigmoid function. The
model is trained by minimizing the cross entropy
of all labeled (context, response) pairs [33]:

examples. Of course, the Recall@2 and Recall@5
are not relevant in the binary classiﬁcation case9.

Method
1 in 2 R@1
1 in 10 R@1
1 in 10 R@2
1 in 10 R@5

TF-IDF
RNN LSTM
65.9% 76.8% 87.8%
41.0% 40.3% 60.4%
54.5% 54.7% 74.5%
70.8% 81.9% 92.6%

L = −

log p(ﬂagn|cn, rn, M ) +

(cid:88)

n

λ
2

||θ = ||2
F

Table 4: Results for the three algorithms using var-
ious recall measures for binary (1 in 2) and 1 in 10
(1 in 10) next utterance classiﬁcation %.

where ||θ||2
F is the Frobenius norm of θ = {M, b}.
In our experiments, we use λ = 0 for computa-
tional simplicity.

For training, we used a 1:1 ratio between true re-
sponses (ﬂag = 1), and negative responses (ﬂag=0)
drawn randomly from elsewhere in the training
set. The RNN architecture is set to 1 hidden layer
with 50 neurons. The Wh matrix is initialized us-
ing orthogonal weights [23], while Wx is initial-
ized using a uniform distribution with values be-
tween -0.01 and 0.01. We use Adam as our opti-
mizer [15], with gradients clipped to 10. We found
that weight initialization as well as the choice of
optimizer were critical for training the RNNs.

4.3 LSTM

In addition to the RNN model, we consider the
same architecture but changed the hidden units
to long-short term memory (LSTM) units [12].
LSTMs were introduced in order to model longer-
term dependencies. This is accomplished using a
series of gates that determine whether a new in-
put should be remembered, forgotten (and the old
value retained), or used as output. The error sig-
nal can now be fed back indeﬁnitely into the gates
of the LSTM unit. This helps overcome the van-
ishing and exploding gradient problems in stan-
dard RNNs, where the error gradients would oth-
erwise decrease or increase at an exponential rate.
In training, we used 1 hidden layer with 200 neu-
rons. The hyper-parameter conﬁguration (includ-
ing number of neurons) was optimized indepen-
dently for RNNs and LSTMs using a validation
set extracted from the training data.

5 Empirical Results

The results for the TF-IDF, RNN, and LSTM mod-
els are shown in Table 4. The models were eval-
uated using both 1 (1 in 2) and 9 (1 in 10) false

We observe that the LSTM outperforms both
the RNN and TF-IDF on all evaluation metrics.
It is interesting to note that TF-IDF actually out-
performs the RNN on the Recall@1 case for the
1 in 10 classiﬁcation. This is most likely due to
the limited ability of the RNN to take into account
long contexts, which can be overcome by using the
LSTM. An example output of the LSTM where the
response is correctly classiﬁed is shown in Table 5.
We also show, in Figure 3, the increase in per-
formance of the LSTM as the amount of data used
for training increases. This conﬁrms the impor-
tance of having a large training set.

Context
""any apache hax around ? i just deleted all of
__path__ - which package provides it ?",
"reconﬁguring apache do n’t solve it ?"

Ranked Responses
1. "does n’t seem to, no"
2. "you can log in but not transfer ﬁles ?"

Flag
1
0

Table 5: Example showing the ranked responses
from the LSTM. Each utterance is shown after pre-
processing steps.

6 Discussion

This paper presents the Ubuntu Dialogue Corpus,
a large dataset for research in unstructured multi-
turn dialogue systems. We describe the construc-
tion of the dataset and its properties. The availabil-
ity of a dataset of this size opens up several inter-
esting possibilities for research into dialogue sys-
tems based on rich neural-network architectures.
We present preliminary results demonstrating use
of this dataset to train an RNN and an LSTM for
the task of selecting the next best response in a

9Note that these results are on the original dataset. Results
on the new dataset should not be compared to the old dataset;
baselines on the new dataset will be released shortly.

6.3 State Tracking and Utterance Generation

The work described here focuses on the task of re-
sponse selection. This can be seen as an interme-
diate step between slot ﬁlling and utterance gener-
ation. In slot ﬁlling, the set of candidate outputs
(states) is identiﬁed a priori through knowledge
engineering, and is typically smaller than the set
of responses considered in our work. When the
set of candidate responses is close to the size of
the dataset (e.g. all utterances ever recorded), then
we are quite close to the response generation case.
There are several reasons not to proceed directly
to response generation. First, it is likely that cur-
rent algorithms are not yet able to generate good
results for this task, and it is preferable to tackle
metrics for which we can make progress. Second,
we do not yet have a suitable metric for evaluat-
ing performance in the response generation case.
One option is to use the BLEU [18] or METEOR
[16] scores from machine translation. However,
using BLEU to evaluate dialogue systems has been
shown to give extremely low scores [28], due to
the large space of potential sensible responses [7].
Further, since the BLEU score is calculated us-
ing N-grams [18], it would provide a very low
score for reasonable responses that do not have
any words in common with the ground-truth next
utterance.

Alternatively, one could measure the difference
between the generated utterance and the actual
sentence by comparing their representations in
some embedding (or semantic) space. However,
different models inevitably use different embed-
dings, necessitating a standardized embedding for
evaluation purposes. Such a standardized embed-
dings has yet to be created.

Another possibility is to use human subjects to
score automatically generated responses, but time
and expense make this a highly impractical option.
In summary, while it is possible that current lan-
guage models have outgrown the use of slot ﬁll-
ing as a metric, we are currently unable to mea-
sure their ability in next utterance generation in
a standardized, meaningful and inexpensive way.
This motivates our choice of response selection as
a useful metric for the time being.

Acknowledgments

The authors gratefully acknowledge ﬁnancial sup-
port for this work by the Samsung Advanced
Institute of Technology (SAIT) and the Natural

Figure 3: The LSTM (with 200 hidden units),
showing Recall@1 for the 1 in 10 classiﬁcation,
with increasing dataset sizes.

conversation; we obtain signiﬁcantly better results
with the LSTM architecture. There are several in-
teresting directions for future work.

6.1 Conversation Disentanglement

Our approach to conversation disentanglement
consists of a small set of rules. More sophisticated
techniques have been proposed, such as training a
maximum-entropy classiﬁer to cluster utterances
into separate dialogues [6]. However, since we
are not trying to replicate the exact conversation
between two users, but only to retrieve plausible
natural dialogues, the heuristic method presented
in this paper may be sufﬁcient. This seems sup-
ported through qualitative examination of the data,
but could be the subject of more formal evaluation.

6.2 Altering Test Set Difﬁculty

One of the interesting properties of the response
selection task is the ability to alter the task dif-
ﬁculty in a controlled manner. We demonstrated
this by moving from 1 to 9 false responses, and
by varying the Recall@k parameter. In the future,
instead of choosing false responses randomly, we
will consider selecting false responses that are
similar to the actual response (e.g. as measured by
cosine similarity). A dialogue model that performs
well on this more difﬁcult task should also manage
to capture a more ﬁne-grained semantic meaning
of sentences, as compared to a model that naively
picks replies with the most words in common with
the context such as TF-IDF.

Sciences and Engineering Research Council of
Canada (NSERC). We would like to thank Lau-
rent Charlin for his input into this paper, as well as
Gabriel Forgues and Eric Crawford for interesting
discussions.

References

[1] Y. Bengio, A. Courville, and P. Vincent.
Representation learning: A review and new
Pattern Analysis and Ma-
perspectives.
chine Intelligence, IEEE Transactions on,
35(8):1798–1828, 2013.

[2] A. Bordes, J. Weston, and N. Usunier. Open
question answering with weakly supervised
embedding models. In MLKDD, pages 165–
180. Springer, 2014.
J. Boyd-Graber, B. Satinoff, H. He, and
H. Daume. Besting the quiz master: Crowd-
sourcing incremental classiﬁcation games. In
EMNLP, 2012.

[3]

[5]

[4] K. Cho, B. van Merrienboer, C. Gulcehre,
F. Bougares, H. Schwenk, and Y. Ben-
gio. Learning phrase representations using
rnn encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078,
2014.
J. Deng, W. Dong, R. Socher, L.J. Li, K. Li,
and L. Fei-Fei. Imagenet: A large-scale hier-
archical image database. In CVPR, 2009.
[6] M. Elsner and E. Charniak. You talking to
me? a corpus and algorithm for conversa-
In ACL, pages 834–
tion disentanglement.
842, 2008.

[7] M. Galley, C. Brockett, A. Sordoni, Y. Ji,
M. Auli, C. Quirk, M. Mitchell, J. Gao, and
B. Dolan. deltableu: A discriminative metric
for generation tasks with intrinsically diverse
arXiv preprint arXiv:1506.06863,
targets.
2015.
J.J. Godfrey, E.C. Holliman, and J. Mc-
Switchboard: Telephone speech
Daniel.
corpus for research and development.
In
ICASSP, 1992.

[8]

[9] M. Henderson, B. Thomson, and J. Williams.
Dialog state tracking challenge 2 & 3, 2014.
[10] M. Henderson, B. Thomson, and J. Williams.
The second dialog state tracking challenge.
In SIGDIAL, page 263, 2014.

[11] M. Henderson, B. Thomson, and S. Young.
Word-based dialog state tracking with recur-

rent neural networks. In SIGDIAL, page 292,
2014.

[12] S. Hochreiter and J. Schmidhuber. Long
short-term memory. Neural computation,
9(8):1735–1780, 1997.

[13] Dialog state tracking challenge 4.
[14] S. Jafarpour, C. Burges, and A. Ritter. Filter,
rank, and transfer the knowledge: Learning
to chat. Advances in Ranking, 10, 2010.

[15] D.P. Kingma and J. Ba.

Adam: A
method for stochastic optimization. CoRR,
abs/1412.6980, 2014.

[16] A. Lavie and M.J. Denkowski. The ME-
TEOR metric for automatic evaluation of
Machine Translation. Machine Translation,
23(2-3):105–115, 2009.

[17] L.R. Medsker and L.C. Jain. Recurrent neu-
ral networks. Design and Applications, 2001.
[18] K. Papineni, S. Roukos, T. Ward, and W.J.
Zhu. Bleu: a method for automatic evalua-
tion of machine translation. In ACL, 2002.

[19] J. Pennington, R. Socher, and C.D. Manning.
GloVe: Global Vectors for Word Representa-
tion. In EMNLP, 2014.

[20] J. Ramos. Using tf-idf to determine word rel-
evance in document queries. In ICML, 2003.
[21] A. Ritter, C. Cherry, and W. Dolan. Unsu-
pervised modeling of twitter conversations.
2010.

[22] A. Ritter, C. Cherry, and W. Dolan. Data-
driven response generation in social media.
In EMNLP, pages 583–593, 2011.

[23] A.M. Saxe, J.L. McClelland, and S. Ganguli.
Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks.
arXiv preprint arXiv:1312.6120, 2013.
[24] J. Schatzmann, K. Georgila, and S. Young.
Quantitative evaluation of user simulation
techniques for spoken dialogue systems. In
SIGDIAL, 2005.

[25] L. Shang, Z. Lu, and H. Li.

Neural
responding machine for short-text conver-
arXiv preprint arXiv:1503.02364,
sation.
2015.

[26] B. A. Shawar and E. Atwell. Chatbots: are
In LDV Forum, vol-

they really useful?
ume 22, pages 29–49, 2007.

[27] S. Singh, D. Litman, M. Kearns, and
M. Walker. Optimizing dialogue manage-
ment with reinforcement learning: Experi-
ments with the NJFun system. Journal of

Artiﬁcial Intelligence Research, 16:105–133,
2002.

[28] A. Sordoni, M. Galley, M. Auli, C. Brock-
ett, Y. Ji, M. Mitchell, J.Y. Nie, J. Gao,
and W. Dolan. A neural network approach
to context-sensitive generation of conversa-
tional responses. 2015.

[29] D.C. Uthus and D.W. Aha. Extending word
highlighting in multiparticipant chat. Tech-
nical report, DTIC Document, 2013.

[30] D.C. Uthus and D.W Aha. The ubuntu chat
corpus for multiparticipant chat analysis. In
AAAI Spring Symposium on Analyzing Mi-
crotext, pages 99–102, 2013.

[31] H. Wang, Z. Lu, H. Li, and E. Chen. A
dataset for research on short-text conversa-
tions. In EMNLP, 2013.

[32] J. Williams, A. Raux, D. Ramachandran, and
A. Black. The dialog state tracking chal-
lenge. In SIGDIAL, pages 404–413, 2013.

[33] L. Yu, K. M. Hermann, P. Blunsom,
Deep learning for an-
arXiv preprint

and S. Pulman.
swer sentence selection.
arXiv:1412.1632, 2014.

[34] M.D. Zeiler.

Adadelta:

an adaptive
arXiv preprint

learning rate method.
arXiv:1212.5701, 2012.

Appendix A: Dialogue excerpts

Figure 4: Example chat room conversation from
the #ubuntu channel of the Ubuntu Chat Logs
(top), with the disentangled conversations for the
Ubuntu Dialogue Corpus (bottom).

Time

03:44

03:45
03:45
03:45

User

Old

kuja
Taru
bur[n]er

03:45

kuja

03:45
03:45
03:45

Taru
LiveCD
kuja

03:46

_pm

03:46
Sender

Old

Taru
Recipient

bur[n]er

Old

kuja
Taru
kuja

Taru
kuja

Taru

Taru
Kuja
Taru

Kuja
Taru

Kuja

Utterance

I dont run graphical ubuntu,
I run ubuntu server.
Taru: Haha sucker.
Kuja: ?
Old: you can use "ps ax"
and "kill (PID#)"
Taru: Anyways, you made
the changes right?
Kuja: Yes.
or killall speedlink
Taru: Then from the terminal
type: sudo apt-get update
if i install the beta version,
how can i update it when
the ﬁnal version comes out?
Kuja: I did.
Utterance

I dont run graphical ubuntu,
I run ubuntu server.
you can use "ps ax" and
"kill (PID#)"

Haha sucker.
?
Anyways, you made the
changes right?
Yes.
Then from the terminal type:
sudo apt-get update
I did.

Time

[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:22]

User

dell
cucho
RC
RC
dell
dell
RC
dell
dell

[12:22]

cucho

Utterance

well, can I move the drives?
dell: ah not like that
dell: you can’t move the drives
dell: deﬁnitely not
ok
lol
this is the problem with RAID:)
RC haha yeah
cucho, I guess I could
just get an enclosure
and copy via USB...
dell: i would advise you to get
the disk

Sender

Recipient

Utterance

dell
cucho
dell

dell
cucho

cucho

dell

dell
RC

dell

dell

RC

well, can I move the drives?
ah not like that
I guess I could just get an
enclosure and copy via USB
i would advise you to get the
disk

well, can I move the drives?
you can’t move the drives.
deﬁnitely not. this is
the problem with RAID :)
haha yeah

Figure 5: Example of before (top box) and after
(bottom box) the algorithm adds and concatenates
utterances in dialogue extraction. Since RC only
addresses dell, all of his utterances are added,
however this is not done for dell as he addresses
both RC and cucho.

The Ubuntu Dialogue Corpus: A Large Dataset for Research in
Unstructured Multi-Turn Dialogue Systems

Ryan Lowe∗*, Nissan Pow*, Iulian V. Serban† and Joelle Pineau*

*School of Computer Science, McGill University, Montreal, Canada
†Department of Computer Science and Operations Research, Universié de Montréal, Montreal, Canada

6
1
0
2
 
b
e
F
 
4
 
 
]
L
C
.
s
c
[
 
 
3
v
9
0
9
8
0
.
6
0
5
1
:
v
i
X
r
a

Abstract

This paper introduces the Ubuntu Dia-
logue Corpus, a dataset containing almost
1 million multi-turn dialogues, with a to-
tal of over 7 million utterances and 100
million words. This provides a unique re-
source for research into building dialogue
managers based on neural language mod-
els that can make use of large amounts
of unlabeled data. The dataset has both
the multi-turn property of conversations
in the Dialog State Tracking Challenge
datasets, and the unstructured nature of in-
teractions from microblog services such
as Twitter. We also describe two neural
learning architectures suitable for analyz-
ing this dataset, and provide benchmark
performance on the task of selecting the
best next response.

1

Introduction

The ability for a computer to converse in a nat-
ural and coherent manner with a human has long
been held as one of the primary objectives of artiﬁ-
cial intelligence (AI). In this paper we consider the
problem of building dialogue agents that have the
ability to interact in one-on-one multi-turn con-
versations on a diverse set of topics. We primar-
ily target unstructured dialogues, where there is
no a priori logical representation for the informa-
tion exchanged during the conversation. This is in
contrast to recent systems which focus on struc-
tured dialogue tasks, using a slot-ﬁlling represen-
tation [10, 27, 32].

We observe that in several subﬁelds of AI—
computer vision, speech recognition, machine
translation—fundamental break-throughs were
achieved in recent years using machine learning

methods, more speciﬁcally with neural architec-
tures [1]; however, it is worth noting that many
of the most successful approaches, in particular
convolutional and recurrent neural networks, were
known for many years prior.
It is therefore rea-
sonable to attribute this progress to three major
factors: 1) the public distribution of very large
rich datasets [5], 2) the availability of substantial
computing power, and 3) the development of new
training methods for neural architectures, in par-
ticular leveraging unlabeled data. Similar progress
has not yet been observed in the development of
dialogue systems. We hypothesize that this is due
to the lack of sufﬁciently large datasets, and aim
to overcome this barrier by providing a new large
corpus for research in multi-turn conversation.

The new Ubuntu Dialogue Corpus consists of
almost one million two-person conversations ex-
tracted from the Ubuntu chat logs1, used to receive
technical support for various Ubuntu-related prob-
lems. The conversations have an average of 8 turns
each, with a minimum of 3 turns. All conversa-
tions are carried out in text form (not audio). The
dataset is orders of magnitude larger than struc-
tured corpuses such as those of the Dialogue State
Tracking Challenge [32]. It is on the same scale as
recent datasets for solving problems such as ques-
tion answering and analysis of microblog services,
such as Twitter [22, 25, 28, 33], but each conversa-
tion in our dataset includes several more turns, as
well as longer utterances. Furthermore, because
it targets a speciﬁc domain, namely technical sup-
port, it can be used as a case study for the devel-
opment of AI agents in targeted applications, in
contrast to chatbox agents that often lack a well-
deﬁned goal [26].

In addition to the corpus, we present learning
architectures suitable for analyzing this dataset,
ranging from the simple frequency-inverse docu-

1These logs are available from 2004 to 2015 at http:

∗The ﬁrst two authors contributed equally.

//irclogs.ubuntu.com/

ment frequency (TF-IDF) approach, to more so-
phisticated neural models including a Recurrent
Neural Network (RNN) and a Long Short-Term
Memory (LSTM) architecture. We provide bench-
trained
mark performance of these algorithms,
with our new corpus, on the task of selecting the
best next response, which can be achieved with-
out requiring any human labeling. The dataset is
ready for public release2. The code developed for
the empirical results is also available3.

2 Related Work

We brieﬂy review existing dialogue datasets, and
some of the more recent learning architectures
used for both structured and unstructured dia-
logues. This is by no means an exhaustive list
(due to space constraints), but surveys resources
most related to our contribution. A list of datasets
discussed is provided in Table 1.

2.1 Dialogue Datasets

The Switchboard dataset [8], and the Dialogue
State Tracking Challenge (DSTC) datasets [32]
have been used to train and validate dialogue man-
agement systems for interactive information re-
trieval. The problem is typically formalized as a
slot ﬁlling task, where agents attempt to predict
the goal of a user during the conversation. These
datasets have been signiﬁcant resources for struc-
tured dialogues, and have allowed major progress
in this ﬁeld, though they are quite small compared
to datasets currently used for training neural archi-
tectures.

Recently, a few datasets have been used con-
taining unstructured dialogues extracted from
Twitter4. Ritter et al. [21] collected 1.3 million
conversations; this was extended in [28] to take ad-
vantage of longer contexts by using A-B-A triples.
Shang et al. [25] used data from a similar Chinese
website called Weibo5. However to our knowl-
edge, these datasets have not been made public,
and furthermore, the post-reply format of such mi-
croblogging services is perhaps not as represen-
tative of natural dialogue between humans as the
continuous stream of messages in a chat room. In

2Note that a new version of

is now
https://github.com/rkadlec/
available:
ubuntu-ranking-dataset-creator.
This ver-
sion makes some adjustments and ﬁxes some bugs from the
ﬁrst version.

the dataset

3http://github.com/npow/ubottu
4https://twitter.com/
5http://www.weibo.com/

fact, Ritter et al. estimate that only 37% of posts
on Twitter are ‘conversational in nature’, and 69%
of their collected data contained exchanges of only
length 2 [21]. We hypothesize that chat-room style
messaging is more closely correlated to human-to-
human dialogue than micro-blogging websites, or
forum-based sites such as Reddit.

Part of the Ubuntu chat logs have previously
been aggregated into a dataset, called the Ubuntu
Chat Corpus [30]. However that resource pre-
serves the multi-participant structure and thus is
less amenable to the investigation of more tradi-
tional two-party conversations.

Also weakly related to our contribution is the
problem of question-answer systems.
Several
datasets of question-answer pairs are available [3],
however these interactions are much shorter than
what we seek to study.

2.2 Learning Architectures

Most dialogue research has historically focused
on structured slot-ﬁlling tasks [24]. Various ap-
proaches were proposed, yet few attempts lever-
age more recent developments in neural learning
architectures. A notable exception is the work of
Henderson et al. [11], which proposes an RNN
structure, initialized with a denoising autoencoder,
to tackle the DSTC 3 domain.

Work on unstructured dialogues, recently pi-
oneered by Ritter et al. [22], proposed a re-
sponse generation model for Twitter data based on
ideas from Statistical Machine Translation. This
is shown to give superior performance to previ-
ous information retrieval (e.g. nearest neighbour)
approaches [14]. This idea was further devel-
oped by Sordoni et al. [28] to exploit information
from a longer context, using a structure similar to
the Recurrent Neural Network Encoder-Decoder
model [4]. This achieves rather poor performance
on A-B-A Twitter triples when measured by the
BLEU score (a standard for machine translation),
yet performs comparatively better than the model
of Ritter et al. [22]. Their results are also veriﬁed
with a human-subject study. A similar encoder-
decoder framework is presented in [25]. This
model uses one RNN to transform the input to
some vector representation, and another RNN to
‘decode’ this representation to a response by gen-
erating one word at a time. This model is also eval-
uated in a human-subject study, although much
smaller in size than in [28]. Overall, these models

Dataset

Switchboard [8]

DSTC1 [32]

DSTC2 [10]

DSTC3 [9]

DSTC4[13]

Twitter
Corpus [21]
Twitter Triple
Corpus [28]
Sina Weibo [25]

Ubuntu Dialogue
Corpus

Type

Human-human
spoken
Human-computer
spoken
Human-computer
spoken
Human-computer
spoken
Human-human
spoken
Human-human
micro-blog
Human-human
micro-blog
Human-human
micro-blog
Human-human
chat

Task

Various

State
tracking
State
tracking
State
tracking
State
tracking
Next utterance
generation
Next utterance
generation
Next utterance
generation
Next utterance
classiﬁcation

# Dialogues

# Utterances

# Words

Description

2,400

—

3,000,000

15,000

210,000

3,000

2,265

35

24,000

15,000

—

1,300,000

3,000,000

29,000,000

87,000,000

4,435,959

8,871,918

—

—

—

—

—

—

930,000

7,100,000

100,000,000

Telephone conversations
on pre-speciﬁed topics
Bus ride information
system
Restaurant booking
system
Tourist information
system
21 hours of tourist info
exchange over Skype
Post/ replies extracted
from Twitter
A-B-A triples from
Twitter replies
Post/ reply pairs extracted
from Weibo
Extracted from Ubuntu
Chat Logs

Table 1: A selection of structured and unstructured large-scale datasets applicable to dialogue systems.
Faded datasets are not publicly available. The last entry is our contribution.

highlight the potential of neural learning architec-
tures for interactive systems, yet so far they have
been limited to very short conversations.

3 The Ubuntu Dialogue Corpus

We seek a large dataset for research in dialogue
systems with the following properties:

• Two-way (or dyadic) conversation, as op-
posed to multi-participant chat, preferably
human-human.

• Large number of conversations; 105 − 106
is typical of datasets used for neural-network
learning in other areas of AI.

• Many conversations with several turns (more

• Task-speciﬁc domain, as opposed to chatbot

than 3).

systems.

All of these requirements are satisﬁed by the
Ubuntu Dialogue Corpus presented in this paper.

3.1 Ubuntu Chat Logs

The Ubuntu Chat Logs refer to a collection of logs
from Ubuntu-related chat rooms on the Freenode
Internet Relay Chat (IRC) network. This protocol
allows for real-time chat between a large number
of participants. Each chat room, or channel, has
a particular topic, and every channel participant
can see all the messages posted in a given chan-
nel. Many of these channels are used for obtaining
technical support with various Ubuntu issues.

As the contents of each channel are moderated,
most interactions follow a similar pattern. A new
user joins the channel, and asks a general ques-
tion about a problem they are having with Ubuntu.
Then, another more experienced user replies with

a potential solution, after ﬁrst addressing the ’user-
name’ of the ﬁrst user. This is called a name men-
tion [29], and is done to avoid confusion in the
channel — at any given time during the day, there
can be between 1 and 20 simultaneous conversa-
tions happening in some channels.
In the most
popular channels, there is almost never a time
when only one conversation is occurring; this ren-
ders it particularly problematic to extract dyadic
dialogues. A conversation between a pair of users
generally stops when the problem has been solved,
though some users occasionally continue to dis-
cuss a topic not related to Ubuntu.

Despite the nature of the chat room being a con-
stant stream of messages from multiple users, it is
through the fairly rigid structure in the messages
that we can extract the dialogues between users.
Figure 4 shows an example chat room conversa-
tion from the #ubuntu channel as well as the ex-
tracted dialogues, which illustrates how users usu-
ally state the username of the intended message
recipient before writing their reply (we refer to all
replies and initial questions as ‘utterances’). For
example, it is clear that users ‘Taru’ and ‘kuja’
are engaged in a dialogue, as are users ‘Old’ and
‘bur[n]er’, while user ‘_pm’ is asking an initial
question, and ‘LiveCD’ is perhaps elaborating on
a previous comment.

3.2 Dataset Creation

In order to create the Ubuntu Dialogue Corpus,
ﬁrst a method had to be devised to extract dyadic
dialogues from the chat room multi-party conver-
sations. The ﬁrst step was to separate every mes-
sage into 4-tuples of (time, sender, recipient, utter-
ance). Given these 4-tuples, it is straightforward to

group all tuples where there is a matching sender
and recipient. Although it is easy to separate the
time and the sender from the rest, ﬁnding the in-
tended recipient of the message is not always triv-
ial.

3.2.1 Recipient Identiﬁcation
While in most cases the recipient is the ﬁrst word
of the utterance, it is sometimes located at the end,
or not at all in the case of initial questions. Fur-
thermore, some users choose names correspond-
ing to common English words, such as ‘the’ or
‘stop’, which could lead to many false positives.
In order to solve this issue, we create a dictionary
of usernames from the current and previous days,
and compare the ﬁrst word of each utterance to its
If a match is found, and the word does
entries.
not correspond to a very common English word6,
it is assumed that this user was the intended recip-
ient of the message. If no matches are found, it is
assumed that the message was an initial question,
and the recipient value is left empty.

3.2.2 Utterance Creation
The dialogue extraction algorithm works back-
wards from the ﬁrst response to ﬁnd the initial
question that was replied to, within a time frame
of 3 minutes. A ﬁrst response is identiﬁed by the
presence of a recipient name (someone from the
recent conversation history). The initial question
is identiﬁed to be the most recent utterance by the
recipient identiﬁed in the ﬁrst response.

All utterances that do not qualify as a ﬁrst re-
sponse or an initial question are discarded; initial
questions that do not generate any response are
also discarded. We additionally discard conversa-
tions longer than ﬁve utterances where one user
says more than 80% of the utterances, as these are
typically not representative of real chat dialogues.
Finally, we consider only extracted dialogues that
consist of 3 turns or more to encourage the model-
ing of longer-term dependencies.

To alleviate the problem of ‘holes’ in the dia-
logue, where one user does not address the other
explicitly, as in Figure 5, we check whether each
user talks to someone else for the duration of their
conversation. If not, all non-addressed utterances
are added to the dialogue. An example conversa-
tion along with the extracted dialogues is shown
in Figure 5. Note that we also concatenate all con-
secutive utterances from a given user.

6We use the GNU Aspell spell checking dictionary.

Figure 1: Plot of number of conversations with a
given number of turns. Both axes use a log scale.

# dialogues (human-human)
# utterances (in total)
# words (in total)
Min. # turns per dialogue
Avg. # turns per dialogue
Avg. # words per utterance
Median conversation length (min)

930,000
7,100,000
100,000,000
3
7.71
10.34
6

Table 2: Properties of Ubuntu Dialogue Corpus.

We do not apply any further pre-processing (e.g.
tokenization, stemming) to the data as released in
the Ubuntu Dialogue Corpus. However the use of
pre-processing is standard for most NLP systems,
and was also used in our analysis (see Section 4.)

3.2.3 Special Cases and Limitations

It is often the case that a user will post an ini-
tial question, and multiple people will respond to
it with different answers.
In this instance, each
conversation between the ﬁrst user and the user
who replied is treated as a separate dialogue. This
has the unfortunate side-effect of having the ini-
tial question appear multiple times in several dia-
logues. However the number of such cases is suf-
ﬁciently small compared to the size of the dataset.
Another issue to note is that the utterance post-
ing time is not considered for segmenting conver-
sations between two users. Even if two users have
a conversation that spans multiple hours, or even
days, this is treated as a single dialogue. However,
such dialogues are rare. We include the posting
time in the corpus so that other researchers may
ﬁlter as desired.

3.3 Dataset Statistics

Table 2 summarizes properties of the Ubuntu Dia-
logue Corpus. One of the most important features

of the Ubuntu chat logs is its size. This is cru-
cial for research into building dialogue managers
based on neural architectures. Another important
characteristic is the number of turns in these dia-
logues. The distribution of the number of turns is
shown in Figure 1. It can be seen that the num-
ber of dialogues and turns per dialogue follow an
approximate power law relationship.

3.4 Test Set Generation

We set aside 2% of the Ubuntu Dialogue Corpus
conversations (randomly selected) to form a test
set that can be used for evaluation of response se-
lection algorithms. Compared to the rest of the
corpus, this test set has been further processed to
extract a pair of (context, response, ﬂag) triples
from each dialogue. The ﬂag is a Boolean vari-
able indicating whether or not the response was the
actual next utterance after the given context. The
response is a target (output) utterance which we
aim to correctly identify. The context consists of
the sequence of utterances appearing in dialogue
prior to the response. We create a pair of triples,
where one triple contains the correct response (i.e.
the actual next utterance in the dialogue), and the
other triple contains a false response, sampled ran-
domly from elsewhere within the test set. The ﬂag
is set to 1 in the ﬁrst case and to 0 in the second
case. An example pair is shown in Table 3. To
make the task harder, we can move from pairs of
responses (one correct, one incorrect) to a larger
set of wrong responses (all with ﬂag=0). In our
experiments below, we consider both the case of 1
wrong response and 10 wrong responses.

Context
well, can I move the drives?
__EOS__ ah not like that

well, can I move the drives?
__EOS__ ah not like that

Response
I guess I could just
get an enclosure and
copy via USB
you can use "ps ax"
and "kill (PID #)"

Flag
1

0

Table 3: Test set example with (context, reply,
ﬂag) format. The ’__EOS__’ tag is used to denote
the end of an utterance within the context.

Since we want to learn to predict all parts of a
conversation, as opposed to only the closing state-
ment, we consider various portions of context for
the conversations in the test set. The context size is
determined stochastically using a simple formula:

c = min(t − 1, n − 1),

Here, C denotes the maximum desired context
size, which we set to C = 20. The last term is
the desired minimum context size, which we set
to be 2. Parameter t is the actual length of that
dialogue (thus the constraint that c ≤ t − 1), and
n is a random number corresponding to the ran-
domly sampled context length, that is selected to
be inversely proportional to C.

In practice, this leads to short test dialogues
having short contexts, while longer dialogues are
often broken into short or medium-length seg-
ments, with the occasional long context of 10 or
more turns.

3.5 Evaluation Metric

We consider the task of best response selection.
This can be achieved by processing the data as de-
scribed in Section 3.4, without requiring any hu-
man labels. This classiﬁcation task is an adapta-
tion of the recall and precision metrics previously
applied to dialogue datasets [24].

A family of metrics often used in language tasks
is Recall@k (denoted R@1 R@2, R@5 below).
Here the agent is asked to select the k most likely
responses, and it is correct if the true response is
among these k candidates. Only the R@1 metric
is relevant in the case of binary classiﬁcation (as
in the Table 3 example).

Although a language model that performs well
on response classiﬁcation is not a gauge of good
performance on next utterance generation, we hy-
pothesize that improvements on a model with re-
gards to the classiﬁcation task will eventually lead
to improvements for the generation task. See Sec-
tion 6 for further discussion of this point.

4 Learning Architectures for
Unstructured Dialogues

To provide further evidence of the value of
our dataset for research into neural architectures
for dialogue managers, we provide performance
benchmarks for two neural learning algorithms, as
well as one naive baseline. The approaches con-
sidered are: TF-IDF, Recurrent Neural networks
(RNN), and Long Short-Term Memory (LSTM).
Prior to applying each method, we perform stan-
dard pre-processing of the data using the NLTK7
library and Twitter tokenizer8 to parse each utter-
ance. We use generic tags for various word cat-

where n =

+ 2, η ∼ U nif (C/2, 10C)

10C
η

7www.nltk.org/
8http://www.ark.cs.cmu.edu/TweetNLP/

egories, such as names, locations, organizations,
URLs, and system paths.

To train the RNN and LSTM architectures, we
process the full training Ubuntu Dialogue Corpus
into the same format as the test set described in
Section 3.4, extracting (context, response, ﬂag)
triples from dialogues. For the training set, we
do not sample the context length, but instead con-
sider each utterance (starting at the 3rd one) as a
potential response, with the previous utterances as
its context. So a dialogue of length 10 yields 8
training examples. Since these are overlapping,
they are clearly not independent, but we consider
this a minor issue given the size of the dataset (we
further alleviate the issue by shufﬂing the training
examples). Negative responses are selected at ran-
dom from the rest of the training data.

4.1 TF-IDF

Term frequency-inverse document frequency is a
statistic that intends to capture how important a
given word is to some document, which in our case
is the context [20]. It is a technique often used in
document classiﬁcation and information retrieval.
The ‘term-frequency’ term is simply a count of the
number of times a word appears in a given context,
while the ‘inverse document frequency’ term puts
a penalty on how often the word appears elsewhere
in the corpus. The ﬁnal score is calculated as the
product of these two terms, and has the form:

tﬁdf(w, d, D) = f (w, d)×log

N
|{d ∈ D : w ∈ d}|

,

where f (w, d) indicates the number of times word
w appeared in context d, N is the total number
of dialogues, and the denominator represents the
number of dialogues in which the word w appears.
For classiﬁcation, the TF-IDF vectors are ﬁrst
calculated for the context and each of the candi-
date responses. Given a set of candidate response
vectors, the one with the highest cosine similarity
to the context vector is selected as the output. For
Recall@k, the top k responses are returned.

4.2 RNN

Recurrent neural networks are a variant of neural
networks that allows for time-delayed directed cy-
cles between units [17]. This leads to the forma-
tion of an internal state of the network, ht, which
allows it to model time-dependent data. The in-
ternal state is updated at each time step as some

Figure 2: Diagram of our model. The RNNs have
tied weights. c, r are the last hidden states from
the RNNs. ci, ri are word vectors for the context
and response, i < t. We consider contexts up to a
maximum of t = 160.

function of the observed variables xt, and the hid-
den state at the previous time step ht−1. Wx and
Wh are matrices associated with the input and hid-
den state.

ht = f (Whht−1 + Wxxt).

A diagram of an RNN can be seen in Figure 2.
RNNs have been the primary building block of
many current neural language models [22, 28],
which use RNNs for an encoder and decoder. The
ﬁrst RNN is used to encode the given context,
and the second RNN generates a response by us-
ing beam-search, where its initial hidden state is
biased using the ﬁnal hidden state from the ﬁrst
RNN. In our work, we are concerned with classi-
ﬁcation of responses, instead of generation. We
build upon the approach in [2], which has also
been recently applied to the problem of question
answering [33].

We utilize a siamese network consisting of two
RNNs with tied weights to produce the embed-
dings for the context and response. Given some
input context and response, we compute their em-
beddings — c, r ∈ Rd, respectively — by feeding
the word embeddings one at a time into its respec-
tive RNN. Word embeddings are initialized using
the pre-trained vectors (Common Crawl, 840B to-
kens from [19]), and ﬁne-tuned during training.
The hidden state of the RNN is updated at each
step, and the ﬁnal hidden state represents a sum-
mary of the input utterance. Using the ﬁnal hid-
den states from both RNNs, we then calculate the
probability that this is a valid pair:

p(ﬂag = 1|c, r, M ) = σ(cT M r + b),

where the bias b and the matrix M ∈ Rd×d are
learned model parameters. This can be thought
of as a generative approach; given some input re-
sponse, we generate a context with the product
c(cid:48) = M r, and measure the similarity to the actual
context using the dot product. This is converted
to a probability with the sigmoid function. The
model is trained by minimizing the cross entropy
of all labeled (context, response) pairs [33]:

examples. Of course, the Recall@2 and Recall@5
are not relevant in the binary classiﬁcation case9.

Method
1 in 2 R@1
1 in 10 R@1
1 in 10 R@2
1 in 10 R@5

TF-IDF
RNN LSTM
65.9% 76.8% 87.8%
41.0% 40.3% 60.4%
54.5% 54.7% 74.5%
70.8% 81.9% 92.6%

L = −

log p(ﬂagn|cn, rn, M ) +

(cid:88)

n

λ
2

||θ = ||2
F

Table 4: Results for the three algorithms using var-
ious recall measures for binary (1 in 2) and 1 in 10
(1 in 10) next utterance classiﬁcation %.

where ||θ||2
F is the Frobenius norm of θ = {M, b}.
In our experiments, we use λ = 0 for computa-
tional simplicity.

For training, we used a 1:1 ratio between true re-
sponses (ﬂag = 1), and negative responses (ﬂag=0)
drawn randomly from elsewhere in the training
set. The RNN architecture is set to 1 hidden layer
with 50 neurons. The Wh matrix is initialized us-
ing orthogonal weights [23], while Wx is initial-
ized using a uniform distribution with values be-
tween -0.01 and 0.01. We use Adam as our opti-
mizer [15], with gradients clipped to 10. We found
that weight initialization as well as the choice of
optimizer were critical for training the RNNs.

4.3 LSTM

In addition to the RNN model, we consider the
same architecture but changed the hidden units
to long-short term memory (LSTM) units [12].
LSTMs were introduced in order to model longer-
term dependencies. This is accomplished using a
series of gates that determine whether a new in-
put should be remembered, forgotten (and the old
value retained), or used as output. The error sig-
nal can now be fed back indeﬁnitely into the gates
of the LSTM unit. This helps overcome the van-
ishing and exploding gradient problems in stan-
dard RNNs, where the error gradients would oth-
erwise decrease or increase at an exponential rate.
In training, we used 1 hidden layer with 200 neu-
rons. The hyper-parameter conﬁguration (includ-
ing number of neurons) was optimized indepen-
dently for RNNs and LSTMs using a validation
set extracted from the training data.

5 Empirical Results

The results for the TF-IDF, RNN, and LSTM mod-
els are shown in Table 4. The models were eval-
uated using both 1 (1 in 2) and 9 (1 in 10) false

We observe that the LSTM outperforms both
the RNN and TF-IDF on all evaluation metrics.
It is interesting to note that TF-IDF actually out-
performs the RNN on the Recall@1 case for the
1 in 10 classiﬁcation. This is most likely due to
the limited ability of the RNN to take into account
long contexts, which can be overcome by using the
LSTM. An example output of the LSTM where the
response is correctly classiﬁed is shown in Table 5.
We also show, in Figure 3, the increase in per-
formance of the LSTM as the amount of data used
for training increases. This conﬁrms the impor-
tance of having a large training set.

Context
""any apache hax around ? i just deleted all of
__path__ - which package provides it ?",
"reconﬁguring apache do n’t solve it ?"

Ranked Responses
1. "does n’t seem to, no"
2. "you can log in but not transfer ﬁles ?"

Flag
1
0

Table 5: Example showing the ranked responses
from the LSTM. Each utterance is shown after pre-
processing steps.

6 Discussion

This paper presents the Ubuntu Dialogue Corpus,
a large dataset for research in unstructured multi-
turn dialogue systems. We describe the construc-
tion of the dataset and its properties. The availabil-
ity of a dataset of this size opens up several inter-
esting possibilities for research into dialogue sys-
tems based on rich neural-network architectures.
We present preliminary results demonstrating use
of this dataset to train an RNN and an LSTM for
the task of selecting the next best response in a

9Note that these results are on the original dataset. Results
on the new dataset should not be compared to the old dataset;
baselines on the new dataset will be released shortly.

6.3 State Tracking and Utterance Generation

The work described here focuses on the task of re-
sponse selection. This can be seen as an interme-
diate step between slot ﬁlling and utterance gener-
ation. In slot ﬁlling, the set of candidate outputs
(states) is identiﬁed a priori through knowledge
engineering, and is typically smaller than the set
of responses considered in our work. When the
set of candidate responses is close to the size of
the dataset (e.g. all utterances ever recorded), then
we are quite close to the response generation case.
There are several reasons not to proceed directly
to response generation. First, it is likely that cur-
rent algorithms are not yet able to generate good
results for this task, and it is preferable to tackle
metrics for which we can make progress. Second,
we do not yet have a suitable metric for evaluat-
ing performance in the response generation case.
One option is to use the BLEU [18] or METEOR
[16] scores from machine translation. However,
using BLEU to evaluate dialogue systems has been
shown to give extremely low scores [28], due to
the large space of potential sensible responses [7].
Further, since the BLEU score is calculated us-
ing N-grams [18], it would provide a very low
score for reasonable responses that do not have
any words in common with the ground-truth next
utterance.

Alternatively, one could measure the difference
between the generated utterance and the actual
sentence by comparing their representations in
some embedding (or semantic) space. However,
different models inevitably use different embed-
dings, necessitating a standardized embedding for
evaluation purposes. Such a standardized embed-
dings has yet to be created.

Another possibility is to use human subjects to
score automatically generated responses, but time
and expense make this a highly impractical option.
In summary, while it is possible that current lan-
guage models have outgrown the use of slot ﬁll-
ing as a metric, we are currently unable to mea-
sure their ability in next utterance generation in
a standardized, meaningful and inexpensive way.
This motivates our choice of response selection as
a useful metric for the time being.

Acknowledgments

The authors gratefully acknowledge ﬁnancial sup-
port for this work by the Samsung Advanced
Institute of Technology (SAIT) and the Natural

Figure 3: The LSTM (with 200 hidden units),
showing Recall@1 for the 1 in 10 classiﬁcation,
with increasing dataset sizes.

conversation; we obtain signiﬁcantly better results
with the LSTM architecture. There are several in-
teresting directions for future work.

6.1 Conversation Disentanglement

Our approach to conversation disentanglement
consists of a small set of rules. More sophisticated
techniques have been proposed, such as training a
maximum-entropy classiﬁer to cluster utterances
into separate dialogues [6]. However, since we
are not trying to replicate the exact conversation
between two users, but only to retrieve plausible
natural dialogues, the heuristic method presented
in this paper may be sufﬁcient. This seems sup-
ported through qualitative examination of the data,
but could be the subject of more formal evaluation.

6.2 Altering Test Set Difﬁculty

One of the interesting properties of the response
selection task is the ability to alter the task dif-
ﬁculty in a controlled manner. We demonstrated
this by moving from 1 to 9 false responses, and
by varying the Recall@k parameter. In the future,
instead of choosing false responses randomly, we
will consider selecting false responses that are
similar to the actual response (e.g. as measured by
cosine similarity). A dialogue model that performs
well on this more difﬁcult task should also manage
to capture a more ﬁne-grained semantic meaning
of sentences, as compared to a model that naively
picks replies with the most words in common with
the context such as TF-IDF.

Sciences and Engineering Research Council of
Canada (NSERC). We would like to thank Lau-
rent Charlin for his input into this paper, as well as
Gabriel Forgues and Eric Crawford for interesting
discussions.

References

[1] Y. Bengio, A. Courville, and P. Vincent.
Representation learning: A review and new
Pattern Analysis and Ma-
perspectives.
chine Intelligence, IEEE Transactions on,
35(8):1798–1828, 2013.

[2] A. Bordes, J. Weston, and N. Usunier. Open
question answering with weakly supervised
embedding models. In MLKDD, pages 165–
180. Springer, 2014.
J. Boyd-Graber, B. Satinoff, H. He, and
H. Daume. Besting the quiz master: Crowd-
sourcing incremental classiﬁcation games. In
EMNLP, 2012.

[3]

[5]

[4] K. Cho, B. van Merrienboer, C. Gulcehre,
F. Bougares, H. Schwenk, and Y. Ben-
gio. Learning phrase representations using
rnn encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078,
2014.
J. Deng, W. Dong, R. Socher, L.J. Li, K. Li,
and L. Fei-Fei. Imagenet: A large-scale hier-
archical image database. In CVPR, 2009.
[6] M. Elsner and E. Charniak. You talking to
me? a corpus and algorithm for conversa-
In ACL, pages 834–
tion disentanglement.
842, 2008.

[7] M. Galley, C. Brockett, A. Sordoni, Y. Ji,
M. Auli, C. Quirk, M. Mitchell, J. Gao, and
B. Dolan. deltableu: A discriminative metric
for generation tasks with intrinsically diverse
arXiv preprint arXiv:1506.06863,
targets.
2015.
J.J. Godfrey, E.C. Holliman, and J. Mc-
Switchboard: Telephone speech
Daniel.
corpus for research and development.
In
ICASSP, 1992.

[8]

[9] M. Henderson, B. Thomson, and J. Williams.
Dialog state tracking challenge 2 & 3, 2014.
[10] M. Henderson, B. Thomson, and J. Williams.
The second dialog state tracking challenge.
In SIGDIAL, page 263, 2014.

[11] M. Henderson, B. Thomson, and S. Young.
Word-based dialog state tracking with recur-

rent neural networks. In SIGDIAL, page 292,
2014.

[12] S. Hochreiter and J. Schmidhuber. Long
short-term memory. Neural computation,
9(8):1735–1780, 1997.

[13] Dialog state tracking challenge 4.
[14] S. Jafarpour, C. Burges, and A. Ritter. Filter,
rank, and transfer the knowledge: Learning
to chat. Advances in Ranking, 10, 2010.

[15] D.P. Kingma and J. Ba.

Adam: A
method for stochastic optimization. CoRR,
abs/1412.6980, 2014.

[16] A. Lavie and M.J. Denkowski. The ME-
TEOR metric for automatic evaluation of
Machine Translation. Machine Translation,
23(2-3):105–115, 2009.

[17] L.R. Medsker and L.C. Jain. Recurrent neu-
ral networks. Design and Applications, 2001.
[18] K. Papineni, S. Roukos, T. Ward, and W.J.
Zhu. Bleu: a method for automatic evalua-
tion of machine translation. In ACL, 2002.

[19] J. Pennington, R. Socher, and C.D. Manning.
GloVe: Global Vectors for Word Representa-
tion. In EMNLP, 2014.

[20] J. Ramos. Using tf-idf to determine word rel-
evance in document queries. In ICML, 2003.
[21] A. Ritter, C. Cherry, and W. Dolan. Unsu-
pervised modeling of twitter conversations.
2010.

[22] A. Ritter, C. Cherry, and W. Dolan. Data-
driven response generation in social media.
In EMNLP, pages 583–593, 2011.

[23] A.M. Saxe, J.L. McClelland, and S. Ganguli.
Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks.
arXiv preprint arXiv:1312.6120, 2013.
[24] J. Schatzmann, K. Georgila, and S. Young.
Quantitative evaluation of user simulation
techniques for spoken dialogue systems. In
SIGDIAL, 2005.

[25] L. Shang, Z. Lu, and H. Li.

Neural
responding machine for short-text conver-
arXiv preprint arXiv:1503.02364,
sation.
2015.

[26] B. A. Shawar and E. Atwell. Chatbots: are
In LDV Forum, vol-

they really useful?
ume 22, pages 29–49, 2007.

[27] S. Singh, D. Litman, M. Kearns, and
M. Walker. Optimizing dialogue manage-
ment with reinforcement learning: Experi-
ments with the NJFun system. Journal of

Artiﬁcial Intelligence Research, 16:105–133,
2002.

[28] A. Sordoni, M. Galley, M. Auli, C. Brock-
ett, Y. Ji, M. Mitchell, J.Y. Nie, J. Gao,
and W. Dolan. A neural network approach
to context-sensitive generation of conversa-
tional responses. 2015.

[29] D.C. Uthus and D.W. Aha. Extending word
highlighting in multiparticipant chat. Tech-
nical report, DTIC Document, 2013.

[30] D.C. Uthus and D.W Aha. The ubuntu chat
corpus for multiparticipant chat analysis. In
AAAI Spring Symposium on Analyzing Mi-
crotext, pages 99–102, 2013.

[31] H. Wang, Z. Lu, H. Li, and E. Chen. A
dataset for research on short-text conversa-
tions. In EMNLP, 2013.

[32] J. Williams, A. Raux, D. Ramachandran, and
A. Black. The dialog state tracking chal-
lenge. In SIGDIAL, pages 404–413, 2013.

[33] L. Yu, K. M. Hermann, P. Blunsom,
Deep learning for an-
arXiv preprint

and S. Pulman.
swer sentence selection.
arXiv:1412.1632, 2014.

[34] M.D. Zeiler.

Adadelta:

an adaptive
arXiv preprint

learning rate method.
arXiv:1212.5701, 2012.

Appendix A: Dialogue excerpts

Figure 4: Example chat room conversation from
the #ubuntu channel of the Ubuntu Chat Logs
(top), with the disentangled conversations for the
Ubuntu Dialogue Corpus (bottom).

Time

03:44

03:45
03:45
03:45

User

Old

kuja
Taru
bur[n]er

03:45

kuja

03:45
03:45
03:45

Taru
LiveCD
kuja

03:46

_pm

03:46
Sender

Old

Taru
Recipient

bur[n]er

Old

kuja
Taru
kuja

Taru
kuja

Taru

Taru
Kuja
Taru

Kuja
Taru

Kuja

Utterance

I dont run graphical ubuntu,
I run ubuntu server.
Taru: Haha sucker.
Kuja: ?
Old: you can use "ps ax"
and "kill (PID#)"
Taru: Anyways, you made
the changes right?
Kuja: Yes.
or killall speedlink
Taru: Then from the terminal
type: sudo apt-get update
if i install the beta version,
how can i update it when
the ﬁnal version comes out?
Kuja: I did.
Utterance

I dont run graphical ubuntu,
I run ubuntu server.
you can use "ps ax" and
"kill (PID#)"

Haha sucker.
?
Anyways, you made the
changes right?
Yes.
Then from the terminal type:
sudo apt-get update
I did.

Time

[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:22]

User

dell
cucho
RC
RC
dell
dell
RC
dell
dell

[12:22]

cucho

Utterance

well, can I move the drives?
dell: ah not like that
dell: you can’t move the drives
dell: deﬁnitely not
ok
lol
this is the problem with RAID:)
RC haha yeah
cucho, I guess I could
just get an enclosure
and copy via USB...
dell: i would advise you to get
the disk

Sender

Recipient

Utterance

dell
cucho
dell

dell
cucho

cucho

dell

dell
RC

dell

dell

RC

well, can I move the drives?
ah not like that
I guess I could just get an
enclosure and copy via USB
i would advise you to get the
disk

well, can I move the drives?
you can’t move the drives.
deﬁnitely not. this is
the problem with RAID :)
haha yeah

Figure 5: Example of before (top box) and after
(bottom box) the algorithm adds and concatenates
utterances in dialogue extraction. Since RC only
addresses dell, all of his utterances are added,
however this is not done for dell as he addresses
both RC and cucho.

The Ubuntu Dialogue Corpus: A Large Dataset for Research in
Unstructured Multi-Turn Dialogue Systems

Ryan Lowe∗*, Nissan Pow*, Iulian V. Serban† and Joelle Pineau*

*School of Computer Science, McGill University, Montreal, Canada
†Department of Computer Science and Operations Research, Universié de Montréal, Montreal, Canada

6
1
0
2
 
b
e
F
 
4
 
 
]
L
C
.
s
c
[
 
 
3
v
9
0
9
8
0
.
6
0
5
1
:
v
i
X
r
a

Abstract

This paper introduces the Ubuntu Dia-
logue Corpus, a dataset containing almost
1 million multi-turn dialogues, with a to-
tal of over 7 million utterances and 100
million words. This provides a unique re-
source for research into building dialogue
managers based on neural language mod-
els that can make use of large amounts
of unlabeled data. The dataset has both
the multi-turn property of conversations
in the Dialog State Tracking Challenge
datasets, and the unstructured nature of in-
teractions from microblog services such
as Twitter. We also describe two neural
learning architectures suitable for analyz-
ing this dataset, and provide benchmark
performance on the task of selecting the
best next response.

1

Introduction

The ability for a computer to converse in a nat-
ural and coherent manner with a human has long
been held as one of the primary objectives of artiﬁ-
cial intelligence (AI). In this paper we consider the
problem of building dialogue agents that have the
ability to interact in one-on-one multi-turn con-
versations on a diverse set of topics. We primar-
ily target unstructured dialogues, where there is
no a priori logical representation for the informa-
tion exchanged during the conversation. This is in
contrast to recent systems which focus on struc-
tured dialogue tasks, using a slot-ﬁlling represen-
tation [10, 27, 32].

We observe that in several subﬁelds of AI—
computer vision, speech recognition, machine
translation—fundamental break-throughs were
achieved in recent years using machine learning

methods, more speciﬁcally with neural architec-
tures [1]; however, it is worth noting that many
of the most successful approaches, in particular
convolutional and recurrent neural networks, were
known for many years prior.
It is therefore rea-
sonable to attribute this progress to three major
factors: 1) the public distribution of very large
rich datasets [5], 2) the availability of substantial
computing power, and 3) the development of new
training methods for neural architectures, in par-
ticular leveraging unlabeled data. Similar progress
has not yet been observed in the development of
dialogue systems. We hypothesize that this is due
to the lack of sufﬁciently large datasets, and aim
to overcome this barrier by providing a new large
corpus for research in multi-turn conversation.

The new Ubuntu Dialogue Corpus consists of
almost one million two-person conversations ex-
tracted from the Ubuntu chat logs1, used to receive
technical support for various Ubuntu-related prob-
lems. The conversations have an average of 8 turns
each, with a minimum of 3 turns. All conversa-
tions are carried out in text form (not audio). The
dataset is orders of magnitude larger than struc-
tured corpuses such as those of the Dialogue State
Tracking Challenge [32]. It is on the same scale as
recent datasets for solving problems such as ques-
tion answering and analysis of microblog services,
such as Twitter [22, 25, 28, 33], but each conversa-
tion in our dataset includes several more turns, as
well as longer utterances. Furthermore, because
it targets a speciﬁc domain, namely technical sup-
port, it can be used as a case study for the devel-
opment of AI agents in targeted applications, in
contrast to chatbox agents that often lack a well-
deﬁned goal [26].

In addition to the corpus, we present learning
architectures suitable for analyzing this dataset,
ranging from the simple frequency-inverse docu-

1These logs are available from 2004 to 2015 at http:

∗The ﬁrst two authors contributed equally.

//irclogs.ubuntu.com/

ment frequency (TF-IDF) approach, to more so-
phisticated neural models including a Recurrent
Neural Network (RNN) and a Long Short-Term
Memory (LSTM) architecture. We provide bench-
trained
mark performance of these algorithms,
with our new corpus, on the task of selecting the
best next response, which can be achieved with-
out requiring any human labeling. The dataset is
ready for public release2. The code developed for
the empirical results is also available3.

2 Related Work

We brieﬂy review existing dialogue datasets, and
some of the more recent learning architectures
used for both structured and unstructured dia-
logues. This is by no means an exhaustive list
(due to space constraints), but surveys resources
most related to our contribution. A list of datasets
discussed is provided in Table 1.

2.1 Dialogue Datasets

The Switchboard dataset [8], and the Dialogue
State Tracking Challenge (DSTC) datasets [32]
have been used to train and validate dialogue man-
agement systems for interactive information re-
trieval. The problem is typically formalized as a
slot ﬁlling task, where agents attempt to predict
the goal of a user during the conversation. These
datasets have been signiﬁcant resources for struc-
tured dialogues, and have allowed major progress
in this ﬁeld, though they are quite small compared
to datasets currently used for training neural archi-
tectures.

Recently, a few datasets have been used con-
taining unstructured dialogues extracted from
Twitter4. Ritter et al. [21] collected 1.3 million
conversations; this was extended in [28] to take ad-
vantage of longer contexts by using A-B-A triples.
Shang et al. [25] used data from a similar Chinese
website called Weibo5. However to our knowl-
edge, these datasets have not been made public,
and furthermore, the post-reply format of such mi-
croblogging services is perhaps not as represen-
tative of natural dialogue between humans as the
continuous stream of messages in a chat room. In

2Note that a new version of

is now
https://github.com/rkadlec/
available:
ubuntu-ranking-dataset-creator.
This ver-
sion makes some adjustments and ﬁxes some bugs from the
ﬁrst version.

the dataset

3http://github.com/npow/ubottu
4https://twitter.com/
5http://www.weibo.com/

fact, Ritter et al. estimate that only 37% of posts
on Twitter are ‘conversational in nature’, and 69%
of their collected data contained exchanges of only
length 2 [21]. We hypothesize that chat-room style
messaging is more closely correlated to human-to-
human dialogue than micro-blogging websites, or
forum-based sites such as Reddit.

Part of the Ubuntu chat logs have previously
been aggregated into a dataset, called the Ubuntu
Chat Corpus [30]. However that resource pre-
serves the multi-participant structure and thus is
less amenable to the investigation of more tradi-
tional two-party conversations.

Also weakly related to our contribution is the
problem of question-answer systems.
Several
datasets of question-answer pairs are available [3],
however these interactions are much shorter than
what we seek to study.

2.2 Learning Architectures

Most dialogue research has historically focused
on structured slot-ﬁlling tasks [24]. Various ap-
proaches were proposed, yet few attempts lever-
age more recent developments in neural learning
architectures. A notable exception is the work of
Henderson et al. [11], which proposes an RNN
structure, initialized with a denoising autoencoder,
to tackle the DSTC 3 domain.

Work on unstructured dialogues, recently pi-
oneered by Ritter et al. [22], proposed a re-
sponse generation model for Twitter data based on
ideas from Statistical Machine Translation. This
is shown to give superior performance to previ-
ous information retrieval (e.g. nearest neighbour)
approaches [14]. This idea was further devel-
oped by Sordoni et al. [28] to exploit information
from a longer context, using a structure similar to
the Recurrent Neural Network Encoder-Decoder
model [4]. This achieves rather poor performance
on A-B-A Twitter triples when measured by the
BLEU score (a standard for machine translation),
yet performs comparatively better than the model
of Ritter et al. [22]. Their results are also veriﬁed
with a human-subject study. A similar encoder-
decoder framework is presented in [25]. This
model uses one RNN to transform the input to
some vector representation, and another RNN to
‘decode’ this representation to a response by gen-
erating one word at a time. This model is also eval-
uated in a human-subject study, although much
smaller in size than in [28]. Overall, these models

Dataset

Switchboard [8]

DSTC1 [32]

DSTC2 [10]

DSTC3 [9]

DSTC4[13]

Twitter
Corpus [21]
Twitter Triple
Corpus [28]
Sina Weibo [25]

Ubuntu Dialogue
Corpus

Type

Human-human
spoken
Human-computer
spoken
Human-computer
spoken
Human-computer
spoken
Human-human
spoken
Human-human
micro-blog
Human-human
micro-blog
Human-human
micro-blog
Human-human
chat

Task

Various

State
tracking
State
tracking
State
tracking
State
tracking
Next utterance
generation
Next utterance
generation
Next utterance
generation
Next utterance
classiﬁcation

# Dialogues

# Utterances

# Words

Description

2,400

—

3,000,000

15,000

210,000

3,000

2,265

35

24,000

15,000

—

1,300,000

3,000,000

29,000,000

87,000,000

4,435,959

8,871,918

—

—

—

—

—

—

930,000

7,100,000

100,000,000

Telephone conversations
on pre-speciﬁed topics
Bus ride information
system
Restaurant booking
system
Tourist information
system
21 hours of tourist info
exchange over Skype
Post/ replies extracted
from Twitter
A-B-A triples from
Twitter replies
Post/ reply pairs extracted
from Weibo
Extracted from Ubuntu
Chat Logs

Table 1: A selection of structured and unstructured large-scale datasets applicable to dialogue systems.
Faded datasets are not publicly available. The last entry is our contribution.

highlight the potential of neural learning architec-
tures for interactive systems, yet so far they have
been limited to very short conversations.

3 The Ubuntu Dialogue Corpus

We seek a large dataset for research in dialogue
systems with the following properties:

• Two-way (or dyadic) conversation, as op-
posed to multi-participant chat, preferably
human-human.

• Large number of conversations; 105 − 106
is typical of datasets used for neural-network
learning in other areas of AI.

• Many conversations with several turns (more

• Task-speciﬁc domain, as opposed to chatbot

than 3).

systems.

All of these requirements are satisﬁed by the
Ubuntu Dialogue Corpus presented in this paper.

3.1 Ubuntu Chat Logs

The Ubuntu Chat Logs refer to a collection of logs
from Ubuntu-related chat rooms on the Freenode
Internet Relay Chat (IRC) network. This protocol
allows for real-time chat between a large number
of participants. Each chat room, or channel, has
a particular topic, and every channel participant
can see all the messages posted in a given chan-
nel. Many of these channels are used for obtaining
technical support with various Ubuntu issues.

As the contents of each channel are moderated,
most interactions follow a similar pattern. A new
user joins the channel, and asks a general ques-
tion about a problem they are having with Ubuntu.
Then, another more experienced user replies with

a potential solution, after ﬁrst addressing the ’user-
name’ of the ﬁrst user. This is called a name men-
tion [29], and is done to avoid confusion in the
channel — at any given time during the day, there
can be between 1 and 20 simultaneous conversa-
tions happening in some channels.
In the most
popular channels, there is almost never a time
when only one conversation is occurring; this ren-
ders it particularly problematic to extract dyadic
dialogues. A conversation between a pair of users
generally stops when the problem has been solved,
though some users occasionally continue to dis-
cuss a topic not related to Ubuntu.

Despite the nature of the chat room being a con-
stant stream of messages from multiple users, it is
through the fairly rigid structure in the messages
that we can extract the dialogues between users.
Figure 4 shows an example chat room conversa-
tion from the #ubuntu channel as well as the ex-
tracted dialogues, which illustrates how users usu-
ally state the username of the intended message
recipient before writing their reply (we refer to all
replies and initial questions as ‘utterances’). For
example, it is clear that users ‘Taru’ and ‘kuja’
are engaged in a dialogue, as are users ‘Old’ and
‘bur[n]er’, while user ‘_pm’ is asking an initial
question, and ‘LiveCD’ is perhaps elaborating on
a previous comment.

3.2 Dataset Creation

In order to create the Ubuntu Dialogue Corpus,
ﬁrst a method had to be devised to extract dyadic
dialogues from the chat room multi-party conver-
sations. The ﬁrst step was to separate every mes-
sage into 4-tuples of (time, sender, recipient, utter-
ance). Given these 4-tuples, it is straightforward to

group all tuples where there is a matching sender
and recipient. Although it is easy to separate the
time and the sender from the rest, ﬁnding the in-
tended recipient of the message is not always triv-
ial.

3.2.1 Recipient Identiﬁcation
While in most cases the recipient is the ﬁrst word
of the utterance, it is sometimes located at the end,
or not at all in the case of initial questions. Fur-
thermore, some users choose names correspond-
ing to common English words, such as ‘the’ or
‘stop’, which could lead to many false positives.
In order to solve this issue, we create a dictionary
of usernames from the current and previous days,
and compare the ﬁrst word of each utterance to its
If a match is found, and the word does
entries.
not correspond to a very common English word6,
it is assumed that this user was the intended recip-
ient of the message. If no matches are found, it is
assumed that the message was an initial question,
and the recipient value is left empty.

3.2.2 Utterance Creation
The dialogue extraction algorithm works back-
wards from the ﬁrst response to ﬁnd the initial
question that was replied to, within a time frame
of 3 minutes. A ﬁrst response is identiﬁed by the
presence of a recipient name (someone from the
recent conversation history). The initial question
is identiﬁed to be the most recent utterance by the
recipient identiﬁed in the ﬁrst response.

All utterances that do not qualify as a ﬁrst re-
sponse or an initial question are discarded; initial
questions that do not generate any response are
also discarded. We additionally discard conversa-
tions longer than ﬁve utterances where one user
says more than 80% of the utterances, as these are
typically not representative of real chat dialogues.
Finally, we consider only extracted dialogues that
consist of 3 turns or more to encourage the model-
ing of longer-term dependencies.

To alleviate the problem of ‘holes’ in the dia-
logue, where one user does not address the other
explicitly, as in Figure 5, we check whether each
user talks to someone else for the duration of their
conversation. If not, all non-addressed utterances
are added to the dialogue. An example conversa-
tion along with the extracted dialogues is shown
in Figure 5. Note that we also concatenate all con-
secutive utterances from a given user.

6We use the GNU Aspell spell checking dictionary.

Figure 1: Plot of number of conversations with a
given number of turns. Both axes use a log scale.

# dialogues (human-human)
# utterances (in total)
# words (in total)
Min. # turns per dialogue
Avg. # turns per dialogue
Avg. # words per utterance
Median conversation length (min)

930,000
7,100,000
100,000,000
3
7.71
10.34
6

Table 2: Properties of Ubuntu Dialogue Corpus.

We do not apply any further pre-processing (e.g.
tokenization, stemming) to the data as released in
the Ubuntu Dialogue Corpus. However the use of
pre-processing is standard for most NLP systems,
and was also used in our analysis (see Section 4.)

3.2.3 Special Cases and Limitations

It is often the case that a user will post an ini-
tial question, and multiple people will respond to
it with different answers.
In this instance, each
conversation between the ﬁrst user and the user
who replied is treated as a separate dialogue. This
has the unfortunate side-effect of having the ini-
tial question appear multiple times in several dia-
logues. However the number of such cases is suf-
ﬁciently small compared to the size of the dataset.
Another issue to note is that the utterance post-
ing time is not considered for segmenting conver-
sations between two users. Even if two users have
a conversation that spans multiple hours, or even
days, this is treated as a single dialogue. However,
such dialogues are rare. We include the posting
time in the corpus so that other researchers may
ﬁlter as desired.

3.3 Dataset Statistics

Table 2 summarizes properties of the Ubuntu Dia-
logue Corpus. One of the most important features

of the Ubuntu chat logs is its size. This is cru-
cial for research into building dialogue managers
based on neural architectures. Another important
characteristic is the number of turns in these dia-
logues. The distribution of the number of turns is
shown in Figure 1. It can be seen that the num-
ber of dialogues and turns per dialogue follow an
approximate power law relationship.

3.4 Test Set Generation

We set aside 2% of the Ubuntu Dialogue Corpus
conversations (randomly selected) to form a test
set that can be used for evaluation of response se-
lection algorithms. Compared to the rest of the
corpus, this test set has been further processed to
extract a pair of (context, response, ﬂag) triples
from each dialogue. The ﬂag is a Boolean vari-
able indicating whether or not the response was the
actual next utterance after the given context. The
response is a target (output) utterance which we
aim to correctly identify. The context consists of
the sequence of utterances appearing in dialogue
prior to the response. We create a pair of triples,
where one triple contains the correct response (i.e.
the actual next utterance in the dialogue), and the
other triple contains a false response, sampled ran-
domly from elsewhere within the test set. The ﬂag
is set to 1 in the ﬁrst case and to 0 in the second
case. An example pair is shown in Table 3. To
make the task harder, we can move from pairs of
responses (one correct, one incorrect) to a larger
set of wrong responses (all with ﬂag=0). In our
experiments below, we consider both the case of 1
wrong response and 10 wrong responses.

Context
well, can I move the drives?
__EOS__ ah not like that

well, can I move the drives?
__EOS__ ah not like that

Response
I guess I could just
get an enclosure and
copy via USB
you can use "ps ax"
and "kill (PID #)"

Flag
1

0

Table 3: Test set example with (context, reply,
ﬂag) format. The ’__EOS__’ tag is used to denote
the end of an utterance within the context.

Since we want to learn to predict all parts of a
conversation, as opposed to only the closing state-
ment, we consider various portions of context for
the conversations in the test set. The context size is
determined stochastically using a simple formula:

c = min(t − 1, n − 1),

Here, C denotes the maximum desired context
size, which we set to C = 20. The last term is
the desired minimum context size, which we set
to be 2. Parameter t is the actual length of that
dialogue (thus the constraint that c ≤ t − 1), and
n is a random number corresponding to the ran-
domly sampled context length, that is selected to
be inversely proportional to C.

In practice, this leads to short test dialogues
having short contexts, while longer dialogues are
often broken into short or medium-length seg-
ments, with the occasional long context of 10 or
more turns.

3.5 Evaluation Metric

We consider the task of best response selection.
This can be achieved by processing the data as de-
scribed in Section 3.4, without requiring any hu-
man labels. This classiﬁcation task is an adapta-
tion of the recall and precision metrics previously
applied to dialogue datasets [24].

A family of metrics often used in language tasks
is Recall@k (denoted R@1 R@2, R@5 below).
Here the agent is asked to select the k most likely
responses, and it is correct if the true response is
among these k candidates. Only the R@1 metric
is relevant in the case of binary classiﬁcation (as
in the Table 3 example).

Although a language model that performs well
on response classiﬁcation is not a gauge of good
performance on next utterance generation, we hy-
pothesize that improvements on a model with re-
gards to the classiﬁcation task will eventually lead
to improvements for the generation task. See Sec-
tion 6 for further discussion of this point.

4 Learning Architectures for
Unstructured Dialogues

To provide further evidence of the value of
our dataset for research into neural architectures
for dialogue managers, we provide performance
benchmarks for two neural learning algorithms, as
well as one naive baseline. The approaches con-
sidered are: TF-IDF, Recurrent Neural networks
(RNN), and Long Short-Term Memory (LSTM).
Prior to applying each method, we perform stan-
dard pre-processing of the data using the NLTK7
library and Twitter tokenizer8 to parse each utter-
ance. We use generic tags for various word cat-

where n =

+ 2, η ∼ U nif (C/2, 10C)

10C
η

7www.nltk.org/
8http://www.ark.cs.cmu.edu/TweetNLP/

egories, such as names, locations, organizations,
URLs, and system paths.

To train the RNN and LSTM architectures, we
process the full training Ubuntu Dialogue Corpus
into the same format as the test set described in
Section 3.4, extracting (context, response, ﬂag)
triples from dialogues. For the training set, we
do not sample the context length, but instead con-
sider each utterance (starting at the 3rd one) as a
potential response, with the previous utterances as
its context. So a dialogue of length 10 yields 8
training examples. Since these are overlapping,
they are clearly not independent, but we consider
this a minor issue given the size of the dataset (we
further alleviate the issue by shufﬂing the training
examples). Negative responses are selected at ran-
dom from the rest of the training data.

4.1 TF-IDF

Term frequency-inverse document frequency is a
statistic that intends to capture how important a
given word is to some document, which in our case
is the context [20]. It is a technique often used in
document classiﬁcation and information retrieval.
The ‘term-frequency’ term is simply a count of the
number of times a word appears in a given context,
while the ‘inverse document frequency’ term puts
a penalty on how often the word appears elsewhere
in the corpus. The ﬁnal score is calculated as the
product of these two terms, and has the form:

tﬁdf(w, d, D) = f (w, d)×log

N
|{d ∈ D : w ∈ d}|

,

where f (w, d) indicates the number of times word
w appeared in context d, N is the total number
of dialogues, and the denominator represents the
number of dialogues in which the word w appears.
For classiﬁcation, the TF-IDF vectors are ﬁrst
calculated for the context and each of the candi-
date responses. Given a set of candidate response
vectors, the one with the highest cosine similarity
to the context vector is selected as the output. For
Recall@k, the top k responses are returned.

4.2 RNN

Recurrent neural networks are a variant of neural
networks that allows for time-delayed directed cy-
cles between units [17]. This leads to the forma-
tion of an internal state of the network, ht, which
allows it to model time-dependent data. The in-
ternal state is updated at each time step as some

Figure 2: Diagram of our model. The RNNs have
tied weights. c, r are the last hidden states from
the RNNs. ci, ri are word vectors for the context
and response, i < t. We consider contexts up to a
maximum of t = 160.

function of the observed variables xt, and the hid-
den state at the previous time step ht−1. Wx and
Wh are matrices associated with the input and hid-
den state.

ht = f (Whht−1 + Wxxt).

A diagram of an RNN can be seen in Figure 2.
RNNs have been the primary building block of
many current neural language models [22, 28],
which use RNNs for an encoder and decoder. The
ﬁrst RNN is used to encode the given context,
and the second RNN generates a response by us-
ing beam-search, where its initial hidden state is
biased using the ﬁnal hidden state from the ﬁrst
RNN. In our work, we are concerned with classi-
ﬁcation of responses, instead of generation. We
build upon the approach in [2], which has also
been recently applied to the problem of question
answering [33].

We utilize a siamese network consisting of two
RNNs with tied weights to produce the embed-
dings for the context and response. Given some
input context and response, we compute their em-
beddings — c, r ∈ Rd, respectively — by feeding
the word embeddings one at a time into its respec-
tive RNN. Word embeddings are initialized using
the pre-trained vectors (Common Crawl, 840B to-
kens from [19]), and ﬁne-tuned during training.
The hidden state of the RNN is updated at each
step, and the ﬁnal hidden state represents a sum-
mary of the input utterance. Using the ﬁnal hid-
den states from both RNNs, we then calculate the
probability that this is a valid pair:

p(ﬂag = 1|c, r, M ) = σ(cT M r + b),

where the bias b and the matrix M ∈ Rd×d are
learned model parameters. This can be thought
of as a generative approach; given some input re-
sponse, we generate a context with the product
c(cid:48) = M r, and measure the similarity to the actual
context using the dot product. This is converted
to a probability with the sigmoid function. The
model is trained by minimizing the cross entropy
of all labeled (context, response) pairs [33]:

examples. Of course, the Recall@2 and Recall@5
are not relevant in the binary classiﬁcation case9.

Method
1 in 2 R@1
1 in 10 R@1
1 in 10 R@2
1 in 10 R@5

TF-IDF
RNN LSTM
65.9% 76.8% 87.8%
41.0% 40.3% 60.4%
54.5% 54.7% 74.5%
70.8% 81.9% 92.6%

L = −

log p(ﬂagn|cn, rn, M ) +

(cid:88)

n

λ
2

||θ = ||2
F

Table 4: Results for the three algorithms using var-
ious recall measures for binary (1 in 2) and 1 in 10
(1 in 10) next utterance classiﬁcation %.

where ||θ||2
F is the Frobenius norm of θ = {M, b}.
In our experiments, we use λ = 0 for computa-
tional simplicity.

For training, we used a 1:1 ratio between true re-
sponses (ﬂag = 1), and negative responses (ﬂag=0)
drawn randomly from elsewhere in the training
set. The RNN architecture is set to 1 hidden layer
with 50 neurons. The Wh matrix is initialized us-
ing orthogonal weights [23], while Wx is initial-
ized using a uniform distribution with values be-
tween -0.01 and 0.01. We use Adam as our opti-
mizer [15], with gradients clipped to 10. We found
that weight initialization as well as the choice of
optimizer were critical for training the RNNs.

4.3 LSTM

In addition to the RNN model, we consider the
same architecture but changed the hidden units
to long-short term memory (LSTM) units [12].
LSTMs were introduced in order to model longer-
term dependencies. This is accomplished using a
series of gates that determine whether a new in-
put should be remembered, forgotten (and the old
value retained), or used as output. The error sig-
nal can now be fed back indeﬁnitely into the gates
of the LSTM unit. This helps overcome the van-
ishing and exploding gradient problems in stan-
dard RNNs, where the error gradients would oth-
erwise decrease or increase at an exponential rate.
In training, we used 1 hidden layer with 200 neu-
rons. The hyper-parameter conﬁguration (includ-
ing number of neurons) was optimized indepen-
dently for RNNs and LSTMs using a validation
set extracted from the training data.

5 Empirical Results

The results for the TF-IDF, RNN, and LSTM mod-
els are shown in Table 4. The models were eval-
uated using both 1 (1 in 2) and 9 (1 in 10) false

We observe that the LSTM outperforms both
the RNN and TF-IDF on all evaluation metrics.
It is interesting to note that TF-IDF actually out-
performs the RNN on the Recall@1 case for the
1 in 10 classiﬁcation. This is most likely due to
the limited ability of the RNN to take into account
long contexts, which can be overcome by using the
LSTM. An example output of the LSTM where the
response is correctly classiﬁed is shown in Table 5.
We also show, in Figure 3, the increase in per-
formance of the LSTM as the amount of data used
for training increases. This conﬁrms the impor-
tance of having a large training set.

Context
""any apache hax around ? i just deleted all of
__path__ - which package provides it ?",
"reconﬁguring apache do n’t solve it ?"

Ranked Responses
1. "does n’t seem to, no"
2. "you can log in but not transfer ﬁles ?"

Flag
1
0

Table 5: Example showing the ranked responses
from the LSTM. Each utterance is shown after pre-
processing steps.

6 Discussion

This paper presents the Ubuntu Dialogue Corpus,
a large dataset for research in unstructured multi-
turn dialogue systems. We describe the construc-
tion of the dataset and its properties. The availabil-
ity of a dataset of this size opens up several inter-
esting possibilities for research into dialogue sys-
tems based on rich neural-network architectures.
We present preliminary results demonstrating use
of this dataset to train an RNN and an LSTM for
the task of selecting the next best response in a

9Note that these results are on the original dataset. Results
on the new dataset should not be compared to the old dataset;
baselines on the new dataset will be released shortly.

6.3 State Tracking and Utterance Generation

The work described here focuses on the task of re-
sponse selection. This can be seen as an interme-
diate step between slot ﬁlling and utterance gener-
ation. In slot ﬁlling, the set of candidate outputs
(states) is identiﬁed a priori through knowledge
engineering, and is typically smaller than the set
of responses considered in our work. When the
set of candidate responses is close to the size of
the dataset (e.g. all utterances ever recorded), then
we are quite close to the response generation case.
There are several reasons not to proceed directly
to response generation. First, it is likely that cur-
rent algorithms are not yet able to generate good
results for this task, and it is preferable to tackle
metrics for which we can make progress. Second,
we do not yet have a suitable metric for evaluat-
ing performance in the response generation case.
One option is to use the BLEU [18] or METEOR
[16] scores from machine translation. However,
using BLEU to evaluate dialogue systems has been
shown to give extremely low scores [28], due to
the large space of potential sensible responses [7].
Further, since the BLEU score is calculated us-
ing N-grams [18], it would provide a very low
score for reasonable responses that do not have
any words in common with the ground-truth next
utterance.

Alternatively, one could measure the difference
between the generated utterance and the actual
sentence by comparing their representations in
some embedding (or semantic) space. However,
different models inevitably use different embed-
dings, necessitating a standardized embedding for
evaluation purposes. Such a standardized embed-
dings has yet to be created.

Another possibility is to use human subjects to
score automatically generated responses, but time
and expense make this a highly impractical option.
In summary, while it is possible that current lan-
guage models have outgrown the use of slot ﬁll-
ing as a metric, we are currently unable to mea-
sure their ability in next utterance generation in
a standardized, meaningful and inexpensive way.
This motivates our choice of response selection as
a useful metric for the time being.

Acknowledgments

The authors gratefully acknowledge ﬁnancial sup-
port for this work by the Samsung Advanced
Institute of Technology (SAIT) and the Natural

Figure 3: The LSTM (with 200 hidden units),
showing Recall@1 for the 1 in 10 classiﬁcation,
with increasing dataset sizes.

conversation; we obtain signiﬁcantly better results
with the LSTM architecture. There are several in-
teresting directions for future work.

6.1 Conversation Disentanglement

Our approach to conversation disentanglement
consists of a small set of rules. More sophisticated
techniques have been proposed, such as training a
maximum-entropy classiﬁer to cluster utterances
into separate dialogues [6]. However, since we
are not trying to replicate the exact conversation
between two users, but only to retrieve plausible
natural dialogues, the heuristic method presented
in this paper may be sufﬁcient. This seems sup-
ported through qualitative examination of the data,
but could be the subject of more formal evaluation.

6.2 Altering Test Set Difﬁculty

One of the interesting properties of the response
selection task is the ability to alter the task dif-
ﬁculty in a controlled manner. We demonstrated
this by moving from 1 to 9 false responses, and
by varying the Recall@k parameter. In the future,
instead of choosing false responses randomly, we
will consider selecting false responses that are
similar to the actual response (e.g. as measured by
cosine similarity). A dialogue model that performs
well on this more difﬁcult task should also manage
to capture a more ﬁne-grained semantic meaning
of sentences, as compared to a model that naively
picks replies with the most words in common with
the context such as TF-IDF.

Sciences and Engineering Research Council of
Canada (NSERC). We would like to thank Lau-
rent Charlin for his input into this paper, as well as
Gabriel Forgues and Eric Crawford for interesting
discussions.

References

[1] Y. Bengio, A. Courville, and P. Vincent.
Representation learning: A review and new
Pattern Analysis and Ma-
perspectives.
chine Intelligence, IEEE Transactions on,
35(8):1798–1828, 2013.

[2] A. Bordes, J. Weston, and N. Usunier. Open
question answering with weakly supervised
embedding models. In MLKDD, pages 165–
180. Springer, 2014.
J. Boyd-Graber, B. Satinoff, H. He, and
H. Daume. Besting the quiz master: Crowd-
sourcing incremental classiﬁcation games. In
EMNLP, 2012.

[3]

[5]

[4] K. Cho, B. van Merrienboer, C. Gulcehre,
F. Bougares, H. Schwenk, and Y. Ben-
gio. Learning phrase representations using
rnn encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078,
2014.
J. Deng, W. Dong, R. Socher, L.J. Li, K. Li,
and L. Fei-Fei. Imagenet: A large-scale hier-
archical image database. In CVPR, 2009.
[6] M. Elsner and E. Charniak. You talking to
me? a corpus and algorithm for conversa-
In ACL, pages 834–
tion disentanglement.
842, 2008.

[7] M. Galley, C. Brockett, A. Sordoni, Y. Ji,
M. Auli, C. Quirk, M. Mitchell, J. Gao, and
B. Dolan. deltableu: A discriminative metric
for generation tasks with intrinsically diverse
arXiv preprint arXiv:1506.06863,
targets.
2015.
J.J. Godfrey, E.C. Holliman, and J. Mc-
Switchboard: Telephone speech
Daniel.
corpus for research and development.
In
ICASSP, 1992.

[8]

[9] M. Henderson, B. Thomson, and J. Williams.
Dialog state tracking challenge 2 & 3, 2014.
[10] M. Henderson, B. Thomson, and J. Williams.
The second dialog state tracking challenge.
In SIGDIAL, page 263, 2014.

[11] M. Henderson, B. Thomson, and S. Young.
Word-based dialog state tracking with recur-

rent neural networks. In SIGDIAL, page 292,
2014.

[12] S. Hochreiter and J. Schmidhuber. Long
short-term memory. Neural computation,
9(8):1735–1780, 1997.

[13] Dialog state tracking challenge 4.
[14] S. Jafarpour, C. Burges, and A. Ritter. Filter,
rank, and transfer the knowledge: Learning
to chat. Advances in Ranking, 10, 2010.

[15] D.P. Kingma and J. Ba.

Adam: A
method for stochastic optimization. CoRR,
abs/1412.6980, 2014.

[16] A. Lavie and M.J. Denkowski. The ME-
TEOR metric for automatic evaluation of
Machine Translation. Machine Translation,
23(2-3):105–115, 2009.

[17] L.R. Medsker and L.C. Jain. Recurrent neu-
ral networks. Design and Applications, 2001.
[18] K. Papineni, S. Roukos, T. Ward, and W.J.
Zhu. Bleu: a method for automatic evalua-
tion of machine translation. In ACL, 2002.

[19] J. Pennington, R. Socher, and C.D. Manning.
GloVe: Global Vectors for Word Representa-
tion. In EMNLP, 2014.

[20] J. Ramos. Using tf-idf to determine word rel-
evance in document queries. In ICML, 2003.
[21] A. Ritter, C. Cherry, and W. Dolan. Unsu-
pervised modeling of twitter conversations.
2010.

[22] A. Ritter, C. Cherry, and W. Dolan. Data-
driven response generation in social media.
In EMNLP, pages 583–593, 2011.

[23] A.M. Saxe, J.L. McClelland, and S. Ganguli.
Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks.
arXiv preprint arXiv:1312.6120, 2013.
[24] J. Schatzmann, K. Georgila, and S. Young.
Quantitative evaluation of user simulation
techniques for spoken dialogue systems. In
SIGDIAL, 2005.

[25] L. Shang, Z. Lu, and H. Li.

Neural
responding machine for short-text conver-
arXiv preprint arXiv:1503.02364,
sation.
2015.

[26] B. A. Shawar and E. Atwell. Chatbots: are
In LDV Forum, vol-

they really useful?
ume 22, pages 29–49, 2007.

[27] S. Singh, D. Litman, M. Kearns, and
M. Walker. Optimizing dialogue manage-
ment with reinforcement learning: Experi-
ments with the NJFun system. Journal of

Artiﬁcial Intelligence Research, 16:105–133,
2002.

[28] A. Sordoni, M. Galley, M. Auli, C. Brock-
ett, Y. Ji, M. Mitchell, J.Y. Nie, J. Gao,
and W. Dolan. A neural network approach
to context-sensitive generation of conversa-
tional responses. 2015.

[29] D.C. Uthus and D.W. Aha. Extending word
highlighting in multiparticipant chat. Tech-
nical report, DTIC Document, 2013.

[30] D.C. Uthus and D.W Aha. The ubuntu chat
corpus for multiparticipant chat analysis. In
AAAI Spring Symposium on Analyzing Mi-
crotext, pages 99–102, 2013.

[31] H. Wang, Z. Lu, H. Li, and E. Chen. A
dataset for research on short-text conversa-
tions. In EMNLP, 2013.

[32] J. Williams, A. Raux, D. Ramachandran, and
A. Black. The dialog state tracking chal-
lenge. In SIGDIAL, pages 404–413, 2013.

[33] L. Yu, K. M. Hermann, P. Blunsom,
Deep learning for an-
arXiv preprint

and S. Pulman.
swer sentence selection.
arXiv:1412.1632, 2014.

[34] M.D. Zeiler.

Adadelta:

an adaptive
arXiv preprint

learning rate method.
arXiv:1212.5701, 2012.

Appendix A: Dialogue excerpts

Figure 4: Example chat room conversation from
the #ubuntu channel of the Ubuntu Chat Logs
(top), with the disentangled conversations for the
Ubuntu Dialogue Corpus (bottom).

Time

03:44

03:45
03:45
03:45

User

Old

kuja
Taru
bur[n]er

03:45

kuja

03:45
03:45
03:45

Taru
LiveCD
kuja

03:46

_pm

03:46
Sender

Old

Taru
Recipient

bur[n]er

Old

kuja
Taru
kuja

Taru
kuja

Taru

Taru
Kuja
Taru

Kuja
Taru

Kuja

Utterance

I dont run graphical ubuntu,
I run ubuntu server.
Taru: Haha sucker.
Kuja: ?
Old: you can use "ps ax"
and "kill (PID#)"
Taru: Anyways, you made
the changes right?
Kuja: Yes.
or killall speedlink
Taru: Then from the terminal
type: sudo apt-get update
if i install the beta version,
how can i update it when
the ﬁnal version comes out?
Kuja: I did.
Utterance

I dont run graphical ubuntu,
I run ubuntu server.
you can use "ps ax" and
"kill (PID#)"

Haha sucker.
?
Anyways, you made the
changes right?
Yes.
Then from the terminal type:
sudo apt-get update
I did.

Time

[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:22]

User

dell
cucho
RC
RC
dell
dell
RC
dell
dell

[12:22]

cucho

Utterance

well, can I move the drives?
dell: ah not like that
dell: you can’t move the drives
dell: deﬁnitely not
ok
lol
this is the problem with RAID:)
RC haha yeah
cucho, I guess I could
just get an enclosure
and copy via USB...
dell: i would advise you to get
the disk

Sender

Recipient

Utterance

dell
cucho
dell

dell
cucho

cucho

dell

dell
RC

dell

dell

RC

well, can I move the drives?
ah not like that
I guess I could just get an
enclosure and copy via USB
i would advise you to get the
disk

well, can I move the drives?
you can’t move the drives.
deﬁnitely not. this is
the problem with RAID :)
haha yeah

Figure 5: Example of before (top box) and after
(bottom box) the algorithm adds and concatenates
utterances in dialogue extraction. Since RC only
addresses dell, all of his utterances are added,
however this is not done for dell as he addresses
both RC and cucho.

The Ubuntu Dialogue Corpus: A Large Dataset for Research in
Unstructured Multi-Turn Dialogue Systems

Ryan Lowe∗*, Nissan Pow*, Iulian V. Serban† and Joelle Pineau*

*School of Computer Science, McGill University, Montreal, Canada
†Department of Computer Science and Operations Research, Universié de Montréal, Montreal, Canada

6
1
0
2
 
b
e
F
 
4
 
 
]
L
C
.
s
c
[
 
 
3
v
9
0
9
8
0
.
6
0
5
1
:
v
i
X
r
a

Abstract

This paper introduces the Ubuntu Dia-
logue Corpus, a dataset containing almost
1 million multi-turn dialogues, with a to-
tal of over 7 million utterances and 100
million words. This provides a unique re-
source for research into building dialogue
managers based on neural language mod-
els that can make use of large amounts
of unlabeled data. The dataset has both
the multi-turn property of conversations
in the Dialog State Tracking Challenge
datasets, and the unstructured nature of in-
teractions from microblog services such
as Twitter. We also describe two neural
learning architectures suitable for analyz-
ing this dataset, and provide benchmark
performance on the task of selecting the
best next response.

1

Introduction

The ability for a computer to converse in a nat-
ural and coherent manner with a human has long
been held as one of the primary objectives of artiﬁ-
cial intelligence (AI). In this paper we consider the
problem of building dialogue agents that have the
ability to interact in one-on-one multi-turn con-
versations on a diverse set of topics. We primar-
ily target unstructured dialogues, where there is
no a priori logical representation for the informa-
tion exchanged during the conversation. This is in
contrast to recent systems which focus on struc-
tured dialogue tasks, using a slot-ﬁlling represen-
tation [10, 27, 32].

We observe that in several subﬁelds of AI—
computer vision, speech recognition, machine
translation—fundamental break-throughs were
achieved in recent years using machine learning

methods, more speciﬁcally with neural architec-
tures [1]; however, it is worth noting that many
of the most successful approaches, in particular
convolutional and recurrent neural networks, were
known for many years prior.
It is therefore rea-
sonable to attribute this progress to three major
factors: 1) the public distribution of very large
rich datasets [5], 2) the availability of substantial
computing power, and 3) the development of new
training methods for neural architectures, in par-
ticular leveraging unlabeled data. Similar progress
has not yet been observed in the development of
dialogue systems. We hypothesize that this is due
to the lack of sufﬁciently large datasets, and aim
to overcome this barrier by providing a new large
corpus for research in multi-turn conversation.

The new Ubuntu Dialogue Corpus consists of
almost one million two-person conversations ex-
tracted from the Ubuntu chat logs1, used to receive
technical support for various Ubuntu-related prob-
lems. The conversations have an average of 8 turns
each, with a minimum of 3 turns. All conversa-
tions are carried out in text form (not audio). The
dataset is orders of magnitude larger than struc-
tured corpuses such as those of the Dialogue State
Tracking Challenge [32]. It is on the same scale as
recent datasets for solving problems such as ques-
tion answering and analysis of microblog services,
such as Twitter [22, 25, 28, 33], but each conversa-
tion in our dataset includes several more turns, as
well as longer utterances. Furthermore, because
it targets a speciﬁc domain, namely technical sup-
port, it can be used as a case study for the devel-
opment of AI agents in targeted applications, in
contrast to chatbox agents that often lack a well-
deﬁned goal [26].

In addition to the corpus, we present learning
architectures suitable for analyzing this dataset,
ranging from the simple frequency-inverse docu-

1These logs are available from 2004 to 2015 at http:

∗The ﬁrst two authors contributed equally.

//irclogs.ubuntu.com/

ment frequency (TF-IDF) approach, to more so-
phisticated neural models including a Recurrent
Neural Network (RNN) and a Long Short-Term
Memory (LSTM) architecture. We provide bench-
trained
mark performance of these algorithms,
with our new corpus, on the task of selecting the
best next response, which can be achieved with-
out requiring any human labeling. The dataset is
ready for public release2. The code developed for
the empirical results is also available3.

2 Related Work

We brieﬂy review existing dialogue datasets, and
some of the more recent learning architectures
used for both structured and unstructured dia-
logues. This is by no means an exhaustive list
(due to space constraints), but surveys resources
most related to our contribution. A list of datasets
discussed is provided in Table 1.

2.1 Dialogue Datasets

The Switchboard dataset [8], and the Dialogue
State Tracking Challenge (DSTC) datasets [32]
have been used to train and validate dialogue man-
agement systems for interactive information re-
trieval. The problem is typically formalized as a
slot ﬁlling task, where agents attempt to predict
the goal of a user during the conversation. These
datasets have been signiﬁcant resources for struc-
tured dialogues, and have allowed major progress
in this ﬁeld, though they are quite small compared
to datasets currently used for training neural archi-
tectures.

Recently, a few datasets have been used con-
taining unstructured dialogues extracted from
Twitter4. Ritter et al. [21] collected 1.3 million
conversations; this was extended in [28] to take ad-
vantage of longer contexts by using A-B-A triples.
Shang et al. [25] used data from a similar Chinese
website called Weibo5. However to our knowl-
edge, these datasets have not been made public,
and furthermore, the post-reply format of such mi-
croblogging services is perhaps not as represen-
tative of natural dialogue between humans as the
continuous stream of messages in a chat room. In

2Note that a new version of

is now
https://github.com/rkadlec/
available:
ubuntu-ranking-dataset-creator.
This ver-
sion makes some adjustments and ﬁxes some bugs from the
ﬁrst version.

the dataset

3http://github.com/npow/ubottu
4https://twitter.com/
5http://www.weibo.com/

fact, Ritter et al. estimate that only 37% of posts
on Twitter are ‘conversational in nature’, and 69%
of their collected data contained exchanges of only
length 2 [21]. We hypothesize that chat-room style
messaging is more closely correlated to human-to-
human dialogue than micro-blogging websites, or
forum-based sites such as Reddit.

Part of the Ubuntu chat logs have previously
been aggregated into a dataset, called the Ubuntu
Chat Corpus [30]. However that resource pre-
serves the multi-participant structure and thus is
less amenable to the investigation of more tradi-
tional two-party conversations.

Also weakly related to our contribution is the
problem of question-answer systems.
Several
datasets of question-answer pairs are available [3],
however these interactions are much shorter than
what we seek to study.

2.2 Learning Architectures

Most dialogue research has historically focused
on structured slot-ﬁlling tasks [24]. Various ap-
proaches were proposed, yet few attempts lever-
age more recent developments in neural learning
architectures. A notable exception is the work of
Henderson et al. [11], which proposes an RNN
structure, initialized with a denoising autoencoder,
to tackle the DSTC 3 domain.

Work on unstructured dialogues, recently pi-
oneered by Ritter et al. [22], proposed a re-
sponse generation model for Twitter data based on
ideas from Statistical Machine Translation. This
is shown to give superior performance to previ-
ous information retrieval (e.g. nearest neighbour)
approaches [14]. This idea was further devel-
oped by Sordoni et al. [28] to exploit information
from a longer context, using a structure similar to
the Recurrent Neural Network Encoder-Decoder
model [4]. This achieves rather poor performance
on A-B-A Twitter triples when measured by the
BLEU score (a standard for machine translation),
yet performs comparatively better than the model
of Ritter et al. [22]. Their results are also veriﬁed
with a human-subject study. A similar encoder-
decoder framework is presented in [25]. This
model uses one RNN to transform the input to
some vector representation, and another RNN to
‘decode’ this representation to a response by gen-
erating one word at a time. This model is also eval-
uated in a human-subject study, although much
smaller in size than in [28]. Overall, these models

Dataset

Switchboard [8]

DSTC1 [32]

DSTC2 [10]

DSTC3 [9]

DSTC4[13]

Twitter
Corpus [21]
Twitter Triple
Corpus [28]
Sina Weibo [25]

Ubuntu Dialogue
Corpus

Type

Human-human
spoken
Human-computer
spoken
Human-computer
spoken
Human-computer
spoken
Human-human
spoken
Human-human
micro-blog
Human-human
micro-blog
Human-human
micro-blog
Human-human
chat

Task

Various

State
tracking
State
tracking
State
tracking
State
tracking
Next utterance
generation
Next utterance
generation
Next utterance
generation
Next utterance
classiﬁcation

# Dialogues

# Utterances

# Words

Description

2,400

—

3,000,000

15,000

210,000

3,000

2,265

35

24,000

15,000

—

1,300,000

3,000,000

29,000,000

87,000,000

4,435,959

8,871,918

—

—

—

—

—

—

930,000

7,100,000

100,000,000

Telephone conversations
on pre-speciﬁed topics
Bus ride information
system
Restaurant booking
system
Tourist information
system
21 hours of tourist info
exchange over Skype
Post/ replies extracted
from Twitter
A-B-A triples from
Twitter replies
Post/ reply pairs extracted
from Weibo
Extracted from Ubuntu
Chat Logs

Table 1: A selection of structured and unstructured large-scale datasets applicable to dialogue systems.
Faded datasets are not publicly available. The last entry is our contribution.

highlight the potential of neural learning architec-
tures for interactive systems, yet so far they have
been limited to very short conversations.

3 The Ubuntu Dialogue Corpus

We seek a large dataset for research in dialogue
systems with the following properties:

• Two-way (or dyadic) conversation, as op-
posed to multi-participant chat, preferably
human-human.

• Large number of conversations; 105 − 106
is typical of datasets used for neural-network
learning in other areas of AI.

• Many conversations with several turns (more

• Task-speciﬁc domain, as opposed to chatbot

than 3).

systems.

All of these requirements are satisﬁed by the
Ubuntu Dialogue Corpus presented in this paper.

3.1 Ubuntu Chat Logs

The Ubuntu Chat Logs refer to a collection of logs
from Ubuntu-related chat rooms on the Freenode
Internet Relay Chat (IRC) network. This protocol
allows for real-time chat between a large number
of participants. Each chat room, or channel, has
a particular topic, and every channel participant
can see all the messages posted in a given chan-
nel. Many of these channels are used for obtaining
technical support with various Ubuntu issues.

As the contents of each channel are moderated,
most interactions follow a similar pattern. A new
user joins the channel, and asks a general ques-
tion about a problem they are having with Ubuntu.
Then, another more experienced user replies with

a potential solution, after ﬁrst addressing the ’user-
name’ of the ﬁrst user. This is called a name men-
tion [29], and is done to avoid confusion in the
channel — at any given time during the day, there
can be between 1 and 20 simultaneous conversa-
tions happening in some channels.
In the most
popular channels, there is almost never a time
when only one conversation is occurring; this ren-
ders it particularly problematic to extract dyadic
dialogues. A conversation between a pair of users
generally stops when the problem has been solved,
though some users occasionally continue to dis-
cuss a topic not related to Ubuntu.

Despite the nature of the chat room being a con-
stant stream of messages from multiple users, it is
through the fairly rigid structure in the messages
that we can extract the dialogues between users.
Figure 4 shows an example chat room conversa-
tion from the #ubuntu channel as well as the ex-
tracted dialogues, which illustrates how users usu-
ally state the username of the intended message
recipient before writing their reply (we refer to all
replies and initial questions as ‘utterances’). For
example, it is clear that users ‘Taru’ and ‘kuja’
are engaged in a dialogue, as are users ‘Old’ and
‘bur[n]er’, while user ‘_pm’ is asking an initial
question, and ‘LiveCD’ is perhaps elaborating on
a previous comment.

3.2 Dataset Creation

In order to create the Ubuntu Dialogue Corpus,
ﬁrst a method had to be devised to extract dyadic
dialogues from the chat room multi-party conver-
sations. The ﬁrst step was to separate every mes-
sage into 4-tuples of (time, sender, recipient, utter-
ance). Given these 4-tuples, it is straightforward to

group all tuples where there is a matching sender
and recipient. Although it is easy to separate the
time and the sender from the rest, ﬁnding the in-
tended recipient of the message is not always triv-
ial.

3.2.1 Recipient Identiﬁcation
While in most cases the recipient is the ﬁrst word
of the utterance, it is sometimes located at the end,
or not at all in the case of initial questions. Fur-
thermore, some users choose names correspond-
ing to common English words, such as ‘the’ or
‘stop’, which could lead to many false positives.
In order to solve this issue, we create a dictionary
of usernames from the current and previous days,
and compare the ﬁrst word of each utterance to its
If a match is found, and the word does
entries.
not correspond to a very common English word6,
it is assumed that this user was the intended recip-
ient of the message. If no matches are found, it is
assumed that the message was an initial question,
and the recipient value is left empty.

3.2.2 Utterance Creation
The dialogue extraction algorithm works back-
wards from the ﬁrst response to ﬁnd the initial
question that was replied to, within a time frame
of 3 minutes. A ﬁrst response is identiﬁed by the
presence of a recipient name (someone from the
recent conversation history). The initial question
is identiﬁed to be the most recent utterance by the
recipient identiﬁed in the ﬁrst response.

All utterances that do not qualify as a ﬁrst re-
sponse or an initial question are discarded; initial
questions that do not generate any response are
also discarded. We additionally discard conversa-
tions longer than ﬁve utterances where one user
says more than 80% of the utterances, as these are
typically not representative of real chat dialogues.
Finally, we consider only extracted dialogues that
consist of 3 turns or more to encourage the model-
ing of longer-term dependencies.

To alleviate the problem of ‘holes’ in the dia-
logue, where one user does not address the other
explicitly, as in Figure 5, we check whether each
user talks to someone else for the duration of their
conversation. If not, all non-addressed utterances
are added to the dialogue. An example conversa-
tion along with the extracted dialogues is shown
in Figure 5. Note that we also concatenate all con-
secutive utterances from a given user.

6We use the GNU Aspell spell checking dictionary.

Figure 1: Plot of number of conversations with a
given number of turns. Both axes use a log scale.

# dialogues (human-human)
# utterances (in total)
# words (in total)
Min. # turns per dialogue
Avg. # turns per dialogue
Avg. # words per utterance
Median conversation length (min)

930,000
7,100,000
100,000,000
3
7.71
10.34
6

Table 2: Properties of Ubuntu Dialogue Corpus.

We do not apply any further pre-processing (e.g.
tokenization, stemming) to the data as released in
the Ubuntu Dialogue Corpus. However the use of
pre-processing is standard for most NLP systems,
and was also used in our analysis (see Section 4.)

3.2.3 Special Cases and Limitations

It is often the case that a user will post an ini-
tial question, and multiple people will respond to
it with different answers.
In this instance, each
conversation between the ﬁrst user and the user
who replied is treated as a separate dialogue. This
has the unfortunate side-effect of having the ini-
tial question appear multiple times in several dia-
logues. However the number of such cases is suf-
ﬁciently small compared to the size of the dataset.
Another issue to note is that the utterance post-
ing time is not considered for segmenting conver-
sations between two users. Even if two users have
a conversation that spans multiple hours, or even
days, this is treated as a single dialogue. However,
such dialogues are rare. We include the posting
time in the corpus so that other researchers may
ﬁlter as desired.

3.3 Dataset Statistics

Table 2 summarizes properties of the Ubuntu Dia-
logue Corpus. One of the most important features

of the Ubuntu chat logs is its size. This is cru-
cial for research into building dialogue managers
based on neural architectures. Another important
characteristic is the number of turns in these dia-
logues. The distribution of the number of turns is
shown in Figure 1. It can be seen that the num-
ber of dialogues and turns per dialogue follow an
approximate power law relationship.

3.4 Test Set Generation

We set aside 2% of the Ubuntu Dialogue Corpus
conversations (randomly selected) to form a test
set that can be used for evaluation of response se-
lection algorithms. Compared to the rest of the
corpus, this test set has been further processed to
extract a pair of (context, response, ﬂag) triples
from each dialogue. The ﬂag is a Boolean vari-
able indicating whether or not the response was the
actual next utterance after the given context. The
response is a target (output) utterance which we
aim to correctly identify. The context consists of
the sequence of utterances appearing in dialogue
prior to the response. We create a pair of triples,
where one triple contains the correct response (i.e.
the actual next utterance in the dialogue), and the
other triple contains a false response, sampled ran-
domly from elsewhere within the test set. The ﬂag
is set to 1 in the ﬁrst case and to 0 in the second
case. An example pair is shown in Table 3. To
make the task harder, we can move from pairs of
responses (one correct, one incorrect) to a larger
set of wrong responses (all with ﬂag=0). In our
experiments below, we consider both the case of 1
wrong response and 10 wrong responses.

Context
well, can I move the drives?
__EOS__ ah not like that

well, can I move the drives?
__EOS__ ah not like that

Response
I guess I could just
get an enclosure and
copy via USB
you can use "ps ax"
and "kill (PID #)"

Flag
1

0

Table 3: Test set example with (context, reply,
ﬂag) format. The ’__EOS__’ tag is used to denote
the end of an utterance within the context.

Since we want to learn to predict all parts of a
conversation, as opposed to only the closing state-
ment, we consider various portions of context for
the conversations in the test set. The context size is
determined stochastically using a simple formula:

c = min(t − 1, n − 1),

Here, C denotes the maximum desired context
size, which we set to C = 20. The last term is
the desired minimum context size, which we set
to be 2. Parameter t is the actual length of that
dialogue (thus the constraint that c ≤ t − 1), and
n is a random number corresponding to the ran-
domly sampled context length, that is selected to
be inversely proportional to C.

In practice, this leads to short test dialogues
having short contexts, while longer dialogues are
often broken into short or medium-length seg-
ments, with the occasional long context of 10 or
more turns.

3.5 Evaluation Metric

We consider the task of best response selection.
This can be achieved by processing the data as de-
scribed in Section 3.4, without requiring any hu-
man labels. This classiﬁcation task is an adapta-
tion of the recall and precision metrics previously
applied to dialogue datasets [24].

A family of metrics often used in language tasks
is Recall@k (denoted R@1 R@2, R@5 below).
Here the agent is asked to select the k most likely
responses, and it is correct if the true response is
among these k candidates. Only the R@1 metric
is relevant in the case of binary classiﬁcation (as
in the Table 3 example).

Although a language model that performs well
on response classiﬁcation is not a gauge of good
performance on next utterance generation, we hy-
pothesize that improvements on a model with re-
gards to the classiﬁcation task will eventually lead
to improvements for the generation task. See Sec-
tion 6 for further discussion of this point.

4 Learning Architectures for
Unstructured Dialogues

To provide further evidence of the value of
our dataset for research into neural architectures
for dialogue managers, we provide performance
benchmarks for two neural learning algorithms, as
well as one naive baseline. The approaches con-
sidered are: TF-IDF, Recurrent Neural networks
(RNN), and Long Short-Term Memory (LSTM).
Prior to applying each method, we perform stan-
dard pre-processing of the data using the NLTK7
library and Twitter tokenizer8 to parse each utter-
ance. We use generic tags for various word cat-

where n =

+ 2, η ∼ U nif (C/2, 10C)

10C
η

7www.nltk.org/
8http://www.ark.cs.cmu.edu/TweetNLP/

egories, such as names, locations, organizations,
URLs, and system paths.

To train the RNN and LSTM architectures, we
process the full training Ubuntu Dialogue Corpus
into the same format as the test set described in
Section 3.4, extracting (context, response, ﬂag)
triples from dialogues. For the training set, we
do not sample the context length, but instead con-
sider each utterance (starting at the 3rd one) as a
potential response, with the previous utterances as
its context. So a dialogue of length 10 yields 8
training examples. Since these are overlapping,
they are clearly not independent, but we consider
this a minor issue given the size of the dataset (we
further alleviate the issue by shufﬂing the training
examples). Negative responses are selected at ran-
dom from the rest of the training data.

4.1 TF-IDF

Term frequency-inverse document frequency is a
statistic that intends to capture how important a
given word is to some document, which in our case
is the context [20]. It is a technique often used in
document classiﬁcation and information retrieval.
The ‘term-frequency’ term is simply a count of the
number of times a word appears in a given context,
while the ‘inverse document frequency’ term puts
a penalty on how often the word appears elsewhere
in the corpus. The ﬁnal score is calculated as the
product of these two terms, and has the form:

tﬁdf(w, d, D) = f (w, d)×log

N
|{d ∈ D : w ∈ d}|

,

where f (w, d) indicates the number of times word
w appeared in context d, N is the total number
of dialogues, and the denominator represents the
number of dialogues in which the word w appears.
For classiﬁcation, the TF-IDF vectors are ﬁrst
calculated for the context and each of the candi-
date responses. Given a set of candidate response
vectors, the one with the highest cosine similarity
to the context vector is selected as the output. For
Recall@k, the top k responses are returned.

4.2 RNN

Recurrent neural networks are a variant of neural
networks that allows for time-delayed directed cy-
cles between units [17]. This leads to the forma-
tion of an internal state of the network, ht, which
allows it to model time-dependent data. The in-
ternal state is updated at each time step as some

Figure 2: Diagram of our model. The RNNs have
tied weights. c, r are the last hidden states from
the RNNs. ci, ri are word vectors for the context
and response, i < t. We consider contexts up to a
maximum of t = 160.

function of the observed variables xt, and the hid-
den state at the previous time step ht−1. Wx and
Wh are matrices associated with the input and hid-
den state.

ht = f (Whht−1 + Wxxt).

A diagram of an RNN can be seen in Figure 2.
RNNs have been the primary building block of
many current neural language models [22, 28],
which use RNNs for an encoder and decoder. The
ﬁrst RNN is used to encode the given context,
and the second RNN generates a response by us-
ing beam-search, where its initial hidden state is
biased using the ﬁnal hidden state from the ﬁrst
RNN. In our work, we are concerned with classi-
ﬁcation of responses, instead of generation. We
build upon the approach in [2], which has also
been recently applied to the problem of question
answering [33].

We utilize a siamese network consisting of two
RNNs with tied weights to produce the embed-
dings for the context and response. Given some
input context and response, we compute their em-
beddings — c, r ∈ Rd, respectively — by feeding
the word embeddings one at a time into its respec-
tive RNN. Word embeddings are initialized using
the pre-trained vectors (Common Crawl, 840B to-
kens from [19]), and ﬁne-tuned during training.
The hidden state of the RNN is updated at each
step, and the ﬁnal hidden state represents a sum-
mary of the input utterance. Using the ﬁnal hid-
den states from both RNNs, we then calculate the
probability that this is a valid pair:

p(ﬂag = 1|c, r, M ) = σ(cT M r + b),

where the bias b and the matrix M ∈ Rd×d are
learned model parameters. This can be thought
of as a generative approach; given some input re-
sponse, we generate a context with the product
c(cid:48) = M r, and measure the similarity to the actual
context using the dot product. This is converted
to a probability with the sigmoid function. The
model is trained by minimizing the cross entropy
of all labeled (context, response) pairs [33]:

examples. Of course, the Recall@2 and Recall@5
are not relevant in the binary classiﬁcation case9.

Method
1 in 2 R@1
1 in 10 R@1
1 in 10 R@2
1 in 10 R@5

TF-IDF
RNN LSTM
65.9% 76.8% 87.8%
41.0% 40.3% 60.4%
54.5% 54.7% 74.5%
70.8% 81.9% 92.6%

L = −

log p(ﬂagn|cn, rn, M ) +

(cid:88)

n

λ
2

||θ = ||2
F

Table 4: Results for the three algorithms using var-
ious recall measures for binary (1 in 2) and 1 in 10
(1 in 10) next utterance classiﬁcation %.

where ||θ||2
F is the Frobenius norm of θ = {M, b}.
In our experiments, we use λ = 0 for computa-
tional simplicity.

For training, we used a 1:1 ratio between true re-
sponses (ﬂag = 1), and negative responses (ﬂag=0)
drawn randomly from elsewhere in the training
set. The RNN architecture is set to 1 hidden layer
with 50 neurons. The Wh matrix is initialized us-
ing orthogonal weights [23], while Wx is initial-
ized using a uniform distribution with values be-
tween -0.01 and 0.01. We use Adam as our opti-
mizer [15], with gradients clipped to 10. We found
that weight initialization as well as the choice of
optimizer were critical for training the RNNs.

4.3 LSTM

In addition to the RNN model, we consider the
same architecture but changed the hidden units
to long-short term memory (LSTM) units [12].
LSTMs were introduced in order to model longer-
term dependencies. This is accomplished using a
series of gates that determine whether a new in-
put should be remembered, forgotten (and the old
value retained), or used as output. The error sig-
nal can now be fed back indeﬁnitely into the gates
of the LSTM unit. This helps overcome the van-
ishing and exploding gradient problems in stan-
dard RNNs, where the error gradients would oth-
erwise decrease or increase at an exponential rate.
In training, we used 1 hidden layer with 200 neu-
rons. The hyper-parameter conﬁguration (includ-
ing number of neurons) was optimized indepen-
dently for RNNs and LSTMs using a validation
set extracted from the training data.

5 Empirical Results

The results for the TF-IDF, RNN, and LSTM mod-
els are shown in Table 4. The models were eval-
uated using both 1 (1 in 2) and 9 (1 in 10) false

We observe that the LSTM outperforms both
the RNN and TF-IDF on all evaluation metrics.
It is interesting to note that TF-IDF actually out-
performs the RNN on the Recall@1 case for the
1 in 10 classiﬁcation. This is most likely due to
the limited ability of the RNN to take into account
long contexts, which can be overcome by using the
LSTM. An example output of the LSTM where the
response is correctly classiﬁed is shown in Table 5.
We also show, in Figure 3, the increase in per-
formance of the LSTM as the amount of data used
for training increases. This conﬁrms the impor-
tance of having a large training set.

Context
""any apache hax around ? i just deleted all of
__path__ - which package provides it ?",
"reconﬁguring apache do n’t solve it ?"

Ranked Responses
1. "does n’t seem to, no"
2. "you can log in but not transfer ﬁles ?"

Flag
1
0

Table 5: Example showing the ranked responses
from the LSTM. Each utterance is shown after pre-
processing steps.

6 Discussion

This paper presents the Ubuntu Dialogue Corpus,
a large dataset for research in unstructured multi-
turn dialogue systems. We describe the construc-
tion of the dataset and its properties. The availabil-
ity of a dataset of this size opens up several inter-
esting possibilities for research into dialogue sys-
tems based on rich neural-network architectures.
We present preliminary results demonstrating use
of this dataset to train an RNN and an LSTM for
the task of selecting the next best response in a

9Note that these results are on the original dataset. Results
on the new dataset should not be compared to the old dataset;
baselines on the new dataset will be released shortly.

6.3 State Tracking and Utterance Generation

The work described here focuses on the task of re-
sponse selection. This can be seen as an interme-
diate step between slot ﬁlling and utterance gener-
ation. In slot ﬁlling, the set of candidate outputs
(states) is identiﬁed a priori through knowledge
engineering, and is typically smaller than the set
of responses considered in our work. When the
set of candidate responses is close to the size of
the dataset (e.g. all utterances ever recorded), then
we are quite close to the response generation case.
There are several reasons not to proceed directly
to response generation. First, it is likely that cur-
rent algorithms are not yet able to generate good
results for this task, and it is preferable to tackle
metrics for which we can make progress. Second,
we do not yet have a suitable metric for evaluat-
ing performance in the response generation case.
One option is to use the BLEU [18] or METEOR
[16] scores from machine translation. However,
using BLEU to evaluate dialogue systems has been
shown to give extremely low scores [28], due to
the large space of potential sensible responses [7].
Further, since the BLEU score is calculated us-
ing N-grams [18], it would provide a very low
score for reasonable responses that do not have
any words in common with the ground-truth next
utterance.

Alternatively, one could measure the difference
between the generated utterance and the actual
sentence by comparing their representations in
some embedding (or semantic) space. However,
different models inevitably use different embed-
dings, necessitating a standardized embedding for
evaluation purposes. Such a standardized embed-
dings has yet to be created.

Another possibility is to use human subjects to
score automatically generated responses, but time
and expense make this a highly impractical option.
In summary, while it is possible that current lan-
guage models have outgrown the use of slot ﬁll-
ing as a metric, we are currently unable to mea-
sure their ability in next utterance generation in
a standardized, meaningful and inexpensive way.
This motivates our choice of response selection as
a useful metric for the time being.

Acknowledgments

The authors gratefully acknowledge ﬁnancial sup-
port for this work by the Samsung Advanced
Institute of Technology (SAIT) and the Natural

Figure 3: The LSTM (with 200 hidden units),
showing Recall@1 for the 1 in 10 classiﬁcation,
with increasing dataset sizes.

conversation; we obtain signiﬁcantly better results
with the LSTM architecture. There are several in-
teresting directions for future work.

6.1 Conversation Disentanglement

Our approach to conversation disentanglement
consists of a small set of rules. More sophisticated
techniques have been proposed, such as training a
maximum-entropy classiﬁer to cluster utterances
into separate dialogues [6]. However, since we
are not trying to replicate the exact conversation
between two users, but only to retrieve plausible
natural dialogues, the heuristic method presented
in this paper may be sufﬁcient. This seems sup-
ported through qualitative examination of the data,
but could be the subject of more formal evaluation.

6.2 Altering Test Set Difﬁculty

One of the interesting properties of the response
selection task is the ability to alter the task dif-
ﬁculty in a controlled manner. We demonstrated
this by moving from 1 to 9 false responses, and
by varying the Recall@k parameter. In the future,
instead of choosing false responses randomly, we
will consider selecting false responses that are
similar to the actual response (e.g. as measured by
cosine similarity). A dialogue model that performs
well on this more difﬁcult task should also manage
to capture a more ﬁne-grained semantic meaning
of sentences, as compared to a model that naively
picks replies with the most words in common with
the context such as TF-IDF.

Sciences and Engineering Research Council of
Canada (NSERC). We would like to thank Lau-
rent Charlin for his input into this paper, as well as
Gabriel Forgues and Eric Crawford for interesting
discussions.

References

[1] Y. Bengio, A. Courville, and P. Vincent.
Representation learning: A review and new
Pattern Analysis and Ma-
perspectives.
chine Intelligence, IEEE Transactions on,
35(8):1798–1828, 2013.

[2] A. Bordes, J. Weston, and N. Usunier. Open
question answering with weakly supervised
embedding models. In MLKDD, pages 165–
180. Springer, 2014.
J. Boyd-Graber, B. Satinoff, H. He, and
H. Daume. Besting the quiz master: Crowd-
sourcing incremental classiﬁcation games. In
EMNLP, 2012.

[3]

[5]

[4] K. Cho, B. van Merrienboer, C. Gulcehre,
F. Bougares, H. Schwenk, and Y. Ben-
gio. Learning phrase representations using
rnn encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078,
2014.
J. Deng, W. Dong, R. Socher, L.J. Li, K. Li,
and L. Fei-Fei. Imagenet: A large-scale hier-
archical image database. In CVPR, 2009.
[6] M. Elsner and E. Charniak. You talking to
me? a corpus and algorithm for conversa-
In ACL, pages 834–
tion disentanglement.
842, 2008.

[7] M. Galley, C. Brockett, A. Sordoni, Y. Ji,
M. Auli, C. Quirk, M. Mitchell, J. Gao, and
B. Dolan. deltableu: A discriminative metric
for generation tasks with intrinsically diverse
arXiv preprint arXiv:1506.06863,
targets.
2015.
J.J. Godfrey, E.C. Holliman, and J. Mc-
Switchboard: Telephone speech
Daniel.
corpus for research and development.
In
ICASSP, 1992.

[8]

[9] M. Henderson, B. Thomson, and J. Williams.
Dialog state tracking challenge 2 & 3, 2014.
[10] M. Henderson, B. Thomson, and J. Williams.
The second dialog state tracking challenge.
In SIGDIAL, page 263, 2014.

[11] M. Henderson, B. Thomson, and S. Young.
Word-based dialog state tracking with recur-

rent neural networks. In SIGDIAL, page 292,
2014.

[12] S. Hochreiter and J. Schmidhuber. Long
short-term memory. Neural computation,
9(8):1735–1780, 1997.

[13] Dialog state tracking challenge 4.
[14] S. Jafarpour, C. Burges, and A. Ritter. Filter,
rank, and transfer the knowledge: Learning
to chat. Advances in Ranking, 10, 2010.

[15] D.P. Kingma and J. Ba.

Adam: A
method for stochastic optimization. CoRR,
abs/1412.6980, 2014.

[16] A. Lavie and M.J. Denkowski. The ME-
TEOR metric for automatic evaluation of
Machine Translation. Machine Translation,
23(2-3):105–115, 2009.

[17] L.R. Medsker and L.C. Jain. Recurrent neu-
ral networks. Design and Applications, 2001.
[18] K. Papineni, S. Roukos, T. Ward, and W.J.
Zhu. Bleu: a method for automatic evalua-
tion of machine translation. In ACL, 2002.

[19] J. Pennington, R. Socher, and C.D. Manning.
GloVe: Global Vectors for Word Representa-
tion. In EMNLP, 2014.

[20] J. Ramos. Using tf-idf to determine word rel-
evance in document queries. In ICML, 2003.
[21] A. Ritter, C. Cherry, and W. Dolan. Unsu-
pervised modeling of twitter conversations.
2010.

[22] A. Ritter, C. Cherry, and W. Dolan. Data-
driven response generation in social media.
In EMNLP, pages 583–593, 2011.

[23] A.M. Saxe, J.L. McClelland, and S. Ganguli.
Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks.
arXiv preprint arXiv:1312.6120, 2013.
[24] J. Schatzmann, K. Georgila, and S. Young.
Quantitative evaluation of user simulation
techniques for spoken dialogue systems. In
SIGDIAL, 2005.

[25] L. Shang, Z. Lu, and H. Li.

Neural
responding machine for short-text conver-
arXiv preprint arXiv:1503.02364,
sation.
2015.

[26] B. A. Shawar and E. Atwell. Chatbots: are
In LDV Forum, vol-

they really useful?
ume 22, pages 29–49, 2007.

[27] S. Singh, D. Litman, M. Kearns, and
M. Walker. Optimizing dialogue manage-
ment with reinforcement learning: Experi-
ments with the NJFun system. Journal of

Artiﬁcial Intelligence Research, 16:105–133,
2002.

[28] A. Sordoni, M. Galley, M. Auli, C. Brock-
ett, Y. Ji, M. Mitchell, J.Y. Nie, J. Gao,
and W. Dolan. A neural network approach
to context-sensitive generation of conversa-
tional responses. 2015.

[29] D.C. Uthus and D.W. Aha. Extending word
highlighting in multiparticipant chat. Tech-
nical report, DTIC Document, 2013.

[30] D.C. Uthus and D.W Aha. The ubuntu chat
corpus for multiparticipant chat analysis. In
AAAI Spring Symposium on Analyzing Mi-
crotext, pages 99–102, 2013.

[31] H. Wang, Z. Lu, H. Li, and E. Chen. A
dataset for research on short-text conversa-
tions. In EMNLP, 2013.

[32] J. Williams, A. Raux, D. Ramachandran, and
A. Black. The dialog state tracking chal-
lenge. In SIGDIAL, pages 404–413, 2013.

[33] L. Yu, K. M. Hermann, P. Blunsom,
Deep learning for an-
arXiv preprint

and S. Pulman.
swer sentence selection.
arXiv:1412.1632, 2014.

[34] M.D. Zeiler.

Adadelta:

an adaptive
arXiv preprint

learning rate method.
arXiv:1212.5701, 2012.

Appendix A: Dialogue excerpts

Figure 4: Example chat room conversation from
the #ubuntu channel of the Ubuntu Chat Logs
(top), with the disentangled conversations for the
Ubuntu Dialogue Corpus (bottom).

Time

03:44

03:45
03:45
03:45

User

Old

kuja
Taru
bur[n]er

03:45

kuja

03:45
03:45
03:45

Taru
LiveCD
kuja

03:46

_pm

03:46
Sender

Old

Taru
Recipient

bur[n]er

Old

kuja
Taru
kuja

Taru
kuja

Taru

Taru
Kuja
Taru

Kuja
Taru

Kuja

Utterance

I dont run graphical ubuntu,
I run ubuntu server.
Taru: Haha sucker.
Kuja: ?
Old: you can use "ps ax"
and "kill (PID#)"
Taru: Anyways, you made
the changes right?
Kuja: Yes.
or killall speedlink
Taru: Then from the terminal
type: sudo apt-get update
if i install the beta version,
how can i update it when
the ﬁnal version comes out?
Kuja: I did.
Utterance

I dont run graphical ubuntu,
I run ubuntu server.
you can use "ps ax" and
"kill (PID#)"

Haha sucker.
?
Anyways, you made the
changes right?
Yes.
Then from the terminal type:
sudo apt-get update
I did.

Time

[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:21]
[12:22]

User

dell
cucho
RC
RC
dell
dell
RC
dell
dell

[12:22]

cucho

Utterance

well, can I move the drives?
dell: ah not like that
dell: you can’t move the drives
dell: deﬁnitely not
ok
lol
this is the problem with RAID:)
RC haha yeah
cucho, I guess I could
just get an enclosure
and copy via USB...
dell: i would advise you to get
the disk

Sender

Recipient

Utterance

dell
cucho
dell

dell
cucho

cucho

dell

dell
RC

dell

dell

RC

well, can I move the drives?
ah not like that
I guess I could just get an
enclosure and copy via USB
i would advise you to get the
disk

well, can I move the drives?
you can’t move the drives.
deﬁnitely not. this is
the problem with RAID :)
haha yeah

Figure 5: Example of before (top box) and after
(bottom box) the algorithm adds and concatenates
utterances in dialogue extraction. Since RC only
addresses dell, all of his utterances are added,
however this is not done for dell as he addresses
both RC and cucho.

