6
1
0
2
 
b
e
F
 
5
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
8
5
4
0
.
1
1
5
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2016

A TEST OF RELATIVE SIMILARITY FOR MODEL
SELECTION IN GENERATIVE MODELS

Wacha Bounliphone,12∗ Eugene Belilovsky,12∗ & Matthew B. Blaschko2
1CentraleSup´elec & Inria Saclay, Universit´e Paris-Saclay, 92295 Chˆatenay-Malabry, France
2ESAT-PSI, KU Leuven, Kasteelpark Arenberg 10, 3001 Leuven, Belgium
{wacha.bounliphone,eugene.belilovsky}@inria.fr
matthew.blaschko@esat.kuleuven.be

Ioannis Antonoglou
Google Deepmind
5 New Street Square
London EC4A 3TW, UK
ioannisa@google.com

Arthur Gretton
Gatsby Computational Neuroscience Unit
University College London
25 Howland Street
London W1T 4JG, UK
arthur.gretton@gmail.com

ABSTRACT

Probabilistic generative models provide a powerful framework for representing
data that avoids the expense of manual annotation typically needed by discrimi-
native approaches. Model selection in this generative setting can be challenging,
however, particularly when likelihoods are not easily accessible. To address this
issue, we introduce a statistical test of relative similarity, which is used to de-
termine which of two models generates samples that are signiﬁcantly closer to
a real-world reference dataset of interest. We use as our test statistic the differ-
ence in maximum mean discrepancies (MMDs) between the reference dataset and
each model dataset, and derive a powerful, low-variance test based on the joint
asymptotic distribution of the MMDs between each reference-model pair. In ex-
periments on deep generative models, including the variational auto-encoder and
generative moment matching network, the tests provide a meaningful ranking of
model performance as a function of parameter and training settings.

1

INTRODUCTION

Generative models based on deep learning techniques aim to provide sophisticated and accurate
models of data, without expensive manual annotation (Bengio, 2009; Kingma et al., 2014). This
is especially of interest as deep networks tend to require comparatively large training samples to
achieve a good result (Krizhevsky et al., 2012). Model selection within this class of techniques can
be a challenge, however. First, likelihoods can be difﬁcult to compute for some families of recently
proposed models based on deep learning (Goodfellow et al., 2014; Li et al., 2015b). The current
best method to evaluate such models is based on Parzen-window estimates of the log likelihood
(Goodfellow et al., 2014, Section 5). Second, if we are given two models with similar likelihoods,
we typically do not have a computationally inexpensive hypothesis test to determine whether one
likelihood is signiﬁcantly higher than the other. Permutation testing or other generic strategies are
often computationally prohibitive, bearing in mind the relatively high computational requirements
of deep networks (Krizhevsky et al., 2012).

In this work, we provide an alternative strategy for model selection, based on a novel, non-parametric
hypothesis test of relative similarity. We treat the two trained networks being compared as generative
models (Goodfellow et al., 2014; Hinton et al., 2006; Salakhutdinov and Hinton, 2009), and test
whether the ﬁrst candidate model generates samples signiﬁcantly closer to a reference validation
set. The null hypothesis is that the ordering is reversed, and the second candidate model is closer to

∗These authors contributed equally to this work

1

Published as a conference paper at ICLR 2016

the reference (further, both samples are assumed to remain distinct from the reference, as will be the
case for any sufﬁciently complex modeling problem).

Our model selection criterion is based on the maximum mean discrepancy (MMD) (Gretton et al.,
2006; 2012a), which represents the distance between embeddings of empirical distributions in a re-
producing kernel Hilbert space (RKHS). The maximum mean discrepancy is a metric on the space of
probability distirbutions when a characteristic kernel is used (Fukumizu et al., 2008; Sriperumbudur
et al., 2010), meaning that the distribution embeddings are unique for each probability measure.
Recently, the MMD has been used in training generative models adversarially, (Li et al., 2015b;
Dziugaite et al., 2015), where the MMD measures the distance of the generated samples to some
reference target set; it has been used for statistical model criticism (Lloyd and Ghahramani, 2015);
and to minimize the effect of nuisance variables on learned representations (Louizos et al., 2016).

Rather than train a single model using the MMD distance to a reference distribution, our goal in
this work is to evaluate the relative performance of two models, by testing whether one generates
samples signiﬁcantly closer to the reference distribution than the other. This extends the applicability
of the MMD to problems of model selection and evaluation. Key to this result is a novel expression
for the joint asymptotic distribution of two correlated MMDs (between samples generated from
each model, and samples from the reference distribution). Li et al. (2015a) have derived the joint
distribution of a speciﬁc MMD estimator under the assumption that the distributions are equal. By
contrast, we derive the case in which the distributions are unequal, as is expected due to irreducible
model error.

We provide a detailed introduction to the MMD and its associated notation in Section 2. We derive
the joint asymptotic distribution of the MMDs in Section 3: this uses similar ideas to the relative
dependence test in Bounliphone et al. (2015), with the additional complexity due to there being three
independent samples to deal with, rather than a single joint sample. We formulate a hypothesis test
of relative similarity, to determine whether the difference in MMDs is statistically signiﬁcant. Our
ﬁrst test benchmark is on a synthetic data for which the ground truth is known (Section 4), where
we verify that the test performs correctly under the null and the alternative.

Finally, in Section 5, we demonstrate the performance of our test over a broad selection of model
comparison problems in the deep learning setting, by evaluating relative similarity of pairs of model
outputs to a validation set over a range of training regimes and settings. Our benchmark models in-
clude the variational auto-encoder (Kingma and Welling, 2014) and the generative moment matching
network (Li et al., 2015b). We ﬁrst demonstrate that the test performs as expected in scenarios where
the same model is trained with different training set sizes, and the relative ordering of model per-
formance is known. We then ﬁx the training set size and change various architectural parameters
of these networks, showing which models are signiﬁcantly preferred with our test. We validate the
rankings returned by the test using a separate set of data for which we compute alternate metrics for
assessing the models, such as classiﬁcation accuracy and likelihood.

2 BACKGROUND MATERIAL

In comparing samples from distributions, we use the Maximum Mean Discrepancy (MMD) (Gretton
et al., 2006; 2012a). We brieﬂy review this statistic and its asymptotic behaviour for a single pair of
samples.

Deﬁnition 1. (Gretton et al., 2012a, Deﬁnition 2: Maximum Mean Discrepancy (MMD)) Let F be
an RKHS, with the continuous feature mapping ϕ(x) ∈ F from each x ∈ X , such that the inner
product between the features is given by the kernel function k(x, x(cid:48)) := (cid:104)φ(x), φ(x(cid:48))(cid:105). Then the
squared population MMD is

MMD2(F, Px, Py) = Ex,x(cid:48) [k(x, x(cid:48))] − 2 Ex,y [k(x, y)] + Ey,y(cid:48) [k(y, y(cid:48))] .

(1)

The following theorem describes an unbiased quadratic-time estimate of the MMD, and its asymp-
totic distribution when Px and Py are different.
Theorem 1. (Gretton et al., 2012a, Lemma 6 & Corollary 16: Unbiased empirical estimate &
Asymptotic distribution of MMD2
u(F, Xm, Yn)) Deﬁne observations Xm := {x1, ..., xm} and

2

Published as a conference paper at ICLR 2016

(3)

(5)

Yn := {y1, ..., yn} independently and identically distributed (i.i.d.) from Px and Py, respectively.
An unbiased empirical estimate of MMD2(F, Px, Py) is a sum of two U -statistics and a sample
average,

MMD2

u(F, Xm, Yn) =

k(xi, xj) +

k(yi, yj)

(2)

1
n(n − 1)

n
(cid:88)

n
(cid:88)

i=1

j(cid:54)=i

1
m(m − 1)

m
(cid:88)

m
(cid:88)

i=1

j(cid:54)=i

−

2
mn

m
(cid:88)

n
(cid:88)

i=1

j=1

k(xi, yj).

Let V := (v1, ..., vm) be m i.i.d. random variables, where v := (x, y) ∼ Px × Py. When m = n,
an unbiased empirical estimate of MMD2(F, Px, Py) is

MMD2

u(F, Xm, Ym) =

1
m(m − 1)

m
(cid:88)

i(cid:54)=j

h(vi, vj)

with h(vi, vj) = k(xi, xj) + k(yi, yj) − k(xi, yj) − k(xj, yi). We assume E(h2) < ∞. When
Px (cid:54)= Py, MMD2
u(F, X, Y ) converges in distribution to a Gaussian according to
√
u(F, Xm, Yn) − MMD2(F, Px, Py)(cid:1) D−→ N (cid:0)0, σ2
m (cid:0)MMD2

(4)

XY

(cid:1)

where

uniformly at rate 1/

XY = 4 (cid:0)Ev1 [(Ev2h(v1, v2))2] − [(Ev1,v2 h(v1, v2))2](cid:1)
σ2
√
m.

A two-sample test may be constructed using the MMD as a test statistic, however when Px = Py
the statistic is degenerate, and the asymptotic distribution is a weighted sum of χ2 variables (which
can have inﬁnitely many terms, (Gretton et al., 2012a)).

By contrast, our problem setting is to determine with high signiﬁcance whether a target distribution
Px is closer to one of two candidate distributions Py, Pz, based on two empirical estimates of the
MMD and their variances. This requires us to characterize σ2
u(F, Xm, Yn) as well
as the covariance of two dependent estimates, MMD2
u(F, Xm, Zr) (the
dependence arises from the shared sample Xm). Fortunately, degeneracy does not arise if we assume
Py, Pz are each distinct from Px.

u(F, Xm, Yn) and MMD2

XY for MMD2

In the next section, we obtain the joint asymptotic distribution of two dependent MMD statistics. We
demonstrate how this joint distribution can be empirically estimated, and use the resulting parametric
form to construct a computationally efﬁcient and powerful hypothesis test for relative similarity.

3

JOINT ASYMPTOTIC DISTRIBUTION OF TWO CORRELATED MMDS AND A
RESULTING TEST STATISTIC

In this section, we derive our statistical test for relative similarity as measured by MMD. In order to
maximize the statistical efﬁciency of the test, we will reuse samples from the reference distribution,
denoted by Px, to compute the MMD estimates with two candidate distributions Py and Pz. We
consider two MMD estimates MMD2
u(F, Xm, Yn) and MMD2
u(F, Xm, Zr), and as the data sample
Xm is identical between them, these estimates will be correlated. We therefore ﬁrst derive the joint
asymptotic distribution of these two metrics and use this to construct a statistical test.
Theorem 2. We assume that Px (cid:54)= Py, Px (cid:54)= Pz, E(k(xi, xj)) < ∞, E(k(yi, yj)) < ∞ and
E(k(xi, yj)) < ∞, then
√

(cid:19)(cid:19)

(cid:19)

(cid:19)

(cid:19)(cid:19)

(cid:18)(cid:18)MMD2
MMD2

m

u(F, Xm, Yn)
u(F, Xm, Zr)

−

(cid:18)MMD2(F, Px, Py)
MMD2(F, Px, Pz)

(cid:18)(cid:18)0
0

(cid:18) σ2

XY
σXY XZ

,

σXY XZ
σ2

XZ

d−→ N

(6)

We substitute the kernel MMD deﬁnition from Equation (2), expand the terms in the expectation,
and determine their empirical estimates in order to compute the variances in practice. The proof and
additional details of the following derivations are given in Appendix A.

3

Published as a conference paper at ICLR 2016

An empirical estimate of σXY XZ in Equation (6), neglecting higher order terms, can be computed
in O(m2):

σXY XZ ≈

(cid:18)

(cid:19)2

1

(cid:18)

−

m(m − 1)2 eT ˜Kxx ˜Kxxe −
1
m(m − 1)r
1
m(m − 1)n

−

(cid:18)

eT ˜Kxxe

1
m(m − 1)
1
m2(m − 1)r
1
m2(m − 1)n

(cid:18) 1

+

mnr

eT KyxKxze −

eT KxyeeT Kxze

1
m2nr

(cid:19)

eT ˜KxxKxze −

eT ˜KxxeeT Kxze

eT ˜KxxKxye −

eT ˜KxxeeT Kxze

(7)

(cid:19)

(cid:19)

where e is a vector of 1s with appropriate size, while ˜Kxx, Kxy and Kxz refer to the kernel matri-
ces, with ˜K indicating that the diagonal entries have been set to zero (cf. Appendix A). Similarly,
Equation (5) is constructed as in Equation (7).

Based on the empirical distribution from Equation (6), we now describe a statistical test to solve the
following problem:
Problem 1 (Relative similarity test). Let Px, Py, x and y be deﬁned as above, z be an independent
random variables with distribution Pz. Given observations Xm := {x1, ..., xm}, Yn := {y1, ..., yn}
from Px, Py and Pz respectively such that Px (cid:54)= Py, Px (cid:54)= Pz,
and Zr := {z1, ..., zr} i.i.d.
we test the hypothesis that Px is closer to Pz than Py i.e. we test the null hypothesis H0:
MMD(F, Px, Py) ≤ MMD(F, Px, Pz) versus the alternative hypothesis H1: MMD(F, Px, Py) >
MMD(F, Px, Pz) at a given signiﬁcance level α

u(F, Xm, Yn) − MMD2

The test statistic MMD2
u(F, Xm, Zr) is used to compute the p-value p for
the standard normal distribution. The test statistic is obtained by rotating the joint distribution
(cf. Eq. 6) by π
4 about the origin, and integrating the resulting projection on the ﬁrst axis, in a
manner similar to Bounliphone et al. (2015). Denote the asymptotically normal distribution of
√
u(F, Xm, Zr)]T as N (µ, Σ). The resulting distribution from ro-
tating by π/4 and projecting onto the primary axis is N (cid:0)[Rµ]1, [RΣRT ]11

u(F, Xm, Yn); MMD2

m[MMD2

(cid:1) where

[Rµ]1 =

(MMD2

u(F, Xm, Yn) − MMD2

u(F, Xm, Zr))

[RΣRT ]11 =

XY + σ2

XZ − 2σXY XZ)

√

2
2
1
(σ2
2

with R is the rotation by π/4. Then, the p-values for testing H0 versus H1 are
(cid:33)

(cid:32)

MMD2

u(F, Xm, Yn) − MMD2

u(F, Xm, Zr)

p ≤ Φ

−

(cid:112)σ2

XY + σ2
where Φ is the CDF of a standard normal distribution. We have made code for performing the test
is available.1

XZ − 2σXY XZ

4 EXPERIMENTAL VALIDATION OF THE RELATIVE MMD TEST

We verify the validity of the hypothesis test described above using a synthetic data set in which we
can directly control the relative similarity between distributions. We constructed three Gaussian dis-
tributions as illustrated in Figure 1. These Gaussian distributions are speciﬁed with different means
so that we can control the degree of relative similarity between them. The question is whether the
similarity between X and Z is greater than the similarity between X and Y . In these experiments, we
used a Gaussian kernel with bandwidth selected as the median pairwise distance between data points,
and we ﬁxed µY = [−20, −20], µZ = [20, 20] and varied µX such that µX = (1 − γ)µY + γµZ, for
41 regularly spaced values of γ ∈ [0.1, 0.9] (avoiding the degenerate cases Px = Py or Px = Pz).

1Code and examples can be found at https://github.com/eugenium/MMD

(8)

(9)

(10)

4

Published as a conference paper at ICLR 2016

u(F, X, Y ) is almost equal to MMD2

Figure 3 shows the p-values of the relative similarity test for different distribution. When γ is
varying around 0.5, i.e., when MMD2
u(F, X, Z), the p-values
quickly transition from 1 to 0, indicating strong discrimination of the test. In Figure 2, we compare
the power of our test to the power of a naive test when the reference sample is split in two, and
the MMDs have no covariance: clearly, the latter simple approach does worse than ours (a similar
comparison in testing relative dependence returned the same advantage for a test based on the joint
distribution; see Bounliphone et al. (2015, Section 3)). Figure 4 shows an empirical scatter plot of
the pairs of MMD statistics along with a 2σ iso-curve of the estimated distribution, demonstrating
that the parametric Gaussian distribution is well calibrated to the empirical values. Futhermore, we
validate our derived formulas using simulations in Appendix B, where we show the p-values have
the correct distribution under the null.

s
t
s
e
t

e
h
t

f
o

r
e
w
o
P

Figure 1: Illustration of

the synthetic dataset
where X, Y and Z are respectively
Gaussian distributed with mean µX =
[0, 0]T , µY = [−20, −20]T , µZ =
[20, 20]T and with variance ( 1 0

0 1 ).

γ

Figure 2: Comparison of the power of the pro-
posed method to an independent test
analogous to Bounliphone et al. (2015,
Section 3) as a function of γ.

s
e
u
l
a
v
-
p

γ
m = 1000

Figure 3: We ﬁxed µY = [−5, −5], µZ = [5, 5] and
varied µX such that µX = (1 − γ)µY +
γµZ , for 41 regularly spaced values of γ ∈
[0.1, 0.9] versus p-values for 100 repeated
tests.

Figure 4: The empirical scatter plot of the joint MMD
statistics with m = 1000 for 200 repeated
tests, along with the 2σ iso-curve of the an-
alytical Gaussian distribution estimated by
Equation (6). The analytical distribution
closely matches the empirical scatter plot,
verifying the correctness of the variances.

5 MODEL SELECTION FOR DEEP UNSUPERVISED NEURAL NETWORKS

An important potential application of the Relative MMD can be found in recent work on unsu-
pervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014;
Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015b; Goodfellow et al.,
2014). As noted by several authors, the evaluation of generative models is a challenging open prob-
lem (Li et al., 2015b; Goodfellow et al., 2014), and the distributions of samples from these models
are very complex and difﬁcult to evaluate. The relative MMD performance can be used to compare
different model settings, or even model families, in a statistically valid framework. To compare two
models using our test, we generate samples from both, and compare these to a set of real target data
samples that were not used to train either model.

In the experiments in the sequel we focus on the recently introduced variational auto-encoder (VAE)
(Kingma and Welling, 2014) and the generative moment matching networks (GMMN) (Li et al.,

5

Published as a conference paper at ICLR 2016

(a)

(b)

Figure 5: (a) Variational auto-encoder reference model. We have 400 hidden nodes (both encoder and decoder)
and 20 latent variables in the reference model for our experiments.
(b) Auto-Encoder + GMMN
reference model. The auto-encoder (indicated in orange) is trained separately and has 1024 and
32 hidden nodes in decode and encode hidden layers. The GMMN has 10 variables generated by
the prior, and the hidden layers have 64, 256, 256, 1024 nodes in each layer respectively. In both
networks red arrows indicate the data ﬂow during sampling

2015b). The former trains an encoder and decoder network jointly minimizing a regularized vari-
ational lower bound (Kingma and Welling, 2014). While the latter class of models is purely gen-
erative minimizing an MMD based objective, this model works best when coupled with a separate
auto-encoder which reduces the dimensionality of the data. An architectural schematic for both
classes of models is provided in Fig. 5. Both these models can be trained using standard backprop-
agation (Rumelhart et al., 1988). Using the latent variable prior we can directly sample the data
distribution of these models without using MCMC procedures (Hinton et al., 2006; Salakhutdinov
and Hinton, 2009).

We use the MNIST and FreyFace datasets for our analysis (LeCun et al., 1998; Kingma and Welling,
2014; Goodfellow et al., 2014). We ﬁrst demonstrate the effectiveness of our test in a setting where
we have a theoretical basis for expecting superiority of one unsupervised model versus another.
Speciﬁcally, we use a setup where more training samples were used to create one model versus
the other. We ﬁnd that the Relative MMD framework agrees with the expected results (models
trained with more data generalize better). We then demonstrate how the Relative MMD can be used
in evaluating network architecture choices, and we show that our test strongly agrees with other
established metrics, but in contrast can provide signiﬁcance results using just the validation data
while other methods may require an additional test set.

Several practical matters must be considered when applying the Relative MMD test. The selection
of kernel can affect the quality of results, particularly more suitable kernels can give a faster conver-
gence. In this work we extend the logic of the median heuristic (Gretton et al., 2012b) for bandwidth
selection by computing the median pairwise distance between samples from Px and Py and aver-
aging that with the median pairwise distance between samples from Px and Pz, which helps to
maximize the difference between the two MMD statistics. Although the derivations for the variance
of our statistic hold for all cases, the estimates require asymptotic arguments and thus a sufﬁciently
large n. Selecting the kernel bandwidth in an appropriate range can therefore substantially increase
the power of the test at a ﬁxed sample size. While we observed the median heuristic to work well
in our experiments, there are cases where alternative choices of kernel can provide greater power:
for instance, the kernel can be chosen to maximize the expected test power on a held-out dataset
(Gretton et al., 2012b).

6

Published as a conference paper at ICLR 2016

5.1 VARIATIONAL AUTO-ENCODER SAMPLE SIZE AND ARCHITECTURE EXPERIMENTS

We use the architecture from Kingma and Welling (2014) with a hidden layer at both the encoder
and decoder and a latent variable layer as shown in Figure 5a. We use sigmoidal activation for
the hidden layers of encoder and decoder. For the FreyFace data, we use a Gaussian prior on the
latent space and data space. For MNIST, we used a Bernoulli prior for the data space. We ﬁx the
training set size of the second auto-encoder to 300 images for the FreyFace data and 1500 images
for the MNIST data. We vary the number of training samples for the ﬁrst auto-encoder. We then
generate samples from both auto-encoders and compare them using Relative MMD to a held out
set of data. We use 1500 FreyFace samples as the target in Relative MMD and 15000 images from
MNIST. Since a single sample of the data might lead to better generalization performance by chance,
we repeat this experiment multiple times and record whether the relative similarity test indicated a
network is preferred or if it failed to reject the null hypothesis. The results are shown in Figure 6
which demonstrates that we are closely following the expected model preferences. Additionally for
MNIST we use another separate set of supervised training and test data. We encode this data using
both auto-encoders and use logistic regression to obtain a classiﬁcation accuracy. The indicated
accuracies closely match the results of the relative similarity test, further validating the test.

(a)

(c)

(b)

Figure 6: We show the effect of (a) varying the train-
ing set size of one auto-encoder trained on
MNIST data. (c) As a secondary valida-
tion we compute the classiﬁcation accu-
racy of MNIST on a separate train/test set
encoded using encoder 1 and encoder 2.
(b) We then show the effect of varying the
training set size of one auto-encoder using
the FreyFace data. We note that due to the
size of the FreyFace dataset, we limit the
range of ratios used. From this ﬁgure we
see that the results of the relative similarity
test match our expectation: more data pro-
duces models which more closely match
the true distribution.

We consider model selection between networks using different architectures. We train two encoders,
one a ﬁxed reference model (400 hidden units and 20 latent variables), and the other varying as
speciﬁed in Table 1. 25000 images from the MNIST data set were used for training. We use another
20000 images as the target data in Relative MMD. Finally, we use a set of 10000 training and 10000
test images for a supervised task experiment. We use the labels in the MNIST data and perform
training and classiﬁcation using an (cid:96)2-regularized logistic regression on the encoded features. In
addition we use the supervised task test data to evaluate the variational lower bound of the data under
the two models (Kingma and Welling, 2014). We show the result of this experiment in Table 1. For
each comparison we take a different subset of training data which helps demonstrate the variation in

7

Published as a conference paper at ICLR 2016

lower bound and accuracy when re-training the reference architecture. We use a signiﬁcance value
of 5% and indicate when the test favors one auto-encoder over another or fails to reject the null
hypothesis. We ﬁnd that Relative MMD evaluation of the models closely matches performance on
the supervised task and the test set variational lower bound.

Accuracy (%) Accuracy (%) Lower Bound Lower Bound

Hidden Latent Result
VAE 1
200
200
400
800
800

VAE 1 RelativeMMD VAE 1
Favor VAE 2
5
Favor VAE 2
20
Favor VAE 1
50
Favor VAE 1
20
Favor VAE 1
50

92.8 ± 0.3
92.6 ± 0.3
94.6 ± 0.2
94.8 ± 0.2
94.2 ± 0.3

VAE 2
94.7 ± 0.2
94.5 ± 0.2
94.0 ± 0.2
93.9 ± 0.2
94.5 ± 0.2

VAE 1
-126
-115
-99.6
-111
-101

VAE 2
-97
-105
-123.44
-115
-103

Table 1: We compare several variational auto encoder (VAE) architectural choices for the number of hidden
units in both decoder and encoder and the number of latent variables for the VAE. The reference
encoder, denoted encoder 2, has 400 hidden units and 20 latent variables. We denote the competing
architectural models as encoder 1. We vary the number of hidden nodes in both the decoder and
encoder and the number of latent variables. Our test closely follows the performance difference of the
auto-encoder on a supervised task (MNIST digit classiﬁcation) as well as the variational lower bound
on a withheld set of data. The data used for evaluating the Accuracy and Lower Bound is separate
from that used to train the auto-encoders and for the hypothesis test.

5.2 GENERATIVE MOMENT MATCHING NETWORKS ARCHITECTURE EXPERIMENTS

We demonstrate our hypothesis test on a different class of deep generative models called Generative
Moment Matching Networks (GMMN) Li et al. (2015b). This recently introduced model has shown
competitive performance in terms of test set likelihood on the MNIST data. Furthermore the training
of this model is based on the MMD criterion. Li et al. (2015b) proposes to use that model along with
an auto-encoder, which is the setup we employ in this work. Here a standard auto-encoder model is
trained on the data to obtain a low dimensional representation, then a GMMN network is trained on
the latent representations (Figure 5).

We use the relative similarity test to evaluate various architectural choices in this new class of mod-
els. We start from the baseline model speciﬁed in Li et al. (2015b) and associated software. The
details of the reference model are speciﬁed in Figure 5.

We vary the number of auto-encoder hidden layers (1 to 4), generative model layers(1, 4, or 5), the
number of network nodes (all or 50% of the reference model), and use of drop-out on the auto-
encoder. We use the same training set of 55000, validation set of 5000 and test set of 10000 as in
(Li et al., 2015b; Goodfellow et al., 2014). In total we train 48 models. We use these to compare
4 simpliﬁed binary network architecture choices using the Relative MMD: using dropout on the
auto-encoder, few (1) or more (4 or 5) GMMN layers, few (1 or 2) or more (3 or 4) auto-encoder
layers, and the number of network nodes. We use our test to compare these model settings using
the validation set as the target in the relative similarity test, and samples from the models as the two
sources. To validate our results we compare it to likelihoods computed on the test set. The results
are shown in Table 2. We see that the likelihood results computed on a separate test set follow the
conclusions obtained from MMD on the validation set. Particularly, we ﬁnd that using fewer hidden
layers for the GMMN and more hidden nodes generally produces better models.

5.3 DISCUSSION

In these experiments we have seen that the Relative MMD test can be used to compare deep gen-
erative models obtaining judgements aligned with other metrics. Comparisons to other metrics are
important for verifying our test is sensible, but it can occlude the fact that MMD is a valid eval-
uation technique on its own. When evaluating only sample generating models where likelihood
computation is not possible, MMD is an appropriate and tractable metric to consider in addition to
Parzen-Window log likelihoods and visual appearance of the samples. In several ways it is poten-
tially more appropriate than Parzen-windows as it allows one to consider directly the discrepancy
between the test data samples and the model samples while allowing for signiﬁcance results. In such
a situation, comparing the performance of several models using the MMD against a single set of test

8

Published as a conference paper at ICLR 2016

Experimental Condition (A/B)
Dropout/No Dropout
More/Fewer GMMN Layers
More/Fewer Nodes
More/Fewer AE layers

RelativeMMD Preference
B
Inconclusive
A
−9.01 ± 55.43
360
17
199
393 −73.99 ± 40.96
14
105
125.2 ± 43.4
113
13
450
41.78 ± 44.07
324
21
231

76.76 ± 42.83
249.6 ± 8.07
−57 ± 49.57
25.96 ± 55.85

Avg Likelihood A Avg Likelihood B

Table 2: For each experimental condition (e.g. dropout or no dropout) we show the number of times the Relative
MMD prefers models in group 1 or 2 and number of inconclusive tests. We use the validation set as the
target data for Relative MMD. An average likelihood for the MNIST test set for each group is shown
with error bars. We can see that the MMD choices are in agreement with likelihood evaluations.
Particularly we identify that models with fewer GMMN layers and models with more nodes have
more favourable samples, which is conﬁrmed by the likelihood results.

samples, the Relative MMD test can provide an automatic signiﬁcance value without expensive
cross-validation procedures.

Gaussian kernels are closely related to Parzen-window estimates, thus computing an MMD in this
case can be considered related to comparing Parzen window log-likelihoods. The MMD gives sev-
eral advantages, however. First, the asymptotics of MMD are quite different to Parzen-windows,
since the Parzen-window bandwidth shrinks as n grows. Asymptotics of relative tests with shrink-
ing bandwidth are unknown: even for two samples this is challenging (Krishnamurthy et al., 2015).
Other two sample tests are not easily extendable to relative tests (Rosenbaum, 2005; Friedman and
Rafsky, 1979; Hall and Tajvidi, 2002). This is because the tests above rely on graph edge counting
or nearest neighbor-type statistics, and null distributions are obtained via combinatorial arguments
which are not easily extended from two to three samples. MMD is a U -statistic, hence its asymptotic
behavior is much more easily generalised to multiple dependent statistics.

There are two primary advantages of the MMD over the variational lower bound, where it is known
(Kingma and Welling, 2014): ﬁrst, we have a characterization of the asymptotic behavior, which
allows us to determine when the difference in performance is signiﬁcant; second, comparing two
lower bounds produced from two different models is unreliable, as we do not know how conservative
either lower bound is.

6 CONCLUSION

We have described a novel non-parametric statistical hypothesis test for relative similarity based on
the Maximum Mean Discrepancy. The test is consistent, and the computation time is quadratic.
Our proposed test statistic is theoretically justiﬁed for the task of comparing samples from arbitrary
distributions as it can be shown to converge to a quantity which compares all moments of the two
pairs of distributions.

We evaluate test performance on synthetic data, where the degree of similarity can be controlled.
Our experimental results on model selection for deep generative networks show that Relative MMD
can be a useful approach to comparing such models. There is a strong correspondence between the
test resuts and the expected likelihood, prediction accuracy, and variational lower bounds on the
models tested. Moreover, our test has the advantage over these alternatives of providing guarantees
of statistical signiﬁcance to its conclusions. This suggests that the relative similarity test will be
useful in evaluating hypotheses about network architectures, for example that AE-GMMN models
may generalize better when fewer layers are used in the generative model. Code for our method is
available.2

ACKNOWLEDGMENTS

We thank Joel Veness for helpful comments. This work is partially funded by Internal Funds KU
Leuven, ERC Grant 259112, FP7-MC-CIG 334380, the Royal Academy of Engineering through the
Newton Alumni Scheme, and DIGITEO 2013-0788D-SOPRANO. WB is supported by a Centrale-
Sup´elec fellowship.

2 https://github.com/eugenium/MMD

9

Published as a conference paper at ICLR 2016

REFERENCES

1–127, 2009.

Y. Bengio. Learning deep architectures for AI. Foundations and trends in Machine Learning, 2(1):

Y. Bengio, E. Thibodeau-Laufer, G. Alain, and J. Yosinski. Deep generative stochastic networks
trainable by backprop. In Proceedings of the 31st International Conference on Machine Learning,
2014.

W. Bounliphone, A. Gretton, A. Tenenhaus, and M. B. Blaschko. A low variance consistent test
of relative dependency. In F. Bach and D. Blei, editors, Proceedings of The 32nd International
Conference on Machine Learning, volume 37 of JMLR Workshop and Conference Proceedings,
pages 20–29, 2015.

G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training generative neural networks via maximum
mean discrepancy optimization. In Conference on Uncertainty in Artiﬁcial Intelligence, 2015.

J. H. Friedman and L. C. Rafsky. Multivariate generalizations of the Wald-Wolfowitz and Smirnov

two-sample tests. The Annals of Statistics, pages 697–717, 1979.

K. Fukumizu, A. Gretton, X. Sun, and B. Sch¨olkopf. Kernel measures of conditional dependence.

pages 489–496, Cambridge, MA, 2008. MIT Press.

I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems,
pages 2672–2680, 2014.

A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. J. Smola. A kernel method for
the two-sample-problem. In Advances in neural information processing systems, pages 513–520,
2006.

A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch¨olkopf, and A. Smola. A kernel two-sample test.

The Journal of Machine Learning Research, 13(1):723–773, 2012a.

A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, K. Fukumizu, and B. K.
Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. In F. Pereira, C. Burges,
L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25,
pages 1205–1213. 2012b.

P. Hall and N. Tajvidi. Permutation tests for equality of distributions in high-dimensional settings.

Biometrika, 89(2):359–374, 2002.

G. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural

computation, 18(7):1527–1554, 2006.

W. Hoeffding. A class of statistics with asymptotically normal distribution. The annals of mathe-

matical statistics, pages 293–325, 1948.

D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on

Learning Representations, 2014.

D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling. Semi-supervised learning with deep
generative models. In Advances in Neural Information Processing Systems, pages 3581–3589,
2014.

A. Krishnamurthy, K. Kandasamy, B. P´oczos, and L. A. Wasserman. On estimating L2

2 divergence.
In Proceedings of the Eighteenth International Conference on Artiﬁcial Intelligence and Statistics,
2015.

A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012.

H. Larochelle and I. Murray. The neural autoregressive distribution estimator. Journal of Machine

Learning Research, 15:29–37, 2011.

10

Published as a conference paper at ICLR 2016

Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

S. Li, Y. Xie, H. Dai, and L. Song. M-statistic for kernel change-point detection. In Advances in

Neural Information Processing Systems, pages 3348–3356, 2015a.

Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In International Confer-

ence on Machine Learning, pages 1718–1727, 2015b.

J. R. Lloyd and Z. Ghahramani. Statistical model criticism using kernel two sample tests. In Ad-

vances in Neural Information Processing Systems, 2015.

C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel. The variational fair auto encoder. In

International Conference on Learning Representations, 2016.

P. R. Rosenbaum. An exact distribution-free test comparing two multivariate distributions based on
adjacency. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(4):
515–530, 2005.

D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating
errors. In J. A. Anderson and E. Rosenfeld, editors, Neurocomputing: Foundations of Research,
pages 696–699. MIT Press, Cambridge, MA, USA, 1988.

R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In International Conference on

Artiﬁcial Intelligence and Statistics, pages 448–455, 2009.

R. J. Serﬂing. Approximation theorems of mathematical statistics, volume 162. John Wiley & Sons,

2009.

B. K. Sriperumbudur, A. Gretton, K. Fukumizu, G. R. Lanckriet, and B. Sch¨olkopf. Hilbert space
embeddings and metrics on probability measures. Journal of Machine Learning Research, 11:
1517–1561, 2010.

A DETAILED DERIVATIONS OF THE TEST VARIANCE AND COVARIANCE

The variance and the covariance for a U -statistic is described in Hoeffding (1948, Eq. 5.13) and
Serﬂing (2009, Chap. 5).

Let V := (v1, ..., vm) be m iid random variables where v := (x, y) ∼ Px × Py. An unbiased
estimator of MMD2(F, Px, Py) is

MMD2

u(F, Xm, Yn) =

1
m(m − 1)

m
(cid:88)

i(cid:54)=j

h(vi, vj)

with h(vi, vj) = k(xi, xj) + k(yi, yj) − k(xi, yj) − k(xj, yi).

Similarly, let W := (w1, ..., wm) be m iid random variables where w := (x, z) ∼ Px × Pz. An
unbiased estimator of MMD2(F, Px, Pz) is

MMD2

u(F, Xm, Zr) =

m
(cid:88)

1
m(m − 1)

g(wi, wj)

i(cid:54)=j
with g(wi, wj) = k(xi, xj) + k(zi, zj) − k(xi, zj) − k(xj, zi)

Then the variance/covariance for a U -statistic with a kernel of order 2 is given by

Equation (13) with neglecting higher terms can be written as

V ar(MMD2

u) =

4(m − 2)
m(m − 1)

ζ1 +

2
m(m − 1)

ζ2

V ar(MMD2

u) =

4(m − 2)
m(m − 1)

ζ1 + O(m−2)

(11)

(12)

(13)

(14)

where for the variance term, ζ1 = Var [Ev1 [h(v1, V2)]] and for the covariance term ζ1 =
Var [Ev1,w1 [h(v1, V2)g(w1, W2)]].

11

Published as a conference paper at ICLR 2016

Notation [ ˜Kxx]ij = [Kxx]ij for all i (cid:54)= j and [ ˜Kxx(cid:48)]ij = 0 for j = i. Same for ˜Kyy and ˜Kzz.
We will also make use of the fact that k(xi, xj) = (cid:104)φ(xi), φ(xj)(cid:105) for an appropriately chosen inner
product, and function φ. We then denote

(cid:90)

µx :=

φ(x)dPx.

(15)

A.1 VARIANCE OF MMD

We note many terms in expansion of the squares above cancel out due to independence. For example
Ex1,y1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(x1), µy(cid:105)] − Ey1 [(cid:104)φ(y1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µy(cid:105)] = 0.
We can thus simplify to the following expression for ζ1

(cid:104)

(Ex2,y2 [h(x1, y1)])2(cid:105)
(16)
(cid:2)((cid:104)φ(x1), µx(cid:105) + (cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)µx, φ(y1)(cid:105))2(cid:3) − (cid:0)MMD2(F, PX , PY )(cid:1)2
(17)

− (cid:0)MMD2(F, PX , PY )(cid:1)2

ζ1 = Ex1,y1

= Ex1,y1

= Ex1,y1

(cid:2)(cid:104)φ(x1), µx(cid:105)2 + 2(cid:104)φ(x1), µx(cid:105)(cid:104)φ(y1), µy(cid:105) − 2(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µy(cid:105)
− 2(cid:104)φ(x1), µx(cid:105)(cid:104)φ(y1), µx(cid:105) + (cid:104)φ(y1), µy(cid:105)2
− 2(cid:104)φ(y1), µy(cid:105)(cid:104)φ(x1), µy(cid:105) − 2(cid:104)φ(y1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)
+ (cid:104)φ(x1), µy(cid:105)2 + 2(cid:104)φ(x1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)
+ (cid:104)φ(y1), µx(cid:105)2(cid:3) − (cid:0)MMD2(F, PX , PY )(cid:1)2

= Ex1 [(cid:104)φ(x1), µx(cid:105)2] − Ex1[(cid:104)φ(x1), µx(cid:105)]2

− 2(Ex1[(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µy(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ex1 [(cid:104)φ(x1), µy(cid:105)])
+ Ey1 [(cid:104)φ(y1), µy(cid:105)2] − Ey1 [(cid:104)φ(y1), µy(cid:105)]2
− 2(Ey1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)] − Ey1[(cid:104)φ(y1), µy(cid:105)]Ey1[(cid:104)φ(y1), µx(cid:105)])
+ Ex1[(cid:104)φ(x1), µy(cid:105)2] − Ex1 [(cid:104)φ(x1), µy(cid:105)]2
+ Ey1 [(cid:104)φ(y1), µx(cid:105)2] − Ey1 [(cid:104)φ(y1), µx(cid:105)]2

Substituting empirical expectations over the data sample for the population expectations in Eq. (19)
gives

(18)

(19)

(20)

ζ1 ≈

1

m(m − 1)2 eT ˜Kxx ˜Kxxe −
(cid:18)

− 2

1
m(m − 1)n

(cid:18)

1
m(m − 1)

(cid:19)2

eT ˜Kxxe

eT ˜KxxKxye −

1
m2(m − 1)n

eT ˜KxxeeT Kxye

(cid:19)

(cid:18)

1

+

n(n − 1)2 eT ˜Kyy ˜Kyye −
(cid:18)

− 2

1
n(n − 1)m

eT ˜KyyKyxe −

(cid:19)2

eT ˜Kyye

1
n(n − 1)
1
n2(n − 1)m
(cid:19)2

+

1
m2n

eT ˜KyyeeT Kxye

(cid:19)

+

1
n2m

eT KyxKxye − 2

eT Kxye

eT KxyKyxe

(cid:18) 1
nm

12

Published as a conference paper at ICLR 2016

Derivation of the ﬁrst term for example

Ex1[(cid:104)x1, µx(cid:105)2] ≈

(cid:104)φ(xi),

φ(xj)(cid:105)(cid:104)φ(xi),

φ(xk)(cid:105)

(21)

1
m − 1

m
(cid:88)

k=1
k(cid:54)=i

k(xi, xj)k(xi, xk)

1
m

m
(cid:88)

i=1

=

1
m(m − 1)2

1
m − 1

m
(cid:88)

j=1
j(cid:54)=i

m
(cid:88)

m
(cid:88)

m
(cid:88)

i=1

j=1
j(cid:54)=i

k=1
k(cid:54)=i

1

=

m(m − 1)2 eT ˜Kxx ˜Kxxe

A.2 COVARIANCE OF MMD

We note many terms in expansion of the squares above cancel out due to independence. For example
Ex1,z1 [(cid:104)φ(x1), µx(cid:105)(cid:104)φ(z1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ez1 [(cid:104)φ(z1), µz(cid:105)] = 0.
We can thus simplify to the following expression for ζ1

ζ1 = Ex1,y1,z1 [Ex2,y2,z2 [h(x1, y1)g(x1, z1)]] − (cid:0)MMD2(F, PX , PY ) MMD2(F, PX , PZ)(cid:1)

(22)

(23)

= Ex1,y1,z1[((cid:104)φ(x1), µx(cid:105) + (cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)φ(x1), µy)(cid:105))

((cid:104)φ(x1), µx)(cid:105) + (cid:104)φ(z1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105))]

− MMD2(F, PX , PY ) MMD2(F, PX , PZ)

= Ex1

(cid:2)(cid:104)φ(x1), µx(cid:105)2(cid:3) − Ex1 [(cid:104)φ(x1), µx(cid:105)]2

− (Ex1 [(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)])
− (Ex1 [(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µy(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ex1 [(cid:104)φ(x1), µy(cid:105)])
+ Ex1 [(cid:104)φ(x1), µy(cid:105)(cid:104)φ(x1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)]

(cid:18)

(cid:19)2

≈

1

(cid:18)

−

m(m − 1)2 eT ˜Kxx ˜Kxxe −
1
m(m − 1)r
1
m(m − 1)n

−

(cid:18)

eT ˜Kxxe

1
m(m − 1)
1
m2(m − 1)r
1
m2(m − 1)n

(cid:18) 1

+

mnr

eT KyxKxze −

eT KxyeeT Kxze

1
m2nr

(cid:19)

eT ˜KxxKxze −

eT ˜KxxeeT Kxze

eT ˜KxxKxye −

eT ˜KxxeeT Kxze

(cid:19)

(cid:19)

A.3 DERIVATION OF THE VARIANCE OF THE DIFFERENCE OF TWO MMD STATISTICS

In this section we propose an alternate strategy of deriving directly the variance of a u-statistic of
the difference of MMDs with a joint variable. This formulation agrees with the derivation of the
covariance matrix and subsequent projection, and provides extra insights.

Let D := (d1, ..., dm) be m iid random variables where d := (x, y, z) ∼ Px × Py × Pz. Then the
difference of the unbiased estimators of MMD2(F, Px, Py) and MMD2(F, Px, Pz) is given by

MMD2

u(F, x, y) − MMD2

u(F, x, z) =

f (di, dj)

(24)

1
m(m − 1)

m
(cid:88)

i(cid:54)=j

with f , the kernel of MMD2(F, Px, Py) − MMD2(F, Px, Pz) of order 2 as follows

f (d1, d2) = (k(x1, x2) + k(y1, y2) − k(x1, y2) − k(x2, y1))

− (k(x1, x2) + k(z1, z2) − k(x1, z2) − k(x2, z1))

= (k(y1, y2) − k(x1, y2) − k(x2, y1)) − (k(z1, z2) − k(x1, z2) − k(x2, z1))

(25)

13

Published as a conference paper at ICLR 2016

Equation (24) is a U -statistic and thus we can apply Equation (14) to obtain its variance. We ﬁrst
note

Ed1(f (d1, d2)) :=(cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)µx, φ(y1)(cid:105)

− ((cid:104)φ(z1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105) − (cid:104)µx, φ(z1)(cid:105))

Ed1,d2(f (d1, d2)) := MMD2(F, Px, Py) − MMD2(F, Px, Pz)

We are now ready to derive the dominant leading term,ζ1, in the variance expression (14).

Term ζ1
ζ1 : = Var(Ed1 (f (d1, d2)))

= Ex1,y1,z1[((cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)µx, φ(y1)(cid:105) − ((cid:104)φ(z1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105) − (cid:104)µx, φ(z1)(cid:105))2]

− (MMD2(F, Px, Py) − MMD2(F, Px, Pz))2

(29)
We note many terms in expansion of the squares above cancel out due to independence. For example
Ey1,z1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(z1), µz(cid:105)] − Ey1[(cid:104)φ(y1), µy(cid:105)] Ez1 [(cid:104)φ(z1), µz(cid:105)] = 0.
We can thus simplify to the following expression for ζ1
ζ1 = Ey1 [(cid:104)φ(y1), µy(cid:105)2] − Ey1[(cid:104)φ(y1), µy(cid:105)]2

(30)

(26)

(27)

(28)

+ Ex1[(cid:104)φ(x1), µy(cid:105)2] − Ex1[(cid:104)φ(x1), µy(cid:105)]2
+ Ey1 [(cid:104)µx, φ(y1)(cid:105)2] − Ey1[(cid:104)µx, φ(y1)(cid:105)]2
+ Ez1 [(cid:104)φ(z1), µz(cid:105)2] − Ez1[(cid:104)φ(z1), µz(cid:105)]2
+ Ex1[(cid:104)φ(x1), µz(cid:105)2] − Ex1[(cid:104)φ(x1), µz(cid:105)]2
+ Ez1 [(cid:104)µx, φ(z1)(cid:105)2] − Ez1 [(cid:104)µx, φ(z1)(cid:105)]2
− 2(Ey1[(cid:104)φ(y1), µy(cid:105)(cid:104)µx, φ(y1)(cid:105)] − Ey1 [(cid:104)φ(y1), µy(cid:105)] Ey1[(cid:104)µx, φ(y1)(cid:105)])
− 2(Ex1 [(cid:104)φ(x1), µy(cid:105)(cid:104)φ(x1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)])
− 2(Ez1[(cid:104)φ(z1), µz(cid:105)(cid:104)µx, φ(z1)(cid:105)] − Ez1 [(cid:104)φ(z1), µz(cid:105)] Ez1[(cid:104)µx, φ(z1)(cid:105)])

We can empirically approximate these terms as follows:

1

ζ1 ≈

n(n − 1)2 eT ˜Kyy ˜Kyye −

(cid:18)

1
n(n − 1)

(cid:19)2

eT ˜Kyye

(31)

+

+

+

+

+

(cid:19)2

(cid:19)2

(cid:18) 1
nm
(cid:18) 1
nm
(cid:18)

1
n2m

eT K T

xyKxye −

eT Kxye

1
nm2 eT KxyK T

xye −

1

r(r − 1)2 eT ˜Kzz ˜Kzze −
(cid:18) 1
1
rm2 eT KxzK T
rm
(cid:18) 1
1
r2m
rm
(cid:18)

xzKxze −

eT K T

xze −

eT Kxye

1
r(r − 1)
(cid:19)2

eT Kxze

(cid:19)2

eT Kxze

(cid:19)2

eT ˜Kzze

− 2

− 2

− 2

1
n(n − 1)m

(cid:18) 1

(cid:18)

nmr
1
r(r − 1)m

eT ˜KyyKyxe −

eT ˜Kyye ×

eT Kxye

1
n(n − 1)

eT ˜K T

xyKxze −

eT Kxye ×

eT Kxze

1
rm

eT ˜KzzK T

xze −

1
n(n − 1)

eT ˜Kyye ×

eT Kxye

1
nm
(cid:19)

1
nm

(cid:19)

(cid:19)

1
nm

14

Published as a conference paper at ICLR 2016

A.4 EQUALITY OF DERIVATIONS

In this section, we prove that Equation (9) is equal to the variance of the difference of 2
MMD2(F, Px, Py) and MMD2(F, Px, Pz).

σ2
XY + σXZ − 2σXY XZ = Ey1

(cid:2)(cid:104)φ(y1), µy(cid:105)2(cid:3) − Ey1 [(cid:104)φ(y1), µy(cid:105)]2

(32)

(cid:2)(cid:104)φ(z1), µy(cid:105)2(cid:3) − Ez1 [(cid:104)φ(z1), µy(cid:105)]2

+ Ez1
− 2 (Ey1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)] − Ey1 [(cid:104)φ(y1), µy(cid:105)] Ey1 [(cid:104)φ(y1), µx(cid:105)])
− 2 (Ez1 [(cid:104)φ(z1), µz(cid:105)(cid:104)φ(z1), µx(cid:105)] − Ez1 [(cid:104)φ(z1), µz(cid:105)] Ez1 [(cid:104)φ(z1), µx(cid:105)])
(cid:2)(cid:104)φ(x1), µy(cid:105)2(cid:3) − Ex1 [(cid:104)φ(x1), µy(cid:105)]2
+ Ex1
(cid:2)(cid:104)φ(y1), µz(cid:105)2(cid:3) − Ey1 [(cid:104)φ(y1), µz(cid:105)]2
+ Ey1
(cid:2)(cid:104)φ(y1), µx(cid:105)2(cid:3) − Ey1 [(cid:104)φ(y1), µx(cid:105)]2
+ Ey1
(cid:2)(cid:104)φ(z1), µx(cid:105)2(cid:3) − Ez1 [(cid:104)φ(z1), µx(cid:105)]2
+ Ez1
− 2 (Ex1 [(cid:104)φ(x1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)])

We have shown that Equation (9) is equal to Equation (31).

B CALIBRATION OF THE TEST

We show here that our derived test is well calibrated. A calibrated
test should output a uniform distribution of p-values when the two
MMD distances are equal. The empirical distributions of p-values
for various sets of Px, Py and Pz are given in Figure 8. Similarly,
for a given signiﬁcance level α, the false positive rate should be
equal to α. The empirical false positive rates for varying α are
shown in Figure 7 further demonstrating the proper calibration of
the test.

y
c
n
e
u
q
e
r
F

p-values

Figure 7: Calibration of the rela-

tive similarity test

15

Published as a conference paper at ICLR 2016

(a) Illustration of the synthetic data with different

(b) Uniform histogram of p-values

means for X, Y and Z.

(c) Illustration of the synthetic data with different

(d) Uniform histogram of p-values

means and orientations for X, Y and Z.

(e) Illustration of the synthetic data with different ori-

(f) Uniform histogram of p-values

entations for X, Y and Z.

Figure 8: Calibration of the relative similarity test

16

6
1
0
2
 
b
e
F
 
5
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
8
5
4
0
.
1
1
5
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2016

A TEST OF RELATIVE SIMILARITY FOR MODEL
SELECTION IN GENERATIVE MODELS

Wacha Bounliphone,12∗ Eugene Belilovsky,12∗ & Matthew B. Blaschko2
1CentraleSup´elec & Inria Saclay, Universit´e Paris-Saclay, 92295 Chˆatenay-Malabry, France
2ESAT-PSI, KU Leuven, Kasteelpark Arenberg 10, 3001 Leuven, Belgium
{wacha.bounliphone,eugene.belilovsky}@inria.fr
matthew.blaschko@esat.kuleuven.be

Ioannis Antonoglou
Google Deepmind
5 New Street Square
London EC4A 3TW, UK
ioannisa@google.com

Arthur Gretton
Gatsby Computational Neuroscience Unit
University College London
25 Howland Street
London W1T 4JG, UK
arthur.gretton@gmail.com

ABSTRACT

Probabilistic generative models provide a powerful framework for representing
data that avoids the expense of manual annotation typically needed by discrimi-
native approaches. Model selection in this generative setting can be challenging,
however, particularly when likelihoods are not easily accessible. To address this
issue, we introduce a statistical test of relative similarity, which is used to de-
termine which of two models generates samples that are signiﬁcantly closer to
a real-world reference dataset of interest. We use as our test statistic the differ-
ence in maximum mean discrepancies (MMDs) between the reference dataset and
each model dataset, and derive a powerful, low-variance test based on the joint
asymptotic distribution of the MMDs between each reference-model pair. In ex-
periments on deep generative models, including the variational auto-encoder and
generative moment matching network, the tests provide a meaningful ranking of
model performance as a function of parameter and training settings.

1

INTRODUCTION

Generative models based on deep learning techniques aim to provide sophisticated and accurate
models of data, without expensive manual annotation (Bengio, 2009; Kingma et al., 2014). This
is especially of interest as deep networks tend to require comparatively large training samples to
achieve a good result (Krizhevsky et al., 2012). Model selection within this class of techniques can
be a challenge, however. First, likelihoods can be difﬁcult to compute for some families of recently
proposed models based on deep learning (Goodfellow et al., 2014; Li et al., 2015b). The current
best method to evaluate such models is based on Parzen-window estimates of the log likelihood
(Goodfellow et al., 2014, Section 5). Second, if we are given two models with similar likelihoods,
we typically do not have a computationally inexpensive hypothesis test to determine whether one
likelihood is signiﬁcantly higher than the other. Permutation testing or other generic strategies are
often computationally prohibitive, bearing in mind the relatively high computational requirements
of deep networks (Krizhevsky et al., 2012).

In this work, we provide an alternative strategy for model selection, based on a novel, non-parametric
hypothesis test of relative similarity. We treat the two trained networks being compared as generative
models (Goodfellow et al., 2014; Hinton et al., 2006; Salakhutdinov and Hinton, 2009), and test
whether the ﬁrst candidate model generates samples signiﬁcantly closer to a reference validation
set. The null hypothesis is that the ordering is reversed, and the second candidate model is closer to

∗These authors contributed equally to this work

1

Published as a conference paper at ICLR 2016

the reference (further, both samples are assumed to remain distinct from the reference, as will be the
case for any sufﬁciently complex modeling problem).

Our model selection criterion is based on the maximum mean discrepancy (MMD) (Gretton et al.,
2006; 2012a), which represents the distance between embeddings of empirical distributions in a re-
producing kernel Hilbert space (RKHS). The maximum mean discrepancy is a metric on the space of
probability distirbutions when a characteristic kernel is used (Fukumizu et al., 2008; Sriperumbudur
et al., 2010), meaning that the distribution embeddings are unique for each probability measure.
Recently, the MMD has been used in training generative models adversarially, (Li et al., 2015b;
Dziugaite et al., 2015), where the MMD measures the distance of the generated samples to some
reference target set; it has been used for statistical model criticism (Lloyd and Ghahramani, 2015);
and to minimize the effect of nuisance variables on learned representations (Louizos et al., 2016).

Rather than train a single model using the MMD distance to a reference distribution, our goal in
this work is to evaluate the relative performance of two models, by testing whether one generates
samples signiﬁcantly closer to the reference distribution than the other. This extends the applicability
of the MMD to problems of model selection and evaluation. Key to this result is a novel expression
for the joint asymptotic distribution of two correlated MMDs (between samples generated from
each model, and samples from the reference distribution). Li et al. (2015a) have derived the joint
distribution of a speciﬁc MMD estimator under the assumption that the distributions are equal. By
contrast, we derive the case in which the distributions are unequal, as is expected due to irreducible
model error.

We provide a detailed introduction to the MMD and its associated notation in Section 2. We derive
the joint asymptotic distribution of the MMDs in Section 3: this uses similar ideas to the relative
dependence test in Bounliphone et al. (2015), with the additional complexity due to there being three
independent samples to deal with, rather than a single joint sample. We formulate a hypothesis test
of relative similarity, to determine whether the difference in MMDs is statistically signiﬁcant. Our
ﬁrst test benchmark is on a synthetic data for which the ground truth is known (Section 4), where
we verify that the test performs correctly under the null and the alternative.

Finally, in Section 5, we demonstrate the performance of our test over a broad selection of model
comparison problems in the deep learning setting, by evaluating relative similarity of pairs of model
outputs to a validation set over a range of training regimes and settings. Our benchmark models in-
clude the variational auto-encoder (Kingma and Welling, 2014) and the generative moment matching
network (Li et al., 2015b). We ﬁrst demonstrate that the test performs as expected in scenarios where
the same model is trained with different training set sizes, and the relative ordering of model per-
formance is known. We then ﬁx the training set size and change various architectural parameters
of these networks, showing which models are signiﬁcantly preferred with our test. We validate the
rankings returned by the test using a separate set of data for which we compute alternate metrics for
assessing the models, such as classiﬁcation accuracy and likelihood.

2 BACKGROUND MATERIAL

In comparing samples from distributions, we use the Maximum Mean Discrepancy (MMD) (Gretton
et al., 2006; 2012a). We brieﬂy review this statistic and its asymptotic behaviour for a single pair of
samples.

Deﬁnition 1. (Gretton et al., 2012a, Deﬁnition 2: Maximum Mean Discrepancy (MMD)) Let F be
an RKHS, with the continuous feature mapping ϕ(x) ∈ F from each x ∈ X , such that the inner
product between the features is given by the kernel function k(x, x(cid:48)) := (cid:104)φ(x), φ(x(cid:48))(cid:105). Then the
squared population MMD is

MMD2(F, Px, Py) = Ex,x(cid:48) [k(x, x(cid:48))] − 2 Ex,y [k(x, y)] + Ey,y(cid:48) [k(y, y(cid:48))] .

(1)

The following theorem describes an unbiased quadratic-time estimate of the MMD, and its asymp-
totic distribution when Px and Py are different.
Theorem 1. (Gretton et al., 2012a, Lemma 6 & Corollary 16: Unbiased empirical estimate &
Asymptotic distribution of MMD2
u(F, Xm, Yn)) Deﬁne observations Xm := {x1, ..., xm} and

2

Published as a conference paper at ICLR 2016

(3)

(5)

Yn := {y1, ..., yn} independently and identically distributed (i.i.d.) from Px and Py, respectively.
An unbiased empirical estimate of MMD2(F, Px, Py) is a sum of two U -statistics and a sample
average,

MMD2

u(F, Xm, Yn) =

k(xi, xj) +

k(yi, yj)

(2)

1
n(n − 1)

n
(cid:88)

n
(cid:88)

i=1

j(cid:54)=i

1
m(m − 1)

m
(cid:88)

m
(cid:88)

i=1

j(cid:54)=i

−

2
mn

m
(cid:88)

n
(cid:88)

i=1

j=1

k(xi, yj).

Let V := (v1, ..., vm) be m i.i.d. random variables, where v := (x, y) ∼ Px × Py. When m = n,
an unbiased empirical estimate of MMD2(F, Px, Py) is

MMD2

u(F, Xm, Ym) =

1
m(m − 1)

m
(cid:88)

i(cid:54)=j

h(vi, vj)

with h(vi, vj) = k(xi, xj) + k(yi, yj) − k(xi, yj) − k(xj, yi). We assume E(h2) < ∞. When
Px (cid:54)= Py, MMD2
u(F, X, Y ) converges in distribution to a Gaussian according to
√
u(F, Xm, Yn) − MMD2(F, Px, Py)(cid:1) D−→ N (cid:0)0, σ2
m (cid:0)MMD2

(4)

XY

(cid:1)

where

uniformly at rate 1/

XY = 4 (cid:0)Ev1 [(Ev2h(v1, v2))2] − [(Ev1,v2 h(v1, v2))2](cid:1)
σ2
√
m.

A two-sample test may be constructed using the MMD as a test statistic, however when Px = Py
the statistic is degenerate, and the asymptotic distribution is a weighted sum of χ2 variables (which
can have inﬁnitely many terms, (Gretton et al., 2012a)).

By contrast, our problem setting is to determine with high signiﬁcance whether a target distribution
Px is closer to one of two candidate distributions Py, Pz, based on two empirical estimates of the
MMD and their variances. This requires us to characterize σ2
u(F, Xm, Yn) as well
as the covariance of two dependent estimates, MMD2
u(F, Xm, Zr) (the
dependence arises from the shared sample Xm). Fortunately, degeneracy does not arise if we assume
Py, Pz are each distinct from Px.

u(F, Xm, Yn) and MMD2

XY for MMD2

In the next section, we obtain the joint asymptotic distribution of two dependent MMD statistics. We
demonstrate how this joint distribution can be empirically estimated, and use the resulting parametric
form to construct a computationally efﬁcient and powerful hypothesis test for relative similarity.

3

JOINT ASYMPTOTIC DISTRIBUTION OF TWO CORRELATED MMDS AND A
RESULTING TEST STATISTIC

In this section, we derive our statistical test for relative similarity as measured by MMD. In order to
maximize the statistical efﬁciency of the test, we will reuse samples from the reference distribution,
denoted by Px, to compute the MMD estimates with two candidate distributions Py and Pz. We
consider two MMD estimates MMD2
u(F, Xm, Yn) and MMD2
u(F, Xm, Zr), and as the data sample
Xm is identical between them, these estimates will be correlated. We therefore ﬁrst derive the joint
asymptotic distribution of these two metrics and use this to construct a statistical test.
Theorem 2. We assume that Px (cid:54)= Py, Px (cid:54)= Pz, E(k(xi, xj)) < ∞, E(k(yi, yj)) < ∞ and
E(k(xi, yj)) < ∞, then
√

(cid:19)(cid:19)

(cid:19)

(cid:19)

(cid:19)(cid:19)

(cid:18)(cid:18)MMD2
MMD2

m

u(F, Xm, Yn)
u(F, Xm, Zr)

−

(cid:18)MMD2(F, Px, Py)
MMD2(F, Px, Pz)

(cid:18)(cid:18)0
0

(cid:18) σ2

XY
σXY XZ

,

σXY XZ
σ2

XZ

d−→ N

(6)

We substitute the kernel MMD deﬁnition from Equation (2), expand the terms in the expectation,
and determine their empirical estimates in order to compute the variances in practice. The proof and
additional details of the following derivations are given in Appendix A.

3

Published as a conference paper at ICLR 2016

An empirical estimate of σXY XZ in Equation (6), neglecting higher order terms, can be computed
in O(m2):

σXY XZ ≈

(cid:18)

(cid:19)2

1

(cid:18)

−

m(m − 1)2 eT ˜Kxx ˜Kxxe −
1
m(m − 1)r
1
m(m − 1)n

−

(cid:18)

eT ˜Kxxe

1
m(m − 1)
1
m2(m − 1)r
1
m2(m − 1)n

(cid:18) 1

+

mnr

eT KyxKxze −

eT KxyeeT Kxze

1
m2nr

(cid:19)

eT ˜KxxKxze −

eT ˜KxxeeT Kxze

eT ˜KxxKxye −

eT ˜KxxeeT Kxze

(7)

(cid:19)

(cid:19)

where e is a vector of 1s with appropriate size, while ˜Kxx, Kxy and Kxz refer to the kernel matri-
ces, with ˜K indicating that the diagonal entries have been set to zero (cf. Appendix A). Similarly,
Equation (5) is constructed as in Equation (7).

Based on the empirical distribution from Equation (6), we now describe a statistical test to solve the
following problem:
Problem 1 (Relative similarity test). Let Px, Py, x and y be deﬁned as above, z be an independent
random variables with distribution Pz. Given observations Xm := {x1, ..., xm}, Yn := {y1, ..., yn}
from Px, Py and Pz respectively such that Px (cid:54)= Py, Px (cid:54)= Pz,
and Zr := {z1, ..., zr} i.i.d.
we test the hypothesis that Px is closer to Pz than Py i.e. we test the null hypothesis H0:
MMD(F, Px, Py) ≤ MMD(F, Px, Pz) versus the alternative hypothesis H1: MMD(F, Px, Py) >
MMD(F, Px, Pz) at a given signiﬁcance level α

u(F, Xm, Yn) − MMD2

The test statistic MMD2
u(F, Xm, Zr) is used to compute the p-value p for
the standard normal distribution. The test statistic is obtained by rotating the joint distribution
(cf. Eq. 6) by π
4 about the origin, and integrating the resulting projection on the ﬁrst axis, in a
manner similar to Bounliphone et al. (2015). Denote the asymptotically normal distribution of
√
u(F, Xm, Zr)]T as N (µ, Σ). The resulting distribution from ro-
tating by π/4 and projecting onto the primary axis is N (cid:0)[Rµ]1, [RΣRT ]11

u(F, Xm, Yn); MMD2

m[MMD2

(cid:1) where

[Rµ]1 =

(MMD2

u(F, Xm, Yn) − MMD2

u(F, Xm, Zr))

[RΣRT ]11 =

XY + σ2

XZ − 2σXY XZ)

√

2
2
1
(σ2
2

with R is the rotation by π/4. Then, the p-values for testing H0 versus H1 are
(cid:33)

(cid:32)

MMD2

u(F, Xm, Yn) − MMD2

u(F, Xm, Zr)

p ≤ Φ

−

(cid:112)σ2

XY + σ2
where Φ is the CDF of a standard normal distribution. We have made code for performing the test
is available.1

XZ − 2σXY XZ

4 EXPERIMENTAL VALIDATION OF THE RELATIVE MMD TEST

We verify the validity of the hypothesis test described above using a synthetic data set in which we
can directly control the relative similarity between distributions. We constructed three Gaussian dis-
tributions as illustrated in Figure 1. These Gaussian distributions are speciﬁed with different means
so that we can control the degree of relative similarity between them. The question is whether the
similarity between X and Z is greater than the similarity between X and Y . In these experiments, we
used a Gaussian kernel with bandwidth selected as the median pairwise distance between data points,
and we ﬁxed µY = [−20, −20], µZ = [20, 20] and varied µX such that µX = (1 − γ)µY + γµZ, for
41 regularly spaced values of γ ∈ [0.1, 0.9] (avoiding the degenerate cases Px = Py or Px = Pz).

1Code and examples can be found at https://github.com/eugenium/MMD

(8)

(9)

(10)

4

Published as a conference paper at ICLR 2016

u(F, X, Y ) is almost equal to MMD2

Figure 3 shows the p-values of the relative similarity test for different distribution. When γ is
varying around 0.5, i.e., when MMD2
u(F, X, Z), the p-values
quickly transition from 1 to 0, indicating strong discrimination of the test. In Figure 2, we compare
the power of our test to the power of a naive test when the reference sample is split in two, and
the MMDs have no covariance: clearly, the latter simple approach does worse than ours (a similar
comparison in testing relative dependence returned the same advantage for a test based on the joint
distribution; see Bounliphone et al. (2015, Section 3)). Figure 4 shows an empirical scatter plot of
the pairs of MMD statistics along with a 2σ iso-curve of the estimated distribution, demonstrating
that the parametric Gaussian distribution is well calibrated to the empirical values. Futhermore, we
validate our derived formulas using simulations in Appendix B, where we show the p-values have
the correct distribution under the null.

s
t
s
e
t

e
h
t

f
o

r
e
w
o
P

Figure 1: Illustration of

the synthetic dataset
where X, Y and Z are respectively
Gaussian distributed with mean µX =
[0, 0]T , µY = [−20, −20]T , µZ =
[20, 20]T and with variance ( 1 0

0 1 ).

γ

Figure 2: Comparison of the power of the pro-
posed method to an independent test
analogous to Bounliphone et al. (2015,
Section 3) as a function of γ.

s
e
u
l
a
v
-
p

γ
m = 1000

Figure 3: We ﬁxed µY = [−5, −5], µZ = [5, 5] and
varied µX such that µX = (1 − γ)µY +
γµZ , for 41 regularly spaced values of γ ∈
[0.1, 0.9] versus p-values for 100 repeated
tests.

Figure 4: The empirical scatter plot of the joint MMD
statistics with m = 1000 for 200 repeated
tests, along with the 2σ iso-curve of the an-
alytical Gaussian distribution estimated by
Equation (6). The analytical distribution
closely matches the empirical scatter plot,
verifying the correctness of the variances.

5 MODEL SELECTION FOR DEEP UNSUPERVISED NEURAL NETWORKS

An important potential application of the Relative MMD can be found in recent work on unsu-
pervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014;
Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015b; Goodfellow et al.,
2014). As noted by several authors, the evaluation of generative models is a challenging open prob-
lem (Li et al., 2015b; Goodfellow et al., 2014), and the distributions of samples from these models
are very complex and difﬁcult to evaluate. The relative MMD performance can be used to compare
different model settings, or even model families, in a statistically valid framework. To compare two
models using our test, we generate samples from both, and compare these to a set of real target data
samples that were not used to train either model.

In the experiments in the sequel we focus on the recently introduced variational auto-encoder (VAE)
(Kingma and Welling, 2014) and the generative moment matching networks (GMMN) (Li et al.,

5

Published as a conference paper at ICLR 2016

(a)

(b)

Figure 5: (a) Variational auto-encoder reference model. We have 400 hidden nodes (both encoder and decoder)
and 20 latent variables in the reference model for our experiments.
(b) Auto-Encoder + GMMN
reference model. The auto-encoder (indicated in orange) is trained separately and has 1024 and
32 hidden nodes in decode and encode hidden layers. The GMMN has 10 variables generated by
the prior, and the hidden layers have 64, 256, 256, 1024 nodes in each layer respectively. In both
networks red arrows indicate the data ﬂow during sampling

2015b). The former trains an encoder and decoder network jointly minimizing a regularized vari-
ational lower bound (Kingma and Welling, 2014). While the latter class of models is purely gen-
erative minimizing an MMD based objective, this model works best when coupled with a separate
auto-encoder which reduces the dimensionality of the data. An architectural schematic for both
classes of models is provided in Fig. 5. Both these models can be trained using standard backprop-
agation (Rumelhart et al., 1988). Using the latent variable prior we can directly sample the data
distribution of these models without using MCMC procedures (Hinton et al., 2006; Salakhutdinov
and Hinton, 2009).

We use the MNIST and FreyFace datasets for our analysis (LeCun et al., 1998; Kingma and Welling,
2014; Goodfellow et al., 2014). We ﬁrst demonstrate the effectiveness of our test in a setting where
we have a theoretical basis for expecting superiority of one unsupervised model versus another.
Speciﬁcally, we use a setup where more training samples were used to create one model versus
the other. We ﬁnd that the Relative MMD framework agrees with the expected results (models
trained with more data generalize better). We then demonstrate how the Relative MMD can be used
in evaluating network architecture choices, and we show that our test strongly agrees with other
established metrics, but in contrast can provide signiﬁcance results using just the validation data
while other methods may require an additional test set.

Several practical matters must be considered when applying the Relative MMD test. The selection
of kernel can affect the quality of results, particularly more suitable kernels can give a faster conver-
gence. In this work we extend the logic of the median heuristic (Gretton et al., 2012b) for bandwidth
selection by computing the median pairwise distance between samples from Px and Py and aver-
aging that with the median pairwise distance between samples from Px and Pz, which helps to
maximize the difference between the two MMD statistics. Although the derivations for the variance
of our statistic hold for all cases, the estimates require asymptotic arguments and thus a sufﬁciently
large n. Selecting the kernel bandwidth in an appropriate range can therefore substantially increase
the power of the test at a ﬁxed sample size. While we observed the median heuristic to work well
in our experiments, there are cases where alternative choices of kernel can provide greater power:
for instance, the kernel can be chosen to maximize the expected test power on a held-out dataset
(Gretton et al., 2012b).

6

Published as a conference paper at ICLR 2016

5.1 VARIATIONAL AUTO-ENCODER SAMPLE SIZE AND ARCHITECTURE EXPERIMENTS

We use the architecture from Kingma and Welling (2014) with a hidden layer at both the encoder
and decoder and a latent variable layer as shown in Figure 5a. We use sigmoidal activation for
the hidden layers of encoder and decoder. For the FreyFace data, we use a Gaussian prior on the
latent space and data space. For MNIST, we used a Bernoulli prior for the data space. We ﬁx the
training set size of the second auto-encoder to 300 images for the FreyFace data and 1500 images
for the MNIST data. We vary the number of training samples for the ﬁrst auto-encoder. We then
generate samples from both auto-encoders and compare them using Relative MMD to a held out
set of data. We use 1500 FreyFace samples as the target in Relative MMD and 15000 images from
MNIST. Since a single sample of the data might lead to better generalization performance by chance,
we repeat this experiment multiple times and record whether the relative similarity test indicated a
network is preferred or if it failed to reject the null hypothesis. The results are shown in Figure 6
which demonstrates that we are closely following the expected model preferences. Additionally for
MNIST we use another separate set of supervised training and test data. We encode this data using
both auto-encoders and use logistic regression to obtain a classiﬁcation accuracy. The indicated
accuracies closely match the results of the relative similarity test, further validating the test.

(a)

(c)

(b)

Figure 6: We show the effect of (a) varying the train-
ing set size of one auto-encoder trained on
MNIST data. (c) As a secondary valida-
tion we compute the classiﬁcation accu-
racy of MNIST on a separate train/test set
encoded using encoder 1 and encoder 2.
(b) We then show the effect of varying the
training set size of one auto-encoder using
the FreyFace data. We note that due to the
size of the FreyFace dataset, we limit the
range of ratios used. From this ﬁgure we
see that the results of the relative similarity
test match our expectation: more data pro-
duces models which more closely match
the true distribution.

We consider model selection between networks using different architectures. We train two encoders,
one a ﬁxed reference model (400 hidden units and 20 latent variables), and the other varying as
speciﬁed in Table 1. 25000 images from the MNIST data set were used for training. We use another
20000 images as the target data in Relative MMD. Finally, we use a set of 10000 training and 10000
test images for a supervised task experiment. We use the labels in the MNIST data and perform
training and classiﬁcation using an (cid:96)2-regularized logistic regression on the encoded features. In
addition we use the supervised task test data to evaluate the variational lower bound of the data under
the two models (Kingma and Welling, 2014). We show the result of this experiment in Table 1. For
each comparison we take a different subset of training data which helps demonstrate the variation in

7

Published as a conference paper at ICLR 2016

lower bound and accuracy when re-training the reference architecture. We use a signiﬁcance value
of 5% and indicate when the test favors one auto-encoder over another or fails to reject the null
hypothesis. We ﬁnd that Relative MMD evaluation of the models closely matches performance on
the supervised task and the test set variational lower bound.

Accuracy (%) Accuracy (%) Lower Bound Lower Bound

Hidden Latent Result
VAE 1
200
200
400
800
800

VAE 1 RelativeMMD VAE 1
Favor VAE 2
5
Favor VAE 2
20
Favor VAE 1
50
Favor VAE 1
20
Favor VAE 1
50

92.8 ± 0.3
92.6 ± 0.3
94.6 ± 0.2
94.8 ± 0.2
94.2 ± 0.3

VAE 2
94.7 ± 0.2
94.5 ± 0.2
94.0 ± 0.2
93.9 ± 0.2
94.5 ± 0.2

VAE 1
-126
-115
-99.6
-111
-101

VAE 2
-97
-105
-123.44
-115
-103

Table 1: We compare several variational auto encoder (VAE) architectural choices for the number of hidden
units in both decoder and encoder and the number of latent variables for the VAE. The reference
encoder, denoted encoder 2, has 400 hidden units and 20 latent variables. We denote the competing
architectural models as encoder 1. We vary the number of hidden nodes in both the decoder and
encoder and the number of latent variables. Our test closely follows the performance difference of the
auto-encoder on a supervised task (MNIST digit classiﬁcation) as well as the variational lower bound
on a withheld set of data. The data used for evaluating the Accuracy and Lower Bound is separate
from that used to train the auto-encoders and for the hypothesis test.

5.2 GENERATIVE MOMENT MATCHING NETWORKS ARCHITECTURE EXPERIMENTS

We demonstrate our hypothesis test on a different class of deep generative models called Generative
Moment Matching Networks (GMMN) Li et al. (2015b). This recently introduced model has shown
competitive performance in terms of test set likelihood on the MNIST data. Furthermore the training
of this model is based on the MMD criterion. Li et al. (2015b) proposes to use that model along with
an auto-encoder, which is the setup we employ in this work. Here a standard auto-encoder model is
trained on the data to obtain a low dimensional representation, then a GMMN network is trained on
the latent representations (Figure 5).

We use the relative similarity test to evaluate various architectural choices in this new class of mod-
els. We start from the baseline model speciﬁed in Li et al. (2015b) and associated software. The
details of the reference model are speciﬁed in Figure 5.

We vary the number of auto-encoder hidden layers (1 to 4), generative model layers(1, 4, or 5), the
number of network nodes (all or 50% of the reference model), and use of drop-out on the auto-
encoder. We use the same training set of 55000, validation set of 5000 and test set of 10000 as in
(Li et al., 2015b; Goodfellow et al., 2014). In total we train 48 models. We use these to compare
4 simpliﬁed binary network architecture choices using the Relative MMD: using dropout on the
auto-encoder, few (1) or more (4 or 5) GMMN layers, few (1 or 2) or more (3 or 4) auto-encoder
layers, and the number of network nodes. We use our test to compare these model settings using
the validation set as the target in the relative similarity test, and samples from the models as the two
sources. To validate our results we compare it to likelihoods computed on the test set. The results
are shown in Table 2. We see that the likelihood results computed on a separate test set follow the
conclusions obtained from MMD on the validation set. Particularly, we ﬁnd that using fewer hidden
layers for the GMMN and more hidden nodes generally produces better models.

5.3 DISCUSSION

In these experiments we have seen that the Relative MMD test can be used to compare deep gen-
erative models obtaining judgements aligned with other metrics. Comparisons to other metrics are
important for verifying our test is sensible, but it can occlude the fact that MMD is a valid eval-
uation technique on its own. When evaluating only sample generating models where likelihood
computation is not possible, MMD is an appropriate and tractable metric to consider in addition to
Parzen-Window log likelihoods and visual appearance of the samples. In several ways it is poten-
tially more appropriate than Parzen-windows as it allows one to consider directly the discrepancy
between the test data samples and the model samples while allowing for signiﬁcance results. In such
a situation, comparing the performance of several models using the MMD against a single set of test

8

Published as a conference paper at ICLR 2016

Experimental Condition (A/B)
Dropout/No Dropout
More/Fewer GMMN Layers
More/Fewer Nodes
More/Fewer AE layers

RelativeMMD Preference
B
Inconclusive
A
−9.01 ± 55.43
360
17
199
393 −73.99 ± 40.96
14
105
125.2 ± 43.4
113
13
450
41.78 ± 44.07
324
21
231

76.76 ± 42.83
249.6 ± 8.07
−57 ± 49.57
25.96 ± 55.85

Avg Likelihood A Avg Likelihood B

Table 2: For each experimental condition (e.g. dropout or no dropout) we show the number of times the Relative
MMD prefers models in group 1 or 2 and number of inconclusive tests. We use the validation set as the
target data for Relative MMD. An average likelihood for the MNIST test set for each group is shown
with error bars. We can see that the MMD choices are in agreement with likelihood evaluations.
Particularly we identify that models with fewer GMMN layers and models with more nodes have
more favourable samples, which is conﬁrmed by the likelihood results.

samples, the Relative MMD test can provide an automatic signiﬁcance value without expensive
cross-validation procedures.

Gaussian kernels are closely related to Parzen-window estimates, thus computing an MMD in this
case can be considered related to comparing Parzen window log-likelihoods. The MMD gives sev-
eral advantages, however. First, the asymptotics of MMD are quite different to Parzen-windows,
since the Parzen-window bandwidth shrinks as n grows. Asymptotics of relative tests with shrink-
ing bandwidth are unknown: even for two samples this is challenging (Krishnamurthy et al., 2015).
Other two sample tests are not easily extendable to relative tests (Rosenbaum, 2005; Friedman and
Rafsky, 1979; Hall and Tajvidi, 2002). This is because the tests above rely on graph edge counting
or nearest neighbor-type statistics, and null distributions are obtained via combinatorial arguments
which are not easily extended from two to three samples. MMD is a U -statistic, hence its asymptotic
behavior is much more easily generalised to multiple dependent statistics.

There are two primary advantages of the MMD over the variational lower bound, where it is known
(Kingma and Welling, 2014): ﬁrst, we have a characterization of the asymptotic behavior, which
allows us to determine when the difference in performance is signiﬁcant; second, comparing two
lower bounds produced from two different models is unreliable, as we do not know how conservative
either lower bound is.

6 CONCLUSION

We have described a novel non-parametric statistical hypothesis test for relative similarity based on
the Maximum Mean Discrepancy. The test is consistent, and the computation time is quadratic.
Our proposed test statistic is theoretically justiﬁed for the task of comparing samples from arbitrary
distributions as it can be shown to converge to a quantity which compares all moments of the two
pairs of distributions.

We evaluate test performance on synthetic data, where the degree of similarity can be controlled.
Our experimental results on model selection for deep generative networks show that Relative MMD
can be a useful approach to comparing such models. There is a strong correspondence between the
test resuts and the expected likelihood, prediction accuracy, and variational lower bounds on the
models tested. Moreover, our test has the advantage over these alternatives of providing guarantees
of statistical signiﬁcance to its conclusions. This suggests that the relative similarity test will be
useful in evaluating hypotheses about network architectures, for example that AE-GMMN models
may generalize better when fewer layers are used in the generative model. Code for our method is
available.2

ACKNOWLEDGMENTS

We thank Joel Veness for helpful comments. This work is partially funded by Internal Funds KU
Leuven, ERC Grant 259112, FP7-MC-CIG 334380, the Royal Academy of Engineering through the
Newton Alumni Scheme, and DIGITEO 2013-0788D-SOPRANO. WB is supported by a Centrale-
Sup´elec fellowship.

2 https://github.com/eugenium/MMD

9

Published as a conference paper at ICLR 2016

REFERENCES

1–127, 2009.

Y. Bengio. Learning deep architectures for AI. Foundations and trends in Machine Learning, 2(1):

Y. Bengio, E. Thibodeau-Laufer, G. Alain, and J. Yosinski. Deep generative stochastic networks
trainable by backprop. In Proceedings of the 31st International Conference on Machine Learning,
2014.

W. Bounliphone, A. Gretton, A. Tenenhaus, and M. B. Blaschko. A low variance consistent test
of relative dependency. In F. Bach and D. Blei, editors, Proceedings of The 32nd International
Conference on Machine Learning, volume 37 of JMLR Workshop and Conference Proceedings,
pages 20–29, 2015.

G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training generative neural networks via maximum
mean discrepancy optimization. In Conference on Uncertainty in Artiﬁcial Intelligence, 2015.

J. H. Friedman and L. C. Rafsky. Multivariate generalizations of the Wald-Wolfowitz and Smirnov

two-sample tests. The Annals of Statistics, pages 697–717, 1979.

K. Fukumizu, A. Gretton, X. Sun, and B. Sch¨olkopf. Kernel measures of conditional dependence.

pages 489–496, Cambridge, MA, 2008. MIT Press.

I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems,
pages 2672–2680, 2014.

A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. J. Smola. A kernel method for
the two-sample-problem. In Advances in neural information processing systems, pages 513–520,
2006.

A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch¨olkopf, and A. Smola. A kernel two-sample test.

The Journal of Machine Learning Research, 13(1):723–773, 2012a.

A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, K. Fukumizu, and B. K.
Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. In F. Pereira, C. Burges,
L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25,
pages 1205–1213. 2012b.

P. Hall and N. Tajvidi. Permutation tests for equality of distributions in high-dimensional settings.

Biometrika, 89(2):359–374, 2002.

G. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural

computation, 18(7):1527–1554, 2006.

W. Hoeffding. A class of statistics with asymptotically normal distribution. The annals of mathe-

matical statistics, pages 293–325, 1948.

D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on

Learning Representations, 2014.

D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling. Semi-supervised learning with deep
generative models. In Advances in Neural Information Processing Systems, pages 3581–3589,
2014.

A. Krishnamurthy, K. Kandasamy, B. P´oczos, and L. A. Wasserman. On estimating L2

2 divergence.
In Proceedings of the Eighteenth International Conference on Artiﬁcial Intelligence and Statistics,
2015.

A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012.

H. Larochelle and I. Murray. The neural autoregressive distribution estimator. Journal of Machine

Learning Research, 15:29–37, 2011.

10

Published as a conference paper at ICLR 2016

Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

S. Li, Y. Xie, H. Dai, and L. Song. M-statistic for kernel change-point detection. In Advances in

Neural Information Processing Systems, pages 3348–3356, 2015a.

Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In International Confer-

ence on Machine Learning, pages 1718–1727, 2015b.

J. R. Lloyd and Z. Ghahramani. Statistical model criticism using kernel two sample tests. In Ad-

vances in Neural Information Processing Systems, 2015.

C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel. The variational fair auto encoder. In

International Conference on Learning Representations, 2016.

P. R. Rosenbaum. An exact distribution-free test comparing two multivariate distributions based on
adjacency. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(4):
515–530, 2005.

D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating
errors. In J. A. Anderson and E. Rosenfeld, editors, Neurocomputing: Foundations of Research,
pages 696–699. MIT Press, Cambridge, MA, USA, 1988.

R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In International Conference on

Artiﬁcial Intelligence and Statistics, pages 448–455, 2009.

R. J. Serﬂing. Approximation theorems of mathematical statistics, volume 162. John Wiley & Sons,

2009.

B. K. Sriperumbudur, A. Gretton, K. Fukumizu, G. R. Lanckriet, and B. Sch¨olkopf. Hilbert space
embeddings and metrics on probability measures. Journal of Machine Learning Research, 11:
1517–1561, 2010.

A DETAILED DERIVATIONS OF THE TEST VARIANCE AND COVARIANCE

The variance and the covariance for a U -statistic is described in Hoeffding (1948, Eq. 5.13) and
Serﬂing (2009, Chap. 5).

Let V := (v1, ..., vm) be m iid random variables where v := (x, y) ∼ Px × Py. An unbiased
estimator of MMD2(F, Px, Py) is

MMD2

u(F, Xm, Yn) =

1
m(m − 1)

m
(cid:88)

i(cid:54)=j

h(vi, vj)

with h(vi, vj) = k(xi, xj) + k(yi, yj) − k(xi, yj) − k(xj, yi).

Similarly, let W := (w1, ..., wm) be m iid random variables where w := (x, z) ∼ Px × Pz. An
unbiased estimator of MMD2(F, Px, Pz) is

MMD2

u(F, Xm, Zr) =

m
(cid:88)

1
m(m − 1)

g(wi, wj)

i(cid:54)=j
with g(wi, wj) = k(xi, xj) + k(zi, zj) − k(xi, zj) − k(xj, zi)

Then the variance/covariance for a U -statistic with a kernel of order 2 is given by

Equation (13) with neglecting higher terms can be written as

V ar(MMD2

u) =

4(m − 2)
m(m − 1)

ζ1 +

2
m(m − 1)

ζ2

V ar(MMD2

u) =

4(m − 2)
m(m − 1)

ζ1 + O(m−2)

(11)

(12)

(13)

(14)

where for the variance term, ζ1 = Var [Ev1 [h(v1, V2)]] and for the covariance term ζ1 =
Var [Ev1,w1 [h(v1, V2)g(w1, W2)]].

11

Published as a conference paper at ICLR 2016

Notation [ ˜Kxx]ij = [Kxx]ij for all i (cid:54)= j and [ ˜Kxx(cid:48)]ij = 0 for j = i. Same for ˜Kyy and ˜Kzz.
We will also make use of the fact that k(xi, xj) = (cid:104)φ(xi), φ(xj)(cid:105) for an appropriately chosen inner
product, and function φ. We then denote

(cid:90)

µx :=

φ(x)dPx.

(15)

A.1 VARIANCE OF MMD

We note many terms in expansion of the squares above cancel out due to independence. For example
Ex1,y1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(x1), µy(cid:105)] − Ey1 [(cid:104)φ(y1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µy(cid:105)] = 0.
We can thus simplify to the following expression for ζ1

(cid:104)

(Ex2,y2 [h(x1, y1)])2(cid:105)
(16)
(cid:2)((cid:104)φ(x1), µx(cid:105) + (cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)µx, φ(y1)(cid:105))2(cid:3) − (cid:0)MMD2(F, PX , PY )(cid:1)2
(17)

− (cid:0)MMD2(F, PX , PY )(cid:1)2

ζ1 = Ex1,y1

= Ex1,y1

= Ex1,y1

(cid:2)(cid:104)φ(x1), µx(cid:105)2 + 2(cid:104)φ(x1), µx(cid:105)(cid:104)φ(y1), µy(cid:105) − 2(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µy(cid:105)
− 2(cid:104)φ(x1), µx(cid:105)(cid:104)φ(y1), µx(cid:105) + (cid:104)φ(y1), µy(cid:105)2
− 2(cid:104)φ(y1), µy(cid:105)(cid:104)φ(x1), µy(cid:105) − 2(cid:104)φ(y1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)
+ (cid:104)φ(x1), µy(cid:105)2 + 2(cid:104)φ(x1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)
+ (cid:104)φ(y1), µx(cid:105)2(cid:3) − (cid:0)MMD2(F, PX , PY )(cid:1)2

= Ex1 [(cid:104)φ(x1), µx(cid:105)2] − Ex1[(cid:104)φ(x1), µx(cid:105)]2

− 2(Ex1[(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µy(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ex1 [(cid:104)φ(x1), µy(cid:105)])
+ Ey1 [(cid:104)φ(y1), µy(cid:105)2] − Ey1 [(cid:104)φ(y1), µy(cid:105)]2
− 2(Ey1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)] − Ey1[(cid:104)φ(y1), µy(cid:105)]Ey1[(cid:104)φ(y1), µx(cid:105)])
+ Ex1[(cid:104)φ(x1), µy(cid:105)2] − Ex1 [(cid:104)φ(x1), µy(cid:105)]2
+ Ey1 [(cid:104)φ(y1), µx(cid:105)2] − Ey1 [(cid:104)φ(y1), µx(cid:105)]2

Substituting empirical expectations over the data sample for the population expectations in Eq. (19)
gives

(18)

(19)

(20)

ζ1 ≈

1

m(m − 1)2 eT ˜Kxx ˜Kxxe −
(cid:18)

− 2

1
m(m − 1)n

(cid:18)

1
m(m − 1)

(cid:19)2

eT ˜Kxxe

eT ˜KxxKxye −

1
m2(m − 1)n

eT ˜KxxeeT Kxye

(cid:19)

(cid:18)

1

+

n(n − 1)2 eT ˜Kyy ˜Kyye −
(cid:18)

− 2

1
n(n − 1)m

eT ˜KyyKyxe −

(cid:19)2

eT ˜Kyye

1
n(n − 1)
1
n2(n − 1)m
(cid:19)2

+

1
m2n

eT ˜KyyeeT Kxye

(cid:19)

+

1
n2m

eT KyxKxye − 2

eT Kxye

eT KxyKyxe

(cid:18) 1
nm

12

Published as a conference paper at ICLR 2016

Derivation of the ﬁrst term for example

Ex1[(cid:104)x1, µx(cid:105)2] ≈

(cid:104)φ(xi),

φ(xj)(cid:105)(cid:104)φ(xi),

φ(xk)(cid:105)

(21)

1
m − 1

m
(cid:88)

k=1
k(cid:54)=i

k(xi, xj)k(xi, xk)

1
m

m
(cid:88)

i=1

=

1
m(m − 1)2

1
m − 1

m
(cid:88)

j=1
j(cid:54)=i

m
(cid:88)

m
(cid:88)

m
(cid:88)

i=1

j=1
j(cid:54)=i

k=1
k(cid:54)=i

1

=

m(m − 1)2 eT ˜Kxx ˜Kxxe

A.2 COVARIANCE OF MMD

We note many terms in expansion of the squares above cancel out due to independence. For example
Ex1,z1 [(cid:104)φ(x1), µx(cid:105)(cid:104)φ(z1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ez1 [(cid:104)φ(z1), µz(cid:105)] = 0.
We can thus simplify to the following expression for ζ1

ζ1 = Ex1,y1,z1 [Ex2,y2,z2 [h(x1, y1)g(x1, z1)]] − (cid:0)MMD2(F, PX , PY ) MMD2(F, PX , PZ)(cid:1)

(22)

(23)

= Ex1,y1,z1[((cid:104)φ(x1), µx(cid:105) + (cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)φ(x1), µy)(cid:105))

((cid:104)φ(x1), µx)(cid:105) + (cid:104)φ(z1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105))]

− MMD2(F, PX , PY ) MMD2(F, PX , PZ)

= Ex1

(cid:2)(cid:104)φ(x1), µx(cid:105)2(cid:3) − Ex1 [(cid:104)φ(x1), µx(cid:105)]2

− (Ex1 [(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)])
− (Ex1 [(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µy(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ex1 [(cid:104)φ(x1), µy(cid:105)])
+ Ex1 [(cid:104)φ(x1), µy(cid:105)(cid:104)φ(x1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)]

(cid:18)

(cid:19)2

≈

1

(cid:18)

−

m(m − 1)2 eT ˜Kxx ˜Kxxe −
1
m(m − 1)r
1
m(m − 1)n

−

(cid:18)

eT ˜Kxxe

1
m(m − 1)
1
m2(m − 1)r
1
m2(m − 1)n

(cid:18) 1

+

mnr

eT KyxKxze −

eT KxyeeT Kxze

1
m2nr

(cid:19)

eT ˜KxxKxze −

eT ˜KxxeeT Kxze

eT ˜KxxKxye −

eT ˜KxxeeT Kxze

(cid:19)

(cid:19)

A.3 DERIVATION OF THE VARIANCE OF THE DIFFERENCE OF TWO MMD STATISTICS

In this section we propose an alternate strategy of deriving directly the variance of a u-statistic of
the difference of MMDs with a joint variable. This formulation agrees with the derivation of the
covariance matrix and subsequent projection, and provides extra insights.

Let D := (d1, ..., dm) be m iid random variables where d := (x, y, z) ∼ Px × Py × Pz. Then the
difference of the unbiased estimators of MMD2(F, Px, Py) and MMD2(F, Px, Pz) is given by

MMD2

u(F, x, y) − MMD2

u(F, x, z) =

f (di, dj)

(24)

1
m(m − 1)

m
(cid:88)

i(cid:54)=j

with f , the kernel of MMD2(F, Px, Py) − MMD2(F, Px, Pz) of order 2 as follows

f (d1, d2) = (k(x1, x2) + k(y1, y2) − k(x1, y2) − k(x2, y1))

− (k(x1, x2) + k(z1, z2) − k(x1, z2) − k(x2, z1))

= (k(y1, y2) − k(x1, y2) − k(x2, y1)) − (k(z1, z2) − k(x1, z2) − k(x2, z1))

(25)

13

Published as a conference paper at ICLR 2016

Equation (24) is a U -statistic and thus we can apply Equation (14) to obtain its variance. We ﬁrst
note

Ed1(f (d1, d2)) :=(cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)µx, φ(y1)(cid:105)

− ((cid:104)φ(z1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105) − (cid:104)µx, φ(z1)(cid:105))

Ed1,d2(f (d1, d2)) := MMD2(F, Px, Py) − MMD2(F, Px, Pz)

We are now ready to derive the dominant leading term,ζ1, in the variance expression (14).

Term ζ1
ζ1 : = Var(Ed1 (f (d1, d2)))

= Ex1,y1,z1[((cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)µx, φ(y1)(cid:105) − ((cid:104)φ(z1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105) − (cid:104)µx, φ(z1)(cid:105))2]

− (MMD2(F, Px, Py) − MMD2(F, Px, Pz))2

(29)
We note many terms in expansion of the squares above cancel out due to independence. For example
Ey1,z1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(z1), µz(cid:105)] − Ey1[(cid:104)φ(y1), µy(cid:105)] Ez1 [(cid:104)φ(z1), µz(cid:105)] = 0.
We can thus simplify to the following expression for ζ1
ζ1 = Ey1 [(cid:104)φ(y1), µy(cid:105)2] − Ey1[(cid:104)φ(y1), µy(cid:105)]2

(30)

(26)

(27)

(28)

+ Ex1[(cid:104)φ(x1), µy(cid:105)2] − Ex1[(cid:104)φ(x1), µy(cid:105)]2
+ Ey1 [(cid:104)µx, φ(y1)(cid:105)2] − Ey1[(cid:104)µx, φ(y1)(cid:105)]2
+ Ez1 [(cid:104)φ(z1), µz(cid:105)2] − Ez1[(cid:104)φ(z1), µz(cid:105)]2
+ Ex1[(cid:104)φ(x1), µz(cid:105)2] − Ex1[(cid:104)φ(x1), µz(cid:105)]2
+ Ez1 [(cid:104)µx, φ(z1)(cid:105)2] − Ez1 [(cid:104)µx, φ(z1)(cid:105)]2
− 2(Ey1[(cid:104)φ(y1), µy(cid:105)(cid:104)µx, φ(y1)(cid:105)] − Ey1 [(cid:104)φ(y1), µy(cid:105)] Ey1[(cid:104)µx, φ(y1)(cid:105)])
− 2(Ex1 [(cid:104)φ(x1), µy(cid:105)(cid:104)φ(x1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)])
− 2(Ez1[(cid:104)φ(z1), µz(cid:105)(cid:104)µx, φ(z1)(cid:105)] − Ez1 [(cid:104)φ(z1), µz(cid:105)] Ez1[(cid:104)µx, φ(z1)(cid:105)])

We can empirically approximate these terms as follows:

1

ζ1 ≈

n(n − 1)2 eT ˜Kyy ˜Kyye −

(cid:18)

1
n(n − 1)

(cid:19)2

eT ˜Kyye

(31)

+

+

+

+

+

(cid:19)2

(cid:19)2

(cid:18) 1
nm
(cid:18) 1
nm
(cid:18)

1
n2m

eT K T

xyKxye −

eT Kxye

1
nm2 eT KxyK T

xye −

1

r(r − 1)2 eT ˜Kzz ˜Kzze −
(cid:18) 1
1
rm2 eT KxzK T
rm
(cid:18) 1
1
r2m
rm
(cid:18)

xzKxze −

eT K T

xze −

eT Kxye

1
r(r − 1)
(cid:19)2

eT Kxze

(cid:19)2

eT Kxze

(cid:19)2

eT ˜Kzze

− 2

− 2

− 2

1
n(n − 1)m

(cid:18) 1

(cid:18)

nmr
1
r(r − 1)m

eT ˜KyyKyxe −

eT ˜Kyye ×

eT Kxye

1
n(n − 1)

eT ˜K T

xyKxze −

eT Kxye ×

eT Kxze

1
rm

eT ˜KzzK T

xze −

1
n(n − 1)

eT ˜Kyye ×

eT Kxye

1
nm
(cid:19)

1
nm

(cid:19)

(cid:19)

1
nm

14

Published as a conference paper at ICLR 2016

A.4 EQUALITY OF DERIVATIONS

In this section, we prove that Equation (9) is equal to the variance of the difference of 2
MMD2(F, Px, Py) and MMD2(F, Px, Pz).

σ2
XY + σXZ − 2σXY XZ = Ey1

(cid:2)(cid:104)φ(y1), µy(cid:105)2(cid:3) − Ey1 [(cid:104)φ(y1), µy(cid:105)]2

(32)

(cid:2)(cid:104)φ(z1), µy(cid:105)2(cid:3) − Ez1 [(cid:104)φ(z1), µy(cid:105)]2

+ Ez1
− 2 (Ey1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)] − Ey1 [(cid:104)φ(y1), µy(cid:105)] Ey1 [(cid:104)φ(y1), µx(cid:105)])
− 2 (Ez1 [(cid:104)φ(z1), µz(cid:105)(cid:104)φ(z1), µx(cid:105)] − Ez1 [(cid:104)φ(z1), µz(cid:105)] Ez1 [(cid:104)φ(z1), µx(cid:105)])
(cid:2)(cid:104)φ(x1), µy(cid:105)2(cid:3) − Ex1 [(cid:104)φ(x1), µy(cid:105)]2
+ Ex1
(cid:2)(cid:104)φ(y1), µz(cid:105)2(cid:3) − Ey1 [(cid:104)φ(y1), µz(cid:105)]2
+ Ey1
(cid:2)(cid:104)φ(y1), µx(cid:105)2(cid:3) − Ey1 [(cid:104)φ(y1), µx(cid:105)]2
+ Ey1
(cid:2)(cid:104)φ(z1), µx(cid:105)2(cid:3) − Ez1 [(cid:104)φ(z1), µx(cid:105)]2
+ Ez1
− 2 (Ex1 [(cid:104)φ(x1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)])

We have shown that Equation (9) is equal to Equation (31).

B CALIBRATION OF THE TEST

We show here that our derived test is well calibrated. A calibrated
test should output a uniform distribution of p-values when the two
MMD distances are equal. The empirical distributions of p-values
for various sets of Px, Py and Pz are given in Figure 8. Similarly,
for a given signiﬁcance level α, the false positive rate should be
equal to α. The empirical false positive rates for varying α are
shown in Figure 7 further demonstrating the proper calibration of
the test.

y
c
n
e
u
q
e
r
F

p-values

Figure 7: Calibration of the rela-

tive similarity test

15

Published as a conference paper at ICLR 2016

(a) Illustration of the synthetic data with different

(b) Uniform histogram of p-values

means for X, Y and Z.

(c) Illustration of the synthetic data with different

(d) Uniform histogram of p-values

means and orientations for X, Y and Z.

(e) Illustration of the synthetic data with different ori-

(f) Uniform histogram of p-values

entations for X, Y and Z.

Figure 8: Calibration of the relative similarity test

16

6
1
0
2
 
b
e
F
 
5
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
8
5
4
0
.
1
1
5
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2016

A TEST OF RELATIVE SIMILARITY FOR MODEL
SELECTION IN GENERATIVE MODELS

Wacha Bounliphone,12∗ Eugene Belilovsky,12∗ & Matthew B. Blaschko2
1CentraleSup´elec & Inria Saclay, Universit´e Paris-Saclay, 92295 Chˆatenay-Malabry, France
2ESAT-PSI, KU Leuven, Kasteelpark Arenberg 10, 3001 Leuven, Belgium
{wacha.bounliphone,eugene.belilovsky}@inria.fr
matthew.blaschko@esat.kuleuven.be

Ioannis Antonoglou
Google Deepmind
5 New Street Square
London EC4A 3TW, UK
ioannisa@google.com

Arthur Gretton
Gatsby Computational Neuroscience Unit
University College London
25 Howland Street
London W1T 4JG, UK
arthur.gretton@gmail.com

ABSTRACT

Probabilistic generative models provide a powerful framework for representing
data that avoids the expense of manual annotation typically needed by discrimi-
native approaches. Model selection in this generative setting can be challenging,
however, particularly when likelihoods are not easily accessible. To address this
issue, we introduce a statistical test of relative similarity, which is used to de-
termine which of two models generates samples that are signiﬁcantly closer to
a real-world reference dataset of interest. We use as our test statistic the differ-
ence in maximum mean discrepancies (MMDs) between the reference dataset and
each model dataset, and derive a powerful, low-variance test based on the joint
asymptotic distribution of the MMDs between each reference-model pair. In ex-
periments on deep generative models, including the variational auto-encoder and
generative moment matching network, the tests provide a meaningful ranking of
model performance as a function of parameter and training settings.

1

INTRODUCTION

Generative models based on deep learning techniques aim to provide sophisticated and accurate
models of data, without expensive manual annotation (Bengio, 2009; Kingma et al., 2014). This
is especially of interest as deep networks tend to require comparatively large training samples to
achieve a good result (Krizhevsky et al., 2012). Model selection within this class of techniques can
be a challenge, however. First, likelihoods can be difﬁcult to compute for some families of recently
proposed models based on deep learning (Goodfellow et al., 2014; Li et al., 2015b). The current
best method to evaluate such models is based on Parzen-window estimates of the log likelihood
(Goodfellow et al., 2014, Section 5). Second, if we are given two models with similar likelihoods,
we typically do not have a computationally inexpensive hypothesis test to determine whether one
likelihood is signiﬁcantly higher than the other. Permutation testing or other generic strategies are
often computationally prohibitive, bearing in mind the relatively high computational requirements
of deep networks (Krizhevsky et al., 2012).

In this work, we provide an alternative strategy for model selection, based on a novel, non-parametric
hypothesis test of relative similarity. We treat the two trained networks being compared as generative
models (Goodfellow et al., 2014; Hinton et al., 2006; Salakhutdinov and Hinton, 2009), and test
whether the ﬁrst candidate model generates samples signiﬁcantly closer to a reference validation
set. The null hypothesis is that the ordering is reversed, and the second candidate model is closer to

∗These authors contributed equally to this work

1

Published as a conference paper at ICLR 2016

the reference (further, both samples are assumed to remain distinct from the reference, as will be the
case for any sufﬁciently complex modeling problem).

Our model selection criterion is based on the maximum mean discrepancy (MMD) (Gretton et al.,
2006; 2012a), which represents the distance between embeddings of empirical distributions in a re-
producing kernel Hilbert space (RKHS). The maximum mean discrepancy is a metric on the space of
probability distirbutions when a characteristic kernel is used (Fukumizu et al., 2008; Sriperumbudur
et al., 2010), meaning that the distribution embeddings are unique for each probability measure.
Recently, the MMD has been used in training generative models adversarially, (Li et al., 2015b;
Dziugaite et al., 2015), where the MMD measures the distance of the generated samples to some
reference target set; it has been used for statistical model criticism (Lloyd and Ghahramani, 2015);
and to minimize the effect of nuisance variables on learned representations (Louizos et al., 2016).

Rather than train a single model using the MMD distance to a reference distribution, our goal in
this work is to evaluate the relative performance of two models, by testing whether one generates
samples signiﬁcantly closer to the reference distribution than the other. This extends the applicability
of the MMD to problems of model selection and evaluation. Key to this result is a novel expression
for the joint asymptotic distribution of two correlated MMDs (between samples generated from
each model, and samples from the reference distribution). Li et al. (2015a) have derived the joint
distribution of a speciﬁc MMD estimator under the assumption that the distributions are equal. By
contrast, we derive the case in which the distributions are unequal, as is expected due to irreducible
model error.

We provide a detailed introduction to the MMD and its associated notation in Section 2. We derive
the joint asymptotic distribution of the MMDs in Section 3: this uses similar ideas to the relative
dependence test in Bounliphone et al. (2015), with the additional complexity due to there being three
independent samples to deal with, rather than a single joint sample. We formulate a hypothesis test
of relative similarity, to determine whether the difference in MMDs is statistically signiﬁcant. Our
ﬁrst test benchmark is on a synthetic data for which the ground truth is known (Section 4), where
we verify that the test performs correctly under the null and the alternative.

Finally, in Section 5, we demonstrate the performance of our test over a broad selection of model
comparison problems in the deep learning setting, by evaluating relative similarity of pairs of model
outputs to a validation set over a range of training regimes and settings. Our benchmark models in-
clude the variational auto-encoder (Kingma and Welling, 2014) and the generative moment matching
network (Li et al., 2015b). We ﬁrst demonstrate that the test performs as expected in scenarios where
the same model is trained with different training set sizes, and the relative ordering of model per-
formance is known. We then ﬁx the training set size and change various architectural parameters
of these networks, showing which models are signiﬁcantly preferred with our test. We validate the
rankings returned by the test using a separate set of data for which we compute alternate metrics for
assessing the models, such as classiﬁcation accuracy and likelihood.

2 BACKGROUND MATERIAL

In comparing samples from distributions, we use the Maximum Mean Discrepancy (MMD) (Gretton
et al., 2006; 2012a). We brieﬂy review this statistic and its asymptotic behaviour for a single pair of
samples.

Deﬁnition 1. (Gretton et al., 2012a, Deﬁnition 2: Maximum Mean Discrepancy (MMD)) Let F be
an RKHS, with the continuous feature mapping ϕ(x) ∈ F from each x ∈ X , such that the inner
product between the features is given by the kernel function k(x, x(cid:48)) := (cid:104)φ(x), φ(x(cid:48))(cid:105). Then the
squared population MMD is

MMD2(F, Px, Py) = Ex,x(cid:48) [k(x, x(cid:48))] − 2 Ex,y [k(x, y)] + Ey,y(cid:48) [k(y, y(cid:48))] .

(1)

The following theorem describes an unbiased quadratic-time estimate of the MMD, and its asymp-
totic distribution when Px and Py are different.
Theorem 1. (Gretton et al., 2012a, Lemma 6 & Corollary 16: Unbiased empirical estimate &
Asymptotic distribution of MMD2
u(F, Xm, Yn)) Deﬁne observations Xm := {x1, ..., xm} and

2

Published as a conference paper at ICLR 2016

(3)

(5)

Yn := {y1, ..., yn} independently and identically distributed (i.i.d.) from Px and Py, respectively.
An unbiased empirical estimate of MMD2(F, Px, Py) is a sum of two U -statistics and a sample
average,

MMD2

u(F, Xm, Yn) =

k(xi, xj) +

k(yi, yj)

(2)

1
n(n − 1)

n
(cid:88)

n
(cid:88)

i=1

j(cid:54)=i

1
m(m − 1)

m
(cid:88)

m
(cid:88)

i=1

j(cid:54)=i

−

2
mn

m
(cid:88)

n
(cid:88)

i=1

j=1

k(xi, yj).

Let V := (v1, ..., vm) be m i.i.d. random variables, where v := (x, y) ∼ Px × Py. When m = n,
an unbiased empirical estimate of MMD2(F, Px, Py) is

MMD2

u(F, Xm, Ym) =

1
m(m − 1)

m
(cid:88)

i(cid:54)=j

h(vi, vj)

with h(vi, vj) = k(xi, xj) + k(yi, yj) − k(xi, yj) − k(xj, yi). We assume E(h2) < ∞. When
Px (cid:54)= Py, MMD2
u(F, X, Y ) converges in distribution to a Gaussian according to
√
u(F, Xm, Yn) − MMD2(F, Px, Py)(cid:1) D−→ N (cid:0)0, σ2
m (cid:0)MMD2

(4)

XY

(cid:1)

where

uniformly at rate 1/

XY = 4 (cid:0)Ev1 [(Ev2h(v1, v2))2] − [(Ev1,v2 h(v1, v2))2](cid:1)
σ2
√
m.

A two-sample test may be constructed using the MMD as a test statistic, however when Px = Py
the statistic is degenerate, and the asymptotic distribution is a weighted sum of χ2 variables (which
can have inﬁnitely many terms, (Gretton et al., 2012a)).

By contrast, our problem setting is to determine with high signiﬁcance whether a target distribution
Px is closer to one of two candidate distributions Py, Pz, based on two empirical estimates of the
MMD and their variances. This requires us to characterize σ2
u(F, Xm, Yn) as well
as the covariance of two dependent estimates, MMD2
u(F, Xm, Zr) (the
dependence arises from the shared sample Xm). Fortunately, degeneracy does not arise if we assume
Py, Pz are each distinct from Px.

u(F, Xm, Yn) and MMD2

XY for MMD2

In the next section, we obtain the joint asymptotic distribution of two dependent MMD statistics. We
demonstrate how this joint distribution can be empirically estimated, and use the resulting parametric
form to construct a computationally efﬁcient and powerful hypothesis test for relative similarity.

3

JOINT ASYMPTOTIC DISTRIBUTION OF TWO CORRELATED MMDS AND A
RESULTING TEST STATISTIC

In this section, we derive our statistical test for relative similarity as measured by MMD. In order to
maximize the statistical efﬁciency of the test, we will reuse samples from the reference distribution,
denoted by Px, to compute the MMD estimates with two candidate distributions Py and Pz. We
consider two MMD estimates MMD2
u(F, Xm, Yn) and MMD2
u(F, Xm, Zr), and as the data sample
Xm is identical between them, these estimates will be correlated. We therefore ﬁrst derive the joint
asymptotic distribution of these two metrics and use this to construct a statistical test.
Theorem 2. We assume that Px (cid:54)= Py, Px (cid:54)= Pz, E(k(xi, xj)) < ∞, E(k(yi, yj)) < ∞ and
E(k(xi, yj)) < ∞, then
√

(cid:19)(cid:19)

(cid:19)

(cid:19)

(cid:19)(cid:19)

(cid:18)(cid:18)MMD2
MMD2

m

u(F, Xm, Yn)
u(F, Xm, Zr)

−

(cid:18)MMD2(F, Px, Py)
MMD2(F, Px, Pz)

(cid:18)(cid:18)0
0

(cid:18) σ2

XY
σXY XZ

,

σXY XZ
σ2

XZ

d−→ N

(6)

We substitute the kernel MMD deﬁnition from Equation (2), expand the terms in the expectation,
and determine their empirical estimates in order to compute the variances in practice. The proof and
additional details of the following derivations are given in Appendix A.

3

Published as a conference paper at ICLR 2016

An empirical estimate of σXY XZ in Equation (6), neglecting higher order terms, can be computed
in O(m2):

σXY XZ ≈

(cid:18)

(cid:19)2

1

(cid:18)

−

m(m − 1)2 eT ˜Kxx ˜Kxxe −
1
m(m − 1)r
1
m(m − 1)n

−

(cid:18)

eT ˜Kxxe

1
m(m − 1)
1
m2(m − 1)r
1
m2(m − 1)n

(cid:18) 1

+

mnr

eT KyxKxze −

eT KxyeeT Kxze

1
m2nr

(cid:19)

eT ˜KxxKxze −

eT ˜KxxeeT Kxze

eT ˜KxxKxye −

eT ˜KxxeeT Kxze

(7)

(cid:19)

(cid:19)

where e is a vector of 1s with appropriate size, while ˜Kxx, Kxy and Kxz refer to the kernel matri-
ces, with ˜K indicating that the diagonal entries have been set to zero (cf. Appendix A). Similarly,
Equation (5) is constructed as in Equation (7).

Based on the empirical distribution from Equation (6), we now describe a statistical test to solve the
following problem:
Problem 1 (Relative similarity test). Let Px, Py, x and y be deﬁned as above, z be an independent
random variables with distribution Pz. Given observations Xm := {x1, ..., xm}, Yn := {y1, ..., yn}
from Px, Py and Pz respectively such that Px (cid:54)= Py, Px (cid:54)= Pz,
and Zr := {z1, ..., zr} i.i.d.
we test the hypothesis that Px is closer to Pz than Py i.e. we test the null hypothesis H0:
MMD(F, Px, Py) ≤ MMD(F, Px, Pz) versus the alternative hypothesis H1: MMD(F, Px, Py) >
MMD(F, Px, Pz) at a given signiﬁcance level α

u(F, Xm, Yn) − MMD2

The test statistic MMD2
u(F, Xm, Zr) is used to compute the p-value p for
the standard normal distribution. The test statistic is obtained by rotating the joint distribution
(cf. Eq. 6) by π
4 about the origin, and integrating the resulting projection on the ﬁrst axis, in a
manner similar to Bounliphone et al. (2015). Denote the asymptotically normal distribution of
√
u(F, Xm, Zr)]T as N (µ, Σ). The resulting distribution from ro-
tating by π/4 and projecting onto the primary axis is N (cid:0)[Rµ]1, [RΣRT ]11

u(F, Xm, Yn); MMD2

m[MMD2

(cid:1) where

[Rµ]1 =

(MMD2

u(F, Xm, Yn) − MMD2

u(F, Xm, Zr))

[RΣRT ]11 =

XY + σ2

XZ − 2σXY XZ)

√

2
2
1
(σ2
2

with R is the rotation by π/4. Then, the p-values for testing H0 versus H1 are
(cid:33)

(cid:32)

MMD2

u(F, Xm, Yn) − MMD2

u(F, Xm, Zr)

p ≤ Φ

−

(cid:112)σ2

XY + σ2
where Φ is the CDF of a standard normal distribution. We have made code for performing the test
is available.1

XZ − 2σXY XZ

4 EXPERIMENTAL VALIDATION OF THE RELATIVE MMD TEST

We verify the validity of the hypothesis test described above using a synthetic data set in which we
can directly control the relative similarity between distributions. We constructed three Gaussian dis-
tributions as illustrated in Figure 1. These Gaussian distributions are speciﬁed with different means
so that we can control the degree of relative similarity between them. The question is whether the
similarity between X and Z is greater than the similarity between X and Y . In these experiments, we
used a Gaussian kernel with bandwidth selected as the median pairwise distance between data points,
and we ﬁxed µY = [−20, −20], µZ = [20, 20] and varied µX such that µX = (1 − γ)µY + γµZ, for
41 regularly spaced values of γ ∈ [0.1, 0.9] (avoiding the degenerate cases Px = Py or Px = Pz).

1Code and examples can be found at https://github.com/eugenium/MMD

(8)

(9)

(10)

4

Published as a conference paper at ICLR 2016

u(F, X, Y ) is almost equal to MMD2

Figure 3 shows the p-values of the relative similarity test for different distribution. When γ is
varying around 0.5, i.e., when MMD2
u(F, X, Z), the p-values
quickly transition from 1 to 0, indicating strong discrimination of the test. In Figure 2, we compare
the power of our test to the power of a naive test when the reference sample is split in two, and
the MMDs have no covariance: clearly, the latter simple approach does worse than ours (a similar
comparison in testing relative dependence returned the same advantage for a test based on the joint
distribution; see Bounliphone et al. (2015, Section 3)). Figure 4 shows an empirical scatter plot of
the pairs of MMD statistics along with a 2σ iso-curve of the estimated distribution, demonstrating
that the parametric Gaussian distribution is well calibrated to the empirical values. Futhermore, we
validate our derived formulas using simulations in Appendix B, where we show the p-values have
the correct distribution under the null.

s
t
s
e
t

e
h
t

f
o

r
e
w
o
P

Figure 1: Illustration of

the synthetic dataset
where X, Y and Z are respectively
Gaussian distributed with mean µX =
[0, 0]T , µY = [−20, −20]T , µZ =
[20, 20]T and with variance ( 1 0

0 1 ).

γ

Figure 2: Comparison of the power of the pro-
posed method to an independent test
analogous to Bounliphone et al. (2015,
Section 3) as a function of γ.

s
e
u
l
a
v
-
p

γ
m = 1000

Figure 3: We ﬁxed µY = [−5, −5], µZ = [5, 5] and
varied µX such that µX = (1 − γ)µY +
γµZ , for 41 regularly spaced values of γ ∈
[0.1, 0.9] versus p-values for 100 repeated
tests.

Figure 4: The empirical scatter plot of the joint MMD
statistics with m = 1000 for 200 repeated
tests, along with the 2σ iso-curve of the an-
alytical Gaussian distribution estimated by
Equation (6). The analytical distribution
closely matches the empirical scatter plot,
verifying the correctness of the variances.

5 MODEL SELECTION FOR DEEP UNSUPERVISED NEURAL NETWORKS

An important potential application of the Relative MMD can be found in recent work on unsu-
pervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014;
Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015b; Goodfellow et al.,
2014). As noted by several authors, the evaluation of generative models is a challenging open prob-
lem (Li et al., 2015b; Goodfellow et al., 2014), and the distributions of samples from these models
are very complex and difﬁcult to evaluate. The relative MMD performance can be used to compare
different model settings, or even model families, in a statistically valid framework. To compare two
models using our test, we generate samples from both, and compare these to a set of real target data
samples that were not used to train either model.

In the experiments in the sequel we focus on the recently introduced variational auto-encoder (VAE)
(Kingma and Welling, 2014) and the generative moment matching networks (GMMN) (Li et al.,

5

Published as a conference paper at ICLR 2016

(a)

(b)

Figure 5: (a) Variational auto-encoder reference model. We have 400 hidden nodes (both encoder and decoder)
and 20 latent variables in the reference model for our experiments.
(b) Auto-Encoder + GMMN
reference model. The auto-encoder (indicated in orange) is trained separately and has 1024 and
32 hidden nodes in decode and encode hidden layers. The GMMN has 10 variables generated by
the prior, and the hidden layers have 64, 256, 256, 1024 nodes in each layer respectively. In both
networks red arrows indicate the data ﬂow during sampling

2015b). The former trains an encoder and decoder network jointly minimizing a regularized vari-
ational lower bound (Kingma and Welling, 2014). While the latter class of models is purely gen-
erative minimizing an MMD based objective, this model works best when coupled with a separate
auto-encoder which reduces the dimensionality of the data. An architectural schematic for both
classes of models is provided in Fig. 5. Both these models can be trained using standard backprop-
agation (Rumelhart et al., 1988). Using the latent variable prior we can directly sample the data
distribution of these models without using MCMC procedures (Hinton et al., 2006; Salakhutdinov
and Hinton, 2009).

We use the MNIST and FreyFace datasets for our analysis (LeCun et al., 1998; Kingma and Welling,
2014; Goodfellow et al., 2014). We ﬁrst demonstrate the effectiveness of our test in a setting where
we have a theoretical basis for expecting superiority of one unsupervised model versus another.
Speciﬁcally, we use a setup where more training samples were used to create one model versus
the other. We ﬁnd that the Relative MMD framework agrees with the expected results (models
trained with more data generalize better). We then demonstrate how the Relative MMD can be used
in evaluating network architecture choices, and we show that our test strongly agrees with other
established metrics, but in contrast can provide signiﬁcance results using just the validation data
while other methods may require an additional test set.

Several practical matters must be considered when applying the Relative MMD test. The selection
of kernel can affect the quality of results, particularly more suitable kernels can give a faster conver-
gence. In this work we extend the logic of the median heuristic (Gretton et al., 2012b) for bandwidth
selection by computing the median pairwise distance between samples from Px and Py and aver-
aging that with the median pairwise distance between samples from Px and Pz, which helps to
maximize the difference between the two MMD statistics. Although the derivations for the variance
of our statistic hold for all cases, the estimates require asymptotic arguments and thus a sufﬁciently
large n. Selecting the kernel bandwidth in an appropriate range can therefore substantially increase
the power of the test at a ﬁxed sample size. While we observed the median heuristic to work well
in our experiments, there are cases where alternative choices of kernel can provide greater power:
for instance, the kernel can be chosen to maximize the expected test power on a held-out dataset
(Gretton et al., 2012b).

6

Published as a conference paper at ICLR 2016

5.1 VARIATIONAL AUTO-ENCODER SAMPLE SIZE AND ARCHITECTURE EXPERIMENTS

We use the architecture from Kingma and Welling (2014) with a hidden layer at both the encoder
and decoder and a latent variable layer as shown in Figure 5a. We use sigmoidal activation for
the hidden layers of encoder and decoder. For the FreyFace data, we use a Gaussian prior on the
latent space and data space. For MNIST, we used a Bernoulli prior for the data space. We ﬁx the
training set size of the second auto-encoder to 300 images for the FreyFace data and 1500 images
for the MNIST data. We vary the number of training samples for the ﬁrst auto-encoder. We then
generate samples from both auto-encoders and compare them using Relative MMD to a held out
set of data. We use 1500 FreyFace samples as the target in Relative MMD and 15000 images from
MNIST. Since a single sample of the data might lead to better generalization performance by chance,
we repeat this experiment multiple times and record whether the relative similarity test indicated a
network is preferred or if it failed to reject the null hypothesis. The results are shown in Figure 6
which demonstrates that we are closely following the expected model preferences. Additionally for
MNIST we use another separate set of supervised training and test data. We encode this data using
both auto-encoders and use logistic regression to obtain a classiﬁcation accuracy. The indicated
accuracies closely match the results of the relative similarity test, further validating the test.

(a)

(c)

(b)

Figure 6: We show the effect of (a) varying the train-
ing set size of one auto-encoder trained on
MNIST data. (c) As a secondary valida-
tion we compute the classiﬁcation accu-
racy of MNIST on a separate train/test set
encoded using encoder 1 and encoder 2.
(b) We then show the effect of varying the
training set size of one auto-encoder using
the FreyFace data. We note that due to the
size of the FreyFace dataset, we limit the
range of ratios used. From this ﬁgure we
see that the results of the relative similarity
test match our expectation: more data pro-
duces models which more closely match
the true distribution.

We consider model selection between networks using different architectures. We train two encoders,
one a ﬁxed reference model (400 hidden units and 20 latent variables), and the other varying as
speciﬁed in Table 1. 25000 images from the MNIST data set were used for training. We use another
20000 images as the target data in Relative MMD. Finally, we use a set of 10000 training and 10000
test images for a supervised task experiment. We use the labels in the MNIST data and perform
training and classiﬁcation using an (cid:96)2-regularized logistic regression on the encoded features. In
addition we use the supervised task test data to evaluate the variational lower bound of the data under
the two models (Kingma and Welling, 2014). We show the result of this experiment in Table 1. For
each comparison we take a different subset of training data which helps demonstrate the variation in

7

Published as a conference paper at ICLR 2016

lower bound and accuracy when re-training the reference architecture. We use a signiﬁcance value
of 5% and indicate when the test favors one auto-encoder over another or fails to reject the null
hypothesis. We ﬁnd that Relative MMD evaluation of the models closely matches performance on
the supervised task and the test set variational lower bound.

Accuracy (%) Accuracy (%) Lower Bound Lower Bound

Hidden Latent Result
VAE 1
200
200
400
800
800

VAE 1 RelativeMMD VAE 1
Favor VAE 2
5
Favor VAE 2
20
Favor VAE 1
50
Favor VAE 1
20
Favor VAE 1
50

92.8 ± 0.3
92.6 ± 0.3
94.6 ± 0.2
94.8 ± 0.2
94.2 ± 0.3

VAE 2
94.7 ± 0.2
94.5 ± 0.2
94.0 ± 0.2
93.9 ± 0.2
94.5 ± 0.2

VAE 1
-126
-115
-99.6
-111
-101

VAE 2
-97
-105
-123.44
-115
-103

Table 1: We compare several variational auto encoder (VAE) architectural choices for the number of hidden
units in both decoder and encoder and the number of latent variables for the VAE. The reference
encoder, denoted encoder 2, has 400 hidden units and 20 latent variables. We denote the competing
architectural models as encoder 1. We vary the number of hidden nodes in both the decoder and
encoder and the number of latent variables. Our test closely follows the performance difference of the
auto-encoder on a supervised task (MNIST digit classiﬁcation) as well as the variational lower bound
on a withheld set of data. The data used for evaluating the Accuracy and Lower Bound is separate
from that used to train the auto-encoders and for the hypothesis test.

5.2 GENERATIVE MOMENT MATCHING NETWORKS ARCHITECTURE EXPERIMENTS

We demonstrate our hypothesis test on a different class of deep generative models called Generative
Moment Matching Networks (GMMN) Li et al. (2015b). This recently introduced model has shown
competitive performance in terms of test set likelihood on the MNIST data. Furthermore the training
of this model is based on the MMD criterion. Li et al. (2015b) proposes to use that model along with
an auto-encoder, which is the setup we employ in this work. Here a standard auto-encoder model is
trained on the data to obtain a low dimensional representation, then a GMMN network is trained on
the latent representations (Figure 5).

We use the relative similarity test to evaluate various architectural choices in this new class of mod-
els. We start from the baseline model speciﬁed in Li et al. (2015b) and associated software. The
details of the reference model are speciﬁed in Figure 5.

We vary the number of auto-encoder hidden layers (1 to 4), generative model layers(1, 4, or 5), the
number of network nodes (all or 50% of the reference model), and use of drop-out on the auto-
encoder. We use the same training set of 55000, validation set of 5000 and test set of 10000 as in
(Li et al., 2015b; Goodfellow et al., 2014). In total we train 48 models. We use these to compare
4 simpliﬁed binary network architecture choices using the Relative MMD: using dropout on the
auto-encoder, few (1) or more (4 or 5) GMMN layers, few (1 or 2) or more (3 or 4) auto-encoder
layers, and the number of network nodes. We use our test to compare these model settings using
the validation set as the target in the relative similarity test, and samples from the models as the two
sources. To validate our results we compare it to likelihoods computed on the test set. The results
are shown in Table 2. We see that the likelihood results computed on a separate test set follow the
conclusions obtained from MMD on the validation set. Particularly, we ﬁnd that using fewer hidden
layers for the GMMN and more hidden nodes generally produces better models.

5.3 DISCUSSION

In these experiments we have seen that the Relative MMD test can be used to compare deep gen-
erative models obtaining judgements aligned with other metrics. Comparisons to other metrics are
important for verifying our test is sensible, but it can occlude the fact that MMD is a valid eval-
uation technique on its own. When evaluating only sample generating models where likelihood
computation is not possible, MMD is an appropriate and tractable metric to consider in addition to
Parzen-Window log likelihoods and visual appearance of the samples. In several ways it is poten-
tially more appropriate than Parzen-windows as it allows one to consider directly the discrepancy
between the test data samples and the model samples while allowing for signiﬁcance results. In such
a situation, comparing the performance of several models using the MMD against a single set of test

8

Published as a conference paper at ICLR 2016

Experimental Condition (A/B)
Dropout/No Dropout
More/Fewer GMMN Layers
More/Fewer Nodes
More/Fewer AE layers

RelativeMMD Preference
B
Inconclusive
A
−9.01 ± 55.43
360
17
199
393 −73.99 ± 40.96
14
105
125.2 ± 43.4
113
13
450
41.78 ± 44.07
324
21
231

76.76 ± 42.83
249.6 ± 8.07
−57 ± 49.57
25.96 ± 55.85

Avg Likelihood A Avg Likelihood B

Table 2: For each experimental condition (e.g. dropout or no dropout) we show the number of times the Relative
MMD prefers models in group 1 or 2 and number of inconclusive tests. We use the validation set as the
target data for Relative MMD. An average likelihood for the MNIST test set for each group is shown
with error bars. We can see that the MMD choices are in agreement with likelihood evaluations.
Particularly we identify that models with fewer GMMN layers and models with more nodes have
more favourable samples, which is conﬁrmed by the likelihood results.

samples, the Relative MMD test can provide an automatic signiﬁcance value without expensive
cross-validation procedures.

Gaussian kernels are closely related to Parzen-window estimates, thus computing an MMD in this
case can be considered related to comparing Parzen window log-likelihoods. The MMD gives sev-
eral advantages, however. First, the asymptotics of MMD are quite different to Parzen-windows,
since the Parzen-window bandwidth shrinks as n grows. Asymptotics of relative tests with shrink-
ing bandwidth are unknown: even for two samples this is challenging (Krishnamurthy et al., 2015).
Other two sample tests are not easily extendable to relative tests (Rosenbaum, 2005; Friedman and
Rafsky, 1979; Hall and Tajvidi, 2002). This is because the tests above rely on graph edge counting
or nearest neighbor-type statistics, and null distributions are obtained via combinatorial arguments
which are not easily extended from two to three samples. MMD is a U -statistic, hence its asymptotic
behavior is much more easily generalised to multiple dependent statistics.

There are two primary advantages of the MMD over the variational lower bound, where it is known
(Kingma and Welling, 2014): ﬁrst, we have a characterization of the asymptotic behavior, which
allows us to determine when the difference in performance is signiﬁcant; second, comparing two
lower bounds produced from two different models is unreliable, as we do not know how conservative
either lower bound is.

6 CONCLUSION

We have described a novel non-parametric statistical hypothesis test for relative similarity based on
the Maximum Mean Discrepancy. The test is consistent, and the computation time is quadratic.
Our proposed test statistic is theoretically justiﬁed for the task of comparing samples from arbitrary
distributions as it can be shown to converge to a quantity which compares all moments of the two
pairs of distributions.

We evaluate test performance on synthetic data, where the degree of similarity can be controlled.
Our experimental results on model selection for deep generative networks show that Relative MMD
can be a useful approach to comparing such models. There is a strong correspondence between the
test resuts and the expected likelihood, prediction accuracy, and variational lower bounds on the
models tested. Moreover, our test has the advantage over these alternatives of providing guarantees
of statistical signiﬁcance to its conclusions. This suggests that the relative similarity test will be
useful in evaluating hypotheses about network architectures, for example that AE-GMMN models
may generalize better when fewer layers are used in the generative model. Code for our method is
available.2

ACKNOWLEDGMENTS

We thank Joel Veness for helpful comments. This work is partially funded by Internal Funds KU
Leuven, ERC Grant 259112, FP7-MC-CIG 334380, the Royal Academy of Engineering through the
Newton Alumni Scheme, and DIGITEO 2013-0788D-SOPRANO. WB is supported by a Centrale-
Sup´elec fellowship.

2 https://github.com/eugenium/MMD

9

Published as a conference paper at ICLR 2016

REFERENCES

1–127, 2009.

Y. Bengio. Learning deep architectures for AI. Foundations and trends in Machine Learning, 2(1):

Y. Bengio, E. Thibodeau-Laufer, G. Alain, and J. Yosinski. Deep generative stochastic networks
trainable by backprop. In Proceedings of the 31st International Conference on Machine Learning,
2014.

W. Bounliphone, A. Gretton, A. Tenenhaus, and M. B. Blaschko. A low variance consistent test
of relative dependency. In F. Bach and D. Blei, editors, Proceedings of The 32nd International
Conference on Machine Learning, volume 37 of JMLR Workshop and Conference Proceedings,
pages 20–29, 2015.

G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training generative neural networks via maximum
mean discrepancy optimization. In Conference on Uncertainty in Artiﬁcial Intelligence, 2015.

J. H. Friedman and L. C. Rafsky. Multivariate generalizations of the Wald-Wolfowitz and Smirnov

two-sample tests. The Annals of Statistics, pages 697–717, 1979.

K. Fukumizu, A. Gretton, X. Sun, and B. Sch¨olkopf. Kernel measures of conditional dependence.

pages 489–496, Cambridge, MA, 2008. MIT Press.

I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems,
pages 2672–2680, 2014.

A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. J. Smola. A kernel method for
the two-sample-problem. In Advances in neural information processing systems, pages 513–520,
2006.

A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch¨olkopf, and A. Smola. A kernel two-sample test.

The Journal of Machine Learning Research, 13(1):723–773, 2012a.

A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, K. Fukumizu, and B. K.
Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. In F. Pereira, C. Burges,
L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25,
pages 1205–1213. 2012b.

P. Hall and N. Tajvidi. Permutation tests for equality of distributions in high-dimensional settings.

Biometrika, 89(2):359–374, 2002.

G. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural

computation, 18(7):1527–1554, 2006.

W. Hoeffding. A class of statistics with asymptotically normal distribution. The annals of mathe-

matical statistics, pages 293–325, 1948.

D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on

Learning Representations, 2014.

D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling. Semi-supervised learning with deep
generative models. In Advances in Neural Information Processing Systems, pages 3581–3589,
2014.

A. Krishnamurthy, K. Kandasamy, B. P´oczos, and L. A. Wasserman. On estimating L2

2 divergence.
In Proceedings of the Eighteenth International Conference on Artiﬁcial Intelligence and Statistics,
2015.

A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012.

H. Larochelle and I. Murray. The neural autoregressive distribution estimator. Journal of Machine

Learning Research, 15:29–37, 2011.

10

Published as a conference paper at ICLR 2016

Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

S. Li, Y. Xie, H. Dai, and L. Song. M-statistic for kernel change-point detection. In Advances in

Neural Information Processing Systems, pages 3348–3356, 2015a.

Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In International Confer-

ence on Machine Learning, pages 1718–1727, 2015b.

J. R. Lloyd and Z. Ghahramani. Statistical model criticism using kernel two sample tests. In Ad-

vances in Neural Information Processing Systems, 2015.

C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel. The variational fair auto encoder. In

International Conference on Learning Representations, 2016.

P. R. Rosenbaum. An exact distribution-free test comparing two multivariate distributions based on
adjacency. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(4):
515–530, 2005.

D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating
errors. In J. A. Anderson and E. Rosenfeld, editors, Neurocomputing: Foundations of Research,
pages 696–699. MIT Press, Cambridge, MA, USA, 1988.

R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In International Conference on

Artiﬁcial Intelligence and Statistics, pages 448–455, 2009.

R. J. Serﬂing. Approximation theorems of mathematical statistics, volume 162. John Wiley & Sons,

2009.

B. K. Sriperumbudur, A. Gretton, K. Fukumizu, G. R. Lanckriet, and B. Sch¨olkopf. Hilbert space
embeddings and metrics on probability measures. Journal of Machine Learning Research, 11:
1517–1561, 2010.

A DETAILED DERIVATIONS OF THE TEST VARIANCE AND COVARIANCE

The variance and the covariance for a U -statistic is described in Hoeffding (1948, Eq. 5.13) and
Serﬂing (2009, Chap. 5).

Let V := (v1, ..., vm) be m iid random variables where v := (x, y) ∼ Px × Py. An unbiased
estimator of MMD2(F, Px, Py) is

MMD2

u(F, Xm, Yn) =

1
m(m − 1)

m
(cid:88)

i(cid:54)=j

h(vi, vj)

with h(vi, vj) = k(xi, xj) + k(yi, yj) − k(xi, yj) − k(xj, yi).

Similarly, let W := (w1, ..., wm) be m iid random variables where w := (x, z) ∼ Px × Pz. An
unbiased estimator of MMD2(F, Px, Pz) is

MMD2

u(F, Xm, Zr) =

m
(cid:88)

1
m(m − 1)

g(wi, wj)

i(cid:54)=j
with g(wi, wj) = k(xi, xj) + k(zi, zj) − k(xi, zj) − k(xj, zi)

Then the variance/covariance for a U -statistic with a kernel of order 2 is given by

Equation (13) with neglecting higher terms can be written as

V ar(MMD2

u) =

4(m − 2)
m(m − 1)

ζ1 +

2
m(m − 1)

ζ2

V ar(MMD2

u) =

4(m − 2)
m(m − 1)

ζ1 + O(m−2)

(11)

(12)

(13)

(14)

where for the variance term, ζ1 = Var [Ev1 [h(v1, V2)]] and for the covariance term ζ1 =
Var [Ev1,w1 [h(v1, V2)g(w1, W2)]].

11

Published as a conference paper at ICLR 2016

Notation [ ˜Kxx]ij = [Kxx]ij for all i (cid:54)= j and [ ˜Kxx(cid:48)]ij = 0 for j = i. Same for ˜Kyy and ˜Kzz.
We will also make use of the fact that k(xi, xj) = (cid:104)φ(xi), φ(xj)(cid:105) for an appropriately chosen inner
product, and function φ. We then denote

(cid:90)

µx :=

φ(x)dPx.

(15)

A.1 VARIANCE OF MMD

We note many terms in expansion of the squares above cancel out due to independence. For example
Ex1,y1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(x1), µy(cid:105)] − Ey1 [(cid:104)φ(y1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µy(cid:105)] = 0.
We can thus simplify to the following expression for ζ1

(cid:104)

(Ex2,y2 [h(x1, y1)])2(cid:105)
(16)
(cid:2)((cid:104)φ(x1), µx(cid:105) + (cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)µx, φ(y1)(cid:105))2(cid:3) − (cid:0)MMD2(F, PX , PY )(cid:1)2
(17)

− (cid:0)MMD2(F, PX , PY )(cid:1)2

ζ1 = Ex1,y1

= Ex1,y1

= Ex1,y1

(cid:2)(cid:104)φ(x1), µx(cid:105)2 + 2(cid:104)φ(x1), µx(cid:105)(cid:104)φ(y1), µy(cid:105) − 2(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µy(cid:105)
− 2(cid:104)φ(x1), µx(cid:105)(cid:104)φ(y1), µx(cid:105) + (cid:104)φ(y1), µy(cid:105)2
− 2(cid:104)φ(y1), µy(cid:105)(cid:104)φ(x1), µy(cid:105) − 2(cid:104)φ(y1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)
+ (cid:104)φ(x1), µy(cid:105)2 + 2(cid:104)φ(x1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)
+ (cid:104)φ(y1), µx(cid:105)2(cid:3) − (cid:0)MMD2(F, PX , PY )(cid:1)2

= Ex1 [(cid:104)φ(x1), µx(cid:105)2] − Ex1[(cid:104)φ(x1), µx(cid:105)]2

− 2(Ex1[(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µy(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ex1 [(cid:104)φ(x1), µy(cid:105)])
+ Ey1 [(cid:104)φ(y1), µy(cid:105)2] − Ey1 [(cid:104)φ(y1), µy(cid:105)]2
− 2(Ey1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)] − Ey1[(cid:104)φ(y1), µy(cid:105)]Ey1[(cid:104)φ(y1), µx(cid:105)])
+ Ex1[(cid:104)φ(x1), µy(cid:105)2] − Ex1 [(cid:104)φ(x1), µy(cid:105)]2
+ Ey1 [(cid:104)φ(y1), µx(cid:105)2] − Ey1 [(cid:104)φ(y1), µx(cid:105)]2

Substituting empirical expectations over the data sample for the population expectations in Eq. (19)
gives

(18)

(19)

(20)

ζ1 ≈

1

m(m − 1)2 eT ˜Kxx ˜Kxxe −
(cid:18)

− 2

1
m(m − 1)n

(cid:18)

1
m(m − 1)

(cid:19)2

eT ˜Kxxe

eT ˜KxxKxye −

1
m2(m − 1)n

eT ˜KxxeeT Kxye

(cid:19)

(cid:18)

1

+

n(n − 1)2 eT ˜Kyy ˜Kyye −
(cid:18)

− 2

1
n(n − 1)m

eT ˜KyyKyxe −

(cid:19)2

eT ˜Kyye

1
n(n − 1)
1
n2(n − 1)m
(cid:19)2

+

1
m2n

eT ˜KyyeeT Kxye

(cid:19)

+

1
n2m

eT KyxKxye − 2

eT Kxye

eT KxyKyxe

(cid:18) 1
nm

12

Published as a conference paper at ICLR 2016

Derivation of the ﬁrst term for example

Ex1[(cid:104)x1, µx(cid:105)2] ≈

(cid:104)φ(xi),

φ(xj)(cid:105)(cid:104)φ(xi),

φ(xk)(cid:105)

(21)

1
m − 1

m
(cid:88)

k=1
k(cid:54)=i

k(xi, xj)k(xi, xk)

1
m

m
(cid:88)

i=1

=

1
m(m − 1)2

1
m − 1

m
(cid:88)

j=1
j(cid:54)=i

m
(cid:88)

m
(cid:88)

m
(cid:88)

i=1

j=1
j(cid:54)=i

k=1
k(cid:54)=i

1

=

m(m − 1)2 eT ˜Kxx ˜Kxxe

A.2 COVARIANCE OF MMD

We note many terms in expansion of the squares above cancel out due to independence. For example
Ex1,z1 [(cid:104)φ(x1), µx(cid:105)(cid:104)φ(z1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ez1 [(cid:104)φ(z1), µz(cid:105)] = 0.
We can thus simplify to the following expression for ζ1

ζ1 = Ex1,y1,z1 [Ex2,y2,z2 [h(x1, y1)g(x1, z1)]] − (cid:0)MMD2(F, PX , PY ) MMD2(F, PX , PZ)(cid:1)

(22)

(23)

= Ex1,y1,z1[((cid:104)φ(x1), µx(cid:105) + (cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)φ(x1), µy)(cid:105))

((cid:104)φ(x1), µx)(cid:105) + (cid:104)φ(z1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105))]

− MMD2(F, PX , PY ) MMD2(F, PX , PZ)

= Ex1

(cid:2)(cid:104)φ(x1), µx(cid:105)2(cid:3) − Ex1 [(cid:104)φ(x1), µx(cid:105)]2

− (Ex1 [(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)])
− (Ex1 [(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µy(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ex1 [(cid:104)φ(x1), µy(cid:105)])
+ Ex1 [(cid:104)φ(x1), µy(cid:105)(cid:104)φ(x1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)]

(cid:18)

(cid:19)2

≈

1

(cid:18)

−

m(m − 1)2 eT ˜Kxx ˜Kxxe −
1
m(m − 1)r
1
m(m − 1)n

−

(cid:18)

eT ˜Kxxe

1
m(m − 1)
1
m2(m − 1)r
1
m2(m − 1)n

(cid:18) 1

+

mnr

eT KyxKxze −

eT KxyeeT Kxze

1
m2nr

(cid:19)

eT ˜KxxKxze −

eT ˜KxxeeT Kxze

eT ˜KxxKxye −

eT ˜KxxeeT Kxze

(cid:19)

(cid:19)

A.3 DERIVATION OF THE VARIANCE OF THE DIFFERENCE OF TWO MMD STATISTICS

In this section we propose an alternate strategy of deriving directly the variance of a u-statistic of
the difference of MMDs with a joint variable. This formulation agrees with the derivation of the
covariance matrix and subsequent projection, and provides extra insights.

Let D := (d1, ..., dm) be m iid random variables where d := (x, y, z) ∼ Px × Py × Pz. Then the
difference of the unbiased estimators of MMD2(F, Px, Py) and MMD2(F, Px, Pz) is given by

MMD2

u(F, x, y) − MMD2

u(F, x, z) =

f (di, dj)

(24)

1
m(m − 1)

m
(cid:88)

i(cid:54)=j

with f , the kernel of MMD2(F, Px, Py) − MMD2(F, Px, Pz) of order 2 as follows

f (d1, d2) = (k(x1, x2) + k(y1, y2) − k(x1, y2) − k(x2, y1))

− (k(x1, x2) + k(z1, z2) − k(x1, z2) − k(x2, z1))

= (k(y1, y2) − k(x1, y2) − k(x2, y1)) − (k(z1, z2) − k(x1, z2) − k(x2, z1))

(25)

13

Published as a conference paper at ICLR 2016

Equation (24) is a U -statistic and thus we can apply Equation (14) to obtain its variance. We ﬁrst
note

Ed1(f (d1, d2)) :=(cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)µx, φ(y1)(cid:105)

− ((cid:104)φ(z1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105) − (cid:104)µx, φ(z1)(cid:105))

Ed1,d2(f (d1, d2)) := MMD2(F, Px, Py) − MMD2(F, Px, Pz)

We are now ready to derive the dominant leading term,ζ1, in the variance expression (14).

Term ζ1
ζ1 : = Var(Ed1 (f (d1, d2)))

= Ex1,y1,z1[((cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)µx, φ(y1)(cid:105) − ((cid:104)φ(z1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105) − (cid:104)µx, φ(z1)(cid:105))2]

− (MMD2(F, Px, Py) − MMD2(F, Px, Pz))2

(29)
We note many terms in expansion of the squares above cancel out due to independence. For example
Ey1,z1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(z1), µz(cid:105)] − Ey1[(cid:104)φ(y1), µy(cid:105)] Ez1 [(cid:104)φ(z1), µz(cid:105)] = 0.
We can thus simplify to the following expression for ζ1
ζ1 = Ey1 [(cid:104)φ(y1), µy(cid:105)2] − Ey1[(cid:104)φ(y1), µy(cid:105)]2

(30)

(26)

(27)

(28)

+ Ex1[(cid:104)φ(x1), µy(cid:105)2] − Ex1[(cid:104)φ(x1), µy(cid:105)]2
+ Ey1 [(cid:104)µx, φ(y1)(cid:105)2] − Ey1[(cid:104)µx, φ(y1)(cid:105)]2
+ Ez1 [(cid:104)φ(z1), µz(cid:105)2] − Ez1[(cid:104)φ(z1), µz(cid:105)]2
+ Ex1[(cid:104)φ(x1), µz(cid:105)2] − Ex1[(cid:104)φ(x1), µz(cid:105)]2
+ Ez1 [(cid:104)µx, φ(z1)(cid:105)2] − Ez1 [(cid:104)µx, φ(z1)(cid:105)]2
− 2(Ey1[(cid:104)φ(y1), µy(cid:105)(cid:104)µx, φ(y1)(cid:105)] − Ey1 [(cid:104)φ(y1), µy(cid:105)] Ey1[(cid:104)µx, φ(y1)(cid:105)])
− 2(Ex1 [(cid:104)φ(x1), µy(cid:105)(cid:104)φ(x1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)])
− 2(Ez1[(cid:104)φ(z1), µz(cid:105)(cid:104)µx, φ(z1)(cid:105)] − Ez1 [(cid:104)φ(z1), µz(cid:105)] Ez1[(cid:104)µx, φ(z1)(cid:105)])

We can empirically approximate these terms as follows:

1

ζ1 ≈

n(n − 1)2 eT ˜Kyy ˜Kyye −

(cid:18)

1
n(n − 1)

(cid:19)2

eT ˜Kyye

(31)

+

+

+

+

+

(cid:19)2

(cid:19)2

(cid:18) 1
nm
(cid:18) 1
nm
(cid:18)

1
n2m

eT K T

xyKxye −

eT Kxye

1
nm2 eT KxyK T

xye −

1

r(r − 1)2 eT ˜Kzz ˜Kzze −
(cid:18) 1
1
rm2 eT KxzK T
rm
(cid:18) 1
1
r2m
rm
(cid:18)

xzKxze −

eT K T

xze −

eT Kxye

1
r(r − 1)
(cid:19)2

eT Kxze

(cid:19)2

eT Kxze

(cid:19)2

eT ˜Kzze

− 2

− 2

− 2

1
n(n − 1)m

(cid:18) 1

(cid:18)

nmr
1
r(r − 1)m

eT ˜KyyKyxe −

eT ˜Kyye ×

eT Kxye

1
n(n − 1)

eT ˜K T

xyKxze −

eT Kxye ×

eT Kxze

1
rm

eT ˜KzzK T

xze −

1
n(n − 1)

eT ˜Kyye ×

eT Kxye

1
nm
(cid:19)

1
nm

(cid:19)

(cid:19)

1
nm

14

Published as a conference paper at ICLR 2016

A.4 EQUALITY OF DERIVATIONS

In this section, we prove that Equation (9) is equal to the variance of the difference of 2
MMD2(F, Px, Py) and MMD2(F, Px, Pz).

σ2
XY + σXZ − 2σXY XZ = Ey1

(cid:2)(cid:104)φ(y1), µy(cid:105)2(cid:3) − Ey1 [(cid:104)φ(y1), µy(cid:105)]2

(32)

(cid:2)(cid:104)φ(z1), µy(cid:105)2(cid:3) − Ez1 [(cid:104)φ(z1), µy(cid:105)]2

+ Ez1
− 2 (Ey1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)] − Ey1 [(cid:104)φ(y1), µy(cid:105)] Ey1 [(cid:104)φ(y1), µx(cid:105)])
− 2 (Ez1 [(cid:104)φ(z1), µz(cid:105)(cid:104)φ(z1), µx(cid:105)] − Ez1 [(cid:104)φ(z1), µz(cid:105)] Ez1 [(cid:104)φ(z1), µx(cid:105)])
(cid:2)(cid:104)φ(x1), µy(cid:105)2(cid:3) − Ex1 [(cid:104)φ(x1), µy(cid:105)]2
+ Ex1
(cid:2)(cid:104)φ(y1), µz(cid:105)2(cid:3) − Ey1 [(cid:104)φ(y1), µz(cid:105)]2
+ Ey1
(cid:2)(cid:104)φ(y1), µx(cid:105)2(cid:3) − Ey1 [(cid:104)φ(y1), µx(cid:105)]2
+ Ey1
(cid:2)(cid:104)φ(z1), µx(cid:105)2(cid:3) − Ez1 [(cid:104)φ(z1), µx(cid:105)]2
+ Ez1
− 2 (Ex1 [(cid:104)φ(x1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)])

We have shown that Equation (9) is equal to Equation (31).

B CALIBRATION OF THE TEST

We show here that our derived test is well calibrated. A calibrated
test should output a uniform distribution of p-values when the two
MMD distances are equal. The empirical distributions of p-values
for various sets of Px, Py and Pz are given in Figure 8. Similarly,
for a given signiﬁcance level α, the false positive rate should be
equal to α. The empirical false positive rates for varying α are
shown in Figure 7 further demonstrating the proper calibration of
the test.

y
c
n
e
u
q
e
r
F

p-values

Figure 7: Calibration of the rela-

tive similarity test

15

Published as a conference paper at ICLR 2016

(a) Illustration of the synthetic data with different

(b) Uniform histogram of p-values

means for X, Y and Z.

(c) Illustration of the synthetic data with different

(d) Uniform histogram of p-values

means and orientations for X, Y and Z.

(e) Illustration of the synthetic data with different ori-

(f) Uniform histogram of p-values

entations for X, Y and Z.

Figure 8: Calibration of the relative similarity test

16

