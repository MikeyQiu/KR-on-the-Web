6
1
0
2
 
g
u
A
 
9
 
 
]
L
C
.
s
c
[
 
 
3
v
9
5
7
1
0
.
7
0
6
1
:
v
i
X
r
a

Bag of Tricks for Efﬁcient Text Classiﬁcation

Armand Joulin

Edouard Grave

Piotr Bojanowski

Tomas Mikolov

Facebook AI Research
{ajoulin,egrave,bojanowski,tmikolov}@fb.com

Abstract

This paper explores a simple and efﬁcient
Our ex-
baseline for text classiﬁcation.
periments show that our
text classi-
fast
ﬁer fastText is often on par with deep
learning classiﬁers in terms of accuracy, and
many orders of magnitude faster for training
and evaluation. We can train fastText on
more than one billion words in less than ten
minutes using a standard multicore CPU, and
classify half a million sentences among 312K
classes in less than a minute.

1 Introduction

Text classiﬁcation is an important task in Natural
Language Processing with many applications, such
as web search, information retrieval, ranking and
classiﬁcation (Deerwester et al., 1990;
document
Pang and Lee, 2008).
Recently, models based
on neural networks have become increasingly
popular
Zhang and LeCun, 2015;
Conneau et al., 2016). While these models achieve
very good performance in practice, they tend to be
relatively slow both at train and test time, limiting
their use on very large datasets.

(Kim, 2014;

linear

classiﬁers

Meanwhile,

strong baselines

of-
are
text
for
ten considered as
(Joachims, 1998;
problems
classiﬁcation
Fan et al., 2008).
McCallum and Nigam, 1998;
Despite their simplicity,
they often obtain state-
of-the-art performances if the right features are
used (Wang and Manning, 2012).
They also
have the potential
to scale to very large cor-
pus (Agarwal et al., 2014).

In this work, we explore ways to scale these
baselines to very large corpus with a large output
space, in the context of text classiﬁcation. Inspired
by the recent work in efﬁcient word representation
learning (Mikolov et al., 2013; Levy et al., 2015),
we show that linear models with a rank constraint
and a fast loss approximation can train on a billion
words within ten minutes, while achieving perfor-
mance on par with the state-of-the-art. We evalu-
ate the quality of our approach fastText1 on two
different tasks, namely tag prediction and sentiment
analysis.

2 Model architecture

A simple and efﬁcient baseline for
sentence
classiﬁcation is to represent sentences as bag of
words (BoW) and train a linear classiﬁer, e.g., a
logistic regression or an SVM (Joachims, 1998;
Fan et al., 2008). However,
linear classiﬁers do
not share parameters among features and classes.
This possibly limits their generalization in the
context of large output space where some classes
have very few examples.
Common solutions
to this problem are to factorize the linear clas-
(Schutze, 1992;
siﬁer
Mikolov et al., 2013)
use multilayer
neural
(Collobert and Weston, 2008;
networks
Zhang et al., 2015).

into low rank matrices

or

to

Figure 1 shows a simple linear model with rank
constraint. The ﬁrst weight matrix A is a look-up
table over the words. The word representations are
then averaged into a text representation, which is in
turn fed to a linear classiﬁer. The text representa-

1https://github.com/facebookresearch/fastText

output

hidden

x1

x2

. . .

xN −1

xN

Figure 1: Model architecture of fastText for a sentence with
N ngram features x1, . . . , xN . The features are embedded and
averaged to form the hidden variable.

tion is an hidden variable which can be potentially
be reused. This architecture is similar to the cbow
model of Mikolov et al. (2013), where the middle
word is replaced by a label. We use the softmax
function f to compute the probability distribution
over the predeﬁned classes. For a set of N doc-
uments, this leads to minimizing the negative log-
likelihood over the classes:

−

1
N

N

X
n=1

yn log(f (BAxn)),

where xn is the normalized bag of features of the n-
th document, yn the label, A and B the weight matri-
ces. This model is trained asynchronously on mul-
tiple CPUs using stochastic gradient descent and a
linearly decaying learning rate.

2.1 Hierarchical softmax

When the number of classes is large, computing the
linear classiﬁer is computationally expensive. More
precisely, the computational complexity is O(kh)
where k is the number of classes and h the di-
mension of the text representation. In order to im-
prove our running time, we use a hierarchical soft-
max (Goodman, 2001) based on the Huffman cod-
ing tree (Mikolov et al., 2013). During training, the
computational complexity drops to O(h log2(k)).

The hierarchical softmax is also advantageous at
test time when searching for the most likely class.
Each node is associated with a probability that is the
probability of the path from the root to that node. If
the node is at depth l + 1 with parents n1, . . . , nl, its
probability is

P (nl+1) =

P (ni).

l

Y
i=1

This means that the probability of a node is always
lower than the one of its parent. Exploring the tree
with a depth ﬁrst search and tracking the maximum
probability among the leaves allows us to discard
any branch associated with a small probability. In
practice, we observe a reduction of the complexity
to O(h log2(k)) at test time. This approach is fur-
ther extended to compute the T -top targets at the
cost of O(log(T )), using a binary heap.

2.2 N-gram features

Bag of words is invariant to word order but taking
explicitly this order into account is often computa-
tionally very expensive.
Instead, we use a bag of
n-grams as additional features to capture some par-
tial information about the local word order. This
is very efﬁcient in practice while achieving compa-
rable results to methods that explicitly use the or-
der (Wang and Manning, 2012).

We maintain a fast and memory efﬁcient
mapping of the n-grams by using the hashing
trick (Weinberger et al., 2009) with the same hash-
ing function as in Mikolov et al. (2011) and 10M
bins if we only used bigrams, and 100M otherwise.

3 Experiments

We evaluate fastText on two different
tasks.
First, we compare it to existing text classifers on the
problem of sentiment analysis. Then, we evaluate
its capacity to scale to large output space on a tag
prediction dataset. Note that our model could be im-
plemented with the Vowpal Wabbit library,2 but we
observe in practice, that our tailored implementation
is at least 2-5× faster.

3.1 Sentiment analysis

and

employ

evaluation

baselines. We

and
datasets

the
Datasets
protocol
same
8
the n-grams
of Zhang et al. (2015). We report
from Zhang et al. (2015),
and TFIDF baselines
level convolutional
as well as
model
(char-CNN) of Zhang and LeCun (2015),
the character based convolution recurrent net-
work (char-CRNN) of (Xiao and Cho, 2016) and
the very deep convolutional network (VDCNN)
We also compare
of Conneau et al. (2016).

the character

2Using the options --nn, --ngrams and --log multi

Model

AG Sogou DBP Yelp P. Yelp F. Yah. A. Amz. F. Amz. P.

BoW (Zhang et al., 2015)
ngrams (Zhang et al., 2015)
ngrams TFIDF (Zhang et al., 2015)
char-CNN (Zhang and LeCun, 2015)
char-CRNN (Xiao and Cho, 2016)
VDCNN (Conneau et al., 2016)

fastText, h = 10
fastText, h = 10, bigram

88.8
92.0
92.4
87.2
91.4
91.3

91.5
92.5

92.9
97.1
97.2
95.1
95.2
96.8

93.9
96.8

96.6
98.6
98.7
98.3
98.6
98.7

98.1
98.6

92.2
95.6
95.4
94.7
94.5
95.7

93.8
95.7

58.0
56.3
54.8
62.0
61.8
64.7

60.4
63.9

68.9
68.5
68.5
71.2
71.7
73.4

72.0
72.3

54.6
54.3
52.4
59.5
59.2
63.0

55.8
60.2

90.4
92.0
91.5
94.5
94.1
95.7

91.2
94.6

Table 1: Test accuracy [%] on sentiment datasets. FastText has been run with the same parameters for all the datasets. It has
10 hidden units and we evaluate it with and without bigrams. For char-CNN, we show the best reported numbers without data
augmentation.

Zhang and LeCun (2015)

Conneau et al. (2016)

fastText

small char-CNN big char-CNN

depth=9

depth=17

depth=29

h = 10, bigram

AG
Sogou
DBpedia
Yelp P.
Yelp F.
Yah. A.
Amz. F.
Amz. P.

1h
-
2h
-
-
8h
2d
2d

3h
-
5h
-
-
1d
5d
5d

24m
25m
27m
28m
29m
1h
2h45
2h45

37m
41m
44m
43m
45m
1h33
4h20
4h25

51m
56m
1h
1h09
1h12
2h
7h
7h

1s
7s
2s
3s
4s
5s
9s
10s

Table 2: Training time for a single epoch on sentiment analysis datasets compared to char-CNN and VDCNN.

following their evaluation
to Tang et al. (2015)
protocol. We report
their main baselines as
well as their two approaches based on recurrent
networks (Conv-GRNN and LSTM-GRNN).

Results. We present the results in Figure 1. We
use 10 hidden units and run fastText for 5
epochs with a learning rate selected on a valida-
tion set from {0.05, 0.1, 0.25, 0.5}. On this task,
adding bigram information improves the perfor-
mance by 1-4%. Overall our accuracy is slightly
better than char-CNN and char-CRNN and, a bit
worse than VDCNN. Note that we can increase
the accuracy slightly by using more n-grams, for
example with trigrams, the performance on Sogou
goes up to 97.1%. Finally, Figure 3 shows that
our method is competitive with the methods pre-
sented in Tang et al. (2015). We tune the hyper-
parameters on the validation set and observe that
using n-grams up to 5 leads to the best perfor-
mance. Unlike Tang et al. (2015), fastText does
not use pre-trained word embeddings, which can be
explained the 1% difference in accuracy.

Model

Yelp’13 Yelp’14 Yelp’15

IMDB

59.8
SVM+TF
59.7
CNN
Conv-GRNN
63.7
LSTM-GRNN 65.1

fastText

64.2

61.8
61.0
65.5
67.1

66.2

62.4
61.5
66.0
67.6

66.6

40.5
37.5
42.5
45.3

45.2

Table 3: Comparision with Tang et al. (2015). The hyper-
parameters are chosen on the validation set. We report the test

accuracy.

Training time. Both char-CNN and VDCNN are
trained on a NVIDIA Tesla K40 GPU, while our
models are trained on a CPU using 20 threads. Ta-
ble 2 shows that methods using convolutions are sev-
eral orders of magnitude slower than fastText.
While it is possible to have a 10× speed up for
char-CNN by using more recent CUDA implemen-
tations of convolutions, fastText takes less than
a minute to train on these datasets. The GRNNs
method of Tang et al. (2015) takes around 12 hours
per epoch on CPU with a single thread. Our speed-

Input

taiyoucon 2011 digitals: individuals digital pho-
tos from the anime convention taiyoucon 2011 in
mesa, arizona. if you know the model and/or the
character, please comment.

Prediction

#cosplay

Tags

2012 twin cities pride 2012 twin cities pride pa-
rade

#minneapolis

beagle enjoys the snowfall

#snow

#24mm #anime #animeconvention
#arizona #canon #con #convention
#cos #cosplay #costume #mesa #play
#taiyou #taiyoucon

#2012twincitiesprideparade
neapolis #mn #usa

#min-

#2007 #beagle #hillsboro #january
#maddison #maddy #oregon #snow

christmas

euclid avenue

#christmas

#cameraphone #mobile

#newyorkcity

#cleveland #euclidavenue

Table 4: Examples from the validation set of YFCC100M dataset obtained with fastText with 200 hidden units and bigrams.
We show a few correct and incorrect tag predictions.

up compared to neural network based methods in-
creases with the size of the dataset, going up to at
least a 15,000× speed-up.

Model

3.2 Tag prediction

Dataset and baselines. To test scalability of
our approach,
further evaluation is carried on
(Thomee et al., 2016)
the YFCC100M dataset
which consists of almost 100M images with cap-
tions, titles and tags. We focus on predicting the
tags according to the title and caption (we do not
use the images). We remove the words and tags
occurring less than 100 times and split the data
into a train, validation and test set.
The train
set contains 91,188,648 examples (1.5B tokens).
The validation has 930,497 examples and the test
set 543,424. The vocabulary size is 297,141 and
there are 312,116 unique tags. We will release a
script that recreates this dataset so that our numbers
could be reproduced. We report precision at 1.

We consider a frequency-based baseline which
tag. We also com-
predicts the most frequent
pare with Tagspace (Weston et al., 2014), which is
a tag prediction model similar to ours, but based on
the Wsabie model of Weston et al. (2011). While
the Tagspace model is described using convolutions,
we consider the linear version, which achieves com-
parable performance but is much faster.

Results and training time. Table 5 presents a
comparison of fastText and the baselines. We
run fastText for 5 epochs and compare it
to Tagspace for two sizes of the hidden layer, i.e., 50

prec@1

Running time

Freq. baseline
Tagspace, h = 50
Tagspace, h = 200

2.2
30.1
35.6

fastText, h = 50
31.2
fastText, h = 50, bigram 36.7
fastText, h = 200
41.1
fastText, h = 200, bigram 46.1

Train

-
3h8
5h32

6m40
7m47
10m34
13m38

Test

-
6h
15h

48s
50s
1m29
1m37

Table 5: Prec@1 on the test set for tag prediction on
YFCC100M. We also report the training time and test time.

Test time is reported for a single thread, while training uses 20

threads for both models.

and 200. Both models achieve a similar perfor-
mance with a small hidden layer, but adding bi-
grams gives us a signiﬁcant boost in accuracy. At
test time, Tagspace needs to compute the scores
for all the classes which makes it relatively slow,
while our fast inference gives a signiﬁcant speed-up
when the number of classes is large (more than 300K
here). Overall, we are more than an order of mag-
nitude faster to obtain model with a better quality.
The speedup of the test phase is even more signiﬁ-
cant (a 600× speedup). Table 4 shows some quali-
tative examples.

4 Discussion and conclusion

In this work, we propose a simple baseline method
for text classiﬁcation. Unlike unsupervisedly trained
word vectors from word2vec, our word features can

be averaged together to form good sentence repre-
sentations. In several tasks, fastText obtains per-
formance on par with recently proposed methods in-
spired by deep learning, while being much faster.
Although deep neural networks have in theory much
higher representational power than shallow models,
it is not clear if simple text classiﬁcation problems
such as sentiment analysis are the right ones to eval-
uate them. We will publish our code so that the
research community can easily build on top of our
work.

Acknowledgement. We thank Gabriel Synnaeve,
Herv´e G´egou, Jason Weston and L´eon Bottou for
their help and comments. We also thank Alexis Con-
neau, Duyu Tang and Zichao Zhang for providing us
with information about their methods.

References

[Agarwal et al.2014] Alekh Agarwal, Olivier Chapelle,
Miroslav Dud´ık, and John Langford. 2014. A reliable
effective terascale linear learning system. JMLR.
[Collobert and Weston2008] Ronan Collobert and Jason
Weston. 2008. A uniﬁed architecture for natural lan-
guage processing: Deep neural networks with multi-
task learning. In ICML.

[Conneau et al.2016] Alexis Conneau, Holger Schwenk,
Lo¨ıc Barrault, and Yann Lecun. 2016. Very deep con-
volutional networks for natural language processing.
arXiv preprint arXiv:1606.01781.

[Deerwester et al.1990] Scott Deerwester, Susan T Du-
mais, George W Furnas, Thomas K Landauer, and
Richard Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American society for informa-
tion science.

[Fan et al.2008] Rong-En Fan, Kai-Wei Chang, Cho-Jui
Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Li-
blinear: A library for large linear classiﬁcation. JMLR.
[Goodman2001] Joshua Goodman. 2001. Classes for fast

maximum entropy training. In ICASSP.

[Joachims1998] Thorsten Joachims. 1998. Text catego-
rization with support vector machines: Learning with
many relevant features. Springer.

[Kim2014] Yoon Kim. 2014. Convolutional neural net-

works for sentence classiﬁcation. In EMNLP.

[Levy et al.2015] Omer Levy, Yoav Goldberg, and Ido
Dagan. 2015. Improving distributional similarity with
lessons learned from word embeddings. TACL.

[McCallum and Nigam1998] Andrew McCallum and Ka-
mal Nigam. 1998. A comparison of event models for

naive bayes text classiﬁcation. In AAAI workshop on
learning for text categorization.

[Mikolov et al.2011] Tom´aˇs Mikolov, Anoop Deoras,
Daniel Povey, Luk´aˇs Burget, and Jan ˇCernock`y. 2011.
Strategies for training large scale neural network lan-
guage models.
In Workshop on Automatic Speech
Recognition and Understanding. IEEE.

[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013. Efﬁcient estimation
of word representations in vector space. arXiv preprint
arXiv:1301.3781.

[Pang and Lee2008] Bo Pang and Lillian Lee.

2008.
Opinion mining and sentiment analysis. Foundations
and trends in information retrieval.

[Schutze1992] Hinrich Schutze. 1992. Dimensions of

meaning. In Supercomputing.

[Tang et al.2015] Duyu Tang, Bing Qin, and Ting Liu.
2015. Document modeling with gated recurrent neural
network for sentiment classiﬁcation. In EMNLP.
[Thomee et al.2016] Bart Thomee, David A Shamma,
Gerald Friedland, Benjamin Elizalde, Karl Ni, Dou-
2016.
glas Poland, Damian Borth, and Li-Jia Li.
Yfcc100m: The new data in multimedia research. vol-
ume 59, pages 64–73. ACM.

[Wang and Manning2012] Sida Wang and Christopher D
Manning. 2012. Baselines and bigrams: Simple, good
sentiment and topic classiﬁcation. In ACL.

[Weinberger et al.2009] Kilian Weinberger, Anirban Das-
gupta, John Langford, Alex Smola, and Josh Atten-
berg. 2009. Feature hashing for large scale multitask
learning. In ICML.

[Weston et al.2011] Jason Weston, Samy Bengio, and
Nicolas Usunier. 2011. Wsabie: Scaling up to large
vocabulary image annotation. In IJCAI.

[Weston et al.2014] Jason Weston, Sumit Chopra, and
Keith Adams. 2014. #tagspace: Semantic embed-
dings from hashtags. In EMNLP.

[Xiao and Cho2016] Yijun Xiao and Kyunghyun Cho.
2016. Efﬁcient character-level document classiﬁcation
by combining convolution and recurrent layers. arXiv
preprint arXiv:1602.00367.

[Zhang and LeCun2015] Xiang Zhang and Yann LeCun.
2015. Text understanding from scratch. arXiv preprint
arXiv:1502.01710.

[Zhang et al.2015] Xiang Zhang, Junbo Zhao, and Yann
LeCun. 2015. Character-level convolutional networks
for text classiﬁcation. In NIPS.

