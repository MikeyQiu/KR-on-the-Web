Bayesian Nonlinear Support Vector Machines for
Big Data

Florian Wenzel1, Théo Galy-Fajou1, Matthäus Deutsch2, and Marius Kloft1

1 Humboldt University of Berlin, Germany
2 G+J Digital Products Hamburg, Germany
{wenzelfl,galy,kloft}@hu-berlin.de, mdeutsch@outlook.com

Abstract. We propose a fast inference method for Bayesian nonlinear
support vector machines that leverages stochastic variational inference and
inducing points. Our experiments show that the proposed method is faster
than competing Bayesian approaches and scales easily to millions of data
points. It provides additional features over frequentist competitors such as
accurate predictive uncertainty estimates and automatic hyperparameter
search.

Keywords: Bayesian Approximative Inference, Support Vector Ma-
chines, Kernel Methods, Big Data

1

Introduction

Statistical machine learning branches into two classic strands of research: Bayesian
and frequentist. In the classic supervised learning setting, both paradigms aim
to ﬁnd, based on training data, a function fβ that predicts well on yet unseen
test data. The diﬀerence in the Bayesian and frequentist approach lies in the
treatment of the parameter vector β of this function. In the frequentist setting,
we select the parameter β that minimizes a certain loss given the training data,
from a restricted set B of limited complexity. In the Bayesian school of thinking,
we express our prior belief about the parameter, in the form of a probability
distribution over the parameter vector. When we observe data, we adapt our
belief, resulting in a posterior distribution over β

Advantages of the Bayesian approach include automatic treatment of hyper-
parameters and direct quantiﬁcation of the uncertainty3 of the prediction in the
form of class membership probabilities which can be of tremendous importance
in practice. As examples consider the following. (1) We have collected blood
samples of cancer patients and controls. The aim is to screen individuals that
have increased likelihood of developing cancer. The knowledge of the uncertainty
in those predictions is invaluable to clinicians. (2) In the domain of physics it
is important to have a sense about the certainty level of predictions since it

3 Note that frequentist approaches can also lead to other forms of uncertainty estimates,
e.g. in form of conﬁdence intervals. But since the classic SVM does not exhibit a
probabilistic formulation these uncertainty estimates cannot be directly computed.

7
1
0
2
 
l
u
J
 
8
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
2
3
5
5
0
.
7
0
7
1
:
v
i
X
r
a

2

Florian Wenzel et al.

is mandatory to assert the statistical conﬁdence in any physical variable mea-
surement. (3) In the general context of decision making, it is crucial that the
uncertainty of the estimated outcome of an action can be reliably determined.

Recently, it was shown that the support vector machine (SVM) [1]—which
is a classic supervised classiﬁcation algorithm— admits a Bayesian interpreta-
tion through the technique of data augmentation [2,3]. This so-called Bayesian
nonlinear SVM combines the best of both worlds: it inherits the geometric in-
terpretation, its robustness against outliers, state-of-the-art accuracy [4], and
theoretical error guarantees [5] from the frequentist formulation of the SVM,
but like Bayesian methods it also allows for ﬂexible feature modeling, automatic
hyperparameter tuning, and predictive uncertainty quantiﬁcation.

However, existing inference methods for the Bayesian support vector machine
(such as the expectation conditional maximization method introduced in [3])
scale rather poorly with the number of samples and are limited in application
to datasets with thousands of data points [3]. Based on stochastic variational
inference [6] and inducing points [7], we develop in this paper a fast and scalable
inference method for the nonlinear Bayesian SVM.

Our experiments show superior performance of our method over competing
methods for uncertainty quantiﬁcation of SVMs such as Platt’s method [8].
Furthermore, we show that our approach is faster (by one to three orders of
magnitude) than the following competitors: expectation conditional maximization
(ECM) for nonlinear Bayesian SVM by [3], Gaussian process classiﬁcation [9],
and the recently proposed scalable variational Gaussian process classiﬁcation
method [10]. We apply our method to the domain of particle physics, namely on
the SUSY dataset [11] (a standard benchmark in particle physics containing 5
million data points) where our method takes only 10 minutes to train on a single
CPU machine.

Our experiments demonstrate that Bayesian inference techniques are mature
enough to compete with corresponding frequentist approaches (such as nonlinear
SVMs) in terms of scalability to big data, yet they oﬀer additional beneﬁts such
as uncertainty estimation and automated hyperparameter search.

Our paper is structured as follows. In section 2 we discuss related work and
review the Bayesian nonlinear SVM model in section 3. In section 4 we propose
our novel scalable inference algorithm, show how to optimize hyperparameters
and obtain an approximate predictive distribution. We discuss also the special
case of the linear SVM, for which we propose a specially tailored fast inference
algorithm. Section 5 concludes with experimental results.

2 Related Work

There has recently been signiﬁcant interest in utilizing max-margin based dis-
criminative Bayesian models for various applications. For example, [12] employs
a max-margin based Bayesian classiﬁcation to discover latent semantic structures
for topic models, [13] uses a max-margin approach for eﬃcient Bayesian matrix

Bayesian Nonlinear SVMs for Big Data

3

factorization, and [14] develops a new max-margin approach to Hidden Markov
models.

All these approaches apply the Bayesian reformulation of the classic SVM
introduced by [2]. This model is extended by [3] to the nonlinear case. The authors
show improved accuracy compared to standard methods such as (non-Bayesian)
SVMs and Gaussian process (GP) classiﬁcation.

However, the inference methods proposed in [2] and [3] have the drawback
that they partially rely on point estimates of the latent variables and do not scale
well to large datasets. In [15] the authors apply mean ﬁeld variational inference
to the linear case of the model, but their proposed technique does not lead to
substantial performance improvements and neglects the nonlinear model.

Uncertainty estimation for SVMs is usually done via Platt’s technique [8],
which consists of applying a logistic regression on the function scores produced by
the SVM. In contrast, our technique directly yields a sound predictive distribution
instead of using a heuristically motivated transformation. We make use of the idea
of inducing point GPs to develop a scalable inference method for the Bayesian
nonlinear SVM. Sparse GPs using pseudo-inputs were ﬁrst introduced in [16].
Building on this idea Hensman et al. developed a stochastic variational inference
scheme for GP regression and GP classiﬁcation [7,10]. We further extend this
ideas to the setting of Bayesian nonlinear SVM.

3 The Bayesian SVM Model

Let D = {xi, yi}n
i=1 be n observations where xi ∈ Rd is a feature vector with
corresponding labels yi ∈ {−1, 1}. The SVM aims to ﬁnd an optimal score
function f by solving the following regularized risk minimization objective:

arg min

γR (f ) +

max (0, 1 − yif (xi)) ,

(1)

f

n
(cid:88)

i=1

where R is a regularizer function controlling the complexity of the decision
function f , and γ is a hyperparameter to adjust the trade-oﬀ between training
error and the complexity of f . The loss max (0, 1 − yf (x)) is called hinge loss.
The classiﬁer is then deﬁned as sign(f (x)).

For the case of a linear decision function, i.e. f (x) = xT β, the SVM optimiza-

tion problem (1) is equivalent to estimating the mode of a pseudo-posterior

p(β|D) ∝

L(yi|xi, β)p(β).

n
(cid:89)

i=1

Here p(β) denotes a prior such that log p(β) ∝ −2γR(β). In the following we use
the prior β ∼ N (0, Σ), where Σ ∈ Rd×d is a positive deﬁnite matrix. From a
frequentist SVM view, this choice generalizes the usual L2-regularization to non-
isotropic regularizers. Note that our proposed framework can be easily extended to

4

Florian Wenzel et al.

other regularization techniques by adjusting the prior on β (e.g. block (cid:96)(2,p)-norm
regularization which is known as multiple kernel learning [17]). In order to obtain
a Bayesian interpretation of the SVM, we need to deﬁne a pseudolikelihood L
such that the following holds,

L (y|x, f (·)) ∝ exp (−2 max(1 − yif (xi), 0)) .

(2)

By introducing latent variables λ := (λ1, . . . , λn)(cid:62) (data augmentation) and
making use of integral identities stemming from function theory, [2] show that
the speciﬁcation of L in terms of the following marginal distribution satisﬁes (2):

L(yi|xi, β) =

(cid:90) ∞

0

√

1
2πλi

(cid:32)

exp

−

1
2

(cid:0)1 + λi − yixT

i β(cid:1)2

(cid:33)

λi

dλi.

(3)

Writing X ∈ Rd×n for the matrix of data points and Y = diag(y), the full
conditional distributions of this model are

β|λ, Σ, D ∼ N (cid:0)B(λ−1 + 1), B(cid:1) ,
λi|β, Di ∼ GIG (cid:0)1/2, 1, (1 − yix(cid:62)

i β)2(cid:1) ,

(4)

with Z = Y X, B−1 = ZΛ−1Z (cid:62) + Σ−1, Λ = diag(λ) and where GIG denotes a
generalized inverse Gaussian distribution. The n latent variables λi of the model
scale the variance of the full posteriors locally. The model thus constitutes a
special case of a normal variance-mean mixture, where we implicitly impose
the improper prior p(λ) = 1[0,∞)(λ) on λ. This could be generalized by using a
generalized inverse Gaussian prior on λi, leading to a conjugate model for λi.
Henao et al. show that in the case of an exponential prior on λi, this leads to
a skewed Laplace full conditional for λi. Note that this, however, destroys the
equivalency to the frequentist linear SVM.

By using the ideas of Gaussian processes [9], Henao et al. develop a nonlinear
(kernelized) version of this model [3]. They assume a continuous decision function
f (x) to be drawn from a zero-mean Gaussian process GP(0, k), where k is a
kernel function. The random Gaussian vector f = (f1, ..., fn)(cid:62) corresponds to
f (x) evaluated at the data points. They substitute the linear function x(cid:62)
i β by fi
in (3) and obtain the conditional posteriors

f |λ, D ∼ N (cid:0)CY (λ−1 + 1), C(cid:1) ,
λi|fi, Di ∼ GIG (cid:0)1/2, 1, (1 − yifi)2(cid:1) ,
with C −1 = Λ−1 +K −1. For a test point x∗ the conditional predictive distribution
for f∗ = f (x∗) under this model is
f∗|λ, x∗, D ∼ N (cid:0)k(cid:62)

∗ (K + Λ)−1Y (1 + λ), k∗∗ − k(cid:62)

∗ (K + Λ)−1k∗

(5)

(cid:1) ,

where K := k(X, X), kX∗ := k(X, x∗), k∗∗ := k(x∗, x∗). The conditional class
membership probability is

p(y∗ = 1|λ, x∗, D) = Φ

(cid:18) kT

∗ (K + Λ)−1Y (1 + λ)

1 + k∗∗ − k(cid:62)

∗ (K + Λ)−1k∗

(cid:19)

,

Bayesian Nonlinear SVMs for Big Data

5

where Φ(.) is the probit link function.

Note that the conditional posteriors as well as the class membership prob-
ability still depend on the local latent variables λi. We are interested in the
marginal predictive distributions, but unfortunately the latent variables cannot
be integrated out analytically. Both [2] and [3] propose MCMC-algorithms and
stepwise inference schemes similar to EM-algorithms to overcome this problem.
These methods do not scale well to big data problems and the probability es-
timation still relies on point estimates of the n-dimensional λ. We overcome
these problems proposing a scalable inference method and obtaining approximate
marginal predictive distributions (that are not conditioned on λ).

4 Scalable Inference and Automated Hyperparameter

Tuning

In the following we develop a fast and reliable inference method for the Bayesian
nonlinear SVM. Our method builds on the idea of using inducing points for
Gaussian Processes in a stochastic variational inference setting [7] that scales
easily to millions of data points. We proceed by ﬁrst discussing a standard batch
variational scheme in section 4.1 and then in section 4.2 we develop our fast and
scalable inference method. We show how to automatically tune hyperparameters
in section 4.3 and obtain uncertainty estimates for predictions in section 4.4.
Finally, we discuss the special case of the Bayesian linear SVM in section 4.5.

4.1 Batch Variational Inference

The idea of variational inference is to approximate the typically intractable pos-
terior of a probabilistic model by a variational (typically factorized) distribution.
We ﬁnd the optimal approximating distribution by maximizing a lower bound
on the evidence (the so-called ELBO) with respect to the parameters of the
variational distribution, which is equivalent to minimizing the Kullback-Leibler
divergence between the variational distribution and the posterior [18,19].

In this section we ﬁrst develop a batch variational inference scheme [18,19],
which uses the full dataset in every iteration. We follow the structured mean
ﬁeld approach and choose the variational distributions within the same fami-
lies as the full conditional distributions q(f, λ) = q(f ) (cid:81)n
i=1 q(λi), with q(f ) ≡
N (µ, ζ) and q(λi) ≡ GIG(1/2, 1, αi). The coordinate ascent updates can be com-
puted by the expected natural parameters of the corresponding full conditionals
(5) leading to

αi = Eq(f )[(1 − yifi)2] = (1 − y(cid:62)
i µ)2 + y(cid:62)
(cid:16)
ζ = Eq(λ)[(cid:0)Λ−1 + K −1(cid:1)−1
A− 1
µ = ζEq(λ)[Y (λ−1 + 1)] = ζY (α− 1

2 + 1).

i ζyi,
2 + K −1(cid:17)−1

] =

,

This concludes the batch variational inference scheme.

6

Florian Wenzel et al.

The downside of this approach is that it does not scale to big datasets. The
covariance matrix of the variational distribution q(f ) has dimension n × n and
has to be updated and inverted at every inference step. This operation exhibits
the computational complexity O(n3), where n is the number of data points.
Furthermore, in this setup we cannot apply stochastic gradient descent. We show
how to overcome both problems in the next section paving the way to perform
inference on big datasets.

4.2 Stochastic Variational Inference Using Inducing Points

We aim to develop a stochastic variational inference (SVI) scheme using only
minibatches of the data in each iteration. The Bayesian nonlinear SVM model
does not exhibit a set of global variables. Both the number of latent variables λ
and the observations of the latent GP f grow with number of data points (c.f.
eq.5), i.e. they are local variables. This hinders us from directly developing a SVI
scheme. We make use of the concept of inducing points [7] imposing a sparse GP
acting as global variable. This allows us to apply SVI and reduces the complexity
to O(m3), where m is the number of inducing points, which is independent of
the number of data points.

We augment our original model (5) with m < n inducing points. Let u ∈ Rm
be pseudo observations at inducing locations {ˆx1, ..., ˆxm}. We employ a prior on
the inducing points, p(u) = N (0, Kmm) and connect f and u setting

p(f |u) = N (KnmK −1

mmu, (cid:101)K)

(6)

where Kmm is the kernel matrix resulting from evaluating the kernel function
between all inducing points locations, Knm is the cross-covariance between the
data points and the inducing points and (cid:101)K is given by (cid:101)K = Knn −KnmK −1
mmKmn.
The augmented model exhibits the joint distribution

p(y, u, f, λ) = p(y, λ|f )p(f |u)p(u).

Note that we can recover the original joint distribution by marginalizing over u.
We now aim to apply the methodology of variational inference to the marginal
joint distribution p(y, u, λ) = (cid:82) p(y, u, f, λ)df . We impose a variational distribu-
tion q(u) = N (u|µ, ζ) on the inducing points u. We follow [7] and apply Jensen’s
inequality to obtain a lower bond on the following intractable conditional proba-
bility,

log p(y, λ|u) = log Ep(f |u) [p(y, λ|f )]
≥ Ep(f |u) [log p(y, λ|f )]

Ep(fi|u) [log p(yi, λi|fi)]

=

=

n
(cid:88)

i=1
n
(cid:88)

i=1

Ep(fi|u)

(cid:20)

(cid:18)

log

(2πλi)− 1

2 exp

(cid:18)

−

1
2

(1 + λi − yifi)2
λi

(cid:19)(cid:19)(cid:21)

Bayesian Nonlinear SVMs for Big Data

7

Ep(fi|u)

(cid:20)
log λi +

(1 + λi − yifi)2
λi

(cid:21)

log λi +

Ep(fi|u)

(cid:19)
(cid:2)(1 + λi − yifi)2(cid:3)

c
= −

= −

1
2

1
2

1
2

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:18)

(cid:18)

=: L1.

1
λi

1
λi

= −

log λi +

(cid:16)

(cid:101)Kii + (cid:0)1 + λi − yiKimK −1

mmu(cid:1)2(cid:17)(cid:19)

Plugging the lower bound L1 into the standard evidence lower bound (ELBO)
[18] leads to the new variational objective

log p(y) ≥ Eq [log p(y, λ, u)] − Eq [log q(λ, u)]

= Eq [log p(y, λ|u)] + Eq [log p(u)] − Eq [log q(λ, u)]
≥ Eq [L1] + Eq [log p(u)] − Eq [log q(λ, u)]

n
(cid:88)

(cid:20)
log λi +

Eq

= −

1
2

i=1

(cid:16)

1
λi

(cid:101)Kii + (cid:0)1 + λi − yiKimK −1

mmu(cid:1)2(cid:17)(cid:21)

− KL (q(u)||p(u)) − Eq(λ) [log q(λ)]

The expectations can be computed analytically (details are given in the appendix)
and we obtain L in closed form,

=: L.

1
2

+

−

i=1
n
(cid:88)

i=1

(cid:16)

α− 1

2

i

1
2

c
=

L

log |ζ| −

tr(K −1

mmζ) −

µ(cid:62)K −1

1
2

n
(cid:88)

(cid:26)

√

log(B 1
4

(

αi)) +

log(αi)

1
2

1
2

mmµ + y(cid:62)κµ
(cid:27)

1 − αi − 2yiκi.µ +

κ(µµ(cid:62) + ζ)κ(cid:62) + (cid:101)K

(cid:16)

(cid:17)

(cid:17)

,

ii

(7)

(8)

2

mm and B 1

where κ = KnmK −1
(.) is the modiﬁed Bessel function with parameter
1
2 [20]. This objective is amenable to stochastic optimization where we subsample
from the sum to obtain a noisy gradient estimate. We develop a stochastic
variational inference scheme by following noisy natural gradients of the variational
objective L. Using the natural gradient over the standard euclidean gradient is
often favorable since natural gradients are invariant to reparameterization of
the variational family [21,22] and provide eﬀective second-order optimization
updates [23,6]. The natural gradients of L w.r.t. the Gaussian natural parameters
η1 = ζ −1µ, η2 = − 1

2 ζ −1 are

(cid:101)∇η1L = κ(cid:62)Y (α− 1

2 + 1) − η1
mm + κ(cid:62)A− 1

(K −1

1
2

(cid:101)∇η2L = −

2 κ) − η2,

(9)

(10)

8

Florian Wenzel et al.

with A = diag(α). Details can be found in the appendix. The natural gradient
updates always lead to a positive deﬁnite covariance matrix4 and in our imple-
mentation ζ has not to be parametrized in any way to ensure positive-deﬁniteness.
The derivative of L w.r.t. αi is

(1 − yiκiµ)2 + yi(κiζκ(cid:62)

i + (cid:101)Kii)yi

∇αL =

√
4

3

αi

−

1
√
αi

4

.

(11)

Setting it to zero gives the coordinate ascent update for αi,

αi = (1 − yiκiµ)2 + yi(κiζκ(cid:62)

i + (cid:101)Kii)yi.

Details can be found in the appendix. The inducing point locations can be
either treated as hyperparameters and optimized while training [24] or can be
ﬁxed before optimizing the variational objective. We follow the ﬁrst approach
which is often preferred in a stochastic variational inference setup [7,10]. The
inducing point locations can be either randomly chosen as subset of the training
set or via a density estimator. In our experiments we have observed that the
k-means clustering algorithm (kMeans) [25] yields the best results. Combining
our results, we obtain a fast stochastic variational inference algorithm for the
Bayesian nonlinear SVM which is outlined in alg. 1. We apply the adaptive
learning rate method described in [26].

mm and (cid:101)K = Knn − KnmK −1

mmKmn

Algorithm 1 Inducing Point SVI
1: set the learning rate schedule ρt appropriately
2: initialize η1, η2
3: select m inducing points locations (e.g. via kMeans)
4: compute kernel matrices K −1
5: while not converged do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: return α1, . . . , αn, µ, ζ

get S = minibatch index set of size s
update αi = (1 − yiκiµ)2 + yi(κiζκ(cid:62)
compute AS = diag(αi,
compute ˆη1 = κ(cid:62)Y (α− 1
2 (K −1
compute ˆη2 = − 1
update η1 = (1 − ρt)η1 + ρt ˆη1
update η2 = (1 − ρt)η2 + ρt ˆη2
compute ζ = − 1
compute µ = ζη1

2 + 1)
mm + κ(cid:62)A− 1

i + (cid:101)Kii)yi

2 η−1

i ∈ S)

2 κ)

2

4.3 Auto Tuning of Hyperparameters

The probabilistic formulation of the SVM lets us directly learn the hyperparame-
ters while training. To this end we maximize the marginal likelihood p(y|X, h),

4 This follows directly since Kmm and A− 1

2 are positive deﬁnite.

Bayesian Nonlinear SVMs for Big Data

9

where h denotes the set of hyperparameters (this approach is called empirical
Bayes [27]). We follow an approximate approach and optimize the ﬁtted varia-
tional lower bound L(h) over h by alternating between optimization steps w.r.t.
the variational parameters and the hyperparameters [28]. We include a gradient
ascent step w.r.t. h after multiple variational updates in the SVI scheme, this is
commonly known as Type II maximum likelihood (ML-II) [9]

h(t) = h(t−1) + (cid:101)ρt∇hL(α(t−1), µ(t−1), ζ (t−1), h).

(12)

Since the standard SVM does not exhibit a probabilistic formulation, the hy-
perparameters have to be tuned via computationally very expensive methods
as grid search and cross validation. Our approach allows us to estimate the
hyperparameters during training time and lets us follow gradients instead of only
evaluating single hyperparameters.

In the appendix we provide the gradient of the variational objective L w.r.t.
to a general kernel and show how to optimize arbitrary diﬀerentiable hyper-
parameters. Our experiments exemplify our automated hyperparameter tuning
approach by optimizing the hyper parameter of an RBF kernel.

4.4 Uncertainty Predictions

Besides the advantage of automated hyperparameter tuning, the probabilistic
formulation of the SVM leads directly to uncertainty estimates of the predictions.
The standard SVM lacks this capability, and only heuristic approaches as e.g.
Platt [8] exist. Using the approximate posterior q(u|D) = N (u|µ, ζ) obtained
by our stochastic variational inference method (alg. 1) we compute the class
membership probability for a test point x∗,

p(f ∗|x∗, D) =

p(y∗|u, x∗)p(u|D)du

(cid:90)

(cid:90)

p(y∗|u, x∗)q(u|D)du

≈
= N (cid:0)y∗|K∗mK −1
=: q(f ∗|x∗, D),

mmm, K∗∗ − K∗mK −1

mm(Km∗ + ζK −1

mmKm∗)(cid:1)

where K∗m denotes the kernel matrix between test and inducing points and
K∗∗ the kernel matrix between test points. This leads to the approximate class
membership distribution

q(y∗|x∗, D) = Φ

(cid:18)

K∗∗ − K∗mK −1

mmm

K∗mK −1
mm(Km∗ + ζK −1

mmKm∗) + 1

(cid:19)

(13)

where Φ(.) is the probit link function. Note that we already computed inverse
K −1
mm for the training procedure leading to a computational overhead stemming
only from simple matrix multiplication. Our experiments show that (13) leads to
reasonable uncertainty estimates.

10

Florian Wenzel et al.

4.5 Special Case of Linear Bayesian SVM

We now consider the special case of using a linear kernel. If we are interested in
this case we may consider the Bayesian model for the linear SVM proposed by
Polson et al. (c.f. eq. 4). This can be favorable over using the nonlinear version
since this model is formulated in primal space and, therefore, the computational
complexity depends on the dimension d and not on the number of data points
n. Furthermore, focusing directly on the linear model allows us to optimize the
true ELBO, Eq [log p(y, λ, β)] − Eq [log q(λ, β)], without the need of relying on a
lower bound (as in eq. 7). This typically leads to a better approximate posterior.
We again follow the structured mean ﬁeld approach and chose our variational

distributions to be in the same families as the full conditionals (4),

q(λi) ≡ GIG(

, 1, αi) and q(β) ≡ N (µ, ζ).

1
2

We use again the fact that the coordinate updates of the variational param-
eters can be obtained by computing the expected natural parameters of the
corresponding full conditionals (4) and obtain

i ζzi

i µ)2 + zT
2 Z T + Σ−1)−1

αi = (1 − zT
ζ = (ZA− 1
µ = ζZ(α− 1

2 + 1),

(14)

where α = (αi)1≤i≤n, A = diag(α) and Z = Y X. Since the Bayesian Linear
SVM model exhibits global and local variables we can directly employ stochastic
variational inference by subsampling the data and only updating minibatches of
α. Note that for the linear case the covariance matrices have size d × d, i.e. being
independent of the number of data points. Therefore, the SVI algorithm (14) for
the Bayesian Linear SVM exhibits the computational complexity O(d3). Luts et.
al develop a batch variational inference scheme for the Bayesian linear SVM but
do not scale to big datasets.

The hyperparameter can be tuned analogously to (12). The class membership

probabilities are

p(y∗ = 1|x∗, D) ≈

Φ(f∗)p(f∗|f, x∗)q(f |D)df df∗ = Φ

(cid:90)

(cid:18) x(cid:62)
∗ µ
x(cid:62)
∗ ζx∗ + 1

(cid:19)

,

where x∗ are the test points and q(f |D) = N (f |µ, ζ) the approximate posterior
obtained by the above described SVI scheme.

5 Experiments

We compare our approach against the expectation conditional maximization
(ECM) method proposed by Henao et. al [3], Gaussian process classiﬁcation
(GPC) [9], its recently proposed scalable stochastic variational inference version

Bayesian Nonlinear SVMs for Big Data

11

(S-GPC) [10], and libSVM with Platt scaling [29,8] (SVM + Platt). For all
experiments we use an RBF kernel5 with length-scale parameter θ. We perform
all experiments using only one CPU core with 2.9 GHz and 386 GB RAM.
Code is available at github.com/theogf/BayesianSVM.

5.1 Prediction Performance and Uncertainty Estimation

We experiment on seven real-world datasets and compare the prediction perfor-
mance, the quality of the uncertainty estimates and run time of the methods. The
results are presented in table 1. We show that our method (S-BSVM) is up to 22
times faster than the direct competitor ECM and up to 700 times faster than
Gaussian process classiﬁcation6 while outperforming the competitors in terms
of prediction performance and quality of uncertainty estimates in most cases.
The non-probabilistic SVM is naturally the fastest method. Combined with the
heuristic Platt scaling approach it leads to class membership probabilities but,
however, still lacks the advantages of a probabilistic model (as e.g. uncertainty
quantiﬁcation of the learned parameters and automatic hyperparameter tuning).
To evaluate the quality of the uncertainty estimates we compute the Brier score
which is considered as a good performance measure for probabilistic predictions
i=1 (yi − q(xi))2, where yi ∈ {0, 1} is the observed
[30] being deﬁned as BS = 1
n
output and q(xi) ∈ [0, 1] is the predicted class membership probability. Note that
smaller Brier score indicates better performance.

(cid:80)N

The datasets are all from the Rätsch benchmark datasets [31] commonly
used to test the accuracy of binary nonlinear classiﬁers. We perform a 10-fold
cross-validation and use an RBF kernel with ﬁxed parameters for all methods.
For S-BSVM we choose the number of inducing points as 20% of the training set
size, except for the datasets Splice, German and Waveform where we use 100
inducing points. For each dataset minibatches of 10 samples are used.

5.2 Big Data Experiments

We demonstrate the scalability of our method on the SUSY dataset [11] containing
5 million points with 17 features. This dataset size is very common in particle
physics due to the simplicity of artiﬁcially generating new events as well as
the quantity of data coming from particle detectors. Since it is important to
have a sense of the conﬁdence of the predictions for such datasets the Bayesian
SVM is an appropriate choice. We use an RBF kernel7, 64 inducing points and
minibatches of 100 points. The training of our model takes only 10 minutes
without any parallelization. We use the the area under the receiver operating

5 The RBF kernel is deﬁned as k(x1, x2, θ) = exp

, where θ is the length

(cid:16)

− ||x1−x2||
θ2

(cid:17)

6 For a comparison with the stochastic variational inference version of GPC, see

scale parameter.

section 5.3.

7 The length scale parameter tuning is not included in the training time. We found

θ = 5.0 by our proposed automatic tuning approach.

12

Florian Wenzel et al.

Dataset

n

dim.

Breast
Cancer

263 9

Diabetes 768 8

Flare

144 9

German

1000 20

Heart

270 13

Splice

2991 60

Waveform 5000 21

SVM + Platt

67

33

6.7

1.4

0.26

0.11

0.04

GPC

3.9
.36 ± .12 .36 ± .12 .36 ± .11 .36 ± .12

S-BSVM ECM
.26 ± .07 .27 ± .10 .27 ± .07 .27 ± .09
Error
Brier Score .18 ± .03 .19 ± .05 .18 ± .03 .19 ± .04
0.32
Time [s]
Error
.22 ± .06 .25 ± .07 .23 ± .07 .24 ± .07
Brier Score .16 ± .04 .17 ± .04 .15 ± .04 .16 ± .04
Time [s]
Error
Brier Score .22 ± .05 .25 ± .07 .24 ± .03 .24 ± .04
Time [s]
0.08
.24 ± .11 .25 ± .12 .25 ± .13 .27 ± .10
Error
Brier Score .17 ± .06 .17 ± .05 .17 ± .06 .18 ± .05
Time [s]
Error
Brier Score .13 ± .04 .14 ± .04 .12 ± .03 .12 ± .04
Time [s]
Error
Brier Score .17 ± .01 .18 ± .01 .40 ± .14 .11 ± .01
Time [s]
Error
Brier Score .06 ± .01 .15 ± .01 .06 ± .01 .06 ± .01
Time [s]

0.34
.13 ± .03 .11 ± .03 .32 ± .14 .14 ± .01

12
.16 ± .06 .19 ± .09 .16 ± .06 .17 ± .07

18
.09 ± .02 .10 ± .02 .10 ± .02 .10 ± .02

0.15

0.01

0.04

8691

12.5

406

419

115

264

1.3

2.3

2.2

1.8

80

6

Table 1. Average prediction error and Brier score with one standard deviation.

characteristic (ROC) curve (AUC) as performance measure since it is a standard
evaluation measure on this dataset [11].

Our method achieves an AUC of 0.84 and a Brier score of 0.22, whereby the
state-of-the-art obtains an AUC of 0.88 using a deep neural network (5 layers,
300 hidden units each) [11]. Note that this approach takes much longer to train
and does not include uncertainty estimates.

5.3 Run Time

We examine the run time of our methods and the competitors. We include both
the batch variational inference method (B-BSVM) described in section 4.1 and
our fast and scalable inference method (S-BSVM) described in section 4.2 in the
experiments. For each method we iteratively evaluate the prediction performance
on a held-out dataset given a certain training time budget. The prediction error as
function of the training time is shown in ﬁg. 1. We experiment on the Waveform
dataset from the Rätsch benchmark dataset (N = 5000, d = 21). We use an
RBF kernel with ﬁxed length-scale parameter θ = 5.0 and for the stochastic
variational inference methods, S-BSVM and S-GPC, we use a batch size of 10
and 100 inducing points.

Our scalable method (S-BSVM) is around 10 times faster than the direct
competitor ECM while having slightly better prediction performance. The batch

Bayesian Nonlinear SVMs for Big Data

13

variational inference version (B-BSVM) is the slowest of the Bayesian SVM infer-
ence methods. The related probabilistic model, Gaussian process classiﬁcation, is
around 5000 times slower than S-BSVM. Its stochastic inducing point version
(S-GPC) has comparable run time to S-BSVM but is very unstable leading to bad
prediction performance. S-GPC showed these instabilities for multiple settings
of the hyperparameters. The classic SVM (libSVM) has a similar run time as
our method. The speed and prediction performance of S-BSVM depend on the
number of inducing points. See section 5.5 for an empirical study. Note that the
run time in table 1 is determined after the methods have converged.

Fig. 1. Prediction error on held-out dataset vs. training time.

5.4 Auto Tuning of Hyperparameters

In section 4.3 we show that our inference method possesses the ability of automatic
hyperparameter tunning. In this experiment we demonstrate that our method,
indeed, ﬁnds the optimal length-scale hyperparameter of the RBF kernel. We
use the optimizing scheme (12) and alternate between 10 variational parameter
updates and one hyperparameter update. We compute the true validation loss of
the length-scale parameter θ by a grid search approach which consists of training
our model (S-BSVM) for each θ and measuring the prediction performance using
10-fold cross validation. In ﬁg. 2 we plot the validation loss and the length-scale
parameter found by our method. We ﬁnd the true optimum by only using 5
hyperparameter optimization steps. Training and hyperparameter optimization
takes only 0.3 seconds for our method, whereas grid search takes 188 seconds
(with a grid size of 1000 points).

14

Florian Wenzel et al.

Fig. 2. Average validation loss as function of the RBF kernel length-scale parameter
θ, computed by grid search and 10-fold cross validation. The red circle represents the
hyperparameter found by our proposed automatic tuning approach.

5.5

Inducing Points Selection

The sparse GP model used in our inference scheme builds on a set of inducing
points where both the number and the locations of the inducing points are free
parameters. We investigate three diﬀerent inducing point selection methods:
random subset selection from the training set, the Gaussian Mixture Model
(GMM), and the k-means clustering algorithm with an improved k-means++
seeding (kMeans) [32]. Furthermore we show how the number of inducing points
aﬀects the prediction accuracy and the run time. We test the three inducing
point selection methods on the USPS dataset [33] which we reduced to a binary
problem using only the digits 3 and 5 (N=1350 and d=256). For all methods we
progressively increase the number of inducing points and compute the prediction
error by 10-fold cross validation. We present our results in ﬁg. 3.

The GMM is unable to ﬁt large numbers of samples and dimensions and fails
to converge for almost all datasets tried, therefore, we do not include it in the
plot. Using the k-means selection algorithm leads for small numbers of inducing
points to much better prediction performance than random subset selection.
Furthermore, we show that using only a small fraction of inducing points (around
1% of the original dataset) leads to a nearly optimal prediction performance by
simultaneously signiﬁcantly decreasing the run time. We observe similar results
on all datasets we considered.

6 Conclusion

We presented a fast, scalable and reliable approximate inference method for the
Bayesian nonlinear SVM. While previous methods were restricted to rather small
datasets our method enables the application of the Bayesian nonlinear SVM to
large real world datasets containing millions of samples. Our experiments showed

Bayesian Nonlinear SVMs for Big Data

15

Fig. 3. Average prediction error and training time as functions of the number of inducing
points selected by two diﬀerent methods with one standard deviation (using 10-fold
cross validation).

that our method is orders of magnitudes faster than the state-of-the-art while
still yielding comparable prediction accuracies. We showed how to automatically
tune the hyperparameters and obtain prediction uncertainties which is important
in many real world scenarios.

In future work we plan to further extend the Bayesian nonlinear SVM model to
deal with missing data and account for correlations between data points building
on ideas from [34]. Furthermore, we want to develop Bayesian formulations of
important variants of the SVM as for instance one-class SVMs [35].

Acknowledgments. We thank Stephan Mandt, Manfred Opper and Patrick
Jähnichen for fruitful discussions. This work was partly funded by the German
Research Foundation (DFG) award KL 2698/2-1.

References

Anal. (2011)

1. Cortes, C., Vapnik, V.: Support-Vector Networks. Machine Learning (1995)
2. Polson, N.G., Scott, S.L.: Data augmentation for support vector machines. Bayesian

3. Henao, R., Yuan, X., Carin, L.: Bayesian Nonlinear Support Vector Machines and

Discriminative Factor Modeling. NIPS (2014)

4. Fernández-Delgado, M., Cernadas, E., Barro, S., Amorim, D.: Do we need hundreds

of classiﬁers to solve real world classiﬁcation problems? JMLR (2014)

5. Mohri, M., Rostamizadeh, A., Talwalkar, A.: Foundations of machine learning.

6. Hoﬀman, M.D., Blei, D.M., Wang, C., Paisley, J.: Stochastic Variational Inference.

MIT press (2012)

JMLR (2013)

7. Hensman, J., Fusi, N., Lawrence, N.D.: Gaussian processes for big data.

In:

Conference on Uncertainty in Artiﬁcial Intellegence. (2013)

16

Florian Wenzel et al.

8. Platt, P.J.C.: Probabilistic Outputs for Support Vector Machines and Comparisons
to Regularized Likelihood Methods. Advances in Large Margin Classiﬁer (1999)
9. Rasmussen, C.E., Williams, C.K.I.: Gaussian Processes for Machine Learning

(Adaptive Computation and Machine Learning). The MIT Press (2005)

10. Hensman, J., Matthews, A.: Scalable Variational Gaussian Process Classiﬁcation.

AISTATS (2015)

11. Baldi, P., Sadowski, P., Whiteson, D.: Searching for exotic particles in high-energy

physics with deep learning. Nature communications (2014) 4308

12. Zhu, J., Chen, N., Perkins, H., Zhang, B.: Gibbs Max-margin Topic Models with

13. Xu, M., Zhu, J., Zhang, B.: Fast Max-Margin Matrix Factorization with Data

Data Augmentation. JMLR (2014)

Augmentation. ICML (2013) 978–986

14. Zhang, Jun, Zhang: Max-Margin Inﬁnite Hidden Markov Models. ICML (2014)
15. Luts, J., Ormerod, J.T.: Mean ﬁeld variational bayesian inference for support vector

machine classiﬁcation. Comput. Stat. Data Anal. (May 2014) 163–176
16. Snelson, E., Ghahramani, Z.: Sparse GPs using Pseudo-inputs. NIPS (2006)
17. Kloft, M., Brefeld, U., Sonnenburg, S., Zien, A.: lp-norm multiple kernel learning.

JMLR (Mar) (2011) 953–997

18. Jordan, M.I., Ghahramani, Z., Jaakkola, T.S., Saul, L.K.: An Introduction to

Variational Methods for Graphical Models. Mach. Learn. (1999)

19. Wainwright, M.J., Jordan, M.I.: Graphical models, exponential families, and
variational inference. Found. Trends Mach. Learn. (1-2) (January 2008) 1–305
20. Jørgensen, B.: Statistical properties of the generalized inverse Gaussian distribution.

Springer Science & Business Media (2012)

21. Amari, S., Nagaoka, H.: Methods of Information Geometry. Am. Math. Soc. (2007)
22. Martens, J.: New insights and perspectives on the natural gradient method. Arxiv

Preprint (2017)

23. Amari, S.: Natural grad. works eﬃciently in learning. Neural Computation (1998)
24. Titsias, M.K.: Variational learning of inducing variables in sparse gaussian processes.

In: In Artiﬁcial Intelligence and Statistics 12. (2009) 567–574

25. Murphy: Machine Learning: A Probabilistic Perspective. The MIT Press (2012)
26. Ranganath, R., Wang, C., Blei, D.M., Xing, E.P.: An Adaptive Learning Rate for

Stochastic Variational Inference. ICML (2013)

27. Maritz, J., Lwin, T.: Empirical Bayes Methods with Applications. Monographs on

Statistics and Applied Probability. (1989)

28. Mandt, S., Hoﬀman, M., Blei, D.: A Variational Analysis of Stochastic Gradient

Algorithms. ICML (2016)

29. Chang, C.C., Lin, C.J.: LIBSVM: A library for support vector machines. ACM

Transactions on Intelligent Systems and Technology (2011) 27:1–27:27

30. Brier, G.W.: Veriﬁcation of forecasts expressed in terms of probability. Monthly

31. Diethe, T.: 13 benchmark datasets derived from the UCI, DELVE and STATLOG

32. Bachem, O., Lucic, M., Hassani, H., Krause, A.: Fast and Provably Good Seedings

weather review (1) (1950) 1–3

repositories (2015)

for k-Means. NIPS (2016)

33. Lichman, M.: UCI machine learning repository (2013)
34. Mandt, S., Wenzel, F., Nakajima, S., Cunningham, J.P., Lippert, C., Kloft, M.:

Sparse Probit Linear Mixed Model. Machine Learning Journal (2017)

35. Perdisci, R., Gu, G., Lee, W.: Using an Ensemble of One-Class SVM Classiﬁers to

H. P.-based Anomaly Detection Systems. Data Mining (2006)

A Appendix

Bayesian Nonlinear SVMs for Big Data

17

A.1 Derivation of the Variational Objective

In the following we give the details of the derivation of the variational objective
(8) for the inducing point model in section 4.2. The variational objective as
deﬁned in (7) is

L = Eq [L1] + Eq [log p(u)] − Eq [log q(λ, u)]

n
(cid:88)

(cid:20)
log λi +

Eq

= −

1
2

i=1

(cid:16)

1
λi

(cid:101)Kii + (cid:0)1 + λi − yiKimK −1

mmu(cid:1)2(cid:17)(cid:21)

− KL (q(u)||p(u)) − Eq(λ) [log q(λ)] .

Using the abbreviation κi = KimK −1

mm the ﬁrst expectation term simpliﬁes to

(cid:20)
log λi +

Eq

(cid:16)

1
λi

= Eq[log λi] + Eq

mmu(cid:1)2(cid:17)(cid:21)

(cid:101)Kii + (cid:0)1 + λi − yiKimK −1

λ−1
i


 (cid:101)Kii + 1 + λ2

i + y2
i
(cid:124)(cid:123)(cid:122)(cid:125)
=1

(κiu)2 + 2λi − 2yiκiu − 2λiyiκiu









c
= Eq(λi)[log λi] +

(cid:101)Kii + 1 + λ2

i + (κiµ)2 + κiζκ(cid:62)

i − 2yiκiµ

+ Eq(λi)[λi] − 2yiκiµ

(cid:17)

(cid:17)

= Eq(λi)[log λi] +

(cid:101)Kii + (1 − yiκiµ)2 + λ2

i + κiζκ(cid:62)
i

+ Eq(λi)[λi] − 2yiκiµ.

(cid:16)

(cid:16)

1
√
αi
1
√
αi

The entropy of q(λi) is

Eq(λi) [log q(λi)] = Eq(λi)

−

log(αi) −

log(λi) − log(2) − log(B 1

αi)) −

λi +

(cid:20)

1
4

1
2

c
= −

c
= −

1
4
1
4

1
2
1
2

log(αi) −

Eαi [log(λi)] − log(B 1

αi)) −

Eαi [λi] −

Eαi

log(αi) −

Eαi [log(λi)] − log(B 1

αi)) −

Eαi [λi] −

√
(

2

1
2
1
2

(cid:19)(cid:21)

αi
λi
(cid:20) 1
λi

(cid:21)

(cid:18)

1
2
αi
2
√

αi
2

,

√
(

√
(

2

2

where B 1
2

(.) is the modiﬁed Bessel function with parameter 1

2 [20].

18

Florian Wenzel et al.

1
2
(cid:16)

n
(cid:88)

(cid:110)

c
=

L

1
2

i=1
1
4

+

n
(cid:88)

=

(cid:110)

−

1
√
αi

2

i=1
− KL (q(u)||p(u))

n
(cid:88)

(cid:110)

c
=

−

(cid:16)

1
√
αi

2

By summing the terms the remaining expectations cancel out and we obtain

−

Eq(λi)[log λi] −

(cid:101)Kii + (1 − yiκiµ)2 + λ2

Eq(λi)[λi] + yiκiµ

(cid:16)

1
√
αi
2

log(αi) +

Eq(λi) [log(λi)] + log(B 1

(

αi)) +

Eq(λi) [λi] +

− KL (q(u)||p(u))

√

2

1
2

i + κiζκ(cid:62)
i
√

(cid:17)

−

1
2

(cid:111)

(cid:101)Kii + (1 − yiκiµ)2 + λ2

i + κiζκ(cid:62)

i − αi

+ yiκiµ +

log(αi) + log(B 1

(

αi))

(cid:17)

(cid:17)

αi
2
1
4

1
4

√

(cid:111)

√

(cid:111)

2

2

(cid:101)Kii + (1 − yiκiµ)2 + λ2

i + κiζκ(cid:62)

i − αi

+ yiκiµ +

log(αi) + log(B 1

(

αi))

log |ζ| −

tr(K −1

1
2
tr(K −1

mmζ) −
1
2

mmζ) −

1
2

µ(cid:62)K −1

mmµ

1
2
µ(cid:62)K −1

mmµ + y(cid:62)κµ

=

log |ζ| −

i=1
1
2

+

1
2

+

n
(cid:88)

(cid:26)

i=1

√
(

log(B 1
4

1
2

αi)) +

log(αi) −

1 − αi − 2yiκi.µ +

κ(µµ(cid:62) + ζ)κ(cid:62) + (cid:101)K

(cid:16)

(cid:16)

α− 1

2

i

1
2

(cid:17)

(cid:17)(cid:27)

.

ii

A.2 Euclidean and Natural Gradients of the Variational Objective

First, we compute the standard euclidean gradients of L. The derivative w.r.t.
the mean and covariance matrix are

dL
dζ

=

(cid:18) d
dζ

1
2

log |ζ| −

tr(K −1

mmζ)

+

d
dζ

−

1
√
αi
2

d
dζ

yiκiζκ(cid:62)

i yi

(cid:19)

N
(cid:88)

i=1
Y 2κ(cid:62)A− 1

2 κ

−

(cid:1)T

(cid:0)ζ −1(cid:1)T

(cid:0)K −1

1
2
mm − κ(cid:62)A− 1
ζ −1 − K −1

mm

(cid:16)

−

2 κ

1
2
(cid:17)

=

1
2
1
2
=: L(cid:48)
ζ,

=

with A = diag(α) and

dL
dµ

= −

1
2

d
dµ

µT K −1

mmµ +

yiκiµ +

(1 − yiκiµ)2

1
√
αi
2

d
dµ

N
(cid:88)

i=1

d
dµ

N
(cid:88)

i=1

yiκi +

mmµ +

1
√
αi
mmµ + κ(cid:62)y + κ(cid:62)Y α− 1
K −1

mm + κ(cid:62)A− 1

2 κ

(cid:17)

= −K −1

= −K −1
(cid:16)

= −

=: L(cid:48)
µ.

(cid:0)yiκ(cid:62)

i − y2

i κ(cid:62)

i κiµ(cid:1)

2 + κ(cid:62)A− 1
µ + κ(cid:62)Y (α− 1

2 κµ

2 + 1)

Bayesian Nonlinear SVMs for Big Data

19

The derivative w.r.t. parameter alphai of the generalized inverse Gaussian distri-
bution is

dL
dαi

1
4

d
dαi

log(αi) +

d
dαi

log(K 1
2

√
(

αi)) +

√

1
2

d
dαi

αi −

(1 − yiκiµ)2 + yi(κiζκ(cid:62)

i + (cid:101)Kii)yi

2

d
dαi

1
√
αi

=

=

=

1
4αi

− (

+

1
4αi

1
√
αi

2

) +

1
√
αi
4

+

(1 − yiκiµ)2 + yi(κiζκ(cid:62)

i + (cid:101)Kii)yi

√
4

3

αi

−

1
√
αi
4

.

(1 − yiκiµ)2 + yi(κiζκ(cid:62)

i + (cid:101)Kii)yi

√

4

3

αi

The natural gradient can be computed by pre-multiplying the euclidean gradient
with the inverse Fisher information matrix [21]. Applied to a Gaussian distribution
this leads to the following expressions for the natural gradient w.r.t. the natural
parameters [21],

(cid:101)∇(η1,η2)L(η) = (cid:0)L(cid:48)

µ(η) − 2L(cid:48)

ζ(η)µ, L(cid:48)

ζ(η)(cid:1) .

Using the identities η1 = ζ −1µ and η2 = − 1

2 ζ −1 we obtain

L(cid:48)

µ(η) =

K −1

mm + κ(cid:62)A− 1

2 κ

2 η1 + κ(cid:62)Y (α− 1
η−1

2 + 1)

(cid:17)

L(cid:48)

ζ(η) =

−2η2 − K −1

mm − κ(cid:62)A− 1

2 κ

= −

(K −1

mm + κ(cid:62)A− 1

2 κ) − η2

(cid:17)

1
2

Finally, this leads to the natural gradients with respect to the natural parameters,

(cid:16)

(cid:16)

1
2
1
2

mm + κ(cid:62)A− 1

2 κ

2 η1 + κ(cid:62)Y (α− 1
η−1

2 + 1) +

−2η2 − K −1

mm − κ(cid:62)A− 1

2 κ

η−1
2 η1

(cid:17)

(cid:17)

(cid:16)

1
2

µ − 2L(cid:48)
(cid:101)∇η1L = L(cid:48)
(cid:16)
1
2

K −1

=

ζµ

= κ(cid:62)Y (α− 1

2 + 1) − η1,

and

(cid:101)∇η2L = L(cid:48)

ζ = −

(K −1

mm + κ(cid:62)A− 1

2 κ) − η2.

1
2

A.3 Optimization of the Kernel Hyperparameters
We consider a general multiple kernel approach. Let k(x, x(cid:48)) = (cid:80)
j γjkj(x, x(cid:48), θj)
be the kernel function where θj denote the hyperparameters of the kernel function
kj (e.g. the length scale parameter of an RBF kernel) and γj the corresponding
kernel weight. Let ω = {θj, γj}j=1,...,J be the collection of all hyperparameters.
The derivative of the variational objective L w.r.t. to the hyperparameters is

dL
dω

= −

(cid:16)

d
1
2
dω
(cid:62)
+ α− 1

2

log |Kmm| + tr(K −1

(cid:16)

diag

κ(µµ(cid:62) + ζ)κ(cid:62) + (cid:101)K

mmζ) + µT K −1
(cid:17) (cid:17)

mmµ − 2(1 + α− 1

2

(cid:62)

)Y κµ

20

Florian Wenzel et al.

Using the abbreviations J ω
obtain

∗∗ = dK∗∗
dω

and ιω = dκ

dω = (J ω

nm − κJ ω

mm) K −1

mm we

dL
dω

= −

1
2
+ α− 1

2

(cid:16)
Tr (cid:0)K −1
mmJ ω
(cid:104)

(cid:62)

mm

(cid:0)I − K −1
mmζ(cid:1)(cid:1) −
(cid:16)(cid:0)µµ(cid:62) + ζ(cid:1) ιω (cid:62) − J ω

diag

κ

(cid:16)

mmJ ω

µ(cid:62)K −1
(cid:17)

mmK −1
+ ιω (cid:0)(cid:0)µµ(cid:62) + ζ(cid:1) κ(cid:62) − Kmn

mm + 2(1 + α− 1

2

(cid:62)

)Y ιω(cid:17)
(cid:105)(cid:17)

.

(cid:1) + J ω

nn

µ

mn

To compute the gradient w.r.t. to speciﬁc hyperparameters we only have to plug
in the derivatives of the kernel function dK∗∗

dω into the above formula.

