Conditional Generators of Words Deﬁnitions

Artyom Gadetsky
National Research University
Higher School of Economics
artygadetsky@yandex.ru

Ilya Yakubovskiy
Joom
yakubovskiy@joom.com

Dmitry Vetrov
National Research University
Higher School of Economics
Samsung-HSE Laboratory
vetrovd@yandex.ru

Abstract

We explore recently introduced deﬁni-
tion modeling technique that provided the
tool for evaluation of different distributed
vector representations of words through
modeling dictionary deﬁnitions of words.
In this work, we study the problem of
word ambiguities in deﬁnition modeling
and propose a possible solution by em-
ploying latent variable modeling and soft
attention mechanisms. Our quantitative
and qualitative evaluation and analysis of
the model shows that taking into account
words ambiguity and polysemy leads to
performance improvement.

1

Introduction

Continuous representations of words are used in
many natural language processing (NLP) applica-
tions. Using pre-trained high-quality word em-
beddings are most effective if not millions of
training examples are available, which is true for
most tasks in NLP (Kumar et al., 2016; Karpa-
thy and Fei-Fei, 2015). Recently, several unsu-
pervised methods were introduced to learn word
vectors from large corpora of texts (Mikolov et al.,
2013; Pennington et al., 2014; Joulin et al., 2016).
Learned vector representations have been shown
to have useful and interesting properties. For ex-
ample, Mikolov et al.
(2013) showed that vec-
tor operations such as subtraction or addition re-
ﬂect semantic relations between words. Despite
all these properties it is hard to precisely evalu-
ate embeddings because analogy relation or word
similarity tasks measure learned information indi-
rectly.

Quite recently Noraset et al. (2017) introduced
a more direct way for word embeddings evalu-
ation. Authors suggested considering deﬁnition

In deﬁnition
modeling as the evaluation task.
modeling vector representations of words are used
for conditional generation of corresponding word
deﬁnitions. The primary motivation is that high-
quality word embedding should contain all useful
information to reconstruct the deﬁnition. The im-
portant drawback of Noraset et al. (2017) deﬁni-
tion models is that they cannot take into account
words with several different meanings. These
problems are related to word disambiguation task,
which is a common problem in natural language
processing. Such examples of polysemantic words
as “bank“ or “spring“ whose meanings can only
be disambiguated using their contexts.
In such
cases, proposed models tend to generate deﬁni-
tions based on the most frequent meaning of the
corresponding word. Therefore, building models
that incorporate word sense disambiguation is an
important research direction in natural language
processing.

In this work, we study the problem of word
ambiguity in deﬁnition modeling task. We pro-
pose several models which can be possible so-
lutions to it. One of them is based on recently
proposed Adaptive Skip Gram model (Bartunov
et al., 2016), the generalized version of the orig-
inal SkipGram Word2Vec, which can differ word
meanings using word context. The second one
is the attention-based model that uses the context
of a word being deﬁned to determine components
of embedding referring to relevant word meaning.
Our contributions are as follows:
(1) we intro-
duce two models based on recurrent neural net-
work (RNN) language models, (2) we collect new
dataset of deﬁnitions, which is larger in number
of unique words than proposed in Noraset et al.
(2017) and also supplement it with examples of the
word usage (3) ﬁnally, in the experiment section
we show that our models outperform previously
proposed models and have the ability to generate

8
1
0
2
 
n
u
J
 
6
2
 
 
]
L
C
.
s
c
[
 
 
1
v
0
9
0
0
1
.
6
0
8
1
:
v
i
X
r
a

deﬁnitions depending on the meaning of words.

2 Related Work

2.1 Constructing Embeddings Using

Dictionary Deﬁnitions

Several works utilize word deﬁnitions to learn em-
beddings. For example, Hill et al. (2016) use deﬁ-
nitions to construct sentence embeddings. Authors
propose to train recurrent neural network produc-
ing an embedding of the dictionary deﬁnition that
is close to an embedding of the corresponding
word. The model is evaluated with the reverse
dictionary task. Bahdanau et al. (2017) suggest
using deﬁnitions to compute embeddings for out-
of-vocabulary words. In comparison to Hill et al.
(2016) work, dictionary reader network is trained
end-to-end for a speciﬁc task.

2.2 Deﬁnition Modeling

Deﬁnition modeling was introduced in Noraset
et al. (2017) work. The goal of the deﬁnition
model p(D|w∗) is to predict the probability of
words in the deﬁnition D = {w1, . . . , wT } given
the word being deﬁned w∗. The joint probability
is decomposed into separate conditional probabil-
ities, each of which is modeled using the recurrent
neural network with soft-max activation, applied
to its logits.

p(D|w∗) =

p(wt|wi<t, w∗)

(1)

T
(cid:89)

t=1

Authors of deﬁnition modeling consider follow-
ing conditional models and their combinations:
Seed (S) - providing word being deﬁned at the ﬁrst
step of the RNN, Input (I) - concatenation of em-
bedding for word being deﬁned with embedding
of word on corresponding time step of the RNN,
Gated (G), which is the modiﬁcation of GRU cell.
Authors use a character-level convolutional neu-
ral network (CNN) to provide character-level in-
formation about the word being deﬁned, this fea-
ture vector is denoted as (CH). One more type of
conditioning referred to as (HE), is hypernym re-
lations between words, extracted using Hearst-like
patterns.

3 Word Embeddings

Many natural language processing applications
treat words as atomic units and represent them
as continuous vectors for further use in machine

learning models. Therefore, learning high-quality
vector representations is the important task.

3.1 Skip-gram

One of the most popular and frequently used vec-
tor representations is Skip-gram model. The orig-
inal Skip-gram model consists of grouped word
prediction tasks. Each task is formulated as a pre-
diction of the word v given word w using their in-
put and output representations:

p(v|w, θ) =

woutv)

exp(inT
v(cid:48)=1 exp(inT

woutv(cid:48))

(cid:80)V

(2)

where θ and V stand for the set of input and out-
put word representations, and dictionary size re-
spectively. These individual prediction tasks are
grouped in a way to independently predict all ad-
jacent (with some sliding window) words y =
{y1, . . . yC} given the central word x:

p(y|x, θ) =

p(yj|x, θ)

(3)

The joint probability of the model is written as fol-
lows:

p(Y |X, θ) =

p(yi|xi, θ)

(4)

(cid:89)

j

N
(cid:89)

i=1

where (X, Y ) = {xi, yi}N
i=1 are training pairs of
words and corresponding contexts and θ stands for
trainable parameters.

Also, optimization of the original Skip-gram
objective can be changed to a negative sampling
procedure as described in the original paper or hi-
erarchical soft-max prediction model (Mnih and
Hinton, 2009) can be used instead of (2) to deal
with computational costs of the denominator. Af-
ter training, the input representations are treated as
word vectors.

3.2 Adaptive Skip-gram

Skip-gram model maintains only one vector repre-
sentation per word that leads to mixing of mean-
ings for polysemantic words. Bartunov et al.
(2016) propose a solution to the described prob-
lem using latent variable modeling. They extend
Skip-gram to Adaptive Skip-gram (AdaGram) in
a way to automatically learn the required num-
ber of vector representations for each word using
Bayesian nonparametric approach. In comparison

with Skip-gram AdaGram assumes several mean-
ings for each word and therefore keeps several
vectors representations for each word. They in-
troduce latent variable z that encodes the index of
meaning and extend (2) to p(v|z, w, θ). They use
hierarchical soft-max approach rather than nega-
tive sampling to overcome computing denomina-
tor.

One important property of the model is an abil-
ity to disambiguate words using context. More
formally, after training on data D = {xi, yi}N
i=1
we may compute the posterior probability of word
meaning given context and take the word vector
with the highest probability.:

p(z = k|x, y, θ) ∝

(cid:90)

(8)

p(v|z = k, w, θ) =

σ(ch(n)inT

wkoutn)

∝ p(y|x, z = k, θ)

p(z = k|β, x)q(β)dβ

(cid:89)

n∈path(v)

(5)
Here inwk stands for input representation of word
w with meaning index k and output representa-
tions are associated with nodes in a binary tree,
where leaves are all possible words in model vo-
cabulary with unique paths from the root to the
corresponding leaf. ch(n) is a function which re-
turns 1 or -1 to each node in the path(·) depending
on whether n is a left or a right child of the previ-
ous node in the path. Huffman tree is often used
for computational efﬁciency.

To automatically determine the number of
meanings for each word authors use the con-
structive deﬁnition of Dirichlet process via stick-
breaking representation (p(z = k|w, β)), which is
commonly used prior distribution on discrete la-
tent variables when the number of possible values
is unknown (e.g. inﬁnite mixtures).

p(z = k|w, β) = βwk

(1 − βwr)

p(βwk|α) = Beta(βwk|1, α)

k−1
(cid:89)

r=1

This model assumes that an inﬁnite number of
meanings for each word may exist. Providing that
we have a ﬁnite amount of data, it can be shown
that only several meanings for each word will have
non-zero prior probabilities.

Finally, the joint probability of all variables in

AdaGram model has the following form:

p(Y, Z, β|X, α, θ) =

p(βwk|α)·

V
(cid:89)

∞
(cid:89)

w=1

k=1

·

N
(cid:89)

i=1

C
(cid:89)

j=1

[p(zi|xi, β)

p(yij|zi, xi, θ)]

This knowledge about word meaning will be
further utilized in one of our models as
disambiguation(x|y).

4 Models

In this section, we describe our extension to orig-
inal deﬁnition model. The goal of the extended
deﬁnition model is to predict the probability of a
deﬁnition D = {w1, . . . , wT } given a word being
deﬁned w∗ and its context C = {c1, . . . , cm} (e.g.
example of use of this word). As it was motivated
earlier, the context will provide proper information
about word meaning. The joint probability is also
decomposed in the conditional probabilities, each
of which is provided with the information about
context:

p(D|w∗, C) =

p(wt|wi<t, w∗, C)

(9)

T
(cid:89)

t=1

Our ﬁrst model is based on original Input (I) con-
ditioned on Adaptive Skip-gram vector represen-
tations. To determine which word embedding to
provide as Input (I) we disambiguate word being
deﬁned using its context words C. More formally
our Input (I) conditioning is turning in:

ht = g([v∗; vt], ht−1)
v∗ = disambiguation(w∗|C)

(10)

(6)

4.1 AdaGram based

(7)

where g is the recurrent cell, [a; b] denotes vec-
tor concatenation, v∗ and vt are embedding of
word being deﬁned w and embedding of word wt
respectively. We refer to this model as Input Adap-
tive (I-Adaptive).

Model is trained by optimizing Evidence Lower
Bound using stochastic variational
inference
(Hoffman et al., 2013) with fully factorized vari-
ational approximation of the posterior distribution
p(Z, β|X, Y, α, θ) ≈ q(Z)q(β).

4.2 Attention based

Adaptive Skip-gram model is very sensitive to
the choice of concentration parameter in Dirich-
let process. The improper setting will cause many

similar vectors representations with smoothed
meanings due to theoretical guarantees on a num-
ber of learned components. To overcome this
problem and to get rid of careful tuning of this
hyper-parameter we introduce following model:

Split
#Words
#Entries
#Tokens
Avg length

train
33,128
97,855
1,078,828
11.03

val
8,867
12,232
134,486
10.99

test
8,850
12,232
133,987
10.95

Table 1: Statistics of new dataset

ht = g([a∗; vt], ht−1)
a∗ = v∗ (cid:12) mask

mask = σ(W

(cid:80)m

i=1 AN N (ci)
m

+ b)

(11)

where (cid:12) is an element-wise product, σ is a
logistic sigmoid function and AN N is attention
neural network, which is a feed-forward neural
network. We motivate these updates by the fact,
that after learning Skip-gram model on a large cor-
pus, vector representation for each word will ab-
sorb information about every meaning of the word.
Using soft binary mask dependent on word context
we extract components of word embedding rele-
vant to corresponding meaning. We refer to this
model as Input Attention (I-Attention).

4.3 Attention SkipGram

For attention-based model, we use different em-
beddings for context words. Because of that, we
pre-train attention block containing embeddings,
attention neural network and linear layer weights
by optimizing a negative sampling loss function in
the same manner as the original Skip-gram model:

vwI )

log σ(v(cid:48)T
wO
k
(cid:88)

+

i=1

Ewi∼Pn(w)[log σ(−v(cid:48)T

wivwI )]

(12)

where v(cid:48)

wO , vwI and v(cid:48)

wi are vector representa-
tion of ”positive” example, anchor word and nega-
tive example respectively. Vector vwI is computed
using embedding of wI and attention mechanism
proposed in previous section.

5 Experiments

5.1 Data

We collected new dataset of deﬁnitions using Ox-
fordDictionaries.com (2018) API. Each entry is a
triplet, containing the word, its deﬁnition and ex-
ample of the use of this word in the given meaning.
It is important to note that in our data set words can
have one or more meanings, depending on the cor-
responding entry in the Oxford Dictionary. Table
1 shows basic statistics of the new dataset.

Figure 1: Perplexities of S+I Attention model
for the case of pre-training (solid lines) and for
the case when the model is trained from scratch
(dashed lines).

5.2 Pre-training

It is well-known that good language model can of-
ten improve metrics such as BLEU for a particu-
lar NLP task (Jozefowicz et al., 2016). According
to this, we decided to pre-train our models. For
this purpose, WikiText-103 dataset (Merity et al.,
2016) was chosen. During pre-training we set v∗
(eq. 10) to zero vector to make our models purely
unconditional. Embeddings for these language
models were initialized by Google Word2Vec vec-
tors1 and were ﬁne-tuned. Figure 1 shows that
this procedure helps to decrease perplexity and
prevents over-ﬁtting. Attention Skip-gram vectors
were also trained on the WikiText-103.

5.3 Results

Both our models are LSTM networks (Hochre-
iter and Schmidhuber, 1997) with an embedding
layer. The attention-based model has own em-
bedding layer, mapping context words to vector
representations. Firstly, we pre-train our mod-
els using the procedure, described above. Then,
we train them on the collected dataset maximiz-
ing log-likelihood objective using Adam (Kingma
and Ba, 2014). Also, we anneal learning rate by

1https://code.google.com/archive/p/word2vec/

Word
star

Context
she got star treatment

star

bright star in the sky

sentence
sentence
head
head

reprint

rape

invisible

shake

nickname

sentence in prison
write up the sentence
the head of a man
he will be the head of the ofﬁce
they never reprinted the
famous treatise
the woman was raped on
her way home at night
he pushed the string through
an inconspicuous hole
my faith has been shaken
the nickname for the u.s.
constitution is ‘old ironsides ’

Deﬁnition
a person who is very important
a small circle of a celestial object
or planet that is seen in a circle
an act of restraining someone or something
a piece of text written to be printed
the upper part of a human body
the chief part of an organization, institution, etc
a written or printed version of
a book or other publication

the act of killing

not able to be seen

cause to be unable to think clearly

a name for a person or thing that is not genuine

Table 2: Examples of deﬁnitions generated by S + I-Attention model for the words and contexts from the
test set.

Model
S+G+CH+HE (1)
S+G+CH+HE (2)
S+G+CH+HE (3)
S + I-Adaptive (2)
S + I-Adaptive (3)
S + I-Attention (2)
S + I-Attention (3)

PPL
45.62
46.12
46.80
46.08
46.93
43.54
44.9

BLEU
11.62 ± 0.05
-
-
11.53 ± 0.03
-
12.08 ± 0.02
-

Table 3: Performance comparison between best
model proposed by Noraset et al. (2017) and our
models on the test set. Number in brackets means
number of LSTM layers. BLEU is averaged across
3 trials.

a factor of 10 if validation loss doesn’t decrease
per epochs. We use original Adaptive Skip-gram
vectors as inputs to S+I-Adaptive, which were ob-
tained from the ofﬁcial repository2. We compare
different models using perplexity and BLEU score
on the test set. BLEU score is computed only for
models with the lowest perplexity and only on the
test words that have multiple meanings. The re-
sults are presented in Table 3. We see that both
models that utilize knowledge about meaning of
the word have better performance than the com-
peting one. We generated deﬁnitions using S + I-
Attention model with simple temperature sampling

algorithm (τ = 0.1). Table 2 shows the examples.
The source code and dataset will be freely avail-
able 3.

6 Conclusion

In the paper, we proposed two deﬁnition models
which can work with polysemantic words. We
evaluate them using perplexity and measure the
deﬁnition generation accuracy with BLEU score.
Obtained results show that incorporating informa-
tion about word senses leads to improved met-
rics. Moreover, generated deﬁnitions show that
even implicit word context can help to differ word
meanings. In future work, we plan to explore in-
dividual components of word embedding and the
mask produced by our attention-based model to
get a deeper understanding of vectors representa-
tions of words.

Acknowledgments

This work was partly supported by Samsung Re-
search, Samsung Electronics, Sberbank AI Lab
and the Russian Science Foundation grant 17-71-
20072.

2https://github.com/sbos/AdaGram.jl

3https://github.com/agadetsky/pytorch-deﬁnitions

Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems
21, pages 1081–1088. Curran Associates, Inc.

Thanapon Noraset, Chen Liang, Larry Birnbaum, and
Doug Downey. 2017. Deﬁnition modeling: Learn-
ing to deﬁne word embeddings in natural language.
In 31st AAAI Conference on Artiﬁcial Intelligence,
AAAI 2017, pages 3259–3266. AAAI press.

OxfordDictionaries.com. 2018. Oxford University

Press.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

References

Dzmitry Bahdanau, Tom Bosc, Stanislaw Jastrzebski,
Edward Grefenstette, Pascal Vincent, and Yoshua
Bengio. 2017. Learning to compute word embed-
dings on the ﬂy. arXiv preprint arXiv:1706.00286.

Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin,
and Dmitry Vetrov. 2016. Breaking sticks and ambi-
guities with adaptive skip-gram. In Artiﬁcial Intelli-
gence and Statistics, pages 130–138.

Felix Hill, KyungHyun Cho, Anna Korhonen, and
Yoshua Bengio. 2016.
Learning to understand
phrases by embedding the dictionary. Transactions
of the Association for Computational Linguistics,
4:17–30.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Matthew D. Hoffman, David M. Blei, Chong Wang,
and John Paisley. 2013. Stochastic variational in-
Journal of Machine Learning Research,
ference.
14:1303–1347.

Armand Joulin, Edouard Grave, Piotr Bojanowski,
Matthijs Douze, H´erve J´egou, and Tomas Mikolov.
2016. Fasttext.zip: Compressing text classiﬁcation
models. arXiv preprint arXiv:1612.03651.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Exploring
arXiv preprint

Shazeer, and Yonghui Wu. 2016.
the limits of language modeling.
arXiv:1602.02410.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
the IEEE conference
tions.
on computer vision and pattern recognition, pages
3128–3137.

In Proceedings of

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit
Iyyer, James Bradbury, Ishaan Gulrajani, Victor
Zhong, Romain Paulus, and Richard Socher. 2016.
Ask me anything: Dynamic memory networks for
natural language processing. In Proceedings of The
33rd International Conference on Machine Learn-
ing, volume 48 of Proceedings of Machine Learning
Research, pages 1378–1387. PMLR.

Stephen Merity, Caiming Xiong, James Bradbury, and
Pointer sentinel mixture

Richard Socher. 2016.
models. arXiv preprint arXiv:1609.07843.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
In C. J. C. Burges, L. Bottou, M. Welling,
ality.
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

Conditional Generators of Words Deﬁnitions

Artyom Gadetsky
National Research University
Higher School of Economics
artygadetsky@yandex.ru

Ilya Yakubovskiy
Joom
yakubovskiy@joom.com

Dmitry Vetrov
National Research University
Higher School of Economics
Samsung-HSE Laboratory
vetrovd@yandex.ru

Abstract

We explore recently introduced deﬁni-
tion modeling technique that provided the
tool for evaluation of different distributed
vector representations of words through
modeling dictionary deﬁnitions of words.
In this work, we study the problem of
word ambiguities in deﬁnition modeling
and propose a possible solution by em-
ploying latent variable modeling and soft
attention mechanisms. Our quantitative
and qualitative evaluation and analysis of
the model shows that taking into account
words ambiguity and polysemy leads to
performance improvement.

1

Introduction

Continuous representations of words are used in
many natural language processing (NLP) applica-
tions. Using pre-trained high-quality word em-
beddings are most effective if not millions of
training examples are available, which is true for
most tasks in NLP (Kumar et al., 2016; Karpa-
thy and Fei-Fei, 2015). Recently, several unsu-
pervised methods were introduced to learn word
vectors from large corpora of texts (Mikolov et al.,
2013; Pennington et al., 2014; Joulin et al., 2016).
Learned vector representations have been shown
to have useful and interesting properties. For ex-
ample, Mikolov et al.
(2013) showed that vec-
tor operations such as subtraction or addition re-
ﬂect semantic relations between words. Despite
all these properties it is hard to precisely evalu-
ate embeddings because analogy relation or word
similarity tasks measure learned information indi-
rectly.

Quite recently Noraset et al. (2017) introduced
a more direct way for word embeddings evalu-
ation. Authors suggested considering deﬁnition

In deﬁnition
modeling as the evaluation task.
modeling vector representations of words are used
for conditional generation of corresponding word
deﬁnitions. The primary motivation is that high-
quality word embedding should contain all useful
information to reconstruct the deﬁnition. The im-
portant drawback of Noraset et al. (2017) deﬁni-
tion models is that they cannot take into account
words with several different meanings. These
problems are related to word disambiguation task,
which is a common problem in natural language
processing. Such examples of polysemantic words
as “bank“ or “spring“ whose meanings can only
be disambiguated using their contexts.
In such
cases, proposed models tend to generate deﬁni-
tions based on the most frequent meaning of the
corresponding word. Therefore, building models
that incorporate word sense disambiguation is an
important research direction in natural language
processing.

In this work, we study the problem of word
ambiguity in deﬁnition modeling task. We pro-
pose several models which can be possible so-
lutions to it. One of them is based on recently
proposed Adaptive Skip Gram model (Bartunov
et al., 2016), the generalized version of the orig-
inal SkipGram Word2Vec, which can differ word
meanings using word context. The second one
is the attention-based model that uses the context
of a word being deﬁned to determine components
of embedding referring to relevant word meaning.
Our contributions are as follows:
(1) we intro-
duce two models based on recurrent neural net-
work (RNN) language models, (2) we collect new
dataset of deﬁnitions, which is larger in number
of unique words than proposed in Noraset et al.
(2017) and also supplement it with examples of the
word usage (3) ﬁnally, in the experiment section
we show that our models outperform previously
proposed models and have the ability to generate

8
1
0
2
 
n
u
J
 
6
2
 
 
]
L
C
.
s
c
[
 
 
1
v
0
9
0
0
1
.
6
0
8
1
:
v
i
X
r
a

deﬁnitions depending on the meaning of words.

2 Related Work

2.1 Constructing Embeddings Using

Dictionary Deﬁnitions

Several works utilize word deﬁnitions to learn em-
beddings. For example, Hill et al. (2016) use deﬁ-
nitions to construct sentence embeddings. Authors
propose to train recurrent neural network produc-
ing an embedding of the dictionary deﬁnition that
is close to an embedding of the corresponding
word. The model is evaluated with the reverse
dictionary task. Bahdanau et al. (2017) suggest
using deﬁnitions to compute embeddings for out-
of-vocabulary words. In comparison to Hill et al.
(2016) work, dictionary reader network is trained
end-to-end for a speciﬁc task.

2.2 Deﬁnition Modeling

Deﬁnition modeling was introduced in Noraset
et al. (2017) work. The goal of the deﬁnition
model p(D|w∗) is to predict the probability of
words in the deﬁnition D = {w1, . . . , wT } given
the word being deﬁned w∗. The joint probability
is decomposed into separate conditional probabil-
ities, each of which is modeled using the recurrent
neural network with soft-max activation, applied
to its logits.

p(D|w∗) =

p(wt|wi<t, w∗)

(1)

T
(cid:89)

t=1

Authors of deﬁnition modeling consider follow-
ing conditional models and their combinations:
Seed (S) - providing word being deﬁned at the ﬁrst
step of the RNN, Input (I) - concatenation of em-
bedding for word being deﬁned with embedding
of word on corresponding time step of the RNN,
Gated (G), which is the modiﬁcation of GRU cell.
Authors use a character-level convolutional neu-
ral network (CNN) to provide character-level in-
formation about the word being deﬁned, this fea-
ture vector is denoted as (CH). One more type of
conditioning referred to as (HE), is hypernym re-
lations between words, extracted using Hearst-like
patterns.

3 Word Embeddings

Many natural language processing applications
treat words as atomic units and represent them
as continuous vectors for further use in machine

learning models. Therefore, learning high-quality
vector representations is the important task.

3.1 Skip-gram

One of the most popular and frequently used vec-
tor representations is Skip-gram model. The orig-
inal Skip-gram model consists of grouped word
prediction tasks. Each task is formulated as a pre-
diction of the word v given word w using their in-
put and output representations:

p(v|w, θ) =

woutv)

exp(inT
v(cid:48)=1 exp(inT

woutv(cid:48))

(cid:80)V

(2)

where θ and V stand for the set of input and out-
put word representations, and dictionary size re-
spectively. These individual prediction tasks are
grouped in a way to independently predict all ad-
jacent (with some sliding window) words y =
{y1, . . . yC} given the central word x:

p(y|x, θ) =

p(yj|x, θ)

(3)

The joint probability of the model is written as fol-
lows:

p(Y |X, θ) =

p(yi|xi, θ)

(4)

(cid:89)

j

N
(cid:89)

i=1

where (X, Y ) = {xi, yi}N
i=1 are training pairs of
words and corresponding contexts and θ stands for
trainable parameters.

Also, optimization of the original Skip-gram
objective can be changed to a negative sampling
procedure as described in the original paper or hi-
erarchical soft-max prediction model (Mnih and
Hinton, 2009) can be used instead of (2) to deal
with computational costs of the denominator. Af-
ter training, the input representations are treated as
word vectors.

3.2 Adaptive Skip-gram

Skip-gram model maintains only one vector repre-
sentation per word that leads to mixing of mean-
ings for polysemantic words. Bartunov et al.
(2016) propose a solution to the described prob-
lem using latent variable modeling. They extend
Skip-gram to Adaptive Skip-gram (AdaGram) in
a way to automatically learn the required num-
ber of vector representations for each word using
Bayesian nonparametric approach. In comparison

with Skip-gram AdaGram assumes several mean-
ings for each word and therefore keeps several
vectors representations for each word. They in-
troduce latent variable z that encodes the index of
meaning and extend (2) to p(v|z, w, θ). They use
hierarchical soft-max approach rather than nega-
tive sampling to overcome computing denomina-
tor.

One important property of the model is an abil-
ity to disambiguate words using context. More
formally, after training on data D = {xi, yi}N
i=1
we may compute the posterior probability of word
meaning given context and take the word vector
with the highest probability.:

p(z = k|x, y, θ) ∝

(cid:90)

(8)

p(v|z = k, w, θ) =

σ(ch(n)inT

wkoutn)

∝ p(y|x, z = k, θ)

p(z = k|β, x)q(β)dβ

(cid:89)

n∈path(v)

(5)
Here inwk stands for input representation of word
w with meaning index k and output representa-
tions are associated with nodes in a binary tree,
where leaves are all possible words in model vo-
cabulary with unique paths from the root to the
corresponding leaf. ch(n) is a function which re-
turns 1 or -1 to each node in the path(·) depending
on whether n is a left or a right child of the previ-
ous node in the path. Huffman tree is often used
for computational efﬁciency.

To automatically determine the number of
meanings for each word authors use the con-
structive deﬁnition of Dirichlet process via stick-
breaking representation (p(z = k|w, β)), which is
commonly used prior distribution on discrete la-
tent variables when the number of possible values
is unknown (e.g. inﬁnite mixtures).

p(z = k|w, β) = βwk

(1 − βwr)

p(βwk|α) = Beta(βwk|1, α)

k−1
(cid:89)

r=1

This model assumes that an inﬁnite number of
meanings for each word may exist. Providing that
we have a ﬁnite amount of data, it can be shown
that only several meanings for each word will have
non-zero prior probabilities.

Finally, the joint probability of all variables in

AdaGram model has the following form:

p(Y, Z, β|X, α, θ) =

p(βwk|α)·

V
(cid:89)

∞
(cid:89)

w=1

k=1

·

N
(cid:89)

i=1

C
(cid:89)

j=1

[p(zi|xi, β)

p(yij|zi, xi, θ)]

This knowledge about word meaning will be
further utilized in one of our models as
disambiguation(x|y).

4 Models

In this section, we describe our extension to orig-
inal deﬁnition model. The goal of the extended
deﬁnition model is to predict the probability of a
deﬁnition D = {w1, . . . , wT } given a word being
deﬁned w∗ and its context C = {c1, . . . , cm} (e.g.
example of use of this word). As it was motivated
earlier, the context will provide proper information
about word meaning. The joint probability is also
decomposed in the conditional probabilities, each
of which is provided with the information about
context:

p(D|w∗, C) =

p(wt|wi<t, w∗, C)

(9)

T
(cid:89)

t=1

Our ﬁrst model is based on original Input (I) con-
ditioned on Adaptive Skip-gram vector represen-
tations. To determine which word embedding to
provide as Input (I) we disambiguate word being
deﬁned using its context words C. More formally
our Input (I) conditioning is turning in:

ht = g([v∗; vt], ht−1)
v∗ = disambiguation(w∗|C)

(10)

(6)

4.1 AdaGram based

(7)

where g is the recurrent cell, [a; b] denotes vec-
tor concatenation, v∗ and vt are embedding of
word being deﬁned w and embedding of word wt
respectively. We refer to this model as Input Adap-
tive (I-Adaptive).

Model is trained by optimizing Evidence Lower
Bound using stochastic variational
inference
(Hoffman et al., 2013) with fully factorized vari-
ational approximation of the posterior distribution
p(Z, β|X, Y, α, θ) ≈ q(Z)q(β).

4.2 Attention based

Adaptive Skip-gram model is very sensitive to
the choice of concentration parameter in Dirich-
let process. The improper setting will cause many

similar vectors representations with smoothed
meanings due to theoretical guarantees on a num-
ber of learned components. To overcome this
problem and to get rid of careful tuning of this
hyper-parameter we introduce following model:

Split
#Words
#Entries
#Tokens
Avg length

train
33,128
97,855
1,078,828
11.03

val
8,867
12,232
134,486
10.99

test
8,850
12,232
133,987
10.95

Table 1: Statistics of new dataset

ht = g([a∗; vt], ht−1)
a∗ = v∗ (cid:12) mask

mask = σ(W

(cid:80)m

i=1 AN N (ci)
m

+ b)

(11)

where (cid:12) is an element-wise product, σ is a
logistic sigmoid function and AN N is attention
neural network, which is a feed-forward neural
network. We motivate these updates by the fact,
that after learning Skip-gram model on a large cor-
pus, vector representation for each word will ab-
sorb information about every meaning of the word.
Using soft binary mask dependent on word context
we extract components of word embedding rele-
vant to corresponding meaning. We refer to this
model as Input Attention (I-Attention).

4.3 Attention SkipGram

For attention-based model, we use different em-
beddings for context words. Because of that, we
pre-train attention block containing embeddings,
attention neural network and linear layer weights
by optimizing a negative sampling loss function in
the same manner as the original Skip-gram model:

vwI )

log σ(v(cid:48)T
wO
k
(cid:88)

+

i=1

Ewi∼Pn(w)[log σ(−v(cid:48)T

wivwI )]

(12)

where v(cid:48)

wO , vwI and v(cid:48)

wi are vector representa-
tion of ”positive” example, anchor word and nega-
tive example respectively. Vector vwI is computed
using embedding of wI and attention mechanism
proposed in previous section.

5 Experiments

5.1 Data

We collected new dataset of deﬁnitions using Ox-
fordDictionaries.com (2018) API. Each entry is a
triplet, containing the word, its deﬁnition and ex-
ample of the use of this word in the given meaning.
It is important to note that in our data set words can
have one or more meanings, depending on the cor-
responding entry in the Oxford Dictionary. Table
1 shows basic statistics of the new dataset.

Figure 1: Perplexities of S+I Attention model
for the case of pre-training (solid lines) and for
the case when the model is trained from scratch
(dashed lines).

5.2 Pre-training

It is well-known that good language model can of-
ten improve metrics such as BLEU for a particu-
lar NLP task (Jozefowicz et al., 2016). According
to this, we decided to pre-train our models. For
this purpose, WikiText-103 dataset (Merity et al.,
2016) was chosen. During pre-training we set v∗
(eq. 10) to zero vector to make our models purely
unconditional. Embeddings for these language
models were initialized by Google Word2Vec vec-
tors1 and were ﬁne-tuned. Figure 1 shows that
this procedure helps to decrease perplexity and
prevents over-ﬁtting. Attention Skip-gram vectors
were also trained on the WikiText-103.

5.3 Results

Both our models are LSTM networks (Hochre-
iter and Schmidhuber, 1997) with an embedding
layer. The attention-based model has own em-
bedding layer, mapping context words to vector
representations. Firstly, we pre-train our mod-
els using the procedure, described above. Then,
we train them on the collected dataset maximiz-
ing log-likelihood objective using Adam (Kingma
and Ba, 2014). Also, we anneal learning rate by

1https://code.google.com/archive/p/word2vec/

Word
star

Context
she got star treatment

star

bright star in the sky

sentence
sentence
head
head

reprint

rape

invisible

shake

nickname

sentence in prison
write up the sentence
the head of a man
he will be the head of the ofﬁce
they never reprinted the
famous treatise
the woman was raped on
her way home at night
he pushed the string through
an inconspicuous hole
my faith has been shaken
the nickname for the u.s.
constitution is ‘old ironsides ’

Deﬁnition
a person who is very important
a small circle of a celestial object
or planet that is seen in a circle
an act of restraining someone or something
a piece of text written to be printed
the upper part of a human body
the chief part of an organization, institution, etc
a written or printed version of
a book or other publication

the act of killing

not able to be seen

cause to be unable to think clearly

a name for a person or thing that is not genuine

Table 2: Examples of deﬁnitions generated by S + I-Attention model for the words and contexts from the
test set.

Model
S+G+CH+HE (1)
S+G+CH+HE (2)
S+G+CH+HE (3)
S + I-Adaptive (2)
S + I-Adaptive (3)
S + I-Attention (2)
S + I-Attention (3)

PPL
45.62
46.12
46.80
46.08
46.93
43.54
44.9

BLEU
11.62 ± 0.05
-
-
11.53 ± 0.03
-
12.08 ± 0.02
-

Table 3: Performance comparison between best
model proposed by Noraset et al. (2017) and our
models on the test set. Number in brackets means
number of LSTM layers. BLEU is averaged across
3 trials.

a factor of 10 if validation loss doesn’t decrease
per epochs. We use original Adaptive Skip-gram
vectors as inputs to S+I-Adaptive, which were ob-
tained from the ofﬁcial repository2. We compare
different models using perplexity and BLEU score
on the test set. BLEU score is computed only for
models with the lowest perplexity and only on the
test words that have multiple meanings. The re-
sults are presented in Table 3. We see that both
models that utilize knowledge about meaning of
the word have better performance than the com-
peting one. We generated deﬁnitions using S + I-
Attention model with simple temperature sampling

algorithm (τ = 0.1). Table 2 shows the examples.
The source code and dataset will be freely avail-
able 3.

6 Conclusion

In the paper, we proposed two deﬁnition models
which can work with polysemantic words. We
evaluate them using perplexity and measure the
deﬁnition generation accuracy with BLEU score.
Obtained results show that incorporating informa-
tion about word senses leads to improved met-
rics. Moreover, generated deﬁnitions show that
even implicit word context can help to differ word
meanings. In future work, we plan to explore in-
dividual components of word embedding and the
mask produced by our attention-based model to
get a deeper understanding of vectors representa-
tions of words.

Acknowledgments

This work was partly supported by Samsung Re-
search, Samsung Electronics, Sberbank AI Lab
and the Russian Science Foundation grant 17-71-
20072.

2https://github.com/sbos/AdaGram.jl

3https://github.com/agadetsky/pytorch-deﬁnitions

Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems
21, pages 1081–1088. Curran Associates, Inc.

Thanapon Noraset, Chen Liang, Larry Birnbaum, and
Doug Downey. 2017. Deﬁnition modeling: Learn-
ing to deﬁne word embeddings in natural language.
In 31st AAAI Conference on Artiﬁcial Intelligence,
AAAI 2017, pages 3259–3266. AAAI press.

OxfordDictionaries.com. 2018. Oxford University

Press.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

References

Dzmitry Bahdanau, Tom Bosc, Stanislaw Jastrzebski,
Edward Grefenstette, Pascal Vincent, and Yoshua
Bengio. 2017. Learning to compute word embed-
dings on the ﬂy. arXiv preprint arXiv:1706.00286.

Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin,
and Dmitry Vetrov. 2016. Breaking sticks and ambi-
guities with adaptive skip-gram. In Artiﬁcial Intelli-
gence and Statistics, pages 130–138.

Felix Hill, KyungHyun Cho, Anna Korhonen, and
Yoshua Bengio. 2016.
Learning to understand
phrases by embedding the dictionary. Transactions
of the Association for Computational Linguistics,
4:17–30.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Matthew D. Hoffman, David M. Blei, Chong Wang,
and John Paisley. 2013. Stochastic variational in-
Journal of Machine Learning Research,
ference.
14:1303–1347.

Armand Joulin, Edouard Grave, Piotr Bojanowski,
Matthijs Douze, H´erve J´egou, and Tomas Mikolov.
2016. Fasttext.zip: Compressing text classiﬁcation
models. arXiv preprint arXiv:1612.03651.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Exploring
arXiv preprint

Shazeer, and Yonghui Wu. 2016.
the limits of language modeling.
arXiv:1602.02410.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
the IEEE conference
tions.
on computer vision and pattern recognition, pages
3128–3137.

In Proceedings of

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit
Iyyer, James Bradbury, Ishaan Gulrajani, Victor
Zhong, Romain Paulus, and Richard Socher. 2016.
Ask me anything: Dynamic memory networks for
natural language processing. In Proceedings of The
33rd International Conference on Machine Learn-
ing, volume 48 of Proceedings of Machine Learning
Research, pages 1378–1387. PMLR.

Stephen Merity, Caiming Xiong, James Bradbury, and
Pointer sentinel mixture

Richard Socher. 2016.
models. arXiv preprint arXiv:1609.07843.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
In C. J. C. Burges, L. Bottou, M. Welling,
ality.
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

Conditional Generators of Words Deﬁnitions

Artyom Gadetsky
National Research University
Higher School of Economics
artygadetsky@yandex.ru

Ilya Yakubovskiy
Joom
yakubovskiy@joom.com

Dmitry Vetrov
National Research University
Higher School of Economics
Samsung-HSE Laboratory
vetrovd@yandex.ru

Abstract

We explore recently introduced deﬁni-
tion modeling technique that provided the
tool for evaluation of different distributed
vector representations of words through
modeling dictionary deﬁnitions of words.
In this work, we study the problem of
word ambiguities in deﬁnition modeling
and propose a possible solution by em-
ploying latent variable modeling and soft
attention mechanisms. Our quantitative
and qualitative evaluation and analysis of
the model shows that taking into account
words ambiguity and polysemy leads to
performance improvement.

1

Introduction

Continuous representations of words are used in
many natural language processing (NLP) applica-
tions. Using pre-trained high-quality word em-
beddings are most effective if not millions of
training examples are available, which is true for
most tasks in NLP (Kumar et al., 2016; Karpa-
thy and Fei-Fei, 2015). Recently, several unsu-
pervised methods were introduced to learn word
vectors from large corpora of texts (Mikolov et al.,
2013; Pennington et al., 2014; Joulin et al., 2016).
Learned vector representations have been shown
to have useful and interesting properties. For ex-
ample, Mikolov et al.
(2013) showed that vec-
tor operations such as subtraction or addition re-
ﬂect semantic relations between words. Despite
all these properties it is hard to precisely evalu-
ate embeddings because analogy relation or word
similarity tasks measure learned information indi-
rectly.

Quite recently Noraset et al. (2017) introduced
a more direct way for word embeddings evalu-
ation. Authors suggested considering deﬁnition

In deﬁnition
modeling as the evaluation task.
modeling vector representations of words are used
for conditional generation of corresponding word
deﬁnitions. The primary motivation is that high-
quality word embedding should contain all useful
information to reconstruct the deﬁnition. The im-
portant drawback of Noraset et al. (2017) deﬁni-
tion models is that they cannot take into account
words with several different meanings. These
problems are related to word disambiguation task,
which is a common problem in natural language
processing. Such examples of polysemantic words
as “bank“ or “spring“ whose meanings can only
be disambiguated using their contexts.
In such
cases, proposed models tend to generate deﬁni-
tions based on the most frequent meaning of the
corresponding word. Therefore, building models
that incorporate word sense disambiguation is an
important research direction in natural language
processing.

In this work, we study the problem of word
ambiguity in deﬁnition modeling task. We pro-
pose several models which can be possible so-
lutions to it. One of them is based on recently
proposed Adaptive Skip Gram model (Bartunov
et al., 2016), the generalized version of the orig-
inal SkipGram Word2Vec, which can differ word
meanings using word context. The second one
is the attention-based model that uses the context
of a word being deﬁned to determine components
of embedding referring to relevant word meaning.
Our contributions are as follows:
(1) we intro-
duce two models based on recurrent neural net-
work (RNN) language models, (2) we collect new
dataset of deﬁnitions, which is larger in number
of unique words than proposed in Noraset et al.
(2017) and also supplement it with examples of the
word usage (3) ﬁnally, in the experiment section
we show that our models outperform previously
proposed models and have the ability to generate

8
1
0
2
 
n
u
J
 
6
2
 
 
]
L
C
.
s
c
[
 
 
1
v
0
9
0
0
1
.
6
0
8
1
:
v
i
X
r
a

deﬁnitions depending on the meaning of words.

2 Related Work

2.1 Constructing Embeddings Using

Dictionary Deﬁnitions

Several works utilize word deﬁnitions to learn em-
beddings. For example, Hill et al. (2016) use deﬁ-
nitions to construct sentence embeddings. Authors
propose to train recurrent neural network produc-
ing an embedding of the dictionary deﬁnition that
is close to an embedding of the corresponding
word. The model is evaluated with the reverse
dictionary task. Bahdanau et al. (2017) suggest
using deﬁnitions to compute embeddings for out-
of-vocabulary words. In comparison to Hill et al.
(2016) work, dictionary reader network is trained
end-to-end for a speciﬁc task.

2.2 Deﬁnition Modeling

Deﬁnition modeling was introduced in Noraset
et al. (2017) work. The goal of the deﬁnition
model p(D|w∗) is to predict the probability of
words in the deﬁnition D = {w1, . . . , wT } given
the word being deﬁned w∗. The joint probability
is decomposed into separate conditional probabil-
ities, each of which is modeled using the recurrent
neural network with soft-max activation, applied
to its logits.

p(D|w∗) =

p(wt|wi<t, w∗)

(1)

T
(cid:89)

t=1

Authors of deﬁnition modeling consider follow-
ing conditional models and their combinations:
Seed (S) - providing word being deﬁned at the ﬁrst
step of the RNN, Input (I) - concatenation of em-
bedding for word being deﬁned with embedding
of word on corresponding time step of the RNN,
Gated (G), which is the modiﬁcation of GRU cell.
Authors use a character-level convolutional neu-
ral network (CNN) to provide character-level in-
formation about the word being deﬁned, this fea-
ture vector is denoted as (CH). One more type of
conditioning referred to as (HE), is hypernym re-
lations between words, extracted using Hearst-like
patterns.

3 Word Embeddings

Many natural language processing applications
treat words as atomic units and represent them
as continuous vectors for further use in machine

learning models. Therefore, learning high-quality
vector representations is the important task.

3.1 Skip-gram

One of the most popular and frequently used vec-
tor representations is Skip-gram model. The orig-
inal Skip-gram model consists of grouped word
prediction tasks. Each task is formulated as a pre-
diction of the word v given word w using their in-
put and output representations:

p(v|w, θ) =

woutv)

exp(inT
v(cid:48)=1 exp(inT

woutv(cid:48))

(cid:80)V

(2)

where θ and V stand for the set of input and out-
put word representations, and dictionary size re-
spectively. These individual prediction tasks are
grouped in a way to independently predict all ad-
jacent (with some sliding window) words y =
{y1, . . . yC} given the central word x:

p(y|x, θ) =

p(yj|x, θ)

(3)

The joint probability of the model is written as fol-
lows:

p(Y |X, θ) =

p(yi|xi, θ)

(4)

(cid:89)

j

N
(cid:89)

i=1

where (X, Y ) = {xi, yi}N
i=1 are training pairs of
words and corresponding contexts and θ stands for
trainable parameters.

Also, optimization of the original Skip-gram
objective can be changed to a negative sampling
procedure as described in the original paper or hi-
erarchical soft-max prediction model (Mnih and
Hinton, 2009) can be used instead of (2) to deal
with computational costs of the denominator. Af-
ter training, the input representations are treated as
word vectors.

3.2 Adaptive Skip-gram

Skip-gram model maintains only one vector repre-
sentation per word that leads to mixing of mean-
ings for polysemantic words. Bartunov et al.
(2016) propose a solution to the described prob-
lem using latent variable modeling. They extend
Skip-gram to Adaptive Skip-gram (AdaGram) in
a way to automatically learn the required num-
ber of vector representations for each word using
Bayesian nonparametric approach. In comparison

with Skip-gram AdaGram assumes several mean-
ings for each word and therefore keeps several
vectors representations for each word. They in-
troduce latent variable z that encodes the index of
meaning and extend (2) to p(v|z, w, θ). They use
hierarchical soft-max approach rather than nega-
tive sampling to overcome computing denomina-
tor.

One important property of the model is an abil-
ity to disambiguate words using context. More
formally, after training on data D = {xi, yi}N
i=1
we may compute the posterior probability of word
meaning given context and take the word vector
with the highest probability.:

p(z = k|x, y, θ) ∝

(cid:90)

(8)

p(v|z = k, w, θ) =

σ(ch(n)inT

wkoutn)

∝ p(y|x, z = k, θ)

p(z = k|β, x)q(β)dβ

(cid:89)

n∈path(v)

(5)
Here inwk stands for input representation of word
w with meaning index k and output representa-
tions are associated with nodes in a binary tree,
where leaves are all possible words in model vo-
cabulary with unique paths from the root to the
corresponding leaf. ch(n) is a function which re-
turns 1 or -1 to each node in the path(·) depending
on whether n is a left or a right child of the previ-
ous node in the path. Huffman tree is often used
for computational efﬁciency.

To automatically determine the number of
meanings for each word authors use the con-
structive deﬁnition of Dirichlet process via stick-
breaking representation (p(z = k|w, β)), which is
commonly used prior distribution on discrete la-
tent variables when the number of possible values
is unknown (e.g. inﬁnite mixtures).

p(z = k|w, β) = βwk

(1 − βwr)

p(βwk|α) = Beta(βwk|1, α)

k−1
(cid:89)

r=1

This model assumes that an inﬁnite number of
meanings for each word may exist. Providing that
we have a ﬁnite amount of data, it can be shown
that only several meanings for each word will have
non-zero prior probabilities.

Finally, the joint probability of all variables in

AdaGram model has the following form:

p(Y, Z, β|X, α, θ) =

p(βwk|α)·

V
(cid:89)

∞
(cid:89)

w=1

k=1

·

N
(cid:89)

i=1

C
(cid:89)

j=1

[p(zi|xi, β)

p(yij|zi, xi, θ)]

This knowledge about word meaning will be
further utilized in one of our models as
disambiguation(x|y).

4 Models

In this section, we describe our extension to orig-
inal deﬁnition model. The goal of the extended
deﬁnition model is to predict the probability of a
deﬁnition D = {w1, . . . , wT } given a word being
deﬁned w∗ and its context C = {c1, . . . , cm} (e.g.
example of use of this word). As it was motivated
earlier, the context will provide proper information
about word meaning. The joint probability is also
decomposed in the conditional probabilities, each
of which is provided with the information about
context:

p(D|w∗, C) =

p(wt|wi<t, w∗, C)

(9)

T
(cid:89)

t=1

Our ﬁrst model is based on original Input (I) con-
ditioned on Adaptive Skip-gram vector represen-
tations. To determine which word embedding to
provide as Input (I) we disambiguate word being
deﬁned using its context words C. More formally
our Input (I) conditioning is turning in:

ht = g([v∗; vt], ht−1)
v∗ = disambiguation(w∗|C)

(10)

(6)

4.1 AdaGram based

(7)

where g is the recurrent cell, [a; b] denotes vec-
tor concatenation, v∗ and vt are embedding of
word being deﬁned w and embedding of word wt
respectively. We refer to this model as Input Adap-
tive (I-Adaptive).

Model is trained by optimizing Evidence Lower
Bound using stochastic variational
inference
(Hoffman et al., 2013) with fully factorized vari-
ational approximation of the posterior distribution
p(Z, β|X, Y, α, θ) ≈ q(Z)q(β).

4.2 Attention based

Adaptive Skip-gram model is very sensitive to
the choice of concentration parameter in Dirich-
let process. The improper setting will cause many

similar vectors representations with smoothed
meanings due to theoretical guarantees on a num-
ber of learned components. To overcome this
problem and to get rid of careful tuning of this
hyper-parameter we introduce following model:

Split
#Words
#Entries
#Tokens
Avg length

train
33,128
97,855
1,078,828
11.03

val
8,867
12,232
134,486
10.99

test
8,850
12,232
133,987
10.95

Table 1: Statistics of new dataset

ht = g([a∗; vt], ht−1)
a∗ = v∗ (cid:12) mask

mask = σ(W

(cid:80)m

i=1 AN N (ci)
m

+ b)

(11)

where (cid:12) is an element-wise product, σ is a
logistic sigmoid function and AN N is attention
neural network, which is a feed-forward neural
network. We motivate these updates by the fact,
that after learning Skip-gram model on a large cor-
pus, vector representation for each word will ab-
sorb information about every meaning of the word.
Using soft binary mask dependent on word context
we extract components of word embedding rele-
vant to corresponding meaning. We refer to this
model as Input Attention (I-Attention).

4.3 Attention SkipGram

For attention-based model, we use different em-
beddings for context words. Because of that, we
pre-train attention block containing embeddings,
attention neural network and linear layer weights
by optimizing a negative sampling loss function in
the same manner as the original Skip-gram model:

vwI )

log σ(v(cid:48)T
wO
k
(cid:88)

+

i=1

Ewi∼Pn(w)[log σ(−v(cid:48)T

wivwI )]

(12)

where v(cid:48)

wO , vwI and v(cid:48)

wi are vector representa-
tion of ”positive” example, anchor word and nega-
tive example respectively. Vector vwI is computed
using embedding of wI and attention mechanism
proposed in previous section.

5 Experiments

5.1 Data

We collected new dataset of deﬁnitions using Ox-
fordDictionaries.com (2018) API. Each entry is a
triplet, containing the word, its deﬁnition and ex-
ample of the use of this word in the given meaning.
It is important to note that in our data set words can
have one or more meanings, depending on the cor-
responding entry in the Oxford Dictionary. Table
1 shows basic statistics of the new dataset.

Figure 1: Perplexities of S+I Attention model
for the case of pre-training (solid lines) and for
the case when the model is trained from scratch
(dashed lines).

5.2 Pre-training

It is well-known that good language model can of-
ten improve metrics such as BLEU for a particu-
lar NLP task (Jozefowicz et al., 2016). According
to this, we decided to pre-train our models. For
this purpose, WikiText-103 dataset (Merity et al.,
2016) was chosen. During pre-training we set v∗
(eq. 10) to zero vector to make our models purely
unconditional. Embeddings for these language
models were initialized by Google Word2Vec vec-
tors1 and were ﬁne-tuned. Figure 1 shows that
this procedure helps to decrease perplexity and
prevents over-ﬁtting. Attention Skip-gram vectors
were also trained on the WikiText-103.

5.3 Results

Both our models are LSTM networks (Hochre-
iter and Schmidhuber, 1997) with an embedding
layer. The attention-based model has own em-
bedding layer, mapping context words to vector
representations. Firstly, we pre-train our mod-
els using the procedure, described above. Then,
we train them on the collected dataset maximiz-
ing log-likelihood objective using Adam (Kingma
and Ba, 2014). Also, we anneal learning rate by

1https://code.google.com/archive/p/word2vec/

Word
star

Context
she got star treatment

star

bright star in the sky

sentence
sentence
head
head

reprint

rape

invisible

shake

nickname

sentence in prison
write up the sentence
the head of a man
he will be the head of the ofﬁce
they never reprinted the
famous treatise
the woman was raped on
her way home at night
he pushed the string through
an inconspicuous hole
my faith has been shaken
the nickname for the u.s.
constitution is ‘old ironsides ’

Deﬁnition
a person who is very important
a small circle of a celestial object
or planet that is seen in a circle
an act of restraining someone or something
a piece of text written to be printed
the upper part of a human body
the chief part of an organization, institution, etc
a written or printed version of
a book or other publication

the act of killing

not able to be seen

cause to be unable to think clearly

a name for a person or thing that is not genuine

Table 2: Examples of deﬁnitions generated by S + I-Attention model for the words and contexts from the
test set.

Model
S+G+CH+HE (1)
S+G+CH+HE (2)
S+G+CH+HE (3)
S + I-Adaptive (2)
S + I-Adaptive (3)
S + I-Attention (2)
S + I-Attention (3)

PPL
45.62
46.12
46.80
46.08
46.93
43.54
44.9

BLEU
11.62 ± 0.05
-
-
11.53 ± 0.03
-
12.08 ± 0.02
-

Table 3: Performance comparison between best
model proposed by Noraset et al. (2017) and our
models on the test set. Number in brackets means
number of LSTM layers. BLEU is averaged across
3 trials.

a factor of 10 if validation loss doesn’t decrease
per epochs. We use original Adaptive Skip-gram
vectors as inputs to S+I-Adaptive, which were ob-
tained from the ofﬁcial repository2. We compare
different models using perplexity and BLEU score
on the test set. BLEU score is computed only for
models with the lowest perplexity and only on the
test words that have multiple meanings. The re-
sults are presented in Table 3. We see that both
models that utilize knowledge about meaning of
the word have better performance than the com-
peting one. We generated deﬁnitions using S + I-
Attention model with simple temperature sampling

algorithm (τ = 0.1). Table 2 shows the examples.
The source code and dataset will be freely avail-
able 3.

6 Conclusion

In the paper, we proposed two deﬁnition models
which can work with polysemantic words. We
evaluate them using perplexity and measure the
deﬁnition generation accuracy with BLEU score.
Obtained results show that incorporating informa-
tion about word senses leads to improved met-
rics. Moreover, generated deﬁnitions show that
even implicit word context can help to differ word
meanings. In future work, we plan to explore in-
dividual components of word embedding and the
mask produced by our attention-based model to
get a deeper understanding of vectors representa-
tions of words.

Acknowledgments

This work was partly supported by Samsung Re-
search, Samsung Electronics, Sberbank AI Lab
and the Russian Science Foundation grant 17-71-
20072.

2https://github.com/sbos/AdaGram.jl

3https://github.com/agadetsky/pytorch-deﬁnitions

Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems
21, pages 1081–1088. Curran Associates, Inc.

Thanapon Noraset, Chen Liang, Larry Birnbaum, and
Doug Downey. 2017. Deﬁnition modeling: Learn-
ing to deﬁne word embeddings in natural language.
In 31st AAAI Conference on Artiﬁcial Intelligence,
AAAI 2017, pages 3259–3266. AAAI press.

OxfordDictionaries.com. 2018. Oxford University

Press.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

References

Dzmitry Bahdanau, Tom Bosc, Stanislaw Jastrzebski,
Edward Grefenstette, Pascal Vincent, and Yoshua
Bengio. 2017. Learning to compute word embed-
dings on the ﬂy. arXiv preprint arXiv:1706.00286.

Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin,
and Dmitry Vetrov. 2016. Breaking sticks and ambi-
guities with adaptive skip-gram. In Artiﬁcial Intelli-
gence and Statistics, pages 130–138.

Felix Hill, KyungHyun Cho, Anna Korhonen, and
Yoshua Bengio. 2016.
Learning to understand
phrases by embedding the dictionary. Transactions
of the Association for Computational Linguistics,
4:17–30.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Matthew D. Hoffman, David M. Blei, Chong Wang,
and John Paisley. 2013. Stochastic variational in-
Journal of Machine Learning Research,
ference.
14:1303–1347.

Armand Joulin, Edouard Grave, Piotr Bojanowski,
Matthijs Douze, H´erve J´egou, and Tomas Mikolov.
2016. Fasttext.zip: Compressing text classiﬁcation
models. arXiv preprint arXiv:1612.03651.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Exploring
arXiv preprint

Shazeer, and Yonghui Wu. 2016.
the limits of language modeling.
arXiv:1602.02410.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
the IEEE conference
tions.
on computer vision and pattern recognition, pages
3128–3137.

In Proceedings of

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit
Iyyer, James Bradbury, Ishaan Gulrajani, Victor
Zhong, Romain Paulus, and Richard Socher. 2016.
Ask me anything: Dynamic memory networks for
natural language processing. In Proceedings of The
33rd International Conference on Machine Learn-
ing, volume 48 of Proceedings of Machine Learning
Research, pages 1378–1387. PMLR.

Stephen Merity, Caiming Xiong, James Bradbury, and
Pointer sentinel mixture

Richard Socher. 2016.
models. arXiv preprint arXiv:1609.07843.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
In C. J. C. Burges, L. Bottou, M. Welling,
ality.
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

