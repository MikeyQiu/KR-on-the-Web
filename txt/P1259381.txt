Technical report

Nested Sequential Monte Carlo Methods

Christian A. Naesseth, Fredrik Lindsten and Thomas B. Sch¨on

Please cite this version:

•

Christian A. Naesseth, Fredrik Lindsten and Thomas B. Sch¨on. Nested Sequential
Monte Carlo Methods. In Proceedings of the 32 nd International Conference on Ma-
chine Learning, Lille, France, 2015. JMLR: W&CP volume 37.

Abstract
We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences
of probability distributions, even where the random variables are high-dimensional. NSMC gen-
eralises the SMC framework by requiring only approximate, properly weighted, samples from the
SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC
can in itself be used to produce such properly weighted samples. Consequently, one NSMC sam-
pler can be used to construct an efﬁcient high-dimensional proposal distribution for another NSMC
sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to
consider complex and high-dimensional models using SMC. We show results that motivate the
efﬁcacy of our approach on several ﬁltering problems with dimensions in the order of 100 to 1 000.
Keywords: high-dimensional inference, high-dimensional particle ﬁlter, exact approximation, op-
spatio-temporal models
timal proposal,

sequential Monte Carlo,

importance sampling,

5
1
0
2
 
p
e
S
 
1
1
 
 
]

O
C

.
t
a
t
s
[
 
 
3
v
6
3
5
2
0
.
2
0
5
1
:
v
i
X
r
a

NAESSETH, LINDSTEN AND SCH ¨ON

Nested Sequential Monte Carlo Methods

Christian A. Naesseth
Link¨oping University, Link¨oping, Sweden

CHRISTIAN.A.NAESSETH@LIU.SE

Fredrik Lindsten
The University of Cambridge, Cambridge, United Kingdom

FREDRIK.LINDSTEN@ENG.CAM.AC.UK

Thomas B. Sch¨on
Uppsala University, Uppsala, Sweden

THOMAS.SCHON@IT.UU.SE

Abstract
We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences
of probability distributions, even where the random variables are high-dimensional. NSMC gen-
eralises the SMC framework by requiring only approximate, properly weighted, samples from the
SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC
can in itself be used to produce such properly weighted samples. Consequently, one NSMC sam-
pler can be used to construct an efﬁcient high-dimensional proposal distribution for another NSMC
sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to
consider complex and high-dimensional models using SMC. We show results that motivate the
efﬁcacy of our approach on several ﬁltering problems with dimensions in the order of 100 to 1 000.
Keywords:
high-dimensional inference, high-dimensional particle ﬁlter, exact approximation,
optimal proposal, sequential Monte Carlo, importance sampling, spatio-temporal models

1. Introduction

Inference in complex and high-dimensional statistical models is a very challenging problem that is
ubiquitous in applications. Examples include, but are deﬁnitely not limited to, climate informatics
(Monteleoni et al., 2013), bioinformatics (Cohen, 2004) and machine learning (Wainwright and
Jordan, 2008). In particular, we are interested in sequential Bayesian inference, which involves
computing integrals of the form

¯πk(f ) := E¯πk [f (X1:k)] =

f (x1:k)¯πk(x1:k)dx1:k,

(1)

for some sequence of probability densities

¯πk(x1:k) = Z−1

πk πk(x1:k),
with normalisation constants Zπk = (cid:82) πk(x1:k)dx1:k. Note that x1:k := (x1, . . . , xk)
Xk. The
typical scenario that we consider is the well-known problem of inference in time series or state space
models (Shumway and Stoffer, 2011; Capp´e et al., 2005). Here the index k corresponds to time and
we want to process some observations y1:k in a sequential manner to compute expectations with
respect to the ﬁltering distribution ¯πk(dxk) = P(Xk
y1:k). To be speciﬁc, we are interested
in settings where

dxk

1,

≥

∈

∈

k

|

(2)

(i) Xk is high-dimensional, i.e. Xk

Rd with d

1, and

∈

(cid:29)

(cid:90)

2

NESTED SEQUENTIAL MONTE CARLO METHODS

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

k

1

−

k

· · ·

k + 1

Figure 1: Example of a spatio-temporal model where ¯πk(x1:k) is described by a k
R2×3.

graphical model and xk

×

×

2

3 undirected

∈

(ii) there are local dependencies among the latent variables X1:k, both w.r.t. time k and between

the individual components of the (high-dimensional) vectors Xk.

One example of the type of models we consider are the so-called spatio-temporal models (Wikle,
2015; Cressie and Wikle, 2011; Rue and Held, 2005). In Figure 1 we provide a probabilistic graph-
ical model representation of a spatio-temporal model that we will explore further in Section 6.

Sequential Monte Carlo (SMC) methods, reviewed in Section 2.1, comprise one of the most
successful methodologies for sequential Bayesian inference. However, SMC struggles in high-
dimensions and these methods are rarely used for dimensions, say, d
10 (Rebeschini and van
Handel, 2015). The purpose of the NSMC methodology is to push this limit well beyond d = 10.

≥

The basic strategy, described in Section 2.2, is to mimic the behaviour of a so-called fully
adapted SMC algorithm. Full adaptation can drastically improve the efﬁciency of SMC in high di-
mensions. Unfortunately, it can rarely be implemented in practice since the fully adapted proposal
distributions are typically intractable. NSMC addresses this difﬁculty by requiring only approxi-
mate, properly weighted, samples from the proposal distribution. The proper weighting condition
ensures the validity of NSMC, thus providing a generalisation of the family of SMC methods. Fur-
thermore, NSMC will itself produce properly weighted samples. Consequently, it is possible to
use one NSMC procedure within another to construct efﬁcient high-dimensional proposal distribu-
tions. This nesting of the algorithm can be done to an arbitrary degree. For instance, for the model
depicted in Figure 1 we could use three nested samplers, one for each dimension of the “volume”.
The main methodological development is concentrated to Sections 3–4. We introduce the con-
cept of proper weighting, approximations of the proposal distribution, and nesting of Monte Carlo
algorithms. Throughout Section 3 we consider simple importance sampling and in Section 4 we
extend the development to the sequential setting.

We deliberately defer the discussion of the existing body of related work until Section 5, to open
up for a better understanding of the relationships to the new developments presented in Sections 3–
4. We also discuss various attractive features of NSMC that are of interest in high-dimensional
settings, e.g. the fact that it is easy to distribute the computation, which results in improved memory
efﬁciency and lower communication costs. Section 6 proﬁles our method extensively with a state-of-
the-art competing algorithm on several high-dimensional data sets. We also show the performance

3

NAESSETH, LINDSTEN AND SCH ¨ON

of inference and the modularity of the method on a d = 1 056 dimensional climatological spatio-
temporal model (Fu et al., 2012) structured according to Figure 1. Finally, in Section 7 we conclude
the paper with some ﬁnal remarks.

2. Background and Inference Strategy

2.1 Sequential Monte Carlo

Evaluating ¯πk(f ) as well as the normalisation constant Zπk in (2) is typically intractable and we
need to resort to approximations. SMC methods, or particle ﬁlters (PF), constitute a popular class
of numerical approximations for sequential inference problems. Here we give a high-level intro-
duction to the concepts underlying SMC methods, and postpone the details to Section 4. For a more
extensive treatment we refer to Doucet and Johansen (2011); Capp´e et al. (2005); Doucet et al.
(2001). In particular, we will use the auxiliary SMC method as proposed by Pitt and Shephard
(1999).

At iteration k

−

1, the SMC sampler approximates the target distribution ¯πk−1 by a collection of
N
i=1. These samples deﬁne an empirical point-mass

1:k−1, W i

(X i

weighted particles (samples)
approximation of the target distribution

{

k−1)
}

¯πN
k−1(dx1:k−1) :=

N
(cid:88)

i=1

W i
k−1
(cid:96) W (cid:96)

(cid:80)

k−1

δX i

1:k−1

(dx1:k−1),

(3)

where δX (dx) denotes a Dirac measure at X. Each iteration of the SMC algorithm can then con-
ceptually be described by three steps, resampling, propagation, and weighting.

The resampling step puts emphasis on the most promising particles by discarding the unlikely
ones and duplicating the likely ones. The propagation and weighting steps essentially correspond to
using importance sampling when changing the target distribution from ¯πk−1 to ¯πk, i.e. simulating
new particles from a proposal distribution and then computing corresponding importance weights.

2.2 Adapting the Proposal Distribution

The ﬁrst working SMC algorithm was the bootstrap PF by Gordon et al. (1993), which propagates
particles by sampling from the system dynamics and computes importance weights according to the
observation likelihood (in the state space setting). However, it is well known that the bootstrap PF
suffers from weight collapse in high-dimensional settings (Bickel et al., 2008), i.e. the estimate is
dominated by a single particle with weight close to one. This is an effect of the mismatch between
the importance sampling proposal and the target distribution, which typically gets more pronounced
in high dimensions.

More efﬁcient proposals, partially alleviating the degeneracy issue for some models, can be de-
signed by adapting the proposal distribution to the target distribution (see Section 4.2). In Naesseth
et al. (2014a) we make use of the fully adapted SMC method (Pitt and Shephard, 1999) for doing
inference in a (fairly) high-dimensional discrete model where xk is a 60-dimensional discrete vec-
tor. We can then make use of forward ﬁltering and backward simulation, operating on the individual
components of each xk, in order to sample from the fully adapted SMC proposals. However, this
method is limited to models where the latent space is either discrete or Gaussian and the optimal
proposal can be identiﬁed with a tree-structured graphical model. Our development here can be

4

NESTED SEQUENTIAL MONTE CARLO METHODS

seen as a non-trivial extension of this technique. Instead of coupling one SMC sampler with an
exact forward ﬁlter/backward simulator (which in fact reduces to an instance of standard SMC),
we derive a way of coupling multiple SMC samplers and SMC-based backward simulators. This
allows us to construct procedures for mimicking the efﬁcient fully adapted proposals for arbitrary
latent spaces and structures in high-dimensional models.

3. Proper Weighting and Nested Importance Sampling

In this section we will lay the groundwork for the derivation of the class of NSMC algorithms.
We start by considering the simpler case of importance sampling (IS), which is a fundamental
component of SMC, and introduce the key concepts that we make use of. In particular, we will use
a (slightly nonstandard) presentation of an algorithm as an instance of a class, in the object-oriented
sense, and show that these classes can be nested to an arbitrary degree.

3.1 Exact Approximation of the Proposal Distribution

i=1 W i)−1 (cid:80)N

i=1 W if (X i), with W i = Zqπ(X i)

Let ¯π(x) = Z−1
π π(x) be a target distribution of interest. IS can be used to estimate an expectation
¯π(f ) := E¯π[f (X)] by sampling from a proposal distribution ¯q(x) = Z−1
q q(x) and computing the
estimator ((cid:80)N
N
i=1 are the
, and where
}
weighted samples. It is possible to replace the IS weight by a nonnegative unbiased estimate, and
still obtain a valid (consistent, etc.) algorithm (Liu, 2001, p. 37). One way to motivate this approach
is by considering the random weight to be an auxiliary variable and to extend the target distribution
accordingly. Our development is in the same ﬂavour, but we will use a more explicit condition on
the relationship between the random weights and the simulated particles. Speciﬁcally, we will make
use of the following key property to formally justify the proposed algorithms.

(X i, W i)

q(X i)

{

Deﬁnition 1 (Properly weighted sample). A (random) pair (X, W ) is properly weighted for an
0 and E[f (X)W ] = p(f ) := (cid:82) f (x)p(x)dx for all measurable
unnormalised distribution p if W
functions f .

≥

{

≡

(cid:80)N

(cid:104) 1
N

(X i, W i)

i=1 W i(cid:105)

= (cid:82) p(x)dx =: Zp.

Note that proper weighting of

malising constant of p. Indeed, taking f (x)

N
i=1 implies unbiasedness of the estimate of the nor-
}
1 gives E
Interestingly, to construct a valid IS algorithm for our target ¯π it is sufﬁcient to generate samples
that are properly weighted w.r.t. the proposal distribution q. To formalise this claim, assume that we
are not able to simulate exactly from ¯q, but that it is possible to evaluate the unnormalised density
q point-wise. Furthermore, assume we have access to a class Q, which works as follows. The
constructor of Q requires the speciﬁcation of an unnormalised density function, say, q, which will
be approximated by the procedures of Q. Furthermore, to highlight the fact that we will typically use
IS (and SMC) to construct Q, the constructor also takes as an argument a precision parameter M ,
corresponding to the number of samples used by the “internal” Monte Carlo procedure. An object
is then instantiated as q = Q(q, M ). The class Q is assumed to have the following properties:

(A1) Let q = Q(q, M ). Assume that:

1. The construction of q results in the generation of a (possibly random) member variable, ac-
cessible as (cid:98)Zq = q.GetZ(). The variable (cid:98)Zq is a nonnegative, unbiased estimate of the nor-
malising constant Zq = (cid:82) q(x)dx.

5

NAESSETH, LINDSTEN AND SCH ¨ON

2. Q has a member function Simulate which returns a (possibly random) variable X = q.Simulate(),

such that (X, (cid:98)Zq) is properly weighted for q.

With the deﬁnition of Q in place, it is possible to generalise1 the basic importance sampler as in
N
(X i, W i)
Algorithm 1, which generates weighted samples
i=1 targeting ¯π. Note that Algorithm 1
}
{
is different from a random weight IS, since it approximates the proposal distribution (and not just
the importance weights).

Algorithm 1 Nested IS (steps 1–3 for i = 1, . . . , N )

1. Initialise qi = Q(q, M ).

2. Set (cid:98)Zi

q = qi.GetZ() and X i = qi.Simulate().

3. Set W i =

qπ(X i)
(cid:98)Zi
q(X i)

.

4. Compute (cid:98)Zπ = 1
N

(cid:80)N

i=1 W i.

To see the validity of Algorithm 1 we can interpret the sampler as a standard IS algorithm for
an extended target distribution, deﬁned as ¯Π(x, u) := u ¯Q(x, u)¯π(x)q−1(x), where ¯Q(x, u) is the
joint PDF of the random pair (q.Simulate(), q.GetZ()). Note that ¯Π is indeed a PDF that admits ¯π
as a marginal; for any measurable subset A

X,

⊆

¯Π(A

R+) =

1A(x)

¯Q(x, u)dxdu = E

(cid:90)

u ¯π(x)
q(x)

×

(cid:20)

(cid:98)Zq

(cid:21)

1A(X)¯π(X)
q(X)

(cid:18)

= ¯q

1A

(cid:19)

¯π
q

Zq = ¯π(A),

where the penultimate equality follows from the fact that (X, (cid:98)Zq) is properly weighted for q. Fur-
thermore, the standard unnormalised IS weight for a sampler with target ¯Π and proposal ¯Q is given
by u π/q, in agreement with Algorithm 1.

Algorithm 1 is an example of what is referred to as an exact approximation; see e.g., Andrieu and
Roberts (2009); Andrieu et al. (2010). Algorithmically, the method appears to be an approximation
of an IS, but samples generated by the algorithm nevertheless target the correct distribution ¯π.

3.2 Modularity of Nested IS

To be able to implement Algorithm 1 we need to deﬁne a class Q with the required properties
(A1). The modularity of the procedure (as well as its name) comes from the fact that we can use
Algorithm 1 also in this respect. Indeed, let us now view ¯π—the target distribution of Algorithm 1—
as the proposal distribution for another Nested IS procedure and consider the following deﬁnition
of Q:

1. Algorithm 1 is executed at the construction of the object p = Q(π, N ), and p.GetZ() returns

the normalising constant estimate (cid:98)Zπ.

2. p.Simulate() simulates a categorical random variable B with P(B = i) = W i/ (cid:80)N

(cid:96)=1 W (cid:96)

and returns X B.

1. With q.GetZ() (cid:55)→ Z and q.Simulate() returning a sample from ¯q we obtain the standard IS method.

6

NESTED SEQUENTIAL MONTE CARLO METHODS

Now, for any measurable f we have,

E[f (X B) (cid:98)Zπ] =

E

f (X i) (cid:98)Zπ

(cid:20)

N
(cid:88)

i=1

(cid:21)

=

W i
N (cid:98)Zπ

1
N

N
(cid:88)

i=1

(cid:34)
f (X i)

E

(cid:35)

qπ(X i)
(cid:98)Zi
q(X i)

= ¯q

(cid:19)

(cid:18) f π
q

Zq = ¯π(f )Zπ,

where, again, we use the fact that (X i, (cid:98)Zi
is properly weighted for π and that our deﬁnition of Q(π, N ) indeed satisﬁes condition (A1).

q) is properly weighted for q. This implies that (X B, (cid:98)Zπ)

The Nested IS algorithm in itself is unlikely to be of direct practical interest. However, in the
next section we will, essentially, repeat the preceding derivation in the context of SMC to develop
the NSMC method.

4. Nested Sequential Monte Carlo

4.1 Fully Adapted SMC Samplers

Let us return to the sequential inference problem. As before, let ¯πk(x1:k) = Z−1
πk πk(x1:k) denote
the target distribution at “time” k. The unnormalised density πk can be evaluated point-wise, but
the normalising constant Zπk is typically unknown. We will use SMC to simulate sequentially
n
k=1. In particular, we consider the fully adapted SMC sampler (Pitt
from the distributions
and Shephard, 1999), which corresponds to a speciﬁc choice of resampling weights and proposal
distribution, chosen in such a way that the importance weights are all equal to 1/N . Speciﬁcally,
the proposal distribution (often referred to as the optimal proposal) is given by ¯qk(xk
x1:k−1) =
Zqk (x1:k−1)−1qk(xk

x1:k−1), where

¯πk

{

}

|

|

qk(xk

x1:k−1) :=

|

πk(x1:k)
πk−1(x1:k−1)

.

In addition, the normalising “constant” Zqk (x1:k−1) = (cid:82) qk(xk
the resampling weights, i.e. the particles at time k
before they are propagated to time k. For notational simplicity, we use the convention x1:0 =
q1(x1
|
Algorithm 2.

x1:k−1)dxk is further used to deﬁne
1 are resampled according to Zqk (x1:k−1)
,
∅
x1:0) = π1(x1) and Zq1(x1:0) = Zπ1. The fully adapted auxiliary SMC sampler is given in

−

|

k}

As mentioned above, at each iteration k = 1, . . . , n, the method produces unweighted samples
N
X i
i=1 approximating ¯πk. It also produces an unbiased estimate (cid:98)Zπk of Zπk (Del Moral, 2004,
{
Proposition 7.4.1). The algorithm is expressed in a slightly non-standard form; at iteration k we
loop over the ancestor particles, i.e. the particles after resampling at iteration k
1, and let each
ancestor particle j generate mj
k offsprings. (The variable L is just for bookkeeping.) This is done
to clarify the connection with the NSMC procedure below. Furthermore, we have included a (com-
N
pletely superﬂuous) resampling step at iteration k = 1, where the “dummy variables”
i=1
{
N
i=1. The analogue of
are resampled according to the (all equal) weights
this step is, however, used in the NSMC algorithm, where the initial normalising constant Zπ1 is
estimated. We thus have to resample the corresponding initial particle systems accordingly.

Zπ1}
{

Zq1(X i

N
i=1 =

1:0)
}

1:0}

X i

−

{

7

NAESSETH, LINDSTEN AND SCH ¨ON

Algorithm 2 SMC (fully adapted)

1. Set (cid:98)Zπ0 = 1.

2. for k = 1 to n

(a) Compute (cid:98)Zπk = (cid:98)Zπk−1 ×
(b) Draw m1:N
k
1, . . . , N .

(c) Set L

0

←

(d) for j = 1 to N
i. Draw X i

ii. Set L

¯qk(
k ∼
· |
L + mj
k.

←

1
N

(cid:80)N

j=1 Zqk (X j

1:k−1).

from a multinomial distribution with probabilities

1:k−1)

Zqk (X j
(cid:96)=1 Zqk (X (cid:96)

1:k−1)

(cid:80)N

, for j =

X j

1:k−1) and let X i

1:k = (X j

1:k−1, X i

k) for i = L+1, . . . , L+mj
k.

4.2 Fully Adapted Nested SMC Samplers

In analogue with Section 3, assume now that we are not able to simulate exactly from ¯qk, nor
compute Zqk . Instead, we have access to a class Q which satisﬁes condition (A1). The proposed
NSMC method is then given by Algorithm 3.

Algorithm 3 Nested SMC (fully adapted)

1. Set (cid:98)Zπ0 = 1.

2. for k = 1 to n

1:k−1), M ) for j = 1, . . . , N .

X j

· |
qk = qj.GetZ() for j = 1, . . . , N .
j=1 (cid:98)Zj
qk

(a) Initialise qj = Q(qk(
(b) Set (cid:98)Zj
(c) Compute (cid:98)Zπk = (cid:98)Zπk−1 ×
(d) Draw m1:N

(cid:110) 1
N

(cid:80)N

(cid:111)

.

from a multinomial distribution with probabilities

for j =

(cid:98)Zj
qk
(cid:96)=1 (cid:98)Z(cid:96)
qk

(cid:80)N

i. Compute X i

k = qj.Simulate() and let X i

1:k = (X j

1:k−1, X i

k) for i = L+1, . . . , L+

k
1, . . . , N .

(e) Set L

0

←

(f) for j = 1 to N

mj
k.
ii. delete qj.
iii. Set L

←

L + mj
k.

8

NESTED SEQUENTIAL MONTE CARLO METHODS

Algorithm 3 can be seen as an exact approximation of the fully adapted SMC sampler in Al-
gorithm 2.
(In Appendix A.1 we provide a formulation of NSMC with arbitrary proposals and
resampling weights.) We replace the exact computation of Zqk and exact simulation from ¯qk, by the
approximate procedures available through Q. Despite this approximation, however, Algorithm 3 is
a valid SMC method. This is formalised by the following theorem.

Theorem 2. Assume that Q satisﬁes condition (A1). Then, under certain regularity conditions on
the function f : Xk
k (f ), both speciﬁed in Appendix A.1.2,
we have

Rd and for an asymptotic variance ΣM

(cid:55)→

N 1/2

(cid:32)

1
N

N
(cid:88)

i=1

(cid:33)

D

f (X i

1:k)

¯πk(f )

−

−→ N

(0, ΣM

k (f )),

where

X i

{

1:k}

M

i=1 are generated by Algorithm 3 and D
−→

denotes convergence in distribution.

Proof See Appendix A.1.2.

Remark 3. The key point with Theorem 2 is that, under certain regularity conditions, the NSMC
method converges at rate √N even for a ﬁxed (and ﬁnite) value of the precision parameter M . The
asymptotic variance ΣM
k (f ), however, will depend on the accuracy and properties of the approxi-
mative procedures of Q. We leave it as future work to establish more informative results, relating
the asymptotic variance of NSMC to that of the ideal, fully adapted SMC sampler.

4.3 Backward Simulation and Modularity of NSMC

As previously mentioned, the NSMC procedure is modular in the sense that we can make use of
Algorithm 3 also to deﬁne the class Q. Thus, we now view ¯πn as the proposal distribution that we
wish to approximately sample from using NSMC. Algorithm 3 directly generates an estimate (cid:98)Zπn
of the normalising constant of πn (which indeed is unbiased, see Theorem 6). However, we also
need to generate a sample (cid:101)X1:n such that ( (cid:101)X1:n, (cid:98)Zπn) is properly weighted for πn.

1, . . . , N

The simplest approach, akin to the Nested IS procedure described in Section 3.2, is to draw Bn
1:n . This will indeed result in a valid deﬁnition
uniformly on
of the Simulate procedure. However, this approach will suffer from the well known path degen-
eracy of SMC samplers. In particular, since we call qj.Simulate() multiple times in Step 2(f)i of
Algorithm 3, we risk to obtain (very) strongly correlated samples by this simple approach.

and return (cid:101)X1:n = X Bn

}

{

It is possible to improve the performance of the above procedure by instead making use of
a backward simulator (Godsill et al., 2004; Lindsten and Sch¨on, 2013) to simulate (cid:101)X1:n. The
backward simulator, given in Algorithm 4, is a type of smoothing algorithm; it makes use of the
particles generated by a forward pass of Algorithm 3 to simulate backward in “time” a trajectory
(cid:101)X1:n approximately distributed according to ¯πn.

Remark 4. Algorithm 4 assumes unweighted particles and can thus be used in conjunction with the
fully adapted NSMC procedure of Algorithm 2. If, however, the forward ﬁlter is not fully adapted
the weights need to be accounted for in the backward simulation; see Appendix A.1.3.

The modularity of NSMC is established by the following result.

9

NAESSETH, LINDSTEN AND SCH ¨ON

Algorithm 4 Backward simulator (fully adapted)

1. Draw Bn uniformly on

1, . . . , N

{

.

}

2. Set (cid:101)Xn = X Bn
n .

3. for k = n

1 to 1

−
(a) Compute (cid:102)W j

k =

πn((X j

1:k, (cid:101)Xk+1:n))
πk(X j
1:k)

for j = 1, . . . , N .

(b) Draw Bk from a categorical distribution with probabilities

for j = 1, . . . , N .

(cid:102)W j
k
(cid:96)=1 (cid:102)W (cid:96)
k

(cid:80)N

(c) Set (cid:101)Xk:n = (X Bk

k , (cid:101)Xk+1:n).

Deﬁnition 5. Let p = Q(πn, N ) be deﬁned as follows:

1. The constructor executes Algorithm 3 with target distribution πn and with N particles, and

p.GetZ() returns the estimate of the normalising constant (cid:98)Zπn.

2. p.Simulate() executes Algorithm 4 and returns (cid:101)X1:n.

Theorem 6. The class Q deﬁned as in Deﬁnition 5 satisﬁes condition (A1).

Proof See Appendix A.1.3.

A direct, and important, consequence of Theorem 6 is that NSMC can be used as a component of
powerful learning algorithms, such as the particle Markov chain Monte Carlo (PMCMC) method
(Andrieu et al., 2010) and many of the other methods discussed in Section 5. Since standard SMC
is a special case of NSMC, Theorem 6 implies proper weighting also of SMC.

5. Practicalities and Related Work

There has been much recent interest in using SMC within SMC in various ways. The SMC2 by
Chopin et al. (2013) and the recent method by Crisan and M´ıguez (2013) are sequential learning
algorithms for state space models, where one SMC sampler for the parameters is coupled with an-
other SMC sampler for the latent states. Johansen et al. (2012) and Chen et al. (2011) address the
state inference problem by splitting the state variable into different components and run coupled
SMC samplers for these components. These methods differ substantially from NSMC; they solve
different problems and the “internal” SMC sampler(s) is constructed in a different way (for approxi-
mate marginalisation instead of for approximate simulation). Another related method is the random
weights PF of Fearnhead et al. (2010a), requiring exact samples from ¯q and where the importance
weights are estimated using a nested Monte Carlo algorithm.

The method most closely related to NSMC is the space-time particle ﬁlter (ST-PF) (Beskos
et al., 2014a), which has been developed independently and in parallel with our work. The ST-PF
is also designed for solving inference problems in high-dimensional models. It can be seen as a
island PF (Verg´e et al., 2015) implementation of the method presented by Naesseth et al. (2014b).
Speciﬁcally, for a spatio-temporal models they run an island PF over both spatial and temporal

10

NESTED SEQUENTIAL MONTE CARLO METHODS

dimensions. However, the ST-PF does not generate an approximation of the fully adapted SMC
sampler.

Another key distinction between NSMC and ST-PF is that in the latter each particle in the
“outer” SMC sampler comprises a complete particle system from the “inner” SMC sampler. For
NSMC, on the other hand, the particles will simply correspond to different hypotheses about the
latent variables (as in standard SMC), regardless of how many samplers that are nested. This is
a key feature of NSMC, since it implies that it is easily distributed over the particles. The main
N
j=1 and the calls to the Simulate
computational effort of Algorithm 3 is the construction of
}
procedure, which can be done independently for each particle. This leads to improved memory
efﬁciency and lower communication costs. Furthermore, we have found (see Section 6) that NSMC
can outperform ST-PF even when run on a single machine with matched computational costs.

qj

{

Another strength of NSMC methods are their relative ease of implementation, which we show
in Section 6.3. We use the framework to sample from what is essentially a cubic grid Markov ran-
dom ﬁeld (MRF) model just by implementing three nested samplers, each with a target distribution
deﬁned on a simple chain.

There are also other SMC-based methods designed for high-dimensional problems, e.g., the
block PF studied by Rebeschini and van Handel (2015), the location particle smoother by Briggs
et al. (2013) and the PF-based methods reviewed in Djuric and Bugallo (2013). However, these
methods are all inconsistent, as they are based on various approximations that result in systematic
errors.

The previously mentioned PMCMC (Andrieu et al., 2010) is a related method, where SMC
is used as a component of an MCMC algorithm. We make use of a very similar extended space
approach to motivate the validity of our algorithm. Note that our proposed algorithm can be used as
a component in PMCMC and most of the other algorithms mentioned above, which further increases
the scope of models it can handle.

6. Experimental Results

We illustrate NSMC on three high-dimensional examples, both with real and synthetic data. We
compare NSMC with standard (bootstrap) PF and the ST-PF of Beskos et al. (2014a) with equal
computational budgets on a single machine (i.e., neglecting the fact that NSMC is more easily dis-
tributed). These methods are, to the best of our knowledge, the only other available consistent online
methods for full Bayesian inference in general sequential models. For more detailed explanations
of the models and additional results, see Appendix A.32.

6.1 Gaussian State Space Model

=

Xk, Yk
{

We start by considering a high-dimensional Gaussian state space model, where we have access to
the true solution through belief propagation. The latent variables and measurements
,
}
d
with
k lattice Gaussian MRF. The true data is
l=1, are modeled by a d
simulated from a nearly identical state space model (see Appendix A.3.1). We run a 2-level NSMC
sampler. The outer level is fully adapted, i.e. the proposal distribution is qk = p(xk
xk−1, yk),
which thus constitute the target distribution for the inner level. To generate properly weighted
samples from qk, we use a bootstrap PF operating on the d components of the vector xk. Note that

Xk,l, Yk,l
{

X1:k, Y1:k

×

}

}

{

|

2. Code available at https://github.com/can-cs/nestedsmc

11

NAESSETH, LINDSTEN AND SCH ¨ON

d = 50

d = 100

d = 200

S
S
E

S
R
E

Figure 2: Top: Median (over dimension) ESS (4) and 15–85% percentiles (shaded region). Bottom:
The ERS (5) based on the resampling weights in the (outermost) particle ﬁlter. The results
are based on 100 independent runs for the Gaussian MRF with dimension d.

we only use bootstrap proposals where the actual sampling takes place, and that the conditional
distribution p(xk

xk−1, yk) is not explicitly used.

We simulate data from this model for k = 1, . . . , 100 for different values of d = dim(xk)

50, 100, 200
{
with both the ST-PF and standard (bootstrap) PF.

∈
. The exact ﬁltering marginals are computed using belief propagation.We compare
}

|

The results are evaluated based on the effective sample size (ESS, see e.g. Fearnhead et al.

(2010b)) deﬁned as,

ESS(xk,l) =

(cid:18)

(cid:20)

E

((cid:98)xk,l−µk,l)2
σ2

k,l

(cid:21)(cid:19)−1

,

|

where (cid:98)xk,l denote the mean estimates and µk,l and σ2
k,l denote the true mean and variance of
y1:k obtained from belief propagation. The expectation in (4) is approximated by averag-
xk,l
ing over 100 independent runs of the involved algorithms. The ESS reﬂects the estimator accuracy,
obvious by the deﬁnition which is tightly related to the mean-squared-error. Intuitively the ESS
corresponds to the equivalent number of i.i.d. samples needed for the same accuracy.

We also consider the effective resample size (ERS, Kong et al. (1994)), which is based on the

resampling weights at the top levels in the respective SMC algorithms,

(4)

(5)

The ERS is an estimate of the effective number of unique particles (or particle systems in the case
of ST-PF) available at each resampling step.

We use N = 500 and M = 2

d for NSMC and match the computational time for ST-PF
and bootstrap PF. We report the results in Figure 2. The bootstrap PF is omitted from d = 100,

·

ERS =

(cid:16)(cid:80)N

i=1 (cid:98)Zi
qk
(cid:16)
(cid:98)Zi
qk

i=1

(cid:80)N

(cid:17)2
(cid:17)2 .

12

NESTED SEQUENTIAL MONTE CARLO METHODS

200 due to its poor performance already for d = 50 (which is to be expected). Each dimension
l = 1, . . . , d provides us with a value of the ESS, so we present the median (lines) and 15–85%
percentiles (shaded regions) in the ﬁrst row of Figure 2. The ERS is displayed in the second row of
Figure 2. Note that ESS gives a better reﬂection of estimation accuracy than ERS.

We have conducted additional experiments with different model parameters and different choices
for N and M (some additional results are given in Appendix A.3.1). Overall the results seem to be
in agreement with the ones presented here, however ST-PF seems to be more robust to the trade-off
between N and M . A rule-of-thumb for NSMC is to generally try to keep N as high as possible,
while still maintaining a reasonably large ERS.

6.2 Non-Gaussian State Space Model

Next, we consider an example with a non-Gaussian SSM, borrowed from Beskos et al. (2014a)
where the full details of the model are given. The transition probability p(xk
xk−1) is a localised
|
xk) is t-distributed. The model dimension
Gaussian mixture and the measurement probability p(yk
is d = 1 024. Beskos et al. (2014a) report improvements for ST-PF over both the bootstrap PF
and the block PF by Rebeschini and van Handel (2015). We use N = M = 100 for both ST-PF
and NSMC (the special structure of this model implies that there is no signiﬁcant computational
overhead from running backward sampling) and the bootstrap PF is given N = 10 000. In Figure 3

|

Figure 3: Median ESS with 15

85% percentiles (shaded region) for the non-Gaussian SSM.

−

we report the ESS (4), estimated according to Carpenter et al. (1999). The ESS for the bootstrap PF
is close to 0, for ST-PF around 1–2, and for NSMC slightly higher at 7–8. However, we note that all
methods perform quite poorly on this model, and to obtain satisfactory results it would be necessary
to use more particles.

6.3 Spatio-Temporal Model – Drought Detection

In this ﬁnal example we study the problem of detecting droughts based on measured precipitation
data (Jones and Harris, 2013) for different locations on earth. We look at the situation in North
America during the years 1901–1950 and the Sahel region in Africa during the years 1950–2000.
These spatial regions and time frames were chosen since they include two of the most devastating
droughts during the last century, the so-called Dust Bowl in the US during the 1930s (Schubert

13

et al., 2004) and the decades long drought in the Sahel region in Africa starting in the 1960s (Foley
et al., 2003; Hoerling et al., 2006). We consider the spatio-temporal model deﬁned by Fu et al.

NAESSETH, LINDSTEN AND SCH ¨ON

Xk−1

· · ·

Xk+1

· · ·

Xk,1:2,1

Xk,1:2,3

N
→

Xk

M1
→
Xk,1:2,2

↓ M2

↓ M2

↓ M2

Figure 4: Illustration of the three-level NSMC.

{

Xk,i,j

(2012) and compare with the results therein. Each location in a region is modelled to be in either
a normal state 0 or in an abnormal state 1 (drought). Measurements are given by precipitation (in
millimeters) for each location and year. At every time instance k our latent structure is described by
I,J
a rectangular 2D grid Xk =
i=1,j=1; in essence this is the model showcased in Figure 1. Fu
et al. (2012) considers the problem of ﬁnding the maximum aposteriori conﬁguration, using a linear
programming relaxation. We will instead compute an approximation of the full posterior ﬁltering
distribution ¯πk(xk) = p(xk
y1:k). The rectangular structure is used to instantiate an NSMC method
that on the ﬁrst level targets the full posterior ﬁltering distribution. To sample from Xk we run, on
the second level, an NSMC procedure that operates on the “columns” Xk,1:I,j, j = 1, . . . , J.
Finally, to sample each column Xk,1:I,j we run a third level of SMC, that operates on the individual
components Xk,i,j, i = 1, . . . , I, using a bootstrap proposal. The structure of our NSMC method
applied to this particular problem is illustrated in Figure 4.

}

|

0.5, 0.7, 0.9
{

Figure 5 gives the results on the parts of North America that we consider. The ﬁrst row shows the
, for both regions.
number of locations where the estimate of p(xk,i,j = 1) exceeds
}
These results seems to be in agreement with Fu et al. (2012, Figures 3, 6). However, we also
receive an approximation of the full posterior and can visualise uncertainty in our estimates, as
illustrated by the three different levels of posterior probability for drought. In general, we obtain
a rich sample diversity from the posterior distribution. However, for some problematic years the
sampler degenerates, with the result that the three credibility levels all coincide. This is also visible
in the second row of Figure 5, where we show the posterior estimates p(xk,i,j
y1:k) for the years
1939–1941, overlayed on the regions of interest. For year 1940 the sampler degenerates and only
reports 0-1 probabilities for all sites. Naturally, one way to improve the estimates is to run the
sampler with a larger number of particles, which has been kept very low in this proof-of-concept.

|

7. Conclusions

We have shown that a straightforward NSMC implementation with fairly few particles can attain
reasonable approximations to the ﬁltering problem for dimensions in the order of hundreds, or
even thousands. This means that NSMC methods takes the SMC framework an important step
closer to being viable for high-dimensional statistical inference problems. However, NSMC is not

14

NESTED SEQUENTIAL MONTE CARLO METHODS

North America region

Sahel region

North America 1939

North America 1940

North America 1941

Figure 5: Top: Number of locations with estimated p(x = 1) >

for the two regions.
Bottom: Estimate of p(xt,i = 1) for all sites over a span of 3 years. All results for
N = 100, N1 =

0.5, 0.7, 0.9
}

, N2 = 20.

{

30, 40
}

{

a silver bullet for solving high-dimensional inference problems, and the approximation accuracy
will be highly model dependent. Hence, much work remains to be done, for instance on combining
NSMC with other techniques for high-dimensional inference such as localisation (Rebeschini and
van Handel, 2015) and annealing (Beskos et al., 2014b), in order to solve even more challenging
problems.

Acknowledgments

This work was supported by the projects: Learning of complex dynamical systems (Contract num-
ber: 637-2014-466) and Probabilistic modeling of dynamical systems (Contract number: 621-2013-
5524), both funded by the Swedish Research Council.

15

NAESSETH, LINDSTEN AND SCH ¨ON

Appendix A. Appendix

In this appendix we start out in Section A.1 by providing a more general formulation of the NSMC
method and proofs of the central limit and proper weighting theorems of the main manuscript.
We also detail (Section A.2) a straightforward extension of nested IS to a sequential version. We
show that a special case of this nested sequential IS turns out to be more or less equivalent to the
importance sampling squared algorithm by Tran et al. (2013). This relationship serves as evidence
that illustrates that the NSMC framework being more widely applicable than the scope of problems
considered in this article. Finally, in Section A.3 we give more details and results on the experiments
considered in the main manuscript.

A.1 Nested Sequential Monte Carlo

We start by presenting a general formulation of a nested auxiliary SMC sampler in Algorithm 5. In
this formulation, qk(xk

x1:k−1) is an arbitrary (unnormalised) proposal, normalised by

|

(cid:90)

Zqk (x1:k−1) =

qk(xk

x1:k−1)dxk.

|

Furthermore, the resampling weights are obtain by multiplying the importance weights with the
arbitrary adjustment multipliers νk−1(x1:k−1, Zqk ), which may depend on both the state sequence
x1:k−1 and the normalising constant (estimate). The fully adapted NSMC sampler (Algorithm 3 in
the main document) is obtained as a special case if we choose

qk(xk

x1:k−1) =

|

πk(x1:k)
πk−1(x1:k−1)

and νk−1(x1:k−1, Zqk ) = Zqk , in which case the importance weights are indeed given by W i

1.

k ≡

A.1.1 NESTED SMC IS SMC

The validity of Algorithm 5 can be established by interpreting the algorithm as a standard SMC
procedure for a sequence of extended target distributions.
If (cid:98)Zqk is computed deterministically,
proper weighting (i.e., unbiasedness) ensures that (cid:98)Zqk = Zqk and it is evident that the algorithm
reduces to a standard SMC sampler. Hence, we consider the case when the normalising constant
estimates (cid:98)Zqk are random.

k−1(uk−1

For k = 1, . . . , n + 1, let us introduce the random variable Uk−1 which encodes the complete
internal state of the object q generated by q = Q(qk(
x1:k−1), M ). Let the distribution of Uk−1 be
denoted as ¯ψM
x1:k−1). To put Algorithm 5 into a standard (auxiliary) SMC framework,
we shall interpret steps 2a–2b of Algorithm 5 as being the last two steps carried out during iteration
1, rather than the ﬁrst two steps carried out during iteration k. This does not alter the algorithm
k
per se, but it results in that the resampling step is conducted ﬁrst at each iteration, which is typically
the case for standard auxiliary SMC formulations.

−

· |

|

The estimator of the normalising constant is computable from the internal state of q, so that we
can introduce a function τk such that (cid:98)Zqk = τk(Uk−1). Furthermore, note that the simulation of Xk
via Xk = q.Simulate() is based solely on the internal state Uk−1, and denote by ¯γM
Uk−1) the
distribution of Xk.

k (xk

|

16

NESTED SEQUENTIAL MONTE CARLO METHODS

Algorithm 5 Nested SMC (auxiliary SMC formulation)

i=1 to arbitrary dummy variables. Set W i
N

0 = 1 for i = 1, . . . , N . Set (cid:98)Zπ0 = 1.

1. Set

X i
0}
{
2. for k = 1 to n

(a) Initialise qj = Q(qk(
(b) Compute (cid:98)Zj
(c) Compute (cid:98)νj

X j

· |

qk = qj.GetZ() for j = 1, . . . , N .
1:k−1, (cid:98)Zj
k−1 = νk−1(X j

qk ) for j = 1, . . . , N .

1:k−1), M ) for j = 1, . . . , N .

from a multinomial distribution with probabilities

k−1W j
(cid:98)νj
(cid:80)N
(cid:96)=1 (cid:98)ν(cid:96)

k−1
k−1W (cid:96)

k−1

for

(d) Draw m1:N

k

j = 1, . . . , N .

(e) Set L

0

←

(f) for j = 1 to N

mj
k.

ii. Compute W i

k =

iii. delete qj.
iv. Set L

L + mj
k.

←
(g) Compute (cid:98)Zπk = (cid:98)Zπk−1 ×

(cid:110) 1
N

i. Compute X i

k = qj.Simulate() and let X i

1:k = (X j

1:k−1, X i

k) for i = L+1, . . . , L+

πk(X i
πk−1(X j

1:k)
1:k−1)

(cid:98)Zj
qk
(cid:98)νj
k−1qk(X i
k |

X j

1:k−1)

for i = L + 1, . . . , L + mj
k.

(cid:80)N

j=1 (cid:98)νj

k−1W j

k−1

(cid:111)

(cid:110)
((cid:80)N

×

j=1 W j

k )/((cid:80)N

j=1 W j

k−1)

(cid:111)

.

Lemma 7. Assume that Q satisﬁes condition (A1) in the main manuscript. Then,

(cid:90)

τk(uk−1)¯γM

k (xk

uk−1) ¯ψM

k−1(uk−1

x1:k−1)duk−1 = qk(xk

x1:k−1).

|

Proof The pair (Xk, τk(Uk−1)) are properly weighted for qk. Hence, for a measurable function f ,

E[f (Xk)τk(Uk−1)

x1:k−1] =

|

f (xk)τk(uk−1)¯γM
(cid:90)

k (xk

|

uk−1) ¯ψM

k−1(uk−1
(cid:90)

|

x1:k−1)duk−1dxk

= Zk(x1:k−1)

f (xk)¯qk(xk

x1:k−1)dxk =

f (xk)qk(xk

x1:k−1)dxk.

|

Since f is arbitrary, the result follows.

|

(cid:90) (cid:90)

|

|

We can now deﬁne the sequence of (unnormalised) extended target distributions for the Nested

SMC sampler as,

Πk(x1:k, u0:k) :=

τk(uk−1) ¯ψM

k (uk
|
qk(xk
|

x1:k)¯γM
x1:k−1)

k (xk

uk−1)

|

πk(x1:k)
πk−1(x1:k−1)

Πk−1(x1:k−1, u0:k−1),

and Π0(u0) = ¯ψM

0 (u0). We write Θk = Xk

Uk for the domain of Πk.

×

17

NAESSETH, LINDSTEN AND SCH ¨ON

|

|

Lemma 8. Assume that Q satisﬁes condition (A1) in the main manuscript. Then,

(cid:90)

τk(uk−1)¯γM

k (xk

|

uk−1)Πk−1(x1:k−1, u0:k−1)du0:k−1 = πk−1(x1:k−1)qk(xk

x1:k−1).

Proof The proof follows by induction. At k = 1, we have (cid:82) τ1(u0)¯γM
q1(x1) by Lemma 7. Hence, assume that the hypothesis holds for k

1 (x1

|

1 and consider

u0) ¯ψM

0 (u0)du0 =

≥

uk)τk(uk−1) ¯ψM

k (uk

x1:k)¯γM

uk−1)Πk−1(x1:k−1, u0:k−1)du0:k

(cid:90)

τk+1(uk)¯γM

k+1(xk+1

|
πk(x1:k)
πk−1(x1:k−1)qk(xk

uk)Πk(x1:k, u0:k)du0:k

=

(cid:90)

·

·

=

(cid:90)

=

x1:k−1)

|
k+1(xk+1

τk+1(uk)¯γM
πk(x1:k) (cid:0)(cid:82) τk+1(uk)¯γM

|

k+1(xk+1
|
πk−1(x1:k−1)qk(xk

k (xk
(cid:1)

|

x1:k)duk

|
k (uk

uk) ¯ψM
x1:k−1)

|

|

uk−1)Πk−1(x1:k−1, u0:k−1)du0:k−1

τk(uk−1)¯γM

k (xk

|
πk(x1:k)qk+1(xk+1

|

x1:k)πk−1(x1:k−1)qk(xk

x1:k−1)

|

= πk(x1:k)qk+1(xk+1

x1:k),

πk−1(x1:k−1)qk(xk

x1:k−1)

|

where the penultimate equality follows by applying Lemma 7 and the induction hypothesis to the
two integrals, respectively.

As a corollary to Lemma 8, it follows that

(cid:90)

Πk(x1:k, u0:k)du0:k = πk(x1:k).

(6)

Consequently, Πk is normalised by the same constant Zπk as πk, and by deﬁning ¯Πk(x1:k, u0:k) :=
Z−1
πk Πk(x1:k, u0:k) we obtain a probability distribution which admits ¯πk as a marginal (note that
¯Π0 = Π0, which is normalised by construction). This implies that we can use ¯Πk as a proxy
for ¯πk in a Monte Carlo algorithm, i.e., samples drawn from ¯Πk can be used to compute ex-
pectations w.r.t. ¯πk. This is precisely what Algorithm 5 does; it is a standard auxiliary SMC
sampler for the (unnormalised) target sequence Πk, k = 0, . . . , n, with adjustment multiplier
weights νk−1(x1:k−1, τk(uk−1)) and proposal distribution ¯γM
x1:k). The (stan-
dard) weight function for this sampler is thus given by

uk−1) ¯ψM

k (xk

k (uk

|

|

Wk(x1:k, u0:k)

πk(x1:k)
πk−1(x1:k−1)

∝

τk(uk−1)

,

(7)

νk−1(x1:k−1, τk(uk−1))qk(xk

x1:k−1)

|

which is the same as the expression on line 2(f)ii of Algorithm 5.

A.1.2 CENTRAL LIMIT THEOREM – PROOF OF THEOREM 2 IN THE MAIN MANUSCRIPT

Now that we have established that Nested SMC is in fact a standard auxiliary SMC sampler, albeit
on an extended state space, we can reuse existing convergence results from the SMC literature; see

18

NESTED SEQUENTIAL MONTE CARLO METHODS

e.g., Johansen and Doucet (2008); Douc and Moulines (2008); Douc et al. (2009); Chopin (2004)
or the extensive textbook by Del Moral (2004).

Here, in order to prove Theorem 2 of the main manuscript, we make use of the result for the
auxiliary SMC sampler by Johansen and Doucet (2008), which in turn is based on the central limit
theorem by Chopin (2004). The technique used by Johansen and Doucet (2008) is to reinterpret (as
detailed below) the auxiliary SMC sampler as a sequential importance sampling and resampling
(SISR) particle ﬁlter, by introducing the modiﬁed (unnormalised) target distribution

Π(cid:48)

k(x1:k, u0:k) := νk(x1:k, τk+1(uk))Πk(x1:k, u0:k).

(8)

The auxiliary SMC sampler described in the previous section can then be viewed as a SISR algo-
rithm for (8). Indeed, if we write ¯QM
uk−1) for
the joint proposal distribution of (xk, uk), then the weight function for this SISR sampler is given
by

x1:k−1, uk−1) := ¯ψM

k (xk, uk

x1:k)¯γM

k (xk

k (uk

|

|

|

W (cid:48)

k(x1:k, u0:k) :=

¯Π(cid:48)

k(x1:k, u0:k)

¯QM

k (xk, uk

x1:k−1, uk−1) ¯Π(cid:48)

k−1(x1:k−1, u0:k−1)

νk(x1:k, τk+1(uk))Wk(x1:k, u0:k),

∝

(9)

where Wk is deﬁned in (7). This weight expression thus accounts for both the importance weights
and the adjustment multipliers of the auxiliary SMC sampler formulation.

Since this SISR algorithm does not target ¯Πk (and thus not ¯πk) directly, we use an additional IS
step to compute estimators of expectations w.r.t. to ¯π. The proposal distribution for this IS procedure
is given by

¯Γk(x1:k, u0:k) := ¯QM

k (xk, uk

x1:k−1, uk−1) ¯Π(cid:48)

k−1(x1:k−1, u0:k−1).

(10)

Note that we obtain an approximation of (10) after the propagation Step 2(f)i of Algorithm 5, but
before the weighting step. The resulting IS weights, for target distribution ¯Πk(x1:k, u0:k) and with
proposal distribution (10), are given by

|

|

¯Πk(x1:k, u0:k)
¯Γk(x1:k, u0:k)

=: ωk(x1:k, u0:k)

Wk(x1:k, u0:k).

∝

Hence, with f : Xk
(cid:55)→
obvious abuse of notation) by the estimator

Rd being a test function of interest we can estimate E¯πk [f ] = E ¯Πk [f ] (with

N
(cid:88)

i=1

kf (X i
W i
1:k)
(cid:80)N
(cid:96)=1 W (cid:96)
k

,

(11)

which, again, is in agreement with Algorithm 5.

We have now reinterpreted the NSMC algorithm; ﬁrst as a standard auxiliary SMC sampler,
and then further as a standard SISR method. Consequently, we are now in the position of directly
applying, e.g., the central limit theorem by Chopin (2004, Theorem 1). The conditions and the
statement of the theorem are reproduced here for clarity.

19

NAESSETH, LINDSTEN AND SCH ¨ON

For any measurable function f : Θ0

measurable function f : Θk

Rd,

Rd, let (cid:101)V M

0 (f ) = Var ¯ψM

0

(cid:55)→

(f ) and deﬁne, for any

(cid:55)→
(cid:101)V M
k (f ) = (cid:98)V M
k−1(E ¯QM
V M
k (W (cid:48)
k (f ) = (cid:101)V M
(cid:98)V M
k (f ) = V M

k(f

k

[f ]) + E ¯Π(cid:48)
E ¯Π(cid:48)

[f ])),

k−1

−

k
(f ),

k (f ) + Var ¯Π(cid:48)

k

[Var ¯QM

k

(f )],

k > 0,

0,

0.

k

k

≥

≥

Deﬁne recursively Φk to be the set of measurable functions f : Θk
δ > 0 with E¯Γk [
2+δ] <
(cid:107)
(cid:107)
Φk−1. Furthermore, assume that the identity function f
≡
follows by Chopin (2004, Theorem 1 and Lemma A.1) that

W (cid:48)

kf

∞

and such that the function (x1:k−1, u0:k−1)

Rd such that there exists a
E ¯QM
kf ] is in
(cid:55)→
1 belongs to Φk for each k. Then, it

[W (cid:48)

(cid:55)→

k

N 1/2

(cid:32) N
(cid:88)

i=1

1
N

f (X i

1:k, U i

0:k)

E¯Γk [f ])

−

−→ N

(0, (cid:101)V M

k (f )),

(12)

(cid:33)

D

for any function f such that the function (x1:k−1, u0:k−1)
exists a δ > 0 such that E¯Γk [
(cid:107)
samples obtained after the propagation Step 2(f)i of Algorithm 5, but before the weighting step.

E¯Γk [f ]] is in Φk−1 and there
. The convergence in (12) thus holds for the unweighted

2+δ] <
(cid:107)

E ¯QM

∞

(cid:55)→

−

[f

f

k

To complete the proof, it remains to translate (12) into a similar result for the IS estimator (11).
To this end we make use of Chopin (2004, Lemma A.2) which is related to the IS correction step
Rd denote the
of the SMC algorithm. Speciﬁcally, for a function f : Xk
Rd such
extension of f to Θk, deﬁned by f e(x1:k, u0:k) = f (x1:k). Then, for any f : Xk
E ¯QM
[ωkf e] is in Φk−1 and there exists a δ > 0 such that
that the function (x1:k−1, u0:k−1)
E¯Γk [
2+δ] <
ωkf e
(cid:107)

Rd, let f e : Θk

, we have

(cid:55)→

(cid:55)→

(cid:55)→

(cid:55)→

∞

(cid:107)

k

N 1/2

(cid:32) N
(cid:88)

i=1

kf (X i
W i
1:k)
(cid:80)N
(cid:96)=1 W (cid:96)
k −

(cid:33)

D

¯πk(f )

−→ N

(0, ΣM

k (f )),

where

(X i
{

1:k, W i
k)

i=1 are generated by Algorithm 5 and ΣM
M
}

k (f ) = (cid:101)V M

k (ωk(f e

E ¯Πk [f e])).

−

A.1.3 NESTED SMC GENERATES PROPERLY WEIGHTED SAMPLES – PROOF OF THEOREM 6

IN THE MAIN MANUSCRIPT

In the previous two sections we showed that the NSMC procedure is a valid inference algorithm
for ¯πn. Next, we turn our attention to the modularity of the method and the validity of using the
algorithm as a component in another NSMC sampler. Let us start by stating a more general version
of the backward simulator in Algorithm 6. Clearly, if the forward NSMC procedure is fully adapted
W i

1, Algorithm 6 reduces to the backward simulator stated in the main manuscript.

k ≡
We will now show that the pair ( (cid:98)Zπn, (cid:101)X1:n) generated by Algorithms 5 and 6 is properly

weighted for πn(x1:n), and thereby prove Theorem 6 in the main manuscript.

The proof is based on the particle Markov chain Monte Carlo (PMCMC) construction (Andrieu
et al., 2010). The idea used by Andrieu et al. (2010) was to construct an extended target distribution,
incorporating all the random variables generated by an SMC sampler as auxiliary variables. This
opened up for using SMC approximations within MCMC in a provably correct way; these seem-
ingly approximate methods simply correspond to standard MCMC samplers for the (nonstandard)

20

NESTED SEQUENTIAL MONTE CARLO METHODS

Algorithm 6 Backward simulator

1. Draw Bn from a categorical distribution with probabilities

for j = 1, . . . , N .

2. Set (cid:101)Xn = X Bn
n .

3. for k = n

1 to 1

−
(a) Compute (cid:102)W j

k = W j

k

πn((X j

1:k, (cid:101)Xk+1:n))
πk(X j
1:k)

for j = 1, . . . , N .

(b) Draw Bk from a categorical distribution with probabilities

for j = 1, . . . , N .

W j
n
(cid:96)=1 W (cid:96)
n

(cid:80)N

(cid:102)W j
k
(cid:96)=1 (cid:102)W (cid:96)
k

(cid:80)N

(c) Set (cid:101)Xk:n = (X Bk

k , (cid:101)Xk+1:n).

4. return (cid:101)X1:n

extended target distribution. Here we will use the same technique to prove the proper weighing
property of the NSMC procedure.

We start by introducing some additional notation for the auxiliary variables of the extended
target construction. While Algorithm 5 is expressed using multinomial random variables m1:N
in the resampling step, it is more convenient for the sake of the proof to explicitly introduce the
i=1; see e.g., Andrieu et al. (2010). That is, Ai
N
ancestor indices
k is a categorical random
Ai
1 of particle X i
k
k. The
variable on
1:k−1 is ancestor particle at iteration k
, such that X
resampling Step 2d of Algorithm 5 can then equivalently be expressed as: simulate independently
Ai

N
i=1 from the categorical distribution with probabilities

k}
{
1, . . . , N
}
{

Ai

−

k

{

k}

k−1W j
(cid:98)νj
(cid:80)N
(cid:96)=1 (cid:98)ν(cid:96)

k−1
k−1W (cid:96)

k−1

.

{

X 1

k , . . . , X N
k }

Let Xk =
, denote all the
particles, internal states of the proposals, and ancestor indices, respectively, generated at iteration k
of the NSMC algorithm. We can then write down the joint distribution of all the random variables
generated in executing Algorithm 5 (up to an irrelevant permutation of the particle indices) as,

k , . . . , U N
U 1
k }
{

k, . . . , AN
k }

, and Ak =

, Uk =

A1
{

¯ΨNSMC(x1:n, u0:n, a1:n) =

¯ψM
0 (ui
0)

(cid:40) N
(cid:89)

i=1

(cid:41) n
(cid:89)




N
(cid:89)

k=1



i=1

ai
k
(cid:98)ν
k−1W
(cid:80)N
(cid:96)=1 (cid:98)ν(cid:96)

ai
k
k−1
k−1W (cid:96)

k−1

¯QM

k (xi

k, ui
k |

ai
k
1:k−1, u

ai
k
k−1)

x






,

(13)

where we interpret (cid:98)νi

k and W i

k as deterministic functions of (xi

1:k, ui

0:k).

21

NAESSETH, LINDSTEN AND SCH ¨ON

Let Bn denote a random variable deﬁned on

. The extended target distribution for

1, . . . , N
{

}

PMCMC samplers corresponding to (13) is then given by

¯Φ(x1:n, u0:n, a1:n, bn) :=

¯ΨNSMC(x1:n, u0:n, a1:n),

(14)

(cid:98)Zπn
Zπn

W bn
n
(cid:96)=1 W (cid:96)
n

(cid:80)N

where (cid:98)Zπn is a deterministic function of (x1:n, u0:n, a1:n). We know from Andrieu et al. (2010)
that ¯Φ is a probability distribution which admits ¯Πn as its marginal distribution for (X bn
0:n).
Consequently, by (6) it follows that the marginal distribution of X bn
1:n is ¯πn. For later reference we
deﬁne recursively bk−1 := abk
k for k = 1, . . . , n, the particle indices for the trajectory obtained by
tracing backward the genealogy of the bn’th particle at iteration n.

1:n, U bn

We now turn our attention to the backward simulator in Algorithm 6. Backward simulation
has indeed been used in the context of PMCMC, see e.g. Whiteley (2010); Lindsten and Sch¨on
(2013); Lindsten et al. (2014). The strategy used for combining PMCMC with backward simulation
is to show that each step of the backward sampler corresponds to a partially collapsed Gibbs sam-
pling step for the extended target distribution ¯Φ. This implies that the backward sampler leaves ¯Φ
invariant.

We use the same approach here, but we need to be careful in how we apply the existing results,
since the PMCMC distribution ¯Φ is deﬁned w.r.t. to ¯Πn, whereas the backward simulator of Algo-
rithm 6 works with the original target distribution ¯πn. Nevertheless, from the proof of Lemma 1 by
Lindsten et al. (2014) it follows that we can write the following collapsed conditional distribution
of ¯Φ as:

¯Φ(bk, ubk:n
k:n |

x1:k, u0:k−1, a1:k, x

Πn(

W bk
k

∝

bk+1:n
k+1:n , bk+1:n)
bk+1:n
xbk
1:k, x
,
k+1:n }
{
{
bk
a
0:k−1, ubk
Πk(xbk
k
u
1:k,
k }
{

)

bk
a
0:k−1, ubk:n
k
u
k:n }

)

k (ubk
¯ψM
k |

xbk
1:k).

(15)

To simplify this expression, consider,

Πn(x1:n, u0:n)
Πk(x1:k, u0:k)

n
(cid:89)

(cid:26) τs(us−1) ¯ψM

=

=

s=k+1
¯ψM
n (un
¯ψM
k (uk

x1:n)
x1:k)

|
|

s (us
|
qs(xs
|

x1:s)¯γM
x1:s−1)

(cid:40) n
(cid:89)

τs(us−1) ¯ψM

s=k+1

s (xs

us−1)

|

(cid:27)

πs(x1:s)
πs−1(x1:s−1)

s−1(us−1
qs(xs

x1:s−1)¯γM
x1:s−1)

s (xs

us−1)

|

πn(x1:n)
πk(x1:k)

.

(cid:41)

|
|

(16)

By Lemma 7 we know that each factor of the product (in brackets) on the second line integrates to
1 over us−1. Hence, plugging (16) into (15) and integrating over ubk:n

k:n yields

¯Φ(bk

|

x1:k, u0:k−1, a1:k, x

bk+1:n
k+1:n , bk+1:n)

πn(
{

W bk
k

∝

xbk
1:k, x
πk(xbk

bk+1:n
)
k+1:n }
1:k)

,

which coincides with the expression used to simulate the index Bk in Algorithm 6. Hence, sim-
ulation of Bk indeed corresponds to a partially collapsed Gibbs sampling step for ¯Φ and it will
thus leave ¯Φ invariant. (Note that, in comparison with the PMCMC sampler derived by Lindsten

22

NESTED SEQUENTIAL MONTE CARLO METHODS

et al. (2014) we further marginalise over the variables ubk:n
partially collapsed Gibbs step.)

k:n which, however, still results in a valid

We now have all the components needed to prove proper weighting of the combined NSMC/backward

simulation procedure. For notational simplicity, we write

¯ΨBS,k(bk) = ¯Φ(bk

x1:k, u0:k−1, a1:k, x

bk+1:n
k+1:n , bk+1:n),

|

for the distribution of Bk in Algorithm 6. Let ( (cid:98)Zπn, (cid:101)X1:n) be generated by Algorithms 5 and 6. Let
f be a measurable function and consider

E[ (cid:98)Zπnf ( (cid:101)X1:n)] =

(cid:90)

(cid:98)Zπnf (X

b(cid:48)
1:n
1:n )

¯ΨBS,k(db(cid:48)
k)

¯ΨNSMC(d(x1:n, u0:n, a1:n))

(cid:41)

(cid:41)

(cid:40) n
(cid:89)

k=1
(cid:40)n−1
(cid:89)

k=1

= Zπn

(cid:90)

f (X

b(cid:48)
1:n
1:n )

¯ΨBS,k(db(cid:48)
k)

¯Φ(d(x1:n, u0:n, a1:n, b(cid:48)

n)),

where, for the second equality, we have used the deﬁnition (14) and noted that ¯ΨBS,n(bn) =

W bn
n
(cid:96)=1 W (cid:96)
n

(cid:80)N

. However, by the invariance of ¯ΨBS,k w.r.t. ¯Φ, it follows that

E[ (cid:98)Zπnf ( (cid:101)X1:n)] = Zπn

f (X b1:n

1:n ) ¯Φ(d(x1:n, u0:n, a1:n, bn)) = Zπn ¯πn(f ),

(cid:90)

which completes the proof.

A.2 Nested Sequential Importance Sampling

Here we give the deﬁnition of the nested sequential importance sampler and we show that a special
case of this is the importance sampling squared (IS2) method by Tran et al. (2013).

A.2.1 NESTED SEQUENTIAL IMPORTANCE SAMPLING

We present a straightforward extension of the Nested IS class to a sequential IS version. Consider
the following deﬁnition of the Nested SIS Q:

1. Algorithm 7 is executed at the construction of the object p = Q(πn, N ), and p.GetZ() returns

the normalising constant estimate (cid:98)Zπn.

2. p.Simulate() simulates a categorical random variable B with P(B = i) = W i

n/ (cid:80)N

(cid:96)=1 W (cid:96)
n

and returns X B

1:n.

Note that we do not require that the procedure Q is identical for each individual proposal qk,
thus we have a ﬂexibility in designing our algorithm as can be seen in the example in Section A.2.2.
We can motivate the algorithm in the same way as for Nested IS and similar theoretical results hold,
i.e. Nested SIS is properly weighted for πn and it admits ¯πn as a marginal.

A.2.2 RELATION TO IS2

Here we will show how IS2, proposed by Tran et al. (2013), can be viewed as a special case of
Nested SIS. We are interested in approximating the posterior distribution of parameters θ given

23

NAESSETH, LINDSTEN AND SCH ¨ON

Algorithm 7 Nested SIS (all i for 1, . . . , N )

1. Initialise qi = Q(q1(
·

), M ).

2. Set (cid:98)Zi

q1 = qi.GetZ(), X i

1 = qi.Simulate().

q1π1(X i
(cid:98)Zi
1)
q1(X i
1)

.

3. Set W i

1 =

4. delete qi.

5. for k = 2 to n:

· |

(a) Initialise qi = Q(qk(
(b) Set (cid:98)Zi

X i
1:k−1), M ).
k = qi.Simulate().
qk = qi.GetZ(), X i
qk πk(X i
(cid:98)Zi
1:k−1)
k |
X i
qk(X i
k |

X i
1:k−1)

k = W i

(c) Set W i

k−1

.

(d) delete qi.
(e) Set X i

1:k ←
6. Compute (cid:98)Zπn = 1
N

(X i

1:k−1, X i
k)
(cid:80)N

i=1 W i
n.

some observed values y

We assume that the data likelihood p(y
an integral

|

¯π(θ

y)

p(y

θ)p(θ).

∝

|
θ) can, by introducing a latent variable x, be computed as

|

Now, let our target distribution in Nested SIS be ¯π2(θ, x) = ¯π2(x
We set our proposal distributions to be

|

θ)¯π1(θ) = p(y | x,θ)p(x | θ)

p(y | θ)

p(θ).

(cid:90)

p(y

θ) =

p(y

x, θ)p(x

θ) dx.

|

|

|

¯q1(θ) = gIS(θ),

¯q2(x

θ) =

|

p(y

|

x, θ)p(x
θ)
p(y

|

θ)

.

|
First, Q(q1(
), 1) runs an exact sampler from the proposal gIS. Then at iteration k = 2 we let the
·
nested procedure Q(q2(
y, θ), giving us
properly weighted samples for q2. Putting all this together gives us samples θi distributed according
to gIS(θ) and weighted by

θi), M ) be a standard IS algorithm with proposal h(x

· |

|

W i

2 ∝

p(θi)
gIS(θi) ·

p(y

xi, θi)p(xi

|

(cid:80)M

θi) 1
(cid:96)=1
M
xi, θi)p(xi

|
p(y

|

θi)

|

p(y | x(cid:96),θi)p(x(cid:96) | θi)
h(x(cid:96) | y,θi)

= (cid:98)pM (y

θi)p(θi)

|
gIS(θi)

,

(17)

24

NESTED SEQUENTIAL MONTE CARLO METHODS

θi) = M −1 (cid:80)M
where (cid:98)pM (y
(cid:96)=1
identical to the IS2 algorithm proposed by Tran et al. (2013).

p(y | x(cid:96),θi)p(x(cid:96) | θi)
h(x(cid:96) | y,θi)

|

. Thus we obtain a Nested SIS method that is

A.3 Further Details on the Experiments

We provide some further details and results for the experiments presented in the main manuscript.

A.3.1 GAUSSIAN STATE SPACE MODEL

We generate data from a synthetic d-dimensional (dim(xk) = d) dynamical/spatio-temporal3 model
deﬁned by

xk

xk−1

|
yk

∼ N

∼ N

xk

|

(xk; µk(xk−1), Σ),
(yk; xk, τ −1

φ I),

where Σ and µk are given as follows

Σ =

τρ + τψ
τψ













−
0
...
...
0
0

τψ
−
τρ + 2τψ
. . .
. . .
...
0
0

0
τψ
−
. . .
. . .
. . .
0
0

· · ·
0
. . .
. . .
. . .
0
0

0
0

0

· · ·
· · ·
. . .
. . .
. . .
−
τψ τρ + 2τψ
−
0

τψ

τψ

0

−

0
0

0

0

−1













,

0
τψ
−
τρ + τψ

µk(xk−1) = aτρΣxk−1.

Alternatively, in a more standard state space model notation, we have

xk = Axk−1 + vk, vk
yk = xk + ek, ek

∼ N
(0, R),

(0, Q),

∼ N

where A = aτρΣ, Q = Σ and R = τ −1
(1, 0.5, 1, 10) are known.

φ I. We assume that the parameters θ = (τψ, a, τρ, τφ) =

To do inference with this generated data-set

we propose to target the following slightly

yk

{

}

different model

p(x1:k, y1:k)

φ(xj, yj)ρ(xj)ψ(xj, xj−1),

k
(cid:89)

j=1

∝

3. Note that in a previous version this was erraneously stated as equivivalent to the Gaussian MRF we use for sequential
inference. Thus this example actually illustrates a problem where we have a misspeciﬁed model. However, this
misspeciﬁcation does not lead to any discernible difference in the MSE results. This because the exact ﬁltering
marginals for the two different models (LGSS, GMRF) with the parameters chosen differs with orders of magnitudes
much lower than the Monte Carlo errors.

25

NAESSETH, LINDSTEN AND SCH ¨ON

where the observation potential φ and interaction potentials ρ and ψ are given by

φ(xk, yk) =

φl(xk,l, yk,l) =

e−

τφ
2 (xk,l−yk,l)2

,

ψ(xk) =

ψl(xk,l, xk,l−1) =

τψ
2 (xk,l−xk,l−1)2

e−

ρ(xk, xk−1) =

ρl(xk,l, xk−1,l) =

τρ
2 (xk,l−axk−1,l)2

e−

,

.

d
(cid:89)

l=1

d
(cid:89)

l=2
d
(cid:89)

l=1

d
(cid:89)

l=1
d
(cid:89)

l=2
d
(cid:89)

l=1

k) lattice MRF, i.e. it grows with “time” k. The
This can be visualised as a Gaussian rectangular (d
×
goal is to estimate the ﬁltering distribution p(xk
y1:k). Note that this model has almost identical
|
ﬁltering marginals as the data generating distribution and leads to a simpler implementation of
NSMC and ST-PF.

Results (mean-squared-error, MSE) comparing NSMC and ST-PF for different settings of N
and M can be found in the ﬁrst row of Figure 6 and the second row displays the results when
comparing ST-PF to the SMC method by Naesseth et al. (2014b) for equal computational budgets.
We show median (over dimensions d) MSE for posterior marginal mean and variance estimates
of the respective algorithms. True values are obtained using belief propagation. Note that setting
N = 1 in ST-PF can be viewed as a special case of the SMC method by Naesseth et al. (2014b).

A.3.2 SPATIO-TEMPORAL MODEL – DROUGHT DETECTION

We present the full model for drought detection in our notation, this is essentially the model by Fu
et al. (2012) adapted for estimating the ﬁltering distribution. The latent variables for each location
on a ﬁnite world grid, xk,i,j, are binary, i.e. 0 being normal state and 1 being the abnormal (drought)
state. Measurements, yk,i,j, are available as real valued precipitation values in millimeters. The
probabilistic model for ﬁltering is given as,

k
(cid:89)

∝

n=1

(cid:40)

1
2σ2
i,j

−

p(x1:k, y1:k)

φ(xn, yn)ρ(xn)ψ(xn, xn−1),

(18a)

where

φ(xk, yk) =

exp

(yk,i,j

µab,i,jxk,i,j

µnorm,i,j(1

−

−

−

(cid:41)

xk,i,j))2

,

(18b)

ρ(xk) =

exp (cid:8)C1

(cid:0)1xk,i,j =xk,i,j−1 + 1xk,i,j =xk,i−1,j

(cid:1)(cid:9) ,

ψ(xk, xk−1) =

exp (cid:8)C21xk,i,j =xk−1,i,j

(cid:9) .

(18c)

(18d)

I
(cid:89)

J
(cid:89)

i=1

j=1

I
(cid:89)

J
(cid:89)

i=1

j=1

I
(cid:89)

J
(cid:89)

i=1

j=1

Here, 1 is the indicator function, and with the convention that all expressions in (18c) that end up
with index 0 evalute to 0. The parameters C1, C2 are set to 0.5, 3 as in (Fu et al., 2012). Location

26

NESTED SEQUENTIAL MONTE CARLO METHODS

Median MSE for E[xk,(cid:96)]

Median MSE for Var(xk,(cid:96))

Median MSE for E[xk,(cid:96)]

Median MSE for Var(xk,(cid:96))

Figure 6: Top: Comparisons for different settings of N and M on the 50-dimensional SSM. Bot-
tom: Illustrating the connection between ST-PF and the SMC method by Naesseth et al.
(2014b).

44 region with latitude 6

55◦N and longitude 90

2012. For the North America region we consider a 20

based parameters σi,j, µab,i,j, µnorm,i,j are estimated based on data from the CRU dataset with world
precipitation data from years 1901
30
120◦W . For the Sahel region we consider a
region with latitude 35
35◦E. Note that for a few locations in
24
Africa (Sahel region) the average yearly precipitation was constant. For these locations we simply
set µnorm,i,j to be this value, µab,i,j = 0 and σ2
i,j to be the mean variance of all locations, thus this
might have introduced some artifacts. Some representative results for the Sahel region are displayed
in Figure 7.

30◦N and longitude 10◦W

×

−

−

−

−

×

−

References

C. Andrieu and G. O. Roberts. The pseudo-marginal approach for efﬁcient Monte Carlo computa-

tions. The Annals of Statistics, 37(2):697–725, 2009.

Christophe Andrieu, Arnaud Doucet, and Roman Holenstein. Particle Markov chain Monte Carlo
methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):
269–342, 2010.

27

NAESSETH, LINDSTEN AND SCH ¨ON

Sahel region 1986

Sahel region 1987

Sahel region 1988

Figure 7: Estimate of P(Xk,i,j = 1

y1:k) for all sites over a span of 3 years. All results for N =

100, N1 =

30, 40
}

{

|
, N2 = 20.

A. Beskos, D. Crisan, A. Jasra, K. Kamatani, and Y. Zhou. A stable particle ﬁlter in high-

dimensions. ArXiv:1412.3501, December 2014a.

Alexandros Beskos, Dan Crisan, and Ajay Jasra. On the stability of sequential Monte Carlo methods

in high dimensions. Ann. Appl. Probab., 24(4):1396–1445, 08 2014b.

Peter Bickel, Bo Li, and Thomas Bengtsson. Sharp failure rates for the bootstrap particle ﬁlter
in high dimensions, volume Volume 3 of Collections, pages 318–329. Institute of Mathematical
Statistics, Beachwood, Ohio, USA, 2008.

Jonathan Briggs, Michael Dowd, and Renate Meyer. Data assimilation for large-scale spatio-

temporal systems using a location particle smoother. Environmetrics, 24(2):81–97, 2013.

Olivier Capp´e, Eric Moulines, and Tobias Ryd´en. Inference in Hidden Markov Models. Springer-

Verlag New York, Inc., Secaucus, NJ, USA, 2005. ISBN 0387402640.

J. Carpenter, P. Clifford, and P. Fearnhead. Improved particle ﬁlter for nonlinear problems. IEE

Proceedings Radar, Sonar and Navigation, 146(1):2–7, 1999.

Tianshi Chen, Thomas B. Sch¨on, Henrik Ohlsson, and Lennart Ljung. Decentralized particle ﬁlter
with arbitrary state decomposition. IEEE Transactions on Signal Processing, 59(2):465–478, Feb
2011.

N. Chopin. Central limit theorem for sequential Monte Carlo methods and its application to

Bayesian inference. The Annals of Statistics, 32(6):2385–2411, 2004.

N. Chopin, P. E. Jacob, and O. Papaspiliopoulos. SMC2: an efﬁcient algorithm for sequential
analysis of state space models. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 75(3):397–426, 2013.

Jacques Cohen. Bioinformaticsan introduction for computer scientists. ACM Computing Surveys

(CSUR), 36(2):122–158, 2004.

N. Cressie and C. K. Wikle. Statistics for spatio-temporal data. Wiley, 2011.

28

NESTED SEQUENTIAL MONTE CARLO METHODS

D. Crisan and J. M´ıguez. Nested particle ﬁlters for online parameter estimation in discrete-time

state-space Markov models. ArXiv:1308.1883, August 2013.

P. Del Moral. Feynman-Kac Formulae - Genealogical and Interacting Particle Systems with Appli-

cations. Probability and its Applications. Springer, 2004.

Petar M Djuric and M´onica F Bugallo. Particle ﬁltering for high-dimensional systems. In Compu-
tational Advances in Multi-Sensor Adaptive Processing (CAMSAP), 2013 IEEE 5th International
Workshop on, pages 352–355. IEEE, 2013.

R. Douc and E. Moulines. Limit theorems for weighted samples with applications to sequential

Monte Carlo. The Annals of Statistics, 36(5):2344–2376, 2008.

R. Douc, E. Moulines, and J. Olsson. Optimality of the auxiliary particle ﬁlter. Probability and

Mathematical Statistics, 29:1–28, 2009.

A. Doucet and A. M. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years later.
In D. Crisan and B. Rozovsky, editors, Nonlinear Filtering Handbook. Oxford University Press,
2011.

Arnaud Doucet, Nando De Freitas, and Neil Gordon. An introduction to sequential Monte Carlo

methods. Springer, 2001.

Paul Fearnhead, Omiros Papaspiliopoulos, Gareth O. Roberts, and Andrew Stuart. Random-weight
particle ﬁltering of continuous time processes. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 72(4):497–512, 2010a.

Paul Fearnhead, David Wyncoll, and Jonathan Tawn. A sequential smoothing algorithm with linear

computational cost. Biometrika, 97(2):447–464, 2010b.

J. A. Foley, M. T. Coe, M. Scheffer, and G. Wang. Regime shifts in the sahara and sahel: Interactions

between ecological and climatic systems in northern africa. Ecosystems, 6:524–539, 2003.

Qiang Fu, Arindam Banerjee, Stefan Liess, and Peter K. Snyder. Drought detection of the last
century: An MRF-based approach. In Proceedings of the 2012 SIAM International Conference
on Data Mining, pages 24–34, Anaheim, CA, USA, April 2012.

S. J. Godsill, A. Doucet, and M. West. Monte Carlo smoothing for nonlinear time series. Journal

of the American Statistical Association, 99(465):156–168, March 2004.

N. J. Gordon, D. J. Salmond, and A. F. M. Smith. Novel approach to nonlinear/non-Gaussian
Bayesian state estimation. Radar and Signal Processing, IEE Proceedings F, 140(2):107 –113,
April 1993.

M. Hoerling, J. Hurrell, J. Eischeid, and A. Phillips. Detection and attribution of twentieth-century

northern and southern african rainfall change. Journal of Climate, 19:3989–4008, 2006.

A. M. Johansen and A. Doucet. A note on auxiliary particle ﬁlters. Statistics & Probability Letters,

78(12):1498–1504, 2008.

29

NAESSETH, LINDSTEN AND SCH ¨ON

A. M. Johansen, N. Whiteley, and A. Doucet. Exact approximation of Rao-Blackwellised particle
ﬁlters. In Proceesings of the 16th IFAC Symposium on System Identiﬁcation (SYSID), pages 488–
493, Brussels, Belgium, 2012.

P.D. Jones and I. Harris. CRU TS3.21: Climatic research unit (CRU) time-series (ts) version 3.21
of high resolution gridded data of month-by-month variation in climate (jan. 1901- dec. 2012).
NCAS British Atmospheric Data Centre, sep 2013. URL http://dx.doi.org/10.5285/
D0E1585D-3417-485F-87AE-4FCECF10A992.

R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Transactions of the

ASME, Journal of Basic Engineering, 82:35–45, 1960.

A. Kong, J. S. Liu, and W. H. Wong. Sequential imputations and Bayesian missing data problems.

Journal of the American Statistical Association, 89(425):278–288, 1994.

F. Lindsten and T. B. Sch¨on. Backward simulation methods for Monte Carlo statistical inference.

Foundations and Trends in Machine Learning, 6(1):1–143, 2013.

F. Lindsten, M. I. Jordan, and T. B. Sch¨on. Particle Gibbs with ancestor sampling. Journal of

Machine Learning Research, 15:2145–2184, 2014.

Jun S Liu. Monte Carlo strategies in scientiﬁc computing. Springer Science & Business Media,

2001.

Claire Monteleoni, Gavin A. Schmidt, Francis Alexander, Alexandru Niculescu-Mizil, Karsten
Steinhaeuser, Michael Tippett, Arindam Banerjee, M. Benno Blumenthal, Jason E. Smerdon Au-
In Ting Yu, Nitesh Chawla, and
roop R. Ganguly, and Marco Tedesco. Climate informatics.
Simeon Simoff, editors, Computational Intelligent Data Analysis for Sustainable Development.
Chapman and Hall/CRC, London, 2013.

Christian A. Naesseth, Fredrik Lindsten, and Thomas B. Sch¨on. Capacity estimation of two-
In The 2014 IEEE Information Theory

dimensional channels using sequential Monte Carlo.
Workshop (ITW), pages 431–435, Nov 2014a.

Christian A. Naesseth, Fredrik Lindsten, and Thomas B Sch¨on. Sequential Monte Carlo for graphi-
cal models. In Advances in Neural Information Processing Systems 27, pages 1862–1870. Curran
Associates, Inc., 2014b.

Michael K Pitt and Neil Shephard. Filtering via simulation: Auxiliary particle ﬁlters. Journal of the

American statistical association, 94(446):590–599, 1999.

P. Rebeschini and R. van Handel. Can local particle ﬁlters beat the curse of dimensionality? Ann.

Appl. Probab. (to appear), 2015.

Raton, FL, USA, 2005.

H. Rue and L. Held. Gaussian Markov Random Fields, Theory and Applications. CDC Press, Boca

S. D. Schubert, M. J. Suarez, P. J. Pegion, R. D. Koster, and J. T. Bacmeister. On the cause of the

1930s dust bowl. Science, 303:1855–1859, 2004.

30

NESTED SEQUENTIAL MONTE CARLO METHODS

R. H. Shumway and D. S. Stoffer. Time Series Analysis and Its Applications – with R examples.

Springer Texts in Statistics. Springer, New York, USA, third edition, 2011.

M.-N. Tran, M. Scharth, M. K. Pitt, and R. Kohn.

Importance sampling squared for Bayesian

inference in latent variable models. ArXiv:1309.3339, sep 2013.

Christelle Verg´e, Cyrille Dubarry, Pierre Del Moral, and Eric Moulines. On parallel implementation
of sequential Monte Carlo methods: the island particle model. Statistics and Computing, 25(2):
243–260, 2015.

Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational

inference. Foundations and Trends R
(cid:13)

in Machine Learning, 1(1-2):1–305, 2008.

N. Whiteley. Discussion on Particle Markov chain Monte Carlo methods. Journal of the Royal

Statistical Society: Series B, 72(3):306–307, 2010.

C. K. Wikle. Modern perspectives on statistics for spatio-temporal data. WIREs Computational

Statistics, 7(1):86–98, 2015.

31

Technical report

Nested Sequential Monte Carlo Methods

Christian A. Naesseth, Fredrik Lindsten and Thomas B. Sch¨on

Please cite this version:

•

Christian A. Naesseth, Fredrik Lindsten and Thomas B. Sch¨on. Nested Sequential
Monte Carlo Methods. In Proceedings of the 32 nd International Conference on Ma-
chine Learning, Lille, France, 2015. JMLR: W&CP volume 37.

Abstract
We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences
of probability distributions, even where the random variables are high-dimensional. NSMC gen-
eralises the SMC framework by requiring only approximate, properly weighted, samples from the
SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC
can in itself be used to produce such properly weighted samples. Consequently, one NSMC sam-
pler can be used to construct an efﬁcient high-dimensional proposal distribution for another NSMC
sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to
consider complex and high-dimensional models using SMC. We show results that motivate the
efﬁcacy of our approach on several ﬁltering problems with dimensions in the order of 100 to 1 000.
Keywords: high-dimensional inference, high-dimensional particle ﬁlter, exact approximation, op-
spatio-temporal models
timal proposal,

sequential Monte Carlo,

importance sampling,

5
1
0
2
 
p
e
S
 
1
1
 
 
]

O
C

.
t
a
t
s
[
 
 
3
v
6
3
5
2
0
.
2
0
5
1
:
v
i
X
r
a

NAESSETH, LINDSTEN AND SCH ¨ON

Nested Sequential Monte Carlo Methods

Christian A. Naesseth
Link¨oping University, Link¨oping, Sweden

CHRISTIAN.A.NAESSETH@LIU.SE

Fredrik Lindsten
The University of Cambridge, Cambridge, United Kingdom

FREDRIK.LINDSTEN@ENG.CAM.AC.UK

Thomas B. Sch¨on
Uppsala University, Uppsala, Sweden

THOMAS.SCHON@IT.UU.SE

Abstract
We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences
of probability distributions, even where the random variables are high-dimensional. NSMC gen-
eralises the SMC framework by requiring only approximate, properly weighted, samples from the
SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC
can in itself be used to produce such properly weighted samples. Consequently, one NSMC sam-
pler can be used to construct an efﬁcient high-dimensional proposal distribution for another NSMC
sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to
consider complex and high-dimensional models using SMC. We show results that motivate the
efﬁcacy of our approach on several ﬁltering problems with dimensions in the order of 100 to 1 000.
Keywords:
high-dimensional inference, high-dimensional particle ﬁlter, exact approximation,
optimal proposal, sequential Monte Carlo, importance sampling, spatio-temporal models

1. Introduction

Inference in complex and high-dimensional statistical models is a very challenging problem that is
ubiquitous in applications. Examples include, but are deﬁnitely not limited to, climate informatics
(Monteleoni et al., 2013), bioinformatics (Cohen, 2004) and machine learning (Wainwright and
Jordan, 2008). In particular, we are interested in sequential Bayesian inference, which involves
computing integrals of the form

¯πk(f ) := E¯πk [f (X1:k)] =

f (x1:k)¯πk(x1:k)dx1:k,

(1)

for some sequence of probability densities

¯πk(x1:k) = Z−1

πk πk(x1:k),
with normalisation constants Zπk = (cid:82) πk(x1:k)dx1:k. Note that x1:k := (x1, . . . , xk)
Xk. The
typical scenario that we consider is the well-known problem of inference in time series or state space
models (Shumway and Stoffer, 2011; Capp´e et al., 2005). Here the index k corresponds to time and
we want to process some observations y1:k in a sequential manner to compute expectations with
respect to the ﬁltering distribution ¯πk(dxk) = P(Xk
y1:k). To be speciﬁc, we are interested
in settings where

dxk

1,

≥

∈

∈

k

|

(2)

(i) Xk is high-dimensional, i.e. Xk

Rd with d

1, and

∈

(cid:29)

(cid:90)

2

NESTED SEQUENTIAL MONTE CARLO METHODS

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

k

1

−

k

· · ·

k + 1

Figure 1: Example of a spatio-temporal model where ¯πk(x1:k) is described by a k
R2×3.

graphical model and xk

×

×

2

3 undirected

∈

(ii) there are local dependencies among the latent variables X1:k, both w.r.t. time k and between

the individual components of the (high-dimensional) vectors Xk.

One example of the type of models we consider are the so-called spatio-temporal models (Wikle,
2015; Cressie and Wikle, 2011; Rue and Held, 2005). In Figure 1 we provide a probabilistic graph-
ical model representation of a spatio-temporal model that we will explore further in Section 6.

Sequential Monte Carlo (SMC) methods, reviewed in Section 2.1, comprise one of the most
successful methodologies for sequential Bayesian inference. However, SMC struggles in high-
dimensions and these methods are rarely used for dimensions, say, d
10 (Rebeschini and van
Handel, 2015). The purpose of the NSMC methodology is to push this limit well beyond d = 10.

≥

The basic strategy, described in Section 2.2, is to mimic the behaviour of a so-called fully
adapted SMC algorithm. Full adaptation can drastically improve the efﬁciency of SMC in high di-
mensions. Unfortunately, it can rarely be implemented in practice since the fully adapted proposal
distributions are typically intractable. NSMC addresses this difﬁculty by requiring only approxi-
mate, properly weighted, samples from the proposal distribution. The proper weighting condition
ensures the validity of NSMC, thus providing a generalisation of the family of SMC methods. Fur-
thermore, NSMC will itself produce properly weighted samples. Consequently, it is possible to
use one NSMC procedure within another to construct efﬁcient high-dimensional proposal distribu-
tions. This nesting of the algorithm can be done to an arbitrary degree. For instance, for the model
depicted in Figure 1 we could use three nested samplers, one for each dimension of the “volume”.
The main methodological development is concentrated to Sections 3–4. We introduce the con-
cept of proper weighting, approximations of the proposal distribution, and nesting of Monte Carlo
algorithms. Throughout Section 3 we consider simple importance sampling and in Section 4 we
extend the development to the sequential setting.

We deliberately defer the discussion of the existing body of related work until Section 5, to open
up for a better understanding of the relationships to the new developments presented in Sections 3–
4. We also discuss various attractive features of NSMC that are of interest in high-dimensional
settings, e.g. the fact that it is easy to distribute the computation, which results in improved memory
efﬁciency and lower communication costs. Section 6 proﬁles our method extensively with a state-of-
the-art competing algorithm on several high-dimensional data sets. We also show the performance

3

NAESSETH, LINDSTEN AND SCH ¨ON

of inference and the modularity of the method on a d = 1 056 dimensional climatological spatio-
temporal model (Fu et al., 2012) structured according to Figure 1. Finally, in Section 7 we conclude
the paper with some ﬁnal remarks.

2. Background and Inference Strategy

2.1 Sequential Monte Carlo

Evaluating ¯πk(f ) as well as the normalisation constant Zπk in (2) is typically intractable and we
need to resort to approximations. SMC methods, or particle ﬁlters (PF), constitute a popular class
of numerical approximations for sequential inference problems. Here we give a high-level intro-
duction to the concepts underlying SMC methods, and postpone the details to Section 4. For a more
extensive treatment we refer to Doucet and Johansen (2011); Capp´e et al. (2005); Doucet et al.
(2001). In particular, we will use the auxiliary SMC method as proposed by Pitt and Shephard
(1999).

At iteration k

−

1, the SMC sampler approximates the target distribution ¯πk−1 by a collection of
N
i=1. These samples deﬁne an empirical point-mass

1:k−1, W i

(X i

weighted particles (samples)
approximation of the target distribution

{

k−1)
}

¯πN
k−1(dx1:k−1) :=

N
(cid:88)

i=1

W i
k−1
(cid:96) W (cid:96)

(cid:80)

k−1

δX i

1:k−1

(dx1:k−1),

(3)

where δX (dx) denotes a Dirac measure at X. Each iteration of the SMC algorithm can then con-
ceptually be described by three steps, resampling, propagation, and weighting.

The resampling step puts emphasis on the most promising particles by discarding the unlikely
ones and duplicating the likely ones. The propagation and weighting steps essentially correspond to
using importance sampling when changing the target distribution from ¯πk−1 to ¯πk, i.e. simulating
new particles from a proposal distribution and then computing corresponding importance weights.

2.2 Adapting the Proposal Distribution

The ﬁrst working SMC algorithm was the bootstrap PF by Gordon et al. (1993), which propagates
particles by sampling from the system dynamics and computes importance weights according to the
observation likelihood (in the state space setting). However, it is well known that the bootstrap PF
suffers from weight collapse in high-dimensional settings (Bickel et al., 2008), i.e. the estimate is
dominated by a single particle with weight close to one. This is an effect of the mismatch between
the importance sampling proposal and the target distribution, which typically gets more pronounced
in high dimensions.

More efﬁcient proposals, partially alleviating the degeneracy issue for some models, can be de-
signed by adapting the proposal distribution to the target distribution (see Section 4.2). In Naesseth
et al. (2014a) we make use of the fully adapted SMC method (Pitt and Shephard, 1999) for doing
inference in a (fairly) high-dimensional discrete model where xk is a 60-dimensional discrete vec-
tor. We can then make use of forward ﬁltering and backward simulation, operating on the individual
components of each xk, in order to sample from the fully adapted SMC proposals. However, this
method is limited to models where the latent space is either discrete or Gaussian and the optimal
proposal can be identiﬁed with a tree-structured graphical model. Our development here can be

4

NESTED SEQUENTIAL MONTE CARLO METHODS

seen as a non-trivial extension of this technique. Instead of coupling one SMC sampler with an
exact forward ﬁlter/backward simulator (which in fact reduces to an instance of standard SMC),
we derive a way of coupling multiple SMC samplers and SMC-based backward simulators. This
allows us to construct procedures for mimicking the efﬁcient fully adapted proposals for arbitrary
latent spaces and structures in high-dimensional models.

3. Proper Weighting and Nested Importance Sampling

In this section we will lay the groundwork for the derivation of the class of NSMC algorithms.
We start by considering the simpler case of importance sampling (IS), which is a fundamental
component of SMC, and introduce the key concepts that we make use of. In particular, we will use
a (slightly nonstandard) presentation of an algorithm as an instance of a class, in the object-oriented
sense, and show that these classes can be nested to an arbitrary degree.

3.1 Exact Approximation of the Proposal Distribution

i=1 W i)−1 (cid:80)N

i=1 W if (X i), with W i = Zqπ(X i)

Let ¯π(x) = Z−1
π π(x) be a target distribution of interest. IS can be used to estimate an expectation
¯π(f ) := E¯π[f (X)] by sampling from a proposal distribution ¯q(x) = Z−1
q q(x) and computing the
estimator ((cid:80)N
N
i=1 are the
, and where
}
weighted samples. It is possible to replace the IS weight by a nonnegative unbiased estimate, and
still obtain a valid (consistent, etc.) algorithm (Liu, 2001, p. 37). One way to motivate this approach
is by considering the random weight to be an auxiliary variable and to extend the target distribution
accordingly. Our development is in the same ﬂavour, but we will use a more explicit condition on
the relationship between the random weights and the simulated particles. Speciﬁcally, we will make
use of the following key property to formally justify the proposed algorithms.

(X i, W i)

q(X i)

{

Deﬁnition 1 (Properly weighted sample). A (random) pair (X, W ) is properly weighted for an
0 and E[f (X)W ] = p(f ) := (cid:82) f (x)p(x)dx for all measurable
unnormalised distribution p if W
functions f .

≥

{

≡

(cid:80)N

(cid:104) 1
N

(X i, W i)

i=1 W i(cid:105)

= (cid:82) p(x)dx =: Zp.

Note that proper weighting of

malising constant of p. Indeed, taking f (x)

N
i=1 implies unbiasedness of the estimate of the nor-
}
1 gives E
Interestingly, to construct a valid IS algorithm for our target ¯π it is sufﬁcient to generate samples
that are properly weighted w.r.t. the proposal distribution q. To formalise this claim, assume that we
are not able to simulate exactly from ¯q, but that it is possible to evaluate the unnormalised density
q point-wise. Furthermore, assume we have access to a class Q, which works as follows. The
constructor of Q requires the speciﬁcation of an unnormalised density function, say, q, which will
be approximated by the procedures of Q. Furthermore, to highlight the fact that we will typically use
IS (and SMC) to construct Q, the constructor also takes as an argument a precision parameter M ,
corresponding to the number of samples used by the “internal” Monte Carlo procedure. An object
is then instantiated as q = Q(q, M ). The class Q is assumed to have the following properties:

(A1) Let q = Q(q, M ). Assume that:

1. The construction of q results in the generation of a (possibly random) member variable, ac-
cessible as (cid:98)Zq = q.GetZ(). The variable (cid:98)Zq is a nonnegative, unbiased estimate of the nor-
malising constant Zq = (cid:82) q(x)dx.

5

NAESSETH, LINDSTEN AND SCH ¨ON

2. Q has a member function Simulate which returns a (possibly random) variable X = q.Simulate(),

such that (X, (cid:98)Zq) is properly weighted for q.

With the deﬁnition of Q in place, it is possible to generalise1 the basic importance sampler as in
N
(X i, W i)
Algorithm 1, which generates weighted samples
i=1 targeting ¯π. Note that Algorithm 1
}
{
is different from a random weight IS, since it approximates the proposal distribution (and not just
the importance weights).

Algorithm 1 Nested IS (steps 1–3 for i = 1, . . . , N )

1. Initialise qi = Q(q, M ).

2. Set (cid:98)Zi

q = qi.GetZ() and X i = qi.Simulate().

3. Set W i =

qπ(X i)
(cid:98)Zi
q(X i)

.

4. Compute (cid:98)Zπ = 1
N

(cid:80)N

i=1 W i.

To see the validity of Algorithm 1 we can interpret the sampler as a standard IS algorithm for
an extended target distribution, deﬁned as ¯Π(x, u) := u ¯Q(x, u)¯π(x)q−1(x), where ¯Q(x, u) is the
joint PDF of the random pair (q.Simulate(), q.GetZ()). Note that ¯Π is indeed a PDF that admits ¯π
as a marginal; for any measurable subset A

X,

⊆

¯Π(A

R+) =

1A(x)

¯Q(x, u)dxdu = E

(cid:90)

u ¯π(x)
q(x)

×

(cid:20)

(cid:98)Zq

(cid:21)

1A(X)¯π(X)
q(X)

(cid:18)

= ¯q

1A

(cid:19)

¯π
q

Zq = ¯π(A),

where the penultimate equality follows from the fact that (X, (cid:98)Zq) is properly weighted for q. Fur-
thermore, the standard unnormalised IS weight for a sampler with target ¯Π and proposal ¯Q is given
by u π/q, in agreement with Algorithm 1.

Algorithm 1 is an example of what is referred to as an exact approximation; see e.g., Andrieu and
Roberts (2009); Andrieu et al. (2010). Algorithmically, the method appears to be an approximation
of an IS, but samples generated by the algorithm nevertheless target the correct distribution ¯π.

3.2 Modularity of Nested IS

To be able to implement Algorithm 1 we need to deﬁne a class Q with the required properties
(A1). The modularity of the procedure (as well as its name) comes from the fact that we can use
Algorithm 1 also in this respect. Indeed, let us now view ¯π—the target distribution of Algorithm 1—
as the proposal distribution for another Nested IS procedure and consider the following deﬁnition
of Q:

1. Algorithm 1 is executed at the construction of the object p = Q(π, N ), and p.GetZ() returns

the normalising constant estimate (cid:98)Zπ.

2. p.Simulate() simulates a categorical random variable B with P(B = i) = W i/ (cid:80)N

(cid:96)=1 W (cid:96)

and returns X B.

1. With q.GetZ() (cid:55)→ Z and q.Simulate() returning a sample from ¯q we obtain the standard IS method.

6

NESTED SEQUENTIAL MONTE CARLO METHODS

Now, for any measurable f we have,

E[f (X B) (cid:98)Zπ] =

E

f (X i) (cid:98)Zπ

(cid:20)

N
(cid:88)

i=1

(cid:21)

=

W i
N (cid:98)Zπ

1
N

N
(cid:88)

i=1

(cid:34)
f (X i)

E

(cid:35)

qπ(X i)
(cid:98)Zi
q(X i)

= ¯q

(cid:19)

(cid:18) f π
q

Zq = ¯π(f )Zπ,

where, again, we use the fact that (X i, (cid:98)Zi
is properly weighted for π and that our deﬁnition of Q(π, N ) indeed satisﬁes condition (A1).

q) is properly weighted for q. This implies that (X B, (cid:98)Zπ)

The Nested IS algorithm in itself is unlikely to be of direct practical interest. However, in the
next section we will, essentially, repeat the preceding derivation in the context of SMC to develop
the NSMC method.

4. Nested Sequential Monte Carlo

4.1 Fully Adapted SMC Samplers

Let us return to the sequential inference problem. As before, let ¯πk(x1:k) = Z−1
πk πk(x1:k) denote
the target distribution at “time” k. The unnormalised density πk can be evaluated point-wise, but
the normalising constant Zπk is typically unknown. We will use SMC to simulate sequentially
n
k=1. In particular, we consider the fully adapted SMC sampler (Pitt
from the distributions
and Shephard, 1999), which corresponds to a speciﬁc choice of resampling weights and proposal
distribution, chosen in such a way that the importance weights are all equal to 1/N . Speciﬁcally,
the proposal distribution (often referred to as the optimal proposal) is given by ¯qk(xk
x1:k−1) =
Zqk (x1:k−1)−1qk(xk

x1:k−1), where

¯πk

{

}

|

|

qk(xk

x1:k−1) :=

|

πk(x1:k)
πk−1(x1:k−1)

.

In addition, the normalising “constant” Zqk (x1:k−1) = (cid:82) qk(xk
the resampling weights, i.e. the particles at time k
before they are propagated to time k. For notational simplicity, we use the convention x1:0 =
q1(x1
|
Algorithm 2.

x1:k−1)dxk is further used to deﬁne
1 are resampled according to Zqk (x1:k−1)
,
∅
x1:0) = π1(x1) and Zq1(x1:0) = Zπ1. The fully adapted auxiliary SMC sampler is given in

−

|

k}

As mentioned above, at each iteration k = 1, . . . , n, the method produces unweighted samples
N
X i
i=1 approximating ¯πk. It also produces an unbiased estimate (cid:98)Zπk of Zπk (Del Moral, 2004,
{
Proposition 7.4.1). The algorithm is expressed in a slightly non-standard form; at iteration k we
loop over the ancestor particles, i.e. the particles after resampling at iteration k
1, and let each
ancestor particle j generate mj
k offsprings. (The variable L is just for bookkeeping.) This is done
to clarify the connection with the NSMC procedure below. Furthermore, we have included a (com-
N
pletely superﬂuous) resampling step at iteration k = 1, where the “dummy variables”
i=1
{
N
i=1. The analogue of
are resampled according to the (all equal) weights
this step is, however, used in the NSMC algorithm, where the initial normalising constant Zπ1 is
estimated. We thus have to resample the corresponding initial particle systems accordingly.

Zπ1}
{

Zq1(X i

N
i=1 =

1:0)
}

1:0}

X i

−

{

7

NAESSETH, LINDSTEN AND SCH ¨ON

Algorithm 2 SMC (fully adapted)

1. Set (cid:98)Zπ0 = 1.

2. for k = 1 to n

(a) Compute (cid:98)Zπk = (cid:98)Zπk−1 ×
(b) Draw m1:N
k
1, . . . , N .

(c) Set L

0

←

(d) for j = 1 to N
i. Draw X i

ii. Set L

¯qk(
k ∼
· |
L + mj
k.

←

1
N

(cid:80)N

j=1 Zqk (X j

1:k−1).

from a multinomial distribution with probabilities

1:k−1)

Zqk (X j
(cid:96)=1 Zqk (X (cid:96)

1:k−1)

(cid:80)N

, for j =

X j

1:k−1) and let X i

1:k = (X j

1:k−1, X i

k) for i = L+1, . . . , L+mj
k.

4.2 Fully Adapted Nested SMC Samplers

In analogue with Section 3, assume now that we are not able to simulate exactly from ¯qk, nor
compute Zqk . Instead, we have access to a class Q which satisﬁes condition (A1). The proposed
NSMC method is then given by Algorithm 3.

Algorithm 3 Nested SMC (fully adapted)

1. Set (cid:98)Zπ0 = 1.

2. for k = 1 to n

1:k−1), M ) for j = 1, . . . , N .

X j

· |
qk = qj.GetZ() for j = 1, . . . , N .
j=1 (cid:98)Zj
qk

(a) Initialise qj = Q(qk(
(b) Set (cid:98)Zj
(c) Compute (cid:98)Zπk = (cid:98)Zπk−1 ×
(d) Draw m1:N

(cid:110) 1
N

(cid:80)N

(cid:111)

.

from a multinomial distribution with probabilities

for j =

(cid:98)Zj
qk
(cid:96)=1 (cid:98)Z(cid:96)
qk

(cid:80)N

i. Compute X i

k = qj.Simulate() and let X i

1:k = (X j

1:k−1, X i

k) for i = L+1, . . . , L+

k
1, . . . , N .

(e) Set L

0

←

(f) for j = 1 to N

mj
k.
ii. delete qj.
iii. Set L

←

L + mj
k.

8

NESTED SEQUENTIAL MONTE CARLO METHODS

Algorithm 3 can be seen as an exact approximation of the fully adapted SMC sampler in Al-
gorithm 2.
(In Appendix A.1 we provide a formulation of NSMC with arbitrary proposals and
resampling weights.) We replace the exact computation of Zqk and exact simulation from ¯qk, by the
approximate procedures available through Q. Despite this approximation, however, Algorithm 3 is
a valid SMC method. This is formalised by the following theorem.

Theorem 2. Assume that Q satisﬁes condition (A1). Then, under certain regularity conditions on
the function f : Xk
k (f ), both speciﬁed in Appendix A.1.2,
we have

Rd and for an asymptotic variance ΣM

(cid:55)→

N 1/2

(cid:32)

1
N

N
(cid:88)

i=1

(cid:33)

D

f (X i

1:k)

¯πk(f )

−

−→ N

(0, ΣM

k (f )),

where

X i

{

1:k}

M

i=1 are generated by Algorithm 3 and D
−→

denotes convergence in distribution.

Proof See Appendix A.1.2.

Remark 3. The key point with Theorem 2 is that, under certain regularity conditions, the NSMC
method converges at rate √N even for a ﬁxed (and ﬁnite) value of the precision parameter M . The
asymptotic variance ΣM
k (f ), however, will depend on the accuracy and properties of the approxi-
mative procedures of Q. We leave it as future work to establish more informative results, relating
the asymptotic variance of NSMC to that of the ideal, fully adapted SMC sampler.

4.3 Backward Simulation and Modularity of NSMC

As previously mentioned, the NSMC procedure is modular in the sense that we can make use of
Algorithm 3 also to deﬁne the class Q. Thus, we now view ¯πn as the proposal distribution that we
wish to approximately sample from using NSMC. Algorithm 3 directly generates an estimate (cid:98)Zπn
of the normalising constant of πn (which indeed is unbiased, see Theorem 6). However, we also
need to generate a sample (cid:101)X1:n such that ( (cid:101)X1:n, (cid:98)Zπn) is properly weighted for πn.

1, . . . , N

The simplest approach, akin to the Nested IS procedure described in Section 3.2, is to draw Bn
1:n . This will indeed result in a valid deﬁnition
uniformly on
of the Simulate procedure. However, this approach will suffer from the well known path degen-
eracy of SMC samplers. In particular, since we call qj.Simulate() multiple times in Step 2(f)i of
Algorithm 3, we risk to obtain (very) strongly correlated samples by this simple approach.

and return (cid:101)X1:n = X Bn

{

}

It is possible to improve the performance of the above procedure by instead making use of
a backward simulator (Godsill et al., 2004; Lindsten and Sch¨on, 2013) to simulate (cid:101)X1:n. The
backward simulator, given in Algorithm 4, is a type of smoothing algorithm; it makes use of the
particles generated by a forward pass of Algorithm 3 to simulate backward in “time” a trajectory
(cid:101)X1:n approximately distributed according to ¯πn.

Remark 4. Algorithm 4 assumes unweighted particles and can thus be used in conjunction with the
fully adapted NSMC procedure of Algorithm 2. If, however, the forward ﬁlter is not fully adapted
the weights need to be accounted for in the backward simulation; see Appendix A.1.3.

The modularity of NSMC is established by the following result.

9

NAESSETH, LINDSTEN AND SCH ¨ON

Algorithm 4 Backward simulator (fully adapted)

1. Draw Bn uniformly on

1, . . . , N

{

.

}

2. Set (cid:101)Xn = X Bn
n .

3. for k = n

1 to 1

−
(a) Compute (cid:102)W j

k =

πn((X j

1:k, (cid:101)Xk+1:n))
πk(X j
1:k)

for j = 1, . . . , N .

(b) Draw Bk from a categorical distribution with probabilities

for j = 1, . . . , N .

(cid:102)W j
k
(cid:96)=1 (cid:102)W (cid:96)
k

(cid:80)N

(c) Set (cid:101)Xk:n = (X Bk

k , (cid:101)Xk+1:n).

Deﬁnition 5. Let p = Q(πn, N ) be deﬁned as follows:

1. The constructor executes Algorithm 3 with target distribution πn and with N particles, and

p.GetZ() returns the estimate of the normalising constant (cid:98)Zπn.

2. p.Simulate() executes Algorithm 4 and returns (cid:101)X1:n.

Theorem 6. The class Q deﬁned as in Deﬁnition 5 satisﬁes condition (A1).

Proof See Appendix A.1.3.

A direct, and important, consequence of Theorem 6 is that NSMC can be used as a component of
powerful learning algorithms, such as the particle Markov chain Monte Carlo (PMCMC) method
(Andrieu et al., 2010) and many of the other methods discussed in Section 5. Since standard SMC
is a special case of NSMC, Theorem 6 implies proper weighting also of SMC.

5. Practicalities and Related Work

There has been much recent interest in using SMC within SMC in various ways. The SMC2 by
Chopin et al. (2013) and the recent method by Crisan and M´ıguez (2013) are sequential learning
algorithms for state space models, where one SMC sampler for the parameters is coupled with an-
other SMC sampler for the latent states. Johansen et al. (2012) and Chen et al. (2011) address the
state inference problem by splitting the state variable into different components and run coupled
SMC samplers for these components. These methods differ substantially from NSMC; they solve
different problems and the “internal” SMC sampler(s) is constructed in a different way (for approxi-
mate marginalisation instead of for approximate simulation). Another related method is the random
weights PF of Fearnhead et al. (2010a), requiring exact samples from ¯q and where the importance
weights are estimated using a nested Monte Carlo algorithm.

The method most closely related to NSMC is the space-time particle ﬁlter (ST-PF) (Beskos
et al., 2014a), which has been developed independently and in parallel with our work. The ST-PF
is also designed for solving inference problems in high-dimensional models. It can be seen as a
island PF (Verg´e et al., 2015) implementation of the method presented by Naesseth et al. (2014b).
Speciﬁcally, for a spatio-temporal models they run an island PF over both spatial and temporal

10

NESTED SEQUENTIAL MONTE CARLO METHODS

dimensions. However, the ST-PF does not generate an approximation of the fully adapted SMC
sampler.

Another key distinction between NSMC and ST-PF is that in the latter each particle in the
“outer” SMC sampler comprises a complete particle system from the “inner” SMC sampler. For
NSMC, on the other hand, the particles will simply correspond to different hypotheses about the
latent variables (as in standard SMC), regardless of how many samplers that are nested. This is
a key feature of NSMC, since it implies that it is easily distributed over the particles. The main
N
j=1 and the calls to the Simulate
computational effort of Algorithm 3 is the construction of
}
procedure, which can be done independently for each particle. This leads to improved memory
efﬁciency and lower communication costs. Furthermore, we have found (see Section 6) that NSMC
can outperform ST-PF even when run on a single machine with matched computational costs.

qj

{

Another strength of NSMC methods are their relative ease of implementation, which we show
in Section 6.3. We use the framework to sample from what is essentially a cubic grid Markov ran-
dom ﬁeld (MRF) model just by implementing three nested samplers, each with a target distribution
deﬁned on a simple chain.

There are also other SMC-based methods designed for high-dimensional problems, e.g., the
block PF studied by Rebeschini and van Handel (2015), the location particle smoother by Briggs
et al. (2013) and the PF-based methods reviewed in Djuric and Bugallo (2013). However, these
methods are all inconsistent, as they are based on various approximations that result in systematic
errors.

The previously mentioned PMCMC (Andrieu et al., 2010) is a related method, where SMC
is used as a component of an MCMC algorithm. We make use of a very similar extended space
approach to motivate the validity of our algorithm. Note that our proposed algorithm can be used as
a component in PMCMC and most of the other algorithms mentioned above, which further increases
the scope of models it can handle.

6. Experimental Results

We illustrate NSMC on three high-dimensional examples, both with real and synthetic data. We
compare NSMC with standard (bootstrap) PF and the ST-PF of Beskos et al. (2014a) with equal
computational budgets on a single machine (i.e., neglecting the fact that NSMC is more easily dis-
tributed). These methods are, to the best of our knowledge, the only other available consistent online
methods for full Bayesian inference in general sequential models. For more detailed explanations
of the models and additional results, see Appendix A.32.

6.1 Gaussian State Space Model

=

Xk, Yk
{

We start by considering a high-dimensional Gaussian state space model, where we have access to
the true solution through belief propagation. The latent variables and measurements
,
}
d
with
k lattice Gaussian MRF. The true data is
l=1, are modeled by a d
simulated from a nearly identical state space model (see Appendix A.3.1). We run a 2-level NSMC
sampler. The outer level is fully adapted, i.e. the proposal distribution is qk = p(xk
xk−1, yk),
which thus constitute the target distribution for the inner level. To generate properly weighted
samples from qk, we use a bootstrap PF operating on the d components of the vector xk. Note that

Xk,l, Yk,l
{

X1:k, Y1:k

×

}

}

{

|

2. Code available at https://github.com/can-cs/nestedsmc

11

NAESSETH, LINDSTEN AND SCH ¨ON

d = 50

d = 100

d = 200

S
S
E

S
R
E

Figure 2: Top: Median (over dimension) ESS (4) and 15–85% percentiles (shaded region). Bottom:
The ERS (5) based on the resampling weights in the (outermost) particle ﬁlter. The results
are based on 100 independent runs for the Gaussian MRF with dimension d.

we only use bootstrap proposals where the actual sampling takes place, and that the conditional
distribution p(xk

xk−1, yk) is not explicitly used.

We simulate data from this model for k = 1, . . . , 100 for different values of d = dim(xk)

50, 100, 200
{
with both the ST-PF and standard (bootstrap) PF.

∈
. The exact ﬁltering marginals are computed using belief propagation.We compare
}

|

The results are evaluated based on the effective sample size (ESS, see e.g. Fearnhead et al.

(2010b)) deﬁned as,

ESS(xk,l) =

(cid:18)

(cid:20)

E

((cid:98)xk,l−µk,l)2
σ2

k,l

(cid:21)(cid:19)−1

,

|

where (cid:98)xk,l denote the mean estimates and µk,l and σ2
k,l denote the true mean and variance of
y1:k obtained from belief propagation. The expectation in (4) is approximated by averag-
xk,l
ing over 100 independent runs of the involved algorithms. The ESS reﬂects the estimator accuracy,
obvious by the deﬁnition which is tightly related to the mean-squared-error. Intuitively the ESS
corresponds to the equivalent number of i.i.d. samples needed for the same accuracy.

We also consider the effective resample size (ERS, Kong et al. (1994)), which is based on the

resampling weights at the top levels in the respective SMC algorithms,

(4)

(5)

The ERS is an estimate of the effective number of unique particles (or particle systems in the case
of ST-PF) available at each resampling step.

We use N = 500 and M = 2

d for NSMC and match the computational time for ST-PF
and bootstrap PF. We report the results in Figure 2. The bootstrap PF is omitted from d = 100,

·

ERS =

(cid:16)(cid:80)N

i=1 (cid:98)Zi
qk
(cid:16)
(cid:98)Zi
qk

i=1

(cid:80)N

(cid:17)2
(cid:17)2 .

12

NESTED SEQUENTIAL MONTE CARLO METHODS

200 due to its poor performance already for d = 50 (which is to be expected). Each dimension
l = 1, . . . , d provides us with a value of the ESS, so we present the median (lines) and 15–85%
percentiles (shaded regions) in the ﬁrst row of Figure 2. The ERS is displayed in the second row of
Figure 2. Note that ESS gives a better reﬂection of estimation accuracy than ERS.

We have conducted additional experiments with different model parameters and different choices
for N and M (some additional results are given in Appendix A.3.1). Overall the results seem to be
in agreement with the ones presented here, however ST-PF seems to be more robust to the trade-off
between N and M . A rule-of-thumb for NSMC is to generally try to keep N as high as possible,
while still maintaining a reasonably large ERS.

6.2 Non-Gaussian State Space Model

Next, we consider an example with a non-Gaussian SSM, borrowed from Beskos et al. (2014a)
where the full details of the model are given. The transition probability p(xk
xk−1) is a localised
|
xk) is t-distributed. The model dimension
Gaussian mixture and the measurement probability p(yk
is d = 1 024. Beskos et al. (2014a) report improvements for ST-PF over both the bootstrap PF
and the block PF by Rebeschini and van Handel (2015). We use N = M = 100 for both ST-PF
and NSMC (the special structure of this model implies that there is no signiﬁcant computational
overhead from running backward sampling) and the bootstrap PF is given N = 10 000. In Figure 3

|

Figure 3: Median ESS with 15

85% percentiles (shaded region) for the non-Gaussian SSM.

−

we report the ESS (4), estimated according to Carpenter et al. (1999). The ESS for the bootstrap PF
is close to 0, for ST-PF around 1–2, and for NSMC slightly higher at 7–8. However, we note that all
methods perform quite poorly on this model, and to obtain satisfactory results it would be necessary
to use more particles.

6.3 Spatio-Temporal Model – Drought Detection

In this ﬁnal example we study the problem of detecting droughts based on measured precipitation
data (Jones and Harris, 2013) for different locations on earth. We look at the situation in North
America during the years 1901–1950 and the Sahel region in Africa during the years 1950–2000.
These spatial regions and time frames were chosen since they include two of the most devastating
droughts during the last century, the so-called Dust Bowl in the US during the 1930s (Schubert

13

et al., 2004) and the decades long drought in the Sahel region in Africa starting in the 1960s (Foley
et al., 2003; Hoerling et al., 2006). We consider the spatio-temporal model deﬁned by Fu et al.

NAESSETH, LINDSTEN AND SCH ¨ON

Xk−1

· · ·

Xk+1

· · ·

Xk,1:2,1

Xk,1:2,3

N
→

Xk

M1
→
Xk,1:2,2

↓ M2

↓ M2

↓ M2

Figure 4: Illustration of the three-level NSMC.

{

Xk,i,j

(2012) and compare with the results therein. Each location in a region is modelled to be in either
a normal state 0 or in an abnormal state 1 (drought). Measurements are given by precipitation (in
millimeters) for each location and year. At every time instance k our latent structure is described by
I,J
a rectangular 2D grid Xk =
i=1,j=1; in essence this is the model showcased in Figure 1. Fu
et al. (2012) considers the problem of ﬁnding the maximum aposteriori conﬁguration, using a linear
programming relaxation. We will instead compute an approximation of the full posterior ﬁltering
distribution ¯πk(xk) = p(xk
y1:k). The rectangular structure is used to instantiate an NSMC method
that on the ﬁrst level targets the full posterior ﬁltering distribution. To sample from Xk we run, on
the second level, an NSMC procedure that operates on the “columns” Xk,1:I,j, j = 1, . . . , J.
Finally, to sample each column Xk,1:I,j we run a third level of SMC, that operates on the individual
components Xk,i,j, i = 1, . . . , I, using a bootstrap proposal. The structure of our NSMC method
applied to this particular problem is illustrated in Figure 4.

}

|

0.5, 0.7, 0.9
{

Figure 5 gives the results on the parts of North America that we consider. The ﬁrst row shows the
, for both regions.
number of locations where the estimate of p(xk,i,j = 1) exceeds
}
These results seems to be in agreement with Fu et al. (2012, Figures 3, 6). However, we also
receive an approximation of the full posterior and can visualise uncertainty in our estimates, as
illustrated by the three different levels of posterior probability for drought. In general, we obtain
a rich sample diversity from the posterior distribution. However, for some problematic years the
sampler degenerates, with the result that the three credibility levels all coincide. This is also visible
in the second row of Figure 5, where we show the posterior estimates p(xk,i,j
y1:k) for the years
1939–1941, overlayed on the regions of interest. For year 1940 the sampler degenerates and only
reports 0-1 probabilities for all sites. Naturally, one way to improve the estimates is to run the
sampler with a larger number of particles, which has been kept very low in this proof-of-concept.

|

7. Conclusions

We have shown that a straightforward NSMC implementation with fairly few particles can attain
reasonable approximations to the ﬁltering problem for dimensions in the order of hundreds, or
even thousands. This means that NSMC methods takes the SMC framework an important step
closer to being viable for high-dimensional statistical inference problems. However, NSMC is not

14

NESTED SEQUENTIAL MONTE CARLO METHODS

North America region

Sahel region

North America 1939

North America 1940

North America 1941

Figure 5: Top: Number of locations with estimated p(x = 1) >

for the two regions.
Bottom: Estimate of p(xt,i = 1) for all sites over a span of 3 years. All results for
N = 100, N1 =

0.5, 0.7, 0.9
}

, N2 = 20.

{

30, 40
}

{

a silver bullet for solving high-dimensional inference problems, and the approximation accuracy
will be highly model dependent. Hence, much work remains to be done, for instance on combining
NSMC with other techniques for high-dimensional inference such as localisation (Rebeschini and
van Handel, 2015) and annealing (Beskos et al., 2014b), in order to solve even more challenging
problems.

Acknowledgments

This work was supported by the projects: Learning of complex dynamical systems (Contract num-
ber: 637-2014-466) and Probabilistic modeling of dynamical systems (Contract number: 621-2013-
5524), both funded by the Swedish Research Council.

15

NAESSETH, LINDSTEN AND SCH ¨ON

Appendix A. Appendix

In this appendix we start out in Section A.1 by providing a more general formulation of the NSMC
method and proofs of the central limit and proper weighting theorems of the main manuscript.
We also detail (Section A.2) a straightforward extension of nested IS to a sequential version. We
show that a special case of this nested sequential IS turns out to be more or less equivalent to the
importance sampling squared algorithm by Tran et al. (2013). This relationship serves as evidence
that illustrates that the NSMC framework being more widely applicable than the scope of problems
considered in this article. Finally, in Section A.3 we give more details and results on the experiments
considered in the main manuscript.

A.1 Nested Sequential Monte Carlo

We start by presenting a general formulation of a nested auxiliary SMC sampler in Algorithm 5. In
this formulation, qk(xk

x1:k−1) is an arbitrary (unnormalised) proposal, normalised by

|

(cid:90)

Zqk (x1:k−1) =

qk(xk

x1:k−1)dxk.

|

Furthermore, the resampling weights are obtain by multiplying the importance weights with the
arbitrary adjustment multipliers νk−1(x1:k−1, Zqk ), which may depend on both the state sequence
x1:k−1 and the normalising constant (estimate). The fully adapted NSMC sampler (Algorithm 3 in
the main document) is obtained as a special case if we choose

qk(xk

x1:k−1) =

|

πk(x1:k)
πk−1(x1:k−1)

and νk−1(x1:k−1, Zqk ) = Zqk , in which case the importance weights are indeed given by W i

1.

k ≡

A.1.1 NESTED SMC IS SMC

The validity of Algorithm 5 can be established by interpreting the algorithm as a standard SMC
procedure for a sequence of extended target distributions.
If (cid:98)Zqk is computed deterministically,
proper weighting (i.e., unbiasedness) ensures that (cid:98)Zqk = Zqk and it is evident that the algorithm
reduces to a standard SMC sampler. Hence, we consider the case when the normalising constant
estimates (cid:98)Zqk are random.

k−1(uk−1

For k = 1, . . . , n + 1, let us introduce the random variable Uk−1 which encodes the complete
internal state of the object q generated by q = Q(qk(
x1:k−1), M ). Let the distribution of Uk−1 be
denoted as ¯ψM
x1:k−1). To put Algorithm 5 into a standard (auxiliary) SMC framework,
we shall interpret steps 2a–2b of Algorithm 5 as being the last two steps carried out during iteration
1, rather than the ﬁrst two steps carried out during iteration k. This does not alter the algorithm
k
per se, but it results in that the resampling step is conducted ﬁrst at each iteration, which is typically
the case for standard auxiliary SMC formulations.

−

· |

|

The estimator of the normalising constant is computable from the internal state of q, so that we
can introduce a function τk such that (cid:98)Zqk = τk(Uk−1). Furthermore, note that the simulation of Xk
via Xk = q.Simulate() is based solely on the internal state Uk−1, and denote by ¯γM
Uk−1) the
distribution of Xk.

k (xk

|

16

NESTED SEQUENTIAL MONTE CARLO METHODS

Algorithm 5 Nested SMC (auxiliary SMC formulation)

i=1 to arbitrary dummy variables. Set W i
N

0 = 1 for i = 1, . . . , N . Set (cid:98)Zπ0 = 1.

1. Set

X i
0}
{
2. for k = 1 to n

(a) Initialise qj = Q(qk(
(b) Compute (cid:98)Zj
(c) Compute (cid:98)νj

X j

· |

qk = qj.GetZ() for j = 1, . . . , N .
1:k−1, (cid:98)Zj
k−1 = νk−1(X j

qk ) for j = 1, . . . , N .

1:k−1), M ) for j = 1, . . . , N .

from a multinomial distribution with probabilities

k−1W j
(cid:98)νj
(cid:80)N
(cid:96)=1 (cid:98)ν(cid:96)

k−1
k−1W (cid:96)

k−1

for

(d) Draw m1:N

k

j = 1, . . . , N .

(e) Set L

0

←

(f) for j = 1 to N

mj
k.

ii. Compute W i

k =

iii. delete qj.
iv. Set L

L + mj
k.

←
(g) Compute (cid:98)Zπk = (cid:98)Zπk−1 ×

(cid:110) 1
N

i. Compute X i

k = qj.Simulate() and let X i

1:k = (X j

1:k−1, X i

k) for i = L+1, . . . , L+

πk(X i
πk−1(X j

1:k)
1:k−1)

(cid:98)Zj
qk
(cid:98)νj
k−1qk(X i
k |

X j

1:k−1)

for i = L + 1, . . . , L + mj
k.

(cid:80)N

j=1 (cid:98)νj

k−1W j

k−1

(cid:111)

(cid:110)
((cid:80)N

×

j=1 W j

k )/((cid:80)N

j=1 W j

k−1)

(cid:111)

.

Lemma 7. Assume that Q satisﬁes condition (A1) in the main manuscript. Then,

(cid:90)

τk(uk−1)¯γM

k (xk

uk−1) ¯ψM

k−1(uk−1

x1:k−1)duk−1 = qk(xk

x1:k−1).

|

Proof The pair (Xk, τk(Uk−1)) are properly weighted for qk. Hence, for a measurable function f ,

E[f (Xk)τk(Uk−1)

x1:k−1] =

|

f (xk)τk(uk−1)¯γM
(cid:90)

k (xk

|

uk−1) ¯ψM

k−1(uk−1
(cid:90)

|

x1:k−1)duk−1dxk

= Zk(x1:k−1)

f (xk)¯qk(xk

x1:k−1)dxk =

f (xk)qk(xk

x1:k−1)dxk.

|

Since f is arbitrary, the result follows.

|

(cid:90) (cid:90)

|

|

We can now deﬁne the sequence of (unnormalised) extended target distributions for the Nested

SMC sampler as,

Πk(x1:k, u0:k) :=

τk(uk−1) ¯ψM

k (uk
|
qk(xk
|

x1:k)¯γM
x1:k−1)

k (xk

uk−1)

|

πk(x1:k)
πk−1(x1:k−1)

Πk−1(x1:k−1, u0:k−1),

and Π0(u0) = ¯ψM

0 (u0). We write Θk = Xk

Uk for the domain of Πk.

×

17

NAESSETH, LINDSTEN AND SCH ¨ON

|

|

Lemma 8. Assume that Q satisﬁes condition (A1) in the main manuscript. Then,

(cid:90)

τk(uk−1)¯γM

k (xk

|

uk−1)Πk−1(x1:k−1, u0:k−1)du0:k−1 = πk−1(x1:k−1)qk(xk

x1:k−1).

Proof The proof follows by induction. At k = 1, we have (cid:82) τ1(u0)¯γM
q1(x1) by Lemma 7. Hence, assume that the hypothesis holds for k

1 (x1

|

1 and consider

u0) ¯ψM

0 (u0)du0 =

≥

uk)τk(uk−1) ¯ψM

k (uk

x1:k)¯γM

uk−1)Πk−1(x1:k−1, u0:k−1)du0:k

(cid:90)

τk+1(uk)¯γM

k+1(xk+1

|
πk(x1:k)
πk−1(x1:k−1)qk(xk

uk)Πk(x1:k, u0:k)du0:k

=

(cid:90)

·

·

=

(cid:90)

=

x1:k−1)

|
k+1(xk+1

τk+1(uk)¯γM
πk(x1:k) (cid:0)(cid:82) τk+1(uk)¯γM

|

k+1(xk+1
|
πk−1(x1:k−1)qk(xk

k (xk
(cid:1)

|

x1:k)duk

|
k (uk

uk) ¯ψM
x1:k−1)

|

|

uk−1)Πk−1(x1:k−1, u0:k−1)du0:k−1

τk(uk−1)¯γM

k (xk

|
πk(x1:k)qk+1(xk+1

|

x1:k)πk−1(x1:k−1)qk(xk

x1:k−1)

|

= πk(x1:k)qk+1(xk+1

x1:k),

πk−1(x1:k−1)qk(xk

x1:k−1)

|

where the penultimate equality follows by applying Lemma 7 and the induction hypothesis to the
two integrals, respectively.

As a corollary to Lemma 8, it follows that

(cid:90)

Πk(x1:k, u0:k)du0:k = πk(x1:k).

(6)

Consequently, Πk is normalised by the same constant Zπk as πk, and by deﬁning ¯Πk(x1:k, u0:k) :=
Z−1
πk Πk(x1:k, u0:k) we obtain a probability distribution which admits ¯πk as a marginal (note that
¯Π0 = Π0, which is normalised by construction). This implies that we can use ¯Πk as a proxy
for ¯πk in a Monte Carlo algorithm, i.e., samples drawn from ¯Πk can be used to compute ex-
pectations w.r.t. ¯πk. This is precisely what Algorithm 5 does; it is a standard auxiliary SMC
sampler for the (unnormalised) target sequence Πk, k = 0, . . . , n, with adjustment multiplier
weights νk−1(x1:k−1, τk(uk−1)) and proposal distribution ¯γM
x1:k). The (stan-
dard) weight function for this sampler is thus given by

uk−1) ¯ψM

k (xk

k (uk

|

|

Wk(x1:k, u0:k)

πk(x1:k)
πk−1(x1:k−1)

∝

τk(uk−1)

,

(7)

νk−1(x1:k−1, τk(uk−1))qk(xk

x1:k−1)

|

which is the same as the expression on line 2(f)ii of Algorithm 5.

A.1.2 CENTRAL LIMIT THEOREM – PROOF OF THEOREM 2 IN THE MAIN MANUSCRIPT

Now that we have established that Nested SMC is in fact a standard auxiliary SMC sampler, albeit
on an extended state space, we can reuse existing convergence results from the SMC literature; see

18

NESTED SEQUENTIAL MONTE CARLO METHODS

e.g., Johansen and Doucet (2008); Douc and Moulines (2008); Douc et al. (2009); Chopin (2004)
or the extensive textbook by Del Moral (2004).

Here, in order to prove Theorem 2 of the main manuscript, we make use of the result for the
auxiliary SMC sampler by Johansen and Doucet (2008), which in turn is based on the central limit
theorem by Chopin (2004). The technique used by Johansen and Doucet (2008) is to reinterpret (as
detailed below) the auxiliary SMC sampler as a sequential importance sampling and resampling
(SISR) particle ﬁlter, by introducing the modiﬁed (unnormalised) target distribution

Π(cid:48)

k(x1:k, u0:k) := νk(x1:k, τk+1(uk))Πk(x1:k, u0:k).

(8)

The auxiliary SMC sampler described in the previous section can then be viewed as a SISR algo-
rithm for (8). Indeed, if we write ¯QM
uk−1) for
the joint proposal distribution of (xk, uk), then the weight function for this SISR sampler is given
by

x1:k−1, uk−1) := ¯ψM

k (xk, uk

x1:k)¯γM

k (xk

k (uk

|

|

|

W (cid:48)

k(x1:k, u0:k) :=

¯Π(cid:48)

k(x1:k, u0:k)

¯QM

k (xk, uk

x1:k−1, uk−1) ¯Π(cid:48)

k−1(x1:k−1, u0:k−1)

νk(x1:k, τk+1(uk))Wk(x1:k, u0:k),

∝

(9)

where Wk is deﬁned in (7). This weight expression thus accounts for both the importance weights
and the adjustment multipliers of the auxiliary SMC sampler formulation.

Since this SISR algorithm does not target ¯Πk (and thus not ¯πk) directly, we use an additional IS
step to compute estimators of expectations w.r.t. to ¯π. The proposal distribution for this IS procedure
is given by

¯Γk(x1:k, u0:k) := ¯QM

k (xk, uk

x1:k−1, uk−1) ¯Π(cid:48)

k−1(x1:k−1, u0:k−1).

(10)

Note that we obtain an approximation of (10) after the propagation Step 2(f)i of Algorithm 5, but
before the weighting step. The resulting IS weights, for target distribution ¯Πk(x1:k, u0:k) and with
proposal distribution (10), are given by

|

|

¯Πk(x1:k, u0:k)
¯Γk(x1:k, u0:k)

=: ωk(x1:k, u0:k)

Wk(x1:k, u0:k).

∝

Hence, with f : Xk
(cid:55)→
obvious abuse of notation) by the estimator

Rd being a test function of interest we can estimate E¯πk [f ] = E ¯Πk [f ] (with

N
(cid:88)

i=1

kf (X i
W i
1:k)
(cid:80)N
(cid:96)=1 W (cid:96)
k

,

(11)

which, again, is in agreement with Algorithm 5.

We have now reinterpreted the NSMC algorithm; ﬁrst as a standard auxiliary SMC sampler,
and then further as a standard SISR method. Consequently, we are now in the position of directly
applying, e.g., the central limit theorem by Chopin (2004, Theorem 1). The conditions and the
statement of the theorem are reproduced here for clarity.

19

NAESSETH, LINDSTEN AND SCH ¨ON

For any measurable function f : Θ0

measurable function f : Θk

Rd,

Rd, let (cid:101)V M

0 (f ) = Var ¯ψM

0

(cid:55)→

(f ) and deﬁne, for any

(cid:55)→
(cid:101)V M
k (f ) = (cid:98)V M
k−1(E ¯QM
V M
k (W (cid:48)
k (f ) = (cid:101)V M
(cid:98)V M
k (f ) = V M

k(f

k

[f ]) + E ¯Π(cid:48)
E ¯Π(cid:48)

[f ])),

k−1

−

k
(f ),

k (f ) + Var ¯Π(cid:48)

k

[Var ¯QM

k

(f )],

k > 0,

0,

0.

k

k

≥

≥

Deﬁne recursively Φk to be the set of measurable functions f : Θk
δ > 0 with E¯Γk [
2+δ] <
(cid:107)
(cid:107)
Φk−1. Furthermore, assume that the identity function f
≡
follows by Chopin (2004, Theorem 1 and Lemma A.1) that

W (cid:48)

kf

∞

and such that the function (x1:k−1, u0:k−1)

Rd such that there exists a
E ¯QM
kf ] is in
(cid:55)→
1 belongs to Φk for each k. Then, it

[W (cid:48)

(cid:55)→

k

N 1/2

(cid:32) N
(cid:88)

i=1

1
N

f (X i

1:k, U i

0:k)

E¯Γk [f ])

−

−→ N

(0, (cid:101)V M

k (f )),

(12)

(cid:33)

D

for any function f such that the function (x1:k−1, u0:k−1)
exists a δ > 0 such that E¯Γk [
(cid:107)
samples obtained after the propagation Step 2(f)i of Algorithm 5, but before the weighting step.

E¯Γk [f ]] is in Φk−1 and there
. The convergence in (12) thus holds for the unweighted

2+δ] <
(cid:107)

E ¯QM

∞

(cid:55)→

−

[f

f

k

To complete the proof, it remains to translate (12) into a similar result for the IS estimator (11).
To this end we make use of Chopin (2004, Lemma A.2) which is related to the IS correction step
Rd denote the
of the SMC algorithm. Speciﬁcally, for a function f : Xk
Rd such
extension of f to Θk, deﬁned by f e(x1:k, u0:k) = f (x1:k). Then, for any f : Xk
E ¯QM
[ωkf e] is in Φk−1 and there exists a δ > 0 such that
that the function (x1:k−1, u0:k−1)
E¯Γk [
2+δ] <
ωkf e
(cid:107)

Rd, let f e : Θk

, we have

(cid:55)→

(cid:55)→

(cid:55)→

(cid:55)→

∞

(cid:107)

k

N 1/2

(cid:32) N
(cid:88)

i=1

kf (X i
W i
1:k)
(cid:80)N
(cid:96)=1 W (cid:96)
k −

(cid:33)

D

¯πk(f )

−→ N

(0, ΣM

k (f )),

where

(X i
{

1:k, W i
k)

i=1 are generated by Algorithm 5 and ΣM
M
}

k (f ) = (cid:101)V M

k (ωk(f e

E ¯Πk [f e])).

−

A.1.3 NESTED SMC GENERATES PROPERLY WEIGHTED SAMPLES – PROOF OF THEOREM 6

IN THE MAIN MANUSCRIPT

In the previous two sections we showed that the NSMC procedure is a valid inference algorithm
for ¯πn. Next, we turn our attention to the modularity of the method and the validity of using the
algorithm as a component in another NSMC sampler. Let us start by stating a more general version
of the backward simulator in Algorithm 6. Clearly, if the forward NSMC procedure is fully adapted
W i

1, Algorithm 6 reduces to the backward simulator stated in the main manuscript.

k ≡
We will now show that the pair ( (cid:98)Zπn, (cid:101)X1:n) generated by Algorithms 5 and 6 is properly

weighted for πn(x1:n), and thereby prove Theorem 6 in the main manuscript.

The proof is based on the particle Markov chain Monte Carlo (PMCMC) construction (Andrieu
et al., 2010). The idea used by Andrieu et al. (2010) was to construct an extended target distribution,
incorporating all the random variables generated by an SMC sampler as auxiliary variables. This
opened up for using SMC approximations within MCMC in a provably correct way; these seem-
ingly approximate methods simply correspond to standard MCMC samplers for the (nonstandard)

20

NESTED SEQUENTIAL MONTE CARLO METHODS

Algorithm 6 Backward simulator

1. Draw Bn from a categorical distribution with probabilities

for j = 1, . . . , N .

2. Set (cid:101)Xn = X Bn
n .

3. for k = n

1 to 1

−
(a) Compute (cid:102)W j

k = W j

k

πn((X j

1:k, (cid:101)Xk+1:n))
πk(X j
1:k)

for j = 1, . . . , N .

(b) Draw Bk from a categorical distribution with probabilities

for j = 1, . . . , N .

W j
n
(cid:96)=1 W (cid:96)
n

(cid:80)N

(cid:102)W j
k
(cid:96)=1 (cid:102)W (cid:96)
k

(cid:80)N

(c) Set (cid:101)Xk:n = (X Bk

k , (cid:101)Xk+1:n).

4. return (cid:101)X1:n

extended target distribution. Here we will use the same technique to prove the proper weighing
property of the NSMC procedure.

We start by introducing some additional notation for the auxiliary variables of the extended
target construction. While Algorithm 5 is expressed using multinomial random variables m1:N
in the resampling step, it is more convenient for the sake of the proof to explicitly introduce the
i=1; see e.g., Andrieu et al. (2010). That is, Ai
N
ancestor indices
k is a categorical random
Ai
1 of particle X i
k
k. The
variable on
1:k−1 is ancestor particle at iteration k
, such that X
resampling Step 2d of Algorithm 5 can then equivalently be expressed as: simulate independently
Ai

N
i=1 from the categorical distribution with probabilities

k}
{
1, . . . , N
}
{

Ai

−

k

{

k}

k−1W j
(cid:98)νj
(cid:80)N
(cid:96)=1 (cid:98)ν(cid:96)

k−1
k−1W (cid:96)

k−1

.

{

X 1

k , . . . , X N
k }

Let Xk =
, denote all the
particles, internal states of the proposals, and ancestor indices, respectively, generated at iteration k
of the NSMC algorithm. We can then write down the joint distribution of all the random variables
generated in executing Algorithm 5 (up to an irrelevant permutation of the particle indices) as,

k , . . . , U N
U 1
k }
{

k, . . . , AN
k }

, and Ak =

, Uk =

A1
{

¯ΨNSMC(x1:n, u0:n, a1:n) =

¯ψM
0 (ui
0)

(cid:40) N
(cid:89)

i=1

(cid:41) n
(cid:89)




N
(cid:89)

k=1



i=1

ai
k
(cid:98)ν
k−1W
(cid:80)N
(cid:96)=1 (cid:98)ν(cid:96)

ai
k
k−1
k−1W (cid:96)

k−1

¯QM

k (xi

k, ui
k |

ai
k
1:k−1, u

ai
k
k−1)

x






,

(13)

where we interpret (cid:98)νi

k and W i

k as deterministic functions of (xi

1:k, ui

0:k).

21

NAESSETH, LINDSTEN AND SCH ¨ON

Let Bn denote a random variable deﬁned on

. The extended target distribution for

1, . . . , N
{

}

PMCMC samplers corresponding to (13) is then given by

¯Φ(x1:n, u0:n, a1:n, bn) :=

¯ΨNSMC(x1:n, u0:n, a1:n),

(14)

(cid:98)Zπn
Zπn

W bn
n
(cid:96)=1 W (cid:96)
n

(cid:80)N

where (cid:98)Zπn is a deterministic function of (x1:n, u0:n, a1:n). We know from Andrieu et al. (2010)
that ¯Φ is a probability distribution which admits ¯Πn as its marginal distribution for (X bn
0:n).
Consequently, by (6) it follows that the marginal distribution of X bn
1:n is ¯πn. For later reference we
deﬁne recursively bk−1 := abk
k for k = 1, . . . , n, the particle indices for the trajectory obtained by
tracing backward the genealogy of the bn’th particle at iteration n.

1:n, U bn

We now turn our attention to the backward simulator in Algorithm 6. Backward simulation
has indeed been used in the context of PMCMC, see e.g. Whiteley (2010); Lindsten and Sch¨on
(2013); Lindsten et al. (2014). The strategy used for combining PMCMC with backward simulation
is to show that each step of the backward sampler corresponds to a partially collapsed Gibbs sam-
pling step for the extended target distribution ¯Φ. This implies that the backward sampler leaves ¯Φ
invariant.

We use the same approach here, but we need to be careful in how we apply the existing results,
since the PMCMC distribution ¯Φ is deﬁned w.r.t. to ¯Πn, whereas the backward simulator of Algo-
rithm 6 works with the original target distribution ¯πn. Nevertheless, from the proof of Lemma 1 by
Lindsten et al. (2014) it follows that we can write the following collapsed conditional distribution
of ¯Φ as:

¯Φ(bk, ubk:n
k:n |

x1:k, u0:k−1, a1:k, x

Πn(

W bk
k

∝

bk+1:n
k+1:n , bk+1:n)
bk+1:n
xbk
1:k, x
,
k+1:n }
{
{
bk
a
0:k−1, ubk
Πk(xbk
k
u
1:k,
k }
{

)

bk
a
0:k−1, ubk:n
k
u
k:n }

)

k (ubk
¯ψM
k |

xbk
1:k).

(15)

To simplify this expression, consider,

Πn(x1:n, u0:n)
Πk(x1:k, u0:k)

n
(cid:89)

(cid:26) τs(us−1) ¯ψM

=

=

s=k+1
¯ψM
n (un
¯ψM
k (uk

x1:n)
x1:k)

|
|

s (us
|
qs(xs
|

x1:s)¯γM
x1:s−1)

(cid:40) n
(cid:89)

τs(us−1) ¯ψM

s=k+1

s (xs

us−1)

|

(cid:27)

πs(x1:s)
πs−1(x1:s−1)

s−1(us−1
qs(xs

x1:s−1)¯γM
x1:s−1)

s (xs

us−1)

|

πn(x1:n)
πk(x1:k)

.

(cid:41)

|
|

(16)

By Lemma 7 we know that each factor of the product (in brackets) on the second line integrates to
1 over us−1. Hence, plugging (16) into (15) and integrating over ubk:n

k:n yields

¯Φ(bk

|

x1:k, u0:k−1, a1:k, x

bk+1:n
k+1:n , bk+1:n)

πn(
{

W bk
k

∝

xbk
1:k, x
πk(xbk

bk+1:n
)
k+1:n }
1:k)

,

which coincides with the expression used to simulate the index Bk in Algorithm 6. Hence, sim-
ulation of Bk indeed corresponds to a partially collapsed Gibbs sampling step for ¯Φ and it will
thus leave ¯Φ invariant. (Note that, in comparison with the PMCMC sampler derived by Lindsten

22

NESTED SEQUENTIAL MONTE CARLO METHODS

et al. (2014) we further marginalise over the variables ubk:n
partially collapsed Gibbs step.)

k:n which, however, still results in a valid

We now have all the components needed to prove proper weighting of the combined NSMC/backward

simulation procedure. For notational simplicity, we write

¯ΨBS,k(bk) = ¯Φ(bk

x1:k, u0:k−1, a1:k, x

bk+1:n
k+1:n , bk+1:n),

|

for the distribution of Bk in Algorithm 6. Let ( (cid:98)Zπn, (cid:101)X1:n) be generated by Algorithms 5 and 6. Let
f be a measurable function and consider

E[ (cid:98)Zπnf ( (cid:101)X1:n)] =

(cid:90)

(cid:98)Zπnf (X

b(cid:48)
1:n
1:n )

¯ΨBS,k(db(cid:48)
k)

¯ΨNSMC(d(x1:n, u0:n, a1:n))

(cid:41)

(cid:41)

(cid:40) n
(cid:89)

k=1
(cid:40)n−1
(cid:89)

k=1

= Zπn

(cid:90)

f (X

b(cid:48)
1:n
1:n )

¯ΨBS,k(db(cid:48)
k)

¯Φ(d(x1:n, u0:n, a1:n, b(cid:48)

n)),

where, for the second equality, we have used the deﬁnition (14) and noted that ¯ΨBS,n(bn) =

W bn
n
(cid:96)=1 W (cid:96)
n

(cid:80)N

. However, by the invariance of ¯ΨBS,k w.r.t. ¯Φ, it follows that

E[ (cid:98)Zπnf ( (cid:101)X1:n)] = Zπn

f (X b1:n

1:n ) ¯Φ(d(x1:n, u0:n, a1:n, bn)) = Zπn ¯πn(f ),

(cid:90)

which completes the proof.

A.2 Nested Sequential Importance Sampling

Here we give the deﬁnition of the nested sequential importance sampler and we show that a special
case of this is the importance sampling squared (IS2) method by Tran et al. (2013).

A.2.1 NESTED SEQUENTIAL IMPORTANCE SAMPLING

We present a straightforward extension of the Nested IS class to a sequential IS version. Consider
the following deﬁnition of the Nested SIS Q:

1. Algorithm 7 is executed at the construction of the object p = Q(πn, N ), and p.GetZ() returns

the normalising constant estimate (cid:98)Zπn.

2. p.Simulate() simulates a categorical random variable B with P(B = i) = W i

n/ (cid:80)N

(cid:96)=1 W (cid:96)
n

and returns X B

1:n.

Note that we do not require that the procedure Q is identical for each individual proposal qk,
thus we have a ﬂexibility in designing our algorithm as can be seen in the example in Section A.2.2.
We can motivate the algorithm in the same way as for Nested IS and similar theoretical results hold,
i.e. Nested SIS is properly weighted for πn and it admits ¯πn as a marginal.

A.2.2 RELATION TO IS2

Here we will show how IS2, proposed by Tran et al. (2013), can be viewed as a special case of
Nested SIS. We are interested in approximating the posterior distribution of parameters θ given

23

NAESSETH, LINDSTEN AND SCH ¨ON

Algorithm 7 Nested SIS (all i for 1, . . . , N )

1. Initialise qi = Q(q1(
·

), M ).

2. Set (cid:98)Zi

q1 = qi.GetZ(), X i

1 = qi.Simulate().

q1π1(X i
(cid:98)Zi
1)
q1(X i
1)

.

3. Set W i

1 =

4. delete qi.

5. for k = 2 to n:

· |

(a) Initialise qi = Q(qk(
(b) Set (cid:98)Zi

X i
1:k−1), M ).
k = qi.Simulate().
qk = qi.GetZ(), X i
qk πk(X i
(cid:98)Zi
1:k−1)
k |
X i
qk(X i
k |

X i
1:k−1)

k = W i

(c) Set W i

k−1

.

(d) delete qi.
(e) Set X i

1:k ←
6. Compute (cid:98)Zπn = 1
N

(X i

1:k−1, X i
k)
(cid:80)N

i=1 W i
n.

some observed values y

We assume that the data likelihood p(y
an integral

|

¯π(θ

y)

p(y

θ)p(θ).

∝

|
θ) can, by introducing a latent variable x, be computed as

|

Now, let our target distribution in Nested SIS be ¯π2(θ, x) = ¯π2(x
We set our proposal distributions to be

|

θ)¯π1(θ) = p(y | x,θ)p(x | θ)

p(y | θ)

p(θ).

(cid:90)

p(y

θ) =

p(y

x, θ)p(x

θ) dx.

|

|

|

¯q1(θ) = gIS(θ),

¯q2(x

θ) =

|

p(y

|

x, θ)p(x
θ)
p(y

|

θ)

.

|
First, Q(q1(
), 1) runs an exact sampler from the proposal gIS. Then at iteration k = 2 we let the
·
nested procedure Q(q2(
y, θ), giving us
properly weighted samples for q2. Putting all this together gives us samples θi distributed according
to gIS(θ) and weighted by

θi), M ) be a standard IS algorithm with proposal h(x

· |

|

W i

2 ∝

p(θi)
gIS(θi) ·

p(y

xi, θi)p(xi

|

(cid:80)M

θi) 1
(cid:96)=1
M
xi, θi)p(xi

|
p(y

|

θi)

|

p(y | x(cid:96),θi)p(x(cid:96) | θi)
h(x(cid:96) | y,θi)

= (cid:98)pM (y

θi)p(θi)

|
gIS(θi)

,

(17)

24

NESTED SEQUENTIAL MONTE CARLO METHODS

θi) = M −1 (cid:80)M
where (cid:98)pM (y
(cid:96)=1
identical to the IS2 algorithm proposed by Tran et al. (2013).

p(y | x(cid:96),θi)p(x(cid:96) | θi)
h(x(cid:96) | y,θi)

|

. Thus we obtain a Nested SIS method that is

A.3 Further Details on the Experiments

We provide some further details and results for the experiments presented in the main manuscript.

A.3.1 GAUSSIAN STATE SPACE MODEL

We generate data from a synthetic d-dimensional (dim(xk) = d) dynamical/spatio-temporal3 model
deﬁned by

xk

xk−1

|
yk

∼ N

∼ N

xk

|

(xk; µk(xk−1), Σ),
(yk; xk, τ −1

φ I),

where Σ and µk are given as follows

Σ =

τρ + τψ
τψ













−
0
...
...
0
0

τψ
−
τρ + 2τψ
. . .
. . .
...
0
0

0
τψ
−
. . .
. . .
. . .
0
0

· · ·
0
. . .
. . .
. . .
0
0

0
0

0

· · ·
· · ·
. . .
. . .
. . .
−
τψ τρ + 2τψ
−
0

τψ

τψ

0

−

0
0

0

0

−1













,

0
τψ
−
τρ + τψ

µk(xk−1) = aτρΣxk−1.

Alternatively, in a more standard state space model notation, we have

xk = Axk−1 + vk, vk
yk = xk + ek, ek

∼ N
(0, R),

(0, Q),

∼ N

where A = aτρΣ, Q = Σ and R = τ −1
(1, 0.5, 1, 10) are known.

φ I. We assume that the parameters θ = (τψ, a, τρ, τφ) =

To do inference with this generated data-set

we propose to target the following slightly

yk

{

}

different model

p(x1:k, y1:k)

φ(xj, yj)ρ(xj)ψ(xj, xj−1),

k
(cid:89)

j=1

∝

3. Note that in a previous version this was erraneously stated as equivivalent to the Gaussian MRF we use for sequential
inference. Thus this example actually illustrates a problem where we have a misspeciﬁed model. However, this
misspeciﬁcation does not lead to any discernible difference in the MSE results. This because the exact ﬁltering
marginals for the two different models (LGSS, GMRF) with the parameters chosen differs with orders of magnitudes
much lower than the Monte Carlo errors.

25

NAESSETH, LINDSTEN AND SCH ¨ON

where the observation potential φ and interaction potentials ρ and ψ are given by

φ(xk, yk) =

φl(xk,l, yk,l) =

e−

τφ
2 (xk,l−yk,l)2

,

ψ(xk) =

ψl(xk,l, xk,l−1) =

τψ
2 (xk,l−xk,l−1)2

e−

ρ(xk, xk−1) =

ρl(xk,l, xk−1,l) =

τρ
2 (xk,l−axk−1,l)2

e−

,

.

d
(cid:89)

l=1

d
(cid:89)

l=2
d
(cid:89)

l=1

d
(cid:89)

l=1
d
(cid:89)

l=2
d
(cid:89)

l=1

k) lattice MRF, i.e. it grows with “time” k. The
This can be visualised as a Gaussian rectangular (d
×
goal is to estimate the ﬁltering distribution p(xk
y1:k). Note that this model has almost identical
|
ﬁltering marginals as the data generating distribution and leads to a simpler implementation of
NSMC and ST-PF.

Results (mean-squared-error, MSE) comparing NSMC and ST-PF for different settings of N
and M can be found in the ﬁrst row of Figure 6 and the second row displays the results when
comparing ST-PF to the SMC method by Naesseth et al. (2014b) for equal computational budgets.
We show median (over dimensions d) MSE for posterior marginal mean and variance estimates
of the respective algorithms. True values are obtained using belief propagation. Note that setting
N = 1 in ST-PF can be viewed as a special case of the SMC method by Naesseth et al. (2014b).

A.3.2 SPATIO-TEMPORAL MODEL – DROUGHT DETECTION

We present the full model for drought detection in our notation, this is essentially the model by Fu
et al. (2012) adapted for estimating the ﬁltering distribution. The latent variables for each location
on a ﬁnite world grid, xk,i,j, are binary, i.e. 0 being normal state and 1 being the abnormal (drought)
state. Measurements, yk,i,j, are available as real valued precipitation values in millimeters. The
probabilistic model for ﬁltering is given as,

k
(cid:89)

∝

n=1

(cid:40)

1
2σ2
i,j

−

p(x1:k, y1:k)

φ(xn, yn)ρ(xn)ψ(xn, xn−1),

(18a)

where

φ(xk, yk) =

exp

(yk,i,j

µab,i,jxk,i,j

µnorm,i,j(1

−

−

−

(cid:41)

xk,i,j))2

,

(18b)

ρ(xk) =

exp (cid:8)C1

(cid:0)1xk,i,j =xk,i,j−1 + 1xk,i,j =xk,i−1,j

(cid:1)(cid:9) ,

ψ(xk, xk−1) =

exp (cid:8)C21xk,i,j =xk−1,i,j

(cid:9) .

(18c)

(18d)

I
(cid:89)

J
(cid:89)

i=1

j=1

I
(cid:89)

J
(cid:89)

i=1

j=1

I
(cid:89)

J
(cid:89)

i=1

j=1

Here, 1 is the indicator function, and with the convention that all expressions in (18c) that end up
with index 0 evalute to 0. The parameters C1, C2 are set to 0.5, 3 as in (Fu et al., 2012). Location

26

NESTED SEQUENTIAL MONTE CARLO METHODS

Median MSE for E[xk,(cid:96)]

Median MSE for Var(xk,(cid:96))

Median MSE for E[xk,(cid:96)]

Median MSE for Var(xk,(cid:96))

Figure 6: Top: Comparisons for different settings of N and M on the 50-dimensional SSM. Bot-
tom: Illustrating the connection between ST-PF and the SMC method by Naesseth et al.
(2014b).

44 region with latitude 6

55◦N and longitude 90

2012. For the North America region we consider a 20

based parameters σi,j, µab,i,j, µnorm,i,j are estimated based on data from the CRU dataset with world
precipitation data from years 1901
30
120◦W . For the Sahel region we consider a
region with latitude 35
35◦E. Note that for a few locations in
24
Africa (Sahel region) the average yearly precipitation was constant. For these locations we simply
set µnorm,i,j to be this value, µab,i,j = 0 and σ2
i,j to be the mean variance of all locations, thus this
might have introduced some artifacts. Some representative results for the Sahel region are displayed
in Figure 7.

30◦N and longitude 10◦W

−

×

−

−

×

−

−

References

C. Andrieu and G. O. Roberts. The pseudo-marginal approach for efﬁcient Monte Carlo computa-

tions. The Annals of Statistics, 37(2):697–725, 2009.

Christophe Andrieu, Arnaud Doucet, and Roman Holenstein. Particle Markov chain Monte Carlo
methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):
269–342, 2010.

27

NAESSETH, LINDSTEN AND SCH ¨ON

Sahel region 1986

Sahel region 1987

Sahel region 1988

Figure 7: Estimate of P(Xk,i,j = 1

y1:k) for all sites over a span of 3 years. All results for N =

100, N1 =

30, 40
}

{

|
, N2 = 20.

A. Beskos, D. Crisan, A. Jasra, K. Kamatani, and Y. Zhou. A stable particle ﬁlter in high-

dimensions. ArXiv:1412.3501, December 2014a.

Alexandros Beskos, Dan Crisan, and Ajay Jasra. On the stability of sequential Monte Carlo methods

in high dimensions. Ann. Appl. Probab., 24(4):1396–1445, 08 2014b.

Peter Bickel, Bo Li, and Thomas Bengtsson. Sharp failure rates for the bootstrap particle ﬁlter
in high dimensions, volume Volume 3 of Collections, pages 318–329. Institute of Mathematical
Statistics, Beachwood, Ohio, USA, 2008.

Jonathan Briggs, Michael Dowd, and Renate Meyer. Data assimilation for large-scale spatio-

temporal systems using a location particle smoother. Environmetrics, 24(2):81–97, 2013.

Olivier Capp´e, Eric Moulines, and Tobias Ryd´en. Inference in Hidden Markov Models. Springer-

Verlag New York, Inc., Secaucus, NJ, USA, 2005. ISBN 0387402640.

J. Carpenter, P. Clifford, and P. Fearnhead. Improved particle ﬁlter for nonlinear problems. IEE

Proceedings Radar, Sonar and Navigation, 146(1):2–7, 1999.

Tianshi Chen, Thomas B. Sch¨on, Henrik Ohlsson, and Lennart Ljung. Decentralized particle ﬁlter
with arbitrary state decomposition. IEEE Transactions on Signal Processing, 59(2):465–478, Feb
2011.

N. Chopin. Central limit theorem for sequential Monte Carlo methods and its application to

Bayesian inference. The Annals of Statistics, 32(6):2385–2411, 2004.

N. Chopin, P. E. Jacob, and O. Papaspiliopoulos. SMC2: an efﬁcient algorithm for sequential
analysis of state space models. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 75(3):397–426, 2013.

Jacques Cohen. Bioinformaticsan introduction for computer scientists. ACM Computing Surveys

(CSUR), 36(2):122–158, 2004.

N. Cressie and C. K. Wikle. Statistics for spatio-temporal data. Wiley, 2011.

28

NESTED SEQUENTIAL MONTE CARLO METHODS

D. Crisan and J. M´ıguez. Nested particle ﬁlters for online parameter estimation in discrete-time

state-space Markov models. ArXiv:1308.1883, August 2013.

P. Del Moral. Feynman-Kac Formulae - Genealogical and Interacting Particle Systems with Appli-

cations. Probability and its Applications. Springer, 2004.

Petar M Djuric and M´onica F Bugallo. Particle ﬁltering for high-dimensional systems. In Compu-
tational Advances in Multi-Sensor Adaptive Processing (CAMSAP), 2013 IEEE 5th International
Workshop on, pages 352–355. IEEE, 2013.

R. Douc and E. Moulines. Limit theorems for weighted samples with applications to sequential

Monte Carlo. The Annals of Statistics, 36(5):2344–2376, 2008.

R. Douc, E. Moulines, and J. Olsson. Optimality of the auxiliary particle ﬁlter. Probability and

Mathematical Statistics, 29:1–28, 2009.

A. Doucet and A. M. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years later.
In D. Crisan and B. Rozovsky, editors, Nonlinear Filtering Handbook. Oxford University Press,
2011.

Arnaud Doucet, Nando De Freitas, and Neil Gordon. An introduction to sequential Monte Carlo

methods. Springer, 2001.

Paul Fearnhead, Omiros Papaspiliopoulos, Gareth O. Roberts, and Andrew Stuart. Random-weight
particle ﬁltering of continuous time processes. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 72(4):497–512, 2010a.

Paul Fearnhead, David Wyncoll, and Jonathan Tawn. A sequential smoothing algorithm with linear

computational cost. Biometrika, 97(2):447–464, 2010b.

J. A. Foley, M. T. Coe, M. Scheffer, and G. Wang. Regime shifts in the sahara and sahel: Interactions

between ecological and climatic systems in northern africa. Ecosystems, 6:524–539, 2003.

Qiang Fu, Arindam Banerjee, Stefan Liess, and Peter K. Snyder. Drought detection of the last
century: An MRF-based approach. In Proceedings of the 2012 SIAM International Conference
on Data Mining, pages 24–34, Anaheim, CA, USA, April 2012.

S. J. Godsill, A. Doucet, and M. West. Monte Carlo smoothing for nonlinear time series. Journal

of the American Statistical Association, 99(465):156–168, March 2004.

N. J. Gordon, D. J. Salmond, and A. F. M. Smith. Novel approach to nonlinear/non-Gaussian
Bayesian state estimation. Radar and Signal Processing, IEE Proceedings F, 140(2):107 –113,
April 1993.

M. Hoerling, J. Hurrell, J. Eischeid, and A. Phillips. Detection and attribution of twentieth-century

northern and southern african rainfall change. Journal of Climate, 19:3989–4008, 2006.

A. M. Johansen and A. Doucet. A note on auxiliary particle ﬁlters. Statistics & Probability Letters,

78(12):1498–1504, 2008.

29

NAESSETH, LINDSTEN AND SCH ¨ON

A. M. Johansen, N. Whiteley, and A. Doucet. Exact approximation of Rao-Blackwellised particle
ﬁlters. In Proceesings of the 16th IFAC Symposium on System Identiﬁcation (SYSID), pages 488–
493, Brussels, Belgium, 2012.

P.D. Jones and I. Harris. CRU TS3.21: Climatic research unit (CRU) time-series (ts) version 3.21
of high resolution gridded data of month-by-month variation in climate (jan. 1901- dec. 2012).
NCAS British Atmospheric Data Centre, sep 2013. URL http://dx.doi.org/10.5285/
D0E1585D-3417-485F-87AE-4FCECF10A992.

R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Transactions of the

ASME, Journal of Basic Engineering, 82:35–45, 1960.

A. Kong, J. S. Liu, and W. H. Wong. Sequential imputations and Bayesian missing data problems.

Journal of the American Statistical Association, 89(425):278–288, 1994.

F. Lindsten and T. B. Sch¨on. Backward simulation methods for Monte Carlo statistical inference.

Foundations and Trends in Machine Learning, 6(1):1–143, 2013.

F. Lindsten, M. I. Jordan, and T. B. Sch¨on. Particle Gibbs with ancestor sampling. Journal of

Machine Learning Research, 15:2145–2184, 2014.

Jun S Liu. Monte Carlo strategies in scientiﬁc computing. Springer Science & Business Media,

2001.

Claire Monteleoni, Gavin A. Schmidt, Francis Alexander, Alexandru Niculescu-Mizil, Karsten
Steinhaeuser, Michael Tippett, Arindam Banerjee, M. Benno Blumenthal, Jason E. Smerdon Au-
In Ting Yu, Nitesh Chawla, and
roop R. Ganguly, and Marco Tedesco. Climate informatics.
Simeon Simoff, editors, Computational Intelligent Data Analysis for Sustainable Development.
Chapman and Hall/CRC, London, 2013.

Christian A. Naesseth, Fredrik Lindsten, and Thomas B. Sch¨on. Capacity estimation of two-
In The 2014 IEEE Information Theory

dimensional channels using sequential Monte Carlo.
Workshop (ITW), pages 431–435, Nov 2014a.

Christian A. Naesseth, Fredrik Lindsten, and Thomas B Sch¨on. Sequential Monte Carlo for graphi-
cal models. In Advances in Neural Information Processing Systems 27, pages 1862–1870. Curran
Associates, Inc., 2014b.

Michael K Pitt and Neil Shephard. Filtering via simulation: Auxiliary particle ﬁlters. Journal of the

American statistical association, 94(446):590–599, 1999.

P. Rebeschini and R. van Handel. Can local particle ﬁlters beat the curse of dimensionality? Ann.

Appl. Probab. (to appear), 2015.

Raton, FL, USA, 2005.

H. Rue and L. Held. Gaussian Markov Random Fields, Theory and Applications. CDC Press, Boca

S. D. Schubert, M. J. Suarez, P. J. Pegion, R. D. Koster, and J. T. Bacmeister. On the cause of the

1930s dust bowl. Science, 303:1855–1859, 2004.

30

NESTED SEQUENTIAL MONTE CARLO METHODS

R. H. Shumway and D. S. Stoffer. Time Series Analysis and Its Applications – with R examples.

Springer Texts in Statistics. Springer, New York, USA, third edition, 2011.

M.-N. Tran, M. Scharth, M. K. Pitt, and R. Kohn.

Importance sampling squared for Bayesian

inference in latent variable models. ArXiv:1309.3339, sep 2013.

Christelle Verg´e, Cyrille Dubarry, Pierre Del Moral, and Eric Moulines. On parallel implementation
of sequential Monte Carlo methods: the island particle model. Statistics and Computing, 25(2):
243–260, 2015.

Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational

inference. Foundations and Trends R
(cid:13)

in Machine Learning, 1(1-2):1–305, 2008.

N. Whiteley. Discussion on Particle Markov chain Monte Carlo methods. Journal of the Royal

Statistical Society: Series B, 72(3):306–307, 2010.

C. K. Wikle. Modern perspectives on statistics for spatio-temporal data. WIREs Computational

Statistics, 7(1):86–98, 2015.

31

Technical report

Nested Sequential Monte Carlo Methods

Christian A. Naesseth, Fredrik Lindsten and Thomas B. Sch¨on

Please cite this version:

•

Christian A. Naesseth, Fredrik Lindsten and Thomas B. Sch¨on. Nested Sequential
Monte Carlo Methods. In Proceedings of the 32 nd International Conference on Ma-
chine Learning, Lille, France, 2015. JMLR: W&CP volume 37.

Abstract
We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences
of probability distributions, even where the random variables are high-dimensional. NSMC gen-
eralises the SMC framework by requiring only approximate, properly weighted, samples from the
SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC
can in itself be used to produce such properly weighted samples. Consequently, one NSMC sam-
pler can be used to construct an efﬁcient high-dimensional proposal distribution for another NSMC
sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to
consider complex and high-dimensional models using SMC. We show results that motivate the
efﬁcacy of our approach on several ﬁltering problems with dimensions in the order of 100 to 1 000.
Keywords: high-dimensional inference, high-dimensional particle ﬁlter, exact approximation, op-
spatio-temporal models
timal proposal,

sequential Monte Carlo,

importance sampling,

5
1
0
2
 
p
e
S
 
1
1
 
 
]

O
C

.
t
a
t
s
[
 
 
3
v
6
3
5
2
0
.
2
0
5
1
:
v
i
X
r
a

NAESSETH, LINDSTEN AND SCH ¨ON

Nested Sequential Monte Carlo Methods

Christian A. Naesseth
Link¨oping University, Link¨oping, Sweden

CHRISTIAN.A.NAESSETH@LIU.SE

Fredrik Lindsten
The University of Cambridge, Cambridge, United Kingdom

FREDRIK.LINDSTEN@ENG.CAM.AC.UK

Thomas B. Sch¨on
Uppsala University, Uppsala, Sweden

THOMAS.SCHON@IT.UU.SE

Abstract
We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences
of probability distributions, even where the random variables are high-dimensional. NSMC gen-
eralises the SMC framework by requiring only approximate, properly weighted, samples from the
SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC
can in itself be used to produce such properly weighted samples. Consequently, one NSMC sam-
pler can be used to construct an efﬁcient high-dimensional proposal distribution for another NSMC
sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to
consider complex and high-dimensional models using SMC. We show results that motivate the
efﬁcacy of our approach on several ﬁltering problems with dimensions in the order of 100 to 1 000.
Keywords:
high-dimensional inference, high-dimensional particle ﬁlter, exact approximation,
optimal proposal, sequential Monte Carlo, importance sampling, spatio-temporal models

1. Introduction

Inference in complex and high-dimensional statistical models is a very challenging problem that is
ubiquitous in applications. Examples include, but are deﬁnitely not limited to, climate informatics
(Monteleoni et al., 2013), bioinformatics (Cohen, 2004) and machine learning (Wainwright and
Jordan, 2008). In particular, we are interested in sequential Bayesian inference, which involves
computing integrals of the form

¯πk(f ) := E¯πk [f (X1:k)] =

f (x1:k)¯πk(x1:k)dx1:k,

(1)

for some sequence of probability densities

¯πk(x1:k) = Z−1

πk πk(x1:k),
with normalisation constants Zπk = (cid:82) πk(x1:k)dx1:k. Note that x1:k := (x1, . . . , xk)
Xk. The
typical scenario that we consider is the well-known problem of inference in time series or state space
models (Shumway and Stoffer, 2011; Capp´e et al., 2005). Here the index k corresponds to time and
we want to process some observations y1:k in a sequential manner to compute expectations with
respect to the ﬁltering distribution ¯πk(dxk) = P(Xk
y1:k). To be speciﬁc, we are interested
in settings where

dxk

1,

≥

∈

∈

k

|

(2)

(i) Xk is high-dimensional, i.e. Xk

Rd with d

1, and

∈

(cid:29)

(cid:90)

2

NESTED SEQUENTIAL MONTE CARLO METHODS

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

k

1

−

k

· · ·

k + 1

Figure 1: Example of a spatio-temporal model where ¯πk(x1:k) is described by a k
R2×3.

graphical model and xk

×

×

2

3 undirected

∈

(ii) there are local dependencies among the latent variables X1:k, both w.r.t. time k and between

the individual components of the (high-dimensional) vectors Xk.

One example of the type of models we consider are the so-called spatio-temporal models (Wikle,
2015; Cressie and Wikle, 2011; Rue and Held, 2005). In Figure 1 we provide a probabilistic graph-
ical model representation of a spatio-temporal model that we will explore further in Section 6.

Sequential Monte Carlo (SMC) methods, reviewed in Section 2.1, comprise one of the most
successful methodologies for sequential Bayesian inference. However, SMC struggles in high-
dimensions and these methods are rarely used for dimensions, say, d
10 (Rebeschini and van
Handel, 2015). The purpose of the NSMC methodology is to push this limit well beyond d = 10.

≥

The basic strategy, described in Section 2.2, is to mimic the behaviour of a so-called fully
adapted SMC algorithm. Full adaptation can drastically improve the efﬁciency of SMC in high di-
mensions. Unfortunately, it can rarely be implemented in practice since the fully adapted proposal
distributions are typically intractable. NSMC addresses this difﬁculty by requiring only approxi-
mate, properly weighted, samples from the proposal distribution. The proper weighting condition
ensures the validity of NSMC, thus providing a generalisation of the family of SMC methods. Fur-
thermore, NSMC will itself produce properly weighted samples. Consequently, it is possible to
use one NSMC procedure within another to construct efﬁcient high-dimensional proposal distribu-
tions. This nesting of the algorithm can be done to an arbitrary degree. For instance, for the model
depicted in Figure 1 we could use three nested samplers, one for each dimension of the “volume”.
The main methodological development is concentrated to Sections 3–4. We introduce the con-
cept of proper weighting, approximations of the proposal distribution, and nesting of Monte Carlo
algorithms. Throughout Section 3 we consider simple importance sampling and in Section 4 we
extend the development to the sequential setting.

We deliberately defer the discussion of the existing body of related work until Section 5, to open
up for a better understanding of the relationships to the new developments presented in Sections 3–
4. We also discuss various attractive features of NSMC that are of interest in high-dimensional
settings, e.g. the fact that it is easy to distribute the computation, which results in improved memory
efﬁciency and lower communication costs. Section 6 proﬁles our method extensively with a state-of-
the-art competing algorithm on several high-dimensional data sets. We also show the performance

3

NAESSETH, LINDSTEN AND SCH ¨ON

of inference and the modularity of the method on a d = 1 056 dimensional climatological spatio-
temporal model (Fu et al., 2012) structured according to Figure 1. Finally, in Section 7 we conclude
the paper with some ﬁnal remarks.

2. Background and Inference Strategy

2.1 Sequential Monte Carlo

Evaluating ¯πk(f ) as well as the normalisation constant Zπk in (2) is typically intractable and we
need to resort to approximations. SMC methods, or particle ﬁlters (PF), constitute a popular class
of numerical approximations for sequential inference problems. Here we give a high-level intro-
duction to the concepts underlying SMC methods, and postpone the details to Section 4. For a more
extensive treatment we refer to Doucet and Johansen (2011); Capp´e et al. (2005); Doucet et al.
(2001). In particular, we will use the auxiliary SMC method as proposed by Pitt and Shephard
(1999).

At iteration k

−

1, the SMC sampler approximates the target distribution ¯πk−1 by a collection of
N
i=1. These samples deﬁne an empirical point-mass

1:k−1, W i

(X i

weighted particles (samples)
approximation of the target distribution

{

k−1)
}

¯πN
k−1(dx1:k−1) :=

N
(cid:88)

i=1

W i
k−1
(cid:96) W (cid:96)

(cid:80)

k−1

δX i

1:k−1

(dx1:k−1),

(3)

where δX (dx) denotes a Dirac measure at X. Each iteration of the SMC algorithm can then con-
ceptually be described by three steps, resampling, propagation, and weighting.

The resampling step puts emphasis on the most promising particles by discarding the unlikely
ones and duplicating the likely ones. The propagation and weighting steps essentially correspond to
using importance sampling when changing the target distribution from ¯πk−1 to ¯πk, i.e. simulating
new particles from a proposal distribution and then computing corresponding importance weights.

2.2 Adapting the Proposal Distribution

The ﬁrst working SMC algorithm was the bootstrap PF by Gordon et al. (1993), which propagates
particles by sampling from the system dynamics and computes importance weights according to the
observation likelihood (in the state space setting). However, it is well known that the bootstrap PF
suffers from weight collapse in high-dimensional settings (Bickel et al., 2008), i.e. the estimate is
dominated by a single particle with weight close to one. This is an effect of the mismatch between
the importance sampling proposal and the target distribution, which typically gets more pronounced
in high dimensions.

More efﬁcient proposals, partially alleviating the degeneracy issue for some models, can be de-
signed by adapting the proposal distribution to the target distribution (see Section 4.2). In Naesseth
et al. (2014a) we make use of the fully adapted SMC method (Pitt and Shephard, 1999) for doing
inference in a (fairly) high-dimensional discrete model where xk is a 60-dimensional discrete vec-
tor. We can then make use of forward ﬁltering and backward simulation, operating on the individual
components of each xk, in order to sample from the fully adapted SMC proposals. However, this
method is limited to models where the latent space is either discrete or Gaussian and the optimal
proposal can be identiﬁed with a tree-structured graphical model. Our development here can be

4

NESTED SEQUENTIAL MONTE CARLO METHODS

seen as a non-trivial extension of this technique. Instead of coupling one SMC sampler with an
exact forward ﬁlter/backward simulator (which in fact reduces to an instance of standard SMC),
we derive a way of coupling multiple SMC samplers and SMC-based backward simulators. This
allows us to construct procedures for mimicking the efﬁcient fully adapted proposals for arbitrary
latent spaces and structures in high-dimensional models.

3. Proper Weighting and Nested Importance Sampling

In this section we will lay the groundwork for the derivation of the class of NSMC algorithms.
We start by considering the simpler case of importance sampling (IS), which is a fundamental
component of SMC, and introduce the key concepts that we make use of. In particular, we will use
a (slightly nonstandard) presentation of an algorithm as an instance of a class, in the object-oriented
sense, and show that these classes can be nested to an arbitrary degree.

3.1 Exact Approximation of the Proposal Distribution

i=1 W i)−1 (cid:80)N

i=1 W if (X i), with W i = Zqπ(X i)

Let ¯π(x) = Z−1
π π(x) be a target distribution of interest. IS can be used to estimate an expectation
¯π(f ) := E¯π[f (X)] by sampling from a proposal distribution ¯q(x) = Z−1
q q(x) and computing the
estimator ((cid:80)N
N
i=1 are the
, and where
}
weighted samples. It is possible to replace the IS weight by a nonnegative unbiased estimate, and
still obtain a valid (consistent, etc.) algorithm (Liu, 2001, p. 37). One way to motivate this approach
is by considering the random weight to be an auxiliary variable and to extend the target distribution
accordingly. Our development is in the same ﬂavour, but we will use a more explicit condition on
the relationship between the random weights and the simulated particles. Speciﬁcally, we will make
use of the following key property to formally justify the proposed algorithms.

(X i, W i)

q(X i)

{

Deﬁnition 1 (Properly weighted sample). A (random) pair (X, W ) is properly weighted for an
0 and E[f (X)W ] = p(f ) := (cid:82) f (x)p(x)dx for all measurable
unnormalised distribution p if W
functions f .

≥

{

≡

(cid:80)N

(cid:104) 1
N

(X i, W i)

i=1 W i(cid:105)

= (cid:82) p(x)dx =: Zp.

Note that proper weighting of

malising constant of p. Indeed, taking f (x)

N
i=1 implies unbiasedness of the estimate of the nor-
}
1 gives E
Interestingly, to construct a valid IS algorithm for our target ¯π it is sufﬁcient to generate samples
that are properly weighted w.r.t. the proposal distribution q. To formalise this claim, assume that we
are not able to simulate exactly from ¯q, but that it is possible to evaluate the unnormalised density
q point-wise. Furthermore, assume we have access to a class Q, which works as follows. The
constructor of Q requires the speciﬁcation of an unnormalised density function, say, q, which will
be approximated by the procedures of Q. Furthermore, to highlight the fact that we will typically use
IS (and SMC) to construct Q, the constructor also takes as an argument a precision parameter M ,
corresponding to the number of samples used by the “internal” Monte Carlo procedure. An object
is then instantiated as q = Q(q, M ). The class Q is assumed to have the following properties:

(A1) Let q = Q(q, M ). Assume that:

1. The construction of q results in the generation of a (possibly random) member variable, ac-
cessible as (cid:98)Zq = q.GetZ(). The variable (cid:98)Zq is a nonnegative, unbiased estimate of the nor-
malising constant Zq = (cid:82) q(x)dx.

5

NAESSETH, LINDSTEN AND SCH ¨ON

2. Q has a member function Simulate which returns a (possibly random) variable X = q.Simulate(),

such that (X, (cid:98)Zq) is properly weighted for q.

With the deﬁnition of Q in place, it is possible to generalise1 the basic importance sampler as in
N
(X i, W i)
Algorithm 1, which generates weighted samples
i=1 targeting ¯π. Note that Algorithm 1
}
{
is different from a random weight IS, since it approximates the proposal distribution (and not just
the importance weights).

Algorithm 1 Nested IS (steps 1–3 for i = 1, . . . , N )

1. Initialise qi = Q(q, M ).

2. Set (cid:98)Zi

q = qi.GetZ() and X i = qi.Simulate().

3. Set W i =

qπ(X i)
(cid:98)Zi
q(X i)

.

4. Compute (cid:98)Zπ = 1
N

(cid:80)N

i=1 W i.

To see the validity of Algorithm 1 we can interpret the sampler as a standard IS algorithm for
an extended target distribution, deﬁned as ¯Π(x, u) := u ¯Q(x, u)¯π(x)q−1(x), where ¯Q(x, u) is the
joint PDF of the random pair (q.Simulate(), q.GetZ()). Note that ¯Π is indeed a PDF that admits ¯π
as a marginal; for any measurable subset A

X,

⊆

¯Π(A

R+) =

1A(x)

¯Q(x, u)dxdu = E

(cid:90)

u ¯π(x)
q(x)

×

(cid:20)

(cid:98)Zq

(cid:21)

1A(X)¯π(X)
q(X)

(cid:18)

= ¯q

1A

(cid:19)

¯π
q

Zq = ¯π(A),

where the penultimate equality follows from the fact that (X, (cid:98)Zq) is properly weighted for q. Fur-
thermore, the standard unnormalised IS weight for a sampler with target ¯Π and proposal ¯Q is given
by u π/q, in agreement with Algorithm 1.

Algorithm 1 is an example of what is referred to as an exact approximation; see e.g., Andrieu and
Roberts (2009); Andrieu et al. (2010). Algorithmically, the method appears to be an approximation
of an IS, but samples generated by the algorithm nevertheless target the correct distribution ¯π.

3.2 Modularity of Nested IS

To be able to implement Algorithm 1 we need to deﬁne a class Q with the required properties
(A1). The modularity of the procedure (as well as its name) comes from the fact that we can use
Algorithm 1 also in this respect. Indeed, let us now view ¯π—the target distribution of Algorithm 1—
as the proposal distribution for another Nested IS procedure and consider the following deﬁnition
of Q:

1. Algorithm 1 is executed at the construction of the object p = Q(π, N ), and p.GetZ() returns

the normalising constant estimate (cid:98)Zπ.

2. p.Simulate() simulates a categorical random variable B with P(B = i) = W i/ (cid:80)N

(cid:96)=1 W (cid:96)

and returns X B.

1. With q.GetZ() (cid:55)→ Z and q.Simulate() returning a sample from ¯q we obtain the standard IS method.

6

NESTED SEQUENTIAL MONTE CARLO METHODS

Now, for any measurable f we have,

E[f (X B) (cid:98)Zπ] =

E

f (X i) (cid:98)Zπ

(cid:20)

N
(cid:88)

i=1

(cid:21)

=

W i
N (cid:98)Zπ

1
N

N
(cid:88)

i=1

(cid:34)
f (X i)

E

(cid:35)

qπ(X i)
(cid:98)Zi
q(X i)

= ¯q

(cid:19)

(cid:18) f π
q

Zq = ¯π(f )Zπ,

where, again, we use the fact that (X i, (cid:98)Zi
is properly weighted for π and that our deﬁnition of Q(π, N ) indeed satisﬁes condition (A1).

q) is properly weighted for q. This implies that (X B, (cid:98)Zπ)

The Nested IS algorithm in itself is unlikely to be of direct practical interest. However, in the
next section we will, essentially, repeat the preceding derivation in the context of SMC to develop
the NSMC method.

4. Nested Sequential Monte Carlo

4.1 Fully Adapted SMC Samplers

Let us return to the sequential inference problem. As before, let ¯πk(x1:k) = Z−1
πk πk(x1:k) denote
the target distribution at “time” k. The unnormalised density πk can be evaluated point-wise, but
the normalising constant Zπk is typically unknown. We will use SMC to simulate sequentially
n
k=1. In particular, we consider the fully adapted SMC sampler (Pitt
from the distributions
and Shephard, 1999), which corresponds to a speciﬁc choice of resampling weights and proposal
distribution, chosen in such a way that the importance weights are all equal to 1/N . Speciﬁcally,
the proposal distribution (often referred to as the optimal proposal) is given by ¯qk(xk
x1:k−1) =
Zqk (x1:k−1)−1qk(xk

x1:k−1), where

¯πk

{

}

|

|

qk(xk

x1:k−1) :=

|

πk(x1:k)
πk−1(x1:k−1)

.

In addition, the normalising “constant” Zqk (x1:k−1) = (cid:82) qk(xk
the resampling weights, i.e. the particles at time k
before they are propagated to time k. For notational simplicity, we use the convention x1:0 =
q1(x1
|
Algorithm 2.

x1:k−1)dxk is further used to deﬁne
1 are resampled according to Zqk (x1:k−1)
,
∅
x1:0) = π1(x1) and Zq1(x1:0) = Zπ1. The fully adapted auxiliary SMC sampler is given in

−

|

k}

As mentioned above, at each iteration k = 1, . . . , n, the method produces unweighted samples
N
X i
i=1 approximating ¯πk. It also produces an unbiased estimate (cid:98)Zπk of Zπk (Del Moral, 2004,
{
Proposition 7.4.1). The algorithm is expressed in a slightly non-standard form; at iteration k we
loop over the ancestor particles, i.e. the particles after resampling at iteration k
1, and let each
ancestor particle j generate mj
k offsprings. (The variable L is just for bookkeeping.) This is done
to clarify the connection with the NSMC procedure below. Furthermore, we have included a (com-
N
pletely superﬂuous) resampling step at iteration k = 1, where the “dummy variables”
i=1
{
N
i=1. The analogue of
are resampled according to the (all equal) weights
this step is, however, used in the NSMC algorithm, where the initial normalising constant Zπ1 is
estimated. We thus have to resample the corresponding initial particle systems accordingly.

Zπ1}
{

Zq1(X i

N
i=1 =

1:0)
}

1:0}

X i

−

{

7

NAESSETH, LINDSTEN AND SCH ¨ON

Algorithm 2 SMC (fully adapted)

1. Set (cid:98)Zπ0 = 1.

2. for k = 1 to n

(a) Compute (cid:98)Zπk = (cid:98)Zπk−1 ×
(b) Draw m1:N
k
1, . . . , N .

(c) Set L

0

←

(d) for j = 1 to N
i. Draw X i

ii. Set L

¯qk(
k ∼
· |
L + mj
k.

←

1
N

(cid:80)N

j=1 Zqk (X j

1:k−1).

from a multinomial distribution with probabilities

1:k−1)

Zqk (X j
(cid:96)=1 Zqk (X (cid:96)

1:k−1)

(cid:80)N

, for j =

X j

1:k−1) and let X i

1:k = (X j

1:k−1, X i

k) for i = L+1, . . . , L+mj
k.

4.2 Fully Adapted Nested SMC Samplers

In analogue with Section 3, assume now that we are not able to simulate exactly from ¯qk, nor
compute Zqk . Instead, we have access to a class Q which satisﬁes condition (A1). The proposed
NSMC method is then given by Algorithm 3.

Algorithm 3 Nested SMC (fully adapted)

1. Set (cid:98)Zπ0 = 1.

2. for k = 1 to n

1:k−1), M ) for j = 1, . . . , N .

X j

· |
qk = qj.GetZ() for j = 1, . . . , N .
j=1 (cid:98)Zj
qk

(a) Initialise qj = Q(qk(
(b) Set (cid:98)Zj
(c) Compute (cid:98)Zπk = (cid:98)Zπk−1 ×
(d) Draw m1:N

(cid:110) 1
N

(cid:80)N

(cid:111)

.

from a multinomial distribution with probabilities

for j =

(cid:98)Zj
qk
(cid:96)=1 (cid:98)Z(cid:96)
qk

(cid:80)N

i. Compute X i

k = qj.Simulate() and let X i

1:k = (X j

1:k−1, X i

k) for i = L+1, . . . , L+

k
1, . . . , N .

(e) Set L

0

←

(f) for j = 1 to N

mj
k.
ii. delete qj.
iii. Set L

←

L + mj
k.

8

NESTED SEQUENTIAL MONTE CARLO METHODS

Algorithm 3 can be seen as an exact approximation of the fully adapted SMC sampler in Al-
gorithm 2.
(In Appendix A.1 we provide a formulation of NSMC with arbitrary proposals and
resampling weights.) We replace the exact computation of Zqk and exact simulation from ¯qk, by the
approximate procedures available through Q. Despite this approximation, however, Algorithm 3 is
a valid SMC method. This is formalised by the following theorem.

Theorem 2. Assume that Q satisﬁes condition (A1). Then, under certain regularity conditions on
the function f : Xk
k (f ), both speciﬁed in Appendix A.1.2,
we have

Rd and for an asymptotic variance ΣM

(cid:55)→

N 1/2

(cid:32)

1
N

N
(cid:88)

i=1

(cid:33)

D

f (X i

1:k)

¯πk(f )

−

−→ N

(0, ΣM

k (f )),

where

X i

{

1:k}

M

i=1 are generated by Algorithm 3 and D
−→

denotes convergence in distribution.

Proof See Appendix A.1.2.

Remark 3. The key point with Theorem 2 is that, under certain regularity conditions, the NSMC
method converges at rate √N even for a ﬁxed (and ﬁnite) value of the precision parameter M . The
asymptotic variance ΣM
k (f ), however, will depend on the accuracy and properties of the approxi-
mative procedures of Q. We leave it as future work to establish more informative results, relating
the asymptotic variance of NSMC to that of the ideal, fully adapted SMC sampler.

4.3 Backward Simulation and Modularity of NSMC

As previously mentioned, the NSMC procedure is modular in the sense that we can make use of
Algorithm 3 also to deﬁne the class Q. Thus, we now view ¯πn as the proposal distribution that we
wish to approximately sample from using NSMC. Algorithm 3 directly generates an estimate (cid:98)Zπn
of the normalising constant of πn (which indeed is unbiased, see Theorem 6). However, we also
need to generate a sample (cid:101)X1:n such that ( (cid:101)X1:n, (cid:98)Zπn) is properly weighted for πn.

1, . . . , N

The simplest approach, akin to the Nested IS procedure described in Section 3.2, is to draw Bn
1:n . This will indeed result in a valid deﬁnition
uniformly on
of the Simulate procedure. However, this approach will suffer from the well known path degen-
eracy of SMC samplers. In particular, since we call qj.Simulate() multiple times in Step 2(f)i of
Algorithm 3, we risk to obtain (very) strongly correlated samples by this simple approach.

and return (cid:101)X1:n = X Bn

{

}

It is possible to improve the performance of the above procedure by instead making use of
a backward simulator (Godsill et al., 2004; Lindsten and Sch¨on, 2013) to simulate (cid:101)X1:n. The
backward simulator, given in Algorithm 4, is a type of smoothing algorithm; it makes use of the
particles generated by a forward pass of Algorithm 3 to simulate backward in “time” a trajectory
(cid:101)X1:n approximately distributed according to ¯πn.

Remark 4. Algorithm 4 assumes unweighted particles and can thus be used in conjunction with the
fully adapted NSMC procedure of Algorithm 2. If, however, the forward ﬁlter is not fully adapted
the weights need to be accounted for in the backward simulation; see Appendix A.1.3.

The modularity of NSMC is established by the following result.

9

NAESSETH, LINDSTEN AND SCH ¨ON

Algorithm 4 Backward simulator (fully adapted)

1. Draw Bn uniformly on

1, . . . , N

{

.

}

2. Set (cid:101)Xn = X Bn
n .

3. for k = n

1 to 1

−
(a) Compute (cid:102)W j

k =

πn((X j

1:k, (cid:101)Xk+1:n))
πk(X j
1:k)

for j = 1, . . . , N .

(b) Draw Bk from a categorical distribution with probabilities

for j = 1, . . . , N .

(cid:102)W j
k
(cid:96)=1 (cid:102)W (cid:96)
k

(cid:80)N

(c) Set (cid:101)Xk:n = (X Bk

k , (cid:101)Xk+1:n).

Deﬁnition 5. Let p = Q(πn, N ) be deﬁned as follows:

1. The constructor executes Algorithm 3 with target distribution πn and with N particles, and

p.GetZ() returns the estimate of the normalising constant (cid:98)Zπn.

2. p.Simulate() executes Algorithm 4 and returns (cid:101)X1:n.

Theorem 6. The class Q deﬁned as in Deﬁnition 5 satisﬁes condition (A1).

Proof See Appendix A.1.3.

A direct, and important, consequence of Theorem 6 is that NSMC can be used as a component of
powerful learning algorithms, such as the particle Markov chain Monte Carlo (PMCMC) method
(Andrieu et al., 2010) and many of the other methods discussed in Section 5. Since standard SMC
is a special case of NSMC, Theorem 6 implies proper weighting also of SMC.

5. Practicalities and Related Work

There has been much recent interest in using SMC within SMC in various ways. The SMC2 by
Chopin et al. (2013) and the recent method by Crisan and M´ıguez (2013) are sequential learning
algorithms for state space models, where one SMC sampler for the parameters is coupled with an-
other SMC sampler for the latent states. Johansen et al. (2012) and Chen et al. (2011) address the
state inference problem by splitting the state variable into different components and run coupled
SMC samplers for these components. These methods differ substantially from NSMC; they solve
different problems and the “internal” SMC sampler(s) is constructed in a different way (for approxi-
mate marginalisation instead of for approximate simulation). Another related method is the random
weights PF of Fearnhead et al. (2010a), requiring exact samples from ¯q and where the importance
weights are estimated using a nested Monte Carlo algorithm.

The method most closely related to NSMC is the space-time particle ﬁlter (ST-PF) (Beskos
et al., 2014a), which has been developed independently and in parallel with our work. The ST-PF
is also designed for solving inference problems in high-dimensional models. It can be seen as a
island PF (Verg´e et al., 2015) implementation of the method presented by Naesseth et al. (2014b).
Speciﬁcally, for a spatio-temporal models they run an island PF over both spatial and temporal

10

NESTED SEQUENTIAL MONTE CARLO METHODS

dimensions. However, the ST-PF does not generate an approximation of the fully adapted SMC
sampler.

Another key distinction between NSMC and ST-PF is that in the latter each particle in the
“outer” SMC sampler comprises a complete particle system from the “inner” SMC sampler. For
NSMC, on the other hand, the particles will simply correspond to different hypotheses about the
latent variables (as in standard SMC), regardless of how many samplers that are nested. This is
a key feature of NSMC, since it implies that it is easily distributed over the particles. The main
N
j=1 and the calls to the Simulate
computational effort of Algorithm 3 is the construction of
}
procedure, which can be done independently for each particle. This leads to improved memory
efﬁciency and lower communication costs. Furthermore, we have found (see Section 6) that NSMC
can outperform ST-PF even when run on a single machine with matched computational costs.

qj

{

Another strength of NSMC methods are their relative ease of implementation, which we show
in Section 6.3. We use the framework to sample from what is essentially a cubic grid Markov ran-
dom ﬁeld (MRF) model just by implementing three nested samplers, each with a target distribution
deﬁned on a simple chain.

There are also other SMC-based methods designed for high-dimensional problems, e.g., the
block PF studied by Rebeschini and van Handel (2015), the location particle smoother by Briggs
et al. (2013) and the PF-based methods reviewed in Djuric and Bugallo (2013). However, these
methods are all inconsistent, as they are based on various approximations that result in systematic
errors.

The previously mentioned PMCMC (Andrieu et al., 2010) is a related method, where SMC
is used as a component of an MCMC algorithm. We make use of a very similar extended space
approach to motivate the validity of our algorithm. Note that our proposed algorithm can be used as
a component in PMCMC and most of the other algorithms mentioned above, which further increases
the scope of models it can handle.

6. Experimental Results

We illustrate NSMC on three high-dimensional examples, both with real and synthetic data. We
compare NSMC with standard (bootstrap) PF and the ST-PF of Beskos et al. (2014a) with equal
computational budgets on a single machine (i.e., neglecting the fact that NSMC is more easily dis-
tributed). These methods are, to the best of our knowledge, the only other available consistent online
methods for full Bayesian inference in general sequential models. For more detailed explanations
of the models and additional results, see Appendix A.32.

6.1 Gaussian State Space Model

=

Xk, Yk
{

We start by considering a high-dimensional Gaussian state space model, where we have access to
the true solution through belief propagation. The latent variables and measurements
,
}
d
with
k lattice Gaussian MRF. The true data is
l=1, are modeled by a d
simulated from a nearly identical state space model (see Appendix A.3.1). We run a 2-level NSMC
sampler. The outer level is fully adapted, i.e. the proposal distribution is qk = p(xk
xk−1, yk),
which thus constitute the target distribution for the inner level. To generate properly weighted
samples from qk, we use a bootstrap PF operating on the d components of the vector xk. Note that

Xk,l, Yk,l
{

X1:k, Y1:k

×

}

}

{

|

2. Code available at https://github.com/can-cs/nestedsmc

11

NAESSETH, LINDSTEN AND SCH ¨ON

d = 50

d = 100

d = 200

S
S
E

S
R
E

Figure 2: Top: Median (over dimension) ESS (4) and 15–85% percentiles (shaded region). Bottom:
The ERS (5) based on the resampling weights in the (outermost) particle ﬁlter. The results
are based on 100 independent runs for the Gaussian MRF with dimension d.

we only use bootstrap proposals where the actual sampling takes place, and that the conditional
distribution p(xk

xk−1, yk) is not explicitly used.

We simulate data from this model for k = 1, . . . , 100 for different values of d = dim(xk)

50, 100, 200
{
with both the ST-PF and standard (bootstrap) PF.

∈
. The exact ﬁltering marginals are computed using belief propagation.We compare
}

|

The results are evaluated based on the effective sample size (ESS, see e.g. Fearnhead et al.

(2010b)) deﬁned as,

ESS(xk,l) =

(cid:18)

(cid:20)

E

((cid:98)xk,l−µk,l)2
σ2

k,l

(cid:21)(cid:19)−1

,

|

where (cid:98)xk,l denote the mean estimates and µk,l and σ2
k,l denote the true mean and variance of
y1:k obtained from belief propagation. The expectation in (4) is approximated by averag-
xk,l
ing over 100 independent runs of the involved algorithms. The ESS reﬂects the estimator accuracy,
obvious by the deﬁnition which is tightly related to the mean-squared-error. Intuitively the ESS
corresponds to the equivalent number of i.i.d. samples needed for the same accuracy.

We also consider the effective resample size (ERS, Kong et al. (1994)), which is based on the

resampling weights at the top levels in the respective SMC algorithms,

(4)

(5)

The ERS is an estimate of the effective number of unique particles (or particle systems in the case
of ST-PF) available at each resampling step.

We use N = 500 and M = 2

d for NSMC and match the computational time for ST-PF
and bootstrap PF. We report the results in Figure 2. The bootstrap PF is omitted from d = 100,

·

ERS =

(cid:16)(cid:80)N

i=1 (cid:98)Zi
qk
(cid:16)
(cid:98)Zi
qk

i=1

(cid:80)N

(cid:17)2
(cid:17)2 .

12

NESTED SEQUENTIAL MONTE CARLO METHODS

200 due to its poor performance already for d = 50 (which is to be expected). Each dimension
l = 1, . . . , d provides us with a value of the ESS, so we present the median (lines) and 15–85%
percentiles (shaded regions) in the ﬁrst row of Figure 2. The ERS is displayed in the second row of
Figure 2. Note that ESS gives a better reﬂection of estimation accuracy than ERS.

We have conducted additional experiments with different model parameters and different choices
for N and M (some additional results are given in Appendix A.3.1). Overall the results seem to be
in agreement with the ones presented here, however ST-PF seems to be more robust to the trade-off
between N and M . A rule-of-thumb for NSMC is to generally try to keep N as high as possible,
while still maintaining a reasonably large ERS.

6.2 Non-Gaussian State Space Model

Next, we consider an example with a non-Gaussian SSM, borrowed from Beskos et al. (2014a)
where the full details of the model are given. The transition probability p(xk
xk−1) is a localised
|
xk) is t-distributed. The model dimension
Gaussian mixture and the measurement probability p(yk
is d = 1 024. Beskos et al. (2014a) report improvements for ST-PF over both the bootstrap PF
and the block PF by Rebeschini and van Handel (2015). We use N = M = 100 for both ST-PF
and NSMC (the special structure of this model implies that there is no signiﬁcant computational
overhead from running backward sampling) and the bootstrap PF is given N = 10 000. In Figure 3

|

Figure 3: Median ESS with 15

85% percentiles (shaded region) for the non-Gaussian SSM.

−

we report the ESS (4), estimated according to Carpenter et al. (1999). The ESS for the bootstrap PF
is close to 0, for ST-PF around 1–2, and for NSMC slightly higher at 7–8. However, we note that all
methods perform quite poorly on this model, and to obtain satisfactory results it would be necessary
to use more particles.

6.3 Spatio-Temporal Model – Drought Detection

In this ﬁnal example we study the problem of detecting droughts based on measured precipitation
data (Jones and Harris, 2013) for different locations on earth. We look at the situation in North
America during the years 1901–1950 and the Sahel region in Africa during the years 1950–2000.
These spatial regions and time frames were chosen since they include two of the most devastating
droughts during the last century, the so-called Dust Bowl in the US during the 1930s (Schubert

13

et al., 2004) and the decades long drought in the Sahel region in Africa starting in the 1960s (Foley
et al., 2003; Hoerling et al., 2006). We consider the spatio-temporal model deﬁned by Fu et al.

NAESSETH, LINDSTEN AND SCH ¨ON

Xk−1

· · ·

Xk+1

· · ·

Xk,1:2,1

Xk,1:2,3

N
→

Xk

M1
→
Xk,1:2,2

↓ M2

↓ M2

↓ M2

Figure 4: Illustration of the three-level NSMC.

{

Xk,i,j

(2012) and compare with the results therein. Each location in a region is modelled to be in either
a normal state 0 or in an abnormal state 1 (drought). Measurements are given by precipitation (in
millimeters) for each location and year. At every time instance k our latent structure is described by
I,J
a rectangular 2D grid Xk =
i=1,j=1; in essence this is the model showcased in Figure 1. Fu
et al. (2012) considers the problem of ﬁnding the maximum aposteriori conﬁguration, using a linear
programming relaxation. We will instead compute an approximation of the full posterior ﬁltering
distribution ¯πk(xk) = p(xk
y1:k). The rectangular structure is used to instantiate an NSMC method
that on the ﬁrst level targets the full posterior ﬁltering distribution. To sample from Xk we run, on
the second level, an NSMC procedure that operates on the “columns” Xk,1:I,j, j = 1, . . . , J.
Finally, to sample each column Xk,1:I,j we run a third level of SMC, that operates on the individual
components Xk,i,j, i = 1, . . . , I, using a bootstrap proposal. The structure of our NSMC method
applied to this particular problem is illustrated in Figure 4.

}

|

0.5, 0.7, 0.9
{

Figure 5 gives the results on the parts of North America that we consider. The ﬁrst row shows the
, for both regions.
number of locations where the estimate of p(xk,i,j = 1) exceeds
}
These results seems to be in agreement with Fu et al. (2012, Figures 3, 6). However, we also
receive an approximation of the full posterior and can visualise uncertainty in our estimates, as
illustrated by the three different levels of posterior probability for drought. In general, we obtain
a rich sample diversity from the posterior distribution. However, for some problematic years the
sampler degenerates, with the result that the three credibility levels all coincide. This is also visible
in the second row of Figure 5, where we show the posterior estimates p(xk,i,j
y1:k) for the years
1939–1941, overlayed on the regions of interest. For year 1940 the sampler degenerates and only
reports 0-1 probabilities for all sites. Naturally, one way to improve the estimates is to run the
sampler with a larger number of particles, which has been kept very low in this proof-of-concept.

|

7. Conclusions

We have shown that a straightforward NSMC implementation with fairly few particles can attain
reasonable approximations to the ﬁltering problem for dimensions in the order of hundreds, or
even thousands. This means that NSMC methods takes the SMC framework an important step
closer to being viable for high-dimensional statistical inference problems. However, NSMC is not

14

NESTED SEQUENTIAL MONTE CARLO METHODS

North America region

Sahel region

North America 1939

North America 1940

North America 1941

Figure 5: Top: Number of locations with estimated p(x = 1) >

for the two regions.
Bottom: Estimate of p(xt,i = 1) for all sites over a span of 3 years. All results for
N = 100, N1 =

0.5, 0.7, 0.9
}

, N2 = 20.

{

30, 40
}

{

a silver bullet for solving high-dimensional inference problems, and the approximation accuracy
will be highly model dependent. Hence, much work remains to be done, for instance on combining
NSMC with other techniques for high-dimensional inference such as localisation (Rebeschini and
van Handel, 2015) and annealing (Beskos et al., 2014b), in order to solve even more challenging
problems.

Acknowledgments

This work was supported by the projects: Learning of complex dynamical systems (Contract num-
ber: 637-2014-466) and Probabilistic modeling of dynamical systems (Contract number: 621-2013-
5524), both funded by the Swedish Research Council.

15

NAESSETH, LINDSTEN AND SCH ¨ON

Appendix A. Appendix

In this appendix we start out in Section A.1 by providing a more general formulation of the NSMC
method and proofs of the central limit and proper weighting theorems of the main manuscript.
We also detail (Section A.2) a straightforward extension of nested IS to a sequential version. We
show that a special case of this nested sequential IS turns out to be more or less equivalent to the
importance sampling squared algorithm by Tran et al. (2013). This relationship serves as evidence
that illustrates that the NSMC framework being more widely applicable than the scope of problems
considered in this article. Finally, in Section A.3 we give more details and results on the experiments
considered in the main manuscript.

A.1 Nested Sequential Monte Carlo

We start by presenting a general formulation of a nested auxiliary SMC sampler in Algorithm 5. In
this formulation, qk(xk

x1:k−1) is an arbitrary (unnormalised) proposal, normalised by

|

(cid:90)

Zqk (x1:k−1) =

qk(xk

x1:k−1)dxk.

|

Furthermore, the resampling weights are obtain by multiplying the importance weights with the
arbitrary adjustment multipliers νk−1(x1:k−1, Zqk ), which may depend on both the state sequence
x1:k−1 and the normalising constant (estimate). The fully adapted NSMC sampler (Algorithm 3 in
the main document) is obtained as a special case if we choose

qk(xk

x1:k−1) =

|

πk(x1:k)
πk−1(x1:k−1)

and νk−1(x1:k−1, Zqk ) = Zqk , in which case the importance weights are indeed given by W i

1.

k ≡

A.1.1 NESTED SMC IS SMC

The validity of Algorithm 5 can be established by interpreting the algorithm as a standard SMC
procedure for a sequence of extended target distributions.
If (cid:98)Zqk is computed deterministically,
proper weighting (i.e., unbiasedness) ensures that (cid:98)Zqk = Zqk and it is evident that the algorithm
reduces to a standard SMC sampler. Hence, we consider the case when the normalising constant
estimates (cid:98)Zqk are random.

k−1(uk−1

For k = 1, . . . , n + 1, let us introduce the random variable Uk−1 which encodes the complete
internal state of the object q generated by q = Q(qk(
x1:k−1), M ). Let the distribution of Uk−1 be
denoted as ¯ψM
x1:k−1). To put Algorithm 5 into a standard (auxiliary) SMC framework,
we shall interpret steps 2a–2b of Algorithm 5 as being the last two steps carried out during iteration
1, rather than the ﬁrst two steps carried out during iteration k. This does not alter the algorithm
k
per se, but it results in that the resampling step is conducted ﬁrst at each iteration, which is typically
the case for standard auxiliary SMC formulations.

−

· |

|

The estimator of the normalising constant is computable from the internal state of q, so that we
can introduce a function τk such that (cid:98)Zqk = τk(Uk−1). Furthermore, note that the simulation of Xk
via Xk = q.Simulate() is based solely on the internal state Uk−1, and denote by ¯γM
Uk−1) the
distribution of Xk.

k (xk

|

16

NESTED SEQUENTIAL MONTE CARLO METHODS

Algorithm 5 Nested SMC (auxiliary SMC formulation)

i=1 to arbitrary dummy variables. Set W i
N

0 = 1 for i = 1, . . . , N . Set (cid:98)Zπ0 = 1.

1. Set

X i
0}
{
2. for k = 1 to n

(a) Initialise qj = Q(qk(
(b) Compute (cid:98)Zj
(c) Compute (cid:98)νj

X j

· |

qk = qj.GetZ() for j = 1, . . . , N .
1:k−1, (cid:98)Zj
k−1 = νk−1(X j

qk ) for j = 1, . . . , N .

1:k−1), M ) for j = 1, . . . , N .

from a multinomial distribution with probabilities

k−1W j
(cid:98)νj
(cid:80)N
(cid:96)=1 (cid:98)ν(cid:96)

k−1
k−1W (cid:96)

k−1

for

(d) Draw m1:N

k

j = 1, . . . , N .

(e) Set L

0

←

(f) for j = 1 to N

mj
k.

ii. Compute W i

k =

iii. delete qj.
iv. Set L

L + mj
k.

←
(g) Compute (cid:98)Zπk = (cid:98)Zπk−1 ×

(cid:110) 1
N

i. Compute X i

k = qj.Simulate() and let X i

1:k = (X j

1:k−1, X i

k) for i = L+1, . . . , L+

πk(X i
πk−1(X j

1:k)
1:k−1)

(cid:98)Zj
qk
(cid:98)νj
k−1qk(X i
k |

X j

1:k−1)

for i = L + 1, . . . , L + mj
k.

(cid:80)N

j=1 (cid:98)νj

k−1W j

k−1

(cid:111)

(cid:110)
((cid:80)N

×

j=1 W j

k )/((cid:80)N

j=1 W j

k−1)

(cid:111)

.

Lemma 7. Assume that Q satisﬁes condition (A1) in the main manuscript. Then,

(cid:90)

τk(uk−1)¯γM

k (xk

uk−1) ¯ψM

k−1(uk−1

x1:k−1)duk−1 = qk(xk

x1:k−1).

|

Proof The pair (Xk, τk(Uk−1)) are properly weighted for qk. Hence, for a measurable function f ,

E[f (Xk)τk(Uk−1)

x1:k−1] =

|

f (xk)τk(uk−1)¯γM
(cid:90)

k (xk

|

uk−1) ¯ψM

k−1(uk−1
(cid:90)

|

x1:k−1)duk−1dxk

= Zk(x1:k−1)

f (xk)¯qk(xk

x1:k−1)dxk =

f (xk)qk(xk

x1:k−1)dxk.

|

Since f is arbitrary, the result follows.

|

(cid:90) (cid:90)

|

|

We can now deﬁne the sequence of (unnormalised) extended target distributions for the Nested

SMC sampler as,

Πk(x1:k, u0:k) :=

τk(uk−1) ¯ψM

k (uk
|
qk(xk
|

x1:k)¯γM
x1:k−1)

k (xk

uk−1)

|

πk(x1:k)
πk−1(x1:k−1)

Πk−1(x1:k−1, u0:k−1),

and Π0(u0) = ¯ψM

0 (u0). We write Θk = Xk

Uk for the domain of Πk.

×

17

NAESSETH, LINDSTEN AND SCH ¨ON

|

|

Lemma 8. Assume that Q satisﬁes condition (A1) in the main manuscript. Then,

(cid:90)

τk(uk−1)¯γM

k (xk

|

uk−1)Πk−1(x1:k−1, u0:k−1)du0:k−1 = πk−1(x1:k−1)qk(xk

x1:k−1).

Proof The proof follows by induction. At k = 1, we have (cid:82) τ1(u0)¯γM
q1(x1) by Lemma 7. Hence, assume that the hypothesis holds for k

1 (x1

|

1 and consider

u0) ¯ψM

0 (u0)du0 =

≥

uk)τk(uk−1) ¯ψM

k (uk

x1:k)¯γM

uk−1)Πk−1(x1:k−1, u0:k−1)du0:k

(cid:90)

τk+1(uk)¯γM

k+1(xk+1

|
πk(x1:k)
πk−1(x1:k−1)qk(xk

uk)Πk(x1:k, u0:k)du0:k

=

(cid:90)

·

·

=

(cid:90)

=

x1:k−1)

|
k+1(xk+1

τk+1(uk)¯γM
πk(x1:k) (cid:0)(cid:82) τk+1(uk)¯γM

|

k+1(xk+1
|
πk−1(x1:k−1)qk(xk

k (xk
(cid:1)

|

x1:k)duk

|
k (uk

uk) ¯ψM
x1:k−1)

|

|

uk−1)Πk−1(x1:k−1, u0:k−1)du0:k−1

τk(uk−1)¯γM

k (xk

|
πk(x1:k)qk+1(xk+1

|

x1:k)πk−1(x1:k−1)qk(xk

x1:k−1)

|

= πk(x1:k)qk+1(xk+1

x1:k),

πk−1(x1:k−1)qk(xk

x1:k−1)

|

where the penultimate equality follows by applying Lemma 7 and the induction hypothesis to the
two integrals, respectively.

As a corollary to Lemma 8, it follows that

(cid:90)

Πk(x1:k, u0:k)du0:k = πk(x1:k).

(6)

Consequently, Πk is normalised by the same constant Zπk as πk, and by deﬁning ¯Πk(x1:k, u0:k) :=
Z−1
πk Πk(x1:k, u0:k) we obtain a probability distribution which admits ¯πk as a marginal (note that
¯Π0 = Π0, which is normalised by construction). This implies that we can use ¯Πk as a proxy
for ¯πk in a Monte Carlo algorithm, i.e., samples drawn from ¯Πk can be used to compute ex-
pectations w.r.t. ¯πk. This is precisely what Algorithm 5 does; it is a standard auxiliary SMC
sampler for the (unnormalised) target sequence Πk, k = 0, . . . , n, with adjustment multiplier
weights νk−1(x1:k−1, τk(uk−1)) and proposal distribution ¯γM
x1:k). The (stan-
dard) weight function for this sampler is thus given by

uk−1) ¯ψM

k (xk

k (uk

|

|

Wk(x1:k, u0:k)

πk(x1:k)
πk−1(x1:k−1)

∝

τk(uk−1)

,

(7)

νk−1(x1:k−1, τk(uk−1))qk(xk

x1:k−1)

|

which is the same as the expression on line 2(f)ii of Algorithm 5.

A.1.2 CENTRAL LIMIT THEOREM – PROOF OF THEOREM 2 IN THE MAIN MANUSCRIPT

Now that we have established that Nested SMC is in fact a standard auxiliary SMC sampler, albeit
on an extended state space, we can reuse existing convergence results from the SMC literature; see

18

NESTED SEQUENTIAL MONTE CARLO METHODS

e.g., Johansen and Doucet (2008); Douc and Moulines (2008); Douc et al. (2009); Chopin (2004)
or the extensive textbook by Del Moral (2004).

Here, in order to prove Theorem 2 of the main manuscript, we make use of the result for the
auxiliary SMC sampler by Johansen and Doucet (2008), which in turn is based on the central limit
theorem by Chopin (2004). The technique used by Johansen and Doucet (2008) is to reinterpret (as
detailed below) the auxiliary SMC sampler as a sequential importance sampling and resampling
(SISR) particle ﬁlter, by introducing the modiﬁed (unnormalised) target distribution

Π(cid:48)

k(x1:k, u0:k) := νk(x1:k, τk+1(uk))Πk(x1:k, u0:k).

(8)

The auxiliary SMC sampler described in the previous section can then be viewed as a SISR algo-
rithm for (8). Indeed, if we write ¯QM
uk−1) for
the joint proposal distribution of (xk, uk), then the weight function for this SISR sampler is given
by

x1:k−1, uk−1) := ¯ψM

k (xk, uk

x1:k)¯γM

k (xk

k (uk

|

|

|

W (cid:48)

k(x1:k, u0:k) :=

¯Π(cid:48)

k(x1:k, u0:k)

¯QM

k (xk, uk

x1:k−1, uk−1) ¯Π(cid:48)

k−1(x1:k−1, u0:k−1)

νk(x1:k, τk+1(uk))Wk(x1:k, u0:k),

∝

(9)

where Wk is deﬁned in (7). This weight expression thus accounts for both the importance weights
and the adjustment multipliers of the auxiliary SMC sampler formulation.

Since this SISR algorithm does not target ¯Πk (and thus not ¯πk) directly, we use an additional IS
step to compute estimators of expectations w.r.t. to ¯π. The proposal distribution for this IS procedure
is given by

¯Γk(x1:k, u0:k) := ¯QM

k (xk, uk

x1:k−1, uk−1) ¯Π(cid:48)

k−1(x1:k−1, u0:k−1).

(10)

Note that we obtain an approximation of (10) after the propagation Step 2(f)i of Algorithm 5, but
before the weighting step. The resulting IS weights, for target distribution ¯Πk(x1:k, u0:k) and with
proposal distribution (10), are given by

|

|

¯Πk(x1:k, u0:k)
¯Γk(x1:k, u0:k)

=: ωk(x1:k, u0:k)

Wk(x1:k, u0:k).

∝

Hence, with f : Xk
(cid:55)→
obvious abuse of notation) by the estimator

Rd being a test function of interest we can estimate E¯πk [f ] = E ¯Πk [f ] (with

N
(cid:88)

i=1

kf (X i
W i
1:k)
(cid:80)N
(cid:96)=1 W (cid:96)
k

,

(11)

which, again, is in agreement with Algorithm 5.

We have now reinterpreted the NSMC algorithm; ﬁrst as a standard auxiliary SMC sampler,
and then further as a standard SISR method. Consequently, we are now in the position of directly
applying, e.g., the central limit theorem by Chopin (2004, Theorem 1). The conditions and the
statement of the theorem are reproduced here for clarity.

19

NAESSETH, LINDSTEN AND SCH ¨ON

For any measurable function f : Θ0

measurable function f : Θk

Rd,

Rd, let (cid:101)V M

0 (f ) = Var ¯ψM

0

(cid:55)→

(f ) and deﬁne, for any

(cid:55)→
(cid:101)V M
k (f ) = (cid:98)V M
k−1(E ¯QM
V M
k (W (cid:48)
k (f ) = (cid:101)V M
(cid:98)V M
k (f ) = V M

k(f

k

[f ]) + E ¯Π(cid:48)
E ¯Π(cid:48)

[f ])),

k−1

−

k
(f ),

k (f ) + Var ¯Π(cid:48)

k

[Var ¯QM

k

(f )],

k > 0,

0,

0.

k

k

≥

≥

Deﬁne recursively Φk to be the set of measurable functions f : Θk
δ > 0 with E¯Γk [
2+δ] <
(cid:107)
(cid:107)
Φk−1. Furthermore, assume that the identity function f
≡
follows by Chopin (2004, Theorem 1 and Lemma A.1) that

W (cid:48)

kf

∞

and such that the function (x1:k−1, u0:k−1)

Rd such that there exists a
E ¯QM
kf ] is in
(cid:55)→
1 belongs to Φk for each k. Then, it

[W (cid:48)

(cid:55)→

k

N 1/2

(cid:32) N
(cid:88)

i=1

1
N

f (X i

1:k, U i

0:k)

E¯Γk [f ])

−

−→ N

(0, (cid:101)V M

k (f )),

(12)

(cid:33)

D

for any function f such that the function (x1:k−1, u0:k−1)
exists a δ > 0 such that E¯Γk [
(cid:107)
samples obtained after the propagation Step 2(f)i of Algorithm 5, but before the weighting step.

E¯Γk [f ]] is in Φk−1 and there
. The convergence in (12) thus holds for the unweighted

2+δ] <
(cid:107)

E ¯QM

(cid:55)→

∞

−

[f

f

k

To complete the proof, it remains to translate (12) into a similar result for the IS estimator (11).
To this end we make use of Chopin (2004, Lemma A.2) which is related to the IS correction step
Rd denote the
of the SMC algorithm. Speciﬁcally, for a function f : Xk
Rd such
extension of f to Θk, deﬁned by f e(x1:k, u0:k) = f (x1:k). Then, for any f : Xk
E ¯QM
[ωkf e] is in Φk−1 and there exists a δ > 0 such that
that the function (x1:k−1, u0:k−1)
E¯Γk [
2+δ] <
ωkf e
(cid:107)

Rd, let f e : Θk

, we have

(cid:55)→

(cid:55)→

(cid:55)→

∞

(cid:55)→

(cid:107)

k

N 1/2

(cid:32) N
(cid:88)

i=1

kf (X i
W i
1:k)
(cid:80)N
(cid:96)=1 W (cid:96)
k −

(cid:33)

D

¯πk(f )

−→ N

(0, ΣM

k (f )),

where

(X i
{

1:k, W i
k)

i=1 are generated by Algorithm 5 and ΣM
M
}

k (f ) = (cid:101)V M

k (ωk(f e

E ¯Πk [f e])).

−

A.1.3 NESTED SMC GENERATES PROPERLY WEIGHTED SAMPLES – PROOF OF THEOREM 6

IN THE MAIN MANUSCRIPT

In the previous two sections we showed that the NSMC procedure is a valid inference algorithm
for ¯πn. Next, we turn our attention to the modularity of the method and the validity of using the
algorithm as a component in another NSMC sampler. Let us start by stating a more general version
of the backward simulator in Algorithm 6. Clearly, if the forward NSMC procedure is fully adapted
W i

1, Algorithm 6 reduces to the backward simulator stated in the main manuscript.

k ≡
We will now show that the pair ( (cid:98)Zπn, (cid:101)X1:n) generated by Algorithms 5 and 6 is properly

weighted for πn(x1:n), and thereby prove Theorem 6 in the main manuscript.

The proof is based on the particle Markov chain Monte Carlo (PMCMC) construction (Andrieu
et al., 2010). The idea used by Andrieu et al. (2010) was to construct an extended target distribution,
incorporating all the random variables generated by an SMC sampler as auxiliary variables. This
opened up for using SMC approximations within MCMC in a provably correct way; these seem-
ingly approximate methods simply correspond to standard MCMC samplers for the (nonstandard)

20

NESTED SEQUENTIAL MONTE CARLO METHODS

Algorithm 6 Backward simulator

1. Draw Bn from a categorical distribution with probabilities

for j = 1, . . . , N .

2. Set (cid:101)Xn = X Bn
n .

3. for k = n

1 to 1

−
(a) Compute (cid:102)W j

k = W j

k

πn((X j

1:k, (cid:101)Xk+1:n))
πk(X j
1:k)

for j = 1, . . . , N .

(b) Draw Bk from a categorical distribution with probabilities

for j = 1, . . . , N .

W j
n
(cid:96)=1 W (cid:96)
n

(cid:80)N

(cid:102)W j
k
(cid:96)=1 (cid:102)W (cid:96)
k

(cid:80)N

(c) Set (cid:101)Xk:n = (X Bk

k , (cid:101)Xk+1:n).

4. return (cid:101)X1:n

extended target distribution. Here we will use the same technique to prove the proper weighing
property of the NSMC procedure.

We start by introducing some additional notation for the auxiliary variables of the extended
target construction. While Algorithm 5 is expressed using multinomial random variables m1:N
in the resampling step, it is more convenient for the sake of the proof to explicitly introduce the
i=1; see e.g., Andrieu et al. (2010). That is, Ai
N
ancestor indices
k is a categorical random
Ai
1 of particle X i
k
k. The
variable on
1:k−1 is ancestor particle at iteration k
, such that X
resampling Step 2d of Algorithm 5 can then equivalently be expressed as: simulate independently
Ai

N
i=1 from the categorical distribution with probabilities

k}
{
1, . . . , N
}
{

Ai

−

k

{

k}

k−1W j
(cid:98)νj
(cid:80)N
(cid:96)=1 (cid:98)ν(cid:96)

k−1
k−1W (cid:96)

k−1

.

{

X 1

k , . . . , X N
k }

Let Xk =
, denote all the
particles, internal states of the proposals, and ancestor indices, respectively, generated at iteration k
of the NSMC algorithm. We can then write down the joint distribution of all the random variables
generated in executing Algorithm 5 (up to an irrelevant permutation of the particle indices) as,

k , . . . , U N
U 1
k }
{

k, . . . , AN
k }

, and Ak =

, Uk =

A1
{

¯ΨNSMC(x1:n, u0:n, a1:n) =

¯ψM
0 (ui
0)

(cid:40) N
(cid:89)

i=1

(cid:41) n
(cid:89)




N
(cid:89)

k=1



i=1

ai
k
(cid:98)ν
k−1W
(cid:80)N
(cid:96)=1 (cid:98)ν(cid:96)

ai
k
k−1
k−1W (cid:96)

k−1

¯QM

k (xi

k, ui
k |

ai
k
1:k−1, u

ai
k
k−1)

x






,

(13)

where we interpret (cid:98)νi

k and W i

k as deterministic functions of (xi

1:k, ui

0:k).

21

NAESSETH, LINDSTEN AND SCH ¨ON

Let Bn denote a random variable deﬁned on

. The extended target distribution for

1, . . . , N
{

}

PMCMC samplers corresponding to (13) is then given by

¯Φ(x1:n, u0:n, a1:n, bn) :=

¯ΨNSMC(x1:n, u0:n, a1:n),

(14)

(cid:98)Zπn
Zπn

W bn
n
(cid:96)=1 W (cid:96)
n

(cid:80)N

where (cid:98)Zπn is a deterministic function of (x1:n, u0:n, a1:n). We know from Andrieu et al. (2010)
that ¯Φ is a probability distribution which admits ¯Πn as its marginal distribution for (X bn
0:n).
Consequently, by (6) it follows that the marginal distribution of X bn
1:n is ¯πn. For later reference we
deﬁne recursively bk−1 := abk
k for k = 1, . . . , n, the particle indices for the trajectory obtained by
tracing backward the genealogy of the bn’th particle at iteration n.

1:n, U bn

We now turn our attention to the backward simulator in Algorithm 6. Backward simulation
has indeed been used in the context of PMCMC, see e.g. Whiteley (2010); Lindsten and Sch¨on
(2013); Lindsten et al. (2014). The strategy used for combining PMCMC with backward simulation
is to show that each step of the backward sampler corresponds to a partially collapsed Gibbs sam-
pling step for the extended target distribution ¯Φ. This implies that the backward sampler leaves ¯Φ
invariant.

We use the same approach here, but we need to be careful in how we apply the existing results,
since the PMCMC distribution ¯Φ is deﬁned w.r.t. to ¯Πn, whereas the backward simulator of Algo-
rithm 6 works with the original target distribution ¯πn. Nevertheless, from the proof of Lemma 1 by
Lindsten et al. (2014) it follows that we can write the following collapsed conditional distribution
of ¯Φ as:

¯Φ(bk, ubk:n
k:n |

x1:k, u0:k−1, a1:k, x

Πn(

W bk
k

∝

bk+1:n
k+1:n , bk+1:n)
bk+1:n
xbk
1:k, x
,
k+1:n }
{
{
bk
a
0:k−1, ubk
Πk(xbk
k
u
1:k,
k }
{

)

bk
a
0:k−1, ubk:n
k
u
k:n }

)

k (ubk
¯ψM
k |

xbk
1:k).

(15)

To simplify this expression, consider,

Πn(x1:n, u0:n)
Πk(x1:k, u0:k)

n
(cid:89)

(cid:26) τs(us−1) ¯ψM

=

=

s=k+1
¯ψM
n (un
¯ψM
k (uk

x1:n)
x1:k)

|
|

s (us
|
qs(xs
|

x1:s)¯γM
x1:s−1)

(cid:40) n
(cid:89)

τs(us−1) ¯ψM

s=k+1

s (xs

us−1)

|

(cid:27)

πs(x1:s)
πs−1(x1:s−1)

s−1(us−1
qs(xs

x1:s−1)¯γM
x1:s−1)

s (xs

us−1)

|

πn(x1:n)
πk(x1:k)

.

(cid:41)

|
|

(16)

By Lemma 7 we know that each factor of the product (in brackets) on the second line integrates to
1 over us−1. Hence, plugging (16) into (15) and integrating over ubk:n

k:n yields

¯Φ(bk

|

x1:k, u0:k−1, a1:k, x

bk+1:n
k+1:n , bk+1:n)

πn(
{

W bk
k

∝

xbk
1:k, x
πk(xbk

bk+1:n
)
k+1:n }
1:k)

,

which coincides with the expression used to simulate the index Bk in Algorithm 6. Hence, sim-
ulation of Bk indeed corresponds to a partially collapsed Gibbs sampling step for ¯Φ and it will
thus leave ¯Φ invariant. (Note that, in comparison with the PMCMC sampler derived by Lindsten

22

NESTED SEQUENTIAL MONTE CARLO METHODS

et al. (2014) we further marginalise over the variables ubk:n
partially collapsed Gibbs step.)

k:n which, however, still results in a valid

We now have all the components needed to prove proper weighting of the combined NSMC/backward

simulation procedure. For notational simplicity, we write

¯ΨBS,k(bk) = ¯Φ(bk

x1:k, u0:k−1, a1:k, x

bk+1:n
k+1:n , bk+1:n),

|

for the distribution of Bk in Algorithm 6. Let ( (cid:98)Zπn, (cid:101)X1:n) be generated by Algorithms 5 and 6. Let
f be a measurable function and consider

E[ (cid:98)Zπnf ( (cid:101)X1:n)] =

(cid:90)

(cid:98)Zπnf (X

b(cid:48)
1:n
1:n )

¯ΨBS,k(db(cid:48)
k)

¯ΨNSMC(d(x1:n, u0:n, a1:n))

(cid:41)

(cid:41)

(cid:40) n
(cid:89)

k=1
(cid:40)n−1
(cid:89)

k=1

= Zπn

(cid:90)

f (X

b(cid:48)
1:n
1:n )

¯ΨBS,k(db(cid:48)
k)

¯Φ(d(x1:n, u0:n, a1:n, b(cid:48)

n)),

where, for the second equality, we have used the deﬁnition (14) and noted that ¯ΨBS,n(bn) =

W bn
n
(cid:96)=1 W (cid:96)
n

(cid:80)N

. However, by the invariance of ¯ΨBS,k w.r.t. ¯Φ, it follows that

E[ (cid:98)Zπnf ( (cid:101)X1:n)] = Zπn

f (X b1:n

1:n ) ¯Φ(d(x1:n, u0:n, a1:n, bn)) = Zπn ¯πn(f ),

(cid:90)

which completes the proof.

A.2 Nested Sequential Importance Sampling

Here we give the deﬁnition of the nested sequential importance sampler and we show that a special
case of this is the importance sampling squared (IS2) method by Tran et al. (2013).

A.2.1 NESTED SEQUENTIAL IMPORTANCE SAMPLING

We present a straightforward extension of the Nested IS class to a sequential IS version. Consider
the following deﬁnition of the Nested SIS Q:

1. Algorithm 7 is executed at the construction of the object p = Q(πn, N ), and p.GetZ() returns

the normalising constant estimate (cid:98)Zπn.

2. p.Simulate() simulates a categorical random variable B with P(B = i) = W i

n/ (cid:80)N

(cid:96)=1 W (cid:96)
n

and returns X B

1:n.

Note that we do not require that the procedure Q is identical for each individual proposal qk,
thus we have a ﬂexibility in designing our algorithm as can be seen in the example in Section A.2.2.
We can motivate the algorithm in the same way as for Nested IS and similar theoretical results hold,
i.e. Nested SIS is properly weighted for πn and it admits ¯πn as a marginal.

A.2.2 RELATION TO IS2

Here we will show how IS2, proposed by Tran et al. (2013), can be viewed as a special case of
Nested SIS. We are interested in approximating the posterior distribution of parameters θ given

23

NAESSETH, LINDSTEN AND SCH ¨ON

Algorithm 7 Nested SIS (all i for 1, . . . , N )

1. Initialise qi = Q(q1(
·

), M ).

2. Set (cid:98)Zi

q1 = qi.GetZ(), X i

1 = qi.Simulate().

q1π1(X i
(cid:98)Zi
1)
q1(X i
1)

.

3. Set W i

1 =

4. delete qi.

5. for k = 2 to n:

· |

(a) Initialise qi = Q(qk(
(b) Set (cid:98)Zi

X i
1:k−1), M ).
k = qi.Simulate().
qk = qi.GetZ(), X i
qk πk(X i
(cid:98)Zi
1:k−1)
k |
X i
qk(X i
k |

X i
1:k−1)

k = W i

(c) Set W i

k−1

.

(d) delete qi.
(e) Set X i

1:k ←
6. Compute (cid:98)Zπn = 1
N

(X i

1:k−1, X i
k)
(cid:80)N

i=1 W i
n.

some observed values y

We assume that the data likelihood p(y
an integral

|

¯π(θ

y)

p(y

θ)p(θ).

∝

|
θ) can, by introducing a latent variable x, be computed as

|

Now, let our target distribution in Nested SIS be ¯π2(θ, x) = ¯π2(x
We set our proposal distributions to be

|

θ)¯π1(θ) = p(y | x,θ)p(x | θ)

p(y | θ)

p(θ).

(cid:90)

p(y

θ) =

p(y

x, θ)p(x

θ) dx.

|

|

|

¯q1(θ) = gIS(θ),

¯q2(x

θ) =

|

p(y

|

x, θ)p(x
θ)
p(y

|

θ)

.

|
First, Q(q1(
), 1) runs an exact sampler from the proposal gIS. Then at iteration k = 2 we let the
·
nested procedure Q(q2(
y, θ), giving us
properly weighted samples for q2. Putting all this together gives us samples θi distributed according
to gIS(θ) and weighted by

θi), M ) be a standard IS algorithm with proposal h(x

· |

|

W i

2 ∝

p(θi)
gIS(θi) ·

p(y

xi, θi)p(xi

|

(cid:80)M

θi) 1
(cid:96)=1
M
xi, θi)p(xi

|
p(y

|

θi)

|

p(y | x(cid:96),θi)p(x(cid:96) | θi)
h(x(cid:96) | y,θi)

= (cid:98)pM (y

θi)p(θi)

|
gIS(θi)

,

(17)

24

NESTED SEQUENTIAL MONTE CARLO METHODS

θi) = M −1 (cid:80)M
where (cid:98)pM (y
(cid:96)=1
identical to the IS2 algorithm proposed by Tran et al. (2013).

p(y | x(cid:96),θi)p(x(cid:96) | θi)
h(x(cid:96) | y,θi)

|

. Thus we obtain a Nested SIS method that is

A.3 Further Details on the Experiments

We provide some further details and results for the experiments presented in the main manuscript.

A.3.1 GAUSSIAN STATE SPACE MODEL

We generate data from a synthetic d-dimensional (dim(xk) = d) dynamical/spatio-temporal3 model
deﬁned by

xk

xk−1

|
yk

∼ N

∼ N

xk

|

(xk; µk(xk−1), Σ),
(yk; xk, τ −1

φ I),

where Σ and µk are given as follows

Σ =

τρ + τψ
τψ













−
0
...
...
0
0

τψ
−
τρ + 2τψ
. . .
. . .
...
0
0

0
τψ
−
. . .
. . .
. . .
0
0

· · ·
0
. . .
. . .
. . .
0
0

0
0

0

· · ·
· · ·
. . .
. . .
. . .
−
τψ τρ + 2τψ
−
0

τψ

τψ

0

−

0
0

0

0

−1













,

0
τψ
−
τρ + τψ

µk(xk−1) = aτρΣxk−1.

Alternatively, in a more standard state space model notation, we have

xk = Axk−1 + vk, vk
yk = xk + ek, ek

∼ N
(0, R),

(0, Q),

∼ N

where A = aτρΣ, Q = Σ and R = τ −1
(1, 0.5, 1, 10) are known.

φ I. We assume that the parameters θ = (τψ, a, τρ, τφ) =

To do inference with this generated data-set

we propose to target the following slightly

yk

{

}

different model

p(x1:k, y1:k)

φ(xj, yj)ρ(xj)ψ(xj, xj−1),

k
(cid:89)

j=1

∝

3. Note that in a previous version this was erraneously stated as equivivalent to the Gaussian MRF we use for sequential
inference. Thus this example actually illustrates a problem where we have a misspeciﬁed model. However, this
misspeciﬁcation does not lead to any discernible difference in the MSE results. This because the exact ﬁltering
marginals for the two different models (LGSS, GMRF) with the parameters chosen differs with orders of magnitudes
much lower than the Monte Carlo errors.

25

NAESSETH, LINDSTEN AND SCH ¨ON

where the observation potential φ and interaction potentials ρ and ψ are given by

φ(xk, yk) =

φl(xk,l, yk,l) =

e−

τφ
2 (xk,l−yk,l)2

,

ψ(xk) =

ψl(xk,l, xk,l−1) =

τψ
2 (xk,l−xk,l−1)2

e−

ρ(xk, xk−1) =

ρl(xk,l, xk−1,l) =

τρ
2 (xk,l−axk−1,l)2

e−

,

.

d
(cid:89)

l=1

d
(cid:89)

l=2
d
(cid:89)

l=1

d
(cid:89)

l=1
d
(cid:89)

l=2
d
(cid:89)

l=1

k) lattice MRF, i.e. it grows with “time” k. The
This can be visualised as a Gaussian rectangular (d
×
goal is to estimate the ﬁltering distribution p(xk
y1:k). Note that this model has almost identical
|
ﬁltering marginals as the data generating distribution and leads to a simpler implementation of
NSMC and ST-PF.

Results (mean-squared-error, MSE) comparing NSMC and ST-PF for different settings of N
and M can be found in the ﬁrst row of Figure 6 and the second row displays the results when
comparing ST-PF to the SMC method by Naesseth et al. (2014b) for equal computational budgets.
We show median (over dimensions d) MSE for posterior marginal mean and variance estimates
of the respective algorithms. True values are obtained using belief propagation. Note that setting
N = 1 in ST-PF can be viewed as a special case of the SMC method by Naesseth et al. (2014b).

A.3.2 SPATIO-TEMPORAL MODEL – DROUGHT DETECTION

We present the full model for drought detection in our notation, this is essentially the model by Fu
et al. (2012) adapted for estimating the ﬁltering distribution. The latent variables for each location
on a ﬁnite world grid, xk,i,j, are binary, i.e. 0 being normal state and 1 being the abnormal (drought)
state. Measurements, yk,i,j, are available as real valued precipitation values in millimeters. The
probabilistic model for ﬁltering is given as,

k
(cid:89)

∝

n=1

(cid:40)

1
2σ2
i,j

−

p(x1:k, y1:k)

φ(xn, yn)ρ(xn)ψ(xn, xn−1),

(18a)

where

φ(xk, yk) =

exp

(yk,i,j

µab,i,jxk,i,j

µnorm,i,j(1

−

−

−

(cid:41)

xk,i,j))2

,

(18b)

ρ(xk) =

exp (cid:8)C1

(cid:0)1xk,i,j =xk,i,j−1 + 1xk,i,j =xk,i−1,j

(cid:1)(cid:9) ,

ψ(xk, xk−1) =

exp (cid:8)C21xk,i,j =xk−1,i,j

(cid:9) .

(18c)

(18d)

I
(cid:89)

J
(cid:89)

i=1

j=1

I
(cid:89)

J
(cid:89)

i=1

j=1

I
(cid:89)

J
(cid:89)

i=1

j=1

Here, 1 is the indicator function, and with the convention that all expressions in (18c) that end up
with index 0 evalute to 0. The parameters C1, C2 are set to 0.5, 3 as in (Fu et al., 2012). Location

26

NESTED SEQUENTIAL MONTE CARLO METHODS

Median MSE for E[xk,(cid:96)]

Median MSE for Var(xk,(cid:96))

Median MSE for E[xk,(cid:96)]

Median MSE for Var(xk,(cid:96))

Figure 6: Top: Comparisons for different settings of N and M on the 50-dimensional SSM. Bot-
tom: Illustrating the connection between ST-PF and the SMC method by Naesseth et al.
(2014b).

44 region with latitude 6

55◦N and longitude 90

2012. For the North America region we consider a 20

based parameters σi,j, µab,i,j, µnorm,i,j are estimated based on data from the CRU dataset with world
precipitation data from years 1901
30
120◦W . For the Sahel region we consider a
region with latitude 35
35◦E. Note that for a few locations in
24
Africa (Sahel region) the average yearly precipitation was constant. For these locations we simply
set µnorm,i,j to be this value, µab,i,j = 0 and σ2
i,j to be the mean variance of all locations, thus this
might have introduced some artifacts. Some representative results for the Sahel region are displayed
in Figure 7.

30◦N and longitude 10◦W

−

−

−

×

−

×

−

References

C. Andrieu and G. O. Roberts. The pseudo-marginal approach for efﬁcient Monte Carlo computa-

tions. The Annals of Statistics, 37(2):697–725, 2009.

Christophe Andrieu, Arnaud Doucet, and Roman Holenstein. Particle Markov chain Monte Carlo
methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):
269–342, 2010.

27

NAESSETH, LINDSTEN AND SCH ¨ON

Sahel region 1986

Sahel region 1987

Sahel region 1988

Figure 7: Estimate of P(Xk,i,j = 1

y1:k) for all sites over a span of 3 years. All results for N =

100, N1 =

30, 40
}

{

|
, N2 = 20.

A. Beskos, D. Crisan, A. Jasra, K. Kamatani, and Y. Zhou. A stable particle ﬁlter in high-

dimensions. ArXiv:1412.3501, December 2014a.

Alexandros Beskos, Dan Crisan, and Ajay Jasra. On the stability of sequential Monte Carlo methods

in high dimensions. Ann. Appl. Probab., 24(4):1396–1445, 08 2014b.

Peter Bickel, Bo Li, and Thomas Bengtsson. Sharp failure rates for the bootstrap particle ﬁlter
in high dimensions, volume Volume 3 of Collections, pages 318–329. Institute of Mathematical
Statistics, Beachwood, Ohio, USA, 2008.

Jonathan Briggs, Michael Dowd, and Renate Meyer. Data assimilation for large-scale spatio-

temporal systems using a location particle smoother. Environmetrics, 24(2):81–97, 2013.

Olivier Capp´e, Eric Moulines, and Tobias Ryd´en. Inference in Hidden Markov Models. Springer-

Verlag New York, Inc., Secaucus, NJ, USA, 2005. ISBN 0387402640.

J. Carpenter, P. Clifford, and P. Fearnhead. Improved particle ﬁlter for nonlinear problems. IEE

Proceedings Radar, Sonar and Navigation, 146(1):2–7, 1999.

Tianshi Chen, Thomas B. Sch¨on, Henrik Ohlsson, and Lennart Ljung. Decentralized particle ﬁlter
with arbitrary state decomposition. IEEE Transactions on Signal Processing, 59(2):465–478, Feb
2011.

N. Chopin. Central limit theorem for sequential Monte Carlo methods and its application to

Bayesian inference. The Annals of Statistics, 32(6):2385–2411, 2004.

N. Chopin, P. E. Jacob, and O. Papaspiliopoulos. SMC2: an efﬁcient algorithm for sequential
analysis of state space models. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 75(3):397–426, 2013.

Jacques Cohen. Bioinformaticsan introduction for computer scientists. ACM Computing Surveys

(CSUR), 36(2):122–158, 2004.

N. Cressie and C. K. Wikle. Statistics for spatio-temporal data. Wiley, 2011.

28

NESTED SEQUENTIAL MONTE CARLO METHODS

D. Crisan and J. M´ıguez. Nested particle ﬁlters for online parameter estimation in discrete-time

state-space Markov models. ArXiv:1308.1883, August 2013.

P. Del Moral. Feynman-Kac Formulae - Genealogical and Interacting Particle Systems with Appli-

cations. Probability and its Applications. Springer, 2004.

Petar M Djuric and M´onica F Bugallo. Particle ﬁltering for high-dimensional systems. In Compu-
tational Advances in Multi-Sensor Adaptive Processing (CAMSAP), 2013 IEEE 5th International
Workshop on, pages 352–355. IEEE, 2013.

R. Douc and E. Moulines. Limit theorems for weighted samples with applications to sequential

Monte Carlo. The Annals of Statistics, 36(5):2344–2376, 2008.

R. Douc, E. Moulines, and J. Olsson. Optimality of the auxiliary particle ﬁlter. Probability and

Mathematical Statistics, 29:1–28, 2009.

A. Doucet and A. M. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years later.
In D. Crisan and B. Rozovsky, editors, Nonlinear Filtering Handbook. Oxford University Press,
2011.

Arnaud Doucet, Nando De Freitas, and Neil Gordon. An introduction to sequential Monte Carlo

methods. Springer, 2001.

Paul Fearnhead, Omiros Papaspiliopoulos, Gareth O. Roberts, and Andrew Stuart. Random-weight
particle ﬁltering of continuous time processes. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 72(4):497–512, 2010a.

Paul Fearnhead, David Wyncoll, and Jonathan Tawn. A sequential smoothing algorithm with linear

computational cost. Biometrika, 97(2):447–464, 2010b.

J. A. Foley, M. T. Coe, M. Scheffer, and G. Wang. Regime shifts in the sahara and sahel: Interactions

between ecological and climatic systems in northern africa. Ecosystems, 6:524–539, 2003.

Qiang Fu, Arindam Banerjee, Stefan Liess, and Peter K. Snyder. Drought detection of the last
century: An MRF-based approach. In Proceedings of the 2012 SIAM International Conference
on Data Mining, pages 24–34, Anaheim, CA, USA, April 2012.

S. J. Godsill, A. Doucet, and M. West. Monte Carlo smoothing for nonlinear time series. Journal

of the American Statistical Association, 99(465):156–168, March 2004.

N. J. Gordon, D. J. Salmond, and A. F. M. Smith. Novel approach to nonlinear/non-Gaussian
Bayesian state estimation. Radar and Signal Processing, IEE Proceedings F, 140(2):107 –113,
April 1993.

M. Hoerling, J. Hurrell, J. Eischeid, and A. Phillips. Detection and attribution of twentieth-century

northern and southern african rainfall change. Journal of Climate, 19:3989–4008, 2006.

A. M. Johansen and A. Doucet. A note on auxiliary particle ﬁlters. Statistics & Probability Letters,

78(12):1498–1504, 2008.

29

NAESSETH, LINDSTEN AND SCH ¨ON

A. M. Johansen, N. Whiteley, and A. Doucet. Exact approximation of Rao-Blackwellised particle
ﬁlters. In Proceesings of the 16th IFAC Symposium on System Identiﬁcation (SYSID), pages 488–
493, Brussels, Belgium, 2012.

P.D. Jones and I. Harris. CRU TS3.21: Climatic research unit (CRU) time-series (ts) version 3.21
of high resolution gridded data of month-by-month variation in climate (jan. 1901- dec. 2012).
NCAS British Atmospheric Data Centre, sep 2013. URL http://dx.doi.org/10.5285/
D0E1585D-3417-485F-87AE-4FCECF10A992.

R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Transactions of the

ASME, Journal of Basic Engineering, 82:35–45, 1960.

A. Kong, J. S. Liu, and W. H. Wong. Sequential imputations and Bayesian missing data problems.

Journal of the American Statistical Association, 89(425):278–288, 1994.

F. Lindsten and T. B. Sch¨on. Backward simulation methods for Monte Carlo statistical inference.

Foundations and Trends in Machine Learning, 6(1):1–143, 2013.

F. Lindsten, M. I. Jordan, and T. B. Sch¨on. Particle Gibbs with ancestor sampling. Journal of

Machine Learning Research, 15:2145–2184, 2014.

Jun S Liu. Monte Carlo strategies in scientiﬁc computing. Springer Science & Business Media,

2001.

Claire Monteleoni, Gavin A. Schmidt, Francis Alexander, Alexandru Niculescu-Mizil, Karsten
Steinhaeuser, Michael Tippett, Arindam Banerjee, M. Benno Blumenthal, Jason E. Smerdon Au-
In Ting Yu, Nitesh Chawla, and
roop R. Ganguly, and Marco Tedesco. Climate informatics.
Simeon Simoff, editors, Computational Intelligent Data Analysis for Sustainable Development.
Chapman and Hall/CRC, London, 2013.

Christian A. Naesseth, Fredrik Lindsten, and Thomas B. Sch¨on. Capacity estimation of two-
In The 2014 IEEE Information Theory

dimensional channels using sequential Monte Carlo.
Workshop (ITW), pages 431–435, Nov 2014a.

Christian A. Naesseth, Fredrik Lindsten, and Thomas B Sch¨on. Sequential Monte Carlo for graphi-
cal models. In Advances in Neural Information Processing Systems 27, pages 1862–1870. Curran
Associates, Inc., 2014b.

Michael K Pitt and Neil Shephard. Filtering via simulation: Auxiliary particle ﬁlters. Journal of the

American statistical association, 94(446):590–599, 1999.

P. Rebeschini and R. van Handel. Can local particle ﬁlters beat the curse of dimensionality? Ann.

Appl. Probab. (to appear), 2015.

Raton, FL, USA, 2005.

H. Rue and L. Held. Gaussian Markov Random Fields, Theory and Applications. CDC Press, Boca

S. D. Schubert, M. J. Suarez, P. J. Pegion, R. D. Koster, and J. T. Bacmeister. On the cause of the

1930s dust bowl. Science, 303:1855–1859, 2004.

30

NESTED SEQUENTIAL MONTE CARLO METHODS

R. H. Shumway and D. S. Stoffer. Time Series Analysis and Its Applications – with R examples.

Springer Texts in Statistics. Springer, New York, USA, third edition, 2011.

M.-N. Tran, M. Scharth, M. K. Pitt, and R. Kohn.

Importance sampling squared for Bayesian

inference in latent variable models. ArXiv:1309.3339, sep 2013.

Christelle Verg´e, Cyrille Dubarry, Pierre Del Moral, and Eric Moulines. On parallel implementation
of sequential Monte Carlo methods: the island particle model. Statistics and Computing, 25(2):
243–260, 2015.

Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational

inference. Foundations and Trends R
(cid:13)

in Machine Learning, 1(1-2):1–305, 2008.

N. Whiteley. Discussion on Particle Markov chain Monte Carlo methods. Journal of the Royal

Statistical Society: Series B, 72(3):306–307, 2010.

C. K. Wikle. Modern perspectives on statistics for spatio-temporal data. WIREs Computational

Statistics, 7(1):86–98, 2015.

31

Technical report

Nested Sequential Monte Carlo Methods

Christian A. Naesseth, Fredrik Lindsten and Thomas B. Sch¨on

Please cite this version:

•

Christian A. Naesseth, Fredrik Lindsten and Thomas B. Sch¨on. Nested Sequential
Monte Carlo Methods. In Proceedings of the 32 nd International Conference on Ma-
chine Learning, Lille, France, 2015. JMLR: W&CP volume 37.

Abstract
We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences
of probability distributions, even where the random variables are high-dimensional. NSMC gen-
eralises the SMC framework by requiring only approximate, properly weighted, samples from the
SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC
can in itself be used to produce such properly weighted samples. Consequently, one NSMC sam-
pler can be used to construct an efﬁcient high-dimensional proposal distribution for another NSMC
sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to
consider complex and high-dimensional models using SMC. We show results that motivate the
efﬁcacy of our approach on several ﬁltering problems with dimensions in the order of 100 to 1 000.
Keywords: high-dimensional inference, high-dimensional particle ﬁlter, exact approximation, op-
spatio-temporal models
timal proposal,

sequential Monte Carlo,

importance sampling,

5
1
0
2
 
p
e
S
 
1
1
 
 
]

O
C

.
t
a
t
s
[
 
 
3
v
6
3
5
2
0
.
2
0
5
1
:
v
i
X
r
a

NAESSETH, LINDSTEN AND SCH ¨ON

Nested Sequential Monte Carlo Methods

Christian A. Naesseth
Link¨oping University, Link¨oping, Sweden

CHRISTIAN.A.NAESSETH@LIU.SE

Fredrik Lindsten
The University of Cambridge, Cambridge, United Kingdom

FREDRIK.LINDSTEN@ENG.CAM.AC.UK

Thomas B. Sch¨on
Uppsala University, Uppsala, Sweden

THOMAS.SCHON@IT.UU.SE

Abstract
We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences
of probability distributions, even where the random variables are high-dimensional. NSMC gen-
eralises the SMC framework by requiring only approximate, properly weighted, samples from the
SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC
can in itself be used to produce such properly weighted samples. Consequently, one NSMC sam-
pler can be used to construct an efﬁcient high-dimensional proposal distribution for another NSMC
sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to
consider complex and high-dimensional models using SMC. We show results that motivate the
efﬁcacy of our approach on several ﬁltering problems with dimensions in the order of 100 to 1 000.
Keywords:
high-dimensional inference, high-dimensional particle ﬁlter, exact approximation,
optimal proposal, sequential Monte Carlo, importance sampling, spatio-temporal models

1. Introduction

Inference in complex and high-dimensional statistical models is a very challenging problem that is
ubiquitous in applications. Examples include, but are deﬁnitely not limited to, climate informatics
(Monteleoni et al., 2013), bioinformatics (Cohen, 2004) and machine learning (Wainwright and
Jordan, 2008). In particular, we are interested in sequential Bayesian inference, which involves
computing integrals of the form

¯πk(f ) := E¯πk [f (X1:k)] =

f (x1:k)¯πk(x1:k)dx1:k,

(1)

for some sequence of probability densities

¯πk(x1:k) = Z−1

πk πk(x1:k),
with normalisation constants Zπk = (cid:82) πk(x1:k)dx1:k. Note that x1:k := (x1, . . . , xk)
Xk. The
typical scenario that we consider is the well-known problem of inference in time series or state space
models (Shumway and Stoffer, 2011; Capp´e et al., 2005). Here the index k corresponds to time and
we want to process some observations y1:k in a sequential manner to compute expectations with
respect to the ﬁltering distribution ¯πk(dxk) = P(Xk
y1:k). To be speciﬁc, we are interested
in settings where

dxk

1,

≥

∈

∈

k

|

(2)

(i) Xk is high-dimensional, i.e. Xk

Rd with d

1, and

∈

(cid:29)

(cid:90)

2

NESTED SEQUENTIAL MONTE CARLO METHODS

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

· · ·

k

1

−

k

· · ·

k + 1

Figure 1: Example of a spatio-temporal model where ¯πk(x1:k) is described by a k
R2×3.

graphical model and xk

×

×

2

3 undirected

∈

(ii) there are local dependencies among the latent variables X1:k, both w.r.t. time k and between

the individual components of the (high-dimensional) vectors Xk.

One example of the type of models we consider are the so-called spatio-temporal models (Wikle,
2015; Cressie and Wikle, 2011; Rue and Held, 2005). In Figure 1 we provide a probabilistic graph-
ical model representation of a spatio-temporal model that we will explore further in Section 6.

Sequential Monte Carlo (SMC) methods, reviewed in Section 2.1, comprise one of the most
successful methodologies for sequential Bayesian inference. However, SMC struggles in high-
dimensions and these methods are rarely used for dimensions, say, d
10 (Rebeschini and van
Handel, 2015). The purpose of the NSMC methodology is to push this limit well beyond d = 10.

≥

The basic strategy, described in Section 2.2, is to mimic the behaviour of a so-called fully
adapted SMC algorithm. Full adaptation can drastically improve the efﬁciency of SMC in high di-
mensions. Unfortunately, it can rarely be implemented in practice since the fully adapted proposal
distributions are typically intractable. NSMC addresses this difﬁculty by requiring only approxi-
mate, properly weighted, samples from the proposal distribution. The proper weighting condition
ensures the validity of NSMC, thus providing a generalisation of the family of SMC methods. Fur-
thermore, NSMC will itself produce properly weighted samples. Consequently, it is possible to
use one NSMC procedure within another to construct efﬁcient high-dimensional proposal distribu-
tions. This nesting of the algorithm can be done to an arbitrary degree. For instance, for the model
depicted in Figure 1 we could use three nested samplers, one for each dimension of the “volume”.
The main methodological development is concentrated to Sections 3–4. We introduce the con-
cept of proper weighting, approximations of the proposal distribution, and nesting of Monte Carlo
algorithms. Throughout Section 3 we consider simple importance sampling and in Section 4 we
extend the development to the sequential setting.

We deliberately defer the discussion of the existing body of related work until Section 5, to open
up for a better understanding of the relationships to the new developments presented in Sections 3–
4. We also discuss various attractive features of NSMC that are of interest in high-dimensional
settings, e.g. the fact that it is easy to distribute the computation, which results in improved memory
efﬁciency and lower communication costs. Section 6 proﬁles our method extensively with a state-of-
the-art competing algorithm on several high-dimensional data sets. We also show the performance

3

NAESSETH, LINDSTEN AND SCH ¨ON

of inference and the modularity of the method on a d = 1 056 dimensional climatological spatio-
temporal model (Fu et al., 2012) structured according to Figure 1. Finally, in Section 7 we conclude
the paper with some ﬁnal remarks.

2. Background and Inference Strategy

2.1 Sequential Monte Carlo

Evaluating ¯πk(f ) as well as the normalisation constant Zπk in (2) is typically intractable and we
need to resort to approximations. SMC methods, or particle ﬁlters (PF), constitute a popular class
of numerical approximations for sequential inference problems. Here we give a high-level intro-
duction to the concepts underlying SMC methods, and postpone the details to Section 4. For a more
extensive treatment we refer to Doucet and Johansen (2011); Capp´e et al. (2005); Doucet et al.
(2001). In particular, we will use the auxiliary SMC method as proposed by Pitt and Shephard
(1999).

At iteration k

−

1, the SMC sampler approximates the target distribution ¯πk−1 by a collection of
N
i=1. These samples deﬁne an empirical point-mass

1:k−1, W i

(X i

weighted particles (samples)
approximation of the target distribution

{

k−1)
}

¯πN
k−1(dx1:k−1) :=

N
(cid:88)

i=1

W i
k−1
(cid:96) W (cid:96)

(cid:80)

k−1

δX i

1:k−1

(dx1:k−1),

(3)

where δX (dx) denotes a Dirac measure at X. Each iteration of the SMC algorithm can then con-
ceptually be described by three steps, resampling, propagation, and weighting.

The resampling step puts emphasis on the most promising particles by discarding the unlikely
ones and duplicating the likely ones. The propagation and weighting steps essentially correspond to
using importance sampling when changing the target distribution from ¯πk−1 to ¯πk, i.e. simulating
new particles from a proposal distribution and then computing corresponding importance weights.

2.2 Adapting the Proposal Distribution

The ﬁrst working SMC algorithm was the bootstrap PF by Gordon et al. (1993), which propagates
particles by sampling from the system dynamics and computes importance weights according to the
observation likelihood (in the state space setting). However, it is well known that the bootstrap PF
suffers from weight collapse in high-dimensional settings (Bickel et al., 2008), i.e. the estimate is
dominated by a single particle with weight close to one. This is an effect of the mismatch between
the importance sampling proposal and the target distribution, which typically gets more pronounced
in high dimensions.

More efﬁcient proposals, partially alleviating the degeneracy issue for some models, can be de-
signed by adapting the proposal distribution to the target distribution (see Section 4.2). In Naesseth
et al. (2014a) we make use of the fully adapted SMC method (Pitt and Shephard, 1999) for doing
inference in a (fairly) high-dimensional discrete model where xk is a 60-dimensional discrete vec-
tor. We can then make use of forward ﬁltering and backward simulation, operating on the individual
components of each xk, in order to sample from the fully adapted SMC proposals. However, this
method is limited to models where the latent space is either discrete or Gaussian and the optimal
proposal can be identiﬁed with a tree-structured graphical model. Our development here can be

4

NESTED SEQUENTIAL MONTE CARLO METHODS

seen as a non-trivial extension of this technique. Instead of coupling one SMC sampler with an
exact forward ﬁlter/backward simulator (which in fact reduces to an instance of standard SMC),
we derive a way of coupling multiple SMC samplers and SMC-based backward simulators. This
allows us to construct procedures for mimicking the efﬁcient fully adapted proposals for arbitrary
latent spaces and structures in high-dimensional models.

3. Proper Weighting and Nested Importance Sampling

In this section we will lay the groundwork for the derivation of the class of NSMC algorithms.
We start by considering the simpler case of importance sampling (IS), which is a fundamental
component of SMC, and introduce the key concepts that we make use of. In particular, we will use
a (slightly nonstandard) presentation of an algorithm as an instance of a class, in the object-oriented
sense, and show that these classes can be nested to an arbitrary degree.

3.1 Exact Approximation of the Proposal Distribution

i=1 W i)−1 (cid:80)N

i=1 W if (X i), with W i = Zqπ(X i)

Let ¯π(x) = Z−1
π π(x) be a target distribution of interest. IS can be used to estimate an expectation
¯π(f ) := E¯π[f (X)] by sampling from a proposal distribution ¯q(x) = Z−1
q q(x) and computing the
estimator ((cid:80)N
N
i=1 are the
, and where
}
weighted samples. It is possible to replace the IS weight by a nonnegative unbiased estimate, and
still obtain a valid (consistent, etc.) algorithm (Liu, 2001, p. 37). One way to motivate this approach
is by considering the random weight to be an auxiliary variable and to extend the target distribution
accordingly. Our development is in the same ﬂavour, but we will use a more explicit condition on
the relationship between the random weights and the simulated particles. Speciﬁcally, we will make
use of the following key property to formally justify the proposed algorithms.

(X i, W i)

q(X i)

{

Deﬁnition 1 (Properly weighted sample). A (random) pair (X, W ) is properly weighted for an
0 and E[f (X)W ] = p(f ) := (cid:82) f (x)p(x)dx for all measurable
unnormalised distribution p if W
functions f .

≥

{

≡

(cid:80)N

(cid:104) 1
N

(X i, W i)

i=1 W i(cid:105)

= (cid:82) p(x)dx =: Zp.

Note that proper weighting of

malising constant of p. Indeed, taking f (x)

N
i=1 implies unbiasedness of the estimate of the nor-
}
1 gives E
Interestingly, to construct a valid IS algorithm for our target ¯π it is sufﬁcient to generate samples
that are properly weighted w.r.t. the proposal distribution q. To formalise this claim, assume that we
are not able to simulate exactly from ¯q, but that it is possible to evaluate the unnormalised density
q point-wise. Furthermore, assume we have access to a class Q, which works as follows. The
constructor of Q requires the speciﬁcation of an unnormalised density function, say, q, which will
be approximated by the procedures of Q. Furthermore, to highlight the fact that we will typically use
IS (and SMC) to construct Q, the constructor also takes as an argument a precision parameter M ,
corresponding to the number of samples used by the “internal” Monte Carlo procedure. An object
is then instantiated as q = Q(q, M ). The class Q is assumed to have the following properties:

(A1) Let q = Q(q, M ). Assume that:

1. The construction of q results in the generation of a (possibly random) member variable, ac-
cessible as (cid:98)Zq = q.GetZ(). The variable (cid:98)Zq is a nonnegative, unbiased estimate of the nor-
malising constant Zq = (cid:82) q(x)dx.

5

NAESSETH, LINDSTEN AND SCH ¨ON

2. Q has a member function Simulate which returns a (possibly random) variable X = q.Simulate(),

such that (X, (cid:98)Zq) is properly weighted for q.

With the deﬁnition of Q in place, it is possible to generalise1 the basic importance sampler as in
N
(X i, W i)
Algorithm 1, which generates weighted samples
i=1 targeting ¯π. Note that Algorithm 1
}
{
is different from a random weight IS, since it approximates the proposal distribution (and not just
the importance weights).

Algorithm 1 Nested IS (steps 1–3 for i = 1, . . . , N )

1. Initialise qi = Q(q, M ).

2. Set (cid:98)Zi

q = qi.GetZ() and X i = qi.Simulate().

3. Set W i =

qπ(X i)
(cid:98)Zi
q(X i)

.

4. Compute (cid:98)Zπ = 1
N

(cid:80)N

i=1 W i.

To see the validity of Algorithm 1 we can interpret the sampler as a standard IS algorithm for
an extended target distribution, deﬁned as ¯Π(x, u) := u ¯Q(x, u)¯π(x)q−1(x), where ¯Q(x, u) is the
joint PDF of the random pair (q.Simulate(), q.GetZ()). Note that ¯Π is indeed a PDF that admits ¯π
as a marginal; for any measurable subset A

X,

⊆

¯Π(A

R+) =

1A(x)

¯Q(x, u)dxdu = E

(cid:90)

u ¯π(x)
q(x)

×

(cid:20)

(cid:98)Zq

(cid:21)

1A(X)¯π(X)
q(X)

(cid:18)

= ¯q

1A

(cid:19)

¯π
q

Zq = ¯π(A),

where the penultimate equality follows from the fact that (X, (cid:98)Zq) is properly weighted for q. Fur-
thermore, the standard unnormalised IS weight for a sampler with target ¯Π and proposal ¯Q is given
by u π/q, in agreement with Algorithm 1.

Algorithm 1 is an example of what is referred to as an exact approximation; see e.g., Andrieu and
Roberts (2009); Andrieu et al. (2010). Algorithmically, the method appears to be an approximation
of an IS, but samples generated by the algorithm nevertheless target the correct distribution ¯π.

3.2 Modularity of Nested IS

To be able to implement Algorithm 1 we need to deﬁne a class Q with the required properties
(A1). The modularity of the procedure (as well as its name) comes from the fact that we can use
Algorithm 1 also in this respect. Indeed, let us now view ¯π—the target distribution of Algorithm 1—
as the proposal distribution for another Nested IS procedure and consider the following deﬁnition
of Q:

1. Algorithm 1 is executed at the construction of the object p = Q(π, N ), and p.GetZ() returns

the normalising constant estimate (cid:98)Zπ.

2. p.Simulate() simulates a categorical random variable B with P(B = i) = W i/ (cid:80)N

(cid:96)=1 W (cid:96)

and returns X B.

1. With q.GetZ() (cid:55)→ Z and q.Simulate() returning a sample from ¯q we obtain the standard IS method.

6

NESTED SEQUENTIAL MONTE CARLO METHODS

Now, for any measurable f we have,

E[f (X B) (cid:98)Zπ] =

E

f (X i) (cid:98)Zπ

(cid:20)

N
(cid:88)

i=1

(cid:21)

=

W i
N (cid:98)Zπ

1
N

N
(cid:88)

i=1

(cid:34)
f (X i)

E

(cid:35)

qπ(X i)
(cid:98)Zi
q(X i)

= ¯q

(cid:19)

(cid:18) f π
q

Zq = ¯π(f )Zπ,

where, again, we use the fact that (X i, (cid:98)Zi
is properly weighted for π and that our deﬁnition of Q(π, N ) indeed satisﬁes condition (A1).

q) is properly weighted for q. This implies that (X B, (cid:98)Zπ)

The Nested IS algorithm in itself is unlikely to be of direct practical interest. However, in the
next section we will, essentially, repeat the preceding derivation in the context of SMC to develop
the NSMC method.

4. Nested Sequential Monte Carlo

4.1 Fully Adapted SMC Samplers

Let us return to the sequential inference problem. As before, let ¯πk(x1:k) = Z−1
πk πk(x1:k) denote
the target distribution at “time” k. The unnormalised density πk can be evaluated point-wise, but
the normalising constant Zπk is typically unknown. We will use SMC to simulate sequentially
n
k=1. In particular, we consider the fully adapted SMC sampler (Pitt
from the distributions
and Shephard, 1999), which corresponds to a speciﬁc choice of resampling weights and proposal
distribution, chosen in such a way that the importance weights are all equal to 1/N . Speciﬁcally,
the proposal distribution (often referred to as the optimal proposal) is given by ¯qk(xk
x1:k−1) =
Zqk (x1:k−1)−1qk(xk

x1:k−1), where

¯πk

}

{

|

|

qk(xk

x1:k−1) :=

|

πk(x1:k)
πk−1(x1:k−1)

.

In addition, the normalising “constant” Zqk (x1:k−1) = (cid:82) qk(xk
the resampling weights, i.e. the particles at time k
before they are propagated to time k. For notational simplicity, we use the convention x1:0 =
q1(x1
|
Algorithm 2.

x1:k−1)dxk is further used to deﬁne
1 are resampled according to Zqk (x1:k−1)
,
∅
x1:0) = π1(x1) and Zq1(x1:0) = Zπ1. The fully adapted auxiliary SMC sampler is given in

−

|

k}

As mentioned above, at each iteration k = 1, . . . , n, the method produces unweighted samples
N
X i
i=1 approximating ¯πk. It also produces an unbiased estimate (cid:98)Zπk of Zπk (Del Moral, 2004,
{
Proposition 7.4.1). The algorithm is expressed in a slightly non-standard form; at iteration k we
loop over the ancestor particles, i.e. the particles after resampling at iteration k
1, and let each
ancestor particle j generate mj
k offsprings. (The variable L is just for bookkeeping.) This is done
to clarify the connection with the NSMC procedure below. Furthermore, we have included a (com-
N
pletely superﬂuous) resampling step at iteration k = 1, where the “dummy variables”
i=1
{
N
i=1. The analogue of
are resampled according to the (all equal) weights
this step is, however, used in the NSMC algorithm, where the initial normalising constant Zπ1 is
estimated. We thus have to resample the corresponding initial particle systems accordingly.

Zπ1}
{

Zq1(X i

N
i=1 =

1:0)
}

1:0}

X i

−

{

7

NAESSETH, LINDSTEN AND SCH ¨ON

Algorithm 2 SMC (fully adapted)

1. Set (cid:98)Zπ0 = 1.

2. for k = 1 to n

(a) Compute (cid:98)Zπk = (cid:98)Zπk−1 ×
(b) Draw m1:N
k
1, . . . , N .

(c) Set L

0

←

(d) for j = 1 to N
i. Draw X i

ii. Set L

¯qk(
k ∼
· |
L + mj
k.

←

1
N

(cid:80)N

j=1 Zqk (X j

1:k−1).

from a multinomial distribution with probabilities

1:k−1)

Zqk (X j
(cid:96)=1 Zqk (X (cid:96)

1:k−1)

(cid:80)N

, for j =

X j

1:k−1) and let X i

1:k = (X j

1:k−1, X i

k) for i = L+1, . . . , L+mj
k.

4.2 Fully Adapted Nested SMC Samplers

In analogue with Section 3, assume now that we are not able to simulate exactly from ¯qk, nor
compute Zqk . Instead, we have access to a class Q which satisﬁes condition (A1). The proposed
NSMC method is then given by Algorithm 3.

Algorithm 3 Nested SMC (fully adapted)

1. Set (cid:98)Zπ0 = 1.

2. for k = 1 to n

1:k−1), M ) for j = 1, . . . , N .

X j

· |
qk = qj.GetZ() for j = 1, . . . , N .
j=1 (cid:98)Zj
qk

(a) Initialise qj = Q(qk(
(b) Set (cid:98)Zj
(c) Compute (cid:98)Zπk = (cid:98)Zπk−1 ×
(d) Draw m1:N

(cid:110) 1
N

(cid:80)N

(cid:111)

.

from a multinomial distribution with probabilities

for j =

(cid:98)Zj
qk
(cid:96)=1 (cid:98)Z(cid:96)
qk

(cid:80)N

i. Compute X i

k = qj.Simulate() and let X i

1:k = (X j

1:k−1, X i

k) for i = L+1, . . . , L+

k
1, . . . , N .

(e) Set L

0

←

(f) for j = 1 to N

mj
k.
ii. delete qj.
iii. Set L

←

L + mj
k.

8

NESTED SEQUENTIAL MONTE CARLO METHODS

Algorithm 3 can be seen as an exact approximation of the fully adapted SMC sampler in Al-
gorithm 2.
(In Appendix A.1 we provide a formulation of NSMC with arbitrary proposals and
resampling weights.) We replace the exact computation of Zqk and exact simulation from ¯qk, by the
approximate procedures available through Q. Despite this approximation, however, Algorithm 3 is
a valid SMC method. This is formalised by the following theorem.

Theorem 2. Assume that Q satisﬁes condition (A1). Then, under certain regularity conditions on
the function f : Xk
k (f ), both speciﬁed in Appendix A.1.2,
we have

Rd and for an asymptotic variance ΣM

(cid:55)→

N 1/2

(cid:32)

1
N

N
(cid:88)

i=1

(cid:33)

D

f (X i

1:k)

¯πk(f )

−

−→ N

(0, ΣM

k (f )),

where

X i

{

1:k}

M

i=1 are generated by Algorithm 3 and D
−→

denotes convergence in distribution.

Proof See Appendix A.1.2.

Remark 3. The key point with Theorem 2 is that, under certain regularity conditions, the NSMC
method converges at rate √N even for a ﬁxed (and ﬁnite) value of the precision parameter M . The
asymptotic variance ΣM
k (f ), however, will depend on the accuracy and properties of the approxi-
mative procedures of Q. We leave it as future work to establish more informative results, relating
the asymptotic variance of NSMC to that of the ideal, fully adapted SMC sampler.

4.3 Backward Simulation and Modularity of NSMC

As previously mentioned, the NSMC procedure is modular in the sense that we can make use of
Algorithm 3 also to deﬁne the class Q. Thus, we now view ¯πn as the proposal distribution that we
wish to approximately sample from using NSMC. Algorithm 3 directly generates an estimate (cid:98)Zπn
of the normalising constant of πn (which indeed is unbiased, see Theorem 6). However, we also
need to generate a sample (cid:101)X1:n such that ( (cid:101)X1:n, (cid:98)Zπn) is properly weighted for πn.

1, . . . , N

The simplest approach, akin to the Nested IS procedure described in Section 3.2, is to draw Bn
1:n . This will indeed result in a valid deﬁnition
uniformly on
of the Simulate procedure. However, this approach will suffer from the well known path degen-
eracy of SMC samplers. In particular, since we call qj.Simulate() multiple times in Step 2(f)i of
Algorithm 3, we risk to obtain (very) strongly correlated samples by this simple approach.

and return (cid:101)X1:n = X Bn

}

{

It is possible to improve the performance of the above procedure by instead making use of
a backward simulator (Godsill et al., 2004; Lindsten and Sch¨on, 2013) to simulate (cid:101)X1:n. The
backward simulator, given in Algorithm 4, is a type of smoothing algorithm; it makes use of the
particles generated by a forward pass of Algorithm 3 to simulate backward in “time” a trajectory
(cid:101)X1:n approximately distributed according to ¯πn.

Remark 4. Algorithm 4 assumes unweighted particles and can thus be used in conjunction with the
fully adapted NSMC procedure of Algorithm 2. If, however, the forward ﬁlter is not fully adapted
the weights need to be accounted for in the backward simulation; see Appendix A.1.3.

The modularity of NSMC is established by the following result.

9

NAESSETH, LINDSTEN AND SCH ¨ON

Algorithm 4 Backward simulator (fully adapted)

1. Draw Bn uniformly on

1, . . . , N

{

.

}

2. Set (cid:101)Xn = X Bn
n .

3. for k = n

1 to 1

−
(a) Compute (cid:102)W j

k =

πn((X j

1:k, (cid:101)Xk+1:n))
πk(X j
1:k)

for j = 1, . . . , N .

(b) Draw Bk from a categorical distribution with probabilities

for j = 1, . . . , N .

(cid:102)W j
k
(cid:96)=1 (cid:102)W (cid:96)
k

(cid:80)N

(c) Set (cid:101)Xk:n = (X Bk

k , (cid:101)Xk+1:n).

Deﬁnition 5. Let p = Q(πn, N ) be deﬁned as follows:

1. The constructor executes Algorithm 3 with target distribution πn and with N particles, and

p.GetZ() returns the estimate of the normalising constant (cid:98)Zπn.

2. p.Simulate() executes Algorithm 4 and returns (cid:101)X1:n.

Theorem 6. The class Q deﬁned as in Deﬁnition 5 satisﬁes condition (A1).

Proof See Appendix A.1.3.

A direct, and important, consequence of Theorem 6 is that NSMC can be used as a component of
powerful learning algorithms, such as the particle Markov chain Monte Carlo (PMCMC) method
(Andrieu et al., 2010) and many of the other methods discussed in Section 5. Since standard SMC
is a special case of NSMC, Theorem 6 implies proper weighting also of SMC.

5. Practicalities and Related Work

There has been much recent interest in using SMC within SMC in various ways. The SMC2 by
Chopin et al. (2013) and the recent method by Crisan and M´ıguez (2013) are sequential learning
algorithms for state space models, where one SMC sampler for the parameters is coupled with an-
other SMC sampler for the latent states. Johansen et al. (2012) and Chen et al. (2011) address the
state inference problem by splitting the state variable into different components and run coupled
SMC samplers for these components. These methods differ substantially from NSMC; they solve
different problems and the “internal” SMC sampler(s) is constructed in a different way (for approxi-
mate marginalisation instead of for approximate simulation). Another related method is the random
weights PF of Fearnhead et al. (2010a), requiring exact samples from ¯q and where the importance
weights are estimated using a nested Monte Carlo algorithm.

The method most closely related to NSMC is the space-time particle ﬁlter (ST-PF) (Beskos
et al., 2014a), which has been developed independently and in parallel with our work. The ST-PF
is also designed for solving inference problems in high-dimensional models. It can be seen as a
island PF (Verg´e et al., 2015) implementation of the method presented by Naesseth et al. (2014b).
Speciﬁcally, for a spatio-temporal models they run an island PF over both spatial and temporal

10

NESTED SEQUENTIAL MONTE CARLO METHODS

dimensions. However, the ST-PF does not generate an approximation of the fully adapted SMC
sampler.

Another key distinction between NSMC and ST-PF is that in the latter each particle in the
“outer” SMC sampler comprises a complete particle system from the “inner” SMC sampler. For
NSMC, on the other hand, the particles will simply correspond to different hypotheses about the
latent variables (as in standard SMC), regardless of how many samplers that are nested. This is
a key feature of NSMC, since it implies that it is easily distributed over the particles. The main
N
j=1 and the calls to the Simulate
computational effort of Algorithm 3 is the construction of
}
procedure, which can be done independently for each particle. This leads to improved memory
efﬁciency and lower communication costs. Furthermore, we have found (see Section 6) that NSMC
can outperform ST-PF even when run on a single machine with matched computational costs.

qj

{

Another strength of NSMC methods are their relative ease of implementation, which we show
in Section 6.3. We use the framework to sample from what is essentially a cubic grid Markov ran-
dom ﬁeld (MRF) model just by implementing three nested samplers, each with a target distribution
deﬁned on a simple chain.

There are also other SMC-based methods designed for high-dimensional problems, e.g., the
block PF studied by Rebeschini and van Handel (2015), the location particle smoother by Briggs
et al. (2013) and the PF-based methods reviewed in Djuric and Bugallo (2013). However, these
methods are all inconsistent, as they are based on various approximations that result in systematic
errors.

The previously mentioned PMCMC (Andrieu et al., 2010) is a related method, where SMC
is used as a component of an MCMC algorithm. We make use of a very similar extended space
approach to motivate the validity of our algorithm. Note that our proposed algorithm can be used as
a component in PMCMC and most of the other algorithms mentioned above, which further increases
the scope of models it can handle.

6. Experimental Results

We illustrate NSMC on three high-dimensional examples, both with real and synthetic data. We
compare NSMC with standard (bootstrap) PF and the ST-PF of Beskos et al. (2014a) with equal
computational budgets on a single machine (i.e., neglecting the fact that NSMC is more easily dis-
tributed). These methods are, to the best of our knowledge, the only other available consistent online
methods for full Bayesian inference in general sequential models. For more detailed explanations
of the models and additional results, see Appendix A.32.

6.1 Gaussian State Space Model

=

Xk, Yk
{

We start by considering a high-dimensional Gaussian state space model, where we have access to
the true solution through belief propagation. The latent variables and measurements
,
}
d
with
k lattice Gaussian MRF. The true data is
l=1, are modeled by a d
simulated from a nearly identical state space model (see Appendix A.3.1). We run a 2-level NSMC
sampler. The outer level is fully adapted, i.e. the proposal distribution is qk = p(xk
xk−1, yk),
which thus constitute the target distribution for the inner level. To generate properly weighted
samples from qk, we use a bootstrap PF operating on the d components of the vector xk. Note that

Xk,l, Yk,l
{

X1:k, Y1:k

×

}

}

{

|

2. Code available at https://github.com/can-cs/nestedsmc

11

NAESSETH, LINDSTEN AND SCH ¨ON

d = 50

d = 100

d = 200

S
S
E

S
R
E

Figure 2: Top: Median (over dimension) ESS (4) and 15–85% percentiles (shaded region). Bottom:
The ERS (5) based on the resampling weights in the (outermost) particle ﬁlter. The results
are based on 100 independent runs for the Gaussian MRF with dimension d.

we only use bootstrap proposals where the actual sampling takes place, and that the conditional
distribution p(xk

xk−1, yk) is not explicitly used.

We simulate data from this model for k = 1, . . . , 100 for different values of d = dim(xk)

50, 100, 200
{
with both the ST-PF and standard (bootstrap) PF.

∈
. The exact ﬁltering marginals are computed using belief propagation.We compare
}

|

The results are evaluated based on the effective sample size (ESS, see e.g. Fearnhead et al.

(2010b)) deﬁned as,

ESS(xk,l) =

(cid:18)

(cid:20)

E

((cid:98)xk,l−µk,l)2
σ2

k,l

(cid:21)(cid:19)−1

,

|

where (cid:98)xk,l denote the mean estimates and µk,l and σ2
k,l denote the true mean and variance of
y1:k obtained from belief propagation. The expectation in (4) is approximated by averag-
xk,l
ing over 100 independent runs of the involved algorithms. The ESS reﬂects the estimator accuracy,
obvious by the deﬁnition which is tightly related to the mean-squared-error. Intuitively the ESS
corresponds to the equivalent number of i.i.d. samples needed for the same accuracy.

We also consider the effective resample size (ERS, Kong et al. (1994)), which is based on the

resampling weights at the top levels in the respective SMC algorithms,

(4)

(5)

The ERS is an estimate of the effective number of unique particles (or particle systems in the case
of ST-PF) available at each resampling step.

We use N = 500 and M = 2

d for NSMC and match the computational time for ST-PF
and bootstrap PF. We report the results in Figure 2. The bootstrap PF is omitted from d = 100,

·

ERS =

(cid:16)(cid:80)N

i=1 (cid:98)Zi
qk
(cid:16)
(cid:98)Zi
qk

i=1

(cid:80)N

(cid:17)2
(cid:17)2 .

12

NESTED SEQUENTIAL MONTE CARLO METHODS

200 due to its poor performance already for d = 50 (which is to be expected). Each dimension
l = 1, . . . , d provides us with a value of the ESS, so we present the median (lines) and 15–85%
percentiles (shaded regions) in the ﬁrst row of Figure 2. The ERS is displayed in the second row of
Figure 2. Note that ESS gives a better reﬂection of estimation accuracy than ERS.

We have conducted additional experiments with different model parameters and different choices
for N and M (some additional results are given in Appendix A.3.1). Overall the results seem to be
in agreement with the ones presented here, however ST-PF seems to be more robust to the trade-off
between N and M . A rule-of-thumb for NSMC is to generally try to keep N as high as possible,
while still maintaining a reasonably large ERS.

6.2 Non-Gaussian State Space Model

Next, we consider an example with a non-Gaussian SSM, borrowed from Beskos et al. (2014a)
where the full details of the model are given. The transition probability p(xk
xk−1) is a localised
|
xk) is t-distributed. The model dimension
Gaussian mixture and the measurement probability p(yk
is d = 1 024. Beskos et al. (2014a) report improvements for ST-PF over both the bootstrap PF
and the block PF by Rebeschini and van Handel (2015). We use N = M = 100 for both ST-PF
and NSMC (the special structure of this model implies that there is no signiﬁcant computational
overhead from running backward sampling) and the bootstrap PF is given N = 10 000. In Figure 3

|

Figure 3: Median ESS with 15

85% percentiles (shaded region) for the non-Gaussian SSM.

−

we report the ESS (4), estimated according to Carpenter et al. (1999). The ESS for the bootstrap PF
is close to 0, for ST-PF around 1–2, and for NSMC slightly higher at 7–8. However, we note that all
methods perform quite poorly on this model, and to obtain satisfactory results it would be necessary
to use more particles.

6.3 Spatio-Temporal Model – Drought Detection

In this ﬁnal example we study the problem of detecting droughts based on measured precipitation
data (Jones and Harris, 2013) for different locations on earth. We look at the situation in North
America during the years 1901–1950 and the Sahel region in Africa during the years 1950–2000.
These spatial regions and time frames were chosen since they include two of the most devastating
droughts during the last century, the so-called Dust Bowl in the US during the 1930s (Schubert

13

et al., 2004) and the decades long drought in the Sahel region in Africa starting in the 1960s (Foley
et al., 2003; Hoerling et al., 2006). We consider the spatio-temporal model deﬁned by Fu et al.

NAESSETH, LINDSTEN AND SCH ¨ON

Xk−1

· · ·

Xk+1

· · ·

Xk,1:2,1

Xk,1:2,3

N
→

Xk

M1
→
Xk,1:2,2

↓ M2

↓ M2

↓ M2

Figure 4: Illustration of the three-level NSMC.

{

Xk,i,j

(2012) and compare with the results therein. Each location in a region is modelled to be in either
a normal state 0 or in an abnormal state 1 (drought). Measurements are given by precipitation (in
millimeters) for each location and year. At every time instance k our latent structure is described by
I,J
a rectangular 2D grid Xk =
i=1,j=1; in essence this is the model showcased in Figure 1. Fu
et al. (2012) considers the problem of ﬁnding the maximum aposteriori conﬁguration, using a linear
programming relaxation. We will instead compute an approximation of the full posterior ﬁltering
distribution ¯πk(xk) = p(xk
y1:k). The rectangular structure is used to instantiate an NSMC method
that on the ﬁrst level targets the full posterior ﬁltering distribution. To sample from Xk we run, on
the second level, an NSMC procedure that operates on the “columns” Xk,1:I,j, j = 1, . . . , J.
Finally, to sample each column Xk,1:I,j we run a third level of SMC, that operates on the individual
components Xk,i,j, i = 1, . . . , I, using a bootstrap proposal. The structure of our NSMC method
applied to this particular problem is illustrated in Figure 4.

}

|

0.5, 0.7, 0.9
{

Figure 5 gives the results on the parts of North America that we consider. The ﬁrst row shows the
, for both regions.
number of locations where the estimate of p(xk,i,j = 1) exceeds
}
These results seems to be in agreement with Fu et al. (2012, Figures 3, 6). However, we also
receive an approximation of the full posterior and can visualise uncertainty in our estimates, as
illustrated by the three different levels of posterior probability for drought. In general, we obtain
a rich sample diversity from the posterior distribution. However, for some problematic years the
sampler degenerates, with the result that the three credibility levels all coincide. This is also visible
in the second row of Figure 5, where we show the posterior estimates p(xk,i,j
y1:k) for the years
1939–1941, overlayed on the regions of interest. For year 1940 the sampler degenerates and only
reports 0-1 probabilities for all sites. Naturally, one way to improve the estimates is to run the
sampler with a larger number of particles, which has been kept very low in this proof-of-concept.

|

7. Conclusions

We have shown that a straightforward NSMC implementation with fairly few particles can attain
reasonable approximations to the ﬁltering problem for dimensions in the order of hundreds, or
even thousands. This means that NSMC methods takes the SMC framework an important step
closer to being viable for high-dimensional statistical inference problems. However, NSMC is not

14

NESTED SEQUENTIAL MONTE CARLO METHODS

North America region

Sahel region

North America 1939

North America 1940

North America 1941

Figure 5: Top: Number of locations with estimated p(x = 1) >

for the two regions.
Bottom: Estimate of p(xt,i = 1) for all sites over a span of 3 years. All results for
N = 100, N1 =

0.5, 0.7, 0.9
}

, N2 = 20.

{

30, 40
}

{

a silver bullet for solving high-dimensional inference problems, and the approximation accuracy
will be highly model dependent. Hence, much work remains to be done, for instance on combining
NSMC with other techniques for high-dimensional inference such as localisation (Rebeschini and
van Handel, 2015) and annealing (Beskos et al., 2014b), in order to solve even more challenging
problems.

Acknowledgments

This work was supported by the projects: Learning of complex dynamical systems (Contract num-
ber: 637-2014-466) and Probabilistic modeling of dynamical systems (Contract number: 621-2013-
5524), both funded by the Swedish Research Council.

15

NAESSETH, LINDSTEN AND SCH ¨ON

Appendix A. Appendix

In this appendix we start out in Section A.1 by providing a more general formulation of the NSMC
method and proofs of the central limit and proper weighting theorems of the main manuscript.
We also detail (Section A.2) a straightforward extension of nested IS to a sequential version. We
show that a special case of this nested sequential IS turns out to be more or less equivalent to the
importance sampling squared algorithm by Tran et al. (2013). This relationship serves as evidence
that illustrates that the NSMC framework being more widely applicable than the scope of problems
considered in this article. Finally, in Section A.3 we give more details and results on the experiments
considered in the main manuscript.

A.1 Nested Sequential Monte Carlo

We start by presenting a general formulation of a nested auxiliary SMC sampler in Algorithm 5. In
this formulation, qk(xk

x1:k−1) is an arbitrary (unnormalised) proposal, normalised by

|

(cid:90)

Zqk (x1:k−1) =

qk(xk

x1:k−1)dxk.

|

Furthermore, the resampling weights are obtain by multiplying the importance weights with the
arbitrary adjustment multipliers νk−1(x1:k−1, Zqk ), which may depend on both the state sequence
x1:k−1 and the normalising constant (estimate). The fully adapted NSMC sampler (Algorithm 3 in
the main document) is obtained as a special case if we choose

qk(xk

x1:k−1) =

|

πk(x1:k)
πk−1(x1:k−1)

and νk−1(x1:k−1, Zqk ) = Zqk , in which case the importance weights are indeed given by W i

1.

k ≡

A.1.1 NESTED SMC IS SMC

The validity of Algorithm 5 can be established by interpreting the algorithm as a standard SMC
procedure for a sequence of extended target distributions.
If (cid:98)Zqk is computed deterministically,
proper weighting (i.e., unbiasedness) ensures that (cid:98)Zqk = Zqk and it is evident that the algorithm
reduces to a standard SMC sampler. Hence, we consider the case when the normalising constant
estimates (cid:98)Zqk are random.

k−1(uk−1

For k = 1, . . . , n + 1, let us introduce the random variable Uk−1 which encodes the complete
internal state of the object q generated by q = Q(qk(
x1:k−1), M ). Let the distribution of Uk−1 be
denoted as ¯ψM
x1:k−1). To put Algorithm 5 into a standard (auxiliary) SMC framework,
we shall interpret steps 2a–2b of Algorithm 5 as being the last two steps carried out during iteration
1, rather than the ﬁrst two steps carried out during iteration k. This does not alter the algorithm
k
per se, but it results in that the resampling step is conducted ﬁrst at each iteration, which is typically
the case for standard auxiliary SMC formulations.

−

· |

|

The estimator of the normalising constant is computable from the internal state of q, so that we
can introduce a function τk such that (cid:98)Zqk = τk(Uk−1). Furthermore, note that the simulation of Xk
via Xk = q.Simulate() is based solely on the internal state Uk−1, and denote by ¯γM
Uk−1) the
distribution of Xk.

k (xk

|

16

NESTED SEQUENTIAL MONTE CARLO METHODS

Algorithm 5 Nested SMC (auxiliary SMC formulation)

i=1 to arbitrary dummy variables. Set W i
N

0 = 1 for i = 1, . . . , N . Set (cid:98)Zπ0 = 1.

1. Set

X i
0}
{
2. for k = 1 to n

(a) Initialise qj = Q(qk(
(b) Compute (cid:98)Zj
(c) Compute (cid:98)νj

X j

· |

qk = qj.GetZ() for j = 1, . . . , N .
1:k−1, (cid:98)Zj
k−1 = νk−1(X j

qk ) for j = 1, . . . , N .

1:k−1), M ) for j = 1, . . . , N .

from a multinomial distribution with probabilities

k−1W j
(cid:98)νj
(cid:80)N
(cid:96)=1 (cid:98)ν(cid:96)

k−1
k−1W (cid:96)

k−1

for

(d) Draw m1:N

k

j = 1, . . . , N .

(e) Set L

0

←

(f) for j = 1 to N

mj
k.

ii. Compute W i

k =

iii. delete qj.
iv. Set L

L + mj
k.

←
(g) Compute (cid:98)Zπk = (cid:98)Zπk−1 ×

(cid:110) 1
N

i. Compute X i

k = qj.Simulate() and let X i

1:k = (X j

1:k−1, X i

k) for i = L+1, . . . , L+

πk(X i
πk−1(X j

1:k)
1:k−1)

(cid:98)Zj
qk
(cid:98)νj
k−1qk(X i
k |

X j

1:k−1)

for i = L + 1, . . . , L + mj
k.

(cid:80)N

j=1 (cid:98)νj

k−1W j

k−1

(cid:111)

(cid:110)
((cid:80)N

×

j=1 W j

k )/((cid:80)N

j=1 W j

k−1)

(cid:111)

.

Lemma 7. Assume that Q satisﬁes condition (A1) in the main manuscript. Then,

(cid:90)

τk(uk−1)¯γM

k (xk

uk−1) ¯ψM

k−1(uk−1

x1:k−1)duk−1 = qk(xk

x1:k−1).

|

Proof The pair (Xk, τk(Uk−1)) are properly weighted for qk. Hence, for a measurable function f ,

E[f (Xk)τk(Uk−1)

x1:k−1] =

|

f (xk)τk(uk−1)¯γM
(cid:90)

k (xk

|

uk−1) ¯ψM

k−1(uk−1
(cid:90)

|

x1:k−1)duk−1dxk

= Zk(x1:k−1)

f (xk)¯qk(xk

x1:k−1)dxk =

f (xk)qk(xk

x1:k−1)dxk.

|

Since f is arbitrary, the result follows.

|

(cid:90) (cid:90)

|

|

We can now deﬁne the sequence of (unnormalised) extended target distributions for the Nested

SMC sampler as,

Πk(x1:k, u0:k) :=

τk(uk−1) ¯ψM

k (uk
|
qk(xk
|

x1:k)¯γM
x1:k−1)

k (xk

uk−1)

|

πk(x1:k)
πk−1(x1:k−1)

Πk−1(x1:k−1, u0:k−1),

and Π0(u0) = ¯ψM

0 (u0). We write Θk = Xk

Uk for the domain of Πk.

×

17

NAESSETH, LINDSTEN AND SCH ¨ON

|

|

Lemma 8. Assume that Q satisﬁes condition (A1) in the main manuscript. Then,

(cid:90)

τk(uk−1)¯γM

k (xk

|

uk−1)Πk−1(x1:k−1, u0:k−1)du0:k−1 = πk−1(x1:k−1)qk(xk

x1:k−1).

Proof The proof follows by induction. At k = 1, we have (cid:82) τ1(u0)¯γM
q1(x1) by Lemma 7. Hence, assume that the hypothesis holds for k

1 (x1

|

1 and consider

u0) ¯ψM

0 (u0)du0 =

≥

uk)τk(uk−1) ¯ψM

k (uk

x1:k)¯γM

uk−1)Πk−1(x1:k−1, u0:k−1)du0:k

(cid:90)

τk+1(uk)¯γM

k+1(xk+1

|
πk(x1:k)
πk−1(x1:k−1)qk(xk

uk)Πk(x1:k, u0:k)du0:k

=

(cid:90)

·

·

=

(cid:90)

=

x1:k−1)

|
k+1(xk+1

τk+1(uk)¯γM
πk(x1:k) (cid:0)(cid:82) τk+1(uk)¯γM

|

k+1(xk+1
|
πk−1(x1:k−1)qk(xk

k (xk
(cid:1)

|

x1:k)duk

|
k (uk

uk) ¯ψM
x1:k−1)

|

|

uk−1)Πk−1(x1:k−1, u0:k−1)du0:k−1

τk(uk−1)¯γM

k (xk

|
πk(x1:k)qk+1(xk+1

|

x1:k)πk−1(x1:k−1)qk(xk

x1:k−1)

|

= πk(x1:k)qk+1(xk+1

x1:k),

πk−1(x1:k−1)qk(xk

x1:k−1)

|

where the penultimate equality follows by applying Lemma 7 and the induction hypothesis to the
two integrals, respectively.

As a corollary to Lemma 8, it follows that

(cid:90)

Πk(x1:k, u0:k)du0:k = πk(x1:k).

(6)

Consequently, Πk is normalised by the same constant Zπk as πk, and by deﬁning ¯Πk(x1:k, u0:k) :=
Z−1
πk Πk(x1:k, u0:k) we obtain a probability distribution which admits ¯πk as a marginal (note that
¯Π0 = Π0, which is normalised by construction). This implies that we can use ¯Πk as a proxy
for ¯πk in a Monte Carlo algorithm, i.e., samples drawn from ¯Πk can be used to compute ex-
pectations w.r.t. ¯πk. This is precisely what Algorithm 5 does; it is a standard auxiliary SMC
sampler for the (unnormalised) target sequence Πk, k = 0, . . . , n, with adjustment multiplier
weights νk−1(x1:k−1, τk(uk−1)) and proposal distribution ¯γM
x1:k). The (stan-
dard) weight function for this sampler is thus given by

uk−1) ¯ψM

k (xk

k (uk

|

|

Wk(x1:k, u0:k)

πk(x1:k)
πk−1(x1:k−1)

∝

τk(uk−1)

,

(7)

νk−1(x1:k−1, τk(uk−1))qk(xk

x1:k−1)

|

which is the same as the expression on line 2(f)ii of Algorithm 5.

A.1.2 CENTRAL LIMIT THEOREM – PROOF OF THEOREM 2 IN THE MAIN MANUSCRIPT

Now that we have established that Nested SMC is in fact a standard auxiliary SMC sampler, albeit
on an extended state space, we can reuse existing convergence results from the SMC literature; see

18

NESTED SEQUENTIAL MONTE CARLO METHODS

e.g., Johansen and Doucet (2008); Douc and Moulines (2008); Douc et al. (2009); Chopin (2004)
or the extensive textbook by Del Moral (2004).

Here, in order to prove Theorem 2 of the main manuscript, we make use of the result for the
auxiliary SMC sampler by Johansen and Doucet (2008), which in turn is based on the central limit
theorem by Chopin (2004). The technique used by Johansen and Doucet (2008) is to reinterpret (as
detailed below) the auxiliary SMC sampler as a sequential importance sampling and resampling
(SISR) particle ﬁlter, by introducing the modiﬁed (unnormalised) target distribution

Π(cid:48)

k(x1:k, u0:k) := νk(x1:k, τk+1(uk))Πk(x1:k, u0:k).

(8)

The auxiliary SMC sampler described in the previous section can then be viewed as a SISR algo-
rithm for (8). Indeed, if we write ¯QM
uk−1) for
the joint proposal distribution of (xk, uk), then the weight function for this SISR sampler is given
by

x1:k−1, uk−1) := ¯ψM

k (xk, uk

x1:k)¯γM

k (xk

k (uk

|

|

|

W (cid:48)

k(x1:k, u0:k) :=

¯Π(cid:48)

k(x1:k, u0:k)

¯QM

k (xk, uk

x1:k−1, uk−1) ¯Π(cid:48)

k−1(x1:k−1, u0:k−1)

νk(x1:k, τk+1(uk))Wk(x1:k, u0:k),

∝

(9)

where Wk is deﬁned in (7). This weight expression thus accounts for both the importance weights
and the adjustment multipliers of the auxiliary SMC sampler formulation.

Since this SISR algorithm does not target ¯Πk (and thus not ¯πk) directly, we use an additional IS
step to compute estimators of expectations w.r.t. to ¯π. The proposal distribution for this IS procedure
is given by

¯Γk(x1:k, u0:k) := ¯QM

k (xk, uk

x1:k−1, uk−1) ¯Π(cid:48)

k−1(x1:k−1, u0:k−1).

(10)

Note that we obtain an approximation of (10) after the propagation Step 2(f)i of Algorithm 5, but
before the weighting step. The resulting IS weights, for target distribution ¯Πk(x1:k, u0:k) and with
proposal distribution (10), are given by

|

|

¯Πk(x1:k, u0:k)
¯Γk(x1:k, u0:k)

=: ωk(x1:k, u0:k)

Wk(x1:k, u0:k).

∝

Hence, with f : Xk
(cid:55)→
obvious abuse of notation) by the estimator

Rd being a test function of interest we can estimate E¯πk [f ] = E ¯Πk [f ] (with

N
(cid:88)

i=1

kf (X i
W i
1:k)
(cid:80)N
(cid:96)=1 W (cid:96)
k

,

(11)

which, again, is in agreement with Algorithm 5.

We have now reinterpreted the NSMC algorithm; ﬁrst as a standard auxiliary SMC sampler,
and then further as a standard SISR method. Consequently, we are now in the position of directly
applying, e.g., the central limit theorem by Chopin (2004, Theorem 1). The conditions and the
statement of the theorem are reproduced here for clarity.

19

NAESSETH, LINDSTEN AND SCH ¨ON

For any measurable function f : Θ0

measurable function f : Θk

Rd,

Rd, let (cid:101)V M

0 (f ) = Var ¯ψM

0

(cid:55)→

(f ) and deﬁne, for any

(cid:55)→
(cid:101)V M
k (f ) = (cid:98)V M
k−1(E ¯QM
V M
k (W (cid:48)
k (f ) = (cid:101)V M
(cid:98)V M
k (f ) = V M

k(f

k

[f ]) + E ¯Π(cid:48)
E ¯Π(cid:48)

[f ])),

k−1

−

k
(f ),

k (f ) + Var ¯Π(cid:48)

k

[Var ¯QM

k

(f )],

k > 0,

0,

0.

k

k

≥

≥

Deﬁne recursively Φk to be the set of measurable functions f : Θk
δ > 0 with E¯Γk [
2+δ] <
(cid:107)
(cid:107)
Φk−1. Furthermore, assume that the identity function f
≡
follows by Chopin (2004, Theorem 1 and Lemma A.1) that

W (cid:48)

kf

∞

and such that the function (x1:k−1, u0:k−1)

Rd such that there exists a
E ¯QM
kf ] is in
(cid:55)→
1 belongs to Φk for each k. Then, it

[W (cid:48)

(cid:55)→

k

N 1/2

(cid:32) N
(cid:88)

i=1

1
N

f (X i

1:k, U i

0:k)

E¯Γk [f ])

−

−→ N

(0, (cid:101)V M

k (f )),

(12)

(cid:33)

D

for any function f such that the function (x1:k−1, u0:k−1)
exists a δ > 0 such that E¯Γk [
(cid:107)
samples obtained after the propagation Step 2(f)i of Algorithm 5, but before the weighting step.

E¯Γk [f ]] is in Φk−1 and there
. The convergence in (12) thus holds for the unweighted

2+δ] <
(cid:107)

E ¯QM

∞

(cid:55)→

−

[f

f

k

To complete the proof, it remains to translate (12) into a similar result for the IS estimator (11).
To this end we make use of Chopin (2004, Lemma A.2) which is related to the IS correction step
Rd denote the
of the SMC algorithm. Speciﬁcally, for a function f : Xk
Rd such
extension of f to Θk, deﬁned by f e(x1:k, u0:k) = f (x1:k). Then, for any f : Xk
E ¯QM
[ωkf e] is in Φk−1 and there exists a δ > 0 such that
that the function (x1:k−1, u0:k−1)
E¯Γk [
2+δ] <
ωkf e
(cid:107)

Rd, let f e : Θk

, we have

(cid:55)→

(cid:55)→

(cid:55)→

∞

(cid:55)→

(cid:107)

k

N 1/2

(cid:32) N
(cid:88)

i=1

kf (X i
W i
1:k)
(cid:80)N
(cid:96)=1 W (cid:96)
k −

(cid:33)

D

¯πk(f )

−→ N

(0, ΣM

k (f )),

where

(X i
{

1:k, W i
k)

i=1 are generated by Algorithm 5 and ΣM
M
}

k (f ) = (cid:101)V M

k (ωk(f e

E ¯Πk [f e])).

−

A.1.3 NESTED SMC GENERATES PROPERLY WEIGHTED SAMPLES – PROOF OF THEOREM 6

IN THE MAIN MANUSCRIPT

In the previous two sections we showed that the NSMC procedure is a valid inference algorithm
for ¯πn. Next, we turn our attention to the modularity of the method and the validity of using the
algorithm as a component in another NSMC sampler. Let us start by stating a more general version
of the backward simulator in Algorithm 6. Clearly, if the forward NSMC procedure is fully adapted
W i

1, Algorithm 6 reduces to the backward simulator stated in the main manuscript.

k ≡
We will now show that the pair ( (cid:98)Zπn, (cid:101)X1:n) generated by Algorithms 5 and 6 is properly

weighted for πn(x1:n), and thereby prove Theorem 6 in the main manuscript.

The proof is based on the particle Markov chain Monte Carlo (PMCMC) construction (Andrieu
et al., 2010). The idea used by Andrieu et al. (2010) was to construct an extended target distribution,
incorporating all the random variables generated by an SMC sampler as auxiliary variables. This
opened up for using SMC approximations within MCMC in a provably correct way; these seem-
ingly approximate methods simply correspond to standard MCMC samplers for the (nonstandard)

20

NESTED SEQUENTIAL MONTE CARLO METHODS

Algorithm 6 Backward simulator

1. Draw Bn from a categorical distribution with probabilities

for j = 1, . . . , N .

2. Set (cid:101)Xn = X Bn
n .

3. for k = n

1 to 1

−
(a) Compute (cid:102)W j

k = W j

k

πn((X j

1:k, (cid:101)Xk+1:n))
πk(X j
1:k)

for j = 1, . . . , N .

(b) Draw Bk from a categorical distribution with probabilities

for j = 1, . . . , N .

W j
n
(cid:96)=1 W (cid:96)
n

(cid:80)N

(cid:102)W j
k
(cid:96)=1 (cid:102)W (cid:96)
k

(cid:80)N

(c) Set (cid:101)Xk:n = (X Bk

k , (cid:101)Xk+1:n).

4. return (cid:101)X1:n

extended target distribution. Here we will use the same technique to prove the proper weighing
property of the NSMC procedure.

We start by introducing some additional notation for the auxiliary variables of the extended
target construction. While Algorithm 5 is expressed using multinomial random variables m1:N
in the resampling step, it is more convenient for the sake of the proof to explicitly introduce the
i=1; see e.g., Andrieu et al. (2010). That is, Ai
N
ancestor indices
k is a categorical random
Ai
1 of particle X i
k
k. The
variable on
1:k−1 is ancestor particle at iteration k
, such that X
resampling Step 2d of Algorithm 5 can then equivalently be expressed as: simulate independently
Ai

N
i=1 from the categorical distribution with probabilities

k}
{
1, . . . , N
}
{

Ai

−

k

{

k}

k−1W j
(cid:98)νj
(cid:80)N
(cid:96)=1 (cid:98)ν(cid:96)

k−1
k−1W (cid:96)

k−1

.

{

X 1

k , . . . , X N
k }

Let Xk =
, denote all the
particles, internal states of the proposals, and ancestor indices, respectively, generated at iteration k
of the NSMC algorithm. We can then write down the joint distribution of all the random variables
generated in executing Algorithm 5 (up to an irrelevant permutation of the particle indices) as,

k , . . . , U N
U 1
k }
{

k, . . . , AN
k }

, and Ak =

, Uk =

A1
{

¯ΨNSMC(x1:n, u0:n, a1:n) =

¯ψM
0 (ui
0)

(cid:40) N
(cid:89)

i=1

(cid:41) n
(cid:89)




N
(cid:89)

k=1



i=1

ai
k
(cid:98)ν
k−1W
(cid:80)N
(cid:96)=1 (cid:98)ν(cid:96)

ai
k
k−1
k−1W (cid:96)

k−1

¯QM

k (xi

k, ui
k |

ai
k
1:k−1, u

ai
k
k−1)

x






,

(13)

where we interpret (cid:98)νi

k and W i

k as deterministic functions of (xi

1:k, ui

0:k).

21

NAESSETH, LINDSTEN AND SCH ¨ON

Let Bn denote a random variable deﬁned on

. The extended target distribution for

1, . . . , N
{

}

PMCMC samplers corresponding to (13) is then given by

¯Φ(x1:n, u0:n, a1:n, bn) :=

¯ΨNSMC(x1:n, u0:n, a1:n),

(14)

(cid:98)Zπn
Zπn

W bn
n
(cid:96)=1 W (cid:96)
n

(cid:80)N

where (cid:98)Zπn is a deterministic function of (x1:n, u0:n, a1:n). We know from Andrieu et al. (2010)
that ¯Φ is a probability distribution which admits ¯Πn as its marginal distribution for (X bn
0:n).
Consequently, by (6) it follows that the marginal distribution of X bn
1:n is ¯πn. For later reference we
deﬁne recursively bk−1 := abk
k for k = 1, . . . , n, the particle indices for the trajectory obtained by
tracing backward the genealogy of the bn’th particle at iteration n.

1:n, U bn

We now turn our attention to the backward simulator in Algorithm 6. Backward simulation
has indeed been used in the context of PMCMC, see e.g. Whiteley (2010); Lindsten and Sch¨on
(2013); Lindsten et al. (2014). The strategy used for combining PMCMC with backward simulation
is to show that each step of the backward sampler corresponds to a partially collapsed Gibbs sam-
pling step for the extended target distribution ¯Φ. This implies that the backward sampler leaves ¯Φ
invariant.

We use the same approach here, but we need to be careful in how we apply the existing results,
since the PMCMC distribution ¯Φ is deﬁned w.r.t. to ¯Πn, whereas the backward simulator of Algo-
rithm 6 works with the original target distribution ¯πn. Nevertheless, from the proof of Lemma 1 by
Lindsten et al. (2014) it follows that we can write the following collapsed conditional distribution
of ¯Φ as:

¯Φ(bk, ubk:n
k:n |

x1:k, u0:k−1, a1:k, x

Πn(

W bk
k

∝

bk+1:n
k+1:n , bk+1:n)
bk+1:n
xbk
1:k, x
,
k+1:n }
{
{
bk
a
0:k−1, ubk
Πk(xbk
k
u
1:k,
k }
{

)

bk
a
0:k−1, ubk:n
k
u
k:n }

)

k (ubk
¯ψM
k |

xbk
1:k).

(15)

To simplify this expression, consider,

Πn(x1:n, u0:n)
Πk(x1:k, u0:k)

n
(cid:89)

(cid:26) τs(us−1) ¯ψM

=

=

s=k+1
¯ψM
n (un
¯ψM
k (uk

x1:n)
x1:k)

|
|

s (us
|
qs(xs
|

x1:s)¯γM
x1:s−1)

(cid:40) n
(cid:89)

τs(us−1) ¯ψM

s=k+1

s (xs

us−1)

|

(cid:27)

πs(x1:s)
πs−1(x1:s−1)

s−1(us−1
qs(xs

x1:s−1)¯γM
x1:s−1)

s (xs

us−1)

|

πn(x1:n)
πk(x1:k)

.

(cid:41)

|
|

(16)

By Lemma 7 we know that each factor of the product (in brackets) on the second line integrates to
1 over us−1. Hence, plugging (16) into (15) and integrating over ubk:n

k:n yields

¯Φ(bk

|

x1:k, u0:k−1, a1:k, x

bk+1:n
k+1:n , bk+1:n)

πn(
{

W bk
k

∝

xbk
1:k, x
πk(xbk

bk+1:n
)
k+1:n }
1:k)

,

which coincides with the expression used to simulate the index Bk in Algorithm 6. Hence, sim-
ulation of Bk indeed corresponds to a partially collapsed Gibbs sampling step for ¯Φ and it will
thus leave ¯Φ invariant. (Note that, in comparison with the PMCMC sampler derived by Lindsten

22

NESTED SEQUENTIAL MONTE CARLO METHODS

et al. (2014) we further marginalise over the variables ubk:n
partially collapsed Gibbs step.)

k:n which, however, still results in a valid

We now have all the components needed to prove proper weighting of the combined NSMC/backward

simulation procedure. For notational simplicity, we write

¯ΨBS,k(bk) = ¯Φ(bk

x1:k, u0:k−1, a1:k, x

bk+1:n
k+1:n , bk+1:n),

|

for the distribution of Bk in Algorithm 6. Let ( (cid:98)Zπn, (cid:101)X1:n) be generated by Algorithms 5 and 6. Let
f be a measurable function and consider

E[ (cid:98)Zπnf ( (cid:101)X1:n)] =

(cid:90)

(cid:98)Zπnf (X

b(cid:48)
1:n
1:n )

¯ΨBS,k(db(cid:48)
k)

¯ΨNSMC(d(x1:n, u0:n, a1:n))

(cid:41)

(cid:41)

(cid:40) n
(cid:89)

k=1
(cid:40)n−1
(cid:89)

k=1

= Zπn

(cid:90)

f (X

b(cid:48)
1:n
1:n )

¯ΨBS,k(db(cid:48)
k)

¯Φ(d(x1:n, u0:n, a1:n, b(cid:48)

n)),

where, for the second equality, we have used the deﬁnition (14) and noted that ¯ΨBS,n(bn) =

W bn
n
(cid:96)=1 W (cid:96)
n

(cid:80)N

. However, by the invariance of ¯ΨBS,k w.r.t. ¯Φ, it follows that

E[ (cid:98)Zπnf ( (cid:101)X1:n)] = Zπn

f (X b1:n

1:n ) ¯Φ(d(x1:n, u0:n, a1:n, bn)) = Zπn ¯πn(f ),

(cid:90)

which completes the proof.

A.2 Nested Sequential Importance Sampling

Here we give the deﬁnition of the nested sequential importance sampler and we show that a special
case of this is the importance sampling squared (IS2) method by Tran et al. (2013).

A.2.1 NESTED SEQUENTIAL IMPORTANCE SAMPLING

We present a straightforward extension of the Nested IS class to a sequential IS version. Consider
the following deﬁnition of the Nested SIS Q:

1. Algorithm 7 is executed at the construction of the object p = Q(πn, N ), and p.GetZ() returns

the normalising constant estimate (cid:98)Zπn.

2. p.Simulate() simulates a categorical random variable B with P(B = i) = W i

n/ (cid:80)N

(cid:96)=1 W (cid:96)
n

and returns X B

1:n.

Note that we do not require that the procedure Q is identical for each individual proposal qk,
thus we have a ﬂexibility in designing our algorithm as can be seen in the example in Section A.2.2.
We can motivate the algorithm in the same way as for Nested IS and similar theoretical results hold,
i.e. Nested SIS is properly weighted for πn and it admits ¯πn as a marginal.

A.2.2 RELATION TO IS2

Here we will show how IS2, proposed by Tran et al. (2013), can be viewed as a special case of
Nested SIS. We are interested in approximating the posterior distribution of parameters θ given

23

NAESSETH, LINDSTEN AND SCH ¨ON

Algorithm 7 Nested SIS (all i for 1, . . . , N )

1. Initialise qi = Q(q1(
·

), M ).

2. Set (cid:98)Zi

q1 = qi.GetZ(), X i

1 = qi.Simulate().

q1π1(X i
(cid:98)Zi
1)
q1(X i
1)

.

3. Set W i

1 =

4. delete qi.

5. for k = 2 to n:

· |

(a) Initialise qi = Q(qk(
(b) Set (cid:98)Zi

X i
1:k−1), M ).
k = qi.Simulate().
qk = qi.GetZ(), X i
qk πk(X i
(cid:98)Zi
1:k−1)
k |
X i
qk(X i
k |

X i
1:k−1)

k = W i

(c) Set W i

k−1

.

(d) delete qi.
(e) Set X i

1:k ←
6. Compute (cid:98)Zπn = 1
N

(X i

1:k−1, X i
k)
(cid:80)N

i=1 W i
n.

some observed values y

We assume that the data likelihood p(y
an integral

|

¯π(θ

y)

p(y

θ)p(θ).

∝

|
θ) can, by introducing a latent variable x, be computed as

|

Now, let our target distribution in Nested SIS be ¯π2(θ, x) = ¯π2(x
We set our proposal distributions to be

|

θ)¯π1(θ) = p(y | x,θ)p(x | θ)

p(y | θ)

p(θ).

(cid:90)

p(y

θ) =

p(y

x, θ)p(x

θ) dx.

|

|

|

¯q1(θ) = gIS(θ),

¯q2(x

θ) =

|

p(y

|

x, θ)p(x
θ)
p(y

|

θ)

.

|
First, Q(q1(
), 1) runs an exact sampler from the proposal gIS. Then at iteration k = 2 we let the
·
nested procedure Q(q2(
y, θ), giving us
properly weighted samples for q2. Putting all this together gives us samples θi distributed according
to gIS(θ) and weighted by

θi), M ) be a standard IS algorithm with proposal h(x

· |

|

W i

2 ∝

p(θi)
gIS(θi) ·

p(y

xi, θi)p(xi

|

(cid:80)M

θi) 1
(cid:96)=1
M
xi, θi)p(xi

|
p(y

|

θi)

|

p(y | x(cid:96),θi)p(x(cid:96) | θi)
h(x(cid:96) | y,θi)

= (cid:98)pM (y

θi)p(θi)

|
gIS(θi)

,

(17)

24

NESTED SEQUENTIAL MONTE CARLO METHODS

θi) = M −1 (cid:80)M
where (cid:98)pM (y
(cid:96)=1
identical to the IS2 algorithm proposed by Tran et al. (2013).

p(y | x(cid:96),θi)p(x(cid:96) | θi)
h(x(cid:96) | y,θi)

|

. Thus we obtain a Nested SIS method that is

A.3 Further Details on the Experiments

We provide some further details and results for the experiments presented in the main manuscript.

A.3.1 GAUSSIAN STATE SPACE MODEL

We generate data from a synthetic d-dimensional (dim(xk) = d) dynamical/spatio-temporal3 model
deﬁned by

xk

xk−1

|
yk

∼ N

∼ N

xk

|

(xk; µk(xk−1), Σ),
(yk; xk, τ −1

φ I),

where Σ and µk are given as follows

Σ =

τρ + τψ
τψ













−
0
...
...
0
0

τψ
−
τρ + 2τψ
. . .
. . .
...
0
0

0
τψ
−
. . .
. . .
. . .
0
0

· · ·
0
. . .
. . .
. . .
0
0

0
0

0

· · ·
· · ·
. . .
. . .
. . .
−
τψ τρ + 2τψ
−
0

τψ

τψ

0

−

0
0

0

0

−1













,

0
τψ
−
τρ + τψ

µk(xk−1) = aτρΣxk−1.

Alternatively, in a more standard state space model notation, we have

xk = Axk−1 + vk, vk
yk = xk + ek, ek

∼ N
(0, R),

(0, Q),

∼ N

where A = aτρΣ, Q = Σ and R = τ −1
(1, 0.5, 1, 10) are known.

φ I. We assume that the parameters θ = (τψ, a, τρ, τφ) =

To do inference with this generated data-set

we propose to target the following slightly

yk

{

}

different model

p(x1:k, y1:k)

φ(xj, yj)ρ(xj)ψ(xj, xj−1),

k
(cid:89)

j=1

∝

3. Note that in a previous version this was erraneously stated as equivivalent to the Gaussian MRF we use for sequential
inference. Thus this example actually illustrates a problem where we have a misspeciﬁed model. However, this
misspeciﬁcation does not lead to any discernible difference in the MSE results. This because the exact ﬁltering
marginals for the two different models (LGSS, GMRF) with the parameters chosen differs with orders of magnitudes
much lower than the Monte Carlo errors.

25

NAESSETH, LINDSTEN AND SCH ¨ON

where the observation potential φ and interaction potentials ρ and ψ are given by

φ(xk, yk) =

φl(xk,l, yk,l) =

e−

τφ
2 (xk,l−yk,l)2

,

ψ(xk) =

ψl(xk,l, xk,l−1) =

τψ
2 (xk,l−xk,l−1)2

e−

ρ(xk, xk−1) =

ρl(xk,l, xk−1,l) =

τρ
2 (xk,l−axk−1,l)2

e−

,

.

d
(cid:89)

l=1

d
(cid:89)

l=2
d
(cid:89)

l=1

d
(cid:89)

l=1
d
(cid:89)

l=2
d
(cid:89)

l=1

k) lattice MRF, i.e. it grows with “time” k. The
This can be visualised as a Gaussian rectangular (d
×
goal is to estimate the ﬁltering distribution p(xk
y1:k). Note that this model has almost identical
|
ﬁltering marginals as the data generating distribution and leads to a simpler implementation of
NSMC and ST-PF.

Results (mean-squared-error, MSE) comparing NSMC and ST-PF for different settings of N
and M can be found in the ﬁrst row of Figure 6 and the second row displays the results when
comparing ST-PF to the SMC method by Naesseth et al. (2014b) for equal computational budgets.
We show median (over dimensions d) MSE for posterior marginal mean and variance estimates
of the respective algorithms. True values are obtained using belief propagation. Note that setting
N = 1 in ST-PF can be viewed as a special case of the SMC method by Naesseth et al. (2014b).

A.3.2 SPATIO-TEMPORAL MODEL – DROUGHT DETECTION

We present the full model for drought detection in our notation, this is essentially the model by Fu
et al. (2012) adapted for estimating the ﬁltering distribution. The latent variables for each location
on a ﬁnite world grid, xk,i,j, are binary, i.e. 0 being normal state and 1 being the abnormal (drought)
state. Measurements, yk,i,j, are available as real valued precipitation values in millimeters. The
probabilistic model for ﬁltering is given as,

k
(cid:89)

∝

n=1

(cid:40)

1
2σ2
i,j

−

p(x1:k, y1:k)

φ(xn, yn)ρ(xn)ψ(xn, xn−1),

(18a)

where

φ(xk, yk) =

exp

(yk,i,j

µab,i,jxk,i,j

µnorm,i,j(1

−

−

−

(cid:41)

xk,i,j))2

,

(18b)

ρ(xk) =

exp (cid:8)C1

(cid:0)1xk,i,j =xk,i,j−1 + 1xk,i,j =xk,i−1,j

(cid:1)(cid:9) ,

ψ(xk, xk−1) =

exp (cid:8)C21xk,i,j =xk−1,i,j

(cid:9) .

(18c)

(18d)

I
(cid:89)

J
(cid:89)

i=1

j=1

I
(cid:89)

J
(cid:89)

i=1

j=1

I
(cid:89)

J
(cid:89)

i=1

j=1

Here, 1 is the indicator function, and with the convention that all expressions in (18c) that end up
with index 0 evalute to 0. The parameters C1, C2 are set to 0.5, 3 as in (Fu et al., 2012). Location

26

NESTED SEQUENTIAL MONTE CARLO METHODS

Median MSE for E[xk,(cid:96)]

Median MSE for Var(xk,(cid:96))

Median MSE for E[xk,(cid:96)]

Median MSE for Var(xk,(cid:96))

Figure 6: Top: Comparisons for different settings of N and M on the 50-dimensional SSM. Bot-
tom: Illustrating the connection between ST-PF and the SMC method by Naesseth et al.
(2014b).

44 region with latitude 6

55◦N and longitude 90

2012. For the North America region we consider a 20

based parameters σi,j, µab,i,j, µnorm,i,j are estimated based on data from the CRU dataset with world
precipitation data from years 1901
30
120◦W . For the Sahel region we consider a
region with latitude 35
35◦E. Note that for a few locations in
24
Africa (Sahel region) the average yearly precipitation was constant. For these locations we simply
set µnorm,i,j to be this value, µab,i,j = 0 and σ2
i,j to be the mean variance of all locations, thus this
might have introduced some artifacts. Some representative results for the Sahel region are displayed
in Figure 7.

30◦N and longitude 10◦W

−

−

−

×

−

×

−

References

C. Andrieu and G. O. Roberts. The pseudo-marginal approach for efﬁcient Monte Carlo computa-

tions. The Annals of Statistics, 37(2):697–725, 2009.

Christophe Andrieu, Arnaud Doucet, and Roman Holenstein. Particle Markov chain Monte Carlo
methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):
269–342, 2010.

27

NAESSETH, LINDSTEN AND SCH ¨ON

Sahel region 1986

Sahel region 1987

Sahel region 1988

Figure 7: Estimate of P(Xk,i,j = 1

y1:k) for all sites over a span of 3 years. All results for N =

100, N1 =

30, 40
}

{

|
, N2 = 20.

A. Beskos, D. Crisan, A. Jasra, K. Kamatani, and Y. Zhou. A stable particle ﬁlter in high-

dimensions. ArXiv:1412.3501, December 2014a.

Alexandros Beskos, Dan Crisan, and Ajay Jasra. On the stability of sequential Monte Carlo methods

in high dimensions. Ann. Appl. Probab., 24(4):1396–1445, 08 2014b.

Peter Bickel, Bo Li, and Thomas Bengtsson. Sharp failure rates for the bootstrap particle ﬁlter
in high dimensions, volume Volume 3 of Collections, pages 318–329. Institute of Mathematical
Statistics, Beachwood, Ohio, USA, 2008.

Jonathan Briggs, Michael Dowd, and Renate Meyer. Data assimilation for large-scale spatio-

temporal systems using a location particle smoother. Environmetrics, 24(2):81–97, 2013.

Olivier Capp´e, Eric Moulines, and Tobias Ryd´en. Inference in Hidden Markov Models. Springer-

Verlag New York, Inc., Secaucus, NJ, USA, 2005. ISBN 0387402640.

J. Carpenter, P. Clifford, and P. Fearnhead. Improved particle ﬁlter for nonlinear problems. IEE

Proceedings Radar, Sonar and Navigation, 146(1):2–7, 1999.

Tianshi Chen, Thomas B. Sch¨on, Henrik Ohlsson, and Lennart Ljung. Decentralized particle ﬁlter
with arbitrary state decomposition. IEEE Transactions on Signal Processing, 59(2):465–478, Feb
2011.

N. Chopin. Central limit theorem for sequential Monte Carlo methods and its application to

Bayesian inference. The Annals of Statistics, 32(6):2385–2411, 2004.

N. Chopin, P. E. Jacob, and O. Papaspiliopoulos. SMC2: an efﬁcient algorithm for sequential
analysis of state space models. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 75(3):397–426, 2013.

Jacques Cohen. Bioinformaticsan introduction for computer scientists. ACM Computing Surveys

(CSUR), 36(2):122–158, 2004.

N. Cressie and C. K. Wikle. Statistics for spatio-temporal data. Wiley, 2011.

28

NESTED SEQUENTIAL MONTE CARLO METHODS

D. Crisan and J. M´ıguez. Nested particle ﬁlters for online parameter estimation in discrete-time

state-space Markov models. ArXiv:1308.1883, August 2013.

P. Del Moral. Feynman-Kac Formulae - Genealogical and Interacting Particle Systems with Appli-

cations. Probability and its Applications. Springer, 2004.

Petar M Djuric and M´onica F Bugallo. Particle ﬁltering for high-dimensional systems. In Compu-
tational Advances in Multi-Sensor Adaptive Processing (CAMSAP), 2013 IEEE 5th International
Workshop on, pages 352–355. IEEE, 2013.

R. Douc and E. Moulines. Limit theorems for weighted samples with applications to sequential

Monte Carlo. The Annals of Statistics, 36(5):2344–2376, 2008.

R. Douc, E. Moulines, and J. Olsson. Optimality of the auxiliary particle ﬁlter. Probability and

Mathematical Statistics, 29:1–28, 2009.

A. Doucet and A. M. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years later.
In D. Crisan and B. Rozovsky, editors, Nonlinear Filtering Handbook. Oxford University Press,
2011.

Arnaud Doucet, Nando De Freitas, and Neil Gordon. An introduction to sequential Monte Carlo

methods. Springer, 2001.

Paul Fearnhead, Omiros Papaspiliopoulos, Gareth O. Roberts, and Andrew Stuart. Random-weight
particle ﬁltering of continuous time processes. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 72(4):497–512, 2010a.

Paul Fearnhead, David Wyncoll, and Jonathan Tawn. A sequential smoothing algorithm with linear

computational cost. Biometrika, 97(2):447–464, 2010b.

J. A. Foley, M. T. Coe, M. Scheffer, and G. Wang. Regime shifts in the sahara and sahel: Interactions

between ecological and climatic systems in northern africa. Ecosystems, 6:524–539, 2003.

Qiang Fu, Arindam Banerjee, Stefan Liess, and Peter K. Snyder. Drought detection of the last
century: An MRF-based approach. In Proceedings of the 2012 SIAM International Conference
on Data Mining, pages 24–34, Anaheim, CA, USA, April 2012.

S. J. Godsill, A. Doucet, and M. West. Monte Carlo smoothing for nonlinear time series. Journal

of the American Statistical Association, 99(465):156–168, March 2004.

N. J. Gordon, D. J. Salmond, and A. F. M. Smith. Novel approach to nonlinear/non-Gaussian
Bayesian state estimation. Radar and Signal Processing, IEE Proceedings F, 140(2):107 –113,
April 1993.

M. Hoerling, J. Hurrell, J. Eischeid, and A. Phillips. Detection and attribution of twentieth-century

northern and southern african rainfall change. Journal of Climate, 19:3989–4008, 2006.

A. M. Johansen and A. Doucet. A note on auxiliary particle ﬁlters. Statistics & Probability Letters,

78(12):1498–1504, 2008.

29

NAESSETH, LINDSTEN AND SCH ¨ON

A. M. Johansen, N. Whiteley, and A. Doucet. Exact approximation of Rao-Blackwellised particle
ﬁlters. In Proceesings of the 16th IFAC Symposium on System Identiﬁcation (SYSID), pages 488–
493, Brussels, Belgium, 2012.

P.D. Jones and I. Harris. CRU TS3.21: Climatic research unit (CRU) time-series (ts) version 3.21
of high resolution gridded data of month-by-month variation in climate (jan. 1901- dec. 2012).
NCAS British Atmospheric Data Centre, sep 2013. URL http://dx.doi.org/10.5285/
D0E1585D-3417-485F-87AE-4FCECF10A992.

R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Transactions of the

ASME, Journal of Basic Engineering, 82:35–45, 1960.

A. Kong, J. S. Liu, and W. H. Wong. Sequential imputations and Bayesian missing data problems.

Journal of the American Statistical Association, 89(425):278–288, 1994.

F. Lindsten and T. B. Sch¨on. Backward simulation methods for Monte Carlo statistical inference.

Foundations and Trends in Machine Learning, 6(1):1–143, 2013.

F. Lindsten, M. I. Jordan, and T. B. Sch¨on. Particle Gibbs with ancestor sampling. Journal of

Machine Learning Research, 15:2145–2184, 2014.

Jun S Liu. Monte Carlo strategies in scientiﬁc computing. Springer Science & Business Media,

2001.

Claire Monteleoni, Gavin A. Schmidt, Francis Alexander, Alexandru Niculescu-Mizil, Karsten
Steinhaeuser, Michael Tippett, Arindam Banerjee, M. Benno Blumenthal, Jason E. Smerdon Au-
In Ting Yu, Nitesh Chawla, and
roop R. Ganguly, and Marco Tedesco. Climate informatics.
Simeon Simoff, editors, Computational Intelligent Data Analysis for Sustainable Development.
Chapman and Hall/CRC, London, 2013.

Christian A. Naesseth, Fredrik Lindsten, and Thomas B. Sch¨on. Capacity estimation of two-
In The 2014 IEEE Information Theory

dimensional channels using sequential Monte Carlo.
Workshop (ITW), pages 431–435, Nov 2014a.

Christian A. Naesseth, Fredrik Lindsten, and Thomas B Sch¨on. Sequential Monte Carlo for graphi-
cal models. In Advances in Neural Information Processing Systems 27, pages 1862–1870. Curran
Associates, Inc., 2014b.

Michael K Pitt and Neil Shephard. Filtering via simulation: Auxiliary particle ﬁlters. Journal of the

American statistical association, 94(446):590–599, 1999.

P. Rebeschini and R. van Handel. Can local particle ﬁlters beat the curse of dimensionality? Ann.

Appl. Probab. (to appear), 2015.

Raton, FL, USA, 2005.

H. Rue and L. Held. Gaussian Markov Random Fields, Theory and Applications. CDC Press, Boca

S. D. Schubert, M. J. Suarez, P. J. Pegion, R. D. Koster, and J. T. Bacmeister. On the cause of the

1930s dust bowl. Science, 303:1855–1859, 2004.

30

NESTED SEQUENTIAL MONTE CARLO METHODS

R. H. Shumway and D. S. Stoffer. Time Series Analysis and Its Applications – with R examples.

Springer Texts in Statistics. Springer, New York, USA, third edition, 2011.

M.-N. Tran, M. Scharth, M. K. Pitt, and R. Kohn.

Importance sampling squared for Bayesian

inference in latent variable models. ArXiv:1309.3339, sep 2013.

Christelle Verg´e, Cyrille Dubarry, Pierre Del Moral, and Eric Moulines. On parallel implementation
of sequential Monte Carlo methods: the island particle model. Statistics and Computing, 25(2):
243–260, 2015.

Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational

inference. Foundations and Trends R
(cid:13)

in Machine Learning, 1(1-2):1–305, 2008.

N. Whiteley. Discussion on Particle Markov chain Monte Carlo methods. Journal of the Royal

Statistical Society: Series B, 72(3):306–307, 2010.

C. K. Wikle. Modern perspectives on statistics for spatio-temporal data. WIREs Computational

Statistics, 7(1):86–98, 2015.

31

