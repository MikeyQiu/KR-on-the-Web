Neural Baby Talk

Jiasen Lu1∗

Jianwei Yang1∗ Dhruv Batra1,2 Devi Parikh1,2

1Georgia Institute of Technology

2Facebook AI Research

{jiasenlu, jw2yang, dbatra, parikh}@gatech.edu

8
1
0
2
 
r
a

M
 
7
2
 
 
]

V
C
.
s
c
[
 
 
1
v
5
4
8
9
0
.
3
0
8
1
:
v
i
X
r
a

Abstract

We introduce a novel framework for image captioning
that can produce natural language explicitly grounded in
entities that object detectors ﬁnd in the image. Our ap-
proach reconciles classical slot ﬁlling approaches (that are
generally better grounded in images) with modern neu-
ral captioning approaches (that are generally more natu-
ral sounding and accurate). Our approach ﬁrst generates
a sentence ‘template’ with slot locations explicitly tied to
speciﬁc image regions. These slots are then ﬁlled in by
visual concepts identiﬁed in the regions by object detec-
tors. The entire architecture (sentence template generation
and slot ﬁlling with object detectors) is end-to-end differen-
tiable. We verify the effectiveness of our proposed model
on different image captioning tasks. On standard image
captioning and novel object captioning, our model reaches
state-of-the-art on both COCO and Flickr30k datasets.
We also demonstrate that our model has unique advan-
tages when the train and test distributions of scene com-
positions – and hence language priors of associated cap-
tions – are different. Code has been made available at:
https://github.com/jiasenlu/NeuralBabyTalk.

1. Introduction

Image captioning is a challenging problem that lies at the
intersection of computer vision and natural language pro-
cessing. It involves generating a natural language sentence
that accurately summarizes the contents of an image. Im-
age captioning is also an important ﬁrst step towards real-
world applications with signiﬁcant practical impact, rang-
ing from aiding visually impaired users to personal assis-
tants to human-robot interaction [5, 9].

State-of-art image captioning models today tend to be
monolithic neural models, essentially of the “encoder-
decoder” paradigm. Images are encoded into a vector with
a convolutional neural network (CNN), and captions are de-
coded from this vector using a Recurrent Neural Network
(RNN), with the entire system trained end-to-end. While

Figure 1. Example captions generated by (a) Baby Talk [24], (c)
neural image captioning [20] and (b) our Neural Baby Talk ap-
proach. Our method generates the sentence “template” with slot
locations (illustrated with ﬁlled boxes) explicitly tied to image re-
gions (drawn in the image in corresponding colors). These slots
are then ﬁlled by object detectors with concepts found in regions.

there are many recent extensions of this basic idea to in-
clude attention [46, 12, 50, 48, 27], it is well-understood
that models still lack visual grounding (i.e., do not associate
named concepts to pixels in the image). They often tend to
‘look’ at different regions than humans would and tend to
copy captions from training data [8].

For instance, in Fig. 1 a neural image captioning ap-
proach [20] describes the image as “A dog is sitting on a
couch with a toy.” This is not quite accurate. But if one
were to really squint at the image, it (arguably) does per-
haps look like a scene where a dog could be sitting on a
couch with a toy. It certainly is common to ﬁnd dogs sitting
on couches with toys. A-priori, the description is reason-
able. That’s exactly what today’s neural captioning models
tend to do – produce generic plausible captions based on the
language model1 that match a ﬁrst-glance gist of the scene.
While this may sufﬁce for common scenes, images that dif-
fer from canonical scenes – given the diversity in our visual
world, there are plenty of such images – tend to be under-
served by these models.

If we take a step back – do we really need the language
model to do the heavy lifting in image captioning? Given

∗Equal contribution

1frequently, directly reproduced from a caption in the training data.

1

Figure 2. From left to right is the generated caption using the same captioning model but with different detectors: 1) No detector; 2) A
weak detector that only detects “person” and “sandwich”; 3) A detector trained on COCO [26] categories (including “teddy bear”). 4) A
detector that can detect novel concepts (e.g. “Mr. Ted” and “pie” that never occurred in the captioning training data). Different colors show
a correspondence between the visual word and grounding regions.

the unprecedented progress we are seeing in object recog-
nition2 (e.g., object detection, semantic segmentation, in-
stance segmentation, pose estimation), it seems like the vi-
sion pipeline can certainly do better than rely on just a ﬁrst-
glance gist of the scene. In fact, today’s state-of-the-art ob-
ject detectors can successfully detect the table and cake in
the image in Fig. 1(c)! The caption ought to be able to talk
about the table and cake actually detected as opposed to
letting the language model hallucinate a couch and a toy
simply because that sounds plausible.

Interestingly, some of the ﬁrst attempts at image caption-
ing [13, 24, 25, 34] – before the deep learning “revolution”
– relied heavily on outputs of object detectors and attribute
classiﬁers to describe images. For instance, consider the
output of Baby Talk [24] in Fig. 1, that used a slot ﬁlling
approach to talk about all the objects and attributes found in
the scene via a templated caption. The language is unnatu-
ral but the caption is very much grounded in what the model
sees in the image. Today’s approaches fall at the other ex-
treme on the spectrum – the language generated by modern
neural image captioning approaches is much more natural
but tends to be much less grounded in the image.

In this paper, we introduce Neural Baby Talk that recon-
ciles these methodologies. It produces natural language ex-
plicitly grounded in entities found by object detectors. It is
a neural approach that generates a sentence “template” with
slot locations explicitly tied to image regions. These slots
are then ﬁlled by object recognizers with concepts found in
the regions. The entire approach is trained end-to-end. This
results in natural sounding and grounded captions.

Our main technical contribution is a novel neural de-
coder for grounded image captioning. Speciﬁcally, at each
time step, the model decides whether to generate a word
from the textual vocabulary or generate a “visual” word.
The visual word is essentially a token that will hold the slot
for a word that is to describe a speciﬁc region in the image.
For instance, for the image in Fig. 1, the generated sequence

may be “A <region−17> is sitting at a <region−123>
with a <region−3>.” The visual words (<region−[.]>’s)
are then ﬁlled in during a second stage that classiﬁes
each of the indicated regions (e.g., <region−17>→puppy,
<region−123>→table), resulting in a ﬁnal description of
“A puppy is sitting at a table with a cake.” – a free-form
natural language description that is grounded in the image.
One nice feature of our model is that it allows for different
object detectors to be plugged in easily. As a result, a va-
riety of captions can be produced for the same image using
different detection backends. See Fig. 2 for an illustration.
Contributions: Our contributions are as follows:

• We present Neural Baby Talk – a novel framework for
visually grounded image captioning that explicitly lo-
calizes objects in the image while generating free-form
natural language descriptions.

• Ours is a two-stage approach that ﬁrst generates a hy-
brid template that contains a mix of (text) words and
slots explicitly associated with image regions, and then
ﬁlls in the slots with (text) words by recognizing the
content in the corresponding image regions.

• We propose a robust image captioning task to bench-
mark compositionality of image captioning algorithms
where at test time the model encounters images con-
taining known objects but in novel combinations (e.g.,
the model has seen dogs on couches and people at ta-
bles during training, but at test time encounters a dog
at a table). Generalizing to such novel compositions is
one way to demonstrate image grounding as opposed
to simply leveraging correlations from training data.
• Our proposed method achieves state-of-the-art perfor-
mance on COCO and Flickr30k datasets on the stan-
dard image captioning task, and signiﬁcantly outper-
forms existing approaches on the robust image cap-
tioning and novel object captioning tasks.

2. Related Work

2e.g., 11% absolute increase in average precision in object detection in

the COCO challenge in the last year.

Some of the earlier approaches generated templated im-
age captions via slot-ﬁlling. For instance, Kulkarni et

al. [24] detect objects, attributes, and prepositions, jointly
reason about these through a CRF, and ﬁnally ﬁll appropri-
ate slots in a template. Farhadi et al. [13] compute a triplet
for a scene, and use this templated “meaning” representa-
tion to retrieve a caption from a database. [25, 34] use more
powerful language templates such as a syntactically well-
formed tree. These approaches tend to either produce cap-
tions that are relevant to the image but not natural sounding,
or captions that are natural (e.g. retrieved from a database of
captions) but may not be sufﬁciently grounded in the image.
Neural models for image captioning have been receiv-
ing increased attention in the last few years [23, 32, 7, 45,
11, 20]. State-of-the-art neural approaches include atten-
tion mechanisms [46, 12, 50, 48, 27, 39, 3] that identify re-
gions in the image to “ground” emitted words. In practice,
these attention regions tend to be quite blurry, and rarely
correspond to semantically meaningful individual entities
(e.g., objects instances) in the image. Our approach grounds
words in object detections, which by design identify con-
crete semantic entities (object instances) in the image.

There has been some recent interest in grounding natu-
ral language in images. Dense Captioning [19] generates
descriptions for speciﬁc image regions.
In contrast, our
model produces captions for the entire image, with words
grounded in concrete entities in the image. Another related
line of work is on resolving referring expressions [21] (or
description-based object retrieval [37, 17, 18, 40] – given a
description of an object in the image, identify which ob-
ject is being referred to) or referring expression genera-
tion [21, 29, 31, 51] (given an object in the image, generate a
discriminative description of the object). While the interest
in grounded language is in common, our task is different.

One natural strength of our model is its ability to in-
corporate different object detectors, including the ability to
generate captions with novel objects (never seen before in
training captions). In that context, our work is related to
prior works on novel object captioning [4, 43, 49, 2]. As
we describe in Sec. 4.3, our method outperforms these ap-
proaches by 14.6% on the averaged F1 score.

3. Method

Given an image I, the goal of our method is to gener-
ate visually grounded descriptions y = {y1, . . . , yT }. Let
rI = {r1, ..., rN } be the set of N images regions extracted
from I. When generating an entity word in the caption, we
want to ground it in a speciﬁc image region r ∈ rI . Fol-
lowing the standard supervised learning paradigm, we learn
parameters θ of our model by maximizing the likelihood of
the correct caption:

θ∗ = arg max

θ

(cid:88)

log p(y|I; θ)

(1)

(I,y)
Using chain rule, the joint probability distribution can be

decomposed over a sequence of tokens:

p(y|I) =

p(yt|y1:t−1, I)

(2)

T
(cid:89)

t=1

where we drop the dependency on model parameters to
avoid notational clutter. We introduce a latent variable rt
to denote a speciﬁc image region so that yt can explicitly
ground in it. Thus the probability of yt is decomposed to:

p(yt|y1:t−1, I) = p(yt|rt, y1:t−1, I)p(rt|y1:t−1, I)

(3)

In our framework, yt can be of one of two types: a vi-
sual word or a textual word, denoted as yvis and ytxt re-
spectively. A visual word yvis is a type of word that is
grounded in a speciﬁc image region drawn from rI . A tex-
tual word ytxt is a word from the remainder of the caption.
It is drawn from the language model , which is associated
with a “default” sentinel “region” ˜r obtained from the lan-
guage model [27] (discussed in Sec. 3.1). For example, as
illustrated in Fig. 1, “puppy” and “cake” grounded in the
bounding box of category “dog” and “cake” respectively,
are visual words. While “with” and “sitting” are not associ-
ated with any image regions and thus are textual words.

With this, Eq. 1 can be decomposed into two cascaded
objectives. First, maximizing the probability of generating
the sentence “template”. A sequence of grounding regions
associated with the visual words interspersed with the tex-
tual words can be viewed as a sentence “template”, where
the grounding regions are slots to be ﬁlled in with visual
words.3 An example template (Fig. 3) is “A <region−2>
is laying on the <region−4> near a <region−7>. Sec-
ond, maximizing the probability of visual words yvis
con-
ditioned on the grounding regions and object detection in-
formation, e.g., categories recognized by detector. In the
template example above, the model will ﬁll the slots with
‘cat’, ‘laptop’ and ‘chair’ respectively.

t

In the following, we ﬁrst describe how we generate the
slotted caption template (Sec. 3.1), and then how the slots
are ﬁlled in to obtain the ﬁnal image description (Sec. 3.2).
The overall objective function is described in Sec. 3.3 and
the implementation details in Sec. 3.4.

3.1. “Slotted” Caption Template Generation

Given an image I, and the corresponding caption y, the
candidate grounding regions are obtained by using a pre-
trained Faster-RCNN network [38]. To generate the cap-
tion “template”, we use a recurrent neural network, which
is commonly used as the decoder for image captioning
[32, 45]. At each time step, we compute the RNN hidden
state ht according to the previous hidden state ht−1 and the
input xt such that ht = RNN(xt, ht−1). At training time,

3Our approach is not limited to any pre-speciﬁed bank of templates.
Rather, our approach automatically generates a template (with placehold-
ers – slots – for visually grounded words), which may be any one of the
exponentially many possible templates.

P t

r = softmax([ut; wT

h tanh(Wsst + Wzht)])

(9)
where Ws ∈ Rd×d and Wz ∈ Rd×d are the parameters.
Notably, Wz and wh are the same parameters as in Eq. 4.
P t
r is the probability distribution over grounding regions rI
and visual sentinel ˜r. The last element of the vector in Eq. 9
captures p(˜r|y1:t−1).

We feed the hidden state ht into a softmax layer to obtain
the probability over textual words conditioned on the image,
all previous words, and the visual sentinel:

P t

txt = softmax (Wqht)
(10)
where Wq ∈ RV ×d, d is hidden state size, and V is textual
vocabulary size. Plugging in Eq. 10 and p(˜r|y1:t−1) from
the last element of the vector in Eq. 9 into Eq. 6 gives us the
probability of generating a textual word in the template.

3.2. Caption Reﬁnement: Filling in The Slots

To ﬁll the slots in the generated template with visual
words grounded in image regions, we leverage the outputs
of an object detection network. Given a grounding region,
the category can be obtained through any detection frame-
work [38]. But outputs of detection networks are typically
singular coarse labels e.g. “dog”. Captions often refer to
these entities in a ﬁne-grained fashion e.g. “puppy” or in
the plural form “dogs”. In order to accommodate for these
linguistic variations, the visual word yvis in our model is
a reﬁnement of the category name by considering the fol-
lowing two factors: First, determine the plurality – whether
it should be singular or plural. Second, determine the ﬁne-
grained class (if any). Using two single layer MLPs with
ReLU activation f (·), we compute them with:

P t
b = softmax (Wbfb ([vt; ht]))
g = softmax (cid:0)U T Wgfg ([vt; ht])(cid:1)
P t

(11)

(12)

Wb ∈ R2×d, Wg ∈ R300×d are the weight parameters.
U ∈ R300×k is the glove vector embeddings [36] for k
ﬁne-grained words associated with the category name. The
visual word yvis
is then determined by plurality and ﬁne-
grained class (e.g., if plurality is plural, and the ﬁne-grained
class is “puppy”, the visual word will be “puppies”).

t

3.3. Objective

Most standard image captioning datasets (e.g. COCO
[26]) do not contain phrase grounding annotations, while
some datasets do (e.g. Flickr30k [37]). Our training objec-
tive (presented next) can incorporate different kinds of su-
pervision – be it strong annotations indicating which words
in the caption are grounded in which boxes in the image, or
weak supervision where objects are annotated in the image
but are not aligned to words in the caption. Given the tar-
get ground truth caption y∗
1:T and a image captioning model

Figure 3. One block of the proposed approach. Given an image,
proposals from any object detector and current word “A”, the ﬁgure
shows the process to predict the next visual word “cat”.

xt is the ground truth token (teacher forcing) and at test time
is the sampled token yt−1. Our decoder consists of an atten-
tion based LSTM layer [39] that takes convolution feature
maps as input. Details can be found in Sec. 3.4. To generate
the “slot” for visual words, we use a pointer network [44]
that modulates a content-based attention mechanism over
the grounding regions. Let vt ∈ Rd×1 be the region fea-
ture of rt, which is calculated based on Faster R-CNN. We
compute the pointing vector with:

h tanh(Wvvt + Wzht)

i = wT
ut
P t
= softmax(ut)
rI

(4)

(5)

Since textual words ytxt

where Wv ∈ Rm×d, Wz ∈ Rd×d and wh ∈ Rd×1 are pa-
rameters to be learned. The softmax normalizes the vector
ut to be a distribution over grounding regions rI .
t

are not tied to speciﬁc regions
in the image, inspired by [27], we add a “visual sentinel”
˜r as a latent variable to serve as dummy grounding for the
textual word. The visual sentinel can be thought of as a la-
tent representation of what the decoder already knows about
the image. The probability of a textual word ytxt

then is:

t

p(ytxt
t

|y1:t−1) = p(ytxt

t

|˜r, y1:t−1)p(˜r|y1:t−1)

(6)

where we drop the dependency on I to avoid clutter.

We ﬁrst describe how the visual sentinel is computed,
and then how the textual words are determined based on the
visual sentinel. Following [27], when the decoder RNN is
an LSTM [16], the representation for visual sentinel st can
be obtained by:

gt = σ (Wxxt + Whht−1)
st = gt (cid:12) tanh (ct)

(7)

(8)

where Wx ∈ Rd×d, Wh ∈ Rd×d. xt is the LSTM input
at time step t, and gt is the gate applied on the cell state
ct. (cid:12) represents element-wise product, σ the logistic sig-
moid activation. Modifying Eq. 5, the probability over the
grounding regions including the visual sentinel is:

with parameters θ, we minimize the cross entropy loss:

L(θ) = −

T
(cid:88)

t=1

(cid:16)

(cid:122)
p(y∗

t |˜r, y∗

log

Textual word probability
(cid:125)(cid:124)
1:t−1)p(˜r|y∗

1:t−1)1(y∗

(cid:123)
t =ytxt) +

p (cid:0)b∗
(cid:124)

t , s∗

t |rt, y∗
(cid:123)(cid:122)
Caption reﬁnement

1:t−1

(cid:1)

(cid:125)

m
(cid:88)

i=1

(cid:0) 1
m
(cid:124)

p (cid:0)ri

t|y∗

1:t−1

(cid:1) (cid:1)1(y∗

t =yvis)

(cid:17)

(cid:123)(cid:122)
Averaged target region probability

(cid:125)

(13)
where y∗
t is the word from the ground truth caption at time
t. 1(y∗
t =ytxt) is the indicator function which equals to 1 if
y∗
t is textual word and 0 otherwise. b∗
t are the target
ground truth plurality and ﬁnd-grained class. {ri
i=1 ∈ rI
are the target grounding regions of the visual word at time
t. We maximize the averaged log probability of the target
grounding regions.

t and s∗

t}m

t}m

Visual word extraction. During training, visual words
in a caption are dynamically identiﬁed by matching the base
form of each word (using the Stanford lemmatization tool-
box [30]) against a vocabulary of visual words (details of
how to get visual word can be found in dataset Sec. 4). The
grounding regions {ri
i=1 for a visual word yt is identiﬁed
by computing the IoU of all boxes detected by the object
detection network with the ground truth bounding box as-
sociated with the category corresponding to yt. If the score
exceeds a threshold of 0.5 and the grounding region label
matches the visual word, the bounding boxes are selected
as the grounding regions. E.g., given a target visual word
“cat”, if there are no proposals that match the target bound-
ing box, the model predicts the textual word “cat” instead.

3.4. Implementation Details

Detection model. We use Faster R-CNN [38] with
ResNet-101 [15] to obtain region proposals for the image.
We use an IoU threshold of 0.7 for region proposal sup-
pression and 0.3 for class suppressions. A class detection
conﬁdence threshold of 0.5 is used to select regions.

i ; vl

i; vg

Region feature. We use a pre-trained ResNet-101 [15]
in our model. The image is ﬁrst resized to 576×576 and we
random crop 512 × 512 as the input to the CNN network.
Given proposals from the pre-trained detection model, the
feature vi for region i is a concatenation of 3 different fea-
tures vi = [vp
i ] where vp
i is the pooling feature of
RoI align layer [14] given the proposal coordinates, vl
i is
the location feature and vg
i is the glove vector embedding of
the class label for region i. Let xmin, ymin, xmax, ymax be the
bounding box coordinates of the region b; WI and HI be the
width and height of the image I. Then the location feature
vl
i can be obtained by projecting the normalized location
xmin
[
WI

] into another embedding space.

xmax
WI

ymax
HI

ymin
HI

Language model. We use an attention model with two
LSTM layers [3] as our base attention model. Given N re-

,

,

,

Figure 4. Language model used in our approach.

gion features from detection proposals V = {v1, . . . , vN }
and CNN features from the last convolution layer at K grids
ˆV = {ˆv1, . . . , ˆvK}, the language model has two separate
attention layers shown in Fig 4. The attention distribution
over the image features for detection proposals is:
z tanh (cid:0)WvV + (Wght)1T (cid:1)

(14)

zt = wT
αt = softmax(zt)

where Wv ∈ Rm×d, Wg ∈ Rd×d and w ∈ Rd×1. 1 ∈ RN
is a vector with all elements set to 1. αt is the attention
weight over N image location features.

Training details. In our experiments, we use a two layer
LSTM with hidden size 1024. The number of hidden units
in the attention layer and the size of the input word em-
bedding are 512. We use the Adam [22] optimizer with an
initial learning rate of 5 × 10−4 and anneal the learning rate
by a factor of 0.8 every three epochs. We train the model
up to 50 epochs with early stopping. Note that we do not
ﬁnetune the CNN network during training. We set the batch
size to be 100 for COCO [26] and 50 for Flickr30k [37].

4. Experimental Results

Datasets. We experiment with two datasets. Flickr30k
Entities [37] contains 275,755 bounding boxes from 31,783
images associated with natural language phrases. Each im-
age is annotated with 5 crowdsourced captions. For each
annotated phrase in the caption, we identify visual words
by selecting the inner most NP (noun phrase) tag from
the Stanford part-of-speech tagger [6]. We use Stanford
Lemmatization Toolbox [30] to get the base form of the en-
tity words resulting in 2,567 unique words.

COCO [26] contains 82,783, 40,504 and 40,775 images
for training, validation and testing respectively. Each im-
age has around 5 crowdsourced captions. Unlike Flickr30k
Entities, COCO does not have bounding box annotations
associated with speciﬁc phrases or entities in the caption.
To identify visual words, we manually constructed an ob-
ject category to word mapping that maps object categories
like <person> to a list of potential ﬁne-grained labels like
[“child”, “baker”, ...]. This results in 80 categories with a
total of 413 ﬁne-grained classes. See supp. for details.

Figure 5. Generated captions and corresponding visual grounding regions on the standard image captioning task (Top: COCO, Bottom:
Flickr30k). Different colors show a correspondence between the visual words and grounding regions. Grey regions are the proposals not
selected in the caption. First 3 columns show success and last column shows failure cases (words are grounded in the wrong region).

Method

BLEU1 BLEU4 METEOR CIDEr SPICE

Method

BLEU1 BLEU4 METEOR CIDEr SPICE

Hard-Attention [46]
ATT-FCN [50]
Adaptive [27]

NBT
NBToracle

66.9
64.7
67.7

69.0
72.0

19.9
23.0
25.1

27.1
28.5

18.5
18.9
20.4

21.7
23.1

-
-
53.1

57.5
64.8

-
-
14.5

15.6
19.6

Table 1. Performance on the test portion of Karpathy et al. [20]’s
splits on Flickr30k Entities dataset.

Adaptive [27]
Att2in [39]
Up-Down [3]

Att2in∗ [39]
Up-Down† [3]

NBT
NBToracle

74.2
-
74.5

-
79.8

75.5
75.9

32.5
31.3
33.4

33.3
36.3

34.7
34.9

26.6
26.0
26.1

26.3
27.7

27.1
27.4

108.5
101.3
105.4

111.4
120.1

107.2
108.9

19.5
-
19.2

-
21.4

20.1
20.4

Detector pre-training. We use open an source imple-
mentation [47] of Faster-RCNN [38] to train the detector.
For Flickr30K Entities, we use visual words that occur at
least 100 times as detection labels, resulting in a total of 460
detection labels. Since detection labels and visual words
have a one-to-one mapping, we do not have ﬁne-grained
classes for the Flickr30K Entities dataset – the caption re-
ﬁnement process only determines the plurality of detection
labels. For COCO, ground truth detection annotations are
used to train the object detector.

Caption pre-processing. We truncate captions longer
than 16 words for both COCO and Flickr30k Entities
dataset. We then build a vocabulary of words that occur at
least 5 times in the training set, resulting in 9,587 and 6,864
words for COCO and Flickr30k Entities, respectively.

4.1. Standard Image Captioning

For standard image captioning, we use splits from
Karpathy et al. [20] on COCO/Flickr30k. We report re-
sults using the COCO captioning evaluation toolkit [26],
which reports the widely used automatic evaluation metrics,
BLEU [35], METEOR [10], CIDEr [42] and SPICE [1].

Table 2. Performance on the test portion of Karpathy et al. [20]’s
splits on COCO dataset. ∗ directly optimizes the CIDEr Metric, †
uses better image features, and are thus not directly comparable.

We present our methods trained on different object de-
tectors: Flickr and COCO. We compare our approach (re-
ferred to as NBT) to recently proposed Hard-Attention [46],
ATT-FCN [50] and Adaptive [27] on Flickr30k, and Att2in
[39], Up-Down [3] on COCO. Since object detectors have
not yet achieved near-perfect accuracies on these datasets,
we also report the performance of our model under an oracle
setting, where the ground truth object region and category
is also provided during test time. (referred to as NBToracle)
This can be viewed as the upper bound of our method when
we have perfect object detectors.

Table 1 shows results on the Flickr30k dataset. We see
that our method achieves state of the art on all automatic
evaluation metrics, outperforming the previous state-of-art
model Adaptive [27] by 2.0 and 4.4 on BLEU4 and CIDEr.
When using ground truth proposals, NBToracle signiﬁcantly
outperforms previous methods, improving 5.1 on SPICE,
which implies that our method could further beneﬁt from
improved object detectors.

Figure 6. Generated captions and corresponding visual grounding regions for the robust image captioning task. “cat-remote”, “man-bird”,
“dog-skateboard” and “orange-bird” are co-occurring categories excluded in the training split. First 3 columns show success and last
column shows failure case (orange was not mentioned).

Method

BLEU4 METEOR CIDEr SPICE Accuracy

Att2in [39]
Up-Down [3]

NBT
NBToracle

31.5
31.6

31.7
31.9

24.6
25.0

25.2
25.5

90.6
92.0

94.1
95.5

17.7
18.1

18.3
18.7

39.0
39.7

42.4
45.7

Table 3. Performance on the test portion of the robust image cap-
tioning split on COCO dataset.

Table 2 shows results on the COCO dataset. Our method
outperforms 4 out of 5 automatic evaluation metrics com-
pared to the state of the art [39, 27, 3] without using better
visual features or directly optimizing the CIDEr metric. In-
terestingly, the NBToracle has little improvement over NBT.
We suspect the reason is that explicit ground truth anno-
tation is absent for visual words. Our model can be fur-
ther improved with explicit co-reference supervision where
the ground truth location annotation of the visual word is
provided. Fig. 5 shows qualitative results on both datasets.
We see that our model learns to correctly identify the visual
word, and ground it in image regions even under weak su-
pervision (COCO). Our model is also robust to erroneous
detections and produces correct captions (3rd column).

4.2. Robust Image Captioning

To quantitatively evaluate image captioning models for
novel scene compositions, we present a new split of the
COCO dataset, called the robust-COCO split. This new
split is created by re-organizing the train and val splits of
the COCO dataset such that the distribution of co-occurring
objects in train is different from test. We also present a new
metric to evaluate grounding.

Robust split. To create the new split, we ﬁrst identify
entity words that belong to the 80 COCO object categories
by following the same pre-processing procedure. For each
image, we get a list of object categories that are mentioned
in the caption. We then calculate the co-occurrence statis-
tics for these 80 object categories. Starting from the least
co-occurring category pairs, we greedily add them to the
test set and ensure that for each category, at least half the
instances of each category are in the train set. As a re-

sult, there are sufﬁcient examples from each category in
train, but at test time we see novel compositions (pairs)
of categories. Remaining images are assigned to the train-
ing set. The ﬁnal split has 110,234/3,915/9,138 images in
train/val/test respectively.

Evaluation metric. To evaluate visual grounding on the
robust-COCO split, we want a metric that indicates whether
or not a generated caption includes the new object combina-
tion. Common automatic evaluation metrics such as BLEU
[35] and CIDEr [42] measure the overall sentence ﬂuency.
We also measure whether the generated caption contains the
novel co-occurring categories that exist in the ground truth
caption. A generated caption is deemed 100% accurate if it
contains at least one mention of the compositionally novel
category-pairs in any ground truth annotation that describe
the image.

Results and analysis. We compare our method with
state of the art Att2in [39] and Up-Down [3]. These are
implemented using the open source implementation from
[28] that can replicate results on Karpathy’s split. We fol-
low the experimental setting from [39] and train the model
using the robust-COCO train set. Table 3 shows the results
on the robust-COCO split. As we can see, all models per-
form worse on the robust-COCO split than the Karpathy’s
split by 2∼3 points in general. Our method outperforms
the previous state of the art methods on all metrics, outper-
forming Up-Down [3] by 2.7 on the proposed metric. The
oracle setting (NBToracle) has consistent improvements on
all metrics, improving 3.3 on the proposed metric.

Fig. 6 shows qualitative results on the robust image cap-
tioning task. Our model successfully produces a caption
with novel compositions, such as “cat-remote”, “man-bird”
and “dog-skateboard” to describe the image. The last col-
umn shows failure cases where our model didn’t select “or-
ange” in the caption. We can force our model to produce
a caption containing “orange” and “bird” using constrained
beam search [2], further illustrated in Sec. 4.3.

4.3. Novel Object Captioning

Since our model directly ﬁlls the “slotted” caption tem-
plate with the concept, it can seamlessly generate descrip-

Figure 7. Generated captions and corresponding visual grounding regions for the novel object captioning task. “zebra”, “tennis racket”,
“bus” and “pizza” are categories excluded in the training split. First 3 columns show success and last column shows a failure case.

Out-of-Domain Test Data

In-Domain Test Data

Method

bottle

bus

couch microwave pizza racket

suitcase zebra Avg SPICE METEOR CIDEr SPICE METEOR CIDER

DCC [4]
NOC [43]
C-LSTM [49]
Base+T4 [2]

NBT∗+G
NBT†+G
NBT†+T1
NBT†+T2

4.6
17.8
29.7
16.3

7.1
14.0
36.2
38.3

29.8
68.8
74.4
67.8

73.7
74.8
77.7
80.0

45.9
25.6
38.8
48.2

34.4
42.8
43.9
54.0

28.1
24.7
27.8
29.7

61.9
63.7
65.8
70.3

64.6
69.3
68.2
77.2

59.9
74.4
70.3
81.1

52.2
68.1
70.3
57.1

20.2
19.0
19.8
74.8

13.2
39.9
44.8
49.9

42.3
44.5
51.2
67.8

79.9
89.0
91.4
85.7

88.5
92.0
93.7
96.6

39.8
49.1
55.7
54.0

48.5
53.2
57.3
70.3

13.4
-
-
15.9

15.7
16.6
16.7
17.4

21.0
21.4
23.0
23.3

22.8
23.9
23.9
24.1

59.1
-
-
77.9

77.0
84.0
85.7
86.0

15.9
-
-
18.0

17.5
18.4
18.4
18.0

23.0
-
-
24.5

24.3
25.3
25.5
25.0

77.2
-
-
86.3

87.4
94.0
95.2
92.1

Table 4. Evaluation of captions generated using the proposed method. G means greedy decoding, and T1−2 means using constrained beam
search [2] with 1−2 top detected concepts. ∗ is the result using VGG-16 [41] and † is the result using ResNet-101.

tions for out-of-domain images. We replicated an existing
experimental design [4] on COCO which excludes all the
image-sentence pairs that contain at least one of eight ob-
jects in COCO. The excluded objects are ‘bottle’, “bus”,
“couch”, “microwave”, “pizza”, “racket”, “suitcase” and
“zebra”. We follow the same splits for training, valida-
tion, and testing as in prior work [4]. We use Faster R-
CNN in conjunction with ResNet-101 which is pre-trained
on COCO train split as the detection model. Note that we
do not pre-train the language model using COCO captions
as in [4, 43, 49], and simply replace the novel object’s word
embedding with an existing one which belongs to the same
super-category in COCO (e.g., bus ← car).

Following [2], the test set is split into in-domain and out-
of-domain subsets. We report F1 as in [4], which checks if
the speciﬁc excluded object is mentioned in the generated
caption. To evaluate the quality of the generated caption, we
use SPICE, METEOR and CIDEr metrics and the scores on
out-of-domain test data are macro-averaged across eight ex-
cluded categories. For consistency with previous work [3],
the inverse document frequency statistics used by CIDEr are
determined across the entire test set.

As illustrated in Table 4.1, simply using greedy decod-
ing, our model (NBT∗+G) can successfully caption novel
concepts with minimum changes to the model. When us-
ing ResNet-101 and constrained beam search [2], our model
signiﬁcantly outperforms prior works under F1 scores,
SPICE, METEOR, and CIDEr, across both out-of-domain
and in-domain test data. Speciﬁcally, NBT†+T2 outper-

forms the previous state-of-art model C-LSTM by 14.6%
on average F1 scores. From the category F1 scores, we
can see that our model is less likely to select small objects,
e.g. “bottle”, “racket” when only using the greedy decod-
ing. Since the visual words are grounded at the object-level,
by using [2], our model was able to signiﬁcantly boost the
captioning performance on out-of-domain images. Fig. 7
shows qualitative novel object captioning results. Also see
rightmost example in Fig. 2.

5. Conclusion

In this paper, we introduce Neural Baby Talk, a novel im-
age captioning framework that produces natural language
explicitly grounded in entities object detectors ﬁnd in im-
ages. Our approach is a two-stage approach that ﬁrst gener-
ates a hybrid template that contains a mix of words from a
text vocabulary as well as slots corresponding to image re-
gions. It then ﬁlls the slots based on categories recognized
by object detectors in the image regions. We also introduce
a robust image captioning split by re-organizing the train
and val splits of the COCO dataset. Experimental results on
standard, robust, and novel object image captioning tasks
validate the effectiveness of our proposed approach.

Acknowledgements This work was funded in part by: NSF
CAREER awards to DB, DP; ONR YIP awards to DP, DB;
ONR Grants N00014-14-1-{0679,2713}; PGA Family Founda-
tion award to DP; Google FRAs to DP, DB; and Amazon ARAs to
DP, DB; DARPA XAI grant to DB, DP.

6. Appendix: COCO Fine-grained Categories

The COCO [26] dataset does not have bounding box annota-
tions associated with speciﬁc phrases or entities in the caption.
We use category level detection annotations and create a cate-
gory mapping list that maps the object categories like <Person>
to a list of potential ﬁne-grained labels like [“child”, “man”,
“baker”,...]. We ﬁrst use the Stanford lemmatization toolbox [30]
to get the base form of the entity words in the caption. For each
category class, we retrieve the top 200 similar words in the Word-
Vec [33] space. We then manually verify each word in the list,
resulting in 413 ﬁne-grained classes. A complete list of the ﬁne-
grained class for each object category can be found in Table 5 and
Table 6.

References

[1] P. Anderson, B. Fernando, M. Johnson, and S. Gould. Spice:
Semantic propositional image caption evaluation. In ECCV,
2016. 6

[2] P. Anderson, B. Fernando, M. Johnson, and S. Gould.
Guided open vocabulary image captioning with constrained
beam search. EMNLP, 2017. 3, 7, 8

[3] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,
S. Gould, and L. Zhang. Bottom-up and top-down atten-
tion for image captioning and visual question answering. In
CVPR, 2018. 3, 5, 6, 7, 8

[4] L. Anne Hendricks, S. Venugopalan, M. Rohrbach,
R. Mooney, K. Saenko, and T. Darrell. Deep composi-
tional captioning: Describing novel object categories with-
out paired training data. In CVPR, 2016. 3, 8

[5] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.
Zitnick, and D. Parikh. VQA: Visual Question Answering.
In ICCV, 2015. 1

[6] D. Chen and C. Manning. A fast and accurate dependency

parser using neural networks. In EMNLP, 2014. 5

[7] X. Chen and C. Lawrence Zitnick. Mind’s eye: A recurrent
visual representation for image caption generation. In CVPR,
2015. 3

[8] A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, and D. Ba-
tra. Human Attention in Visual Question Answering: Do
Humans and Deep Networks Look at the Same Regions? In
EMNLP, 2016. 1

[9] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M.
Moura, D. Parikh, and D. Batra. Visual Dialog. In CVPR,
2017. 1

[10] M. Denkowski and A. Lavie. Meteor universal: Language
speciﬁc translation evaluation for any target language.
In
EACL 2014 Workshop on Statistical Machine Translation,
2014. 6

[11] J. Donahue, L. Anne Hendricks,

S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual
recognition and description. In CVPR, 2015. 3

[12] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng,
P. Doll´ar, J. Gao, X. He, M. Mitchell, J. C. Platt, et al. From
captions to visual concepts and back. In CVPR, 2015. 1, 3

[13] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every pic-
ture tells a story: Generating sentences from images.
In
ECCV, 2010. 2, 3

[14] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask r-cnn.

ICCV, 2017. 5

[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 5

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997. 4

[17] R. Hu, M. Rohrbach,

and
K. Saenko. Modeling relationships in referential expres-
sions with compositional modular networks. arXiv preprint
arXiv:1611.09978, 2016. 3

J. Andreas, T. Darrell,

[18] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Dar-
rell. Natural language object retrieval. In CVPR, 2016. 3
[19] J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully
convolutional localization networks for dense captioning. In
CVPR, 2016. 3

[20] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
In CVPR, 2015.

ments for generating image descriptions.
1, 3, 6

[21] S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg.
Referitgame: Referring to objects in photographs of natural
scenes. In EMNLP, 2014. 3

[22] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014. 5

[23] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Multimodal

neural language models. In ICML, 2014. 3

[24] G. Kulkarni, V. Premraj, V. Ordonez, S. Dhar, S. Li, Y. Choi,
A. C. Berg, and T. L. Berg. Babytalk: Understanding and
generating simple image descriptions. In CVPR, 2011. 1, 2,
3

[25] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and
Y. Choi. Collective generation of natural image descriptions.
In ACL, 2012. 2, 3

[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 2, 4, 5, 6, 9
[27] J. Lu, C. Xiong, D. Parikh, and R. Socher. Knowing when
to look: Adaptive attention via a visual sentinel for image
captioning. In CVPR, 2017. 1, 3, 4, 6, 7

[28] R. Luo.

Unofﬁcial

for
sequence
caption-
for
https://github.com/ruotianluo/

implementation
image

self-critical
ing.
self-critical.pytorch, 2017. 7

training

pytorch

[29] R. Luo and G. Shakhnarovich. Comprehension-guided refer-

ring expressions. In CVPR, 2017. 3

[30] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J.
Bethard, and D. McClosky. The Stanford CoreNLP natural
language processing toolkit. In ACL, 2014. 5, 9

[31] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and
K. Murphy. Generation and comprehension of unambiguous
object descriptions. In CVPR, 2016. 3

[32] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille.
Deep captioning with multimodal recurrent neural networks
(m-rnn). In ICLR, 2015. 3

[33] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
preprint arXiv:1301.3781, 2013. 9

[34] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal,
A. Berg, K. Yamaguchi, T. Berg, K. Stratos,
and
H. Daum´e III. Midge: Generating image descriptions from
computer vision detections. In EACL, 2012. 2, 3

[35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a
method for automatic evaluation of machine translation. In
ACL, 2002. 6, 7

[36] J. Pennington, R. Socher, and C. Manning. Glove: Global
vectors for word representation. In EMNLP, 2014. 4
[37] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,
J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer image-
to-sentence models. In ICCV, 2015. 3, 4, 5

[38] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 3, 4, 5, 6

[39] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel.
Self-critical sequence training for image captioning.
In
CVPR, 2017. 3, 4, 6, 7

[40] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and
B. Schiele. Grounding of textual phrases in images by re-
construction. In ECCV, 2016. 3

[41] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 8

[42] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider:
In CVPR,

Consensus-based image description evaluation.
2015. 6, 7

[43] S. Venugopalan, L. A. Hendricks, M. Rohrbach, R. Mooney,
T. Darrell, and K. Saenko. Captioning images with diverse
objects. In CVPR, 2017. 3, 8

[44] O. Vinyals, M. Fortunato, and N. Jaitly. Pointer networks. In

NIPS, 2015. 4

[45] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and

tell: A neural image caption generator. In CVPR, 2015. 3

[46] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdi-
nov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural
In ICML,
image caption generation with visual attention.
2015. 1, 3, 6

[47] J. Yang, J. Lu, D. Batra, and D. Parikh. A faster pytorch
implementation of faster r-cnn. https://github.com/
jwyang/faster-rcnn.pytorch, 2017. 6

[48] Z. Yang, Y. Yuan, Y. Wu, R. Salakhutdinov, and W. W. Co-
hen. Encode, review, and decode: Reviewer module for cap-
tion generation. In NIPS, 2016. 1, 3
[49] T. Yao, Y. Pan, Y. Li, and T. Mei.

Incorporating copying
mechanism in image captioning for learning novel objects.
In CVPR, 2017. 3, 8

[50] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image cap-
tioning with semantic attention. In CVPR, 2016. 1, 3, 6
[51] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Mod-
eling context in referring expressions. In ECCV, 2016. 3

Object category

Fine-grained class

<person>

<bicycle>
<car>
<motorcycle>
<airplane>
<bus>
<train>
<truck>
<boat>

person, girl, boy, man, woman, kid, child, chef, baker, people, adult, rider, children, baby, worker, passenger, sister, biker, policeman,
ofﬁcer, lady, cowboy, bride, groom, male, female, guy, traveler, mother, father, gentleman, pitcher, player, skier, snowboarder,
skater, skateboarder, foreigner, caller, offender, coworker, trespasser, patient, politician, soldier, grandchild, serviceman, walker,
drinker, doctor, bicyclist, thief, buyer, teenager, student, camper, driver, solider, hunter, shopper, villager, cop
bicycle, bike, unicycle, minibike, trike
car, automobile, van, minivan, sedan, suv, hatchback, cab, jeep, coupe, taxicab, limo, taxi
motorcycle, scooter, motor bike, motor cycle, motorbike, moped
airplane, jetliner, plane, air plane, monoplane, aircraft, jet, airbus, biplane, seaplane bus, minibus, trolley
bus, minibus, schoolbus, trolley
train, locomotive, tramway, caboose
truck, pickup, lorry, hauler, ﬁretruck
boat, ship, liner, sailboat, motorboat, dinghy, powerboat, speedboat, canoe, skiff, yacht, kayak, catamaran, pontoon, houseboat, vessel,
rowboat, trawler, ferryboat, watercraft, tugboat, schooner, barge, ferry, sailboard, paddleboat, lifeboat, freighter, steamboat, riverboat,
surfboard, battleship, steamship
trafﬁc light, street light, trafﬁc signal, stop light, streetlight, stoplight
ﬁre hydrant, hydrant
stop sign, street sign

<trafﬁc light>
<ﬁre hydrant>
<stop sign>
<parking meter> parking meter
<bench>
<cat>
<dog>

<horse>
<sheep>
<cow>
<elephant>
<bear>
<zebra>
<giraffe>
<backpack>
<umbrella>
<handbag>
<tie>
<suitcase>
<frisbee>
<skis>
<snowboard>
<sports ball>
<kite>
<baseball bat>
<baseball glove> baseball glove
<skateboard>
<surfboard>
<tennis racket>
<bottle>

bench, pew
cat, kitten, feline, tabby
dog, puppy, beagle, pup, chihuahua, schnauzer, dachshund, rottweiler, canine, pitbull, collie, pug, terrier, poodle, labrador, doggie,
doberman, mutt, doggy, spaniel, bulldog, sheepdog, weimaraner, corgi, cocker, greyhound, retriever, brindle, hound, whippet, husky
horse, colt, pony, racehorse, stallion, equine, mare, foal, palomino, mustang, clydesdale, bronc, bronco
sheep, lamb, goat, ram, cattle, ewe
cow, cattle, oxen, ox, calf, ewe, holstein, heifer, buffalo, bull, zebu, bison
elephant
bear, panda
zebra
giraffe
backpack, knapsack
umbrella
handbag, handbag, wallet, purse, briefcase
tie
suitcase, suit case, luggage
frisbee
skis, ski
snowboard
sports ball, baseball, ball, football, soccer, basketball, softball, volleyball, pinball, fastball, racquetball
kite
baseball bat

skateboard
surfboard, longboard, skimboard, shortboard, wakeboard
tennis racket
bottle

Table 5. COCO category mapping list for visual words.

Object category

Fine-grained class

<wine glass>
<cup>
<fork>
<knife>
<spoon>
<bowl>
<banana>
<apple>
<sandwich>
<orange>
<broccoli>
<carrot>
<hot dog>
<pizza>
<donut>
<cake>
<bird>

<chair>
<couch>
<potted plant>
<bed>
<dining table>
<toilet>
<tv>
<laptop>
<mouse>
<remote>
<keyboard>
<cell phone>
<sink>
<refrigerator>
<book>
<clock>
<vase>
<scissors>
<teddy bear>
<hair drier>
<toothbrush>

wine glass
cup
fork
knife, pocketknife, knive
spoon
bowl, container, plate
banana
apple
sandwich, burger, sub, cheeseburger, hamburger
orange, lemons
broccoli
carrot
hot dog
pizza
donut, doughnut, bagel
cake, cheesecake, cupcake, shortcake, coffeecake, pancake
bird, ostrich, owl, seagull, goose, duck, parakeet, falcon, robin, pelican, waterfowl, heron, hummingbird, mallard, ﬁnch, pigeon, sparrow,
seabird, osprey, blackbird, fowl, shorebird, woodpecker, egret, chickadee, quail, bluebird, kingﬁsher, buzzard, willet, gull, swan, bluejay,
ﬂamingo, cormorant, parrot, loon, gosling, waterbird, pheasant, rooster, sandpiper, crow, raven, turkey, oriole, cowbird, warbler, magpie,
peacock, cockatiel, lorikeet, pufﬁn, vulture, condor, macaw, peafowl, cockatoo, songbird
chair, seat, recliner, stool
couch, sofa, recliner, futon, loveseat, settee, chesterﬁeld
potted plant, houseplant
bed
dining table, table
toilet, urinal, commode, lavatory, potty
tv, monitor, televison, television
laptop, computer, notebook, netbook, lenovo, macbook
mouse
remote
keyboard
cell phone, mobile phone, phone, cellphone, cellphone, telephone, phon, smartphone, iPhone
sink
refrigerator, fridge, refrigerator, fridge, freezer, refridgerator, frig
book
clock
vase
scissors
teddy bear, teddybear
hair drier, hairdryer
toothbrush

Table 6. COCO category mapping list for visual words (continued).

Neural Baby Talk

Jiasen Lu1∗

Jianwei Yang1∗ Dhruv Batra1,2 Devi Parikh1,2

1Georgia Institute of Technology

2Facebook AI Research

{jiasenlu, jw2yang, dbatra, parikh}@gatech.edu

8
1
0
2
 
r
a

M
 
7
2
 
 
]

V
C
.
s
c
[
 
 
1
v
5
4
8
9
0
.
3
0
8
1
:
v
i
X
r
a

Abstract

We introduce a novel framework for image captioning
that can produce natural language explicitly grounded in
entities that object detectors ﬁnd in the image. Our ap-
proach reconciles classical slot ﬁlling approaches (that are
generally better grounded in images) with modern neu-
ral captioning approaches (that are generally more natu-
ral sounding and accurate). Our approach ﬁrst generates
a sentence ‘template’ with slot locations explicitly tied to
speciﬁc image regions. These slots are then ﬁlled in by
visual concepts identiﬁed in the regions by object detec-
tors. The entire architecture (sentence template generation
and slot ﬁlling with object detectors) is end-to-end differen-
tiable. We verify the effectiveness of our proposed model
on different image captioning tasks. On standard image
captioning and novel object captioning, our model reaches
state-of-the-art on both COCO and Flickr30k datasets.
We also demonstrate that our model has unique advan-
tages when the train and test distributions of scene com-
positions – and hence language priors of associated cap-
tions – are different. Code has been made available at:
https://github.com/jiasenlu/NeuralBabyTalk.

1. Introduction

Image captioning is a challenging problem that lies at the
intersection of computer vision and natural language pro-
cessing. It involves generating a natural language sentence
that accurately summarizes the contents of an image. Im-
age captioning is also an important ﬁrst step towards real-
world applications with signiﬁcant practical impact, rang-
ing from aiding visually impaired users to personal assis-
tants to human-robot interaction [5, 9].

State-of-art image captioning models today tend to be
monolithic neural models, essentially of the “encoder-
decoder” paradigm. Images are encoded into a vector with
a convolutional neural network (CNN), and captions are de-
coded from this vector using a Recurrent Neural Network
(RNN), with the entire system trained end-to-end. While

Figure 1. Example captions generated by (a) Baby Talk [24], (c)
neural image captioning [20] and (b) our Neural Baby Talk ap-
proach. Our method generates the sentence “template” with slot
locations (illustrated with ﬁlled boxes) explicitly tied to image re-
gions (drawn in the image in corresponding colors). These slots
are then ﬁlled by object detectors with concepts found in regions.

there are many recent extensions of this basic idea to in-
clude attention [46, 12, 50, 48, 27], it is well-understood
that models still lack visual grounding (i.e., do not associate
named concepts to pixels in the image). They often tend to
‘look’ at different regions than humans would and tend to
copy captions from training data [8].

For instance, in Fig. 1 a neural image captioning ap-
proach [20] describes the image as “A dog is sitting on a
couch with a toy.” This is not quite accurate. But if one
were to really squint at the image, it (arguably) does per-
haps look like a scene where a dog could be sitting on a
couch with a toy. It certainly is common to ﬁnd dogs sitting
on couches with toys. A-priori, the description is reason-
able. That’s exactly what today’s neural captioning models
tend to do – produce generic plausible captions based on the
language model1 that match a ﬁrst-glance gist of the scene.
While this may sufﬁce for common scenes, images that dif-
fer from canonical scenes – given the diversity in our visual
world, there are plenty of such images – tend to be under-
served by these models.

If we take a step back – do we really need the language
model to do the heavy lifting in image captioning? Given

∗Equal contribution

1frequently, directly reproduced from a caption in the training data.

1

Figure 2. From left to right is the generated caption using the same captioning model but with different detectors: 1) No detector; 2) A
weak detector that only detects “person” and “sandwich”; 3) A detector trained on COCO [26] categories (including “teddy bear”). 4) A
detector that can detect novel concepts (e.g. “Mr. Ted” and “pie” that never occurred in the captioning training data). Different colors show
a correspondence between the visual word and grounding regions.

the unprecedented progress we are seeing in object recog-
nition2 (e.g., object detection, semantic segmentation, in-
stance segmentation, pose estimation), it seems like the vi-
sion pipeline can certainly do better than rely on just a ﬁrst-
glance gist of the scene. In fact, today’s state-of-the-art ob-
ject detectors can successfully detect the table and cake in
the image in Fig. 1(c)! The caption ought to be able to talk
about the table and cake actually detected as opposed to
letting the language model hallucinate a couch and a toy
simply because that sounds plausible.

Interestingly, some of the ﬁrst attempts at image caption-
ing [13, 24, 25, 34] – before the deep learning “revolution”
– relied heavily on outputs of object detectors and attribute
classiﬁers to describe images. For instance, consider the
output of Baby Talk [24] in Fig. 1, that used a slot ﬁlling
approach to talk about all the objects and attributes found in
the scene via a templated caption. The language is unnatu-
ral but the caption is very much grounded in what the model
sees in the image. Today’s approaches fall at the other ex-
treme on the spectrum – the language generated by modern
neural image captioning approaches is much more natural
but tends to be much less grounded in the image.

In this paper, we introduce Neural Baby Talk that recon-
ciles these methodologies. It produces natural language ex-
plicitly grounded in entities found by object detectors. It is
a neural approach that generates a sentence “template” with
slot locations explicitly tied to image regions. These slots
are then ﬁlled by object recognizers with concepts found in
the regions. The entire approach is trained end-to-end. This
results in natural sounding and grounded captions.

Our main technical contribution is a novel neural de-
coder for grounded image captioning. Speciﬁcally, at each
time step, the model decides whether to generate a word
from the textual vocabulary or generate a “visual” word.
The visual word is essentially a token that will hold the slot
for a word that is to describe a speciﬁc region in the image.
For instance, for the image in Fig. 1, the generated sequence

may be “A <region−17> is sitting at a <region−123>
with a <region−3>.” The visual words (<region−[.]>’s)
are then ﬁlled in during a second stage that classiﬁes
each of the indicated regions (e.g., <region−17>→puppy,
<region−123>→table), resulting in a ﬁnal description of
“A puppy is sitting at a table with a cake.” – a free-form
natural language description that is grounded in the image.
One nice feature of our model is that it allows for different
object detectors to be plugged in easily. As a result, a va-
riety of captions can be produced for the same image using
different detection backends. See Fig. 2 for an illustration.
Contributions: Our contributions are as follows:

• We present Neural Baby Talk – a novel framework for
visually grounded image captioning that explicitly lo-
calizes objects in the image while generating free-form
natural language descriptions.

• Ours is a two-stage approach that ﬁrst generates a hy-
brid template that contains a mix of (text) words and
slots explicitly associated with image regions, and then
ﬁlls in the slots with (text) words by recognizing the
content in the corresponding image regions.

• We propose a robust image captioning task to bench-
mark compositionality of image captioning algorithms
where at test time the model encounters images con-
taining known objects but in novel combinations (e.g.,
the model has seen dogs on couches and people at ta-
bles during training, but at test time encounters a dog
at a table). Generalizing to such novel compositions is
one way to demonstrate image grounding as opposed
to simply leveraging correlations from training data.
• Our proposed method achieves state-of-the-art perfor-
mance on COCO and Flickr30k datasets on the stan-
dard image captioning task, and signiﬁcantly outper-
forms existing approaches on the robust image cap-
tioning and novel object captioning tasks.

2. Related Work

2e.g., 11% absolute increase in average precision in object detection in

the COCO challenge in the last year.

Some of the earlier approaches generated templated im-
age captions via slot-ﬁlling. For instance, Kulkarni et

al. [24] detect objects, attributes, and prepositions, jointly
reason about these through a CRF, and ﬁnally ﬁll appropri-
ate slots in a template. Farhadi et al. [13] compute a triplet
for a scene, and use this templated “meaning” representa-
tion to retrieve a caption from a database. [25, 34] use more
powerful language templates such as a syntactically well-
formed tree. These approaches tend to either produce cap-
tions that are relevant to the image but not natural sounding,
or captions that are natural (e.g. retrieved from a database of
captions) but may not be sufﬁciently grounded in the image.
Neural models for image captioning have been receiv-
ing increased attention in the last few years [23, 32, 7, 45,
11, 20]. State-of-the-art neural approaches include atten-
tion mechanisms [46, 12, 50, 48, 27, 39, 3] that identify re-
gions in the image to “ground” emitted words. In practice,
these attention regions tend to be quite blurry, and rarely
correspond to semantically meaningful individual entities
(e.g., objects instances) in the image. Our approach grounds
words in object detections, which by design identify con-
crete semantic entities (object instances) in the image.

There has been some recent interest in grounding natu-
ral language in images. Dense Captioning [19] generates
descriptions for speciﬁc image regions.
In contrast, our
model produces captions for the entire image, with words
grounded in concrete entities in the image. Another related
line of work is on resolving referring expressions [21] (or
description-based object retrieval [37, 17, 18, 40] – given a
description of an object in the image, identify which ob-
ject is being referred to) or referring expression genera-
tion [21, 29, 31, 51] (given an object in the image, generate a
discriminative description of the object). While the interest
in grounded language is in common, our task is different.

One natural strength of our model is its ability to in-
corporate different object detectors, including the ability to
generate captions with novel objects (never seen before in
training captions). In that context, our work is related to
prior works on novel object captioning [4, 43, 49, 2]. As
we describe in Sec. 4.3, our method outperforms these ap-
proaches by 14.6% on the averaged F1 score.

3. Method

Given an image I, the goal of our method is to gener-
ate visually grounded descriptions y = {y1, . . . , yT }. Let
rI = {r1, ..., rN } be the set of N images regions extracted
from I. When generating an entity word in the caption, we
want to ground it in a speciﬁc image region r ∈ rI . Fol-
lowing the standard supervised learning paradigm, we learn
parameters θ of our model by maximizing the likelihood of
the correct caption:

θ∗ = arg max

θ

(cid:88)

log p(y|I; θ)

(1)

(I,y)
Using chain rule, the joint probability distribution can be

decomposed over a sequence of tokens:

p(y|I) =

p(yt|y1:t−1, I)

(2)

T
(cid:89)

t=1

where we drop the dependency on model parameters to
avoid notational clutter. We introduce a latent variable rt
to denote a speciﬁc image region so that yt can explicitly
ground in it. Thus the probability of yt is decomposed to:

p(yt|y1:t−1, I) = p(yt|rt, y1:t−1, I)p(rt|y1:t−1, I)

(3)

In our framework, yt can be of one of two types: a vi-
sual word or a textual word, denoted as yvis and ytxt re-
spectively. A visual word yvis is a type of word that is
grounded in a speciﬁc image region drawn from rI . A tex-
tual word ytxt is a word from the remainder of the caption.
It is drawn from the language model , which is associated
with a “default” sentinel “region” ˜r obtained from the lan-
guage model [27] (discussed in Sec. 3.1). For example, as
illustrated in Fig. 1, “puppy” and “cake” grounded in the
bounding box of category “dog” and “cake” respectively,
are visual words. While “with” and “sitting” are not associ-
ated with any image regions and thus are textual words.

With this, Eq. 1 can be decomposed into two cascaded
objectives. First, maximizing the probability of generating
the sentence “template”. A sequence of grounding regions
associated with the visual words interspersed with the tex-
tual words can be viewed as a sentence “template”, where
the grounding regions are slots to be ﬁlled in with visual
words.3 An example template (Fig. 3) is “A <region−2>
is laying on the <region−4> near a <region−7>. Sec-
ond, maximizing the probability of visual words yvis
con-
ditioned on the grounding regions and object detection in-
formation, e.g., categories recognized by detector. In the
template example above, the model will ﬁll the slots with
‘cat’, ‘laptop’ and ‘chair’ respectively.

t

In the following, we ﬁrst describe how we generate the
slotted caption template (Sec. 3.1), and then how the slots
are ﬁlled in to obtain the ﬁnal image description (Sec. 3.2).
The overall objective function is described in Sec. 3.3 and
the implementation details in Sec. 3.4.

3.1. “Slotted” Caption Template Generation

Given an image I, and the corresponding caption y, the
candidate grounding regions are obtained by using a pre-
trained Faster-RCNN network [38]. To generate the cap-
tion “template”, we use a recurrent neural network, which
is commonly used as the decoder for image captioning
[32, 45]. At each time step, we compute the RNN hidden
state ht according to the previous hidden state ht−1 and the
input xt such that ht = RNN(xt, ht−1). At training time,

3Our approach is not limited to any pre-speciﬁed bank of templates.
Rather, our approach automatically generates a template (with placehold-
ers – slots – for visually grounded words), which may be any one of the
exponentially many possible templates.

P t

r = softmax([ut; wT

h tanh(Wsst + Wzht)])

(9)
where Ws ∈ Rd×d and Wz ∈ Rd×d are the parameters.
Notably, Wz and wh are the same parameters as in Eq. 4.
P t
r is the probability distribution over grounding regions rI
and visual sentinel ˜r. The last element of the vector in Eq. 9
captures p(˜r|y1:t−1).

We feed the hidden state ht into a softmax layer to obtain
the probability over textual words conditioned on the image,
all previous words, and the visual sentinel:

P t

txt = softmax (Wqht)
(10)
where Wq ∈ RV ×d, d is hidden state size, and V is textual
vocabulary size. Plugging in Eq. 10 and p(˜r|y1:t−1) from
the last element of the vector in Eq. 9 into Eq. 6 gives us the
probability of generating a textual word in the template.

3.2. Caption Reﬁnement: Filling in The Slots

To ﬁll the slots in the generated template with visual
words grounded in image regions, we leverage the outputs
of an object detection network. Given a grounding region,
the category can be obtained through any detection frame-
work [38]. But outputs of detection networks are typically
singular coarse labels e.g. “dog”. Captions often refer to
these entities in a ﬁne-grained fashion e.g. “puppy” or in
the plural form “dogs”. In order to accommodate for these
linguistic variations, the visual word yvis in our model is
a reﬁnement of the category name by considering the fol-
lowing two factors: First, determine the plurality – whether
it should be singular or plural. Second, determine the ﬁne-
grained class (if any). Using two single layer MLPs with
ReLU activation f (·), we compute them with:

P t
b = softmax (Wbfb ([vt; ht]))
g = softmax (cid:0)U T Wgfg ([vt; ht])(cid:1)
P t

(11)

(12)

Wb ∈ R2×d, Wg ∈ R300×d are the weight parameters.
U ∈ R300×k is the glove vector embeddings [36] for k
ﬁne-grained words associated with the category name. The
visual word yvis
is then determined by plurality and ﬁne-
grained class (e.g., if plurality is plural, and the ﬁne-grained
class is “puppy”, the visual word will be “puppies”).

t

3.3. Objective

Most standard image captioning datasets (e.g. COCO
[26]) do not contain phrase grounding annotations, while
some datasets do (e.g. Flickr30k [37]). Our training objec-
tive (presented next) can incorporate different kinds of su-
pervision – be it strong annotations indicating which words
in the caption are grounded in which boxes in the image, or
weak supervision where objects are annotated in the image
but are not aligned to words in the caption. Given the tar-
get ground truth caption y∗
1:T and a image captioning model

Figure 3. One block of the proposed approach. Given an image,
proposals from any object detector and current word “A”, the ﬁgure
shows the process to predict the next visual word “cat”.

xt is the ground truth token (teacher forcing) and at test time
is the sampled token yt−1. Our decoder consists of an atten-
tion based LSTM layer [39] that takes convolution feature
maps as input. Details can be found in Sec. 3.4. To generate
the “slot” for visual words, we use a pointer network [44]
that modulates a content-based attention mechanism over
the grounding regions. Let vt ∈ Rd×1 be the region fea-
ture of rt, which is calculated based on Faster R-CNN. We
compute the pointing vector with:

h tanh(Wvvt + Wzht)

i = wT
ut
P t
= softmax(ut)
rI

(4)

(5)

Since textual words ytxt

where Wv ∈ Rm×d, Wz ∈ Rd×d and wh ∈ Rd×1 are pa-
rameters to be learned. The softmax normalizes the vector
ut to be a distribution over grounding regions rI .
t

are not tied to speciﬁc regions
in the image, inspired by [27], we add a “visual sentinel”
˜r as a latent variable to serve as dummy grounding for the
textual word. The visual sentinel can be thought of as a la-
tent representation of what the decoder already knows about
the image. The probability of a textual word ytxt

then is:

t

p(ytxt
t

|y1:t−1) = p(ytxt

t

|˜r, y1:t−1)p(˜r|y1:t−1)

(6)

where we drop the dependency on I to avoid clutter.

We ﬁrst describe how the visual sentinel is computed,
and then how the textual words are determined based on the
visual sentinel. Following [27], when the decoder RNN is
an LSTM [16], the representation for visual sentinel st can
be obtained by:

gt = σ (Wxxt + Whht−1)
st = gt (cid:12) tanh (ct)

(7)

(8)

where Wx ∈ Rd×d, Wh ∈ Rd×d. xt is the LSTM input
at time step t, and gt is the gate applied on the cell state
ct. (cid:12) represents element-wise product, σ the logistic sig-
moid activation. Modifying Eq. 5, the probability over the
grounding regions including the visual sentinel is:

with parameters θ, we minimize the cross entropy loss:

L(θ) = −

T
(cid:88)

t=1

(cid:16)

(cid:122)
p(y∗

t |˜r, y∗

log

Textual word probability
(cid:125)(cid:124)
1:t−1)p(˜r|y∗

1:t−1)1(y∗

(cid:123)
t =ytxt) +

p (cid:0)b∗
(cid:124)

t , s∗

t |rt, y∗
(cid:123)(cid:122)
Caption reﬁnement

1:t−1

(cid:1)

(cid:125)

m
(cid:88)

i=1

(cid:0) 1
m
(cid:124)

p (cid:0)ri

t|y∗

1:t−1

(cid:1) (cid:1)1(y∗

t =yvis)

(cid:17)

(cid:123)(cid:122)
Averaged target region probability

(cid:125)

(13)
where y∗
t is the word from the ground truth caption at time
t. 1(y∗
t =ytxt) is the indicator function which equals to 1 if
y∗
t is textual word and 0 otherwise. b∗
t are the target
ground truth plurality and ﬁnd-grained class. {ri
i=1 ∈ rI
are the target grounding regions of the visual word at time
t. We maximize the averaged log probability of the target
grounding regions.

t and s∗

t}m

t}m

Visual word extraction. During training, visual words
in a caption are dynamically identiﬁed by matching the base
form of each word (using the Stanford lemmatization tool-
box [30]) against a vocabulary of visual words (details of
how to get visual word can be found in dataset Sec. 4). The
grounding regions {ri
i=1 for a visual word yt is identiﬁed
by computing the IoU of all boxes detected by the object
detection network with the ground truth bounding box as-
sociated with the category corresponding to yt. If the score
exceeds a threshold of 0.5 and the grounding region label
matches the visual word, the bounding boxes are selected
as the grounding regions. E.g., given a target visual word
“cat”, if there are no proposals that match the target bound-
ing box, the model predicts the textual word “cat” instead.

3.4. Implementation Details

Detection model. We use Faster R-CNN [38] with
ResNet-101 [15] to obtain region proposals for the image.
We use an IoU threshold of 0.7 for region proposal sup-
pression and 0.3 for class suppressions. A class detection
conﬁdence threshold of 0.5 is used to select regions.

i ; vl

i; vg

Region feature. We use a pre-trained ResNet-101 [15]
in our model. The image is ﬁrst resized to 576×576 and we
random crop 512 × 512 as the input to the CNN network.
Given proposals from the pre-trained detection model, the
feature vi for region i is a concatenation of 3 different fea-
tures vi = [vp
i ] where vp
i is the pooling feature of
RoI align layer [14] given the proposal coordinates, vl
i is
the location feature and vg
i is the glove vector embedding of
the class label for region i. Let xmin, ymin, xmax, ymax be the
bounding box coordinates of the region b; WI and HI be the
width and height of the image I. Then the location feature
vl
i can be obtained by projecting the normalized location
xmin
[
WI

] into another embedding space.

xmax
WI

ymax
HI

ymin
HI

Language model. We use an attention model with two
LSTM layers [3] as our base attention model. Given N re-

,

,

,

Figure 4. Language model used in our approach.

gion features from detection proposals V = {v1, . . . , vN }
and CNN features from the last convolution layer at K grids
ˆV = {ˆv1, . . . , ˆvK}, the language model has two separate
attention layers shown in Fig 4. The attention distribution
over the image features for detection proposals is:
z tanh (cid:0)WvV + (Wght)1T (cid:1)

(14)

zt = wT
αt = softmax(zt)

where Wv ∈ Rm×d, Wg ∈ Rd×d and w ∈ Rd×1. 1 ∈ RN
is a vector with all elements set to 1. αt is the attention
weight over N image location features.

Training details. In our experiments, we use a two layer
LSTM with hidden size 1024. The number of hidden units
in the attention layer and the size of the input word em-
bedding are 512. We use the Adam [22] optimizer with an
initial learning rate of 5 × 10−4 and anneal the learning rate
by a factor of 0.8 every three epochs. We train the model
up to 50 epochs with early stopping. Note that we do not
ﬁnetune the CNN network during training. We set the batch
size to be 100 for COCO [26] and 50 for Flickr30k [37].

4. Experimental Results

Datasets. We experiment with two datasets. Flickr30k
Entities [37] contains 275,755 bounding boxes from 31,783
images associated with natural language phrases. Each im-
age is annotated with 5 crowdsourced captions. For each
annotated phrase in the caption, we identify visual words
by selecting the inner most NP (noun phrase) tag from
the Stanford part-of-speech tagger [6]. We use Stanford
Lemmatization Toolbox [30] to get the base form of the en-
tity words resulting in 2,567 unique words.

COCO [26] contains 82,783, 40,504 and 40,775 images
for training, validation and testing respectively. Each im-
age has around 5 crowdsourced captions. Unlike Flickr30k
Entities, COCO does not have bounding box annotations
associated with speciﬁc phrases or entities in the caption.
To identify visual words, we manually constructed an ob-
ject category to word mapping that maps object categories
like <person> to a list of potential ﬁne-grained labels like
[“child”, “baker”, ...]. This results in 80 categories with a
total of 413 ﬁne-grained classes. See supp. for details.

Figure 5. Generated captions and corresponding visual grounding regions on the standard image captioning task (Top: COCO, Bottom:
Flickr30k). Different colors show a correspondence between the visual words and grounding regions. Grey regions are the proposals not
selected in the caption. First 3 columns show success and last column shows failure cases (words are grounded in the wrong region).

Method

BLEU1 BLEU4 METEOR CIDEr SPICE

Method

BLEU1 BLEU4 METEOR CIDEr SPICE

Hard-Attention [46]
ATT-FCN [50]
Adaptive [27]

NBT
NBToracle

66.9
64.7
67.7

69.0
72.0

19.9
23.0
25.1

27.1
28.5

18.5
18.9
20.4

21.7
23.1

-
-
53.1

57.5
64.8

-
-
14.5

15.6
19.6

Table 1. Performance on the test portion of Karpathy et al. [20]’s
splits on Flickr30k Entities dataset.

Adaptive [27]
Att2in [39]
Up-Down [3]

Att2in∗ [39]
Up-Down† [3]

NBT
NBToracle

74.2
-
74.5

-
79.8

75.5
75.9

32.5
31.3
33.4

33.3
36.3

34.7
34.9

26.6
26.0
26.1

26.3
27.7

27.1
27.4

108.5
101.3
105.4

111.4
120.1

107.2
108.9

19.5
-
19.2

-
21.4

20.1
20.4

Detector pre-training. We use open an source imple-
mentation [47] of Faster-RCNN [38] to train the detector.
For Flickr30K Entities, we use visual words that occur at
least 100 times as detection labels, resulting in a total of 460
detection labels. Since detection labels and visual words
have a one-to-one mapping, we do not have ﬁne-grained
classes for the Flickr30K Entities dataset – the caption re-
ﬁnement process only determines the plurality of detection
labels. For COCO, ground truth detection annotations are
used to train the object detector.

Caption pre-processing. We truncate captions longer
than 16 words for both COCO and Flickr30k Entities
dataset. We then build a vocabulary of words that occur at
least 5 times in the training set, resulting in 9,587 and 6,864
words for COCO and Flickr30k Entities, respectively.

4.1. Standard Image Captioning

For standard image captioning, we use splits from
Karpathy et al. [20] on COCO/Flickr30k. We report re-
sults using the COCO captioning evaluation toolkit [26],
which reports the widely used automatic evaluation metrics,
BLEU [35], METEOR [10], CIDEr [42] and SPICE [1].

Table 2. Performance on the test portion of Karpathy et al. [20]’s
splits on COCO dataset. ∗ directly optimizes the CIDEr Metric, †
uses better image features, and are thus not directly comparable.

We present our methods trained on different object de-
tectors: Flickr and COCO. We compare our approach (re-
ferred to as NBT) to recently proposed Hard-Attention [46],
ATT-FCN [50] and Adaptive [27] on Flickr30k, and Att2in
[39], Up-Down [3] on COCO. Since object detectors have
not yet achieved near-perfect accuracies on these datasets,
we also report the performance of our model under an oracle
setting, where the ground truth object region and category
is also provided during test time. (referred to as NBToracle)
This can be viewed as the upper bound of our method when
we have perfect object detectors.

Table 1 shows results on the Flickr30k dataset. We see
that our method achieves state of the art on all automatic
evaluation metrics, outperforming the previous state-of-art
model Adaptive [27] by 2.0 and 4.4 on BLEU4 and CIDEr.
When using ground truth proposals, NBToracle signiﬁcantly
outperforms previous methods, improving 5.1 on SPICE,
which implies that our method could further beneﬁt from
improved object detectors.

Figure 6. Generated captions and corresponding visual grounding regions for the robust image captioning task. “cat-remote”, “man-bird”,
“dog-skateboard” and “orange-bird” are co-occurring categories excluded in the training split. First 3 columns show success and last
column shows failure case (orange was not mentioned).

Method

BLEU4 METEOR CIDEr SPICE Accuracy

Att2in [39]
Up-Down [3]

NBT
NBToracle

31.5
31.6

31.7
31.9

24.6
25.0

25.2
25.5

90.6
92.0

94.1
95.5

17.7
18.1

18.3
18.7

39.0
39.7

42.4
45.7

Table 3. Performance on the test portion of the robust image cap-
tioning split on COCO dataset.

Table 2 shows results on the COCO dataset. Our method
outperforms 4 out of 5 automatic evaluation metrics com-
pared to the state of the art [39, 27, 3] without using better
visual features or directly optimizing the CIDEr metric. In-
terestingly, the NBToracle has little improvement over NBT.
We suspect the reason is that explicit ground truth anno-
tation is absent for visual words. Our model can be fur-
ther improved with explicit co-reference supervision where
the ground truth location annotation of the visual word is
provided. Fig. 5 shows qualitative results on both datasets.
We see that our model learns to correctly identify the visual
word, and ground it in image regions even under weak su-
pervision (COCO). Our model is also robust to erroneous
detections and produces correct captions (3rd column).

4.2. Robust Image Captioning

To quantitatively evaluate image captioning models for
novel scene compositions, we present a new split of the
COCO dataset, called the robust-COCO split. This new
split is created by re-organizing the train and val splits of
the COCO dataset such that the distribution of co-occurring
objects in train is different from test. We also present a new
metric to evaluate grounding.

Robust split. To create the new split, we ﬁrst identify
entity words that belong to the 80 COCO object categories
by following the same pre-processing procedure. For each
image, we get a list of object categories that are mentioned
in the caption. We then calculate the co-occurrence statis-
tics for these 80 object categories. Starting from the least
co-occurring category pairs, we greedily add them to the
test set and ensure that for each category, at least half the
instances of each category are in the train set. As a re-

sult, there are sufﬁcient examples from each category in
train, but at test time we see novel compositions (pairs)
of categories. Remaining images are assigned to the train-
ing set. The ﬁnal split has 110,234/3,915/9,138 images in
train/val/test respectively.

Evaluation metric. To evaluate visual grounding on the
robust-COCO split, we want a metric that indicates whether
or not a generated caption includes the new object combina-
tion. Common automatic evaluation metrics such as BLEU
[35] and CIDEr [42] measure the overall sentence ﬂuency.
We also measure whether the generated caption contains the
novel co-occurring categories that exist in the ground truth
caption. A generated caption is deemed 100% accurate if it
contains at least one mention of the compositionally novel
category-pairs in any ground truth annotation that describe
the image.

Results and analysis. We compare our method with
state of the art Att2in [39] and Up-Down [3]. These are
implemented using the open source implementation from
[28] that can replicate results on Karpathy’s split. We fol-
low the experimental setting from [39] and train the model
using the robust-COCO train set. Table 3 shows the results
on the robust-COCO split. As we can see, all models per-
form worse on the robust-COCO split than the Karpathy’s
split by 2∼3 points in general. Our method outperforms
the previous state of the art methods on all metrics, outper-
forming Up-Down [3] by 2.7 on the proposed metric. The
oracle setting (NBToracle) has consistent improvements on
all metrics, improving 3.3 on the proposed metric.

Fig. 6 shows qualitative results on the robust image cap-
tioning task. Our model successfully produces a caption
with novel compositions, such as “cat-remote”, “man-bird”
and “dog-skateboard” to describe the image. The last col-
umn shows failure cases where our model didn’t select “or-
ange” in the caption. We can force our model to produce
a caption containing “orange” and “bird” using constrained
beam search [2], further illustrated in Sec. 4.3.

4.3. Novel Object Captioning

Since our model directly ﬁlls the “slotted” caption tem-
plate with the concept, it can seamlessly generate descrip-

Figure 7. Generated captions and corresponding visual grounding regions for the novel object captioning task. “zebra”, “tennis racket”,
“bus” and “pizza” are categories excluded in the training split. First 3 columns show success and last column shows a failure case.

Out-of-Domain Test Data

In-Domain Test Data

Method

bottle

bus

couch microwave pizza racket

suitcase zebra Avg SPICE METEOR CIDEr SPICE METEOR CIDER

DCC [4]
NOC [43]
C-LSTM [49]
Base+T4 [2]

NBT∗+G
NBT†+G
NBT†+T1
NBT†+T2

4.6
17.8
29.7
16.3

7.1
14.0
36.2
38.3

29.8
68.8
74.4
67.8

73.7
74.8
77.7
80.0

45.9
25.6
38.8
48.2

34.4
42.8
43.9
54.0

28.1
24.7
27.8
29.7

61.9
63.7
65.8
70.3

64.6
69.3
68.2
77.2

59.9
74.4
70.3
81.1

52.2
68.1
70.3
57.1

20.2
19.0
19.8
74.8

13.2
39.9
44.8
49.9

42.3
44.5
51.2
67.8

79.9
89.0
91.4
85.7

88.5
92.0
93.7
96.6

39.8
49.1
55.7
54.0

48.5
53.2
57.3
70.3

13.4
-
-
15.9

15.7
16.6
16.7
17.4

21.0
21.4
23.0
23.3

22.8
23.9
23.9
24.1

59.1
-
-
77.9

77.0
84.0
85.7
86.0

15.9
-
-
18.0

17.5
18.4
18.4
18.0

23.0
-
-
24.5

24.3
25.3
25.5
25.0

77.2
-
-
86.3

87.4
94.0
95.2
92.1

Table 4. Evaluation of captions generated using the proposed method. G means greedy decoding, and T1−2 means using constrained beam
search [2] with 1−2 top detected concepts. ∗ is the result using VGG-16 [41] and † is the result using ResNet-101.

tions for out-of-domain images. We replicated an existing
experimental design [4] on COCO which excludes all the
image-sentence pairs that contain at least one of eight ob-
jects in COCO. The excluded objects are ‘bottle’, “bus”,
“couch”, “microwave”, “pizza”, “racket”, “suitcase” and
“zebra”. We follow the same splits for training, valida-
tion, and testing as in prior work [4]. We use Faster R-
CNN in conjunction with ResNet-101 which is pre-trained
on COCO train split as the detection model. Note that we
do not pre-train the language model using COCO captions
as in [4, 43, 49], and simply replace the novel object’s word
embedding with an existing one which belongs to the same
super-category in COCO (e.g., bus ← car).

Following [2], the test set is split into in-domain and out-
of-domain subsets. We report F1 as in [4], which checks if
the speciﬁc excluded object is mentioned in the generated
caption. To evaluate the quality of the generated caption, we
use SPICE, METEOR and CIDEr metrics and the scores on
out-of-domain test data are macro-averaged across eight ex-
cluded categories. For consistency with previous work [3],
the inverse document frequency statistics used by CIDEr are
determined across the entire test set.

As illustrated in Table 4.1, simply using greedy decod-
ing, our model (NBT∗+G) can successfully caption novel
concepts with minimum changes to the model. When us-
ing ResNet-101 and constrained beam search [2], our model
signiﬁcantly outperforms prior works under F1 scores,
SPICE, METEOR, and CIDEr, across both out-of-domain
and in-domain test data. Speciﬁcally, NBT†+T2 outper-

forms the previous state-of-art model C-LSTM by 14.6%
on average F1 scores. From the category F1 scores, we
can see that our model is less likely to select small objects,
e.g. “bottle”, “racket” when only using the greedy decod-
ing. Since the visual words are grounded at the object-level,
by using [2], our model was able to signiﬁcantly boost the
captioning performance on out-of-domain images. Fig. 7
shows qualitative novel object captioning results. Also see
rightmost example in Fig. 2.

5. Conclusion

In this paper, we introduce Neural Baby Talk, a novel im-
age captioning framework that produces natural language
explicitly grounded in entities object detectors ﬁnd in im-
ages. Our approach is a two-stage approach that ﬁrst gener-
ates a hybrid template that contains a mix of words from a
text vocabulary as well as slots corresponding to image re-
gions. It then ﬁlls the slots based on categories recognized
by object detectors in the image regions. We also introduce
a robust image captioning split by re-organizing the train
and val splits of the COCO dataset. Experimental results on
standard, robust, and novel object image captioning tasks
validate the effectiveness of our proposed approach.

Acknowledgements This work was funded in part by: NSF
CAREER awards to DB, DP; ONR YIP awards to DP, DB;
ONR Grants N00014-14-1-{0679,2713}; PGA Family Founda-
tion award to DP; Google FRAs to DP, DB; and Amazon ARAs to
DP, DB; DARPA XAI grant to DB, DP.

6. Appendix: COCO Fine-grained Categories

The COCO [26] dataset does not have bounding box annota-
tions associated with speciﬁc phrases or entities in the caption.
We use category level detection annotations and create a cate-
gory mapping list that maps the object categories like <Person>
to a list of potential ﬁne-grained labels like [“child”, “man”,
“baker”,...]. We ﬁrst use the Stanford lemmatization toolbox [30]
to get the base form of the entity words in the caption. For each
category class, we retrieve the top 200 similar words in the Word-
Vec [33] space. We then manually verify each word in the list,
resulting in 413 ﬁne-grained classes. A complete list of the ﬁne-
grained class for each object category can be found in Table 5 and
Table 6.

References

[1] P. Anderson, B. Fernando, M. Johnson, and S. Gould. Spice:
Semantic propositional image caption evaluation. In ECCV,
2016. 6

[2] P. Anderson, B. Fernando, M. Johnson, and S. Gould.
Guided open vocabulary image captioning with constrained
beam search. EMNLP, 2017. 3, 7, 8

[3] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,
S. Gould, and L. Zhang. Bottom-up and top-down atten-
tion for image captioning and visual question answering. In
CVPR, 2018. 3, 5, 6, 7, 8

[4] L. Anne Hendricks, S. Venugopalan, M. Rohrbach,
R. Mooney, K. Saenko, and T. Darrell. Deep composi-
tional captioning: Describing novel object categories with-
out paired training data. In CVPR, 2016. 3, 8

[5] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.
Zitnick, and D. Parikh. VQA: Visual Question Answering.
In ICCV, 2015. 1

[6] D. Chen and C. Manning. A fast and accurate dependency

parser using neural networks. In EMNLP, 2014. 5

[7] X. Chen and C. Lawrence Zitnick. Mind’s eye: A recurrent
visual representation for image caption generation. In CVPR,
2015. 3

[8] A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, and D. Ba-
tra. Human Attention in Visual Question Answering: Do
Humans and Deep Networks Look at the Same Regions? In
EMNLP, 2016. 1

[9] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M.
Moura, D. Parikh, and D. Batra. Visual Dialog. In CVPR,
2017. 1

[10] M. Denkowski and A. Lavie. Meteor universal: Language
speciﬁc translation evaluation for any target language.
In
EACL 2014 Workshop on Statistical Machine Translation,
2014. 6

[11] J. Donahue, L. Anne Hendricks,

S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual
recognition and description. In CVPR, 2015. 3

[12] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng,
P. Doll´ar, J. Gao, X. He, M. Mitchell, J. C. Platt, et al. From
captions to visual concepts and back. In CVPR, 2015. 1, 3

[13] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every pic-
ture tells a story: Generating sentences from images.
In
ECCV, 2010. 2, 3

[14] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask r-cnn.

ICCV, 2017. 5

[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 5

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997. 4

[17] R. Hu, M. Rohrbach,

and
K. Saenko. Modeling relationships in referential expres-
sions with compositional modular networks. arXiv preprint
arXiv:1611.09978, 2016. 3

J. Andreas, T. Darrell,

[18] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Dar-
rell. Natural language object retrieval. In CVPR, 2016. 3
[19] J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully
convolutional localization networks for dense captioning. In
CVPR, 2016. 3

[20] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
In CVPR, 2015.

ments for generating image descriptions.
1, 3, 6

[21] S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg.
Referitgame: Referring to objects in photographs of natural
scenes. In EMNLP, 2014. 3

[22] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014. 5

[23] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Multimodal

neural language models. In ICML, 2014. 3

[24] G. Kulkarni, V. Premraj, V. Ordonez, S. Dhar, S. Li, Y. Choi,
A. C. Berg, and T. L. Berg. Babytalk: Understanding and
generating simple image descriptions. In CVPR, 2011. 1, 2,
3

[25] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and
Y. Choi. Collective generation of natural image descriptions.
In ACL, 2012. 2, 3

[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 2, 4, 5, 6, 9
[27] J. Lu, C. Xiong, D. Parikh, and R. Socher. Knowing when
to look: Adaptive attention via a visual sentinel for image
captioning. In CVPR, 2017. 1, 3, 4, 6, 7

[28] R. Luo.

Unofﬁcial

for
sequence
caption-
for
https://github.com/ruotianluo/

implementation
image

self-critical
ing.
self-critical.pytorch, 2017. 7

training

pytorch

[29] R. Luo and G. Shakhnarovich. Comprehension-guided refer-

ring expressions. In CVPR, 2017. 3

[30] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J.
Bethard, and D. McClosky. The Stanford CoreNLP natural
language processing toolkit. In ACL, 2014. 5, 9

[31] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and
K. Murphy. Generation and comprehension of unambiguous
object descriptions. In CVPR, 2016. 3

[32] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille.
Deep captioning with multimodal recurrent neural networks
(m-rnn). In ICLR, 2015. 3

[33] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
preprint arXiv:1301.3781, 2013. 9

[34] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal,
A. Berg, K. Yamaguchi, T. Berg, K. Stratos,
and
H. Daum´e III. Midge: Generating image descriptions from
computer vision detections. In EACL, 2012. 2, 3

[35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a
method for automatic evaluation of machine translation. In
ACL, 2002. 6, 7

[36] J. Pennington, R. Socher, and C. Manning. Glove: Global
vectors for word representation. In EMNLP, 2014. 4
[37] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,
J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer image-
to-sentence models. In ICCV, 2015. 3, 4, 5

[38] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 3, 4, 5, 6

[39] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel.
Self-critical sequence training for image captioning.
In
CVPR, 2017. 3, 4, 6, 7

[40] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and
B. Schiele. Grounding of textual phrases in images by re-
construction. In ECCV, 2016. 3

[41] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 8

[42] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider:
In CVPR,

Consensus-based image description evaluation.
2015. 6, 7

[43] S. Venugopalan, L. A. Hendricks, M. Rohrbach, R. Mooney,
T. Darrell, and K. Saenko. Captioning images with diverse
objects. In CVPR, 2017. 3, 8

[44] O. Vinyals, M. Fortunato, and N. Jaitly. Pointer networks. In

NIPS, 2015. 4

[45] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and

tell: A neural image caption generator. In CVPR, 2015. 3

[46] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdi-
nov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural
In ICML,
image caption generation with visual attention.
2015. 1, 3, 6

[47] J. Yang, J. Lu, D. Batra, and D. Parikh. A faster pytorch
implementation of faster r-cnn. https://github.com/
jwyang/faster-rcnn.pytorch, 2017. 6

[48] Z. Yang, Y. Yuan, Y. Wu, R. Salakhutdinov, and W. W. Co-
hen. Encode, review, and decode: Reviewer module for cap-
tion generation. In NIPS, 2016. 1, 3
[49] T. Yao, Y. Pan, Y. Li, and T. Mei.

Incorporating copying
mechanism in image captioning for learning novel objects.
In CVPR, 2017. 3, 8

[50] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image cap-
tioning with semantic attention. In CVPR, 2016. 1, 3, 6
[51] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Mod-
eling context in referring expressions. In ECCV, 2016. 3

Object category

Fine-grained class

<person>

<bicycle>
<car>
<motorcycle>
<airplane>
<bus>
<train>
<truck>
<boat>

person, girl, boy, man, woman, kid, child, chef, baker, people, adult, rider, children, baby, worker, passenger, sister, biker, policeman,
ofﬁcer, lady, cowboy, bride, groom, male, female, guy, traveler, mother, father, gentleman, pitcher, player, skier, snowboarder,
skater, skateboarder, foreigner, caller, offender, coworker, trespasser, patient, politician, soldier, grandchild, serviceman, walker,
drinker, doctor, bicyclist, thief, buyer, teenager, student, camper, driver, solider, hunter, shopper, villager, cop
bicycle, bike, unicycle, minibike, trike
car, automobile, van, minivan, sedan, suv, hatchback, cab, jeep, coupe, taxicab, limo, taxi
motorcycle, scooter, motor bike, motor cycle, motorbike, moped
airplane, jetliner, plane, air plane, monoplane, aircraft, jet, airbus, biplane, seaplane bus, minibus, trolley
bus, minibus, schoolbus, trolley
train, locomotive, tramway, caboose
truck, pickup, lorry, hauler, ﬁretruck
boat, ship, liner, sailboat, motorboat, dinghy, powerboat, speedboat, canoe, skiff, yacht, kayak, catamaran, pontoon, houseboat, vessel,
rowboat, trawler, ferryboat, watercraft, tugboat, schooner, barge, ferry, sailboard, paddleboat, lifeboat, freighter, steamboat, riverboat,
surfboard, battleship, steamship
trafﬁc light, street light, trafﬁc signal, stop light, streetlight, stoplight
ﬁre hydrant, hydrant
stop sign, street sign

<trafﬁc light>
<ﬁre hydrant>
<stop sign>
<parking meter> parking meter
<bench>
<cat>
<dog>

<horse>
<sheep>
<cow>
<elephant>
<bear>
<zebra>
<giraffe>
<backpack>
<umbrella>
<handbag>
<tie>
<suitcase>
<frisbee>
<skis>
<snowboard>
<sports ball>
<kite>
<baseball bat>
<baseball glove> baseball glove
<skateboard>
<surfboard>
<tennis racket>
<bottle>

bench, pew
cat, kitten, feline, tabby
dog, puppy, beagle, pup, chihuahua, schnauzer, dachshund, rottweiler, canine, pitbull, collie, pug, terrier, poodle, labrador, doggie,
doberman, mutt, doggy, spaniel, bulldog, sheepdog, weimaraner, corgi, cocker, greyhound, retriever, brindle, hound, whippet, husky
horse, colt, pony, racehorse, stallion, equine, mare, foal, palomino, mustang, clydesdale, bronc, bronco
sheep, lamb, goat, ram, cattle, ewe
cow, cattle, oxen, ox, calf, ewe, holstein, heifer, buffalo, bull, zebu, bison
elephant
bear, panda
zebra
giraffe
backpack, knapsack
umbrella
handbag, handbag, wallet, purse, briefcase
tie
suitcase, suit case, luggage
frisbee
skis, ski
snowboard
sports ball, baseball, ball, football, soccer, basketball, softball, volleyball, pinball, fastball, racquetball
kite
baseball bat

skateboard
surfboard, longboard, skimboard, shortboard, wakeboard
tennis racket
bottle

Table 5. COCO category mapping list for visual words.

Object category

Fine-grained class

<wine glass>
<cup>
<fork>
<knife>
<spoon>
<bowl>
<banana>
<apple>
<sandwich>
<orange>
<broccoli>
<carrot>
<hot dog>
<pizza>
<donut>
<cake>
<bird>

<chair>
<couch>
<potted plant>
<bed>
<dining table>
<toilet>
<tv>
<laptop>
<mouse>
<remote>
<keyboard>
<cell phone>
<sink>
<refrigerator>
<book>
<clock>
<vase>
<scissors>
<teddy bear>
<hair drier>
<toothbrush>

wine glass
cup
fork
knife, pocketknife, knive
spoon
bowl, container, plate
banana
apple
sandwich, burger, sub, cheeseburger, hamburger
orange, lemons
broccoli
carrot
hot dog
pizza
donut, doughnut, bagel
cake, cheesecake, cupcake, shortcake, coffeecake, pancake
bird, ostrich, owl, seagull, goose, duck, parakeet, falcon, robin, pelican, waterfowl, heron, hummingbird, mallard, ﬁnch, pigeon, sparrow,
seabird, osprey, blackbird, fowl, shorebird, woodpecker, egret, chickadee, quail, bluebird, kingﬁsher, buzzard, willet, gull, swan, bluejay,
ﬂamingo, cormorant, parrot, loon, gosling, waterbird, pheasant, rooster, sandpiper, crow, raven, turkey, oriole, cowbird, warbler, magpie,
peacock, cockatiel, lorikeet, pufﬁn, vulture, condor, macaw, peafowl, cockatoo, songbird
chair, seat, recliner, stool
couch, sofa, recliner, futon, loveseat, settee, chesterﬁeld
potted plant, houseplant
bed
dining table, table
toilet, urinal, commode, lavatory, potty
tv, monitor, televison, television
laptop, computer, notebook, netbook, lenovo, macbook
mouse
remote
keyboard
cell phone, mobile phone, phone, cellphone, cellphone, telephone, phon, smartphone, iPhone
sink
refrigerator, fridge, refrigerator, fridge, freezer, refridgerator, frig
book
clock
vase
scissors
teddy bear, teddybear
hair drier, hairdryer
toothbrush

Table 6. COCO category mapping list for visual words (continued).

Neural Baby Talk

Jiasen Lu1∗

Jianwei Yang1∗ Dhruv Batra1,2 Devi Parikh1,2

1Georgia Institute of Technology

2Facebook AI Research

{jiasenlu, jw2yang, dbatra, parikh}@gatech.edu

8
1
0
2
 
r
a

M
 
7
2
 
 
]

V
C
.
s
c
[
 
 
1
v
5
4
8
9
0
.
3
0
8
1
:
v
i
X
r
a

Abstract

We introduce a novel framework for image captioning
that can produce natural language explicitly grounded in
entities that object detectors ﬁnd in the image. Our ap-
proach reconciles classical slot ﬁlling approaches (that are
generally better grounded in images) with modern neu-
ral captioning approaches (that are generally more natu-
ral sounding and accurate). Our approach ﬁrst generates
a sentence ‘template’ with slot locations explicitly tied to
speciﬁc image regions. These slots are then ﬁlled in by
visual concepts identiﬁed in the regions by object detec-
tors. The entire architecture (sentence template generation
and slot ﬁlling with object detectors) is end-to-end differen-
tiable. We verify the effectiveness of our proposed model
on different image captioning tasks. On standard image
captioning and novel object captioning, our model reaches
state-of-the-art on both COCO and Flickr30k datasets.
We also demonstrate that our model has unique advan-
tages when the train and test distributions of scene com-
positions – and hence language priors of associated cap-
tions – are different. Code has been made available at:
https://github.com/jiasenlu/NeuralBabyTalk.

1. Introduction

Image captioning is a challenging problem that lies at the
intersection of computer vision and natural language pro-
cessing. It involves generating a natural language sentence
that accurately summarizes the contents of an image. Im-
age captioning is also an important ﬁrst step towards real-
world applications with signiﬁcant practical impact, rang-
ing from aiding visually impaired users to personal assis-
tants to human-robot interaction [5, 9].

State-of-art image captioning models today tend to be
monolithic neural models, essentially of the “encoder-
decoder” paradigm. Images are encoded into a vector with
a convolutional neural network (CNN), and captions are de-
coded from this vector using a Recurrent Neural Network
(RNN), with the entire system trained end-to-end. While

Figure 1. Example captions generated by (a) Baby Talk [24], (c)
neural image captioning [20] and (b) our Neural Baby Talk ap-
proach. Our method generates the sentence “template” with slot
locations (illustrated with ﬁlled boxes) explicitly tied to image re-
gions (drawn in the image in corresponding colors). These slots
are then ﬁlled by object detectors with concepts found in regions.

there are many recent extensions of this basic idea to in-
clude attention [46, 12, 50, 48, 27], it is well-understood
that models still lack visual grounding (i.e., do not associate
named concepts to pixels in the image). They often tend to
‘look’ at different regions than humans would and tend to
copy captions from training data [8].

For instance, in Fig. 1 a neural image captioning ap-
proach [20] describes the image as “A dog is sitting on a
couch with a toy.” This is not quite accurate. But if one
were to really squint at the image, it (arguably) does per-
haps look like a scene where a dog could be sitting on a
couch with a toy. It certainly is common to ﬁnd dogs sitting
on couches with toys. A-priori, the description is reason-
able. That’s exactly what today’s neural captioning models
tend to do – produce generic plausible captions based on the
language model1 that match a ﬁrst-glance gist of the scene.
While this may sufﬁce for common scenes, images that dif-
fer from canonical scenes – given the diversity in our visual
world, there are plenty of such images – tend to be under-
served by these models.

If we take a step back – do we really need the language
model to do the heavy lifting in image captioning? Given

∗Equal contribution

1frequently, directly reproduced from a caption in the training data.

1

Figure 2. From left to right is the generated caption using the same captioning model but with different detectors: 1) No detector; 2) A
weak detector that only detects “person” and “sandwich”; 3) A detector trained on COCO [26] categories (including “teddy bear”). 4) A
detector that can detect novel concepts (e.g. “Mr. Ted” and “pie” that never occurred in the captioning training data). Different colors show
a correspondence between the visual word and grounding regions.

the unprecedented progress we are seeing in object recog-
nition2 (e.g., object detection, semantic segmentation, in-
stance segmentation, pose estimation), it seems like the vi-
sion pipeline can certainly do better than rely on just a ﬁrst-
glance gist of the scene. In fact, today’s state-of-the-art ob-
ject detectors can successfully detect the table and cake in
the image in Fig. 1(c)! The caption ought to be able to talk
about the table and cake actually detected as opposed to
letting the language model hallucinate a couch and a toy
simply because that sounds plausible.

Interestingly, some of the ﬁrst attempts at image caption-
ing [13, 24, 25, 34] – before the deep learning “revolution”
– relied heavily on outputs of object detectors and attribute
classiﬁers to describe images. For instance, consider the
output of Baby Talk [24] in Fig. 1, that used a slot ﬁlling
approach to talk about all the objects and attributes found in
the scene via a templated caption. The language is unnatu-
ral but the caption is very much grounded in what the model
sees in the image. Today’s approaches fall at the other ex-
treme on the spectrum – the language generated by modern
neural image captioning approaches is much more natural
but tends to be much less grounded in the image.

In this paper, we introduce Neural Baby Talk that recon-
ciles these methodologies. It produces natural language ex-
plicitly grounded in entities found by object detectors. It is
a neural approach that generates a sentence “template” with
slot locations explicitly tied to image regions. These slots
are then ﬁlled by object recognizers with concepts found in
the regions. The entire approach is trained end-to-end. This
results in natural sounding and grounded captions.

Our main technical contribution is a novel neural de-
coder for grounded image captioning. Speciﬁcally, at each
time step, the model decides whether to generate a word
from the textual vocabulary or generate a “visual” word.
The visual word is essentially a token that will hold the slot
for a word that is to describe a speciﬁc region in the image.
For instance, for the image in Fig. 1, the generated sequence

may be “A <region−17> is sitting at a <region−123>
with a <region−3>.” The visual words (<region−[.]>’s)
are then ﬁlled in during a second stage that classiﬁes
each of the indicated regions (e.g., <region−17>→puppy,
<region−123>→table), resulting in a ﬁnal description of
“A puppy is sitting at a table with a cake.” – a free-form
natural language description that is grounded in the image.
One nice feature of our model is that it allows for different
object detectors to be plugged in easily. As a result, a va-
riety of captions can be produced for the same image using
different detection backends. See Fig. 2 for an illustration.
Contributions: Our contributions are as follows:

• We present Neural Baby Talk – a novel framework for
visually grounded image captioning that explicitly lo-
calizes objects in the image while generating free-form
natural language descriptions.

• Ours is a two-stage approach that ﬁrst generates a hy-
brid template that contains a mix of (text) words and
slots explicitly associated with image regions, and then
ﬁlls in the slots with (text) words by recognizing the
content in the corresponding image regions.

• We propose a robust image captioning task to bench-
mark compositionality of image captioning algorithms
where at test time the model encounters images con-
taining known objects but in novel combinations (e.g.,
the model has seen dogs on couches and people at ta-
bles during training, but at test time encounters a dog
at a table). Generalizing to such novel compositions is
one way to demonstrate image grounding as opposed
to simply leveraging correlations from training data.
• Our proposed method achieves state-of-the-art perfor-
mance on COCO and Flickr30k datasets on the stan-
dard image captioning task, and signiﬁcantly outper-
forms existing approaches on the robust image cap-
tioning and novel object captioning tasks.

2. Related Work

2e.g., 11% absolute increase in average precision in object detection in

the COCO challenge in the last year.

Some of the earlier approaches generated templated im-
age captions via slot-ﬁlling. For instance, Kulkarni et

al. [24] detect objects, attributes, and prepositions, jointly
reason about these through a CRF, and ﬁnally ﬁll appropri-
ate slots in a template. Farhadi et al. [13] compute a triplet
for a scene, and use this templated “meaning” representa-
tion to retrieve a caption from a database. [25, 34] use more
powerful language templates such as a syntactically well-
formed tree. These approaches tend to either produce cap-
tions that are relevant to the image but not natural sounding,
or captions that are natural (e.g. retrieved from a database of
captions) but may not be sufﬁciently grounded in the image.
Neural models for image captioning have been receiv-
ing increased attention in the last few years [23, 32, 7, 45,
11, 20]. State-of-the-art neural approaches include atten-
tion mechanisms [46, 12, 50, 48, 27, 39, 3] that identify re-
gions in the image to “ground” emitted words. In practice,
these attention regions tend to be quite blurry, and rarely
correspond to semantically meaningful individual entities
(e.g., objects instances) in the image. Our approach grounds
words in object detections, which by design identify con-
crete semantic entities (object instances) in the image.

There has been some recent interest in grounding natu-
ral language in images. Dense Captioning [19] generates
descriptions for speciﬁc image regions.
In contrast, our
model produces captions for the entire image, with words
grounded in concrete entities in the image. Another related
line of work is on resolving referring expressions [21] (or
description-based object retrieval [37, 17, 18, 40] – given a
description of an object in the image, identify which ob-
ject is being referred to) or referring expression genera-
tion [21, 29, 31, 51] (given an object in the image, generate a
discriminative description of the object). While the interest
in grounded language is in common, our task is different.

One natural strength of our model is its ability to in-
corporate different object detectors, including the ability to
generate captions with novel objects (never seen before in
training captions). In that context, our work is related to
prior works on novel object captioning [4, 43, 49, 2]. As
we describe in Sec. 4.3, our method outperforms these ap-
proaches by 14.6% on the averaged F1 score.

3. Method

Given an image I, the goal of our method is to gener-
ate visually grounded descriptions y = {y1, . . . , yT }. Let
rI = {r1, ..., rN } be the set of N images regions extracted
from I. When generating an entity word in the caption, we
want to ground it in a speciﬁc image region r ∈ rI . Fol-
lowing the standard supervised learning paradigm, we learn
parameters θ of our model by maximizing the likelihood of
the correct caption:

θ∗ = arg max

θ

(cid:88)

log p(y|I; θ)

(1)

(I,y)
Using chain rule, the joint probability distribution can be

decomposed over a sequence of tokens:

p(y|I) =

p(yt|y1:t−1, I)

(2)

T
(cid:89)

t=1

where we drop the dependency on model parameters to
avoid notational clutter. We introduce a latent variable rt
to denote a speciﬁc image region so that yt can explicitly
ground in it. Thus the probability of yt is decomposed to:

p(yt|y1:t−1, I) = p(yt|rt, y1:t−1, I)p(rt|y1:t−1, I)

(3)

In our framework, yt can be of one of two types: a vi-
sual word or a textual word, denoted as yvis and ytxt re-
spectively. A visual word yvis is a type of word that is
grounded in a speciﬁc image region drawn from rI . A tex-
tual word ytxt is a word from the remainder of the caption.
It is drawn from the language model , which is associated
with a “default” sentinel “region” ˜r obtained from the lan-
guage model [27] (discussed in Sec. 3.1). For example, as
illustrated in Fig. 1, “puppy” and “cake” grounded in the
bounding box of category “dog” and “cake” respectively,
are visual words. While “with” and “sitting” are not associ-
ated with any image regions and thus are textual words.

With this, Eq. 1 can be decomposed into two cascaded
objectives. First, maximizing the probability of generating
the sentence “template”. A sequence of grounding regions
associated with the visual words interspersed with the tex-
tual words can be viewed as a sentence “template”, where
the grounding regions are slots to be ﬁlled in with visual
words.3 An example template (Fig. 3) is “A <region−2>
is laying on the <region−4> near a <region−7>. Sec-
ond, maximizing the probability of visual words yvis
con-
ditioned on the grounding regions and object detection in-
formation, e.g., categories recognized by detector. In the
template example above, the model will ﬁll the slots with
‘cat’, ‘laptop’ and ‘chair’ respectively.

t

In the following, we ﬁrst describe how we generate the
slotted caption template (Sec. 3.1), and then how the slots
are ﬁlled in to obtain the ﬁnal image description (Sec. 3.2).
The overall objective function is described in Sec. 3.3 and
the implementation details in Sec. 3.4.

3.1. “Slotted” Caption Template Generation

Given an image I, and the corresponding caption y, the
candidate grounding regions are obtained by using a pre-
trained Faster-RCNN network [38]. To generate the cap-
tion “template”, we use a recurrent neural network, which
is commonly used as the decoder for image captioning
[32, 45]. At each time step, we compute the RNN hidden
state ht according to the previous hidden state ht−1 and the
input xt such that ht = RNN(xt, ht−1). At training time,

3Our approach is not limited to any pre-speciﬁed bank of templates.
Rather, our approach automatically generates a template (with placehold-
ers – slots – for visually grounded words), which may be any one of the
exponentially many possible templates.

P t

r = softmax([ut; wT

h tanh(Wsst + Wzht)])

(9)
where Ws ∈ Rd×d and Wz ∈ Rd×d are the parameters.
Notably, Wz and wh are the same parameters as in Eq. 4.
P t
r is the probability distribution over grounding regions rI
and visual sentinel ˜r. The last element of the vector in Eq. 9
captures p(˜r|y1:t−1).

We feed the hidden state ht into a softmax layer to obtain
the probability over textual words conditioned on the image,
all previous words, and the visual sentinel:

P t

txt = softmax (Wqht)
(10)
where Wq ∈ RV ×d, d is hidden state size, and V is textual
vocabulary size. Plugging in Eq. 10 and p(˜r|y1:t−1) from
the last element of the vector in Eq. 9 into Eq. 6 gives us the
probability of generating a textual word in the template.

3.2. Caption Reﬁnement: Filling in The Slots

To ﬁll the slots in the generated template with visual
words grounded in image regions, we leverage the outputs
of an object detection network. Given a grounding region,
the category can be obtained through any detection frame-
work [38]. But outputs of detection networks are typically
singular coarse labels e.g. “dog”. Captions often refer to
these entities in a ﬁne-grained fashion e.g. “puppy” or in
the plural form “dogs”. In order to accommodate for these
linguistic variations, the visual word yvis in our model is
a reﬁnement of the category name by considering the fol-
lowing two factors: First, determine the plurality – whether
it should be singular or plural. Second, determine the ﬁne-
grained class (if any). Using two single layer MLPs with
ReLU activation f (·), we compute them with:

P t
b = softmax (Wbfb ([vt; ht]))
g = softmax (cid:0)U T Wgfg ([vt; ht])(cid:1)
P t

(11)

(12)

Wb ∈ R2×d, Wg ∈ R300×d are the weight parameters.
U ∈ R300×k is the glove vector embeddings [36] for k
ﬁne-grained words associated with the category name. The
visual word yvis
is then determined by plurality and ﬁne-
grained class (e.g., if plurality is plural, and the ﬁne-grained
class is “puppy”, the visual word will be “puppies”).

t

3.3. Objective

Most standard image captioning datasets (e.g. COCO
[26]) do not contain phrase grounding annotations, while
some datasets do (e.g. Flickr30k [37]). Our training objec-
tive (presented next) can incorporate different kinds of su-
pervision – be it strong annotations indicating which words
in the caption are grounded in which boxes in the image, or
weak supervision where objects are annotated in the image
but are not aligned to words in the caption. Given the tar-
get ground truth caption y∗
1:T and a image captioning model

Figure 3. One block of the proposed approach. Given an image,
proposals from any object detector and current word “A”, the ﬁgure
shows the process to predict the next visual word “cat”.

xt is the ground truth token (teacher forcing) and at test time
is the sampled token yt−1. Our decoder consists of an atten-
tion based LSTM layer [39] that takes convolution feature
maps as input. Details can be found in Sec. 3.4. To generate
the “slot” for visual words, we use a pointer network [44]
that modulates a content-based attention mechanism over
the grounding regions. Let vt ∈ Rd×1 be the region fea-
ture of rt, which is calculated based on Faster R-CNN. We
compute the pointing vector with:

h tanh(Wvvt + Wzht)

i = wT
ut
P t
= softmax(ut)
rI

(4)

(5)

Since textual words ytxt

where Wv ∈ Rm×d, Wz ∈ Rd×d and wh ∈ Rd×1 are pa-
rameters to be learned. The softmax normalizes the vector
ut to be a distribution over grounding regions rI .
t

are not tied to speciﬁc regions
in the image, inspired by [27], we add a “visual sentinel”
˜r as a latent variable to serve as dummy grounding for the
textual word. The visual sentinel can be thought of as a la-
tent representation of what the decoder already knows about
the image. The probability of a textual word ytxt

then is:

t

p(ytxt
t

|y1:t−1) = p(ytxt

t

|˜r, y1:t−1)p(˜r|y1:t−1)

(6)

where we drop the dependency on I to avoid clutter.

We ﬁrst describe how the visual sentinel is computed,
and then how the textual words are determined based on the
visual sentinel. Following [27], when the decoder RNN is
an LSTM [16], the representation for visual sentinel st can
be obtained by:

gt = σ (Wxxt + Whht−1)
st = gt (cid:12) tanh (ct)

(7)

(8)

where Wx ∈ Rd×d, Wh ∈ Rd×d. xt is the LSTM input
at time step t, and gt is the gate applied on the cell state
ct. (cid:12) represents element-wise product, σ the logistic sig-
moid activation. Modifying Eq. 5, the probability over the
grounding regions including the visual sentinel is:

with parameters θ, we minimize the cross entropy loss:

L(θ) = −

T
(cid:88)

t=1

(cid:16)

(cid:122)
p(y∗

t |˜r, y∗

log

Textual word probability
(cid:125)(cid:124)
1:t−1)p(˜r|y∗

1:t−1)1(y∗

(cid:123)
t =ytxt) +

p (cid:0)b∗
(cid:124)

t , s∗

t |rt, y∗
(cid:123)(cid:122)
Caption reﬁnement

1:t−1

(cid:1)

(cid:125)

m
(cid:88)

i=1

(cid:0) 1
m
(cid:124)

p (cid:0)ri

t|y∗

1:t−1

(cid:1) (cid:1)1(y∗

t =yvis)

(cid:17)

(cid:123)(cid:122)
Averaged target region probability

(cid:125)

(13)
where y∗
t is the word from the ground truth caption at time
t. 1(y∗
t =ytxt) is the indicator function which equals to 1 if
y∗
t is textual word and 0 otherwise. b∗
t are the target
ground truth plurality and ﬁnd-grained class. {ri
i=1 ∈ rI
are the target grounding regions of the visual word at time
t. We maximize the averaged log probability of the target
grounding regions.

t and s∗

t}m

t}m

Visual word extraction. During training, visual words
in a caption are dynamically identiﬁed by matching the base
form of each word (using the Stanford lemmatization tool-
box [30]) against a vocabulary of visual words (details of
how to get visual word can be found in dataset Sec. 4). The
grounding regions {ri
i=1 for a visual word yt is identiﬁed
by computing the IoU of all boxes detected by the object
detection network with the ground truth bounding box as-
sociated with the category corresponding to yt. If the score
exceeds a threshold of 0.5 and the grounding region label
matches the visual word, the bounding boxes are selected
as the grounding regions. E.g., given a target visual word
“cat”, if there are no proposals that match the target bound-
ing box, the model predicts the textual word “cat” instead.

3.4. Implementation Details

Detection model. We use Faster R-CNN [38] with
ResNet-101 [15] to obtain region proposals for the image.
We use an IoU threshold of 0.7 for region proposal sup-
pression and 0.3 for class suppressions. A class detection
conﬁdence threshold of 0.5 is used to select regions.

i ; vl

i; vg

Region feature. We use a pre-trained ResNet-101 [15]
in our model. The image is ﬁrst resized to 576×576 and we
random crop 512 × 512 as the input to the CNN network.
Given proposals from the pre-trained detection model, the
feature vi for region i is a concatenation of 3 different fea-
tures vi = [vp
i ] where vp
i is the pooling feature of
RoI align layer [14] given the proposal coordinates, vl
i is
the location feature and vg
i is the glove vector embedding of
the class label for region i. Let xmin, ymin, xmax, ymax be the
bounding box coordinates of the region b; WI and HI be the
width and height of the image I. Then the location feature
vl
i can be obtained by projecting the normalized location
xmin
[
WI

] into another embedding space.

xmax
WI

ymax
HI

ymin
HI

Language model. We use an attention model with two
LSTM layers [3] as our base attention model. Given N re-

,

,

,

Figure 4. Language model used in our approach.

gion features from detection proposals V = {v1, . . . , vN }
and CNN features from the last convolution layer at K grids
ˆV = {ˆv1, . . . , ˆvK}, the language model has two separate
attention layers shown in Fig 4. The attention distribution
over the image features for detection proposals is:
z tanh (cid:0)WvV + (Wght)1T (cid:1)

(14)

zt = wT
αt = softmax(zt)

where Wv ∈ Rm×d, Wg ∈ Rd×d and w ∈ Rd×1. 1 ∈ RN
is a vector with all elements set to 1. αt is the attention
weight over N image location features.

Training details. In our experiments, we use a two layer
LSTM with hidden size 1024. The number of hidden units
in the attention layer and the size of the input word em-
bedding are 512. We use the Adam [22] optimizer with an
initial learning rate of 5 × 10−4 and anneal the learning rate
by a factor of 0.8 every three epochs. We train the model
up to 50 epochs with early stopping. Note that we do not
ﬁnetune the CNN network during training. We set the batch
size to be 100 for COCO [26] and 50 for Flickr30k [37].

4. Experimental Results

Datasets. We experiment with two datasets. Flickr30k
Entities [37] contains 275,755 bounding boxes from 31,783
images associated with natural language phrases. Each im-
age is annotated with 5 crowdsourced captions. For each
annotated phrase in the caption, we identify visual words
by selecting the inner most NP (noun phrase) tag from
the Stanford part-of-speech tagger [6]. We use Stanford
Lemmatization Toolbox [30] to get the base form of the en-
tity words resulting in 2,567 unique words.

COCO [26] contains 82,783, 40,504 and 40,775 images
for training, validation and testing respectively. Each im-
age has around 5 crowdsourced captions. Unlike Flickr30k
Entities, COCO does not have bounding box annotations
associated with speciﬁc phrases or entities in the caption.
To identify visual words, we manually constructed an ob-
ject category to word mapping that maps object categories
like <person> to a list of potential ﬁne-grained labels like
[“child”, “baker”, ...]. This results in 80 categories with a
total of 413 ﬁne-grained classes. See supp. for details.

Figure 5. Generated captions and corresponding visual grounding regions on the standard image captioning task (Top: COCO, Bottom:
Flickr30k). Different colors show a correspondence between the visual words and grounding regions. Grey regions are the proposals not
selected in the caption. First 3 columns show success and last column shows failure cases (words are grounded in the wrong region).

Method

BLEU1 BLEU4 METEOR CIDEr SPICE

Method

BLEU1 BLEU4 METEOR CIDEr SPICE

Hard-Attention [46]
ATT-FCN [50]
Adaptive [27]

NBT
NBToracle

66.9
64.7
67.7

69.0
72.0

19.9
23.0
25.1

27.1
28.5

18.5
18.9
20.4

21.7
23.1

-
-
53.1

57.5
64.8

-
-
14.5

15.6
19.6

Table 1. Performance on the test portion of Karpathy et al. [20]’s
splits on Flickr30k Entities dataset.

Adaptive [27]
Att2in [39]
Up-Down [3]

Att2in∗ [39]
Up-Down† [3]

NBT
NBToracle

74.2
-
74.5

-
79.8

75.5
75.9

32.5
31.3
33.4

33.3
36.3

34.7
34.9

26.6
26.0
26.1

26.3
27.7

27.1
27.4

108.5
101.3
105.4

111.4
120.1

107.2
108.9

19.5
-
19.2

-
21.4

20.1
20.4

Detector pre-training. We use open an source imple-
mentation [47] of Faster-RCNN [38] to train the detector.
For Flickr30K Entities, we use visual words that occur at
least 100 times as detection labels, resulting in a total of 460
detection labels. Since detection labels and visual words
have a one-to-one mapping, we do not have ﬁne-grained
classes for the Flickr30K Entities dataset – the caption re-
ﬁnement process only determines the plurality of detection
labels. For COCO, ground truth detection annotations are
used to train the object detector.

Caption pre-processing. We truncate captions longer
than 16 words for both COCO and Flickr30k Entities
dataset. We then build a vocabulary of words that occur at
least 5 times in the training set, resulting in 9,587 and 6,864
words for COCO and Flickr30k Entities, respectively.

4.1. Standard Image Captioning

For standard image captioning, we use splits from
Karpathy et al. [20] on COCO/Flickr30k. We report re-
sults using the COCO captioning evaluation toolkit [26],
which reports the widely used automatic evaluation metrics,
BLEU [35], METEOR [10], CIDEr [42] and SPICE [1].

Table 2. Performance on the test portion of Karpathy et al. [20]’s
splits on COCO dataset. ∗ directly optimizes the CIDEr Metric, †
uses better image features, and are thus not directly comparable.

We present our methods trained on different object de-
tectors: Flickr and COCO. We compare our approach (re-
ferred to as NBT) to recently proposed Hard-Attention [46],
ATT-FCN [50] and Adaptive [27] on Flickr30k, and Att2in
[39], Up-Down [3] on COCO. Since object detectors have
not yet achieved near-perfect accuracies on these datasets,
we also report the performance of our model under an oracle
setting, where the ground truth object region and category
is also provided during test time. (referred to as NBToracle)
This can be viewed as the upper bound of our method when
we have perfect object detectors.

Table 1 shows results on the Flickr30k dataset. We see
that our method achieves state of the art on all automatic
evaluation metrics, outperforming the previous state-of-art
model Adaptive [27] by 2.0 and 4.4 on BLEU4 and CIDEr.
When using ground truth proposals, NBToracle signiﬁcantly
outperforms previous methods, improving 5.1 on SPICE,
which implies that our method could further beneﬁt from
improved object detectors.

Figure 6. Generated captions and corresponding visual grounding regions for the robust image captioning task. “cat-remote”, “man-bird”,
“dog-skateboard” and “orange-bird” are co-occurring categories excluded in the training split. First 3 columns show success and last
column shows failure case (orange was not mentioned).

Method

BLEU4 METEOR CIDEr SPICE Accuracy

Att2in [39]
Up-Down [3]

NBT
NBToracle

31.5
31.6

31.7
31.9

24.6
25.0

25.2
25.5

90.6
92.0

94.1
95.5

17.7
18.1

18.3
18.7

39.0
39.7

42.4
45.7

Table 3. Performance on the test portion of the robust image cap-
tioning split on COCO dataset.

Table 2 shows results on the COCO dataset. Our method
outperforms 4 out of 5 automatic evaluation metrics com-
pared to the state of the art [39, 27, 3] without using better
visual features or directly optimizing the CIDEr metric. In-
terestingly, the NBToracle has little improvement over NBT.
We suspect the reason is that explicit ground truth anno-
tation is absent for visual words. Our model can be fur-
ther improved with explicit co-reference supervision where
the ground truth location annotation of the visual word is
provided. Fig. 5 shows qualitative results on both datasets.
We see that our model learns to correctly identify the visual
word, and ground it in image regions even under weak su-
pervision (COCO). Our model is also robust to erroneous
detections and produces correct captions (3rd column).

4.2. Robust Image Captioning

To quantitatively evaluate image captioning models for
novel scene compositions, we present a new split of the
COCO dataset, called the robust-COCO split. This new
split is created by re-organizing the train and val splits of
the COCO dataset such that the distribution of co-occurring
objects in train is different from test. We also present a new
metric to evaluate grounding.

Robust split. To create the new split, we ﬁrst identify
entity words that belong to the 80 COCO object categories
by following the same pre-processing procedure. For each
image, we get a list of object categories that are mentioned
in the caption. We then calculate the co-occurrence statis-
tics for these 80 object categories. Starting from the least
co-occurring category pairs, we greedily add them to the
test set and ensure that for each category, at least half the
instances of each category are in the train set. As a re-

sult, there are sufﬁcient examples from each category in
train, but at test time we see novel compositions (pairs)
of categories. Remaining images are assigned to the train-
ing set. The ﬁnal split has 110,234/3,915/9,138 images in
train/val/test respectively.

Evaluation metric. To evaluate visual grounding on the
robust-COCO split, we want a metric that indicates whether
or not a generated caption includes the new object combina-
tion. Common automatic evaluation metrics such as BLEU
[35] and CIDEr [42] measure the overall sentence ﬂuency.
We also measure whether the generated caption contains the
novel co-occurring categories that exist in the ground truth
caption. A generated caption is deemed 100% accurate if it
contains at least one mention of the compositionally novel
category-pairs in any ground truth annotation that describe
the image.

Results and analysis. We compare our method with
state of the art Att2in [39] and Up-Down [3]. These are
implemented using the open source implementation from
[28] that can replicate results on Karpathy’s split. We fol-
low the experimental setting from [39] and train the model
using the robust-COCO train set. Table 3 shows the results
on the robust-COCO split. As we can see, all models per-
form worse on the robust-COCO split than the Karpathy’s
split by 2∼3 points in general. Our method outperforms
the previous state of the art methods on all metrics, outper-
forming Up-Down [3] by 2.7 on the proposed metric. The
oracle setting (NBToracle) has consistent improvements on
all metrics, improving 3.3 on the proposed metric.

Fig. 6 shows qualitative results on the robust image cap-
tioning task. Our model successfully produces a caption
with novel compositions, such as “cat-remote”, “man-bird”
and “dog-skateboard” to describe the image. The last col-
umn shows failure cases where our model didn’t select “or-
ange” in the caption. We can force our model to produce
a caption containing “orange” and “bird” using constrained
beam search [2], further illustrated in Sec. 4.3.

4.3. Novel Object Captioning

Since our model directly ﬁlls the “slotted” caption tem-
plate with the concept, it can seamlessly generate descrip-

Figure 7. Generated captions and corresponding visual grounding regions for the novel object captioning task. “zebra”, “tennis racket”,
“bus” and “pizza” are categories excluded in the training split. First 3 columns show success and last column shows a failure case.

Out-of-Domain Test Data

In-Domain Test Data

Method

bottle

bus

couch microwave pizza racket

suitcase zebra Avg SPICE METEOR CIDEr SPICE METEOR CIDER

DCC [4]
NOC [43]
C-LSTM [49]
Base+T4 [2]

NBT∗+G
NBT†+G
NBT†+T1
NBT†+T2

4.6
17.8
29.7
16.3

7.1
14.0
36.2
38.3

29.8
68.8
74.4
67.8

73.7
74.8
77.7
80.0

45.9
25.6
38.8
48.2

34.4
42.8
43.9
54.0

28.1
24.7
27.8
29.7

61.9
63.7
65.8
70.3

64.6
69.3
68.2
77.2

59.9
74.4
70.3
81.1

52.2
68.1
70.3
57.1

20.2
19.0
19.8
74.8

13.2
39.9
44.8
49.9

42.3
44.5
51.2
67.8

79.9
89.0
91.4
85.7

88.5
92.0
93.7
96.6

39.8
49.1
55.7
54.0

48.5
53.2
57.3
70.3

13.4
-
-
15.9

15.7
16.6
16.7
17.4

21.0
21.4
23.0
23.3

22.8
23.9
23.9
24.1

59.1
-
-
77.9

77.0
84.0
85.7
86.0

15.9
-
-
18.0

17.5
18.4
18.4
18.0

23.0
-
-
24.5

24.3
25.3
25.5
25.0

77.2
-
-
86.3

87.4
94.0
95.2
92.1

Table 4. Evaluation of captions generated using the proposed method. G means greedy decoding, and T1−2 means using constrained beam
search [2] with 1−2 top detected concepts. ∗ is the result using VGG-16 [41] and † is the result using ResNet-101.

tions for out-of-domain images. We replicated an existing
experimental design [4] on COCO which excludes all the
image-sentence pairs that contain at least one of eight ob-
jects in COCO. The excluded objects are ‘bottle’, “bus”,
“couch”, “microwave”, “pizza”, “racket”, “suitcase” and
“zebra”. We follow the same splits for training, valida-
tion, and testing as in prior work [4]. We use Faster R-
CNN in conjunction with ResNet-101 which is pre-trained
on COCO train split as the detection model. Note that we
do not pre-train the language model using COCO captions
as in [4, 43, 49], and simply replace the novel object’s word
embedding with an existing one which belongs to the same
super-category in COCO (e.g., bus ← car).

Following [2], the test set is split into in-domain and out-
of-domain subsets. We report F1 as in [4], which checks if
the speciﬁc excluded object is mentioned in the generated
caption. To evaluate the quality of the generated caption, we
use SPICE, METEOR and CIDEr metrics and the scores on
out-of-domain test data are macro-averaged across eight ex-
cluded categories. For consistency with previous work [3],
the inverse document frequency statistics used by CIDEr are
determined across the entire test set.

As illustrated in Table 4.1, simply using greedy decod-
ing, our model (NBT∗+G) can successfully caption novel
concepts with minimum changes to the model. When us-
ing ResNet-101 and constrained beam search [2], our model
signiﬁcantly outperforms prior works under F1 scores,
SPICE, METEOR, and CIDEr, across both out-of-domain
and in-domain test data. Speciﬁcally, NBT†+T2 outper-

forms the previous state-of-art model C-LSTM by 14.6%
on average F1 scores. From the category F1 scores, we
can see that our model is less likely to select small objects,
e.g. “bottle”, “racket” when only using the greedy decod-
ing. Since the visual words are grounded at the object-level,
by using [2], our model was able to signiﬁcantly boost the
captioning performance on out-of-domain images. Fig. 7
shows qualitative novel object captioning results. Also see
rightmost example in Fig. 2.

5. Conclusion

In this paper, we introduce Neural Baby Talk, a novel im-
age captioning framework that produces natural language
explicitly grounded in entities object detectors ﬁnd in im-
ages. Our approach is a two-stage approach that ﬁrst gener-
ates a hybrid template that contains a mix of words from a
text vocabulary as well as slots corresponding to image re-
gions. It then ﬁlls the slots based on categories recognized
by object detectors in the image regions. We also introduce
a robust image captioning split by re-organizing the train
and val splits of the COCO dataset. Experimental results on
standard, robust, and novel object image captioning tasks
validate the effectiveness of our proposed approach.

Acknowledgements This work was funded in part by: NSF
CAREER awards to DB, DP; ONR YIP awards to DP, DB;
ONR Grants N00014-14-1-{0679,2713}; PGA Family Founda-
tion award to DP; Google FRAs to DP, DB; and Amazon ARAs to
DP, DB; DARPA XAI grant to DB, DP.

6. Appendix: COCO Fine-grained Categories

The COCO [26] dataset does not have bounding box annota-
tions associated with speciﬁc phrases or entities in the caption.
We use category level detection annotations and create a cate-
gory mapping list that maps the object categories like <Person>
to a list of potential ﬁne-grained labels like [“child”, “man”,
“baker”,...]. We ﬁrst use the Stanford lemmatization toolbox [30]
to get the base form of the entity words in the caption. For each
category class, we retrieve the top 200 similar words in the Word-
Vec [33] space. We then manually verify each word in the list,
resulting in 413 ﬁne-grained classes. A complete list of the ﬁne-
grained class for each object category can be found in Table 5 and
Table 6.

References

[1] P. Anderson, B. Fernando, M. Johnson, and S. Gould. Spice:
Semantic propositional image caption evaluation. In ECCV,
2016. 6

[2] P. Anderson, B. Fernando, M. Johnson, and S. Gould.
Guided open vocabulary image captioning with constrained
beam search. EMNLP, 2017. 3, 7, 8

[3] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,
S. Gould, and L. Zhang. Bottom-up and top-down atten-
tion for image captioning and visual question answering. In
CVPR, 2018. 3, 5, 6, 7, 8

[4] L. Anne Hendricks, S. Venugopalan, M. Rohrbach,
R. Mooney, K. Saenko, and T. Darrell. Deep composi-
tional captioning: Describing novel object categories with-
out paired training data. In CVPR, 2016. 3, 8

[5] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.
Zitnick, and D. Parikh. VQA: Visual Question Answering.
In ICCV, 2015. 1

[6] D. Chen and C. Manning. A fast and accurate dependency

parser using neural networks. In EMNLP, 2014. 5

[7] X. Chen and C. Lawrence Zitnick. Mind’s eye: A recurrent
visual representation for image caption generation. In CVPR,
2015. 3

[8] A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, and D. Ba-
tra. Human Attention in Visual Question Answering: Do
Humans and Deep Networks Look at the Same Regions? In
EMNLP, 2016. 1

[9] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M.
Moura, D. Parikh, and D. Batra. Visual Dialog. In CVPR,
2017. 1

[10] M. Denkowski and A. Lavie. Meteor universal: Language
speciﬁc translation evaluation for any target language.
In
EACL 2014 Workshop on Statistical Machine Translation,
2014. 6

[11] J. Donahue, L. Anne Hendricks,

S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual
recognition and description. In CVPR, 2015. 3

[12] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng,
P. Doll´ar, J. Gao, X. He, M. Mitchell, J. C. Platt, et al. From
captions to visual concepts and back. In CVPR, 2015. 1, 3

[13] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every pic-
ture tells a story: Generating sentences from images.
In
ECCV, 2010. 2, 3

[14] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask r-cnn.

ICCV, 2017. 5

[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 5

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997. 4

[17] R. Hu, M. Rohrbach,

and
K. Saenko. Modeling relationships in referential expres-
sions with compositional modular networks. arXiv preprint
arXiv:1611.09978, 2016. 3

J. Andreas, T. Darrell,

[18] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Dar-
rell. Natural language object retrieval. In CVPR, 2016. 3
[19] J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully
convolutional localization networks for dense captioning. In
CVPR, 2016. 3

[20] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
In CVPR, 2015.

ments for generating image descriptions.
1, 3, 6

[21] S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg.
Referitgame: Referring to objects in photographs of natural
scenes. In EMNLP, 2014. 3

[22] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014. 5

[23] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Multimodal

neural language models. In ICML, 2014. 3

[24] G. Kulkarni, V. Premraj, V. Ordonez, S. Dhar, S. Li, Y. Choi,
A. C. Berg, and T. L. Berg. Babytalk: Understanding and
generating simple image descriptions. In CVPR, 2011. 1, 2,
3

[25] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and
Y. Choi. Collective generation of natural image descriptions.
In ACL, 2012. 2, 3

[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 2, 4, 5, 6, 9
[27] J. Lu, C. Xiong, D. Parikh, and R. Socher. Knowing when
to look: Adaptive attention via a visual sentinel for image
captioning. In CVPR, 2017. 1, 3, 4, 6, 7

[28] R. Luo.

Unofﬁcial

for
sequence
caption-
for
https://github.com/ruotianluo/

implementation
image

self-critical
ing.
self-critical.pytorch, 2017. 7

training

pytorch

[29] R. Luo and G. Shakhnarovich. Comprehension-guided refer-

ring expressions. In CVPR, 2017. 3

[30] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J.
Bethard, and D. McClosky. The Stanford CoreNLP natural
language processing toolkit. In ACL, 2014. 5, 9

[31] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and
K. Murphy. Generation and comprehension of unambiguous
object descriptions. In CVPR, 2016. 3

[32] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille.
Deep captioning with multimodal recurrent neural networks
(m-rnn). In ICLR, 2015. 3

[33] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
preprint arXiv:1301.3781, 2013. 9

[34] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal,
A. Berg, K. Yamaguchi, T. Berg, K. Stratos,
and
H. Daum´e III. Midge: Generating image descriptions from
computer vision detections. In EACL, 2012. 2, 3

[35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a
method for automatic evaluation of machine translation. In
ACL, 2002. 6, 7

[36] J. Pennington, R. Socher, and C. Manning. Glove: Global
vectors for word representation. In EMNLP, 2014. 4
[37] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,
J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer image-
to-sentence models. In ICCV, 2015. 3, 4, 5

[38] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 3, 4, 5, 6

[39] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel.
Self-critical sequence training for image captioning.
In
CVPR, 2017. 3, 4, 6, 7

[40] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and
B. Schiele. Grounding of textual phrases in images by re-
construction. In ECCV, 2016. 3

[41] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 8

[42] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider:
In CVPR,

Consensus-based image description evaluation.
2015. 6, 7

[43] S. Venugopalan, L. A. Hendricks, M. Rohrbach, R. Mooney,
T. Darrell, and K. Saenko. Captioning images with diverse
objects. In CVPR, 2017. 3, 8

[44] O. Vinyals, M. Fortunato, and N. Jaitly. Pointer networks. In

NIPS, 2015. 4

[45] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and

tell: A neural image caption generator. In CVPR, 2015. 3

[46] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdi-
nov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural
In ICML,
image caption generation with visual attention.
2015. 1, 3, 6

[47] J. Yang, J. Lu, D. Batra, and D. Parikh. A faster pytorch
implementation of faster r-cnn. https://github.com/
jwyang/faster-rcnn.pytorch, 2017. 6

[48] Z. Yang, Y. Yuan, Y. Wu, R. Salakhutdinov, and W. W. Co-
hen. Encode, review, and decode: Reviewer module for cap-
tion generation. In NIPS, 2016. 1, 3
[49] T. Yao, Y. Pan, Y. Li, and T. Mei.

Incorporating copying
mechanism in image captioning for learning novel objects.
In CVPR, 2017. 3, 8

[50] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image cap-
tioning with semantic attention. In CVPR, 2016. 1, 3, 6
[51] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Mod-
eling context in referring expressions. In ECCV, 2016. 3

Object category

Fine-grained class

<person>

<bicycle>
<car>
<motorcycle>
<airplane>
<bus>
<train>
<truck>
<boat>

person, girl, boy, man, woman, kid, child, chef, baker, people, adult, rider, children, baby, worker, passenger, sister, biker, policeman,
ofﬁcer, lady, cowboy, bride, groom, male, female, guy, traveler, mother, father, gentleman, pitcher, player, skier, snowboarder,
skater, skateboarder, foreigner, caller, offender, coworker, trespasser, patient, politician, soldier, grandchild, serviceman, walker,
drinker, doctor, bicyclist, thief, buyer, teenager, student, camper, driver, solider, hunter, shopper, villager, cop
bicycle, bike, unicycle, minibike, trike
car, automobile, van, minivan, sedan, suv, hatchback, cab, jeep, coupe, taxicab, limo, taxi
motorcycle, scooter, motor bike, motor cycle, motorbike, moped
airplane, jetliner, plane, air plane, monoplane, aircraft, jet, airbus, biplane, seaplane bus, minibus, trolley
bus, minibus, schoolbus, trolley
train, locomotive, tramway, caboose
truck, pickup, lorry, hauler, ﬁretruck
boat, ship, liner, sailboat, motorboat, dinghy, powerboat, speedboat, canoe, skiff, yacht, kayak, catamaran, pontoon, houseboat, vessel,
rowboat, trawler, ferryboat, watercraft, tugboat, schooner, barge, ferry, sailboard, paddleboat, lifeboat, freighter, steamboat, riverboat,
surfboard, battleship, steamship
trafﬁc light, street light, trafﬁc signal, stop light, streetlight, stoplight
ﬁre hydrant, hydrant
stop sign, street sign

<trafﬁc light>
<ﬁre hydrant>
<stop sign>
<parking meter> parking meter
<bench>
<cat>
<dog>

<horse>
<sheep>
<cow>
<elephant>
<bear>
<zebra>
<giraffe>
<backpack>
<umbrella>
<handbag>
<tie>
<suitcase>
<frisbee>
<skis>
<snowboard>
<sports ball>
<kite>
<baseball bat>
<baseball glove> baseball glove
<skateboard>
<surfboard>
<tennis racket>
<bottle>

bench, pew
cat, kitten, feline, tabby
dog, puppy, beagle, pup, chihuahua, schnauzer, dachshund, rottweiler, canine, pitbull, collie, pug, terrier, poodle, labrador, doggie,
doberman, mutt, doggy, spaniel, bulldog, sheepdog, weimaraner, corgi, cocker, greyhound, retriever, brindle, hound, whippet, husky
horse, colt, pony, racehorse, stallion, equine, mare, foal, palomino, mustang, clydesdale, bronc, bronco
sheep, lamb, goat, ram, cattle, ewe
cow, cattle, oxen, ox, calf, ewe, holstein, heifer, buffalo, bull, zebu, bison
elephant
bear, panda
zebra
giraffe
backpack, knapsack
umbrella
handbag, handbag, wallet, purse, briefcase
tie
suitcase, suit case, luggage
frisbee
skis, ski
snowboard
sports ball, baseball, ball, football, soccer, basketball, softball, volleyball, pinball, fastball, racquetball
kite
baseball bat

skateboard
surfboard, longboard, skimboard, shortboard, wakeboard
tennis racket
bottle

Table 5. COCO category mapping list for visual words.

Object category

Fine-grained class

<wine glass>
<cup>
<fork>
<knife>
<spoon>
<bowl>
<banana>
<apple>
<sandwich>
<orange>
<broccoli>
<carrot>
<hot dog>
<pizza>
<donut>
<cake>
<bird>

<chair>
<couch>
<potted plant>
<bed>
<dining table>
<toilet>
<tv>
<laptop>
<mouse>
<remote>
<keyboard>
<cell phone>
<sink>
<refrigerator>
<book>
<clock>
<vase>
<scissors>
<teddy bear>
<hair drier>
<toothbrush>

wine glass
cup
fork
knife, pocketknife, knive
spoon
bowl, container, plate
banana
apple
sandwich, burger, sub, cheeseburger, hamburger
orange, lemons
broccoli
carrot
hot dog
pizza
donut, doughnut, bagel
cake, cheesecake, cupcake, shortcake, coffeecake, pancake
bird, ostrich, owl, seagull, goose, duck, parakeet, falcon, robin, pelican, waterfowl, heron, hummingbird, mallard, ﬁnch, pigeon, sparrow,
seabird, osprey, blackbird, fowl, shorebird, woodpecker, egret, chickadee, quail, bluebird, kingﬁsher, buzzard, willet, gull, swan, bluejay,
ﬂamingo, cormorant, parrot, loon, gosling, waterbird, pheasant, rooster, sandpiper, crow, raven, turkey, oriole, cowbird, warbler, magpie,
peacock, cockatiel, lorikeet, pufﬁn, vulture, condor, macaw, peafowl, cockatoo, songbird
chair, seat, recliner, stool
couch, sofa, recliner, futon, loveseat, settee, chesterﬁeld
potted plant, houseplant
bed
dining table, table
toilet, urinal, commode, lavatory, potty
tv, monitor, televison, television
laptop, computer, notebook, netbook, lenovo, macbook
mouse
remote
keyboard
cell phone, mobile phone, phone, cellphone, cellphone, telephone, phon, smartphone, iPhone
sink
refrigerator, fridge, refrigerator, fridge, freezer, refridgerator, frig
book
clock
vase
scissors
teddy bear, teddybear
hair drier, hairdryer
toothbrush

Table 6. COCO category mapping list for visual words (continued).

Neural Baby Talk

Jiasen Lu1∗

Jianwei Yang1∗ Dhruv Batra1,2 Devi Parikh1,2

1Georgia Institute of Technology

2Facebook AI Research

{jiasenlu, jw2yang, dbatra, parikh}@gatech.edu

8
1
0
2
 
r
a

M
 
7
2
 
 
]

V
C
.
s
c
[
 
 
1
v
5
4
8
9
0
.
3
0
8
1
:
v
i
X
r
a

Abstract

We introduce a novel framework for image captioning
that can produce natural language explicitly grounded in
entities that object detectors ﬁnd in the image. Our ap-
proach reconciles classical slot ﬁlling approaches (that are
generally better grounded in images) with modern neu-
ral captioning approaches (that are generally more natu-
ral sounding and accurate). Our approach ﬁrst generates
a sentence ‘template’ with slot locations explicitly tied to
speciﬁc image regions. These slots are then ﬁlled in by
visual concepts identiﬁed in the regions by object detec-
tors. The entire architecture (sentence template generation
and slot ﬁlling with object detectors) is end-to-end differen-
tiable. We verify the effectiveness of our proposed model
on different image captioning tasks. On standard image
captioning and novel object captioning, our model reaches
state-of-the-art on both COCO and Flickr30k datasets.
We also demonstrate that our model has unique advan-
tages when the train and test distributions of scene com-
positions – and hence language priors of associated cap-
tions – are different. Code has been made available at:
https://github.com/jiasenlu/NeuralBabyTalk.

1. Introduction

Image captioning is a challenging problem that lies at the
intersection of computer vision and natural language pro-
cessing. It involves generating a natural language sentence
that accurately summarizes the contents of an image. Im-
age captioning is also an important ﬁrst step towards real-
world applications with signiﬁcant practical impact, rang-
ing from aiding visually impaired users to personal assis-
tants to human-robot interaction [5, 9].

State-of-art image captioning models today tend to be
monolithic neural models, essentially of the “encoder-
decoder” paradigm. Images are encoded into a vector with
a convolutional neural network (CNN), and captions are de-
coded from this vector using a Recurrent Neural Network
(RNN), with the entire system trained end-to-end. While

Figure 1. Example captions generated by (a) Baby Talk [24], (c)
neural image captioning [20] and (b) our Neural Baby Talk ap-
proach. Our method generates the sentence “template” with slot
locations (illustrated with ﬁlled boxes) explicitly tied to image re-
gions (drawn in the image in corresponding colors). These slots
are then ﬁlled by object detectors with concepts found in regions.

there are many recent extensions of this basic idea to in-
clude attention [46, 12, 50, 48, 27], it is well-understood
that models still lack visual grounding (i.e., do not associate
named concepts to pixels in the image). They often tend to
‘look’ at different regions than humans would and tend to
copy captions from training data [8].

For instance, in Fig. 1 a neural image captioning ap-
proach [20] describes the image as “A dog is sitting on a
couch with a toy.” This is not quite accurate. But if one
were to really squint at the image, it (arguably) does per-
haps look like a scene where a dog could be sitting on a
couch with a toy. It certainly is common to ﬁnd dogs sitting
on couches with toys. A-priori, the description is reason-
able. That’s exactly what today’s neural captioning models
tend to do – produce generic plausible captions based on the
language model1 that match a ﬁrst-glance gist of the scene.
While this may sufﬁce for common scenes, images that dif-
fer from canonical scenes – given the diversity in our visual
world, there are plenty of such images – tend to be under-
served by these models.

If we take a step back – do we really need the language
model to do the heavy lifting in image captioning? Given

∗Equal contribution

1frequently, directly reproduced from a caption in the training data.

1

Figure 2. From left to right is the generated caption using the same captioning model but with different detectors: 1) No detector; 2) A
weak detector that only detects “person” and “sandwich”; 3) A detector trained on COCO [26] categories (including “teddy bear”). 4) A
detector that can detect novel concepts (e.g. “Mr. Ted” and “pie” that never occurred in the captioning training data). Different colors show
a correspondence between the visual word and grounding regions.

the unprecedented progress we are seeing in object recog-
nition2 (e.g., object detection, semantic segmentation, in-
stance segmentation, pose estimation), it seems like the vi-
sion pipeline can certainly do better than rely on just a ﬁrst-
glance gist of the scene. In fact, today’s state-of-the-art ob-
ject detectors can successfully detect the table and cake in
the image in Fig. 1(c)! The caption ought to be able to talk
about the table and cake actually detected as opposed to
letting the language model hallucinate a couch and a toy
simply because that sounds plausible.

Interestingly, some of the ﬁrst attempts at image caption-
ing [13, 24, 25, 34] – before the deep learning “revolution”
– relied heavily on outputs of object detectors and attribute
classiﬁers to describe images. For instance, consider the
output of Baby Talk [24] in Fig. 1, that used a slot ﬁlling
approach to talk about all the objects and attributes found in
the scene via a templated caption. The language is unnatu-
ral but the caption is very much grounded in what the model
sees in the image. Today’s approaches fall at the other ex-
treme on the spectrum – the language generated by modern
neural image captioning approaches is much more natural
but tends to be much less grounded in the image.

In this paper, we introduce Neural Baby Talk that recon-
ciles these methodologies. It produces natural language ex-
plicitly grounded in entities found by object detectors. It is
a neural approach that generates a sentence “template” with
slot locations explicitly tied to image regions. These slots
are then ﬁlled by object recognizers with concepts found in
the regions. The entire approach is trained end-to-end. This
results in natural sounding and grounded captions.

Our main technical contribution is a novel neural de-
coder for grounded image captioning. Speciﬁcally, at each
time step, the model decides whether to generate a word
from the textual vocabulary or generate a “visual” word.
The visual word is essentially a token that will hold the slot
for a word that is to describe a speciﬁc region in the image.
For instance, for the image in Fig. 1, the generated sequence

may be “A <region−17> is sitting at a <region−123>
with a <region−3>.” The visual words (<region−[.]>’s)
are then ﬁlled in during a second stage that classiﬁes
each of the indicated regions (e.g., <region−17>→puppy,
<region−123>→table), resulting in a ﬁnal description of
“A puppy is sitting at a table with a cake.” – a free-form
natural language description that is grounded in the image.
One nice feature of our model is that it allows for different
object detectors to be plugged in easily. As a result, a va-
riety of captions can be produced for the same image using
different detection backends. See Fig. 2 for an illustration.
Contributions: Our contributions are as follows:

• We present Neural Baby Talk – a novel framework for
visually grounded image captioning that explicitly lo-
calizes objects in the image while generating free-form
natural language descriptions.

• Ours is a two-stage approach that ﬁrst generates a hy-
brid template that contains a mix of (text) words and
slots explicitly associated with image regions, and then
ﬁlls in the slots with (text) words by recognizing the
content in the corresponding image regions.

• We propose a robust image captioning task to bench-
mark compositionality of image captioning algorithms
where at test time the model encounters images con-
taining known objects but in novel combinations (e.g.,
the model has seen dogs on couches and people at ta-
bles during training, but at test time encounters a dog
at a table). Generalizing to such novel compositions is
one way to demonstrate image grounding as opposed
to simply leveraging correlations from training data.
• Our proposed method achieves state-of-the-art perfor-
mance on COCO and Flickr30k datasets on the stan-
dard image captioning task, and signiﬁcantly outper-
forms existing approaches on the robust image cap-
tioning and novel object captioning tasks.

2. Related Work

2e.g., 11% absolute increase in average precision in object detection in

the COCO challenge in the last year.

Some of the earlier approaches generated templated im-
age captions via slot-ﬁlling. For instance, Kulkarni et

al. [24] detect objects, attributes, and prepositions, jointly
reason about these through a CRF, and ﬁnally ﬁll appropri-
ate slots in a template. Farhadi et al. [13] compute a triplet
for a scene, and use this templated “meaning” representa-
tion to retrieve a caption from a database. [25, 34] use more
powerful language templates such as a syntactically well-
formed tree. These approaches tend to either produce cap-
tions that are relevant to the image but not natural sounding,
or captions that are natural (e.g. retrieved from a database of
captions) but may not be sufﬁciently grounded in the image.
Neural models for image captioning have been receiv-
ing increased attention in the last few years [23, 32, 7, 45,
11, 20]. State-of-the-art neural approaches include atten-
tion mechanisms [46, 12, 50, 48, 27, 39, 3] that identify re-
gions in the image to “ground” emitted words. In practice,
these attention regions tend to be quite blurry, and rarely
correspond to semantically meaningful individual entities
(e.g., objects instances) in the image. Our approach grounds
words in object detections, which by design identify con-
crete semantic entities (object instances) in the image.

There has been some recent interest in grounding natu-
ral language in images. Dense Captioning [19] generates
descriptions for speciﬁc image regions.
In contrast, our
model produces captions for the entire image, with words
grounded in concrete entities in the image. Another related
line of work is on resolving referring expressions [21] (or
description-based object retrieval [37, 17, 18, 40] – given a
description of an object in the image, identify which ob-
ject is being referred to) or referring expression genera-
tion [21, 29, 31, 51] (given an object in the image, generate a
discriminative description of the object). While the interest
in grounded language is in common, our task is different.

One natural strength of our model is its ability to in-
corporate different object detectors, including the ability to
generate captions with novel objects (never seen before in
training captions). In that context, our work is related to
prior works on novel object captioning [4, 43, 49, 2]. As
we describe in Sec. 4.3, our method outperforms these ap-
proaches by 14.6% on the averaged F1 score.

3. Method

Given an image I, the goal of our method is to gener-
ate visually grounded descriptions y = {y1, . . . , yT }. Let
rI = {r1, ..., rN } be the set of N images regions extracted
from I. When generating an entity word in the caption, we
want to ground it in a speciﬁc image region r ∈ rI . Fol-
lowing the standard supervised learning paradigm, we learn
parameters θ of our model by maximizing the likelihood of
the correct caption:

θ∗ = arg max

θ

(cid:88)

log p(y|I; θ)

(1)

(I,y)
Using chain rule, the joint probability distribution can be

decomposed over a sequence of tokens:

p(y|I) =

p(yt|y1:t−1, I)

(2)

T
(cid:89)

t=1

where we drop the dependency on model parameters to
avoid notational clutter. We introduce a latent variable rt
to denote a speciﬁc image region so that yt can explicitly
ground in it. Thus the probability of yt is decomposed to:

p(yt|y1:t−1, I) = p(yt|rt, y1:t−1, I)p(rt|y1:t−1, I)

(3)

In our framework, yt can be of one of two types: a vi-
sual word or a textual word, denoted as yvis and ytxt re-
spectively. A visual word yvis is a type of word that is
grounded in a speciﬁc image region drawn from rI . A tex-
tual word ytxt is a word from the remainder of the caption.
It is drawn from the language model , which is associated
with a “default” sentinel “region” ˜r obtained from the lan-
guage model [27] (discussed in Sec. 3.1). For example, as
illustrated in Fig. 1, “puppy” and “cake” grounded in the
bounding box of category “dog” and “cake” respectively,
are visual words. While “with” and “sitting” are not associ-
ated with any image regions and thus are textual words.

With this, Eq. 1 can be decomposed into two cascaded
objectives. First, maximizing the probability of generating
the sentence “template”. A sequence of grounding regions
associated with the visual words interspersed with the tex-
tual words can be viewed as a sentence “template”, where
the grounding regions are slots to be ﬁlled in with visual
words.3 An example template (Fig. 3) is “A <region−2>
is laying on the <region−4> near a <region−7>. Sec-
ond, maximizing the probability of visual words yvis
con-
ditioned on the grounding regions and object detection in-
formation, e.g., categories recognized by detector. In the
template example above, the model will ﬁll the slots with
‘cat’, ‘laptop’ and ‘chair’ respectively.

t

In the following, we ﬁrst describe how we generate the
slotted caption template (Sec. 3.1), and then how the slots
are ﬁlled in to obtain the ﬁnal image description (Sec. 3.2).
The overall objective function is described in Sec. 3.3 and
the implementation details in Sec. 3.4.

3.1. “Slotted” Caption Template Generation

Given an image I, and the corresponding caption y, the
candidate grounding regions are obtained by using a pre-
trained Faster-RCNN network [38]. To generate the cap-
tion “template”, we use a recurrent neural network, which
is commonly used as the decoder for image captioning
[32, 45]. At each time step, we compute the RNN hidden
state ht according to the previous hidden state ht−1 and the
input xt such that ht = RNN(xt, ht−1). At training time,

3Our approach is not limited to any pre-speciﬁed bank of templates.
Rather, our approach automatically generates a template (with placehold-
ers – slots – for visually grounded words), which may be any one of the
exponentially many possible templates.

P t

r = softmax([ut; wT

h tanh(Wsst + Wzht)])

(9)
where Ws ∈ Rd×d and Wz ∈ Rd×d are the parameters.
Notably, Wz and wh are the same parameters as in Eq. 4.
P t
r is the probability distribution over grounding regions rI
and visual sentinel ˜r. The last element of the vector in Eq. 9
captures p(˜r|y1:t−1).

We feed the hidden state ht into a softmax layer to obtain
the probability over textual words conditioned on the image,
all previous words, and the visual sentinel:

P t

txt = softmax (Wqht)
(10)
where Wq ∈ RV ×d, d is hidden state size, and V is textual
vocabulary size. Plugging in Eq. 10 and p(˜r|y1:t−1) from
the last element of the vector in Eq. 9 into Eq. 6 gives us the
probability of generating a textual word in the template.

3.2. Caption Reﬁnement: Filling in The Slots

To ﬁll the slots in the generated template with visual
words grounded in image regions, we leverage the outputs
of an object detection network. Given a grounding region,
the category can be obtained through any detection frame-
work [38]. But outputs of detection networks are typically
singular coarse labels e.g. “dog”. Captions often refer to
these entities in a ﬁne-grained fashion e.g. “puppy” or in
the plural form “dogs”. In order to accommodate for these
linguistic variations, the visual word yvis in our model is
a reﬁnement of the category name by considering the fol-
lowing two factors: First, determine the plurality – whether
it should be singular or plural. Second, determine the ﬁne-
grained class (if any). Using two single layer MLPs with
ReLU activation f (·), we compute them with:

P t
b = softmax (Wbfb ([vt; ht]))
g = softmax (cid:0)U T Wgfg ([vt; ht])(cid:1)
P t

(11)

(12)

Wb ∈ R2×d, Wg ∈ R300×d are the weight parameters.
U ∈ R300×k is the glove vector embeddings [36] for k
ﬁne-grained words associated with the category name. The
visual word yvis
is then determined by plurality and ﬁne-
grained class (e.g., if plurality is plural, and the ﬁne-grained
class is “puppy”, the visual word will be “puppies”).

t

3.3. Objective

Most standard image captioning datasets (e.g. COCO
[26]) do not contain phrase grounding annotations, while
some datasets do (e.g. Flickr30k [37]). Our training objec-
tive (presented next) can incorporate different kinds of su-
pervision – be it strong annotations indicating which words
in the caption are grounded in which boxes in the image, or
weak supervision where objects are annotated in the image
but are not aligned to words in the caption. Given the tar-
get ground truth caption y∗
1:T and a image captioning model

Figure 3. One block of the proposed approach. Given an image,
proposals from any object detector and current word “A”, the ﬁgure
shows the process to predict the next visual word “cat”.

xt is the ground truth token (teacher forcing) and at test time
is the sampled token yt−1. Our decoder consists of an atten-
tion based LSTM layer [39] that takes convolution feature
maps as input. Details can be found in Sec. 3.4. To generate
the “slot” for visual words, we use a pointer network [44]
that modulates a content-based attention mechanism over
the grounding regions. Let vt ∈ Rd×1 be the region fea-
ture of rt, which is calculated based on Faster R-CNN. We
compute the pointing vector with:

h tanh(Wvvt + Wzht)

i = wT
ut
P t
= softmax(ut)
rI

(4)

(5)

Since textual words ytxt

where Wv ∈ Rm×d, Wz ∈ Rd×d and wh ∈ Rd×1 are pa-
rameters to be learned. The softmax normalizes the vector
ut to be a distribution over grounding regions rI .
t

are not tied to speciﬁc regions
in the image, inspired by [27], we add a “visual sentinel”
˜r as a latent variable to serve as dummy grounding for the
textual word. The visual sentinel can be thought of as a la-
tent representation of what the decoder already knows about
the image. The probability of a textual word ytxt

then is:

t

p(ytxt
t

|y1:t−1) = p(ytxt

t

|˜r, y1:t−1)p(˜r|y1:t−1)

(6)

where we drop the dependency on I to avoid clutter.

We ﬁrst describe how the visual sentinel is computed,
and then how the textual words are determined based on the
visual sentinel. Following [27], when the decoder RNN is
an LSTM [16], the representation for visual sentinel st can
be obtained by:

gt = σ (Wxxt + Whht−1)
st = gt (cid:12) tanh (ct)

(7)

(8)

where Wx ∈ Rd×d, Wh ∈ Rd×d. xt is the LSTM input
at time step t, and gt is the gate applied on the cell state
ct. (cid:12) represents element-wise product, σ the logistic sig-
moid activation. Modifying Eq. 5, the probability over the
grounding regions including the visual sentinel is:

with parameters θ, we minimize the cross entropy loss:

L(θ) = −

T
(cid:88)

t=1

(cid:16)

(cid:122)
p(y∗

t |˜r, y∗

log

Textual word probability
(cid:125)(cid:124)
1:t−1)p(˜r|y∗

1:t−1)1(y∗

(cid:123)
t =ytxt) +

p (cid:0)b∗
(cid:124)

t , s∗

t |rt, y∗
(cid:123)(cid:122)
Caption reﬁnement

1:t−1

(cid:1)

(cid:125)

m
(cid:88)

i=1

(cid:0) 1
m
(cid:124)

p (cid:0)ri

t|y∗

1:t−1

(cid:1) (cid:1)1(y∗

t =yvis)

(cid:17)

(cid:123)(cid:122)
Averaged target region probability

(cid:125)

(13)
where y∗
t is the word from the ground truth caption at time
t. 1(y∗
t =ytxt) is the indicator function which equals to 1 if
y∗
t is textual word and 0 otherwise. b∗
t are the target
ground truth plurality and ﬁnd-grained class. {ri
i=1 ∈ rI
are the target grounding regions of the visual word at time
t. We maximize the averaged log probability of the target
grounding regions.

t and s∗

t}m

t}m

Visual word extraction. During training, visual words
in a caption are dynamically identiﬁed by matching the base
form of each word (using the Stanford lemmatization tool-
box [30]) against a vocabulary of visual words (details of
how to get visual word can be found in dataset Sec. 4). The
grounding regions {ri
i=1 for a visual word yt is identiﬁed
by computing the IoU of all boxes detected by the object
detection network with the ground truth bounding box as-
sociated with the category corresponding to yt. If the score
exceeds a threshold of 0.5 and the grounding region label
matches the visual word, the bounding boxes are selected
as the grounding regions. E.g., given a target visual word
“cat”, if there are no proposals that match the target bound-
ing box, the model predicts the textual word “cat” instead.

3.4. Implementation Details

Detection model. We use Faster R-CNN [38] with
ResNet-101 [15] to obtain region proposals for the image.
We use an IoU threshold of 0.7 for region proposal sup-
pression and 0.3 for class suppressions. A class detection
conﬁdence threshold of 0.5 is used to select regions.

i ; vl

i; vg

Region feature. We use a pre-trained ResNet-101 [15]
in our model. The image is ﬁrst resized to 576×576 and we
random crop 512 × 512 as the input to the CNN network.
Given proposals from the pre-trained detection model, the
feature vi for region i is a concatenation of 3 different fea-
tures vi = [vp
i ] where vp
i is the pooling feature of
RoI align layer [14] given the proposal coordinates, vl
i is
the location feature and vg
i is the glove vector embedding of
the class label for region i. Let xmin, ymin, xmax, ymax be the
bounding box coordinates of the region b; WI and HI be the
width and height of the image I. Then the location feature
vl
i can be obtained by projecting the normalized location
xmin
[
WI

] into another embedding space.

xmax
WI

ymax
HI

ymin
HI

Language model. We use an attention model with two
LSTM layers [3] as our base attention model. Given N re-

,

,

,

Figure 4. Language model used in our approach.

gion features from detection proposals V = {v1, . . . , vN }
and CNN features from the last convolution layer at K grids
ˆV = {ˆv1, . . . , ˆvK}, the language model has two separate
attention layers shown in Fig 4. The attention distribution
over the image features for detection proposals is:
z tanh (cid:0)WvV + (Wght)1T (cid:1)

(14)

zt = wT
αt = softmax(zt)

where Wv ∈ Rm×d, Wg ∈ Rd×d and w ∈ Rd×1. 1 ∈ RN
is a vector with all elements set to 1. αt is the attention
weight over N image location features.

Training details. In our experiments, we use a two layer
LSTM with hidden size 1024. The number of hidden units
in the attention layer and the size of the input word em-
bedding are 512. We use the Adam [22] optimizer with an
initial learning rate of 5 × 10−4 and anneal the learning rate
by a factor of 0.8 every three epochs. We train the model
up to 50 epochs with early stopping. Note that we do not
ﬁnetune the CNN network during training. We set the batch
size to be 100 for COCO [26] and 50 for Flickr30k [37].

4. Experimental Results

Datasets. We experiment with two datasets. Flickr30k
Entities [37] contains 275,755 bounding boxes from 31,783
images associated with natural language phrases. Each im-
age is annotated with 5 crowdsourced captions. For each
annotated phrase in the caption, we identify visual words
by selecting the inner most NP (noun phrase) tag from
the Stanford part-of-speech tagger [6]. We use Stanford
Lemmatization Toolbox [30] to get the base form of the en-
tity words resulting in 2,567 unique words.

COCO [26] contains 82,783, 40,504 and 40,775 images
for training, validation and testing respectively. Each im-
age has around 5 crowdsourced captions. Unlike Flickr30k
Entities, COCO does not have bounding box annotations
associated with speciﬁc phrases or entities in the caption.
To identify visual words, we manually constructed an ob-
ject category to word mapping that maps object categories
like <person> to a list of potential ﬁne-grained labels like
[“child”, “baker”, ...]. This results in 80 categories with a
total of 413 ﬁne-grained classes. See supp. for details.

Figure 5. Generated captions and corresponding visual grounding regions on the standard image captioning task (Top: COCO, Bottom:
Flickr30k). Different colors show a correspondence between the visual words and grounding regions. Grey regions are the proposals not
selected in the caption. First 3 columns show success and last column shows failure cases (words are grounded in the wrong region).

Method

BLEU1 BLEU4 METEOR CIDEr SPICE

Method

BLEU1 BLEU4 METEOR CIDEr SPICE

Hard-Attention [46]
ATT-FCN [50]
Adaptive [27]

NBT
NBToracle

66.9
64.7
67.7

69.0
72.0

19.9
23.0
25.1

27.1
28.5

18.5
18.9
20.4

21.7
23.1

-
-
53.1

57.5
64.8

-
-
14.5

15.6
19.6

Table 1. Performance on the test portion of Karpathy et al. [20]’s
splits on Flickr30k Entities dataset.

Adaptive [27]
Att2in [39]
Up-Down [3]

Att2in∗ [39]
Up-Down† [3]

NBT
NBToracle

74.2
-
74.5

-
79.8

75.5
75.9

32.5
31.3
33.4

33.3
36.3

34.7
34.9

26.6
26.0
26.1

26.3
27.7

27.1
27.4

108.5
101.3
105.4

111.4
120.1

107.2
108.9

19.5
-
19.2

-
21.4

20.1
20.4

Detector pre-training. We use open an source imple-
mentation [47] of Faster-RCNN [38] to train the detector.
For Flickr30K Entities, we use visual words that occur at
least 100 times as detection labels, resulting in a total of 460
detection labels. Since detection labels and visual words
have a one-to-one mapping, we do not have ﬁne-grained
classes for the Flickr30K Entities dataset – the caption re-
ﬁnement process only determines the plurality of detection
labels. For COCO, ground truth detection annotations are
used to train the object detector.

Caption pre-processing. We truncate captions longer
than 16 words for both COCO and Flickr30k Entities
dataset. We then build a vocabulary of words that occur at
least 5 times in the training set, resulting in 9,587 and 6,864
words for COCO and Flickr30k Entities, respectively.

4.1. Standard Image Captioning

For standard image captioning, we use splits from
Karpathy et al. [20] on COCO/Flickr30k. We report re-
sults using the COCO captioning evaluation toolkit [26],
which reports the widely used automatic evaluation metrics,
BLEU [35], METEOR [10], CIDEr [42] and SPICE [1].

Table 2. Performance on the test portion of Karpathy et al. [20]’s
splits on COCO dataset. ∗ directly optimizes the CIDEr Metric, †
uses better image features, and are thus not directly comparable.

We present our methods trained on different object de-
tectors: Flickr and COCO. We compare our approach (re-
ferred to as NBT) to recently proposed Hard-Attention [46],
ATT-FCN [50] and Adaptive [27] on Flickr30k, and Att2in
[39], Up-Down [3] on COCO. Since object detectors have
not yet achieved near-perfect accuracies on these datasets,
we also report the performance of our model under an oracle
setting, where the ground truth object region and category
is also provided during test time. (referred to as NBToracle)
This can be viewed as the upper bound of our method when
we have perfect object detectors.

Table 1 shows results on the Flickr30k dataset. We see
that our method achieves state of the art on all automatic
evaluation metrics, outperforming the previous state-of-art
model Adaptive [27] by 2.0 and 4.4 on BLEU4 and CIDEr.
When using ground truth proposals, NBToracle signiﬁcantly
outperforms previous methods, improving 5.1 on SPICE,
which implies that our method could further beneﬁt from
improved object detectors.

Figure 6. Generated captions and corresponding visual grounding regions for the robust image captioning task. “cat-remote”, “man-bird”,
“dog-skateboard” and “orange-bird” are co-occurring categories excluded in the training split. First 3 columns show success and last
column shows failure case (orange was not mentioned).

Method

BLEU4 METEOR CIDEr SPICE Accuracy

Att2in [39]
Up-Down [3]

NBT
NBToracle

31.5
31.6

31.7
31.9

24.6
25.0

25.2
25.5

90.6
92.0

94.1
95.5

17.7
18.1

18.3
18.7

39.0
39.7

42.4
45.7

Table 3. Performance on the test portion of the robust image cap-
tioning split on COCO dataset.

Table 2 shows results on the COCO dataset. Our method
outperforms 4 out of 5 automatic evaluation metrics com-
pared to the state of the art [39, 27, 3] without using better
visual features or directly optimizing the CIDEr metric. In-
terestingly, the NBToracle has little improvement over NBT.
We suspect the reason is that explicit ground truth anno-
tation is absent for visual words. Our model can be fur-
ther improved with explicit co-reference supervision where
the ground truth location annotation of the visual word is
provided. Fig. 5 shows qualitative results on both datasets.
We see that our model learns to correctly identify the visual
word, and ground it in image regions even under weak su-
pervision (COCO). Our model is also robust to erroneous
detections and produces correct captions (3rd column).

4.2. Robust Image Captioning

To quantitatively evaluate image captioning models for
novel scene compositions, we present a new split of the
COCO dataset, called the robust-COCO split. This new
split is created by re-organizing the train and val splits of
the COCO dataset such that the distribution of co-occurring
objects in train is different from test. We also present a new
metric to evaluate grounding.

Robust split. To create the new split, we ﬁrst identify
entity words that belong to the 80 COCO object categories
by following the same pre-processing procedure. For each
image, we get a list of object categories that are mentioned
in the caption. We then calculate the co-occurrence statis-
tics for these 80 object categories. Starting from the least
co-occurring category pairs, we greedily add them to the
test set and ensure that for each category, at least half the
instances of each category are in the train set. As a re-

sult, there are sufﬁcient examples from each category in
train, but at test time we see novel compositions (pairs)
of categories. Remaining images are assigned to the train-
ing set. The ﬁnal split has 110,234/3,915/9,138 images in
train/val/test respectively.

Evaluation metric. To evaluate visual grounding on the
robust-COCO split, we want a metric that indicates whether
or not a generated caption includes the new object combina-
tion. Common automatic evaluation metrics such as BLEU
[35] and CIDEr [42] measure the overall sentence ﬂuency.
We also measure whether the generated caption contains the
novel co-occurring categories that exist in the ground truth
caption. A generated caption is deemed 100% accurate if it
contains at least one mention of the compositionally novel
category-pairs in any ground truth annotation that describe
the image.

Results and analysis. We compare our method with
state of the art Att2in [39] and Up-Down [3]. These are
implemented using the open source implementation from
[28] that can replicate results on Karpathy’s split. We fol-
low the experimental setting from [39] and train the model
using the robust-COCO train set. Table 3 shows the results
on the robust-COCO split. As we can see, all models per-
form worse on the robust-COCO split than the Karpathy’s
split by 2∼3 points in general. Our method outperforms
the previous state of the art methods on all metrics, outper-
forming Up-Down [3] by 2.7 on the proposed metric. The
oracle setting (NBToracle) has consistent improvements on
all metrics, improving 3.3 on the proposed metric.

Fig. 6 shows qualitative results on the robust image cap-
tioning task. Our model successfully produces a caption
with novel compositions, such as “cat-remote”, “man-bird”
and “dog-skateboard” to describe the image. The last col-
umn shows failure cases where our model didn’t select “or-
ange” in the caption. We can force our model to produce
a caption containing “orange” and “bird” using constrained
beam search [2], further illustrated in Sec. 4.3.

4.3. Novel Object Captioning

Since our model directly ﬁlls the “slotted” caption tem-
plate with the concept, it can seamlessly generate descrip-

Figure 7. Generated captions and corresponding visual grounding regions for the novel object captioning task. “zebra”, “tennis racket”,
“bus” and “pizza” are categories excluded in the training split. First 3 columns show success and last column shows a failure case.

Out-of-Domain Test Data

In-Domain Test Data

Method

bottle

bus

couch microwave pizza racket

suitcase zebra Avg SPICE METEOR CIDEr SPICE METEOR CIDER

DCC [4]
NOC [43]
C-LSTM [49]
Base+T4 [2]

NBT∗+G
NBT†+G
NBT†+T1
NBT†+T2

4.6
17.8
29.7
16.3

7.1
14.0
36.2
38.3

29.8
68.8
74.4
67.8

73.7
74.8
77.7
80.0

45.9
25.6
38.8
48.2

34.4
42.8
43.9
54.0

28.1
24.7
27.8
29.7

61.9
63.7
65.8
70.3

64.6
69.3
68.2
77.2

59.9
74.4
70.3
81.1

52.2
68.1
70.3
57.1

20.2
19.0
19.8
74.8

13.2
39.9
44.8
49.9

42.3
44.5
51.2
67.8

79.9
89.0
91.4
85.7

88.5
92.0
93.7
96.6

39.8
49.1
55.7
54.0

48.5
53.2
57.3
70.3

13.4
-
-
15.9

15.7
16.6
16.7
17.4

21.0
21.4
23.0
23.3

22.8
23.9
23.9
24.1

59.1
-
-
77.9

77.0
84.0
85.7
86.0

15.9
-
-
18.0

17.5
18.4
18.4
18.0

23.0
-
-
24.5

24.3
25.3
25.5
25.0

77.2
-
-
86.3

87.4
94.0
95.2
92.1

Table 4. Evaluation of captions generated using the proposed method. G means greedy decoding, and T1−2 means using constrained beam
search [2] with 1−2 top detected concepts. ∗ is the result using VGG-16 [41] and † is the result using ResNet-101.

tions for out-of-domain images. We replicated an existing
experimental design [4] on COCO which excludes all the
image-sentence pairs that contain at least one of eight ob-
jects in COCO. The excluded objects are ‘bottle’, “bus”,
“couch”, “microwave”, “pizza”, “racket”, “suitcase” and
“zebra”. We follow the same splits for training, valida-
tion, and testing as in prior work [4]. We use Faster R-
CNN in conjunction with ResNet-101 which is pre-trained
on COCO train split as the detection model. Note that we
do not pre-train the language model using COCO captions
as in [4, 43, 49], and simply replace the novel object’s word
embedding with an existing one which belongs to the same
super-category in COCO (e.g., bus ← car).

Following [2], the test set is split into in-domain and out-
of-domain subsets. We report F1 as in [4], which checks if
the speciﬁc excluded object is mentioned in the generated
caption. To evaluate the quality of the generated caption, we
use SPICE, METEOR and CIDEr metrics and the scores on
out-of-domain test data are macro-averaged across eight ex-
cluded categories. For consistency with previous work [3],
the inverse document frequency statistics used by CIDEr are
determined across the entire test set.

As illustrated in Table 4.1, simply using greedy decod-
ing, our model (NBT∗+G) can successfully caption novel
concepts with minimum changes to the model. When us-
ing ResNet-101 and constrained beam search [2], our model
signiﬁcantly outperforms prior works under F1 scores,
SPICE, METEOR, and CIDEr, across both out-of-domain
and in-domain test data. Speciﬁcally, NBT†+T2 outper-

forms the previous state-of-art model C-LSTM by 14.6%
on average F1 scores. From the category F1 scores, we
can see that our model is less likely to select small objects,
e.g. “bottle”, “racket” when only using the greedy decod-
ing. Since the visual words are grounded at the object-level,
by using [2], our model was able to signiﬁcantly boost the
captioning performance on out-of-domain images. Fig. 7
shows qualitative novel object captioning results. Also see
rightmost example in Fig. 2.

5. Conclusion

In this paper, we introduce Neural Baby Talk, a novel im-
age captioning framework that produces natural language
explicitly grounded in entities object detectors ﬁnd in im-
ages. Our approach is a two-stage approach that ﬁrst gener-
ates a hybrid template that contains a mix of words from a
text vocabulary as well as slots corresponding to image re-
gions. It then ﬁlls the slots based on categories recognized
by object detectors in the image regions. We also introduce
a robust image captioning split by re-organizing the train
and val splits of the COCO dataset. Experimental results on
standard, robust, and novel object image captioning tasks
validate the effectiveness of our proposed approach.

Acknowledgements This work was funded in part by: NSF
CAREER awards to DB, DP; ONR YIP awards to DP, DB;
ONR Grants N00014-14-1-{0679,2713}; PGA Family Founda-
tion award to DP; Google FRAs to DP, DB; and Amazon ARAs to
DP, DB; DARPA XAI grant to DB, DP.

6. Appendix: COCO Fine-grained Categories

The COCO [26] dataset does not have bounding box annota-
tions associated with speciﬁc phrases or entities in the caption.
We use category level detection annotations and create a cate-
gory mapping list that maps the object categories like <Person>
to a list of potential ﬁne-grained labels like [“child”, “man”,
“baker”,...]. We ﬁrst use the Stanford lemmatization toolbox [30]
to get the base form of the entity words in the caption. For each
category class, we retrieve the top 200 similar words in the Word-
Vec [33] space. We then manually verify each word in the list,
resulting in 413 ﬁne-grained classes. A complete list of the ﬁne-
grained class for each object category can be found in Table 5 and
Table 6.

References

[1] P. Anderson, B. Fernando, M. Johnson, and S. Gould. Spice:
Semantic propositional image caption evaluation. In ECCV,
2016. 6

[2] P. Anderson, B. Fernando, M. Johnson, and S. Gould.
Guided open vocabulary image captioning with constrained
beam search. EMNLP, 2017. 3, 7, 8

[3] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,
S. Gould, and L. Zhang. Bottom-up and top-down atten-
tion for image captioning and visual question answering. In
CVPR, 2018. 3, 5, 6, 7, 8

[4] L. Anne Hendricks, S. Venugopalan, M. Rohrbach,
R. Mooney, K. Saenko, and T. Darrell. Deep composi-
tional captioning: Describing novel object categories with-
out paired training data. In CVPR, 2016. 3, 8

[5] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.
Zitnick, and D. Parikh. VQA: Visual Question Answering.
In ICCV, 2015. 1

[6] D. Chen and C. Manning. A fast and accurate dependency

parser using neural networks. In EMNLP, 2014. 5

[7] X. Chen and C. Lawrence Zitnick. Mind’s eye: A recurrent
visual representation for image caption generation. In CVPR,
2015. 3

[8] A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, and D. Ba-
tra. Human Attention in Visual Question Answering: Do
Humans and Deep Networks Look at the Same Regions? In
EMNLP, 2016. 1

[9] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M.
Moura, D. Parikh, and D. Batra. Visual Dialog. In CVPR,
2017. 1

[10] M. Denkowski and A. Lavie. Meteor universal: Language
speciﬁc translation evaluation for any target language.
In
EACL 2014 Workshop on Statistical Machine Translation,
2014. 6

[11] J. Donahue, L. Anne Hendricks,

S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual
recognition and description. In CVPR, 2015. 3

[12] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng,
P. Doll´ar, J. Gao, X. He, M. Mitchell, J. C. Platt, et al. From
captions to visual concepts and back. In CVPR, 2015. 1, 3

[13] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every pic-
ture tells a story: Generating sentences from images.
In
ECCV, 2010. 2, 3

[14] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask r-cnn.

ICCV, 2017. 5

[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 5

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997. 4

[17] R. Hu, M. Rohrbach,

and
K. Saenko. Modeling relationships in referential expres-
sions with compositional modular networks. arXiv preprint
arXiv:1611.09978, 2016. 3

J. Andreas, T. Darrell,

[18] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Dar-
rell. Natural language object retrieval. In CVPR, 2016. 3
[19] J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully
convolutional localization networks for dense captioning. In
CVPR, 2016. 3

[20] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
In CVPR, 2015.

ments for generating image descriptions.
1, 3, 6

[21] S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg.
Referitgame: Referring to objects in photographs of natural
scenes. In EMNLP, 2014. 3

[22] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014. 5

[23] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Multimodal

neural language models. In ICML, 2014. 3

[24] G. Kulkarni, V. Premraj, V. Ordonez, S. Dhar, S. Li, Y. Choi,
A. C. Berg, and T. L. Berg. Babytalk: Understanding and
generating simple image descriptions. In CVPR, 2011. 1, 2,
3

[25] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and
Y. Choi. Collective generation of natural image descriptions.
In ACL, 2012. 2, 3

[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 2, 4, 5, 6, 9
[27] J. Lu, C. Xiong, D. Parikh, and R. Socher. Knowing when
to look: Adaptive attention via a visual sentinel for image
captioning. In CVPR, 2017. 1, 3, 4, 6, 7

[28] R. Luo.

Unofﬁcial

for
sequence
caption-
for
https://github.com/ruotianluo/

implementation
image

self-critical
ing.
self-critical.pytorch, 2017. 7

training

pytorch

[29] R. Luo and G. Shakhnarovich. Comprehension-guided refer-

ring expressions. In CVPR, 2017. 3

[30] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J.
Bethard, and D. McClosky. The Stanford CoreNLP natural
language processing toolkit. In ACL, 2014. 5, 9

[31] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and
K. Murphy. Generation and comprehension of unambiguous
object descriptions. In CVPR, 2016. 3

[32] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille.
Deep captioning with multimodal recurrent neural networks
(m-rnn). In ICLR, 2015. 3

[33] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
preprint arXiv:1301.3781, 2013. 9

[34] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal,
A. Berg, K. Yamaguchi, T. Berg, K. Stratos,
and
H. Daum´e III. Midge: Generating image descriptions from
computer vision detections. In EACL, 2012. 2, 3

[35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a
method for automatic evaluation of machine translation. In
ACL, 2002. 6, 7

[36] J. Pennington, R. Socher, and C. Manning. Glove: Global
vectors for word representation. In EMNLP, 2014. 4
[37] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,
J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer image-
to-sentence models. In ICCV, 2015. 3, 4, 5

[38] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 3, 4, 5, 6

[39] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel.
Self-critical sequence training for image captioning.
In
CVPR, 2017. 3, 4, 6, 7

[40] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and
B. Schiele. Grounding of textual phrases in images by re-
construction. In ECCV, 2016. 3

[41] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 8

[42] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider:
In CVPR,

Consensus-based image description evaluation.
2015. 6, 7

[43] S. Venugopalan, L. A. Hendricks, M. Rohrbach, R. Mooney,
T. Darrell, and K. Saenko. Captioning images with diverse
objects. In CVPR, 2017. 3, 8

[44] O. Vinyals, M. Fortunato, and N. Jaitly. Pointer networks. In

NIPS, 2015. 4

[45] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and

tell: A neural image caption generator. In CVPR, 2015. 3

[46] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdi-
nov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural
In ICML,
image caption generation with visual attention.
2015. 1, 3, 6

[47] J. Yang, J. Lu, D. Batra, and D. Parikh. A faster pytorch
implementation of faster r-cnn. https://github.com/
jwyang/faster-rcnn.pytorch, 2017. 6

[48] Z. Yang, Y. Yuan, Y. Wu, R. Salakhutdinov, and W. W. Co-
hen. Encode, review, and decode: Reviewer module for cap-
tion generation. In NIPS, 2016. 1, 3
[49] T. Yao, Y. Pan, Y. Li, and T. Mei.

Incorporating copying
mechanism in image captioning for learning novel objects.
In CVPR, 2017. 3, 8

[50] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image cap-
tioning with semantic attention. In CVPR, 2016. 1, 3, 6
[51] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Mod-
eling context in referring expressions. In ECCV, 2016. 3

Object category

Fine-grained class

<person>

<bicycle>
<car>
<motorcycle>
<airplane>
<bus>
<train>
<truck>
<boat>

person, girl, boy, man, woman, kid, child, chef, baker, people, adult, rider, children, baby, worker, passenger, sister, biker, policeman,
ofﬁcer, lady, cowboy, bride, groom, male, female, guy, traveler, mother, father, gentleman, pitcher, player, skier, snowboarder,
skater, skateboarder, foreigner, caller, offender, coworker, trespasser, patient, politician, soldier, grandchild, serviceman, walker,
drinker, doctor, bicyclist, thief, buyer, teenager, student, camper, driver, solider, hunter, shopper, villager, cop
bicycle, bike, unicycle, minibike, trike
car, automobile, van, minivan, sedan, suv, hatchback, cab, jeep, coupe, taxicab, limo, taxi
motorcycle, scooter, motor bike, motor cycle, motorbike, moped
airplane, jetliner, plane, air plane, monoplane, aircraft, jet, airbus, biplane, seaplane bus, minibus, trolley
bus, minibus, schoolbus, trolley
train, locomotive, tramway, caboose
truck, pickup, lorry, hauler, ﬁretruck
boat, ship, liner, sailboat, motorboat, dinghy, powerboat, speedboat, canoe, skiff, yacht, kayak, catamaran, pontoon, houseboat, vessel,
rowboat, trawler, ferryboat, watercraft, tugboat, schooner, barge, ferry, sailboard, paddleboat, lifeboat, freighter, steamboat, riverboat,
surfboard, battleship, steamship
trafﬁc light, street light, trafﬁc signal, stop light, streetlight, stoplight
ﬁre hydrant, hydrant
stop sign, street sign

<trafﬁc light>
<ﬁre hydrant>
<stop sign>
<parking meter> parking meter
<bench>
<cat>
<dog>

<horse>
<sheep>
<cow>
<elephant>
<bear>
<zebra>
<giraffe>
<backpack>
<umbrella>
<handbag>
<tie>
<suitcase>
<frisbee>
<skis>
<snowboard>
<sports ball>
<kite>
<baseball bat>
<baseball glove> baseball glove
<skateboard>
<surfboard>
<tennis racket>
<bottle>

bench, pew
cat, kitten, feline, tabby
dog, puppy, beagle, pup, chihuahua, schnauzer, dachshund, rottweiler, canine, pitbull, collie, pug, terrier, poodle, labrador, doggie,
doberman, mutt, doggy, spaniel, bulldog, sheepdog, weimaraner, corgi, cocker, greyhound, retriever, brindle, hound, whippet, husky
horse, colt, pony, racehorse, stallion, equine, mare, foal, palomino, mustang, clydesdale, bronc, bronco
sheep, lamb, goat, ram, cattle, ewe
cow, cattle, oxen, ox, calf, ewe, holstein, heifer, buffalo, bull, zebu, bison
elephant
bear, panda
zebra
giraffe
backpack, knapsack
umbrella
handbag, handbag, wallet, purse, briefcase
tie
suitcase, suit case, luggage
frisbee
skis, ski
snowboard
sports ball, baseball, ball, football, soccer, basketball, softball, volleyball, pinball, fastball, racquetball
kite
baseball bat

skateboard
surfboard, longboard, skimboard, shortboard, wakeboard
tennis racket
bottle

Table 5. COCO category mapping list for visual words.

Object category

Fine-grained class

<wine glass>
<cup>
<fork>
<knife>
<spoon>
<bowl>
<banana>
<apple>
<sandwich>
<orange>
<broccoli>
<carrot>
<hot dog>
<pizza>
<donut>
<cake>
<bird>

<chair>
<couch>
<potted plant>
<bed>
<dining table>
<toilet>
<tv>
<laptop>
<mouse>
<remote>
<keyboard>
<cell phone>
<sink>
<refrigerator>
<book>
<clock>
<vase>
<scissors>
<teddy bear>
<hair drier>
<toothbrush>

wine glass
cup
fork
knife, pocketknife, knive
spoon
bowl, container, plate
banana
apple
sandwich, burger, sub, cheeseburger, hamburger
orange, lemons
broccoli
carrot
hot dog
pizza
donut, doughnut, bagel
cake, cheesecake, cupcake, shortcake, coffeecake, pancake
bird, ostrich, owl, seagull, goose, duck, parakeet, falcon, robin, pelican, waterfowl, heron, hummingbird, mallard, ﬁnch, pigeon, sparrow,
seabird, osprey, blackbird, fowl, shorebird, woodpecker, egret, chickadee, quail, bluebird, kingﬁsher, buzzard, willet, gull, swan, bluejay,
ﬂamingo, cormorant, parrot, loon, gosling, waterbird, pheasant, rooster, sandpiper, crow, raven, turkey, oriole, cowbird, warbler, magpie,
peacock, cockatiel, lorikeet, pufﬁn, vulture, condor, macaw, peafowl, cockatoo, songbird
chair, seat, recliner, stool
couch, sofa, recliner, futon, loveseat, settee, chesterﬁeld
potted plant, houseplant
bed
dining table, table
toilet, urinal, commode, lavatory, potty
tv, monitor, televison, television
laptop, computer, notebook, netbook, lenovo, macbook
mouse
remote
keyboard
cell phone, mobile phone, phone, cellphone, cellphone, telephone, phon, smartphone, iPhone
sink
refrigerator, fridge, refrigerator, fridge, freezer, refridgerator, frig
book
clock
vase
scissors
teddy bear, teddybear
hair drier, hairdryer
toothbrush

Table 6. COCO category mapping list for visual words (continued).

