Joint Embedding of Meta-Path and Meta-Graph for
Heterogeneous Information Networks

Lichao Sun†, Lifang He‡∗, Zhipeng Huang§, Bokai Cao¶, Congying Xia†, Xiaokai Wei¶ and Philip S. Yu†
†University of Illinois at Chicago, Chicago, IL ‡Cornell University, New York City, NY
§The University of Hong Kong, China ¶Facebook, Menlo Park, CA

Email:

lsun29, cxia8, psyu
}

{

@uic.edu,

@gmail.com, zphuang@cs.hku.hk
lifanghescut,caobokai.why,weixiaokai
}
{

8
1
0
2
 
p
e
S
 
1
1
 
 
]
I
S
.
s
c
[
 
 
1
v
0
1
1
4
0
.
9
0
8
1
:
v
i
X
r
a

information, but

Abstract—Meta-graph is currently the most powerful tool
for similarity search on heterogeneous information networks,
where a meta-graph is a composition of meta-paths that
captures the complex structural information. However, current
relevance computing based on meta-graph only considers the
complex structural
ignores its embedded
meta-paths information. To address this problem, we propose
MEta-GrAph-based network embedding models, called MEGA
and MEGA++, respectively. The MEGA model uses normal-
ized relevance or similarity measures that are derived from a
meta-graph and its embedded meta-paths between nodes simul-
taneously, and then leverages tensor decomposition method to
perform node embedding. The MEGA++ further facilitates the
use of coupled tensor-matrix decomposition method to obtain
a joint embedding for nodes, which simultaneously considers
the hidden relations of all meta information of a meta-graph.
Extensive experiments on two real datasets demonstrate that
MEGA and MEGA++ are more effective than state-of-the-art
approaches.

Keywords-node embedding, heterogeneous information net-

works, tensor learning, meta graph

I. INTRODUCTION

Many information retrieval and mining tasks such as
node classiﬁcation [7], clustering [30], link prediction [28],
and information diffusion [25] become time-consuming in
large-scale networks. This motivates researchers to develop
network embedding techniques which aim to learn a dis-
tributed representation vector for each node in a network. An
effective network embedding should preserve the similarity
between nodes in order to reconstruct the original network.
The word2vec [21] idea has inspired many studies for
network representation learning, most of which are in the
context of homogeneous information networks, such as
DeepWalk [22], LINE [31], and node2vec [9]. A homoge-
neous information network is a simple structural network,
where all nodes and links are considered to belong to a
single class.

However, in practice, there are usually multiple types of
nodes (e.g., authors and papers in DBLP) and links (e.g.,
cite and publish) that compose a heterogeneous information
network (HIN). To measure the similarity between nodes
in HINs, many customized similarity or relevance measures

∗Corresponding author.

Figure 1: An example of meta-paths

2, and meta-graph

1,

P

P

of the bibliographic data.

S

→

→

→

paper

paper

based on meta-paths have been proposed in recent years
[16], [29]. For example, a meta-path author
→
author (denoted as AP V P A) indicates
venue
two authors having their publications in the same venue.
Comparing to meta-path-based relevance measures utilizing
only simple structural information, meta-graph [15] is re-
cently proposed to capture complex structural information
in HINs. In short, meta-graph is a special directed acyclic
graph (DAG) which contains at least two embedded meta-
paths, such as a DAG containing AP V P A and AP T P A as
shown in Figure 1, where T is the topic of a paper.

Meta-graph is an effective tool to calculate the relevance
score between nodes in HINs, where a higher score indicates
that there are more meta-graph instances between two nodes,
i.e., a closer relationship. How to explore meta-graphs for
representation learning in HINs is still an open question. An
intuitive idea for meta-graph-based representation learning
is to learn the node embedding by leveraging multiple
meta-graphs between nodes in HINs. However, existing
meta-graph-based relevance measures only utilize the strong
relations as deﬁned by the meta-graphs themselves, and
they usually ignore the weak relations as indicated by their
embedded meta-paths. To address this problem, we propose
to learn the node embedding by leveraging both meta-graph
and its embedded meta-paths for similarity search. An ef-
fective representation learning based on a single meta-graph
should contain both strong and weak relations embedded

in this meta-graph. In addition, we explore a novel meta-
graph-based similarity measure to compute relevance scores
that can better capture the strong relations between nodes in
HINs.

In summary,

there are three-fold contributions of this
paper: 1) We are the ﬁrst
to propose the meta-graph-
based node embedding method in HINs. Speciﬁcally, we
develop two kinds of node embedding methods based on
meta-graph, named MEGA and MEGA++ respectively. 2)
We introduce GraphSim which is an effective meta-graph-
based similarity measure with best performance comparing
to previous meta-graph-based similarity measures, such as
StructCount and SCSE. 3) Our approaches show the best
performance comparing to other competing methods on two
real-world datasets.

II. PRELIMINARY AND PROBLEM FORMULATION

In this section, we ﬁrst introduce some related concepts
and notations from multilinear algebra. Then, we review
some concepts and approaches involved in HIN analysis
including meta-graph and relevance measure. Last part, we
formulate the problem of node embedding in HINs.

A. Multilinear Algebra

The basic mathematical object of multilinear algebra is the
tensor, a higher order generalization of vectors (ﬁrst order
tensors) and matrices (second order tensors) to multiple
indices. The order of a tensor is the number of dimensions,
also known as modes or ways. An N -th order tensor is rep-
RI1×I2×···×IN , where In is the cardinality
resented as
, N
1, 2,
of its n-th mode, n
. An element of a vector
x, a matrix X, or a tensor
is denoted by xi, xi,j, xi,j,k,
etc., depending on the number of modes. All vectors are
column vectors unless otherwise speciﬁed. For an arbitrary
RI×J , its i-th row and j-th column vector are
matrix X
denoted by xi and xj, respectively.

· · ·
X

X ∈

∈ {

∈

}

Deﬁnitions of outer product, partial symmetric tensor,
mode-n matricization, and CP factorization are given below,
which will be applied to present our approach.

∈

∈

RIn for n

Deﬁnition 1: (Outer Product) The outer product of N
[1 : N ] is an N -th order tensor
=

vectors x(n)
and deﬁned element-wise by (cid:0)x(1)
x(1)
i1 · · ·
Deﬁnition 2: (Partial Symmetric Tensor) An N -th order
tensor is a rank-one partial symmetric tensor if it is partial
symmetric on modes i1, ..., ij
1, ..., N , and can be written
as the tensor product of N vectors, i.e.,

◦ · · · ◦
for all values of the indices.

x(N )(cid:1)

x(N )
iN

i1,··· ,iN

∈

= x(1)

X
= x(ij ).

x(N )

◦ · · · ◦

where x(i1) =

· · ·

Deﬁnition 3: (Mode-n Matricization) The mode-n ma-

tricization or unfolding of an N -th order tensor
RI1×I2×···×IN is denoted by X(n) and is of size In
where Jn = ΠN

m=1,m(cid:54)=nIm.

X ∈
Jn,
×

(cid:74)·(cid:75)
the

(3)

Deﬁnition 4: (CP Factorization) For a general tensor
X ∈
RI1×···×IN , its CANDECOMP/PARAFAC (CP) factoriza-
tion is

=

X

R
(cid:88)

r=1

x(1)
r ◦ · · · ◦

x(N )

r =

X(1), ..., X(N )

(2)

(cid:75)

(cid:74)
1 , ..., x(n)

R ] are factor

[1 : N ], X(n) = [x(n)

R, R is the number of factors, and

where for n
matrices of size In
×
is used for shorthand.

∈

To obtain the CP factorization

,
(cid:75)
objective is to minimize the following estimation error:

· · ·

(cid:74)

, X(N )

X(1),

=

min

X(1),

, X(N )

L

· · ·

X(1),··· ,X(N )(cid:107)X − (cid:74)
, X(N ). A
is not jointly convex w.r.t. X(1),
However,
widely used optimization technique is the Alternating Least
Squares (ALS) algorithm, which alternatively minimize
for each variable while ﬁxing the other, that is,

· · ·

L

L

2
F
(cid:75)(cid:107)

X(n)

←

arg min
X(n) (cid:107)
i(cid:54)=nX(i) = X(N )
N

where

(cid:12)
B. Meta Graph

X(n) −

X(n)(

i(cid:54)=nX(i))T
N

(4)

2
F
(cid:107)

X(n−1)

X(n+1)

X(1).

· · · (cid:12)

(cid:12) · · ·

(cid:12)

(cid:12)

,

Deﬁnition 5: (Meta-Graph [15]) A meta-graph S is a
directed acyclic graph (DAG) deﬁned on a HIN schema
TG = (
). A meta-graph S contains a single source node
ns with 0 in degree and a single target node nt with 0 out
degree. Mathematically, a meta-graph S = (A, B, ns, nt),
where A is a set of nodes, B is a set of edges, ns is the of
source node, and nt is the target node,.

R

O

Since a meta-graph only has one source node and one
target node, not all sub-graphs of HINs can be meta-graph.
Deﬁnition 6: (Meta-graph-based Relevance Measure)
, the similarity
is deﬁned

Given a HIN G = (V, E) and a meta-graph
of any two nodes vs, vt
as:

G
V with respect to

∈

G

(cid:88)

s =

gvs→vt ∈G

s(vs, vt

gvs→vt)

|

(5)

where gvs→vt

is a meta-graph instance of
, and
s(vs, vt
gvs→vt) is the relevance score between vs and vt,
which will be determined by the number of meta-graph
instances connecting them.

G

|

Prior works provide different meta-graph-based relevance

measures, such as StructCount, SCSE and BSCSE [15].

C. Problem Formulation

(1)

We study the problem of meta-graph-based node em-
bedding in the HIN. Given a HIN G = (V, E), we
have two goals in this study. First, we want to explore a
customized meta-graph-based relevance measure which can
more efﬁciently capture the complex structural information.
Second, we aim at ﬁnding an effective node embedding that

can better preserve the closeness between nodes in a HIN
based on a meta-graph and its embeded meta-paths analysis.
Speciﬁcally, we integrate all the similarity information of a
meta-graph and its embedded meta-paths into a symmetric
matrix and a partial symmetric tensor, and perform multi-
linear analysis of the coupled partial symmetric tensor and
symmetric matrix to ﬁnd the node embedding.

III. METHODS

In this section, we will introduce a brand new similarity
measure, and the embedding techniques of MEGA++. First,
we will introduce a meta-graph-based similarity measure
named GraphSim. Then, we proposed a coupled tensor-
matrix decomposition to obtain a joint embedding for nodes
in HINs.

A. GraphSim: A Normalized version of StructCount

First, we want to propose a new meta-graph-based sim-
ilarity measure called GraphSim. In previous work, Huang
et al. [15] proposed three meta-graph-based similarity mea-
sures: StructCount, SCSE, and BSCSE which is a mixed
measure based on previous two measures. GraphSim can be
viewed as a normalized version of StructCount.

StructCount [15] is a straightforward meta-graph-based
similarity measure in HIN, which counts the number of
meta-graph instances in the graph G with an ns as source
and an nt as target object.

Deﬁnition 7: (GraphSim) A meta-graph-based similarity
, GraphSim be-

measure. Given a symmetric meta-graph
V is deﬁned as:
tween two nodes vt, vs
gvt→vs|
+
}|

∈
2
× |{
gvt→vt∈G

gvt→vt|

s(vt, vs) =

|{

G

gvt→vs∈G
}|
gvs→vs |
gvs→vs∈G
|{

}|(6)
where gvt→vsis a meta-graph instance between vt and vs,
gvt→vt is that between vt and vt, and gvs→vs is that between
vs and vs.

G

∈

V by following

Comparing to StructCount, GraphSim is normalized ver-
sion of StructCount. sGraphSim(vt, vs) is determined by two
parts: First, the number of meta-graph instance between
vt, vs
; Second, the balance of their
visibility, where the visibility is deﬁned as the number
of meta-graph instances between themselves. Normalized
relevance score can present better relation between different
nodes. For example, an author A1 published all his four
papers with A2. A3 published ﬁve papers with A2, and
A3 published other ﬁve papers with other authors. Without
the relation between A2 and A3 is
normalized process,
closer than A2 and A1. However, for common sense, we
should agree A2 and A1 have closer relation, which indicates
GraphSim is a better measure.

Comparing to three measures in [15], s(vt, vs) of Graph-
Sim is between 0 and 1 like SCSE. However, SCSE mea-
sures the random walk probability that vs expands a meta-
graph instance to vt. In our application, we ﬁnd GraphSim

shows better performance than all those three meta-graph
measures [15].

B. MEGA++: Node Embedding by CTMD

∈

RM ×M , and for each meta-path

In this section, we show how to jointly consider similarity
information of a meta-graph and its embedded meta-paths
to learn a node embedding. The basic idea is the integration
of similarity matrices and coupled embedding by joint
factorization. Speciﬁcally, we ﬁrst compute a meta-graph
similarity matrix using the proposed GraphSim, denoted as
Y
, compute an embed-
ded meta-path similarity matrix using the PathSim, denoted
RM ×M . Next, we concatenate the embedded
as M
meta-path similarity matrices of different embedded meta-
paths to form a third-order tensor comprising three modes:
, MN ]
nodes, nodes, and paths, denoted as
RM ×M ×N . Then, we introduce a novel coupled tensor-
∈
matrix decomposition (CTMD) method to ﬁnd common
and Y. Last, we use the latent
latent features between
features to measure the similarity between different nodes
in the HIN.

= [M1,

· · ·

X

X

P

∈

In the following, we detail the CTMD method, which
can be seen as a special case of the coupled tensor-matrix
decomposition [1] with input partial symmetric tensor
and
symmetric matrix Y. Notice that since similarity matrix is
is a partial symmetric tensor, and
symmetric, the resulting
X
Y is a symmetric matrix.

X

Tensors (including matrix) provide a natural and efﬁcient
representation for a meta-graph data, but there is no guar-
antee that such representation will be good for subsequent
learning, since learning will only be successful if the regu-
larities that underlie the data can be discerned by the model.
Tensor factorization is a powerful tool to analyze tensors. In
previous work, it was found that CP factorization (which is a
higher order generalization of SVD) is particularly effective
to acknowledge the connections and ﬁnd valuable features
among tensor data [32]. Motivated by these observations,
we exploit the beneﬁts of CP and SVD factorizations to
ﬁnd an effective embedding in the sense of meta-path-based
similarity tensor

and meta-graph similarity matrix Y.

Based on above analysis, we design our CTMD objective

X

function as below:

min
P,T (cid:107)X − (cid:74)

P, P, T

2
F + α
(cid:75)(cid:107)

Y
(cid:107)

−

PPT

2
F

(cid:107)

(7)

RM ×R and T

RN ×R are latent matrices.
where P
∈
Speciﬁcally, P is jointly learned from both meta-graph and
meta-path similarity information.

∈

to P and T together,

The objective function in Eq. (7) is non-convex with
respect
thus there is no closed-
form solution. We introduce an effective iteration method
to solve this problem. The main idea is to decouple the
parameters using an Alternating Direction Method of Multi-
pliers (ADMM) approach [4], by alternatively optimizing the
objective with respect to one variable, while ﬁxing others.

Figure 2: Framework of Meta-graph-based Node Embedding with Coupled Tensor-Matrix Decomposition

Update P: First, we optimize P while ﬁxing T. Notice
that the objective function in Eq. (7) involves a fourth-order
term with respect to P which is difﬁcult to optimize directly.
To obviate this problem, we use a variable substitution
technique and minimize the following objective function

min
P,Q (cid:107)X − (cid:74)
s.t. P = Q

P, Q, T

2
F + α
(cid:75)(cid:107)

Y
(cid:107)

−

PQT

2
F

(cid:107)

where Q

RM ×R is an auxiliary variable.
The augmented Lagrangian function of Eq. (8) is

∈

(P, Q) =

P, Q, T

L

(cid:107)X − (cid:74)
+ tr(UT(P

2
F + α
Y
(cid:75)(cid:107)
(cid:107)
λ
Q)) +
2 (cid:107)

P

−

PQT

2
F

(cid:107)

−

Q

2
F
(cid:107)

−

RM ×R is the Lagrange multiplier, and λ is
where U
the penalty parameter which can be adjusted efﬁciently
according to [18].

∈

To compute P, Eq. (9) can be transformed as

min
P (cid:107)X(1) −

PFT

2
F + α
(cid:107)

(cid:107)

Y

−

PQT

2
F +
(cid:107)

λ
2 (cid:107)

P

−

Q +

U

1
λ
(10)

2
F
(cid:107)

where X(1) ∈
and F = T
(cid:12)

∈

RM ×(M N ) is the mode-1 matricization of
Q

R(M N )×R.

,

X

zero, we obtain the closed-form solution

P = (2X(1)F + 2αYQ + λQ

U)(2FTF + 2αQTQ + λI)−1

−

To efﬁciently compute FTF, we consider the following
property of the Khatri-Rao product of two matrices

FTF = (T

Q)T(T

Q) = TTT

QTQ

(cid:12)

(cid:12)
Then the auxiliary matrix Q can be optimized successively
in a similar way, and the solution is

∗

(12)

(8)

Q = (2X(2)G + 2αYTP + λP + U)(2GTG + 2αPTP + λI)−1

where X(2) is the mode-2 matricization of
T

P.

X

, and G =

(cid:12)
Moreover, we optimize the Lagrange multiplier U using

(9)

the gradient descent method by

Update T: Next, we optimize T while ﬁxing P and S.

We need to optimize the following objective function

U

U + λ(P

Q)

←

−

min

T (cid:107)

X(3) −

THT

2
F
(cid:107)

where X(3) is the mode-3 matricization of
Q

P.

X

, and H =

(cid:12)
By setting the derivative of Eq. (15) with respect to T to

(13)

(14)

(15)

T = (X(3)H)(HTH)−1

(16)

(11)

The overall algorithm is summarized in Algorithm 1.

By setting the derivative of Eq. (10) with respect to P to

zero, we obtain the closed-form solution as

Algorithm 1 Coupled Tensor-Matrix Decomposition
(CTMD)
Input: Meta-path similarity tensor

, and meta-graph sim-

X

Table I: Statistics of Datasets

DBLP
MOVIE

|V |
15,649
25,643

|E|
51,377
40,173

Avg. degree
6.57
3.13

|L|
4
5

|R|
4
4

ilarity matrix Y

Output: Embedding matrix P
1: : Set λmax = 106, ρ = 1.15
2: : Initialize P, Q, T
3: loop convergence
4:
5:
6:

: Update P by Eq. (11)
: Update Q by Eq. (13)
: Update T by Eq. (16)
: Update µ by µ

∼ N

7:
8: end loop

min(ρµ, µmax)

←

(0, 1), U = 0, λ = 10−6

C. Time Complexity

Each iteration in Algorithm 1 consists of simple matrix
operations. Therefore, rough estimates of its computational
complexity can be easily derived based on ADMM [17].

−

The estimate for the update of P according to Eq. (11)
is as follows: O(M 2N R) for the computation of the term
U; O((M + N )R2) for the com-
2X(1)F + 2αYQ + λQ
putation of the term 2FTF + 2αQTQ + λI due to Eq. (12)
and O(R3) for its Cholesky decomposition; O(M K 2) for
the computation of the system solution that gives the updated
value of P. An analogous estimate can be derived for the
update of Q.
Overall,

the updates of model parameters P and Q,
require O(R3 +(M +N )R2 +M 2N R) arithmetic operations
in total.

Algorithm 2 MEGA++
Input: An HIN G, a particular meta-graph g, a embedded
meta-path pi of a meta-graph g, and an empty array PA

Output: Embedding matrix P

: Pi = PathSim(G, pi)
: PA = [PA; Pi] store Pi into PA

∈

1: : Y = GraphSim(G, g)
2: loop pi
g
3:
4:
5: end loop
6: :

= concatenate(PA)

X

meta-paths
7: : P = CTMD(

, Y)

X

is a tensor of

’s embedded

X

S

IV. EXPERIMENTS

In this section, we conduct extensive experiments in order
to test the effectiveness of the proposed methods: GraphSim,
MEGA and MEGA++. We ﬁrst introduce two real-life
datasets and a set of methods to be compared. Then, we
evaluate the effectiveness of proposed methods on four data
mining tasks: clustering, classiﬁcation, parameter analysis
and time analysis.

Figure 3: Guided meta-graphs:
for M2M task, and

2 is for V2V task.

S

1 is the guided meta-graph

S

We use two real datasets (e.g. DBLP-4-Area and YAGO
Movie) in the evaluation. Table I shows some statistics
about them. DBLP-4-Area [29] is the subset of original
DBLP, which contains 5,237 papers (P), 5,915 authors (A),
18 venues (V), 4,479 topics (T). The authors and venues
are from 4 areas: database, data mining, machine learning
and information retrieval. YAGO Movie is a subset of
YAGO [15], which contains 7,332 movies (M), 10,789 actors
(A), 1,741 directors (D), 3,392 producers (P) and 1,483
composers (C). The movies are divided into ﬁve genres:
action, horror, adventure, sci-ﬁ and crime. The guided meta-
graphs are designed for three tasks as shown in the Figures
1 and 3.

The proposed methods are compared with meta-graph-
based relevance measures (e.g. StructCount, SCSE, and
BSCSE [15]), and network embedding approaches (e.g.
DeepWalk [22], and LINE [31]) in clustering and classi-
ﬁcation tasks. The experimental results are shown in the
following sections.

A. Clustering Results

We ﬁrst conduct a clustering task to evaluate the perfor-
mance of the compared methods on DBLP and YAGO Movie
datasets. For DBLP, we use the areas of authors as ground-
truth label for clustering authors (A2A), and use the areas
of venues as labels for clustering venues (V2V). For YAGO
Movie, we use the genres of movies as labels (M2M). To be
speciﬁc, we use k-means on the derived meta-graph-based
relevance matrices for the clustering task. To evaluate the
results, we use NMI and purity as evaluation metrics.

Clustering results of the three tasks are shown in Ta-
ble II. Comparing to previous meta-graph-based relevance
measures, the proposed GraphSim always shows the best
performance of all. We observe at least 19.94% improvement

Table II: Clustering performance

Task
DBLP
(V2V)
DBLP
(A2A)
Moive
(M2M)

Overall

Pre. Meta-Graph Measures

Method
NMI
Purity
NMI
Purity
NMI
Purity
NMI
Purity

StuctCount
0.2634
0.5000
0.0338
0.2997
0.0011
0.2991
0.0994
0.3663

SCSE
0.6309
0.7333
0.0156
0.2822
0.0008
0.2988
0.2158
0.4381

BSCSE(α = 1)
0.6309
0.7333
0.0156
0.2823
0.0008
0.2988
0.2158
0.4381

Pre. Network Embedding
LINE∗
0.7954
0.8042
0.3920
0.7135
0.0008
0.2981
0.3961
0.6052

DeepWalk∗
0.8258
0.8584
0.4896
0.7941
0.0007
0.2981
0.4387
0.6502

OUR WROKS
GraphSim MEGA MEGA++
0.8521
0.8817
0.5263
0.7956
0.0045
0.3017
0.4610
0.6597

0.8718
0.8956
0.5315
0.7989
0.0045
0.3032
0.4693
0.6659

0.8479
0.8744
0.2150
0.4903
0.0021
0.3002
0.3550
0.5550

Table III: Classiﬁcation performance

Pre. Meta-Graph Measures

Task
DBLP
(A2A)
Movie
(M2M)

Overall

Method
Macro-F1
Micro-F1
Macro-F1
Micro-F1
Macro-F1
Micro-F1

StuctCount
0.734
0.730
0.126
0.281
0.430
0.506

SCSE
0.616
0.634
0.111
0.276
0.364
0.455

BSCSE(α = 0)
0.734
0.730
0.126
0.281
0.430
0.506

Pre. Network Embedding
LINE∗
0.816
0.817
0.186
0.241
0.501
0.540

DeepWalk∗
0.839
0.840
0.189
0.245
0.514
0.543

OUR WROKS
GraphSim MEGA MEGA++
0.863
0.863
0.298
0.342
0.581
0.603

0.867
0.867
0.310
0.352
0.589
0.610

0.818
0.819
0.125
0.307
0.472
0.563

in NMI of GraphSim method when compared with the
previous meta-graph-based relevance measure on clustering
the venues and authors in DBLP, respectively. The clustering
results can be sensitive to initialization of centroid seeds,
so we set 100 times of random initializations. All methods
show worse performance on YAGO Movie than DBLP, but
the proposed methods, especially MEGA++, show the best
performance comparing to prior works..

B. Classiﬁcation Results

We then conduct a classiﬁcation task. Comparing to the
clustering task, in DBLP we do not evaluate the results of
classifying the venues, as the total number of venues is
only 18. We ﬁrst apply previous methods and our works to
generate the similarity matrices or embedding space of the
original network. Then, we randomly partition the samples,
and set 80% samples as training set and the rest as testing
set. Last, we apply k nearest neighbor (k-NN) classiﬁer with
k = 5 to evaluate the methods with training and testing
dataset [15], [29]. To prevent the special case of random
partition, we repeat and use different random partition 10
times in total. For multi-label classiﬁcation task, we use
the average Macro-F1 score and Micro-F1 score as the
evaluation metrics.

GraphSim outperforms the existing relevance measures
(e.g StructCount, SCSE and BSCSE) because it represents a
better relations between objects in the HINs by normalizing
the presence of meta-graph structures. MEGA++ outper-
forms all the baselines because it captures both lower-order
(i.e. meta-path) and higher-order (i.e. meta-graph) structural
information by facilitating the use of coupled tensor-matrix
decomposition method to obtain a joint embedding for nodes
in HINs.

Table IV: Time analysis: three tasks of MEGA++: R is the
dimension of the embedding, and second is the time scale

R
DBLP (A2A)
DBLP (V2V)
MOVIE (M2M)

1
0.338
0.129
1.625

5
4.693
0.169
6.013

10
9.731
0.445
12.69

15
9.689
0.667
19.57

C. Parameter Analysis

In this section, we ﬁrst analyze the parameter sensitivity
of our methods as shown in Figure 4. We use two evaluation
metrics, Normalized Mutual Information(NMI), and Purity
(both the larger, the better), to evaluate the performances
of our methods for clustering task. In Figure 4 (a)-(b), the
penalty parameters λ of MEGA is used for minimizing the
Frobenius Norm of embedding space P and Q in Eq. (8),
and the best performance is achieved when λ is set as
3.2768e-04. From 4 (b)-(c), setting the embedding dimen-
sions as 5 shows the best performance for both MEGA
and MEGA++. MEGA++ outperforms MEGA with the
same number of embedding dimensions. The Figure 4 (e)-
(f) show the two penalty parameters λ and α of MEGA++.
The penalty parameter λ of MEGA++ is the same as that
in MEGA. The penalty parameter α of MEGA++ is used
for minimizing the Frobenius Norm of meta-graph similarity
matrix Y and its embedding space in Eq. (7). We ﬁnd that
λ = 0.6711 and α = 1.6 produce the best performance of
clustering task.
D. Time Analysis

In this section, we evaluate the execution time of
MEGA++. In Table IV, it shows the execution time is
linear with respect to the embedding dimensions. Based

[33]. [6], [33] learn the node embedding by deep learning
encoder methods. However, none previous node embedding
methods consider the meta-graph and its embedded meta-
paths information.
B. Tensor Learning and Embedding

Just

like deep learning,

tensor learning becomes very
hot and popular topic in recent years due to the stronger
computing capability and lower computation cost [5], [11],
[12], [14], [19], [20], [24]. Coupled tensor matrix embedding
tries to fuse multiple information sources where matrices and
tensors sharing some common modes are jointly embedding
[8]. A gradient-based optimization approach for joint tensor-
matrix analysis is proposed by Acar et al. [1].

C. Multi-view Learning

Multi-view learning is a hot idea to think one object
with different views [13], [23], [26], [27]. In this paper,
we think the HIN with different views such as meta-paths
and meta-graph, and fuse the different information for node
embedding. However, none of these frameworks can be
directly applicable to learn jointly embedding with a partial
symmetric tensor and a symmetric matrix, and also do
not leverage meta-path and meta-structure information for
similarity search in HINs.

VI. CONCLUSION AND FUTURE WORK

In this paper, we proposed a new meta-graph-based rel-
evance measure, i.e. GraphSim, and two node embeddings,
i.e. MEGA and MEGA++, by leveraging a meta-graph
and its embedded meta-paths similarity information. In the
experiment, MEGA++ shows better performance than other
compared methods in different tasks. In the future, we can
expend our proposed node embedding for a single meta-
graph to multiple meta-graphs node embedding in a HIN.
Meanwhile, we can utilize heterogeneous and homogeneous
information together for node embedding.

VII. ACKNOWLEDGE

This work is supported in part by NSFC through grants
No. 61503253 and 61672313, NSF through grants No.
IIS-1526499, IIS-1763325, and CNS-1626432, and NSF of
Guangdong Province through grant No. 2017A030313339.

REFERENCES

[1] Evrim Acar, Tamara G Kolda, and Daniel M Dunlavy. All-
at-once optimization for coupled matrix and tensor factoriza-
tions. arXiv:1105.3422, 2011.

[2] Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy,
Vanja Josifovski, and Alexander J Smola. Distributed large-
scale natural graph factorization. In WWW. ACM, 2013.

[3] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and
spectral techniques for embedding and clustering. In NIPS,
2001.

(a)

(b)

(c)

(d)

(e)

(f)

Figure 4: Parameters analysis: two metrics including Purity
(a)(c)(e) and NMI (b)(d)(f). (a)-(b) analyzes λ of MEGA.
(c)-(d) analyzes the embedding dimensions R of MEGA
and MEGA++. (e)-(f) analyzes two parameters λ and α of
MEGA++.

on the time complexity of MEGA++, when we have a
ﬁxed size of dataset, the embedding dimensions R, and the
number of views N are linear with respect to the execution
time. Sometimes, MEGA++ can be early stopped when it
is already converge, so as to the same time consuming of
DBLP (A2A) with R = 10 and R = 15. The same results
are shown in the real testing on three tasks, which show the
efﬁciency of MEGA++.

V. RELATED WORK

A. Network Embedding

Network embedding want to learn a low-dimensional rep-
resentations from a network. Previous traditional works [3]
usually construct the afﬁnity graph using the feature vectors
of the vertexes and then compute the eigenvectors of the
afﬁnity graph. Some other groups use matrix factorization
to represent graph as adjacency matrix [2].

Recently, DeepWalk [22] and LINE [31] are proposed
for learning the network embedding. Besides these two
most popular node embedding methods, many other net-
work embedding are proposed recent years [6], [7], [10],

[5] Bokai Cao, Lifang He, Xiaokai Wei, Mengqi Xing, Philip S
t-bne: Tensor-based

Yu, Heide Klumpp, and Alex D Leow.
brain network embedding. In SDM. SIAM, 2017.

[6] Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C
Aggarwal, and Thomas S Huang. Heterogeneous network
embedding via deep architectures. In KDD. ACM, 2015.

[7] Ting Chen and Yizhou Sun. Task-guided and path-augmented
heterogeneous network embedding for author identiﬁcation.
In WSDM. ACM, 2017.

[4] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and
Jonathan Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers.
Foundations and Trends® in Machine Learning, 2011.
[20] Chun-Ta Lu, Lifang He, Weixiang Shao, Bokai Cao, and
Philip S Yu. Multilinear factorization machines for multi-task
multi-view learning. In WSDM. ACM, 2017.

[21] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
Efﬁcient estimation of word representations in vector space.
arXiv, 2013.

[8] Beyza Ermis¸, Evrim Acar, and A Taylan Cemgil. Link
prediction in heterogeneous data via generalized coupled
tensor factorization. DMKD, 2015.

[22] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk:
In KDD. ACM,

Online learning of social representations.
2014.

[9] Aditya Grover and Jure Leskovec. node2vec: Scalable feature

learning for networks. In KDD. ACM, 2016.

[10] Huan Gui, Jialu Liu, Fangbo Tao, Meng Jiang, Brandon
Norick, and Jiawei Han. Large-scale embedding learning in
heterogeneous event data. In ICDM. IEEE, 2016.

[11] Tengjiao Guo, Le Han, Lifang He, and Xiaowei Yang. A ga-
based feature selection and parameter optimization for linear
support higher-order tensor machine. Neurocomputing, 2014.

[23] Weixiang Shao, Lifang He, Chun-Ta Lu, Xiaokai Wei, and
Philip S Yu. Online unsupervised multi-view feature selec-
tion. ICDM, 2016.

[24] Weixiang Shao, Lifang He, and S Yu Philip. Clustering
on multi-source incomplete data via tensor modeling and
factorization. In PAKDD. Springer, 2015.

[25] Lichao Sun, Weiran Huang, Philip S Yu, and Wei Chen.
Multi-round inﬂuence maximization. In KDD. ACM, 2018.

[12] Lifang He, Xiangnan Kong, Philip S Yu, Xiaowei Yang,
Ann B Ragin, and Zhifeng Hao. Dusk: A dual structure-
preserving kernel for supervised tensor learning with appli-
cations to neuroimages. In SDM. SIAM, 2014.

[26] Lichao Sun, Yuqi Wang, Bokai Cao, S Yu Philip, Witawas
Srisa-An, and Alex D Leow. Sequential keystroke behavioral
biometrics for mobile user identiﬁcation via multi-view deep
learning. In ECML-PKDD. Springer, 2017.

[13] Lifang He, Chun-Ta Lu, Hao Ding, Shen Wang, Linlin Shen,
S Yu Philip, and Ann B Ragin. Multi-way multi-level kernel
modeling for neuroimaging classiﬁcation. In CVPR, 2017.

[27] Lichao Sun, Xiaokai Wei, Jiawei Zhang, Lifang He, S Yu
Philip, and Witawas Srisa-an. Contaminant removal for
android malware detection systems. In BigData. IEEE, 2017.

[14] Lifang He, Chun-Ta Lu, Guixiang Ma, Shen Wang, Linlin
Shen, S Yu Philip, and Ann B Ragin. Kernelized support
tensor machines. In ICML, 2017.

[28] Yizhou Sun, Jiawei Han, Charu C Aggarwal, and Nitesh V
Chawla. When will it happen?: relationship prediction in
heterogeneous information networks. In WSDM. ACM, 2012.

[15] Zhipeng Huang, Yudian Zheng, Reynold Cheng, Yizhou Sun,
Nikos Mamoulis, and Xiang Li. Meta structure: Computing
relevance in large heterogeneous information networks.
In
KDD. ACM, 2016.

[16] Ni Lao and William W Cohen. Relational retrieval using
a combination of path-constrained random walks. Machine
learning, 2010.

[17] Athanasios P Liavas and Nicholas D Sidiropoulos. Parallel
algorithms for constrained tensor factorization via alternating
direction method of multipliers. TSP, 2015.

[18] Zhouchen Lin, Risheng Liu, and Zhixun Su. Linearized
alternating direction method with adaptive penalty for low-
rank representation. In NIPS, 2011.

[19] Xiaolan Liu, Tengjiao Guo, Lifang He, and Xiaowei Yang.
A low-rank approximation-based transductive support tensor
machine for semisupervised classiﬁcation. TIP, 2015.

[29] Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S Yu, and Tianyi
Wu. Pathsim: Meta path-based top-k similarity search in
heterogeneous information networks. VLDB, 2011.

[30] Yizhou Sun, Brandon Norick, Jiawei Han, Xifeng Yan,
Philip S Yu, and Xiao Yu. Pathselclus: Integrating meta-path
selection with user-guided object clustering in heterogeneous
information networks. TKDD, 2013.

[31] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan,
and Qiaozhu Mei. Line: Large-scale information network
embedding. In WWW. ACM, 2015.

[32] Charles F Van Loan. Structured matrix problems from tensors.
In Exploiting Hidden Structure in Matrix Computations:
Algorithms and Applications. Springer, 2016.

[33] Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep
network embedding. In KDD. ACM, 2016.

