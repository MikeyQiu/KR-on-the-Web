7
1
0
2
 
c
e
D
 
2
1

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
8
3
6
3
0
.
1
1
7
1
:
v
i
X
r
a

Provably Accurate Double-Sparse Coding

Thanh V. Nguyen
Iowa State University, ECE Department

Raymond K. W. Wong
Texas A&M University, Statistics Department

Chinmay Hegde ∗
Iowa State University, ECE Department

Editor: TBD

thanhng@iastate.edu

raywong@stat.tamu.edu

chinmay@iastate.edu

Abstract
Sparse coding is a crucial subroutine in algorithms for various signal processing, deep learning,
and other machine learning applications. The central goal is to learn an overcomplete dictionary
that can sparsely represent a given input dataset. However, a key challenge is that storage, trans-
mission, and processing of the learned dictionary can be untenably high if the data dimension is
high. In this paper, we consider the double-sparsity model introduced by Rubinstein et al. (2010b)
where the dictionary itself is the product of a ﬁxed, known basis and a data-adaptive sparse com-
ponent. First, we introduce a simple algorithm for double-sparse coding that can be amenable to
eﬃcient implementation via neural architectures. Second, we theoretically analyze its performance
and demonstrate asymptotic sample complexity and running time beneﬁts over existing (provable)
approaches for sparse coding. To our knowledge, our work introduces the ﬁrst computationally
eﬃcient algorithm for double-sparse coding that enjoys rigorous statistical guarantees. Finally,
we support our analysis via several numerical experiments on simulated data, conﬁrming that our
method can indeed be useful in problem sizes encountered in practical applications.
Keywords: Sparse coding, provable algorithms, unsupervised learning

1. Introduction

1.1 Motivation

Representing signals as sparse linear combinations of atoms from a dictionary is a popular approach
in many domains. In this paper, we study the problem of dictionary learning (also known as sparse
coding), where the goal is to learn an eﬃcient basis (dictionary) that represents the underlying
class of signals well. In the typical sparse coding setup, the dictionary is overcomplete (i.e., the
cardinality of the dictionary exceeds the ambient signal dimension) while the representation is
sparse (i.e., each signal is encoded by a combination of only very few dictionary atoms.)

Sparse coding has a rich history in diverse ﬁelds such as signal processing, machine learning, and
computational neuroscience. Discovering optimal basis representations of data is a central focus of
image analysis (Krim et al., 1999; Elad and Aharon, 2006; Rubinstein et al., 2010a), and dictionary
learning has proven widely successful in imaging problems such as denoising, deconvolution, inpaint-

∗. This work is supported in part by the National Science Foundation under the grants CCF-1566281 and DMS-
1612985. An abbreviated conference version will appear in the proceedings of AAAI 2018 (Nguyen et al., 2018).

1

ing, and compressive sensing (Elad and Aharon, 2006; Candes and Tao, 2005; Rubinstein et al.,
2010a). Sparse coding approaches have also been used as a core building block of deep learn-
ing systems for prediction (Gregor and LeCun, 2010; Boureau et al., 2010) and associative mem-
ory (Mazumdar and Rawat, 2017). Interestingly, the seminal work by Olshausen and Field (1997)
has shown intimate connections between sparse coding and neuroscience: the dictionaries learned
from image patches of natural scenes bear strikingly resemblance to spatial receptive ﬁelds observed
in mammalian primary visual cortex.

From a mathematical standpoint, the sparse coding problem is formulated as follows. Given p
Rn
m (m > n)
data samples Y = [y(1), y(2), . . . , y(p)]
and corresponding sparse code vectors X = [x(1), x(2), . . . , x(p)]
p such that the representation
DX ﬁts the data samples as well as possible. Typically, one obtains the dictionary and the code
vectors as the solution to the following optimization problem:

p, the goal is to ﬁnd a dictionary D

Rm

Rn

∈

∈

∈

×

×

×

min
D,X L

(D, X) =

y(j)
k

−

Dx(j)

2
2,
k

p

1
2

Xj=1
(x(j))

S

≤

s.t.

p

S

Xj=1

(1)

) is some sparsity-inducing penalty function on the code vectors, such as the ℓ1-norm.
(
·
controls the reconstruction error while the constraint enforces the sparsity

However, even a cursory attempt at solving the optimization problem (1) reveals the following

S

where
The objective function
of the representation.

L

obstacles:

1. Theoretical challenges. The constrained optimization problem (1) involves a non-convex
(in fact, bilinear) objective function, as well as potentially non-convex constraints depending
on the choice of the sparsity-promoting function
(for example, the ℓ0 function.) Hence,
obtaining provably correct algorithms for this problem can be challenging. Indeed, the vast
majority of practical approaches for sparse coding have been heuristics (Engan et al., 1999;
Aharon et al., 2006; Mairal et al., 2009); Recent works in the theoretical machine learning
community have bucked this trend, providing provably accurate algorithms if certain assump-
tions are satisﬁed (Spielman et al., 2012; Agarwal et al., 2014; Arora et al., 2015; Sun et al.,
2015; B lasiok and Nelson, 2016; law Adamczak, 2016; Chatterji and Bartlett, 2017). How-
ever, relatively few of these newer methods have been shown to provide good empirical per-
formance in actual sparse coding problems.

S

2. Practical challenges. Even if theoretical correctness issues were to be set aside, and we are
somehow able to eﬃciently learn sparse codes of the input data, we often ﬁnd that applications
using such learned sparse codes encounter memory and running-time issues. Indeed, in the
overcomplete case, only the storage of the learned dictionary D incurs mn = Ω(n2) memory
cost, which is prohibitive when n is large. Therefore, in practical applications (such as image
analysis) one typically resorts to chop the data into smaller blocks (e.g., partitioning image
data into patches) to make the problem manageable.

A related line of research has been devoted to learning dictionaries that obey some type of struc-
ture. Such structural information can be leveraged to incorporate prior knowledge of underlying

2

signals as well as to resolve computational challenges due to the data dimension. For instance, the
dictionary is assumed to be separable, or obey a convolutional structure. One such variant is the
double-sparse coding problem (Rubinstein et al., 2010b; Sulam et al., 2016) where the dictionary
D itself exhibits a sparse structure. To be speciﬁc, the dictionary is expressed as:

D = ΦA,

×

∈

Rn

n, and a learned “synthesis” matrix
i.e., it is composed of a known “base dictionary” Φ
m whose columns are sparse. The base dictionary Φ is typically any ﬁxed basis chosen
A
according to domain knowledge, while the synthesis matrix A is column-wise sparse and is to be
learned from the data. The basis Φ is typically orthonormal (such as the canonical or wavelet
basis); however, there are cases where the base dictionary Φ is overcomplete (Rubinstein et al.,
2010b; Sulam et al., 2016).

∈

×

Rn

There are several reasons why such the double-sparsity model can be useful. First, the double-
sparsity assumption is rather appealing from a conceptual standpoint, since it lets us combine
the knowledge of decades of modeling eﬀorts in harmonic analysis with the ﬂexibility of learning
new representations tailored to speciﬁc data families. Moreover, such a double-sparsity model has
computational beneﬁts. If the columns of A are (say) r-sparse (i.e., each column contains no more
than r
n non-zeroes) then the overall burden of storing, transmitting, and computing with A is
much lower than that for general unstructured dictionaries. Finally, such a model lends itself well
to interpretable learned features if the atoms of the base dictionary are semantically meaningful.

≪

All the above reasons have spurred researchers to develop a series of algorithms to learn
doubly-sparse codes (Rubinstein et al., 2010b; Sulam et al., 2016). However, despite their empiri-
cal promise, no theoretical analysis of their performance have been reported in the literature and
to date, we are unaware of a provably accurate, polynomial-time algorithm for the double-sparse
coding problem. Our goal in this paper is precisely to ﬁll this gap.

1.2 Our Contributions

In this paper, we provide a new framework for double-sparse coding. To the best of our knowledge,
our approach is the ﬁrst method that enjoys provable statistical and algorithmic guarantees for
this problem. In addition, our approach enjoys three beneﬁts: we demonstrate that the method is
neurally plausible (i.e., its execution can plausibly be achieved using a neural network architecture),
robust to noise, as well as practically useful.

Inspired by the aforementioned recent theoretical advances in sparse coding, we assume a
learning-theoretic setup where the data samples arise from a ground-truth generative model. Infor-
mally, suppose there exists a true (but unknown) synthesis matrix A∗ that is column-wise r-sparse,
and the ith data sample is generated as:

y(i) = ΦA∗x∗

(i) + noise,

i = 1, 2, . . . , p

(i) is independently drawn from a distribution supported on the set of k-
where the code vector x∗
sparse vectors. We desire to learn the underlying matrix A∗. Informally, suppose that the synthesis
matrix A∗ is incoherent (the columns of A∗ are suﬃciently close to orthogonal) and has bounded
spectral norm. Finally, suppose that the number of dictionary elements, m, is at most a constant
multiple of n. All of these assumptions are standard1.

1. We clarify both the data and the noise model more concretely in Section 2 below.

3

We will demonstrate that the true synthesis matrix A∗ can be recovered (with small error) in
a tractable manner as suﬃciently many samples are provided. Speciﬁcally, we make the following
novel contributions:

1. We propose a new algorithm that produces a coarse estimate of the synthesis matrix that
is suﬃciently close to the ground truth A∗. In contrast with previous double-sparse coding
methods (such as Sulam et al. (2016)), our algorithm is not based on alternating minimiza-
tion. Rather, it builds upon spectral initialization-based ideas that have recently gained
popularity in non-convex machine learning (Zhang et al., 2016; Wang et al., 2016).

2. Given the above coarse estimate of the synthesis matrix A∗, we propose a descent-style algo-
rithm to reﬁne the above estimate of A∗. This algorithm is simpler than previously studied
double-sparse coding algorithms (such as the Trainlets approach of Sulam et al. (2016)), while
still giving good statistical performance. Moreover, this algorithm can be realized in a manner
amenable to neural implementations.

3. We provide a rigorous analysis of both algorithms. Put together, our analysis produces
the ﬁrst provably polynomial-time algorithm for double-sparse coding. We show that the
algorithm provably returns a good estimate of the ground-truth; in particular, in the absence
of noise we prove that Ω(mr polylog n) samples are suﬃcient for a good enough initialization
in the ﬁrst algorithm, as well as guaranteed linear convergence of the descent phase up to a
precise error parameter that can be interpreted as the radius of convergence.

Indeed, our analysis shows that employing the double-sparsity model helps in this context,
and leads to a strict improvement in sample complexity, as well as running time over previous
rigorous methods for (regular) sparse coding such as Arora et al. (2015).

4. We also analyze our approach in a more realistic setting with the presence of additive noise
and demonstrate its stability. We prove that Ω(mr polylog n) samples are suﬃcient to obtain
a good enough estimate in the initialization, and also to obtain guaranteed linear convergence
during descent to provably recover A∗.

5. We underline the beneﬁt of the double-sparse structure over the regular model by analyzing
the algorithms in Arora et al. (2015) under the noisy setting. As a result, we obtain the
, which demonstrates a negative eﬀect of noise
sample complexity O
on this approach.

mn2
k )polylog n

(mk + σ2
ε

(cid:0)

(cid:1)

6. We rigorously develop a hard thresholding intialization that extends the spectral scheme
in Arora et al. (2015). Additionally, we provide more results for the case where A is orthonor-
mal, sparse dictionary to relax the condition on r, which may be of independent interest.

7. While our analysis mainly consists of suﬃciency results and involves several (absolute) un-
speciﬁed constants, in practice we have found that these constants are reasonable. We justify
our observations by reporting a suite of numerical experiments on synthetic test datasets.

Overall, our approach results in strict improvement in sample complexity, as well as running
time, over previous rigorously analyzed methods for (regular) sparse coding, such as Arora et al.
(2015). See Table 1 for a detailed comparison.

4

Setting

Reference

Sample complexity
(w/o noise)

Sample complexity
(w/ noise)

Upper bound on
running time

Expt

Regular

MOD (Engan et al., 1999)

K-SVD (Aharon et al., 2006)

Spielman et al. (2012)

Arora et al. (2014)

Gribonval et al. (2015a)

Arora et al. (2015)

Double Sparsity (Rubinstein et al., 2010b)

Double
Sparse

Gribonval et al. (2015b)

Trainlets (Sulam et al., 2016)

✗

✗

✗

✗

O(n2 log n)

e
O(m2/k2)

O(nm3)

e
O(mk)

e
O(mr)

e
O(mr)

✗

✗

✗

✗

✗

✗

✗

O(nm3)

e
O(mr)

e
Ω(n4)

e
O(np2)

e
O(mn2p)

✗

✗

✗

✗

✗

✗

✓

✓

✓

✗

✗

✗

✓

✗

✓

✓

This paper

e
O(mr + σ2
ε

mnr
k )

e
O(mnp)

Table 1: Comparison of various sparse coding techniques. Expt: whether numerical experiments have
been conducted. ✗ in all other columns indicates no provable guarantees. Here, n is the signal
dimension, and m is the number of atoms. The sparsity levels for A and x are r and k respectively,
and p is the sample size.

1.3 Techniques

At a high level, our method is an adaptation of the seminal approach of Arora et al. (2015). As
is common in the statistical learning literature, we assume a “ground-truth” generative model for
the observed data samples, and attempt to estimate the parameters of the generative model given
a suﬃcient number of samples. In our case, the parameters correspond to the synthesis matrix A∗,
which is column-wise r-sparse. The natural approach is to formulate a loss function in terms of A
such as Equation (1), and perform gradient descent with respect to the surface of the loss function
to learn A∗.

The key challenge in sparse coding is that the gradient is inherently coupled with the codes of the
training samples (i.e., the columns of X ∗), which are unknown a priori. However, the main insight
of Arora et al. (2015) is that within a small enough neighborhood of A∗, a noisy version of X ∗ can be
estimated, and therefore the overall method is similar to performing approximate gradient descent.
Formulating the actual algorithm as a noisy variant of approximate gradient descent allows us to
overcome the ﬁnite-sample variability of the loss, and obtain a descent property directly related to
(the population parameter) A∗.

The second stage of our approach (i.e., our descent-style algorithm) leverages this intuition.
However, instead of standard gradient descent, we perform approximate projected gradient descent,
such that the column-wise r-sparsity property is enforced in each new estimate of A∗. Indeed, such
an extra projection step is critical in showing a sample complexity improvement over the existing
approach of Arora et al. (2015).The key novelty is in ﬁguring out how to perform the projection in
each gradient iteration. For this purpose, we develop a novel initialization algorithm that identiﬁes
the locations of the non-zeroes in A∗ even before commencing the descent phase. This is nontrivially

5

diﬀerent from initialization schemes used in previous rigorous methods for sparse coding, and the
analysis is somewhat more involved.

In Arora et al. (2015), (the principal eigenvector of) a weighted covariance matrix of y (esti-
mated by the weighted average of outer products yiyT
i ) is shown to provide a coarse estimate of
a dictionary atom. We extend this idea and rigoriously show that the diagonal of the weighted
covariance matrix serves as a good indicator of the support of a column in A∗. The success relies on
the concentration of the diagonal vector with dimension n, instead of the covariance matrix with
n. With the support selected, our scheme only utilizes a reduced weighted covariance
dimensions n
matrix with dimensions at most r
r. This initialization scheme enables us to eﬀectively reduce
the dimension of the problem, and therefore leads to signiﬁcant improvement in sample complexity
and running time over previous (provable) sparse coding methods when the data representation
sparsity k is much smaller than m.

×

×

Further, we rigorously analyze the proposed algorithms in the presence of noise with a bounded
expected norm. Our analysis shows that our method is stable, and in the case of i.i.d. Gaussian noise
with bounded expected ℓ2-norms, is at least a polynomial factor better than previous polynomial
time algorithms for sparse coding.

The empirical performance of our proposed method is demonstrated by a suite of numerical
experiments on synthetic datasets.In particular, we show that our proposed methods are simple
and practical, and improve upon previous provable algorithms for sparse coding.

1.4 Paper Organization

The remainder of this paper is organized as follows. Section 2 introduces notation, key model
assumptions, and informal statements of our main theoretical results. Section 3 outlines our ini-
tialization algorithm (along with supporting theoretical results) while Section 4 presents our descent
algorithm (along with supporting theoretical results). Section 5 provides a numerical study of the
eﬃciency of our proposed algorithms, and compares it with previously proposed methods. Finally,
Section 6 concludes with a short discussion. All technical proofs are relegated to the appendix.

2. Setup and Main Results

∈

= 0
}

}
[m] : xi 6

as the support set of x. Given any subset S

for any integer m > 1. For any vector x = [x1, x2, . . . , xm]T

2.1 Notation
Rm,
We deﬁne [m] ,
1, . . . , m
∈
{
we write supp(x) ,
[m], xS
i
{
m, we
corresponds to the sub-vector of x indexed by the elements of S. For any matrix A
×
i and AT
to represent the i-th column and the j-th row respectively. For some appropriate
use A
j
•
•
S ) be the submatrix of A with rows (respectively columns)
(respectively, A
sets R and S, let AR
•
•
i, we use AR,i
indexed by the elements in R (respectively S). In addition, for the i-th column A
•
to denote the sub-vector indexed by the elements of R. For notational simplicity, we use AT
to
R
indicate (AR
) to represent
and sgn(
·
•
the element-wise Hadamard operator and the element-wise sign function respectively. Further,
thresholdK (x) is a thresholding operator that replaces any elements of x with magnitude less than
K by zero.

)T , the tranpose of A after a row selection. Besides, we use

⊆
Rn

∈

◦

•

x

The ℓ2-norm

A
for a vector x and the spectral norm
for a matrix A appear several
k
k
,
times. In some cases, we also utilize the Frobenius norm
kF and the operator norm
A
k1,2 is essentially the maximal Euclidean norm of any column of A.
A
max
k

. The norm
k

A
k

k1,2

Ax

1k

k1≤

k

k

k

x

k

6

Ω(g(n))) if f (n) is upper bounded (respectively,
constant. Next, f (n) = Θ(g(n)) if and only if f (n) = O(g(n)) and f (n) = Ω(g(n)). Also
and
f (n) = o(g(n)) (or f (n) = ω(g(n))) if limn

For clarity, we adopt asymptotic notations extensively. We write f (n) = O(g(n)) (or f (n) =
lower bounded) by g(n) up to some positive
Ω
O represent Ω and O up to a multiplicative poly-logarithmic factor respectively. Finally
e
f (n)/g(n)
|
Throughout the paper, we use the phrase “with high probability” (abbreviated to w.h.p.) to
ω(1). In addition, g(n) = O∗(f (n))

describe an event with failure probability of order at most n−
means g(n)

Kf (n) for some small enough constant K.

f (n)/g(n)
|

= 0 (limn

→∞ |

→∞ |

∞

=

).

e

≤

2.2 Model

Suppose that the observed samples are given by

y(i) = Dx∗

(i) + ε,

i = 1, . . . , p,

i.e., we are given p samples of y generated from a ﬁxed (but unknown) dictionary D where the sparse
speciﬁed below. In the double-sparse
code x∗ and the error ε are drawn from a joint distribution
n is a known
setting, the dictionary is assumed to follow a decomposition D = ΦA∗, where Φ
orthonormal basis matrix and A∗ is an unknown, ground truth synthesis matrix. An alternative
(and interesting) setting is an overcomplete Φ with a square A∗, which our analysis below does not
cover; we defer this to future work. Our approach relies upon the following assumptions on the
synthesis dictionary A∗:

Rn

D

∈

×

A1 A∗ is overcomplete (i.e., m

n) with m = O(n).

≥

A2 A∗ is µ-incoherent, i.e., for all i

= j,

A∗
i, A∗
j i| ≤
•
•

|h

µ/√n.

A3 A∗
i has at most r non-zero elements, and is normalized such that
•
A∗ij| ≥

= 0 and τ = Ω(1/√r).

τ for A∗ij 6

|

A∗
ik
k
•

= 1 for all i. Moreover,

A4 A∗ has bounded spectral norm such that

A∗k ≤
k

O(

m/n).

p

All these assumptions are standard. In Assumption A2, the incoherence µ is typically of order
O(log n) with high probability for a normal random matrix (Arora et al., 2014). Assumption A3
is a common assumption in sparse signal recovery. The bounded spectral norm assumption is
In addition to Assumptions A1-A4, we make the following
also standard (Arora et al., 2015).
distributional assumptions on

:

D

B1 Support S = supp(x∗) is of size at most k and uniformly drawn without replacement from

[m] such that P[i

S] = Θ(k/m) and P[i, j

S] = Θ(k2/m2) for some i, j

[m] and i

= j.

∈
B2 The nonzero entries x∗S are pairwise independent and sub-Gaussian given the support S with

∈

∈

E[x∗i |
i
B3 For i

∈

S] = 0 and E[x∗
2
i

S] = 1.

i
|

∈

S,

x∗i | ≥
|

∈

C where 0 < C

1.

≤

B4 The additive noise ε has i.i.d. Gaussian entries with variance σ2

ε with σε = O(1/√n).

7

For the rest of the paper, we set Φ = In, the identity matrix of size n. This only simpliﬁes the

arguments but does not change the problem because one can study an equivalent model:

y′ = Ax∗ + ε′,

where y′ = ΦT y and ε′ = ΦT ε, as ΦT Φ = In. Due to the Gaussianity of ε, ε′ also has independent
entries. Although this property is speciﬁc to Gaussian noise, all the analysis carried out below
can be extended to sub-Gaussian noise with minor (but rather tedious) changes in concentration
arguments.

Our goal is to devise an algorithm that produces an provably “good” estimate of A∗. For this,
we need to deﬁne a suitable measure of “goodness”. We use the following notion of distance that
measures the maximal column-wise diﬀerence in ℓ2-norm under some suitable transformation.

Deﬁnition 1 ((δ, κ)-nearness). A is said to be δ-close to A∗ if there is a permutation π : [m]
and a sign ﬂip σ : [m] :
to be (δ, κ)-near to A∗ if

[m]
δ for every i. In addition, A is said

→

σ(i)A
A∗
π(i) −
ik ≤
•
•
also holds.
A∗k

k
κ
k

such that
1
}
{±
A∗k ≤
A
π −
k
•

For notational simplicity, in our theorems we simply replace π and σ in Deﬁnition 1 with the
identity permutation π(i) = i and the positive sign σ(
) = +1 while keeping in mind that in reality
·
we are referring to one element of the equivalence class of all permutations and sign ﬂip transforms
of A∗.

We will also need some technical tools from Arora et al. (2015) to analyze our gradient descent-
Rn to optimize
style method. Consider any iterative algorithm that looks for a desired solution z∗ ∈
some function f (z). Suppose that the algorithm produces a sequence of estimates z1, . . . , zs via
the update rule:

zs+1 = zs

ηgs,

−

for some vector gs and scalar step size η. The goal is to characterize “good” directions gs such that
the sequence converges to z∗ under the Euclidean distance. The following gives one such suﬃcient
condition for gs.
Deﬁnition 2. A vector gs at the sth iteration is (α, β, γs)-correlated with a desired solution z∗ if

gs, zs

h

z∗

−

i ≥

zs

α
k

z∗

2 + β
k

gs
k

2
k

−

−

γs.

We know from convex optimization that if f is 2α-strongly convex and 1/2β-smooth, and gs is
∇zf (z), then gs is (α, β, 0)-correlated with z∗. In our setting, the desired
chosen as the gradient
solution corresponds to A∗, the ground-truth synthesis matrix. In Arora et al. (2015), it is shown
that gs = Ey[(Asx
y)sgn(x)T ], where x = thresholdC/2((As)T y) indeed satisﬁes Deﬁnition 2. This
gs is a population quantity and not explicitly available, but one can estimate such gs using an
gs is a random variable, so we also need a related
empirical average. The corresponding estimator
correlated-with-high-probability condition:

−

Deﬁnition 3. A direction
tion z∗ if, w.h.p.,

gs at the sth iteration is (α, β, γs)-correlated-w.h.p. with a desired solu-

gs, zs

b
h

z∗

−

i ≥

zs

α
k

z∗

2 + β
k

gs
k

2
k

−

−

γs.

From Deﬁnition 2, one can establish a form of descent property in each update step, as shown

b

b

in Theorem 1.

b

8

Theorem 1. Suppose that gs satisﬁes the condition described in Deﬁnition 2 for s = 1, 2, . . . , T .
Moreover, 0 < η

s=1 γs. Then, the following holds for all s:

2β and γ = maxT

≤

zs+1

k

z∗

2
k

−

(1

2αη)
k

−

≤

zs

z∗

2 + 2ηγs.
k

−

In particular, the above update converges geometrically to z∗ with an error γ/α. That is,

−
We can obtain a similar result for Deﬁnition 3 except that

−

≤

−

k

zs+1
k

2

z∗

(1

2αη)s

z0
k

z∗

2 + 2γ/α.
k

expectation.

zs+1

k

z∗k

−

2 is replaced with its

Armed with the above tools, we now state some informal versions of our main results:

Theorem 2 (Provably correct initialization, informal). There exists a neurally plausible algorithm
to produce an initial estimate A0 that has the correct support and is (δ, 2)-near to A∗ with high
probability.
O(mr) respectively. This
algorithm works when the sparsity level satisﬁes r = O∗(log n).

Its running time and sample complexity are

O(mnp) and

e

e

Our algorithm can be regarded as an extension of Arora et al. (2015) to the double-sparse
setting. It reconstructs the support of one single column and then estimates its direction in the
subspace deﬁned by the support. Our proposed algorithm enjoys neural plausibility by implement-
ing a thresholding non-linearity and Oja’s update rule. We provide a neural implementation of our
algorithm in Appendix G. The adaption to the sparse structure results in a strict improvement
upon the original algorithm both in running time and sample complexity. However, our algorithm
is limited to the sparsity level r = O∗(log n), which is rather small but plausible from the modeling
standpoint. For comparison, we analyze a natural extension of the algorithm of Arora et al. (2015)
with an extra hard-thresholding step for every learned atom. We obtain the same order restriction
on r, but somewhat worse bounds on sample complexity and running time. The details are found
in Appendix F.

We hypothesize that a stronger incoherence assumption can lead to provably correct initial-
ization for a much wider range of r. For purposes of theoretical analysis, we consider the special
case of a perfectly incoherent synthesis matrix A∗ such that µ = 0 and m = n. In this case, we
, which is an exponential
can indeed improve the sparsity parameter to r = O∗
improvement. This analysis is given in Appendix E.

min( √n
log2 n

n
k2 log2 n

)

,

(cid:0)

(cid:1)

Theorem 3 (Provably correct descent, informal). There exists a neurally plausible algorithm for
double-sparse coding that converges to A∗ with geometric rate when the initial estimate A0 has the
correct support and (δ, 2)-near to A∗. The running time per iteration is O(mkp + mrp) and the
sample complexity is

O(m + σ2
ε

mnr
k ).

e

Similar to Arora et al. (2015), our proposed algorithm enjoys neural plausibility. Moreover, we
can achieve a better running time and sample complexity per iteration than previous methods,
particularly in the noisy case. We show in Appendix F that in this regime the sample complexity
O(m + σ2
1/2, the sample complexity
of Arora et al. (2015) is
ε
O(m) in the noiseless case. In contrast, our proposed method
bound is signiﬁcantly worse than
leverages the sparse structure to overcome this problem and obtain improved results.

k ). For instance, when σε ≍

n−

mn2

e

We are now ready to introduce our methods in detail. As discussed above, our approach consists
of two stages: an initialization algorithm that produces a coarse estimate of A∗, and a descent-style
algorithm that reﬁnes this estimate to accurately recover A∗.

e

9

Algorithm 1 Truncated Pairwise Reweighting

∅

Initialize L =
Randomly divide p samples into two disjoint sets
While
P1 at random

< m. Pick u and v from

For every l = 1, 2, . . . , n; compute

L
|

|

P1 and

P2 of sizes p1 and p2 respectively

p2

el =

1
p2

y(i), u
h

ih

y(i), v

(y(i)

l )2

i

e(r′) < O∗(r/ log2 n)

Xi=1
b
en) in descending order

e2, . . . ,

e(r′+1)/

e(r′) ≥
b

Sort (
e1,
If r′ ≤
O(k/mr) and
r s.t
R be set of the r largest entries of
Let
b
b
(y(i)
y(i)
Mu,v = 1
y(i), v
b
b
b
b
b
p2
i
R
R
b
Mu,v
top singular values of
δ1, δ2 ←
c
z bR ←
If δ1 ≥

top singular vector of

Ω(k/m) and δ2 < O∗(k/m log n)

Mu,v
c

p2
i=1h

y(i), u

e
)T
b

P

c

ih

If dist(

z, l) > 1/ log n for any l

L

∈

±

Update L = L

z
∪ {

}

Return A0 = (L1, . . . , Lm)

3. Stage 1: Initialization

In this section, we present a neurally plausible algorithm that can produce a coarse initial estimate
of the ground truth A∗. We give a neural implementation of the algorithm in Appendix G.

Our algorithm is an adaptation from the algorithm in Arora et al. (2015). The idea is to estimate
dictionary atoms in a greedy fashion by iteratively re-weighting the given samples. The samples
are re-scaled in a way that the weighted (sample) covariance matrix has the dominant ﬁrst singular
value, and its corresponding eigenvector is close to one particular atom with high probability.
However, while this algorithm is conceptually very appealing, it incurs severe computational costs
O(mn2p) in expectation, which is unrealistic
in practice. More precisely, the overall running time is
for large-scale problems.

e

To overcome this burden, we leverage the double-sparsity assumption in our generative model to
obtain a more eﬃcient approach. The high-level idea is to ﬁrst estimate the support of each column
in the synthesis matrix A∗, and then obtain a coarse estimate of the nonzero coeﬃcients of each
column based on knowledge of its support. The key ingredient of our method is a novel spectral
procedure that gives us an estimate of the column supports purely from the observed samples. The
full algorithm, that we call Truncated Pairwise Reweighting, is listed in pseudocode form below as
Algorithm 1.

Let us provide some intuition of our algorithm. Fix a sample y = A∗x∗ + ε from the available

training set, and consider samples

u = A∗α + εu, v = A∗α′ + εv.

10

Now, consider the (very coarse) estimate for the sparse code of u with respect to A∗:

β = A∗

T u = A∗

T A∗α + A∗

T εu.

y, u
h

x∗, β

x∗, α
i

.

i ≈ h

i ≈ h

As long as A∗ is incoherent enough and εu is small, the estimate β behaves just like α, in the sense
that for each sample y:

Moreover, the above inner products are large only if α and x∗ share some elements in their supports;
depends on whether or not x∗
else, they are likely to be small. Likewise, the weight
shares the support with both α and α′.

y, u
h

y, v

ih

i

i

ih

y, v

y, u
h

Now, suppose that we have a mechanism to isolate pairs u and v who share exactly one atom
among their sparse representations. Then by scaling each sample y with an increasing function
of
and linearly adding the samples, we magnify the importance of the samples that
are aligned with that atom, and diminish the rest. The ﬁnal direction can be obtained via the
top principal component of the reweighted samples and hence can be used as a coarse estimate of
the atom. This is exactly the approach adopted in Arora et al. (2015). However, in our double-
sparse coding setting, we know that the estimated atom should be sparse as well. Therefore, we
can naturally perform an extra “sparsiﬁcation” step of the output. An extended algorithm and
its correctness are provided in Appendix F. However, as we discussed above, the computational
complexity of the re-weighting step still remains.

We overcome this obstacle by ﬁrst identifying the locations of the nonzero entries in each atom.

Speciﬁcally, deﬁne the matrix:

Mu,v =

p2

1
p2

y(i), u
h
Xi=1

ih

y(i), v

y(i)

y(i).

i

◦

Then, the diagonal entries of Mu,v reveals the support of the atom of A∗ shared among u and v: the
r-largest entries of Mu,v will correspond to the support we seek. Since the desired direction remains
unchanged in the r-dimensional subspace of its nonzero elements, we can restrict our attention to
Mu,v, and proceed as before. This truncation
this subspace, construct a reduced covariance matrix
O(mnp),
step alleviates the computational burden by a signiﬁcant amount; the running time is now
which improves the original by a factor of n.

c

The success of the above procedure relies upon whether or not we can isolate pairs u and v that
share one dictionary atom. Fortunately, this can be done via checking the decay of the singular
values of the (reduced) covariance matrix. Here too, we show via our analysis that the truncation
step plays an important role. Overall, our proposed algorithm not only accelerates the initialization
in terms of running time, but also improves the sample complexity over Arora et al. (2015). The
performance of Algorithm 1 is described in the following theorem, whose formal proof is deferred
to Appendix B.

e

√n
k log3 n

Theorem 4. Suppose that Assumptions B1-B4 hold and Assumptions A1-A3 satify with µ =
Ω(mr), then with high probability
O∗
Algorithm 1 returns an initial estimate A0 whose columns share the same support as A∗ and with
(δ, 2)-nearness to A∗ with δ = O∗(1/ log n).

and r = O∗(log n). When p1 =

Ω(m) and p2 =

e

e

(cid:1)

(cid:0)

The limit on r arises from the minimum non-zero coeﬃcient τ of A∗. Since the columns of A∗
are standardized, τ should degenerate as r grows. In other words, it is getting harder to distinguish

11

the “signal” coeﬃcients from zero as r grows with n. However, this limitation can be relaxed when
a better incoherence available, for example the orthonormal case. We study this in Appendix E.

To provide some intuition about the working of the algorithm (and its proof), let us analyze it in
the case where we have access to inﬁnite number of samples. This setting, of course, is unrealistic.
However, the analysis is much simpler and more transparent since we can focus on expected values
rather than empirical averages. Moreover, the analysis reveals several key lemmas, which we will
reuse extensively for proving Theorem 4. First, we give some intuition behind the deﬁnition of the
“scores”,

el.

Lemma 1. Fix samples u and v and suppose that y = A∗x∗ + ε is a random sample independent
of u, v. The expected value of the score for the lth component of y is given by:

b

el , E[
h

y, u

y, v

ih

i

y2
l ] =

2
li + perturbation terms
qiciβiβ′iA∗

where qi = P[i
absolute value at most O∗(k/m log n).

S], qij = P[i, j

∈

∈

V
U
Xi
∩
∈
S] and ci = E[x4
i
i |

∈

S]. Moreover, the perturbation terms have

i

From Assumption B1, we know that qi = Θ(k/m), qij = Θ(k2/m2) and ci = Θ(1). Besides,
= o(1) for i /
U . Consider the ﬁrst
∈
or that l does not belong to support
V =
τ .

= Ω(1) for i
we will show later that
βi| ≈ |
|
2
li . Clearly, E0 = 0 if U
V qiciβiβ′iA∗
term E0 =
U
∩
∈
of any atom in U
Ω(τ 2k/m) = Ω(k/mr) since

αi|
∈
V . On the contrary, as E0 6
qiciβiβ′i| ≥
|

βi|
|
V =
∩
∅
= 0 and U
∩
A∗li| ≥
|

Therefore, Lemma 1 suggests that if u and v share a unique atom among their sparse represen-
tations, and r is not too large, then we can indeed recover the correct support of the shared atom.
When this is the case, the expected scores corresponding to the nonzero elements of the shared
atom will dominate the remaining of the scores.

2
qiciβiβ′iA∗
li | ≥
|

, then E0 =

Ω(k/m) and

U , and

i
}
{

P

∩

Now, given that we can isolate the support R of the corresponding atom, the remaining questions
are how best we can estimate its non-zero coeﬃcients, and when u and v share a unique elements
in their supports. These issues are handled in the following lemmas.

Lemma 2. Suppose that u = A∗α + εu and v = A∗α′ + εv are two random samples. Let U and
V denote the supports of α and α′ respectively. R is the support of some atom of interest. The
truncated re-weighting matrix is formulated as

Mu,v , E[
y, u
h

ih

y, v

yRyT

R] =

i

T
R,i + perturbation terms
qiciβiβ′iA∗R,iA∗

V
U
Xi
∩
∈

where the perturbation terms have norms at most O∗(k/m log n).

Using the same argument for bounding E0 in Lemma 1, we can see that M0 , qiciβiβ′iA∗R,iA∗
T
R,i
has norm at least Ω(k/m) when u and v share a unique element i (
= 1). According to this
k
lemma, the spectral norm of M0 dominates those of the other perturbation terms. Thus, given R
i.
we can use the ﬁrst singular vector of Mu,v as an estimate of A∗
•

A∗R,ik

Lemma 3. Under the setup of Theorem 4, suppose u = A∗α + εu and v = A∗α′ + εv are two
random samples with supports U and V respectively. R = supp(A∗i ). If u and v share the unique
atom i, the ﬁrst r largest entries of el is at least O(k/mr) and belong to R. Moreover, the top
singular vector of Mu,v is δ-close to A∗R,i for O∗(1/ log n).

12

i’s support directly follows Lemma 1. For the latter part, recall from
Proof. The recovery of A∗
•
Lemma 2 that

T
R,i + perturbation terms
Mu,v = qiciβiβ′iA∗R,iA∗

The perturbation terms have norms bounded by O∗(k/m log n). On the other hand, the ﬁrst term
is has norm at least Ω(k/m) since
Ω(k/m).
Then using Wedin’s Theorem to Mu,v, we can conclude that the top singular vector must be
O∗(k/m log n)/Ω(k/m) = O∗(1/ log n) -close to A∗R,i.

= 1 for the correct support R and

qiciβiβ′i| ≥

A∗R,ik

k

|

Lemma 4. Under the setup of Theorem 4, suppose u = A∗α+εu and v = A∗α′ +εv are two random
samples with supports U and V respectively. If the top singular value of Mu,v is at least Ω(k/m) and
the second largest one is at most O∗(k/m log n), then u and v share a unique dictionary element
with high probability.

Proof. The proof follows from that of Lemma 37 in Arora et al. (2015). The main idea is to
separate the possible cases of how u and v share support and to use Lemma 2 with the bounded
perturbation terms to conclude when u and v share exactly one. We note that due to the condition
where
O∗(r/ log n), it must be the case that u and v share only
one atom or share more than one atoms with the same support. When their supports overlap more
than one, then the ﬁrst singular value cannot dominate the second one, and hence it must not be
b
the case.

Ω(k/mr) and

e(s) ≤

e(s) ≥

e(s+1)/

b

b

Similar to (Arora et al., 2015), our initialization algorithm requires

tation to estimate all the atoms, hence the expected running time is
Lemma 1 and 2 are deferred to Appendix B.

O(m) iterations in expec-
O(mnp). All the proofs of

e

e

4. Stage 2: Descent

We now adapt the neural sparse coding approach of Arora et al. (2015) to obtain an improved
estimate of A∗. As mentioned earlier, at a high level the algorithm is akin to performing approximate
gradient descent. The insight is that within a small enough neighborhood (in the sense of δ-
closeness) of the true A∗, an estimate of the ground-truth code vectors, X ∗, can be constructed
using a neurally plausible algorithm.

The innovation, in our case, is the double-sparsity model since we know a priori that A∗ is itself
sparse. Under suﬃciently many samples, the support of A∗ can be deduced from the initialization
stage; therefore we perform an extra projection step in each iteration of gradient descent. In this
sense, our method is non-trivially diﬀerent from Arora et al. (2015). The full algorithm is presented
as Algorithm 2.

As discussed in Section 2, convergence of noisy approximate gradient descent can be achieved
gs is correlated-w.h.p. with the true solution. However, an analogous convergence result
as long as
for projected gradient descent does not exist in the literature. We ﬁll this gap via a careful analysis.
gs (i.e., when it
Due to the projection, we only require the correlated-w.h.p. property for part of
is restricted to some support set) with A∗. The descent property is still achieved via Theorem 5.
(A, X); therefore, we can
Due to various perturbation terms,
∇AL
k/n). The performance
only reﬁne the estimate of A∗ until the column-wise error is of order O(
of Algorithm 2 can be characterized via the following theorem.
b

g is only a biased estimate of

b

b

p

13

Algorithm 2 Double-Sparse Coding Descent Algorithm

Initialize A0 is (δ, 2)-near to A∗. H = (hij )n
Repeat for s = 0, 1, . . . , T

×

m where hij = 1 if i

supp(A0
j) and 0 otherwise.
•

∈

Encode: x(i) = thresholdC/2((As)T y(i))
PH(As
Update: As+1 =
−
p
i=1(Asx(i)
where

gs = 1
p

η

η

gs)
gs) = As
−
y(i))sgn(x(i))T and
−
b
b

for i = 1, 2, . . . , p
PH (

PH (G) = H

◦

G

P

b

Theorem 5. Suppose that the initial estimate A0 has the correct column supports and is (δ, 2)-near
to A∗ with δ = O∗(1/ log n). If Algorithm 2 is provided with p =
Ω(mr) fresh samples at each step
and η = Θ(m/k), then

E[
As
i −
k
•

A∗
ik
•

2]

(1

ρ)s

A0
i −
•

A∗
ik
•

k
for some 0 < ρ < 1/2 and for s = 1, 2, . . . , T . Consequently, As converges to A∗ geometrically until
column-wise error O(

k/n).

p

≤

−

e
2 + O(

k/n)

p

We defer the full proof of Theorem 5 to Section D. In this section, we take a step towards
gs in the inﬁnite sample case, which is equivalent to its
y)sgn(x)T ]. We establish the (α, β, γs)-correlation of a truncated version

understanding the algorithm by analyzing
expectation gs , E[(Asx
of gs
i with A∗
i to obtain the descent in Theorem 6 for the inﬁnite sample case.
•
•

Theorem 6. Suppose that the initial estimate A0 has the correct column supports and is (δ, 2)-near
to A∗. If Algorithm 2 is provided with inﬁnite number of samples at each step and η = Θ(m/k),
then

−

b

ρ)
k
for some 0 < ρ < 1/2 and for s = 1, 2, . . . , T . Consequently, it converges to A∗ geometrically until
column-wise error is O(k/n).

A∗
ik
•

A∗
ik
•

(1

≤

−

k

(cid:0)

(cid:1)

As+1
i −
•

2 + O

k2/n2

As
i −
•

2

Note that the better error O(k2/n2) is due to the fact that inﬁnitely many samples are given.
k/n) in Theorem 5 is a trade-oﬀ between the accuracy and the sample complexity
The term O(
of the algorithm. The proof of this theorem composes of two steps with two main results: 1) an
explicit form of gs (Lemma 6); 2) (α, β, γs)-correlation of column-wise gs with A∗ (Lemma 6). The
proof of those lemmas are deferred to Appendix C. Since the correlation primarily relies on the
(δ, 2)-nearness of As to A∗ that is provided initially and maintained at each step, then we need to
argue that the nearness is preserved after each step.

p

Lemma 5. Suppose that the initial estimate A0 has the correct column supports and is (δ, 2)-near to
A∗. The column-wise update has the form gs
R,i = piqi(λs
ζ) where R = supp(As
i),
•
λs
i =

A∗R,i + ξs

i As

i ±

and

As
i, A∗
ii
h
•
•

i = As
ξs
R,

idiag(qij)(As
•−

R,i −
i)T A∗
i/qi.
•

−
Moreover, ξi has norm bounded by O(k/n) for δ = O∗(1/ log n) and ζ is negligible.

We underline that the correct support of As allows us to obtain the closed-form expression of
gs
i. Likewise, the expression (8) suggests that gs
Ri,i in terms of As
i is almost equal to
i and A∗
•
•
•
i) (since λs
piqi(As
i. With Lemma 5, we will
1), which directs to the desired solution A∗
A∗
i ≈
i −
•
•
•
i and the nearness
prove the (α, β, γs)-correlation of the approximate gradient to each column A∗
•
of each new update to the true solution A∗.

14

4.1 (α, β, γs)-Correlation
Lemma 6. Suppose that As to be (δ, 2)-near to A∗ and R = supp(A∗
i), then 2gs
•
correlated with A∗R,i; that is

R,i is (α, 1/2α, ǫ2/α)-

A∗R,ii ≥
Futhermore, the descent is achieved by

R,i −

R,i, As

2gs
h

As
α
k

R,i −

A∗R,ik

2 + 1/(2α)

2
gs
R,ik
k

−

ǫ2/α

As+1
i −
k
•

2

(1

≤

−

2αη)s

A0
i −
k
•

A∗
ik
•

2 + ηǫ2

s/α

where δ = O∗(1/ log n) and ǫ = O

.

A∗
ik
•
k2
mn

(cid:0)

(cid:1)

Proof. Throughout the proof, we omit the superscript s for simplicity and denote 2α = piqi. First,
i as a combination of the true direction As
we rewrite gs
i −
•
•

i and a term with small norm:
A∗
•

gR,i = 2α(AR,i −

A∗R,i) + v,

i + ǫi] with norm bounded. In fact, since A
where v = 2α[(λi −
i, and both
i is δ-close to A∗
1)A
•
•
•
A
2α(λi −
have unit norm, then
A∗
A
O(k/n) from
i −
i −
ik
•
•
•
the inequality (9). Therefore,

1)A
ik
•

ξik ≤

= α
k

A∗
ik
•

α
k

and

≤

k

k

2

v
k

k

=

2α(λi −

1)AR,i + 2αξik ≤

k

α
k

AR,i −

A∗R,ik

+ ǫ

where ǫ = O(k2/mn). Now, we make use of (2) to show the ﬁrst part of Lemma 6:

2gR,i, AR,i −

h

A∗R,ii

= 4α
k

AR,i −

A∗R,ik

2v, AR,i −

h

.
A∗R,ii

2 +

We want to lower bound the inner product term with respect to
Eﬀectively, from (2)

2 and

gRi,ik

k

AR,i −
k

2.
A∗R,ik

4α
h

v, A
i −
•

A∗
ii
•

=

2

2

gR,ik
k
gR,ik

−

−

4α2
6α2

AR,i −
AR,i −

k

k

≥ k

2

v
k
− k
2ǫ2,

−

2

2

A∗R,ik
A∗R,ik
2(α2

2

where the last step is due to Cauchy-Schwarz inequality:

v
k
in (3) for the right hand side of (4), we get the ﬁrst result:

AR,i −
k

A∗R,ik

≤

k

2 + ǫ2).

Substitute 2
h

v, A
i −
•

A∗
ii
•

2gR,i, AR,i −
h

A∗R,ii ≥

α
k

AR,i −

A∗R,ik

2 +

1
2α k

2

gR,ik

−

ǫ2
α

.

(2)

(3)

(4)

The second part is directly followed from Theorem 1. Moreover, we have pi = Θ(k/m) and qi =
Θ(1), then α = Θ(k/m), β = Θ(m/k) and γs = O(k3/mn2). Then gs
correlated with the true solution A∗R,i.

R,i is (Ω(k/m), Ω(m/k), O(k3/mn2))-

Proof of Theorem 6. The descent in Theorem 6 directly follows from the above lemma. Next, we
will establish the nearness for the update at step s:

15

2,000
Sample size

4,000

2,000
Sample size

4,000

2,000
Sample size

4,000

e
t
a
r

y
r
e
v
o
c
e
R

e
t
a
r

y
r
e
v
o
c
e
R

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

r
o
r
r
e

n
o
i
t
c
u
r
t
s
n
o
c
e
R

r
o
r
r
e

n
o
i
t
c
u
r
t
s
n
o
c
e
R

8

6

4

2

0

8

6

4

2

0

0

0

Ours
Arora
Arora+HT
Trainlets

Ours
Arora
Arora+HT
Trainlets

e
m

i
t

g
n

i

n
n
u
R

e
m

i
t

g
n

i

n
n
u
R

4

3

2

1

0

6

4

2

0

0

0

2,000
Sample size

4,000

2,000
Sample size

4,000

2,000
Sample size

4,000

Figure 1: (top) The performance of four methods on three metrics (recovery rate, reconstruction error and
running time) in sample size in the noiseless case. (bottom) The same metrics are measured for
the noisy case.

4.2 Nearness

Lemma 7. Suppose that As is (δ, 2)-near to A∗, then

As+1

k

A∗k ≤

2
k

A∗k

−

Proof. From Lemma 5 we have gs
i = piqi(λiAs
i) + A
A∗
i −
•
•
•
idiag(qij)AT
R, then it is obvious that gs
iA∗
[n]
¯R,i = A ¯R,
i ±
\
•−
•
follows the proof of Lemma 24 in (Arora et al., 2015) for the nearness with full gs = gs
ﬁnish the proof for this lemma.

ζ. Denote ¯R =
idiag(qij)AT
iA∗
i ±
•−
•
•−
ζ is bounded by O(k2/m2). Then we
¯R,i to

R,i + gs

−

In sum, we have shown the descent property of Algorithm 2 in the inﬁnite sample case. The
gs around its mean to the sample complexity is provided in Section D.

study of the concentration of
In the next section, we corroborate our theory by some numerical results on synthetic data.

5. Empirical Study

b

We compare our method with three diﬀerent methods for both standard sparse and double-sparse
coding. For the standard approach, we implement the algorithm proposed in Arora et al. (2015),
which currently is the best theoretically sound method for provable sparse coding. However, since
their method does not explicitly leverage the double-sparsity model, we also implement a heuristic
modiﬁcation that performs a hard thresholding (HT)-based post-processing step in the initializa-
tion and learning procedures (which we dub Arora + HT ). The ﬁnal comparison is the Trainlets
approach of Sulam et al. (2016).

16

2 block is of form [1 1; 1

We generate a synthetic training dataset according to the model described in Section 2. The
base dictionary Φ is the identity matrix of size n = 64 and the square synthesis matrix A∗ is a
block diagonal matrix with 32 blocks. Each 2
1] (i.e., the column
×
sparsity r = 2) . The support of x∗ is drawn uniformly over all 6-dimensional subsets of [m],
In our simulations
and the nonzero coeﬃcients are randomly set to
with noise, we add Gaussian noise ε with entrywise variance σ2
ε = 0.01 to each of those above
samples. For all the approaches except Trainlets, we use T = 2000 iterations for the initialization
procedure, and set the number of steps in the descent stage to 25. Since Trainlets does not have
a speciﬁed initialization procedure, we initialize it with a random Gaussian matrix upon which
column-wise sparse thresholding is then performed. The learning step of Trainlets2 is executed for
50 iterations, which tolerates its initialization deﬁciency. For each Monte Carlo trial, we uniformly
draw p samples, feed these samples to the four diﬀerent algorithms, and observe their ability to
reconstruct A∗. Matlab implementation of our algorithms is available online3.

1 with equal probability.

−

±

We evaluate these approaches on three metrics as a function of the number of available samples:
(i) fraction of trials in which each algorithm successfully recovers the ground truth A∗; (ii) recon-
struction error; and (iii) running time. The synthesis matrix is said to be “successfully recovered”
if the Frobenius norm of the diﬀerence between the estimate
A and the ground truth A∗ is smaller
4 in the noiseless case, and to 0.5 in the other. All three
than a threshold which is set to 10−
metrics are averaged over 100 Monte Carlo simulations. As discussed above, the Frobenius norm is
only meaningful under a suitable permutation and sign ﬂip transformation linking
A and A∗. We
estimate this transformation using a simple maximum weight matching algorithm. Speciﬁcally, we
construct a weighted bipartite graph with nodes representing columns of A∗ and
A and adjacency
is taken element-wise. We compute the optimal matching
matrix deﬁned as G =
using the Hungarian algorithm, and then estimate the sign ﬂips by looking at the sign of the inner
products between the matched columns.

, where
A
|

T
A∗
|

|·|

b

b

b

b

The results of our experiments are shown in Figure 1 with the top and bottom rows respectively
for the noiseless and noisy cases. The two leftmost ﬁgures suggest that all algorithms exhibit a
“phase transitions” in sample complexity that occurs in the range of 500-2000 samples.
In the
noiseless case, our method achieves the phase transition with the fewest number of samples. In the
noisy case, our method nearly matches the best sample complexity performance (next to Trainlets,
which is a heuristic and computationally expensive). Our method achieves the best performance
in terms of (wall-clock) running time in all cases.

6. Conclusion

In this paper, we have addressed an open theoretical question on learning sparse dictionaries under
a special type of generative model. Our proposed algorithm consists of a novel initialization step
followed by a descent-style step, both are able to take advantage of the sparse structure. We rigor-
ously demonstrate its eﬃcacy in both sample- and computation-complexity over existing heuristics
as well as provable approaches for double-sparse and regular sparse coding. This results in the ﬁrst
known provable approach for double-sparse coding problem with statistical and algorithmic guaran-
tees. Besides, we also show three beneﬁts of our approach: neural plausibility, robustness to noise
and practical usefulness via the numerical experiments.

2. We utilize Trainlets’s implementation provided at http://jsulam.cswp.cs.technion.ac.il/home/software/.
3. https://github.com/thanh-isu/double-sparse-coding

17

Nevertheless, several fundamental questions regarding our approach remain. First, our initial-
ization method (in the overcomplete case) achieves its theoretical guarantees under fairly stringent
limitations on the sparsity level r. This arises due to our reweighted spectral initialization strategy,
and it is an open question whether a better initialization strategy exists (or whether these types
of initialization are required at all). Second, our analysis holds for complete (ﬁxed) bases Φ, and
it remains open to study the setting where Φ is over-complete. Finally, understanding the reasons
behind the very promising practical performance of methods based on heuristics, such as Trainlets,
on real-world data remains a very challenging open problem.

18

Appendix A. Auxiliary Lemma

Claim 1 (Maximal row ℓ1-norm). Given that
Θ(

m/n).

A∗k

k

p

Proof. Recall the deﬁnition of the operator norm:

2
F = m and

A∗k
k

= O(

m/n), then

T
A∗

k

k1,2 =

p

T
A∗
k

k1,2 = sup

=0

x

AT x
k
x

k

k

sup
=0
x

AT x
k
k
x
k1 ≤
k
A∗kF /√n =

k

=

T
A∗

k

k

= O(

m/n).

p

2
F = m,

T
A∗

k
m/n).

k1,2 ≥ k

Since
T
A∗
k

A∗k
k
k1,2 = Θ(
p
Along with Assumptions A1 and A3, the above claim implies the number of nonzero entries
in each row is O(r). This Claim is an important ingredient in our analysis of our initialization
algorithm shown in Section 3.

m/n. Combining with the above, we have

p

Appendix B. Analysis of Initialization Algorithm

B.1 Proof of Lemma 1

The proof of Lemma 1 can be divided into three steps: 1) we ﬁrst establish useful properties of β
with respect to α; 2) we then explicitly derive el in terms of the generative model parameters and β;
and 3) we ﬁnally bound the error terms in E based on the ﬁrst result and appropriate assumptions.

Claim 2. In the generative model,

x∗k ≤
k

O(√k) and

ε
k ≤

k

O(σε√n) with high probability.

Proof. The claim directly follows from the fact that x∗ is a k-sparse random vector whose nonzero
entries are independent sub-Gaussian with variance 1. Meanwhile, ε has n independent Gaussian
entries of variance σ2
ε .

e

e

Despite its simplicity, this claim will be used in many proofs throughout the paper. Note also
that in this section we will calculate the expectation over y and often refer probabilistic bounds
(w.h.p.) under the randomness of u and v.

Claim 3. Suppose that u = A∗α + εu is a random sample and U = supp(α). Let β = A∗
O(√k + σε√n).
w.h.p., we have (a)

√n + σε log n for each i and (b)

µk log n

βi −
|

αi| ≤

β
k

k ≤

T u, then,

Proof. The proof mostly follows from Claim 36 of Arora et al. (2015), with an additional consider-
and observe that
ation of the error εu. Write W = U

e

=

αi|

βi −
|

i εu| ≤ |h
A∗
•
T
k/n. Moreover, αW has k
µ
Since A∗ is µ-incoherence, then
i A∗
A∗
W k ≤
•
•
T
Gaussian entries of variance 1, therefore
i, αW i| ≤
W A∗
A∗
p
|h
•
•
that εu has independent Gaussian entries of variance σ2
variance (
k
µk log n

1 independent sub-
√n with high probability. Also recall
T
ε , then A∗
i εu is Gaussian with the same
•
αi| ≤

σε log n with high probability. Consequently,

= 1). Hence

i, αW i|

T
i ε
A∗
|
•

i, εui|

A∗
ik
•

βi −

µk log n

| ≤

−

+

|h

k

|

T
W A∗
A∗
•
•

√n + σε log n, which is the ﬁrst part of the claim.

Next, in order to bound

, we express β as

β
k
k
T A∗
U αU + A∗
•

β

k

k

=

A∗
k

T εuk ≤ k

A∗

A∗
U kk
•

αU k

kk

+

A∗
k

εuk

kk

i
\{
}
T
T
W αW + A∗
i A∗
A∗
|
•
•
•

19

O(√k) and

A∗k ≤

αU k ≤
k

O(1) , we complete the proof for the second part.

Using Claim 2 to get
εuk ≤
k
A∗
U k ≤ k
k
•
Claim 3 suggests that the diﬀerence between βi and αi is bounded above by O∗(1/ log2 n)
U and
o(1)
∈
O(√k) w.h.p.

w.h.p. if µ = O∗( √n
βi| ≤
|
We will use these results multiple times in the next few proofs.

O∗(1/ log2 n) otherwise. On the other hand, under Assumption B4,

O(σε√n) w.h.p., and further noticing that

). Therefore, w.h.p., C

O(log m) for i

βi| ≤ |

+ o(1)

k log3 n

αi|

β
k

k ≤

≤ |

−

≤

e

e

Proof of Lemma 1. We decompose dl into small parts so that the stochastic model

is made use.

y, v

el = E[
y, u
h
ih
= E
x∗, β
h
ih
= E1 + E2 +
(cid:2)(cid:8)

i
+ E9

· · ·

l ] = E[
y2
A∗x∗ + ε, u
A∗l
(
h
h
·
T (βvT + β′uT )ε + uT εεT v
+ x∗

A∗x∗ + ε, v

i
x∗, β′

ih

i

, x∗

+ ε)2]

i
, x∗

A∗l
h
•

2 + 2
h

A∗l
•

i

, x∗

εl + εl

i

(cid:9)(cid:8)

(cid:9)(cid:3)

where the terms are

e

D

i

ih
, x∗

2]
, x∗
i
εl]
, x∗

A∗l
•
A∗l
ih
•
ε2
l ]
i
T (βvT + β′uT )ε
T (βvT + β′uT )εεl
(cid:3)

x∗, β′
ih
x∗, β′

E1 = E[
x∗, β
ih
h
E2 = 2E[
x∗, β
h
ih
E3 = E[
x∗, β
x∗, β′
h
2x∗
E4 = E
A∗l
i
h
·
E5 = E
x∗
A∗l
(cid:2)
i
h
·
E6 = E
(βvT + β′uT )εε2
(cid:2)
l
2]
E7 = E[uT εεT v
A∗l
(cid:2)
(cid:3)
h
•
E8 = 2E[uT εεT v
εl]
A∗l
h
i
•
E9 = E[uT εεT vε2
l ]

i
, x∗

, x∗

, x∗

(cid:3)

(5)

Because x∗ and ε are independent and zero-mean, E2 and E4 are clearly zero. Moreover,

E6 = (βvT + β′uT )E[εε2

l ] = 0

due to the fact that E[εjε2

l ] = 0, for j

= l, and E[ε3

l ] = 0. Also,

We bound the remaining terms separately in the following claims.

T
E8 = A∗
l
•

E[x∗]E

uT εεT vεl

= 0.

(cid:2)

(cid:3)

Claim 4. In the decomposition (5), E1 is of the form

E1 =

2
li +
qiciβiβ′iA∗

2
li +
qiciβiβ′iA∗

2
lj + 2βiβ′jA∗liA∗lj)
qij(βiβ′iA∗

V
U
Xi
∩
∈

V
U
Xi /
∩
∈

=i
Xj

where all those terms except

li have magnitude at most O∗(k/m log2 n) w.h.p.
2
V qiciβiβ′iA∗
∩

i

U

∈

P

20

Proof. Using the generative model in Assumptions B1-B4, we have

E1 = E[
x∗, β
h
= ES
E

x∗

ih
S[

x∗, β′

A∗l
•

ih
βix∗i

, x∗

2]

i
β′ix∗i

A∗lix∗i

2]

|

(cid:2)

S
Xi
∈
2
li +
qiciβiβ′iA∗

S
Xi
∈

S

(cid:0) Xi

(cid:1)
∈
2
lj + 2βiβ′jA∗liA∗lj)
qij(βiβ′iA∗

(cid:3)

=

=

[m]
Xi
∈

[m],j
Xi,j
∈

=i

2
li +
qiciβiβ′iA∗

2
li +
qiciβiβ′iA∗

2
lj + 2βiβ′jA∗liA∗lj),
qij(βiβ′iA∗

Xi /
V
U
∩
∈
S], qij = P[i, j

V
U
Xi
∈
∩
where we have used the qi = P[i
S] and Assumptions
B1-B4. We now prove that the last three terms are upper bounded by O∗(k/m log n). The key
observation is that all these terms typically involve a quadratic form of the l-th row A∗l
whose
•
norm is bounded by O(1) (by Claim 1 and Assumption A4). Moreover,
is relatively small for
V )
(U
[m]
U
i /
\
∈
to bound

V while qij = Θ(k2/m2). For the second term, we apply the Claim 3 for i

Xj
=i
S] and ci = E[x4
i
i |

= 0, then with high probability

βiβ′i|
|

∈

∈

∈

∩

∈

∩

|

βiβ′i|

. Assume αi = 0 and α′i 6
βiβ′i| ≤ |
|
Using the bound qici = Θ(k/m), we have w.h.p.,

αi)(β′i −

(βi −

α′i)
|

+

βiα′i| ≤
|

O∗(1/ log n)

2
qiciβiβ′iA∗
li

max
i

qiciβiβ′i|
|

≤

2
A∗
li ≤

max
i

qiciβiβ′i|k
|

A∗

2
1,2 ≤

k

O∗(k/m log n).

V
U
Xi /
(cid:12)
∩
∈
(cid:12)
(cid:12)
For the third term, we make use of the bounds on
β′k ≤

V
U
Xi /
∩
∈

(cid:12)
(cid:12)
(cid:12)

β
k

kk

O(k) w.h.p., and on qij = Θ(k2/m2). More precisely, w.h.p.,

β
k

k

and

β′k
k

from the previous claim where

=i
Xj
(cid:12)
(cid:12)
(cid:12)

2
qijβiβ′iA∗
lj

e

=

βiβ′i

2
qijA∗
lj

(cid:12)
(cid:12)
(cid:12)

≤

Xi
(cid:12)
(cid:12)
(cid:12)
(max
=j
i

=i
Xj

Xi

qij)

βiβ′i|
|

|
Xi
2
A∗
lj

≤

(cid:12)
(cid:12)
(cid:12)
(cid:16)Xj

βiβ′i|

≤

(cid:17)

=i

(cid:0)Xj
(max
=j
i

2
qijA∗
lj

(cid:1)
β

qij)
k

β′

A∗

kk

kk

2
1,2 ≤

k

O(k3/m2),

e

where the second last inequality follows from the Cauchy-Schwarz inequality. For the last term, we
= j and
write it in a matrix form as
(Qβ)ij = 0 for i = j. Then

T
=i qijβiβ′jA∗liA∗lj = A∗
l
•

where (Qβ)ij = qijβiβ′j for i

QβA∗l
•

j

where
Qβk
k
mately,

2
F =

=j q2

i

(maxi

=j q2
ij)
k

β

2
k

β′k
k

≤

2. Ulti-

P
T
A∗
l
|
•
i (β′j )2
ijβ2

QβA∗l

•| ≤ k

(maxi

≤

A∗l

Qβkk
=j q2
ij)

2

•k

≤ k
i β2
i

QβkF k
j(β′j)2

A∗

2
1,2,

k

qijβiβ′jA∗liA∗lj

(max
=j
i

≤

P
β
qij)
k

kk

β′

A∗

P
kk

2
1,2 ≤
k

O(k3/m2).

P

=i
Xj
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Under Assumption k = O∗( √n
O∗(k/m log2 n). As a result, the two terms
above are bounded by the same amount O∗(k/m log n) w.h.p., so we complete the proof of the
claim.

log n ), then

O(k3/m2)

≤

e

e

21

Claim 5. In the decomposition (5),

E5|
|
S] = 1 and qi = P[i

,
E3|
|

,

Proof. Recall that E[x2
i |
E3 = E[
h

∈
l ] = σ2
ε2
ε
i

x∗, β

x∗, β′

ih

E7|
|

and

E9|

|

is at most O∗(k/m log2 n).

S] = Θ(k/m) for S = supp(x∗), then

ES

E

x∗

S[

|

βiβ′jx∗i x∗j ]

S
Xi,j
∈

(cid:3)

(cid:2)
σ2
ε qiβiβ′i

= σ2
ε

ES[

βiβ′i] =

S
Xi
∈

Xi

Denote Q = diag(q1, q2, . . . , qm), then
O(k3/mn) where we have used
β
k
handle the seventh term before E5:
e

k ≤

E7 = E[uT εεT v

, x∗

A∗l
•

h

2] = E[
h

A∗l
•

i

=

Q

σ2
σ2
ε k2/m) =
Qβ, β′i| ≤
E3|
ε k
ε h
|
|
O(√k) w.h.p. and σε ≤
O(1/√n). For convenience, we
e
, x∗

β′k ≤

O(σ2

u, v

u, v

kk

qiA2

QAl

kk

β

e

2]uT E[εεT ]v =
i

li = σ2
ε h

AT
l
•

i

σ2
ε h

i

•

Xi

O(√k) w.h.p. Consequently,

To bound this term, we use Claim 9 in Appendix D to have
2
u, v
h
•k
O(m/n) and σε ≤
e

σ2
ε k
O(1/√n). Now, the ﬁrth term E5 is expressed as follows

u
k
k
u, v
|h

E7| ≤
|

A∗α + εuk ≤
k
O(k2/mn) because
i| ≤

i ≤

Al

kk

Q

=

e

e

O(√k) w.h.p. and

Al

2
•k

k

≤

, x∗

x∗
i
T
x∗x∗

E5 = E
A∗l
h
·
E
T
= A∗
(cid:2)
l
•
T
= σ2
Q(vlβ + ulβ′)
ε A∗
(cid:3)
(cid:2)
l
•

T (βvT + β′uT )εεl
(βvT + β′uT )E[εεl]
(cid:3)

Observe that
β
2
k ≤
kk
k
bounded by

u

The last term

σ2
ε k

Q(vlβ + ulβ′)
k ≤
u
k ≤
k

T
A∗
E5| ≤
l
• kk
|
O(k) w.h.p. using the result
O(k2/mn).
e
e

e

T
σ2
Q
A∗
ε k
l
• kk
O(k) and

vlβ + ulβ′k
kk
β
k ≤
k

and that

vlβ + ulβ′k ≤
k
O(k) from Claim 3, then E5

E9 = E[uT εεT vε2

l ] = uT E

εεT ε2
l

v = 9σ4
ε h

because the independent entries of ε and E[ε4
Since m = O(n) and k
and

O(k2/n2).
E7|
,
E5|
|
|
e
E9|
|
Combining the bounds from Claim 4, 5 for every single term in (5), we ﬁnish the proof for
Lemma 1.

v
kk
log n ), we obtain the same bound O∗(k/m log2 n) for

, and conclude the proof of the claim.

k ≤
,
E3|
|

O∗( √n

9σ4
ε k

≤

u

|

(cid:2)
l ] = 9σ4

(cid:3)
ε . Therefore,

e

u, v

i
E9| ≤

B.2 Proof of Lemma 2

We prove this lemma by using the same strategy used to prove Lemma 1.

yRyT
R]
A∗x∗ + ε, v

i

y, v

Mu,v , E[
y, u
h
ih
= E[
A∗x∗ + ε, u
h
= E
h
= M1 +
(cid:2)(cid:8)

ih
x∗, β′
ih
+ M8,

x∗, β

· · ·

i

(A∗R
•

x∗ + εR)(A∗R
•

i

+ x∗

T (βvT + β′uT )ε + uT εεT v

x∗ + εR)T ]
A∗R
•

(cid:9)(cid:8)

22

x∗x∗

T
T A∗
R

+ A∗R
•

•

x∗εT

R + εRx∗

T
T A∗
R

+ εRεT
R

•

(cid:9)(cid:3)

in which only nontrivial terms are kept in place, including

T
T A∗
R

]

•

x∗εT
R]
T
T A∗
R

]

•

i

ih

ih

x∗x∗

x∗, β′

A∗R
•
εRεT
R]

M1 = E[
x∗, β
h
M2 = E[
x∗, β
x∗, β′
h
i
M3 = E[x∗
T (βvT + β′uT )εA∗R
•
M4 = E[x∗
T (βvT + β′uT )εεRx∗
M5 = E[uT εεT vA∗R
T
T A∗
]
x∗x∗
R
•
M6 = E[uT εεT vA∗R
x∗εT
R]
•
M7 = E[uT εεT vεT
T
T A∗
Rx∗
R
M8 = E[uT εεT vεRεT
R]

•

•

]

(6)

By swapping inner product terms and taking advantage of the independence, we can show that
M6 = E[A∗R
] = 0. The remaining are bounded in
the next claims.

R] = 0 and M7 = E[uT εεT vεT

x∗uT εεT vεT

T
T A∗
R

Rx∗

•

•

Claim 6. In the decomposition (6),

M1 =

T
R,i + E′1 + E′2 + E′3
qiciβiβ′iA∗R,iA∗

Xi
V
U
∩
∈
T
V qiciβiβ′iA∗R,iA∗
R,i, E′2 =
∩

where E′1 =
T
β′iA∗R,iβjA∗
R,j) have norms bounded by O∗(k/m log n).
P

P

i /
∈

U

i

T
=j qijβiβ′iA∗R,jA∗
R,j and E′3 =

T
=j qij(βiA∗R,iβ′jA∗
R,j+

i

P

Proof. The expression of M1 is obtained in the same way as E1 is derived in the proof of Lemma
1. To prove the claim, we bound all the terms with respect to the spectral norm of A∗R
and make
•
use of Assumption A4 to ﬁnd the exact upper bound.
T
(U
R,S where S = [m]
For the ﬁrst term E′1, rewrite E′1 = A∗R,SD1A∗
\
qiciβiβ′i| ≤
D1k ≤
S|
∈

V ) and D1 is a diagonal
∩
O∗(k/m log n) as shown in

matrix whose entries are qiciβiβ′i. Clearly,
Claim 4, then

maxi

k

E′1k ≤

k

max
S |
i
∈

qiciβiβ′i|k

A∗R,Sk

2

≤

max
S |
i
∈

qiciβiβ′i|k

A∗R

2
•k

≤

O∗(k/m log n)

O(1). The second term E′2 is a sum of positive semideﬁnite matrices,

where
A∗R,Sk ≤ k
k
β
and
k ≤
k

A∗R
•k ≤
O(k log n), then

E′2 =

T
qijβiβ′iA∗R,jA∗
R,j (cid:22)

max
=j
i

qij

βiβ′i

=j
Xi

(cid:17)(cid:16)Xj
2
which implies that
A∗R
•k
T
form as the last term in Claim 4, which is E′3 = A∗
R
•

=j qij)
k

E′2k ≤

β′kk

(cid:16)Xi

(maxi

kk

β

k

T
A∗R,jA∗
R,j

(max
=j
i

β

qij)
k

β′

A∗R
k
•

kk

(cid:22)

T
A∗
R

•

O(k3/m2). Observe that E′3 has the same

E′3k ≤ k
k

Qβkk

A∗R

2
•k

(max
=j
i

qij)
k

≤

2

A∗R

•k

≤

O(k3/m2)

(cid:17)

. Then

≤
QβA∗R
•
e
β′

β

kk

kk

By Claim 3, we have

β

k

k

and

β′k

k

then we complete the proof for Lemma 6.

are bounded by O(√k log n), and note that k

O∗(√n/ log n),

e

≤

23

Claim 7. In the decomposition (6), M2, M3, M4, M5 and M8 have norms bounded by O∗(k/m log n).

Proof. Recall the deﬁnition of Q in Claim 5 and use the fact that E[x∗x∗
E[
x∗, β
ε qiβiβ′iIr. Then,
R] =
h
The next three terms all involve A∗R
•

σ2
ε maxi qik

x∗, β′i
ih

β′k ≤
whose norm is bounded according to Assumption A4.

T ] = Q, we can get M2 =
ε k2 log2 n/m).

M2k ≤

O(σ2

εRεT

i σ2

P

kk

β

k

Speciﬁcally,

M3 = E[x∗
= A∗R
•
= A∗R
•

x∗εT

R] = E[A∗R
•
T ](βvT + β′uT )E[εεT
R]

T (βvT + β′uT )εA∗R
•
E[x∗x∗
Q(βvT + β′uT )E[εεT

R],

x∗x∗

T (βvT + β′uT )εεT
R]

and

M4 = E[x∗

T (βvT + β′uT )εεRx∗

] = E[εRεT (vβT + uβ′

T )x∗x∗

T
T A∗
R

]

•

= E[εRεT ](vβT + uβ′
= E[εRεT ](vβT + uβ′

T A∗
T
R
•
T )E[x∗x∗
T
T ]A∗
R
T
T )QA∗
,
R

•

and the ﬁfth term M5 = E[uT εεT vA∗R
E[εεT
We already have
R]
k
k
remaining work is to bound
A∗uvT
βvT
A∗kk
k
k
k
O(σ2
in norm by
≤
The remaining term is

•
E[x∗x∗
T A∗
T
ε uT vA∗R
= σ2
.
R
•
•
•
uT v
Q
O(k) (proof of Claim 9), then the
O(k/m) and
k ≤
k
|
T directly follows. We have
, then the bound of vβT + uβ′
βvT + β′uT
k
k
v
u
O(k). Therefore, all three terms M3, M4 and M5 are bounded
kk
O(k3/mn).
e

ε uT vA∗R
•

k ≤ k
ε k2/m)

T
T ]A∗
R

] = σ2

= σ2
ε ,

T
QA∗
R

x∗x∗

k ≤

| ≤

=

e

•

•

e

e

M8 = E[uT εεT vεRεT

R] = E[

uivjεiεj

εRεT
R]

= E[

uiviε2

i εRεT
R

(cid:0)Xi
R
∈
ε uRvT
R

= σ4

(cid:0)Xi,j
] + E[

(cid:1)
uivjεiεj

εRεT
R]

(cid:1)

=j

(cid:0)Xi

(cid:1)

where uR = A∗R
•
O(√k). Therefore,
bound all the above terms by O∗(k/m log n) and ﬁnish the proof of Claim 7.
e

α + (εu)R and vR = A∗R
α′ + (εv)R. We can see that
•
O(k3/n2). Since m = O(n) and k
ε k) =

e
Combine the results of Claim 6 and 7, we complete the proof of Lemma 2.

uRk ≤ k
k
≤

M 8
k

O(σ4

k ≤

e

+

A∗R
(εu)Rk ≤
•kk
k
O∗( √n
log n ), then we can

α
k

Appendix C. Analysis of Main Algorithm

C.1 Simple Encoding

y)sgn(x)T is random over y and x that is obtained from the encoding step.
We can see that (Asx
We follow (Arora et al., 2015) to derive the closed form of gs = E[(Asx
y)sgn(x)T ] by proving
that the encoding recovers the sign of x∗ with high probability as long as As is close enough to A∗.

−

−

Lemma 8. Assume that As is δ-close to A∗ for δ = O(r/n log n) and µ
then with probability over random samples y = A∗x∗ + ε

≤

√n
2k , and k

≥

Ω(log m)

sgn(thresholdC/2

(As)T y

= sgn(x∗)

(7)

(cid:0)
24

(cid:1)

Proof of Lemma 8. We follow the same proof strategy from (Arora et al., 2015) (Lemmas 16 and
17) to prove a more general version in which the noise ε is taken into account. Write S = supp(x∗)
and skip the superscript s on As for the readability. What we need is to show S =
[m] :
S with high probability. Following
) = sgn(x∗i ) for each i
i, y
A
h
i
•
the same argument of (Arora et al., 2015), we prove in below a stronger statement that, even
i, y
A
conditioned on the support S, S =
h
•

and then sgn(
h

with high probability.

As
i, y
•

C/2
}

C/2
}

[m] :

i ≥

i ≥

i
{

i
{

∈

∈

∈

Rewrite

i, y
A
h
•

i

=

i, A∗x∗ + ε
A
i
h
•

=

i, A∗
A
ii
•
•

h

x∗i +

i, A∗
A
j i
•
•

x∗j +

i, ε
A
i
h
•

,

h
Xj
=i

and observe that, due to the closeness of A
i, the ﬁrst term is either close to x∗i or equal to
i and A∗
•
•
S. Meanwhile, the rest are small due to the incoherence and the
0 depending on whether or not i
concentration in the weighted average of noise. We will show that both Zi =
x∗j
and

i, A∗
A
ji
•
•

i
\{

}h

∈

S

i, ε
A
h
i
•
The cross-term Zi =

are bounded by C/8 with high probability.
i, A∗
A
j i
•
•
dom variables, which is another sub-Gaussian random variable with variance σ2
Note that

i
\{

P

}h

S

x∗j is a sum of zero-mean independent sub-Gaussian ran-

S

i
\{

}h

i, A∗
A
j i
•
•

2.

Zi =

P

i, A∗
A
ji
h
•
•

A
i −
h
•
where we use Cauchy-Schwarz inequality and the µ-incoherence of A∗. Therefore,

i, A∗
A∗
j i
•
•

A
i −
•

2µ2/n + 2
h

2
i, A∗
A∗
ji
•
•

2 +

≤

≤

2

(cid:0)

(cid:1)

h

2

P
i, A∗
A∗
ji
•
•

2,

σ2
Zi ≤

T
2µ2k/n + 2
S (A
A∗
i −
k
•
•
√n
2k , to conclude 2µ2k/n

≤

under µ
Applying Bernstein’s inequality, we get
. In fact,
bound the noise term
a sub-Gaussian with variance σ2
Notice that σε = O(1/√n).

i, ε
A
i
h
•

≤

i)
A∗
k
•

2
F ≤

2µ2k/n + 2
k

A∗
Sk
•

2

A
i −
•

k

2
A∗
ik
•

≤

O(1/ log n),

O(1/ log n) we need 1/k = O(1/ log n), i.e. k = Ω(log n).
C/8 with high probability. What remains is to
is sum of n Gaussian random variables, which is
σε log n with high probability.

Zi| ≤
|
i, ε
A
i
h
•
ε . It is easy to see that

i, ε
A
•

|h

i| ≤

Finally, we combine these bounds to have
i, y
A
•

S, then
i, ε
A
h
•
C/2 and negligible otherwise. Using union bound for every i = 1, 2, . . . , m, we ﬁnish

C/4. Therefore, for i

Zi +
|

i| ≤

∈

|h
the proof of the Lemma.

i| ≥

Lemma 8 enables us to derive the expected update direction gs = E[(Asx

y)sgn(x)T ] explicitly.

−

C.2 Approximate Gradient in Expectation

−

Proof of Lemma 5. Having the result from Lemma 8, we are now able to study the expected update
y)sgn(x)T ]. Recall that As is the update at the s-th iteration and x ,
direction gs = E[(Asx
thresholdC/2((As)T y). Based on the generative model, denote pi = E[x∗i sgn(x∗i )
S]
i
|
and qij = P[i, j
S]. Throughout this section, we will use ζ to denote any vector whose norm is
negligible although they can be diﬀerent across their appearances. A
i denotes the sub-matrix of
A whose i-th column is removed. To avoid overwhelming appearance of the superscript s, we skip
it from As for neatness. Denote
Fx∗ is the event under which the support of x is the same as that
of x∗, and ¯
Fx∗ = 1[sgn(x) = sgn(x∗)] and 1
Fx∗ = 1.

Fx∗ is its complement. In other words, 1

S], qi = P[i

Fx∗ + 1 ¯

∈

∈

∈

−

i = E[(Ax
gs
•

−

y)sgn(xi)] = E[(Ax

y)sgn(xi)1

−

Fx∗ ]

±

ζ

25

Fx∗ we have Ax = A

S AT
Using the fact that y = A∗x∗ + ε and that under
Sy =
SxS = A
•
•
•
SAT
SAT
A
SA∗x∗ + A
Sε. Using the independence of ε and x∗ to get rid of the noise term, we get
•
•
•
•
i = E[(A
S AT
gs
S −
•
•
•
= E[(A
S AT
S −
•
•
= E[(A
S AT
S −
•
•
= E[(A
S AT
S −
•
•

S AT
S −
•
•
ζ
(Independence of ε and x’s)
±
1 ¯
Fx∗ )]

In)A∗x∗sgn(xi)1
In)A∗x∗sgn(x∗i )(1
In)A∗x∗sgn(x∗i )]

Fx∗ ] + E[(A
Fx∗ ]
−
ζ

Fx∗ event)

In)εsgn(xi)1

In)A∗x∗1

(Under

Fx∗ ]

±

±

±

ζ

ζ

Recall from the generative model assumptions that S = supp(x∗) is random and the entries of x∗
are pairwise independent given the support, so

i = ESE
gs
x∗
•
= piES,i
= piES,i

|

S AT
S[(A
S −
•
•
S AT
S[(A
S −
•
∈
•
iAT
S[(A
i −
•
∈
•

In)A∗x∗sgn(x∗i )]
i]
In)A∗
ζ
±
•
i] + piES,i
In)A∗
•

S[
∈

ζ

±

iAT
= piqi(A
i −
•
•

i + pi
In)A∗
•

= piqi(λiA
i −
•

i) + piA
A∗
•

•−

Xl
=i
S,l
∈
lAT
lA∗
qilA
i ±
•
•
•

ζ

[m],l
Xl
=i
∈
idiag(qij)AT
•−

iA∗
i ±
•

ζ

lAT
i]
lA∗
A
•
•
•

±

ζ

where λs
expression of the expected approximate gradient at iteration s:

idiag(qij)AT
•−

As
i, A∗
ii
h
•
•

i = AR,

. Let ξs

i =

−

iA∗
i/qi for j = 1, . . . , m, we now have the full
•

(8)

(9)

R,i = piqi(λiAs
gs

A∗R,i + ξs
i )

R,i −

ζR.

±

What remains is to bound norms of ξs and ζ. We have
ξs
A∗i k
along with the fact that
i k
k
qij
ξs
i k ≤ k
qi k
k

= 1, we can bound

max
=i
j

As

ik

Ri,

k

−

As
−

ik ≤

O(k/n).

As
R,

k

ik ≤ k

−

As
−

ik ≤

O(

m/n) w.h.p. Then,

p

Next, we show that norm of ζ is negligible. In fact,
it suﬃces to bound norm of (Ax
Section D. This concludes the proof for Lemma 5.

−

Fx∗ happens with very high probability, then
y)sgn(xi) which will be done using Lemma 12 and Lemma 11 in

Appendix D. Sample Complexity

In previous sections, we rigorously analyzed both initialization and learning algorithms as if the
expectations gs, e and Mu,v were given. Here we show that corresponding estimates based on
empirical means are suﬃcient for the algorithms to succeed, and identify how may samples are
required. Technically, this requires the study of their concentrations around their expectations.
Having had these concentrations, we are ready to prove Theorems 4 and 5.

The entire section involves a variety of concentration bounds. Here we make heavy use of
Bernstein’s inequality for diﬀerent types of random variables (including scalar, vector and matrix).
The Bernstein’s inequality is stated as follows.

26

Lemma 9 (Bernstein’s Inequality). Suppose that Z (1), Z (2), . . . , Z (p) are p i.i.d. samples from some
almost surely and
distribution

σ2 for each j, then

. If E[Z] = 0,

E[Z (j)(Z (j))T

Z (j)

k ≤

D

k

k ≤ R
p

(10)

k
σ2
p

(cid:19)

+

R
p

s

1
p

Z (j)

≤

O

e

(cid:18)

(cid:13)
(cid:13)
(cid:13)

Xj=1
(cid:13)
(cid:13)
(cid:13)

holds with probability 1

n−

ω(1).

−

Since all random variables (or their norms) are not bounded almost surely in our model setting,

(log(1/ρ))C ]

≤

we make use of a technical lemma that is used in Arora et al. (2015) to handle the issue.
Lemma 10 (Arora et al. (2015)). Suppose a random variable Z satisﬁes P[
k
ρ for some constant C > 0, then
(a) If p = nO(1), it holds that
(b)
e
Ω(

) for each j with probability 1

k
ω(1).

k ≥ R

= n−

ω(1).

Z (j)

k ≤

n−

O(

R

−

Z

Z

E[Z1
k
k

k≥

)]
k
R

e

p
This lemma suggests that if 1
i=1 Z (j)(1
)) concentrates around its mean with
p
p
high probability, then so does 1
i=1 Z (j) because the part outside the truncation level can be
P
p
ignored. Since all random variables of our interest are sub-Gaussian or a product of sub-Gaussian
that satisfy this lemma, we can apply Lemma 9 to the corresponding truncated random variables
with carefully chosen truncation levels. Then the original random variables concentrate likewise.

1
k

Z (j)

P

e
Ω(

−

k≥

R

In the next proofs, we deﬁne suitable random variables and identify good bounds of

and
σ2 for them. Note that in this section, the expectations are taken over y by conditioning on u
and v. This aligns with the construction that the estimators of e and Mu,v are empirical averages
over i.i.d. samples of y, while u and v are kept ﬁxed. Due to the dependency on u and v, these
(conditional) expectations inherit randomness from u and v, and we will formulate probabilistic
bounds for them.

R

The application of Bernstein’s inequality requires a bound on

eΩ(
k≥
achieve that by the following technical lemma, where ˜Z is a standardized version of Z.
Lemma 11. Suppose a random variable ˜Z ˜Z T = aT where a
They are both random. Suppose P[a

> 0 is a constant. Then,

ω(1) and

1
k

−

≥

Z

] = n−

E[ZZ T (1
k

0 and T is positive semi-deﬁnite.

. We

))]
k

R

E[ ˜Z ˜Z T (1

k

≥ A
1
˜Z
k

−

)]

k≥B

k ≤ Ak

B
E[T ]
k

+ O(n−

ω(1))

Proof. To show this, we make use of the decomposition ˜Z ˜Z T = aT and a truncation for a. Specif-
ically,

E[ ˜Z ˜Z T (1
k

1
k

˜Z

−

)]
k

k≥B

T (1

1
˜Z
k

−

k≥B

)]
k

≥A

˜Z

)]

k≥B

)]
k
T

≥Ak

1
˜Z
k

k≥B

k≥B
)T (1

≥A

1
˜Z
−
k
+ E[a1a
)T ]
k
≥A
2(1
E[
aT
k
k
4(1
˜Z
E[
k
k
P[a

−
1
˜Z
k
1/2

−
]

2

k≥B

+

E[a1a
k
1
(1
˜Z
k
−
k
)]E[1a
)]P[a

≥A
]

]

≥ A

(cid:1)

)]

k≥B
1/2

1/2
(cid:1)

= E[aT (1
E[a(1
E[a(1

≤ k

≤ k

−

−

1
k
1a

1a

−
E[T ]
k
E[T ]
k
E[T ]
k
E[T ]
k

≤ Ak

≤ Ak

≤ Ak

≤ Ak

+

+

(cid:0)

+

(cid:0)
B
+ O(n−

(cid:0)

≥ A
ω(1)),

(cid:1)

27

where at the third step we used T (1
semi-deﬁnite and 1

1
k

˜Z

−

k≥B ∈ {

0, 1
}

1
k

˜Z

)]

T because of the fact that T is the positive

−
. Then, we ﬁnish the proof of the lemma.

k≥B

(cid:22)

D.1 Sample Complexity of Algorithm 1

e and the reduced weighted covariance matrix
Mu,v depends upon
b
R
u,v. We will show that we only need
e, we denote it by
c
O(m) samples to be able to recover the support of one particular atom and up to some speciﬁed

In Algorithm 1, we empirically compute the “scores”
Mu,v to produce an estimate for each column of A∗. Since the construction of
the support estimate
c
p =
level of column-wise error with high probability.

R given by ranking

c

M

b

b

b

e

Lemma 12. Consider Algorithm 1 in which p is the given number of samples. For any pair u and
v, then with high probability a)
O∗(k/m log n) when p =
sets of one particular atom.
e

u,vk ≤
R and R are respectively the estimated and correct support
e

e
e
k ≤
k
Ω(mr) where

O∗(k/m log2 n) when p =

Ω(m) and b)

b
R
u,v −

M
k

M R

c

−

b

b

D.1.1 Proof of Theorem 4

Using Lemma 12, we are ready to prove the Thereom 4. According to Lemma 1 when U
we can write

e as

∩

V =

,
i
}
{

b

e = qiciβiβ′iA∗R,i ◦

e
A∗R,i + perturbation terms + (

e),

−

e as an additional perturbation with the same magnitude O∗(k/m log2 n) in the
and consider
e
−
w.h.p. The ﬁrst part of Lemma 3 suggests that when u and v share exactly one atom
sense of
k · k∞
e is the same as supp(A∗i ) with high probability.
R including r largest elements of
i, then the set
b

b

b

Once we have
b

R, we again write

M

b
R
u,v using Lemma 2 as
b

b
b
c
T
R
R,i + perturbation terms + (
u,v = qiciβiβ′iA∗R,iA∗
M

M

b
R
u,v −

M R

u,v),

b
c
R
u,v −

M

and consider
in the sense of the spectral norm
singular vectors of

c

M

M R

c
u,v as an additional perturbation with the same magnitude O∗(k/m log n)
w.h.p. Using the second part of Lemma 3, we have the top

b
R
u,v is O∗(1/ log n) -close to A∗R,i with high probability.

k · k

Since every vector added to the list L in Algorithm 1 is close to one of the dictionary, then
A0 must be δ-close to A∗. In addition, the nearness ofA0 to A∗ is guaranteed via an appropriate
A close to A0 and
projection onto the convex set
. Finally, we ﬁnish the
A
|
{
proof of Theorem 4.

A∗k}

A
k

2
k

k ≤

c

=

B

D.1.2 Proof of Lemma 12, Part a

y, v

[n], consider p i.i.d. realizations Z (1), Z (2), . . . , Z (p) of the random variable
For some ﬁxed l
∈
Z ,
O∗(k/m log2 n)
y2
l , then
y, u
e
h
k∞ ≤
holds with high probability, we ﬁrst study the concentration for the l-th entry of
e and then
e
and its variance E[Z 2]
take the union bound over all l = 1, 2, . . . , n. We derive upper bounds for
b
|
in order to apply Bernstein’s inequality in (12) to the truncated version of Z.

i=1 Z (i) and el = E[Z]. To show that

el = 1
p

P

ih

−

−

Z

b

k

e

i

p

|

b

O(k) and E[Z 2]

O(k2/m) with high probability.

Claim 8.

Z
|

| ≤

e

≤

e

28

Again, the expectation is taken over y by conditioning on u and v, and therefore is still random
due to the randomness of u and v. To show Claim 8, we begin with proving the following auxiliary
claim.

Claim 9.

y

O(√k) and

y, u

O(√k) with high probability.

k

k ≤

|h

i| ≤

Proof. From the generative model, we have

e

e

y
k

k

=

Sx∗S + ε
A∗
k
•

where S = supp(x∗). From Claim 2,
overcomplete and has bounded spectral norm, then
O(1). Therefore,
w.h.p., which is the ﬁrst part of the proof. To bound the second term, we write it as

k
A∗
Sk ≤ k
•

e

k

k

,

+

ε
k
k

x∗Sk

A∗
S kk
•
O(σε√n) w.h.p. In addition, A∗ is
O(√k)

ε
k ≤ k
ε
k ≤
A∗k ≤
e

y
k

k ≤

+

Sx∗Sk
A∗
•
O(√k) and

k

k ≤ k
x∗Sk ≤

=

y, u

Sx∗S + ε, u
A∗
•

T
S u
x∗S, A∗
•
T
S u
Similar to y, we have
A∗
k
•
probability. Since u and x∗ are independent sub-Gaussian and
T
e
variance at most O(√k),
S u
x∗S, A∗
•
y, u
quently,

i|
i| ≤ |h
O(√k) w.h.p. and hence

O(k) w.h.p. Similarly,

i| ≤

O(√k) w.h.p., and we conclude the proof of the claim.

.
i|
T
O(√k) with high
u
A∗
S kk
k ≤ k
•
T
x∗S, A∗
are sub-exponential with
S u
h
i
•
O(√k) w.h.p. Conse-
ε, u
i| ≤

k ≤

k ≤

|h
u

ε, u

+

|h

|h

|h

|h

i|

k

e

e

|h

y, v

e
y2
l =

i| ≤
, x∗i
A∗l
(
Proof of Claim 8. We have Z =
y, u
h
i
h
•
w.h.p. according to Claim 9. What remains is to bound y2
A∗l
l = (
h
2
is sub-Gaussian with variance ES(
•
1,2 = O(1), then
k
+εl| ≤
A∗l
εl| ≤
Similarly for εl,
|h
|
•
with high probability the bound
2y2
2y2
y, u
l h
i

T
2
A∗
li )
S A∗
i
∈
O(σε log n) w.h.p. Ultimately,

To bound the variance term, we write Z 2 =

P
Z
|

y, u
h

, x∗i

O(k).

≤ k

y, v

| ≤

ih

ih

i

2y2
i

y, v
we get
to both terms, then
e

l ≤

h

O(k) and

Z
|

| ≤

E[Z 2(1

e
Z

|≥

1
|

−

Ω(k))]
e

O(k)E[
h

y, u
i

≤

2y2

l ] + O(n−

ω(1)),

e
+ εl)2 with
O(k)
i ≤
, x∗i
A∗l
, x∗i
h
•
e
O(log n) w.h.p.
|h
O(log n), and hence we obtain

y, v
y, u
h
+ εl)2. Because
, x∗i| ≤
A∗l
•

ih

l . Note that, from the ﬁrst part,
O(k) w.h.p.. We apply Lemma 11 with some appropriate scaling

y, v

e

h

i

where E[
e
y, u
i
h
Section “Analysis of Initialization Algorithm”,

2y2

l ] is equal to el for pair u, v with v = u. From Lemma 1 and its proof in Appendix

E[
y, u
i
h

2y2

l ] =

qiciβ2

2
li + perturbation terms,
i A∗

m

Xi=1

in which the perturbation terms are bounded by O∗(k/m log2 n) w.h.p. (following Claims 4 and 5).
(max qiciβ2
2
O(log m)
A∗l
i )
i A∗
The dominant term
li ≤
≤
k
(Claim 3). Then we complete the proof of the second part.

O(k/m) w.h.p. because

βi| ≤
|

i qiciβ2

2
•k

P

e

Proof of Lemma 12, Part a. We are now ready to prove Part a of Lemma 12. We apply Bernstein’s
inequality in Lemma 9 for the truncated random variable Z (i)(1
O(k) and
variance σ2 =

O(k2/m) from Claim 8, then

)) with

1
Z (i)
|

e
Ω(

R

−

=

|≥

R

Z (i)(1

e
−

1
|

Z (i)

e
Ω(

))

|≥

R

−

E[Z(1

1
Z
|

−

e
Ω(

|≥

R

p

Xi=1

1
p

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

O(k)
p

e

O(k2/m)
p

+

s

≤

e

))]
(cid:13)
(cid:13)
(cid:13)
(cid:13)

29

e

O∗(k/m log n), (11)

Ω(m). Then

el = 1
w.h.p. for p =
p
bound over l = 1, 2, . . . , n, we get
the proof of 12, Part a.

e

b

p
i=1 Z (i) also concentrates with high probability. Take the union
O∗(k/m log n) with high probability and complete
e

k
P

−

e
k∞ ≤

D.1.3 Proof of Lemma 12, Part b

b

b
R
u,v −

M R

M

Next, we will prove that
prove the concentration inequalities for the case when conditioned on the event that
c
to R w.h.p. Again, what we need to derive are an upper norm bound
variable Z ,
yRyT
R and its variance.

O∗(k/m log n) with high probability. We only need to
R is equivalent
of the matrix random

u,vk ≤

y, v

R

b

k

y, u
h

ih

i

Claim 10.

Z

O(kr) and

k

k ≤

E[ZZ T ]
k

k ≤

O(k2r/m) hold with high probability.

k

Z

y, u

Proof. We have
k
2 =
yRk
whereas
This implies
Z
P
k ≤
k
We take advantage of the bounds of
and

l ≤

e

2

2

i|k

yRk

e
2 with
y, v
ih
i| ≤
O(r log2 n) w.h.p. because yl ≤

e
O(k) w.h.p. (according to Claim 9)
k ≤ |h
R y2
O(log n) w.h.p. (proof of Claim 8).
i
∈
e
O(kr) w.h.p. The second part is handled similarly as in the proof of Claim 8.
O(kr)
Mu,v in Lemma 2. Speciﬁcally, using the ﬁrst part

y, u

y, v

ih

|h

O(kr), and applying Lemma 11, then

Z
k

k ≤

i

y, v
h

yRk
k
E[ZZ T (1
k

≤

e
1
Z
k

−

k≥

Ω(kr))]
e

k ≤

E[
y, u
i
h

2yRyT
R]
k

+

O(kr)O(n−

ω(1))

O(kr)
k

,
Mu,uk

≤

e

where Mu,u arises from the application of Lemma 2. Recall that

e

e

e

Mu,u =

qiciβ2

T
R,i + perturbation terms,
i A∗R,iA∗

c
O(kr)
k

Xi

where the perturbation terms are all bounded by O∗(k/m log n) w.h.p. by Claims 6 and 7.
addition,

In

qiciβ2

T
i A∗R,iA∗
R,ik ≤

(max
i

qiciβ2
i )
k

A∗R

2
•k

O(k/m)
k

A∗

≤

k

≤

2

O(k/m)

k
Xi

w.h.p. Finally, the variance bound is

O(k2r/m) w.h.p.
Then, applying Bernstein’s inequality in Lemma 9 to the truncated version of Z with

and variance σ2 =

O(k2r/m) and obtain the concentration for the full Z to get

e

e

e

=

O(kr)

R

e

e

M R
k

u,v −

M R

u,vk ≤

c

O(kr)
p

O(k2r/m)
p

+

s

≤

e

e

O∗(k/m log n)

w.h.p. when the number of samples is p =

We have proved that
k
b
R
u,v −

M

k

M R

M R
u,v −
M R
u,vk ≤
c

Ω(mr) under Assumption A4.1.
O∗(k/m log n) as conditioned on the support consistency
u,vk ≤
e
O∗(k/m log n) is easily followed by the law of total probability

event holds w.h.p.
through the tail bounds on the conditional and marginal probabilities (i.e. P[
k
c
R = R]) and P[
O∗(k/m log n)
|
the spectral bounds.

u,vk ≤
= R]. We ﬁnish the proof of Lemma 12, Part b for both cases of

u,v −

M R

M R

c

R

b

b

30

D.2 Proof of Theorem 5 and Sample Complexity of Algorithm 2

In this section, we prove Theorem 5 and identify sample complexity per iteration of Algorithm 2. We
divide the proof into two steps: 1) show that when As is (δs, 2)-near to A∗ for δs = O∗(1/ log n), the
O(k2/mn)+αo(δ2
s )
approximate gradient estimate
, and 2) show that the nearness is preserved at each iteration. These correspond to showing the
following lemmas:

gs is (α, β, γs)-correlated-whp with A∗ with γs ≤

b

Lemma 13. At iteration s of Algorithm 2, suppose that As has each column correctly supported
and is (δs, 2)-near to A∗ and that η = O(m/k). Denote R = supp(As
gs
R,i is
i), then the update
•
O(k2/mn) + αo(δ2
(α, β, γs)-correlated-whp with A∗R,i where α = Ω(k/m), β = Ω(m/k) and γs ≤
s )
for δs = O∗(1/ log n).

b

Note that this is a ﬁnite-sample version of Lemma 6.

Lemma 14. If As is (δs, 2)-near to A∗ and number of samples used in step s is p =
.
with high probability

Proof of Theorem 5. The correlation of
column-wise error according to Theorem 1. Along with Lemma 14, the theorem follows directly.

A∗k
2
k
gi with A∗i , described in Lemma 13, implies the descent of

e

As+1
k

A∗k ≤

−

Ω(m), then

D.2.1 Proof of Lemma 13

b

We prove Lemma 13 by obtaining a tail bound on the diﬀerence between
Bernstein’s inequality in Lemma 9.

R,i and gs
gs

R,i using the

Lemma 15. At iteration s of Algorithm 2, suppose that As has each column correctly supported and
is (δs, 2)-near to A∗. For R = supp(As
(o(δs) + O(ǫs))
i ) = supp(A∗i ), then
·
mnr
k ).
with high probability for δs = O∗(1/ log n) and ǫs = O(

gs
R,i −
k/n) when p =

O(k/m)
Ω(m + σ2
ε

gs
R,ik ≤

p
To prove this lemma, we study the concentration of

b
gs
R,i, which is a sum of random vector
e
of the form (y
S, with
i
Ax)Rsgn(xi)
|
S = supp(x∗) and x = thresholdC/2(AT y). Then, using the following technical lemma to bridge
the gap in concentration of the two variables. We adopt this strategy from Arora et al. (2015) for
our purpose.

Ax)Rsgn(xi). We consider random variable Z , (y

−

−

∈

b

k

b

Claim 11. Suppose that Z (1), Z (2), . . . , Z (N ) are i.i.d. samples of the random variable Z = (y
i
Ax)Rsgn(xi)
|

S. Then,

∈

−

o(δs) + O(ǫs)

(12)

(cid:13)
(cid:13)
(cid:13)
holds with probability when N =

Xj=1
Ω(k + σ2

N

1
N

Z (j)

−

≤

E[Z]
(cid:13)
(cid:13)
(cid:13)

ε nr), δs = O∗(1/ log n) and ǫs = O(

k/n).

Proof of Lemma 15. Once we have done the proof of Claim 11, we can easily prove Lemma 15. We
recycle the proof of Lemma 43 in Arora et al. (2015).
supp(x∗

Write W =

and N =

gR,i as

e

j : i
{

∈

(j))
}

W
|

, then express
|

p

gR,i =

N
p

1
N

(y(j)

−

Ax(j))Rsgn(x(j)

b
i ),

b

Xj

31

j(y(j)

1
where
W
|
E[(y
Ax)Rsgn(xi)] = E[(y
Following Claim 11, we have

P

−

−

|

−

Ax(j))Rsgn(x(j)

i ) is distributed as 1
N

Ax)Rsgn(xi)1i

S ] = E[Z]P[i
P
∈

∈

N
j=1 Z (j) with N =

. Note that
W
|
|
S] = qiE[Z] with qi = Θ(k/m).

gs
R,i −

gs
R,ik ≤

k

O(k/m)

O(k/m)

(o(δs) + O(ǫs)),

·

1
N

N

Xj=1

(cid:13)
(cid:13)
(cid:13)

Z (j)

−

≤

E[Z]
(cid:13)
(cid:13)
(cid:13)

holds with high probability as p = Ω(mN/k). Substituting N in Claim 11, we obtain the results
in Lemma 15.

b

Proof of Claim 11. We are now ready to prove the claim. What we need are good bounds for
k
and its variance, then we can apply Bernstein’s inequality in Lemma 9 for the truncated version of
Z, then Z is also concentrates likewise.

Z
k

holds with high probability for

=

O(δs√k + µk/√n + σε√r) with δs =

Claim 12.
k
O∗(1/ log n).

Z

k ≤ R

R

e

−

(y

Proof. From the generative model and the support consistency of the encoding step, we have
S y = AT
Sx∗S + AT
Sx∗S + ε and xS = AT
Sε. Then,
SA∗
y = A∗x∗ + ε = A∗
•
•
•
•
•
AR,SAT
AR,SAT
S ε
SA∗
•
•
•
S )x∗S + (In −
AR,S)x∗S + AR,S(Ik −
O(σwk
M w
k
e
kF + σεk

Ax)R = (A∗R,Sx∗S + εR)
S x∗S −
AT
= (A∗R,S −
SA∗
•
•
Using the fact that x∗S and ε are sub-Gaussian and that
probability for a ﬁxed M and a sub-Gaussian w of variance σ2

SAT
S)R
A
•
•
Now, we need to bound those Frobenius norms. The ﬁrst quantity is easily bounded as

kF ) holds with high

SAT
S )R
A
•
•
•

k ≤
w, we have

AR,S(Ik −

AR,SkF +

AT
S )
SA∗
•
•

Ax)Rsgn(xi)

A∗R,S −

(In −

•kF ).

O(
k

(y
k

k ≤

M

−

−

ε.

k

e

A∗R,S −
k

AR,SkF ≤ k

A∗
S −
•

A
S kF ≤
•

δs√k,

(13)

since A is δs-close to A∗. To handle the other two, we use the fact that
this fact for the second term, we have

U V
k

kF ≤ k

U

V

kF . Using

kk

AR,S(Ik −
k

AT
S)
S A∗
•
•

kF ≤ k

AR,Skk

(Ik −

AT
S )
SA∗
•
•

kF ,

where
advantage of the closeness and incoherence properties:

AR,Sk ≤ k

•k ≤

AR

k

O(1) due to the nearness. The second part is rearranged to take

Ik −

k

AT
SA∗
SkF ≤ k
•
•

S)T A∗
T
(A
A∗
S A∗
A∗
Ik −
S −
SkF
S −
•
•
•
•
•
S)T A∗
T
SkF +
A∗
(A
S A∗
A∗
Ik −
S −
S kF
k
•
•
•
•
•
T
A∗
A
A∗
SkF +
S A∗
Ik −
A∗
S −
S kF
Skk
•
•
•
•
•
µk/√n + O(δs√k),

k

≤ k

≤ k

≤

where we have used
δs√k in (13) and

µk/√n because of the µ-incoherence of A∗,

A
S −
•

A∗
SkF ≤
•

k

O(1). Accordingly, the second Frobenius norm is bounded by

k

T
S A∗
Ik −
A∗
SkF ≤
k
•
•
A∗k ≤
A∗
Sk ≤ k
•
AR,S(Ik −

k

AT
S )
SA∗
•
•

kF ≤

O

µk/√n + δs√k

.

(cid:0)

32

(cid:1)

(14)

SAT
The noise term is handled using the eigen-decomposition U ΛU T of A
S, then with high prob-
•
•
ability

(In −

(U U T
SAT
•kF =
S)R
A
k
k
•
•
where the last inequality
A∗k ≤
O(1) due to the nearness. Putting (13), (14) and (15) together, we obtain the bounds in Claim
12.

•kF =
O(1) follows by

(In −
A
Sk ≤ k
•

kF ≤ k
A

•kF ≤
+

kk
A∗k

In −
A

O(√r), (15)

U ΛU T )R

A∗k ≤

UR
k
•

In −

k ≤ k

−
Λ

3
k

k ≤

UR

Λ)

−

Λ

k

k

k

Next, we determine a bound for the variance of Z.

2] = E[
Claim 13. E[
Ax)Rsgn(xi)
Z
k
k
−
k
k
s k + k2/n + σ2
O(δ2
ε r) with δs = O∗(1/ log n).

(y

2

i
|

S]

∈

≤

σ2 holds with high probability for σ2 =

Proof. We explicitly calculate the variance using the fact that x∗S is conditionally independent
given S, and so is ε. x∗S and ε are also independent and have zero mean. Then we can decompose
the norm into three terms in which the dot product is zero in expectation and the others can be
S ] = Ik, E[εεT ] = σεIn.
T
shortened using the fact that E[x∗Sx∗
SAT
AR,S AT
i
ε
A
SA∗
S)x∗S + (In −
S)R
∈
k
|
·
•
•
•
•
2
2
E[
AR,SAT
SAT
S] + σ2
SA∗
S)R
A
In −
i
i
S k
ε
F |
F |
k
∈
•k
∈
•
•
•
•
AT
Then, by re-writing A∗R,S −
S )
SA∗
AR,S)+AR,S(Ik−
S as before, we get the form (A∗R,S −
•
•
in which the ﬁrst term has norm bounded by δs√k. The second is further decomposed as

S] = E[
(A∗R,S −
k
= E[
A∗R,S −
k
AR,SAT
SA∗
•
•

Ax)Rsgn(xi)
k

E[
(y
k

S]]

i
|

−

∈

2

2

S].

E[
AR,S (Ik −
k

AT
S)
SA∗
k
•
•

2
i
F |

∈

S]

sup
S k

AR,Sk

≤

2E[
Ik −
k

2
AT
SA∗
i
S k
F |
•
•

∈

S],

(16)

where supSk
using the proof from Arora et al. (2015):

AR,Sk ≤ k

•k ≤

AR

O(1). We will bound E[
k

Ik −

2
AT
SA∗
i
F |
S k
•
•

∈

S]

≤

O(kδ2

s ) + O(k2/n)

E[
Ik −
k

AT
SA∗
Sk
•
•

2
i
F |

∈

S] = E[

1
4 k

A
j −
•

A∗
jk
•

2] + qij

= E[

S
Xj
∈

S
Xj
∈

k
Xj
=i

(1

−

j)2 +
AT
jA∗
•
•

AT
jA∗
,
k
•
•
Xj
S
∈
2 + qik
AT
iA∗
,
jk
•
•

−

−

AT
jA∗
,
•
•

2
jk

i
|

−

∈

S]

2 + qik
ik

AT
,
•

iA∗
ik
•

−

2,

where A
,
−
•
any j = 1, 2, . . . , m,

i is the matrix A with the i-th column removed, qij ≤

O(k2/m2) and qi ≤

O(k/m). For

AT
jA∗
,
•
•

k

jk

−

2 =

k

T
j A∗
A∗
,
•
•
−
j, A∗
A∗
li
•
•

2
j)T A∗
A∗
j + (A
j −
,
jk
•
•
•
−
2 +
j)T A∗
A∗
(A
j −
,
k
•
•
•

2

jk

−

≤

h
Xl
=j

≤

h
Xl
=j

j, A∗
A∗
li
•
•

2 +

A
j −
k
•

A∗
jk
•

2

A∗
,
k
•

−

2
jk

≤

µ2 + δ2
s .

The last inequality invokes the µ-incoherence, δ-closeness and the spectral norm of A∗. Similarly,
we come up with the same bound for

2. Consequently,

2 and

AT
iA∗
,
•
•

k

ik

−

AT
iA∗
,
ik
k
•
−
•
s ) + O(k2/n).
O(kδ2

E[
k

Ik −

2
AT
S A∗
i
Sk
F |
•
•

∈

S]

≤

(17)

33

For the last term, we invoke the inequality (15) (Claim 12) to get

SAT
S)R
A
•
•
Putting (16), (17) and (18) together and using
σ2 = O(δ2

•k
AR
k
ε r) with δs = O(1/ log2 n) . Finally, we complete the proof.
We now apply truncated Bernstein’s inequality to the random variable Z (j)(1

s k + k2/n + σ2

E[
(In −
k

2
i
F |

•k ≤

S]

≤

∈

r

1, we obtain the variance bound of Z:

(18)

))
Ω(
R
O(δs√k + µk/√n + σε√r) and σ2 = O(δ2
s k +

1
k

Z (j)

−

k≥

with
R
k2/n + σ2

and σ2 in Claims 12 and 13, which are
ε r). Then, (1/N )

=

R

N
j= Z (j) also concentrates:
e

N

P
Z (j)

Xi=1

1
N

(cid:13)
(cid:13)
(cid:13)

E[Z]

−

O

≤

R
N

+

O

(cid:13)
(cid:13)
(cid:13)

(cid:16)

(cid:17)

e
Ω(k + σ2

e

σ2
N

(cid:18)r

(cid:19)

= o(δs) + O(

k/n)

p

ε nr). Then, we ﬁnally ﬁnish the proof of Claim

holds with high probability when N =
11.

e

Proof of Lemma 13. With Claim 11, we study the concentration of
we consider this diﬀerence as an error term of the expectation gs
correlation of

gs
R,i. Using the expression in Lemma 5 with high probability, we can write

R,i around its mean gs
gs

R,i. Now,
R,i and using Lemma 6 to show the

b

b

R,i + (gs
where
+ O(k/m)
correlated-whp with A∗R,i where α = Ω(k/m), β = Ω(m/k) and γs ≤
, then we have done the proof Lemma 13.

gs
R,i = gs
A∗R,ik
b

gs
R,i) = 2α(AR,i −

AR,i −

R,i −

α
k

k ≤

b

k

v

·

A∗R,i) + v,

(o(δs) + O(ǫs)). By Lemma 6, we have

O(k/m)

·

b

gs
R,i is (α, β, γs)-
k/n))

(o(δs) + O(

p

D.2.2 Proof of Lemma 14

gs with A∗ w.h.p. and established the descent property of Al-
We have shown the correlation of
gorithm 2. The next step is to show that the nearness is preserved at each iteration. To prove
As+1
holds with high probability, we recall the update rule
k

A∗k ≤

A∗k

2
k

−

b

PH (
gs. Here H = (hij) where hij = 1 if i

As+1 = As

−

η

gs),

gs) = H

◦

PH(

where
j) and hij = 0 otherwise. Also,
supp(A
•
note that As is (δs, 2)-near to A∗ for δs = O∗(1/ log n). We already proved that this holds for the
exact expectation gs in Lemma 7. To prove for
gs, we again apply matrix Bernstein’s inequality to
gs)
bound
k
Consider a matrix random variable Z ,
E[ZZ T ]
b
k
k

= O(1).
by O(k/m) because η = Θ(m/k) and
Ax)sgn(x)T ). Our goal is to bound the spectral
PH((y
b
E[Z T Z]
since Z is asymmetric. To simplify our notations,
norm
k
we denote by xR the vector x by zeroing out the elements not in R. Also, denote Ri = supp(hi)
and S = supp(x). Then Z can be written explicitly as

b
− PH (
and, both

b
kPH(gs)
Z

A∗k

and

b
∈

−

k

k

k

k

Z = [(y

Ax)R1 sgn(x1), . . . , (y

Ax)Rmsgn(xm)],

−

−

where many columns are zero since x is k-sparse. The following claims follow from the proof of
Claim 42 in Arora et al. (2015). Here we state and detail some important steps.

Claim 14.

Z

O(k) holds with high probability.

k

k ≤

e

34

Proof. With high probability

Z
k

k ≤

(y

2
Ax)Risgn(xi)
k

−

≤

√k

(y

k

Ax)Rik

−

k
sXi
S
∈

(y
k

−

where we use Claim 12 with

Claim 15.

E[ZZ T ]

O(k2/n) and

k

k ≤

Ax)Rk ≤
k

E[Z T Z]
e

k ≤

O(δs√k) w.h.p., then

Z

O(k) holds w.h.p.

k
O(k2/n) with high probability.

k ≤

e

Proof. The ﬁrst term is easily handled. Speciﬁcally, with high probability

e

E[ZZ T ]

k

k ≤ k

E[

(y

−

Ax)Ri sgn(xi)2(y

Ax)T

Ri]
k

−

=

E[
k

(y

Ax)Ri(y

−

−

Ax)T

Ri ]

k ≤

O(k2/n),

S
Xi
∈

S
Xi
∈

where the last inequality follows from the proof of Claim 42 in Arora et al. (2015), which is tedious
to be repeated.
To bound

, we use bound of the full matrix (y
O(√k) w.h.p. is similar to what derived in Claim 12. Then with high probability,

Ax)sgn(x)T . Note that

E[Z T Z]
k

y
k

k ≤

Ax

−

−

k

E[Z T Z]
k

E[sgn(x)(y

Ax)T (y

Ax)sgn(x)T ]

k ≤ k

e
where E[sgn(x)sgn(x)T ] = diag(q1, q2, . . . , qm) has norm bounded by O(k/m). We now can apply
O(k2/m), then with
Bernstein’s inequality for the truncated version of Z with
p =

O(k) and σ2 =

O(m),

k ≤

k ≤

R

−

=

−

e

e

E[sgn(x)sgn(x)T ]

O(k2/m).

O(k)
k

e

kPH(gs)

− PH(

gs)

k ≤

O(k)
p

e

e
O(k2/m)
p

≤

+

s

e

e

O∗(k/m)

holds with high probability. Finally, we invoke the bound η = O(m/k) and complete the proof.

b

Appendix E. A Special Case: Orthonormal A∗

We extend our results for the special case where the dictionary is orthonormal. As such, the
dictionary is perfectly incoherent and bounded (i.e., µ = 0 and

= 1).

Theorem 7. Suppose that A∗ is orthonormal. When p1 =
Ω(nr), then with high
probability Algorithm 1 returns an initial estimate A0 whose columns share the same support as
A∗ and with (δ, 2)-nearness to A∗ with δ = O∗(1/ log n). The sparsity of A∗ can be achieved up to
r = O∗

e

e

)

.

,

min( √n
log2 n

n
k2 log2 n

We use the same initialization procedure for this special case and achieve a better order of r.

(cid:0)

(cid:1)

The proof of Theorem 7 follows the analysis for the general case with following two results:

A∗k
k
Ω(n) and p2 =

Claim 16 (Special case of Claim 3). Suppose that u = A∗α + εu is a random sample and U =
supp(α). Let β = A∗
O(√k log n + σε√n log n).

T u, then w.h.p., we have (a)

σε log n for each i and (b)

αi| ≤

βi −
|

k ≤

β

k

T ǫu, then βi −
A∗
i, ǫui
=
Proof. We have β = A∗
α
h
k
•
in Claim 2, we have the claim proved.
α
and
A∗
probability bounds of
k
k
h
•
O(σε log2 n) and have the following
βiβ′i| ≤
V ,
U
We draw from the claim that for any i /
∈

T u = α + A∗
,
i, ǫui

. Using

ǫuk
k

ǫuk
k

αi =

and

−

∩

β

k

|

result:

35

Lemma 16. Fix samples u and v and suppose that y = A∗x∗ + ε is a random sample independent
of u, v. The expected value of the score for the lth component of y is given by:

el , E[
h

y, u

y, v

ih

i

y2
l ] =

2
li + perturbation terms
qiciβiβ′iA∗

where qi = P[i
absolute value at most O∗

S], qij = P[i, j

∈

V
U
Xi
∩
∈
S] and ci = E[x4
i
i |

∈
k/n log2 n max(1/√n, k2/n)

∈

S]. Moreover, the perturbation terms have
.

Proof. Lemma follows Lemma 1 via Claim 3 except that the second term of E1 is bounded by
O(k log2 n/n3/2).

(cid:0)

(cid:1)

Appendix F. Extensions of Arora et al. (2015)

F.1 Sample complexity in noisy case

In this section, we study the sample complexity of the algorithms in Arora et al. (2015) in the
presence of noise. While noise with order σε = O(1/√n) does not change the sample complexity
of the initialization algorithm, it aﬀects that of the descent stage. The analysis involves producing
a sharp bound for

.

gs
,i −
•

gs
ik
•

k

Lemma 17. For a regular dictionary A∗, suppose As is (δs, 2)-near to A∗ with δs = O∗(1/ log n),
then with high probability

k/n)) when p =

(o(δ) + O(

O(k/m)

b

Ω(m + σ2
ε

mn2
k ).

gs
,i −
k
•
Proof. This follows directly from Lemma 15 where r = n.
b

We tighten the original analysis to obtain the complexity

gs
ik ≤
•

·

case. Putting together with p =
mn2
sample complexity
k ) for the algorithms in Arora et al. (2015) in the noise regime.

O(mk + σ2
ε

e

e

p
Ω(mk) for the noiseless
Ω(m) instead of
Ω(mk) required by the initialization, we then have the overall

e

e

F.2 Extension of Arora et al. (2015)’s initialization algorithm for sparse case

e

We study a simple and straightforward extension of the initialization algorithm of Arora et al.
(2015) for the sparse case. This extension is produced by adding an extra projection, and is
described in Figure 3. The recovery of the support of A∗ is guaranteed by the following Lemma:
Lemma 18. Suppose that z∗ ∈
Provided z is δ-close to z∗ and z0 =
z∗ has the same support.

Rn is r-sparse whose nonzero entries are at least τ in magnitude.
Hr(z) with δ = O∗(1/ log n) and r = O∗(log2 n), then z0 and

Proof. Since z0 is δ-close to z∗, then

δ for every i. For i

supp(z∗),

∈

z0
z∗k ≤
−
k
zi −
z∗i | − |
zi| ≥ |
|
δ. Since τ > O(1/√r)

δ and

zi −
|
τ
z∗i | ≥

−

z∗i | ≤
δ

and for i /
≫
∈
support z∗, and hence z0 and z∗ has the same support.

supp(z∗),

zi| ≤
|

δ, then the r-largest entries of z are in the

√n
k log3 n

Theorem 8. Suppose that Assumptions B1-B4 hold and Assumptions A1-A3 satify with µ =
Ω(mk), then with high probability
O∗
Algorithm 3 returns an initial estimate A0 whose columns share the same support as A∗ and with
e
(δ, 2)-nearness to A∗ with δ = O∗(1/ log n).

and r = O∗(log2 n). When p1 =

Ω(m) and p2 =

e

(cid:1)

(cid:0)

36

Algorithm 3 Pairwise Reweighting with Hard-Thresholding

∅

Initialize L =
Randomly divide p samples into two disjoint sets
While
P1 at random
Reconstruct the re-weighted covariance matrix

< m. Pick u and v from

L
|

|

Mu,v:

P1 and

P2 of sizes p1 and p2 respectively

p2

1
p2

y(i), u
h
Xi=1

ih

Mu,v =

c

c
y(i), v

y(i)(y(i))T
i

Compute the top singular values δ1, δ2 and top singular vector z of
If δ1 ≥
z =
If z is not within distance 1/ log n of any vector in L even with sign ﬂip

Hr keeps r largest entries of z

Ω(k/m) and δ2 < O∗(k/m log n)
Hr(z), where
z
L = L
∪ {

Mu,v

c

}

Return A0 = (L1, . . . , Lm)

This algorithm requires r = O∗(log2 n), which is somewhat better than ours. However, the

sample complexity and running time is inferior as compared with our novel algorithm.

Appendix G. Neural Implementation of Our Approach

We now brieﬂy describe why our algorithm is “neurally plausible”. Basically, similar to the argu-
ment in Arora et al. (2015), we describe at a very high level how our algorithm can be implemented
via a neural network architecture. One should note that although both our initialization and de-
scent stages are non-trivial modiﬁcations of those in Arora et al. (2015), both still inherit the nice
neural plausiblity property.

G.1 Neural implementation of Stage 1: Initialization

Recall that the initialization stage includes two main steps: (i) estimate the support of each column
of the synthesis matrix, and (ii) compute the top principal component(s) of a certain truncated
weighted covariance matrix. Both steps involve simple vector and matrix-vector manipulations that
can be implemented plausibly using basic neuronal manipulations.
y, u

For the support estimation step, we compute the product
y, u
i
h

y, followed by a thresh-
y
y, u
i
olding. The inner products,
can be computed using neurons via an online manner
where the samples arrive in sequence; the thresholding can be implemented via a ReLU-type non-
linearity.

and

y, v

ih

◦

h

i

h

For the second step, it is well known that the top principal components of a matrix can be

computed in a neural (Hebbian) fashion using Oja’s Rule Oja (1992).

G.2 Neural implementation of Stage 2: Descent

Our neural implementation of the descent stage (Algorithm 2), shown in Figure 2, mimics the
architecture of Arora et al. (2015), which describes a simple two-layer network architecture for
computing a single gradient update of A. The only diﬀerence in our case is that most of the value

37

y

r

x =
threshold(AT y)

+

+

+

+

+

+

−

−

−

Aij

+

+
+

xj

Hebbian rule
Aij = ηrixj

∇

Figure 2: Neural network implementation of Algorithm 2. The network takes the image y as input
and produces the sparse representation x as output. The hidden layer represents the
residual between the image and its reconstruction Ax. The weights Aij’s are stored on
synapses, but most of them are zero and shown by the dotted lines.

in A are set to zero, or in other words, our network is sparse. The network takes values y from the
input layer and produce x as the output; there is an intermediate layer in between connecting the
middle layer with the output via synapses. The synaptic weights are stored on A. The weights are
updated by Hebbian learning. In our case, since A is sparse (with support given by R, as estimated
in the ﬁrst stage), we enforce the condition the corresponding synapses are inactive. In the output
layer, as in the initialization stage, the neurons can use a ReLU-type non-linear activation function
to enforce the sparsity of x.

References

Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon.
Learning sparsely used overcomplete dictionaries. In Conference on Learning Theory, pages 123–
137, 2014.

Michal Aharon, Michael Elad, and Alfred Bruckstein. k-svd: An algorithm for designing over-
complete dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):
4311–4322, 2006.

Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcom-

plete dictionaries. In Conference on Learning Theory, pages 779–806, 2014.

Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, eﬃcient, and neural algorithms

for sparse coding. In Conference on Learning Theory, pages 113–149, 2015.

38

Jaros law B lasiok and Jelani Nelson. An improved analysis of the er-spud dictionary learning algo-

rithm. arXiv preprint arXiv:1602.05719, 2016.

Y-Lan Boureau, Francis Bach, Yann LeCun, and Jean Ponce. Learning mid-level features for
recognition. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on,
pages 2559–2566. IEEE, 2010.

Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on

information theory, 51(12):4203–4215, 2005.

Niladri Chatterji and Peter Bartlett. Alternating minimization for dictionary learning with random

initialization. 2017. arXiv:1711.03634v1.

Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over

learned dictionaries. IEEE Transactions on Image processing, 15(12):3736–3745, 2006.

Kjersti Engan, Sven Ole Aase, and J Hakon Husoy. Method of optimal directions for frame design. In
IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 5,
pages 2443–2446. IEEE, 1999.

Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of

the 27th International Conference on Machine Learning (ICML), pages 399–406, 2010.

R´emi Gribonval, Rodolphe Jenatton, and Francis Bach. Sparse and spurious: dictionary learning
with noise and outliers. IEEE Transactions on Information Theory, 61(11):6298–6319, 2015a.

R´emi Gribonval, Rodolphe Jenatton, Francis Bach, Martin Kleinsteuber, and Matthias Seibert.
Sample complexity of dictionary learning and other matrix factorizations. IEEE Transactions
on Information Theory, 61(6):3469–3486, 2015b.

Hamid Krim, Dewey Tucker, Stephane Mallat, and David Donoho. On denoising and best signal

representation. IEEE Transactions on Information Theory, 45(7):2225–2238, 1999.

Rados law Adamczak. A note on the sample complexity of the er-spud algorithm by spielman,
wang and wright for exact recovery of sparsely used dictionaries. Journal of Machine Learning
Research, 17:1–18, 2016.

Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for
sparse coding. In Proceedings of the 26th International Conference on Machine Learning (ICML),
pages 689–696, 2009.

Arya Mazumdar and Ankit Singh Rawat. Associative memory using dictionary learning and ex-
pander decoding. In Proc. Conf. American Assoc. Artiﬁcial Intelligence (AAAI), pages 267–273,
2017.

Thanh V. Nguyen, Raymond K. W. Wong, and Chinmay Hegde. A provable approach for double-

sparse coding. In Proc. Conf. American Assoc. Artiﬁcial Intelligence (AAAI), Feb. 2018.

Erkki Oja. Principal components, minor components, and linear neural networks. Neural networks,

5(6):927–935, 1992.

39

Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy

employed by v1? Vision research, 37(23):3311–3325, 1997.

Ron Rubinstein, Alfred M Bruckstein, and Michael Elad. Dictionaries for sparse representation

modeling. Proceedings of the IEEE, 98(6):1045–1057, 2010a.

Ron Rubinstein, Michael Zibulevsky, and Michael Elad. Double sparsity: Learning sparse dictionar-
ies for sparse signal approximation. IEEE Transactions on Signal Processing, 58(3):1553–1564,
2010b.

Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries.

In Conference on Learning Theory, pages 37–1, 2012.

Jeremias Sulam, Boaz Ophir, Michael Zibulevsky, and Michael Elad. Trainlets: Dictionary learning

in high dimensions. IEEE Transactions on Signal Processing, 64(12):3180–3193, 2016.

Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery using nonconvex optimization.
In Proceedings of the 33rd International Conference on Machine Learning (ICML), pages 2351–
2360, 2015.

Lingxiao Wang, Xiao Zhang, and Quanquan Gu. A uniﬁed computational and statistical framework

for nonconvex low-rank matrix estimation. arXiv preprint arXiv:1610.05275, 2016.

Yuchen Zhang, Xi Chen, Dengyong Zhou, and Michael I Jordan. Spectral methods meet em: A
provably optimal algorithm for crowdsourcing. Journal of Machine Learning Research, 17(1):
3537–3580, 2016.

40

7
1
0
2
 
c
e
D
 
2
1

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
8
3
6
3
0
.
1
1
7
1
:
v
i
X
r
a

Provably Accurate Double-Sparse Coding

Thanh V. Nguyen
Iowa State University, ECE Department

Raymond K. W. Wong
Texas A&M University, Statistics Department

Chinmay Hegde ∗
Iowa State University, ECE Department

Editor: TBD

thanhng@iastate.edu

raywong@stat.tamu.edu

chinmay@iastate.edu

Abstract
Sparse coding is a crucial subroutine in algorithms for various signal processing, deep learning,
and other machine learning applications. The central goal is to learn an overcomplete dictionary
that can sparsely represent a given input dataset. However, a key challenge is that storage, trans-
mission, and processing of the learned dictionary can be untenably high if the data dimension is
high. In this paper, we consider the double-sparsity model introduced by Rubinstein et al. (2010b)
where the dictionary itself is the product of a ﬁxed, known basis and a data-adaptive sparse com-
ponent. First, we introduce a simple algorithm for double-sparse coding that can be amenable to
eﬃcient implementation via neural architectures. Second, we theoretically analyze its performance
and demonstrate asymptotic sample complexity and running time beneﬁts over existing (provable)
approaches for sparse coding. To our knowledge, our work introduces the ﬁrst computationally
eﬃcient algorithm for double-sparse coding that enjoys rigorous statistical guarantees. Finally,
we support our analysis via several numerical experiments on simulated data, conﬁrming that our
method can indeed be useful in problem sizes encountered in practical applications.
Keywords: Sparse coding, provable algorithms, unsupervised learning

1. Introduction

1.1 Motivation

Representing signals as sparse linear combinations of atoms from a dictionary is a popular approach
in many domains. In this paper, we study the problem of dictionary learning (also known as sparse
coding), where the goal is to learn an eﬃcient basis (dictionary) that represents the underlying
class of signals well. In the typical sparse coding setup, the dictionary is overcomplete (i.e., the
cardinality of the dictionary exceeds the ambient signal dimension) while the representation is
sparse (i.e., each signal is encoded by a combination of only very few dictionary atoms.)

Sparse coding has a rich history in diverse ﬁelds such as signal processing, machine learning, and
computational neuroscience. Discovering optimal basis representations of data is a central focus of
image analysis (Krim et al., 1999; Elad and Aharon, 2006; Rubinstein et al., 2010a), and dictionary
learning has proven widely successful in imaging problems such as denoising, deconvolution, inpaint-

∗. This work is supported in part by the National Science Foundation under the grants CCF-1566281 and DMS-
1612985. An abbreviated conference version will appear in the proceedings of AAAI 2018 (Nguyen et al., 2018).

1

ing, and compressive sensing (Elad and Aharon, 2006; Candes and Tao, 2005; Rubinstein et al.,
2010a). Sparse coding approaches have also been used as a core building block of deep learn-
ing systems for prediction (Gregor and LeCun, 2010; Boureau et al., 2010) and associative mem-
ory (Mazumdar and Rawat, 2017). Interestingly, the seminal work by Olshausen and Field (1997)
has shown intimate connections between sparse coding and neuroscience: the dictionaries learned
from image patches of natural scenes bear strikingly resemblance to spatial receptive ﬁelds observed
in mammalian primary visual cortex.

From a mathematical standpoint, the sparse coding problem is formulated as follows. Given p
Rn
m (m > n)
data samples Y = [y(1), y(2), . . . , y(p)]
and corresponding sparse code vectors X = [x(1), x(2), . . . , x(p)]
p such that the representation
DX ﬁts the data samples as well as possible. Typically, one obtains the dictionary and the code
vectors as the solution to the following optimization problem:

p, the goal is to ﬁnd a dictionary D

Rm

Rn

∈

∈

∈

×

×

×

min
D,X L

(D, X) =

y(j)
k

−

Dx(j)

2
2,
k

p

1
2

Xj=1
(x(j))

S

≤

s.t.

p

S

Xj=1

(1)

) is some sparsity-inducing penalty function on the code vectors, such as the ℓ1-norm.
(
·
controls the reconstruction error while the constraint enforces the sparsity

However, even a cursory attempt at solving the optimization problem (1) reveals the following

S

where
The objective function
of the representation.

L

obstacles:

1. Theoretical challenges. The constrained optimization problem (1) involves a non-convex
(in fact, bilinear) objective function, as well as potentially non-convex constraints depending
on the choice of the sparsity-promoting function
(for example, the ℓ0 function.) Hence,
obtaining provably correct algorithms for this problem can be challenging. Indeed, the vast
majority of practical approaches for sparse coding have been heuristics (Engan et al., 1999;
Aharon et al., 2006; Mairal et al., 2009); Recent works in the theoretical machine learning
community have bucked this trend, providing provably accurate algorithms if certain assump-
tions are satisﬁed (Spielman et al., 2012; Agarwal et al., 2014; Arora et al., 2015; Sun et al.,
2015; B lasiok and Nelson, 2016; law Adamczak, 2016; Chatterji and Bartlett, 2017). How-
ever, relatively few of these newer methods have been shown to provide good empirical per-
formance in actual sparse coding problems.

S

2. Practical challenges. Even if theoretical correctness issues were to be set aside, and we are
somehow able to eﬃciently learn sparse codes of the input data, we often ﬁnd that applications
using such learned sparse codes encounter memory and running-time issues. Indeed, in the
overcomplete case, only the storage of the learned dictionary D incurs mn = Ω(n2) memory
cost, which is prohibitive when n is large. Therefore, in practical applications (such as image
analysis) one typically resorts to chop the data into smaller blocks (e.g., partitioning image
data into patches) to make the problem manageable.

A related line of research has been devoted to learning dictionaries that obey some type of struc-
ture. Such structural information can be leveraged to incorporate prior knowledge of underlying

2

signals as well as to resolve computational challenges due to the data dimension. For instance, the
dictionary is assumed to be separable, or obey a convolutional structure. One such variant is the
double-sparse coding problem (Rubinstein et al., 2010b; Sulam et al., 2016) where the dictionary
D itself exhibits a sparse structure. To be speciﬁc, the dictionary is expressed as:

D = ΦA,

×

∈

Rn

n, and a learned “synthesis” matrix
i.e., it is composed of a known “base dictionary” Φ
m whose columns are sparse. The base dictionary Φ is typically any ﬁxed basis chosen
A
according to domain knowledge, while the synthesis matrix A is column-wise sparse and is to be
learned from the data. The basis Φ is typically orthonormal (such as the canonical or wavelet
basis); however, there are cases where the base dictionary Φ is overcomplete (Rubinstein et al.,
2010b; Sulam et al., 2016).

∈

×

Rn

There are several reasons why such the double-sparsity model can be useful. First, the double-
sparsity assumption is rather appealing from a conceptual standpoint, since it lets us combine
the knowledge of decades of modeling eﬀorts in harmonic analysis with the ﬂexibility of learning
new representations tailored to speciﬁc data families. Moreover, such a double-sparsity model has
computational beneﬁts. If the columns of A are (say) r-sparse (i.e., each column contains no more
than r
n non-zeroes) then the overall burden of storing, transmitting, and computing with A is
much lower than that for general unstructured dictionaries. Finally, such a model lends itself well
to interpretable learned features if the atoms of the base dictionary are semantically meaningful.

≪

All the above reasons have spurred researchers to develop a series of algorithms to learn
doubly-sparse codes (Rubinstein et al., 2010b; Sulam et al., 2016). However, despite their empiri-
cal promise, no theoretical analysis of their performance have been reported in the literature and
to date, we are unaware of a provably accurate, polynomial-time algorithm for the double-sparse
coding problem. Our goal in this paper is precisely to ﬁll this gap.

1.2 Our Contributions

In this paper, we provide a new framework for double-sparse coding. To the best of our knowledge,
our approach is the ﬁrst method that enjoys provable statistical and algorithmic guarantees for
this problem. In addition, our approach enjoys three beneﬁts: we demonstrate that the method is
neurally plausible (i.e., its execution can plausibly be achieved using a neural network architecture),
robust to noise, as well as practically useful.

Inspired by the aforementioned recent theoretical advances in sparse coding, we assume a
learning-theoretic setup where the data samples arise from a ground-truth generative model. Infor-
mally, suppose there exists a true (but unknown) synthesis matrix A∗ that is column-wise r-sparse,
and the ith data sample is generated as:

y(i) = ΦA∗x∗

(i) + noise,

i = 1, 2, . . . , p

(i) is independently drawn from a distribution supported on the set of k-
where the code vector x∗
sparse vectors. We desire to learn the underlying matrix A∗. Informally, suppose that the synthesis
matrix A∗ is incoherent (the columns of A∗ are suﬃciently close to orthogonal) and has bounded
spectral norm. Finally, suppose that the number of dictionary elements, m, is at most a constant
multiple of n. All of these assumptions are standard1.

1. We clarify both the data and the noise model more concretely in Section 2 below.

3

We will demonstrate that the true synthesis matrix A∗ can be recovered (with small error) in
a tractable manner as suﬃciently many samples are provided. Speciﬁcally, we make the following
novel contributions:

1. We propose a new algorithm that produces a coarse estimate of the synthesis matrix that
is suﬃciently close to the ground truth A∗. In contrast with previous double-sparse coding
methods (such as Sulam et al. (2016)), our algorithm is not based on alternating minimiza-
tion. Rather, it builds upon spectral initialization-based ideas that have recently gained
popularity in non-convex machine learning (Zhang et al., 2016; Wang et al., 2016).

2. Given the above coarse estimate of the synthesis matrix A∗, we propose a descent-style algo-
rithm to reﬁne the above estimate of A∗. This algorithm is simpler than previously studied
double-sparse coding algorithms (such as the Trainlets approach of Sulam et al. (2016)), while
still giving good statistical performance. Moreover, this algorithm can be realized in a manner
amenable to neural implementations.

3. We provide a rigorous analysis of both algorithms. Put together, our analysis produces
the ﬁrst provably polynomial-time algorithm for double-sparse coding. We show that the
algorithm provably returns a good estimate of the ground-truth; in particular, in the absence
of noise we prove that Ω(mr polylog n) samples are suﬃcient for a good enough initialization
in the ﬁrst algorithm, as well as guaranteed linear convergence of the descent phase up to a
precise error parameter that can be interpreted as the radius of convergence.

Indeed, our analysis shows that employing the double-sparsity model helps in this context,
and leads to a strict improvement in sample complexity, as well as running time over previous
rigorous methods for (regular) sparse coding such as Arora et al. (2015).

4. We also analyze our approach in a more realistic setting with the presence of additive noise
and demonstrate its stability. We prove that Ω(mr polylog n) samples are suﬃcient to obtain
a good enough estimate in the initialization, and also to obtain guaranteed linear convergence
during descent to provably recover A∗.

5. We underline the beneﬁt of the double-sparse structure over the regular model by analyzing
the algorithms in Arora et al. (2015) under the noisy setting. As a result, we obtain the
, which demonstrates a negative eﬀect of noise
sample complexity O
on this approach.

mn2
k )polylog n

(mk + σ2
ε

(cid:0)

(cid:1)

6. We rigorously develop a hard thresholding intialization that extends the spectral scheme
in Arora et al. (2015). Additionally, we provide more results for the case where A is orthonor-
mal, sparse dictionary to relax the condition on r, which may be of independent interest.

7. While our analysis mainly consists of suﬃciency results and involves several (absolute) un-
speciﬁed constants, in practice we have found that these constants are reasonable. We justify
our observations by reporting a suite of numerical experiments on synthetic test datasets.

Overall, our approach results in strict improvement in sample complexity, as well as running
time, over previous rigorously analyzed methods for (regular) sparse coding, such as Arora et al.
(2015). See Table 1 for a detailed comparison.

4

Setting

Reference

Sample complexity
(w/o noise)

Sample complexity
(w/ noise)

Upper bound on
running time

Expt

Regular

MOD (Engan et al., 1999)

K-SVD (Aharon et al., 2006)

Spielman et al. (2012)

Arora et al. (2014)

Gribonval et al. (2015a)

Arora et al. (2015)

Double Sparsity (Rubinstein et al., 2010b)

Double
Sparse

Gribonval et al. (2015b)

Trainlets (Sulam et al., 2016)

✗

✗

✗

✗

O(n2 log n)

e
O(m2/k2)

O(nm3)

e
O(mk)

e
O(mr)

e
O(mr)

✗

✗

✗

✗

✗

✗

✗

O(nm3)

e
O(mr)

e
Ω(n4)

e
O(np2)

e
O(mn2p)

✗

✗

✗

✗

✗

✗

✓

✓

✓

✗

✗

✗

✓

✗

✓

✓

This paper

e
O(mr + σ2
ε

mnr
k )

e
O(mnp)

Table 1: Comparison of various sparse coding techniques. Expt: whether numerical experiments have
been conducted. ✗ in all other columns indicates no provable guarantees. Here, n is the signal
dimension, and m is the number of atoms. The sparsity levels for A and x are r and k respectively,
and p is the sample size.

1.3 Techniques

At a high level, our method is an adaptation of the seminal approach of Arora et al. (2015). As
is common in the statistical learning literature, we assume a “ground-truth” generative model for
the observed data samples, and attempt to estimate the parameters of the generative model given
a suﬃcient number of samples. In our case, the parameters correspond to the synthesis matrix A∗,
which is column-wise r-sparse. The natural approach is to formulate a loss function in terms of A
such as Equation (1), and perform gradient descent with respect to the surface of the loss function
to learn A∗.

The key challenge in sparse coding is that the gradient is inherently coupled with the codes of the
training samples (i.e., the columns of X ∗), which are unknown a priori. However, the main insight
of Arora et al. (2015) is that within a small enough neighborhood of A∗, a noisy version of X ∗ can be
estimated, and therefore the overall method is similar to performing approximate gradient descent.
Formulating the actual algorithm as a noisy variant of approximate gradient descent allows us to
overcome the ﬁnite-sample variability of the loss, and obtain a descent property directly related to
(the population parameter) A∗.

The second stage of our approach (i.e., our descent-style algorithm) leverages this intuition.
However, instead of standard gradient descent, we perform approximate projected gradient descent,
such that the column-wise r-sparsity property is enforced in each new estimate of A∗. Indeed, such
an extra projection step is critical in showing a sample complexity improvement over the existing
approach of Arora et al. (2015).The key novelty is in ﬁguring out how to perform the projection in
each gradient iteration. For this purpose, we develop a novel initialization algorithm that identiﬁes
the locations of the non-zeroes in A∗ even before commencing the descent phase. This is nontrivially

5

diﬀerent from initialization schemes used in previous rigorous methods for sparse coding, and the
analysis is somewhat more involved.

In Arora et al. (2015), (the principal eigenvector of) a weighted covariance matrix of y (esti-
mated by the weighted average of outer products yiyT
i ) is shown to provide a coarse estimate of
a dictionary atom. We extend this idea and rigoriously show that the diagonal of the weighted
covariance matrix serves as a good indicator of the support of a column in A∗. The success relies on
the concentration of the diagonal vector with dimension n, instead of the covariance matrix with
n. With the support selected, our scheme only utilizes a reduced weighted covariance
dimensions n
matrix with dimensions at most r
r. This initialization scheme enables us to eﬀectively reduce
the dimension of the problem, and therefore leads to signiﬁcant improvement in sample complexity
and running time over previous (provable) sparse coding methods when the data representation
sparsity k is much smaller than m.

×

×

Further, we rigorously analyze the proposed algorithms in the presence of noise with a bounded
expected norm. Our analysis shows that our method is stable, and in the case of i.i.d. Gaussian noise
with bounded expected ℓ2-norms, is at least a polynomial factor better than previous polynomial
time algorithms for sparse coding.

The empirical performance of our proposed method is demonstrated by a suite of numerical
experiments on synthetic datasets.In particular, we show that our proposed methods are simple
and practical, and improve upon previous provable algorithms for sparse coding.

1.4 Paper Organization

The remainder of this paper is organized as follows. Section 2 introduces notation, key model
assumptions, and informal statements of our main theoretical results. Section 3 outlines our ini-
tialization algorithm (along with supporting theoretical results) while Section 4 presents our descent
algorithm (along with supporting theoretical results). Section 5 provides a numerical study of the
eﬃciency of our proposed algorithms, and compares it with previously proposed methods. Finally,
Section 6 concludes with a short discussion. All technical proofs are relegated to the appendix.

2. Setup and Main Results

∈

= 0
}

}
[m] : xi 6

as the support set of x. Given any subset S

for any integer m > 1. For any vector x = [x1, x2, . . . , xm]T

2.1 Notation
Rm,
We deﬁne [m] ,
1, . . . , m
∈
{
we write supp(x) ,
[m], xS
i
{
m, we
corresponds to the sub-vector of x indexed by the elements of S. For any matrix A
×
i and AT
to represent the i-th column and the j-th row respectively. For some appropriate
use A
j
•
•
S ) be the submatrix of A with rows (respectively columns)
(respectively, A
sets R and S, let AR
•
•
i, we use AR,i
indexed by the elements in R (respectively S). In addition, for the i-th column A
•
to denote the sub-vector indexed by the elements of R. For notational simplicity, we use AT
to
R
indicate (AR
) to represent
and sgn(
·
•
the element-wise Hadamard operator and the element-wise sign function respectively. Further,
thresholdK (x) is a thresholding operator that replaces any elements of x with magnitude less than
K by zero.

)T , the tranpose of A after a row selection. Besides, we use

⊆
Rn

∈

◦

•

x

The ℓ2-norm

A
for a vector x and the spectral norm
for a matrix A appear several
k
k
,
times. In some cases, we also utilize the Frobenius norm
kF and the operator norm
A
k1,2 is essentially the maximal Euclidean norm of any column of A.
A
max
k

. The norm
k

A
k

k1,2

Ax

1k

k1≤

k

k

k

x

k

6

Ω(g(n))) if f (n) is upper bounded (respectively,
constant. Next, f (n) = Θ(g(n)) if and only if f (n) = O(g(n)) and f (n) = Ω(g(n)). Also
and
f (n) = o(g(n)) (or f (n) = ω(g(n))) if limn

For clarity, we adopt asymptotic notations extensively. We write f (n) = O(g(n)) (or f (n) =
lower bounded) by g(n) up to some positive
Ω
O represent Ω and O up to a multiplicative poly-logarithmic factor respectively. Finally
e
f (n)/g(n)
|
Throughout the paper, we use the phrase “with high probability” (abbreviated to w.h.p.) to
ω(1). In addition, g(n) = O∗(f (n))

describe an event with failure probability of order at most n−
means g(n)

Kf (n) for some small enough constant K.

f (n)/g(n)
|

= 0 (limn

→∞ |

→∞ |

∞

=

).

e

≤

2.2 Model

Suppose that the observed samples are given by

y(i) = Dx∗

(i) + ε,

i = 1, . . . , p,

i.e., we are given p samples of y generated from a ﬁxed (but unknown) dictionary D where the sparse
speciﬁed below. In the double-sparse
code x∗ and the error ε are drawn from a joint distribution
n is a known
setting, the dictionary is assumed to follow a decomposition D = ΦA∗, where Φ
orthonormal basis matrix and A∗ is an unknown, ground truth synthesis matrix. An alternative
(and interesting) setting is an overcomplete Φ with a square A∗, which our analysis below does not
cover; we defer this to future work. Our approach relies upon the following assumptions on the
synthesis dictionary A∗:

Rn

D

∈

×

A1 A∗ is overcomplete (i.e., m

n) with m = O(n).

≥

A2 A∗ is µ-incoherent, i.e., for all i

= j,

A∗
i, A∗
j i| ≤
•
•

|h

µ/√n.

A3 A∗
i has at most r non-zero elements, and is normalized such that
•
A∗ij| ≥

= 0 and τ = Ω(1/√r).

τ for A∗ij 6

|

A∗
ik
k
•

= 1 for all i. Moreover,

A4 A∗ has bounded spectral norm such that

A∗k ≤
k

O(

m/n).

p

All these assumptions are standard. In Assumption A2, the incoherence µ is typically of order
O(log n) with high probability for a normal random matrix (Arora et al., 2014). Assumption A3
is a common assumption in sparse signal recovery. The bounded spectral norm assumption is
In addition to Assumptions A1-A4, we make the following
also standard (Arora et al., 2015).
distributional assumptions on

:

D

B1 Support S = supp(x∗) is of size at most k and uniformly drawn without replacement from

[m] such that P[i

S] = Θ(k/m) and P[i, j

S] = Θ(k2/m2) for some i, j

[m] and i

= j.

∈
B2 The nonzero entries x∗S are pairwise independent and sub-Gaussian given the support S with

∈

∈

E[x∗i |
i
B3 For i

∈

S] = 0 and E[x∗
2
i

S] = 1.

i
|

∈

S,

x∗i | ≥
|

∈

C where 0 < C

1.

≤

B4 The additive noise ε has i.i.d. Gaussian entries with variance σ2

ε with σε = O(1/√n).

7

For the rest of the paper, we set Φ = In, the identity matrix of size n. This only simpliﬁes the

arguments but does not change the problem because one can study an equivalent model:

y′ = Ax∗ + ε′,

where y′ = ΦT y and ε′ = ΦT ε, as ΦT Φ = In. Due to the Gaussianity of ε, ε′ also has independent
entries. Although this property is speciﬁc to Gaussian noise, all the analysis carried out below
can be extended to sub-Gaussian noise with minor (but rather tedious) changes in concentration
arguments.

Our goal is to devise an algorithm that produces an provably “good” estimate of A∗. For this,
we need to deﬁne a suitable measure of “goodness”. We use the following notion of distance that
measures the maximal column-wise diﬀerence in ℓ2-norm under some suitable transformation.

Deﬁnition 1 ((δ, κ)-nearness). A is said to be δ-close to A∗ if there is a permutation π : [m]
and a sign ﬂip σ : [m] :
to be (δ, κ)-near to A∗ if

[m]
δ for every i. In addition, A is said

→

σ(i)A
A∗
π(i) −
ik ≤
•
•
also holds.
A∗k

k
κ
k

such that
1
}
{±
A∗k ≤
A
π −
k
•

For notational simplicity, in our theorems we simply replace π and σ in Deﬁnition 1 with the
identity permutation π(i) = i and the positive sign σ(
) = +1 while keeping in mind that in reality
·
we are referring to one element of the equivalence class of all permutations and sign ﬂip transforms
of A∗.

We will also need some technical tools from Arora et al. (2015) to analyze our gradient descent-
Rn to optimize
style method. Consider any iterative algorithm that looks for a desired solution z∗ ∈
some function f (z). Suppose that the algorithm produces a sequence of estimates z1, . . . , zs via
the update rule:

zs+1 = zs

ηgs,

−

for some vector gs and scalar step size η. The goal is to characterize “good” directions gs such that
the sequence converges to z∗ under the Euclidean distance. The following gives one such suﬃcient
condition for gs.
Deﬁnition 2. A vector gs at the sth iteration is (α, β, γs)-correlated with a desired solution z∗ if

gs, zs

h

z∗

−

i ≥

zs

α
k

z∗

2 + β
k

gs
k

2
k

−

−

γs.

We know from convex optimization that if f is 2α-strongly convex and 1/2β-smooth, and gs is
∇zf (z), then gs is (α, β, 0)-correlated with z∗. In our setting, the desired
chosen as the gradient
solution corresponds to A∗, the ground-truth synthesis matrix. In Arora et al. (2015), it is shown
that gs = Ey[(Asx
y)sgn(x)T ], where x = thresholdC/2((As)T y) indeed satisﬁes Deﬁnition 2. This
gs is a population quantity and not explicitly available, but one can estimate such gs using an
gs is a random variable, so we also need a related
empirical average. The corresponding estimator
correlated-with-high-probability condition:

−

Deﬁnition 3. A direction
tion z∗ if, w.h.p.,

gs at the sth iteration is (α, β, γs)-correlated-w.h.p. with a desired solu-

gs, zs

b
h

z∗

−

i ≥

zs

α
k

z∗

2 + β
k

gs
k

2
k

−

−

γs.

From Deﬁnition 2, one can establish a form of descent property in each update step, as shown

b

b

in Theorem 1.

b

8

Theorem 1. Suppose that gs satisﬁes the condition described in Deﬁnition 2 for s = 1, 2, . . . , T .
Moreover, 0 < η

s=1 γs. Then, the following holds for all s:

2β and γ = maxT

≤

zs+1

k

z∗

2
k

−

(1

2αη)
k

−

≤

zs

z∗

2 + 2ηγs.
k

−

In particular, the above update converges geometrically to z∗ with an error γ/α. That is,

−
We can obtain a similar result for Deﬁnition 3 except that

−

−

≤

k

zs+1
k

2

z∗

(1

2αη)s

z0
k

z∗

2 + 2γ/α.
k

expectation.

zs+1

k

z∗k

−

2 is replaced with its

Armed with the above tools, we now state some informal versions of our main results:

Theorem 2 (Provably correct initialization, informal). There exists a neurally plausible algorithm
to produce an initial estimate A0 that has the correct support and is (δ, 2)-near to A∗ with high
probability.
O(mr) respectively. This
algorithm works when the sparsity level satisﬁes r = O∗(log n).

Its running time and sample complexity are

O(mnp) and

e

e

Our algorithm can be regarded as an extension of Arora et al. (2015) to the double-sparse
setting. It reconstructs the support of one single column and then estimates its direction in the
subspace deﬁned by the support. Our proposed algorithm enjoys neural plausibility by implement-
ing a thresholding non-linearity and Oja’s update rule. We provide a neural implementation of our
algorithm in Appendix G. The adaption to the sparse structure results in a strict improvement
upon the original algorithm both in running time and sample complexity. However, our algorithm
is limited to the sparsity level r = O∗(log n), which is rather small but plausible from the modeling
standpoint. For comparison, we analyze a natural extension of the algorithm of Arora et al. (2015)
with an extra hard-thresholding step for every learned atom. We obtain the same order restriction
on r, but somewhat worse bounds on sample complexity and running time. The details are found
in Appendix F.

We hypothesize that a stronger incoherence assumption can lead to provably correct initial-
ization for a much wider range of r. For purposes of theoretical analysis, we consider the special
case of a perfectly incoherent synthesis matrix A∗ such that µ = 0 and m = n. In this case, we
, which is an exponential
can indeed improve the sparsity parameter to r = O∗
improvement. This analysis is given in Appendix E.

min( √n
log2 n

n
k2 log2 n

)

,

(cid:0)

(cid:1)

Theorem 3 (Provably correct descent, informal). There exists a neurally plausible algorithm for
double-sparse coding that converges to A∗ with geometric rate when the initial estimate A0 has the
correct support and (δ, 2)-near to A∗. The running time per iteration is O(mkp + mrp) and the
sample complexity is

O(m + σ2
ε

mnr
k ).

e

Similar to Arora et al. (2015), our proposed algorithm enjoys neural plausibility. Moreover, we
can achieve a better running time and sample complexity per iteration than previous methods,
particularly in the noisy case. We show in Appendix F that in this regime the sample complexity
O(m + σ2
1/2, the sample complexity
of Arora et al. (2015) is
ε
O(m) in the noiseless case. In contrast, our proposed method
bound is signiﬁcantly worse than
leverages the sparse structure to overcome this problem and obtain improved results.

k ). For instance, when σε ≍

n−

mn2

e

We are now ready to introduce our methods in detail. As discussed above, our approach consists
of two stages: an initialization algorithm that produces a coarse estimate of A∗, and a descent-style
algorithm that reﬁnes this estimate to accurately recover A∗.

e

9

Algorithm 1 Truncated Pairwise Reweighting

∅

Initialize L =
Randomly divide p samples into two disjoint sets
While
P1 at random

< m. Pick u and v from

For every l = 1, 2, . . . , n; compute

L
|

|

P1 and

P2 of sizes p1 and p2 respectively

p2

el =

1
p2

y(i), u
h

ih

y(i), v

(y(i)

l )2

i

e(r′) < O∗(r/ log2 n)

Xi=1
b
en) in descending order

e2, . . . ,

e(r′+1)/

e(r′) ≥
b

Sort (
e1,
If r′ ≤
O(k/mr) and
r s.t
R be set of the r largest entries of
Let
b
b
(y(i)
y(i)
Mu,v = 1
y(i), v
b
b
b
b
b
p2
i
R
R
b
Mu,v
top singular values of
δ1, δ2 ←
c
z bR ←
If δ1 ≥

top singular vector of

Ω(k/m) and δ2 < O∗(k/m log n)

Mu,v
c

p2
i=1h

y(i), u

e
)T
b

P

c

ih

If dist(

z, l) > 1/ log n for any l

L

∈

±

Update L = L

z
∪ {

}

Return A0 = (L1, . . . , Lm)

3. Stage 1: Initialization

In this section, we present a neurally plausible algorithm that can produce a coarse initial estimate
of the ground truth A∗. We give a neural implementation of the algorithm in Appendix G.

Our algorithm is an adaptation from the algorithm in Arora et al. (2015). The idea is to estimate
dictionary atoms in a greedy fashion by iteratively re-weighting the given samples. The samples
are re-scaled in a way that the weighted (sample) covariance matrix has the dominant ﬁrst singular
value, and its corresponding eigenvector is close to one particular atom with high probability.
However, while this algorithm is conceptually very appealing, it incurs severe computational costs
O(mn2p) in expectation, which is unrealistic
in practice. More precisely, the overall running time is
for large-scale problems.

e

To overcome this burden, we leverage the double-sparsity assumption in our generative model to
obtain a more eﬃcient approach. The high-level idea is to ﬁrst estimate the support of each column
in the synthesis matrix A∗, and then obtain a coarse estimate of the nonzero coeﬃcients of each
column based on knowledge of its support. The key ingredient of our method is a novel spectral
procedure that gives us an estimate of the column supports purely from the observed samples. The
full algorithm, that we call Truncated Pairwise Reweighting, is listed in pseudocode form below as
Algorithm 1.

Let us provide some intuition of our algorithm. Fix a sample y = A∗x∗ + ε from the available

training set, and consider samples

u = A∗α + εu, v = A∗α′ + εv.

10

Now, consider the (very coarse) estimate for the sparse code of u with respect to A∗:

β = A∗

T u = A∗

T A∗α + A∗

T εu.

y, u
h

x∗, β

x∗, α
i

.

i ≈ h

i ≈ h

As long as A∗ is incoherent enough and εu is small, the estimate β behaves just like α, in the sense
that for each sample y:

Moreover, the above inner products are large only if α and x∗ share some elements in their supports;
depends on whether or not x∗
else, they are likely to be small. Likewise, the weight
shares the support with both α and α′.

y, u
h

y, v

ih

i

i

ih

y, v

y, u
h

Now, suppose that we have a mechanism to isolate pairs u and v who share exactly one atom
among their sparse representations. Then by scaling each sample y with an increasing function
of
and linearly adding the samples, we magnify the importance of the samples that
are aligned with that atom, and diminish the rest. The ﬁnal direction can be obtained via the
top principal component of the reweighted samples and hence can be used as a coarse estimate of
the atom. This is exactly the approach adopted in Arora et al. (2015). However, in our double-
sparse coding setting, we know that the estimated atom should be sparse as well. Therefore, we
can naturally perform an extra “sparsiﬁcation” step of the output. An extended algorithm and
its correctness are provided in Appendix F. However, as we discussed above, the computational
complexity of the re-weighting step still remains.

We overcome this obstacle by ﬁrst identifying the locations of the nonzero entries in each atom.

Speciﬁcally, deﬁne the matrix:

Mu,v =

p2

1
p2

y(i), u
h
Xi=1

ih

y(i), v

y(i)

y(i).

i

◦

Then, the diagonal entries of Mu,v reveals the support of the atom of A∗ shared among u and v: the
r-largest entries of Mu,v will correspond to the support we seek. Since the desired direction remains
unchanged in the r-dimensional subspace of its nonzero elements, we can restrict our attention to
Mu,v, and proceed as before. This truncation
this subspace, construct a reduced covariance matrix
O(mnp),
step alleviates the computational burden by a signiﬁcant amount; the running time is now
which improves the original by a factor of n.

c

The success of the above procedure relies upon whether or not we can isolate pairs u and v that
share one dictionary atom. Fortunately, this can be done via checking the decay of the singular
values of the (reduced) covariance matrix. Here too, we show via our analysis that the truncation
step plays an important role. Overall, our proposed algorithm not only accelerates the initialization
in terms of running time, but also improves the sample complexity over Arora et al. (2015). The
performance of Algorithm 1 is described in the following theorem, whose formal proof is deferred
to Appendix B.

e

√n
k log3 n

Theorem 4. Suppose that Assumptions B1-B4 hold and Assumptions A1-A3 satify with µ =
Ω(mr), then with high probability
O∗
Algorithm 1 returns an initial estimate A0 whose columns share the same support as A∗ and with
(δ, 2)-nearness to A∗ with δ = O∗(1/ log n).

and r = O∗(log n). When p1 =

Ω(m) and p2 =

e

e

(cid:0)

(cid:1)

The limit on r arises from the minimum non-zero coeﬃcient τ of A∗. Since the columns of A∗
are standardized, τ should degenerate as r grows. In other words, it is getting harder to distinguish

11

the “signal” coeﬃcients from zero as r grows with n. However, this limitation can be relaxed when
a better incoherence available, for example the orthonormal case. We study this in Appendix E.

To provide some intuition about the working of the algorithm (and its proof), let us analyze it in
the case where we have access to inﬁnite number of samples. This setting, of course, is unrealistic.
However, the analysis is much simpler and more transparent since we can focus on expected values
rather than empirical averages. Moreover, the analysis reveals several key lemmas, which we will
reuse extensively for proving Theorem 4. First, we give some intuition behind the deﬁnition of the
“scores”,

el.

Lemma 1. Fix samples u and v and suppose that y = A∗x∗ + ε is a random sample independent
of u, v. The expected value of the score for the lth component of y is given by:

b

el , E[
h

y, u

y, v

ih

i

y2
l ] =

2
li + perturbation terms
qiciβiβ′iA∗

where qi = P[i
absolute value at most O∗(k/m log n).

S], qij = P[i, j

∈

∈

V
U
Xi
∩
∈
S] and ci = E[x4
i
i |

∈

S]. Moreover, the perturbation terms have

i

From Assumption B1, we know that qi = Θ(k/m), qij = Θ(k2/m2) and ci = Θ(1). Besides,
= o(1) for i /
U . Consider the ﬁrst
∈
or that l does not belong to support
V =
τ .

= Ω(1) for i
we will show later that
βi| ≈ |
|
2
li . Clearly, E0 = 0 if U
V qiciβiβ′iA∗
term E0 =
U
∩
∈
of any atom in U
Ω(τ 2k/m) = Ω(k/mr) since

αi|
∈
V . On the contrary, as E0 6
qiciβiβ′i| ≥
|

βi|
|
V =
∩
∅
= 0 and U
∩
A∗li| ≥
|

Therefore, Lemma 1 suggests that if u and v share a unique atom among their sparse represen-
tations, and r is not too large, then we can indeed recover the correct support of the shared atom.
When this is the case, the expected scores corresponding to the nonzero elements of the shared
atom will dominate the remaining of the scores.

2
qiciβiβ′iA∗
li | ≥
|

, then E0 =

Ω(k/m) and

U , and

i
}
{

P

∩

Now, given that we can isolate the support R of the corresponding atom, the remaining questions
are how best we can estimate its non-zero coeﬃcients, and when u and v share a unique elements
in their supports. These issues are handled in the following lemmas.

Lemma 2. Suppose that u = A∗α + εu and v = A∗α′ + εv are two random samples. Let U and
V denote the supports of α and α′ respectively. R is the support of some atom of interest. The
truncated re-weighting matrix is formulated as

Mu,v , E[
y, u
h

ih

y, v

yRyT

R] =

i

T
R,i + perturbation terms
qiciβiβ′iA∗R,iA∗

V
U
Xi
∩
∈

where the perturbation terms have norms at most O∗(k/m log n).

Using the same argument for bounding E0 in Lemma 1, we can see that M0 , qiciβiβ′iA∗R,iA∗
T
R,i
has norm at least Ω(k/m) when u and v share a unique element i (
= 1). According to this
k
lemma, the spectral norm of M0 dominates those of the other perturbation terms. Thus, given R
i.
we can use the ﬁrst singular vector of Mu,v as an estimate of A∗
•

A∗R,ik

Lemma 3. Under the setup of Theorem 4, suppose u = A∗α + εu and v = A∗α′ + εv are two
random samples with supports U and V respectively. R = supp(A∗i ). If u and v share the unique
atom i, the ﬁrst r largest entries of el is at least O(k/mr) and belong to R. Moreover, the top
singular vector of Mu,v is δ-close to A∗R,i for O∗(1/ log n).

12

i’s support directly follows Lemma 1. For the latter part, recall from
Proof. The recovery of A∗
•
Lemma 2 that

T
R,i + perturbation terms
Mu,v = qiciβiβ′iA∗R,iA∗

The perturbation terms have norms bounded by O∗(k/m log n). On the other hand, the ﬁrst term
is has norm at least Ω(k/m) since
Ω(k/m).
Then using Wedin’s Theorem to Mu,v, we can conclude that the top singular vector must be
O∗(k/m log n)/Ω(k/m) = O∗(1/ log n) -close to A∗R,i.

= 1 for the correct support R and

qiciβiβ′i| ≥

A∗R,ik

k

|

Lemma 4. Under the setup of Theorem 4, suppose u = A∗α+εu and v = A∗α′ +εv are two random
samples with supports U and V respectively. If the top singular value of Mu,v is at least Ω(k/m) and
the second largest one is at most O∗(k/m log n), then u and v share a unique dictionary element
with high probability.

Proof. The proof follows from that of Lemma 37 in Arora et al. (2015). The main idea is to
separate the possible cases of how u and v share support and to use Lemma 2 with the bounded
perturbation terms to conclude when u and v share exactly one. We note that due to the condition
where
O∗(r/ log n), it must be the case that u and v share only
one atom or share more than one atoms with the same support. When their supports overlap more
than one, then the ﬁrst singular value cannot dominate the second one, and hence it must not be
b
the case.

Ω(k/mr) and

e(s) ≤

e(s) ≥

e(s+1)/

b

b

Similar to (Arora et al., 2015), our initialization algorithm requires

tation to estimate all the atoms, hence the expected running time is
Lemma 1 and 2 are deferred to Appendix B.

O(m) iterations in expec-
O(mnp). All the proofs of

e

e

4. Stage 2: Descent

We now adapt the neural sparse coding approach of Arora et al. (2015) to obtain an improved
estimate of A∗. As mentioned earlier, at a high level the algorithm is akin to performing approximate
gradient descent. The insight is that within a small enough neighborhood (in the sense of δ-
closeness) of the true A∗, an estimate of the ground-truth code vectors, X ∗, can be constructed
using a neurally plausible algorithm.

The innovation, in our case, is the double-sparsity model since we know a priori that A∗ is itself
sparse. Under suﬃciently many samples, the support of A∗ can be deduced from the initialization
stage; therefore we perform an extra projection step in each iteration of gradient descent. In this
sense, our method is non-trivially diﬀerent from Arora et al. (2015). The full algorithm is presented
as Algorithm 2.

As discussed in Section 2, convergence of noisy approximate gradient descent can be achieved
gs is correlated-w.h.p. with the true solution. However, an analogous convergence result
as long as
for projected gradient descent does not exist in the literature. We ﬁll this gap via a careful analysis.
gs (i.e., when it
Due to the projection, we only require the correlated-w.h.p. property for part of
is restricted to some support set) with A∗. The descent property is still achieved via Theorem 5.
(A, X); therefore, we can
Due to various perturbation terms,
∇AL
k/n). The performance
only reﬁne the estimate of A∗ until the column-wise error is of order O(
of Algorithm 2 can be characterized via the following theorem.
b

g is only a biased estimate of

b

b

p

13

Algorithm 2 Double-Sparse Coding Descent Algorithm

Initialize A0 is (δ, 2)-near to A∗. H = (hij )n
Repeat for s = 0, 1, . . . , T

×

m where hij = 1 if i

supp(A0
j) and 0 otherwise.
•

∈

Encode: x(i) = thresholdC/2((As)T y(i))
PH(As
Update: As+1 =
−
p
i=1(Asx(i)
where

gs = 1
p

η

η

gs)
gs) = As
−
y(i))sgn(x(i))T and
−
b
b

for i = 1, 2, . . . , p
PH (

PH (G) = H

◦

G

P

b

Theorem 5. Suppose that the initial estimate A0 has the correct column supports and is (δ, 2)-near
to A∗ with δ = O∗(1/ log n). If Algorithm 2 is provided with p =
Ω(mr) fresh samples at each step
and η = Θ(m/k), then

E[
As
i −
k
•

A∗
ik
•

2]

(1

ρ)s

A0
i −
•

A∗
ik
•

k
for some 0 < ρ < 1/2 and for s = 1, 2, . . . , T . Consequently, As converges to A∗ geometrically until
column-wise error O(

k/n).

p

≤

−

e
2 + O(

k/n)

p

We defer the full proof of Theorem 5 to Section D. In this section, we take a step towards
gs in the inﬁnite sample case, which is equivalent to its
y)sgn(x)T ]. We establish the (α, β, γs)-correlation of a truncated version

understanding the algorithm by analyzing
expectation gs , E[(Asx
of gs
i with A∗
i to obtain the descent in Theorem 6 for the inﬁnite sample case.
•
•

Theorem 6. Suppose that the initial estimate A0 has the correct column supports and is (δ, 2)-near
to A∗. If Algorithm 2 is provided with inﬁnite number of samples at each step and η = Θ(m/k),
then

−

b

ρ)
k
for some 0 < ρ < 1/2 and for s = 1, 2, . . . , T . Consequently, it converges to A∗ geometrically until
column-wise error is O(k/n).

A∗
ik
•

A∗
ik
•

(1

≤

−

k

(cid:0)

(cid:1)

As+1
i −
•

2 + O

k2/n2

As
i −
•

2

Note that the better error O(k2/n2) is due to the fact that inﬁnitely many samples are given.
k/n) in Theorem 5 is a trade-oﬀ between the accuracy and the sample complexity
The term O(
of the algorithm. The proof of this theorem composes of two steps with two main results: 1) an
explicit form of gs (Lemma 6); 2) (α, β, γs)-correlation of column-wise gs with A∗ (Lemma 6). The
proof of those lemmas are deferred to Appendix C. Since the correlation primarily relies on the
(δ, 2)-nearness of As to A∗ that is provided initially and maintained at each step, then we need to
argue that the nearness is preserved after each step.

p

Lemma 5. Suppose that the initial estimate A0 has the correct column supports and is (δ, 2)-near to
A∗. The column-wise update has the form gs
R,i = piqi(λs
ζ) where R = supp(As
i),
•
λs
i =

A∗R,i + ξs

i As

i ±

and

As
i, A∗
ii
h
•
•

i = As
ξs
R,

idiag(qij)(As
•−

R,i −
i)T A∗
i/qi.
•

−
Moreover, ξi has norm bounded by O(k/n) for δ = O∗(1/ log n) and ζ is negligible.

We underline that the correct support of As allows us to obtain the closed-form expression of
gs
i. Likewise, the expression (8) suggests that gs
Ri,i in terms of As
i is almost equal to
i and A∗
•
•
•
i) (since λs
piqi(As
i. With Lemma 5, we will
1), which directs to the desired solution A∗
A∗
i ≈
i −
•
•
•
i and the nearness
prove the (α, β, γs)-correlation of the approximate gradient to each column A∗
•
of each new update to the true solution A∗.

14

4.1 (α, β, γs)-Correlation
Lemma 6. Suppose that As to be (δ, 2)-near to A∗ and R = supp(A∗
i), then 2gs
•
correlated with A∗R,i; that is

R,i is (α, 1/2α, ǫ2/α)-

A∗R,ii ≥
Futhermore, the descent is achieved by

R,i −

R,i, As

2gs
h

As
α
k

R,i −

A∗R,ik

2 + 1/(2α)

2
gs
R,ik
k

−

ǫ2/α

As+1
i −
k
•

2

(1

≤

−

2αη)s

A0
i −
k
•

A∗
ik
•

2 + ηǫ2

s/α

where δ = O∗(1/ log n) and ǫ = O

.

A∗
ik
•
k2
mn

(cid:0)

(cid:1)

Proof. Throughout the proof, we omit the superscript s for simplicity and denote 2α = piqi. First,
i as a combination of the true direction As
we rewrite gs
i −
•
•

i and a term with small norm:
A∗
•

gR,i = 2α(AR,i −

A∗R,i) + v,

i + ǫi] with norm bounded. In fact, since A
where v = 2α[(λi −
i, and both
i is δ-close to A∗
1)A
•
•
•
A
2α(λi −
have unit norm, then
A∗
A
O(k/n) from
i −
i −
ik
•
•
•
the inequality (9). Therefore,

1)A
ik
•

ξik ≤

= α
k

A∗
ik
•

α
k

and

≤

k

k

2

v
k

k

=

2α(λi −

1)AR,i + 2αξik ≤

k

α
k

AR,i −

A∗R,ik

+ ǫ

where ǫ = O(k2/mn). Now, we make use of (2) to show the ﬁrst part of Lemma 6:

2gR,i, AR,i −

h

A∗R,ii

= 4α
k

AR,i −

A∗R,ik

2v, AR,i −

h

.
A∗R,ii

2 +

We want to lower bound the inner product term with respect to
Eﬀectively, from (2)

2 and

gRi,ik

k

AR,i −
k

2.
A∗R,ik

4α
h

v, A
i −
•

A∗
ii
•

=

2

2

gR,ik
k
gR,ik

−

−

4α2
6α2

AR,i −
AR,i −

k

k

≥ k

2

v
k
− k
2ǫ2,

−

2

2

A∗R,ik
A∗R,ik
2(α2

2

where the last step is due to Cauchy-Schwarz inequality:

v
k
in (3) for the right hand side of (4), we get the ﬁrst result:

AR,i −
k

A∗R,ik

≤

k

2 + ǫ2).

Substitute 2
h

v, A
i −
•

A∗
ii
•

2gR,i, AR,i −
h

A∗R,ii ≥

α
k

AR,i −

A∗R,ik

2 +

1
2α k

2

gR,ik

−

ǫ2
α

.

(2)

(3)

(4)

The second part is directly followed from Theorem 1. Moreover, we have pi = Θ(k/m) and qi =
Θ(1), then α = Θ(k/m), β = Θ(m/k) and γs = O(k3/mn2). Then gs
correlated with the true solution A∗R,i.

R,i is (Ω(k/m), Ω(m/k), O(k3/mn2))-

Proof of Theorem 6. The descent in Theorem 6 directly follows from the above lemma. Next, we
will establish the nearness for the update at step s:

15

2,000
Sample size

4,000

2,000
Sample size

4,000

2,000
Sample size

4,000

e
t
a
r

y
r
e
v
o
c
e
R

e
t
a
r

y
r
e
v
o
c
e
R

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

r
o
r
r
e

n
o
i
t
c
u
r
t
s
n
o
c
e
R

r
o
r
r
e

n
o
i
t
c
u
r
t
s
n
o
c
e
R

8

6

4

2

0

8

6

4

2

0

0

0

Ours
Arora
Arora+HT
Trainlets

Ours
Arora
Arora+HT
Trainlets

e
m

i
t

g
n

i

n
n
u
R

e
m

i
t

g
n

i

n
n
u
R

4

3

2

1

0

6

4

2

0

0

0

2,000
Sample size

4,000

2,000
Sample size

4,000

2,000
Sample size

4,000

Figure 1: (top) The performance of four methods on three metrics (recovery rate, reconstruction error and
running time) in sample size in the noiseless case. (bottom) The same metrics are measured for
the noisy case.

4.2 Nearness

Lemma 7. Suppose that As is (δ, 2)-near to A∗, then

As+1

k

A∗k ≤

2
k

A∗k

−

Proof. From Lemma 5 we have gs
i = piqi(λiAs
i) + A
A∗
i −
•
•
•
idiag(qij)AT
R, then it is obvious that gs
iA∗
[n]
¯R,i = A ¯R,
i ±
\
•−
•
follows the proof of Lemma 24 in (Arora et al., 2015) for the nearness with full gs = gs
ﬁnish the proof for this lemma.

ζ. Denote ¯R =
idiag(qij)AT
iA∗
i ±
•−
•
•−
ζ is bounded by O(k2/m2). Then we
¯R,i to

R,i + gs

−

In sum, we have shown the descent property of Algorithm 2 in the inﬁnite sample case. The
gs around its mean to the sample complexity is provided in Section D.

study of the concentration of
In the next section, we corroborate our theory by some numerical results on synthetic data.

5. Empirical Study

b

We compare our method with three diﬀerent methods for both standard sparse and double-sparse
coding. For the standard approach, we implement the algorithm proposed in Arora et al. (2015),
which currently is the best theoretically sound method for provable sparse coding. However, since
their method does not explicitly leverage the double-sparsity model, we also implement a heuristic
modiﬁcation that performs a hard thresholding (HT)-based post-processing step in the initializa-
tion and learning procedures (which we dub Arora + HT ). The ﬁnal comparison is the Trainlets
approach of Sulam et al. (2016).

16

2 block is of form [1 1; 1

We generate a synthetic training dataset according to the model described in Section 2. The
base dictionary Φ is the identity matrix of size n = 64 and the square synthesis matrix A∗ is a
block diagonal matrix with 32 blocks. Each 2
1] (i.e., the column
×
sparsity r = 2) . The support of x∗ is drawn uniformly over all 6-dimensional subsets of [m],
In our simulations
and the nonzero coeﬃcients are randomly set to
with noise, we add Gaussian noise ε with entrywise variance σ2
ε = 0.01 to each of those above
samples. For all the approaches except Trainlets, we use T = 2000 iterations for the initialization
procedure, and set the number of steps in the descent stage to 25. Since Trainlets does not have
a speciﬁed initialization procedure, we initialize it with a random Gaussian matrix upon which
column-wise sparse thresholding is then performed. The learning step of Trainlets2 is executed for
50 iterations, which tolerates its initialization deﬁciency. For each Monte Carlo trial, we uniformly
draw p samples, feed these samples to the four diﬀerent algorithms, and observe their ability to
reconstruct A∗. Matlab implementation of our algorithms is available online3.

1 with equal probability.

−

±

We evaluate these approaches on three metrics as a function of the number of available samples:
(i) fraction of trials in which each algorithm successfully recovers the ground truth A∗; (ii) recon-
struction error; and (iii) running time. The synthesis matrix is said to be “successfully recovered”
if the Frobenius norm of the diﬀerence between the estimate
A and the ground truth A∗ is smaller
4 in the noiseless case, and to 0.5 in the other. All three
than a threshold which is set to 10−
metrics are averaged over 100 Monte Carlo simulations. As discussed above, the Frobenius norm is
only meaningful under a suitable permutation and sign ﬂip transformation linking
A and A∗. We
estimate this transformation using a simple maximum weight matching algorithm. Speciﬁcally, we
construct a weighted bipartite graph with nodes representing columns of A∗ and
A and adjacency
is taken element-wise. We compute the optimal matching
matrix deﬁned as G =
using the Hungarian algorithm, and then estimate the sign ﬂips by looking at the sign of the inner
products between the matched columns.

, where
A
|

T
A∗
|

|·|

b

b

b

b

The results of our experiments are shown in Figure 1 with the top and bottom rows respectively
for the noiseless and noisy cases. The two leftmost ﬁgures suggest that all algorithms exhibit a
“phase transitions” in sample complexity that occurs in the range of 500-2000 samples.
In the
noiseless case, our method achieves the phase transition with the fewest number of samples. In the
noisy case, our method nearly matches the best sample complexity performance (next to Trainlets,
which is a heuristic and computationally expensive). Our method achieves the best performance
in terms of (wall-clock) running time in all cases.

6. Conclusion

In this paper, we have addressed an open theoretical question on learning sparse dictionaries under
a special type of generative model. Our proposed algorithm consists of a novel initialization step
followed by a descent-style step, both are able to take advantage of the sparse structure. We rigor-
ously demonstrate its eﬃcacy in both sample- and computation-complexity over existing heuristics
as well as provable approaches for double-sparse and regular sparse coding. This results in the ﬁrst
known provable approach for double-sparse coding problem with statistical and algorithmic guaran-
tees. Besides, we also show three beneﬁts of our approach: neural plausibility, robustness to noise
and practical usefulness via the numerical experiments.

2. We utilize Trainlets’s implementation provided at http://jsulam.cswp.cs.technion.ac.il/home/software/.
3. https://github.com/thanh-isu/double-sparse-coding

17

Nevertheless, several fundamental questions regarding our approach remain. First, our initial-
ization method (in the overcomplete case) achieves its theoretical guarantees under fairly stringent
limitations on the sparsity level r. This arises due to our reweighted spectral initialization strategy,
and it is an open question whether a better initialization strategy exists (or whether these types
of initialization are required at all). Second, our analysis holds for complete (ﬁxed) bases Φ, and
it remains open to study the setting where Φ is over-complete. Finally, understanding the reasons
behind the very promising practical performance of methods based on heuristics, such as Trainlets,
on real-world data remains a very challenging open problem.

18

Appendix A. Auxiliary Lemma

Claim 1 (Maximal row ℓ1-norm). Given that
Θ(

m/n).

A∗k

k

p

Proof. Recall the deﬁnition of the operator norm:

2
F = m and

A∗k
k

= O(

m/n), then

T
A∗

k

k1,2 =

p

T
A∗
k

k1,2 = sup

=0

x

AT x
k
x

k

k

sup
=0
x

AT x
k
k
x
k1 ≤
k
A∗kF /√n =

k

=

T
A∗

k

k

= O(

m/n).

p

2
F = m,

T
A∗

k
m/n).

k1,2 ≥ k

Since
T
A∗
k

A∗k
k
k1,2 = Θ(
p
Along with Assumptions A1 and A3, the above claim implies the number of nonzero entries
in each row is O(r). This Claim is an important ingredient in our analysis of our initialization
algorithm shown in Section 3.

m/n. Combining with the above, we have

p

Appendix B. Analysis of Initialization Algorithm

B.1 Proof of Lemma 1

The proof of Lemma 1 can be divided into three steps: 1) we ﬁrst establish useful properties of β
with respect to α; 2) we then explicitly derive el in terms of the generative model parameters and β;
and 3) we ﬁnally bound the error terms in E based on the ﬁrst result and appropriate assumptions.

Claim 2. In the generative model,

x∗k ≤
k

O(√k) and

ε
k ≤

k

O(σε√n) with high probability.

Proof. The claim directly follows from the fact that x∗ is a k-sparse random vector whose nonzero
entries are independent sub-Gaussian with variance 1. Meanwhile, ε has n independent Gaussian
entries of variance σ2
ε .

e

e

Despite its simplicity, this claim will be used in many proofs throughout the paper. Note also
that in this section we will calculate the expectation over y and often refer probabilistic bounds
(w.h.p.) under the randomness of u and v.

Claim 3. Suppose that u = A∗α + εu is a random sample and U = supp(α). Let β = A∗
O(√k + σε√n).
w.h.p., we have (a)

√n + σε log n for each i and (b)

µk log n

βi −
|

αi| ≤

β
k

k ≤

T u, then,

Proof. The proof mostly follows from Claim 36 of Arora et al. (2015), with an additional consider-
and observe that
ation of the error εu. Write W = U

e

=

αi|

βi −
|

i εu| ≤ |h
A∗
•
T
k/n. Moreover, αW has k
µ
Since A∗ is µ-incoherence, then
i A∗
A∗
W k ≤
•
•
T
Gaussian entries of variance 1, therefore
i, αW i| ≤
W A∗
A∗
p
|h
•
•
that εu has independent Gaussian entries of variance σ2
variance (
k
µk log n

1 independent sub-
√n with high probability. Also recall
T
ε , then A∗
i εu is Gaussian with the same
•
αi| ≤

σε log n with high probability. Consequently,

= 1). Hence

i, αW i|

T
i ε
A∗
|
•

i, εui|

A∗
ik
•

βi −

µk log n

| ≤

−

+

|h

k

|

T
W A∗
A∗
•
•

√n + σε log n, which is the ﬁrst part of the claim.

Next, in order to bound

, we express β as

β
k
k
T A∗
U αU + A∗
•

β

k

k

=

A∗
k

T εuk ≤ k

A∗

A∗
U kk
•

αU k

kk

+

A∗
k

εuk

kk

i
\{
}
T
T
W αW + A∗
i A∗
A∗
|
•
•
•

19

O(√k) and

A∗k ≤

αU k ≤
k

O(1) , we complete the proof for the second part.

Using Claim 2 to get
εuk ≤
k
A∗
U k ≤ k
k
•
Claim 3 suggests that the diﬀerence between βi and αi is bounded above by O∗(1/ log2 n)
U and
o(1)
∈
O(√k) w.h.p.

w.h.p. if µ = O∗( √n
βi| ≤
|
We will use these results multiple times in the next few proofs.

O∗(1/ log2 n) otherwise. On the other hand, under Assumption B4,

O(σε√n) w.h.p., and further noticing that

). Therefore, w.h.p., C

O(log m) for i

βi| ≤ |

+ o(1)

k log3 n

αi|

β
k

k ≤

≤ |

≤

−

e

e

Proof of Lemma 1. We decompose dl into small parts so that the stochastic model

is made use.

y, v

el = E[
y, u
h
ih
= E
x∗, β
h
ih
= E1 + E2 +
(cid:2)(cid:8)

i
+ E9

· · ·

l ] = E[
y2
A∗x∗ + ε, u
A∗l
(
h
h
·
T (βvT + β′uT )ε + uT εεT v
+ x∗

A∗x∗ + ε, v

i
x∗, β′

ih

i

, x∗

+ ε)2]

i
, x∗

A∗l
h
•

2 + 2
h

A∗l
•

i

, x∗

εl + εl

i

(cid:9)(cid:8)

(cid:9)(cid:3)

where the terms are

e

D

i

ih
, x∗

2]
, x∗
i
εl]
, x∗

A∗l
•
A∗l
ih
•
ε2
l ]
i
T (βvT + β′uT )ε
T (βvT + β′uT )εεl
(cid:3)

x∗, β′
ih
x∗, β′

E1 = E[
x∗, β
ih
h
E2 = 2E[
x∗, β
h
ih
E3 = E[
x∗, β
x∗, β′
h
2x∗
E4 = E
A∗l
i
h
·
E5 = E
x∗
A∗l
(cid:2)
i
h
·
E6 = E
(βvT + β′uT )εε2
(cid:2)
l
2]
E7 = E[uT εεT v
A∗l
(cid:2)
(cid:3)
h
•
E8 = 2E[uT εεT v
εl]
A∗l
h
i
•
E9 = E[uT εεT vε2
l ]

i
, x∗

, x∗

, x∗

(cid:3)

(5)

Because x∗ and ε are independent and zero-mean, E2 and E4 are clearly zero. Moreover,

E6 = (βvT + β′uT )E[εε2

l ] = 0

due to the fact that E[εjε2

l ] = 0, for j

= l, and E[ε3

l ] = 0. Also,

We bound the remaining terms separately in the following claims.

T
E8 = A∗
l
•

E[x∗]E

uT εεT vεl

= 0.

(cid:2)

(cid:3)

Claim 4. In the decomposition (5), E1 is of the form

E1 =

2
li +
qiciβiβ′iA∗

2
li +
qiciβiβ′iA∗

2
lj + 2βiβ′jA∗liA∗lj)
qij(βiβ′iA∗

V
U
Xi
∩
∈

V
U
Xi /
∩
∈

=i
Xj

where all those terms except

li have magnitude at most O∗(k/m log2 n) w.h.p.
2
V qiciβiβ′iA∗
∩

i

U

∈

P

20

Proof. Using the generative model in Assumptions B1-B4, we have

E1 = E[
x∗, β
h
= ES
E

x∗

ih
S[

x∗, β′

A∗l
•

ih
βix∗i

, x∗

2]

i
β′ix∗i

A∗lix∗i

2]

|

(cid:2)

S
Xi
∈
2
li +
qiciβiβ′iA∗

S
Xi
∈

S

(cid:0) Xi

(cid:1)
∈
2
lj + 2βiβ′jA∗liA∗lj)
qij(βiβ′iA∗

(cid:3)

=

=

[m]
Xi
∈

[m],j
Xi,j
∈

=i

2
li +
qiciβiβ′iA∗

2
li +
qiciβiβ′iA∗

2
lj + 2βiβ′jA∗liA∗lj),
qij(βiβ′iA∗

Xi /
V
U
∩
∈
S], qij = P[i, j

V
U
Xi
∈
∩
where we have used the qi = P[i
S] and Assumptions
B1-B4. We now prove that the last three terms are upper bounded by O∗(k/m log n). The key
observation is that all these terms typically involve a quadratic form of the l-th row A∗l
whose
•
norm is bounded by O(1) (by Claim 1 and Assumption A4). Moreover,
is relatively small for
V )
(U
[m]
U
i /
\
∈
to bound

V while qij = Θ(k2/m2). For the second term, we apply the Claim 3 for i

Xj
=i
S] and ci = E[x4
i
i |

= 0, then with high probability

βiβ′i|
|

∈

∈

∈

∩

∩

∈

|

βiβ′i|

. Assume αi = 0 and α′i 6
βiβ′i| ≤ |
|
Using the bound qici = Θ(k/m), we have w.h.p.,

αi)(β′i −

(βi −

α′i)
|

+

βiα′i| ≤
|

O∗(1/ log n)

2
qiciβiβ′iA∗
li

max
i

qiciβiβ′i|
|

≤

2
A∗
li ≤

max
i

qiciβiβ′i|k
|

A∗

2
1,2 ≤

k

O∗(k/m log n).

V
U
Xi /
(cid:12)
∩
∈
(cid:12)
(cid:12)
For the third term, we make use of the bounds on
β′k ≤

V
U
Xi /
∩
∈

(cid:12)
(cid:12)
(cid:12)

β
k

kk

O(k) w.h.p., and on qij = Θ(k2/m2). More precisely, w.h.p.,

β
k

k

and

β′k
k

from the previous claim where

=i
Xj
(cid:12)
(cid:12)
(cid:12)

2
qijβiβ′iA∗
lj

e

=

βiβ′i

2
qijA∗
lj

(cid:12)
(cid:12)
(cid:12)

≤

Xi
(cid:12)
(cid:12)
(cid:12)
(max
=j
i

=i
Xj

Xi

qij)

βiβ′i|
|

|
Xi
2
A∗
lj

≤

(cid:12)
(cid:12)
(cid:12)
(cid:16)Xj

βiβ′i|

≤

(cid:17)

=i

(cid:0)Xj
(max
=j
i

2
qijA∗
lj

(cid:1)
β

qij)
k

β′

A∗

kk

kk

2
1,2 ≤

k

O(k3/m2),

e

where the second last inequality follows from the Cauchy-Schwarz inequality. For the last term, we
= j and
write it in a matrix form as
(Qβ)ij = 0 for i = j. Then

T
=i qijβiβ′jA∗liA∗lj = A∗
l
•

where (Qβ)ij = qijβiβ′j for i

QβA∗l
•

j

where
Qβk
k
mately,

2
F =

=j q2

i

(maxi

=j q2
ij)
k

β

2
k

β′k
k

≤

2. Ulti-

P
T
A∗
l
|
•
i (β′j )2
ijβ2

QβA∗l

•| ≤ k

(maxi

≤

A∗l

Qβkk
=j q2
ij)

2

•k

≤ k
i β2
i

QβkF k
j(β′j)2

A∗

2
1,2,

k

qijβiβ′jA∗liA∗lj

(max
=j
i

≤

P
β
qij)
k

kk

β′

A∗

P
kk

2
1,2 ≤
k

O(k3/m2).

P

=i
Xj
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Under Assumption k = O∗( √n
O∗(k/m log2 n). As a result, the two terms
above are bounded by the same amount O∗(k/m log n) w.h.p., so we complete the proof of the
claim.

log n ), then

O(k3/m2)

≤

e

e

21

Claim 5. In the decomposition (5),

E5|
|
S] = 1 and qi = P[i

,
E3|
|

,

Proof. Recall that E[x2
i |
E3 = E[
h

∈
l ] = σ2
ε2
ε
i

x∗, β

x∗, β′

ih

E7|
|

and

E9|

|

is at most O∗(k/m log2 n).

S] = Θ(k/m) for S = supp(x∗), then

ES

E

x∗

S[

|

βiβ′jx∗i x∗j ]

S
Xi,j
∈

(cid:3)

(cid:2)
σ2
ε qiβiβ′i

= σ2
ε

ES[

βiβ′i] =

S
Xi
∈

Xi

Denote Q = diag(q1, q2, . . . , qm), then
O(k3/mn) where we have used
β
k
handle the seventh term before E5:
e

k ≤

E7 = E[uT εεT v

, x∗

A∗l
•

h

2] = E[
h

A∗l
•

i

=

Q

σ2
σ2
ε k2/m) =
Qβ, β′i| ≤
E3|
ε k
ε h
|
|
O(√k) w.h.p. and σε ≤
O(1/√n). For convenience, we
e
, x∗

β′k ≤

O(σ2

u, v

u, v

kk

qiA2

QAl

kk

β

e

2]uT E[εεT ]v =
i

li = σ2
ε h

AT
l
•

i

σ2
ε h

i

•

Xi

O(√k) w.h.p. Consequently,

To bound this term, we use Claim 9 in Appendix D to have
2
u, v
h
•k
O(m/n) and σε ≤
e

σ2
ε k
O(1/√n). Now, the ﬁrth term E5 is expressed as follows

u
k
k
u, v
|h

E7| ≤
|

A∗α + εuk ≤
k
O(k2/mn) because
i| ≤

i ≤

Al

kk

Q

=

e

e

O(√k) w.h.p. and

Al

2
•k

k

≤

, x∗

x∗
i
T
x∗x∗

E5 = E
A∗l
h
·
E
T
= A∗
(cid:2)
l
•
T
= σ2
Q(vlβ + ulβ′)
ε A∗
(cid:3)
(cid:2)
l
•

T (βvT + β′uT )εεl
(βvT + β′uT )E[εεl]
(cid:3)

Observe that
β
2
k ≤
kk
k
bounded by

u

The last term

σ2
ε k

Q(vlβ + ulβ′)
k ≤
u
k ≤
k

T
A∗
E5| ≤
l
• kk
|
O(k) w.h.p. using the result
O(k2/mn).
e
e

e

T
σ2
Q
A∗
ε k
l
• kk
O(k) and

vlβ + ulβ′k
kk
β
k ≤
k

and that

vlβ + ulβ′k ≤
k
O(k) from Claim 3, then E5

E9 = E[uT εεT vε2

l ] = uT E

εεT ε2
l

v = 9σ4
ε h

because the independent entries of ε and E[ε4
Since m = O(n) and k
and

O(k2/n2).
E7|
,
E5|
|
|
e
E9|
|
Combining the bounds from Claim 4, 5 for every single term in (5), we ﬁnish the proof for
Lemma 1.

v
kk
log n ), we obtain the same bound O∗(k/m log2 n) for

, and conclude the proof of the claim.

k ≤
,
E3|
|

O∗( √n

9σ4
ε k

≤

u

|

(cid:2)
l ] = 9σ4

(cid:3)
ε . Therefore,

e

u, v

i
E9| ≤

B.2 Proof of Lemma 2

We prove this lemma by using the same strategy used to prove Lemma 1.

yRyT
R]
A∗x∗ + ε, v

i

y, v

Mu,v , E[
y, u
h
ih
= E[
A∗x∗ + ε, u
h
= E
h
= M1 +
(cid:2)(cid:8)

ih
x∗, β′
ih
+ M8,

x∗, β

· · ·

i

(A∗R
•

x∗ + εR)(A∗R
•

i

+ x∗

T (βvT + β′uT )ε + uT εεT v

x∗ + εR)T ]
A∗R
•

(cid:9)(cid:8)

22

x∗x∗

T
T A∗
R

+ A∗R
•

•

x∗εT

R + εRx∗

T
T A∗
R

+ εRεT
R

•

(cid:9)(cid:3)

in which only nontrivial terms are kept in place, including

T
T A∗
R

]

•

x∗εT
R]
T
T A∗
R

]

•

i

ih

ih

x∗x∗

x∗, β′

A∗R
•
εRεT
R]

M1 = E[
x∗, β
h
M2 = E[
x∗, β
x∗, β′
h
i
M3 = E[x∗
T (βvT + β′uT )εA∗R
•
M4 = E[x∗
T (βvT + β′uT )εεRx∗
M5 = E[uT εεT vA∗R
T
T A∗
]
x∗x∗
R
•
M6 = E[uT εεT vA∗R
x∗εT
R]
•
M7 = E[uT εεT vεT
T
T A∗
Rx∗
R
M8 = E[uT εεT vεRεT
R]

•

•

]

(6)

By swapping inner product terms and taking advantage of the independence, we can show that
M6 = E[A∗R
] = 0. The remaining are bounded in
the next claims.

R] = 0 and M7 = E[uT εεT vεT

x∗uT εεT vεT

T
T A∗
R

Rx∗

•

•

Claim 6. In the decomposition (6),

M1 =

T
R,i + E′1 + E′2 + E′3
qiciβiβ′iA∗R,iA∗

Xi
V
U
∩
∈
T
V qiciβiβ′iA∗R,iA∗
R,i, E′2 =
∩

where E′1 =
T
β′iA∗R,iβjA∗
R,j) have norms bounded by O∗(k/m log n).
P

P

i /
∈

U

i

T
=j qijβiβ′iA∗R,jA∗
R,j and E′3 =

T
=j qij(βiA∗R,iβ′jA∗
R,j+

i

P

Proof. The expression of M1 is obtained in the same way as E1 is derived in the proof of Lemma
1. To prove the claim, we bound all the terms with respect to the spectral norm of A∗R
and make
•
use of Assumption A4 to ﬁnd the exact upper bound.
T
(U
R,S where S = [m]
For the ﬁrst term E′1, rewrite E′1 = A∗R,SD1A∗
\
qiciβiβ′i| ≤
D1k ≤
S|
∈

V ) and D1 is a diagonal
∩
O∗(k/m log n) as shown in

matrix whose entries are qiciβiβ′i. Clearly,
Claim 4, then

maxi

k

E′1k ≤

k

max
S |
i
∈

qiciβiβ′i|k

A∗R,Sk

2

≤

max
S |
i
∈

qiciβiβ′i|k

A∗R

2
•k

≤

O∗(k/m log n)

O(1). The second term E′2 is a sum of positive semideﬁnite matrices,

where
A∗R,Sk ≤ k
k
β
and
k ≤
k

A∗R
•k ≤
O(k log n), then

E′2 =

T
qijβiβ′iA∗R,jA∗
R,j (cid:22)

max
=j
i

qij

βiβ′i

=j
Xi

(cid:17)(cid:16)Xj
2
which implies that
A∗R
•k
T
form as the last term in Claim 4, which is E′3 = A∗
R
•

=j qij)
k

E′2k ≤

β′kk

(cid:16)Xi

(maxi

kk

β

k

T
A∗R,jA∗
R,j

(max
=j
i

β

qij)
k

β′

A∗R
k
•

kk

(cid:22)

T
A∗
R

•

O(k3/m2). Observe that E′3 has the same

E′3k ≤ k
k

Qβkk

A∗R

2
•k

(max
=j
i

qij)
k

≤

2

A∗R

•k

≤

O(k3/m2)

(cid:17)

. Then

≤
QβA∗R
•
e
β′

β

kk

kk

By Claim 3, we have

β

k

k

and

β′k

k

then we complete the proof for Lemma 6.

are bounded by O(√k log n), and note that k

O∗(√n/ log n),

e

≤

23

Claim 7. In the decomposition (6), M2, M3, M4, M5 and M8 have norms bounded by O∗(k/m log n).

Proof. Recall the deﬁnition of Q in Claim 5 and use the fact that E[x∗x∗
E[
x∗, β
ε qiβiβ′iIr. Then,
R] =
h
The next three terms all involve A∗R
•

σ2
ε maxi qik

x∗, β′i
ih

β′k ≤
whose norm is bounded according to Assumption A4.

T ] = Q, we can get M2 =
ε k2 log2 n/m).

M2k ≤

O(σ2

εRεT

i σ2

P

kk

β

k

Speciﬁcally,

M3 = E[x∗
= A∗R
•
= A∗R
•

x∗εT

R] = E[A∗R
•
T ](βvT + β′uT )E[εεT
R]

T (βvT + β′uT )εA∗R
•
E[x∗x∗
Q(βvT + β′uT )E[εεT

R],

x∗x∗

T (βvT + β′uT )εεT
R]

and

M4 = E[x∗

T (βvT + β′uT )εεRx∗

] = E[εRεT (vβT + uβ′

T )x∗x∗

T
T A∗
R

]

•

= E[εRεT ](vβT + uβ′
= E[εRεT ](vβT + uβ′

T A∗
T
R
•
T )E[x∗x∗
T
T ]A∗
R
T
T )QA∗
,
R

•

and the ﬁfth term M5 = E[uT εεT vA∗R
E[εεT
We already have
R]
k
k
remaining work is to bound
A∗uvT
βvT
A∗kk
k
k
k
O(σ2
in norm by
≤
The remaining term is

•
E[x∗x∗
T A∗
T
ε uT vA∗R
= σ2
.
R
•
•
•
uT v
Q
O(k) (proof of Claim 9), then the
O(k/m) and
k ≤
k
|
T directly follows. We have
, then the bound of vβT + uβ′
βvT + β′uT
k
k
v
u
O(k). Therefore, all three terms M3, M4 and M5 are bounded
kk
O(k3/mn).
e

ε uT vA∗R
•

k ≤ k
ε k2/m)

T
T ]A∗
R

] = σ2

= σ2
ε ,

T
QA∗
R

x∗x∗

k ≤

| ≤

=

e

•

•

e

e

M8 = E[uT εεT vεRεT

R] = E[

uivjεiεj

εRεT
R]

= E[

uiviε2

i εRεT
R

(cid:0)Xi
R
∈
ε uRvT
R

= σ4

(cid:0)Xi,j
] + E[

(cid:1)
uivjεiεj

εRεT
R]

(cid:1)

=j

(cid:0)Xi

(cid:1)

where uR = A∗R
•
O(√k). Therefore,
bound all the above terms by O∗(k/m log n) and ﬁnish the proof of Claim 7.
e

α + (εu)R and vR = A∗R
α′ + (εv)R. We can see that
•
O(k3/n2). Since m = O(n) and k
ε k) =

e
Combine the results of Claim 6 and 7, we complete the proof of Lemma 2.

uRk ≤ k
k
≤

M 8
k

O(σ4

k ≤

e

+

A∗R
(εu)Rk ≤
•kk
k
O∗( √n
log n ), then we can

α
k

Appendix C. Analysis of Main Algorithm

C.1 Simple Encoding

y)sgn(x)T is random over y and x that is obtained from the encoding step.
We can see that (Asx
We follow (Arora et al., 2015) to derive the closed form of gs = E[(Asx
y)sgn(x)T ] by proving
that the encoding recovers the sign of x∗ with high probability as long as As is close enough to A∗.

−

−

Lemma 8. Assume that As is δ-close to A∗ for δ = O(r/n log n) and µ
then with probability over random samples y = A∗x∗ + ε

≤

√n
2k , and k

≥

Ω(log m)

sgn(thresholdC/2

(As)T y

= sgn(x∗)

(7)

(cid:0)
24

(cid:1)

Proof of Lemma 8. We follow the same proof strategy from (Arora et al., 2015) (Lemmas 16 and
17) to prove a more general version in which the noise ε is taken into account. Write S = supp(x∗)
and skip the superscript s on As for the readability. What we need is to show S =
[m] :
S with high probability. Following
) = sgn(x∗i ) for each i
i, y
A
h
i
•
the same argument of (Arora et al., 2015), we prove in below a stronger statement that, even
i, y
A
conditioned on the support S, S =
h
•

and then sgn(
h

with high probability.

As
i, y
•

C/2
}

C/2
}

[m] :

i ≥

i ≥

i
{

i
{

∈

∈

∈

Rewrite

i, y
A
h
•

i

=

i, A∗x∗ + ε
A
i
h
•

=

i, A∗
A
ii
•
•

h

x∗i +

i, A∗
A
j i
•
•

x∗j +

i, ε
A
i
h
•

,

h
Xj
=i

and observe that, due to the closeness of A
i, the ﬁrst term is either close to x∗i or equal to
i and A∗
•
•
S. Meanwhile, the rest are small due to the incoherence and the
0 depending on whether or not i
concentration in the weighted average of noise. We will show that both Zi =
x∗j
and

i, A∗
A
ji
•
•

i
\{

}h

∈

S

i, ε
A
h
i
•
The cross-term Zi =

are bounded by C/8 with high probability.
i, A∗
A
j i
•
•
dom variables, which is another sub-Gaussian random variable with variance σ2
Note that

i
\{

P

}h

S

x∗j is a sum of zero-mean independent sub-Gaussian ran-

S

i
\{

}h

i, A∗
A
j i
•
•

2.

Zi =

P

i, A∗
A
ji
h
•
•

A
i −
h
•
where we use Cauchy-Schwarz inequality and the µ-incoherence of A∗. Therefore,

i, A∗
A∗
j i
•
•

A
i −
•

2µ2/n + 2
h

2
i, A∗
A∗
ji
•
•

2 +

≤

≤

2

(cid:0)

(cid:1)

h

2

P
i, A∗
A∗
ji
•
•

2,

σ2
Zi ≤

T
2µ2k/n + 2
S (A
A∗
i −
k
•
•
√n
2k , to conclude 2µ2k/n

≤

under µ
Applying Bernstein’s inequality, we get
. In fact,
bound the noise term
a sub-Gaussian with variance σ2
Notice that σε = O(1/√n).

i, ε
A
i
h
•

≤

i)
A∗
k
•

2
F ≤

2µ2k/n + 2
k

A∗
Sk
•

2

A
i −
•

k

2
A∗
ik
•

≤

O(1/ log n),

O(1/ log n) we need 1/k = O(1/ log n), i.e. k = Ω(log n).
C/8 with high probability. What remains is to
is sum of n Gaussian random variables, which is
σε log n with high probability.

Zi| ≤
|
i, ε
A
i
h
•
ε . It is easy to see that

i, ε
A
•

|h

i| ≤

Finally, we combine these bounds to have
i, y
A
•

S, then
i, ε
A
h
•
C/2 and negligible otherwise. Using union bound for every i = 1, 2, . . . , m, we ﬁnish

C/4. Therefore, for i

Zi +
|

i| ≤

∈

|h
the proof of the Lemma.

i| ≥

Lemma 8 enables us to derive the expected update direction gs = E[(Asx

y)sgn(x)T ] explicitly.

−

C.2 Approximate Gradient in Expectation

−

Proof of Lemma 5. Having the result from Lemma 8, we are now able to study the expected update
y)sgn(x)T ]. Recall that As is the update at the s-th iteration and x ,
direction gs = E[(Asx
thresholdC/2((As)T y). Based on the generative model, denote pi = E[x∗i sgn(x∗i )
S]
i
|
and qij = P[i, j
S]. Throughout this section, we will use ζ to denote any vector whose norm is
negligible although they can be diﬀerent across their appearances. A
i denotes the sub-matrix of
A whose i-th column is removed. To avoid overwhelming appearance of the superscript s, we skip
it from As for neatness. Denote
Fx∗ is the event under which the support of x is the same as that
of x∗, and ¯
Fx∗ = 1[sgn(x) = sgn(x∗)] and 1
Fx∗ = 1.

Fx∗ is its complement. In other words, 1

S], qi = P[i

Fx∗ + 1 ¯

∈

∈

∈

−

i = E[(Ax
gs
•

−

y)sgn(xi)] = E[(Ax

y)sgn(xi)1

−

Fx∗ ]

±

ζ

25

Fx∗ we have Ax = A

S AT
Using the fact that y = A∗x∗ + ε and that under
Sy =
SxS = A
•
•
•
SAT
SAT
A
SA∗x∗ + A
Sε. Using the independence of ε and x∗ to get rid of the noise term, we get
•
•
•
•
i = E[(A
S AT
gs
S −
•
•
•
= E[(A
S AT
S −
•
•
= E[(A
S AT
S −
•
•
= E[(A
S AT
S −
•
•

S AT
S −
•
•
ζ
(Independence of ε and x’s)
±
1 ¯
Fx∗ )]

In)A∗x∗sgn(xi)1
In)A∗x∗sgn(x∗i )(1
In)A∗x∗sgn(x∗i )]

Fx∗ ] + E[(A
Fx∗ ]
−
ζ

Fx∗ event)

In)εsgn(xi)1

In)A∗x∗1

(Under

Fx∗ ]

±

±

±

ζ

ζ

Recall from the generative model assumptions that S = supp(x∗) is random and the entries of x∗
are pairwise independent given the support, so

i = ESE
gs
x∗
•
= piES,i
= piES,i

|

S AT
S[(A
S −
•
•
S AT
S[(A
S −
•
∈
•
iAT
S[(A
i −
•
∈
•

In)A∗x∗sgn(x∗i )]
i]
In)A∗
ζ
±
•
i] + piES,i
In)A∗
•

S[
∈

ζ

±

iAT
= piqi(A
i −
•
•

i + pi
In)A∗
•

= piqi(λiA
i −
•

i) + piA
A∗
•

•−

Xl
=i
S,l
∈
lAT
lA∗
qilA
i ±
•
•
•

ζ

[m],l
Xl
=i
∈
idiag(qij)AT
•−

iA∗
i ±
•

ζ

lAT
i]
lA∗
A
•
•
•

±

ζ

where λs
expression of the expected approximate gradient at iteration s:

idiag(qij)AT
•−

As
i, A∗
ii
h
•
•

i = AR,

. Let ξs

i =

−

iA∗
i/qi for j = 1, . . . , m, we now have the full
•

(8)

(9)

R,i = piqi(λiAs
gs

A∗R,i + ξs
i )

R,i −

ζR.

±

What remains is to bound norms of ξs and ζ. We have
ξs
A∗i k
along with the fact that
i k
k
qij
ξs
i k ≤ k
qi k
k

= 1, we can bound

max
=i
j

As

ik

Ri,

k

−

As
−

ik ≤

O(k/n).

As
R,

k

ik ≤ k

−

As
−

ik ≤

O(

m/n) w.h.p. Then,

p

Next, we show that norm of ζ is negligible. In fact,
it suﬃces to bound norm of (Ax
Section D. This concludes the proof for Lemma 5.

−

Fx∗ happens with very high probability, then
y)sgn(xi) which will be done using Lemma 12 and Lemma 11 in

Appendix D. Sample Complexity

In previous sections, we rigorously analyzed both initialization and learning algorithms as if the
expectations gs, e and Mu,v were given. Here we show that corresponding estimates based on
empirical means are suﬃcient for the algorithms to succeed, and identify how may samples are
required. Technically, this requires the study of their concentrations around their expectations.
Having had these concentrations, we are ready to prove Theorems 4 and 5.

The entire section involves a variety of concentration bounds. Here we make heavy use of
Bernstein’s inequality for diﬀerent types of random variables (including scalar, vector and matrix).
The Bernstein’s inequality is stated as follows.

26

Lemma 9 (Bernstein’s Inequality). Suppose that Z (1), Z (2), . . . , Z (p) are p i.i.d. samples from some
almost surely and
distribution

σ2 for each j, then

. If E[Z] = 0,

E[Z (j)(Z (j))T

Z (j)

k ≤

D

k

k ≤ R
p

(10)

k
σ2
p

(cid:19)

+

R
p

s

1
p

Z (j)

≤

O

e

(cid:18)

(cid:13)
(cid:13)
(cid:13)

Xj=1
(cid:13)
(cid:13)
(cid:13)

holds with probability 1

n−

ω(1).

−

Since all random variables (or their norms) are not bounded almost surely in our model setting,

(log(1/ρ))C ]

≤

we make use of a technical lemma that is used in Arora et al. (2015) to handle the issue.
Lemma 10 (Arora et al. (2015)). Suppose a random variable Z satisﬁes P[
k
ρ for some constant C > 0, then
(a) If p = nO(1), it holds that
(b)
e
Ω(

) for each j with probability 1

k
ω(1).

k ≥ R

= n−

ω(1).

Z (j)

k ≤

n−

O(

R

−

Z

Z

E[Z1
k
k

k≥

)]
k
R

e

p
This lemma suggests that if 1
i=1 Z (j)(1
)) concentrates around its mean with
p
p
high probability, then so does 1
i=1 Z (j) because the part outside the truncation level can be
P
p
ignored. Since all random variables of our interest are sub-Gaussian or a product of sub-Gaussian
that satisfy this lemma, we can apply Lemma 9 to the corresponding truncated random variables
with carefully chosen truncation levels. Then the original random variables concentrate likewise.

1
k

Z (j)

P

e
Ω(

−

k≥

R

In the next proofs, we deﬁne suitable random variables and identify good bounds of

and
σ2 for them. Note that in this section, the expectations are taken over y by conditioning on u
and v. This aligns with the construction that the estimators of e and Mu,v are empirical averages
over i.i.d. samples of y, while u and v are kept ﬁxed. Due to the dependency on u and v, these
(conditional) expectations inherit randomness from u and v, and we will formulate probabilistic
bounds for them.

R

The application of Bernstein’s inequality requires a bound on

eΩ(
k≥
achieve that by the following technical lemma, where ˜Z is a standardized version of Z.
Lemma 11. Suppose a random variable ˜Z ˜Z T = aT where a
They are both random. Suppose P[a

> 0 is a constant. Then,

ω(1) and

1
k

−

≥

Z

] = n−

E[ZZ T (1
k

0 and T is positive semi-deﬁnite.

. We

))]
k

R

E[ ˜Z ˜Z T (1

k

≥ A
1
˜Z
k

−

)]

k≥B

k ≤ Ak

B
E[T ]
k

+ O(n−

ω(1))

Proof. To show this, we make use of the decomposition ˜Z ˜Z T = aT and a truncation for a. Specif-
ically,

E[ ˜Z ˜Z T (1
k

1
k

˜Z

−

)]
k

k≥B

T (1

1
˜Z
k

−

k≥B

)]
k

≥A

˜Z

)]

k≥B

)]
k
T

≥Ak

1
˜Z
k

k≥B

k≥B
)T (1

≥A

1
˜Z
−
k
+ E[a1a
)T ]
k
≥A
2(1
E[
aT
k
k
4(1
˜Z
E[
k
k
P[a

−
1
˜Z
k
1/2

−
]

2

k≥B

+

E[a1a
k
1
(1
˜Z
k
−
k
)]E[1a
)]P[a

≥A
]

]

≥ A

(cid:1)

)]

k≥B
1/2

1/2
(cid:1)

= E[aT (1
E[a(1
E[a(1

≤ k

≤ k

−

−

1
k
1a

1a

−
E[T ]
k
E[T ]
k
E[T ]
k
E[T ]
k

≤ Ak

≤ Ak

≤ Ak

≤ Ak

+

+

(cid:0)

+

(cid:0)
B
+ O(n−

(cid:0)

≥ A
ω(1)),

(cid:1)

27

where at the third step we used T (1
semi-deﬁnite and 1

1
k

˜Z

−

k≥B ∈ {

0, 1
}

1
k

˜Z

)]

T because of the fact that T is the positive

−
. Then, we ﬁnish the proof of the lemma.

k≥B

(cid:22)

D.1 Sample Complexity of Algorithm 1

e and the reduced weighted covariance matrix
Mu,v depends upon
b
R
u,v. We will show that we only need
e, we denote it by
c
O(m) samples to be able to recover the support of one particular atom and up to some speciﬁed

In Algorithm 1, we empirically compute the “scores”
Mu,v to produce an estimate for each column of A∗. Since the construction of
the support estimate
c
p =
level of column-wise error with high probability.

R given by ranking

c

M

b

b

b

e

Lemma 12. Consider Algorithm 1 in which p is the given number of samples. For any pair u and
v, then with high probability a)
O∗(k/m log n) when p =
sets of one particular atom.
e

u,vk ≤
R and R are respectively the estimated and correct support
e

e
e
k ≤
k
Ω(mr) where

O∗(k/m log2 n) when p =

Ω(m) and b)

b
R
u,v −

M
k

M R

c

−

b

b

D.1.1 Proof of Theorem 4

Using Lemma 12, we are ready to prove the Thereom 4. According to Lemma 1 when U
we can write

e as

∩

V =

,
i
}
{

b

e = qiciβiβ′iA∗R,i ◦

e
A∗R,i + perturbation terms + (

e),

−

e as an additional perturbation with the same magnitude O∗(k/m log2 n) in the
and consider
e
−
w.h.p. The ﬁrst part of Lemma 3 suggests that when u and v share exactly one atom
sense of
k · k∞
e is the same as supp(A∗i ) with high probability.
R including r largest elements of
i, then the set
b

b

b

Once we have
b

R, we again write

M

b
R
u,v using Lemma 2 as
b

b
b
c
T
R
R,i + perturbation terms + (
u,v = qiciβiβ′iA∗R,iA∗
M

M

b
R
u,v −

M R

u,v),

b
c
R
u,v −

M

and consider
in the sense of the spectral norm
singular vectors of

c

M

M R

c
u,v as an additional perturbation with the same magnitude O∗(k/m log n)
w.h.p. Using the second part of Lemma 3, we have the top

b
R
u,v is O∗(1/ log n) -close to A∗R,i with high probability.

k · k

Since every vector added to the list L in Algorithm 1 is close to one of the dictionary, then
A0 must be δ-close to A∗. In addition, the nearness ofA0 to A∗ is guaranteed via an appropriate
A close to A0 and
projection onto the convex set
. Finally, we ﬁnish the
A
|
{
proof of Theorem 4.

A∗k}

A
k

2
k

k ≤

c

=

B

D.1.2 Proof of Lemma 12, Part a

y, v

[n], consider p i.i.d. realizations Z (1), Z (2), . . . , Z (p) of the random variable
For some ﬁxed l
∈
Z ,
O∗(k/m log2 n)
y2
l , then
y, u
e
h
k∞ ≤
holds with high probability, we ﬁrst study the concentration for the l-th entry of
e and then
e
and its variance E[Z 2]
take the union bound over all l = 1, 2, . . . , n. We derive upper bounds for
b
|
in order to apply Bernstein’s inequality in (12) to the truncated version of Z.

i=1 Z (i) and el = E[Z]. To show that

el = 1
p

P

ih

−

−

Z

b

k

e

i

p

|

b

O(k) and E[Z 2]

O(k2/m) with high probability.

Claim 8.

Z
|

| ≤

e

≤

e

28

Again, the expectation is taken over y by conditioning on u and v, and therefore is still random
due to the randomness of u and v. To show Claim 8, we begin with proving the following auxiliary
claim.

Claim 9.

y

O(√k) and

y, u

O(√k) with high probability.

k

k ≤

|h

i| ≤

Proof. From the generative model, we have

e

e

y
k

k

=

Sx∗S + ε
A∗
k
•

where S = supp(x∗). From Claim 2,
overcomplete and has bounded spectral norm, then
O(1). Therefore,
w.h.p., which is the ﬁrst part of the proof. To bound the second term, we write it as

k
A∗
Sk ≤ k
•

e

k

k

,

+

ε
k
k

x∗Sk

A∗
S kk
•
O(σε√n) w.h.p. In addition, A∗ is
O(√k)

ε
k ≤ k
ε
k ≤
A∗k ≤
e

y
k

k ≤

+

Sx∗Sk
A∗
•
O(√k) and

k

k ≤ k
x∗Sk ≤

=

y, u

Sx∗S + ε, u
A∗
•

T
S u
x∗S, A∗
•
T
S u
Similar to y, we have
A∗
k
•
probability. Since u and x∗ are independent sub-Gaussian and
T
e
variance at most O(√k),
S u
x∗S, A∗
•
y, u
quently,

i|
i| ≤ |h
O(√k) w.h.p. and hence

O(k) w.h.p. Similarly,

i| ≤

O(√k) w.h.p., and we conclude the proof of the claim.

.
i|
T
O(√k) with high
u
A∗
S kk
k ≤ k
•
T
x∗S, A∗
are sub-exponential with
S u
h
i
•
O(√k) w.h.p. Conse-
ε, u
i| ≤

k ≤

k ≤

|h
u

ε, u

+

|h

|h

|h

|h

i|

k

e

e

|h

y, v

e
y2
l =

i| ≤
, x∗i
A∗l
(
Proof of Claim 8. We have Z =
y, u
h
i
h
•
w.h.p. according to Claim 9. What remains is to bound y2
A∗l
l = (
h
2
is sub-Gaussian with variance ES(
•
1,2 = O(1), then
k
+εl| ≤
A∗l
εl| ≤
Similarly for εl,
|h
|
•
with high probability the bound
2y2
2y2
y, u
l h
i

T
2
A∗
li )
S A∗
i
∈
O(σε log n) w.h.p. Ultimately,

To bound the variance term, we write Z 2 =

P
Z
|

y, u
h

, x∗i

O(k).

≤ k

y, v

| ≤

ih

ih

i

2y2
i

y, v
we get
to both terms, then
e

l ≤

h

O(k) and

Z
|

| ≤

E[Z 2(1

e
Z

|≥

1
|

−

Ω(k))]
e

O(k)E[
h

y, u
i

≤

2y2

l ] + O(n−

ω(1)),

e
+ εl)2 with
O(k)
i ≤
, x∗i
A∗l
, x∗i
h
•
e
O(log n) w.h.p.
|h
O(log n), and hence we obtain

y, v
y, u
h
+ εl)2. Because
, x∗i| ≤
A∗l
•

ih

l . Note that, from the ﬁrst part,
O(k) w.h.p.. We apply Lemma 11 with some appropriate scaling

y, v

e

h

i

where E[
e
y, u
i
h
Section “Analysis of Initialization Algorithm”,

2y2

l ] is equal to el for pair u, v with v = u. From Lemma 1 and its proof in Appendix

E[
y, u
i
h

2y2

l ] =

qiciβ2

2
li + perturbation terms,
i A∗

m

Xi=1

in which the perturbation terms are bounded by O∗(k/m log2 n) w.h.p. (following Claims 4 and 5).
(max qiciβ2
2
O(log m)
A∗l
i )
i A∗
The dominant term
li ≤
≤
k
(Claim 3). Then we complete the proof of the second part.

O(k/m) w.h.p. because

βi| ≤
|

i qiciβ2

2
•k

P

e

Proof of Lemma 12, Part a. We are now ready to prove Part a of Lemma 12. We apply Bernstein’s
inequality in Lemma 9 for the truncated random variable Z (i)(1
O(k) and
variance σ2 =

O(k2/m) from Claim 8, then

)) with

1
Z (i)
|

e
Ω(

R

−

=

|≥

R

Z (i)(1

e
−

1
|

Z (i)

e
Ω(

))

|≥

R

−

E[Z(1

1
Z
|

−

e
Ω(

|≥

R

p

Xi=1

1
p

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

O(k)
p

e

O(k2/m)
p

+

s

≤

e

))]
(cid:13)
(cid:13)
(cid:13)
(cid:13)

29

e

O∗(k/m log n), (11)

Ω(m). Then

el = 1
w.h.p. for p =
p
bound over l = 1, 2, . . . , n, we get
the proof of 12, Part a.

e

b

p
i=1 Z (i) also concentrates with high probability. Take the union
O∗(k/m log n) with high probability and complete
e

k
P

−

e
k∞ ≤

D.1.3 Proof of Lemma 12, Part b

b

b
R
u,v −

M R

M

Next, we will prove that
prove the concentration inequalities for the case when conditioned on the event that
c
to R w.h.p. Again, what we need to derive are an upper norm bound
variable Z ,
yRyT
R and its variance.

O∗(k/m log n) with high probability. We only need to
R is equivalent
of the matrix random

u,vk ≤

y, v

R

b

k

y, u
h

ih

i

Claim 10.

Z

O(kr) and

k

k ≤

E[ZZ T ]
k

k ≤

O(k2r/m) hold with high probability.

k

Z

Proof. We have
k
2 =
yRk
whereas
This implies
Z
P
k ≤
k
We take advantage of the bounds of
and

l ≤

e

2

2

i|k

y, u

yRk

e
2 with
y, v
ih
i| ≤
O(r log2 n) w.h.p. because yl ≤

e
O(k) w.h.p. (according to Claim 9)
k ≤ |h
R y2
O(log n) w.h.p. (proof of Claim 8).
i
∈
e
O(kr) w.h.p. The second part is handled similarly as in the proof of Claim 8.
O(kr)
Mu,v in Lemma 2. Speciﬁcally, using the ﬁrst part

y, u

y, v

ih

|h

O(kr), and applying Lemma 11, then

Z
k

k ≤

i

y, v
h

yRk
k
E[ZZ T (1
k

≤

e
1
Z
k

−

k≥

Ω(kr))]
e

k ≤

E[
y, u
i
h

2yRyT
R]
k

+

O(kr)O(n−

ω(1))

O(kr)
k

,
Mu,uk

≤

e

where Mu,u arises from the application of Lemma 2. Recall that

e

e

e

Mu,u =

qiciβ2

T
R,i + perturbation terms,
i A∗R,iA∗

c
O(kr)
k

Xi

where the perturbation terms are all bounded by O∗(k/m log n) w.h.p. by Claims 6 and 7.
addition,

In

qiciβ2

T
i A∗R,iA∗
R,ik ≤

(max
i

qiciβ2
i )
k

A∗R

2
•k

O(k/m)
k

A∗

≤

k

≤

2

O(k/m)

k
Xi

w.h.p. Finally, the variance bound is

O(k2r/m) w.h.p.
Then, applying Bernstein’s inequality in Lemma 9 to the truncated version of Z with

and variance σ2 =

O(k2r/m) and obtain the concentration for the full Z to get

e

e

e

=

O(kr)

R

e

e

M R
k

u,v −

M R

u,vk ≤

c

O(kr)
p

O(k2r/m)
p

+

s

≤

e

e

O∗(k/m log n)

w.h.p. when the number of samples is p =

We have proved that
k
b
R
u,v −

M

k

M R

M R
u,v −
M R
u,vk ≤
c

Ω(mr) under Assumption A4.1.
O∗(k/m log n) as conditioned on the support consistency
u,vk ≤
e
O∗(k/m log n) is easily followed by the law of total probability

event holds w.h.p.
through the tail bounds on the conditional and marginal probabilities (i.e. P[
k
c
R = R]) and P[
O∗(k/m log n)
|
the spectral bounds.

u,vk ≤
= R]. We ﬁnish the proof of Lemma 12, Part b for both cases of

u,v −

M R

M R

c

R

b

b

30

D.2 Proof of Theorem 5 and Sample Complexity of Algorithm 2

In this section, we prove Theorem 5 and identify sample complexity per iteration of Algorithm 2. We
divide the proof into two steps: 1) show that when As is (δs, 2)-near to A∗ for δs = O∗(1/ log n), the
O(k2/mn)+αo(δ2
s )
approximate gradient estimate
, and 2) show that the nearness is preserved at each iteration. These correspond to showing the
following lemmas:

gs is (α, β, γs)-correlated-whp with A∗ with γs ≤

b

Lemma 13. At iteration s of Algorithm 2, suppose that As has each column correctly supported
and is (δs, 2)-near to A∗ and that η = O(m/k). Denote R = supp(As
gs
R,i is
i), then the update
•
O(k2/mn) + αo(δ2
(α, β, γs)-correlated-whp with A∗R,i where α = Ω(k/m), β = Ω(m/k) and γs ≤
s )
for δs = O∗(1/ log n).

b

Note that this is a ﬁnite-sample version of Lemma 6.

Lemma 14. If As is (δs, 2)-near to A∗ and number of samples used in step s is p =
.
with high probability

Proof of Theorem 5. The correlation of
column-wise error according to Theorem 1. Along with Lemma 14, the theorem follows directly.

A∗k
2
k
gi with A∗i , described in Lemma 13, implies the descent of

e

As+1
k

A∗k ≤

−

Ω(m), then

D.2.1 Proof of Lemma 13

b

We prove Lemma 13 by obtaining a tail bound on the diﬀerence between
Bernstein’s inequality in Lemma 9.

R,i and gs
gs

R,i using the

Lemma 15. At iteration s of Algorithm 2, suppose that As has each column correctly supported and
is (δs, 2)-near to A∗. For R = supp(As
(o(δs) + O(ǫs))
i ) = supp(A∗i ), then
·
mnr
k ).
with high probability for δs = O∗(1/ log n) and ǫs = O(

gs
R,i −
k/n) when p =

O(k/m)
Ω(m + σ2
ε

gs
R,ik ≤

p
To prove this lemma, we study the concentration of

b
gs
R,i, which is a sum of random vector
e
of the form (y
S, with
i
Ax)Rsgn(xi)
|
S = supp(x∗) and x = thresholdC/2(AT y). Then, using the following technical lemma to bridge
the gap in concentration of the two variables. We adopt this strategy from Arora et al. (2015) for
our purpose.

Ax)Rsgn(xi). We consider random variable Z , (y

−

−

∈

b

k

b

Claim 11. Suppose that Z (1), Z (2), . . . , Z (N ) are i.i.d. samples of the random variable Z = (y
i
Ax)Rsgn(xi)
|

S. Then,

∈

−

o(δs) + O(ǫs)

(12)

(cid:13)
(cid:13)
(cid:13)
holds with probability when N =

Xj=1
Ω(k + σ2

N

1
N

Z (j)

−

≤

E[Z]
(cid:13)
(cid:13)
(cid:13)

ε nr), δs = O∗(1/ log n) and ǫs = O(

k/n).

Proof of Lemma 15. Once we have done the proof of Claim 11, we can easily prove Lemma 15. We
recycle the proof of Lemma 43 in Arora et al. (2015).
supp(x∗

Write W =

and N =

gR,i as

e

j : i
{

∈

(j))
}

W
|

, then express
|

p

gR,i =

N
p

1
N

(y(j)

−

Ax(j))Rsgn(x(j)

b
i ),

b

Xj

31

j(y(j)

1
where
W
|
E[(y
Ax)Rsgn(xi)] = E[(y
Following Claim 11, we have

P

−

−

|

−

Ax(j))Rsgn(x(j)

i ) is distributed as 1
N

Ax)Rsgn(xi)1i

S ] = E[Z]P[i
P
∈

∈

N
j=1 Z (j) with N =

. Note that
W
|
|
S] = qiE[Z] with qi = Θ(k/m).

gs
R,i −

gs
R,ik ≤

k

O(k/m)

O(k/m)

(o(δs) + O(ǫs)),

·

1
N

N

Xj=1

(cid:13)
(cid:13)
(cid:13)

Z (j)

−

≤

E[Z]
(cid:13)
(cid:13)
(cid:13)

holds with high probability as p = Ω(mN/k). Substituting N in Claim 11, we obtain the results
in Lemma 15.

b

Proof of Claim 11. We are now ready to prove the claim. What we need are good bounds for
k
and its variance, then we can apply Bernstein’s inequality in Lemma 9 for the truncated version of
Z, then Z is also concentrates likewise.

Z
k

holds with high probability for

=

O(δs√k + µk/√n + σε√r) with δs =

Claim 12.
k
O∗(1/ log n).

Z

k ≤ R

R

e

−

(y

Proof. From the generative model and the support consistency of the encoding step, we have
S y = AT
Sx∗S + AT
Sx∗S + ε and xS = AT
Sε. Then,
SA∗
y = A∗x∗ + ε = A∗
•
•
•
•
•
AR,SAT
AR,SAT
S ε
SA∗
•
•
•
S )x∗S + (In −
AR,S)x∗S + AR,S(Ik −
O(σwk
M w
k
e
kF + σεk

Ax)R = (A∗R,Sx∗S + εR)
S x∗S −
AT
= (A∗R,S −
SA∗
•
•
Using the fact that x∗S and ε are sub-Gaussian and that
probability for a ﬁxed M and a sub-Gaussian w of variance σ2

SAT
S)R
A
•
•
Now, we need to bound those Frobenius norms. The ﬁrst quantity is easily bounded as

kF ) holds with high

SAT
S )R
A
•
•
•

k ≤
w, we have

AR,S(Ik −

AR,SkF +

AT
S )
SA∗
•
•

Ax)Rsgn(xi)

A∗R,S −

(In −

•kF ).

O(
k

(y
k

k ≤

M

−

−

ε.

k

e

A∗R,S −
k

AR,SkF ≤ k

A∗
S −
•

A
S kF ≤
•

δs√k,

(13)

since A is δs-close to A∗. To handle the other two, we use the fact that
this fact for the second term, we have

U V
k

kF ≤ k

U

V

kF . Using

kk

AR,S(Ik −
k

AT
S)
S A∗
•
•

kF ≤ k

AR,Skk

(Ik −

AT
S )
SA∗
•
•

kF ,

where
advantage of the closeness and incoherence properties:

AR,Sk ≤ k

•k ≤

AR

k

O(1) due to the nearness. The second part is rearranged to take

Ik −

k

AT
SA∗
SkF ≤ k
•
•

S)T A∗
T
(A
A∗
S A∗
A∗
Ik −
S −
SkF
S −
•
•
•
•
•
S)T A∗
T
SkF +
A∗
(A
S A∗
A∗
Ik −
S −
S kF
k
•
•
•
•
•
T
A∗
A
A∗
SkF +
S A∗
Ik −
A∗
S −
S kF
Skk
•
•
•
•
•
µk/√n + O(δs√k),

k

≤ k

≤ k

≤

where we have used
δs√k in (13) and

µk/√n because of the µ-incoherence of A∗,

A
S −
•

A∗
SkF ≤
•

k

O(1). Accordingly, the second Frobenius norm is bounded by

k

T
S A∗
Ik −
A∗
SkF ≤
k
•
•
A∗k ≤
A∗
Sk ≤ k
•
AR,S(Ik −

k

AT
S )
SA∗
•
•

kF ≤

O

µk/√n + δs√k

.

(cid:0)

32

(cid:1)

(14)

SAT
The noise term is handled using the eigen-decomposition U ΛU T of A
S, then with high prob-
•
•
ability

(In −

(U U T
SAT
•kF =
S)R
A
k
k
•
•
where the last inequality
A∗k ≤
O(1) due to the nearness. Putting (13), (14) and (15) together, we obtain the bounds in Claim
12.

•kF =
O(1) follows by

(In −
A
Sk ≤ k
•

kF ≤ k
A

•kF ≤
+

kk
A∗k

In −
A

O(√r), (15)

U ΛU T )R

A∗k ≤

UR
k
•

In −

k ≤ k

−
Λ

3
k

k ≤

UR

Λ)

−

Λ

k

k

k

Next, we determine a bound for the variance of Z.

2] = E[
Claim 13. E[
Ax)Rsgn(xi)
Z
k
k
−
k
k
s k + k2/n + σ2
O(δ2
ε r) with δs = O∗(1/ log n).

(y

2

i
|

S]

∈

≤

σ2 holds with high probability for σ2 =

Proof. We explicitly calculate the variance using the fact that x∗S is conditionally independent
given S, and so is ε. x∗S and ε are also independent and have zero mean. Then we can decompose
the norm into three terms in which the dot product is zero in expectation and the others can be
S ] = Ik, E[εεT ] = σεIn.
T
shortened using the fact that E[x∗Sx∗
SAT
AR,S AT
i
ε
A
SA∗
S)x∗S + (In −
S)R
∈
k
|
·
•
•
•
•
2
2
E[
AR,SAT
SAT
S] + σ2
SA∗
S)R
A
In −
i
i
S k
ε
F |
F |
k
∈
•k
∈
•
•
•
•
AT
Then, by re-writing A∗R,S −
S )
SA∗
AR,S)+AR,S(Ik−
S as before, we get the form (A∗R,S −
•
•
in which the ﬁrst term has norm bounded by δs√k. The second is further decomposed as

S] = E[
(A∗R,S −
k
= E[
A∗R,S −
k
AR,SAT
SA∗
•
•

Ax)Rsgn(xi)
k

E[
(y
k

S]]

i
|

−

∈

2

2

S].

E[
AR,S (Ik −
k

AT
S)
SA∗
k
•
•

2
i
F |

∈

S]

sup
S k

AR,Sk

≤

2E[
Ik −
k

2
AT
SA∗
i
S k
F |
•
•

∈

S],

(16)

where supSk
using the proof from Arora et al. (2015):

AR,Sk ≤ k

•k ≤

AR

O(1). We will bound E[
k

Ik −

2
AT
SA∗
i
F |
S k
•
•

∈

S]

≤

O(kδ2

s ) + O(k2/n)

E[
Ik −
k

AT
SA∗
Sk
•
•

2
i
F |

∈

S] = E[

1
4 k

A
j −
•

A∗
jk
•

2] + qij

= E[

S
Xj
∈

S
Xj
∈

k
Xj
=i

(1

−

j)2 +
AT
jA∗
•
•

AT
jA∗
,
k
•
•
Xj
S
∈
2 + qik
AT
iA∗
,
jk
•
•

−

−

AT
jA∗
,
•
•

2
jk

i
|

−

∈

S]

2 + qik
ik

AT
,
•

iA∗
ik
•

−

2,

where A
,
−
•
any j = 1, 2, . . . , m,

i is the matrix A with the i-th column removed, qij ≤

O(k2/m2) and qi ≤

O(k/m). For

AT
jA∗
,
•
•

k

jk

−

2 =

k

T
j A∗
A∗
,
•
•
−
j, A∗
A∗
li
•
•

2
j)T A∗
A∗
j + (A
j −
,
jk
•
•
•
−
2 +
j)T A∗
A∗
(A
j −
,
k
•
•
•

2

jk

−

≤

h
Xl
=j

≤

h
Xl
=j

j, A∗
A∗
li
•
•

2 +

A
j −
k
•

A∗
jk
•

2

A∗
,
k
•

−

2
jk

≤

µ2 + δ2
s .

The last inequality invokes the µ-incoherence, δ-closeness and the spectral norm of A∗. Similarly,
we come up with the same bound for

2. Consequently,

2 and

AT
iA∗
,
•
•

k

ik

−

AT
iA∗
,
ik
k
•
−
•
s ) + O(k2/n).
O(kδ2

E[
k

Ik −

2
AT
S A∗
i
Sk
F |
•
•

∈

S]

≤

(17)

33

For the last term, we invoke the inequality (15) (Claim 12) to get

SAT
S)R
A
•
•
Putting (16), (17) and (18) together and using
σ2 = O(δ2

•k
AR
k
ε r) with δs = O(1/ log2 n) . Finally, we complete the proof.
We now apply truncated Bernstein’s inequality to the random variable Z (j)(1

s k + k2/n + σ2

E[
(In −
k

2
i
F |

•k ≤

S]

≤

∈

r

1, we obtain the variance bound of Z:

(18)

))
Ω(
R
O(δs√k + µk/√n + σε√r) and σ2 = O(δ2
s k +

1
k

Z (j)

−

k≥

with
R
k2/n + σ2

and σ2 in Claims 12 and 13, which are
ε r). Then, (1/N )

=

R

N
j= Z (j) also concentrates:
e

N

P
Z (j)

Xi=1

1
N

(cid:13)
(cid:13)
(cid:13)

E[Z]

−

O

≤

R
N

+

O

(cid:13)
(cid:13)
(cid:13)

(cid:16)

(cid:17)

e
Ω(k + σ2

e

σ2
N

(cid:18)r

(cid:19)

= o(δs) + O(

k/n)

p

ε nr). Then, we ﬁnally ﬁnish the proof of Claim

holds with high probability when N =
11.

e

Proof of Lemma 13. With Claim 11, we study the concentration of
we consider this diﬀerence as an error term of the expectation gs
correlation of

gs
R,i. Using the expression in Lemma 5 with high probability, we can write

R,i around its mean gs
gs

R,i. Now,
R,i and using Lemma 6 to show the

b

b

R,i + (gs
where
+ O(k/m)
correlated-whp with A∗R,i where α = Ω(k/m), β = Ω(m/k) and γs ≤
, then we have done the proof Lemma 13.

gs
R,i = gs
A∗R,ik
b

gs
R,i) = 2α(AR,i −

AR,i −

R,i −

α
k

k ≤

b

k

v

·

A∗R,i) + v,

(o(δs) + O(ǫs)). By Lemma 6, we have

O(k/m)

·

b

gs
R,i is (α, β, γs)-
k/n))

(o(δs) + O(

p

D.2.2 Proof of Lemma 14

gs with A∗ w.h.p. and established the descent property of Al-
We have shown the correlation of
gorithm 2. The next step is to show that the nearness is preserved at each iteration. To prove
As+1
holds with high probability, we recall the update rule
k

A∗k ≤

A∗k

2
k

−

b

PH (
gs. Here H = (hij) where hij = 1 if i

As+1 = As

−

η

gs),

gs) = H

◦

PH(

where
j) and hij = 0 otherwise. Also,
supp(A
•
note that As is (δs, 2)-near to A∗ for δs = O∗(1/ log n). We already proved that this holds for the
exact expectation gs in Lemma 7. To prove for
gs, we again apply matrix Bernstein’s inequality to
gs)
bound
k
Consider a matrix random variable Z ,
E[ZZ T ]
b
k
k

= O(1).
by O(k/m) because η = Θ(m/k) and
Ax)sgn(x)T ). Our goal is to bound the spectral
PH((y
b
E[Z T Z]
since Z is asymmetric. To simplify our notations,
norm
k
we denote by xR the vector x by zeroing out the elements not in R. Also, denote Ri = supp(hi)
and S = supp(x). Then Z can be written explicitly as

b
− PH (
and, both

b
kPH(gs)
Z

A∗k

and

b
∈

−

k

k

k

k

Z = [(y

Ax)R1 sgn(x1), . . . , (y

Ax)Rmsgn(xm)],

−

−

where many columns are zero since x is k-sparse. The following claims follow from the proof of
Claim 42 in Arora et al. (2015). Here we state and detail some important steps.

Claim 14.

Z

O(k) holds with high probability.

k

k ≤

e

34

Proof. With high probability

Z
k

k ≤

(y

2
Ax)Risgn(xi)
k

−

≤

√k

(y

k

Ax)Rik

−

k
sXi
S
∈

(y
k

−

where we use Claim 12 with

Claim 15.

E[ZZ T ]

O(k2/n) and

k

k ≤

Ax)Rk ≤
k

E[Z T Z]
e

k ≤

O(δs√k) w.h.p., then

Z

O(k) holds w.h.p.

k
O(k2/n) with high probability.

k ≤

e

Proof. The ﬁrst term is easily handled. Speciﬁcally, with high probability

e

E[ZZ T ]

k

k ≤ k

E[

(y

−

Ax)Ri sgn(xi)2(y

Ax)T

Ri]
k

−

=

E[
k

(y

Ax)Ri(y

−

−

Ax)T

Ri ]

k ≤

O(k2/n),

S
Xi
∈

S
Xi
∈

where the last inequality follows from the proof of Claim 42 in Arora et al. (2015), which is tedious
to be repeated.
To bound

, we use bound of the full matrix (y
O(√k) w.h.p. is similar to what derived in Claim 12. Then with high probability,

Ax)sgn(x)T . Note that

E[Z T Z]
k

y
k

k ≤

Ax

−

−

k

E[Z T Z]
k

E[sgn(x)(y

Ax)T (y

Ax)sgn(x)T ]

k ≤ k

e
where E[sgn(x)sgn(x)T ] = diag(q1, q2, . . . , qm) has norm bounded by O(k/m). We now can apply
O(k2/m), then with
Bernstein’s inequality for the truncated version of Z with
p =

O(k) and σ2 =

O(m),

k ≤

k ≤

R

=

−

−

e

e

E[sgn(x)sgn(x)T ]

O(k2/m).

O(k)
k

e

kPH(gs)

− PH(

gs)

k ≤

O(k)
p

e

e
O(k2/m)
p

≤

+

s

e

e

O∗(k/m)

holds with high probability. Finally, we invoke the bound η = O(m/k) and complete the proof.

b

Appendix E. A Special Case: Orthonormal A∗

We extend our results for the special case where the dictionary is orthonormal. As such, the
dictionary is perfectly incoherent and bounded (i.e., µ = 0 and

= 1).

Theorem 7. Suppose that A∗ is orthonormal. When p1 =
Ω(nr), then with high
probability Algorithm 1 returns an initial estimate A0 whose columns share the same support as
A∗ and with (δ, 2)-nearness to A∗ with δ = O∗(1/ log n). The sparsity of A∗ can be achieved up to
r = O∗

e

e

)

.

,

min( √n
log2 n

n
k2 log2 n

We use the same initialization procedure for this special case and achieve a better order of r.

(cid:0)

(cid:1)

The proof of Theorem 7 follows the analysis for the general case with following two results:

A∗k
k
Ω(n) and p2 =

Claim 16 (Special case of Claim 3). Suppose that u = A∗α + εu is a random sample and U =
supp(α). Let β = A∗
O(√k log n + σε√n log n).

T u, then w.h.p., we have (a)

σε log n for each i and (b)

αi| ≤

βi −
|

k ≤

β

k

T ǫu, then βi −
A∗
i, ǫui
=
Proof. We have β = A∗
α
h
k
•
in Claim 2, we have the claim proved.
α
and
A∗
probability bounds of
k
k
h
•
O(σε log2 n) and have the following
βiβ′i| ≤
V ,
U
We draw from the claim that for any i /
∈

T u = α + A∗
,
i, ǫui

. Using

ǫuk
k

ǫuk
k

αi =

and

−

∩

β

k

|

result:

35

Lemma 16. Fix samples u and v and suppose that y = A∗x∗ + ε is a random sample independent
of u, v. The expected value of the score for the lth component of y is given by:

el , E[
h

y, u

y, v

ih

i

y2
l ] =

2
li + perturbation terms
qiciβiβ′iA∗

where qi = P[i
absolute value at most O∗

S], qij = P[i, j

∈

V
U
Xi
∩
∈
S] and ci = E[x4
i
i |

∈
k/n log2 n max(1/√n, k2/n)

∈

S]. Moreover, the perturbation terms have
.

Proof. Lemma follows Lemma 1 via Claim 3 except that the second term of E1 is bounded by
O(k log2 n/n3/2).

(cid:0)

(cid:1)

Appendix F. Extensions of Arora et al. (2015)

F.1 Sample complexity in noisy case

In this section, we study the sample complexity of the algorithms in Arora et al. (2015) in the
presence of noise. While noise with order σε = O(1/√n) does not change the sample complexity
of the initialization algorithm, it aﬀects that of the descent stage. The analysis involves producing
a sharp bound for

.

gs
,i −
•

gs
ik
•

k

Lemma 17. For a regular dictionary A∗, suppose As is (δs, 2)-near to A∗ with δs = O∗(1/ log n),
then with high probability

k/n)) when p =

(o(δ) + O(

O(k/m)

b

Ω(m + σ2
ε

mn2
k ).

gs
,i −
k
•
Proof. This follows directly from Lemma 15 where r = n.
b

We tighten the original analysis to obtain the complexity

gs
ik ≤
•

·

case. Putting together with p =
mn2
sample complexity
k ) for the algorithms in Arora et al. (2015) in the noise regime.

O(mk + σ2
ε

e

e

p
Ω(mk) for the noiseless
Ω(m) instead of
Ω(mk) required by the initialization, we then have the overall

e

e

F.2 Extension of Arora et al. (2015)’s initialization algorithm for sparse case

e

We study a simple and straightforward extension of the initialization algorithm of Arora et al.
(2015) for the sparse case. This extension is produced by adding an extra projection, and is
described in Figure 3. The recovery of the support of A∗ is guaranteed by the following Lemma:
Lemma 18. Suppose that z∗ ∈
Provided z is δ-close to z∗ and z0 =
z∗ has the same support.

Rn is r-sparse whose nonzero entries are at least τ in magnitude.
Hr(z) with δ = O∗(1/ log n) and r = O∗(log2 n), then z0 and

Proof. Since z0 is δ-close to z∗, then

δ for every i. For i

supp(z∗),

∈

z0
z∗k ≤
−
k
zi −
z∗i | − |
zi| ≥ |
|
δ. Since τ > O(1/√r)

δ and

zi −
|
τ
z∗i | ≥

−

z∗i | ≤
δ

and for i /
≫
∈
support z∗, and hence z0 and z∗ has the same support.

supp(z∗),

zi| ≤
|

δ, then the r-largest entries of z are in the

√n
k log3 n

Theorem 8. Suppose that Assumptions B1-B4 hold and Assumptions A1-A3 satify with µ =
Ω(mk), then with high probability
O∗
Algorithm 3 returns an initial estimate A0 whose columns share the same support as A∗ and with
e
(δ, 2)-nearness to A∗ with δ = O∗(1/ log n).

and r = O∗(log2 n). When p1 =

Ω(m) and p2 =

e

(cid:1)

(cid:0)

36

Algorithm 3 Pairwise Reweighting with Hard-Thresholding

∅

Initialize L =
Randomly divide p samples into two disjoint sets
While
P1 at random
Reconstruct the re-weighted covariance matrix

< m. Pick u and v from

L
|

|

Mu,v:

P1 and

P2 of sizes p1 and p2 respectively

p2

1
p2

y(i), u
h
Xi=1

ih

Mu,v =

c

c
y(i), v

y(i)(y(i))T
i

Compute the top singular values δ1, δ2 and top singular vector z of
If δ1 ≥
z =
If z is not within distance 1/ log n of any vector in L even with sign ﬂip

Hr keeps r largest entries of z

Ω(k/m) and δ2 < O∗(k/m log n)
Hr(z), where
z
L = L
∪ {

Mu,v

c

}

Return A0 = (L1, . . . , Lm)

This algorithm requires r = O∗(log2 n), which is somewhat better than ours. However, the

sample complexity and running time is inferior as compared with our novel algorithm.

Appendix G. Neural Implementation of Our Approach

We now brieﬂy describe why our algorithm is “neurally plausible”. Basically, similar to the argu-
ment in Arora et al. (2015), we describe at a very high level how our algorithm can be implemented
via a neural network architecture. One should note that although both our initialization and de-
scent stages are non-trivial modiﬁcations of those in Arora et al. (2015), both still inherit the nice
neural plausiblity property.

G.1 Neural implementation of Stage 1: Initialization

Recall that the initialization stage includes two main steps: (i) estimate the support of each column
of the synthesis matrix, and (ii) compute the top principal component(s) of a certain truncated
weighted covariance matrix. Both steps involve simple vector and matrix-vector manipulations that
can be implemented plausibly using basic neuronal manipulations.
y, u

For the support estimation step, we compute the product
y, u
i
h

y, followed by a thresh-
y
y, u
i
olding. The inner products,
can be computed using neurons via an online manner
where the samples arrive in sequence; the thresholding can be implemented via a ReLU-type non-
linearity.

and

y, v

ih

◦

h

h

i

For the second step, it is well known that the top principal components of a matrix can be

computed in a neural (Hebbian) fashion using Oja’s Rule Oja (1992).

G.2 Neural implementation of Stage 2: Descent

Our neural implementation of the descent stage (Algorithm 2), shown in Figure 2, mimics the
architecture of Arora et al. (2015), which describes a simple two-layer network architecture for
computing a single gradient update of A. The only diﬀerence in our case is that most of the value

37

y

r

x =
threshold(AT y)

+

+

+

+

+

+

−

−

−

Aij

+

+
+

xj

Hebbian rule
Aij = ηrixj

∇

Figure 2: Neural network implementation of Algorithm 2. The network takes the image y as input
and produces the sparse representation x as output. The hidden layer represents the
residual between the image and its reconstruction Ax. The weights Aij’s are stored on
synapses, but most of them are zero and shown by the dotted lines.

in A are set to zero, or in other words, our network is sparse. The network takes values y from the
input layer and produce x as the output; there is an intermediate layer in between connecting the
middle layer with the output via synapses. The synaptic weights are stored on A. The weights are
updated by Hebbian learning. In our case, since A is sparse (with support given by R, as estimated
in the ﬁrst stage), we enforce the condition the corresponding synapses are inactive. In the output
layer, as in the initialization stage, the neurons can use a ReLU-type non-linear activation function
to enforce the sparsity of x.

References

Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon.
Learning sparsely used overcomplete dictionaries. In Conference on Learning Theory, pages 123–
137, 2014.

Michal Aharon, Michael Elad, and Alfred Bruckstein. k-svd: An algorithm for designing over-
complete dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):
4311–4322, 2006.

Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcom-

plete dictionaries. In Conference on Learning Theory, pages 779–806, 2014.

Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, eﬃcient, and neural algorithms

for sparse coding. In Conference on Learning Theory, pages 113–149, 2015.

38

Jaros law B lasiok and Jelani Nelson. An improved analysis of the er-spud dictionary learning algo-

rithm. arXiv preprint arXiv:1602.05719, 2016.

Y-Lan Boureau, Francis Bach, Yann LeCun, and Jean Ponce. Learning mid-level features for
recognition. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on,
pages 2559–2566. IEEE, 2010.

Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on

information theory, 51(12):4203–4215, 2005.

Niladri Chatterji and Peter Bartlett. Alternating minimization for dictionary learning with random

initialization. 2017. arXiv:1711.03634v1.

Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over

learned dictionaries. IEEE Transactions on Image processing, 15(12):3736–3745, 2006.

Kjersti Engan, Sven Ole Aase, and J Hakon Husoy. Method of optimal directions for frame design. In
IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 5,
pages 2443–2446. IEEE, 1999.

Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of

the 27th International Conference on Machine Learning (ICML), pages 399–406, 2010.

R´emi Gribonval, Rodolphe Jenatton, and Francis Bach. Sparse and spurious: dictionary learning
with noise and outliers. IEEE Transactions on Information Theory, 61(11):6298–6319, 2015a.

R´emi Gribonval, Rodolphe Jenatton, Francis Bach, Martin Kleinsteuber, and Matthias Seibert.
Sample complexity of dictionary learning and other matrix factorizations. IEEE Transactions
on Information Theory, 61(6):3469–3486, 2015b.

Hamid Krim, Dewey Tucker, Stephane Mallat, and David Donoho. On denoising and best signal

representation. IEEE Transactions on Information Theory, 45(7):2225–2238, 1999.

Rados law Adamczak. A note on the sample complexity of the er-spud algorithm by spielman,
wang and wright for exact recovery of sparsely used dictionaries. Journal of Machine Learning
Research, 17:1–18, 2016.

Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for
sparse coding. In Proceedings of the 26th International Conference on Machine Learning (ICML),
pages 689–696, 2009.

Arya Mazumdar and Ankit Singh Rawat. Associative memory using dictionary learning and ex-
pander decoding. In Proc. Conf. American Assoc. Artiﬁcial Intelligence (AAAI), pages 267–273,
2017.

Thanh V. Nguyen, Raymond K. W. Wong, and Chinmay Hegde. A provable approach for double-

sparse coding. In Proc. Conf. American Assoc. Artiﬁcial Intelligence (AAAI), Feb. 2018.

Erkki Oja. Principal components, minor components, and linear neural networks. Neural networks,

5(6):927–935, 1992.

39

Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy

employed by v1? Vision research, 37(23):3311–3325, 1997.

Ron Rubinstein, Alfred M Bruckstein, and Michael Elad. Dictionaries for sparse representation

modeling. Proceedings of the IEEE, 98(6):1045–1057, 2010a.

Ron Rubinstein, Michael Zibulevsky, and Michael Elad. Double sparsity: Learning sparse dictionar-
ies for sparse signal approximation. IEEE Transactions on Signal Processing, 58(3):1553–1564,
2010b.

Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries.

In Conference on Learning Theory, pages 37–1, 2012.

Jeremias Sulam, Boaz Ophir, Michael Zibulevsky, and Michael Elad. Trainlets: Dictionary learning

in high dimensions. IEEE Transactions on Signal Processing, 64(12):3180–3193, 2016.

Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery using nonconvex optimization.
In Proceedings of the 33rd International Conference on Machine Learning (ICML), pages 2351–
2360, 2015.

Lingxiao Wang, Xiao Zhang, and Quanquan Gu. A uniﬁed computational and statistical framework

for nonconvex low-rank matrix estimation. arXiv preprint arXiv:1610.05275, 2016.

Yuchen Zhang, Xi Chen, Dengyong Zhou, and Michael I Jordan. Spectral methods meet em: A
provably optimal algorithm for crowdsourcing. Journal of Machine Learning Research, 17(1):
3537–3580, 2016.

40

7
1
0
2
 
c
e
D
 
2
1

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
8
3
6
3
0
.
1
1
7
1
:
v
i
X
r
a

Provably Accurate Double-Sparse Coding

Thanh V. Nguyen
Iowa State University, ECE Department

Raymond K. W. Wong
Texas A&M University, Statistics Department

Chinmay Hegde ∗
Iowa State University, ECE Department

Editor: TBD

thanhng@iastate.edu

raywong@stat.tamu.edu

chinmay@iastate.edu

Abstract
Sparse coding is a crucial subroutine in algorithms for various signal processing, deep learning,
and other machine learning applications. The central goal is to learn an overcomplete dictionary
that can sparsely represent a given input dataset. However, a key challenge is that storage, trans-
mission, and processing of the learned dictionary can be untenably high if the data dimension is
high. In this paper, we consider the double-sparsity model introduced by Rubinstein et al. (2010b)
where the dictionary itself is the product of a ﬁxed, known basis and a data-adaptive sparse com-
ponent. First, we introduce a simple algorithm for double-sparse coding that can be amenable to
eﬃcient implementation via neural architectures. Second, we theoretically analyze its performance
and demonstrate asymptotic sample complexity and running time beneﬁts over existing (provable)
approaches for sparse coding. To our knowledge, our work introduces the ﬁrst computationally
eﬃcient algorithm for double-sparse coding that enjoys rigorous statistical guarantees. Finally,
we support our analysis via several numerical experiments on simulated data, conﬁrming that our
method can indeed be useful in problem sizes encountered in practical applications.
Keywords: Sparse coding, provable algorithms, unsupervised learning

1. Introduction

1.1 Motivation

Representing signals as sparse linear combinations of atoms from a dictionary is a popular approach
in many domains. In this paper, we study the problem of dictionary learning (also known as sparse
coding), where the goal is to learn an eﬃcient basis (dictionary) that represents the underlying
class of signals well. In the typical sparse coding setup, the dictionary is overcomplete (i.e., the
cardinality of the dictionary exceeds the ambient signal dimension) while the representation is
sparse (i.e., each signal is encoded by a combination of only very few dictionary atoms.)

Sparse coding has a rich history in diverse ﬁelds such as signal processing, machine learning, and
computational neuroscience. Discovering optimal basis representations of data is a central focus of
image analysis (Krim et al., 1999; Elad and Aharon, 2006; Rubinstein et al., 2010a), and dictionary
learning has proven widely successful in imaging problems such as denoising, deconvolution, inpaint-

∗. This work is supported in part by the National Science Foundation under the grants CCF-1566281 and DMS-
1612985. An abbreviated conference version will appear in the proceedings of AAAI 2018 (Nguyen et al., 2018).

1

ing, and compressive sensing (Elad and Aharon, 2006; Candes and Tao, 2005; Rubinstein et al.,
2010a). Sparse coding approaches have also been used as a core building block of deep learn-
ing systems for prediction (Gregor and LeCun, 2010; Boureau et al., 2010) and associative mem-
ory (Mazumdar and Rawat, 2017). Interestingly, the seminal work by Olshausen and Field (1997)
has shown intimate connections between sparse coding and neuroscience: the dictionaries learned
from image patches of natural scenes bear strikingly resemblance to spatial receptive ﬁelds observed
in mammalian primary visual cortex.

From a mathematical standpoint, the sparse coding problem is formulated as follows. Given p
Rn
m (m > n)
data samples Y = [y(1), y(2), . . . , y(p)]
and corresponding sparse code vectors X = [x(1), x(2), . . . , x(p)]
p such that the representation
DX ﬁts the data samples as well as possible. Typically, one obtains the dictionary and the code
vectors as the solution to the following optimization problem:

p, the goal is to ﬁnd a dictionary D

Rm

Rn

∈

∈

∈

×

×

×

min
D,X L

(D, X) =

y(j)
k

−

Dx(j)

2
2,
k

p

1
2

Xj=1
(x(j))

S

≤

s.t.

p

S

Xj=1

(1)

) is some sparsity-inducing penalty function on the code vectors, such as the ℓ1-norm.
(
·
controls the reconstruction error while the constraint enforces the sparsity

However, even a cursory attempt at solving the optimization problem (1) reveals the following

S

where
The objective function
of the representation.

L

obstacles:

1. Theoretical challenges. The constrained optimization problem (1) involves a non-convex
(in fact, bilinear) objective function, as well as potentially non-convex constraints depending
on the choice of the sparsity-promoting function
(for example, the ℓ0 function.) Hence,
obtaining provably correct algorithms for this problem can be challenging. Indeed, the vast
majority of practical approaches for sparse coding have been heuristics (Engan et al., 1999;
Aharon et al., 2006; Mairal et al., 2009); Recent works in the theoretical machine learning
community have bucked this trend, providing provably accurate algorithms if certain assump-
tions are satisﬁed (Spielman et al., 2012; Agarwal et al., 2014; Arora et al., 2015; Sun et al.,
2015; B lasiok and Nelson, 2016; law Adamczak, 2016; Chatterji and Bartlett, 2017). How-
ever, relatively few of these newer methods have been shown to provide good empirical per-
formance in actual sparse coding problems.

S

2. Practical challenges. Even if theoretical correctness issues were to be set aside, and we are
somehow able to eﬃciently learn sparse codes of the input data, we often ﬁnd that applications
using such learned sparse codes encounter memory and running-time issues. Indeed, in the
overcomplete case, only the storage of the learned dictionary D incurs mn = Ω(n2) memory
cost, which is prohibitive when n is large. Therefore, in practical applications (such as image
analysis) one typically resorts to chop the data into smaller blocks (e.g., partitioning image
data into patches) to make the problem manageable.

A related line of research has been devoted to learning dictionaries that obey some type of struc-
ture. Such structural information can be leveraged to incorporate prior knowledge of underlying

2

signals as well as to resolve computational challenges due to the data dimension. For instance, the
dictionary is assumed to be separable, or obey a convolutional structure. One such variant is the
double-sparse coding problem (Rubinstein et al., 2010b; Sulam et al., 2016) where the dictionary
D itself exhibits a sparse structure. To be speciﬁc, the dictionary is expressed as:

D = ΦA,

×

∈

Rn

n, and a learned “synthesis” matrix
i.e., it is composed of a known “base dictionary” Φ
m whose columns are sparse. The base dictionary Φ is typically any ﬁxed basis chosen
A
according to domain knowledge, while the synthesis matrix A is column-wise sparse and is to be
learned from the data. The basis Φ is typically orthonormal (such as the canonical or wavelet
basis); however, there are cases where the base dictionary Φ is overcomplete (Rubinstein et al.,
2010b; Sulam et al., 2016).

∈

×

Rn

There are several reasons why such the double-sparsity model can be useful. First, the double-
sparsity assumption is rather appealing from a conceptual standpoint, since it lets us combine
the knowledge of decades of modeling eﬀorts in harmonic analysis with the ﬂexibility of learning
new representations tailored to speciﬁc data families. Moreover, such a double-sparsity model has
computational beneﬁts. If the columns of A are (say) r-sparse (i.e., each column contains no more
than r
n non-zeroes) then the overall burden of storing, transmitting, and computing with A is
much lower than that for general unstructured dictionaries. Finally, such a model lends itself well
to interpretable learned features if the atoms of the base dictionary are semantically meaningful.

≪

All the above reasons have spurred researchers to develop a series of algorithms to learn
doubly-sparse codes (Rubinstein et al., 2010b; Sulam et al., 2016). However, despite their empiri-
cal promise, no theoretical analysis of their performance have been reported in the literature and
to date, we are unaware of a provably accurate, polynomial-time algorithm for the double-sparse
coding problem. Our goal in this paper is precisely to ﬁll this gap.

1.2 Our Contributions

In this paper, we provide a new framework for double-sparse coding. To the best of our knowledge,
our approach is the ﬁrst method that enjoys provable statistical and algorithmic guarantees for
this problem. In addition, our approach enjoys three beneﬁts: we demonstrate that the method is
neurally plausible (i.e., its execution can plausibly be achieved using a neural network architecture),
robust to noise, as well as practically useful.

Inspired by the aforementioned recent theoretical advances in sparse coding, we assume a
learning-theoretic setup where the data samples arise from a ground-truth generative model. Infor-
mally, suppose there exists a true (but unknown) synthesis matrix A∗ that is column-wise r-sparse,
and the ith data sample is generated as:

y(i) = ΦA∗x∗

(i) + noise,

i = 1, 2, . . . , p

(i) is independently drawn from a distribution supported on the set of k-
where the code vector x∗
sparse vectors. We desire to learn the underlying matrix A∗. Informally, suppose that the synthesis
matrix A∗ is incoherent (the columns of A∗ are suﬃciently close to orthogonal) and has bounded
spectral norm. Finally, suppose that the number of dictionary elements, m, is at most a constant
multiple of n. All of these assumptions are standard1.

1. We clarify both the data and the noise model more concretely in Section 2 below.

3

We will demonstrate that the true synthesis matrix A∗ can be recovered (with small error) in
a tractable manner as suﬃciently many samples are provided. Speciﬁcally, we make the following
novel contributions:

1. We propose a new algorithm that produces a coarse estimate of the synthesis matrix that
is suﬃciently close to the ground truth A∗. In contrast with previous double-sparse coding
methods (such as Sulam et al. (2016)), our algorithm is not based on alternating minimiza-
tion. Rather, it builds upon spectral initialization-based ideas that have recently gained
popularity in non-convex machine learning (Zhang et al., 2016; Wang et al., 2016).

2. Given the above coarse estimate of the synthesis matrix A∗, we propose a descent-style algo-
rithm to reﬁne the above estimate of A∗. This algorithm is simpler than previously studied
double-sparse coding algorithms (such as the Trainlets approach of Sulam et al. (2016)), while
still giving good statistical performance. Moreover, this algorithm can be realized in a manner
amenable to neural implementations.

3. We provide a rigorous analysis of both algorithms. Put together, our analysis produces
the ﬁrst provably polynomial-time algorithm for double-sparse coding. We show that the
algorithm provably returns a good estimate of the ground-truth; in particular, in the absence
of noise we prove that Ω(mr polylog n) samples are suﬃcient for a good enough initialization
in the ﬁrst algorithm, as well as guaranteed linear convergence of the descent phase up to a
precise error parameter that can be interpreted as the radius of convergence.

Indeed, our analysis shows that employing the double-sparsity model helps in this context,
and leads to a strict improvement in sample complexity, as well as running time over previous
rigorous methods for (regular) sparse coding such as Arora et al. (2015).

4. We also analyze our approach in a more realistic setting with the presence of additive noise
and demonstrate its stability. We prove that Ω(mr polylog n) samples are suﬃcient to obtain
a good enough estimate in the initialization, and also to obtain guaranteed linear convergence
during descent to provably recover A∗.

5. We underline the beneﬁt of the double-sparse structure over the regular model by analyzing
the algorithms in Arora et al. (2015) under the noisy setting. As a result, we obtain the
, which demonstrates a negative eﬀect of noise
sample complexity O
on this approach.

mn2
k )polylog n

(mk + σ2
ε

(cid:0)

(cid:1)

6. We rigorously develop a hard thresholding intialization that extends the spectral scheme
in Arora et al. (2015). Additionally, we provide more results for the case where A is orthonor-
mal, sparse dictionary to relax the condition on r, which may be of independent interest.

7. While our analysis mainly consists of suﬃciency results and involves several (absolute) un-
speciﬁed constants, in practice we have found that these constants are reasonable. We justify
our observations by reporting a suite of numerical experiments on synthetic test datasets.

Overall, our approach results in strict improvement in sample complexity, as well as running
time, over previous rigorously analyzed methods for (regular) sparse coding, such as Arora et al.
(2015). See Table 1 for a detailed comparison.

4

Setting

Reference

Sample complexity
(w/o noise)

Sample complexity
(w/ noise)

Upper bound on
running time

Expt

Regular

MOD (Engan et al., 1999)

K-SVD (Aharon et al., 2006)

Spielman et al. (2012)

Arora et al. (2014)

Gribonval et al. (2015a)

Arora et al. (2015)

Double Sparsity (Rubinstein et al., 2010b)

Double
Sparse

Gribonval et al. (2015b)

Trainlets (Sulam et al., 2016)

✗

✗

✗

✗

O(n2 log n)

e
O(m2/k2)

O(nm3)

e
O(mk)

e
O(mr)

e
O(mr)

✗

✗

✗

✗

✗

✗

✗

O(nm3)

e
O(mr)

e
Ω(n4)

e
O(np2)

e
O(mn2p)

✗

✗

✗

✗

✗

✗

✓

✓

✓

✗

✗

✗

✓

✗

✓

✓

This paper

e
O(mr + σ2
ε

mnr
k )

e
O(mnp)

Table 1: Comparison of various sparse coding techniques. Expt: whether numerical experiments have
been conducted. ✗ in all other columns indicates no provable guarantees. Here, n is the signal
dimension, and m is the number of atoms. The sparsity levels for A and x are r and k respectively,
and p is the sample size.

1.3 Techniques

At a high level, our method is an adaptation of the seminal approach of Arora et al. (2015). As
is common in the statistical learning literature, we assume a “ground-truth” generative model for
the observed data samples, and attempt to estimate the parameters of the generative model given
a suﬃcient number of samples. In our case, the parameters correspond to the synthesis matrix A∗,
which is column-wise r-sparse. The natural approach is to formulate a loss function in terms of A
such as Equation (1), and perform gradient descent with respect to the surface of the loss function
to learn A∗.

The key challenge in sparse coding is that the gradient is inherently coupled with the codes of the
training samples (i.e., the columns of X ∗), which are unknown a priori. However, the main insight
of Arora et al. (2015) is that within a small enough neighborhood of A∗, a noisy version of X ∗ can be
estimated, and therefore the overall method is similar to performing approximate gradient descent.
Formulating the actual algorithm as a noisy variant of approximate gradient descent allows us to
overcome the ﬁnite-sample variability of the loss, and obtain a descent property directly related to
(the population parameter) A∗.

The second stage of our approach (i.e., our descent-style algorithm) leverages this intuition.
However, instead of standard gradient descent, we perform approximate projected gradient descent,
such that the column-wise r-sparsity property is enforced in each new estimate of A∗. Indeed, such
an extra projection step is critical in showing a sample complexity improvement over the existing
approach of Arora et al. (2015).The key novelty is in ﬁguring out how to perform the projection in
each gradient iteration. For this purpose, we develop a novel initialization algorithm that identiﬁes
the locations of the non-zeroes in A∗ even before commencing the descent phase. This is nontrivially

5

diﬀerent from initialization schemes used in previous rigorous methods for sparse coding, and the
analysis is somewhat more involved.

In Arora et al. (2015), (the principal eigenvector of) a weighted covariance matrix of y (esti-
mated by the weighted average of outer products yiyT
i ) is shown to provide a coarse estimate of
a dictionary atom. We extend this idea and rigoriously show that the diagonal of the weighted
covariance matrix serves as a good indicator of the support of a column in A∗. The success relies on
the concentration of the diagonal vector with dimension n, instead of the covariance matrix with
n. With the support selected, our scheme only utilizes a reduced weighted covariance
dimensions n
matrix with dimensions at most r
r. This initialization scheme enables us to eﬀectively reduce
the dimension of the problem, and therefore leads to signiﬁcant improvement in sample complexity
and running time over previous (provable) sparse coding methods when the data representation
sparsity k is much smaller than m.

×

×

Further, we rigorously analyze the proposed algorithms in the presence of noise with a bounded
expected norm. Our analysis shows that our method is stable, and in the case of i.i.d. Gaussian noise
with bounded expected ℓ2-norms, is at least a polynomial factor better than previous polynomial
time algorithms for sparse coding.

The empirical performance of our proposed method is demonstrated by a suite of numerical
experiments on synthetic datasets.In particular, we show that our proposed methods are simple
and practical, and improve upon previous provable algorithms for sparse coding.

1.4 Paper Organization

The remainder of this paper is organized as follows. Section 2 introduces notation, key model
assumptions, and informal statements of our main theoretical results. Section 3 outlines our ini-
tialization algorithm (along with supporting theoretical results) while Section 4 presents our descent
algorithm (along with supporting theoretical results). Section 5 provides a numerical study of the
eﬃciency of our proposed algorithms, and compares it with previously proposed methods. Finally,
Section 6 concludes with a short discussion. All technical proofs are relegated to the appendix.

2. Setup and Main Results

∈

= 0
}

}
[m] : xi 6

as the support set of x. Given any subset S

for any integer m > 1. For any vector x = [x1, x2, . . . , xm]T

2.1 Notation
Rm,
We deﬁne [m] ,
1, . . . , m
∈
{
we write supp(x) ,
[m], xS
i
{
m, we
corresponds to the sub-vector of x indexed by the elements of S. For any matrix A
×
i and AT
to represent the i-th column and the j-th row respectively. For some appropriate
use A
j
•
•
S ) be the submatrix of A with rows (respectively columns)
(respectively, A
sets R and S, let AR
•
•
i, we use AR,i
indexed by the elements in R (respectively S). In addition, for the i-th column A
•
to denote the sub-vector indexed by the elements of R. For notational simplicity, we use AT
to
R
indicate (AR
) to represent
and sgn(
·
•
the element-wise Hadamard operator and the element-wise sign function respectively. Further,
thresholdK (x) is a thresholding operator that replaces any elements of x with magnitude less than
K by zero.

)T , the tranpose of A after a row selection. Besides, we use

⊆
Rn

∈

◦

•

x

The ℓ2-norm

A
for a vector x and the spectral norm
for a matrix A appear several
k
k
,
times. In some cases, we also utilize the Frobenius norm
kF and the operator norm
A
k1,2 is essentially the maximal Euclidean norm of any column of A.
A
max
k

. The norm
k

A
k

k1,2

Ax

1k

k1≤

k

k

k

x

k

6

Ω(g(n))) if f (n) is upper bounded (respectively,
constant. Next, f (n) = Θ(g(n)) if and only if f (n) = O(g(n)) and f (n) = Ω(g(n)). Also
and
f (n) = o(g(n)) (or f (n) = ω(g(n))) if limn

For clarity, we adopt asymptotic notations extensively. We write f (n) = O(g(n)) (or f (n) =
lower bounded) by g(n) up to some positive
Ω
O represent Ω and O up to a multiplicative poly-logarithmic factor respectively. Finally
e
f (n)/g(n)
|
Throughout the paper, we use the phrase “with high probability” (abbreviated to w.h.p.) to
ω(1). In addition, g(n) = O∗(f (n))

describe an event with failure probability of order at most n−
means g(n)

Kf (n) for some small enough constant K.

f (n)/g(n)
|

= 0 (limn

→∞ |

→∞ |

∞

=

).

e

≤

2.2 Model

Suppose that the observed samples are given by

y(i) = Dx∗

(i) + ε,

i = 1, . . . , p,

i.e., we are given p samples of y generated from a ﬁxed (but unknown) dictionary D where the sparse
speciﬁed below. In the double-sparse
code x∗ and the error ε are drawn from a joint distribution
n is a known
setting, the dictionary is assumed to follow a decomposition D = ΦA∗, where Φ
orthonormal basis matrix and A∗ is an unknown, ground truth synthesis matrix. An alternative
(and interesting) setting is an overcomplete Φ with a square A∗, which our analysis below does not
cover; we defer this to future work. Our approach relies upon the following assumptions on the
synthesis dictionary A∗:

Rn

D

∈

×

A1 A∗ is overcomplete (i.e., m

n) with m = O(n).

≥

A2 A∗ is µ-incoherent, i.e., for all i

= j,

A∗
i, A∗
j i| ≤
•
•

|h

µ/√n.

A3 A∗
i has at most r non-zero elements, and is normalized such that
•
A∗ij| ≥

= 0 and τ = Ω(1/√r).

τ for A∗ij 6

|

A∗
ik
k
•

= 1 for all i. Moreover,

A4 A∗ has bounded spectral norm such that

A∗k ≤
k

O(

m/n).

p

All these assumptions are standard. In Assumption A2, the incoherence µ is typically of order
O(log n) with high probability for a normal random matrix (Arora et al., 2014). Assumption A3
is a common assumption in sparse signal recovery. The bounded spectral norm assumption is
In addition to Assumptions A1-A4, we make the following
also standard (Arora et al., 2015).
distributional assumptions on

:

D

B1 Support S = supp(x∗) is of size at most k and uniformly drawn without replacement from

[m] such that P[i

S] = Θ(k/m) and P[i, j

S] = Θ(k2/m2) for some i, j

[m] and i

= j.

∈
B2 The nonzero entries x∗S are pairwise independent and sub-Gaussian given the support S with

∈

∈

E[x∗i |
i
B3 For i

∈

S] = 0 and E[x∗
2
i

S] = 1.

i
|

∈

S,

x∗i | ≥
|

∈

C where 0 < C

1.

≤

B4 The additive noise ε has i.i.d. Gaussian entries with variance σ2

ε with σε = O(1/√n).

7

For the rest of the paper, we set Φ = In, the identity matrix of size n. This only simpliﬁes the

arguments but does not change the problem because one can study an equivalent model:

y′ = Ax∗ + ε′,

where y′ = ΦT y and ε′ = ΦT ε, as ΦT Φ = In. Due to the Gaussianity of ε, ε′ also has independent
entries. Although this property is speciﬁc to Gaussian noise, all the analysis carried out below
can be extended to sub-Gaussian noise with minor (but rather tedious) changes in concentration
arguments.

Our goal is to devise an algorithm that produces an provably “good” estimate of A∗. For this,
we need to deﬁne a suitable measure of “goodness”. We use the following notion of distance that
measures the maximal column-wise diﬀerence in ℓ2-norm under some suitable transformation.

Deﬁnition 1 ((δ, κ)-nearness). A is said to be δ-close to A∗ if there is a permutation π : [m]
and a sign ﬂip σ : [m] :
to be (δ, κ)-near to A∗ if

[m]
δ for every i. In addition, A is said

→

σ(i)A
A∗
π(i) −
ik ≤
•
•
also holds.
A∗k

k
κ
k

such that
1
}
{±
A∗k ≤
A
π −
k
•

For notational simplicity, in our theorems we simply replace π and σ in Deﬁnition 1 with the
identity permutation π(i) = i and the positive sign σ(
) = +1 while keeping in mind that in reality
·
we are referring to one element of the equivalence class of all permutations and sign ﬂip transforms
of A∗.

We will also need some technical tools from Arora et al. (2015) to analyze our gradient descent-
Rn to optimize
style method. Consider any iterative algorithm that looks for a desired solution z∗ ∈
some function f (z). Suppose that the algorithm produces a sequence of estimates z1, . . . , zs via
the update rule:

zs+1 = zs

ηgs,

−

for some vector gs and scalar step size η. The goal is to characterize “good” directions gs such that
the sequence converges to z∗ under the Euclidean distance. The following gives one such suﬃcient
condition for gs.
Deﬁnition 2. A vector gs at the sth iteration is (α, β, γs)-correlated with a desired solution z∗ if

gs, zs

h

z∗

−

i ≥

zs

α
k

z∗

2 + β
k

gs
k

2
k

−

−

γs.

We know from convex optimization that if f is 2α-strongly convex and 1/2β-smooth, and gs is
∇zf (z), then gs is (α, β, 0)-correlated with z∗. In our setting, the desired
chosen as the gradient
solution corresponds to A∗, the ground-truth synthesis matrix. In Arora et al. (2015), it is shown
that gs = Ey[(Asx
y)sgn(x)T ], where x = thresholdC/2((As)T y) indeed satisﬁes Deﬁnition 2. This
gs is a population quantity and not explicitly available, but one can estimate such gs using an
gs is a random variable, so we also need a related
empirical average. The corresponding estimator
correlated-with-high-probability condition:

−

Deﬁnition 3. A direction
tion z∗ if, w.h.p.,

gs at the sth iteration is (α, β, γs)-correlated-w.h.p. with a desired solu-

gs, zs

b
h

z∗

−

i ≥

zs

α
k

z∗

2 + β
k

gs
k

2
k

−

−

γs.

From Deﬁnition 2, one can establish a form of descent property in each update step, as shown

b

b

in Theorem 1.

b

8

Theorem 1. Suppose that gs satisﬁes the condition described in Deﬁnition 2 for s = 1, 2, . . . , T .
Moreover, 0 < η

s=1 γs. Then, the following holds for all s:

2β and γ = maxT

≤

zs+1

k

z∗

2
k

−

(1

2αη)
k

−

≤

zs

z∗

2 + 2ηγs.
k

−

In particular, the above update converges geometrically to z∗ with an error γ/α. That is,

−
We can obtain a similar result for Deﬁnition 3 except that

−

≤

−

k

zs+1
k

2

z∗

(1

2αη)s

z0
k

z∗

2 + 2γ/α.
k

expectation.

zs+1

k

z∗k

−

2 is replaced with its

Armed with the above tools, we now state some informal versions of our main results:

Theorem 2 (Provably correct initialization, informal). There exists a neurally plausible algorithm
to produce an initial estimate A0 that has the correct support and is (δ, 2)-near to A∗ with high
probability.
O(mr) respectively. This
algorithm works when the sparsity level satisﬁes r = O∗(log n).

Its running time and sample complexity are

O(mnp) and

e

e

Our algorithm can be regarded as an extension of Arora et al. (2015) to the double-sparse
setting. It reconstructs the support of one single column and then estimates its direction in the
subspace deﬁned by the support. Our proposed algorithm enjoys neural plausibility by implement-
ing a thresholding non-linearity and Oja’s update rule. We provide a neural implementation of our
algorithm in Appendix G. The adaption to the sparse structure results in a strict improvement
upon the original algorithm both in running time and sample complexity. However, our algorithm
is limited to the sparsity level r = O∗(log n), which is rather small but plausible from the modeling
standpoint. For comparison, we analyze a natural extension of the algorithm of Arora et al. (2015)
with an extra hard-thresholding step for every learned atom. We obtain the same order restriction
on r, but somewhat worse bounds on sample complexity and running time. The details are found
in Appendix F.

We hypothesize that a stronger incoherence assumption can lead to provably correct initial-
ization for a much wider range of r. For purposes of theoretical analysis, we consider the special
case of a perfectly incoherent synthesis matrix A∗ such that µ = 0 and m = n. In this case, we
, which is an exponential
can indeed improve the sparsity parameter to r = O∗
improvement. This analysis is given in Appendix E.

min( √n
log2 n

n
k2 log2 n

)

,

(cid:0)

(cid:1)

Theorem 3 (Provably correct descent, informal). There exists a neurally plausible algorithm for
double-sparse coding that converges to A∗ with geometric rate when the initial estimate A0 has the
correct support and (δ, 2)-near to A∗. The running time per iteration is O(mkp + mrp) and the
sample complexity is

O(m + σ2
ε

mnr
k ).

e

Similar to Arora et al. (2015), our proposed algorithm enjoys neural plausibility. Moreover, we
can achieve a better running time and sample complexity per iteration than previous methods,
particularly in the noisy case. We show in Appendix F that in this regime the sample complexity
O(m + σ2
1/2, the sample complexity
of Arora et al. (2015) is
ε
O(m) in the noiseless case. In contrast, our proposed method
bound is signiﬁcantly worse than
leverages the sparse structure to overcome this problem and obtain improved results.

k ). For instance, when σε ≍

n−

mn2

e

We are now ready to introduce our methods in detail. As discussed above, our approach consists
of two stages: an initialization algorithm that produces a coarse estimate of A∗, and a descent-style
algorithm that reﬁnes this estimate to accurately recover A∗.

e

9

Algorithm 1 Truncated Pairwise Reweighting

∅

Initialize L =
Randomly divide p samples into two disjoint sets
While
P1 at random

< m. Pick u and v from

For every l = 1, 2, . . . , n; compute

L
|

|

P1 and

P2 of sizes p1 and p2 respectively

p2

el =

1
p2

y(i), u
h

ih

y(i), v

(y(i)

l )2

i

e(r′) < O∗(r/ log2 n)

Xi=1
b
en) in descending order

e2, . . . ,

e(r′+1)/

e(r′) ≥
b

Sort (
e1,
If r′ ≤
O(k/mr) and
r s.t
R be set of the r largest entries of
Let
b
b
(y(i)
y(i)
Mu,v = 1
y(i), v
b
b
b
b
b
p2
i
R
R
b
Mu,v
top singular values of
δ1, δ2 ←
c
z bR ←
If δ1 ≥

top singular vector of

Ω(k/m) and δ2 < O∗(k/m log n)

Mu,v
c

p2
i=1h

y(i), u

e
)T
b

P

c

ih

If dist(

z, l) > 1/ log n for any l

L

∈

±

Update L = L

z
∪ {

}

Return A0 = (L1, . . . , Lm)

3. Stage 1: Initialization

In this section, we present a neurally plausible algorithm that can produce a coarse initial estimate
of the ground truth A∗. We give a neural implementation of the algorithm in Appendix G.

Our algorithm is an adaptation from the algorithm in Arora et al. (2015). The idea is to estimate
dictionary atoms in a greedy fashion by iteratively re-weighting the given samples. The samples
are re-scaled in a way that the weighted (sample) covariance matrix has the dominant ﬁrst singular
value, and its corresponding eigenvector is close to one particular atom with high probability.
However, while this algorithm is conceptually very appealing, it incurs severe computational costs
O(mn2p) in expectation, which is unrealistic
in practice. More precisely, the overall running time is
for large-scale problems.

e

To overcome this burden, we leverage the double-sparsity assumption in our generative model to
obtain a more eﬃcient approach. The high-level idea is to ﬁrst estimate the support of each column
in the synthesis matrix A∗, and then obtain a coarse estimate of the nonzero coeﬃcients of each
column based on knowledge of its support. The key ingredient of our method is a novel spectral
procedure that gives us an estimate of the column supports purely from the observed samples. The
full algorithm, that we call Truncated Pairwise Reweighting, is listed in pseudocode form below as
Algorithm 1.

Let us provide some intuition of our algorithm. Fix a sample y = A∗x∗ + ε from the available

training set, and consider samples

u = A∗α + εu, v = A∗α′ + εv.

10

Now, consider the (very coarse) estimate for the sparse code of u with respect to A∗:

β = A∗

T u = A∗

T A∗α + A∗

T εu.

y, u
h

x∗, β

x∗, α
i

.

i ≈ h

i ≈ h

As long as A∗ is incoherent enough and εu is small, the estimate β behaves just like α, in the sense
that for each sample y:

Moreover, the above inner products are large only if α and x∗ share some elements in their supports;
depends on whether or not x∗
else, they are likely to be small. Likewise, the weight
shares the support with both α and α′.

y, u
h

y, v

ih

i

i

ih

y, v

y, u
h

Now, suppose that we have a mechanism to isolate pairs u and v who share exactly one atom
among their sparse representations. Then by scaling each sample y with an increasing function
of
and linearly adding the samples, we magnify the importance of the samples that
are aligned with that atom, and diminish the rest. The ﬁnal direction can be obtained via the
top principal component of the reweighted samples and hence can be used as a coarse estimate of
the atom. This is exactly the approach adopted in Arora et al. (2015). However, in our double-
sparse coding setting, we know that the estimated atom should be sparse as well. Therefore, we
can naturally perform an extra “sparsiﬁcation” step of the output. An extended algorithm and
its correctness are provided in Appendix F. However, as we discussed above, the computational
complexity of the re-weighting step still remains.

We overcome this obstacle by ﬁrst identifying the locations of the nonzero entries in each atom.

Speciﬁcally, deﬁne the matrix:

Mu,v =

p2

1
p2

y(i), u
h
Xi=1

ih

y(i), v

y(i)

y(i).

i

◦

Then, the diagonal entries of Mu,v reveals the support of the atom of A∗ shared among u and v: the
r-largest entries of Mu,v will correspond to the support we seek. Since the desired direction remains
unchanged in the r-dimensional subspace of its nonzero elements, we can restrict our attention to
Mu,v, and proceed as before. This truncation
this subspace, construct a reduced covariance matrix
O(mnp),
step alleviates the computational burden by a signiﬁcant amount; the running time is now
which improves the original by a factor of n.

c

The success of the above procedure relies upon whether or not we can isolate pairs u and v that
share one dictionary atom. Fortunately, this can be done via checking the decay of the singular
values of the (reduced) covariance matrix. Here too, we show via our analysis that the truncation
step plays an important role. Overall, our proposed algorithm not only accelerates the initialization
in terms of running time, but also improves the sample complexity over Arora et al. (2015). The
performance of Algorithm 1 is described in the following theorem, whose formal proof is deferred
to Appendix B.

e

√n
k log3 n

Theorem 4. Suppose that Assumptions B1-B4 hold and Assumptions A1-A3 satify with µ =
Ω(mr), then with high probability
O∗
Algorithm 1 returns an initial estimate A0 whose columns share the same support as A∗ and with
(δ, 2)-nearness to A∗ with δ = O∗(1/ log n).

and r = O∗(log n). When p1 =

Ω(m) and p2 =

e

e

(cid:1)

(cid:0)

The limit on r arises from the minimum non-zero coeﬃcient τ of A∗. Since the columns of A∗
are standardized, τ should degenerate as r grows. In other words, it is getting harder to distinguish

11

the “signal” coeﬃcients from zero as r grows with n. However, this limitation can be relaxed when
a better incoherence available, for example the orthonormal case. We study this in Appendix E.

To provide some intuition about the working of the algorithm (and its proof), let us analyze it in
the case where we have access to inﬁnite number of samples. This setting, of course, is unrealistic.
However, the analysis is much simpler and more transparent since we can focus on expected values
rather than empirical averages. Moreover, the analysis reveals several key lemmas, which we will
reuse extensively for proving Theorem 4. First, we give some intuition behind the deﬁnition of the
“scores”,

el.

Lemma 1. Fix samples u and v and suppose that y = A∗x∗ + ε is a random sample independent
of u, v. The expected value of the score for the lth component of y is given by:

b

el , E[
h

y, u

y, v

ih

i

y2
l ] =

2
li + perturbation terms
qiciβiβ′iA∗

where qi = P[i
absolute value at most O∗(k/m log n).

S], qij = P[i, j

∈

∈

V
U
Xi
∩
∈
S] and ci = E[x4
i
i |

∈

S]. Moreover, the perturbation terms have

i

From Assumption B1, we know that qi = Θ(k/m), qij = Θ(k2/m2) and ci = Θ(1). Besides,
= o(1) for i /
U . Consider the ﬁrst
∈
or that l does not belong to support
V =
τ .

= Ω(1) for i
we will show later that
βi| ≈ |
|
2
li . Clearly, E0 = 0 if U
V qiciβiβ′iA∗
term E0 =
U
∩
∈
of any atom in U
Ω(τ 2k/m) = Ω(k/mr) since

αi|
∈
V . On the contrary, as E0 6
qiciβiβ′i| ≥
|

βi|
|
V =
∩
∅
= 0 and U
∩
A∗li| ≥
|

Therefore, Lemma 1 suggests that if u and v share a unique atom among their sparse represen-
tations, and r is not too large, then we can indeed recover the correct support of the shared atom.
When this is the case, the expected scores corresponding to the nonzero elements of the shared
atom will dominate the remaining of the scores.

2
qiciβiβ′iA∗
li | ≥
|

, then E0 =

Ω(k/m) and

U , and

i
}
{

P

∩

Now, given that we can isolate the support R of the corresponding atom, the remaining questions
are how best we can estimate its non-zero coeﬃcients, and when u and v share a unique elements
in their supports. These issues are handled in the following lemmas.

Lemma 2. Suppose that u = A∗α + εu and v = A∗α′ + εv are two random samples. Let U and
V denote the supports of α and α′ respectively. R is the support of some atom of interest. The
truncated re-weighting matrix is formulated as

Mu,v , E[
y, u
h

ih

y, v

yRyT

R] =

i

T
R,i + perturbation terms
qiciβiβ′iA∗R,iA∗

V
U
Xi
∩
∈

where the perturbation terms have norms at most O∗(k/m log n).

Using the same argument for bounding E0 in Lemma 1, we can see that M0 , qiciβiβ′iA∗R,iA∗
T
R,i
has norm at least Ω(k/m) when u and v share a unique element i (
= 1). According to this
k
lemma, the spectral norm of M0 dominates those of the other perturbation terms. Thus, given R
i.
we can use the ﬁrst singular vector of Mu,v as an estimate of A∗
•

A∗R,ik

Lemma 3. Under the setup of Theorem 4, suppose u = A∗α + εu and v = A∗α′ + εv are two
random samples with supports U and V respectively. R = supp(A∗i ). If u and v share the unique
atom i, the ﬁrst r largest entries of el is at least O(k/mr) and belong to R. Moreover, the top
singular vector of Mu,v is δ-close to A∗R,i for O∗(1/ log n).

12

i’s support directly follows Lemma 1. For the latter part, recall from
Proof. The recovery of A∗
•
Lemma 2 that

T
R,i + perturbation terms
Mu,v = qiciβiβ′iA∗R,iA∗

The perturbation terms have norms bounded by O∗(k/m log n). On the other hand, the ﬁrst term
is has norm at least Ω(k/m) since
Ω(k/m).
Then using Wedin’s Theorem to Mu,v, we can conclude that the top singular vector must be
O∗(k/m log n)/Ω(k/m) = O∗(1/ log n) -close to A∗R,i.

= 1 for the correct support R and

qiciβiβ′i| ≥

A∗R,ik

k

|

Lemma 4. Under the setup of Theorem 4, suppose u = A∗α+εu and v = A∗α′ +εv are two random
samples with supports U and V respectively. If the top singular value of Mu,v is at least Ω(k/m) and
the second largest one is at most O∗(k/m log n), then u and v share a unique dictionary element
with high probability.

Proof. The proof follows from that of Lemma 37 in Arora et al. (2015). The main idea is to
separate the possible cases of how u and v share support and to use Lemma 2 with the bounded
perturbation terms to conclude when u and v share exactly one. We note that due to the condition
where
O∗(r/ log n), it must be the case that u and v share only
one atom or share more than one atoms with the same support. When their supports overlap more
than one, then the ﬁrst singular value cannot dominate the second one, and hence it must not be
b
the case.

Ω(k/mr) and

e(s) ≤

e(s) ≥

e(s+1)/

b

b

Similar to (Arora et al., 2015), our initialization algorithm requires

tation to estimate all the atoms, hence the expected running time is
Lemma 1 and 2 are deferred to Appendix B.

O(m) iterations in expec-
O(mnp). All the proofs of

e

e

4. Stage 2: Descent

We now adapt the neural sparse coding approach of Arora et al. (2015) to obtain an improved
estimate of A∗. As mentioned earlier, at a high level the algorithm is akin to performing approximate
gradient descent. The insight is that within a small enough neighborhood (in the sense of δ-
closeness) of the true A∗, an estimate of the ground-truth code vectors, X ∗, can be constructed
using a neurally plausible algorithm.

The innovation, in our case, is the double-sparsity model since we know a priori that A∗ is itself
sparse. Under suﬃciently many samples, the support of A∗ can be deduced from the initialization
stage; therefore we perform an extra projection step in each iteration of gradient descent. In this
sense, our method is non-trivially diﬀerent from Arora et al. (2015). The full algorithm is presented
as Algorithm 2.

As discussed in Section 2, convergence of noisy approximate gradient descent can be achieved
gs is correlated-w.h.p. with the true solution. However, an analogous convergence result
as long as
for projected gradient descent does not exist in the literature. We ﬁll this gap via a careful analysis.
gs (i.e., when it
Due to the projection, we only require the correlated-w.h.p. property for part of
is restricted to some support set) with A∗. The descent property is still achieved via Theorem 5.
(A, X); therefore, we can
Due to various perturbation terms,
∇AL
k/n). The performance
only reﬁne the estimate of A∗ until the column-wise error is of order O(
of Algorithm 2 can be characterized via the following theorem.
b

g is only a biased estimate of

b

b

p

13

Algorithm 2 Double-Sparse Coding Descent Algorithm

Initialize A0 is (δ, 2)-near to A∗. H = (hij )n
Repeat for s = 0, 1, . . . , T

×

m where hij = 1 if i

supp(A0
j) and 0 otherwise.
•

∈

Encode: x(i) = thresholdC/2((As)T y(i))
PH(As
Update: As+1 =
−
p
i=1(Asx(i)
where

gs = 1
p

η

η

gs)
gs) = As
−
y(i))sgn(x(i))T and
−
b
b

for i = 1, 2, . . . , p
PH (

PH (G) = H

◦

G

P

b

Theorem 5. Suppose that the initial estimate A0 has the correct column supports and is (δ, 2)-near
to A∗ with δ = O∗(1/ log n). If Algorithm 2 is provided with p =
Ω(mr) fresh samples at each step
and η = Θ(m/k), then

E[
As
i −
k
•

A∗
ik
•

2]

(1

ρ)s

A0
i −
•

A∗
ik
•

k
for some 0 < ρ < 1/2 and for s = 1, 2, . . . , T . Consequently, As converges to A∗ geometrically until
column-wise error O(

k/n).

p

≤

−

e
2 + O(

k/n)

p

We defer the full proof of Theorem 5 to Section D. In this section, we take a step towards
gs in the inﬁnite sample case, which is equivalent to its
y)sgn(x)T ]. We establish the (α, β, γs)-correlation of a truncated version

understanding the algorithm by analyzing
expectation gs , E[(Asx
of gs
i with A∗
i to obtain the descent in Theorem 6 for the inﬁnite sample case.
•
•

Theorem 6. Suppose that the initial estimate A0 has the correct column supports and is (δ, 2)-near
to A∗. If Algorithm 2 is provided with inﬁnite number of samples at each step and η = Θ(m/k),
then

−

b

ρ)
k
for some 0 < ρ < 1/2 and for s = 1, 2, . . . , T . Consequently, it converges to A∗ geometrically until
column-wise error is O(k/n).

A∗
ik
•

A∗
ik
•

(1

≤

−

k

(cid:0)

(cid:1)

As+1
i −
•

2 + O

k2/n2

As
i −
•

2

Note that the better error O(k2/n2) is due to the fact that inﬁnitely many samples are given.
k/n) in Theorem 5 is a trade-oﬀ between the accuracy and the sample complexity
The term O(
of the algorithm. The proof of this theorem composes of two steps with two main results: 1) an
explicit form of gs (Lemma 6); 2) (α, β, γs)-correlation of column-wise gs with A∗ (Lemma 6). The
proof of those lemmas are deferred to Appendix C. Since the correlation primarily relies on the
(δ, 2)-nearness of As to A∗ that is provided initially and maintained at each step, then we need to
argue that the nearness is preserved after each step.

p

Lemma 5. Suppose that the initial estimate A0 has the correct column supports and is (δ, 2)-near to
A∗. The column-wise update has the form gs
R,i = piqi(λs
ζ) where R = supp(As
i),
•
λs
i =

A∗R,i + ξs

i As

i ±

and

As
i, A∗
ii
h
•
•

i = As
ξs
R,

idiag(qij)(As
•−

R,i −
i)T A∗
i/qi.
•

−
Moreover, ξi has norm bounded by O(k/n) for δ = O∗(1/ log n) and ζ is negligible.

We underline that the correct support of As allows us to obtain the closed-form expression of
gs
i. Likewise, the expression (8) suggests that gs
Ri,i in terms of As
i is almost equal to
i and A∗
•
•
•
i) (since λs
piqi(As
i. With Lemma 5, we will
1), which directs to the desired solution A∗
A∗
i ≈
i −
•
•
•
i and the nearness
prove the (α, β, γs)-correlation of the approximate gradient to each column A∗
•
of each new update to the true solution A∗.

14

4.1 (α, β, γs)-Correlation
Lemma 6. Suppose that As to be (δ, 2)-near to A∗ and R = supp(A∗
i), then 2gs
•
correlated with A∗R,i; that is

R,i is (α, 1/2α, ǫ2/α)-

A∗R,ii ≥
Futhermore, the descent is achieved by

R,i −

R,i, As

2gs
h

As
α
k

R,i −

A∗R,ik

2 + 1/(2α)

2
gs
R,ik
k

−

ǫ2/α

As+1
i −
k
•

2

(1

≤

−

2αη)s

A0
i −
k
•

A∗
ik
•

2 + ηǫ2

s/α

where δ = O∗(1/ log n) and ǫ = O

.

A∗
ik
•
k2
mn

(cid:0)

(cid:1)

Proof. Throughout the proof, we omit the superscript s for simplicity and denote 2α = piqi. First,
i as a combination of the true direction As
we rewrite gs
i −
•
•

i and a term with small norm:
A∗
•

gR,i = 2α(AR,i −

A∗R,i) + v,

i + ǫi] with norm bounded. In fact, since A
where v = 2α[(λi −
i, and both
i is δ-close to A∗
1)A
•
•
•
A
2α(λi −
have unit norm, then
A∗
A
O(k/n) from
i −
i −
ik
•
•
•
the inequality (9). Therefore,

1)A
ik
•

ξik ≤

= α
k

A∗
ik
•

α
k

and

≤

k

k

2

v
k

k

=

2α(λi −

1)AR,i + 2αξik ≤

k

α
k

AR,i −

A∗R,ik

+ ǫ

where ǫ = O(k2/mn). Now, we make use of (2) to show the ﬁrst part of Lemma 6:

2gR,i, AR,i −

h

A∗R,ii

= 4α
k

AR,i −

A∗R,ik

2v, AR,i −

h

.
A∗R,ii

2 +

We want to lower bound the inner product term with respect to
Eﬀectively, from (2)

2 and

gRi,ik

k

AR,i −
k

2.
A∗R,ik

4α
h

v, A
i −
•

A∗
ii
•

=

2

2

gR,ik
k
gR,ik

−

−

4α2
6α2

AR,i −
AR,i −

k

k

≥ k

2

v
k
− k
2ǫ2,

−

2

2

A∗R,ik
A∗R,ik
2(α2

2

where the last step is due to Cauchy-Schwarz inequality:

v
k
in (3) for the right hand side of (4), we get the ﬁrst result:

AR,i −
k

A∗R,ik

≤

k

2 + ǫ2).

Substitute 2
h

v, A
i −
•

A∗
ii
•

2gR,i, AR,i −
h

A∗R,ii ≥

α
k

AR,i −

A∗R,ik

2 +

1
2α k

2

gR,ik

−

ǫ2
α

.

(2)

(3)

(4)

The second part is directly followed from Theorem 1. Moreover, we have pi = Θ(k/m) and qi =
Θ(1), then α = Θ(k/m), β = Θ(m/k) and γs = O(k3/mn2). Then gs
correlated with the true solution A∗R,i.

R,i is (Ω(k/m), Ω(m/k), O(k3/mn2))-

Proof of Theorem 6. The descent in Theorem 6 directly follows from the above lemma. Next, we
will establish the nearness for the update at step s:

15

2,000
Sample size

4,000

2,000
Sample size

4,000

2,000
Sample size

4,000

e
t
a
r

y
r
e
v
o
c
e
R

e
t
a
r

y
r
e
v
o
c
e
R

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

r
o
r
r
e

n
o
i
t
c
u
r
t
s
n
o
c
e
R

r
o
r
r
e

n
o
i
t
c
u
r
t
s
n
o
c
e
R

8

6

4

2

0

8

6

4

2

0

0

0

Ours
Arora
Arora+HT
Trainlets

Ours
Arora
Arora+HT
Trainlets

e
m

i
t

g
n

i

n
n
u
R

e
m

i
t

g
n

i

n
n
u
R

4

3

2

1

0

6

4

2

0

0

0

2,000
Sample size

4,000

2,000
Sample size

4,000

2,000
Sample size

4,000

Figure 1: (top) The performance of four methods on three metrics (recovery rate, reconstruction error and
running time) in sample size in the noiseless case. (bottom) The same metrics are measured for
the noisy case.

4.2 Nearness

Lemma 7. Suppose that As is (δ, 2)-near to A∗, then

As+1

k

A∗k ≤

2
k

A∗k

−

Proof. From Lemma 5 we have gs
i = piqi(λiAs
i) + A
A∗
i −
•
•
•
idiag(qij)AT
R, then it is obvious that gs
iA∗
[n]
¯R,i = A ¯R,
i ±
\
•−
•
follows the proof of Lemma 24 in (Arora et al., 2015) for the nearness with full gs = gs
ﬁnish the proof for this lemma.

ζ. Denote ¯R =
idiag(qij)AT
iA∗
i ±
•−
•
•−
ζ is bounded by O(k2/m2). Then we
¯R,i to

R,i + gs

−

In sum, we have shown the descent property of Algorithm 2 in the inﬁnite sample case. The
gs around its mean to the sample complexity is provided in Section D.

study of the concentration of
In the next section, we corroborate our theory by some numerical results on synthetic data.

5. Empirical Study

b

We compare our method with three diﬀerent methods for both standard sparse and double-sparse
coding. For the standard approach, we implement the algorithm proposed in Arora et al. (2015),
which currently is the best theoretically sound method for provable sparse coding. However, since
their method does not explicitly leverage the double-sparsity model, we also implement a heuristic
modiﬁcation that performs a hard thresholding (HT)-based post-processing step in the initializa-
tion and learning procedures (which we dub Arora + HT ). The ﬁnal comparison is the Trainlets
approach of Sulam et al. (2016).

16

2 block is of form [1 1; 1

We generate a synthetic training dataset according to the model described in Section 2. The
base dictionary Φ is the identity matrix of size n = 64 and the square synthesis matrix A∗ is a
block diagonal matrix with 32 blocks. Each 2
1] (i.e., the column
×
sparsity r = 2) . The support of x∗ is drawn uniformly over all 6-dimensional subsets of [m],
In our simulations
and the nonzero coeﬃcients are randomly set to
with noise, we add Gaussian noise ε with entrywise variance σ2
ε = 0.01 to each of those above
samples. For all the approaches except Trainlets, we use T = 2000 iterations for the initialization
procedure, and set the number of steps in the descent stage to 25. Since Trainlets does not have
a speciﬁed initialization procedure, we initialize it with a random Gaussian matrix upon which
column-wise sparse thresholding is then performed. The learning step of Trainlets2 is executed for
50 iterations, which tolerates its initialization deﬁciency. For each Monte Carlo trial, we uniformly
draw p samples, feed these samples to the four diﬀerent algorithms, and observe their ability to
reconstruct A∗. Matlab implementation of our algorithms is available online3.

1 with equal probability.

−

±

We evaluate these approaches on three metrics as a function of the number of available samples:
(i) fraction of trials in which each algorithm successfully recovers the ground truth A∗; (ii) recon-
struction error; and (iii) running time. The synthesis matrix is said to be “successfully recovered”
if the Frobenius norm of the diﬀerence between the estimate
A and the ground truth A∗ is smaller
4 in the noiseless case, and to 0.5 in the other. All three
than a threshold which is set to 10−
metrics are averaged over 100 Monte Carlo simulations. As discussed above, the Frobenius norm is
only meaningful under a suitable permutation and sign ﬂip transformation linking
A and A∗. We
estimate this transformation using a simple maximum weight matching algorithm. Speciﬁcally, we
construct a weighted bipartite graph with nodes representing columns of A∗ and
A and adjacency
is taken element-wise. We compute the optimal matching
matrix deﬁned as G =
using the Hungarian algorithm, and then estimate the sign ﬂips by looking at the sign of the inner
products between the matched columns.

, where
A
|

T
A∗
|

|·|

b

b

b

b

The results of our experiments are shown in Figure 1 with the top and bottom rows respectively
for the noiseless and noisy cases. The two leftmost ﬁgures suggest that all algorithms exhibit a
“phase transitions” in sample complexity that occurs in the range of 500-2000 samples.
In the
noiseless case, our method achieves the phase transition with the fewest number of samples. In the
noisy case, our method nearly matches the best sample complexity performance (next to Trainlets,
which is a heuristic and computationally expensive). Our method achieves the best performance
in terms of (wall-clock) running time in all cases.

6. Conclusion

In this paper, we have addressed an open theoretical question on learning sparse dictionaries under
a special type of generative model. Our proposed algorithm consists of a novel initialization step
followed by a descent-style step, both are able to take advantage of the sparse structure. We rigor-
ously demonstrate its eﬃcacy in both sample- and computation-complexity over existing heuristics
as well as provable approaches for double-sparse and regular sparse coding. This results in the ﬁrst
known provable approach for double-sparse coding problem with statistical and algorithmic guaran-
tees. Besides, we also show three beneﬁts of our approach: neural plausibility, robustness to noise
and practical usefulness via the numerical experiments.

2. We utilize Trainlets’s implementation provided at http://jsulam.cswp.cs.technion.ac.il/home/software/.
3. https://github.com/thanh-isu/double-sparse-coding

17

Nevertheless, several fundamental questions regarding our approach remain. First, our initial-
ization method (in the overcomplete case) achieves its theoretical guarantees under fairly stringent
limitations on the sparsity level r. This arises due to our reweighted spectral initialization strategy,
and it is an open question whether a better initialization strategy exists (or whether these types
of initialization are required at all). Second, our analysis holds for complete (ﬁxed) bases Φ, and
it remains open to study the setting where Φ is over-complete. Finally, understanding the reasons
behind the very promising practical performance of methods based on heuristics, such as Trainlets,
on real-world data remains a very challenging open problem.

18

Appendix A. Auxiliary Lemma

Claim 1 (Maximal row ℓ1-norm). Given that
Θ(

m/n).

A∗k

k

p

Proof. Recall the deﬁnition of the operator norm:

2
F = m and

A∗k
k

= O(

m/n), then

T
A∗

k

k1,2 =

p

T
A∗
k

k1,2 = sup

=0

x

AT x
k
x

k

k

sup
=0
x

AT x
k
k
x
k1 ≤
k
A∗kF /√n =

k

=

T
A∗

k

k

= O(

m/n).

p

2
F = m,

T
A∗

k
m/n).

k1,2 ≥ k

Since
T
A∗
k

A∗k
k
k1,2 = Θ(
p
Along with Assumptions A1 and A3, the above claim implies the number of nonzero entries
in each row is O(r). This Claim is an important ingredient in our analysis of our initialization
algorithm shown in Section 3.

m/n. Combining with the above, we have

p

Appendix B. Analysis of Initialization Algorithm

B.1 Proof of Lemma 1

The proof of Lemma 1 can be divided into three steps: 1) we ﬁrst establish useful properties of β
with respect to α; 2) we then explicitly derive el in terms of the generative model parameters and β;
and 3) we ﬁnally bound the error terms in E based on the ﬁrst result and appropriate assumptions.

Claim 2. In the generative model,

x∗k ≤
k

O(√k) and

ε
k ≤

k

O(σε√n) with high probability.

Proof. The claim directly follows from the fact that x∗ is a k-sparse random vector whose nonzero
entries are independent sub-Gaussian with variance 1. Meanwhile, ε has n independent Gaussian
entries of variance σ2
ε .

e

e

Despite its simplicity, this claim will be used in many proofs throughout the paper. Note also
that in this section we will calculate the expectation over y and often refer probabilistic bounds
(w.h.p.) under the randomness of u and v.

Claim 3. Suppose that u = A∗α + εu is a random sample and U = supp(α). Let β = A∗
O(√k + σε√n).
w.h.p., we have (a)

√n + σε log n for each i and (b)

µk log n

βi −
|

αi| ≤

β
k

k ≤

T u, then,

Proof. The proof mostly follows from Claim 36 of Arora et al. (2015), with an additional consider-
and observe that
ation of the error εu. Write W = U

e

=

αi|

βi −
|

i εu| ≤ |h
A∗
•
T
k/n. Moreover, αW has k
µ
Since A∗ is µ-incoherence, then
i A∗
A∗
W k ≤
•
•
T
Gaussian entries of variance 1, therefore
i, αW i| ≤
W A∗
A∗
p
|h
•
•
that εu has independent Gaussian entries of variance σ2
variance (
k
µk log n

1 independent sub-
√n with high probability. Also recall
T
ε , then A∗
i εu is Gaussian with the same
•
αi| ≤

σε log n with high probability. Consequently,

= 1). Hence

i, αW i|

T
i ε
A∗
|
•

i, εui|

A∗
ik
•

βi −

µk log n

| ≤

+

−

|h

k

|

T
W A∗
A∗
•
•

√n + σε log n, which is the ﬁrst part of the claim.

Next, in order to bound

, we express β as

β
k
k
T A∗
U αU + A∗
•

β

k

k

=

A∗
k

T εuk ≤ k

A∗

A∗
U kk
•

αU k

kk

+

A∗
k

εuk

kk

i
\{
}
T
T
W αW + A∗
i A∗
A∗
|
•
•
•

19

O(√k) and

A∗k ≤

αU k ≤
k

O(1) , we complete the proof for the second part.

Using Claim 2 to get
εuk ≤
k
A∗
U k ≤ k
k
•
Claim 3 suggests that the diﬀerence between βi and αi is bounded above by O∗(1/ log2 n)
U and
o(1)
∈
O(√k) w.h.p.

w.h.p. if µ = O∗( √n
βi| ≤
|
We will use these results multiple times in the next few proofs.

O∗(1/ log2 n) otherwise. On the other hand, under Assumption B4,

O(σε√n) w.h.p., and further noticing that

). Therefore, w.h.p., C

O(log m) for i

βi| ≤ |

+ o(1)

k log3 n

αi|

β
k

k ≤

≤ |

−

≤

e

e

Proof of Lemma 1. We decompose dl into small parts so that the stochastic model

is made use.

y, v

el = E[
y, u
h
ih
= E
x∗, β
h
ih
= E1 + E2 +
(cid:2)(cid:8)

i
+ E9

· · ·

l ] = E[
y2
A∗x∗ + ε, u
A∗l
(
h
h
·
T (βvT + β′uT )ε + uT εεT v
+ x∗

A∗x∗ + ε, v

i
x∗, β′

ih

i

, x∗

+ ε)2]

i
, x∗

A∗l
h
•

2 + 2
h

A∗l
•

i

, x∗

εl + εl

i

(cid:9)(cid:8)

(cid:9)(cid:3)

where the terms are

e

D

i

ih
, x∗

2]
, x∗
i
εl]
, x∗

A∗l
•
A∗l
ih
•
ε2
l ]
i
T (βvT + β′uT )ε
T (βvT + β′uT )εεl
(cid:3)

x∗, β′
ih
x∗, β′

E1 = E[
x∗, β
ih
h
E2 = 2E[
x∗, β
h
ih
E3 = E[
x∗, β
x∗, β′
h
2x∗
E4 = E
A∗l
i
h
·
E5 = E
x∗
A∗l
(cid:2)
i
h
·
E6 = E
(βvT + β′uT )εε2
(cid:2)
l
2]
E7 = E[uT εεT v
A∗l
(cid:2)
(cid:3)
h
•
E8 = 2E[uT εεT v
εl]
A∗l
h
i
•
E9 = E[uT εεT vε2
l ]

i
, x∗

, x∗

, x∗

(cid:3)

(5)

Because x∗ and ε are independent and zero-mean, E2 and E4 are clearly zero. Moreover,

E6 = (βvT + β′uT )E[εε2

l ] = 0

due to the fact that E[εjε2

l ] = 0, for j

= l, and E[ε3

l ] = 0. Also,

We bound the remaining terms separately in the following claims.

T
E8 = A∗
l
•

E[x∗]E

uT εεT vεl

= 0.

(cid:2)

(cid:3)

Claim 4. In the decomposition (5), E1 is of the form

E1 =

2
li +
qiciβiβ′iA∗

2
li +
qiciβiβ′iA∗

2
lj + 2βiβ′jA∗liA∗lj)
qij(βiβ′iA∗

V
U
Xi
∩
∈

V
U
Xi /
∩
∈

=i
Xj

where all those terms except

li have magnitude at most O∗(k/m log2 n) w.h.p.
2
V qiciβiβ′iA∗
∩

i

U

∈

P

20

Proof. Using the generative model in Assumptions B1-B4, we have

E1 = E[
x∗, β
h
= ES
E

x∗

ih
S[

x∗, β′

A∗l
•

ih
βix∗i

, x∗

2]

i
β′ix∗i

A∗lix∗i

2]

|

(cid:2)

S
Xi
∈
2
li +
qiciβiβ′iA∗

S
Xi
∈

S

(cid:0) Xi

(cid:1)
∈
2
lj + 2βiβ′jA∗liA∗lj)
qij(βiβ′iA∗

(cid:3)

=

=

[m]
Xi
∈

[m],j
Xi,j
∈

=i

2
li +
qiciβiβ′iA∗

2
li +
qiciβiβ′iA∗

2
lj + 2βiβ′jA∗liA∗lj),
qij(βiβ′iA∗

Xi /
V
U
∩
∈
S], qij = P[i, j

V
U
Xi
∈
∩
where we have used the qi = P[i
S] and Assumptions
B1-B4. We now prove that the last three terms are upper bounded by O∗(k/m log n). The key
observation is that all these terms typically involve a quadratic form of the l-th row A∗l
whose
•
norm is bounded by O(1) (by Claim 1 and Assumption A4). Moreover,
is relatively small for
V )
(U
[m]
U
i /
\
∈
to bound

V while qij = Θ(k2/m2). For the second term, we apply the Claim 3 for i

Xj
=i
S] and ci = E[x4
i
i |

= 0, then with high probability

βiβ′i|
|

∈

∈

∈

∩

∩

∈

|

βiβ′i|

. Assume αi = 0 and α′i 6
βiβ′i| ≤ |
|
Using the bound qici = Θ(k/m), we have w.h.p.,

αi)(β′i −

(βi −

α′i)
|

+

βiα′i| ≤
|

O∗(1/ log n)

2
qiciβiβ′iA∗
li

max
i

qiciβiβ′i|
|

≤

2
A∗
li ≤

max
i

qiciβiβ′i|k
|

A∗

2
1,2 ≤

k

O∗(k/m log n).

V
U
Xi /
(cid:12)
∩
∈
(cid:12)
(cid:12)
For the third term, we make use of the bounds on
β′k ≤

V
U
Xi /
∩
∈

(cid:12)
(cid:12)
(cid:12)

β
k

kk

O(k) w.h.p., and on qij = Θ(k2/m2). More precisely, w.h.p.,

β
k

k

and

β′k
k

from the previous claim where

=i
Xj
(cid:12)
(cid:12)
(cid:12)

2
qijβiβ′iA∗
lj

e

=

βiβ′i

2
qijA∗
lj

(cid:12)
(cid:12)
(cid:12)

≤

Xi
(cid:12)
(cid:12)
(cid:12)
(max
=j
i

=i
Xj

Xi

qij)

βiβ′i|
|

|
Xi
2
A∗
lj

≤

(cid:12)
(cid:12)
(cid:12)
(cid:16)Xj

βiβ′i|

≤

(cid:17)

=i

(cid:0)Xj
(max
=j
i

2
qijA∗
lj

(cid:1)
β

qij)
k

β′

A∗

kk

kk

2
1,2 ≤

k

O(k3/m2),

e

where the second last inequality follows from the Cauchy-Schwarz inequality. For the last term, we
= j and
write it in a matrix form as
(Qβ)ij = 0 for i = j. Then

T
=i qijβiβ′jA∗liA∗lj = A∗
l
•

where (Qβ)ij = qijβiβ′j for i

QβA∗l
•

j

where
Qβk
k
mately,

2
F =

=j q2

i

(maxi

=j q2
ij)
k

β

2
k

β′k
k

≤

2. Ulti-

P
T
A∗
l
|
•
i (β′j )2
ijβ2

QβA∗l

•| ≤ k

(maxi

≤

A∗l

Qβkk
=j q2
ij)

2

•k

≤ k
i β2
i

QβkF k
j(β′j)2

A∗

2
1,2,

k

qijβiβ′jA∗liA∗lj

(max
=j
i

≤

P
β
qij)
k

kk

β′

A∗

P
kk

2
1,2 ≤
k

O(k3/m2).

P

=i
Xj
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Under Assumption k = O∗( √n
O∗(k/m log2 n). As a result, the two terms
above are bounded by the same amount O∗(k/m log n) w.h.p., so we complete the proof of the
claim.

log n ), then

O(k3/m2)

≤

e

e

21

Claim 5. In the decomposition (5),

E5|
|
S] = 1 and qi = P[i

,
E3|
|

,

Proof. Recall that E[x2
i |
E3 = E[
h

∈
l ] = σ2
ε2
ε
i

x∗, β

x∗, β′

ih

E7|
|

and

E9|

|

is at most O∗(k/m log2 n).

S] = Θ(k/m) for S = supp(x∗), then

ES

E

x∗

S[

|

βiβ′jx∗i x∗j ]

S
Xi,j
∈

(cid:3)

(cid:2)
σ2
ε qiβiβ′i

= σ2
ε

ES[

βiβ′i] =

S
Xi
∈

Xi

Denote Q = diag(q1, q2, . . . , qm), then
O(k3/mn) where we have used
β
k
handle the seventh term before E5:
e

k ≤

E7 = E[uT εεT v

, x∗

A∗l
•

h

2] = E[
h

A∗l
•

i

=

Q

σ2
σ2
ε k2/m) =
Qβ, β′i| ≤
E3|
ε k
ε h
|
|
O(√k) w.h.p. and σε ≤
O(1/√n). For convenience, we
e
, x∗

β′k ≤

O(σ2

u, v

u, v

kk

qiA2

QAl

kk

β

e

2]uT E[εεT ]v =
i

li = σ2
ε h

AT
l
•

i

σ2
ε h

i

•

Xi

O(√k) w.h.p. Consequently,

To bound this term, we use Claim 9 in Appendix D to have
2
u, v
h
•k
O(m/n) and σε ≤
e

σ2
ε k
O(1/√n). Now, the ﬁrth term E5 is expressed as follows

u
k
k
u, v
|h

E7| ≤
|

A∗α + εuk ≤
k
O(k2/mn) because
i| ≤

i ≤

Al

kk

Q

=

e

e

O(√k) w.h.p. and

Al

2
•k

k

≤

, x∗

x∗
i
T
x∗x∗

E5 = E
A∗l
h
·
E
T
= A∗
(cid:2)
l
•
T
= σ2
Q(vlβ + ulβ′)
ε A∗
(cid:3)
(cid:2)
l
•

T (βvT + β′uT )εεl
(βvT + β′uT )E[εεl]
(cid:3)

Observe that
β
2
k ≤
kk
k
bounded by

u

The last term

σ2
ε k

Q(vlβ + ulβ′)
k ≤
u
k ≤
k

T
A∗
E5| ≤
l
• kk
|
O(k) w.h.p. using the result
O(k2/mn).
e
e

e

T
σ2
Q
A∗
ε k
l
• kk
O(k) and

vlβ + ulβ′k
kk
β
k ≤
k

and that

vlβ + ulβ′k ≤
k
O(k) from Claim 3, then E5

E9 = E[uT εεT vε2

l ] = uT E

εεT ε2
l

v = 9σ4
ε h

because the independent entries of ε and E[ε4
Since m = O(n) and k
and

O(k2/n2).
E7|
,
E5|
|
|
e
E9|
|
Combining the bounds from Claim 4, 5 for every single term in (5), we ﬁnish the proof for
Lemma 1.

v
kk
log n ), we obtain the same bound O∗(k/m log2 n) for

, and conclude the proof of the claim.

k ≤
,
E3|
|

O∗( √n

9σ4
ε k

≤

u

|

(cid:2)
l ] = 9σ4

(cid:3)
ε . Therefore,

e

u, v

i
E9| ≤

B.2 Proof of Lemma 2

We prove this lemma by using the same strategy used to prove Lemma 1.

yRyT
R]
A∗x∗ + ε, v

i

y, v

Mu,v , E[
y, u
h
ih
= E[
A∗x∗ + ε, u
h
= E
h
= M1 +
(cid:2)(cid:8)

ih
x∗, β′
ih
+ M8,

x∗, β

· · ·

i

(A∗R
•

x∗ + εR)(A∗R
•

i

+ x∗

T (βvT + β′uT )ε + uT εεT v

x∗ + εR)T ]
A∗R
•

(cid:9)(cid:8)

22

x∗x∗

T
T A∗
R

+ A∗R
•

•

x∗εT

R + εRx∗

T
T A∗
R

+ εRεT
R

•

(cid:9)(cid:3)

in which only nontrivial terms are kept in place, including

T
T A∗
R

]

•

x∗εT
R]
T
T A∗
R

]

•

i

ih

ih

x∗x∗

x∗, β′

A∗R
•
εRεT
R]

M1 = E[
x∗, β
h
M2 = E[
x∗, β
x∗, β′
h
i
M3 = E[x∗
T (βvT + β′uT )εA∗R
•
M4 = E[x∗
T (βvT + β′uT )εεRx∗
M5 = E[uT εεT vA∗R
T
T A∗
]
x∗x∗
R
•
M6 = E[uT εεT vA∗R
x∗εT
R]
•
M7 = E[uT εεT vεT
T
T A∗
Rx∗
R
M8 = E[uT εεT vεRεT
R]

•

•

]

(6)

By swapping inner product terms and taking advantage of the independence, we can show that
M6 = E[A∗R
] = 0. The remaining are bounded in
the next claims.

R] = 0 and M7 = E[uT εεT vεT

x∗uT εεT vεT

T
T A∗
R

Rx∗

•

•

Claim 6. In the decomposition (6),

M1 =

T
R,i + E′1 + E′2 + E′3
qiciβiβ′iA∗R,iA∗

Xi
V
U
∩
∈
T
V qiciβiβ′iA∗R,iA∗
R,i, E′2 =
∩

where E′1 =
T
β′iA∗R,iβjA∗
R,j) have norms bounded by O∗(k/m log n).
P

P

i /
∈

U

i

T
=j qijβiβ′iA∗R,jA∗
R,j and E′3 =

T
=j qij(βiA∗R,iβ′jA∗
R,j+

i

P

Proof. The expression of M1 is obtained in the same way as E1 is derived in the proof of Lemma
1. To prove the claim, we bound all the terms with respect to the spectral norm of A∗R
and make
•
use of Assumption A4 to ﬁnd the exact upper bound.
T
(U
R,S where S = [m]
For the ﬁrst term E′1, rewrite E′1 = A∗R,SD1A∗
\
qiciβiβ′i| ≤
D1k ≤
S|
∈

V ) and D1 is a diagonal
∩
O∗(k/m log n) as shown in

matrix whose entries are qiciβiβ′i. Clearly,
Claim 4, then

maxi

k

E′1k ≤

k

max
S |
i
∈

qiciβiβ′i|k

A∗R,Sk

2

≤

max
S |
i
∈

qiciβiβ′i|k

A∗R

2
•k

≤

O∗(k/m log n)

O(1). The second term E′2 is a sum of positive semideﬁnite matrices,

where
A∗R,Sk ≤ k
k
β
and
k ≤
k

A∗R
•k ≤
O(k log n), then

E′2 =

T
qijβiβ′iA∗R,jA∗
R,j (cid:22)

max
=j
i

qij

βiβ′i

=j
Xi

(cid:17)(cid:16)Xj
2
which implies that
A∗R
•k
T
form as the last term in Claim 4, which is E′3 = A∗
R
•

=j qij)
k

E′2k ≤

β′kk

(cid:16)Xi

(maxi

kk

β

k

T
A∗R,jA∗
R,j

(max
=j
i

β

qij)
k

β′

A∗R
k
•

kk

(cid:22)

T
A∗
R

•

O(k3/m2). Observe that E′3 has the same

E′3k ≤ k
k

Qβkk

A∗R

2
•k

(max
=j
i

qij)
k

≤

2

A∗R

•k

≤

O(k3/m2)

(cid:17)

. Then

≤
QβA∗R
•
e
β′

β

kk

kk

By Claim 3, we have

β

k

k

and

β′k

k

then we complete the proof for Lemma 6.

are bounded by O(√k log n), and note that k

O∗(√n/ log n),

e

≤

23

Claim 7. In the decomposition (6), M2, M3, M4, M5 and M8 have norms bounded by O∗(k/m log n).

Proof. Recall the deﬁnition of Q in Claim 5 and use the fact that E[x∗x∗
E[
x∗, β
ε qiβiβ′iIr. Then,
R] =
h
The next three terms all involve A∗R
•

σ2
ε maxi qik

x∗, β′i
ih

β′k ≤
whose norm is bounded according to Assumption A4.

T ] = Q, we can get M2 =
ε k2 log2 n/m).

M2k ≤

O(σ2

εRεT

i σ2

P

kk

β

k

Speciﬁcally,

M3 = E[x∗
= A∗R
•
= A∗R
•

x∗εT

R] = E[A∗R
•
T ](βvT + β′uT )E[εεT
R]

T (βvT + β′uT )εA∗R
•
E[x∗x∗
Q(βvT + β′uT )E[εεT

R],

x∗x∗

T (βvT + β′uT )εεT
R]

and

M4 = E[x∗

T (βvT + β′uT )εεRx∗

] = E[εRεT (vβT + uβ′

T )x∗x∗

T
T A∗
R

]

•

= E[εRεT ](vβT + uβ′
= E[εRεT ](vβT + uβ′

T A∗
T
R
•
T )E[x∗x∗
T
T ]A∗
R
T
T )QA∗
,
R

•

and the ﬁfth term M5 = E[uT εεT vA∗R
E[εεT
We already have
R]
k
k
remaining work is to bound
A∗uvT
βvT
A∗kk
k
k
k
O(σ2
in norm by
≤
The remaining term is

•
E[x∗x∗
T A∗
T
ε uT vA∗R
= σ2
.
R
•
•
•
uT v
Q
O(k) (proof of Claim 9), then the
O(k/m) and
k ≤
k
|
T directly follows. We have
, then the bound of vβT + uβ′
βvT + β′uT
k
k
v
u
O(k). Therefore, all three terms M3, M4 and M5 are bounded
kk
O(k3/mn).
e

ε uT vA∗R
•

k ≤ k
ε k2/m)

T
T ]A∗
R

] = σ2

= σ2
ε ,

T
QA∗
R

x∗x∗

k ≤

| ≤

=

e

•

•

e

e

M8 = E[uT εεT vεRεT

R] = E[

uivjεiεj

εRεT
R]

= E[

uiviε2

i εRεT
R

(cid:0)Xi
R
∈
ε uRvT
R

= σ4

(cid:0)Xi,j
] + E[

(cid:1)
uivjεiεj

εRεT
R]

(cid:1)

=j

(cid:0)Xi

(cid:1)

where uR = A∗R
•
O(√k). Therefore,
bound all the above terms by O∗(k/m log n) and ﬁnish the proof of Claim 7.
e

α + (εu)R and vR = A∗R
α′ + (εv)R. We can see that
•
O(k3/n2). Since m = O(n) and k
ε k) =

e
Combine the results of Claim 6 and 7, we complete the proof of Lemma 2.

uRk ≤ k
k
≤

M 8
k

O(σ4

k ≤

e

+

A∗R
(εu)Rk ≤
•kk
k
O∗( √n
log n ), then we can

α
k

Appendix C. Analysis of Main Algorithm

C.1 Simple Encoding

y)sgn(x)T is random over y and x that is obtained from the encoding step.
We can see that (Asx
We follow (Arora et al., 2015) to derive the closed form of gs = E[(Asx
y)sgn(x)T ] by proving
that the encoding recovers the sign of x∗ with high probability as long as As is close enough to A∗.

−

−

Lemma 8. Assume that As is δ-close to A∗ for δ = O(r/n log n) and µ
then with probability over random samples y = A∗x∗ + ε

≤

√n
2k , and k

≥

Ω(log m)

sgn(thresholdC/2

(As)T y

= sgn(x∗)

(7)

(cid:0)
24

(cid:1)

Proof of Lemma 8. We follow the same proof strategy from (Arora et al., 2015) (Lemmas 16 and
17) to prove a more general version in which the noise ε is taken into account. Write S = supp(x∗)
and skip the superscript s on As for the readability. What we need is to show S =
[m] :
S with high probability. Following
) = sgn(x∗i ) for each i
i, y
A
h
i
•
the same argument of (Arora et al., 2015), we prove in below a stronger statement that, even
i, y
A
conditioned on the support S, S =
h
•

and then sgn(
h

with high probability.

As
i, y
•

C/2
}

C/2
}

[m] :

i ≥

i ≥

i
{

i
{

∈

∈

∈

Rewrite

i, y
A
h
•

i

=

i, A∗x∗ + ε
A
i
h
•

=

i, A∗
A
ii
•
•

h

x∗i +

i, A∗
A
j i
•
•

x∗j +

i, ε
A
i
h
•

,

h
Xj
=i

and observe that, due to the closeness of A
i, the ﬁrst term is either close to x∗i or equal to
i and A∗
•
•
S. Meanwhile, the rest are small due to the incoherence and the
0 depending on whether or not i
concentration in the weighted average of noise. We will show that both Zi =
x∗j
and

i, A∗
A
ji
•
•

i
\{

}h

∈

S

i, ε
A
h
i
•
The cross-term Zi =

are bounded by C/8 with high probability.
i, A∗
A
j i
•
•
dom variables, which is another sub-Gaussian random variable with variance σ2
Note that

i
\{

P

}h

S

x∗j is a sum of zero-mean independent sub-Gaussian ran-

S

i
\{

}h

i, A∗
A
j i
•
•

2.

Zi =

P

i, A∗
A
ji
h
•
•

A
i −
h
•
where we use Cauchy-Schwarz inequality and the µ-incoherence of A∗. Therefore,

i, A∗
A∗
j i
•
•

A
i −
•

2µ2/n + 2
h

2
i, A∗
A∗
ji
•
•

2 +

≤

≤

2

(cid:0)

(cid:1)

h

2

P
i, A∗
A∗
ji
•
•

2,

σ2
Zi ≤

T
2µ2k/n + 2
S (A
A∗
i −
k
•
•
√n
2k , to conclude 2µ2k/n

≤

under µ
Applying Bernstein’s inequality, we get
. In fact,
bound the noise term
a sub-Gaussian with variance σ2
Notice that σε = O(1/√n).

i, ε
A
i
h
•

≤

i)
A∗
k
•

2
F ≤

2µ2k/n + 2
k

A∗
Sk
•

2

A
i −
•

k

2
A∗
ik
•

≤

O(1/ log n),

O(1/ log n) we need 1/k = O(1/ log n), i.e. k = Ω(log n).
C/8 with high probability. What remains is to
is sum of n Gaussian random variables, which is
σε log n with high probability.

Zi| ≤
|
i, ε
A
i
h
•
ε . It is easy to see that

i, ε
A
•

|h

i| ≤

Finally, we combine these bounds to have
i, y
A
•

S, then
i, ε
A
h
•
C/2 and negligible otherwise. Using union bound for every i = 1, 2, . . . , m, we ﬁnish

C/4. Therefore, for i

Zi +
|

i| ≤

∈

|h
the proof of the Lemma.

i| ≥

Lemma 8 enables us to derive the expected update direction gs = E[(Asx

y)sgn(x)T ] explicitly.

−

C.2 Approximate Gradient in Expectation

−

Proof of Lemma 5. Having the result from Lemma 8, we are now able to study the expected update
y)sgn(x)T ]. Recall that As is the update at the s-th iteration and x ,
direction gs = E[(Asx
thresholdC/2((As)T y). Based on the generative model, denote pi = E[x∗i sgn(x∗i )
S]
i
|
and qij = P[i, j
S]. Throughout this section, we will use ζ to denote any vector whose norm is
negligible although they can be diﬀerent across their appearances. A
i denotes the sub-matrix of
A whose i-th column is removed. To avoid overwhelming appearance of the superscript s, we skip
it from As for neatness. Denote
Fx∗ is the event under which the support of x is the same as that
of x∗, and ¯
Fx∗ = 1[sgn(x) = sgn(x∗)] and 1
Fx∗ = 1.

Fx∗ is its complement. In other words, 1

S], qi = P[i

Fx∗ + 1 ¯

∈

∈

∈

−

i = E[(Ax
gs
•

−

y)sgn(xi)] = E[(Ax

y)sgn(xi)1

−

Fx∗ ]

±

ζ

25

Fx∗ we have Ax = A

S AT
Using the fact that y = A∗x∗ + ε and that under
Sy =
SxS = A
•
•
•
SAT
SAT
A
SA∗x∗ + A
Sε. Using the independence of ε and x∗ to get rid of the noise term, we get
•
•
•
•
i = E[(A
S AT
gs
S −
•
•
•
= E[(A
S AT
S −
•
•
= E[(A
S AT
S −
•
•
= E[(A
S AT
S −
•
•

S AT
S −
•
•
ζ
(Independence of ε and x’s)
±
1 ¯
Fx∗ )]

In)A∗x∗sgn(xi)1
In)A∗x∗sgn(x∗i )(1
In)A∗x∗sgn(x∗i )]

Fx∗ ] + E[(A
Fx∗ ]
−
ζ

Fx∗ event)

In)εsgn(xi)1

In)A∗x∗1

(Under

Fx∗ ]

±

±

±

ζ

ζ

Recall from the generative model assumptions that S = supp(x∗) is random and the entries of x∗
are pairwise independent given the support, so

i = ESE
gs
x∗
•
= piES,i
= piES,i

|

S AT
S[(A
S −
•
•
S AT
S[(A
S −
•
∈
•
iAT
S[(A
i −
•
∈
•

In)A∗x∗sgn(x∗i )]
i]
In)A∗
ζ
±
•
i] + piES,i
In)A∗
•

S[
∈

ζ

±

iAT
= piqi(A
i −
•
•

i + pi
In)A∗
•

= piqi(λiA
i −
•

i) + piA
A∗
•

•−

Xl
=i
S,l
∈
lAT
lA∗
qilA
i ±
•
•
•

ζ

[m],l
Xl
=i
∈
idiag(qij)AT
•−

iA∗
i ±
•

ζ

lAT
i]
lA∗
A
•
•
•

±

ζ

where λs
expression of the expected approximate gradient at iteration s:

idiag(qij)AT
•−

As
i, A∗
ii
h
•
•

i = AR,

. Let ξs

i =

−

iA∗
i/qi for j = 1, . . . , m, we now have the full
•

(8)

(9)

R,i = piqi(λiAs
gs

A∗R,i + ξs
i )

R,i −

ζR.

±

What remains is to bound norms of ξs and ζ. We have
ξs
A∗i k
along with the fact that
i k
k
qij
ξs
i k ≤ k
qi k
k

= 1, we can bound

max
=i
j

As

ik

Ri,

k

−

As
−

ik ≤

O(k/n).

As
R,

k

ik ≤ k

−

As
−

ik ≤

O(

m/n) w.h.p. Then,

p

Next, we show that norm of ζ is negligible. In fact,
it suﬃces to bound norm of (Ax
Section D. This concludes the proof for Lemma 5.

−

Fx∗ happens with very high probability, then
y)sgn(xi) which will be done using Lemma 12 and Lemma 11 in

Appendix D. Sample Complexity

In previous sections, we rigorously analyzed both initialization and learning algorithms as if the
expectations gs, e and Mu,v were given. Here we show that corresponding estimates based on
empirical means are suﬃcient for the algorithms to succeed, and identify how may samples are
required. Technically, this requires the study of their concentrations around their expectations.
Having had these concentrations, we are ready to prove Theorems 4 and 5.

The entire section involves a variety of concentration bounds. Here we make heavy use of
Bernstein’s inequality for diﬀerent types of random variables (including scalar, vector and matrix).
The Bernstein’s inequality is stated as follows.

26

Lemma 9 (Bernstein’s Inequality). Suppose that Z (1), Z (2), . . . , Z (p) are p i.i.d. samples from some
almost surely and
distribution

σ2 for each j, then

. If E[Z] = 0,

E[Z (j)(Z (j))T

Z (j)

k ≤

D

k

k ≤ R
p

(10)

k
σ2
p

(cid:19)

+

R
p

s

1
p

Z (j)

≤

O

e

(cid:18)

(cid:13)
(cid:13)
(cid:13)

Xj=1
(cid:13)
(cid:13)
(cid:13)

holds with probability 1

n−

ω(1).

−

Since all random variables (or their norms) are not bounded almost surely in our model setting,

(log(1/ρ))C ]

≤

we make use of a technical lemma that is used in Arora et al. (2015) to handle the issue.
Lemma 10 (Arora et al. (2015)). Suppose a random variable Z satisﬁes P[
k
ρ for some constant C > 0, then
(a) If p = nO(1), it holds that
(b)
e
Ω(

) for each j with probability 1

k
ω(1).

k ≥ R

= n−

ω(1).

Z (j)

k ≤

n−

O(

R

−

Z

Z

E[Z1
k
k

k≥

)]
k
R

e

p
This lemma suggests that if 1
i=1 Z (j)(1
)) concentrates around its mean with
p
p
high probability, then so does 1
i=1 Z (j) because the part outside the truncation level can be
P
p
ignored. Since all random variables of our interest are sub-Gaussian or a product of sub-Gaussian
that satisfy this lemma, we can apply Lemma 9 to the corresponding truncated random variables
with carefully chosen truncation levels. Then the original random variables concentrate likewise.

1
k

Z (j)

P

e
Ω(

−

k≥

R

In the next proofs, we deﬁne suitable random variables and identify good bounds of

and
σ2 for them. Note that in this section, the expectations are taken over y by conditioning on u
and v. This aligns with the construction that the estimators of e and Mu,v are empirical averages
over i.i.d. samples of y, while u and v are kept ﬁxed. Due to the dependency on u and v, these
(conditional) expectations inherit randomness from u and v, and we will formulate probabilistic
bounds for them.

R

The application of Bernstein’s inequality requires a bound on

eΩ(
k≥
achieve that by the following technical lemma, where ˜Z is a standardized version of Z.
Lemma 11. Suppose a random variable ˜Z ˜Z T = aT where a
They are both random. Suppose P[a

> 0 is a constant. Then,

ω(1) and

1
k

−

≥

Z

] = n−

E[ZZ T (1
k

0 and T is positive semi-deﬁnite.

. We

))]
k

R

E[ ˜Z ˜Z T (1

k

≥ A
1
˜Z
k

−

)]

k≥B

k ≤ Ak

B
E[T ]
k

+ O(n−

ω(1))

Proof. To show this, we make use of the decomposition ˜Z ˜Z T = aT and a truncation for a. Specif-
ically,

E[ ˜Z ˜Z T (1
k

1
k

˜Z

−

)]
k

k≥B

T (1

1
˜Z
k

−

k≥B

)]
k

≥A

˜Z

)]

k≥B

)]
k
T

≥Ak

1
˜Z
k

k≥B

k≥B
)T (1

≥A

1
˜Z
−
k
+ E[a1a
)T ]
k
≥A
2(1
E[
aT
k
k
4(1
˜Z
E[
k
k
P[a

−
1
˜Z
k
1/2

−
]

2

k≥B

+

E[a1a
k
1
(1
˜Z
k
−
k
)]E[1a
)]P[a

≥A
]

]

≥ A

(cid:1)

)]

k≥B
1/2

1/2
(cid:1)

= E[aT (1
E[a(1
E[a(1

≤ k

≤ k

−

−

1
k
1a

1a

−
E[T ]
k
E[T ]
k
E[T ]
k
E[T ]
k

≤ Ak

≤ Ak

≤ Ak

≤ Ak

+

+

(cid:0)

+

(cid:0)
B
+ O(n−

(cid:0)

≥ A
ω(1)),

(cid:1)

27

where at the third step we used T (1
semi-deﬁnite and 1

1
k

˜Z

−

k≥B ∈ {

0, 1
}

1
k

˜Z

)]

T because of the fact that T is the positive

−
. Then, we ﬁnish the proof of the lemma.

k≥B

(cid:22)

D.1 Sample Complexity of Algorithm 1

e and the reduced weighted covariance matrix
Mu,v depends upon
b
R
u,v. We will show that we only need
e, we denote it by
c
O(m) samples to be able to recover the support of one particular atom and up to some speciﬁed

In Algorithm 1, we empirically compute the “scores”
Mu,v to produce an estimate for each column of A∗. Since the construction of
the support estimate
c
p =
level of column-wise error with high probability.

R given by ranking

c

M

b

b

b

e

Lemma 12. Consider Algorithm 1 in which p is the given number of samples. For any pair u and
v, then with high probability a)
O∗(k/m log n) when p =
sets of one particular atom.
e

u,vk ≤
R and R are respectively the estimated and correct support
e

e
e
k ≤
k
Ω(mr) where

O∗(k/m log2 n) when p =

Ω(m) and b)

b
R
u,v −

M
k

M R

c

−

b

b

D.1.1 Proof of Theorem 4

Using Lemma 12, we are ready to prove the Thereom 4. According to Lemma 1 when U
we can write

e as

∩

V =

,
i
}
{

b

e = qiciβiβ′iA∗R,i ◦

e
A∗R,i + perturbation terms + (

e),

−

e as an additional perturbation with the same magnitude O∗(k/m log2 n) in the
and consider
e
−
w.h.p. The ﬁrst part of Lemma 3 suggests that when u and v share exactly one atom
sense of
k · k∞
e is the same as supp(A∗i ) with high probability.
R including r largest elements of
i, then the set
b

b

b

Once we have
b

R, we again write

M

b
R
u,v using Lemma 2 as
b

b
b
c
T
R
R,i + perturbation terms + (
u,v = qiciβiβ′iA∗R,iA∗
M

M

b
R
u,v −

M R

u,v),

b
c
R
u,v −

M

and consider
in the sense of the spectral norm
singular vectors of

c

M

M R

c
u,v as an additional perturbation with the same magnitude O∗(k/m log n)
w.h.p. Using the second part of Lemma 3, we have the top

b
R
u,v is O∗(1/ log n) -close to A∗R,i with high probability.

k · k

Since every vector added to the list L in Algorithm 1 is close to one of the dictionary, then
A0 must be δ-close to A∗. In addition, the nearness ofA0 to A∗ is guaranteed via an appropriate
A close to A0 and
projection onto the convex set
. Finally, we ﬁnish the
A
|
{
proof of Theorem 4.

A∗k}

A
k

2
k

k ≤

c

=

B

D.1.2 Proof of Lemma 12, Part a

y, v

[n], consider p i.i.d. realizations Z (1), Z (2), . . . , Z (p) of the random variable
For some ﬁxed l
∈
Z ,
O∗(k/m log2 n)
y2
l , then
y, u
e
h
k∞ ≤
holds with high probability, we ﬁrst study the concentration for the l-th entry of
e and then
e
and its variance E[Z 2]
take the union bound over all l = 1, 2, . . . , n. We derive upper bounds for
b
|
in order to apply Bernstein’s inequality in (12) to the truncated version of Z.

i=1 Z (i) and el = E[Z]. To show that

el = 1
p

P

ih

−

−

Z

b

k

e

i

p

|

b

O(k) and E[Z 2]

O(k2/m) with high probability.

Claim 8.

Z
|

| ≤

e

≤

e

28

Again, the expectation is taken over y by conditioning on u and v, and therefore is still random
due to the randomness of u and v. To show Claim 8, we begin with proving the following auxiliary
claim.

Claim 9.

y

O(√k) and

y, u

O(√k) with high probability.

k

k ≤

|h

i| ≤

Proof. From the generative model, we have

e

e

y
k

k

=

Sx∗S + ε
A∗
k
•

where S = supp(x∗). From Claim 2,
overcomplete and has bounded spectral norm, then
O(1). Therefore,
w.h.p., which is the ﬁrst part of the proof. To bound the second term, we write it as

k
A∗
Sk ≤ k
•

e

k

k

,

+

ε
k
k

x∗Sk

A∗
S kk
•
O(σε√n) w.h.p. In addition, A∗ is
O(√k)

ε
k ≤ k
ε
k ≤
A∗k ≤
e

y
k

k ≤

+

Sx∗Sk
A∗
•
O(√k) and

k

k ≤ k
x∗Sk ≤

=

y, u

Sx∗S + ε, u
A∗
•

T
S u
x∗S, A∗
•
T
S u
Similar to y, we have
A∗
k
•
probability. Since u and x∗ are independent sub-Gaussian and
T
e
variance at most O(√k),
S u
x∗S, A∗
•
y, u
quently,

i|
i| ≤ |h
O(√k) w.h.p. and hence

O(k) w.h.p. Similarly,

i| ≤

O(√k) w.h.p., and we conclude the proof of the claim.

.
i|
T
O(√k) with high
u
A∗
S kk
k ≤ k
•
T
x∗S, A∗
are sub-exponential with
S u
h
i
•
O(√k) w.h.p. Conse-
ε, u
i| ≤

k ≤

k ≤

|h
u

ε, u

+

|h

|h

|h

|h

i|

k

e

e

|h

y, v

e
y2
l =

i| ≤
, x∗i
A∗l
(
Proof of Claim 8. We have Z =
y, u
h
i
h
•
w.h.p. according to Claim 9. What remains is to bound y2
A∗l
l = (
h
2
is sub-Gaussian with variance ES(
•
1,2 = O(1), then
k
+εl| ≤
A∗l
εl| ≤
Similarly for εl,
|h
|
•
with high probability the bound
2y2
2y2
y, u
l h
i

T
2
A∗
li )
S A∗
i
∈
O(σε log n) w.h.p. Ultimately,

To bound the variance term, we write Z 2 =

P
Z
|

y, u
h

, x∗i

O(k).

≤ k

y, v

| ≤

ih

ih

i

2y2
i

y, v
we get
to both terms, then
e

l ≤

h

O(k) and

Z
|

| ≤

E[Z 2(1

e
Z

|≥

1
|

−

Ω(k))]
e

O(k)E[
h

y, u
i

≤

2y2

l ] + O(n−

ω(1)),

e
+ εl)2 with
O(k)
i ≤
, x∗i
A∗l
, x∗i
h
•
e
O(log n) w.h.p.
|h
O(log n), and hence we obtain

y, v
y, u
h
+ εl)2. Because
, x∗i| ≤
A∗l
•

ih

l . Note that, from the ﬁrst part,
O(k) w.h.p.. We apply Lemma 11 with some appropriate scaling

y, v

e

h

i

where E[
e
y, u
i
h
Section “Analysis of Initialization Algorithm”,

2y2

l ] is equal to el for pair u, v with v = u. From Lemma 1 and its proof in Appendix

E[
y, u
i
h

2y2

l ] =

qiciβ2

2
li + perturbation terms,
i A∗

m

Xi=1

in which the perturbation terms are bounded by O∗(k/m log2 n) w.h.p. (following Claims 4 and 5).
(max qiciβ2
2
O(log m)
A∗l
i )
i A∗
The dominant term
li ≤
≤
k
(Claim 3). Then we complete the proof of the second part.

O(k/m) w.h.p. because

βi| ≤
|

i qiciβ2

2
•k

P

e

Proof of Lemma 12, Part a. We are now ready to prove Part a of Lemma 12. We apply Bernstein’s
inequality in Lemma 9 for the truncated random variable Z (i)(1
O(k) and
variance σ2 =

O(k2/m) from Claim 8, then

)) with

1
Z (i)
|

e
Ω(

R

−

=

|≥

R

Z (i)(1

e
−

1
|

Z (i)

e
Ω(

))

|≥

R

−

E[Z(1

1
Z
|

−

e
Ω(

|≥

R

p

Xi=1

1
p

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

O(k)
p

e

O(k2/m)
p

+

s

≤

e

))]
(cid:13)
(cid:13)
(cid:13)
(cid:13)

29

e

O∗(k/m log n), (11)

Ω(m). Then

el = 1
w.h.p. for p =
p
bound over l = 1, 2, . . . , n, we get
the proof of 12, Part a.

e

b

p
i=1 Z (i) also concentrates with high probability. Take the union
O∗(k/m log n) with high probability and complete
e

k
P

−

e
k∞ ≤

D.1.3 Proof of Lemma 12, Part b

b

b
R
u,v −

M R

M

Next, we will prove that
prove the concentration inequalities for the case when conditioned on the event that
c
to R w.h.p. Again, what we need to derive are an upper norm bound
variable Z ,
yRyT
R and its variance.

O∗(k/m log n) with high probability. We only need to
R is equivalent
of the matrix random

u,vk ≤

y, v

R

b

k

y, u
h

ih

i

Claim 10.

Z

O(kr) and

k

k ≤

E[ZZ T ]
k

k ≤

O(k2r/m) hold with high probability.

k

Z

Proof. We have
k
2 =
yRk
whereas
This implies
Z
P
k ≤
k
We take advantage of the bounds of
and

e

2

2

i|k

y, u

yRk

e
2 with
y, v
ih
i| ≤
O(r log2 n) w.h.p. because yl ≤

e
O(k) w.h.p. (according to Claim 9)
k ≤ |h
R y2
O(log n) w.h.p. (proof of Claim 8).
i
∈
e
O(kr) w.h.p. The second part is handled similarly as in the proof of Claim 8.
O(kr)
Mu,v in Lemma 2. Speciﬁcally, using the ﬁrst part

l ≤

y, u

y, v

ih

|h

O(kr), and applying Lemma 11, then

Z
k

k ≤

i

y, v
h

yRk
k
E[ZZ T (1
k

≤

e
1
Z
k

−

k≥

Ω(kr))]
e

k ≤

E[
y, u
i
h

2yRyT
R]
k

+

O(kr)O(n−

ω(1))

O(kr)
k

,
Mu,uk

≤

e

where Mu,u arises from the application of Lemma 2. Recall that

e

e

e

Mu,u =

qiciβ2

T
R,i + perturbation terms,
i A∗R,iA∗

c
O(kr)
k

Xi

where the perturbation terms are all bounded by O∗(k/m log n) w.h.p. by Claims 6 and 7.
addition,

In

qiciβ2

T
i A∗R,iA∗
R,ik ≤

(max
i

qiciβ2
i )
k

A∗R

2
•k

O(k/m)
k

A∗

≤

k

≤

2

O(k/m)

k
Xi

w.h.p. Finally, the variance bound is

O(k2r/m) w.h.p.
Then, applying Bernstein’s inequality in Lemma 9 to the truncated version of Z with

and variance σ2 =

O(k2r/m) and obtain the concentration for the full Z to get

e

e

e

=

O(kr)

R

e

e

M R
k

u,v −

M R

u,vk ≤

c

O(kr)
p

O(k2r/m)
p

+

s

≤

e

e

O∗(k/m log n)

w.h.p. when the number of samples is p =

We have proved that
k
b
R
u,v −

M

k

M R

M R
u,v −
M R
u,vk ≤
c

Ω(mr) under Assumption A4.1.
O∗(k/m log n) as conditioned on the support consistency
u,vk ≤
e
O∗(k/m log n) is easily followed by the law of total probability

event holds w.h.p.
through the tail bounds on the conditional and marginal probabilities (i.e. P[
k
c
R = R]) and P[
O∗(k/m log n)
|
the spectral bounds.

u,vk ≤
= R]. We ﬁnish the proof of Lemma 12, Part b for both cases of

u,v −

M R

M R

c

R

b

b

30

D.2 Proof of Theorem 5 and Sample Complexity of Algorithm 2

In this section, we prove Theorem 5 and identify sample complexity per iteration of Algorithm 2. We
divide the proof into two steps: 1) show that when As is (δs, 2)-near to A∗ for δs = O∗(1/ log n), the
O(k2/mn)+αo(δ2
s )
approximate gradient estimate
, and 2) show that the nearness is preserved at each iteration. These correspond to showing the
following lemmas:

gs is (α, β, γs)-correlated-whp with A∗ with γs ≤

b

Lemma 13. At iteration s of Algorithm 2, suppose that As has each column correctly supported
and is (δs, 2)-near to A∗ and that η = O(m/k). Denote R = supp(As
gs
R,i is
i), then the update
•
O(k2/mn) + αo(δ2
(α, β, γs)-correlated-whp with A∗R,i where α = Ω(k/m), β = Ω(m/k) and γs ≤
s )
for δs = O∗(1/ log n).

b

Note that this is a ﬁnite-sample version of Lemma 6.

Lemma 14. If As is (δs, 2)-near to A∗ and number of samples used in step s is p =
.
with high probability

Proof of Theorem 5. The correlation of
column-wise error according to Theorem 1. Along with Lemma 14, the theorem follows directly.

A∗k
2
k
gi with A∗i , described in Lemma 13, implies the descent of

e

As+1
k

A∗k ≤

−

Ω(m), then

D.2.1 Proof of Lemma 13

b

We prove Lemma 13 by obtaining a tail bound on the diﬀerence between
Bernstein’s inequality in Lemma 9.

R,i and gs
gs

R,i using the

Lemma 15. At iteration s of Algorithm 2, suppose that As has each column correctly supported and
is (δs, 2)-near to A∗. For R = supp(As
(o(δs) + O(ǫs))
i ) = supp(A∗i ), then
·
mnr
k ).
with high probability for δs = O∗(1/ log n) and ǫs = O(

gs
R,i −
k/n) when p =

O(k/m)
Ω(m + σ2
ε

gs
R,ik ≤

p
To prove this lemma, we study the concentration of

b
gs
R,i, which is a sum of random vector
e
of the form (y
S, with
i
Ax)Rsgn(xi)
|
S = supp(x∗) and x = thresholdC/2(AT y). Then, using the following technical lemma to bridge
the gap in concentration of the two variables. We adopt this strategy from Arora et al. (2015) for
our purpose.

Ax)Rsgn(xi). We consider random variable Z , (y

−

−

∈

b

k

b

Claim 11. Suppose that Z (1), Z (2), . . . , Z (N ) are i.i.d. samples of the random variable Z = (y
i
Ax)Rsgn(xi)
|

S. Then,

∈

−

o(δs) + O(ǫs)

(12)

(cid:13)
(cid:13)
(cid:13)
holds with probability when N =

Xj=1
Ω(k + σ2

N

1
N

Z (j)

−

≤

E[Z]
(cid:13)
(cid:13)
(cid:13)

ε nr), δs = O∗(1/ log n) and ǫs = O(

k/n).

Proof of Lemma 15. Once we have done the proof of Claim 11, we can easily prove Lemma 15. We
recycle the proof of Lemma 43 in Arora et al. (2015).
supp(x∗

Write W =

and N =

gR,i as

e

j : i
{

∈

(j))
}

W
|

, then express
|

p

gR,i =

N
p

1
N

(y(j)

−

Ax(j))Rsgn(x(j)

b
i ),

b

Xj

31

j(y(j)

1
where
W
|
E[(y
Ax)Rsgn(xi)] = E[(y
Following Claim 11, we have

P

−

−

|

−

Ax(j))Rsgn(x(j)

i ) is distributed as 1
N

Ax)Rsgn(xi)1i

S ] = E[Z]P[i
P
∈

∈

N
j=1 Z (j) with N =

. Note that
W
|
|
S] = qiE[Z] with qi = Θ(k/m).

gs
R,i −

gs
R,ik ≤

k

O(k/m)

O(k/m)

(o(δs) + O(ǫs)),

·

1
N

N

Xj=1

(cid:13)
(cid:13)
(cid:13)

Z (j)

−

≤

E[Z]
(cid:13)
(cid:13)
(cid:13)

holds with high probability as p = Ω(mN/k). Substituting N in Claim 11, we obtain the results
in Lemma 15.

b

Proof of Claim 11. We are now ready to prove the claim. What we need are good bounds for
k
and its variance, then we can apply Bernstein’s inequality in Lemma 9 for the truncated version of
Z, then Z is also concentrates likewise.

Z
k

holds with high probability for

=

O(δs√k + µk/√n + σε√r) with δs =

Claim 12.
k
O∗(1/ log n).

Z

k ≤ R

R

e

−

(y

Proof. From the generative model and the support consistency of the encoding step, we have
S y = AT
Sx∗S + AT
Sx∗S + ε and xS = AT
Sε. Then,
SA∗
y = A∗x∗ + ε = A∗
•
•
•
•
•
AR,SAT
AR,SAT
S ε
SA∗
•
•
•
S )x∗S + (In −
AR,S)x∗S + AR,S(Ik −
O(σwk
M w
k
e
kF + σεk

Ax)R = (A∗R,Sx∗S + εR)
S x∗S −
AT
= (A∗R,S −
SA∗
•
•
Using the fact that x∗S and ε are sub-Gaussian and that
probability for a ﬁxed M and a sub-Gaussian w of variance σ2

SAT
S)R
A
•
•
Now, we need to bound those Frobenius norms. The ﬁrst quantity is easily bounded as

kF ) holds with high

SAT
S )R
A
•
•
•

k ≤
w, we have

AR,S(Ik −

AR,SkF +

AT
S )
SA∗
•
•

Ax)Rsgn(xi)

A∗R,S −

(In −

•kF ).

O(
k

(y
k

k ≤

M

−

−

ε.

k

e

A∗R,S −
k

AR,SkF ≤ k

A∗
S −
•

A
S kF ≤
•

δs√k,

(13)

since A is δs-close to A∗. To handle the other two, we use the fact that
this fact for the second term, we have

U V
k

kF ≤ k

U

V

kF . Using

kk

AR,S(Ik −
k

AT
S)
S A∗
•
•

kF ≤ k

AR,Skk

(Ik −

AT
S )
SA∗
•
•

kF ,

where
advantage of the closeness and incoherence properties:

AR,Sk ≤ k

•k ≤

AR

k

O(1) due to the nearness. The second part is rearranged to take

Ik −

k

AT
SA∗
SkF ≤ k
•
•

S)T A∗
T
(A
A∗
S A∗
A∗
Ik −
S −
SkF
S −
•
•
•
•
•
S)T A∗
T
SkF +
A∗
(A
S A∗
A∗
Ik −
S −
S kF
k
•
•
•
•
•
T
A∗
A
A∗
SkF +
S A∗
Ik −
A∗
S −
S kF
Skk
•
•
•
•
•
µk/√n + O(δs√k),

k

≤ k

≤ k

≤

where we have used
δs√k in (13) and

µk/√n because of the µ-incoherence of A∗,

A
S −
•

A∗
SkF ≤
•

k

O(1). Accordingly, the second Frobenius norm is bounded by

k

T
S A∗
Ik −
A∗
SkF ≤
k
•
•
A∗k ≤
A∗
Sk ≤ k
•
AR,S(Ik −

k

AT
S )
SA∗
•
•

kF ≤

O

µk/√n + δs√k

.

(cid:0)

32

(cid:1)

(14)

SAT
The noise term is handled using the eigen-decomposition U ΛU T of A
S, then with high prob-
•
•
ability

(In −

(U U T
SAT
•kF =
S)R
A
k
k
•
•
where the last inequality
A∗k ≤
O(1) due to the nearness. Putting (13), (14) and (15) together, we obtain the bounds in Claim
12.

•kF =
O(1) follows by

(In −
A
Sk ≤ k
•

kF ≤ k
A

•kF ≤
+

kk
A∗k

In −
A

O(√r), (15)

U ΛU T )R

A∗k ≤

UR
k
•

In −

k ≤ k

−
Λ

3
k

k ≤

UR

Λ)

−

Λ

k

k

k

Next, we determine a bound for the variance of Z.

2] = E[
Claim 13. E[
Ax)Rsgn(xi)
Z
k
k
−
k
k
s k + k2/n + σ2
O(δ2
ε r) with δs = O∗(1/ log n).

(y

2

i
|

S]

∈

≤

σ2 holds with high probability for σ2 =

Proof. We explicitly calculate the variance using the fact that x∗S is conditionally independent
given S, and so is ε. x∗S and ε are also independent and have zero mean. Then we can decompose
the norm into three terms in which the dot product is zero in expectation and the others can be
S ] = Ik, E[εεT ] = σεIn.
T
shortened using the fact that E[x∗Sx∗
SAT
AR,S AT
i
ε
A
SA∗
S)x∗S + (In −
S)R
∈
k
|
·
•
•
•
•
2
2
E[
AR,SAT
SAT
S] + σ2
SA∗
S)R
A
In −
i
i
S k
ε
F |
F |
k
∈
•k
∈
•
•
•
•
AT
Then, by re-writing A∗R,S −
S )
SA∗
AR,S)+AR,S(Ik−
S as before, we get the form (A∗R,S −
•
•
in which the ﬁrst term has norm bounded by δs√k. The second is further decomposed as

S] = E[
(A∗R,S −
k
= E[
A∗R,S −
k
AR,SAT
SA∗
•
•

Ax)Rsgn(xi)
k

E[
(y
k

S]]

i
|

−

∈

2

2

S].

E[
AR,S (Ik −
k

AT
S)
SA∗
k
•
•

2
i
F |

∈

S]

sup
S k

AR,Sk

≤

2E[
Ik −
k

2
AT
SA∗
i
S k
F |
•
•

∈

S],

(16)

where supSk
using the proof from Arora et al. (2015):

AR,Sk ≤ k

•k ≤

AR

O(1). We will bound E[
k

Ik −

2
AT
SA∗
i
F |
S k
•
•

∈

S]

≤

O(kδ2

s ) + O(k2/n)

E[
Ik −
k

AT
SA∗
Sk
•
•

2
i
F |

∈

S] = E[

1
4 k

A
j −
•

A∗
jk
•

2] + qij

= E[

S
Xj
∈

S
Xj
∈

k
Xj
=i

(1

−

j)2 +
AT
jA∗
•
•

AT
jA∗
,
k
•
•
Xj
S
∈
2 + qik
AT
iA∗
,
jk
•
•

−

−

AT
jA∗
,
•
•

2
jk

i
|

−

∈

S]

2 + qik
ik

AT
,
•

iA∗
ik
•

−

2,

where A
,
−
•
any j = 1, 2, . . . , m,

i is the matrix A with the i-th column removed, qij ≤

O(k2/m2) and qi ≤

O(k/m). For

AT
jA∗
,
•
•

k

jk

−

2 =

k

T
j A∗
A∗
,
•
•
−
j, A∗
A∗
li
•
•

2
j)T A∗
A∗
j + (A
j −
,
jk
•
•
•
−
2 +
j)T A∗
A∗
(A
j −
,
k
•
•
•

2

jk

−

≤

h
Xl
=j

≤

h
Xl
=j

j, A∗
A∗
li
•
•

2 +

A
j −
k
•

A∗
jk
•

2

A∗
,
k
•

−

2
jk

≤

µ2 + δ2
s .

The last inequality invokes the µ-incoherence, δ-closeness and the spectral norm of A∗. Similarly,
we come up with the same bound for

2. Consequently,

2 and

AT
iA∗
,
•
•

k

ik

−

AT
iA∗
,
ik
k
•
−
•
s ) + O(k2/n).
O(kδ2

E[
k

Ik −

2
AT
S A∗
i
Sk
F |
•
•

∈

S]

≤

(17)

33

For the last term, we invoke the inequality (15) (Claim 12) to get

SAT
S)R
A
•
•
Putting (16), (17) and (18) together and using
σ2 = O(δ2

•k
AR
k
ε r) with δs = O(1/ log2 n) . Finally, we complete the proof.
We now apply truncated Bernstein’s inequality to the random variable Z (j)(1

s k + k2/n + σ2

E[
(In −
k

2
i
F |

•k ≤

S]

≤

∈

r

1, we obtain the variance bound of Z:

(18)

))
Ω(
R
O(δs√k + µk/√n + σε√r) and σ2 = O(δ2
s k +

1
k

Z (j)

−

k≥

with
R
k2/n + σ2

and σ2 in Claims 12 and 13, which are
ε r). Then, (1/N )

=

R

N
j= Z (j) also concentrates:
e

N

P
Z (j)

Xi=1

1
N

(cid:13)
(cid:13)
(cid:13)

E[Z]

−

O

≤

R
N

+

O

(cid:13)
(cid:13)
(cid:13)

(cid:16)

(cid:17)

e
Ω(k + σ2

e

σ2
N

(cid:18)r

(cid:19)

= o(δs) + O(

k/n)

p

ε nr). Then, we ﬁnally ﬁnish the proof of Claim

holds with high probability when N =
11.

e

Proof of Lemma 13. With Claim 11, we study the concentration of
we consider this diﬀerence as an error term of the expectation gs
correlation of

gs
R,i. Using the expression in Lemma 5 with high probability, we can write

R,i around its mean gs
gs

R,i. Now,
R,i and using Lemma 6 to show the

b

b

R,i + (gs
where
+ O(k/m)
correlated-whp with A∗R,i where α = Ω(k/m), β = Ω(m/k) and γs ≤
, then we have done the proof Lemma 13.

gs
R,i = gs
A∗R,ik
b

gs
R,i) = 2α(AR,i −

AR,i −

R,i −

α
k

k ≤

b

k

v

·

A∗R,i) + v,

(o(δs) + O(ǫs)). By Lemma 6, we have

O(k/m)

·

b

gs
R,i is (α, β, γs)-
k/n))

(o(δs) + O(

p

D.2.2 Proof of Lemma 14

gs with A∗ w.h.p. and established the descent property of Al-
We have shown the correlation of
gorithm 2. The next step is to show that the nearness is preserved at each iteration. To prove
As+1
holds with high probability, we recall the update rule
k

A∗k ≤

A∗k

2
k

−

b

PH (
gs. Here H = (hij) where hij = 1 if i

As+1 = As

−

η

gs),

gs) = H

◦

PH(

where
j) and hij = 0 otherwise. Also,
supp(A
•
note that As is (δs, 2)-near to A∗ for δs = O∗(1/ log n). We already proved that this holds for the
exact expectation gs in Lemma 7. To prove for
gs, we again apply matrix Bernstein’s inequality to
gs)
bound
k
Consider a matrix random variable Z ,
E[ZZ T ]
b
k
k

= O(1).
by O(k/m) because η = Θ(m/k) and
Ax)sgn(x)T ). Our goal is to bound the spectral
PH((y
b
E[Z T Z]
since Z is asymmetric. To simplify our notations,
norm
k
we denote by xR the vector x by zeroing out the elements not in R. Also, denote Ri = supp(hi)
and S = supp(x). Then Z can be written explicitly as

b
− PH (
and, both

b
kPH(gs)
Z

A∗k

and

b
∈

−

k

k

k

k

Z = [(y

Ax)R1 sgn(x1), . . . , (y

Ax)Rmsgn(xm)],

−

−

where many columns are zero since x is k-sparse. The following claims follow from the proof of
Claim 42 in Arora et al. (2015). Here we state and detail some important steps.

Claim 14.

Z

O(k) holds with high probability.

k

k ≤

e

34

Proof. With high probability

Z
k

k ≤

(y

2
Ax)Risgn(xi)
k

−

≤

√k

(y

k

Ax)Rik

−

k
sXi
S
∈

(y
k

−

where we use Claim 12 with

Claim 15.

E[ZZ T ]

O(k2/n) and

k

k ≤

Ax)Rk ≤
k

E[Z T Z]
e

k ≤

O(δs√k) w.h.p., then

Z

O(k) holds w.h.p.

k
O(k2/n) with high probability.

k ≤

e

Proof. The ﬁrst term is easily handled. Speciﬁcally, with high probability

e

E[ZZ T ]

k

k ≤ k

E[

(y

−

Ax)Ri sgn(xi)2(y

Ax)T

Ri]
k

−

=

E[
k

(y

Ax)Ri(y

−

−

Ax)T

Ri ]

k ≤

O(k2/n),

S
Xi
∈

S
Xi
∈

where the last inequality follows from the proof of Claim 42 in Arora et al. (2015), which is tedious
to be repeated.
To bound

, we use bound of the full matrix (y
O(√k) w.h.p. is similar to what derived in Claim 12. Then with high probability,

Ax)sgn(x)T . Note that

E[Z T Z]
k

y
k

k ≤

Ax

−

−

k

E[Z T Z]
k

E[sgn(x)(y

Ax)T (y

Ax)sgn(x)T ]

k ≤ k

e
where E[sgn(x)sgn(x)T ] = diag(q1, q2, . . . , qm) has norm bounded by O(k/m). We now can apply
O(k2/m), then with
Bernstein’s inequality for the truncated version of Z with
p =

O(k) and σ2 =

O(m),

k ≤

k ≤

R

=

−

−

e

e

E[sgn(x)sgn(x)T ]

O(k2/m).

O(k)
k

e

kPH(gs)

− PH(

gs)

k ≤

O(k)
p

e

e
O(k2/m)
p

≤

+

s

e

e

O∗(k/m)

holds with high probability. Finally, we invoke the bound η = O(m/k) and complete the proof.

b

Appendix E. A Special Case: Orthonormal A∗

We extend our results for the special case where the dictionary is orthonormal. As such, the
dictionary is perfectly incoherent and bounded (i.e., µ = 0 and

= 1).

Theorem 7. Suppose that A∗ is orthonormal. When p1 =
Ω(nr), then with high
probability Algorithm 1 returns an initial estimate A0 whose columns share the same support as
A∗ and with (δ, 2)-nearness to A∗ with δ = O∗(1/ log n). The sparsity of A∗ can be achieved up to
r = O∗

e

e

)

.

,

min( √n
log2 n

n
k2 log2 n

We use the same initialization procedure for this special case and achieve a better order of r.

(cid:0)

(cid:1)

The proof of Theorem 7 follows the analysis for the general case with following two results:

A∗k
k
Ω(n) and p2 =

Claim 16 (Special case of Claim 3). Suppose that u = A∗α + εu is a random sample and U =
supp(α). Let β = A∗
O(√k log n + σε√n log n).

T u, then w.h.p., we have (a)

σε log n for each i and (b)

αi| ≤

βi −
|

k ≤

β

k

T ǫu, then βi −
A∗
i, ǫui
=
Proof. We have β = A∗
α
h
k
•
in Claim 2, we have the claim proved.
α
and
A∗
probability bounds of
k
k
h
•
O(σε log2 n) and have the following
βiβ′i| ≤
V ,
U
We draw from the claim that for any i /
∈

T u = α + A∗
,
i, ǫui

. Using

ǫuk
k

ǫuk
k

αi =

and

−

∩

β

k

|

result:

35

Lemma 16. Fix samples u and v and suppose that y = A∗x∗ + ε is a random sample independent
of u, v. The expected value of the score for the lth component of y is given by:

el , E[
h

y, u

y, v

ih

i

y2
l ] =

2
li + perturbation terms
qiciβiβ′iA∗

where qi = P[i
absolute value at most O∗

S], qij = P[i, j

∈

V
U
Xi
∩
∈
S] and ci = E[x4
i
i |

∈
k/n log2 n max(1/√n, k2/n)

∈

S]. Moreover, the perturbation terms have
.

Proof. Lemma follows Lemma 1 via Claim 3 except that the second term of E1 is bounded by
O(k log2 n/n3/2).

(cid:0)

(cid:1)

Appendix F. Extensions of Arora et al. (2015)

F.1 Sample complexity in noisy case

In this section, we study the sample complexity of the algorithms in Arora et al. (2015) in the
presence of noise. While noise with order σε = O(1/√n) does not change the sample complexity
of the initialization algorithm, it aﬀects that of the descent stage. The analysis involves producing
a sharp bound for

.

gs
,i −
•

gs
ik
•

k

Lemma 17. For a regular dictionary A∗, suppose As is (δs, 2)-near to A∗ with δs = O∗(1/ log n),
then with high probability

k/n)) when p =

(o(δ) + O(

O(k/m)

b

Ω(m + σ2
ε

mn2
k ).

gs
,i −
k
•
Proof. This follows directly from Lemma 15 where r = n.
b

We tighten the original analysis to obtain the complexity

gs
ik ≤
•

·

case. Putting together with p =
mn2
sample complexity
k ) for the algorithms in Arora et al. (2015) in the noise regime.

O(mk + σ2
ε

e

e

p
Ω(mk) for the noiseless
Ω(m) instead of
Ω(mk) required by the initialization, we then have the overall

e

e

F.2 Extension of Arora et al. (2015)’s initialization algorithm for sparse case

e

We study a simple and straightforward extension of the initialization algorithm of Arora et al.
(2015) for the sparse case. This extension is produced by adding an extra projection, and is
described in Figure 3. The recovery of the support of A∗ is guaranteed by the following Lemma:
Lemma 18. Suppose that z∗ ∈
Provided z is δ-close to z∗ and z0 =
z∗ has the same support.

Rn is r-sparse whose nonzero entries are at least τ in magnitude.
Hr(z) with δ = O∗(1/ log n) and r = O∗(log2 n), then z0 and

Proof. Since z0 is δ-close to z∗, then

δ for every i. For i

supp(z∗),

∈

z0
z∗k ≤
−
k
zi −
z∗i | − |
zi| ≥ |
|
δ. Since τ > O(1/√r)

δ and

zi −
|
τ
z∗i | ≥

−

z∗i | ≤
δ

and for i /
≫
∈
support z∗, and hence z0 and z∗ has the same support.

supp(z∗),

zi| ≤
|

δ, then the r-largest entries of z are in the

√n
k log3 n

Theorem 8. Suppose that Assumptions B1-B4 hold and Assumptions A1-A3 satify with µ =
Ω(mk), then with high probability
O∗
Algorithm 3 returns an initial estimate A0 whose columns share the same support as A∗ and with
e
(δ, 2)-nearness to A∗ with δ = O∗(1/ log n).

and r = O∗(log2 n). When p1 =

Ω(m) and p2 =

e

(cid:0)

(cid:1)

36

Algorithm 3 Pairwise Reweighting with Hard-Thresholding

∅

Initialize L =
Randomly divide p samples into two disjoint sets
While
P1 at random
Reconstruct the re-weighted covariance matrix

< m. Pick u and v from

L
|

|

Mu,v:

P1 and

P2 of sizes p1 and p2 respectively

p2

1
p2

y(i), u
h
Xi=1

ih

Mu,v =

c

c
y(i), v

y(i)(y(i))T
i

Compute the top singular values δ1, δ2 and top singular vector z of
If δ1 ≥
z =
If z is not within distance 1/ log n of any vector in L even with sign ﬂip

Hr keeps r largest entries of z

Ω(k/m) and δ2 < O∗(k/m log n)
Hr(z), where
z
L = L
∪ {

Mu,v

c

}

Return A0 = (L1, . . . , Lm)

This algorithm requires r = O∗(log2 n), which is somewhat better than ours. However, the

sample complexity and running time is inferior as compared with our novel algorithm.

Appendix G. Neural Implementation of Our Approach

We now brieﬂy describe why our algorithm is “neurally plausible”. Basically, similar to the argu-
ment in Arora et al. (2015), we describe at a very high level how our algorithm can be implemented
via a neural network architecture. One should note that although both our initialization and de-
scent stages are non-trivial modiﬁcations of those in Arora et al. (2015), both still inherit the nice
neural plausiblity property.

G.1 Neural implementation of Stage 1: Initialization

Recall that the initialization stage includes two main steps: (i) estimate the support of each column
of the synthesis matrix, and (ii) compute the top principal component(s) of a certain truncated
weighted covariance matrix. Both steps involve simple vector and matrix-vector manipulations that
can be implemented plausibly using basic neuronal manipulations.
y, u

For the support estimation step, we compute the product
y, u
i
h

y, followed by a thresh-
y
y, u
i
olding. The inner products,
can be computed using neurons via an online manner
where the samples arrive in sequence; the thresholding can be implemented via a ReLU-type non-
linearity.

and

y, v

ih

◦

h

h

i

For the second step, it is well known that the top principal components of a matrix can be

computed in a neural (Hebbian) fashion using Oja’s Rule Oja (1992).

G.2 Neural implementation of Stage 2: Descent

Our neural implementation of the descent stage (Algorithm 2), shown in Figure 2, mimics the
architecture of Arora et al. (2015), which describes a simple two-layer network architecture for
computing a single gradient update of A. The only diﬀerence in our case is that most of the value

37

y

r

x =
threshold(AT y)

+

+

+

+

+

+

−

−

−

Aij

+

+
+

xj

Hebbian rule
Aij = ηrixj

∇

Figure 2: Neural network implementation of Algorithm 2. The network takes the image y as input
and produces the sparse representation x as output. The hidden layer represents the
residual between the image and its reconstruction Ax. The weights Aij’s are stored on
synapses, but most of them are zero and shown by the dotted lines.

in A are set to zero, or in other words, our network is sparse. The network takes values y from the
input layer and produce x as the output; there is an intermediate layer in between connecting the
middle layer with the output via synapses. The synaptic weights are stored on A. The weights are
updated by Hebbian learning. In our case, since A is sparse (with support given by R, as estimated
in the ﬁrst stage), we enforce the condition the corresponding synapses are inactive. In the output
layer, as in the initialization stage, the neurons can use a ReLU-type non-linear activation function
to enforce the sparsity of x.

References

Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon.
Learning sparsely used overcomplete dictionaries. In Conference on Learning Theory, pages 123–
137, 2014.

Michal Aharon, Michael Elad, and Alfred Bruckstein. k-svd: An algorithm for designing over-
complete dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):
4311–4322, 2006.

Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcom-

plete dictionaries. In Conference on Learning Theory, pages 779–806, 2014.

Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, eﬃcient, and neural algorithms

for sparse coding. In Conference on Learning Theory, pages 113–149, 2015.

38

Jaros law B lasiok and Jelani Nelson. An improved analysis of the er-spud dictionary learning algo-

rithm. arXiv preprint arXiv:1602.05719, 2016.

Y-Lan Boureau, Francis Bach, Yann LeCun, and Jean Ponce. Learning mid-level features for
recognition. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on,
pages 2559–2566. IEEE, 2010.

Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on

information theory, 51(12):4203–4215, 2005.

Niladri Chatterji and Peter Bartlett. Alternating minimization for dictionary learning with random

initialization. 2017. arXiv:1711.03634v1.

Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over

learned dictionaries. IEEE Transactions on Image processing, 15(12):3736–3745, 2006.

Kjersti Engan, Sven Ole Aase, and J Hakon Husoy. Method of optimal directions for frame design. In
IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 5,
pages 2443–2446. IEEE, 1999.

Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of

the 27th International Conference on Machine Learning (ICML), pages 399–406, 2010.

R´emi Gribonval, Rodolphe Jenatton, and Francis Bach. Sparse and spurious: dictionary learning
with noise and outliers. IEEE Transactions on Information Theory, 61(11):6298–6319, 2015a.

R´emi Gribonval, Rodolphe Jenatton, Francis Bach, Martin Kleinsteuber, and Matthias Seibert.
Sample complexity of dictionary learning and other matrix factorizations. IEEE Transactions
on Information Theory, 61(6):3469–3486, 2015b.

Hamid Krim, Dewey Tucker, Stephane Mallat, and David Donoho. On denoising and best signal

representation. IEEE Transactions on Information Theory, 45(7):2225–2238, 1999.

Rados law Adamczak. A note on the sample complexity of the er-spud algorithm by spielman,
wang and wright for exact recovery of sparsely used dictionaries. Journal of Machine Learning
Research, 17:1–18, 2016.

Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for
sparse coding. In Proceedings of the 26th International Conference on Machine Learning (ICML),
pages 689–696, 2009.

Arya Mazumdar and Ankit Singh Rawat. Associative memory using dictionary learning and ex-
pander decoding. In Proc. Conf. American Assoc. Artiﬁcial Intelligence (AAAI), pages 267–273,
2017.

Thanh V. Nguyen, Raymond K. W. Wong, and Chinmay Hegde. A provable approach for double-

sparse coding. In Proc. Conf. American Assoc. Artiﬁcial Intelligence (AAAI), Feb. 2018.

Erkki Oja. Principal components, minor components, and linear neural networks. Neural networks,

5(6):927–935, 1992.

39

Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy

employed by v1? Vision research, 37(23):3311–3325, 1997.

Ron Rubinstein, Alfred M Bruckstein, and Michael Elad. Dictionaries for sparse representation

modeling. Proceedings of the IEEE, 98(6):1045–1057, 2010a.

Ron Rubinstein, Michael Zibulevsky, and Michael Elad. Double sparsity: Learning sparse dictionar-
ies for sparse signal approximation. IEEE Transactions on Signal Processing, 58(3):1553–1564,
2010b.

Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries.

In Conference on Learning Theory, pages 37–1, 2012.

Jeremias Sulam, Boaz Ophir, Michael Zibulevsky, and Michael Elad. Trainlets: Dictionary learning

in high dimensions. IEEE Transactions on Signal Processing, 64(12):3180–3193, 2016.

Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery using nonconvex optimization.
In Proceedings of the 33rd International Conference on Machine Learning (ICML), pages 2351–
2360, 2015.

Lingxiao Wang, Xiao Zhang, and Quanquan Gu. A uniﬁed computational and statistical framework

for nonconvex low-rank matrix estimation. arXiv preprint arXiv:1610.05275, 2016.

Yuchen Zhang, Xi Chen, Dengyong Zhou, and Michael I Jordan. Spectral methods meet em: A
provably optimal algorithm for crowdsourcing. Journal of Machine Learning Research, 17(1):
3537–3580, 2016.

40

7
1
0
2
 
c
e
D
 
2
1

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
8
3
6
3
0
.
1
1
7
1
:
v
i
X
r
a

Provably Accurate Double-Sparse Coding

Thanh V. Nguyen
Iowa State University, ECE Department

Raymond K. W. Wong
Texas A&M University, Statistics Department

Chinmay Hegde ∗
Iowa State University, ECE Department

Editor: TBD

thanhng@iastate.edu

raywong@stat.tamu.edu

chinmay@iastate.edu

Abstract
Sparse coding is a crucial subroutine in algorithms for various signal processing, deep learning,
and other machine learning applications. The central goal is to learn an overcomplete dictionary
that can sparsely represent a given input dataset. However, a key challenge is that storage, trans-
mission, and processing of the learned dictionary can be untenably high if the data dimension is
high. In this paper, we consider the double-sparsity model introduced by Rubinstein et al. (2010b)
where the dictionary itself is the product of a ﬁxed, known basis and a data-adaptive sparse com-
ponent. First, we introduce a simple algorithm for double-sparse coding that can be amenable to
eﬃcient implementation via neural architectures. Second, we theoretically analyze its performance
and demonstrate asymptotic sample complexity and running time beneﬁts over existing (provable)
approaches for sparse coding. To our knowledge, our work introduces the ﬁrst computationally
eﬃcient algorithm for double-sparse coding that enjoys rigorous statistical guarantees. Finally,
we support our analysis via several numerical experiments on simulated data, conﬁrming that our
method can indeed be useful in problem sizes encountered in practical applications.
Keywords: Sparse coding, provable algorithms, unsupervised learning

1. Introduction

1.1 Motivation

Representing signals as sparse linear combinations of atoms from a dictionary is a popular approach
in many domains. In this paper, we study the problem of dictionary learning (also known as sparse
coding), where the goal is to learn an eﬃcient basis (dictionary) that represents the underlying
class of signals well. In the typical sparse coding setup, the dictionary is overcomplete (i.e., the
cardinality of the dictionary exceeds the ambient signal dimension) while the representation is
sparse (i.e., each signal is encoded by a combination of only very few dictionary atoms.)

Sparse coding has a rich history in diverse ﬁelds such as signal processing, machine learning, and
computational neuroscience. Discovering optimal basis representations of data is a central focus of
image analysis (Krim et al., 1999; Elad and Aharon, 2006; Rubinstein et al., 2010a), and dictionary
learning has proven widely successful in imaging problems such as denoising, deconvolution, inpaint-

∗. This work is supported in part by the National Science Foundation under the grants CCF-1566281 and DMS-
1612985. An abbreviated conference version will appear in the proceedings of AAAI 2018 (Nguyen et al., 2018).

1

ing, and compressive sensing (Elad and Aharon, 2006; Candes and Tao, 2005; Rubinstein et al.,
2010a). Sparse coding approaches have also been used as a core building block of deep learn-
ing systems for prediction (Gregor and LeCun, 2010; Boureau et al., 2010) and associative mem-
ory (Mazumdar and Rawat, 2017). Interestingly, the seminal work by Olshausen and Field (1997)
has shown intimate connections between sparse coding and neuroscience: the dictionaries learned
from image patches of natural scenes bear strikingly resemblance to spatial receptive ﬁelds observed
in mammalian primary visual cortex.

From a mathematical standpoint, the sparse coding problem is formulated as follows. Given p
Rn
m (m > n)
data samples Y = [y(1), y(2), . . . , y(p)]
and corresponding sparse code vectors X = [x(1), x(2), . . . , x(p)]
p such that the representation
DX ﬁts the data samples as well as possible. Typically, one obtains the dictionary and the code
vectors as the solution to the following optimization problem:

p, the goal is to ﬁnd a dictionary D

Rm

Rn

∈

∈

∈

×

×

×

min
D,X L

(D, X) =

y(j)
k

−

Dx(j)

2
2,
k

p

1
2

Xj=1
(x(j))

S

≤

s.t.

p

S

Xj=1

(1)

) is some sparsity-inducing penalty function on the code vectors, such as the ℓ1-norm.
(
·
controls the reconstruction error while the constraint enforces the sparsity

However, even a cursory attempt at solving the optimization problem (1) reveals the following

S

where
The objective function
of the representation.

L

obstacles:

1. Theoretical challenges. The constrained optimization problem (1) involves a non-convex
(in fact, bilinear) objective function, as well as potentially non-convex constraints depending
on the choice of the sparsity-promoting function
(for example, the ℓ0 function.) Hence,
obtaining provably correct algorithms for this problem can be challenging. Indeed, the vast
majority of practical approaches for sparse coding have been heuristics (Engan et al., 1999;
Aharon et al., 2006; Mairal et al., 2009); Recent works in the theoretical machine learning
community have bucked this trend, providing provably accurate algorithms if certain assump-
tions are satisﬁed (Spielman et al., 2012; Agarwal et al., 2014; Arora et al., 2015; Sun et al.,
2015; B lasiok and Nelson, 2016; law Adamczak, 2016; Chatterji and Bartlett, 2017). How-
ever, relatively few of these newer methods have been shown to provide good empirical per-
formance in actual sparse coding problems.

S

2. Practical challenges. Even if theoretical correctness issues were to be set aside, and we are
somehow able to eﬃciently learn sparse codes of the input data, we often ﬁnd that applications
using such learned sparse codes encounter memory and running-time issues. Indeed, in the
overcomplete case, only the storage of the learned dictionary D incurs mn = Ω(n2) memory
cost, which is prohibitive when n is large. Therefore, in practical applications (such as image
analysis) one typically resorts to chop the data into smaller blocks (e.g., partitioning image
data into patches) to make the problem manageable.

A related line of research has been devoted to learning dictionaries that obey some type of struc-
ture. Such structural information can be leveraged to incorporate prior knowledge of underlying

2

signals as well as to resolve computational challenges due to the data dimension. For instance, the
dictionary is assumed to be separable, or obey a convolutional structure. One such variant is the
double-sparse coding problem (Rubinstein et al., 2010b; Sulam et al., 2016) where the dictionary
D itself exhibits a sparse structure. To be speciﬁc, the dictionary is expressed as:

D = ΦA,

×

∈

Rn

n, and a learned “synthesis” matrix
i.e., it is composed of a known “base dictionary” Φ
m whose columns are sparse. The base dictionary Φ is typically any ﬁxed basis chosen
A
according to domain knowledge, while the synthesis matrix A is column-wise sparse and is to be
learned from the data. The basis Φ is typically orthonormal (such as the canonical or wavelet
basis); however, there are cases where the base dictionary Φ is overcomplete (Rubinstein et al.,
2010b; Sulam et al., 2016).

∈

×

Rn

There are several reasons why such the double-sparsity model can be useful. First, the double-
sparsity assumption is rather appealing from a conceptual standpoint, since it lets us combine
the knowledge of decades of modeling eﬀorts in harmonic analysis with the ﬂexibility of learning
new representations tailored to speciﬁc data families. Moreover, such a double-sparsity model has
computational beneﬁts. If the columns of A are (say) r-sparse (i.e., each column contains no more
than r
n non-zeroes) then the overall burden of storing, transmitting, and computing with A is
much lower than that for general unstructured dictionaries. Finally, such a model lends itself well
to interpretable learned features if the atoms of the base dictionary are semantically meaningful.

≪

All the above reasons have spurred researchers to develop a series of algorithms to learn
doubly-sparse codes (Rubinstein et al., 2010b; Sulam et al., 2016). However, despite their empiri-
cal promise, no theoretical analysis of their performance have been reported in the literature and
to date, we are unaware of a provably accurate, polynomial-time algorithm for the double-sparse
coding problem. Our goal in this paper is precisely to ﬁll this gap.

1.2 Our Contributions

In this paper, we provide a new framework for double-sparse coding. To the best of our knowledge,
our approach is the ﬁrst method that enjoys provable statistical and algorithmic guarantees for
this problem. In addition, our approach enjoys three beneﬁts: we demonstrate that the method is
neurally plausible (i.e., its execution can plausibly be achieved using a neural network architecture),
robust to noise, as well as practically useful.

Inspired by the aforementioned recent theoretical advances in sparse coding, we assume a
learning-theoretic setup where the data samples arise from a ground-truth generative model. Infor-
mally, suppose there exists a true (but unknown) synthesis matrix A∗ that is column-wise r-sparse,
and the ith data sample is generated as:

y(i) = ΦA∗x∗

(i) + noise,

i = 1, 2, . . . , p

(i) is independently drawn from a distribution supported on the set of k-
where the code vector x∗
sparse vectors. We desire to learn the underlying matrix A∗. Informally, suppose that the synthesis
matrix A∗ is incoherent (the columns of A∗ are suﬃciently close to orthogonal) and has bounded
spectral norm. Finally, suppose that the number of dictionary elements, m, is at most a constant
multiple of n. All of these assumptions are standard1.

1. We clarify both the data and the noise model more concretely in Section 2 below.

3

We will demonstrate that the true synthesis matrix A∗ can be recovered (with small error) in
a tractable manner as suﬃciently many samples are provided. Speciﬁcally, we make the following
novel contributions:

1. We propose a new algorithm that produces a coarse estimate of the synthesis matrix that
is suﬃciently close to the ground truth A∗. In contrast with previous double-sparse coding
methods (such as Sulam et al. (2016)), our algorithm is not based on alternating minimiza-
tion. Rather, it builds upon spectral initialization-based ideas that have recently gained
popularity in non-convex machine learning (Zhang et al., 2016; Wang et al., 2016).

2. Given the above coarse estimate of the synthesis matrix A∗, we propose a descent-style algo-
rithm to reﬁne the above estimate of A∗. This algorithm is simpler than previously studied
double-sparse coding algorithms (such as the Trainlets approach of Sulam et al. (2016)), while
still giving good statistical performance. Moreover, this algorithm can be realized in a manner
amenable to neural implementations.

3. We provide a rigorous analysis of both algorithms. Put together, our analysis produces
the ﬁrst provably polynomial-time algorithm for double-sparse coding. We show that the
algorithm provably returns a good estimate of the ground-truth; in particular, in the absence
of noise we prove that Ω(mr polylog n) samples are suﬃcient for a good enough initialization
in the ﬁrst algorithm, as well as guaranteed linear convergence of the descent phase up to a
precise error parameter that can be interpreted as the radius of convergence.

Indeed, our analysis shows that employing the double-sparsity model helps in this context,
and leads to a strict improvement in sample complexity, as well as running time over previous
rigorous methods for (regular) sparse coding such as Arora et al. (2015).

4. We also analyze our approach in a more realistic setting with the presence of additive noise
and demonstrate its stability. We prove that Ω(mr polylog n) samples are suﬃcient to obtain
a good enough estimate in the initialization, and also to obtain guaranteed linear convergence
during descent to provably recover A∗.

5. We underline the beneﬁt of the double-sparse structure over the regular model by analyzing
the algorithms in Arora et al. (2015) under the noisy setting. As a result, we obtain the
, which demonstrates a negative eﬀect of noise
sample complexity O
on this approach.

mn2
k )polylog n

(mk + σ2
ε

(cid:0)

(cid:1)

6. We rigorously develop a hard thresholding intialization that extends the spectral scheme
in Arora et al. (2015). Additionally, we provide more results for the case where A is orthonor-
mal, sparse dictionary to relax the condition on r, which may be of independent interest.

7. While our analysis mainly consists of suﬃciency results and involves several (absolute) un-
speciﬁed constants, in practice we have found that these constants are reasonable. We justify
our observations by reporting a suite of numerical experiments on synthetic test datasets.

Overall, our approach results in strict improvement in sample complexity, as well as running
time, over previous rigorously analyzed methods for (regular) sparse coding, such as Arora et al.
(2015). See Table 1 for a detailed comparison.

4

Setting

Reference

Sample complexity
(w/o noise)

Sample complexity
(w/ noise)

Upper bound on
running time

Expt

Regular

MOD (Engan et al., 1999)

K-SVD (Aharon et al., 2006)

Spielman et al. (2012)

Arora et al. (2014)

Gribonval et al. (2015a)

Arora et al. (2015)

Double Sparsity (Rubinstein et al., 2010b)

Double
Sparse

Gribonval et al. (2015b)

Trainlets (Sulam et al., 2016)

✗

✗

✗

✗

O(n2 log n)

e
O(m2/k2)

O(nm3)

e
O(mk)

e
O(mr)

e
O(mr)

✗

✗

✗

✗

✗

✗

✗

O(nm3)

e
O(mr)

e
Ω(n4)

e
O(np2)

e
O(mn2p)

✗

✗

✗

✗

✗

✗

✓

✓

✓

✗

✗

✗

✓

✗

✓

✓

This paper

e
O(mr + σ2
ε

mnr
k )

e
O(mnp)

Table 1: Comparison of various sparse coding techniques. Expt: whether numerical experiments have
been conducted. ✗ in all other columns indicates no provable guarantees. Here, n is the signal
dimension, and m is the number of atoms. The sparsity levels for A and x are r and k respectively,
and p is the sample size.

1.3 Techniques

At a high level, our method is an adaptation of the seminal approach of Arora et al. (2015). As
is common in the statistical learning literature, we assume a “ground-truth” generative model for
the observed data samples, and attempt to estimate the parameters of the generative model given
a suﬃcient number of samples. In our case, the parameters correspond to the synthesis matrix A∗,
which is column-wise r-sparse. The natural approach is to formulate a loss function in terms of A
such as Equation (1), and perform gradient descent with respect to the surface of the loss function
to learn A∗.

The key challenge in sparse coding is that the gradient is inherently coupled with the codes of the
training samples (i.e., the columns of X ∗), which are unknown a priori. However, the main insight
of Arora et al. (2015) is that within a small enough neighborhood of A∗, a noisy version of X ∗ can be
estimated, and therefore the overall method is similar to performing approximate gradient descent.
Formulating the actual algorithm as a noisy variant of approximate gradient descent allows us to
overcome the ﬁnite-sample variability of the loss, and obtain a descent property directly related to
(the population parameter) A∗.

The second stage of our approach (i.e., our descent-style algorithm) leverages this intuition.
However, instead of standard gradient descent, we perform approximate projected gradient descent,
such that the column-wise r-sparsity property is enforced in each new estimate of A∗. Indeed, such
an extra projection step is critical in showing a sample complexity improvement over the existing
approach of Arora et al. (2015).The key novelty is in ﬁguring out how to perform the projection in
each gradient iteration. For this purpose, we develop a novel initialization algorithm that identiﬁes
the locations of the non-zeroes in A∗ even before commencing the descent phase. This is nontrivially

5

diﬀerent from initialization schemes used in previous rigorous methods for sparse coding, and the
analysis is somewhat more involved.

In Arora et al. (2015), (the principal eigenvector of) a weighted covariance matrix of y (esti-
mated by the weighted average of outer products yiyT
i ) is shown to provide a coarse estimate of
a dictionary atom. We extend this idea and rigoriously show that the diagonal of the weighted
covariance matrix serves as a good indicator of the support of a column in A∗. The success relies on
the concentration of the diagonal vector with dimension n, instead of the covariance matrix with
n. With the support selected, our scheme only utilizes a reduced weighted covariance
dimensions n
matrix with dimensions at most r
r. This initialization scheme enables us to eﬀectively reduce
the dimension of the problem, and therefore leads to signiﬁcant improvement in sample complexity
and running time over previous (provable) sparse coding methods when the data representation
sparsity k is much smaller than m.

×

×

Further, we rigorously analyze the proposed algorithms in the presence of noise with a bounded
expected norm. Our analysis shows that our method is stable, and in the case of i.i.d. Gaussian noise
with bounded expected ℓ2-norms, is at least a polynomial factor better than previous polynomial
time algorithms for sparse coding.

The empirical performance of our proposed method is demonstrated by a suite of numerical
experiments on synthetic datasets.In particular, we show that our proposed methods are simple
and practical, and improve upon previous provable algorithms for sparse coding.

1.4 Paper Organization

The remainder of this paper is organized as follows. Section 2 introduces notation, key model
assumptions, and informal statements of our main theoretical results. Section 3 outlines our ini-
tialization algorithm (along with supporting theoretical results) while Section 4 presents our descent
algorithm (along with supporting theoretical results). Section 5 provides a numerical study of the
eﬃciency of our proposed algorithms, and compares it with previously proposed methods. Finally,
Section 6 concludes with a short discussion. All technical proofs are relegated to the appendix.

2. Setup and Main Results

∈

= 0
}

}
[m] : xi 6

as the support set of x. Given any subset S

for any integer m > 1. For any vector x = [x1, x2, . . . , xm]T

2.1 Notation
Rm,
We deﬁne [m] ,
1, . . . , m
∈
{
we write supp(x) ,
[m], xS
i
{
m, we
corresponds to the sub-vector of x indexed by the elements of S. For any matrix A
×
i and AT
to represent the i-th column and the j-th row respectively. For some appropriate
use A
j
•
•
S ) be the submatrix of A with rows (respectively columns)
(respectively, A
sets R and S, let AR
•
•
i, we use AR,i
indexed by the elements in R (respectively S). In addition, for the i-th column A
•
to denote the sub-vector indexed by the elements of R. For notational simplicity, we use AT
to
R
indicate (AR
) to represent
and sgn(
·
•
the element-wise Hadamard operator and the element-wise sign function respectively. Further,
thresholdK (x) is a thresholding operator that replaces any elements of x with magnitude less than
K by zero.

)T , the tranpose of A after a row selection. Besides, we use

⊆
Rn

∈

◦

•

x

The ℓ2-norm

A
for a vector x and the spectral norm
for a matrix A appear several
k
k
,
times. In some cases, we also utilize the Frobenius norm
kF and the operator norm
A
k1,2 is essentially the maximal Euclidean norm of any column of A.
A
max
k

. The norm
k

A
k

k1,2

Ax

1k

k1≤

k

k

k

x

k

6

Ω(g(n))) if f (n) is upper bounded (respectively,
constant. Next, f (n) = Θ(g(n)) if and only if f (n) = O(g(n)) and f (n) = Ω(g(n)). Also
and
f (n) = o(g(n)) (or f (n) = ω(g(n))) if limn

For clarity, we adopt asymptotic notations extensively. We write f (n) = O(g(n)) (or f (n) =
lower bounded) by g(n) up to some positive
Ω
O represent Ω and O up to a multiplicative poly-logarithmic factor respectively. Finally
e
f (n)/g(n)
|
Throughout the paper, we use the phrase “with high probability” (abbreviated to w.h.p.) to
ω(1). In addition, g(n) = O∗(f (n))

describe an event with failure probability of order at most n−
means g(n)

Kf (n) for some small enough constant K.

f (n)/g(n)
|

= 0 (limn

→∞ |

→∞ |

∞

=

).

e

≤

2.2 Model

Suppose that the observed samples are given by

y(i) = Dx∗

(i) + ε,

i = 1, . . . , p,

i.e., we are given p samples of y generated from a ﬁxed (but unknown) dictionary D where the sparse
speciﬁed below. In the double-sparse
code x∗ and the error ε are drawn from a joint distribution
n is a known
setting, the dictionary is assumed to follow a decomposition D = ΦA∗, where Φ
orthonormal basis matrix and A∗ is an unknown, ground truth synthesis matrix. An alternative
(and interesting) setting is an overcomplete Φ with a square A∗, which our analysis below does not
cover; we defer this to future work. Our approach relies upon the following assumptions on the
synthesis dictionary A∗:

Rn

D

∈

×

A1 A∗ is overcomplete (i.e., m

n) with m = O(n).

≥

A2 A∗ is µ-incoherent, i.e., for all i

= j,

A∗
i, A∗
j i| ≤
•
•

|h

µ/√n.

A3 A∗
i has at most r non-zero elements, and is normalized such that
•
A∗ij| ≥

= 0 and τ = Ω(1/√r).

τ for A∗ij 6

|

A∗
ik
k
•

= 1 for all i. Moreover,

A4 A∗ has bounded spectral norm such that

A∗k ≤
k

O(

m/n).

p

All these assumptions are standard. In Assumption A2, the incoherence µ is typically of order
O(log n) with high probability for a normal random matrix (Arora et al., 2014). Assumption A3
is a common assumption in sparse signal recovery. The bounded spectral norm assumption is
In addition to Assumptions A1-A4, we make the following
also standard (Arora et al., 2015).
distributional assumptions on

:

D

B1 Support S = supp(x∗) is of size at most k and uniformly drawn without replacement from

[m] such that P[i

S] = Θ(k/m) and P[i, j

S] = Θ(k2/m2) for some i, j

[m] and i

= j.

∈
B2 The nonzero entries x∗S are pairwise independent and sub-Gaussian given the support S with

∈

∈

E[x∗i |
i
B3 For i

∈

S] = 0 and E[x∗
2
i

S] = 1.

i
|

∈

S,

x∗i | ≥
|

∈

C where 0 < C

1.

≤

B4 The additive noise ε has i.i.d. Gaussian entries with variance σ2

ε with σε = O(1/√n).

7

For the rest of the paper, we set Φ = In, the identity matrix of size n. This only simpliﬁes the

arguments but does not change the problem because one can study an equivalent model:

y′ = Ax∗ + ε′,

where y′ = ΦT y and ε′ = ΦT ε, as ΦT Φ = In. Due to the Gaussianity of ε, ε′ also has independent
entries. Although this property is speciﬁc to Gaussian noise, all the analysis carried out below
can be extended to sub-Gaussian noise with minor (but rather tedious) changes in concentration
arguments.

Our goal is to devise an algorithm that produces an provably “good” estimate of A∗. For this,
we need to deﬁne a suitable measure of “goodness”. We use the following notion of distance that
measures the maximal column-wise diﬀerence in ℓ2-norm under some suitable transformation.

Deﬁnition 1 ((δ, κ)-nearness). A is said to be δ-close to A∗ if there is a permutation π : [m]
and a sign ﬂip σ : [m] :
to be (δ, κ)-near to A∗ if

[m]
δ for every i. In addition, A is said

→

σ(i)A
A∗
π(i) −
ik ≤
•
•
also holds.
A∗k

k
κ
k

such that
1
}
{±
A∗k ≤
A
π −
k
•

For notational simplicity, in our theorems we simply replace π and σ in Deﬁnition 1 with the
identity permutation π(i) = i and the positive sign σ(
) = +1 while keeping in mind that in reality
·
we are referring to one element of the equivalence class of all permutations and sign ﬂip transforms
of A∗.

We will also need some technical tools from Arora et al. (2015) to analyze our gradient descent-
Rn to optimize
style method. Consider any iterative algorithm that looks for a desired solution z∗ ∈
some function f (z). Suppose that the algorithm produces a sequence of estimates z1, . . . , zs via
the update rule:

zs+1 = zs

ηgs,

−

for some vector gs and scalar step size η. The goal is to characterize “good” directions gs such that
the sequence converges to z∗ under the Euclidean distance. The following gives one such suﬃcient
condition for gs.
Deﬁnition 2. A vector gs at the sth iteration is (α, β, γs)-correlated with a desired solution z∗ if

gs, zs

h

z∗

−

i ≥

zs

α
k

z∗

2 + β
k

gs
k

2
k

−

−

γs.

We know from convex optimization that if f is 2α-strongly convex and 1/2β-smooth, and gs is
∇zf (z), then gs is (α, β, 0)-correlated with z∗. In our setting, the desired
chosen as the gradient
solution corresponds to A∗, the ground-truth synthesis matrix. In Arora et al. (2015), it is shown
that gs = Ey[(Asx
y)sgn(x)T ], where x = thresholdC/2((As)T y) indeed satisﬁes Deﬁnition 2. This
gs is a population quantity and not explicitly available, but one can estimate such gs using an
gs is a random variable, so we also need a related
empirical average. The corresponding estimator
correlated-with-high-probability condition:

−

Deﬁnition 3. A direction
tion z∗ if, w.h.p.,

gs at the sth iteration is (α, β, γs)-correlated-w.h.p. with a desired solu-

gs, zs

b
h

z∗

−

i ≥

zs

α
k

z∗

2 + β
k

gs
k

2
k

−

−

γs.

From Deﬁnition 2, one can establish a form of descent property in each update step, as shown

b

b

in Theorem 1.

b

8

Theorem 1. Suppose that gs satisﬁes the condition described in Deﬁnition 2 for s = 1, 2, . . . , T .
Moreover, 0 < η

s=1 γs. Then, the following holds for all s:

2β and γ = maxT

≤

zs+1

k

z∗

2
k

−

(1

2αη)
k

−

≤

zs

z∗

2 + 2ηγs.
k

−

In particular, the above update converges geometrically to z∗ with an error γ/α. That is,

−
We can obtain a similar result for Deﬁnition 3 except that

−

−

≤

k

zs+1
k

2

z∗

(1

2αη)s

z0
k

z∗

2 + 2γ/α.
k

expectation.

zs+1

k

z∗k

−

2 is replaced with its

Armed with the above tools, we now state some informal versions of our main results:

Theorem 2 (Provably correct initialization, informal). There exists a neurally plausible algorithm
to produce an initial estimate A0 that has the correct support and is (δ, 2)-near to A∗ with high
probability.
O(mr) respectively. This
algorithm works when the sparsity level satisﬁes r = O∗(log n).

Its running time and sample complexity are

O(mnp) and

e

e

Our algorithm can be regarded as an extension of Arora et al. (2015) to the double-sparse
setting. It reconstructs the support of one single column and then estimates its direction in the
subspace deﬁned by the support. Our proposed algorithm enjoys neural plausibility by implement-
ing a thresholding non-linearity and Oja’s update rule. We provide a neural implementation of our
algorithm in Appendix G. The adaption to the sparse structure results in a strict improvement
upon the original algorithm both in running time and sample complexity. However, our algorithm
is limited to the sparsity level r = O∗(log n), which is rather small but plausible from the modeling
standpoint. For comparison, we analyze a natural extension of the algorithm of Arora et al. (2015)
with an extra hard-thresholding step for every learned atom. We obtain the same order restriction
on r, but somewhat worse bounds on sample complexity and running time. The details are found
in Appendix F.

We hypothesize that a stronger incoherence assumption can lead to provably correct initial-
ization for a much wider range of r. For purposes of theoretical analysis, we consider the special
case of a perfectly incoherent synthesis matrix A∗ such that µ = 0 and m = n. In this case, we
, which is an exponential
can indeed improve the sparsity parameter to r = O∗
improvement. This analysis is given in Appendix E.

min( √n
log2 n

n
k2 log2 n

)

,

(cid:0)

(cid:1)

Theorem 3 (Provably correct descent, informal). There exists a neurally plausible algorithm for
double-sparse coding that converges to A∗ with geometric rate when the initial estimate A0 has the
correct support and (δ, 2)-near to A∗. The running time per iteration is O(mkp + mrp) and the
sample complexity is

O(m + σ2
ε

mnr
k ).

e

Similar to Arora et al. (2015), our proposed algorithm enjoys neural plausibility. Moreover, we
can achieve a better running time and sample complexity per iteration than previous methods,
particularly in the noisy case. We show in Appendix F that in this regime the sample complexity
O(m + σ2
1/2, the sample complexity
of Arora et al. (2015) is
ε
O(m) in the noiseless case. In contrast, our proposed method
bound is signiﬁcantly worse than
leverages the sparse structure to overcome this problem and obtain improved results.

k ). For instance, when σε ≍

n−

mn2

e

We are now ready to introduce our methods in detail. As discussed above, our approach consists
of two stages: an initialization algorithm that produces a coarse estimate of A∗, and a descent-style
algorithm that reﬁnes this estimate to accurately recover A∗.

e

9

Algorithm 1 Truncated Pairwise Reweighting

∅

Initialize L =
Randomly divide p samples into two disjoint sets
While
P1 at random

< m. Pick u and v from

For every l = 1, 2, . . . , n; compute

L
|

|

P1 and

P2 of sizes p1 and p2 respectively

p2

el =

1
p2

y(i), u
h

ih

y(i), v

(y(i)

l )2

i

e(r′) < O∗(r/ log2 n)

Xi=1
b
en) in descending order

e2, . . . ,

e(r′+1)/

e(r′) ≥
b

Sort (
e1,
If r′ ≤
O(k/mr) and
r s.t
R be set of the r largest entries of
Let
b
b
(y(i)
y(i)
Mu,v = 1
y(i), v
b
b
b
b
b
p2
i
R
R
b
Mu,v
top singular values of
δ1, δ2 ←
c
z bR ←
If δ1 ≥

top singular vector of

Ω(k/m) and δ2 < O∗(k/m log n)

Mu,v
c

p2
i=1h

y(i), u

e
)T
b

P

c

ih

If dist(

z, l) > 1/ log n for any l

L

∈

±

Update L = L

z
∪ {

}

Return A0 = (L1, . . . , Lm)

3. Stage 1: Initialization

In this section, we present a neurally plausible algorithm that can produce a coarse initial estimate
of the ground truth A∗. We give a neural implementation of the algorithm in Appendix G.

Our algorithm is an adaptation from the algorithm in Arora et al. (2015). The idea is to estimate
dictionary atoms in a greedy fashion by iteratively re-weighting the given samples. The samples
are re-scaled in a way that the weighted (sample) covariance matrix has the dominant ﬁrst singular
value, and its corresponding eigenvector is close to one particular atom with high probability.
However, while this algorithm is conceptually very appealing, it incurs severe computational costs
O(mn2p) in expectation, which is unrealistic
in practice. More precisely, the overall running time is
for large-scale problems.

e

To overcome this burden, we leverage the double-sparsity assumption in our generative model to
obtain a more eﬃcient approach. The high-level idea is to ﬁrst estimate the support of each column
in the synthesis matrix A∗, and then obtain a coarse estimate of the nonzero coeﬃcients of each
column based on knowledge of its support. The key ingredient of our method is a novel spectral
procedure that gives us an estimate of the column supports purely from the observed samples. The
full algorithm, that we call Truncated Pairwise Reweighting, is listed in pseudocode form below as
Algorithm 1.

Let us provide some intuition of our algorithm. Fix a sample y = A∗x∗ + ε from the available

training set, and consider samples

u = A∗α + εu, v = A∗α′ + εv.

10

Now, consider the (very coarse) estimate for the sparse code of u with respect to A∗:

β = A∗

T u = A∗

T A∗α + A∗

T εu.

y, u
h

x∗, β

x∗, α
i

.

i ≈ h

i ≈ h

As long as A∗ is incoherent enough and εu is small, the estimate β behaves just like α, in the sense
that for each sample y:

Moreover, the above inner products are large only if α and x∗ share some elements in their supports;
depends on whether or not x∗
else, they are likely to be small. Likewise, the weight
shares the support with both α and α′.

y, u
h

y, v

ih

i

i

ih

y, v

y, u
h

Now, suppose that we have a mechanism to isolate pairs u and v who share exactly one atom
among their sparse representations. Then by scaling each sample y with an increasing function
of
and linearly adding the samples, we magnify the importance of the samples that
are aligned with that atom, and diminish the rest. The ﬁnal direction can be obtained via the
top principal component of the reweighted samples and hence can be used as a coarse estimate of
the atom. This is exactly the approach adopted in Arora et al. (2015). However, in our double-
sparse coding setting, we know that the estimated atom should be sparse as well. Therefore, we
can naturally perform an extra “sparsiﬁcation” step of the output. An extended algorithm and
its correctness are provided in Appendix F. However, as we discussed above, the computational
complexity of the re-weighting step still remains.

We overcome this obstacle by ﬁrst identifying the locations of the nonzero entries in each atom.

Speciﬁcally, deﬁne the matrix:

Mu,v =

p2

1
p2

y(i), u
h
Xi=1

ih

y(i), v

y(i)

y(i).

i

◦

Then, the diagonal entries of Mu,v reveals the support of the atom of A∗ shared among u and v: the
r-largest entries of Mu,v will correspond to the support we seek. Since the desired direction remains
unchanged in the r-dimensional subspace of its nonzero elements, we can restrict our attention to
Mu,v, and proceed as before. This truncation
this subspace, construct a reduced covariance matrix
O(mnp),
step alleviates the computational burden by a signiﬁcant amount; the running time is now
which improves the original by a factor of n.

c

The success of the above procedure relies upon whether or not we can isolate pairs u and v that
share one dictionary atom. Fortunately, this can be done via checking the decay of the singular
values of the (reduced) covariance matrix. Here too, we show via our analysis that the truncation
step plays an important role. Overall, our proposed algorithm not only accelerates the initialization
in terms of running time, but also improves the sample complexity over Arora et al. (2015). The
performance of Algorithm 1 is described in the following theorem, whose formal proof is deferred
to Appendix B.

e

√n
k log3 n

Theorem 4. Suppose that Assumptions B1-B4 hold and Assumptions A1-A3 satify with µ =
Ω(mr), then with high probability
O∗
Algorithm 1 returns an initial estimate A0 whose columns share the same support as A∗ and with
(δ, 2)-nearness to A∗ with δ = O∗(1/ log n).

and r = O∗(log n). When p1 =

Ω(m) and p2 =

e

e

(cid:0)

(cid:1)

The limit on r arises from the minimum non-zero coeﬃcient τ of A∗. Since the columns of A∗
are standardized, τ should degenerate as r grows. In other words, it is getting harder to distinguish

11

the “signal” coeﬃcients from zero as r grows with n. However, this limitation can be relaxed when
a better incoherence available, for example the orthonormal case. We study this in Appendix E.

To provide some intuition about the working of the algorithm (and its proof), let us analyze it in
the case where we have access to inﬁnite number of samples. This setting, of course, is unrealistic.
However, the analysis is much simpler and more transparent since we can focus on expected values
rather than empirical averages. Moreover, the analysis reveals several key lemmas, which we will
reuse extensively for proving Theorem 4. First, we give some intuition behind the deﬁnition of the
“scores”,

el.

Lemma 1. Fix samples u and v and suppose that y = A∗x∗ + ε is a random sample independent
of u, v. The expected value of the score for the lth component of y is given by:

b

el , E[
h

y, u

y, v

ih

i

y2
l ] =

2
li + perturbation terms
qiciβiβ′iA∗

where qi = P[i
absolute value at most O∗(k/m log n).

S], qij = P[i, j

∈

∈

V
U
Xi
∩
∈
S] and ci = E[x4
i
i |

∈

S]. Moreover, the perturbation terms have

i

From Assumption B1, we know that qi = Θ(k/m), qij = Θ(k2/m2) and ci = Θ(1). Besides,
= o(1) for i /
U . Consider the ﬁrst
∈
or that l does not belong to support
V =
τ .

= Ω(1) for i
we will show later that
βi| ≈ |
|
2
li . Clearly, E0 = 0 if U
V qiciβiβ′iA∗
term E0 =
U
∩
∈
of any atom in U
Ω(τ 2k/m) = Ω(k/mr) since

αi|
∈
V . On the contrary, as E0 6
qiciβiβ′i| ≥
|

βi|
|
V =
∩
∅
= 0 and U
∩
A∗li| ≥
|

Therefore, Lemma 1 suggests that if u and v share a unique atom among their sparse represen-
tations, and r is not too large, then we can indeed recover the correct support of the shared atom.
When this is the case, the expected scores corresponding to the nonzero elements of the shared
atom will dominate the remaining of the scores.

2
qiciβiβ′iA∗
li | ≥
|

, then E0 =

Ω(k/m) and

U , and

i
}
{

P

∩

Now, given that we can isolate the support R of the corresponding atom, the remaining questions
are how best we can estimate its non-zero coeﬃcients, and when u and v share a unique elements
in their supports. These issues are handled in the following lemmas.

Lemma 2. Suppose that u = A∗α + εu and v = A∗α′ + εv are two random samples. Let U and
V denote the supports of α and α′ respectively. R is the support of some atom of interest. The
truncated re-weighting matrix is formulated as

Mu,v , E[
y, u
h

ih

y, v

yRyT

R] =

i

T
R,i + perturbation terms
qiciβiβ′iA∗R,iA∗

V
U
Xi
∩
∈

where the perturbation terms have norms at most O∗(k/m log n).

Using the same argument for bounding E0 in Lemma 1, we can see that M0 , qiciβiβ′iA∗R,iA∗
T
R,i
has norm at least Ω(k/m) when u and v share a unique element i (
= 1). According to this
k
lemma, the spectral norm of M0 dominates those of the other perturbation terms. Thus, given R
i.
we can use the ﬁrst singular vector of Mu,v as an estimate of A∗
•

A∗R,ik

Lemma 3. Under the setup of Theorem 4, suppose u = A∗α + εu and v = A∗α′ + εv are two
random samples with supports U and V respectively. R = supp(A∗i ). If u and v share the unique
atom i, the ﬁrst r largest entries of el is at least O(k/mr) and belong to R. Moreover, the top
singular vector of Mu,v is δ-close to A∗R,i for O∗(1/ log n).

12

i’s support directly follows Lemma 1. For the latter part, recall from
Proof. The recovery of A∗
•
Lemma 2 that

T
R,i + perturbation terms
Mu,v = qiciβiβ′iA∗R,iA∗

The perturbation terms have norms bounded by O∗(k/m log n). On the other hand, the ﬁrst term
is has norm at least Ω(k/m) since
Ω(k/m).
Then using Wedin’s Theorem to Mu,v, we can conclude that the top singular vector must be
O∗(k/m log n)/Ω(k/m) = O∗(1/ log n) -close to A∗R,i.

= 1 for the correct support R and

qiciβiβ′i| ≥

A∗R,ik

k

|

Lemma 4. Under the setup of Theorem 4, suppose u = A∗α+εu and v = A∗α′ +εv are two random
samples with supports U and V respectively. If the top singular value of Mu,v is at least Ω(k/m) and
the second largest one is at most O∗(k/m log n), then u and v share a unique dictionary element
with high probability.

Proof. The proof follows from that of Lemma 37 in Arora et al. (2015). The main idea is to
separate the possible cases of how u and v share support and to use Lemma 2 with the bounded
perturbation terms to conclude when u and v share exactly one. We note that due to the condition
where
O∗(r/ log n), it must be the case that u and v share only
one atom or share more than one atoms with the same support. When their supports overlap more
than one, then the ﬁrst singular value cannot dominate the second one, and hence it must not be
b
the case.

Ω(k/mr) and

e(s) ≤

e(s) ≥

e(s+1)/

b

b

Similar to (Arora et al., 2015), our initialization algorithm requires

tation to estimate all the atoms, hence the expected running time is
Lemma 1 and 2 are deferred to Appendix B.

O(m) iterations in expec-
O(mnp). All the proofs of

e

e

4. Stage 2: Descent

We now adapt the neural sparse coding approach of Arora et al. (2015) to obtain an improved
estimate of A∗. As mentioned earlier, at a high level the algorithm is akin to performing approximate
gradient descent. The insight is that within a small enough neighborhood (in the sense of δ-
closeness) of the true A∗, an estimate of the ground-truth code vectors, X ∗, can be constructed
using a neurally plausible algorithm.

The innovation, in our case, is the double-sparsity model since we know a priori that A∗ is itself
sparse. Under suﬃciently many samples, the support of A∗ can be deduced from the initialization
stage; therefore we perform an extra projection step in each iteration of gradient descent. In this
sense, our method is non-trivially diﬀerent from Arora et al. (2015). The full algorithm is presented
as Algorithm 2.

As discussed in Section 2, convergence of noisy approximate gradient descent can be achieved
gs is correlated-w.h.p. with the true solution. However, an analogous convergence result
as long as
for projected gradient descent does not exist in the literature. We ﬁll this gap via a careful analysis.
gs (i.e., when it
Due to the projection, we only require the correlated-w.h.p. property for part of
is restricted to some support set) with A∗. The descent property is still achieved via Theorem 5.
(A, X); therefore, we can
Due to various perturbation terms,
∇AL
k/n). The performance
only reﬁne the estimate of A∗ until the column-wise error is of order O(
of Algorithm 2 can be characterized via the following theorem.
b

g is only a biased estimate of

b

b

p

13

Algorithm 2 Double-Sparse Coding Descent Algorithm

Initialize A0 is (δ, 2)-near to A∗. H = (hij )n
Repeat for s = 0, 1, . . . , T

×

m where hij = 1 if i

supp(A0
j) and 0 otherwise.
•

∈

Encode: x(i) = thresholdC/2((As)T y(i))
PH(As
Update: As+1 =
−
p
i=1(Asx(i)
where

gs = 1
p

η

η

gs)
gs) = As
−
y(i))sgn(x(i))T and
−
b
b

for i = 1, 2, . . . , p
PH (

PH (G) = H

◦

G

P

b

Theorem 5. Suppose that the initial estimate A0 has the correct column supports and is (δ, 2)-near
to A∗ with δ = O∗(1/ log n). If Algorithm 2 is provided with p =
Ω(mr) fresh samples at each step
and η = Θ(m/k), then

E[
As
i −
k
•

A∗
ik
•

2]

(1

ρ)s

A0
i −
•

A∗
ik
•

k
for some 0 < ρ < 1/2 and for s = 1, 2, . . . , T . Consequently, As converges to A∗ geometrically until
column-wise error O(

k/n).

p

≤

−

e
2 + O(

k/n)

p

We defer the full proof of Theorem 5 to Section D. In this section, we take a step towards
gs in the inﬁnite sample case, which is equivalent to its
y)sgn(x)T ]. We establish the (α, β, γs)-correlation of a truncated version

understanding the algorithm by analyzing
expectation gs , E[(Asx
of gs
i with A∗
i to obtain the descent in Theorem 6 for the inﬁnite sample case.
•
•

Theorem 6. Suppose that the initial estimate A0 has the correct column supports and is (δ, 2)-near
to A∗. If Algorithm 2 is provided with inﬁnite number of samples at each step and η = Θ(m/k),
then

−

b

ρ)
k
for some 0 < ρ < 1/2 and for s = 1, 2, . . . , T . Consequently, it converges to A∗ geometrically until
column-wise error is O(k/n).

A∗
ik
•

A∗
ik
•

(1

≤

−

k

(cid:0)

(cid:1)

As+1
i −
•

2 + O

k2/n2

As
i −
•

2

Note that the better error O(k2/n2) is due to the fact that inﬁnitely many samples are given.
k/n) in Theorem 5 is a trade-oﬀ between the accuracy and the sample complexity
The term O(
of the algorithm. The proof of this theorem composes of two steps with two main results: 1) an
explicit form of gs (Lemma 6); 2) (α, β, γs)-correlation of column-wise gs with A∗ (Lemma 6). The
proof of those lemmas are deferred to Appendix C. Since the correlation primarily relies on the
(δ, 2)-nearness of As to A∗ that is provided initially and maintained at each step, then we need to
argue that the nearness is preserved after each step.

p

Lemma 5. Suppose that the initial estimate A0 has the correct column supports and is (δ, 2)-near to
A∗. The column-wise update has the form gs
R,i = piqi(λs
ζ) where R = supp(As
i),
•
λs
i =

A∗R,i + ξs

i As

i ±

and

As
i, A∗
ii
h
•
•

i = As
ξs
R,

idiag(qij)(As
•−

R,i −
i)T A∗
i/qi.
•

−
Moreover, ξi has norm bounded by O(k/n) for δ = O∗(1/ log n) and ζ is negligible.

We underline that the correct support of As allows us to obtain the closed-form expression of
gs
i. Likewise, the expression (8) suggests that gs
Ri,i in terms of As
i is almost equal to
i and A∗
•
•
•
i) (since λs
piqi(As
i. With Lemma 5, we will
1), which directs to the desired solution A∗
A∗
i ≈
i −
•
•
•
i and the nearness
prove the (α, β, γs)-correlation of the approximate gradient to each column A∗
•
of each new update to the true solution A∗.

14

4.1 (α, β, γs)-Correlation
Lemma 6. Suppose that As to be (δ, 2)-near to A∗ and R = supp(A∗
i), then 2gs
•
correlated with A∗R,i; that is

R,i is (α, 1/2α, ǫ2/α)-

A∗R,ii ≥
Futhermore, the descent is achieved by

R,i −

R,i, As

2gs
h

As
α
k

R,i −

A∗R,ik

2 + 1/(2α)

2
gs
R,ik
k

−

ǫ2/α

As+1
i −
k
•

2

(1

≤

−

2αη)s

A0
i −
k
•

A∗
ik
•

2 + ηǫ2

s/α

where δ = O∗(1/ log n) and ǫ = O

.

A∗
ik
•
k2
mn

(cid:0)

(cid:1)

Proof. Throughout the proof, we omit the superscript s for simplicity and denote 2α = piqi. First,
i as a combination of the true direction As
we rewrite gs
i −
•
•

i and a term with small norm:
A∗
•

gR,i = 2α(AR,i −

A∗R,i) + v,

i + ǫi] with norm bounded. In fact, since A
where v = 2α[(λi −
i, and both
i is δ-close to A∗
1)A
•
•
•
A
2α(λi −
have unit norm, then
A∗
A
O(k/n) from
i −
i −
ik
•
•
•
the inequality (9). Therefore,

1)A
ik
•

ξik ≤

= α
k

A∗
ik
•

α
k

and

≤

k

k

2

v
k

k

=

2α(λi −

1)AR,i + 2αξik ≤

k

α
k

AR,i −

A∗R,ik

+ ǫ

where ǫ = O(k2/mn). Now, we make use of (2) to show the ﬁrst part of Lemma 6:

2gR,i, AR,i −

h

A∗R,ii

= 4α
k

AR,i −

A∗R,ik

2v, AR,i −

h

.
A∗R,ii

2 +

We want to lower bound the inner product term with respect to
Eﬀectively, from (2)

2 and

gRi,ik

k

AR,i −
k

2.
A∗R,ik

4α
h

v, A
i −
•

A∗
ii
•

=

2

2

gR,ik
k
gR,ik

−

−

4α2
6α2

AR,i −
AR,i −

k

k

≥ k

2

v
k
− k
2ǫ2,

−

2

2

A∗R,ik
A∗R,ik
2(α2

2

where the last step is due to Cauchy-Schwarz inequality:

v
k
in (3) for the right hand side of (4), we get the ﬁrst result:

AR,i −
k

A∗R,ik

≤

k

2 + ǫ2).

Substitute 2
h

v, A
i −
•

A∗
ii
•

2gR,i, AR,i −
h

A∗R,ii ≥

α
k

AR,i −

A∗R,ik

2 +

1
2α k

2

gR,ik

−

ǫ2
α

.

(2)

(3)

(4)

The second part is directly followed from Theorem 1. Moreover, we have pi = Θ(k/m) and qi =
Θ(1), then α = Θ(k/m), β = Θ(m/k) and γs = O(k3/mn2). Then gs
correlated with the true solution A∗R,i.

R,i is (Ω(k/m), Ω(m/k), O(k3/mn2))-

Proof of Theorem 6. The descent in Theorem 6 directly follows from the above lemma. Next, we
will establish the nearness for the update at step s:

15

2,000
Sample size

4,000

2,000
Sample size

4,000

2,000
Sample size

4,000

e
t
a
r

y
r
e
v
o
c
e
R

e
t
a
r

y
r
e
v
o
c
e
R

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

r
o
r
r
e

n
o
i
t
c
u
r
t
s
n
o
c
e
R

r
o
r
r
e

n
o
i
t
c
u
r
t
s
n
o
c
e
R

8

6

4

2

0

8

6

4

2

0

0

0

Ours
Arora
Arora+HT
Trainlets

Ours
Arora
Arora+HT
Trainlets

e
m

i
t

g
n

i

n
n
u
R

e
m

i
t

g
n

i

n
n
u
R

4

3

2

1

0

6

4

2

0

0

0

2,000
Sample size

4,000

2,000
Sample size

4,000

2,000
Sample size

4,000

Figure 1: (top) The performance of four methods on three metrics (recovery rate, reconstruction error and
running time) in sample size in the noiseless case. (bottom) The same metrics are measured for
the noisy case.

4.2 Nearness

Lemma 7. Suppose that As is (δ, 2)-near to A∗, then

As+1

k

A∗k ≤

2
k

A∗k

−

Proof. From Lemma 5 we have gs
i = piqi(λiAs
i) + A
A∗
i −
•
•
•
idiag(qij)AT
R, then it is obvious that gs
iA∗
[n]
¯R,i = A ¯R,
i ±
\
•−
•
follows the proof of Lemma 24 in (Arora et al., 2015) for the nearness with full gs = gs
ﬁnish the proof for this lemma.

ζ. Denote ¯R =
idiag(qij)AT
iA∗
i ±
•−
•
•−
ζ is bounded by O(k2/m2). Then we
¯R,i to

R,i + gs

−

In sum, we have shown the descent property of Algorithm 2 in the inﬁnite sample case. The
gs around its mean to the sample complexity is provided in Section D.

study of the concentration of
In the next section, we corroborate our theory by some numerical results on synthetic data.

5. Empirical Study

b

We compare our method with three diﬀerent methods for both standard sparse and double-sparse
coding. For the standard approach, we implement the algorithm proposed in Arora et al. (2015),
which currently is the best theoretically sound method for provable sparse coding. However, since
their method does not explicitly leverage the double-sparsity model, we also implement a heuristic
modiﬁcation that performs a hard thresholding (HT)-based post-processing step in the initializa-
tion and learning procedures (which we dub Arora + HT ). The ﬁnal comparison is the Trainlets
approach of Sulam et al. (2016).

16

2 block is of form [1 1; 1

We generate a synthetic training dataset according to the model described in Section 2. The
base dictionary Φ is the identity matrix of size n = 64 and the square synthesis matrix A∗ is a
block diagonal matrix with 32 blocks. Each 2
1] (i.e., the column
×
sparsity r = 2) . The support of x∗ is drawn uniformly over all 6-dimensional subsets of [m],
In our simulations
and the nonzero coeﬃcients are randomly set to
with noise, we add Gaussian noise ε with entrywise variance σ2
ε = 0.01 to each of those above
samples. For all the approaches except Trainlets, we use T = 2000 iterations for the initialization
procedure, and set the number of steps in the descent stage to 25. Since Trainlets does not have
a speciﬁed initialization procedure, we initialize it with a random Gaussian matrix upon which
column-wise sparse thresholding is then performed. The learning step of Trainlets2 is executed for
50 iterations, which tolerates its initialization deﬁciency. For each Monte Carlo trial, we uniformly
draw p samples, feed these samples to the four diﬀerent algorithms, and observe their ability to
reconstruct A∗. Matlab implementation of our algorithms is available online3.

1 with equal probability.

±

−

We evaluate these approaches on three metrics as a function of the number of available samples:
(i) fraction of trials in which each algorithm successfully recovers the ground truth A∗; (ii) recon-
struction error; and (iii) running time. The synthesis matrix is said to be “successfully recovered”
if the Frobenius norm of the diﬀerence between the estimate
A and the ground truth A∗ is smaller
4 in the noiseless case, and to 0.5 in the other. All three
than a threshold which is set to 10−
metrics are averaged over 100 Monte Carlo simulations. As discussed above, the Frobenius norm is
only meaningful under a suitable permutation and sign ﬂip transformation linking
A and A∗. We
estimate this transformation using a simple maximum weight matching algorithm. Speciﬁcally, we
construct a weighted bipartite graph with nodes representing columns of A∗ and
A and adjacency
is taken element-wise. We compute the optimal matching
matrix deﬁned as G =
using the Hungarian algorithm, and then estimate the sign ﬂips by looking at the sign of the inner
products between the matched columns.

, where
A
|

T
A∗
|

|·|

b

b

b

b

The results of our experiments are shown in Figure 1 with the top and bottom rows respectively
for the noiseless and noisy cases. The two leftmost ﬁgures suggest that all algorithms exhibit a
“phase transitions” in sample complexity that occurs in the range of 500-2000 samples.
In the
noiseless case, our method achieves the phase transition with the fewest number of samples. In the
noisy case, our method nearly matches the best sample complexity performance (next to Trainlets,
which is a heuristic and computationally expensive). Our method achieves the best performance
in terms of (wall-clock) running time in all cases.

6. Conclusion

In this paper, we have addressed an open theoretical question on learning sparse dictionaries under
a special type of generative model. Our proposed algorithm consists of a novel initialization step
followed by a descent-style step, both are able to take advantage of the sparse structure. We rigor-
ously demonstrate its eﬃcacy in both sample- and computation-complexity over existing heuristics
as well as provable approaches for double-sparse and regular sparse coding. This results in the ﬁrst
known provable approach for double-sparse coding problem with statistical and algorithmic guaran-
tees. Besides, we also show three beneﬁts of our approach: neural plausibility, robustness to noise
and practical usefulness via the numerical experiments.

2. We utilize Trainlets’s implementation provided at http://jsulam.cswp.cs.technion.ac.il/home/software/.
3. https://github.com/thanh-isu/double-sparse-coding

17

Nevertheless, several fundamental questions regarding our approach remain. First, our initial-
ization method (in the overcomplete case) achieves its theoretical guarantees under fairly stringent
limitations on the sparsity level r. This arises due to our reweighted spectral initialization strategy,
and it is an open question whether a better initialization strategy exists (or whether these types
of initialization are required at all). Second, our analysis holds for complete (ﬁxed) bases Φ, and
it remains open to study the setting where Φ is over-complete. Finally, understanding the reasons
behind the very promising practical performance of methods based on heuristics, such as Trainlets,
on real-world data remains a very challenging open problem.

18

Appendix A. Auxiliary Lemma

Claim 1 (Maximal row ℓ1-norm). Given that
Θ(

m/n).

A∗k

k

p

Proof. Recall the deﬁnition of the operator norm:

2
F = m and

A∗k
k

= O(

m/n), then

T
A∗

k

k1,2 =

p

T
A∗
k

k1,2 = sup

=0

x

AT x
k
x

k

k

sup
=0
x

AT x
k
k
x
k1 ≤
k
A∗kF /√n =

k

=

T
A∗

k

k

= O(

m/n).

p

2
F = m,

T
A∗

k
m/n).

k1,2 ≥ k

Since
T
A∗
k

A∗k
k
k1,2 = Θ(
p
Along with Assumptions A1 and A3, the above claim implies the number of nonzero entries
in each row is O(r). This Claim is an important ingredient in our analysis of our initialization
algorithm shown in Section 3.

m/n. Combining with the above, we have

p

Appendix B. Analysis of Initialization Algorithm

B.1 Proof of Lemma 1

The proof of Lemma 1 can be divided into three steps: 1) we ﬁrst establish useful properties of β
with respect to α; 2) we then explicitly derive el in terms of the generative model parameters and β;
and 3) we ﬁnally bound the error terms in E based on the ﬁrst result and appropriate assumptions.

Claim 2. In the generative model,

x∗k ≤
k

O(√k) and

ε
k ≤

k

O(σε√n) with high probability.

Proof. The claim directly follows from the fact that x∗ is a k-sparse random vector whose nonzero
entries are independent sub-Gaussian with variance 1. Meanwhile, ε has n independent Gaussian
entries of variance σ2
ε .

e

e

Despite its simplicity, this claim will be used in many proofs throughout the paper. Note also
that in this section we will calculate the expectation over y and often refer probabilistic bounds
(w.h.p.) under the randomness of u and v.

Claim 3. Suppose that u = A∗α + εu is a random sample and U = supp(α). Let β = A∗
O(√k + σε√n).
w.h.p., we have (a)

√n + σε log n for each i and (b)

µk log n

βi −
|

αi| ≤

β
k

k ≤

T u, then,

Proof. The proof mostly follows from Claim 36 of Arora et al. (2015), with an additional consider-
and observe that
ation of the error εu. Write W = U

e

=

αi|

βi −
|

i εu| ≤ |h
A∗
•
T
k/n. Moreover, αW has k
µ
Since A∗ is µ-incoherence, then
i A∗
A∗
W k ≤
•
•
T
Gaussian entries of variance 1, therefore
i, αW i| ≤
W A∗
A∗
p
|h
•
•
that εu has independent Gaussian entries of variance σ2
variance (
k
µk log n

1 independent sub-
√n with high probability. Also recall
T
ε , then A∗
i εu is Gaussian with the same
•
αi| ≤

σε log n with high probability. Consequently,

= 1). Hence

i, αW i|

T
i ε
A∗
|
•

i, εui|

A∗
ik
•

βi −

µk log n

| ≤

−

+

|h

k

|

T
W A∗
A∗
•
•

√n + σε log n, which is the ﬁrst part of the claim.

Next, in order to bound

, we express β as

β
k
k
T A∗
U αU + A∗
•

β

k

k

=

A∗
k

T εuk ≤ k

A∗

A∗
U kk
•

αU k

kk

+

A∗
k

εuk

kk

i
\{
}
T
T
W αW + A∗
i A∗
A∗
|
•
•
•

19

O(√k) and

A∗k ≤

αU k ≤
k

O(1) , we complete the proof for the second part.

Using Claim 2 to get
εuk ≤
k
A∗
U k ≤ k
k
•
Claim 3 suggests that the diﬀerence between βi and αi is bounded above by O∗(1/ log2 n)
U and
o(1)
∈
O(√k) w.h.p.

w.h.p. if µ = O∗( √n
βi| ≤
|
We will use these results multiple times in the next few proofs.

O∗(1/ log2 n) otherwise. On the other hand, under Assumption B4,

O(σε√n) w.h.p., and further noticing that

). Therefore, w.h.p., C

O(log m) for i

βi| ≤ |

+ o(1)

k log3 n

αi|

β
k

k ≤

≤ |

≤

−

e

e

Proof of Lemma 1. We decompose dl into small parts so that the stochastic model

is made use.

y, v

el = E[
y, u
h
ih
= E
x∗, β
h
ih
= E1 + E2 +
(cid:2)(cid:8)

i
+ E9

· · ·

l ] = E[
y2
A∗x∗ + ε, u
A∗l
(
h
h
·
T (βvT + β′uT )ε + uT εεT v
+ x∗

A∗x∗ + ε, v

i
x∗, β′

ih

i

, x∗

+ ε)2]

i
, x∗

A∗l
h
•

2 + 2
h

A∗l
•

i

, x∗

εl + εl

i

(cid:9)(cid:8)

(cid:9)(cid:3)

where the terms are

e

D

i

ih
, x∗

2]
, x∗
i
εl]
, x∗

A∗l
•
A∗l
ih
•
ε2
l ]
i
T (βvT + β′uT )ε
T (βvT + β′uT )εεl
(cid:3)

x∗, β′
ih
x∗, β′

E1 = E[
x∗, β
ih
h
E2 = 2E[
x∗, β
h
ih
E3 = E[
x∗, β
x∗, β′
h
2x∗
E4 = E
A∗l
i
h
·
E5 = E
x∗
A∗l
(cid:2)
i
h
·
E6 = E
(βvT + β′uT )εε2
(cid:2)
l
2]
E7 = E[uT εεT v
A∗l
(cid:2)
(cid:3)
h
•
E8 = 2E[uT εεT v
εl]
A∗l
h
i
•
E9 = E[uT εεT vε2
l ]

i
, x∗

, x∗

, x∗

(cid:3)

(5)

Because x∗ and ε are independent and zero-mean, E2 and E4 are clearly zero. Moreover,

E6 = (βvT + β′uT )E[εε2

l ] = 0

due to the fact that E[εjε2

l ] = 0, for j

= l, and E[ε3

l ] = 0. Also,

We bound the remaining terms separately in the following claims.

T
E8 = A∗
l
•

E[x∗]E

uT εεT vεl

= 0.

(cid:2)

(cid:3)

Claim 4. In the decomposition (5), E1 is of the form

E1 =

2
li +
qiciβiβ′iA∗

2
li +
qiciβiβ′iA∗

2
lj + 2βiβ′jA∗liA∗lj)
qij(βiβ′iA∗

V
U
Xi
∩
∈

V
U
Xi /
∩
∈

=i
Xj

where all those terms except

li have magnitude at most O∗(k/m log2 n) w.h.p.
2
V qiciβiβ′iA∗
∩

i

U

∈

P

20

Proof. Using the generative model in Assumptions B1-B4, we have

E1 = E[
x∗, β
h
= ES
E

x∗

ih
S[

x∗, β′

A∗l
•

ih
βix∗i

, x∗

2]

i
β′ix∗i

A∗lix∗i

2]

|

(cid:2)

S
Xi
∈
2
li +
qiciβiβ′iA∗

S
Xi
∈

S

(cid:0) Xi

(cid:1)
∈
2
lj + 2βiβ′jA∗liA∗lj)
qij(βiβ′iA∗

(cid:3)

=

=

[m]
Xi
∈

[m],j
Xi,j
∈

=i

2
li +
qiciβiβ′iA∗

2
li +
qiciβiβ′iA∗

2
lj + 2βiβ′jA∗liA∗lj),
qij(βiβ′iA∗

Xi /
V
U
∩
∈
S], qij = P[i, j

V
U
Xi
∈
∩
where we have used the qi = P[i
S] and Assumptions
B1-B4. We now prove that the last three terms are upper bounded by O∗(k/m log n). The key
observation is that all these terms typically involve a quadratic form of the l-th row A∗l
whose
•
norm is bounded by O(1) (by Claim 1 and Assumption A4). Moreover,
is relatively small for
V )
(U
[m]
U
i /
\
∈
to bound

V while qij = Θ(k2/m2). For the second term, we apply the Claim 3 for i

Xj
=i
S] and ci = E[x4
i
i |

= 0, then with high probability

βiβ′i|
|

∈

∈

∈

∩

∩

∈

|

βiβ′i|

. Assume αi = 0 and α′i 6
βiβ′i| ≤ |
|
Using the bound qici = Θ(k/m), we have w.h.p.,

αi)(β′i −

(βi −

α′i)
|

+

βiα′i| ≤
|

O∗(1/ log n)

2
qiciβiβ′iA∗
li

max
i

qiciβiβ′i|
|

≤

2
A∗
li ≤

max
i

qiciβiβ′i|k
|

A∗

2
1,2 ≤

k

O∗(k/m log n).

V
U
Xi /
(cid:12)
∩
∈
(cid:12)
(cid:12)
For the third term, we make use of the bounds on
β′k ≤

V
U
Xi /
∩
∈

(cid:12)
(cid:12)
(cid:12)

β
k

kk

O(k) w.h.p., and on qij = Θ(k2/m2). More precisely, w.h.p.,

β
k

k

and

β′k
k

from the previous claim where

=i
Xj
(cid:12)
(cid:12)
(cid:12)

2
qijβiβ′iA∗
lj

e

=

βiβ′i

2
qijA∗
lj

(cid:12)
(cid:12)
(cid:12)

≤

Xi
(cid:12)
(cid:12)
(cid:12)
(max
=j
i

=i
Xj

Xi

qij)

βiβ′i|
|

|
Xi
2
A∗
lj

≤

(cid:12)
(cid:12)
(cid:12)
(cid:16)Xj

βiβ′i|

≤

(cid:17)

=i

(cid:0)Xj
(max
=j
i

2
qijA∗
lj

(cid:1)
β

qij)
k

β′

A∗

kk

kk

2
1,2 ≤

k

O(k3/m2),

e

where the second last inequality follows from the Cauchy-Schwarz inequality. For the last term, we
= j and
write it in a matrix form as
(Qβ)ij = 0 for i = j. Then

T
=i qijβiβ′jA∗liA∗lj = A∗
l
•

where (Qβ)ij = qijβiβ′j for i

QβA∗l
•

j

where
Qβk
k
mately,

2
F =

=j q2

i

(maxi

=j q2
ij)
k

β

2
k

β′k
k

≤

2. Ulti-

P
T
A∗
l
|
•
i (β′j )2
ijβ2

QβA∗l

•| ≤ k

(maxi

≤

A∗l

Qβkk
=j q2
ij)

2

•k

≤ k
i β2
i

QβkF k
j(β′j)2

A∗

2
1,2,

k

qijβiβ′jA∗liA∗lj

(max
=j
i

≤

P
β
qij)
k

kk

β′

A∗

P
kk

2
1,2 ≤
k

O(k3/m2).

P

=i
Xj
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Under Assumption k = O∗( √n
O∗(k/m log2 n). As a result, the two terms
above are bounded by the same amount O∗(k/m log n) w.h.p., so we complete the proof of the
claim.

log n ), then

O(k3/m2)

≤

e

e

21

Claim 5. In the decomposition (5),

E5|
|
S] = 1 and qi = P[i

,
E3|
|

,

Proof. Recall that E[x2
i |
E3 = E[
h

∈
l ] = σ2
ε2
ε
i

x∗, β

x∗, β′

ih

E7|
|

and

E9|

|

is at most O∗(k/m log2 n).

S] = Θ(k/m) for S = supp(x∗), then

ES

E

x∗

S[

|

βiβ′jx∗i x∗j ]

S
Xi,j
∈

(cid:3)

(cid:2)
σ2
ε qiβiβ′i

= σ2
ε

ES[

βiβ′i] =

S
Xi
∈

Xi

Denote Q = diag(q1, q2, . . . , qm), then
O(k3/mn) where we have used
β
k
handle the seventh term before E5:
e

k ≤

E7 = E[uT εεT v

, x∗

A∗l
•

h

2] = E[
h

A∗l
•

i

=

Q

σ2
σ2
ε k2/m) =
Qβ, β′i| ≤
E3|
ε k
ε h
|
|
O(√k) w.h.p. and σε ≤
O(1/√n). For convenience, we
e
, x∗

β′k ≤

O(σ2

u, v

u, v

kk

qiA2

QAl

kk

β

e

2]uT E[εεT ]v =
i

li = σ2
ε h

AT
l
•

i

σ2
ε h

i

•

Xi

O(√k) w.h.p. Consequently,

To bound this term, we use Claim 9 in Appendix D to have
2
u, v
h
•k
O(m/n) and σε ≤
e

σ2
ε k
O(1/√n). Now, the ﬁrth term E5 is expressed as follows

u
k
k
u, v
|h

E7| ≤
|

A∗α + εuk ≤
k
O(k2/mn) because
i| ≤

i ≤

Al

kk

Q

=

e

e

O(√k) w.h.p. and

Al

2
•k

k

≤

, x∗

x∗
i
T
x∗x∗

E5 = E
A∗l
h
·
E
T
= A∗
(cid:2)
l
•
T
= σ2
Q(vlβ + ulβ′)
ε A∗
(cid:3)
(cid:2)
l
•

T (βvT + β′uT )εεl
(βvT + β′uT )E[εεl]
(cid:3)

Observe that
β
2
k ≤
kk
k
bounded by

u

The last term

σ2
ε k

Q(vlβ + ulβ′)
k ≤
u
k ≤
k

T
A∗
E5| ≤
l
• kk
|
O(k) w.h.p. using the result
O(k2/mn).
e
e

e

T
σ2
Q
A∗
ε k
l
• kk
O(k) and

vlβ + ulβ′k
kk
β
k ≤
k

and that

vlβ + ulβ′k ≤
k
O(k) from Claim 3, then E5

E9 = E[uT εεT vε2

l ] = uT E

εεT ε2
l

v = 9σ4
ε h

because the independent entries of ε and E[ε4
Since m = O(n) and k
and

O(k2/n2).
E7|
,
E5|
|
|
e
E9|
|
Combining the bounds from Claim 4, 5 for every single term in (5), we ﬁnish the proof for
Lemma 1.

v
kk
log n ), we obtain the same bound O∗(k/m log2 n) for

, and conclude the proof of the claim.

k ≤
,
E3|
|

O∗( √n

9σ4
ε k

≤

u

|

(cid:2)
l ] = 9σ4

(cid:3)
ε . Therefore,

e

u, v

i
E9| ≤

B.2 Proof of Lemma 2

We prove this lemma by using the same strategy used to prove Lemma 1.

yRyT
R]
A∗x∗ + ε, v

i

y, v

Mu,v , E[
y, u
h
ih
= E[
A∗x∗ + ε, u
h
= E
h
= M1 +
(cid:2)(cid:8)

ih
x∗, β′
ih
+ M8,

x∗, β

· · ·

i

(A∗R
•

x∗ + εR)(A∗R
•

i

+ x∗

T (βvT + β′uT )ε + uT εεT v

x∗ + εR)T ]
A∗R
•

(cid:9)(cid:8)

22

x∗x∗

T
T A∗
R

+ A∗R
•

•

x∗εT

R + εRx∗

T
T A∗
R

+ εRεT
R

•

(cid:9)(cid:3)

in which only nontrivial terms are kept in place, including

T
T A∗
R

]

•

x∗εT
R]
T
T A∗
R

]

•

i

ih

ih

x∗x∗

x∗, β′

A∗R
•
εRεT
R]

M1 = E[
x∗, β
h
M2 = E[
x∗, β
x∗, β′
h
i
M3 = E[x∗
T (βvT + β′uT )εA∗R
•
M4 = E[x∗
T (βvT + β′uT )εεRx∗
M5 = E[uT εεT vA∗R
T
T A∗
]
x∗x∗
R
•
M6 = E[uT εεT vA∗R
x∗εT
R]
•
M7 = E[uT εεT vεT
T
T A∗
Rx∗
R
M8 = E[uT εεT vεRεT
R]

•

•

]

(6)

By swapping inner product terms and taking advantage of the independence, we can show that
M6 = E[A∗R
] = 0. The remaining are bounded in
the next claims.

R] = 0 and M7 = E[uT εεT vεT

x∗uT εεT vεT

T
T A∗
R

Rx∗

•

•

Claim 6. In the decomposition (6),

M1 =

T
R,i + E′1 + E′2 + E′3
qiciβiβ′iA∗R,iA∗

Xi
V
U
∩
∈
T
V qiciβiβ′iA∗R,iA∗
R,i, E′2 =
∩

where E′1 =
T
β′iA∗R,iβjA∗
R,j) have norms bounded by O∗(k/m log n).
P

P

i /
∈

U

i

T
=j qijβiβ′iA∗R,jA∗
R,j and E′3 =

T
=j qij(βiA∗R,iβ′jA∗
R,j+

i

P

Proof. The expression of M1 is obtained in the same way as E1 is derived in the proof of Lemma
1. To prove the claim, we bound all the terms with respect to the spectral norm of A∗R
and make
•
use of Assumption A4 to ﬁnd the exact upper bound.
T
(U
R,S where S = [m]
For the ﬁrst term E′1, rewrite E′1 = A∗R,SD1A∗
\
qiciβiβ′i| ≤
D1k ≤
S|
∈

V ) and D1 is a diagonal
∩
O∗(k/m log n) as shown in

matrix whose entries are qiciβiβ′i. Clearly,
Claim 4, then

maxi

k

E′1k ≤

k

max
S |
i
∈

qiciβiβ′i|k

A∗R,Sk

2

≤

max
S |
i
∈

qiciβiβ′i|k

A∗R

2
•k

≤

O∗(k/m log n)

O(1). The second term E′2 is a sum of positive semideﬁnite matrices,

where
A∗R,Sk ≤ k
k
β
and
k ≤
k

A∗R
•k ≤
O(k log n), then

E′2 =

T
qijβiβ′iA∗R,jA∗
R,j (cid:22)

max
=j
i

qij

βiβ′i

=j
Xi

(cid:17)(cid:16)Xj
2
which implies that
A∗R
•k
T
form as the last term in Claim 4, which is E′3 = A∗
R
•

=j qij)
k

E′2k ≤

β′kk

(cid:16)Xi

(maxi

kk

β

k

T
A∗R,jA∗
R,j

(max
=j
i

β

qij)
k

β′

A∗R
k
•

kk

(cid:22)

T
A∗
R

•

O(k3/m2). Observe that E′3 has the same

E′3k ≤ k
k

Qβkk

A∗R

2
•k

(max
=j
i

qij)
k

≤

2

A∗R

•k

≤

O(k3/m2)

(cid:17)

. Then

≤
QβA∗R
•
e
β′

β

kk

kk

By Claim 3, we have

β

k

k

and

β′k

k

then we complete the proof for Lemma 6.

are bounded by O(√k log n), and note that k

O∗(√n/ log n),

e

≤

23

Claim 7. In the decomposition (6), M2, M3, M4, M5 and M8 have norms bounded by O∗(k/m log n).

Proof. Recall the deﬁnition of Q in Claim 5 and use the fact that E[x∗x∗
E[
x∗, β
ε qiβiβ′iIr. Then,
R] =
h
The next three terms all involve A∗R
•

σ2
ε maxi qik

x∗, β′i
ih

β′k ≤
whose norm is bounded according to Assumption A4.

T ] = Q, we can get M2 =
ε k2 log2 n/m).

M2k ≤

O(σ2

εRεT

i σ2

P

kk

β

k

Speciﬁcally,

M3 = E[x∗
= A∗R
•
= A∗R
•

x∗εT

R] = E[A∗R
•
T ](βvT + β′uT )E[εεT
R]

T (βvT + β′uT )εA∗R
•
E[x∗x∗
Q(βvT + β′uT )E[εεT

R],

x∗x∗

T (βvT + β′uT )εεT
R]

and

M4 = E[x∗

T (βvT + β′uT )εεRx∗

] = E[εRεT (vβT + uβ′

T )x∗x∗

T
T A∗
R

]

•

= E[εRεT ](vβT + uβ′
= E[εRεT ](vβT + uβ′

T A∗
T
R
•
T )E[x∗x∗
T
T ]A∗
R
T
T )QA∗
,
R

•

and the ﬁfth term M5 = E[uT εεT vA∗R
E[εεT
We already have
R]
k
k
remaining work is to bound
A∗uvT
βvT
A∗kk
k
k
k
O(σ2
in norm by
≤
The remaining term is

•
E[x∗x∗
T A∗
T
ε uT vA∗R
= σ2
.
R
•
•
•
uT v
Q
O(k) (proof of Claim 9), then the
O(k/m) and
k ≤
k
|
T directly follows. We have
, then the bound of vβT + uβ′
βvT + β′uT
k
k
v
u
O(k). Therefore, all three terms M3, M4 and M5 are bounded
kk
O(k3/mn).
e

ε uT vA∗R
•

k ≤ k
ε k2/m)

T
T ]A∗
R

] = σ2

= σ2
ε ,

T
QA∗
R

x∗x∗

k ≤

| ≤

=

e

•

•

e

e

M8 = E[uT εεT vεRεT

R] = E[

uivjεiεj

εRεT
R]

= E[

uiviε2

i εRεT
R

(cid:0)Xi
R
∈
ε uRvT
R

= σ4

(cid:0)Xi,j
] + E[

(cid:1)
uivjεiεj

εRεT
R]

(cid:1)

=j

(cid:0)Xi

(cid:1)

where uR = A∗R
•
O(√k). Therefore,
bound all the above terms by O∗(k/m log n) and ﬁnish the proof of Claim 7.
e

α + (εu)R and vR = A∗R
α′ + (εv)R. We can see that
•
O(k3/n2). Since m = O(n) and k
ε k) =

e
Combine the results of Claim 6 and 7, we complete the proof of Lemma 2.

uRk ≤ k
k
≤

M 8
k

O(σ4

k ≤

e

+

A∗R
(εu)Rk ≤
•kk
k
O∗( √n
log n ), then we can

α
k

Appendix C. Analysis of Main Algorithm

C.1 Simple Encoding

y)sgn(x)T is random over y and x that is obtained from the encoding step.
We can see that (Asx
We follow (Arora et al., 2015) to derive the closed form of gs = E[(Asx
y)sgn(x)T ] by proving
that the encoding recovers the sign of x∗ with high probability as long as As is close enough to A∗.

−

−

Lemma 8. Assume that As is δ-close to A∗ for δ = O(r/n log n) and µ
then with probability over random samples y = A∗x∗ + ε

≤

√n
2k , and k

≥

Ω(log m)

sgn(thresholdC/2

(As)T y

= sgn(x∗)

(7)

(cid:0)
24

(cid:1)

Proof of Lemma 8. We follow the same proof strategy from (Arora et al., 2015) (Lemmas 16 and
17) to prove a more general version in which the noise ε is taken into account. Write S = supp(x∗)
and skip the superscript s on As for the readability. What we need is to show S =
[m] :
S with high probability. Following
) = sgn(x∗i ) for each i
i, y
A
h
i
•
the same argument of (Arora et al., 2015), we prove in below a stronger statement that, even
i, y
A
conditioned on the support S, S =
h
•

and then sgn(
h

with high probability.

As
i, y
•

C/2
}

C/2
}

[m] :

i ≥

i ≥

i
{

i
{

∈

∈

∈

Rewrite

i, y
A
h
•

i

=

i, A∗x∗ + ε
A
i
h
•

=

i, A∗
A
ii
•
•

h

x∗i +

i, A∗
A
j i
•
•

x∗j +

i, ε
A
i
h
•

,

h
Xj
=i

and observe that, due to the closeness of A
i, the ﬁrst term is either close to x∗i or equal to
i and A∗
•
•
S. Meanwhile, the rest are small due to the incoherence and the
0 depending on whether or not i
concentration in the weighted average of noise. We will show that both Zi =
x∗j
and

i, A∗
A
ji
•
•

i
\{

}h

∈

S

i, ε
A
h
i
•
The cross-term Zi =

are bounded by C/8 with high probability.
i, A∗
A
j i
•
•
dom variables, which is another sub-Gaussian random variable with variance σ2
Note that

i
\{

P

}h

S

x∗j is a sum of zero-mean independent sub-Gaussian ran-

S

i
\{

}h

i, A∗
A
j i
•
•

2.

Zi =

P

i, A∗
A
ji
h
•
•

A
i −
h
•
where we use Cauchy-Schwarz inequality and the µ-incoherence of A∗. Therefore,

i, A∗
A∗
j i
•
•

A
i −
•

2µ2/n + 2
h

2
i, A∗
A∗
ji
•
•

2 +

≤

≤

2

(cid:0)

(cid:1)

h

2

P
i, A∗
A∗
ji
•
•

2,

σ2
Zi ≤

T
2µ2k/n + 2
S (A
A∗
i −
k
•
•
√n
2k , to conclude 2µ2k/n

≤

under µ
Applying Bernstein’s inequality, we get
. In fact,
bound the noise term
a sub-Gaussian with variance σ2
Notice that σε = O(1/√n).

i, ε
A
i
h
•

≤

i)
A∗
k
•

2
F ≤

2µ2k/n + 2
k

A∗
Sk
•

2

A
i −
•

k

2
A∗
ik
•

≤

O(1/ log n),

O(1/ log n) we need 1/k = O(1/ log n), i.e. k = Ω(log n).
C/8 with high probability. What remains is to
is sum of n Gaussian random variables, which is
σε log n with high probability.

Zi| ≤
|
i, ε
A
i
h
•
ε . It is easy to see that

i, ε
A
•

|h

i| ≤

Finally, we combine these bounds to have
i, y
A
•

S, then
i, ε
A
h
•
C/2 and negligible otherwise. Using union bound for every i = 1, 2, . . . , m, we ﬁnish

C/4. Therefore, for i

Zi +
|

i| ≤

∈

|h
the proof of the Lemma.

i| ≥

Lemma 8 enables us to derive the expected update direction gs = E[(Asx

y)sgn(x)T ] explicitly.

−

C.2 Approximate Gradient in Expectation

−

Proof of Lemma 5. Having the result from Lemma 8, we are now able to study the expected update
y)sgn(x)T ]. Recall that As is the update at the s-th iteration and x ,
direction gs = E[(Asx
thresholdC/2((As)T y). Based on the generative model, denote pi = E[x∗i sgn(x∗i )
S]
i
|
and qij = P[i, j
S]. Throughout this section, we will use ζ to denote any vector whose norm is
negligible although they can be diﬀerent across their appearances. A
i denotes the sub-matrix of
A whose i-th column is removed. To avoid overwhelming appearance of the superscript s, we skip
it from As for neatness. Denote
Fx∗ is the event under which the support of x is the same as that
of x∗, and ¯
Fx∗ = 1[sgn(x) = sgn(x∗)] and 1
Fx∗ = 1.

Fx∗ is its complement. In other words, 1

S], qi = P[i

Fx∗ + 1 ¯

∈

∈

∈

−

i = E[(Ax
gs
•

−

y)sgn(xi)] = E[(Ax

y)sgn(xi)1

−

Fx∗ ]

±

ζ

25

Fx∗ we have Ax = A

S AT
Using the fact that y = A∗x∗ + ε and that under
Sy =
SxS = A
•
•
•
SAT
SAT
A
SA∗x∗ + A
Sε. Using the independence of ε and x∗ to get rid of the noise term, we get
•
•
•
•
i = E[(A
S AT
gs
S −
•
•
•
= E[(A
S AT
S −
•
•
= E[(A
S AT
S −
•
•
= E[(A
S AT
S −
•
•

S AT
S −
•
•
ζ
(Independence of ε and x’s)
±
1 ¯
Fx∗ )]

In)A∗x∗sgn(xi)1
In)A∗x∗sgn(x∗i )(1
In)A∗x∗sgn(x∗i )]

Fx∗ ] + E[(A
Fx∗ ]
−
ζ

Fx∗ event)

In)εsgn(xi)1

In)A∗x∗1

(Under

Fx∗ ]

±

±

±

ζ

ζ

Recall from the generative model assumptions that S = supp(x∗) is random and the entries of x∗
are pairwise independent given the support, so

i = ESE
gs
x∗
•
= piES,i
= piES,i

|

S AT
S[(A
S −
•
•
S AT
S[(A
S −
•
∈
•
iAT
S[(A
i −
•
∈
•

In)A∗x∗sgn(x∗i )]
i]
In)A∗
ζ
±
•
i] + piES,i
In)A∗
•

S[
∈

ζ

±

iAT
= piqi(A
i −
•
•

i + pi
In)A∗
•

= piqi(λiA
i −
•

i) + piA
A∗
•

•−

Xl
=i
S,l
∈
lAT
lA∗
qilA
i ±
•
•
•

ζ

[m],l
Xl
=i
∈
idiag(qij)AT
•−

iA∗
i ±
•

ζ

lAT
i]
lA∗
A
•
•
•

±

ζ

where λs
expression of the expected approximate gradient at iteration s:

idiag(qij)AT
•−

As
i, A∗
ii
h
•
•

i = AR,

. Let ξs

i =

−

iA∗
i/qi for j = 1, . . . , m, we now have the full
•

(8)

(9)

R,i = piqi(λiAs
gs

A∗R,i + ξs
i )

R,i −

ζR.

±

What remains is to bound norms of ξs and ζ. We have
ξs
A∗i k
along with the fact that
i k
k
qij
ξs
i k ≤ k
qi k
k

= 1, we can bound

max
=i
j

As

ik

Ri,

k

−

As
−

ik ≤

O(k/n).

As
R,

k

ik ≤ k

−

As
−

ik ≤

O(

m/n) w.h.p. Then,

p

Next, we show that norm of ζ is negligible. In fact,
it suﬃces to bound norm of (Ax
Section D. This concludes the proof for Lemma 5.

−

Fx∗ happens with very high probability, then
y)sgn(xi) which will be done using Lemma 12 and Lemma 11 in

Appendix D. Sample Complexity

In previous sections, we rigorously analyzed both initialization and learning algorithms as if the
expectations gs, e and Mu,v were given. Here we show that corresponding estimates based on
empirical means are suﬃcient for the algorithms to succeed, and identify how may samples are
required. Technically, this requires the study of their concentrations around their expectations.
Having had these concentrations, we are ready to prove Theorems 4 and 5.

The entire section involves a variety of concentration bounds. Here we make heavy use of
Bernstein’s inequality for diﬀerent types of random variables (including scalar, vector and matrix).
The Bernstein’s inequality is stated as follows.

26

Lemma 9 (Bernstein’s Inequality). Suppose that Z (1), Z (2), . . . , Z (p) are p i.i.d. samples from some
almost surely and
distribution

σ2 for each j, then

. If E[Z] = 0,

E[Z (j)(Z (j))T

Z (j)

k ≤

D

k

k ≤ R
p

(10)

k
σ2
p

(cid:19)

+

R
p

s

1
p

Z (j)

≤

O

e

(cid:18)

(cid:13)
(cid:13)
(cid:13)

Xj=1
(cid:13)
(cid:13)
(cid:13)

holds with probability 1

n−

ω(1).

−

Since all random variables (or their norms) are not bounded almost surely in our model setting,

(log(1/ρ))C ]

≤

we make use of a technical lemma that is used in Arora et al. (2015) to handle the issue.
Lemma 10 (Arora et al. (2015)). Suppose a random variable Z satisﬁes P[
k
ρ for some constant C > 0, then
(a) If p = nO(1), it holds that
(b)
e
Ω(

) for each j with probability 1

k
ω(1).

k ≥ R

= n−

ω(1).

Z (j)

k ≤

n−

O(

R

−

Z

Z

E[Z1
k
k

k≥

)]
k
R

e

p
This lemma suggests that if 1
i=1 Z (j)(1
)) concentrates around its mean with
p
p
high probability, then so does 1
i=1 Z (j) because the part outside the truncation level can be
P
p
ignored. Since all random variables of our interest are sub-Gaussian or a product of sub-Gaussian
that satisfy this lemma, we can apply Lemma 9 to the corresponding truncated random variables
with carefully chosen truncation levels. Then the original random variables concentrate likewise.

1
k

Z (j)

P

e
Ω(

−

k≥

R

In the next proofs, we deﬁne suitable random variables and identify good bounds of

and
σ2 for them. Note that in this section, the expectations are taken over y by conditioning on u
and v. This aligns with the construction that the estimators of e and Mu,v are empirical averages
over i.i.d. samples of y, while u and v are kept ﬁxed. Due to the dependency on u and v, these
(conditional) expectations inherit randomness from u and v, and we will formulate probabilistic
bounds for them.

R

The application of Bernstein’s inequality requires a bound on

eΩ(
k≥
achieve that by the following technical lemma, where ˜Z is a standardized version of Z.
Lemma 11. Suppose a random variable ˜Z ˜Z T = aT where a
They are both random. Suppose P[a

> 0 is a constant. Then,

ω(1) and

1
k

−

≥

Z

] = n−

E[ZZ T (1
k

0 and T is positive semi-deﬁnite.

. We

))]
k

R

E[ ˜Z ˜Z T (1

k

≥ A
1
˜Z
k

−

)]

k≥B

k ≤ Ak

B
E[T ]
k

+ O(n−

ω(1))

Proof. To show this, we make use of the decomposition ˜Z ˜Z T = aT and a truncation for a. Specif-
ically,

E[ ˜Z ˜Z T (1
k

1
k

˜Z

−

)]
k

k≥B

T (1

1
˜Z
k

−

k≥B

)]
k

≥A

˜Z

)]

k≥B

)]
k
T

≥Ak

1
˜Z
k

k≥B

k≥B
)T (1

≥A

1
˜Z
−
k
+ E[a1a
)T ]
k
≥A
2(1
E[
aT
k
k
4(1
˜Z
E[
k
k
P[a

−
1
˜Z
k
1/2

−
]

2

k≥B

+

E[a1a
k
1
(1
˜Z
k
−
k
)]E[1a
)]P[a

≥A
]

]

≥ A

(cid:1)

)]

k≥B
1/2

1/2
(cid:1)

= E[aT (1
E[a(1
E[a(1

≤ k

≤ k

−

−

1
k
1a

1a

−
E[T ]
k
E[T ]
k
E[T ]
k
E[T ]
k

≤ Ak

≤ Ak

≤ Ak

≤ Ak

+

+

(cid:0)

+

(cid:0)
B
+ O(n−

(cid:0)

≥ A
ω(1)),

(cid:1)

27

where at the third step we used T (1
semi-deﬁnite and 1

1
k

˜Z

−

k≥B ∈ {

0, 1
}

1
k

˜Z

)]

T because of the fact that T is the positive

−
. Then, we ﬁnish the proof of the lemma.

k≥B

(cid:22)

D.1 Sample Complexity of Algorithm 1

e and the reduced weighted covariance matrix
Mu,v depends upon
b
R
u,v. We will show that we only need
e, we denote it by
c
O(m) samples to be able to recover the support of one particular atom and up to some speciﬁed

In Algorithm 1, we empirically compute the “scores”
Mu,v to produce an estimate for each column of A∗. Since the construction of
the support estimate
c
p =
level of column-wise error with high probability.

R given by ranking

c

M

b

b

b

e

Lemma 12. Consider Algorithm 1 in which p is the given number of samples. For any pair u and
v, then with high probability a)
O∗(k/m log n) when p =
sets of one particular atom.
e

u,vk ≤
R and R are respectively the estimated and correct support
e

e
e
k ≤
k
Ω(mr) where

O∗(k/m log2 n) when p =

Ω(m) and b)

b
R
u,v −

M
k

M R

c

−

b

b

D.1.1 Proof of Theorem 4

Using Lemma 12, we are ready to prove the Thereom 4. According to Lemma 1 when U
we can write

e as

∩

V =

,
i
}
{

b

e = qiciβiβ′iA∗R,i ◦

e
A∗R,i + perturbation terms + (

e),

−

e as an additional perturbation with the same magnitude O∗(k/m log2 n) in the
and consider
e
−
w.h.p. The ﬁrst part of Lemma 3 suggests that when u and v share exactly one atom
sense of
k · k∞
e is the same as supp(A∗i ) with high probability.
R including r largest elements of
i, then the set
b

b

b

Once we have
b

R, we again write

M

b
R
u,v using Lemma 2 as
b

b
b
c
T
R
R,i + perturbation terms + (
u,v = qiciβiβ′iA∗R,iA∗
M

M

b
R
u,v −

M R

u,v),

b
c
R
u,v −

M

and consider
in the sense of the spectral norm
singular vectors of

c

M

M R

c
u,v as an additional perturbation with the same magnitude O∗(k/m log n)
w.h.p. Using the second part of Lemma 3, we have the top

b
R
u,v is O∗(1/ log n) -close to A∗R,i with high probability.

k · k

Since every vector added to the list L in Algorithm 1 is close to one of the dictionary, then
A0 must be δ-close to A∗. In addition, the nearness ofA0 to A∗ is guaranteed via an appropriate
A close to A0 and
projection onto the convex set
. Finally, we ﬁnish the
A
|
{
proof of Theorem 4.

A∗k}

A
k

2
k

k ≤

c

=

B

D.1.2 Proof of Lemma 12, Part a

y, v

[n], consider p i.i.d. realizations Z (1), Z (2), . . . , Z (p) of the random variable
For some ﬁxed l
∈
Z ,
O∗(k/m log2 n)
y2
l , then
y, u
e
h
k∞ ≤
holds with high probability, we ﬁrst study the concentration for the l-th entry of
e and then
e
and its variance E[Z 2]
take the union bound over all l = 1, 2, . . . , n. We derive upper bounds for
b
|
in order to apply Bernstein’s inequality in (12) to the truncated version of Z.

i=1 Z (i) and el = E[Z]. To show that

el = 1
p

P

ih

−

−

Z

b

k

e

i

p

|

b

O(k) and E[Z 2]

O(k2/m) with high probability.

Claim 8.

Z
|

| ≤

e

≤

e

28

Again, the expectation is taken over y by conditioning on u and v, and therefore is still random
due to the randomness of u and v. To show Claim 8, we begin with proving the following auxiliary
claim.

Claim 9.

y

O(√k) and

y, u

O(√k) with high probability.

k

k ≤

|h

i| ≤

Proof. From the generative model, we have

e

e

y
k

k

=

Sx∗S + ε
A∗
k
•

where S = supp(x∗). From Claim 2,
overcomplete and has bounded spectral norm, then
O(1). Therefore,
w.h.p., which is the ﬁrst part of the proof. To bound the second term, we write it as

k
A∗
Sk ≤ k
•

e

k

k

,

+

ε
k
k

x∗Sk

A∗
S kk
•
O(σε√n) w.h.p. In addition, A∗ is
O(√k)

ε
k ≤ k
ε
k ≤
A∗k ≤
e

y
k

k ≤

+

Sx∗Sk
A∗
•
O(√k) and

k

k ≤ k
x∗Sk ≤

=

y, u

Sx∗S + ε, u
A∗
•

T
S u
x∗S, A∗
•
T
S u
Similar to y, we have
A∗
k
•
probability. Since u and x∗ are independent sub-Gaussian and
T
e
variance at most O(√k),
S u
x∗S, A∗
•
y, u
quently,

i|
i| ≤ |h
O(√k) w.h.p. and hence

O(k) w.h.p. Similarly,

i| ≤

O(√k) w.h.p., and we conclude the proof of the claim.

.
i|
T
O(√k) with high
u
A∗
S kk
k ≤ k
•
T
x∗S, A∗
are sub-exponential with
S u
h
i
•
O(√k) w.h.p. Conse-
ε, u
i| ≤

k ≤

k ≤

|h
u

ε, u

+

|h

|h

|h

|h

i|

k

e

e

|h

y, v

e
y2
l =

i| ≤
, x∗i
A∗l
(
Proof of Claim 8. We have Z =
y, u
h
i
h
•
w.h.p. according to Claim 9. What remains is to bound y2
A∗l
l = (
h
2
is sub-Gaussian with variance ES(
•
1,2 = O(1), then
k
+εl| ≤
A∗l
εl| ≤
Similarly for εl,
|h
|
•
with high probability the bound
2y2
2y2
y, u
l h
i

T
2
A∗
li )
S A∗
i
∈
O(σε log n) w.h.p. Ultimately,

To bound the variance term, we write Z 2 =

P
Z
|

y, u
h

, x∗i

O(k).

≤ k

y, v

| ≤

ih

ih

i

2y2
i

y, v
we get
to both terms, then
e

l ≤

h

O(k) and

Z
|

| ≤

E[Z 2(1

e
Z

|≥

1
|

−

Ω(k))]
e

O(k)E[
h

y, u
i

≤

2y2

l ] + O(n−

ω(1)),

e
+ εl)2 with
O(k)
i ≤
, x∗i
A∗l
, x∗i
h
•
e
O(log n) w.h.p.
|h
O(log n), and hence we obtain

y, v
y, u
h
+ εl)2. Because
, x∗i| ≤
A∗l
•

ih

l . Note that, from the ﬁrst part,
O(k) w.h.p.. We apply Lemma 11 with some appropriate scaling

y, v

e

i

h

where E[
e
y, u
i
h
Section “Analysis of Initialization Algorithm”,

2y2

l ] is equal to el for pair u, v with v = u. From Lemma 1 and its proof in Appendix

E[
y, u
i
h

2y2

l ] =

qiciβ2

2
li + perturbation terms,
i A∗

m

Xi=1

in which the perturbation terms are bounded by O∗(k/m log2 n) w.h.p. (following Claims 4 and 5).
(max qiciβ2
2
O(log m)
A∗l
i )
i A∗
The dominant term
li ≤
≤
k
(Claim 3). Then we complete the proof of the second part.

O(k/m) w.h.p. because

βi| ≤
|

i qiciβ2

2
•k

P

e

Proof of Lemma 12, Part a. We are now ready to prove Part a of Lemma 12. We apply Bernstein’s
inequality in Lemma 9 for the truncated random variable Z (i)(1
O(k) and
variance σ2 =

O(k2/m) from Claim 8, then

)) with

1
Z (i)
|

e
Ω(

R

−

=

|≥

R

Z (i)(1

e
−

1
|

Z (i)

e
Ω(

))

|≥

R

−

E[Z(1

1
Z
|

−

e
Ω(

|≥

R

p

Xi=1

1
p

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

O(k)
p

e

O(k2/m)
p

+

s

≤

e

))]
(cid:13)
(cid:13)
(cid:13)
(cid:13)

29

e

O∗(k/m log n), (11)

Ω(m). Then

el = 1
w.h.p. for p =
p
bound over l = 1, 2, . . . , n, we get
the proof of 12, Part a.

e

b

p
i=1 Z (i) also concentrates with high probability. Take the union
O∗(k/m log n) with high probability and complete
e

k
P

−

e
k∞ ≤

D.1.3 Proof of Lemma 12, Part b

b

b
R
u,v −

M R

M

Next, we will prove that
prove the concentration inequalities for the case when conditioned on the event that
c
to R w.h.p. Again, what we need to derive are an upper norm bound
variable Z ,
yRyT
R and its variance.

O∗(k/m log n) with high probability. We only need to
R is equivalent
of the matrix random

u,vk ≤

y, v

R

b

k

y, u
h

ih

i

Claim 10.

Z

O(kr) and

k

k ≤

E[ZZ T ]
k

k ≤

O(k2r/m) hold with high probability.

k

Z

y, u

Proof. We have
k
2 =
yRk
whereas
This implies
Z
P
k ≤
k
We take advantage of the bounds of
and

e

2

2

i|k

yRk

e
2 with
y, v
ih
i| ≤
O(r log2 n) w.h.p. because yl ≤

e
O(k) w.h.p. (according to Claim 9)
k ≤ |h
R y2
O(log n) w.h.p. (proof of Claim 8).
i
∈
e
O(kr) w.h.p. The second part is handled similarly as in the proof of Claim 8.
O(kr)
Mu,v in Lemma 2. Speciﬁcally, using the ﬁrst part

l ≤

y, u

y, v

ih

|h

O(kr), and applying Lemma 11, then

Z
k

k ≤

i

y, v
h

yRk
k
E[ZZ T (1
k

≤

e
1
Z
k

−

k≥

Ω(kr))]
e

k ≤

E[
y, u
i
h

2yRyT
R]
k

+

O(kr)O(n−

ω(1))

O(kr)
k

,
Mu,uk

≤

e

where Mu,u arises from the application of Lemma 2. Recall that

e

e

e

Mu,u =

qiciβ2

T
R,i + perturbation terms,
i A∗R,iA∗

c
O(kr)
k

Xi

where the perturbation terms are all bounded by O∗(k/m log n) w.h.p. by Claims 6 and 7.
addition,

In

qiciβ2

T
i A∗R,iA∗
R,ik ≤

(max
i

qiciβ2
i )
k

A∗R

2
•k

O(k/m)
k

A∗

≤

k

≤

2

O(k/m)

k
Xi

w.h.p. Finally, the variance bound is

O(k2r/m) w.h.p.
Then, applying Bernstein’s inequality in Lemma 9 to the truncated version of Z with

and variance σ2 =

O(k2r/m) and obtain the concentration for the full Z to get

e

e

e

=

O(kr)

R

e

e

M R
k

u,v −

M R

u,vk ≤

c

O(kr)
p

O(k2r/m)
p

+

s

≤

e

e

O∗(k/m log n)

w.h.p. when the number of samples is p =

We have proved that
k
b
R
u,v −

M

M R

M R
u,v −
M R
u,vk ≤
c

Ω(mr) under Assumption A4.1.
O∗(k/m log n) as conditioned on the support consistency
u,vk ≤
e
O∗(k/m log n) is easily followed by the law of total probability

k

event holds w.h.p.
through the tail bounds on the conditional and marginal probabilities (i.e. P[
k
c
R = R]) and P[
O∗(k/m log n)
|
the spectral bounds.

u,vk ≤
= R]. We ﬁnish the proof of Lemma 12, Part b for both cases of

u,v −

M R

M R

c

R

b

b

30

D.2 Proof of Theorem 5 and Sample Complexity of Algorithm 2

In this section, we prove Theorem 5 and identify sample complexity per iteration of Algorithm 2. We
divide the proof into two steps: 1) show that when As is (δs, 2)-near to A∗ for δs = O∗(1/ log n), the
O(k2/mn)+αo(δ2
s )
approximate gradient estimate
, and 2) show that the nearness is preserved at each iteration. These correspond to showing the
following lemmas:

gs is (α, β, γs)-correlated-whp with A∗ with γs ≤

b

Lemma 13. At iteration s of Algorithm 2, suppose that As has each column correctly supported
and is (δs, 2)-near to A∗ and that η = O(m/k). Denote R = supp(As
gs
R,i is
i), then the update
•
O(k2/mn) + αo(δ2
(α, β, γs)-correlated-whp with A∗R,i where α = Ω(k/m), β = Ω(m/k) and γs ≤
s )
for δs = O∗(1/ log n).

b

Note that this is a ﬁnite-sample version of Lemma 6.

Lemma 14. If As is (δs, 2)-near to A∗ and number of samples used in step s is p =
.
with high probability

Proof of Theorem 5. The correlation of
column-wise error according to Theorem 1. Along with Lemma 14, the theorem follows directly.

A∗k
2
k
gi with A∗i , described in Lemma 13, implies the descent of

e

As+1
k

A∗k ≤

−

Ω(m), then

D.2.1 Proof of Lemma 13

b

We prove Lemma 13 by obtaining a tail bound on the diﬀerence between
Bernstein’s inequality in Lemma 9.

R,i and gs
gs

R,i using the

Lemma 15. At iteration s of Algorithm 2, suppose that As has each column correctly supported and
is (δs, 2)-near to A∗. For R = supp(As
(o(δs) + O(ǫs))
i ) = supp(A∗i ), then
·
mnr
k ).
with high probability for δs = O∗(1/ log n) and ǫs = O(

gs
R,i −
k/n) when p =

O(k/m)
Ω(m + σ2
ε

gs
R,ik ≤

p
To prove this lemma, we study the concentration of

b
gs
R,i, which is a sum of random vector
e
of the form (y
S, with
i
Ax)Rsgn(xi)
|
S = supp(x∗) and x = thresholdC/2(AT y). Then, using the following technical lemma to bridge
the gap in concentration of the two variables. We adopt this strategy from Arora et al. (2015) for
our purpose.

Ax)Rsgn(xi). We consider random variable Z , (y

−

−

∈

b

k

b

Claim 11. Suppose that Z (1), Z (2), . . . , Z (N ) are i.i.d. samples of the random variable Z = (y
i
Ax)Rsgn(xi)
|

S. Then,

∈

−

o(δs) + O(ǫs)

(12)

(cid:13)
(cid:13)
(cid:13)
holds with probability when N =

Xj=1
Ω(k + σ2

N

1
N

Z (j)

−

≤

E[Z]
(cid:13)
(cid:13)
(cid:13)

ε nr), δs = O∗(1/ log n) and ǫs = O(

k/n).

Proof of Lemma 15. Once we have done the proof of Claim 11, we can easily prove Lemma 15. We
recycle the proof of Lemma 43 in Arora et al. (2015).
supp(x∗

Write W =

and N =

gR,i as

e

j : i
{

∈

(j))
}

W
|

, then express
|

p

gR,i =

N
p

1
N

(y(j)

−

Ax(j))Rsgn(x(j)

b
i ),

b

Xj

31

j(y(j)

1
where
W
|
E[(y
Ax)Rsgn(xi)] = E[(y
Following Claim 11, we have

P

−

−

|

−

Ax(j))Rsgn(x(j)

i ) is distributed as 1
N

Ax)Rsgn(xi)1i

S ] = E[Z]P[i
P
∈

∈

N
j=1 Z (j) with N =

. Note that
W
|
|
S] = qiE[Z] with qi = Θ(k/m).

gs
R,i −

gs
R,ik ≤

k

O(k/m)

O(k/m)

(o(δs) + O(ǫs)),

·

1
N

N

Xj=1

(cid:13)
(cid:13)
(cid:13)

Z (j)

−

≤

E[Z]
(cid:13)
(cid:13)
(cid:13)

holds with high probability as p = Ω(mN/k). Substituting N in Claim 11, we obtain the results
in Lemma 15.

b

Proof of Claim 11. We are now ready to prove the claim. What we need are good bounds for
k
and its variance, then we can apply Bernstein’s inequality in Lemma 9 for the truncated version of
Z, then Z is also concentrates likewise.

Z
k

holds with high probability for

=

O(δs√k + µk/√n + σε√r) with δs =

Claim 12.
k
O∗(1/ log n).

Z

k ≤ R

R

e

−

(y

Proof. From the generative model and the support consistency of the encoding step, we have
S y = AT
Sx∗S + AT
Sx∗S + ε and xS = AT
Sε. Then,
SA∗
y = A∗x∗ + ε = A∗
•
•
•
•
•
AR,SAT
AR,SAT
S ε
SA∗
•
•
•
S )x∗S + (In −
AR,S)x∗S + AR,S(Ik −
O(σwk
M w
k
e
kF + σεk

Ax)R = (A∗R,Sx∗S + εR)
S x∗S −
AT
= (A∗R,S −
SA∗
•
•
Using the fact that x∗S and ε are sub-Gaussian and that
probability for a ﬁxed M and a sub-Gaussian w of variance σ2

SAT
S)R
A
•
•
Now, we need to bound those Frobenius norms. The ﬁrst quantity is easily bounded as

kF ) holds with high

SAT
S )R
A
•
•
•

k ≤
w, we have

AR,S(Ik −

AR,SkF +

AT
S )
SA∗
•
•

Ax)Rsgn(xi)

A∗R,S −

(In −

•kF ).

O(
k

(y
k

k ≤

M

−

−

ε.

k

e

A∗R,S −
k

AR,SkF ≤ k

A∗
S −
•

A
S kF ≤
•

δs√k,

(13)

since A is δs-close to A∗. To handle the other two, we use the fact that
this fact for the second term, we have

U V
k

kF ≤ k

U

V

kF . Using

kk

AR,S(Ik −
k

AT
S)
S A∗
•
•

kF ≤ k

AR,Skk

(Ik −

AT
S )
SA∗
•
•

kF ,

where
advantage of the closeness and incoherence properties:

AR,Sk ≤ k

•k ≤

AR

k

O(1) due to the nearness. The second part is rearranged to take

Ik −

k

AT
SA∗
SkF ≤ k
•
•

S)T A∗
T
(A
A∗
S A∗
A∗
Ik −
S −
SkF
S −
•
•
•
•
•
S)T A∗
T
SkF +
A∗
(A
S A∗
A∗
Ik −
S −
S kF
k
•
•
•
•
•
T
A∗
A
A∗
SkF +
S A∗
Ik −
A∗
S −
S kF
Skk
•
•
•
•
•
µk/√n + O(δs√k),

k

≤ k

≤ k

≤

where we have used
δs√k in (13) and

µk/√n because of the µ-incoherence of A∗,

A
S −
•

A∗
SkF ≤
•

k

O(1). Accordingly, the second Frobenius norm is bounded by

k

T
S A∗
Ik −
A∗
SkF ≤
k
•
•
A∗k ≤
A∗
Sk ≤ k
•
AR,S(Ik −

k

AT
S )
SA∗
•
•

kF ≤

O

µk/√n + δs√k

.

(cid:0)

32

(cid:1)

(14)

SAT
The noise term is handled using the eigen-decomposition U ΛU T of A
S, then with high prob-
•
•
ability

(In −

(U U T
SAT
•kF =
S)R
A
k
k
•
•
where the last inequality
A∗k ≤
O(1) due to the nearness. Putting (13), (14) and (15) together, we obtain the bounds in Claim
12.

•kF =
O(1) follows by

(In −
A
Sk ≤ k
•

kF ≤ k
A

•kF ≤
+

kk
A∗k

In −
A

O(√r), (15)

U ΛU T )R

A∗k ≤

UR
k
•

In −

k ≤ k

−
Λ

3
k

k ≤

UR

Λ)

−

Λ

k

k

k

Next, we determine a bound for the variance of Z.

2] = E[
Claim 13. E[
Ax)Rsgn(xi)
Z
k
k
−
k
k
s k + k2/n + σ2
O(δ2
ε r) with δs = O∗(1/ log n).

(y

2

i
|

S]

∈

≤

σ2 holds with high probability for σ2 =

Proof. We explicitly calculate the variance using the fact that x∗S is conditionally independent
given S, and so is ε. x∗S and ε are also independent and have zero mean. Then we can decompose
the norm into three terms in which the dot product is zero in expectation and the others can be
S ] = Ik, E[εεT ] = σεIn.
T
shortened using the fact that E[x∗Sx∗
SAT
AR,S AT
i
ε
A
SA∗
S)x∗S + (In −
S)R
∈
k
|
·
•
•
•
•
2
2
E[
AR,SAT
SAT
S] + σ2
SA∗
S)R
A
In −
i
i
S k
ε
F |
F |
k
∈
•k
∈
•
•
•
•
AT
Then, by re-writing A∗R,S −
S )
SA∗
AR,S)+AR,S(Ik−
S as before, we get the form (A∗R,S −
•
•
in which the ﬁrst term has norm bounded by δs√k. The second is further decomposed as

S] = E[
(A∗R,S −
k
= E[
A∗R,S −
k
AR,SAT
SA∗
•
•

Ax)Rsgn(xi)
k

E[
(y
k

S]]

i
|

−

∈

2

2

S].

E[
AR,S (Ik −
k

AT
S)
SA∗
k
•
•

2
i
F |

∈

S]

sup
S k

AR,Sk

≤

2E[
Ik −
k

2
AT
SA∗
i
S k
F |
•
•

∈

S],

(16)

where supSk
using the proof from Arora et al. (2015):

AR,Sk ≤ k

•k ≤

AR

O(1). We will bound E[
k

Ik −

2
AT
SA∗
i
F |
S k
•
•

∈

S]

≤

O(kδ2

s ) + O(k2/n)

E[
Ik −
k

AT
SA∗
Sk
•
•

2
i
F |

∈

S] = E[

1
4 k

A
j −
•

A∗
jk
•

2] + qij

= E[

S
Xj
∈

S
Xj
∈

k
Xj
=i

(1

−

j)2 +
AT
jA∗
•
•

AT
jA∗
,
k
•
•
Xj
S
∈
2 + qik
AT
iA∗
,
jk
•
•

−

−

AT
jA∗
,
•
•

2
jk

i
|

−

∈

S]

2 + qik
ik

AT
,
•

iA∗
ik
•

−

2,

where A
,
−
•
any j = 1, 2, . . . , m,

i is the matrix A with the i-th column removed, qij ≤

O(k2/m2) and qi ≤

O(k/m). For

AT
jA∗
,
•
•

k

jk

−

2 =

k

T
j A∗
A∗
,
•
•
−
j, A∗
A∗
li
•
•

2
j)T A∗
A∗
j + (A
j −
,
jk
•
•
•
−
2 +
j)T A∗
A∗
(A
j −
,
k
•
•
•

2

jk

−

≤

h
Xl
=j

≤

h
Xl
=j

j, A∗
A∗
li
•
•

2 +

A
j −
k
•

A∗
jk
•

2

A∗
,
k
•

−

2
jk

≤

µ2 + δ2
s .

The last inequality invokes the µ-incoherence, δ-closeness and the spectral norm of A∗. Similarly,
we come up with the same bound for

2. Consequently,

2 and

AT
iA∗
,
•
•

k

ik

−

AT
iA∗
,
ik
k
•
−
•
s ) + O(k2/n).
O(kδ2

E[
k

Ik −

2
AT
S A∗
i
Sk
F |
•
•

∈

S]

≤

(17)

33

For the last term, we invoke the inequality (15) (Claim 12) to get

SAT
S)R
A
•
•
Putting (16), (17) and (18) together and using
σ2 = O(δ2

•k
AR
k
ε r) with δs = O(1/ log2 n) . Finally, we complete the proof.
We now apply truncated Bernstein’s inequality to the random variable Z (j)(1

s k + k2/n + σ2

E[
(In −
k

2
i
F |

•k ≤

S]

≤

∈

r

1, we obtain the variance bound of Z:

(18)

))
Ω(
R
O(δs√k + µk/√n + σε√r) and σ2 = O(δ2
s k +

1
k

Z (j)

−

k≥

with
R
k2/n + σ2

and σ2 in Claims 12 and 13, which are
ε r). Then, (1/N )

=

R

N
j= Z (j) also concentrates:
e

N

P
Z (j)

Xi=1

1
N

(cid:13)
(cid:13)
(cid:13)

E[Z]

−

O

≤

R
N

+

O

(cid:13)
(cid:13)
(cid:13)

(cid:16)

(cid:17)

e
Ω(k + σ2

e

σ2
N

(cid:18)r

(cid:19)

= o(δs) + O(

k/n)

p

ε nr). Then, we ﬁnally ﬁnish the proof of Claim

holds with high probability when N =
11.

e

Proof of Lemma 13. With Claim 11, we study the concentration of
we consider this diﬀerence as an error term of the expectation gs
correlation of

gs
R,i. Using the expression in Lemma 5 with high probability, we can write

R,i around its mean gs
gs

R,i. Now,
R,i and using Lemma 6 to show the

b

b

R,i + (gs
where
+ O(k/m)
correlated-whp with A∗R,i where α = Ω(k/m), β = Ω(m/k) and γs ≤
, then we have done the proof Lemma 13.

gs
R,i = gs
A∗R,ik
b

gs
R,i) = 2α(AR,i −

AR,i −

R,i −

α
k

k ≤

b

k

v

·

A∗R,i) + v,

(o(δs) + O(ǫs)). By Lemma 6, we have

O(k/m)

·

b

gs
R,i is (α, β, γs)-
k/n))

(o(δs) + O(

p

D.2.2 Proof of Lemma 14

gs with A∗ w.h.p. and established the descent property of Al-
We have shown the correlation of
gorithm 2. The next step is to show that the nearness is preserved at each iteration. To prove
As+1
holds with high probability, we recall the update rule
k

A∗k ≤

A∗k

2
k

−

b

PH (
gs. Here H = (hij) where hij = 1 if i

As+1 = As

−

η

gs),

gs) = H

◦

PH(

where
j) and hij = 0 otherwise. Also,
supp(A
•
note that As is (δs, 2)-near to A∗ for δs = O∗(1/ log n). We already proved that this holds for the
exact expectation gs in Lemma 7. To prove for
gs, we again apply matrix Bernstein’s inequality to
gs)
bound
k
Consider a matrix random variable Z ,
E[ZZ T ]
b
k
k

= O(1).
by O(k/m) because η = Θ(m/k) and
Ax)sgn(x)T ). Our goal is to bound the spectral
PH((y
b
E[Z T Z]
since Z is asymmetric. To simplify our notations,
norm
k
we denote by xR the vector x by zeroing out the elements not in R. Also, denote Ri = supp(hi)
and S = supp(x). Then Z can be written explicitly as

b
− PH (
and, both

b
kPH(gs)
Z

A∗k

and

b
∈

−

k

k

k

k

Z = [(y

Ax)R1 sgn(x1), . . . , (y

Ax)Rmsgn(xm)],

−

−

where many columns are zero since x is k-sparse. The following claims follow from the proof of
Claim 42 in Arora et al. (2015). Here we state and detail some important steps.

Claim 14.

Z

O(k) holds with high probability.

k

k ≤

e

34

Proof. With high probability

Z
k

k ≤

(y

2
Ax)Risgn(xi)
k

−

≤

√k

(y

k

Ax)Rik

−

k
sXi
S
∈

(y
k

−

where we use Claim 12 with

Claim 15.

E[ZZ T ]

O(k2/n) and

k

k ≤

Ax)Rk ≤
k

E[Z T Z]
e

k ≤

O(δs√k) w.h.p., then

Z

O(k) holds w.h.p.

k
O(k2/n) with high probability.

k ≤

e

Proof. The ﬁrst term is easily handled. Speciﬁcally, with high probability

e

E[ZZ T ]

k

k ≤ k

E[

(y

−

Ax)Ri sgn(xi)2(y

Ax)T

Ri]
k

−

=

E[
k

(y

Ax)Ri(y

−

−

Ax)T

Ri ]

k ≤

O(k2/n),

S
Xi
∈

S
Xi
∈

where the last inequality follows from the proof of Claim 42 in Arora et al. (2015), which is tedious
to be repeated.
To bound

, we use bound of the full matrix (y
O(√k) w.h.p. is similar to what derived in Claim 12. Then with high probability,

Ax)sgn(x)T . Note that

E[Z T Z]
k

y
k

k ≤

Ax

−

−

k

E[Z T Z]
k

E[sgn(x)(y

Ax)T (y

Ax)sgn(x)T ]

k ≤ k

e
where E[sgn(x)sgn(x)T ] = diag(q1, q2, . . . , qm) has norm bounded by O(k/m). We now can apply
O(k2/m), then with
Bernstein’s inequality for the truncated version of Z with
p =

O(k) and σ2 =

O(m),

k ≤

k ≤

R

=

−

−

e

e

E[sgn(x)sgn(x)T ]

O(k2/m).

O(k)
k

e

kPH(gs)

− PH(

gs)

k ≤

O(k)
p

e

e
O(k2/m)
p

≤

+

s

e

e

O∗(k/m)

holds with high probability. Finally, we invoke the bound η = O(m/k) and complete the proof.

b

Appendix E. A Special Case: Orthonormal A∗

We extend our results for the special case where the dictionary is orthonormal. As such, the
dictionary is perfectly incoherent and bounded (i.e., µ = 0 and

= 1).

Theorem 7. Suppose that A∗ is orthonormal. When p1 =
Ω(nr), then with high
probability Algorithm 1 returns an initial estimate A0 whose columns share the same support as
A∗ and with (δ, 2)-nearness to A∗ with δ = O∗(1/ log n). The sparsity of A∗ can be achieved up to
r = O∗

e

e

)

.

,

min( √n
log2 n

n
k2 log2 n

We use the same initialization procedure for this special case and achieve a better order of r.

(cid:0)

(cid:1)

The proof of Theorem 7 follows the analysis for the general case with following two results:

A∗k
k
Ω(n) and p2 =

Claim 16 (Special case of Claim 3). Suppose that u = A∗α + εu is a random sample and U =
supp(α). Let β = A∗
O(√k log n + σε√n log n).

T u, then w.h.p., we have (a)

σε log n for each i and (b)

αi| ≤

βi −
|

k ≤

β

k

T ǫu, then βi −
A∗
i, ǫui
=
Proof. We have β = A∗
α
h
k
•
in Claim 2, we have the claim proved.
α
and
A∗
probability bounds of
k
k
h
•
O(σε log2 n) and have the following
βiβ′i| ≤
V ,
U
We draw from the claim that for any i /
∈

T u = α + A∗
,
i, ǫui

. Using

ǫuk
k

ǫuk
k

αi =

and

−

∩

β

k

|

result:

35

Lemma 16. Fix samples u and v and suppose that y = A∗x∗ + ε is a random sample independent
of u, v. The expected value of the score for the lth component of y is given by:

el , E[
h

y, u

y, v

ih

i

y2
l ] =

2
li + perturbation terms
qiciβiβ′iA∗

where qi = P[i
absolute value at most O∗

S], qij = P[i, j

∈

V
U
Xi
∩
∈
S] and ci = E[x4
i
i |

∈
k/n log2 n max(1/√n, k2/n)

∈

S]. Moreover, the perturbation terms have
.

Proof. Lemma follows Lemma 1 via Claim 3 except that the second term of E1 is bounded by
O(k log2 n/n3/2).

(cid:0)

(cid:1)

Appendix F. Extensions of Arora et al. (2015)

F.1 Sample complexity in noisy case

In this section, we study the sample complexity of the algorithms in Arora et al. (2015) in the
presence of noise. While noise with order σε = O(1/√n) does not change the sample complexity
of the initialization algorithm, it aﬀects that of the descent stage. The analysis involves producing
a sharp bound for

.

gs
,i −
•

gs
ik
•

k

Lemma 17. For a regular dictionary A∗, suppose As is (δs, 2)-near to A∗ with δs = O∗(1/ log n),
then with high probability

k/n)) when p =

(o(δ) + O(

O(k/m)

b

Ω(m + σ2
ε

mn2
k ).

gs
,i −
k
•
Proof. This follows directly from Lemma 15 where r = n.
b

We tighten the original analysis to obtain the complexity

gs
ik ≤
•

·

case. Putting together with p =
mn2
sample complexity
k ) for the algorithms in Arora et al. (2015) in the noise regime.

O(mk + σ2
ε

e

e

p
Ω(mk) for the noiseless
Ω(m) instead of
Ω(mk) required by the initialization, we then have the overall

e

e

F.2 Extension of Arora et al. (2015)’s initialization algorithm for sparse case

e

We study a simple and straightforward extension of the initialization algorithm of Arora et al.
(2015) for the sparse case. This extension is produced by adding an extra projection, and is
described in Figure 3. The recovery of the support of A∗ is guaranteed by the following Lemma:
Lemma 18. Suppose that z∗ ∈
Provided z is δ-close to z∗ and z0 =
z∗ has the same support.

Rn is r-sparse whose nonzero entries are at least τ in magnitude.
Hr(z) with δ = O∗(1/ log n) and r = O∗(log2 n), then z0 and

Proof. Since z0 is δ-close to z∗, then

δ for every i. For i

supp(z∗),

∈

z0
z∗k ≤
−
k
zi −
z∗i | − |
zi| ≥ |
|
δ. Since τ > O(1/√r)

δ and

zi −
|
τ
z∗i | ≥

−

z∗i | ≤
δ

and for i /
≫
∈
support z∗, and hence z0 and z∗ has the same support.

supp(z∗),

zi| ≤
|

δ, then the r-largest entries of z are in the

√n
k log3 n

Theorem 8. Suppose that Assumptions B1-B4 hold and Assumptions A1-A3 satify with µ =
Ω(mk), then with high probability
O∗
Algorithm 3 returns an initial estimate A0 whose columns share the same support as A∗ and with
e
(δ, 2)-nearness to A∗ with δ = O∗(1/ log n).

and r = O∗(log2 n). When p1 =

Ω(m) and p2 =

e

(cid:1)

(cid:0)

36

Algorithm 3 Pairwise Reweighting with Hard-Thresholding

∅

Initialize L =
Randomly divide p samples into two disjoint sets
While
P1 at random
Reconstruct the re-weighted covariance matrix

< m. Pick u and v from

L
|

|

Mu,v:

P1 and

P2 of sizes p1 and p2 respectively

p2

1
p2

y(i), u
h
Xi=1

ih

Mu,v =

c

c
y(i), v

y(i)(y(i))T
i

Compute the top singular values δ1, δ2 and top singular vector z of
If δ1 ≥
z =
If z is not within distance 1/ log n of any vector in L even with sign ﬂip

Hr keeps r largest entries of z

Ω(k/m) and δ2 < O∗(k/m log n)
Hr(z), where
z
L = L
∪ {

Mu,v

c

}

Return A0 = (L1, . . . , Lm)

This algorithm requires r = O∗(log2 n), which is somewhat better than ours. However, the

sample complexity and running time is inferior as compared with our novel algorithm.

Appendix G. Neural Implementation of Our Approach

We now brieﬂy describe why our algorithm is “neurally plausible”. Basically, similar to the argu-
ment in Arora et al. (2015), we describe at a very high level how our algorithm can be implemented
via a neural network architecture. One should note that although both our initialization and de-
scent stages are non-trivial modiﬁcations of those in Arora et al. (2015), both still inherit the nice
neural plausiblity property.

G.1 Neural implementation of Stage 1: Initialization

Recall that the initialization stage includes two main steps: (i) estimate the support of each column
of the synthesis matrix, and (ii) compute the top principal component(s) of a certain truncated
weighted covariance matrix. Both steps involve simple vector and matrix-vector manipulations that
can be implemented plausibly using basic neuronal manipulations.
y, u

For the support estimation step, we compute the product
y, u
i
h

y, followed by a thresh-
y
y, u
i
olding. The inner products,
can be computed using neurons via an online manner
where the samples arrive in sequence; the thresholding can be implemented via a ReLU-type non-
linearity.

and

y, v

ih

◦

h

i

h

For the second step, it is well known that the top principal components of a matrix can be

computed in a neural (Hebbian) fashion using Oja’s Rule Oja (1992).

G.2 Neural implementation of Stage 2: Descent

Our neural implementation of the descent stage (Algorithm 2), shown in Figure 2, mimics the
architecture of Arora et al. (2015), which describes a simple two-layer network architecture for
computing a single gradient update of A. The only diﬀerence in our case is that most of the value

37

y

r

x =
threshold(AT y)

+

+

+

+

+

+

−

−

−

Aij

+

+
+

xj

Hebbian rule
Aij = ηrixj

∇

Figure 2: Neural network implementation of Algorithm 2. The network takes the image y as input
and produces the sparse representation x as output. The hidden layer represents the
residual between the image and its reconstruction Ax. The weights Aij’s are stored on
synapses, but most of them are zero and shown by the dotted lines.

in A are set to zero, or in other words, our network is sparse. The network takes values y from the
input layer and produce x as the output; there is an intermediate layer in between connecting the
middle layer with the output via synapses. The synaptic weights are stored on A. The weights are
updated by Hebbian learning. In our case, since A is sparse (with support given by R, as estimated
in the ﬁrst stage), we enforce the condition the corresponding synapses are inactive. In the output
layer, as in the initialization stage, the neurons can use a ReLU-type non-linear activation function
to enforce the sparsity of x.

References

Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon.
Learning sparsely used overcomplete dictionaries. In Conference on Learning Theory, pages 123–
137, 2014.

Michal Aharon, Michael Elad, and Alfred Bruckstein. k-svd: An algorithm for designing over-
complete dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):
4311–4322, 2006.

Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcom-

plete dictionaries. In Conference on Learning Theory, pages 779–806, 2014.

Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, eﬃcient, and neural algorithms

for sparse coding. In Conference on Learning Theory, pages 113–149, 2015.

38

Jaros law B lasiok and Jelani Nelson. An improved analysis of the er-spud dictionary learning algo-

rithm. arXiv preprint arXiv:1602.05719, 2016.

Y-Lan Boureau, Francis Bach, Yann LeCun, and Jean Ponce. Learning mid-level features for
recognition. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on,
pages 2559–2566. IEEE, 2010.

Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on

information theory, 51(12):4203–4215, 2005.

Niladri Chatterji and Peter Bartlett. Alternating minimization for dictionary learning with random

initialization. 2017. arXiv:1711.03634v1.

Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over

learned dictionaries. IEEE Transactions on Image processing, 15(12):3736–3745, 2006.

Kjersti Engan, Sven Ole Aase, and J Hakon Husoy. Method of optimal directions for frame design. In
IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 5,
pages 2443–2446. IEEE, 1999.

Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of

the 27th International Conference on Machine Learning (ICML), pages 399–406, 2010.

R´emi Gribonval, Rodolphe Jenatton, and Francis Bach. Sparse and spurious: dictionary learning
with noise and outliers. IEEE Transactions on Information Theory, 61(11):6298–6319, 2015a.

R´emi Gribonval, Rodolphe Jenatton, Francis Bach, Martin Kleinsteuber, and Matthias Seibert.
Sample complexity of dictionary learning and other matrix factorizations. IEEE Transactions
on Information Theory, 61(6):3469–3486, 2015b.

Hamid Krim, Dewey Tucker, Stephane Mallat, and David Donoho. On denoising and best signal

representation. IEEE Transactions on Information Theory, 45(7):2225–2238, 1999.

Rados law Adamczak. A note on the sample complexity of the er-spud algorithm by spielman,
wang and wright for exact recovery of sparsely used dictionaries. Journal of Machine Learning
Research, 17:1–18, 2016.

Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for
sparse coding. In Proceedings of the 26th International Conference on Machine Learning (ICML),
pages 689–696, 2009.

Arya Mazumdar and Ankit Singh Rawat. Associative memory using dictionary learning and ex-
pander decoding. In Proc. Conf. American Assoc. Artiﬁcial Intelligence (AAAI), pages 267–273,
2017.

Thanh V. Nguyen, Raymond K. W. Wong, and Chinmay Hegde. A provable approach for double-

sparse coding. In Proc. Conf. American Assoc. Artiﬁcial Intelligence (AAAI), Feb. 2018.

Erkki Oja. Principal components, minor components, and linear neural networks. Neural networks,

5(6):927–935, 1992.

39

Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy

employed by v1? Vision research, 37(23):3311–3325, 1997.

Ron Rubinstein, Alfred M Bruckstein, and Michael Elad. Dictionaries for sparse representation

modeling. Proceedings of the IEEE, 98(6):1045–1057, 2010a.

Ron Rubinstein, Michael Zibulevsky, and Michael Elad. Double sparsity: Learning sparse dictionar-
ies for sparse signal approximation. IEEE Transactions on Signal Processing, 58(3):1553–1564,
2010b.

Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries.

In Conference on Learning Theory, pages 37–1, 2012.

Jeremias Sulam, Boaz Ophir, Michael Zibulevsky, and Michael Elad. Trainlets: Dictionary learning

in high dimensions. IEEE Transactions on Signal Processing, 64(12):3180–3193, 2016.

Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery using nonconvex optimization.
In Proceedings of the 33rd International Conference on Machine Learning (ICML), pages 2351–
2360, 2015.

Lingxiao Wang, Xiao Zhang, and Quanquan Gu. A uniﬁed computational and statistical framework

for nonconvex low-rank matrix estimation. arXiv preprint arXiv:1610.05275, 2016.

Yuchen Zhang, Xi Chen, Dengyong Zhou, and Michael I Jordan. Spectral methods meet em: A
provably optimal algorithm for crowdsourcing. Journal of Machine Learning Research, 17(1):
3537–3580, 2016.

40

