Uniﬁed Multi-Criteria Chinese Word Segmentation with BERT

Zhen Ke1,2, Liang Shi2, Erli Meng2, Bin Wang2, Xipeng Qiu1, Xuanjing Huang1
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University1
Xiaomi AI Lab, Xiaomi Inc., Beijing, China2
{zke17,xpqiu,xjhuang}@fudan.edu.cn
{shiliang1,mengerli,wangbin11}@xiaomi.com

0
2
0
2
 
r
p
A
 
3
1
 
 
]
L
C
.
s
c
[
 
 
1
v
8
0
8
5
0
.
4
0
0
2
:
v
i
X
r
a

Abstract

Multi-Criteria Chinese Word Segmentation
(MCCWS) aims at ﬁnding word boundaries
in a Chinese sentence composed of continu-
ous characters while multiple segmentation cri-
teria exist. The uniﬁed framework has been
widely used in MCCWS and shows its effec-
tiveness. Besides, the pre-trained BERT lan-
guage model has been also introduced into the
MCCWS task in a multi-task learning frame-
work.
In this paper, we combine the su-
periority of the uniﬁed framework and pre-
trained language model, and propose a uni-
ﬁed MCCWS model based on BERT. More-
over, we augment the uniﬁed BERT-based MC-
CWS model with the bigram features and an
auxiliary criterion classiﬁcation task. Exper-
iments on eight datasets with diverse criteria
demonstrate that our methods could achieve
new state-of-the-art results for MCCWS.

1

Introduction

Chinese Word Segmentation (CWS) is a fundamen-
tal task in Chinese Natural Language Processing
(NLP), which aims to identify word boundaries in a
Chinese sentence consisting of continuous charac-
ters. Most approaches transform the CWS problem
into a character-based sequence labeling problem,
in which each character is assigned a label to denote
its position in the target word (Xue, 2003; Zheng
et al., 2013; Chen et al., 2015a,b; Zhang et al.,
2016; Ma et al., 2018; Yang et al., 2019). Recently,
the CWS task of multiple segmentation criteria is
studied, which can be formulated as Multi-Criteria
Chinese Word Segmentation (MCCWS) task (Chen
et al., 2017; He et al., 2019; Gong et al., 2019;
Huang et al., 2019; Qiu et al., 2019). In MCCWS,
a sentence can be segmented into different word
units in different criteria. An example of MCCWS
is showed in Table 1.

Existing MCCWS methods can be divided into

Li Na
李娜

Criteria
CTB
PKU 李 娜 进入 半 决赛
进入 半 决赛
MSRA

the semi-ﬁnal
半决赛

entered
进入

李娜

Table 1: An example of MCCWS.

two frameworks:
the multi-task learning frame-
work and the uniﬁed framework. The multi-task
learning framework treats MCCWS as a multi-task
learning problem (Caruana, 1997), with each cri-
terion as a single task (Chen et al., 2017; Huang
et al., 2019). A shared layer is used to obtain the
common features of all criteria, and a private layer
of each criterion obtains the criterion-speciﬁc fea-
tures. The multi-task learning framework can learn
shared knowledge but not efﬁciently, which suffers
from high model complexity and complex training
algorithm. The uniﬁed framework employs a fully
shared model for all criteria in MCCWS, and the
criterion is fed as input (He et al., 2019; Gong et al.,
2019; Qiu et al., 2019). The uniﬁed framework is
more concise, in which the shared knowledge are
fully used and training process is simpliﬁed, com-
pared with the multi-task learning framework.

Recently, the pre-trained BERT language model
(Devlin et al., 2019) have showed its powerful ca-
pability to learn prior linguistic knowledge, which
have been proved beneﬁcial for many NLP tasks.
Huang et al. (2019) also introduced BERT to MC-
CWS in the multi-task learning framework and
achieved excellent performance. However, due to
the powerful ability of BERT, it is unnecessary
to use multiple criterion-speciﬁc projection layers,
which motivates us to explore BERT for MCCWS
in the more concise uniﬁed framework.

In this paper, we propose a BERT-based uniﬁed
MCCWS model, to incorporate the superiority of
uniﬁed framework and pre-trained language model
for the ﬁrst time. Besides, the bigram features have

been proved to be effective for CWS (Chen et al.,
2015b; Zhang et al., 2016). Thus, a fusion layer
and an attention layer are further employed to inte-
grate the bigram features and the output of BERT.
To ensure the uniﬁed model retrains the criterion in-
formation, an auxiliary criterion classiﬁcation task
is introduced to further boosts the performance.
Different to the previous work, our model does not
use the CRF (Lafferty et al., 2001) inference layer,
thereby being more efﬁcient in inference phase by
fully utilizing the ability of parallel computation.
Experiments show that our model can achieve new
state-of-the-art results on eight standard datasets.

2 Methodology

As previous work (Chen et al., 2017; Huang et al.,
2019; Qiu et al., 2019),
the MCCWS task is
viewed as a character-based sequence labeling
problem. Speciﬁcally, given a Chinese sentence
X = {x1, x2, ..., xT } composed of continuous
characters and a criteria c, the model should output
a CWS label sequence Y = {y1, y2, ..., yT } with
yt ∈ {B, M, E, S}, denoting the beginning, mid-
dle, end of a word, or a word as single character.

In our model, we ﬁrst augment the input sen-
tence by inserting a speciﬁc criterion token before
it, and encode the augmented sentence using the
BERT model (Devlin et al., 2019). Besides, we con-
vert character-based bigram features into bigram
embedding. Then, we use a fusion gate mecha-
nism to integrate the hidden representation from
BERT with the bigram embedding. A multi-head
attention is followed to interchange context infor-
mation among the fused representations. Finally,
a multi-layer Perceptron (MLP) decoder is used
to decode the segmentation labels. Besides, an
auxiliary criterion classiﬁer is employed to recon-
struct the criterion class from the input. The overall
architecture of our model is displayed in Figure 1.

2.1 Augmented Input Sentence

We add a speciﬁc criterion token before the original
input sentence X to obtain the augmented sentence
X (cid:48) = {x0, x1, .., xT }. For example, we add the to-
ken <pku> before the original sentence indicating
that it adheres to the PKU criterion.

2.2 Encoder

BERT BERT (Devlin et al., 2019) is short for
Bidirectional Encoder Representations from Trans-
formers, which is a Transformer (Vaswani et al.,

Figure 1: Overall architecture of our proposed model.

2017) based bidirectional language model pre-
trained on a large-scale unsupervised corpus. We
employ BERT as our basic encoder for MCCWS
to introduce prior linguistic knowledge, convert-
ing the augmented sentence to hidden character
representations:

H = BERT(X (cid:48)),

(1)

where H ∈ R(T +1)×dh.

Bigram Embedding The bigram features have
proved beneﬁcial for MCCWS (Chen et al., 2017;
He et al., 2019; Qiu et al., 2019). Therefore, we
construct the bigram feature for every character
by concatenating it with the previous character,
B = {x0x1, x1x2, ..., xT −1xT }. Then, we convert
the bigram features to bigram embedding vectors
by looking them up in an embedding table

E = BigramEmbed(B),

(2)

where E ∈ RT ×de.

Fusion Layer We use a fusion gate mechanism
to integrate the hidden character representations
and the bigram embedding representations. We re-
fer to the t-th hidden character vector as ht ∈ Rdh,
the t-th bigram embedding as et ∈ Rde. The fusion
gate mechanism can be formulated as follows,

h(cid:48)
t = tanh (Whht + bh) ,
e(cid:48)
t = tanh (Weet + be) ,
gt = σ (Wf hht + Wf eet + bf ) ,
ft = gt (cid:12) h(cid:48)
t, gt, ft ∈ Rdh, gt is the fusion gate
where h(cid:48)
vector, ft is the fused vector, and σ is the sigmoid
function.

t + (1 − gt) (cid:12) e(cid:48)
t,

t, e(cid:48)

(3)

Multi-Head Attention Next, we employ a multi-
head attention (Vaswani et al., 2017) to obtain the
contextual representations. It is necessary to con-
textualize the fused representations because the
fusion layer can only integrate the unigram and
bigram representations character-wise, lacking the
knowledge in the context. The multi-head attention
with residual connection (He et al., 2016) and layer
normalization (Ba et al., 2016) can be formulated
as

O = LayerNorm (MultiHead(F) + F) ,

(4)

where F ∈ RT ×dh is the fused representations and
O ∈ RT ×dh is the ﬁnal output representations from
the encoder.

2.3 Decoder

CWS Label Decoder The output representations
are converted into the probabilities over the CWS
labels by an MLP layer,

P (yt|X, c) = softmax (Woot + bo)yt

P (Y |X, c) =

P (yt|X, c)

T
(cid:88)

t=1

(5)

where Wo ∈ R4×dh. P (yt|X, c) means the proba-
bility that the t-th CWS label is yt, and P (Y |X, c)
means the probability of the label sequence Y ,
given the input sentence X and the criterion c.

Auxiliary Criterion Classiﬁer To avoid the cri-
terion information lost in forward propagation, we
add an auxiliary criterion classiﬁer to reconstruct
the criterion from the hidden representations. The
probability of reconstructing criterion c is

P (c|X, c) = softmax (Wch0 + bc)c ,

(6)

where W ∈ RC×dh and C is the number of criteria.

2.4 Loss

We use the negative log likelihood objective as our
loss function, and add the losses for CWS labeling
and criterion classiﬁcation directly

L = −

log P (Yi|Xi, ci) + log P (ci|Xi, ci),

(7)

N
(cid:88)

i=1

where N is the number of training samples.

3 Experiments

3.1 Datasets

(Jin and Chen, 2008), among which MSRA, PKU,
CTB, NCC and SXU are simpliﬁed Chinese
datasets, while AS, CKIP and CITYU are tradi-
tional Chinese datasets. As previous work (He
et al., 2019; Qiu et al., 2019), we convert all tra-
ditional Chinese datasets into simpliﬁed Chinese
using the OpenCC library 1. All datasets are prepro-
cessed by converting all full-width digits, punctua-
tion and Latin letters to half-width, and replacing
the continuous English characters and digits with a
speciﬁc token respectively.

3.2 Settings

We use the Chinese BERT model pre-trained with
whole word masking (BERT-wwm) (Cui et al.,
2019), whose number of layers is 12, number of
heads if 12, hidden size is dh = 768. We use
the bottom 6 layers as our BERT model for the
balance of performance and speed. The size of
bigram embeddings is set to be de = 100 and the
bigram embeddings are pre-trained on the Chinese
Wikipedia corpus as Qiu et al. (2019). The dropout
probability of all hidden layers is set to be 0.1.

The AdamW (Loshchilov and Hutter, 2019) op-
timizer is used in our ﬁne-tuning process, with
β1 = 0.9, β2 = 0.999 and weight decay of 0.01.
The initial learning rate is 2e-5, and a linear warm-
up of ratio 0.1 is used to adjust the learning rate
dynamically. We set the number of epochs to 10
and the batch size to 64. Our model is implemented
using the fastNLP library 2.

3.3 Overall Results

Table 2 shows results of on the test sets of eight
CWS dataset, with F1 values as metrics. The results
are displayed in four blocks.

The ﬁrst block shows the single criterion meth-
ods for CWS. Bi-LSTM (Chen et al., 2017) is the
baseline method trained on each dataset separately.
The second block shows the multi-task learn-
ing framework methods. Adversarial (Chen et al.,
2017) constructs a multi-task model which adopts
an adversarial strategy to learn more criteria-
invariant representations. Multi-task BERT
(Huang et al., 2019) uses pre-trained BERT as the
sentence encoder.

The third block shows the uniﬁed framework
methods. Uniﬁed Bi-LSTM (He et al., 2016) in-
troduces two artiﬁcial tokens at the beginning and

We experiment on eight CWS datasets from
SIGHAN2005 (Emerson, 2005) and SIGHAN2008

1https://github.com/BYVoid/OpenCC
2https://github.com/fastnlp/fastNLP

MSRA

AS

PKU

CTB

CITYU NCC

SXU

Avg.

94.20

93.30

95.30

92.17

95.17

94.14

Method

Bi-LSTM

Adversarial
Multi-Task BERT

95.84

96.04
97.9

Uniﬁed Bi-LSTM 97.35
97.78
Switch-LSTMs
98.05
Transformer

Uniﬁed BERT
- Bigram
- CLS
- CLS - Bigram

98.45
98.38
98.48
98.41

94.64
96.6

95.47
95.22
96.44

96.90
96.88
96.86
96.83

94.32
96.6

95.78
96.15
96.41

96.89
96.87
96.92
96.83

96.18
-

95.84
97.26
96.99

97.20
97.14
97.13
97.13

CKIP

93.06

94.26
-

95.73
94.99
96.51

96.83
96.72
96.84
96.76

94.07

95.55
97.6

95.60
96.22
96.91

97.07
97.05
97.07
97.05

92.83
-

94.34
94.12
96.04

96.48
96.33
96.55
96.33

96.04
97.3

96.49
97.25
97.61

97.81
97.74
97.72
97.67

94.98
-

95.73
96.12
96.87

97.204
97.139
97.196
97.126

Table 2: F1 values on test sets of eight standard CWS datasets. There are four blocks, indicating single criterion
methods, multi-task learning framework methods, uniﬁed framework methods, and our methods, respectively. The
maximum F1 values are highlighted for each dataset.

Method

MSRA

AS

PKU

CTB

CKIP CITYU NCC

SXU

Avg.

Bi-LSTM
Adversarial
Switch-LSTMs
Transformer
Multi-Task BERT
Uniﬁed BERT

66.28
71.60
64.20
78.92
84.0
83.35

70.07
73.50
77.33
76.39
76.9
79.26

66.09
72.67
69.88
78.91
80.1
79.71

76.47
82.48
83.89
87.00
-
87.77

72.12
77.59
77.69
82.89
-
84.36

65.79
81.40
73.58
86.91
89.7
87.27

59.11
63.31
69.76
79.30
-
81.03

71.27
77.10
78.69
85.08
86.0
86.05

68.40
74.96
74.38
81.92
-
83.60

Table 3: OOV recalls on test sets of eight CWS datasets. The best results are highlighted for each dataset.

ending of input sentence to specify the target cri-
terion. Switch-LSTMs (Gong et al., 2019) uses
Switch-LSTMs as the backbone network. Trans-
former (Qiu et al., 2019) uses the Transformer net-
work (Vaswani et al., 2017) as the basic encoder.

shows the beneﬁt of bigram features and criterion
classiﬁcation. The complete model obtains the best
average performance and achieve new state-of-the-
art results for MCCWS, showing the effectiveness
of our method.

The fourth block shows our methods for MC-
CWS. Uniﬁed BERT is the complete model dis-
cussed in Section 2, which incorporates the uniﬁed
framework and the pre-trained BERT model. It is
also augmented by the fused bigram features and
the auxiliary criterion classiﬁcation task. Besides,
- Bigram is the Uniﬁed BERT model without bi-
gram features, while the - CLS model is the model
without criterion classiﬁcation and - CLS - Bigram
is the model without bigram features and criterion
classiﬁcation.

From Table 2, we could see that our proposed
methods outperform previous methods obviously
on nearly all datasets, especially two previous best
methods: BERT based multi-task method Multi-
task BERT and uniﬁed method Transformer.
These promotions should be attributed to the prior
knowledge introduced by BERT and the shared
knowledge captured by the uniﬁed framework. Be-
sides, we could see that the complete Uniﬁed
BERT model outperforms the model without bi-
gram features (- Bigram) and the model without
auxiliary criterion classiﬁcation (- CLS), which

3.4 OOV Words

OOV words means Out-of-Vocabulary words. Ac-
cording to previous work (Ma et al., 2018; Huang
et al., 2019), OOV error is a critical contribution
to the total error of CWS. We use OOV recall as
metrics to evaluate the performance on OOV words.
The results are showed in Table 3.

We can see that our proposed method can
achieve best OOV performance on ﬁve datasets,
and comparable performance on the other three
datasets. The results demonstrate the effectiveness
of our method on OOV words.

4 Conclusion

In this paper, we mainly focus on the Multi-Criteria
Chinese Word Segmentation (MCCWS) task. We
propose a new method which incorporates the supe-
riority of the uniﬁed framework and the pre-trained
BERT model. Augmented with bigram features and
an auxiliary criterion classiﬁcation task, we achieve
the new state-of-the-art results for MCCWS.

References

Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.
2016. Layer normalization. ArXiv, abs/1607.06450.

Rich Caruana. 1997. Multitask learning. Machine

Learning, 28(1):41–75.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015a. Gated recursive neural network for
Chinese word segmentation. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1744–1753, Beijing,
China. Association for Computational Linguistics.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR).

Weipeng Huang, Xingyi Cheng, Kunlong Chen,
Taifeng Wang, and Wei Chu. 2019. Toward fast
and accurate neural chinese word segmentation with
multi-criteria learning. CoRR, abs/1903.04190.

Guangjin Jin and Xiao Chen. 2008. The fourth inter-
national Chinese language processing bakeoff: Chi-
nese word segmentation, named entity recognition
In Proceedings of the
and Chinese POS tagging.
Sixth SIGHAN Workshop on Chinese Language Pro-
cessing.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,
and Xuanjing Huang. 2015b. Long short-term mem-
ory neural networks for Chinese word segmentation.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1197–1206, Lisbon, Portugal. Association for Com-
putational Linguistics.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random ﬁelds:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
’01, pages 282–289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.

Xinchi Chen, Zhan Shi, Xipeng Qiu, and Xuanjing
Huang. 2017. Adversarial multi-criteria learning
In Proceedings
for Chinese word segmentation.
of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 1193–1203, Vancouver, Canada. Association
for Computational Linguistics.

Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin,
Ziqing Yang, Shijin Wang, and Guoping Hu. 2019.
Pre-training with whole word masking for chinese
BERT. CoRR, abs/1906.08101.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Thomas Emerson. 2005. The second international Chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing.

Jingjing Gong, Xinchi Chen, Tao Gui, and Xipeng
Qiu. 2019. Switch-LSTMs for multi-criteria chinese
In Proceedings of the AAAI
word segmentation.
Conference on Artiﬁcial Intelligence.

Han He, Lei Wu, Hua Yan, Zhimin Gao, Yi Feng, and
George Townsend. 2019. Effective neural solution
for multi-criteria word segmentation. In Smart Intel-
ligent Computing and Applications, pages 133–142,
Singapore. Springer Singapore.

Ilya Loshchilov and Frank Hutter. 2019. Decoupled
In International Con-

weight decay regularization.
ference on Learning Representations.

Ji Ma, Kuzman Ganchev, and David Weiss. 2018.
State-of-the-art Chinese word segmentation with bi-
LSTMs. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 4902–4908, Brussels, Belgium. Association
for Computational Linguistics.

Xipeng Qiu, Hengzhi Pei, Hang Yan, and Xuanjing
Huang. 2019. Multi-criteria chinese word segmen-
tation with transformer. CoRR, abs/1906.12035.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need.
In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, pages 5998–6008. Curran Asso-
ciates, Inc.

Nianwen Xue. 2003. Chinese word segmentation as
character tagging. In International Journal of Com-
putational Linguistics & Chinese Language Process-
ing, Volume 8, Number 1, February 2003: Special Is-
sue on Word Formation and Chinese Language Pro-
cessing, pages 29–48.

Jie Yang, Yue Zhang, and Shuailong Liang. 2019. Sub-
word encoding in lattice LSTM for Chinese word
In Proceedings of the 2019 Confer-
segmentation.
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Pa-
pers), pages 2720–2725, Minneapolis, Minnesota.
Association for Computational Linguistics.

Meishan Zhang, Yue Zhang, and Guohong Fu. 2016.
Transition-based neural word segmentation. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 421–431, Berlin, Germany. Associa-
tion for Computational Linguistics.

Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013.
Deep learning for Chinese word segmentation and
In Proceedings of the 2013 Confer-
POS tagging.
ence on Empirical Methods in Natural Language
Processing, pages 647–657, Seattle, Washington,
USA. Association for Computational Linguistics.

