8
1
0
2
 
y
a
M
 
0
2
 
 
]

G
L
.
s
c
[
 
 
2
v
7
7
5
4
0
.
2
1
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

LEARNING FROM NOISY SINGLY-LABELED DATA

Ashish Khetan
University of Illinois at Urbana-Champaign
Urbana, IL 61801
khetan2@illinois.edu

Zachary C. Lipton
Amazon Web Services
Seattle, WA 98101
liptoz@amazon.com

Animashree Anandkumar
Amazon Web Services
Seattle, WA 98101
anima@amazon.com

ABSTRACT

Supervised learning depends on annotated examples, which are taken to be the
ground truth. But these labels often come from noisy crowdsourcing platforms,
like Amazon Mechanical Turk. Practitioners typically collect multiple labels per
example and aggregate the results to mitigate noise (the classic crowdsourcing
problem). Given a ﬁxed annotation budget and unlimited unlabeled data, redun-
dant annotation comes at the expense of fewer labeled examples. This raises two
fundamental questions: (1) How can we best learn from noisy workers? (2) How
should we allocate our labeling budget to maximize the performance of a classi-
ﬁer? We propose a new algorithm for jointly modeling labels and worker quality
from noisy crowd-sourced data. The alternating minimization proceeds in rounds,
estimating worker quality from disagreement with the current model and then
updating the model by optimizing a loss function that accounts for the current
estimate of worker quality. Unlike previous approaches, even with only one an-
notation per example, our algorithm can estimate worker quality. We establish a
generalization error bound for models learned with our algorithm and establish
theoretically that it’s better to label many examples once (vs less multiply) when
worker quality exceeds a threshold. Experiments conducted on both ImageNet
(with simulated noisy workers) and MS-COCO (using the real crowdsourced la-
bels) conﬁrm our algorithm’s beneﬁts.

1

INTRODUCTION

Recent advances in supervised learning owe, in part, to the availability of large annotated datasets.
For instance, the performance of modern image classiﬁers saturates only with millions of labeled
examples. This poses an economic problem: Assembling such datasets typically requires the labor
of human annotators. If we conﬁned the labor pool to experts, this work might be prohibitively ex-
pensive. Therefore, most practitioners turn to crowdsourcing platforms such as Amazon Mechanical
Turk (AMT), which connect employers with low-skilled workers who perform simple tasks, such as
classifying images, at low cost.

Compared to experts, crowd-workers provide noisier annotations, possibly owing to high variation
in worker skill; and a per-answer compensation structure that encourages rapid answers, even at the
expense of accuracy. To address variation in worker skill, practitioners typically collect multiple in-
dependent labels for each training example from different workers. In practice, these labels are often
aggregated by applying a simple majority vote. Academics have proposed many efﬁcient algorithms
for estimating the ground truth from noisy annotations. Research addressing the crowd-sourcing
problem goes back to the early 1970s. Dawid & Skene (1979) proposed a probabilistic model to
jointly estimate worker skills and ground truth labels and used expectation maximization (EM) to
estimate the parameters. Whitehill et al. (2009); Welinder et al. (2010); Zhou et al. (2015) proposed
generalizations of the Dawid-Skene model, e.g. by estimating the difﬁculty of each example.

1

Published as a conference paper at ICLR 2018

Although the downstream goal of many crowdsourcing projects is to train supervised learning mod-
els, research in the two disciplines tends to proceed in isolation. Crowdsourcing research seldom
accounts for the downstream utility of the produced annotations as training data in machine learning
(ML) algorithms. And ML research seldom exploits the noisy labels collected from multiple human
workers. A few recent papers use the original noisy labels and the corresponding worker identities
together with the predictions of a supervised learning model trained on those same labels, to estimate
the ground truth (Branson et al., 2017; Guan et al., 2017; Welinder et al., 2010). However, these pa-
pers do not realize the full potential of combining modeling and crowd-sourcing. In particular, they
are unable to estimate worker qualities when there is only one label per training example.

This paper presents a new supervised learning algorithm that alternately models the labels and
worker quality. The EM algorithm bootstraps itself in the following way: Given a trained model,
the algorithm estimates worker qualities using the disagreement between workers and the current
predictions of the learning algorithm. Given estimated worker qualities, our algorithm optimizes a
suitably modiﬁed loss function. We show that accurate estimates of worker quality can be obtained
even when only collecting one label per example provided that each worker labels sufﬁciently many
examples. An accurate estimate of the worker qualities leads to learning a better model. This
addresses a shortcoming of the prior work and overcomes a signiﬁcant hurdle to achieving practical
crowdsourcing without redundancy.

We give theoretical guarantees on the performance of our algorithm. We analyze the two alternating
steps: (a) estimating worker qualities from disagreement with the model, (b) learning a model by
optimizing the modiﬁed loss function. We obtain a bound on the accuracy of the estimated worker
qualities and the generalization error of the model. Through the generalization error bound, we
establish that it is better to label many examples once than to label less examples multiply when
worker quality is above a threshold. Empirically, we verify our approach on several multi-class
classiﬁcation datasets: ImageNet and CIFAR10 (with simulated noisy workers), and MS-COCO
(using the real noisy annotator labels). Our experiments validate that when the cost of obtaining
unlabeled examples is negligible and the total annotation budget is ﬁxed, it is best to collect a single
label per training example for as many examples as possible. We emphasize that although this paper
applies our approach to classiﬁcation problems, the main ideas of the algorithm can be extended to
other tasks in supervised learning.

2 RELATED WORK

The traditional crowdsourcing problem addresses the challenge of aggregating multiple noisy labels.
A naive approach is to aggregate the labels based on majority voting. More sophisticated agreement-
based algorithms jointly model worker skills and ground truth labels, estimating both using EM or
similar techniques (Dawid & Skene, 1979; Jin & Ghahramani, 2003; Whitehill et al., 2009; Welinder
et al., 2010; Zhou et al., 2012; Liu et al., 2012; Dalvi et al., 2013; Liu et al., 2012). Zhang et al.
(2014) shows that the EM algorithm with spectral initialization achieves minimax optimal perfor-
mance under the Dawid-Skene model. Karger et al. (2014) introduces a message-passing algorithm
for estimating binary labels under the Dawid-Skene model, showing that it performs strictly better
than majority voting when the number of labels per example exceeds some threshold. Similar ob-
servations are made by (Bragg et al., 2016). A primary criticism of EM-based approaches is that in
practice, it’s rare to collect more than 3 to 5 labels per example; and with so little redundancy, the
small gains achieved by EM over majority voting are not compelling to practitioners. In contrast,
our algorithm performs well in the low-redundancy setting. Even with just one label per example,
we can accurately estimate worker quality.

Several prior crowdsourcing papers incorporate the predictions of a supervised learning model, to-
gether with the noisy labels, to estimate the ground truth labels. Welinder et al. (2010) consider
binary classiﬁcation and frames the problem as a generative Bayesian model on the features of the
examples and the labels. Branson et al. (2017) considers a generalization of the Dawid-Skene model
and estimates its parameters using supervised learning in the loop. In particular, they consider a joint
probability over observed image features, ground truth labels, and the worker labels and computes
the maximum likelihood estimate of the true labels using alternating minimization. We also consider
a joint probability model but it is signiﬁcantly different from theirs as we assume that the optimal
labeling function gives the ground truth labels. We maximize the joint likelihood using a variation

2

Published as a conference paper at ICLR 2018

of expectation maximization to learn the optimal labeling function and the true labels. Further, they
train the supervised learning model using the intermediate predictions of the labels whereas we train
the model by minimizing a weighted loss function where the weights are the intermediate posterior
probability distribution of the labels. Moreover, with only one label per example, their algorithm
fails and estimates all the workers to be equally good. They only consider binary classiﬁcation,
whereas we verify our algorithm on multi-class (ten classes) classiﬁcation problem.

A rich body of work addresses human-in-loop annotation for computer vision tasks. However, these
works assume that humans are experts, i.e., that they give noiseless annotations (Branson et al.,
2010; Deng et al., 2013; Wah et al., 2011). We assume workers are unreliable and have varying
skills. A recent work by Ratner et al. (2016) also proposes to use predictions of a supervised learning
model to estimate the ground truth. However, their algorithm is signiﬁcantly different than ours as
it does not use iterative estimation technique, and their approach of incorporating worker quality
parameters in the supervised learning model is different. Their theoretical results are limited to the
linear classiﬁers.

Another line of work employs active learning, iteratively ﬁltering out examples for which aggregated
labels have high conﬁdence and collect additional labels for the remaining examples (Whitehill et al.,
2009; Welinder & Perona, 2010; Khetan & Oh, 2016). The underlying modeling assumption in these
papers is that the questions have varying levels of difﬁculty. At each iteration, these approaches
employ an EM-based algorithm to estimate the ground truth label of the remaining unclassiﬁed
examples. For simplicity, our paper does not address example difﬁculties, but we could easily extend
our model and algorithm to accommodate this complexity.

Several papers analyze whether repeated labeling is useful. Sheng et al. (2008) analyzed the effect of
repeated labeling and showed that it depends upon the relative cost of getting an unlabeled example
and the cost of labeling. Ipeirotis et al. (2014) shows that if worker quality is below a threshold then
repeated labeling is useful, otherwise not. Lin et al. (2014a; 2016) argues that it also depends upon
expressiveness of the classiﬁer in addition to the factors considered by others. However, these works
do not exploit predictions of the supervised learning algorithm to estimate the ground truth labels,
and hence their ﬁndings do not extend to our methodology.

Another body of work that is relevant to our problem is learning with noisy labels where usual as-
sumption is that all the labels are generated through the same noisy rate given their ground truth
label. Recently Natarajan et al. (2013) proposed a generic unbiased loss function for binary clas-
siﬁcation with noisy labels. They employed a modiﬁed loss function that can be expressed as a
weighted sum of the original loss function, and gave theoretical bounds on the performance. How-
ever, their weights become unstably large when the noise rate is large, and hence the weights need
to be tuned. Sukhbaatar et al. (2014); Jindal et al. (2016) learns noise rate as parameters of the
model. A recent work by Guan et al. (2017) trains an individual softmax layer for each expert and
then predicts their weighted sum where weights are also learned by the model. It is not scalable to
crowdsourcing scenario where there are thousands of workers. There are works that aim to create
noise-robust models (Joulin et al., 2016; Krause et al., 2016), but they are not relevant to our work.

3 PROBLEM FORMULATION

Let D be the underlying true distribution generating pairs (X, Y ) ∈ X × K from which n i.i.d.
samples (X1, Y1), (X2, Y2), · · · , (Xn, Yn) are drawn, where K denotes the set of possible labels
K := {1, 2, · · · , K}, and X ⊆ Rd denotes the set of euclidean features. We denote the marginal
distribution of Y by {q1, q2, · · · , qK}, which is unknown to us. Consider a pool of m workers
indexed by 1, 2, · · · , m. We use [m] to denote the set {1, 2, · · · , m}. For each i-th sample Xi, r
workers {wij}j∈[r] ∈ [m]r are selected randomly, independent of the sample Xi. Each selected
worker provides a noisy label Zij for the sample Xi, where the distribution of Zij depends on the
selected worker and the true label Yi. We call r the redundancy and, for simplicity, assume it to
be the same for each sample. However, our algorithm can also be applied when redundancy varies
across the samples. We use Z (r)
to denote {Zij}j∈[r], the set of r labels collected on the i-th
example, and w(r)

to denote {wij}j∈[r].

i

i

Following Dawid & Skene (1979), we assume the probability that the a-th worker labels an item in
class k ∈ K as class s ∈ K is independent of any particular chosen item, that is, it is a constant over

3

Published as a conference paper at ICLR 2018

i ∈ [n]. Let us denote this constant by πks; by deﬁnition, (cid:80)
π(a) ∈ [0, 1]K×K the confusion matrix of the a-th worker. In particular, the distribution of Z is:

s∈K πks = 1 for all k ∈ K, and we call

P [Zij = s | Yi = k, wij = a] = π(a)
ks .

(1)

The diagonal entries of the confusion matrix correspond to the probabilities of correctly labeling an
example. The off-diagonal entries represent the probability of mislabeling. We use π to denote the
collection of confusion matrices {π(a)}a∈[m].
We assume nr workers w1,1, w1,2, · · · , wn,r are selected uniformly at
random from a
pool of m workers with replacement and a batch of r workers are assigned to each of
the examples X1, X2, · · · , Xn.
The corrupted labels along with the worker information
(X1, Z (r)
1 ), · · · , (Xn, Z (r)
Let F be the hypothesis class, and f ∈ F, f : X → RK, denote a vector valued predictor function.
Let (cid:96)(f (X), Y ) denote a loss function. For a predictor f , its (cid:96)-risk under D is deﬁned as

n ) are what the learning algorithm sees.

1 , w(r)

n , w(r)

R(cid:96),D(f )

:= E(X,Y )∼D [(cid:96)(f (X), Y )] .

(2)

1 , w(r)

Given the observed samples (X1, Z (r)
n ), we want to learn a good pre-
dictor function (cid:98)f ∈ F such that its risk under the true distribution D, R(cid:96),D( (cid:98)f ) is minimal. Having
access to only noisy labels Z (r) by workers w(r), we compute (cid:98)f as the one which minimizes a
(cid:98)π,(cid:98)q(f (X), Z (r), w(r)). Where (cid:98)π denote an estimate of confusion
suitably modiﬁed loss function (cid:96)
matrix π, and (cid:98)q an estimate of q, the prior distribution on Y . We deﬁne (cid:96)
(cid:98)π,(cid:98)q in the following section.

1 ), · · · , (Xn, Z (r)

n , w(r)

4 ALGORITHM

Assume that there exists a function f ∗ ∈ F such that f ∗(Xi) = Yi for all i ∈ [n]. Under the
Dawid-Skene model (described in previous section), the joint likelihood of true labeling function
f ∗(Xi) and observed labels {Zij}i∈[n],j∈[r] as a function of confusion matrices of workers π can be
written as

L (cid:0)π; f ∗, {Xi}i∈[n], {Zij}i∈[n],j∈[r]
(cid:32)





(cid:1) :=

n
(cid:89)

(cid:88)



i=1

k∈K

r
(cid:89)

(cid:88)

j=1

s∈K

qkI[f ∗(Xi) = k]



I[Zij = s]π(wij )

ks

(3)

(cid:33)




 .

qk’s are the marginal distribution of the true labels Yi’s. We estimate the worker confusion matrices
π and the true labeling function f ∗ by maximizing the likelihood function L(π; f ∗(X), Z). Ob-
serve that the likelihood function L(π; f ∗(X), Z) is different than the standard likelihood function
of Dawid-Skene model in that we replace each true hidden labels Yi by f ∗(Xi). Like the EM al-
gorithm introduced in (Dawid & Skene, 1979), we propose ‘Model Bootstrapped EM’ (MBEM) to
estimate confusion matrices π and the true labeling function f ∗. EM converges to the true confusion
matrices and the true labels, given an appropriate spectral initialization of worker confusion matrices
(Zhang et al., 2014). We show in Section 4.4 that MBEM converges under mild conditions when
the worker quality is above a threshold and the number of training examples is sufﬁciently large. In
the following two subsections, we motivate and explain our iterative algorithm to estimate the true
labeling function f ∗, given a good estimate of worker confusion matrices π and vice-versa.

4.1 LEARNING WITH NOISY LABELS

To begin, we ask, what is the optimal approach to learn the predictor function (cid:98)f when for each
worker we have (cid:98)π, a good estimation of the true confusion matrix π, and (cid:98)q, an estimate of the prior?
A recent paper, Natarajan et al. (2013) proposes minimizing an unbiased loss function speciﬁcally,
a weighted sum of the original loss over each possible ground truth label. They provide weights for
binary classiﬁcation where each example is labeled by only one worker. Consider a worker with
confusion matrix π, where πy > 1/2 and π−y > 1/2 represent her probability of correctly labeling
the examples belonging to class y and −y respectively. Then their weights are π−y/(πy + π−y − 1)
for class y and −(1 − πy)/(πy + π−y − 1) for class −y. It is evident that their weights become

4

Published as a conference paper at ICLR 2018

unstably large when the probabilities of correct classiﬁcation πy and π−y are close to 1/2, limiting
the method’s usefulness in practice. As explained below, for the same scenario, our weights would
be πy/(1 + πy − π−y) for class y and (1 − π−y)/(1 + πy − π−y) for class −y. Inspired by their idea,
we propose weighing the loss function according to the posterior distribution of the true label given
the Z (r) observed labels and an estimate of the confusion matrices of the worker who provided those
labels. In particular, we deﬁne (cid:96)

(cid:98)π,(cid:98)q to be

(cid:98)π,(cid:98)q(f (X), Z (r), w(r))
(cid:96)

:=

P
(cid:98)π,(cid:98)q[Y = k | Z (r); w(r)] (cid:96)(f (X), Y = k) .

(4)

(cid:88)

k∈K

If the observed label is uniformly random, then all weights are equal and the loss is identical for all
predictor functions f . Absent noise, we recover the original loss function. Under the Dawid-Skene
model, given the observed noisy labels Z (r), an estimate of confusion matrices (cid:98)π, and an estimate
of prior (cid:98)q, the posterior distribution of the true labels can be computed as follows:
I[Zij = s](cid:98)π(wij )

(cid:16) (cid:80)

(cid:81)r

j=1

(cid:17)

ks

(cid:98)qk
(cid:16)

s∈K
(cid:16) (cid:80)

(cid:80)

k(cid:48)∈K

(cid:98)qk(cid:48) (cid:81)r

j=1

I[Zij = s](cid:98)π(wij )

k(cid:48)s

s∈K

(cid:17)(cid:17) ,

(5)

(cid:98)π,(cid:98)q[Yi = k | Z (r)
P

i

; w(r)
i

] =

where I[.] is the indicator function which takes value one if the identity inside it is true, otherwise
zero. We give guarantees on the performance of the proposed loss function in Theorem 4.1. In
practice, it is robust to noise level and signiﬁcantly outperforms the unbiased loss function. Given
(cid:98)π,(cid:98)q, we learn the predictor function (cid:98)f by minimizing the empirical risk
(cid:96)

(cid:98)f ← arg min
f ∈F

(cid:98)π,(cid:98)q(f (Xi), Z (r)
(cid:96)

i

, w(r)
i

) .

(6)

1
n

n
(cid:88)

i=1

4.2 ESTIMATING ANNOTATOR NOISE

The next question is: how do we get a good estimate (cid:98)π of the true confusion matrix π for each worker.
If redundancy r is sufﬁciently large, we can employ the EM algorithm. However, in practical appli-
cations, redundancy is typically three or ﬁve. With so little redundancy, the standard applications of
EM are of limited use. In this paper we look to transcend this problem, posing the question: Can we
estimate confusion matrices of workers even when there is only one label per example? While this
isn’t possible in the standard approach, we can overcome this obstacle by incorporating a supervised
learning model into the process of assessing worker quality.

Under the Dawid-Skene model, the EM algorithm estimates the ground truth labels and the con-
fusion matrices in the following way: It alternately ﬁxes the ground truth labels and the confusion
matrices by their estimates and and updates its estimate of the other by maximizing the likelihood
of the observed labels. The alternating maximization begins by initializing the ground truth labels
with a majority vote. With only 1 label per example, EM estimates that all the workers are perfect.

We propose using model predictions as estimates of the ground truth labels. Our model is initially
trained on the majority vote of the labels. In particular, if the model prediction is {ti}i∈[n], where
ti ∈ K, then the maximum likelihood estimate of confusion matrices and the prior distribution is
given below. For the a-th worker, (cid:98)π(a)
(cid:80)n

ks for k, s ∈ K, and (cid:98)qk for k ∈ K, we have,

(cid:80)r

(cid:98)π(a)
ks =

i=1

j=1

(cid:80)n

I[wij = a]I[ti = k]I[Zij = s]
(cid:80)r
I[wij = a]I[ti = k]

,

i=1

j=1

(cid:98)qk = (1/n)

I[ti = k]

(7)

n
(cid:88)

i=1

The estimate is effective when the hypothesis class F is expressive enough and the learner is robust
to noise. Thus the model should, in general, have small training error on correctly labeled examples
and large training error on wrongly labeled examples. Consider the case when there is only one
label per example. The model will be trained on the raw noisy labels given by the workers. For
simplicity, assume that each worker is either a hammer (always correct) or a spammer (chooses
labels uniformly random). By comparing model predictions with the training labels, we can identify
which workers are hammers and which are spammers, as long as each worker labels sufﬁciently
many examples. We expect a hammer to agree with the model more often than a spammer.

5

Published as a conference paper at ICLR 2018

4.3

ITERATIVE ALGORITHM

Building upon the previous two ideas, we present ‘Model Bootstrapped EM’, an iterative algorithm
for efﬁcient learning from noisy labels with small redundancy. MBEM takes data, noisy labels, and
the corresponding worker IDs, and returns the best predictor function (cid:98)f in the hypothesis class F.
In the ﬁrst round, we compute the weights of the modiﬁed loss function (cid:96)
(cid:98)π,(cid:98)q by using the weighted
majority vote. Then we obtain an estimate of the worker confusion matrices (cid:98)π using the maximum
likelihood estimator by taking the model predictions as the ground truth labels. In the second round,
weights of the loss function are computed as the posterior probability distribution of the ground
truth labels conditioned on the noisy labels and the estimate of the confusion matrices obtained
in the previous round.
In our experiments, only two rounds are required to achieve substantial
improvements over baselines.

Algorithm 1 Model Bootstrapped EM (MBEM)
Input: {(Xi, Z (r)
i
Output: (cid:98)f : predictor function
Initialize posterior distribution using weighted majority vote

)}i∈[n], T : number of iterations

, w(r)
i

(cid:98)π,(cid:98)q[Yi = k | Z (r)
P

i

; w(r)
i

] ← (1/r) (cid:80)r

j=1

I[Zij = k] , for k ∈ K, i ∈ [n]

Repeat T times:

P

1
n

i=1

k∈K

(cid:80)n

(cid:98)π,(cid:98)q[Yi = k | Z (r)

learn predictor function (cid:98)f
(cid:80)
(cid:98)f ← arg minf ∈F
predict on training examples
ti ← arg maxk∈K (cid:98)f (Xi)k, for i ∈ [n]
estimate confusion matrices (cid:98)π and prior class distribution (cid:98)q given {ti}i∈[n]
(cid:98)π(a) ← Equation (7), for a ∈ [m]; (cid:98)q ← Equation (7)
estimate label posterior distribution given (cid:98)π, (cid:98)q
(cid:98)π,(cid:98)q[Yi = k | Z (r)
P

], ← Equation (5), for k ∈ K, i ∈ [n]

] (cid:96)(f (Xi), Yi = k)

; w(r)
i

; w(r)
i

i

i

Return (cid:98)f

4.4 PERFORMANCE GUARANTEES

The following result gives guarantee on the excess risk for the learned predictor function (cid:98)f in terms
of the VC dimension of the hypothesis class F. Recall that risk of a function f w.r.t. loss function (cid:96)
is deﬁned to be R(cid:96),D(f ) := E(X,Y )∼D [(cid:96)(f (X), Y )], Equation (2). We assume that the classiﬁcation
problem is binary, and the distribution q, prior on ground truth labels Y , is uniform and is known to
us. We give guarantees on the excess risk of the predictor function (cid:98)f , and accuracy of (cid:98)π estimated
in the second round. For the purpose of analysis, we assume that fresh samples are used in each
round for computing function (cid:98)f and estimating (cid:98)π. In other words, we assume that (cid:98)f and (cid:98)π are
each computed using n/4 fresh samples in the ﬁrst two rounds. We deﬁne α and β(cid:15) to capture the
average worker quality. Here, we give their concise bound for a special case when all the workers
are identical, and their confusion matrix is represented by a single parameter, 0 ≤ ρ < 1/2. Where
πkk = 1 − ρ, and πks = ρ for k (cid:54)= s. Each worker makes a mistake with probability ρ. β(cid:15) ≤
(cid:1)(τ u + τ r−u)−1, where τ := (ρ + (cid:15))/(1 − ρ − (cid:15)). α for this special case is ρ. A
(ρ + (cid:15))r (cid:80)r
general deﬁnition of α and β(cid:15) for any confusion matrices π is provided in the Appendix.
Theorem 4.1. Deﬁne N := nr to be the number of total annotations collected on n training exam-
ples with redundancy r. Suppose minf ∈F R(cid:96),D(f ) ≤ 1/4. For any hypothesis class F with a ﬁnite
VC dimension V , and any δ < 1, there exists a universal constant C such that if N is large enough
and satisﬁes

(cid:0)r
u

u=0

N ≥ max

(cid:26)

Cr(cid:0)(cid:0)√

V + (cid:112)log(1/δ)(cid:1)/(1 − 2α)(cid:1)2

, 212m log(26m/δ)

,

(cid:27)

then for binary classiﬁcation with 0-1 loss function (cid:96), (cid:98)f and (cid:98)π returned by Algorithm 1 after T = 2
iterations satisﬁes

R(cid:96),D( (cid:98)f ) − min
f ∈F

R(cid:96),D(f ) ≤

√

C
r
1 − 2β(cid:15)

(cid:32)(cid:114)

(cid:114)

V
N

+

log(1/δ)
N

(cid:33)

,

(8)

(9)

6

Published as a conference paper at ICLR 2018

and (cid:107)(cid:98)π(a) − π(a)(cid:107)∞ ≤ (cid:15)1 for all a ∈ [m], with probability at least 1 − δ. Where (cid:15) := 24γ +
28(cid:112)m log(26mδ)/N , and γ := minf ∈F R(cid:96),D(f ) + C(
V + (cid:112)log(1/δ))/((1 − 2α)(cid:112)N/r). (cid:15)1
is deﬁned to be (cid:15) with α in it replaced by β(cid:15).

√

The price we pay in generalization error bound on (cid:98)f is (1 − 2β(cid:15)). Note that, when n is large, (cid:15) goes
to zero, and β(cid:15) ≤ 2ρ(1 − ρ), for r = 1.

If minf ∈F R(cid:96),D(f ) is sufﬁciently small, VC dimension is ﬁnite, and ρ is bounded away from 1/2
then for n = O(m log(m)/r), we get (cid:15)1 to be sufﬁciently small. Therefore, for any redundancy r,
error in confusion matrix estimation is small when the number of training examples is sufﬁciently
large. Hence, for N large enough, using Equation (9) and the bound on β(cid:15), we get that for ﬁxed
total annotation budget, the optimal choice of redundancy r is 1 when the worker quality (1 − ρ) is
above a threshold. In particular, if (1 − ρ) ≥ 0.825 then label once is the optimal strategy. However,
in experiments we observe that with our algorithm the choice of r = 1 is optimal even for much
smaller values of worker quality.

5 EXPERIMENTS

We experimentally investigate our algorithm, MBEM, on multiple large datasets. On CIFAR-10
(Krizhevsky & Hinton, 2009) and ImageNet (Deng et al., 2009), we draw noisy labels from synthetic
worker models. We conﬁrm our results on multiple worker models. On the MS-COCO dataset (Lin
et al., 2014b), we accessed the real raw data that was used to produce this annotation. We compare
MBEM against the following baselines:

• MV: First aggregate labels by performing a majority vote, then train the model.
• weighted-MV: Model learned using weighted loss function with weights set by majority vote.
• EM: First aggregate labels using EM. Then train model in the standard fashion. (Dawid & Skene,

1979)

• weighted-EM: Model learned using weighted loss function with weights set by standard EM.
• oracle weighted EM: This model is learned by minimizing (cid:96)π, using the true confusion matrices.
• oracle correctly labeled: This baseline is trained using the standard loss function (cid:96) but only using

those training examples for which at least one of the r workers has given the true label.

Note that oracle models cannot be deployed in practice. We show them to build understanding
only. In the plots, the dashed lines correspond to MV and EM algorithm. The black dashed-dotted
line shows generalization error if the model is trained using ground truth labels on all the training
examples. For experiments with synthetic noisy workers, we consider two models of worker skill:

• hammer-spammer: Each worker is either a hammer (always correct) with probability γ or a

spammer (chooses labels uniformly at random).

• class-wise hammer-spammer: Each worker can be a hammer for some subset of classes and a
spammer for the others. The confusion matrix in this case has two types of rows: (a) hammer
class: row with all off-diagonal elements being 0. (b) spammer class: row with all elements being
1/|K|. A worker is a hammer for any class k ∈ K with probability γ.

We sample m confusion matrices {π(a)}a∈[m] according to the given worker skill distribution for a
given γ. We assign r workers uniformly at random to each example. Given the ground truth labels,
we generate noisy labels according to the probabilities given in a worker’s confusion matrix, using
Equation (1). While our synthetic workers are sampled from these speciﬁc worker skill models, our
algorithms do not use this information to estimate the confusion matrices. A Python implementation
of the MBEM algorithm is available for download at https://github.com/khetan2/MBEM.

CIFAR-10 This dataset has a total of 60K images belonging to 10 different classes where each
class is represented by an equal number of images. We use 50K images for training the model
and 10K images for testing. We use the ground truth labels to generate noisy labels from synthetic
workers. We choose m = 100, and for each worker, sample confusion matrix of size 10 × 10
according to the worker skill distribution. All our experiments are carried out with a 20-layer ResNet

7

Published as a conference paper at ICLR 2018

hammer-spammer workers

r
o
r
r
e

n
o
i
t
a
z
i
l
a
r
e
n
e
G

r
o
r
r
e

n
o
i
t
a
z
i
l
a
r
e
n
e
G

worker quality

redundancy

redundancy (ﬁxed budget)

class-wise hammer-spammer workers

worker quality

redundancy

redundancy (ﬁxed budget)

Figure 1: Plots for CIFAR-10. Line colors- black: oracle correctly labeled, red: oracle weighted
EM, blue: MBEM, green: weighted EM, yellow: weighted MV.

which achieves an accuracy of 91.5%. With the larger ResNet-200, we can obtain a higher accuracy
of 93.5% but to save training time we restrict our attention to ResNet-20. We run MBEM 1 for
T = 2 rounds. We assume that the prior distribution (cid:98)q is uniform. We report mean accuracy of 5
runs and its standard error for all the experiments.

Figure 1 shows plots for CIFAR-10 dataset under various settings. The three plots in the ﬁrst row
correspond to “hammer-spammer” worker skill distribution and the plots in the second row corre-
spond to “class-wise hammer-spammer” distribution. In the ﬁrst plot, we ﬁx redundancy r = 1, and
plot generalization error of the model for varying hammer probability γ. MBEM signiﬁcantly out-
performs all baselines and closely matches the Oracle weighted EM. This implies MBEM recovers
worker confusion matrices accurately even when we have only one label per example. When there
is only one label per example, MV, weighted-MV, EM, and weighted-EM all reduce learning with
the standard loss function (cid:96).

In the second plot, we ﬁx hammer probability γ = 0.2, and vary redundancy r. This plot shows that
weighted-MV and weighted-EM perform signiﬁcantly better than MV and EM and conﬁrms that
our approach of weighing the loss function with posterior probability is effective. MBEM performs
much better than weighted-EM at small redundancy, demonstrating the effect of our bootstrapping
idea. However, when redundancy is large, EM works as good as MBEM.

In the third plot, we show that when the total annotation budget is ﬁxed, it is optimal to collect
one label per example for as many examples as possible. We ﬁxed hammer probability γ = 0.2.
Here, when redundancy is increased from 1 to 2, the number of of available training examples is
reduced by 50%, and so on. Performance of weighted-EM improves when redundancy is increased
from 1 to 5, showing that with the standard EM algorithm it might be better to collect redundant
annotations for fewer example (as it leads to better estimation of worker qualities) than to singly
annotate more examples. However, MBEM always performs better than the standard EM algorithm,
achieving lowest generalization error with many singly annotated examples. Unlike standard EM,
MBEM can estimate worker qualities even with singly annotated examples by comparing them with
model predictions. This corroborates our theoretical result that label-once is the optimal strategy
when worker quality is above a threshold. The plots corresponding to class-wise hammer-spammer
workers follow the same trend. Estimation of confusion matrices in this setting is difﬁcult and hence
the gap between MBEM and the baselines is less pronounced.

ImageNet The ImageNet-1K dataset contains 1.2M training examples and 50K validation exam-
ples. We divide test set in two parts: 10K for validation and 40K for test. Each example belongs to
one of the possible 1000 classes. We implement our algorithms using a ResNet-18 that achieves top-

8

Published as a conference paper at ICLR 2018

hammer-spammer workers

worker quality - 0.2

worker quality - 0.6

class-wise hammer-spammer workers
worker quality - 0.2
worker quality - 0.6

r
o
r
r
e

n
o
i
t
a
z
i
l
a
r
e
n
e
G

redundancy (ﬁxed budget)

redundancy (ﬁxed budget)

redundancy (ﬁxed budget)

redundancy (ﬁxed budget)

Figure 2: Plots for ImageNet. Solid lines represent top-5 error, dashed-lines represent top-1 error.
Line colors- blue: MBEM, green: weighted majority vote, yellow: majority vote

Approach
majority vote
EM
MBEM
ground truth labels

F1 score
0.433
0.447
0.451
0.512

e
r
o
c
s
1
F
n
o
i
t
a
z
i
l
a
r
e
n
e
G

redundancy (ﬁxed budget)

Figure 3: Results on raw MS-COCO annotations.
1 accuracy of 69.5% and top-5 accuracy of 89% on ground truth labels. We use m = 1000 simulated
workers. Although in general, a worker can mislabel an example to one of the 1000 possible classes,
our simulated workers mislabel an example to only one of the 10 possible classes. This captures the
intuition that even with a larger number of classes, perhaps only a small number are easily confused
for each other. Therefore, each workers’ confusion matrix is of size 10 × 10. Note that without this
assumption, there is little hope of estimating a 1000 × 1000 confusion matrix for each worker by
collecting only approximately 1200 noisy labels from a worker. The rest of the settings are the same
as in our CIFAR-10 experiments. In Figure 2, we ﬁx total annotation budget to be 1.2M and vary
redundancy from 1 to 9. When redundancy is 9, we have only (1.2/9)M training examples, each
labeled by 9 workers. MBEM outperforms baselines in each of the plots, achieving the minimum
generalization error with many singly annotated training examples.

MS-COCO These experiments use the real raw annotations collected when MS-COCO was
crowdsourced. Each image in the dataset has multiple objects (approximately 3 on average). For
validation set images (out of 40K), labels were collected from 9 workers on average. Each worker
marks which out of the 80 possible objects are present. However, on many examples workers dis-
agree. These annotations were collected to label bounding boxes but we ask a different question:
what is the best way to learn a model to perform multi-object classiﬁcation, using these noisy an-
notations. We use 35K images for training the model and 1K for validation and 4K for testing. We
use raw noisy annotations for training the model and the ﬁnal MS-COCO annotations as the ground
truth for the validation and test set. We use ResNet-98 deep learning model and train independent
binary classiﬁer for each of the 80 object classes. Table in Figure 3 shows generalization F1 score
of four different algorithms: majority vote, EM, MBEM using all 9 noisy annotations on each of the
training examples, and a model trained using the ground truth labels. MBEM performs signiﬁcantly
better than the standard majority vote and slightly improves over EM. In the plot, we ﬁx the total
annotation budget to 35K. We vary redundancy from 1 to 7, and accordingly reduce the number of
training examples to keep the total number of annotations ﬁxed. When redundancy is r < 9 we
select uniformly at random r of the original 9 noisy annotations. Again, we ﬁnd it best to singly an-
notate as many examples as possible when the total annotation budget is ﬁxed. MBEM signiﬁcantly
outperforms majority voting and EM at small redundancy.

6 CONCLUSION

We introduced a new algorithm for learning from noisy crowd workers. We also presented a new
theoretical and empirical demonstration of the insight that when examples are cheap and annotations
expensive, it’s better to label many examples once than to label few multiply when worker quality is

9

Published as a conference paper at ICLR 2018

above a threshold. Many avenues seem ripe for future work. We are especially keen to incorporate
our approach into active query schemes, choosing not only which examples to annotate, but which
annotator to route them to based on our models current knowledge of both the data and the worker
confusion matrices.

REFERENCES

Jonathan Bragg, Daniel S Weld, et al. Optimal testing for crowd workers. In Proceedings of the
2016 International Conference on Autonomous Agents & Multiagent Systems, pp. 966–974. In-
ternational Foundation for Autonomous Agents and Multiagent Systems, 2016.

Steve Branson, Catherine Wah, Florian Schroff, Boris Babenko, Peter Welinder, Pietro Perona, and
Serge Belongie. Visual recognition with humans in the loop. Computer Vision–ECCV 2010, pp.
438–451, 2010.

Steve Branson, Grant Van Horn, and Pietro Perona. Lean crowdsourcing: Combining humans and
machines in an online system. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 7474–7483, 2017.

Nilesh Dalvi, Anirban Dasgupta, Ravi Kumar, and Vibhor Rastogi. Aggregating crowdsourced
In Proceedings of the 22nd international conference on World Wide Web, pp.

binary ratings.
285–294. ACM, 2013.

Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates

using the em algorithm. Applied statistics, pp. 20–28, 1979.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical

Image Database. In CVPR09, 2009.

Jia Deng, Jonathan Krause, and Li Fei-Fei. Fine-grained crowdsourcing for ﬁne-grained recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 580–587,
2013.

Melody Y Guan, Varun Gulshan, Andrew M Dai, and Geoffrey E Hinton. Who said what: Modeling

individual labelers improves classiﬁcation. arXiv preprint arXiv:1703.08774, 2017.

Panagiotis G Ipeirotis, Foster Provost, Victor S Sheng, and Jing Wang. Repeated labeling using

multiple noisy labelers. Data Mining and Knowledge Discovery, 28(2):402–441, 2014.

Rong Jin and Zoubin Ghahramani. Learning with multiple labels. In Advances in neural information

processing systems, pp. 921–928, 2003.

Ishan Jindal, Matthew Nokleby, and Xuewen Chen. Learning deep networks from noisy labels with
dropout regularization. In Data Mining (ICDM), 2016 IEEE 16th International Conference on,
pp. 967–972. IEEE, 2016.

Armand Joulin, Laurens van der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual fea-
tures from large weakly supervised data. In European Conference on Computer Vision, pp. 67–84.
Springer, 2016.

David R Karger, Sewoong Oh, and Devavrat Shah. Budget-optimal task allocation for reliable

crowdsourcing systems. Operations Research, 62(1):1–24, 2014.

Ashish Khetan and Sewoong Oh. Achieving budget-optimality with adaptive schemes in crowd-

sourcing. In Advances in Neural Information Processing Systems, pp. 4844–4852, 2016.

Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard Zhou, Alexander Toshev, Tom Duerig,
James Philbin, and Li Fei-Fei. The unreasonable effectiveness of noisy data for ﬁne-grained
recognition. In European Conference on Computer Vision, pp. 301–320. Springer, 2016.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Christopher H Lin, Daniel S Weld, et al. To re (label), or not to re (label). In Second AAAI conference

on human computation and crowdsourcing, 2014a.

10

Published as a conference paper at ICLR 2018

Christopher H Lin, M Mausam, and Daniel S Weld. Re-active learning: Active learning with rela-

beling. In AAAI, pp. 1845–1852, 2016.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
In European

Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context.
conference on computer vision, pp. 740–755. Springer, 2014b.

Qiang Liu, Jian Peng, and Alexander T Ihler. Variational inference for crowdsourcing. In Advances

in neural information processing systems, pp. 692–700, 2012.

Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with

noisy labels. In Advances in neural information processing systems, pp. 1196–1204, 2013.

Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R´e. Data pro-
gramming: Creating large training sets, quickly. In Advances in Neural Information Processing
Systems, pp. 3567–3575, 2016.

Victor S Sheng, Foster Provost, and Panagiotis G Ipeirotis. Get another label? improving data
quality and data mining using multiple, noisy labelers. In Proceedings of the 14th ACM SIGKDD
international conference on Knowledge discovery and data mining, pp. 614–622. ACM, 2008.

Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training

convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080, 2014.

Catherine Wah, Steve Branson, Pietro Perona, and Serge Belongie. Multiclass recognition and
part localization with humans in the loop. In Computer Vision (ICCV), 2011 IEEE International
Conference on, pp. 2524–2531. IEEE, 2011.

Peter Welinder and Pietro Perona. Online crowdsourcing: rating annotators and obtaining cost-
effective labels. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE
Computer Society Conference on, pp. 25–32. IEEE, 2010.

Peter Welinder, Steve Branson, Pietro Perona, and Serge J Belongie. The multidimensional wisdom

of crowds. In Advances in neural information processing systems, pp. 2424–2432, 2010.

Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier R Movellan, and Paul L Ruvolo. Whose
vote should count more: Optimal integration of labels from labelers of unknown expertise. In
Advances in neural information processing systems, pp. 2035–2043, 2009.

Yuchen Zhang, Xi Chen, Denny Zhou, and Michael I Jordan. Spectral methods meet em: A provably
optimal algorithm for crowdsourcing. In Advances in neural information processing systems, pp.
1260–1268, 2014.

Dengyong Zhou, Qiang Liu, John C Platt, Christopher Meek, and Nihar B Shah. Regularized

minimax conditional entropy for crowdsourcing. arXiv preprint arXiv:1503.07240, 2015.

Denny Zhou, Sumit Basu, Yi Mao, and John C Platt. Learning from the wisdom of crowds by
minimax entropy. In Advances in Neural Information Processing Systems, pp. 2195–2203, 2012.

11

Published as a conference paper at ICLR 2018

APPENDIX

A PROOF OF THEOREM 4.1

(cid:98)π,(cid:98)q to (cid:96)

Assuming the prior on Y , distribution q, to be uniform, we change the notation for the modiﬁed loss
(cid:98)π. Observe that for binary classiﬁcation, Z (r) ∈ {±1}r. Let ρ denote the posterior
function (cid:96)
distribution of Y , Equation (5), when q is uniform. Let τ denote the probability of observing an
instance of Z (r) as a function of the latent true confusion matrices π, conditioned on the ground
truth label Y = y.

ρ

(cid:98)π(y, Z (r), w(r)) := P

(cid:98)π[Y = y | Z (r); w(r)] ,

τπ(y, Z (r), w(r)) := Pπ[Z (r) | Y = y; w(r)] . (10)

Let W denote the uniform distribution over a pool of m workers, from which nr workers are selected
i.i.d. with replacement, and a batch of r workers are assigned to each example Xi. We deﬁne the
following quantities which play an important role in our analysis.
(cid:20) (cid:88)

(cid:21)
(cid:98)π(−y, Z (r), w(r))τπ(y, Z (r), w(r))
ρ

.

:= Ew∼W

β

(cid:98)π(y)

(11)

Z(r)∈{±1}r

(cid:20)

(cid:26) (cid:88)

β

(cid:98)π

:= Ew∼W

max
y∈{±1}

α(y)

:= Ew∼W

Z(r)∈{±1}r

(cid:104)
Pπ[Z = −y | Y = y; w]
(cid:104)

(cid:105)

.

α := Ew∼W

Pπ[Z = −y | Y = y; w]

max
y∈{±1}

(cid:105)

.

(cid:98)π(−y, Z (r), w(r))τπ(y, Z (r), w(r))
ρ

.

(12)

(cid:27)(cid:21)

For any given (cid:98)π with |(cid:98)π(a)
deﬁnition of β
(cid:98)π such that β
following bound on β(cid:15).

ks − π(a)

ks | ≤ (cid:15), for all a ∈ [m], k, s ∈ K, we can compute β(cid:15) from the
(cid:98)π ≤ β(cid:15). For the special case described in Section 4.4, we have the

β(cid:15) ≤

r
(cid:88)

u=0

(ρ + (cid:15))(r−u)(1 − ρ − (cid:15))u
(ρ + (cid:15))u(1 − ρ − (cid:15))(r−u) + (ρ + (cid:15))(r−u)(1 − ρ − (cid:15))u

(cid:19)

(cid:18)r
u

(1 − ρ)r−uρu (15)

= (ρ + (cid:15))r

(cid:19)(cid:32)(cid:18) ρ + (cid:15)
(cid:18)r
u

1 − ρ − (cid:15)

(cid:19)u

(cid:18) ρ + (cid:15)

+

1 − ρ − (cid:15)

(cid:19)r−u(cid:33)−1

.

r
(cid:88)

u=0

It can easily be checked that β(cid:15) ≤ (ρ + (cid:15))r (cid:80)(cid:100)r/2(cid:101)
We present two lemma that analyze the two alternative steps of our algorithm. The following lemma
gives a bound on the excess risk of function (cid:98)f learnt by minimizing the modiﬁed loss function (cid:96)
(cid:98)π.
Lemma A.1. Under the assumptions of Theorem 4.1, the excess risk of function (cid:98)f in Equation (6),
computed with posterior distribution P

(cid:1)(1 − ρ − (cid:15))u(ρ + (cid:15))−u.

(cid:98)π (5) using n training examples is bounded by

(cid:0)r
u

u=0

R(cid:96),D( (cid:98)f ) − min
f ∈F

R(cid:96),D(f ) ≤

C
1 − 2β

(cid:98)π

(cid:32)(cid:114)

(cid:114)

V
n

+

log(1/δ1)
n

(cid:33)

,

with probability at least 1 − δ1, where C is a universal constant. When P
majority vote, while initializing the iterative Algorithm 1, the above bound holds with β
by α.

(cid:98)π is computed using
(cid:98)π replaced

The following lemma gives an (cid:96)∞ norm bound on confusion matrices (cid:98)π estimated using model
prediction (cid:98)f (X) as the ground truth labels. In the analysis, we assume fresh samples are used for
estimating confusion matrices in step 3, Algorithm 1. Therefore the function (cid:98)f is independent of the
samples Xi’s on which (cid:98)π is estimated. Let K = |K|.

12

(13)

(14)

(16)

(17)

Published as a conference paper at ICLR 2018

Lemma A.2. Under the assumptions of Theorem 4.1, (cid:96)∞ error in estimated confusion matrices (cid:98)π
as computed in Equation (7), using n samples and a predictor function (cid:98)f with risk R(cid:96),D ≤ δ, is
bounded by

(cid:12)
(cid:12)(cid:98)π(a)
(cid:12)

ks − π(a)

ks

(cid:12)
(cid:12)
(cid:12) ≤

2δ + 16(cid:112)m log(4mK 2δ1)/(nr)
1/K − δ − 8(cid:112)m log(4mK 2/δ1)/(nr)

,

∀ a ∈ [m], k, s ∈ K , (18)

with probability at least 1 − δ1.

First we apply Lemma A.1 with P
(cid:98)π computed using majority vote. We get a bound on the risk
of function (cid:98)f computed in the ﬁrst round. With this (cid:98)f , we apply Lemma A.2. When n is
sufﬁciently large such that Equation (8) holds, the denominator in Equation (18), 1/K − δ −
8(cid:112)m log(4mK 2/δ1)/(nr) ≥ 1/8. Therefore, in the ﬁrst round, the error in confusion matrix
estimation is bounded by (cid:15), which is deﬁned in the Theorem.
For the second round: we apply Lemma A.1 with P
(cid:98)π computed as the posterior distribution (5).
Where (cid:96)∞ error in (cid:98)π is bounded by (cid:15). This gives the desired bound in (9). With this (cid:98)f , we apply
Lemma A.2 and obtain (cid:96)∞ error in (cid:98)π bounded by (cid:15)1, which is deﬁned in the Theorem.
For the given probability of error δ in the Theorem, we chose δ1 in both the lemma to be δ/4 such
that with union bound we get the desired probability of δ.

A.1 PROOF OF LEMMA A.1

Let f ∗ := arg minf ∈F R(cid:96),D(f ). Let’s denote the distribution of (X, Z (r), w(r)) by DW,π,r. For
ease of notation, we denote DW,π,r by Dπ. Similar to R(cid:96),D, risk of decision function f with respect
to the modiﬁed loss function (cid:96)

1. (cid:96)

(cid:98)π-risk under Dπ: R(cid:96)

2. Empirical (cid:96)

(cid:98)π is characterized by the following quantities:
(cid:98)π,Dπ (f ) := E
(cid:98)π-risk on samples: (cid:98)R(cid:96)

(X,Z(r),w(r))∼Dπ
(cid:80)n
(cid:98)π,Dπ (f ) := 1

(cid:98)π(f (X), Z (r), w(r))(cid:3).
, w(r)
(cid:98)π(f (Xi), Z (r)
).
i

i=1 (cid:96)

(cid:2)(cid:96)

n

i

With the above deﬁnitions, we have the following,

R(cid:96),D( (cid:98)f ) − R(cid:96),D(f ∗)

= R(cid:96)

(cid:98)π,Dπ ( (cid:98)f ) − R(cid:96)

≤ R(cid:96)

(cid:98)π,Dπ ( (cid:98)f ) − R(cid:96)

= (cid:98)R(cid:96)

(cid:98)π,Dπ ( (cid:98)f ) − (cid:98)R(cid:96)

(cid:16)

(cid:98)π,Dπ (f ∗) +
(cid:98)π,Dπ (f ∗) + 2β
(cid:16)
(cid:98)π,Dπ (f ∗) +
(cid:17)
R(cid:96),D( (cid:98)f ) − R(cid:96),D(f ∗)

(cid:16)

+2β

(cid:98)π

≤ 2 max
f ∈F
(cid:32)(cid:114)

≤ C

(cid:12)
(cid:12)
(cid:12) (cid:98)R(cid:96)

V
n

+

(cid:98)π,Dπ (f ) − R(cid:96)

(cid:114)

log(1/δ)
n

(cid:98)π,Dπ (f )
(cid:33)

+ 2β

(cid:98)π

(cid:17)

R(cid:96),D( (cid:98)f ) − R(cid:96)

(cid:98)π,Dπ ( (cid:98)f )

− (R(cid:96),D(f ∗) − R(cid:96)
(cid:17)
(cid:16)
R(cid:96),D( (cid:98)f ) − R(cid:96),D(f ∗)
(cid:17)
(cid:98)π,Dπ ( (cid:98)f )

(cid:98)π,Dπ ( (cid:98)f ) − (cid:98)R(cid:96)

(cid:98)R(cid:96)

+

(cid:16)

R(cid:96)

(cid:98)π

(cid:98)π,Dπ (f ∗))

(cid:98)π,Dπ (f ∗) − R(cid:96)

(cid:98)π,Dπ (f ∗)

(cid:16)

(cid:12)
(cid:12)
(cid:12) + 2β

(cid:98)π

R(cid:96),D( (cid:98)f ) − R(cid:96),D(f ∗)

(cid:17)

(cid:16)

(cid:17)
R(cid:96),D( (cid:98)f ) − R(cid:96),D(f ∗)

,

where (19) follows from Equation (24). (20) follows from the fact that (cid:98)f is the minimizer of (cid:98)R(cid:96)
as computed in (6).
hypothesis class F, and C is a universal constant.

(cid:98)π,Dπ
(21) follows from the basic excess-risk bound. V is the VC dimension of

Following shows the inequality used in Equation (19). For binary classiﬁcation, we denote the two
classes by Y, −Y .

= R(cid:96),D( (cid:98)f ) − R(cid:96)
= E(X,Y )∼D

(cid:104)

(cid:98)π,Dπ ( (cid:98)f ) − (R(cid:96),D(f ∗) − R(cid:96)
(cid:98)π(Y )

(cid:96)( (cid:98)f (X), Y ) − (cid:96)(f ∗(X), Y )

(cid:98)π,Dπ (f ∗))
(cid:17)
−

(cid:16)(cid:16)

(cid:16)

= 2E(X,Y )∼D

(cid:96)( (cid:98)f (X), Y ) − (cid:96)(f ∗(X), Y )

(cid:17)(cid:105)

β
(cid:104)

(cid:16)

β

(cid:98)π(Y )
(cid:17)
R(cid:96),D( (cid:98)f ) − R(cid:96),D(f ∗)

(cid:16)

,

≤ 2β

(cid:98)π

13

(cid:96)( (cid:98)f (X), −Y ) − (cid:96)(f ∗(X), −Y )

(19)
(cid:17)

(20)

(21)

(cid:17)(cid:17)(cid:105)
(22)

(23)

(24)

Published as a conference paper at ICLR 2018

where (22) follows from Equation (26).
(cid:96)(f (X), Y ) + (cid:96)(f (X), −Y ) = 1. (24) follows from the deﬁnition of β
When (cid:96)
by α. α is deﬁned in (14).

(23) follows from the fact that for 0-1 loss function
(cid:98)π deﬁned in Equation (12).
(cid:98)π replaced

(cid:98)π is computed using weighted majority vote of the workers then (24) holds with β

Following shows the equality used in Equation (22). Using the notations ρ
(cid:98)π and τπ, in the following,
for any function f ∈ F, we compute the excess risk due to the unbiasedness of the modiﬁed loss
function (cid:96)

(cid:98)π.

−E

(X,Y,w(r))∼Dπ

(1 − ρ

(cid:98)π(−Y, Z (r), w(r)))(cid:96)(f (X), Y )

(25)

(X,Z(r),w(r))∼Dπ [(cid:96)

(cid:98)π(f (X), Z (r), w(r))]

R(cid:96),D(f ) − R(cid:96)

(cid:98)π,Dπ (f )

= E(X,Y )∼D [(cid:96)(f (X), Y )] − E
= E(X,Y )∼D [(cid:96)(f (X), Y )]

(cid:34)

(cid:88)

(cid:16)

Z(r)∈{±1}r

+ρ

(cid:17)
(cid:98)π(−Y, Z (r), w(r))(cid:96)(f (X), −Y )

τπ(Y, Z (r), w(r))

(cid:35)

= E(X,Y )∼D [β

(cid:98)π(Y ) ((cid:96)(f (X), Y ) − (cid:96)(f (X), −Y ))] ,

where β
Observe that when (cid:96)
holds with β

(cid:98)π(Y ) is deﬁned in (11). Where (25) follows from the deﬁnition of (cid:96)

(cid:98)π given in Equation (4).
(cid:98)π is computed using weighted majority vote of the workers then Equation (26)

(cid:98)π(Y ) replaced by α(y). α(y) is deﬁned in (13).

A.2 PROOF OF LEMMA A.2

Recall that we have

(cid:98)π(a)
ks =

(cid:80)n

(cid:80)r

i=1

j=1

(cid:80)n

I[wij = a]I[ti = k]I[Zij = s]
(cid:80)r
I[wij = a]I[ti = k]

i=1

j=1

Let ti denote (cid:98)f (Xi). By the deﬁnition of risk, for any k ∈ K, we have
(cid:104)(cid:12)
(cid:12)I[Yi = k] − I[ti = k](cid:12)

(cid:105)
(cid:12) = 1

= δ .

P

Let |K| = K. Deﬁne, for ﬁxed a ∈ [m], and k, s ∈ K,
n
(cid:88)

r
(cid:88)

A :=

I[wij = a]I[ti = k]I[Zij = s] ,

¯A :=

nrπks
mK

B :=

I[wij = a]I[ti = k] ,

¯B :=

nr
mK

C :=

I[wij = a]

(cid:12)
(cid:12)
(cid:12)

(cid:12)
I[Yi = k] − I[ti = k]
(cid:12)
(cid:12) ,

¯C :=

nrδ
m

,

D :=

I[wij = a]I[Yi = k]I[Zij = s] ,

E :=

I[wij = a]I[Yi = k] .

i=1
n
(cid:88)

j=1
r
(cid:88)

i=1
n
(cid:88)

j=1
r
(cid:88)

i=1
n
(cid:88)

j=1
r
(cid:88)

i=1
n
(cid:88)

j=1
r
(cid:88)

i=1

j=1

Note that A, B, C, D, E depend upon a ∈ [m], k, s ∈ K. However, for ease of notations, we have
not included the subscripts. We have,

(cid:12)
(cid:12)(cid:98)π(a)
(cid:12)

ks − π(a)

ks

(cid:12)
(cid:12)
(cid:12) =

A − Bπks
B

|(A − ¯A) − (B − ¯B)πks|
| ¯B + (B − ¯B)|
|A − ¯A| + |(B − ¯B)|πks
| ¯B| − |B − ¯B|

=

≤

14

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

Published as a conference paper at ICLR 2018

Now, we have,

We have,

|A − ¯A| ≤ |A − D| + |D − ¯A|

≤ C + |D − ¯A| .

|B − ¯B| ≤ |B − E| + |E − ¯B|

≤ C + |E − ¯B|

C ≤

(cid:114)

nrδ
m

+

3nrδ log(2mK/δ1)
m

,

Observe that C is a sum of nr i.i.d. Bernoulli random variables with mean δ/m. Using Chernoff
bound we get that

for all a ∈ [m], and k ∈ K with probability at least 1 − δ1. Similarly, D is a sum of nr i.i.d.
Bernoulli random variables with mean πks/(mk). Again, using Chernoff bound we get that
(cid:114)

(cid:12)D − ¯A(cid:12)
(cid:12)

(cid:12) ≤

3nrπks log(2mK 2/δ1)
mK

,

for all a ∈ [m], k, s ∈ K with probability at least 1 − δ1. From the bound on |D − ¯A|, it follows that

|E − ¯B| ≤

(cid:114)

3nr log(2mK 2/δ1)
m

Collecting Equations (33)-(38), we have for all a ∈ [m], k, s ∈ K

(cid:12)
(cid:12)(cid:98)π(a)
(cid:12)

ks − π(a)

ks

(cid:12)
(cid:12)
(cid:12) ≤

2δ + 16(cid:112)m log(2mK 2δ1/(nr)
1/K − δ − 8(cid:112)m log(2mK 2/δ1)/(nr)

,

with probability at least 1 − 2δ1.

(34)

(35)

(36)

(37)

(38)

(39)

15

8
1
0
2
 
y
a
M
 
0
2
 
 
]

G
L
.
s
c
[
 
 
2
v
7
7
5
4
0
.
2
1
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

LEARNING FROM NOISY SINGLY-LABELED DATA

Ashish Khetan
University of Illinois at Urbana-Champaign
Urbana, IL 61801
khetan2@illinois.edu

Zachary C. Lipton
Amazon Web Services
Seattle, WA 98101
liptoz@amazon.com

Animashree Anandkumar
Amazon Web Services
Seattle, WA 98101
anima@amazon.com

ABSTRACT

Supervised learning depends on annotated examples, which are taken to be the
ground truth. But these labels often come from noisy crowdsourcing platforms,
like Amazon Mechanical Turk. Practitioners typically collect multiple labels per
example and aggregate the results to mitigate noise (the classic crowdsourcing
problem). Given a ﬁxed annotation budget and unlimited unlabeled data, redun-
dant annotation comes at the expense of fewer labeled examples. This raises two
fundamental questions: (1) How can we best learn from noisy workers? (2) How
should we allocate our labeling budget to maximize the performance of a classi-
ﬁer? We propose a new algorithm for jointly modeling labels and worker quality
from noisy crowd-sourced data. The alternating minimization proceeds in rounds,
estimating worker quality from disagreement with the current model and then
updating the model by optimizing a loss function that accounts for the current
estimate of worker quality. Unlike previous approaches, even with only one an-
notation per example, our algorithm can estimate worker quality. We establish a
generalization error bound for models learned with our algorithm and establish
theoretically that it’s better to label many examples once (vs less multiply) when
worker quality exceeds a threshold. Experiments conducted on both ImageNet
(with simulated noisy workers) and MS-COCO (using the real crowdsourced la-
bels) conﬁrm our algorithm’s beneﬁts.

1

INTRODUCTION

Recent advances in supervised learning owe, in part, to the availability of large annotated datasets.
For instance, the performance of modern image classiﬁers saturates only with millions of labeled
examples. This poses an economic problem: Assembling such datasets typically requires the labor
of human annotators. If we conﬁned the labor pool to experts, this work might be prohibitively ex-
pensive. Therefore, most practitioners turn to crowdsourcing platforms such as Amazon Mechanical
Turk (AMT), which connect employers with low-skilled workers who perform simple tasks, such as
classifying images, at low cost.

Compared to experts, crowd-workers provide noisier annotations, possibly owing to high variation
in worker skill; and a per-answer compensation structure that encourages rapid answers, even at the
expense of accuracy. To address variation in worker skill, practitioners typically collect multiple in-
dependent labels for each training example from different workers. In practice, these labels are often
aggregated by applying a simple majority vote. Academics have proposed many efﬁcient algorithms
for estimating the ground truth from noisy annotations. Research addressing the crowd-sourcing
problem goes back to the early 1970s. Dawid & Skene (1979) proposed a probabilistic model to
jointly estimate worker skills and ground truth labels and used expectation maximization (EM) to
estimate the parameters. Whitehill et al. (2009); Welinder et al. (2010); Zhou et al. (2015) proposed
generalizations of the Dawid-Skene model, e.g. by estimating the difﬁculty of each example.

1

Published as a conference paper at ICLR 2018

Although the downstream goal of many crowdsourcing projects is to train supervised learning mod-
els, research in the two disciplines tends to proceed in isolation. Crowdsourcing research seldom
accounts for the downstream utility of the produced annotations as training data in machine learning
(ML) algorithms. And ML research seldom exploits the noisy labels collected from multiple human
workers. A few recent papers use the original noisy labels and the corresponding worker identities
together with the predictions of a supervised learning model trained on those same labels, to estimate
the ground truth (Branson et al., 2017; Guan et al., 2017; Welinder et al., 2010). However, these pa-
pers do not realize the full potential of combining modeling and crowd-sourcing. In particular, they
are unable to estimate worker qualities when there is only one label per training example.

This paper presents a new supervised learning algorithm that alternately models the labels and
worker quality. The EM algorithm bootstraps itself in the following way: Given a trained model,
the algorithm estimates worker qualities using the disagreement between workers and the current
predictions of the learning algorithm. Given estimated worker qualities, our algorithm optimizes a
suitably modiﬁed loss function. We show that accurate estimates of worker quality can be obtained
even when only collecting one label per example provided that each worker labels sufﬁciently many
examples. An accurate estimate of the worker qualities leads to learning a better model. This
addresses a shortcoming of the prior work and overcomes a signiﬁcant hurdle to achieving practical
crowdsourcing without redundancy.

We give theoretical guarantees on the performance of our algorithm. We analyze the two alternating
steps: (a) estimating worker qualities from disagreement with the model, (b) learning a model by
optimizing the modiﬁed loss function. We obtain a bound on the accuracy of the estimated worker
qualities and the generalization error of the model. Through the generalization error bound, we
establish that it is better to label many examples once than to label less examples multiply when
worker quality is above a threshold. Empirically, we verify our approach on several multi-class
classiﬁcation datasets: ImageNet and CIFAR10 (with simulated noisy workers), and MS-COCO
(using the real noisy annotator labels). Our experiments validate that when the cost of obtaining
unlabeled examples is negligible and the total annotation budget is ﬁxed, it is best to collect a single
label per training example for as many examples as possible. We emphasize that although this paper
applies our approach to classiﬁcation problems, the main ideas of the algorithm can be extended to
other tasks in supervised learning.

2 RELATED WORK

The traditional crowdsourcing problem addresses the challenge of aggregating multiple noisy labels.
A naive approach is to aggregate the labels based on majority voting. More sophisticated agreement-
based algorithms jointly model worker skills and ground truth labels, estimating both using EM or
similar techniques (Dawid & Skene, 1979; Jin & Ghahramani, 2003; Whitehill et al., 2009; Welinder
et al., 2010; Zhou et al., 2012; Liu et al., 2012; Dalvi et al., 2013; Liu et al., 2012). Zhang et al.
(2014) shows that the EM algorithm with spectral initialization achieves minimax optimal perfor-
mance under the Dawid-Skene model. Karger et al. (2014) introduces a message-passing algorithm
for estimating binary labels under the Dawid-Skene model, showing that it performs strictly better
than majority voting when the number of labels per example exceeds some threshold. Similar ob-
servations are made by (Bragg et al., 2016). A primary criticism of EM-based approaches is that in
practice, it’s rare to collect more than 3 to 5 labels per example; and with so little redundancy, the
small gains achieved by EM over majority voting are not compelling to practitioners. In contrast,
our algorithm performs well in the low-redundancy setting. Even with just one label per example,
we can accurately estimate worker quality.

Several prior crowdsourcing papers incorporate the predictions of a supervised learning model, to-
gether with the noisy labels, to estimate the ground truth labels. Welinder et al. (2010) consider
binary classiﬁcation and frames the problem as a generative Bayesian model on the features of the
examples and the labels. Branson et al. (2017) considers a generalization of the Dawid-Skene model
and estimates its parameters using supervised learning in the loop. In particular, they consider a joint
probability over observed image features, ground truth labels, and the worker labels and computes
the maximum likelihood estimate of the true labels using alternating minimization. We also consider
a joint probability model but it is signiﬁcantly different from theirs as we assume that the optimal
labeling function gives the ground truth labels. We maximize the joint likelihood using a variation

2

Published as a conference paper at ICLR 2018

of expectation maximization to learn the optimal labeling function and the true labels. Further, they
train the supervised learning model using the intermediate predictions of the labels whereas we train
the model by minimizing a weighted loss function where the weights are the intermediate posterior
probability distribution of the labels. Moreover, with only one label per example, their algorithm
fails and estimates all the workers to be equally good. They only consider binary classiﬁcation,
whereas we verify our algorithm on multi-class (ten classes) classiﬁcation problem.

A rich body of work addresses human-in-loop annotation for computer vision tasks. However, these
works assume that humans are experts, i.e., that they give noiseless annotations (Branson et al.,
2010; Deng et al., 2013; Wah et al., 2011). We assume workers are unreliable and have varying
skills. A recent work by Ratner et al. (2016) also proposes to use predictions of a supervised learning
model to estimate the ground truth. However, their algorithm is signiﬁcantly different than ours as
it does not use iterative estimation technique, and their approach of incorporating worker quality
parameters in the supervised learning model is different. Their theoretical results are limited to the
linear classiﬁers.

Another line of work employs active learning, iteratively ﬁltering out examples for which aggregated
labels have high conﬁdence and collect additional labels for the remaining examples (Whitehill et al.,
2009; Welinder & Perona, 2010; Khetan & Oh, 2016). The underlying modeling assumption in these
papers is that the questions have varying levels of difﬁculty. At each iteration, these approaches
employ an EM-based algorithm to estimate the ground truth label of the remaining unclassiﬁed
examples. For simplicity, our paper does not address example difﬁculties, but we could easily extend
our model and algorithm to accommodate this complexity.

Several papers analyze whether repeated labeling is useful. Sheng et al. (2008) analyzed the effect of
repeated labeling and showed that it depends upon the relative cost of getting an unlabeled example
and the cost of labeling. Ipeirotis et al. (2014) shows that if worker quality is below a threshold then
repeated labeling is useful, otherwise not. Lin et al. (2014a; 2016) argues that it also depends upon
expressiveness of the classiﬁer in addition to the factors considered by others. However, these works
do not exploit predictions of the supervised learning algorithm to estimate the ground truth labels,
and hence their ﬁndings do not extend to our methodology.

Another body of work that is relevant to our problem is learning with noisy labels where usual as-
sumption is that all the labels are generated through the same noisy rate given their ground truth
label. Recently Natarajan et al. (2013) proposed a generic unbiased loss function for binary clas-
siﬁcation with noisy labels. They employed a modiﬁed loss function that can be expressed as a
weighted sum of the original loss function, and gave theoretical bounds on the performance. How-
ever, their weights become unstably large when the noise rate is large, and hence the weights need
to be tuned. Sukhbaatar et al. (2014); Jindal et al. (2016) learns noise rate as parameters of the
model. A recent work by Guan et al. (2017) trains an individual softmax layer for each expert and
then predicts their weighted sum where weights are also learned by the model. It is not scalable to
crowdsourcing scenario where there are thousands of workers. There are works that aim to create
noise-robust models (Joulin et al., 2016; Krause et al., 2016), but they are not relevant to our work.

3 PROBLEM FORMULATION

Let D be the underlying true distribution generating pairs (X, Y ) ∈ X × K from which n i.i.d.
samples (X1, Y1), (X2, Y2), · · · , (Xn, Yn) are drawn, where K denotes the set of possible labels
K := {1, 2, · · · , K}, and X ⊆ Rd denotes the set of euclidean features. We denote the marginal
distribution of Y by {q1, q2, · · · , qK}, which is unknown to us. Consider a pool of m workers
indexed by 1, 2, · · · , m. We use [m] to denote the set {1, 2, · · · , m}. For each i-th sample Xi, r
workers {wij}j∈[r] ∈ [m]r are selected randomly, independent of the sample Xi. Each selected
worker provides a noisy label Zij for the sample Xi, where the distribution of Zij depends on the
selected worker and the true label Yi. We call r the redundancy and, for simplicity, assume it to
be the same for each sample. However, our algorithm can also be applied when redundancy varies
across the samples. We use Z (r)
to denote {Zij}j∈[r], the set of r labels collected on the i-th
example, and w(r)

to denote {wij}j∈[r].

i

i

Following Dawid & Skene (1979), we assume the probability that the a-th worker labels an item in
class k ∈ K as class s ∈ K is independent of any particular chosen item, that is, it is a constant over

3

Published as a conference paper at ICLR 2018

i ∈ [n]. Let us denote this constant by πks; by deﬁnition, (cid:80)
π(a) ∈ [0, 1]K×K the confusion matrix of the a-th worker. In particular, the distribution of Z is:

s∈K πks = 1 for all k ∈ K, and we call

P [Zij = s | Yi = k, wij = a] = π(a)
ks .

(1)

The diagonal entries of the confusion matrix correspond to the probabilities of correctly labeling an
example. The off-diagonal entries represent the probability of mislabeling. We use π to denote the
collection of confusion matrices {π(a)}a∈[m].
We assume nr workers w1,1, w1,2, · · · , wn,r are selected uniformly at
random from a
pool of m workers with replacement and a batch of r workers are assigned to each of
the examples X1, X2, · · · , Xn.
The corrupted labels along with the worker information
(X1, Z (r)
1 ), · · · , (Xn, Z (r)
Let F be the hypothesis class, and f ∈ F, f : X → RK, denote a vector valued predictor function.
Let (cid:96)(f (X), Y ) denote a loss function. For a predictor f , its (cid:96)-risk under D is deﬁned as

n ) are what the learning algorithm sees.

1 , w(r)

n , w(r)

R(cid:96),D(f )

:= E(X,Y )∼D [(cid:96)(f (X), Y )] .

(2)

1 , w(r)

Given the observed samples (X1, Z (r)
n ), we want to learn a good pre-
dictor function (cid:98)f ∈ F such that its risk under the true distribution D, R(cid:96),D( (cid:98)f ) is minimal. Having
access to only noisy labels Z (r) by workers w(r), we compute (cid:98)f as the one which minimizes a
(cid:98)π,(cid:98)q(f (X), Z (r), w(r)). Where (cid:98)π denote an estimate of confusion
suitably modiﬁed loss function (cid:96)
matrix π, and (cid:98)q an estimate of q, the prior distribution on Y . We deﬁne (cid:96)
(cid:98)π,(cid:98)q in the following section.

1 ), · · · , (Xn, Z (r)

n , w(r)

4 ALGORITHM

Assume that there exists a function f ∗ ∈ F such that f ∗(Xi) = Yi for all i ∈ [n]. Under the
Dawid-Skene model (described in previous section), the joint likelihood of true labeling function
f ∗(Xi) and observed labels {Zij}i∈[n],j∈[r] as a function of confusion matrices of workers π can be
written as

L (cid:0)π; f ∗, {Xi}i∈[n], {Zij}i∈[n],j∈[r]
(cid:32)





(cid:1) :=

n
(cid:89)

(cid:88)



i=1

k∈K

r
(cid:89)

(cid:88)

j=1

s∈K

qkI[f ∗(Xi) = k]



I[Zij = s]π(wij )

ks

(3)

(cid:33)




 .

qk’s are the marginal distribution of the true labels Yi’s. We estimate the worker confusion matrices
π and the true labeling function f ∗ by maximizing the likelihood function L(π; f ∗(X), Z). Ob-
serve that the likelihood function L(π; f ∗(X), Z) is different than the standard likelihood function
of Dawid-Skene model in that we replace each true hidden labels Yi by f ∗(Xi). Like the EM al-
gorithm introduced in (Dawid & Skene, 1979), we propose ‘Model Bootstrapped EM’ (MBEM) to
estimate confusion matrices π and the true labeling function f ∗. EM converges to the true confusion
matrices and the true labels, given an appropriate spectral initialization of worker confusion matrices
(Zhang et al., 2014). We show in Section 4.4 that MBEM converges under mild conditions when
the worker quality is above a threshold and the number of training examples is sufﬁciently large. In
the following two subsections, we motivate and explain our iterative algorithm to estimate the true
labeling function f ∗, given a good estimate of worker confusion matrices π and vice-versa.

4.1 LEARNING WITH NOISY LABELS

To begin, we ask, what is the optimal approach to learn the predictor function (cid:98)f when for each
worker we have (cid:98)π, a good estimation of the true confusion matrix π, and (cid:98)q, an estimate of the prior?
A recent paper, Natarajan et al. (2013) proposes minimizing an unbiased loss function speciﬁcally,
a weighted sum of the original loss over each possible ground truth label. They provide weights for
binary classiﬁcation where each example is labeled by only one worker. Consider a worker with
confusion matrix π, where πy > 1/2 and π−y > 1/2 represent her probability of correctly labeling
the examples belonging to class y and −y respectively. Then their weights are π−y/(πy + π−y − 1)
for class y and −(1 − πy)/(πy + π−y − 1) for class −y. It is evident that their weights become

4

Published as a conference paper at ICLR 2018

unstably large when the probabilities of correct classiﬁcation πy and π−y are close to 1/2, limiting
the method’s usefulness in practice. As explained below, for the same scenario, our weights would
be πy/(1 + πy − π−y) for class y and (1 − π−y)/(1 + πy − π−y) for class −y. Inspired by their idea,
we propose weighing the loss function according to the posterior distribution of the true label given
the Z (r) observed labels and an estimate of the confusion matrices of the worker who provided those
labels. In particular, we deﬁne (cid:96)

(cid:98)π,(cid:98)q to be

(cid:98)π,(cid:98)q(f (X), Z (r), w(r))
(cid:96)

:=

P
(cid:98)π,(cid:98)q[Y = k | Z (r); w(r)] (cid:96)(f (X), Y = k) .

(4)

(cid:88)

k∈K

If the observed label is uniformly random, then all weights are equal and the loss is identical for all
predictor functions f . Absent noise, we recover the original loss function. Under the Dawid-Skene
model, given the observed noisy labels Z (r), an estimate of confusion matrices (cid:98)π, and an estimate
of prior (cid:98)q, the posterior distribution of the true labels can be computed as follows:
I[Zij = s](cid:98)π(wij )

(cid:16) (cid:80)

(cid:81)r

j=1

(cid:17)

ks

(cid:98)qk
(cid:16)

s∈K
(cid:16) (cid:80)

(cid:80)

k(cid:48)∈K

(cid:98)qk(cid:48) (cid:81)r

j=1

I[Zij = s](cid:98)π(wij )

k(cid:48)s

s∈K

(cid:17)(cid:17) ,

(5)

(cid:98)π,(cid:98)q[Yi = k | Z (r)
P

i

; w(r)
i

] =

where I[.] is the indicator function which takes value one if the identity inside it is true, otherwise
zero. We give guarantees on the performance of the proposed loss function in Theorem 4.1. In
practice, it is robust to noise level and signiﬁcantly outperforms the unbiased loss function. Given
(cid:98)π,(cid:98)q, we learn the predictor function (cid:98)f by minimizing the empirical risk
(cid:96)

(cid:98)f ← arg min
f ∈F

(cid:98)π,(cid:98)q(f (Xi), Z (r)
(cid:96)

i

, w(r)
i

) .

(6)

1
n

n
(cid:88)

i=1

4.2 ESTIMATING ANNOTATOR NOISE

The next question is: how do we get a good estimate (cid:98)π of the true confusion matrix π for each worker.
If redundancy r is sufﬁciently large, we can employ the EM algorithm. However, in practical appli-
cations, redundancy is typically three or ﬁve. With so little redundancy, the standard applications of
EM are of limited use. In this paper we look to transcend this problem, posing the question: Can we
estimate confusion matrices of workers even when there is only one label per example? While this
isn’t possible in the standard approach, we can overcome this obstacle by incorporating a supervised
learning model into the process of assessing worker quality.

Under the Dawid-Skene model, the EM algorithm estimates the ground truth labels and the con-
fusion matrices in the following way: It alternately ﬁxes the ground truth labels and the confusion
matrices by their estimates and and updates its estimate of the other by maximizing the likelihood
of the observed labels. The alternating maximization begins by initializing the ground truth labels
with a majority vote. With only 1 label per example, EM estimates that all the workers are perfect.

We propose using model predictions as estimates of the ground truth labels. Our model is initially
trained on the majority vote of the labels. In particular, if the model prediction is {ti}i∈[n], where
ti ∈ K, then the maximum likelihood estimate of confusion matrices and the prior distribution is
given below. For the a-th worker, (cid:98)π(a)
(cid:80)n

ks for k, s ∈ K, and (cid:98)qk for k ∈ K, we have,

(cid:80)r

(cid:98)π(a)
ks =

i=1

j=1

(cid:80)n

I[wij = a]I[ti = k]I[Zij = s]
(cid:80)r
I[wij = a]I[ti = k]

,

i=1

j=1

(cid:98)qk = (1/n)

I[ti = k]

(7)

n
(cid:88)

i=1

The estimate is effective when the hypothesis class F is expressive enough and the learner is robust
to noise. Thus the model should, in general, have small training error on correctly labeled examples
and large training error on wrongly labeled examples. Consider the case when there is only one
label per example. The model will be trained on the raw noisy labels given by the workers. For
simplicity, assume that each worker is either a hammer (always correct) or a spammer (chooses
labels uniformly random). By comparing model predictions with the training labels, we can identify
which workers are hammers and which are spammers, as long as each worker labels sufﬁciently
many examples. We expect a hammer to agree with the model more often than a spammer.

5

Published as a conference paper at ICLR 2018

4.3

ITERATIVE ALGORITHM

Building upon the previous two ideas, we present ‘Model Bootstrapped EM’, an iterative algorithm
for efﬁcient learning from noisy labels with small redundancy. MBEM takes data, noisy labels, and
the corresponding worker IDs, and returns the best predictor function (cid:98)f in the hypothesis class F.
In the ﬁrst round, we compute the weights of the modiﬁed loss function (cid:96)
(cid:98)π,(cid:98)q by using the weighted
majority vote. Then we obtain an estimate of the worker confusion matrices (cid:98)π using the maximum
likelihood estimator by taking the model predictions as the ground truth labels. In the second round,
weights of the loss function are computed as the posterior probability distribution of the ground
truth labels conditioned on the noisy labels and the estimate of the confusion matrices obtained
in the previous round.
In our experiments, only two rounds are required to achieve substantial
improvements over baselines.

Algorithm 1 Model Bootstrapped EM (MBEM)
Input: {(Xi, Z (r)
i
Output: (cid:98)f : predictor function
Initialize posterior distribution using weighted majority vote

)}i∈[n], T : number of iterations

, w(r)
i

(cid:98)π,(cid:98)q[Yi = k | Z (r)
P

i

; w(r)
i

] ← (1/r) (cid:80)r

j=1

I[Zij = k] , for k ∈ K, i ∈ [n]

Repeat T times:

P

1
n

i=1

k∈K

(cid:80)n

(cid:98)π,(cid:98)q[Yi = k | Z (r)

learn predictor function (cid:98)f
(cid:80)
(cid:98)f ← arg minf ∈F
predict on training examples
ti ← arg maxk∈K (cid:98)f (Xi)k, for i ∈ [n]
estimate confusion matrices (cid:98)π and prior class distribution (cid:98)q given {ti}i∈[n]
(cid:98)π(a) ← Equation (7), for a ∈ [m]; (cid:98)q ← Equation (7)
estimate label posterior distribution given (cid:98)π, (cid:98)q
(cid:98)π,(cid:98)q[Yi = k | Z (r)
P

], ← Equation (5), for k ∈ K, i ∈ [n]

] (cid:96)(f (Xi), Yi = k)

; w(r)
i

; w(r)
i

i

i

Return (cid:98)f

4.4 PERFORMANCE GUARANTEES

The following result gives guarantee on the excess risk for the learned predictor function (cid:98)f in terms
of the VC dimension of the hypothesis class F. Recall that risk of a function f w.r.t. loss function (cid:96)
is deﬁned to be R(cid:96),D(f ) := E(X,Y )∼D [(cid:96)(f (X), Y )], Equation (2). We assume that the classiﬁcation
problem is binary, and the distribution q, prior on ground truth labels Y , is uniform and is known to
us. We give guarantees on the excess risk of the predictor function (cid:98)f , and accuracy of (cid:98)π estimated
in the second round. For the purpose of analysis, we assume that fresh samples are used in each
round for computing function (cid:98)f and estimating (cid:98)π. In other words, we assume that (cid:98)f and (cid:98)π are
each computed using n/4 fresh samples in the ﬁrst two rounds. We deﬁne α and β(cid:15) to capture the
average worker quality. Here, we give their concise bound for a special case when all the workers
are identical, and their confusion matrix is represented by a single parameter, 0 ≤ ρ < 1/2. Where
πkk = 1 − ρ, and πks = ρ for k (cid:54)= s. Each worker makes a mistake with probability ρ. β(cid:15) ≤
(cid:1)(τ u + τ r−u)−1, where τ := (ρ + (cid:15))/(1 − ρ − (cid:15)). α for this special case is ρ. A
(ρ + (cid:15))r (cid:80)r
general deﬁnition of α and β(cid:15) for any confusion matrices π is provided in the Appendix.
Theorem 4.1. Deﬁne N := nr to be the number of total annotations collected on n training exam-
ples with redundancy r. Suppose minf ∈F R(cid:96),D(f ) ≤ 1/4. For any hypothesis class F with a ﬁnite
VC dimension V , and any δ < 1, there exists a universal constant C such that if N is large enough
and satisﬁes

(cid:0)r
u

u=0

N ≥ max

(cid:26)

Cr(cid:0)(cid:0)√

V + (cid:112)log(1/δ)(cid:1)/(1 − 2α)(cid:1)2

, 212m log(26m/δ)

,

(cid:27)

then for binary classiﬁcation with 0-1 loss function (cid:96), (cid:98)f and (cid:98)π returned by Algorithm 1 after T = 2
iterations satisﬁes

R(cid:96),D( (cid:98)f ) − min
f ∈F

R(cid:96),D(f ) ≤

√

C
r
1 − 2β(cid:15)

(cid:32)(cid:114)

(cid:114)

V
N

+

log(1/δ)
N

(cid:33)

,

(8)

(9)

6

Published as a conference paper at ICLR 2018

and (cid:107)(cid:98)π(a) − π(a)(cid:107)∞ ≤ (cid:15)1 for all a ∈ [m], with probability at least 1 − δ. Where (cid:15) := 24γ +
28(cid:112)m log(26mδ)/N , and γ := minf ∈F R(cid:96),D(f ) + C(
V + (cid:112)log(1/δ))/((1 − 2α)(cid:112)N/r). (cid:15)1
is deﬁned to be (cid:15) with α in it replaced by β(cid:15).

√

The price we pay in generalization error bound on (cid:98)f is (1 − 2β(cid:15)). Note that, when n is large, (cid:15) goes
to zero, and β(cid:15) ≤ 2ρ(1 − ρ), for r = 1.

If minf ∈F R(cid:96),D(f ) is sufﬁciently small, VC dimension is ﬁnite, and ρ is bounded away from 1/2
then for n = O(m log(m)/r), we get (cid:15)1 to be sufﬁciently small. Therefore, for any redundancy r,
error in confusion matrix estimation is small when the number of training examples is sufﬁciently
large. Hence, for N large enough, using Equation (9) and the bound on β(cid:15), we get that for ﬁxed
total annotation budget, the optimal choice of redundancy r is 1 when the worker quality (1 − ρ) is
above a threshold. In particular, if (1 − ρ) ≥ 0.825 then label once is the optimal strategy. However,
in experiments we observe that with our algorithm the choice of r = 1 is optimal even for much
smaller values of worker quality.

5 EXPERIMENTS

We experimentally investigate our algorithm, MBEM, on multiple large datasets. On CIFAR-10
(Krizhevsky & Hinton, 2009) and ImageNet (Deng et al., 2009), we draw noisy labels from synthetic
worker models. We conﬁrm our results on multiple worker models. On the MS-COCO dataset (Lin
et al., 2014b), we accessed the real raw data that was used to produce this annotation. We compare
MBEM against the following baselines:

• MV: First aggregate labels by performing a majority vote, then train the model.
• weighted-MV: Model learned using weighted loss function with weights set by majority vote.
• EM: First aggregate labels using EM. Then train model in the standard fashion. (Dawid & Skene,

1979)

• weighted-EM: Model learned using weighted loss function with weights set by standard EM.
• oracle weighted EM: This model is learned by minimizing (cid:96)π, using the true confusion matrices.
• oracle correctly labeled: This baseline is trained using the standard loss function (cid:96) but only using

those training examples for which at least one of the r workers has given the true label.

Note that oracle models cannot be deployed in practice. We show them to build understanding
only. In the plots, the dashed lines correspond to MV and EM algorithm. The black dashed-dotted
line shows generalization error if the model is trained using ground truth labels on all the training
examples. For experiments with synthetic noisy workers, we consider two models of worker skill:

• hammer-spammer: Each worker is either a hammer (always correct) with probability γ or a

spammer (chooses labels uniformly at random).

• class-wise hammer-spammer: Each worker can be a hammer for some subset of classes and a
spammer for the others. The confusion matrix in this case has two types of rows: (a) hammer
class: row with all off-diagonal elements being 0. (b) spammer class: row with all elements being
1/|K|. A worker is a hammer for any class k ∈ K with probability γ.

We sample m confusion matrices {π(a)}a∈[m] according to the given worker skill distribution for a
given γ. We assign r workers uniformly at random to each example. Given the ground truth labels,
we generate noisy labels according to the probabilities given in a worker’s confusion matrix, using
Equation (1). While our synthetic workers are sampled from these speciﬁc worker skill models, our
algorithms do not use this information to estimate the confusion matrices. A Python implementation
of the MBEM algorithm is available for download at https://github.com/khetan2/MBEM.

CIFAR-10 This dataset has a total of 60K images belonging to 10 different classes where each
class is represented by an equal number of images. We use 50K images for training the model
and 10K images for testing. We use the ground truth labels to generate noisy labels from synthetic
workers. We choose m = 100, and for each worker, sample confusion matrix of size 10 × 10
according to the worker skill distribution. All our experiments are carried out with a 20-layer ResNet

7

Published as a conference paper at ICLR 2018

hammer-spammer workers

r
o
r
r
e

n
o
i
t
a
z
i
l
a
r
e
n
e
G

r
o
r
r
e

n
o
i
t
a
z
i
l
a
r
e
n
e
G

worker quality

redundancy

redundancy (ﬁxed budget)

class-wise hammer-spammer workers

worker quality

redundancy

redundancy (ﬁxed budget)

Figure 1: Plots for CIFAR-10. Line colors- black: oracle correctly labeled, red: oracle weighted
EM, blue: MBEM, green: weighted EM, yellow: weighted MV.

which achieves an accuracy of 91.5%. With the larger ResNet-200, we can obtain a higher accuracy
of 93.5% but to save training time we restrict our attention to ResNet-20. We run MBEM 1 for
T = 2 rounds. We assume that the prior distribution (cid:98)q is uniform. We report mean accuracy of 5
runs and its standard error for all the experiments.

Figure 1 shows plots for CIFAR-10 dataset under various settings. The three plots in the ﬁrst row
correspond to “hammer-spammer” worker skill distribution and the plots in the second row corre-
spond to “class-wise hammer-spammer” distribution. In the ﬁrst plot, we ﬁx redundancy r = 1, and
plot generalization error of the model for varying hammer probability γ. MBEM signiﬁcantly out-
performs all baselines and closely matches the Oracle weighted EM. This implies MBEM recovers
worker confusion matrices accurately even when we have only one label per example. When there
is only one label per example, MV, weighted-MV, EM, and weighted-EM all reduce learning with
the standard loss function (cid:96).

In the second plot, we ﬁx hammer probability γ = 0.2, and vary redundancy r. This plot shows that
weighted-MV and weighted-EM perform signiﬁcantly better than MV and EM and conﬁrms that
our approach of weighing the loss function with posterior probability is effective. MBEM performs
much better than weighted-EM at small redundancy, demonstrating the effect of our bootstrapping
idea. However, when redundancy is large, EM works as good as MBEM.

In the third plot, we show that when the total annotation budget is ﬁxed, it is optimal to collect
one label per example for as many examples as possible. We ﬁxed hammer probability γ = 0.2.
Here, when redundancy is increased from 1 to 2, the number of of available training examples is
reduced by 50%, and so on. Performance of weighted-EM improves when redundancy is increased
from 1 to 5, showing that with the standard EM algorithm it might be better to collect redundant
annotations for fewer example (as it leads to better estimation of worker qualities) than to singly
annotate more examples. However, MBEM always performs better than the standard EM algorithm,
achieving lowest generalization error with many singly annotated examples. Unlike standard EM,
MBEM can estimate worker qualities even with singly annotated examples by comparing them with
model predictions. This corroborates our theoretical result that label-once is the optimal strategy
when worker quality is above a threshold. The plots corresponding to class-wise hammer-spammer
workers follow the same trend. Estimation of confusion matrices in this setting is difﬁcult and hence
the gap between MBEM and the baselines is less pronounced.

ImageNet The ImageNet-1K dataset contains 1.2M training examples and 50K validation exam-
ples. We divide test set in two parts: 10K for validation and 40K for test. Each example belongs to
one of the possible 1000 classes. We implement our algorithms using a ResNet-18 that achieves top-

8

Published as a conference paper at ICLR 2018

hammer-spammer workers

worker quality - 0.2

worker quality - 0.6

class-wise hammer-spammer workers
worker quality - 0.2
worker quality - 0.6

r
o
r
r
e

n
o
i
t
a
z
i
l
a
r
e
n
e
G

redundancy (ﬁxed budget)

redundancy (ﬁxed budget)

redundancy (ﬁxed budget)

redundancy (ﬁxed budget)

Figure 2: Plots for ImageNet. Solid lines represent top-5 error, dashed-lines represent top-1 error.
Line colors- blue: MBEM, green: weighted majority vote, yellow: majority vote

Approach
majority vote
EM
MBEM
ground truth labels

F1 score
0.433
0.447
0.451
0.512

e
r
o
c
s
1
F
n
o
i
t
a
z
i
l
a
r
e
n
e
G

redundancy (ﬁxed budget)

Figure 3: Results on raw MS-COCO annotations.
1 accuracy of 69.5% and top-5 accuracy of 89% on ground truth labels. We use m = 1000 simulated
workers. Although in general, a worker can mislabel an example to one of the 1000 possible classes,
our simulated workers mislabel an example to only one of the 10 possible classes. This captures the
intuition that even with a larger number of classes, perhaps only a small number are easily confused
for each other. Therefore, each workers’ confusion matrix is of size 10 × 10. Note that without this
assumption, there is little hope of estimating a 1000 × 1000 confusion matrix for each worker by
collecting only approximately 1200 noisy labels from a worker. The rest of the settings are the same
as in our CIFAR-10 experiments. In Figure 2, we ﬁx total annotation budget to be 1.2M and vary
redundancy from 1 to 9. When redundancy is 9, we have only (1.2/9)M training examples, each
labeled by 9 workers. MBEM outperforms baselines in each of the plots, achieving the minimum
generalization error with many singly annotated training examples.

MS-COCO These experiments use the real raw annotations collected when MS-COCO was
crowdsourced. Each image in the dataset has multiple objects (approximately 3 on average). For
validation set images (out of 40K), labels were collected from 9 workers on average. Each worker
marks which out of the 80 possible objects are present. However, on many examples workers dis-
agree. These annotations were collected to label bounding boxes but we ask a different question:
what is the best way to learn a model to perform multi-object classiﬁcation, using these noisy an-
notations. We use 35K images for training the model and 1K for validation and 4K for testing. We
use raw noisy annotations for training the model and the ﬁnal MS-COCO annotations as the ground
truth for the validation and test set. We use ResNet-98 deep learning model and train independent
binary classiﬁer for each of the 80 object classes. Table in Figure 3 shows generalization F1 score
of four different algorithms: majority vote, EM, MBEM using all 9 noisy annotations on each of the
training examples, and a model trained using the ground truth labels. MBEM performs signiﬁcantly
better than the standard majority vote and slightly improves over EM. In the plot, we ﬁx the total
annotation budget to 35K. We vary redundancy from 1 to 7, and accordingly reduce the number of
training examples to keep the total number of annotations ﬁxed. When redundancy is r < 9 we
select uniformly at random r of the original 9 noisy annotations. Again, we ﬁnd it best to singly an-
notate as many examples as possible when the total annotation budget is ﬁxed. MBEM signiﬁcantly
outperforms majority voting and EM at small redundancy.

6 CONCLUSION

We introduced a new algorithm for learning from noisy crowd workers. We also presented a new
theoretical and empirical demonstration of the insight that when examples are cheap and annotations
expensive, it’s better to label many examples once than to label few multiply when worker quality is

9

Published as a conference paper at ICLR 2018

above a threshold. Many avenues seem ripe for future work. We are especially keen to incorporate
our approach into active query schemes, choosing not only which examples to annotate, but which
annotator to route them to based on our models current knowledge of both the data and the worker
confusion matrices.

REFERENCES

Jonathan Bragg, Daniel S Weld, et al. Optimal testing for crowd workers. In Proceedings of the
2016 International Conference on Autonomous Agents & Multiagent Systems, pp. 966–974. In-
ternational Foundation for Autonomous Agents and Multiagent Systems, 2016.

Steve Branson, Catherine Wah, Florian Schroff, Boris Babenko, Peter Welinder, Pietro Perona, and
Serge Belongie. Visual recognition with humans in the loop. Computer Vision–ECCV 2010, pp.
438–451, 2010.

Steve Branson, Grant Van Horn, and Pietro Perona. Lean crowdsourcing: Combining humans and
machines in an online system. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 7474–7483, 2017.

Nilesh Dalvi, Anirban Dasgupta, Ravi Kumar, and Vibhor Rastogi. Aggregating crowdsourced
In Proceedings of the 22nd international conference on World Wide Web, pp.

binary ratings.
285–294. ACM, 2013.

Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates

using the em algorithm. Applied statistics, pp. 20–28, 1979.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical

Image Database. In CVPR09, 2009.

Jia Deng, Jonathan Krause, and Li Fei-Fei. Fine-grained crowdsourcing for ﬁne-grained recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 580–587,
2013.

Melody Y Guan, Varun Gulshan, Andrew M Dai, and Geoffrey E Hinton. Who said what: Modeling

individual labelers improves classiﬁcation. arXiv preprint arXiv:1703.08774, 2017.

Panagiotis G Ipeirotis, Foster Provost, Victor S Sheng, and Jing Wang. Repeated labeling using

multiple noisy labelers. Data Mining and Knowledge Discovery, 28(2):402–441, 2014.

Rong Jin and Zoubin Ghahramani. Learning with multiple labels. In Advances in neural information

processing systems, pp. 921–928, 2003.

Ishan Jindal, Matthew Nokleby, and Xuewen Chen. Learning deep networks from noisy labels with
dropout regularization. In Data Mining (ICDM), 2016 IEEE 16th International Conference on,
pp. 967–972. IEEE, 2016.

Armand Joulin, Laurens van der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual fea-
tures from large weakly supervised data. In European Conference on Computer Vision, pp. 67–84.
Springer, 2016.

David R Karger, Sewoong Oh, and Devavrat Shah. Budget-optimal task allocation for reliable

crowdsourcing systems. Operations Research, 62(1):1–24, 2014.

Ashish Khetan and Sewoong Oh. Achieving budget-optimality with adaptive schemes in crowd-

sourcing. In Advances in Neural Information Processing Systems, pp. 4844–4852, 2016.

Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard Zhou, Alexander Toshev, Tom Duerig,
James Philbin, and Li Fei-Fei. The unreasonable effectiveness of noisy data for ﬁne-grained
recognition. In European Conference on Computer Vision, pp. 301–320. Springer, 2016.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Christopher H Lin, Daniel S Weld, et al. To re (label), or not to re (label). In Second AAAI conference

on human computation and crowdsourcing, 2014a.

10

Published as a conference paper at ICLR 2018

Christopher H Lin, M Mausam, and Daniel S Weld. Re-active learning: Active learning with rela-

beling. In AAAI, pp. 1845–1852, 2016.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
In European

Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context.
conference on computer vision, pp. 740–755. Springer, 2014b.

Qiang Liu, Jian Peng, and Alexander T Ihler. Variational inference for crowdsourcing. In Advances

in neural information processing systems, pp. 692–700, 2012.

Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with

noisy labels. In Advances in neural information processing systems, pp. 1196–1204, 2013.

Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R´e. Data pro-
gramming: Creating large training sets, quickly. In Advances in Neural Information Processing
Systems, pp. 3567–3575, 2016.

Victor S Sheng, Foster Provost, and Panagiotis G Ipeirotis. Get another label? improving data
quality and data mining using multiple, noisy labelers. In Proceedings of the 14th ACM SIGKDD
international conference on Knowledge discovery and data mining, pp. 614–622. ACM, 2008.

Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training

convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080, 2014.

Catherine Wah, Steve Branson, Pietro Perona, and Serge Belongie. Multiclass recognition and
part localization with humans in the loop. In Computer Vision (ICCV), 2011 IEEE International
Conference on, pp. 2524–2531. IEEE, 2011.

Peter Welinder and Pietro Perona. Online crowdsourcing: rating annotators and obtaining cost-
effective labels. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE
Computer Society Conference on, pp. 25–32. IEEE, 2010.

Peter Welinder, Steve Branson, Pietro Perona, and Serge J Belongie. The multidimensional wisdom

of crowds. In Advances in neural information processing systems, pp. 2424–2432, 2010.

Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier R Movellan, and Paul L Ruvolo. Whose
vote should count more: Optimal integration of labels from labelers of unknown expertise. In
Advances in neural information processing systems, pp. 2035–2043, 2009.

Yuchen Zhang, Xi Chen, Denny Zhou, and Michael I Jordan. Spectral methods meet em: A provably
optimal algorithm for crowdsourcing. In Advances in neural information processing systems, pp.
1260–1268, 2014.

Dengyong Zhou, Qiang Liu, John C Platt, Christopher Meek, and Nihar B Shah. Regularized

minimax conditional entropy for crowdsourcing. arXiv preprint arXiv:1503.07240, 2015.

Denny Zhou, Sumit Basu, Yi Mao, and John C Platt. Learning from the wisdom of crowds by
minimax entropy. In Advances in Neural Information Processing Systems, pp. 2195–2203, 2012.

11

Published as a conference paper at ICLR 2018

APPENDIX

A PROOF OF THEOREM 4.1

(cid:98)π,(cid:98)q to (cid:96)

Assuming the prior on Y , distribution q, to be uniform, we change the notation for the modiﬁed loss
(cid:98)π. Observe that for binary classiﬁcation, Z (r) ∈ {±1}r. Let ρ denote the posterior
function (cid:96)
distribution of Y , Equation (5), when q is uniform. Let τ denote the probability of observing an
instance of Z (r) as a function of the latent true confusion matrices π, conditioned on the ground
truth label Y = y.

ρ

(cid:98)π(y, Z (r), w(r)) := P

(cid:98)π[Y = y | Z (r); w(r)] ,

τπ(y, Z (r), w(r)) := Pπ[Z (r) | Y = y; w(r)] . (10)

Let W denote the uniform distribution over a pool of m workers, from which nr workers are selected
i.i.d. with replacement, and a batch of r workers are assigned to each example Xi. We deﬁne the
following quantities which play an important role in our analysis.
(cid:20) (cid:88)

(cid:21)
(cid:98)π(−y, Z (r), w(r))τπ(y, Z (r), w(r))
ρ

.

:= Ew∼W

β

(cid:98)π(y)

(11)

Z(r)∈{±1}r

(cid:20)

(cid:26) (cid:88)

β

(cid:98)π

:= Ew∼W

max
y∈{±1}

α(y)

:= Ew∼W

Z(r)∈{±1}r

(cid:104)
Pπ[Z = −y | Y = y; w]
(cid:104)

(cid:105)

.

α := Ew∼W

Pπ[Z = −y | Y = y; w]

max
y∈{±1}

(cid:105)

.

(cid:98)π(−y, Z (r), w(r))τπ(y, Z (r), w(r))
ρ

.

(12)

(cid:27)(cid:21)

For any given (cid:98)π with |(cid:98)π(a)
deﬁnition of β
(cid:98)π such that β
following bound on β(cid:15).

ks − π(a)

ks | ≤ (cid:15), for all a ∈ [m], k, s ∈ K, we can compute β(cid:15) from the
(cid:98)π ≤ β(cid:15). For the special case described in Section 4.4, we have the

β(cid:15) ≤

r
(cid:88)

u=0

(ρ + (cid:15))(r−u)(1 − ρ − (cid:15))u
(ρ + (cid:15))u(1 − ρ − (cid:15))(r−u) + (ρ + (cid:15))(r−u)(1 − ρ − (cid:15))u

(cid:19)

(cid:18)r
u

(1 − ρ)r−uρu (15)

= (ρ + (cid:15))r

(cid:19)(cid:32)(cid:18) ρ + (cid:15)
(cid:18)r
u

1 − ρ − (cid:15)

(cid:19)u

(cid:18) ρ + (cid:15)

+

1 − ρ − (cid:15)

(cid:19)r−u(cid:33)−1

.

r
(cid:88)

u=0

It can easily be checked that β(cid:15) ≤ (ρ + (cid:15))r (cid:80)(cid:100)r/2(cid:101)
We present two lemma that analyze the two alternative steps of our algorithm. The following lemma
gives a bound on the excess risk of function (cid:98)f learnt by minimizing the modiﬁed loss function (cid:96)
(cid:98)π.
Lemma A.1. Under the assumptions of Theorem 4.1, the excess risk of function (cid:98)f in Equation (6),
computed with posterior distribution P

(cid:1)(1 − ρ − (cid:15))u(ρ + (cid:15))−u.

(cid:98)π (5) using n training examples is bounded by

(cid:0)r
u

u=0

R(cid:96),D( (cid:98)f ) − min
f ∈F

R(cid:96),D(f ) ≤

C
1 − 2β

(cid:98)π

(cid:32)(cid:114)

(cid:114)

V
n

+

log(1/δ1)
n

(cid:33)

,

with probability at least 1 − δ1, where C is a universal constant. When P
majority vote, while initializing the iterative Algorithm 1, the above bound holds with β
by α.

(cid:98)π is computed using
(cid:98)π replaced

The following lemma gives an (cid:96)∞ norm bound on confusion matrices (cid:98)π estimated using model
prediction (cid:98)f (X) as the ground truth labels. In the analysis, we assume fresh samples are used for
estimating confusion matrices in step 3, Algorithm 1. Therefore the function (cid:98)f is independent of the
samples Xi’s on which (cid:98)π is estimated. Let K = |K|.

12

(13)

(14)

(16)

(17)

Published as a conference paper at ICLR 2018

Lemma A.2. Under the assumptions of Theorem 4.1, (cid:96)∞ error in estimated confusion matrices (cid:98)π
as computed in Equation (7), using n samples and a predictor function (cid:98)f with risk R(cid:96),D ≤ δ, is
bounded by

(cid:12)
(cid:12)(cid:98)π(a)
(cid:12)

ks − π(a)

ks

(cid:12)
(cid:12)
(cid:12) ≤

2δ + 16(cid:112)m log(4mK 2δ1)/(nr)
1/K − δ − 8(cid:112)m log(4mK 2/δ1)/(nr)

,

∀ a ∈ [m], k, s ∈ K , (18)

with probability at least 1 − δ1.

First we apply Lemma A.1 with P
(cid:98)π computed using majority vote. We get a bound on the risk
of function (cid:98)f computed in the ﬁrst round. With this (cid:98)f , we apply Lemma A.2. When n is
sufﬁciently large such that Equation (8) holds, the denominator in Equation (18), 1/K − δ −
8(cid:112)m log(4mK 2/δ1)/(nr) ≥ 1/8. Therefore, in the ﬁrst round, the error in confusion matrix
estimation is bounded by (cid:15), which is deﬁned in the Theorem.
For the second round: we apply Lemma A.1 with P
(cid:98)π computed as the posterior distribution (5).
Where (cid:96)∞ error in (cid:98)π is bounded by (cid:15). This gives the desired bound in (9). With this (cid:98)f , we apply
Lemma A.2 and obtain (cid:96)∞ error in (cid:98)π bounded by (cid:15)1, which is deﬁned in the Theorem.
For the given probability of error δ in the Theorem, we chose δ1 in both the lemma to be δ/4 such
that with union bound we get the desired probability of δ.

A.1 PROOF OF LEMMA A.1

Let f ∗ := arg minf ∈F R(cid:96),D(f ). Let’s denote the distribution of (X, Z (r), w(r)) by DW,π,r. For
ease of notation, we denote DW,π,r by Dπ. Similar to R(cid:96),D, risk of decision function f with respect
to the modiﬁed loss function (cid:96)

1. (cid:96)

(cid:98)π-risk under Dπ: R(cid:96)

2. Empirical (cid:96)

(cid:98)π is characterized by the following quantities:
(cid:98)π,Dπ (f ) := E
(cid:98)π-risk on samples: (cid:98)R(cid:96)

(X,Z(r),w(r))∼Dπ
(cid:80)n
(cid:98)π,Dπ (f ) := 1

(cid:98)π(f (X), Z (r), w(r))(cid:3).
, w(r)
(cid:98)π(f (Xi), Z (r)
).
i

i=1 (cid:96)

(cid:2)(cid:96)

n

i

With the above deﬁnitions, we have the following,

R(cid:96),D( (cid:98)f ) − R(cid:96),D(f ∗)

= R(cid:96)

(cid:98)π,Dπ ( (cid:98)f ) − R(cid:96)

≤ R(cid:96)

(cid:98)π,Dπ ( (cid:98)f ) − R(cid:96)

= (cid:98)R(cid:96)

(cid:98)π,Dπ ( (cid:98)f ) − (cid:98)R(cid:96)

(cid:16)

(cid:98)π,Dπ (f ∗) +
(cid:98)π,Dπ (f ∗) + 2β
(cid:16)
(cid:98)π,Dπ (f ∗) +
(cid:17)
R(cid:96),D( (cid:98)f ) − R(cid:96),D(f ∗)

(cid:16)

+2β

(cid:98)π

≤ 2 max
f ∈F
(cid:32)(cid:114)

≤ C

(cid:12)
(cid:12)
(cid:12) (cid:98)R(cid:96)

V
n

+

(cid:98)π,Dπ (f ) − R(cid:96)

(cid:114)

log(1/δ)
n

(cid:98)π,Dπ (f )
(cid:33)

+ 2β

(cid:98)π

(cid:17)

R(cid:96),D( (cid:98)f ) − R(cid:96)

(cid:98)π,Dπ ( (cid:98)f )

− (R(cid:96),D(f ∗) − R(cid:96)
(cid:17)
(cid:16)
R(cid:96),D( (cid:98)f ) − R(cid:96),D(f ∗)
(cid:17)
(cid:98)π,Dπ ( (cid:98)f )

(cid:98)π,Dπ ( (cid:98)f ) − (cid:98)R(cid:96)

(cid:98)R(cid:96)

+

(cid:16)

R(cid:96)

(cid:98)π

(cid:98)π,Dπ (f ∗))

(cid:98)π,Dπ (f ∗) − R(cid:96)

(cid:98)π,Dπ (f ∗)

(cid:16)

(cid:12)
(cid:12)
(cid:12) + 2β

(cid:98)π

R(cid:96),D( (cid:98)f ) − R(cid:96),D(f ∗)

(cid:17)

(cid:16)

(cid:17)
R(cid:96),D( (cid:98)f ) − R(cid:96),D(f ∗)

,

where (19) follows from Equation (24). (20) follows from the fact that (cid:98)f is the minimizer of (cid:98)R(cid:96)
as computed in (6).
hypothesis class F, and C is a universal constant.

(cid:98)π,Dπ
(21) follows from the basic excess-risk bound. V is the VC dimension of

Following shows the inequality used in Equation (19). For binary classiﬁcation, we denote the two
classes by Y, −Y .

= R(cid:96),D( (cid:98)f ) − R(cid:96)
= E(X,Y )∼D

(cid:104)

(cid:98)π,Dπ ( (cid:98)f ) − (R(cid:96),D(f ∗) − R(cid:96)
(cid:98)π(Y )

(cid:96)( (cid:98)f (X), Y ) − (cid:96)(f ∗(X), Y )

(cid:98)π,Dπ (f ∗))
(cid:17)
−

(cid:16)(cid:16)

(cid:16)

= 2E(X,Y )∼D

(cid:96)( (cid:98)f (X), Y ) − (cid:96)(f ∗(X), Y )

(cid:17)(cid:105)

β
(cid:104)

(cid:16)

β

(cid:98)π(Y )
(cid:17)
R(cid:96),D( (cid:98)f ) − R(cid:96),D(f ∗)

(cid:16)

,

≤ 2β

(cid:98)π

13

(cid:96)( (cid:98)f (X), −Y ) − (cid:96)(f ∗(X), −Y )

(19)
(cid:17)

(20)

(21)

(cid:17)(cid:17)(cid:105)
(22)

(23)

(24)

Published as a conference paper at ICLR 2018

where (22) follows from Equation (26).
(cid:96)(f (X), Y ) + (cid:96)(f (X), −Y ) = 1. (24) follows from the deﬁnition of β
When (cid:96)
by α. α is deﬁned in (14).

(23) follows from the fact that for 0-1 loss function
(cid:98)π deﬁned in Equation (12).
(cid:98)π replaced

(cid:98)π is computed using weighted majority vote of the workers then (24) holds with β

Following shows the equality used in Equation (22). Using the notations ρ
(cid:98)π and τπ, in the following,
for any function f ∈ F, we compute the excess risk due to the unbiasedness of the modiﬁed loss
function (cid:96)

(cid:98)π.

−E

(X,Y,w(r))∼Dπ

(1 − ρ

(cid:98)π(−Y, Z (r), w(r)))(cid:96)(f (X), Y )

(25)

(X,Z(r),w(r))∼Dπ [(cid:96)

(cid:98)π(f (X), Z (r), w(r))]

R(cid:96),D(f ) − R(cid:96)

(cid:98)π,Dπ (f )

= E(X,Y )∼D [(cid:96)(f (X), Y )] − E
= E(X,Y )∼D [(cid:96)(f (X), Y )]

(cid:34)

(cid:88)

(cid:16)

Z(r)∈{±1}r

+ρ

(cid:17)
(cid:98)π(−Y, Z (r), w(r))(cid:96)(f (X), −Y )

τπ(Y, Z (r), w(r))

(cid:35)

= E(X,Y )∼D [β

(cid:98)π(Y ) ((cid:96)(f (X), Y ) − (cid:96)(f (X), −Y ))] ,

where β
Observe that when (cid:96)
holds with β

(cid:98)π(Y ) is deﬁned in (11). Where (25) follows from the deﬁnition of (cid:96)

(cid:98)π given in Equation (4).
(cid:98)π is computed using weighted majority vote of the workers then Equation (26)

(cid:98)π(Y ) replaced by α(y). α(y) is deﬁned in (13).

A.2 PROOF OF LEMMA A.2

Recall that we have

(cid:98)π(a)
ks =

(cid:80)n

(cid:80)r

i=1

j=1

(cid:80)n

I[wij = a]I[ti = k]I[Zij = s]
(cid:80)r
I[wij = a]I[ti = k]

i=1

j=1

Let ti denote (cid:98)f (Xi). By the deﬁnition of risk, for any k ∈ K, we have
(cid:104)(cid:12)
(cid:12)I[Yi = k] − I[ti = k](cid:12)

(cid:105)
(cid:12) = 1

= δ .

P

Let |K| = K. Deﬁne, for ﬁxed a ∈ [m], and k, s ∈ K,
n
(cid:88)

r
(cid:88)

A :=

I[wij = a]I[ti = k]I[Zij = s] ,

¯A :=

nrπks
mK

B :=

I[wij = a]I[ti = k] ,

¯B :=

nr
mK

C :=

I[wij = a]

(cid:12)
(cid:12)
(cid:12)

(cid:12)
I[Yi = k] − I[ti = k]
(cid:12)
(cid:12) ,

¯C :=

nrδ
m

,

D :=

I[wij = a]I[Yi = k]I[Zij = s] ,

E :=

I[wij = a]I[Yi = k] .

i=1
n
(cid:88)

j=1
r
(cid:88)

i=1
n
(cid:88)

j=1
r
(cid:88)

i=1
n
(cid:88)

j=1
r
(cid:88)

i=1
n
(cid:88)

j=1
r
(cid:88)

i=1

j=1

Note that A, B, C, D, E depend upon a ∈ [m], k, s ∈ K. However, for ease of notations, we have
not included the subscripts. We have,

(cid:12)
(cid:12)(cid:98)π(a)
(cid:12)

ks − π(a)

ks

(cid:12)
(cid:12)
(cid:12) =

A − Bπks
B

|(A − ¯A) − (B − ¯B)πks|
| ¯B + (B − ¯B)|
|A − ¯A| + |(B − ¯B)|πks
| ¯B| − |B − ¯B|

=

≤

14

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

Published as a conference paper at ICLR 2018

Now, we have,

We have,

|A − ¯A| ≤ |A − D| + |D − ¯A|

≤ C + |D − ¯A| .

|B − ¯B| ≤ |B − E| + |E − ¯B|

≤ C + |E − ¯B|

C ≤

(cid:114)

nrδ
m

+

3nrδ log(2mK/δ1)
m

,

Observe that C is a sum of nr i.i.d. Bernoulli random variables with mean δ/m. Using Chernoff
bound we get that

for all a ∈ [m], and k ∈ K with probability at least 1 − δ1. Similarly, D is a sum of nr i.i.d.
Bernoulli random variables with mean πks/(mk). Again, using Chernoff bound we get that
(cid:114)

(cid:12)D − ¯A(cid:12)
(cid:12)

(cid:12) ≤

3nrπks log(2mK 2/δ1)
mK

,

for all a ∈ [m], k, s ∈ K with probability at least 1 − δ1. From the bound on |D − ¯A|, it follows that

|E − ¯B| ≤

(cid:114)

3nr log(2mK 2/δ1)
m

Collecting Equations (33)-(38), we have for all a ∈ [m], k, s ∈ K

(cid:12)
(cid:12)(cid:98)π(a)
(cid:12)

ks − π(a)

ks

(cid:12)
(cid:12)
(cid:12) ≤

2δ + 16(cid:112)m log(2mK 2δ1/(nr)
1/K − δ − 8(cid:112)m log(2mK 2/δ1)/(nr)

,

with probability at least 1 − 2δ1.

(34)

(35)

(36)

(37)

(38)

(39)

15

