9
1
0
2
 
n
u
J
 
9
 
 
]

R

I
.
s
c
[
 
 
1
v
8
9
8
4
0
.
6
0
9
1
:
v
i
X
r
a

1

Hierarchical Taxonomy-Aware and Attentional
Graph Capsule RCNNs for Large-Scale
Multi-Label Text Classiﬁcation

Hao Peng, Jianxin Li, Member, IEEE, Qiran Gong, Senzhang Wang, Lifang He,
Bo Li, Lihong Wang, and Philip S. Yu, Fellow, IEEE,

Abstract—CNNs, RNNs, GCNs, and CapsNets have shown signiﬁcant insights in representation learning and are widely used in
various text mining tasks such as large-scale multi-label text classiﬁcation. However, most existing deep models for multi-label text
classiﬁcation consider either the non-consecutive and long-distance semantics or the sequential semantics, but how to consider them
both coherently is less studied. In addition, most existing methods treat output labels as independent medoids, but ignore the
hierarchical relations among them, leading to useful semantic information loss. In this paper, we propose a novel hierarchical
taxonomy-aware and attentional graph capsule recurrent CNNs framework for large-scale multi-label text classiﬁcation. Speciﬁcally, we
ﬁrst propose to model each document as a word order preserved graph-of-words and normalize it as a corresponding words-matrix
representation which preserves both the non-consecutive, long-distance and local sequential semantics. Then the words-matrix is
input to the proposed attentional graph capsule recurrent CNNs for more effectively learning the semantic features. To leverage the
hierarchical relations among the class labels, we propose a hierarchical taxonomy embedding method to learn their representations,
and deﬁne a novel weighted margin loss by incorporating the label representation similarity. Extensive evaluations on three datasets
show that our model signiﬁcantly improves the performance of large-scale multi-label text classiﬁcation by comparing with
state-of-the-art approaches.

Index Terms—Multi-label text classiﬁcation, document modeling, graph rcnn, attention network, capsule network, meta-paths,
taxonomy embedding

(cid:70)

1 INTRODUCTION

As a fundamental text mining task, text classiﬁcation aims
to assign a text with one or several category labels such
as topic labels and sentiment labels. Traditional approaches
represent the text as sparse lexical features due to the
simplicity and effectiveness [1]. For example, bag-of-words
and n-gram are widely used to extract textual features, and
then a general machine learning model such as Bayesian,
logistic regression or SVM is utilized for text classiﬁcation.
[2],
With the development of deep learning techniques
[3], variants of neural network based models have been
exploited from a large body of innovations, such as re-
current neural networks [4], [5], [6], [7], diversiﬁed con-

•

• Hao Peng, Jianxin Li, Qiran Gong and Bo Li are with Beijing Ad-
vanced Innovation Center for Big Data and Brain Computing, Bei-
hang University, Beijing 100083, and also with the State Key Labora-
tory of Software Development Environment, Beihang University, Bei-
jing 100083, China, China. E-mail:{penghao, lijx, libo}@act.buaa.edu.cn,
allen gong@buaa.edu.cn.
Senzhang Wang is with the College of Computer Science and Technology,
Nanjing University of Aeronautics and Astronautics, Nanjing 211106,
China. E-mail: szwang@nuaa.edu.cn.
Lifang He is with the Department of Biostatistics and Epidemiology,
University of Pennsylvania, Philadelphia, PA 19104, USA. E-mail:
lfhe@pennmedicine.upenn.edu.
Lihong Wang is with the National Computer Network Emergency Re-
sponse Technical Team/Coordination Center of China, Beijing 100029,
China. E-mail: wlh@isc.org.cn.

•

•

• Philip S. Yu is with the Department of Computer Science, University of
Illinois at Chicago, Chicago, IL 60607, USA. E-mail: psyu@uic.edu.

Manuscript received May 6, 2019. (Corresponding author: Jianxin Li.)

volutional neural networks [8], [9], [10], [11], [12], capsule
neural networks [13] and adversarial structures [14], [15].
These deep models have achieved inspiring performance
gains on text classiﬁcation due to their powerful capacity
in representing the text as a ﬁx-size feature map with rich
semantics information.

Recently, three popular deep learning architectures have
attracted increasing research attention for text data, i.e.,
recurrent neural networks (RNNs) [6], [16], [7], [17], con-
volutional neural networks (CNNs) [8], [12], [10] and graph
convolutional networks (GCNs) [11], [9]. RNNs are more
powerful on capturing the semantics of short text [18],
but are less effective to learn semantic features of long
documents. Although the bi-directional block self-attention
networks are proposed [7] to better model text or sentence,
they consider documents as natural sequences of words, and
ignore the long-distance semantic between paragraphs or
sentences. CNNs simply evaluate the semantic composition
of the consecutive words extracted with n-gram, while
n-gram may lose the long-distance semantic dependency
among the words [1]. Compared with RNNs and CNNs,
GCNs can better capture the non-consecutive phrases and
long-distance word dependency semantics [11], [9], but
ignore the sequential information. To sum up, there still
lacks of a model that can simultaneously capture the non-
consecutive, long-distance and sequential semantics of text.
Meanwhile, as the text labels of some real-world text classiﬁ-
cation tasks are characterized by large hierarchies, there may
exist strong dependency among the class labels [19], [20],

[21]. Existing deep learning models cannot effectively and
efﬁciently leverage the hierarchical dependencies among
labels for improving the classiﬁcation performance, either.

It is non-trivial to obtain a desirable classiﬁcation perfor-
mance for large-scale multi-label text due to the following
major challenges. First, although there are many methods
for document modeling, how to represent a document by
fully preserving its rich and complex semantic information
still remains an open problem [22]. It is challenging to come
up with a document modeling method that can fully capture
the semantics of a document, including the non-consecutive,
long-distance and sequential semantics of the words. Sec-
ond, existing CNNs, RNNs and GCNs models usually can
only capture partial textual features. It is challenging to de-
sign a deep learning model that can simultaneously capture
multiple types of textual features mentioned above. Third,
although some recursive regularization based hierarchical
text classiﬁcation models
[23], [24], [11], [25] consider
the pair-wise relation between labels, they fail to consider
their hierarchical relations. In addition, the computation
of the above regularized models is expensive due to the
use of Euclidean constraints. How to make full use of the
hierarchical label-dependencies among labels to improve
the classiﬁcation accuracy and reduce the computational
complexity is also challenging.

To address

the above challenges, we propose a
novel Hierarchical taxonomy-awarE and Attentional Graph
Capsule Recurrent CNNs framework called HE-AGCRCNN
for large-scale multi-label text classiﬁcation. Speciﬁcally, our
framework contains three major parts: word order pre-
served graph-of-words for document modeling, attentional
capsule recurrent CNNs for features learning, and hierar-
chical taxonomy-aware weighted margin loss for multi-label
text classiﬁcation. Next we will elaborate the three parts as
follows.

Word Order Preserved Graph-of-Words for Document
Modeling. We regard each word as a vertex, the word co-
occurrence relationships within a sliding window as edges,
and the positional index of a word appearing in the doc-
ument as its attribute. In this way, we build a word order
preserved graph-of-words to represent a document. Then
we select top N central words from the graph-of-words
based on the closeness centrality, and construct a subgraph
for each central word from neighbors by breadth ﬁrst search
(BFS) and depth ﬁrst search (DFS). To preserve local se-
quential, non-consecutive and long-distance semantics, we
next normalize each subgraph to blocks of word sequences
that retains local word order information by utilizing the
attribute of the vertex, and construct an arranged words-
matrix for the N sub-graphs. To incorporate more semantic
information, we use a pre-trained word embedding vectors
based on word2vec
[26], [27] as word representation in
the arranged words-matrix. Finally, each document is rep-
resented as a corresponding 3-D tensor whose three dimen-
sions are the selected central words, the ordered neighbor
words sequence, and the embedding vector of each word,
respectively.

Attentional Capsule Recurrent Convolutional Neural
Networks. An attentional capsule recurrent CNN (RCNN)
model is designed to make use of the document tensor as
input for document features learning. The proposal model

2

ﬁrst uses two attentional RCNN layers to learn different lev-
els of text features with both non-consecutive, long-distance
and local sequential semantics. Here, we not only guarantee
the independence of the feature representation between sub-
graphs, but also model different impacts among different
blocks of word sequences. When the convolution kernel
slides horizontally along the combining long-distance and
local sequential ordering of words, the attentional RNN
unit is used to encode the output of the previous step of
CNN, and the output of current step of attentional RNN to
produce the ﬁnal output feature map in the RCNNs layer.
Then a capsule network layer is used to implement an itera-
tive routing process to learn the intrinsic spatial relationship
between text features from lower to higher levels for each
sub-graph. In the ﬁnal DigitCaps layer, the activity vector
of each capsule indicates the presence of an instance of each
class and is used to calculate the classiﬁcation loss.

Hierarchical Taxonomy-Aware Weighted Margin Loss.
Considering the hierarchical taxonomy of the labels, we
design two types of meta-paths, and use them to conduct
random walk on the hierarchical label taxonomy network
to generate label sequences. Therefore, the hierarchical tax-
onomy relation among the labels can be encoded in a
continuous vector space with the skip-gram [27] on the
sequences. In this way, the distance between two labels can
be measured by calculating the cosine similarity of their
label vectors. By taking the distance between labels into
consideration, we design a new weighted margin loss to
guide the training of proposed models in multi-label text
classiﬁcation.

We conduct extensive evaluations on our proposed
framework by comparing it with state-of-the-art methods
on three benchmark datasets, comparing with traditional
shallow models and recent deep learning models. The re-
sults show that our approach outperforms them by a large
margin in both efﬁciency and effectiveness on large-scale
multi-label text classiﬁcation.

The contributions of this paper are summarized below.

• A novel hierarchical taxonomy-aware and attentional
graph capsule recurrent CNNs framework is pro-
posed for large-scale multi-label text classiﬁcation.
• A new word order preserved graph-of-words
method is proposed to better model document and
more effectively extract textual features. The new
document modeling method preserves both non-
consecutive, long-distance and local sequential se-
mantics.

• A new word sequence block level attention recurrent
neural network is proposed to better learn local
sequential semantics of text.

• A novel hierarchical taxonomy-aware weighted mar-
gin loss is proposed to better measure the distance of
classes in hierarchy and guide the proposed models
training.

• Extensive evaluations on three benchmark datasets
demonstrate the efﬁciency and effectiveness of the
proposal.

The rest of the paper is organized as follows. We ﬁrst
introduce the word order preserved graph-of-words based
document modeling in Section 2. Then we present the model

3

Fig. 1. Illustration of converting a document to an arranged words-matrix representation. We ﬁrst construct a word order preserved graph-of-words
from the original document, and then a top N nodes (words) sequence is selected from the ranking of each node’s closeness centrality feature.
For each node (word) in the sequence, a corresponding sub-graph is extracted and normalized as a sequence of words that retain local word order
information.

architecture in Sections 3 and 4. The evaluation is conducted
in Section 5. Finally, we review related work in Section 6
followed by the conclusion and future work in Section 7.

2 WORD ORDER PRESERVED GRAPH-OF-WORDS
FOR DOCUMENT MODELING

In this section, we introduce how we model a document as
a word order preserved graph-of-words, and how to extract
central words and sub-graphs from it to preserve both non-
consecutive, long-distance and local sequential semantics of
the document. Formally, we denote the training document
set as D = {ds, Ts}M
s=1, where M is the total number of
documents in D, ds is a document, Ts is the label set of ds
and Ts ⊂ S. We also denote the set of labels as S = {vi|i =
1, 2, · · · , L}, where L is the total number of labels.

2.1 Word Order Preserved Graph-of-Words Construc-
tion

In order to preserve more semantic information of text,
we model a document as a word order preserved graph-
of-words. We regard each word as a vertex, the word co-
occurrence relationships within a sliding window as edges,
and the positional index appearing in the document as its
attribute, as shown in the step 1 of Figure 1.

We ﬁrst split a document into a set of sentences and
extract tokens using Stanford CoreNLP tool1. We also em-
ploy a lemmatization of each token using Stanford CoreNLP,
and remove the stop words. Then we construct an edge
between two word nodes if they co-occur in a pre-deﬁned
ﬁxed-size sliding window, and the weight of the edge is the
times of their co-occurrence. Meanwhile, we record all the
positional indexes where a word appears in the document
as its attribute. For example, for the ﬁrst sentence “Musk
told the electric car company that...” shown in the document
of Figure 1, we perform lemmatization on the second word
“told” to get “tell” with attribute “2”, and build a directed
edge from “Musk” to each of the words in the sliding
window. As shown in the word order preserved graph-of-
words of Figure 1, the word “Company” appears at the
5-th, 19-th, 35-th, 55-th, 99-th, etc. positions, respectively.
Note that the word order preserved graph-of-words is a
weighted directed graph with the positional indexes as the

1. http://stanfordnlp.github.io/CoreNLP/

node attributes. For example, in the word order preserved
graph-of-words of Figure 1, the weight of the edge between
nodes “Company” and “Car” is 6 meaning that “Company”
and “Car” has a total of 6 co-occurrences in the sliding
window.

2.2 Arranged Words-Matrix Generation

We denote the word order preserved graph-of-words as
G = (V, E, W, A), where V denotes the node set and
|V | = n, E denotes the edge set and |E| = m, W denotes
the weights of the edges and A denotes the attributes of
the nodes. We extract top N central words from G based on
node’s closeness centrality feature. Here, in order to calculate
closeness centrality for each node, we use d(v, u) to denote the
shortest-path distance between nodes v and u by using the
Dijkstra algorithm. For each node v, its closeness centrality
can be calculated by Cv = (n − 1)/(cid:80)
u∈V,u(cid:54)=v d(v, u). So
we can arrange the nodes in order of largest to smallest
according to their closeness centrality features. The larger the
closeness centrality, more important the node is in the graph.
As shown in the word order preserved graph-of-words of
Figure 1, the closeness centrality of word “Company” is the
highest 0.3714 among the words’ in the graph. Then we
select the top N central nodes from the node “Company”
to the node “Open”, as shown in the step 2 of Figure 1.
Next, we introduce how to extract sub-graph G(v) for each
selected central node v.

First, we extract the nodes and edges from the neighbor-
hood of each central node in the order of breadth ﬁrst search
(BFS), depth ﬁrst search (DFS) and the node’s closeness
centrality feature to build a subgraph. Meanwhile, we limit
the number of nodes in the subgraph to be no more than
K, as shown in the step 3 of Figure 1. In this way, the sub-
graph G(v) contains both the non-consecutive, long-distance
and local sequential information of the central word v in
the document. To further save the above information of a
subgraph, we order the words in the sub-graph G(v) by
their nodes (words) attributes. For the most case where a
subgraph contains multiple sentences, we guarantee that the
long sentences are in the front and the short sentences are in
the back. As shown in the ﬁrst line of the arranged words-
matrix in Figure 1, we convert the ﬁrst sub-graph G(v) as
sequences liking“electric car company plan purchase million
common and company expect ”. As a result, we normalize each
subgraph as sequences of nodes (words) that keep the same

4

Fig. 2. Architecture of the proposed hierarchical taxonomy-aware and attentional graph capsule recurrent convolution neural network. It consists of
document modeling, attentional capsule recurrent CNN, and hierarchical taxonomy-aware weighted margin loss for multi-label text classiﬁcation.
The network input is the original document. The length of the activity vector of each capsule in DigitCaps layer indicates presence of an instance of
each class.

length T . If the number of words in the sequence is less
than T , it is padded with zeros. Finally, we concatenate all
the normalized sequences of the N central words into an
arranged words-matrix, as shown in step 4 of Figure 1. The
red nodes represent central words.

2.3 Uniﬁed Representation of the Documents

[26]

For better representing the original words in the words-
matrix, we use word2vec [27],
to incorporate as
much word semantic information as possible. Speciﬁcally,
word2vec is trained on a larger corpus, i.e., Wikipedia.
All parameters for word2vec are set to be default values.
In this way, we have a 3-D tensor representation for each
document, where the padded vectors are zero vectors with
the same dimension. Then the convolution, recurrent and
capsule networks introduced in the next section will be
operated over the uniﬁed representations of the documents.

3 ATTENTIONAL CAPSULE RECURRENT CNN
In this section, we introduce the proposed attentional cap-
sule recurrent CNN model. After converting each document
into a 3-D tensor representation, we design a three layers of
attentional capsule recurrent CNN model to learn both the
non-consecutive, long-distance and local sequential feature.
From the input document to the output labels, the archi-
tecture is shown in Figure 2. Speciﬁcally, the three layers
of neural networks contain two major parts: two layers of
attentional recurrent convolution neural networks and one
layer of capsule networks for rich feature learning. Note
that this is a general framework and the number of atten-
tional recurrent convolution layers can be adjusted based on
speciﬁc dataset for classiﬁcation, and the parameter conﬁg-
uration of self-attentional recurrent operators and capsule
networks can be customized in different text classiﬁcation
tasks.

3.1 Attentional Recurrent CNN

Different from the architecture of existing recurrent convo-
lutional neural networks [28], which encode sentences or
document as a dense vector for classiﬁcation, our proposed
attentional recurrent convolution neural networks encode

whole document as 3-D feature map. The attentional re-
current CNN model takes the N × T × D size of 3-D
tensor extracted from the document and word embedding
as the input, where N is the number of central words, T is
the length of normalized sequence of words and D is the
dimension of word embedding, as shown in Figure 2. The
output of the two layers of attentional recurrent convolution
networks is the other 3-D feature map as input of the
proposed capsule network.

In the ﬁrst layer, the convolution operator ﬁlters the
input tensor with k1 kernels of size 1 × 3 × D with a
horizontal stride of 2 elements and a vertical stride of 1
element, which is illustrated with the black convolution
slide direction arrow in Figure 2. We use ReLU as the
activation function to speed up the training process and
avoid over-ﬁtting. Here, convolution kernel serves as a
composition of the semantics in the receptive ﬁeld to extract
the higher level semantic features. Meanwhile, we employ
a masked attentional recurrent neural operator to capture
the local sequential semantic for each sub-graph G(v). The
attentional recurrent neural operator acts on each horizontal
words sequence, which is illustrated with the red recurrent
slide direction arrow in Figure 2. However, as we know, we
convert the subgraph G(v) into blocks of word sequences
according to the properties of the nodes. We give three
different blocks, as shown in Figure 3, to illustrate the
blocks of word sequences, which consist of each line of the
arranged words-matrix. In order to measure the different
impacts of different number of blocks on the local sequential
semantic learning, we customize the masked attentional pa-
rameter shared long short-term memory, namely Attention-
LSTM, to learn the rich local sequential semantic for each
sub-graph G(v). Since our proposed framework needs to
learn feature for multiple documents, the attention-LSTM
module guarantees that any subgraphs with the same order
and number of blocks share the same attention parameters.
For example, for the T -th subgraph G(u) from the docu-
ment di, assuming that it contains the q blocks of word
sequences, the parameter of the masked attention module is
the αBT ,q,1 , αBT ,q,2 , . . . , αBT ,q,q . However, for the T -th sub-
graph G(v) from the document dj, assuming that it contains
the p blocks of word sequences, the parameter of the masked
attention module is the αBT ,p,1, αBT ,p,2, . . . , αBT ,p,p . But,

5

(2)

the Attentional-LSTM unit can be deﬁned as:

ft =σg(Wf αBxt + Uf ct−1 + bf ),
it =σg(WiαBxt + Uict−1 + bi),
ot =σg(WoαBxt + Uoct−1 + bo),
ct =ftct−1 + itσc(WcαBxt + bc),
ht =otσh(ct),

where t refers to the index of the horizontal convolution
sequence, ft refers to the forgotten gate, it refers to the input
gate, ot refers to the output gate, ct is the cell state, and αB
refers to the attentional parameter. Since the output of the
convolution network is input to the LSTM, and the output
of the LSTM is the feature map, xt = xl

j and xt+1 = xl

j+1.

3.2 Capsule Networks with Dynamic Routing

Since the capsule network can effectively learn some aspect
features of textual representation [29], the output of the
two layers of attentional recurrent convolution networks is
N × (T − 4) × k2 size of feature map and is input to the next
capsule networks with dynamic routing layer. In order to
independently learn the features of each subgraph into the
corresponding capsule vectors, different from existing tex-
tual capsule networks [13], our proposed capsule networks
guarantee the independence of feature between sub-graphs,
as shown in Figure 2.

The capsules contain groups of locally invariant neurons
that learn to recognize the presence of features and encode
their properties into vector outputs, with the vector length
representing the presence of the features. The primary cap-
sule layer is a convolution capsule layer with M channels
of capsules, as shown in Figure 2. Each primary capsule
contains m convolution units with a T +12
9 ×k2 size of kernel
and a vertical stride of 1, and can be seen as the output of all
N × T +12
9 × k2 convolution units. Here, we guarantee the
independence of the representation of sub-graph G(v). In
total, the primary capsules have N ×M capsule outputs, and
each output is a m-dimensional vector, as shown in Figure 2.
We can see all the primary capsules as a convolution layer
with Eq. 3 as its block non-linearity.

,

vj =

sj
(cid:107)sj(cid:107)

(cid:107)sj(cid:107)2
1 + (cid:107)sj(cid:107)2 ·
where vj is the output of capsule j, and sj is its total input.
For all but the ﬁrst layer of capsules, the total input to a
capsule j is a weighted sum over all the prediction vectors
ˆuj|i from the capsules in the layer below, and is calculated
by multiplying the output ui of a capsule in the layer below
by a weight matrix Wij as following

(3)

sj =

cij ˆuj|i,

ˆuj|i = Wijui,

(4)

(cid:88)

i

where cij is the coupling coefﬁcient that is determined by
the iterative dynamic routing process. The coupling coefﬁ-
cients between capsule i and all the other capsules in the
layer above sum to 1. They are determined by a routing
softmax whose initial logits bij are the log prior probabilities
that capsule i should be coupled to capsule j. The cij can be
calculated as following

cij =

exp(bik)
k exp(bik)

.

(cid:80)

(5)

Fig. 3. Illustration of blocks of word sequences. The red word refers
to the central word for each subgraph. The words in any block are the
contexts of the central word at different locations in a document.

among any one block, each word shares the same attention
parameter. For example, in Figure 3, we assume they are
converted from the top 3 subgraphs. In the 1-th block of
the 3-th subgraph, the words electric, car, company, plan, and
purchase share the same masked attention parameter αB3,2,1 .
Then, after the ﬁrst attentional recurrent convolution layer,
there is an N × (T − 2) × k1 size of feature map. Compared
with traditional convolution and recurrent networks on text
data [8], [12], [10], [11], [28], [7], the signiﬁcant difference
of our designed attentional recurrent CNN units is that it
can integrate the long-distance, non-consecutive and local
sequential semantics of the corresponding sub-graph G(v).
The second attentional recurrent convolution layer takes
the output of the ﬁrst attentional recurrent convolution as
its input, and ﬁlters it with k2 kernels of size 1 × 3 × k1
with a horizontal stride of 2 elements and a vertical stride of
1 element, which are illustrated with the black convolution
slide direction arrow and red recurrent slide direction arrow
in Figure 2. We still guarantee that each horizontal fea-
ture map characterizes the semantics of corresponding sub-
graph G(v), and the attentional recurrent and convolution
operators between different sub-graphs are independent.
In the second layer, for each sub-graph G(v), the number
of attentional parameter is same with the ﬁrst layer, but
they are separated in training. After the second attentional
recurrent convolution layer’s operation, a N × (T − 4) × k2
size of feature map is generated.

More formally, we we give the deﬁnitions of convolu-
tion operator and Attentional-LSTM unit, respectively. The
convolution operator can be deﬁned as

xl
j = f (

xl−1
i

· kl

ij + bl

j),

(cid:88)

i∈Mj

(1)

where xl
j represents the j-th feature map of the l-th layer
of the convolution network, and l ∈ {1, 2}. This formula
shows the convolution operation and the summation for all
the associated feature maps xl−l
and the j-th convolution
ij of layer l, and then add an offset parameter bl
kernel kl
j.
Finally, a ReLU activation function f is applied. Meanwhile,

i

The ﬁnal DigitCaps layer has n capsules per digit class
and each of these capsules receives input from all the
other capsules in the layer below. Wij is a weight matrix
between each ui, i ∈ (1, M × N ) in primary capsules and
vj, j ∈ (1, L), where L refers to the number of classes.

As the length of the capsule’s output vector represents
the presence of a class, the length (cid:107)vk(cid:107) of each capsule
in the ﬁnal layer can then be viewed as the probability
of the text belonging to a particular class k. The length
of the activity vector of each capsule in DigitCaps layer is
used to calculate the classiﬁcation loss. This encourages the
network to learn a more general representation of text with
classiﬁcation task. Different from the capsule networks [30],
[31], [32] applied in the ﬁeld of computational vision, we
consider the distance between the raw text and the output
of the reconstructed representation in the word embedding
space to be relatively large in practice. We do not perform
text reconstruction during training. Next, we introduce how
to design a weighted margin loss to measure distance of
classes in hierarchy and guide the training of Attentional
Capsule Recurrent CNN model.

4 HIERARCHICAL TAXONOMY-AWARE WEIGHTED
MARGIN LOSS

Intuitively, the distances between any two classes on the
hierarchy are different, but popular margin loss in capsule
network [30] and other distance measures in multi-label
learning [33] between classes didn’t consider the hierar-
chical relations among labels. So, we explore a hierarchical
taxonomy-aware weighted margin loss to guide the training
of the proposed model in Section 3.

For more formally, we denote the hierarchical taxonomy
structure of the labels as HG = {V, E}, where vertices V are
classes S and the directed edges E represent the hierarchical
parent-child relationship among the labels. In large-scale
multi-label text classiﬁcation, for a document ds and the
corresponding positive labels set Ts, the number of labels
in Ts is usually much smaller than the remaining negative
ones in S. Therefore, it will lead to a large loss of the
objective function. In fact, in hierarchical label network, the
closer the edge relationship between nodes, the closer the
semantic distance between labels. In order to conveniently
capture the relationship between labels on the hierarchical
label structure, we design two meta-paths to guide random
walk on the label structure, and generate label sequences to
learn label representation.

Figure 4 illustrates the hierarchical taxonomy structure
of the labels, where each node refers to a label/class, and
each directed edge represents a parent-child relationship.
Note that the taxonomy network is not a strict hierarchical
structure, and may contain cycles. For example, the two
hierarchical relations of “Economic”-“International
trade”-
“Arms sales” and “Economic”-“Defense economy”-“Arms sales”
can form a cycle. Both hierarchical structure of taxonomy
and graph structure of taxonomy are common in practice,
and can be modeled as a hierarchical graph-of-labels. For
conveniently calculating the distance between any two la-
bels, we measure the discrete cosine distance through their
representation vectors. As shown in Figure 4, we extract
the following two types of meta-paths from the hierarchical

6

Fig. 4. Illustrations of the hierarchical taxonomy of the labels and the two
meta-paths. Vertices represent classes, and edges represent hierarchi-
cal parent-child relations.

graph-of-labels, “Child1 - Father0 - Child2” and “Father1 -
Child0 - Father2”. Actually, the two types of meta-paths
control the directions of the random walk to the upper
and lower layers, respectively. So, we perform meta-paths
guided random walk to generate sequences of labels. Here,
we set that the probability of selecting two meta-paths is
equal during random walking. Similar to metapath2vec [34]
and Deepwalk [35], we also use the skip-gram with negative
sampling
[26], [27] to encode the relations among the
labels/classes into a continuous vector space. We optimize
the following objective function, which maximizes the log-
probability of observing a network neighborhood NS(l) for
a node l conditioned on its feature representation, given by
g :

max
g

(cid:88)

l∈V

[− log Zl +

f (ni) · f (l)],

(6)

(cid:88)

ni∈NS (l)

1
where g(x) =
1+exp(−x) is the sigmoid function. Since it will
be time consuming to compute Zl = (cid:80)
nl∈V exp(g(l)·g(nl))
for large network, we approximate it using the negative
sampling technology. We optimize the Eq. 6 by using
stochastic gradient method.

Thus, given a tag label/class l ∈ V, we can approximate
a semantic distance between any other label li, i ∈ [1, L]
by calculating their discrete cosine distance between their
embedding vectors as following:

d(l, li) = 1 − cos(vec(l), vec(li)).

(7)

Next, in order to take advantage of dependencies among
labels to guide the training of the proposed attentional
capsule recurrent CNN model, we design a hierarchical
taxonomy-aware weighted margin loss objective function:

L =

[Tk max(0, m+ − (cid:107)vk(cid:107))2

L
(cid:88)

k=1

(8)

+ λ · p · αk · (1 − Tk) · max(0, (cid:107)vk(cid:107) − m−)2],

where Tk = 1 if and only if a digit of class k is present,
and m+ and m− are the given thresholds for the upper and
lower bounds. The λ down-weighting of the loss for absent
digit classes stops the initial learning from shrinking the

lengths of the activity vectors of all the digit capsules, such
as 0.5 in the original capsule networks [30], [31]. αk ∈ [0, 1]
is the minimum semantic distance from negative label k to
the positive labels set in the hierarchical label network. The
total loss is the sum of the losses of all the digit capsules.
Formally, for a document ds, the positive label set is Ts ⊂ S.
And for any negative label k, the αk is:

αk = 1 − max
t∈Ts

(cos(vec(t), vec(k))).

(9)

Meanwhile, to make an unbiased and smooth overall ob-
jective function after integrating the weight, we add an
adjustment factor p that satisﬁes (cid:80)L
k=1 p · αk = 1. We can
approximate the distribution of the semantic distance αk
by 1 − e−x, x ∈ [1, L] to obtain an approximation of the
adjustment factor p for different datasets.

5 EXPERIMENTS

In this section, we conduct experiments to evaluate the
performance of the proposed framework. We will ﬁrst intro-
duce the used datasets, the evaluation metrics, methods for
comparison, and experimental settings. Then, we will com-
pare our methods with baselines, and provide the analysis
and discussions on the results.

where:

5.1 Datasets

We use two datasets RCV1 and EUR-Lex for large-scale
multi-label text classiﬁcation, and use the Reuters-21578 to
evaluate the effectiveness of our proposed capsule network
in transferring from single-label to multi-label classiﬁcation.
The statistics of the datasets is shown in Table 1.

• Reuters Corpus Volume I (RCV1) [36] is a manually la-
beled newswire collection of Reuters News from 1996-1997.
It consists of over 800,000 manually categorized newswire
stories by Reuters Ltd for research purposes. Multiple topics
can be assigned to each newswire story and there are 103
topics in total. The news documents are categorized with
respect to three controlled vocabularies: industries, topics
and regions. The relations among the labels are typically
graphic structure with self-loops. We use the topic-based hi-
erarchical classiﬁcation because it has been widely adopted
in evaluation.
• EUR-Lex

[37] is a collection of documents about
European Union law. It contains many different types of
documents, including treaties, legislation, case-law and leg-
islative proposals, which are indexed according to several
orthogonal categorization schemes to allow for multiple
search facilities. The most important categorization is pro-
vided by the EUROVOC descriptors, which forms a topic
hierarchy with almost 4000 categories regarding different
aspects of European law. Directory code classes are orga-
nized in a hierarchy of 4 levels with a typical tree structure.
Since the dataset contains several European languages, we
choose English version of documents.

• Reuters-21578 [38], [13] is a collection appeared on the
Reuters newswire in 1987. We follow [13] 2 to choose 6,700
documents from the Reuters ﬁnancial newswire service,
where each document contains either multiple labels or

2. https://github.com/andyweizhao/capsule text classiﬁcation

7

a single label. And we also focus 10 popular topics, and
reprocess the corpus to evaluate the capability of capsule
networks of transferring from single-label to multi-label text
classiﬁcation. For development and training, we only use
the single-label documents in the development and training
sets. For testing, we only uses the multi-label documents
in testing dataset. Note that this dataset is only for test-
ing the advantages of the transferring from single-label to
multi-label classiﬁcation task of our capsule network that
incorporates multiple semantics.

5.2 Evaluation Metrics and Baselines

We use the standard evaluation metrics [39] to measure the
performance of all the methods.

• Micro-F1 is a metric considering the overall precision
and recall of all the labels. Let T Pt, F Pt, F Nt denote the
true-positives, false-positives and false-negatives for the t-
th label in label set S respectively. The M icro-F1 is deﬁned
as:

M icro-F1 =

2P R
P + R

,

(10)

P recision(P ) =

Recall(R) =

(cid:80)

(cid:80)

(cid:80)
(cid:80)

t∈S T Pt
t∈S T Pt + F Pt
t∈S T Pt
t∈S T Pt + F Nt

.

,

• Macro-F1 is a metric which evaluates the averaged
F1 of all the different class-labels. Different from Micro-
F1 that gives equal weight to all the instances, M acro-F1
gives equal weight to each label in the averaging process.
Formally, M acro-F1 is deﬁned as:

M acro − F1 =

1
|S|

(cid:88)

t∈S

2PtRt
Pt + Rt

, where

Pt =

T Pt
T Pt + F Pt

,

Rt =

T Pt
T Pt + F Nt

,

(11)

Meanwhile, we compare our model with both traditional
text classiﬁcation methods and recent state-of-the-art deep
learning based methods.

• Flat baselines. This type of methods generally ﬁrst
extract the TF-IDF features from the document, and then
input them into the classiﬁcation model such as Logistic
Regression (LR) and Support Vector Machines (SVM). We
call them ﬂat baselines since they ignore both the relations
among the words and the relations among the labels, and
simply train a multi-class classiﬁer.

• N-gram, sequence-of-words or graph-of-words based
models. These methods extract N-gram features, sequence-
of-words or graph-of-words from the document as the in-
put of classiﬁcation models. These features are suitable for
deep learning models, such as CNN-non-static [8], RCNN
[28], Deep CNN [12], XML-CNN [10], DGCNN-3 [11],
Hierarchical LSTM (HLSTM)
[40], Hierarchical Attention
Network [6] (HAN) and Bi-directional Block Self-Attention
Network [7] (Bi-BloSAN) etc. For example, HLSTM model
learns sentence representations based on words sequences,
and then use RNN models to encode document repre-
sentations based on the learned sentence representations.

TABLE 1
Statistics of the three datasets. Training, Development, Testing, and Class-Labels denote the total number of training, testing samples and labels,
respectively. Words/Sample is the average number of words per sample. Labels/Sample is the average number of labels per sample, and
Sample/Labels is the average number of documents per label.
Class-Labels Depth Words/Sample

Datasets
RCV1
EUR-Lex
Reuters-21578

Training Development
23,149
15,449
5800

-
-
600

Testing
784,446
3,865
300

103
3,956
10

6
4
-

268.95
1229.77
257.32

Labels/Sample
3.24
5.32
-

Samples/Label
729.67
15.59
-

8

TABLE 2
Comparison of main functions among variations of HE-AGCRCNN.

Sorting

LSTM Attentional LSTM Capsule Weighted Margin Loss

Models
TGCNN(No-R)
TGCNN
TGRCNN
GCCNN
TAGRCNN
GCRCNN
AGCRCNN
HE-TGCNN
HE-TGRCNN
HE-GCCNN
HE-TAGRCNN
HE-GCRCNN
HE-AGCRCNN

CNNs
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

HAN uses a global attention mechanism to attend useful
words and sentences. Bi-BloSAN further splits the sequence
into several blocks and employs intra-block and inter-block
self-attentions to capture both local and long-range context
dependencies, respectively.

• Hierarchical models. These methods make use of
the hierarchical or graphical label network to design hi-
erarchical classiﬁcation classiﬁers, such as Top-down Sup-
port Vector Machines (TD-SVM)
[41], Hierarchical SVMs
[42], Hierarchically Regularized Logistic Regression (HR-
LR), Hierarchically Regularized Support Vector Machines
(HR-SVM) [24], [23], and Hierarchically Regularized Deep
Graph CNN (HR-DGCNN-3) [11], etc.

• Sequence generation model. These methods view
the multi-label classiﬁcation task as a sequence generation
problem, and apply a sequence generation model, such as
SGM+GE [43], with decoder structure to solve it.

• Capsule Neural Networks. These methods are
shown to be effective in capturing the spatial features of
text, including Capsule Networks with Dynamic Routing
(Capsule-A and Capsule-B)
[13]. These capsule networks
rely on N-gram convolution networks to extract shallow
features and then use dynamic or static routing to learn the
relationships between features. Capsule-B model employs
parallel networks with different sizes of ﬁlters. It has been
proven to have a better effect than the Capsule-A.

• Variations of HE-AGCRCNN. We implement the
following several variants of our proposed method. Three
layers of Graph CNN (TGCNN(No-R)): no-sorting nor-
malization process and without the hierarchical weighted
margin loss, attentional LSTM units and capsule network;
Three layers of Graph CNN (TGCNN): without the hi-
erarchical weighted margin loss, attentional LSTM units
and capsule network; Three layers of Graph Recurrent
CNN (TGRCNN): without the hierarchical weighted mar-
gin loss, attention units and capsule network; Graph Cap-
sule CNN (GCCNN): without the hierarchical weighted
margin loss and attention LSTM units; Three layers of
Attentional Graph Recurrent CNN (TAGRCNN): without

the hierarchical weighted margin loss and capsule net-
work; Graph Capsule Recurrent CNN (GCRCNN): with-
out the hierarchical weighted margin loss and attention
units; Three layers of Attentional Graph Capsule Recur-
rent CNN (AGCRCNN: without the hierarchical weighted
margin loss), and the hierarchical weighted margin loss
based models (HE-TGCNN, HE-TGRCNN, HE-GCCNN,
HE-TAGRCNN and HE-GCRCNN). All these models have
3 layers of convolutional layers. In order to clearly present
the advantages of the variations of HE-AGCRCNN, we give
a table 2 of models that enhance functionality.

For all the baselines, we use the implementations or open
source codes of these models released by authors and other
researchers, and report the best performance of the results
in our experiments.

5.3 Experimental Settings

All our experiments were performed on 64 core Intel
Xeon CPU E5-2680 v4@2.40GHz with 512GB RAM and
8×NVIDIA Tesla P100-PICE GPUs. The operating system
and software platforms are Ubuntu 5.4.0, Python 3.5.2, and
Pytorch 0.4.0. The training and testing datasets are shown
in Table 1. In document modeling, the top N numbers of
central words are set to 100 (RCV1 and Reuters-21578) and
200 (EUR-Lex). For the sub-graph, the upper bound value K
(the maximum number of vertices) is set to 25. The length T
of the normalized word sequence is set to 20. The dimension
D of word embedding is set to 50. Here, we use word2vec
technology to train 50 dimensional word embedding over
the 100 billion words from Wikipedia corpus based on Skip-
gram with Negative Sample model with window size of 5.
For the hierarchical taxonomy embedding, we employ 50
threads to execute the random walk in parallel, and for each
walk we use 500 steps. The dimension of label embedding
vector is set to 200. For all the deep learning based models,
the common parameters of training the models are empiri-
cally set, such as batch size = 32 and learning rate = 0.001
with Adam optimization algorithm.

9

Fig. 5. The performance comparisons of reconstructing label network by the two hierarchical taxonomy embedding methods on various thresholds.

(a) RCV1

(b) EUR-Lex

For the non-capsule neural network models, such
as TGCNN(No-R), TGCNN, TGRCNN, TAGRCNN, HE-
TGCNN, HE-TGRCNN and HE-TAGRCNN, we use a 2-
layer fully connected networks and a sigmoid layer, and
the popular cross entropy or the hierarchical taxonomy em-
bedding based weighted margin loss as objective function.
For the capsule network models, we use the the original
margin loss or hierarchical taxonomy embedding based
weighted margin loss as the objective function. For capsule
based models, the dimension of capsule vector m is 16, the
channel of convolution capsule M is 64, the dimensions of
DigitCaps are 32 × 103 for the RCV1 dataset, 64 × 3956
for the EUR-Lex, and 32 × 10 for the reprocessed Reuters-
21578, and m+ = 0.9, m− = 0.1, λ = 0.5. Considering
the number of the class labels and the average number of
labels per sample, we set the adjustment factor p to 0.01
for RCV1, 0.001 for EUR-Lex and 0.1 for the reprocessed
Reuters-21578 in the Eq. 8. All convolution kernels are 1 × 3
in size. The numbers of convolution kernels per layer are
64 and 128. The LSTM operator contains 128 hidden layer
units. The numbers of neurons in the fully connected layers
are 1024 and 512 in RCV1, and 2048 and 4096 in EUR-
Lex. Our models can achieve the best performance results
among 20 to 70 epoches. For the experiment of transferring
single-label model to multi-label classiﬁcation, on the one
hand, in order to be consistent with baseline [13], we select
the same number of training, testing, and validation data
as shown in Table 1. On the other hand, as the labels of
the reprocessed Reuters-21578 is part of RCV1’s, we reuse
the vector representation of the RCV1 label of our the
hierarchical taxonomy-aware weighted margin loss in the
transferring experiment.

5.4 Evaluation on Label Embedding

In order to study whether the proposed meta-paths based
random walk can learn desirable label embedding that
reﬂects the hierarchical taxonomy relations among them,
we use the meta-paths guided random walk and traditional
random walk to generate the two label sequences respec-
tively, and then generate two label vectors by the same skip-
gram method. After obtaining the two vectors, we calculate
the cosine distance between them, and use it to reconstruct

the relations among the labels. When the distance between
two label vectors is larger than the threshold, we add a
edge between the two labels. We employ the Macro-F1 and
Micro-F1 to evaluate the performance of reconstructing the
relations in hierarchy.

Figure 5(a) and ﬁgure 5(b) show the results of the two
label embedding vectors on the relation reconstruction task
in the two datasets. One can see that overall the meta-
paths guided random walk approach performs better for
capturing the hierarchical taxonomy semantics than the
traditional random walk approach. For RCV1, the most suit-
able thresholds of meta-paths based taxonomy embedding
are 0.660 and 0.940, and the highest Macro-F1 and Micro-F1
are 0.337 and 0.310, respectively. For the traditional random
walk based taxonomy embedding, the highest Macro-F1
and Micro-F1 are 0.275 and 0.272, respectively. For EUR-
Lex, the most suitable thresholds of meta-paths based taxon-
omy embedding are 0.440 and 0.880, and the highest Macro-
F1 and Micro-F1 are 0.168 and 0.406, respectively. For the
traditional random walk based taxonomy embedding, the
highest Macro-F1 and Micro-F1 are 0.164 and 0.405, re-
spectively. One can observe that the performance difference
between the two random walk methods is relatively small
on EUR-Lex. This is probably because the taxonomy label
structure of EUR-Lex is a hierarchical tree. Although the
method of measuring the discrete cosine distance between
any two labels based on unsupervised heterogeneous net-
work representation learning vector is approximate, it’s a
convenient method to estimate label distance for any two
labels.

5.5 Performance Evaluation on RCV1

Next, we evaluate the performance on the RCV1 dataset
through the multi-label text classiﬁcation task. RCV1 is a
dataset that training samples are much fewer than testing
samples, as shown in Table 1.

The experiment results are shown in Table 3. Among the
traditional text classiﬁcation algorithms, one can see that
the HR-SVM performs better than TD-SVM, HSVM, SVM,
HR-LR and LR. For deep learning approaches, one can see
that the performance of RNN based algorithms HLSTM and
HAN are comparable to SVM and LR. RCNN performs

worse on both settings. For ﬁne-grained topical classiﬁca-
tion, the above recurrent models may not have advantages
because it compresses the whole document as a dense vector
for classiﬁcation. The RNN models are more suitable to
sentiment classiﬁcation for short text, but is not suitable to
learn features for long document [11]. For CNN models, it
is shown that XML-CNN does not perform very well on
RCV1. However, the deeper model DCNN improves the
performance by 9% in terms of Macro-F1 and 4% in terms of
Micro-F1. For capsule network, one can see that the Capsule-
B achieves comparable performance with DCNN model.
For sequence generation model, the SGM+GE improves the
performance by 2% in terms of Macro-F1 and Micro-F1 com-
pared with the XML-CNN model. For GCNN models, both
DGCNN-3 and HR-DGCNN-3 improve the performance by
4% in terms of Macro-F1 and 3% in terms of Micro-F1
compared with Capsule-B. It demonstrates that graph-of-
words representation is effective in modeling documents in
multi-label text classiﬁcation. For the popular bi-directional
block self-attention network, the Bi-BloSAN improves the
performance by 8% in terms of Macro-F1 and 3% in terms
of Micro-F1 compared with the HAN model.

For the proposed models, we try different model con-
ﬁgurations listed in Table
2. The results are shown in
Table 3. One can see that the LSTM units, attentional LSTM
units, capsule networks and hierarchical taxonomy-aware
weighted margin loss are all helpful to improve the classi-
ﬁcation performance. The proposed HE-AGCRCNN model
outperforms the HR-DGCNN-3 by 8% in terms of Macro-
F1. The simpliﬁed model TGCNN(No-R) also achieves com-
parable Macro-F1 and Micro-F1 with DGCNN-3. Mean-
while, without attentional LSTM units, capsule network and
hierarchical label dependencies, the TGCNN also outper-
forms most of the baselines. Based on the arranged words-
matrix representation, LSTM units and attentional LSTM
units, the TGRCNN and TAGRCNN models achieve 5%-6%
improvements in terms of Macro-F1 over HR-DGCNN-3.
This improvements show the importance of local sequential
semantics for text features. Among the proposed models,
one can see that capsule networks averagely achieve 1%
gain in both Macro-F1 and Micro-F1. Overall, the hier-
archical taxonomy-aware weighted margin loss can also
improve the performances by 2% in terms of Macro-F1
and 1% in terms of Micro-F1. Finally, the proposed HE-
AGCRCNN model achieves the highest 0.513 Macro-F1 and
0.778 Micro-F1 performance. The results of the different
document modeling methods show that by representing
document as arranged words-matrix, the proposed model
can gain performance improvement for multi-label text
classiﬁcation in RCV1. One can also see that HR-SVM,
HR-DGCNN-3 and HE-AGCRCNN represent two different
ways of using the hierarchical label dependencies, and both
improve the classiﬁcation performance over RCV1 dataset.
We will present the timeliness analysis of the experiment in
section 5.8.

5.6 Performance Evaluation on EUR-Lex

As the number of labels in EUR-Lex is large, we use more
neurons in the fully connected layers and set a larger dimen-
sion of capsule vector in the DigitCaps layer, as presented in

TABLE 3
Comparison of results on RCV1 and EUR-Lex.

RCV1

EUR-Lex

Marco-F1 Micro-F1 Marco-F1 Micro-F1

10

Models

LR
SVM
HSVM
TD-SVM
HR-LR
HR-SVM
HLSTM
HAN
RCNN
XML-CNN
DCNN
DGCNN-3
HR-DGCNN-3
SGM+GE
Bi-BloSAN
Capsule-B
TGCNN(No-R)
TGCNN
GCCNN
TGRCNN
TAGRCNN
GCRCNN
AGCRCNN
HE-TGCNN
HE-GCCNN
HE-TGRCNN
HE-TAGRCNN
HE-GCRCNN
HE-AGCRCNN

0.328
0.330
0.333
0.337
0.322
0.386
0.310
0.327
0.293
0.301
0.399
0.432
0.433
0.348
0.401
0.399
0.443
0.472
0.480
0.484
0.490
0.488
0.494
0.482
0.491
0.495
0.504
0.505
0.513

0.692
0.691
0.693
0.696
0.716
0.728
0.673
0.696
0.686
0.695
0.732
0.761
0.762
0.719
0.720
0.739
0.745
0.747
0.749
0.754
0.759
0.765
0.769
0.751
0.754
0.762
0.773
0.772
0.778

0.181
0.185
0.189
0.198
0.180
0.223
0.183
0.184
0.168
0.179
0.231
0.237
0.241
0.216
0.219
0.226
0.244
0.257
0.261
0.265
0.270
0.275
0.283
0.283
0.290
0.292
0.298
0.297
0.330

0.522
0.551
0.567
0.571
0.583
0.609
0.562
0.566
0.554
0.583
0.611
0.632
0.649
0.628
0.619
0.600
0.648
0.655
0.658
0.667
0.673
0.668
0.675
0.683
0.688
0.680
0.685
0.684
0.688

Section 5.3. For the proposed models, we also try different
conﬁgurations, and the results are shown in Table 3.

From the results one can see that LSTM units, attentional
LSTM units, capsule networks and hierarchical taxonomy-
aware weighted margin loss are all helpful to improve
classiﬁcation performance on the EUR-Lex dataset. HE-
AGCRCNN model achieves about 6% improvements in
terms of Macro-F1 and 4% gains in terms of Micro-F1
over the HR-DGCNN-3 model. Without using hierarchical
label dependencies, LSTM units, attentional LSTM units
and capsule networks, the TGCNN also performs better
than HR-DGCNN-3, and the results are 0.257 and 0.655 for
Macro-F1 and Micro-F1, respectively. When we do not order
the words in the sub-graph, the results of TGCNN(No-R) are
0.244 and 0.648 for Macro-F1 and Micro-F1, respectively.
The performance gap between TGCNN(No-R) and TGCNN
shows the importance of local sequential semantics for text
classiﬁcation with the same three layers of Graph CNN
models. Based on the arranged words-matrix representa-
tion, the LSTM units can help to improve 1% performance
comparing GCCNN and GCRCNN. The performance gap
between TGRCNN and TAGRCNN shows that the masked
attentional units can help to improve about 0.5% perfor-
mance. The hierarchical taxonomy-aware weighted margin
loss also helps to improve about 1%-3% performances in
terms of Macro-F1 or Micro-F1. Compared with the im-
provements of the hierarchical taxonomy-aware weighted
margin loss in the RCV1 dataset, the improvements in the
EUR-Lex dataset are greater by using the same weighted
margin loss. Finally, HE-AGCRCNN model achieves 0.330
Macro-F1 and 0.688 Micro-F1, which are both the highest
performance. The experimental results again demonstrate
that by representing document as an arranged words-matrix

TABLE 4
Comparison of the transferring capacity from single-label to multi-label
text classiﬁcation on the reprocessed Reuters-21578 dataset.
P
Models
0.867
LSTM
0.823
BiLSTM
0.920
CNN-non-static
0.882
Capsule-A
0.954
Capsule-B
0.962
GCCNN
0.970
GCRCNN
0.973
AGCRCNN
0.965
HE-GCCNN
HE-GCRCNN
0.974
HE-AGCRCNN 0.978

Micro-F1
0.635
0.643
0.704
0.820
0.858
0.905
0.917
0.921
0.910
0.924
0.927

R
0.547
0.559
0.597
0.801
0.820
0.856
0.871
0.875
0.862
0.879
0.882

and incorporating the proposed deep models, one can gain
beneﬁts from non-consecutive, long-distance and sequential
semantics for topical multi-label text classiﬁcation.

5.7 Performance Evaluation on Reuters-21578

A signiﬁcant advantage of capsule network is that it per-
forms much better in the transferring single-label to multi-
label classiﬁcation task [13]. Different from traditional deep
learning classiﬁcation models that are based on fully con-
nected network, capsule networks use activity vectors of
each capsule in DigitCaps layer to indicate the presence of
an instance of each class. We also perform the model transfer
capacity experiment of the proposed capsule network on the
reprocessed Reuters-21578 dataset [13].

The comparison results are shown in Table 4. The base-
line results are also reported from the work [13], and HE-
AGCRCNN outperforms all the baselines. Compared with
capsule-based models, the performances of LSTM, BiLSTM
and CNN-non-static are the worst. From the results one can
see that GCCNN, GCRCNN, AGCRCNN, HE-GCCNN, HE-
GCRCNN and HE-AGCRCNN models all have achieved
about 5%-7% improvements in terms of Micro-F1 over
the existing best baseline Capsule-B. Even without the at-
tentional LSTM units, the simpliﬁed GCCNN can achieve
0.905 performance in terms of Micro-F1. Compared with
the popular Capsule-A and Capsule-B models, our pro-
posed GCCNN, GCRCNN, AGCRCNN, HE-GCCNN, HE-
GCRCNN and HE-AGCRCNN models integrate more non-
consecutive, long-distance and local sequential semantics,
and make use of the hierarchical label dependencies. Finally,
the proposed HE-AGCRCNN model achieves the highest
0.927 performance in terms of Micro-F1. The experimental
improvements again prove the effectiveness of our proposed
capsule models in learning rich textual features.

5.8 Training Efﬁciency Evaluation

To evaluate the training efﬁciency of the model, we next
show the training time of the proposed model and its
variants on both RCV1 and EUR-LEX datasets in Table 5.
Here, for the RCV1, since the number of samples in the test
set is about 34 times larger than the number of samples in
the training set, as shown in Table 1, we perform the testing
by using multi-core CPUs.

One can observe that most of these models can quickly
achieve a promising classiﬁcation result with less than 3
hours except for the models with the LSTM or capsule

11

TABLE 5
Comparison of training time on GPUs.
Models
TGCNN
GCCNN
TGRCNN
TAGRCNN
GCRCNN
AGCRCNN
HE-TGCNN
HE-GCCNN
HE-TGRCNN
HE-TAGRCNN
HE-GCRCNN
HE-AGCRCNN

RCV1(hr.)
0.166
0.537
0.381
0.382
1.116
1.117
0.167
0.542
0.385
0.386
1.118
1.119

EUR-Lex(hr.)
1.100
3.415
2.579
2.580
6.327
6.328
1.167
3.421
2.583
2.584
6.334
6.335

unites. For example, the TGCNN and HE-TGCNN models
converge quickly with less than 0.2 hour on RCV1 dataset
and less than 1.2 hours on EUR-Lex dataset. Meanwhile, the
training time on RCV1 dataset is much less than EUR-Lex.
This is mainly because the EUR-Lex dataset has a larger
document representation and more parameters, according
to Table
1. We also verify that the models integrating
more feature extraction operators, such as LSTM units,
attentional LSTM units and capsule networks, will take
longer time to train for achieving a desirable classiﬁcation
performance. Although HE-AGCRCNN model takes 1.119
hours and 6.335 hours to train for RCV1 and EUR-Lex
datasets, respectively, it achieves the highest classiﬁcation
performance. One can also see that the hierarchical tax-
onomy embedding based weighted margin loss does not
add much computational time compared with the recursive
regularized optimization models [11], [24], [23]. Usually, the
time consumptions of the above recursive regularization
based models are expensive for the large number of pa-
rameters and constraints on the Euclidean distance of the
parameters. In particular, the time consumptions of recur-
sive regularization optimized deep learning model, such as
HR-DGCNN-3, is generally measured in days [11].

5.9 Case study

To gain a closer view of what’s the two attention layers
and output capsules in a document captured by our models,
we visualize parts of the attention probability or alignment
score by heatmaps in Figure 6. The red words are central
words in the document, each block of word sequence is con-
text of central words. For each central word, there are two
layers of masked attention. We choose the blocks of word
sequences from the 1-th, 2-th and 3-th central words. One
can see that the weights of the upper left parts are higher
than other places. This is probably because the contextual
semantics of the front central words are more representative
of the subject of the article. For the output capsule vectors,
there are 4 vectors whose modulus length is greater than 0.9,
corresponding to the category of output. Note that Sports
category comes out due to the words game, match, defeat
and win, although chess is not really a physical activity
sport.

6 RELATED WORK

As our work is closely related to text classiﬁcation, textual
deep learning models and graph convolution networks, in

12

Fig. 6. The attention visualizations and output capsule vectors for the 3093newsML sample in RCV1. The left one is parts of attention visualizations
on blocks of word sequences, and the right one is the length of output capsule vectors.

this section we will review related works from the three
aspects.

Tradition text classiﬁcation models use feature engineer-
ing and feature selection to obtain features for text classi-
ﬁcation [1]. For example, Latent Dirichlet Allocation [44]
has been widely used to extract topics from corpus, and
then represent documents in the topic space. It performs
better than Bag-Of-Word (BOW) when the feature numbers
are small. However, when the size of words in vocabulary
increases, it does not show advantage over BOW on text
classiﬁcation [44]. There are also some existing work that
tried to convert texts to graphs [45]. Similar to our proposed
methods, they used word co-occurrence to construct graphs
from texts, and then they applied similarity measure on
graph to deﬁne new document similarity and features for
[45]. For hierarchical large-scale multi-label text clas-
text
siﬁcation, many efforts have been put on how to leverage
the hierarchy of labels to improve the classiﬁcation results.
Recently, a recursive regularization of weight euclidean
constraint with classiﬁers has been developed, and shown
to be the out-performance in large-scale hierarchical text
classiﬁcation problems [24], [23].

For deep learning models, there have been RNNs, CNNs,
and capsule models applied to text classiﬁcation. For ex-
ample, hierarchical RNN has been proposed for long doc-
ument classiﬁcation [16] and later attention model is also
introduced to emphasize important sentences and words [6].
Similar to RNNs, the recently proposed self-attention based
sentence embedding technologies [46], [17], [7] have shown
effectively capturing both long-range and local dependen-
cies in sentiment-level tasks. For example, Bi-BloSAN [7]
is a bi-directional block self-attention network to learn text
representation and models text as sequences. For CNNs
models, Kalchbrenner et al.
[47] and Kim et al. [8] used
simpler CNN for text classiﬁcation, and showed signiﬁcant
improvements over traditional texts classiﬁcation methods.
Zhang et al. [48] and Conneau et al. [12] used a character
level CNN with very deep architecture to compete with tra-
ditional BOW or n-gram models. The combination of CNNs
and RNNs are also developed which shows improvements
over topical and sentiment classiﬁcation problems
[28].
Capsule networks were proposed by Hinton et. [49], [30],
[31] as a kind of supervised representation learning meth-
ods, in which groups of neurons are called capsules. Capsule
network has been proved effective in learning the intrinsic
[13]
spatial relationship between features [13], [29], [50].
showed that Capsule networks can help to improve low-

data and label transfer learning. However, as mentioned
in the introduction, existing textual deep learning models
are not compatible with diverse text semantic coherently
learning. Compared with our work, these previous studies
only considered N-gram or sequential text modeling, but
ignored high level of non-consecutive and long-distance
semantics of text. They did not study and utilize the depen-
dency among the the labels, either. Although there are prior
works [24], [11], [25], [51] on modeling pair-wise relation
between labels for multi-label classiﬁcation, they fail to
consider their hierarchical relations, and the computation of
the above models is expensive due to the use of euclidean
constraints in their regularization.

GCN derived from graph signal processing [52], [53],
and the graph convolution operation has been recognized as
the problem of learning ﬁlter parameters that were replaced
by a self-loop graph adjacency matrix, updating network
weights, and extended by utilizing fast localized spectral
ﬁlers and efﬁcient pooling operations in [54], [55], [56].
With the development of GCN technologies, graphs embed-
ding approaches, such as PSCN [57] and GCAPS-CNN [58],
have been developed in graph classiﬁcation tasks. Recently,
the recursively regularized deep graph-cnn [11] has been
proposed to combine graph-of-words representation, graph
label dependency for large-scale
CNN, and hierarchical
text classiﬁcation. Then the Text GCN model [9] has been
proposed to capture global word co-occurrence information
and perform text classiﬁcation without word embeddings
or other external knowledge. Although long-distance and
non-continuous text features are fully considered in the two
models, the existing graph convolutional neural network
models ignore the continuous and sequential semantics of
words in the text. In addition, the recursive regularization is
usually time consuming due to the euclidean constraint.

7 CONCLUSION AND FUTURE WORK
In this paper, we present a novel end-to-end hierarchi-
cal taxonomy-aware and attentional graph capsule recur-
rent CNN framework for large-scale multi-label text clas-
siﬁcation. We ﬁrst propose to convert each document as
an arranged words-matrix that preserves both the non-
consecutive, long-distance and local sequential semantics
for fully representing the document. Based on our docu-
ment modeling, we next propose a HE-AGCRCNN model
to coherently learn multiple types of textual features. In
order to better learn local sequential semantics, we design
a masked attentional LSTM to model the different impacts

among different blocks of word sequences, and enhance
the sequential features learning. To incorporate the hier-
archical relations among the labels, we further propose a
novel hierarchical taxonomy-aware weighted margin loss to
improve the performance of multi-label text classiﬁcation.
The advantageous performance of our proposed models
over other competing methods is evident as it obtained the
best results on all the RCV1 and EUR-Lex datasets in our
comparative evaluation. Compared to the N-gram based
textual capsule networks, we verify the effectiveness of our
proposed capsule models in learning rich textual features
in transferring single-label to multi-label classiﬁcation task.
The experimental results show the effectiveness and efﬁ-
ciency of our model in multi-label text classiﬁcation.

In the future, we plan to invest subgraph-level attention
capsule network, and upgrade our hierarchical taxonomy-
aware and attentional graph capsule recurrent CNN to self
attention rnn/cnn models [7], [17], variable-size convolution
kernels [59] and BERT [60] pre-trains based models, and
popularize to more sophisticated text classiﬁcation datasets
and applications.

REFERENCES

[1] C. C. Aggarwal and C. Zhai, “A survey of text classiﬁcation

algorithms,” in Mining Text Data, 2012, pp. 163–222.

[2] Y. Lecun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol.

[3]

521, no. 7553, p. 436, 2015.
I. Goodfellow, Y. Bengio, and A. Courville, Deep learning.
MIT Press, 2016.

The

[4] K. S. Tai, R. Socher, and C. D. Manning, “Improved semantic
representations from tree-structured long short-term memory net-
works,” in ACL, 2015, pp. 1556–1566.

[5] Y. Wang, M. Huang, X. Zhu, and L. Zhao, “Attention-based LSTM
for aspect-level sentiment classiﬁcation,” in EMNLP, 2016, pp.
606–615.

[6] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierar-
chical attention networks for document classiﬁcation,” in NAACL,
2017, pp. 1480–1489.

[7] T. Shen, T. Zhou, G. Long, J. Jiang, and C. Zhang, “Bi-directional
block self-attention for fast and memory-efﬁcient sequence mod-
eling,” in ICLR, 2018.

[8] Y. Kim, “Convolutional neural networks for sentence classiﬁca-

tion,” in EMNLP, 2014, pp. 1746–1751.

[9] L. Yao, C. Mao, and Y. Luo, “Graph convolutional networks for

text classiﬁcation,” CoRR, vol. abs/1809.05679, 2018.

[10] J. Liu, W. Chang, Y. Wu, and Y. Yang, “Deep learning for extreme
multi-label text classiﬁcation,” in SIGIR, 2017, pp. 115–124.
[11] H. Peng, J. Li, Y. He, Y. Liu, M. Bao, L. Wang, Y. Song, and Q. Yang,
“Large-scale hierarchical text classiﬁcation with recursively regu-
larized deep graph-cnn,” in WWW, 2018, pp. 1063–1072.

[12] A. Conneau, H. Schwenk, L. Barrault, and Y. Lecun, “Very deep
convolutional networks for text classiﬁcation,” pp. 1107–1116,
2016.

[13] M. Yang, W. Zhao, J. Ye, Z. Lei, Z. Zhao, and S. Zhang, “Investigat-
ing capsule networks with dynamic routing for text classiﬁcation,”
in EMNLP, 2018, pp. 3110–3119.

[14] P. Liu, X. Qiu, and X. Huang, “Adversarial multi-task learning for

text classiﬁcation,” in ACL, 2017, pp. 1–10.

[15] T. Miyato, A. M. Dai, and I. Goodfellow, “Adversarial training

methods for semi-supervised text classiﬁcation,” 2016.

[16] D. Tang, B. Qin, and T. Liu, “Document modeling with gated
recurrent neural network for sentiment classiﬁcation,” in EMNLP,
2015, pp. 1422–1432.

[17] T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan, and C. Zhang, “Disan:
Directional self-attention network for rnn/cnn-free language un-
derstanding,” in Thirty-Second AAAI Conference on Artiﬁcial Intelli-
gence, 2018.

[18] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural
probabilistic language model,” JMLR, vol. 3, no. Feb, pp. 1137–
1155, 2003.

13

tion,” in ICDM.

[19] A. Sun and E.-P. Lim, “Hierarchical text classiﬁcation and evalua-
IEEE, 2001, pp. 521–528.
[20] G.-R. Xue, D. Xing, Q. Yang, and Y. Yu, “Deep classiﬁcation
in large-scale text hierarchies,” in Proceedings of the 31st annual
international ACM SIGIR conference on Research and development in
information retrieval. ACM, 2008, pp. 619–626.

[21] S. Gopal, Y. Yang, B. Bai, and A. Niculescu-Mizil, “Bayesian
models for large-scale hierarchical classiﬁcation,” in NIPS, 2012,
pp. 2420–2428.

[22] M. W. Berry, Survey of Text Mining. Berlin, Heidelberg: Springer-

Verlag, 2003.

[23] S. Gopal and Y. Yang, “Hierarchical bayesian inference and recur-
sive regularization for large-scale classiﬁcation,” TKDD, pp. 18:1–
18:23, 2015.

[24] G. Siddharth and Y. Yiming, “Recursive regularization for large-
scale classiﬁcation with hierarchical and graphical dependencies,”
in KDD, 2013, pp. 257–265.

[25] S. Xie, X. Kong, J. Gao, W. Fan, and S. Y. Philip, “Multilabel
consensus classiﬁcation,” in 2013 IEEE 13th International Conference
on Data Mining.

IEEE, 2013, pp. 1241–1246.
[26] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their com-
positionality,” in NIPS, 2013, pp. 3111–3119.

[27] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation

of word representations in vector space,” Computer Science, 2013.

[28] S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent convolutional neural
networks for text classiﬁcation,” in AAAI, 2015, pp. 2267–2273.
[29] L. Xiao, H. Zhang, W. Chen, Y. Wang, and Y. Jin, “Mcapsnet:
Capsule network for text with multi-task learning,” in Proceedings
of the 2018 Conference on Empirical Methods in Natural Language
Processing, 2018, pp. 4565–4574.

[30] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between

capsules,” in NIPS, 2017, pp. 3859–3869.

[31] G. Hinton, N. Frosst, and S. Sabour, “Matrix capsules with em

routing,” 2018.

[32] A. Jim´enez-S´anchez, S. Albarqouni, and D. Mateus, “Capsule net-
works against medical imaging data challenges,” in Intravascular
Imaging and Computer Assisted Stenting and Large-Scale Annotation
of Biomedical Data and Expert Label Synthesis.
Springer, 2018, pp.
150–160.

[33] M.-L. Zhang and Z.-H. Zhou, “A review on multi-label learning
algorithms,” IEEE transactions on knowledge and data engineering,
vol. 26, no. 8, pp. 1819–1837, 2014.

[34] Y. Dong, N. V. Chawla, and A. Swami, “metapath2vec: Scalable
representation learning for heterogeneous networks,” in Proceed-
ings of the 23rd ACM SIGKDD international conference on knowledge
discovery and data mining. ACM, 2017, pp. 135–144.

[35] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning
of social representations,” in Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery and data mining.
ACM, 2014, pp. 701–710.

[36] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li, “RCV1: A new bench-
mark collection for text categorization research,” JMLR, vol. 5, pp.
361–397, 2004.

[37] E. Loza Menc´ıa and J. F ¨urnkranz, “Efﬁcient pairwise multilabel
classiﬁcation for large-scale problems in the legal domain,” in
ECML/PKDD, 2008, pp. 50–65.

[38] D. D. Lewis, “An evaluation of phrasal and clustered representa-
tions on a text categorization task,” in SIGIR, 1992, pp. 37–50.
[39] Y. Yang, “An evaluation of statistical approaches to text catego-
rization,” Information retrieval, vol. 1, no. 1-2, pp. 69–90, 1999.
[40] H. Chen, M. Sun, C. Tu, Y. Lin, and Z. Liu, “Neural sentiment
classiﬁcation with user and product attention,” in EMNLP, 2016,
pp. 1650–1659.

[41] T. Liu, Y. Yang, H. Wan, H. Zeng, Z. Chen, and W. Ma, “Support
vector machines classiﬁcation with a very large-scale taxonomy,”
SIGKDD Explorations, vol. 7, no. 1, pp. 36–43, 2005.

[42] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun, “Large
margin methods for structured and interdependent output vari-
ables,” JMLR, vol. 6, pp. 1453–1484, 2005.

[43] P. Yang, X. Sun, W. Li, S. Ma, W. Wu, and H. Wang, “Sgm: sequence
generation model for multi-label classiﬁcation,” in ACL, 2018, pp.
3915–3926.

[44] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,”

JMLR, vol. 3, pp. 993–1022, 2003.

[45] F. Rousseau, E. Kiagias, and M. Vazirgiannis, “Text categorization

as a graph classiﬁcation problem,” in ACL, 2015, pp. 1702–1712.

[46] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and
Y. Bengio, “A structured self-attentive sentence embedding,” in
ICLR, 2017.

[47] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convolu-
tional neural network for modelling sentences,” in ACL, 2014, pp.
655–665.

[48] X. Zhang, J. J. Zhao, and Y. LeCun, “Character-level convolutional
networks for text classiﬁcation,” in NIPS, 2015, pp. 649–657.
[49] G. E. Hinton, A. Krizhevsky, and S. D. Wang, “Transforming auto-

encoders,” in ICANN, 2011, pp. 44–51.

[50] N. Zhang, S. Deng, Z. Sun, X. Chen, W. Zhang, and H. Chen,
“Attention-based capsule networks with dynamic routing for re-
lation extraction,” in Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, 2018, pp. 986–992.

[51] A. Garg, J. Noyola, R. Verma, A. Saxena, and A. Jami, “Exploring
correlation between labels to improve multi-label classiﬁcation,”
arXiv preprint arXiv:1511.07953, 2015.

[52] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Van-
dergheynst, “The emerging ﬁeld of signal processing on graphs:
Extending high-dimensional data analysis to networks and other
irregular domains,” IEEE Signal Process. Mag., vol. 30, no. 3, pp.
83–98, 2013.

[53] J. Bruna, W. Zaremba, A. Szlam, and Y. Lecun, “Spectral networks
and locally connected networks on graphs,” Computer Science,
2013.

[54] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional
neural networks on graphs with fast localized spectral ﬁltering,”
in NIPS, 2016, pp. 3837–3845.

[56] D. K. Duvenaud, D. Maclaurin,

[55] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with
graph convolutional networks,” CoRR, vol. abs/1609.02907, 2016.
J. Aguilera-Iparraguirre,
R. G ´omez-Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P.
Adams, “Convolutional networks on graphs for learning molec-
ular ﬁngerprints,” in NIPS, 2015, pp. 2224–2232.

[57] M. Niepert, M. Ahmed, and K. Kutzkov, “Learning convolutional
neural networks for graphs,” in ICML, 2016, pp. 2014–2023.
[58] S. Verma and Z. Zhang, “Graph capsule convolutional neural

networks,” CoRR, vol. abs/1805.08090, 2018.

[59] W. Yin and H. Sch ¨utze, “Multichannel variable-size convolution
for sentence classiﬁcation,” in Proceedings of the Nineteenth Confer-
ence on Computational Natural Language Learning, 2015, pp. 204–214.
[60] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-
training of deep bidirectional transformers for language under-
standing,” arXiv preprint arXiv:1810.04805, 2018.

Hao Peng is currently a Ph.D. candidate at the
State Key Laboratory of Software Development
Environment, and Beijing Advanced Innovation
Center for Big Data and Brain Computing in Bei-
hang University. His research interests include
representation learning, text mining and urban
computing.

is currently a Professor with the
Jianxin Li
State Key Laboratory of Software Development
Environment, and Beijing Advanced Innovation
Center for Big Data and Brain Computing in
Beihang University. His current research inter-
ests include social network, machine learning,
distributed system, virtualization, big data, trust
management and network security.

14

Qiran Gong is currently a B.E. candidate at
the State Key Laboratory of Software Develop-
ment Environment in Beihang University, Beijing,
China. His research interests include social net-
work mining and text mining.

Senzhang Wang is currently an Associate Pro-
fessor with the Collage of Computer Science and
Technology, Nanjing University of Aeronautics
and Astronautics, Nanjing. His current research
interests include data mining, urban computing
and social network analysis.

Lifang He is currently a Postdoctoral Research
Associate at the Department of Biostatistics and
Epidemiology at the University of Pennsylvania.
Her current research interests include machine
learning, data mining, tensor analysis, biomedi-
cal informatics.

Bo Li , is currently an Associate Professor with
the State Key Laboratory of Software Develop-
ment Environment, and Beijing Advanced Inno-
vation Center for Big Data and Brain Computing
in Beihang University. His current research inter-
ests include big data computing theory, machine
learning and computer security.

Lihong Wang is a professor in National Com-
puter Network Emergency Response Technical
Team/Coordination Center of China. Her current
research interests include information security,
cloud computing, big data mining and analytics,
information retrieval and data mining.

Philip S. Yu is a Distinguished Professor and
the Wexler Chair in Information Technology at
the Department of Computer Science, Univer-
sity of Illinois at Chicago. Before joining UIC,
he was at the IBM Watson Research Center,
where he built a world-renowned data mining
and database department. He is a Fellow of
the ACM and IEEE. Dr. Yu is the recipient of
ACM SIGKDD 2016 Innovation Award for his
inﬂuential research and scientiﬁc contributions
on mining, fusion and anonymization of big data,

the IEEE Computer Societys 2013 Technical Achievement Award for
pioneering and fundamentally innovative contributions to the scalable
indexing, querying, searching, mining and anonymization of big data and
the Research Contributions Award from IEEE Intl. Conference on Data
Mining (ICDM) in 2003 for his pioneering contributions to the ﬁeld of data
mining. Dr. Yu has published more than 1,100 referred conference and
journal papers cited more than 103,000 times with an H-index of 152. He
has applied for more than 300 patents. Dr. Yu was the Editor-in-Chiefs of
ACM Transactions on Knowledge Discovery from Data (2011-2017) and
IEEE Transactions on Knowledge and Data Engineering (2001-2004).

15

