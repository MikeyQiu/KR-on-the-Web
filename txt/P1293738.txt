Unsupervised Deep Homography: A Fast and Robust Homography
Estimation Model

Ty Nguyen∗, Steven W. Chen∗, Shreyas S. Shivakumar, Camillo J. Taylor, Vijay Kumar

8
1
0
2
 
b
e
F
 
1
2
 
 
]

V
C
.
s
c
[
 
 
3
v
6
6
9
3
0
.
9
0
7
1
:
v
i
X
r
a

In this

Abstract— Homography estimation between multiple aerial
images can provide relative pose estimation for collaborative
autonomous exploration and monitoring. The usage on a robotic
system requires a fast and robust homography estimation
algorithm.
study, we propose an unsupervised
learning algorithm that trains a Deep Convolutional Neural
Network to estimate planar homographies. We compare
feature-based and
the proposed algorithm to traditional
direct methods, as well as a corresponding supervised
learning
results demonstrate
that compared to traditional approaches, the unsupervised
algorithm achieves faster inference speed, while maintaining
comparable or better accuracy and robustness to illumination
variation. In addition, our unsupervised method has superior
adaptability and performance compared to the corresponding
supervised deep learning method. Our image dataset and a
Tensorﬂow implementation of our work are available at htt ps :
//github.com/tynguyen/unsupervisedDeepHomographyRAL2018.

algorithm. Our

empirical

I. INTRODUCTION

A homography is a mapping between two images of a
planar surface from different perspectives. They play an
essential role in robotics and computer vision applications
such as image mosaicing [1], monocular SLAM [2], 3D
camera pose reconstruction [3] and virtual touring [4], [5].
For example, homographies are applicable in scenes viewed
at a far distance by an arbitrary moving camera [6], which
are the situations encountered in UAV imagery. However, to
work well in the aerial multi-robot setting, the homography
estimation algorithm needs to be reliable and fast.

The two traditional approaches for homography estimation
are direct methods and feature-based methods [7]. Direct
methods, such as the seminal Lucas-Kanade algorithm [8],
use pixel-to-pixel matching by shifting or warping the images
relative to each other and comparing the pixel
intensity
values using an error metric such as the sum of squared dif-
ferences (SSD). They initialize a guess for the homography
parameters and use a search or optimization technique such
as gradient descent to minimize the error function [9]. The
robustness of direct methods can be improved by using dif-
ferent performance criterion such as the enhanced correlation
coefﬁcient (ECC) [10], integrating feature-based methods
with direct methods [11], or by representing the images in
the Fourier domain [12]. In addition, the speed of direct
methods can be increased by using efﬁcient compositional
image alignment schemes [13].

The authors are with GRASP Lab, University of Pennsylvania, Philadel-
phia, PA 19104, USA, {tynguyen, chenste, sshreyas,
cjtaylor, kumar}@seas.upenn.edu.

∗: The authors have equal contributions

Fig. 1: Above: Synthetic data; Below: Real data; Homog-
raphy estimation results from the unsupervised neural net-
work. Red represents the ground truth correspondences,
and yellow represents the estimated correspondences. These
images depict an example of large levels of displacement
and illumination shifts in which feature-based, direct and/or
supervised learning methods fail.

The second approach are feature-based methods. These
methods ﬁrst extract keypoints in each image using local
invariant features (e.g. Scale Invariant Feature Transform
(SIFT) [14]). They then establish a correspondence between
the two sets of keypoints using feature matching, and use
RANSAC [15] to ﬁnd the best homography estimate. While
these methods have better performance than direct methods,
they can be inaccurate when they fail to detect sufﬁcient
keypoints, or produce incorrect keypoint correspondences
due to illumination and large viewpoint differences between
the images [16]. In addition, these methods are signiﬁcantly
faster than direct methods but can still be slow due to the
computation of the features, leading to the development of
other feature types such as Oriented FAST and Rotated
BRIEF (ORB) [17] which are more computationally efﬁcient
than SIFT, but have worse performance.

Inspired by the success of data-driven Deep Convolutional
Neural Networks (CNN) in computer vision, there has been
an emergence of CNN approaches to estimating optical
ﬂow [18], [19], [20], dense matching [21], [22], depth esti-
mation [23], and homography estimation [24]. Most of these
works, including the most relevant work on homography
estimation,
the estimation problem as a supervised
learning task. These supervised approaches use ground truth
labels, and as a result are limited to synthetic datasets where
the ground truth can be generated for free, or require costly
labeling of real-world data sets.

treat

Our work develops an unsupervised, end-to-end, deep

learning algorithm to estimate homographies. It improves
upon these prior traditional and supervised learning methods
by minimizing a pixel-wise intensity error metric that does
not need ground truth data. Unlike the hand-crafted feature-
based approaches, or the supervised approach that needs
costly labels, our model is adaptive and can easily learn
good features speciﬁc to different data sets. Furthermore, our
framework has fast inference times since it is highly parallel.
These adaptive and speed properties make our unsupervised
networks especially suitable for real world robotic tasks, such
as stitching UAV images.

We demonstrate that our unsupervised homography esti-
mation algorithm has comparable or better accuracy, and bet-
ter inference speed, than feature-based, direct, and supervised
deep learning methods on synthetic and real-world UAV data
sets. In addition, we demonstrate that it can handle large
displacements (∼ 65% image overlap) with large illumination
variation. Fig. 1 illustrates qualitative results on these data
sets, where our unsupervised method is able to estimate the
homography whereas the other approaches cannot.

it

Our unsupervised algorithm is a hybrid approach that
combines the strengths of deep learning with the strengths of
both traditional direct methods and feature-based methods.
It is similar to feature-based methods because it also relies
on features to compute the homography estimates, but it
differs in that
learns the features rather than deﬁning
them. It is also similar to the direct methods because the
error signal used to drive the network training is a pixel-
wise error. However, rather than performing an online op-
timization process, it transfers the computation ofﬂine and
”caches” the results through these learned features. Similar
unsupervised deep learning approaches have been successful
in computer vision tasks such as monocular depth and camera
motion estimation [25], indicating that our framework can
be scaled to tackle general nonlinear motions such as those
encountered in optical ﬂow.

II. PROBLEM FORMULATION

We assume that images are obtained by a perspective pin-
hole camera and present points by homogeneous coordinates,
so that a point (u, v)T is represented as (u, v, 1)T and a point
(x, y, z)T is equivalent to the point (x/z, y/z, 1)T . Suppose that
x = (u, v, 1)T and x(cid:48) = (u(cid:48), v(cid:48), 1)T are two points. A planar
projective transformation or homography that maps x ↔ x(cid:48)
is a linear transformation represented by a non-singular 3 × 3
matrix H such that:





 =







u(cid:48)
v(cid:48)
1

h11
h21
h31

h12
h22
h32







u
v
 Or

1

h13
h23
h33

x(cid:48) = Hx

(1)

Since H can be multiplied by an arbitrary non-zero scale
factor without altering the projective transformation, only
the ratio of the matrix elements is signiﬁcant, leaving H
eight independent ratios corresponding to eight degrees of
freedom. This mapping equation can also represented by two

equations:

u(cid:48) =

h11u + h12v + h13
h31u + h32v + h33

; v(cid:48) =

h21u + h22v + h23
h31u + h32v + h33

(2)

The problem of ﬁnding the homography induced by two
images IA and IB is to ﬁnd a homography HAB such that
Eqn. (1) holds for all points in the overlapping of the two
images.

III. SUPERVISED DEEP HOMOGRAPHY MODEL

The deep learning approach most similar to our work
is the Deep Image Homography Estimation [24]. In this
work, DeTone et al. use supervised learning to train a deep
neural network on a synthetic data set. They use the 4-
point homography parameterization H4pt [26] rather than
the conventional 3 × 3 parameterization H. Suppose that
uA
k , 1)T for k = 1, 2, 3, 4 are
k = (uA
4 ﬁxed points in image IA and IB respectively, such that
uk
2 = Huk
k . Then H4pt is
the 4 × 2 matrix of points (∆uk, ∆vk). Both parameterizations
are equivalent since there is a one-to-one correspondence
between them.

1. Let ∆uk = uB

k , 1)T and uB

k , ∆vk = vB

k = (uB

k − uA

k − vA

k , vA

k , vB

In a deep learning framework though,

this parameter-
ization is more suitable than the 3 × 3 parameterization
H because H mixes the rotation,
translation, scale, and
shear components of the homography transformation. The
rotation and shear components tend to have a much smaller
magnitude than the translation component, and as a result
although an error in their values can greatly impact H, it will
have a small effect on the L2 loss function of the elements of
H, which is detrimental for training the neural network. In
addition, the high variance in the magnitude of the elements
of the 3 × 3 homography matrix makes it difﬁcult to enforce
H to be non-singular. The 4-point parameterization does not
suffer from these problems.

The network architecture is based on VGGNet [27], and is
depicted in Fig. 2(a). The network input is a batch of image
patch pairs. The patch pairs are generated by taking a full-
sized image, cropping a square patch PA at a random position
p, perturbing the four corners of by a random value within
[−ρ, ρ] to generate a homography HAB, applying (HAB)−1 to
the full-sized image, and then cropping a square patch PB of
the same size and at the same location as the patch PA from
the warped image. These image patches are used to avoid
strange border effects near the edges during the synthetic
data generation process, and to standardize the network input
size. The applied homography HAB is saved in the 4 point
parameterization format, H∗
4pt . The network outputs a 4 point
parameterization estimate ˜H4pt .

The error signal used for gradient backpropagation is the
Euclidean L2 norm, denoted as LH , of the estimated 4-point
homography ˜H4pt versus the ground truth H∗

4pt :

LH =

|| ˜H4pt − H∗

4pt ||2
2

1
2

(3)

Fig. 2: Overview of homography estimation methods; (a) Benchmark supervised deep learning approach; (b) Feature-based
methods; and (c) Our unsupervised method. DLT: direct linear transform; PSGG: parameterized sampling grid generator;
DS: differentiable sampling.

IV. UNSUPERVISED DEEP HOMOGRAPHY MODEL

While the supervised deep learning method has promising
results, it is limited in real world applications since it requires
ground truth labels. Drawing inspiration from traditional
direct methods for homography estimation, we can deﬁne an
analogous loss function. Given an image pair IA(x) and IB(x)
with discrete pixel locations represented by homogeneous
coordinates {xi = (xi, yi, 1)T }, we want our network to output
˜H4pt that minimizes the average L1 pixel-wise photometric
loss

LPW =

|IA(H (xi)) − IB(xi)|

(4)

1
|xi| ∑

xi

where ˜H4pt deﬁnes the homography transformation H (xi).
We chose the L1 error versus the L2 error because previous
work has observed that it is more suitable for image align-
ment problems [28], and empirically we found the network
to be easier to train with the L1 error. This loss function is
unsupervised since there is no ground truth label. Similar to
the supervised case, we choose the 4-point parameterization
which is more suitable than the 3 × 3 parameterization.

In order to compare our unsupervised deep learning al-
gorithm with the supervised algorithm, we use the same
VGGNet architecture to output the ˜H4pt . Fig. 2(c) depicts
our unsupervised learning model. The regression module
represents the VGGNet architecture and is shared by both the
supervised and unsupervised methods. Although we do not
investigate other possible architectures, different regression
models such as SqueezeNet [29] may yield better perfor-
mance due to advantages in size and computation require-

ments. The second half of Fig. 2(c) represents the main
contribution of this work, which consists of the differentiable
layers that allow the network to be successfully trained with
the loss function (4).

Using the pixel-wise photometric loss function yields ad-
ditional training challenges. First, every operation, including
the warping operation H (xi), must remain differentiable
to allow the network to be trained via backpropagation.
Second, since the error signal depends on differences in
image intensity values rather than the differences in the
homography parameters, training the deep network is not
necessarily as easy or stable. Another implication of using
a pixel-wise photometric loss function is the implied as-
sumption that lighting and contrast between the input images
remains consistent. In traditional direct methods such as
ECC,
this appearance variation problem is addressed by
modifying the loss function or preprocessing the images. In
our unsupervised algorithm, we standardize our images by
the mean and variance of the intensities of all pixels in our
training dataset, perform data augmentation by injecting ran-
dom illumination shifts, and use the standard L1 photometric
loss. We found that even without modifying the loss function,
our deep neural network is still able to learn to be invariant
to illumination changes.

A. Model Inputs

The input to our model consists of three parts. The ﬁrst
part is a 2-channel image of size 128 × 128 × 2 which is
the stack of PA and PB - two patches cropped from the two
images IA and IB. The second part is the four corners in IA,

denoted as CA
necessary for warping.

4pt . Image IA is also part of the input as it is

ˆbi is a vector with 2 elements representing the last column
of Ai subtracted from both sides of the equation,

(5)

C. Spatial Transformation Layer

B. Tensor Direct Linear Transform

We develop a Tensor Direct Linear Transform (Tensor
DLT) layer to compute a differentiable mapping from the 4-
point parameterization ˜H4pt to ˜H, the 3 × 3 parameterization
of homography. This layer essentially applies the DLT algo-
rithm [30] to tensors, while remaining differentiable to allow
backpropagation during training. As shown in Fig. 2(c), the
input to this layer are the corresponding corners in the image
pairs CA
4pt , and the output is the estimate of the 3×3
homography parameterization ˜H.

4pt and ˜CB

The DLT algorithm is used to solve for the homography
matrix H given a set of four point correspondences [30]. Let
H be the homography induced by a set of four 2D to 2D
correspondences, xi ↔ x(cid:48)
i. According to the deﬁnition of a
homography given in Eqn. (1), x(cid:48)
i ∼ Hxi. This relation can
also be expressed as x(cid:48)
i × Hxi = 0.



Let h jT be the j-th row of H, then:
xT
i h1
xT
i h2
xT
i h3

h1T xi
h2T xi
h3T xi
where h j is the column vector representation of h jT .

Hxi =

 =















(6)

 = 0





i, v(cid:48)

Let x(cid:48)

i = (u(cid:48)

x(cid:48)
i × Hxi =

i, 1)T , then:
ixT
i h3 − xT
v(cid:48)
i h2
ixT
xT
i h1 − u(cid:48)
i h3
ixT
ixT
i h2 − v(cid:48)
u(cid:48)
i h1
This equation can be rewritten as:

3×1 −xT
0T
i
xT
i
ixT
−v(cid:48)
i











(7)

 = 0.

h1
h2
h3

ixT
v(cid:48)
i
0T
ixT
3×1 −u(cid:48)
i
0T
ixT
u(cid:48)
i
3×1
which has the form A(3)
i h = 0 for each i = 1, 2, 3, 4 corre-
spondence pair, where A(3)
is a 3 × 9 matrix, and h is a
vector with 9 elements consisting of the entries of H. Since
the last row in A(3)
is dependent on the other rows, we are
left with two linear equations Aih = 0 where Ai is the ﬁrst
2 rows of A(3)

.

i

i



Given a set of 4 correspondences, we can create a system
of equations to solve for h and thus H. For each i, we can
stack Ai to form Ah = 0. Solving for h results in ﬁnding
a vector in the null space of A. One popular approach is
singular value decomposition (SVD) [31], which is a dif-
ferentiable operation. However, taking the gradients in SVD
has high time complexity and has practical implementation
issues [32]. An alternative solution to this problem is to make
the assumption that the last element of h3, which is H33 is
equal to 1 [33].

With this assumption and the fact

that xi = (ui, vi, 1),
we can rewrite Eqn. (7) in the form ˆAi ˆh = ˆbi for each
i = 1, 2, 3, 4 correspondence points where ˆAi is the 2 × 8
matrix representing the ﬁrst 8 columns of Ai,
v(cid:48)
v(cid:48)
ivi
iui
iui −u(cid:48)
0 −u(cid:48)
ivi

0 −ui −vi −1
1

(cid:20) 0
ui

ˆAi =

0
vi

(cid:21)

0

0

,

i

ˆbi = [−v(cid:48)

i, u(cid:48)

i]T ,

and ˆh is a vector consisting of the ﬁrst 8 elements of h (with
H33 omitted).

By stacking these equations, we get:

ˆA ˆh = ˆb,
(8)
Eqn. (8) has a desirable form because ˆh, and thus H, can be
solved for using ˆA+, the pseudo-inverse of ˆA. This operation
is simple and differentiable with respect to the coordinates
of xi and x(cid:48)
i. In addition, the gradients are easier to calculate
than for SVD.

This approach may still fail if the correspondence points
are collinear: if three of the correspondence points are on the
same line, then solving for H is undetermined. We alleviate
this problem by ﬁrst making the initial guess of H4pt to be
zero, implying that ˜CB
4pt . We then set a small learning
4pt does not
rate such that after each training iteration,
move too far away from CA

4pt ∼ CA

˜CB

4pt .

The next layer applies the 3 × 3 homography estimate
˜H output by the Tensor DLT to the pixel coordinates xi
of image IA in order to get warped coordinates H (xi).
These warped coordinates are necessary in computing the
photometric loss function in Eqn. (4) that will train our
neural network. In addition to warping the coordinates, this
layer must also be differentiable so that the error gradients
can ﬂow through via backpropagation. We thus extend the
Spatial Transformer Layer introduced in [34] by applying it
to homography transformations.

This layer performs an inverse warping in order to avoid
holes in the warped image. This process consists of 3 steps:
(1) Normalized inverse computation ˜Hinv of the homogra-
phy estimate; (2) Parameterized Sampling Grid Generator
(PSGG); and (3) Differentiable Sampling (DS).

The ﬁrst step, computing a normalized inverse, involves
normalizing the height and width coordinates of images IA
and IB into a range such that −1 ≤ ui, vi ≤ 1 and −1 ≤ u(cid:48)
i ≤
1. Thus given a 3 × 3 homography estimate ˜H, the inverse
˜Hinv used for warping is computed as follows:

i, v(cid:48)

˜Hinv = M−1 ˜H−1M


W (cid:48)/2
0
0

0

W (cid:48)/2
H(cid:48)/2 H(cid:48)/2





0

1

where M =



with W (cid:48) and H(cid:48) are the width and height of the IB.

The second step (PSGG) creates a grid G = {Gi} of
the same size as the second image IB. Each grid element
Gi = (u(cid:48)
i) corresponds to pixels of the second image
IB. Applying the inverse homography ˜Hinv to these grid
coordinates provides a grid of pixels in the ﬁrst image IA.

i, v(cid:48)





ui
vi
1


 = Hinv(Gi) = ˜Hinv









u(cid:48)
i
v(cid:48)
i
1

(9)

Based on the sampling points Hinv(Gi) computed from
PSGG, the last step (DS) produces a sampled warped im-
age V of size H(cid:48) × W (cid:48) with C channels, where V (xi) =
IA(H (xi)).

The sampling kernel k(·) is applied to the grid Hinv(Gi)

and the resulting image V is deﬁned as

V C
i =

Ic
nmk(ui − m; Φu)k(vi − n; Φv),

H
∑
n

W
∑
m

∀i ∈ [1...H(cid:48)W (cid:48)]

, ∀c ∈ [1..C]

(10)

where H,W are the height and width of the input image IA,
Φu and Φv are the parameters of k(·) deﬁning the image
interpolation. Ic
nm is the value at location (n, m) in channel
c of the input image, and V c
is the value of the output
i
pixel at location (ui, vi) in channel c. Here, we use bilinear
interpolation such that the Eqn. (10) becomes

V C
i =

Ic
nm max(0, 1 − |ui − m|) max(0, 1 − |vi − n|)

H
∑
n

W
∑
m

(11)

To allow backpropagation of the loss function, gradients with
respect to I and G for bilinear interpolation are deﬁned as

∂V c
i
∂ Ic
nm

=

H
∑
n

W
∑
m

∂V c
i
∂ ui =

H
∑
n

W
∑
m

∂V c
i
∂ vi =

H
∑
n

W
∑
m

max(0, 1 − |ui − m|) max(0, 1 − |vi − n|) (12)

Ic
nm max(0, 1 − |vi − n|)

Ic
nm max(0, 1 − |ui − m|)











0 if |m − ui| ≥ 1
1 if m ≥ ui
−1 if m < ui

0 if |n − vi| ≥ 1
1 if n ≥ vi
−1 if n < vi

(13)

(14)

This allows backpropagation of the loss gradients using the
chain rule because ∂ ui
can be easily derived from
∂ h jk
Eqn. 2.

and ∂ vi
∂ h jk

V. EVALUATION RESULTS

The intended use case for our algorithm is in estimating
homographies for aerial multi-robot systems applications
such as image mosaicing and collision avoidance. As a re-
sult, we demonstrate our unsupervised algorithm’s accuracy,
inference speed, and robustness to illumination variation
relative to SIFT, ORB, ECC and the supervised deep learning
method. We evaluate these methods on a synthetic dataset
similar to the dataset used in [24], and on a real-world aerial
image dataset. Since ORB’s performance is inferior to that
of SIFT, we only report ORB’s performance in Fig. 4 and
omit it in the remaining ﬁgures.

Both the supervised and unsupervised approaches use
the VGGNet architecture to generate homography estimates
˜H4pt. The deep learning approaches are implemented in Ten-
sorﬂow [35] using stochastic gradient descent with a batch
size of 128, and an Adam Optimizer [36] with, β1 = 0.9,
β2 = 0.999 and ε = 10−8. We empirically chose the initial

learning rate for the supervised algorithm and unsupervised
algorithm to be 0.0005 and 0.0001 respectively.

The ECC direct method is a standard Python OpenCV im-
plementation while the feature-based approaches are Python
OpenCV implementations of SIFT RANSAC and ORB
RANSAC. We found that in our synthetic dataset, using
all detected features gives better performance, while in our
aerial dataset, choosing the 50 best features is superior. These
feature pairs are then used to calculate the homography using
RANSAC with a threshold of 5 pixels. For the ECC method,
we use identity matrix as the initialization and set 1000 as
the maximum number of iterations.

A. Synthetic Data Results

This section analyzes the performance proﬁle of the Un-
supervised, Supervised, SIFT, and ECC methods on our
synthetic dataset. We want to test how well our approach
performs under illumination variation and large image dis-
placement.

To account for illumination variation, we globally stan-
dardize our images based on the mean and variance of pixel
intensities of all images in our training dataset. We addition-
ally inject random color, brightness and gamma shifts during
the training. We do not utilize any further preprocessing and
use the L1 photometric loss function. To highlight the effect
of displacement amount on each method, we break down the
accuracy performance in terms of: 85% image overlap (small
displacement), 75% image overlap (moderate displacement),
and 65% image overlap (large displacement). We follow the
synthetic data generation process on the MS-COCO dataset
used in [24]. The amount of image overlap is controlled by
the point perturbation parameter ρ. The evaluation metric is
the 4pt-Homography RMSE from Eqn. (3) comparing the
estimated homography to the ground truth homography.

We train the deep networks from scratch for 300, 000
iterations over ∼ 30 hours, using two GPUs. This long
training procedure only needs to be performed once, as
the resulting model can be used as an initial pre-trained
model for other data sets. We observed that the supervised
model started overﬁtting after 150, 000 iterations so stopped
training early. SIFT, ORB and ECC estimated homographies
using the full images, while the deep learning methods are
only given access to the small patches (∼ 21% pixels).
This disadvantages our methods, and would result in better
performance for the traditional methods, at the expense of
slower running times.

Fig. 3 displays the results of each method broken down
by overlap and performance percentile. We break down the
results by performance percentile to illustrate the various per-
formance proﬁles of each method. Speciﬁcally, SIFT tends
to do very well 60% of the time, but in the worst 40% of the
time it performs very poorly, sometimes completely failing
to detect enough features to estimate a homography. On the
other hand, the deep learning methods tends to have much
more consistent performance, which can be more desireable
in practical applications such as using homographies for
collision avoidance for aerial multi-robot systems. Both the

Fig. 3: Synthetic 4pt-Homography RMSE (lower is better). Unsupervised has comparable performance with the supervised
method and performs better than the other approaches especially when the displacement is large.

B. Aerial Dataset Results

This section analyzes the performance proﬁle of each
method on a representative dataset of aerial imagery cap-
tured by a UAV. In addition to accuracy performance, an
equally important consideration for real world application is
inference speed. As a result, we also discuss the performance
to speed tradeoffs of each method.

Our aerial dataset contains 350 image pairs resized to
240 × 320, captured by a DJI Phantom 3 Pro platform in
Yardley, Pensylvania, USA in 2017. We divided it into 300
train and 50 test samples. We did not label the train set,
but for evaluation purposes, we manually labeled the ground
truth by picking 4 pairs of correspondences for each test
sample. We also randomly inject illumination noise in both
the training and testing sets. The evaluation metrics are the
same for the synthetic data. To reduce training time, we
ﬁnetune the neural networks on the aerial image data. Our
unsupervised algorithm can directly use the aerial dataset
image pairs. However, since we do not have ground truth
homography labels, we have to perform a similar synthetic
data generation process as in the synthetic dataset in order
to ﬁnetune the supervised neural network. We ﬁne tune both
models over 150, 000 iterations for roughly 15 hours with
data augmentation.

Fig. 6 displays the performance proﬁle for the Unsuper-
vised, Supervised, SIFT, and ECC methods. Fig 4 displays
the speed and performance tradeoff for these methods, and
additionally the featured based method ORB. The feature-
based methods are tested on a 16-core Intel Xeon CPU, and
the deep learning methods are tested on the same CPU and
an NVIDIA Titan X GPU. The closer to the lower left hand
corner, the better the performance and faster the runtime.

Both Figs. 6 and 4 demonstrate that our unsupervised al-
gorithm has the best performance of all methods. In addition,
Fig. 4 also shows that our unsupervised method on the GPU
has both the best performance and the fastest inference times.
SIFT has the second best performance after our unsupervised
algorithm, but has a much slower runtime (approximately
200 times slower). ORB has a faster runtime than SIFT, but at
the expense of poorer performance. The ECC direct method
approach has the worst performance and runtime of all the
methods. A qualitative example where both SIFT and ECC
fail to deliver a good result while our method succeeds is
illustrated in Fig. 5.

Fig. 4: Speed Versus Performance Tradeoff. Lower left is
the computational
better. Sufﬁxes GPU and CPU reﬂect
resource. All the feature-based methods are run on the CPU.
The unsupervised network run on the GPU dominates all the
other methods by having both the highest throughput and best
performance.

learning methods and the feature-based methods outperform
the direct method (ECC).

Interestingly, whereas direct method ECC has problems
with illumination variation and large displacement, our un-
supervised method is able to handle these scenarios even
though it uses photometric loss functions. One potential
hypothesis is that our method can be viewed as a hybrid
between direct methods and feature based methods. The
large receptive ﬁeld of neural networks may allow it to
handle large image displacement better than a direct method.
In addition, whereas image gradients are used to update
homography parameters in direct methods, with neural net-
works, these gradients are used to update network parameters
which correspond to improving learned features. Finally,
direct methods are an online optimization process that use
gradients from a single pair of images, whereas training a
deep network is an ofﬂine optimization process that averages
gradients across multiple images. Injecting noise into this
training process can further improve robustness to different
appearance variations. Understanding the relationship be-
tween the neural network and photometric loss functions is
an important direction for future work.

(a) Unsupervised (Success, RMSE = 15.6)

(b) Unsupervised (Success, RMSE = 4.50)

(c) SIFT (Fail, RMSE = 105.2)

(d) SIFT (Success, RMSE = 6.06)

(e) ECC(Success, RMSE = 66.4)

(f) ECC(Success, RMSE = 48.10)

Fig. 5: Qualitative visualization of estimation methods on aerial dataset. Left: hard case, right: moderate case. ECC performs
better than SIFT in the case of small displacement, but performs worse than SIFT in case of large displacement. Unsupervised
network outperforms both SIFT and ECC approaches. Supervised network is omitted due to limited space and its poor
performance on this dataset.

pervised algorithm. Our aerial dataset results highlight the
fact that even though synthetic data can be generated from
real images, a pair of synthetic images is still very different
from a pair of real images. These results demonstrate that the
independence of our unsupervised algorithm from expensive
ground truth labels has large practical implications for real-
world performance.

VI. CONCLUSIONS

We have introduced an unsupervised algorithm that trains
a deep neural network to estimate planar homographies. Our
approach outperforms the corresponding supervised network
on both synthetic and real-world datasets, demonstrating the
superiority of unsupervised learning in image warping prob-
lems. Our approach achieves faster inference speed, while
maintaining comparable or better accuracy than feature-
based and direct methods. We demonstrate that the unsu-
pervised approach is able to handle large displacements and
large illumination variations that are typically challenging
for direct approaches that use the same photometric loss
function. The speed and adaptive nature of our algorithm
makes it especially useful in aerial multi-robot applications
that can exploit parallel computation.

In this work, we do not investigate robustness against
occlusion, leaving it as future work. However, as suggested
in [24], we could potentially address this issue by using data
augmentation techniques such as artiﬁcially inserting random

Fig. 6: 4pt-homography RMSE on aerial images (lower is
better). Unsupervised outperforms other approaches signiﬁ-
cantly.

One of the most interesting results is that while the super-
vised and unsupervised approaches performed comparably
on the synthetic data, the supervised approach had drastically
poorer performance on the aerial image dataset. This shift
is due to the fact that ground truth labels are not available
for our aerial dataset. The generalization gap from synthetic
(train) to real (test) data is an important problem in machine
learning. The best practical approach is to additionally ﬁne-
tune the model on the new distribution of data. In a robotic
ﬁeld experiment, this can be achieved by ﬂying the UAV to
collect a few sample images and ﬁne-tuning on those images.
However, this ﬁne-tuning is only possible with our unsu-

occluding shapes into the training images. Another direction
for future work is investigating different improvements to
achieve sub-pixel accuracy in the top 30% performance
percentile.

Finally, our approach is easily scalable to more general
warping motions. Our ﬁndings provide additional evidence
for applying deep learning methods, speciﬁcally unsuper-
vised learning, to various robotic perception problems such
as stereo depth estimation, or visual odometry. Our insights
on estimating homographies with unsupervised deep neural
network approaches provide an initial step in a structured
progression of applying these methods to larger problems.

VII. ACKNOWLEDGEMENTS

We gratefully acknowledge the support of ARL grants
W911NF-08-2-0004 and W911NF-10-2-0016, ARO grant
W911NF-13-1-0350, N00014-14-1-0510, N00014-09-1-
1051, N00014-11-1-0725, N00014-15-1-2115 and N00014-
09-1-103, DARPA grants HR001151626/HR0011516850
USDA grant 2015-67021-23857 NSF grants IIS-1138847,
IIS-1426840 CNS-1446592 CNS-1521617 and IIS-1328805,
Qualcomm Research, United Technologies, and TerraSwarm,
one of six centers of STARnet, a Semiconductor Research
Corporation program sponsored by MARCO and DARPA.
We would also like to thank Aerial Applications for the
UAV data set.

REFERENCES

[1] M. Brown, D. G. Lowe, et al., “Recognising panoramas.” in ICCV,

vol. 3, 2003, p. 1218.

[2] M. Shridhar and K.-Y. Neo, “Monocular slam for real-time applica-

tions on mobile platforms,” 2015.

[3] Z. Zhang and A. R. Hanson, “3d reconstruction based on homography

mapping,” Proc. ARPA96, pp. 1007–1012, 1996.

[4] Z. Pan, X. Fang, J. Shi, and D. Xu, “Easy tour: a new image-based
virtual tour system,” in Proceedings of the 2004 ACM SIGGRAPH
international conference on Virtual Reality continuum and its appli-
cations in industry. ACM, 2004, pp. 467–471.

[5] C.-Y. Tang, Y.-L. Wu, P.-C. Hu, H.-C. Lin, and W.-C. Chen, “Self-
calibration for metric 3d reconstruction using homography.” in MVA,
2007, pp. 86–89.

[6] D. Capel, “Image mosaicing,” in Image Mosaicing and Super-

resolution. Springer, 2004, pp. 47–79.

[7] R. Szeliski, “Image alignment and stitching: A tutorial,” Foundations
and Trends R(cid:13) in Computer Graphics and Vision, vol. 2, no. 1, pp.
1–104, 2006.

[8] B. D. Lucas and T. Kanade, “An iterative image registration technique
with an application to stereo vision,” in Proceedings of
the 7th
International Joint Conference on Artiﬁcial Intelligence - Volume
2, ser. IJCAI’81.
San Francisco, CA, USA: Morgan Kaufmann
Publishers Inc., 1981, pp. 674–679.

[9] S. Baker and I. Matthews, “Lucas-kanade 20 years on: A unifying
framework,” International journal of computer vision, vol. 56, no. 3,
pp. 221–255, 2004.

[10] G. D. Evangelidis and E. Z. Psarakis, “Parametric image alignment
using enhanced correlation coefﬁcient maximization,” IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, vol. 30, no. 10,
pp. 1858–1865, 2008.

[11] Q. Yan, Y. Xu, X. Yang, and T. Nguyen, “Heask: Robust homography
estimation based on appearance similarity and keypoint correspon-
dences,” Pattern Recognition, vol. 47, no. 1, pp. 368–387, 2014.
[12] S. Lucey, R. Navarathna, A. B. Ashraf, and S. Sridharan, “Fourier
lucas-kanade algorithm,” IEEE transactions on pattern analysis and
machine intelligence, vol. 35, no. 6, pp. 1383–1396, 2013.

[13] E. Mu˜noz, P. M´arquez-Neila, and L. Baumela, “Rationalizing efﬁcient
compositional image alignment,” International Journal of Computer
Vision, vol. 112, no. 3, pp. 354–372, 2015.

[14] D. G. Lowe, “Distinctive image features from scale-invariant key-
points,” International journal of computer vision, vol. 60, no. 2, pp.
91–110, 2004.

[15] M. A. Fischler and R. C. Bolles, “Random sample consensus: A
paradigm for model ﬁtting with applications to image analysis and
automated cartography,” Commun. ACM, vol. 24, no. 6, pp. 381–395,
June 1981.

[16] F.-l. Wu and X.-y. Fang, “An improved ransac homography algorithm
for feature based image mosaic,” in Proceedings of the 7th WSEAS
International Conference on Signal Processing, Computational Geom-
etry & Artiﬁcial Vision. World Scientiﬁc and Engineering Academy
and Society (WSEAS), 2007, pp. 202–207.

[17] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “Orb: An efﬁcient
alternative to sift or surf,” in Computer Vision (ICCV), 2011 IEEE
international conference on.

IEEE, 2011, pp. 2564–2571.

[18] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid, “Deepﬂow:
Large displacement optical ﬂow with deep matching,” in Proceedings
of the IEEE International Conference on Computer Vision, 2013, pp.
1385–1392.

[19] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox,
“Flownet 2.0: Evolution of optical ﬂow estimation with deep net-
works,” arXiv preprint arXiv:1612.01925, 2016.

[20] P. Fischer, A. Dosovitskiy, E. Ilg, P. H¨ausser, C. Hazırbas¸, V. Golkov,
P. van der Smagt, D. Cremers, and T. Brox, “Flownet: Learning optical
ﬂow with convolutional networks,” arXiv preprint arXiv:1504.06852,
2015.

[21] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid, “Deepmatch-
ing: Hierarchical deformable dense matching,” International Journal
of Computer Vision, vol. 120, no. 3, pp. 300–323, 2016.

[22] H. Altwaijry, A. Veit, S. J. Belongie, and C. Tech, “Learning to detect
and match keypoints with deep architectures.” in BMVC, 2016.
[23] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a
single image using a multi-scale deep network,” in Advances in neural
information processing systems, 2014, pp. 2366–2374.

[24] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Deep image homog-

raphy estimation,” arXiv preprint arXiv:1606.03798, 2016.

[25] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsuper-
vised learning of depth and ego-motion from video,” arXiv preprint
arXiv:1704.07813, 2017.

[26] S. Baker, A. Datta, and T. Kanade, “Parameterizing homographies,”

Technical Report CMU-RI-TR-06-11, 2006.

[27] K. Simonyan and A. Zisserman, “Very deep convolutional networks
for large-scale image recognition,” arXiv preprint arXiv:1409.1556,
2014.

[28] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Is l2 a good loss function
for neural networks for image processing?” ArXiv e-prints, vol. 1511,
2015.

[29] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,
and K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer
parameters and¡ 0.5 mb model size,” arXiv preprint arXiv:1602.07360,
2016.

[30] R. I. Hartley and A. Zisserman, Multiple View Geometry in Computer
Cambridge University Press, ISBN: 0521540518,

Vision, 2nd ed.
2004.

[31] G. H. Golub and C. Reinsch, “Singular value decomposition and least
squares solutions,” Numerische mathematik, vol. 14, no. 5, pp. 403–
420, 1970.

[32] T. Papadopoulo and M. I. Lourakis, “Estimating the jacobian of the
singular value decomposition: Theory and applications,” in European
Conference on Computer Vision. Springer, 2000, pp. 554–570.
[33] R. Hartley and A. Zisserman, Multiple view geometry in computer

vision. Cambridge university press, 2003.

[34] M. Jaderberg, K. Simonyan, A. Zisserman, et al., “Spatial transformer
networks,” in Advances in Neural Information Processing Systems,
2015, pp. 2017–2025.

[35] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, et al., “Tensorﬂow: Large-
scale machine learning on heterogeneous distributed systems,” arXiv
preprint arXiv:1603.04467, 2016.

[36] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

Unsupervised Deep Homography: A Fast and Robust Homography
Estimation Model

Ty Nguyen∗, Steven W. Chen∗, Shreyas S. Shivakumar, Camillo J. Taylor, Vijay Kumar

8
1
0
2
 
b
e
F
 
1
2
 
 
]

V
C
.
s
c
[
 
 
3
v
6
6
9
3
0
.
9
0
7
1
:
v
i
X
r
a

In this

Abstract— Homography estimation between multiple aerial
images can provide relative pose estimation for collaborative
autonomous exploration and monitoring. The usage on a robotic
system requires a fast and robust homography estimation
algorithm.
study, we propose an unsupervised
learning algorithm that trains a Deep Convolutional Neural
Network to estimate planar homographies. We compare
feature-based and
the proposed algorithm to traditional
direct methods, as well as a corresponding supervised
learning
results demonstrate
that compared to traditional approaches, the unsupervised
algorithm achieves faster inference speed, while maintaining
comparable or better accuracy and robustness to illumination
variation. In addition, our unsupervised method has superior
adaptability and performance compared to the corresponding
supervised deep learning method. Our image dataset and a
Tensorﬂow implementation of our work are available at htt ps :
//github.com/tynguyen/unsupervisedDeepHomographyRAL2018.

algorithm. Our

empirical

I. INTRODUCTION

A homography is a mapping between two images of a
planar surface from different perspectives. They play an
essential role in robotics and computer vision applications
such as image mosaicing [1], monocular SLAM [2], 3D
camera pose reconstruction [3] and virtual touring [4], [5].
For example, homographies are applicable in scenes viewed
at a far distance by an arbitrary moving camera [6], which
are the situations encountered in UAV imagery. However, to
work well in the aerial multi-robot setting, the homography
estimation algorithm needs to be reliable and fast.

The two traditional approaches for homography estimation
are direct methods and feature-based methods [7]. Direct
methods, such as the seminal Lucas-Kanade algorithm [8],
use pixel-to-pixel matching by shifting or warping the images
relative to each other and comparing the pixel
intensity
values using an error metric such as the sum of squared dif-
ferences (SSD). They initialize a guess for the homography
parameters and use a search or optimization technique such
as gradient descent to minimize the error function [9]. The
robustness of direct methods can be improved by using dif-
ferent performance criterion such as the enhanced correlation
coefﬁcient (ECC) [10], integrating feature-based methods
with direct methods [11], or by representing the images in
the Fourier domain [12]. In addition, the speed of direct
methods can be increased by using efﬁcient compositional
image alignment schemes [13].

The authors are with GRASP Lab, University of Pennsylvania, Philadel-
phia, PA 19104, USA, {tynguyen, chenste, sshreyas,
cjtaylor, kumar}@seas.upenn.edu.

∗: The authors have equal contributions

Fig. 1: Above: Synthetic data; Below: Real data; Homog-
raphy estimation results from the unsupervised neural net-
work. Red represents the ground truth correspondences,
and yellow represents the estimated correspondences. These
images depict an example of large levels of displacement
and illumination shifts in which feature-based, direct and/or
supervised learning methods fail.

The second approach are feature-based methods. These
methods ﬁrst extract keypoints in each image using local
invariant features (e.g. Scale Invariant Feature Transform
(SIFT) [14]). They then establish a correspondence between
the two sets of keypoints using feature matching, and use
RANSAC [15] to ﬁnd the best homography estimate. While
these methods have better performance than direct methods,
they can be inaccurate when they fail to detect sufﬁcient
keypoints, or produce incorrect keypoint correspondences
due to illumination and large viewpoint differences between
the images [16]. In addition, these methods are signiﬁcantly
faster than direct methods but can still be slow due to the
computation of the features, leading to the development of
other feature types such as Oriented FAST and Rotated
BRIEF (ORB) [17] which are more computationally efﬁcient
than SIFT, but have worse performance.

Inspired by the success of data-driven Deep Convolutional
Neural Networks (CNN) in computer vision, there has been
an emergence of CNN approaches to estimating optical
ﬂow [18], [19], [20], dense matching [21], [22], depth esti-
mation [23], and homography estimation [24]. Most of these
works, including the most relevant work on homography
estimation,
the estimation problem as a supervised
learning task. These supervised approaches use ground truth
labels, and as a result are limited to synthetic datasets where
the ground truth can be generated for free, or require costly
labeling of real-world data sets.

treat

Our work develops an unsupervised, end-to-end, deep

learning algorithm to estimate homographies. It improves
upon these prior traditional and supervised learning methods
by minimizing a pixel-wise intensity error metric that does
not need ground truth data. Unlike the hand-crafted feature-
based approaches, or the supervised approach that needs
costly labels, our model is adaptive and can easily learn
good features speciﬁc to different data sets. Furthermore, our
framework has fast inference times since it is highly parallel.
These adaptive and speed properties make our unsupervised
networks especially suitable for real world robotic tasks, such
as stitching UAV images.

We demonstrate that our unsupervised homography esti-
mation algorithm has comparable or better accuracy, and bet-
ter inference speed, than feature-based, direct, and supervised
deep learning methods on synthetic and real-world UAV data
sets. In addition, we demonstrate that it can handle large
displacements (∼ 65% image overlap) with large illumination
variation. Fig. 1 illustrates qualitative results on these data
sets, where our unsupervised method is able to estimate the
homography whereas the other approaches cannot.

it

Our unsupervised algorithm is a hybrid approach that
combines the strengths of deep learning with the strengths of
both traditional direct methods and feature-based methods.
It is similar to feature-based methods because it also relies
on features to compute the homography estimates, but it
differs in that
learns the features rather than deﬁning
them. It is also similar to the direct methods because the
error signal used to drive the network training is a pixel-
wise error. However, rather than performing an online op-
timization process, it transfers the computation ofﬂine and
”caches” the results through these learned features. Similar
unsupervised deep learning approaches have been successful
in computer vision tasks such as monocular depth and camera
motion estimation [25], indicating that our framework can
be scaled to tackle general nonlinear motions such as those
encountered in optical ﬂow.

II. PROBLEM FORMULATION

We assume that images are obtained by a perspective pin-
hole camera and present points by homogeneous coordinates,
so that a point (u, v)T is represented as (u, v, 1)T and a point
(x, y, z)T is equivalent to the point (x/z, y/z, 1)T . Suppose that
x = (u, v, 1)T and x(cid:48) = (u(cid:48), v(cid:48), 1)T are two points. A planar
projective transformation or homography that maps x ↔ x(cid:48)
is a linear transformation represented by a non-singular 3 × 3
matrix H such that:





 =







u(cid:48)
v(cid:48)
1

h11
h21
h31

h12
h22
h32







u
v
 Or

1

h13
h23
h33

x(cid:48) = Hx

(1)

Since H can be multiplied by an arbitrary non-zero scale
factor without altering the projective transformation, only
the ratio of the matrix elements is signiﬁcant, leaving H
eight independent ratios corresponding to eight degrees of
freedom. This mapping equation can also represented by two

equations:

u(cid:48) =

h11u + h12v + h13
h31u + h32v + h33

; v(cid:48) =

h21u + h22v + h23
h31u + h32v + h33

(2)

The problem of ﬁnding the homography induced by two
images IA and IB is to ﬁnd a homography HAB such that
Eqn. (1) holds for all points in the overlapping of the two
images.

III. SUPERVISED DEEP HOMOGRAPHY MODEL

The deep learning approach most similar to our work
is the Deep Image Homography Estimation [24]. In this
work, DeTone et al. use supervised learning to train a deep
neural network on a synthetic data set. They use the 4-
point homography parameterization H4pt [26] rather than
the conventional 3 × 3 parameterization H. Suppose that
uA
k , 1)T for k = 1, 2, 3, 4 are
k = (uA
4 ﬁxed points in image IA and IB respectively, such that
uk
2 = Huk
k . Then H4pt is
the 4 × 2 matrix of points (∆uk, ∆vk). Both parameterizations
are equivalent since there is a one-to-one correspondence
between them.

1. Let ∆uk = uB

k , 1)T and uB

k , ∆vk = vB

k = (uB

k − uA

k − vA

k , vA

k , vB

In a deep learning framework though,

this parameter-
ization is more suitable than the 3 × 3 parameterization
H because H mixes the rotation,
translation, scale, and
shear components of the homography transformation. The
rotation and shear components tend to have a much smaller
magnitude than the translation component, and as a result
although an error in their values can greatly impact H, it will
have a small effect on the L2 loss function of the elements of
H, which is detrimental for training the neural network. In
addition, the high variance in the magnitude of the elements
of the 3 × 3 homography matrix makes it difﬁcult to enforce
H to be non-singular. The 4-point parameterization does not
suffer from these problems.

The network architecture is based on VGGNet [27], and is
depicted in Fig. 2(a). The network input is a batch of image
patch pairs. The patch pairs are generated by taking a full-
sized image, cropping a square patch PA at a random position
p, perturbing the four corners of by a random value within
[−ρ, ρ] to generate a homography HAB, applying (HAB)−1 to
the full-sized image, and then cropping a square patch PB of
the same size and at the same location as the patch PA from
the warped image. These image patches are used to avoid
strange border effects near the edges during the synthetic
data generation process, and to standardize the network input
size. The applied homography HAB is saved in the 4 point
parameterization format, H∗
4pt . The network outputs a 4 point
parameterization estimate ˜H4pt .

The error signal used for gradient backpropagation is the
Euclidean L2 norm, denoted as LH , of the estimated 4-point
homography ˜H4pt versus the ground truth H∗

4pt :

LH =

|| ˜H4pt − H∗

4pt ||2
2

1
2

(3)

Fig. 2: Overview of homography estimation methods; (a) Benchmark supervised deep learning approach; (b) Feature-based
methods; and (c) Our unsupervised method. DLT: direct linear transform; PSGG: parameterized sampling grid generator;
DS: differentiable sampling.

IV. UNSUPERVISED DEEP HOMOGRAPHY MODEL

While the supervised deep learning method has promising
results, it is limited in real world applications since it requires
ground truth labels. Drawing inspiration from traditional
direct methods for homography estimation, we can deﬁne an
analogous loss function. Given an image pair IA(x) and IB(x)
with discrete pixel locations represented by homogeneous
coordinates {xi = (xi, yi, 1)T }, we want our network to output
˜H4pt that minimizes the average L1 pixel-wise photometric
loss

LPW =

|IA(H (xi)) − IB(xi)|

(4)

1
|xi| ∑

xi

where ˜H4pt deﬁnes the homography transformation H (xi).
We chose the L1 error versus the L2 error because previous
work has observed that it is more suitable for image align-
ment problems [28], and empirically we found the network
to be easier to train with the L1 error. This loss function is
unsupervised since there is no ground truth label. Similar to
the supervised case, we choose the 4-point parameterization
which is more suitable than the 3 × 3 parameterization.

In order to compare our unsupervised deep learning al-
gorithm with the supervised algorithm, we use the same
VGGNet architecture to output the ˜H4pt . Fig. 2(c) depicts
our unsupervised learning model. The regression module
represents the VGGNet architecture and is shared by both the
supervised and unsupervised methods. Although we do not
investigate other possible architectures, different regression
models such as SqueezeNet [29] may yield better perfor-
mance due to advantages in size and computation require-

ments. The second half of Fig. 2(c) represents the main
contribution of this work, which consists of the differentiable
layers that allow the network to be successfully trained with
the loss function (4).

Using the pixel-wise photometric loss function yields ad-
ditional training challenges. First, every operation, including
the warping operation H (xi), must remain differentiable
to allow the network to be trained via backpropagation.
Second, since the error signal depends on differences in
image intensity values rather than the differences in the
homography parameters, training the deep network is not
necessarily as easy or stable. Another implication of using
a pixel-wise photometric loss function is the implied as-
sumption that lighting and contrast between the input images
remains consistent. In traditional direct methods such as
ECC,
this appearance variation problem is addressed by
modifying the loss function or preprocessing the images. In
our unsupervised algorithm, we standardize our images by
the mean and variance of the intensities of all pixels in our
training dataset, perform data augmentation by injecting ran-
dom illumination shifts, and use the standard L1 photometric
loss. We found that even without modifying the loss function,
our deep neural network is still able to learn to be invariant
to illumination changes.

A. Model Inputs

The input to our model consists of three parts. The ﬁrst
part is a 2-channel image of size 128 × 128 × 2 which is
the stack of PA and PB - two patches cropped from the two
images IA and IB. The second part is the four corners in IA,

denoted as CA
necessary for warping.

4pt . Image IA is also part of the input as it is

ˆbi is a vector with 2 elements representing the last column
of Ai subtracted from both sides of the equation,

(5)

C. Spatial Transformation Layer

B. Tensor Direct Linear Transform

We develop a Tensor Direct Linear Transform (Tensor
DLT) layer to compute a differentiable mapping from the 4-
point parameterization ˜H4pt to ˜H, the 3 × 3 parameterization
of homography. This layer essentially applies the DLT algo-
rithm [30] to tensors, while remaining differentiable to allow
backpropagation during training. As shown in Fig. 2(c), the
input to this layer are the corresponding corners in the image
pairs CA
4pt , and the output is the estimate of the 3×3
homography parameterization ˜H.

4pt and ˜CB

The DLT algorithm is used to solve for the homography
matrix H given a set of four point correspondences [30]. Let
H be the homography induced by a set of four 2D to 2D
correspondences, xi ↔ x(cid:48)
i. According to the deﬁnition of a
homography given in Eqn. (1), x(cid:48)
i ∼ Hxi. This relation can
also be expressed as x(cid:48)
i × Hxi = 0.



Let h jT be the j-th row of H, then:
xT
i h1
xT
i h2
xT
i h3

h1T xi
h2T xi
h3T xi
where h j is the column vector representation of h jT .

Hxi =

 =















(6)

 = 0





i, v(cid:48)

Let x(cid:48)

i = (u(cid:48)

x(cid:48)
i × Hxi =

i, 1)T , then:
ixT
i h3 − xT
v(cid:48)
i h2
ixT
xT
i h1 − u(cid:48)
i h3
ixT
ixT
i h2 − v(cid:48)
u(cid:48)
i h1
This equation can be rewritten as:

3×1 −xT
0T
i
xT
i
ixT
−v(cid:48)
i











(7)

 = 0.

h1
h2
h3

ixT
v(cid:48)
i
0T
ixT
3×1 −u(cid:48)
i
0T
ixT
u(cid:48)
i
3×1
which has the form A(3)
i h = 0 for each i = 1, 2, 3, 4 corre-
spondence pair, where A(3)
is a 3 × 9 matrix, and h is a
vector with 9 elements consisting of the entries of H. Since
the last row in A(3)
is dependent on the other rows, we are
left with two linear equations Aih = 0 where Ai is the ﬁrst
2 rows of A(3)

.

i

i



Given a set of 4 correspondences, we can create a system
of equations to solve for h and thus H. For each i, we can
stack Ai to form Ah = 0. Solving for h results in ﬁnding
a vector in the null space of A. One popular approach is
singular value decomposition (SVD) [31], which is a dif-
ferentiable operation. However, taking the gradients in SVD
has high time complexity and has practical implementation
issues [32]. An alternative solution to this problem is to make
the assumption that the last element of h3, which is H33 is
equal to 1 [33].

With this assumption and the fact

that xi = (ui, vi, 1),
we can rewrite Eqn. (7) in the form ˆAi ˆh = ˆbi for each
i = 1, 2, 3, 4 correspondence points where ˆAi is the 2 × 8
matrix representing the ﬁrst 8 columns of Ai,
v(cid:48)
v(cid:48)
ivi
iui
iui −u(cid:48)
0 −u(cid:48)
ivi

0 −ui −vi −1
1

(cid:20) 0
ui

ˆAi =

0
vi

(cid:21)

0

0

,

i

ˆbi = [−v(cid:48)

i, u(cid:48)

i]T ,

and ˆh is a vector consisting of the ﬁrst 8 elements of h (with
H33 omitted).

By stacking these equations, we get:

ˆA ˆh = ˆb,
(8)
Eqn. (8) has a desirable form because ˆh, and thus H, can be
solved for using ˆA+, the pseudo-inverse of ˆA. This operation
is simple and differentiable with respect to the coordinates
of xi and x(cid:48)
i. In addition, the gradients are easier to calculate
than for SVD.

This approach may still fail if the correspondence points
are collinear: if three of the correspondence points are on the
same line, then solving for H is undetermined. We alleviate
this problem by ﬁrst making the initial guess of H4pt to be
zero, implying that ˜CB
4pt . We then set a small learning
4pt does not
rate such that after each training iteration,
move too far away from CA

4pt ∼ CA

˜CB

4pt .

The next layer applies the 3 × 3 homography estimate
˜H output by the Tensor DLT to the pixel coordinates xi
of image IA in order to get warped coordinates H (xi).
These warped coordinates are necessary in computing the
photometric loss function in Eqn. (4) that will train our
neural network. In addition to warping the coordinates, this
layer must also be differentiable so that the error gradients
can ﬂow through via backpropagation. We thus extend the
Spatial Transformer Layer introduced in [34] by applying it
to homography transformations.

This layer performs an inverse warping in order to avoid
holes in the warped image. This process consists of 3 steps:
(1) Normalized inverse computation ˜Hinv of the homogra-
phy estimate; (2) Parameterized Sampling Grid Generator
(PSGG); and (3) Differentiable Sampling (DS).

The ﬁrst step, computing a normalized inverse, involves
normalizing the height and width coordinates of images IA
and IB into a range such that −1 ≤ ui, vi ≤ 1 and −1 ≤ u(cid:48)
i ≤
1. Thus given a 3 × 3 homography estimate ˜H, the inverse
˜Hinv used for warping is computed as follows:

i, v(cid:48)

˜Hinv = M−1 ˜H−1M


W (cid:48)/2
0
0

0

W (cid:48)/2
H(cid:48)/2 H(cid:48)/2





0

1

where M =



with W (cid:48) and H(cid:48) are the width and height of the IB.

The second step (PSGG) creates a grid G = {Gi} of
the same size as the second image IB. Each grid element
Gi = (u(cid:48)
i) corresponds to pixels of the second image
IB. Applying the inverse homography ˜Hinv to these grid
coordinates provides a grid of pixels in the ﬁrst image IA.

i, v(cid:48)





ui
vi
1


 = Hinv(Gi) = ˜Hinv









u(cid:48)
i
v(cid:48)
i
1

(9)

Based on the sampling points Hinv(Gi) computed from
PSGG, the last step (DS) produces a sampled warped im-
age V of size H(cid:48) × W (cid:48) with C channels, where V (xi) =
IA(H (xi)).

The sampling kernel k(·) is applied to the grid Hinv(Gi)

and the resulting image V is deﬁned as

V C
i =

Ic
nmk(ui − m; Φu)k(vi − n; Φv),

H
∑
n

W
∑
m

∀i ∈ [1...H(cid:48)W (cid:48)]

, ∀c ∈ [1..C]

(10)

where H,W are the height and width of the input image IA,
Φu and Φv are the parameters of k(·) deﬁning the image
interpolation. Ic
nm is the value at location (n, m) in channel
c of the input image, and V c
is the value of the output
i
pixel at location (ui, vi) in channel c. Here, we use bilinear
interpolation such that the Eqn. (10) becomes

V C
i =

Ic
nm max(0, 1 − |ui − m|) max(0, 1 − |vi − n|)

H
∑
n

W
∑
m

(11)

To allow backpropagation of the loss function, gradients with
respect to I and G for bilinear interpolation are deﬁned as

∂V c
i
∂ Ic
nm

=

H
∑
n

W
∑
m

∂V c
i
∂ ui =

H
∑
n

W
∑
m

∂V c
i
∂ vi =

H
∑
n

W
∑
m

max(0, 1 − |ui − m|) max(0, 1 − |vi − n|) (12)

Ic
nm max(0, 1 − |vi − n|)

Ic
nm max(0, 1 − |ui − m|)











0 if |m − ui| ≥ 1
1 if m ≥ ui
−1 if m < ui

0 if |n − vi| ≥ 1
1 if n ≥ vi
−1 if n < vi

(13)

(14)

This allows backpropagation of the loss gradients using the
chain rule because ∂ ui
can be easily derived from
∂ h jk
Eqn. 2.

and ∂ vi
∂ h jk

V. EVALUATION RESULTS

The intended use case for our algorithm is in estimating
homographies for aerial multi-robot systems applications
such as image mosaicing and collision avoidance. As a re-
sult, we demonstrate our unsupervised algorithm’s accuracy,
inference speed, and robustness to illumination variation
relative to SIFT, ORB, ECC and the supervised deep learning
method. We evaluate these methods on a synthetic dataset
similar to the dataset used in [24], and on a real-world aerial
image dataset. Since ORB’s performance is inferior to that
of SIFT, we only report ORB’s performance in Fig. 4 and
omit it in the remaining ﬁgures.

Both the supervised and unsupervised approaches use
the VGGNet architecture to generate homography estimates
˜H4pt. The deep learning approaches are implemented in Ten-
sorﬂow [35] using stochastic gradient descent with a batch
size of 128, and an Adam Optimizer [36] with, β1 = 0.9,
β2 = 0.999 and ε = 10−8. We empirically chose the initial

learning rate for the supervised algorithm and unsupervised
algorithm to be 0.0005 and 0.0001 respectively.

The ECC direct method is a standard Python OpenCV im-
plementation while the feature-based approaches are Python
OpenCV implementations of SIFT RANSAC and ORB
RANSAC. We found that in our synthetic dataset, using
all detected features gives better performance, while in our
aerial dataset, choosing the 50 best features is superior. These
feature pairs are then used to calculate the homography using
RANSAC with a threshold of 5 pixels. For the ECC method,
we use identity matrix as the initialization and set 1000 as
the maximum number of iterations.

A. Synthetic Data Results

This section analyzes the performance proﬁle of the Un-
supervised, Supervised, SIFT, and ECC methods on our
synthetic dataset. We want to test how well our approach
performs under illumination variation and large image dis-
placement.

To account for illumination variation, we globally stan-
dardize our images based on the mean and variance of pixel
intensities of all images in our training dataset. We addition-
ally inject random color, brightness and gamma shifts during
the training. We do not utilize any further preprocessing and
use the L1 photometric loss function. To highlight the effect
of displacement amount on each method, we break down the
accuracy performance in terms of: 85% image overlap (small
displacement), 75% image overlap (moderate displacement),
and 65% image overlap (large displacement). We follow the
synthetic data generation process on the MS-COCO dataset
used in [24]. The amount of image overlap is controlled by
the point perturbation parameter ρ. The evaluation metric is
the 4pt-Homography RMSE from Eqn. (3) comparing the
estimated homography to the ground truth homography.

We train the deep networks from scratch for 300, 000
iterations over ∼ 30 hours, using two GPUs. This long
training procedure only needs to be performed once, as
the resulting model can be used as an initial pre-trained
model for other data sets. We observed that the supervised
model started overﬁtting after 150, 000 iterations so stopped
training early. SIFT, ORB and ECC estimated homographies
using the full images, while the deep learning methods are
only given access to the small patches (∼ 21% pixels).
This disadvantages our methods, and would result in better
performance for the traditional methods, at the expense of
slower running times.

Fig. 3 displays the results of each method broken down
by overlap and performance percentile. We break down the
results by performance percentile to illustrate the various per-
formance proﬁles of each method. Speciﬁcally, SIFT tends
to do very well 60% of the time, but in the worst 40% of the
time it performs very poorly, sometimes completely failing
to detect enough features to estimate a homography. On the
other hand, the deep learning methods tends to have much
more consistent performance, which can be more desireable
in practical applications such as using homographies for
collision avoidance for aerial multi-robot systems. Both the

Fig. 3: Synthetic 4pt-Homography RMSE (lower is better). Unsupervised has comparable performance with the supervised
method and performs better than the other approaches especially when the displacement is large.

B. Aerial Dataset Results

This section analyzes the performance proﬁle of each
method on a representative dataset of aerial imagery cap-
tured by a UAV. In addition to accuracy performance, an
equally important consideration for real world application is
inference speed. As a result, we also discuss the performance
to speed tradeoffs of each method.

Our aerial dataset contains 350 image pairs resized to
240 × 320, captured by a DJI Phantom 3 Pro platform in
Yardley, Pensylvania, USA in 2017. We divided it into 300
train and 50 test samples. We did not label the train set,
but for evaluation purposes, we manually labeled the ground
truth by picking 4 pairs of correspondences for each test
sample. We also randomly inject illumination noise in both
the training and testing sets. The evaluation metrics are the
same for the synthetic data. To reduce training time, we
ﬁnetune the neural networks on the aerial image data. Our
unsupervised algorithm can directly use the aerial dataset
image pairs. However, since we do not have ground truth
homography labels, we have to perform a similar synthetic
data generation process as in the synthetic dataset in order
to ﬁnetune the supervised neural network. We ﬁne tune both
models over 150, 000 iterations for roughly 15 hours with
data augmentation.

Fig. 6 displays the performance proﬁle for the Unsuper-
vised, Supervised, SIFT, and ECC methods. Fig 4 displays
the speed and performance tradeoff for these methods, and
additionally the featured based method ORB. The feature-
based methods are tested on a 16-core Intel Xeon CPU, and
the deep learning methods are tested on the same CPU and
an NVIDIA Titan X GPU. The closer to the lower left hand
corner, the better the performance and faster the runtime.

Both Figs. 6 and 4 demonstrate that our unsupervised al-
gorithm has the best performance of all methods. In addition,
Fig. 4 also shows that our unsupervised method on the GPU
has both the best performance and the fastest inference times.
SIFT has the second best performance after our unsupervised
algorithm, but has a much slower runtime (approximately
200 times slower). ORB has a faster runtime than SIFT, but at
the expense of poorer performance. The ECC direct method
approach has the worst performance and runtime of all the
methods. A qualitative example where both SIFT and ECC
fail to deliver a good result while our method succeeds is
illustrated in Fig. 5.

Fig. 4: Speed Versus Performance Tradeoff. Lower left is
the computational
better. Sufﬁxes GPU and CPU reﬂect
resource. All the feature-based methods are run on the CPU.
The unsupervised network run on the GPU dominates all the
other methods by having both the highest throughput and best
performance.

learning methods and the feature-based methods outperform
the direct method (ECC).

Interestingly, whereas direct method ECC has problems
with illumination variation and large displacement, our un-
supervised method is able to handle these scenarios even
though it uses photometric loss functions. One potential
hypothesis is that our method can be viewed as a hybrid
between direct methods and feature based methods. The
large receptive ﬁeld of neural networks may allow it to
handle large image displacement better than a direct method.
In addition, whereas image gradients are used to update
homography parameters in direct methods, with neural net-
works, these gradients are used to update network parameters
which correspond to improving learned features. Finally,
direct methods are an online optimization process that use
gradients from a single pair of images, whereas training a
deep network is an ofﬂine optimization process that averages
gradients across multiple images. Injecting noise into this
training process can further improve robustness to different
appearance variations. Understanding the relationship be-
tween the neural network and photometric loss functions is
an important direction for future work.

(a) Unsupervised (Success, RMSE = 15.6)

(b) Unsupervised (Success, RMSE = 4.50)

(c) SIFT (Fail, RMSE = 105.2)

(d) SIFT (Success, RMSE = 6.06)

(e) ECC(Success, RMSE = 66.4)

(f) ECC(Success, RMSE = 48.10)

Fig. 5: Qualitative visualization of estimation methods on aerial dataset. Left: hard case, right: moderate case. ECC performs
better than SIFT in the case of small displacement, but performs worse than SIFT in case of large displacement. Unsupervised
network outperforms both SIFT and ECC approaches. Supervised network is omitted due to limited space and its poor
performance on this dataset.

pervised algorithm. Our aerial dataset results highlight the
fact that even though synthetic data can be generated from
real images, a pair of synthetic images is still very different
from a pair of real images. These results demonstrate that the
independence of our unsupervised algorithm from expensive
ground truth labels has large practical implications for real-
world performance.

VI. CONCLUSIONS

We have introduced an unsupervised algorithm that trains
a deep neural network to estimate planar homographies. Our
approach outperforms the corresponding supervised network
on both synthetic and real-world datasets, demonstrating the
superiority of unsupervised learning in image warping prob-
lems. Our approach achieves faster inference speed, while
maintaining comparable or better accuracy than feature-
based and direct methods. We demonstrate that the unsu-
pervised approach is able to handle large displacements and
large illumination variations that are typically challenging
for direct approaches that use the same photometric loss
function. The speed and adaptive nature of our algorithm
makes it especially useful in aerial multi-robot applications
that can exploit parallel computation.

In this work, we do not investigate robustness against
occlusion, leaving it as future work. However, as suggested
in [24], we could potentially address this issue by using data
augmentation techniques such as artiﬁcially inserting random

Fig. 6: 4pt-homography RMSE on aerial images (lower is
better). Unsupervised outperforms other approaches signiﬁ-
cantly.

One of the most interesting results is that while the super-
vised and unsupervised approaches performed comparably
on the synthetic data, the supervised approach had drastically
poorer performance on the aerial image dataset. This shift
is due to the fact that ground truth labels are not available
for our aerial dataset. The generalization gap from synthetic
(train) to real (test) data is an important problem in machine
learning. The best practical approach is to additionally ﬁne-
tune the model on the new distribution of data. In a robotic
ﬁeld experiment, this can be achieved by ﬂying the UAV to
collect a few sample images and ﬁne-tuning on those images.
However, this ﬁne-tuning is only possible with our unsu-

occluding shapes into the training images. Another direction
for future work is investigating different improvements to
achieve sub-pixel accuracy in the top 30% performance
percentile.

Finally, our approach is easily scalable to more general
warping motions. Our ﬁndings provide additional evidence
for applying deep learning methods, speciﬁcally unsuper-
vised learning, to various robotic perception problems such
as stereo depth estimation, or visual odometry. Our insights
on estimating homographies with unsupervised deep neural
network approaches provide an initial step in a structured
progression of applying these methods to larger problems.

VII. ACKNOWLEDGEMENTS

We gratefully acknowledge the support of ARL grants
W911NF-08-2-0004 and W911NF-10-2-0016, ARO grant
W911NF-13-1-0350, N00014-14-1-0510, N00014-09-1-
1051, N00014-11-1-0725, N00014-15-1-2115 and N00014-
09-1-103, DARPA grants HR001151626/HR0011516850
USDA grant 2015-67021-23857 NSF grants IIS-1138847,
IIS-1426840 CNS-1446592 CNS-1521617 and IIS-1328805,
Qualcomm Research, United Technologies, and TerraSwarm,
one of six centers of STARnet, a Semiconductor Research
Corporation program sponsored by MARCO and DARPA.
We would also like to thank Aerial Applications for the
UAV data set.

REFERENCES

[1] M. Brown, D. G. Lowe, et al., “Recognising panoramas.” in ICCV,

vol. 3, 2003, p. 1218.

[2] M. Shridhar and K.-Y. Neo, “Monocular slam for real-time applica-

tions on mobile platforms,” 2015.

[3] Z. Zhang and A. R. Hanson, “3d reconstruction based on homography

mapping,” Proc. ARPA96, pp. 1007–1012, 1996.

[4] Z. Pan, X. Fang, J. Shi, and D. Xu, “Easy tour: a new image-based
virtual tour system,” in Proceedings of the 2004 ACM SIGGRAPH
international conference on Virtual Reality continuum and its appli-
cations in industry. ACM, 2004, pp. 467–471.

[5] C.-Y. Tang, Y.-L. Wu, P.-C. Hu, H.-C. Lin, and W.-C. Chen, “Self-
calibration for metric 3d reconstruction using homography.” in MVA,
2007, pp. 86–89.

[6] D. Capel, “Image mosaicing,” in Image Mosaicing and Super-

resolution. Springer, 2004, pp. 47–79.

[7] R. Szeliski, “Image alignment and stitching: A tutorial,” Foundations
and Trends R(cid:13) in Computer Graphics and Vision, vol. 2, no. 1, pp.
1–104, 2006.

[8] B. D. Lucas and T. Kanade, “An iterative image registration technique
with an application to stereo vision,” in Proceedings of
the 7th
International Joint Conference on Artiﬁcial Intelligence - Volume
2, ser. IJCAI’81.
San Francisco, CA, USA: Morgan Kaufmann
Publishers Inc., 1981, pp. 674–679.

[9] S. Baker and I. Matthews, “Lucas-kanade 20 years on: A unifying
framework,” International journal of computer vision, vol. 56, no. 3,
pp. 221–255, 2004.

[10] G. D. Evangelidis and E. Z. Psarakis, “Parametric image alignment
using enhanced correlation coefﬁcient maximization,” IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, vol. 30, no. 10,
pp. 1858–1865, 2008.

[11] Q. Yan, Y. Xu, X. Yang, and T. Nguyen, “Heask: Robust homography
estimation based on appearance similarity and keypoint correspon-
dences,” Pattern Recognition, vol. 47, no. 1, pp. 368–387, 2014.
[12] S. Lucey, R. Navarathna, A. B. Ashraf, and S. Sridharan, “Fourier
lucas-kanade algorithm,” IEEE transactions on pattern analysis and
machine intelligence, vol. 35, no. 6, pp. 1383–1396, 2013.

[13] E. Mu˜noz, P. M´arquez-Neila, and L. Baumela, “Rationalizing efﬁcient
compositional image alignment,” International Journal of Computer
Vision, vol. 112, no. 3, pp. 354–372, 2015.

[14] D. G. Lowe, “Distinctive image features from scale-invariant key-
points,” International journal of computer vision, vol. 60, no. 2, pp.
91–110, 2004.

[15] M. A. Fischler and R. C. Bolles, “Random sample consensus: A
paradigm for model ﬁtting with applications to image analysis and
automated cartography,” Commun. ACM, vol. 24, no. 6, pp. 381–395,
June 1981.

[16] F.-l. Wu and X.-y. Fang, “An improved ransac homography algorithm
for feature based image mosaic,” in Proceedings of the 7th WSEAS
International Conference on Signal Processing, Computational Geom-
etry & Artiﬁcial Vision. World Scientiﬁc and Engineering Academy
and Society (WSEAS), 2007, pp. 202–207.

[17] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “Orb: An efﬁcient
alternative to sift or surf,” in Computer Vision (ICCV), 2011 IEEE
international conference on.

IEEE, 2011, pp. 2564–2571.

[18] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid, “Deepﬂow:
Large displacement optical ﬂow with deep matching,” in Proceedings
of the IEEE International Conference on Computer Vision, 2013, pp.
1385–1392.

[19] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox,
“Flownet 2.0: Evolution of optical ﬂow estimation with deep net-
works,” arXiv preprint arXiv:1612.01925, 2016.

[20] P. Fischer, A. Dosovitskiy, E. Ilg, P. H¨ausser, C. Hazırbas¸, V. Golkov,
P. van der Smagt, D. Cremers, and T. Brox, “Flownet: Learning optical
ﬂow with convolutional networks,” arXiv preprint arXiv:1504.06852,
2015.

[21] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid, “Deepmatch-
ing: Hierarchical deformable dense matching,” International Journal
of Computer Vision, vol. 120, no. 3, pp. 300–323, 2016.

[22] H. Altwaijry, A. Veit, S. J. Belongie, and C. Tech, “Learning to detect
and match keypoints with deep architectures.” in BMVC, 2016.
[23] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a
single image using a multi-scale deep network,” in Advances in neural
information processing systems, 2014, pp. 2366–2374.

[24] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Deep image homog-

raphy estimation,” arXiv preprint arXiv:1606.03798, 2016.

[25] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsuper-
vised learning of depth and ego-motion from video,” arXiv preprint
arXiv:1704.07813, 2017.

[26] S. Baker, A. Datta, and T. Kanade, “Parameterizing homographies,”

Technical Report CMU-RI-TR-06-11, 2006.

[27] K. Simonyan and A. Zisserman, “Very deep convolutional networks
for large-scale image recognition,” arXiv preprint arXiv:1409.1556,
2014.

[28] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Is l2 a good loss function
for neural networks for image processing?” ArXiv e-prints, vol. 1511,
2015.

[29] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,
and K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer
parameters and¡ 0.5 mb model size,” arXiv preprint arXiv:1602.07360,
2016.

[30] R. I. Hartley and A. Zisserman, Multiple View Geometry in Computer
Cambridge University Press, ISBN: 0521540518,

Vision, 2nd ed.
2004.

[31] G. H. Golub and C. Reinsch, “Singular value decomposition and least
squares solutions,” Numerische mathematik, vol. 14, no. 5, pp. 403–
420, 1970.

[32] T. Papadopoulo and M. I. Lourakis, “Estimating the jacobian of the
singular value decomposition: Theory and applications,” in European
Conference on Computer Vision. Springer, 2000, pp. 554–570.
[33] R. Hartley and A. Zisserman, Multiple view geometry in computer

vision. Cambridge university press, 2003.

[34] M. Jaderberg, K. Simonyan, A. Zisserman, et al., “Spatial transformer
networks,” in Advances in Neural Information Processing Systems,
2015, pp. 2017–2025.

[35] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, et al., “Tensorﬂow: Large-
scale machine learning on heterogeneous distributed systems,” arXiv
preprint arXiv:1603.04467, 2016.

[36] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

Unsupervised Deep Homography: A Fast and Robust Homography
Estimation Model

Ty Nguyen∗, Steven W. Chen∗, Shreyas S. Shivakumar, Camillo J. Taylor, Vijay Kumar

8
1
0
2
 
b
e
F
 
1
2
 
 
]

V
C
.
s
c
[
 
 
3
v
6
6
9
3
0
.
9
0
7
1
:
v
i
X
r
a

In this

Abstract— Homography estimation between multiple aerial
images can provide relative pose estimation for collaborative
autonomous exploration and monitoring. The usage on a robotic
system requires a fast and robust homography estimation
algorithm.
study, we propose an unsupervised
learning algorithm that trains a Deep Convolutional Neural
Network to estimate planar homographies. We compare
feature-based and
the proposed algorithm to traditional
direct methods, as well as a corresponding supervised
learning
results demonstrate
that compared to traditional approaches, the unsupervised
algorithm achieves faster inference speed, while maintaining
comparable or better accuracy and robustness to illumination
variation. In addition, our unsupervised method has superior
adaptability and performance compared to the corresponding
supervised deep learning method. Our image dataset and a
Tensorﬂow implementation of our work are available at htt ps :
//github.com/tynguyen/unsupervisedDeepHomographyRAL2018.

algorithm. Our

empirical

I. INTRODUCTION

A homography is a mapping between two images of a
planar surface from different perspectives. They play an
essential role in robotics and computer vision applications
such as image mosaicing [1], monocular SLAM [2], 3D
camera pose reconstruction [3] and virtual touring [4], [5].
For example, homographies are applicable in scenes viewed
at a far distance by an arbitrary moving camera [6], which
are the situations encountered in UAV imagery. However, to
work well in the aerial multi-robot setting, the homography
estimation algorithm needs to be reliable and fast.

The two traditional approaches for homography estimation
are direct methods and feature-based methods [7]. Direct
methods, such as the seminal Lucas-Kanade algorithm [8],
use pixel-to-pixel matching by shifting or warping the images
relative to each other and comparing the pixel
intensity
values using an error metric such as the sum of squared dif-
ferences (SSD). They initialize a guess for the homography
parameters and use a search or optimization technique such
as gradient descent to minimize the error function [9]. The
robustness of direct methods can be improved by using dif-
ferent performance criterion such as the enhanced correlation
coefﬁcient (ECC) [10], integrating feature-based methods
with direct methods [11], or by representing the images in
the Fourier domain [12]. In addition, the speed of direct
methods can be increased by using efﬁcient compositional
image alignment schemes [13].

The authors are with GRASP Lab, University of Pennsylvania, Philadel-
phia, PA 19104, USA, {tynguyen, chenste, sshreyas,
cjtaylor, kumar}@seas.upenn.edu.

∗: The authors have equal contributions

Fig. 1: Above: Synthetic data; Below: Real data; Homog-
raphy estimation results from the unsupervised neural net-
work. Red represents the ground truth correspondences,
and yellow represents the estimated correspondences. These
images depict an example of large levels of displacement
and illumination shifts in which feature-based, direct and/or
supervised learning methods fail.

The second approach are feature-based methods. These
methods ﬁrst extract keypoints in each image using local
invariant features (e.g. Scale Invariant Feature Transform
(SIFT) [14]). They then establish a correspondence between
the two sets of keypoints using feature matching, and use
RANSAC [15] to ﬁnd the best homography estimate. While
these methods have better performance than direct methods,
they can be inaccurate when they fail to detect sufﬁcient
keypoints, or produce incorrect keypoint correspondences
due to illumination and large viewpoint differences between
the images [16]. In addition, these methods are signiﬁcantly
faster than direct methods but can still be slow due to the
computation of the features, leading to the development of
other feature types such as Oriented FAST and Rotated
BRIEF (ORB) [17] which are more computationally efﬁcient
than SIFT, but have worse performance.

Inspired by the success of data-driven Deep Convolutional
Neural Networks (CNN) in computer vision, there has been
an emergence of CNN approaches to estimating optical
ﬂow [18], [19], [20], dense matching [21], [22], depth esti-
mation [23], and homography estimation [24]. Most of these
works, including the most relevant work on homography
estimation,
the estimation problem as a supervised
learning task. These supervised approaches use ground truth
labels, and as a result are limited to synthetic datasets where
the ground truth can be generated for free, or require costly
labeling of real-world data sets.

treat

Our work develops an unsupervised, end-to-end, deep

learning algorithm to estimate homographies. It improves
upon these prior traditional and supervised learning methods
by minimizing a pixel-wise intensity error metric that does
not need ground truth data. Unlike the hand-crafted feature-
based approaches, or the supervised approach that needs
costly labels, our model is adaptive and can easily learn
good features speciﬁc to different data sets. Furthermore, our
framework has fast inference times since it is highly parallel.
These adaptive and speed properties make our unsupervised
networks especially suitable for real world robotic tasks, such
as stitching UAV images.

We demonstrate that our unsupervised homography esti-
mation algorithm has comparable or better accuracy, and bet-
ter inference speed, than feature-based, direct, and supervised
deep learning methods on synthetic and real-world UAV data
sets. In addition, we demonstrate that it can handle large
displacements (∼ 65% image overlap) with large illumination
variation. Fig. 1 illustrates qualitative results on these data
sets, where our unsupervised method is able to estimate the
homography whereas the other approaches cannot.

it

Our unsupervised algorithm is a hybrid approach that
combines the strengths of deep learning with the strengths of
both traditional direct methods and feature-based methods.
It is similar to feature-based methods because it also relies
on features to compute the homography estimates, but it
differs in that
learns the features rather than deﬁning
them. It is also similar to the direct methods because the
error signal used to drive the network training is a pixel-
wise error. However, rather than performing an online op-
timization process, it transfers the computation ofﬂine and
”caches” the results through these learned features. Similar
unsupervised deep learning approaches have been successful
in computer vision tasks such as monocular depth and camera
motion estimation [25], indicating that our framework can
be scaled to tackle general nonlinear motions such as those
encountered in optical ﬂow.

II. PROBLEM FORMULATION

We assume that images are obtained by a perspective pin-
hole camera and present points by homogeneous coordinates,
so that a point (u, v)T is represented as (u, v, 1)T and a point
(x, y, z)T is equivalent to the point (x/z, y/z, 1)T . Suppose that
x = (u, v, 1)T and x(cid:48) = (u(cid:48), v(cid:48), 1)T are two points. A planar
projective transformation or homography that maps x ↔ x(cid:48)
is a linear transformation represented by a non-singular 3 × 3
matrix H such that:





 =







u(cid:48)
v(cid:48)
1

h11
h21
h31

h12
h22
h32







u
v
 Or

1

h13
h23
h33

x(cid:48) = Hx

(1)

Since H can be multiplied by an arbitrary non-zero scale
factor without altering the projective transformation, only
the ratio of the matrix elements is signiﬁcant, leaving H
eight independent ratios corresponding to eight degrees of
freedom. This mapping equation can also represented by two

equations:

u(cid:48) =

h11u + h12v + h13
h31u + h32v + h33

; v(cid:48) =

h21u + h22v + h23
h31u + h32v + h33

(2)

The problem of ﬁnding the homography induced by two
images IA and IB is to ﬁnd a homography HAB such that
Eqn. (1) holds for all points in the overlapping of the two
images.

III. SUPERVISED DEEP HOMOGRAPHY MODEL

The deep learning approach most similar to our work
is the Deep Image Homography Estimation [24]. In this
work, DeTone et al. use supervised learning to train a deep
neural network on a synthetic data set. They use the 4-
point homography parameterization H4pt [26] rather than
the conventional 3 × 3 parameterization H. Suppose that
uA
k , 1)T for k = 1, 2, 3, 4 are
k = (uA
4 ﬁxed points in image IA and IB respectively, such that
uk
2 = Huk
k . Then H4pt is
the 4 × 2 matrix of points (∆uk, ∆vk). Both parameterizations
are equivalent since there is a one-to-one correspondence
between them.

1. Let ∆uk = uB

k , 1)T and uB

k , ∆vk = vB

k = (uB

k − uA

k − vA

k , vA

k , vB

In a deep learning framework though,

this parameter-
ization is more suitable than the 3 × 3 parameterization
H because H mixes the rotation,
translation, scale, and
shear components of the homography transformation. The
rotation and shear components tend to have a much smaller
magnitude than the translation component, and as a result
although an error in their values can greatly impact H, it will
have a small effect on the L2 loss function of the elements of
H, which is detrimental for training the neural network. In
addition, the high variance in the magnitude of the elements
of the 3 × 3 homography matrix makes it difﬁcult to enforce
H to be non-singular. The 4-point parameterization does not
suffer from these problems.

The network architecture is based on VGGNet [27], and is
depicted in Fig. 2(a). The network input is a batch of image
patch pairs. The patch pairs are generated by taking a full-
sized image, cropping a square patch PA at a random position
p, perturbing the four corners of by a random value within
[−ρ, ρ] to generate a homography HAB, applying (HAB)−1 to
the full-sized image, and then cropping a square patch PB of
the same size and at the same location as the patch PA from
the warped image. These image patches are used to avoid
strange border effects near the edges during the synthetic
data generation process, and to standardize the network input
size. The applied homography HAB is saved in the 4 point
parameterization format, H∗
4pt . The network outputs a 4 point
parameterization estimate ˜H4pt .

The error signal used for gradient backpropagation is the
Euclidean L2 norm, denoted as LH , of the estimated 4-point
homography ˜H4pt versus the ground truth H∗

4pt :

LH =

|| ˜H4pt − H∗

4pt ||2
2

1
2

(3)

Fig. 2: Overview of homography estimation methods; (a) Benchmark supervised deep learning approach; (b) Feature-based
methods; and (c) Our unsupervised method. DLT: direct linear transform; PSGG: parameterized sampling grid generator;
DS: differentiable sampling.

IV. UNSUPERVISED DEEP HOMOGRAPHY MODEL

While the supervised deep learning method has promising
results, it is limited in real world applications since it requires
ground truth labels. Drawing inspiration from traditional
direct methods for homography estimation, we can deﬁne an
analogous loss function. Given an image pair IA(x) and IB(x)
with discrete pixel locations represented by homogeneous
coordinates {xi = (xi, yi, 1)T }, we want our network to output
˜H4pt that minimizes the average L1 pixel-wise photometric
loss

LPW =

|IA(H (xi)) − IB(xi)|

(4)

1
|xi| ∑

xi

where ˜H4pt deﬁnes the homography transformation H (xi).
We chose the L1 error versus the L2 error because previous
work has observed that it is more suitable for image align-
ment problems [28], and empirically we found the network
to be easier to train with the L1 error. This loss function is
unsupervised since there is no ground truth label. Similar to
the supervised case, we choose the 4-point parameterization
which is more suitable than the 3 × 3 parameterization.

In order to compare our unsupervised deep learning al-
gorithm with the supervised algorithm, we use the same
VGGNet architecture to output the ˜H4pt . Fig. 2(c) depicts
our unsupervised learning model. The regression module
represents the VGGNet architecture and is shared by both the
supervised and unsupervised methods. Although we do not
investigate other possible architectures, different regression
models such as SqueezeNet [29] may yield better perfor-
mance due to advantages in size and computation require-

ments. The second half of Fig. 2(c) represents the main
contribution of this work, which consists of the differentiable
layers that allow the network to be successfully trained with
the loss function (4).

Using the pixel-wise photometric loss function yields ad-
ditional training challenges. First, every operation, including
the warping operation H (xi), must remain differentiable
to allow the network to be trained via backpropagation.
Second, since the error signal depends on differences in
image intensity values rather than the differences in the
homography parameters, training the deep network is not
necessarily as easy or stable. Another implication of using
a pixel-wise photometric loss function is the implied as-
sumption that lighting and contrast between the input images
remains consistent. In traditional direct methods such as
ECC,
this appearance variation problem is addressed by
modifying the loss function or preprocessing the images. In
our unsupervised algorithm, we standardize our images by
the mean and variance of the intensities of all pixels in our
training dataset, perform data augmentation by injecting ran-
dom illumination shifts, and use the standard L1 photometric
loss. We found that even without modifying the loss function,
our deep neural network is still able to learn to be invariant
to illumination changes.

A. Model Inputs

The input to our model consists of three parts. The ﬁrst
part is a 2-channel image of size 128 × 128 × 2 which is
the stack of PA and PB - two patches cropped from the two
images IA and IB. The second part is the four corners in IA,

denoted as CA
necessary for warping.

4pt . Image IA is also part of the input as it is

ˆbi is a vector with 2 elements representing the last column
of Ai subtracted from both sides of the equation,

(5)

C. Spatial Transformation Layer

B. Tensor Direct Linear Transform

We develop a Tensor Direct Linear Transform (Tensor
DLT) layer to compute a differentiable mapping from the 4-
point parameterization ˜H4pt to ˜H, the 3 × 3 parameterization
of homography. This layer essentially applies the DLT algo-
rithm [30] to tensors, while remaining differentiable to allow
backpropagation during training. As shown in Fig. 2(c), the
input to this layer are the corresponding corners in the image
pairs CA
4pt , and the output is the estimate of the 3×3
homography parameterization ˜H.

4pt and ˜CB

The DLT algorithm is used to solve for the homography
matrix H given a set of four point correspondences [30]. Let
H be the homography induced by a set of four 2D to 2D
correspondences, xi ↔ x(cid:48)
i. According to the deﬁnition of a
homography given in Eqn. (1), x(cid:48)
i ∼ Hxi. This relation can
also be expressed as x(cid:48)
i × Hxi = 0.



Let h jT be the j-th row of H, then:
xT
i h1
xT
i h2
xT
i h3

h1T xi
h2T xi
h3T xi
where h j is the column vector representation of h jT .

Hxi =

 =















(6)

 = 0





i, v(cid:48)

Let x(cid:48)

i = (u(cid:48)

x(cid:48)
i × Hxi =

i, 1)T , then:
ixT
i h3 − xT
v(cid:48)
i h2
ixT
xT
i h1 − u(cid:48)
i h3
ixT
ixT
i h2 − v(cid:48)
u(cid:48)
i h1
This equation can be rewritten as:

3×1 −xT
0T
i
xT
i
ixT
−v(cid:48)
i











(7)

 = 0.

h1
h2
h3

ixT
v(cid:48)
i
0T
ixT
3×1 −u(cid:48)
i
0T
ixT
u(cid:48)
i
3×1
which has the form A(3)
i h = 0 for each i = 1, 2, 3, 4 corre-
spondence pair, where A(3)
is a 3 × 9 matrix, and h is a
vector with 9 elements consisting of the entries of H. Since
the last row in A(3)
is dependent on the other rows, we are
left with two linear equations Aih = 0 where Ai is the ﬁrst
2 rows of A(3)

.

i

i



Given a set of 4 correspondences, we can create a system
of equations to solve for h and thus H. For each i, we can
stack Ai to form Ah = 0. Solving for h results in ﬁnding
a vector in the null space of A. One popular approach is
singular value decomposition (SVD) [31], which is a dif-
ferentiable operation. However, taking the gradients in SVD
has high time complexity and has practical implementation
issues [32]. An alternative solution to this problem is to make
the assumption that the last element of h3, which is H33 is
equal to 1 [33].

With this assumption and the fact

that xi = (ui, vi, 1),
we can rewrite Eqn. (7) in the form ˆAi ˆh = ˆbi for each
i = 1, 2, 3, 4 correspondence points where ˆAi is the 2 × 8
matrix representing the ﬁrst 8 columns of Ai,
v(cid:48)
v(cid:48)
ivi
iui
iui −u(cid:48)
0 −u(cid:48)
ivi

0 −ui −vi −1
1

(cid:20) 0
ui

ˆAi =

0
vi

(cid:21)

0

0

,

i

ˆbi = [−v(cid:48)

i, u(cid:48)

i]T ,

and ˆh is a vector consisting of the ﬁrst 8 elements of h (with
H33 omitted).

By stacking these equations, we get:

ˆA ˆh = ˆb,
(8)
Eqn. (8) has a desirable form because ˆh, and thus H, can be
solved for using ˆA+, the pseudo-inverse of ˆA. This operation
is simple and differentiable with respect to the coordinates
of xi and x(cid:48)
i. In addition, the gradients are easier to calculate
than for SVD.

This approach may still fail if the correspondence points
are collinear: if three of the correspondence points are on the
same line, then solving for H is undetermined. We alleviate
this problem by ﬁrst making the initial guess of H4pt to be
zero, implying that ˜CB
4pt . We then set a small learning
4pt does not
rate such that after each training iteration,
move too far away from CA

4pt ∼ CA

˜CB

4pt .

The next layer applies the 3 × 3 homography estimate
˜H output by the Tensor DLT to the pixel coordinates xi
of image IA in order to get warped coordinates H (xi).
These warped coordinates are necessary in computing the
photometric loss function in Eqn. (4) that will train our
neural network. In addition to warping the coordinates, this
layer must also be differentiable so that the error gradients
can ﬂow through via backpropagation. We thus extend the
Spatial Transformer Layer introduced in [34] by applying it
to homography transformations.

This layer performs an inverse warping in order to avoid
holes in the warped image. This process consists of 3 steps:
(1) Normalized inverse computation ˜Hinv of the homogra-
phy estimate; (2) Parameterized Sampling Grid Generator
(PSGG); and (3) Differentiable Sampling (DS).

The ﬁrst step, computing a normalized inverse, involves
normalizing the height and width coordinates of images IA
and IB into a range such that −1 ≤ ui, vi ≤ 1 and −1 ≤ u(cid:48)
i ≤
1. Thus given a 3 × 3 homography estimate ˜H, the inverse
˜Hinv used for warping is computed as follows:

i, v(cid:48)

˜Hinv = M−1 ˜H−1M


W (cid:48)/2
0
0

0

W (cid:48)/2
H(cid:48)/2 H(cid:48)/2





0

1

where M =



with W (cid:48) and H(cid:48) are the width and height of the IB.

The second step (PSGG) creates a grid G = {Gi} of
the same size as the second image IB. Each grid element
Gi = (u(cid:48)
i) corresponds to pixels of the second image
IB. Applying the inverse homography ˜Hinv to these grid
coordinates provides a grid of pixels in the ﬁrst image IA.

i, v(cid:48)





ui
vi
1


 = Hinv(Gi) = ˜Hinv









u(cid:48)
i
v(cid:48)
i
1

(9)

Based on the sampling points Hinv(Gi) computed from
PSGG, the last step (DS) produces a sampled warped im-
age V of size H(cid:48) × W (cid:48) with C channels, where V (xi) =
IA(H (xi)).

The sampling kernel k(·) is applied to the grid Hinv(Gi)

and the resulting image V is deﬁned as

V C
i =

Ic
nmk(ui − m; Φu)k(vi − n; Φv),

H
∑
n

W
∑
m

∀i ∈ [1...H(cid:48)W (cid:48)]

, ∀c ∈ [1..C]

(10)

where H,W are the height and width of the input image IA,
Φu and Φv are the parameters of k(·) deﬁning the image
interpolation. Ic
nm is the value at location (n, m) in channel
c of the input image, and V c
is the value of the output
i
pixel at location (ui, vi) in channel c. Here, we use bilinear
interpolation such that the Eqn. (10) becomes

V C
i =

Ic
nm max(0, 1 − |ui − m|) max(0, 1 − |vi − n|)

H
∑
n

W
∑
m

(11)

To allow backpropagation of the loss function, gradients with
respect to I and G for bilinear interpolation are deﬁned as

∂V c
i
∂ Ic
nm

=

H
∑
n

W
∑
m

∂V c
i
∂ ui =

H
∑
n

W
∑
m

∂V c
i
∂ vi =

H
∑
n

W
∑
m

max(0, 1 − |ui − m|) max(0, 1 − |vi − n|) (12)

Ic
nm max(0, 1 − |vi − n|)

Ic
nm max(0, 1 − |ui − m|)











0 if |m − ui| ≥ 1
1 if m ≥ ui
−1 if m < ui

0 if |n − vi| ≥ 1
1 if n ≥ vi
−1 if n < vi

(13)

(14)

This allows backpropagation of the loss gradients using the
chain rule because ∂ ui
can be easily derived from
∂ h jk
Eqn. 2.

and ∂ vi
∂ h jk

V. EVALUATION RESULTS

The intended use case for our algorithm is in estimating
homographies for aerial multi-robot systems applications
such as image mosaicing and collision avoidance. As a re-
sult, we demonstrate our unsupervised algorithm’s accuracy,
inference speed, and robustness to illumination variation
relative to SIFT, ORB, ECC and the supervised deep learning
method. We evaluate these methods on a synthetic dataset
similar to the dataset used in [24], and on a real-world aerial
image dataset. Since ORB’s performance is inferior to that
of SIFT, we only report ORB’s performance in Fig. 4 and
omit it in the remaining ﬁgures.

Both the supervised and unsupervised approaches use
the VGGNet architecture to generate homography estimates
˜H4pt. The deep learning approaches are implemented in Ten-
sorﬂow [35] using stochastic gradient descent with a batch
size of 128, and an Adam Optimizer [36] with, β1 = 0.9,
β2 = 0.999 and ε = 10−8. We empirically chose the initial

learning rate for the supervised algorithm and unsupervised
algorithm to be 0.0005 and 0.0001 respectively.

The ECC direct method is a standard Python OpenCV im-
plementation while the feature-based approaches are Python
OpenCV implementations of SIFT RANSAC and ORB
RANSAC. We found that in our synthetic dataset, using
all detected features gives better performance, while in our
aerial dataset, choosing the 50 best features is superior. These
feature pairs are then used to calculate the homography using
RANSAC with a threshold of 5 pixels. For the ECC method,
we use identity matrix as the initialization and set 1000 as
the maximum number of iterations.

A. Synthetic Data Results

This section analyzes the performance proﬁle of the Un-
supervised, Supervised, SIFT, and ECC methods on our
synthetic dataset. We want to test how well our approach
performs under illumination variation and large image dis-
placement.

To account for illumination variation, we globally stan-
dardize our images based on the mean and variance of pixel
intensities of all images in our training dataset. We addition-
ally inject random color, brightness and gamma shifts during
the training. We do not utilize any further preprocessing and
use the L1 photometric loss function. To highlight the effect
of displacement amount on each method, we break down the
accuracy performance in terms of: 85% image overlap (small
displacement), 75% image overlap (moderate displacement),
and 65% image overlap (large displacement). We follow the
synthetic data generation process on the MS-COCO dataset
used in [24]. The amount of image overlap is controlled by
the point perturbation parameter ρ. The evaluation metric is
the 4pt-Homography RMSE from Eqn. (3) comparing the
estimated homography to the ground truth homography.

We train the deep networks from scratch for 300, 000
iterations over ∼ 30 hours, using two GPUs. This long
training procedure only needs to be performed once, as
the resulting model can be used as an initial pre-trained
model for other data sets. We observed that the supervised
model started overﬁtting after 150, 000 iterations so stopped
training early. SIFT, ORB and ECC estimated homographies
using the full images, while the deep learning methods are
only given access to the small patches (∼ 21% pixels).
This disadvantages our methods, and would result in better
performance for the traditional methods, at the expense of
slower running times.

Fig. 3 displays the results of each method broken down
by overlap and performance percentile. We break down the
results by performance percentile to illustrate the various per-
formance proﬁles of each method. Speciﬁcally, SIFT tends
to do very well 60% of the time, but in the worst 40% of the
time it performs very poorly, sometimes completely failing
to detect enough features to estimate a homography. On the
other hand, the deep learning methods tends to have much
more consistent performance, which can be more desireable
in practical applications such as using homographies for
collision avoidance for aerial multi-robot systems. Both the

Fig. 3: Synthetic 4pt-Homography RMSE (lower is better). Unsupervised has comparable performance with the supervised
method and performs better than the other approaches especially when the displacement is large.

B. Aerial Dataset Results

This section analyzes the performance proﬁle of each
method on a representative dataset of aerial imagery cap-
tured by a UAV. In addition to accuracy performance, an
equally important consideration for real world application is
inference speed. As a result, we also discuss the performance
to speed tradeoffs of each method.

Our aerial dataset contains 350 image pairs resized to
240 × 320, captured by a DJI Phantom 3 Pro platform in
Yardley, Pensylvania, USA in 2017. We divided it into 300
train and 50 test samples. We did not label the train set,
but for evaluation purposes, we manually labeled the ground
truth by picking 4 pairs of correspondences for each test
sample. We also randomly inject illumination noise in both
the training and testing sets. The evaluation metrics are the
same for the synthetic data. To reduce training time, we
ﬁnetune the neural networks on the aerial image data. Our
unsupervised algorithm can directly use the aerial dataset
image pairs. However, since we do not have ground truth
homography labels, we have to perform a similar synthetic
data generation process as in the synthetic dataset in order
to ﬁnetune the supervised neural network. We ﬁne tune both
models over 150, 000 iterations for roughly 15 hours with
data augmentation.

Fig. 6 displays the performance proﬁle for the Unsuper-
vised, Supervised, SIFT, and ECC methods. Fig 4 displays
the speed and performance tradeoff for these methods, and
additionally the featured based method ORB. The feature-
based methods are tested on a 16-core Intel Xeon CPU, and
the deep learning methods are tested on the same CPU and
an NVIDIA Titan X GPU. The closer to the lower left hand
corner, the better the performance and faster the runtime.

Both Figs. 6 and 4 demonstrate that our unsupervised al-
gorithm has the best performance of all methods. In addition,
Fig. 4 also shows that our unsupervised method on the GPU
has both the best performance and the fastest inference times.
SIFT has the second best performance after our unsupervised
algorithm, but has a much slower runtime (approximately
200 times slower). ORB has a faster runtime than SIFT, but at
the expense of poorer performance. The ECC direct method
approach has the worst performance and runtime of all the
methods. A qualitative example where both SIFT and ECC
fail to deliver a good result while our method succeeds is
illustrated in Fig. 5.

Fig. 4: Speed Versus Performance Tradeoff. Lower left is
the computational
better. Sufﬁxes GPU and CPU reﬂect
resource. All the feature-based methods are run on the CPU.
The unsupervised network run on the GPU dominates all the
other methods by having both the highest throughput and best
performance.

learning methods and the feature-based methods outperform
the direct method (ECC).

Interestingly, whereas direct method ECC has problems
with illumination variation and large displacement, our un-
supervised method is able to handle these scenarios even
though it uses photometric loss functions. One potential
hypothesis is that our method can be viewed as a hybrid
between direct methods and feature based methods. The
large receptive ﬁeld of neural networks may allow it to
handle large image displacement better than a direct method.
In addition, whereas image gradients are used to update
homography parameters in direct methods, with neural net-
works, these gradients are used to update network parameters
which correspond to improving learned features. Finally,
direct methods are an online optimization process that use
gradients from a single pair of images, whereas training a
deep network is an ofﬂine optimization process that averages
gradients across multiple images. Injecting noise into this
training process can further improve robustness to different
appearance variations. Understanding the relationship be-
tween the neural network and photometric loss functions is
an important direction for future work.

(a) Unsupervised (Success, RMSE = 15.6)

(b) Unsupervised (Success, RMSE = 4.50)

(c) SIFT (Fail, RMSE = 105.2)

(d) SIFT (Success, RMSE = 6.06)

(e) ECC(Success, RMSE = 66.4)

(f) ECC(Success, RMSE = 48.10)

Fig. 5: Qualitative visualization of estimation methods on aerial dataset. Left: hard case, right: moderate case. ECC performs
better than SIFT in the case of small displacement, but performs worse than SIFT in case of large displacement. Unsupervised
network outperforms both SIFT and ECC approaches. Supervised network is omitted due to limited space and its poor
performance on this dataset.

pervised algorithm. Our aerial dataset results highlight the
fact that even though synthetic data can be generated from
real images, a pair of synthetic images is still very different
from a pair of real images. These results demonstrate that the
independence of our unsupervised algorithm from expensive
ground truth labels has large practical implications for real-
world performance.

VI. CONCLUSIONS

We have introduced an unsupervised algorithm that trains
a deep neural network to estimate planar homographies. Our
approach outperforms the corresponding supervised network
on both synthetic and real-world datasets, demonstrating the
superiority of unsupervised learning in image warping prob-
lems. Our approach achieves faster inference speed, while
maintaining comparable or better accuracy than feature-
based and direct methods. We demonstrate that the unsu-
pervised approach is able to handle large displacements and
large illumination variations that are typically challenging
for direct approaches that use the same photometric loss
function. The speed and adaptive nature of our algorithm
makes it especially useful in aerial multi-robot applications
that can exploit parallel computation.

In this work, we do not investigate robustness against
occlusion, leaving it as future work. However, as suggested
in [24], we could potentially address this issue by using data
augmentation techniques such as artiﬁcially inserting random

Fig. 6: 4pt-homography RMSE on aerial images (lower is
better). Unsupervised outperforms other approaches signiﬁ-
cantly.

One of the most interesting results is that while the super-
vised and unsupervised approaches performed comparably
on the synthetic data, the supervised approach had drastically
poorer performance on the aerial image dataset. This shift
is due to the fact that ground truth labels are not available
for our aerial dataset. The generalization gap from synthetic
(train) to real (test) data is an important problem in machine
learning. The best practical approach is to additionally ﬁne-
tune the model on the new distribution of data. In a robotic
ﬁeld experiment, this can be achieved by ﬂying the UAV to
collect a few sample images and ﬁne-tuning on those images.
However, this ﬁne-tuning is only possible with our unsu-

occluding shapes into the training images. Another direction
for future work is investigating different improvements to
achieve sub-pixel accuracy in the top 30% performance
percentile.

Finally, our approach is easily scalable to more general
warping motions. Our ﬁndings provide additional evidence
for applying deep learning methods, speciﬁcally unsuper-
vised learning, to various robotic perception problems such
as stereo depth estimation, or visual odometry. Our insights
on estimating homographies with unsupervised deep neural
network approaches provide an initial step in a structured
progression of applying these methods to larger problems.

VII. ACKNOWLEDGEMENTS

We gratefully acknowledge the support of ARL grants
W911NF-08-2-0004 and W911NF-10-2-0016, ARO grant
W911NF-13-1-0350, N00014-14-1-0510, N00014-09-1-
1051, N00014-11-1-0725, N00014-15-1-2115 and N00014-
09-1-103, DARPA grants HR001151626/HR0011516850
USDA grant 2015-67021-23857 NSF grants IIS-1138847,
IIS-1426840 CNS-1446592 CNS-1521617 and IIS-1328805,
Qualcomm Research, United Technologies, and TerraSwarm,
one of six centers of STARnet, a Semiconductor Research
Corporation program sponsored by MARCO and DARPA.
We would also like to thank Aerial Applications for the
UAV data set.

REFERENCES

[1] M. Brown, D. G. Lowe, et al., “Recognising panoramas.” in ICCV,

vol. 3, 2003, p. 1218.

[2] M. Shridhar and K.-Y. Neo, “Monocular slam for real-time applica-

tions on mobile platforms,” 2015.

[3] Z. Zhang and A. R. Hanson, “3d reconstruction based on homography

mapping,” Proc. ARPA96, pp. 1007–1012, 1996.

[4] Z. Pan, X. Fang, J. Shi, and D. Xu, “Easy tour: a new image-based
virtual tour system,” in Proceedings of the 2004 ACM SIGGRAPH
international conference on Virtual Reality continuum and its appli-
cations in industry. ACM, 2004, pp. 467–471.

[5] C.-Y. Tang, Y.-L. Wu, P.-C. Hu, H.-C. Lin, and W.-C. Chen, “Self-
calibration for metric 3d reconstruction using homography.” in MVA,
2007, pp. 86–89.

[6] D. Capel, “Image mosaicing,” in Image Mosaicing and Super-

resolution. Springer, 2004, pp. 47–79.

[7] R. Szeliski, “Image alignment and stitching: A tutorial,” Foundations
and Trends R(cid:13) in Computer Graphics and Vision, vol. 2, no. 1, pp.
1–104, 2006.

[8] B. D. Lucas and T. Kanade, “An iterative image registration technique
with an application to stereo vision,” in Proceedings of
the 7th
International Joint Conference on Artiﬁcial Intelligence - Volume
2, ser. IJCAI’81.
San Francisco, CA, USA: Morgan Kaufmann
Publishers Inc., 1981, pp. 674–679.

[9] S. Baker and I. Matthews, “Lucas-kanade 20 years on: A unifying
framework,” International journal of computer vision, vol. 56, no. 3,
pp. 221–255, 2004.

[10] G. D. Evangelidis and E. Z. Psarakis, “Parametric image alignment
using enhanced correlation coefﬁcient maximization,” IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, vol. 30, no. 10,
pp. 1858–1865, 2008.

[11] Q. Yan, Y. Xu, X. Yang, and T. Nguyen, “Heask: Robust homography
estimation based on appearance similarity and keypoint correspon-
dences,” Pattern Recognition, vol. 47, no. 1, pp. 368–387, 2014.
[12] S. Lucey, R. Navarathna, A. B. Ashraf, and S. Sridharan, “Fourier
lucas-kanade algorithm,” IEEE transactions on pattern analysis and
machine intelligence, vol. 35, no. 6, pp. 1383–1396, 2013.

[13] E. Mu˜noz, P. M´arquez-Neila, and L. Baumela, “Rationalizing efﬁcient
compositional image alignment,” International Journal of Computer
Vision, vol. 112, no. 3, pp. 354–372, 2015.

[14] D. G. Lowe, “Distinctive image features from scale-invariant key-
points,” International journal of computer vision, vol. 60, no. 2, pp.
91–110, 2004.

[15] M. A. Fischler and R. C. Bolles, “Random sample consensus: A
paradigm for model ﬁtting with applications to image analysis and
automated cartography,” Commun. ACM, vol. 24, no. 6, pp. 381–395,
June 1981.

[16] F.-l. Wu and X.-y. Fang, “An improved ransac homography algorithm
for feature based image mosaic,” in Proceedings of the 7th WSEAS
International Conference on Signal Processing, Computational Geom-
etry & Artiﬁcial Vision. World Scientiﬁc and Engineering Academy
and Society (WSEAS), 2007, pp. 202–207.

[17] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “Orb: An efﬁcient
alternative to sift or surf,” in Computer Vision (ICCV), 2011 IEEE
international conference on.

IEEE, 2011, pp. 2564–2571.

[18] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid, “Deepﬂow:
Large displacement optical ﬂow with deep matching,” in Proceedings
of the IEEE International Conference on Computer Vision, 2013, pp.
1385–1392.

[19] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox,
“Flownet 2.0: Evolution of optical ﬂow estimation with deep net-
works,” arXiv preprint arXiv:1612.01925, 2016.

[20] P. Fischer, A. Dosovitskiy, E. Ilg, P. H¨ausser, C. Hazırbas¸, V. Golkov,
P. van der Smagt, D. Cremers, and T. Brox, “Flownet: Learning optical
ﬂow with convolutional networks,” arXiv preprint arXiv:1504.06852,
2015.

[21] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid, “Deepmatch-
ing: Hierarchical deformable dense matching,” International Journal
of Computer Vision, vol. 120, no. 3, pp. 300–323, 2016.

[22] H. Altwaijry, A. Veit, S. J. Belongie, and C. Tech, “Learning to detect
and match keypoints with deep architectures.” in BMVC, 2016.
[23] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a
single image using a multi-scale deep network,” in Advances in neural
information processing systems, 2014, pp. 2366–2374.

[24] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Deep image homog-

raphy estimation,” arXiv preprint arXiv:1606.03798, 2016.

[25] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsuper-
vised learning of depth and ego-motion from video,” arXiv preprint
arXiv:1704.07813, 2017.

[26] S. Baker, A. Datta, and T. Kanade, “Parameterizing homographies,”

Technical Report CMU-RI-TR-06-11, 2006.

[27] K. Simonyan and A. Zisserman, “Very deep convolutional networks
for large-scale image recognition,” arXiv preprint arXiv:1409.1556,
2014.

[28] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Is l2 a good loss function
for neural networks for image processing?” ArXiv e-prints, vol. 1511,
2015.

[29] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,
and K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer
parameters and¡ 0.5 mb model size,” arXiv preprint arXiv:1602.07360,
2016.

[30] R. I. Hartley and A. Zisserman, Multiple View Geometry in Computer
Cambridge University Press, ISBN: 0521540518,

Vision, 2nd ed.
2004.

[31] G. H. Golub and C. Reinsch, “Singular value decomposition and least
squares solutions,” Numerische mathematik, vol. 14, no. 5, pp. 403–
420, 1970.

[32] T. Papadopoulo and M. I. Lourakis, “Estimating the jacobian of the
singular value decomposition: Theory and applications,” in European
Conference on Computer Vision. Springer, 2000, pp. 554–570.
[33] R. Hartley and A. Zisserman, Multiple view geometry in computer

vision. Cambridge university press, 2003.

[34] M. Jaderberg, K. Simonyan, A. Zisserman, et al., “Spatial transformer
networks,” in Advances in Neural Information Processing Systems,
2015, pp. 2017–2025.

[35] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, et al., “Tensorﬂow: Large-
scale machine learning on heterogeneous distributed systems,” arXiv
preprint arXiv:1603.04467, 2016.

[36] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

Unsupervised Deep Homography: A Fast and Robust Homography
Estimation Model

Ty Nguyen∗, Steven W. Chen∗, Shreyas S. Shivakumar, Camillo J. Taylor, Vijay Kumar

8
1
0
2
 
b
e
F
 
1
2
 
 
]

V
C
.
s
c
[
 
 
3
v
6
6
9
3
0
.
9
0
7
1
:
v
i
X
r
a

In this

Abstract— Homography estimation between multiple aerial
images can provide relative pose estimation for collaborative
autonomous exploration and monitoring. The usage on a robotic
system requires a fast and robust homography estimation
algorithm.
study, we propose an unsupervised
learning algorithm that trains a Deep Convolutional Neural
Network to estimate planar homographies. We compare
feature-based and
the proposed algorithm to traditional
direct methods, as well as a corresponding supervised
learning
results demonstrate
that compared to traditional approaches, the unsupervised
algorithm achieves faster inference speed, while maintaining
comparable or better accuracy and robustness to illumination
variation. In addition, our unsupervised method has superior
adaptability and performance compared to the corresponding
supervised deep learning method. Our image dataset and a
Tensorﬂow implementation of our work are available at htt ps :
//github.com/tynguyen/unsupervisedDeepHomographyRAL2018.

algorithm. Our

empirical

I. INTRODUCTION

A homography is a mapping between two images of a
planar surface from different perspectives. They play an
essential role in robotics and computer vision applications
such as image mosaicing [1], monocular SLAM [2], 3D
camera pose reconstruction [3] and virtual touring [4], [5].
For example, homographies are applicable in scenes viewed
at a far distance by an arbitrary moving camera [6], which
are the situations encountered in UAV imagery. However, to
work well in the aerial multi-robot setting, the homography
estimation algorithm needs to be reliable and fast.

The two traditional approaches for homography estimation
are direct methods and feature-based methods [7]. Direct
methods, such as the seminal Lucas-Kanade algorithm [8],
use pixel-to-pixel matching by shifting or warping the images
relative to each other and comparing the pixel
intensity
values using an error metric such as the sum of squared dif-
ferences (SSD). They initialize a guess for the homography
parameters and use a search or optimization technique such
as gradient descent to minimize the error function [9]. The
robustness of direct methods can be improved by using dif-
ferent performance criterion such as the enhanced correlation
coefﬁcient (ECC) [10], integrating feature-based methods
with direct methods [11], or by representing the images in
the Fourier domain [12]. In addition, the speed of direct
methods can be increased by using efﬁcient compositional
image alignment schemes [13].

The authors are with GRASP Lab, University of Pennsylvania, Philadel-
phia, PA 19104, USA, {tynguyen, chenste, sshreyas,
cjtaylor, kumar}@seas.upenn.edu.

∗: The authors have equal contributions

Fig. 1: Above: Synthetic data; Below: Real data; Homog-
raphy estimation results from the unsupervised neural net-
work. Red represents the ground truth correspondences,
and yellow represents the estimated correspondences. These
images depict an example of large levels of displacement
and illumination shifts in which feature-based, direct and/or
supervised learning methods fail.

The second approach are feature-based methods. These
methods ﬁrst extract keypoints in each image using local
invariant features (e.g. Scale Invariant Feature Transform
(SIFT) [14]). They then establish a correspondence between
the two sets of keypoints using feature matching, and use
RANSAC [15] to ﬁnd the best homography estimate. While
these methods have better performance than direct methods,
they can be inaccurate when they fail to detect sufﬁcient
keypoints, or produce incorrect keypoint correspondences
due to illumination and large viewpoint differences between
the images [16]. In addition, these methods are signiﬁcantly
faster than direct methods but can still be slow due to the
computation of the features, leading to the development of
other feature types such as Oriented FAST and Rotated
BRIEF (ORB) [17] which are more computationally efﬁcient
than SIFT, but have worse performance.

Inspired by the success of data-driven Deep Convolutional
Neural Networks (CNN) in computer vision, there has been
an emergence of CNN approaches to estimating optical
ﬂow [18], [19], [20], dense matching [21], [22], depth esti-
mation [23], and homography estimation [24]. Most of these
works, including the most relevant work on homography
estimation,
the estimation problem as a supervised
learning task. These supervised approaches use ground truth
labels, and as a result are limited to synthetic datasets where
the ground truth can be generated for free, or require costly
labeling of real-world data sets.

treat

Our work develops an unsupervised, end-to-end, deep

learning algorithm to estimate homographies. It improves
upon these prior traditional and supervised learning methods
by minimizing a pixel-wise intensity error metric that does
not need ground truth data. Unlike the hand-crafted feature-
based approaches, or the supervised approach that needs
costly labels, our model is adaptive and can easily learn
good features speciﬁc to different data sets. Furthermore, our
framework has fast inference times since it is highly parallel.
These adaptive and speed properties make our unsupervised
networks especially suitable for real world robotic tasks, such
as stitching UAV images.

We demonstrate that our unsupervised homography esti-
mation algorithm has comparable or better accuracy, and bet-
ter inference speed, than feature-based, direct, and supervised
deep learning methods on synthetic and real-world UAV data
sets. In addition, we demonstrate that it can handle large
displacements (∼ 65% image overlap) with large illumination
variation. Fig. 1 illustrates qualitative results on these data
sets, where our unsupervised method is able to estimate the
homography whereas the other approaches cannot.

it

Our unsupervised algorithm is a hybrid approach that
combines the strengths of deep learning with the strengths of
both traditional direct methods and feature-based methods.
It is similar to feature-based methods because it also relies
on features to compute the homography estimates, but it
differs in that
learns the features rather than deﬁning
them. It is also similar to the direct methods because the
error signal used to drive the network training is a pixel-
wise error. However, rather than performing an online op-
timization process, it transfers the computation ofﬂine and
”caches” the results through these learned features. Similar
unsupervised deep learning approaches have been successful
in computer vision tasks such as monocular depth and camera
motion estimation [25], indicating that our framework can
be scaled to tackle general nonlinear motions such as those
encountered in optical ﬂow.

II. PROBLEM FORMULATION

We assume that images are obtained by a perspective pin-
hole camera and present points by homogeneous coordinates,
so that a point (u, v)T is represented as (u, v, 1)T and a point
(x, y, z)T is equivalent to the point (x/z, y/z, 1)T . Suppose that
x = (u, v, 1)T and x(cid:48) = (u(cid:48), v(cid:48), 1)T are two points. A planar
projective transformation or homography that maps x ↔ x(cid:48)
is a linear transformation represented by a non-singular 3 × 3
matrix H such that:





 =







u(cid:48)
v(cid:48)
1

h11
h21
h31

h12
h22
h32







u
v
 Or

1

h13
h23
h33

x(cid:48) = Hx

(1)

Since H can be multiplied by an arbitrary non-zero scale
factor without altering the projective transformation, only
the ratio of the matrix elements is signiﬁcant, leaving H
eight independent ratios corresponding to eight degrees of
freedom. This mapping equation can also represented by two

equations:

u(cid:48) =

h11u + h12v + h13
h31u + h32v + h33

; v(cid:48) =

h21u + h22v + h23
h31u + h32v + h33

(2)

The problem of ﬁnding the homography induced by two
images IA and IB is to ﬁnd a homography HAB such that
Eqn. (1) holds for all points in the overlapping of the two
images.

III. SUPERVISED DEEP HOMOGRAPHY MODEL

The deep learning approach most similar to our work
is the Deep Image Homography Estimation [24]. In this
work, DeTone et al. use supervised learning to train a deep
neural network on a synthetic data set. They use the 4-
point homography parameterization H4pt [26] rather than
the conventional 3 × 3 parameterization H. Suppose that
uA
k , 1)T for k = 1, 2, 3, 4 are
k = (uA
4 ﬁxed points in image IA and IB respectively, such that
uk
2 = Huk
k . Then H4pt is
the 4 × 2 matrix of points (∆uk, ∆vk). Both parameterizations
are equivalent since there is a one-to-one correspondence
between them.

1. Let ∆uk = uB

k , 1)T and uB

k , ∆vk = vB

k = (uB

k − uA

k − vA

k , vA

k , vB

In a deep learning framework though,

this parameter-
ization is more suitable than the 3 × 3 parameterization
H because H mixes the rotation,
translation, scale, and
shear components of the homography transformation. The
rotation and shear components tend to have a much smaller
magnitude than the translation component, and as a result
although an error in their values can greatly impact H, it will
have a small effect on the L2 loss function of the elements of
H, which is detrimental for training the neural network. In
addition, the high variance in the magnitude of the elements
of the 3 × 3 homography matrix makes it difﬁcult to enforce
H to be non-singular. The 4-point parameterization does not
suffer from these problems.

The network architecture is based on VGGNet [27], and is
depicted in Fig. 2(a). The network input is a batch of image
patch pairs. The patch pairs are generated by taking a full-
sized image, cropping a square patch PA at a random position
p, perturbing the four corners of by a random value within
[−ρ, ρ] to generate a homography HAB, applying (HAB)−1 to
the full-sized image, and then cropping a square patch PB of
the same size and at the same location as the patch PA from
the warped image. These image patches are used to avoid
strange border effects near the edges during the synthetic
data generation process, and to standardize the network input
size. The applied homography HAB is saved in the 4 point
parameterization format, H∗
4pt . The network outputs a 4 point
parameterization estimate ˜H4pt .

The error signal used for gradient backpropagation is the
Euclidean L2 norm, denoted as LH , of the estimated 4-point
homography ˜H4pt versus the ground truth H∗

4pt :

LH =

|| ˜H4pt − H∗

4pt ||2
2

1
2

(3)

Fig. 2: Overview of homography estimation methods; (a) Benchmark supervised deep learning approach; (b) Feature-based
methods; and (c) Our unsupervised method. DLT: direct linear transform; PSGG: parameterized sampling grid generator;
DS: differentiable sampling.

IV. UNSUPERVISED DEEP HOMOGRAPHY MODEL

While the supervised deep learning method has promising
results, it is limited in real world applications since it requires
ground truth labels. Drawing inspiration from traditional
direct methods for homography estimation, we can deﬁne an
analogous loss function. Given an image pair IA(x) and IB(x)
with discrete pixel locations represented by homogeneous
coordinates {xi = (xi, yi, 1)T }, we want our network to output
˜H4pt that minimizes the average L1 pixel-wise photometric
loss

LPW =

|IA(H (xi)) − IB(xi)|

(4)

1
|xi| ∑

xi

where ˜H4pt deﬁnes the homography transformation H (xi).
We chose the L1 error versus the L2 error because previous
work has observed that it is more suitable for image align-
ment problems [28], and empirically we found the network
to be easier to train with the L1 error. This loss function is
unsupervised since there is no ground truth label. Similar to
the supervised case, we choose the 4-point parameterization
which is more suitable than the 3 × 3 parameterization.

In order to compare our unsupervised deep learning al-
gorithm with the supervised algorithm, we use the same
VGGNet architecture to output the ˜H4pt . Fig. 2(c) depicts
our unsupervised learning model. The regression module
represents the VGGNet architecture and is shared by both the
supervised and unsupervised methods. Although we do not
investigate other possible architectures, different regression
models such as SqueezeNet [29] may yield better perfor-
mance due to advantages in size and computation require-

ments. The second half of Fig. 2(c) represents the main
contribution of this work, which consists of the differentiable
layers that allow the network to be successfully trained with
the loss function (4).

Using the pixel-wise photometric loss function yields ad-
ditional training challenges. First, every operation, including
the warping operation H (xi), must remain differentiable
to allow the network to be trained via backpropagation.
Second, since the error signal depends on differences in
image intensity values rather than the differences in the
homography parameters, training the deep network is not
necessarily as easy or stable. Another implication of using
a pixel-wise photometric loss function is the implied as-
sumption that lighting and contrast between the input images
remains consistent. In traditional direct methods such as
ECC,
this appearance variation problem is addressed by
modifying the loss function or preprocessing the images. In
our unsupervised algorithm, we standardize our images by
the mean and variance of the intensities of all pixels in our
training dataset, perform data augmentation by injecting ran-
dom illumination shifts, and use the standard L1 photometric
loss. We found that even without modifying the loss function,
our deep neural network is still able to learn to be invariant
to illumination changes.

A. Model Inputs

The input to our model consists of three parts. The ﬁrst
part is a 2-channel image of size 128 × 128 × 2 which is
the stack of PA and PB - two patches cropped from the two
images IA and IB. The second part is the four corners in IA,

denoted as CA
necessary for warping.

4pt . Image IA is also part of the input as it is

ˆbi is a vector with 2 elements representing the last column
of Ai subtracted from both sides of the equation,

(5)

C. Spatial Transformation Layer

B. Tensor Direct Linear Transform

We develop a Tensor Direct Linear Transform (Tensor
DLT) layer to compute a differentiable mapping from the 4-
point parameterization ˜H4pt to ˜H, the 3 × 3 parameterization
of homography. This layer essentially applies the DLT algo-
rithm [30] to tensors, while remaining differentiable to allow
backpropagation during training. As shown in Fig. 2(c), the
input to this layer are the corresponding corners in the image
pairs CA
4pt , and the output is the estimate of the 3×3
homography parameterization ˜H.

4pt and ˜CB

The DLT algorithm is used to solve for the homography
matrix H given a set of four point correspondences [30]. Let
H be the homography induced by a set of four 2D to 2D
correspondences, xi ↔ x(cid:48)
i. According to the deﬁnition of a
homography given in Eqn. (1), x(cid:48)
i ∼ Hxi. This relation can
also be expressed as x(cid:48)
i × Hxi = 0.



Let h jT be the j-th row of H, then:
xT
i h1
xT
i h2
xT
i h3

h1T xi
h2T xi
h3T xi
where h j is the column vector representation of h jT .

Hxi =

 =















(6)

 = 0





i, v(cid:48)

Let x(cid:48)

i = (u(cid:48)

x(cid:48)
i × Hxi =

i, 1)T , then:
ixT
i h3 − xT
v(cid:48)
i h2
ixT
xT
i h1 − u(cid:48)
i h3
ixT
ixT
i h2 − v(cid:48)
u(cid:48)
i h1
This equation can be rewritten as:

3×1 −xT
0T
i
xT
i
ixT
−v(cid:48)
i











(7)

 = 0.

h1
h2
h3

ixT
v(cid:48)
i
0T
ixT
3×1 −u(cid:48)
i
0T
ixT
u(cid:48)
i
3×1
which has the form A(3)
i h = 0 for each i = 1, 2, 3, 4 corre-
spondence pair, where A(3)
is a 3 × 9 matrix, and h is a
vector with 9 elements consisting of the entries of H. Since
the last row in A(3)
is dependent on the other rows, we are
left with two linear equations Aih = 0 where Ai is the ﬁrst
2 rows of A(3)

.

i

i



Given a set of 4 correspondences, we can create a system
of equations to solve for h and thus H. For each i, we can
stack Ai to form Ah = 0. Solving for h results in ﬁnding
a vector in the null space of A. One popular approach is
singular value decomposition (SVD) [31], which is a dif-
ferentiable operation. However, taking the gradients in SVD
has high time complexity and has practical implementation
issues [32]. An alternative solution to this problem is to make
the assumption that the last element of h3, which is H33 is
equal to 1 [33].

With this assumption and the fact

that xi = (ui, vi, 1),
we can rewrite Eqn. (7) in the form ˆAi ˆh = ˆbi for each
i = 1, 2, 3, 4 correspondence points where ˆAi is the 2 × 8
matrix representing the ﬁrst 8 columns of Ai,
v(cid:48)
v(cid:48)
ivi
iui
iui −u(cid:48)
0 −u(cid:48)
ivi

0 −ui −vi −1
1

(cid:20) 0
ui

ˆAi =

0
vi

(cid:21)

0

0

,

i

ˆbi = [−v(cid:48)

i, u(cid:48)

i]T ,

and ˆh is a vector consisting of the ﬁrst 8 elements of h (with
H33 omitted).

By stacking these equations, we get:

ˆA ˆh = ˆb,
(8)
Eqn. (8) has a desirable form because ˆh, and thus H, can be
solved for using ˆA+, the pseudo-inverse of ˆA. This operation
is simple and differentiable with respect to the coordinates
of xi and x(cid:48)
i. In addition, the gradients are easier to calculate
than for SVD.

This approach may still fail if the correspondence points
are collinear: if three of the correspondence points are on the
same line, then solving for H is undetermined. We alleviate
this problem by ﬁrst making the initial guess of H4pt to be
zero, implying that ˜CB
4pt . We then set a small learning
4pt does not
rate such that after each training iteration,
move too far away from CA

4pt ∼ CA

˜CB

4pt .

The next layer applies the 3 × 3 homography estimate
˜H output by the Tensor DLT to the pixel coordinates xi
of image IA in order to get warped coordinates H (xi).
These warped coordinates are necessary in computing the
photometric loss function in Eqn. (4) that will train our
neural network. In addition to warping the coordinates, this
layer must also be differentiable so that the error gradients
can ﬂow through via backpropagation. We thus extend the
Spatial Transformer Layer introduced in [34] by applying it
to homography transformations.

This layer performs an inverse warping in order to avoid
holes in the warped image. This process consists of 3 steps:
(1) Normalized inverse computation ˜Hinv of the homogra-
phy estimate; (2) Parameterized Sampling Grid Generator
(PSGG); and (3) Differentiable Sampling (DS).

The ﬁrst step, computing a normalized inverse, involves
normalizing the height and width coordinates of images IA
and IB into a range such that −1 ≤ ui, vi ≤ 1 and −1 ≤ u(cid:48)
i ≤
1. Thus given a 3 × 3 homography estimate ˜H, the inverse
˜Hinv used for warping is computed as follows:

i, v(cid:48)

˜Hinv = M−1 ˜H−1M


W (cid:48)/2
0
0

0

W (cid:48)/2
H(cid:48)/2 H(cid:48)/2





0

1

where M =



with W (cid:48) and H(cid:48) are the width and height of the IB.

The second step (PSGG) creates a grid G = {Gi} of
the same size as the second image IB. Each grid element
Gi = (u(cid:48)
i) corresponds to pixels of the second image
IB. Applying the inverse homography ˜Hinv to these grid
coordinates provides a grid of pixels in the ﬁrst image IA.

i, v(cid:48)





ui
vi
1


 = Hinv(Gi) = ˜Hinv









u(cid:48)
i
v(cid:48)
i
1

(9)

Based on the sampling points Hinv(Gi) computed from
PSGG, the last step (DS) produces a sampled warped im-
age V of size H(cid:48) × W (cid:48) with C channels, where V (xi) =
IA(H (xi)).

The sampling kernel k(·) is applied to the grid Hinv(Gi)

and the resulting image V is deﬁned as

V C
i =

Ic
nmk(ui − m; Φu)k(vi − n; Φv),

H
∑
n

W
∑
m

∀i ∈ [1...H(cid:48)W (cid:48)]

, ∀c ∈ [1..C]

(10)

where H,W are the height and width of the input image IA,
Φu and Φv are the parameters of k(·) deﬁning the image
interpolation. Ic
nm is the value at location (n, m) in channel
c of the input image, and V c
is the value of the output
i
pixel at location (ui, vi) in channel c. Here, we use bilinear
interpolation such that the Eqn. (10) becomes

V C
i =

Ic
nm max(0, 1 − |ui − m|) max(0, 1 − |vi − n|)

H
∑
n

W
∑
m

(11)

To allow backpropagation of the loss function, gradients with
respect to I and G for bilinear interpolation are deﬁned as

∂V c
i
∂ Ic
nm

=

H
∑
n

W
∑
m

∂V c
i
∂ ui =

H
∑
n

W
∑
m

∂V c
i
∂ vi =

H
∑
n

W
∑
m

max(0, 1 − |ui − m|) max(0, 1 − |vi − n|) (12)

Ic
nm max(0, 1 − |vi − n|)

Ic
nm max(0, 1 − |ui − m|)











0 if |m − ui| ≥ 1
1 if m ≥ ui
−1 if m < ui

0 if |n − vi| ≥ 1
1 if n ≥ vi
−1 if n < vi

(13)

(14)

This allows backpropagation of the loss gradients using the
chain rule because ∂ ui
can be easily derived from
∂ h jk
Eqn. 2.

and ∂ vi
∂ h jk

V. EVALUATION RESULTS

The intended use case for our algorithm is in estimating
homographies for aerial multi-robot systems applications
such as image mosaicing and collision avoidance. As a re-
sult, we demonstrate our unsupervised algorithm’s accuracy,
inference speed, and robustness to illumination variation
relative to SIFT, ORB, ECC and the supervised deep learning
method. We evaluate these methods on a synthetic dataset
similar to the dataset used in [24], and on a real-world aerial
image dataset. Since ORB’s performance is inferior to that
of SIFT, we only report ORB’s performance in Fig. 4 and
omit it in the remaining ﬁgures.

Both the supervised and unsupervised approaches use
the VGGNet architecture to generate homography estimates
˜H4pt. The deep learning approaches are implemented in Ten-
sorﬂow [35] using stochastic gradient descent with a batch
size of 128, and an Adam Optimizer [36] with, β1 = 0.9,
β2 = 0.999 and ε = 10−8. We empirically chose the initial

learning rate for the supervised algorithm and unsupervised
algorithm to be 0.0005 and 0.0001 respectively.

The ECC direct method is a standard Python OpenCV im-
plementation while the feature-based approaches are Python
OpenCV implementations of SIFT RANSAC and ORB
RANSAC. We found that in our synthetic dataset, using
all detected features gives better performance, while in our
aerial dataset, choosing the 50 best features is superior. These
feature pairs are then used to calculate the homography using
RANSAC with a threshold of 5 pixels. For the ECC method,
we use identity matrix as the initialization and set 1000 as
the maximum number of iterations.

A. Synthetic Data Results

This section analyzes the performance proﬁle of the Un-
supervised, Supervised, SIFT, and ECC methods on our
synthetic dataset. We want to test how well our approach
performs under illumination variation and large image dis-
placement.

To account for illumination variation, we globally stan-
dardize our images based on the mean and variance of pixel
intensities of all images in our training dataset. We addition-
ally inject random color, brightness and gamma shifts during
the training. We do not utilize any further preprocessing and
use the L1 photometric loss function. To highlight the effect
of displacement amount on each method, we break down the
accuracy performance in terms of: 85% image overlap (small
displacement), 75% image overlap (moderate displacement),
and 65% image overlap (large displacement). We follow the
synthetic data generation process on the MS-COCO dataset
used in [24]. The amount of image overlap is controlled by
the point perturbation parameter ρ. The evaluation metric is
the 4pt-Homography RMSE from Eqn. (3) comparing the
estimated homography to the ground truth homography.

We train the deep networks from scratch for 300, 000
iterations over ∼ 30 hours, using two GPUs. This long
training procedure only needs to be performed once, as
the resulting model can be used as an initial pre-trained
model for other data sets. We observed that the supervised
model started overﬁtting after 150, 000 iterations so stopped
training early. SIFT, ORB and ECC estimated homographies
using the full images, while the deep learning methods are
only given access to the small patches (∼ 21% pixels).
This disadvantages our methods, and would result in better
performance for the traditional methods, at the expense of
slower running times.

Fig. 3 displays the results of each method broken down
by overlap and performance percentile. We break down the
results by performance percentile to illustrate the various per-
formance proﬁles of each method. Speciﬁcally, SIFT tends
to do very well 60% of the time, but in the worst 40% of the
time it performs very poorly, sometimes completely failing
to detect enough features to estimate a homography. On the
other hand, the deep learning methods tends to have much
more consistent performance, which can be more desireable
in practical applications such as using homographies for
collision avoidance for aerial multi-robot systems. Both the

Fig. 3: Synthetic 4pt-Homography RMSE (lower is better). Unsupervised has comparable performance with the supervised
method and performs better than the other approaches especially when the displacement is large.

B. Aerial Dataset Results

This section analyzes the performance proﬁle of each
method on a representative dataset of aerial imagery cap-
tured by a UAV. In addition to accuracy performance, an
equally important consideration for real world application is
inference speed. As a result, we also discuss the performance
to speed tradeoffs of each method.

Our aerial dataset contains 350 image pairs resized to
240 × 320, captured by a DJI Phantom 3 Pro platform in
Yardley, Pensylvania, USA in 2017. We divided it into 300
train and 50 test samples. We did not label the train set,
but for evaluation purposes, we manually labeled the ground
truth by picking 4 pairs of correspondences for each test
sample. We also randomly inject illumination noise in both
the training and testing sets. The evaluation metrics are the
same for the synthetic data. To reduce training time, we
ﬁnetune the neural networks on the aerial image data. Our
unsupervised algorithm can directly use the aerial dataset
image pairs. However, since we do not have ground truth
homography labels, we have to perform a similar synthetic
data generation process as in the synthetic dataset in order
to ﬁnetune the supervised neural network. We ﬁne tune both
models over 150, 000 iterations for roughly 15 hours with
data augmentation.

Fig. 6 displays the performance proﬁle for the Unsuper-
vised, Supervised, SIFT, and ECC methods. Fig 4 displays
the speed and performance tradeoff for these methods, and
additionally the featured based method ORB. The feature-
based methods are tested on a 16-core Intel Xeon CPU, and
the deep learning methods are tested on the same CPU and
an NVIDIA Titan X GPU. The closer to the lower left hand
corner, the better the performance and faster the runtime.

Both Figs. 6 and 4 demonstrate that our unsupervised al-
gorithm has the best performance of all methods. In addition,
Fig. 4 also shows that our unsupervised method on the GPU
has both the best performance and the fastest inference times.
SIFT has the second best performance after our unsupervised
algorithm, but has a much slower runtime (approximately
200 times slower). ORB has a faster runtime than SIFT, but at
the expense of poorer performance. The ECC direct method
approach has the worst performance and runtime of all the
methods. A qualitative example where both SIFT and ECC
fail to deliver a good result while our method succeeds is
illustrated in Fig. 5.

Fig. 4: Speed Versus Performance Tradeoff. Lower left is
the computational
better. Sufﬁxes GPU and CPU reﬂect
resource. All the feature-based methods are run on the CPU.
The unsupervised network run on the GPU dominates all the
other methods by having both the highest throughput and best
performance.

learning methods and the feature-based methods outperform
the direct method (ECC).

Interestingly, whereas direct method ECC has problems
with illumination variation and large displacement, our un-
supervised method is able to handle these scenarios even
though it uses photometric loss functions. One potential
hypothesis is that our method can be viewed as a hybrid
between direct methods and feature based methods. The
large receptive ﬁeld of neural networks may allow it to
handle large image displacement better than a direct method.
In addition, whereas image gradients are used to update
homography parameters in direct methods, with neural net-
works, these gradients are used to update network parameters
which correspond to improving learned features. Finally,
direct methods are an online optimization process that use
gradients from a single pair of images, whereas training a
deep network is an ofﬂine optimization process that averages
gradients across multiple images. Injecting noise into this
training process can further improve robustness to different
appearance variations. Understanding the relationship be-
tween the neural network and photometric loss functions is
an important direction for future work.

(a) Unsupervised (Success, RMSE = 15.6)

(b) Unsupervised (Success, RMSE = 4.50)

(c) SIFT (Fail, RMSE = 105.2)

(d) SIFT (Success, RMSE = 6.06)

(e) ECC(Success, RMSE = 66.4)

(f) ECC(Success, RMSE = 48.10)

Fig. 5: Qualitative visualization of estimation methods on aerial dataset. Left: hard case, right: moderate case. ECC performs
better than SIFT in the case of small displacement, but performs worse than SIFT in case of large displacement. Unsupervised
network outperforms both SIFT and ECC approaches. Supervised network is omitted due to limited space and its poor
performance on this dataset.

pervised algorithm. Our aerial dataset results highlight the
fact that even though synthetic data can be generated from
real images, a pair of synthetic images is still very different
from a pair of real images. These results demonstrate that the
independence of our unsupervised algorithm from expensive
ground truth labels has large practical implications for real-
world performance.

VI. CONCLUSIONS

We have introduced an unsupervised algorithm that trains
a deep neural network to estimate planar homographies. Our
approach outperforms the corresponding supervised network
on both synthetic and real-world datasets, demonstrating the
superiority of unsupervised learning in image warping prob-
lems. Our approach achieves faster inference speed, while
maintaining comparable or better accuracy than feature-
based and direct methods. We demonstrate that the unsu-
pervised approach is able to handle large displacements and
large illumination variations that are typically challenging
for direct approaches that use the same photometric loss
function. The speed and adaptive nature of our algorithm
makes it especially useful in aerial multi-robot applications
that can exploit parallel computation.

In this work, we do not investigate robustness against
occlusion, leaving it as future work. However, as suggested
in [24], we could potentially address this issue by using data
augmentation techniques such as artiﬁcially inserting random

Fig. 6: 4pt-homography RMSE on aerial images (lower is
better). Unsupervised outperforms other approaches signiﬁ-
cantly.

One of the most interesting results is that while the super-
vised and unsupervised approaches performed comparably
on the synthetic data, the supervised approach had drastically
poorer performance on the aerial image dataset. This shift
is due to the fact that ground truth labels are not available
for our aerial dataset. The generalization gap from synthetic
(train) to real (test) data is an important problem in machine
learning. The best practical approach is to additionally ﬁne-
tune the model on the new distribution of data. In a robotic
ﬁeld experiment, this can be achieved by ﬂying the UAV to
collect a few sample images and ﬁne-tuning on those images.
However, this ﬁne-tuning is only possible with our unsu-

occluding shapes into the training images. Another direction
for future work is investigating different improvements to
achieve sub-pixel accuracy in the top 30% performance
percentile.

Finally, our approach is easily scalable to more general
warping motions. Our ﬁndings provide additional evidence
for applying deep learning methods, speciﬁcally unsuper-
vised learning, to various robotic perception problems such
as stereo depth estimation, or visual odometry. Our insights
on estimating homographies with unsupervised deep neural
network approaches provide an initial step in a structured
progression of applying these methods to larger problems.

VII. ACKNOWLEDGEMENTS

We gratefully acknowledge the support of ARL grants
W911NF-08-2-0004 and W911NF-10-2-0016, ARO grant
W911NF-13-1-0350, N00014-14-1-0510, N00014-09-1-
1051, N00014-11-1-0725, N00014-15-1-2115 and N00014-
09-1-103, DARPA grants HR001151626/HR0011516850
USDA grant 2015-67021-23857 NSF grants IIS-1138847,
IIS-1426840 CNS-1446592 CNS-1521617 and IIS-1328805,
Qualcomm Research, United Technologies, and TerraSwarm,
one of six centers of STARnet, a Semiconductor Research
Corporation program sponsored by MARCO and DARPA.
We would also like to thank Aerial Applications for the
UAV data set.

REFERENCES

[1] M. Brown, D. G. Lowe, et al., “Recognising panoramas.” in ICCV,

vol. 3, 2003, p. 1218.

[2] M. Shridhar and K.-Y. Neo, “Monocular slam for real-time applica-

tions on mobile platforms,” 2015.

[3] Z. Zhang and A. R. Hanson, “3d reconstruction based on homography

mapping,” Proc. ARPA96, pp. 1007–1012, 1996.

[4] Z. Pan, X. Fang, J. Shi, and D. Xu, “Easy tour: a new image-based
virtual tour system,” in Proceedings of the 2004 ACM SIGGRAPH
international conference on Virtual Reality continuum and its appli-
cations in industry. ACM, 2004, pp. 467–471.

[5] C.-Y. Tang, Y.-L. Wu, P.-C. Hu, H.-C. Lin, and W.-C. Chen, “Self-
calibration for metric 3d reconstruction using homography.” in MVA,
2007, pp. 86–89.

[6] D. Capel, “Image mosaicing,” in Image Mosaicing and Super-

resolution. Springer, 2004, pp. 47–79.

[7] R. Szeliski, “Image alignment and stitching: A tutorial,” Foundations
and Trends R(cid:13) in Computer Graphics and Vision, vol. 2, no. 1, pp.
1–104, 2006.

[8] B. D. Lucas and T. Kanade, “An iterative image registration technique
with an application to stereo vision,” in Proceedings of
the 7th
International Joint Conference on Artiﬁcial Intelligence - Volume
2, ser. IJCAI’81.
San Francisco, CA, USA: Morgan Kaufmann
Publishers Inc., 1981, pp. 674–679.

[9] S. Baker and I. Matthews, “Lucas-kanade 20 years on: A unifying
framework,” International journal of computer vision, vol. 56, no. 3,
pp. 221–255, 2004.

[10] G. D. Evangelidis and E. Z. Psarakis, “Parametric image alignment
using enhanced correlation coefﬁcient maximization,” IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, vol. 30, no. 10,
pp. 1858–1865, 2008.

[11] Q. Yan, Y. Xu, X. Yang, and T. Nguyen, “Heask: Robust homography
estimation based on appearance similarity and keypoint correspon-
dences,” Pattern Recognition, vol. 47, no. 1, pp. 368–387, 2014.
[12] S. Lucey, R. Navarathna, A. B. Ashraf, and S. Sridharan, “Fourier
lucas-kanade algorithm,” IEEE transactions on pattern analysis and
machine intelligence, vol. 35, no. 6, pp. 1383–1396, 2013.

[13] E. Mu˜noz, P. M´arquez-Neila, and L. Baumela, “Rationalizing efﬁcient
compositional image alignment,” International Journal of Computer
Vision, vol. 112, no. 3, pp. 354–372, 2015.

[14] D. G. Lowe, “Distinctive image features from scale-invariant key-
points,” International journal of computer vision, vol. 60, no. 2, pp.
91–110, 2004.

[15] M. A. Fischler and R. C. Bolles, “Random sample consensus: A
paradigm for model ﬁtting with applications to image analysis and
automated cartography,” Commun. ACM, vol. 24, no. 6, pp. 381–395,
June 1981.

[16] F.-l. Wu and X.-y. Fang, “An improved ransac homography algorithm
for feature based image mosaic,” in Proceedings of the 7th WSEAS
International Conference on Signal Processing, Computational Geom-
etry & Artiﬁcial Vision. World Scientiﬁc and Engineering Academy
and Society (WSEAS), 2007, pp. 202–207.

[17] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “Orb: An efﬁcient
alternative to sift or surf,” in Computer Vision (ICCV), 2011 IEEE
international conference on.

IEEE, 2011, pp. 2564–2571.

[18] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid, “Deepﬂow:
Large displacement optical ﬂow with deep matching,” in Proceedings
of the IEEE International Conference on Computer Vision, 2013, pp.
1385–1392.

[19] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox,
“Flownet 2.0: Evolution of optical ﬂow estimation with deep net-
works,” arXiv preprint arXiv:1612.01925, 2016.

[20] P. Fischer, A. Dosovitskiy, E. Ilg, P. H¨ausser, C. Hazırbas¸, V. Golkov,
P. van der Smagt, D. Cremers, and T. Brox, “Flownet: Learning optical
ﬂow with convolutional networks,” arXiv preprint arXiv:1504.06852,
2015.

[21] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid, “Deepmatch-
ing: Hierarchical deformable dense matching,” International Journal
of Computer Vision, vol. 120, no. 3, pp. 300–323, 2016.

[22] H. Altwaijry, A. Veit, S. J. Belongie, and C. Tech, “Learning to detect
and match keypoints with deep architectures.” in BMVC, 2016.
[23] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a
single image using a multi-scale deep network,” in Advances in neural
information processing systems, 2014, pp. 2366–2374.

[24] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Deep image homog-

raphy estimation,” arXiv preprint arXiv:1606.03798, 2016.

[25] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsuper-
vised learning of depth and ego-motion from video,” arXiv preprint
arXiv:1704.07813, 2017.

[26] S. Baker, A. Datta, and T. Kanade, “Parameterizing homographies,”

Technical Report CMU-RI-TR-06-11, 2006.

[27] K. Simonyan and A. Zisserman, “Very deep convolutional networks
for large-scale image recognition,” arXiv preprint arXiv:1409.1556,
2014.

[28] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Is l2 a good loss function
for neural networks for image processing?” ArXiv e-prints, vol. 1511,
2015.

[29] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,
and K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer
parameters and¡ 0.5 mb model size,” arXiv preprint arXiv:1602.07360,
2016.

[30] R. I. Hartley and A. Zisserman, Multiple View Geometry in Computer
Cambridge University Press, ISBN: 0521540518,

Vision, 2nd ed.
2004.

[31] G. H. Golub and C. Reinsch, “Singular value decomposition and least
squares solutions,” Numerische mathematik, vol. 14, no. 5, pp. 403–
420, 1970.

[32] T. Papadopoulo and M. I. Lourakis, “Estimating the jacobian of the
singular value decomposition: Theory and applications,” in European
Conference on Computer Vision. Springer, 2000, pp. 554–570.
[33] R. Hartley and A. Zisserman, Multiple view geometry in computer

vision. Cambridge university press, 2003.

[34] M. Jaderberg, K. Simonyan, A. Zisserman, et al., “Spatial transformer
networks,” in Advances in Neural Information Processing Systems,
2015, pp. 2017–2025.

[35] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, et al., “Tensorﬂow: Large-
scale machine learning on heterogeneous distributed systems,” arXiv
preprint arXiv:1603.04467, 2016.

[36] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

