8
1
0
2
 
n
a
J
 
9
2
 
 
]

G
L
.
s
c
[
 
 
4
v
3
0
4
2
0
.
3
0
7
1
:
v
i
X
r
a

On Structured Prediction Theory with Calibrated
Convex Surrogate Losses

Anton Osokin
INRIA/ENS∗, Paris, France
HSE†, Moscow, Russia

Francis Bach
INRIA/ENS∗, Paris, France

Simon Lacoste-Julien
MILA and DIRO
Université de Montréal, Canada

Abstract

We provide novel theoretical insights on structured prediction in the context of
efﬁcient convex surrogate loss minimization with consistency guarantees. For any
task loss, we construct a convex surrogate that can be optimized via stochastic
gradient descent and we prove tight bounds on the so-called “calibration function”
relating the excess surrogate risk to the actual risk. In contrast to prior related
work, we carefully monitor the effect of the exponential number of classes in the
learning guarantees as well as on the optimization complexity. As an interesting
consequence, we formalize the intuition that some task losses make learning harder
than others, and that the classical 0-1 loss is ill-suited for structured prediction.

1

Introduction

Structured prediction is a subﬁeld of machine learning aiming at making multiple interrelated
predictions simultaneously. The desired outputs (labels) are typically organized in some structured
object such as a sequence, a graph, an image, etc. Tasks of this type appear in many practical domains
such as computer vision [34], natural language processing [42] and bioinformatics [19].

The structured prediction setup has at least two typical properties differentiating it from the classical
binary classiﬁcation problems extensively studied in learning theory:
1. Exponential number of classes: this brings both additional computational and statistical challenges.
By exponential, we mean exponentially large in the size of the natural dimension of output, e.g., the
number of all possible sequences is exponential w.r.t. the sequence length.
2. Cost-sensitive learning: in typical applications, prediction mistakes are not all equally costly.
The prediction error is usually measured with a highly-structured task-speciﬁc loss function, e.g.,
Hamming distance between sequences of multi-label variables or mean average precision for ranking.

Despite many algorithmic advances to tackle structured prediction problems [4, 35], there have been
relatively few papers devoted to its theoretical understanding. Notable recent exceptions that made
signiﬁcant progress include Cortes et al. [13] and London et al. [28] (see references therein) which
proposed data-dependent generalization error bounds in terms of popular empirical convex surrogate
losses such as the structured hinge loss [44, 45, 47]. A question not addressed by these works is
whether their algorithms are consistent: does minimizing their convex bounds with inﬁnite data lead
to the minimization of the task loss as well? Alternatively, the structured probit and ramp losses are
consistent [31, 30], but non-convex and thus it is hard to obtain computational guarantees for them.
In this paper, we aim at getting the property of consistency for surrogate losses that can be efﬁciently
minimized with guarantees, and thus we consider convex surrogate losses.

The consistency of convex surrogates is well understood in the case of binary classiﬁcation [50, 5, 43]
and there is signiﬁcant progress in the case of multi-class 0-1 loss [49, 46] and general multi-

∗DI École normale supérieure, CNRS, PSL Research University
†National Research University Higher School of Economics

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

class loss functions [3, 39, 48]. A large body of work speciﬁcally focuses on the related tasks of
ranking [18, 9, 40] and ordinal regression [37].
Contributions. In this paper, we study consistent convex surrogate losses speciﬁcally in the context
of an exponential number of classes. We argue that even while being consistent, a convex surrogate
might not allow efﬁcient learning. As a concrete example, Ciliberto et al. [10] recently proposed a
consistent approach to structured prediction, but the constant in their generalization error bound can
be exponentially large as we explain in Section 5. There are two possible sources of difﬁculties from
the optimization perspective: to reach adequate accuracy on the task loss, one might need to optimize
a surrogate loss to exponentially small accuracy; or to reach adequate accuracy on the surrogate loss,
one might need an exponential number of algorithm steps because of exponentially large constants
in the convergence rate. We propose a theoretical framework that jointly tackles these two aspects
and allows to judge the feasibility of efﬁcient learning. In particular, we construct a calibration
function [43], i.e., a function setting the relationship between accuracy on the surrogate and task
losses, and normalize it by the means of convergence rate of an optimization algorithm.

Aiming for the simplest possible application of our framework, we propose a family of convex
surrogates that are consistent for any given task loss and can be optimized using stochastic gradient
descent. For a special case of our family (quadratic surrogate), we provide a complete analysis
including general lower and upper bounds on the calibration function for any task loss, with exact
values for the 0-1, block 0-1 and Hamming losses. We observe that to have a tractable learning
algorithm, one needs both a structured loss (not the 0-1 loss) and appropriate constraints on the
predictor, e.g., in the form of linear constraints for the score vector functions. Our framework also
indicates that in some cases it might be beneﬁcial to use non-consistent surrogates. In particular, a
non-consistent surrogate might allow optimization only up to speciﬁc accuracy, but exponentially
faster than a consistent one.

We introduce the structured prediction setting suitable for studying consistency in Sections 2 and 3.
We analyze the calibration function for the quadratic surrogate loss in Section 4. We review the
related works in Section 5 and conclude in Section 6.

2 Structured prediction setup

In structured prediction, the goal is to predict a structured output y ∈ Y (such as a sequence, a graph,
an image) given an input x ∈ X . The quality of prediction is measured by a task-dependent loss
function L( ˆy, y | x) ≥ 0 specifying the cost for predicting ˆy when the correct output is y. In this
paper, we consider the case when the number of possible predictions and the number of possible
labels are both ﬁnite. For simplicity,1 we also assume that the sets of possible predictions and correct
outputs always coincide and do not depend on x. We refer to this set as the set of labels Y, denote its
cardinality by k, and map its elements to 1, . . . , k. In this setting, assuming that the loss function
depends only on ˆy and y, but not on x directly, the loss is deﬁned by a loss matrix L ∈ Rk×k. We
assume that all the elements of the matrix L are non-negative and will use Lmax to denote the maximal
element. Compared to multi-class classiﬁcation, k is typically exponentially large in the size of the
natural dimension of y, e.g., contains all possible sequences of symbols from a ﬁnite alphabet.

Following standard practices in structured prediction [12, 44], we deﬁne the prediction model by
a score function f : X → Rk specifying a score fy(x) for each possible output y ∈ Y. The ﬁnal
prediction is done by selecting a label with the maximal value of the score

pred(f(x)) := argmax

f ˆy(x),

ˆy∈Y

(1)

with some ﬁxed strategy to resolve ties. To simplify the analysis, we assume that among the labels
with maximal scores, the predictor always picks the one with the smallest index.

The goal of prediction-based machine learning consists in ﬁnding a predictor that works well on
the unseen test set, i.e., data points coming from the same distribution D as the one generating the
training data. One way to formalize this is to minimize the generalization error, often referred to as
the actual (or population) risk based on the loss L,

RL(f) := IE(x,y)∼D L(cid:0)pred(f(x)), y(cid:1).
(2)
Minimizing the actual risk (2) is usually hard. The standard approach is to minimize a surrogate risk,
which is a different objective easier to optimize, e.g., convex. We deﬁne a surrogate loss as a function

1Our analysis is generalizable to rectangular losses, e.g., ranking losses studied by Ramaswamy et al. [40].

2

Φ : Rk × Y → R depending on a score vector f = f(x) ∈ Rk and a target label y ∈ Y as input
arguments. We denote the y-th component of f with fy. The surrogate risk (the Φ-risk) is deﬁned as
RΦ(f) := IE(x,y)∼D Φ(f(x), y),
(3)
where the expectation is taken w.r.t. the data-generating distribution D. To make the minimization
of (3) well-deﬁned, we always assume that the surrogate loss Φ is bounded from below and continuous.

Examples of common surrogate losses include the structured hinge-loss [44, 47] ΦSSVM(f , y) :=
(cid:0)f ˆy + L( ˆy, y)(cid:1) − fy, the log loss (maximum likelihood learning) used, e.g., in conditional
max ˆy∈Y
random ﬁelds [25], Φlog(f , y) := log((cid:80)
In terms of task losses, we consider the unstructured 0-1 loss L01( ˆy, y) := [ ˆy (cid:54)= y],2 and the
two following structured losses: block 0-1 loss with b equal blocks of labels L01,b( ˆy, y) :=
[ ˆy and y are not in the same block]; and (normalized) Hamming loss between tuples of T binary
variables yt: LHam,T ( ˆy, y) := 1
t=1[ˆyt (cid:54)= yt]. To illustrate some aspects of our analysis, we also
T
look at the mixed loss L01,b,η: a convex combination of the 0-1 and block 0-1 losses, deﬁned as
L01,b,η := ηL01 + (1 − η)L01,b for some η ∈ [0, 1].

ˆy∈Y exp f ˆy) − fy, and their hybrids [38, 21, 22, 41].

(cid:80)T

3 Consistency for structured prediction

3.1 Calibration function

We now formalize the connection between the actual risk RL and the surrogate Φ-risk RΦ via the
so-called calibration function, see Deﬁnition 1 below [5, 49, 43, 18, 3]. As it is standard for this
kind of analysis, the setup is non-parametric, i.e. it does not take into account the dependency of
scores on input variables x. For now, we assume that a family of score functions FF consists of all
vector-valued Borel measurable functions f : X → F where F ⊆ Rk is a subspace of allowed score
vectors, which will play an important role in our analysis. This setting is equivalent to a pointwise
analysis, i.e, looking at the different input x independently. We bring the dependency on the input
back into the analysis in Section 3.3 where we assume a speciﬁc family of score functions.

Let DX represent the marginal distribution for D on x and IP(· | x) denote its conditional given x.
We can now rewrite the risk RL and Φ-risk RΦ as

RL(f) = IEx∼DX (cid:96)(f(x), IP(· | x)), RΦ(f) = IEx∼DX φ(f(x), IP(· | x)),
where the conditional risk (cid:96) and the conditional Φ-risk φ depend on a vector of scores f and a
conditional distribution on the set of output labels q as

(cid:88)k

(cid:88)k

(cid:96)(f , q) :=

qcL(pred(f ), c), φ(f , q) :=
The calibration function HΦ,L,F between the surrogate loss Φ and the task loss L relates the excess
surrogate risk with the actual excess risk via the excess risk bound:

qcΦ(f , c).

c=1

c=1

HΦ,L,F (δ(cid:96)(f , q)) ≤ δφ(f , q), ∀f ∈ F, ∀q ∈ ∆k,
(4)
where δφ(f , q) = φ(f , q) − inf ˆf ∈F φ( ˆf , q), δ(cid:96)(f , q) = (cid:96)(f , q) − inf ˆf ∈F (cid:96)( ˆf , q) are the excess
risks and ∆k denotes the probability simplex on k elements.

In other words, to ﬁnd a vector f that yields an excess risk smaller than ε, we need to optimize the
Φ-risk up to HΦ,L,F (ε) accuracy (in the worst case). We make this statement precise in Theorem 2
below, and now proceed to the formal deﬁnition of the calibration function.
Deﬁnition 1 (Calibration function). For a task loss L, a surrogate loss Φ, a set of feasible scores F,
the calibration function HΦ,L,F (ε) (deﬁned for ε ≥ 0) equals the inﬁmum excess of the conditional
surrogate risk when the excess of the conditional actual risk is at least ε:

HΦ,L,F (ε) := inf

δφ(f , q)

f ∈F , q∈∆k
s.t.

δ(cid:96)(f , q) ≥ ε.

(5)

(6)

We set HΦ,L,F (ε) to +∞ when the feasible set is empty.

By construction, HΦ,L,F is non-decreasing on [0, +∞), HΦ,L,F (ε) ≥ 0, the inequality (4) holds,
and HΦ,L,F (0) = 0. Note that HΦ,L,F can be non-convex and even non-continuous (see examples
in Figure 1). Also, note that large values of HΦ,L,F (ε) are better.

2Here we use the Iverson bracket notation, i.e., [A] := 1 if a logical expression A is true, and zero otherwise.

3

(a): Hamming loss LHam,T

(b): Mixed loss L01,b,0.4

Figure 1: Calibration functions for the quadratic surrogate Φquad (12) deﬁned in Section 4 and two
different task losses. (a) – the calibration functions for the Hamming loss LHam,T when used without
constraints on the scores, F = Rk (in red), and with the tight constraints implying consistency,
F = span(LHam,T ) (in blue). The red curve can grow exponentially slower than the blue one. (b) –
the calibration functions for the mixed loss L01,b,η with η = 0.4 (see Section 2 for the deﬁnition) when
used without constraints on the scores (red) and with tight constraints for the block 0-1 loss (blue).
The blue curve represents level-0.2 consistency. The calibration function equals zero for ε ≤ η/2,
but grows exponentially faster than the red curve representing a consistent approach and thus could
be better for small η. More details on the calibration functions in this ﬁgure are given in Section 4.

3.2 Notion of consistency

(7)

(8)

We use the calibration function HΦ,L,F to set a connection between optimizing the surrogate and
task losses by Theorem 2, which is similar to Theorem 3 of Zhang [49].
Theorem 2 (Calibration connection). Let HΦ,L,F be the calibration function between the surrogate
loss Φ and the task loss L with feasible set of scores F ⊆ Rk. Let ˇHΦ,L,F be a convex non-decreasing
lower bound of the calibration function. Assume that Φ is continuous and bounded from below. Then,
for any ε > 0 with ﬁnite ˇHΦ,L,F (ε) and any f ∈ FF , we have

where R∗

Φ,F := inf f∈FF RΦ(f) and R∗

RΦ(f) < R∗

Φ,F + ˇHΦ,L,F (ε) ⇒ RL(f) < R∗
L,F := inf f∈FF RL(f).

L,F + ε,

Proof. We take the expectation of (4) w.r.t. x, where the second argument of (cid:96) is set to the conditional
distribution IP(· | x). Then, we apply Jensen’s inequality (since ˇHΦ,L,F is convex) to get

ˇHΦ,L,F (RL(f) − R∗

L,F ) ≤ RΦ(f) − R∗

Φ,F < ˇHΦ,L,F (ε),

which implies (7) by monotonicity of ˇHΦ,L,F .

A suitable convex non-decreasing lower bound ˇHΦ,L,F (ε) required by Theorem 2 always exists, e.g.,
the zero constant. However, in this case Theorem 2 is not informative, because the l.h.s. of (7) is
never true. Zhang [49, Proposition 25] claims that ˇHΦ,L,F deﬁned as the lower convex envelope of
the calibration function HΦ,L,F satisﬁes ˇHΦ,L,F (ε) > 0, ∀ε > 0, if HΦ,L,F (ε) > 0, ∀ε > 0, and,
e.g., the set of labels is ﬁnite. This statement implies that an informative ˇHΦ,L,F always exists and
allows to characterize consistency through properties of the calibration function HΦ,L,F .

We now deﬁne a notion of level-η consistency, which is more general than consistency.
Deﬁnition 3 (level-η consistency). A surrogate loss Φ is consistent up to level η ≥ 0 w.r.t. a task
loss L and a set of scores F if and only if the calibration function satisﬁes HΦ,L,F (ε) > 0 for all
ε > η and there exists ˆε > η such that HΦ,L,F (ˆε) is ﬁnite.

Looking solely at (standard level-0) consistency vs. inconsistency might be too coarse to capture
practical properties related to optimization accuracy (see, e.g., [29]). For example, if HΦ,L,F (ε) = 0
only for very small values of ε, then the method can still optimize the actual risk up to a certain
level which might be good enough in practice, especially if it means that it can be optimized faster.
Examples of calibration functions for consistent and inconsistent surrogate losses are shown in
Figure 1.
Other notions of consistency. Deﬁnition 3 with η = 0 and F = Rk results in the standard setting
often appearing in the literature. In particular, in this case Theorem 2 implies Fisher consistency as

4

formulated, e.g., by Pedregosa et al. [37] for general losses and Lin [27] for binary classiﬁcation.
This setting is also closely related to many deﬁnitions of consistency used in the literature. For
example, for a bounded from below and continuous surrogate, it is equivalent to inﬁnite-sample
consistency [49], classiﬁcation calibration [46], edge-consistency [18], (L, Rk)-calibration [39],
prediction calibration [48]. See [49, Appendix A] for the detailed discussion.
Role of F. Let the approximation error for the restricted set of scores F be deﬁned as R∗
L :=
inf f∈FF RL(f) − inf f RL(f). For any conditional distribution q, the score vector f := −Lq will
yield an optimal prediction. Thus the condition span(L) ⊆ F is sufﬁcient for F to have zero
approximation error for any distribution D, and for our 0-consistency condition to imply the standard
Fisher consistency with respect to L. In the following, we will see that a restricted F can both play a
role for computational efﬁciency as well as statistical efﬁciency (thus losses with smaller span(L)
might be easier to work with).

L,F −R∗

3.3 Connection to optimization accuracy and statistical efﬁciency

The scale of a calibration function is not intrinsically well-deﬁned: we could multiply the surrogate
function by a scalar and it would multiply the calibration function by the same scalar, without
changing the optimization problem. Intuitively, we would like the surrogate loss to be of order 1. If
with this scale the calibration function is exponentially small (has a 1/k factor), then we have strong
evidence that the stochastic optimization will be difﬁcult (and thus learning will be slow).

To formalize this intuition, we add to the picture the complexity of optimizing the surrogate loss with
a stochastic approximation algorithm. By using a scale-invariant convergence rate, we provide a
natural normalization of the calibration function. The following two observations are central to the
theoretical insights provided in our work:

1. Scale. For a properly scaled surrogate loss, the scale of the calibration function is a good indication
of whether a stochastic approximation algorithm will take a large number of iterations (in the worst
case) to obtain guarantees of small excess of the actual risk (and vice-versa, a large coefﬁcient
indicates a small number of iterations). The actual veriﬁcation requires computing the normalization
quantities given in Theorem 6 below.
2. Statistics. The bound on the number of iterations directly relates to the number of training
examples that would be needed to learn, if we see each iteration of the stochastic approximation
algorithm as using one training example to optimize the expected surrogate.

To analyze the statistical convergence of surrogate risk optimization, we have to specify the set of
score functions that we work with. We assume that the structure on input x ∈ X is deﬁned by a
positive deﬁnite kernel K : X × X → R. We denote the corresponding reproducing kernel Hilbert
space (RKHS) by H and its explicit feature map by ψ(x) ∈ H. By the reproducing property, we
have (cid:104)f, ψ(x)(cid:105)H = f (x) for all x ∈ X , f ∈ H, where (cid:104)·, ·(cid:105)H is the inner product in the RKHS. We
deﬁne the subspace of allowed scores F ⊆ Rk via the span of the columns of a matrix F ∈ Rk×r.
The matrix F explicitly deﬁnes the structure of the score function. With this notation, we will assume
that the score function is of the form f(x) = F W ψ(x), where W : H → Rr is a linear operator
to be learned (a matrix if H is of ﬁnite dimension) that represents a collection of r elements in H,
transforming ψ(x) to a vector in Rr by applying the RKHS inner product r times.3 Note that for
structured losses, we usually have r (cid:28) k. The set of all score functions is thus obtained by varying W
in this deﬁnition and is denoted by FF,H. As a concrete example of a score family FF,H for structured
prediction, consider the standard sequence model with unary and pairwise potentials. In this case, the
dimension r equals T s + (T − 1)s2, where T is the sequence length and s is the number of labels
of each variable. The columns of the matrix F consist of 2T − 1 groups (one for each unary and
pairwise potential). Each row of F has exactly one entry equal to one in each column group (with
zeros elsewhere).
In this setting, we use the online projected averaged stochastic subgradient descent ASGD4 (stochastic
w.r.t. data (x(n), y(n)) ∼ D) to minimize the surrogate risk directly [6]. The n-th update consists in
(9)

(cid:2)W (n−1) − γ(n)F T∇Φψ(x(n))T(cid:3),

W (n) := PD

3Note that if rank(F ) = r, our setup is equivalent to assuming a joint kernel [47] in the product form:

Kjoint((x, c), (x(cid:48), c(cid:48))) := K(x, x(cid:48))F (c, :)F (c(cid:48), :)T, where F (c, :) is the row c for matrix F .

4See, e.g., [36] for the formal setup of kernel ASGD.

5

where F T∇Φψ(x(n))T : H → Rr is the stochastic functional gradient, γ(n) is the step size
and PD is the projection on the ball of radius D w.r.t. the Hilbert–Schmidt norm5. The vector
∇Φ ∈ Rk is a regular gradient of the sampled surrogate Φ(f(x(n)), y(n)) w.r.t. the scores, ∇Φ =
∇f Φ(f , y(n))|f =f(x(n)). We wrote the above update using an explicit feature map ψ for notational
simplicity, but kernel ASGD can also be implemented without it by using the kernel trick. The
convergence properties of ASGD in RKHS are analogous to the ﬁnite-dimensional ASGD because
they rely on dimension-free quantities. To use a simple convergence analysis, we follow Ciliberto
et al. [10] and make the following simplifying assumption:
Assumption 4 (Well-speciﬁed optimization w.r.t. the function class FF,H). The distribution D is
Φ,F := inf f∈FF RΦ(f) has some global minimum f∗ that also belongs to FF,H.
such that R∗

Assumption 4 simply means that each row of W ∗ deﬁning f∗ belongs to the RKHS H implying
a ﬁnite norm (cid:107)W ∗(cid:107)HS. Assumption 4 can be relaxed if the kernel K is universal, but then the
convergence analysis becomes much more complicated [36].
Theorem 5 (Convergence rate). Under Assumption 4 and assuming that (i) the functions Φ(f , y)
are bounded from below and convex w.r.t. f ∈ Rk for all y ∈ Y; (ii) the expected square of the norm
HS ≤ M 2 and (iii) (cid:107)W ∗(cid:107)HS ≤ D,
of the stochastic gradient is bounded, IE(x,y)∼D(cid:107)F T∇Φψ(x)T(cid:107)2
then running the ASGD algorithm (9) with the constant step-size γ := 2D
for N steps admits the
√
N
following expected suboptimality for the averaged iterate ¯f(N ):

M

IE[RΦ(¯f(N ))] − R∗

Φ,F ≤ 2DM√
N

where ¯f(N ) := 1
N

(cid:88)N

n=1

F W (n)ψ(x(n))T.

(10)

Theorem 5 is a straight-forward extension of classical results [33, 36].

By combining the convergence rate of Theorem 5 with Theorem 2 that connects the surrogate and
actual risks, we get Theorem 6 which explicitly gives the number of iterations required to achieve
ε accuracy on the expected population risk (see App. A for the proof). Note that since ASGD is
applied in an online fashion, Theorem 6 also serves as the sample complexity bound, i.e., says how
many samples are needed to achieve ε target accuracy (compared to the best prediction rule if F has
zero approximation error).
Theorem 6 (Learning complexity). Under the assumptions of Theorem 5, for any ε > 0, the random
(w.r.t. the observed training set) output ¯f(N ) ∈ FF,H of the ASGD algorithm after

N > N ∗ := 4D2M 2
Φ,L,F (ε)

ˇH 2

(11)

iterations has the expected excess risk bounded with ε, i.e., IE[RL(¯f(N ))] < R∗

L,F + ε.

4 Calibration function analysis for quadratic surrogate

A major challenge to applying Theorem 6 is the computation of the calibration function HΦ,L,F . In
App. C, we present a generalization to arbitrary multi-class losses of a surrogate loss class from Zhang
[49, Section 4.4.2] that is consistent for any task loss L. Here, we consider the simplest example of
this family, called the quadratic surrogate Φquad, which has the advantage that we can bound or even
compute exactly its calibration function. We deﬁne the quadratic surrogate as

Φquad(f , y) := 1

2k (cid:107)f + L(:, y)(cid:107)2

2 = 1
2k

(f 2

c + 2fcL(c, y) + L(c, y)2).

(12)

One simple sufﬁcient condition for the surrogate (12) to be consistent and also to have zero approxi-
mation error is that F fully contains span(L). To make the dependence on the score subspace explicit,
we parameterize it with a matrix F ∈ Rk×r with the number of columns r typically being much
smaller than the number of labels k. With this notation, we have F = span(F ) = {F θ | θ ∈ Rr},
and the dimensionality of F equals the rank of F , which is at most r.6

5The Hilbert–Schmidt norm of a linear operator A is deﬁned as (cid:107)A(cid:107)HS =

trA‡A where A‡ is the adjoint
operator. In the case of ﬁnite dimension, the Hilbert–Schmidt norm coincides with the Frobenius matrix norm.
6Evaluating Φquad requires computing F TF and F TL(:, y) for which direct computation is intractable when
k is exponential, but which can be done in closed form for the structured losses we consider (the Hamming and
block 0-1 loss). More generally, these operations require suitable inference algorithms. See also App. F.

√

k
(cid:88)

c=1

6

For the quadratic surrogate (12), the excess of the expected surrogate takes a simple form:

δφquad(F θ, q) = 1

2k (cid:107)F θ + Lq(cid:107)2
2.

(13)

Equation (13) holds under the assumption that the subspace F contains the column space of the
loss matrix span(L), which also means that the set F contains the optimal prediction for any q (see
Lemma 9 in App. B for the proof). Importantly, the function δφquad(F θ, q) is jointly convex in the
conditional probability q and parameters θ, which simpliﬁes its analysis.

Lower bound on the calibration function. We now present our main technical result: a lower
bound on the calibration function for the surrogate loss Φquad (12). This lower bound characterizes
the easiness of learning with this surrogate given the scaling intuition mentioned in Section 3.3. The
proof of Theorem 7 is given in App. D.1.
Theorem 7 (Lower bound on HΦquad ). For any task loss L, its quadratic surrogate Φquad, and a score
subspace F containing the column space of L, the calibration function can be lower bounded:

HΦquad,L,F (ε) ≥

ε2
2k maxi(cid:54)=j (cid:107)PF ∆ij (cid:107)2
2

≥ ε2
4k ,

(14)

where PF is the orthogonal projection on the subspace F and ∆ij = ei − ej ∈ Rk with ec being
the c-th basis vector of the standard basis in Rk.

Lower bound for speciﬁc losses. We now discuss the meaning of the bound (14) for some speciﬁc
losses (the detailed derivations are given in App. D.3). For the 0-1, block 0-1 and Hamming losses
(L01, L01,b and LHam,T , respectively) with the smallest possible score subspaces F, the bound (14)
gives ε2
8T , respectively. All these bounds are tight (see App. E). However, if F = Rk
the bound (14) is not tight for the block 0-1 and mixed losses (see also App. E). In particular, the
bound (14) cannot detect level-η consistency for η > 0 (see Def. 3) and does not change when the
loss changes, but the score subspace stays the same.

4b and ε2

4k , ε2

Upper bound on the calibration function. Theorem 8 below gives an upper bound on the calibration
function holding for unconstrained scores, i.e, F = Rk (see the proof in App. D.2). This result shows
that without some appropriate constraints on the scores, efﬁcient learning is not guaranteed (in the
worst case) because of the 1/k scaling of the calibration function.
Theorem 8 (Upper bound on HΦquad ). If a loss matrix L with Lmax > 0 deﬁnes a pseudometric7 on
labels and there are no constraints on the scores, i.e., F = Rk, then the calibration function for the
quadratic surrogate Φquad can be upper bounded: HΦquad,L,Rk (ε) ≤ ε2
2k ,

0 ≤ ε ≤ Lmax.

From our lower bound in Theorem 7 (which guarantees consistency), the natural constraint on
the score is F = span(L), with the dimension of this space giving an indication of the intrinsic
“difﬁculty” of a loss. Computations for the lower bounds in some speciﬁc cases (see App. D.3 for
details) show that the 0-1 loss is “hard” while the block 0-1 loss and the Hamming loss are “easy”.
Note that in all these cases the lower bound (14) is tight, see the discussion below.

Exact calibration functions. Note that the bounds proven in Theorems 7 and 8 imply that, in the
case of no constraints on the scores F = Rk, for the 0-1, block 0-1 and Hamming losses, we have
ε2
4k ≤ HΦquad,L,Rk (ε) ≤ ε2
2k ,
where L is the matrix deﬁning a loss. For completeness, in App. E, we compute the exact calibration
functions for the 0-1 and block 0-1 losses. Note that the calibration function for the 0-1 loss equals the
lower bound, illustrating the worst-case scenario. To get some intuition, an example of a conditional
distribution q that gives the (worst case) value to the calibration function (for several losses) is
qi = 1
In what follows, we provide the calibration functions in the cases with constraints on the scores. For
the block 0-1 loss with b equal blocks and under constraints that the scores within blocks are equal,
the calibration function equals (see Proposition 14 of App. E.2)

2 and qc = 0 for c (cid:54)∈ {i, j}. See the proof of Proposition 12 in App. E.1.

2 , qj = 1

2 − ε

2 + ε

(15)

HΦquad,L01,b,F01,b (ε) = ε2
4b ,

0 ≤ ε ≤ 1.

(16)

7A pseudometric is a function d(a, b) satisfying the following axioms: d(x, y) ≥ 0, d(x, x) = 0 (but

possibly d(x, y) = 0 for some x (cid:54)= y), d(x, y) = d(y, x), d(x, z) ≤ d(x, y) + d(y, z).

7

For the Hamming loss deﬁned over T binary variables and under constraints implying separable
scores, the calibration function equals (see Proposition 15 in App. E.3)

HΦquad,LHam,T ,FHam,T (ε) = ε2

8T , 0 ≤ ε ≤ 1.

(17)

The calibration functions (16) and (17) depend on the quantities representing the actual complexities
of the loss (the number of blocks b and the length of the sequence T ) and can be exponentially larger
than the upper bound for the unconstrained case.

In the case of mixed 0-1 and block 0-1 loss, if the scores f are constrained to be equal inside the
blocks, i.e., belong to the subspace F01,b = span(L01,b) (cid:40) Rk, then the calibration function is equal
to 0 for ε ≤ η
2 , implying inconsistency (and also note that the approximation error can be as big as η
for F01,b). However, for ε > η
2 , the calibration function is of the order 1
2 )2. See Figure 1b for
the illustration of this calibration function and Proposition 17 of App. E.4 for the exact formulation
and the proof. Note that while the calibration function for the constrained case is inconsistent, its
value can be exponentially larger than the one for the unconstrained case for ε big enough and when
the blocks are exponentially large (see Proposition 16 of App. E.4).

b (ε − η

Computation of the SGD constants. Applying the learning complexity Theorem 6 requires to
compute the quantity DM where D bounds the norm of the optimal solution and M bounds the
expected square of the norm of the stochastic gradient. In App. F, we provide a way to bound this
quantity for our quadratic surrogate (12) under the simplifying assumption that each conditional qc(x)
(seen as function of x) belongs to the RKHS H (which implies Assumption 4). In particular, we get

DM = L2

maxξ(κ(F )

rRQmax),

ξ(z) = z2 + z,

√

(18)

where κ(F ) is the condition number of the matrix F , R is an upper bound on the RKHS norm of
object feature maps (cid:107)ψ(x)(cid:107)H. We deﬁne Qmax as an upper bound on (cid:80)k
c=1 (cid:107)qc(cid:107)H (can be seen as
the generalization of the inequality (cid:80)k
c=1 qc ≤ 1 for probabilities). The constants R and Qmax depend
on the data, the constant Lmax depends on the loss, r and κ(F ) depend on the choice of matrix F .

We compute the constant DM for the speciﬁc losses that we considered in App. F.1. For the 0-1, block
0-1 and Hamming losses, we have DM = O(k), DM = O(b) and DM = O(log3
2 k), respectively.
These computations indicate that the quadratic surrogate allows efﬁcient learning for structured block
0-1 and Hamming losses, but that the convergence could be slow in the worst case for the 0-1 loss.

5 Related works

Consistency for multi-class problems. Building on signiﬁcant progress for the case of binary
classiﬁcation, see, e.g. [5], there has been a lot of interest in the multi-class case. Zhang [49] and
Tewari & Bartlett [46] analyze the consistency of many existing surrogates for the 0-1 loss. Gao &
Zhou [20] focus on multi-label classiﬁcation. Narasimhan et al. [32] provide a consistent algorithm
for arbitrary multi-class loss deﬁned by a function of the confusion matrix. Recently, Ramaswamy &
Agarwal [39] introduce the notion of convex calibrated dimension, as the minimal dimensionality of
the score vector that is required for consistency. In particular, they showed that for the Hamming loss
on T binary variables, this dimension is at most T . In our analysis, we use scores of rank (T + 1),
see (35) in App. D.3, yielding a similar result.

The task of ranking has attracted a lot of attention and [18, 8, 9, 40] analyze different families of
surrogate and task losses proving their (in-)consistency. In this line of work, Ramaswamy et al.
[40] propose a quadratic surrogate for an arbitrary low rank loss which is related to our quadratic
surrogate (12). They also prove that several important ranking losses, i.e., precision@q, expected
rank utility, mean average precision and pairwise disagreement, are of low-rank. We conjecture that
our approach is compatible with these losses and leave precise connections as future work.

Structured SVM (SSVM) and friends. SSVM [44, 45, 47] is one of the most used convex surrogates
for tasks with structured outputs, thus, its consistency has been a question of great interest. It is
known that Crammer-Singer multi-class SVM [15], which SSVM is built on, is not consistent for
0-1 loss unless there is a majority class with probability at least 1
2 [49, 31]. However, it is consistent
for the “abstain” and ordinal losses in the case of 3 classes [39]. Structured ramp loss and probit
surrogates are closely related to SSVM and are consistent [31, 16, 30, 23], but not convex.

8

Recently, Do˘gan et al. [17] categorized different versions of multi-class SVM and analyzed them
from Fisher and universal consistency point of views. In particular, they highlight differences between
Fisher and universal consistency and give examples of surrogates that are Fisher consistent, but not
universally consistent and vice versa. They also highlight that the Crammer-Singer SVM is neither
Fisher, not universally consistent even with a careful choice of regularizer.

i=1 (cid:107)g(xi) − ψo(yi)(cid:107)2

Quadratic surrogates for structured prediction. Ciliberto et al. [10] and Brouard et al. [7] consider
minimizing (cid:80)n
H aiming to match the RKHS embedding of inputs g : X → H
to the feature maps of outputs ψo : Y → H. In their frameworks, the task loss is not considered at
the learning stage, but only at the prediction stage. Our quadratic surrogate (12) depends on the loss
directly. The empirical risk deﬁned by both their and our objectives can be minimized analytically with
the help of the kernel trick and, moreover, the resulting predictors are identical. However, performing
such computation in the case of large dataset can be intractable and the generalization properties have
to be taken care of, e.g., by the means of regularization. In the large-scale scenario, it is more natural
to apply stochastic optimization (e.g., kernel ASGD) that directly minimizes the population risk and
has better dependency on the dataset size. When combined with stochastic optimization, the two
approaches lead to different behavior. In our framework, we need to estimate r = rank(L) scalar
functions, but the alternative needs to estimate k functions (if, e.g., ψo(y) = ey ∈ Rk), which results
in signiﬁcant differences for low-rank losses, such as block 0-1 and Hamming.

Calibration functions. Bartlett et al. [5] and Steinwart [43] provide calibration functions for most
existing surrogates for binary classiﬁcation. All these functions differ in term of shape, but are
roughly similar in terms of constants. Pedregosa et al. [37] generalize these results to the case of
ordinal regression. However, their calibration functions have at best a 1/k factor if the surrogate is
normalized w.r.t. the number of classes. The task of ranking has been of signiﬁcant interest. However,
most of the literature [e.g., 11, 14, 24, 1], only focuses on calibration functions (in the form of regret
bounds) for bipartite ranking, which is more akin to cost-sensitive binary classiﬁcation.

Ávila Pires et al. [3] generalize the theoretical framework developed by Steinwart [43] and present
results for the multi-class SVM of Lee et al. [26] (the score vectors are constrained to sum to zero)
that can be built for any task loss of interest. Their surrogate Φ is of the form (cid:80)
c∈Y L(c, y)a(fc)
where (cid:80)
c∈Y fc = 0 and a(f ) is some convex function with all subgradients at zero being positive.
The recent work by Ávila Pires & Szepesvári [2] reﬁnes the results, but speciﬁcally for the case of
0-1 loss. In this line of work, the surrogate is typically not normalized by k, and if normalized the
calibration functions have the constant 1/k appearing.

Finally, Ciliberto et al. [10] provide the calibration function for their quadratic surrogate. Assuming
that the loss can be represented as L( ˆy, y) = (cid:104)V ψo( ˆy), ψo(y)(cid:105)HY , ˆy, y ∈ Y (this assumption can
always be satisﬁed in the case of a ﬁnite number of labels, by taking V as the loss matrix L and
ψo(y) := ey ∈ Rk where ey is the y-th vector of the standard basis in Rk). In their Theorem 2, they
provide an excess risk bound leading to a lower bound on the corresponding calibration function
HΦ,L,Rk (ε) ≥ ε2
where a constant c∆ = (cid:107)V (cid:107)2 maxy∈Y (cid:107)ψo(y)(cid:107) simply equals the spectral norm
c2
∆
of the loss matrix for the ﬁnite-dimensional construction provided above. However, the spectral
norm of the loss matrix is exponentially large even for highly structured losses such as the block 0-1
and Hamming losses, i.e., (cid:107)L01,b(cid:107)2 = k − k
2 . This conclusion puts the objective
of Ciliberto et al. [10] in line with ours when no constraints are put on the scores.

b , (cid:107)LHam,T (cid:107)2 = k

6 Conclusion

In this paper, we studied the consistency of convex surrogate losses speciﬁcally in the context
of structured prediction. We analyzed calibration functions and proposed an optimization-based
normalization aiming to connect consistency with the existence of efﬁcient learning algorithms.
Finally, we instantiated all components of our framework for several losses by computing the
calibration functions and the constants coming from the normalization. By carefully monitoring
exponential constants, we highlighted the difference between tractable and intractable task losses.

These were ﬁrst steps in advancing our theoretical understanding of consistent structured prediction.
Further steps include analyzing more losses such as the low-rank ranking losses studied by Ra-
maswamy et al. [40] and, instead of considering constraints on the scores, one could instead put
constraints on the set of distributions to investigate the effect on the calibration function.

9

We would like to thank Pascal Germain for useful discussions. This work was partly supported
by the ERC grant Activia (no. 307574), the NSERC Discovery Grant RGPIN-2017-06936 and the
MSR-INRIA Joint Center.

Acknowledgements

References

[1] Agarwal, Shivani. Surrogate regret bounds for bipartite ranking via strongly proper losses.

Journal of Machine Learning Research (JMLR), 15(1):1653–1674, 2014.

[2] Ávila Pires, Bernardo and Szepesvári, Csaba. Multiclass classiﬁcation calibration functions.

arXiv, 1609.06385v1, 2016.

[3] Ávila Pires, Bernardo, Ghavamzadeh, Mohammad, and Szepesvári, Csaba. Cost-sensitive

multiclass classiﬁcation risk bounds. In ICML, 2013.

[4] Bakir, Gökhan, Hofmann, Thomas, Schölkopf, Bernhard, Smola, Alexander J., Taskar, Ben,

and Vishwanathan, S.V.N. Predicting Structured Data. MIT press, 2007.

[5] Bartlett, Peter L., Jordan, Michael I., and McAuliffe, Jon D. Convexity, classiﬁcation, and risk

bounds. Journal of the American Statistical Association, 101(473):138–156, 2006.

[6] Bousquet, Olivier and Bottou, Léon. The tradeoffs of large scale learning. In NIPS, 2008.

[7] Brouard, Céline, Szafranski, Marie, and d’Alché-Buc, Florence. Input output kernel regression:
Supervised and semi-supervised structured output prediction with operator-valued kernels.
Journal of Machine Learning Research (JMLR), 17(176):1–48, 2016.

[8] Buffoni, David, Gallinari, Patrick, Usunier, Nicolas, and Calauzènes, Clément. Learning scoring

functions with order-preserving losses and standardized supervision. In ICML, 2011.

[9] Calauzènes, Clément, Usunier, Nicolas, and Gallinari, Patrick. On the (non-)existence of convex,

calibrated surrogate losses for ranking. In NIPS, 2012.

[10] Ciliberto, Carlo, Rudi, Alessandro, and Rosasco, Lorenzo. A consistent regularization approach

for structured prediction. In NIPS, 2016.

[11] Clémençon, Stéphan, Lugosi, Gábor, and Vayatis, Nicolas. Ranking and empirical minimization

of U-statistics. The Annals of Statistics, pp. 844–874, 2008.

[12] Collins, Michael. Discriminative training methods for hidden Markov models: Theory and

experiments with perceptron algorithms. In EMNLP, 2002.

[13] Cortes, Corinna, Kuznetsov, Vitaly, Mohri, Mehryar, and Yang, Scott. Structured prediction

theory based on factor graph complexity. In NIPS, 2016.

[14] Cossock, David and Zhang, Tong. Statistical analysis of bayes optimal subset ranking. IEEE

Transactions on Information Theory, 54(11):5140–5154, 2008.

[15] Crammer, Koby and Singer, Yoram. On the algorithmic implementation of multiclass kernel-
based vector machines. Journal of Machine Learning Research (JMLR), 2:265–292, 2001.

[16] Do, Chuong B., Le, Quoc, Teo, Choon Hui, Chapelle, Olivier, and Smola, Alex. Tighter bounds

for structured estimation. In NIPS, 2009.

[17] Do˘gan, Ürün, Glasmachers, Tobias, and Igel, Christian. A uniﬁed view on multi-class support
vector classiﬁcation. Journal of Machine Learning Research (JMLR), 17(45):1–32, 2016.

[18] Duchi, John C., Mackey, Lester W., and Jordan, Michael I. On the consistency of ranking

algorithms. In ICML, 2010.

[19] Durbin, Richard, Eddy, Sean, Krogh, Anders, and Mitchison, Graeme. Biological sequence
analysis: probabilistic models of proteins and nucleic acids. Cambridge university press, 1998.

10

[20] Gao, Wei and Zhou, Zhi-Hua. On the consistency of multi-label learning. In COLT, 2011.

[21] Gimpel, Kevin and Smith, Noah A. Softmax-margin CRFs: Training loglinear models with cost

functions. In NAACL, 2010.

[22] Hazan, Tamir and Urtasun, Raquel. A primal-dual message-passing algorithm for approximated

large scale structured prediction. In NIPS, 2010.

[23] Keshet, Joseph. Optimizing the measure of performance in structured prediction. In Advanced

Structured Prediction. MIT Press, 2014.

[24] Kotlowski, Wojciech, Dembczynski, Krzysztof, and Huellermeier, Eyke. Bipartite ranking

through minimization of univariate loss. In ICML, 2011.

[25] Lafferty, John, McCallum, Andrew, and Pereira, Fernando. Conditional random ﬁelds: Proba-

bilistic models for segmenting and labeling sequence data. In ICML, 2001.

[26] Lee, Yoonkyung, Lin, Yi, and Wahba, Grace. Multicategory support vector machines: Theory
and application to the classiﬁcation of microarray data and satellite radiance data. Journal of
the American Statistical Association, 99(465):67–81, 2004.

[27] Lin, Yi. A note on margin-based loss functions in classiﬁcation. Statistics & Probability Letters,

68(1):73–82, 2004.

[28] London, Ben, Huang, Bert, and Getoor, Lise. Stability and generalization in structured prediction.

Journal of Machine Learning Research (JMLR), 17(222):1–52, 2016.

[29] Long, Phil and Servedio, Rocco. Consistency versus realizable H-consistency for multiclass

classiﬁcation. In ICML, 2013.

[30] McAllester, D. A. and Keshet, J. Generalization bounds and consistency for latent structural

probit and ramp loss. In NIPS, 2011.

[31] McAllester, David. Generalization bounds and consistency for structured labeling. In Predicting

Structured Data. MIT Press, 2007.

[32] Narasimhan, Harikrishna, Ramaswamy, Harish G., Saha, Aadirupa, and Agarwal, Shivani.

Consistent multiclass algorithms for complex performance measures. In ICML, 2015.

[33] Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Robust stochastic approximation
approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.

[34] Nowozin, Sebastian and Lampert, Christoph H. Structured learning and prediction in computer
vision. Foundations and Trends in Computer Graphics and Vision, 6(3–4):185–365, 2011.

[35] Nowozin, Sebastian, Gehler, Peter V., Jancsary, Jeremy, and Lampert, Christoph H. Advanced

Structured Prediction. MIT Press, 2014.

[36] Orabona, Francesco. Simultaneous model selection and optimization through parameter-free

stochastic learning. In NIPS, 2014.

[37] Pedregosa, Fabian, Bach, Francis, and Gramfort, Alexandre. On the consistency of ordinal
regression methods. Journal of Machine Learning Research (JMLR), 18(55):1–35, 2017.

[38] Pletscher, Patrick, Ong, Cheng Soon, and Buhmann, Joachim M. Entropy and margin maxi-

mization for structured output learning. In ECML PKDD, 2010.

[39] Ramaswamy, Harish G. and Agarwal, Shivani. Convex calibration dimension for multiclass

loss matrices. Journal of Machine Learning Research (JMLR), 17(14):1–45, 2016.

[40] Ramaswamy, Harish G., Agarwal, Shivani, and Tewari, Ambuj. Convex calibrated surrogates

for low-rank loss matrices with applications to subset ranking losses. In NIPS, 2013.

[41] Shi, Qinfeng, Reid, Mark, Caetano, Tiberio, van den Hengel, Anton, and Wang, Zhenhua. A
hybrid loss for multiclass and structured prediction. IEEE transactions on pattern analysis and
machine intelligence (TPAMI), 37(1):2–12, 2015.

11

[42] Smith, Noah A. Linguistic structure prediction. Synthesis lectures on human language tech-

[43] Steinwart, Ingo. How to compare different loss functions and their risks. Constructive Approxi-

[44] Taskar, Ben, Guestrin, Carlos, and Koller, Daphne. Max-margin markov networks. In NIPS,

nologies, 4(2):1–274, 2011.

mation, 26(2):225–287, 2007.

2003.

[45] Taskar, Ben, Chatalbashev, Vassil, Koller, Daphne, and Guestrin, Carlos. Learning structured

prediction models: a large margin approach. In ICML, 2005.

[46] Tewari, Ambuj and Bartlett, Peter L. On the consistency of multiclass classiﬁcation methods.

Journal of Machine Learning Research (JMLR), 8:1007–1025, 2007.

[47] Tsochantaridis, I., Joachims, T., Hofmann, T., and Altun, Y. Large margin methods for
structured and interdependent output variables. Journal of Machine Learning Research (JMLR),
6:1453–1484, 2005.

[48] Williamson, Robert C., Vernet, Elodie, and Reid, Mark D. Composite multiclass losses. Journal

of Machine Learning Research (JMLR), 17(223):1–52, 2016.

[49] Zhang, Tong. Statistical analysis of some multi-category large margin classiﬁcation methods.

Journal of Machine Learning Research (JMLR), 5:1225–1251, 2004.

[50] Zhang, Tong. Statistical behavior and consistency of classiﬁcation methods based on convex

risk minimization. Annals of Statistics, 32(1):56–134, 2004.

12

Supplementary Material (Appendix)

On Structured Prediction Theory with Calibrated
Convex Surrogate Losses

Outline

Section A: Proof of learning complexity Theorem 6.
Section B: Technical lemmas useful for the proofs.
Section C: Discussion and consistency results on a family of surrogate losses.
Section D: Bounds on the calibration functions.
Section D.1: Theorem 7 – a lower bound.
Section D.2: Theorem 8 – an upper bound.
Section D.3: Computation of the bounds for speciﬁc task losses.

Section E: Computations of the exact calibration functions for the quadratic surrogate.

Section E.1: 0-1 loss.
Section E.2: Block 0-1 loss.
Section E.3: Hamming loss.
Section E.4: Mixed 0-1 and block 0-1 loss.

Section F: Computing constants appearing in the SGD rate.
Section G: Properties of the basis of the Hamming loss.

A Learning complexity theorem

Theorem 6 (Learning complexity). Under the assumptions of Theorem 5, for any ε > 0, the random
(w.r.t. the observed training set) output ¯f(N ) ∈ FF,H of the ASGD algorithm after

N > N ∗ := 4D2M 2
Φ,L,F (ε)

ˇH 2

(19)

iterations has the expected excess risk bounded with ε, i.e., IE[RL(¯f(N ))] < R∗

L,F + ε.

Proof. By (10) from Theorem 5, N steps of the algorithm, in expectation, result in ˇHΦ,L,F (ε)
accuracy on the surrogate risk, i.e., IE[RΦ(¯f(N ))] − R∗
Φ,F < ˇHΦ,L,F (ε). We now generalize the
proof of Theorem 2 to the case of expectation w.r.t. ¯f(N ) depending on the random samples used
by the ASGD algorithm. We take the expectation of (4) w.r.t. ¯f(N ) substituted as f and use Jensen’s
inequality (by convexity of ˇHΦ,L,F ) to get IE[RΦ(¯f(N ))]−R∗
L,F )] ≥
ˇHΦ,L,F (IE[RL(¯f(N ))] − R∗
L,F <
ε.

Φ,F ≥ IE[ ˇHΦ,L,F (RL(¯f(N ))−R∗
L,F ). Finally, monotonicity of ˇHΦ,L,F implies IE[RL(¯f(N ))] − R∗

B Technical lemmas

In this section, we prove two technical lemmas that simplify the proofs of the main theoretical claims
of the paper.

Lemma 9 computes the excess of the weighted surrogate risk δφ for the quadratic loss Φquad (12),
which is central to our analysis presented in Section 4. The key property of this result is that the
excess δφ is jointly convex w.r.t. the parameters θ and conditional distribution q, which simpliﬁes
further analysis.

13

Lemma 10 allows to cope with the combinatorial aspect of the computation of the calibration function.
In particular, when the excess of the weighted surrogate risk is convex, Lemma 10 reduces the
computation of the calibration function to a set of convex optimization problems, which often can be
solved analytically. For symmetric losses, such as the 0-1, block 0-1 and Hamming losses, Lemma 10
also provides “symmetry breaking”, meaning that many of the obtained convex optimization problems
are identical up to a permutation of labels.
Lemma 9. Consider the quadratic surrogate Φquad (12) deﬁned for a task loss L. Let a subspace
of scores F ⊆ Rk be parametrized by θ ∈ Rr, i.e., f = F θ ∈ F with F ∈ Rk×r, and assume that
span(L) ⊆ F. Then, the excess of the weighted surrogate loss can be expressed as

δφquad(F θ, q) := φquad(F θ, q) − inf
θ(cid:48)∈Rr

φquad(F θ(cid:48), q) = 1

2k (cid:107)F θ + Lq(cid:107)2
2.

Proof. By using the deﬁnition of the quadratic surrogate Φquad (12), we have

φ(f (θ), q) = 1

2k (θTF TF θ + 2θTF TLq) + r(q),
θ∗ := argminθ φ(f (θ), q) = −(F TF )†F TLq,

δφ(f (θ), q) = 1

2k (θTF TF θ + 2θTF TLq

+ qTLTF (F TF )†F TLq),

where r(q) denotes the quantity independent of parameters θ. Note that PF := F (F TF )†F T is
the orthogonal projection on the subspace span(F ), so if span(L) ⊆ span(F ) we have PF L = L,
which ﬁnishes the proof.

Lemma 10. In the case of a ﬁnite number k of labels, for any task loss L, a surrogate loss Φ that is
continuous and bounded from below, and a set of scores F, the calibration function can be written as

HΦ,L,F (ε) = min

Hij(ε),

i,j∈pred(F )
i(cid:54)=j

where the set pred(F) ⊆ Y is deﬁned as the set of labels that the predictor can predict for some
feasible scores and Hij is deﬁned via minimization of the same objective as (5), but w.r.t. a smaller
domain:

(20)

(21)

δφ(f , q),

Hij(ε) = inf
f ,q
s.t. (cid:96)i(q) ≤ (cid:96)j(q) − ε,

(cid:96)i(q) ≤ (cid:96)c(q), ∀c ∈ pred(F),
fj ≥ fc, ∀c ∈ pred(F),
f ∈ F,
q ∈ ∆k.

Here (cid:96)c(q) := (Lq)c is the expected loss if predicting label c. Index i represents a label with the
smallest expected loss while index j represents a label with the largest score.

Proof. We use the notation Fj to deﬁne the set of score vectors f where the predictor pred(f ) takes a
value j, i.e., Fj := {f ∈ F | pred(f ) = j}. The union of the sets Fj, j ∈ pred(F), equals the whole
set F. It is possible that sets Fj do not fully contain their boundary because of the usage of a particular
tie-breaking strategy, but their closure can be expressed as F j := {f ∈ F | fj ≥ fc, ∀c ∈ pred(F)}.

If f ∈ Fj, i.e. j = pred(f ), then the feasible set of probability vectors q for which a label i is one of
the best possible predictions (i.e. δ(cid:96)(f , q) = (cid:96)j(q) − (cid:96)i(q) ≥ ε) is

∆k,i,j,ε := {q ∈ ∆k | (cid:96)i(q) ≤ (cid:96)c(q), ∀c ∈ pred(F); (cid:96)j(q) − (cid:96)i(q) ≥ ε},

because inf f (cid:48)∈F (cid:96)(f (cid:48), q) = minc∈pred(F ) (cid:96)c(q).
The union of the sets {Fj × ∆k,i,j,ε}i,j∈pred(F ) thus exactly equals the feasibility set of the optimiza-
tion problem (5)-(6) (note that this is not true for the union of the sets {F j × ∆k,i,j,ε}i,j∈pred(F ),
which can be strictly larger), thus we can rewrite the deﬁnition of the calibration function as follows:

HΦ,L,F (ε) = min

i,j∈pred(F )
i(cid:54)=j

inf
f ∈Fj ,
q∈∆k,i,j,ε

δφ(f , q).

(22)

14

To ﬁnish the proof, we use Lemma 27 of [49] claiming that the function δφ(f , q) is continuous w.r.t.
both q and f , which allows us to substitute sets Fj in (22) with their closures F j without changing
the value of the inﬁmum.

C Consistent surrogate losses

An ideal surrogate should not only be consistent, but also allow efﬁcient optimization, by, e.g., being
convex and allowing fast computation of stochastic gradients. In this paper, we study a generalization
to arbitrary multi-class losses of a surrogate loss class from Zhang [49, Section 4.4.2]8 that satisﬁes
these requirements:

(cid:88)k

Φa,b(f , y) := 1
k

(23)
where a, b : R → R are convex functions. A generic method to minimize this surrogate is to use
any version of the SGD algorithm, while computing the stochastic gradient by sampling y from the
data generating distribution and a label c uniformly. In the case of the quadratic surrogate Φquad, we
proposed instead in the main paper to compute the sum over c analytically instead of sampling c.

(cid:0)L(c, y)a(fc) + b(fc)(cid:1),

c=1

Extending the argument from Zhang [49], we show that the surrogates of the form (23) are consistent
w.r.t. a task loss L under some sufﬁcient assumptions formalized in Theorem 11.
Theorem 11 (Sufﬁcient conditions for consistency). The surrogate loss Φa,b is consistent w.r.t. a task
loss L, i.e., HΦa,b,L,Rk (ε) > 0 for any ε > 0, under the following conditions on the functions a(f )
and b(f ):

1. The functions a and b are convex and differentiable.
2. The function ca(f ) + b(f ) is bounded from below and has a unique global minimizer (ﬁnite

or inﬁnite) for all c ∈ [0, Lmax].

3. The functions a(f ) and b(cid:48)(f )

a(cid:48)(f ) are strictly increasing.

Proof. Consider an arbitrary conditional probability vector q ∈ ∆k. Assumption 2 then implies
that the global minimizer f ∗ of the conditional surrogate risk φ(f , q) w.r.t. f is unique. Assump-
tion 1 allows us to set the derivatives to zero and obtain b(cid:48)(f ∗
c )
c ) = −(cid:96)c(q) where (cid:96)c(q) := (Lq)c.
a(cid:48)(f ∗
Assumption 3 then implies that f ∗

i holds if and only if (cid:96)j(q) ≤ (cid:96)i(q).

j ≥ f ∗

Now, we will prove by contradiction that H(ε) := HΦa,b,L,Rk (ε) > 0 for any ε > 0. Assume that
for some ε > 0 we have H(ε) = 0. Lemma 10 then implies that for some i, j ∈ Y, i (cid:54)= j, we have
Hij(ε) = 0. Note that the domain of (21) deﬁning Hij is separable w.r.t. q and f . We can now
rewrite (21) as

Hij(ε) = inf

q∈∆k,i,j,ε

δφ∗(q), where δφ∗(q) := inf
f ∈F j

δφ(f , q),

where ∆k,i,j,ε and F j are deﬁned in the proof of Lemma 10. Lemma 27 of [49] implies that the
function δφ∗(q) is a continuous function of q. Given that ∆k,i,j,ε is a compact set, the inﬁmum is
achieved at some point q∗ ∈ ∆k,i,j,ε. For this q∗, the global minimum w.r.t. f exists (Assumption 2).
The uniqueness of the global minimum implies that we have f ∗
c . The argument at
the beginning of this proof then implies (cid:96)j(q∗) ≤ (cid:96)i(q∗) which contradicts the inequality (cid:96)i(q∗) ≤
(cid:96)j(q∗) − ε in the deﬁnition of ∆k,i,j,ε.

j = maxc∈Y f ∗

Note that Theorem 11 actually proves that the surrogate Φa,b is order-preserving [49], which is a
stronger property than consistency.

Below, we give several examples of possible functions a(f ), b(f ) that satisfy the conditions in
Theorem 11 and their corresponding f ∗((cid:96)) := argminf ∈Rk φ(f , q) when (cid:96) := Lq:

1. If a(f ) = f , b(f ) = f 2
2. If a(f ) = 1
Lmax
2 log( 1
(cid:96)).
Lmax

1

2 then f ∗((cid:96)) = −(cid:96), leading to our quadratic surrogate (12).

(exp(f ) − exp(−f )), b(f ) = exp(−f ) then f ∗((cid:96)) = 1

2 log(1 − 1
Lmax

(cid:96)) −

8Zhang [49] refers to this surrogate as “decoupled unconstrained background discriminative surrogate”. Note

the 1/k scaling to make Φa,b of order 1.

15

3. If a(f ) = 1
Lmax

f , b(f ) = log(1 + exp(−f )) then f ∗((cid:96)) = log(1 − 1
Lmax

(cid:96)) − log( 1
Lmax

(cid:96)).

In the case of binary classiﬁcation, these surrogates reduce to L2-, exponential, and logistic losses,
respectively.

D Bounds on the calibration function

D.1 Lower bound

Theorem 7 (Lower bound on HΦquad ). For any task loss L, its quadratic surrogate Φquad, and a score
subspace F containing the column space of L, the calibration function can be lower bounded:

HΦquad,L,F (ε) ≥

ε2
2k maxi(cid:54)=j (cid:107)PF ∆ij (cid:107)2
2

≥ ε2
4k ,

where PF is the orthogonal projection on the subspace F and ∆ij = ei − ej ∈ Rk with ec being
the c-th basis vector of the standard basis in Rk.

Proof. First, let us assume that the score subspace F is deﬁned as the column space of a matrix F ∈
Rk×r, i.e., f (θ) = F θ. Lemma 9 gives us expression (13) for δφquad(F θ, q), which is jointly convex
w.r.t. a conditional probability vector q and parameters θ.

The optimization problem (5)-(6) is non-convex because the constraint (6) on the excess risk depends
of the predictor function pred(f ), see Eq. (1), containing the argmax operation. However, if we
constrain the predictor to output label j, i.e., fj ≥ fc, ∀c, and the label delivering the smallest possible
expected loss to be i, i.e., (Lq)i ≤ (Lq)c, ∀c, the problem becomes convex because all the constraints
are linear and the objective is convex. Lemma 10 in App. B allows to bound the calibration function
Hij(ε),9 where Hij(ε) is
with the minimization w.r.t. selected labels i and j, HΦquad,L,F (ε) ≥ min
i(cid:54)=j
deﬁned as follows:

1

Hij(ε) = min
θ,q
s.t. (Lq)i ≤ (Lq)j − ε,

2k (cid:107)F θ + Lq(cid:107)2
2,

(Lq)i ≤ (Lq)c, ∀c ∈ pred(F)
(F θ)j ≥ (F θ)c, ∀c ∈ pred(F)
q ∈ ∆k.

To obtain a lower bound, we relax (24) by removing some of the constraints and arrive at

1

2k (cid:107)F θ + Lq(cid:107)2
2,

Hij(ε) ≥ min
θ,q
s.t. ∆T
∆T

ijLq ≤ −ε,
ijF θ ≤ 0,

(27)
ijF θ = (F θ)i − (F θ)j, and ∆ij = ei − ej ∈ Rk with ec ∈ Rk

where ∆T
ijLq = (Lq)i − (Lq)j, ∆T
being a vector of all zeros with 1 at position c.

The constraint (26) can be readily substituted with equality

∆T
without changing the minimum because multiplication of both q and θ by the constant −ε
preserves feasibility and can only decrease the objective (25).

ijLq = −ε,

(28)
ij Lq ∈ (0, 1]
∆T

We now explicitly solve the resulting constraint optimization problem via the KKT optimality
conditions. The stationarity constraints give us

1

k F T(F θ + Lq) + µF T∆ij = 0,
k LT(F θ + Lq) + νLT∆ij = 0;
9To simplify the statement of Theorem 7, we removed the constraints i, j ∈ pred(F) from Lemma 10 which
said that we should consider only the labels that can be predicted with some feasible scores. A potentially tighter
lower bound can be obtained by keeping the i, j ∈ pred(F) constraint.

(29)

(30)

1

16

(24)

(25)

(26)

the complementary slackness gives µ∆T
µ ≥ 0.

Equation (29) allows to compute

ijF θ = 0 and the feasibility constraints give (28), (27), and

θ = −(F TF )†(kµF T∆ij + F TLq).

By substituting (31) into (30) and by using the identity (because L ∈ span(F )):

PF L = F (F TF )†F TL = L,
(32)
we get (µ − ν)LT∆ij = 0. If LT∆ij = 0, the problem (25), (27), (28) is infeasible for ε > 0
implying Hij(ε) = +∞. Otherwise, we have µ = ν.

From (31) and (32), we also have that:

F θ + Lq = −kµPF ∆ij.

By plugging (31) into the complementary slackness condition and combining with (28), we get
µ2k(cid:107)PF ∆ij(cid:107)2

2 = µε

implying that either µ = 0 or µk(cid:107)PF ∆ij(cid:107)2
2 = ε. In the ﬁrst case, Eq. (33) implies F θ = −Lq
making satisfying both (28) and (27) impossible. Thus, the later is satisﬁed implying that the
objective (25) is equal to10

1

2k (cid:107)F θ + Lq(cid:107)2

2 =

ε2
2k(cid:107)PF ∆ij (cid:107)2
2

.

Finally, orthogonal projections contract the L2-norm, thus (cid:107)PF ∆ij(cid:107)2
lower bound in the statement of the theorem and ﬁnishes the proof.

2 ≤ 2, which gives the second

D.2 Upper bound

Theorem 8 (Upper bound on HΦquad ). If a loss matrix L with Lmax > 0 deﬁnes a pseudometric7 on
labels and there are no constraints on the scores, i.e., F = Rk, then the calibration function for the
quadratic surrogate Φquad can be upper bounded:
HΦquad,L,F (ε) ≤ ε2
2k ,

0 ≤ ε ≤ Lmax.

Proof. After applying Lemmas 9 and 10, we arrive at

(31)

(33)

(34)

1

2k (cid:107)f + Lq(cid:107)2,

Hij(ε) = inf
f ,q
s.t. (cid:96)i(q) ≤ (cid:96)j(q) − ε,

(cid:96)i(q) ≤ (cid:96)c(q), ∀c ∈ Y,
fj ≥ fc, ∀c ∈ Y,
f ∈ Rk,
q ∈ ∆k.

2 − ε
We now consider labels i and j such that Lij = Lmax > 0 and the point qi = 1
2Lij
(non-negative for ε ≤ Lmax). We let qc = 0 for c (cid:54)∈ {i, j}, fj = fi = −(cid:96)i(q) and fc = −(cid:96)c(q) for
c (cid:54)∈ {i, j}. We now show that this assignment is feasible.

2 + ε
2Lij

, qj = 1

We have (cid:96)j(q) = qiLji + qjLjj = qiLji = qiLij by symmetry of L. Similarly, (cid:96)i(q) = qjLij and
thus

(cid:96)j(q) − (cid:96)i(q) = Lij

ε
Lij

= ε.

We also have

(cid:96)c(q) − (cid:96)i(q) = qiLci + qjLcj − qjLij ≥ qj(Lic + Lcj − Lij) ≥ 0.
The ﬁrst inequality uses qi ≥ qj and the second inequality uses the fact that L satisﬁes the triangle
inequality (as a pseudometric). Finally, fj − fc = −(cid:96)i(q) + (cid:96)c(q) ≥ 0.

We thus have shown that the deﬁned point is feasible, so we compute its objective value. We have

1

2k (cid:107)f + Lq(cid:107)2 = 1

2k ((cid:96)j(q) − (cid:96)i(q))2 = ε2
2k ,

which completes the proof.

µ∗ = ∞).

10The possibility PF ∆ij = 0 is also covered by this equation with the convention that 1/0 = ∞ (in this case,

17

D.3 Computation of the lower bounds for speciﬁc task losses

4k , which is shown to be tight in Proposition 12 of Section E.1.

0-1 loss. Let L01 denote the loss matrix of the 0-1 loss, i.e., L01(i, j) := [i (cid:54)= j].2 It is convenient
k − Ik, where 1k ∈ Rk is the vector of all ones and
to rewrite it with a matrix notation L01 = 1k1T
Ik ∈ Rk×k is the identity matrix. We have rank(L01) = k (for k ≥ 2), thus span(L) = Rk. By
putting no constraints on the scores, we can easily apply Theorem 7 and obtain the lower bound
of ε2
Block 0-1 loss. We use the symbol L01,b to denote the loss matrix of the block 0-1 loss with b blocks,
i.e., L01,b(i, j) := [i and j are not in the same block]. We use sv to denote the size of block v,
v = 1, ..., b, and then s1 + · · · + sb = k. In the case when all the blocks are of equal sizes, we denote
their size by s and have k = bs.
k − U U T where the columns of the matrix U ∈ Rk×b
With a matrix notation, we have L01,b = 1k1T
are indicators of the blocks. We have rank(L01,b) = b and can simply deﬁne F01,b := span(F01,b)
with F01,b := U . If we assume that all the blocks have equal size, then we have U TU = sIb and
(cid:107)PF01,b ∆ij(cid:107)2
s if labels i and j belong to different blocks, while PF01,b ∆ij = 0 if i and j belong
to the same block. This leads to the lower bound ε2
4b , which is shown to be tight in Proposition 14 of
Section E.2.

2 = 2

Hamming loss. Consider the (normalized) Hamming loss between tuples of T binary variables,
where ˆyt and yt are the t-th variables of a prediction ˆy and a correct label y, respectively:

LHam,T ( ˆy, y) := 1
T

(cid:88)T

[ˆyt (cid:54)= yt]

t=1

(35)

= 1
T

= 1
T

= 1
T

(cid:88)T

(cid:88)T

(cid:88)T

t=1

t=1

t=1

([ˆyt = 0][yt = 1] + [ˆyt = 1][yt = 0])

(1 − [ˆyt = 1])[yt = 1] + [ˆyt = 1][yt = 0])

[yt = 1] + 1
T

(cid:88)T

t=1

([yt = 0] − [yt = 1]) [ˆyt = 1]

= α0(y) +

αt(y)[ˆyt = 1],

(cid:88)T

t=1

The vectors αt(·) depend only on the column index of the loss matrix. The decomposition (35) im-
plies that FHam,T := span(FHam,T ) equals to span(LHam,T ) for FHam,T := [ 1
2 12T , h(1), . . . , h(T )],
(h(t)) ˆy := [ˆyt = 1], t = 1, . . . , T . We also have that rank(LHam,T ) = rank(FHam,T ) = T + 1.
In Section G, we show that maxi(cid:54)=j (cid:107)PFHam,T ∆ij(cid:107)2
bound (14), we get HΦquad,LHam,T ,FHam,T ≥ ε2
of Section E.3.

2T . By plugging this identity into the lower
8T , which appears to be tight according to Proposition 15

2 = 4T

Non-tight cases. In the cases of the block 0-1 loss and the mixed 0-1 and block 0-1 loss (Proposi-
tions 13 and 16, respectively), we observe gaps between the lower bound (14) and the exact calibration
functions, which shows the limitations of the bound. In particular, it cannot detect level-η consistency
for η > 0 (see Deﬁnition 3) and does not change when the loss changes, but the score subspace stays
the same.

E Exact calibration functions for quadratic surrogate

This section presents our derivations for the exact values of the calibration functions for different
losses. While doing these derivations, we have used numerical simulations and symbolic derivations
to check for correctness. Our numerical and symbolic tools are available online.11

E.1 0-1 loss

Proposition 12. Let L01 be the 0-1 loss, i.e., L01(i, j) = [i (cid:54)= j]. Then, the calibration function
equals the following quadratic function w.r.t. ε:

HΦquad,L01,Rk (ε) = ε2
4k ,
11https://github.com/aosokin/consistentSurrogates_derivations

0 ≤ ε ≤ 1.

(36)

18

Note that in the case of binary classiﬁcation, the function (36) is equal to the calibration function for
the least squares and truncated least squares surrogates [5, 43].

Proof. First, Lemma 9 with F = Rk and F = Ik gives us the expression

δφquad(F θ, q) = 1

2k (cid:107)f + Lq(cid:107)2
2,

(37)

with f = θ ∈ Rk.

We now reduce the optimization problem (5)-(6) to a convex one by using Lemma 10 and by
writing HΦquad,L01,Rk (ε) = mini(cid:54)=j∈Y Hij(ε), which holds because pred(Rk) = Y. Because of the
symmetries of the 0-1 loss, all the choices of i and j give the same (up to a permutation of labels)
optimization problem to compute Hij(ε). The deﬁnition of the 0-1 loss implies (Lq)c = 1−qc, which
simpliﬁes the excess of the expected task loss appearing in (6) to δ(cid:96)(f , q) = (Lq)j −(Lq)i = qi −qj.
After putting all these together, we get

(38)

Hij(ε) = min
f ,q

1
2k

k
(cid:88)

(fc + 1 − qc)2,

c=1
s.t. qi ≥ qj + ε,

qi ≥ qc, c = 1, . . . , k,
fj ≥ fc, c = 1, . . . , k,

k
(cid:88)

c=1

qc = 1,

qc ≥ 0.

2 ; f ∗

2 − ε

c = −1, c (cid:54)∈ {i, j}, f ∗ := f ∗

2 + ε
We claim that there exists an optimal point of (38), f ∗, q∗, such that q∗
2 ,
j = 1
q∗
j . Note that apart from the speciﬁc value of f ∗, this
is the same point used to prove the upper bound of Theorem 8. After proving this, we will minimize
the objective w.r.t. remaining scores at this point.12
First, if any q∗
operation

c = δ > 0, c (cid:54)∈ {i, j}, we can safely move this probability mass to qi and qj with the

c = 0, c (cid:54)∈ {i, j}, q∗

i = f ∗

i = 1

i + δ
2 ,
i + δ
2 ,

i := q∗
q∗
i := f ∗
f ∗

c := q∗
q∗
c := f ∗
f ∗

c − δ = 0,
c − δ,

j + δ
2 ,
j + δ
2 ,
which keeps all the constraints of (38) feasible and does not change the objective value.
Second, all the scores f ∗
the objective. With this, setting f ∗
violate the constraints.
We now show that the equality q∗
the operation

j + ε can hold at the optimum. Indeed, if q∗

c have to belong to the segment [−1, 0] otherwise clipping them will decrease
c := −1, c (cid:54)∈ {i, j} can only decrease the objective and will not

j := q∗
q∗
j := f ∗
f ∗

j = δ(cid:48) > ε,

i = q∗

i − q∗

q∗
i := q∗
i := f ∗
f ∗

i − δ(cid:48)−ε
2 ,
i − δ(cid:48)−ε
2 ,

q∗
j := q∗
j := f ∗
f ∗

j + δ(cid:48)−ε
2 ,
j + δ(cid:48)−ε
2 .

(39)

keeps the objective the same and maintains the feasibility constraints. So combining with q∗
j = 1
i = 1
we can now conclude that q∗
We now show that the equality f ∗
j can hold at the optimum. First, we know that the values
f ∗
i and f ∗
i − 1], otherwise we can always truncate the values to
the borders of the segment and get an improvement of the objective. Finally, since the inequality
f ∗
j ≥ f ∗
i − 1 to
minimize the objective.

i must hold, we conclude that f ∗

j belong to the segment [q∗

2 + ε
2 , q∗
i = f ∗

is closest to its target q∗

2 is an optimal point.

j := f ∗ so that f ∗
i

j − 1, q∗

i = f ∗

2 − ε

i + q∗

j = 1,

12Note that just showing the feasibility of the assigned values q∗ and f ∗ give us an upper bound on the
calibration function. In the case of the 0-1 loss, it appears that this upper bound matches the lower bound
provided by Theorem 7, so we do not need to prove optimality explicitly. However, we still give this proof as a
simple illustration of the proof technique as its structure will be re-used also for the cases when the bound of
Theorem 7 is not tight.

19

At the optimal point deﬁned above, it remains to ﬁnd the value f ∗ delivering the minimum of the
objective. We can achieve this by computing

Hij(ε) = 1

2k min

f ∈[−1,0]

(f + 1

2 − ε

2 )2 + (f + 1

2 + ε

2 )2,

which implies f ∗ = −0.5 and HΦquad,L01,Rk (ε) = ε2
4k .

2 + ε

2 − ε
Remark. We note that the conditional distribution used in the proof above, qi = 1
2 ,
qc = 0, c (cid:54)∈ {i, j}, is somewhat unsatisfying from the perspective of explaining why learning the
0-1 loss might be difﬁcult. Indeed, it looks like a gradient based learning algorithm that would start
with all values fc = −1 would at the end only optimize over fi and fj as the gradient with respect
to fc for c /∈ {i, j} would stay at zero in Φquad(f , y) (12) given that only i or j could appear in y.
From this observation, one could think that the calibration function perspective is misleading as SGD
could have faster convergence rate than predicted by the worst case for this situation. Fortunately,
3(k−2) for c (cid:54)∈ {i, j}, fi = fj = 1
one can easily check that the point qi = 1
2 , qj = 1
3
and fc = −(cid:96)c(q) for c (cid:54)∈ {i, j} is feasible for (38) and yields the same optimal value of ε2
4k for the
objective, thus providing another example where the exponential multiclass nature is more readily
apparent and cannot be ﬁxed by some “natural initialization” of the learning algorithm.

2 , qc = 1

2 , qj = 1

3 − ε

3 + ε

E.2 Block 0-1 loss

Recall that L01,b is the block 0-1 loss, i.e., L01,b(i, j) = [i and j are not in the same block]. We use
b to denote the total number of blocks and sv to denote the size of block v, v = 1, ..., b. In this
section, we compute the calibration functions for the case of unconstrained scores (Proposition 13)
and for the case of the scores belonging to the column span of the loss matrix (Proposition 14).
Proposition 13. Without constraints on the scores, the calibration function for the block 0-1 loss
equals the following quadratic function w.r.t. ε:

HΦquad,L01,b,Rk (ε) = ε2

4k min

v=1,...,b

2sv

sv+1 ≤ ε2
2k ,

0 ≤ ε ≤ 1.

Note that when sv = 1 for some v, we have HΦquad,L01,b,Rk (ε) matching to the ε2
Theorem 7. When sv → ∞ for all blocks, we have HΦquad,L01,b,Rk (ε) matching to the ε2
of Theorem 8.

4k lower bound of
2k upper bound

Proof. This proof is of the same structure as the proof of Proposition 12 above.

We use b(i) ∈ 1, . . . , b to denote the block to which label i belongs and Yv to denote the set of labels
that belong to block v. We also use Qv, v ∈ 1, . . . , b, as a shortcut to (cid:80)
qi, which is the total
probability mass on block v.

i∈Yv

We start by noting that the i-th component of the vector (L01,b)q equals 1 − Qb(i). By applying
Lemmas 9, 10, we get

Hij(ε) = min
f ,q

1
2k

b
(cid:88)

(cid:88)

(fc + 1 − Qb(c))2,

v=1

c∈Yv
Qb(i) − Qb(j) ≥ ε,
Qb(i) ≥ Qu, u = 1, . . . , b,
fj ≥ fc, c = 1, . . . , k,
(cid:88)k

qc = 1,

qc ≥ 0.

c=1

(40)

(41)

Analogously to Proposition 12, we claim that there exists an optimal point of (40) such that qc = 0,
c (cid:54)∈ {i, j}; qi = 0.5 + ε

2 = Qb(j); fc = −1, c (cid:54)∈ Yij := Yb(i) ∪ Yb(j).

2 = Qb(i); qj = 0.5 − ε

At ﬁrst, note that if b(i) = b(j), then the constraint (41) is never feasible, so we’ll assume that
b(i) (cid:54)= b(j).

20

We will now show that we can consider only conﬁgurations with all the probability mass on the
two selected blocks. Consider some optimal point f ∗, q∗ and denote with δ = (cid:80)
q∗
c the
probability mass on the unselected blocks. The operation
c + δ
i + δ

f ∗
c := −1, c (cid:54)∈ Yij
q∗
c := 0, c (cid:54)∈ Yij

f ∗
c := f ∗
i := q∗
q∗

2 , c ∈ Yij,
j := q∗
2 , q∗

j + δ
2 ,

c∈Y\Yij

can only decrease the objective of (40) because the summands corresponding to the unselected blocks
are set to zero. All the constraints stay feasible and the summands corresponding to the selected
blocks keep their values.
The probability mass within the block b(i) can be safely moved to q∗
i without changing the objective
or violating any constraints. Analogously, the probability mass within the block b(j) can be safely
moved to q∗
j + ε and thus that
i = 1
2 + ε
q∗
At the point deﬁned above, we now minimize the objective (40) w.r.t. fc, c ∈ Yij. At an optimal
point, all values f ∗
b(i) − 1], otherwise we can always
truncate the values to the borders of the segment and get an improvement of the objective. For all the
scores f ∗

j . By reusing the operation (39), we can now ensure that q∗
2 and q∗

c , c ∈ Yij, belong to the segment [Q∗

c , c (cid:54)= j, the following identity holds

b(j) − 1, Q∗

i = q∗

2 − ε
2 .

j = 1

(cid:40)

f ∗
c =

Q∗
b(c) − 1, if Q∗
f ∗
j .

b(c) − 1 < f ∗
j ,

Combining with the segment constraint, it implies that in the block of the label i, we have f ∗
c ∈ Yb(i), and, in the block of the label j, we have f ∗

b(j) − 1, c ∈ Yb(j) \ j.

c = Q∗

c = f ∗
j ,

By plugging the obtained values of q∗
get

c and f ∗

c into (40) and denoting the value f ∗

j + 0.5 with ˜f , we

(cid:16)

sb(i)( ˜f − ε

2 )2 + ( ˜f + ε

2 )2(cid:17)

,

1
2k

Hij(ε) = min
˜f
s.t. ˜f ∈ [− ε

2 , ε
2 ].

By setting the derivative of the objective (43) to zero, we get

(42)

(43)

which belongs to the segment [− ε

2 , ε

2 ]. We compute the function value at this point:

˜f = ε
2

sb(i)−1
sb(i)+1 ,

Hij(ε) = ε2
4k

2sb(i)
sb(i)+1 ,

which ﬁnishes the proof.

Proposition 14. Let the scores f be piecewise constant on the blocks of the loss, i.e. belong to the
subspace F01,b = span(L01,b) ⊆ Rk. Then, the calibration function equals the following quadratic
function w.r.t. ε:

HΦquad,L01,b,F01,b (ε) = ε2

4k min
v(cid:54)=u

2svsu
sv+su

,

0 ≤ ε ≤ 1.

If all the blocks are of the same size, we have HΦquad,L01,b,F01,b (ε) = ε2
blocks.

4b where b is the number of

Proof. The constraints on scores f ∈ F01,b simply imply that the scores within all the blocks are
equal. Having this in mind, the proof exactly matches the proof of Proposition 13 until the argument
around Eq. (42). Now we cannot set the scores of the block b(j) to different values, and, thus they
are all equal to f ∗.

By plugging the obtained values of q∗
get

c and f ∗

c into (40) and denoting the value f ∗

j + 0.5 with ˜f , we

(cid:16)

sb(i)( ˜f − ε

2 )2 + sb(j)( ˜f + ε

2 )2(cid:17)

,

1
2k

Hij(ε) = min
˜f
s.t. ˜f ∈ [− ε

(44)

2 , ε
2 ].

21

By setting the derivative of the objective (44) to zero, we get

˜f = ε
2

sb(i)−sb(j)
sb(i)+sb(j)

,

which belongs to the segment [− ε

2 , ε

2 ]. We now compute the function value at this point:
,

Hij(ε) = ε2
4k

2sb(i)sb(j)
sb(i)+sb(j)

which ﬁnishes the proof.

E.3 Hamming loss

Recall that LHam,T is the Hamming loss deﬁned over T binary variables (see Eq. (35) for the precise
deﬁnition). In this section, we compute the calibration function for the case of the scores belonging
to the column span of the loss matrix (Proposition 15).
Proposition 15. Assume that the scores f always belong to the column span of the Hamming loss
matrix LHam,T , i.e., FHam,T = span(LHam,T ) ⊆ Rk. Then, the calibration function can be computed
as follows:

HΦquad,LHam,T ,FHam,T (ε) = ε2
8T ,

0 ≤ ε ≤ 1.

Proof. We start the proof by applying Lemma 10 and by studying the vector of the expected
losses (LHam,T )q. We note that the ˆy-th element (cid:96) ˆy(q), ˆy = (ˆyt)T
t=1, ˆyt ∈ {0, 1}, has a simple form
of

(cid:96) ˆy(q) =

(cid:88)

qy
T

T
(cid:88)

y∈Y

t=1

[ˆyt (cid:54)= yt] = 1 − 1
T

qy[ˆyt = yt].

T
(cid:88)

(cid:88)

t=1

y∈Y

The quantity (cid:80)
y∈Y qy[ˆyt = yt] corresponds to the marginal probability of a variable t taking a
label ˆyt. Note that the expected loss (cid:96) ˆy(q) only depends on q through marginal probabilities, thus
two distributions q1 and q2 with the same marginals would be indistinguishable when plugged
in the optimization problem for Hij(ε) (21), given that both the constraints and the objective (by
Lemma 9) only depend on q through the expected loss (cid:96) ˆy(q). Having this in mind, we can consider
(cid:0)qt[yt = 1] + (1 − qt)[yt = 0](cid:1), where qt ∈ [0, 1],
only separable distributions, i.e., qy = (cid:81)T
t = 1, . . . , T , are the parameters deﬁning the distribution.

t=1

By combining the notation above with Lemmas 9 and 10, we arrive at the following optimization
problem:

H ˜y ˆy(ε) = min
f ,q

1
2k

fy +1− 1
T

(cid:88)T

t=1

(cid:17)2

,

qt,yt

s.t. 1
T

(qt,˜yt −qt,ˆyt) ≥ ε,

k
(cid:88)

(cid:16)

y∈Y
(cid:88)T

t=1

(cid:88)T

(qt,˜yt −qt,yt) ≥ 0, ∀y ∈ Y,

t=1

1
T
f ˆy ≥ fy, ∀y ∈ Y,
0 ≤ qt ≤ 1, t = 1, . . . , T,
f ∈ F,

(48)
(49)
(50)
where qt,yt is a shortcut to qt[yt = 1] + (1 − qt)[yt = 0] and labels ˜y and ˆy serve as the selected
labels i and j, respectively.
The calibration function HΦquad,LHam,T ,FHam,T (ε) = ε2
8T in the formulation of this proposition matches
the lower bound provided by Theorem 7 in Section D.3. Thus, it sufﬁces to construct a feasible
w.r.t. (46)-(50) assignment of variables f , q and labels ˜y, ˆy such that the objective equals the lower
bound.

It sufﬁces to simply set ˜y to all zeros and ˆy to all ones. In this case, the constraints (46) and (47) take
the simpliﬁed form:

(45)

(46)

(47)

(51)

(52)

(cid:88)T

(1 − 2qt) ≥ ε,

1
T
t=1
qt ≤ 1
2 , t = 1, . . . , T.

22

We now set qt := 1
2 1k. This point is clearly feasible when
0 ≤ ε ≤ 1, so it remains to compute the value of the objective. We complete the proof by writing
(let w be the count of ones in an assignment y):

2 , t = 1, . . . , T , and f := − 1

2 − ε

k
(cid:88)

(cid:16)

1
2k

1
2k

1
2k

y∈Y

T
(cid:88)

w=0

T
(cid:88)

w=0

fy +1− 1
T

(cid:88)T

t=1

(cid:17)2

qt,yt

=

(cid:1)(cid:0) 1

(cid:0)T
w

2 − 1

T (w( 1

2 − ε

2 ) + (T − w)( 1

2 + ε

2 ))(cid:1)2

=

(cid:0)T
w

(cid:1)( ε

2 − wε

T )2 = ε2

2k

(cid:0)T
w

(cid:1)( 1

4 − w

T + w2

T 2 ) =

T
(cid:88)

w=0

ε2
2k ( 1

4 2T − 1
where we use the equality k = 2T and the identities (cid:80)T
(cid:80)T

T T 2T −1 + 1

(cid:1) = T (T + 1)2T −2.

t=0 t2(cid:0)T

t

T 2 T (T + 1)2T −2) = ε2
8T ,

(cid:0)T
t

(cid:1) = 2T , (cid:80)T

t=0 t(cid:0)T

t

(cid:1) = T 2T −1,

t=0

E.4 Mixed 0-1 and block 0-1 loss

Recall that L01,b,η is the convex combination of the 0-1 loss and the block 0-1 loss with b blocks, i.e.,
L01,b,η = ηL01 + (1 − η)L01,b, 0 ≤ η ≤ 1. Let all the blocks be of the same size s = k
b ≥ 2. In this
section, we compute the calibration functions for the case of unconstrained scores (Proposition 16)
and for the case when scores belong to the column span of the loss matrix (Proposition 17).
Proposition 16. If there are no constraints on scores f then the calibration function

HΦquad,L01,b,η,Rk (ε) =

(cid:40) ε2
ε ≤ η
4k ,
2k(s+1) − η(ε+1)(s−1)

1−η ,

4k(s+1)

ε2s

(2ε−εη−η)

η
1−η ≤ ε ≤ 1

shows that the surrogate is consistent.

Note that when η = 0, we have H(ε) = ε2
4k
H(ε) = ε2

4k , which matches Proposition 12.

2s
s+1 as in Proposition 13. When η ≥ 0.5 we have

Proof. This proof is very similar to the proof of Proposition 13, but technically more involved.

We start by noting that the i-th element of the vector (L01,b,η)q equals

(cid:88)

j: b(j)(cid:54)=b(i)

(cid:88)

j: j(cid:54)=i

(1 − η)qj +

ηqj = η(1 − qi) + (1 − η)(1 − Qb(i)),

(53)

where for b(i) and Qv we reuse the notation deﬁned in the proof of Proposition 13. By combining
this with Lemmas 9 and 10, we get

Hij(ε)=min
f ,q

1
2k

b
(cid:88)

(cid:88)

v=1

c∈Yv

(fc + 1 − ηqc − (1 − η)Qb(c))2,

(54)

s.t. η(qi − qj) + (1 − η)(Qb(i) − Qb(j)) ≥ ε,

η(qi − qc) + (1 − η)(Qb(i) − Qb(c)) ≥ 0, ∀c
fj ≥ fc, ∀c,

k
(cid:88)

c=1

qc = 1,

qc ≥ 0, ∀c.

The blocks are all of the same size so we need to consider just the two cases: 1) the selected labels
belong to the same block, i.e., b(i) = b(j); 2) the selected labels belong to the two different blocks,
i.e., b(i) (cid:54)= b(j).

23

The ﬁrst case can be proven by a straight forward generalization of the proof of Proposition 12. Given
that the loss value is bounded by 1, the maximal possible value of ε when the constraints can be
feasible equals η. Thus, we have Hij(ε) = ε2
We will now proceed to the second case b(i) (cid:54)= b(j). We show that

4k for ε ≤ η and +∞ otherwise.

Hij(ε)=

(cid:40) ε2
for ε ≤ η
4k ,
2k(s+1) − η(ε+1)(s−1)

1−η ,

4k(s+1)

ε2s

(2ε − εη − η), otherwise.

Similarly to the arguments used in Propositions 12 and 13, we claim that there is an optimal
point of (54) such that q∗
c = −1 for
c (cid:54)∈ Yij := Yb(i) ∪ Yb(j).
First, we will show that we can consider only conﬁgurations with all the probability mass on the two
selected blocks b(i) and b(j). Given any optimal point f ∗ and q∗, the operation (with δ = (cid:80)
q∗
c )

c = 0, c (cid:54)∈ {i, j}; q∗

i = 0.5 + ε

j = 0.5 − ε

2 ; and f ∗

2 ; q∗

c(cid:54)∈Yij

f ∗
i := f ∗
i + δ
2 ,
f ∗
j := f ∗
j + δ
2 ,
f ∗
c := −1, c (cid:54)∈ Yij
f ∗
c := f ∗

q∗
i := q∗
i + δ
2 ,
q∗
j := q∗
j + δ
2 ,
q∗
c := 0, c (cid:54)∈ Yij

c + (1 − η) δ

2 , c ∈ Yij \ {i, j}
can only decrease the objective of (54) because the summands corresponding to the unselected b − 2
blocks are set to zero. All the constraints stay feasible and the values corresponding to the blocks
b(i) and b(j) do not change. The last operation is required, because the values Qb(i), Qb(j) change
when we change qi and qj. Adding (1 − η) δ
2 to some scores compensates this and cannot violate the
constraints because f ∗

2 ≥ (1 − η) δ
2 .
Now we will show that it is possible to move all the mass to the two selected labels i and j. We
cannot simply move the mass within one block, but need to create some overﬂow and move it to
another block in a speciﬁc way. Consider δ := q∗
a, which is some non-zero mass on a non-selected
label of the block b(i). Then, the operation

j goes up by δ

f ∗
i := f ∗
j := f ∗
f ∗
f ∗
a := f ∗
f ∗
c := f ∗
c := f ∗
f ∗

i + δ η
2 ,
j + δ η
2 ,
a + δ η
2 (η − 3),
c − δ η
2 (1 − η), c ∈ Yi \ {i, a}
c + δ η
2 (1 − η), c ∈ Yj \ {j}

2 ),

q∗
i := q∗
j := q∗
q∗
a := q∗
q∗

i + δ(1 − η
j + δ η
2 ,
a − δ = 0,

does no change the objective value of (54) because the quantities fc + 1 − ηqc − (1 − η)Qb(c),
c ∈ Yij, stay constant and all the constraints of (54) stay feasible. We repeat this operation for all
a ∈ Yb(i) \ {i} and, thus, move all the probability mass within the block b(i) to the label i. In the
block b(j), an analogous operation can move all the mass to the label j.
It remains to show that q∗

j = ε. Indeed, if q∗

i − q∗

i − q∗

j = δ(cid:48) > ε, the operation analogous to (39)
i − δ(cid:48)−ε
2 ,
j + δ(cid:48)−ε
2 ,

q∗
i := q∗
j := q∗
q∗

i − δ(cid:48)−ε
2 ,
j + δ(cid:48)−ε
2 ,
c − (1 − η) δ(cid:48)−ε
c + (1 − η) δ(cid:48)−ε

f ∗
i := f ∗
j := f ∗
f ∗
f ∗
c := f ∗
f ∗
c := f ∗
i − q∗

2 , c ∈ Yb(i) \ {i},
2 , c ∈ Yb(j) \ {j}
i = 0.5 + ε
2 and q∗

can always set q∗
scores of the block b(i) go down and all the scores of the block b(j) go up at most as much as f ∗
the constraints fj ≥ fc cannot get violated.

2 . After this operation, all the
j , so

j = ε, and thus q∗

j = 0.5 − ε

We now proceed with the computation of Hij(ε). First, we note that convexity and symmetries
of (54) implies that all the non-selected scores within each block are equal.13 Denote the scores of the

13If these optimal scores are not equal, by symmetry, one can obtain the same objective and feasibility by
permuting their corresponding values. By taking a uniform convex combination on all permutations, we obtain a
point where all the scores are equal, and by convexity, would yield a lower objective value.

24

non-selected labels of the block b(i) by f (cid:48)
by f (cid:48)
j.
Analogous to all the previous propositions, the truncation argument gives us that all the values f ∗
c
belong to the segment [−1, −0.5 + ε
c , c (cid:54)= j, the following identity
holds:

i , and the scores of the non-selected labels of the block b(j)

2 ]. For all the optimal values f ∗

(cid:40)

f ∗
c =

f ∗
if ηq∗
j ,
c + (1 − η)Q∗
ηq∗

c + (1 − η)Q∗
b(c) − 1,

b(c) − 1 ≥ f ∗
j ,
otherwise.

i wants to equal the maximal possible value −0.5 + ε

Given that f ∗
this value by f .
By, plugging the values of q∗ and f ∗ provided above into the objective of (54), we get

2 , it implies that f ∗

i = f ∗

j . Denote

(cid:16)

1
2k

(f +0.5− ε

2 )2 +(s−1)(f (cid:48)

i +1−(1−η)(0.5+ ε

(f +0.5+ ε

2 )2 +(s−1)(f (cid:48)

j +1−(1−η)(0.5− ε

2 ))2+
2 ))2(cid:17)
2 (1 + ε)(1 − η) − 1, f (cid:48)∗

.

(55)

By minimizing (55) without constraints, we get f ∗ = −0.5, f (cid:48)∗
1
2 (1 − ε)(1 − η) − 1. We now need to compare f (cid:48)∗
and f ∗ ≥ f (cid:48)∗

i and f (cid:48)∗

i = 1

j =
j with f ∗ to satisfy the constraints f ∗ ≥ f (cid:48)∗
i

j . First, we have that
j = 1

f ∗ −f (cid:48)∗

Second, we have

2 (η + ε − ηε) ≥ 0, for 0 ≤ ε ≤ 1 and 0 ≤ η ≤ 1.

f ∗ −f (cid:48)∗

i = 1

2 (η − ε + ηε) ≥ 0, for 0 ≤ ε ≤ η

1−η we have both f (cid:48)

We can now conclude that when ε ≤ η
minimum points leading to Hij(ε) = ε2
4k .
Now, consider the case ε > η
1−η . We have the constraint f ≥ f (cid:48)
i = f . The new unconstrained minimum w.r.t. f equals f ∗ = 1
f (cid:48)
We now show that the inequality f ∗ ≥ f (cid:48)∗
j still holds. We have

1−η and 0 ≤ η ≤ 1.
i and f (cid:48)

j equal to their unconstrained

i violated, so at the minimum we have
s+1 (−1−(s−1)(1− 1
2 (1−η)(1−ε))).

f ∗ − f (cid:48)∗

j = η+εs−ηεs

s+1 ≥ 0, for 0 ≤ ε ≤ 1 and 0 ≤ η ≤ 1.

Substitution of f (cid:48)∗

into (55) gives us

i = f ∗ and f (cid:48)∗
j
(cid:16) ε2s
2(s+1) − η(ε+1)(s−1)

4(s+1)

1
k

(2ε − εη − η)

,

(cid:17)

which equals Hij(ε) for 1 ≥ ε > η

1−η .

Comparing cases 1 and 2, we observe that Hij(ε) from case 2 is never larger than the one of case 1,
thus case 2 provides the overall calibration function Hij(ε).

Proposition 17. If the scores f are constrained to be equal inside the blocks, i.e. belong to the
subspace F01,b = span(L01,b) ⊆ Rk, then the calibration function

HΦquad,L01,b,η,F01,b (ε) =




(ε−

η
2 )2
4b



0,

(

ηb
k +1−η)2
η
2 )2
(1−

,

η
2 ≤ ε ≤ 1,
0 ≤ ε ≤ η
2

shows that the surrogate is consistent up to level η
2 .

When η = 0, we have H(ε) = ε2
which corresponds to the case of inconsistent surrogate (0-1 loss and constrained scores).

4b as in Proposition 14. When η > 0 we have H(ε) = 0 for small ε,

Proof. This proof combines ideas from Proposition 16 and Proposition 14.

Note that contrary to all the previous results, Lemma 9 is not applicable, because, for b < k, we have
that span(L01,b,η) = Rk (cid:54)⊂ F01,b = span(L01,b).

25

We now derive an analog of Lemma 9 for this speciﬁc case. We deﬁne the subspace of scores F01,b =
{F θ | θ ∈ Rb} with a matrix F := F01,b ∈ Rk×b with columns containing the indicator vectors of
the blocks. We have F TF = sIb and thus (F TF )−1 = 1
s Ib. We shortcut the loss matrix L01,b,η to L
and rewrite it as

k − ηIk − (1 − η)F F T.
By redoing the derivation of Lemma 9, we arrive at a different excess surrogate:

L = ηL01 + (1 − η)L01,b = 1k1T

φ(f (θ), q) = 1

2k (sθTθ + 2θTF TLq) + r(q),

δφ(f (θ), q) = 1
= s

θ∗ := argminθ φ(f (θ), q) = − 1
2k (sθTθ + 2θTF TLq + 1
s F TLq(cid:107)2
2k (cid:107)θ + 1
s
(cid:88)

2

s F TLq,
s qTLTF F TLq)

= s
2k

(θv + 1 − (1 − η)Qv − η

s Qv)2,

v=1

where Qv = (cid:80)
of block v.

c∈Yv

qc is the total probability mass on block v and Yv ⊂ Y denotes the set of labels

Analogously to Proposition 16 we can now apply Lemma 10 and obtain Hij(ε).
b
(cid:88)

(θv + 1 − (1−η)Qv − η

s Qv)2,

Hij(ε)=min
θ,q

s
2k

v=1

(56)

s.t. η(qi − qj) + (1 − η)(Qb(i) − Qb(j)) ≥ ε,

η(qi − qc) + (1 − η)(Qb(i) − Qb(c)) ≥ 0, ∀c
θb(j) ≥ θu, ∀u = 1, . . . , b,

k
(cid:88)

c=1

qc = 1,

qc ≥ 0, ∀c.

The main difference to (54) consists in the fact that we now minimize w.r.t. θ instead of f .

Note that because of the way the predictor pred(f (θ)) resolves ties (among the labels with maximal
scores it always picks the label with the smallest index), not all labels can be predicted. Speciﬁcally,
only one label from each block can be picked. This argument allows us to assume that b(i) (cid:54)= b(j) in
the remainder of this proof.
First, let us prove the case for ε ≤ η
2 . We explicitly provide a feasible assignment of variables
where the objective equals zero. We set qi = 1
2(s−1) , c ∈ Yb(j) \ {j}. All the other
labels (including j and the unselected labels of the block b(i)) receive zero probability mass. This
assignment of q implies Qb(i) = Qb(j) = 1
2 and the zero mass on the other blocks. We also set θb(i)
2 + η
1
and θb(j) to (1−η) 1
2 − 1 to ensure zero objective value. Verifying other feasibility constraints
s
we have η(qi − qj) + (1 − η)(Qb(i) − Qb(j)) = η
2 ≥ ε and η(qi − qc) + (1 − η)(Qb(i) − Qb(c)) =
η( 1

2(s−1) ) ≥ 0, c ∈ Yb(j) \ {j}. Other constraints are trivially satisﬁed.

2 and qc = 1

2 − 1

2−η ; q∗

b(i) = 1+ε−η

c = 0, c ∈ Yb(i) \ {i} (other labels in the block b(i)); q∗

2 . As usual, we claim the following values of the variables f
v = −1, v (cid:54)∈ {b(i), b(j)}; and q∗
i =
1−ε
j = 0, q∗
(2−η)(s−1) ,

Now, consider the case of ε > η
and q result in an optimal point. We have q∗
Q∗
c ∈ Yb(j) \ {j} (other labels in the block b(j)).
First, we will show that we can consider only conﬁgurations with all the probability mass on the
two selected blocks b(i) and b(j). Given some optimal variables f ∗ and q∗, the operation (with
δ = (cid:80)

c = 0, c (cid:54)∈ Yij; θ∗

c =

c∈Y\Yij

i := q∗
q∗

i + δ
2 ,

j := q∗
q∗

j + δ
2 ,

q∗
c )
q∗
c := 0, c ∈ Y \ Yij,
θ∗
v := −1, v /∈ {b(i), b(j)},
2 (1 − η + η
θ∗
b(i) := θ∗
s ),
2 (1 − η + η
b(j) := θ∗
θ∗
s )

b(i) + δ
b(j) + δ

26

can only decrease the objective of (56) because the summands corresponding to the unselected b − 2
blocks are set to zero. All the constraints stay feasible and the values corresponding to the blocks b(i)
and b(j) do not change.

Now, we move the mass within the two selected blocks. To start with, moving the mass within one
block does not change the objective, because it depends only on Qb(c) and not on q directly. In the
block b(i), it is safe to increase qi and decrease the mass on the other labels, because qi enters the
constraints with the positive sign and while the others enter with the negative sign. So we let qc = 0
for c ∈ Yb(i)/{i} and Qb(i) = qi. We also have Qb(j) = 1 − qi as the mass on all other blocks is
zero.

Moving mass within the block b(j) is more complicated, as moving mass to some label c of this
block might violate the constraints of (56) on qi. We start by considering the ﬁrst constraint in (56),
using Qb(j) = 1 − qi, we get:

By using qj ≥ 0 and ε ≥ η

qi ≥ ε + ηqj + (1 − η)(1 − qi).
2 , the inequality (57) implies that qi ≥ 1

2 and thus that

Now the second constraint of (56) that we want to satisfy is:

qc ≤ Qb(j) ≤ 1
2

∀c ∈ Yb(j) .

qi ≥ ηqc + (1 − η)Qb(j) ∀c ∈ Yb(j) .

(57)

(58)

(59)

Using (58), we have that the RHS of (59) is ≤ 1/2, and so since qi ≥ 1/2, we have that (59) is
satisﬁed for any valid mass distribution on block b(j) (i.e. such that Qb(j) ≤ 1/2). Using qj = 0
gives the most possibilities for the value of qi in the constraint (57). Moreover, the constraint (57) is
more stringent than the constraint (59), i.e. if it is satisﬁed, the second one is also satisﬁed; so we
focus only on the ﬁrst constraint.

As in the proof of all other propositions, we can make the constraint (57) an equality for the optimum
by generalizing the transformation of (39) which makes the constraint tight without changing the
objective and maintaining feasibility. So (57) as an equality with qj = 0 yields the value

i = 1+ε−η
q∗
2−η .

2−η

and Qb(j) = 1 − q∗

So to summarize at this point, we have q∗
c = 0, Yb(i) (cid:54)∈ {b(i), b(j)}.
i = 1+ε−η
q∗
i . The precise distribution of mass for c ∈ Yb(j)/{j} does not
matter (any distribution is feasible and does not inﬂuence the objective, only the total mass matters),
1−ε
but for concreteness, we can choose them to all have the same mass yielding q∗
(2−η)(s−1) ,
c ∈ Yb(j) \ {j}.

c = 0, c ∈ Yb(i) \ {i}; q∗

j = 0; q∗

c =

s Q∗

b(j) + η

b(j) − 1, (1 − η)Q∗

We now ﬁnish the computation of Hij(ε). First, we note that, due to the truncation argument
similar to the one mentioned in the paragraph after (39), we have both θ∗
j in the segment
[(1 − η)Q∗
j = θ∗
j ≥ θ∗
i =: θ
at the optimum.
Substituting the values Q∗
b(j) provided above into the objective of (56) and performing
unconstrained minimization w.r.t. θ (we use the help of MATLAB symbolic toolbox to set the
derivative to zero) we get

i and θ∗
i , we have θ∗

b(i) − 1] and since θ∗

b(i) and Q∗

b(i) + η

s Q∗

θ∗ = − s−η+ηs

2s

Hij(ε) =

s(ε−

η
2 )2(
4k(1−

η
s +1−η)2
η
2 )2

,

and, consequently,

which ﬁnishes the proof.

F Constants in the SGD rate

To formalize the learning difﬁculty by bounding the required number of iterations to get a good value
of the risk (Theorem 6), we need to bound the constants D and M . In this section, we provide a

27

way to bound these constants for the quadratic surrogate Φquad (12) under a simplifying assumption
slightly stronger than the well-speciﬁed model Assumption 4.

Consider the family of score functions FF,H deﬁned via an explicit feature map ψ(x) ∈ H, i.e.,
fW (x) = F W ψ(x), where a matrix F ∈ Rk×r deﬁnes the structure and an operator (which we think
of as a matrix with one dimension being inﬁnite) W : H → Rr contains the learnable parameters.
Then the surrogate risk can be written as

RΦ(fW ) = IE(x,y)∼D

1

2k (cid:107)F W ψ(x) + L(:, y)(cid:107)2

Rk

and its stochastic w.r.t. (x, y) gradient as

gx,y(W ) = 1

k F T(F W ψ(x) + L(:, y))ψ(x)T

(60)

where L(:, y) denotes the column of the loss matrix corresponding to the correct label y. Note that
computing the stochastic gradient requires performing products F TF and F TL(:, y) for which direct
computation is intractable when k is exponential, but which can be done in closed form for the
structured losses we consider (the Hamming and block 0-1 loss). More generally, these operations
require suitable inference algorithms.

To derive the constants, we use a simplifying assumption stronger than Assumption 4 in the case of
quadratic surrogate: we assume that the conditional qc(x), seen as a function of x, belongs to the
RKHS H, which by the reproducing property implies that for each c = 1, . . . , k, there exists vc ∈ H
such that qc(x) = (cid:104)vc, ψ(x)(cid:105)H for all x ∈ X . Concatenating all vc, we get an operator V : H → Rk.
To derive the bound, we also assume that (cid:80)k
c=1 (cid:107)vc(cid:107)H ≤ Qmax and (cid:107)ψ(x)(cid:107)H ≤ R for all x ∈ X . In
the following, we use the notation qx to denote the vector in Rk with components qc(x), c = 1, . . . , k,
for a ﬁxed x, and thus qx = V ψ(x).

Under these assumptions, we can write the theoretical minimum of the surrogate risk. The gradient
of the surrogate risk gives

k∇W RΦ(fW ) = F TF W IEx∼DX (ψ(x)ψ(x)T) + F TLIEx∼DX (qxψ(x)T)

= F TF W IEx∼DX (ψ(x)ψ(x)T) + F TLV IEx∼DX (ψ(x)ψ(x)T)
= (cid:0)F TF W + F TLV (cid:1) IEx∼DX (ψ(x)ψ(x)T).
Setting the content of the parenthesis to zero gives that W ∗ = −(F TF )†F TLV is a solution to the
stationary condition equation ∇W RΦ(fW ) = 0.
We can now bound the Hilbert-Schmidt norm of this choice of optimal parameters W ∗ as
(cid:107)W ∗(cid:107)HS = (cid:107)(F TF )†F TLV (cid:107)HS

√

≤ (cid:107)(F TF )†F T(cid:107)HS(cid:107)LV (cid:107)HS
r(cid:107)(F TF )†F T(cid:107)2(cid:107)LV (cid:107)HS
≤
rσ−1
rσ−1

min (F )(cid:107)LV (cid:107)HS
√
min (F )

kLmaxQmax =: D

√

√

=

≤

//submultiplicativity of (cid:107) · (cid:107)HS
//connection of (cid:107) · (cid:107)HS and (cid:107) · (cid:107)2 via r = rank(F )
//rotation invariance of (cid:107) · (cid:107)2

//the deﬁnition of (cid:107) · (cid:107)HS and triangular inequality

where (cid:107) · (cid:107)HS and (cid:107) · (cid:107)2 denote the Hilbert-Schmidt and spectral norms, respectively, and σ−1
min (F )
stands for the smallest singular value of the matrix F . The last inequality follows from the deﬁnition of
the Hilbert-Schmidt norm (cid:107)LV (cid:107)2
H and from the triangular inequality
c=1 L(i, c)vc(cid:107)H ≤ (cid:80)k
(cid:107) (cid:80)k
Analogously, we now bound the Hilbert-Schmidt norm of the stochastic gradient gx,y(W ).

c=1 |L(i, c)|(cid:107)vc(cid:107)H ≤ LmaxQmax thus giving (cid:107)LV (cid:107)HS ≤

c=1 L(i, c)vc(cid:107)2

HS = (cid:80)k

i=1 (cid:107) (cid:80)k

kLmaxQmax.

√

(cid:107)gx,y(W )(cid:107)HS ≤ 1
≤ 1
≤ 1
≤ 1

k (cid:107)F TF W ψ(x) + F TL(:, y))(cid:107)2(cid:107)ψ(x)(cid:107)H
k ((cid:107)F TF W ψ(x)(cid:107)2 + (cid:107)F TL(:, y))(cid:107)2)(cid:107)ψ(x)(cid:107)H
k ((cid:107)F TF (cid:107)2(cid:107)W (cid:107)HS(cid:107)ψ(x)(cid:107)H + (cid:107)F (cid:107)2(cid:107)L(:, y))(cid:107)2)(cid:107)ψ(x)(cid:107)H
√
k σ2

max(F )DR2 + 1

kLmaxR =: M

k σmax(F )

where R is an upper bound on (cid:107)ψ(x)(cid:107)H and σmax(F ) is a maximal singular value of F . Here the ﬁrst
inequality follows from the fact that the rank of gx,y(W ) equals 1 and from submultiplicativity of

28

DM = κ2(F )R2rL2

the spectral norm. We also use the inequality (cid:107)W ψ(x)(cid:107)2 ≤ (cid:107)W (cid:107)HS(cid:107)ψ(x)(cid:107)H, which follows from
the properties of the Hilbert-Schmidt norm.
The bound of Theorem 5 contains the quantity DM and the step size of ASGD depends on D
M , so, to
be practical, both quantities cannot be exponential (for numerical stability; but the important quantity
is the number of iterations from Theorem 6). We have
maxQ2
max(F )
k R2 + σmax(F )σmin(F )

D = σ2
where κ(F ) = σmax
is the condition number of F . Note that the quantity DM is invariant to the
σmin
scaling of the matrix F . The quantity D
M scales proportionally to the square of the scale of F and
thus rescaling F can always bring it to O(1). For the rest of the analysis, we consider R and Qmax to
be well-behaved constants and thus focus on the dependence of the quantity DM on F and L.

maxQmax = L2

ξ(z) = z2 + z,

max + κ(F )R

maxξ(κ(F )

rRQmax),

R
Qmax

rL2

√

√

M

√

k

r

F.1 Constants for speciﬁc losses

We now estimate the product DM from (18) for the 0-1, block 0-1 and Hamming losses. For the
deﬁnition of the losses and the corresponding matrices F , we refer to Section D.3.

0-1 loss. For the 0-1 loss L01 and F = Ik, we have Lmax = 1, r = k, σmin = σmax = 1, thus
DM = O(k) is very large leading to very slow convergence of ASGD.

√

s, thus DM = O(b).

Block 0-1 loss. For the block 0-1 loss L01,b and matrix F01,b, we have Lmax = 1, r = b, σmin =
σmax =
Hamming loss. For the Hamming loss, we have Lmax = 1, r = log2 k + 1, κ(FHam,T ) ≤ log2 k + 2
(see the derivation in Section G). Finally, we have DM = O(log3

2 k).

G Properties of the basis of the Hamming loss

As deﬁned in (35), the matrix LHam,T ∈ Rk×k is the matrix of the Hamming loss between tuples
of T binary variables, and the number of labels equals k = 2T . Also recall that FHam,T :=
[ 1
2 12T , h(1), . . . , h(T )], (h(t)) ˆy := [ˆyt = 1], t = 1, . . . , T . We have FHam,T = span(FHam,T ) =
span(LHam,T ) and rank(LHam,T ) = rank(FHam,T ) = T +1.
We now explicitly compute maxi(cid:54)=j (cid:107)PFHam,T ∆ij(cid:107)2

2. We shortcut FHam,T by F and compute

F TF = 2T −2

(61)








1
1
1
· · ·
1

1
2
1
· · ·
· · ·

· · ·
1
2
· · ·
1








1
· · ·
· · ·
1
2

.

We can compute the inverse matrix explicitly as well:

(F TF )−1 = 22−T

−1
−1
· · ·
−1
The vector F T∆ij equals the difference of the two rows of F , i.e., [0, c1, . . . , cT ]T ∈ RT +1 with
each ct ∈ {−1, 0, +1}. We explicitly compute the square norm (cid:107)PFHam,T ∆ij(cid:107)2
2:











(62)

.



1 + T −1
1
0
· · ·
· · ·



· · · −1
· · ·
0
· · ·
1
0
· · ·
1
0

(cid:107)PFHam,T ∆ij(cid:107)2

2 = ∆T

ijF (F TF )−1F T∆ij = [0, c1, . . . , cT ](F TF )−1[0, c1, . . . , cT ]T = 22−T

where the last equality follows from the identity submatrix of (62) and from the zero in the ﬁrst
position of the vector F T∆ij. The quantity (cid:107)PFHam,T ∆ij(cid:107)2
2 is maximized when none of ct equals
zero, which is achievable, e.g., when the label i corresponds to all zeros and the label j to all ones.
We now have maxi(cid:54)=j (cid:107)PFHam,T ∆ij(cid:107)2

2 = 4T
2T .

T
(cid:88)

t=1

c2
t ,

29

We now compute the smallest and largest eigenvalues of the Gram matrix (61) for FHam,T . Ignoring
the scaling factor 2T −2, we see by Gaussian elimination that the determinant and thus the product
of all eigenvalues equals 1.
If we subtract IT +1 the matrix becomes of rank 2, meaning that
T − 1 eigenvalues equal 1. The trace, i.e., the sum of the eigenvalues of (61), without the scaling
factor 2T −2 equals 2T + 1. Summing up, we have λminλmax = 1 and λmin + λmax = T + 2. We can
T ] and λmax = 1
now compute λmin = 1
T 2 + 4T ) ∈
√
[T + 1, T + 2]. By putting back the multiplicative factor, we get σmin =

2 (T + 2 +
√
λmin ≥
(cid:112)log2 k + 2, and thus the condition number is κ ≤ log2 k + 2.

T 2 + 4T ) ∈ [ 1

2 (T + 2 −

k
log2 k+2

T +2 , 1

λmax ≤

σmax =

√
2

and

√

√

√

√

k
2

30

8
1
0
2
 
n
a
J
 
9
2
 
 
]

G
L
.
s
c
[
 
 
4
v
3
0
4
2
0
.
3
0
7
1
:
v
i
X
r
a

On Structured Prediction Theory with Calibrated
Convex Surrogate Losses

Anton Osokin
INRIA/ENS∗, Paris, France
HSE†, Moscow, Russia

Francis Bach
INRIA/ENS∗, Paris, France

Simon Lacoste-Julien
MILA and DIRO
Université de Montréal, Canada

Abstract

We provide novel theoretical insights on structured prediction in the context of
efﬁcient convex surrogate loss minimization with consistency guarantees. For any
task loss, we construct a convex surrogate that can be optimized via stochastic
gradient descent and we prove tight bounds on the so-called “calibration function”
relating the excess surrogate risk to the actual risk. In contrast to prior related
work, we carefully monitor the effect of the exponential number of classes in the
learning guarantees as well as on the optimization complexity. As an interesting
consequence, we formalize the intuition that some task losses make learning harder
than others, and that the classical 0-1 loss is ill-suited for structured prediction.

1

Introduction

Structured prediction is a subﬁeld of machine learning aiming at making multiple interrelated
predictions simultaneously. The desired outputs (labels) are typically organized in some structured
object such as a sequence, a graph, an image, etc. Tasks of this type appear in many practical domains
such as computer vision [34], natural language processing [42] and bioinformatics [19].

The structured prediction setup has at least two typical properties differentiating it from the classical
binary classiﬁcation problems extensively studied in learning theory:
1. Exponential number of classes: this brings both additional computational and statistical challenges.
By exponential, we mean exponentially large in the size of the natural dimension of output, e.g., the
number of all possible sequences is exponential w.r.t. the sequence length.
2. Cost-sensitive learning: in typical applications, prediction mistakes are not all equally costly.
The prediction error is usually measured with a highly-structured task-speciﬁc loss function, e.g.,
Hamming distance between sequences of multi-label variables or mean average precision for ranking.

Despite many algorithmic advances to tackle structured prediction problems [4, 35], there have been
relatively few papers devoted to its theoretical understanding. Notable recent exceptions that made
signiﬁcant progress include Cortes et al. [13] and London et al. [28] (see references therein) which
proposed data-dependent generalization error bounds in terms of popular empirical convex surrogate
losses such as the structured hinge loss [44, 45, 47]. A question not addressed by these works is
whether their algorithms are consistent: does minimizing their convex bounds with inﬁnite data lead
to the minimization of the task loss as well? Alternatively, the structured probit and ramp losses are
consistent [31, 30], but non-convex and thus it is hard to obtain computational guarantees for them.
In this paper, we aim at getting the property of consistency for surrogate losses that can be efﬁciently
minimized with guarantees, and thus we consider convex surrogate losses.

The consistency of convex surrogates is well understood in the case of binary classiﬁcation [50, 5, 43]
and there is signiﬁcant progress in the case of multi-class 0-1 loss [49, 46] and general multi-

∗DI École normale supérieure, CNRS, PSL Research University
†National Research University Higher School of Economics

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

class loss functions [3, 39, 48]. A large body of work speciﬁcally focuses on the related tasks of
ranking [18, 9, 40] and ordinal regression [37].
Contributions. In this paper, we study consistent convex surrogate losses speciﬁcally in the context
of an exponential number of classes. We argue that even while being consistent, a convex surrogate
might not allow efﬁcient learning. As a concrete example, Ciliberto et al. [10] recently proposed a
consistent approach to structured prediction, but the constant in their generalization error bound can
be exponentially large as we explain in Section 5. There are two possible sources of difﬁculties from
the optimization perspective: to reach adequate accuracy on the task loss, one might need to optimize
a surrogate loss to exponentially small accuracy; or to reach adequate accuracy on the surrogate loss,
one might need an exponential number of algorithm steps because of exponentially large constants
in the convergence rate. We propose a theoretical framework that jointly tackles these two aspects
and allows to judge the feasibility of efﬁcient learning. In particular, we construct a calibration
function [43], i.e., a function setting the relationship between accuracy on the surrogate and task
losses, and normalize it by the means of convergence rate of an optimization algorithm.

Aiming for the simplest possible application of our framework, we propose a family of convex
surrogates that are consistent for any given task loss and can be optimized using stochastic gradient
descent. For a special case of our family (quadratic surrogate), we provide a complete analysis
including general lower and upper bounds on the calibration function for any task loss, with exact
values for the 0-1, block 0-1 and Hamming losses. We observe that to have a tractable learning
algorithm, one needs both a structured loss (not the 0-1 loss) and appropriate constraints on the
predictor, e.g., in the form of linear constraints for the score vector functions. Our framework also
indicates that in some cases it might be beneﬁcial to use non-consistent surrogates. In particular, a
non-consistent surrogate might allow optimization only up to speciﬁc accuracy, but exponentially
faster than a consistent one.

We introduce the structured prediction setting suitable for studying consistency in Sections 2 and 3.
We analyze the calibration function for the quadratic surrogate loss in Section 4. We review the
related works in Section 5 and conclude in Section 6.

2 Structured prediction setup

In structured prediction, the goal is to predict a structured output y ∈ Y (such as a sequence, a graph,
an image) given an input x ∈ X . The quality of prediction is measured by a task-dependent loss
function L( ˆy, y | x) ≥ 0 specifying the cost for predicting ˆy when the correct output is y. In this
paper, we consider the case when the number of possible predictions and the number of possible
labels are both ﬁnite. For simplicity,1 we also assume that the sets of possible predictions and correct
outputs always coincide and do not depend on x. We refer to this set as the set of labels Y, denote its
cardinality by k, and map its elements to 1, . . . , k. In this setting, assuming that the loss function
depends only on ˆy and y, but not on x directly, the loss is deﬁned by a loss matrix L ∈ Rk×k. We
assume that all the elements of the matrix L are non-negative and will use Lmax to denote the maximal
element. Compared to multi-class classiﬁcation, k is typically exponentially large in the size of the
natural dimension of y, e.g., contains all possible sequences of symbols from a ﬁnite alphabet.

Following standard practices in structured prediction [12, 44], we deﬁne the prediction model by
a score function f : X → Rk specifying a score fy(x) for each possible output y ∈ Y. The ﬁnal
prediction is done by selecting a label with the maximal value of the score

pred(f(x)) := argmax

f ˆy(x),

ˆy∈Y

(1)

with some ﬁxed strategy to resolve ties. To simplify the analysis, we assume that among the labels
with maximal scores, the predictor always picks the one with the smallest index.

The goal of prediction-based machine learning consists in ﬁnding a predictor that works well on
the unseen test set, i.e., data points coming from the same distribution D as the one generating the
training data. One way to formalize this is to minimize the generalization error, often referred to as
the actual (or population) risk based on the loss L,

RL(f) := IE(x,y)∼D L(cid:0)pred(f(x)), y(cid:1).
(2)
Minimizing the actual risk (2) is usually hard. The standard approach is to minimize a surrogate risk,
which is a different objective easier to optimize, e.g., convex. We deﬁne a surrogate loss as a function

1Our analysis is generalizable to rectangular losses, e.g., ranking losses studied by Ramaswamy et al. [40].

2

Φ : Rk × Y → R depending on a score vector f = f(x) ∈ Rk and a target label y ∈ Y as input
arguments. We denote the y-th component of f with fy. The surrogate risk (the Φ-risk) is deﬁned as
RΦ(f) := IE(x,y)∼D Φ(f(x), y),
(3)
where the expectation is taken w.r.t. the data-generating distribution D. To make the minimization
of (3) well-deﬁned, we always assume that the surrogate loss Φ is bounded from below and continuous.

Examples of common surrogate losses include the structured hinge-loss [44, 47] ΦSSVM(f , y) :=
(cid:0)f ˆy + L( ˆy, y)(cid:1) − fy, the log loss (maximum likelihood learning) used, e.g., in conditional
max ˆy∈Y
random ﬁelds [25], Φlog(f , y) := log((cid:80)
In terms of task losses, we consider the unstructured 0-1 loss L01( ˆy, y) := [ ˆy (cid:54)= y],2 and the
two following structured losses: block 0-1 loss with b equal blocks of labels L01,b( ˆy, y) :=
[ ˆy and y are not in the same block]; and (normalized) Hamming loss between tuples of T binary
variables yt: LHam,T ( ˆy, y) := 1
t=1[ˆyt (cid:54)= yt]. To illustrate some aspects of our analysis, we also
T
look at the mixed loss L01,b,η: a convex combination of the 0-1 and block 0-1 losses, deﬁned as
L01,b,η := ηL01 + (1 − η)L01,b for some η ∈ [0, 1].

ˆy∈Y exp f ˆy) − fy, and their hybrids [38, 21, 22, 41].

(cid:80)T

3 Consistency for structured prediction

3.1 Calibration function

We now formalize the connection between the actual risk RL and the surrogate Φ-risk RΦ via the
so-called calibration function, see Deﬁnition 1 below [5, 49, 43, 18, 3]. As it is standard for this
kind of analysis, the setup is non-parametric, i.e. it does not take into account the dependency of
scores on input variables x. For now, we assume that a family of score functions FF consists of all
vector-valued Borel measurable functions f : X → F where F ⊆ Rk is a subspace of allowed score
vectors, which will play an important role in our analysis. This setting is equivalent to a pointwise
analysis, i.e, looking at the different input x independently. We bring the dependency on the input
back into the analysis in Section 3.3 where we assume a speciﬁc family of score functions.

Let DX represent the marginal distribution for D on x and IP(· | x) denote its conditional given x.
We can now rewrite the risk RL and Φ-risk RΦ as

RL(f) = IEx∼DX (cid:96)(f(x), IP(· | x)), RΦ(f) = IEx∼DX φ(f(x), IP(· | x)),
where the conditional risk (cid:96) and the conditional Φ-risk φ depend on a vector of scores f and a
conditional distribution on the set of output labels q as

(cid:88)k

(cid:88)k

(cid:96)(f , q) :=

qcL(pred(f ), c), φ(f , q) :=
The calibration function HΦ,L,F between the surrogate loss Φ and the task loss L relates the excess
surrogate risk with the actual excess risk via the excess risk bound:

qcΦ(f , c).

c=1

c=1

HΦ,L,F (δ(cid:96)(f , q)) ≤ δφ(f , q), ∀f ∈ F, ∀q ∈ ∆k,
(4)
where δφ(f , q) = φ(f , q) − inf ˆf ∈F φ( ˆf , q), δ(cid:96)(f , q) = (cid:96)(f , q) − inf ˆf ∈F (cid:96)( ˆf , q) are the excess
risks and ∆k denotes the probability simplex on k elements.

In other words, to ﬁnd a vector f that yields an excess risk smaller than ε, we need to optimize the
Φ-risk up to HΦ,L,F (ε) accuracy (in the worst case). We make this statement precise in Theorem 2
below, and now proceed to the formal deﬁnition of the calibration function.
Deﬁnition 1 (Calibration function). For a task loss L, a surrogate loss Φ, a set of feasible scores F,
the calibration function HΦ,L,F (ε) (deﬁned for ε ≥ 0) equals the inﬁmum excess of the conditional
surrogate risk when the excess of the conditional actual risk is at least ε:

HΦ,L,F (ε) := inf

δφ(f , q)

f ∈F , q∈∆k
s.t.

δ(cid:96)(f , q) ≥ ε.

(5)

(6)

We set HΦ,L,F (ε) to +∞ when the feasible set is empty.

By construction, HΦ,L,F is non-decreasing on [0, +∞), HΦ,L,F (ε) ≥ 0, the inequality (4) holds,
and HΦ,L,F (0) = 0. Note that HΦ,L,F can be non-convex and even non-continuous (see examples
in Figure 1). Also, note that large values of HΦ,L,F (ε) are better.

2Here we use the Iverson bracket notation, i.e., [A] := 1 if a logical expression A is true, and zero otherwise.

3

(a): Hamming loss LHam,T

(b): Mixed loss L01,b,0.4

Figure 1: Calibration functions for the quadratic surrogate Φquad (12) deﬁned in Section 4 and two
different task losses. (a) – the calibration functions for the Hamming loss LHam,T when used without
constraints on the scores, F = Rk (in red), and with the tight constraints implying consistency,
F = span(LHam,T ) (in blue). The red curve can grow exponentially slower than the blue one. (b) –
the calibration functions for the mixed loss L01,b,η with η = 0.4 (see Section 2 for the deﬁnition) when
used without constraints on the scores (red) and with tight constraints for the block 0-1 loss (blue).
The blue curve represents level-0.2 consistency. The calibration function equals zero for ε ≤ η/2,
but grows exponentially faster than the red curve representing a consistent approach and thus could
be better for small η. More details on the calibration functions in this ﬁgure are given in Section 4.

3.2 Notion of consistency

(7)

(8)

We use the calibration function HΦ,L,F to set a connection between optimizing the surrogate and
task losses by Theorem 2, which is similar to Theorem 3 of Zhang [49].
Theorem 2 (Calibration connection). Let HΦ,L,F be the calibration function between the surrogate
loss Φ and the task loss L with feasible set of scores F ⊆ Rk. Let ˇHΦ,L,F be a convex non-decreasing
lower bound of the calibration function. Assume that Φ is continuous and bounded from below. Then,
for any ε > 0 with ﬁnite ˇHΦ,L,F (ε) and any f ∈ FF , we have

where R∗

Φ,F := inf f∈FF RΦ(f) and R∗

RΦ(f) < R∗

Φ,F + ˇHΦ,L,F (ε) ⇒ RL(f) < R∗
L,F := inf f∈FF RL(f).

L,F + ε,

Proof. We take the expectation of (4) w.r.t. x, where the second argument of (cid:96) is set to the conditional
distribution IP(· | x). Then, we apply Jensen’s inequality (since ˇHΦ,L,F is convex) to get

ˇHΦ,L,F (RL(f) − R∗

L,F ) ≤ RΦ(f) − R∗

Φ,F < ˇHΦ,L,F (ε),

which implies (7) by monotonicity of ˇHΦ,L,F .

A suitable convex non-decreasing lower bound ˇHΦ,L,F (ε) required by Theorem 2 always exists, e.g.,
the zero constant. However, in this case Theorem 2 is not informative, because the l.h.s. of (7) is
never true. Zhang [49, Proposition 25] claims that ˇHΦ,L,F deﬁned as the lower convex envelope of
the calibration function HΦ,L,F satisﬁes ˇHΦ,L,F (ε) > 0, ∀ε > 0, if HΦ,L,F (ε) > 0, ∀ε > 0, and,
e.g., the set of labels is ﬁnite. This statement implies that an informative ˇHΦ,L,F always exists and
allows to characterize consistency through properties of the calibration function HΦ,L,F .

We now deﬁne a notion of level-η consistency, which is more general than consistency.
Deﬁnition 3 (level-η consistency). A surrogate loss Φ is consistent up to level η ≥ 0 w.r.t. a task
loss L and a set of scores F if and only if the calibration function satisﬁes HΦ,L,F (ε) > 0 for all
ε > η and there exists ˆε > η such that HΦ,L,F (ˆε) is ﬁnite.

Looking solely at (standard level-0) consistency vs. inconsistency might be too coarse to capture
practical properties related to optimization accuracy (see, e.g., [29]). For example, if HΦ,L,F (ε) = 0
only for very small values of ε, then the method can still optimize the actual risk up to a certain
level which might be good enough in practice, especially if it means that it can be optimized faster.
Examples of calibration functions for consistent and inconsistent surrogate losses are shown in
Figure 1.
Other notions of consistency. Deﬁnition 3 with η = 0 and F = Rk results in the standard setting
often appearing in the literature. In particular, in this case Theorem 2 implies Fisher consistency as

4

formulated, e.g., by Pedregosa et al. [37] for general losses and Lin [27] for binary classiﬁcation.
This setting is also closely related to many deﬁnitions of consistency used in the literature. For
example, for a bounded from below and continuous surrogate, it is equivalent to inﬁnite-sample
consistency [49], classiﬁcation calibration [46], edge-consistency [18], (L, Rk)-calibration [39],
prediction calibration [48]. See [49, Appendix A] for the detailed discussion.
Role of F. Let the approximation error for the restricted set of scores F be deﬁned as R∗
L :=
inf f∈FF RL(f) − inf f RL(f). For any conditional distribution q, the score vector f := −Lq will
yield an optimal prediction. Thus the condition span(L) ⊆ F is sufﬁcient for F to have zero
approximation error for any distribution D, and for our 0-consistency condition to imply the standard
Fisher consistency with respect to L. In the following, we will see that a restricted F can both play a
role for computational efﬁciency as well as statistical efﬁciency (thus losses with smaller span(L)
might be easier to work with).

L,F −R∗

3.3 Connection to optimization accuracy and statistical efﬁciency

The scale of a calibration function is not intrinsically well-deﬁned: we could multiply the surrogate
function by a scalar and it would multiply the calibration function by the same scalar, without
changing the optimization problem. Intuitively, we would like the surrogate loss to be of order 1. If
with this scale the calibration function is exponentially small (has a 1/k factor), then we have strong
evidence that the stochastic optimization will be difﬁcult (and thus learning will be slow).

To formalize this intuition, we add to the picture the complexity of optimizing the surrogate loss with
a stochastic approximation algorithm. By using a scale-invariant convergence rate, we provide a
natural normalization of the calibration function. The following two observations are central to the
theoretical insights provided in our work:

1. Scale. For a properly scaled surrogate loss, the scale of the calibration function is a good indication
of whether a stochastic approximation algorithm will take a large number of iterations (in the worst
case) to obtain guarantees of small excess of the actual risk (and vice-versa, a large coefﬁcient
indicates a small number of iterations). The actual veriﬁcation requires computing the normalization
quantities given in Theorem 6 below.
2. Statistics. The bound on the number of iterations directly relates to the number of training
examples that would be needed to learn, if we see each iteration of the stochastic approximation
algorithm as using one training example to optimize the expected surrogate.

To analyze the statistical convergence of surrogate risk optimization, we have to specify the set of
score functions that we work with. We assume that the structure on input x ∈ X is deﬁned by a
positive deﬁnite kernel K : X × X → R. We denote the corresponding reproducing kernel Hilbert
space (RKHS) by H and its explicit feature map by ψ(x) ∈ H. By the reproducing property, we
have (cid:104)f, ψ(x)(cid:105)H = f (x) for all x ∈ X , f ∈ H, where (cid:104)·, ·(cid:105)H is the inner product in the RKHS. We
deﬁne the subspace of allowed scores F ⊆ Rk via the span of the columns of a matrix F ∈ Rk×r.
The matrix F explicitly deﬁnes the structure of the score function. With this notation, we will assume
that the score function is of the form f(x) = F W ψ(x), where W : H → Rr is a linear operator
to be learned (a matrix if H is of ﬁnite dimension) that represents a collection of r elements in H,
transforming ψ(x) to a vector in Rr by applying the RKHS inner product r times.3 Note that for
structured losses, we usually have r (cid:28) k. The set of all score functions is thus obtained by varying W
in this deﬁnition and is denoted by FF,H. As a concrete example of a score family FF,H for structured
prediction, consider the standard sequence model with unary and pairwise potentials. In this case, the
dimension r equals T s + (T − 1)s2, where T is the sequence length and s is the number of labels
of each variable. The columns of the matrix F consist of 2T − 1 groups (one for each unary and
pairwise potential). Each row of F has exactly one entry equal to one in each column group (with
zeros elsewhere).
In this setting, we use the online projected averaged stochastic subgradient descent ASGD4 (stochastic
w.r.t. data (x(n), y(n)) ∼ D) to minimize the surrogate risk directly [6]. The n-th update consists in
(9)

(cid:2)W (n−1) − γ(n)F T∇Φψ(x(n))T(cid:3),

W (n) := PD

3Note that if rank(F ) = r, our setup is equivalent to assuming a joint kernel [47] in the product form:

Kjoint((x, c), (x(cid:48), c(cid:48))) := K(x, x(cid:48))F (c, :)F (c(cid:48), :)T, where F (c, :) is the row c for matrix F .

4See, e.g., [36] for the formal setup of kernel ASGD.

5

where F T∇Φψ(x(n))T : H → Rr is the stochastic functional gradient, γ(n) is the step size
and PD is the projection on the ball of radius D w.r.t. the Hilbert–Schmidt norm5. The vector
∇Φ ∈ Rk is a regular gradient of the sampled surrogate Φ(f(x(n)), y(n)) w.r.t. the scores, ∇Φ =
∇f Φ(f , y(n))|f =f(x(n)). We wrote the above update using an explicit feature map ψ for notational
simplicity, but kernel ASGD can also be implemented without it by using the kernel trick. The
convergence properties of ASGD in RKHS are analogous to the ﬁnite-dimensional ASGD because
they rely on dimension-free quantities. To use a simple convergence analysis, we follow Ciliberto
et al. [10] and make the following simplifying assumption:
Assumption 4 (Well-speciﬁed optimization w.r.t. the function class FF,H). The distribution D is
Φ,F := inf f∈FF RΦ(f) has some global minimum f∗ that also belongs to FF,H.
such that R∗

Assumption 4 simply means that each row of W ∗ deﬁning f∗ belongs to the RKHS H implying
a ﬁnite norm (cid:107)W ∗(cid:107)HS. Assumption 4 can be relaxed if the kernel K is universal, but then the
convergence analysis becomes much more complicated [36].
Theorem 5 (Convergence rate). Under Assumption 4 and assuming that (i) the functions Φ(f , y)
are bounded from below and convex w.r.t. f ∈ Rk for all y ∈ Y; (ii) the expected square of the norm
HS ≤ M 2 and (iii) (cid:107)W ∗(cid:107)HS ≤ D,
of the stochastic gradient is bounded, IE(x,y)∼D(cid:107)F T∇Φψ(x)T(cid:107)2
then running the ASGD algorithm (9) with the constant step-size γ := 2D
for N steps admits the
√
N
following expected suboptimality for the averaged iterate ¯f(N ):

M

IE[RΦ(¯f(N ))] − R∗

Φ,F ≤ 2DM√
N

where ¯f(N ) := 1
N

(cid:88)N

n=1

F W (n)ψ(x(n))T.

(10)

Theorem 5 is a straight-forward extension of classical results [33, 36].

By combining the convergence rate of Theorem 5 with Theorem 2 that connects the surrogate and
actual risks, we get Theorem 6 which explicitly gives the number of iterations required to achieve
ε accuracy on the expected population risk (see App. A for the proof). Note that since ASGD is
applied in an online fashion, Theorem 6 also serves as the sample complexity bound, i.e., says how
many samples are needed to achieve ε target accuracy (compared to the best prediction rule if F has
zero approximation error).
Theorem 6 (Learning complexity). Under the assumptions of Theorem 5, for any ε > 0, the random
(w.r.t. the observed training set) output ¯f(N ) ∈ FF,H of the ASGD algorithm after

N > N ∗ := 4D2M 2
Φ,L,F (ε)

ˇH 2

(11)

iterations has the expected excess risk bounded with ε, i.e., IE[RL(¯f(N ))] < R∗

L,F + ε.

4 Calibration function analysis for quadratic surrogate

A major challenge to applying Theorem 6 is the computation of the calibration function HΦ,L,F . In
App. C, we present a generalization to arbitrary multi-class losses of a surrogate loss class from Zhang
[49, Section 4.4.2] that is consistent for any task loss L. Here, we consider the simplest example of
this family, called the quadratic surrogate Φquad, which has the advantage that we can bound or even
compute exactly its calibration function. We deﬁne the quadratic surrogate as

Φquad(f , y) := 1

2k (cid:107)f + L(:, y)(cid:107)2

2 = 1
2k

(f 2

c + 2fcL(c, y) + L(c, y)2).

(12)

One simple sufﬁcient condition for the surrogate (12) to be consistent and also to have zero approxi-
mation error is that F fully contains span(L). To make the dependence on the score subspace explicit,
we parameterize it with a matrix F ∈ Rk×r with the number of columns r typically being much
smaller than the number of labels k. With this notation, we have F = span(F ) = {F θ | θ ∈ Rr},
and the dimensionality of F equals the rank of F , which is at most r.6

5The Hilbert–Schmidt norm of a linear operator A is deﬁned as (cid:107)A(cid:107)HS =

trA‡A where A‡ is the adjoint
operator. In the case of ﬁnite dimension, the Hilbert–Schmidt norm coincides with the Frobenius matrix norm.
6Evaluating Φquad requires computing F TF and F TL(:, y) for which direct computation is intractable when
k is exponential, but which can be done in closed form for the structured losses we consider (the Hamming and
block 0-1 loss). More generally, these operations require suitable inference algorithms. See also App. F.

√

k
(cid:88)

c=1

6

For the quadratic surrogate (12), the excess of the expected surrogate takes a simple form:

δφquad(F θ, q) = 1

2k (cid:107)F θ + Lq(cid:107)2
2.

(13)

Equation (13) holds under the assumption that the subspace F contains the column space of the
loss matrix span(L), which also means that the set F contains the optimal prediction for any q (see
Lemma 9 in App. B for the proof). Importantly, the function δφquad(F θ, q) is jointly convex in the
conditional probability q and parameters θ, which simpliﬁes its analysis.

Lower bound on the calibration function. We now present our main technical result: a lower
bound on the calibration function for the surrogate loss Φquad (12). This lower bound characterizes
the easiness of learning with this surrogate given the scaling intuition mentioned in Section 3.3. The
proof of Theorem 7 is given in App. D.1.
Theorem 7 (Lower bound on HΦquad ). For any task loss L, its quadratic surrogate Φquad, and a score
subspace F containing the column space of L, the calibration function can be lower bounded:

HΦquad,L,F (ε) ≥

ε2
2k maxi(cid:54)=j (cid:107)PF ∆ij (cid:107)2
2

≥ ε2
4k ,

(14)

where PF is the orthogonal projection on the subspace F and ∆ij = ei − ej ∈ Rk with ec being
the c-th basis vector of the standard basis in Rk.

Lower bound for speciﬁc losses. We now discuss the meaning of the bound (14) for some speciﬁc
losses (the detailed derivations are given in App. D.3). For the 0-1, block 0-1 and Hamming losses
(L01, L01,b and LHam,T , respectively) with the smallest possible score subspaces F, the bound (14)
gives ε2
8T , respectively. All these bounds are tight (see App. E). However, if F = Rk
the bound (14) is not tight for the block 0-1 and mixed losses (see also App. E). In particular, the
bound (14) cannot detect level-η consistency for η > 0 (see Def. 3) and does not change when the
loss changes, but the score subspace stays the same.

4b and ε2

4k , ε2

Upper bound on the calibration function. Theorem 8 below gives an upper bound on the calibration
function holding for unconstrained scores, i.e, F = Rk (see the proof in App. D.2). This result shows
that without some appropriate constraints on the scores, efﬁcient learning is not guaranteed (in the
worst case) because of the 1/k scaling of the calibration function.
Theorem 8 (Upper bound on HΦquad ). If a loss matrix L with Lmax > 0 deﬁnes a pseudometric7 on
labels and there are no constraints on the scores, i.e., F = Rk, then the calibration function for the
quadratic surrogate Φquad can be upper bounded: HΦquad,L,Rk (ε) ≤ ε2
2k ,

0 ≤ ε ≤ Lmax.

From our lower bound in Theorem 7 (which guarantees consistency), the natural constraint on
the score is F = span(L), with the dimension of this space giving an indication of the intrinsic
“difﬁculty” of a loss. Computations for the lower bounds in some speciﬁc cases (see App. D.3 for
details) show that the 0-1 loss is “hard” while the block 0-1 loss and the Hamming loss are “easy”.
Note that in all these cases the lower bound (14) is tight, see the discussion below.

Exact calibration functions. Note that the bounds proven in Theorems 7 and 8 imply that, in the
case of no constraints on the scores F = Rk, for the 0-1, block 0-1 and Hamming losses, we have
ε2
4k ≤ HΦquad,L,Rk (ε) ≤ ε2
2k ,
where L is the matrix deﬁning a loss. For completeness, in App. E, we compute the exact calibration
functions for the 0-1 and block 0-1 losses. Note that the calibration function for the 0-1 loss equals the
lower bound, illustrating the worst-case scenario. To get some intuition, an example of a conditional
distribution q that gives the (worst case) value to the calibration function (for several losses) is
qi = 1
In what follows, we provide the calibration functions in the cases with constraints on the scores. For
the block 0-1 loss with b equal blocks and under constraints that the scores within blocks are equal,
the calibration function equals (see Proposition 14 of App. E.2)

2 and qc = 0 for c (cid:54)∈ {i, j}. See the proof of Proposition 12 in App. E.1.

2 , qj = 1

2 − ε

2 + ε

(15)

HΦquad,L01,b,F01,b (ε) = ε2
4b ,

0 ≤ ε ≤ 1.

(16)

7A pseudometric is a function d(a, b) satisfying the following axioms: d(x, y) ≥ 0, d(x, x) = 0 (but

possibly d(x, y) = 0 for some x (cid:54)= y), d(x, y) = d(y, x), d(x, z) ≤ d(x, y) + d(y, z).

7

For the Hamming loss deﬁned over T binary variables and under constraints implying separable
scores, the calibration function equals (see Proposition 15 in App. E.3)

HΦquad,LHam,T ,FHam,T (ε) = ε2

8T , 0 ≤ ε ≤ 1.

(17)

The calibration functions (16) and (17) depend on the quantities representing the actual complexities
of the loss (the number of blocks b and the length of the sequence T ) and can be exponentially larger
than the upper bound for the unconstrained case.

In the case of mixed 0-1 and block 0-1 loss, if the scores f are constrained to be equal inside the
blocks, i.e., belong to the subspace F01,b = span(L01,b) (cid:40) Rk, then the calibration function is equal
to 0 for ε ≤ η
2 , implying inconsistency (and also note that the approximation error can be as big as η
for F01,b). However, for ε > η
2 , the calibration function is of the order 1
2 )2. See Figure 1b for
the illustration of this calibration function and Proposition 17 of App. E.4 for the exact formulation
and the proof. Note that while the calibration function for the constrained case is inconsistent, its
value can be exponentially larger than the one for the unconstrained case for ε big enough and when
the blocks are exponentially large (see Proposition 16 of App. E.4).

b (ε − η

Computation of the SGD constants. Applying the learning complexity Theorem 6 requires to
compute the quantity DM where D bounds the norm of the optimal solution and M bounds the
expected square of the norm of the stochastic gradient. In App. F, we provide a way to bound this
quantity for our quadratic surrogate (12) under the simplifying assumption that each conditional qc(x)
(seen as function of x) belongs to the RKHS H (which implies Assumption 4). In particular, we get

DM = L2

maxξ(κ(F )

rRQmax),

ξ(z) = z2 + z,

√

(18)

where κ(F ) is the condition number of the matrix F , R is an upper bound on the RKHS norm of
object feature maps (cid:107)ψ(x)(cid:107)H. We deﬁne Qmax as an upper bound on (cid:80)k
c=1 (cid:107)qc(cid:107)H (can be seen as
the generalization of the inequality (cid:80)k
c=1 qc ≤ 1 for probabilities). The constants R and Qmax depend
on the data, the constant Lmax depends on the loss, r and κ(F ) depend on the choice of matrix F .

We compute the constant DM for the speciﬁc losses that we considered in App. F.1. For the 0-1, block
0-1 and Hamming losses, we have DM = O(k), DM = O(b) and DM = O(log3
2 k), respectively.
These computations indicate that the quadratic surrogate allows efﬁcient learning for structured block
0-1 and Hamming losses, but that the convergence could be slow in the worst case for the 0-1 loss.

5 Related works

Consistency for multi-class problems. Building on signiﬁcant progress for the case of binary
classiﬁcation, see, e.g. [5], there has been a lot of interest in the multi-class case. Zhang [49] and
Tewari & Bartlett [46] analyze the consistency of many existing surrogates for the 0-1 loss. Gao &
Zhou [20] focus on multi-label classiﬁcation. Narasimhan et al. [32] provide a consistent algorithm
for arbitrary multi-class loss deﬁned by a function of the confusion matrix. Recently, Ramaswamy &
Agarwal [39] introduce the notion of convex calibrated dimension, as the minimal dimensionality of
the score vector that is required for consistency. In particular, they showed that for the Hamming loss
on T binary variables, this dimension is at most T . In our analysis, we use scores of rank (T + 1),
see (35) in App. D.3, yielding a similar result.

The task of ranking has attracted a lot of attention and [18, 8, 9, 40] analyze different families of
surrogate and task losses proving their (in-)consistency. In this line of work, Ramaswamy et al.
[40] propose a quadratic surrogate for an arbitrary low rank loss which is related to our quadratic
surrogate (12). They also prove that several important ranking losses, i.e., precision@q, expected
rank utility, mean average precision and pairwise disagreement, are of low-rank. We conjecture that
our approach is compatible with these losses and leave precise connections as future work.

Structured SVM (SSVM) and friends. SSVM [44, 45, 47] is one of the most used convex surrogates
for tasks with structured outputs, thus, its consistency has been a question of great interest. It is
known that Crammer-Singer multi-class SVM [15], which SSVM is built on, is not consistent for
0-1 loss unless there is a majority class with probability at least 1
2 [49, 31]. However, it is consistent
for the “abstain” and ordinal losses in the case of 3 classes [39]. Structured ramp loss and probit
surrogates are closely related to SSVM and are consistent [31, 16, 30, 23], but not convex.

8

Recently, Do˘gan et al. [17] categorized different versions of multi-class SVM and analyzed them
from Fisher and universal consistency point of views. In particular, they highlight differences between
Fisher and universal consistency and give examples of surrogates that are Fisher consistent, but not
universally consistent and vice versa. They also highlight that the Crammer-Singer SVM is neither
Fisher, not universally consistent even with a careful choice of regularizer.

i=1 (cid:107)g(xi) − ψo(yi)(cid:107)2

Quadratic surrogates for structured prediction. Ciliberto et al. [10] and Brouard et al. [7] consider
minimizing (cid:80)n
H aiming to match the RKHS embedding of inputs g : X → H
to the feature maps of outputs ψo : Y → H. In their frameworks, the task loss is not considered at
the learning stage, but only at the prediction stage. Our quadratic surrogate (12) depends on the loss
directly. The empirical risk deﬁned by both their and our objectives can be minimized analytically with
the help of the kernel trick and, moreover, the resulting predictors are identical. However, performing
such computation in the case of large dataset can be intractable and the generalization properties have
to be taken care of, e.g., by the means of regularization. In the large-scale scenario, it is more natural
to apply stochastic optimization (e.g., kernel ASGD) that directly minimizes the population risk and
has better dependency on the dataset size. When combined with stochastic optimization, the two
approaches lead to different behavior. In our framework, we need to estimate r = rank(L) scalar
functions, but the alternative needs to estimate k functions (if, e.g., ψo(y) = ey ∈ Rk), which results
in signiﬁcant differences for low-rank losses, such as block 0-1 and Hamming.

Calibration functions. Bartlett et al. [5] and Steinwart [43] provide calibration functions for most
existing surrogates for binary classiﬁcation. All these functions differ in term of shape, but are
roughly similar in terms of constants. Pedregosa et al. [37] generalize these results to the case of
ordinal regression. However, their calibration functions have at best a 1/k factor if the surrogate is
normalized w.r.t. the number of classes. The task of ranking has been of signiﬁcant interest. However,
most of the literature [e.g., 11, 14, 24, 1], only focuses on calibration functions (in the form of regret
bounds) for bipartite ranking, which is more akin to cost-sensitive binary classiﬁcation.

Ávila Pires et al. [3] generalize the theoretical framework developed by Steinwart [43] and present
results for the multi-class SVM of Lee et al. [26] (the score vectors are constrained to sum to zero)
that can be built for any task loss of interest. Their surrogate Φ is of the form (cid:80)
c∈Y L(c, y)a(fc)
where (cid:80)
c∈Y fc = 0 and a(f ) is some convex function with all subgradients at zero being positive.
The recent work by Ávila Pires & Szepesvári [2] reﬁnes the results, but speciﬁcally for the case of
0-1 loss. In this line of work, the surrogate is typically not normalized by k, and if normalized the
calibration functions have the constant 1/k appearing.

Finally, Ciliberto et al. [10] provide the calibration function for their quadratic surrogate. Assuming
that the loss can be represented as L( ˆy, y) = (cid:104)V ψo( ˆy), ψo(y)(cid:105)HY , ˆy, y ∈ Y (this assumption can
always be satisﬁed in the case of a ﬁnite number of labels, by taking V as the loss matrix L and
ψo(y) := ey ∈ Rk where ey is the y-th vector of the standard basis in Rk). In their Theorem 2, they
provide an excess risk bound leading to a lower bound on the corresponding calibration function
HΦ,L,Rk (ε) ≥ ε2
where a constant c∆ = (cid:107)V (cid:107)2 maxy∈Y (cid:107)ψo(y)(cid:107) simply equals the spectral norm
c2
∆
of the loss matrix for the ﬁnite-dimensional construction provided above. However, the spectral
norm of the loss matrix is exponentially large even for highly structured losses such as the block 0-1
and Hamming losses, i.e., (cid:107)L01,b(cid:107)2 = k − k
2 . This conclusion puts the objective
of Ciliberto et al. [10] in line with ours when no constraints are put on the scores.

b , (cid:107)LHam,T (cid:107)2 = k

6 Conclusion

In this paper, we studied the consistency of convex surrogate losses speciﬁcally in the context
of structured prediction. We analyzed calibration functions and proposed an optimization-based
normalization aiming to connect consistency with the existence of efﬁcient learning algorithms.
Finally, we instantiated all components of our framework for several losses by computing the
calibration functions and the constants coming from the normalization. By carefully monitoring
exponential constants, we highlighted the difference between tractable and intractable task losses.

These were ﬁrst steps in advancing our theoretical understanding of consistent structured prediction.
Further steps include analyzing more losses such as the low-rank ranking losses studied by Ra-
maswamy et al. [40] and, instead of considering constraints on the scores, one could instead put
constraints on the set of distributions to investigate the effect on the calibration function.

9

We would like to thank Pascal Germain for useful discussions. This work was partly supported
by the ERC grant Activia (no. 307574), the NSERC Discovery Grant RGPIN-2017-06936 and the
MSR-INRIA Joint Center.

Acknowledgements

References

[1] Agarwal, Shivani. Surrogate regret bounds for bipartite ranking via strongly proper losses.

Journal of Machine Learning Research (JMLR), 15(1):1653–1674, 2014.

[2] Ávila Pires, Bernardo and Szepesvári, Csaba. Multiclass classiﬁcation calibration functions.

arXiv, 1609.06385v1, 2016.

[3] Ávila Pires, Bernardo, Ghavamzadeh, Mohammad, and Szepesvári, Csaba. Cost-sensitive

multiclass classiﬁcation risk bounds. In ICML, 2013.

[4] Bakir, Gökhan, Hofmann, Thomas, Schölkopf, Bernhard, Smola, Alexander J., Taskar, Ben,

and Vishwanathan, S.V.N. Predicting Structured Data. MIT press, 2007.

[5] Bartlett, Peter L., Jordan, Michael I., and McAuliffe, Jon D. Convexity, classiﬁcation, and risk

bounds. Journal of the American Statistical Association, 101(473):138–156, 2006.

[6] Bousquet, Olivier and Bottou, Léon. The tradeoffs of large scale learning. In NIPS, 2008.

[7] Brouard, Céline, Szafranski, Marie, and d’Alché-Buc, Florence. Input output kernel regression:
Supervised and semi-supervised structured output prediction with operator-valued kernels.
Journal of Machine Learning Research (JMLR), 17(176):1–48, 2016.

[8] Buffoni, David, Gallinari, Patrick, Usunier, Nicolas, and Calauzènes, Clément. Learning scoring

functions with order-preserving losses and standardized supervision. In ICML, 2011.

[9] Calauzènes, Clément, Usunier, Nicolas, and Gallinari, Patrick. On the (non-)existence of convex,

calibrated surrogate losses for ranking. In NIPS, 2012.

[10] Ciliberto, Carlo, Rudi, Alessandro, and Rosasco, Lorenzo. A consistent regularization approach

for structured prediction. In NIPS, 2016.

[11] Clémençon, Stéphan, Lugosi, Gábor, and Vayatis, Nicolas. Ranking and empirical minimization

of U-statistics. The Annals of Statistics, pp. 844–874, 2008.

[12] Collins, Michael. Discriminative training methods for hidden Markov models: Theory and

experiments with perceptron algorithms. In EMNLP, 2002.

[13] Cortes, Corinna, Kuznetsov, Vitaly, Mohri, Mehryar, and Yang, Scott. Structured prediction

theory based on factor graph complexity. In NIPS, 2016.

[14] Cossock, David and Zhang, Tong. Statistical analysis of bayes optimal subset ranking. IEEE

Transactions on Information Theory, 54(11):5140–5154, 2008.

[15] Crammer, Koby and Singer, Yoram. On the algorithmic implementation of multiclass kernel-
based vector machines. Journal of Machine Learning Research (JMLR), 2:265–292, 2001.

[16] Do, Chuong B., Le, Quoc, Teo, Choon Hui, Chapelle, Olivier, and Smola, Alex. Tighter bounds

for structured estimation. In NIPS, 2009.

[17] Do˘gan, Ürün, Glasmachers, Tobias, and Igel, Christian. A uniﬁed view on multi-class support
vector classiﬁcation. Journal of Machine Learning Research (JMLR), 17(45):1–32, 2016.

[18] Duchi, John C., Mackey, Lester W., and Jordan, Michael I. On the consistency of ranking

algorithms. In ICML, 2010.

[19] Durbin, Richard, Eddy, Sean, Krogh, Anders, and Mitchison, Graeme. Biological sequence
analysis: probabilistic models of proteins and nucleic acids. Cambridge university press, 1998.

10

[20] Gao, Wei and Zhou, Zhi-Hua. On the consistency of multi-label learning. In COLT, 2011.

[21] Gimpel, Kevin and Smith, Noah A. Softmax-margin CRFs: Training loglinear models with cost

functions. In NAACL, 2010.

[22] Hazan, Tamir and Urtasun, Raquel. A primal-dual message-passing algorithm for approximated

large scale structured prediction. In NIPS, 2010.

[23] Keshet, Joseph. Optimizing the measure of performance in structured prediction. In Advanced

Structured Prediction. MIT Press, 2014.

[24] Kotlowski, Wojciech, Dembczynski, Krzysztof, and Huellermeier, Eyke. Bipartite ranking

through minimization of univariate loss. In ICML, 2011.

[25] Lafferty, John, McCallum, Andrew, and Pereira, Fernando. Conditional random ﬁelds: Proba-

bilistic models for segmenting and labeling sequence data. In ICML, 2001.

[26] Lee, Yoonkyung, Lin, Yi, and Wahba, Grace. Multicategory support vector machines: Theory
and application to the classiﬁcation of microarray data and satellite radiance data. Journal of
the American Statistical Association, 99(465):67–81, 2004.

[27] Lin, Yi. A note on margin-based loss functions in classiﬁcation. Statistics & Probability Letters,

68(1):73–82, 2004.

[28] London, Ben, Huang, Bert, and Getoor, Lise. Stability and generalization in structured prediction.

Journal of Machine Learning Research (JMLR), 17(222):1–52, 2016.

[29] Long, Phil and Servedio, Rocco. Consistency versus realizable H-consistency for multiclass

classiﬁcation. In ICML, 2013.

[30] McAllester, D. A. and Keshet, J. Generalization bounds and consistency for latent structural

probit and ramp loss. In NIPS, 2011.

[31] McAllester, David. Generalization bounds and consistency for structured labeling. In Predicting

Structured Data. MIT Press, 2007.

[32] Narasimhan, Harikrishna, Ramaswamy, Harish G., Saha, Aadirupa, and Agarwal, Shivani.

Consistent multiclass algorithms for complex performance measures. In ICML, 2015.

[33] Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Robust stochastic approximation
approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.

[34] Nowozin, Sebastian and Lampert, Christoph H. Structured learning and prediction in computer
vision. Foundations and Trends in Computer Graphics and Vision, 6(3–4):185–365, 2011.

[35] Nowozin, Sebastian, Gehler, Peter V., Jancsary, Jeremy, and Lampert, Christoph H. Advanced

Structured Prediction. MIT Press, 2014.

[36] Orabona, Francesco. Simultaneous model selection and optimization through parameter-free

stochastic learning. In NIPS, 2014.

[37] Pedregosa, Fabian, Bach, Francis, and Gramfort, Alexandre. On the consistency of ordinal
regression methods. Journal of Machine Learning Research (JMLR), 18(55):1–35, 2017.

[38] Pletscher, Patrick, Ong, Cheng Soon, and Buhmann, Joachim M. Entropy and margin maxi-

mization for structured output learning. In ECML PKDD, 2010.

[39] Ramaswamy, Harish G. and Agarwal, Shivani. Convex calibration dimension for multiclass

loss matrices. Journal of Machine Learning Research (JMLR), 17(14):1–45, 2016.

[40] Ramaswamy, Harish G., Agarwal, Shivani, and Tewari, Ambuj. Convex calibrated surrogates

for low-rank loss matrices with applications to subset ranking losses. In NIPS, 2013.

[41] Shi, Qinfeng, Reid, Mark, Caetano, Tiberio, van den Hengel, Anton, and Wang, Zhenhua. A
hybrid loss for multiclass and structured prediction. IEEE transactions on pattern analysis and
machine intelligence (TPAMI), 37(1):2–12, 2015.

11

[42] Smith, Noah A. Linguistic structure prediction. Synthesis lectures on human language tech-

[43] Steinwart, Ingo. How to compare different loss functions and their risks. Constructive Approxi-

[44] Taskar, Ben, Guestrin, Carlos, and Koller, Daphne. Max-margin markov networks. In NIPS,

nologies, 4(2):1–274, 2011.

mation, 26(2):225–287, 2007.

2003.

[45] Taskar, Ben, Chatalbashev, Vassil, Koller, Daphne, and Guestrin, Carlos. Learning structured

prediction models: a large margin approach. In ICML, 2005.

[46] Tewari, Ambuj and Bartlett, Peter L. On the consistency of multiclass classiﬁcation methods.

Journal of Machine Learning Research (JMLR), 8:1007–1025, 2007.

[47] Tsochantaridis, I., Joachims, T., Hofmann, T., and Altun, Y. Large margin methods for
structured and interdependent output variables. Journal of Machine Learning Research (JMLR),
6:1453–1484, 2005.

[48] Williamson, Robert C., Vernet, Elodie, and Reid, Mark D. Composite multiclass losses. Journal

of Machine Learning Research (JMLR), 17(223):1–52, 2016.

[49] Zhang, Tong. Statistical analysis of some multi-category large margin classiﬁcation methods.

Journal of Machine Learning Research (JMLR), 5:1225–1251, 2004.

[50] Zhang, Tong. Statistical behavior and consistency of classiﬁcation methods based on convex

risk minimization. Annals of Statistics, 32(1):56–134, 2004.

12

Supplementary Material (Appendix)

On Structured Prediction Theory with Calibrated
Convex Surrogate Losses

Outline

Section A: Proof of learning complexity Theorem 6.
Section B: Technical lemmas useful for the proofs.
Section C: Discussion and consistency results on a family of surrogate losses.
Section D: Bounds on the calibration functions.
Section D.1: Theorem 7 – a lower bound.
Section D.2: Theorem 8 – an upper bound.
Section D.3: Computation of the bounds for speciﬁc task losses.

Section E: Computations of the exact calibration functions for the quadratic surrogate.

Section E.1: 0-1 loss.
Section E.2: Block 0-1 loss.
Section E.3: Hamming loss.
Section E.4: Mixed 0-1 and block 0-1 loss.

Section F: Computing constants appearing in the SGD rate.
Section G: Properties of the basis of the Hamming loss.

A Learning complexity theorem

Theorem 6 (Learning complexity). Under the assumptions of Theorem 5, for any ε > 0, the random
(w.r.t. the observed training set) output ¯f(N ) ∈ FF,H of the ASGD algorithm after

N > N ∗ := 4D2M 2
Φ,L,F (ε)

ˇH 2

(19)

iterations has the expected excess risk bounded with ε, i.e., IE[RL(¯f(N ))] < R∗

L,F + ε.

Proof. By (10) from Theorem 5, N steps of the algorithm, in expectation, result in ˇHΦ,L,F (ε)
accuracy on the surrogate risk, i.e., IE[RΦ(¯f(N ))] − R∗
Φ,F < ˇHΦ,L,F (ε). We now generalize the
proof of Theorem 2 to the case of expectation w.r.t. ¯f(N ) depending on the random samples used
by the ASGD algorithm. We take the expectation of (4) w.r.t. ¯f(N ) substituted as f and use Jensen’s
inequality (by convexity of ˇHΦ,L,F ) to get IE[RΦ(¯f(N ))]−R∗
L,F )] ≥
ˇHΦ,L,F (IE[RL(¯f(N ))] − R∗
L,F <
ε.

Φ,F ≥ IE[ ˇHΦ,L,F (RL(¯f(N ))−R∗
L,F ). Finally, monotonicity of ˇHΦ,L,F implies IE[RL(¯f(N ))] − R∗

B Technical lemmas

In this section, we prove two technical lemmas that simplify the proofs of the main theoretical claims
of the paper.

Lemma 9 computes the excess of the weighted surrogate risk δφ for the quadratic loss Φquad (12),
which is central to our analysis presented in Section 4. The key property of this result is that the
excess δφ is jointly convex w.r.t. the parameters θ and conditional distribution q, which simpliﬁes
further analysis.

13

Lemma 10 allows to cope with the combinatorial aspect of the computation of the calibration function.
In particular, when the excess of the weighted surrogate risk is convex, Lemma 10 reduces the
computation of the calibration function to a set of convex optimization problems, which often can be
solved analytically. For symmetric losses, such as the 0-1, block 0-1 and Hamming losses, Lemma 10
also provides “symmetry breaking”, meaning that many of the obtained convex optimization problems
are identical up to a permutation of labels.
Lemma 9. Consider the quadratic surrogate Φquad (12) deﬁned for a task loss L. Let a subspace
of scores F ⊆ Rk be parametrized by θ ∈ Rr, i.e., f = F θ ∈ F with F ∈ Rk×r, and assume that
span(L) ⊆ F. Then, the excess of the weighted surrogate loss can be expressed as

δφquad(F θ, q) := φquad(F θ, q) − inf
θ(cid:48)∈Rr

φquad(F θ(cid:48), q) = 1

2k (cid:107)F θ + Lq(cid:107)2
2.

Proof. By using the deﬁnition of the quadratic surrogate Φquad (12), we have

φ(f (θ), q) = 1

2k (θTF TF θ + 2θTF TLq) + r(q),
θ∗ := argminθ φ(f (θ), q) = −(F TF )†F TLq,

δφ(f (θ), q) = 1

2k (θTF TF θ + 2θTF TLq

+ qTLTF (F TF )†F TLq),

where r(q) denotes the quantity independent of parameters θ. Note that PF := F (F TF )†F T is
the orthogonal projection on the subspace span(F ), so if span(L) ⊆ span(F ) we have PF L = L,
which ﬁnishes the proof.

Lemma 10. In the case of a ﬁnite number k of labels, for any task loss L, a surrogate loss Φ that is
continuous and bounded from below, and a set of scores F, the calibration function can be written as

HΦ,L,F (ε) = min

Hij(ε),

i,j∈pred(F )
i(cid:54)=j

where the set pred(F) ⊆ Y is deﬁned as the set of labels that the predictor can predict for some
feasible scores and Hij is deﬁned via minimization of the same objective as (5), but w.r.t. a smaller
domain:

(20)

(21)

δφ(f , q),

Hij(ε) = inf
f ,q
s.t. (cid:96)i(q) ≤ (cid:96)j(q) − ε,

(cid:96)i(q) ≤ (cid:96)c(q), ∀c ∈ pred(F),
fj ≥ fc, ∀c ∈ pred(F),
f ∈ F,
q ∈ ∆k.

Here (cid:96)c(q) := (Lq)c is the expected loss if predicting label c. Index i represents a label with the
smallest expected loss while index j represents a label with the largest score.

Proof. We use the notation Fj to deﬁne the set of score vectors f where the predictor pred(f ) takes a
value j, i.e., Fj := {f ∈ F | pred(f ) = j}. The union of the sets Fj, j ∈ pred(F), equals the whole
set F. It is possible that sets Fj do not fully contain their boundary because of the usage of a particular
tie-breaking strategy, but their closure can be expressed as F j := {f ∈ F | fj ≥ fc, ∀c ∈ pred(F)}.

If f ∈ Fj, i.e. j = pred(f ), then the feasible set of probability vectors q for which a label i is one of
the best possible predictions (i.e. δ(cid:96)(f , q) = (cid:96)j(q) − (cid:96)i(q) ≥ ε) is

∆k,i,j,ε := {q ∈ ∆k | (cid:96)i(q) ≤ (cid:96)c(q), ∀c ∈ pred(F); (cid:96)j(q) − (cid:96)i(q) ≥ ε},

because inf f (cid:48)∈F (cid:96)(f (cid:48), q) = minc∈pred(F ) (cid:96)c(q).
The union of the sets {Fj × ∆k,i,j,ε}i,j∈pred(F ) thus exactly equals the feasibility set of the optimiza-
tion problem (5)-(6) (note that this is not true for the union of the sets {F j × ∆k,i,j,ε}i,j∈pred(F ),
which can be strictly larger), thus we can rewrite the deﬁnition of the calibration function as follows:

HΦ,L,F (ε) = min

i,j∈pred(F )
i(cid:54)=j

inf
f ∈Fj ,
q∈∆k,i,j,ε

δφ(f , q).

(22)

14

To ﬁnish the proof, we use Lemma 27 of [49] claiming that the function δφ(f , q) is continuous w.r.t.
both q and f , which allows us to substitute sets Fj in (22) with their closures F j without changing
the value of the inﬁmum.

C Consistent surrogate losses

An ideal surrogate should not only be consistent, but also allow efﬁcient optimization, by, e.g., being
convex and allowing fast computation of stochastic gradients. In this paper, we study a generalization
to arbitrary multi-class losses of a surrogate loss class from Zhang [49, Section 4.4.2]8 that satisﬁes
these requirements:

(cid:88)k

Φa,b(f , y) := 1
k

(23)
where a, b : R → R are convex functions. A generic method to minimize this surrogate is to use
any version of the SGD algorithm, while computing the stochastic gradient by sampling y from the
data generating distribution and a label c uniformly. In the case of the quadratic surrogate Φquad, we
proposed instead in the main paper to compute the sum over c analytically instead of sampling c.

(cid:0)L(c, y)a(fc) + b(fc)(cid:1),

c=1

Extending the argument from Zhang [49], we show that the surrogates of the form (23) are consistent
w.r.t. a task loss L under some sufﬁcient assumptions formalized in Theorem 11.
Theorem 11 (Sufﬁcient conditions for consistency). The surrogate loss Φa,b is consistent w.r.t. a task
loss L, i.e., HΦa,b,L,Rk (ε) > 0 for any ε > 0, under the following conditions on the functions a(f )
and b(f ):

1. The functions a and b are convex and differentiable.
2. The function ca(f ) + b(f ) is bounded from below and has a unique global minimizer (ﬁnite

or inﬁnite) for all c ∈ [0, Lmax].

3. The functions a(f ) and b(cid:48)(f )

a(cid:48)(f ) are strictly increasing.

Proof. Consider an arbitrary conditional probability vector q ∈ ∆k. Assumption 2 then implies
that the global minimizer f ∗ of the conditional surrogate risk φ(f , q) w.r.t. f is unique. Assump-
tion 1 allows us to set the derivatives to zero and obtain b(cid:48)(f ∗
c )
c ) = −(cid:96)c(q) where (cid:96)c(q) := (Lq)c.
a(cid:48)(f ∗
Assumption 3 then implies that f ∗

i holds if and only if (cid:96)j(q) ≤ (cid:96)i(q).

j ≥ f ∗

Now, we will prove by contradiction that H(ε) := HΦa,b,L,Rk (ε) > 0 for any ε > 0. Assume that
for some ε > 0 we have H(ε) = 0. Lemma 10 then implies that for some i, j ∈ Y, i (cid:54)= j, we have
Hij(ε) = 0. Note that the domain of (21) deﬁning Hij is separable w.r.t. q and f . We can now
rewrite (21) as

Hij(ε) = inf

q∈∆k,i,j,ε

δφ∗(q), where δφ∗(q) := inf
f ∈F j

δφ(f , q),

where ∆k,i,j,ε and F j are deﬁned in the proof of Lemma 10. Lemma 27 of [49] implies that the
function δφ∗(q) is a continuous function of q. Given that ∆k,i,j,ε is a compact set, the inﬁmum is
achieved at some point q∗ ∈ ∆k,i,j,ε. For this q∗, the global minimum w.r.t. f exists (Assumption 2).
The uniqueness of the global minimum implies that we have f ∗
c . The argument at
the beginning of this proof then implies (cid:96)j(q∗) ≤ (cid:96)i(q∗) which contradicts the inequality (cid:96)i(q∗) ≤
(cid:96)j(q∗) − ε in the deﬁnition of ∆k,i,j,ε.

j = maxc∈Y f ∗

Note that Theorem 11 actually proves that the surrogate Φa,b is order-preserving [49], which is a
stronger property than consistency.

Below, we give several examples of possible functions a(f ), b(f ) that satisfy the conditions in
Theorem 11 and their corresponding f ∗((cid:96)) := argminf ∈Rk φ(f , q) when (cid:96) := Lq:

1. If a(f ) = f , b(f ) = f 2
2. If a(f ) = 1
Lmax
2 log( 1
(cid:96)).
Lmax

1

2 then f ∗((cid:96)) = −(cid:96), leading to our quadratic surrogate (12).

(exp(f ) − exp(−f )), b(f ) = exp(−f ) then f ∗((cid:96)) = 1

2 log(1 − 1
Lmax

(cid:96)) −

8Zhang [49] refers to this surrogate as “decoupled unconstrained background discriminative surrogate”. Note

the 1/k scaling to make Φa,b of order 1.

15

3. If a(f ) = 1
Lmax

f , b(f ) = log(1 + exp(−f )) then f ∗((cid:96)) = log(1 − 1
Lmax

(cid:96)) − log( 1
Lmax

(cid:96)).

In the case of binary classiﬁcation, these surrogates reduce to L2-, exponential, and logistic losses,
respectively.

D Bounds on the calibration function

D.1 Lower bound

Theorem 7 (Lower bound on HΦquad ). For any task loss L, its quadratic surrogate Φquad, and a score
subspace F containing the column space of L, the calibration function can be lower bounded:

HΦquad,L,F (ε) ≥

ε2
2k maxi(cid:54)=j (cid:107)PF ∆ij (cid:107)2
2

≥ ε2
4k ,

where PF is the orthogonal projection on the subspace F and ∆ij = ei − ej ∈ Rk with ec being
the c-th basis vector of the standard basis in Rk.

Proof. First, let us assume that the score subspace F is deﬁned as the column space of a matrix F ∈
Rk×r, i.e., f (θ) = F θ. Lemma 9 gives us expression (13) for δφquad(F θ, q), which is jointly convex
w.r.t. a conditional probability vector q and parameters θ.

The optimization problem (5)-(6) is non-convex because the constraint (6) on the excess risk depends
of the predictor function pred(f ), see Eq. (1), containing the argmax operation. However, if we
constrain the predictor to output label j, i.e., fj ≥ fc, ∀c, and the label delivering the smallest possible
expected loss to be i, i.e., (Lq)i ≤ (Lq)c, ∀c, the problem becomes convex because all the constraints
are linear and the objective is convex. Lemma 10 in App. B allows to bound the calibration function
Hij(ε),9 where Hij(ε) is
with the minimization w.r.t. selected labels i and j, HΦquad,L,F (ε) ≥ min
i(cid:54)=j
deﬁned as follows:

1

Hij(ε) = min
θ,q
s.t. (Lq)i ≤ (Lq)j − ε,

2k (cid:107)F θ + Lq(cid:107)2
2,

(Lq)i ≤ (Lq)c, ∀c ∈ pred(F)
(F θ)j ≥ (F θ)c, ∀c ∈ pred(F)
q ∈ ∆k.

To obtain a lower bound, we relax (24) by removing some of the constraints and arrive at

1

2k (cid:107)F θ + Lq(cid:107)2
2,

Hij(ε) ≥ min
θ,q
s.t. ∆T
∆T

ijLq ≤ −ε,
ijF θ ≤ 0,

(27)
ijF θ = (F θ)i − (F θ)j, and ∆ij = ei − ej ∈ Rk with ec ∈ Rk

where ∆T
ijLq = (Lq)i − (Lq)j, ∆T
being a vector of all zeros with 1 at position c.

The constraint (26) can be readily substituted with equality

∆T
without changing the minimum because multiplication of both q and θ by the constant −ε
preserves feasibility and can only decrease the objective (25).

ijLq = −ε,

(28)
ij Lq ∈ (0, 1]
∆T

We now explicitly solve the resulting constraint optimization problem via the KKT optimality
conditions. The stationarity constraints give us

1

k F T(F θ + Lq) + µF T∆ij = 0,
k LT(F θ + Lq) + νLT∆ij = 0;
9To simplify the statement of Theorem 7, we removed the constraints i, j ∈ pred(F) from Lemma 10 which
said that we should consider only the labels that can be predicted with some feasible scores. A potentially tighter
lower bound can be obtained by keeping the i, j ∈ pred(F) constraint.

(29)

(30)

1

16

(24)

(25)

(26)

the complementary slackness gives µ∆T
µ ≥ 0.

Equation (29) allows to compute

ijF θ = 0 and the feasibility constraints give (28), (27), and

θ = −(F TF )†(kµF T∆ij + F TLq).

By substituting (31) into (30) and by using the identity (because L ∈ span(F )):

PF L = F (F TF )†F TL = L,
(32)
we get (µ − ν)LT∆ij = 0. If LT∆ij = 0, the problem (25), (27), (28) is infeasible for ε > 0
implying Hij(ε) = +∞. Otherwise, we have µ = ν.

From (31) and (32), we also have that:

F θ + Lq = −kµPF ∆ij.

By plugging (31) into the complementary slackness condition and combining with (28), we get
µ2k(cid:107)PF ∆ij(cid:107)2

2 = µε

implying that either µ = 0 or µk(cid:107)PF ∆ij(cid:107)2
2 = ε. In the ﬁrst case, Eq. (33) implies F θ = −Lq
making satisfying both (28) and (27) impossible. Thus, the later is satisﬁed implying that the
objective (25) is equal to10

1

2k (cid:107)F θ + Lq(cid:107)2

2 =

ε2
2k(cid:107)PF ∆ij (cid:107)2
2

.

Finally, orthogonal projections contract the L2-norm, thus (cid:107)PF ∆ij(cid:107)2
lower bound in the statement of the theorem and ﬁnishes the proof.

2 ≤ 2, which gives the second

D.2 Upper bound

Theorem 8 (Upper bound on HΦquad ). If a loss matrix L with Lmax > 0 deﬁnes a pseudometric7 on
labels and there are no constraints on the scores, i.e., F = Rk, then the calibration function for the
quadratic surrogate Φquad can be upper bounded:
HΦquad,L,F (ε) ≤ ε2
2k ,

0 ≤ ε ≤ Lmax.

Proof. After applying Lemmas 9 and 10, we arrive at

(31)

(33)

(34)

1

2k (cid:107)f + Lq(cid:107)2,

Hij(ε) = inf
f ,q
s.t. (cid:96)i(q) ≤ (cid:96)j(q) − ε,

(cid:96)i(q) ≤ (cid:96)c(q), ∀c ∈ Y,
fj ≥ fc, ∀c ∈ Y,
f ∈ Rk,
q ∈ ∆k.

2 − ε
We now consider labels i and j such that Lij = Lmax > 0 and the point qi = 1
2Lij
(non-negative for ε ≤ Lmax). We let qc = 0 for c (cid:54)∈ {i, j}, fj = fi = −(cid:96)i(q) and fc = −(cid:96)c(q) for
c (cid:54)∈ {i, j}. We now show that this assignment is feasible.

2 + ε
2Lij

, qj = 1

We have (cid:96)j(q) = qiLji + qjLjj = qiLji = qiLij by symmetry of L. Similarly, (cid:96)i(q) = qjLij and
thus

(cid:96)j(q) − (cid:96)i(q) = Lij

ε
Lij

= ε.

We also have

(cid:96)c(q) − (cid:96)i(q) = qiLci + qjLcj − qjLij ≥ qj(Lic + Lcj − Lij) ≥ 0.
The ﬁrst inequality uses qi ≥ qj and the second inequality uses the fact that L satisﬁes the triangle
inequality (as a pseudometric). Finally, fj − fc = −(cid:96)i(q) + (cid:96)c(q) ≥ 0.

We thus have shown that the deﬁned point is feasible, so we compute its objective value. We have

1

2k (cid:107)f + Lq(cid:107)2 = 1

2k ((cid:96)j(q) − (cid:96)i(q))2 = ε2
2k ,

which completes the proof.

µ∗ = ∞).

10The possibility PF ∆ij = 0 is also covered by this equation with the convention that 1/0 = ∞ (in this case,

17

D.3 Computation of the lower bounds for speciﬁc task losses

4k , which is shown to be tight in Proposition 12 of Section E.1.

0-1 loss. Let L01 denote the loss matrix of the 0-1 loss, i.e., L01(i, j) := [i (cid:54)= j].2 It is convenient
k − Ik, where 1k ∈ Rk is the vector of all ones and
to rewrite it with a matrix notation L01 = 1k1T
Ik ∈ Rk×k is the identity matrix. We have rank(L01) = k (for k ≥ 2), thus span(L) = Rk. By
putting no constraints on the scores, we can easily apply Theorem 7 and obtain the lower bound
of ε2
Block 0-1 loss. We use the symbol L01,b to denote the loss matrix of the block 0-1 loss with b blocks,
i.e., L01,b(i, j) := [i and j are not in the same block]. We use sv to denote the size of block v,
v = 1, ..., b, and then s1 + · · · + sb = k. In the case when all the blocks are of equal sizes, we denote
their size by s and have k = bs.
k − U U T where the columns of the matrix U ∈ Rk×b
With a matrix notation, we have L01,b = 1k1T
are indicators of the blocks. We have rank(L01,b) = b and can simply deﬁne F01,b := span(F01,b)
with F01,b := U . If we assume that all the blocks have equal size, then we have U TU = sIb and
(cid:107)PF01,b ∆ij(cid:107)2
s if labels i and j belong to different blocks, while PF01,b ∆ij = 0 if i and j belong
to the same block. This leads to the lower bound ε2
4b , which is shown to be tight in Proposition 14 of
Section E.2.

2 = 2

Hamming loss. Consider the (normalized) Hamming loss between tuples of T binary variables,
where ˆyt and yt are the t-th variables of a prediction ˆy and a correct label y, respectively:

LHam,T ( ˆy, y) := 1
T

(cid:88)T

[ˆyt (cid:54)= yt]

t=1

(35)

= 1
T

= 1
T

= 1
T

(cid:88)T

(cid:88)T

(cid:88)T

t=1

t=1

t=1

([ˆyt = 0][yt = 1] + [ˆyt = 1][yt = 0])

(1 − [ˆyt = 1])[yt = 1] + [ˆyt = 1][yt = 0])

[yt = 1] + 1
T

(cid:88)T

t=1

([yt = 0] − [yt = 1]) [ˆyt = 1]

= α0(y) +

αt(y)[ˆyt = 1],

(cid:88)T

t=1

The vectors αt(·) depend only on the column index of the loss matrix. The decomposition (35) im-
plies that FHam,T := span(FHam,T ) equals to span(LHam,T ) for FHam,T := [ 1
2 12T , h(1), . . . , h(T )],
(h(t)) ˆy := [ˆyt = 1], t = 1, . . . , T . We also have that rank(LHam,T ) = rank(FHam,T ) = T + 1.
In Section G, we show that maxi(cid:54)=j (cid:107)PFHam,T ∆ij(cid:107)2
bound (14), we get HΦquad,LHam,T ,FHam,T ≥ ε2
of Section E.3.

2T . By plugging this identity into the lower
8T , which appears to be tight according to Proposition 15

2 = 4T

Non-tight cases. In the cases of the block 0-1 loss and the mixed 0-1 and block 0-1 loss (Proposi-
tions 13 and 16, respectively), we observe gaps between the lower bound (14) and the exact calibration
functions, which shows the limitations of the bound. In particular, it cannot detect level-η consistency
for η > 0 (see Deﬁnition 3) and does not change when the loss changes, but the score subspace stays
the same.

E Exact calibration functions for quadratic surrogate

This section presents our derivations for the exact values of the calibration functions for different
losses. While doing these derivations, we have used numerical simulations and symbolic derivations
to check for correctness. Our numerical and symbolic tools are available online.11

E.1 0-1 loss

Proposition 12. Let L01 be the 0-1 loss, i.e., L01(i, j) = [i (cid:54)= j]. Then, the calibration function
equals the following quadratic function w.r.t. ε:

HΦquad,L01,Rk (ε) = ε2
4k ,
11https://github.com/aosokin/consistentSurrogates_derivations

0 ≤ ε ≤ 1.

(36)

18

Note that in the case of binary classiﬁcation, the function (36) is equal to the calibration function for
the least squares and truncated least squares surrogates [5, 43].

Proof. First, Lemma 9 with F = Rk and F = Ik gives us the expression

δφquad(F θ, q) = 1

2k (cid:107)f + Lq(cid:107)2
2,

(37)

with f = θ ∈ Rk.

We now reduce the optimization problem (5)-(6) to a convex one by using Lemma 10 and by
writing HΦquad,L01,Rk (ε) = mini(cid:54)=j∈Y Hij(ε), which holds because pred(Rk) = Y. Because of the
symmetries of the 0-1 loss, all the choices of i and j give the same (up to a permutation of labels)
optimization problem to compute Hij(ε). The deﬁnition of the 0-1 loss implies (Lq)c = 1−qc, which
simpliﬁes the excess of the expected task loss appearing in (6) to δ(cid:96)(f , q) = (Lq)j −(Lq)i = qi −qj.
After putting all these together, we get

(38)

Hij(ε) = min
f ,q

1
2k

k
(cid:88)

(fc + 1 − qc)2,

c=1
s.t. qi ≥ qj + ε,

qi ≥ qc, c = 1, . . . , k,
fj ≥ fc, c = 1, . . . , k,

k
(cid:88)

c=1

qc = 1,

qc ≥ 0.

2 ; f ∗

2 − ε

c = −1, c (cid:54)∈ {i, j}, f ∗ := f ∗

2 + ε
We claim that there exists an optimal point of (38), f ∗, q∗, such that q∗
2 ,
j = 1
q∗
j . Note that apart from the speciﬁc value of f ∗, this
is the same point used to prove the upper bound of Theorem 8. After proving this, we will minimize
the objective w.r.t. remaining scores at this point.12
First, if any q∗
operation

c = δ > 0, c (cid:54)∈ {i, j}, we can safely move this probability mass to qi and qj with the

c = 0, c (cid:54)∈ {i, j}, q∗

i = f ∗

i = 1

i + δ
2 ,
i + δ
2 ,

i := q∗
q∗
i := f ∗
f ∗

c := q∗
q∗
c := f ∗
f ∗

c − δ = 0,
c − δ,

j + δ
2 ,
j + δ
2 ,
which keeps all the constraints of (38) feasible and does not change the objective value.
Second, all the scores f ∗
the objective. With this, setting f ∗
violate the constraints.
We now show that the equality q∗
the operation

j + ε can hold at the optimum. Indeed, if q∗

c have to belong to the segment [−1, 0] otherwise clipping them will decrease
c := −1, c (cid:54)∈ {i, j} can only decrease the objective and will not

j := q∗
q∗
j := f ∗
f ∗

j = δ(cid:48) > ε,

i = q∗

i − q∗

q∗
i := q∗
i := f ∗
f ∗

i − δ(cid:48)−ε
2 ,
i − δ(cid:48)−ε
2 ,

q∗
j := q∗
j := f ∗
f ∗

j + δ(cid:48)−ε
2 ,
j + δ(cid:48)−ε
2 .

(39)

keeps the objective the same and maintains the feasibility constraints. So combining with q∗
j = 1
i = 1
we can now conclude that q∗
We now show that the equality f ∗
j can hold at the optimum. First, we know that the values
f ∗
i and f ∗
i − 1], otherwise we can always truncate the values to
the borders of the segment and get an improvement of the objective. Finally, since the inequality
f ∗
j ≥ f ∗
i − 1 to
minimize the objective.

i must hold, we conclude that f ∗

j belong to the segment [q∗

2 + ε
2 , q∗
i = f ∗

is closest to its target q∗

2 is an optimal point.

j := f ∗ so that f ∗
i

j − 1, q∗

i = f ∗

2 − ε

i + q∗

j = 1,

12Note that just showing the feasibility of the assigned values q∗ and f ∗ give us an upper bound on the
calibration function. In the case of the 0-1 loss, it appears that this upper bound matches the lower bound
provided by Theorem 7, so we do not need to prove optimality explicitly. However, we still give this proof as a
simple illustration of the proof technique as its structure will be re-used also for the cases when the bound of
Theorem 7 is not tight.

19

At the optimal point deﬁned above, it remains to ﬁnd the value f ∗ delivering the minimum of the
objective. We can achieve this by computing

Hij(ε) = 1

2k min

f ∈[−1,0]

(f + 1

2 − ε

2 )2 + (f + 1

2 + ε

2 )2,

which implies f ∗ = −0.5 and HΦquad,L01,Rk (ε) = ε2
4k .

2 + ε

2 − ε
Remark. We note that the conditional distribution used in the proof above, qi = 1
2 ,
qc = 0, c (cid:54)∈ {i, j}, is somewhat unsatisfying from the perspective of explaining why learning the
0-1 loss might be difﬁcult. Indeed, it looks like a gradient based learning algorithm that would start
with all values fc = −1 would at the end only optimize over fi and fj as the gradient with respect
to fc for c /∈ {i, j} would stay at zero in Φquad(f , y) (12) given that only i or j could appear in y.
From this observation, one could think that the calibration function perspective is misleading as SGD
could have faster convergence rate than predicted by the worst case for this situation. Fortunately,
3(k−2) for c (cid:54)∈ {i, j}, fi = fj = 1
one can easily check that the point qi = 1
2 , qj = 1
3
and fc = −(cid:96)c(q) for c (cid:54)∈ {i, j} is feasible for (38) and yields the same optimal value of ε2
4k for the
objective, thus providing another example where the exponential multiclass nature is more readily
apparent and cannot be ﬁxed by some “natural initialization” of the learning algorithm.

2 , qc = 1

2 , qj = 1

3 + ε

3 − ε

E.2 Block 0-1 loss

Recall that L01,b is the block 0-1 loss, i.e., L01,b(i, j) = [i and j are not in the same block]. We use
b to denote the total number of blocks and sv to denote the size of block v, v = 1, ..., b. In this
section, we compute the calibration functions for the case of unconstrained scores (Proposition 13)
and for the case of the scores belonging to the column span of the loss matrix (Proposition 14).
Proposition 13. Without constraints on the scores, the calibration function for the block 0-1 loss
equals the following quadratic function w.r.t. ε:

HΦquad,L01,b,Rk (ε) = ε2

4k min

v=1,...,b

2sv

sv+1 ≤ ε2
2k ,

0 ≤ ε ≤ 1.

Note that when sv = 1 for some v, we have HΦquad,L01,b,Rk (ε) matching to the ε2
Theorem 7. When sv → ∞ for all blocks, we have HΦquad,L01,b,Rk (ε) matching to the ε2
of Theorem 8.

4k lower bound of
2k upper bound

Proof. This proof is of the same structure as the proof of Proposition 12 above.

We use b(i) ∈ 1, . . . , b to denote the block to which label i belongs and Yv to denote the set of labels
that belong to block v. We also use Qv, v ∈ 1, . . . , b, as a shortcut to (cid:80)
qi, which is the total
probability mass on block v.

i∈Yv

We start by noting that the i-th component of the vector (L01,b)q equals 1 − Qb(i). By applying
Lemmas 9, 10, we get

Hij(ε) = min
f ,q

1
2k

b
(cid:88)

(cid:88)

(fc + 1 − Qb(c))2,

v=1

c∈Yv
Qb(i) − Qb(j) ≥ ε,
Qb(i) ≥ Qu, u = 1, . . . , b,
fj ≥ fc, c = 1, . . . , k,
(cid:88)k

qc = 1,

qc ≥ 0.

c=1

(40)

(41)

Analogously to Proposition 12, we claim that there exists an optimal point of (40) such that qc = 0,
c (cid:54)∈ {i, j}; qi = 0.5 + ε

2 = Qb(j); fc = −1, c (cid:54)∈ Yij := Yb(i) ∪ Yb(j).

2 = Qb(i); qj = 0.5 − ε

At ﬁrst, note that if b(i) = b(j), then the constraint (41) is never feasible, so we’ll assume that
b(i) (cid:54)= b(j).

20

We will now show that we can consider only conﬁgurations with all the probability mass on the
two selected blocks. Consider some optimal point f ∗, q∗ and denote with δ = (cid:80)
q∗
c the
probability mass on the unselected blocks. The operation
c + δ
i + δ

f ∗
c := −1, c (cid:54)∈ Yij
q∗
c := 0, c (cid:54)∈ Yij

f ∗
c := f ∗
i := q∗
q∗

2 , c ∈ Yij,
j := q∗
2 , q∗

j + δ
2 ,

c∈Y\Yij

can only decrease the objective of (40) because the summands corresponding to the unselected blocks
are set to zero. All the constraints stay feasible and the summands corresponding to the selected
blocks keep their values.
The probability mass within the block b(i) can be safely moved to q∗
i without changing the objective
or violating any constraints. Analogously, the probability mass within the block b(j) can be safely
moved to q∗
j + ε and thus that
i = 1
2 + ε
q∗
At the point deﬁned above, we now minimize the objective (40) w.r.t. fc, c ∈ Yij. At an optimal
point, all values f ∗
b(i) − 1], otherwise we can always
truncate the values to the borders of the segment and get an improvement of the objective. For all the
scores f ∗

j . By reusing the operation (39), we can now ensure that q∗
2 and q∗

c , c ∈ Yij, belong to the segment [Q∗

c , c (cid:54)= j, the following identity holds

b(j) − 1, Q∗

i = q∗

2 − ε
2 .

j = 1

(cid:40)

f ∗
c =

Q∗
b(c) − 1, if Q∗
f ∗
j .

b(c) − 1 < f ∗
j ,

Combining with the segment constraint, it implies that in the block of the label i, we have f ∗
c ∈ Yb(i), and, in the block of the label j, we have f ∗

b(j) − 1, c ∈ Yb(j) \ j.

c = Q∗

c = f ∗
j ,

By plugging the obtained values of q∗
get

c and f ∗

c into (40) and denoting the value f ∗

j + 0.5 with ˜f , we

(cid:16)

sb(i)( ˜f − ε

2 )2 + ( ˜f + ε

2 )2(cid:17)

,

1
2k

Hij(ε) = min
˜f
s.t. ˜f ∈ [− ε

2 , ε
2 ].

By setting the derivative of the objective (43) to zero, we get

(42)

(43)

which belongs to the segment [− ε

2 , ε

2 ]. We compute the function value at this point:

˜f = ε
2

sb(i)−1
sb(i)+1 ,

Hij(ε) = ε2
4k

2sb(i)
sb(i)+1 ,

which ﬁnishes the proof.

Proposition 14. Let the scores f be piecewise constant on the blocks of the loss, i.e. belong to the
subspace F01,b = span(L01,b) ⊆ Rk. Then, the calibration function equals the following quadratic
function w.r.t. ε:

HΦquad,L01,b,F01,b (ε) = ε2

4k min
v(cid:54)=u

2svsu
sv+su

,

0 ≤ ε ≤ 1.

If all the blocks are of the same size, we have HΦquad,L01,b,F01,b (ε) = ε2
blocks.

4b where b is the number of

Proof. The constraints on scores f ∈ F01,b simply imply that the scores within all the blocks are
equal. Having this in mind, the proof exactly matches the proof of Proposition 13 until the argument
around Eq. (42). Now we cannot set the scores of the block b(j) to different values, and, thus they
are all equal to f ∗.

By plugging the obtained values of q∗
get

c and f ∗

c into (40) and denoting the value f ∗

j + 0.5 with ˜f , we

(cid:16)

sb(i)( ˜f − ε

2 )2 + sb(j)( ˜f + ε

2 )2(cid:17)

,

1
2k

Hij(ε) = min
˜f
s.t. ˜f ∈ [− ε

(44)

2 , ε
2 ].

21

By setting the derivative of the objective (44) to zero, we get

˜f = ε
2

sb(i)−sb(j)
sb(i)+sb(j)

,

which belongs to the segment [− ε

2 , ε

2 ]. We now compute the function value at this point:
,

Hij(ε) = ε2
4k

2sb(i)sb(j)
sb(i)+sb(j)

which ﬁnishes the proof.

E.3 Hamming loss

Recall that LHam,T is the Hamming loss deﬁned over T binary variables (see Eq. (35) for the precise
deﬁnition). In this section, we compute the calibration function for the case of the scores belonging
to the column span of the loss matrix (Proposition 15).
Proposition 15. Assume that the scores f always belong to the column span of the Hamming loss
matrix LHam,T , i.e., FHam,T = span(LHam,T ) ⊆ Rk. Then, the calibration function can be computed
as follows:

HΦquad,LHam,T ,FHam,T (ε) = ε2
8T ,

0 ≤ ε ≤ 1.

Proof. We start the proof by applying Lemma 10 and by studying the vector of the expected
losses (LHam,T )q. We note that the ˆy-th element (cid:96) ˆy(q), ˆy = (ˆyt)T
t=1, ˆyt ∈ {0, 1}, has a simple form
of

(cid:96) ˆy(q) =

(cid:88)

qy
T

T
(cid:88)

y∈Y

t=1

[ˆyt (cid:54)= yt] = 1 − 1
T

qy[ˆyt = yt].

T
(cid:88)

(cid:88)

t=1

y∈Y

The quantity (cid:80)
y∈Y qy[ˆyt = yt] corresponds to the marginal probability of a variable t taking a
label ˆyt. Note that the expected loss (cid:96) ˆy(q) only depends on q through marginal probabilities, thus
two distributions q1 and q2 with the same marginals would be indistinguishable when plugged
in the optimization problem for Hij(ε) (21), given that both the constraints and the objective (by
Lemma 9) only depend on q through the expected loss (cid:96) ˆy(q). Having this in mind, we can consider
(cid:0)qt[yt = 1] + (1 − qt)[yt = 0](cid:1), where qt ∈ [0, 1],
only separable distributions, i.e., qy = (cid:81)T
t = 1, . . . , T , are the parameters deﬁning the distribution.

t=1

By combining the notation above with Lemmas 9 and 10, we arrive at the following optimization
problem:

H ˜y ˆy(ε) = min
f ,q

1
2k

fy +1− 1
T

(cid:88)T

t=1

(cid:17)2

,

qt,yt

s.t. 1
T

(qt,˜yt −qt,ˆyt) ≥ ε,

k
(cid:88)

(cid:16)

y∈Y
(cid:88)T

t=1

(cid:88)T

(qt,˜yt −qt,yt) ≥ 0, ∀y ∈ Y,

t=1

1
T
f ˆy ≥ fy, ∀y ∈ Y,
0 ≤ qt ≤ 1, t = 1, . . . , T,
f ∈ F,

(48)
(49)
(50)
where qt,yt is a shortcut to qt[yt = 1] + (1 − qt)[yt = 0] and labels ˜y and ˆy serve as the selected
labels i and j, respectively.
The calibration function HΦquad,LHam,T ,FHam,T (ε) = ε2
8T in the formulation of this proposition matches
the lower bound provided by Theorem 7 in Section D.3. Thus, it sufﬁces to construct a feasible
w.r.t. (46)-(50) assignment of variables f , q and labels ˜y, ˆy such that the objective equals the lower
bound.

It sufﬁces to simply set ˜y to all zeros and ˆy to all ones. In this case, the constraints (46) and (47) take
the simpliﬁed form:

(45)

(46)

(47)

(51)

(52)

(cid:88)T

(1 − 2qt) ≥ ε,

1
T
t=1
qt ≤ 1
2 , t = 1, . . . , T.

22

We now set qt := 1
2 1k. This point is clearly feasible when
0 ≤ ε ≤ 1, so it remains to compute the value of the objective. We complete the proof by writing
(let w be the count of ones in an assignment y):

2 , t = 1, . . . , T , and f := − 1

2 − ε

k
(cid:88)

(cid:16)

1
2k

1
2k

1
2k

y∈Y

T
(cid:88)

w=0

T
(cid:88)

w=0

fy +1− 1
T

(cid:88)T

t=1

(cid:17)2

qt,yt

=

(cid:1)(cid:0) 1

(cid:0)T
w

2 − 1

T (w( 1

2 − ε

2 ) + (T − w)( 1

2 + ε

2 ))(cid:1)2

=

(cid:0)T
w

(cid:1)( ε

2 − wε

T )2 = ε2

2k

(cid:0)T
w

(cid:1)( 1

4 − w

T + w2

T 2 ) =

T
(cid:88)

w=0

ε2
2k ( 1

4 2T − 1
where we use the equality k = 2T and the identities (cid:80)T
(cid:80)T

T T 2T −1 + 1

(cid:1) = T (T + 1)2T −2.

t=0 t2(cid:0)T

t

T 2 T (T + 1)2T −2) = ε2
8T ,

(cid:0)T
t

(cid:1) = 2T , (cid:80)T

t=0 t(cid:0)T

t

(cid:1) = T 2T −1,

t=0

E.4 Mixed 0-1 and block 0-1 loss

Recall that L01,b,η is the convex combination of the 0-1 loss and the block 0-1 loss with b blocks, i.e.,
L01,b,η = ηL01 + (1 − η)L01,b, 0 ≤ η ≤ 1. Let all the blocks be of the same size s = k
b ≥ 2. In this
section, we compute the calibration functions for the case of unconstrained scores (Proposition 16)
and for the case when scores belong to the column span of the loss matrix (Proposition 17).
Proposition 16. If there are no constraints on scores f then the calibration function

HΦquad,L01,b,η,Rk (ε) =

(cid:40) ε2
ε ≤ η
4k ,
2k(s+1) − η(ε+1)(s−1)

1−η ,

4k(s+1)

ε2s

(2ε−εη−η)

η
1−η ≤ ε ≤ 1

shows that the surrogate is consistent.

Note that when η = 0, we have H(ε) = ε2
4k
H(ε) = ε2

4k , which matches Proposition 12.

2s
s+1 as in Proposition 13. When η ≥ 0.5 we have

Proof. This proof is very similar to the proof of Proposition 13, but technically more involved.

We start by noting that the i-th element of the vector (L01,b,η)q equals

(cid:88)

j: b(j)(cid:54)=b(i)

(cid:88)

j: j(cid:54)=i

(1 − η)qj +

ηqj = η(1 − qi) + (1 − η)(1 − Qb(i)),

(53)

where for b(i) and Qv we reuse the notation deﬁned in the proof of Proposition 13. By combining
this with Lemmas 9 and 10, we get

Hij(ε)=min
f ,q

1
2k

b
(cid:88)

(cid:88)

v=1

c∈Yv

(fc + 1 − ηqc − (1 − η)Qb(c))2,

(54)

s.t. η(qi − qj) + (1 − η)(Qb(i) − Qb(j)) ≥ ε,

η(qi − qc) + (1 − η)(Qb(i) − Qb(c)) ≥ 0, ∀c
fj ≥ fc, ∀c,

k
(cid:88)

c=1

qc = 1,

qc ≥ 0, ∀c.

The blocks are all of the same size so we need to consider just the two cases: 1) the selected labels
belong to the same block, i.e., b(i) = b(j); 2) the selected labels belong to the two different blocks,
i.e., b(i) (cid:54)= b(j).

23

The ﬁrst case can be proven by a straight forward generalization of the proof of Proposition 12. Given
that the loss value is bounded by 1, the maximal possible value of ε when the constraints can be
feasible equals η. Thus, we have Hij(ε) = ε2
We will now proceed to the second case b(i) (cid:54)= b(j). We show that

4k for ε ≤ η and +∞ otherwise.

Hij(ε)=

(cid:40) ε2
for ε ≤ η
4k ,
2k(s+1) − η(ε+1)(s−1)

1−η ,

4k(s+1)

ε2s

(2ε − εη − η), otherwise.

Similarly to the arguments used in Propositions 12 and 13, we claim that there is an optimal
point of (54) such that q∗
c = −1 for
c (cid:54)∈ Yij := Yb(i) ∪ Yb(j).
First, we will show that we can consider only conﬁgurations with all the probability mass on the two
selected blocks b(i) and b(j). Given any optimal point f ∗ and q∗, the operation (with δ = (cid:80)
q∗
c )

c = 0, c (cid:54)∈ {i, j}; q∗

i = 0.5 + ε

j = 0.5 − ε

2 ; and f ∗

2 ; q∗

c(cid:54)∈Yij

f ∗
i := f ∗
i + δ
2 ,
f ∗
j := f ∗
j + δ
2 ,
f ∗
c := −1, c (cid:54)∈ Yij
f ∗
c := f ∗

q∗
i := q∗
i + δ
2 ,
q∗
j := q∗
j + δ
2 ,
q∗
c := 0, c (cid:54)∈ Yij

c + (1 − η) δ

2 , c ∈ Yij \ {i, j}
can only decrease the objective of (54) because the summands corresponding to the unselected b − 2
blocks are set to zero. All the constraints stay feasible and the values corresponding to the blocks
b(i) and b(j) do not change. The last operation is required, because the values Qb(i), Qb(j) change
when we change qi and qj. Adding (1 − η) δ
2 to some scores compensates this and cannot violate the
constraints because f ∗

2 ≥ (1 − η) δ
2 .
Now we will show that it is possible to move all the mass to the two selected labels i and j. We
cannot simply move the mass within one block, but need to create some overﬂow and move it to
another block in a speciﬁc way. Consider δ := q∗
a, which is some non-zero mass on a non-selected
label of the block b(i). Then, the operation

j goes up by δ

f ∗
i := f ∗
j := f ∗
f ∗
f ∗
a := f ∗
f ∗
c := f ∗
c := f ∗
f ∗

i + δ η
2 ,
j + δ η
2 ,
a + δ η
2 (η − 3),
c − δ η
2 (1 − η), c ∈ Yi \ {i, a}
c + δ η
2 (1 − η), c ∈ Yj \ {j}

2 ),

q∗
i := q∗
j := q∗
q∗
a := q∗
q∗

i + δ(1 − η
j + δ η
2 ,
a − δ = 0,

does no change the objective value of (54) because the quantities fc + 1 − ηqc − (1 − η)Qb(c),
c ∈ Yij, stay constant and all the constraints of (54) stay feasible. We repeat this operation for all
a ∈ Yb(i) \ {i} and, thus, move all the probability mass within the block b(i) to the label i. In the
block b(j), an analogous operation can move all the mass to the label j.
It remains to show that q∗

j = ε. Indeed, if q∗

i − q∗

i − q∗

j = δ(cid:48) > ε, the operation analogous to (39)
i − δ(cid:48)−ε
2 ,
j + δ(cid:48)−ε
2 ,

q∗
i := q∗
j := q∗
q∗

i − δ(cid:48)−ε
2 ,
j + δ(cid:48)−ε
2 ,
c − (1 − η) δ(cid:48)−ε
c + (1 − η) δ(cid:48)−ε

f ∗
i := f ∗
j := f ∗
f ∗
f ∗
c := f ∗
f ∗
c := f ∗
i − q∗

2 , c ∈ Yb(i) \ {i},
2 , c ∈ Yb(j) \ {j}
i = 0.5 + ε
2 and q∗

can always set q∗
scores of the block b(i) go down and all the scores of the block b(j) go up at most as much as f ∗
the constraints fj ≥ fc cannot get violated.

2 . After this operation, all the
j , so

j = ε, and thus q∗

j = 0.5 − ε

We now proceed with the computation of Hij(ε). First, we note that convexity and symmetries
of (54) implies that all the non-selected scores within each block are equal.13 Denote the scores of the

13If these optimal scores are not equal, by symmetry, one can obtain the same objective and feasibility by
permuting their corresponding values. By taking a uniform convex combination on all permutations, we obtain a
point where all the scores are equal, and by convexity, would yield a lower objective value.

24

non-selected labels of the block b(i) by f (cid:48)
by f (cid:48)
j.
Analogous to all the previous propositions, the truncation argument gives us that all the values f ∗
c
belong to the segment [−1, −0.5 + ε
c , c (cid:54)= j, the following identity
holds:

i , and the scores of the non-selected labels of the block b(j)

2 ]. For all the optimal values f ∗

(cid:40)

f ∗
c =

f ∗
if ηq∗
j ,
c + (1 − η)Q∗
ηq∗

c + (1 − η)Q∗
b(c) − 1,

b(c) − 1 ≥ f ∗
j ,
otherwise.

i wants to equal the maximal possible value −0.5 + ε

Given that f ∗
this value by f .
By, plugging the values of q∗ and f ∗ provided above into the objective of (54), we get

2 , it implies that f ∗

i = f ∗

j . Denote

(cid:16)

1
2k

(f +0.5− ε

2 )2 +(s−1)(f (cid:48)

i +1−(1−η)(0.5+ ε

(f +0.5+ ε

2 )2 +(s−1)(f (cid:48)

j +1−(1−η)(0.5− ε

2 ))2+
2 ))2(cid:17)
2 (1 + ε)(1 − η) − 1, f (cid:48)∗

.

(55)

By minimizing (55) without constraints, we get f ∗ = −0.5, f (cid:48)∗
1
2 (1 − ε)(1 − η) − 1. We now need to compare f (cid:48)∗
and f ∗ ≥ f (cid:48)∗

i and f (cid:48)∗

i = 1

j =
j with f ∗ to satisfy the constraints f ∗ ≥ f (cid:48)∗
i

j . First, we have that
j = 1

f ∗ −f (cid:48)∗

Second, we have

2 (η + ε − ηε) ≥ 0, for 0 ≤ ε ≤ 1 and 0 ≤ η ≤ 1.

f ∗ −f (cid:48)∗

i = 1

2 (η − ε + ηε) ≥ 0, for 0 ≤ ε ≤ η

1−η we have both f (cid:48)

We can now conclude that when ε ≤ η
minimum points leading to Hij(ε) = ε2
4k .
Now, consider the case ε > η
1−η . We have the constraint f ≥ f (cid:48)
i = f . The new unconstrained minimum w.r.t. f equals f ∗ = 1
f (cid:48)
We now show that the inequality f ∗ ≥ f (cid:48)∗
j still holds. We have

1−η and 0 ≤ η ≤ 1.
i and f (cid:48)

j equal to their unconstrained

i violated, so at the minimum we have
s+1 (−1−(s−1)(1− 1
2 (1−η)(1−ε))).

f ∗ − f (cid:48)∗

j = η+εs−ηεs

s+1 ≥ 0, for 0 ≤ ε ≤ 1 and 0 ≤ η ≤ 1.

Substitution of f (cid:48)∗

into (55) gives us

i = f ∗ and f (cid:48)∗
j
(cid:16) ε2s
2(s+1) − η(ε+1)(s−1)

4(s+1)

1
k

(2ε − εη − η)

,

(cid:17)

which equals Hij(ε) for 1 ≥ ε > η

1−η .

Comparing cases 1 and 2, we observe that Hij(ε) from case 2 is never larger than the one of case 1,
thus case 2 provides the overall calibration function Hij(ε).

Proposition 17. If the scores f are constrained to be equal inside the blocks, i.e. belong to the
subspace F01,b = span(L01,b) ⊆ Rk, then the calibration function

HΦquad,L01,b,η,F01,b (ε) =




(ε−

η
2 )2
4b



0,

(

ηb
k +1−η)2
η
2 )2
(1−

,

η
2 ≤ ε ≤ 1,
0 ≤ ε ≤ η
2

shows that the surrogate is consistent up to level η
2 .

When η = 0, we have H(ε) = ε2
which corresponds to the case of inconsistent surrogate (0-1 loss and constrained scores).

4b as in Proposition 14. When η > 0 we have H(ε) = 0 for small ε,

Proof. This proof combines ideas from Proposition 16 and Proposition 14.

Note that contrary to all the previous results, Lemma 9 is not applicable, because, for b < k, we have
that span(L01,b,η) = Rk (cid:54)⊂ F01,b = span(L01,b).

25

We now derive an analog of Lemma 9 for this speciﬁc case. We deﬁne the subspace of scores F01,b =
{F θ | θ ∈ Rb} with a matrix F := F01,b ∈ Rk×b with columns containing the indicator vectors of
the blocks. We have F TF = sIb and thus (F TF )−1 = 1
s Ib. We shortcut the loss matrix L01,b,η to L
and rewrite it as

k − ηIk − (1 − η)F F T.
By redoing the derivation of Lemma 9, we arrive at a different excess surrogate:

L = ηL01 + (1 − η)L01,b = 1k1T

φ(f (θ), q) = 1

2k (sθTθ + 2θTF TLq) + r(q),

δφ(f (θ), q) = 1
= s

θ∗ := argminθ φ(f (θ), q) = − 1
2k (sθTθ + 2θTF TLq + 1
s F TLq(cid:107)2
2k (cid:107)θ + 1
s
(cid:88)

2

s F TLq,
s qTLTF F TLq)

= s
2k

(θv + 1 − (1 − η)Qv − η

s Qv)2,

v=1

where Qv = (cid:80)
of block v.

c∈Yv

qc is the total probability mass on block v and Yv ⊂ Y denotes the set of labels

Analogously to Proposition 16 we can now apply Lemma 10 and obtain Hij(ε).
b
(cid:88)

(θv + 1 − (1−η)Qv − η

s Qv)2,

Hij(ε)=min
θ,q

s
2k

v=1

(56)

s.t. η(qi − qj) + (1 − η)(Qb(i) − Qb(j)) ≥ ε,

η(qi − qc) + (1 − η)(Qb(i) − Qb(c)) ≥ 0, ∀c
θb(j) ≥ θu, ∀u = 1, . . . , b,

k
(cid:88)

c=1

qc = 1,

qc ≥ 0, ∀c.

The main difference to (54) consists in the fact that we now minimize w.r.t. θ instead of f .

Note that because of the way the predictor pred(f (θ)) resolves ties (among the labels with maximal
scores it always picks the label with the smallest index), not all labels can be predicted. Speciﬁcally,
only one label from each block can be picked. This argument allows us to assume that b(i) (cid:54)= b(j) in
the remainder of this proof.
First, let us prove the case for ε ≤ η
2 . We explicitly provide a feasible assignment of variables
where the objective equals zero. We set qi = 1
2(s−1) , c ∈ Yb(j) \ {j}. All the other
labels (including j and the unselected labels of the block b(i)) receive zero probability mass. This
assignment of q implies Qb(i) = Qb(j) = 1
2 and the zero mass on the other blocks. We also set θb(i)
2 + η
1
and θb(j) to (1−η) 1
2 − 1 to ensure zero objective value. Verifying other feasibility constraints
s
we have η(qi − qj) + (1 − η)(Qb(i) − Qb(j)) = η
2 ≥ ε and η(qi − qc) + (1 − η)(Qb(i) − Qb(c)) =
η( 1

2(s−1) ) ≥ 0, c ∈ Yb(j) \ {j}. Other constraints are trivially satisﬁed.

2 and qc = 1

2 − 1

2−η ; q∗

b(i) = 1+ε−η

c = 0, c ∈ Yb(i) \ {i} (other labels in the block b(i)); q∗

2 . As usual, we claim the following values of the variables f
v = −1, v (cid:54)∈ {b(i), b(j)}; and q∗
i =
1−ε
j = 0, q∗
(2−η)(s−1) ,

Now, consider the case of ε > η
and q result in an optimal point. We have q∗
Q∗
c ∈ Yb(j) \ {j} (other labels in the block b(j)).
First, we will show that we can consider only conﬁgurations with all the probability mass on the
two selected blocks b(i) and b(j). Given some optimal variables f ∗ and q∗, the operation (with
δ = (cid:80)

c = 0, c (cid:54)∈ Yij; θ∗

c =

c∈Y\Yij

i := q∗
q∗

i + δ
2 ,

j := q∗
q∗

j + δ
2 ,

q∗
c )
q∗
c := 0, c ∈ Y \ Yij,
θ∗
v := −1, v /∈ {b(i), b(j)},
2 (1 − η + η
θ∗
b(i) := θ∗
s ),
2 (1 − η + η
b(j) := θ∗
θ∗
s )

b(i) + δ
b(j) + δ

26

can only decrease the objective of (56) because the summands corresponding to the unselected b − 2
blocks are set to zero. All the constraints stay feasible and the values corresponding to the blocks b(i)
and b(j) do not change.

Now, we move the mass within the two selected blocks. To start with, moving the mass within one
block does not change the objective, because it depends only on Qb(c) and not on q directly. In the
block b(i), it is safe to increase qi and decrease the mass on the other labels, because qi enters the
constraints with the positive sign and while the others enter with the negative sign. So we let qc = 0
for c ∈ Yb(i)/{i} and Qb(i) = qi. We also have Qb(j) = 1 − qi as the mass on all other blocks is
zero.

Moving mass within the block b(j) is more complicated, as moving mass to some label c of this
block might violate the constraints of (56) on qi. We start by considering the ﬁrst constraint in (56),
using Qb(j) = 1 − qi, we get:

By using qj ≥ 0 and ε ≥ η

qi ≥ ε + ηqj + (1 − η)(1 − qi).
2 , the inequality (57) implies that qi ≥ 1

2 and thus that

Now the second constraint of (56) that we want to satisfy is:

qc ≤ Qb(j) ≤ 1
2

∀c ∈ Yb(j) .

qi ≥ ηqc + (1 − η)Qb(j) ∀c ∈ Yb(j) .

(57)

(58)

(59)

Using (58), we have that the RHS of (59) is ≤ 1/2, and so since qi ≥ 1/2, we have that (59) is
satisﬁed for any valid mass distribution on block b(j) (i.e. such that Qb(j) ≤ 1/2). Using qj = 0
gives the most possibilities for the value of qi in the constraint (57). Moreover, the constraint (57) is
more stringent than the constraint (59), i.e. if it is satisﬁed, the second one is also satisﬁed; so we
focus only on the ﬁrst constraint.

As in the proof of all other propositions, we can make the constraint (57) an equality for the optimum
by generalizing the transformation of (39) which makes the constraint tight without changing the
objective and maintaining feasibility. So (57) as an equality with qj = 0 yields the value

i = 1+ε−η
q∗
2−η .

2−η

and Qb(j) = 1 − q∗

So to summarize at this point, we have q∗
c = 0, Yb(i) (cid:54)∈ {b(i), b(j)}.
i = 1+ε−η
q∗
i . The precise distribution of mass for c ∈ Yb(j)/{j} does not
matter (any distribution is feasible and does not inﬂuence the objective, only the total mass matters),
1−ε
but for concreteness, we can choose them to all have the same mass yielding q∗
(2−η)(s−1) ,
c ∈ Yb(j) \ {j}.

c = 0, c ∈ Yb(i) \ {i}; q∗

j = 0; q∗

c =

s Q∗

b(j) + η

b(j) − 1, (1 − η)Q∗

We now ﬁnish the computation of Hij(ε). First, we note that, due to the truncation argument
similar to the one mentioned in the paragraph after (39), we have both θ∗
j in the segment
[(1 − η)Q∗
j = θ∗
j ≥ θ∗
i =: θ
at the optimum.
Substituting the values Q∗
b(j) provided above into the objective of (56) and performing
unconstrained minimization w.r.t. θ (we use the help of MATLAB symbolic toolbox to set the
derivative to zero) we get

i and θ∗
i , we have θ∗

b(i) − 1] and since θ∗

b(i) and Q∗

b(i) + η

s Q∗

θ∗ = − s−η+ηs

2s

Hij(ε) =

s(ε−

η
2 )2(
4k(1−

η
s +1−η)2
η
2 )2

,

and, consequently,

which ﬁnishes the proof.

F Constants in the SGD rate

To formalize the learning difﬁculty by bounding the required number of iterations to get a good value
of the risk (Theorem 6), we need to bound the constants D and M . In this section, we provide a

27

way to bound these constants for the quadratic surrogate Φquad (12) under a simplifying assumption
slightly stronger than the well-speciﬁed model Assumption 4.

Consider the family of score functions FF,H deﬁned via an explicit feature map ψ(x) ∈ H, i.e.,
fW (x) = F W ψ(x), where a matrix F ∈ Rk×r deﬁnes the structure and an operator (which we think
of as a matrix with one dimension being inﬁnite) W : H → Rr contains the learnable parameters.
Then the surrogate risk can be written as

RΦ(fW ) = IE(x,y)∼D

1

2k (cid:107)F W ψ(x) + L(:, y)(cid:107)2

Rk

and its stochastic w.r.t. (x, y) gradient as

gx,y(W ) = 1

k F T(F W ψ(x) + L(:, y))ψ(x)T

(60)

where L(:, y) denotes the column of the loss matrix corresponding to the correct label y. Note that
computing the stochastic gradient requires performing products F TF and F TL(:, y) for which direct
computation is intractable when k is exponential, but which can be done in closed form for the
structured losses we consider (the Hamming and block 0-1 loss). More generally, these operations
require suitable inference algorithms.

To derive the constants, we use a simplifying assumption stronger than Assumption 4 in the case of
quadratic surrogate: we assume that the conditional qc(x), seen as a function of x, belongs to the
RKHS H, which by the reproducing property implies that for each c = 1, . . . , k, there exists vc ∈ H
such that qc(x) = (cid:104)vc, ψ(x)(cid:105)H for all x ∈ X . Concatenating all vc, we get an operator V : H → Rk.
To derive the bound, we also assume that (cid:80)k
c=1 (cid:107)vc(cid:107)H ≤ Qmax and (cid:107)ψ(x)(cid:107)H ≤ R for all x ∈ X . In
the following, we use the notation qx to denote the vector in Rk with components qc(x), c = 1, . . . , k,
for a ﬁxed x, and thus qx = V ψ(x).

Under these assumptions, we can write the theoretical minimum of the surrogate risk. The gradient
of the surrogate risk gives

k∇W RΦ(fW ) = F TF W IEx∼DX (ψ(x)ψ(x)T) + F TLIEx∼DX (qxψ(x)T)

= F TF W IEx∼DX (ψ(x)ψ(x)T) + F TLV IEx∼DX (ψ(x)ψ(x)T)
= (cid:0)F TF W + F TLV (cid:1) IEx∼DX (ψ(x)ψ(x)T).
Setting the content of the parenthesis to zero gives that W ∗ = −(F TF )†F TLV is a solution to the
stationary condition equation ∇W RΦ(fW ) = 0.
We can now bound the Hilbert-Schmidt norm of this choice of optimal parameters W ∗ as
(cid:107)W ∗(cid:107)HS = (cid:107)(F TF )†F TLV (cid:107)HS

√

≤ (cid:107)(F TF )†F T(cid:107)HS(cid:107)LV (cid:107)HS
r(cid:107)(F TF )†F T(cid:107)2(cid:107)LV (cid:107)HS
≤
rσ−1
rσ−1

min (F )(cid:107)LV (cid:107)HS
√
min (F )

kLmaxQmax =: D

√

√

=

≤

//submultiplicativity of (cid:107) · (cid:107)HS
//connection of (cid:107) · (cid:107)HS and (cid:107) · (cid:107)2 via r = rank(F )
//rotation invariance of (cid:107) · (cid:107)2

//the deﬁnition of (cid:107) · (cid:107)HS and triangular inequality

where (cid:107) · (cid:107)HS and (cid:107) · (cid:107)2 denote the Hilbert-Schmidt and spectral norms, respectively, and σ−1
min (F )
stands for the smallest singular value of the matrix F . The last inequality follows from the deﬁnition of
the Hilbert-Schmidt norm (cid:107)LV (cid:107)2
H and from the triangular inequality
c=1 L(i, c)vc(cid:107)H ≤ (cid:80)k
(cid:107) (cid:80)k
Analogously, we now bound the Hilbert-Schmidt norm of the stochastic gradient gx,y(W ).

c=1 |L(i, c)|(cid:107)vc(cid:107)H ≤ LmaxQmax thus giving (cid:107)LV (cid:107)HS ≤

c=1 L(i, c)vc(cid:107)2

HS = (cid:80)k

i=1 (cid:107) (cid:80)k

kLmaxQmax.

√

(cid:107)gx,y(W )(cid:107)HS ≤ 1
≤ 1
≤ 1
≤ 1

k (cid:107)F TF W ψ(x) + F TL(:, y))(cid:107)2(cid:107)ψ(x)(cid:107)H
k ((cid:107)F TF W ψ(x)(cid:107)2 + (cid:107)F TL(:, y))(cid:107)2)(cid:107)ψ(x)(cid:107)H
k ((cid:107)F TF (cid:107)2(cid:107)W (cid:107)HS(cid:107)ψ(x)(cid:107)H + (cid:107)F (cid:107)2(cid:107)L(:, y))(cid:107)2)(cid:107)ψ(x)(cid:107)H
√
k σ2

max(F )DR2 + 1

kLmaxR =: M

k σmax(F )

where R is an upper bound on (cid:107)ψ(x)(cid:107)H and σmax(F ) is a maximal singular value of F . Here the ﬁrst
inequality follows from the fact that the rank of gx,y(W ) equals 1 and from submultiplicativity of

28

DM = κ2(F )R2rL2

the spectral norm. We also use the inequality (cid:107)W ψ(x)(cid:107)2 ≤ (cid:107)W (cid:107)HS(cid:107)ψ(x)(cid:107)H, which follows from
the properties of the Hilbert-Schmidt norm.
The bound of Theorem 5 contains the quantity DM and the step size of ASGD depends on D
M , so, to
be practical, both quantities cannot be exponential (for numerical stability; but the important quantity
is the number of iterations from Theorem 6). We have
maxQ2
max(F )
k R2 + σmax(F )σmin(F )

D = σ2
where κ(F ) = σmax
is the condition number of F . Note that the quantity DM is invariant to the
σmin
scaling of the matrix F . The quantity D
M scales proportionally to the square of the scale of F and
thus rescaling F can always bring it to O(1). For the rest of the analysis, we consider R and Qmax to
be well-behaved constants and thus focus on the dependence of the quantity DM on F and L.

maxQmax = L2

ξ(z) = z2 + z,

max + κ(F )R

maxξ(κ(F )

rRQmax),

R
Qmax

rL2

√

√

M

√

k

r

F.1 Constants for speciﬁc losses

We now estimate the product DM from (18) for the 0-1, block 0-1 and Hamming losses. For the
deﬁnition of the losses and the corresponding matrices F , we refer to Section D.3.

0-1 loss. For the 0-1 loss L01 and F = Ik, we have Lmax = 1, r = k, σmin = σmax = 1, thus
DM = O(k) is very large leading to very slow convergence of ASGD.

√

s, thus DM = O(b).

Block 0-1 loss. For the block 0-1 loss L01,b and matrix F01,b, we have Lmax = 1, r = b, σmin =
σmax =
Hamming loss. For the Hamming loss, we have Lmax = 1, r = log2 k + 1, κ(FHam,T ) ≤ log2 k + 2
(see the derivation in Section G). Finally, we have DM = O(log3

2 k).

G Properties of the basis of the Hamming loss

As deﬁned in (35), the matrix LHam,T ∈ Rk×k is the matrix of the Hamming loss between tuples
of T binary variables, and the number of labels equals k = 2T . Also recall that FHam,T :=
[ 1
2 12T , h(1), . . . , h(T )], (h(t)) ˆy := [ˆyt = 1], t = 1, . . . , T . We have FHam,T = span(FHam,T ) =
span(LHam,T ) and rank(LHam,T ) = rank(FHam,T ) = T +1.
We now explicitly compute maxi(cid:54)=j (cid:107)PFHam,T ∆ij(cid:107)2

2. We shortcut FHam,T by F and compute

F TF = 2T −2

(61)








1
1
1
· · ·
1

1
2
1
· · ·
· · ·

· · ·
1
2
· · ·
1








1
· · ·
· · ·
1
2

.

We can compute the inverse matrix explicitly as well:

(F TF )−1 = 22−T

−1
−1
· · ·
−1
The vector F T∆ij equals the difference of the two rows of F , i.e., [0, c1, . . . , cT ]T ∈ RT +1 with
each ct ∈ {−1, 0, +1}. We explicitly compute the square norm (cid:107)PFHam,T ∆ij(cid:107)2
2:











(62)

.



1 + T −1
1
0
· · ·
· · ·



· · · −1
· · ·
0
· · ·
1
0
· · ·
1
0

(cid:107)PFHam,T ∆ij(cid:107)2

2 = ∆T

ijF (F TF )−1F T∆ij = [0, c1, . . . , cT ](F TF )−1[0, c1, . . . , cT ]T = 22−T

where the last equality follows from the identity submatrix of (62) and from the zero in the ﬁrst
position of the vector F T∆ij. The quantity (cid:107)PFHam,T ∆ij(cid:107)2
2 is maximized when none of ct equals
zero, which is achievable, e.g., when the label i corresponds to all zeros and the label j to all ones.
We now have maxi(cid:54)=j (cid:107)PFHam,T ∆ij(cid:107)2

2 = 4T
2T .

T
(cid:88)

t=1

c2
t ,

29

We now compute the smallest and largest eigenvalues of the Gram matrix (61) for FHam,T . Ignoring
the scaling factor 2T −2, we see by Gaussian elimination that the determinant and thus the product
of all eigenvalues equals 1.
If we subtract IT +1 the matrix becomes of rank 2, meaning that
T − 1 eigenvalues equal 1. The trace, i.e., the sum of the eigenvalues of (61), without the scaling
factor 2T −2 equals 2T + 1. Summing up, we have λminλmax = 1 and λmin + λmax = T + 2. We can
T ] and λmax = 1
now compute λmin = 1
T 2 + 4T ) ∈
√
[T + 1, T + 2]. By putting back the multiplicative factor, we get σmin =

2 (T + 2 +
√
λmin ≥
(cid:112)log2 k + 2, and thus the condition number is κ ≤ log2 k + 2.

T 2 + 4T ) ∈ [ 1

2 (T + 2 −

k
log2 k+2

T +2 , 1

λmax ≤

σmax =

√
2

and

√

√

√

√

k
2

30

