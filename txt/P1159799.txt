Maximum Classiﬁer Discrepancy for Unsupervised Domain Adaptation

Kuniaki Saito1, Kohei Watanabe1, Yoshitaka Ushiku1, and Tatsuya Harada1,2

1The University of Tokyo, 2RIKEN

k-saito,watanabe,ushiku,harada
}

{

@mi.t.u-tokyo.ac.jp

8
1
0
2
 
r
p
A
 
3
 
 
]

V
C
.
s
c
[
 
 
4
v
0
6
5
2
0
.
2
1
7
1
:
v
i
X
r
a

Abstract

In this work, we present a method for unsupervised do-
main adaptation. Many adversarial learning methods train
domain classiﬁer networks to distinguish the features as ei-
ther a source or target and train a feature generator net-
work to mimic the discriminator. Two problems exist with
these methods. First, the domain classiﬁer only tries to dis-
tinguish the features as a source or target and thus does not
consider task-speciﬁc decision boundaries between classes.
Therefore, a trained generator can generate ambiguous fea-
tures near class boundaries. Second, these methods aim to
completely match the feature distributions between different
domains, which is difﬁcult because of each domain’s char-
acteristics.

To solve these problems, we introduce a new approach
that attempts to align distributions of source and target by
utilizing the task-speciﬁc decision boundaries. We propose
to maximize the discrepancy between two classiﬁers’ out-
puts to detect target samples that are far from the sup-
port of the source. A feature generator learns to gener-
ate target features near the support to minimize the dis-
crepancy. Our method outperforms other methods on sev-
eral datasets of image classiﬁcation and semantic segmen-
tation. The codes are available at https://github.
com/mil-tokyo/MCD_DA

1. Introduction

The classiﬁcation accuracy of images has improved sub-
stantially with the advent of deep convolutional neural net-
works (CNN) which utilize numerous labeled samples [16].
However, collecting numerous labeled samples in various
domains is expensive and time-consuming.

Domain adaptation (DA) tackles this problem by trans-
ferring knowledge from a label-rich domain (i.e., source do-
main) to a label-scarce domain (i.e., target domain). DA
aims to train a classiﬁer using source samples that general-
ize well to the target domain. However, each domain’s sam-
ples have different characteristics, which makes the prob-

Figure 1. (Best viewed in color.) Comparison of previous and the
proposed distribution matching methods.. Left: Previous meth-
ods try to match different distributions by mimicing the domain
classiﬁer. They do not consider the decision boundary. Right:
Our proposed method attempts to detect target samples outside the
support of the source distribution using task-speciﬁc classiﬁers.

lem difﬁcult to solve. Consider neural networks trained on
labeled source images collected from the Web. Although
such neural networks perform well on the source images,
correctly recognizing target images collected from a real
camera is difﬁcult for them. This is because the target im-
ages can have different characteristics from the source im-
ages, such as change of light, noise, and angle in which
the image is captured. Furthermore, regarding unsupervised
DA (UDA), we have access to labeled source samples and
only unlabeled target samples. We must construct a model
that works well on target samples despite the absence of
their labels during training. UDA is the most challenging
situation, and we propose a method for UDA in this study.

Many UDA algorithms, particularly those for training
neural networks, attempt to match the distribution of the
source features with that of the target without considering
the category of the samples [8, 37, 4, 40]. In particular, do-
main classiﬁer-based adaptation algorithms have been ap-
plied to many tasks [8, 4]. The methods utilize two players
to align distributions in an adversarial manner: domain clas-
siﬁer (i.e., a discriminator) and feature generator. Source
and target samples are input to the same feature generator.

1

Features from the feature generator are shared by the dis-
criminator and a task-speciﬁc classiﬁer. The discriminator
is trained to discriminate the domain labels of the features
generated by the generator whereas the generator is trained
to fool it. The generator aims to match distributions be-
tween the source and target because such distributions will
mimic the discriminator. They assume that such target fea-
tures are classiﬁed correctly by the task-speciﬁc classiﬁer
because they are aligned with the source samples.

However, this method should fail to extract discrimi-
native features because it does not consider the relation-
ship between target samples and the task-speciﬁc decision
boundary when aligning distributions. As shown in the left
side of Fig. 1, the generator can generate ambiguous fea-
tures near the boundary because it simply tries to make the
two distributions similar.

To overcome both problems, we propose to align distri-
butions of features from source and target domain by using
the classiﬁer’s output for the target samples.

We introduce a new adversarial learning method that uti-
lizes two types of players:
task-speciﬁc classiﬁers and a
feature generator. task-speciﬁc classiﬁers denotes the clas-
siﬁers trained for each task such as object classiﬁcation or
semantic segmentation. Two classiﬁers take features from
the generator. Two classiﬁers try to classify source samples
correctly and, simultaneously, are trained to detect the tar-
get samples that are far from the support of the source. The
samples existing far from the support do not have discrimi-
native features because they are not clearly categorized into
some classes. Thus, our method utilizes the task-speciﬁc
classiﬁers as a discriminator. Generator tries to fool the
classiﬁers. In other words, it is trained to generate target
features near the support while considering classiﬁers’ out-
put for target samples. Thus, our method allows the gen-
erator to generate discriminative features for target samples
because it considers the relationship between the decision
boundary and target samples. This training is achieved in
an adversarial manner. In addition, please note that we do
not use domain labels in our method.

We evaluate our method on image recognition and se-
mantic segmentation. In many settings, our method outper-
forms other methods by a large margin. The contributions
of our paper are summarized as follows:

We propose a novel adversarial training method for do-
main adaptation that tries to align the distribution of
a target domain by considering task-speciﬁc decision
boundaries.

•

•

•

2. Related Work

Training CNN for DA can be realized through vari-
ous strategies. Ghifary et al. proposed using an autoen-
coder for the target domain to obtain domain-invariant fea-
tures [9]. Sener et al. proposed using clustering techniques
and pseudo-labels to obtain discriminative features [33].
Taigman et al. proposed cross-domain image translation
methods [38]. Matching distributions of the middle fea-
tures in CNN is considered to be effective in realizing an
accurate adaptation. To this end, numerous methods have
been proposed [8, 37, 4, 29, 40, 36].

The representative method of distribution matching in-
volves training a domain classiﬁer using the middle features
and generating the features that deceive the domain classi-
ﬁer [8]. This method utilizes the techniques used in gen-
erative adversarial networks [10]. The domain classiﬁer is
trained to predict the domain of each input, and the category
classiﬁer is trained to predict the task-speciﬁc category la-
bels. Feature extraction layers are shared by the two classi-
ﬁers. The layers are trained to correctly predict the label of
source samples as well as to deceive the domain classiﬁer.
Thus, the distributions of the middle features of the target
and source samples are made similar. Some methods utilize
maximum mean discrepancy (MMD) [22, 21], which can
be applied to measure the divergence in high-dimensional
space between different domains. This approach can train
the CNN to simultaneously minimize both the divergence
and category loss for the source domain. These methods
are based on the theory proposed by [2], which states that
the error on the target domain is bounded by the divergence
of the distributions. To our understanding, these distribu-
tion aligning methods using GAN or MMD do not con-
sider the relationship between target samples and decision
boundaries. To tackle these problems, we propose a novel
approach using task-speciﬁc classiﬁers as a discriminator.

Consensus regularization is a technique used in multi-
source domain adaptation and multi-view learning, in which
multiple classiﬁers are trained to maximize the consensus
of their outputs [23]. In our method, we address a training
step that minimizes the consensus of two classiﬁers, which
is totally different from consensus regularization. Consen-
sus regularization utilizes samples of multi-source domains
to construct different classiﬁers as in [23]. In order to con-
struct different classiﬁers, it relies on the different character-
istics of samples in different source domains. By contrast,
our method can construct different classiﬁers from only one
source domain.

We conﬁrm the behavior of our method through a toy
problem.

3. Method

We extensively evaluate our method on various tasks:
digit classiﬁcation, object classiﬁcation, and semantic
segmentation.

In this section, we present the detail of our proposed
method. First, we give the overall idea of our method in
Section 3.1. Second, we explain about the loss function we

Figure 2. (Best viewed in color.) Example of two classiﬁers with an overview of the proposed method. Discrepancy refers to the disagree-
ment between the predictions of two classiﬁers. First, we can see that the target samples outside the support of the source can be measured
by two different classiﬁers (Leftmost, Two different classiﬁers). Second, regarding the training procedure, we solve a minimax problem in
which we ﬁnd two classiﬁers that maximize the discrepancy on the target sample, and then generate features that minimize this discrepancy.

used in experiments in Section 3.2. Finally, we explain the
entire training procedure of our method in Section 3.3.

3.1. Overall Idea

}

We have access to a labeled source image xs and a corre-
sponding label ys drawn from a set of labeled source images
, as well as an unlabeled target image xt drawn
Xs, Ys
{
from unlabeled target images Xt. We train a feature gener-
ator network G, which takes inputs xs or xt, and classiﬁer
networks F1 and F2, which take features from G. F1 and
F2 classify them into K classes, that is, they output a K-
dimensional vector of logits. We obtain class probabilities
by applying the softmax function for the vector. We use
x) to denote the K-dimensional
the notation p1(y
|
probabilistic outputs for input x obtained by F1 and F2 re-
spectively.

x), p2(y

|

The goal of our method is to align source and target fea-
tures by utilizing the task-speciﬁc classiﬁers as a discrim-
inator in order to consider the relationship between class
boundaries and target samples. For this objective, we have
to detect target samples far from the support of the source.
The question is how to detect target samples far from the
support. These target samples are likely to be misclassi-
ﬁed by the classiﬁer learned from source samples because
they are near the class boundaries. Then, in order to de-
tect these target samples, we propose to utilize the disagree-
ment of the two classiﬁers on the prediction for target sam-
ples. Consider two classiﬁers (F1 and F2) that have dif-
ferent characteristics in the leftmost side of Fig. 2. We
assume that the two classiﬁers can classify source samples
correctly. This assumption is realistic because we have ac-
cess to labeled source samples in the setting of UDA. In ad-
dition, please note that F1 and F2 are initialized differently

to obtain different classiﬁers from the beginning of training.
Here, we have the key intuition that target samples outside
the support of the source are likely to be classiﬁed differ-
ently by the two distinct classiﬁers. This region is denoted
by black lines in the leftmost side of Fig. 2 (Discrepancy
Region). Conversely, if we can measure the disagreement
between the two classiﬁers and train the generator to mini-
mize the disagreement, the generator will avoid generating
target features outside the support of the source. Here, we
consider measuring the difference for a target sample using
the following equation, d(p1(y
xt)) where d de-
|
notes the function measuring divergence between two prob-
abilistic outputs. This term indicates how the two classiﬁers
disagree on their predictions and, hereafter, we call the term
as discrepancy. Our goal is to obtain a feature generator that
can minimize the discrepancy on target samples.

xt), p2(y

|

In order to effectively detect target samples outside the
support of the source, we propose to train discriminators
(F1 and F2) to maximize the discrepancy given target fea-
tures (Maximize Discrepancy in Fig. 2). Without this opera-
tion, the two classiﬁers can be very similar ones and cannot
detect target samples outside the support of the source. We
then train the generator to fool the discriminator, that is,
by minimizing the discrepancy (Minimize Discrepancy in
Fig. 2). This operation encourages the target samples to be
generated inside the support of the source. This adversarial
learning steps are repeated in our method. Our goal is to
obtain the features, in which the support of the target is in-
cluded by that of the source (Obtained Distributions in Fig.
2). We show the loss function used for discrepancy loss in
the next section. Then, we detail the training procedure.

K
(cid:88)

k=1

(Xs, Ys) =

L

E

−

(xs,ys)∼(Xs,Ys)

1l[k=ys] log p(y

xs)
|

(3)
Step B In this step, we train the classiﬁers (F1, F2) as a dis-
criminator for a ﬁxed generator (G). By training the classi-
ﬁers to increase the discrepancy, they can detect the target
samples excluded by the support of the source. This step
corresponds to Step B in Fig. 3. We add a classiﬁcation loss
on the source samples. Without this loss, we experimentally
found that our algorithm’s performance drops signiﬁcantly.
We use the same number of source and target samples to
update the model. The objective is as follows:

(4)

(5)

min
F1,F2 L

(Xs, Ys)

− Ladv(Xt).

Ladv(Xt) = Ext∼Xt[d(p1(y

|

xt), p2(y

xt))]
|

Step C We train the generator to minimize the discrepancy
for ﬁxed classiﬁers. This step corresponds to Step C in
Fig. 3. The number n indicates the number of times we re-
peat this for the same mini-batch. This number is a hyper-
parameter of our method. This term denotes the trade-off
between the generator and the classiﬁers. The objective is
as follows:

G Ladv(Xt).
min
These three steps are repeated in our method. To our un-
derstanding, the order of the three steps is not important.
Instead, our major concern is to train the classiﬁers and gen-
erator in an adversarial manner under the condition that they
can classify source samples correctly.

(6)

3.4. Theoretical Insight

Since our method is motivated by the theory proposed
by Ben-David et al. [1], we want to show the relationship
between our method and the theory in this section.

∆

H

-distance (dH∆H(

Ben-David et al. [1] proposed the theory that bounds the
expected error on the target samples, RT (h), by using three
terms: (i) expected error on the source domain, RS (h); (ii)
)), which is measured as the
H
discrepancy between two classiﬁers; and (iii) the shared er-
denote source
ror of the ideal joint hypothesis, λ.
and target domain respectively. Another theory [2] bounds
the error on the target domain, which introduced
-distance
(dH(
)) for domain divergence. The two theories and
their relationships can be explained as follows.

and

H

S

S

S

T

T

T

,

,

Theorem 1 Let H be the hypothesis class. Given two do-
mains

, we have

and

S

T

h

∀

∈

H, RT (h)

RS (h) +

dH∆H(

,

) + λ

S

T

(7)

≤

≤

RS (h) +

dH(

,

) + λ

S

T

1
2
1
2

Figure 3. Adversarial training steps of our method. We separate the
network into two modules: generator (G) and classiﬁers (F1 , F2 ).
The classiﬁers learn to maximize the discrepancy Step B on the
target samples, and the generator learns to minimize the discrep-
ancy Step C. Please note that we employ a training Step A to
ensure the discriminative features for source samples.

3.2. Discrepancy Loss

In this study, we utilize the absolute values of the dif-
ference between the two classiﬁers’ probabilistic outputs as
discrepancy loss:

d(p1, p2) =

1
K

K
(cid:88)

k=1

p1k −
|

,
p2k|

(1)

where the p1k and p2k denote probability output of p1 and
p2 for class k respectively. The choice for L1-distance is
based on the Theorem . Additionally, we experimentally
found that L2-distance does not work well.

3.3. Training Steps

To sum up the previous discussion in Section 3.1, we
need to train two classiﬁers, which take inputs from the gen-
erator and maximize d(p1(y
xt)), and the gener-
xt), p2(y
|
|
ator which tries to mimic the classiﬁers. Both the classiﬁers
and generator must classify source samples correctly. We
will show the manner in which to achieve this. We solve
this problem in three steps.

Step A First, we train both classiﬁers and generator to
classify the source samples correctly. In order to make clas-
siﬁers and generator obtain task-speciﬁc discriminative fea-
tures, this step is crucial. We train the networks to minimize
softmax cross entropy. The objective is as follows:

min
G,F1,F2 L

(Xs, Ys).

(2)

where

dH∆H(S, T ) = 2

(cid:12)
(cid:12)
E
(cid:12)
(cid:12)
x∼S

I(cid:2)h(x) (cid:54)= h

(x)(cid:3) − E

I(cid:2)h(x) (cid:54)= h

(cid:48)

x∼T

(cid:48)

(cid:12)
(cid:12)
(x)(cid:3)
(cid:12)
(cid:12)

sup
(h,h(cid:48) )∈H2
(cid:12)
(cid:12)
E
(cid:12)
(cid:12)
x∼S

dH(S, T ) = 2 sup
h∈H

I(cid:2)h(x) (cid:54)= 1(cid:3) − E

I(cid:2)h(x) (cid:54)= 1(cid:3)

,

x∼T

(cid:12)
(cid:12)
(cid:12)
(cid:12)

λ = min [RS (h) + RT (h)]

Here, RT (h) is the error of hypothesis h on the target do-
main, and RS (h) is the corresponding error on the source
domain. I[a] is the indicator function, which is 1 if predicate
a is true and 0 otherwise.

-distance is shown to be empirically measured by the er-
H
ror of the domain classiﬁer, which is trained to discrimi-
nate the domain of features. λ is a constant—the shared
error of the ideal joint hypothesis—which is considered suf-
ﬁciently low to achieve an accurate adaptation. Earlier stud-
ies [8, 37, 4, 29, 40] attempted to measure and minimize
-
H
distance in order to realize the adaptation. As this inequality
-distance. We
suggests,
-distance upper-bounds the
will show the relationship between our method and
-
distance.

H

H

H

H

H

∆

∆

(cid:48)

T

),

= h

I(cid:2)h(x)

Regarding dH∆H(

if we consider that h and
,
S
h(cid:48) can classify source samples correctly,
the term
(x)(cid:3) is assumed to be very low. h and
E
x∼S
h(cid:48) should agree on their predictions on source sam-
) is approximately calculated as
ples. Thus, dH∆H(
T
(x)(cid:3), which denotes the supremum
E
x∼T

sup
(h,h(cid:48))∈H2
of the expected disagreement of two classiﬁers’ predictions
on target samples.

,
S
= h

I(cid:2)h(x)

(cid:48)

We assume that h and h

share the feature extraction part.
Then, we decompose the hypothesis h into G and F1, and
(cid:48)
into G and F2. G, F1 and F2 correspond to the net-
h
work in our method. If we substitute these notations into
(x)(cid:3) and for ﬁxed G, the term
the

I(cid:2)h(x)

= h

(cid:48)

(cid:48)

sup
(h,h(cid:48))∈H2
will become

E
x∼T

(8)

(9)

sup
F1,F2

E
x∼T

I [F1 ◦

G(x)

= F2 ◦

G(x)].

Furthermore, if we replace sup with max and minimize the
term with respect to G, we obtain

min
G

max
F1,F2

E
x∼T

I [F1 ◦

G(x)

= F2 ◦

G(x)].

This equation is very similar to the mini-max problem we
solve in our method, in which classiﬁers are trained to maxi-
mize their discrepancy on target samples and generator tries
to minimize it. Although we must train all networks to min-
imize the classiﬁcation loss on source samples, we can see
the connection to the theory proposed by [1].
4. Experiments on Classiﬁcation

First, we observed the behavior of our model on toy
problem. Then, we performed an extensive evaluation of the
proposed methods on the following datasets: digits, trafﬁc
signs, and object classiﬁcation.

Comparison of three decision boundaries

(a) Source Only

(b) No Step C

(c) Proposed

Figure 4. (Best viewed in color.) Red and green points indicate
the source samples of class 0 and 1, respectively. Blue points are
target samples generated by rotating source samples. The dashed
and normal lines are two decision boundaries in our method. The
pink and light green regions are where the results of both classiﬁers
are class 0 and 1, respectively. Fig. 4(a) is the model trained
only on source samples. Fig. 4(b) is the model trained to increase
discrepancy of the two classiﬁers on target samples without using
Step C. Fig. 4(c) shows our proposed method.

4.1. Experiments on Toy Datasets

In the ﬁrst experiment, we observed the behavior of the
proposed method on inter twinning moons 2D problems, in
which we used scikit-learn [27] to generate the target sam-
ples by rotating the source samples. The goal of the ex-
periment was to observe the learned classiﬁers’ boundary.
For the source samples, we generated a lower moon and an
upper moon, labeled 0 and 1, respectively. Target samples
were generated by rotating the angle of the distribution of
the source samples. We generated 300 source and target
samples per class as the training samples. In this experi-
ment, we compared the decision boundary obtained from
our method with that obtained from both the model trained
only on source samples and from that trained only to in-
crease the discrepancy. In order to train the second compa-
rable model, we simply skipped Step C in Section 3.3 dur-
ing training. We tested the method on 1000 target samples
and visualized the learned decision boundary with source
and target samples. Other details including the network ar-
chitecture used in this experiment are provided in our sup-
plementary material. As we expected, when we trained
the two classiﬁers to increase the discrepancy on the target
samples, two classiﬁers largely disagreed on their predic-
tions on target samples (Fig. 4(b)). This is clear when com-
pared to the source only model (Fig. 4(a)). Two classiﬁers
were trained on the source samples without adaptation, and
the boundaries seemed to be nearly the same. Then, our
proposed method attempted to generate target samples that
reduce the discrepancy. Therefore, we could expect that the
two classiﬁers will be similar. Fig. 4(c) demonstrates the
assumption. The decision boundaries are drawn consider-
ing the target samples. The two classiﬁers output nearly the
same prediction for target samples, and they classiﬁed most
target samples correctly.

(a) SVHN to MNIST

(b) SYN SIGN to GTSRB

(c) Source Only

(d) Adapted (Ours)

Figure 5. (Best viewed in color.) Left: Relationship between discrepancy loss (blue line) and accuracy (red and green lines) during
training. As discrepancy loss decreased, accuracy improved. Right: Visualization of features obtained from last pooling layer of the
generator in adaptation from SYN SIGNS to GTSRB using t-SNE [24]. Red and blue points indicate the target and source samples,
respectively. All samples are testing samples. We can see that applying our method makes the target samples discriminative.

METHOD

Source Only

[21]
[7]

MMD
†
DANN
†
DSN
[4]
†
ADDA [39]
CoGAN [19]
PixelDA [3]
Ours (n = 2)
Ours (n = 3)
Ours (n = 4)

[32]

ATDA
ASSC [11]
DRCN [9]

†

to
GTSRB
85.1

SYNSIG MNIST MNIST*
to
USPS
76.7

SVHN
to
MNIST
67.1
Distribution Matching based Methods
71.1
71.1
82.7

to
USPS*
79.4

-
77.1

±
91.3

1.8

91.1
88.7
93.1
-
-
-
93.5
94.0
94.4

82.8
±
-

0.2
0.8

0.8
0.8
0.7

89.4
±
91.2
±
-
92.1
93.8
94.2
Other Methods
-
96.2
-

0.4
0.4
0.3

±
±
±

±
±
±

1.3

91.8

0.09

±

81.1
85.1
-
-
-
95.9

-
-
-

93.1
95.6
96.5

1.9
0.9
0.3

±
±
±

76.0
±
-
-
94.2
95.9
96.2

±
±
±

1.8

2.6
0.5
0.4

86.2

95.7
82.0

1.5
0.1

±
±

USPS
to
MNIST
63.4

-
73.0
±
-
90.1
±
89.1
±
-
90.0
91.8
94.1

±
±
±

0.2

0.8
0.8

1.4
0.9
0.3

-
-

±

73.7

0.04

Table 1. Results of the visual DA experiment on the digits and
trafﬁc signs datasets. The results are cited from each study. The
score of MMD is cited from DSN [4]. Please note that † means
that the method used a few labeled target samples as validation,
which is different from our setting. We repeated each experiment
5 times and report the average and the standard deviation of the
accuracy. The accuracy was obtained from classiﬁer F1. Including
the methods that used the labeled target samples for validation, our
method achieved good performance. MNIST* and USPS* mean
that we used all of the training samples to train the model.

4.2. Experiments on Digits Datasets

In this experiment, we evaluate the adaptation of the
model on three scenarios. The example datasets are pre-
sented in the supplementary material.

We assessed four types of adaptation scenarios by using
the digits datasets, namely MNIST [17], Street View House
Numbers (SVHN) [26], and USPS [14]. We further evalu-
ated our method on the trafﬁc sign datasets, Synthetic Traf-
ﬁc Signs (SYN SIGNS) [25] and the German Trafﬁc Signs
Recognition Benchmark [35] (GTSRB). In this experiment,
we employed the CNN architecture used in [7] and [3]. We
added batch normalization to each layer in these models.
We used Adam [15] to optimize our model and set the learn-

×

10−4 in all experiments. We set the batch
ing rate as 2.0
size to 128 in all experiments. The hyper-parameter peculiar
to our method was n, which denotes the number of times
we update the feature generator to mimic classiﬁers. We
varied the value of n from 2 to 4 in our experiment and ob-
served the sensitivity to the hyper-parameter. We followed
the protocol of unsupervised domain adaptation and did not
use validation samples to tune hyper-parameters. The other
details are provided in our supplementary material due to a
limit of space.
SVHN
SVHN [26] and MNIST [17] have distinct properties be-
cause SVHN datasets contain images with a colored back-
ground, multiple digits, and extremely blurred digits, mean-
ing that the domain divergence is very large between these
datasets.

MNIST

→

SYN SIGNS

GTSRB In this experiment, we evaluated
the adaptation from synthesized trafﬁc signs datasets (SYN
SIGNS dataset [7]) to real-world signs datasets (GTSRB
dataset [35]). These datasets contain 43 types of classes.

→

↔

MNIST

USPS We also evaluate our method on
MNIST and USPS datasets [17] to compare our method
with other methods. We followed the different protocols
provided by the paper, ADDA [39] and PixelDA [3].

Results Table 1 lists the accuracies for the target sam-
ples, and Fig. 5(a) and 5(b) show the relationship between
the discrepancy loss and accuracy during training. For the
source only model, we used the same network architecture
as used in our method. Details are provided in the sup-
plementary material. We extensively compared our meth-
ods with distribution matching-based methods as shown in
Table 1. The proposed method outperformed these meth-
ods in all settings. The performance improved as we in-
creased the value of n. Although other methods such as
ATDA [32] performed better than our method in some sit-
uations, the method utilized a few labeled target samples
to decide hyper-parameters for each dataset. The perfor-
mance of our method will improve too if we can choose the

Method
Source Only
MMD [21]
DANN [7]
Ours (n = 2)
Ours (n = 3)
Ours (n = 4)

plane
55.1
87.1
81.9
81.1
90.3
87.0

bcycl
53.3
63.0
77.7
55.3
49.3
60.9

bus
61.9
76.5
82.8
83.6
82.1
83.7

car
59.1
42.0
44.3
65.7
62.9
64.0

horse
80.6
90.3
81.2
87.6
91.8
88.9

knife mcycl
79.7
17.9
85.9
42.9
65.1
29.5
83.1
72.7
83.8
69.4
79.6
84.7

person
31.2
53.1
28.6
73.9
72.8
76.9

plant
81.0
49.7
51.9
85.3
79.8
88.6

sktbrd
26.5
36.3
54.6
47.7
53.3
40.3

train
73.5
85.8
82.8
73.2
81.5
83.0

truck mean
52.4
8.5
61.1
20.7
57.4
7.8
69.7
27.1
29.7
70.6
71.9
25.8

Table 2. Accuracy of ResNet101 model ﬁne-tuned on the VisDA dataset. The reported accuracy was obtained after 10 epoch updates.

best hyper-parameters for each dataset. As Fig. 5(a) and
5(b) show, as the discrepancy loss diminishes, the accuracy
improves, conﬁrming that minimizing the discrepancy for
target samples can result in accurate adaptation.

We visualized learned features as shown in Fig. 5(c) and
5(d). Our method did not match the distributions of source
and target completely as shown in Fig. 5(d). However,
the target samples seemed to be aligned with each class of
source samples. Although the target samples did not sep-
arate well in the non-adapted situation, they did separate
clearly as do source samples in the adapted situation.

4.3. Experiments on VisDA Classiﬁcation Dataset

We further evaluated our method on an object classiﬁca-
tion setting. The VisDA dataset [28] was used in this ex-
periment, which evaluated adaptation from synthetic-object
to real-object images. To date, this dataset represents the
largest for cross-domain object classiﬁcation, with over
280K images across 12 categories in the combined train-
ing, validation, and testing domains. The source images
were generated by rendering 3D models of the same ob-
ject categories as in the real data from different angles and
under different lighting conditions.
It contains 152,397
synthetic images. The validation images were collected
from MSCOCO [18] and they amount to 55,388 in total.
In our experiment, we considered the images of valida-
tion splits as the target domain and trained models in un-
supervised domain adaptation settings. We evaluate the
performance of ResNet101 [12] model pre-trained on Ima-
genet [6]. The ﬁnal fully-connected layer was removed and
all layers were updated with the same learning rate because
this dataset has abundant source and target samples. We re-
garded the pre-trained model as a generator network and we
used three-layered fully-connected networks for classiﬁca-
tion networks. The batch size was set to 32 and we used
10−3 to optimize the model.
SGD with learning rate 1.0
×
We report the accuracy after 10 epochs. The training de-
tails for baseline methods are written in our supplementary
material due to the limit of space.

Results Our method achieved an accuracy much better
than other distribution matching based methods (Table 2).
In addition, our method performed better than the source
only model in all classes, whereas MMD and DANN per-
form worse than the source only model in some classes such

as car and plant. We can clearly see the clear effective-
ness of our method in this regard. In this experiment, as the
value of n increase, the performance improved. We think
that it was because of the large domain difference between
synthetic objects and real images. The generator had to be
updated many times to align such distributions.

5. Experiments on Semantic Segmentation

We further applied our method to semantic segmenta-
tion. Considering a huge annotation cost for semantic seg-
mentation datasets, adaptation between different domains is
an important problem in semantic segmentation.

Implementation Detail We used the publicly available
synthetic dataset GTA5 [30] or Synthia [31] as the source
domain dataset and real dataset Cityscapes [5] as the tar-
get domain dataset. Following the work [13, 43],
the
Cityscapes validation set was used as our test set. As our
training set, the Cityscapes train set was used. During train-
ing, we randomly sampled just a single sample (setting the
batch size to 1 because of the GPU memory limit) from both
the images (and their labels) of the source dataset and the
remaining images of the target dataset but with no labels.

We applied our method to VGG-16 [34] based FCN-
8s [20] and DRN-D-105 [42] to evaluate our method. The
details of models, including their architecture and other
hyper-parameters, are described in the supplementary ma-
terial.

We used Momentum SGD to optimize our model and
set the momentum rate to 0.9 and the learning rate to
10−3 in all experiments. The image size was resized
1.0
×
to 1024
512. Here, we report the output of F1 after 50,000
×
iterations.

Results Table 3, Table 4, and Fig. 6 show quantitative
and qualitative results, respectively. These results illustrate
that even with a large domain difference between synthetic
to real images, our method is capable of improving the per-
formance. Considering the mIoU of the model trained only
on source samples, we can see the clear effectiveness of our
adaptation method. Also, compared to the score of DANN,
our method shows clearly better performance.

Figure 6. Qualitative results on adaptation from GTA5 to Cityscapes. DRN-105 is used to obtain these results.

Network method

VGG-16

Source Only
FCN Wld [13]
CDA (I) [43]
Ours (k=2)
Ours (k=3)
Ours (k=4)
Source Only
DANN [7]
Ours (k=2)
Ours (k=3)
Ours (k=4)

mIoU road
25.9
24.9
70.4
27.1
26.4
23.1
87.4
28.0
86.0
27.3
86.4
28.8
36.4
22.2
64.3
32.8
39.7
90.3
90.8
38.9
89.2
38.1

sdwk
10.9
32.4
10.8
15.4
10.5
8.5
14.2
23.2
31.0
35.6
23.2

bldng wall
3.3
50.5
14.9
62.1
10.2
69.7
17.4
75.5
20.0
75.1
18.6
76.1
16.4
67.4
11.3
73.4
19.7
78.5
80.5
22.9
23.6
80.2

fence
12.2
5.4
9.4
9.9
2.9
9.7
12.0
18.6
17.3
15.5
18.1

pole
25.4
10.9
20.2
16.2
19.4
14.9
20.1
29.0
28.6
27.5
27.7

light
28.6
14.2
13.6
11.9
8.4
7.8
8.7
31.8
30.9
24.9
25.0

sign
13.0
2.7
14.0
0.6
0.7
0.6
0.7
14.9
16.1
15.1
9.3

vgttn
78.3
79.2
56.9
80.6
78.4
82.8
69.8
82.0
83.7
84.2
84.4

trrn
7.3
21.3
2.8
28.1
19.4
32.7
13.3
16.8
30.0
31.8
34.6

sky
63.9
64.6
63.8
60.2
74.8
71.4
56.9
73.2
69.1
77.4
79.5

person
52.1
44.1
31.8
32.5
23.2
25.2
37.0
53.9
58.5
54.6
53.2

rider
7.9
4.2
10.6
0.9
0.3
1.1
0.4
12.4
19.6
17.2
16.0

car
66.3
70.4
60.5
75.4
74.1
76.3
53.6
53.3
81.5
82.0
84.1

truck
5.2
8.0
10.9
13.6
14.3
16.1
10.6
20.4
23.8
21.6
26.0

bus
7.8
7.3
3.4
4.8
10.4
17.1
3.2
11.0
30.0
29.0
22.5

train mcycl
13.7
0.9
3.5
0.0
10.9
3.8
0.7
0.1
0.1
0.2
0.2
1.4
0.9
0.2
18.7
5.0
25.7
5.7
21.8
1.3
16.7
5.2

bcycl
0.7
0.0
9.5
0.0
0.0
0.0
0.0
9.8
14.3
5.3
4.8

DRN-105

Table 3. Adaptation results on the semantic segmentation. We evaluate adaptation from GTA5 to Cityscapes dataset.

Network

VGG-16

DRN 105

method
Source Only [43]
FCN Wld [13]
CDA (I+SP) [43]
Source Only
DANN [7]
Ours (k=2)
Ours (k=3)
Ours (k=4)

mIoU road
5.6
22.0
11.5
20.2
65.2
29.0
14.9
23.4
67.0
32.5
83.5
36.3
37.3
84.8
88.1
37.2

sdwlk
11.2
19.6
26.1
11.4
29.1
40.9
43.6
43.2

bldng wall
0.8
59.6
4.4
30.8
0.1
74.9
1.9
58.7
14.3
71.5
6.0
77.6
79.0
3.9
2.4
79.1

fence
0.5
0.0
0.5
0.0
0.1
0.1
0.2
0.1

pole
21.5
20.3
10.7
24.1
28.1
27.9
29.1
27.3

light
8.0
0.1
3.7
1.2
12.6
6.2
7.2
7.4

sign
5.3
11.7
3.0
6.0
10.3
6.0
5.5
4.9

vgttn
72.4
42.3
76.1
68.8
72.7
83.1
83.8
83.4

sky
75.6
68.7
70.6
76.0
76.7
83.5
83.1
81.1

prsn
35.1
51.2
47.1
54.3
48.3
51.5
51.0
51.3

ridr
9.0
3.8
8.2
7.1
12.7
11.8
11.7
10.9

car
23.6
54.0
43.2
34.2
62.5
78.9
79.9
82.1

bus mcycl
4.5
3.2
20.7
15.0
11.3
19.8
27.2
29.0

0.5
0.2
0.7
0.8
2.7
4.6
6.2
5.7

bcycl
18.0
0.6
13.1
0.0
0.0
0.0
0.0
0.0

Table 4. Adaptation results on the semantic segmentation. We evaluate adaptation from Synthia to Cityscapes dataset.

6. Conclusion

In this paper, we proposed a new approach for UDA,
which utilizes task-speciﬁc classiﬁers to align distributions.
We propose to utilize task-speciﬁc classiﬁers as discrimi-
nators that try to detect target samples that are far from the
support of the source. A feature generator learns to generate
target features near the support to fool the classiﬁers. Since
the generator uses feedback from task-speciﬁc classiﬁers, it
will avoid generating target features near class boundaries.
We extensively evaluated our method on image classiﬁca-

tion and semantic segmentation datasets. In almost all ex-
periments, our method outperformed state-of-the-art meth-
ods. We provide the results when applying gradient reversal
layer [7] in the supplementary material, which enables to
update parameters of the model in one step.

7. Acknowledgements

The work was partially supported by CREST,
JST, and was partially funded by the ImPACT Pro-
and
gram of

for Science, Technology,

the Council

Innovation (Cabinet Ofﬁce, Government of

Japan).

[19] M.-Y. Liu and O. Tuzel. Coupled generative adversarial net-

References

[1] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira,
and J. W. Vaughan. A theory of learning from different do-
mains. Machine learning, 79(1-2):151–175, 2010. 4, 5
[2] S. Ben-David, J. Blitzer, K. Crammer, F. Pereira, et al. Anal-
ysis of representations for domain adaptation. In NIPS, 2007.
2, 4

[3] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Kr-
ishnan. Unsupervised pixel-level domain adaptation with
generative adversarial networks. In CVPR, 2017. 6, 10
[4] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and
D. Erhan. Domain separation networks. In NIPS, 2016. 1, 2,
5, 6

[5] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In CVPR, 2016. 7, 11

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 7

[7] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation
by backpropagation. In ICML, 2014. 6, 7, 8, 10, 11, 12
[8] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
F. Laviolette, M. Marchand, and V. Lempitsky. Domain-
adversarial training of neural networks. JMLR, 17(59):1–35,
2016. 1, 2, 5

[9] M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and
W. Li. Deep reconstruction-classiﬁcation networks for un-
supervised domain adaptation. In ECCV, 2016. 2, 6

[10] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets.
In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors,
NIPS, 2014. 2

[11] P. Haeusser, T. Frerix, A. Mordvintsev, and D. Cremers. As-

sociative domain adaptation. In ICCV, 2017. 6

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 7

[13] J. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the
wild: Pixel-level adversarial and constraint-based adapta-
tion. arXiv:1612.02649, 2016. 7, 8, 12

[14] J. J. Hull. A database for handwritten text recognition re-

search. PAMI, 16(5):550–554, 1994. 6

[15] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv:1412.6980, 2014. 6

[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1

[17] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998. 6

[18] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 7

Imagenet
In

works. In NIPS, 2016. 6

[20] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015. 7
[21] M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning trans-
In ICML,

ferable features with deep adaptation networks.
2015. 2, 6, 7, 10

[22] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised
domain adaptation with residual transfer networks. In NIPS,
2016. 2

[23] P. Luo, F. Zhuang, H. Xiong, Y. Xiong, and Q. He. Transfer
learning from multiple source domains via consensus regu-
larization. In CIKM, 2008. 2

[24] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.

JMLR, 9(11):2579–2605, 2008. 6

[25] B. Moiseev, A. Konev, A. Chigorin, and A. Konushin. Eval-
uation of trafﬁc sign recognition methods trained on synthet-
ically generated data. In ACIVS, 2013. 6

[26] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS workshop on deep learning and unsu-
pervised feature learning, 2011. 6

[27] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, et al. Scikit-learn: Machine learning in python.
JMLR, 12(10):2825–2830, 2011. 5

[28] X. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and
K. Saenko. Visda: The visual domain adaptation challenge.
arXiv:1710.06924, 2017. 7, 11

[29] S. Purushotham, W. Carvalho, T. Nilanon, and Y. Liu. Vari-
ational recurrent adversarial deep domain adaptation.
In
ICLR, 2017. 2, 5

[30] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for
data: Ground truth from computer games. In ECCV, 2016.
7, 11

[31] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M.
Lopez. The synthia dataset: A large collection of synthetic
images for semantic segmentation of urban scenes. In CVPR,
2016. 7, 11

[32] K. Saito, Y. Ushiku, and T. Harada. Asymmetric tri-training
for unsupervised domain adaptation. In ICML, 2017. 6
[33] O. Sener, H. O. Song, A. Saxena, and S. Savarese. Learning
transferrable representations for unsupervised domain adap-
tation. In NIPS, 2016. 2

[34] K. Simonyan and A. Zisserman.

Very deep con-
large-scale image recognition.

volutional networks for
arXiv:1409.1556, 2014. 7

[35] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The ger-
man trafﬁc sign recognition benchmark: a multi-class classi-
ﬁcation competition. In IJCNN, 2011. 6

[36] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy

domain adaptation. In AAAI, 2016. 2

[37] B. Sun and K. Saenko. Deep coral: Correlation alignment
for deep domain adaptation. In ECCV Workshops, 2016. 1,
2, 5

[38] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-

domain image generation. In ICLR, 2017. 2

[39] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial
discriminative domain adaptation. In CVPR, 2017. 6, 10
[40] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell.
Deep domain confusion: Maximizing for domain invariance.
arXiv:1412.3474, 2014. 1, 2, 5

[41] F. Yu and V. Koltun. Multi-scale context aggregation by di-

lated convolutions. In ICLR, 2016. 11

[42] F. Yu, V. Koltun, and T. Funkhouser. Dilated residual net-

works. In CVPR, 2017. 7, 11

[43] Y. Zhang, P. David, and B. Gong. Curriculum domain adap-
tation for semantic segmentation of urban scenes. In ICCV,
2017. 7, 8, 12

We would like to show supplementary information for
our main paper. First, we introduce the detail of the ex-
periments. Finally, we show some additional results of our
method.

We show the detail of experiments on toy dataset in main

Toy Dataset Experiment

paper.

Detail on experimental setting

The detail of experiment on toy dataset is shown in this
section. When generating target samples, we set the rota-
tion angle 30 in experiments of our main paper. We used
10−4 to optimizer the model.
Adam with learning rate 2.0
×
The batch size was set to 200. For a feature generator, we
used 3-layered fully-connected networks with 15 neurons in
hidden layer, in which ReLU is used as the activation func-
tion. For classiﬁers, we used three-layed fully-connected
networks with 15 neurons in hidden layer and 2 neurons
in output layer. The decision boundary shown in the main
paper is obtained when we rotate the source samples 30 de-
grees to generate target samples. We set n to 3 in this ex-
periment.

Experiment on Digit Dataset

We report the accuracy after training 20,000 iterations
except for the adaptation between MNIST and USPS. Due
to the lack of training samples of the datasets, we stopped
training after 200 epochs (13 iterations per one epoch)
to prevent over-ﬁtting. We followed the protocol pre-
sented by [7] in the following three adaptation scenarios.
SVHN
MNIST In this adaptation scenario, we used the
standard training set as training samples, and testing set as
testing samples both for source and target samples.

→

SYN DIGITS

SVHN We used 479400 source samples
and 73257 target samples for training, 26032 samples for
testing.

→

SYN SIGNS

GTSRB We randomly selected 31367
samples for target training and evaluated the accuracy on
the rest.

→

↔

MNIST

USPS In this setting, we followed the differ-
ent protocols provided by the paper, ADDA [39] and Pix-
elDA [3]. The former protocol provides the setting where a
part of training samples are utilized during training. 2,000
training samples are picked up for MNIST and 1,800 sam-
ples are used for USPS. The latter one allows to utilize all
training samples during training. We utilized the architec-
ture used as a classiﬁcation network in PixelDA [3]. We
added Batch Normalization layer to the architecture.

Experiment on VisDA Classiﬁcation Dataset

The detail of architecture we used and the detail of other

methods are shown in this section.

Class Balance Loss In addition to feature alignment
loss, we used a class balance loss to improve the accuracy in
this experiment. Please note that we incorporated this loss
in comparable methods too. We aimed to assign the target
samples to each classes equally. Without this loss, the target
samples can be aligned in an unbalanced way. The loss is
calculated as follows:

Ext∼Xt

K
(cid:88)

k=1

log p(y = k

xt)

|

(10)

The constant term λ = 0.01 was multiplied to the loss and
add this loss in Step 2 and Step 3 of our method. This loss
was also introduced in MMD and DANN too when updating
parameters of the networks.

For the fully-connected layers of classiﬁcation networks,
we set the number of neurons to 1000. In order to fairly
compare our method with others, we used the exact the
same architecture for other methods.

MMD We calculated the maximum mean discrepancy
(MMD) [21], namely the last layer of feature generator net-
works. We used RBF kernels to calculate the loss. We used
the the following standard deviation parameters:

σ = [0.1, 0.05, 0.01, 0.0001, 0.00001]

(11)

We changed the number of the kernels and their parame-
ters, but we could not observe signiﬁcant performance dif-
ference. We report the performance after 5 epochs. We
could not see any improvement after the epoch.

DANN To train a model ([7]), we used two-layered do-
main classiﬁcation networks. We set the number of neurons
in the hidden layer as 100. We also used Batch Normal-
ization, ReLU and dropout layer. Experimentally, we did
not see any improvement when the network architecture is
changed. According to the original method ([7]), learning
rate is decreased every iteration. However, in our experi-
ment, we could not see improvement, thus, we ﬁxed learn-
10−3. In addition, we did not introduce gra-
ing rate 1.0
dient reversal layer for our model. We separately update
discriminator and generator. We report the accuracy after 1
epoch.

×

Experiments on Semantic Segmentation

Additional Results

We describe the details of our experiments on semantic

Training via Gradient Reversal Layer

In our main paper, we provide the training procedure that
consists of three training steps and the number of updat-
ing generator (k) is a hyper-parameter in our method. We
found that introducing gradient reversal layer (GRL) [7] en-
ables to update our model in only one step and works well
in many settings. This improvement makes training faster
and deletes hyper-parameter in our method. We provide the
detail of the improvement and some experimental results
here.

Training Procedure We simply applied gradient rever-
sal layer when updating classiﬁers and generator in an ad-
versarial manner. The layer ﬂips the sign of gradients when
back-propagating the gradient. Therefore, update for maxi-
mizing the discrepancy via classiﬁer and minimizing it via
generator was conducted simultaneously. We publicize the
code with this implementation.

Results The experimental results on semantic segmenta-
tion are shown in Table 5,6, and Fig. 7. Our model with
GRL shows the same level of performance compared to the
model trained with our proposed training procedure.

Sensitivity to Hyper-Parameter

The number of updating generator

is the hyper-
parameter peculiar to our method. Therefore, we show ad-
ditional experimental results related to it. We employed the
adaptation from SVHN to MNIST and conducted experi-
ments where n = 5, 6. The accuracy was 96.0% and 96.2%
on average. The accuracy seems to increase as we increase
the value though it saturates. Training time required to ob-
tain high accuracy can increase too. However, considering
the results of GRL on semantic segmentation, the relation-
ship between the accuracy and the number of n seems to
depend on which datasets to adapt.

segmentation.

Details

Datasets GTA [30], Synthia [31] and Cityscapes [5]
are vehicle-egocentric image datasets but GTA and Synthia
are synthetic and Cityscapes is real world dataset. GTA
is collected from the open world in the realistically ren-
dered computer game Grand Theft Auto V (GTA, or GTA5).
It contains 24,996 images, whose semantic segmentation
annotations are fully compatible with the classes used in
Cityscapes. Cityscapes is collected in 50 cities in Germany
and nearby countries. We only used dense pixel-level anno-
tated dataset collected in 27 cities. It contains 2,975 training
set, 500 validation set, and 1525 test set. We used training
and validation set. Please note that the labels of Cityscapes
are just used for evaluation and never used in training. Sim-
ilarly, we used the training splits of Synthia dataset to train
our model.

Training Details When training, we ignored the pixel-
wise loss that is annotated backward (void). Therefore,
when testing, no predicted backward label existed. The
10−5 and we used no
weight decay ratio was set to 2
augmentation methods.

×

Network Architecture We applied our method to FCN-
8s based on VGG-16 network. Convolution layers in orig-
inal VGG-16 networks are used as generator and fully-
connected layers are used as classiﬁers. For DRN-D-105,
we followed the implementation of https://github.
com/fyu/drn. We applied our method to dilated residual
networks [41, 42] for base networks. We used DRN-D-105
model. We used the last convolution networks as classiﬁer
networks. All of lower layers are used as a generator.

TP

Evaluation Metrics As evaluation metrics, we use
intersection-over-union (IoU) and pixel accuracy. We use
the evaluation code1 released along with VisDA challenge
[28].
It calculates the PASCAL VOC intersection-over-
TP+FP+FN , where TP, FP, and FN are
union, i.e., IoU =
the numbers of true positive, false positive, and false nega-
tive pixels, respectively, determined over the whole test set.
For further discussing our result, we also compute pixel ac-
curacy, pixelAcc. = Σinii
, where nii denotes number of
Σiti
pixels of class i predicted to belong to class j and ti denotes
total number of pixels of class i in ground truth segmenta-
tion.

1https://github.com/VisionLearningGroup/taskcv-2017-

public/blob/master/segmentation/eval.py

Network method

VGG-16

Source Only
FCN Wld [13]
CDA (I) [43]
Ours (k=2)
Ours (k=3)
Ours (k=4)
Ours (GRL)
Source Only
DANN [7]
Ours (k=2)
Ours (k=3)
Ours (k=4)
Ours (GRL)

mIoU road
25.9
24.9
70.4
27.1
26.4
23.1
87.4
28.0
86.0
27.3
86.4
28.8
86.2
27.3
36.4
22.2
64.3
32.8
90.3
39.7
90.8
38.9
89.2
38.1
39.9
90.4

sdwk
10.9
32.4
10.8
15.4
10.5
8.5
16.1
14.2
23.2
31.0
35.6
23.2
34.5

bldng wall
3.3
50.5
14.9
62.1
10.2
69.7
17.4
75.5
20.0
75.1
18.6
76.1
20.7
74.4
16.4
67.4
11.3
73.4
19.7
78.5
80.5
22.9
23.6
80.2
20.4
79.3

fence
12.2
5.4
9.4
9.9
2.9
9.7
9.5
12.0
18.6
17.3
15.5
18.1
20.9

pole
25.4
10.9
20.2
16.2
19.4
14.9
21.5
20.1
29.0
28.6
27.5
27.7
33.1

light
28.6
14.2
13.6
11.9
8.4
7.8
14.8
8.7
31.8
30.9
24.9
25.0
28.3

sign
13.0
2.7
14.0
0.6
0.7
0.6
0.1
0.7
14.9
16.1
15.1
9.3
18.5

vgttn
78.3
79.2
56.9
80.6
78.4
82.8
80.4
69.8
82.0
83.7
84.2
84.4
82.4

trrn
7.3
21.3
2.8
28.1
19.4
32.7
27.8
13.3
16.8
30.0
31.8
34.6
22.6

sky
63.9
64.6
63.8
60.2
74.8
71.4
50.3
56.9
73.2
69.1
77.4
79.5
75.5

person
52.1
44.1
31.8
32.5
23.2
25.2
33.9
37.0
53.9
58.5
54.6
53.2
57.6

rider
7.9
4.2
10.6
0.9
0.3
1.1
1.2
0.4
12.4
19.6
17.2
16.0
18.6

car
66.3
70.4
60.5
75.4
74.1
76.3
67.6
53.6
53.3
81.5
82.0
84.1
82.7

truck
5.2
8.0
10.9
13.6
14.3
16.1
10.8
10.6
20.4
23.8
21.6
26.0
24.1

bus
7.8
7.3
3.4
4.8
10.4
17.1
3.0
3.2
11.0
30.0
29.0
22.5
25.6

train mcycl
13.7
0.9
3.5
0.0
10.9
3.8
0.7
0.1
0.1
0.2
0.2
1.4
0.9
0.2
0.9
0.2
18.7
5.0
25.7
5.7
21.8
1.3
16.7
5.2
23.9
7.6

bcycl
0.7
0.0
9.5
0.0
0.0
0.0
0.0
0.0
9.8
14.3
5.3
4.8
12.3

DRN-105

Table 5. Adaptation results on the semantic segmentation. We evaluate adaptation from GTA5 to Cityscapes dataset.

Network

VGG-16

DRN 105

method
Source Only [43]
FCN Wld [13]
CDA (I+SP) [43]
Source Only
DANN [7]
Ours (k=2)
Ours (k=3)
Ours (k=4)
Ours (GRL)

mIoU road
5.6
22.0
11.5
20.2
65.2
29.0
14.9
23.4
67.0
32.5
83.5
36.3
84.8
37.3
88.1
37.2
74.7
34.8

sdwlk
11.2
19.6
26.1
11.4
29.1
40.9
43.6
43.2
35.5

bldng wall
0.8
59.6
4.4
30.8
0.1
74.9
1.9
58.7
14.3
71.5
6.0
77.6
3.9
79.0
2.4
79.1
6.2
75.9

fence
0.5
0.0
0.5
0.0
0.1
0.1
0.2
0.1
0.1

pole
21.5
20.3
10.7
24.1
28.1
27.9
29.1
27.3
29.0

light
8.0
0.1
3.7
1.2
12.6
6.2
7.2
7.4
7.4

sign
5.3
11.7
3.0
6.0
10.3
6.0
5.5
4.9
6.1

vgttn
72.4
42.3
76.1
68.8
72.7
83.1
83.8
83.4
82.9

sky
75.6
68.7
70.6
76.0
76.7
83.5
83.1
81.1
83.4

prsn
35.1
51.2
47.1
54.3
48.3
51.5
51.0
51.3
47.8

ridr
9.0
3.8
8.2
7.1
12.7
11.8
11.7
10.9
9.2

car
23.6
54.0
43.2
34.2
62.5
78.9
79.9
82.1
71.7

bus mcycl
4.5
3.2
20.7
15.0
11.3
19.8
27.2
29.0
19.3

0.5
0.2
0.7
0.8
2.7
4.6
6.2
5.7
7.0

bcycl
18.0
0.6
13.1
0.0
0.0
0.0
0.0
0.0
0.0

Table 6. Adaptation results on the semantic segmentation. We evaluate adaptation from Synthia to Cityscapes dataset.

Figure 7. Qualitative results on adaptation from GTA5 to Cityscapes. From top to bottom, input, ground truth, result of source only model,
DANN, and our proposed method.

Maximum Classiﬁer Discrepancy for Unsupervised Domain Adaptation

Kuniaki Saito1, Kohei Watanabe1, Yoshitaka Ushiku1, and Tatsuya Harada1,2

1The University of Tokyo, 2RIKEN

k-saito,watanabe,ushiku,harada
}

{

@mi.t.u-tokyo.ac.jp

8
1
0
2
 
r
p
A
 
3
 
 
]

V
C
.
s
c
[
 
 
4
v
0
6
5
2
0
.
2
1
7
1
:
v
i
X
r
a

Abstract

In this work, we present a method for unsupervised do-
main adaptation. Many adversarial learning methods train
domain classiﬁer networks to distinguish the features as ei-
ther a source or target and train a feature generator net-
work to mimic the discriminator. Two problems exist with
these methods. First, the domain classiﬁer only tries to dis-
tinguish the features as a source or target and thus does not
consider task-speciﬁc decision boundaries between classes.
Therefore, a trained generator can generate ambiguous fea-
tures near class boundaries. Second, these methods aim to
completely match the feature distributions between different
domains, which is difﬁcult because of each domain’s char-
acteristics.

To solve these problems, we introduce a new approach
that attempts to align distributions of source and target by
utilizing the task-speciﬁc decision boundaries. We propose
to maximize the discrepancy between two classiﬁers’ out-
puts to detect target samples that are far from the sup-
port of the source. A feature generator learns to gener-
ate target features near the support to minimize the dis-
crepancy. Our method outperforms other methods on sev-
eral datasets of image classiﬁcation and semantic segmen-
tation. The codes are available at https://github.
com/mil-tokyo/MCD_DA

1. Introduction

The classiﬁcation accuracy of images has improved sub-
stantially with the advent of deep convolutional neural net-
works (CNN) which utilize numerous labeled samples [16].
However, collecting numerous labeled samples in various
domains is expensive and time-consuming.

Domain adaptation (DA) tackles this problem by trans-
ferring knowledge from a label-rich domain (i.e., source do-
main) to a label-scarce domain (i.e., target domain). DA
aims to train a classiﬁer using source samples that general-
ize well to the target domain. However, each domain’s sam-
ples have different characteristics, which makes the prob-

Figure 1. (Best viewed in color.) Comparison of previous and the
proposed distribution matching methods.. Left: Previous meth-
ods try to match different distributions by mimicing the domain
classiﬁer. They do not consider the decision boundary. Right:
Our proposed method attempts to detect target samples outside the
support of the source distribution using task-speciﬁc classiﬁers.

lem difﬁcult to solve. Consider neural networks trained on
labeled source images collected from the Web. Although
such neural networks perform well on the source images,
correctly recognizing target images collected from a real
camera is difﬁcult for them. This is because the target im-
ages can have different characteristics from the source im-
ages, such as change of light, noise, and angle in which
the image is captured. Furthermore, regarding unsupervised
DA (UDA), we have access to labeled source samples and
only unlabeled target samples. We must construct a model
that works well on target samples despite the absence of
their labels during training. UDA is the most challenging
situation, and we propose a method for UDA in this study.

Many UDA algorithms, particularly those for training
neural networks, attempt to match the distribution of the
source features with that of the target without considering
the category of the samples [8, 37, 4, 40]. In particular, do-
main classiﬁer-based adaptation algorithms have been ap-
plied to many tasks [8, 4]. The methods utilize two players
to align distributions in an adversarial manner: domain clas-
siﬁer (i.e., a discriminator) and feature generator. Source
and target samples are input to the same feature generator.

1

Features from the feature generator are shared by the dis-
criminator and a task-speciﬁc classiﬁer. The discriminator
is trained to discriminate the domain labels of the features
generated by the generator whereas the generator is trained
to fool it. The generator aims to match distributions be-
tween the source and target because such distributions will
mimic the discriminator. They assume that such target fea-
tures are classiﬁed correctly by the task-speciﬁc classiﬁer
because they are aligned with the source samples.

However, this method should fail to extract discrimi-
native features because it does not consider the relation-
ship between target samples and the task-speciﬁc decision
boundary when aligning distributions. As shown in the left
side of Fig. 1, the generator can generate ambiguous fea-
tures near the boundary because it simply tries to make the
two distributions similar.

To overcome both problems, we propose to align distri-
butions of features from source and target domain by using
the classiﬁer’s output for the target samples.

We introduce a new adversarial learning method that uti-
lizes two types of players:
task-speciﬁc classiﬁers and a
feature generator. task-speciﬁc classiﬁers denotes the clas-
siﬁers trained for each task such as object classiﬁcation or
semantic segmentation. Two classiﬁers take features from
the generator. Two classiﬁers try to classify source samples
correctly and, simultaneously, are trained to detect the tar-
get samples that are far from the support of the source. The
samples existing far from the support do not have discrimi-
native features because they are not clearly categorized into
some classes. Thus, our method utilizes the task-speciﬁc
classiﬁers as a discriminator. Generator tries to fool the
classiﬁers. In other words, it is trained to generate target
features near the support while considering classiﬁers’ out-
put for target samples. Thus, our method allows the gen-
erator to generate discriminative features for target samples
because it considers the relationship between the decision
boundary and target samples. This training is achieved in
an adversarial manner. In addition, please note that we do
not use domain labels in our method.

We evaluate our method on image recognition and se-
mantic segmentation. In many settings, our method outper-
forms other methods by a large margin. The contributions
of our paper are summarized as follows:

We propose a novel adversarial training method for do-
main adaptation that tries to align the distribution of
a target domain by considering task-speciﬁc decision
boundaries.

•

•

•

2. Related Work

Training CNN for DA can be realized through vari-
ous strategies. Ghifary et al. proposed using an autoen-
coder for the target domain to obtain domain-invariant fea-
tures [9]. Sener et al. proposed using clustering techniques
and pseudo-labels to obtain discriminative features [33].
Taigman et al. proposed cross-domain image translation
methods [38]. Matching distributions of the middle fea-
tures in CNN is considered to be effective in realizing an
accurate adaptation. To this end, numerous methods have
been proposed [8, 37, 4, 29, 40, 36].

The representative method of distribution matching in-
volves training a domain classiﬁer using the middle features
and generating the features that deceive the domain classi-
ﬁer [8]. This method utilizes the techniques used in gen-
erative adversarial networks [10]. The domain classiﬁer is
trained to predict the domain of each input, and the category
classiﬁer is trained to predict the task-speciﬁc category la-
bels. Feature extraction layers are shared by the two classi-
ﬁers. The layers are trained to correctly predict the label of
source samples as well as to deceive the domain classiﬁer.
Thus, the distributions of the middle features of the target
and source samples are made similar. Some methods utilize
maximum mean discrepancy (MMD) [22, 21], which can
be applied to measure the divergence in high-dimensional
space between different domains. This approach can train
the CNN to simultaneously minimize both the divergence
and category loss for the source domain. These methods
are based on the theory proposed by [2], which states that
the error on the target domain is bounded by the divergence
of the distributions. To our understanding, these distribu-
tion aligning methods using GAN or MMD do not con-
sider the relationship between target samples and decision
boundaries. To tackle these problems, we propose a novel
approach using task-speciﬁc classiﬁers as a discriminator.

Consensus regularization is a technique used in multi-
source domain adaptation and multi-view learning, in which
multiple classiﬁers are trained to maximize the consensus
of their outputs [23]. In our method, we address a training
step that minimizes the consensus of two classiﬁers, which
is totally different from consensus regularization. Consen-
sus regularization utilizes samples of multi-source domains
to construct different classiﬁers as in [23]. In order to con-
struct different classiﬁers, it relies on the different character-
istics of samples in different source domains. By contrast,
our method can construct different classiﬁers from only one
source domain.

We conﬁrm the behavior of our method through a toy
problem.

3. Method

We extensively evaluate our method on various tasks:
digit classiﬁcation, object classiﬁcation, and semantic
segmentation.

In this section, we present the detail of our proposed
method. First, we give the overall idea of our method in
Section 3.1. Second, we explain about the loss function we

Figure 2. (Best viewed in color.) Example of two classiﬁers with an overview of the proposed method. Discrepancy refers to the disagree-
ment between the predictions of two classiﬁers. First, we can see that the target samples outside the support of the source can be measured
by two different classiﬁers (Leftmost, Two different classiﬁers). Second, regarding the training procedure, we solve a minimax problem in
which we ﬁnd two classiﬁers that maximize the discrepancy on the target sample, and then generate features that minimize this discrepancy.

used in experiments in Section 3.2. Finally, we explain the
entire training procedure of our method in Section 3.3.

3.1. Overall Idea

}

We have access to a labeled source image xs and a corre-
sponding label ys drawn from a set of labeled source images
, as well as an unlabeled target image xt drawn
Xs, Ys
{
from unlabeled target images Xt. We train a feature gener-
ator network G, which takes inputs xs or xt, and classiﬁer
networks F1 and F2, which take features from G. F1 and
F2 classify them into K classes, that is, they output a K-
dimensional vector of logits. We obtain class probabilities
by applying the softmax function for the vector. We use
x) to denote the K-dimensional
the notation p1(y
|
probabilistic outputs for input x obtained by F1 and F2 re-
spectively.

x), p2(y

|

The goal of our method is to align source and target fea-
tures by utilizing the task-speciﬁc classiﬁers as a discrim-
inator in order to consider the relationship between class
boundaries and target samples. For this objective, we have
to detect target samples far from the support of the source.
The question is how to detect target samples far from the
support. These target samples are likely to be misclassi-
ﬁed by the classiﬁer learned from source samples because
they are near the class boundaries. Then, in order to de-
tect these target samples, we propose to utilize the disagree-
ment of the two classiﬁers on the prediction for target sam-
ples. Consider two classiﬁers (F1 and F2) that have dif-
ferent characteristics in the leftmost side of Fig. 2. We
assume that the two classiﬁers can classify source samples
correctly. This assumption is realistic because we have ac-
cess to labeled source samples in the setting of UDA. In ad-
dition, please note that F1 and F2 are initialized differently

to obtain different classiﬁers from the beginning of training.
Here, we have the key intuition that target samples outside
the support of the source are likely to be classiﬁed differ-
ently by the two distinct classiﬁers. This region is denoted
by black lines in the leftmost side of Fig. 2 (Discrepancy
Region). Conversely, if we can measure the disagreement
between the two classiﬁers and train the generator to mini-
mize the disagreement, the generator will avoid generating
target features outside the support of the source. Here, we
consider measuring the difference for a target sample using
the following equation, d(p1(y
xt)) where d de-
|
notes the function measuring divergence between two prob-
abilistic outputs. This term indicates how the two classiﬁers
disagree on their predictions and, hereafter, we call the term
as discrepancy. Our goal is to obtain a feature generator that
can minimize the discrepancy on target samples.

xt), p2(y

|

In order to effectively detect target samples outside the
support of the source, we propose to train discriminators
(F1 and F2) to maximize the discrepancy given target fea-
tures (Maximize Discrepancy in Fig. 2). Without this opera-
tion, the two classiﬁers can be very similar ones and cannot
detect target samples outside the support of the source. We
then train the generator to fool the discriminator, that is,
by minimizing the discrepancy (Minimize Discrepancy in
Fig. 2). This operation encourages the target samples to be
generated inside the support of the source. This adversarial
learning steps are repeated in our method. Our goal is to
obtain the features, in which the support of the target is in-
cluded by that of the source (Obtained Distributions in Fig.
2). We show the loss function used for discrepancy loss in
the next section. Then, we detail the training procedure.

K
(cid:88)

k=1

(Xs, Ys) =

L

E

−

(xs,ys)∼(Xs,Ys)

1l[k=ys] log p(y

xs)
|

(3)
Step B In this step, we train the classiﬁers (F1, F2) as a dis-
criminator for a ﬁxed generator (G). By training the classi-
ﬁers to increase the discrepancy, they can detect the target
samples excluded by the support of the source. This step
corresponds to Step B in Fig. 3. We add a classiﬁcation loss
on the source samples. Without this loss, we experimentally
found that our algorithm’s performance drops signiﬁcantly.
We use the same number of source and target samples to
update the model. The objective is as follows:

(4)

(5)

min
F1,F2 L

(Xs, Ys)

− Ladv(Xt).

Ladv(Xt) = Ext∼Xt[d(p1(y

|

xt), p2(y

xt))]
|

Step C We train the generator to minimize the discrepancy
for ﬁxed classiﬁers. This step corresponds to Step C in
Fig. 3. The number n indicates the number of times we re-
peat this for the same mini-batch. This number is a hyper-
parameter of our method. This term denotes the trade-off
between the generator and the classiﬁers. The objective is
as follows:

G Ladv(Xt).
min
These three steps are repeated in our method. To our un-
derstanding, the order of the three steps is not important.
Instead, our major concern is to train the classiﬁers and gen-
erator in an adversarial manner under the condition that they
can classify source samples correctly.

(6)

3.4. Theoretical Insight

Since our method is motivated by the theory proposed
by Ben-David et al. [1], we want to show the relationship
between our method and the theory in this section.

∆

H

-distance (dH∆H(

Ben-David et al. [1] proposed the theory that bounds the
expected error on the target samples, RT (h), by using three
terms: (i) expected error on the source domain, RS (h); (ii)
)), which is measured as the
H
discrepancy between two classiﬁers; and (iii) the shared er-
denote source
ror of the ideal joint hypothesis, λ.
and target domain respectively. Another theory [2] bounds
the error on the target domain, which introduced
-distance
(dH(
)) for domain divergence. The two theories and
their relationships can be explained as follows.

and

H

S

S

S

T

T

T

,

,

Theorem 1 Let H be the hypothesis class. Given two do-
mains

, we have

and

S

T

h

∀

∈

H, RT (h)

RS (h) +

dH∆H(

,

) + λ

S

T

(7)

≤

≤

RS (h) +

dH(

,

) + λ

S

T

1
2
1
2

Figure 3. Adversarial training steps of our method. We separate the
network into two modules: generator (G) and classiﬁers (F1 , F2 ).
The classiﬁers learn to maximize the discrepancy Step B on the
target samples, and the generator learns to minimize the discrep-
ancy Step C. Please note that we employ a training Step A to
ensure the discriminative features for source samples.

3.2. Discrepancy Loss

In this study, we utilize the absolute values of the dif-
ference between the two classiﬁers’ probabilistic outputs as
discrepancy loss:

d(p1, p2) =

1
K

K
(cid:88)

k=1

p1k −
|

,
p2k|

(1)

where the p1k and p2k denote probability output of p1 and
p2 for class k respectively. The choice for L1-distance is
based on the Theorem . Additionally, we experimentally
found that L2-distance does not work well.

3.3. Training Steps

To sum up the previous discussion in Section 3.1, we
need to train two classiﬁers, which take inputs from the gen-
erator and maximize d(p1(y
xt)), and the gener-
xt), p2(y
|
|
ator which tries to mimic the classiﬁers. Both the classiﬁers
and generator must classify source samples correctly. We
will show the manner in which to achieve this. We solve
this problem in three steps.

Step A First, we train both classiﬁers and generator to
classify the source samples correctly. In order to make clas-
siﬁers and generator obtain task-speciﬁc discriminative fea-
tures, this step is crucial. We train the networks to minimize
softmax cross entropy. The objective is as follows:

min
G,F1,F2 L

(Xs, Ys).

(2)

where

dH∆H(S, T ) = 2

(cid:12)
(cid:12)
E
(cid:12)
(cid:12)
x∼S

I(cid:2)h(x) (cid:54)= h

(x)(cid:3) − E

I(cid:2)h(x) (cid:54)= h

(cid:48)

x∼T

(cid:48)

(cid:12)
(cid:12)
(x)(cid:3)
(cid:12)
(cid:12)

sup
(h,h(cid:48) )∈H2
(cid:12)
(cid:12)
E
(cid:12)
(cid:12)
x∼S

dH(S, T ) = 2 sup
h∈H

I(cid:2)h(x) (cid:54)= 1(cid:3) − E

I(cid:2)h(x) (cid:54)= 1(cid:3)

,

x∼T

(cid:12)
(cid:12)
(cid:12)
(cid:12)

λ = min [RS (h) + RT (h)]

Here, RT (h) is the error of hypothesis h on the target do-
main, and RS (h) is the corresponding error on the source
domain. I[a] is the indicator function, which is 1 if predicate
a is true and 0 otherwise.

-distance is shown to be empirically measured by the er-
H
ror of the domain classiﬁer, which is trained to discrimi-
nate the domain of features. λ is a constant—the shared
error of the ideal joint hypothesis—which is considered suf-
ﬁciently low to achieve an accurate adaptation. Earlier stud-
ies [8, 37, 4, 29, 40] attempted to measure and minimize
-
H
distance in order to realize the adaptation. As this inequality
-distance. We
suggests,
-distance upper-bounds the
will show the relationship between our method and
-
distance.

H

H

H

H

H

∆

∆

(cid:48)

T

),

= h

I(cid:2)h(x)

Regarding dH∆H(

if we consider that h and
,
S
h(cid:48) can classify source samples correctly,
the term
(x)(cid:3) is assumed to be very low. h and
E
x∼S
h(cid:48) should agree on their predictions on source sam-
) is approximately calculated as
ples. Thus, dH∆H(
T
(x)(cid:3), which denotes the supremum
E
x∼T

sup
(h,h(cid:48))∈H2
of the expected disagreement of two classiﬁers’ predictions
on target samples.

,
S
= h

I(cid:2)h(x)

(cid:48)

We assume that h and h

share the feature extraction part.
Then, we decompose the hypothesis h into G and F1, and
(cid:48)
into G and F2. G, F1 and F2 correspond to the net-
h
work in our method. If we substitute these notations into
(x)(cid:3) and for ﬁxed G, the term
the

I(cid:2)h(x)

= h

(cid:48)

(cid:48)

sup
(h,h(cid:48))∈H2
will become

E
x∼T

(8)

(9)

sup
F1,F2

E
x∼T

I [F1 ◦

G(x)

= F2 ◦

G(x)].

Furthermore, if we replace sup with max and minimize the
term with respect to G, we obtain

min
G

max
F1,F2

E
x∼T

I [F1 ◦

G(x)

= F2 ◦

G(x)].

This equation is very similar to the mini-max problem we
solve in our method, in which classiﬁers are trained to maxi-
mize their discrepancy on target samples and generator tries
to minimize it. Although we must train all networks to min-
imize the classiﬁcation loss on source samples, we can see
the connection to the theory proposed by [1].
4. Experiments on Classiﬁcation

First, we observed the behavior of our model on toy
problem. Then, we performed an extensive evaluation of the
proposed methods on the following datasets: digits, trafﬁc
signs, and object classiﬁcation.

Comparison of three decision boundaries

(a) Source Only

(b) No Step C

(c) Proposed

Figure 4. (Best viewed in color.) Red and green points indicate
the source samples of class 0 and 1, respectively. Blue points are
target samples generated by rotating source samples. The dashed
and normal lines are two decision boundaries in our method. The
pink and light green regions are where the results of both classiﬁers
are class 0 and 1, respectively. Fig. 4(a) is the model trained
only on source samples. Fig. 4(b) is the model trained to increase
discrepancy of the two classiﬁers on target samples without using
Step C. Fig. 4(c) shows our proposed method.

4.1. Experiments on Toy Datasets

In the ﬁrst experiment, we observed the behavior of the
proposed method on inter twinning moons 2D problems, in
which we used scikit-learn [27] to generate the target sam-
ples by rotating the source samples. The goal of the ex-
periment was to observe the learned classiﬁers’ boundary.
For the source samples, we generated a lower moon and an
upper moon, labeled 0 and 1, respectively. Target samples
were generated by rotating the angle of the distribution of
the source samples. We generated 300 source and target
samples per class as the training samples. In this experi-
ment, we compared the decision boundary obtained from
our method with that obtained from both the model trained
only on source samples and from that trained only to in-
crease the discrepancy. In order to train the second compa-
rable model, we simply skipped Step C in Section 3.3 dur-
ing training. We tested the method on 1000 target samples
and visualized the learned decision boundary with source
and target samples. Other details including the network ar-
chitecture used in this experiment are provided in our sup-
plementary material. As we expected, when we trained
the two classiﬁers to increase the discrepancy on the target
samples, two classiﬁers largely disagreed on their predic-
tions on target samples (Fig. 4(b)). This is clear when com-
pared to the source only model (Fig. 4(a)). Two classiﬁers
were trained on the source samples without adaptation, and
the boundaries seemed to be nearly the same. Then, our
proposed method attempted to generate target samples that
reduce the discrepancy. Therefore, we could expect that the
two classiﬁers will be similar. Fig. 4(c) demonstrates the
assumption. The decision boundaries are drawn consider-
ing the target samples. The two classiﬁers output nearly the
same prediction for target samples, and they classiﬁed most
target samples correctly.

(a) SVHN to MNIST

(b) SYN SIGN to GTSRB

(c) Source Only

(d) Adapted (Ours)

Figure 5. (Best viewed in color.) Left: Relationship between discrepancy loss (blue line) and accuracy (red and green lines) during
training. As discrepancy loss decreased, accuracy improved. Right: Visualization of features obtained from last pooling layer of the
generator in adaptation from SYN SIGNS to GTSRB using t-SNE [24]. Red and blue points indicate the target and source samples,
respectively. All samples are testing samples. We can see that applying our method makes the target samples discriminative.

METHOD

Source Only

[21]
[7]

MMD
†
DANN
†
DSN
[4]
†
ADDA [39]
CoGAN [19]
PixelDA [3]
Ours (n = 2)
Ours (n = 3)
Ours (n = 4)

[32]

ATDA
ASSC [11]
DRCN [9]

†

to
GTSRB
85.1

SYNSIG MNIST MNIST*
to
USPS
76.7

SVHN
to
MNIST
67.1
Distribution Matching based Methods
71.1
71.1
82.7

to
USPS*
79.4

-
77.1

±
91.3

1.8

91.1
88.7
93.1
-
-
-
93.5
94.0
94.4

82.8
±
-

0.2
0.8

0.8
0.8
0.7

89.4
±
91.2
±
-
92.1
93.8
94.2
Other Methods
-
96.2
-

0.4
0.4
0.3

±
±
±

±
±
±

1.3

91.8

0.09

±

81.1
85.1
-
-
-
95.9

-
-
-

93.1
95.6
96.5

1.9
0.9
0.3

±
±
±

76.0
±
-
-
94.2
95.9
96.2

±
±
±

1.8

2.6
0.5
0.4

86.2

95.7
82.0

1.5
0.1

±
±

USPS
to
MNIST
63.4

-
73.0
±
-
90.1
±
89.1
±
-
90.0
91.8
94.1

±
±
±

0.2

0.8
0.8

1.4
0.9
0.3

-
-

±

73.7

0.04

Table 1. Results of the visual DA experiment on the digits and
trafﬁc signs datasets. The results are cited from each study. The
score of MMD is cited from DSN [4]. Please note that † means
that the method used a few labeled target samples as validation,
which is different from our setting. We repeated each experiment
5 times and report the average and the standard deviation of the
accuracy. The accuracy was obtained from classiﬁer F1. Including
the methods that used the labeled target samples for validation, our
method achieved good performance. MNIST* and USPS* mean
that we used all of the training samples to train the model.

4.2. Experiments on Digits Datasets

In this experiment, we evaluate the adaptation of the
model on three scenarios. The example datasets are pre-
sented in the supplementary material.

We assessed four types of adaptation scenarios by using
the digits datasets, namely MNIST [17], Street View House
Numbers (SVHN) [26], and USPS [14]. We further evalu-
ated our method on the trafﬁc sign datasets, Synthetic Traf-
ﬁc Signs (SYN SIGNS) [25] and the German Trafﬁc Signs
Recognition Benchmark [35] (GTSRB). In this experiment,
we employed the CNN architecture used in [7] and [3]. We
added batch normalization to each layer in these models.
We used Adam [15] to optimize our model and set the learn-

×

10−4 in all experiments. We set the batch
ing rate as 2.0
size to 128 in all experiments. The hyper-parameter peculiar
to our method was n, which denotes the number of times
we update the feature generator to mimic classiﬁers. We
varied the value of n from 2 to 4 in our experiment and ob-
served the sensitivity to the hyper-parameter. We followed
the protocol of unsupervised domain adaptation and did not
use validation samples to tune hyper-parameters. The other
details are provided in our supplementary material due to a
limit of space.
SVHN
SVHN [26] and MNIST [17] have distinct properties be-
cause SVHN datasets contain images with a colored back-
ground, multiple digits, and extremely blurred digits, mean-
ing that the domain divergence is very large between these
datasets.

MNIST

→

SYN SIGNS

GTSRB In this experiment, we evaluated
the adaptation from synthesized trafﬁc signs datasets (SYN
SIGNS dataset [7]) to real-world signs datasets (GTSRB
dataset [35]). These datasets contain 43 types of classes.

→

↔

MNIST

USPS We also evaluate our method on
MNIST and USPS datasets [17] to compare our method
with other methods. We followed the different protocols
provided by the paper, ADDA [39] and PixelDA [3].

Results Table 1 lists the accuracies for the target sam-
ples, and Fig. 5(a) and 5(b) show the relationship between
the discrepancy loss and accuracy during training. For the
source only model, we used the same network architecture
as used in our method. Details are provided in the sup-
plementary material. We extensively compared our meth-
ods with distribution matching-based methods as shown in
Table 1. The proposed method outperformed these meth-
ods in all settings. The performance improved as we in-
creased the value of n. Although other methods such as
ATDA [32] performed better than our method in some sit-
uations, the method utilized a few labeled target samples
to decide hyper-parameters for each dataset. The perfor-
mance of our method will improve too if we can choose the

Method
Source Only
MMD [21]
DANN [7]
Ours (n = 2)
Ours (n = 3)
Ours (n = 4)

plane
55.1
87.1
81.9
81.1
90.3
87.0

bcycl
53.3
63.0
77.7
55.3
49.3
60.9

bus
61.9
76.5
82.8
83.6
82.1
83.7

car
59.1
42.0
44.3
65.7
62.9
64.0

horse
80.6
90.3
81.2
87.6
91.8
88.9

knife mcycl
79.7
17.9
85.9
42.9
65.1
29.5
83.1
72.7
83.8
69.4
79.6
84.7

person
31.2
53.1
28.6
73.9
72.8
76.9

plant
81.0
49.7
51.9
85.3
79.8
88.6

sktbrd
26.5
36.3
54.6
47.7
53.3
40.3

train
73.5
85.8
82.8
73.2
81.5
83.0

truck mean
52.4
8.5
61.1
20.7
57.4
7.8
69.7
27.1
29.7
70.6
71.9
25.8

Table 2. Accuracy of ResNet101 model ﬁne-tuned on the VisDA dataset. The reported accuracy was obtained after 10 epoch updates.

best hyper-parameters for each dataset. As Fig. 5(a) and
5(b) show, as the discrepancy loss diminishes, the accuracy
improves, conﬁrming that minimizing the discrepancy for
target samples can result in accurate adaptation.

We visualized learned features as shown in Fig. 5(c) and
5(d). Our method did not match the distributions of source
and target completely as shown in Fig. 5(d). However,
the target samples seemed to be aligned with each class of
source samples. Although the target samples did not sep-
arate well in the non-adapted situation, they did separate
clearly as do source samples in the adapted situation.

4.3. Experiments on VisDA Classiﬁcation Dataset

We further evaluated our method on an object classiﬁca-
tion setting. The VisDA dataset [28] was used in this ex-
periment, which evaluated adaptation from synthetic-object
to real-object images. To date, this dataset represents the
largest for cross-domain object classiﬁcation, with over
280K images across 12 categories in the combined train-
ing, validation, and testing domains. The source images
were generated by rendering 3D models of the same ob-
ject categories as in the real data from different angles and
under different lighting conditions.
It contains 152,397
synthetic images. The validation images were collected
from MSCOCO [18] and they amount to 55,388 in total.
In our experiment, we considered the images of valida-
tion splits as the target domain and trained models in un-
supervised domain adaptation settings. We evaluate the
performance of ResNet101 [12] model pre-trained on Ima-
genet [6]. The ﬁnal fully-connected layer was removed and
all layers were updated with the same learning rate because
this dataset has abundant source and target samples. We re-
garded the pre-trained model as a generator network and we
used three-layered fully-connected networks for classiﬁca-
tion networks. The batch size was set to 32 and we used
10−3 to optimize the model.
SGD with learning rate 1.0
×
We report the accuracy after 10 epochs. The training de-
tails for baseline methods are written in our supplementary
material due to the limit of space.

Results Our method achieved an accuracy much better
than other distribution matching based methods (Table 2).
In addition, our method performed better than the source
only model in all classes, whereas MMD and DANN per-
form worse than the source only model in some classes such

as car and plant. We can clearly see the clear effective-
ness of our method in this regard. In this experiment, as the
value of n increase, the performance improved. We think
that it was because of the large domain difference between
synthetic objects and real images. The generator had to be
updated many times to align such distributions.

5. Experiments on Semantic Segmentation

We further applied our method to semantic segmenta-
tion. Considering a huge annotation cost for semantic seg-
mentation datasets, adaptation between different domains is
an important problem in semantic segmentation.

Implementation Detail We used the publicly available
synthetic dataset GTA5 [30] or Synthia [31] as the source
domain dataset and real dataset Cityscapes [5] as the tar-
get domain dataset. Following the work [13, 43],
the
Cityscapes validation set was used as our test set. As our
training set, the Cityscapes train set was used. During train-
ing, we randomly sampled just a single sample (setting the
batch size to 1 because of the GPU memory limit) from both
the images (and their labels) of the source dataset and the
remaining images of the target dataset but with no labels.

We applied our method to VGG-16 [34] based FCN-
8s [20] and DRN-D-105 [42] to evaluate our method. The
details of models, including their architecture and other
hyper-parameters, are described in the supplementary ma-
terial.

We used Momentum SGD to optimize our model and
set the momentum rate to 0.9 and the learning rate to
10−3 in all experiments. The image size was resized
1.0
×
to 1024
512. Here, we report the output of F1 after 50,000
×
iterations.

Results Table 3, Table 4, and Fig. 6 show quantitative
and qualitative results, respectively. These results illustrate
that even with a large domain difference between synthetic
to real images, our method is capable of improving the per-
formance. Considering the mIoU of the model trained only
on source samples, we can see the clear effectiveness of our
adaptation method. Also, compared to the score of DANN,
our method shows clearly better performance.

Figure 6. Qualitative results on adaptation from GTA5 to Cityscapes. DRN-105 is used to obtain these results.

Network method

VGG-16

Source Only
FCN Wld [13]
CDA (I) [43]
Ours (k=2)
Ours (k=3)
Ours (k=4)
Source Only
DANN [7]
Ours (k=2)
Ours (k=3)
Ours (k=4)

mIoU road
25.9
24.9
70.4
27.1
26.4
23.1
87.4
28.0
86.0
27.3
86.4
28.8
36.4
22.2
64.3
32.8
39.7
90.3
90.8
38.9
89.2
38.1

sdwk
10.9
32.4
10.8
15.4
10.5
8.5
14.2
23.2
31.0
35.6
23.2

bldng wall
3.3
50.5
14.9
62.1
10.2
69.7
17.4
75.5
20.0
75.1
18.6
76.1
16.4
67.4
11.3
73.4
19.7
78.5
80.5
22.9
23.6
80.2

fence
12.2
5.4
9.4
9.9
2.9
9.7
12.0
18.6
17.3
15.5
18.1

pole
25.4
10.9
20.2
16.2
19.4
14.9
20.1
29.0
28.6
27.5
27.7

light
28.6
14.2
13.6
11.9
8.4
7.8
8.7
31.8
30.9
24.9
25.0

sign
13.0
2.7
14.0
0.6
0.7
0.6
0.7
14.9
16.1
15.1
9.3

vgttn
78.3
79.2
56.9
80.6
78.4
82.8
69.8
82.0
83.7
84.2
84.4

trrn
7.3
21.3
2.8
28.1
19.4
32.7
13.3
16.8
30.0
31.8
34.6

sky
63.9
64.6
63.8
60.2
74.8
71.4
56.9
73.2
69.1
77.4
79.5

person
52.1
44.1
31.8
32.5
23.2
25.2
37.0
53.9
58.5
54.6
53.2

rider
7.9
4.2
10.6
0.9
0.3
1.1
0.4
12.4
19.6
17.2
16.0

car
66.3
70.4
60.5
75.4
74.1
76.3
53.6
53.3
81.5
82.0
84.1

truck
5.2
8.0
10.9
13.6
14.3
16.1
10.6
20.4
23.8
21.6
26.0

bus
7.8
7.3
3.4
4.8
10.4
17.1
3.2
11.0
30.0
29.0
22.5

train mcycl
13.7
0.9
3.5
0.0
10.9
3.8
0.7
0.1
0.1
0.2
0.2
1.4
0.9
0.2
18.7
5.0
25.7
5.7
21.8
1.3
16.7
5.2

bcycl
0.7
0.0
9.5
0.0
0.0
0.0
0.0
9.8
14.3
5.3
4.8

DRN-105

Table 3. Adaptation results on the semantic segmentation. We evaluate adaptation from GTA5 to Cityscapes dataset.

Network

VGG-16

DRN 105

method
Source Only [43]
FCN Wld [13]
CDA (I+SP) [43]
Source Only
DANN [7]
Ours (k=2)
Ours (k=3)
Ours (k=4)

mIoU road
5.6
22.0
11.5
20.2
65.2
29.0
14.9
23.4
67.0
32.5
83.5
36.3
37.3
84.8
88.1
37.2

sdwlk
11.2
19.6
26.1
11.4
29.1
40.9
43.6
43.2

bldng wall
0.8
59.6
4.4
30.8
0.1
74.9
1.9
58.7
14.3
71.5
6.0
77.6
79.0
3.9
2.4
79.1

fence
0.5
0.0
0.5
0.0
0.1
0.1
0.2
0.1

pole
21.5
20.3
10.7
24.1
28.1
27.9
29.1
27.3

light
8.0
0.1
3.7
1.2
12.6
6.2
7.2
7.4

sign
5.3
11.7
3.0
6.0
10.3
6.0
5.5
4.9

vgttn
72.4
42.3
76.1
68.8
72.7
83.1
83.8
83.4

sky
75.6
68.7
70.6
76.0
76.7
83.5
83.1
81.1

prsn
35.1
51.2
47.1
54.3
48.3
51.5
51.0
51.3

ridr
9.0
3.8
8.2
7.1
12.7
11.8
11.7
10.9

car
23.6
54.0
43.2
34.2
62.5
78.9
79.9
82.1

bus mcycl
4.5
3.2
20.7
15.0
11.3
19.8
27.2
29.0

0.5
0.2
0.7
0.8
2.7
4.6
6.2
5.7

bcycl
18.0
0.6
13.1
0.0
0.0
0.0
0.0
0.0

Table 4. Adaptation results on the semantic segmentation. We evaluate adaptation from Synthia to Cityscapes dataset.

6. Conclusion

In this paper, we proposed a new approach for UDA,
which utilizes task-speciﬁc classiﬁers to align distributions.
We propose to utilize task-speciﬁc classiﬁers as discrimi-
nators that try to detect target samples that are far from the
support of the source. A feature generator learns to generate
target features near the support to fool the classiﬁers. Since
the generator uses feedback from task-speciﬁc classiﬁers, it
will avoid generating target features near class boundaries.
We extensively evaluated our method on image classiﬁca-

tion and semantic segmentation datasets. In almost all ex-
periments, our method outperformed state-of-the-art meth-
ods. We provide the results when applying gradient reversal
layer [7] in the supplementary material, which enables to
update parameters of the model in one step.

7. Acknowledgements

The work was partially supported by CREST,
JST, and was partially funded by the ImPACT Pro-
and
gram of

for Science, Technology,

the Council

Innovation (Cabinet Ofﬁce, Government of

Japan).

[19] M.-Y. Liu and O. Tuzel. Coupled generative adversarial net-

References

[1] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira,
and J. W. Vaughan. A theory of learning from different do-
mains. Machine learning, 79(1-2):151–175, 2010. 4, 5
[2] S. Ben-David, J. Blitzer, K. Crammer, F. Pereira, et al. Anal-
ysis of representations for domain adaptation. In NIPS, 2007.
2, 4

[3] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Kr-
ishnan. Unsupervised pixel-level domain adaptation with
generative adversarial networks. In CVPR, 2017. 6, 10
[4] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and
D. Erhan. Domain separation networks. In NIPS, 2016. 1, 2,
5, 6

[5] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In CVPR, 2016. 7, 11

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 7

[7] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation
by backpropagation. In ICML, 2014. 6, 7, 8, 10, 11, 12
[8] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
F. Laviolette, M. Marchand, and V. Lempitsky. Domain-
adversarial training of neural networks. JMLR, 17(59):1–35,
2016. 1, 2, 5

[9] M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and
W. Li. Deep reconstruction-classiﬁcation networks for un-
supervised domain adaptation. In ECCV, 2016. 2, 6

[10] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets.
In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors,
NIPS, 2014. 2

[11] P. Haeusser, T. Frerix, A. Mordvintsev, and D. Cremers. As-

sociative domain adaptation. In ICCV, 2017. 6

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 7

[13] J. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the
wild: Pixel-level adversarial and constraint-based adapta-
tion. arXiv:1612.02649, 2016. 7, 8, 12

[14] J. J. Hull. A database for handwritten text recognition re-

search. PAMI, 16(5):550–554, 1994. 6

[15] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv:1412.6980, 2014. 6

[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1

[17] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998. 6

[18] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 7

Imagenet
In

works. In NIPS, 2016. 6

[20] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015. 7
[21] M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning trans-
In ICML,

ferable features with deep adaptation networks.
2015. 2, 6, 7, 10

[22] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised
domain adaptation with residual transfer networks. In NIPS,
2016. 2

[23] P. Luo, F. Zhuang, H. Xiong, Y. Xiong, and Q. He. Transfer
learning from multiple source domains via consensus regu-
larization. In CIKM, 2008. 2

[24] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.

JMLR, 9(11):2579–2605, 2008. 6

[25] B. Moiseev, A. Konev, A. Chigorin, and A. Konushin. Eval-
uation of trafﬁc sign recognition methods trained on synthet-
ically generated data. In ACIVS, 2013. 6

[26] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS workshop on deep learning and unsu-
pervised feature learning, 2011. 6

[27] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, et al. Scikit-learn: Machine learning in python.
JMLR, 12(10):2825–2830, 2011. 5

[28] X. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and
K. Saenko. Visda: The visual domain adaptation challenge.
arXiv:1710.06924, 2017. 7, 11

[29] S. Purushotham, W. Carvalho, T. Nilanon, and Y. Liu. Vari-
ational recurrent adversarial deep domain adaptation.
In
ICLR, 2017. 2, 5

[30] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for
data: Ground truth from computer games. In ECCV, 2016.
7, 11

[31] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M.
Lopez. The synthia dataset: A large collection of synthetic
images for semantic segmentation of urban scenes. In CVPR,
2016. 7, 11

[32] K. Saito, Y. Ushiku, and T. Harada. Asymmetric tri-training
for unsupervised domain adaptation. In ICML, 2017. 6
[33] O. Sener, H. O. Song, A. Saxena, and S. Savarese. Learning
transferrable representations for unsupervised domain adap-
tation. In NIPS, 2016. 2

[34] K. Simonyan and A. Zisserman.

Very deep con-
large-scale image recognition.

volutional networks for
arXiv:1409.1556, 2014. 7

[35] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The ger-
man trafﬁc sign recognition benchmark: a multi-class classi-
ﬁcation competition. In IJCNN, 2011. 6

[36] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy

domain adaptation. In AAAI, 2016. 2

[37] B. Sun and K. Saenko. Deep coral: Correlation alignment
for deep domain adaptation. In ECCV Workshops, 2016. 1,
2, 5

[38] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-

domain image generation. In ICLR, 2017. 2

[39] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial
discriminative domain adaptation. In CVPR, 2017. 6, 10
[40] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell.
Deep domain confusion: Maximizing for domain invariance.
arXiv:1412.3474, 2014. 1, 2, 5

[41] F. Yu and V. Koltun. Multi-scale context aggregation by di-

lated convolutions. In ICLR, 2016. 11

[42] F. Yu, V. Koltun, and T. Funkhouser. Dilated residual net-

works. In CVPR, 2017. 7, 11

[43] Y. Zhang, P. David, and B. Gong. Curriculum domain adap-
tation for semantic segmentation of urban scenes. In ICCV,
2017. 7, 8, 12

We would like to show supplementary information for
our main paper. First, we introduce the detail of the ex-
periments. Finally, we show some additional results of our
method.

We show the detail of experiments on toy dataset in main

Toy Dataset Experiment

paper.

Detail on experimental setting

The detail of experiment on toy dataset is shown in this
section. When generating target samples, we set the rota-
tion angle 30 in experiments of our main paper. We used
10−4 to optimizer the model.
Adam with learning rate 2.0
×
The batch size was set to 200. For a feature generator, we
used 3-layered fully-connected networks with 15 neurons in
hidden layer, in which ReLU is used as the activation func-
tion. For classiﬁers, we used three-layed fully-connected
networks with 15 neurons in hidden layer and 2 neurons
in output layer. The decision boundary shown in the main
paper is obtained when we rotate the source samples 30 de-
grees to generate target samples. We set n to 3 in this ex-
periment.

Experiment on Digit Dataset

We report the accuracy after training 20,000 iterations
except for the adaptation between MNIST and USPS. Due
to the lack of training samples of the datasets, we stopped
training after 200 epochs (13 iterations per one epoch)
to prevent over-ﬁtting. We followed the protocol pre-
sented by [7] in the following three adaptation scenarios.
SVHN
MNIST In this adaptation scenario, we used the
standard training set as training samples, and testing set as
testing samples both for source and target samples.

→

SYN DIGITS

SVHN We used 479400 source samples
and 73257 target samples for training, 26032 samples for
testing.

→

SYN SIGNS

GTSRB We randomly selected 31367
samples for target training and evaluated the accuracy on
the rest.

→

↔

MNIST

USPS In this setting, we followed the differ-
ent protocols provided by the paper, ADDA [39] and Pix-
elDA [3]. The former protocol provides the setting where a
part of training samples are utilized during training. 2,000
training samples are picked up for MNIST and 1,800 sam-
ples are used for USPS. The latter one allows to utilize all
training samples during training. We utilized the architec-
ture used as a classiﬁcation network in PixelDA [3]. We
added Batch Normalization layer to the architecture.

Experiment on VisDA Classiﬁcation Dataset

The detail of architecture we used and the detail of other

methods are shown in this section.

Class Balance Loss In addition to feature alignment
loss, we used a class balance loss to improve the accuracy in
this experiment. Please note that we incorporated this loss
in comparable methods too. We aimed to assign the target
samples to each classes equally. Without this loss, the target
samples can be aligned in an unbalanced way. The loss is
calculated as follows:

Ext∼Xt

K
(cid:88)

k=1

log p(y = k

xt)

|

(10)

The constant term λ = 0.01 was multiplied to the loss and
add this loss in Step 2 and Step 3 of our method. This loss
was also introduced in MMD and DANN too when updating
parameters of the networks.

For the fully-connected layers of classiﬁcation networks,
we set the number of neurons to 1000. In order to fairly
compare our method with others, we used the exact the
same architecture for other methods.

MMD We calculated the maximum mean discrepancy
(MMD) [21], namely the last layer of feature generator net-
works. We used RBF kernels to calculate the loss. We used
the the following standard deviation parameters:

σ = [0.1, 0.05, 0.01, 0.0001, 0.00001]

(11)

We changed the number of the kernels and their parame-
ters, but we could not observe signiﬁcant performance dif-
ference. We report the performance after 5 epochs. We
could not see any improvement after the epoch.

DANN To train a model ([7]), we used two-layered do-
main classiﬁcation networks. We set the number of neurons
in the hidden layer as 100. We also used Batch Normal-
ization, ReLU and dropout layer. Experimentally, we did
not see any improvement when the network architecture is
changed. According to the original method ([7]), learning
rate is decreased every iteration. However, in our experi-
ment, we could not see improvement, thus, we ﬁxed learn-
10−3. In addition, we did not introduce gra-
ing rate 1.0
dient reversal layer for our model. We separately update
discriminator and generator. We report the accuracy after 1
epoch.

×

Experiments on Semantic Segmentation

Additional Results

We describe the details of our experiments on semantic

Training via Gradient Reversal Layer

In our main paper, we provide the training procedure that
consists of three training steps and the number of updat-
ing generator (k) is a hyper-parameter in our method. We
found that introducing gradient reversal layer (GRL) [7] en-
ables to update our model in only one step and works well
in many settings. This improvement makes training faster
and deletes hyper-parameter in our method. We provide the
detail of the improvement and some experimental results
here.

Training Procedure We simply applied gradient rever-
sal layer when updating classiﬁers and generator in an ad-
versarial manner. The layer ﬂips the sign of gradients when
back-propagating the gradient. Therefore, update for maxi-
mizing the discrepancy via classiﬁer and minimizing it via
generator was conducted simultaneously. We publicize the
code with this implementation.

Results The experimental results on semantic segmenta-
tion are shown in Table 5,6, and Fig. 7. Our model with
GRL shows the same level of performance compared to the
model trained with our proposed training procedure.

Sensitivity to Hyper-Parameter

The number of updating generator

is the hyper-
parameter peculiar to our method. Therefore, we show ad-
ditional experimental results related to it. We employed the
adaptation from SVHN to MNIST and conducted experi-
ments where n = 5, 6. The accuracy was 96.0% and 96.2%
on average. The accuracy seems to increase as we increase
the value though it saturates. Training time required to ob-
tain high accuracy can increase too. However, considering
the results of GRL on semantic segmentation, the relation-
ship between the accuracy and the number of n seems to
depend on which datasets to adapt.

segmentation.

Details

Datasets GTA [30], Synthia [31] and Cityscapes [5]
are vehicle-egocentric image datasets but GTA and Synthia
are synthetic and Cityscapes is real world dataset. GTA
is collected from the open world in the realistically ren-
dered computer game Grand Theft Auto V (GTA, or GTA5).
It contains 24,996 images, whose semantic segmentation
annotations are fully compatible with the classes used in
Cityscapes. Cityscapes is collected in 50 cities in Germany
and nearby countries. We only used dense pixel-level anno-
tated dataset collected in 27 cities. It contains 2,975 training
set, 500 validation set, and 1525 test set. We used training
and validation set. Please note that the labels of Cityscapes
are just used for evaluation and never used in training. Sim-
ilarly, we used the training splits of Synthia dataset to train
our model.

Training Details When training, we ignored the pixel-
wise loss that is annotated backward (void). Therefore,
when testing, no predicted backward label existed. The
10−5 and we used no
weight decay ratio was set to 2
augmentation methods.

×

Network Architecture We applied our method to FCN-
8s based on VGG-16 network. Convolution layers in orig-
inal VGG-16 networks are used as generator and fully-
connected layers are used as classiﬁers. For DRN-D-105,
we followed the implementation of https://github.
com/fyu/drn. We applied our method to dilated residual
networks [41, 42] for base networks. We used DRN-D-105
model. We used the last convolution networks as classiﬁer
networks. All of lower layers are used as a generator.

TP

Evaluation Metrics As evaluation metrics, we use
intersection-over-union (IoU) and pixel accuracy. We use
the evaluation code1 released along with VisDA challenge
[28].
It calculates the PASCAL VOC intersection-over-
TP+FP+FN , where TP, FP, and FN are
union, i.e., IoU =
the numbers of true positive, false positive, and false nega-
tive pixels, respectively, determined over the whole test set.
For further discussing our result, we also compute pixel ac-
curacy, pixelAcc. = Σinii
, where nii denotes number of
Σiti
pixels of class i predicted to belong to class j and ti denotes
total number of pixels of class i in ground truth segmenta-
tion.

1https://github.com/VisionLearningGroup/taskcv-2017-

public/blob/master/segmentation/eval.py

Network method

VGG-16

Source Only
FCN Wld [13]
CDA (I) [43]
Ours (k=2)
Ours (k=3)
Ours (k=4)
Ours (GRL)
Source Only
DANN [7]
Ours (k=2)
Ours (k=3)
Ours (k=4)
Ours (GRL)

mIoU road
25.9
24.9
70.4
27.1
26.4
23.1
87.4
28.0
86.0
27.3
86.4
28.8
86.2
27.3
36.4
22.2
64.3
32.8
90.3
39.7
90.8
38.9
89.2
38.1
39.9
90.4

sdwk
10.9
32.4
10.8
15.4
10.5
8.5
16.1
14.2
23.2
31.0
35.6
23.2
34.5

bldng wall
3.3
50.5
14.9
62.1
10.2
69.7
17.4
75.5
20.0
75.1
18.6
76.1
20.7
74.4
16.4
67.4
11.3
73.4
19.7
78.5
80.5
22.9
23.6
80.2
20.4
79.3

fence
12.2
5.4
9.4
9.9
2.9
9.7
9.5
12.0
18.6
17.3
15.5
18.1
20.9

pole
25.4
10.9
20.2
16.2
19.4
14.9
21.5
20.1
29.0
28.6
27.5
27.7
33.1

light
28.6
14.2
13.6
11.9
8.4
7.8
14.8
8.7
31.8
30.9
24.9
25.0
28.3

sign
13.0
2.7
14.0
0.6
0.7
0.6
0.1
0.7
14.9
16.1
15.1
9.3
18.5

vgttn
78.3
79.2
56.9
80.6
78.4
82.8
80.4
69.8
82.0
83.7
84.2
84.4
82.4

trrn
7.3
21.3
2.8
28.1
19.4
32.7
27.8
13.3
16.8
30.0
31.8
34.6
22.6

sky
63.9
64.6
63.8
60.2
74.8
71.4
50.3
56.9
73.2
69.1
77.4
79.5
75.5

person
52.1
44.1
31.8
32.5
23.2
25.2
33.9
37.0
53.9
58.5
54.6
53.2
57.6

rider
7.9
4.2
10.6
0.9
0.3
1.1
1.2
0.4
12.4
19.6
17.2
16.0
18.6

car
66.3
70.4
60.5
75.4
74.1
76.3
67.6
53.6
53.3
81.5
82.0
84.1
82.7

truck
5.2
8.0
10.9
13.6
14.3
16.1
10.8
10.6
20.4
23.8
21.6
26.0
24.1

bus
7.8
7.3
3.4
4.8
10.4
17.1
3.0
3.2
11.0
30.0
29.0
22.5
25.6

train mcycl
13.7
0.9
3.5
0.0
10.9
3.8
0.7
0.1
0.1
0.2
0.2
1.4
0.9
0.2
0.9
0.2
18.7
5.0
25.7
5.7
21.8
1.3
16.7
5.2
23.9
7.6

bcycl
0.7
0.0
9.5
0.0
0.0
0.0
0.0
0.0
9.8
14.3
5.3
4.8
12.3

DRN-105

Table 5. Adaptation results on the semantic segmentation. We evaluate adaptation from GTA5 to Cityscapes dataset.

Network

VGG-16

DRN 105

method
Source Only [43]
FCN Wld [13]
CDA (I+SP) [43]
Source Only
DANN [7]
Ours (k=2)
Ours (k=3)
Ours (k=4)
Ours (GRL)

mIoU road
5.6
22.0
11.5
20.2
65.2
29.0
14.9
23.4
67.0
32.5
83.5
36.3
84.8
37.3
88.1
37.2
74.7
34.8

sdwlk
11.2
19.6
26.1
11.4
29.1
40.9
43.6
43.2
35.5

bldng wall
0.8
59.6
4.4
30.8
0.1
74.9
1.9
58.7
14.3
71.5
6.0
77.6
3.9
79.0
2.4
79.1
6.2
75.9

fence
0.5
0.0
0.5
0.0
0.1
0.1
0.2
0.1
0.1

pole
21.5
20.3
10.7
24.1
28.1
27.9
29.1
27.3
29.0

light
8.0
0.1
3.7
1.2
12.6
6.2
7.2
7.4
7.4

sign
5.3
11.7
3.0
6.0
10.3
6.0
5.5
4.9
6.1

vgttn
72.4
42.3
76.1
68.8
72.7
83.1
83.8
83.4
82.9

sky
75.6
68.7
70.6
76.0
76.7
83.5
83.1
81.1
83.4

prsn
35.1
51.2
47.1
54.3
48.3
51.5
51.0
51.3
47.8

ridr
9.0
3.8
8.2
7.1
12.7
11.8
11.7
10.9
9.2

car
23.6
54.0
43.2
34.2
62.5
78.9
79.9
82.1
71.7

bus mcycl
4.5
3.2
20.7
15.0
11.3
19.8
27.2
29.0
19.3

0.5
0.2
0.7
0.8
2.7
4.6
6.2
5.7
7.0

bcycl
18.0
0.6
13.1
0.0
0.0
0.0
0.0
0.0
0.0

Table 6. Adaptation results on the semantic segmentation. We evaluate adaptation from Synthia to Cityscapes dataset.

Figure 7. Qualitative results on adaptation from GTA5 to Cityscapes. From top to bottom, input, ground truth, result of source only model,
DANN, and our proposed method.

Maximum Classiﬁer Discrepancy for Unsupervised Domain Adaptation

Kuniaki Saito1, Kohei Watanabe1, Yoshitaka Ushiku1, and Tatsuya Harada1,2

1The University of Tokyo, 2RIKEN

k-saito,watanabe,ushiku,harada
}

{

@mi.t.u-tokyo.ac.jp

8
1
0
2
 
r
p
A
 
3
 
 
]

V
C
.
s
c
[
 
 
4
v
0
6
5
2
0
.
2
1
7
1
:
v
i
X
r
a

Abstract

In this work, we present a method for unsupervised do-
main adaptation. Many adversarial learning methods train
domain classiﬁer networks to distinguish the features as ei-
ther a source or target and train a feature generator net-
work to mimic the discriminator. Two problems exist with
these methods. First, the domain classiﬁer only tries to dis-
tinguish the features as a source or target and thus does not
consider task-speciﬁc decision boundaries between classes.
Therefore, a trained generator can generate ambiguous fea-
tures near class boundaries. Second, these methods aim to
completely match the feature distributions between different
domains, which is difﬁcult because of each domain’s char-
acteristics.

To solve these problems, we introduce a new approach
that attempts to align distributions of source and target by
utilizing the task-speciﬁc decision boundaries. We propose
to maximize the discrepancy between two classiﬁers’ out-
puts to detect target samples that are far from the sup-
port of the source. A feature generator learns to gener-
ate target features near the support to minimize the dis-
crepancy. Our method outperforms other methods on sev-
eral datasets of image classiﬁcation and semantic segmen-
tation. The codes are available at https://github.
com/mil-tokyo/MCD_DA

1. Introduction

The classiﬁcation accuracy of images has improved sub-
stantially with the advent of deep convolutional neural net-
works (CNN) which utilize numerous labeled samples [16].
However, collecting numerous labeled samples in various
domains is expensive and time-consuming.

Domain adaptation (DA) tackles this problem by trans-
ferring knowledge from a label-rich domain (i.e., source do-
main) to a label-scarce domain (i.e., target domain). DA
aims to train a classiﬁer using source samples that general-
ize well to the target domain. However, each domain’s sam-
ples have different characteristics, which makes the prob-

Figure 1. (Best viewed in color.) Comparison of previous and the
proposed distribution matching methods.. Left: Previous meth-
ods try to match different distributions by mimicing the domain
classiﬁer. They do not consider the decision boundary. Right:
Our proposed method attempts to detect target samples outside the
support of the source distribution using task-speciﬁc classiﬁers.

lem difﬁcult to solve. Consider neural networks trained on
labeled source images collected from the Web. Although
such neural networks perform well on the source images,
correctly recognizing target images collected from a real
camera is difﬁcult for them. This is because the target im-
ages can have different characteristics from the source im-
ages, such as change of light, noise, and angle in which
the image is captured. Furthermore, regarding unsupervised
DA (UDA), we have access to labeled source samples and
only unlabeled target samples. We must construct a model
that works well on target samples despite the absence of
their labels during training. UDA is the most challenging
situation, and we propose a method for UDA in this study.

Many UDA algorithms, particularly those for training
neural networks, attempt to match the distribution of the
source features with that of the target without considering
the category of the samples [8, 37, 4, 40]. In particular, do-
main classiﬁer-based adaptation algorithms have been ap-
plied to many tasks [8, 4]. The methods utilize two players
to align distributions in an adversarial manner: domain clas-
siﬁer (i.e., a discriminator) and feature generator. Source
and target samples are input to the same feature generator.

1

Features from the feature generator are shared by the dis-
criminator and a task-speciﬁc classiﬁer. The discriminator
is trained to discriminate the domain labels of the features
generated by the generator whereas the generator is trained
to fool it. The generator aims to match distributions be-
tween the source and target because such distributions will
mimic the discriminator. They assume that such target fea-
tures are classiﬁed correctly by the task-speciﬁc classiﬁer
because they are aligned with the source samples.

However, this method should fail to extract discrimi-
native features because it does not consider the relation-
ship between target samples and the task-speciﬁc decision
boundary when aligning distributions. As shown in the left
side of Fig. 1, the generator can generate ambiguous fea-
tures near the boundary because it simply tries to make the
two distributions similar.

To overcome both problems, we propose to align distri-
butions of features from source and target domain by using
the classiﬁer’s output for the target samples.

We introduce a new adversarial learning method that uti-
lizes two types of players:
task-speciﬁc classiﬁers and a
feature generator. task-speciﬁc classiﬁers denotes the clas-
siﬁers trained for each task such as object classiﬁcation or
semantic segmentation. Two classiﬁers take features from
the generator. Two classiﬁers try to classify source samples
correctly and, simultaneously, are trained to detect the tar-
get samples that are far from the support of the source. The
samples existing far from the support do not have discrimi-
native features because they are not clearly categorized into
some classes. Thus, our method utilizes the task-speciﬁc
classiﬁers as a discriminator. Generator tries to fool the
classiﬁers. In other words, it is trained to generate target
features near the support while considering classiﬁers’ out-
put for target samples. Thus, our method allows the gen-
erator to generate discriminative features for target samples
because it considers the relationship between the decision
boundary and target samples. This training is achieved in
an adversarial manner. In addition, please note that we do
not use domain labels in our method.

We evaluate our method on image recognition and se-
mantic segmentation. In many settings, our method outper-
forms other methods by a large margin. The contributions
of our paper are summarized as follows:

We propose a novel adversarial training method for do-
main adaptation that tries to align the distribution of
a target domain by considering task-speciﬁc decision
boundaries.

•

•

•

2. Related Work

Training CNN for DA can be realized through vari-
ous strategies. Ghifary et al. proposed using an autoen-
coder for the target domain to obtain domain-invariant fea-
tures [9]. Sener et al. proposed using clustering techniques
and pseudo-labels to obtain discriminative features [33].
Taigman et al. proposed cross-domain image translation
methods [38]. Matching distributions of the middle fea-
tures in CNN is considered to be effective in realizing an
accurate adaptation. To this end, numerous methods have
been proposed [8, 37, 4, 29, 40, 36].

The representative method of distribution matching in-
volves training a domain classiﬁer using the middle features
and generating the features that deceive the domain classi-
ﬁer [8]. This method utilizes the techniques used in gen-
erative adversarial networks [10]. The domain classiﬁer is
trained to predict the domain of each input, and the category
classiﬁer is trained to predict the task-speciﬁc category la-
bels. Feature extraction layers are shared by the two classi-
ﬁers. The layers are trained to correctly predict the label of
source samples as well as to deceive the domain classiﬁer.
Thus, the distributions of the middle features of the target
and source samples are made similar. Some methods utilize
maximum mean discrepancy (MMD) [22, 21], which can
be applied to measure the divergence in high-dimensional
space between different domains. This approach can train
the CNN to simultaneously minimize both the divergence
and category loss for the source domain. These methods
are based on the theory proposed by [2], which states that
the error on the target domain is bounded by the divergence
of the distributions. To our understanding, these distribu-
tion aligning methods using GAN or MMD do not con-
sider the relationship between target samples and decision
boundaries. To tackle these problems, we propose a novel
approach using task-speciﬁc classiﬁers as a discriminator.

Consensus regularization is a technique used in multi-
source domain adaptation and multi-view learning, in which
multiple classiﬁers are trained to maximize the consensus
of their outputs [23]. In our method, we address a training
step that minimizes the consensus of two classiﬁers, which
is totally different from consensus regularization. Consen-
sus regularization utilizes samples of multi-source domains
to construct different classiﬁers as in [23]. In order to con-
struct different classiﬁers, it relies on the different character-
istics of samples in different source domains. By contrast,
our method can construct different classiﬁers from only one
source domain.

We conﬁrm the behavior of our method through a toy
problem.

3. Method

We extensively evaluate our method on various tasks:
digit classiﬁcation, object classiﬁcation, and semantic
segmentation.

In this section, we present the detail of our proposed
method. First, we give the overall idea of our method in
Section 3.1. Second, we explain about the loss function we

Figure 2. (Best viewed in color.) Example of two classiﬁers with an overview of the proposed method. Discrepancy refers to the disagree-
ment between the predictions of two classiﬁers. First, we can see that the target samples outside the support of the source can be measured
by two different classiﬁers (Leftmost, Two different classiﬁers). Second, regarding the training procedure, we solve a minimax problem in
which we ﬁnd two classiﬁers that maximize the discrepancy on the target sample, and then generate features that minimize this discrepancy.

used in experiments in Section 3.2. Finally, we explain the
entire training procedure of our method in Section 3.3.

3.1. Overall Idea

}

We have access to a labeled source image xs and a corre-
sponding label ys drawn from a set of labeled source images
, as well as an unlabeled target image xt drawn
Xs, Ys
{
from unlabeled target images Xt. We train a feature gener-
ator network G, which takes inputs xs or xt, and classiﬁer
networks F1 and F2, which take features from G. F1 and
F2 classify them into K classes, that is, they output a K-
dimensional vector of logits. We obtain class probabilities
by applying the softmax function for the vector. We use
x) to denote the K-dimensional
the notation p1(y
|
probabilistic outputs for input x obtained by F1 and F2 re-
spectively.

x), p2(y

|

The goal of our method is to align source and target fea-
tures by utilizing the task-speciﬁc classiﬁers as a discrim-
inator in order to consider the relationship between class
boundaries and target samples. For this objective, we have
to detect target samples far from the support of the source.
The question is how to detect target samples far from the
support. These target samples are likely to be misclassi-
ﬁed by the classiﬁer learned from source samples because
they are near the class boundaries. Then, in order to de-
tect these target samples, we propose to utilize the disagree-
ment of the two classiﬁers on the prediction for target sam-
ples. Consider two classiﬁers (F1 and F2) that have dif-
ferent characteristics in the leftmost side of Fig. 2. We
assume that the two classiﬁers can classify source samples
correctly. This assumption is realistic because we have ac-
cess to labeled source samples in the setting of UDA. In ad-
dition, please note that F1 and F2 are initialized differently

to obtain different classiﬁers from the beginning of training.
Here, we have the key intuition that target samples outside
the support of the source are likely to be classiﬁed differ-
ently by the two distinct classiﬁers. This region is denoted
by black lines in the leftmost side of Fig. 2 (Discrepancy
Region). Conversely, if we can measure the disagreement
between the two classiﬁers and train the generator to mini-
mize the disagreement, the generator will avoid generating
target features outside the support of the source. Here, we
consider measuring the difference for a target sample using
the following equation, d(p1(y
xt)) where d de-
|
notes the function measuring divergence between two prob-
abilistic outputs. This term indicates how the two classiﬁers
disagree on their predictions and, hereafter, we call the term
as discrepancy. Our goal is to obtain a feature generator that
can minimize the discrepancy on target samples.

xt), p2(y

|

In order to effectively detect target samples outside the
support of the source, we propose to train discriminators
(F1 and F2) to maximize the discrepancy given target fea-
tures (Maximize Discrepancy in Fig. 2). Without this opera-
tion, the two classiﬁers can be very similar ones and cannot
detect target samples outside the support of the source. We
then train the generator to fool the discriminator, that is,
by minimizing the discrepancy (Minimize Discrepancy in
Fig. 2). This operation encourages the target samples to be
generated inside the support of the source. This adversarial
learning steps are repeated in our method. Our goal is to
obtain the features, in which the support of the target is in-
cluded by that of the source (Obtained Distributions in Fig.
2). We show the loss function used for discrepancy loss in
the next section. Then, we detail the training procedure.

K
(cid:88)

k=1

(Xs, Ys) =

L

E

−

(xs,ys)∼(Xs,Ys)

1l[k=ys] log p(y

xs)
|

(3)
Step B In this step, we train the classiﬁers (F1, F2) as a dis-
criminator for a ﬁxed generator (G). By training the classi-
ﬁers to increase the discrepancy, they can detect the target
samples excluded by the support of the source. This step
corresponds to Step B in Fig. 3. We add a classiﬁcation loss
on the source samples. Without this loss, we experimentally
found that our algorithm’s performance drops signiﬁcantly.
We use the same number of source and target samples to
update the model. The objective is as follows:

(4)

(5)

min
F1,F2 L

(Xs, Ys)

− Ladv(Xt).

Ladv(Xt) = Ext∼Xt[d(p1(y

|

xt), p2(y

xt))]
|

Step C We train the generator to minimize the discrepancy
for ﬁxed classiﬁers. This step corresponds to Step C in
Fig. 3. The number n indicates the number of times we re-
peat this for the same mini-batch. This number is a hyper-
parameter of our method. This term denotes the trade-off
between the generator and the classiﬁers. The objective is
as follows:

G Ladv(Xt).
min
These three steps are repeated in our method. To our un-
derstanding, the order of the three steps is not important.
Instead, our major concern is to train the classiﬁers and gen-
erator in an adversarial manner under the condition that they
can classify source samples correctly.

(6)

3.4. Theoretical Insight

Since our method is motivated by the theory proposed
by Ben-David et al. [1], we want to show the relationship
between our method and the theory in this section.

∆

H

-distance (dH∆H(

Ben-David et al. [1] proposed the theory that bounds the
expected error on the target samples, RT (h), by using three
terms: (i) expected error on the source domain, RS (h); (ii)
)), which is measured as the
H
discrepancy between two classiﬁers; and (iii) the shared er-
denote source
ror of the ideal joint hypothesis, λ.
and target domain respectively. Another theory [2] bounds
the error on the target domain, which introduced
-distance
(dH(
)) for domain divergence. The two theories and
their relationships can be explained as follows.

and

H

S

S

S

T

T

T

,

,

Theorem 1 Let H be the hypothesis class. Given two do-
mains

, we have

and

S

T

h

∀

∈

H, RT (h)

RS (h) +

dH∆H(

,

) + λ

S

T

(7)

≤

≤

RS (h) +

dH(

,

) + λ

S

T

1
2
1
2

Figure 3. Adversarial training steps of our method. We separate the
network into two modules: generator (G) and classiﬁers (F1 , F2 ).
The classiﬁers learn to maximize the discrepancy Step B on the
target samples, and the generator learns to minimize the discrep-
ancy Step C. Please note that we employ a training Step A to
ensure the discriminative features for source samples.

3.2. Discrepancy Loss

In this study, we utilize the absolute values of the dif-
ference between the two classiﬁers’ probabilistic outputs as
discrepancy loss:

d(p1, p2) =

1
K

K
(cid:88)

k=1

p1k −
|

,
p2k|

(1)

where the p1k and p2k denote probability output of p1 and
p2 for class k respectively. The choice for L1-distance is
based on the Theorem . Additionally, we experimentally
found that L2-distance does not work well.

3.3. Training Steps

To sum up the previous discussion in Section 3.1, we
need to train two classiﬁers, which take inputs from the gen-
erator and maximize d(p1(y
xt)), and the gener-
xt), p2(y
|
|
ator which tries to mimic the classiﬁers. Both the classiﬁers
and generator must classify source samples correctly. We
will show the manner in which to achieve this. We solve
this problem in three steps.

Step A First, we train both classiﬁers and generator to
classify the source samples correctly. In order to make clas-
siﬁers and generator obtain task-speciﬁc discriminative fea-
tures, this step is crucial. We train the networks to minimize
softmax cross entropy. The objective is as follows:

min
G,F1,F2 L

(Xs, Ys).

(2)

where

dH∆H(S, T ) = 2

(cid:12)
(cid:12)
E
(cid:12)
(cid:12)
x∼S

I(cid:2)h(x) (cid:54)= h

(x)(cid:3) − E

I(cid:2)h(x) (cid:54)= h

(cid:48)

x∼T

(cid:48)

(cid:12)
(cid:12)
(x)(cid:3)
(cid:12)
(cid:12)

sup
(h,h(cid:48) )∈H2
(cid:12)
(cid:12)
E
(cid:12)
(cid:12)
x∼S

dH(S, T ) = 2 sup
h∈H

I(cid:2)h(x) (cid:54)= 1(cid:3) − E

I(cid:2)h(x) (cid:54)= 1(cid:3)

,

x∼T

(cid:12)
(cid:12)
(cid:12)
(cid:12)

λ = min [RS (h) + RT (h)]

Here, RT (h) is the error of hypothesis h on the target do-
main, and RS (h) is the corresponding error on the source
domain. I[a] is the indicator function, which is 1 if predicate
a is true and 0 otherwise.

-distance is shown to be empirically measured by the er-
H
ror of the domain classiﬁer, which is trained to discrimi-
nate the domain of features. λ is a constant—the shared
error of the ideal joint hypothesis—which is considered suf-
ﬁciently low to achieve an accurate adaptation. Earlier stud-
ies [8, 37, 4, 29, 40] attempted to measure and minimize
-
H
distance in order to realize the adaptation. As this inequality
-distance. We
suggests,
-distance upper-bounds the
will show the relationship between our method and
-
distance.

H

H

H

H

H

∆

∆

(cid:48)

T

),

= h

I(cid:2)h(x)

Regarding dH∆H(

if we consider that h and
,
S
h(cid:48) can classify source samples correctly,
the term
(x)(cid:3) is assumed to be very low. h and
E
x∼S
h(cid:48) should agree on their predictions on source sam-
) is approximately calculated as
ples. Thus, dH∆H(
T
(x)(cid:3), which denotes the supremum
E
x∼T

sup
(h,h(cid:48))∈H2
of the expected disagreement of two classiﬁers’ predictions
on target samples.

,
S
= h

I(cid:2)h(x)

(cid:48)

We assume that h and h

share the feature extraction part.
Then, we decompose the hypothesis h into G and F1, and
(cid:48)
into G and F2. G, F1 and F2 correspond to the net-
h
work in our method. If we substitute these notations into
(x)(cid:3) and for ﬁxed G, the term
the

I(cid:2)h(x)

= h

(cid:48)

(cid:48)

sup
(h,h(cid:48))∈H2
will become

E
x∼T

(8)

(9)

sup
F1,F2

E
x∼T

I [F1 ◦

G(x)

= F2 ◦

G(x)].

Furthermore, if we replace sup with max and minimize the
term with respect to G, we obtain

min
G

max
F1,F2

E
x∼T

I [F1 ◦

G(x)

= F2 ◦

G(x)].

This equation is very similar to the mini-max problem we
solve in our method, in which classiﬁers are trained to maxi-
mize their discrepancy on target samples and generator tries
to minimize it. Although we must train all networks to min-
imize the classiﬁcation loss on source samples, we can see
the connection to the theory proposed by [1].
4. Experiments on Classiﬁcation

First, we observed the behavior of our model on toy
problem. Then, we performed an extensive evaluation of the
proposed methods on the following datasets: digits, trafﬁc
signs, and object classiﬁcation.

Comparison of three decision boundaries

(a) Source Only

(b) No Step C

(c) Proposed

Figure 4. (Best viewed in color.) Red and green points indicate
the source samples of class 0 and 1, respectively. Blue points are
target samples generated by rotating source samples. The dashed
and normal lines are two decision boundaries in our method. The
pink and light green regions are where the results of both classiﬁers
are class 0 and 1, respectively. Fig. 4(a) is the model trained
only on source samples. Fig. 4(b) is the model trained to increase
discrepancy of the two classiﬁers on target samples without using
Step C. Fig. 4(c) shows our proposed method.

4.1. Experiments on Toy Datasets

In the ﬁrst experiment, we observed the behavior of the
proposed method on inter twinning moons 2D problems, in
which we used scikit-learn [27] to generate the target sam-
ples by rotating the source samples. The goal of the ex-
periment was to observe the learned classiﬁers’ boundary.
For the source samples, we generated a lower moon and an
upper moon, labeled 0 and 1, respectively. Target samples
were generated by rotating the angle of the distribution of
the source samples. We generated 300 source and target
samples per class as the training samples. In this experi-
ment, we compared the decision boundary obtained from
our method with that obtained from both the model trained
only on source samples and from that trained only to in-
crease the discrepancy. In order to train the second compa-
rable model, we simply skipped Step C in Section 3.3 dur-
ing training. We tested the method on 1000 target samples
and visualized the learned decision boundary with source
and target samples. Other details including the network ar-
chitecture used in this experiment are provided in our sup-
plementary material. As we expected, when we trained
the two classiﬁers to increase the discrepancy on the target
samples, two classiﬁers largely disagreed on their predic-
tions on target samples (Fig. 4(b)). This is clear when com-
pared to the source only model (Fig. 4(a)). Two classiﬁers
were trained on the source samples without adaptation, and
the boundaries seemed to be nearly the same. Then, our
proposed method attempted to generate target samples that
reduce the discrepancy. Therefore, we could expect that the
two classiﬁers will be similar. Fig. 4(c) demonstrates the
assumption. The decision boundaries are drawn consider-
ing the target samples. The two classiﬁers output nearly the
same prediction for target samples, and they classiﬁed most
target samples correctly.

(a) SVHN to MNIST

(b) SYN SIGN to GTSRB

(c) Source Only

(d) Adapted (Ours)

Figure 5. (Best viewed in color.) Left: Relationship between discrepancy loss (blue line) and accuracy (red and green lines) during
training. As discrepancy loss decreased, accuracy improved. Right: Visualization of features obtained from last pooling layer of the
generator in adaptation from SYN SIGNS to GTSRB using t-SNE [24]. Red and blue points indicate the target and source samples,
respectively. All samples are testing samples. We can see that applying our method makes the target samples discriminative.

METHOD

Source Only

[21]
[7]

MMD
†
DANN
†
DSN
[4]
†
ADDA [39]
CoGAN [19]
PixelDA [3]
Ours (n = 2)
Ours (n = 3)
Ours (n = 4)

[32]

ATDA
ASSC [11]
DRCN [9]

†

to
GTSRB
85.1

SYNSIG MNIST MNIST*
to
USPS
76.7

SVHN
to
MNIST
67.1
Distribution Matching based Methods
71.1
71.1
82.7

to
USPS*
79.4

-
77.1

±
91.3

1.8

91.1
88.7
93.1
-
-
-
93.5
94.0
94.4

82.8
±
-

0.2
0.8

0.8
0.8
0.7

89.4
±
91.2
±
-
92.1
93.8
94.2
Other Methods
-
96.2
-

0.4
0.4
0.3

±
±
±

±
±
±

1.3

91.8

0.09

±

81.1
85.1
-
-
-
95.9

-
-
-

93.1
95.6
96.5

1.9
0.9
0.3

±
±
±

76.0
±
-
-
94.2
95.9
96.2

±
±
±

1.8

2.6
0.5
0.4

86.2

95.7
82.0

1.5
0.1

±
±

USPS
to
MNIST
63.4

-
73.0
±
-
90.1
±
89.1
±
-
90.0
91.8
94.1

±
±
±

0.2

0.8
0.8

1.4
0.9
0.3

-
-

±

73.7

0.04

Table 1. Results of the visual DA experiment on the digits and
trafﬁc signs datasets. The results are cited from each study. The
score of MMD is cited from DSN [4]. Please note that † means
that the method used a few labeled target samples as validation,
which is different from our setting. We repeated each experiment
5 times and report the average and the standard deviation of the
accuracy. The accuracy was obtained from classiﬁer F1. Including
the methods that used the labeled target samples for validation, our
method achieved good performance. MNIST* and USPS* mean
that we used all of the training samples to train the model.

4.2. Experiments on Digits Datasets

In this experiment, we evaluate the adaptation of the
model on three scenarios. The example datasets are pre-
sented in the supplementary material.

We assessed four types of adaptation scenarios by using
the digits datasets, namely MNIST [17], Street View House
Numbers (SVHN) [26], and USPS [14]. We further evalu-
ated our method on the trafﬁc sign datasets, Synthetic Traf-
ﬁc Signs (SYN SIGNS) [25] and the German Trafﬁc Signs
Recognition Benchmark [35] (GTSRB). In this experiment,
we employed the CNN architecture used in [7] and [3]. We
added batch normalization to each layer in these models.
We used Adam [15] to optimize our model and set the learn-

×

10−4 in all experiments. We set the batch
ing rate as 2.0
size to 128 in all experiments. The hyper-parameter peculiar
to our method was n, which denotes the number of times
we update the feature generator to mimic classiﬁers. We
varied the value of n from 2 to 4 in our experiment and ob-
served the sensitivity to the hyper-parameter. We followed
the protocol of unsupervised domain adaptation and did not
use validation samples to tune hyper-parameters. The other
details are provided in our supplementary material due to a
limit of space.
SVHN
SVHN [26] and MNIST [17] have distinct properties be-
cause SVHN datasets contain images with a colored back-
ground, multiple digits, and extremely blurred digits, mean-
ing that the domain divergence is very large between these
datasets.

MNIST

→

SYN SIGNS

GTSRB In this experiment, we evaluated
the adaptation from synthesized trafﬁc signs datasets (SYN
SIGNS dataset [7]) to real-world signs datasets (GTSRB
dataset [35]). These datasets contain 43 types of classes.

→

↔

MNIST

USPS We also evaluate our method on
MNIST and USPS datasets [17] to compare our method
with other methods. We followed the different protocols
provided by the paper, ADDA [39] and PixelDA [3].

Results Table 1 lists the accuracies for the target sam-
ples, and Fig. 5(a) and 5(b) show the relationship between
the discrepancy loss and accuracy during training. For the
source only model, we used the same network architecture
as used in our method. Details are provided in the sup-
plementary material. We extensively compared our meth-
ods with distribution matching-based methods as shown in
Table 1. The proposed method outperformed these meth-
ods in all settings. The performance improved as we in-
creased the value of n. Although other methods such as
ATDA [32] performed better than our method in some sit-
uations, the method utilized a few labeled target samples
to decide hyper-parameters for each dataset. The perfor-
mance of our method will improve too if we can choose the

Method
Source Only
MMD [21]
DANN [7]
Ours (n = 2)
Ours (n = 3)
Ours (n = 4)

plane
55.1
87.1
81.9
81.1
90.3
87.0

bcycl
53.3
63.0
77.7
55.3
49.3
60.9

bus
61.9
76.5
82.8
83.6
82.1
83.7

car
59.1
42.0
44.3
65.7
62.9
64.0

horse
80.6
90.3
81.2
87.6
91.8
88.9

knife mcycl
79.7
17.9
85.9
42.9
65.1
29.5
83.1
72.7
83.8
69.4
79.6
84.7

person
31.2
53.1
28.6
73.9
72.8
76.9

plant
81.0
49.7
51.9
85.3
79.8
88.6

sktbrd
26.5
36.3
54.6
47.7
53.3
40.3

train
73.5
85.8
82.8
73.2
81.5
83.0

truck mean
52.4
8.5
61.1
20.7
57.4
7.8
69.7
27.1
29.7
70.6
71.9
25.8

Table 2. Accuracy of ResNet101 model ﬁne-tuned on the VisDA dataset. The reported accuracy was obtained after 10 epoch updates.

best hyper-parameters for each dataset. As Fig. 5(a) and
5(b) show, as the discrepancy loss diminishes, the accuracy
improves, conﬁrming that minimizing the discrepancy for
target samples can result in accurate adaptation.

We visualized learned features as shown in Fig. 5(c) and
5(d). Our method did not match the distributions of source
and target completely as shown in Fig. 5(d). However,
the target samples seemed to be aligned with each class of
source samples. Although the target samples did not sep-
arate well in the non-adapted situation, they did separate
clearly as do source samples in the adapted situation.

4.3. Experiments on VisDA Classiﬁcation Dataset

We further evaluated our method on an object classiﬁca-
tion setting. The VisDA dataset [28] was used in this ex-
periment, which evaluated adaptation from synthetic-object
to real-object images. To date, this dataset represents the
largest for cross-domain object classiﬁcation, with over
280K images across 12 categories in the combined train-
ing, validation, and testing domains. The source images
were generated by rendering 3D models of the same ob-
ject categories as in the real data from different angles and
under different lighting conditions.
It contains 152,397
synthetic images. The validation images were collected
from MSCOCO [18] and they amount to 55,388 in total.
In our experiment, we considered the images of valida-
tion splits as the target domain and trained models in un-
supervised domain adaptation settings. We evaluate the
performance of ResNet101 [12] model pre-trained on Ima-
genet [6]. The ﬁnal fully-connected layer was removed and
all layers were updated with the same learning rate because
this dataset has abundant source and target samples. We re-
garded the pre-trained model as a generator network and we
used three-layered fully-connected networks for classiﬁca-
tion networks. The batch size was set to 32 and we used
10−3 to optimize the model.
SGD with learning rate 1.0
×
We report the accuracy after 10 epochs. The training de-
tails for baseline methods are written in our supplementary
material due to the limit of space.

Results Our method achieved an accuracy much better
than other distribution matching based methods (Table 2).
In addition, our method performed better than the source
only model in all classes, whereas MMD and DANN per-
form worse than the source only model in some classes such

as car and plant. We can clearly see the clear effective-
ness of our method in this regard. In this experiment, as the
value of n increase, the performance improved. We think
that it was because of the large domain difference between
synthetic objects and real images. The generator had to be
updated many times to align such distributions.

5. Experiments on Semantic Segmentation

We further applied our method to semantic segmenta-
tion. Considering a huge annotation cost for semantic seg-
mentation datasets, adaptation between different domains is
an important problem in semantic segmentation.

Implementation Detail We used the publicly available
synthetic dataset GTA5 [30] or Synthia [31] as the source
domain dataset and real dataset Cityscapes [5] as the tar-
get domain dataset. Following the work [13, 43],
the
Cityscapes validation set was used as our test set. As our
training set, the Cityscapes train set was used. During train-
ing, we randomly sampled just a single sample (setting the
batch size to 1 because of the GPU memory limit) from both
the images (and their labels) of the source dataset and the
remaining images of the target dataset but with no labels.

We applied our method to VGG-16 [34] based FCN-
8s [20] and DRN-D-105 [42] to evaluate our method. The
details of models, including their architecture and other
hyper-parameters, are described in the supplementary ma-
terial.

We used Momentum SGD to optimize our model and
set the momentum rate to 0.9 and the learning rate to
10−3 in all experiments. The image size was resized
1.0
×
to 1024
512. Here, we report the output of F1 after 50,000
×
iterations.

Results Table 3, Table 4, and Fig. 6 show quantitative
and qualitative results, respectively. These results illustrate
that even with a large domain difference between synthetic
to real images, our method is capable of improving the per-
formance. Considering the mIoU of the model trained only
on source samples, we can see the clear effectiveness of our
adaptation method. Also, compared to the score of DANN,
our method shows clearly better performance.

Figure 6. Qualitative results on adaptation from GTA5 to Cityscapes. DRN-105 is used to obtain these results.

Network method

VGG-16

Source Only
FCN Wld [13]
CDA (I) [43]
Ours (k=2)
Ours (k=3)
Ours (k=4)
Source Only
DANN [7]
Ours (k=2)
Ours (k=3)
Ours (k=4)

mIoU road
25.9
24.9
70.4
27.1
26.4
23.1
87.4
28.0
86.0
27.3
86.4
28.8
36.4
22.2
64.3
32.8
39.7
90.3
90.8
38.9
89.2
38.1

sdwk
10.9
32.4
10.8
15.4
10.5
8.5
14.2
23.2
31.0
35.6
23.2

bldng wall
3.3
50.5
14.9
62.1
10.2
69.7
17.4
75.5
20.0
75.1
18.6
76.1
16.4
67.4
11.3
73.4
19.7
78.5
80.5
22.9
23.6
80.2

fence
12.2
5.4
9.4
9.9
2.9
9.7
12.0
18.6
17.3
15.5
18.1

pole
25.4
10.9
20.2
16.2
19.4
14.9
20.1
29.0
28.6
27.5
27.7

light
28.6
14.2
13.6
11.9
8.4
7.8
8.7
31.8
30.9
24.9
25.0

sign
13.0
2.7
14.0
0.6
0.7
0.6
0.7
14.9
16.1
15.1
9.3

vgttn
78.3
79.2
56.9
80.6
78.4
82.8
69.8
82.0
83.7
84.2
84.4

trrn
7.3
21.3
2.8
28.1
19.4
32.7
13.3
16.8
30.0
31.8
34.6

sky
63.9
64.6
63.8
60.2
74.8
71.4
56.9
73.2
69.1
77.4
79.5

person
52.1
44.1
31.8
32.5
23.2
25.2
37.0
53.9
58.5
54.6
53.2

rider
7.9
4.2
10.6
0.9
0.3
1.1
0.4
12.4
19.6
17.2
16.0

car
66.3
70.4
60.5
75.4
74.1
76.3
53.6
53.3
81.5
82.0
84.1

truck
5.2
8.0
10.9
13.6
14.3
16.1
10.6
20.4
23.8
21.6
26.0

bus
7.8
7.3
3.4
4.8
10.4
17.1
3.2
11.0
30.0
29.0
22.5

train mcycl
13.7
0.9
3.5
0.0
10.9
3.8
0.7
0.1
0.1
0.2
0.2
1.4
0.9
0.2
18.7
5.0
25.7
5.7
21.8
1.3
16.7
5.2

bcycl
0.7
0.0
9.5
0.0
0.0
0.0
0.0
9.8
14.3
5.3
4.8

DRN-105

Table 3. Adaptation results on the semantic segmentation. We evaluate adaptation from GTA5 to Cityscapes dataset.

Network

VGG-16

DRN 105

method
Source Only [43]
FCN Wld [13]
CDA (I+SP) [43]
Source Only
DANN [7]
Ours (k=2)
Ours (k=3)
Ours (k=4)

mIoU road
5.6
22.0
11.5
20.2
65.2
29.0
14.9
23.4
67.0
32.5
83.5
36.3
37.3
84.8
88.1
37.2

sdwlk
11.2
19.6
26.1
11.4
29.1
40.9
43.6
43.2

bldng wall
0.8
59.6
4.4
30.8
0.1
74.9
1.9
58.7
14.3
71.5
6.0
77.6
79.0
3.9
2.4
79.1

fence
0.5
0.0
0.5
0.0
0.1
0.1
0.2
0.1

pole
21.5
20.3
10.7
24.1
28.1
27.9
29.1
27.3

light
8.0
0.1
3.7
1.2
12.6
6.2
7.2
7.4

sign
5.3
11.7
3.0
6.0
10.3
6.0
5.5
4.9

vgttn
72.4
42.3
76.1
68.8
72.7
83.1
83.8
83.4

sky
75.6
68.7
70.6
76.0
76.7
83.5
83.1
81.1

prsn
35.1
51.2
47.1
54.3
48.3
51.5
51.0
51.3

ridr
9.0
3.8
8.2
7.1
12.7
11.8
11.7
10.9

car
23.6
54.0
43.2
34.2
62.5
78.9
79.9
82.1

bus mcycl
4.5
3.2
20.7
15.0
11.3
19.8
27.2
29.0

0.5
0.2
0.7
0.8
2.7
4.6
6.2
5.7

bcycl
18.0
0.6
13.1
0.0
0.0
0.0
0.0
0.0

Table 4. Adaptation results on the semantic segmentation. We evaluate adaptation from Synthia to Cityscapes dataset.

6. Conclusion

In this paper, we proposed a new approach for UDA,
which utilizes task-speciﬁc classiﬁers to align distributions.
We propose to utilize task-speciﬁc classiﬁers as discrimi-
nators that try to detect target samples that are far from the
support of the source. A feature generator learns to generate
target features near the support to fool the classiﬁers. Since
the generator uses feedback from task-speciﬁc classiﬁers, it
will avoid generating target features near class boundaries.
We extensively evaluated our method on image classiﬁca-

tion and semantic segmentation datasets. In almost all ex-
periments, our method outperformed state-of-the-art meth-
ods. We provide the results when applying gradient reversal
layer [7] in the supplementary material, which enables to
update parameters of the model in one step.

7. Acknowledgements

The work was partially supported by CREST,
JST, and was partially funded by the ImPACT Pro-
and
gram of

for Science, Technology,

the Council

Innovation (Cabinet Ofﬁce, Government of

Japan).

[19] M.-Y. Liu and O. Tuzel. Coupled generative adversarial net-

References

[1] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira,
and J. W. Vaughan. A theory of learning from different do-
mains. Machine learning, 79(1-2):151–175, 2010. 4, 5
[2] S. Ben-David, J. Blitzer, K. Crammer, F. Pereira, et al. Anal-
ysis of representations for domain adaptation. In NIPS, 2007.
2, 4

[3] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Kr-
ishnan. Unsupervised pixel-level domain adaptation with
generative adversarial networks. In CVPR, 2017. 6, 10
[4] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and
D. Erhan. Domain separation networks. In NIPS, 2016. 1, 2,
5, 6

[5] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In CVPR, 2016. 7, 11

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 7

[7] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation
by backpropagation. In ICML, 2014. 6, 7, 8, 10, 11, 12
[8] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
F. Laviolette, M. Marchand, and V. Lempitsky. Domain-
adversarial training of neural networks. JMLR, 17(59):1–35,
2016. 1, 2, 5

[9] M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and
W. Li. Deep reconstruction-classiﬁcation networks for un-
supervised domain adaptation. In ECCV, 2016. 2, 6

[10] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets.
In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors,
NIPS, 2014. 2

[11] P. Haeusser, T. Frerix, A. Mordvintsev, and D. Cremers. As-

sociative domain adaptation. In ICCV, 2017. 6

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 7

[13] J. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the
wild: Pixel-level adversarial and constraint-based adapta-
tion. arXiv:1612.02649, 2016. 7, 8, 12

[14] J. J. Hull. A database for handwritten text recognition re-

search. PAMI, 16(5):550–554, 1994. 6

[15] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv:1412.6980, 2014. 6

[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1

[17] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998. 6

[18] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 7

Imagenet
In

works. In NIPS, 2016. 6

[20] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015. 7
[21] M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning trans-
In ICML,

ferable features with deep adaptation networks.
2015. 2, 6, 7, 10

[22] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised
domain adaptation with residual transfer networks. In NIPS,
2016. 2

[23] P. Luo, F. Zhuang, H. Xiong, Y. Xiong, and Q. He. Transfer
learning from multiple source domains via consensus regu-
larization. In CIKM, 2008. 2

[24] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.

JMLR, 9(11):2579–2605, 2008. 6

[25] B. Moiseev, A. Konev, A. Chigorin, and A. Konushin. Eval-
uation of trafﬁc sign recognition methods trained on synthet-
ically generated data. In ACIVS, 2013. 6

[26] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS workshop on deep learning and unsu-
pervised feature learning, 2011. 6

[27] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, et al. Scikit-learn: Machine learning in python.
JMLR, 12(10):2825–2830, 2011. 5

[28] X. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and
K. Saenko. Visda: The visual domain adaptation challenge.
arXiv:1710.06924, 2017. 7, 11

[29] S. Purushotham, W. Carvalho, T. Nilanon, and Y. Liu. Vari-
ational recurrent adversarial deep domain adaptation.
In
ICLR, 2017. 2, 5

[30] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for
data: Ground truth from computer games. In ECCV, 2016.
7, 11

[31] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M.
Lopez. The synthia dataset: A large collection of synthetic
images for semantic segmentation of urban scenes. In CVPR,
2016. 7, 11

[32] K. Saito, Y. Ushiku, and T. Harada. Asymmetric tri-training
for unsupervised domain adaptation. In ICML, 2017. 6
[33] O. Sener, H. O. Song, A. Saxena, and S. Savarese. Learning
transferrable representations for unsupervised domain adap-
tation. In NIPS, 2016. 2

[34] K. Simonyan and A. Zisserman.

Very deep con-
large-scale image recognition.

volutional networks for
arXiv:1409.1556, 2014. 7

[35] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The ger-
man trafﬁc sign recognition benchmark: a multi-class classi-
ﬁcation competition. In IJCNN, 2011. 6

[36] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy

domain adaptation. In AAAI, 2016. 2

[37] B. Sun and K. Saenko. Deep coral: Correlation alignment
for deep domain adaptation. In ECCV Workshops, 2016. 1,
2, 5

[38] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-

domain image generation. In ICLR, 2017. 2

[39] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial
discriminative domain adaptation. In CVPR, 2017. 6, 10
[40] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell.
Deep domain confusion: Maximizing for domain invariance.
arXiv:1412.3474, 2014. 1, 2, 5

[41] F. Yu and V. Koltun. Multi-scale context aggregation by di-

lated convolutions. In ICLR, 2016. 11

[42] F. Yu, V. Koltun, and T. Funkhouser. Dilated residual net-

works. In CVPR, 2017. 7, 11

[43] Y. Zhang, P. David, and B. Gong. Curriculum domain adap-
tation for semantic segmentation of urban scenes. In ICCV,
2017. 7, 8, 12

We would like to show supplementary information for
our main paper. First, we introduce the detail of the ex-
periments. Finally, we show some additional results of our
method.

We show the detail of experiments on toy dataset in main

Toy Dataset Experiment

paper.

Detail on experimental setting

The detail of experiment on toy dataset is shown in this
section. When generating target samples, we set the rota-
tion angle 30 in experiments of our main paper. We used
10−4 to optimizer the model.
Adam with learning rate 2.0
×
The batch size was set to 200. For a feature generator, we
used 3-layered fully-connected networks with 15 neurons in
hidden layer, in which ReLU is used as the activation func-
tion. For classiﬁers, we used three-layed fully-connected
networks with 15 neurons in hidden layer and 2 neurons
in output layer. The decision boundary shown in the main
paper is obtained when we rotate the source samples 30 de-
grees to generate target samples. We set n to 3 in this ex-
periment.

Experiment on Digit Dataset

We report the accuracy after training 20,000 iterations
except for the adaptation between MNIST and USPS. Due
to the lack of training samples of the datasets, we stopped
training after 200 epochs (13 iterations per one epoch)
to prevent over-ﬁtting. We followed the protocol pre-
sented by [7] in the following three adaptation scenarios.
SVHN
MNIST In this adaptation scenario, we used the
standard training set as training samples, and testing set as
testing samples both for source and target samples.

→

SYN DIGITS

SVHN We used 479400 source samples
and 73257 target samples for training, 26032 samples for
testing.

→

SYN SIGNS

GTSRB We randomly selected 31367
samples for target training and evaluated the accuracy on
the rest.

→

↔

MNIST

USPS In this setting, we followed the differ-
ent protocols provided by the paper, ADDA [39] and Pix-
elDA [3]. The former protocol provides the setting where a
part of training samples are utilized during training. 2,000
training samples are picked up for MNIST and 1,800 sam-
ples are used for USPS. The latter one allows to utilize all
training samples during training. We utilized the architec-
ture used as a classiﬁcation network in PixelDA [3]. We
added Batch Normalization layer to the architecture.

Experiment on VisDA Classiﬁcation Dataset

The detail of architecture we used and the detail of other

methods are shown in this section.

Class Balance Loss In addition to feature alignment
loss, we used a class balance loss to improve the accuracy in
this experiment. Please note that we incorporated this loss
in comparable methods too. We aimed to assign the target
samples to each classes equally. Without this loss, the target
samples can be aligned in an unbalanced way. The loss is
calculated as follows:

Ext∼Xt

K
(cid:88)

k=1

log p(y = k

xt)

|

(10)

The constant term λ = 0.01 was multiplied to the loss and
add this loss in Step 2 and Step 3 of our method. This loss
was also introduced in MMD and DANN too when updating
parameters of the networks.

For the fully-connected layers of classiﬁcation networks,
we set the number of neurons to 1000. In order to fairly
compare our method with others, we used the exact the
same architecture for other methods.

MMD We calculated the maximum mean discrepancy
(MMD) [21], namely the last layer of feature generator net-
works. We used RBF kernels to calculate the loss. We used
the the following standard deviation parameters:

σ = [0.1, 0.05, 0.01, 0.0001, 0.00001]

(11)

We changed the number of the kernels and their parame-
ters, but we could not observe signiﬁcant performance dif-
ference. We report the performance after 5 epochs. We
could not see any improvement after the epoch.

DANN To train a model ([7]), we used two-layered do-
main classiﬁcation networks. We set the number of neurons
in the hidden layer as 100. We also used Batch Normal-
ization, ReLU and dropout layer. Experimentally, we did
not see any improvement when the network architecture is
changed. According to the original method ([7]), learning
rate is decreased every iteration. However, in our experi-
ment, we could not see improvement, thus, we ﬁxed learn-
10−3. In addition, we did not introduce gra-
ing rate 1.0
dient reversal layer for our model. We separately update
discriminator and generator. We report the accuracy after 1
epoch.

×

Experiments on Semantic Segmentation

Additional Results

We describe the details of our experiments on semantic

Training via Gradient Reversal Layer

In our main paper, we provide the training procedure that
consists of three training steps and the number of updat-
ing generator (k) is a hyper-parameter in our method. We
found that introducing gradient reversal layer (GRL) [7] en-
ables to update our model in only one step and works well
in many settings. This improvement makes training faster
and deletes hyper-parameter in our method. We provide the
detail of the improvement and some experimental results
here.

Training Procedure We simply applied gradient rever-
sal layer when updating classiﬁers and generator in an ad-
versarial manner. The layer ﬂips the sign of gradients when
back-propagating the gradient. Therefore, update for maxi-
mizing the discrepancy via classiﬁer and minimizing it via
generator was conducted simultaneously. We publicize the
code with this implementation.

Results The experimental results on semantic segmenta-
tion are shown in Table 5,6, and Fig. 7. Our model with
GRL shows the same level of performance compared to the
model trained with our proposed training procedure.

Sensitivity to Hyper-Parameter

The number of updating generator

is the hyper-
parameter peculiar to our method. Therefore, we show ad-
ditional experimental results related to it. We employed the
adaptation from SVHN to MNIST and conducted experi-
ments where n = 5, 6. The accuracy was 96.0% and 96.2%
on average. The accuracy seems to increase as we increase
the value though it saturates. Training time required to ob-
tain high accuracy can increase too. However, considering
the results of GRL on semantic segmentation, the relation-
ship between the accuracy and the number of n seems to
depend on which datasets to adapt.

segmentation.

Details

Datasets GTA [30], Synthia [31] and Cityscapes [5]
are vehicle-egocentric image datasets but GTA and Synthia
are synthetic and Cityscapes is real world dataset. GTA
is collected from the open world in the realistically ren-
dered computer game Grand Theft Auto V (GTA, or GTA5).
It contains 24,996 images, whose semantic segmentation
annotations are fully compatible with the classes used in
Cityscapes. Cityscapes is collected in 50 cities in Germany
and nearby countries. We only used dense pixel-level anno-
tated dataset collected in 27 cities. It contains 2,975 training
set, 500 validation set, and 1525 test set. We used training
and validation set. Please note that the labels of Cityscapes
are just used for evaluation and never used in training. Sim-
ilarly, we used the training splits of Synthia dataset to train
our model.

Training Details When training, we ignored the pixel-
wise loss that is annotated backward (void). Therefore,
when testing, no predicted backward label existed. The
10−5 and we used no
weight decay ratio was set to 2
augmentation methods.

×

Network Architecture We applied our method to FCN-
8s based on VGG-16 network. Convolution layers in orig-
inal VGG-16 networks are used as generator and fully-
connected layers are used as classiﬁers. For DRN-D-105,
we followed the implementation of https://github.
com/fyu/drn. We applied our method to dilated residual
networks [41, 42] for base networks. We used DRN-D-105
model. We used the last convolution networks as classiﬁer
networks. All of lower layers are used as a generator.

TP

Evaluation Metrics As evaluation metrics, we use
intersection-over-union (IoU) and pixel accuracy. We use
the evaluation code1 released along with VisDA challenge
[28].
It calculates the PASCAL VOC intersection-over-
TP+FP+FN , where TP, FP, and FN are
union, i.e., IoU =
the numbers of true positive, false positive, and false nega-
tive pixels, respectively, determined over the whole test set.
For further discussing our result, we also compute pixel ac-
curacy, pixelAcc. = Σinii
, where nii denotes number of
Σiti
pixels of class i predicted to belong to class j and ti denotes
total number of pixels of class i in ground truth segmenta-
tion.

1https://github.com/VisionLearningGroup/taskcv-2017-

public/blob/master/segmentation/eval.py

Network method

VGG-16

Source Only
FCN Wld [13]
CDA (I) [43]
Ours (k=2)
Ours (k=3)
Ours (k=4)
Ours (GRL)
Source Only
DANN [7]
Ours (k=2)
Ours (k=3)
Ours (k=4)
Ours (GRL)

mIoU road
25.9
24.9
70.4
27.1
26.4
23.1
87.4
28.0
86.0
27.3
86.4
28.8
86.2
27.3
36.4
22.2
64.3
32.8
90.3
39.7
90.8
38.9
89.2
38.1
39.9
90.4

sdwk
10.9
32.4
10.8
15.4
10.5
8.5
16.1
14.2
23.2
31.0
35.6
23.2
34.5

bldng wall
3.3
50.5
14.9
62.1
10.2
69.7
17.4
75.5
20.0
75.1
18.6
76.1
20.7
74.4
16.4
67.4
11.3
73.4
19.7
78.5
80.5
22.9
23.6
80.2
20.4
79.3

fence
12.2
5.4
9.4
9.9
2.9
9.7
9.5
12.0
18.6
17.3
15.5
18.1
20.9

pole
25.4
10.9
20.2
16.2
19.4
14.9
21.5
20.1
29.0
28.6
27.5
27.7
33.1

light
28.6
14.2
13.6
11.9
8.4
7.8
14.8
8.7
31.8
30.9
24.9
25.0
28.3

sign
13.0
2.7
14.0
0.6
0.7
0.6
0.1
0.7
14.9
16.1
15.1
9.3
18.5

vgttn
78.3
79.2
56.9
80.6
78.4
82.8
80.4
69.8
82.0
83.7
84.2
84.4
82.4

trrn
7.3
21.3
2.8
28.1
19.4
32.7
27.8
13.3
16.8
30.0
31.8
34.6
22.6

sky
63.9
64.6
63.8
60.2
74.8
71.4
50.3
56.9
73.2
69.1
77.4
79.5
75.5

person
52.1
44.1
31.8
32.5
23.2
25.2
33.9
37.0
53.9
58.5
54.6
53.2
57.6

rider
7.9
4.2
10.6
0.9
0.3
1.1
1.2
0.4
12.4
19.6
17.2
16.0
18.6

car
66.3
70.4
60.5
75.4
74.1
76.3
67.6
53.6
53.3
81.5
82.0
84.1
82.7

truck
5.2
8.0
10.9
13.6
14.3
16.1
10.8
10.6
20.4
23.8
21.6
26.0
24.1

bus
7.8
7.3
3.4
4.8
10.4
17.1
3.0
3.2
11.0
30.0
29.0
22.5
25.6

train mcycl
13.7
0.9
3.5
0.0
10.9
3.8
0.7
0.1
0.1
0.2
0.2
1.4
0.9
0.2
0.9
0.2
18.7
5.0
25.7
5.7
21.8
1.3
16.7
5.2
23.9
7.6

bcycl
0.7
0.0
9.5
0.0
0.0
0.0
0.0
0.0
9.8
14.3
5.3
4.8
12.3

DRN-105

Table 5. Adaptation results on the semantic segmentation. We evaluate adaptation from GTA5 to Cityscapes dataset.

Network

VGG-16

DRN 105

method
Source Only [43]
FCN Wld [13]
CDA (I+SP) [43]
Source Only
DANN [7]
Ours (k=2)
Ours (k=3)
Ours (k=4)
Ours (GRL)

mIoU road
5.6
22.0
11.5
20.2
65.2
29.0
14.9
23.4
67.0
32.5
83.5
36.3
84.8
37.3
88.1
37.2
74.7
34.8

sdwlk
11.2
19.6
26.1
11.4
29.1
40.9
43.6
43.2
35.5

bldng wall
0.8
59.6
4.4
30.8
0.1
74.9
1.9
58.7
14.3
71.5
6.0
77.6
3.9
79.0
2.4
79.1
6.2
75.9

fence
0.5
0.0
0.5
0.0
0.1
0.1
0.2
0.1
0.1

pole
21.5
20.3
10.7
24.1
28.1
27.9
29.1
27.3
29.0

light
8.0
0.1
3.7
1.2
12.6
6.2
7.2
7.4
7.4

sign
5.3
11.7
3.0
6.0
10.3
6.0
5.5
4.9
6.1

vgttn
72.4
42.3
76.1
68.8
72.7
83.1
83.8
83.4
82.9

sky
75.6
68.7
70.6
76.0
76.7
83.5
83.1
81.1
83.4

prsn
35.1
51.2
47.1
54.3
48.3
51.5
51.0
51.3
47.8

ridr
9.0
3.8
8.2
7.1
12.7
11.8
11.7
10.9
9.2

car
23.6
54.0
43.2
34.2
62.5
78.9
79.9
82.1
71.7

bus mcycl
4.5
3.2
20.7
15.0
11.3
19.8
27.2
29.0
19.3

0.5
0.2
0.7
0.8
2.7
4.6
6.2
5.7
7.0

bcycl
18.0
0.6
13.1
0.0
0.0
0.0
0.0
0.0
0.0

Table 6. Adaptation results on the semantic segmentation. We evaluate adaptation from Synthia to Cityscapes dataset.

Figure 7. Qualitative results on adaptation from GTA5 to Cityscapes. From top to bottom, input, ground truth, result of source only model,
DANN, and our proposed method.

