On Difﬁculties of Cross-Lingual Transfer with Order Differences:
A Case Study on Dependency Parsing

Wasi Uddin Ahmad1∗, Zhisong Zhang2∗, Xuezhe Ma2
wasiahmad@cs.ucla.edu,{zhisongz,xuezhem}@cs.cmu.edu
Eduard Hovy2, Kai-Wei Chang1, Nanyun Peng3†
ehovy@cs.cmu.edu,kwchang@cs.ucla.edu,npeng@isi.edu

1University of California, Los Angeles, 2Carnegie Mellon University
3University of Southern California

Abstract

Different languages might have different word
In this paper, we investigate cross-
orders.
that an order-
transfer and posit
lingual
agnostic model will perform better when trans-
ferring to distant foreign languages. To test our
hypothesis, we train dependency parsers on an
English corpus and evaluate their transfer per-
formance on 30 other languages. Speciﬁcally,
we compare encoders and decoders based on
Recurrent Neural Networks (RNNs) and mod-
iﬁed self-attentive architectures. The former
relies on sequential information while the lat-
ter is more ﬂexible at modeling word order.
Rigorous experiments and detailed analysis
shows that RNN-based architectures transfer
well to languages that are close to English,
while self-attentive models have better overall
cross-lingual transferability and perform espe-
cially well on distant languages.

1

Introduction

transfer, which transfers models
Cross-lingual
across languages, has tremendous practical value.
It reduces the requirement of annotated data for a
target language and is especially useful when the
target language is lack of resources. Recently, this
technique has been applied to many NLP tasks
such as text categorization (Zhou et al., 2016a),
tagging (Kim et al., 2017), dependency parsing
(Guo et al., 2015, 2016) and machine translation
(Zoph et al., 2016). Despite the preliminary suc-
cess, transferring across languages is challenging
as it requires understanding and handling differ-
ences between languages at levels of morphology,
syntax, and semantics. It is especially difﬁcult to
learn invariant features that can robustly transfer
to distant languages.

∗Equal contribution. Listed by alphabetical order.
† Corresponding author.

Prior work on cross-lingual transfer mainly fo-
cused on sharing word-level information by lever-
aging multi-lingual word embeddings (Xiao and
Guo, 2014; Guo et al., 2016; Sil et al., 2018).
However, words are not independent in sentences;
their combinations form larger linguistic units,
known as context. Encoding context information
is vital for many NLP tasks, and a variety of ap-
proaches (e.g., convolutional neural networks and
recurrent neural networks) have been proposed to
encode context as a high-level feature for down-
In this paper, we study how to
stream tasks.
transfer generic contextual information across lan-
guages.

For cross-language transfer, one of the key chal-
lenges is the variation in word order among differ-
ent languages. For example, the Verb-Object pat-
tern in English can hardly be found in Japanese.
This challenge should be taken into considera-
tion in model design. RNN is a prevalent family
of models for many NLP tasks and has demon-
strated compelling performances (Mikolov et al.,
2010; Sutskever et al., 2014; Peters et al., 2018).
However, its sequential nature makes it heavily re-
liant on word order information, which exposes
to the risk of encoding language-speciﬁc order in-
formation that cannot generalize across languages.
We characterize this as the “order-sensitive” prop-
erty. Another family of models known as “Trans-
former” uses self-attention mechanisms to capture
context and was shown to be effective in various
NLP tasks (Vaswani et al., 2017; Liu et al., 2018;
Kitaev and Klein, 2018). With modiﬁcation in
position representations, the self-attention mecha-
nism can be more robust than RNNs to the change
of word order. We refer to this as the “order-free”
property.

In this work, we posit that order-free mod-
els have better transferability than order-sensitive
models because they less suffer from overﬁtting

9
1
0
2
 
r
p
A
 
7
1
 
 
]
L
C
.
s
c
[
 
 
3
v
0
7
5
0
0
.
1
1
8
1
:
v
i
X
r
a

Language
Families
Afro-Asiatic
Austronesian
IE.Baltic
IE.Germanic

IE.Indic
IE.Latin
IE.Romance

IE.Slavic

Japanese
Korean
Sino-Tibetan
Uralic

Languages

Arabic (ar), Hebrew (he)
Indonesian (id)
Latvian (lv)
Danish (da), Dutch (nl), English (en),
German (de), Norwegian (no),
Swedish (sv)
Hindi (hi)
Latin (la)
Catalan (ca), French (fr), Italian (it),
Portuguese (pt), Romanian (ro),
Spanish (es)
Bulgarian (bg), Croatian (hr), Czech
(cs), Polish (pl), Russian (ru), Slovak
(sk), Slovenian (sl), Ukrainian (uk)
Japanese (ja)
Korean (ko)
Chinese (zh)
Estonian (et), Finnish (ﬁ)

Table 1: The selected languages grouped by language
families. “IE” is the abbreviation of Indo-European.

language-speciﬁc word order features. To test
our hypothesis, we ﬁrst quantify language dis-
tance in terms of word order typology, and then
systematically study the transferability of order-
sensitive and order-free neural architectures on
cross-lingual dependency parsing.

We use dependency parsing as a test bed pri-
marily because of the availability of uniﬁed an-
notations across a broad spectrum of languages
(Nivre et al., 2018). Besides, word order typology
is found to inﬂuence dependency parsing (Naseem
et al., 2012; T¨ackstr¨om et al., 2013; Zhang and
Barzilay, 2015; Ammar et al., 2016; Aufrant et al.,
2016). Moreover, parsing is a low-level NLP task
(Hashimoto et al., 2017) that can beneﬁt many
downstream applications (McClosky et al., 2011;
Gamallo et al., 2012; Jie et al., 2017).

We conduct evaluations on 31 languages across
a broad spectrum of language families, as shown
in Table 1. Our empirical results show that order-
free encoding and decoding models generally per-
form better than the order-sensitive ones for cross-
lingual transfer, especially when the source and
target languages are distant.

2 Quantifying Language Distance

We ﬁrst verify that we can measure “language dis-
tance” base on word order since it is a signiﬁ-
cant distinctive feature to differentiate languages
(Dryer, 2007). The World Atlas of Language
Structures (WALS) (Dryer and Haspelmath, 2013)
provides a great reference for word order typology

Figure 1: Hierarchical clustering (with the Nearest
Point Algorithm) dendrogram of the languages by their
word-ordering vectors.

and can be used to construct feature vectors for
languages (Littell et al., 2017). But since we al-
ready have the universal dependency annotations,
we take an empirical way and directly extract word
order features using directed dependency relations
(Liu, 2010).

We conduct our study using the Universal De-
pendencies (UD) Treebanks (v2.2) (Nivre et al.,
2018). We select 31 languages for evaluation and
analysis, with the selection criterion being that the
total token number in the treebanks of that lan-
guage is over 100K. We group these languages by
their language families in Table 1. Detailed sta-
tistical information of the selected languages and
treebanks can be found in Appendix A1.

We look at ﬁner-grained dependency types than
the 37 universal dependency labels2 in UD v2
by augmenting the dependency labels with the
universal part-of-speech (POS) tags of the head
and modiﬁer3 nodes. Speciﬁcally, we use triples
“(ModiﬁerPOS, HeadPOS, DependencyLabel)” as
the augmented dependency types. With this, we
can investigate language differences in a ﬁne-
grained way by deﬁning directions on these triples
(i.e. modiﬁer before head or modiﬁer after head).
We conduct feature selection by ﬁltering out
rare types as they can be unstable. We defer the
results in 52 selected types and more details to Ap-
pendix C. For each dependency type, we collect
the statistics of directionality (Liu, 2010; Wang
and Eisner, 2017). Since there can be only two
directions for an edge, for each dependency type,

1Please refer to the supplementary materials for all the

appendices of this paper.

2http://universaldependencies.org/u/dep/index.html
3In this paper, we use the term of “modiﬁer”, which can

also be described as “dependent” or “child” node.

we use the relative frequency of the left-direction
(modiﬁer before head) as the directional feature.
By concatenating the directional features of all se-
lected triples, we obtain a word-ordering feature
vector for each language. We calculate the word-
ordering distance using these vectors.
In this
work, we simply use Manhattan distance, which
works well as shown in our analysis (Section 4.3).
We perform hierarchical clustering based on the
word-ordering vectors for the selected languages,
following ¨Ostling (2015). As shown in Figure 1,
the grouping of the ground truth language fami-
lies is almost recovered. The two outliers, Ger-
man (de) and Dutch (nl), are indeed different from
English. For instance, German and Dutch adopt
a larger portion of Object-Verb order in embedded
clauses. The above analysis shows that word order
is an important feature to characterize differences
between languages. Therefore, it should be taken
into consideration in the model design.

3 Models

Our primary goal is to conduct cross-lingual trans-
fer of syntactic dependencies without providing
any annotation in the target languages. The over-
all architecture of models that are studied in this
research is described as follows. The ﬁrst layer
is an input embedding layer, for which we simply
concatenate word and POS embeddings. The POS
embeddings are trained from scratch, while the
word embeddings are ﬁxed and initialized with the
multilingual embeddings by Smith et al. (2017).
These inputs are fed to the encoder to get contex-
tual representations, which is further used by the
decoder for predicting parse trees.

For the cross-lingual transfer, we hypothesize
that the models capturing less language-speciﬁc
information of the source language will have bet-
ter transferability. We focus on the word order in-
formation, and explore different encoders and de-
coders that are considered as order-sensitive and
order-free, respectively.

3.1 Contextual Encoders

Considering the sequential nature of languages,
RNN is a natural choice for the encoder. However,
modeling sentences word by word in the sequence
inevitably encodes word order information, which
may be speciﬁc to the source language. To allevi-
ate this problem, we adopt the self-attention based
encoder (Vaswani et al., 2017) for cross-lingual

parsing. It can be less sensitive to word order but
not necessarily less potent at capturing contextual
information, which makes it suitable for our study.

RNNs Encoder Following prior work (Kiper-
wasser and Goldberg, 2016; Dozat and Manning,
2017), we employ k-layer bidirectional LSTMs
(Hochreiter and Schmidhuber, 1997) on top of the
input vectors to obtain contextual representations.
Since it explicitly depends on word order, we will
refer it as an order-sensitive encoder.

Self-Attention Encoder The original
self-
attention encoder (Transformer) takes absolute
positional embeddings as inputs, which capture
much order information. To mitigate this, we
utilize relative position representations (Shaw
et al., 2018), with further simple modiﬁcation
to make it order-agnostic:
the original relative
position representations discriminate left and right
contexts by adding signs to distances, while we
discard the directional information.

We directly base our descriptions on those in
(Shaw et al., 2018). For the relative positional self-
attention encoder, each layer calculates multiple
attention heads. In each head, the input sequence
of vectors x = (x1, . . . , xn) are transformed into
the output sequence of vectors z = (z1, . . . , zn),
based on the self-attention mechanism:

n
(cid:88)

j=1

zi =

αij(xjW V + aV
ij)

αij =

exp eij
k=1 exp eik

(cid:80)n

xiW Q(xjW K + aK
√

ij )T

eij =

dz

ij and aK

Here, aV
ij are relative positional represen-
tations for the two position i and j. Similarly, we
clip the distance with a maximum threshold of k
(which is empirically set to 10), but we do not
discriminate positive and negative values. Instead,
since we do not want the model to be aware of di-
rectional information, we use the absolute values
of the position differences:

ij = wK
aK

clip(|j−i|,k) aV

ij = wV

clip(|j−i|,k)

clip(x, k) = min(|x|, k)

Therefore, the learnable relative postion represen-
tations have k+1 types rather than 2k+1: we have
wK = (wK
k ).
With this, the model knows only what words are
surrounding but cannot tell the directions. Since

k ), and wV = (wV

0 , . . . , wK

0 , . . . , wV

self-attention encoder is less sensitive to word or-
der, we refer to it as an order-free encoder.

3.2 Structured Decoders

With the contextual representations from the en-
coder, the decoder predicts the output tree struc-
tures. We also investigate two types of decoders
with different sensitivity to ordering information.

Stack-Pointer Decoder Recently, Ma et al.
(2018) proposed a top-down transition-based de-
coder and obtained state-of-the-art results. Thus,
we select it as our transition-based decoder. To
be noted, in this Stack-Pointer decoder, RNN is
utilized to record the decoding trajectory and also
can be sensitive to word order. Therefore, we will
refer to it as an order-sensitive decoder.

Graph-based Decoder Graph-based decoders
assume simple factorization and can search glob-
ally for the best structure. Recently, with a deep
biafﬁne attentional scorer, Dozat and Manning
(2017) obtained state-of-the-art results with sim-
ple ﬁrst-order factorization (Eisner, 1996; Mc-
Donald et al., 2005). This method resembles the
self-attention encoder and can be regarded as a
self-attention output layer. Since it does not de-
pend on ordering information, we refer to it as an
order-free decoder.

4 Experiments and Analysis

In this section, we compare four architectures
for cross-lingual transfer dependency parsing with
a different combination of order-free and order-
sensitive encoder and decoder. We conduct sev-
eral detailed analyses showing the pros and cons
of both types of models.

4.1 Setup

In our main experiments4 (those ex-
Settings
cept Section 4.3.5), we take English as the source
language and 30 other languages as target lan-
guages. We only use the source language for
both training and hyper-parameter tuning. Dur-
ing testing, we directly apply the trained model to
target languages with the inputs from target lan-
guages passed through pretrained multilingual em-
beddings that are projected into a common space
as the source language. The projection is done
by the ofﬂine transformation method (Smith et al.,

4Our

implementation

is

publicly

available

at:

https://github.com/uclanlp/CrossLingualDepParser

2017) with pre-trained 300d monolingual embed-
dings from FastText (Bojanowski et al., 2017). We
freeze word embeddings since ﬁne-tuning on them
may disturb the multi-lingual alignments. We also
adopt gold UPOS tags for the inputs.

For other hyper-parameters, we adopted similar
ones as in the Biafﬁne Graph Parser (Dozat and
Manning, 2017) and the Stack-Pointer Parser (Ma
et al., 2018). Detailed hyper-parameter settings
can be found in Appendix B. Throughout our ex-
periments, we adopted the language-independent
UD labels and a sentence length threshold of
140. The evaluation metrics are Unlabeled attach-
ment score (UAS) and labeled attachment score
(LAS) with punctuations excluded5. We trained
our cross-lingual models ﬁve times with different
initializations and reported average scores.

Systems As described before, we have an
order-free (Self-Attention) and an order-sensitive
(BiLSTM-RNN) encoder, as well as an order-free
(Biafﬁne Attention Graph-based) and an order-
sensitive (Stack-Pointer) decoder. The combina-
tion gives us four different models, named in the
format of “Encoder” plus “Decoder”. For clar-
ity, we also mark each model with their encoder-
decoder order sensitivity characteristics. For ex-
ample, “SelfAtt-Graph (OF-OF)” refers to the
model with self-attention order-free encoder and
graph-based order-free decoder. We benchmark
our models with a baseline shift-reduce transition-
based parser, which gave previous state-of-the-
art results for single-source zero-resource cross-
lingual parsing (Guo et al., 2015). Since they
used older datasets, we re-trained the model on our
datasets with their implementation6. We also list
the supervised learning results using the “RNN-
Graph” model on each language as a reference of
the upper-line for cross-lingual parsing.

4.2 Results

The results on the test sets are shown in Table 2.
The languages are ordered by their order typology
distance to English. In preliminary experiments,
we found our lexicalized models performed poorly

5In our evaluations, we exclude tokens whose POS tags
are “PUNCT” or “SYM”. This setting is different from the
one adopted in the CoNLL shared task (Zeman et al., 2018).
However, the patterns are similar as shown in Appendix D
where we report the punctuation-included test evaluations.

6https://github.com/jiangfeng1124/acl15-clnndep. We
also evaluated our models on the older dataset and compared
with their results, as shown in Appendix F.

Lang

en
no
sv
fr
pt
da
es
it
hr
ca
pl
uk
sl
nl
bg
ru
de
he
cs
ro
sk
id
lv
ﬁ
et
zh*
ar
la
ko
hi
ja*
Average

Dist. to SelfAtt-Graph RNN-Graph
English
0.00
0.06
0.07
0.09
0.09
0.10
0.12
0.12
0.13
0.13
0.13
0.13
0.13
0.14
0.14
0.14
0.14
0.14
0.14
0.15
0.17
0.17
0.18
0.20
0.20
0.23
0.26
0.28
0.33
0.40
0.49
0.17

(OF-OF)
90.35/88.40
80.80/72.81
80.98/73.17
77.87/72.78
76.61†/67.75
76.64/67.87
74.49/66.44
80.80/75.82
61.91†/52.86†
73.83/65.13
74.56†/62.23†
60.05/52.28†
68.21†/56.54†
68.55/60.26
79.40†/68.21†
60.63/51.63
71.34†/61.62†
55.29/48.00†
63.10†/53.80†
65.05†/54.10†
66.65/58.15†
49.20†/43.52†
70.78/49.30
66.27/48.69
65.72†/44.87†
42.48†/25.10†
38.12†/28.04†
47.96†/35.21†
34.48†/16.40†
35.50†/26.52†
28.18†/20.91†
64.06†/53.82†

(OS-OF)
90.44/88.31
80.67/72.83
81.23/73.49
78.35†/73.46†
76.46/67.98
77.36/68.81
74.92†/66.91†
81.10/76.23†
60.09/50.67
74.24†/65.57†
71.89/58.59
58.49/51.14
66.27/54.57
67.88/60.11
78.05/66.68
59.99/50.81
69.49/59.31
54.55/46.93
61.88/52.80
63.23/52.11
65.41/56.98
47.05/42.09
71.43†/49.59
66.36/48.74
65.25/44.40
41.53/24.32
32.97/25.48
45.96/33.91
33.66/15.40
29.32/21.41
18.41/11.99
62.71/52.63

SelfAtt-Stack
(OF-OS)
90.18/88.06
80.25/72.07
80.56/72.77
76.79/71.77
75.39/66.67
76.39/67.48
73.15/65.14
79.13/74.16
60.58/51.07
72.39/63.72
73.46/60.49
57.43/49.66
66.55/54.58
67.88/59.46
78.16/66.95
59.36/50.25
69.94/60.09
53.23/45.69
61.26/51.86
62.54/51.46
65.34/56.68
47.32/41.70
69.04/47.80
64.82/47.50
64.12/43.26
40.56/23.32
32.56/23.70
45.49/33.19
32.75/15.04
31.38/23.09
20.72/13.19
62.22/52.00

RNN-Stack
(OS-OS)
91.82†/89.89†
81.75†/73.30†
82.57†/74.25†
75.46/70.49
74.64/66.11
78.22†/68.83
73.11/64.81
80.35/75.32
60.80/51.12
72.03/63.02
72.09/59.75
59.67/51.85
67.76/55.68
69.55†/61.55†
78.83/67.57
60.87/51.96
69.58/59.64
54.89/40.95
62.26/52.32
60.98/49.79
66.56/57.48
46.77/41.28
70.56/48.53
66.25/48.28
64.30/43.50
40.92/23.45
32.85/24.99
43.85/31.25
33.11/14.25
25.91/18.07
15.16/9.32
62.37/51.89

Baseline
(Guo et al., 2015)
87.25/85.04
74.76/65.16
71.84/63.52
73.02/64.67
70.36/60.11
71.34/61.45
68.75/59.59
75.06/67.37
52.92/42.19
68.23/58.15
66.74/53.40
54.10/45.26
60.86/48.06
63.31/53.79
73.08/61.23
55.03/45.09
65.14/54.13
46.03/26.57
56.15/44.77
56.01/44.04
57.75/47.73
40.84/33.67
62.33/41.42
58.51/38.65
56.13/34.86
40.03/20.97
32.69/22.68
39.08/26.17
31.39/12.70
25.74/16.77
15.39/08.41
57.09/45.41

Supervised
(RNN-Graph)
90.44/88.31
94.52/92.88
89.79/86.60
91.90/89.14
93.14/90.82
87.16/84.23
93.17/90.80
94.21/92.38
89.66/83.81
93.98/91.64
94.96/90.68
85.98/82.21
86.79/82.76
90.59/87.52
93.74/89.61
94.11/92.56
88.58/83.68
89.34/84.49
94.03/91.87
90.07/84.50
90.19/86.38
87.19/82.60
83.67/78.13
88.04/85.04
86.76/83.28
73.62/67.67
86.17/81.83
81.05/76.33
85.05/80.76
95.63/92.93
89.06/78.74
89.44/85.62

Table 2: Results (UAS%/LAS%, excluding punctuation) on the test sets. Languages are sorted by the word-
ordering distance to English, as shown in the second column. ‘*’ refers to results of delexicalized models, ‘†’
means that the best transfer model is statistically signiﬁcantly better (by paired bootstrap test, p < 0.05) than all
other transfer models. Models are marked with their encoder and decoder order sensitivity, OF denotes order-free
and OS denotes order-sensitive.

on Chinese (zh) and Japanese (ja). We found the
main reason was that their embeddings were not
well aligned to English. Therefore, we use delex-
icalized models, where only POS tags are used as
inputs. The delexicalized results7 for Chinese and
Japanese are listed in the rows marked with “*”.

Overall, the “SelfAtt-Graph” model performs
the best in over half of the languages and beats the
runner-up “RNN-Graph” by around 1.3 in UAS
and 1.2 in LAS on average. When compared
with “RNN-Stack” and “SelfAtt-Stack”, the av-
erage difference is larger than 1.5 points. This
shows that models capture less word order infor-

7We found delexicalized models to be better only at zh
and ja, for about 5 and 10 points respectively. For other lan-
guages, they performed worse for about 2 to 5 points. We
also tried models without POS, and found them worse for
about 10 points on average. We leave further investigation of
input representations to future work.

mation generally perform better at cross-lingual
parsing. Compared with the baseline, our supe-
rior results show the importance of the contextual
encoder. Compared with the supervised models,
the cross-lingual results are still lower by a large
gap, indicating space for improvements.

After taking a closer look, we ﬁnd an interest-
ing pattern in the results: while the model per-
formances on the source language (English) are
similar, RNN-based models perform better on lan-
guages that are closer to English (upper rows in
the table), whereas for languages that are “distant”
from English, the “SelfAtt-Graph” performs much
better. Such patterns correspond well with our hy-
pothesis, that is, the design of models considering
word order information is crucial in cross-lingual
transfer. We conduct more thorough analysis in
the next subsection.

Model
SelfAtt-Relative (Ours)
SelfAtt-Relative+Dir
RNN
SelfAtt-Absolute
SelfAtt-NoPosi

UAS% LAS%
54.14
64.57
53.62
63.93
52.94
63.25
51.71
61.76
21.45
28.18

Table 3: Comparisons of different encoders (averaged
results over all languages on the original training sets).

4.3 Analysis

We further analyze how different modeling
choices inﬂuence cross-lingual transfer. Since we
have not touched the training sets for languages
other than English, in this subsection, we evaluate
and analyze the performance of target languages
using training splits in UD. Performance of En-
glish is evaluated on the test set. We verify that
the trends observed in test set are similar to those
on the training sets. As mentioned in the previous
section, the bilingual embeddings for Chinese and
Japanese do not align well with English. There-
fore, we report the results with delexicalizing. In
the following, we discuss our observations, and
detailed results are listed in Appendix E.

4.3.1 Encoder Architecture
We assume models that are less sensitive to word
order perform better when transfer to distant lan-
guages. To empirically verify this point, we con-
duct controlled comparisons on various encoders
with the same graph-based decoder. Table 3 shows
the average performances in all languages.

To compare models with various degrees of sen-
sitivity to word order, we include several vari-
ations of self-attention models. The “SelfAtt-
NoPosi” is the self-attention model without any
positional information. Although it is most insen-
sitive to word order, it performs poorly possibly
because of the lack of access to the locality of
contexts. The self-attention model with absolute
positional embeddings (“SelfAtt-Absolute”) also
does not perform well.
In the case of parsing,
relative positional representations may be more
useful as indicated by the improvements brought
by the directional relative position representa-
tions (“SelfAtt-Relative+Dir”) (Shaw et al., 2018).
the RNN encoder ranks between
Interestingly,
“SelfAtt-Relative+Dir” and “SelfAtt-Absolute”;
all these three encoders explicitly capture word or-
der information in some way. Finally, by discard-
ing the information of directions, our relative posi-
tion representation (“SelfAtt-Relative”) performs
the best (signiﬁcantly better at p < 0.05).

Figure 2: Evaluation score differences between Order-
Free (OF) and Order Sensitive (OS) modules. We show
results of both encoder (blue solid curve) and decoder
(dashed red curve). Languages are sorted by their
word-ordering distances to English from left to right.
The position of English is marked with a green bar.

One crucial observation we have is that the pat-
terns of breakdown performances for “SelfAtt-
Relative+Dir” are similar to those of RNN: on
closer languages, the direction-aware model per-
forms better, while on distant
languages the
non-directional one generally obtains better re-
sults. Since the only difference between our pro-
posed “SelfAtt-Relative” model and the “SelfAtt-
Relative+Dir” model is the directional encoding,
we believe the better performances should credit
to its effectiveness in capturing useful context in-
formation without depending too much on the
language-speciﬁc order information.

These results suggest that a model’s sensitivity
to word order indeed affects its cross-lingual trans-
fer performances. In later sections, we stick to our
“SelfAtt-Relative” variation of the self-attentive
encoder and focus on the comparisons among the
four main models.

4.3.2 Performance v.s. Language Distance

We posit that order-free models can do better than
order-sensitive ones on cross-lingual transfer pars-
ing when the target languages have different word
orders to the source language. Now we can ana-
lyze this with the word-ordering distance.

For each target language, we collect two types
of distances when comparing it to English: one is
the word-ordering distance as described in Section
2, the other is the performance distance, which is
the gap of evaluation scores8 between the target
language and English. The performance distance
can represent the general transferability from En-

8In the rest of this paper, we simply average UAS and

LAS for evaluation scores unless otherwise noted.

(a) Adposition & Noun (ADP, NOUN, case)

(b) Adjective & Noun (ADJ, NOUN, amod)

(c) Auxiliary & Verb (AUX, VERB, aux)

(d) Object & Verb (NOUN, VERB, obj)

Figure 3: Analysis on speciﬁc dependency types. To save space, we merge the curves of encoders and decoders
into one ﬁgure. The blue and red curves and left y-axis represent the differences in evaluation scores, the brown
curve and right y-axis represents the relative frequency of left-direction (modiﬁer before head) on this type. The
languages (x-axis) are sorted by this relative frequency from high to low.

glish to this language. We calculate the correla-
tion of these two distances on all the concerned
languages, and the results turn to be quite high:
the Pearson and Spearman correlations are around
0.90 and 0.87 respectively, using the evaluations
of any of our four cross-lingual transfer models.
This suggests that word order can be an important
factor of cross-lingual transferability.

Furthermore, we individually analyze the en-
coders and decoders of the dependency parsers.
Since we have two architectures for each of the
modules, when examining one, we take the high-
est scores obtained by any of the other mod-
ules. For example, when comparing RNN and
Self-Attention encoders, we take the best evalu-
ation scores of “RNN-Graph” and “RNN-Stack”
for RNN and the best of “SelfAtt-Graph” and
“SelfAtt-Stack” for Self-Attention.
Figure 2
shows the score differences of encoding and de-
coding architectures against the languages’ dis-
tances to English. For both the encoding and
decoding module, we observe a similar overall
pattern:
the order-free models, in general, per-
form better than order-sensitive ones in the lan-
guages that are distant from the source language
English. On the other hand, for some languages
that are closer to English, order-sensitive mod-
els perform better, possibly beneﬁting from be-
ing able to capture similar word ordering infor-
mation. The performance gap between order-free

and order-sensitive models are positively corre-
lated with language distance.

4.3.3 Performance Breakdown by Types

Moreover, we compare the results on speciﬁc de-
pendency types using concrete examples. For each
type, we sort the languages by their relative fre-
quencies of left-direction (modiﬁer before head)
and plot the performance differences for encoders
and decoders. We highlight the source language
English in green. Figure 3 shows four typical ex-
ample types: Adposition and Noun, Adjective and
Noun, Auxiliary and Verb, and Object and Verb.
In Figure 3a, we examine the “case” dependency
type between adpositions and nouns. The pattern
is similar to the overall pattern. For languages
that mainly use prepositions as in English, differ-
ent models perform similarly, while for languages
that use postpositions, order-free models get better
results. The patterns of adjective modiﬁer (Figure
3b) and auxiliary (Figure 3c) are also similar.

On dependencies between verbs and object
nouns, although in general order-free models per-
form better, the pattern diverges from what we ex-
pect. There can be several possible explanations
for this. Firstly, the tokens which are noun objects
of verbs only take about 3.1% on average over all
tokens. Considering just this speciﬁc dependency
type, the correlation between frequency distances
and performance differences is 0.64, which is far

d
<-2
-2
-1
1
2
>2

English Average
14.36
15.45
31.55
7.51
9.84
21.29

12.93
11.83
30.42
14.22
10.49
20.11

Table 4: Relative frequencies (%) of dependency dis-
tances. English differs from the Average at d=1.

less than 0.9 when considering all types. There-
fore, although Verb-Object ordering is a typical
example, we cannot take it as the whole story
of word order. Secondly, Verb-Object dependen-
cies can often be difﬁcult to decide. They some-
times are long-ranged and have complex interac-
tions with other words. Therefore, merely reduc-
ing modeling order information can have compli-
cated effects. Moreover, although our relative-
position self-attention encoder does not explicitly
encode word positions, it may still capture some
positional information with relative distances. For
example, the words in the middle of a sentence
will have different distance patterns from those at
the beginning or the end. With this knowledge, the
model can still prefer the pattern where a verb is in
the middle as in English’s Subject-Verb-Object or-
dering and may ﬁnd sentences in Subject-Object-
Verb languages strange. It will be interesting to
explore more ways to weaken or remove this bias.

4.3.4 Analysis on Dependency Distances

We now look into dependency lengths and di-
rections. Here, we combine dependency length
and direction into dependency distance d, by us-
ing negative signs for dependencies with left-
direction (modiﬁer before head) and positive for
right-direction (head before modiﬁer). We ﬁnd a
seemingly strange pattern at dependency distances
|d|=1: for all transfer models, evaluation scores on
d=-1 can reach about 80, but on d=1, the scores
are only around 40. This may be explained by
the relative frequencies of dependency distances
as shown in Table 4, where there is a discrep-
ancy between English and the average of other lan-
guages at d=1. About 80% of the dependencies
with |d|=1 in English is the left direction (mod-
iﬁer before head), while overall other languages
have more right directions at |d|=1. This suggests
an interesting future direction of training on more
source languages with different dependency dis-
tance distributions.

We further compare the four models on the d=1

Figure 4: Evaluation differences of models on d=1 de-
pendencies. Annotations are the same as in Figure 3,
languages are sorted by percentages (represented by the
brown curve and right y-axis) of d=1 dependencies.

Figure 5: Transfer performance of all source-target lan-
guage pairs. The blue and red curves show the averages
over columns and over rows of the source-target pair
performance matrix (see text for details). The brown
curve and the right y-axis legend represent the average
language distance between one language and all others.

dependencies and as shown in Figure 4, the fa-
miliar pattern appears again. The order-free mod-
els perform better at the languages which have
more d=1 dependencies. Such ﬁnding indicates
that our model design of reducing the ability to
capture word order information can help on short-
ranged dependencies of different directions to the
source language. However, the improvements are
still limited. One of the most challenging parts
of unsupervised cross-lingual parsing is modeling
cross-lingually shareable and language-unspeciﬁc
information. In other words, we want ﬂexible yet
powerful models. Our exploration of the order-
free self-attentive models is the ﬁrst step.

4.3.5 Transfer between All Language Pairs

Finally, we investigate the transfer performance of
all source-target language pairs.9 We ﬁrst gen-
erate a performance matrix A, where each en-
try (i, j) records the transfer performance from
a source language i to a target language j. We
then report the following two aggregate perfor-

9Because the size of training corpus for each language
is different in UD, to compare among languages, we train
a parser on the ﬁrst 4,000 sentences for each language and
evaluate its transfer performance on all other languages.

mance measures on A in Figure 5: 1) As-source
reports the average over columns of A for each
row of the source language and 2) As-target re-
ports the average over rows of A for each column
of the target language. As a reference, we also
plot the average word-order distance between one
language to other languages. Results show that
both As-source (blue line) and As-target (red line)
highly are anti-correlated (Pearson correlation co-
efﬁcients are −0.90 and −0.87, respectively) with
average language distance (brown line).

5 Related Work

Cross-language transfer learning employing deep
neural networks has widely been studied in the ar-
eas of natural language processing (Ma and Xia,
2014; Guo et al., 2015; Kim et al., 2017; Kann
et al., 2017; Cotterell and Duh, 2017), speech
recognition (Xu et al., 2014; Huang et al., 2013),
and information retrieval (Vuli´c and Moens, 2015;
Sasaki et al., 2018; Litschko et al., 2018). Learn-
ing the language structure (e.g., morphology, syn-
tax) and transferring knowledge from the source
language to the target language is the main under-
neath challenge, and has been thoroughly investi-
gated for a wide variety of NLP applications, in-
cluding sequence tagging (Yang et al., 2016; Buys
and Botha, 2016), name entity recognition (Xie
et al., 2018), dependency parsing (Tiedemann,
2015; Agi´c et al., 2014), entity coreference reso-
lution and linking (Kundu et al., 2018; Sil et al.,
2018), sentiment classiﬁcation (Zhou et al., 2015,
2016b), and question answering (Joty et al., 2017).
Existing work on unsupervised cross-lingual
dependency parsing, in general, trains a depen-
dency parser on the source language and then
directly run on the target languages. Training
of the monolingual parsers are often delexical-
ized, i.e., removing all lexical features from the
source treebank (Zeman and Resnik, 2008; Mc-
Donald et al., 2013b), and the underlying feature
model is selected from a shared part-of-speech
(POS) representation utilizing the Universal POS
Tagset (Petrov et al., 2012). Another pool of prior
work improves the delexicalized approaches by
adapting the model to ﬁt the target languages bet-
ter. Cross-lingual approaches that facilitate the
usage of lexical features includes choosing the
source language data points suitable for the tar-
get language (Søgaard, 2011; T¨ackstr¨om et al.,
2013), transferring from multiple sources (Mc-

Donald et al., 2011; Guo et al., 2016; T¨ackstr¨om
et al., 2013), using cross-lingual word clusters
(T¨ackstr¨om et al., 2012) and lexicon mapping
(Xiao and Guo, 2014; Guo et al., 2015). In this
paper, we consider single-source transfer–train a
parser on a single source language, and evaluate it
on the target languages to test the transferability of
neural architectures.

transfer

Multilingual

(Ammar et al., 2016;
Naseem et al., 2012; Zhang and Barzilay, 2015)
is another broad category of techniques applied to
parsing where knowledge from many languages
having a common linguistic typology is utilized.
Recent works (Aufrant et al., 2016; Wang and Eis-
ner, 2018a,b) demonstrated the signiﬁcance of ex-
plicitly extracting and modeling linguistic prop-
erties of the target languages to improve cross-
lingual dependency parsing. Our work is different
in that we focus on the neural architectures and
explore their inﬂuences on cross-lingual transfer.

6 Conclusion

In this work, we conduct a comprehensive study
on how the design of neural architectures affects
cross-lingual transfer learning. We examine two
notable families of neural architectures (sequential
RNN v.s. self-attention) using dependency parsing
as the evaluation task. We show that order-free
models perform better than order-sensitive ones
when there is a signiﬁcant difference in the word
order typology between the target and source lan-
guage.
In the future, we plan to explore multi-
source transfer and incorporating prior linguistic
knowledge into the models for better cross-lingual
transfer.

Acknowledgments

We thank anonymous reviewers for their helpful
feedback. We thank Robert ¨Ostling for reaching
out when he saw the earlier arxiv version of the
paper and providing insightful comments about
word order and related citations. We are grateful
for the Stanford NLP group’s comments and feed-
back when we present the preliminary results in
their seminar. We thank Graham Neubig and the
MT/Multilingual Reading Group at CMU-LTI for
helpful discussions. We also thank USC Plus Lab
and UCLA-NLP group for discussion and com-
ments. This work was supported in part by Na-
tional Science Foundation Grant IIS-1760523.

References
ˇZeljko Agi´c, J¨org Tiedemann, Kaja Dobrovoljc, Si-
mon Krek, Danijela Merkler, and Sara Moˇze. 2014.
Cross-lingual dependency parsing of related lan-
guages with rich morphosyntactic tagsets.
In
EMNLP 2014 Workshop on Language Technology
for Closely Related Languages and Language Vari-
ants.

Waleed Ammar, George Mulcaire, Miguel Ballesteros,
Chris Dyer, and Noah Smith. 2016. Many lan-
guages, one parser. Transactions of the Association
for Computational Linguistics 4:431–444.

Lauriane Aufrant, Guillaume Wisniewski, and Franc¸ois
Yvon. 2016. Zero-resource dependency parsing:
Boosting delexicalized cross-lingual transfer with
In Proceedings of COLING
linguistic knowledge.
2016, the 26th International Conference on Compu-
tational Linguistics: Technical Papers. The COL-
ING 2016 Organizing Committee, Osaka, Japan,
pages 119–130.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics 5:135–146.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers). vol-
ume 1, pages 1234–1244.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2016. A representation learn-
ing framework for multi-source transfer parsing. In
Proceedings of the Thirtieth AAAI Conference on
Artiﬁcial Intelligence. AAAI’16, pages 2734–2740.

Kazuma Hashimoto, caiming xiong, Yoshimasa Tsu-
ruoka, and Richard Socher. 2017. A joint many-
task model: Growing a neural network for multiple
In Proceedings of the 2017 Conference
nlp tasks.
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
pages 1923–1933.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Jan Buys and Jan A. Botha. 2016. Cross-lingual mor-
In
phological tagging for low-resource languages.
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Lin-
guistics, Berlin, Germany, pages 1954–1964.

Jui-Ting Huang, Jinyu Li, Dong Yu, Li Deng, and Yi-
fan Gong. 2013. Cross-language knowledge transfer
using multilingual deep neural network with shared
hidden layers. In Acoustics, Speech and Signal Pro-
cessing (ICASSP), 2013 IEEE International Confer-
ence on. IEEE, pages 7304–7308.

Ryan Cotterell and Kevin Duh. 2017.

Low-
resource named entity recognition with cross-
lingual, character-level neural conditional random
In Proceedings of the Eighth International
ﬁelds.
Joint Conference on Natural Language Processing
(Volume 2: Short Papers). volume 2, pages 91–96.

Timothy Dozat and Christopher D Manning. 2017.
Deep biafﬁne attention for neural dependency pars-
ing. Internation Conference on Learning Represen-
tations .

Matthew S Dryer. 2007. Word order. Language typol-

ogy and syntactic description 1:61–131.

Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig.

Jason M Eisner. 1996. Three new probabilistic models
In Pro-
for dependency parsing: An exploration.
ceedings of the 16th conference on Computational
linguistics-Volume 1. Association for Computational
Linguistics, pages 340–345.

Pablo Gamallo, Marcos Garcia,

and Santiago
Fern´andez-Lanza. 2012. Dependency-based open
information extraction. In Proceedings of the joint
workshop on unsupervised and semi-supervised
learning in NLP. Association for Computational
Linguistics, pages 10–18.

Zhanming Jie, Aldrian Obaja Muis, and Wei Lu. 2017.
Efﬁcient dependency-guided named entity recogni-
tion. In Proceedings of the Thirty-First AAAI Con-
ference on Artiﬁcial Intelligence. pages 3457–3465.

Shaﬁq Joty, Preslav Nakov, Llu´ıs M`arquez, and Israa
Jaradat. 2017. Cross-language learning with ad-
In Proceedings of the
versarial neural networks.
21st Conference on Computational Natural Lan-
guage Learning (CoNLL 2017). pages 226–237.

Katharina Kann, Ryan Cotterell, and Hinrich Sch¨utze.
2017. One-shot neural cross-lingual transfer for
paradigm completion. Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics page 19932003.

Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and
Eric Fosler-Lussier. 2017. Cross-lingual transfer
learning for pos tagging without cross-lingual re-
sources. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing. pages 2832–2838.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
Transactions
tional lstm feature representations.
of the Association for Computational Linguistics
4:313–327.

Nikita Kitaev and Dan Klein. 2018. Constituency
In Proceed-
parsing with a self-attentive encoder.
ings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics,
pages 2676–2686.

Gourab Kundu, Avi Sil, Radu Florian, and Wael
Hamza. 2018. Neural cross-lingual coreference res-
olution and its application to entity linking. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers). Association for Computational Linguistics,
pages 395–400.

Robert Litschko, Goran Glavaˇs, Simone Paolo
Ponzetto, and Ivan Vuli´c. 2018. Unsupervised cross-
lingual information retrieval using monolingual data
only. In The 41st International ACM SIGIR Confer-
ence on Research &#38; Development in Informa-
tion Retrieval. SIGIR ’18, pages 1253–1256.

Patrick Littell, David R. Mortensen, Ke Lin, Kather-
ine Kairis, Carlisle Turner, and Lori Levin. 2017.
Uriel and lang2vec: Representing languages as ty-
pological, geographical, and phylogenetic vectors.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Volume 2, Short Papers. Association for
Computational Linguistics, Valencia, Spain, pages
8–14.

Haitao Liu. 2010. Dependency direction as a means
of word-order typology: A method based on depen-
dency treebanks. Lingua 120(6):1567–1578.

Peter J Liu, Mohammad Saleh, Etienne Pot, Ben
Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam
Shazeer. 2018. Generating wikipedia by summa-
Internation Conference on
rizing long sequences.
Learning Representations .

Xuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng,
Graham Neubig, and Eduard Hovy. 2018. Stack-
In Pro-
pointer networks for dependency parsing.
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers).

Xuezhe Ma and Fei Xia. 2014. Unsupervised depen-
dency parsing with transferring distribution via par-
In Pro-
allel guidance and entropy regularization.
ceedings of ACL 2014. Baltimore, Maryland, pages
1337–1348.

David McClosky, Mihai Surdeanu, and Christopher D
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies-Volume 1. Associ-
ation for Computational Linguistics, pages 1626–
1635.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
In Proceedings of ACL-2005.
pendency parsers.
Ann Arbor, Michigan, USA, pages 91–98.

Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T¨ackstr¨om, Claudia Bedini, N´uria
Bertomeu Castell´o, and Jungmee Lee. 2013a. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of ACL-2013. Soﬁa, Bulgaria,
pages 92–97.

Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Os-
car T¨ackstr¨om, et al. 2013b. Universal dependency
In Proceed-
annotation for multilingual parsing.
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers). volume 2, pages 92–97.

Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the conference on empir-
ical methods in natural language processing. Asso-
ciation for Computational Linguistics, pages 62–72.

Tom´aˇs Mikolov, Martin Karaﬁ´at, Luk´aˇs Burget, Jan
ˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model.
In
Eleventh Annual Conference of the International
Speech Communication Association.

Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
In Proceedings of the 50th Annual Meet-
parsing.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers). Association for Compu-
tational Linguistics, Jeju Island, Korea, pages 629–
637.

Joakim Nivre, Mitchell Abrams,

ˇZeljko Agi´c, and
et al. 2018. Universal dependencies 2.2. LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics ( ´UFAL), Faculty of
Mathematics and Physics, Charles University.

Robert ¨Ostling. 2015. Word order typology through
multilingual word alignment. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 2: Short Papers). volume 2, pages 205–211.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers). Association
for Computational Linguistics, pages 2227–2237.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC-2012. Istanbul, Turkey, pages 2089–2096.

Shota Sasaki, Shuo Sun, Shigehiko Schamoni, Kevin
Cross-lingual
Duh, and Kentaro Inui. 2018.
learning-to-rank with shared representations.
In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 2 (Short Papers). volume 2, pages 458–463.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position represen-
In Proceedings of the 2018 Conference of
tations.
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers). Association for
Computational Linguistics, pages 464–468.

Avirup Sil, Gourab Kundu, Radu Florian, and Wael
Hamza. 2018. Neural cross-lingual entity link-
In Proceedings of the Thirty-Second AAAI
ing.
Conference on Artiﬁcial Intelligence, (AAAI-18),
the 30th innovative Applications of Artiﬁcial Intel-
ligence (IAAI-18), and the 8th AAAI Symposium
on Educational Advances in Artiﬁcial Intelligence
(EAAI-18), New Orleans, Louisiana, USA, February
2-7, 2018. pages 5464–5472.

Samuel L Smith, David HP Turban, Steven Hamblin,
and Nils Y Hammerla. 2017. Ofﬂine bilingual word
vectors, orthogonal transformations and the inverted
softmax. Internation Conference on Learning Rep-
resentations .

Anders Søgaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2. Asso-
ciation for Computational Linguistics, pages 682–
686.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Oscar T¨ackstr¨om, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discriminative
In Proceedings of the 2013 Con-
transfer parsers.
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, Atlanta, Georgia, pages 1061–1071.

Oscar T¨ackstr¨om, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discriminative
In Proceedings of the 2013 Con-
transfer parsers.
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, pages 1061–1071.

the association for computational linguistics: Hu-
man language technologies. Association for Com-
putational Linguistics, pages 477–487.

J¨org Tiedemann. 2015.

Cross-lingual dependency
parsing with universal dependencies and predicted
pos labels. In Proceedings of the Third International
Conference on Dependency Linguistics (Depling
2015). pages 340–349.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems. pages 5998–6008.

Ivan Vuli´c and Marie-Francine Moens. 2015. Mono-
lingual and cross-lingual information retrieval mod-
els based on (bilingual) word embeddings. In Pro-
ceedings of the 38th international ACM SIGIR con-
ference on research and development in information
retrieval. ACM, pages 363–372.

Dingquan Wang and Jason Eisner. 2017. Fine-grained
prediction of syntactic typology: Discovering la-
tent structure with supervised learning. Transac-
tions of the Association for Computational Linguis-
tics 5:147–161.

Dingquan Wang and Jason Eisner. 2018a. Surface
statistics of an unknown language indicate how to
parse it. Transactions of the Association for Compu-
tational Linguistics (TACL) .

Dingquan Wang and Jason Eisner. 2018b. Synthetic
data made to order: The case of parsing. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing. pages 1325–1337.

Min Xiao and Yuhong Guo. 2014. Distributed word
representation learning for cross-lingual dependency
In Proceedings of the Eighteenth Confer-
parsing.
ence on Computational Natural Language Learning.
pages 119–129.

Jiateng Xie, Zhilin Yang, Graham Neubig, Noah A.
Smith, and Jaime Carbonell. 2018. Neural cross-
lingual named entity recognition with minimal re-
In Proceedings of the 2018 Conference
sources.
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
pages 369–379.

Yong Xu, Jun Du, Li-Rong Dai, and Chin-Hui Lee.
2014. Cross-language transfer learning for deep
neural network based speech enhancement. In Chi-
nese Spoken Language Processing (ISCSLP), 2014
9th International Symposium on. IEEE, pages 336–
340.

Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of the
2012 conference of the North American chapter of

Zhilin Yang, Ruslan Salakhutdinov, and William Co-
hen. 2016. Multi-task cross-lingual sequence tag-
ging from scratch. arXiv preprint arXiv:1603.06270
.

Daniel Zeman, Jan Hajiˇc, Martin Popel, Martin Pot-
thast, Milan Straka, Filip Ginter, Joakim Nivre, and
Slav Petrov. 2018. Conll 2018 shared task: Mul-
tilingual parsing from raw text to universal depen-
dencies. In Proceedings of the CoNLL 2018 Shared
Task: Multilingual Parsing from Raw Text to Univer-
sal Dependencies. Association for Computational
Linguistics, pages 1–21.

Daniel Zeman and Philip Resnik. 2008.

Cross-
language parser adaptation between related lan-
guages. In Proceedings of the IJCNLP-08 Workshop
on NLP for Less Privileged Languages.

Yuan Zhang and Regina Barzilay. 2015. Hierarchical
low-rank tensors for multilingual transfer parsing.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, Lisbon, Portu-
gal, pages 1857–1867.

Huiwei Zhou, Long Chen, Fulin Shi, and Degen
Huang. 2015. Learning bilingual sentiment word
embeddings for cross-language sentiment classiﬁca-
tion. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers). vol-
ume 1, pages 430–440.

Joey Tianyi Zhou, Sinno Jialin Pan, Ivor W. Tsang,
and Shen-Shyang Ho. 2016a. Transfer learning
for cross-language text categorization through active
correspondences construction. In Proceedings of the
Thirtieth AAAI Conference on Artiﬁcial Intelligence.
AAAI’16, pages 2400–2406.

Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2016b.
Cross-lingual sentiment classiﬁcation with bilingual
In Proceedings
document representation learning.
of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers). volume 1, pages 1403–1412.

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
In Proceedings of the
neural machine translation.
2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 1568–1575.

Supplementary Material: Appendices

A Details of UD Treebanks

The statistics of the Universal Dependency treebanks we used are summarized in Table 5.

Language

Lang. Family

Treebank

Arabic (ar)

Afro-Asiatic

PADT

Bulgarian (bg)

IE.Slavic

BTB

Catalan (ca)

IE.Romance

AnCora

Chinese (zh)

Sino-Tibetan

GSD

Croatian (hr)

IE.Slavic

SET

Czech (cs)

IE.Slavic

PDT,CAC,
CLTT,FicTree

Danish (da)

IE.Germanic

DDT

Dutch (nl)

IE.Germanic

Alpino,
LassySmall

English (en)

IE.Germanic

EWT

Estonian (et)

Uralic

Finnish (ﬁ)

Uralic

EDT

TDT

French (fr)

IE.Romance

GSD

German (de)

IE.Germanic

GSD

Hebrew (he)

Afro-Asiatic

HTB

Hindi (hi)

IE.Indic

HDTB

Indonesian (id)

Austronesian

GSD

Italian (it)

IE.Romance

ISDT

Japanese (ja)

Japanese

Korean (ko)

Korean

GSD

GSD,
Kaist

Latin (la)

IE.Latin

PROIEL

Latvian (lv)

IE.Baltic

LVTB

#Sent.
6075
909
680
8907
1115
1116
13123
1709
1846
3997
500
500
6983
849
1057
102993
11311
12203
4383
564
565
18058
1394
1472
12543
2002
2077
20827
2633
2737
12217
1364
1555
14554
1478
416
13814
799
977
5241
484
491
13304
1659
1684
4477
559
557
13121
564
482
7164
511
557
27410
3016
3276
15906
1234
1260
5424
1051

#Token(w/o punct)
223881(206041)
30239(27339)
28264(26171)
124336(106813)
16089(13822)
15724(13456)
417587(371981)
56482(50452)
57902(51459)
98608(84988)
12663(10890)
12012(10321)
154055(135206)
19543(17211)
23446(20622)
1806230(1542805)
191679(163387)
205597(174771)
80378(69219)
10332(8951)
10023(8573)
261180(228902)
22938(19645)
22622(19734)
204585(180303)
25148(21995)
25096(21898)
287859(240496)
37219(30937)
41273(34837)
162621(138324)
18290(15631)
21041(17908)
356638(316780)
35768(31896)
10020(8795)
263804(229338)
12486(10809)
16498(14132)
137680(122122)
11408(10050)
12281(10895)
281057(262389)
35217(32850)
35430(33010)
97531(82617)
12612(10634)
11780(10026)
276019(244632)
11908(10490)
10417(9237)
161900(144045)
11556(10326)
12615(11258)
353133(312481)
37236(32770)
40043(35286)
171928(171928)
13939(13939)
14091(14091)
80666(66270)
14585(11487)

train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev

Norwegian (no)

IE.Germanic

Polish (pl)

IE.Slavic

Portuguese (pt)

IE.Romance

Bokmaal,
Nynorsk

LFG,
SZ

Bosque,
GSD

Romanian (ro)

IE.Romance

RRT

Russian (ru)

IE.Slavic

SynTagRus

Slovak (sk)

IE.Slavic

Slovenian (sl)

IE.Slavic

Spanish (es)

IE.Romance

SNK

SSJ,
SST

GSD,
AnCora

Swedish (sv)

IE.Germanic

Talbanken

Ukrainian (uk)

IE.Slavic

IU

test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test
train
dev
test

1228
29870
4300
3450
19874
2772
2827
17993
1770
1681
8043
752
729
48814
6584
6491
8483
1060
1061
8556
734
1898
28492
3054
2147
4303
504
1219
4513
577
783

15073(11846)
489217(432597)
67619(59784)
54739(48588)
167251(136504)
23367(19144)
23920(19590)
462494(400343)
42980(37244)
41697(36100)
185113(161429)
17074(14851)
16324(14241)
870474(711647)
118487(95740)
117329(95799)
80575(65042)
12440(10641)
13028(11208)
132003(116730)
14063(12271)
24092(22017)
827053(730062)
89487(78951)
64617(56973)
66645(59268)
9797(8825)
20377(18272)
75098(60976)
10371(8381)
14939(12246)

Table 5: Statistics of the UD Treebanks we used. For language family, “IE” is the abbreviation for Indo-European.
“(w/o) punct” means the numbers of the tokens excluding “PUNCT” and “SYM”.

B Hyper-Parameters

Table 6 summarizes the hyper-parameters that we used in our experiments. Most of them are similar to
those in (Dozat and Manning, 2017) and (Ma et al., 2018).

Input

RNN

Self-Attention

Layer
Word
POS

Encoder

MLP

Training

Encoder

MLP

Training

Hyper-Parameter
dimension
dimension
encoder layer
encoder size
arc MLP size
label MLP size
Dropout
optimizer
learning rate
batch size
encoder layer
dmodel
df f
arc MLP size
label MLP size
Dropout
optimizer
learning rate
batch size

Value
300
50
3
300
512
128
0.33
Adam
0.001
32
6
350
512
512
128
0.2
Adam
0.0001
80

Table 6: Hyper-parameters in our experiments.

C Details about augmented dependency types

Type
(ADP, NOUN, case)
(PUNCT, VERB, punct)
(NOUN, NOUN, nmod)
(ADJ, NOUN, amod)
(DET, NOUN, det)
(VERB, ROOT, root)
(NOUN, VERB, obl)
(NOUN, VERB, obj)
(NOUN, VERB, nsubj)
(PUNCT, NOUN, punct)
(ADV, VERB, advmod)
(AUX, VERB, aux)
(PRON, VERB, nsubj)
(ADP, PROPN, case)
(NOUN, NOUN, conj)
(VERB, NOUN, acl)
(SCONJ, VERB, mark)
(CCONJ, VERB, cc)
(PROPN, NOUN, nmod)
(CCONJ, NOUN, cc)
(NUM, NOUN, nummod)
(PROPN, PROPN, ﬂat)
(VERB, VERB, conj)
(PUNCT, PROPN, punct)
(VERB, VERB, advcl)
(PUNCT, ADJ, punct)

Avg. Freq. (%)
7.47
6.91
4.97
4.92
4.69
4.31
3.96
3.10
2.89
2.75
2.43
2.29
1.53
1.46
1.32
1.31
1.27
1.18
1.14
1.13
1.11
1.09
1.05
0.94
0.89
0.89

#Lang.
31
30
31
31
30
31
30
31
31
30
31
28
30
29
30
31
28
30
30
30
31
26
30
29
30
30

Type
(PROPN, VERB, nsubj)
(PRON, VERB, obj)
(NOUN, ROOT, root)
(VERB, VERB, xcomp)
(VERB, VERB, ccomp)
(ADP, PRON, case)
(AUX, NOUN, cop)
(ADV, ADJ, advmod)
(AUX, ADJ, cop)
(PROPN, VERB, obl)
(PRON, VERB, obl)
(ADV, NOUN, advmod)
(ADJ, ROOT, root)
(PRON, NOUN, nmod)
(NOUN, ADJ, obl)
(PROPN, PROPN, conj)
(NOUN, ADJ, nsubj)
(CCONJ, ADJ, cc)
(PUNCT, NUM, punct)
(NOUN, NOUN, nsubj)
(ADJ, ADJ, conj)
(CCONJ, PROPN, cc)
(PRON, VERB, iobj)
(ADV, ADV, advmod)
(NOUN, NOUN, appos)
(PROPN, VERB, obj)

Avg. Freq. (%)
0.81
0.77
0.66
0.61
0.60
0.57
0.57
0.54
0.50
0.48
0.44
0.41
0.39
0.39
0.37
0.35
0.35
0.29
0.26
0.25
0.25
0.22
0.21
0.19
0.18
0.17

#Lang.
30
30
31
28
30
29
28
29
27
29
30
28
29
22
25
29
30
28
24
31
26
26
21
21
23
24

Table 7: Selected augmented dependency types sorted by their average frequencies. “#Lang.” denotes in how
many languages the speciﬁc type appears. Since the augmented dependency types can be in hundreds or larger
than 1k, but mostly infrequent, we prune them according to average frequency and number of appearing languages.
Our pruning criterion is “F req > 0.1% and #Lang ≥ 20”.

D Punctuation-included Evaluation on the test sets

Language
en
no
sv
fr
pt
da
es
it
hr
ca
pl
uk
sl
nl
bg
ru
de
he
cs
ro
sk
id
lv
ﬁ
et
zh*
ar
la
ko
hi
ja*
Average

SelfAtt-Graph
89.29/87.52
78.47/71.38
79.70/72.69
75.58/71.05
73.07/65.30
74.03/66.52
70.98/63.84
78.19/73.77
60.58/52.60
70.47/62.37
74.78/64.68
57.57/51.16
66.50/55.84
66.92/59.59
76.15/66.48
55.85/48.47
69.61/61.27
53.53/46.98
60.95/53.03
63.11/53.54
65.11/57.76
49.00/44.07
66.53/49.52
64.83/49.83
63.50/45.88
40.46/25.52
37.15/27.79
47.96/35.21
33.96/17.99
36.90/28.52
27.83/21.25
62.21/53.27

RNN-Graph
89.46/87.54
78.47/71.50
79.94/72.99
76.11/71.79
72.82/65.38
74.99/67.67
71.50/64.40
78.63/74.31
58.60/50.28
70.96/62.85
71.73/60.83
56.32/50.25
64.55/53.84
66.45/59.54
74.85/65.01
55.40/47.84
67.60/58.86
53.04/46.16
59.56/51.80
61.19/51.45
63.66/56.38
47.08/42.78
66.95/49.66
65.04/49.98
63.08/45.45
39.54/24.74
32.37/25.42
45.96/33.91
33.08/16.96
30.94/23.55
18.39/12.59
60.91/52.12

SelfAtt-Stack
89.16/87.26
78.11/70.84
79.24/72.24
74.32/69.87
71.61/63.96
73.76/66.15
69.54/62.44
76.52/72.11
59.03/50.65
68.91/60.87
73.82/63.19
54.58/48.18
64.83/53.88
66.05/58.59
74.92/65.23
54.10/46.62
68.18/59.73
51.53/44.76
58.88/50.86
60.31/50.63
63.68/56.21
47.03/42.17
64.50/47.72
63.41/48.61
61.74/44.12
38.37/23.55
31.69/23.46
45.49/33.19
31.68/16.04
32.65/24.92
20.33/13.56
60.26/51.34

RNN-Stack
90.83/89.07
79.61/72.10
81.44/73.98
73.56/69.16
71.21/63.76
75.81/67.76
69.73/62.37
78.29/73.84
59.27/50.72
68.79/60.45
72.24/62.11
57.31/50.81
66.07/55.03
68.10/61.01
75.69/65.96
55.88/48.52
68.02/59.36
53.26/40.83
59.63/51.13
59.38/49.61
64.97/57.08
47.12/42.38
65.98/48.46
64.97/49.63
62.15/44.57
39.26/24.25
32.04/24.73
43.85/31.25
32.81/16.17
26.80/19.49
15.01/9.75
60.62/51.46

Table 8: Evaluations with punctuation included (average UAS%/LAS% over 5 runs) on the test sets. The patterns
are similar to the punctuation-excluded evaluations in the main content.
(Languages are sorted by the word-
ordering distance to English, ‘*’ refers to results of delexicalized models.)

E Results on the original training sets

Language
en◦
no
sv
fr
pt
da
es
it
hr
ca
pl
uk
sl
nl
bg
ru
de
he
cs
ro
sk
id
lv
ﬁ
et
zh*
ar
la
ko
hi
ja*
Average

SelfAtt-Graph
90.35/88.40
80.72/72.45
80.07/71.91
79.31/74.73
77.06/69.33
75.75/67.12
73.91/66.48
80.37/75.48
61.57/52.40
74.40/65.73
75.32/63.26
65.70/57.48
69.13/58.92
68.98/60.00
80.25/68.88
60.50/51.35
67.23/58.27
58.32/49.80
63.04/53.92
65.31/54.22
76.07/62.75
47.92/41.93
71.69/50.43
64.64/46.21
66.63/45.58
41.05/23.85
38.74/28.24
49.04/35.48
34.62/15.14
36.01/27.24
28.19/21.74
64.57/54.14

RNN-Graph
90.44/88.31
80.59/72.41
80.42/72.39
79.99/75.52
77.33/69.91
75.95/67.41
74.39/67.03
80.89/75.99
59.74/50.37
74.94/66.21
73.12/59.76
64.77/56.40
67.35/56.87
68.37/59.52
78.39/67.03
59.55/50.17
66.64/57.48
57.75/49.07
61.75/52.91
63.17/52.16
74.67/61.15
45.07/39.91
72.48/50.85
64.63/46.22
65.78/45.01
40.11/23.02
33.66/25.44
47.12/34.36
33.91/14.16
29.59/21.75
18.23/12.68
63.25/52.94

SelfAtt-Stack
90.18/88.06
80.06/71.60
79.45/71.28
78.62/74.02
75.84/68.22
75.18/66.55
72.84/65.38
79.15/74.17
59.94/50.43
73.01/64.42
74.28/61.46
64.10/55.83
67.74/57.08
68.22/59.02
79.19/67.66
59.01/49.71
66.10/56.89
56.36/47.62
61.11/51.91
63.03/51.95
75.93/61.97
46.23/40.16
70.24/48.97
63.07/44.82
64.94/44.04
39.49/22.68
34.25/24.69
46.78/33.56
32.70/13.77
32.02/23.79
20.53/13.78
62.88/52.44

RNN-Stack
91.82/89.89
81.46/72.75
80.87/72.25
76.84/72.22
75.39/67.75
76.98/67.50
72.46/64.78
79.05/73.91
60.44/50.68
72.75/63.68
73.21/61.02
65.82/57.13
68.95/58.26
69.16/60.11
79.66/68.22
60.71/51.57
65.88/56.63
58.79/43.83
62.21/52.48
61.78/50.52
75.37/60.94
45.62/39.67
71.60/49.56
64.74/46.09
65.06/44.33
39.89/22.49
33.31/24.86
45.26/31.97
32.95/13.14
26.37/18.56
15.21/10.37
62.88/52.16

Table 9: Results (average UAS%/LAS% over 5 runs, excluding punctuation) on the original training sets. (Lan-
guages are sorted by the word-ordering distance to English, ‘*’ refers to results of delexicalized models, ‘en◦’
means that for English we use results on the test set since models are trained with the English training set.)

F Results on Google Universal Dependency Treebanks v2.0

We also ran our models on Google Universal Dependency Treebanks v2.0 (McDonald et al., 2013a),
which is an older dataset that was used by (Guo et al., 2015). The results show that our models perform
better consistently.

Language
German
French
Spanish

SelfAtt-Graph
65.03/55.03
74.45/63.28
72.00/61.50

RNN-Graph
64.60/54.57
76.75/65.20
73.99/63.46

SelfAtt-Stack
63.63/54.40
73.63/62.76
71.73/61.42

RNN-Stack
65.51/55.82
75.13/64.44
74.13/64.00

(Guo et al., 2015)
60.35/51.54
72.93/63.12
71.90/62.28

Table 10: Comparisons (UAS%/LAS%) on Google Universal Dependency Treebanks v2.0.

G Results on speciﬁc dependency types for Czech

In table 11, we show results of Czech on some dependency types with evaluation breakdowns on de-
pendency directions. We select Czech mainly for two reasons: (1) It has the largest dataset; (2) Czech
is famous for relatively ﬂexible word order. Generally, we can see that models that are more ﬂexible
on word ordering perform better. Interestingly, for objective and subjective types, we can see that LAS
scores for all models are quite low even when the correct heads are predicted. The reason might be that
even the relative-positional self-attention encoder can capture some positional information which further
reveals word ordering information in some way.

Direction
mod-ﬁrst
head-ﬁrst
all

Direction
mod-ﬁrst
head-ﬁrst
all

Direction
mod-ﬁrst
head-ﬁrst
all

Direction
mod-ﬁrst
head-ﬁrst
all

Direction
mod-ﬁrst
head-ﬁrst
all

Direction
mod-ﬁrst
head-ﬁrst
all

Direction
mod-ﬁrst
head-ﬁrst
all

Direction
mod-ﬁrst
head-ﬁrst
all

Direction
mod-ﬁrst
head-ﬁrst
all

Percentage
0.97%
99.03%
100.00%

Percentage
99.99%
0.01%
100.00%

RNN-Graph
74.62/74.61
–
74.61/74.61

SelfAtt-Graph
75.34/75.34
–
75.33/75.33

(ADP, NOUN, case): (mod-ﬁrst% in English is 99.92%.)
SelfAtt-Stack
74.46/74.43
–
74.45/74.43
(NOUN, NOUN, nmod): (mod-ﬁrst% in English is 4.72%.)
SelfAtt-Stack
–
20.49/16.61
20.77/16.45
(ADJ, NOUN, amod): (mod-ﬁrst% in English is 99.01%.)
SelfAtt-Stack
85.39/85.21
34.82/27.19
81.85/81.14
(NOUN, VERB, obl): (mod-ﬁrst% in English is 9.62%.)

SelfAtt-Graph
88.93/88.92
41.80/37.03
85.63/85.29

SelfAtt-Graph
–
21.38/17.85
21.64/17.68

RNN-Graph
89.42/89.41
36.52/32.36
85.72/85.41

RNN-Graph
–
18.55/16.20
18.86/16.05

Percentage
92.99%
7.01%
100.00%

Percentage
37.80%
62.20%
100.00%

SelfAtt-Graph
48.84/40.33
62.81/55.97
57.53/50.06

SelfAtt-Stack
48.75/41.08
62.22/55.37
57.13/49.97
(NOUN, VERB, obj): (mod-ﬁrst% in English is 0.72%.)

RNN-Graph
46.39/38.49
60.38/53.41
55.09/47.77

RNN-Graph
53.75/0.46
71.30/62.28
67.68/49.52

SelfAtt-Stack
54.08/0.37
72.12/63.81
68.39/50.71

SelfAtt-Graph
55.56/0.64
73.18/65.24
69.54/51.90

Percentage
20.65%
79.35%
100.00%
(NOUN, VERB, nsubj): (mod-ﬁrst% in English is 85.07%.)
SelfAtt-Stack
60.88/58.24
62.38/2.97
61.48/36.25

Percentage
60.22%
39.78%
100.00%
(ADV, VERB, advmod): (mod-ﬁrst% in English is 58.82%.)
SelfAtt-Stack
86.65/85.30
65.33/64.35
80.29/79.05
(AUX, VERB, aux): (mod-ﬁrst% in English is 99.64%.)

SelfAtt-Graph
61.42/58.33
64.07/3.83
62.47/36.65

SelfAtt-Graph
88.23/87.49
65.79/65.28
81.53/80.86

RNN-Graph
86.43/85.48
65.02/64.33
80.04/79.17

RNN-Graph
58.12/54.51
62.93/3.18
60.03/34.09

Percentage
70.15%
29.85%
100.00%

Percentage
83.71%
16.29%
100.00%

RNN-Graph
84.44/83.52
54.59/50.87
79.57/78.20

SelfAtt-Graph
88.78/88.19
68.18/65.28
85.42/84.46

SelfAtt-Stack
89.03/86.59
63.96/54.02
84.94/81.28
(VERB, VERB, advcl): (mod-ﬁrst% in English is 31.02%.)
SelfAtt-Stack
57.54/55.03
67.27/54.17
63.21/54.53

SelfAtt-Graph
57.51/55.61
71.52/56.68
65.67/56.23

RNN-Graph
56.98/55.60
67.39/56.08
63.04/55.88

Percentage
41.75%
58.25%
100.00%

RNN-Stack
74.17/74.08
–
74.17/74.07

RNN-Stack
–
22.51/19.16
22.78/18.98

RNN-Stack
87.26/86.37
40.59/19.85
83.98/81.71

RNN-Stack
50.16/41.64
61.73/55.32
57.36/50.15

RNN-Stack
60.34/0.18
72.76/64.65
70.20/51.34

RNN-Stack
60.67/58.98
59.94/4.42
60.38/37.28

RNN-Stack
86.64/83.72
61.93/60.53
79.26/76.80

RNN-Stack
82.54/76.33
56.67/20.24
78.32/67.19

RNN-Stack
54.74/51.66
65.93/54.13
61.26/53.10

Table 11: Evaluation breakdowns (UAS%/LAS%) on dependency directions for Czech on some speciﬁc depen-
dency types. “mod-ﬁrst” means the dependency edges whose modiﬁer is before head, “head-ﬁrst” means the
opposite, and “all” indicates both “mod-ﬁrst” and “head-ﬁrst”. “–” replaces results that are unstable because of
rare appearance (below 1%).

