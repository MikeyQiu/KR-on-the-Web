A Neural Network Based Model for Loanword Identiﬁcation in Uyghur

Chenggang Mi†‡, Yating Yang†‡, Lei Wang†‡, Xi Zhou†‡, Tonghai Jiang†‡
†The Xinjiang Technical Institute of Physics & Chemistry of Chinese Academy of Sciences, Urumqi, China
‡Key laboratory of speech language information processing of Xinjiang, Urumqi, China
{micg,yangyt,wanglei,zhouxi,jth}@ms.xjb.ac.cn

Abstract
Lexical borrowing happens in almost all languages. To obtain more bilingual knowledge from monolingual corpora, we propose a neural
network based loanword identiﬁcation model for Uyghur. We build our model on a bidirectional LSTM - CNN framework, which can
capture past and future information effectively and learn both word level and character level features from training data automatically. To
overcome data sparsity that exists in model training, we also suggest three additional features , such as hybrid language model feature,
pronunciation similarity feature and part-of-speech tagging feature to further improve the performance of our proposed approach. We
conduct experiments on Chinese, Arabic and Russian loanword detection in Uyghur. Experimental results show that our proposed
method outperforms several baseline models.

Keywords: Loanword Identiﬁcation, Uyghur Language, BiLSTM-CNN, Language Model, Pronunciation Similarity

1.

Introduction

Lexical borrowing is very common between languages
(Taylor and Grant, 2015).
It is a phenomenon of cross-
linguistic inﬂuence.
If loanwords in resource-poor lan-
guages (e.g. Uyghur) can be identiﬁed effectively, we can
use the donor-receipt word pairs to extend bilingual dictio-
nary. And the bilingual dictionary plays a very important
role in many cross-lingual areas in natural language pro-
cessing (NLP), such as machine translation (Tsvetkov and
Dyer, 2015).
In this paper, we describe a novel method to identify loan-
words in Uyghur texts to alleviate the data sparsity that ex-
ists in Uyghur related NLP tasks. Our loanword identiﬁca-
tion model is based on a bidirectional long-short term mem-
ory (BiLSTM) - convolutional neural networks (CNNs)
framework (Chiu and Nichols, 2016). The BiLSTM have
achieved state-of-the-art performance in varies sequence-
to-sequence learning tasks, a very important reason is that
it can capture past (from previous tagged words) and fu-
ture (from next untagged words) information effectively.
We use it to model word level features. CNNs have been
used in several character level natural language process-
ing (NLP) tasks; we use it to model character level fea-
tures. Therefore, our model can learn both word level and
character level feature from training data, automatically.
Additionally, we also propose three features (hybrid lan-
guage model, pronunciation similarity and part-of-speech
tagging) to argument the BiLSTM-CNN model by exploit
knowledge learned from monolingual corpus. We conduct
experiments on Chinese, Arabic and Russian loanwords de-
tection in Uyghur, respectively. Experimental results show
that our model outperforms several baseline models.

2. Loanwords in Uyghur Language

2.1. An Introduction of Uyghur Language
Uyghur is an ofﬁcial language of the Xinjiang Uyghur Au-
tonomous Region in China, and is widely used in both so-
cial and ofﬁcial spheres, as well as in print, radio and tele-
vision, and is mostly used as a lingua franca by other eth-

nic minorities in Xinjiang. Uyghur belongs to the Turkic
language family. Like other Turkic languages, Uyghur dis-
plays vowel harmony and agglutination, lacks noun classes
or grammatical gender, and is a left-branching language
with subject (S) - object (O) - verb (V) word order.
As an agglutinative language, nouns in Uyghur are inﬂected
for number and case, but not gender and deﬁniteness like
in many other languages(Table 1)1. There are two numbers
(singular, plural) and six different cases in nouns of Uyghur
language. Verbs are inﬂected for tense, voice, aspect and
mood.

Uyghur(in English)
aliqanimda(In my hands)
etrapidikilerni(People around)
qurulmasining(Structured)
qalduridu(Stay)

Uyghur(stem+sufﬁx)
aliqan+im+da
etrap+i+diki+ler+ni
qurulma+si+ning
qal+dur+i+d+u

Table 1: Examples of Uyghur word formation.

2.2. Linguistic Issues of Loanwords in Uyghur
Due to different kinds of language contact through the
history, Uyghur has adopted many loanwords(Kamalov,
2006). Some studies show that larger than 20% of the
vocabulary is from other languages. Kazakh, Uzbek, and
Chagatai are all Turkic languages which had a strong
inﬂuence on Uyghur. Arabic words have also entered
Uyghur through Islamic literature after the introduction of
the Islamic religion around the 10th century.
Recently, Chinese and Russian had the greatest inﬂuence
on Uyghur language. In particular, loanwords from these
two languages are all quite recent. Below are some
examples of loanwords in Uyghur (Table 2):

3. Method
In this section, we describe the details of our proposed
loanword identiﬁcation model. First, we present the BiL-

1In this paper, we write Uyghur with the Latin alphabet.

3575

Uyghur(Chinese)[In English]
shinjang(#ı) [one province in China]
laza(2f) [hot pepper]
shuji((cid:214)P) [secretary]
koi(‹) [Chinese currency]
lengpung(łﬁ) [agar-agar jelly]
dufu(˛@) [tofu]

Uyghur(Russian)[In English]
tElEfon(telefon) [telephone]
uniwErsitEt(universitet) [university]
radiyo(radio) [radio]
pochta(poqta) [post ofﬁce]
wElsipit(velosiped) [bicycle]
oblast(oblast(cid:126)) [region]

Table 2: Examples of Chinese and Russian Loanwords in Uyghur.

employ a convolution and a max layer to extract a new fea-
ture vector from the character embeddings(Figure 2).

STM model, which can model word-level features from
both forward and backward directions. Next, we introduce
the CNN based character-level feature extraction. Then,
we detail two core features (word embedding feature and
character embedding feature) and three additional features
(pronunciation similarity feature, POS tagging feature and
hybrid language model feature) used in our model. Finally,
we present the training and optimization of our proposed
neural network.

3.1. Word-level Tagging with BiLSTM

Similar to previous work used the BiLSTM in other ar-
eas in NLP (Such as NER and speech recognition), we
explored a stacked bi-directional recurrent neural network
with LSTM units to transform word features into loanword
tag scores(Figure 1).
In our paper, we fed extracted features of each word into

Figure 1: BiLSTM Model for Loanwords Identiﬁcation in
Uyghur.

a forward and backward LSTM network. The output of
each network at each time step is decoded by a linear layer
and a log-softmax layer into log-probabilities for each tag
category. The two vectors produced by a linear and a log-
softmax are added to produce the ﬁnal output.

3.2. Character-level Features Extraction Based

on CNN

Convolutional neural networks (CNN) have shown great
success in character level features extraction in NER and
POS tagging. However, the character level CNN has not
been applied in loanwords identiﬁcation. In this paper, we

3576

Figure 2: Character-level Features Extraction Model Based
on CNN.

3.3. Features Deﬁnition
3.3.1. Main Features
Word Embedding Feature
Word embedding is a collective name for techniques in
NLP where words or phrases from the vocabulary are
mapped to vectors represent by real numbers.
It aims
to quantify and categorize semantic similarities between
words based on their distributional properties in large sam-
ples of language data. In this study, we use the word em-
bedding as input to the neural network.
Currently, there is no publicly available Uyghur word em-
beddings, so we trained them by ourselves.
In this pa-
per, we experimented with three sets of word embed-
dings: 1) NEWS word2vec embeddings trained on 2 billion
words from news and government documents; 2) ORAL
word2vec embeddings trained on 1.5 billion words from
short messages and weinxin, 3) HYBRID word2vec em-
beddings trained on 3.5 billion words from both 1) and 2).
We used the open source toolkit Glove2 to train above word
embeddings.
Character Embedding Feature
In this paper, the character embeddings are uniformly sam-
DIM ]. DIM is the dimension
pled from range [−
of embeddings. The character set includes all unique char-
acters in Uyghur language. Besides, there are two more
tokens are also containing in above set: UNKNOWN and
PADDING. The UNKNOWN is indicates all other charac-
ters and PADDING is used for CNN.

DIM ,

(cid:113) 3

3

2https://nlp.stanford.edu/projects/glove/

3.3.2. Additional Features
Besides the main features described in previous section, we
also proposed four features to further improve the perfor-
mance of our approach. Intuitively, we may think that the
loanword in Uyghur should have a similar pronunciation
with its corresponding word in donor language. Therefore,
we take the pronunciation similarity as an important
feature in our model. Due to most loanwords are nouns, we
constraint the output of our approach by a part-of-speech
tag feature. A loanword may adapt the pronunciation
system of receipt language when borrowing from the donor
language. So the pronunciation of a loanword has the
features of pronunciation systems of both receipt and donor
languages. We use a hybrid language model to represent
this feature. In this paper, we focus on Chinese, Arabic and
Russian loanwords in Uyghur.
Pronunciation Similarity (ps)
Loanword (LW) usually have a similar pronunciation to
their corresponding donor language (DW) word. Previous
work detected loanwords in Uyghur according to string
similarity between LW and DW, which ﬁrst transfer the
pronunciation similarity as string similarity. However, we
found that there exist many differences among different
writing systems, which have negative effects on the
loanwords detection. To overcome this, we proposed a
method that transfers both words into strings according
to the International Phonetic Alphabet (IPA) . After that,
we compute string similarity based on the Edit Distance
algorithm (aka Levenshtein Distance).
Part-Of-Speech Tagging (pos)
Most loanwords are nouns. Therefore, we use part-of-
speech information as one of the additional feature to
constraint the output of our model. The POS tags are
obtained by an in-house Uyghur POS tagger which was
developed by us.
Hybrid Language Model (hlm)
there are different pronunciation systems be-
Usually,
tween recipient
Each
language and donor language.
pronunciation system can be represented by a certain
character-level language model. When lexical borrowing,
the pronunciation of a word in donor language may adapts
the pronunciation system of the recipient language. This
inspired us to combine these two language models to
feature the pronunciation of loanwords.

phlm = (1 − λ1puyg) + λ2pdnr

(1)

Where puyg is the language model probability of a given
character sequence in Uyghur, pdnr is the language model
probability of above sequence in donor languages. λ1 and
λ2 are weights which can be obtained during model opti-
mization.

3.4. Neural Network Training

3.4.1. Tagging Scheme
Similar to the scheme used in named entities recognition
(NER), we used the BIESO tag set in this paper. BIOES,
which stand for Begin (B), Inside (I), End (E), Single (S)
and Other (O), indicating the position of the character in a
certain loanword. For example: With the BIESO tagging

scheme, more information can be considered when neural
networks training.

3.4.2. Network Implementation
We implement the BiLSTM-CNN model used in our paper
based on Theano3, a widely used deep learning Python li-
brary. We trained the loanwords identiﬁcation model based
on sentence-level corpus. We initialized the word em-
bedding and character embedding as previous description.
Other lookup tables used in our model are randomly initial-
ized with values drawn from a standard normal distribution.

3.4.3. Parameters
We tune the hyper-parameters as follows: for CNN, we set
the window size as 5 and use 40 ﬁlters; for bidirectional
LSTM network, we set the initial state as 0.0 and state size
as 300. As mentioned above, we use dropout to regular-
ize our model to alleviate overﬁtting when neural network
training and we set the dropout rate as 0.5.

3.4.4. Optimization Algorithm
We use the min-batch stochastic gradient descent (SGD)
to train our loanwords identiﬁcation model. Each mini-
bath includes multiple sentences with the same number of
Uyghur words. We ﬁnd that training neural network with
dropout is very effective in alleviate the overﬁtting. To
achieve a better performance on development sets, we use
early stopping method in our experiments.

4. Experiments

4.1. Datasets and Settings

To evaluate
the proposed method effectively, we
train the neural network with three groups of cor-
[train-
(tokens)(UYGLWChn:20M/10K/20K*2
pora
ing/develop/test], UYGLWArab:15M/10K/20K*2
and
UYGLWRus:12M/10K/20K*2). Because there are rel-
atively few loanwords (compared with other words) in
Uyghur, we also annotated person names in these three
donor languages as loanwords. These corpora are collected
from government websites and newspapers. Test data for
cross-domain experiments are selected from social medias
such as Weixin and Twitter. We train three donor language
models with corpus selected from previous corpora. The
develop sets and test sets used in our experiments are all
selected from the same domain.
We built the bi-directional LSTM-CNN neural network on
the Theano library. The computations for a model are run
on a GPU. We extract the word embedding and character
embedding based on the open source toolkit Glove4. We
use SRILM5 to obtain the character level language model
for four languages. The POS tagging features are extracted
based on a Uyghur POS tagger, which was developed
by us.We use Precision (P), Recall (R) and F1 score
to evaluate the performance of loanword identiﬁcation
models.

3http://deeplearning.net/software/theano/
4https://nlp.stanford.edu/projects/glove/
5http://www.speech.sri.com/projects/srilm/

3577

Pchn Rchn F1chn
Model
66.35
62.33
CRFs
69.78
71.38
77.28
SSIM 66.32
73.18
68.30
CIBM 78.82
79.08
79.20
78.97
RNN
80.63
81.02
80.24
Ours

Prus Rrus
63.25
71.64
70.02
75.39
73.22
81.03
75.93
82.55
76.30
82.95

F1rus Parab Rarab F1arab
68.72
67.18
70.50
72.61
72.90
76.93
80.32
79.10
81.08
79.49

65.32
67.51
70.71
77.58
78.28

72.50
73.76
75.22
83.26
84.09

Table 3: Experimental Results on Loanword Identiﬁcation Models.

4.2. Experimental Results
4.2.1. Effects on Different Methods
Table 3 presents results on different methods, including
CRFs-based model (CRFs),
the string similarity based
loanword identiﬁcation model (SSIM) (Mi et al., 2013),
identiﬁcation model based on classiﬁcation (CBIM) (Mi et
al., 2014), a RNN based identiﬁcation model (Mi et al.,
2016) and the model proposed in this paper. Due to lack
sufﬁcient annotated corpus, CRFs based model cannot out-
perform other models. After analysis the output of CRFs
based model, we found that only a small number of non-
person name loanwords are identiﬁed, which means the
CRFs based method rely on annotated corpus heavily. The
performance of SSIM outperforms CRFs based model, an
important reason is that the SSIM method based on pro-
nunciation similarity between two words (donor words and
receipt word). Because it combines the advantages of sta-
tistical based and rule based model, the CIBM outperforms
CRFs and SSIM models. Like CRFs, RNN based model
also rely on annotated corpus heavily. However, we found
that the RNN based model can identify more loanwords
(more than person names) than CRFs based model, a possi-
ble reason is that the RNN encoder-decoder framework can
learn features automatically and use its internal memory to
process arbitrary sequences of inputs. To model the word-
level and character-level features, we proposed a BiLSTM-
CNN neural network to identify loanwords from Uyghur
texts. Moreover, three important additional features were
also suggested to overcome data sparseness. Our proposed
method achieved the best score among these approaches.

4.2.2. Experimental Results on Different Features
In Table 4, we present results with different additional
features. We can found that our model (ours) achieves
best performance among all these models. We observe
that our model beneﬁt most from hlm (hybrid language
model) feature. Compare with other two models (BiLSTM-
CNN+ps&pos), model with hlm feature achieve the best
improvements. An important reason is that the hlm feature
combines both pronunciation systems of donor language
and receipt language. Models with string similarity and
POS tag features only reﬂect the shallow semantic infor-
mation. Due to lack of annotated corpus, the model without
any additional features performs worst in all experiments.

4.2.3. Evaluation on Different Domains
Table 5 shows our results on different domains. To show the
capability of our loanwords identiﬁcation model, we eval-
uate our model on different domains, such as news(News)
and social network(socialNet). We can found that the ex-

perimental results on formal corpus (news domain) which
have the same domain with our training corpus are outper-
forming the performance on informal domain (social net-
work). We observed the same situation in all donor lan-
guages. We also found that experimental results on infor-
mal domain are just a little worse compared with results
on formal domain. One possible reason is that our BiL-
STM+CNN model can learn representation of knowledge
beyond given training examples.

5. Related work

In general, word borrowing is often concerned by linguists
(Chen, 2011)(Chen and Chen, 2011). There are relatively
few studies about loanwords in NLP area.(Tsvetkov et al.,
2015) and (Tsvetkov and Dyer, 2016) proposed a morph-
phonological transformation model, features used in this
model are based on optimality theory; experiment has been
proved that with a few training examples, this model can
obtain good performance at predicting donor forms from
borrowed forms.(Tsvetkov and Dyer, 2015) suggest an ap-
proach that uses the lexical borrowing as a model in SMT
framework to translate OOV words in a low-resource lan-
guage. For loanwords detection in Uyghur, string similar-
ity based methods were often used at the early stage(Mi et
al., 2014).(Mi et al., 2016) propose a loanword detection
method based on the perceptron model, several features are
used in model training.

6. Conclusion

In this paper, we have presented a novel model to de-
tect loanwords (mainly Chinese, Arabic and Russian loan-
words) in Uyghur by using a BiLSTM-CNN framework.
Except two main features such word embeddings and char-
acter embeddings, two additional features like donor lan-
guage model feature and hybrid language model feature
are also proposed and integrated the framework to fur-
ther improve the performance.
In the proposed model,
the character-level feature of each word is extracted by the
CNN model based on character embedding and our pro-
posed two features. For each word, the character-level fea-
ture vector is concatenated with the word embedding fea-
ture vector and fed into the BiLSTM model. After that,
these feature vectors are fed to output layers. Experiment
results on loanwords identiﬁcation in Uyghur have pre-
sented that the proposed model can signiﬁcantly improve
the identiﬁcation performance.
Although our model achieves the best results on loanwords
identiﬁcation in Uyghur, we only use loanword taggers in
our training set. In our future work, we plan to integrate

3578

Pchn Rchn F1chn
Feature
72.44
67.89
77.65
BiLSTM-CNN
74.35
70.32
78.86
BiLSTM-CNN+ps
73.88
69.54
BiLSTM-CNN+pos
78.79
74.62
70.37
BiLSTM-CNN+hlm 79.42
80.63
81.02
80.24
BiLSTM-CNN+all

Prus Rrus
68.33
78.02
70.65
81.94
71.28
81.35
73.50
82.29
76.30
82.95

F1rus Parab Rarab F1arab
74.49
72.85
76.02
75.88
75.11
75.98
77.63
77.65
81.08
79.49

70.96
71.52
70.20
73.59
78.28

78.38
81.12
80.76
82.14
84.09

Table 4: Evaluation on Features Used in RNN-based Model.

Domain
socialNet
News

Pchn Rchn F1chn
80.07
80.51
79.63
80.63
81.02
80.24

Prus Rrus
75.22
81.05
76.30
82.95

F1rus Parab Rarab F1arab
80.42
78.03
81.08
79.49

77.45
78.28

83.63
84.09

Table 5: Evaluation on Cross-Domain Corpora.

Tsvetkov, Y. and Dyer, C. (2015). Lexicon stratiﬁcation
for translating out-of-vocabulary words. In Proceedings
of the 53rd Annual Meeting of the Association for Com-
putational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume 2:
Short Papers), pages 125–131. Association for Compu-
tational Linguistics.

Tsvetkov, Y. and Dyer, C. (2016). Cross-lingual bridges
with models of lexical borrowing. Journal of Artiﬁcial
Intelligence Research, 55(1):63–93, January.

Tsvetkov, Y., Ammar, W., and Dyer, C. (2015). Constraint-
based models of lexical borrowing. In Proceedings of
the 2015 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, pages 598–608. Association for
Computational Linguistics.

more linguistic knowledge to further optimize the perfor-
mance of our proposed model.

7. Acknowledgements

We sincerely thank the anonymous reviewers for their
thorough reviewing and valuable suggestions. This work
is supported by the West Light Foundation of The Chi-
nese Academy of Sciences under Grant No.2015-XBQN-
the Xinjiang Key Laboratory Fund under Grant
B-10,
No.2015KL031,
the Xinjiang Science and Technology
Major Project under Grant No.2016A03007-3 and the
Natural Science Foundation of Xinjiang under Grant
No.2015211B034.c

8. Bibliographical References

Chen, Y. and Chen, P. (2011). A comparison on the meth-
ods of uyghur and chinese loan words. Journal of Kash-
gar Teachers College, 32(2):51–55.

Chen, S. (2011). New research on chinese loanwords in the
uyghur language. N.W.Journal of Ethnology, 28(1):176–
180.

Chiu, J. and Nichols, E. (2016). Named entity recognition
with bidirectional lstm-cnns. Transactions of the Associ-
ation for Computational Linguistics, 4:357–370.

Kamalov, A. (2006). Uyghur Studies in Central Asia: A

Historical Review.

Mi, C., Yang, Y., Zhou, X., Li, X., and Yang, M. (2013).
Recognition of chinese loan words in uyghur based on
string similarity. Journal of Chinese Information Pro-
cessing, 27(5):173–179.

Mi, C., Yang, Y., Wang, L., Li, X., and Dalielihan,
K.
(2014). Detection of loan words in uyghur texts.
In Natural Language Processing and Chinese Comput-
ing: Third CCF Conference, NLPCC 2014, Shenzhen,
China, December 5-9, 2014. Proceedings, pages 103–
112, Berlin, Heidelberg. Springer Berlin Heidelberg.
Mi, C., Yang, Y., Zhou, X., Wang, L., Li, X., and Jiang, T.
(2016). Recurrent neural network based loanwords iden-
tiﬁcation in uyghur. In Proceedings of the 30th Paciﬁc
Asia Conference on Language, Information and Compu-
tation: Oral Papers, pages 209–217.

Taylor, J. R. and Grant, A. P. (2015). Lexical borrowing.

3579

