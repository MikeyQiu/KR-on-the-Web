A Modular Task-oriented Dialogue System
Using a Neural Mixture-of-Experts

Jiahuan Pei
University of Amsterdam
Amsterdam, The Netherlands
j.pei@uva.nl

Pengjie Ren
University of Amsterdam
Amsterdam, The Netherlands
p.ren@uva.nl

Maarten de Rijke
University of Amsterdam
Amsterdam, The Netherlands
derijke@uva.nl

9
1
0
2
 
l
u
J
 
0
1
 
 
]
L
C
.
s
c
[
 
 
1
v
6
4
3
5
0
.
7
0
9
1
:
v
i
X
r
a

ABSTRACT
End-to-end Task-oriented Dialogue Systems (TDSs) have attracted
a lot of attention for their superiority (e.g., in terms of global op-
timization) over pipeline modularized TDSs. Previous studies on
end-to-end TDSs use a single-module model to generate responses
for complex dialogue contexts. However, no model consistently
outperforms the others in all cases.

We propose a neural Modular Task-oriented Dialogue System
(MTDS) framework, in which a few expert bots are combined to
generate the response for a given dialogue context. MTDS consists
of a chair bot and several expert bots. Each expert bot is specialized
for a particular situation, e.g., one domain, one type of action of a
system, etc. The chair bot coordinates multiple expert bots and adap-
tively selects an expert bot to generate the appropriate response.
We further propose a Token-level Mixture-of-Expert (TokenMoE)
model to implement MTDS, where the expert bots predict multiple
tokens at each timestamp and the chair bot determines the final
generated token by fully taking into consideration the outputs of
all expert bots. Both the chair bot and the expert bots are jointly
trained in an end-to-end fashion.

To verify the effectiveness of TokenMoE, we carry out extensive
experiments on a benchmark dataset. Compared with the base-
line using a single-module model, our TokenMoE improves the
performance by 8.1% of inform rate and 0.8% of success rate.

CCS CONCEPTS
• Computing methodologies → Discourse, dialogue and prag-
matics; • Information systems → Search interfaces.

KEYWORDS
Task-oriented dialogue systems, Mixture of experts, Neural net-
works

ACM Reference Format:
Jiahuan Pei, Pengjie Ren, and Maarten de Rijke. 2019. A Modular Task-
oriented Dialogue System Using a Neural Mixture-of-Experts. In Proceedings
of WCIS ’19: 1st Workshop on Conversational Interaction Systems (WCIS ’19).
ACM, New York, NY, USA, 7 pages. https://doi.org/xx.xxx/xxx_x

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
WCIS ’19, July 25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-9999-9/18/06. . . $15.00
https://doi.org/xx.xxx/xxx_x

1 INTRODUCTION
As an important branch of spoken dialogue systems, Task-oriented
Dialogue Systems (TDSs) have raised considerable interest due to
their broad applicability, e.g., for booking flight tickets or scheduling
meetings [30, 33]. Unlike open-ended dialogue systems [23], TDSs
aim to assist users to achieve specific goals.

Existing TDS methods can be divided into two broad categories:
modularized pipeline TDSs [3, 6, 33] and end-to-end single-module
TDSs [12, 28]. The former decomposes the task-oriented dialogue
task into modularized pipelines that are addressed by separate mod-
els while the latter proposes to use an end-to-end model to solve
the task. End-to-end single-module TDSs have many attractive
characteristics, e.g., global optimization and easier adaptation to
new domains [6]. However, existing studies on end-to-end single-
module TDSs mostly generates a response token by token, where
each token is drawn from only one distribution over output vocab-
ulary. We think this is unreasonable because the distribution differs
a lot among different intents. Actually, more and more empirical
studies from different machine learning applications suggest that
no model consistently outperforms all others in all cases [9, 19].

Inspired by this intuition, we propose a new Modular Task-
oriented Dialogue System (MTDS) framework, as shown in Fig. 1.
MTDS consists of a chair bot and several expert bots. Each expert

Figure 1: Modular Task-oriented Dialogue System (MTDS)
framework.
bot is specialized for a particular situation, e.g., one domain, one
type of action of a system, etc. The chair bot coordinates multiple
expert bots and adaptively selects an expert bot to generate the
final response. Compared with existing end-to-end single-module
TDSs, the advantages of MTDSs are two-fold. First, the specializa-
tion of different expert bots and the use of a dynamic chair bot
for combining the outputs breaks the bottleneck of a single model.
Second, it is more easily traceable: we can analyze who is to blame
when the model makes a mistake. Under this framework, we further
propose a neural Mixture-of-Expert (MOE) model, namely Token-
level Mixture-of-Expert (TokenMoE), where the expert bots predict

WCIS ’19, July 25, 2019, Paris, France

Jiahuan Pei, Pengjie Ren, and Maarten de Rijke

Figure 2: Overview of TokenMoE. Figure (a) illustrates how does the model generate the token y6 given sequence X as an input.
Figure (b) shows how does the model generate the whole sequence Y as a dialogue response.

multiple tokens at each timestamp and the chair bot determines
the final generated token by taking into account the outputs of all
expert bots. We devise a global-and-local learning scheme to train
TokenMoE. We design a localized expert loss to force each expert to
specialize on a particular task. We also design a global chair loss to
differentiate the loss incurred from different experts [13].

To verify the effectiveness of TokenMoE, we carry out extensive
experiments on the MultiWOZ benchmark dataset. The results show
that, compared with the baseline using a single-module model, our
TokenMoE improves the performance by 8.1% of inform rate, and
0.8% of success rate.

The contributions of this paper can be summarized as follows.
• We propose the MTDS framework, which breaks the bottleneck
of a single-module model and provides better traceability of
mistakes.

• We present the TokenMoE model to implement MTDS at the

• We devise a global-and-local learning scheme to effectively train

token-level.

TokenMoE.

2 METHODOLOGY
Let D = {(Xp , Yp )} | D |
p=1 denote a dataset with |D| independent
random samples of (X , Y ), where X = (x1, . . . , xm ) is a sequence of
dialogue context with m words and Y = (y1, . . . , yn ) is a sequence
of system response with n words. The model aims to optimize the
generation probability of Y conditioned on X , i.e., p(Y |X ).

2.1 MTDS framework
We propose the MTDS framework, which consists of two types of
modules as shown in Figure 1:
• k expert bots, each of which is specialized for a particular sit-
uation, namely intent (e.g., one domain, one type of action of

a system, etc.). Those intents partition dataset D into k pieces
≜ {(X l , Y l )}. Each expert is trained to
S = {Sl }k
predict pl (Y l |X l ). We expect the l-th expert generally performs
better than the others on Sl .

l =1, where Sl

• a chair bot, which learns to coordinate a group of expert bots
to make an optimal decision. The chair bot is trained to predict
p(Y |X ), where (X , Y ) is any sample pair from D.

2.2 TokenMoE model
In this section, we introduce TokenMoE, a token-level implemen-
tation of the MTDS framework. As shown in Figure 2, TokenMoE
consists of three types of components, i.e., a shared encoder, k
expert decoders, and a chair decoder.

Shared context encoder. The role of shared context encoder is
to read the dialogue context sequence and construct their repre-
sentations at each timestamp. Here we follow Budzianowski et al.
[5] and employ a Long Short-Term Memory (LSTM) [14] to map
the input sequence X to hidden vectors {h1, . . . , hm }. The hidden
vector hi at timestep i-th can be represented as:

hi , si = LSTM(emb(xi ), hi−1, si−1),
where emb(xi ) is the embedding of the token xi at i. The initial
state of LSTM s0 is set to 0.
Expert decoder. The l-th expert outputs the probability pl
the vocabulary set V at j-th step by:

j over

(1)

= softmax(UT ol
pl
+ a),
j
j
j−1 ⊕ cl
= LSTM(yl
j , sl
ol
j
where U, a are learnable matrices. sl
initialized by the last state of the shared context encoder. yl

j is the state vector which is

j−1, sl

j−1),

j , ol

(2)

j−1

A Modular Task-oriented Dialogue System

WCIS ’19, July 25, 2019, Paris, France

is the generated token at timestamp j − 1 by expert l. cl
j is the
context vector which is calculated with a concatenation attention
mechanism [1, 18] over the hidden representations from shared
context encoder.

=

cl
j

α l
ji hi ,

m
(cid:213)

i=1

=

(cid:205)m

exp(wl
ji )
i=1 exp(wl
= vT tanh (WT (hi ⊕ sl

ji )

,

α l
ji

wl
ji

j−1) + b),

(3)

(4)

(5)

where α is the attention weights; ⊕ is the concatenation operation.
W, b, v are learnable parameters, which are not shared by different
experts in our experiments.

Chair decoder. The chair decoder estimates the final token pre-
diction distribution pj by combining prediction distribution of all
experts (including chair itself) with a proposed token-level Mixture-
of-Expert (MOE) scheme. As shown in Fig. 2(b), following the typ-
ical neural MOE architecture [22, 24], pj is computed based on
the state sl
j from all experts
(including chair itself) at j as follows.

j and token prediction distribution pl

pj =

βl
j · pl
j ,

k+1
(cid:213)

l =1

j

where pk +1
architecture of the other experts but is trained on all data. βl
normalized importance scores that can be computed as:

is the prediction of the chair, which employs the same
j is the

=

βl
j

l ue,l )

exp(uT
(cid:205)k
b=1 exp(uT

b ue,l )

,

ul
h = s1

= MLP(h),
j ⊕ p1

j ⊕ · · · ⊕ sk

j ⊕ pk

j ⊕ sk +1

j

⊕ pk +1
j

,

where ue,l is an expert-specific, learnable vector that reflects which
dimension of the projected hidden representation is highlighted for
the expert.

Loss function. We devise a global-and-local learning scheme to
train TokenMoE. Each expert l is optimized by a localized expert
loss defined on Sl , which forces each expert to specialize on one of
the portions of data Sl . We use cross-entropy loss for each expert
and the joint loss for all experts are as follows.

Lexperts =

k+1
(cid:213)

(cid:213)

l =1

(X l ,Y l )∈Sl

n
(cid:213)

j=1

µkyl

j log pl
j ,

(6)

j is the token prediction by expert l (Eq. 2) computed on
is a one-hot vector indicating the ground

where pl
the r -th data sample; yl,r
j
truth token at j; and µk is the weight of the k-th expert.

We also design the global chair loss to differentiate the loss
incurred from different experts. The chair can attribute the source
of errors to the expert in charge. For each data sample in D, we
follow the MOE architecture and calculate the combined taken

Table 1: Different settings of learning schemes.

Variants

pj (Eq. 2)

S1
S2
S3
S4

MOE
MOE
w/o MOE
MOE

µk (Eq. 6)
learnable
–
1
k
1
k

λ (Eq. 8)

learnable
0.0
0.5
0.5

(7)

(8)

prediction pj (Eq. 4). Then the total loss incurred by MOE can be
denoted as follows.

Lchair =

yj log pj .

| D |
(cid:213)

n
(cid:213)

r =1

j=1

Our overall optimization follows the joint learning paradigm that
is defined as a weighted combination of constituent loss.

L = λ · Lexperts + (1 − λ) · Lchair ,

where λ is a hyper-parameter.

3 EXPERIMENTAL SETUP
3.1 Research questions
We seek to answer the following research questions.

(RQ1) Is there a single model that consistently outperforms the
others on all domains? The point of this question is to verify
the motivation behind MTDS and TokenMoE.

(RQ2) Does the TokenMoE model outperform the state-of-the-art
end-to-end single-module TDS model? The point of this
question is to determine the effectiveness of the proposed
TokenMoE model.

(RQ3) How do the proposed token-level MOE scheme (Eq. 4 and
Eq. 5) and the global-and-local learning scheme (Eq. 6 and
Eq. 8) in the TokenMoE model affect the final performance?
The point of this question is to do an ablation study on
effective learning schemes.

3.2 Comparison methods
We use the dominant Sequence-to-Sequence (Seq2Seq) model in
an encoder-decoder architecture [6] and reproduce the state-of-
the-art single model baseline, namely Sequence-to-Sequence with
Attention Using LSTM (S2SAttnLSTM) [4, 5], based on the source
code provided by the authors.1

To answer RQ1, we investigate the performance of the following

variants of S2SAttnLSTM on different domains.
• V1. This variant excludes the attention mechanism from the

baseline model and keeps the other settings unchanged.

• V2. This variant changes the LSTM cell as GRU and keeps the

• V3. This variant reduces the number of hidden units to 100 and

other settings the same.

maintains the other settings.

To answer RQ2, we train TokenMoE based on the benchmark
dataset and test how it performs compared to the single-module
baseline.

1https://github.com/budzianowski/multiwoz. For fair comparison, we remove valida-
tion set from training set and report the reproduced results.

WCIS ’19, July 25, 2019, Paris, France

Jiahuan Pei, Pengjie Ren, and Maarten de Rijke

Table 2: Performance of the single-module baseline (S2SAttnLSTM) and its three variations (V1, V2 and V3) on different do-
mains. Bold highlighted results indicate a statistically significant improvement of a metric over the strongest baseline on the
same domain (paired t-test, p < 0.01). UNK denotes a unknown domain excluding the domains described in §3.4. Please note
that the number of the evaluated dialogue turns varies among different domains.

Inform (%)
/V1

/V2

Baseline

/V3 Baseline

Success (%)
/V1

/V2

V3 Baseline

BLEU (%)
/V1

/V2

/V3 Baseline

/V2

/V3

# of turns

Score
/V1

Attraction
Hotel
Restaurant
Taxi
Train
Booking
General
UNK

87.20 86.20 91.80 88.70
89.90 93.90 89.90 90.30
89.20 91.70 86.40 86.10
100.00 100.00 100.00 100.00
77.70 77.70 79.00 81.60
100.00 100.00 100.00 100.00
100.00 100.00 100.00 100.00
100.00 100.00 100.00 100.00

81.30 74.80 83.70 83.70
87.50 91.70 87.40 89.10
85.80 87.80 84.00 83.40
99.90 99.80 99.90 99.80
75.60 74.80 77.20 79.60
100.00 100.00 100.00 100.00
100.00 100.00 100.00 100.00
100.00 100.00 100.00 100.00

15.14 14.95 16.08 14.86
16.60 15.60 15.11 14.13
17.07 17.70 16.07 17.34
17.33 19.18 20.13 18.32
20.35 15.64 22.81 20.62
22.05 21.61 21.96 22.06
20.21 19.53 20.13 20.80
12.40 11.75 13.12 11.80

99.39 95.45 103.83 101.06
105.30 108.40 103.76 103.83
104.57 107.45 101.27 102.09
117.28 119.08 120.08 118.22
97.00 91.89 100.91 101.22
122.05 121.61 121.96 122.06
120.21 119.53 120.13 120.80
112.40 111.75 113.12 111.80

1042
1068
1024
395
1702
1407
2596
81

To answer RQ3, we explore different settings of the learning
schemes by considering alternative choices of pj in Eq. 4, µk in
Eq. 6 and λ in Eq. 8. We summarize different variants in Table 1.

In this work, we are focusing on context-to-act task [5], so natural
language generation (NLG) baselines (e.g., SC-LSTM [27]) will not
be taken into consideration.
3.3 Implementation details
The vocabulary size is the same as in the original paper that releases
the dataset [5], which has 400 tokens. Out-of-vocabulary words
are replaced with “<UNK>”. We set the word embedding size to
50 and all LSTM hidden state sizes to 150. We use Adam [15] as
our optimization algorithm with hyperparameters α = 0.005, β1 =
0.9, β2 = 0.999 and ϵ = 10−8. We also apply gradient clipping
[20] with range [–5, 5] during training. We use l2 regularization
to alleviate overfitting, the weight of which is set to 0.00001. We
set mini-batch size to 64. We use greedy search to generate the
response during testing. Extra techniques (e.g., beam search) are
not incorporated, because our main concern is the modular model
outperforms single-module model instead of the effectiveness of
these popular techniques.
3.4 Dataset
Our experiments are conducted on the Multi-Domain Wizard-of-
Oz (MultiWOZ) [5] dataset. This is the latest large-scale human-to-
human TDS dataset with rich semantic labels (e.g., domains and
dialogue actions) and benchmark results of response generation.2
MultiWOZ consists of ∼10k natural conversations between a tourist
and a clerk. We consider 6 specific action-related domains (i.e., At-
traction, Hotel, Restaurant, Taxi, Train, and Booking) and 1 universal
domain (i.e., General). 67.37% of dialogues are cross-domain which
covers 2–5 domains on average. The average number of turns per
dialogue is 13.68 and a turn contains 13.18 tokens on average. To
facilitate reproducibility of the results, the dataset is randomly split
into into 8,438/1,000/1,000 dialogues for training, validation, and
testing, respectively.
3.5 Evaluation metrics
We use three commonly used evaluation metrics [5]:
• Inform. The fraction of responses that provide a correct entity

out of all responses.

2http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/

• Success. The fraction of responses that answer all the requested

• BLEU. This is a score for comparing a generated response to one

attributes out of all responses.

or more reference responses.

Following Budzianowski et al. [4], we use Score = 0.5*Inform+
0.5*Success+BLEU as the selection criterion to choose the best model
on the validation set and report the performance of the model on
the test set. We utilize a paired t-test to show statistical significance
(p < 0.01) of relative improvements.

4 RESULTS
This section describes the results of our experiments and answers
research questions proposed in §3.

4.1 Performance of single-module TDSs on

different domains (RQ1)

To answer RQ1, we assess the performance of the single-module
baseline S2SAttnLSTM and its three variants with settings (V1, V2,
and V3) described in §3.2 on different domains. The results are
shown in Table 2.

We can see that none of those models can consistently outper-
form the others on all domains and all metrics. That is to say, a
model can achieve its best performance only in some particular sit-
uations. To be specific, S2SAttnLSTM achieves its best performance
only on the Hotel domain in terms of BLEU and Taxi domain in
terms of Success. S2SAttnLSTM/V1 outperforms all other models
on the Restaurant domain on all metrics and on the Hotel domain
(except for BLEU ). S2SAttnLSTM/V2 beats the others on the At-
traction and Taxi domains in terms of all metrics. S2SAttnLSTM/V3
performs best on the Booking and General domains in terms of
BLEU and Score. Overall, S2SAttnLSTM/V1 specializes in, and leads
on, the Hotel and Restaurant domains. S2SAttnLSTM/V2 acts as an
expert bot specialized for the Attraction, Taxi, UNK domains, and
S2SAttnLSTM/V2 serves as an expert bot for the Train, Booking,
General domains. Generally, the experimental results verify the
assumption and motivation of our MTDS framework.

4.2 Overall performance (RQ2)
To answer our main research question, RQ2, we evaluate the perfor-
mance of TokenMoE and the baselines (S2SAttnLSTM, TokenMoE

A Modular Task-oriented Dialogue System

WCIS ’19, July 25, 2019, Paris, France

and their variants with settings V1, V2, V3). The results are shown
in Table 3.
Table 3: Comparison between TokenMoE, the benchmark
baseline S2SAttnLSTM, and their variant models using set-
ting V1, V2, V3, respectively. Bold results indicate a statis-
tically significant improvement over the strongest baseline
(paired t-test, p < 0.01).

Inform (%) Success (%) BLEU (%) Score

Table 4: An example of the generated responses of
S2SAttnLSTM and TokenMoE. A user would prefer to get de-
tail information of the train before booking a ticket.

Model

Response

S2SAttnLSTM

TokenMOE

i have [value_count] trains that match your criteria .
would you like me to book it for you ?
i have train [train_id] that leaves at [value_time] and
arrives at [value_time] . would you like me to book it ?

S2SAttnLSTM [5] 67.20
S2SAttnLSTM/V1 63.60
S2SAttnLSTM/V2 67.20
S2SAttnLSTM/V3 68.60
64.00
TokenMoE/V1
62.60
TokenMoE/V2
62.90
TokenMoE/V3

57.20
52.20
58.90
59.30
52.50
54.30
54.00

TokenMoE

75.30

59.70

17.83
18.10
20.85
19.41
18.95
18.90
18.34

16.81

80.03
76.00
83.90
83.36
77.20
77.35
76.79

84.31

First, TokenMoE outperforms all baseline models by a large mar-
gin in terms of all metrics. Especially, TokenMoE significantly out-
performs the benchmark single-module baseline S2SAttnLSTM, by
8.1% of Inform and 2.5% of Sucecess, which maintains the same
settings as the original paper [5]. This shows that TokenMoE has
an advantage of task completion by providing more appropriate
entities and answering the requested attributes as many as possible.
Second, TokenMoE greatly outperforms TokenMoE/V1 by 11.7%
on Inform and 7.5% on Success. This is true with S2SAttnLSTM and
S2SAttnLSTM/V1 except that the improvements are smaller, i.e.,
3.6% on Inform and 5.0% on Sucecess. On the one hand, this means
that the attention mechanism is effective. On the other hand, this
also shows that the attention mechanisms under our TokenMoE can
be more effective and have an even more important role to play. That
is to say, the MTDS framework has more potential to improve by
separating the modeling of expert and chair bots. TokenMoE/V2 is
inferior to S2SAttnLSTM/V2 when changing the LSTM cell as GRU.
Similarly, TokenMoE/V3 is less effective than S2SAttnLSTM/V3
when decreasing the number of hidden units. This indicates that
TokenMoE is more sensitive to the number of parameters, which
we think is due to the fact that TokenMoE has a hard time learning
the expert bots and their coordination with a small parameter space.
Third, all models achieve about 10% higher values in terms of
Inform than in terms of Success. This shows that the big challenge of
the dialogue generation task is how to answer requested attributes
in a real-time manner. The BLEU scores of all models are quite low
compared with the state-of-the-art result (45.6%) of machine trans-
lation [11] but are similar to the state-of-the-art result (18.9%) for
dialogue generation [5]. This supports prior claims that the BLEU
score is not an ideal measurement for dialogue generation and ex-
plains the reason why we use Score to choose our best model. Table
4 shows an example of the baseline S2SAttnLSTM and TokenMoE
output, which indicates that a lower BLEU still does mean more
appropriate response with more detail information.

4.3 Exploration of learning schemes for

TokenMoE (RQ3)

To address RQ3, we explore how token-level MOE and learning
schemes used in TokenMoE affect the performance. In Table 5, we

report the results of four variants of TokenMoE (see Table 1). The
detailed settings of each variant are as follows:
• S1 regards µk and λ as learnable parameters while the others

regard them as hyperparameters.

• S2 uses token-level MOE but does not use the global-and-local

learning scheme, i.e., λ = 0 in Eq. 8.

• S3 does not use token-level MOE and directly uses the prediction
probability of the chair bot, i.e., pj = pk +1
in Eq. 4, which actually
degenerates into S2SAttnLSTM with the proposed global-and-
local learning.

j

• S4 uses both token-level MOE and global-and-local learning.

Table 5: Comparison of TokenMoE with different learn-
ing schemes (S1, S2, S3, S4) and the benchmark baseline
S2SAttnLSTM. Bold results indicate a statistically signifi-
cant improvement over the strongest baseline (paired t-test,
p < 0.01).

Inform (%) Success (%) BLEU Score

S2SAttnLSTM/V2 67.20

TokenMoE/S1
TokenMoE/S2
TokenMoE/S3
TokenMoE/S4

66.20
66.50
70.60
75.30

58.90

54.90
56.90
60.60
59.70

20.85

83.90

19.11
19.48
18.67
16.81

79.66
81.18
84.27
84.31

First, S1 is worse than the other three variants on all metrics,
which shows that it is not effective to learn µk and λ. The reason is
that TokenMoE may fall into the optimization trap due to learning
µk and λ. That is, TokenMoE learns a very small weight for the
local loss of each expert (i.e., µk ≈ 0) and a large weight for the
global loss of the chair bot (i.e., λ ≈ 1). Afterwards, this loss will
never decrease any more, so the model learns nothing useful.

Second, S2 is even worse than S2SAttnLSTM/V2 on all metrics
which means the performance cannot be improved with the pro-
posed token-level MOE alone. We believe the reason is that token-
level MOE makes the model harder to learn, i.e., the model needs to
learn not only each prediction distribution by the expert and chair
bots but also their combinations. This can be verified by the fact
that with token-level MOE and global-and-local learning, S4 fur-
ther improves Inform by 4.7% compared with S3. Our explanation
is that the global-and-local learning makes token-level MOE easier
to learn by incorporating supervisions on both the prediction dis-
tribution of each expert (local loss in Eq. 6) and their combination
(global loss in Eq. 8).

Third, S3 is better than S2SAttnLSTM/V2 in terms of Inform and
Success. Also, it achieves the best performance on Success. This
shows that TokenMoE with an appropriate scheme is expert in task
accomplishment for a TDS, i.e., TokenMoE/S3 is able to generate

WCIS ’19, July 25, 2019, Paris, France

Jiahuan Pei, Pengjie Ren, and Maarten de Rijke

more correct entities and answer more requested attributes. The
reason behind this is quite clear: with global-and-local learning,
each expert is trained to specialize on a particular domain, which
means the chair and the experts are able to extract more manifold
candidate tokens, each of them holds a unique preference distribu-
tion over the output vocabulary. For example, a Booking expert has
a high probability to produce the intent-oriented token “booked”
in the response “Your order has been booked”. In contrast, with-
out global-and-local learning, the single model prefers to generate
more generic tokens (e.g., “thanks”) that occur most frequently in
all domains.

However, it is worth noting that both S2 and S3 are worse than
S2SAttnLSTM/V2 on BLEU and S4 is even worse than S2 and S3.
This indicates that token-level MOE and global-and-local learning
have a negative influence on the response fluency evaluated by
BLEU. A possible reason is that various candidate tokens from the
chair and experts make the dialogue contexts more complex, which
increases the difficulty of generating a fluent response. Another
reason is that BLEU is not an ideal metric for dialogue generation
task, as we discussed in §4.2.

5 RELATED WORK
There are two dominant frameworks for TDSs: modularized pipeline
TDSs and end-to-end single-module TDSs.

5.1 Modularized pipeline TDSs
Modularized pipeline TDSs frameworks consists of a pipeline with
several modules. Examples include Natural Language Understand-
ing (NLU) [2, 7], Dialogue State Tracking (DST) [21, 34], Policy
Learning (PL), and Natural Language Generation (NLG) [10, 32].
Each module has an explicitly decomposed function for a specialized
subtask, which is beneficial to track errors. Young et al. [33] summa-
rize typical pipeline TDSs that are constitutive of distinct modules
following a POMDP paradigm. Crook et al. [8] develop a TDS plat-
form that is loosely decomposed into three modules, i.e., initial pro-
cessing of input, dialogue state updates, and policy execution. Yan
et al. [31] present a TDS for completing various purchase-related
tasks by optimizing individual upstream-dependent modules, i.e.,
query understanding, state tracking and dialogue management.
However, the pipeline setting of these methods will unavoidably
incur upstream propagation problem [6], module interdependence
problem [6] and joint evaluation problem [33]. Unlike the methods
listed above, our MTDS constists of a group of modules including a
chair bot and several expert bots. This design addresses the module
interdependence problem since each module is independent among
the others. Besides, the chair bot alleviates the error propagation
problem because it is able to manage the overall errors through an
effective learning schemes.

5.2 End-to-end single-module TDSs
End-to-end single-module systems address the TDS task with only
one module, which maps a dialogue context to a response directly
[29]. There is a growing focus in research on end-to-end approaches
for TDSs, which can enjoy global optimization and facilitate eas-
ier adaptation to new domains [6]. Sordoni et al. [25] show that

using an Recurrent Neural Network (RNN) to generate text condi-
tioned on the dialogue history results in more natural conversa-
tions. Later improvements have been made by adding an attention
mechanism [17, 26], by modeling the hierarchical structure of di-
alogues [23], or by jointly learning belief spans [16]. However,
existing studies on end-to-end TDSs mostly use a single-module
underlying model to generate responses for complex dialogue con-
texts. This is practically problematic because dialogue contexts are
very complicated with multiple sources of information [7]. In addi-
tion, previous studies show that it is abnormal to find a single model
that achieves the best results on the overall task based on empirical
studies from different machine learning applications [9, 19].

Different from the methods listed above, which use a single
module to achieve TDSs, our MTDS uses multiple modules (expert
and chair bots), which makes good use of the specialization of
different experts and the generalization of chair for combining the
final outputs. Besides, our MTDS model is able to track who is to
blame when the model makes a mistake.

6 CONCLUSION AND FUTURE WORK
This paper we have presented a neural Modular Task-oriented
Dialogue System (MTDS) framework composed of a chair bot and
several expert bots. We have developed a TokenMoE model under
this MTDS framework, where the expert bots make multiple token-
level predictions at each timestamp and the chair bot predicts the
final generated token by fully considering the whole outputs of
all expert bots. Both the chair bot and the expert bots are jointly
trained in an end-to-end fashion.

We have conducted extensive experiments on the benchmark
dataset MultiWOZ and evaluated the performance in terms of four
automatic metrics (i.e., Inform, Success, BLEU, and Score). We find
that no general single-module TDS model can constantly outper-
form the others on all metrics. This empirical observation facilitates
the design of a new framework, i.e., MTDS framework. We also
verify the effectiveness of TokenMoE model compared with the
baseline using a single-module model. Our TokenMoE outperforms
the best single-module model (S2SAttnLSTM/V2) by 8.1% of in-
form rate and 0.8% of success rate. Besides, it significantly beats
S2SAttnLSTM, the benchmark single-module baseline, by 3.5% of
Inform and 4.2% of Sucecess. In addition, the experimental results
show that learning scheme is an important factor of our TokenMoE
model.

In the future work, we hope to explore Sentence-level Mixture-of-
Expert (SentenceMoE) and combine it with the current TokenMoE
to see whether the hybrid model will further improve the perfor-
mance. Besides, we plan to try more fine-grained expert bots (e.g.,
according to user intents or system actions) and more datasets to
test our new framework and model.

ACKNOWLEDGMENTS
This research was partially supported by Ahold Delhaize, the As-
sociation of Universities in the Netherlands (VSNU), the China
Scholarship Council (CSC), and the Innovation Center for Artifi-
cial Intelligence (ICAI). All content represents the opinion of the
authors, which is not necessarily shared or endorsed by their re-
spective employers and/or sponsors.

A Modular Task-oriented Dialogue System

WCIS ’19, July 25, 2019, Paris, France

Intelligence (AAAI ’16). 3776–3784.

[24] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The
sparsely-gated mixture-of-experts layer. In International Conference on Learning
Representations (ICLR ’17). –.

[25] Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji,
Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015. A neural
network approach to context-sensitive generation of conversational responses. In
Proceedings of the 2015 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies (NAACL-HLT ’15).
196–205.

[26] Oriol Vinyals and Quoc Le. 2015. A neural conversational model. In ICML Deep

Learning Workshop. –.

[27] Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke,
and Steve Young. 2015. Semantically conditioned lstm-based natural language
generation for spoken dialogue systems. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing (EMNLP ’15). 1711–1721.
[28] Tsung-Hsien Wen, David Vandyke, Nikola Mrkšić, Milica Gasic, Lina M Rojas
Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2017. A network-based
end-to-end trainable task-oriented dialogue system. In Proceedings of the 15th
Conference of the European Chapter of the Association for Computational Linguistics
(EACL ’17). 438–449.

[29] Tsung-Hsien Wen, David Vandyke, Nikola Mrkšić, Milica Gasic, Lina M Rojas
Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2017. A network-based
end-to-end trainable task-oriented dialogue system. In Proceedings of the 15th
Conference of the European Chapter of the Association for Computational Linguistics
(EACL ’17). 438–449.

[30] Jason D Williams, Kavosh Asadi, and Geoffrey Zweig. 2017. Hybrid code net-
works: practical and efficient end-to-end dialog control with supervised and
reinforcement learning. In Proceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL ’17). 665–677.

[31] Zhao Yan, Nan Duan, Peng Chen, Ming Zhou, Jianshe Zhou, and Zhoujun Li.
2017. Building task-oriented dialogue systems for online shopping. In Thirty-First
AAAI Conference on Artificial Intelligence (AAAI ’2017). 4618–4626.

[32] Sanghyun Yi, Rahul Goel, Chandra Khatri, Tagyoung Chung, Behnam Hedayatnia,
Anu Venkatesh, Raefer Gabriel, and Dilek Hakkani-Tur. 2019. Towards coherent
and engaging spoken dialog response generation using automatic conversation
evaluators. arXiv preprint arXiv:1904.13015 (2019).

[33] Steve Young, Milica Gašić, Blaise Thomson, and Jason D Williams. 2013. POMDP-
based statistical spoken dialog systems: A review. Proc. IEEE 101, 5 (2013),
1160–1179.

[34] Victor Zhong, Caiming Xiong, and Richard Socher. 2018. Global-locally self-
attentive encoder for dialogue state tracking. In Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics (ACL ’18). 1458–1467.

REFERENCES
[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine
translation by jointly learning to align and translate. In International Conference
on Learning Representations (ICLR ’15). –.

[2] Ankur Bapna, Gokhan Tur, Dilek Hakkani-Tur, and Larry Heck. 2017. Sequential
dialogue context modeling for spoken language understanding. In Proceedings
of the 18th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL ’17).
103–114.

[3] Antoine Bordes and Jason Weston. 2017. Learning end-to-end goal-oriented
dialog. In International Conference on Learning Representations (ICLR ’17). –.
[4] Pawel Budzianowski, Iñigo Casanueva, Bo-Hsiang Tseng, and Milica Gasic. 2018.
Towards end-to-end multi-domain dialogue modelling. Technical Report. Cam-
bridge University.

[5] Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva,
Stefan Ultes, Osman Ramadan, and Milica Gasic. 2018. MultiWOZ-A large-
scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling.
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing (EMNLP ’18). 5016–5026.

[6] Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. 2017. A survey on
dialogue systems: Recent advances and new frontiers. ACM SIGKDD Explorations
Newsletter 19, 2 (2017), 25–35.

[7] Po-Chun Chen, Ta-Chung Chi, Shang-Yu Su, and Yun-Nung Chen. 2017. Dynamic
time-aware attention to speaker roles and contexts for spoken language under-
standing. In Proceedings of 2017 IEEE Workshop on Automatic Speech Recognition
and Understanding (ASRU ’17). 554–560.

[8] Paul Crook, Alex Marin, Vipul Agarwal, Khushboo Aggarwal, Tasos Anastasakos,
Ravi Bikkula, Daniel Boies, Asli Celikyilmaz, Senthilkumar Chandramohan,
Zhaleh Feizollahi, et al. 2016. Task completion platform: A self-serve multi-
domain goal oriented dialogue platform. In Proceedings of the 2016 Conference
of the North American Chapter of the Association for Computational Linguistics
(NAACL ’16). 47–51.

[9] Thomas G Dietterich. 2000. Ensemble methods in machine learning. In Proceed-
ings of the First International Workshop on Multiple Classifier Systems (MCS ’00).
1–15.

[10] Ondrej Dušek and Filip Jurcıcek. 2016. A context-aware natural language genera-
tor for dialogue systems. In Proceedings of the 17th Annual Meeting of the Special
Interest Group on Discourse and Dialogue (SIGDIAL ’16). 185–190.

[11] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding
back-translation at scale. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing (EMNLP ’18). 489–500.

[12] Mihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D Manning.
2017. Key-value retrieval networks for task-oriented dialogue. In Proceedings of
the 18th Annual Meeting of the Special Interest Group on Discourse and Dialogue
(SIGDIAL ’17). 37–49.

[13] Jiang Guo, Darsh J Shah, and Regina Barzilay. 2018. Multi-source domain adapta-
tion with mixture of experts. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing (EMNLP ’18). 4694–4703.

[14] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural

Computation 9, 8 (1997), 1735–1780.

[15] Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimiza-
tion. In International Conference on Learning Representations (ICLR ’15). –.
[16] Wenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun Ren, Xiangnan He, and Dawei
Yin. 2018. Sequicity: Simplifying task-oriented dialogue systems with single
sequence-to-sequence architectures. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (ACL ’18). 1437–1447.

[17] Jiwei Li, Michel Galley, Chris Brockett, Georgios P Spithourakis, Jianfeng Gao,
and Bill Dolan. 2016. A persona-based neural conversation model. In Proceedings
of the 54th Annual Meeting of the Association for Computational Linguistics (ACL
’16). 994–1003.

[18] Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective ap-
proaches to attention-based neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural Language Processing (EMNLP
’15). 1412–1421.

[19] Saeed Masoudnia and Reza Ebrahimpour. 2014. Mixture of experts: a literature

survey. Artificial Intelligence Review 42, 2 (2014), 275–293.

[20] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty
of training recurrent neural networks. In Proceedings of the 30th International
Conference on Machine Learning (ICML ’13). 1310–1318.

[21] Abhinav Rastogi, Raghav Gupta, and Dilek Hakkani-Tur. 2018. Multi-task learning
for joint language understanding and dialogue state tracking. In Proceedings of the
19th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL ’19). 376–384.
[22] Patrick Schwab, Djordje Miladinovic, and Walter Karlen. 2019. Granger-causal
attentive mixtures of experts: Learning important features with neural networks.
In AAAI Conference on Artificial Intelligence (AAAI ’19). –.

[23] Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C Courville, and
Joelle Pineau. 2016. Building end-to-end dialogue systems using generative
hierarchical neural network models. In Thirtieth AAAI Conference on Artificial

