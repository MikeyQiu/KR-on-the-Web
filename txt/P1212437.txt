8
1
0
2
 
p
e
S
 
9
1
 
 
]

R

I
.
s
c
[
 
 
3
v
2
3
2
2
0
.
5
0
8
1
:
v
i
X
r
a

Discrete Factorization Machines for Fast Feature-based Recommendation ∗

Han Liu1, Xiangnan He2, Fuli Feng2, Liqiang Nie1, Rui Liu3, Hanwang Zhang4
1School of Computer Science and Technology, Shandong University
2School of Computing, National University of Singapore
3University of Electronic Science and Technology of China
4School of Computer Science and Engineering, Nanyang Technological University
{hanliu.sdu, xiangnanhe, fulifeng93, nieliqiang, ruiliu011, hanwangzhang}@gmail.com

Abstract

User and item features of side information are cru-
cial for accurate recommendation. However, the
large number of feature dimensions, e.g., usually
larger than 107, results in expensive storage and
computational cost. This prohibits fast recommen-
dation especially on mobile applications where the
computational resource is very limited. In this pa-
per, we develop a generic feature-based recommen-
dation model, called Discrete Factorization Ma-
chine (DFM), for fast and accurate recommenda-
tion. DFM binarizes the real-valued model param-
eters (e.g., ﬂoat32) of every feature embedding into
binary codes (e.g., boolean), and thus supports efﬁ-
cient storage and fast user-item score computation.
To avoid the severe quantization loss of the bina-
rization, we propose a convergent updating rule that
resolves the challenging discrete optimization of
DFM. Through extensive experiments on two real-
world datasets, we show that 1) DFM consistently
outperforms state-of-the-art binarized recommen-
dation models, and 2) DFM shows very competi-
tive performance compared to its real-valued ver-
sion (FM), demonstrating the minimized quantiza-
tion loss.

1 Introduction
Recommendation is ubiquitous in today’s cyber-world — al-
most every one of your Web activities can be viewed as a
recommendation, such as news or music feeds, car or restau-
rant booking, and online shopping. Therefore, accurate rec-
ommender system is not only essential for the quality of
service, but also the proﬁt of the service provider. One
such system should exploit the rich side information beyond
user-item interactions, such as content-based (e.g., user at-
tributes [Wang et al., 2017] and product image features [Yu et
al., 2018]), context-based (e.g., where and when a purchase is
made [Rendle et al., 2011; He and Chua, 2017]), and session-
based (e.g., the recent browsing history of users [Li et al.,

∗This work is accepted by IJCAI 2018.

2017; Bayer et al., 2017]). However, existing collaborative
ﬁltering (CF) based systems merely rely on user and item fea-
tures (e.g., matrix factorization based [He et al., 2016] and the
recently proposed neural collaborative ﬁltering methods [He
et al., 2017; Bai et al., 2017]), which are far from sufﬁcient
to capture the complex decision psychology of the setting and
the mood of a user behavior [Chen et al., 2017].

Factorization Machine (FM) [Rendle, 2011] is one of the
prevalent feature-based recommendation model that lever-
ages rich features of users and items for accurate recommen-
dation. FM can incorporate any side features by concatenat-
ing them into a high-dimensional and sparse feature vector.
The key advantage of it is to learn k-dimensional latent vec-
tors , i.e., the embedding parameters V ∈ Rk×n, for all the
n feature dimensions. They are then used to model pairwise
interactions between features in the embedding space. How-
ever, since n is large (e.g. practical recommender systems
typically need to deal with over millions of items and other
features where n is at least 107 [Wang et al., 2018]), it is im-
possible on-device storage of V. Moreover, it requires large-
scale multiplications of the feature interaction vT
i vj for user-
item score, even linear time-complexity is prohibitively slow
for ﬂoat operations. Therefore, existing FM framework is not
suitable for fast recommendation, especially for mobile users.

In this paper, we propose a novel feature-based recommen-
dation framework, named Discrete Factorization Machine
In a nutshell, DFM re-
(DFM), for fast recommendation.
places the real-valued FM parameters V by binary-valued
B ∈ {±1}k×n. In this way, we can easily store a bit matrix
and perform XOR bit operations instead of ﬂoat multiplica-
tions, making fast recommendation on-the-ﬂy possible. How-
ever, it is well-known that the binarization of real-valued pa-
rameters will lead to signiﬁcant performance drop due to the
quantization loss [Zhang et al., 2016]. To this end, we pro-
pose to directly optimize the binary parameters in an end-to-
end fashion, which is fundamentally different from the widely
adopted two-stage approach that ﬁrst learns real-valued pa-
rameters and then applies round-off binarization [Zhang et
al., 2014]. Our algorithm jointly optimize the two challeng-
ing objectives: 1) to tailor the binary codes B to ﬁt the origi-
nal loss function of FM, and 2) imposing the binary constraint
that is balanced and decorrelated, to encode compact infor-

mation. In particular, we develop an alternating optimization
algorithm to iteratively solve the mixed-integer programming
problems. We evaluate DFM on two real-world datasets Yelp
and Amazon, the results demonstrate that 1) DFM consis-
tently outperforms state-of-the-art binarized recommendation
models, and 2) DFM shows very competitive performance
compared to its real-valued version (FM), demonstrating the
minimized quantization loss.

Our contributions are summarized as follows:

• We propose to binarize FM, a dominant feature-based rec-
ommender model, to enable fast recommendation. To our
knowledge, this is the ﬁrst generic solution for fast recom-
mendation that learns a binary embedding for each feature.
• We develop an efﬁcient algorithm to address the challeng-
ing optimization problem of DFM that involves discrete,
balanced, and de-correlated constraints.

• Through extensive experiments on two real-world datasets,
we demonstrate that DFM outperforms state-of-the-art
hash-based recommendation algorithms.

2 Related Work
We ﬁrst review efﬁcient recommendation algorithms using la-
tent factor models, and then discuss recent advance in discrete
hashing techniques.

2.1 Efﬁcient Recommendation
As pioneer work, [Das et al., 2007] used Locality-Sensitive
Hashing (LSH) [Gionis et al., 1999] to generate hash codes
for Google new users based on their item-sharing history sim-
ilarity. Following the work, [Karatzoglou et al., 2010] applied
random projection for mapping learned user-item latent fac-
tors from traditional CF into the Hamming space to acquire
hash codes for users and items. Similar to the idea of projec-
tion, [Zhou and Zha, 2012] generate binary code from rotated
continuous user-item latent factors by running ITQ [Gong and
Lazebnik, 2011].
In order to derive more compact binary
codes, [Liu et al., 2014] imposed the de-correlation constraint
of different binary codes on continuous user-item latent fac-
tors and then rounded them to produce binary codes. How-
ever, [Zhang et al., 2014] argued that hashing only preserves
similarity between user and item rather than inner product
based preference, so subsequent hashing may harm the accu-
racy of preference predictions, thus they imposed a Constant
Feature Norm(CFN) constraint on user-item continuous la-
tent factors, and then quantized similarities by respectively
thresholding their magnitudes and phases.

The aforementioned work can be easily summarized as two
independents stages: relaxed user-item latent factors learning
with some speciﬁc constraints and binary quantization. How-
ever, such a two-stage relaxation is well-known to suffer from
a large quantization loss according to [Zhang et al., 2016].

2.2 Binary Codes Learning
Direct binary code learning by discrete optimization — is
becoming popular recently in order to decrease quantization
loss aforementioned. Supervised hashing [Luo et al., 2018]
improve on joint optimizations of quantization losses and

intrinsic objective functions, whose signiﬁcant performance
gain over the above two-stage approaches.

In the recommendation area, [Zhang et al., 2016] is
the ﬁrst work that proposes to learn binary codes for
users and items by directly optimizing the recommendation
task. The proposed method Discrete Collaborative Filtering
(DCF) demonstrates superior performance over aforemen-
tioned two-stage efﬁcient recommendation methods. To learn
informative and compact codes, DCF proposes to enforce bal-
anced and de-correlated constraints on the discrete optimiza-
tion. Despite its effectiveness, DCF models only user-item
interactions and cannot be trivially extended to incorporate
side features. As such, it suffers from the cold-start problem
and can not be used as a generic recommendation solution,
e.g., for context-aware [Rendle, 2011] and session-based rec-
ommendation [Bayer et al., 2017]. Same as the relationship
between FM and MF, our DFM method can be seen as a gen-
eralization of DCF that can be used for generic feature-based
recommendation. Speciﬁcally, feeding only ID features of
users and items to DFM will recover the DCF method.
In
addition, our DFM can learn binary codes for each feature,
allowing it to be used for resource-limited recommendation
scenarios, such as context-aware recommendation in mobile
devices. This binary representation learning approach for
feature-based recommendation, to the best of knowledge, has
never been developed before.

The work that is most relevant with this paper is [Lian
et al., 2017], which develops a discrete optimization algo-
rithm named Discrete Content-aware Matrix Factorization
(DCMF), to learn binary codes for users and items at the
presence of their respective content information. It is worth
noting that DCMF can only learn binary codes for each user
ID and item ID, rather than their content features. Since its
prediction model is still MF (i.e.,, the dot product of user
codes and item codes only), it is rather limited in leverag-
ing side features for accurate recommendation. As such,
DCMF only demonstrates minor improvements over DCF for
feature-based collaborative recommendation (cf. Figure 2(a)
for their original paper). Going beyond learning user codes
and item codes, our DFM can learn codes for any side feature
and model the pairwise interactions between feature codes.
As such, our method has much stronger representation abil-
ity than DCMF, demonstrating signiﬁcant improvements over
DCMF in feature-based collaborative recommendation.

3 Preliminaries
We use bold uppercase and lowercase letters as matrices and
vectors, respectively. In particular, we use ai as the a-th col-
umn vector of matrix A. We denote (cid:107) · (cid:107)F as the Frobe-
nius norm of a matrix and tr(·) as the matrix trace. We de-
note sgn(·) : R → {±1} as the round-off function, i.e.,
sgn(x) = +1 if x ≥ 0 and sgn(x) = −1 otherwise.

Factorization Machine (FM) is essentially a score predic-

tion function for a (user, item) pair feature x:
n
(cid:88)

n
(cid:88)

n
(cid:88)

FM(x) := w0 +

wixi +

i=1

i=1

j=i+1

(cid:104)vi, vj(cid:105)xixj,

(1)

where x ∈ Rn is a high-dimensional feature representation
of the rich side-information, concatenated by one-hot user ID

and item ID, user and item content features, location features,
etc. w ∈ Rn is the model bias parameter: wo is the global
bias and wi is the feature bias. V ∈ Rk×n is the latent fea-
ture vector, and every (cid:104)vi, vj(cid:105) models the interaction between
the i-th and j-th feature dimensions. Therefore, V is the key
reason why FM is an effective feature-based recommenda-
tion model, as it captures the rich side-information interac-
tion. However, on-the-ﬂy storing V and computing (cid:104)vi, vj(cid:105)
are prohibitively expensive when n is large. For example, a
practical recommender system for Yelp1 needs to provide rec-
ommendation for over 1, 300, 000 users with about 174, 000
business, which have more than 1, 200, 000 attributes (here,
n = 1, 300, 000 + 174, 000 + 1, 200, 000 = 2, 674, 000).

To this end, we want to use binary codes B ∈ {±1}k×n in-
stead of V, to formulated our proposed framework: Discrete
Factorization Machines (DFM):

DFM(x) := w0 +

wixi +

(cid:104)bi, bj(cid:105)xixj.

(2)

n
(cid:88)

i=1

n
(cid:88)

n
(cid:88)

i=1

j=i+1

However, directly obtain B = sgn(V) will lead to large quan-
tization loss and thus degrade the recommendation accuracy
signiﬁcantly [Zhang et al., 2016]. In the next section, we will
introduce our proposed DFM learning model and discrete op-
timization that tackles the quantization loss.

4 Discrete Factorization Machines
We ﬁrst present the learning objective of DFM and then elab-
orate the optimization process of DFM, which is the key tech-
nical difﬁculty of the paper. At last, we shed some lights on
model initialization, which is known to have a large impact
on a discrete model.

4.1 Model Formulation
Given a training pair (x, y) ∈ V, where y is the groundtruth
score of feature vector x and V denotes the set of all training
instances, the problem of DFM is formulated as:

(cid:88)

(x,y)∈V

(y − w0 −

wixi −

(cid:104)bi, bj(cid:105)xixj)2

n
(cid:88)

i=1

n
(cid:88)

n
(cid:88)

i=1

j=i+1

arg min
w0,w,B

+ α

n
(cid:88)

i=1

w2

i , s.t. B ∈ {±1}k×n, B1 = 0
(cid:124) (cid:123)(cid:122) (cid:125)
Balance

, BBT = nI
(cid:125)
(cid:123)(cid:122)
(cid:124)
De-correlation

(3)

Due to the discrete constraint in DFM, the regularization
(cid:107)B(cid:107)2
F becomes an constant and hence is removed. Addition-
ally, DFM imposes balanced and de-correlated constraints
on the binary codes in order to maximize the information
each bit carries and to make binary codes compact [Zhou
and Zha, 2012]. However, optimizing the objective function
in Eq.(3) is a highly challenging task, since it is generally
NP-hard. To be speciﬁc, ﬁnding the global optimum solution
needs to involve O(2kn) combinatorial search for the binary
codes [Stad, 2001].

Next, we introduce a new learning objective that allows
DFM to be solved in a computationally tractable way. The

1https://www.yelp.ca/dataset

basic idea is to soften the balanced and de-correlated con-
straints. To achieve this, let us ﬁrst introduce a delegate con-
tinuous variable D ∈ D, where D = {D ∈ Rk×n|D1 =
0, DDT = nI}. Then the balanced and de-correlated con-
straints can be softened by minD∈D (cid:107)B − D(cid:107)F . As such, we
can get the softened learning objective for DFM as:

arg min
w0,w,B

(cid:88)

(x,y)∈V

(y − w0 −

wixi −

(cid:104)bi, bj(cid:105)xixj)2

n
(cid:88)

i=1

n
(cid:88)

n
(cid:88)

i=1

j=i+1

+ α

w2

i − 2βtr(BT D),

(4)

n
(cid:88)

i=1

s.t. D1 = 0, DDT = nI, B ∈ {±1}k×n,

where we use 2tr(BT D) instead of (cid:107)B−D(cid:107)F for the ease of
optimization (note that the two terms are identical since BT B
and DT D are constant). β is tunable hyperparameter control-
ling the strength of the softened de-correlation constraint. As
the above Eq.(4) allows a certain discrepancy between B and
D, it makes the binarized optimization problem computation-
ally tractable. Note that if there are feasible solution in Eq.(3),
we can impose a very large β to enforce B to be close to D.
The above Eq.(4) presents the objective function to be op-
timized for DFM. It is worth noting that we do not discard
the discrete constraint and we still perform a direct optimiza-
tion on discrete B. Furthermore, through joint optimization
for the binary codes and the delegate real variables, we can
obtain nearly balanced and uncorrelated binary codes. Next,
we introduce an efﬁcient solution to solve the mixed-integer
optimization problem in Eq.(4).

4.2 Optimization
We employ alternating optimization strategy [Liu et al., 2017]
to solve the problem. Speciﬁcally, we alternatively solve
three subproblems for DFM model in Eq.(4), taking turns to
update each of B, D, w, given others ﬁxed. Next we elabo-
rate on how to solve each of the subproblems.
B-subproblem.
In this subproblem, we aim to optimize B
with ﬁxed D and w. To achieve this, we can update B by
updating each vector br according to
(cid:88)

(cid:88)

bT

r U(

r ˆxˆxT )UT br − 2(
x2

xrψˆxT )UT br

arg min
br∈{±1}k

Vr

Vr

− 2βdT

r br, where ψ = y − w0 − wT x −

(cid:104)ui, uj(cid:105)ˆxi ˆxj

n−1
(cid:88)

n−1
(cid:88)

i=1

j=i+1

where Vr = {(x, y) ∈ V|xr (cid:54)= 0} is the training set for r,
vector ˆx is equal to x excluding element xr, U excludes the
column br of matrix B, and ui is a column in U.

Due to the discrete constraints, the optimization is gener-
ally NP-hard. To this end, we use Discrete Coordinate De-
scent (DCD) [Zhang et al., 2016] to take turns to update each
bit of binary codes br. Denote brt as the t-th bit of br and
br¯t as the rest codes excluding brt, DCD will update brt by
ﬁxing br¯t. Thus, we update brt based on the following rule:
brt ← sgn(cid:0)K(ˆbrt, brt)(cid:1),
(xrψ − x2

r ˆxT Z¯tbr¯t)ˆxT zt + βdrt

ˆbrt =

(cid:88)

(5)

Vr

where Z = UT , zt is the t-th column of the matrix Z while
Z¯t excludes the t-th column from Z, and K(x, y) is a function
that K(x, y) = x if x (cid:54)= 0 and K(x, y) = y otherwise.
Through this way, we can control that when ˆbrt = 0, we do
not update brt.
D-subproblem. When B and w are ﬁxed in Eq.(4), the op-
timization subproblem for D is:

arg max
D

tr(BT D), s.t. D1 = 0, DDT = mI.

(6)

It can be solved with the aid of a centering matrix J =
I − 1
n 11T . Speciﬁcally, by Singular Value Decomposition
(SVD), we have BJ = B = PΣQT , where P ∈ Rk×k(cid:48)
and
Q ∈ Rn×k(cid:48)
are left and right singular vectors correspond-
ing to the r(cid:48)(≤ r) positive singular values in the diagonal
matrix Σ. We ﬁrst apply eigendecomposition for the small

T
k × k matrix B B

= (cid:2)P (cid:98)P(cid:3)

(cid:21)
(cid:20)Σ2 0
0
0

(cid:2)P (cid:98)P(cid:3)T

, where

(cid:98)P are the eigenvectors of the zero eigenvalues. Therefore, by
T
PΣ−1. In order to
the deﬁnition of SVD, we have Q = B
satisfy the constraint D1 = 0, we further obtain additional
(cid:98)Q ∈ Rn×(k−k(cid:48)) by Gram-Schmidt orthogonalization based
on [Q 1]. As such, we have (cid:98)QT 1 = 0. Then we can get the
closed-form update rule for the D-subproblem in Eq.(6) as:

√

D ←

n (cid:2)P (cid:98)P(cid:3) (cid:2)Q (cid:98)Q(cid:3)T

w-subproblem. When B and D are ﬁxed in Eq.(4), the
subproblem is for optimizing w is:

arg min
w0,w

(cid:88)

(x,y)∈V

(φ − w0 −

wixi)2 + α

w2
i ,

n
(cid:88)

i=1

n
(cid:88)

i=1

φ = y −

(cid:104)bi, bj(cid:105)xixj.

n
(cid:88)

n
(cid:88)

i=1

j=i+1

(7)

(8)

Since w is a real-valued vector, it is the standard multivariate
linear regression problem. Thus we can use coordinate de-
scent algorithm provided in the original FM [Rendle, 2011]
to ﬁnd the optimal value of w and the global bias w0.

Initialization

4.3
Since DFM deals with mixed-integer non-convex optimiza-
tion, the initialization of model parameters plays an impor-
tant role for faster convergence and for ﬁnding better local
optimum solution. Here we suggest an efﬁcient initialization
strategy inspired by DCF [Zhang et al., 2016]. It ﬁrst solves
a relaxed optimization problem in Eq.(4) by discarding the
discrete constraints as:

arg min
w0,w,V

(cid:88)

(x,y)∈V

(y − w0 −

wixi −

(cid:104)vi, vj(cid:105)xixj)2

n
(cid:88)

i=1

n
(cid:88)

n
(cid:88)

i=1

j=i+1

+α

w2

i + β(cid:107)V(cid:107)2

F − 2βtr(VT D), s.t. D1 = 0, DDT = nI

n
(cid:88)

i=1

To solve the problem, we can initialize real-valued V and w
randomly and ﬁnd feasible initializations for D by solving D-
subproblem. The optimization can be done alternatively by

solving V by traditional FM, solving D by D-subproblem,
and solving w by gradient descent. Assuming the solution
is (V∗, D∗, w∗, w∗
0), we can then initialize the parameters in
Eq.(4) as:

B ← sgn(V∗), D ← D∗, w ← w∗, w0 ← w∗
0

(9)

5 Experiments
As the key contribution of this work is the design of DFM for
fast feature-based recommendation, we conduct experiments
to answer the following research questions:
RQ1. How does DFM perform as compared to existing
hash-based recommendation methods?
RQ2. How does the key hyper-parameter of DFM impact
its recommendation performance?
RQ3. How efﬁcient is DFM as compared to the real-valued
version of FM?

5.1 Experimental Settings
Datasets. We experiment on two publicly available datasets
with explicit feedbacks from different real-world websites:
Yelp and Amazon. Note that we assume each user has only
one rating for an item and average the scores if an item has
multiple ratings from the same user.

a) Yelp. This dataset [Lian et al., 2017] originally contains
409,117 users, 85,539 items (points of interest on Yelp such
as restaurants and hotels), and 2,685,066 ratings with integer
scores ranging from 1 to 5. Besides, each item has a set of
textual reviews posted by the users.

b) Amazon. This book rating dataset [McAuley et al.,
2015] originally includes 12,886,488 ratings of 929,264 items
(books on Amazon from 2,588,991 users. In this dataset, an
item also has a set of integer rating scores in [1, 5] and a set
of textual reviews.

Considering the extreme sparsity of the original Yelp and
Amazon datasets, we remove users with less than 20 ratings
and items rated by less than 20 users. After the ﬁltering, there
are 13,679 users, 12,922 items, and 640,143 ratings left in
the Yelp dataset. For the Amazon dataset, we remain 35,151
users, 33,195 items, and 1,732,060 ratings. For fair compar-
ison with DCMF, we leave out the side information from the
user ﬁeld and represent an item with the bag-of-words encod-
ing of its textual contents after aggregating all review contents
of the item. Note that we remove stopping words and trun-
cate the vocabulary by selecting the top 8,000 words regard-
ing their Term Frequency–Inverse Document Frequency. By
concatenating the bag-of-words encoding (side information
of the item) and one-hot encoding of user and item ID, we ob-
tain a feature vector of dimensionality 34,601 and 76,346 for
a rating (use-item pair) for Yelp and Amazon, respectively.

Baselines. We implement our proposed DFM method using
Matlab2 and compare it with its real-valued version and state-
of-the-art binarized methods for Collaborative Filtering:
• libFM. This is the original implementation3 of FM which
has achieved great performance for feature-based recom-
mendation with explicit feedbacks. Note that we adopt l2

2Codes are available: https://github.com/hanliu95/DFM
3http://www.libfm.org/

Yelp

Amazon

Figure 1: Performance of NDCG@K (K ranges from 1 to 10) w.r.t., code length ranges for 8 to 64 on the two datasets.

regularization on the parameters to prevent overﬁtting and
use the SGD learner to optimize it.

tel(R) Core(TM) i7-7700k 4 cores CPU at 4.20GHZ, 32GB
RAM, and 64-bit Windows 7 operating system.

• DCF. This is the ﬁrst binarized CF method that can directly

optimize the binary codes [Zhang et al., 2016].

• DCMF. This is the state-of-the-art binarized method for
CF with side information [Lian et al., 2017].
It extends
DCF by encoding the side features as the constraints for
user codes and item codes.

• BCCF. This is a two-stage binarized CF method [Zhou and
Zha, 2012] with a relaxation stage and a quantization stage.
At these two stages, it successively solves MF with bal-
anced code regularization and applies orthogonal rotation
to obtain user codes and item codes.

Note that for DCF and DCMF, we use the original imple-
mentation as released by the authors. For BCCF, we re-
implement it due to the unavailability.

Evaluation Protocols. We ﬁrst randomly split the ratings
from each user into training (50%) and testing (50%). As
practical recommender systems typically recommend a list
of items for a user, we rank the testing items of a user and
evaluate the ranked list with Normalized Discounted Cumu-
lative Gain (NDCG), which has been widely used for evalu-
ating ranking tasks like recommendation [He et al., 2017]. To
evaluate the efﬁciency of DFM and real-valued FM, we use
Testing Time Cost (TTC) [Zhang et al., 2016], where a lower
cost indicates better efﬁciency.

Parameter Settings. As we exactly follow the experimen-
tal settings of [Lian et al., 2017], we refer to their optimal
settings for hyper-parameters of DCMF, DCF, and BCCF.
For libFM, we test the l2 regularization on feature embed-
dings V of {10−i|i = −4, −3, −2, −1, 0, 1, 2}. Under the
same range, we test the de-correlation constraint (i.e., β in
(3)) of DFM. Besides, we test the code length in the
Eq.
range of [8, 16, 32, 64]. It is worth mentioning that we con-
duct all the experiments on a computer equipped with an In-

5.2 Performance Comparison (RQ1)
In Figure 1, we show the recommendation performance
(NDCG@1 to NDCG@10) of DFM and the baseline meth-
ods on the two datasets. The code length varies from 8 to 64.
From the ﬁgure, we have the following observations:
• DFM demonstrates consistent improvements over state-
of-the-art binarized recommendation methods across code
lengths (the average improvement is 7.95% and 2.38% on
Yelp and Amazon, respectively). The performance im-
provements are attributed to the beneﬁts of learning binary
codes for features and modeling their interactions.

• Besides, DFM shows very competitive performance com-
pared to libFM, its real-valued version, with an average
performance drop of only 3.24% and 2.40% on the two
datasets. By increasing the code length, the performance
gap continuously shrinks from 5.68% and 4.76% to 1.46%
and 1.19% on Yelp and Amazon, respectively. One pos-
sible reason is that libFM suffers from overﬁtting as the
increase of its representative capability (i.e., larger code
length) [He and Chua, 2017], whereas binarizing the pa-
rameters can alleviate the overﬁtting problem. This ﬁnding
again veriﬁes the effectiveness of the proposed DFM.
• Between baseline methods, DCF consistently outperforms
BCCF, while slightly underperforms DCMF with an aver-
age performance decrease of 1.58% and 0.76% on the two
datasets, respectively. This is consistent with the ﬁndings
in [Liu et al., 2014] that the direct discrete optimization is
stronger than two-stage approaches and that side informa-
tion makes the user codes and item codes more representa-
tive, which can boost the performance of recommendation.
However, the rather small performance gap between DCF
and DCMF indicates that DCMF fails to make full use
of the side information. The main reason is because that

Table 1: Efﬁciency comparison between DFM (C++ implemen-
tation) and libFM w.r.t., TTC (minutes) where the code length
ranges from 8 to 64 on the two datasets.

Code Length
libFM (TTC)
DFM (TTC)
Acceleration Ratio

Code Length
libFM (TTC)
DFM (TTC)
Acceleration Ratio

Yelp

8
27.18
2.06
13.19

16
56.77
3.56
15.95

Amazon
8
177.03
12.67
13.97

16
357.46
22.50
15.89

32
114.10
6.60
17.29

64
217.64
12.43
17.51

32
716.83
42.56
16.84

64
1, 414.67
81.04
17.46

as Facebook, Instagram, and Youtube, to substantially reduce
the computation cost of their recommendation systems.

6 Conclusions

In this paper, we presented DFM, the ﬁrst binary represen-
tation learning method for generic feature-based recommen-
dation.
In contrast to existing hash-based recommendation
methods that can only learn binary codes for users and items,
our DFM is capable of learning a vector of binary codes for
each feature. As a beneﬁt of such a compact binarized model,
the predictions of DFM can be done efﬁciently in the bi-
nary space. Through extensive experiments on two real-world
datasets, we demonstrate that DFM outperforms state-of-the-
art hash-based recommender systems by a large margin, and
achieves a recommendation accuracy rather close to that of
the original real-valued FM.

This work moves the ﬁrst step towards developing efﬁ-
cient and compact recommender models, which are partic-
ularly useful for large-scale and resource-limited scenarios.
In future, we will explore the potential of DFM for context-
aware recommendation in mobile devices, a typical applica-
tion scenario that requires fast and compact models. More-
over, we will develop pairwise learning method for DFM,
which might be more suitable for personalized ranking task.
With the fast developments of neural recommendation meth-
ods recently [He and Chua, 2017], we will develop binarized
neural recommender models in the next step to further boost
the performance of hash-based recommendation. Besides, we
are interested in deploying DFM for online recommendation
scenarios, and explore how to integrate bandit-based and re-
inforcement learning strategies into DFM. Lastly, we will ex-
plore the potential of DFM in other tasks such as popularity
prediction of online content [Feng et al., 2018].

Acknowledgment This work is supported by the National
Basic Research Program of China (973 Program), No.:
2015CB352502; National Natural Science Foundation of
China, No.: 61772310, No.: 61702300, and No.: 61702302;
and the Project of Thousand Youth Talents 2016. This work
is also part of NExT research, supported by the National Re-
search Foundation, Prime Minister’s Ofﬁce, Singapore under
its IRC@SG Funding Initiative.

Figure 2: Recommendation performance of libFM and DFM
(code length=64) on NDCG@10 w.r.t., l2 regularization (libFM)
and de-correlation constraint (DFM).

DCMF performs prediction only based on user codes and
item codes (which is same as DCF). This inevitably limits
the representation ability of DCMF.

Impact of Hyper-parameter (RQ2)

5.3
Figure 2 shows the recommendation performance of libFM
and DFM on NDCG@10 regarding l2 regularization of
libFM and de-correlation constraint, respectively. We omit
the results on different values of K and code length other
than K = 10 and code length = 64 since they shown the
same trend. First, we can see that the performance of libFM
continuously drops as we decrease the l2 regularization. One
reason is that libFM could easily suffer from overﬁtting [Xiao
et al., 2017]. Second, we observe that DFM performs slightly
worse as decreasing the de-correlation constraint. By setting
the de-correlation constraint and l2 regularization to be zero,
both of DFM and libFM exhibit signiﬁcant performance de-
crease in NDCG@10. Speciﬁcally, the performance of DFM
drops with a 1.91% and 2.05% margin on Yelp and Amazon,
respectively, while libFM encounters a 10.44% and 6.56%
one. The above ﬁndings again demonstrate the overﬁtting
problem of libFM, which leads to libFM to be very sensitive
to the l2 regularization hyper-parameter, while the proposed
DFM is relatively insensitive to its de-correlation constraint
hyper-parameter.

5.4 Efﬁciency Study (RQ3)
As libFM is implemented based on C++, we re-implement
the testing algorithm of DFM with C++ and compile it with
the same C++ compiler (gcc-4.9.3) for a fair comparison. Ta-
ble 1 shows the efﬁciency comparison between DFM and
libFM regarding TTC on the two datasets. We have the fol-
lowing observations:
• DFM achieves signiﬁcant speedups on both datasets re-
garding TTC, signiﬁcantly accelerating the libFM by a
large amplitude (on average, the acceleration ratio over
libFM is 15.99 and 16.04, respectively). This demon-
strates the great advantage of binarizing the real-valued pa-
rameters of FM.

• The acceleration ratio of DFM based on libFM is stable
around 16 times on both the datasets when the code length
increases from 8 to 64.

Along with the comparable recommendation performance of
DFM and libFM, the above ﬁndings indicate that DFM is
an operable solution for many large-scale Web services, such

IEEE Transactions on Pattern Analysis and Machine In-
telligence, 39(1):102–114, 2017.

[Luo et al., 2018] Xin Luo, Liqiang Nie, Xiangnan He,
Ye Wu, Chen Zhen-Duo, and Xin-Shun Xu. Fast scalable
supervised hashing. In SIGIR, 2018.

[McAuley et al., 2015] Julian McAuley, Rahul Pandey, and
Inferring networks of substitutable and
In SIGKDD, pages 785–794,

Jure Leskovec.
complementary products.
2015.

[Rendle et al., 2011] Steffen Rendle,

Zeno Gantner,
and Lars Schmidt-Thieme.
Christoph Freudenthaler,
Fast context-aware recommendations with factorization
machines. In SIGIR, pages 635–644, 2011.

[Rendle, 2011] Steffen Rendle. Factorization machines. In

ICDM, pages 995–1000, 2011.

[Stad, 2001] Johan Stad. Some optimal inapproximability re-

sults. ACM, 2001.

[Wang et al., 2017] Xiang Wang, Xiangnan He, Liqiang Nie,
and Tat-Seng Chua. Item silk road: Recommending items
from information domains to social users. In SIGIR, pages
185–194, 2017.

[Wang et al., 2018] Zihan Wang, Ziheng Jiang, Zhaochun
Ren, Jiliang Tang, and Dawei Yin. A path-constrained
framework for discriminating substitutable and comple-
mentary products in e-commerce. In WSDM, pages 619–
627, 2018.

[Xiao et al., 2017] Jun Xiao, Hao Ye, Xiangnan He, Han-
wang Zhang, Fei Wu, and Tat-Seng Chua. Attentional fac-
torization machines: Learning the weight of feature inter-
actions via attention networks. IJCAI, pages 3119–3125,
2017.

[Yu et al., 2018] Wenhui Yu, Huidi Zhang, Xiangnan He,
Xu Chen, Li Xiong, and Zheng Qin. Aesthetic-based
In WWW, pages 649–658,
clothing recommendation.
2018.

[Zhang et al., 2014] Zhiwei Zhang, Qifan Wang, Lingyun
Ruan, and Luo Si. Preference preserving hashing for efﬁ-
cient recommendation. In SIGIR, pages 183–192, 2014.
[Zhang et al., 2016] Hanwang Zhang, Wei Liu, Wei Liu, Xi-
angnan He, Huanbo Luan, and Tat Seng Chua. Discrete
collaborative ﬁltering. In SIGIR, pages 325–334, 2016.
[Zhou and Zha, 2012] Ke Zhou and Hongyuan Zha. Learn-
ing binary codes for collaborative ﬁltering. In SIGKDD,
pages 498–506, 2012.

References
[Bai et al., 2017] Ting Bai, Ji-Rong Wen, Jun Zhang, and
Wayne Xin Zhao. A neural collaborative ﬁltering model
In CIKM, pages
with interaction-based neighborhood.
1979–1982, 2017.

[Bayer et al., 2017] Immanuel Bayer, Xiangnan He, Bhargav
Kanagal, and Steffen Rendle. A generic coordinate de-
scent framework for learning from implicit feedback. In
WWW, pages 1341–1350, 2017.

[Chen et al., 2017] Jingyuan Chen, Hanwang Zhang, Xiang-
nan He, Liqiang Nie, Wei Liu, and Tat-Seng Chua. Atten-
tive collaborative ﬁltering: Multimedia recommendation
with item- and component-level attention. In SIGIR, pages
335–344, 2017.

[Das et al., 2007] Abhinandan S. Das, Mayur Datar,
Ashutosh Garg, and Shyam Rajaram. Google news
personalization: scalable online collaborative ﬁltering. In
WWW, pages 271–280, 2007.

[Feng et al., 2018] Fuli Feng, Xiangnan He, Yiqun Liu,
Liqiang Nie, and Tat-Seng Chua. Learning on partial-order
hypergraphs. In WWW, pages 1523–1532, 2018.

[Gionis et al., 1999] Aristides Gionis, Piotr Indyk, and Ra-
jeev Motwani. Similarity search in high dimensions via
hashing. In VLDB, pages 518–529, 1999.

[Gong and Lazebnik, 2011] Yunchao Gong and Svetlana
Lazebnik. Iterative quantization: A procrustean approach
to learning binary codes. In CVPR, pages 817–824, 2011.
[He and Chua, 2017] Xiangnan He and Tat-Seng Chua. Neu-
ral factorization machines for sparse predictive analytics.
In SIGIR, pages 355–364, 2017.

[He et al., 2016] Xiangnan He, Hanwang Zhang, Min-Yen
Kan, and Tat-Seng Chua. Fast matrix factorization for on-
In SIGIR,
line recommendation with implicit feedback.
pages 549–558, 2016.

[He et al., 2017] Xiangnan He, Lizi Liao, Hanwang Zhang,
Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collabo-
rative ﬁltering. In WWW, pages 173–182, 2017.

[Karatzoglou et al., 2010] Alexandros Karatzoglou, Alexan-
der J. Smola, and Markus Weimer. Collaborative ﬁltering
on a budget. pages 389–396, 2010.

[Li et al., 2017] Jing Li, Pengjie Ren, Zhumin Chen,
Zhaochun Ren, Tao Lian, and Jun Ma. Neural attentive
In CIKM, pages 1419–
session-based recommendation.
1428, 2017.

[Lian et al., 2017] Defu Lian, Rui Liu, Yong Ge, Kai Zheng,
Xing Xie, and Longbing Cao. Discrete content-aware ma-
trix factorization. In SIGKDD, pages 325–334, 2017.
[Liu et al., 2014] Xianglong Liu, Junfeng He, Cheng Deng,
In CVPR, pages

and Bo Lang. Collaborative hashing.
2147–2154, 2014.

[Liu et al., 2017] An-An Liu, Yu-Ting Su, Wei-Zhi Nie, and
Mohan Kankanhalli. Hierarchical clustering multi-task
learning for joint human action grouping and recognition.

