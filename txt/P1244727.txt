Lost Relatives of the Gumbel Trick

Matej Balog 1 2 Nilesh Tripuraneni 3 Zoubin Ghahramani 1 4 Adrian Weller 1 5

7
1
0
2
 
n
u
J
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
6
1
4
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

The Gumbel trick is a method to sample from a
discrete probability distribution, or to estimate its
normalizing partition function. The method re-
lies on repeatedly applying a random perturba-
tion to the distribution in a particular way, each
time solving for the most likely conﬁguration.
We derive an entire family of related methods,
of which the Gumbel trick is one member, and
show that the new methods have superior prop-
erties in several settings with minimal additional
computational cost. In particular, for the Gum-
bel trick to yield computational beneﬁts for dis-
crete graphical models, Gumbel perturbations on
all conﬁgurations are typically replaced with so-
called low-rank perturbations. We show how a
subfamily of our new methods adapts to this set-
ting, proving new upper and lower bounds on the
log partition function and deriving a family of se-
quential samplers for the Gibbs distribution. Fi-
nally, we balance the discussion by showing how
the simpler analytical form of the Gumbel trick
enables additional theoretical results.

1. Introduction

In this work we are concerned with the fundamental prob-
lem of sampling from a discrete probability distribution and
evaluating its normalizing constant. A probability distribu-
tion p on a discrete sample space X is provided in terms
of its potential function φ : X → [−∞, ∞), correspond-
ing to log-unnormalized probabilities via p(x) = eφ(x)/Z,
where the normalizing constant Z is the partition function.
In this context, p is the Gibbs distribution on X associated
with the potential function φ. The challenges of sampling
from such a discrete probability distribution and estimating
the partition function are fundamental problems with ubiq-

1University of Cambridge, UK 2MPI-IS, T¨ubingen, Germany
3UC Berkeley, USA 4Uber AI Labs, USA 5Alan Turing Institute,
UK. Correspondence to: Matej Balog <ﬁrst.last@gmail.com>.
Code: https://github.com/matejbalog/gumbel-relatives.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

uitous applications in machine learning, classical statistics
and statistical physics (see, e.g., Lauritzen, 1996).

Perturb-and-MAP methods (Papandreou & Yuille, 2010)
constitute a class of randomized algorithms for estimating
partition functions and sampling from Gibbs distributions,
which operate by randomly perturbing the corresponding
potential functions and employing maximum a posteriori
(MAP) solvers on the perturbed models to ﬁnd a maximum
probability conﬁguration. This MAP problem is NP-hard
in general; however, substantial research effort has led to
the development of solvers which can efﬁciently compute
or estimate the MAP solution on many problems that occur
in practice (e.g., Boykov et al., 2001; Kolmogorov, 2006;
Darbon, 2009). Evaluating the partition function is a harder
problem, containing for instance #P-hard counting prob-
lems. The general aim of perturb-and-MAP methods is
to reduce the problem of partition function evaluation, or
the problem of sampling from the Gibbs distribution, to re-
peated instances of the MAP problem (where each instance
is on a different random perturbation of the original model).

The Gumbel trick (Papandreou & Yuille, 2011) relies on
adding Gumbel-distributed noise to each conﬁguration’s
potential φ(x). We derive a wider family of perturb-and-
MAP methods that can be seen as perturbing the model in
different ways – in particular using the Weibull and Fr´echet
distributions alongside the Gumbel. We show that the new
methods can be implemented with essentially no additional
computational cost by simply averaging existing Gumbel
MAP perturbations in different spaces, and that they can
lead to more accurate estimators of the partition function.

Evaluating or perturbing each conﬁguration’s potential
with i.i.d. Gumbel noise can be computationally expensive.
One way to mitigate this is to cleverly prune computation
in regions where the maximum perturbed potential is un-
likely to be found (Maddison et al., 2014; Chen & Ghahra-
mani, 2016). Another approach exploits the product struc-
ture of the sample space in discrete graphical models, re-
placing i.i.d. Gumbel noise with a “low-rank” approxima-
tion. Hazan & Jaakkola (2012); Hazan et al. (2013) showed
that from such an approximation, upper and lower bounds
on the partition function and a sequential sampler for the
Gibbs distribution can still be recovered. We show that a
subfamily of our new methods, consisting of Fr´echet, Ex-
ponential and Weibull tricks, can also be used with low-

Lost Relatives of the Gumbel Trick

rank perturbations, and use these tricks to derive new upper
and lower bounds on the partition function, and to construct
new sequential samplers for the Gibbs distribution.

and-MAP and accept-reject samplers, exploiting the con-
nection between the Gumbel distribution and competing
exponential clocks that we also discuss in Section 2.1.

Our main contributions are as follows:

1. A family of tricks that can be implemented by simply
averaging Gumbel perturbations in different spaces, and
which can lead to more accurate or more sample efﬁ-
cient estimators of Z (Section 2).

2. New upper and lower bounds on the partition function of
a discrete graphical model computable using low-rank
perturbations, and a corresponding family of sequential
samplers for the Gibbs distribution (Section 3).

3. Discussion of advantages of the simpler analytical form
of the Gumbel trick including new links between the er-
rors of estimating Z, sampling, and entropy estimation
using low-rank Gumbel perturbations (Section 4).

Background and Related work The idea of perturbing
the potential function of a discrete graphical model in or-
der to sample from its associated Gibbs distribution was in-
troduced by Papandreou & Yuille (2011), inspired by their
previous work on reducing the sampling problem for Gaus-
sian Markov random ﬁelds to the problem of ﬁnding the
mean, using independent local perturbations of each Gaus-
sian factor (Papandreou & Yuille, 2010). Tarlow et al.
(2012) extended this perturb-and-MAP approach to sam-
pling, in particular by considering more general structured
prediction problems. Hazan & Jaakkola (2012) pointed out
that MAP perturbations are useful not only for sampling the
Gibbs distribution (considering the argmax of the perturbed
model), but also for bounding and approximating the parti-
tion function (by considering the value of the max).

Afterwards, Hazan et al. (2013) derived new lower bounds
on the partition function and proposed a new sampler for
the Gibbs distribution that samples variables of a discrete
graphical model sequentially, using expected values of low-
rank MAP perturbations to construct the conditional proba-
bilities. Due to the low-rank approximation, this algorithm
has the option to reject a sample. Orabona et al. (2014)
and Hazan et al. (2016) subsequently derived measure con-
centration results for the Gumbel distribution that can be
used to control the rejection probability. Maji et al. (2014)
derived an uncertainty measure from random MAP pertur-
bations, using it within a Bayesian active learning frame-
work for interactive image boundary annotation.

Perturb-and-MAP was famously generalized to continuous
spaces by Maddison et al. (2014), replacing the Gumbel
distribution with a Gumbel process and calling the resulting
algorithm A* sampling. Maddison (2016) cast this work
into a uniﬁed framework together with adaptive rejection
sampling techniques, based on the notion of exponential
races. This recent view generally brings together perturb-

Inspired by A* sampling, Kim et al. (2016) proposed an ex-
act sampler for discrete graphical models based on lazily-
instantiated random perturbations, which uses linear pro-
gramming relaxations to prune the optimization space. Fur-
ther recent applications of perturb-and-MAP include struc-
tured prediction in computer vision (Bertasius et al., 2017)
and turning the discrete sampling problem into an opti-
mization task that can be cast as a multi-armed bandit prob-
lem (Chen & Ghahramani, 2016), see Section 5.2 below.

In addition to perturb-and-MAP methods, we are aware of
three other approaches to estimate the partition function
of a discrete graphical model via MAP solver calls. The
WISH method (weighted-integrals-and-sums-by-hashing,
Ermon et al., 2013) relies on repeated MAP inference calls
applied to the model after subjecting it to random hash con-
straints. The Frank-Wolfe method may be applied by itera-
tively updating marginals using a constrained MAP solver
and line search (Belanger et al., 2013; Krishnan et al.,
2015). Weller & Jebara (2014a) instead use just one MAP
call over a discretized mesh of marginals to approximate
the Bethe partition function, which itself is an estimate
(which often performs well) of the true partition function.

2. Relatives of the Gumbel Trick

In this section, we review the Gumbel trick and state the
mechanism by which it can be generalized into an entire
family of tricks. We show how these tricks can equivalently
be viewed as averaging standard Gumbel perturbations in
different spaces, instantiate several examples, and compare
the various tricks’ properties.

Notation Throughout this paper, let X be a ﬁnite sample
space of size N := |X |. Let ˜p : X → [0, ∞) be an unnor-
malized mass function over X and let Z := (cid:80)
x∈X ˜p(x) be
its normalizing partition function. Write p(x) := ˜p(x)/Z
for the normalized version of ˜p, and φ(x) := ln ˜p(x) for the
log-unnormalized probabilities, i.e. the potential function.

We write Exp(λ) for the exponential distribution with rate
(inverse mean) λ and Gumbel(µ) for the Gumbel distribu-
tion with location µ and scale 1. The latter has mean µ + c,
where c ≈ 0.5772 is the Euler-Mascheroni constant.

2.1. The Gumbel Trick

Similarly to the connection between the Gumbel trick and
the Poisson process established by Maddison (2016), we
introduce the Gumbel trick for discrete probability distri-
butions using a simple and elegant construction via com-
peting exponential clocks. Consider N independent clocks,

Table 1: New tricks for constructing unbiased estimators of different transformations f (Z) of the partition function.

Lost Relatives of the Gumbel Trick

Trick

g(x)

Mean f (Z)

Variance of g(T )

Gumbel
Exponential
Weibull α

Fr´echet α

Pareto

Tail t

− ln x − c
x
xα, α > 0

ln Z
1
Z
Z −αΓ(1 + α)

xα, α ∈ (−1, 0) Z −αΓ(1 + α)
ex
1{x>t}

Z
Z−1 for Z > 1
e−tZ

π2
6
1
Z2
Γ(1+2α)−Γ(1+α)2
Z2α
Γ(1+2α)−Γ(1+α)2
Z2α
Z

(Z−1)2(Z−2) for Z > 2

a
e−tZ (1 − e−tZ )

for α > − 1
2

Asymptotic var. of ˆZ
π2
6 Z 2
Z 2
1
α2
1
α2

Γ(1+α)2 − 1
Γ(1+α)2 − 1

(cid:16) Γ(1+2α)

(cid:16) Γ(1+2α)

(cid:17)

(cid:17)

Z 2

Z 2

Z2
(Z−2)2
(1−e−tZ )2
t2

started simultaneously, such that the j-th clock rings after
a random time Tj ∼ Exp(λj). Then it is easy to show that
(1) the time until some clock rings has Exp((cid:80)N
j=1 λj) dis-
tribution, and (2) the probability of the j-th clock ringing
ﬁrst is proportional to its rate λj. These properties are also
widely used in survival analysis (Cox & Oakes, 1984).

Consider N competing exponential clocks {Tx}x∈X , in-
dexed by elements of X , with respective rates λx = ˜p(x).
Property (1) of competing exponential clocks tells us that

(1)

(2)

(1’)

(2’)

Property (2) says that the random variable argminx Tx, tak-
ing values in X , is distributed according to p:

min
x∈X

{Tx} ∼ Exp(Z).

argmin
x∈X

{Tx} ∼ p.

The Gumbel trick is obtained by applying the function
g(x) = − ln x − c to the equalities in distribution (1)
and (2). When g is applied to an Exp(λ) random vari-
able, the result follows the Gumbel(−c + ln λ) distribu-
tion, which can also be represented as ln λ + γ, where
i.i.d.∼ Gumbel(−c)
γ ∼ Gumbel(−c). Deﬁning {γ(x)}x∈X
and noting that g is strictly decreasing, applying the func-
tion g to equalities in distribution (1) and (2), we obtain:

{φ(x) + γ(x)} ∼ Gumbel(−c + ln Z),

max
x∈X
argmax
x∈X

{φ(x) + γ(x)} ∼ p,

where we have recalled that φ(x) = ln λx = ln ˜p(x). The
distribution Gumbel(−c + ln Z) has mean ln Z, and thus
the log partition function can be estimated by averaging
samples (Hazan & Jaakkola, 2012).

2.2. Constructing New Tricks

Given the equality in distribution (1), we can treat the prob-
lem of estimating the partition function Z as a parameter
estimation problem for the exponential distribution. Ap-
plying the function g(x) = − ln x − c as in the Gumbel
trick to obtain a Gumbel(−c + ln Z) random variable, and

estimating its mean to obtain an unbiased estimator of ln Z,
is just one way of inferring information about Z.

We consider applying different functions g to (1); par-
ticularly those functions g that transform the exponential
distribution to another distribution with known mean. As
the original exponential distribution has rate Z, the trans-
formed distribution will have mean f (Z), where f will in
general no longer be the logarithm function. Since we often
are interested in estimating various transformations f (Z)
of Z, this provides us a with a collection of unbiased es-
timators from which to choose. Moreover, further trans-
forming these estimators yields a collection of (biased) es-
timators for other transformations of Z, including Z itself.
Example 1 (Weibull tricks). For any α > 0, applying
the function g(x) = xα to an Exp(λ) random variable
yields a random variable with the Weibull(λ−α, α−1) dis-
tribution with scale λ−α and shape α−1, which has mean
λ−αΓ(1 + α) and can be also represented as λ−αW ,
i.i.d.∼
where W ∼ Weibull(1, α−1). Deﬁning {W (x)}x∈X
Weibull(1, α−1) and noting that g is increasing, applying
g to the equality in distribution (1) gives

{˜p−αW (x)} ∼ Weibull(Z −α, α−1).

(1”)

min
x∈X

Estimating the mean of Weibull(Z −α, α−1) yields an un-
biased estimator of Z −αΓ(1 + α). The special case α = 1
corresponds to the identity function g(x) = x; we call the
resulting trick the Exponential trick.

Table 1 lists several examples of tricks derived this way.
As Example 1 shows, these tricks may not involve addi-
tive perturbation of the potential function φ(x); the Weibull
tricks multiplicatively perturb exponentiated unnormalized
probabilities ˜p−α with Weibull noise. As models of inter-
est are often speciﬁed in terms of potential functions, to be
able to reuse existing MAP solvers in a black-box manner
with the new tricks, we seek an equivalent formulation in
terms of the potential function. The following Proposition
shows that by not passing the function g through the mini-
mization in equation (1), the new tricks can be equivalently
formulated as averaging additive Gumbel perturbations of
the potential function in different spaces.

Lost Relatives of the Gumbel Trick

Figure 1: Analytically computed MSE and variance of Gumbel
and Exponential trick estimators of Z (left) and ln Z (right). The
MSEs are dominated by the variance, so the dashed and solid lines
mostly overlap. See Section 2.3.2 for details.

Figure 2: MSE of estimators of Z (left) and ln Z (right) stem-
ming from Fr´echet (− 1
2 < α < 0), Gumbel (α = 0) and Weibull
tricks (α > 0). See Section 2.3.2 for details.

Proposition 2. For any function g : [0, ∞) → R such that
f (Z) = ET ∼Exp(Z)[g(T )] exists, we have

f (Z) = Eγ

(cid:18)

(cid:20)
g

e−c exp

(cid:18)

− max
x∈X

{φ(x) + γ(x)}

,

(cid:19)(cid:19)(cid:21)

where {γ(x)}x∈X

i.i.d.∼ Gumbel(−c).

Proof. As maxx{φ(x) + γ(x)} ∼ Gumbel(−c + ln Z),
we have e−c exp(maxx{φ(x) + γ(x)}) ∼ Exp(Z) and the
result follows by the assumption relating f and g.

Proposition 2 shows that the new tricks can be implemented
by solving the same MAP problems maxx{φ(x)+γ(x)} as
in the Gumbel trick, and then merely passing the solutions
through the function x (cid:55)→ g(e−c exp(x)) before averaging
them to approximate the expectation.

2.3. Comparing Tricks

2.3.1. ASYMPTOTIC EFFICIENCY

The Delta method (Casella & Berger, 2002) is a simple
technique for assessing the asymptotic variance of esti-
mators that are obtained by a differentiable transforma-
tion of an estimator with known variance. The last col-
umn in Table 1 lists asymptotic variances of correspond-
ing tricks when unbiased estimators of f (Z) are passed
through the function f −1 to yield (biased, but consistent
and non-negative) estimators of Z itself. It is interesting
to examine the constants that multiply Z 2 in some of the
obtained asymptotic variance expressions for the different
tricks. For example, it can be shown using Gurland’s ra-
tio (Gurland, 1956) that this constant is at least 1 for the
Weibull and Fr´echet tricks, which is precisely the value
achieved by the Exponential trick (which corresponds to
α = 1). Moreover, the Gumbel trick constant π2/6 can be
shown to be the limit as α → 0 of the Weibull and Fr´echet
trick constants. In particular, the constant of the Exponen-
tial trick is strictly better than that of the standard Gumbel
trick: 1 < π2/6 ≈ 1.65. This motivates us to compare the
Gumbel and Exponential tricks in more detail.

2.3.2. MEAN SQUARED ERROR (MSE)

For estimators Y , their MSE(Y ) = var(Y ) + bias(Y )2 is
a commonly used comparison metric. When the Gumbel or
Exponential tricks are used to estimate either Z or ln Z, the
biases, variances, and MSEs of the estimators can be com-
puted analytically using standard methods (Appendix A).

For example, the unbiased estimator of ln Z from the Gum-
bel trick can be turned into a consistent non-negative esti-
mator of Z by exponentiation: Y = exp( 1
m=1 Xm),
M
i.i.d.∼ Gumbel(−c + ln Z) are obtained
where X1, . . . , XM
using equation (1’). The bias and variance of Y can be
computed using independence and the moment generating
functions of the Xm’s, see Appendix A for details.

(cid:80)M

Perhaps surprisingly, all estimator properties only depend
on the true value of Z and not on the structure of the model
(distribution p), since the estimators rely only on i.i.d. sam-
ples of a Gumbel(−c + ln Z) random variable. Figure 1
shows the analytically computed estimator variances and
MSEs. For estimating Z itself (left), the Exponential trick
outperforms the Gumbel trick in terms of MSE for all sam-
ple sizes M ≥ 3 (for M ∈ {1, 2}, both estimators have
inﬁnite variance and MSE). The ratio of MSEs quickly ap-
proaches π2/6, and in this regime the Exponential trick re-
quires 1 − 6/π2 ≈ 39% fewer samples than the Gumbel
trick to reach the same MSE. Also, for estimating ln Z,
(Figure 1, right), the Exponential trick provides a lower
MSE estimator for sample sizes M ≥ 2; only for M = 1
the Gumbel trick provides a better estimator.

Note that as biases are available analytically, the estima-
tors can be easily debiased (by subtracting their bias). One
then obtain estimators with MSEs equal to the variances of
the original estimators, shown dashed in Figure 1. The Ex-
ponential trick would then always outperform the Gumbel
trick when estimating ln Z, even with sample size M = 1.

For Weibull tricks with α (cid:54)= 1 and Fr´echet tricks, we esti-
mated the biases and variances of estimators of Z and ln Z
by constructing K = 100, 000 estimators in each case and
evaluating their bias and variance. Figure 2 shows the re-
sults for varying α and several sample sizes M . We plot the

Lost Relatives of the Gumbel Trick

analytically computed value for the Gumbel trick at α = 0,
as we observe that the Weibull trick interpolates between
the Gumbel trick and the Exponential trick as α increases
from 0 to 1. We note that the minimum MSE estimator is
obtained by choosing a value of α that is close to 1, i.e.
the Exponential trick. This agrees with the ﬁnding from
Section 2.3.1 that α = 1 is optimal as M → ∞.

2.4. Bayesian Perspective

A Bayesian approach exposes two choices when construct-
ing estimators of Z, or of its transformations f (Z):

1. A choice of prior distribution p0(Z), encoding prior
beliefs about the value of Z before any observations.
2. A choice of how to summarize the posterior distribu-

tion pM (Z|X1, . . . , XM ) given M samples.

Taking the Jeffrey’s prior p0(Z) ∝ Z −1, an improper prior
that it is invariant under reparametrization, observing M
samples X1, . . . , XM

i.i.d.∼ Exp(Z) yields the posterior:

pM (Z|X1, . . . , XM ) ∝ ZM −1e−Z (cid:80)M
Recognizing the density of a Gamma(M, (cid:80)M
dom variable, the posterior mean is

m=1 Xm.

m=1 Xm) ran-

E[Z|X1, . . . , XM ] =

M
m=1 Xm

(cid:80)M

=

(cid:32)

1
M

M
(cid:88)

m=1

(cid:33)−1

Xm

,

coinciding with the Exponential trick estimator of Z.

3. Low-rank Perturbations

One way of exploiting perturb-and-MAP to yield com-
putational savings is to replace independent perturbations
of each conﬁguration’s potential with an approximation.
Such approximations are available e.g. in discrete graphical
models, where the sampling space X has a product space
structure X = X1 × · · · × Xn, with Xi the state space of
the i-th variable.
Deﬁnition 3 ( (Hazan & Jaakkola, 2012)). The sum-unary
perturbation MAP value is the random variable

U := max
x∈X

(cid:110)

φ(x) +

γi(xi)

(cid:111)
,

n
(cid:88)

i=1

Unary perturbations provide the upper bound ln Z ≤ E[U ]
on the log partition function (Hazan & Jaakkola, 2012), can
be used to construct a sequential sampler for the Gibbs dis-
tribution (Hazan et al., 2013), and, if the perturbations are
scaled down by a factor of n, a lower bound on ln Z can
also be recovered (Hazan et al., 2013). In this section we
show that a subfamily of tricks introduced in Section 2,
consisting of Fr´echet and Weibull (and Exponential) tricks,
is applicable in the low-rank perturbation setting and use
them to derive new families of upper and lower bounds
on ln Z and sequential samplers for the Gibbs distribution.
Please note full proofs are deferred to Appendix B and C.

3.1. Upper Bounds on the Partition Function

The following family of upper bounds on ln Z can be de-
rived from the Fr´echet and Weibull tricks.

Proposition 4. For any α ∈ (−1, 0) ∪ (0, ∞), the upper
bound ln Z ≤ U(α) holds with

U(α) := n

ln Γ(1 + α)
α

+ nc −

ln Eγ

(cid:2)e−αU (cid:3) .

1
α

Proof. (Sketch.) By induction on n, with the induction step
provided by our Clamping Lemma (Lemma 7) below.

To evaluate these bounds in practice, E[e−αU ] is estimated
using samples of U . Corollary 9 of Hazan et al. (2016) can
be used to show that var(e−αU ) is ﬁnite for α > − 1
n ,
√
2
and so then the estimation is well-behaved.

A natural question is how these new bounds relate to the
Gumbel trick upper bound ln Z ≤ E[U ] by Hazan &
Jaakkola (2012). The following result aims to answers this:

Proposition 5. The limit of U(α) as α → 0 exists and
equals U(0) := E[U ], i.e. the Gumbel trick upper bound.

The question remains: When is it advantageous to use a
value α (cid:54)= 0 to obtain a tighter bound on ln Z than the
Gumbel trick bound? The next result can provide guidance:

Proposition 6. The function U(α) is differentiable at α =
0 and the derivative equals

d
dα

(cid:12)
(cid:12)
U(α)
(cid:12)
(cid:12)α=0

=

(cid:18)

n

π2
6

1
2

(cid:19)

− var(U )

.

where {γi(xi) | xi ∈ Xi, 1 ≤ i ≤ n} i.i.d∼ Gumbel(−c).

This deﬁnition involves |X1|+· · ·+|Xn| i.i.d. Gumbel ran-
dom variables, rather than |X |. (With n = 1 this coincides
with full-rank perturbations and U ∼ Gumbel(−c+ln Z).)
For n > 2 the distribution of U is not available analytically.
One can similarly deﬁne the pairwise (or higher-order) per-
turbations, where independent Gumbel noise is added to
each pairwise (or higher-order) potential.

While the variance of U is generally not tractable, in prac-
tice one obtains samples from U to estimate the expectation
in U(α) and these samples can be reused to assess var(U ).
Interestingly, var(U ) equals nπ2/6 for both the uniform
distribution and the distribution concentrated on a single
conﬁguration, and in our empirical investigations always
var(U ) ≤ nπ2/6. Then the derivative at 0 is non-negative
and Fr´echet tricks provide tighter bounds on ln Z. How-
ever, as U(α) is estimated with samples, the question of

Lost Relatives of the Gumbel Trick

estimator variance arises. We investigate the trade-off be-
tween tightness of the bound ln Z ≤ U(α) and the variance
incurred in estimating U(α) empirically in Section 5.3.

can derive sequential sampling procedures from the Fr´echet
and Weibull tricks, leading to the following algorithm.

3.2. Clamping

Consider the partial sum-unary perturbation MAP values,
where the values of the ﬁrst j − 1 variables have been ﬁxed,
and only the rest are perturbed:

Uj(x1, . . . , xj−1) := max
xj ,...,xn

φ(x) +

γi(xi)

.






n
(cid:88)

i=j






The following lemma involving the Uj’s serves three pur-
poses: (I.) it provides the induction step for Proposition 4,
(II.) it shows that clamping never hurts partition function
estimation with Fr´echet and Weibull tricks, and (III.) it will
be used to show that a sequential sampler constructed in
Section 3.3 below is well-deﬁned.

Lemma 7 (Clamping Lemma). For any j ∈ {1, . . . , n}
and (x1, . . . , xj−1) ∈ X1 × · · · × Xj−1, the following in-
equality holds with any α ∈ (−1, 0) ∪ (0, ∞):

(cid:88)

(cid:104)

Eγ

e−(n−j) ln Γ(1+α)−α(n−j)c)e−αUj+1

(cid:105)−1/α

xj ∈Xj
(cid:104)
e−(n−(j−1)) ln Γ(1+α)−α(n−(j−1))c)e−αUj

≤ Eγ

(cid:105)−1/α

Proof. This follows directly from the Fr´echet trick (α ∈
(−1, 0)) or the Weibull trick (α > 0) and representing the
Fr´echet resp. Weibull random variables in terms of Gumbel
random variables. See Appendix B.1 for more details.

Algorithm 1 Sequential sampler for Gibbs distribution
Input: α ∈ (−1, 0) ∪ (0, ∞), potential function φ on X
Output: a sample x from the Gibbs distribution ∝ eφ(x)
1: for j = 1 to n do
for xj ∈ Xj do
2:

Eγ[e−αUj+1(x1 ,...,xj )]−1/α
Eγ[e−αUj (x1,...,xj−1)]−1/α

pj(xj)

3:

4:
5:
6:
7:

Γ(1+α)1/α

pj(xj) ← e−c
pj(reject) ← 1 − (cid:80)
xj ← sample according to pj
if xj == reject then
RESTART (goto 1)

xj ∈Xj

This algorithm is well-deﬁned if pj(reject) ≥ 0 for all j,
which can be shown by canceling terms in the Clamping
Lemma 7. We discuss correctness in Appendix B.2. As for
the Gumbel sequential sampler of Hazan et al. (2013), the
expected number of restarts (and hence the running time)
only depend on the quality of the upper bound (U(α) −
ln Z), and not on the ordering of variables.

3.4. Lower Bounds on the Partition Function

Similarly as in the Gumbel trick case (Hazan et al., 2013),
one can derive lower bounds on ln Z by perturbing an arbi-
trary subset S of variables.
Proposition 9. Let X = X1 × · · · Xn be a product space
and φ a potential function on X . Let α ∈ (−1, 0) ∪ (0, ∞).
For any subset S ⊆ {1, . . . , n} of the variables x1, . . . , xn
we have ln Z ≥

Corollary 8. Clamping never hurts ln Z estimation using
any of the Fr´echet or Weibull upper bounds U(α).

c +

ln Γ(1 + α)
α

−

1
α

ln E

(cid:104)

e−α maxx{φ(x)+γS (xS )}(cid:105)

,

Proof. Applying the function x (cid:55)→ ln(x) to both sides of
the Clamping Lemma 7 with j = 1, the right-hand side
equals U(α), while the left-hand side is the estimate of ln Z
after clamping variable x1.

This was shown previously in restricted settings (Hazan
et al., 2013; Zhao et al., 2016). Similar results showing
that clamping improves partition function estimation have
been obtained for the mean ﬁeld and TRW approxima-
tions (Weller & Domke, 2016), and in certain settings for
the Bethe approximation (Weller & Jebara, 2014b) and L-
FIELD (Zhao et al., 2016).

3.3. Sequential Sampling

where xS := {xi : i ∈ S} and γS(xS) ∼ Gumbel(−c)
independently for each setting of xS.

By averaging n such lower bounds corresponding to single-
ton sets S = {i} together, we obtain a lower bound on ln Z
that involves the average-unary perturbation MAP value

(cid:40)

L := max
x∈X

φ(x) +

(cid:41)

γi(xi)

.

1
n

n
(cid:88)

i=1

Corollary 10. For any α ∈ (−1, 0) ∪ (0, ∞), we have the
lower bound ln Z ≥ L(α), where

L(α) := c +

ln Γ(1 + α)
α

−

1
nα

ln E [exp (−nαL)] .

Hazan et al. (2013) derived a sequential sampling proce-
dure for the Gibbs distribution by exploiting the U(0) Gum-
bel trick upper bound on ln Z.
In the same spirit, one

Again, L(0) := E[L] can be deﬁned by continuity, where
E[L] ≤ ln Z is the Gumbel trick lower bound by Hazan
et al. (2013).

4. Advantages of the Gumbel Trick

5. Experiments

Lost Relatives of the Gumbel Trick

We have seen how the Gumbel trick can be embedded into
a continuous family of tricks, consisting of Fr´echet, Expo-
nential, and Weibull tricks. We showed that the new tricks
can provide more efﬁcient estimators of the partition func-
tion in the full-rank perturbation setting (Section 2), and
in the low-rank perturbation setting lead to sequential sam-
plers and new bounds on ln Z, which can be also more ef-
ﬁcient, as we investigate in Section 5.3. To balance the
discussion of merits of different tricks, in this section we
brieﬂy highlight advantages of the Gumbel trick that stem
from its simpler analytical form.

First, by consulting Table 1 we see that the function g(x) =
− ln x − c has the property that the variance of the resulting
estimator (of ln Z) does not depend on the value of Z; the
function g is a variance stabilizing transformation for the
Exponential distribution.

Second, exploiting the fact that the logarithm function leads
to additive perturbations, Maji et al. (2014) showed that the
entropy of x∗, the conﬁguration with maximum potential
after sum-unary perturbation in the sense of Deﬁnition 3,
can be bounded as H(x∗) ≤ B(p) := (cid:80)n
i )].
We extend this result to show how the errors of bounding
ln Z, sampling, and entropy estimation are related:
Proposition 11. Writing p for the Gibbs distribution and
B(p) := Eγi [γi(x∗

Eγi [γi(x∗

i=1

i )] for the entropy bound, we have
= B(p) − H(x∗)
+ KL(x∗ (cid:107) p)
(cid:125)
(cid:124)
(cid:125)
(cid:123)(cid:122)
sampling error

(cid:123)(cid:122)
error in entropy estimation

(cid:124)

.

(U(0) − ln Z)
(cid:125)
(cid:123)(cid:122)
(cid:124)
error in ln Z bound

Third, the additive character of the Gumbel perturbations
can also be used to derive a new result relating the error of
the lower bound L(0) and of sampling x∗∗ as the conﬁgu-
ration achieving the maximum average-unary perturbation
value L, instead of sampling from the Gibbs distribution p:
Proposition 12. Writing p for the Gibbs distribution,

ln Z − L(0)
(cid:124)
(cid:123)(cid:122)
(cid:125)
error in ln Z bound

≥ KL(x∗∗ (cid:107) p)
(cid:123)(cid:122)
(cid:125)
sampling error

(cid:124)

≥ 0.

Remark. While we knew from Hazan et al. (2013) that
ln Z − L(0) ≥ 0, this is a stronger result showing that
the size of the gap is an upper bound on the KL divergence
between the approximate sampling distribution of x∗∗ and
the Gibbs distribution p.

Proofs of the new results appear in Appendix B.3 and C.2.

Fourth, viewed as a function of the Gumbel perturbations
γ, the random variable U has a bounded gradient, allowing
earlier measure concentration results (Orabona et al., 2014;
Hazan et al., 2016). Proving similar measure concentration
results for the expectations E[e−αU ] appearing in U(α) for
α (cid:54)= 0 may be more challenging.

We conducted experiments with the following aims:

1. To show that the higher efﬁciency of the Exponential
trick in the full-rank perturbation setting is useful in
practice, we compared it to the Gumbel trick in A*
sampling (Maddison et al., 2014) (Section 5.1) and in
the large-scale discrete sampling setting of Chen &
Ghahramani (2016) (Section 5.2).

2. To show that non-zero values of α can lead to bet-
ter estimators of ln Z in the low-rank perturbation set-
ting as well, we compare the Fr´echet and Weibull trick
bounds U(α) to the Gumbel trick bound U(0) on a
common discrete graphical model with different cou-
pling strengths; see Section 5.3.

5.1. A* Sampling

A* sampling (Maddison et al., 2014) is a sampling algo-
rithm for continuous distributions that perturbs the log-
unnormalized density φ with a continuous generalization
of the Gumbel trick, called the Gumbel process, and uses
a variant of A* search to ﬁnd the location of the maxi-
mum of the perturbed φ. Returning the location yields an
exact sample from the original distribution, as in the dis-
crete Gumbel trick. Moreover, the corresponding maxi-
mum value also has the Gumbel(−c + ln Z) distribution
(Maddison et al., 2014). Our analysis in Section 2.3 tells
us that the Exponential trick yields an estimator with lower
MSE than the Gumbel trick; we brieﬂy veriﬁed this on
the Robust Bayesian Regression experiment of Maddison
et al. (2014). We constructed estimators of ln Z from the
Gumbel and Exponential tricks (debiased version, see Sec-
tion 2.3.2), and assessed their variances by constructing
each estimator K = 1000 times and looking at the sam-
ple variance. Figure 3a shows that the Exponential trick
requires up to 40% fewer samples to reach a given MSE.

5.2. Scalable Partition Function Estimation

Chen & Ghahramani (2016) considered sampling from a
discrete distribution of the form p(x) ∝ f0(x) (cid:81)S
s=1 fs(x)
when the number of factors S is large relative to the sam-
ple space size |X |. Computing i.i.d. Gumbel perturbations
γ(x) for each x ∈ X is then relatively cheap compared to
evaluating all potentials φ(x) = f0(x) + (cid:80)S
s=1 ln fs(x).
Chen & Ghahramani (2016) observed that each (perturbed)
potential can be estimated by subsampling the factors, and
potentials that appear unlikely to yield the MAP value can
be pruned off from the search early on. The authors for-
malized the problem as a Multi-armed bandit problem with
a ﬁnite reward population and derived approximate algo-
rithms for efﬁciently ﬁnding the maximum perturbed po-
tential with a probabilistic guarantee.

Lost Relatives of the Gumbel Trick

(a)

(b)

Figure 3: (a) Sample size M required to reach a given MSE using
Gumbel and Exponential trick estimators of ln Z, using samples
from A∗ sampling (see Section 5.1) on a Robust Bayesian Re-
gression task. The Exponential trick is more efﬁcient, requiring
up to 40% fewer samples to reach a given MSE. (b) MSE of ln Z
estimators for different values of α, using M = 100 samples
from the approximate MAP algorithm discussed in Section 5.2,
with different error bounds δ. For small δ, the Exponential trick
is close to optimal, matching the analysis of Section 2.3.2. For
larger δ, the Weibull trick interpolation between the Gumbel and
Exponential tricks can provide an estimator with lower MSE.

While Chen & Ghahramani (2016) considered sampling,
by modifying their procedure to return the value of the
maximum perturbed potential rather than the argmax (cf
equations (1) and (2)), we can estimate the partition func-
tion instead. However, the approximate algorithm only
guarantees to ﬁnd the MAP conﬁguration with a proba-
bility 1 − δ. Figure 3b shows the results of running the
Racing-Normal algorithm of Chen & Ghahramani (2016)
on the synthetic dataset considered by the authors with the
“very hard” noise setting σ = 0.1. For low error bounds δ
the Exponential trick remained close to optimal, but for a
larger error bound the Weibull trick interpolation between
the Gumbel and Exponential tricks proved useful to provide
an estimator with lower MSE.

5.3. Low-rank Perturbation Bounds on ln Z

Hazan & Jaakkola (2012) evaluated tightness of the Gum-
bel trick upper bound U(0) ≥ ln Z on 10 × 10 binary spin
glass models. We show one can obtain more accurate es-
timates of ln Z on such models by choosing α (cid:54)= 0. To
account for the fact that in practice an expectation in U(α)
is replaced with a sample average, we treat U(α) as an esti-
mator of ln Z with asymptotic bias equal to the bound gap
(U(α) − ln Z), and estimate its MSE.

Figure 4 shows the MSEs of U(α) as estimators of ln Z on
10 × 10 (n = 100) binary pairwise grid models with unary
potentials sampled uniformly from [−1, 1] and pairwise po-
tentials from [0, C] (attractive models) or from [−C, C]
(mixed models), for varying coupling strengths C. We re-
placed the expectations in U (α)’s with sample averages of
size M = 100, using libDAI (Mooij, 2010) to solve the
MAP problems yielding these samples. We constructed
each estimator 1000 times to assess its variance.

Figure 4: MSEs of U(α) as estimators of ln Z on 10 × 10 at-
tractive (left, middle) and mixed (right) spin glass model with dif-
ferent coupling strengths C (see Section 5.3). We also show the
percentage of samples saved by using the best α in place of the
Gumbel trick estimator U(0), assuming the asymptotic regime.
For this we only considered α > −1/(2
n) = −0.05, where
variance is provably ﬁnite, see Section 3.1. The MAP problems
were solved using the exact junction tree algorithm (JCT, left and
right), or approximate belief propagation (BP, middle).
In all
cases, when coupling is very low, α close to 0 is optimal. This
also holds for BP when coupling is high. In other regimes, upper
bounds for the Fr´echet trick, i.e. α < 0, provide more accurate
estimators.

√

6. Discussion

By casting partition function evaluation as a parameter esti-
mation problem for the exponential distribution, we derived
a family of methods of which the Gumbel trick is a special
case. These methods can be equivalently seen as (1) per-
turbing models using different distributions, or as (2) av-
eraging standard Gumbel perturbations in different spaces,
allowing implementations with little additional cost.

We showed that in the full-rank perturbation setting, the
new Exponential trick provides an estimator with lower
MSE, or instead allows using up to 40% fewer samples than
the Gumbel trick estimator to reach the same MSE.

In the low-rank perturbation setting, we used our Fr´echet,
Exponential and Weibull tricks to derive new bounds on
ln Z and sequential samplers for the Gibbs distribution, and
showed that these can also behave better than the corre-
sponding Gumbel trick results. However, the optimal trick
to use (as speciﬁed by α) depends on the model, sample
size, and MAP solver used (if approximate). Since in prac-
tice the dominant computational cost is carried by solving
repeated instances of the MAP problem, one can try and as-
sess different values of α on the problem at hand. That said,
we believe that investigating when different tricks yield
better results is an interesting avenue for future work.

Finally, we balanced the discussion by pointing out that the
Gumbel trick has a simpler analytical form which can be
exploited to derive more interesting theoretical statements
in the low-rank perturbation setting. Beyond existing re-
sults, we derived new connections between errors of differ-
ent procedures using low-rank Gumbel perturbations.

Lost Relatives of the Gumbel Trick

Acknowledgements

The authors thank Tamir Hazan for helpful discussions,
and Mark Rowland, Maria Lomeli, and the anonymous
reviewers for helpful comments. AW acknowledges sup-
port by the Alan Turing Institute under EPSRC grant
EP/N510129/1, and by the Leverhulme Trust via the CFI.

References

Belanger, D., Sheldon, D., and McCallum, A. Marginal inference
In NIPS Workshop on Greedy

in MRFs using Frank-Wolfe.
Optimization, Frank-Wolfe and Friends, 2013.

Bertasius, G., Liu, Q., Torresani, L., and Shi, J. Local Perturb-

and-MAP for Structured Prediction. In AISTATS, 2017.

Boykov, Y., Veksler, O., and Zabih, R. Fast approximate energy
IEEE Transactions on pattern

minimization via graph cuts.
analysis and machine intelligence, 23(11):1222–1239, 2001.

Casella, G. and Berger, R.

Statistical inference, volume 2.

Duxbury Paciﬁc Grove, CA, 2002.

Maddison, C. A Poisson process model for Monte Carlo.

In
Hazan, T., Papandreou, G., and Tarlow, D. (eds.), Perturbation,
Optimization, and Statistics. MIT Press, 2016.

Maddison, C., Tarlow, D., and Minka, T. A∗ sampling. In NIPS.

2014.

Maji, S., Hazan, T., and Jaakkola, T. Active boundary annotation

using random MAP perturbations. In AISTATS, 2014.

Mooij, J.

libDAI: A free and open source C++ library for dis-
crete approximate inference in graphical models. Journal of
Machine Learning Research, 11, 2010.

Orabona, F., Hazan, T., Sarwate, A., and Jaakkola, T. On measure
concentration of random maximum a-posteriori perturbations.
In ICML, 2014.

Papandreou, G. and Yuille, A. Gaussian sampling by local pertur-

bations. In NIPS. 2010.

Papandreou, G. and Yuille, A. Perturb-and-MAP random ﬁelds:
Using discrete optimization to learn and sample from energy
models. In Proc. IEEE Int. Conf. on Computer Vision (ICCV),
pp. 193–200, November 2011.

Chen, Y. and Ghahramani, Z. Scalable discrete sampling as a

multi-armed bandit problem. In ICML, 2016.

Tarlow, D., Adams, R., and Zemel, R. Randomized optimum

models for structured prediction. In AISTATS, 2012.

dinavian Actuarial Journal, 1956(2):171–172, 1956.

Wright, S. and Nocedal, J. Numerical optimization. Springer

Wainwright, M. and Jordan, M. Graphical Models, Exponen-
tial Families, and Variational Inference. Found. Trends Mach.
Learn., 1(1-2):1–305, January 2008.

Weller, A. and Domke, J. Clamping improves TRW and mean

ﬁeld approximations. In AISTATS, 2016.

Weller, A. and Jebara, T. Approximating the Bethe partition func-

tion. In UAI, 2014a.

Weller, A. and Jebara, T. Clamping variables and approximate

inference. In NIPS, 2014b.

Science, 35:67–68, 1999.

Zhao, J., Djolonga, J., Tschiatschek, S., and Krause, A. Variable
clamping for optimization-based inference. In NIPS Workshop
on Advances in Approximate Bayesian Inference, December
2016.

Cox, D. and Oakes, D. Analysis of survival data, volume 21. CRC

Press, 1984.

Darbon, J. Global optimization for ﬁrst order Markov random
ﬁelds with submodular priors. Discrete Applied Mathematics,
157(16):3412 – 3423, 2009.

Ermon, S., Sabharwal, A., and Selman, B. Taming the curse of
dimensionality: Discrete integration by hashing and optimiza-
tion. In ICML, 2013.

Gurland, J. An inequality satisﬁed by the Gamma function. Scan-

Hazan, T. and Jaakkola, T. On the partition function and random

maximum a-posteriori perturbations. In ICML, 2012.

Hazan, T., Maji, S., and Jaakkola, T. On sampling from the Gibbs
distribution with random maximum a-posteriori perturbations.
In NIPS. 2013.

Hazan, T., Orabona, F., Sarwate, A., Maji, S., and Jaakkola,
T. High dimensional inference with random maximum a-
posteriori perturbations. CoRR, abs/1602.03571, 2016.

Kim, C., Sabharwal, A., and Ermon, S. Exact sampling with in-
teger linear programs and random perturbations. In AAAI, pp.
3248–3254, 2016.

Kolmogorov, V. Convergent tree-reweighted message passing for
IEEE transactions on pattern analysis

energy minimization.
and machine intelligence, 28(10):1568–1583, 2006.

Krishnan, Rahul G, Lacoste-Julien, Simon, and Sontag, David.
Barrier Frank-Wolfe for Marginal Inference. In NIPS. 2015.

Lauritzen, S. Graphical models. Oxford statistical science series.

Clarendon Press, Oxford, 1996. Autre tirage : 1998.

APPENDIX: Lost Relatives of the Gumbel Trick

Lost Relatives of the Gumbel Trick

Here we provide proofs for the results stated in the main text, together with additional supporting lemmas required for
these proofs.

A. Comparison of Gumbel and Exponential tricks

In Section 2.3.1 we analyzed the asymptotic efﬁciency of different estimators of Z by measuring their asymptotic variance.
(As all our estimators in the full-rank perturbation setting are consistent, their bias is 0 in the limit of inﬁnite data, and so
this asymptotic variance equals the asymptotic MSE.) In the non-asymptotic regime, where an estimator ˆZ is constructed
from a ﬁnite set of M samples, we can analyze both the variance var( ˆZ) and the bias (E[ ˆZ] − Z) of the estimator. While
in most cases these cannot be obtained analytically and there we can resort to an empirical evaluation, for the estimators
stemming from the Gumbel and Exponential tricks analytical treatment turns out to be possible using standard methods.

A.1. Estimating Z

Gumbel trick The Gumbel trick yields an unbiased estimator for ln Z, and we can turn it into a consistent estimator of
Z by exponentiating it:

ˆZ := exp

(cid:32)

1
M

M
(cid:88)

m=1

(cid:33)

Xm

where

X1, . . . , XM

iid∼ Gumbel(−c + ln Z).

Recalling that the moment generating function of a Gumbel(µ) distribution is G(t) = Γ(1 − t)eµt, we can obtain by using
independence of the samples:

E[ ˆZ] =

E[eXm/M ] =

(cid:16)

Γ(1 − 1/M )e(ln Z−c)/M (cid:17)M

= Γ(1 − 1/M )M e−cZ,

E[ ˆZ 2] =

E[e2Xm/M ] =

(cid:16)

Γ(1 − 2/M )e2(ln Z−c)/M (cid:17)M

= Γ(1 − 2/M )M e−2cZ 2.

M
(cid:89)

m=1

M
(cid:89)

m=1

Therefore the squared bias, variance and MSE of the estimator ˆZ are, respectively:

bias( ˆZ)2 = (E[ ˆZ] − Z)2 = Z 2 (cid:0)Γ(1 − 1/M )M e−c − 1(cid:1) ,

var( ˆZ) = E[ ˆZ 2] − E[ ˆZ]2 = Z 2 (cid:0)Γ(1 − 2/M )M e−2c − Γ(1 − 1/M )2M e−2c(cid:1) ,

MSE( ˆZ) = bias( ˆZ)2 + var( ˆZ) = Z 2 (cid:0)Γ(1 − 2/M )M e−2c − 2Γ(1 − 1/M )M e−c + 1(cid:1) .

These formulas hold for M > 2 where the moment generating functions are deﬁned. For M = 1 the estimator has inﬁnite
bias (and inﬁnite variance), and for M = 2 it has inﬁnite variance. Figure 1 (left) shows the functional dependence of
MSE( ˆZ) on the number of samples M ≥ 3, in units of Z 2.

Exponential trick The Exponential trick yields an unbiased estimator of 1/Z, and we can turn it into a consistent
estimator of Z by inverting it:

ˆZ :=

(cid:33)−1

(cid:32)

1
M

M
(cid:88)

m=1

Xm

where

X1, . . . , XM

iid∼ Exp(Z).

As X1, . . . , XM are independent and exponentially distributed with identical rates Z, their sum follows the Gamma distri-
bution with shape M and rate Z. Therefore the estimator ˆZ can be written as ˆZ = M Y , where Y ∼ InvGamma(M, Z).

Recalling the mean and variance of the Inverse-Gamma distribution, we obtain:

Lost Relatives of the Gumbel Trick

bias( ˆZ)2 = (E[ ˆZ] − Z)2 = Z 2

(cid:18) M

M − 1

(cid:19)

− 1

= Z 2

1
M − 1

,

var( ˆZ) = Z 2M 2

1
(M − 1)2(M − 2)
MSE( ˆZ) = bias( ˆZ)2 + var( ˆZ) = Z 2 M − 2 + M 2

,

(M − 1)2(M − 2)

= Z 2

M + 2
(M − 1)(M − 2)

.

Again these formulas hold for M > 2 where the relevant expectations are deﬁned: for M = 1 the estimator has inﬁnite
bias, and for M ∈ {1, 2} it has inﬁnite variance. Figure 1 (left) shows the functional dependence of MSE( ˆZ) on the
number of samples M ≥ 3, in units of Z 2. By inspecting the curves we observe that the Gumbel trick estimator requires
roughly 45% more samples to yield the same MSE as the Exponential trick estimator.

A.2. Estimating ln Z

A similar analysis can be performed for estimating ln Z rather than Z. In that case the Gumbel trick estimator of ln Z is
unbiased and has variance (and thus MSE) equal to 1
M

π2
6 . On the other hand, the Exponential trick estimator is

(cid:100)ln Z = − ln

(cid:32)

1
M

M
(cid:88)

m=1

(cid:33)

Xm

where

X1, . . . , XM

iid∼ Exp(Z).

Again (cid:80)M

m=1 Xm ∼ Gamma(M, Z) and by reference to properties of the Gamma distribution,

bias( (cid:100)ln Z)2 = (E[ ˆZ] − Z)2 = (ln M − (ψ(M ) − ln Z) − ln Z)2 = (ln M − ψ(M ))2 ,

var( (cid:100)ln Z) = ψ1(M ),

MSE( (cid:100)ln Z) = bias( (cid:100)ln Z)2 + var( (cid:100)ln Z) = (ln M − ψ(M ))2 + ψ1(M ),

where ψ(·) is the digamma function and ψ1(·) is the trigamma function. Note that the estimator can be debiased by
subtracting its bias (ln M − ψ(M )). Figure 1 (right) compares the MSE of the Gumbel and Exponential trick estimators
of ln Z. We observe that the Gumbel trick estimator performs better only for M = 1, and even in that case the Exponential
trick estimator is better when debiased.

B. Sum-unary perturbations

Recall that sum-unary perturbations refer to the setting where each variable’s unary potentials are perturbed with Gumbel
noise, and the perturbed potential of a conﬁguration sums the perturbations from all variables (see Deﬁnition 3 in the
main text). Using sum-unary perturbations we can derive a family U(α) of upper bounds on the log partition function
(Proposition 4) and construct sequential samplers for the Gibbs distribution (Algorithm 1). Here we provide proofs for the
related results stated in Sections 3.1 and 3.2.

Notation We will write powβ x for xβ, where x, β ∈ R, when we ﬁnd this increases clarity of our exposition.
Lemma 13 (Weibull and Fr´echet tricks). For any ﬁnite set Y and any function h, we have

pow
−α

pow
−α

(cid:88)

y∈Y

(cid:88)

y∈Y

pow
−1/α

pow
−1/α

h(y) = EW

h(y) = EF

(cid:20)

(cid:26)

min
y

h(y)

W (y)
Γ(1 + α)

(cid:27)(cid:21)

(cid:20)

(cid:26)

max
y

h(y)

F (y)
Γ(1 + α)

(cid:27)(cid:21)

where {W (y)}y∈Y

i.i.d.∼ Weibull(1, α−1)

for α ∈ (0, ∞),

where {F (y)}y∈Y

i.i.d.∼ Fr´echet(1, −α−1)

for α ∈ (−1, 0).

Proof. This follows from setting up competing exponential clocks with rates λy = h(y)−1/α and then applying the func-
tion g(x) = xα as in Example 1 for the case of the Weibull trick. The case of the Fr´echet trick is similar, except that g is
strictly decreasing for α ∈ (−1, 0), hence the maximization in place of the minimization.

Lost Relatives of the Gumbel Trick

B.1. Upper bounds on the partition function

Proposition 4. For any α ∈ (−1, 0) ∪ (0, ∞), the upper bound ln Z ≤ U(α) holds with

U(α) := n

ln Γ(1 + α)
α

+ nc −

ln Eγ

(cid:2)e−αU (cid:3) .

1
α

Proof. We show the result for α ∈ (0, ∞) using the Weibull trick; the case of α ∈ (−1, 0) can be proved similarly using
the Fr´echet trick. The idea is to prove by induction on n that Z −α ≥ e−αU (α), so that the claimed result follows by
applying the monotonically decreasing function x (cid:55)→ − ln(x)/α.

The base case n = 1 is the Clamping Lemma 7 below with j = n = 1. Now assume the claim for n − 1 ≥ 1 and for
xn ∈ Xn deﬁne

Un−1(α, x1) := (n − 1)

+ (n − 1)c −

exp

−α max

φ(x) +

γi(xi)

.

ln Γ(1 + α)
α

1
α

ln Eγ

x2,...,xn

(cid:34)

(cid:32)

(cid:40)

(cid:41)(cid:33)(cid:35)

n
(cid:88)

i=2

With this deﬁnition, the Clamping Lemma with j = 1 states that (cid:80)

pow−1/α e−αUn−1(α,x1) ≤ pow−1/α e−αU (α), so:

x1

Z −α ≥ pow
−α

(cid:88)

x1∈X1

pow
−1/α

e−αUn−1(α,x1)

e−αU (α)

≥ pow
−α

pow
−1/α
= e−αU (α),

[inductive hypothesis]

[Clamping Lemma]

as required to complete the inductive step.

Proposition 5. The limit of U(α) as α → 0 exists and equals U(0) := E[U ], i.e. the Gumbel trick upper bound.

α ln E (cid:2)e−αU (cid:3). The ﬁrst term tends to nψ(1) = −cn as α → 0 by
Proof. Recall that U(α) = n ln Γ(1+α)
L’Hˆopital’s rule, where ψ is the digamma function. The second term is constant in α. In the last term, E (cid:2)e−αU (cid:3) is the
moment generating function of U evaluated at −α, and as such its derivative at α = 0 exists and equals the negative of the
mean of U . Hence by L’Hˆopital’s rule,

+ nc − 1

α

− lim
α→0

1
α

ln E (cid:2)e−αU (cid:3) = − lim

−E[U ]
E [e−αU ]

α→0

= E[U ] = U(0).

The claimed result then follows by the Algebra of Limits, as the contributions of the ﬁrst two terms cancel.

Proposition 6. The function U(α) is differentiable at α = 0 and the derivative equals

d
dα

(cid:12)
(cid:12)
U(α)
(cid:12)
(cid:12)α=0

= n

−

π2
12

var(U )
2

.

Proof. First we show that U(α) is differentiable on (−1, 0) ∪ (0, ∞), and that the limit of the derivative as α → 0 exists
and equals nπ2/12 − var(U )/2.

The ﬁrst term of U(α) is n ln Γ(1+α)
derivative equals

α

, which is differentiable for α ∈ (−1, 0) ∪ (0, ∞) by the Quotient Rule, and its

d
dα

n

ln Γ(1 + α)
α

= n

ψ(1 + α)α − ln Γ(1 + α)
α2

,

where ψ is the digamma function (logarithmic derivative of the gamma function). Applying L’Hˆopital’s rule we note that

lim
α→0

d
dα

n

ln Γ(1 + α)
α

= n lim
α→0

ψ(1 + α) + αψ(1)(1 + α) − ψ(1 + α)
2α

= n

ψ(1)(1)
2

= n

= n

ζ(2)
2

π2
12

,

Lost Relatives of the Gumbel Trick

where ψ(1) is the trigamma function (derivative of the digamma function), whose value at 1 is known to be ζ(2) = π2/6,
the Riemann zeta function evaluated at 2.

The second term of U(α) is constant in α. The last term can be written as K(−α)/(−α), where K is the cumulant
generating function (logarithm of the moment generating function) of the random variable U . The cumulant generating
function is differentiable, and by the Quotient rule

d
dα

K(−α)
−α

= −

αK (cid:48)(−α) − K(−α)
α2

.

Applying L’Hˆopital’s rule we note that

lim
α→0

d
dα

K(−α)
−α

= lim
α→0

K (cid:48)(−α) + αK (cid:48)(cid:48)(−α) − K (cid:48)(−α)
2α

=

K (cid:48)(cid:48)(0)
2

=

var(U )
2

,

where we have used that the second derivative of the cumulant generating function is the variance.

As U(α) is continuous at 0 by construction, the above implies that it has left and right derivatives at 0. As the values of
these derivatives coincide, the function is differentiable at 0 and the derivative has the stated value.

Recall that for a variable index j ∈ {1, . . . , n} we also deﬁned partial sum-unary perturbations

Uj(x1, . . . , xj−1) := max
xj ,...,xn

φ(x) +

γi(xi)

,






n
(cid:88)

i=j






which ﬁx the variables x1, . . . , xj−1 and perturb the remaining ones.
Lemma 7 (Clamping Lemma). For any j ∈ {1, . . . , n} and any ﬁxed partial variable assignment (x1, . . . , xj−1) ∈
X1 × · · · × Xj−1, the following inequality holds with any trick parameter α ∈ (−1, 0) ∪ (0, ∞):

Proof. For α > 0, from the Weibull trick (Lemma 13), using independence of the perturbations and Jensen’s inequality,

xj ∈Xj

≤ Eγ

pow
−α

(cid:88)

xj ∈Xj


(cid:88)

Eγ

(cid:104)

e−(n−j) ln Γ(1+α)−α(n−j)c)e−αUj+1(x1,...,xj )(cid:105)−1/α

(cid:104)

e−(n−(j−1)) ln Γ(1+α)−α(n−(j−1))c)e−αUj (x1,...,xj−1)(cid:105)−1/α

.

EW

pow
−1/α

 min

xj+1,...,xn

˜p(x)−α

W (xi)
Γ(1 + α)

n
(cid:89)

i=j+1

n
(cid:89)

i=j+1









W (xi)
Γ(1 + α)

W (xj)
Γ(1 + α)



















= EW

 min
xj ∈Xj

EW

 min

xj+1,...,xn

˜p(x)−α



≤ EW

 min
xj ,...,xn

˜p(x)−α

n
(cid:89)

i=j

W (xi)
Γ(1 + α)





Representing the Weibull random variables in terms of Gumbel random variables using the transformation W = e−(γ+c)α,
where γ ∼ Gumbel(−c), and manipulating the obtained expressions yields the claimed result.

B.2. Sequential samplers for the Gibbs distribution

Lost Relatives of the Gumbel Trick

The family of sequential samplers for the Gibbs distribution presented in the main text as Algorithm 1 has the same overall
structure as the sequential sampler derived by Hazan et al. (2013) from the Gumbel trick upper bound U(0), and hence
correctness can be argued similarly. Conditioned on accepting the sample, the probability that x = (x1, . . . , xn) is returned
is

n
(cid:89)

i=1

pi(xi) =

n
(cid:89)

i=1

e−c
Γ(1 + α)1/α

Eγ
Eγ

(cid:2)e−αUi+1(x1,...,xi)(cid:3)−1/α
(cid:2)e−αUi(x1,...,xi−1)(cid:3)−1/α

=

e−nc
Γ(1 + α)n/α

(cid:0)e−αφ(x1,...,xn)(cid:1)−1/α
E[e−αU ]−1/α

∝ p(x),

as required to show that the produced samples follow the Gibbs distribution p. Note, however, that in practice one intro-
duces an approximation by replacing expectations with sample averages.

B.3. Relationship between errors of sum-unary Gumbel perturbations

We write x∗ for the (random) MAP conﬁguration after sum-unary perturbation of the potential function, i.e.,

(cid:40)

(cid:41)

x∗ := argmax

φ(x) +

γi(xi)

.

x∈X

n
(cid:88)

i=1

Let qsum(x) := P[x = x∗] be the probability mass function of x∗.

The following results links together the errors acquired when using summed unary perturbations to upper bound the log
partition function ln Z ≤ U(0) using the Gumbel trick upper bound by Hazan & Jaakkola (2012), to approximately sample
from the Gibbs distribution by using qsum instead, and to upper bound the entropy of the approximate distribution qsum
using the bound due to Maji et al. (2014).
Proposition 11. Writing p for the Gibbs distribution, we have

Proof. By conditioning on the maximizing conﬁguration x∗, we can rewrite the Gumbel trick upper bound U(0) as follows:

(U(0) − ln Z)
(cid:125)
(cid:123)(cid:122)
(cid:124)
error in ln Z bound

+ KL(qsum (cid:107) p)
(cid:123)(cid:122)
(cid:125)
sampling error

(cid:124)

= Eγi [γi(x∗

i )] − H(qsum)
(cid:125)
(cid:123)(cid:122)
error in entropy estimation

(cid:124)

.

U(0) = Eγ

θ(x) +

γi(xi)

(cid:34)

(cid:40)

max
x∈X

(cid:32)

qsum(x)

θ(x) + Eγ

γi(xi) | x = x∗

(cid:35)(cid:33)

(cid:41)(cid:35)

(cid:34) n
(cid:88)

i=1

qsum(x)θ(x) +

Eγi [γi(x∗

i )] .

=

=

(cid:88)

x∈X

(cid:88)

x∈X

n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:88)

x∈X
(cid:88)

x∈X

At the same time, the KL divergence between qsum and the Gibbs distribution p generally expands as

KL(qsum (cid:107) p) = −H(qsum) −

qsum(x) ln

exp (θ(x))
˜x∈X exp (θ(˜x))

(cid:80)

= −H(qsum) −

qsum(x)θ(x) + ln Z.

Adding the two equations together and rearranging yields the claimed result.

Lost Relatives of the Gumbel Trick

C. Averaged unary perturbations

C.1. Lower bounds on the partition function

In the main text we stated the following two lower bounds on the log partition function ln Z.
Proposition 9. Let α ∈ (−1, 0) ∪ (0, ∞). For any subset S ⊆ {1, . . . , n} of the variables x1, . . . , xn we have ln Z ≥

c +

ln Γ(1 + α)
α

−

1
α

ln E

(cid:104)

e−α maxx{φ(x)+γS (xS )}(cid:105)

,

where xS := {xi : i ∈ S} and γS(xS) ∼ Gumbel(−c) independently for each setting of xS.

Proof. Let ¯S := {1, . . . , n} \ S. First we handle the case α > 0. We have trivially that

pow−α Z = pow−α

eφ(xS ,x ¯S ) ≤ pow−α

eφ(xS ,x ¯S ).

(cid:88)

(cid:88)

xS

x ¯S

(cid:88)

xS

max
x ¯S

The Weibull trick tells us that pow−α
Applying this to the summation over xS on the right-hand side of the above inequality, we obtain
(cid:34)

y pow−1/α h(y) = EW [miny

h(y)
Γ(1+α) W (y)] where {W (y)}y

(cid:35)

(cid:80)

pow−α Z ≤ EW

pow−α maxx ¯S eφ(xS ,x ¯S )
Γ(1 + α)

min
xS

W (xS)

.

iid∼ Weibull(1, α−1).

Expressing the Weibull random variable W (xS) as e−α(γS (xS )+c) with γS(xS) ∼ Gumbel(−c), the right-hand side can
be simpliﬁed as follows:

Taking the logarithm and dividing by −α < 0 yields the claimed result for positive α. For α ∈ (−1, 0) we proceed
similarly, obtaining that

pow−α Z ≤

1
Γ(1 + α)
e−αc
Γ(1 + α)

=

Eγ

(cid:20)
pow−α max
xS

max
x ¯S

eφ(xS ,x ¯S )eγS (xS )+c

(cid:21)

(cid:104)

Eγ

(cid:16)

exp

−α max

{φ(x) + γS(xS)}

(cid:17)(cid:105)

.

x

pow−α Z ≥ pow−α

(cid:88)

max
x ¯S

eφ(xS ,x ¯S )

(cid:34)

xS
pow−α maxx ¯S eφ(xS ,x ¯S )
Γ(1 + α)

min
xS

(cid:35)

F (xS)

,

= EF

where F (x(S)) ∼ Fr´echet(1, −α−1). Representing these random variables as e−α(γS (xS )+c) with γS(xS) ∼
Gumbel(−c), simplifying as in the previous case and ﬁnally dividing the inequality by −α > 0 yields the claimed re-
sult for α ∈ (−1, 0).

Corollary 10. For any α ∈ (−1, 0) ∪ (0, ∞), we have the lower bound ln Z ≥ L(α), where

L(α) := c +

ln Γ(1 + α)
α

−

1
nα

ln E [exp (−nαL)] ,

Proof. Applying Proposition 9 n times with all singleton sets S = {i} and averaging the obtained lower bounds yields

ln Z ≥ c +

ln Γ(1 + α)
α

−

1
n

n
(cid:88)

i=1

1
α

(cid:104)

ln E

(cid:16)

exp

−α max

{φ(x) + γi(xi)}

x

= c +

ln Γ(1 + α)
α

−

= c +

ln Γ(1 + α)
α

−

ln E

exp

−

(cid:34)

(cid:34)

n
(cid:88)

i=1

(cid:32)

(cid:32)

ln E

exp

−nα

1
nα

1
nα

α max
x

{φ(x) + γi(xi)}

1
n

n
(cid:88)

i=1

max
x

(cid:33)(cid:35)

{φ(x) + γi(xi)}

,

(cid:17)(cid:105)

(cid:33)(cid:35)

Lost Relatives of the Gumbel Trick

where the ﬁrst equality used the fact that the perturbations γi(xi) are mutually independent for different indices i to replace
the product of expectations with the expectation of the product. The claimed result follows by applying Jensen’s inequality
to swap the summation and the convex maxx function, noting that the inequality works out the right way for both positive
and negative α.

Jensen’s inequality can be used to relate the general lower bound L(α) to the Gumbel trick lower bound L(0), showing
that the former cannot be arbitrarily worse than the latter:
Proposition 14. For all α ∈ (−1, 0), the lower bound L(α) on ln Z satisﬁes

L(α) ≥ L(0) +

ln Γ(1 + α)
α

+ c

Proof. Apply Jensen’s inequality with the convex function x (cid:55)→ e−nα to the last term in the deﬁnition of L(α), noting that
the inequality works out the stated way for α < 0.

Note that ln Γ(1+α)
Gumbel lower bound L(0); it merely says that they cannot be arbitrarily worse than L(0).

+ c ≤ 0 for α ∈ (−1, 0) so this result does not imply that the Fr´echet lower bounds are tighter than the

α

C.2. Relationship between errors of averaged-unary Gumbel perturbations

In this section we write x∗ for the (random) MAP conﬁguration after average-unary perturbation of the potential function,
i.e.,

(cid:40)

x∗ := argmax

φ(x) +

x∈X

(cid:41)

γi(xi)

.

1
n

n
(cid:88)

i=1

where {γi(xi) | xi ∈ Xi, 1 ≤ i ≤ n} i.i.d.∼ Gumbel(−c). Let qavg(x) := P[x = x∗] be the probability mass function of x∗.
The Gumbel trick lower bound on the log partition function ln Z due to Hazan et al. (2013) is:

ln Z ≥ L(0) = Lφ(0) := Eγ

(cid:34)

(cid:40)

min
x∈X

φ(x) +

(cid:41)(cid:35)

γi(xi)

.

1
n

n
(cid:88)

i=1

(3)

We show that the gap of this Gumbel trick lower bound on ln Z upper bounds the KL divergence between the approximate
distribution qavg and the Gibbs distribution p. To this end, we ﬁrst need an entropy bound for qavg analogous to Theorem 1
of (Maji et al., 2014).
Theorem 15. The entropy of qavg can be lower bounded using expected values of max-perturbations as follows:

H(qavg) ≥

Eγi [γi(x∗

i )]

1
n

n
(cid:88)

i=1

Remark. Theorem 1 of (Maji et al., 2014) and this Theorem 15 differ in three aspects: (1) the former is an upper bound and
the latter is a lower bound, (2) the former sums the expectations while the latter averages them, and (3) the distributions
qsum and qavg of x∗ in the two theorems are different.

Proof. By the duality relation between negative entropy and the log partition function (Wainwright & Jordan, 2008), the
entropy H(qavg) of the unary-avg perturb-max distribution qavg can be expressed as
(cid:41)

(cid:40)

H(qavg) = inf
ϕ

ln Zϕ −

qavg(x)ϕ(x)

,

where the variable ϕ ranges over all potential functions on X , and Zϕ = (cid:80)
lower bound on the log partition function gives

x∈X exp ϕ(x). Applying the Gumbel trick

(cid:88)

x∈X

(cid:88)

x∈X

(cid:40)

(cid:41)

H(qavg) ≥ inf
ϕ

Lϕ(0) −

qavg(x)ϕ(x)

,

Lost Relatives of the Gumbel Trick

Proposition 16 in Appendix D shows that Lϕ(0) is a convex function of ϕ. The expression − (cid:80)
x∈X q(x)ϕ(x) is a linear
function of ϕ, so also convex, and thus as a sum of two convex functions, the quantity Lϕ(0) − (cid:80)
x∈X q(x)ϕ(x) within
the inﬁmum is a convex function of ϕ. Moreover, Proposition 17 in Appendix D tells us that the partial derivatives can be
computed as

(cid:32)

∂
∂ϕ(x)

(cid:88)

x∈X

(cid:33)

Lϕ(0) −

qavg(x)ϕ(x)

= qϕ(x) − qavg(x)

where qϕ(x) is the unary-avg perturb-max distribution associated with the potential function ϕ. Proposition 18 in Ap-
pendix D conﬁrms that these partial derivatives are continuous, so we observe that as a function of ϕ, the expression
Lϕ(0) − (cid:80)
x∈X qavg(x)ϕ(x) is a convex function with continuous partial derivatives, so it is a differentiable convex func-
tion. This is sufﬁcient to establish that the point ϕ = φ is a global minimum of this function (Wright & Nocedal, 1999).
Hence

(cid:40)

(cid:41)

H(qavg) ≥ inf
ϕ

Lϕ(0) −

qavg(x)ϕ(x)

= Lφ(0) −

qavg(x)φ(x)

(cid:88)

x∈X

(cid:34)

(cid:88)

x∈X

(cid:88)

=

qavg(x)Eγ

φ(x) +

γi(xi) | x = x∗

−

qavg(x)φ(x)

1
n

n
(cid:88)

i=1

(cid:35)

(cid:88)

x∈X

x∈X

1
n

n
(cid:88)

i=1

=

Eγi [γi(x∗

i )]

where we conditioned on the maximizing conﬁguration x∗ when expanding Lφ(0).

Remark. This proof proceeded in the same way as the proof of Maji et al. (2014) for the upper bound, except that es-
tablishing the minimizing conﬁguration of the inﬁmum is a non-trivial step that is actually required in this case. The
second revision of (Hazan et al., 2016) computes the derivative of Uϕ(0) − (cid:80)
x∈X qsum(x)ϕ(x), which is similar to our
Lϕ(0) − (cid:80)
x∈X qavg(x)ϕ(x), by differentiating under the expectation.

Equipped with Theorem 15, we can now show a link between the approximation “errors” of the averaged-unary perturba-
tion MAP conﬁguration distribution qavg (to the Gibbs distribution p) and estimate L(0) (to ln Z).
Proposition 12. Let p be the Gibbs distribution on X . Then

ln Z − L(0)
(cid:125)
(cid:123)(cid:122)
(cid:124)
error in ln Z bound

≥ KL(qavg (cid:107) p)
(cid:123)(cid:122)
(cid:125)
sampling error

(cid:124)

≥ 0

Remark. While we knew from Hazan et al. (2013) that ln Z − L(0) ≥ 0 (i.e. that L(0) is a lower bound on ln Z), this
is a stronger result showing that the size of the gap is an upper bound on the KL divergence between the average-unary
perturbation MAP distribution qavg and the Gibbs distribution p.

Proof. The Kullback-Leibler divergence in question expands as

KL(qavg (cid:107) p) = −H(qavg) −

qavg(x) ln

(cid:80)

= −H(qavg) −

qavg(x)φ(x) + ln Z.

(cid:88)

x∈X

exp φ(x)
˜x∈X exp φ(˜x)

(cid:88)

x∈X

From the proof of Theorem 15 we know that H(qavg) ≥ L(0) − (cid:80)

x∈X qavg(x)φ(x), so

KL(qavg (cid:107) p) ≤ −L(0) +

qavg(x)φ(x) −

qavg(x)φ(x) + ln Z = ln Z − L(0).

(cid:88)

x∈X

(cid:88)

x∈X

D. Technical results

Lost Relatives of the Gumbel Trick

In this section we write L(φ) instead of Lφ(0) for the Gumbel trick lower bound on ln Z associated with the potential
function φ, see equation (3).

Proposition 16. The Gumbel trick lower bound L(φ), viewed as a function of the potentials φ, is convex.

Proof. Convexity can be proved directly from deﬁnition. Let φ1 and φ2 be two arbitrary potential functions on a discrete
product space X , and let λ ∈ [0, 1]. Then

L(λφ1 + (1 − λ)φ2)

= Eγ

max
x∈X

= Eγ

max
x∈X

(cid:34)

(cid:34)

(cid:34)

(cid:40)

(cid:40)

(cid:32)

(cid:40)

λφ1(x) + (1 − λ)φ2(x) +

γi(xi)

(cid:41)(cid:35)

1
n

n
(cid:88)

i=1

(cid:33)

(cid:32)

(cid:41)

(cid:40)

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

= λL(φ1) + (1 − λ)L(φ2),

≤ Eγ

λ max
x∈X

φ1(x) +

γi(xi)

+ (1 − λ) max
x∈X

φ2(x) +

γi(xi)

(cid:33)(cid:41)(cid:35)

(cid:41)(cid:35)

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

λ

φ1(x) +

γi(xi)

+ (1 − λ)

φ2(x) +

γi(xi)

where we have used convexity of the max function to obtain the inequality, and linearity of expectation to arrive at the ﬁnal
equality.

Remark. This convexity proof goes through for other (low-dimensional) perturbations as well, e.g. it also works for Uφ(0).
Proposition 17. The Gumbel trick lower bound L(φ), viewed as a function of the potentials φ, has partial derivatives

∂
∂φ(˜x)

L(φ) = qφ(˜x)

where qφ is the probability mass function of the average-unary perturbation MAP conﬁguration’s distribution associated
with the potential function φ.

Proof. Let ˜x ∈ X , so that φ(˜x) is a general component of φ, and let e˜x be the indicator vector of ˜x. For any δ ∈ R, the
change in the lower bound L due to replacing φ(˜x) with φ(˜x) + δ is

L(φ + δe˜x) − L(φ) = Eγ

φ(x) + δ1{x = ˜x} +

γi(xi)

− Eγ

= Eγ

max
x∈X

φ(x) + δ1{x = ˜x} +

γi(xi)

− max
x∈X

φ(x) +

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:41)(cid:35)

(cid:34)

(cid:40)

max
x∈X

φ(x) +

(cid:41)

(cid:40)

(cid:41)(cid:35)

1
n

n
(cid:88)

i=1

γi(xi)

γi(xi)

(cid:41)(cid:35)

1
n

n
(cid:88)

i=1

max
x∈X

(cid:34)

(cid:34)

(cid:40)

(cid:40)

= Eγ [∆(φ, δ, ˜x, γ)]

by linearity of expectation, where we have denoted by ∆(φ, δ, ˜x, γ) the change in maximum due to replacing the potential
φ(˜x) with φ(˜x) + δ. Let’s condition on the argmax before modifying φ:

L(φ + δe˜x) − L(φ) = Eγ [∆(φ, δ, ˜x, γ)] =

qφ(x)Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax]

(cid:88)

x∈X

Now let’s condition on the size of the gap G between the maximum and the runner-up:

Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax] = P(G ≤ |δ|)Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax, G ≤ |δ|]
+ P(G > |δ|)Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax, G > |δ|]

Let’s examine all four terms on the right-hand side one by one:

Lost Relatives of the Gumbel Trick

1. P(G ≤ |δ|) → P(G = 0) = 0 as δ → 0 by monotonicity of measure.
2. Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax, G ≤ |δ|] ≤ δ since |∆(φ, δ, ˜x, γ)| ≤ |δ| always holds.
3. P(G > |δ|) → P(G ≥ 0) = 1 as δ → 0 by monotonicity of measure.
4. Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax, G > |δ|] = δ1{x = ˜x} since in this case both maximizations in the

deﬁnition of ∆(φ, δ, ˜x, γ) are maximized at x.

Therefore, as δ → 0,

Putting things together, we have

Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax] = o(1)o(δ) + (1 + o(1))δ1{x = ˜x}

lim
δ→0

L(φ + δe˜x) − L(φ)
δ

qφ(x) lim
δ→0

1
δ

Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax]

(cid:88)

x∈X
(cid:88)

=

=

x∈X
= qφ(˜x),

qφ(x)1{x = ˜x}

which proves the stated claim directly from deﬁnition of a partial derivative.

Proposition 18. The probability mass function qφ of the average-unary perturbation MAP conﬁguration’s distribution
associated with a potential function φ is continuous in φ.

Proof. For any x∗ ∈ X we have from deﬁnition

qφ(x∗) = P

(cid:34)
x∗ = argmax

(cid:40)

x∈X

φ(x) +

γi(xi)

1
n

n
(cid:88)

i=1

(cid:41)(cid:35)

(cid:40)

(cid:34)
φ(x∗) +

= P

1
n

n
(cid:88)

i=1

(cid:34)

(cid:40)

= E

1

φ(x∗) +

1
n

n
(cid:88)

i=1

γi(x∗

i ) > max

x∈X \{x∗}

φ(x) +

γi(xi)

(cid:40)

(cid:41)(cid:41)(cid:35)

γi(x∗

i ) > max

x∈X \{x∗}

φ(x) +

γi(xi)

(cid:41)(cid:35)

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

which is continuous in φ by continuity of max, of 1 {· > ·} (as a function of φ) and by the Bounded Convergence Theorem.

Remark. The results above show that the Gumbel trick lower bound L(φ), viewed as a function of the potentials φ, is
convex and has continuous partial derivatives.

Lost Relatives of the Gumbel Trick

Matej Balog 1 2 Nilesh Tripuraneni 3 Zoubin Ghahramani 1 4 Adrian Weller 1 5

7
1
0
2
 
n
u
J
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
6
1
4
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

The Gumbel trick is a method to sample from a
discrete probability distribution, or to estimate its
normalizing partition function. The method re-
lies on repeatedly applying a random perturba-
tion to the distribution in a particular way, each
time solving for the most likely conﬁguration.
We derive an entire family of related methods,
of which the Gumbel trick is one member, and
show that the new methods have superior prop-
erties in several settings with minimal additional
computational cost. In particular, for the Gum-
bel trick to yield computational beneﬁts for dis-
crete graphical models, Gumbel perturbations on
all conﬁgurations are typically replaced with so-
called low-rank perturbations. We show how a
subfamily of our new methods adapts to this set-
ting, proving new upper and lower bounds on the
log partition function and deriving a family of se-
quential samplers for the Gibbs distribution. Fi-
nally, we balance the discussion by showing how
the simpler analytical form of the Gumbel trick
enables additional theoretical results.

1. Introduction

In this work we are concerned with the fundamental prob-
lem of sampling from a discrete probability distribution and
evaluating its normalizing constant. A probability distribu-
tion p on a discrete sample space X is provided in terms
of its potential function φ : X → [−∞, ∞), correspond-
ing to log-unnormalized probabilities via p(x) = eφ(x)/Z,
where the normalizing constant Z is the partition function.
In this context, p is the Gibbs distribution on X associated
with the potential function φ. The challenges of sampling
from such a discrete probability distribution and estimating
the partition function are fundamental problems with ubiq-

1University of Cambridge, UK 2MPI-IS, T¨ubingen, Germany
3UC Berkeley, USA 4Uber AI Labs, USA 5Alan Turing Institute,
UK. Correspondence to: Matej Balog <ﬁrst.last@gmail.com>.
Code: https://github.com/matejbalog/gumbel-relatives.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

uitous applications in machine learning, classical statistics
and statistical physics (see, e.g., Lauritzen, 1996).

Perturb-and-MAP methods (Papandreou & Yuille, 2010)
constitute a class of randomized algorithms for estimating
partition functions and sampling from Gibbs distributions,
which operate by randomly perturbing the corresponding
potential functions and employing maximum a posteriori
(MAP) solvers on the perturbed models to ﬁnd a maximum
probability conﬁguration. This MAP problem is NP-hard
in general; however, substantial research effort has led to
the development of solvers which can efﬁciently compute
or estimate the MAP solution on many problems that occur
in practice (e.g., Boykov et al., 2001; Kolmogorov, 2006;
Darbon, 2009). Evaluating the partition function is a harder
problem, containing for instance #P-hard counting prob-
lems. The general aim of perturb-and-MAP methods is
to reduce the problem of partition function evaluation, or
the problem of sampling from the Gibbs distribution, to re-
peated instances of the MAP problem (where each instance
is on a different random perturbation of the original model).

The Gumbel trick (Papandreou & Yuille, 2011) relies on
adding Gumbel-distributed noise to each conﬁguration’s
potential φ(x). We derive a wider family of perturb-and-
MAP methods that can be seen as perturbing the model in
different ways – in particular using the Weibull and Fr´echet
distributions alongside the Gumbel. We show that the new
methods can be implemented with essentially no additional
computational cost by simply averaging existing Gumbel
MAP perturbations in different spaces, and that they can
lead to more accurate estimators of the partition function.

Evaluating or perturbing each conﬁguration’s potential
with i.i.d. Gumbel noise can be computationally expensive.
One way to mitigate this is to cleverly prune computation
in regions where the maximum perturbed potential is un-
likely to be found (Maddison et al., 2014; Chen & Ghahra-
mani, 2016). Another approach exploits the product struc-
ture of the sample space in discrete graphical models, re-
placing i.i.d. Gumbel noise with a “low-rank” approxima-
tion. Hazan & Jaakkola (2012); Hazan et al. (2013) showed
that from such an approximation, upper and lower bounds
on the partition function and a sequential sampler for the
Gibbs distribution can still be recovered. We show that a
subfamily of our new methods, consisting of Fr´echet, Ex-
ponential and Weibull tricks, can also be used with low-

Lost Relatives of the Gumbel Trick

rank perturbations, and use these tricks to derive new upper
and lower bounds on the partition function, and to construct
new sequential samplers for the Gibbs distribution.

and-MAP and accept-reject samplers, exploiting the con-
nection between the Gumbel distribution and competing
exponential clocks that we also discuss in Section 2.1.

Our main contributions are as follows:

1. A family of tricks that can be implemented by simply
averaging Gumbel perturbations in different spaces, and
which can lead to more accurate or more sample efﬁ-
cient estimators of Z (Section 2).

2. New upper and lower bounds on the partition function of
a discrete graphical model computable using low-rank
perturbations, and a corresponding family of sequential
samplers for the Gibbs distribution (Section 3).

3. Discussion of advantages of the simpler analytical form
of the Gumbel trick including new links between the er-
rors of estimating Z, sampling, and entropy estimation
using low-rank Gumbel perturbations (Section 4).

Background and Related work The idea of perturbing
the potential function of a discrete graphical model in or-
der to sample from its associated Gibbs distribution was in-
troduced by Papandreou & Yuille (2011), inspired by their
previous work on reducing the sampling problem for Gaus-
sian Markov random ﬁelds to the problem of ﬁnding the
mean, using independent local perturbations of each Gaus-
sian factor (Papandreou & Yuille, 2010). Tarlow et al.
(2012) extended this perturb-and-MAP approach to sam-
pling, in particular by considering more general structured
prediction problems. Hazan & Jaakkola (2012) pointed out
that MAP perturbations are useful not only for sampling the
Gibbs distribution (considering the argmax of the perturbed
model), but also for bounding and approximating the parti-
tion function (by considering the value of the max).

Afterwards, Hazan et al. (2013) derived new lower bounds
on the partition function and proposed a new sampler for
the Gibbs distribution that samples variables of a discrete
graphical model sequentially, using expected values of low-
rank MAP perturbations to construct the conditional proba-
bilities. Due to the low-rank approximation, this algorithm
has the option to reject a sample. Orabona et al. (2014)
and Hazan et al. (2016) subsequently derived measure con-
centration results for the Gumbel distribution that can be
used to control the rejection probability. Maji et al. (2014)
derived an uncertainty measure from random MAP pertur-
bations, using it within a Bayesian active learning frame-
work for interactive image boundary annotation.

Perturb-and-MAP was famously generalized to continuous
spaces by Maddison et al. (2014), replacing the Gumbel
distribution with a Gumbel process and calling the resulting
algorithm A* sampling. Maddison (2016) cast this work
into a uniﬁed framework together with adaptive rejection
sampling techniques, based on the notion of exponential
races. This recent view generally brings together perturb-

Inspired by A* sampling, Kim et al. (2016) proposed an ex-
act sampler for discrete graphical models based on lazily-
instantiated random perturbations, which uses linear pro-
gramming relaxations to prune the optimization space. Fur-
ther recent applications of perturb-and-MAP include struc-
tured prediction in computer vision (Bertasius et al., 2017)
and turning the discrete sampling problem into an opti-
mization task that can be cast as a multi-armed bandit prob-
lem (Chen & Ghahramani, 2016), see Section 5.2 below.

In addition to perturb-and-MAP methods, we are aware of
three other approaches to estimate the partition function
of a discrete graphical model via MAP solver calls. The
WISH method (weighted-integrals-and-sums-by-hashing,
Ermon et al., 2013) relies on repeated MAP inference calls
applied to the model after subjecting it to random hash con-
straints. The Frank-Wolfe method may be applied by itera-
tively updating marginals using a constrained MAP solver
and line search (Belanger et al., 2013; Krishnan et al.,
2015). Weller & Jebara (2014a) instead use just one MAP
call over a discretized mesh of marginals to approximate
the Bethe partition function, which itself is an estimate
(which often performs well) of the true partition function.

2. Relatives of the Gumbel Trick

In this section, we review the Gumbel trick and state the
mechanism by which it can be generalized into an entire
family of tricks. We show how these tricks can equivalently
be viewed as averaging standard Gumbel perturbations in
different spaces, instantiate several examples, and compare
the various tricks’ properties.

Notation Throughout this paper, let X be a ﬁnite sample
space of size N := |X |. Let ˜p : X → [0, ∞) be an unnor-
malized mass function over X and let Z := (cid:80)
x∈X ˜p(x) be
its normalizing partition function. Write p(x) := ˜p(x)/Z
for the normalized version of ˜p, and φ(x) := ln ˜p(x) for the
log-unnormalized probabilities, i.e. the potential function.

We write Exp(λ) for the exponential distribution with rate
(inverse mean) λ and Gumbel(µ) for the Gumbel distribu-
tion with location µ and scale 1. The latter has mean µ + c,
where c ≈ 0.5772 is the Euler-Mascheroni constant.

2.1. The Gumbel Trick

Similarly to the connection between the Gumbel trick and
the Poisson process established by Maddison (2016), we
introduce the Gumbel trick for discrete probability distri-
butions using a simple and elegant construction via com-
peting exponential clocks. Consider N independent clocks,

Table 1: New tricks for constructing unbiased estimators of different transformations f (Z) of the partition function.

Lost Relatives of the Gumbel Trick

Trick

g(x)

Mean f (Z)

Variance of g(T )

Gumbel
Exponential
Weibull α

Fr´echet α

Pareto

Tail t

− ln x − c
x
xα, α > 0

ln Z
1
Z
Z −αΓ(1 + α)

xα, α ∈ (−1, 0) Z −αΓ(1 + α)
ex
1{x>t}

Z
Z−1 for Z > 1
e−tZ

π2
6
1
Z2
Γ(1+2α)−Γ(1+α)2
Z2α
Γ(1+2α)−Γ(1+α)2
Z2α
Z

(Z−1)2(Z−2) for Z > 2

a
e−tZ (1 − e−tZ )

for α > − 1
2

Asymptotic var. of ˆZ
π2
6 Z 2
Z 2
1
α2
1
α2

Γ(1+α)2 − 1
Γ(1+α)2 − 1

(cid:16) Γ(1+2α)

(cid:16) Γ(1+2α)

(cid:17)

(cid:17)

Z 2

Z 2

Z2
(Z−2)2
(1−e−tZ )2
t2

started simultaneously, such that the j-th clock rings after
a random time Tj ∼ Exp(λj). Then it is easy to show that
(1) the time until some clock rings has Exp((cid:80)N
j=1 λj) dis-
tribution, and (2) the probability of the j-th clock ringing
ﬁrst is proportional to its rate λj. These properties are also
widely used in survival analysis (Cox & Oakes, 1984).

Consider N competing exponential clocks {Tx}x∈X , in-
dexed by elements of X , with respective rates λx = ˜p(x).
Property (1) of competing exponential clocks tells us that

(1)

(2)

(1’)

(2’)

Property (2) says that the random variable argminx Tx, tak-
ing values in X , is distributed according to p:

min
x∈X

{Tx} ∼ Exp(Z).

argmin
x∈X

{Tx} ∼ p.

The Gumbel trick is obtained by applying the function
g(x) = − ln x − c to the equalities in distribution (1)
and (2). When g is applied to an Exp(λ) random vari-
able, the result follows the Gumbel(−c + ln λ) distribu-
tion, which can also be represented as ln λ + γ, where
i.i.d.∼ Gumbel(−c)
γ ∼ Gumbel(−c). Deﬁning {γ(x)}x∈X
and noting that g is strictly decreasing, applying the func-
tion g to equalities in distribution (1) and (2), we obtain:

{φ(x) + γ(x)} ∼ Gumbel(−c + ln Z),

max
x∈X
argmax
x∈X

{φ(x) + γ(x)} ∼ p,

where we have recalled that φ(x) = ln λx = ln ˜p(x). The
distribution Gumbel(−c + ln Z) has mean ln Z, and thus
the log partition function can be estimated by averaging
samples (Hazan & Jaakkola, 2012).

2.2. Constructing New Tricks

Given the equality in distribution (1), we can treat the prob-
lem of estimating the partition function Z as a parameter
estimation problem for the exponential distribution. Ap-
plying the function g(x) = − ln x − c as in the Gumbel
trick to obtain a Gumbel(−c + ln Z) random variable, and

estimating its mean to obtain an unbiased estimator of ln Z,
is just one way of inferring information about Z.

We consider applying different functions g to (1); par-
ticularly those functions g that transform the exponential
distribution to another distribution with known mean. As
the original exponential distribution has rate Z, the trans-
formed distribution will have mean f (Z), where f will in
general no longer be the logarithm function. Since we often
are interested in estimating various transformations f (Z)
of Z, this provides us a with a collection of unbiased es-
timators from which to choose. Moreover, further trans-
forming these estimators yields a collection of (biased) es-
timators for other transformations of Z, including Z itself.
Example 1 (Weibull tricks). For any α > 0, applying
the function g(x) = xα to an Exp(λ) random variable
yields a random variable with the Weibull(λ−α, α−1) dis-
tribution with scale λ−α and shape α−1, which has mean
λ−αΓ(1 + α) and can be also represented as λ−αW ,
i.i.d.∼
where W ∼ Weibull(1, α−1). Deﬁning {W (x)}x∈X
Weibull(1, α−1) and noting that g is increasing, applying
g to the equality in distribution (1) gives

{˜p−αW (x)} ∼ Weibull(Z −α, α−1).

(1”)

min
x∈X

Estimating the mean of Weibull(Z −α, α−1) yields an un-
biased estimator of Z −αΓ(1 + α). The special case α = 1
corresponds to the identity function g(x) = x; we call the
resulting trick the Exponential trick.

Table 1 lists several examples of tricks derived this way.
As Example 1 shows, these tricks may not involve addi-
tive perturbation of the potential function φ(x); the Weibull
tricks multiplicatively perturb exponentiated unnormalized
probabilities ˜p−α with Weibull noise. As models of inter-
est are often speciﬁed in terms of potential functions, to be
able to reuse existing MAP solvers in a black-box manner
with the new tricks, we seek an equivalent formulation in
terms of the potential function. The following Proposition
shows that by not passing the function g through the mini-
mization in equation (1), the new tricks can be equivalently
formulated as averaging additive Gumbel perturbations of
the potential function in different spaces.

Lost Relatives of the Gumbel Trick

Figure 1: Analytically computed MSE and variance of Gumbel
and Exponential trick estimators of Z (left) and ln Z (right). The
MSEs are dominated by the variance, so the dashed and solid lines
mostly overlap. See Section 2.3.2 for details.

Figure 2: MSE of estimators of Z (left) and ln Z (right) stem-
ming from Fr´echet (− 1
2 < α < 0), Gumbel (α = 0) and Weibull
tricks (α > 0). See Section 2.3.2 for details.

Proposition 2. For any function g : [0, ∞) → R such that
f (Z) = ET ∼Exp(Z)[g(T )] exists, we have

f (Z) = Eγ

(cid:18)

(cid:20)
g

e−c exp

(cid:18)

− max
x∈X

{φ(x) + γ(x)}

,

(cid:19)(cid:19)(cid:21)

where {γ(x)}x∈X

i.i.d.∼ Gumbel(−c).

Proof. As maxx{φ(x) + γ(x)} ∼ Gumbel(−c + ln Z),
we have e−c exp(maxx{φ(x) + γ(x)}) ∼ Exp(Z) and the
result follows by the assumption relating f and g.

Proposition 2 shows that the new tricks can be implemented
by solving the same MAP problems maxx{φ(x)+γ(x)} as
in the Gumbel trick, and then merely passing the solutions
through the function x (cid:55)→ g(e−c exp(x)) before averaging
them to approximate the expectation.

2.3. Comparing Tricks

2.3.1. ASYMPTOTIC EFFICIENCY

The Delta method (Casella & Berger, 2002) is a simple
technique for assessing the asymptotic variance of esti-
mators that are obtained by a differentiable transforma-
tion of an estimator with known variance. The last col-
umn in Table 1 lists asymptotic variances of correspond-
ing tricks when unbiased estimators of f (Z) are passed
through the function f −1 to yield (biased, but consistent
and non-negative) estimators of Z itself. It is interesting
to examine the constants that multiply Z 2 in some of the
obtained asymptotic variance expressions for the different
tricks. For example, it can be shown using Gurland’s ra-
tio (Gurland, 1956) that this constant is at least 1 for the
Weibull and Fr´echet tricks, which is precisely the value
achieved by the Exponential trick (which corresponds to
α = 1). Moreover, the Gumbel trick constant π2/6 can be
shown to be the limit as α → 0 of the Weibull and Fr´echet
trick constants. In particular, the constant of the Exponen-
tial trick is strictly better than that of the standard Gumbel
trick: 1 < π2/6 ≈ 1.65. This motivates us to compare the
Gumbel and Exponential tricks in more detail.

2.3.2. MEAN SQUARED ERROR (MSE)

For estimators Y , their MSE(Y ) = var(Y ) + bias(Y )2 is
a commonly used comparison metric. When the Gumbel or
Exponential tricks are used to estimate either Z or ln Z, the
biases, variances, and MSEs of the estimators can be com-
puted analytically using standard methods (Appendix A).

For example, the unbiased estimator of ln Z from the Gum-
bel trick can be turned into a consistent non-negative esti-
mator of Z by exponentiation: Y = exp( 1
m=1 Xm),
M
i.i.d.∼ Gumbel(−c + ln Z) are obtained
where X1, . . . , XM
using equation (1’). The bias and variance of Y can be
computed using independence and the moment generating
functions of the Xm’s, see Appendix A for details.

(cid:80)M

Perhaps surprisingly, all estimator properties only depend
on the true value of Z and not on the structure of the model
(distribution p), since the estimators rely only on i.i.d. sam-
ples of a Gumbel(−c + ln Z) random variable. Figure 1
shows the analytically computed estimator variances and
MSEs. For estimating Z itself (left), the Exponential trick
outperforms the Gumbel trick in terms of MSE for all sam-
ple sizes M ≥ 3 (for M ∈ {1, 2}, both estimators have
inﬁnite variance and MSE). The ratio of MSEs quickly ap-
proaches π2/6, and in this regime the Exponential trick re-
quires 1 − 6/π2 ≈ 39% fewer samples than the Gumbel
trick to reach the same MSE. Also, for estimating ln Z,
(Figure 1, right), the Exponential trick provides a lower
MSE estimator for sample sizes M ≥ 2; only for M = 1
the Gumbel trick provides a better estimator.

Note that as biases are available analytically, the estima-
tors can be easily debiased (by subtracting their bias). One
then obtain estimators with MSEs equal to the variances of
the original estimators, shown dashed in Figure 1. The Ex-
ponential trick would then always outperform the Gumbel
trick when estimating ln Z, even with sample size M = 1.

For Weibull tricks with α (cid:54)= 1 and Fr´echet tricks, we esti-
mated the biases and variances of estimators of Z and ln Z
by constructing K = 100, 000 estimators in each case and
evaluating their bias and variance. Figure 2 shows the re-
sults for varying α and several sample sizes M . We plot the

Lost Relatives of the Gumbel Trick

analytically computed value for the Gumbel trick at α = 0,
as we observe that the Weibull trick interpolates between
the Gumbel trick and the Exponential trick as α increases
from 0 to 1. We note that the minimum MSE estimator is
obtained by choosing a value of α that is close to 1, i.e.
the Exponential trick. This agrees with the ﬁnding from
Section 2.3.1 that α = 1 is optimal as M → ∞.

2.4. Bayesian Perspective

A Bayesian approach exposes two choices when construct-
ing estimators of Z, or of its transformations f (Z):

1. A choice of prior distribution p0(Z), encoding prior
beliefs about the value of Z before any observations.
2. A choice of how to summarize the posterior distribu-

tion pM (Z|X1, . . . , XM ) given M samples.

Taking the Jeffrey’s prior p0(Z) ∝ Z −1, an improper prior
that it is invariant under reparametrization, observing M
samples X1, . . . , XM

i.i.d.∼ Exp(Z) yields the posterior:

pM (Z|X1, . . . , XM ) ∝ ZM −1e−Z (cid:80)M
Recognizing the density of a Gamma(M, (cid:80)M
dom variable, the posterior mean is

m=1 Xm.

m=1 Xm) ran-

E[Z|X1, . . . , XM ] =

M
m=1 Xm

(cid:80)M

=

(cid:32)

1
M

M
(cid:88)

m=1

(cid:33)−1

Xm

,

coinciding with the Exponential trick estimator of Z.

3. Low-rank Perturbations

One way of exploiting perturb-and-MAP to yield com-
putational savings is to replace independent perturbations
of each conﬁguration’s potential with an approximation.
Such approximations are available e.g. in discrete graphical
models, where the sampling space X has a product space
structure X = X1 × · · · × Xn, with Xi the state space of
the i-th variable.
Deﬁnition 3 ( (Hazan & Jaakkola, 2012)). The sum-unary
perturbation MAP value is the random variable

U := max
x∈X

(cid:110)

φ(x) +

γi(xi)

(cid:111)
,

n
(cid:88)

i=1

Unary perturbations provide the upper bound ln Z ≤ E[U ]
on the log partition function (Hazan & Jaakkola, 2012), can
be used to construct a sequential sampler for the Gibbs dis-
tribution (Hazan et al., 2013), and, if the perturbations are
scaled down by a factor of n, a lower bound on ln Z can
also be recovered (Hazan et al., 2013). In this section we
show that a subfamily of tricks introduced in Section 2,
consisting of Fr´echet and Weibull (and Exponential) tricks,
is applicable in the low-rank perturbation setting and use
them to derive new families of upper and lower bounds
on ln Z and sequential samplers for the Gibbs distribution.
Please note full proofs are deferred to Appendix B and C.

3.1. Upper Bounds on the Partition Function

The following family of upper bounds on ln Z can be de-
rived from the Fr´echet and Weibull tricks.

Proposition 4. For any α ∈ (−1, 0) ∪ (0, ∞), the upper
bound ln Z ≤ U(α) holds with

U(α) := n

ln Γ(1 + α)
α

+ nc −

ln Eγ

(cid:2)e−αU (cid:3) .

1
α

Proof. (Sketch.) By induction on n, with the induction step
provided by our Clamping Lemma (Lemma 7) below.

To evaluate these bounds in practice, E[e−αU ] is estimated
using samples of U . Corollary 9 of Hazan et al. (2016) can
be used to show that var(e−αU ) is ﬁnite for α > − 1
n ,
√
2
and so then the estimation is well-behaved.

A natural question is how these new bounds relate to the
Gumbel trick upper bound ln Z ≤ E[U ] by Hazan &
Jaakkola (2012). The following result aims to answers this:

Proposition 5. The limit of U(α) as α → 0 exists and
equals U(0) := E[U ], i.e. the Gumbel trick upper bound.

The question remains: When is it advantageous to use a
value α (cid:54)= 0 to obtain a tighter bound on ln Z than the
Gumbel trick bound? The next result can provide guidance:

Proposition 6. The function U(α) is differentiable at α =
0 and the derivative equals

d
dα

(cid:12)
(cid:12)
U(α)
(cid:12)
(cid:12)α=0

=

(cid:18)

n

π2
6

1
2

(cid:19)

− var(U )

.

where {γi(xi) | xi ∈ Xi, 1 ≤ i ≤ n} i.i.d∼ Gumbel(−c).

This deﬁnition involves |X1|+· · ·+|Xn| i.i.d. Gumbel ran-
dom variables, rather than |X |. (With n = 1 this coincides
with full-rank perturbations and U ∼ Gumbel(−c+ln Z).)
For n > 2 the distribution of U is not available analytically.
One can similarly deﬁne the pairwise (or higher-order) per-
turbations, where independent Gumbel noise is added to
each pairwise (or higher-order) potential.

While the variance of U is generally not tractable, in prac-
tice one obtains samples from U to estimate the expectation
in U(α) and these samples can be reused to assess var(U ).
Interestingly, var(U ) equals nπ2/6 for both the uniform
distribution and the distribution concentrated on a single
conﬁguration, and in our empirical investigations always
var(U ) ≤ nπ2/6. Then the derivative at 0 is non-negative
and Fr´echet tricks provide tighter bounds on ln Z. How-
ever, as U(α) is estimated with samples, the question of

Lost Relatives of the Gumbel Trick

estimator variance arises. We investigate the trade-off be-
tween tightness of the bound ln Z ≤ U(α) and the variance
incurred in estimating U(α) empirically in Section 5.3.

can derive sequential sampling procedures from the Fr´echet
and Weibull tricks, leading to the following algorithm.

3.2. Clamping

Consider the partial sum-unary perturbation MAP values,
where the values of the ﬁrst j − 1 variables have been ﬁxed,
and only the rest are perturbed:

Uj(x1, . . . , xj−1) := max
xj ,...,xn

φ(x) +

γi(xi)

.






n
(cid:88)

i=j






The following lemma involving the Uj’s serves three pur-
poses: (I.) it provides the induction step for Proposition 4,
(II.) it shows that clamping never hurts partition function
estimation with Fr´echet and Weibull tricks, and (III.) it will
be used to show that a sequential sampler constructed in
Section 3.3 below is well-deﬁned.

Lemma 7 (Clamping Lemma). For any j ∈ {1, . . . , n}
and (x1, . . . , xj−1) ∈ X1 × · · · × Xj−1, the following in-
equality holds with any α ∈ (−1, 0) ∪ (0, ∞):

(cid:88)

(cid:104)

Eγ

e−(n−j) ln Γ(1+α)−α(n−j)c)e−αUj+1

(cid:105)−1/α

xj ∈Xj
(cid:104)
e−(n−(j−1)) ln Γ(1+α)−α(n−(j−1))c)e−αUj

≤ Eγ

(cid:105)−1/α

Proof. This follows directly from the Fr´echet trick (α ∈
(−1, 0)) or the Weibull trick (α > 0) and representing the
Fr´echet resp. Weibull random variables in terms of Gumbel
random variables. See Appendix B.1 for more details.

Algorithm 1 Sequential sampler for Gibbs distribution
Input: α ∈ (−1, 0) ∪ (0, ∞), potential function φ on X
Output: a sample x from the Gibbs distribution ∝ eφ(x)
1: for j = 1 to n do
for xj ∈ Xj do
2:

Eγ[e−αUj+1(x1 ,...,xj )]−1/α
Eγ[e−αUj (x1,...,xj−1)]−1/α

pj(xj)

3:

4:
5:
6:
7:

Γ(1+α)1/α

pj(xj) ← e−c
pj(reject) ← 1 − (cid:80)
xj ← sample according to pj
if xj == reject then
RESTART (goto 1)

xj ∈Xj

This algorithm is well-deﬁned if pj(reject) ≥ 0 for all j,
which can be shown by canceling terms in the Clamping
Lemma 7. We discuss correctness in Appendix B.2. As for
the Gumbel sequential sampler of Hazan et al. (2013), the
expected number of restarts (and hence the running time)
only depend on the quality of the upper bound (U(α) −
ln Z), and not on the ordering of variables.

3.4. Lower Bounds on the Partition Function

Similarly as in the Gumbel trick case (Hazan et al., 2013),
one can derive lower bounds on ln Z by perturbing an arbi-
trary subset S of variables.
Proposition 9. Let X = X1 × · · · Xn be a product space
and φ a potential function on X . Let α ∈ (−1, 0) ∪ (0, ∞).
For any subset S ⊆ {1, . . . , n} of the variables x1, . . . , xn
we have ln Z ≥

Corollary 8. Clamping never hurts ln Z estimation using
any of the Fr´echet or Weibull upper bounds U(α).

c +

ln Γ(1 + α)
α

−

1
α

ln E

(cid:104)

e−α maxx{φ(x)+γS (xS )}(cid:105)

,

Proof. Applying the function x (cid:55)→ ln(x) to both sides of
the Clamping Lemma 7 with j = 1, the right-hand side
equals U(α), while the left-hand side is the estimate of ln Z
after clamping variable x1.

This was shown previously in restricted settings (Hazan
et al., 2013; Zhao et al., 2016). Similar results showing
that clamping improves partition function estimation have
been obtained for the mean ﬁeld and TRW approxima-
tions (Weller & Domke, 2016), and in certain settings for
the Bethe approximation (Weller & Jebara, 2014b) and L-
FIELD (Zhao et al., 2016).

3.3. Sequential Sampling

where xS := {xi : i ∈ S} and γS(xS) ∼ Gumbel(−c)
independently for each setting of xS.

By averaging n such lower bounds corresponding to single-
ton sets S = {i} together, we obtain a lower bound on ln Z
that involves the average-unary perturbation MAP value

(cid:40)

L := max
x∈X

φ(x) +

(cid:41)

γi(xi)

.

1
n

n
(cid:88)

i=1

Corollary 10. For any α ∈ (−1, 0) ∪ (0, ∞), we have the
lower bound ln Z ≥ L(α), where

L(α) := c +

ln Γ(1 + α)
α

−

1
nα

ln E [exp (−nαL)] .

Hazan et al. (2013) derived a sequential sampling proce-
dure for the Gibbs distribution by exploiting the U(0) Gum-
bel trick upper bound on ln Z.
In the same spirit, one

Again, L(0) := E[L] can be deﬁned by continuity, where
E[L] ≤ ln Z is the Gumbel trick lower bound by Hazan
et al. (2013).

4. Advantages of the Gumbel Trick

5. Experiments

Lost Relatives of the Gumbel Trick

We have seen how the Gumbel trick can be embedded into
a continuous family of tricks, consisting of Fr´echet, Expo-
nential, and Weibull tricks. We showed that the new tricks
can provide more efﬁcient estimators of the partition func-
tion in the full-rank perturbation setting (Section 2), and
in the low-rank perturbation setting lead to sequential sam-
plers and new bounds on ln Z, which can be also more ef-
ﬁcient, as we investigate in Section 5.3. To balance the
discussion of merits of different tricks, in this section we
brieﬂy highlight advantages of the Gumbel trick that stem
from its simpler analytical form.

First, by consulting Table 1 we see that the function g(x) =
− ln x − c has the property that the variance of the resulting
estimator (of ln Z) does not depend on the value of Z; the
function g is a variance stabilizing transformation for the
Exponential distribution.

Second, exploiting the fact that the logarithm function leads
to additive perturbations, Maji et al. (2014) showed that the
entropy of x∗, the conﬁguration with maximum potential
after sum-unary perturbation in the sense of Deﬁnition 3,
can be bounded as H(x∗) ≤ B(p) := (cid:80)n
i )].
We extend this result to show how the errors of bounding
ln Z, sampling, and entropy estimation are related:
Proposition 11. Writing p for the Gibbs distribution and
B(p) := Eγi [γi(x∗

Eγi [γi(x∗

i=1

i )] for the entropy bound, we have
= B(p) − H(x∗)
+ KL(x∗ (cid:107) p)
(cid:125)
(cid:124)
(cid:125)
(cid:123)(cid:122)
sampling error

(cid:123)(cid:122)
error in entropy estimation

(cid:124)

.

(U(0) − ln Z)
(cid:125)
(cid:123)(cid:122)
(cid:124)
error in ln Z bound

Third, the additive character of the Gumbel perturbations
can also be used to derive a new result relating the error of
the lower bound L(0) and of sampling x∗∗ as the conﬁgu-
ration achieving the maximum average-unary perturbation
value L, instead of sampling from the Gibbs distribution p:
Proposition 12. Writing p for the Gibbs distribution,

ln Z − L(0)
(cid:124)
(cid:123)(cid:122)
(cid:125)
error in ln Z bound

≥ KL(x∗∗ (cid:107) p)
(cid:123)(cid:122)
(cid:125)
sampling error

(cid:124)

≥ 0.

Remark. While we knew from Hazan et al. (2013) that
ln Z − L(0) ≥ 0, this is a stronger result showing that
the size of the gap is an upper bound on the KL divergence
between the approximate sampling distribution of x∗∗ and
the Gibbs distribution p.

Proofs of the new results appear in Appendix B.3 and C.2.

Fourth, viewed as a function of the Gumbel perturbations
γ, the random variable U has a bounded gradient, allowing
earlier measure concentration results (Orabona et al., 2014;
Hazan et al., 2016). Proving similar measure concentration
results for the expectations E[e−αU ] appearing in U(α) for
α (cid:54)= 0 may be more challenging.

We conducted experiments with the following aims:

1. To show that the higher efﬁciency of the Exponential
trick in the full-rank perturbation setting is useful in
practice, we compared it to the Gumbel trick in A*
sampling (Maddison et al., 2014) (Section 5.1) and in
the large-scale discrete sampling setting of Chen &
Ghahramani (2016) (Section 5.2).

2. To show that non-zero values of α can lead to bet-
ter estimators of ln Z in the low-rank perturbation set-
ting as well, we compare the Fr´echet and Weibull trick
bounds U(α) to the Gumbel trick bound U(0) on a
common discrete graphical model with different cou-
pling strengths; see Section 5.3.

5.1. A* Sampling

A* sampling (Maddison et al., 2014) is a sampling algo-
rithm for continuous distributions that perturbs the log-
unnormalized density φ with a continuous generalization
of the Gumbel trick, called the Gumbel process, and uses
a variant of A* search to ﬁnd the location of the maxi-
mum of the perturbed φ. Returning the location yields an
exact sample from the original distribution, as in the dis-
crete Gumbel trick. Moreover, the corresponding maxi-
mum value also has the Gumbel(−c + ln Z) distribution
(Maddison et al., 2014). Our analysis in Section 2.3 tells
us that the Exponential trick yields an estimator with lower
MSE than the Gumbel trick; we brieﬂy veriﬁed this on
the Robust Bayesian Regression experiment of Maddison
et al. (2014). We constructed estimators of ln Z from the
Gumbel and Exponential tricks (debiased version, see Sec-
tion 2.3.2), and assessed their variances by constructing
each estimator K = 1000 times and looking at the sam-
ple variance. Figure 3a shows that the Exponential trick
requires up to 40% fewer samples to reach a given MSE.

5.2. Scalable Partition Function Estimation

Chen & Ghahramani (2016) considered sampling from a
discrete distribution of the form p(x) ∝ f0(x) (cid:81)S
s=1 fs(x)
when the number of factors S is large relative to the sam-
ple space size |X |. Computing i.i.d. Gumbel perturbations
γ(x) for each x ∈ X is then relatively cheap compared to
evaluating all potentials φ(x) = f0(x) + (cid:80)S
s=1 ln fs(x).
Chen & Ghahramani (2016) observed that each (perturbed)
potential can be estimated by subsampling the factors, and
potentials that appear unlikely to yield the MAP value can
be pruned off from the search early on. The authors for-
malized the problem as a Multi-armed bandit problem with
a ﬁnite reward population and derived approximate algo-
rithms for efﬁciently ﬁnding the maximum perturbed po-
tential with a probabilistic guarantee.

Lost Relatives of the Gumbel Trick

(a)

(b)

Figure 3: (a) Sample size M required to reach a given MSE using
Gumbel and Exponential trick estimators of ln Z, using samples
from A∗ sampling (see Section 5.1) on a Robust Bayesian Re-
gression task. The Exponential trick is more efﬁcient, requiring
up to 40% fewer samples to reach a given MSE. (b) MSE of ln Z
estimators for different values of α, using M = 100 samples
from the approximate MAP algorithm discussed in Section 5.2,
with different error bounds δ. For small δ, the Exponential trick
is close to optimal, matching the analysis of Section 2.3.2. For
larger δ, the Weibull trick interpolation between the Gumbel and
Exponential tricks can provide an estimator with lower MSE.

While Chen & Ghahramani (2016) considered sampling,
by modifying their procedure to return the value of the
maximum perturbed potential rather than the argmax (cf
equations (1) and (2)), we can estimate the partition func-
tion instead. However, the approximate algorithm only
guarantees to ﬁnd the MAP conﬁguration with a proba-
bility 1 − δ. Figure 3b shows the results of running the
Racing-Normal algorithm of Chen & Ghahramani (2016)
on the synthetic dataset considered by the authors with the
“very hard” noise setting σ = 0.1. For low error bounds δ
the Exponential trick remained close to optimal, but for a
larger error bound the Weibull trick interpolation between
the Gumbel and Exponential tricks proved useful to provide
an estimator with lower MSE.

5.3. Low-rank Perturbation Bounds on ln Z

Hazan & Jaakkola (2012) evaluated tightness of the Gum-
bel trick upper bound U(0) ≥ ln Z on 10 × 10 binary spin
glass models. We show one can obtain more accurate es-
timates of ln Z on such models by choosing α (cid:54)= 0. To
account for the fact that in practice an expectation in U(α)
is replaced with a sample average, we treat U(α) as an esti-
mator of ln Z with asymptotic bias equal to the bound gap
(U(α) − ln Z), and estimate its MSE.

Figure 4 shows the MSEs of U(α) as estimators of ln Z on
10 × 10 (n = 100) binary pairwise grid models with unary
potentials sampled uniformly from [−1, 1] and pairwise po-
tentials from [0, C] (attractive models) or from [−C, C]
(mixed models), for varying coupling strengths C. We re-
placed the expectations in U (α)’s with sample averages of
size M = 100, using libDAI (Mooij, 2010) to solve the
MAP problems yielding these samples. We constructed
each estimator 1000 times to assess its variance.

Figure 4: MSEs of U(α) as estimators of ln Z on 10 × 10 at-
tractive (left, middle) and mixed (right) spin glass model with dif-
ferent coupling strengths C (see Section 5.3). We also show the
percentage of samples saved by using the best α in place of the
Gumbel trick estimator U(0), assuming the asymptotic regime.
For this we only considered α > −1/(2
n) = −0.05, where
variance is provably ﬁnite, see Section 3.1. The MAP problems
were solved using the exact junction tree algorithm (JCT, left and
right), or approximate belief propagation (BP, middle).
In all
cases, when coupling is very low, α close to 0 is optimal. This
also holds for BP when coupling is high. In other regimes, upper
bounds for the Fr´echet trick, i.e. α < 0, provide more accurate
estimators.

√

6. Discussion

By casting partition function evaluation as a parameter esti-
mation problem for the exponential distribution, we derived
a family of methods of which the Gumbel trick is a special
case. These methods can be equivalently seen as (1) per-
turbing models using different distributions, or as (2) av-
eraging standard Gumbel perturbations in different spaces,
allowing implementations with little additional cost.

We showed that in the full-rank perturbation setting, the
new Exponential trick provides an estimator with lower
MSE, or instead allows using up to 40% fewer samples than
the Gumbel trick estimator to reach the same MSE.

In the low-rank perturbation setting, we used our Fr´echet,
Exponential and Weibull tricks to derive new bounds on
ln Z and sequential samplers for the Gibbs distribution, and
showed that these can also behave better than the corre-
sponding Gumbel trick results. However, the optimal trick
to use (as speciﬁed by α) depends on the model, sample
size, and MAP solver used (if approximate). Since in prac-
tice the dominant computational cost is carried by solving
repeated instances of the MAP problem, one can try and as-
sess different values of α on the problem at hand. That said,
we believe that investigating when different tricks yield
better results is an interesting avenue for future work.

Finally, we balanced the discussion by pointing out that the
Gumbel trick has a simpler analytical form which can be
exploited to derive more interesting theoretical statements
in the low-rank perturbation setting. Beyond existing re-
sults, we derived new connections between errors of differ-
ent procedures using low-rank Gumbel perturbations.

Lost Relatives of the Gumbel Trick

Acknowledgements

The authors thank Tamir Hazan for helpful discussions,
and Mark Rowland, Maria Lomeli, and the anonymous
reviewers for helpful comments. AW acknowledges sup-
port by the Alan Turing Institute under EPSRC grant
EP/N510129/1, and by the Leverhulme Trust via the CFI.

References

Belanger, D., Sheldon, D., and McCallum, A. Marginal inference
In NIPS Workshop on Greedy

in MRFs using Frank-Wolfe.
Optimization, Frank-Wolfe and Friends, 2013.

Bertasius, G., Liu, Q., Torresani, L., and Shi, J. Local Perturb-

and-MAP for Structured Prediction. In AISTATS, 2017.

Boykov, Y., Veksler, O., and Zabih, R. Fast approximate energy
IEEE Transactions on pattern

minimization via graph cuts.
analysis and machine intelligence, 23(11):1222–1239, 2001.

Casella, G. and Berger, R.

Statistical inference, volume 2.

Duxbury Paciﬁc Grove, CA, 2002.

Maddison, C. A Poisson process model for Monte Carlo.

In
Hazan, T., Papandreou, G., and Tarlow, D. (eds.), Perturbation,
Optimization, and Statistics. MIT Press, 2016.

Maddison, C., Tarlow, D., and Minka, T. A∗ sampling. In NIPS.

2014.

Maji, S., Hazan, T., and Jaakkola, T. Active boundary annotation

using random MAP perturbations. In AISTATS, 2014.

Mooij, J.

libDAI: A free and open source C++ library for dis-
crete approximate inference in graphical models. Journal of
Machine Learning Research, 11, 2010.

Orabona, F., Hazan, T., Sarwate, A., and Jaakkola, T. On measure
concentration of random maximum a-posteriori perturbations.
In ICML, 2014.

Papandreou, G. and Yuille, A. Gaussian sampling by local pertur-

bations. In NIPS. 2010.

Papandreou, G. and Yuille, A. Perturb-and-MAP random ﬁelds:
Using discrete optimization to learn and sample from energy
models. In Proc. IEEE Int. Conf. on Computer Vision (ICCV),
pp. 193–200, November 2011.

Chen, Y. and Ghahramani, Z. Scalable discrete sampling as a

multi-armed bandit problem. In ICML, 2016.

Tarlow, D., Adams, R., and Zemel, R. Randomized optimum

models for structured prediction. In AISTATS, 2012.

dinavian Actuarial Journal, 1956(2):171–172, 1956.

Wright, S. and Nocedal, J. Numerical optimization. Springer

Wainwright, M. and Jordan, M. Graphical Models, Exponen-
tial Families, and Variational Inference. Found. Trends Mach.
Learn., 1(1-2):1–305, January 2008.

Weller, A. and Domke, J. Clamping improves TRW and mean

ﬁeld approximations. In AISTATS, 2016.

Weller, A. and Jebara, T. Approximating the Bethe partition func-

tion. In UAI, 2014a.

Weller, A. and Jebara, T. Clamping variables and approximate

inference. In NIPS, 2014b.

Science, 35:67–68, 1999.

Zhao, J., Djolonga, J., Tschiatschek, S., and Krause, A. Variable
clamping for optimization-based inference. In NIPS Workshop
on Advances in Approximate Bayesian Inference, December
2016.

Cox, D. and Oakes, D. Analysis of survival data, volume 21. CRC

Press, 1984.

Darbon, J. Global optimization for ﬁrst order Markov random
ﬁelds with submodular priors. Discrete Applied Mathematics,
157(16):3412 – 3423, 2009.

Ermon, S., Sabharwal, A., and Selman, B. Taming the curse of
dimensionality: Discrete integration by hashing and optimiza-
tion. In ICML, 2013.

Gurland, J. An inequality satisﬁed by the Gamma function. Scan-

Hazan, T. and Jaakkola, T. On the partition function and random

maximum a-posteriori perturbations. In ICML, 2012.

Hazan, T., Maji, S., and Jaakkola, T. On sampling from the Gibbs
distribution with random maximum a-posteriori perturbations.
In NIPS. 2013.

Hazan, T., Orabona, F., Sarwate, A., Maji, S., and Jaakkola,
T. High dimensional inference with random maximum a-
posteriori perturbations. CoRR, abs/1602.03571, 2016.

Kim, C., Sabharwal, A., and Ermon, S. Exact sampling with in-
teger linear programs and random perturbations. In AAAI, pp.
3248–3254, 2016.

Kolmogorov, V. Convergent tree-reweighted message passing for
IEEE transactions on pattern analysis

energy minimization.
and machine intelligence, 28(10):1568–1583, 2006.

Krishnan, Rahul G, Lacoste-Julien, Simon, and Sontag, David.
Barrier Frank-Wolfe for Marginal Inference. In NIPS. 2015.

Lauritzen, S. Graphical models. Oxford statistical science series.

Clarendon Press, Oxford, 1996. Autre tirage : 1998.

APPENDIX: Lost Relatives of the Gumbel Trick

Lost Relatives of the Gumbel Trick

Here we provide proofs for the results stated in the main text, together with additional supporting lemmas required for
these proofs.

A. Comparison of Gumbel and Exponential tricks

In Section 2.3.1 we analyzed the asymptotic efﬁciency of different estimators of Z by measuring their asymptotic variance.
(As all our estimators in the full-rank perturbation setting are consistent, their bias is 0 in the limit of inﬁnite data, and so
this asymptotic variance equals the asymptotic MSE.) In the non-asymptotic regime, where an estimator ˆZ is constructed
from a ﬁnite set of M samples, we can analyze both the variance var( ˆZ) and the bias (E[ ˆZ] − Z) of the estimator. While
in most cases these cannot be obtained analytically and there we can resort to an empirical evaluation, for the estimators
stemming from the Gumbel and Exponential tricks analytical treatment turns out to be possible using standard methods.

A.1. Estimating Z

Gumbel trick The Gumbel trick yields an unbiased estimator for ln Z, and we can turn it into a consistent estimator of
Z by exponentiating it:

ˆZ := exp

(cid:32)

1
M

M
(cid:88)

m=1

(cid:33)

Xm

where

X1, . . . , XM

iid∼ Gumbel(−c + ln Z).

Recalling that the moment generating function of a Gumbel(µ) distribution is G(t) = Γ(1 − t)eµt, we can obtain by using
independence of the samples:

E[ ˆZ] =

E[eXm/M ] =

(cid:16)

Γ(1 − 1/M )e(ln Z−c)/M (cid:17)M

= Γ(1 − 1/M )M e−cZ,

E[ ˆZ 2] =

E[e2Xm/M ] =

(cid:16)

Γ(1 − 2/M )e2(ln Z−c)/M (cid:17)M

= Γ(1 − 2/M )M e−2cZ 2.

M
(cid:89)

m=1

M
(cid:89)

m=1

Therefore the squared bias, variance and MSE of the estimator ˆZ are, respectively:

bias( ˆZ)2 = (E[ ˆZ] − Z)2 = Z 2 (cid:0)Γ(1 − 1/M )M e−c − 1(cid:1) ,

var( ˆZ) = E[ ˆZ 2] − E[ ˆZ]2 = Z 2 (cid:0)Γ(1 − 2/M )M e−2c − Γ(1 − 1/M )2M e−2c(cid:1) ,

MSE( ˆZ) = bias( ˆZ)2 + var( ˆZ) = Z 2 (cid:0)Γ(1 − 2/M )M e−2c − 2Γ(1 − 1/M )M e−c + 1(cid:1) .

These formulas hold for M > 2 where the moment generating functions are deﬁned. For M = 1 the estimator has inﬁnite
bias (and inﬁnite variance), and for M = 2 it has inﬁnite variance. Figure 1 (left) shows the functional dependence of
MSE( ˆZ) on the number of samples M ≥ 3, in units of Z 2.

Exponential trick The Exponential trick yields an unbiased estimator of 1/Z, and we can turn it into a consistent
estimator of Z by inverting it:

ˆZ :=

(cid:33)−1

(cid:32)

1
M

M
(cid:88)

m=1

Xm

where

X1, . . . , XM

iid∼ Exp(Z).

As X1, . . . , XM are independent and exponentially distributed with identical rates Z, their sum follows the Gamma distri-
bution with shape M and rate Z. Therefore the estimator ˆZ can be written as ˆZ = M Y , where Y ∼ InvGamma(M, Z).

Recalling the mean and variance of the Inverse-Gamma distribution, we obtain:

Lost Relatives of the Gumbel Trick

bias( ˆZ)2 = (E[ ˆZ] − Z)2 = Z 2

(cid:18) M

M − 1

(cid:19)

− 1

= Z 2

1
M − 1

,

var( ˆZ) = Z 2M 2

1
(M − 1)2(M − 2)
MSE( ˆZ) = bias( ˆZ)2 + var( ˆZ) = Z 2 M − 2 + M 2

,

(M − 1)2(M − 2)

= Z 2

M + 2
(M − 1)(M − 2)

.

Again these formulas hold for M > 2 where the relevant expectations are deﬁned: for M = 1 the estimator has inﬁnite
bias, and for M ∈ {1, 2} it has inﬁnite variance. Figure 1 (left) shows the functional dependence of MSE( ˆZ) on the
number of samples M ≥ 3, in units of Z 2. By inspecting the curves we observe that the Gumbel trick estimator requires
roughly 45% more samples to yield the same MSE as the Exponential trick estimator.

A.2. Estimating ln Z

A similar analysis can be performed for estimating ln Z rather than Z. In that case the Gumbel trick estimator of ln Z is
unbiased and has variance (and thus MSE) equal to 1
M

π2
6 . On the other hand, the Exponential trick estimator is

(cid:100)ln Z = − ln

(cid:32)

1
M

M
(cid:88)

m=1

(cid:33)

Xm

where

X1, . . . , XM

iid∼ Exp(Z).

Again (cid:80)M

m=1 Xm ∼ Gamma(M, Z) and by reference to properties of the Gamma distribution,

bias( (cid:100)ln Z)2 = (E[ ˆZ] − Z)2 = (ln M − (ψ(M ) − ln Z) − ln Z)2 = (ln M − ψ(M ))2 ,

var( (cid:100)ln Z) = ψ1(M ),

MSE( (cid:100)ln Z) = bias( (cid:100)ln Z)2 + var( (cid:100)ln Z) = (ln M − ψ(M ))2 + ψ1(M ),

where ψ(·) is the digamma function and ψ1(·) is the trigamma function. Note that the estimator can be debiased by
subtracting its bias (ln M − ψ(M )). Figure 1 (right) compares the MSE of the Gumbel and Exponential trick estimators
of ln Z. We observe that the Gumbel trick estimator performs better only for M = 1, and even in that case the Exponential
trick estimator is better when debiased.

B. Sum-unary perturbations

Recall that sum-unary perturbations refer to the setting where each variable’s unary potentials are perturbed with Gumbel
noise, and the perturbed potential of a conﬁguration sums the perturbations from all variables (see Deﬁnition 3 in the
main text). Using sum-unary perturbations we can derive a family U(α) of upper bounds on the log partition function
(Proposition 4) and construct sequential samplers for the Gibbs distribution (Algorithm 1). Here we provide proofs for the
related results stated in Sections 3.1 and 3.2.

Notation We will write powβ x for xβ, where x, β ∈ R, when we ﬁnd this increases clarity of our exposition.
Lemma 13 (Weibull and Fr´echet tricks). For any ﬁnite set Y and any function h, we have

pow
−α

pow
−α

(cid:88)

y∈Y

(cid:88)

y∈Y

pow
−1/α

pow
−1/α

h(y) = EW

h(y) = EF

(cid:20)

(cid:26)

min
y

h(y)

W (y)
Γ(1 + α)

(cid:27)(cid:21)

(cid:20)

(cid:26)

max
y

h(y)

F (y)
Γ(1 + α)

(cid:27)(cid:21)

where {W (y)}y∈Y

i.i.d.∼ Weibull(1, α−1)

for α ∈ (0, ∞),

where {F (y)}y∈Y

i.i.d.∼ Fr´echet(1, −α−1)

for α ∈ (−1, 0).

Proof. This follows from setting up competing exponential clocks with rates λy = h(y)−1/α and then applying the func-
tion g(x) = xα as in Example 1 for the case of the Weibull trick. The case of the Fr´echet trick is similar, except that g is
strictly decreasing for α ∈ (−1, 0), hence the maximization in place of the minimization.

Lost Relatives of the Gumbel Trick

B.1. Upper bounds on the partition function

Proposition 4. For any α ∈ (−1, 0) ∪ (0, ∞), the upper bound ln Z ≤ U(α) holds with

U(α) := n

ln Γ(1 + α)
α

+ nc −

ln Eγ

(cid:2)e−αU (cid:3) .

1
α

Proof. We show the result for α ∈ (0, ∞) using the Weibull trick; the case of α ∈ (−1, 0) can be proved similarly using
the Fr´echet trick. The idea is to prove by induction on n that Z −α ≥ e−αU (α), so that the claimed result follows by
applying the monotonically decreasing function x (cid:55)→ − ln(x)/α.

The base case n = 1 is the Clamping Lemma 7 below with j = n = 1. Now assume the claim for n − 1 ≥ 1 and for
xn ∈ Xn deﬁne

Un−1(α, x1) := (n − 1)

+ (n − 1)c −

exp

−α max

φ(x) +

γi(xi)

.

ln Γ(1 + α)
α

1
α

ln Eγ

x2,...,xn

(cid:34)

(cid:32)

(cid:40)

(cid:41)(cid:33)(cid:35)

n
(cid:88)

i=2

With this deﬁnition, the Clamping Lemma with j = 1 states that (cid:80)

pow−1/α e−αUn−1(α,x1) ≤ pow−1/α e−αU (α), so:

x1

Z −α ≥ pow
−α

(cid:88)

x1∈X1

pow
−1/α

e−αUn−1(α,x1)

e−αU (α)

≥ pow
−α

pow
−1/α
= e−αU (α),

[inductive hypothesis]

[Clamping Lemma]

as required to complete the inductive step.

Proposition 5. The limit of U(α) as α → 0 exists and equals U(0) := E[U ], i.e. the Gumbel trick upper bound.

α ln E (cid:2)e−αU (cid:3). The ﬁrst term tends to nψ(1) = −cn as α → 0 by
Proof. Recall that U(α) = n ln Γ(1+α)
L’Hˆopital’s rule, where ψ is the digamma function. The second term is constant in α. In the last term, E (cid:2)e−αU (cid:3) is the
moment generating function of U evaluated at −α, and as such its derivative at α = 0 exists and equals the negative of the
mean of U . Hence by L’Hˆopital’s rule,

+ nc − 1

α

− lim
α→0

1
α

ln E (cid:2)e−αU (cid:3) = − lim

−E[U ]
E [e−αU ]

α→0

= E[U ] = U(0).

The claimed result then follows by the Algebra of Limits, as the contributions of the ﬁrst two terms cancel.

Proposition 6. The function U(α) is differentiable at α = 0 and the derivative equals

d
dα

(cid:12)
(cid:12)
U(α)
(cid:12)
(cid:12)α=0

= n

−

π2
12

var(U )
2

.

Proof. First we show that U(α) is differentiable on (−1, 0) ∪ (0, ∞), and that the limit of the derivative as α → 0 exists
and equals nπ2/12 − var(U )/2.

The ﬁrst term of U(α) is n ln Γ(1+α)
derivative equals

α

, which is differentiable for α ∈ (−1, 0) ∪ (0, ∞) by the Quotient Rule, and its

d
dα

n

ln Γ(1 + α)
α

= n

ψ(1 + α)α − ln Γ(1 + α)
α2

,

where ψ is the digamma function (logarithmic derivative of the gamma function). Applying L’Hˆopital’s rule we note that

lim
α→0

d
dα

n

ln Γ(1 + α)
α

= n lim
α→0

ψ(1 + α) + αψ(1)(1 + α) − ψ(1 + α)
2α

= n

ψ(1)(1)
2

= n

= n

ζ(2)
2

π2
12

,

Lost Relatives of the Gumbel Trick

where ψ(1) is the trigamma function (derivative of the digamma function), whose value at 1 is known to be ζ(2) = π2/6,
the Riemann zeta function evaluated at 2.

The second term of U(α) is constant in α. The last term can be written as K(−α)/(−α), where K is the cumulant
generating function (logarithm of the moment generating function) of the random variable U . The cumulant generating
function is differentiable, and by the Quotient rule

d
dα

K(−α)
−α

= −

αK (cid:48)(−α) − K(−α)
α2

.

Applying L’Hˆopital’s rule we note that

lim
α→0

d
dα

K(−α)
−α

= lim
α→0

K (cid:48)(−α) + αK (cid:48)(cid:48)(−α) − K (cid:48)(−α)
2α

=

K (cid:48)(cid:48)(0)
2

=

var(U )
2

,

where we have used that the second derivative of the cumulant generating function is the variance.

As U(α) is continuous at 0 by construction, the above implies that it has left and right derivatives at 0. As the values of
these derivatives coincide, the function is differentiable at 0 and the derivative has the stated value.

Recall that for a variable index j ∈ {1, . . . , n} we also deﬁned partial sum-unary perturbations

Uj(x1, . . . , xj−1) := max
xj ,...,xn

φ(x) +

γi(xi)

,






n
(cid:88)

i=j






which ﬁx the variables x1, . . . , xj−1 and perturb the remaining ones.
Lemma 7 (Clamping Lemma). For any j ∈ {1, . . . , n} and any ﬁxed partial variable assignment (x1, . . . , xj−1) ∈
X1 × · · · × Xj−1, the following inequality holds with any trick parameter α ∈ (−1, 0) ∪ (0, ∞):

Proof. For α > 0, from the Weibull trick (Lemma 13), using independence of the perturbations and Jensen’s inequality,

xj ∈Xj

≤ Eγ

pow
−α

(cid:88)

xj ∈Xj


(cid:88)

Eγ

(cid:104)

e−(n−j) ln Γ(1+α)−α(n−j)c)e−αUj+1(x1,...,xj )(cid:105)−1/α

(cid:104)

e−(n−(j−1)) ln Γ(1+α)−α(n−(j−1))c)e−αUj (x1,...,xj−1)(cid:105)−1/α

.

EW

pow
−1/α

 min

xj+1,...,xn

˜p(x)−α

W (xi)
Γ(1 + α)

n
(cid:89)

i=j+1

n
(cid:89)

i=j+1









W (xi)
Γ(1 + α)

W (xj)
Γ(1 + α)



















= EW

 min
xj ∈Xj

EW

 min

xj+1,...,xn

˜p(x)−α



≤ EW

 min
xj ,...,xn

˜p(x)−α

n
(cid:89)

i=j

W (xi)
Γ(1 + α)





Representing the Weibull random variables in terms of Gumbel random variables using the transformation W = e−(γ+c)α,
where γ ∼ Gumbel(−c), and manipulating the obtained expressions yields the claimed result.

B.2. Sequential samplers for the Gibbs distribution

Lost Relatives of the Gumbel Trick

The family of sequential samplers for the Gibbs distribution presented in the main text as Algorithm 1 has the same overall
structure as the sequential sampler derived by Hazan et al. (2013) from the Gumbel trick upper bound U(0), and hence
correctness can be argued similarly. Conditioned on accepting the sample, the probability that x = (x1, . . . , xn) is returned
is

n
(cid:89)

i=1

pi(xi) =

n
(cid:89)

i=1

e−c
Γ(1 + α)1/α

Eγ
Eγ

(cid:2)e−αUi+1(x1,...,xi)(cid:3)−1/α
(cid:2)e−αUi(x1,...,xi−1)(cid:3)−1/α

=

e−nc
Γ(1 + α)n/α

(cid:0)e−αφ(x1,...,xn)(cid:1)−1/α
E[e−αU ]−1/α

∝ p(x),

as required to show that the produced samples follow the Gibbs distribution p. Note, however, that in practice one intro-
duces an approximation by replacing expectations with sample averages.

B.3. Relationship between errors of sum-unary Gumbel perturbations

We write x∗ for the (random) MAP conﬁguration after sum-unary perturbation of the potential function, i.e.,

(cid:40)

(cid:41)

x∗ := argmax

φ(x) +

γi(xi)

.

x∈X

n
(cid:88)

i=1

Let qsum(x) := P[x = x∗] be the probability mass function of x∗.

The following results links together the errors acquired when using summed unary perturbations to upper bound the log
partition function ln Z ≤ U(0) using the Gumbel trick upper bound by Hazan & Jaakkola (2012), to approximately sample
from the Gibbs distribution by using qsum instead, and to upper bound the entropy of the approximate distribution qsum
using the bound due to Maji et al. (2014).
Proposition 11. Writing p for the Gibbs distribution, we have

Proof. By conditioning on the maximizing conﬁguration x∗, we can rewrite the Gumbel trick upper bound U(0) as follows:

(U(0) − ln Z)
(cid:125)
(cid:123)(cid:122)
(cid:124)
error in ln Z bound

+ KL(qsum (cid:107) p)
(cid:123)(cid:122)
(cid:125)
sampling error

(cid:124)

= Eγi [γi(x∗

i )] − H(qsum)
(cid:125)
(cid:123)(cid:122)
error in entropy estimation

(cid:124)

.

U(0) = Eγ

θ(x) +

γi(xi)

(cid:34)

(cid:40)

max
x∈X

(cid:32)

qsum(x)

θ(x) + Eγ

γi(xi) | x = x∗

(cid:35)(cid:33)

(cid:41)(cid:35)

(cid:34) n
(cid:88)

i=1

qsum(x)θ(x) +

Eγi [γi(x∗

i )] .

=

=

(cid:88)

x∈X

(cid:88)

x∈X

n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:88)

x∈X
(cid:88)

x∈X

At the same time, the KL divergence between qsum and the Gibbs distribution p generally expands as

KL(qsum (cid:107) p) = −H(qsum) −

qsum(x) ln

exp (θ(x))
˜x∈X exp (θ(˜x))

(cid:80)

= −H(qsum) −

qsum(x)θ(x) + ln Z.

Adding the two equations together and rearranging yields the claimed result.

Lost Relatives of the Gumbel Trick

C. Averaged unary perturbations

C.1. Lower bounds on the partition function

In the main text we stated the following two lower bounds on the log partition function ln Z.
Proposition 9. Let α ∈ (−1, 0) ∪ (0, ∞). For any subset S ⊆ {1, . . . , n} of the variables x1, . . . , xn we have ln Z ≥

c +

ln Γ(1 + α)
α

−

1
α

ln E

(cid:104)

e−α maxx{φ(x)+γS (xS )}(cid:105)

,

where xS := {xi : i ∈ S} and γS(xS) ∼ Gumbel(−c) independently for each setting of xS.

Proof. Let ¯S := {1, . . . , n} \ S. First we handle the case α > 0. We have trivially that

pow−α Z = pow−α

eφ(xS ,x ¯S ) ≤ pow−α

eφ(xS ,x ¯S ).

(cid:88)

(cid:88)

xS

x ¯S

(cid:88)

xS

max
x ¯S

The Weibull trick tells us that pow−α
Applying this to the summation over xS on the right-hand side of the above inequality, we obtain
(cid:34)

y pow−1/α h(y) = EW [miny

h(y)
Γ(1+α) W (y)] where {W (y)}y

(cid:35)

(cid:80)

pow−α Z ≤ EW

pow−α maxx ¯S eφ(xS ,x ¯S )
Γ(1 + α)

min
xS

W (xS)

.

iid∼ Weibull(1, α−1).

Expressing the Weibull random variable W (xS) as e−α(γS (xS )+c) with γS(xS) ∼ Gumbel(−c), the right-hand side can
be simpliﬁed as follows:

Taking the logarithm and dividing by −α < 0 yields the claimed result for positive α. For α ∈ (−1, 0) we proceed
similarly, obtaining that

pow−α Z ≤

1
Γ(1 + α)
e−αc
Γ(1 + α)

=

Eγ

(cid:20)
pow−α max
xS

max
x ¯S

eφ(xS ,x ¯S )eγS (xS )+c

(cid:21)

(cid:104)

Eγ

(cid:16)

exp

−α max

{φ(x) + γS(xS)}

(cid:17)(cid:105)

.

x

pow−α Z ≥ pow−α

(cid:88)

max
x ¯S

eφ(xS ,x ¯S )

(cid:34)

xS
pow−α maxx ¯S eφ(xS ,x ¯S )
Γ(1 + α)

min
xS

(cid:35)

F (xS)

,

= EF

where F (x(S)) ∼ Fr´echet(1, −α−1). Representing these random variables as e−α(γS (xS )+c) with γS(xS) ∼
Gumbel(−c), simplifying as in the previous case and ﬁnally dividing the inequality by −α > 0 yields the claimed re-
sult for α ∈ (−1, 0).

Corollary 10. For any α ∈ (−1, 0) ∪ (0, ∞), we have the lower bound ln Z ≥ L(α), where

L(α) := c +

ln Γ(1 + α)
α

−

1
nα

ln E [exp (−nαL)] ,

Proof. Applying Proposition 9 n times with all singleton sets S = {i} and averaging the obtained lower bounds yields

ln Z ≥ c +

ln Γ(1 + α)
α

−

1
n

n
(cid:88)

i=1

1
α

(cid:104)

ln E

(cid:16)

exp

−α max

{φ(x) + γi(xi)}

x

= c +

ln Γ(1 + α)
α

−

= c +

ln Γ(1 + α)
α

−

ln E

exp

−

(cid:34)

(cid:34)

n
(cid:88)

i=1

(cid:32)

(cid:32)

ln E

exp

−nα

1
nα

1
nα

α max
x

{φ(x) + γi(xi)}

1
n

n
(cid:88)

i=1

max
x

(cid:33)(cid:35)

{φ(x) + γi(xi)}

,

(cid:17)(cid:105)

(cid:33)(cid:35)

Lost Relatives of the Gumbel Trick

where the ﬁrst equality used the fact that the perturbations γi(xi) are mutually independent for different indices i to replace
the product of expectations with the expectation of the product. The claimed result follows by applying Jensen’s inequality
to swap the summation and the convex maxx function, noting that the inequality works out the right way for both positive
and negative α.

Jensen’s inequality can be used to relate the general lower bound L(α) to the Gumbel trick lower bound L(0), showing
that the former cannot be arbitrarily worse than the latter:
Proposition 14. For all α ∈ (−1, 0), the lower bound L(α) on ln Z satisﬁes

L(α) ≥ L(0) +

ln Γ(1 + α)
α

+ c

Proof. Apply Jensen’s inequality with the convex function x (cid:55)→ e−nα to the last term in the deﬁnition of L(α), noting that
the inequality works out the stated way for α < 0.

Note that ln Γ(1+α)
Gumbel lower bound L(0); it merely says that they cannot be arbitrarily worse than L(0).

+ c ≤ 0 for α ∈ (−1, 0) so this result does not imply that the Fr´echet lower bounds are tighter than the

α

C.2. Relationship between errors of averaged-unary Gumbel perturbations

In this section we write x∗ for the (random) MAP conﬁguration after average-unary perturbation of the potential function,
i.e.,

(cid:40)

x∗ := argmax

φ(x) +

x∈X

(cid:41)

γi(xi)

.

1
n

n
(cid:88)

i=1

where {γi(xi) | xi ∈ Xi, 1 ≤ i ≤ n} i.i.d.∼ Gumbel(−c). Let qavg(x) := P[x = x∗] be the probability mass function of x∗.
The Gumbel trick lower bound on the log partition function ln Z due to Hazan et al. (2013) is:

ln Z ≥ L(0) = Lφ(0) := Eγ

(cid:34)

(cid:40)

min
x∈X

φ(x) +

(cid:41)(cid:35)

γi(xi)

.

1
n

n
(cid:88)

i=1

(3)

We show that the gap of this Gumbel trick lower bound on ln Z upper bounds the KL divergence between the approximate
distribution qavg and the Gibbs distribution p. To this end, we ﬁrst need an entropy bound for qavg analogous to Theorem 1
of (Maji et al., 2014).
Theorem 15. The entropy of qavg can be lower bounded using expected values of max-perturbations as follows:

H(qavg) ≥

Eγi [γi(x∗

i )]

1
n

n
(cid:88)

i=1

Remark. Theorem 1 of (Maji et al., 2014) and this Theorem 15 differ in three aspects: (1) the former is an upper bound and
the latter is a lower bound, (2) the former sums the expectations while the latter averages them, and (3) the distributions
qsum and qavg of x∗ in the two theorems are different.

Proof. By the duality relation between negative entropy and the log partition function (Wainwright & Jordan, 2008), the
entropy H(qavg) of the unary-avg perturb-max distribution qavg can be expressed as
(cid:41)

(cid:40)

H(qavg) = inf
ϕ

ln Zϕ −

qavg(x)ϕ(x)

,

where the variable ϕ ranges over all potential functions on X , and Zϕ = (cid:80)
lower bound on the log partition function gives

x∈X exp ϕ(x). Applying the Gumbel trick

(cid:88)

x∈X

(cid:88)

x∈X

(cid:40)

(cid:41)

H(qavg) ≥ inf
ϕ

Lϕ(0) −

qavg(x)ϕ(x)

,

Lost Relatives of the Gumbel Trick

Proposition 16 in Appendix D shows that Lϕ(0) is a convex function of ϕ. The expression − (cid:80)
x∈X q(x)ϕ(x) is a linear
function of ϕ, so also convex, and thus as a sum of two convex functions, the quantity Lϕ(0) − (cid:80)
x∈X q(x)ϕ(x) within
the inﬁmum is a convex function of ϕ. Moreover, Proposition 17 in Appendix D tells us that the partial derivatives can be
computed as

(cid:32)

∂
∂ϕ(x)

(cid:88)

x∈X

(cid:33)

Lϕ(0) −

qavg(x)ϕ(x)

= qϕ(x) − qavg(x)

where qϕ(x) is the unary-avg perturb-max distribution associated with the potential function ϕ. Proposition 18 in Ap-
pendix D conﬁrms that these partial derivatives are continuous, so we observe that as a function of ϕ, the expression
Lϕ(0) − (cid:80)
x∈X qavg(x)ϕ(x) is a convex function with continuous partial derivatives, so it is a differentiable convex func-
tion. This is sufﬁcient to establish that the point ϕ = φ is a global minimum of this function (Wright & Nocedal, 1999).
Hence

(cid:40)

(cid:41)

H(qavg) ≥ inf
ϕ

Lϕ(0) −

qavg(x)ϕ(x)

= Lφ(0) −

qavg(x)φ(x)

(cid:88)

x∈X

(cid:34)

(cid:88)

x∈X

(cid:88)

=

qavg(x)Eγ

φ(x) +

γi(xi) | x = x∗

−

qavg(x)φ(x)

1
n

n
(cid:88)

i=1

(cid:35)

(cid:88)

x∈X

x∈X

1
n

n
(cid:88)

i=1

=

Eγi [γi(x∗

i )]

where we conditioned on the maximizing conﬁguration x∗ when expanding Lφ(0).

Remark. This proof proceeded in the same way as the proof of Maji et al. (2014) for the upper bound, except that es-
tablishing the minimizing conﬁguration of the inﬁmum is a non-trivial step that is actually required in this case. The
second revision of (Hazan et al., 2016) computes the derivative of Uϕ(0) − (cid:80)
x∈X qsum(x)ϕ(x), which is similar to our
Lϕ(0) − (cid:80)
x∈X qavg(x)ϕ(x), by differentiating under the expectation.

Equipped with Theorem 15, we can now show a link between the approximation “errors” of the averaged-unary perturba-
tion MAP conﬁguration distribution qavg (to the Gibbs distribution p) and estimate L(0) (to ln Z).
Proposition 12. Let p be the Gibbs distribution on X . Then

ln Z − L(0)
(cid:125)
(cid:123)(cid:122)
(cid:124)
error in ln Z bound

≥ KL(qavg (cid:107) p)
(cid:123)(cid:122)
(cid:125)
sampling error

(cid:124)

≥ 0

Remark. While we knew from Hazan et al. (2013) that ln Z − L(0) ≥ 0 (i.e. that L(0) is a lower bound on ln Z), this
is a stronger result showing that the size of the gap is an upper bound on the KL divergence between the average-unary
perturbation MAP distribution qavg and the Gibbs distribution p.

Proof. The Kullback-Leibler divergence in question expands as

KL(qavg (cid:107) p) = −H(qavg) −

qavg(x) ln

(cid:80)

= −H(qavg) −

qavg(x)φ(x) + ln Z.

(cid:88)

x∈X

exp φ(x)
˜x∈X exp φ(˜x)

(cid:88)

x∈X

From the proof of Theorem 15 we know that H(qavg) ≥ L(0) − (cid:80)

x∈X qavg(x)φ(x), so

KL(qavg (cid:107) p) ≤ −L(0) +

qavg(x)φ(x) −

qavg(x)φ(x) + ln Z = ln Z − L(0).

(cid:88)

x∈X

(cid:88)

x∈X

D. Technical results

Lost Relatives of the Gumbel Trick

In this section we write L(φ) instead of Lφ(0) for the Gumbel trick lower bound on ln Z associated with the potential
function φ, see equation (3).

Proposition 16. The Gumbel trick lower bound L(φ), viewed as a function of the potentials φ, is convex.

Proof. Convexity can be proved directly from deﬁnition. Let φ1 and φ2 be two arbitrary potential functions on a discrete
product space X , and let λ ∈ [0, 1]. Then

L(λφ1 + (1 − λ)φ2)

= Eγ

max
x∈X

= Eγ

max
x∈X

(cid:34)

(cid:34)

(cid:34)

(cid:40)

(cid:40)

(cid:32)

(cid:40)

λφ1(x) + (1 − λ)φ2(x) +

γi(xi)

(cid:41)(cid:35)

1
n

n
(cid:88)

i=1

(cid:33)

(cid:32)

(cid:41)

(cid:40)

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

= λL(φ1) + (1 − λ)L(φ2),

≤ Eγ

λ max
x∈X

φ1(x) +

γi(xi)

+ (1 − λ) max
x∈X

φ2(x) +

γi(xi)

(cid:33)(cid:41)(cid:35)

(cid:41)(cid:35)

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

λ

φ1(x) +

γi(xi)

+ (1 − λ)

φ2(x) +

γi(xi)

where we have used convexity of the max function to obtain the inequality, and linearity of expectation to arrive at the ﬁnal
equality.

Remark. This convexity proof goes through for other (low-dimensional) perturbations as well, e.g. it also works for Uφ(0).
Proposition 17. The Gumbel trick lower bound L(φ), viewed as a function of the potentials φ, has partial derivatives

∂
∂φ(˜x)

L(φ) = qφ(˜x)

where qφ is the probability mass function of the average-unary perturbation MAP conﬁguration’s distribution associated
with the potential function φ.

Proof. Let ˜x ∈ X , so that φ(˜x) is a general component of φ, and let e˜x be the indicator vector of ˜x. For any δ ∈ R, the
change in the lower bound L due to replacing φ(˜x) with φ(˜x) + δ is

L(φ + δe˜x) − L(φ) = Eγ

φ(x) + δ1{x = ˜x} +

γi(xi)

− Eγ

= Eγ

max
x∈X

φ(x) + δ1{x = ˜x} +

γi(xi)

− max
x∈X

φ(x) +

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:41)(cid:35)

(cid:34)

(cid:40)

max
x∈X

φ(x) +

(cid:41)

(cid:40)

(cid:41)(cid:35)

1
n

n
(cid:88)

i=1

γi(xi)

γi(xi)

(cid:41)(cid:35)

1
n

n
(cid:88)

i=1

max
x∈X

(cid:34)

(cid:34)

(cid:40)

(cid:40)

= Eγ [∆(φ, δ, ˜x, γ)]

by linearity of expectation, where we have denoted by ∆(φ, δ, ˜x, γ) the change in maximum due to replacing the potential
φ(˜x) with φ(˜x) + δ. Let’s condition on the argmax before modifying φ:

L(φ + δe˜x) − L(φ) = Eγ [∆(φ, δ, ˜x, γ)] =

qφ(x)Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax]

(cid:88)

x∈X

Now let’s condition on the size of the gap G between the maximum and the runner-up:

Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax] = P(G ≤ |δ|)Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax, G ≤ |δ|]
+ P(G > |δ|)Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax, G > |δ|]

Let’s examine all four terms on the right-hand side one by one:

Lost Relatives of the Gumbel Trick

1. P(G ≤ |δ|) → P(G = 0) = 0 as δ → 0 by monotonicity of measure.
2. Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax, G ≤ |δ|] ≤ δ since |∆(φ, δ, ˜x, γ)| ≤ |δ| always holds.
3. P(G > |δ|) → P(G ≥ 0) = 1 as δ → 0 by monotonicity of measure.
4. Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax, G > |δ|] = δ1{x = ˜x} since in this case both maximizations in the

deﬁnition of ∆(φ, δ, ˜x, γ) are maximized at x.

Therefore, as δ → 0,

Putting things together, we have

Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax] = o(1)o(δ) + (1 + o(1))δ1{x = ˜x}

lim
δ→0

L(φ + δe˜x) − L(φ)
δ

qφ(x) lim
δ→0

1
δ

Eγ [∆(φ, δ, ˜x, γ) | x is the original argmax]

(cid:88)

x∈X
(cid:88)

=

=

x∈X
= qφ(˜x),

qφ(x)1{x = ˜x}

which proves the stated claim directly from deﬁnition of a partial derivative.

Proposition 18. The probability mass function qφ of the average-unary perturbation MAP conﬁguration’s distribution
associated with a potential function φ is continuous in φ.

Proof. For any x∗ ∈ X we have from deﬁnition

qφ(x∗) = P

(cid:34)
x∗ = argmax

(cid:40)

x∈X

φ(x) +

γi(xi)

1
n

n
(cid:88)

i=1

(cid:41)(cid:35)

(cid:40)

(cid:34)
φ(x∗) +

= P

1
n

n
(cid:88)

i=1

(cid:34)

(cid:40)

= E

1

φ(x∗) +

1
n

n
(cid:88)

i=1

γi(x∗

i ) > max

x∈X \{x∗}

φ(x) +

γi(xi)

(cid:40)

(cid:41)(cid:41)(cid:35)

γi(x∗

i ) > max

x∈X \{x∗}

φ(x) +

γi(xi)

(cid:41)(cid:35)

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

which is continuous in φ by continuity of max, of 1 {· > ·} (as a function of φ) and by the Bounded Convergence Theorem.

Remark. The results above show that the Gumbel trick lower bound L(φ), viewed as a function of the potentials φ, is
convex and has continuous partial derivatives.

