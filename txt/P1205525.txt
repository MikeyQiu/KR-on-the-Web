High-Dimensional Regularized Discriminant Analysis

John A. Rameya, Caleb K. Steinb, Phil D. Youngc, Dean M. Youngd

aNovi Labs
bMyeloma Institute, University of Arkansas for Medical Sciences
cDepartment of Management and Information Systems, Baylor University
dDepartment of Statistical Science, Baylor University

7
1
0
2
 
b
e
F
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
2
8
1
1
0
.
2
0
6
1
:
v
i
X
r
a

Abstract

Regularized discriminant analysis (RDA), proposed by Friedman (1989), is a widely popular classiﬁer that

lacks interpretability and is impractical for high-dimensional data sets. Here, we present an interpretable

and computationally eﬃcient classiﬁer called high-dimensional RDA (HDRDA), designed for the small-

sample, high-dimensional setting. For HDRDA, we show that each training observation, regardless of class,

contributes to the class covariance matrix, resulting in an interpretable estimator that borrows from the

pooled sample covariance matrix. Moreover, we show that HDRDA is equivalent to a classiﬁer in a reduced-

feature space with dimension approximately equal to the training sample size. As a result, the matrix

operations employed by HDRDA are computationally linear in the number of features, making the classiﬁer

well-suited for high-dimensional classiﬁcation in practice. We demonstrate that HDRDA is often superior

to several sparse and regularized classiﬁers in terms of classiﬁcation accuracy with three artiﬁcial and six

real high-dimensional data sets. Also, timing comparisons between our HDRDA implementation in the

sparsediscrim R package and the standard RDA formulation in the klaR R package demonstrate that as

the number of features increases, the computational runtime of HDRDA is drastically smaller than that of

Keywords: Regularized discriminant analysis, High-dimensional classiﬁcation, Covariance-matrix

regularization, Singular value decomposition, Multivariate analysis, Dimension reduction

2010 MSC: 62H30, 65F15, 65F20, 65F22

RDA.

1. Introduction

In this paper, we consider the classiﬁcation of small-sample, high-dimensional data, where the number of

features p exceeds the training sample size N . In this setting, well-established classiﬁers, such as linear dis-

criminant analysis (LDA) and quadratic discriminant analysis (QDA), become incalculable because the class

and pooled covariance matrix estimators are singular (Murphy, 2012; Bouveyron, Girard, and Schmid, 2007;

Mkhadri, Celeux, and Nasroallah, 1997). To improve the accuracy of the estimation of the class covariance

matrices estimated in the QDA classiﬁer and to ensure that the covariance matrix estimators are nonsin-

gular, Friedman (1989) proposed the regularized discriminant analysis (RDA) classiﬁer by incorporating a

weighted average of the pooled sample covariance matrix and the class sample covariance matrix. To further

improve the accuracy of the estimation of the class covariance matrix and to stabilize its inverse, Friedman

(1989) also included a regularization component by shrinking the covariance matrix estimator towards the

Preprint submitted to Elsevier

February 7, 2017

identity matrix, which yields a nonsingular estimator following the well-known ridge-regression approach of

Hoerl and Kennard (1970). Despite its popularity, the “borrowing” operation employed in the RDA classi-

ﬁer lacks interpretability (Bensmail and Celeux, 1996). Furthermore, the RDA classiﬁer is impractical for

high-dimensional data sets because it computes the inverse and determinant of the covariance matrices for

each class. Both matrix calculations are computationally expensive because the number of operations grows

at a polynomial rate in the number of features. Moreover, the model selection of the RDA classiﬁer’s two

tuning parameters is computationally burdensome because the matrix inverse and determinant of each class

covariance matrix are computed across multiple cross-validation folds for each candidate tuning-parameter

Here, we present the high-dimensional RDA (HDRDA) classiﬁer, which is intended for the case when

p > N . We reparameterize the RDA classiﬁer similar to that of Hastie, Tibshirani, and Friedman (2008)

and Halbe and Aladjem (2007) and employ a biased covariance-matrix estimator that partially pools the

individual sample covariance matrices from the QDA classiﬁer with the pooled sample covariance matrix from

the LDA classiﬁer. We then shrink the resulting covariance-matrix estimator towards a scaled identity matrix

to ensure positive deﬁniteness. We show that the pooling parameter in the HDRDA classiﬁer determines

the contribution of each training observation to the estimation of each class covariance matrix, enabling

interpretability that has been previously lacking with the RDA classiﬁer (Bensmail and Celeux, 1996). Our

parameterization diﬀers from that of Hastie, Tibshirani, and Friedman (2008) and that of Halbe and Aladjem

(2007) in that our formulation allows the ﬂexibility of various covariance-matrix estimators proposed in the

literature, including a variety of ridge-like estimators, such as the one proposed by Srivastava and Kubokawa

pair.

(2007).

Next, we establish that the matrix operations corresponding to the null space of the pooled sample

covariance matrix are redundant and can be discarded from the HDRDA decision rule without loss of

classiﬁcatory information when we apply reasoning similar to that of Ye and Wang (2006). As a result, we

achieve a substantial reduction in dimension such that the matrix operations used in the HDRDA classiﬁer are

computationally linear in the number of features. Furthermore, we demonstrate that the HDRDA decision

rule is invariant to adjustments to the approximately p − N zero eigenvalues, so that the decision rule in the

original feature space is equivalent to a decision rule in a lower dimension, such that matrix inverses and

determinants of relatively small matrices can be rapidly computed. Finally, we show that several shrinkage

methods that are special cases of the HDRDA classiﬁer have no eﬀect on the approximately p − N zero

eigenvalues of the covariance-matrix estimators when p > N . Such techniques include work from Srivastava

and Kubokawa (2007), Rao and Mitra (1971), and other methods studied by Ramey and Young (2013) and

Xu, Brock, and Parrish (2009).

We also provide an eﬃcient algorithm along with pseudocode to estimate the HDRDA classiﬁer’s tuning

parameters in a grid search via cross-validation. Timing comparisons between our HDRDA implementation

in the sparsediscrim R package available on CRAN and the standard RDA formulation in the klaR R

package demonstrate that as the number of features increases, the computational runtime of the HDRDA

classiﬁer is drastically smaller than that of RDA. In fact, when p = 5000, we show that the HDRDA classiﬁer

2

is 502.786 times faster on average than the RDA classiﬁer. In this scenario, the HDRDA classiﬁer’s model

selection requires 2.979 seconds on average, while that of the RDA classiﬁer requires 24.933 minutes on

average.

Finally, we study the classiﬁcation performance of the HDRDA classiﬁer on six real high-dimensional data

sets along with a simulation design that generalizes the experiments initially conducted by Guo et al. (2007).

We demonstrate that the HDRDA classiﬁer often attains superior classiﬁcation accuracy to several recent

classiﬁers designed for small-sample, high-dimensional data from Tong, Chen, and Zhao (2012), Witten and

Tibshirani (2011), Pang, Tong, and Zhao (2009), and Guo et al. (2007). We also include as a benchmark

the random forest from Breiman (2001) because Fern´andez-Delgado, Cernadas, Barro, and Amorim (2014)

have concluded that the random forest is often superior to other classiﬁers in benchmark studies. We show

that our proposed classiﬁer is competitive and often outperforms the random forest in terms of classiﬁcation

accuracy in the small-sample, high-dimensional setting.

The remainder of this paper is organized as follows. In Section 2 we introduce the classiﬁcation problem

and necessary notation to describe our contributions. In Section 3 we present the HDRDA classiﬁer along

with its interpretation. In Section 4, we provide properties of the HDRDA classiﬁer and a computationally

eﬃcient model-selection procedure. In Section 5, we compare the model-selection timings of the HDRDA

and RDA classiﬁers.

In Section 6 we describe our simulation studies of artiﬁcial and real data sets and

examine the experimental results. We conclude with a brief discussion in Section 7.

2. Preliminaries

2.1. Notation

To facilitate our discussion of covariance-matrix regularization and high-dimensional classiﬁcation, we
require the following notation. Let Ra×b denote the matrix space of all a × b matrices over the real ﬁeld
R. Denote by Im the m × m identity matrix, and let 0m×p be the m × p matrix of zeros, such that 0m
is understood to denote 0m×m. Deﬁne 1m ∈ Rm×1 as a vector of ones. Let AT , A+, and N (A) denote
the transpose, the Moore-Penrose pseudoinverse, and the null space of A ∈ Rm×p, respectively. Denote
by R>
p×p denote the cone of real p × p
positive-semideﬁnite matrices. Let V ⊥ denote the orthogonal complement of a vector space V ⊂ Rp×1. For
c ∈ R, let c+ = 1/c if c (cid:54)= 0 and 0 otherwise.

p×p the cone of real p × p positive-deﬁnite matrices. Similarly, let R≥

2.2. Discriminant Analysis

In discriminant analysis we wish to assign an unlabeled vector x ∈ Rp×1 to one of K unique, known
classes by constructing a classiﬁer from N training observations. Let xi = (xi1, . . . , xip) ∈ Rp×1 be the ith

observation (i = 1, . . . , N ) with true, unique membership yi ∈ {ω1, . . . , ωK}. Denote by nk the number of
training observations realized from class k, such that (cid:80)K
from a mixture distribution p(x) = (cid:80)K

k=1 nk = N . We assume that (xi, yi) is a realization
k=1 p(x|ωk)p(ωk), where p(x|ωk) is the probability density function
(PDF) of the kth class and p(ωk) is the prior probability of class membership of the kth class. We further

assume p(ωk) = p(ωl), 1 ≤ k, l ≤ K, k (cid:54)= l.

3

The QDA classiﬁer is the optimal Bayesian decision rule with respect to a 0 − 1 loss function when
p(x|ωk) is the PDF of the multivariate normal distribution with known mean vectors µk ∈ Rp×1 and known
covariance matrices Σk ∈ R>

p×p, k = 1, 2, . . . , K. Because µk and Σk are typically unknown, we assign an

unlabeled observation x to class ωk with the sample QDA classiﬁer

DQDA(x) = arg min

(x − ¯xk)T (cid:98)Σ−1

k (x − ¯xk) + log | (cid:98)Σk|,

k

where ¯xk and (cid:98)Σk are the maximum-likelihood estimators (MLEs) of µk and Σk, respectively. If we assume

further that Σk = Σ, k = 1, . . . , K, then the pooled sample covariance matrix (cid:98)Σ is substituted for (cid:98)Σk in

(1), where

(1)

(2)

(cid:98)Σ = N −1

nk (cid:98)Σk

K
(cid:88)

k=1

is the MLE for Σ. Here, (1) reduces to the sample LDA classiﬁer. We omit the log-determinant because it

is constant across the K classes.

The smallest eigenvalues of (cid:98)Σk and the directions associated with their eigenvectors can highly inﬂuence

the classiﬁer in (1).

In fact, the eigenvalues of (cid:98)Σk are well-known to be biased if p ≥ nk such that the

smallest eigenvalues are underestimated (Seber, 2004). Moreover, if p > nk, then rank( (cid:98)Σk) ≤ nk, which

implies that at least p − nk eigenvalues of (cid:98)Σk are zero. Furthermore, although more feature information is
available to discriminate among the K classes, if p > nk, (1) is incalculable because (cid:98)Σ−1

k does not exist.

Several regularization methods, such as the methods considered by Xu et al. (2009), Guo et al. (2007),

and Mkhadri (1995), have been proposed in the literature to adjust the eigenvalues of (cid:98)Σk so that (1) is
calculable and provides reduced variability for (cid:98)Σ−1
applies a shrinkage factor γ > 0, so that

k . A common form of the covariance-matrix regularization

(cid:98)Σk(γ) = (cid:98)Σk + γIp,

(3)

similar to a method employed in ridge regression (Hoerl and Kennard, 1970). Equation (3) eﬀectively shrinks

the sample covariance matrix (cid:98)Σk toward Ip, thereby increasing the eigenvalues of (cid:98)Σk by γ. Speciﬁcally,

the zero eigenvalues are replaced with γ, so that (3) is positive deﬁnite. For additional covariance-matrix

regularization methods, see Ramey and Young (2013), Xu et al. (2009), and Ye and Ji (2009).

3. High-Dimensional Regularized Discriminant Analysis

Here, we deﬁne the HDRDA classiﬁer by ﬁrst formulating the covariance-matrix estimator (cid:98)Σk(λ) and

demonstrating its clear interpretation as a linear combination of the crossproducts of the training observations

centered by their respective class sample means. We deﬁne the convex combination

(cid:98)Σk(λ) := (1 − λ) (cid:98)Σk + λ (cid:98)Σ,

k = 1, . . . , K,

(4)

4

where λ ∈ [0, 1] is the pooling parameter. By rewriting (4) in terms of the observations xi, i = 1, . . . , N ,

each centered by its class sample mean, we attain a clear interpretation of (cid:98)Σk(λ). That is,

(cid:98)Σk(λ) =

1 − λ +

(cid:98)Σk +

nk(cid:48) (cid:98)Σk(cid:48)

(cid:18)

(cid:19)

λnk
N

λ
N

K
(cid:88)

k(cid:48)=1
k(cid:48)(cid:54)=k

I(yi = k)xixT

i +

I(yi (cid:54)= k)xixT
i

λ
N

N
(cid:88)

i=1

(cid:18) 1 − λ
nk

+

λ
N

(cid:19) N
(cid:88)

i=1

cik(λ)xixT
i ,

=

=

N
(cid:88)

i=1

where cik(λ) = λN −1 + (1 − λ)n−1

k I(yi = k). From (5), we see that λ weights the contribution of each of
the N observations in estimating Σk from all K classes rather than using only the nk observations from a

single class. As a result, we can interpret (5) as a covariance-matrix estimator that borrows from (cid:98)Σ in (2)

to estimate Σk.

In Figure 1 we plot the contours of ﬁve multivariate normal populations for λ = 0 with unequal covariance

matrices. As λ approaches 1, the contours become more similar, resulting in identical contours for λ = 1. Be-

low, we show that the pooling operation is advantageous in increasing the rank of each (cid:98)Σk(λ) from rank( (cid:98)Σk)

to rank( (cid:98)Σ) for 0 < λ ≤ 1. Notice that if λ = 0, then the observations from the remaining K − 1 classes do

not contribute to the estimation of Σk, corresponding to (cid:98)Σk. Furthermore, if λ = 1, the weights cik(λ) in (5)
reduce to 1/N , corresponding to (cid:98)Σ. For brevity, when λ = 1, we deﬁne X = [(cid:112)c1k(1)xT
N ]T
such that (cid:98)Σ = N −1X T X. Similarly, for λ = 0, we deﬁne Xk = [(cid:112)c1k(0)xT
N ]T such that
(cid:98)Σk = n−1

1 , . . . , (cid:112)cN k(0)xT

1 , . . . , (cid:112)cN k(1)xT

k X T

k Xk.

[Insert Figure 1 approximately here ]

As we have discussed above, several eigenvalue adjustment methods have been proposed that increase

eigenvalues (approximately) equal to 0. To further improve the estimation of Σk and to stabilize the esti-

mator’s inverse, we deﬁne the eigenvalue adjustment of (4) as

˜Σk := αk (cid:98)Σk(λ) + γIp,

where αk ≥ 0 and γ ≥ 0 is an eigenvalue-shrinkage constant. Thus, the pooling parameter λ controls

the amount of estimation information borrowed from (cid:98)Σ to estimate Σk, and the shrinkage parameter γ

determines the degree of eigenvalue shrinkage. The choice of αk allows for a ﬂexible formulation of covariance-

matrix estimators. For instance, if αk = 1, k = 1, . . . , K, then (6) resembles (3). Similarly, if αk = 1 − γ,

then (6) has a form comparable to the RDA classiﬁer from Friedman (1989). Substituting (6) into (1), we

deﬁne the HDRDA classiﬁer as

DHDRDA(x) = arg min

(x − ¯xk)T ˜Σ+

k (x − ¯xk) + log | ˜Σk|.

k

For γ > 0, ˜Σk is nonsingular such that ˜Σ−1
k in (7). If γ = 0, we explicitly set
k
| ˜Σk| equal to the product of the positive eigenvalues of ˜Σk. Following Friedman (1989), we select λ and γ

can be substituted for ˜Σ+

(5)

(6)

(7)

5

from a grid of candidate models via cross-validation (Hastie et al., 2008). We provide an implementation of

(7) in the hdrda function contained in the sparsediscrim R package, which is available on CRAN.

The choice of αk in (6) is one of convenience and allows the ﬂexibility of various covariance-matrix

estimators proposed in the literature. In practice, we generally are not interested in estimating αk because

the estimation of K additional tuning parameters via cross-validation is counterproductive to our goal of

computational eﬃciency. For appropriate values of αk, the HDRDA covariance-matrix estimator includes or

resembles a large family of estimators. Notice that if αk = 1 and λ = 1, (6) is equivalent to the standard

ridge-like covariance-matrix estimator in (3). Other estimators proposed in the literature can be obtained

when one selects γ accordingly. For instance, with γ = tr{ (cid:98)Σ}/ min(N, p), we obtain the estimator from

Srivastava and Kubokawa (2007).

When αk = 1 − γ, (6) resembles the biased covariance-matrix estimator

(cid:98)Σk(λ, γ) = (1 − γ) (cid:98)Σ(RDA)

k

(λ) + γ

(cid:111)

(λ)

tr

(cid:110)
(cid:98)Σ(RDA)
p

k

Ip

(8)

employed in the RDA classiﬁer, where (cid:98)Σ(RDA)
tion parameter that controls the shrinkage of (8) towards Ip weighted by the average of the eigenvalues of
(cid:98)Σ(RDA)
ﬁer is impractical for high-dimensional data because the inverse and determinant of (8) must be calculated

(λ). Despite the similarity of (8) to the HDRDA covariance-matrix estimator in (6), the RDA classi-

(λ) is a pooled estimator of Σk and γ ∈ [0, 1] is a regulariza-

k

k

when substituted into (1). Furthermore, (8) has no clear interpretation.

4. Properties of the HDRDA Classiﬁer

Next, we establish properties of the covariance-matrix estimator and the decision rule employed in the

HDRDA classiﬁer. By doing so, we demonstrate that (7) lends itself to a more eﬃcient calculation. We

decompose (7) into a sum of two components, where the ﬁrst summand consists of matrix operations applied

to low-dimensional matrices and the second summand corresponds to the null space of (cid:98)Σ in (2). We show that

the matrix operations performed on the null space of (cid:98)Σ yield constant quadratic forms across all classes and

can be omitted. For p (cid:29) N , the constant component involves determinants and inverses of high-dimensional

matrices, and by ignoring these calculations, we achieve a substantial reduction in computational costs.

Furthermore, a byproduct is that adjustments to the associated eigenvalues have no eﬀect on (7). Lastly, we

utilize the singular value decomposition to eﬃciently calculate the eigenvalue decomposition of (cid:98)Σ, further

reducing the computational costs of the HDRDA classiﬁer.

First, we require the following relationship regarding the null spaces of (cid:98)Σk(λ), (cid:98)Σ, and (cid:98)Σk.

Lemma 1. Let (cid:98)Σk and (cid:98)Σ be the MLEs of Σk and Σ, respectively. Let (cid:98)Σk(λ) be deﬁned as in (4). Then,

N { (cid:98)Σk(λ)} ⊂ N ( (cid:98)Σ) ⊂ N ( (cid:98)Σk), k = 1, . . . , K.

Proof. Let z ∈ N { (cid:98)Σk(λ)} for some k = 1, . . . , K. Hence, 0 = zT (cid:98)Σk(λ)z = (1 − λ)zT (cid:98)Σkz + λzT (cid:98)Σz.
Because (cid:98)Σk, (cid:98)Σ ∈ R≥
Now, suppose z ∈ N ( (cid:98)Σ). Similarly, we have that 0 = zT (cid:98)Σz = N −1 (cid:80)K
z ∈ N ( (cid:98)Σk) because (cid:98)Σk ∈ R≥

p×p, we have z ∈ N ( (cid:98)Σ) and z ∈ N ( (cid:98)Σk). In particular, we have that N { (cid:98)Σk(λ)} ⊂ N ( (cid:98)Σ).
k=1 nkzT (cid:98)Σkz, which implies that

p×p. Therefore, N ( (cid:98)Σ) ⊂ N ( (cid:98)Σk).

6

In Lemma 2 below, we derive an alternative expression for ˜Σk in terms of the matrix of eigenvectors of (cid:98)Σ.
p×p is the diagonal matrix of eigenvalues

Let (cid:98)Σ = U DU T be the eigendecomposition of (cid:98)Σ such that D ∈ R≥
of (cid:98)Σ with

D =





Dq

0

0

0p−q



 ,

Dq ∈ R>

q×q is the diagonal matrix consisting of the positive eigenvalues of (cid:98)Σ, the columns of U ∈ Rp×p are
the corresponding orthonormal eigenvectors of (cid:98)Σ, and rank( (cid:98)Σ) = q. Then, we partition U = (U1, U2) such
that U1 ∈ Rp×q and U2 ∈ Rp×(p−q).

Lemma 2. Let (cid:98)Σ = U DU T be the eigendecomposition of (cid:98)Σ as above, and suppose that rank( (cid:98)Σ) = q ≤ p.

Then, we have

where

Hence,

˜Σk = U





Wk

0

0

γIp−q


 U T ,

k = 1, . . . , K,

Wk = αk{(1 − λ)U T

1 (cid:98)ΣkU1 + λDq} + γIq.

(9)

(10)

Proof. From Lemma 1, the columns of U2 span the null space of (cid:98)Σk, which implies that (cid:98)ΣkU2 = 0p×(p−q).

U T (cid:98)ΣkU =





U T

1 (cid:98)ΣkU1

0

0

0p−q



 ,

k = 1, . . . , K.

Thus, U T ˜ΣkU = αk{(1 − λ)U T (cid:98)ΣkU + λD} + γIp, and (9) holds because U is orthogonal.

As an immediate consequence of Lemma 2, we have the following corollary.

Corollary 1. Let (cid:98)Σk(λ) be deﬁned as in (4). Then, for λ ∈ (0, 1], rank{ (cid:98)Σk(λ)} = q, k = 1, . . . , K.

Proof. The proof follows when we set γ = 0 in Lemma 2.

Thus, by incorporating each xi into the estimation of Σk, we increase the rank of (cid:98)Σk(λ) to q ≈ N if

λ (cid:54)= 0. Next, we provide an essential result that enables us to prove that (7) is invariant to adjustments to
the eigenvalues of ˜Σk corresponding to the null space of (cid:98)Σ.

Lemma 3. Let U2 be deﬁned as above. Then, for all x ∈ Rp×1, U T
where k (cid:54)= k(cid:48).

2 (x − ¯xk) = U T

2 (x − ¯xk(cid:48)), 1 ≤ k, k(cid:48) ≤ K,

2 ∈ C( (cid:98)Σ)⊥ (Kollo and von Rosen, 2005, Lemma 1.2.5). Now, because xi ∈ C( (cid:98)Σ) (i = 1, . . . , N ), U T

Proof. Let x ∈ Rp×1, and suppose that 1 ≤ k, k(cid:48) ≤ K. Recall that U2 ∈ N ( (cid:98)Σ), which implies that
U T
2 xi =
0p−q. Hence, 0p−q = (cid:80)N
2 ( ¯xk − ¯xk(cid:48)), where βi = (nknk(cid:48))−1{I(yi = k)nk(cid:48) − I(yi = k(cid:48))nk}.
Therefore, U T

2 xi = U T

i=1 βiU T

2 (x − ¯xk) = U T

2 (x − ¯xk(cid:48)).

7

We now present our main result, where we decompose (7) and show that the term requiring the largest

computational costs does not contribute to the classiﬁcation of an unlabeled observation performed using

Lemma 3. Hence, we reduce (7) to an equivalent, more computationally eﬃcient decision rule.

Theorem 1. Let ˜Σk and Wk be deﬁned as in (9) and (10), respectively, and let U1 be deﬁned as above.

Then, the decision rule in (7) is equivalent to

DHDRDA(x) = arg min

(x − ¯xk)T U1W −1

k U T

1 (x − ¯xk) + log |Wk|.

(11)

k

Proof. From (9), we have that





W −1
k

˜Σ+

k = U

0


 U T

0

γ+Ip−q

and | ˜Σk| = γp−q|Wk|, k = 1, . . . , K. Therefore, for all x ∈ Rp×1, we have that

(x − ¯xk)T ˜Σ+

k (x − ¯xk) + log | ˜Σk| = (x − ¯xk)T U1W −1
+ γ+(x − ¯xk)T U2U T

k U T

1 (x − ¯xk)

2 (x − ¯xk) + log |Wk|

+ (p − q) log γ.

Because γ is constant for k = 1, . . . , K, we can omit the (p − q) log γ term and particularly avoid the

calculation of log 0 for γ = 0. Then, the proof follows from Lemma 3 because U T

2 (x − ¯xk) is constant for

k = 1, . . . , K.

Using Theorem 1, we can avoid the time-consuming inverses and determinants of p×p covariance matrices
in (7) and instead calculate these same operations on Wk ∈ Rq×q in (11). The substantial computational

improvements arise because our proposed classiﬁer in (7) is invariant to the term U2, thus yielding an

equivalent classiﬁer in (11) with a substantial reduction in computational complexity. Here, we demonstrate

that the computational eﬃciency in calculating the inverse and determinant of Wk can be further improved

via standard matrix operations when we show that the inverses and determinants of Wk can be performed

on matrices of size nk × nk.

Proposition 1. Let Wk be deﬁned as above. Then, |Wk| = |Γk||Qk| and

W −1

k = Γ−1

k − n−1

k αk(1 − λ)Γ−1

k U T

1 X T

k Q−1

k XkU1Γ−1
k ,

where

and

Qk = Ink + n−1

k αk(1 − λ)XkU1Γ−1

k U T

1 X T
k

Γk = αkλDq + γIq.

8

(12)

(13)

(14)

k αk(1 − λ)U T

Proof. First, we write Wk = n−1
1 X T
from Harville (2008), which states that |A + BT C| = |A||T ||T −1 + CA−1B|, where A ∈ R>

k XkU1 + Γk. To calculate |Wk|, we apply Theorem 18.1.1
a×a, B ∈ Ra×b,
k , T = Ink , and C = XkU1, we have
|Wk| = |Γk||Qk|. Similarly, (12) follows from the well-known Sherman-Woodbury formula (Harville, 2008,

b×b, and C ∈ Rb×a. Thus, setting A = Γk, B = αk(1 − λ)U T

T ∈ R>

1 X T

Theorem 18.2.8) because (A + BT C)−1 = A−1 − A−1B(T −1 + CA−1B)−1CA−1.

Notice that Γk is singular when (λ, γ) = (0, 0) because Γk = 0q, in which case we use the formulation in

(11) instead. Also, notice that if αk is constant across the K classes, then Γk in (14) is independent of k.

Consequently, |Γk| is constant across the K classes and need not be calculated in (11).

4.1. Model Selection

Thus far, we have presented the HDRDA classiﬁer and its properties that facilitate an eﬃcient calculation

of the decision rule. Here, we describe an eﬃcient model-selection procedure along with pseudocode in

Algorithm 1 to select the optimal tuning-parameter estimates from the Cartesian product of candidate

values {λg}G

g=1 × {γh}H

h=1. We estimate the V -fold cross-validation error rate for each candidate pair and
select ((cid:98)λ, (cid:98)γ), which attains the minimum error rate. To calculate the V -fold cross-validation, we partition
the original training data into V mutually exclusive and exhaustive folds that have approximately the same

number of observations. Then, for v = 1, . . . , V , we classify the observations in the vth fold by training

a classiﬁer on the remaining V − 1 folds. We calculate the cross-validation error as the proportion of

misclassiﬁed observations across the V folds.

A primary contributing factor to the eﬃciency of Algorithm 1 is our usage of the compact singular

value decomposition (SVD). Rather than computing the eigenvalue decomposition of (cid:98)Σ to obtain U1, we

instead obtain U1 by computing the eigendecomposition of a much smaller N × N matrix when p (cid:29) N
(Hastie et al., 2008, Chapter 18.3.5). Applying the SVD, we decompose Xc = M ∆U T , where M ∈ RN ×p
is orthogonal, ∆ ∈ R≥
p×p is a diagonal matrix consisting of the singular values of Xc, and U ∈ Rp×p is
c Xc, we have the eigendecomposition (cid:98)Σ = U DU T , where U is the
orthogonal. Recalling that (cid:98)Σ = N −1X T
matrix of eigenvectors of (cid:98)Σ and D = N −1∆ is the diagonal matrix of eigenvalues of (cid:98)Σ. Now, we can obtain
c = M DM T . Next, we
M and D eﬃciently from the eigenvalue decomposition of the N × N matrix XcX T
compute U = X T

c M D+/2, where

D+/2 =





D−1/2
q

0

0

0N −q



 .

We then determine q, the number of numerically nonzero eigenvalues present in D, by calculating the number

of eigenvalues that exceeds some tolerance value, say, 1 × 10−6. We then extract U1 as the ﬁrst q columns

of U .

As a result of the compact SVD, we need calculate XcU1 only once per cross-validation fold, requiring

O(pqN ) ≈ O(pN 2) calculations. Hence, the computational costs of expensive calculations, such as matrix

inverses and determinants, are greatly reduced because they are performed in the q-dimensional subspace.

Similarly, we reduce the dimension of the test data set by calculating XtestU1 once per fold. Conveniently,

9

input : Data matrix X

Parameter grid {λg}G

g=1 × {γh}H

h=1

output: Optimal Estimates (ˆλ, ˆγ)

for v ← 1 to V do

Partition X into Xtrain ∈ RN ×p and Xtest ∈ RNT ×p
for k ← 1 to K do

Extract Xk ∈ Rnk×p from Xtrain
Compute sample mean ¯xk from Xk
Center Xk ← Xk − 1nk ¯xT
k

end

end

1 , . . . , X T

Xc ← [X T
Compute the compact SVD Xc = MqDqU T
1

K]T

Transform Xc ← XcU1

Transform Xtest ← XtestU1

for k ← 1 to K do

Extract Xk ∈ Rnk×q from Xc
Recompute sample mean ¯xk from Xk

for (λ, γ) ∈ {λg}G

g=1 × {γh}H

h=1 do

for k ← 1 to K do

Compute Qk using (13)

Compute Γk using (14)
Compute W −1

using (12)

k

Compute |Wk| = |Γk||Qk|
Compute (x − ¯xk)T U1W −1

k U T

1 (x − ¯xk) + log |Wk| for each row x of Xtest

end

end

Classify test observations Xtest using (11)

Compute the number of misclassiﬁed test observations #{Errorv(λ, γ)}

end
Compute (cid:92)Error(λ, γ) = N −1 (cid:80)V
Report optimal (ˆλ, ˆγ) ← arg min(λ,γ)

v=1 #{Errorv(λ, γ)}
(cid:92)Error(λ, γ)

Algorithm 1: Model selection for the HDRDA classiﬁer

10

we see that the most costly computation involved in Qk and W −1
is XkU1, which can be extracted from
XcU1. Thus, after the initial calculation of XcU1 per cross-validation fold, Qk requires O(nkq2) operations.
Because Qk ∈ Rnk×nk , both its determinant and inverse require O(n3
k) operations. Consequently, W −1
k ∈ Rq×q requires O(q) operations.
requires O(nkq2) operations. Also, the inverse of the diagonal matrix Γ−1
Finally, we remark that |Wk| requires O(n3

k) operations.

k

k

The expressions given in Proposition 1 also expedite the selection of λ and γ via cross-validation

because the most time-consuming matrix operation involved in computing W −1
Rnk×q, which is independent of λ and γ. The subsequent operations in calculating W −1
be simply updated for diﬀerent pairs of λ and γ without repeating the costly computations. Also, rather
than calculating (x − ¯xk)T U1W −1
k)(cid:48)U1W −1
(Xtest − ¯xk1(cid:48)

1 (x − ¯xk) individually for each row x of Xtest, we can calculate
k). The diagonal elements of the resulting matrix contain the indi-

k U T
1 (Xtest − ¯xk1(cid:48)

and |Wk| is XkU1 ∈

and |Wk| can

k

k

k U T
vidual quadratic form of each test observation, xt.

5. Timing Comparisons between RDA and HDRDA

In this section, we demonstrate that the computational performance of the model selection employed in

the HDRDA classiﬁer is substantially faster than that of the RDA classiﬁer on small-sample, high-dimensional

data sets. The relative diﬀerence in runtime between the two classiﬁers drastically increases as p increases. To

compare the two classiﬁers, we generated 25 observations from each of K = 4 multivariate normal populations

with mean vectors µ1 = −3 · 1p, µ2 = −1p, µ3 = 1p, and µ4 = 3 · 1p. We set the covariance matrix of

each population to the p × p identity matrix. For each data set generated, we estimated the parameters λ

and γ for both classiﬁers using a grid of 5 equidistant candidate values between 0 and 1, inclusively. We

set αk = 1 − γ, k = 1, . . . , K, in the HDRDA classiﬁer. At each pair of λ and γ, we computed the 10-fold

cross-validation error rate (Hastie et al., 2008). Then, we selected the model that minimized the 10-fold

cross-validation error rate.

We compared the runtime of both classiﬁers by increasing the number of features from p = 500 to p = 5000

in increments of 500. Next, we generated 100 data sets for each value of p and computed the training and

model selection runtime of both classiﬁers. Our timing comparisons are based on our HDRDA implementation

in the sparsediscrim R package and the standard RDA implementation in the klaR R package. All timing

comparisons were conducted on an Amazon Elastic Compute Cloud (EC2) c4.4xlarge instance using version

3.3.1 of the open-source statistical software R. Our timing comparisons can be reproduced with the code

available at https://github.com/ramhiser/paper-hdrda.

5.1. Timing Comparison Results

In Figure 2, we plotted the runtime of the model selections for both the HDRDA and RDA classiﬁers as

a function of p. We observed that the HDRDA classiﬁer was substantially faster than the RDA classiﬁer as p

increased. In the left panel of Figure 2, we ﬁt a quadratic regression line to the RDA runtimes and a simple

linear regression model to the HDRDA runtimes. For improved understanding, in the right panel we repeated

the same scatterplot and linear ﬁt with the timings restricted to the observed range of the HDRDA timings.

11

Figure 2 suggests that the usage of a matrix inverse and determinant in the klaR R package’s discriminant

function yielded model-selection timings that exceeded linear growth in p. Because the HDRDA classiﬁer

removes inverse and determinants, it was computationally more eﬃcient than the RDA classiﬁer, especially

as p increased. In fact, when p = 5000, the RDA classiﬁer required 24.933 minutes on average to perform

model selection, while the HDRDA classiﬁer selected its optimal model in 2.979 seconds on average. Clearly,

the model selection employed by the HDRDA classiﬁer is substantially faster than that of the RDA classiﬁer.

We quantiﬁed the relative timing comparisons between the two classiﬁers by calculating the ratio of

mean timings of the RDA classiﬁer to the HDRDA classiﬁer for each value of p. We employed nonparametric

bootstrapping to estimate the mean ratio along with 95% conﬁdence intervals. In Figure 3, the bootstrap

sampling distributions for the ratio of mean timings are given. First, we observe that the mean relative

timings increased as p increased. For smaller dimensions, the relative diﬀerence in computing was sizable

with the average ratio of the mean timings equal to 14.513 for p = 500 and a 95% conﬁdence interval

of (14.191, 14.855). Furthermore, the ratio of mean computing times suggested that the RDA classiﬁer is

impractical for higher dimensions. For instance, when p = 5000, the ratio of mean computing times increased

to 502.786 with a 95% conﬁdence interval of (462.863, 546.396).

[Insert Figure 2 approximately here ]

[Insert Figure 3 approximately here ]

6. Classiﬁcation Study

In this section, we compare our proposed classiﬁer with four classiﬁers recently proposed for small-sample,

high-dimensional data along with the random-forest classiﬁer from Breiman (2001) using version 3.3.1 of the

open-source statistical software R. Within our study, we included penalized linear discriminant analysis from

Witten and Tibshirani (2011), implemented in the penalizedLDA package. We also considered shrunken

centroids regularized discriminant analysis from Guo et al. (2007) in the rda package. Because the rda

package does not perform the authors’ “Min-Min” rule automatically, we applied this rule within our R code.

We included two modiﬁcations of diagonal linear discriminant analysis from Tong et al. (2012) and Pang

et al. (2009), where the former employs an improved mean estimator and the latter utilizes an improved

variance estimator. Both classiﬁers are available in the sparsediscrim package. Finally, we incorporated the

random forest as a benchmark based on the ﬁndings of Fern´andez-Delgado et al. (2014), who concluded that

the random forest is often superior to other classiﬁers in benchmark studies. We used the implementation

of the random-forest classiﬁer from the randomForest package with 250 trees and 100 maximum nodes. For

each classifer we explicitly set prior probabilities as equal, if applicable. All other classiﬁer options were set

to their default settings. Below, we refer to each classiﬁer by the ﬁrst author’s surname. All simulations

were conducted on an Amazon EC2 c4.4xlarge instance. Our analyses can be reproduced via the code

available at https://github.com/ramhiser/paper-hdrda.

12

For the HDRDA classiﬁer in (11), we examined the classiﬁcation performance of two models. For the

ﬁrst HDRDA model, we set αk = 1, k = 1, . . . , K, so that the covariance-matrix estimator (6) resembled

(3). We estimated λ from a grid of 21 equidistant candidate values between 0 and 1, inclusively. Similarly,

we estimated γ from a grid consisting of the values 10−1, . . . , 104, and 105. We selected optimal estimates of

λ and γ using 10-fold cross-validation. For the second model, we set αk = 1 − γ, k = 1, . . . , K, to resemble

Friedman’s parameterization, and we estimated both λ and γ from a grid of 21 equidistant candidate values

between 0 and 1, inclusively.

We did not include the RDA classiﬁer in our classiﬁcation study because its training runtime was pro-

hibitively slow on high-dimensional data in our preliminary experiments. As shown in Section 5, the runtime

of the RDA classiﬁer was drastically larger than that of the HDRDA classiﬁer for a tuning grid of size

25 = 5 × 5. Consequently, a fair comparison between the RDA and HDRDA classiﬁers would require model

selection of 441 = 21 × 21 diﬀerent pairs of tuning parameters in the RDA classiﬁer. A tuning grid of this

size yielded excessively slow training runtimes for the RDA implementation from the klaR R package.

6.1. Simulation Study

In this section we compare the competing classiﬁers using the simulation design from Guo et al. (2007).

This design is widely used within the high-dimensional classiﬁcation literature, including the studies by

Ramey and Young (2013) and Witten and Tibshirani (2011). First, we consider the block-diagonal covariance

matrix from Guo et al. (2007),

Σk =


















0100
...
...
· · ·

Σ(ρk)

0100

0100 Σ(−ρk)

0100

0100

0100 Σ(ρk)

· · ·

0100

0100

· · ·

· · ·

· · ·

0100
...
· · ·

0100 Σ(−ρk) 0100
...
. . .
· · ·

0100

· · ·

· · ·


















· · ·
...
...
...
...
· · ·

,

(15)

where the (i, j)th entry of the block matrix Σ(ρk) ∈ R100×100 is

Σ(ρk)

ij = {ρ|i−j|

k

}1≤i,j≤100.

The block-diagonal covariance structure in (15) resembles gene-expression data: within each block of path-

ways, genes are correlated, and the correlation decays as a function of the distance between any two genes.

The original design from Guo et al. (2007) comprised two p-dimensional multivariate normal populations

with a common block-diagonal covariance matrix.

Although the design is indeed standard, the simulation conﬁguration lacks artifacts commonly observed

in real data, such as skewness and extreme outliers. As a result, we wished to investigate the eﬀect of outliers

on the high-dimensional classiﬁers. To accomplish this goal, we generalized the block-diagonal simulation

conﬁguration by sampling from a p-dimensional multivariate contaminated normal distribution. Denoting

the PDF of the p-dimensional multivariate normal distribution by Np(x|µ, Σ), we write the PDF of the kth

13

class as

0.05, . . ., 0.50.

p(x|ωk) = (1 − (cid:15))Np(x|µk, Σk) + (cid:15)Np(x|µk, ηΣk),

(16)

where (cid:15) ∈ [0, 1] is the probability that an observation is contaminated (i.e., drawn from a distribution with

larger variance) and η > 1 scales the covariance matrix Σk to increase the extremity of outliers. For (cid:15) = 0,

we have the benchmark block-diagonal simulation design from Guo et al. (2007). As (cid:15) is increased, the

average number of outliers is increased. In our simulation, we let η = 100 and considered the values of (cid:15) = 0,

We generated K = 3 populations from (16) with Σk given in (15) and set the mean vector of class 1 to

µ1 = 0p. Next, comparable to Guo et al. (2007), the ﬁrst 100 features of µ2 were set to 1/2, while the rest

). For simplicity, we deﬁned µ3 = −µ2. The three populations

diﬀered in their mean vectors in the ﬁrst 100 features corresponding to the ﬁrst block, and no diﬀerence in

were set to 0, i.e., µ2 = (1/2, . . . , 1/2
(cid:125)

(cid:124)

(cid:123)(cid:122)
100

, 0, . . . , 0
(cid:124) (cid:123)(cid:122) (cid:125)
p−100

the means occurred in the remaining blocks.

From each of the K = 3 populations, we sampled 25 training observations (nk = 25 for all k) and 10,000

test observations. After training each classiﬁer on the training data, we classiﬁed the test data sets and

computed the proportion of mislabeled test observations to estimate the classiﬁcation error rate for each

classiﬁer. Repeating this process 500 times, we computed the average of the error-rate estimates for each

classiﬁer. We allowed the number of features to vary from p = 100 to p = 500 in increments of 100 to examine

the classiﬁcation accuracy as the feature dimension increased while maintaining a small sample size. Guo

et al. (2007) originally considered ρk = 0.9 for all k. Alternatively, to explore the more realistic assumption

of unequal covariance matrices, we put ρ1 = 0.1, ρ2 = 0.5, and ρ3 = 0.9.

6.1.1. Simulation Results

In Figure 4, we observed each classiﬁer’s average classiﬁcation error rates for the values of (cid:15) and p.

Unsurprisingly, the average error rate increased for each classiﬁer as the contamination probability (cid:15) increased

regardless of the value of p. Sensitivity to the presence of outliers was most apparent for the Pang, Tong, and

Witten classiﬁers. For smaller dimensions, the random-forest and HDRDA classiﬁers tended to outperform

the remaining classiﬁers with the random forest performing best. As the feature dimension increased with

p ≥ 300, both HDRDA classiﬁers outperformed all other classiﬁers, suggesting that their inherent dimension

reduction better captured the classiﬁcatory information in the small training samples, even in the presence

of outliers.

[Insert Figure 4 approximately here ]

The Pang, Tong, and Witten methods yielded practically the same and consistently the worst error rates

when outliers were present with (cid:15) > 0, suggesting that these classiﬁers were sensitive to outliers. Notice, for

example, that when p = 400, the error rates of the Pang, Tong, and Witten classiﬁers increased dramatically

from approximately 19% when no outliers were present to approximately 43% when (cid:15) = 0.05. The sharp

increase in average error rates for these three classiﬁers continued as (cid:15) increased. Guo’s method always

14

outperformed those of Pang, Witten, and Tong, but after outliers were introduced, the Guo classiﬁer’s

average error rate was not competitive with the HDRDA classiﬁers or the random-forest classiﬁer.

[Insert Figure 5 approximately here ]

In Figure 5, we again examine the simulation results as a function of p for a subset of the values of (cid:15).

This set of plots allows us to investigate the eﬀect of feature dimensionality on classiﬁcation performance.

When no outliers were present (i.e., (cid:15) = 0), the random-forest classiﬁer was outperformed by all other

classiﬁers. Furthermore, the HDRDA classiﬁers were superior in terms of average error rate in this setting.

As p increased, an elevation in average error rate was expected for all classiﬁers, but the increase was not

observed to be substantial.

For (cid:15) > 0, we observed a diﬀerent behavior in classiﬁcation performance. First, the Pang, Tong, and

Witten methods, along with the random-forest method, increased in average error rate as p increased.

Contrarily, the performance of the HDRDA and Guo classiﬁers was hardly aﬀected by p. Also, as discussed

above, the HDRDA classiﬁers were superior to all other classiﬁers for large values of p with only the random-

forest classiﬁer outperforming them in smaller feature-dimension cases.

6.2. Application to Gene Expression Data

We compared the HDRDA classiﬁer to the ﬁve competing classiﬁers on six benchmark gene-expression

microarray data sets. First, we evaluated the classiﬁcation accuracy of each classiﬁer by randomly parti-

tioning the data set under consideration such that 2/3 of the observations were allocated as training data

and the remaining 1/3 of the observations were allocated as a test data set. To expedite the computational

runtime, we reduced the training data to the top 1000 variables by employing the variable-selection method

proposed by Dudoit et al. (2002). We then reduced the test data set to the same 1000 variables. After

training each classiﬁer on the training data, we classiﬁed the test data sets and computed the proportion

of mislabeled test observations to estimate the classiﬁcation error rate for each classiﬁer. Repeating this

process 100 times, we computed the average of the error-rate estimates for each classiﬁer. We next provide

a concise description of each high-dimensional data set examined in our classiﬁcation study.

6.2.1. Chiaretti et al. (2004) Data Set

Chiaretti et al. (2004) measured the gene-expression proﬁles for 128 individuals with acute lymphoblastic

leukemia (ALL) using Aﬀymetrix human 95Av2 arrays. Following Xu et al. (2009), we restricted the data

set to K = 2 classes such that n1 = 74 observations were without cytogenetic abnormalities and n2 = 37

observations had a detected BCR/ABL gene. The robust multichip average normalization method was

applied to all 12,625 gene-expression levels.

6.2.2. Chowdary et al. (2006) Data Set

Chowdary et al. (2006) investigated 52 matched pairs of tissues from colon and breast tumors using

Aﬀymetrix U133A arrays and ribonucleic-acid (RNA) ampliﬁcation. Each tissue pair was gathered from

the same patient and consisted of a snap-frozen tissue and a tissue suspended in an RNAlater preservative.

15

Overall, 31 breast-cancer and 21 colon-cancer pairs were gathered, resulting in K = 2 classes with n1 = 62

and n2 = 42. A purpose of the study was to determine whether the disease state could be identiﬁed using

22,283 gene-expression proﬁles.

6.2.3. Nakayama et al. (2007) Data Set

Nakayama et al. (2007) acquired 105 gene-expression samples of 10 types of soft-tissue tumors through an

oligonucleotide microarray, including 16 samples of synovial sarcoma (SS), 19 samples of myxoid/round cell

liposarcoma (MLS), 3 samples of lipoma, 3 samples of well-diﬀerentiated liposarcoma (WDLS), 15 samples of

dediﬀerentiated liposarcoma (DDLS), 15 samples of myxoﬁbrosarcoma (MFS), 6 samples of leiomyosarcoma

(LMS), 3 samples of malignant nerve sheathe tumor (MPNST), 4 samples of ﬁbrosarcoma (FS), and 21

samples of malignant ﬁbrous histiocytoma (MFH). Nakayama et al. (2007) determined from their data that

these 10 types fell into 4 broader groups: (1) SS; (2) MLS; (3) Lipoma, WDLS, and part of DDLS; (4) Spindle

cell and pleomorophic sarcomas including DDLS, MFS, LMS, MPNST, FS, and MFH. Following Witten and

Tibshirani (2011), we restricted our analysis to the ﬁve tumor types having at least 15 observations.

6.2.4. Shipp et al. (2002) Data Set

According to Shipp et al. (2002), approximately 30%-40% of adult non-Hodgkin lymphomas are diﬀuse

large B-cell lymphomas (DLBCLs). However, only a small proportion of DLBCL patients are cured with

modern chemotherapeutic regimens. Several models have been proposed, such as the International Prognostic

Index (IPI), to determine a patient’s curability. These models rely on clinical covariates, such as age, to

determine if the patient can be cured, and the models are often ineﬀective. Shipp et al. (2002) have argued

that researchers need more eﬀective means to determine a patient’s curability. The authors measured 6,817

gene-expression levels from 58 DLBCL patient samples with customized cDNA (lymphochip) microarrays to

investigate the curability of patients treated with cyclophosphamide, adriamycin, vincristine, and prednisone

(CHOP)-based chemotherapy. Among the 58 DLBCL patient samples, 32 are from cured patients while 26

are from patients with fatal or refractory disease.

6.2.5. Singh et al. (2002) Data Set

Singh et al. (2002) have examined 235 radical prostatectomy specimens from surgery patients between

1995 and 1997. The authors used oligonucleotide microarrays containing probes for approximately 12,600

genes and expressed sequence tags. They have reported that 102 of the radical prostatectomy specimens are

of high quality: 52 prostate tumor samples and 50 non-tumor prostate samples.

6.2.6. Tian et al. (2003) Data Set

Tian et al. (2003) investigated the puriﬁed plasma cells from the bone marrow of control patients along

with patients with newly diagnosed multiple myeloma. Expression proﬁles for 12,2625 genes were obtained via

Aﬀymetrix U95Av2 microarrays. The plasma cells were subjected to biochemical and immunohistochemical

analyses to identify molecular determinants of osteolytic lesions. For 36 multiple-myloma patients, focal

bone lesions could not be detected by magnetic resonance imaging (MRI), whereas MRI was used to detect

such lesions in 137 patients.

16

6.2.7. Classiﬁcation Results

Similar to Witten and Tibshirani (2011), we report the average test error rates obtained over 100 random

training-test partitions in Table 1 along with standard deviations of the test error rates in parentheses. The

HDRDA and Guo classiﬁers were superior in classiﬁcation performance for the majority of the simulations.

The HDRDA classiﬁers yielded the best classiﬁcation accuracy on the Chowdary and Shipp data sets. Al-

though the random forest’s accuracy slightly exceeded the HDRDA classiﬁers on the Tian data set, our

proposed classiﬁers outperformed the other competing classiﬁers considered here. Moreover, the HDRDA

classiﬁers yielded comparable performance on ﬁve of the six data sets.

[Insert Table 1 approximately here ]

The average error-rate estimates for the Pang, Tong, and Witten classiﬁers were comparable across all six

data sets. Furthermore, the average error rates for the Pang and Tong classiﬁers were approximately equal

for all data sets except for the Chiaretti dataset. This result suggests that the mean and variance estimators

used in lieu of the MLEs provided little improvement to classiﬁcation accuracies. However, we investigated

the Pang classiﬁer’s poor performance on the Chiaretti data set and determined that its variance estimator

exhibited numerical instability. The classiﬁer’s denominator was approximately zero for both classes and led

to the poor classiﬁcation performance.

The random-forest classiﬁer was competitive when applied to the Chowdary and Singh data sets and

yielded the smallest error rate of the considered classiﬁers on the Tian data set. The fact that the HDRDA

and Guo classiﬁers typically outperformed the random-forest classiﬁer challenges the claim of Fern´andez-

Delgado et al. (2014) that random forests are typically superior. Further studies should be performed to

validate this statement in the small-sample, high-dimensional setting.

Finally, the Pang, Tong, and Witten classiﬁers consistently yielded the largest average error rates across

the six data sets. Given that the standard deviations were relatively large, we hesitate to generalize claims

regarding the ranking of these three classiﬁers in terms of the average error rate. However, the classiﬁers’

error rates and their variability across multiple random partitions of each data set were large enough that

we might question their beneﬁt when applied to real data.

7. Discussion

We have demonstrated that our proposed HDRDA classiﬁer is competitive with and often superior to ran-

dom forests as well as the Witten, Pang, Tong, and Guo classiﬁers. In fact, we have shown that the HDRDA

classiﬁer often yields superior classiﬁcation accuracy when applied to small-sample, high-dimensional data

sets, conﬁrming the assertions of Mai et al. (2012) and Fan et al. (2012) that diagonal classiﬁers often yield

inferior classiﬁcation performance when compared to other classiﬁcation methods. Furthermore, we have

demonstrated that HDRDA classiﬁers are more robust to the presence of outliers than the diagonal clas-

siﬁers despite their rapid computational performance and their reduction in the number of parameters to

estimate.

17

We also considered the popular penalized linear discriminant analysis from Witten and Tibshirani (2011)

because it was speciﬁcally designed for high-dimensional gene-expression data. We had expected its clas-

siﬁcation performance to be competitive within our classiﬁcation study and perhaps superior. Contrarily,

our empirical studies suggest that the classiﬁer is sensitive to outliers and unable to achieve comparable

results with other classiﬁers designed for small-sample, high-dimensional data. Also, despite the claims of

Fern´andez-Delgado et al. (2014) that random forests are typically superior to other classiﬁers, we observed

that they were indeed competitive but were typically outperformed by classiﬁers developed for small-sample,

high-dimensional data.

We demonstrated that our HDRDA implementation in the sparsediscrim R package can be used in

practice with high-dimensional data sets. In our timing comparisons, we showed that HDRDA model selection

could be employed on data sets with p = 5000 in 2.979 seconds on average. Contrarily, the RDA classiﬁer

implemented in the klaR R package required 24.933 minutes on average to perform model selection on data

sets with p = 5000. Given that the RDA classiﬁer has been shown to have excellent performance in the

high-dimensional setting (Webb and Copsey, 2011) but is limited by its computationally intense model-

selection procedure, our work replaces the RDA classiﬁer for high-dimensional data in practice. This result

is reassuring because the RDA classiﬁer remains widely popular in the literature. In fact, variants of the

RDA classiﬁer have been applied to microarray data (Ching et al., 2012; Li and Wu, 2012; Tai and Pan,

2007; Guo et al., 2007), facial recognition (Zhang et al., 2010; Dai and Yuen, 2007; Lu et al., 2005; Pima and

Aladjem, 2004; Lu and Plataniotis, 2003), handwritten digit recognition (Bouveyron et al., 2007), remote

sensing (Tadjudin and Landgrebe, 1999), seismic detection (Anderson, 2002), and chemical spectra (Wu

et al., 1996; Aeberhard et al., 1993).

The dimension reduction employed in this paper has reduced the dimension to rank( (cid:98)Σ) = q. An inter-

esting extension of our work would reduce the dimension q further to a lower dimension qL < q, perhaps

using a criterion similar to that of principal components analysis. While unclear whether the classiﬁcation

performance would improve via such a method, the eﬃciency of the model selection would certainly improve.

Moreover, if qL = 2 or 3, low-dimensional graphical displays of high-dimensional data could be obtained.

We thank Mrs. Joy Young for her numerous recommendations that enhanced the quality of our writing.

References

Aeberhard, S., Coomans, D., Vel, O. D., 1993. Improvements to the classiﬁcation performance of RDA.

Journal of Chemometrics 7 (2), 99–115.

Anderson, D. N., Aug. 2002. Application of Regularized Discrimination Analysis to Regional Seismic Event

Identiﬁcation. Bulletin of the Seismological Society of America 92 (6), 2391–2399.

Bensmail, H., Celeux, G., Dec. 1996. Regularized Gaussian Discriminant Analysis through Eigenvalue De-

composition. Journal of the American Statistical Association 91 (436), 1743–1748.

Bouveyron, C., Girard, S., Schmid, C., Oct. 2007. High-Dimensional Discriminant Analysis. Communications

in Statistics - Theory and Methods 36 (14), 2607–2623.

18

Breiman, L., 2001. Random Forests - Springer. Machine Learning 45 (1), 5–32.

Chiaretti, S., Li, X., Gentleman, R., Vitale, A., Vignetti, M., Mandelli, F., Ritz, J., Foa, R., 2004. Gene

expression proﬁle of adult T-cell acute lymphocytic leukemia identiﬁes distinct subsets of patients with

diﬀerent response to therapy and survival. Blood 103 (7), 2771–2778.

Ching, W.-K., Chu, D., Liao, L.-Z., Wang, X., Jul. 2012. Regularized orthogonal linear discriminant analysis.

Pattern Recognition 45 (7), 2719–2732.

Chowdary, D., Lathrop, J., Skelton, J., Curtin, K., Briggs, T., Zhang, Y., Yu, J., Wang, Y., Mazumder,

A., Feb. 2006. Prognostic Gene Expression Signatures Can Be Measured in Tissues Collected in RNAlater

Preservative. The Journal of Molecular Diagnostics 8 (1), 31–39.

Dai, D.-Q., Yuen, P. C., Aug. 2007. Face recognition by regularized discriminant analysis. IEEE transactions

on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and

Cybernetics Society 37 (4), 1080–1085.

Dudoit, S., Fridlyand, J., Speed, T. P., Mar. 2002. Comparison of Discrimination Methods for the Classiﬁ-

cation of Tumors Using Gene Expression Data. Journal of the American Statistical Association 97 (457),

Fan, J., Feng, Y., Tong, X., Apr. 2012. A road to classiﬁcation in high dimensional space: the regularized

optimal aﬃne discriminant. Journal of the Royal Statistical Society: Series B (Statistical Methodology)

Fern´andez-Delgado, M., Cernadas, E., Barro, S., Amorim, D., Jan. 2014. Do we need hundreds of classiﬁers

to solve real world classiﬁcation problems? The Journal of Machine Learning Research 15 (1), 3133–3181.

Friedman, J. H., 1989. Regularized Discriminant Analysis. Journal of the American Statistical Association

Guo, Y., Hastie, T., Tibshirani, R., Jan. 2007. Regularized linear discriminant analysis and its application

in microarrays. Biostatistics 8 (1), 86–100.

Halbe, Z., Aladjem, M., Nov. 2007. Regularized mixture discriminant analysis. Pattern Recognition Letters

Harville, D. A., 2008. Matrix Algebra from a Statistician’s Perspective. Springer, New York.

Hastie, T., Tibshirani, R., Friedman, J., Dec. 2008. The Elements of Statistical Learning, 2nd Edition. Data

Mining, Inference, and Prediction. Springer New York, New York, NY.

Hoerl, A. E., Kennard, R. W., Feb. 1970. Ridge Regression: Biased Estimation for Nonorthogonal Problems.

Technometrics 12 (1), 55–67.

Kollo, T., von Rosen, D., 2005. Advanced Multivariate Statistics with Matrices. Vol. 579 of Mathematics

and Its Applications (New York). Springer, Dordrecht.

77–87.

74 (4), 745–771.

84 (405), 165–175.

28 (15), 2104–2115.

19

Li, R., Wu, B., Aug. 2012. Sparse regularized discriminant analysis with application to microarrays. Com-

putational biology and chemistry 39, 14–19.

Lu, J., Plataniotis, K. N., 2003. Regularized discriminant analysis for the small sample size problem in face

recognition. Pattern Recognition Letters.

Lu, J., Plataniotis, K. N., Venetsanopoulos, A. N., 2005. Regularization studies of linear discriminant analysis

in small sample size scenarios with application to face recognition. Pattern Recognition Letters 26 (2),

Mai, Q., Zou, H., Yuan, M., Feb. 2012. A direct approach to sparse discriminant analysis in ultra-high

181–191.

dimensions. Biometrika 99 (1), 29–42.

nition Letters 16 (3), 267–275.

Mkhadri, A., Mar. 1995. Shrinkage parameter for the modiﬁed linear discriminant analysis. Pattern Recog-

Mkhadri, A., Celeux, G., Nasroallah, A., 1997. Regularization in discriminant analysis: an overview. Com-

putational Statistics and Data Analysis 23 (3), 403–423.

Murphy, K. P., Aug. 2012. Machine Learning: A Probabilistic Perspective. The MIT Press, Cambridge,

Massachusetts.

Nakayama, R., Nemoto, T., Takahashi, H., Ohta, T., Kawai, A., Seki, K., Yoshida, T., Toyama, Y., Ichikawa,

H., Hasegawa, T., Apr. 2007. Gene expression analysis of soft tissue sarcomas: characterization and

reclassiﬁcation of malignant ﬁbrous histiocytoma. Nature 20 (7), 749–759.

Pang, H., Tong, T., Zhao, H., Mar. 2009. Shrinkage-based Diagonal Discriminant Analysis and Its Applica-

tions in High-Dimensional Data. Biometrics 65 (4), 1021–1029.

Pima, I., Aladjem, M., 2004. Regularized discriminant analysis for face recognition. Pattern Recognition

37 (9), 1945–1948.

581–596.

Berkeley, pp. 601–620.

Interscience.

Ramey, J., Young, P. D., 2013. A comparison of regularization methods applied to the linear discriminant

function with high-dimensional microarray data. Journal of Statistical Computation and Simulation 83 (3),

Rao, C. R., Mitra, S. K., 1971. Generalized inverse of a matrix and its applications. In: Proceedings of

the Sixth Berkeley Symposium on Mathematical Statistics and Probability. University of California Press,

Seber, G. A. F., Aug. 2004. Multivariate Observations. Wiley Series in Probability and Statistics. Wiley-

Shipp, M. A., Ross, K. N., Tamayo, P., Weng, A. P., Kutok, J. L., Aguiar, R. C. T., Gaasenbeek, M., Angelo,

M., Reich, M., Pinkus, G. S., Ray, T. S., Koval, M. A., Last, K. W., Norton, A., Lister, T. A., Mesirov,

J., Neuberg, D. S., Lander, E. S., Aster, J. C., Golub, T. R., Jan. 2002. Diﬀuse large B-cell lymphoma

20

outcome prediction by gene-expression proﬁling and supervised machine learning. Nature Medicine 8 (1),

68–74.

Singh, D., Febbo, P. G., Ross, K., Jackson, D. G., Manola, J., Ladd, C., Tamayo, P., Renshaw, A. A.,

D’Amico, A. V., Richie, J. P., Lander, E. S., Loda, M., Kantoﬀ, P. W., Golub, T. R., Sellers, W. R., Mar.

2002. Gene expression correlates of clinical prostate cancer behavior. Cancer Cell 1 (2), 203–209.

Srivastava, M. S., Kubokawa, T., 2007. Comparison of discrimination methods for high dimensional data.

Journal of the Japan Statistical Society 37 (1), 123–134.

Tadjudin, S., Landgrebe, D. A., Jul. 1999. Covariance estimation with limited training samples. IEEE

Transactions on Geoscience and Remote Sensing 37 (4), 2113–2118.

Tai, F., Pan, W., Dec. 2007. Incorporating prior knowledge of gene functional groups into regularized dis-

criminant analysis of microarray data. Bioinformatics 23 (23), 3170–3177.

Tian, E., Zhan, F., Walker, R., Rasmussen, E., Ma, Y., Barlogie, B., Shaughnessy, Jr., J. D., Dec. 2003.

The Role of the Wnt-Signaling Antagonist DKK1 in the Development of Osteolytic Lesions in Multiple

Myeloma. New England Journal of Medicine 349 (26), 2483–2494.

Tong, T., Chen, L., Zhao, H., Feb. 2012. Improved mean estimation and its application to diagonal discrim-

inant analysis. Bioinformatics 28 (4), 531–537.

Webb, A. R., Copsey, K. D., Sep. 2011. Statistical Pattern Recognition, 3rd Edition. John Wiley & Sons,

Chichester, West Sussex, UK.

Witten, D. M., Tibshirani, R., Aug. 2011. Penalized classiﬁcation using Fisher’s linear discriminant. Journal

of the Royal Statistical Society: Series B (Statistical Methodology) 73 (5), 753–772.

Wu, W., Mallet, Y., Walczak, B., Penninckx, W., Massart, D. L., Heuerding, S., Erni, F., Aug. 1996.

Comparison of regularized discriminant analysis, linear discriminant analysis, and quadratic discriminant

analysis applied to NIR data. Analytica Chimica Acta 329 (3), 257–265.

Xu, P., Brock, G. N., Parrish, R. S., Mar. 2009. Modiﬁed linear discriminant analysis approaches for classiﬁ-

cation of high-dimensional microarray data. Computational Statistics and Data Analysis 53 (5), 1674–1687.

Ye, J., Ji, S., Nov. 2009. Discriminant Analysis for Dimensionality Reduction: An Overview of Recent

Developments. Biometrics: Theory, Methods, and Applications. John Wiley & Sons, Inc., Hoboken, NJ,

USA.

Ye, J., Wang, T., 2006. Regularized Discriminant Analysis for High Dimensional, Low Sample Size Data. In:

The 12th ACM SIGKDD International Conference. ACM Press, New York, New York, USA, p. 454.

Zhang, Z., Dai, G., Xu, C., Jordan, M. I., Mar. 2010. Regularized Discriminant Analysis, Ridge Regression

and Beyond. The Journal of Machine Learning Research 11.

21

Figure 1: Contours of ﬁve multivariate normal populations as a function of the pooling parameter λ.

22

Figure 2: Timing comparisons (in seconds) between HDRDA and RDA classiﬁers.

23

Figure 3: Distribution of ratios of mean RDA runtime to mean HDRDA runtime across 1000 bootstrap replications.

24

Figure 4: Average classiﬁcation error rates as a function of the contamination probability (cid:15). Approximate standard errors were

no greater than 0.022.

25

Figure 5: Average classiﬁcation error rates as a function of the number of features p. Approximate standard errors were no

greater than 0.022.

26

Classiﬁer

Chiaretti

Chowdary

Nakayama

Shipp

Singh

Tian

Guo

0.111 (0.044)

0.056 (0.051)

0.208 (0.061)

0.086 (0.063)

0.089 (0.055)

0.268 (0.082)

HDRDA Convex

0.115 (0.044)

0.035 (0.026)

0.208 (0.066)

0.073 (0.057)

0.111 (0.059)

0.229 (0.049)

HDRDA Ridge

0.118 (0.050)

0.033 (0.022)

0.208 (0.070)

0.072 (0.065)

0.099 (0.046)

0.225 (0.050)

Pang

0.663 (0.062)

0.197 (0.091)

0.227 (0.062)

0.192 (0.091)

0.221 (0.095)

0.267 (0.054)

Random Forest

0.124 (0.053)

0.045 (0.028)

0.232 (0.063)

0.135 (0.078)

0.093 (0.045)

0.206 (0.044)

Tong

Witten

0.195 (0.068)

0.197 (0.091)

0.227 (0.062)

0.192 (0.091)

0.221 (0.095)

0.267 (0.054)

0.194 (0.068)

0.197 (0.091)

0.232 (0.068)

0.193 (0.092)

0.221 (0.095)

0.264 (0.053)

Table 1: The average of the test error rates obtained on gene-expression data sets over 100 random training-test partitions.

Standard deviations of the test error rates are given in the parentheses. The classiﬁer with the minimum average error rate for

each data set is in bold.

27

High-Dimensional Regularized Discriminant Analysis

John A. Rameya, Caleb K. Steinb, Phil D. Youngc, Dean M. Youngd

aNovi Labs
bMyeloma Institute, University of Arkansas for Medical Sciences
cDepartment of Management and Information Systems, Baylor University
dDepartment of Statistical Science, Baylor University

7
1
0
2
 
b
e
F
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
2
8
1
1
0
.
2
0
6
1
:
v
i
X
r
a

Abstract

Regularized discriminant analysis (RDA), proposed by Friedman (1989), is a widely popular classiﬁer that

lacks interpretability and is impractical for high-dimensional data sets. Here, we present an interpretable

and computationally eﬃcient classiﬁer called high-dimensional RDA (HDRDA), designed for the small-

sample, high-dimensional setting. For HDRDA, we show that each training observation, regardless of class,

contributes to the class covariance matrix, resulting in an interpretable estimator that borrows from the

pooled sample covariance matrix. Moreover, we show that HDRDA is equivalent to a classiﬁer in a reduced-

feature space with dimension approximately equal to the training sample size. As a result, the matrix

operations employed by HDRDA are computationally linear in the number of features, making the classiﬁer

well-suited for high-dimensional classiﬁcation in practice. We demonstrate that HDRDA is often superior

to several sparse and regularized classiﬁers in terms of classiﬁcation accuracy with three artiﬁcial and six

real high-dimensional data sets. Also, timing comparisons between our HDRDA implementation in the

sparsediscrim R package and the standard RDA formulation in the klaR R package demonstrate that as

the number of features increases, the computational runtime of HDRDA is drastically smaller than that of

Keywords: Regularized discriminant analysis, High-dimensional classiﬁcation, Covariance-matrix

regularization, Singular value decomposition, Multivariate analysis, Dimension reduction

2010 MSC: 62H30, 65F15, 65F20, 65F22

RDA.

1. Introduction

In this paper, we consider the classiﬁcation of small-sample, high-dimensional data, where the number of

features p exceeds the training sample size N . In this setting, well-established classiﬁers, such as linear dis-

criminant analysis (LDA) and quadratic discriminant analysis (QDA), become incalculable because the class

and pooled covariance matrix estimators are singular (Murphy, 2012; Bouveyron, Girard, and Schmid, 2007;

Mkhadri, Celeux, and Nasroallah, 1997). To improve the accuracy of the estimation of the class covariance

matrices estimated in the QDA classiﬁer and to ensure that the covariance matrix estimators are nonsin-

gular, Friedman (1989) proposed the regularized discriminant analysis (RDA) classiﬁer by incorporating a

weighted average of the pooled sample covariance matrix and the class sample covariance matrix. To further

improve the accuracy of the estimation of the class covariance matrix and to stabilize its inverse, Friedman

(1989) also included a regularization component by shrinking the covariance matrix estimator towards the

Preprint submitted to Elsevier

February 7, 2017

identity matrix, which yields a nonsingular estimator following the well-known ridge-regression approach of

Hoerl and Kennard (1970). Despite its popularity, the “borrowing” operation employed in the RDA classi-

ﬁer lacks interpretability (Bensmail and Celeux, 1996). Furthermore, the RDA classiﬁer is impractical for

high-dimensional data sets because it computes the inverse and determinant of the covariance matrices for

each class. Both matrix calculations are computationally expensive because the number of operations grows

at a polynomial rate in the number of features. Moreover, the model selection of the RDA classiﬁer’s two

tuning parameters is computationally burdensome because the matrix inverse and determinant of each class

covariance matrix are computed across multiple cross-validation folds for each candidate tuning-parameter

Here, we present the high-dimensional RDA (HDRDA) classiﬁer, which is intended for the case when

p > N . We reparameterize the RDA classiﬁer similar to that of Hastie, Tibshirani, and Friedman (2008)

and Halbe and Aladjem (2007) and employ a biased covariance-matrix estimator that partially pools the

individual sample covariance matrices from the QDA classiﬁer with the pooled sample covariance matrix from

the LDA classiﬁer. We then shrink the resulting covariance-matrix estimator towards a scaled identity matrix

to ensure positive deﬁniteness. We show that the pooling parameter in the HDRDA classiﬁer determines

the contribution of each training observation to the estimation of each class covariance matrix, enabling

interpretability that has been previously lacking with the RDA classiﬁer (Bensmail and Celeux, 1996). Our

parameterization diﬀers from that of Hastie, Tibshirani, and Friedman (2008) and that of Halbe and Aladjem

(2007) in that our formulation allows the ﬂexibility of various covariance-matrix estimators proposed in the

literature, including a variety of ridge-like estimators, such as the one proposed by Srivastava and Kubokawa

pair.

(2007).

Next, we establish that the matrix operations corresponding to the null space of the pooled sample

covariance matrix are redundant and can be discarded from the HDRDA decision rule without loss of

classiﬁcatory information when we apply reasoning similar to that of Ye and Wang (2006). As a result, we

achieve a substantial reduction in dimension such that the matrix operations used in the HDRDA classiﬁer are

computationally linear in the number of features. Furthermore, we demonstrate that the HDRDA decision

rule is invariant to adjustments to the approximately p − N zero eigenvalues, so that the decision rule in the

original feature space is equivalent to a decision rule in a lower dimension, such that matrix inverses and

determinants of relatively small matrices can be rapidly computed. Finally, we show that several shrinkage

methods that are special cases of the HDRDA classiﬁer have no eﬀect on the approximately p − N zero

eigenvalues of the covariance-matrix estimators when p > N . Such techniques include work from Srivastava

and Kubokawa (2007), Rao and Mitra (1971), and other methods studied by Ramey and Young (2013) and

Xu, Brock, and Parrish (2009).

We also provide an eﬃcient algorithm along with pseudocode to estimate the HDRDA classiﬁer’s tuning

parameters in a grid search via cross-validation. Timing comparisons between our HDRDA implementation

in the sparsediscrim R package available on CRAN and the standard RDA formulation in the klaR R

package demonstrate that as the number of features increases, the computational runtime of the HDRDA

classiﬁer is drastically smaller than that of RDA. In fact, when p = 5000, we show that the HDRDA classiﬁer

2

is 502.786 times faster on average than the RDA classiﬁer. In this scenario, the HDRDA classiﬁer’s model

selection requires 2.979 seconds on average, while that of the RDA classiﬁer requires 24.933 minutes on

average.

Finally, we study the classiﬁcation performance of the HDRDA classiﬁer on six real high-dimensional data

sets along with a simulation design that generalizes the experiments initially conducted by Guo et al. (2007).

We demonstrate that the HDRDA classiﬁer often attains superior classiﬁcation accuracy to several recent

classiﬁers designed for small-sample, high-dimensional data from Tong, Chen, and Zhao (2012), Witten and

Tibshirani (2011), Pang, Tong, and Zhao (2009), and Guo et al. (2007). We also include as a benchmark

the random forest from Breiman (2001) because Fern´andez-Delgado, Cernadas, Barro, and Amorim (2014)

have concluded that the random forest is often superior to other classiﬁers in benchmark studies. We show

that our proposed classiﬁer is competitive and often outperforms the random forest in terms of classiﬁcation

accuracy in the small-sample, high-dimensional setting.

The remainder of this paper is organized as follows. In Section 2 we introduce the classiﬁcation problem

and necessary notation to describe our contributions. In Section 3 we present the HDRDA classiﬁer along

with its interpretation. In Section 4, we provide properties of the HDRDA classiﬁer and a computationally

eﬃcient model-selection procedure. In Section 5, we compare the model-selection timings of the HDRDA

and RDA classiﬁers.

In Section 6 we describe our simulation studies of artiﬁcial and real data sets and

examine the experimental results. We conclude with a brief discussion in Section 7.

2. Preliminaries

2.1. Notation

To facilitate our discussion of covariance-matrix regularization and high-dimensional classiﬁcation, we
require the following notation. Let Ra×b denote the matrix space of all a × b matrices over the real ﬁeld
R. Denote by Im the m × m identity matrix, and let 0m×p be the m × p matrix of zeros, such that 0m
is understood to denote 0m×m. Deﬁne 1m ∈ Rm×1 as a vector of ones. Let AT , A+, and N (A) denote
the transpose, the Moore-Penrose pseudoinverse, and the null space of A ∈ Rm×p, respectively. Denote
by R>
p×p denote the cone of real p × p
positive-semideﬁnite matrices. Let V ⊥ denote the orthogonal complement of a vector space V ⊂ Rp×1. For
c ∈ R, let c+ = 1/c if c (cid:54)= 0 and 0 otherwise.

p×p the cone of real p × p positive-deﬁnite matrices. Similarly, let R≥

2.2. Discriminant Analysis

In discriminant analysis we wish to assign an unlabeled vector x ∈ Rp×1 to one of K unique, known
classes by constructing a classiﬁer from N training observations. Let xi = (xi1, . . . , xip) ∈ Rp×1 be the ith

observation (i = 1, . . . , N ) with true, unique membership yi ∈ {ω1, . . . , ωK}. Denote by nk the number of
training observations realized from class k, such that (cid:80)K
from a mixture distribution p(x) = (cid:80)K

k=1 nk = N . We assume that (xi, yi) is a realization
k=1 p(x|ωk)p(ωk), where p(x|ωk) is the probability density function
(PDF) of the kth class and p(ωk) is the prior probability of class membership of the kth class. We further

assume p(ωk) = p(ωl), 1 ≤ k, l ≤ K, k (cid:54)= l.

3

The QDA classiﬁer is the optimal Bayesian decision rule with respect to a 0 − 1 loss function when
p(x|ωk) is the PDF of the multivariate normal distribution with known mean vectors µk ∈ Rp×1 and known
covariance matrices Σk ∈ R>

p×p, k = 1, 2, . . . , K. Because µk and Σk are typically unknown, we assign an

unlabeled observation x to class ωk with the sample QDA classiﬁer

DQDA(x) = arg min

(x − ¯xk)T (cid:98)Σ−1

k (x − ¯xk) + log | (cid:98)Σk|,

k

where ¯xk and (cid:98)Σk are the maximum-likelihood estimators (MLEs) of µk and Σk, respectively. If we assume

further that Σk = Σ, k = 1, . . . , K, then the pooled sample covariance matrix (cid:98)Σ is substituted for (cid:98)Σk in

(1), where

(1)

(2)

(cid:98)Σ = N −1

nk (cid:98)Σk

K
(cid:88)

k=1

is the MLE for Σ. Here, (1) reduces to the sample LDA classiﬁer. We omit the log-determinant because it

is constant across the K classes.

The smallest eigenvalues of (cid:98)Σk and the directions associated with their eigenvectors can highly inﬂuence

the classiﬁer in (1).

In fact, the eigenvalues of (cid:98)Σk are well-known to be biased if p ≥ nk such that the

smallest eigenvalues are underestimated (Seber, 2004). Moreover, if p > nk, then rank( (cid:98)Σk) ≤ nk, which

implies that at least p − nk eigenvalues of (cid:98)Σk are zero. Furthermore, although more feature information is
available to discriminate among the K classes, if p > nk, (1) is incalculable because (cid:98)Σ−1

k does not exist.

Several regularization methods, such as the methods considered by Xu et al. (2009), Guo et al. (2007),

and Mkhadri (1995), have been proposed in the literature to adjust the eigenvalues of (cid:98)Σk so that (1) is
calculable and provides reduced variability for (cid:98)Σ−1
applies a shrinkage factor γ > 0, so that

k . A common form of the covariance-matrix regularization

(cid:98)Σk(γ) = (cid:98)Σk + γIp,

(3)

similar to a method employed in ridge regression (Hoerl and Kennard, 1970). Equation (3) eﬀectively shrinks

the sample covariance matrix (cid:98)Σk toward Ip, thereby increasing the eigenvalues of (cid:98)Σk by γ. Speciﬁcally,

the zero eigenvalues are replaced with γ, so that (3) is positive deﬁnite. For additional covariance-matrix

regularization methods, see Ramey and Young (2013), Xu et al. (2009), and Ye and Ji (2009).

3. High-Dimensional Regularized Discriminant Analysis

Here, we deﬁne the HDRDA classiﬁer by ﬁrst formulating the covariance-matrix estimator (cid:98)Σk(λ) and

demonstrating its clear interpretation as a linear combination of the crossproducts of the training observations

centered by their respective class sample means. We deﬁne the convex combination

(cid:98)Σk(λ) := (1 − λ) (cid:98)Σk + λ (cid:98)Σ,

k = 1, . . . , K,

(4)

4

where λ ∈ [0, 1] is the pooling parameter. By rewriting (4) in terms of the observations xi, i = 1, . . . , N ,

each centered by its class sample mean, we attain a clear interpretation of (cid:98)Σk(λ). That is,

(cid:98)Σk(λ) =

1 − λ +

(cid:98)Σk +

nk(cid:48) (cid:98)Σk(cid:48)

(cid:18)

(cid:19)

λnk
N

λ
N

K
(cid:88)

k(cid:48)=1
k(cid:48)(cid:54)=k

I(yi = k)xixT

i +

I(yi (cid:54)= k)xixT
i

λ
N

N
(cid:88)

i=1

(cid:18) 1 − λ
nk

+

λ
N

(cid:19) N
(cid:88)

i=1

cik(λ)xixT
i ,

=

=

N
(cid:88)

i=1

where cik(λ) = λN −1 + (1 − λ)n−1

k I(yi = k). From (5), we see that λ weights the contribution of each of
the N observations in estimating Σk from all K classes rather than using only the nk observations from a

single class. As a result, we can interpret (5) as a covariance-matrix estimator that borrows from (cid:98)Σ in (2)

to estimate Σk.

In Figure 1 we plot the contours of ﬁve multivariate normal populations for λ = 0 with unequal covariance

matrices. As λ approaches 1, the contours become more similar, resulting in identical contours for λ = 1. Be-

low, we show that the pooling operation is advantageous in increasing the rank of each (cid:98)Σk(λ) from rank( (cid:98)Σk)

to rank( (cid:98)Σ) for 0 < λ ≤ 1. Notice that if λ = 0, then the observations from the remaining K − 1 classes do

not contribute to the estimation of Σk, corresponding to (cid:98)Σk. Furthermore, if λ = 1, the weights cik(λ) in (5)
reduce to 1/N , corresponding to (cid:98)Σ. For brevity, when λ = 1, we deﬁne X = [(cid:112)c1k(1)xT
N ]T
such that (cid:98)Σ = N −1X T X. Similarly, for λ = 0, we deﬁne Xk = [(cid:112)c1k(0)xT
N ]T such that
(cid:98)Σk = n−1

1 , . . . , (cid:112)cN k(0)xT

1 , . . . , (cid:112)cN k(1)xT

k X T

k Xk.

[Insert Figure 1 approximately here ]

As we have discussed above, several eigenvalue adjustment methods have been proposed that increase

eigenvalues (approximately) equal to 0. To further improve the estimation of Σk and to stabilize the esti-

mator’s inverse, we deﬁne the eigenvalue adjustment of (4) as

˜Σk := αk (cid:98)Σk(λ) + γIp,

where αk ≥ 0 and γ ≥ 0 is an eigenvalue-shrinkage constant. Thus, the pooling parameter λ controls

the amount of estimation information borrowed from (cid:98)Σ to estimate Σk, and the shrinkage parameter γ

determines the degree of eigenvalue shrinkage. The choice of αk allows for a ﬂexible formulation of covariance-

matrix estimators. For instance, if αk = 1, k = 1, . . . , K, then (6) resembles (3). Similarly, if αk = 1 − γ,

then (6) has a form comparable to the RDA classiﬁer from Friedman (1989). Substituting (6) into (1), we

deﬁne the HDRDA classiﬁer as

DHDRDA(x) = arg min

(x − ¯xk)T ˜Σ+

k (x − ¯xk) + log | ˜Σk|.

k

For γ > 0, ˜Σk is nonsingular such that ˜Σ−1
k in (7). If γ = 0, we explicitly set
k
| ˜Σk| equal to the product of the positive eigenvalues of ˜Σk. Following Friedman (1989), we select λ and γ

can be substituted for ˜Σ+

(5)

(6)

(7)

5

from a grid of candidate models via cross-validation (Hastie et al., 2008). We provide an implementation of

(7) in the hdrda function contained in the sparsediscrim R package, which is available on CRAN.

The choice of αk in (6) is one of convenience and allows the ﬂexibility of various covariance-matrix

estimators proposed in the literature. In practice, we generally are not interested in estimating αk because

the estimation of K additional tuning parameters via cross-validation is counterproductive to our goal of

computational eﬃciency. For appropriate values of αk, the HDRDA covariance-matrix estimator includes or

resembles a large family of estimators. Notice that if αk = 1 and λ = 1, (6) is equivalent to the standard

ridge-like covariance-matrix estimator in (3). Other estimators proposed in the literature can be obtained

when one selects γ accordingly. For instance, with γ = tr{ (cid:98)Σ}/ min(N, p), we obtain the estimator from

Srivastava and Kubokawa (2007).

When αk = 1 − γ, (6) resembles the biased covariance-matrix estimator

(cid:98)Σk(λ, γ) = (1 − γ) (cid:98)Σ(RDA)

k

(λ) + γ

(cid:111)

(λ)

tr

(cid:110)
(cid:98)Σ(RDA)
p

k

Ip

(8)

employed in the RDA classiﬁer, where (cid:98)Σ(RDA)
tion parameter that controls the shrinkage of (8) towards Ip weighted by the average of the eigenvalues of
(cid:98)Σ(RDA)
ﬁer is impractical for high-dimensional data because the inverse and determinant of (8) must be calculated

(λ). Despite the similarity of (8) to the HDRDA covariance-matrix estimator in (6), the RDA classi-

(λ) is a pooled estimator of Σk and γ ∈ [0, 1] is a regulariza-

k

k

when substituted into (1). Furthermore, (8) has no clear interpretation.

4. Properties of the HDRDA Classiﬁer

Next, we establish properties of the covariance-matrix estimator and the decision rule employed in the

HDRDA classiﬁer. By doing so, we demonstrate that (7) lends itself to a more eﬃcient calculation. We

decompose (7) into a sum of two components, where the ﬁrst summand consists of matrix operations applied

to low-dimensional matrices and the second summand corresponds to the null space of (cid:98)Σ in (2). We show that

the matrix operations performed on the null space of (cid:98)Σ yield constant quadratic forms across all classes and

can be omitted. For p (cid:29) N , the constant component involves determinants and inverses of high-dimensional

matrices, and by ignoring these calculations, we achieve a substantial reduction in computational costs.

Furthermore, a byproduct is that adjustments to the associated eigenvalues have no eﬀect on (7). Lastly, we

utilize the singular value decomposition to eﬃciently calculate the eigenvalue decomposition of (cid:98)Σ, further

reducing the computational costs of the HDRDA classiﬁer.

First, we require the following relationship regarding the null spaces of (cid:98)Σk(λ), (cid:98)Σ, and (cid:98)Σk.

Lemma 1. Let (cid:98)Σk and (cid:98)Σ be the MLEs of Σk and Σ, respectively. Let (cid:98)Σk(λ) be deﬁned as in (4). Then,

N { (cid:98)Σk(λ)} ⊂ N ( (cid:98)Σ) ⊂ N ( (cid:98)Σk), k = 1, . . . , K.

Proof. Let z ∈ N { (cid:98)Σk(λ)} for some k = 1, . . . , K. Hence, 0 = zT (cid:98)Σk(λ)z = (1 − λ)zT (cid:98)Σkz + λzT (cid:98)Σz.
Because (cid:98)Σk, (cid:98)Σ ∈ R≥
Now, suppose z ∈ N ( (cid:98)Σ). Similarly, we have that 0 = zT (cid:98)Σz = N −1 (cid:80)K
z ∈ N ( (cid:98)Σk) because (cid:98)Σk ∈ R≥

p×p, we have z ∈ N ( (cid:98)Σ) and z ∈ N ( (cid:98)Σk). In particular, we have that N { (cid:98)Σk(λ)} ⊂ N ( (cid:98)Σ).
k=1 nkzT (cid:98)Σkz, which implies that

p×p. Therefore, N ( (cid:98)Σ) ⊂ N ( (cid:98)Σk).

6

In Lemma 2 below, we derive an alternative expression for ˜Σk in terms of the matrix of eigenvectors of (cid:98)Σ.
p×p is the diagonal matrix of eigenvalues

Let (cid:98)Σ = U DU T be the eigendecomposition of (cid:98)Σ such that D ∈ R≥
of (cid:98)Σ with

D =





Dq

0

0

0p−q



 ,

Dq ∈ R>

q×q is the diagonal matrix consisting of the positive eigenvalues of (cid:98)Σ, the columns of U ∈ Rp×p are
the corresponding orthonormal eigenvectors of (cid:98)Σ, and rank( (cid:98)Σ) = q. Then, we partition U = (U1, U2) such
that U1 ∈ Rp×q and U2 ∈ Rp×(p−q).

Lemma 2. Let (cid:98)Σ = U DU T be the eigendecomposition of (cid:98)Σ as above, and suppose that rank( (cid:98)Σ) = q ≤ p.

Then, we have

where

Hence,

˜Σk = U





Wk

0

0

γIp−q


 U T ,

k = 1, . . . , K,

Wk = αk{(1 − λ)U T

1 (cid:98)ΣkU1 + λDq} + γIq.

(9)

(10)

Proof. From Lemma 1, the columns of U2 span the null space of (cid:98)Σk, which implies that (cid:98)ΣkU2 = 0p×(p−q).

U T (cid:98)ΣkU =





U T

1 (cid:98)ΣkU1

0

0

0p−q



 ,

k = 1, . . . , K.

Thus, U T ˜ΣkU = αk{(1 − λ)U T (cid:98)ΣkU + λD} + γIp, and (9) holds because U is orthogonal.

As an immediate consequence of Lemma 2, we have the following corollary.

Corollary 1. Let (cid:98)Σk(λ) be deﬁned as in (4). Then, for λ ∈ (0, 1], rank{ (cid:98)Σk(λ)} = q, k = 1, . . . , K.

Proof. The proof follows when we set γ = 0 in Lemma 2.

Thus, by incorporating each xi into the estimation of Σk, we increase the rank of (cid:98)Σk(λ) to q ≈ N if

λ (cid:54)= 0. Next, we provide an essential result that enables us to prove that (7) is invariant to adjustments to
the eigenvalues of ˜Σk corresponding to the null space of (cid:98)Σ.

Lemma 3. Let U2 be deﬁned as above. Then, for all x ∈ Rp×1, U T
where k (cid:54)= k(cid:48).

2 (x − ¯xk) = U T

2 (x − ¯xk(cid:48)), 1 ≤ k, k(cid:48) ≤ K,

2 ∈ C( (cid:98)Σ)⊥ (Kollo and von Rosen, 2005, Lemma 1.2.5). Now, because xi ∈ C( (cid:98)Σ) (i = 1, . . . , N ), U T

Proof. Let x ∈ Rp×1, and suppose that 1 ≤ k, k(cid:48) ≤ K. Recall that U2 ∈ N ( (cid:98)Σ), which implies that
U T
2 xi =
0p−q. Hence, 0p−q = (cid:80)N
2 ( ¯xk − ¯xk(cid:48)), where βi = (nknk(cid:48))−1{I(yi = k)nk(cid:48) − I(yi = k(cid:48))nk}.
Therefore, U T

2 xi = U T

i=1 βiU T

2 (x − ¯xk) = U T

2 (x − ¯xk(cid:48)).

7

We now present our main result, where we decompose (7) and show that the term requiring the largest

computational costs does not contribute to the classiﬁcation of an unlabeled observation performed using

Lemma 3. Hence, we reduce (7) to an equivalent, more computationally eﬃcient decision rule.

Theorem 1. Let ˜Σk and Wk be deﬁned as in (9) and (10), respectively, and let U1 be deﬁned as above.

Then, the decision rule in (7) is equivalent to

DHDRDA(x) = arg min

(x − ¯xk)T U1W −1

k U T

1 (x − ¯xk) + log |Wk|.

(11)

k

Proof. From (9), we have that





W −1
k

˜Σ+

k = U

0


 U T

0

γ+Ip−q

and | ˜Σk| = γp−q|Wk|, k = 1, . . . , K. Therefore, for all x ∈ Rp×1, we have that

(x − ¯xk)T ˜Σ+

k (x − ¯xk) + log | ˜Σk| = (x − ¯xk)T U1W −1
+ γ+(x − ¯xk)T U2U T

k U T

1 (x − ¯xk)

2 (x − ¯xk) + log |Wk|

+ (p − q) log γ.

Because γ is constant for k = 1, . . . , K, we can omit the (p − q) log γ term and particularly avoid the

calculation of log 0 for γ = 0. Then, the proof follows from Lemma 3 because U T

2 (x − ¯xk) is constant for

k = 1, . . . , K.

Using Theorem 1, we can avoid the time-consuming inverses and determinants of p×p covariance matrices
in (7) and instead calculate these same operations on Wk ∈ Rq×q in (11). The substantial computational

improvements arise because our proposed classiﬁer in (7) is invariant to the term U2, thus yielding an

equivalent classiﬁer in (11) with a substantial reduction in computational complexity. Here, we demonstrate

that the computational eﬃciency in calculating the inverse and determinant of Wk can be further improved

via standard matrix operations when we show that the inverses and determinants of Wk can be performed

on matrices of size nk × nk.

Proposition 1. Let Wk be deﬁned as above. Then, |Wk| = |Γk||Qk| and

W −1

k = Γ−1

k − n−1

k αk(1 − λ)Γ−1

k U T

1 X T

k Q−1

k XkU1Γ−1
k ,

where

and

Qk = Ink + n−1

k αk(1 − λ)XkU1Γ−1

k U T

1 X T
k

Γk = αkλDq + γIq.

8

(12)

(13)

(14)

k αk(1 − λ)U T

Proof. First, we write Wk = n−1
1 X T
from Harville (2008), which states that |A + BT C| = |A||T ||T −1 + CA−1B|, where A ∈ R>

k XkU1 + Γk. To calculate |Wk|, we apply Theorem 18.1.1
a×a, B ∈ Ra×b,
k , T = Ink , and C = XkU1, we have
|Wk| = |Γk||Qk|. Similarly, (12) follows from the well-known Sherman-Woodbury formula (Harville, 2008,

b×b, and C ∈ Rb×a. Thus, setting A = Γk, B = αk(1 − λ)U T

T ∈ R>

1 X T

Theorem 18.2.8) because (A + BT C)−1 = A−1 − A−1B(T −1 + CA−1B)−1CA−1.

Notice that Γk is singular when (λ, γ) = (0, 0) because Γk = 0q, in which case we use the formulation in

(11) instead. Also, notice that if αk is constant across the K classes, then Γk in (14) is independent of k.

Consequently, |Γk| is constant across the K classes and need not be calculated in (11).

4.1. Model Selection

Thus far, we have presented the HDRDA classiﬁer and its properties that facilitate an eﬃcient calculation

of the decision rule. Here, we describe an eﬃcient model-selection procedure along with pseudocode in

Algorithm 1 to select the optimal tuning-parameter estimates from the Cartesian product of candidate

values {λg}G

g=1 × {γh}H

h=1. We estimate the V -fold cross-validation error rate for each candidate pair and
select ((cid:98)λ, (cid:98)γ), which attains the minimum error rate. To calculate the V -fold cross-validation, we partition
the original training data into V mutually exclusive and exhaustive folds that have approximately the same

number of observations. Then, for v = 1, . . . , V , we classify the observations in the vth fold by training

a classiﬁer on the remaining V − 1 folds. We calculate the cross-validation error as the proportion of

misclassiﬁed observations across the V folds.

A primary contributing factor to the eﬃciency of Algorithm 1 is our usage of the compact singular

value decomposition (SVD). Rather than computing the eigenvalue decomposition of (cid:98)Σ to obtain U1, we

instead obtain U1 by computing the eigendecomposition of a much smaller N × N matrix when p (cid:29) N
(Hastie et al., 2008, Chapter 18.3.5). Applying the SVD, we decompose Xc = M ∆U T , where M ∈ RN ×p
is orthogonal, ∆ ∈ R≥
p×p is a diagonal matrix consisting of the singular values of Xc, and U ∈ Rp×p is
c Xc, we have the eigendecomposition (cid:98)Σ = U DU T , where U is the
orthogonal. Recalling that (cid:98)Σ = N −1X T
matrix of eigenvectors of (cid:98)Σ and D = N −1∆ is the diagonal matrix of eigenvalues of (cid:98)Σ. Now, we can obtain
c = M DM T . Next, we
M and D eﬃciently from the eigenvalue decomposition of the N × N matrix XcX T
compute U = X T

c M D+/2, where

D+/2 =





D−1/2
q

0

0

0N −q



 .

We then determine q, the number of numerically nonzero eigenvalues present in D, by calculating the number

of eigenvalues that exceeds some tolerance value, say, 1 × 10−6. We then extract U1 as the ﬁrst q columns

of U .

As a result of the compact SVD, we need calculate XcU1 only once per cross-validation fold, requiring

O(pqN ) ≈ O(pN 2) calculations. Hence, the computational costs of expensive calculations, such as matrix

inverses and determinants, are greatly reduced because they are performed in the q-dimensional subspace.

Similarly, we reduce the dimension of the test data set by calculating XtestU1 once per fold. Conveniently,

9

input : Data matrix X

Parameter grid {λg}G

g=1 × {γh}H

h=1

output: Optimal Estimates (ˆλ, ˆγ)

for v ← 1 to V do

Partition X into Xtrain ∈ RN ×p and Xtest ∈ RNT ×p
for k ← 1 to K do

Extract Xk ∈ Rnk×p from Xtrain
Compute sample mean ¯xk from Xk
Center Xk ← Xk − 1nk ¯xT
k

end

end

1 , . . . , X T

Xc ← [X T
Compute the compact SVD Xc = MqDqU T
1

K]T

Transform Xc ← XcU1

Transform Xtest ← XtestU1

for k ← 1 to K do

Extract Xk ∈ Rnk×q from Xc
Recompute sample mean ¯xk from Xk

for (λ, γ) ∈ {λg}G

g=1 × {γh}H

h=1 do

for k ← 1 to K do

Compute Qk using (13)

Compute Γk using (14)
Compute W −1

using (12)

k

Compute |Wk| = |Γk||Qk|
Compute (x − ¯xk)T U1W −1

k U T

1 (x − ¯xk) + log |Wk| for each row x of Xtest

end

end

Classify test observations Xtest using (11)

Compute the number of misclassiﬁed test observations #{Errorv(λ, γ)}

end
Compute (cid:92)Error(λ, γ) = N −1 (cid:80)V
Report optimal (ˆλ, ˆγ) ← arg min(λ,γ)

v=1 #{Errorv(λ, γ)}
(cid:92)Error(λ, γ)

Algorithm 1: Model selection for the HDRDA classiﬁer

10

we see that the most costly computation involved in Qk and W −1
is XkU1, which can be extracted from
XcU1. Thus, after the initial calculation of XcU1 per cross-validation fold, Qk requires O(nkq2) operations.
Because Qk ∈ Rnk×nk , both its determinant and inverse require O(n3
k) operations. Consequently, W −1
k ∈ Rq×q requires O(q) operations.
requires O(nkq2) operations. Also, the inverse of the diagonal matrix Γ−1
Finally, we remark that |Wk| requires O(n3

k) operations.

k

k

The expressions given in Proposition 1 also expedite the selection of λ and γ via cross-validation

because the most time-consuming matrix operation involved in computing W −1
Rnk×q, which is independent of λ and γ. The subsequent operations in calculating W −1
be simply updated for diﬀerent pairs of λ and γ without repeating the costly computations. Also, rather
than calculating (x − ¯xk)T U1W −1
k)(cid:48)U1W −1
(Xtest − ¯xk1(cid:48)

1 (x − ¯xk) individually for each row x of Xtest, we can calculate
k). The diagonal elements of the resulting matrix contain the indi-

k U T
1 (Xtest − ¯xk1(cid:48)

and |Wk| is XkU1 ∈

and |Wk| can

k

k

k U T
vidual quadratic form of each test observation, xt.

5. Timing Comparisons between RDA and HDRDA

In this section, we demonstrate that the computational performance of the model selection employed in

the HDRDA classiﬁer is substantially faster than that of the RDA classiﬁer on small-sample, high-dimensional

data sets. The relative diﬀerence in runtime between the two classiﬁers drastically increases as p increases. To

compare the two classiﬁers, we generated 25 observations from each of K = 4 multivariate normal populations

with mean vectors µ1 = −3 · 1p, µ2 = −1p, µ3 = 1p, and µ4 = 3 · 1p. We set the covariance matrix of

each population to the p × p identity matrix. For each data set generated, we estimated the parameters λ

and γ for both classiﬁers using a grid of 5 equidistant candidate values between 0 and 1, inclusively. We

set αk = 1 − γ, k = 1, . . . , K, in the HDRDA classiﬁer. At each pair of λ and γ, we computed the 10-fold

cross-validation error rate (Hastie et al., 2008). Then, we selected the model that minimized the 10-fold

cross-validation error rate.

We compared the runtime of both classiﬁers by increasing the number of features from p = 500 to p = 5000

in increments of 500. Next, we generated 100 data sets for each value of p and computed the training and

model selection runtime of both classiﬁers. Our timing comparisons are based on our HDRDA implementation

in the sparsediscrim R package and the standard RDA implementation in the klaR R package. All timing

comparisons were conducted on an Amazon Elastic Compute Cloud (EC2) c4.4xlarge instance using version

3.3.1 of the open-source statistical software R. Our timing comparisons can be reproduced with the code

available at https://github.com/ramhiser/paper-hdrda.

5.1. Timing Comparison Results

In Figure 2, we plotted the runtime of the model selections for both the HDRDA and RDA classiﬁers as

a function of p. We observed that the HDRDA classiﬁer was substantially faster than the RDA classiﬁer as p

increased. In the left panel of Figure 2, we ﬁt a quadratic regression line to the RDA runtimes and a simple

linear regression model to the HDRDA runtimes. For improved understanding, in the right panel we repeated

the same scatterplot and linear ﬁt with the timings restricted to the observed range of the HDRDA timings.

11

Figure 2 suggests that the usage of a matrix inverse and determinant in the klaR R package’s discriminant

function yielded model-selection timings that exceeded linear growth in p. Because the HDRDA classiﬁer

removes inverse and determinants, it was computationally more eﬃcient than the RDA classiﬁer, especially

as p increased. In fact, when p = 5000, the RDA classiﬁer required 24.933 minutes on average to perform

model selection, while the HDRDA classiﬁer selected its optimal model in 2.979 seconds on average. Clearly,

the model selection employed by the HDRDA classiﬁer is substantially faster than that of the RDA classiﬁer.

We quantiﬁed the relative timing comparisons between the two classiﬁers by calculating the ratio of

mean timings of the RDA classiﬁer to the HDRDA classiﬁer for each value of p. We employed nonparametric

bootstrapping to estimate the mean ratio along with 95% conﬁdence intervals. In Figure 3, the bootstrap

sampling distributions for the ratio of mean timings are given. First, we observe that the mean relative

timings increased as p increased. For smaller dimensions, the relative diﬀerence in computing was sizable

with the average ratio of the mean timings equal to 14.513 for p = 500 and a 95% conﬁdence interval

of (14.191, 14.855). Furthermore, the ratio of mean computing times suggested that the RDA classiﬁer is

impractical for higher dimensions. For instance, when p = 5000, the ratio of mean computing times increased

to 502.786 with a 95% conﬁdence interval of (462.863, 546.396).

[Insert Figure 2 approximately here ]

[Insert Figure 3 approximately here ]

6. Classiﬁcation Study

In this section, we compare our proposed classiﬁer with four classiﬁers recently proposed for small-sample,

high-dimensional data along with the random-forest classiﬁer from Breiman (2001) using version 3.3.1 of the

open-source statistical software R. Within our study, we included penalized linear discriminant analysis from

Witten and Tibshirani (2011), implemented in the penalizedLDA package. We also considered shrunken

centroids regularized discriminant analysis from Guo et al. (2007) in the rda package. Because the rda

package does not perform the authors’ “Min-Min” rule automatically, we applied this rule within our R code.

We included two modiﬁcations of diagonal linear discriminant analysis from Tong et al. (2012) and Pang

et al. (2009), where the former employs an improved mean estimator and the latter utilizes an improved

variance estimator. Both classiﬁers are available in the sparsediscrim package. Finally, we incorporated the

random forest as a benchmark based on the ﬁndings of Fern´andez-Delgado et al. (2014), who concluded that

the random forest is often superior to other classiﬁers in benchmark studies. We used the implementation

of the random-forest classiﬁer from the randomForest package with 250 trees and 100 maximum nodes. For

each classifer we explicitly set prior probabilities as equal, if applicable. All other classiﬁer options were set

to their default settings. Below, we refer to each classiﬁer by the ﬁrst author’s surname. All simulations

were conducted on an Amazon EC2 c4.4xlarge instance. Our analyses can be reproduced via the code

available at https://github.com/ramhiser/paper-hdrda.

12

For the HDRDA classiﬁer in (11), we examined the classiﬁcation performance of two models. For the

ﬁrst HDRDA model, we set αk = 1, k = 1, . . . , K, so that the covariance-matrix estimator (6) resembled

(3). We estimated λ from a grid of 21 equidistant candidate values between 0 and 1, inclusively. Similarly,

we estimated γ from a grid consisting of the values 10−1, . . . , 104, and 105. We selected optimal estimates of

λ and γ using 10-fold cross-validation. For the second model, we set αk = 1 − γ, k = 1, . . . , K, to resemble

Friedman’s parameterization, and we estimated both λ and γ from a grid of 21 equidistant candidate values

between 0 and 1, inclusively.

We did not include the RDA classiﬁer in our classiﬁcation study because its training runtime was pro-

hibitively slow on high-dimensional data in our preliminary experiments. As shown in Section 5, the runtime

of the RDA classiﬁer was drastically larger than that of the HDRDA classiﬁer for a tuning grid of size

25 = 5 × 5. Consequently, a fair comparison between the RDA and HDRDA classiﬁers would require model

selection of 441 = 21 × 21 diﬀerent pairs of tuning parameters in the RDA classiﬁer. A tuning grid of this

size yielded excessively slow training runtimes for the RDA implementation from the klaR R package.

6.1. Simulation Study

In this section we compare the competing classiﬁers using the simulation design from Guo et al. (2007).

This design is widely used within the high-dimensional classiﬁcation literature, including the studies by

Ramey and Young (2013) and Witten and Tibshirani (2011). First, we consider the block-diagonal covariance

matrix from Guo et al. (2007),

Σk =


















0100
...
...
· · ·

Σ(ρk)

0100

0100 Σ(−ρk)

0100

0100

0100 Σ(ρk)

· · ·

0100

0100

· · ·

· · ·

· · ·

0100
...
· · ·

0100 Σ(−ρk) 0100
...
. . .
· · ·

0100

· · ·

· · ·


















· · ·
...
...
...
...
· · ·

,

(15)

where the (i, j)th entry of the block matrix Σ(ρk) ∈ R100×100 is

Σ(ρk)

ij = {ρ|i−j|

k

}1≤i,j≤100.

The block-diagonal covariance structure in (15) resembles gene-expression data: within each block of path-

ways, genes are correlated, and the correlation decays as a function of the distance between any two genes.

The original design from Guo et al. (2007) comprised two p-dimensional multivariate normal populations

with a common block-diagonal covariance matrix.

Although the design is indeed standard, the simulation conﬁguration lacks artifacts commonly observed

in real data, such as skewness and extreme outliers. As a result, we wished to investigate the eﬀect of outliers

on the high-dimensional classiﬁers. To accomplish this goal, we generalized the block-diagonal simulation

conﬁguration by sampling from a p-dimensional multivariate contaminated normal distribution. Denoting

the PDF of the p-dimensional multivariate normal distribution by Np(x|µ, Σ), we write the PDF of the kth

13

class as

0.05, . . ., 0.50.

p(x|ωk) = (1 − (cid:15))Np(x|µk, Σk) + (cid:15)Np(x|µk, ηΣk),

(16)

where (cid:15) ∈ [0, 1] is the probability that an observation is contaminated (i.e., drawn from a distribution with

larger variance) and η > 1 scales the covariance matrix Σk to increase the extremity of outliers. For (cid:15) = 0,

we have the benchmark block-diagonal simulation design from Guo et al. (2007). As (cid:15) is increased, the

average number of outliers is increased. In our simulation, we let η = 100 and considered the values of (cid:15) = 0,

We generated K = 3 populations from (16) with Σk given in (15) and set the mean vector of class 1 to

µ1 = 0p. Next, comparable to Guo et al. (2007), the ﬁrst 100 features of µ2 were set to 1/2, while the rest

). For simplicity, we deﬁned µ3 = −µ2. The three populations

diﬀered in their mean vectors in the ﬁrst 100 features corresponding to the ﬁrst block, and no diﬀerence in

were set to 0, i.e., µ2 = (1/2, . . . , 1/2
(cid:125)

(cid:124)

(cid:123)(cid:122)
100

, 0, . . . , 0
(cid:124) (cid:123)(cid:122) (cid:125)
p−100

the means occurred in the remaining blocks.

From each of the K = 3 populations, we sampled 25 training observations (nk = 25 for all k) and 10,000

test observations. After training each classiﬁer on the training data, we classiﬁed the test data sets and

computed the proportion of mislabeled test observations to estimate the classiﬁcation error rate for each

classiﬁer. Repeating this process 500 times, we computed the average of the error-rate estimates for each

classiﬁer. We allowed the number of features to vary from p = 100 to p = 500 in increments of 100 to examine

the classiﬁcation accuracy as the feature dimension increased while maintaining a small sample size. Guo

et al. (2007) originally considered ρk = 0.9 for all k. Alternatively, to explore the more realistic assumption

of unequal covariance matrices, we put ρ1 = 0.1, ρ2 = 0.5, and ρ3 = 0.9.

6.1.1. Simulation Results

In Figure 4, we observed each classiﬁer’s average classiﬁcation error rates for the values of (cid:15) and p.

Unsurprisingly, the average error rate increased for each classiﬁer as the contamination probability (cid:15) increased

regardless of the value of p. Sensitivity to the presence of outliers was most apparent for the Pang, Tong, and

Witten classiﬁers. For smaller dimensions, the random-forest and HDRDA classiﬁers tended to outperform

the remaining classiﬁers with the random forest performing best. As the feature dimension increased with

p ≥ 300, both HDRDA classiﬁers outperformed all other classiﬁers, suggesting that their inherent dimension

reduction better captured the classiﬁcatory information in the small training samples, even in the presence

of outliers.

[Insert Figure 4 approximately here ]

The Pang, Tong, and Witten methods yielded practically the same and consistently the worst error rates

when outliers were present with (cid:15) > 0, suggesting that these classiﬁers were sensitive to outliers. Notice, for

example, that when p = 400, the error rates of the Pang, Tong, and Witten classiﬁers increased dramatically

from approximately 19% when no outliers were present to approximately 43% when (cid:15) = 0.05. The sharp

increase in average error rates for these three classiﬁers continued as (cid:15) increased. Guo’s method always

14

outperformed those of Pang, Witten, and Tong, but after outliers were introduced, the Guo classiﬁer’s

average error rate was not competitive with the HDRDA classiﬁers or the random-forest classiﬁer.

[Insert Figure 5 approximately here ]

In Figure 5, we again examine the simulation results as a function of p for a subset of the values of (cid:15).

This set of plots allows us to investigate the eﬀect of feature dimensionality on classiﬁcation performance.

When no outliers were present (i.e., (cid:15) = 0), the random-forest classiﬁer was outperformed by all other

classiﬁers. Furthermore, the HDRDA classiﬁers were superior in terms of average error rate in this setting.

As p increased, an elevation in average error rate was expected for all classiﬁers, but the increase was not

observed to be substantial.

For (cid:15) > 0, we observed a diﬀerent behavior in classiﬁcation performance. First, the Pang, Tong, and

Witten methods, along with the random-forest method, increased in average error rate as p increased.

Contrarily, the performance of the HDRDA and Guo classiﬁers was hardly aﬀected by p. Also, as discussed

above, the HDRDA classiﬁers were superior to all other classiﬁers for large values of p with only the random-

forest classiﬁer outperforming them in smaller feature-dimension cases.

6.2. Application to Gene Expression Data

We compared the HDRDA classiﬁer to the ﬁve competing classiﬁers on six benchmark gene-expression

microarray data sets. First, we evaluated the classiﬁcation accuracy of each classiﬁer by randomly parti-

tioning the data set under consideration such that 2/3 of the observations were allocated as training data

and the remaining 1/3 of the observations were allocated as a test data set. To expedite the computational

runtime, we reduced the training data to the top 1000 variables by employing the variable-selection method

proposed by Dudoit et al. (2002). We then reduced the test data set to the same 1000 variables. After

training each classiﬁer on the training data, we classiﬁed the test data sets and computed the proportion

of mislabeled test observations to estimate the classiﬁcation error rate for each classiﬁer. Repeating this

process 100 times, we computed the average of the error-rate estimates for each classiﬁer. We next provide

a concise description of each high-dimensional data set examined in our classiﬁcation study.

6.2.1. Chiaretti et al. (2004) Data Set

Chiaretti et al. (2004) measured the gene-expression proﬁles for 128 individuals with acute lymphoblastic

leukemia (ALL) using Aﬀymetrix human 95Av2 arrays. Following Xu et al. (2009), we restricted the data

set to K = 2 classes such that n1 = 74 observations were without cytogenetic abnormalities and n2 = 37

observations had a detected BCR/ABL gene. The robust multichip average normalization method was

applied to all 12,625 gene-expression levels.

6.2.2. Chowdary et al. (2006) Data Set

Chowdary et al. (2006) investigated 52 matched pairs of tissues from colon and breast tumors using

Aﬀymetrix U133A arrays and ribonucleic-acid (RNA) ampliﬁcation. Each tissue pair was gathered from

the same patient and consisted of a snap-frozen tissue and a tissue suspended in an RNAlater preservative.

15

Overall, 31 breast-cancer and 21 colon-cancer pairs were gathered, resulting in K = 2 classes with n1 = 62

and n2 = 42. A purpose of the study was to determine whether the disease state could be identiﬁed using

22,283 gene-expression proﬁles.

6.2.3. Nakayama et al. (2007) Data Set

Nakayama et al. (2007) acquired 105 gene-expression samples of 10 types of soft-tissue tumors through an

oligonucleotide microarray, including 16 samples of synovial sarcoma (SS), 19 samples of myxoid/round cell

liposarcoma (MLS), 3 samples of lipoma, 3 samples of well-diﬀerentiated liposarcoma (WDLS), 15 samples of

dediﬀerentiated liposarcoma (DDLS), 15 samples of myxoﬁbrosarcoma (MFS), 6 samples of leiomyosarcoma

(LMS), 3 samples of malignant nerve sheathe tumor (MPNST), 4 samples of ﬁbrosarcoma (FS), and 21

samples of malignant ﬁbrous histiocytoma (MFH). Nakayama et al. (2007) determined from their data that

these 10 types fell into 4 broader groups: (1) SS; (2) MLS; (3) Lipoma, WDLS, and part of DDLS; (4) Spindle

cell and pleomorophic sarcomas including DDLS, MFS, LMS, MPNST, FS, and MFH. Following Witten and

Tibshirani (2011), we restricted our analysis to the ﬁve tumor types having at least 15 observations.

6.2.4. Shipp et al. (2002) Data Set

According to Shipp et al. (2002), approximately 30%-40% of adult non-Hodgkin lymphomas are diﬀuse

large B-cell lymphomas (DLBCLs). However, only a small proportion of DLBCL patients are cured with

modern chemotherapeutic regimens. Several models have been proposed, such as the International Prognostic

Index (IPI), to determine a patient’s curability. These models rely on clinical covariates, such as age, to

determine if the patient can be cured, and the models are often ineﬀective. Shipp et al. (2002) have argued

that researchers need more eﬀective means to determine a patient’s curability. The authors measured 6,817

gene-expression levels from 58 DLBCL patient samples with customized cDNA (lymphochip) microarrays to

investigate the curability of patients treated with cyclophosphamide, adriamycin, vincristine, and prednisone

(CHOP)-based chemotherapy. Among the 58 DLBCL patient samples, 32 are from cured patients while 26

are from patients with fatal or refractory disease.

6.2.5. Singh et al. (2002) Data Set

Singh et al. (2002) have examined 235 radical prostatectomy specimens from surgery patients between

1995 and 1997. The authors used oligonucleotide microarrays containing probes for approximately 12,600

genes and expressed sequence tags. They have reported that 102 of the radical prostatectomy specimens are

of high quality: 52 prostate tumor samples and 50 non-tumor prostate samples.

6.2.6. Tian et al. (2003) Data Set

Tian et al. (2003) investigated the puriﬁed plasma cells from the bone marrow of control patients along

with patients with newly diagnosed multiple myeloma. Expression proﬁles for 12,2625 genes were obtained via

Aﬀymetrix U95Av2 microarrays. The plasma cells were subjected to biochemical and immunohistochemical

analyses to identify molecular determinants of osteolytic lesions. For 36 multiple-myloma patients, focal

bone lesions could not be detected by magnetic resonance imaging (MRI), whereas MRI was used to detect

such lesions in 137 patients.

16

6.2.7. Classiﬁcation Results

Similar to Witten and Tibshirani (2011), we report the average test error rates obtained over 100 random

training-test partitions in Table 1 along with standard deviations of the test error rates in parentheses. The

HDRDA and Guo classiﬁers were superior in classiﬁcation performance for the majority of the simulations.

The HDRDA classiﬁers yielded the best classiﬁcation accuracy on the Chowdary and Shipp data sets. Al-

though the random forest’s accuracy slightly exceeded the HDRDA classiﬁers on the Tian data set, our

proposed classiﬁers outperformed the other competing classiﬁers considered here. Moreover, the HDRDA

classiﬁers yielded comparable performance on ﬁve of the six data sets.

[Insert Table 1 approximately here ]

The average error-rate estimates for the Pang, Tong, and Witten classiﬁers were comparable across all six

data sets. Furthermore, the average error rates for the Pang and Tong classiﬁers were approximately equal

for all data sets except for the Chiaretti dataset. This result suggests that the mean and variance estimators

used in lieu of the MLEs provided little improvement to classiﬁcation accuracies. However, we investigated

the Pang classiﬁer’s poor performance on the Chiaretti data set and determined that its variance estimator

exhibited numerical instability. The classiﬁer’s denominator was approximately zero for both classes and led

to the poor classiﬁcation performance.

The random-forest classiﬁer was competitive when applied to the Chowdary and Singh data sets and

yielded the smallest error rate of the considered classiﬁers on the Tian data set. The fact that the HDRDA

and Guo classiﬁers typically outperformed the random-forest classiﬁer challenges the claim of Fern´andez-

Delgado et al. (2014) that random forests are typically superior. Further studies should be performed to

validate this statement in the small-sample, high-dimensional setting.

Finally, the Pang, Tong, and Witten classiﬁers consistently yielded the largest average error rates across

the six data sets. Given that the standard deviations were relatively large, we hesitate to generalize claims

regarding the ranking of these three classiﬁers in terms of the average error rate. However, the classiﬁers’

error rates and their variability across multiple random partitions of each data set were large enough that

we might question their beneﬁt when applied to real data.

7. Discussion

We have demonstrated that our proposed HDRDA classiﬁer is competitive with and often superior to ran-

dom forests as well as the Witten, Pang, Tong, and Guo classiﬁers. In fact, we have shown that the HDRDA

classiﬁer often yields superior classiﬁcation accuracy when applied to small-sample, high-dimensional data

sets, conﬁrming the assertions of Mai et al. (2012) and Fan et al. (2012) that diagonal classiﬁers often yield

inferior classiﬁcation performance when compared to other classiﬁcation methods. Furthermore, we have

demonstrated that HDRDA classiﬁers are more robust to the presence of outliers than the diagonal clas-

siﬁers despite their rapid computational performance and their reduction in the number of parameters to

estimate.

17

We also considered the popular penalized linear discriminant analysis from Witten and Tibshirani (2011)

because it was speciﬁcally designed for high-dimensional gene-expression data. We had expected its clas-

siﬁcation performance to be competitive within our classiﬁcation study and perhaps superior. Contrarily,

our empirical studies suggest that the classiﬁer is sensitive to outliers and unable to achieve comparable

results with other classiﬁers designed for small-sample, high-dimensional data. Also, despite the claims of

Fern´andez-Delgado et al. (2014) that random forests are typically superior to other classiﬁers, we observed

that they were indeed competitive but were typically outperformed by classiﬁers developed for small-sample,

high-dimensional data.

We demonstrated that our HDRDA implementation in the sparsediscrim R package can be used in

practice with high-dimensional data sets. In our timing comparisons, we showed that HDRDA model selection

could be employed on data sets with p = 5000 in 2.979 seconds on average. Contrarily, the RDA classiﬁer

implemented in the klaR R package required 24.933 minutes on average to perform model selection on data

sets with p = 5000. Given that the RDA classiﬁer has been shown to have excellent performance in the

high-dimensional setting (Webb and Copsey, 2011) but is limited by its computationally intense model-

selection procedure, our work replaces the RDA classiﬁer for high-dimensional data in practice. This result

is reassuring because the RDA classiﬁer remains widely popular in the literature. In fact, variants of the

RDA classiﬁer have been applied to microarray data (Ching et al., 2012; Li and Wu, 2012; Tai and Pan,

2007; Guo et al., 2007), facial recognition (Zhang et al., 2010; Dai and Yuen, 2007; Lu et al., 2005; Pima and

Aladjem, 2004; Lu and Plataniotis, 2003), handwritten digit recognition (Bouveyron et al., 2007), remote

sensing (Tadjudin and Landgrebe, 1999), seismic detection (Anderson, 2002), and chemical spectra (Wu

et al., 1996; Aeberhard et al., 1993).

The dimension reduction employed in this paper has reduced the dimension to rank( (cid:98)Σ) = q. An inter-

esting extension of our work would reduce the dimension q further to a lower dimension qL < q, perhaps

using a criterion similar to that of principal components analysis. While unclear whether the classiﬁcation

performance would improve via such a method, the eﬃciency of the model selection would certainly improve.

Moreover, if qL = 2 or 3, low-dimensional graphical displays of high-dimensional data could be obtained.

We thank Mrs. Joy Young for her numerous recommendations that enhanced the quality of our writing.

References

Aeberhard, S., Coomans, D., Vel, O. D., 1993. Improvements to the classiﬁcation performance of RDA.

Journal of Chemometrics 7 (2), 99–115.

Anderson, D. N., Aug. 2002. Application of Regularized Discrimination Analysis to Regional Seismic Event

Identiﬁcation. Bulletin of the Seismological Society of America 92 (6), 2391–2399.

Bensmail, H., Celeux, G., Dec. 1996. Regularized Gaussian Discriminant Analysis through Eigenvalue De-

composition. Journal of the American Statistical Association 91 (436), 1743–1748.

Bouveyron, C., Girard, S., Schmid, C., Oct. 2007. High-Dimensional Discriminant Analysis. Communications

in Statistics - Theory and Methods 36 (14), 2607–2623.

18

Breiman, L., 2001. Random Forests - Springer. Machine Learning 45 (1), 5–32.

Chiaretti, S., Li, X., Gentleman, R., Vitale, A., Vignetti, M., Mandelli, F., Ritz, J., Foa, R., 2004. Gene

expression proﬁle of adult T-cell acute lymphocytic leukemia identiﬁes distinct subsets of patients with

diﬀerent response to therapy and survival. Blood 103 (7), 2771–2778.

Ching, W.-K., Chu, D., Liao, L.-Z., Wang, X., Jul. 2012. Regularized orthogonal linear discriminant analysis.

Pattern Recognition 45 (7), 2719–2732.

Chowdary, D., Lathrop, J., Skelton, J., Curtin, K., Briggs, T., Zhang, Y., Yu, J., Wang, Y., Mazumder,

A., Feb. 2006. Prognostic Gene Expression Signatures Can Be Measured in Tissues Collected in RNAlater

Preservative. The Journal of Molecular Diagnostics 8 (1), 31–39.

Dai, D.-Q., Yuen, P. C., Aug. 2007. Face recognition by regularized discriminant analysis. IEEE transactions

on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and

Cybernetics Society 37 (4), 1080–1085.

Dudoit, S., Fridlyand, J., Speed, T. P., Mar. 2002. Comparison of Discrimination Methods for the Classiﬁ-

cation of Tumors Using Gene Expression Data. Journal of the American Statistical Association 97 (457),

Fan, J., Feng, Y., Tong, X., Apr. 2012. A road to classiﬁcation in high dimensional space: the regularized

optimal aﬃne discriminant. Journal of the Royal Statistical Society: Series B (Statistical Methodology)

Fern´andez-Delgado, M., Cernadas, E., Barro, S., Amorim, D., Jan. 2014. Do we need hundreds of classiﬁers

to solve real world classiﬁcation problems? The Journal of Machine Learning Research 15 (1), 3133–3181.

Friedman, J. H., 1989. Regularized Discriminant Analysis. Journal of the American Statistical Association

Guo, Y., Hastie, T., Tibshirani, R., Jan. 2007. Regularized linear discriminant analysis and its application

in microarrays. Biostatistics 8 (1), 86–100.

Halbe, Z., Aladjem, M., Nov. 2007. Regularized mixture discriminant analysis. Pattern Recognition Letters

Harville, D. A., 2008. Matrix Algebra from a Statistician’s Perspective. Springer, New York.

Hastie, T., Tibshirani, R., Friedman, J., Dec. 2008. The Elements of Statistical Learning, 2nd Edition. Data

Mining, Inference, and Prediction. Springer New York, New York, NY.

Hoerl, A. E., Kennard, R. W., Feb. 1970. Ridge Regression: Biased Estimation for Nonorthogonal Problems.

Technometrics 12 (1), 55–67.

Kollo, T., von Rosen, D., 2005. Advanced Multivariate Statistics with Matrices. Vol. 579 of Mathematics

and Its Applications (New York). Springer, Dordrecht.

77–87.

74 (4), 745–771.

84 (405), 165–175.

28 (15), 2104–2115.

19

Li, R., Wu, B., Aug. 2012. Sparse regularized discriminant analysis with application to microarrays. Com-

putational biology and chemistry 39, 14–19.

Lu, J., Plataniotis, K. N., 2003. Regularized discriminant analysis for the small sample size problem in face

recognition. Pattern Recognition Letters.

Lu, J., Plataniotis, K. N., Venetsanopoulos, A. N., 2005. Regularization studies of linear discriminant analysis

in small sample size scenarios with application to face recognition. Pattern Recognition Letters 26 (2),

Mai, Q., Zou, H., Yuan, M., Feb. 2012. A direct approach to sparse discriminant analysis in ultra-high

181–191.

dimensions. Biometrika 99 (1), 29–42.

nition Letters 16 (3), 267–275.

Mkhadri, A., Mar. 1995. Shrinkage parameter for the modiﬁed linear discriminant analysis. Pattern Recog-

Mkhadri, A., Celeux, G., Nasroallah, A., 1997. Regularization in discriminant analysis: an overview. Com-

putational Statistics and Data Analysis 23 (3), 403–423.

Murphy, K. P., Aug. 2012. Machine Learning: A Probabilistic Perspective. The MIT Press, Cambridge,

Massachusetts.

Nakayama, R., Nemoto, T., Takahashi, H., Ohta, T., Kawai, A., Seki, K., Yoshida, T., Toyama, Y., Ichikawa,

H., Hasegawa, T., Apr. 2007. Gene expression analysis of soft tissue sarcomas: characterization and

reclassiﬁcation of malignant ﬁbrous histiocytoma. Nature 20 (7), 749–759.

Pang, H., Tong, T., Zhao, H., Mar. 2009. Shrinkage-based Diagonal Discriminant Analysis and Its Applica-

tions in High-Dimensional Data. Biometrics 65 (4), 1021–1029.

Pima, I., Aladjem, M., 2004. Regularized discriminant analysis for face recognition. Pattern Recognition

37 (9), 1945–1948.

581–596.

Berkeley, pp. 601–620.

Interscience.

Ramey, J., Young, P. D., 2013. A comparison of regularization methods applied to the linear discriminant

function with high-dimensional microarray data. Journal of Statistical Computation and Simulation 83 (3),

Rao, C. R., Mitra, S. K., 1971. Generalized inverse of a matrix and its applications. In: Proceedings of

the Sixth Berkeley Symposium on Mathematical Statistics and Probability. University of California Press,

Seber, G. A. F., Aug. 2004. Multivariate Observations. Wiley Series in Probability and Statistics. Wiley-

Shipp, M. A., Ross, K. N., Tamayo, P., Weng, A. P., Kutok, J. L., Aguiar, R. C. T., Gaasenbeek, M., Angelo,

M., Reich, M., Pinkus, G. S., Ray, T. S., Koval, M. A., Last, K. W., Norton, A., Lister, T. A., Mesirov,

J., Neuberg, D. S., Lander, E. S., Aster, J. C., Golub, T. R., Jan. 2002. Diﬀuse large B-cell lymphoma

20

outcome prediction by gene-expression proﬁling and supervised machine learning. Nature Medicine 8 (1),

68–74.

Singh, D., Febbo, P. G., Ross, K., Jackson, D. G., Manola, J., Ladd, C., Tamayo, P., Renshaw, A. A.,

D’Amico, A. V., Richie, J. P., Lander, E. S., Loda, M., Kantoﬀ, P. W., Golub, T. R., Sellers, W. R., Mar.

2002. Gene expression correlates of clinical prostate cancer behavior. Cancer Cell 1 (2), 203–209.

Srivastava, M. S., Kubokawa, T., 2007. Comparison of discrimination methods for high dimensional data.

Journal of the Japan Statistical Society 37 (1), 123–134.

Tadjudin, S., Landgrebe, D. A., Jul. 1999. Covariance estimation with limited training samples. IEEE

Transactions on Geoscience and Remote Sensing 37 (4), 2113–2118.

Tai, F., Pan, W., Dec. 2007. Incorporating prior knowledge of gene functional groups into regularized dis-

criminant analysis of microarray data. Bioinformatics 23 (23), 3170–3177.

Tian, E., Zhan, F., Walker, R., Rasmussen, E., Ma, Y., Barlogie, B., Shaughnessy, Jr., J. D., Dec. 2003.

The Role of the Wnt-Signaling Antagonist DKK1 in the Development of Osteolytic Lesions in Multiple

Myeloma. New England Journal of Medicine 349 (26), 2483–2494.

Tong, T., Chen, L., Zhao, H., Feb. 2012. Improved mean estimation and its application to diagonal discrim-

inant analysis. Bioinformatics 28 (4), 531–537.

Webb, A. R., Copsey, K. D., Sep. 2011. Statistical Pattern Recognition, 3rd Edition. John Wiley & Sons,

Chichester, West Sussex, UK.

Witten, D. M., Tibshirani, R., Aug. 2011. Penalized classiﬁcation using Fisher’s linear discriminant. Journal

of the Royal Statistical Society: Series B (Statistical Methodology) 73 (5), 753–772.

Wu, W., Mallet, Y., Walczak, B., Penninckx, W., Massart, D. L., Heuerding, S., Erni, F., Aug. 1996.

Comparison of regularized discriminant analysis, linear discriminant analysis, and quadratic discriminant

analysis applied to NIR data. Analytica Chimica Acta 329 (3), 257–265.

Xu, P., Brock, G. N., Parrish, R. S., Mar. 2009. Modiﬁed linear discriminant analysis approaches for classiﬁ-

cation of high-dimensional microarray data. Computational Statistics and Data Analysis 53 (5), 1674–1687.

Ye, J., Ji, S., Nov. 2009. Discriminant Analysis for Dimensionality Reduction: An Overview of Recent

Developments. Biometrics: Theory, Methods, and Applications. John Wiley & Sons, Inc., Hoboken, NJ,

USA.

Ye, J., Wang, T., 2006. Regularized Discriminant Analysis for High Dimensional, Low Sample Size Data. In:

The 12th ACM SIGKDD International Conference. ACM Press, New York, New York, USA, p. 454.

Zhang, Z., Dai, G., Xu, C., Jordan, M. I., Mar. 2010. Regularized Discriminant Analysis, Ridge Regression

and Beyond. The Journal of Machine Learning Research 11.

21

Figure 1: Contours of ﬁve multivariate normal populations as a function of the pooling parameter λ.

22

Figure 2: Timing comparisons (in seconds) between HDRDA and RDA classiﬁers.

23

Figure 3: Distribution of ratios of mean RDA runtime to mean HDRDA runtime across 1000 bootstrap replications.

24

Figure 4: Average classiﬁcation error rates as a function of the contamination probability (cid:15). Approximate standard errors were

no greater than 0.022.

25

Figure 5: Average classiﬁcation error rates as a function of the number of features p. Approximate standard errors were no

greater than 0.022.

26

Classiﬁer

Chiaretti

Chowdary

Nakayama

Shipp

Singh

Tian

Guo

0.111 (0.044)

0.056 (0.051)

0.208 (0.061)

0.086 (0.063)

0.089 (0.055)

0.268 (0.082)

HDRDA Convex

0.115 (0.044)

0.035 (0.026)

0.208 (0.066)

0.073 (0.057)

0.111 (0.059)

0.229 (0.049)

HDRDA Ridge

0.118 (0.050)

0.033 (0.022)

0.208 (0.070)

0.072 (0.065)

0.099 (0.046)

0.225 (0.050)

Pang

0.663 (0.062)

0.197 (0.091)

0.227 (0.062)

0.192 (0.091)

0.221 (0.095)

0.267 (0.054)

Random Forest

0.124 (0.053)

0.045 (0.028)

0.232 (0.063)

0.135 (0.078)

0.093 (0.045)

0.206 (0.044)

Tong

Witten

0.195 (0.068)

0.197 (0.091)

0.227 (0.062)

0.192 (0.091)

0.221 (0.095)

0.267 (0.054)

0.194 (0.068)

0.197 (0.091)

0.232 (0.068)

0.193 (0.092)

0.221 (0.095)

0.264 (0.053)

Table 1: The average of the test error rates obtained on gene-expression data sets over 100 random training-test partitions.

Standard deviations of the test error rates are given in the parentheses. The classiﬁer with the minimum average error rate for

each data set is in bold.

27

High-Dimensional Regularized Discriminant Analysis

John A. Rameya, Caleb K. Steinb, Phil D. Youngc, Dean M. Youngd

aNovi Labs
bMyeloma Institute, University of Arkansas for Medical Sciences
cDepartment of Management and Information Systems, Baylor University
dDepartment of Statistical Science, Baylor University

7
1
0
2
 
b
e
F
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
2
8
1
1
0
.
2
0
6
1
:
v
i
X
r
a

Abstract

Regularized discriminant analysis (RDA), proposed by Friedman (1989), is a widely popular classiﬁer that

lacks interpretability and is impractical for high-dimensional data sets. Here, we present an interpretable

and computationally eﬃcient classiﬁer called high-dimensional RDA (HDRDA), designed for the small-

sample, high-dimensional setting. For HDRDA, we show that each training observation, regardless of class,

contributes to the class covariance matrix, resulting in an interpretable estimator that borrows from the

pooled sample covariance matrix. Moreover, we show that HDRDA is equivalent to a classiﬁer in a reduced-

feature space with dimension approximately equal to the training sample size. As a result, the matrix

operations employed by HDRDA are computationally linear in the number of features, making the classiﬁer

well-suited for high-dimensional classiﬁcation in practice. We demonstrate that HDRDA is often superior

to several sparse and regularized classiﬁers in terms of classiﬁcation accuracy with three artiﬁcial and six

real high-dimensional data sets. Also, timing comparisons between our HDRDA implementation in the

sparsediscrim R package and the standard RDA formulation in the klaR R package demonstrate that as

the number of features increases, the computational runtime of HDRDA is drastically smaller than that of

Keywords: Regularized discriminant analysis, High-dimensional classiﬁcation, Covariance-matrix

regularization, Singular value decomposition, Multivariate analysis, Dimension reduction

2010 MSC: 62H30, 65F15, 65F20, 65F22

RDA.

1. Introduction

In this paper, we consider the classiﬁcation of small-sample, high-dimensional data, where the number of

features p exceeds the training sample size N . In this setting, well-established classiﬁers, such as linear dis-

criminant analysis (LDA) and quadratic discriminant analysis (QDA), become incalculable because the class

and pooled covariance matrix estimators are singular (Murphy, 2012; Bouveyron, Girard, and Schmid, 2007;

Mkhadri, Celeux, and Nasroallah, 1997). To improve the accuracy of the estimation of the class covariance

matrices estimated in the QDA classiﬁer and to ensure that the covariance matrix estimators are nonsin-

gular, Friedman (1989) proposed the regularized discriminant analysis (RDA) classiﬁer by incorporating a

weighted average of the pooled sample covariance matrix and the class sample covariance matrix. To further

improve the accuracy of the estimation of the class covariance matrix and to stabilize its inverse, Friedman

(1989) also included a regularization component by shrinking the covariance matrix estimator towards the

Preprint submitted to Elsevier

February 7, 2017

identity matrix, which yields a nonsingular estimator following the well-known ridge-regression approach of

Hoerl and Kennard (1970). Despite its popularity, the “borrowing” operation employed in the RDA classi-

ﬁer lacks interpretability (Bensmail and Celeux, 1996). Furthermore, the RDA classiﬁer is impractical for

high-dimensional data sets because it computes the inverse and determinant of the covariance matrices for

each class. Both matrix calculations are computationally expensive because the number of operations grows

at a polynomial rate in the number of features. Moreover, the model selection of the RDA classiﬁer’s two

tuning parameters is computationally burdensome because the matrix inverse and determinant of each class

covariance matrix are computed across multiple cross-validation folds for each candidate tuning-parameter

Here, we present the high-dimensional RDA (HDRDA) classiﬁer, which is intended for the case when

p > N . We reparameterize the RDA classiﬁer similar to that of Hastie, Tibshirani, and Friedman (2008)

and Halbe and Aladjem (2007) and employ a biased covariance-matrix estimator that partially pools the

individual sample covariance matrices from the QDA classiﬁer with the pooled sample covariance matrix from

the LDA classiﬁer. We then shrink the resulting covariance-matrix estimator towards a scaled identity matrix

to ensure positive deﬁniteness. We show that the pooling parameter in the HDRDA classiﬁer determines

the contribution of each training observation to the estimation of each class covariance matrix, enabling

interpretability that has been previously lacking with the RDA classiﬁer (Bensmail and Celeux, 1996). Our

parameterization diﬀers from that of Hastie, Tibshirani, and Friedman (2008) and that of Halbe and Aladjem

(2007) in that our formulation allows the ﬂexibility of various covariance-matrix estimators proposed in the

literature, including a variety of ridge-like estimators, such as the one proposed by Srivastava and Kubokawa

pair.

(2007).

Next, we establish that the matrix operations corresponding to the null space of the pooled sample

covariance matrix are redundant and can be discarded from the HDRDA decision rule without loss of

classiﬁcatory information when we apply reasoning similar to that of Ye and Wang (2006). As a result, we

achieve a substantial reduction in dimension such that the matrix operations used in the HDRDA classiﬁer are

computationally linear in the number of features. Furthermore, we demonstrate that the HDRDA decision

rule is invariant to adjustments to the approximately p − N zero eigenvalues, so that the decision rule in the

original feature space is equivalent to a decision rule in a lower dimension, such that matrix inverses and

determinants of relatively small matrices can be rapidly computed. Finally, we show that several shrinkage

methods that are special cases of the HDRDA classiﬁer have no eﬀect on the approximately p − N zero

eigenvalues of the covariance-matrix estimators when p > N . Such techniques include work from Srivastava

and Kubokawa (2007), Rao and Mitra (1971), and other methods studied by Ramey and Young (2013) and

Xu, Brock, and Parrish (2009).

We also provide an eﬃcient algorithm along with pseudocode to estimate the HDRDA classiﬁer’s tuning

parameters in a grid search via cross-validation. Timing comparisons between our HDRDA implementation

in the sparsediscrim R package available on CRAN and the standard RDA formulation in the klaR R

package demonstrate that as the number of features increases, the computational runtime of the HDRDA

classiﬁer is drastically smaller than that of RDA. In fact, when p = 5000, we show that the HDRDA classiﬁer

2

is 502.786 times faster on average than the RDA classiﬁer. In this scenario, the HDRDA classiﬁer’s model

selection requires 2.979 seconds on average, while that of the RDA classiﬁer requires 24.933 minutes on

average.

Finally, we study the classiﬁcation performance of the HDRDA classiﬁer on six real high-dimensional data

sets along with a simulation design that generalizes the experiments initially conducted by Guo et al. (2007).

We demonstrate that the HDRDA classiﬁer often attains superior classiﬁcation accuracy to several recent

classiﬁers designed for small-sample, high-dimensional data from Tong, Chen, and Zhao (2012), Witten and

Tibshirani (2011), Pang, Tong, and Zhao (2009), and Guo et al. (2007). We also include as a benchmark

the random forest from Breiman (2001) because Fern´andez-Delgado, Cernadas, Barro, and Amorim (2014)

have concluded that the random forest is often superior to other classiﬁers in benchmark studies. We show

that our proposed classiﬁer is competitive and often outperforms the random forest in terms of classiﬁcation

accuracy in the small-sample, high-dimensional setting.

The remainder of this paper is organized as follows. In Section 2 we introduce the classiﬁcation problem

and necessary notation to describe our contributions. In Section 3 we present the HDRDA classiﬁer along

with its interpretation. In Section 4, we provide properties of the HDRDA classiﬁer and a computationally

eﬃcient model-selection procedure. In Section 5, we compare the model-selection timings of the HDRDA

and RDA classiﬁers.

In Section 6 we describe our simulation studies of artiﬁcial and real data sets and

examine the experimental results. We conclude with a brief discussion in Section 7.

2. Preliminaries

2.1. Notation

To facilitate our discussion of covariance-matrix regularization and high-dimensional classiﬁcation, we
require the following notation. Let Ra×b denote the matrix space of all a × b matrices over the real ﬁeld
R. Denote by Im the m × m identity matrix, and let 0m×p be the m × p matrix of zeros, such that 0m
is understood to denote 0m×m. Deﬁne 1m ∈ Rm×1 as a vector of ones. Let AT , A+, and N (A) denote
the transpose, the Moore-Penrose pseudoinverse, and the null space of A ∈ Rm×p, respectively. Denote
by R>
p×p denote the cone of real p × p
positive-semideﬁnite matrices. Let V ⊥ denote the orthogonal complement of a vector space V ⊂ Rp×1. For
c ∈ R, let c+ = 1/c if c (cid:54)= 0 and 0 otherwise.

p×p the cone of real p × p positive-deﬁnite matrices. Similarly, let R≥

2.2. Discriminant Analysis

In discriminant analysis we wish to assign an unlabeled vector x ∈ Rp×1 to one of K unique, known
classes by constructing a classiﬁer from N training observations. Let xi = (xi1, . . . , xip) ∈ Rp×1 be the ith

observation (i = 1, . . . , N ) with true, unique membership yi ∈ {ω1, . . . , ωK}. Denote by nk the number of
training observations realized from class k, such that (cid:80)K
from a mixture distribution p(x) = (cid:80)K

k=1 nk = N . We assume that (xi, yi) is a realization
k=1 p(x|ωk)p(ωk), where p(x|ωk) is the probability density function
(PDF) of the kth class and p(ωk) is the prior probability of class membership of the kth class. We further

assume p(ωk) = p(ωl), 1 ≤ k, l ≤ K, k (cid:54)= l.

3

The QDA classiﬁer is the optimal Bayesian decision rule with respect to a 0 − 1 loss function when
p(x|ωk) is the PDF of the multivariate normal distribution with known mean vectors µk ∈ Rp×1 and known
covariance matrices Σk ∈ R>

p×p, k = 1, 2, . . . , K. Because µk and Σk are typically unknown, we assign an

unlabeled observation x to class ωk with the sample QDA classiﬁer

DQDA(x) = arg min

(x − ¯xk)T (cid:98)Σ−1

k (x − ¯xk) + log | (cid:98)Σk|,

k

where ¯xk and (cid:98)Σk are the maximum-likelihood estimators (MLEs) of µk and Σk, respectively. If we assume

further that Σk = Σ, k = 1, . . . , K, then the pooled sample covariance matrix (cid:98)Σ is substituted for (cid:98)Σk in

(1), where

(1)

(2)

(cid:98)Σ = N −1

nk (cid:98)Σk

K
(cid:88)

k=1

is the MLE for Σ. Here, (1) reduces to the sample LDA classiﬁer. We omit the log-determinant because it

is constant across the K classes.

The smallest eigenvalues of (cid:98)Σk and the directions associated with their eigenvectors can highly inﬂuence

the classiﬁer in (1).

In fact, the eigenvalues of (cid:98)Σk are well-known to be biased if p ≥ nk such that the

smallest eigenvalues are underestimated (Seber, 2004). Moreover, if p > nk, then rank( (cid:98)Σk) ≤ nk, which

implies that at least p − nk eigenvalues of (cid:98)Σk are zero. Furthermore, although more feature information is
available to discriminate among the K classes, if p > nk, (1) is incalculable because (cid:98)Σ−1

k does not exist.

Several regularization methods, such as the methods considered by Xu et al. (2009), Guo et al. (2007),

and Mkhadri (1995), have been proposed in the literature to adjust the eigenvalues of (cid:98)Σk so that (1) is
calculable and provides reduced variability for (cid:98)Σ−1
applies a shrinkage factor γ > 0, so that

k . A common form of the covariance-matrix regularization

(cid:98)Σk(γ) = (cid:98)Σk + γIp,

(3)

similar to a method employed in ridge regression (Hoerl and Kennard, 1970). Equation (3) eﬀectively shrinks

the sample covariance matrix (cid:98)Σk toward Ip, thereby increasing the eigenvalues of (cid:98)Σk by γ. Speciﬁcally,

the zero eigenvalues are replaced with γ, so that (3) is positive deﬁnite. For additional covariance-matrix

regularization methods, see Ramey and Young (2013), Xu et al. (2009), and Ye and Ji (2009).

3. High-Dimensional Regularized Discriminant Analysis

Here, we deﬁne the HDRDA classiﬁer by ﬁrst formulating the covariance-matrix estimator (cid:98)Σk(λ) and

demonstrating its clear interpretation as a linear combination of the crossproducts of the training observations

centered by their respective class sample means. We deﬁne the convex combination

(cid:98)Σk(λ) := (1 − λ) (cid:98)Σk + λ (cid:98)Σ,

k = 1, . . . , K,

(4)

4

where λ ∈ [0, 1] is the pooling parameter. By rewriting (4) in terms of the observations xi, i = 1, . . . , N ,

each centered by its class sample mean, we attain a clear interpretation of (cid:98)Σk(λ). That is,

(cid:98)Σk(λ) =

1 − λ +

(cid:98)Σk +

nk(cid:48) (cid:98)Σk(cid:48)

(cid:18)

(cid:19)

λnk
N

λ
N

K
(cid:88)

k(cid:48)=1
k(cid:48)(cid:54)=k

I(yi = k)xixT

i +

I(yi (cid:54)= k)xixT
i

λ
N

N
(cid:88)

i=1

(cid:18) 1 − λ
nk

+

λ
N

(cid:19) N
(cid:88)

i=1

cik(λ)xixT
i ,

=

=

N
(cid:88)

i=1

where cik(λ) = λN −1 + (1 − λ)n−1

k I(yi = k). From (5), we see that λ weights the contribution of each of
the N observations in estimating Σk from all K classes rather than using only the nk observations from a

single class. As a result, we can interpret (5) as a covariance-matrix estimator that borrows from (cid:98)Σ in (2)

to estimate Σk.

In Figure 1 we plot the contours of ﬁve multivariate normal populations for λ = 0 with unequal covariance

matrices. As λ approaches 1, the contours become more similar, resulting in identical contours for λ = 1. Be-

low, we show that the pooling operation is advantageous in increasing the rank of each (cid:98)Σk(λ) from rank( (cid:98)Σk)

to rank( (cid:98)Σ) for 0 < λ ≤ 1. Notice that if λ = 0, then the observations from the remaining K − 1 classes do

not contribute to the estimation of Σk, corresponding to (cid:98)Σk. Furthermore, if λ = 1, the weights cik(λ) in (5)
reduce to 1/N , corresponding to (cid:98)Σ. For brevity, when λ = 1, we deﬁne X = [(cid:112)c1k(1)xT
N ]T
such that (cid:98)Σ = N −1X T X. Similarly, for λ = 0, we deﬁne Xk = [(cid:112)c1k(0)xT
N ]T such that
(cid:98)Σk = n−1

1 , . . . , (cid:112)cN k(0)xT

1 , . . . , (cid:112)cN k(1)xT

k X T

k Xk.

[Insert Figure 1 approximately here ]

As we have discussed above, several eigenvalue adjustment methods have been proposed that increase

eigenvalues (approximately) equal to 0. To further improve the estimation of Σk and to stabilize the esti-

mator’s inverse, we deﬁne the eigenvalue adjustment of (4) as

˜Σk := αk (cid:98)Σk(λ) + γIp,

where αk ≥ 0 and γ ≥ 0 is an eigenvalue-shrinkage constant. Thus, the pooling parameter λ controls

the amount of estimation information borrowed from (cid:98)Σ to estimate Σk, and the shrinkage parameter γ

determines the degree of eigenvalue shrinkage. The choice of αk allows for a ﬂexible formulation of covariance-

matrix estimators. For instance, if αk = 1, k = 1, . . . , K, then (6) resembles (3). Similarly, if αk = 1 − γ,

then (6) has a form comparable to the RDA classiﬁer from Friedman (1989). Substituting (6) into (1), we

deﬁne the HDRDA classiﬁer as

DHDRDA(x) = arg min

(x − ¯xk)T ˜Σ+

k (x − ¯xk) + log | ˜Σk|.

k

For γ > 0, ˜Σk is nonsingular such that ˜Σ−1
k in (7). If γ = 0, we explicitly set
k
| ˜Σk| equal to the product of the positive eigenvalues of ˜Σk. Following Friedman (1989), we select λ and γ

can be substituted for ˜Σ+

(5)

(6)

(7)

5

from a grid of candidate models via cross-validation (Hastie et al., 2008). We provide an implementation of

(7) in the hdrda function contained in the sparsediscrim R package, which is available on CRAN.

The choice of αk in (6) is one of convenience and allows the ﬂexibility of various covariance-matrix

estimators proposed in the literature. In practice, we generally are not interested in estimating αk because

the estimation of K additional tuning parameters via cross-validation is counterproductive to our goal of

computational eﬃciency. For appropriate values of αk, the HDRDA covariance-matrix estimator includes or

resembles a large family of estimators. Notice that if αk = 1 and λ = 1, (6) is equivalent to the standard

ridge-like covariance-matrix estimator in (3). Other estimators proposed in the literature can be obtained

when one selects γ accordingly. For instance, with γ = tr{ (cid:98)Σ}/ min(N, p), we obtain the estimator from

Srivastava and Kubokawa (2007).

When αk = 1 − γ, (6) resembles the biased covariance-matrix estimator

(cid:98)Σk(λ, γ) = (1 − γ) (cid:98)Σ(RDA)

k

(λ) + γ

(cid:111)

(λ)

tr

(cid:110)
(cid:98)Σ(RDA)
p

k

Ip

(8)

employed in the RDA classiﬁer, where (cid:98)Σ(RDA)
tion parameter that controls the shrinkage of (8) towards Ip weighted by the average of the eigenvalues of
(cid:98)Σ(RDA)
ﬁer is impractical for high-dimensional data because the inverse and determinant of (8) must be calculated

(λ). Despite the similarity of (8) to the HDRDA covariance-matrix estimator in (6), the RDA classi-

(λ) is a pooled estimator of Σk and γ ∈ [0, 1] is a regulariza-

k

k

when substituted into (1). Furthermore, (8) has no clear interpretation.

4. Properties of the HDRDA Classiﬁer

Next, we establish properties of the covariance-matrix estimator and the decision rule employed in the

HDRDA classiﬁer. By doing so, we demonstrate that (7) lends itself to a more eﬃcient calculation. We

decompose (7) into a sum of two components, where the ﬁrst summand consists of matrix operations applied

to low-dimensional matrices and the second summand corresponds to the null space of (cid:98)Σ in (2). We show that

the matrix operations performed on the null space of (cid:98)Σ yield constant quadratic forms across all classes and

can be omitted. For p (cid:29) N , the constant component involves determinants and inverses of high-dimensional

matrices, and by ignoring these calculations, we achieve a substantial reduction in computational costs.

Furthermore, a byproduct is that adjustments to the associated eigenvalues have no eﬀect on (7). Lastly, we

utilize the singular value decomposition to eﬃciently calculate the eigenvalue decomposition of (cid:98)Σ, further

reducing the computational costs of the HDRDA classiﬁer.

First, we require the following relationship regarding the null spaces of (cid:98)Σk(λ), (cid:98)Σ, and (cid:98)Σk.

Lemma 1. Let (cid:98)Σk and (cid:98)Σ be the MLEs of Σk and Σ, respectively. Let (cid:98)Σk(λ) be deﬁned as in (4). Then,

N { (cid:98)Σk(λ)} ⊂ N ( (cid:98)Σ) ⊂ N ( (cid:98)Σk), k = 1, . . . , K.

Proof. Let z ∈ N { (cid:98)Σk(λ)} for some k = 1, . . . , K. Hence, 0 = zT (cid:98)Σk(λ)z = (1 − λ)zT (cid:98)Σkz + λzT (cid:98)Σz.
Because (cid:98)Σk, (cid:98)Σ ∈ R≥
Now, suppose z ∈ N ( (cid:98)Σ). Similarly, we have that 0 = zT (cid:98)Σz = N −1 (cid:80)K
z ∈ N ( (cid:98)Σk) because (cid:98)Σk ∈ R≥

p×p, we have z ∈ N ( (cid:98)Σ) and z ∈ N ( (cid:98)Σk). In particular, we have that N { (cid:98)Σk(λ)} ⊂ N ( (cid:98)Σ).
k=1 nkzT (cid:98)Σkz, which implies that

p×p. Therefore, N ( (cid:98)Σ) ⊂ N ( (cid:98)Σk).

6

In Lemma 2 below, we derive an alternative expression for ˜Σk in terms of the matrix of eigenvectors of (cid:98)Σ.
p×p is the diagonal matrix of eigenvalues

Let (cid:98)Σ = U DU T be the eigendecomposition of (cid:98)Σ such that D ∈ R≥
of (cid:98)Σ with

D =





Dq

0

0

0p−q



 ,

Dq ∈ R>

q×q is the diagonal matrix consisting of the positive eigenvalues of (cid:98)Σ, the columns of U ∈ Rp×p are
the corresponding orthonormal eigenvectors of (cid:98)Σ, and rank( (cid:98)Σ) = q. Then, we partition U = (U1, U2) such
that U1 ∈ Rp×q and U2 ∈ Rp×(p−q).

Lemma 2. Let (cid:98)Σ = U DU T be the eigendecomposition of (cid:98)Σ as above, and suppose that rank( (cid:98)Σ) = q ≤ p.

Then, we have

where

Hence,

˜Σk = U





Wk

0

0

γIp−q


 U T ,

k = 1, . . . , K,

Wk = αk{(1 − λ)U T

1 (cid:98)ΣkU1 + λDq} + γIq.

(9)

(10)

Proof. From Lemma 1, the columns of U2 span the null space of (cid:98)Σk, which implies that (cid:98)ΣkU2 = 0p×(p−q).

U T (cid:98)ΣkU =





U T

1 (cid:98)ΣkU1

0

0

0p−q



 ,

k = 1, . . . , K.

Thus, U T ˜ΣkU = αk{(1 − λ)U T (cid:98)ΣkU + λD} + γIp, and (9) holds because U is orthogonal.

As an immediate consequence of Lemma 2, we have the following corollary.

Corollary 1. Let (cid:98)Σk(λ) be deﬁned as in (4). Then, for λ ∈ (0, 1], rank{ (cid:98)Σk(λ)} = q, k = 1, . . . , K.

Proof. The proof follows when we set γ = 0 in Lemma 2.

Thus, by incorporating each xi into the estimation of Σk, we increase the rank of (cid:98)Σk(λ) to q ≈ N if

λ (cid:54)= 0. Next, we provide an essential result that enables us to prove that (7) is invariant to adjustments to
the eigenvalues of ˜Σk corresponding to the null space of (cid:98)Σ.

Lemma 3. Let U2 be deﬁned as above. Then, for all x ∈ Rp×1, U T
where k (cid:54)= k(cid:48).

2 (x − ¯xk) = U T

2 (x − ¯xk(cid:48)), 1 ≤ k, k(cid:48) ≤ K,

2 ∈ C( (cid:98)Σ)⊥ (Kollo and von Rosen, 2005, Lemma 1.2.5). Now, because xi ∈ C( (cid:98)Σ) (i = 1, . . . , N ), U T

Proof. Let x ∈ Rp×1, and suppose that 1 ≤ k, k(cid:48) ≤ K. Recall that U2 ∈ N ( (cid:98)Σ), which implies that
U T
2 xi =
0p−q. Hence, 0p−q = (cid:80)N
2 ( ¯xk − ¯xk(cid:48)), where βi = (nknk(cid:48))−1{I(yi = k)nk(cid:48) − I(yi = k(cid:48))nk}.
Therefore, U T

2 xi = U T

i=1 βiU T

2 (x − ¯xk) = U T

2 (x − ¯xk(cid:48)).

7

We now present our main result, where we decompose (7) and show that the term requiring the largest

computational costs does not contribute to the classiﬁcation of an unlabeled observation performed using

Lemma 3. Hence, we reduce (7) to an equivalent, more computationally eﬃcient decision rule.

Theorem 1. Let ˜Σk and Wk be deﬁned as in (9) and (10), respectively, and let U1 be deﬁned as above.

Then, the decision rule in (7) is equivalent to

DHDRDA(x) = arg min

(x − ¯xk)T U1W −1

k U T

1 (x − ¯xk) + log |Wk|.

(11)

k

Proof. From (9), we have that





W −1
k

˜Σ+

k = U

0


 U T

0

γ+Ip−q

and | ˜Σk| = γp−q|Wk|, k = 1, . . . , K. Therefore, for all x ∈ Rp×1, we have that

(x − ¯xk)T ˜Σ+

k (x − ¯xk) + log | ˜Σk| = (x − ¯xk)T U1W −1
+ γ+(x − ¯xk)T U2U T

k U T

1 (x − ¯xk)

2 (x − ¯xk) + log |Wk|

+ (p − q) log γ.

Because γ is constant for k = 1, . . . , K, we can omit the (p − q) log γ term and particularly avoid the

calculation of log 0 for γ = 0. Then, the proof follows from Lemma 3 because U T

2 (x − ¯xk) is constant for

k = 1, . . . , K.

Using Theorem 1, we can avoid the time-consuming inverses and determinants of p×p covariance matrices
in (7) and instead calculate these same operations on Wk ∈ Rq×q in (11). The substantial computational

improvements arise because our proposed classiﬁer in (7) is invariant to the term U2, thus yielding an

equivalent classiﬁer in (11) with a substantial reduction in computational complexity. Here, we demonstrate

that the computational eﬃciency in calculating the inverse and determinant of Wk can be further improved

via standard matrix operations when we show that the inverses and determinants of Wk can be performed

on matrices of size nk × nk.

Proposition 1. Let Wk be deﬁned as above. Then, |Wk| = |Γk||Qk| and

W −1

k = Γ−1

k − n−1

k αk(1 − λ)Γ−1

k U T

1 X T

k Q−1

k XkU1Γ−1
k ,

where

and

Qk = Ink + n−1

k αk(1 − λ)XkU1Γ−1

k U T

1 X T
k

Γk = αkλDq + γIq.

8

(12)

(13)

(14)

k αk(1 − λ)U T

Proof. First, we write Wk = n−1
1 X T
from Harville (2008), which states that |A + BT C| = |A||T ||T −1 + CA−1B|, where A ∈ R>

k XkU1 + Γk. To calculate |Wk|, we apply Theorem 18.1.1
a×a, B ∈ Ra×b,
k , T = Ink , and C = XkU1, we have
|Wk| = |Γk||Qk|. Similarly, (12) follows from the well-known Sherman-Woodbury formula (Harville, 2008,

b×b, and C ∈ Rb×a. Thus, setting A = Γk, B = αk(1 − λ)U T

T ∈ R>

1 X T

Theorem 18.2.8) because (A + BT C)−1 = A−1 − A−1B(T −1 + CA−1B)−1CA−1.

Notice that Γk is singular when (λ, γ) = (0, 0) because Γk = 0q, in which case we use the formulation in

(11) instead. Also, notice that if αk is constant across the K classes, then Γk in (14) is independent of k.

Consequently, |Γk| is constant across the K classes and need not be calculated in (11).

4.1. Model Selection

Thus far, we have presented the HDRDA classiﬁer and its properties that facilitate an eﬃcient calculation

of the decision rule. Here, we describe an eﬃcient model-selection procedure along with pseudocode in

Algorithm 1 to select the optimal tuning-parameter estimates from the Cartesian product of candidate

values {λg}G

g=1 × {γh}H

h=1. We estimate the V -fold cross-validation error rate for each candidate pair and
select ((cid:98)λ, (cid:98)γ), which attains the minimum error rate. To calculate the V -fold cross-validation, we partition
the original training data into V mutually exclusive and exhaustive folds that have approximately the same

number of observations. Then, for v = 1, . . . , V , we classify the observations in the vth fold by training

a classiﬁer on the remaining V − 1 folds. We calculate the cross-validation error as the proportion of

misclassiﬁed observations across the V folds.

A primary contributing factor to the eﬃciency of Algorithm 1 is our usage of the compact singular

value decomposition (SVD). Rather than computing the eigenvalue decomposition of (cid:98)Σ to obtain U1, we

instead obtain U1 by computing the eigendecomposition of a much smaller N × N matrix when p (cid:29) N
(Hastie et al., 2008, Chapter 18.3.5). Applying the SVD, we decompose Xc = M ∆U T , where M ∈ RN ×p
is orthogonal, ∆ ∈ R≥
p×p is a diagonal matrix consisting of the singular values of Xc, and U ∈ Rp×p is
c Xc, we have the eigendecomposition (cid:98)Σ = U DU T , where U is the
orthogonal. Recalling that (cid:98)Σ = N −1X T
matrix of eigenvectors of (cid:98)Σ and D = N −1∆ is the diagonal matrix of eigenvalues of (cid:98)Σ. Now, we can obtain
c = M DM T . Next, we
M and D eﬃciently from the eigenvalue decomposition of the N × N matrix XcX T
compute U = X T

c M D+/2, where

D+/2 =





D−1/2
q

0

0

0N −q



 .

We then determine q, the number of numerically nonzero eigenvalues present in D, by calculating the number

of eigenvalues that exceeds some tolerance value, say, 1 × 10−6. We then extract U1 as the ﬁrst q columns

of U .

As a result of the compact SVD, we need calculate XcU1 only once per cross-validation fold, requiring

O(pqN ) ≈ O(pN 2) calculations. Hence, the computational costs of expensive calculations, such as matrix

inverses and determinants, are greatly reduced because they are performed in the q-dimensional subspace.

Similarly, we reduce the dimension of the test data set by calculating XtestU1 once per fold. Conveniently,

9

input : Data matrix X

Parameter grid {λg}G

g=1 × {γh}H

h=1

output: Optimal Estimates (ˆλ, ˆγ)

for v ← 1 to V do

Partition X into Xtrain ∈ RN ×p and Xtest ∈ RNT ×p
for k ← 1 to K do

Extract Xk ∈ Rnk×p from Xtrain
Compute sample mean ¯xk from Xk
Center Xk ← Xk − 1nk ¯xT
k

end

end

1 , . . . , X T

Xc ← [X T
Compute the compact SVD Xc = MqDqU T
1

K]T

Transform Xc ← XcU1

Transform Xtest ← XtestU1

for k ← 1 to K do

Extract Xk ∈ Rnk×q from Xc
Recompute sample mean ¯xk from Xk

for (λ, γ) ∈ {λg}G

g=1 × {γh}H

h=1 do

for k ← 1 to K do

Compute Qk using (13)

Compute Γk using (14)
Compute W −1

using (12)

k

Compute |Wk| = |Γk||Qk|
Compute (x − ¯xk)T U1W −1

k U T

1 (x − ¯xk) + log |Wk| for each row x of Xtest

end

end

Classify test observations Xtest using (11)

Compute the number of misclassiﬁed test observations #{Errorv(λ, γ)}

end
Compute (cid:92)Error(λ, γ) = N −1 (cid:80)V
Report optimal (ˆλ, ˆγ) ← arg min(λ,γ)

v=1 #{Errorv(λ, γ)}
(cid:92)Error(λ, γ)

Algorithm 1: Model selection for the HDRDA classiﬁer

10

we see that the most costly computation involved in Qk and W −1
is XkU1, which can be extracted from
XcU1. Thus, after the initial calculation of XcU1 per cross-validation fold, Qk requires O(nkq2) operations.
Because Qk ∈ Rnk×nk , both its determinant and inverse require O(n3
k) operations. Consequently, W −1
k ∈ Rq×q requires O(q) operations.
requires O(nkq2) operations. Also, the inverse of the diagonal matrix Γ−1
Finally, we remark that |Wk| requires O(n3

k) operations.

k

k

The expressions given in Proposition 1 also expedite the selection of λ and γ via cross-validation

because the most time-consuming matrix operation involved in computing W −1
Rnk×q, which is independent of λ and γ. The subsequent operations in calculating W −1
be simply updated for diﬀerent pairs of λ and γ without repeating the costly computations. Also, rather
than calculating (x − ¯xk)T U1W −1
k)(cid:48)U1W −1
(Xtest − ¯xk1(cid:48)

1 (x − ¯xk) individually for each row x of Xtest, we can calculate
k). The diagonal elements of the resulting matrix contain the indi-

k U T
1 (Xtest − ¯xk1(cid:48)

and |Wk| is XkU1 ∈

and |Wk| can

k

k

k U T
vidual quadratic form of each test observation, xt.

5. Timing Comparisons between RDA and HDRDA

In this section, we demonstrate that the computational performance of the model selection employed in

the HDRDA classiﬁer is substantially faster than that of the RDA classiﬁer on small-sample, high-dimensional

data sets. The relative diﬀerence in runtime between the two classiﬁers drastically increases as p increases. To

compare the two classiﬁers, we generated 25 observations from each of K = 4 multivariate normal populations

with mean vectors µ1 = −3 · 1p, µ2 = −1p, µ3 = 1p, and µ4 = 3 · 1p. We set the covariance matrix of

each population to the p × p identity matrix. For each data set generated, we estimated the parameters λ

and γ for both classiﬁers using a grid of 5 equidistant candidate values between 0 and 1, inclusively. We

set αk = 1 − γ, k = 1, . . . , K, in the HDRDA classiﬁer. At each pair of λ and γ, we computed the 10-fold

cross-validation error rate (Hastie et al., 2008). Then, we selected the model that minimized the 10-fold

cross-validation error rate.

We compared the runtime of both classiﬁers by increasing the number of features from p = 500 to p = 5000

in increments of 500. Next, we generated 100 data sets for each value of p and computed the training and

model selection runtime of both classiﬁers. Our timing comparisons are based on our HDRDA implementation

in the sparsediscrim R package and the standard RDA implementation in the klaR R package. All timing

comparisons were conducted on an Amazon Elastic Compute Cloud (EC2) c4.4xlarge instance using version

3.3.1 of the open-source statistical software R. Our timing comparisons can be reproduced with the code

available at https://github.com/ramhiser/paper-hdrda.

5.1. Timing Comparison Results

In Figure 2, we plotted the runtime of the model selections for both the HDRDA and RDA classiﬁers as

a function of p. We observed that the HDRDA classiﬁer was substantially faster than the RDA classiﬁer as p

increased. In the left panel of Figure 2, we ﬁt a quadratic regression line to the RDA runtimes and a simple

linear regression model to the HDRDA runtimes. For improved understanding, in the right panel we repeated

the same scatterplot and linear ﬁt with the timings restricted to the observed range of the HDRDA timings.

11

Figure 2 suggests that the usage of a matrix inverse and determinant in the klaR R package’s discriminant

function yielded model-selection timings that exceeded linear growth in p. Because the HDRDA classiﬁer

removes inverse and determinants, it was computationally more eﬃcient than the RDA classiﬁer, especially

as p increased. In fact, when p = 5000, the RDA classiﬁer required 24.933 minutes on average to perform

model selection, while the HDRDA classiﬁer selected its optimal model in 2.979 seconds on average. Clearly,

the model selection employed by the HDRDA classiﬁer is substantially faster than that of the RDA classiﬁer.

We quantiﬁed the relative timing comparisons between the two classiﬁers by calculating the ratio of

mean timings of the RDA classiﬁer to the HDRDA classiﬁer for each value of p. We employed nonparametric

bootstrapping to estimate the mean ratio along with 95% conﬁdence intervals. In Figure 3, the bootstrap

sampling distributions for the ratio of mean timings are given. First, we observe that the mean relative

timings increased as p increased. For smaller dimensions, the relative diﬀerence in computing was sizable

with the average ratio of the mean timings equal to 14.513 for p = 500 and a 95% conﬁdence interval

of (14.191, 14.855). Furthermore, the ratio of mean computing times suggested that the RDA classiﬁer is

impractical for higher dimensions. For instance, when p = 5000, the ratio of mean computing times increased

to 502.786 with a 95% conﬁdence interval of (462.863, 546.396).

[Insert Figure 2 approximately here ]

[Insert Figure 3 approximately here ]

6. Classiﬁcation Study

In this section, we compare our proposed classiﬁer with four classiﬁers recently proposed for small-sample,

high-dimensional data along with the random-forest classiﬁer from Breiman (2001) using version 3.3.1 of the

open-source statistical software R. Within our study, we included penalized linear discriminant analysis from

Witten and Tibshirani (2011), implemented in the penalizedLDA package. We also considered shrunken

centroids regularized discriminant analysis from Guo et al. (2007) in the rda package. Because the rda

package does not perform the authors’ “Min-Min” rule automatically, we applied this rule within our R code.

We included two modiﬁcations of diagonal linear discriminant analysis from Tong et al. (2012) and Pang

et al. (2009), where the former employs an improved mean estimator and the latter utilizes an improved

variance estimator. Both classiﬁers are available in the sparsediscrim package. Finally, we incorporated the

random forest as a benchmark based on the ﬁndings of Fern´andez-Delgado et al. (2014), who concluded that

the random forest is often superior to other classiﬁers in benchmark studies. We used the implementation

of the random-forest classiﬁer from the randomForest package with 250 trees and 100 maximum nodes. For

each classifer we explicitly set prior probabilities as equal, if applicable. All other classiﬁer options were set

to their default settings. Below, we refer to each classiﬁer by the ﬁrst author’s surname. All simulations

were conducted on an Amazon EC2 c4.4xlarge instance. Our analyses can be reproduced via the code

available at https://github.com/ramhiser/paper-hdrda.

12

For the HDRDA classiﬁer in (11), we examined the classiﬁcation performance of two models. For the

ﬁrst HDRDA model, we set αk = 1, k = 1, . . . , K, so that the covariance-matrix estimator (6) resembled

(3). We estimated λ from a grid of 21 equidistant candidate values between 0 and 1, inclusively. Similarly,

we estimated γ from a grid consisting of the values 10−1, . . . , 104, and 105. We selected optimal estimates of

λ and γ using 10-fold cross-validation. For the second model, we set αk = 1 − γ, k = 1, . . . , K, to resemble

Friedman’s parameterization, and we estimated both λ and γ from a grid of 21 equidistant candidate values

between 0 and 1, inclusively.

We did not include the RDA classiﬁer in our classiﬁcation study because its training runtime was pro-

hibitively slow on high-dimensional data in our preliminary experiments. As shown in Section 5, the runtime

of the RDA classiﬁer was drastically larger than that of the HDRDA classiﬁer for a tuning grid of size

25 = 5 × 5. Consequently, a fair comparison between the RDA and HDRDA classiﬁers would require model

selection of 441 = 21 × 21 diﬀerent pairs of tuning parameters in the RDA classiﬁer. A tuning grid of this

size yielded excessively slow training runtimes for the RDA implementation from the klaR R package.

6.1. Simulation Study

In this section we compare the competing classiﬁers using the simulation design from Guo et al. (2007).

This design is widely used within the high-dimensional classiﬁcation literature, including the studies by

Ramey and Young (2013) and Witten and Tibshirani (2011). First, we consider the block-diagonal covariance

matrix from Guo et al. (2007),

Σk =


















0100
...
...
· · ·

Σ(ρk)

0100

0100 Σ(−ρk)

0100

0100

0100 Σ(ρk)

· · ·

0100

0100

· · ·

· · ·

· · ·

0100
...
· · ·

0100 Σ(−ρk) 0100
...
. . .
· · ·

0100

· · ·

· · ·


















· · ·
...
...
...
...
· · ·

,

(15)

where the (i, j)th entry of the block matrix Σ(ρk) ∈ R100×100 is

Σ(ρk)

ij = {ρ|i−j|

k

}1≤i,j≤100.

The block-diagonal covariance structure in (15) resembles gene-expression data: within each block of path-

ways, genes are correlated, and the correlation decays as a function of the distance between any two genes.

The original design from Guo et al. (2007) comprised two p-dimensional multivariate normal populations

with a common block-diagonal covariance matrix.

Although the design is indeed standard, the simulation conﬁguration lacks artifacts commonly observed

in real data, such as skewness and extreme outliers. As a result, we wished to investigate the eﬀect of outliers

on the high-dimensional classiﬁers. To accomplish this goal, we generalized the block-diagonal simulation

conﬁguration by sampling from a p-dimensional multivariate contaminated normal distribution. Denoting

the PDF of the p-dimensional multivariate normal distribution by Np(x|µ, Σ), we write the PDF of the kth

13

class as

0.05, . . ., 0.50.

p(x|ωk) = (1 − (cid:15))Np(x|µk, Σk) + (cid:15)Np(x|µk, ηΣk),

(16)

where (cid:15) ∈ [0, 1] is the probability that an observation is contaminated (i.e., drawn from a distribution with

larger variance) and η > 1 scales the covariance matrix Σk to increase the extremity of outliers. For (cid:15) = 0,

we have the benchmark block-diagonal simulation design from Guo et al. (2007). As (cid:15) is increased, the

average number of outliers is increased. In our simulation, we let η = 100 and considered the values of (cid:15) = 0,

We generated K = 3 populations from (16) with Σk given in (15) and set the mean vector of class 1 to

µ1 = 0p. Next, comparable to Guo et al. (2007), the ﬁrst 100 features of µ2 were set to 1/2, while the rest

). For simplicity, we deﬁned µ3 = −µ2. The three populations

diﬀered in their mean vectors in the ﬁrst 100 features corresponding to the ﬁrst block, and no diﬀerence in

were set to 0, i.e., µ2 = (1/2, . . . , 1/2
(cid:125)

(cid:124)

(cid:123)(cid:122)
100

, 0, . . . , 0
(cid:124) (cid:123)(cid:122) (cid:125)
p−100

the means occurred in the remaining blocks.

From each of the K = 3 populations, we sampled 25 training observations (nk = 25 for all k) and 10,000

test observations. After training each classiﬁer on the training data, we classiﬁed the test data sets and

computed the proportion of mislabeled test observations to estimate the classiﬁcation error rate for each

classiﬁer. Repeating this process 500 times, we computed the average of the error-rate estimates for each

classiﬁer. We allowed the number of features to vary from p = 100 to p = 500 in increments of 100 to examine

the classiﬁcation accuracy as the feature dimension increased while maintaining a small sample size. Guo

et al. (2007) originally considered ρk = 0.9 for all k. Alternatively, to explore the more realistic assumption

of unequal covariance matrices, we put ρ1 = 0.1, ρ2 = 0.5, and ρ3 = 0.9.

6.1.1. Simulation Results

In Figure 4, we observed each classiﬁer’s average classiﬁcation error rates for the values of (cid:15) and p.

Unsurprisingly, the average error rate increased for each classiﬁer as the contamination probability (cid:15) increased

regardless of the value of p. Sensitivity to the presence of outliers was most apparent for the Pang, Tong, and

Witten classiﬁers. For smaller dimensions, the random-forest and HDRDA classiﬁers tended to outperform

the remaining classiﬁers with the random forest performing best. As the feature dimension increased with

p ≥ 300, both HDRDA classiﬁers outperformed all other classiﬁers, suggesting that their inherent dimension

reduction better captured the classiﬁcatory information in the small training samples, even in the presence

of outliers.

[Insert Figure 4 approximately here ]

The Pang, Tong, and Witten methods yielded practically the same and consistently the worst error rates

when outliers were present with (cid:15) > 0, suggesting that these classiﬁers were sensitive to outliers. Notice, for

example, that when p = 400, the error rates of the Pang, Tong, and Witten classiﬁers increased dramatically

from approximately 19% when no outliers were present to approximately 43% when (cid:15) = 0.05. The sharp

increase in average error rates for these three classiﬁers continued as (cid:15) increased. Guo’s method always

14

outperformed those of Pang, Witten, and Tong, but after outliers were introduced, the Guo classiﬁer’s

average error rate was not competitive with the HDRDA classiﬁers or the random-forest classiﬁer.

[Insert Figure 5 approximately here ]

In Figure 5, we again examine the simulation results as a function of p for a subset of the values of (cid:15).

This set of plots allows us to investigate the eﬀect of feature dimensionality on classiﬁcation performance.

When no outliers were present (i.e., (cid:15) = 0), the random-forest classiﬁer was outperformed by all other

classiﬁers. Furthermore, the HDRDA classiﬁers were superior in terms of average error rate in this setting.

As p increased, an elevation in average error rate was expected for all classiﬁers, but the increase was not

observed to be substantial.

For (cid:15) > 0, we observed a diﬀerent behavior in classiﬁcation performance. First, the Pang, Tong, and

Witten methods, along with the random-forest method, increased in average error rate as p increased.

Contrarily, the performance of the HDRDA and Guo classiﬁers was hardly aﬀected by p. Also, as discussed

above, the HDRDA classiﬁers were superior to all other classiﬁers for large values of p with only the random-

forest classiﬁer outperforming them in smaller feature-dimension cases.

6.2. Application to Gene Expression Data

We compared the HDRDA classiﬁer to the ﬁve competing classiﬁers on six benchmark gene-expression

microarray data sets. First, we evaluated the classiﬁcation accuracy of each classiﬁer by randomly parti-

tioning the data set under consideration such that 2/3 of the observations were allocated as training data

and the remaining 1/3 of the observations were allocated as a test data set. To expedite the computational

runtime, we reduced the training data to the top 1000 variables by employing the variable-selection method

proposed by Dudoit et al. (2002). We then reduced the test data set to the same 1000 variables. After

training each classiﬁer on the training data, we classiﬁed the test data sets and computed the proportion

of mislabeled test observations to estimate the classiﬁcation error rate for each classiﬁer. Repeating this

process 100 times, we computed the average of the error-rate estimates for each classiﬁer. We next provide

a concise description of each high-dimensional data set examined in our classiﬁcation study.

6.2.1. Chiaretti et al. (2004) Data Set

Chiaretti et al. (2004) measured the gene-expression proﬁles for 128 individuals with acute lymphoblastic

leukemia (ALL) using Aﬀymetrix human 95Av2 arrays. Following Xu et al. (2009), we restricted the data

set to K = 2 classes such that n1 = 74 observations were without cytogenetic abnormalities and n2 = 37

observations had a detected BCR/ABL gene. The robust multichip average normalization method was

applied to all 12,625 gene-expression levels.

6.2.2. Chowdary et al. (2006) Data Set

Chowdary et al. (2006) investigated 52 matched pairs of tissues from colon and breast tumors using

Aﬀymetrix U133A arrays and ribonucleic-acid (RNA) ampliﬁcation. Each tissue pair was gathered from

the same patient and consisted of a snap-frozen tissue and a tissue suspended in an RNAlater preservative.

15

Overall, 31 breast-cancer and 21 colon-cancer pairs were gathered, resulting in K = 2 classes with n1 = 62

and n2 = 42. A purpose of the study was to determine whether the disease state could be identiﬁed using

22,283 gene-expression proﬁles.

6.2.3. Nakayama et al. (2007) Data Set

Nakayama et al. (2007) acquired 105 gene-expression samples of 10 types of soft-tissue tumors through an

oligonucleotide microarray, including 16 samples of synovial sarcoma (SS), 19 samples of myxoid/round cell

liposarcoma (MLS), 3 samples of lipoma, 3 samples of well-diﬀerentiated liposarcoma (WDLS), 15 samples of

dediﬀerentiated liposarcoma (DDLS), 15 samples of myxoﬁbrosarcoma (MFS), 6 samples of leiomyosarcoma

(LMS), 3 samples of malignant nerve sheathe tumor (MPNST), 4 samples of ﬁbrosarcoma (FS), and 21

samples of malignant ﬁbrous histiocytoma (MFH). Nakayama et al. (2007) determined from their data that

these 10 types fell into 4 broader groups: (1) SS; (2) MLS; (3) Lipoma, WDLS, and part of DDLS; (4) Spindle

cell and pleomorophic sarcomas including DDLS, MFS, LMS, MPNST, FS, and MFH. Following Witten and

Tibshirani (2011), we restricted our analysis to the ﬁve tumor types having at least 15 observations.

6.2.4. Shipp et al. (2002) Data Set

According to Shipp et al. (2002), approximately 30%-40% of adult non-Hodgkin lymphomas are diﬀuse

large B-cell lymphomas (DLBCLs). However, only a small proportion of DLBCL patients are cured with

modern chemotherapeutic regimens. Several models have been proposed, such as the International Prognostic

Index (IPI), to determine a patient’s curability. These models rely on clinical covariates, such as age, to

determine if the patient can be cured, and the models are often ineﬀective. Shipp et al. (2002) have argued

that researchers need more eﬀective means to determine a patient’s curability. The authors measured 6,817

gene-expression levels from 58 DLBCL patient samples with customized cDNA (lymphochip) microarrays to

investigate the curability of patients treated with cyclophosphamide, adriamycin, vincristine, and prednisone

(CHOP)-based chemotherapy. Among the 58 DLBCL patient samples, 32 are from cured patients while 26

are from patients with fatal or refractory disease.

6.2.5. Singh et al. (2002) Data Set

Singh et al. (2002) have examined 235 radical prostatectomy specimens from surgery patients between

1995 and 1997. The authors used oligonucleotide microarrays containing probes for approximately 12,600

genes and expressed sequence tags. They have reported that 102 of the radical prostatectomy specimens are

of high quality: 52 prostate tumor samples and 50 non-tumor prostate samples.

6.2.6. Tian et al. (2003) Data Set

Tian et al. (2003) investigated the puriﬁed plasma cells from the bone marrow of control patients along

with patients with newly diagnosed multiple myeloma. Expression proﬁles for 12,2625 genes were obtained via

Aﬀymetrix U95Av2 microarrays. The plasma cells were subjected to biochemical and immunohistochemical

analyses to identify molecular determinants of osteolytic lesions. For 36 multiple-myloma patients, focal

bone lesions could not be detected by magnetic resonance imaging (MRI), whereas MRI was used to detect

such lesions in 137 patients.

16

6.2.7. Classiﬁcation Results

Similar to Witten and Tibshirani (2011), we report the average test error rates obtained over 100 random

training-test partitions in Table 1 along with standard deviations of the test error rates in parentheses. The

HDRDA and Guo classiﬁers were superior in classiﬁcation performance for the majority of the simulations.

The HDRDA classiﬁers yielded the best classiﬁcation accuracy on the Chowdary and Shipp data sets. Al-

though the random forest’s accuracy slightly exceeded the HDRDA classiﬁers on the Tian data set, our

proposed classiﬁers outperformed the other competing classiﬁers considered here. Moreover, the HDRDA

classiﬁers yielded comparable performance on ﬁve of the six data sets.

[Insert Table 1 approximately here ]

The average error-rate estimates for the Pang, Tong, and Witten classiﬁers were comparable across all six

data sets. Furthermore, the average error rates for the Pang and Tong classiﬁers were approximately equal

for all data sets except for the Chiaretti dataset. This result suggests that the mean and variance estimators

used in lieu of the MLEs provided little improvement to classiﬁcation accuracies. However, we investigated

the Pang classiﬁer’s poor performance on the Chiaretti data set and determined that its variance estimator

exhibited numerical instability. The classiﬁer’s denominator was approximately zero for both classes and led

to the poor classiﬁcation performance.

The random-forest classiﬁer was competitive when applied to the Chowdary and Singh data sets and

yielded the smallest error rate of the considered classiﬁers on the Tian data set. The fact that the HDRDA

and Guo classiﬁers typically outperformed the random-forest classiﬁer challenges the claim of Fern´andez-

Delgado et al. (2014) that random forests are typically superior. Further studies should be performed to

validate this statement in the small-sample, high-dimensional setting.

Finally, the Pang, Tong, and Witten classiﬁers consistently yielded the largest average error rates across

the six data sets. Given that the standard deviations were relatively large, we hesitate to generalize claims

regarding the ranking of these three classiﬁers in terms of the average error rate. However, the classiﬁers’

error rates and their variability across multiple random partitions of each data set were large enough that

we might question their beneﬁt when applied to real data.

7. Discussion

We have demonstrated that our proposed HDRDA classiﬁer is competitive with and often superior to ran-

dom forests as well as the Witten, Pang, Tong, and Guo classiﬁers. In fact, we have shown that the HDRDA

classiﬁer often yields superior classiﬁcation accuracy when applied to small-sample, high-dimensional data

sets, conﬁrming the assertions of Mai et al. (2012) and Fan et al. (2012) that diagonal classiﬁers often yield

inferior classiﬁcation performance when compared to other classiﬁcation methods. Furthermore, we have

demonstrated that HDRDA classiﬁers are more robust to the presence of outliers than the diagonal clas-

siﬁers despite their rapid computational performance and their reduction in the number of parameters to

estimate.

17

We also considered the popular penalized linear discriminant analysis from Witten and Tibshirani (2011)

because it was speciﬁcally designed for high-dimensional gene-expression data. We had expected its clas-

siﬁcation performance to be competitive within our classiﬁcation study and perhaps superior. Contrarily,

our empirical studies suggest that the classiﬁer is sensitive to outliers and unable to achieve comparable

results with other classiﬁers designed for small-sample, high-dimensional data. Also, despite the claims of

Fern´andez-Delgado et al. (2014) that random forests are typically superior to other classiﬁers, we observed

that they were indeed competitive but were typically outperformed by classiﬁers developed for small-sample,

high-dimensional data.

We demonstrated that our HDRDA implementation in the sparsediscrim R package can be used in

practice with high-dimensional data sets. In our timing comparisons, we showed that HDRDA model selection

could be employed on data sets with p = 5000 in 2.979 seconds on average. Contrarily, the RDA classiﬁer

implemented in the klaR R package required 24.933 minutes on average to perform model selection on data

sets with p = 5000. Given that the RDA classiﬁer has been shown to have excellent performance in the

high-dimensional setting (Webb and Copsey, 2011) but is limited by its computationally intense model-

selection procedure, our work replaces the RDA classiﬁer for high-dimensional data in practice. This result

is reassuring because the RDA classiﬁer remains widely popular in the literature. In fact, variants of the

RDA classiﬁer have been applied to microarray data (Ching et al., 2012; Li and Wu, 2012; Tai and Pan,

2007; Guo et al., 2007), facial recognition (Zhang et al., 2010; Dai and Yuen, 2007; Lu et al., 2005; Pima and

Aladjem, 2004; Lu and Plataniotis, 2003), handwritten digit recognition (Bouveyron et al., 2007), remote

sensing (Tadjudin and Landgrebe, 1999), seismic detection (Anderson, 2002), and chemical spectra (Wu

et al., 1996; Aeberhard et al., 1993).

The dimension reduction employed in this paper has reduced the dimension to rank( (cid:98)Σ) = q. An inter-

esting extension of our work would reduce the dimension q further to a lower dimension qL < q, perhaps

using a criterion similar to that of principal components analysis. While unclear whether the classiﬁcation

performance would improve via such a method, the eﬃciency of the model selection would certainly improve.

Moreover, if qL = 2 or 3, low-dimensional graphical displays of high-dimensional data could be obtained.

We thank Mrs. Joy Young for her numerous recommendations that enhanced the quality of our writing.

References

Aeberhard, S., Coomans, D., Vel, O. D., 1993. Improvements to the classiﬁcation performance of RDA.

Journal of Chemometrics 7 (2), 99–115.

Anderson, D. N., Aug. 2002. Application of Regularized Discrimination Analysis to Regional Seismic Event

Identiﬁcation. Bulletin of the Seismological Society of America 92 (6), 2391–2399.

Bensmail, H., Celeux, G., Dec. 1996. Regularized Gaussian Discriminant Analysis through Eigenvalue De-

composition. Journal of the American Statistical Association 91 (436), 1743–1748.

Bouveyron, C., Girard, S., Schmid, C., Oct. 2007. High-Dimensional Discriminant Analysis. Communications

in Statistics - Theory and Methods 36 (14), 2607–2623.

18

Breiman, L., 2001. Random Forests - Springer. Machine Learning 45 (1), 5–32.

Chiaretti, S., Li, X., Gentleman, R., Vitale, A., Vignetti, M., Mandelli, F., Ritz, J., Foa, R., 2004. Gene

expression proﬁle of adult T-cell acute lymphocytic leukemia identiﬁes distinct subsets of patients with

diﬀerent response to therapy and survival. Blood 103 (7), 2771–2778.

Ching, W.-K., Chu, D., Liao, L.-Z., Wang, X., Jul. 2012. Regularized orthogonal linear discriminant analysis.

Pattern Recognition 45 (7), 2719–2732.

Chowdary, D., Lathrop, J., Skelton, J., Curtin, K., Briggs, T., Zhang, Y., Yu, J., Wang, Y., Mazumder,

A., Feb. 2006. Prognostic Gene Expression Signatures Can Be Measured in Tissues Collected in RNAlater

Preservative. The Journal of Molecular Diagnostics 8 (1), 31–39.

Dai, D.-Q., Yuen, P. C., Aug. 2007. Face recognition by regularized discriminant analysis. IEEE transactions

on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and

Cybernetics Society 37 (4), 1080–1085.

Dudoit, S., Fridlyand, J., Speed, T. P., Mar. 2002. Comparison of Discrimination Methods for the Classiﬁ-

cation of Tumors Using Gene Expression Data. Journal of the American Statistical Association 97 (457),

Fan, J., Feng, Y., Tong, X., Apr. 2012. A road to classiﬁcation in high dimensional space: the regularized

optimal aﬃne discriminant. Journal of the Royal Statistical Society: Series B (Statistical Methodology)

Fern´andez-Delgado, M., Cernadas, E., Barro, S., Amorim, D., Jan. 2014. Do we need hundreds of classiﬁers

to solve real world classiﬁcation problems? The Journal of Machine Learning Research 15 (1), 3133–3181.

Friedman, J. H., 1989. Regularized Discriminant Analysis. Journal of the American Statistical Association

Guo, Y., Hastie, T., Tibshirani, R., Jan. 2007. Regularized linear discriminant analysis and its application

in microarrays. Biostatistics 8 (1), 86–100.

Halbe, Z., Aladjem, M., Nov. 2007. Regularized mixture discriminant analysis. Pattern Recognition Letters

Harville, D. A., 2008. Matrix Algebra from a Statistician’s Perspective. Springer, New York.

Hastie, T., Tibshirani, R., Friedman, J., Dec. 2008. The Elements of Statistical Learning, 2nd Edition. Data

Mining, Inference, and Prediction. Springer New York, New York, NY.

Hoerl, A. E., Kennard, R. W., Feb. 1970. Ridge Regression: Biased Estimation for Nonorthogonal Problems.

Technometrics 12 (1), 55–67.

Kollo, T., von Rosen, D., 2005. Advanced Multivariate Statistics with Matrices. Vol. 579 of Mathematics

and Its Applications (New York). Springer, Dordrecht.

77–87.

74 (4), 745–771.

84 (405), 165–175.

28 (15), 2104–2115.

19

Li, R., Wu, B., Aug. 2012. Sparse regularized discriminant analysis with application to microarrays. Com-

putational biology and chemistry 39, 14–19.

Lu, J., Plataniotis, K. N., 2003. Regularized discriminant analysis for the small sample size problem in face

recognition. Pattern Recognition Letters.

Lu, J., Plataniotis, K. N., Venetsanopoulos, A. N., 2005. Regularization studies of linear discriminant analysis

in small sample size scenarios with application to face recognition. Pattern Recognition Letters 26 (2),

Mai, Q., Zou, H., Yuan, M., Feb. 2012. A direct approach to sparse discriminant analysis in ultra-high

181–191.

dimensions. Biometrika 99 (1), 29–42.

nition Letters 16 (3), 267–275.

Mkhadri, A., Mar. 1995. Shrinkage parameter for the modiﬁed linear discriminant analysis. Pattern Recog-

Mkhadri, A., Celeux, G., Nasroallah, A., 1997. Regularization in discriminant analysis: an overview. Com-

putational Statistics and Data Analysis 23 (3), 403–423.

Murphy, K. P., Aug. 2012. Machine Learning: A Probabilistic Perspective. The MIT Press, Cambridge,

Massachusetts.

Nakayama, R., Nemoto, T., Takahashi, H., Ohta, T., Kawai, A., Seki, K., Yoshida, T., Toyama, Y., Ichikawa,

H., Hasegawa, T., Apr. 2007. Gene expression analysis of soft tissue sarcomas: characterization and

reclassiﬁcation of malignant ﬁbrous histiocytoma. Nature 20 (7), 749–759.

Pang, H., Tong, T., Zhao, H., Mar. 2009. Shrinkage-based Diagonal Discriminant Analysis and Its Applica-

tions in High-Dimensional Data. Biometrics 65 (4), 1021–1029.

Pima, I., Aladjem, M., 2004. Regularized discriminant analysis for face recognition. Pattern Recognition

37 (9), 1945–1948.

581–596.

Berkeley, pp. 601–620.

Interscience.

Ramey, J., Young, P. D., 2013. A comparison of regularization methods applied to the linear discriminant

function with high-dimensional microarray data. Journal of Statistical Computation and Simulation 83 (3),

Rao, C. R., Mitra, S. K., 1971. Generalized inverse of a matrix and its applications. In: Proceedings of

the Sixth Berkeley Symposium on Mathematical Statistics and Probability. University of California Press,

Seber, G. A. F., Aug. 2004. Multivariate Observations. Wiley Series in Probability and Statistics. Wiley-

Shipp, M. A., Ross, K. N., Tamayo, P., Weng, A. P., Kutok, J. L., Aguiar, R. C. T., Gaasenbeek, M., Angelo,

M., Reich, M., Pinkus, G. S., Ray, T. S., Koval, M. A., Last, K. W., Norton, A., Lister, T. A., Mesirov,

J., Neuberg, D. S., Lander, E. S., Aster, J. C., Golub, T. R., Jan. 2002. Diﬀuse large B-cell lymphoma

20

outcome prediction by gene-expression proﬁling and supervised machine learning. Nature Medicine 8 (1),

68–74.

Singh, D., Febbo, P. G., Ross, K., Jackson, D. G., Manola, J., Ladd, C., Tamayo, P., Renshaw, A. A.,

D’Amico, A. V., Richie, J. P., Lander, E. S., Loda, M., Kantoﬀ, P. W., Golub, T. R., Sellers, W. R., Mar.

2002. Gene expression correlates of clinical prostate cancer behavior. Cancer Cell 1 (2), 203–209.

Srivastava, M. S., Kubokawa, T., 2007. Comparison of discrimination methods for high dimensional data.

Journal of the Japan Statistical Society 37 (1), 123–134.

Tadjudin, S., Landgrebe, D. A., Jul. 1999. Covariance estimation with limited training samples. IEEE

Transactions on Geoscience and Remote Sensing 37 (4), 2113–2118.

Tai, F., Pan, W., Dec. 2007. Incorporating prior knowledge of gene functional groups into regularized dis-

criminant analysis of microarray data. Bioinformatics 23 (23), 3170–3177.

Tian, E., Zhan, F., Walker, R., Rasmussen, E., Ma, Y., Barlogie, B., Shaughnessy, Jr., J. D., Dec. 2003.

The Role of the Wnt-Signaling Antagonist DKK1 in the Development of Osteolytic Lesions in Multiple

Myeloma. New England Journal of Medicine 349 (26), 2483–2494.

Tong, T., Chen, L., Zhao, H., Feb. 2012. Improved mean estimation and its application to diagonal discrim-

inant analysis. Bioinformatics 28 (4), 531–537.

Webb, A. R., Copsey, K. D., Sep. 2011. Statistical Pattern Recognition, 3rd Edition. John Wiley & Sons,

Chichester, West Sussex, UK.

Witten, D. M., Tibshirani, R., Aug. 2011. Penalized classiﬁcation using Fisher’s linear discriminant. Journal

of the Royal Statistical Society: Series B (Statistical Methodology) 73 (5), 753–772.

Wu, W., Mallet, Y., Walczak, B., Penninckx, W., Massart, D. L., Heuerding, S., Erni, F., Aug. 1996.

Comparison of regularized discriminant analysis, linear discriminant analysis, and quadratic discriminant

analysis applied to NIR data. Analytica Chimica Acta 329 (3), 257–265.

Xu, P., Brock, G. N., Parrish, R. S., Mar. 2009. Modiﬁed linear discriminant analysis approaches for classiﬁ-

cation of high-dimensional microarray data. Computational Statistics and Data Analysis 53 (5), 1674–1687.

Ye, J., Ji, S., Nov. 2009. Discriminant Analysis for Dimensionality Reduction: An Overview of Recent

Developments. Biometrics: Theory, Methods, and Applications. John Wiley & Sons, Inc., Hoboken, NJ,

USA.

Ye, J., Wang, T., 2006. Regularized Discriminant Analysis for High Dimensional, Low Sample Size Data. In:

The 12th ACM SIGKDD International Conference. ACM Press, New York, New York, USA, p. 454.

Zhang, Z., Dai, G., Xu, C., Jordan, M. I., Mar. 2010. Regularized Discriminant Analysis, Ridge Regression

and Beyond. The Journal of Machine Learning Research 11.

21

Figure 1: Contours of ﬁve multivariate normal populations as a function of the pooling parameter λ.

22

Figure 2: Timing comparisons (in seconds) between HDRDA and RDA classiﬁers.

23

Figure 3: Distribution of ratios of mean RDA runtime to mean HDRDA runtime across 1000 bootstrap replications.

24

Figure 4: Average classiﬁcation error rates as a function of the contamination probability (cid:15). Approximate standard errors were

no greater than 0.022.

25

Figure 5: Average classiﬁcation error rates as a function of the number of features p. Approximate standard errors were no

greater than 0.022.

26

Classiﬁer

Chiaretti

Chowdary

Nakayama

Shipp

Singh

Tian

Guo

0.111 (0.044)

0.056 (0.051)

0.208 (0.061)

0.086 (0.063)

0.089 (0.055)

0.268 (0.082)

HDRDA Convex

0.115 (0.044)

0.035 (0.026)

0.208 (0.066)

0.073 (0.057)

0.111 (0.059)

0.229 (0.049)

HDRDA Ridge

0.118 (0.050)

0.033 (0.022)

0.208 (0.070)

0.072 (0.065)

0.099 (0.046)

0.225 (0.050)

Pang

0.663 (0.062)

0.197 (0.091)

0.227 (0.062)

0.192 (0.091)

0.221 (0.095)

0.267 (0.054)

Random Forest

0.124 (0.053)

0.045 (0.028)

0.232 (0.063)

0.135 (0.078)

0.093 (0.045)

0.206 (0.044)

Tong

Witten

0.195 (0.068)

0.197 (0.091)

0.227 (0.062)

0.192 (0.091)

0.221 (0.095)

0.267 (0.054)

0.194 (0.068)

0.197 (0.091)

0.232 (0.068)

0.193 (0.092)

0.221 (0.095)

0.264 (0.053)

Table 1: The average of the test error rates obtained on gene-expression data sets over 100 random training-test partitions.

Standard deviations of the test error rates are given in the parentheses. The classiﬁer with the minimum average error rate for

each data set is in bold.

27

High-Dimensional Regularized Discriminant Analysis

John A. Rameya, Caleb K. Steinb, Phil D. Youngc, Dean M. Youngd

aNovi Labs
bMyeloma Institute, University of Arkansas for Medical Sciences
cDepartment of Management and Information Systems, Baylor University
dDepartment of Statistical Science, Baylor University

7
1
0
2
 
b
e
F
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
2
8
1
1
0
.
2
0
6
1
:
v
i
X
r
a

Abstract

Regularized discriminant analysis (RDA), proposed by Friedman (1989), is a widely popular classiﬁer that

lacks interpretability and is impractical for high-dimensional data sets. Here, we present an interpretable

and computationally eﬃcient classiﬁer called high-dimensional RDA (HDRDA), designed for the small-

sample, high-dimensional setting. For HDRDA, we show that each training observation, regardless of class,

contributes to the class covariance matrix, resulting in an interpretable estimator that borrows from the

pooled sample covariance matrix. Moreover, we show that HDRDA is equivalent to a classiﬁer in a reduced-

feature space with dimension approximately equal to the training sample size. As a result, the matrix

operations employed by HDRDA are computationally linear in the number of features, making the classiﬁer

well-suited for high-dimensional classiﬁcation in practice. We demonstrate that HDRDA is often superior

to several sparse and regularized classiﬁers in terms of classiﬁcation accuracy with three artiﬁcial and six

real high-dimensional data sets. Also, timing comparisons between our HDRDA implementation in the

sparsediscrim R package and the standard RDA formulation in the klaR R package demonstrate that as

the number of features increases, the computational runtime of HDRDA is drastically smaller than that of

Keywords: Regularized discriminant analysis, High-dimensional classiﬁcation, Covariance-matrix

regularization, Singular value decomposition, Multivariate analysis, Dimension reduction

2010 MSC: 62H30, 65F15, 65F20, 65F22

RDA.

1. Introduction

In this paper, we consider the classiﬁcation of small-sample, high-dimensional data, where the number of

features p exceeds the training sample size N . In this setting, well-established classiﬁers, such as linear dis-

criminant analysis (LDA) and quadratic discriminant analysis (QDA), become incalculable because the class

and pooled covariance matrix estimators are singular (Murphy, 2012; Bouveyron, Girard, and Schmid, 2007;

Mkhadri, Celeux, and Nasroallah, 1997). To improve the accuracy of the estimation of the class covariance

matrices estimated in the QDA classiﬁer and to ensure that the covariance matrix estimators are nonsin-

gular, Friedman (1989) proposed the regularized discriminant analysis (RDA) classiﬁer by incorporating a

weighted average of the pooled sample covariance matrix and the class sample covariance matrix. To further

improve the accuracy of the estimation of the class covariance matrix and to stabilize its inverse, Friedman

(1989) also included a regularization component by shrinking the covariance matrix estimator towards the

Preprint submitted to Elsevier

February 7, 2017

identity matrix, which yields a nonsingular estimator following the well-known ridge-regression approach of

Hoerl and Kennard (1970). Despite its popularity, the “borrowing” operation employed in the RDA classi-

ﬁer lacks interpretability (Bensmail and Celeux, 1996). Furthermore, the RDA classiﬁer is impractical for

high-dimensional data sets because it computes the inverse and determinant of the covariance matrices for

each class. Both matrix calculations are computationally expensive because the number of operations grows

at a polynomial rate in the number of features. Moreover, the model selection of the RDA classiﬁer’s two

tuning parameters is computationally burdensome because the matrix inverse and determinant of each class

covariance matrix are computed across multiple cross-validation folds for each candidate tuning-parameter

Here, we present the high-dimensional RDA (HDRDA) classiﬁer, which is intended for the case when

p > N . We reparameterize the RDA classiﬁer similar to that of Hastie, Tibshirani, and Friedman (2008)

and Halbe and Aladjem (2007) and employ a biased covariance-matrix estimator that partially pools the

individual sample covariance matrices from the QDA classiﬁer with the pooled sample covariance matrix from

the LDA classiﬁer. We then shrink the resulting covariance-matrix estimator towards a scaled identity matrix

to ensure positive deﬁniteness. We show that the pooling parameter in the HDRDA classiﬁer determines

the contribution of each training observation to the estimation of each class covariance matrix, enabling

interpretability that has been previously lacking with the RDA classiﬁer (Bensmail and Celeux, 1996). Our

parameterization diﬀers from that of Hastie, Tibshirani, and Friedman (2008) and that of Halbe and Aladjem

(2007) in that our formulation allows the ﬂexibility of various covariance-matrix estimators proposed in the

literature, including a variety of ridge-like estimators, such as the one proposed by Srivastava and Kubokawa

pair.

(2007).

Next, we establish that the matrix operations corresponding to the null space of the pooled sample

covariance matrix are redundant and can be discarded from the HDRDA decision rule without loss of

classiﬁcatory information when we apply reasoning similar to that of Ye and Wang (2006). As a result, we

achieve a substantial reduction in dimension such that the matrix operations used in the HDRDA classiﬁer are

computationally linear in the number of features. Furthermore, we demonstrate that the HDRDA decision

rule is invariant to adjustments to the approximately p − N zero eigenvalues, so that the decision rule in the

original feature space is equivalent to a decision rule in a lower dimension, such that matrix inverses and

determinants of relatively small matrices can be rapidly computed. Finally, we show that several shrinkage

methods that are special cases of the HDRDA classiﬁer have no eﬀect on the approximately p − N zero

eigenvalues of the covariance-matrix estimators when p > N . Such techniques include work from Srivastava

and Kubokawa (2007), Rao and Mitra (1971), and other methods studied by Ramey and Young (2013) and

Xu, Brock, and Parrish (2009).

We also provide an eﬃcient algorithm along with pseudocode to estimate the HDRDA classiﬁer’s tuning

parameters in a grid search via cross-validation. Timing comparisons between our HDRDA implementation

in the sparsediscrim R package available on CRAN and the standard RDA formulation in the klaR R

package demonstrate that as the number of features increases, the computational runtime of the HDRDA

classiﬁer is drastically smaller than that of RDA. In fact, when p = 5000, we show that the HDRDA classiﬁer

2

is 502.786 times faster on average than the RDA classiﬁer. In this scenario, the HDRDA classiﬁer’s model

selection requires 2.979 seconds on average, while that of the RDA classiﬁer requires 24.933 minutes on

average.

Finally, we study the classiﬁcation performance of the HDRDA classiﬁer on six real high-dimensional data

sets along with a simulation design that generalizes the experiments initially conducted by Guo et al. (2007).

We demonstrate that the HDRDA classiﬁer often attains superior classiﬁcation accuracy to several recent

classiﬁers designed for small-sample, high-dimensional data from Tong, Chen, and Zhao (2012), Witten and

Tibshirani (2011), Pang, Tong, and Zhao (2009), and Guo et al. (2007). We also include as a benchmark

the random forest from Breiman (2001) because Fern´andez-Delgado, Cernadas, Barro, and Amorim (2014)

have concluded that the random forest is often superior to other classiﬁers in benchmark studies. We show

that our proposed classiﬁer is competitive and often outperforms the random forest in terms of classiﬁcation

accuracy in the small-sample, high-dimensional setting.

The remainder of this paper is organized as follows. In Section 2 we introduce the classiﬁcation problem

and necessary notation to describe our contributions. In Section 3 we present the HDRDA classiﬁer along

with its interpretation. In Section 4, we provide properties of the HDRDA classiﬁer and a computationally

eﬃcient model-selection procedure. In Section 5, we compare the model-selection timings of the HDRDA

and RDA classiﬁers.

In Section 6 we describe our simulation studies of artiﬁcial and real data sets and

examine the experimental results. We conclude with a brief discussion in Section 7.

2. Preliminaries

2.1. Notation

To facilitate our discussion of covariance-matrix regularization and high-dimensional classiﬁcation, we
require the following notation. Let Ra×b denote the matrix space of all a × b matrices over the real ﬁeld
R. Denote by Im the m × m identity matrix, and let 0m×p be the m × p matrix of zeros, such that 0m
is understood to denote 0m×m. Deﬁne 1m ∈ Rm×1 as a vector of ones. Let AT , A+, and N (A) denote
the transpose, the Moore-Penrose pseudoinverse, and the null space of A ∈ Rm×p, respectively. Denote
by R>
p×p denote the cone of real p × p
positive-semideﬁnite matrices. Let V ⊥ denote the orthogonal complement of a vector space V ⊂ Rp×1. For
c ∈ R, let c+ = 1/c if c (cid:54)= 0 and 0 otherwise.

p×p the cone of real p × p positive-deﬁnite matrices. Similarly, let R≥

2.2. Discriminant Analysis

In discriminant analysis we wish to assign an unlabeled vector x ∈ Rp×1 to one of K unique, known
classes by constructing a classiﬁer from N training observations. Let xi = (xi1, . . . , xip) ∈ Rp×1 be the ith

observation (i = 1, . . . , N ) with true, unique membership yi ∈ {ω1, . . . , ωK}. Denote by nk the number of
training observations realized from class k, such that (cid:80)K
from a mixture distribution p(x) = (cid:80)K

k=1 nk = N . We assume that (xi, yi) is a realization
k=1 p(x|ωk)p(ωk), where p(x|ωk) is the probability density function
(PDF) of the kth class and p(ωk) is the prior probability of class membership of the kth class. We further

assume p(ωk) = p(ωl), 1 ≤ k, l ≤ K, k (cid:54)= l.

3

The QDA classiﬁer is the optimal Bayesian decision rule with respect to a 0 − 1 loss function when
p(x|ωk) is the PDF of the multivariate normal distribution with known mean vectors µk ∈ Rp×1 and known
covariance matrices Σk ∈ R>

p×p, k = 1, 2, . . . , K. Because µk and Σk are typically unknown, we assign an

unlabeled observation x to class ωk with the sample QDA classiﬁer

DQDA(x) = arg min

(x − ¯xk)T (cid:98)Σ−1

k (x − ¯xk) + log | (cid:98)Σk|,

k

where ¯xk and (cid:98)Σk are the maximum-likelihood estimators (MLEs) of µk and Σk, respectively. If we assume

further that Σk = Σ, k = 1, . . . , K, then the pooled sample covariance matrix (cid:98)Σ is substituted for (cid:98)Σk in

(1), where

(1)

(2)

(cid:98)Σ = N −1

nk (cid:98)Σk

K
(cid:88)

k=1

is the MLE for Σ. Here, (1) reduces to the sample LDA classiﬁer. We omit the log-determinant because it

is constant across the K classes.

The smallest eigenvalues of (cid:98)Σk and the directions associated with their eigenvectors can highly inﬂuence

the classiﬁer in (1).

In fact, the eigenvalues of (cid:98)Σk are well-known to be biased if p ≥ nk such that the

smallest eigenvalues are underestimated (Seber, 2004). Moreover, if p > nk, then rank( (cid:98)Σk) ≤ nk, which

implies that at least p − nk eigenvalues of (cid:98)Σk are zero. Furthermore, although more feature information is
available to discriminate among the K classes, if p > nk, (1) is incalculable because (cid:98)Σ−1

k does not exist.

Several regularization methods, such as the methods considered by Xu et al. (2009), Guo et al. (2007),

and Mkhadri (1995), have been proposed in the literature to adjust the eigenvalues of (cid:98)Σk so that (1) is
calculable and provides reduced variability for (cid:98)Σ−1
applies a shrinkage factor γ > 0, so that

k . A common form of the covariance-matrix regularization

(cid:98)Σk(γ) = (cid:98)Σk + γIp,

(3)

similar to a method employed in ridge regression (Hoerl and Kennard, 1970). Equation (3) eﬀectively shrinks

the sample covariance matrix (cid:98)Σk toward Ip, thereby increasing the eigenvalues of (cid:98)Σk by γ. Speciﬁcally,

the zero eigenvalues are replaced with γ, so that (3) is positive deﬁnite. For additional covariance-matrix

regularization methods, see Ramey and Young (2013), Xu et al. (2009), and Ye and Ji (2009).

3. High-Dimensional Regularized Discriminant Analysis

Here, we deﬁne the HDRDA classiﬁer by ﬁrst formulating the covariance-matrix estimator (cid:98)Σk(λ) and

demonstrating its clear interpretation as a linear combination of the crossproducts of the training observations

centered by their respective class sample means. We deﬁne the convex combination

(cid:98)Σk(λ) := (1 − λ) (cid:98)Σk + λ (cid:98)Σ,

k = 1, . . . , K,

(4)

4

where λ ∈ [0, 1] is the pooling parameter. By rewriting (4) in terms of the observations xi, i = 1, . . . , N ,

each centered by its class sample mean, we attain a clear interpretation of (cid:98)Σk(λ). That is,

(cid:98)Σk(λ) =

1 − λ +

(cid:98)Σk +

nk(cid:48) (cid:98)Σk(cid:48)

(cid:18)

(cid:19)

λnk
N

λ
N

K
(cid:88)

k(cid:48)=1
k(cid:48)(cid:54)=k

I(yi = k)xixT

i +

I(yi (cid:54)= k)xixT
i

λ
N

N
(cid:88)

i=1

(cid:18) 1 − λ
nk

+

λ
N

(cid:19) N
(cid:88)

i=1

cik(λ)xixT
i ,

=

=

N
(cid:88)

i=1

where cik(λ) = λN −1 + (1 − λ)n−1

k I(yi = k). From (5), we see that λ weights the contribution of each of
the N observations in estimating Σk from all K classes rather than using only the nk observations from a

single class. As a result, we can interpret (5) as a covariance-matrix estimator that borrows from (cid:98)Σ in (2)

to estimate Σk.

In Figure 1 we plot the contours of ﬁve multivariate normal populations for λ = 0 with unequal covariance

matrices. As λ approaches 1, the contours become more similar, resulting in identical contours for λ = 1. Be-

low, we show that the pooling operation is advantageous in increasing the rank of each (cid:98)Σk(λ) from rank( (cid:98)Σk)

to rank( (cid:98)Σ) for 0 < λ ≤ 1. Notice that if λ = 0, then the observations from the remaining K − 1 classes do

not contribute to the estimation of Σk, corresponding to (cid:98)Σk. Furthermore, if λ = 1, the weights cik(λ) in (5)
reduce to 1/N , corresponding to (cid:98)Σ. For brevity, when λ = 1, we deﬁne X = [(cid:112)c1k(1)xT
N ]T
such that (cid:98)Σ = N −1X T X. Similarly, for λ = 0, we deﬁne Xk = [(cid:112)c1k(0)xT
N ]T such that
(cid:98)Σk = n−1

1 , . . . , (cid:112)cN k(0)xT

1 , . . . , (cid:112)cN k(1)xT

k X T

k Xk.

[Insert Figure 1 approximately here ]

As we have discussed above, several eigenvalue adjustment methods have been proposed that increase

eigenvalues (approximately) equal to 0. To further improve the estimation of Σk and to stabilize the esti-

mator’s inverse, we deﬁne the eigenvalue adjustment of (4) as

˜Σk := αk (cid:98)Σk(λ) + γIp,

where αk ≥ 0 and γ ≥ 0 is an eigenvalue-shrinkage constant. Thus, the pooling parameter λ controls

the amount of estimation information borrowed from (cid:98)Σ to estimate Σk, and the shrinkage parameter γ

determines the degree of eigenvalue shrinkage. The choice of αk allows for a ﬂexible formulation of covariance-

matrix estimators. For instance, if αk = 1, k = 1, . . . , K, then (6) resembles (3). Similarly, if αk = 1 − γ,

then (6) has a form comparable to the RDA classiﬁer from Friedman (1989). Substituting (6) into (1), we

deﬁne the HDRDA classiﬁer as

DHDRDA(x) = arg min

(x − ¯xk)T ˜Σ+

k (x − ¯xk) + log | ˜Σk|.

k

For γ > 0, ˜Σk is nonsingular such that ˜Σ−1
k in (7). If γ = 0, we explicitly set
k
| ˜Σk| equal to the product of the positive eigenvalues of ˜Σk. Following Friedman (1989), we select λ and γ

can be substituted for ˜Σ+

(5)

(6)

(7)

5

from a grid of candidate models via cross-validation (Hastie et al., 2008). We provide an implementation of

(7) in the hdrda function contained in the sparsediscrim R package, which is available on CRAN.

The choice of αk in (6) is one of convenience and allows the ﬂexibility of various covariance-matrix

estimators proposed in the literature. In practice, we generally are not interested in estimating αk because

the estimation of K additional tuning parameters via cross-validation is counterproductive to our goal of

computational eﬃciency. For appropriate values of αk, the HDRDA covariance-matrix estimator includes or

resembles a large family of estimators. Notice that if αk = 1 and λ = 1, (6) is equivalent to the standard

ridge-like covariance-matrix estimator in (3). Other estimators proposed in the literature can be obtained

when one selects γ accordingly. For instance, with γ = tr{ (cid:98)Σ}/ min(N, p), we obtain the estimator from

Srivastava and Kubokawa (2007).

When αk = 1 − γ, (6) resembles the biased covariance-matrix estimator

(cid:98)Σk(λ, γ) = (1 − γ) (cid:98)Σ(RDA)

k

(λ) + γ

(cid:111)

(λ)

tr

(cid:110)
(cid:98)Σ(RDA)
p

k

Ip

(8)

employed in the RDA classiﬁer, where (cid:98)Σ(RDA)
tion parameter that controls the shrinkage of (8) towards Ip weighted by the average of the eigenvalues of
(cid:98)Σ(RDA)
ﬁer is impractical for high-dimensional data because the inverse and determinant of (8) must be calculated

(λ). Despite the similarity of (8) to the HDRDA covariance-matrix estimator in (6), the RDA classi-

(λ) is a pooled estimator of Σk and γ ∈ [0, 1] is a regulariza-

k

k

when substituted into (1). Furthermore, (8) has no clear interpretation.

4. Properties of the HDRDA Classiﬁer

Next, we establish properties of the covariance-matrix estimator and the decision rule employed in the

HDRDA classiﬁer. By doing so, we demonstrate that (7) lends itself to a more eﬃcient calculation. We

decompose (7) into a sum of two components, where the ﬁrst summand consists of matrix operations applied

to low-dimensional matrices and the second summand corresponds to the null space of (cid:98)Σ in (2). We show that

the matrix operations performed on the null space of (cid:98)Σ yield constant quadratic forms across all classes and

can be omitted. For p (cid:29) N , the constant component involves determinants and inverses of high-dimensional

matrices, and by ignoring these calculations, we achieve a substantial reduction in computational costs.

Furthermore, a byproduct is that adjustments to the associated eigenvalues have no eﬀect on (7). Lastly, we

utilize the singular value decomposition to eﬃciently calculate the eigenvalue decomposition of (cid:98)Σ, further

reducing the computational costs of the HDRDA classiﬁer.

First, we require the following relationship regarding the null spaces of (cid:98)Σk(λ), (cid:98)Σ, and (cid:98)Σk.

Lemma 1. Let (cid:98)Σk and (cid:98)Σ be the MLEs of Σk and Σ, respectively. Let (cid:98)Σk(λ) be deﬁned as in (4). Then,

N { (cid:98)Σk(λ)} ⊂ N ( (cid:98)Σ) ⊂ N ( (cid:98)Σk), k = 1, . . . , K.

Proof. Let z ∈ N { (cid:98)Σk(λ)} for some k = 1, . . . , K. Hence, 0 = zT (cid:98)Σk(λ)z = (1 − λ)zT (cid:98)Σkz + λzT (cid:98)Σz.
Because (cid:98)Σk, (cid:98)Σ ∈ R≥
Now, suppose z ∈ N ( (cid:98)Σ). Similarly, we have that 0 = zT (cid:98)Σz = N −1 (cid:80)K
z ∈ N ( (cid:98)Σk) because (cid:98)Σk ∈ R≥

p×p, we have z ∈ N ( (cid:98)Σ) and z ∈ N ( (cid:98)Σk). In particular, we have that N { (cid:98)Σk(λ)} ⊂ N ( (cid:98)Σ).
k=1 nkzT (cid:98)Σkz, which implies that

p×p. Therefore, N ( (cid:98)Σ) ⊂ N ( (cid:98)Σk).

6

In Lemma 2 below, we derive an alternative expression for ˜Σk in terms of the matrix of eigenvectors of (cid:98)Σ.
p×p is the diagonal matrix of eigenvalues

Let (cid:98)Σ = U DU T be the eigendecomposition of (cid:98)Σ such that D ∈ R≥
of (cid:98)Σ with

D =





Dq

0

0

0p−q



 ,

Dq ∈ R>

q×q is the diagonal matrix consisting of the positive eigenvalues of (cid:98)Σ, the columns of U ∈ Rp×p are
the corresponding orthonormal eigenvectors of (cid:98)Σ, and rank( (cid:98)Σ) = q. Then, we partition U = (U1, U2) such
that U1 ∈ Rp×q and U2 ∈ Rp×(p−q).

Lemma 2. Let (cid:98)Σ = U DU T be the eigendecomposition of (cid:98)Σ as above, and suppose that rank( (cid:98)Σ) = q ≤ p.

Then, we have

where

Hence,

˜Σk = U





Wk

0

0

γIp−q


 U T ,

k = 1, . . . , K,

Wk = αk{(1 − λ)U T

1 (cid:98)ΣkU1 + λDq} + γIq.

(9)

(10)

Proof. From Lemma 1, the columns of U2 span the null space of (cid:98)Σk, which implies that (cid:98)ΣkU2 = 0p×(p−q).

U T (cid:98)ΣkU =





U T

1 (cid:98)ΣkU1

0

0

0p−q



 ,

k = 1, . . . , K.

Thus, U T ˜ΣkU = αk{(1 − λ)U T (cid:98)ΣkU + λD} + γIp, and (9) holds because U is orthogonal.

As an immediate consequence of Lemma 2, we have the following corollary.

Corollary 1. Let (cid:98)Σk(λ) be deﬁned as in (4). Then, for λ ∈ (0, 1], rank{ (cid:98)Σk(λ)} = q, k = 1, . . . , K.

Proof. The proof follows when we set γ = 0 in Lemma 2.

Thus, by incorporating each xi into the estimation of Σk, we increase the rank of (cid:98)Σk(λ) to q ≈ N if

λ (cid:54)= 0. Next, we provide an essential result that enables us to prove that (7) is invariant to adjustments to
the eigenvalues of ˜Σk corresponding to the null space of (cid:98)Σ.

Lemma 3. Let U2 be deﬁned as above. Then, for all x ∈ Rp×1, U T
where k (cid:54)= k(cid:48).

2 (x − ¯xk) = U T

2 (x − ¯xk(cid:48)), 1 ≤ k, k(cid:48) ≤ K,

2 ∈ C( (cid:98)Σ)⊥ (Kollo and von Rosen, 2005, Lemma 1.2.5). Now, because xi ∈ C( (cid:98)Σ) (i = 1, . . . , N ), U T

Proof. Let x ∈ Rp×1, and suppose that 1 ≤ k, k(cid:48) ≤ K. Recall that U2 ∈ N ( (cid:98)Σ), which implies that
U T
2 xi =
0p−q. Hence, 0p−q = (cid:80)N
2 ( ¯xk − ¯xk(cid:48)), where βi = (nknk(cid:48))−1{I(yi = k)nk(cid:48) − I(yi = k(cid:48))nk}.
Therefore, U T

2 xi = U T

i=1 βiU T

2 (x − ¯xk) = U T

2 (x − ¯xk(cid:48)).

7

We now present our main result, where we decompose (7) and show that the term requiring the largest

computational costs does not contribute to the classiﬁcation of an unlabeled observation performed using

Lemma 3. Hence, we reduce (7) to an equivalent, more computationally eﬃcient decision rule.

Theorem 1. Let ˜Σk and Wk be deﬁned as in (9) and (10), respectively, and let U1 be deﬁned as above.

Then, the decision rule in (7) is equivalent to

DHDRDA(x) = arg min

(x − ¯xk)T U1W −1

k U T

1 (x − ¯xk) + log |Wk|.

(11)

k

Proof. From (9), we have that





W −1
k

˜Σ+

k = U

0


 U T

0

γ+Ip−q

and | ˜Σk| = γp−q|Wk|, k = 1, . . . , K. Therefore, for all x ∈ Rp×1, we have that

(x − ¯xk)T ˜Σ+

k (x − ¯xk) + log | ˜Σk| = (x − ¯xk)T U1W −1
+ γ+(x − ¯xk)T U2U T

k U T

1 (x − ¯xk)

2 (x − ¯xk) + log |Wk|

+ (p − q) log γ.

Because γ is constant for k = 1, . . . , K, we can omit the (p − q) log γ term and particularly avoid the

calculation of log 0 for γ = 0. Then, the proof follows from Lemma 3 because U T

2 (x − ¯xk) is constant for

k = 1, . . . , K.

Using Theorem 1, we can avoid the time-consuming inverses and determinants of p×p covariance matrices
in (7) and instead calculate these same operations on Wk ∈ Rq×q in (11). The substantial computational

improvements arise because our proposed classiﬁer in (7) is invariant to the term U2, thus yielding an

equivalent classiﬁer in (11) with a substantial reduction in computational complexity. Here, we demonstrate

that the computational eﬃciency in calculating the inverse and determinant of Wk can be further improved

via standard matrix operations when we show that the inverses and determinants of Wk can be performed

on matrices of size nk × nk.

Proposition 1. Let Wk be deﬁned as above. Then, |Wk| = |Γk||Qk| and

W −1

k = Γ−1

k − n−1

k αk(1 − λ)Γ−1

k U T

1 X T

k Q−1

k XkU1Γ−1
k ,

where

and

Qk = Ink + n−1

k αk(1 − λ)XkU1Γ−1

k U T

1 X T
k

Γk = αkλDq + γIq.

8

(12)

(13)

(14)

k αk(1 − λ)U T

Proof. First, we write Wk = n−1
1 X T
from Harville (2008), which states that |A + BT C| = |A||T ||T −1 + CA−1B|, where A ∈ R>

k XkU1 + Γk. To calculate |Wk|, we apply Theorem 18.1.1
a×a, B ∈ Ra×b,
k , T = Ink , and C = XkU1, we have
|Wk| = |Γk||Qk|. Similarly, (12) follows from the well-known Sherman-Woodbury formula (Harville, 2008,

b×b, and C ∈ Rb×a. Thus, setting A = Γk, B = αk(1 − λ)U T

T ∈ R>

1 X T

Theorem 18.2.8) because (A + BT C)−1 = A−1 − A−1B(T −1 + CA−1B)−1CA−1.

Notice that Γk is singular when (λ, γ) = (0, 0) because Γk = 0q, in which case we use the formulation in

(11) instead. Also, notice that if αk is constant across the K classes, then Γk in (14) is independent of k.

Consequently, |Γk| is constant across the K classes and need not be calculated in (11).

4.1. Model Selection

Thus far, we have presented the HDRDA classiﬁer and its properties that facilitate an eﬃcient calculation

of the decision rule. Here, we describe an eﬃcient model-selection procedure along with pseudocode in

Algorithm 1 to select the optimal tuning-parameter estimates from the Cartesian product of candidate

values {λg}G

g=1 × {γh}H

h=1. We estimate the V -fold cross-validation error rate for each candidate pair and
select ((cid:98)λ, (cid:98)γ), which attains the minimum error rate. To calculate the V -fold cross-validation, we partition
the original training data into V mutually exclusive and exhaustive folds that have approximately the same

number of observations. Then, for v = 1, . . . , V , we classify the observations in the vth fold by training

a classiﬁer on the remaining V − 1 folds. We calculate the cross-validation error as the proportion of

misclassiﬁed observations across the V folds.

A primary contributing factor to the eﬃciency of Algorithm 1 is our usage of the compact singular

value decomposition (SVD). Rather than computing the eigenvalue decomposition of (cid:98)Σ to obtain U1, we

instead obtain U1 by computing the eigendecomposition of a much smaller N × N matrix when p (cid:29) N
(Hastie et al., 2008, Chapter 18.3.5). Applying the SVD, we decompose Xc = M ∆U T , where M ∈ RN ×p
is orthogonal, ∆ ∈ R≥
p×p is a diagonal matrix consisting of the singular values of Xc, and U ∈ Rp×p is
c Xc, we have the eigendecomposition (cid:98)Σ = U DU T , where U is the
orthogonal. Recalling that (cid:98)Σ = N −1X T
matrix of eigenvectors of (cid:98)Σ and D = N −1∆ is the diagonal matrix of eigenvalues of (cid:98)Σ. Now, we can obtain
c = M DM T . Next, we
M and D eﬃciently from the eigenvalue decomposition of the N × N matrix XcX T
compute U = X T

c M D+/2, where

D+/2 =





D−1/2
q

0

0

0N −q



 .

We then determine q, the number of numerically nonzero eigenvalues present in D, by calculating the number

of eigenvalues that exceeds some tolerance value, say, 1 × 10−6. We then extract U1 as the ﬁrst q columns

of U .

As a result of the compact SVD, we need calculate XcU1 only once per cross-validation fold, requiring

O(pqN ) ≈ O(pN 2) calculations. Hence, the computational costs of expensive calculations, such as matrix

inverses and determinants, are greatly reduced because they are performed in the q-dimensional subspace.

Similarly, we reduce the dimension of the test data set by calculating XtestU1 once per fold. Conveniently,

9

input : Data matrix X

Parameter grid {λg}G

g=1 × {γh}H

h=1

output: Optimal Estimates (ˆλ, ˆγ)

for v ← 1 to V do

Partition X into Xtrain ∈ RN ×p and Xtest ∈ RNT ×p
for k ← 1 to K do

Extract Xk ∈ Rnk×p from Xtrain
Compute sample mean ¯xk from Xk
Center Xk ← Xk − 1nk ¯xT
k

end

end

1 , . . . , X T

Xc ← [X T
Compute the compact SVD Xc = MqDqU T
1

K]T

Transform Xc ← XcU1

Transform Xtest ← XtestU1

for k ← 1 to K do

Extract Xk ∈ Rnk×q from Xc
Recompute sample mean ¯xk from Xk

for (λ, γ) ∈ {λg}G

g=1 × {γh}H

h=1 do

for k ← 1 to K do

Compute Qk using (13)

Compute Γk using (14)
Compute W −1

using (12)

k

Compute |Wk| = |Γk||Qk|
Compute (x − ¯xk)T U1W −1

k U T

1 (x − ¯xk) + log |Wk| for each row x of Xtest

end

end

Classify test observations Xtest using (11)

Compute the number of misclassiﬁed test observations #{Errorv(λ, γ)}

end
Compute (cid:92)Error(λ, γ) = N −1 (cid:80)V
Report optimal (ˆλ, ˆγ) ← arg min(λ,γ)

v=1 #{Errorv(λ, γ)}
(cid:92)Error(λ, γ)

Algorithm 1: Model selection for the HDRDA classiﬁer

10

we see that the most costly computation involved in Qk and W −1
is XkU1, which can be extracted from
XcU1. Thus, after the initial calculation of XcU1 per cross-validation fold, Qk requires O(nkq2) operations.
Because Qk ∈ Rnk×nk , both its determinant and inverse require O(n3
k) operations. Consequently, W −1
k ∈ Rq×q requires O(q) operations.
requires O(nkq2) operations. Also, the inverse of the diagonal matrix Γ−1
Finally, we remark that |Wk| requires O(n3

k) operations.

k

k

The expressions given in Proposition 1 also expedite the selection of λ and γ via cross-validation

because the most time-consuming matrix operation involved in computing W −1
Rnk×q, which is independent of λ and γ. The subsequent operations in calculating W −1
be simply updated for diﬀerent pairs of λ and γ without repeating the costly computations. Also, rather
than calculating (x − ¯xk)T U1W −1
k)(cid:48)U1W −1
(Xtest − ¯xk1(cid:48)

1 (x − ¯xk) individually for each row x of Xtest, we can calculate
k). The diagonal elements of the resulting matrix contain the indi-

k U T
1 (Xtest − ¯xk1(cid:48)

and |Wk| is XkU1 ∈

and |Wk| can

k

k

k U T
vidual quadratic form of each test observation, xt.

5. Timing Comparisons between RDA and HDRDA

In this section, we demonstrate that the computational performance of the model selection employed in

the HDRDA classiﬁer is substantially faster than that of the RDA classiﬁer on small-sample, high-dimensional

data sets. The relative diﬀerence in runtime between the two classiﬁers drastically increases as p increases. To

compare the two classiﬁers, we generated 25 observations from each of K = 4 multivariate normal populations

with mean vectors µ1 = −3 · 1p, µ2 = −1p, µ3 = 1p, and µ4 = 3 · 1p. We set the covariance matrix of

each population to the p × p identity matrix. For each data set generated, we estimated the parameters λ

and γ for both classiﬁers using a grid of 5 equidistant candidate values between 0 and 1, inclusively. We

set αk = 1 − γ, k = 1, . . . , K, in the HDRDA classiﬁer. At each pair of λ and γ, we computed the 10-fold

cross-validation error rate (Hastie et al., 2008). Then, we selected the model that minimized the 10-fold

cross-validation error rate.

We compared the runtime of both classiﬁers by increasing the number of features from p = 500 to p = 5000

in increments of 500. Next, we generated 100 data sets for each value of p and computed the training and

model selection runtime of both classiﬁers. Our timing comparisons are based on our HDRDA implementation

in the sparsediscrim R package and the standard RDA implementation in the klaR R package. All timing

comparisons were conducted on an Amazon Elastic Compute Cloud (EC2) c4.4xlarge instance using version

3.3.1 of the open-source statistical software R. Our timing comparisons can be reproduced with the code

available at https://github.com/ramhiser/paper-hdrda.

5.1. Timing Comparison Results

In Figure 2, we plotted the runtime of the model selections for both the HDRDA and RDA classiﬁers as

a function of p. We observed that the HDRDA classiﬁer was substantially faster than the RDA classiﬁer as p

increased. In the left panel of Figure 2, we ﬁt a quadratic regression line to the RDA runtimes and a simple

linear regression model to the HDRDA runtimes. For improved understanding, in the right panel we repeated

the same scatterplot and linear ﬁt with the timings restricted to the observed range of the HDRDA timings.

11

Figure 2 suggests that the usage of a matrix inverse and determinant in the klaR R package’s discriminant

function yielded model-selection timings that exceeded linear growth in p. Because the HDRDA classiﬁer

removes inverse and determinants, it was computationally more eﬃcient than the RDA classiﬁer, especially

as p increased. In fact, when p = 5000, the RDA classiﬁer required 24.933 minutes on average to perform

model selection, while the HDRDA classiﬁer selected its optimal model in 2.979 seconds on average. Clearly,

the model selection employed by the HDRDA classiﬁer is substantially faster than that of the RDA classiﬁer.

We quantiﬁed the relative timing comparisons between the two classiﬁers by calculating the ratio of

mean timings of the RDA classiﬁer to the HDRDA classiﬁer for each value of p. We employed nonparametric

bootstrapping to estimate the mean ratio along with 95% conﬁdence intervals. In Figure 3, the bootstrap

sampling distributions for the ratio of mean timings are given. First, we observe that the mean relative

timings increased as p increased. For smaller dimensions, the relative diﬀerence in computing was sizable

with the average ratio of the mean timings equal to 14.513 for p = 500 and a 95% conﬁdence interval

of (14.191, 14.855). Furthermore, the ratio of mean computing times suggested that the RDA classiﬁer is

impractical for higher dimensions. For instance, when p = 5000, the ratio of mean computing times increased

to 502.786 with a 95% conﬁdence interval of (462.863, 546.396).

[Insert Figure 2 approximately here ]

[Insert Figure 3 approximately here ]

6. Classiﬁcation Study

In this section, we compare our proposed classiﬁer with four classiﬁers recently proposed for small-sample,

high-dimensional data along with the random-forest classiﬁer from Breiman (2001) using version 3.3.1 of the

open-source statistical software R. Within our study, we included penalized linear discriminant analysis from

Witten and Tibshirani (2011), implemented in the penalizedLDA package. We also considered shrunken

centroids regularized discriminant analysis from Guo et al. (2007) in the rda package. Because the rda

package does not perform the authors’ “Min-Min” rule automatically, we applied this rule within our R code.

We included two modiﬁcations of diagonal linear discriminant analysis from Tong et al. (2012) and Pang

et al. (2009), where the former employs an improved mean estimator and the latter utilizes an improved

variance estimator. Both classiﬁers are available in the sparsediscrim package. Finally, we incorporated the

random forest as a benchmark based on the ﬁndings of Fern´andez-Delgado et al. (2014), who concluded that

the random forest is often superior to other classiﬁers in benchmark studies. We used the implementation

of the random-forest classiﬁer from the randomForest package with 250 trees and 100 maximum nodes. For

each classifer we explicitly set prior probabilities as equal, if applicable. All other classiﬁer options were set

to their default settings. Below, we refer to each classiﬁer by the ﬁrst author’s surname. All simulations

were conducted on an Amazon EC2 c4.4xlarge instance. Our analyses can be reproduced via the code

available at https://github.com/ramhiser/paper-hdrda.

12

For the HDRDA classiﬁer in (11), we examined the classiﬁcation performance of two models. For the

ﬁrst HDRDA model, we set αk = 1, k = 1, . . . , K, so that the covariance-matrix estimator (6) resembled

(3). We estimated λ from a grid of 21 equidistant candidate values between 0 and 1, inclusively. Similarly,

we estimated γ from a grid consisting of the values 10−1, . . . , 104, and 105. We selected optimal estimates of

λ and γ using 10-fold cross-validation. For the second model, we set αk = 1 − γ, k = 1, . . . , K, to resemble

Friedman’s parameterization, and we estimated both λ and γ from a grid of 21 equidistant candidate values

between 0 and 1, inclusively.

We did not include the RDA classiﬁer in our classiﬁcation study because its training runtime was pro-

hibitively slow on high-dimensional data in our preliminary experiments. As shown in Section 5, the runtime

of the RDA classiﬁer was drastically larger than that of the HDRDA classiﬁer for a tuning grid of size

25 = 5 × 5. Consequently, a fair comparison between the RDA and HDRDA classiﬁers would require model

selection of 441 = 21 × 21 diﬀerent pairs of tuning parameters in the RDA classiﬁer. A tuning grid of this

size yielded excessively slow training runtimes for the RDA implementation from the klaR R package.

6.1. Simulation Study

In this section we compare the competing classiﬁers using the simulation design from Guo et al. (2007).

This design is widely used within the high-dimensional classiﬁcation literature, including the studies by

Ramey and Young (2013) and Witten and Tibshirani (2011). First, we consider the block-diagonal covariance

matrix from Guo et al. (2007),

Σk =


















0100
...
...
· · ·

Σ(ρk)

0100

0100 Σ(−ρk)

0100

0100

0100 Σ(ρk)

· · ·

0100

0100

· · ·

· · ·

· · ·

0100
...
· · ·

0100 Σ(−ρk) 0100
...
. . .
· · ·

0100

· · ·

· · ·


















· · ·
...
...
...
...
· · ·

,

(15)

where the (i, j)th entry of the block matrix Σ(ρk) ∈ R100×100 is

Σ(ρk)

ij = {ρ|i−j|

k

}1≤i,j≤100.

The block-diagonal covariance structure in (15) resembles gene-expression data: within each block of path-

ways, genes are correlated, and the correlation decays as a function of the distance between any two genes.

The original design from Guo et al. (2007) comprised two p-dimensional multivariate normal populations

with a common block-diagonal covariance matrix.

Although the design is indeed standard, the simulation conﬁguration lacks artifacts commonly observed

in real data, such as skewness and extreme outliers. As a result, we wished to investigate the eﬀect of outliers

on the high-dimensional classiﬁers. To accomplish this goal, we generalized the block-diagonal simulation

conﬁguration by sampling from a p-dimensional multivariate contaminated normal distribution. Denoting

the PDF of the p-dimensional multivariate normal distribution by Np(x|µ, Σ), we write the PDF of the kth

13

class as

0.05, . . ., 0.50.

p(x|ωk) = (1 − (cid:15))Np(x|µk, Σk) + (cid:15)Np(x|µk, ηΣk),

(16)

where (cid:15) ∈ [0, 1] is the probability that an observation is contaminated (i.e., drawn from a distribution with

larger variance) and η > 1 scales the covariance matrix Σk to increase the extremity of outliers. For (cid:15) = 0,

we have the benchmark block-diagonal simulation design from Guo et al. (2007). As (cid:15) is increased, the

average number of outliers is increased. In our simulation, we let η = 100 and considered the values of (cid:15) = 0,

We generated K = 3 populations from (16) with Σk given in (15) and set the mean vector of class 1 to

µ1 = 0p. Next, comparable to Guo et al. (2007), the ﬁrst 100 features of µ2 were set to 1/2, while the rest

). For simplicity, we deﬁned µ3 = −µ2. The three populations

diﬀered in their mean vectors in the ﬁrst 100 features corresponding to the ﬁrst block, and no diﬀerence in

were set to 0, i.e., µ2 = (1/2, . . . , 1/2
(cid:125)

(cid:124)

(cid:123)(cid:122)
100

, 0, . . . , 0
(cid:124) (cid:123)(cid:122) (cid:125)
p−100

the means occurred in the remaining blocks.

From each of the K = 3 populations, we sampled 25 training observations (nk = 25 for all k) and 10,000

test observations. After training each classiﬁer on the training data, we classiﬁed the test data sets and

computed the proportion of mislabeled test observations to estimate the classiﬁcation error rate for each

classiﬁer. Repeating this process 500 times, we computed the average of the error-rate estimates for each

classiﬁer. We allowed the number of features to vary from p = 100 to p = 500 in increments of 100 to examine

the classiﬁcation accuracy as the feature dimension increased while maintaining a small sample size. Guo

et al. (2007) originally considered ρk = 0.9 for all k. Alternatively, to explore the more realistic assumption

of unequal covariance matrices, we put ρ1 = 0.1, ρ2 = 0.5, and ρ3 = 0.9.

6.1.1. Simulation Results

In Figure 4, we observed each classiﬁer’s average classiﬁcation error rates for the values of (cid:15) and p.

Unsurprisingly, the average error rate increased for each classiﬁer as the contamination probability (cid:15) increased

regardless of the value of p. Sensitivity to the presence of outliers was most apparent for the Pang, Tong, and

Witten classiﬁers. For smaller dimensions, the random-forest and HDRDA classiﬁers tended to outperform

the remaining classiﬁers with the random forest performing best. As the feature dimension increased with

p ≥ 300, both HDRDA classiﬁers outperformed all other classiﬁers, suggesting that their inherent dimension

reduction better captured the classiﬁcatory information in the small training samples, even in the presence

of outliers.

[Insert Figure 4 approximately here ]

The Pang, Tong, and Witten methods yielded practically the same and consistently the worst error rates

when outliers were present with (cid:15) > 0, suggesting that these classiﬁers were sensitive to outliers. Notice, for

example, that when p = 400, the error rates of the Pang, Tong, and Witten classiﬁers increased dramatically

from approximately 19% when no outliers were present to approximately 43% when (cid:15) = 0.05. The sharp

increase in average error rates for these three classiﬁers continued as (cid:15) increased. Guo’s method always

14

outperformed those of Pang, Witten, and Tong, but after outliers were introduced, the Guo classiﬁer’s

average error rate was not competitive with the HDRDA classiﬁers or the random-forest classiﬁer.

[Insert Figure 5 approximately here ]

In Figure 5, we again examine the simulation results as a function of p for a subset of the values of (cid:15).

This set of plots allows us to investigate the eﬀect of feature dimensionality on classiﬁcation performance.

When no outliers were present (i.e., (cid:15) = 0), the random-forest classiﬁer was outperformed by all other

classiﬁers. Furthermore, the HDRDA classiﬁers were superior in terms of average error rate in this setting.

As p increased, an elevation in average error rate was expected for all classiﬁers, but the increase was not

observed to be substantial.

For (cid:15) > 0, we observed a diﬀerent behavior in classiﬁcation performance. First, the Pang, Tong, and

Witten methods, along with the random-forest method, increased in average error rate as p increased.

Contrarily, the performance of the HDRDA and Guo classiﬁers was hardly aﬀected by p. Also, as discussed

above, the HDRDA classiﬁers were superior to all other classiﬁers for large values of p with only the random-

forest classiﬁer outperforming them in smaller feature-dimension cases.

6.2. Application to Gene Expression Data

We compared the HDRDA classiﬁer to the ﬁve competing classiﬁers on six benchmark gene-expression

microarray data sets. First, we evaluated the classiﬁcation accuracy of each classiﬁer by randomly parti-

tioning the data set under consideration such that 2/3 of the observations were allocated as training data

and the remaining 1/3 of the observations were allocated as a test data set. To expedite the computational

runtime, we reduced the training data to the top 1000 variables by employing the variable-selection method

proposed by Dudoit et al. (2002). We then reduced the test data set to the same 1000 variables. After

training each classiﬁer on the training data, we classiﬁed the test data sets and computed the proportion

of mislabeled test observations to estimate the classiﬁcation error rate for each classiﬁer. Repeating this

process 100 times, we computed the average of the error-rate estimates for each classiﬁer. We next provide

a concise description of each high-dimensional data set examined in our classiﬁcation study.

6.2.1. Chiaretti et al. (2004) Data Set

Chiaretti et al. (2004) measured the gene-expression proﬁles for 128 individuals with acute lymphoblastic

leukemia (ALL) using Aﬀymetrix human 95Av2 arrays. Following Xu et al. (2009), we restricted the data

set to K = 2 classes such that n1 = 74 observations were without cytogenetic abnormalities and n2 = 37

observations had a detected BCR/ABL gene. The robust multichip average normalization method was

applied to all 12,625 gene-expression levels.

6.2.2. Chowdary et al. (2006) Data Set

Chowdary et al. (2006) investigated 52 matched pairs of tissues from colon and breast tumors using

Aﬀymetrix U133A arrays and ribonucleic-acid (RNA) ampliﬁcation. Each tissue pair was gathered from

the same patient and consisted of a snap-frozen tissue and a tissue suspended in an RNAlater preservative.

15

Overall, 31 breast-cancer and 21 colon-cancer pairs were gathered, resulting in K = 2 classes with n1 = 62

and n2 = 42. A purpose of the study was to determine whether the disease state could be identiﬁed using

22,283 gene-expression proﬁles.

6.2.3. Nakayama et al. (2007) Data Set

Nakayama et al. (2007) acquired 105 gene-expression samples of 10 types of soft-tissue tumors through an

oligonucleotide microarray, including 16 samples of synovial sarcoma (SS), 19 samples of myxoid/round cell

liposarcoma (MLS), 3 samples of lipoma, 3 samples of well-diﬀerentiated liposarcoma (WDLS), 15 samples of

dediﬀerentiated liposarcoma (DDLS), 15 samples of myxoﬁbrosarcoma (MFS), 6 samples of leiomyosarcoma

(LMS), 3 samples of malignant nerve sheathe tumor (MPNST), 4 samples of ﬁbrosarcoma (FS), and 21

samples of malignant ﬁbrous histiocytoma (MFH). Nakayama et al. (2007) determined from their data that

these 10 types fell into 4 broader groups: (1) SS; (2) MLS; (3) Lipoma, WDLS, and part of DDLS; (4) Spindle

cell and pleomorophic sarcomas including DDLS, MFS, LMS, MPNST, FS, and MFH. Following Witten and

Tibshirani (2011), we restricted our analysis to the ﬁve tumor types having at least 15 observations.

6.2.4. Shipp et al. (2002) Data Set

According to Shipp et al. (2002), approximately 30%-40% of adult non-Hodgkin lymphomas are diﬀuse

large B-cell lymphomas (DLBCLs). However, only a small proportion of DLBCL patients are cured with

modern chemotherapeutic regimens. Several models have been proposed, such as the International Prognostic

Index (IPI), to determine a patient’s curability. These models rely on clinical covariates, such as age, to

determine if the patient can be cured, and the models are often ineﬀective. Shipp et al. (2002) have argued

that researchers need more eﬀective means to determine a patient’s curability. The authors measured 6,817

gene-expression levels from 58 DLBCL patient samples with customized cDNA (lymphochip) microarrays to

investigate the curability of patients treated with cyclophosphamide, adriamycin, vincristine, and prednisone

(CHOP)-based chemotherapy. Among the 58 DLBCL patient samples, 32 are from cured patients while 26

are from patients with fatal or refractory disease.

6.2.5. Singh et al. (2002) Data Set

Singh et al. (2002) have examined 235 radical prostatectomy specimens from surgery patients between

1995 and 1997. The authors used oligonucleotide microarrays containing probes for approximately 12,600

genes and expressed sequence tags. They have reported that 102 of the radical prostatectomy specimens are

of high quality: 52 prostate tumor samples and 50 non-tumor prostate samples.

6.2.6. Tian et al. (2003) Data Set

Tian et al. (2003) investigated the puriﬁed plasma cells from the bone marrow of control patients along

with patients with newly diagnosed multiple myeloma. Expression proﬁles for 12,2625 genes were obtained via

Aﬀymetrix U95Av2 microarrays. The plasma cells were subjected to biochemical and immunohistochemical

analyses to identify molecular determinants of osteolytic lesions. For 36 multiple-myloma patients, focal

bone lesions could not be detected by magnetic resonance imaging (MRI), whereas MRI was used to detect

such lesions in 137 patients.

16

6.2.7. Classiﬁcation Results

Similar to Witten and Tibshirani (2011), we report the average test error rates obtained over 100 random

training-test partitions in Table 1 along with standard deviations of the test error rates in parentheses. The

HDRDA and Guo classiﬁers were superior in classiﬁcation performance for the majority of the simulations.

The HDRDA classiﬁers yielded the best classiﬁcation accuracy on the Chowdary and Shipp data sets. Al-

though the random forest’s accuracy slightly exceeded the HDRDA classiﬁers on the Tian data set, our

proposed classiﬁers outperformed the other competing classiﬁers considered here. Moreover, the HDRDA

classiﬁers yielded comparable performance on ﬁve of the six data sets.

[Insert Table 1 approximately here ]

The average error-rate estimates for the Pang, Tong, and Witten classiﬁers were comparable across all six

data sets. Furthermore, the average error rates for the Pang and Tong classiﬁers were approximately equal

for all data sets except for the Chiaretti dataset. This result suggests that the mean and variance estimators

used in lieu of the MLEs provided little improvement to classiﬁcation accuracies. However, we investigated

the Pang classiﬁer’s poor performance on the Chiaretti data set and determined that its variance estimator

exhibited numerical instability. The classiﬁer’s denominator was approximately zero for both classes and led

to the poor classiﬁcation performance.

The random-forest classiﬁer was competitive when applied to the Chowdary and Singh data sets and

yielded the smallest error rate of the considered classiﬁers on the Tian data set. The fact that the HDRDA

and Guo classiﬁers typically outperformed the random-forest classiﬁer challenges the claim of Fern´andez-

Delgado et al. (2014) that random forests are typically superior. Further studies should be performed to

validate this statement in the small-sample, high-dimensional setting.

Finally, the Pang, Tong, and Witten classiﬁers consistently yielded the largest average error rates across

the six data sets. Given that the standard deviations were relatively large, we hesitate to generalize claims

regarding the ranking of these three classiﬁers in terms of the average error rate. However, the classiﬁers’

error rates and their variability across multiple random partitions of each data set were large enough that

we might question their beneﬁt when applied to real data.

7. Discussion

We have demonstrated that our proposed HDRDA classiﬁer is competitive with and often superior to ran-

dom forests as well as the Witten, Pang, Tong, and Guo classiﬁers. In fact, we have shown that the HDRDA

classiﬁer often yields superior classiﬁcation accuracy when applied to small-sample, high-dimensional data

sets, conﬁrming the assertions of Mai et al. (2012) and Fan et al. (2012) that diagonal classiﬁers often yield

inferior classiﬁcation performance when compared to other classiﬁcation methods. Furthermore, we have

demonstrated that HDRDA classiﬁers are more robust to the presence of outliers than the diagonal clas-

siﬁers despite their rapid computational performance and their reduction in the number of parameters to

estimate.

17

We also considered the popular penalized linear discriminant analysis from Witten and Tibshirani (2011)

because it was speciﬁcally designed for high-dimensional gene-expression data. We had expected its clas-

siﬁcation performance to be competitive within our classiﬁcation study and perhaps superior. Contrarily,

our empirical studies suggest that the classiﬁer is sensitive to outliers and unable to achieve comparable

results with other classiﬁers designed for small-sample, high-dimensional data. Also, despite the claims of

Fern´andez-Delgado et al. (2014) that random forests are typically superior to other classiﬁers, we observed

that they were indeed competitive but were typically outperformed by classiﬁers developed for small-sample,

high-dimensional data.

We demonstrated that our HDRDA implementation in the sparsediscrim R package can be used in

practice with high-dimensional data sets. In our timing comparisons, we showed that HDRDA model selection

could be employed on data sets with p = 5000 in 2.979 seconds on average. Contrarily, the RDA classiﬁer

implemented in the klaR R package required 24.933 minutes on average to perform model selection on data

sets with p = 5000. Given that the RDA classiﬁer has been shown to have excellent performance in the

high-dimensional setting (Webb and Copsey, 2011) but is limited by its computationally intense model-

selection procedure, our work replaces the RDA classiﬁer for high-dimensional data in practice. This result

is reassuring because the RDA classiﬁer remains widely popular in the literature. In fact, variants of the

RDA classiﬁer have been applied to microarray data (Ching et al., 2012; Li and Wu, 2012; Tai and Pan,

2007; Guo et al., 2007), facial recognition (Zhang et al., 2010; Dai and Yuen, 2007; Lu et al., 2005; Pima and

Aladjem, 2004; Lu and Plataniotis, 2003), handwritten digit recognition (Bouveyron et al., 2007), remote

sensing (Tadjudin and Landgrebe, 1999), seismic detection (Anderson, 2002), and chemical spectra (Wu

et al., 1996; Aeberhard et al., 1993).

The dimension reduction employed in this paper has reduced the dimension to rank( (cid:98)Σ) = q. An inter-

esting extension of our work would reduce the dimension q further to a lower dimension qL < q, perhaps

using a criterion similar to that of principal components analysis. While unclear whether the classiﬁcation

performance would improve via such a method, the eﬃciency of the model selection would certainly improve.

Moreover, if qL = 2 or 3, low-dimensional graphical displays of high-dimensional data could be obtained.

We thank Mrs. Joy Young for her numerous recommendations that enhanced the quality of our writing.

References

Aeberhard, S., Coomans, D., Vel, O. D., 1993. Improvements to the classiﬁcation performance of RDA.

Journal of Chemometrics 7 (2), 99–115.

Anderson, D. N., Aug. 2002. Application of Regularized Discrimination Analysis to Regional Seismic Event

Identiﬁcation. Bulletin of the Seismological Society of America 92 (6), 2391–2399.

Bensmail, H., Celeux, G., Dec. 1996. Regularized Gaussian Discriminant Analysis through Eigenvalue De-

composition. Journal of the American Statistical Association 91 (436), 1743–1748.

Bouveyron, C., Girard, S., Schmid, C., Oct. 2007. High-Dimensional Discriminant Analysis. Communications

in Statistics - Theory and Methods 36 (14), 2607–2623.

18

Breiman, L., 2001. Random Forests - Springer. Machine Learning 45 (1), 5–32.

Chiaretti, S., Li, X., Gentleman, R., Vitale, A., Vignetti, M., Mandelli, F., Ritz, J., Foa, R., 2004. Gene

expression proﬁle of adult T-cell acute lymphocytic leukemia identiﬁes distinct subsets of patients with

diﬀerent response to therapy and survival. Blood 103 (7), 2771–2778.

Ching, W.-K., Chu, D., Liao, L.-Z., Wang, X., Jul. 2012. Regularized orthogonal linear discriminant analysis.

Pattern Recognition 45 (7), 2719–2732.

Chowdary, D., Lathrop, J., Skelton, J., Curtin, K., Briggs, T., Zhang, Y., Yu, J., Wang, Y., Mazumder,

A., Feb. 2006. Prognostic Gene Expression Signatures Can Be Measured in Tissues Collected in RNAlater

Preservative. The Journal of Molecular Diagnostics 8 (1), 31–39.

Dai, D.-Q., Yuen, P. C., Aug. 2007. Face recognition by regularized discriminant analysis. IEEE transactions

on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and

Cybernetics Society 37 (4), 1080–1085.

Dudoit, S., Fridlyand, J., Speed, T. P., Mar. 2002. Comparison of Discrimination Methods for the Classiﬁ-

cation of Tumors Using Gene Expression Data. Journal of the American Statistical Association 97 (457),

Fan, J., Feng, Y., Tong, X., Apr. 2012. A road to classiﬁcation in high dimensional space: the regularized

optimal aﬃne discriminant. Journal of the Royal Statistical Society: Series B (Statistical Methodology)

Fern´andez-Delgado, M., Cernadas, E., Barro, S., Amorim, D., Jan. 2014. Do we need hundreds of classiﬁers

to solve real world classiﬁcation problems? The Journal of Machine Learning Research 15 (1), 3133–3181.

Friedman, J. H., 1989. Regularized Discriminant Analysis. Journal of the American Statistical Association

Guo, Y., Hastie, T., Tibshirani, R., Jan. 2007. Regularized linear discriminant analysis and its application

in microarrays. Biostatistics 8 (1), 86–100.

Halbe, Z., Aladjem, M., Nov. 2007. Regularized mixture discriminant analysis. Pattern Recognition Letters

Harville, D. A., 2008. Matrix Algebra from a Statistician’s Perspective. Springer, New York.

Hastie, T., Tibshirani, R., Friedman, J., Dec. 2008. The Elements of Statistical Learning, 2nd Edition. Data

Mining, Inference, and Prediction. Springer New York, New York, NY.

Hoerl, A. E., Kennard, R. W., Feb. 1970. Ridge Regression: Biased Estimation for Nonorthogonal Problems.

Technometrics 12 (1), 55–67.

Kollo, T., von Rosen, D., 2005. Advanced Multivariate Statistics with Matrices. Vol. 579 of Mathematics

and Its Applications (New York). Springer, Dordrecht.

77–87.

74 (4), 745–771.

84 (405), 165–175.

28 (15), 2104–2115.

19

Li, R., Wu, B., Aug. 2012. Sparse regularized discriminant analysis with application to microarrays. Com-

putational biology and chemistry 39, 14–19.

Lu, J., Plataniotis, K. N., 2003. Regularized discriminant analysis for the small sample size problem in face

recognition. Pattern Recognition Letters.

Lu, J., Plataniotis, K. N., Venetsanopoulos, A. N., 2005. Regularization studies of linear discriminant analysis

in small sample size scenarios with application to face recognition. Pattern Recognition Letters 26 (2),

Mai, Q., Zou, H., Yuan, M., Feb. 2012. A direct approach to sparse discriminant analysis in ultra-high

181–191.

dimensions. Biometrika 99 (1), 29–42.

nition Letters 16 (3), 267–275.

Mkhadri, A., Mar. 1995. Shrinkage parameter for the modiﬁed linear discriminant analysis. Pattern Recog-

Mkhadri, A., Celeux, G., Nasroallah, A., 1997. Regularization in discriminant analysis: an overview. Com-

putational Statistics and Data Analysis 23 (3), 403–423.

Murphy, K. P., Aug. 2012. Machine Learning: A Probabilistic Perspective. The MIT Press, Cambridge,

Massachusetts.

Nakayama, R., Nemoto, T., Takahashi, H., Ohta, T., Kawai, A., Seki, K., Yoshida, T., Toyama, Y., Ichikawa,

H., Hasegawa, T., Apr. 2007. Gene expression analysis of soft tissue sarcomas: characterization and

reclassiﬁcation of malignant ﬁbrous histiocytoma. Nature 20 (7), 749–759.

Pang, H., Tong, T., Zhao, H., Mar. 2009. Shrinkage-based Diagonal Discriminant Analysis and Its Applica-

tions in High-Dimensional Data. Biometrics 65 (4), 1021–1029.

Pima, I., Aladjem, M., 2004. Regularized discriminant analysis for face recognition. Pattern Recognition

37 (9), 1945–1948.

581–596.

Berkeley, pp. 601–620.

Interscience.

Ramey, J., Young, P. D., 2013. A comparison of regularization methods applied to the linear discriminant

function with high-dimensional microarray data. Journal of Statistical Computation and Simulation 83 (3),

Rao, C. R., Mitra, S. K., 1971. Generalized inverse of a matrix and its applications. In: Proceedings of

the Sixth Berkeley Symposium on Mathematical Statistics and Probability. University of California Press,

Seber, G. A. F., Aug. 2004. Multivariate Observations. Wiley Series in Probability and Statistics. Wiley-

Shipp, M. A., Ross, K. N., Tamayo, P., Weng, A. P., Kutok, J. L., Aguiar, R. C. T., Gaasenbeek, M., Angelo,

M., Reich, M., Pinkus, G. S., Ray, T. S., Koval, M. A., Last, K. W., Norton, A., Lister, T. A., Mesirov,

J., Neuberg, D. S., Lander, E. S., Aster, J. C., Golub, T. R., Jan. 2002. Diﬀuse large B-cell lymphoma

20

outcome prediction by gene-expression proﬁling and supervised machine learning. Nature Medicine 8 (1),

68–74.

Singh, D., Febbo, P. G., Ross, K., Jackson, D. G., Manola, J., Ladd, C., Tamayo, P., Renshaw, A. A.,

D’Amico, A. V., Richie, J. P., Lander, E. S., Loda, M., Kantoﬀ, P. W., Golub, T. R., Sellers, W. R., Mar.

2002. Gene expression correlates of clinical prostate cancer behavior. Cancer Cell 1 (2), 203–209.

Srivastava, M. S., Kubokawa, T., 2007. Comparison of discrimination methods for high dimensional data.

Journal of the Japan Statistical Society 37 (1), 123–134.

Tadjudin, S., Landgrebe, D. A., Jul. 1999. Covariance estimation with limited training samples. IEEE

Transactions on Geoscience and Remote Sensing 37 (4), 2113–2118.

Tai, F., Pan, W., Dec. 2007. Incorporating prior knowledge of gene functional groups into regularized dis-

criminant analysis of microarray data. Bioinformatics 23 (23), 3170–3177.

Tian, E., Zhan, F., Walker, R., Rasmussen, E., Ma, Y., Barlogie, B., Shaughnessy, Jr., J. D., Dec. 2003.

The Role of the Wnt-Signaling Antagonist DKK1 in the Development of Osteolytic Lesions in Multiple

Myeloma. New England Journal of Medicine 349 (26), 2483–2494.

Tong, T., Chen, L., Zhao, H., Feb. 2012. Improved mean estimation and its application to diagonal discrim-

inant analysis. Bioinformatics 28 (4), 531–537.

Webb, A. R., Copsey, K. D., Sep. 2011. Statistical Pattern Recognition, 3rd Edition. John Wiley & Sons,

Chichester, West Sussex, UK.

Witten, D. M., Tibshirani, R., Aug. 2011. Penalized classiﬁcation using Fisher’s linear discriminant. Journal

of the Royal Statistical Society: Series B (Statistical Methodology) 73 (5), 753–772.

Wu, W., Mallet, Y., Walczak, B., Penninckx, W., Massart, D. L., Heuerding, S., Erni, F., Aug. 1996.

Comparison of regularized discriminant analysis, linear discriminant analysis, and quadratic discriminant

analysis applied to NIR data. Analytica Chimica Acta 329 (3), 257–265.

Xu, P., Brock, G. N., Parrish, R. S., Mar. 2009. Modiﬁed linear discriminant analysis approaches for classiﬁ-

cation of high-dimensional microarray data. Computational Statistics and Data Analysis 53 (5), 1674–1687.

Ye, J., Ji, S., Nov. 2009. Discriminant Analysis for Dimensionality Reduction: An Overview of Recent

Developments. Biometrics: Theory, Methods, and Applications. John Wiley & Sons, Inc., Hoboken, NJ,

USA.

Ye, J., Wang, T., 2006. Regularized Discriminant Analysis for High Dimensional, Low Sample Size Data. In:

The 12th ACM SIGKDD International Conference. ACM Press, New York, New York, USA, p. 454.

Zhang, Z., Dai, G., Xu, C., Jordan, M. I., Mar. 2010. Regularized Discriminant Analysis, Ridge Regression

and Beyond. The Journal of Machine Learning Research 11.

21

Figure 1: Contours of ﬁve multivariate normal populations as a function of the pooling parameter λ.

22

Figure 2: Timing comparisons (in seconds) between HDRDA and RDA classiﬁers.

23

Figure 3: Distribution of ratios of mean RDA runtime to mean HDRDA runtime across 1000 bootstrap replications.

24

Figure 4: Average classiﬁcation error rates as a function of the contamination probability (cid:15). Approximate standard errors were

no greater than 0.022.

25

Figure 5: Average classiﬁcation error rates as a function of the number of features p. Approximate standard errors were no

greater than 0.022.

26

Classiﬁer

Chiaretti

Chowdary

Nakayama

Shipp

Singh

Tian

Guo

0.111 (0.044)

0.056 (0.051)

0.208 (0.061)

0.086 (0.063)

0.089 (0.055)

0.268 (0.082)

HDRDA Convex

0.115 (0.044)

0.035 (0.026)

0.208 (0.066)

0.073 (0.057)

0.111 (0.059)

0.229 (0.049)

HDRDA Ridge

0.118 (0.050)

0.033 (0.022)

0.208 (0.070)

0.072 (0.065)

0.099 (0.046)

0.225 (0.050)

Pang

0.663 (0.062)

0.197 (0.091)

0.227 (0.062)

0.192 (0.091)

0.221 (0.095)

0.267 (0.054)

Random Forest

0.124 (0.053)

0.045 (0.028)

0.232 (0.063)

0.135 (0.078)

0.093 (0.045)

0.206 (0.044)

Tong

Witten

0.195 (0.068)

0.197 (0.091)

0.227 (0.062)

0.192 (0.091)

0.221 (0.095)

0.267 (0.054)

0.194 (0.068)

0.197 (0.091)

0.232 (0.068)

0.193 (0.092)

0.221 (0.095)

0.264 (0.053)

Table 1: The average of the test error rates obtained on gene-expression data sets over 100 random training-test partitions.

Standard deviations of the test error rates are given in the parentheses. The classiﬁer with the minimum average error rate for

each data set is in bold.

27

