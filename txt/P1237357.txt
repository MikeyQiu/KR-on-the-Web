Low-variance Black-box Gradient Estimates for the Plackett-Luce Distribution

Artyom Gadetsky,1∗† Kirill Struminsky,1∗ Christopher Robinson,2
Novi Quadrianto,1, 2 Dmitry Vetrov1‡
1 National Research University Higher School of Economics
2 Predictive Analytics Lab (PAL), University of Sussex

9
1
0
2
 
v
o
N
 
2
2
 
 
]

G
L
.
s
c
[
 
 
1
v
6
3
0
0
1
.
1
1
9
1
:
v
i
X
r
a

Abstract

Learning models with discrete latent variables using stochas-
tic gradient descent remains a challenge due to the high vari-
ance of gradient estimates. Modern variance reduction tech-
niques mostly consider categorical distributions and have lim-
ited applicability when the number of possible outcomes be-
comes large. In this work, we consider models with latent per-
mutations and propose control variates for the Plackett-Luce
distribution. In particular, the control variates allow us to opti-
mize black-box functions over permutations using stochastic
gradient descent. To illustrate the approach, we consider a va-
riety of causal structure learning tasks for continuous and dis-
crete data. We show that our method outperforms competitive
relaxation-based optimization methods and is also applicable
to non-differentiable score functions.

Introduction

The vast majority of modern machine learning advance-
ments share one central method - gradient-based optimiza-
tion. Stochastic gradients give a scalable solution for learn-
ing, applicable when the loss function is too slow to com-
pute due to the size of data or even intractable. The latter
is often the case when the loss function includes an expec-
tation over random latent variables. The objectives of this
kind naturally arise in multiple settings, including proba-
bilistic latent variable models (Neal and Hinton 1998) and
reinforcement learning (Williams 1992). Often the distribu-
tion of random variables also depends on the optimizable pa-
rameters of the loss function, which in turn makes gradient
estimation harder and less reliable due to the high variance
of stochastic gradients.

Despite the recent breakthroughs in gradient estimation
for continuous latent variables (Kingma and Welling 2013;
Rezende, Mohamed, and Wierstra 2014; Mohamed et al.
2019), gradient estimation for discrete latent variables re-
mains a challenge. Currently, general-purpose estimators

∗Both authors contributed equally to this work.
†Corresponding author. E-mail: artygadetsky@yandex.ru
‡Samsung-HSE Laboratory

(Williams 1992; Mnih and Gregor 2014) remain unreli-
able and the state-of-the-art methods (Tucker et al. 2017;
Grathwohl et al. 2018; Yin and Zhou 2018) exclusively con-
sider the categorical distribution. Although the reduction to
the categorical case allows beneﬁting from gradient estima-
tors for continuous relaxations, such solutions are hard to
translate to discrete distributions with large support.

In this work, we consider a gradient estimator for the
Plackett-Luce distribution, a distribution over permutations.
Permutations naturally occur in various setting, such as
ranking problems (Guiver and Snelson 2009), optimal rout-
ing (Bello et al. 2016) and causal inference (Friedman and
Koller 2003). However, the support of the distribution is su-
perexponential in the number of items k, which makes repre-
senting a distribution as a categorical distribution intractable
even for dozens of items. At the same time, the Plackett-
Luce distribution has O(k) parameters and allows sampling
in O(k log k).

We translate the recent variance reduction techniques
(Tucker et al. 2017; Grathwohl et al. 2018) to the case
of Plackett-Luce distributions. Similarly to REBAR, we
use the difference of the REINFORCE estimator and the
reparametrized estimator for the relaxed model. In partic-
ular, we derive the conditional marginalization step (Tucker
et al. 2017) for the Plackett-Luce case. In our experiments,
we recast causal inference tasks as a variational optimization
over permutations and solve it using a gradient optimization
method. We show that our method outperforms competitive
relaxation-based approaches for optimization over permuta-
tions (Grover et al. 2019; Mena et al. 2018) for differentiable
score functions and is applicable in a wider range of scenar-
ios.

Our main contributions are the following:

• We derive a low-variance gradient estimator for the

Plackett-Luce distribution.

• We apply the gradient estimator to solve variational op-
timization tasks for black-box functions and concentrate
primarily on causal inference tasks for continuous and
discrete data.

Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

• For differentiable functions, we show that relaxation-
based gradient optimization does not work out-of-the-box

for causal inference tasks and propose additional con-
straints to achieve competitive results.

a vector of independent Gumbel random variables zi ∼
G(θi, 1), i = 1, . . . , k

A Brief Tour of Gradient Estimation
We consider a general optimization task minθ Ep(b|θ)[f (b)],
where b is a discrete random variable parametrized by θ. The
expectation can be intractable, for instance when b is a vector
of categorical variables and the support of b is exponential
in the vector length. The standard solution is to construct a
Ep(b|θ)[f (b)]
stochastic estimate for the gradient ˆg(f ) := ∂
∂θ
without explicitly computing the expectation. In this section,
we brieﬂy review the gradient estimation algorithms.

REINFORCE
The REINFORCE estimator (Williams 1992) gives us a
widely-applicable unbiased estimate for the gradient

ˆgREINFORCE(f ) = f (b)

log p(b | θ), b ∼ p(b | θ).

(1)

∂
∂θ

Although an unbiased gradient estimate is sufﬁcient to guar-
antee convergence of stochastic gradient descent, in prac-
tice, the algorithm may not converge due to the high vari-
ance of the estimate (Tucker et al. 2017). The variance of the
REINFORCE estimator can be reduced using control vari-
ates. A Control variate is a function c(b) with a zero mean
Ep(b|θ)[c(b)] = 0 that can be used to deﬁne another unbiased
estimator

ˆgCV(f ) = ˆgREINFORCE(f ) − c(b).

(2)

The variance of the new estimator ˆgCV(f ) is lower than the
variance of ˆgREINFORCE(f ) if c(b) is positively correlated
with the random variable f (b). As an illustration, the gra-
dient of probability ∂
∂θ log p(b | θ) has zero mean, therefore
it can be used as a control variate (Mnih and Gregor 2014).

Reparametrization Gradients for Continuous
Relaxations
The reparametrization trick (Kingma and Welling 2013;
Rezende, Mohamed, and Wierstra 2014) is an alternative un-
biased low-variance gradient estimator, applicable when f is
differentiable and the latent variable bcont is continuous. The
estimator represents the latent variable as a differentiable de-
terminisitc transformation bcont = T (v, θ) of a ﬁxed distri-
bution sample v and parameters θ and estimates the gradient
as

ˆgreparam(f ) =

∂
∂θ
vi ∼ uniform[0, 1], i = 1, . . . , k.

f (bcont) =

∂f
∂T

∂T
∂θ

,

(3)

(4)

Although the reparametrization trick is not applica-
ble when the latent variable b is discrete, (Jang, Gu,
and Poole 2016; Maddison, Mnih, and Teh 2016) pro-
posed the Gumbel-softmax estimator, a modiﬁcation of the
reparametrization trick for the relaxed categorical distribu-
tion.

To sample from a relaxed categorical distribution p(b | θ)
, Gumbel-Softmax ﬁrst samples

with probabilities

(cid:80)

exp θi
j exp θj

zi = T (θi, vi) = θi − log(− log(vi))
vi ∼ uniform[0, 1], i = 1, . . . , k

(5)
(6)
with location parameter θ. According to the Gumbel-max
trick (Maddison, Tarlow, and Minka 2014), the index of
the maximal element H(z) = arg max(z) is a categor-
ical random variable with distribution p(b | θ). Then,
to make the sampler differentiable, the Gumbel-softmax
trick replaces arg max(z) with a relaxation soft max(z) =
1
(exp z1, . . . , exp zk). The gradient estimate is the
(cid:80) exp zi
reparametrization gradient for the relaxed categorical distri-
bution:

∂b
∂z

∂z
∂θ

,

f (b) =

ˆgGumbel(f ) =

∂
∂θ
b = soft max(z),
zi ∼ G(θi, 1), i = 1, . . . , k.

∂f
∂b

(8)
(9)
The resulting reparametrization gradient ˆgGumbel(f ) has
much lower variance than ˆgREINFORCE(f ), but is generally
biased due to the relaxation.

(7)

Relaxation-based Control Variates
Recently, Tucker et al. (2017) and Grathwohl et al. (2018)
proposed control variates for REINFORCE estimator based
on the relaxed conditional distribution. Both works use the
REINFORCE gradient estimator for the relaxed categorical
distribution as a control variate for the non-relaxed estima-
tor. To eliminate the bias of the REINFORCE estimator, they
subtract the low-variance reparametrization gradient estima-
tor.

The key insight of Tucker et al. (2017) is the conditional
marginalization step used to correlate the non-relaxed RE-
INFORCE estimator and the control variate. Importantly, the
conditional marginalization relies on reparametrization trick
for the conditional distribution p(z | b, θ), obtained from
the joint distribution p(b, z | θ) = p(b|z)p(z | θ) of the
Gumbel random vector z and the output of the Gumbel-max
trick b = H(z) = arg max(z). Tucker et al. (2017) derive a
reparametrizable sampling scheme for p(z | b, θ)

˜zi =

(cid:40)− log(− log vi)
(cid:16)
− log vi
exp θi

− log

+ exp(−˜zb)

(cid:17)

i = b

i (cid:54)= b

,

(10)

where vector v is a uniform i.i.d. vector v ∼ uniform[0, 1]k.
This gives a two-step generative process for the distribution
p(z | b, θ). On the ﬁrst step we sample the maximum vari-
able vb from the Gumbel distribtuion and on the second step
we sample the other variables vi, i (cid:54)= b from the Gumbel
distribution trunctated at ˜zb with location parameter θi.

The unbiased RELAX estimator from Grathwohl et al.

(2018) is

ˆgRELAX(f ) =[f (b) − cφ(˜z)]

log p(b | θ)

∂
∂θ
∂
∂θ

+

cφ(z) −

cφ(˜z)

∂
∂θ

b = H(z), z ∼ p(z | θ), ˜z ∼ p(z | b, θ)

(11)

(12)

where cφ(z) is a parametric function optimized to reduce the
variance of the estimator.

Similarly, for a differentiable function f the REBAR es-
timator by Tucker et al. (2017) uses the function f with the
relaxed argument soft max(z) and tunes the scalar parame-
ter η

ˆgREBAR(f ) =[f (b) − ηf (soft max(˜z))]

log p(b | θ)

∂
∂θ

with location parameters speciﬁed by score vector θ

zi = θi − log(− log(vi)), vi ∼ uniform[0, 1].

(16)

Then for a permutation b ∈ Sk the probability of event
{zb1 ≥ · · · ≥ zbk } is

p(zb1 ≥ · · · ≥ zbk ) =

k
(cid:89)

j=1

exp θbj
u=j exp θbu

.

(cid:80)k

(17)

(13)

(14)

Similarly to the Gumbel-max trick, Lemma 1 shows
that an order of a Gumbel-distributed vector is distributed
according to the Plackett-Luce distribution. Following the
lemma, for Plackett-Luce distributions we deﬁne p(z | θ)
to be a Gumbel-distributed vector and H(z) to be a sorting
operation

+ η

f (soft max(z))

− η

f (soft max(˜z))

∂
∂θ
∂
∂θ

b = H(z), z ∼ p(z | θ), ˜z ∼ p(z | b, θ)

Constructing Control Variates for the
Plackett-Luce Distribution
In this paper, we extend the stochastic gradient estimators
ˆgREBAR(f ) and ˆgRELAX(f ) from the categorical distribution
to the Plackett-Luce distribution. With a slight abuse of no-
tation, below we use letter b to denote an integer vector
b = (b1, . . . , bk) ∈ Sk that represent a permutation, θ to
denote the parameters of the Plackett-Luce distribution and
p(b | θ) to denote the Plackett-Luce distribution.

The goal of this section is to deﬁne the two components
required to apply the aforementioned gradient estimators:
the mapping b = H(z) and the two reparametrizable con-
ditional distributions p(z | θ) and p(z|b, θ). After this we
apply the estimators as deﬁned in eq. 11 and eq. 13, but to
emphasize the difference we refer to them as PL-RELAX
and PL-REBAR.

Deﬁnition 1. The Plackett-Luce distribution (Luce 2005;
Plackett 1975) with scores θ = (θ1, . . . , θk) is a distribu-
tion over permutations Sk with the probability of outcome
b ∈ Sk

p(b|θ) =

k
(cid:89)

j=1

exp θbj
u=j exp θbu

.

(cid:80)k

(15)

Intuitively, a sample from the Plackett-Luce distribution b =
(b1, . . . , bk) is generated as a sequence of samples from cat-
egorical distributions. The ﬁrst component b1 comes from
the categorical distribution with logits θ, then the second
components b2 comes from the categorical distribution with
the logits θ without the component θb1 and so on.

The Plackett-Luce can be used for variational optimiza-
tion (Staines and Barber 2012). Indeed, at the lower tem-
peratures θ → θ
T , T (cid:28) 1 the distribution converges to a
divergent distribution. The mode of the Plackett-Luce dis-
tribution is the descending order permutation of the scores
b0 : θb0
, because b0 permutation maximizes
each factor in the product in eq. 15.

≥ · · · ≥ θb0

k

1

Now we will give an alternative deﬁnition of the Plackett-

Luce distribution.

Lemma 1. (appears in (Grover et al. 2019; Yellott Jr 1977))
Let z be a vector of k independent Gumbel random variables

zi ∼ G(θi, 1), i = 1, . . . , k
H(z) = arg sort(z)

(18)
(19)

Our principal discovery is that, similarly to the categori-
cal case, the conditional distribution p(z|b, θ) factorizes into
a sequence of truncated Gumbel distributions. As a conse-
quence, the distribution is reparametrizable and can be used
to construct a control variate for a gradient estimator.
Proposition 1. Let p(b, z | θ) be the joint distribution
with zi ∼ G(θi, 1), b = arg sort(z) and normalized pa-
rameters (cid:80)k
j=1 exp θj = 1. Then for uniform i.i.d samples
vi ∼ uniform[0, 1] and Θi = (cid:80)k
j=i exp θbj for i = 1, . . . , k
the vector ˜z = (˜z1, . . . , ˜zk)

(cid:40)

˜zbi =

− log(− log vi)
− log(− log vi
Θi

+ exp(−˜zbi−1 ))

i = 1
i ≥ 2,

(20)

is a sample from the conditional distriubtion p(z | b, θ).

The proof of the proposition is given in the appendix.
The sampling procedure from Proposition 1 has two prin-
cipal differences from the sampling scheme for the categor-
ical case (see eq. 10). First, the truncation parameter ˜zbi−1
now depends on the previous component i − 1, while for
the categorical case the truncation parameter is deﬁned by
the maximum component. Second, the location parameter is
now a cumulative sum and depends on the previous scores.

Related Work
Jang, Gu, and Poole; Maddison, Mnih, and Teh (2016;
2016) use the Gumbel distribution and Gumbel-max trick
to deﬁne continuous relaxations of discrete distributions,
by providing a gradient estimator which replaces the sam-
pling of a categorical distribution with a differentiable sam-
ple from a Gumbel-Softmax distribution.

The Gumbel-Softmax distribution does not scale to per-
mutations, as distribution over k-dimensional permutations
is equivalent to that over k! categories. Recently, a line of
work proposed various for optimization over permutations.
Linderman et al. (2018) relaxes the discrete set of permu-
tations to Birkhoff polytope, the set of doubly-stochastic
matrices, and extend stick-breaking approach (Sethuraman

Figure 1: Training curves and log-variance of gradient estimators for different estimators on a toy problem: Ep(b|θ)(cid:107)Pb−P0.05(cid:107)2
F

1994) to satisfy polytope constraints. Mena et al. (2018) ob-
tain doubly-stochastic matrices by applying the Sinkhorn
operator. They use the Gumbel-Softmax distribution to
deﬁne a distribution over latent matchings, the implicit
Gumbel-Sinkhorn distribution. Grover et al. (2019) deﬁne
new relaxation to the set of unimodal row-stochastic matri-
ces, the set of matrices that have a unique maximal element
in every row.

Grathwohl et al. (2018) extend Tucker et al. (2017) and
derive control variate for black-box function optimization
combining the REINFORCE estimator and reparametriza-
tion trick. Yin and Zhou (2018) propose gradient estimator
that estimates the gradients of discrete distribution parame-
ters in an augmented space.

For the special case of TSP, (Bello et al. 2016; Kool, van
Hoof, and Welling 2018) introduce an amortized family of
distributions over permutations using a deep autoregressive
model and design control variates that exploit the structure
of the loss function.

Experiments
We demonstrate the effectiveness of the proposed method
with a simple toy task similar to Tucker et al. (2017)
and then continue to the more challenging task of op-
timization over topological orderings for solving causal
structure learning problems. Our PyTorch (Paszke et al.
2017) implementation of the gradient estimators is avail-
able at https://github.com/agadetsky/pytorch-pl-variance-
reduction .

Toy Experiment
As a proof of concept we perform an experiment in min-
imizing Ep(b|θ)(cid:107)Pb − Pt(cid:107)2
F = Ep(b|θ)f (Pb) as a function
of θ where p(b|θ) = Plackett-Luce(b|θ). Pb is permutation
matrix with elements pi,bi = 1 and Pt is a matrix with
k + t on the main diagonal and 1
1
k−1 in the remaining
positions. This problem can be seen as linear sum assign-
ment problem with speciﬁcally constructed doubly stochas-

k − t

tic matrix Pt. It is easy to note that taking k = 2 and
t = 0.05 leads to toy problem similar to that of Tucker
et al. (2017). We focus on t = 0.05 and k = 8 to enable
computation of exact gradients. For the PL-REBAR estima-
tor we take cφ(z) = ηf (σ(z, τ )) where σ(z, τ ) is the con-
tinuous relaxation of permutations described by Grover et
al. (2019). For the PL-RELAX estimator we take cφ(z) =
f (σ(z, τ )) + ρφ(z) where ρφ(z) is a simple neural network
with two linear layers and ReLU activation between them.
Figure 1 shows the relative performance and gradient log-
variance of REINFORCE, PL-REBAR and PL-RELAX. Al-
though the REINFORCE estimator is unbiased, we can see
that the variance of the estimator is too large even for the
simple toy task, therefore the method is completely inap-
plicable for optimization over permutations. On the other
hand, the proposed method signiﬁcantly reduces variance of
the gradient and thus converges to optimal. Also, similarly
to the toy experiment from Grathwohl et al. (2018) paper,
we observe better performance of the PL-RELAX estimator
due to free-form control variate parameterized by a neural
network.

Causal Structure Learning Through Order Search
Directed acyclic graph (DAG) models are popular tools for
describing causal relationships and for guiding attempts to
learn them from data. Learning the structure of a DAG re-
mains challenging because of the combinatorial acyclicity
constraint. A common way to model causal relations is a
structural equation model (SEM). Let X be k-dimensional
random variable, then relations are described as follows:

Xi = fi(Xpa(i), εi),

(21)

where pa(i) is the set of parent vertices of variable Xi and
εi is independent noise. Edge set {∪k
i=1 ∪j∈pa(i) j −→ i}
describes DAG G on k vertices associated with joint distri-
bution PG(X) = (cid:81)k
P(Xi|pa(Xi)). The basic structure
learning problem can therefore be formulated as follows: let
X be data matrix consisting of n i.i.d. samples of random

i=1

SHD

SHD-CPDAG SID

SHD

SHD-CPDAG SID

Table 1: Results for ER and SF graphs of 10 nodes

ER1

Val (cid:98)Q − (cid:98)Q∗
-0.2±1.7
1.8±5.3
13.5±26.9
85.9±101.2
71.4±128.9
N/A

122.3±184.3
SF1

Val (cid:98)Q − (cid:98)Q∗
-0.7±0.3
0.6±2.9
1.6±1.8
22.6±22.4
10.1±5.2
N/A

PL-RELAX
SINKHORNECP
URSECP
SINKHORN
URS
GREEDY-SP

RANDOM

PL-RELAX
SINKHORNECP
URSECP
SINKHORN
URS
GREEDY-SP

5.2±2.5
5.6±2.7
7.4±3.7
12.0±3.7
10.8±2.9
2.2±2.9

5.8±3.2
6.4±2.9
7.4±3.6
12.0±3.7
11.0±3.2
2.4±3.9

18.8±2.5

18.8±2.6

2.2±1.5
2.8±3.2
5.0±1.7
9.0±0.0
9.6±1.2
0.8±0.4

2.4±1.5
3.0±3.2
5.4±2.2
9.2±0.4
9.6±1.2
0.0±0.0

ER4

Val (cid:98)Q − (cid:98)Q∗
12.2±26.3
4.8±10.4
12.4±6.2
4019.6±3138.0
1894.9±1704.8
N/A

10078.2±10770.5
SF4

Val (cid:98)Q − (cid:98)Q∗
-1.3±1.4
-0.3±4.0
2.1±2.3
232.4±251.8
69.6±81.6
N/A

13.0±9.6
14.2±10.2
16.0±8.9
29.4±17.3
26.0±10.5
8.8±15.4

27.2±14.3

2.6±2.2
7.6±12.3
7.0±2.2
17.4±3.8
14.6±2.1
2.8±1.6

29.4±1.9
31.2±2.6
29.8±3.6
36.6±2.4
34.6±2.2
29.8±1.1

35.0±4.4
33.6±2.7
32.8±4.9
37.8±1.7
36.8±2.8
35.4±5.0

25.4±3.2

33.0±4.5

8.2±3.1
6.6±1.5
12.8±2.5
17.2±2.8
14.6±1.4
5.0±9.5

8.8±3.3
7.0±1.8
13.4±2.2
17.6±3.4
14.6±1.2
4.2±9.4

RANDOM

35.4±22.4

17.2±2.6

18.0±2.6

23.6±6.4

240.3±251.0

34.4±2.6

35.6±2.2

SHD

SHD-CPDAG SID

SHD

SHD-CPDAG SID

67.0±1.8
69.6±2.3
67.4±2.1
79.8±6.9
74.4±2.7
71.6±3.8

65.8±5.9

15.4±5.9
11.8±4.0
24.8±5.6
34.4±9.9
29.2±5.8
11.0±12.7

31.4±11.2

SHD

SHD-CPDAG SID

SHD

SHD-CPDAG SID

Table 2: Results for ER and SF graphs of 20 nodes

ER1

Val (cid:98)Q − (cid:98)Q∗
15.7±27.3
10.4±8.7
27.5±34.2
1651.2±3050.4
1189.4±1815.5
N/A

895.1±1270.3
SF1

Val (cid:98)Q − (cid:98)Q∗
-1.5±0.2
1.9±4.3
3.0±2.0
38.3±26.2
38.3±26.2
N/A

PL-RELAX
SINKHORNECP
URSECP
SINKHORN
URS
GREEDY-SP

RANDOM

PL-RELAX
SINKHORNECP
URSECP
SINKHORN
URS
GREEDY-SP

SHD

SHD-CPDAG SID

14.4±5.3
15.8±4.7
20.6±6.3
24.0±6.1
26.4±8.4
18.6±13.5

16.0±6.2
17.0±6.0
21.4±7.2
25.0±6.7
26.6±8.6
18.0±16.6

37.8±5.2

38.8±4.9

61.0±48.7
84.8±56.3
96.8±74.6
131.2±76.5
134.2±75.0
74.0±53.5

146.8±79.9

4.0±0.6
6.6±2.2
10.6±2.0
19.0±0.0
19.0±0.0
2.0±1.4

4.6±0.5
6.6±2.4
10.6±1.6
19.0±0.0
19.0±0.0
0.0±0.0

4.2±0.7
10.4±5.0
14.4±4.0
35.0±2.4
35.0±2.4
7.0±5.1

ER4

Val (cid:98)Q − (cid:98)Q∗
468.8±208.4
2519.0±3715.2
1011.4±745.5
126284.6±194386.3
7179677.6±7874489.3
N/A

109891.2±74968.7
SF4

Val (cid:98)Q − (cid:98)Q∗
-5.8±1.2
-0.4±2.4
8.5±11.8
158.2±99.9
140.7±140.6
N/A

SHD

SHD-CPDAG SID

71.0±5.9
78.0±6.1
75.8±2.9
88.8±6.0
93.0±3.8
103.4±10.9

72.6±3.9
78.8±5.5
76.6±2.9
91.0±5.7
94.4±4.5
105.6±10.5

289.6±9.1
302.2±15.8
300.2±20.3
330.0±14.1
328.0±11.5
288.6±14.7

113.0±4.9

114.4±4.1

330.6±9.2

20.0±4.3
25.6±5.6
30.2±5.8
44.6±5.8
42.0±5.4
50.6±31.5

20.0±4.1
25.8±5.9
30.6±5.2
44.8±6.1
42.8±5.1
49.8±32.3

48.4±16.2
58.6±19.7
72.2±25.0
103.6±20.8
89.8±20.4
69.0±43.2

168.8±29.6

RANDOM

94.0±36.4

36.2±2.6

36.6±2.3

48.6±14.7

635.5±182.6

98.2±6.1

99.2±5.5

ER1

Val (cid:98)Q − (cid:98)Q∗
-1.8±1.3
5.5±7.0
10.3±4.7
90.3±35.8
90.3±35.8
N/A

271.0±71.6
SF1

Val (cid:98)Q − (cid:98)Q∗
-3.9±0.5
25.1±18.2
32.1±44.3
138.2±68.2
138.2±68.2
N/A

PL-RELAX
SINKHORNECP
URSECP
SINKHORN
URS
GREEDY-SP

RANDOM

PL-RELAX
SINKHORNECP
URSECP
SINKHORN
URS
GREEDY-SP

Table 3: Results for ER and SF graphs of 50 nodes

SHD

SHD-CPDAG SID

19.2±6.9
30.0±6.3
41.0±2.4
49.6±4.3
49.6±4.3
38.2±21.6

20.6±7.8
30.8±5.8
40.0±2.7
49.6±4.3
49.6±4.3
38.2±24.6

ER4

Val (cid:98)Q − (cid:98)Q∗
1863.1±1703.2
43463.9±70904.3
22997.9±38346.1
231304.8±290019.0
546793216.7±984510739.7

103.2±55.5
151.8±35.1
177.6±17.1
275.0±42.5
275.0±42.5
151.6±84.3 N/A

99.4±9.3

99.8±9.5

301.2±60.4

SHD

SHD-CPDAG SID

11.4±3.3
28.6±6.5
33.4±10.2
49.0±0.0
49.0±0.0
38.8±39.3

11.8±2.9
28.4±6.1
33.6±10.7
49.0±0.0
49.0±0.0
35.4±39.6

14.4±2.7
58.4±12.1
55.6±32.7
110.6±5.5
110.6±5.5
54.8±20.6

477442.0±661243.9
SF4

Val (cid:98)Q − (cid:98)Q∗
-1.1±7.6
124.3±126.0
164.4±53.1
10238.2±15850.1
7966.9±4838.0
N/A

SHD

SHD-CPDAG SID

220.6±42.8
221.0±14.7
239.4±31.6
248.6±18.5
320.2±26.8
525.6±35.5

221.4±43.5
223.2±15.2
240.2±31.5
250.4±19.1
320.8±27.1
526.8±34.7

1779.6±193.1
1846.4±158.3
1789.8±154.4
1966.8±135.5
2119.0±130.5
1951.4±50.3

360.8±23.5

361.0±23.2

2175.0±52.6

SHD

SHD-CPDAG SID

70.0±9.9
94.4±22.7
110.6±12.8
139.0±8.3
142.8±11.8
381.2±76.2

70.6±11.2
95.6±23.0
111.4±13.7
139.6±8.1
144.2±12.1
384.2±77.0

219.0±20.3
257.2±25.8
319.6±18.1
387.0±37.2
527.4±86.8
963.0±475.7

RANDOM

380.1±207.8

97.8±7.3

97.8±7.3

155.4±31.2

10109.8±2027.0

312.0±14.9

312.4±15.0

807.0±101.7

variable X. Also let D be space of DAGs. Then, given ob-
servations X the task is to ﬁnd DAG G ∈ D or so-called
Bayesian Network for joint distribution P(X):

doubly-stochastic matrices and contains the set of all per-
mutation matrices. Since these methods can’t be used to op-
timize black-box functions we reformulate (26) as:

min
G∈D

Q(G, X)

(22)

min
φ

min
A∈A

1
2n

(cid:107)X −P (φ)AP (φ)T X(cid:107)2

F +λ(cid:107) vec(A)(cid:107)1 (27)

where Q is function that scores DAG G given data.

To incorporate permutations in the objective (22) we con-
sider parametrization of DAG adjacency matrix using nilpo-
tent matrices which are upper triangular in basis induced
by topological ordering, namely WG = P AP T where A
is strictly upper triangular adjacency matrix which describes
parent sets of variables and permutation matrix P which de-
scribes topological ordering. Then optimization over DAGs
can thus be seen as an optimization over topological order-
ings

min
P ∈Pk

(cid:98)Q(P, X),

(23)

where (cid:98)Q scores topological ordering P and Pk is the set
of permutation matrices of size k. Optimization over A is
usually hidden in the computation of (cid:98)Q. It is worth noting
that this approach is similar to order MCMC (Friedman and
Koller 2003), however our work considers gradient-based
optimization over permutations matrices rather than discrete
order changes.

Continuous data We consider
SEMs:

linear additive noise

X = W T X + ε

(24)

where W = P AP T and non-zero elements of A describe
linear coefﬁcients and parent sets for each variable Xi.

As score function (cid:98)Q we take regularized mean squared
loss combined with sparsity-inducing L1 regularization term

(cid:98)Q(P, X) = min
A∈A

1
2n

(cid:107)X − P AP T X(cid:107)2

F + λ(cid:107) vec(A)(cid:107)1,

(25)
where A is the set of strictly upper triangular matrices. Com-
puting (cid:98)Q itself involves optimization problem, which can
be efﬁciently solved using accelerated proximal gradient for
convex composite function optimization (Nesterov 2013).
To apply the proposed method, we reformulate (23) as varia-
tional optimization with respect to parameters of a Plackett-
Luce distribution:

min
θ

E
p(b|θ)

(cid:98)Q(Pb, X)

(26)

where p(b|θ) = Plackett-Luce(b|θ), and Pb is a permutation
matrix with pi,bi = 1. For variational optimization, we only
apply PL-RELAX and treat (cid:98)Q(P, X) as a black-box func-
tion to avoid unrolling the optimizer to compute gradients.

As a concurrent approach, we consider work by Mena et
al. (2018) which proposes relaxing optimization over a set
of permutations to a set of doubly-stochastic matrices us-
ing the Sinkhorn operator. Another recent work by Grover
et al. (2019) proposes relaxation to the set of unimodal
row-stochastic matrices (URS) which intersects the set of

where φ are the parameters of the corresponding relaxation.
We optimize (27) coordinate-wise using gradient descent
with respect to φ and accelerated proximal gradient opti-
mization with respect to A. We refer to the optimization of
this objective as SINKHORN or URS according to the used
relaxation.

We also try an alternative approach for the above relax-
ations. Since P (φ) is not a permutation matrix during train-
ing we extend (27) with an orthogonality constraint and re-
place (cid:107)vec(A)(cid:107) with Hµ(vec(P AP T )) where Hµ is the Hu-
ber relaxation of L1 norm and µ is a hyperparameter control-
ling tightness of relaxation:

min
φ

min
A∈A

s. t.

(cid:107)X − P (φ)AP (φ)T X(cid:107)2

1
2n
+ λHµ(vec(P (φ)AP T (φ)))
(cid:107)P (φ)P T (φ) − Ik(cid:107)2

F = 0

F +

(28)

We use an Augmented Lagrangian (Nemirovski 1999) to
solve this equality constrained optimization problem (ECP)
(28) and refer to the solutions as SINKHORNECP or
URSECP correspondingly.

We simulated graphs from two well-known random graph
models with different degree distributions: Erdos-Renyi ran-
dom graphs and Scale-free networks with k and 4 k ex-
pected number of edges, denoted by ER1, ER4, SF1, SF4 re-
spectively. Given a random acyclic graph we assigned edge
weights independently from U ([−2; −0.5] ∪ [0.5; 2]) to ob-
tain weight matrix W . To generate data matrix X we follow
generating process of linear SEM (24) with standard Gaus-
sian noise.

As a sanity check, we also introduce a simple baseline. We
generate Erdos-Renyi random graphs with the correspond-
ing expected number of edges and refer to it as RANDOM
baseline. For comparison, we also include the Greedy Sparse
Permutation (Greedy-SP) algorithm (Solus et al. 2017). This
algorithm casts DAG structure learning as a linear program-
ming problem with graph sparsity as the linear objective
function, and a sub-polytope of the permutohedron as the
feasible region. Whilst this algorithm also searches permu-
tations as a proxy to DAGs to reduce the size of the search
space, it is in essence a constraint-based method - rather than
optimising a DAG score function, it searches for the sparsest
DAG which satisﬁes the conditional independence relations
found. Conversely, our gradient-based method does not rely
on these conditional independence tests, which typically re-
quire the simplifying assumptions of CI tests, and is able to
use linear as well as non-linear objective functions (e.g. in
the discrete data experiment, the quotient normalized maxi-
mum likelihood score is non-linear and non-differentiable).
For each method we report the score difference (cid:98)Q (25) be-
tween learned and ground truth DAGs on additionally gen-
erated validation samples Xval, as well as three DAG met-

rics from causal inference literature. The quoted score differ-
ence shows the effectiveness of our method for optimizing
the chosen score function, while the DAG metrics show how
well it performs on the problem itself. Structural hamming
distance (SHD) is the number of edge additions, removals,
and reversals required to get from the learned structure to
the ground truth. Multiple DAGs can represent the same set
of conditional independence relations, forming a Markov
equivalence class; this can be represented by a completed
partially directed acyclic graph (CPDAG). We also report
SHD-CPDAG - the SHD between the CPDAG the learned
structure belongs to and that of the true structure. Structural
interventional distance (SID) (Peters and B¨uhlmann 2013)
quantiﬁes the distance between two DAGs in terms of their
respective causal inference statements. This gives an indica-
tion of accuracy of computed interventions using the learned
graph.

We consider graphs of 10, 20 and 50 nodes.
For PL-RELAX we take the mode of the distribution af-
ter training. For SINKHORN relaxation we apply the Hun-
garian algorithm to ﬁnd the closest permutation matrix. For
URS we use the argmax permutation property to obtain the
permutation matrix. Regularization coefﬁcient λ is set to 0.5
for all methods.

Tables 1-3 show the performance of all methods for vary-
ing number of nodes k averaged across 5 random seeds (the
error ranges represent standard deviation). We can see that
the proposed method outperforms baselines in the majority
of settings. Also, it is worth mentioning that SINKHORN
and URS perform poorly in terms of score function values
due to the fact that the optimization is carried out over the
set of relaxed matrices. This leads to deterioration in score
value (cid:98)Q when relaxation is transformed to permutation. As
we can see there is no such problem with ECP versions of re-
laxations, though they perform worse than PL-RELAX and
require additional constrained optimization techniques to be
applied. Also, one more observation should be explained:
PL-RELAX almost always ends up with better solutions in
terms of score function than the ground truth DAG, there-
fore solves the optimization problem well. However, it is not
ideal in terms of metrics. Peters and B¨uhlman (2014) proved
that given enough data, it is possible to identify the ground
truth DAG if data was generated from linear SEM with
Gaussian homogeneous noise. Authors used L0-regularized
mean squared error score function, but it is non-convex and
hard to optimize, therefore L1-regularization is used in prac-
tice. Because of relaxation of the L0 norm and ﬁnite amount
of data all guarantees vanish, and we observe inconsistency
between the metrics of interest and values of the surrogate
score function (cid:98)Q.

Discrete data Due to the discrete and nonlinear nature of
categorical data, it cannot be modeled with the SEM de-
ﬁned previously. Discrete variable networks can however
be modeled as generated by sampling each node’s condi-
tional probability table, depending only on the conﬁgura-
tion of its parent nodes. In the standard general form this
is Xi = fi(Xpa(i)), where fi is assumed to be multinomial,

thus

fi(Xpa(i)) ∼ M ultinomial(ΘXi|P a(Xi))

where ΘXi|P a(Xi) are the conditional probabilities

θi,j,k = P (Xi = k|P a(Xi) = j).

Rather than learning the optimal A for a given P by min-
imising a training loss, we can therefore instead try to max-
imise the marginal likelihood based on the above model

Q(P, X) = max
A∈A

P (X|A, P )

(29)

which can be found using the factorisation

P (X|A, P ) =

P (Xi,pa(i)=j; α).

(30)

d
(cid:89)

qi
(cid:89)

i=1

j=1

As a result of the decomposition of the score by node in
equation (30), the maximum a posteriori (MAP) parent set
can be selected from the set of parents permitted by the topo-
logical ordering for each node, independently of the rest.
Due to the ordering, the graph resulting from combining
each of these MAP parent connections is guaranteed to be
acyclic, thus the exact MAP DAG for a given ordering can
be found. Due to the combinatorial size of even this reduced
search space, the set of permitted parents for a given node
is reduced further, to only those that cannot be easily proven
to be conditionally independent - as determined by a stan-
dard constraint-based method (in this case the PC-stable al-
gorithm (Colombo and Maathuis 2014)). As this ﬁnds the
exact solution for a reduced search space, the result is an
approximation of the best score possible for the ordering.
Whilst this provides an approximate score for any given or-
der, it is a non-differentiable black-box function; therefore
whilst our method can be applied to this permutation op-
timization, options are severely limited - the SINKHORN
and URS methods used for continuous SEM graph bench-
marks for example cannot be used. For a simple evaluation,
Table 4 shows the result of our method on data sampled from
the standard ALARM network compared against random or-
ders, and permutations optimized by order MCMC (Fried-
man and Koller 2003), all using the same MAP DAG method
described above, maximizing the quotient normalized max-
imum likelihood score (Silander et al. 2018). Higher Val
(cid:98)Q− (cid:98)Q∗ is better, other metrics lower is better. Whilst Table 4
shows our algorithm to be less effective than MCMC for this
task, the comparison is not particularly favorable - MCMC
is performed directly on permutations, rather than attempt-
ing to learn the Plackett-Luce distribution over permutations
- thus the MCMC simply attempts to ﬁnd a good local min-
imum in the score space. To give a lower bound to perfor-
mance, we also compare to the MAP DAGs of 1000 random
permutations, computed in the same way as for MCMC and
our algorithm, showing sampling the learned Plackett-Luce
distribution gives permutations far better than random.

Conclusion
In this work we proposed a gradient-based optimization
method, with unique capabilities for application to Plackett-
Luce distributions over permutations. A proof of concept

Table 4: Results for ALARM graph (37 nodes)
SID

SHD-CPDAG

SHD

Val (cid:98)Q − (cid:98)Q∗
-15645.2±3255.8

PL-RELAX
SINKHORN
URS
ORDER MCMC -13404.7±2224.6

14.6±1.7

19.0±2.3

214.2±31.8

N/A
N/A

8.6±1.1

10.6±0.5

104.4±20.8

RANDOM

-75022.7±9647.7

25.8±3.7

30.0±3.9

478.8±70.8

experiment shows our method outperforms existing meth-
ods for differentiable objective functions, whilst also gen-
eralizing to non-differentiable black-box functions, and be-
ing applicable to permutation learning despite the factorial
complexity. This allowed us to extend Plackett-Luce distri-
bution based causal graphical model structure learning be-
yond the simple SEM based methods, to the more general
case of DAGs of arbitrary variable types.

In future, our method could be combined with other stan-
dard scoring functions from Bayesian network literature -
providing they decompose as described in equation (30) -
for DAG structure learning of continuous data from different
model types. Other potential applications include approxi-
mate inference for probabilistic models with latent permuta-
tions, routing problems and combinatorial problems for per-
mutations.

Acknowledgements
This work was partly supported by Sberbank AI Lab and UK
EPSRC project EP/P03442X/1. Kirill Struminsky proved
proposition 1 and was supported by the Russian Science
Foundation grant no. 19-71-30020. The authors thank NRU
HSE for providing computational resources, NVIDIA for
GPU donations, and Amazon for AWS Cloud Credits.

Appendix
We prove Proposition 1 in this section. We ﬁrst discuss the
properties of Gumbel distribution. Then we discuss the gen-
erative processes for the densities used for p(z | b, θ) in
Eq. 20. Then we show that p(b | z)p(z | θ) = p(b | θ)p(z |
b, θ) for the unconditional Gumbel density p(z | θ) and the
Plackett-Luce distribution p(b | θ).

Density for the Gumbel distribution and the
truncated Gumbel distribution
The density function of the Gumbel distribution with loca-
tion parameter µ is

φµ(z) = exp(−z + µ) exp(− exp(−z + µ))

(31)

and the cumulative density function is

Φµ = exp(− exp(−z + µ)).

(32)

Our derivation of the conditional distribution p(b | z, θ) re-
lies on the additive property of the cumulative density func-
tion of the Gumbel distribution

Φlog(exp µ+exp ν)(z) =
exp(− exp(z)(exp µ + exp ν)) = Φµ(z)Φν(z),

(33)

which we enfold in the following auxiliary claim.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

Lemma 2. For permutation b ∈ Sk, score vector θ ∈ Rk
and i = 1, . . . , k and the argument vector z ∈ Rk we have

φθbi

=

(zbi)Φlog((cid:80)k
exp θbi
j=i exp θbj

(cid:80)k

j=i+1 exp θbj )(zbi)

φlog((cid:80)k

j=i exp θbj )(zbi).

Proof. For brevity, we denote exp θi as pi. We then rewrite
the density φlog pbi
(zbi ) through the exponent exp(−zbi +
log pbi) and c.d.f. Φlog pbi
(zbi) and apply the additive prop-
erty in Eq. 38:

j=i+1 pbj )(zbi )

φlog pbi
(zbi)Φlog((cid:80)k
= pbi exp(−zbi)Φlog pbi
= pbi exp(−zbi)Φlog((cid:80)k

(zbi )Φlog((cid:80)k
j=i pbj )(zbi)

j=i+1 pbj )(zbi)

= pbi

(cid:80)k

(cid:80)k

j=i pbj
j=i pbj

=

pbi
j=1 pbj

(cid:80)k

exp(−zbi)Φlog((cid:80)k

j=i pbj )(zbi)

φlog((cid:80)k

j=i pbj )(zbi).

The last step collapses the exponent and the c.d.f. into the
density function φlog((cid:80)k
j=i pbj )(zbi).

Finally, to deﬁne the density of conditional distribution
p(b | z, θ) we deﬁne the density of the truncated Gumbel
distribution φz0

µ (z) ∝ φµ(z)I[z ≤ z0]:

φz0

µ (z) =

φµ(z)
Φµ(z0)

(z)I[z ≤ z0],

(41)

where the superscript z0 denotes the truncation parameter.

Reparametrization for the Gumbel distribution
and the truncated Gumbel distribution
The reparametrization trick requires representing a draw
from a distribution as a deterministic transformation of a
ﬁxed distribution sample and a distribution parameter. For
a sample z from the Gumbel distribution G(µ, 1) with loca-
tion parameter µ the representation is

z = µ − log(− log v), v ∼ uniform[0, 1].

(42)

For the Gumbel distribution truncated at z0 (Maddison, Tar-
low, and Minka 2014) proposed an analogous representation

z = µ − log(− log v + exp(−z0 + µ))

(cid:18)

= − log

log v
exp µ
v ∼ uniform[0, 1].

−

(cid:19)

+ exp(−z0)

(43)

(44)

In particular, the sampling schemes in Eq. 10 and Eq. 20
generate samples from the truncated Gumbel distribution.

The derivation of the conditional distribution
We now derive the conditional distribution and the sampling
scheme deﬁned in Proposition 1.

The joint distribution of the permutation b and the Gumbel

samples z is

p(b, z | θ) = p(b | z)p(z | θ)

(45)

= φθb1

(zb1 )

φθbi

(zbi)I[zbi−1 ≥ zbi ]

(46)

(cid:17)

k
(cid:89)

(cid:16)

i=2

We ﬁrst multiply and divide the joint density by the c.d.f.
Φlog((cid:80)k

i=2 exp θbi )(zb1) and apply Lemma 2

Φlog((cid:80)k
Φlog((cid:80)k

i=2 exp θbi )(zb1)
i=2 exp θbi )(zb1)
φlog((cid:80)k
Φlog((cid:80)k

=

exp θb1
i=1 exp θbi

(cid:80)k

i=1 exp θbi )(zb1)
i=2 exp θbi )(zb1)

k
(cid:89)

i=2

k
(cid:89)

i=2

φθb1

(zb1 )

. . .

(47)

. . . .

(48)

Next, we apply Lemma 2 to combine the c.d.f.
the denominator Φlog((cid:80)k
φθbi

in
j=i exp θbj )(zbi−1 ) and the term

(zbi)I[zbi−1 ≥ zbi] inside the product

(49)

(50)

(51)

=

φθbi
Φlog((cid:80)k
φθbi
Φlog((cid:80)k

(zbi)I[zbi−1 ≥ zbi ]
j=i exp θbj )(zbi−1 )
(zbi)I[zbi−1 ≥ zbi]
j=i exp θbj )(zbi−1)
zbi−1
log((cid:80)k
Φlog((cid:80)k

= exp θbi
(cid:80)k

j=i exp θbj

φ

Φlog((cid:80)k
Φlog((cid:80)k

j=i+1

exp θbj

j=i+1

exp θbj

)(zbi )
)(zbi )

(zbi)

j=i exp θbj )
j=i+1 exp θbj )(zbi)
zbi−1
log((cid:80)k

j=i exp θbj )

exp θbi
j=i exp θbj

and obtain the truncated distribution φ

(zbi )
along with one factor of the Plackett-Luce probability
. Also, after the transformation the summa-

(cid:80)k
tion index in the denominator c.d.f. changes from i to
i + 1. This gives us an induction step that we apply se-
quentially for i = 2, . . . , k − 1. For i = k the de-
nominator c.d.f. Φlog exp θk (zbk−1) and the product term
φlog exp θk (zbk )I[zk−1 ≥ zk] combine into the truncated
Gumbel distribution with density φ

(zbk ).

zbk−1
log exp θk

As a result, we rearrange p(b, z | θ) into the product of
the truncated Gumbel distribution densities p(z | b, θ) and
the probability of the Plackett-Luce distribution p(b | θ):

k
(cid:89)

i=1

(cid:32)

(cid:33)

exp θbi
j=i exp θbj

(cid:80)k

φ0(zb1 )

φ

zbi−1
log (cid:80)k

j=i exp θj

(zbi)

. (52)

k
(cid:89)

i=2

Finally, to obtain the claim of Proposition 1 we apply the

reparametrized sampling scheme deﬁned in Eq. 43.

References
Bello, I.; Pham, H.; Le, Q. V.; Norouzi, M.; and Bengio,
S. 2016. Neural combinatorial optimization with reinforce-
ment learning. arXiv preprint arXiv:1611.09940.
Colombo, D., and Maathuis, M. H. 2014. Order-independent
constraint-based causal structure learning. The Journal of
Machine Learning Research 15(1):3741–3782.

2016.

Categorical
arXiv preprint

Friedman, N., and Koller, D. 2003. Being bayesian about
network structure. a bayesian approach to structure discov-
ery in bayesian networks. Machine learning 50(1-2):95–
125.
Grathwohl, W.; Choi, D.; Wu, Y.; Roeder, G.; and Duve-
naud, D. 2018. Backpropagation through the void: Optimiz-
In
ing control variates for black-box gradient estimation.
International Conference on Learning Representations.
Grover, A.; Wang, E.; Zweig, A.; and Ermon, S. 2019.
Stochastic optimization of sorting networks via continuous
relaxations. In International Conference on Learning Rep-
resentations.
Guiver, J., and Snelson, E. 2009. Bayesian inference for
In proceedings of the 26th
plackett-luce ranking models.
annual international conference on machine learning, 377–
384. ACM.
Jang, E.; Gu, S.; and Poole, B.
reparameterization with gumbel-softmax.
arXiv:1611.01144.
Kingma, D. P., and Welling, M. 2013. Auto-encoding vari-
ational bayes. arXiv preprint arXiv:1312.6114.
Kool, W.; van Hoof, H.; and Welling, M.
2018. At-
tention, learn to solve routing problems! arXiv preprint
arXiv:1803.08475.
Linderman, S.; Mena, G.; Cooper, H.; Paninski, L.; and Cun-
ningham, J. 2018. Reparameterizing the birkhoff poly-
tope for variational permutation inference. In Storkey, A.,
and Perez-Cruz, F., eds., Proceedings of the Twenty-First In-
ternational Conference on Artiﬁcial Intelligence and Statis-
tics, volume 84 of Proceedings of Machine Learning Re-
search, 1618–1627. Playa Blanca, Lanzarote, Canary Is-
lands: PMLR.
Luce, R. D. 2005. Individual Choice Behavior: A Theoreti-
cal Analysis. Courier Corporation.
Maddison, C. J.; Mnih, A.; and Teh, Y. W. 2016. The con-
crete distribution: A continuous relaxation of discrete ran-
dom variables. arXiv preprint arXiv:1611.00712.
Maddison, C. J.; Tarlow, D.; and Minka, T. 2014. A* sam-
pling. In Advances in Neural Information Processing Sys-
tems, 3086–3094.
Mena, G.; Belanger, D.; Linderman, S.; and Snoek, J. 2018.
Learning latent permutations with gumbel-sinkhorn net-
works. In International Conference on Learning Represen-
tations.
Mnih, A., and Gregor, K. 2014. Neural variational in-
arXiv preprint
ference and learning in belief networks.
arXiv:1402.0030.
Mohamed, S.; Rosca, M.; Figurnov, M.; and Mnih, A. 2019.
Monte carlo gradient estimation in machine learning. arXiv
preprint arXiv:1906.10652.
Neal, R. M., and Hinton, G. E. 1998. A view of the em al-
gorithm that justiﬁes incremental, sparse, and other variants.
In Learning in graphical models. Springer. 355–368.
Nemirovski, A. 1999. Optimization ii: Standard numeri-
cal methods for nonlinear continuous optimization. Lecture
notes.

Nesterov, Y. 2013. Gradient methods for minimizing com-
posite functions. Mathematical Programming 140(1):125–
161.
Paszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.;
DeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer,
In NIPS
A. 2017. Automatic differentiation in PyTorch.
Autodiff Workshop.
Peters, J., and B¨uhlman, P. 2014. Identiﬁability of Gaus-
sian structural equation models with equal error variances.
Biometrika 101(1):219–228.
Peters, J., and B¨uhlmann, P. 2013. Structural intervention
distance (sid) for evaluating causal graphs. arXiv preprint
arXiv:1306.1043.
Plackett, R. L. 1975. The analysis of permutations. Journal
of the Royal Statistical Society: Series C (Applied Statistics)
24(2):193–202.
Rezende, D. J.; Mohamed, S.; and Wierstra, D.
2014.
Stochastic backpropagation and approximate inference in
deep generative models. arXiv preprint arXiv:1401.4082.
Sethuraman, J. 1994. A constructive deﬁnition of dirichlet
priors. Statistica sinica 639–650.
Silander, T.; Lepp-aho, J.; Jsaari, E.; and Roos, T. 2018.
Quotient normalized maximum likelihood criterion for
In Storkey, A., and
learning bayesian network structures.
Perez-Cruz, F., eds., Proceedings of the Twenty-First Inter-
national Conference on Artiﬁcial Intelligence and Statistics,
volume 84 of Proceedings of Machine Learning Research,
948–957. Playa Blanca, Lanzarote, Canary Islands: PMLR.
Solus, L.; Wang, Y.; Matejovicova, L.; and Uhler, C. 2017.
Consistency guarantees for permutation-based causal infer-
ence algorithms.
Staines, J., and Barber, D. 2012. Variational optimization.
arXiv preprint arXiv:1212.4507.
Tucker, G.; Mnih, A.; Maddison, C. J.; Lawson, J.; and Sohl-
Dickstein, J. 2017. Rebar: Low-variance, unbiased gradient
estimates for discrete latent variable models. In Guyon, I.;
Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vish-
wanathan, S.; and Garnett, R., eds., Advances in Neural In-
formation Processing Systems 30. Curran Associates, Inc.
2627–2636.
Williams, R. J. 1992. Simple statistical gradient-following
algorithms for connectionist reinforcement learning. Ma-
chine learning 8(3-4):229–256.
Yellott Jr, J. I. 1977. The relationship between luce’s choice
axiom, thurstone’s theory of comparative judgment, and the
double exponential distribution. Journal of Mathematical
Psychology 15(2):109–144.
Yin, M., and Zhou, M. 2018. Arm: Augment-reinforce-
merge gradient for discrete latent variable models. arXiv
preprint arXiv:1807.11143.

