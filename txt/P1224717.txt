Parameter sharing between dependency parsers for related languages

Miryam de Lhoneux1∗ Johannes Bjerva2
1Department of Linguistics and Philology
Uppsala University
Uppsala, Sweden

Isabelle Augenstein2 Anders Søgaard2
2 Department of Computer Science
University of Copenhagen
Copenhagen, Denmark

8
1
0
2
 
t
c
O
 
4
 
 
]
L
C
.
s
c
[
 
 
2
v
5
5
0
9
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

Previous work has suggested that parameter
sharing between transition-based neural de-
pendency parsers for related languages can
lead to better performance, but there is no
consensus on what parameters to share. We
present an evaluation of 27 different parame-
ter sharing strategies across 10 languages, rep-
resenting ﬁve pairs of related languages, each
pair from a different language family. We ﬁnd
that sharing transition classiﬁer parameters al-
ways helps, whereas the usefulness of shar-
ing word and/or character LSTM parameters
varies. Based on this result, we propose an
architecture where the transition classiﬁer is
shared, and the sharing of word and charac-
ter parameters is controlled by a parameter that
can be tuned on validation data. This model is
linguistically motivated and obtains signiﬁcant
improvements over a mono-lingually trained
baseline. We also ﬁnd that sharing transi-
tion classiﬁer parameters helps when training
a parser on unrelated language pairs, but we
ﬁnd that, in the case of unrelated languages,
sharing too many parameters does not help.

1 Introduction

The idea of sharing parameters between parsers
of related languages goes back to early work
in cross-lingual adaptation (Zeman and Resnik,
2008), and the idea has recently received a lot
of interest in the context of neural dependency
parsers (Duong et al., 2015; Ammar et al., 2016;
Susanto and Lu, 2017). Modern neural depen-
dency parsers, however, use different sets of pa-
rameters for representation and scoring, and it is
not clear what parameters it is best to share.

The Universal Dependencies

(UD) project
(Nivre et al., 2016), which is seeking to harmonize
the annotation of dependency treebanks across
∗ Work carried out during a stay at the University of

Copenhagen.

languages, has seen a steady increase in languages
that have a treebank in a common standard. Many
of these languages are low resource and have small
UD treebanks.
It seems interesting to ﬁnd out
ways to leverage the wealth of information con-
tained in these treebanks, especially for low re-
source languages.

In this paper, we evaluate 27 different pa-
rameter sharing strategies. We focus on a par-
ticular transition-based neural dependency parser
(de Lhoneux et al., 2017a,b), which performs
close to the state of the art. This parser has
three sets of parameters:
i) the parameters of a
character-based one-layer, bidirectional LSTM; ii)
the parameters of a word-based two-layer, bidirec-
tional LSTM; iii) and the parameters of a multi-
layered perceptron (MLP) with a single hidden
layer. The two ﬁrst sets are for learning to repre-
sent conﬁgurations; the third for selecting the next
transition. We consider all combinations of shar-
ing these sets of parameters; and in addition, we
consider two ways of sharing each set of parame-
ters, namely with and without a preﬁxed language
embedding. The latter enables partial, soft shar-
ing. In sum, we consider all 33 combinations of no
sharing, hard sharing and soft sharing of the three
sets of parameters. We evaluate the 27 multilin-
gual parsers on 10 languages from the UD project,
representing ﬁve pairs of related languages, each
pair from a different language family. We repeat
the experiment with the same set of languages, but
using pairs of unrelated languages.

Contributions This paper is, to the best of our
knowledge, the ﬁrst to evaluate different parame-
ter sharing strategies for exploiting synergies be-
tween neural dependency parsers of related lan-
guages. We evaluate the different strategies on
10 languages, representing ﬁve different language
families. We ﬁnd that sharing (MLP) transition

Lang Tokens Family

Word order

ar
he
et
ﬁ
hr
ru
it
es
nl
no

Semitic
Semitic
Finnic
Finnic
Slavic
Slavic

208,932
161,685
60,393
67,258
109,965
90,170
113,825 Romance
154,844 Romance
75,796 Germanic No dom. order
76,622 Germanic SVO

VSO
SVO
SVO
SVO
SVO
SVO
SVO
SVO

Table 1: Dataset characteristics

classiﬁer parameters always helps, whereas the
usefulness of sharing LSTM parameters depends
on the language pair. This reﬂects the intuition that
the transition classiﬁer learns hierarchical struc-
tures that are likely to transfer across languages,
based on parser conﬁgurations that abstract away
from several linguistic differences. The similarity
of the input to character- and word-level LSTMs,
on the other hand, will vary depending on the
phonological and morphosyntactic similarity of
the languages in question. Motivated by this ob-
servation, we propose an architecture with hard-
wired transition classiﬁer parameter sharing, but in
which sharing of LSTM parameters is tuned. The
novel architecture signiﬁcantly outperforms our
monolingual baseline on our set of 10 languages.
We additionally investigate parameter sharing of
unrelated languages.

2 The Uppsala dependency parser

The Uppsala parser (de Lhoneux et al., 2017a,b)
consists of three sets of parameters; the param-
eters of the character-based LSTM, those of the
word-based LSTM, and the parameters of the
MLP that predicts transitions. The character-based
LSTM produces representations for the word-
based LSTM, which produces representations for
the MLP. The Uppsala parser is a transition-based
parser (Kiperwasser and Goldberg, 2016), adapted
to the Universal Dependencies (UD) scheme,1
and using the arc-hybrid transition system from
Kuhlmann et al. (2011) extended with a SWAP
transition and a static-dynamic oracle, as de-
scribed in de Lhoneux et al. (2017b). The SWAP

1http://universaldependencies.org/

transition is used to generate non-projective de-
pendency trees (Nivre, 2009).

For an input sentence of length n with words
w1, . . . , wn, the parser creates a sequence of vec-
tors x1:n, where the vector xi representing wi is
the concatenation of a word embedding and the ﬁ-
nal state of the character-based LSTM after pro-
cessing the characters of wi. The character vector
ch(wi) is obtained by running a (bi-directional)
LSTM over the characters chj (1 ≤ j ≤ m)
of wi. Each input element is represented by the
word-level, bi-directional LSTM, as a vector vi =
BILSTM(x1:n, i). For each conﬁguration, the fea-
ture extractor concatenates the LSTM representa-
tions of core elements from the stack and buffer.
Both the embeddings and the LSTMs are trained
together with the model.

A conﬁguration c is represented by a feature
function φ(·) over a subset of its elements. For
transitions are scored by a
each conﬁguration,
classiﬁer, in this case an MLP, and φ(·) is a con-
catenation of BiLSTM vectors on top of the stack
and the beginning of the buffer. The MLP scores
transitions together with the arc labels for transi-
tions that involve adding an arc. In practice, we
use two interpolated MLPs, one which only scores
the transitions, and one which scores transitions
together with the arc label. For simplicity, we re-
fer to that interpolated MLP as the MLP.

3 Parameter sharing

Since our parser has three basic sets of model pa-
rameters, we consider sharing all combinations of
those three sets. We also introduce two ways of
sharing, namely, with or without the addition of a
vector representing the language. This language
embedding enables the model, in theory, to learn
what to share between the two languages in ques-
tion. Since for all three model parameter sets, we
now have three options – not sharing, sharing, or
sharing in the context of a language embedding –
we are left with 33 = 27 parameter sharing strate-
gies; see Table 2.

In the setting where we do not share (✗) word
parameters (W), we construct a different word
lookup table and a different word-level BiLSTM
for each language.
In the setting where we do
hard parameter sharing (D) of word parameters,
we only construct one lookup table and one word
BiLSTM for the languages involved.
In the set-
ting where we do soft sharing (ID) of word pa-

Model C W S

ar

he

es

it

et

ﬁ

nl

no

hr

ru

AV

MONO

76.3

80.2

83.7

83.3

70.4

70.8

77.3

80.8

76.8

82.3

78.2

LANGUAGE-BEST

76.6

80.6

84.4

84.8

72.8

72.9

79.6

82.1

78.0

82.9

79.5

BEST

✗ D ID 76.3

80.3

84.2

84.5

72.1

72.5

78.8

81.4

77.6

82.8

79.1

CHAR D ✗
WORD
STATE

76.4
76.3
✗ D 76.6

✗
✗ D ✗
✗
...
D D D 76.2
ID ID ID 76.3

ALL
SOFT

80.3
79.9
80.3

84.3
83.9
84.0

84.0
84.4
83.7

72.3
72.4
71.5

71.0
71.3
72.9

78.3
77.4
78.3

81.3
80.7
81.5

77.0
76.9
77.4

82.3
82.5
82.8

78.7
78.6
78.9

80.1
79.9

84.0
84.1

84.2
84.4

72.1
72.1

71.4
71.3

78.7
79.6

81.1
81.4

77.0
77.1

82.5
82.5

78.7
78.9

Table 2: Performance on development data (LAS; in %) across select sharing strategies. MONO is our single-task baseline;
LANGUAGE-BEST is using the best sharing strategy for each language (as evaluated on development data); BEST is the overall
best sharing strategy, across languages; CHAR shares only the character-based LSTM parameters; WORD shares only the word-
based LSTM parameters; ALL shares all parameters. D refers to hard sharing, ID refers to soft sharing, using an embedding
of the language ID and ✗ refers to not sharing.

rameters, we share those parameters, and in ad-
dition, concatenate a language embedding li rep-
resenting the language of word wi to the vector
of the word wi at the input of the word BiLSTM:
xi = e(wi) ◦ ch(wi) ◦ li. Similarly for character
parameters (C), we construct a different character
BiLSTM and one character lookup for each lan-
guage (✗), create those for all languages and share
them (D) or share them and concatenate a (ran-
domly initialized) language embedding li repre-
senting the language of word wi at the input of the
character BiLSTM (ID): chj = e(chj ) ◦ li. At the
level of conﬁguration or parser states (S), we either
construct a different MLP for each language (✗),
share the MLP (D) or share it and concatenate a
language embedding li representing the language
of word wi to the vector representing the conﬁgu-
ration, at the input of the MLP (ID): c = φ(·) ◦ li.

4 Experiments

Language pairs We use 10 languages in our ex-
periments, representing ﬁve language pairs from
different language families. Our two SEMITIC lan-
guages are Arabic and Hebrew. These two lan-
guages differ in that Arabic tends to favour VSO
word order whereas Hebrew tends to use SVO, but
are similar in their rich transﬁxing morphology.
Our two FINNO-UGRIC languages are Estonian
and Finnish. These two languages differ in that
Estonian no longer has vowel harmony, but share
a rich agglutinative morphology. Our two SLAVIC
languages are Croatian and Russian. These two

languages differ in that Croatian uses gender in
plural nouns, but otherwise share their rich inﬂec-
tional morphology. Our two ROMANCE languages
are Italian and Spanish. These two languages dif-
fer in that Italian uses a possessive adjective with
a deﬁnite article, but share a fairly strict SVO or-
der. Finally, our two GERMANIC languages are
Dutch and Norwegian. These two languages dif-
fer in morphological complexity, but share word
ordering features to some extent.

Datasets For all 10 languages, we use treebanks
from the Universal Dependencies project. The
dataset characteristics are listed in Table 1. To
keep the results comparable across language pairs,
we down-sample the training set to the size of the
smallest of our languages, Hebrew: we randomly
sample 5000 sentences for each training set. Note
that while this setting makes the experiment some-
what artiﬁcial and will probably overestimate the
beneﬁts that can be obtained from sharing param-
eters when using larger treebanks, we ﬁnd it in-
teresting to see how much low resource languages
can beneﬁt from parameter sharing, as explained
in the introduction.

Baselines and systems This is an evaluation pa-
per, and our results are intended to explore a space
of sharing strategies to ﬁnd better ways of shar-
ing parameters between dependency parsers of
related languages. Our baseline is the Uppsala
parser trained monolingually. Our systems are
parsers trained bilingually by language pair where

we share subsets of parameters between the lan-
guages in the pair, and we report on what sharing
strategies seem superior across the 10 languages
that we consider.

Implementation details A ﬂexible implementa-
tion of parameter strategies for the Uppsala parser
was implemented in Dynet.2 We make the code
publicly available.3

5 Results and discussion

Our results on development sets are presented in
Table 2. We use labeled attachment score (LAS)
as our metric for evaluating parsers. Table 2
presents numbers for a select subset of the 27 shar-
ing strategies. The other results can be found in the
supplementary material. Our main observations
are: (i) that, generally, and as observed in previous
work, multi-task learning helps: all different shar-
ing strategies are on average better than the mono-
lingual baselines, with minor (0.16 LAS points) to
major (0.86 LAS points) average improvements;
and (ii) that sharing the MLP seems to be overall
the 10 best
a better strategy than not sharing it:
strategies share the MLP. Whereas the usefulness
of sharing the MLP seems to be quite robust across
language pairs, the usefulness of sharing word and
character parameters seems more dependent on
the language pairs. This reﬂects the linguistic in-
tuition that character- and word-level LSTMs are
highly sensitive to phonological and morphosyn-
tactic differences such as word order, whereas the
MLP learns to predict less idiosyncratic, hierar-
chical relations from relatively abstract represen-
tations of parser conﬁgurations.

Based on this result, we propose a model
(OURS) where the MLP is shared and the sharing
of word and character parameters is controlled by
a parameter that can be set on validation data.
Results are given in Table 3. We obtain a 0.6 LAS
improvement on average and our proposed model
is signiﬁcantly better than the monolingual base-
line with p < 0.01. Signiﬁcance testing is per-
formed using a randomization test, with the script
from the CoNLL 2017 Shared Task.4

6 Unrelated languages

We repeated the same set of experiments with un-
related language pairs. We hypothesise that pa-

W C

OURS MONO

δ

✗

✗

ar
77.2
ID D 84.3
es
✗
et
71.4
ID
✗
✗
71.6
ﬁ
he D ✗
80.0
hr D ✗
77.9
ID D 85.0
it
ID D 75.5
nl
✗
no
81.1
ID
ru D ✗
83.5

av.

78.8

77.1
83.8
70.5
71.6
79.8
78.0
84.0
74.1
80.1
82.7

78.2

0.1
0.5
0.8
0.1
0.3
-0.1
1.0
1.4
1.0
0.8

0.6

Table 3: LAS on the test sets of the best of 9 sharing strate-
gies and the monolingual baseline. δ is the difference be-
tween OURS AND MONO.

rameter sharing between unrelated language pairs
will be less useful in general than with related lan-
guage pairs. However, it can still be useful, it has
been shown previously that unrelated languages
can beneﬁt from being trained jointly. For exam-
ple, Lynn et al. (2014) have shown that Indonesian
was surprisingly particularly useful for Irish.

The results are presented in Table 4. The ta-
ble only presents part of the results, the rest can
be found in the supplementary material. As ex-
pected, there is much less to be gained from shar-
ing parameters between unrelated pairs. However,
it is possible to improve the monolingual baseline
by sharing some of the parameters.
In general,
sharing the MLP is still a helpful thing to do. It
is most helpful to share the MLP and optionally
one of the two other sets of parameters. Results are
close to the monolingual baseline when everything
is shared. Sharing word and character parameters
but not the MLP hurts accuracy compared to the
monolingual baseline.

7 Related work

Previous work has shown that sharing parame-
ters between dependency parsers for related lan-
guages can lead to improvements (Duong et al.,
2015; Ammar et al., 2016; Susanto and Lu, 2017).
Smith et al. (2018) recently found that sharing pa-
rameters using the same parser as in this paper
(soft sharing of word parameters, hard sharing of
the rest) improves parsing accuracy when train-
ing on related languages, and is especially use-
ful in the low resource case. Similar effects have

2https://github.com/clab/dynet
3https://github.com/coastalcph/uuparser
4https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py

C W S

Model
MONO

he
80.2
80.5
✗ D 80.3
79.8
80.1
79.6
D D D 80.5
ID ID ID 79.8

LANGUAGE-BEST
✗
ID ID ✗
✗
✗ D ✗

BEST
WORST
CHAR D ✗
WORD
ALL
SOFT

no
80.8
81.5
81.5
80.6
80.9
80.9
80.9
80.5

ﬁ
70.8
71.9
71.9
69.2
71.4
71.9
69.8
70.1

hr
76.8
77.6
77.6
76.7
76.8
76.9
76.6
76.6

ru
82.3
82.9
82.7
81.4
82.9
82.2
82.3
82.1

es
83.7
84.0
84.0
83.8
83.9
83.7
83.7
83.9

it
83.3
84.3
83.8
83.2
84.3
83.8
84.0
83.8

et
70.4
72.5
72.5
69.4
70.9
70.9
70.6
70.6

nl
77.3
78.7
78.7
76.6
78.0
77.0
77.4
77.2

ar
76.3
76.5
76.3
76.0
76.5
76.4
76.2
76.3

AV
78.2
78.9
78.9
77.7
78.6
78.3
78.2
78.1

Table 4: Performance on development data (LAS; in %) across select sharing strategies for unrelated languages. MONO is our
single-task baseline; LANGUAGE-BEST is using the best sharing strategy for each language (as evaluated on development data);
BEST and WORST are the overall best and worst sharing strategy across languages; CHAR shares only the character-based
LSTM parameters; WORD shares only the word-based LSTM parameters; ALL shares all parameters. D refers to hard sharing,
ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not sharing.

been observed in machine translation (Dong et al.,
2015; Johnson et al., 2017), for example. Most
studies have only explored a small number of pa-
rameter sharing strategies, however. Vilares et al.
(2016) evaluate parsing with hard parameter shar-
ing for 100 language pairs with a statistical parser.
Naseem et al. (2012) proposed to selectively share
subsets of a parser across languages in the context
of a probabilistic parser.

Options we do not explore here are learning
the architecture jointly with optimizing the task
objective (Misra et al., 2016; Ruder et al., 2017),
or learning an architecture search model
that
predicts an architecture based on the properties
typically with reinforcement learn-
of datasets,
ing (Zoph and Le, 2017; Wong and Gesmundo,
2018; Liang et al., 2018). We also do not ex-
plore the option of sharing selectively based on
more ﬁne-grained typological information about
languages, which related work has indicated could
be useful (Bjerva and Augenstein, 2018). Rather,
we stick to sharing between languages of the same
language families.

The strategies explored here do not exhaust the
space of possible parameter sharing strategies. For
example, we completely ignore soft sharing based
on mean-constrained regularisation (Duong et al.,
2015).

8 Conclusions

made several observations: (a) Generally, multi-
task learning helps. (b) Sharing the MLP param-
eters always helps. It helps to share MLP param-
eters when training a parser on a pair of related
languages, and it also helps if the languages are
unrelated. (c) Sharing word and character param-
eters is differently helpful depending on the lan-
guage. (d) Sharing too many parameters does not
help, when the languages are unrelated.

In future work, we plan to investigate what hap-
pens when training on more than 2 languages.
Here, we focused on a setting with rather small
amounts of balanced data.
It would be interest-
ing to experiment with using datasets that are not
balanced with respect to size. Finally, we have re-
stricted our experiments to a speciﬁc architecture,
using ﬁxed hyperparameters including word and
character embedding dimensions. It would be in-
teresting to experiment with different parsing ar-
chitectures as well as varying those hyperparame-
ters.

Acknowledgments

We acknowledge the computational resources pro-
vided by CSC in Helsinki and Sigma2 in Oslo
through NeIC-NLPL (www.nlpl.eu). The second
author is partially funded by an AdeptMind schol-
arship. The last author was funded by an ERC
Starting Grant.

We present evaluations of 27 parameter sharing
strategies for the Uppsala parser across 10 lan-
guages, representing ﬁve language pairs from ﬁve
different language families. We repeated the ex-
periment with pairs of unrelated languages. We

References

Waleed Ammar, George Mulcaire, Miguel Ballesteros,
Chris Dyer, and Noah A. Smith. 2016. More lan-
guages, one parser. In TACL.

In Proceedings of the 50th Annual Meet-
parsing.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 629–637. Associa-
tion for Computational Linguistics.

Joakim Nivre. 2009. Non-Projective Dependency Pars-
In Proceedings of

ing in Expected Linear Time.
ACL, pages 351–359, Suntec, Singapore.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, et al. 2016. Universal dependencies
v1: A multilingual treebank collection. In Proceed-
ings of the 10th International Conference on Lan-
guage Resources and Evaluation (LREC 2016).

Sebastian Ruder, Joachim Bingel, Isabelle Augenstein,
and Anders Søgaard. 2017. Sluice networks: Learn-
ing what to share between loosely related tasks. In
CoRR, abs/1705.08142.

Aaron Smith, Bernd Bohnet, Miryam de Lhoneux,
Joakim Nivre, Yan Shao, and Sara Stymne. 2018. 82
Treebanks, 34 Models: Universal Dependency Pars-
ing with Multi-Treebank Models. In Proceedings of
the CoNLL 2018 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies.

Raymond Hendy Susanto and Wei Lu. 2017. Neural
architectures for multilingual semantic parsing. In
ACL.

and
David Vilares, Carlos G´omez-Rodr´ıguez,
two lan-
Miguel A Alonso. 2016. One model,
guages: training bilingual parsers with harmonized
In Proceedings of
the 54th Annual
treebanks.
Meeting of
the Association for Computational
Linguistics (Volume 2: Short Papers), volume 2,
pages 425–431.

Catherine Wong and Andrea Gesmundo. 2018. Trans-
fer Learning to Learn with Multitask Neural Model
Search. In ICPR.

Daniel Zeman and Philip Resnik. 2008.

Cross-
Language Parser Adaptation between Related Lan-
guages. In IJCNLP.

Barret Zoph and Quoc Le. 2017. Neural Architecture
Search with Reinforcement Learning. In ICPR.

Johannes Bjerva and Isabelle Augenstein. 2018. From
Phonology to Syntax: Unsupervised Linguistic Ty-
pology at Different Levels with Language Embed-
In Proceedings of the 2018 Conference of
dings.
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers), pages 907–916.
Association for Computational Linguistics.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
tiple language translation. In ACL.

Long Duong, Trevor Cohn, Steven Bird, and Paul
Cook. 2015. Low Resource Dependency Parsing:
Cross-lingual Parameter Sharing in a Neural Net-
work Parser. In Proceedings of ACL.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Vigas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google’s
neural machine translation system. In TACL.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and Accurate Dependency Parsing Using Bidi-
rectional LSTM Feature Representations. TACL,
4:313–327.

Marco Kuhlmann, Carlos G´omez-Rodr´ıguez, and Gior-
gio Satta. 2011. Dynamic Programming Algorithms
for Transition-Based Dependency Parsers. In Pro-
ceedings of ACL, pages 673–682, Portland, Oregon,
USA.

Miryam de Lhoneux, Yan Shao, Ali Basirat, Eliyahu
Kiperwasser, Sara Stymne, Yoav Goldberg, and
Joakim Nivre. 2017a. From raw text to universal
In Proceedings of
dependencies - look, no tags!
the CoNLL 2017 Shared Task: Multilingual Pars-
ing from Raw Text to Universal Dependencies, pages
207–217, Vancouver, Canada.

Miryam de Lhoneux, Sara Stymne, and Joakim Nivre.
2017b. Arc-Hybrid Non-Projective Dependency
Parsing with a Static-Dynamic Oracle. In Proceed-
ings of the 15th International Conference on Parsing
Technologies, pages 99–104, Pisa, Italy.

Jason Liang, Elliot Meyerson, and Risto Miikkulainen.
2018. Evolutionary Architecture Search For Deep
Multitask Networks. In GECCO.

Teresa Lynn, Jennifer Foster, Mark Dras, and Lamia
Tounsi. 2014. Cross-lingual transfer parsing for
low-resourced languages: An irish case study.
In
Proceedings of the First Celtic Language Technol-
ogy Workshop, pages 41–49.

Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and
Martial Hebert. 2016. Cross-Stitch Networks for
Multi-Task Learning. In Proceedings of CVPR.

Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency

C
MONO
✗
✗
✗
✗
✗

D

ID
✗
ID

D

ID
ID
✗

D

D

D

ID

D

D

✗

ID
✗

ID

D

D

ID
ID

W S

ar
76.3
D ID 76.3
D D 76.4
ID ID 76.2
ID D 76.4
✗
ID 76.5
ID D 76.2
✗ D 76.3
✗ D 76.6
ID ID 76.3
ID ID 76.2
✗
✗
76.6
✗
ID 76.2
✗
✗
76.6
✗ D 76.1
✗
ID 76.5
D ID 76.6
ID D 76.1
✗
✗
76.4
D D 76.2
ID ✗
76.6
D D 76.2
D ✗
76.3
D ID 76.0
D ✗
76.1
ID ✗
76.0
D ✗
76.1
ID ✗
75.9

he
80.2
80.3
80.4
80.1
80.4
80.6
79.8
80.2
80.3
79.9
80.1
80.3
79.9
80.3
80.1
80.0
80.1
80.1
80.3
80.1
80.1
79.9
79.9
80.0
79.7
79.3
79.9
79.5

es
83.7
84.2
84.1
84.2
84.2
84.1
84.4
84.0
84.0
84.1
84.3
84.1
84.1
84.0
84.0
84.0
84.0
84.1
84.3
84.0
84.1
83.8
83.9
83.8
83.8
84.1
83.9
84.1

it
83.3
84.5
84.4
84.8
84.3
84.3
84.7
84.4
83.7
84.4
84.5
84.3
84.3
84.2
84.1
84.4
84.6
84.6
84.0
84.2
84.3
84.5
84.4
84.3
84.5
84.4
84.4
84.4

et
70.4
72.1
71.9
71.8
72.2
71.8
72.4
72.8
71.5
72.1
72.5
71.8
71.9
71.2
72.1
71.9
72.1
72.1
72.3
72.1
71.7
72.4
72.4
71.7
71.9
71.5
71.1
72.1

ﬁ
70.8
72.5
72.0
72.4
72.3
72.0
71.5
71.7
72.9
71.3
71.8
71.7
72.0
72.1
72.0
71.7
71.0
71.2
71.0
71.4
71.6
70.3
71.3
70.9
70.4
71.3
70.6
70.5

nl
77.3
78.8
78.7
78.6
78.3
78.5
79.2
78.6
78.3
79.6
78.7
78.3
78.5
78.0
78.2
78.5
78.3
78.0
78.3
78.7
77.6
78.1
77.4
78.3
77.8
77.7
77.8
77.4

no
80.8
81.4
81.5
81.6
81.6
81.5
81.6
82.1
81.5
81.4
81.1
81.5
81.7
81.6
81.8
81.7
81.0
81.4
81.3
81.1
81.0
81.0
80.7
81.0
81.1
80.6
80.9
80.6

hr
76.8
77.6
78.0
77.4
77.4
77.7
76.9
77.2
77.4
77.1
76.7
77.2
77.2
77.3
76.8
76.7
76.9
77.0
77.0
77.0
77.0
77.2
76.9
76.9
76.5
76.7
77.0
77.0

ru
82.3
82.8
82.8
82.9
82.8
82.9
82.8
82.3
82.8
82.5
82.6
82.5
82.3
82.6
82.6
82.3
82.8
82.7
82.3
82.5
82.6
82.6
82.5
82.5
82.3
82.5
82.0
82.2

average
78.2
79.1
79.0
79.0
79.0
79.0
78.9
78.9
78.9
78.9
78.8
78.8
78.8
78.8
78.8
78.8
78.7
78.7
78.7
78.7
78.7
78.6
78.6
78.5
78.4
78.4
78.4
78.4

Table 5: Performance (LAS; in %) across select sharing strategies ranked by average performance. MONO is our single-task
baseline; W refers to sharing the word parameters, C refers to sharing the character parameters, S refers to sharing the MLP
parameters. D refers to hard sharing, ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not
sharing.

C
MONO
✗
✗
✗
✗
ID
ID

D

ID
✗
✗

D

✗

D

✗
✗

ID
ID

D

D

D

ID
ID

D

D

D

ID
ID

W S

he
80.2
✗ D 80.3
✗
✗
80.3
ID ID 80.3
✗
ID 79.7
✗
ID 80.3
✗
✗
80.0
✗
ID 80.3
✗ D 80.4
D ID 80.5
ID D 80.6
✗ D 80.3
D D 80.6
✗
✗
80.1
ID ✗
80.3
D ✗
79.6
D ID 80.3
D D 80.1
D D 80.5
ID ID 80.3
D ID 79.8
ID D 80.0
ID ID 79.8
ID D 80.3
D ✗
80.4
ID ✗
79.6
D ✗
79.6
ID ✗
79.8

no
80.8
81.5
81.7
81.1
81.5
81.6
81.6
81.5
81.4
81.2
81.1
81.1
81.1
80.9
81.3
80.9
81.0
80.8
80.9
80.7
80.9
80.8
80.5
81.1
80.3
80.6
80.1
80.6

ﬁ
70.8
71.9
71.9
72.1
72.2
71.8
71.5
71.5
71.6
72.2
71.9
71.3
71.6
71.4
71.2
71.9
70.5
70.4
69.8
70.2
71.0
69.8
70.1
70.2
70.1
69.4
69.9
69.2

hr
76.8
77.6
77.7
77.7
77.1
77.5
77.2
77.5
77.3
77.0
77.1
77.0
76.8
76.8
77.4
76.9
76.5
77.0
76.6
76.1
76.2
76.2
76.6
76.1
76.6
76.7
76.6
76.7

ru
82.3
82.7
82.5
82.7
82.7
82.6
82.7
82.7
82.6
82.5
82.7
82.6
82.5
82.9
81.9
82.2
82.3
82.2
82.3
82.1
82.1
82.2
82.1
82.2
82.0
81.7
81.8
81.4

es
83.7
84.0
84.0
84.2
83.8
84.1
83.9
83.7
83.9
84.0
84.2
84.0
83.7
83.9
84.2
83.7
83.7
83.8
83.7
83.8
83.7
83.8
83.9
84.1
83.7
83.8
83.5
83.8

it
83.3
83.8
84.2
84.2
84.0
83.8
84.0
83.9
84.1
83.8
84.0
84.2
83.9
84.3
83.9
83.8
83.6
83.8
84.0
83.8
83.6
84.3
83.8
83.7
83.2
83.4
82.9
83.2

et
70.4
72.5
71.8
72.4
72.3
71.8
71.1
71.6
71.9
71.5
71.9
71.8
71.4
70.9
70.7
70.9
71.4
71.0
70.6
70.8
70.9
70.7
70.6
70.3
69.3
69.2
69.2
69.4

nl
77.3
78.7
78.5
77.9
78.6
78.3
79.0
77.9
77.7
78.3
77.1
77.8
78.1
78.0
77.6
77.0
77.8
77.6
77.4
77.6
77.3
77.2
77.2
76.9
76.7
77.6
77.6
76.6

ar
76.3
76.4
76.5
76.0
76.5
76.2
76.4
76.7
76.4
76.1
76.2
76.3
76.3
76.5
75.8
76.4
76.1
76.2
76.2
76.2
76.0
76.2
76.3
76.0
76.2
76.2
76.3
76.0

average
78.2
78.9
78.9
78.9
78.8
78.8
78.7
78.7
78.7
78.7
78.7
78.6
78.6
78.6
78.4
78.3
78.3
78.3
78.2
78.2
78.2
78.1
78.1
78.1
77.8
77.8
77.7
77.7

Table 6: Unrelated language pairs. Performance (LAS; in %) across select sharing strategies ranked by average performance.
MONO is our single-task baseline; W refers to sharing the word parameters, C refers to sharing the character parameters, S
refers to sharing the MLP parameters. D refers to hard sharing, ID refers to soft sharing, using an embedding of the language
ID and ✗ refers to not sharing.

Parameter sharing between dependency parsers for related languages

Miryam de Lhoneux1∗ Johannes Bjerva2
1Department of Linguistics and Philology
Uppsala University
Uppsala, Sweden

Isabelle Augenstein2 Anders Søgaard2
2 Department of Computer Science
University of Copenhagen
Copenhagen, Denmark

8
1
0
2
 
t
c
O
 
4
 
 
]
L
C
.
s
c
[
 
 
2
v
5
5
0
9
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

Previous work has suggested that parameter
sharing between transition-based neural de-
pendency parsers for related languages can
lead to better performance, but there is no
consensus on what parameters to share. We
present an evaluation of 27 different parame-
ter sharing strategies across 10 languages, rep-
resenting ﬁve pairs of related languages, each
pair from a different language family. We ﬁnd
that sharing transition classiﬁer parameters al-
ways helps, whereas the usefulness of shar-
ing word and/or character LSTM parameters
varies. Based on this result, we propose an
architecture where the transition classiﬁer is
shared, and the sharing of word and charac-
ter parameters is controlled by a parameter that
can be tuned on validation data. This model is
linguistically motivated and obtains signiﬁcant
improvements over a mono-lingually trained
baseline. We also ﬁnd that sharing transi-
tion classiﬁer parameters helps when training
a parser on unrelated language pairs, but we
ﬁnd that, in the case of unrelated languages,
sharing too many parameters does not help.

1 Introduction

The idea of sharing parameters between parsers
of related languages goes back to early work
in cross-lingual adaptation (Zeman and Resnik,
2008), and the idea has recently received a lot
of interest in the context of neural dependency
parsers (Duong et al., 2015; Ammar et al., 2016;
Susanto and Lu, 2017). Modern neural depen-
dency parsers, however, use different sets of pa-
rameters for representation and scoring, and it is
not clear what parameters it is best to share.

The Universal Dependencies

(UD) project
(Nivre et al., 2016), which is seeking to harmonize
the annotation of dependency treebanks across
∗ Work carried out during a stay at the University of

Copenhagen.

languages, has seen a steady increase in languages
that have a treebank in a common standard. Many
of these languages are low resource and have small
UD treebanks.
It seems interesting to ﬁnd out
ways to leverage the wealth of information con-
tained in these treebanks, especially for low re-
source languages.

In this paper, we evaluate 27 different pa-
rameter sharing strategies. We focus on a par-
ticular transition-based neural dependency parser
(de Lhoneux et al., 2017a,b), which performs
close to the state of the art. This parser has
three sets of parameters:
i) the parameters of a
character-based one-layer, bidirectional LSTM; ii)
the parameters of a word-based two-layer, bidirec-
tional LSTM; iii) and the parameters of a multi-
layered perceptron (MLP) with a single hidden
layer. The two ﬁrst sets are for learning to repre-
sent conﬁgurations; the third for selecting the next
transition. We consider all combinations of shar-
ing these sets of parameters; and in addition, we
consider two ways of sharing each set of parame-
ters, namely with and without a preﬁxed language
embedding. The latter enables partial, soft shar-
ing. In sum, we consider all 33 combinations of no
sharing, hard sharing and soft sharing of the three
sets of parameters. We evaluate the 27 multilin-
gual parsers on 10 languages from the UD project,
representing ﬁve pairs of related languages, each
pair from a different language family. We repeat
the experiment with the same set of languages, but
using pairs of unrelated languages.

Contributions This paper is, to the best of our
knowledge, the ﬁrst to evaluate different parame-
ter sharing strategies for exploiting synergies be-
tween neural dependency parsers of related lan-
guages. We evaluate the different strategies on
10 languages, representing ﬁve different language
families. We ﬁnd that sharing (MLP) transition

Lang Tokens Family

Word order

ar
he
et
ﬁ
hr
ru
it
es
nl
no

Semitic
Semitic
Finnic
Finnic
Slavic
Slavic

208,932
161,685
60,393
67,258
109,965
90,170
113,825 Romance
154,844 Romance
75,796 Germanic No dom. order
76,622 Germanic SVO

VSO
SVO
SVO
SVO
SVO
SVO
SVO
SVO

Table 1: Dataset characteristics

classiﬁer parameters always helps, whereas the
usefulness of sharing LSTM parameters depends
on the language pair. This reﬂects the intuition that
the transition classiﬁer learns hierarchical struc-
tures that are likely to transfer across languages,
based on parser conﬁgurations that abstract away
from several linguistic differences. The similarity
of the input to character- and word-level LSTMs,
on the other hand, will vary depending on the
phonological and morphosyntactic similarity of
the languages in question. Motivated by this ob-
servation, we propose an architecture with hard-
wired transition classiﬁer parameter sharing, but in
which sharing of LSTM parameters is tuned. The
novel architecture signiﬁcantly outperforms our
monolingual baseline on our set of 10 languages.
We additionally investigate parameter sharing of
unrelated languages.

2 The Uppsala dependency parser

The Uppsala parser (de Lhoneux et al., 2017a,b)
consists of three sets of parameters; the param-
eters of the character-based LSTM, those of the
word-based LSTM, and the parameters of the
MLP that predicts transitions. The character-based
LSTM produces representations for the word-
based LSTM, which produces representations for
the MLP. The Uppsala parser is a transition-based
parser (Kiperwasser and Goldberg, 2016), adapted
to the Universal Dependencies (UD) scheme,1
and using the arc-hybrid transition system from
Kuhlmann et al. (2011) extended with a SWAP
transition and a static-dynamic oracle, as de-
scribed in de Lhoneux et al. (2017b). The SWAP

1http://universaldependencies.org/

transition is used to generate non-projective de-
pendency trees (Nivre, 2009).

For an input sentence of length n with words
w1, . . . , wn, the parser creates a sequence of vec-
tors x1:n, where the vector xi representing wi is
the concatenation of a word embedding and the ﬁ-
nal state of the character-based LSTM after pro-
cessing the characters of wi. The character vector
ch(wi) is obtained by running a (bi-directional)
LSTM over the characters chj (1 ≤ j ≤ m)
of wi. Each input element is represented by the
word-level, bi-directional LSTM, as a vector vi =
BILSTM(x1:n, i). For each conﬁguration, the fea-
ture extractor concatenates the LSTM representa-
tions of core elements from the stack and buffer.
Both the embeddings and the LSTMs are trained
together with the model.

A conﬁguration c is represented by a feature
function φ(·) over a subset of its elements. For
transitions are scored by a
each conﬁguration,
classiﬁer, in this case an MLP, and φ(·) is a con-
catenation of BiLSTM vectors on top of the stack
and the beginning of the buffer. The MLP scores
transitions together with the arc labels for transi-
tions that involve adding an arc. In practice, we
use two interpolated MLPs, one which only scores
the transitions, and one which scores transitions
together with the arc label. For simplicity, we re-
fer to that interpolated MLP as the MLP.

3 Parameter sharing

Since our parser has three basic sets of model pa-
rameters, we consider sharing all combinations of
those three sets. We also introduce two ways of
sharing, namely, with or without the addition of a
vector representing the language. This language
embedding enables the model, in theory, to learn
what to share between the two languages in ques-
tion. Since for all three model parameter sets, we
now have three options – not sharing, sharing, or
sharing in the context of a language embedding –
we are left with 33 = 27 parameter sharing strate-
gies; see Table 2.

In the setting where we do not share (✗) word
parameters (W), we construct a different word
lookup table and a different word-level BiLSTM
for each language.
In the setting where we do
hard parameter sharing (D) of word parameters,
we only construct one lookup table and one word
BiLSTM for the languages involved.
In the set-
ting where we do soft sharing (ID) of word pa-

Model C W S

ar

he

es

it

et

ﬁ

nl

no

hr

ru

AV

MONO

76.3

80.2

83.7

83.3

70.4

70.8

77.3

80.8

76.8

82.3

78.2

LANGUAGE-BEST

76.6

80.6

84.4

84.8

72.8

72.9

79.6

82.1

78.0

82.9

79.5

BEST

✗ D ID 76.3

80.3

84.2

84.5

72.1

72.5

78.8

81.4

77.6

82.8

79.1

CHAR D ✗
WORD
STATE

76.4
76.3
✗ D 76.6

✗
✗ D ✗
✗
...
D D D 76.2
ID ID ID 76.3

ALL
SOFT

80.3
79.9
80.3

84.3
83.9
84.0

84.0
84.4
83.7

72.3
72.4
71.5

71.0
71.3
72.9

78.3
77.4
78.3

81.3
80.7
81.5

77.0
76.9
77.4

82.3
82.5
82.8

78.7
78.6
78.9

80.1
79.9

84.0
84.1

84.2
84.4

72.1
72.1

71.4
71.3

78.7
79.6

81.1
81.4

77.0
77.1

82.5
82.5

78.7
78.9

Table 2: Performance on development data (LAS; in %) across select sharing strategies. MONO is our single-task baseline;
LANGUAGE-BEST is using the best sharing strategy for each language (as evaluated on development data); BEST is the overall
best sharing strategy, across languages; CHAR shares only the character-based LSTM parameters; WORD shares only the word-
based LSTM parameters; ALL shares all parameters. D refers to hard sharing, ID refers to soft sharing, using an embedding
of the language ID and ✗ refers to not sharing.

rameters, we share those parameters, and in ad-
dition, concatenate a language embedding li rep-
resenting the language of word wi to the vector
of the word wi at the input of the word BiLSTM:
xi = e(wi) ◦ ch(wi) ◦ li. Similarly for character
parameters (C), we construct a different character
BiLSTM and one character lookup for each lan-
guage (✗), create those for all languages and share
them (D) or share them and concatenate a (ran-
domly initialized) language embedding li repre-
senting the language of word wi at the input of the
character BiLSTM (ID): chj = e(chj ) ◦ li. At the
level of conﬁguration or parser states (S), we either
construct a different MLP for each language (✗),
share the MLP (D) or share it and concatenate a
language embedding li representing the language
of word wi to the vector representing the conﬁgu-
ration, at the input of the MLP (ID): c = φ(·) ◦ li.

4 Experiments

Language pairs We use 10 languages in our ex-
periments, representing ﬁve language pairs from
different language families. Our two SEMITIC lan-
guages are Arabic and Hebrew. These two lan-
guages differ in that Arabic tends to favour VSO
word order whereas Hebrew tends to use SVO, but
are similar in their rich transﬁxing morphology.
Our two FINNO-UGRIC languages are Estonian
and Finnish. These two languages differ in that
Estonian no longer has vowel harmony, but share
a rich agglutinative morphology. Our two SLAVIC
languages are Croatian and Russian. These two

languages differ in that Croatian uses gender in
plural nouns, but otherwise share their rich inﬂec-
tional morphology. Our two ROMANCE languages
are Italian and Spanish. These two languages dif-
fer in that Italian uses a possessive adjective with
a deﬁnite article, but share a fairly strict SVO or-
der. Finally, our two GERMANIC languages are
Dutch and Norwegian. These two languages dif-
fer in morphological complexity, but share word
ordering features to some extent.

Datasets For all 10 languages, we use treebanks
from the Universal Dependencies project. The
dataset characteristics are listed in Table 1. To
keep the results comparable across language pairs,
we down-sample the training set to the size of the
smallest of our languages, Hebrew: we randomly
sample 5000 sentences for each training set. Note
that while this setting makes the experiment some-
what artiﬁcial and will probably overestimate the
beneﬁts that can be obtained from sharing param-
eters when using larger treebanks, we ﬁnd it in-
teresting to see how much low resource languages
can beneﬁt from parameter sharing, as explained
in the introduction.

Baselines and systems This is an evaluation pa-
per, and our results are intended to explore a space
of sharing strategies to ﬁnd better ways of shar-
ing parameters between dependency parsers of
related languages. Our baseline is the Uppsala
parser trained monolingually. Our systems are
parsers trained bilingually by language pair where

we share subsets of parameters between the lan-
guages in the pair, and we report on what sharing
strategies seem superior across the 10 languages
that we consider.

Implementation details A ﬂexible implementa-
tion of parameter strategies for the Uppsala parser
was implemented in Dynet.2 We make the code
publicly available.3

5 Results and discussion

Our results on development sets are presented in
Table 2. We use labeled attachment score (LAS)
as our metric for evaluating parsers. Table 2
presents numbers for a select subset of the 27 shar-
ing strategies. The other results can be found in the
supplementary material. Our main observations
are: (i) that, generally, and as observed in previous
work, multi-task learning helps: all different shar-
ing strategies are on average better than the mono-
lingual baselines, with minor (0.16 LAS points) to
major (0.86 LAS points) average improvements;
and (ii) that sharing the MLP seems to be overall
the 10 best
a better strategy than not sharing it:
strategies share the MLP. Whereas the usefulness
of sharing the MLP seems to be quite robust across
language pairs, the usefulness of sharing word and
character parameters seems more dependent on
the language pairs. This reﬂects the linguistic in-
tuition that character- and word-level LSTMs are
highly sensitive to phonological and morphosyn-
tactic differences such as word order, whereas the
MLP learns to predict less idiosyncratic, hierar-
chical relations from relatively abstract represen-
tations of parser conﬁgurations.

Based on this result, we propose a model
(OURS) where the MLP is shared and the sharing
of word and character parameters is controlled by
a parameter that can be set on validation data.
Results are given in Table 3. We obtain a 0.6 LAS
improvement on average and our proposed model
is signiﬁcantly better than the monolingual base-
line with p < 0.01. Signiﬁcance testing is per-
formed using a randomization test, with the script
from the CoNLL 2017 Shared Task.4

6 Unrelated languages

We repeated the same set of experiments with un-
related language pairs. We hypothesise that pa-

W C

OURS MONO

δ

✗

✗

ar
77.2
ID D 84.3
es
✗
et
71.4
ID
✗
✗
71.6
ﬁ
he D ✗
80.0
hr D ✗
77.9
ID D 85.0
it
ID D 75.5
nl
✗
no
81.1
ID
ru D ✗
83.5

av.

78.8

77.1
83.8
70.5
71.6
79.8
78.0
84.0
74.1
80.1
82.7

78.2

0.1
0.5
0.8
0.1
0.3
-0.1
1.0
1.4
1.0
0.8

0.6

Table 3: LAS on the test sets of the best of 9 sharing strate-
gies and the monolingual baseline. δ is the difference be-
tween OURS AND MONO.

rameter sharing between unrelated language pairs
will be less useful in general than with related lan-
guage pairs. However, it can still be useful, it has
been shown previously that unrelated languages
can beneﬁt from being trained jointly. For exam-
ple, Lynn et al. (2014) have shown that Indonesian
was surprisingly particularly useful for Irish.

The results are presented in Table 4. The ta-
ble only presents part of the results, the rest can
be found in the supplementary material. As ex-
pected, there is much less to be gained from shar-
ing parameters between unrelated pairs. However,
it is possible to improve the monolingual baseline
by sharing some of the parameters.
In general,
sharing the MLP is still a helpful thing to do. It
is most helpful to share the MLP and optionally
one of the two other sets of parameters. Results are
close to the monolingual baseline when everything
is shared. Sharing word and character parameters
but not the MLP hurts accuracy compared to the
monolingual baseline.

7 Related work

Previous work has shown that sharing parame-
ters between dependency parsers for related lan-
guages can lead to improvements (Duong et al.,
2015; Ammar et al., 2016; Susanto and Lu, 2017).
Smith et al. (2018) recently found that sharing pa-
rameters using the same parser as in this paper
(soft sharing of word parameters, hard sharing of
the rest) improves parsing accuracy when train-
ing on related languages, and is especially use-
ful in the low resource case. Similar effects have

2https://github.com/clab/dynet
3https://github.com/coastalcph/uuparser
4https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py

C W S

Model
MONO

he
80.2
80.5
✗ D 80.3
79.8
80.1
79.6
D D D 80.5
ID ID ID 79.8

LANGUAGE-BEST
✗
ID ID ✗
✗
✗ D ✗

BEST
WORST
CHAR D ✗
WORD
ALL
SOFT

no
80.8
81.5
81.5
80.6
80.9
80.9
80.9
80.5

ﬁ
70.8
71.9
71.9
69.2
71.4
71.9
69.8
70.1

hr
76.8
77.6
77.6
76.7
76.8
76.9
76.6
76.6

ru
82.3
82.9
82.7
81.4
82.9
82.2
82.3
82.1

es
83.7
84.0
84.0
83.8
83.9
83.7
83.7
83.9

it
83.3
84.3
83.8
83.2
84.3
83.8
84.0
83.8

et
70.4
72.5
72.5
69.4
70.9
70.9
70.6
70.6

nl
77.3
78.7
78.7
76.6
78.0
77.0
77.4
77.2

ar
76.3
76.5
76.3
76.0
76.5
76.4
76.2
76.3

AV
78.2
78.9
78.9
77.7
78.6
78.3
78.2
78.1

Table 4: Performance on development data (LAS; in %) across select sharing strategies for unrelated languages. MONO is our
single-task baseline; LANGUAGE-BEST is using the best sharing strategy for each language (as evaluated on development data);
BEST and WORST are the overall best and worst sharing strategy across languages; CHAR shares only the character-based
LSTM parameters; WORD shares only the word-based LSTM parameters; ALL shares all parameters. D refers to hard sharing,
ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not sharing.

been observed in machine translation (Dong et al.,
2015; Johnson et al., 2017), for example. Most
studies have only explored a small number of pa-
rameter sharing strategies, however. Vilares et al.
(2016) evaluate parsing with hard parameter shar-
ing for 100 language pairs with a statistical parser.
Naseem et al. (2012) proposed to selectively share
subsets of a parser across languages in the context
of a probabilistic parser.

Options we do not explore here are learning
the architecture jointly with optimizing the task
objective (Misra et al., 2016; Ruder et al., 2017),
or learning an architecture search model
that
predicts an architecture based on the properties
typically with reinforcement learn-
of datasets,
ing (Zoph and Le, 2017; Wong and Gesmundo,
2018; Liang et al., 2018). We also do not ex-
plore the option of sharing selectively based on
more ﬁne-grained typological information about
languages, which related work has indicated could
be useful (Bjerva and Augenstein, 2018). Rather,
we stick to sharing between languages of the same
language families.

The strategies explored here do not exhaust the
space of possible parameter sharing strategies. For
example, we completely ignore soft sharing based
on mean-constrained regularisation (Duong et al.,
2015).

8 Conclusions

made several observations: (a) Generally, multi-
task learning helps. (b) Sharing the MLP param-
eters always helps. It helps to share MLP param-
eters when training a parser on a pair of related
languages, and it also helps if the languages are
unrelated. (c) Sharing word and character param-
eters is differently helpful depending on the lan-
guage. (d) Sharing too many parameters does not
help, when the languages are unrelated.

In future work, we plan to investigate what hap-
pens when training on more than 2 languages.
Here, we focused on a setting with rather small
amounts of balanced data.
It would be interest-
ing to experiment with using datasets that are not
balanced with respect to size. Finally, we have re-
stricted our experiments to a speciﬁc architecture,
using ﬁxed hyperparameters including word and
character embedding dimensions. It would be in-
teresting to experiment with different parsing ar-
chitectures as well as varying those hyperparame-
ters.

Acknowledgments

We acknowledge the computational resources pro-
vided by CSC in Helsinki and Sigma2 in Oslo
through NeIC-NLPL (www.nlpl.eu). The second
author is partially funded by an AdeptMind schol-
arship. The last author was funded by an ERC
Starting Grant.

We present evaluations of 27 parameter sharing
strategies for the Uppsala parser across 10 lan-
guages, representing ﬁve language pairs from ﬁve
different language families. We repeated the ex-
periment with pairs of unrelated languages. We

References

Waleed Ammar, George Mulcaire, Miguel Ballesteros,
Chris Dyer, and Noah A. Smith. 2016. More lan-
guages, one parser. In TACL.

In Proceedings of the 50th Annual Meet-
parsing.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 629–637. Associa-
tion for Computational Linguistics.

Joakim Nivre. 2009. Non-Projective Dependency Pars-
In Proceedings of

ing in Expected Linear Time.
ACL, pages 351–359, Suntec, Singapore.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, et al. 2016. Universal dependencies
v1: A multilingual treebank collection. In Proceed-
ings of the 10th International Conference on Lan-
guage Resources and Evaluation (LREC 2016).

Sebastian Ruder, Joachim Bingel, Isabelle Augenstein,
and Anders Søgaard. 2017. Sluice networks: Learn-
ing what to share between loosely related tasks. In
CoRR, abs/1705.08142.

Aaron Smith, Bernd Bohnet, Miryam de Lhoneux,
Joakim Nivre, Yan Shao, and Sara Stymne. 2018. 82
Treebanks, 34 Models: Universal Dependency Pars-
ing with Multi-Treebank Models. In Proceedings of
the CoNLL 2018 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies.

Raymond Hendy Susanto and Wei Lu. 2017. Neural
architectures for multilingual semantic parsing. In
ACL.

and
David Vilares, Carlos G´omez-Rodr´ıguez,
two lan-
Miguel A Alonso. 2016. One model,
guages: training bilingual parsers with harmonized
In Proceedings of
the 54th Annual
treebanks.
Meeting of
the Association for Computational
Linguistics (Volume 2: Short Papers), volume 2,
pages 425–431.

Catherine Wong and Andrea Gesmundo. 2018. Trans-
fer Learning to Learn with Multitask Neural Model
Search. In ICPR.

Daniel Zeman and Philip Resnik. 2008.

Cross-
Language Parser Adaptation between Related Lan-
guages. In IJCNLP.

Barret Zoph and Quoc Le. 2017. Neural Architecture
Search with Reinforcement Learning. In ICPR.

Johannes Bjerva and Isabelle Augenstein. 2018. From
Phonology to Syntax: Unsupervised Linguistic Ty-
pology at Different Levels with Language Embed-
In Proceedings of the 2018 Conference of
dings.
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers), pages 907–916.
Association for Computational Linguistics.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
tiple language translation. In ACL.

Long Duong, Trevor Cohn, Steven Bird, and Paul
Cook. 2015. Low Resource Dependency Parsing:
Cross-lingual Parameter Sharing in a Neural Net-
work Parser. In Proceedings of ACL.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Vigas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google’s
neural machine translation system. In TACL.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and Accurate Dependency Parsing Using Bidi-
rectional LSTM Feature Representations. TACL,
4:313–327.

Marco Kuhlmann, Carlos G´omez-Rodr´ıguez, and Gior-
gio Satta. 2011. Dynamic Programming Algorithms
for Transition-Based Dependency Parsers. In Pro-
ceedings of ACL, pages 673–682, Portland, Oregon,
USA.

Miryam de Lhoneux, Yan Shao, Ali Basirat, Eliyahu
Kiperwasser, Sara Stymne, Yoav Goldberg, and
Joakim Nivre. 2017a. From raw text to universal
In Proceedings of
dependencies - look, no tags!
the CoNLL 2017 Shared Task: Multilingual Pars-
ing from Raw Text to Universal Dependencies, pages
207–217, Vancouver, Canada.

Miryam de Lhoneux, Sara Stymne, and Joakim Nivre.
2017b. Arc-Hybrid Non-Projective Dependency
Parsing with a Static-Dynamic Oracle. In Proceed-
ings of the 15th International Conference on Parsing
Technologies, pages 99–104, Pisa, Italy.

Jason Liang, Elliot Meyerson, and Risto Miikkulainen.
2018. Evolutionary Architecture Search For Deep
Multitask Networks. In GECCO.

Teresa Lynn, Jennifer Foster, Mark Dras, and Lamia
Tounsi. 2014. Cross-lingual transfer parsing for
low-resourced languages: An irish case study.
In
Proceedings of the First Celtic Language Technol-
ogy Workshop, pages 41–49.

Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and
Martial Hebert. 2016. Cross-Stitch Networks for
Multi-Task Learning. In Proceedings of CVPR.

Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency

C
MONO
✗
✗
✗
✗
✗

D

ID
✗
ID

D

ID
ID
✗

D

D

D

ID

D

D

✗

ID
✗

ID

D

D

ID
ID

W S

ar
76.3
D ID 76.3
D D 76.4
ID ID 76.2
ID D 76.4
✗
ID 76.5
ID D 76.2
✗ D 76.3
✗ D 76.6
ID ID 76.3
ID ID 76.2
✗
✗
76.6
✗
ID 76.2
✗
✗
76.6
✗ D 76.1
✗
ID 76.5
D ID 76.6
ID D 76.1
✗
✗
76.4
D D 76.2
ID ✗
76.6
D D 76.2
D ✗
76.3
D ID 76.0
D ✗
76.1
ID ✗
76.0
D ✗
76.1
ID ✗
75.9

he
80.2
80.3
80.4
80.1
80.4
80.6
79.8
80.2
80.3
79.9
80.1
80.3
79.9
80.3
80.1
80.0
80.1
80.1
80.3
80.1
80.1
79.9
79.9
80.0
79.7
79.3
79.9
79.5

es
83.7
84.2
84.1
84.2
84.2
84.1
84.4
84.0
84.0
84.1
84.3
84.1
84.1
84.0
84.0
84.0
84.0
84.1
84.3
84.0
84.1
83.8
83.9
83.8
83.8
84.1
83.9
84.1

it
83.3
84.5
84.4
84.8
84.3
84.3
84.7
84.4
83.7
84.4
84.5
84.3
84.3
84.2
84.1
84.4
84.6
84.6
84.0
84.2
84.3
84.5
84.4
84.3
84.5
84.4
84.4
84.4

et
70.4
72.1
71.9
71.8
72.2
71.8
72.4
72.8
71.5
72.1
72.5
71.8
71.9
71.2
72.1
71.9
72.1
72.1
72.3
72.1
71.7
72.4
72.4
71.7
71.9
71.5
71.1
72.1

ﬁ
70.8
72.5
72.0
72.4
72.3
72.0
71.5
71.7
72.9
71.3
71.8
71.7
72.0
72.1
72.0
71.7
71.0
71.2
71.0
71.4
71.6
70.3
71.3
70.9
70.4
71.3
70.6
70.5

nl
77.3
78.8
78.7
78.6
78.3
78.5
79.2
78.6
78.3
79.6
78.7
78.3
78.5
78.0
78.2
78.5
78.3
78.0
78.3
78.7
77.6
78.1
77.4
78.3
77.8
77.7
77.8
77.4

no
80.8
81.4
81.5
81.6
81.6
81.5
81.6
82.1
81.5
81.4
81.1
81.5
81.7
81.6
81.8
81.7
81.0
81.4
81.3
81.1
81.0
81.0
80.7
81.0
81.1
80.6
80.9
80.6

hr
76.8
77.6
78.0
77.4
77.4
77.7
76.9
77.2
77.4
77.1
76.7
77.2
77.2
77.3
76.8
76.7
76.9
77.0
77.0
77.0
77.0
77.2
76.9
76.9
76.5
76.7
77.0
77.0

ru
82.3
82.8
82.8
82.9
82.8
82.9
82.8
82.3
82.8
82.5
82.6
82.5
82.3
82.6
82.6
82.3
82.8
82.7
82.3
82.5
82.6
82.6
82.5
82.5
82.3
82.5
82.0
82.2

average
78.2
79.1
79.0
79.0
79.0
79.0
78.9
78.9
78.9
78.9
78.8
78.8
78.8
78.8
78.8
78.8
78.7
78.7
78.7
78.7
78.7
78.6
78.6
78.5
78.4
78.4
78.4
78.4

Table 5: Performance (LAS; in %) across select sharing strategies ranked by average performance. MONO is our single-task
baseline; W refers to sharing the word parameters, C refers to sharing the character parameters, S refers to sharing the MLP
parameters. D refers to hard sharing, ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not
sharing.

C
MONO
✗
✗
✗
✗
ID
ID

D

ID
✗
✗

D

✗

D

✗
✗

ID
ID

D

D

D

ID
ID

D

D

D

ID
ID

W S

he
80.2
✗ D 80.3
✗
✗
80.3
ID ID 80.3
✗
ID 79.7
✗
ID 80.3
✗
✗
80.0
✗
ID 80.3
✗ D 80.4
D ID 80.5
ID D 80.6
✗ D 80.3
D D 80.6
✗
✗
80.1
ID ✗
80.3
D ✗
79.6
D ID 80.3
D D 80.1
D D 80.5
ID ID 80.3
D ID 79.8
ID D 80.0
ID ID 79.8
ID D 80.3
D ✗
80.4
ID ✗
79.6
D ✗
79.6
ID ✗
79.8

no
80.8
81.5
81.7
81.1
81.5
81.6
81.6
81.5
81.4
81.2
81.1
81.1
81.1
80.9
81.3
80.9
81.0
80.8
80.9
80.7
80.9
80.8
80.5
81.1
80.3
80.6
80.1
80.6

ﬁ
70.8
71.9
71.9
72.1
72.2
71.8
71.5
71.5
71.6
72.2
71.9
71.3
71.6
71.4
71.2
71.9
70.5
70.4
69.8
70.2
71.0
69.8
70.1
70.2
70.1
69.4
69.9
69.2

hr
76.8
77.6
77.7
77.7
77.1
77.5
77.2
77.5
77.3
77.0
77.1
77.0
76.8
76.8
77.4
76.9
76.5
77.0
76.6
76.1
76.2
76.2
76.6
76.1
76.6
76.7
76.6
76.7

ru
82.3
82.7
82.5
82.7
82.7
82.6
82.7
82.7
82.6
82.5
82.7
82.6
82.5
82.9
81.9
82.2
82.3
82.2
82.3
82.1
82.1
82.2
82.1
82.2
82.0
81.7
81.8
81.4

es
83.7
84.0
84.0
84.2
83.8
84.1
83.9
83.7
83.9
84.0
84.2
84.0
83.7
83.9
84.2
83.7
83.7
83.8
83.7
83.8
83.7
83.8
83.9
84.1
83.7
83.8
83.5
83.8

it
83.3
83.8
84.2
84.2
84.0
83.8
84.0
83.9
84.1
83.8
84.0
84.2
83.9
84.3
83.9
83.8
83.6
83.8
84.0
83.8
83.6
84.3
83.8
83.7
83.2
83.4
82.9
83.2

et
70.4
72.5
71.8
72.4
72.3
71.8
71.1
71.6
71.9
71.5
71.9
71.8
71.4
70.9
70.7
70.9
71.4
71.0
70.6
70.8
70.9
70.7
70.6
70.3
69.3
69.2
69.2
69.4

nl
77.3
78.7
78.5
77.9
78.6
78.3
79.0
77.9
77.7
78.3
77.1
77.8
78.1
78.0
77.6
77.0
77.8
77.6
77.4
77.6
77.3
77.2
77.2
76.9
76.7
77.6
77.6
76.6

ar
76.3
76.4
76.5
76.0
76.5
76.2
76.4
76.7
76.4
76.1
76.2
76.3
76.3
76.5
75.8
76.4
76.1
76.2
76.2
76.2
76.0
76.2
76.3
76.0
76.2
76.2
76.3
76.0

average
78.2
78.9
78.9
78.9
78.8
78.8
78.7
78.7
78.7
78.7
78.7
78.6
78.6
78.6
78.4
78.3
78.3
78.3
78.2
78.2
78.2
78.1
78.1
78.1
77.8
77.8
77.7
77.7

Table 6: Unrelated language pairs. Performance (LAS; in %) across select sharing strategies ranked by average performance.
MONO is our single-task baseline; W refers to sharing the word parameters, C refers to sharing the character parameters, S
refers to sharing the MLP parameters. D refers to hard sharing, ID refers to soft sharing, using an embedding of the language
ID and ✗ refers to not sharing.

Parameter sharing between dependency parsers for related languages

Miryam de Lhoneux1∗ Johannes Bjerva2
1Department of Linguistics and Philology
Uppsala University
Uppsala, Sweden

Isabelle Augenstein2 Anders Søgaard2
2 Department of Computer Science
University of Copenhagen
Copenhagen, Denmark

8
1
0
2
 
t
c
O
 
4
 
 
]
L
C
.
s
c
[
 
 
2
v
5
5
0
9
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

Previous work has suggested that parameter
sharing between transition-based neural de-
pendency parsers for related languages can
lead to better performance, but there is no
consensus on what parameters to share. We
present an evaluation of 27 different parame-
ter sharing strategies across 10 languages, rep-
resenting ﬁve pairs of related languages, each
pair from a different language family. We ﬁnd
that sharing transition classiﬁer parameters al-
ways helps, whereas the usefulness of shar-
ing word and/or character LSTM parameters
varies. Based on this result, we propose an
architecture where the transition classiﬁer is
shared, and the sharing of word and charac-
ter parameters is controlled by a parameter that
can be tuned on validation data. This model is
linguistically motivated and obtains signiﬁcant
improvements over a mono-lingually trained
baseline. We also ﬁnd that sharing transi-
tion classiﬁer parameters helps when training
a parser on unrelated language pairs, but we
ﬁnd that, in the case of unrelated languages,
sharing too many parameters does not help.

1 Introduction

The idea of sharing parameters between parsers
of related languages goes back to early work
in cross-lingual adaptation (Zeman and Resnik,
2008), and the idea has recently received a lot
of interest in the context of neural dependency
parsers (Duong et al., 2015; Ammar et al., 2016;
Susanto and Lu, 2017). Modern neural depen-
dency parsers, however, use different sets of pa-
rameters for representation and scoring, and it is
not clear what parameters it is best to share.

The Universal Dependencies

(UD) project
(Nivre et al., 2016), which is seeking to harmonize
the annotation of dependency treebanks across
∗ Work carried out during a stay at the University of

Copenhagen.

languages, has seen a steady increase in languages
that have a treebank in a common standard. Many
of these languages are low resource and have small
UD treebanks.
It seems interesting to ﬁnd out
ways to leverage the wealth of information con-
tained in these treebanks, especially for low re-
source languages.

In this paper, we evaluate 27 different pa-
rameter sharing strategies. We focus on a par-
ticular transition-based neural dependency parser
(de Lhoneux et al., 2017a,b), which performs
close to the state of the art. This parser has
three sets of parameters:
i) the parameters of a
character-based one-layer, bidirectional LSTM; ii)
the parameters of a word-based two-layer, bidirec-
tional LSTM; iii) and the parameters of a multi-
layered perceptron (MLP) with a single hidden
layer. The two ﬁrst sets are for learning to repre-
sent conﬁgurations; the third for selecting the next
transition. We consider all combinations of shar-
ing these sets of parameters; and in addition, we
consider two ways of sharing each set of parame-
ters, namely with and without a preﬁxed language
embedding. The latter enables partial, soft shar-
ing. In sum, we consider all 33 combinations of no
sharing, hard sharing and soft sharing of the three
sets of parameters. We evaluate the 27 multilin-
gual parsers on 10 languages from the UD project,
representing ﬁve pairs of related languages, each
pair from a different language family. We repeat
the experiment with the same set of languages, but
using pairs of unrelated languages.

Contributions This paper is, to the best of our
knowledge, the ﬁrst to evaluate different parame-
ter sharing strategies for exploiting synergies be-
tween neural dependency parsers of related lan-
guages. We evaluate the different strategies on
10 languages, representing ﬁve different language
families. We ﬁnd that sharing (MLP) transition

Lang Tokens Family

Word order

ar
he
et
ﬁ
hr
ru
it
es
nl
no

Semitic
Semitic
Finnic
Finnic
Slavic
Slavic

208,932
161,685
60,393
67,258
109,965
90,170
113,825 Romance
154,844 Romance
75,796 Germanic No dom. order
76,622 Germanic SVO

VSO
SVO
SVO
SVO
SVO
SVO
SVO
SVO

Table 1: Dataset characteristics

classiﬁer parameters always helps, whereas the
usefulness of sharing LSTM parameters depends
on the language pair. This reﬂects the intuition that
the transition classiﬁer learns hierarchical struc-
tures that are likely to transfer across languages,
based on parser conﬁgurations that abstract away
from several linguistic differences. The similarity
of the input to character- and word-level LSTMs,
on the other hand, will vary depending on the
phonological and morphosyntactic similarity of
the languages in question. Motivated by this ob-
servation, we propose an architecture with hard-
wired transition classiﬁer parameter sharing, but in
which sharing of LSTM parameters is tuned. The
novel architecture signiﬁcantly outperforms our
monolingual baseline on our set of 10 languages.
We additionally investigate parameter sharing of
unrelated languages.

2 The Uppsala dependency parser

The Uppsala parser (de Lhoneux et al., 2017a,b)
consists of three sets of parameters; the param-
eters of the character-based LSTM, those of the
word-based LSTM, and the parameters of the
MLP that predicts transitions. The character-based
LSTM produces representations for the word-
based LSTM, which produces representations for
the MLP. The Uppsala parser is a transition-based
parser (Kiperwasser and Goldberg, 2016), adapted
to the Universal Dependencies (UD) scheme,1
and using the arc-hybrid transition system from
Kuhlmann et al. (2011) extended with a SWAP
transition and a static-dynamic oracle, as de-
scribed in de Lhoneux et al. (2017b). The SWAP

1http://universaldependencies.org/

transition is used to generate non-projective de-
pendency trees (Nivre, 2009).

For an input sentence of length n with words
w1, . . . , wn, the parser creates a sequence of vec-
tors x1:n, where the vector xi representing wi is
the concatenation of a word embedding and the ﬁ-
nal state of the character-based LSTM after pro-
cessing the characters of wi. The character vector
ch(wi) is obtained by running a (bi-directional)
LSTM over the characters chj (1 ≤ j ≤ m)
of wi. Each input element is represented by the
word-level, bi-directional LSTM, as a vector vi =
BILSTM(x1:n, i). For each conﬁguration, the fea-
ture extractor concatenates the LSTM representa-
tions of core elements from the stack and buffer.
Both the embeddings and the LSTMs are trained
together with the model.

A conﬁguration c is represented by a feature
function φ(·) over a subset of its elements. For
transitions are scored by a
each conﬁguration,
classiﬁer, in this case an MLP, and φ(·) is a con-
catenation of BiLSTM vectors on top of the stack
and the beginning of the buffer. The MLP scores
transitions together with the arc labels for transi-
tions that involve adding an arc. In practice, we
use two interpolated MLPs, one which only scores
the transitions, and one which scores transitions
together with the arc label. For simplicity, we re-
fer to that interpolated MLP as the MLP.

3 Parameter sharing

Since our parser has three basic sets of model pa-
rameters, we consider sharing all combinations of
those three sets. We also introduce two ways of
sharing, namely, with or without the addition of a
vector representing the language. This language
embedding enables the model, in theory, to learn
what to share between the two languages in ques-
tion. Since for all three model parameter sets, we
now have three options – not sharing, sharing, or
sharing in the context of a language embedding –
we are left with 33 = 27 parameter sharing strate-
gies; see Table 2.

In the setting where we do not share (✗) word
parameters (W), we construct a different word
lookup table and a different word-level BiLSTM
for each language.
In the setting where we do
hard parameter sharing (D) of word parameters,
we only construct one lookup table and one word
BiLSTM for the languages involved.
In the set-
ting where we do soft sharing (ID) of word pa-

Model C W S

ar

he

es

it

et

ﬁ

nl

no

hr

ru

AV

MONO

76.3

80.2

83.7

83.3

70.4

70.8

77.3

80.8

76.8

82.3

78.2

LANGUAGE-BEST

76.6

80.6

84.4

84.8

72.8

72.9

79.6

82.1

78.0

82.9

79.5

BEST

✗ D ID 76.3

80.3

84.2

84.5

72.1

72.5

78.8

81.4

77.6

82.8

79.1

CHAR D ✗
WORD
STATE

76.4
76.3
✗ D 76.6

✗
✗ D ✗
✗
...
D D D 76.2
ID ID ID 76.3

ALL
SOFT

80.3
79.9
80.3

84.3
83.9
84.0

84.0
84.4
83.7

72.3
72.4
71.5

71.0
71.3
72.9

78.3
77.4
78.3

81.3
80.7
81.5

77.0
76.9
77.4

82.3
82.5
82.8

78.7
78.6
78.9

80.1
79.9

84.0
84.1

84.2
84.4

72.1
72.1

71.4
71.3

78.7
79.6

81.1
81.4

77.0
77.1

82.5
82.5

78.7
78.9

Table 2: Performance on development data (LAS; in %) across select sharing strategies. MONO is our single-task baseline;
LANGUAGE-BEST is using the best sharing strategy for each language (as evaluated on development data); BEST is the overall
best sharing strategy, across languages; CHAR shares only the character-based LSTM parameters; WORD shares only the word-
based LSTM parameters; ALL shares all parameters. D refers to hard sharing, ID refers to soft sharing, using an embedding
of the language ID and ✗ refers to not sharing.

rameters, we share those parameters, and in ad-
dition, concatenate a language embedding li rep-
resenting the language of word wi to the vector
of the word wi at the input of the word BiLSTM:
xi = e(wi) ◦ ch(wi) ◦ li. Similarly for character
parameters (C), we construct a different character
BiLSTM and one character lookup for each lan-
guage (✗), create those for all languages and share
them (D) or share them and concatenate a (ran-
domly initialized) language embedding li repre-
senting the language of word wi at the input of the
character BiLSTM (ID): chj = e(chj ) ◦ li. At the
level of conﬁguration or parser states (S), we either
construct a different MLP for each language (✗),
share the MLP (D) or share it and concatenate a
language embedding li representing the language
of word wi to the vector representing the conﬁgu-
ration, at the input of the MLP (ID): c = φ(·) ◦ li.

4 Experiments

Language pairs We use 10 languages in our ex-
periments, representing ﬁve language pairs from
different language families. Our two SEMITIC lan-
guages are Arabic and Hebrew. These two lan-
guages differ in that Arabic tends to favour VSO
word order whereas Hebrew tends to use SVO, but
are similar in their rich transﬁxing morphology.
Our two FINNO-UGRIC languages are Estonian
and Finnish. These two languages differ in that
Estonian no longer has vowel harmony, but share
a rich agglutinative morphology. Our two SLAVIC
languages are Croatian and Russian. These two

languages differ in that Croatian uses gender in
plural nouns, but otherwise share their rich inﬂec-
tional morphology. Our two ROMANCE languages
are Italian and Spanish. These two languages dif-
fer in that Italian uses a possessive adjective with
a deﬁnite article, but share a fairly strict SVO or-
der. Finally, our two GERMANIC languages are
Dutch and Norwegian. These two languages dif-
fer in morphological complexity, but share word
ordering features to some extent.

Datasets For all 10 languages, we use treebanks
from the Universal Dependencies project. The
dataset characteristics are listed in Table 1. To
keep the results comparable across language pairs,
we down-sample the training set to the size of the
smallest of our languages, Hebrew: we randomly
sample 5000 sentences for each training set. Note
that while this setting makes the experiment some-
what artiﬁcial and will probably overestimate the
beneﬁts that can be obtained from sharing param-
eters when using larger treebanks, we ﬁnd it in-
teresting to see how much low resource languages
can beneﬁt from parameter sharing, as explained
in the introduction.

Baselines and systems This is an evaluation pa-
per, and our results are intended to explore a space
of sharing strategies to ﬁnd better ways of shar-
ing parameters between dependency parsers of
related languages. Our baseline is the Uppsala
parser trained monolingually. Our systems are
parsers trained bilingually by language pair where

we share subsets of parameters between the lan-
guages in the pair, and we report on what sharing
strategies seem superior across the 10 languages
that we consider.

Implementation details A ﬂexible implementa-
tion of parameter strategies for the Uppsala parser
was implemented in Dynet.2 We make the code
publicly available.3

5 Results and discussion

Our results on development sets are presented in
Table 2. We use labeled attachment score (LAS)
as our metric for evaluating parsers. Table 2
presents numbers for a select subset of the 27 shar-
ing strategies. The other results can be found in the
supplementary material. Our main observations
are: (i) that, generally, and as observed in previous
work, multi-task learning helps: all different shar-
ing strategies are on average better than the mono-
lingual baselines, with minor (0.16 LAS points) to
major (0.86 LAS points) average improvements;
and (ii) that sharing the MLP seems to be overall
the 10 best
a better strategy than not sharing it:
strategies share the MLP. Whereas the usefulness
of sharing the MLP seems to be quite robust across
language pairs, the usefulness of sharing word and
character parameters seems more dependent on
the language pairs. This reﬂects the linguistic in-
tuition that character- and word-level LSTMs are
highly sensitive to phonological and morphosyn-
tactic differences such as word order, whereas the
MLP learns to predict less idiosyncratic, hierar-
chical relations from relatively abstract represen-
tations of parser conﬁgurations.

Based on this result, we propose a model
(OURS) where the MLP is shared and the sharing
of word and character parameters is controlled by
a parameter that can be set on validation data.
Results are given in Table 3. We obtain a 0.6 LAS
improvement on average and our proposed model
is signiﬁcantly better than the monolingual base-
line with p < 0.01. Signiﬁcance testing is per-
formed using a randomization test, with the script
from the CoNLL 2017 Shared Task.4

6 Unrelated languages

We repeated the same set of experiments with un-
related language pairs. We hypothesise that pa-

W C

OURS MONO

δ

✗

✗

ar
77.2
ID D 84.3
es
✗
et
71.4
ID
✗
✗
71.6
ﬁ
he D ✗
80.0
hr D ✗
77.9
ID D 85.0
it
ID D 75.5
nl
✗
no
81.1
ID
ru D ✗
83.5

av.

78.8

77.1
83.8
70.5
71.6
79.8
78.0
84.0
74.1
80.1
82.7

78.2

0.1
0.5
0.8
0.1
0.3
-0.1
1.0
1.4
1.0
0.8

0.6

Table 3: LAS on the test sets of the best of 9 sharing strate-
gies and the monolingual baseline. δ is the difference be-
tween OURS AND MONO.

rameter sharing between unrelated language pairs
will be less useful in general than with related lan-
guage pairs. However, it can still be useful, it has
been shown previously that unrelated languages
can beneﬁt from being trained jointly. For exam-
ple, Lynn et al. (2014) have shown that Indonesian
was surprisingly particularly useful for Irish.

The results are presented in Table 4. The ta-
ble only presents part of the results, the rest can
be found in the supplementary material. As ex-
pected, there is much less to be gained from shar-
ing parameters between unrelated pairs. However,
it is possible to improve the monolingual baseline
by sharing some of the parameters.
In general,
sharing the MLP is still a helpful thing to do. It
is most helpful to share the MLP and optionally
one of the two other sets of parameters. Results are
close to the monolingual baseline when everything
is shared. Sharing word and character parameters
but not the MLP hurts accuracy compared to the
monolingual baseline.

7 Related work

Previous work has shown that sharing parame-
ters between dependency parsers for related lan-
guages can lead to improvements (Duong et al.,
2015; Ammar et al., 2016; Susanto and Lu, 2017).
Smith et al. (2018) recently found that sharing pa-
rameters using the same parser as in this paper
(soft sharing of word parameters, hard sharing of
the rest) improves parsing accuracy when train-
ing on related languages, and is especially use-
ful in the low resource case. Similar effects have

2https://github.com/clab/dynet
3https://github.com/coastalcph/uuparser
4https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py

C W S

Model
MONO

he
80.2
80.5
✗ D 80.3
79.8
80.1
79.6
D D D 80.5
ID ID ID 79.8

LANGUAGE-BEST
✗
ID ID ✗
✗
✗ D ✗

BEST
WORST
CHAR D ✗
WORD
ALL
SOFT

no
80.8
81.5
81.5
80.6
80.9
80.9
80.9
80.5

ﬁ
70.8
71.9
71.9
69.2
71.4
71.9
69.8
70.1

hr
76.8
77.6
77.6
76.7
76.8
76.9
76.6
76.6

ru
82.3
82.9
82.7
81.4
82.9
82.2
82.3
82.1

es
83.7
84.0
84.0
83.8
83.9
83.7
83.7
83.9

it
83.3
84.3
83.8
83.2
84.3
83.8
84.0
83.8

et
70.4
72.5
72.5
69.4
70.9
70.9
70.6
70.6

nl
77.3
78.7
78.7
76.6
78.0
77.0
77.4
77.2

ar
76.3
76.5
76.3
76.0
76.5
76.4
76.2
76.3

AV
78.2
78.9
78.9
77.7
78.6
78.3
78.2
78.1

Table 4: Performance on development data (LAS; in %) across select sharing strategies for unrelated languages. MONO is our
single-task baseline; LANGUAGE-BEST is using the best sharing strategy for each language (as evaluated on development data);
BEST and WORST are the overall best and worst sharing strategy across languages; CHAR shares only the character-based
LSTM parameters; WORD shares only the word-based LSTM parameters; ALL shares all parameters. D refers to hard sharing,
ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not sharing.

been observed in machine translation (Dong et al.,
2015; Johnson et al., 2017), for example. Most
studies have only explored a small number of pa-
rameter sharing strategies, however. Vilares et al.
(2016) evaluate parsing with hard parameter shar-
ing for 100 language pairs with a statistical parser.
Naseem et al. (2012) proposed to selectively share
subsets of a parser across languages in the context
of a probabilistic parser.

Options we do not explore here are learning
the architecture jointly with optimizing the task
objective (Misra et al., 2016; Ruder et al., 2017),
or learning an architecture search model
that
predicts an architecture based on the properties
typically with reinforcement learn-
of datasets,
ing (Zoph and Le, 2017; Wong and Gesmundo,
2018; Liang et al., 2018). We also do not ex-
plore the option of sharing selectively based on
more ﬁne-grained typological information about
languages, which related work has indicated could
be useful (Bjerva and Augenstein, 2018). Rather,
we stick to sharing between languages of the same
language families.

The strategies explored here do not exhaust the
space of possible parameter sharing strategies. For
example, we completely ignore soft sharing based
on mean-constrained regularisation (Duong et al.,
2015).

8 Conclusions

made several observations: (a) Generally, multi-
task learning helps. (b) Sharing the MLP param-
eters always helps. It helps to share MLP param-
eters when training a parser on a pair of related
languages, and it also helps if the languages are
unrelated. (c) Sharing word and character param-
eters is differently helpful depending on the lan-
guage. (d) Sharing too many parameters does not
help, when the languages are unrelated.

In future work, we plan to investigate what hap-
pens when training on more than 2 languages.
Here, we focused on a setting with rather small
amounts of balanced data.
It would be interest-
ing to experiment with using datasets that are not
balanced with respect to size. Finally, we have re-
stricted our experiments to a speciﬁc architecture,
using ﬁxed hyperparameters including word and
character embedding dimensions. It would be in-
teresting to experiment with different parsing ar-
chitectures as well as varying those hyperparame-
ters.

Acknowledgments

We acknowledge the computational resources pro-
vided by CSC in Helsinki and Sigma2 in Oslo
through NeIC-NLPL (www.nlpl.eu). The second
author is partially funded by an AdeptMind schol-
arship. The last author was funded by an ERC
Starting Grant.

We present evaluations of 27 parameter sharing
strategies for the Uppsala parser across 10 lan-
guages, representing ﬁve language pairs from ﬁve
different language families. We repeated the ex-
periment with pairs of unrelated languages. We

References

Waleed Ammar, George Mulcaire, Miguel Ballesteros,
Chris Dyer, and Noah A. Smith. 2016. More lan-
guages, one parser. In TACL.

In Proceedings of the 50th Annual Meet-
parsing.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 629–637. Associa-
tion for Computational Linguistics.

Joakim Nivre. 2009. Non-Projective Dependency Pars-
In Proceedings of

ing in Expected Linear Time.
ACL, pages 351–359, Suntec, Singapore.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, et al. 2016. Universal dependencies
v1: A multilingual treebank collection. In Proceed-
ings of the 10th International Conference on Lan-
guage Resources and Evaluation (LREC 2016).

Sebastian Ruder, Joachim Bingel, Isabelle Augenstein,
and Anders Søgaard. 2017. Sluice networks: Learn-
ing what to share between loosely related tasks. In
CoRR, abs/1705.08142.

Aaron Smith, Bernd Bohnet, Miryam de Lhoneux,
Joakim Nivre, Yan Shao, and Sara Stymne. 2018. 82
Treebanks, 34 Models: Universal Dependency Pars-
ing with Multi-Treebank Models. In Proceedings of
the CoNLL 2018 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies.

Raymond Hendy Susanto and Wei Lu. 2017. Neural
architectures for multilingual semantic parsing. In
ACL.

and
David Vilares, Carlos G´omez-Rodr´ıguez,
two lan-
Miguel A Alonso. 2016. One model,
guages: training bilingual parsers with harmonized
In Proceedings of
the 54th Annual
treebanks.
Meeting of
the Association for Computational
Linguistics (Volume 2: Short Papers), volume 2,
pages 425–431.

Catherine Wong and Andrea Gesmundo. 2018. Trans-
fer Learning to Learn with Multitask Neural Model
Search. In ICPR.

Daniel Zeman and Philip Resnik. 2008.

Cross-
Language Parser Adaptation between Related Lan-
guages. In IJCNLP.

Barret Zoph and Quoc Le. 2017. Neural Architecture
Search with Reinforcement Learning. In ICPR.

Johannes Bjerva and Isabelle Augenstein. 2018. From
Phonology to Syntax: Unsupervised Linguistic Ty-
pology at Different Levels with Language Embed-
In Proceedings of the 2018 Conference of
dings.
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers), pages 907–916.
Association for Computational Linguistics.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
tiple language translation. In ACL.

Long Duong, Trevor Cohn, Steven Bird, and Paul
Cook. 2015. Low Resource Dependency Parsing:
Cross-lingual Parameter Sharing in a Neural Net-
work Parser. In Proceedings of ACL.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Vigas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google’s
neural machine translation system. In TACL.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and Accurate Dependency Parsing Using Bidi-
rectional LSTM Feature Representations. TACL,
4:313–327.

Marco Kuhlmann, Carlos G´omez-Rodr´ıguez, and Gior-
gio Satta. 2011. Dynamic Programming Algorithms
for Transition-Based Dependency Parsers. In Pro-
ceedings of ACL, pages 673–682, Portland, Oregon,
USA.

Miryam de Lhoneux, Yan Shao, Ali Basirat, Eliyahu
Kiperwasser, Sara Stymne, Yoav Goldberg, and
Joakim Nivre. 2017a. From raw text to universal
In Proceedings of
dependencies - look, no tags!
the CoNLL 2017 Shared Task: Multilingual Pars-
ing from Raw Text to Universal Dependencies, pages
207–217, Vancouver, Canada.

Miryam de Lhoneux, Sara Stymne, and Joakim Nivre.
2017b. Arc-Hybrid Non-Projective Dependency
Parsing with a Static-Dynamic Oracle. In Proceed-
ings of the 15th International Conference on Parsing
Technologies, pages 99–104, Pisa, Italy.

Jason Liang, Elliot Meyerson, and Risto Miikkulainen.
2018. Evolutionary Architecture Search For Deep
Multitask Networks. In GECCO.

Teresa Lynn, Jennifer Foster, Mark Dras, and Lamia
Tounsi. 2014. Cross-lingual transfer parsing for
low-resourced languages: An irish case study.
In
Proceedings of the First Celtic Language Technol-
ogy Workshop, pages 41–49.

Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and
Martial Hebert. 2016. Cross-Stitch Networks for
Multi-Task Learning. In Proceedings of CVPR.

Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency

C
MONO
✗
✗
✗
✗
✗

D

ID
✗
ID

D

ID
ID
✗

D

D

D

ID

D

D

✗

ID
✗

ID

D

D

ID
ID

W S

ar
76.3
D ID 76.3
D D 76.4
ID ID 76.2
ID D 76.4
✗
ID 76.5
ID D 76.2
✗ D 76.3
✗ D 76.6
ID ID 76.3
ID ID 76.2
✗
✗
76.6
✗
ID 76.2
✗
✗
76.6
✗ D 76.1
✗
ID 76.5
D ID 76.6
ID D 76.1
✗
✗
76.4
D D 76.2
ID ✗
76.6
D D 76.2
D ✗
76.3
D ID 76.0
D ✗
76.1
ID ✗
76.0
D ✗
76.1
ID ✗
75.9

he
80.2
80.3
80.4
80.1
80.4
80.6
79.8
80.2
80.3
79.9
80.1
80.3
79.9
80.3
80.1
80.0
80.1
80.1
80.3
80.1
80.1
79.9
79.9
80.0
79.7
79.3
79.9
79.5

es
83.7
84.2
84.1
84.2
84.2
84.1
84.4
84.0
84.0
84.1
84.3
84.1
84.1
84.0
84.0
84.0
84.0
84.1
84.3
84.0
84.1
83.8
83.9
83.8
83.8
84.1
83.9
84.1

it
83.3
84.5
84.4
84.8
84.3
84.3
84.7
84.4
83.7
84.4
84.5
84.3
84.3
84.2
84.1
84.4
84.6
84.6
84.0
84.2
84.3
84.5
84.4
84.3
84.5
84.4
84.4
84.4

et
70.4
72.1
71.9
71.8
72.2
71.8
72.4
72.8
71.5
72.1
72.5
71.8
71.9
71.2
72.1
71.9
72.1
72.1
72.3
72.1
71.7
72.4
72.4
71.7
71.9
71.5
71.1
72.1

ﬁ
70.8
72.5
72.0
72.4
72.3
72.0
71.5
71.7
72.9
71.3
71.8
71.7
72.0
72.1
72.0
71.7
71.0
71.2
71.0
71.4
71.6
70.3
71.3
70.9
70.4
71.3
70.6
70.5

nl
77.3
78.8
78.7
78.6
78.3
78.5
79.2
78.6
78.3
79.6
78.7
78.3
78.5
78.0
78.2
78.5
78.3
78.0
78.3
78.7
77.6
78.1
77.4
78.3
77.8
77.7
77.8
77.4

no
80.8
81.4
81.5
81.6
81.6
81.5
81.6
82.1
81.5
81.4
81.1
81.5
81.7
81.6
81.8
81.7
81.0
81.4
81.3
81.1
81.0
81.0
80.7
81.0
81.1
80.6
80.9
80.6

hr
76.8
77.6
78.0
77.4
77.4
77.7
76.9
77.2
77.4
77.1
76.7
77.2
77.2
77.3
76.8
76.7
76.9
77.0
77.0
77.0
77.0
77.2
76.9
76.9
76.5
76.7
77.0
77.0

ru
82.3
82.8
82.8
82.9
82.8
82.9
82.8
82.3
82.8
82.5
82.6
82.5
82.3
82.6
82.6
82.3
82.8
82.7
82.3
82.5
82.6
82.6
82.5
82.5
82.3
82.5
82.0
82.2

average
78.2
79.1
79.0
79.0
79.0
79.0
78.9
78.9
78.9
78.9
78.8
78.8
78.8
78.8
78.8
78.8
78.7
78.7
78.7
78.7
78.7
78.6
78.6
78.5
78.4
78.4
78.4
78.4

Table 5: Performance (LAS; in %) across select sharing strategies ranked by average performance. MONO is our single-task
baseline; W refers to sharing the word parameters, C refers to sharing the character parameters, S refers to sharing the MLP
parameters. D refers to hard sharing, ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not
sharing.

C
MONO
✗
✗
✗
✗
ID
ID

D

ID
✗
✗

D

✗

D

✗
✗

ID
ID

D

D

D

ID
ID

D

D

D

ID
ID

W S

he
80.2
✗ D 80.3
✗
✗
80.3
ID ID 80.3
✗
ID 79.7
✗
ID 80.3
✗
✗
80.0
✗
ID 80.3
✗ D 80.4
D ID 80.5
ID D 80.6
✗ D 80.3
D D 80.6
✗
✗
80.1
ID ✗
80.3
D ✗
79.6
D ID 80.3
D D 80.1
D D 80.5
ID ID 80.3
D ID 79.8
ID D 80.0
ID ID 79.8
ID D 80.3
D ✗
80.4
ID ✗
79.6
D ✗
79.6
ID ✗
79.8

no
80.8
81.5
81.7
81.1
81.5
81.6
81.6
81.5
81.4
81.2
81.1
81.1
81.1
80.9
81.3
80.9
81.0
80.8
80.9
80.7
80.9
80.8
80.5
81.1
80.3
80.6
80.1
80.6

ﬁ
70.8
71.9
71.9
72.1
72.2
71.8
71.5
71.5
71.6
72.2
71.9
71.3
71.6
71.4
71.2
71.9
70.5
70.4
69.8
70.2
71.0
69.8
70.1
70.2
70.1
69.4
69.9
69.2

hr
76.8
77.6
77.7
77.7
77.1
77.5
77.2
77.5
77.3
77.0
77.1
77.0
76.8
76.8
77.4
76.9
76.5
77.0
76.6
76.1
76.2
76.2
76.6
76.1
76.6
76.7
76.6
76.7

ru
82.3
82.7
82.5
82.7
82.7
82.6
82.7
82.7
82.6
82.5
82.7
82.6
82.5
82.9
81.9
82.2
82.3
82.2
82.3
82.1
82.1
82.2
82.1
82.2
82.0
81.7
81.8
81.4

es
83.7
84.0
84.0
84.2
83.8
84.1
83.9
83.7
83.9
84.0
84.2
84.0
83.7
83.9
84.2
83.7
83.7
83.8
83.7
83.8
83.7
83.8
83.9
84.1
83.7
83.8
83.5
83.8

it
83.3
83.8
84.2
84.2
84.0
83.8
84.0
83.9
84.1
83.8
84.0
84.2
83.9
84.3
83.9
83.8
83.6
83.8
84.0
83.8
83.6
84.3
83.8
83.7
83.2
83.4
82.9
83.2

et
70.4
72.5
71.8
72.4
72.3
71.8
71.1
71.6
71.9
71.5
71.9
71.8
71.4
70.9
70.7
70.9
71.4
71.0
70.6
70.8
70.9
70.7
70.6
70.3
69.3
69.2
69.2
69.4

nl
77.3
78.7
78.5
77.9
78.6
78.3
79.0
77.9
77.7
78.3
77.1
77.8
78.1
78.0
77.6
77.0
77.8
77.6
77.4
77.6
77.3
77.2
77.2
76.9
76.7
77.6
77.6
76.6

ar
76.3
76.4
76.5
76.0
76.5
76.2
76.4
76.7
76.4
76.1
76.2
76.3
76.3
76.5
75.8
76.4
76.1
76.2
76.2
76.2
76.0
76.2
76.3
76.0
76.2
76.2
76.3
76.0

average
78.2
78.9
78.9
78.9
78.8
78.8
78.7
78.7
78.7
78.7
78.7
78.6
78.6
78.6
78.4
78.3
78.3
78.3
78.2
78.2
78.2
78.1
78.1
78.1
77.8
77.8
77.7
77.7

Table 6: Unrelated language pairs. Performance (LAS; in %) across select sharing strategies ranked by average performance.
MONO is our single-task baseline; W refers to sharing the word parameters, C refers to sharing the character parameters, S
refers to sharing the MLP parameters. D refers to hard sharing, ID refers to soft sharing, using an embedding of the language
ID and ✗ refers to not sharing.

Parameter sharing between dependency parsers for related languages

Miryam de Lhoneux1∗ Johannes Bjerva2
1Department of Linguistics and Philology
Uppsala University
Uppsala, Sweden

Isabelle Augenstein2 Anders Søgaard2
2 Department of Computer Science
University of Copenhagen
Copenhagen, Denmark

8
1
0
2
 
t
c
O
 
4
 
 
]
L
C
.
s
c
[
 
 
2
v
5
5
0
9
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

Previous work has suggested that parameter
sharing between transition-based neural de-
pendency parsers for related languages can
lead to better performance, but there is no
consensus on what parameters to share. We
present an evaluation of 27 different parame-
ter sharing strategies across 10 languages, rep-
resenting ﬁve pairs of related languages, each
pair from a different language family. We ﬁnd
that sharing transition classiﬁer parameters al-
ways helps, whereas the usefulness of shar-
ing word and/or character LSTM parameters
varies. Based on this result, we propose an
architecture where the transition classiﬁer is
shared, and the sharing of word and charac-
ter parameters is controlled by a parameter that
can be tuned on validation data. This model is
linguistically motivated and obtains signiﬁcant
improvements over a mono-lingually trained
baseline. We also ﬁnd that sharing transi-
tion classiﬁer parameters helps when training
a parser on unrelated language pairs, but we
ﬁnd that, in the case of unrelated languages,
sharing too many parameters does not help.

1 Introduction

The idea of sharing parameters between parsers
of related languages goes back to early work
in cross-lingual adaptation (Zeman and Resnik,
2008), and the idea has recently received a lot
of interest in the context of neural dependency
parsers (Duong et al., 2015; Ammar et al., 2016;
Susanto and Lu, 2017). Modern neural depen-
dency parsers, however, use different sets of pa-
rameters for representation and scoring, and it is
not clear what parameters it is best to share.

The Universal Dependencies

(UD) project
(Nivre et al., 2016), which is seeking to harmonize
the annotation of dependency treebanks across
∗ Work carried out during a stay at the University of

Copenhagen.

languages, has seen a steady increase in languages
that have a treebank in a common standard. Many
of these languages are low resource and have small
UD treebanks.
It seems interesting to ﬁnd out
ways to leverage the wealth of information con-
tained in these treebanks, especially for low re-
source languages.

In this paper, we evaluate 27 different pa-
rameter sharing strategies. We focus on a par-
ticular transition-based neural dependency parser
(de Lhoneux et al., 2017a,b), which performs
close to the state of the art. This parser has
three sets of parameters:
i) the parameters of a
character-based one-layer, bidirectional LSTM; ii)
the parameters of a word-based two-layer, bidirec-
tional LSTM; iii) and the parameters of a multi-
layered perceptron (MLP) with a single hidden
layer. The two ﬁrst sets are for learning to repre-
sent conﬁgurations; the third for selecting the next
transition. We consider all combinations of shar-
ing these sets of parameters; and in addition, we
consider two ways of sharing each set of parame-
ters, namely with and without a preﬁxed language
embedding. The latter enables partial, soft shar-
ing. In sum, we consider all 33 combinations of no
sharing, hard sharing and soft sharing of the three
sets of parameters. We evaluate the 27 multilin-
gual parsers on 10 languages from the UD project,
representing ﬁve pairs of related languages, each
pair from a different language family. We repeat
the experiment with the same set of languages, but
using pairs of unrelated languages.

Contributions This paper is, to the best of our
knowledge, the ﬁrst to evaluate different parame-
ter sharing strategies for exploiting synergies be-
tween neural dependency parsers of related lan-
guages. We evaluate the different strategies on
10 languages, representing ﬁve different language
families. We ﬁnd that sharing (MLP) transition

Lang Tokens Family

Word order

ar
he
et
ﬁ
hr
ru
it
es
nl
no

Semitic
Semitic
Finnic
Finnic
Slavic
Slavic

208,932
161,685
60,393
67,258
109,965
90,170
113,825 Romance
154,844 Romance
75,796 Germanic No dom. order
76,622 Germanic SVO

VSO
SVO
SVO
SVO
SVO
SVO
SVO
SVO

Table 1: Dataset characteristics

classiﬁer parameters always helps, whereas the
usefulness of sharing LSTM parameters depends
on the language pair. This reﬂects the intuition that
the transition classiﬁer learns hierarchical struc-
tures that are likely to transfer across languages,
based on parser conﬁgurations that abstract away
from several linguistic differences. The similarity
of the input to character- and word-level LSTMs,
on the other hand, will vary depending on the
phonological and morphosyntactic similarity of
the languages in question. Motivated by this ob-
servation, we propose an architecture with hard-
wired transition classiﬁer parameter sharing, but in
which sharing of LSTM parameters is tuned. The
novel architecture signiﬁcantly outperforms our
monolingual baseline on our set of 10 languages.
We additionally investigate parameter sharing of
unrelated languages.

2 The Uppsala dependency parser

The Uppsala parser (de Lhoneux et al., 2017a,b)
consists of three sets of parameters; the param-
eters of the character-based LSTM, those of the
word-based LSTM, and the parameters of the
MLP that predicts transitions. The character-based
LSTM produces representations for the word-
based LSTM, which produces representations for
the MLP. The Uppsala parser is a transition-based
parser (Kiperwasser and Goldberg, 2016), adapted
to the Universal Dependencies (UD) scheme,1
and using the arc-hybrid transition system from
Kuhlmann et al. (2011) extended with a SWAP
transition and a static-dynamic oracle, as de-
scribed in de Lhoneux et al. (2017b). The SWAP

1http://universaldependencies.org/

transition is used to generate non-projective de-
pendency trees (Nivre, 2009).

For an input sentence of length n with words
w1, . . . , wn, the parser creates a sequence of vec-
tors x1:n, where the vector xi representing wi is
the concatenation of a word embedding and the ﬁ-
nal state of the character-based LSTM after pro-
cessing the characters of wi. The character vector
ch(wi) is obtained by running a (bi-directional)
LSTM over the characters chj (1 ≤ j ≤ m)
of wi. Each input element is represented by the
word-level, bi-directional LSTM, as a vector vi =
BILSTM(x1:n, i). For each conﬁguration, the fea-
ture extractor concatenates the LSTM representa-
tions of core elements from the stack and buffer.
Both the embeddings and the LSTMs are trained
together with the model.

A conﬁguration c is represented by a feature
function φ(·) over a subset of its elements. For
transitions are scored by a
each conﬁguration,
classiﬁer, in this case an MLP, and φ(·) is a con-
catenation of BiLSTM vectors on top of the stack
and the beginning of the buffer. The MLP scores
transitions together with the arc labels for transi-
tions that involve adding an arc. In practice, we
use two interpolated MLPs, one which only scores
the transitions, and one which scores transitions
together with the arc label. For simplicity, we re-
fer to that interpolated MLP as the MLP.

3 Parameter sharing

Since our parser has three basic sets of model pa-
rameters, we consider sharing all combinations of
those three sets. We also introduce two ways of
sharing, namely, with or without the addition of a
vector representing the language. This language
embedding enables the model, in theory, to learn
what to share between the two languages in ques-
tion. Since for all three model parameter sets, we
now have three options – not sharing, sharing, or
sharing in the context of a language embedding –
we are left with 33 = 27 parameter sharing strate-
gies; see Table 2.

In the setting where we do not share (✗) word
parameters (W), we construct a different word
lookup table and a different word-level BiLSTM
for each language.
In the setting where we do
hard parameter sharing (D) of word parameters,
we only construct one lookup table and one word
BiLSTM for the languages involved.
In the set-
ting where we do soft sharing (ID) of word pa-

Model C W S

ar

he

es

it

et

ﬁ

nl

no

hr

ru

AV

MONO

76.3

80.2

83.7

83.3

70.4

70.8

77.3

80.8

76.8

82.3

78.2

LANGUAGE-BEST

76.6

80.6

84.4

84.8

72.8

72.9

79.6

82.1

78.0

82.9

79.5

BEST

✗ D ID 76.3

80.3

84.2

84.5

72.1

72.5

78.8

81.4

77.6

82.8

79.1

CHAR D ✗
WORD
STATE

76.4
76.3
✗ D 76.6

✗
✗ D ✗
✗
...
D D D 76.2
ID ID ID 76.3

ALL
SOFT

80.3
79.9
80.3

84.3
83.9
84.0

84.0
84.4
83.7

72.3
72.4
71.5

71.0
71.3
72.9

78.3
77.4
78.3

81.3
80.7
81.5

77.0
76.9
77.4

82.3
82.5
82.8

78.7
78.6
78.9

80.1
79.9

84.0
84.1

84.2
84.4

72.1
72.1

71.4
71.3

78.7
79.6

81.1
81.4

77.0
77.1

82.5
82.5

78.7
78.9

Table 2: Performance on development data (LAS; in %) across select sharing strategies. MONO is our single-task baseline;
LANGUAGE-BEST is using the best sharing strategy for each language (as evaluated on development data); BEST is the overall
best sharing strategy, across languages; CHAR shares only the character-based LSTM parameters; WORD shares only the word-
based LSTM parameters; ALL shares all parameters. D refers to hard sharing, ID refers to soft sharing, using an embedding
of the language ID and ✗ refers to not sharing.

rameters, we share those parameters, and in ad-
dition, concatenate a language embedding li rep-
resenting the language of word wi to the vector
of the word wi at the input of the word BiLSTM:
xi = e(wi) ◦ ch(wi) ◦ li. Similarly for character
parameters (C), we construct a different character
BiLSTM and one character lookup for each lan-
guage (✗), create those for all languages and share
them (D) or share them and concatenate a (ran-
domly initialized) language embedding li repre-
senting the language of word wi at the input of the
character BiLSTM (ID): chj = e(chj ) ◦ li. At the
level of conﬁguration or parser states (S), we either
construct a different MLP for each language (✗),
share the MLP (D) or share it and concatenate a
language embedding li representing the language
of word wi to the vector representing the conﬁgu-
ration, at the input of the MLP (ID): c = φ(·) ◦ li.

4 Experiments

Language pairs We use 10 languages in our ex-
periments, representing ﬁve language pairs from
different language families. Our two SEMITIC lan-
guages are Arabic and Hebrew. These two lan-
guages differ in that Arabic tends to favour VSO
word order whereas Hebrew tends to use SVO, but
are similar in their rich transﬁxing morphology.
Our two FINNO-UGRIC languages are Estonian
and Finnish. These two languages differ in that
Estonian no longer has vowel harmony, but share
a rich agglutinative morphology. Our two SLAVIC
languages are Croatian and Russian. These two

languages differ in that Croatian uses gender in
plural nouns, but otherwise share their rich inﬂec-
tional morphology. Our two ROMANCE languages
are Italian and Spanish. These two languages dif-
fer in that Italian uses a possessive adjective with
a deﬁnite article, but share a fairly strict SVO or-
der. Finally, our two GERMANIC languages are
Dutch and Norwegian. These two languages dif-
fer in morphological complexity, but share word
ordering features to some extent.

Datasets For all 10 languages, we use treebanks
from the Universal Dependencies project. The
dataset characteristics are listed in Table 1. To
keep the results comparable across language pairs,
we down-sample the training set to the size of the
smallest of our languages, Hebrew: we randomly
sample 5000 sentences for each training set. Note
that while this setting makes the experiment some-
what artiﬁcial and will probably overestimate the
beneﬁts that can be obtained from sharing param-
eters when using larger treebanks, we ﬁnd it in-
teresting to see how much low resource languages
can beneﬁt from parameter sharing, as explained
in the introduction.

Baselines and systems This is an evaluation pa-
per, and our results are intended to explore a space
of sharing strategies to ﬁnd better ways of shar-
ing parameters between dependency parsers of
related languages. Our baseline is the Uppsala
parser trained monolingually. Our systems are
parsers trained bilingually by language pair where

we share subsets of parameters between the lan-
guages in the pair, and we report on what sharing
strategies seem superior across the 10 languages
that we consider.

Implementation details A ﬂexible implementa-
tion of parameter strategies for the Uppsala parser
was implemented in Dynet.2 We make the code
publicly available.3

5 Results and discussion

Our results on development sets are presented in
Table 2. We use labeled attachment score (LAS)
as our metric for evaluating parsers. Table 2
presents numbers for a select subset of the 27 shar-
ing strategies. The other results can be found in the
supplementary material. Our main observations
are: (i) that, generally, and as observed in previous
work, multi-task learning helps: all different shar-
ing strategies are on average better than the mono-
lingual baselines, with minor (0.16 LAS points) to
major (0.86 LAS points) average improvements;
and (ii) that sharing the MLP seems to be overall
the 10 best
a better strategy than not sharing it:
strategies share the MLP. Whereas the usefulness
of sharing the MLP seems to be quite robust across
language pairs, the usefulness of sharing word and
character parameters seems more dependent on
the language pairs. This reﬂects the linguistic in-
tuition that character- and word-level LSTMs are
highly sensitive to phonological and morphosyn-
tactic differences such as word order, whereas the
MLP learns to predict less idiosyncratic, hierar-
chical relations from relatively abstract represen-
tations of parser conﬁgurations.

Based on this result, we propose a model
(OURS) where the MLP is shared and the sharing
of word and character parameters is controlled by
a parameter that can be set on validation data.
Results are given in Table 3. We obtain a 0.6 LAS
improvement on average and our proposed model
is signiﬁcantly better than the monolingual base-
line with p < 0.01. Signiﬁcance testing is per-
formed using a randomization test, with the script
from the CoNLL 2017 Shared Task.4

6 Unrelated languages

We repeated the same set of experiments with un-
related language pairs. We hypothesise that pa-

W C

OURS MONO

δ

✗

✗

ar
77.2
ID D 84.3
es
✗
et
71.4
ID
✗
✗
71.6
ﬁ
he D ✗
80.0
hr D ✗
77.9
ID D 85.0
it
ID D 75.5
nl
✗
no
81.1
ID
ru D ✗
83.5

av.

78.8

77.1
83.8
70.5
71.6
79.8
78.0
84.0
74.1
80.1
82.7

78.2

0.1
0.5
0.8
0.1
0.3
-0.1
1.0
1.4
1.0
0.8

0.6

Table 3: LAS on the test sets of the best of 9 sharing strate-
gies and the monolingual baseline. δ is the difference be-
tween OURS AND MONO.

rameter sharing between unrelated language pairs
will be less useful in general than with related lan-
guage pairs. However, it can still be useful, it has
been shown previously that unrelated languages
can beneﬁt from being trained jointly. For exam-
ple, Lynn et al. (2014) have shown that Indonesian
was surprisingly particularly useful for Irish.

The results are presented in Table 4. The ta-
ble only presents part of the results, the rest can
be found in the supplementary material. As ex-
pected, there is much less to be gained from shar-
ing parameters between unrelated pairs. However,
it is possible to improve the monolingual baseline
by sharing some of the parameters.
In general,
sharing the MLP is still a helpful thing to do. It
is most helpful to share the MLP and optionally
one of the two other sets of parameters. Results are
close to the monolingual baseline when everything
is shared. Sharing word and character parameters
but not the MLP hurts accuracy compared to the
monolingual baseline.

7 Related work

Previous work has shown that sharing parame-
ters between dependency parsers for related lan-
guages can lead to improvements (Duong et al.,
2015; Ammar et al., 2016; Susanto and Lu, 2017).
Smith et al. (2018) recently found that sharing pa-
rameters using the same parser as in this paper
(soft sharing of word parameters, hard sharing of
the rest) improves parsing accuracy when train-
ing on related languages, and is especially use-
ful in the low resource case. Similar effects have

2https://github.com/clab/dynet
3https://github.com/coastalcph/uuparser
4https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py

C W S

Model
MONO

he
80.2
80.5
✗ D 80.3
79.8
80.1
79.6
D D D 80.5
ID ID ID 79.8

LANGUAGE-BEST
✗
ID ID ✗
✗
✗ D ✗

BEST
WORST
CHAR D ✗
WORD
ALL
SOFT

no
80.8
81.5
81.5
80.6
80.9
80.9
80.9
80.5

ﬁ
70.8
71.9
71.9
69.2
71.4
71.9
69.8
70.1

hr
76.8
77.6
77.6
76.7
76.8
76.9
76.6
76.6

ru
82.3
82.9
82.7
81.4
82.9
82.2
82.3
82.1

es
83.7
84.0
84.0
83.8
83.9
83.7
83.7
83.9

it
83.3
84.3
83.8
83.2
84.3
83.8
84.0
83.8

et
70.4
72.5
72.5
69.4
70.9
70.9
70.6
70.6

nl
77.3
78.7
78.7
76.6
78.0
77.0
77.4
77.2

ar
76.3
76.5
76.3
76.0
76.5
76.4
76.2
76.3

AV
78.2
78.9
78.9
77.7
78.6
78.3
78.2
78.1

Table 4: Performance on development data (LAS; in %) across select sharing strategies for unrelated languages. MONO is our
single-task baseline; LANGUAGE-BEST is using the best sharing strategy for each language (as evaluated on development data);
BEST and WORST are the overall best and worst sharing strategy across languages; CHAR shares only the character-based
LSTM parameters; WORD shares only the word-based LSTM parameters; ALL shares all parameters. D refers to hard sharing,
ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not sharing.

been observed in machine translation (Dong et al.,
2015; Johnson et al., 2017), for example. Most
studies have only explored a small number of pa-
rameter sharing strategies, however. Vilares et al.
(2016) evaluate parsing with hard parameter shar-
ing for 100 language pairs with a statistical parser.
Naseem et al. (2012) proposed to selectively share
subsets of a parser across languages in the context
of a probabilistic parser.

Options we do not explore here are learning
the architecture jointly with optimizing the task
objective (Misra et al., 2016; Ruder et al., 2017),
or learning an architecture search model
that
predicts an architecture based on the properties
typically with reinforcement learn-
of datasets,
ing (Zoph and Le, 2017; Wong and Gesmundo,
2018; Liang et al., 2018). We also do not ex-
plore the option of sharing selectively based on
more ﬁne-grained typological information about
languages, which related work has indicated could
be useful (Bjerva and Augenstein, 2018). Rather,
we stick to sharing between languages of the same
language families.

The strategies explored here do not exhaust the
space of possible parameter sharing strategies. For
example, we completely ignore soft sharing based
on mean-constrained regularisation (Duong et al.,
2015).

8 Conclusions

made several observations: (a) Generally, multi-
task learning helps. (b) Sharing the MLP param-
eters always helps. It helps to share MLP param-
eters when training a parser on a pair of related
languages, and it also helps if the languages are
unrelated. (c) Sharing word and character param-
eters is differently helpful depending on the lan-
guage. (d) Sharing too many parameters does not
help, when the languages are unrelated.

In future work, we plan to investigate what hap-
pens when training on more than 2 languages.
Here, we focused on a setting with rather small
amounts of balanced data.
It would be interest-
ing to experiment with using datasets that are not
balanced with respect to size. Finally, we have re-
stricted our experiments to a speciﬁc architecture,
using ﬁxed hyperparameters including word and
character embedding dimensions. It would be in-
teresting to experiment with different parsing ar-
chitectures as well as varying those hyperparame-
ters.

Acknowledgments

We acknowledge the computational resources pro-
vided by CSC in Helsinki and Sigma2 in Oslo
through NeIC-NLPL (www.nlpl.eu). The second
author is partially funded by an AdeptMind schol-
arship. The last author was funded by an ERC
Starting Grant.

We present evaluations of 27 parameter sharing
strategies for the Uppsala parser across 10 lan-
guages, representing ﬁve language pairs from ﬁve
different language families. We repeated the ex-
periment with pairs of unrelated languages. We

References

Waleed Ammar, George Mulcaire, Miguel Ballesteros,
Chris Dyer, and Noah A. Smith. 2016. More lan-
guages, one parser. In TACL.

In Proceedings of the 50th Annual Meet-
parsing.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 629–637. Associa-
tion for Computational Linguistics.

Joakim Nivre. 2009. Non-Projective Dependency Pars-
In Proceedings of

ing in Expected Linear Time.
ACL, pages 351–359, Suntec, Singapore.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, et al. 2016. Universal dependencies
v1: A multilingual treebank collection. In Proceed-
ings of the 10th International Conference on Lan-
guage Resources and Evaluation (LREC 2016).

Sebastian Ruder, Joachim Bingel, Isabelle Augenstein,
and Anders Søgaard. 2017. Sluice networks: Learn-
ing what to share between loosely related tasks. In
CoRR, abs/1705.08142.

Aaron Smith, Bernd Bohnet, Miryam de Lhoneux,
Joakim Nivre, Yan Shao, and Sara Stymne. 2018. 82
Treebanks, 34 Models: Universal Dependency Pars-
ing with Multi-Treebank Models. In Proceedings of
the CoNLL 2018 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies.

Raymond Hendy Susanto and Wei Lu. 2017. Neural
architectures for multilingual semantic parsing. In
ACL.

and
David Vilares, Carlos G´omez-Rodr´ıguez,
two lan-
Miguel A Alonso. 2016. One model,
guages: training bilingual parsers with harmonized
In Proceedings of
the 54th Annual
treebanks.
Meeting of
the Association for Computational
Linguistics (Volume 2: Short Papers), volume 2,
pages 425–431.

Catherine Wong and Andrea Gesmundo. 2018. Trans-
fer Learning to Learn with Multitask Neural Model
Search. In ICPR.

Daniel Zeman and Philip Resnik. 2008.

Cross-
Language Parser Adaptation between Related Lan-
guages. In IJCNLP.

Barret Zoph and Quoc Le. 2017. Neural Architecture
Search with Reinforcement Learning. In ICPR.

Johannes Bjerva and Isabelle Augenstein. 2018. From
Phonology to Syntax: Unsupervised Linguistic Ty-
pology at Different Levels with Language Embed-
In Proceedings of the 2018 Conference of
dings.
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers), pages 907–916.
Association for Computational Linguistics.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
tiple language translation. In ACL.

Long Duong, Trevor Cohn, Steven Bird, and Paul
Cook. 2015. Low Resource Dependency Parsing:
Cross-lingual Parameter Sharing in a Neural Net-
work Parser. In Proceedings of ACL.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Vigas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google’s
neural machine translation system. In TACL.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and Accurate Dependency Parsing Using Bidi-
rectional LSTM Feature Representations. TACL,
4:313–327.

Marco Kuhlmann, Carlos G´omez-Rodr´ıguez, and Gior-
gio Satta. 2011. Dynamic Programming Algorithms
for Transition-Based Dependency Parsers. In Pro-
ceedings of ACL, pages 673–682, Portland, Oregon,
USA.

Miryam de Lhoneux, Yan Shao, Ali Basirat, Eliyahu
Kiperwasser, Sara Stymne, Yoav Goldberg, and
Joakim Nivre. 2017a. From raw text to universal
In Proceedings of
dependencies - look, no tags!
the CoNLL 2017 Shared Task: Multilingual Pars-
ing from Raw Text to Universal Dependencies, pages
207–217, Vancouver, Canada.

Miryam de Lhoneux, Sara Stymne, and Joakim Nivre.
2017b. Arc-Hybrid Non-Projective Dependency
Parsing with a Static-Dynamic Oracle. In Proceed-
ings of the 15th International Conference on Parsing
Technologies, pages 99–104, Pisa, Italy.

Jason Liang, Elliot Meyerson, and Risto Miikkulainen.
2018. Evolutionary Architecture Search For Deep
Multitask Networks. In GECCO.

Teresa Lynn, Jennifer Foster, Mark Dras, and Lamia
Tounsi. 2014. Cross-lingual transfer parsing for
low-resourced languages: An irish case study.
In
Proceedings of the First Celtic Language Technol-
ogy Workshop, pages 41–49.

Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and
Martial Hebert. 2016. Cross-Stitch Networks for
Multi-Task Learning. In Proceedings of CVPR.

Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency

C
MONO
✗
✗
✗
✗
✗

D

ID
✗
ID

D

ID
ID
✗

D

D

D

ID

D

D

✗

ID
✗

ID

D

D

ID
ID

W S

ar
76.3
D ID 76.3
D D 76.4
ID ID 76.2
ID D 76.4
✗
ID 76.5
ID D 76.2
✗ D 76.3
✗ D 76.6
ID ID 76.3
ID ID 76.2
✗
✗
76.6
✗
ID 76.2
✗
✗
76.6
✗ D 76.1
✗
ID 76.5
D ID 76.6
ID D 76.1
✗
✗
76.4
D D 76.2
ID ✗
76.6
D D 76.2
D ✗
76.3
D ID 76.0
D ✗
76.1
ID ✗
76.0
D ✗
76.1
ID ✗
75.9

he
80.2
80.3
80.4
80.1
80.4
80.6
79.8
80.2
80.3
79.9
80.1
80.3
79.9
80.3
80.1
80.0
80.1
80.1
80.3
80.1
80.1
79.9
79.9
80.0
79.7
79.3
79.9
79.5

es
83.7
84.2
84.1
84.2
84.2
84.1
84.4
84.0
84.0
84.1
84.3
84.1
84.1
84.0
84.0
84.0
84.0
84.1
84.3
84.0
84.1
83.8
83.9
83.8
83.8
84.1
83.9
84.1

it
83.3
84.5
84.4
84.8
84.3
84.3
84.7
84.4
83.7
84.4
84.5
84.3
84.3
84.2
84.1
84.4
84.6
84.6
84.0
84.2
84.3
84.5
84.4
84.3
84.5
84.4
84.4
84.4

et
70.4
72.1
71.9
71.8
72.2
71.8
72.4
72.8
71.5
72.1
72.5
71.8
71.9
71.2
72.1
71.9
72.1
72.1
72.3
72.1
71.7
72.4
72.4
71.7
71.9
71.5
71.1
72.1

ﬁ
70.8
72.5
72.0
72.4
72.3
72.0
71.5
71.7
72.9
71.3
71.8
71.7
72.0
72.1
72.0
71.7
71.0
71.2
71.0
71.4
71.6
70.3
71.3
70.9
70.4
71.3
70.6
70.5

nl
77.3
78.8
78.7
78.6
78.3
78.5
79.2
78.6
78.3
79.6
78.7
78.3
78.5
78.0
78.2
78.5
78.3
78.0
78.3
78.7
77.6
78.1
77.4
78.3
77.8
77.7
77.8
77.4

no
80.8
81.4
81.5
81.6
81.6
81.5
81.6
82.1
81.5
81.4
81.1
81.5
81.7
81.6
81.8
81.7
81.0
81.4
81.3
81.1
81.0
81.0
80.7
81.0
81.1
80.6
80.9
80.6

hr
76.8
77.6
78.0
77.4
77.4
77.7
76.9
77.2
77.4
77.1
76.7
77.2
77.2
77.3
76.8
76.7
76.9
77.0
77.0
77.0
77.0
77.2
76.9
76.9
76.5
76.7
77.0
77.0

ru
82.3
82.8
82.8
82.9
82.8
82.9
82.8
82.3
82.8
82.5
82.6
82.5
82.3
82.6
82.6
82.3
82.8
82.7
82.3
82.5
82.6
82.6
82.5
82.5
82.3
82.5
82.0
82.2

average
78.2
79.1
79.0
79.0
79.0
79.0
78.9
78.9
78.9
78.9
78.8
78.8
78.8
78.8
78.8
78.8
78.7
78.7
78.7
78.7
78.7
78.6
78.6
78.5
78.4
78.4
78.4
78.4

Table 5: Performance (LAS; in %) across select sharing strategies ranked by average performance. MONO is our single-task
baseline; W refers to sharing the word parameters, C refers to sharing the character parameters, S refers to sharing the MLP
parameters. D refers to hard sharing, ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not
sharing.

C
MONO
✗
✗
✗
✗
ID
ID

D

ID
✗
✗

D

✗

D

✗
✗

ID
ID

D

D

D

ID
ID

D

D

D

ID
ID

W S

he
80.2
✗ D 80.3
✗
✗
80.3
ID ID 80.3
✗
ID 79.7
✗
ID 80.3
✗
✗
80.0
✗
ID 80.3
✗ D 80.4
D ID 80.5
ID D 80.6
✗ D 80.3
D D 80.6
✗
✗
80.1
ID ✗
80.3
D ✗
79.6
D ID 80.3
D D 80.1
D D 80.5
ID ID 80.3
D ID 79.8
ID D 80.0
ID ID 79.8
ID D 80.3
D ✗
80.4
ID ✗
79.6
D ✗
79.6
ID ✗
79.8

no
80.8
81.5
81.7
81.1
81.5
81.6
81.6
81.5
81.4
81.2
81.1
81.1
81.1
80.9
81.3
80.9
81.0
80.8
80.9
80.7
80.9
80.8
80.5
81.1
80.3
80.6
80.1
80.6

ﬁ
70.8
71.9
71.9
72.1
72.2
71.8
71.5
71.5
71.6
72.2
71.9
71.3
71.6
71.4
71.2
71.9
70.5
70.4
69.8
70.2
71.0
69.8
70.1
70.2
70.1
69.4
69.9
69.2

hr
76.8
77.6
77.7
77.7
77.1
77.5
77.2
77.5
77.3
77.0
77.1
77.0
76.8
76.8
77.4
76.9
76.5
77.0
76.6
76.1
76.2
76.2
76.6
76.1
76.6
76.7
76.6
76.7

ru
82.3
82.7
82.5
82.7
82.7
82.6
82.7
82.7
82.6
82.5
82.7
82.6
82.5
82.9
81.9
82.2
82.3
82.2
82.3
82.1
82.1
82.2
82.1
82.2
82.0
81.7
81.8
81.4

es
83.7
84.0
84.0
84.2
83.8
84.1
83.9
83.7
83.9
84.0
84.2
84.0
83.7
83.9
84.2
83.7
83.7
83.8
83.7
83.8
83.7
83.8
83.9
84.1
83.7
83.8
83.5
83.8

it
83.3
83.8
84.2
84.2
84.0
83.8
84.0
83.9
84.1
83.8
84.0
84.2
83.9
84.3
83.9
83.8
83.6
83.8
84.0
83.8
83.6
84.3
83.8
83.7
83.2
83.4
82.9
83.2

et
70.4
72.5
71.8
72.4
72.3
71.8
71.1
71.6
71.9
71.5
71.9
71.8
71.4
70.9
70.7
70.9
71.4
71.0
70.6
70.8
70.9
70.7
70.6
70.3
69.3
69.2
69.2
69.4

nl
77.3
78.7
78.5
77.9
78.6
78.3
79.0
77.9
77.7
78.3
77.1
77.8
78.1
78.0
77.6
77.0
77.8
77.6
77.4
77.6
77.3
77.2
77.2
76.9
76.7
77.6
77.6
76.6

ar
76.3
76.4
76.5
76.0
76.5
76.2
76.4
76.7
76.4
76.1
76.2
76.3
76.3
76.5
75.8
76.4
76.1
76.2
76.2
76.2
76.0
76.2
76.3
76.0
76.2
76.2
76.3
76.0

average
78.2
78.9
78.9
78.9
78.8
78.8
78.7
78.7
78.7
78.7
78.7
78.6
78.6
78.6
78.4
78.3
78.3
78.3
78.2
78.2
78.2
78.1
78.1
78.1
77.8
77.8
77.7
77.7

Table 6: Unrelated language pairs. Performance (LAS; in %) across select sharing strategies ranked by average performance.
MONO is our single-task baseline; W refers to sharing the word parameters, C refers to sharing the character parameters, S
refers to sharing the MLP parameters. D refers to hard sharing, ID refers to soft sharing, using an embedding of the language
ID and ✗ refers to not sharing.

