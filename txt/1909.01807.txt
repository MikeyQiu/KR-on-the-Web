ICDM 2019 Knowledge Graph Contest: Team UWA

1st Michael Stewart
The University of Western Australia
Perth, Australia
michael.stewart@research.uwa.edu.au

2nd Majigsuren Enkhsaikhan
The University of Western Australia
Perth, Australia
majigsuren.enkhsaikhan@research.uwa.edu.au

3rd Wei Liu
The University of Western Australia
Perth, Australia
wei.liu@uwa.edu.au

9
1
0
2
 
p
e
S
 
4
 
 
]
L
C
.
s
c
[
 
 
1
v
7
0
8
1
0
.
9
0
9
1
:
v
i
X
r
a

A. Introduction

I. MODEL

We begin our report by discussing the challenges we ex-
perienced and the motivation behind our approach. We then
describe each component of our system in detail.

Our ﬁrst approach for addressing the contest speciﬁcation
was a novel end-to-end, deep learning-based system. The most
challenging task was to ﬁnd a way to represent the data;
considering a sentence may have zero or many triples, and that
the relations should be obtained directly from the text, it was
exceedingly difﬁcult to represent the input data in such a way
that allowed the model to predict a decently-sized set of valid
triples from a given document. Our best deep learning-based
approach produced high-quality triples, but only in very small
numbers. We hence decided to veer away from deep learning
and capitalise on the wide variety of readily-available natural
language processing tools.

For general English text, resources are available including
several annotated benchmark datasets and off-the-shelf tools.
For example, CoNLL-2003 English benchmark dataset [1]
is a collection of Reuters news-wire articles, annotated with
four entity types: persons, organizations, locations, and mis-
cellaneous names1. It contains around 300,000 tokens of
22,137 sentences. OntoNotes5.0 [2] is an annotated corpus of
2.9 million words from news, phone conversations, weblogs,
broadcast, talk shows in three languages (English, Chinese,
and Arabic) with structural information (syntax and predi-
cate argument structure) and shallow semantics (word sense
linked to an ontology and coreference)2. Off-the-shelf standard
named entity recognition (NER) tools are able to recognize
named entities of a restricted list of pre-deﬁned entity types,
such as location, person names, organization names, money,
date, and time. Popular tools include NLTK [3], SpaCy [4],
Stanford Named Entity Recogniser [5] and AllenNLP [6], [7].
However, when it comes to real-world applications, such
as the domain speciﬁc text
in automotive engineering or
public security, we face the low-resource data problem similar
to machine translation between rare languages. There is no
benchmark annotated dataset relevant to those domains, and it
is near-impossible to ﬁnd the right pivot language that allows
us to take advantages of existing high resource NER tools.
In automotive engineering domain, car types and car related

names are more important than person or organisation names.
For example, in the sentence Ford re-tuned the suspension and
magnetic dampers to allow the GT350 to stiffen the suspension
for better performance on the track, the important entities are
Ford, GT350, suspension, and magnetic dampers, but NER
tools can only capture Ford and GT350 as entities and ignore
the other phrases. In order to avoid missing salient information
units, chunking of noun phrases for entities and chunking of
action related phrases for relations are performed in this work.
Our team also experimented with Open Information Extrac-
tion (OpenIE) [8] and knowledge graph construction systems.
There are a wide range of OpenIE systems available, with
recent approaches incorporating neural networks in order to
maximise performance [9]. We found that OpenIE tends to
produce a vast number of triples, with many subjects or objects
being long sequences of words as opposed to useful entities.
This is detrimental to the contest task, which demands a re-
ﬁned set of high-quality triples. Knowledge graph construction
systems, such as T2KG [10], rely on ﬁxed relation types and
as such are also undesirable for the contest task.

We ultimately found that the best performance was achieved
by maintaining a high level of simplicity and utilising a
pipeline-based approach. Our system is built using well-
established natural language processing frameworks such as
NLTK3 and SpaCy4, and makes use of standard techniques
such as tokenisation, part-of-speech (POS) tagging, named en-
tity recognition, coreference resolution, and noun/verb phrase
chunking. We incorporate several of our own algorithms in
order to address the aforementioned shortcomings of NER on
domain-speciﬁc data.

B. Triple extraction system

Our triple extraction system adopts a pipeline-based ap-
proach in order to convert a document into a set of triples.
It comprises seven distinct stages, as shown in Figure 1.

Text cleaning: Text data is cleaned to manage special
characters such as hyphen and quotation marks and also break
sentences joined together with no space between them.

Text processing: The text is processed through tokenisa-
tion, POS tagging, entity recognition and dependency parsing
steps using SpaCy. The results are shown in Table I for
the following text: Ford Motor Company is an American

1https://www.clips.uantwerpen.be/conll2003/ner/
2https://catalog.ldc.upenn.edu/LDC2013T19

3https://www.nltk.org/
4https://spacy.io/

multinational automaker that has its main headquarters in
Dearborn, Michigan, a suburb of Detroit. The company was
founded by Henry Ford and incorporated on June 16, 1903.

Fig. 1. A diagram of the core components of our triple extraction system.

Chunking: Noun phrases (NPs) and verb phrases are chun-
ked, as shown in Table II. Noun chunks are phrases that have
a noun and the words describing the noun. For example, an
American multinational automaker and a suburb of Detroit.
We also implemented the chunking of action words, so that
verb phrases can contain verbs, particles and/or adverbs that
represent more meaningful relations between entities. For
example, was founded by and incorporated on.

Algorithm 1 Chunking of noun phrases and verb phrases

1: procedure CHUNKPHRASES(document)
for each sentence in document do
2:
(cid:46) Chunk noun phrases (NPs) and tag as ENTITY

3:
4:

5:
6:

7:
8:
9:

10:
11:

12:

chunk NPs
chunk (cid:48)((cid:48)+N P +(cid:48))(cid:48)
chunk N P +(cid:48) of (cid:48) + N P
chunk N P + N P

(cid:46) NP
(cid:46) (NP)
(cid:46) NP of NP
(cid:46) NP NP

(cid:46) Chunk verb phrases and tag as VERB

chunk V ERB + P ART
chunk V ERB + ADP
chunk ADP + V ERB
chunk P ART + V ERB
chunk V ERB + V ERB

(cid:46) verb + particle
(cid:46) verb + adpositions
(cid:46) adpositions + verb
(cid:46) particle + verb
(cid:46) verb + verb
(cid:46) Document with phrase chunks

return document

Coreference Resolution: A list of coreferenced items is
created using NeuralCoref5. For our example the following two
coreference items are identiﬁed: Ford Motor Company - its and
Ford Motor Company - The company. Coreference items are
resolved on the triples by replacing the original phrase with the
referred phrase for each item. For example, The company will
be replaced by Ford Motor Company. In the case of pronouns
such as its, her, his or their, we ignore the coreference items.

As we prefer main headquarters over Ford Motor Company
main headquarters, since main headquarters will be connected
to Ford Motor Company by the triples.

Triple Mapping: Triples are created from the sentences in
head, relation, tail format using Algorithm 2. First, head and
tail entities are extracted with their relations from the sentences
and creates a list of triples. Second, a graph is created from
those triples to uncover the relations among named entities
in separate sentences. Based on the relations of prepositions
such as in, on, at, more triples are created to provide more
links between named entities in the graph. Finally, the triples
created by these two steps are joined to make the full list of
triples for the given text.

Triple Filtering: To improve the quality of the triples, the
ﬁltering is performed to remove any triple with a stop word
as a head entity. The stop words include NLTK stop words,
names of days (Monday to Sunday) and names of months
(January to December).

Article Removal: To clean the entities we removed some
tokens including articles (e.g., a, an, the), possessive pronouns
(e.g., its, their) and demonstrative pronouns (e.g., that, these)
from the head and tails of each triple.

C. Visualisation system

Fig. 2. The additional stages performed by our system prior to visualisation
in order to display more detailed information about each triple.

Our visualisation system, which displays the results of the
triple extraction system, performs three additional techniques
in order to maximise the information displayed via our web
application. After the triples have been generated as per
Section I-B, they are post-processed and appended with the
degree/betweenness of the head and tail nodes, structured
relation(s) corresponding to the verb relation of each triple, and
the named entity classes of each head and tail. This process
is displayed in Figure 2. The source code of our visualisation
system is available on Github6.

The degree/betweenness calculation determines the degree
and betweenness centrality of the head and tail of each

5https://github.com/huggingface/neuralcoref

6https://github.com/Michael-Stewart-Webdev/text2kg-visualisation

GPE

GPE

NORP

Entity Type
ORG
ORG
ORG

Token Id
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37

Token
Ford
Motor
Company
is
an
American
multinational
automaker
that
has
its
main
headquarters
in
Dearborn
,
Michigan
,
a
suburb
of
Detroit
.
The
company
was
founded
by
Henry
Ford
and
incorporated
on
June
16
,
1903
.

Coarse Grained POS
PROPN
PROPN
PROPN
VERB
DET
ADJ
ADJ
NOUN
DET
VERB
DET
ADJ
NOUN
ADP
PROPN
PUNCT
PROPN
PUNCT
DET
NOUN
ADP
PROPN
PUNCT
DET
NOUN
VERB
VERB
ADP
PROPN
PROPN
CCONJ
VERB
ADP
PROPN
NUM
PUNCT
NUM
PUNCT
TABLE I
TEXT PROCESSING: TOKENISATION, POS TAGGING, ENTITY RECOGNITION, AND DEPENDENCY PARSING.

Dependency
compound
compound
nsubj
ROOT
det
amod
amod
attr
nsubj
relcl
poss
amod
dobj
prep
pobj
punct
appos
punct
det
dobj
prep
pobj
punct
det
nsubjpass
auxpass
ROOT
agent
compound
pobj
cc
conj
prep
pobj
nummod
punct
nummod
punct

POS
NNP
NNP
NNP
VBZ
DT
JJ
JJ
NN
WDT
VBZ
PRP
JJ
NN
IN
NNP
,
NNP
,
DT
NN
IN
NNP
.
DT
NN
VBD
VBN
IN
NNP
NNP
CC
VBD
IN
NNP
CD
,
CD
.

Start
0
5
11
19
22
25
34
48
58
63
67
71
76
89
92
100
102
110
112
114
121
124
131
133
137
145
149
157
160
166
171
175
188
191
196
198
200
204

IOB
B
I
I
O
O
B
O
O
O
O
O
O
O
O
B
O
B
O
O
O
O
B
O
O
O
O
O
O
B
I
O
O
O
B
I
I
I
O

End
3
9
17
20
23
32
46
56
61
65
69
74
87
90
99
100
109
110
112
119
122
130
131
135
143
147
155
158
164
169
173
186
189
194
197
198
203
204

DATE
DATE
DATE
DATE

PERSON
PERSON

GPE

Sent #
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1

Phrase #
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

Phrase
Ford Motor Company
is
an American multinational automaker
that
has
its main headquarters
in
Dearborn
,
Michigan
,
a suburb of Detroit
.
The company
was founded by
Henry Ford
and
incorporated on
June 16, 1903
.

TABLE II
CHUNKS OF NOUN PHRASES AND VERB PHRASES.

Type
ENTITY
VERB
ENTITY
DET
VERB
ENTITY
ADP
ENTITY
PUNCT
ENTITY
PUNCT
ENTITY
PUNCT
ENTITY
VERB
ENTITY
CCONJ
VERB
ENTITY
PUNCT

triple. In graph theory, degree refers to the number of edges
connected to a node [11]. For triples, this directly corresponds
to the number of triples in which each phrase appears. Be-

tweenness centrality, on the other hand, measures the extent to
which each vertex lies along the paths between other vertices.
Phrases that exert a high degree of inﬂuence over the ﬂow of
the graph, such as company names (“Ford”, “BYD”) tend to
have a high betweenness value and are hence more important
than other terms. Incorporating the degree and betweenness
calculations allows for this information to be conveyed in the
visualisation.

The relation extraction component maps the relation
phrase of each triple to one or more structured relation types.
This allows for the graph visualisation to display structured
relation types when desired by the user. Our system currently
maps each relation phrase to its corresponding SemEval [12]
relation. To accomplish this we use an attention-based bidirec-
tional Long Short-Term Memory (LSTM) model [13], which
maps a sequence of words padded with entity markers ((cid:104)e1(cid:105)
and (cid:104)e2(cid:105)) to a ﬁxed relation type. We create sequences using
the head and tail of each triple as (cid:104)e1(cid:105) and (cid:104)e2(cid:105) respectively,
and feed them into a pretrained model (trained on the SemEval
2010 Task 8 dataset) to obtain the corresponding SemEval
relation. SemEval contains nine types of semantic relations
and an additional type for other relations.

Finally, the named entity recognition (NER) component

2:

4:

6:

8:

12:

14:

16:

18:

Algorithm 2 Triple mapping algorithm

procedure GETTRIPLES(document)

for each sentence in document do

heads ← entities on the left side of r
tails ← entities on the right side of r
for each h in heads do

for each t in tails do

triples ← triples + [h, r, t]

10:

return triples

procedure EXTRACTTRIPLES(document)

relations ← verbs + prepositions + postpositions
for each r in relations do

(cid:46) Select relations such as showcased, has, in, to, during

(cid:46) Get the head entities for the relation r
(cid:46) Get the tail entities for the relation r

(cid:46) Add [head, relation, tail] to the list of triples
(cid:46) Return the list of triples

(cid:46) Extract triples from the document at the sentence level
triples ← GETTRIPLES(document)
(cid:46) Extract the triples at the document level using the graph shortest paths
G ← create graph(triples)
paths ← get shortest paths(G)
for each h, t in pairs of named entities do

if h and t connected by a path using ’in’, ’at’, ’on’ prepositions then

triples ← triples + [h,(cid:48) in(cid:48), t]

return triples

(cid:46) Build a graph from the triples using NetworkX package
(cid:46) Get all shortest paths between named entities

(cid:46) Add [head, ’in’, tail] to the list of triples
(cid:46) Return the full list of triples

B. Coreference Resolution

determines the semantic type of the head and tail of each
triple. We label each phrase with one of ﬁve types: PER,
ORG, LOC, MISC, and O, based upon the Wikipedia NER
scheme [14]. The raw text is ﬁrst labelled via SpaCy, yielding
a set of entities E. Each phrase (head and tail) in each triple
are then compared to every entity e ∈ E and assigned the
same label as e when the phrase is highly similar to e in
terms of edit distance. One caveat of performing the NER
after the triple extraction pipeline is that there is no contextual
information passed to the named entity recognition model.
However, applying NER immediately prior to visualisation
allows for a greater level of abstraction and ﬂexibility.

II. EVALUATION AND CONCLUSION

A. Triple Extraction

In order to evaluate the quality of our triple extraction
system, we consider the following two sentences: Ford Motor
Company is an American multinational automaker that has
its main headquarters in Dearborn, Michigan, a suburb of
Detroit. It was founded by Henry Ford and incorporated on
June 16, 1903.

Table III displays the subject, predicate, object

triples
from our triple extraction system and shows the additional
information provided by the visualisation system: the SemEval
relation type, the named entity types of the heads and tails,
and the degree and betweenness of each head and tail.

The triples show some of the notable strengths of our
model: the chunking component ensures useful phrases such as
“Ford Motor Company” and “Henry Ford” appear in multiple
triples. Furthermore, our system is able to extract useful triples
with “in” relations via the triple mapping component, such as
(Ford Motor Company, in, Dearborn).

Fig. 3. An example graph generated by our triple extraction system. The
nodes are coloured based on their named entity types. The node sizes are
based on their degree centrality values.

To highlight the effectiveness of our coreference resolution
component, we introduce an additional sentence to our exam-
ple so that it becomes:
Ford Motor Company is an American multinational au-
tomaker that has its main headquarters in Dearborn, Michi-
gan, a suburb of Detroit. It was founded by Henry Ford and
incorporated on June 16, 1903. The company is listed on the

New York Stock Exchange and it is controlled by the Ford
family.

Figure 3 shows the result of visualising the above sen-
tences via our web application. The underlined words
(Ford Motor Company in sentence 1, It in sentence 2 and
The company and it in sentence 3) represent the same entity
Ford Motor Company. The visualisation in Figure 3 clearly
shows Ford Motor Company as the shared entity between the
three sentences. The node Ford Motor Company appears the
biggest among all nodes in the graph, to represent the highest
degree centrality of the node in that graph.

C. Conclusion

In conclusion, our system uses a pipeline-based approach
to extract a set of triples from a given document. It offers a
simple and effective solution to the challenge of knowledge
graph construction from domain-speciﬁc text. It also provides
the facility to visualise useful information about each triple
such as the degree, betweenness, structured relation type(s),
and named entity types.

It is important to note that the graph edit distance metric
that is commonly used to automatically evaluate the quality
of triples is only capable of structural analysis. In order to
improve the metric it could be combined with meaningful
semantic measures such as those present
in the machine
translation and image captioning domains (e.g. SPICE [15]).
Another option would be to incorporate a simple sum of word
embeddings over each triple so that semantic information is
captured by the metric.

In future we plan to continue working on our end-to-end

deep learning-based triple extraction model.

III. EXTERNAL RESOURCES

Our

triple extraction system uses the aforementioned
NLTK [3] and SpaCy [4] at various stages throughout the
pipeline.

Our visualisation system is written in Flask7. The front-end
visualisations are written primarily in D3.js8. The attention-
based Bi-LSTM [13] for relation extraction is implemented
in Tensorﬂow [16], and trained on the SemEval 2010 Task
8 dataset [12]. The degree and betweenness calculations are
performed via NetworkX9.

ACKNOWLEDGEMENT

We would like to thank our team members Morgan Lewis
and Thomas Smoker, who are in the early stage of their PhD
candidatures, for their contributions on literature search.

7https://www.fullstackpython.com/ﬂask.html
8https://d3js.org/
9https://networkx.github.io

T
w
t
e
B

H
w
t
e
B

T
g
e
D

H
g
e
D

n
o
i
t
a
m
r
o
f
n
i

l
a
n
o
i
t
i
d
d
A

e
l
p
i
r
T

)
R
(

n
o
i
t
a
l
e
R

5
7

.

0

5
7

.

0

5
7

.

0

5
7

.

1

0
0

.

5
7

.

0

5
7

.

0

5
7

.

0

0

.

0

0

.

1

0

.

0

5
7
0

.

5
7
0

.

5
7

.

0

.

.

0
1
1

.

0
1
1

.

0
1
1

.

0
1
1

.

0
1
1

0

.

1
1

5
7
1

.

5
7
1

.

5
7

.

1

5
7

.

1

0

.

0

0
1

.

0
1

.

0

.

1

3

3

3

2

5

2

3

3

3

4

2

3

3

3

6

6

6

6

6

6

5

5

5

5

2

4

4

4

T
e
p
y
T

C
O
L

C
O
L

R
E
P

C
O
L

C
O
L

O

O

O

O

O

O

C
O
L

C
O
L

O

H
e
p
y
T

G
R
O

G
R
O

G
R
O

G
R
O

G
R
O

G
R
O

O

O

O

O

O

O

O

R
E
P

n
o
i
t
a
l
e
R

l
a
v
E
m
e
S

r
e
n
i
a
t
n
o
C

-
t
n
e
t
n
o
C

r
e
n
i
a
t
n
o
C

-
t
n
e
t
n
o
C

n
o
i
t
c
e
l
l
o
C

-
r
e
b
m
e
M

e
l
o
h
W

-
t
n
e
n
o
p
m
o
C

y
c
n
e
g
A

-
t
n
e
m
u
r
t
s
n
I

r
e
c
u
d
o
r
P
-
t
c
u
d
o
r
P

n
o
i
t
c
e
l
l
o
C

-
r
e
b
m
e
M

n
o
i
t
c
e
l
l
o
C

-
r
e
b
m
e
M

n
o
i
t
c
e
l
l
o
C

-
r
e
b
m
e
M

e
l
o
h
W

-
t
n
e
n
o
p
m
o
C

r
e
n
i
a
t
n
o
C

-
t
n
e
t
n
o
C

r
e
n
i
a
t
n
o
C

-
t
n
e
t
n
o
C

n
o
i
t
c
e
l
l
o
C

-
r
e
b
m
e
M

t
c
e
f
f
E
-
e
s
u
a
C

r
e
k
a
m
o
t
u
a

l
a
n
o
i
t
a
n
i
t
l
u
m
n
a
c
i
r
e
m
A

d
r
o
F

y
r
n
e
H

y
b

d
e
d
n
u
o
f

s
a
w

3
0
9
1

,
6
1

e
n
u
J

n
o

d
e
t
a
r
o
p
r
o
c
n
i

t
i
o
r
t
e
D

f
o

b
r
u
b
u
s

3
0
9
1

,
6
1

e
n
u
J

n
r
o
b
r
a
e
D

n
a
g
i
h
c
i

M

)
T
(

l
i
a
T

s
r
e
t
r
a
u
q
d
a
e
h

n
i
a
m

t
i
o
r
t
e
D

f
o

b
r
u
b
u
s

n
r
o
b
r
a
e
D

n
a
g
i
h
c
i

M

t
i
o
r
t
e
D

f
o

b
r
u
b
u
s

n
r
o
b
r
a
e
D

n
a
g
i
h
c
i

M

n
i

n
i

n
i

n
i

s
i

n
i

n
i

n
i

n
i

n
i

n
i

s
a
h

r
e
k
a
m
o
t
u
a

l
a
n
o
i
t
a
n
i
t
l
u
m
n
a
c
i
r
e
m
A

r
e
k
a
m
o
t
u
a

l
a
n
o
i
t
a
n
i
t
l
u
m
n
a
c
i
r
e
m
A

r
e
k
a
m
o
t
u
a

l
a
n
o
i
t
a
n
i
t
l
u
m
n
a
c
i
r
e
m
A

r
e
k
a
m
o
t
u
a

l
a
n
o
i
t
a
n
i
t
l
u
m
n
a
c
i
r
e
m
A

s
r
e
t
r
a
u
q
d
a
e
h

s
r
e
t
r
a
u
q
d
a
e
h

s
r
e
t
r
a
u
q
d
a
e
h

n
i
a
m

n
i
a
m

n
i
a
m

d
r
o
F

y
r
n
e
H

y
n
a
p
m
o
C

r
o
t
o
M
d
r
o
F

y
n
a
p
m
o
C

r
o
t
o
M
d
r
o
F

y
n
a
p
m
o
C

r
o
t
o
M
d
r
o
F

y
n
a
p
m
o
C

r
o
t
o
M
d
r
o
F

y
n
a
p
m
o
C

r
o
t
o
M
d
r
o
F

y
n
a
p
m
o
C

r
o
t
o
M
d
r
o
F

)
H
(

d
a
e
H

I
I
I

E
L
B
A
T

M
E
T
S
Y
S

N
O

I
T
A
S
I
L
A
U
S
I

V

R
U
O

A

I

V

E
L
P
I

R
T

H
C
A
E

O
T

D
E
D
N
E
P
P
A

N
O

I
T
A
M
R
O
F
N

I

L
A
N
O

I
T
I

D
D
A

E
H
T

H
T
I
W
G
N
O
L
A

M
E
T
S
Y
S

N
O

I
T
C
A
R
T
X
E

E
L
P
I

R
T

R
U
O

Y
B

D
E
C
U
D
O
R
P

S
E
L
P
I

R
T

,

E
L
P
M
A
X
E

REFERENCES

[1] E. F. T. K. Sang and F. De Meulder, “Introduction to the conll-2003
shared task: Language-independent named entity recognition,” CoNLL-
2003, 2003.

[2] R. Weischedel, M. Palmer, M. Marcus, E. Hovy, S. Pradhan,
L. Ramshaw, N. Xue, A. Taylor, J. Kaufman, M. Franchini et al.,
“Ontonotes release 5.0 ldc2013t19,” Linguistic Data Consortium,
Philadelphia, PA, 2013.

[3] S. Bird, E. Klein, and E. Loper, Natural language processing with
” O’Reilly

Python: analyzing text with the natural language toolkit.
Media, Inc.”, 2009.

[4] M. Honnibal, “Spacy,” 2017. [Online]. Available: https://explosion.ai/

blog/introducing-spacy

[5] J. R. Finkel, T. Grenager, and C. Manning, “Incorporating non-local
information into information extraction systems by gibbs sampling,” in
Proceedings of the 43rd annual meeting on association for computa-
tional linguistics. Association for Computational Linguistics, 2005,
pp. 363–370.

[6] M. Gardner, J. Grus, M. Neumann, O. Tafjord, P. Dasigi, N. F. Liu,
M. Peters, M. Schmitz, and L. S. Zettlemoyer, “Allennlp: A deep
semantic natural language processing platform,” 2017.

[7] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,
and L. Zettlemoyer, “Deep contextualized word representations,” arXiv
preprint arXiv:1802.05365, 2018.

[8] C. Niklaus, M. Cetto, A. Freitas, and S. Handschuh, “A survey on open

information extraction,” arXiv preprint arXiv:1806.05599, 2018.

[9] L. Cui, F. Wei, and M. Zhou, “Neural open information extraction,”

arXiv preprint arXiv:1805.04270, 2018.

[10] N. Kertkeidkachorn and R. Ichise, “T2kg: An end-to-end system for
creating knowledge graph from unstructured text,” in Workshops at the
Thirty-First AAAI Conference on Artiﬁcial Intelligence, 2017.

[11] R. Diestel, “Graph theory. 2005,” Grad. Texts in Math, vol. 101, 2005.
[12] I. Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov, D. ´O S´eaghdha, S. Pad´o,
M. Pennacchiotti, L. Romano, and S. Szpakowicz, “Semeval-2010 task
8: Multi-way classiﬁcation of semantic relations between pairs of nomi-
nals,” in Proceedings of the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions. Association for Computational
Linguistics, 2009, pp. 94–99.

[13] P. Zhou, W. Shi, J. Tian, Z. Qi, B. Li, H. Hao, and B. Xu, “Attention-
based bidirectional long short-term memory networks for relation classi-
ﬁcation,” in Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers), 2016, pp. 207–
212.

[14] J. Nothman, N. Ringland, W. Radford, T. Murphy, and J. R. Curran,
“Learning multilingual named entity recognition from wikipedia,” Arti-
ﬁcial Intelligence, vol. 194, pp. 151–175, 2013.

[15] P. Anderson, B. Fernando, M. Johnson, and S. Gould, “Spice: Semantic
propositional image caption evaluation,” in European Conference on
Computer Vision. Springer, 2016, pp. 382–398.

[16] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for large-
scale machine learning,” in 12th {USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 16), 2016, pp. 265–283.

