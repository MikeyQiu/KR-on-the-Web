7
1
0
2
 
c
e
D
 
7
 
 
]

G
L
.
s
c
[
 
 
4
v
0
8
0
4
.
6
0
3
1
:
v
i
X
r
a

Parallel Coordinate Descent Newton Method
for Eﬃcient L1-Regularized Loss Minimization∗

An Bian
ybian@inf.ethz.ch
ETH Zurich

Xiong Li
li.xiong@gmail.com
CNCERT

Yuncai Liu
whomliu@sjtu.edu.cn
Shanghai Jiao Tong University

Ming-Hsuan Yang
mhyang@ucmerced.edu
University of California, Merced

Abstract

The recent years have witnessed advances in parallel algorithms for large scale
optimization problems. Notwithstanding the demonstrated success, existing algo-
rithms that parallelize over features are usually limited by divergence issues under
high parallelism or require data preprocessing to alleviate these problems. In this
work, we propose a Parallel Coordinate Descent algorithm using approximate New-
ton steps (PCDN) that is guaranteed to converge globally without data prepro-
cessing. The key component of the PCDN algorithm is the high-dimensional line
search, which guarantees the global convergence with high parallelism. The PCDN
algorithm randomly partitions the feature set into b subsets/bundles of size P , and
sequentially processes each bundle by ﬁrst computing the descent directions for
each feature in parallel and then conducting P -dimensional line search to compute
the step size. We show that (i) the PCDN algorithm is guaranteed to converge
globally despite increasing parallelism; (ii) the PCDN algorithm converges to the
speciﬁed accuracy (cid:15) within the limited iteration number of T(cid:15), and T(cid:15) decreases with
increasing parallelism. In addition, the data transfer and synchronization cost of
the P -dimensional line search can be minimized by maintaining intermediate quan-
tities. For concreteness, the proposed PCDN algorithm is applied to L1-regularized
logistic regression and L1-regularized L2-loss SVM problems. Experimental eval-
uations on seven benchmark datasets show that the proposed PCDN algorithm
exploits parallelism well and outperforms the state-of-the-art methods.

1. Introduction

High dimensional L1-minimization problems arise in a wide range of applications in-
cluding sparse logistic regression [15], L1-regularized support vector machine (SVM)
classiﬁcation [4], image coding [11], and face recognition [23]. To solve L1-optimization

∗Source code is available at https://github.com/bianan/ParallelCDN

problems eﬃciently, several algorithms based on coordinate gradient descent (CGD) [22],
stochastic gradient [21], interior point [8] and trust region [12] have been devel-
oped, among which the Coordinate Descent Newton (CDN) [24] and improved GLM-
NET [25] methods have demonstrated promising results for L1-regularized linear op-
timization problems.

Within the L1-optimization framework, large datasets with high dimensional fea-
tures entail scalable and eﬃcient parallel algorithms. Several methods perform par-
allelization over samples [9; 16; 26; 27] although usually there are more features than
samples in L1-regularized problems. Richt´arik et al. [17] show that randomized coor-
dinate descent methods can be accelerated by parallelization for solving Lasso prob-
lems, and the work is further extended to distributed settings [14; 18]. In addition,
Bradley et al. [2] propose the Shotgun CDN (SCDN) method for L1-regularized lo-
gistic regression by directly parallelizing the updates of features based on the CDN
algorithm [24]. However, the SCDN method is not guaranteed to converge when the
number of updated features in parallel is greater than a threshold, and thereby limits
its ability of exploiting high parallelism. While this problem can be alleviated by
preprocessing samples (e.g., feature clustering) to achieve higher parallelism [20], it
requires additional computational overhead. The Accelerated Shotgun method [13]
is a ﬁrst-order algorithm without backtrack line search which has fast convergence.
However, it can only deal with the objective functions without regularization terms.
Scherrer et al. [19] present a generic framework for parallel coordinate descent meth-
ods which includes Shotgun, Greedy, Thread-Greedy and Coloring. Their empirical
convergence and scalability tests do not favor any of these methods over the others,
and no theoretical analysis is presented for the general framework.

In [1] Bian et al. present a high dimensional line search algorithm to ensure global
convergence while performing parallel coordinate updates for L1-regularized logistic
regression problem. While this method performs well, no analysis of convergence rate
is presented. In this work, by further exploring the idea, we propose a generalized
Parallel Coordinate Descent method using approximate Newton steps (PCDN) for
generic L1-optimization problems, and present thorough theoretical analysis on the
proposed method.

The contributions and novelty of this work are summarized as follows. We present
theoretical analysis on the upper bound of the expected line search step in each
iteration. We analyze the iteration complexity of the proposed PCDN algorithm
and show that, for any bundle size P (i.e., parallelism), it is guaranteed to converge
to a speciﬁed accuracy (cid:15) within T(cid:15) iterations. The iteration number T(cid:15) decreases
with the increasing of parallelism (bundle size P ).
In addition, we show that in
our implementation, the P -dimensional line search does not need to access all the
training data on each thread and the synchronization cost of the P -dimensional line
search can be minimized. Extensive experiments on L1-regularized classiﬁcation and
regression problems with real-world datasets demonstrate that the proposed PCDN

2

algorithm is a highly parallelized approach with guaranteed global convergence and
fast convergence rate.

2. L1-Regularized Loss Minimization
For ease of presentation, we summarize the mathematical notations in Table 1.

Table 1: Mathematical notations in this work.
s, n # training samples and # features
i, j
t
k
ej

sample index and feature index
cumulative inner iteration index
outer iteration index
indicator vector
2-norm and 1-norm

,

1

N {

1, 2, ..., n

, feature index set
}

feature index subset or “bundle”
bundle size

B ⊆ N
P =
|B|
Rn unknown vector of model weights
w
∈
Rs×n design matrix, whose i-th row is xi

(cid:107) · (cid:107)

(cid:107) · (cid:107)

X

∈
(xi, yi)

sample-label pair

Consider an unconstrained L1-regularized minimization problem over a training

set

(xi, yi)
}

{

s
i=1 with the following general form:

min
w∈Rn

F (w) := min
w∈Rn

c

ϕ(w; xi, yi) +

w

(cid:107)

1

(cid:107)

s
(cid:88)

i=1
L(w) +

= min
w∈Rn

w

1,

(cid:107)

(cid:107)

where L(w) := c (cid:80)s
i=1 ϕ(w; xi, yi) is the overall loss function; ϕ(w; xi, yi) is a convex
and non-negative loss function; and c > 0 is the regularization parameter. For L1-
regularized logistic regression, the loss function is,

ϕlog(w; xi, yi) = log(1 + e−yiw(cid:62)xi),

and for L1-regularized L2-loss SVM, the loss function is

ϕsvm(w; xi, yi) = max(0, 1

yiw(cid:62)xi)2.

−

A number of algorithms have been proposed to solve these problems. We discuss
two related solvers based on Coordinate Descent Newton [24] and its parallel variant,
Shotgun CDN [2], in this section.

(1)

(2)

(3)

3

2.1 Coordinate Descent Newton

Based on the Coordinate Gradient Descent (CGD) method [22], Yuan et al. [24]
demonstrate that the CDN method is eﬃcient for solving large scale L1-regularized
minimization. The overall procedure is summarized in Algorithm 1. Given the current

for all j

Algorithm 1: CDN [24]
1 initialize w0 = 0n×1;
do
2 for k = 0, 1, 2,
do
3
j = d(wk,j; j) by solving (4);
j ej) by solving (6);

compute dk
ﬁnd αk,j = α(wk,j, dk
// 1-dimensional line search
wk,j + αk,jdk
wk,j+1

· · ·
∈ N

4

5

6

j ej;

←

(4)

(5)

(6)

(7)

model w, for the selected feature j
where2

∈ N

, w is updated in the direction dj = d(w; j)ej,

d(w; j) := arg min

jL(w)d +

d {∇

1
2∇

jjL(w)d2 +
2

wj + d
|

|}

,

which has the following closed form solution,

d(w; j) =






∇j L(w)+1
∇2
jj L(w)
∇j L(w)−1
∇2
jj L(w)
wj

−

−

−

if

jL(w)

∇

if

jL(w)
otherwise.

∇

2
jjL(w)wj,
≤∇
2
jjL(w)wj,
≥∇

The Armijo rule [3] is used to determine the step size. Let q be the line search step
index, the step size α = α(w, d) is determined by

α(w, d) := max

q=0,1,···{

βq

F (w + βqd)
|

−

F (w)

βqσ∆
}

,

≤

∈

∈

where β

(0, 1), σ

(0, 1), and

∈
∆ :=

∇

L(w)(cid:62)d + γd(cid:62)Hd +

w + d
(cid:107)

1
(cid:107)

w

1,

− (cid:107)

(cid:107)

where γ

[0, 1) and H = diag(

2L(w)).

∇

This rule requires function evaluations in each line search step, straightforward
implementation would need to access the whole design matrix X for each function
evaluation, which is intractable for parallel system with limited memory bandwidth.
We will show in Section 3.1 that, this problem can be solved using implementation
technique of retaining intermediate quantities, which also makes it possible to apply
high-dimensional line search to our parallel algorithm.

2. For L(w) that is not C 2-smooth, e.g. the L1-regularized L2-loss SVM, use the generalized Hessian

[24], which is denoted by

2L(w) with a little abuse of notation in this work.

∇

4

Algorithm 2: Shotgun CDN for logistic regression [2]
1 choose ¯P
∈
2 while not converged do
3

[1, n/ρ + 1], initialize w = 0n×1;

in parallel on ¯P processors;

4

5

6

7

8

choose j
uniformly at random;
obtain dj = d(w; j) by solving (4);
ﬁnd αj = α(w, djej) by solving (6);

∈ N

w

←

w + αjdjej;

// 1-dimensional line search

2.2 SCDN for L1-Regularized Logistic Regression

The SCDN method [2] is developed to solve L1-regularized logistic regression prob-
lems. For presentation clarity, we summarize the main steps of the SCDN method
in Algorithm 2. This method ﬁrst determines the parallelism (number of parallel
updates) ¯P , and then in each iteration updates the randomly picked ¯P features in
parallel, where each feature update corresponds to one iteration in the inner loop of
the CDN method (see Algorithm 1). However, the parallel updates for ¯P features
increase the risk of divergence due to feature correlations. Bradley et al. [2] provide a
problem-speciﬁc measure for the parallelization potential of the SCDN method based
on the spectral radius ρ of X(cid:62)X. With this measure, an upper bound ¯P
n/ρ + 1,
is given to achieve speedups linear in ¯P . However, ρ can be very large for most large
scale datasets (e.g., ρ = 20, 228, 800 for the gisette dataset with n = 5000 without
column-wise normalization) and thus limits the parallelizability of SCDN. Clearly it
is of great interest to develop algorithms with strong convergence guarantees under
high parallelism for large scale L1-regularized minimization problems.

≤

3. The Proposed PCDN Algorithm

As described in Section 2.2, the SCDN method is not guaranteed to converge when
the number of features to be updated in parallel is greater than a threshold, i.e.,
¯P > n/ρ + 1. To exploit higher parallelism, we propose a coordinate descent algo-
rithm using multidimensional approximate Newton steps and high dimensional line
search. When computing the multidimensional Newton descent direction of the sec-
ond order approximation subproblem, we set the oﬀ-diagonal elements of the Hessian
to zeros, such that we can compute the multidimensional approximate Newton descent
direction by computing the corresponding one-dimensional Newton descent directions
in parallel.

The main steps of the proposed PCDN method are summarized in Algorithm 3.
In the k-th iteration of the outer loop, we randomly partition the feature index set

5

4

5

6

7

8

9

N

Algorithm 3: PCDN algorithm
1 choose P
2 for k = 0, 1, 2,
kb+1,
3

do
,

(k+1)b−1

∈

[1, n], initialize w0 = 0n×1;

} ←

· · ·
· · ·

kb,
{B
B
to (8);
for t = kb, kb + 1,

B

0n×1;

dt
←
for all j

, (k + 1)b

1 do

· · ·

−

∈ B
obtain dt

t in parallel do
j = d(wt; j) by solving (4);

random disjoint partitions of

according

N

ﬁnd αt = α(wt, dt) by solving (6);
// P -dimensional line search (see Algorithm 4 for detail)
wt+1

wt + αtdt;

←

into b disjoint subsets in a Gauss-Seidel manner,

kb

kb+1

=

N

B

∪ B

∪ · · · ∪ B

(k+1)b−1, k = 0, 1, 2, . . .

(8)

n
P (cid:101)

denotes a subset, i.e., a bundle, in this work; P =
is the number of bundles partitioned from

is the bundle size;
where
B
. The PCDN algorithm
and b =
sequentially processes each bundle by computing the approximate Newton descent
direction in each iteration of the inner loop. In the t-th iteration3, the P -dimensional
approximate Newton descent direction is computed by,

|B|

N

(cid:100)

d(w;

t)(cid:44) arg min
d

B

∇

BtL(w)d+

dTHBtd+
(cid:107)

wBt+d

(cid:26)

(cid:27)
1
(cid:107)

,

where we only use the diagonal elements of the Hessian, i.e., HBt (cid:44) diag(
2
BtL(w)),
∇
to make the computing of one-dimensional Newton descent direction independent of
each other and enable the parallelization. That is,

d(w;

t)= arg min

BtL(w)d +

dT diag(

2
BtL(w))d

B

d {∇

∇

jL(w)d +

d ∇

1
jjL(w)d2 +
2
2 ∇

wj + d
|
|

(cid:27)

ej

+

1

(cid:107)
(cid:88)

wBt + d
(cid:26)

(cid:107)
}
arg min

=

=

j∈Bt
(cid:88)

j∈Bt

d(w; j)ej,

(9)

3. Note that t is the cumulative iteration index, and refers to the inner loop in the following

discussion of PCDN.

1
2

1
2

6

B

In the t-th iteration, we ﬁrst
where (9) is from the deﬁnition of d(w; j) in (4).
t in
compute the one-dimensional descent directions dt
t).
parallel, which constitutes the P -dimensional descent direction dt (dt
We then use the P -dimensional Armijo line search (step 8) to compute the step size
αt of the bundle along dt, and update the model for the features in

j (step 7) for P features in
j
∀

t (step 9).

B
(cid:54)∈ B

j = 0,

The PCDN algorithm is diﬀerent from the SCDN method in three aspects: (1)
PCDN randomly partitions the feature set into bundles and performs parallelization
for features of each bundle, while SCDN does not; (2) PCDN performs P -dimensional
line search for a bundle of features while SCDN performs 1-dimensional line search for
each feature; (3) PCDN is guaranteed to reach global convergence for high parallelism
whereas SCDN is not.

The P -dimensional line search is the key procedure that guarantees the conver-
gence of PCDN. With P -dimensional line search, the objective function F (w) in (1)
t (See Lemma 1(c) of Section 4). In
is ensured to be non-increasing for any bundle
general, the P -dimensional line search tends to have a large step size if the features
in

t are less correlated, and a small step size otherwise.
B
The bundle size P controls the ratio between computation and data communica-
tion. From Algorithm 3, in each outer iteration, it updates n features (computation)
times high-dimensional line search (which requires synchroniza-
while conducts
tion and communication). The bundle size P aﬀects convergence rate (See Theorem
2) as well, and the choice of P is discussed at length in Section 5.1.

n
P (cid:101)

B

(cid:100)

The PCDN algorithm can better exploit parallelism than the SCDN method. In
step 7 of Algorithm 3, the descent direction for P features can be computed in parallel
on P threads. We show in Section 4 that the proposed PCDN algorithm is guaranteed
[1, n]. Therefore, the bundle size P which
to reach global convergence, for any P
∈
measures the parallelism can be large when the number of features n is large.
In
contrast, for SCDN, the number of parallel updates ¯P is no more than n/ρ + 1 [2].

3.1 PCDN on Multicore

We use the technique of retaining intermediate quantities, in a way similar to the
that in [6], by which two crucial implementation issues are addressed simultaneously.
First, due to limited memory bandwidth, we lower data transfer by ensuring that one
core is only needed to access data of one feature. Second, we lower synchronization
cost of the P -dimensional line search such that the PCDN algorithm only requires
one implicit barrier synchronization in each iteration.
In our implementation, the
line search procedure (6) does not require direct function value evaluation and thus
avoids accessing all the training data on each core. Namely, the core processing on
the j-th feature only needs to access the data related to the j-th feature (i.e., the j-th
column xj of the design matrix X).

Without loss of generality, let us take logistic regression for instance. We retain
intermediate quantities d(cid:62)xi and ew(cid:62)xi (i = 1,
, s). For the Armijo line search
(summarized in Algorithm 4), we use the descent condition expressed by intermediate

· · ·

7

Algorithm 4: Eﬃcient high dimensional line Search (logistic regression here for
example)
1 compute d(cid:62)xi, i = 1,
do
2 for q = 0, 1, 2,
3

· · ·
if (10) is satisﬁed then

, s ;

// parallel

· · ·

4

5

6

7

8

9

w + βqd;

w
←
ew(cid:62)xi
break;

←

ew(cid:62)xieβqd(cid:62)xi ;

else

∆
←
d(cid:62)xi

β∆;

←

βd(cid:62)xi, i = 1,

, s ;

· · ·

quantities in the following equation,

// parallel

// parallel

(10)

(11)

F (w + βqd)

s
(cid:88)

c(

log(

i=1
σβq(

≤

∇

w+βqd

F (w) =
−
e(w+βqd)(cid:62)xi + 1
e(w+βqd)(cid:62)xi + eβqd(cid:62)xi

(cid:107)

w

1+

1
− (cid:107)
(cid:107)
(cid:107)
) + βq(cid:88)
d(cid:62)xi)

i:yi=−1

L(w)(cid:62)d + γd(cid:62)Hd +

w + d
(cid:107)

1
(cid:107)

w

1)

− (cid:107)

(cid:107)

which is equivalent to the descent condition in (6). More speciﬁcally, in Algorithm 3,
the core processing the j-th feature only needs to access xj twice in the t-th iteration.
For the ﬁrst time at step 7 of Algorithm 3, xj is accessed and the retained ew(cid:62)xi

is used to compute the j-th gradient and Hessian,

jL(w) = c

(τ (yiw(cid:62)xi)

1)yixij,

∇

∇

s
(cid:88)

i=1
s
(cid:88)

i=1

−

−

2
jjL(w) = c

τ (yiw(cid:62)xi)(1

τ (yiw(cid:62)xi))x2
ij,

where τ (s) = 1
1+e−s . They are then used to compute d(w; j) in (4). For the second
time at step 8 of Algorithm 3, xj is accessed and d is used to update d(cid:62)xi, which is
then used with ew(cid:62)xi to check the descent condition in (10).

The proposed PCDN algorithm requires much less time for each outer iteration

than the CDN method, which is analyzed in Section B of the appendix.

4. Convergence of PCDN

In this section, we analyze the convergence of the proposed PCDN algorithm from
three aspects: convergence of P -dimensional line search, global convergence and con-
vergence rate. For presentation clarity, we ﬁrst discuss the main results and present

8

all the proofs in the appendix. Before analyzing the convergence of PCDN, we present
the following lemma.

dt

wt
{

,
}

,
}

Lemma 1. Let
rithm 3, ¯λ(
minimum element of (X(cid:62)X)jj where j
(a) EBt[¯λ(
B

αt
{

B

{

}

t) be the maximum element of (X(cid:62)X)jj where j

as well as

t

{B

}

be sequences generated by Algo-
t, and λk be the k-th

. The following results hold.

∈ B

∈ N
t)] is monotonically increasing with respect to P ; EBt[¯λ(
t)] is constant
B
= λn); EBt[¯λ(
t)]/P is mono-
B

with respect to P if λi is constant (i.e., λ1 =
tonically decreasing with respect to P .

· · ·

(b) For L1-regularized logistic regression in (2) and L1-regularized L2-loss SVMs
in (3), the diagonal elements of the (generalized) Hessian of the loss function
L(w) have positive lower bound h and upper bound ¯h, and the upper bound only
depends on the design matrix X. That is,

j

,

2
jjL(w)

θc(X(cid:62)X)jj = θc

x2
ij,

∇

≤

∀

∈ N

s
(cid:88)

i=1

where θ = 1

2
jjL(w)
≤ ∇
4 for logistic regression and θ = 2 for L2-loss SVM.

0 < h

¯h = θc¯λ(

N

≤

),

(c) The objective
rule satisﬁes

F (wt)
}

{

is non-increasing and ∆t (7) in the Armijo line search

∆t

(γ

1)dt(cid:62)Htdt,

≤
F (wt + αtdt)

−
F (wt)

−

σαt∆t

0.

≤

≤

We note that Lemma 1(a) is used to analyze the iteration number T(cid:15) given the ex-
pected accuracy (cid:15), Lemma 1(b) is used to prove Theorem 1 and 2. Lemma 1(c)
ensures the descent of the objective theoretically and gives an upper bound for ∆t in
the Armijo line search, and is used to prove Theorem 1 and 2. Note that the upper
1)(dt)(cid:62)Htdt is only related to the second order measurement.
bound (γ

be a sequence
Theorem 1 (Convergence of P -dimensional line search). Let
generated by Algorithm 3, and ¯λ(
. The P-dimensional
line search converges in ﬁnite steps, and the expected line search step number in each
iteration is bounded by

(X(cid:62)X)jj
t) = max
{

∈ B

{B

B

}

}

j

|

t

t

−

(12)

(13)

(14)

(15)

(16)

E[qt]

1 + logβ−1

≤

2h(1

θc
σ + σγ)
−
logβ−1 P + logβ−1 E[¯λ(
B

t)],

+

1
2

where the expectation is with respect to the random choice of
(0, 1) and γ
step number in the t-th iteration; β
of the Armijo rule (6); θ and h is in Lemma 1(b).

(0, 1), σ

∈

∈

B
∈

t; qt is the line search
[0, 1) are parameters

9

As E[¯λ(

B

t)] is monotonically increasing with respect to P (Lemma 1(a)), Theo-
rem 1 dictates that the upper bound of E[qt] (the expected line search step number
in each iteration) increases with the bundle size P . Since more line search steps lead
to smaller step size (α), Theorem 1 is consistent with the intuition that smaller step
size is used when features inside a bundle are more correlated.

Global convergence of PCDN. In Section A.5 of the appendix, we prove the global
convergence of PCDN by connecting it to the general framework in [22]. By proving
that all assumptions are satisﬁed, we show that, assuming that
is the sequence
is an optimum. This analysis
generated by Algorithm 3, then any limit point of
guarantees that the PCDN algorithm converges globally for any bundle size P
[1, n]
(i.e., without regard to the level of parallelism).

wt
{

wt

∈

}

{

}

t

Theorem 2 (Convergence rate of PCDN). Assume w∗ minimize (1);
j

(X(cid:62)X)jj
{B
{
wT be the output of Algorithm 3 after T + 1 iterations. Then,

be sequences generated by Algorithm 3; ¯λ(

t) := max

B

}

|

wt
t

{
∈ B

}
}

and
and

E[F (wT ]
−
nE[¯λ(
t)]
B
P (T + 1) ·

F (w∗)
(cid:20)
θc
2ξ

(cid:107)

≤
w∗

(cid:107)

2 +

(cid:21)

,

F (0)

σ(1

γ)h

−

(0, 1) and
[0, 1) are parameters in the Armijo rule (6). In addition, θ as well as h (positive
t)] is determined by

where the expectation is computed with respect to random choice of
γ
jjL(w)) are given in Lemma 1(b), and E[¯λ(
lower bound of
B
the bundle size P and design matrix X; ξ is a positive constant.

∇

∈

∈

B

2

t; σ

Based on Theorem 2, we obtain the upper bound (T up

(cid:15) ) of the iteration number

T(cid:15) satisfying a speciﬁed accuracy (cid:15):

t)]

nE[¯λ(
P (cid:15)

B

T(cid:15)

≤

:= T up

(cid:15) ∝

(cid:20)

θc
2ξ

t)]

(cid:107)

,

·
E[¯λ(
B
P (cid:15)

w∗

2 +
(cid:107)

F (0)

(cid:21)

σ(1

γ)h

−

(17)

which means that the proposed PCDN algorithm achieves speedups linear in the
bundle size P compared to the CDN method if E[¯λ(
t)] remains constant4. In gen-
eral, E[¯λ(
t)] increases with respect to P (from Lemma 1(a)), and thus makes the
speedup sublinear. Furthermore, since E[¯λ(
t)]/P decreases with respect to P from
Lemma 1(a), T up
(cid:15) decreases with respect to P , and so does T(cid:15). Thus the PCDN algo-
rithm requires fewer iterations with larger bundle size P to converge to (cid:15) accuracy.

B

B

B

To verify the upper bound T up

(17) of the iteration number T(cid:15) for a given accuracy
(cid:15), we set (cid:15) = 10−3 and show the iteration number T(cid:15) as a function of the bundle size

(cid:15)

4. If we perform feature-wise normalization over the training data X to ensure λ1 = λ2 =

then E[¯λ(

t)] remains constant according to Lemma 1(a).

= λn,

· · ·

B

10

(a) real-sim, Logistic regression

(b) a9a, L2-loss SVM classiﬁcation

Figure 1: E[¯λ(

t)]/P and T(cid:15) as a function of bundle size P .

B

B

t)]/P instead of T up

P in Figure 1, where two document datasets, a9a and real-sim (See Section 5.1 for
t)]/P , we plot
details about the datasets) are used. Since T up
E[¯λ(
in Figure 1. The results match the upper bound in (17):
for given (cid:15), T(cid:15) (solid green lines) is positively correlated with E[¯λ(
t)]/P (dotted blue
B
lines).
In addition, T(cid:15) decreases with respect to P . These results show that with
larger bundle size P , fewer iterations are needed by the PCDN algorithm to converge
to (cid:15) accuracy.

is proportional to E[¯λ(

B

(cid:15)

(cid:15)

5. Experiments

In this section we present experimental results of the proposed PCDN algorithm,
with comparisons to the state-of-the-art methods on L1-regularized loss minimization
problems using several benchmark datasets.

5.1 Experimental Setup

Datasets. Seven benchmark datasets5 are used in our experiments and the char-
acteristics are summarized in Table 2. The news20, rcv1, a9a and real-sim datasets
consist of document data points that are normalized to unit vectors. The a9a dataset
is from UCI data repository, and the gisette set consists of handwriting digit data
points from the NIPS 2003 feature selection challenge where features are linearly
scaled to the [
1, 1] interval. The kdda dataset has been used for the KDD Cup 2010
data mining competition. The webspam dataset is the collection of Web pages that
are created to manipulate search engines and deceive Web users.

−

Bundle Size Choice. For each dataset, the optimal bundle size P ∗ under which
the PCDN algorithm achieves minimum runtime is determined as follows. For Al-

5. The datasets are available at http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets.

11

Table 2: Summary of datasets:.

is The number of non-zero elements (NNZs) in training
data is denoted by “train NNZ”; the average number of NNZs in the data cor-
responding to each feature is “NNZ/feature” denotes; ; “spa.” means train data
sparsity, which is the ratio of zero elements in X; “c∗ SVM” and “c∗ logistic” de-
note the best regularization parameter c∗ for L2-loss SVM and logistic regression,
respectively, which are determined according to [24].

Dataset
a9a
real-sim
news20
gisette
rcv1
kdda
webspam

s
26,049
57,848
15,997
6,000
541,920
8,407,752
280,000

n
123
20,958
1,355,191
5,000
47,236
20,216,830
16,609,143

train NNZ NNZ/feature
2,937
142
5
5,946
839
15
63

361,278
2,968,110
7,281,110
29,729,997
39,625,144
305,613,510
1,043,724,776

spa./% c∗ SVM c∗ logistic
2.0
0.5
4.0
1.0
64.0
64.0
0.25
0.25
4.0
1.0
1.0
1.0
64.0
64.0

88.72
99.76
99.97
0.9
99.85
99.99
99.9775

gorithm 3, the expected runtime of the t-th inner iteration of PCDN time(t) can be
approximated by

E[time(t)]

(P/#thread)

tdc + E[qt]

tls,

(18)

≈

·
t; #thread is the number
where the expectation is based on a random choice of
of threads used by PCDN and ﬁxed to be 23 in our experiments; tdc is the time for
computing the descent direction (step 7 in Algorithm 3); tls is the time for a step of
P -dimensional line search, which is approximately constant with varying P (as shown
in Section B of the appendix).

B

·

Table 3: Optimal bundle size P ∗ for each dataset. #thread = 23. The second row shows

P ∗ for logistic regression, the third row shows P ∗ for L2-loss SVM.

a9a
123
85

real-sim news20

1250
500

400
150

gisette
20
15

rcv1
1600
350

kdda webspam
29500
95000

31750
86000

As E[qt] increases with respect to the bundle size P (based on Theorem 1),
In addition, as the PCDN
E[time(t)] increases with respect to P based on (18).
algorithm requires fewer iterations for larger P to converge to (cid:15) accuracy (from (17)
in Section 4), it is essential to make a trade-oﬀ between the increasing runtime per
iteration E[time(t)] and the decreasing iteration number T(cid:15) to select the optimal bun-
dle size P ∗. In practice, we run PCDN with varying P . Figure 2 shows the training
time as a function of bundle size P for the real-sim dataset and the optimal bundle

12

(a) Logistic regression

(b) L2-loss SVM classiﬁcation

Figure 2: Training time v.s. bundle size P for the real-sim dataset with stopping
criteria (cid:15) = 10−3, the intersection of the horizontal and vertical black line
shows the optimal bundle size P ∗.

size P ∗ can be determined. In this work, we empirically select the optimal P ∗ for
each dataset (See Table 3).

We note that it is not necessary to obtain the optimal P to achieve signiﬁcant
speedup, as a wide range of P will suﬃce to achieve the same goal in practice. As
shown in Figure 2(a), when P is greater than 500, it achieves considerable speedup
higher than SCDN (5 times faster than SCDN for P =500). For a new dataset, one
can ﬁrst select a relatively large P (about 5% of #features) with the most relaxed
stopping criteria for a pilot experiment, and then adjust P for best performance when
necessary.

Evaluated Methods. We evaluate the proposed PCDN algorithm against the
state-of-the-art L1-regularized optimization approaches, including newGLMNET [25],
CDN [24], Shotgun-CDN (SCDN)6 [2], interior-point method (IPM) [8] and trust re-
gion Newton (TRON) [12] methods with C/C++ implementations. For the Armijo
line search procedure (6) in the PCDN, CDN and SCDN methods, we set σ = 0.01,
γ = 0 and β = 0.5 for fair comparisons. The OpenMP library is used for parallel
programming. The stopping criteria used in the experiments are similar to the outer
stopping condition used in [25].

The source code of the proposed PCDN algorithm will be made available to the

public, and the implementation details are listed below:

6. Since the experimental validation in [2] has shown that SCDN is much faster than the SGD-
type algorithms (including SGD, Parallel SGD [16; 26], and SMIDAS [21]) for datasets with
more features, and SCDN performs well on datasets with more samples than features, we only
compare the PCDN algorithm with the SCDN scheme here. The SCDN algorithm is also a
competitive representative of the generic parallel coordinate descent algorithms in [19].

13

CDN: we implement this method based on the source code in the LIBLINEAR7
toolbox. Since the shrinking procedure cannot be performed inside the parallel
loop of the SCDN and PCDN methods, we use an equivalent implementation of
the CDN scheme for fair comparisons, where the shrinking procedure is modiﬁed
such that it is consistent with the other parallel algorithms.

SCDN: We set ¯P = 8 for the SCDN method following Bradley et al. [2].

PCDN: We implement this algorithm with conditions consistent with all other
methods.

TRON: We set σ = 0.01 and β = 0.1 in the projected line search according to
Yuan et al. [24]. We use it as baseline algorithm for L2-loss SVM.

newGLMNET: We use the same setting and implementation provided by Yuan
et al. [25]. Since it is outperformed by CDN for L2-loss SVM, we only use it as
baseline algorithm for logistic regression experiments.

IPM: We use the source code8 and default settings in [8]. We use it as a baseline
interior-point algorithm for logistic regression.

•

•

•

•

•

•

Platform. All experiments are carried out on a 64 bit machine with Intel Xeon 2.4
GHz CPU and 64 GB main memory. We set #thread = 23 for PCDN on a 24-core
machine, which is far less than the optimal bundle size P ∗ given in Table 3. We note
that the descent direction (step 7 in Algorithm 3) of the PCDN algorithm can be
fully parallelized on several hundreds even to thousands of threads.

5.2 L1-Regularized L2-Loss SVM

Figure 3 shows the runtime performance of the PCDN, CDN and TRON methods
with the best regularization parameter c∗ (determined based on Yuan et al. [24]) and
varying stopping criteria (cid:15) (equivalent for three solvers). Experimental results show
that the proposed PCDN algorithm performs favorably against the other methods. As
a feature-based parallel algorithm, the proposed PCDN solver performs well for sparse
datasets with more features as shown by the results on the rcv1 and news20 datasets,
which are very sparse (training data sparsity, deﬁned by the ratio of zero elements in
design matrix X and explained in Table 2, is 99.85% and 99.97%, respectively) with a
large number of features (47,236 and 1,355,191). In such cases, the PCDN algorithm
performs well against the TRON method. For the news20 dataset, the PCDN solver
is 29 times faster than TRON method and 18 times faster than CDN approach. We
note that for the a9a dataset, the PCDN solver is sometimes slightly slower than the
TRON method since it is a relatively dense dataset with fewer features than samples
(only 123 features with 26,049 samples).

7. liblinear version 1.7, http://www.csie.ntu.edu.tw/~cjlin/liblinear/.
8. version 0.8.2, http://www.stanford.edu/~boyd/l1_logreg/

14

(a) rcv1 c∗ = 4.0

(b) news20 c∗ = 64.0

(c) a9a c∗ = 0.5

Figure 3: Runtime comparisons for L2-loss SVM classiﬁcation where each marker
compares a solver with PCDN on one dataset. The y-axis and x-axis show
the runtime of a solver as well as PCDN on the same problem. Markers
above the diagonal line indicate that PCDN is faster.

5.3 L1-Regularized Logistic Regression

We compare the runtime performance of the PCDN algorithm with the newGLMNET,
IPM, SCDN and CDN methods on L1-regularized logistic regression with a bias term.
Figure 4 shows the trace of function value (row 1) and test accuracy (row 2) with
respect to the log runtime. Overall, the PCDN solver performs favorably against
the other methods where the best speedup over the CDN method is 17.49 (with
#thread = 23). The speedup can be higher if more threads are used. For the rcv1
dataset in Figure 4(a), the bundle size which reﬂects parallelism of PCDN is as high
as 1,600.

For the gisette dataset shown in Figure 4(b), the SCDN method is slower than
the CDN scheme. This can be attributed to that the SCDN method is sensitive to
correlation among features. Note that for gisette with 6,000 features the optimal
P ∗ for PCDN is only 20, which also indicates the high correlation among features.
For the kdda dataset, the computational load for IPM is prohibitively long, and not
included in Figure 4(d). Despite the required runtime, the IPM method achieves
higher accuracy on the rcv1 and real-sim datasets.

Figure 4(e) shows that the PCDN performs favorably against the state-of-the-
art methods on the large webspam dataset which consists of 1,043,724,776 non-zero
elements. For the kdda dataset, the PCDN algorithm is slower than the SCDN method
in the beginning but converges faster than the others in the end as shown in Figure
4(d). Except for the correlation among features and memory bandwidth limit, another
issue that would signiﬁcantly aﬀect the performance of PCDN is the workload of the
parallel threads. For the PCDN algorithm, each thread ﬁrst processes one feature of
the data, and then switches to the next feature. Thus, the parallel processing time

15

rcv1 (cid:15) =

(a)
10−3

(b) gisette (cid:15)
= 10−4

(c) real-sim (cid:15)
= 10−6

(d) kdda (cid:15) =
10−3
3.7

∗

(e) webspam
(cid:15) = 10−2

16
Figure 4: Runtime performance of PCDN, newGLMNET, SCDN, IPM and CDN for
function value. Second column: test

logistic regression. First column:
accuracy.

of PCDN, which contributes to the acceleration, is proportional to the workload of
the parallel threads. The workload of each thread is approximately proportional to
the number of non-zero elements (NNZs) of the data corresponding to the feature
being processed. To verify that, we compute the average number of NNZs per feature
(NNZ/feature column in Table 2), and show that there are only 15 NNZs/feature in
the kdda dataset, while there are 63 NNZs/feature in the webspam dataset. These
results explain the performance diﬀerence of the PCDN algorithm on these two large
datasets.

5.4 Scalability of PCDN

We also evaluate the scalability of PCDN in two aspects: whether PCDN can maintain
the speed-up when the data size is increased, and whether PCDN can achieve better
speed-up when the available computing resource (e.g., number of cores) is increased.
To analyze the eﬀect of data size, we maintain all the other factors, e.g., correlation
among features, the same in the experiments. To this end, we duplicate the samples
to create datasets from 100% of original size to 2000%. Figure 5 shows the scalability
over diﬀerent number of cores and data size.

(a) diﬀerent #core

(b) diﬀerent data size

Figure 5: Speedup of PCDN on the rcv1 dataset.

Eﬀect of Number of Cores. Figure 5(a) shows that the speedup of the PCDN
algorithm is larger at the beginning when the number of cores is increased (i.e., the
parallel eﬃciency decreases with more parallelism) which can be explained by the
Amdahl’s law: First, as the number of cores increase, the parallelized part takes less
and less time. However, the serial part takes approximately the same constant time.
Second, with more cores, there is increasing parallelization overhead, e.g., more data
transfer, and thereby lowering parallel eﬃciency.

Eﬀect of Data Size. Figure 5(b) shows that the speedup is approximately constant
with larger data size, which shows the weak scaling property of parallel algorithms.

17

It is noteworthy that for very large dataset, the size of the data for each feature is
also quite large that it may exceed the memory bandwidth.

5.5 Discussions

The high dimensional line search plays the key role in ensuring global convergence of
PCDN. In this work, we use the Armijo line search as a speciﬁc realization, and it is
worth exploring other ways to perform the line search. In addition, the computational
cost of line search can be further reduced by deriving the (approximate) optimal line
search step number, as what is performed for solving the dual linear SVM problem
in [10].

The bundle size P controls the ratio between computation and communication,
and thus aﬀecting the running time of PCDN. Although we present an empirical
method to choose a good P in Section 5.1, it is of great interest to develop a principled
approach to determine the optimal value for P .

Another direction to pursue is to extend the PCDN algorithm within a distributed
framework in a way similar to the Parallel SGD [27] and Downpour SGD [5] meth-
ods, to deal with very large datasets with lots of samples, that do not ﬁt into one
single machine. This can be achieved by ﬁrst randomly distributing training data of
diﬀerent samples to diﬀerent machines (i.e., parallelizing over samples), and applying
the PCDN algorithm over a subset (i.e., parallelizing over features) on each machine,
and aggregating all the models in the end. As a shared memory parallel algorithm,
the PCDN algorithm can also be implemented with the stale synchronous parallel
model [7] to achieve better performance.

6. Concluding Remarks

We propose an algorithm termed Parallel Coordinate Descent with approximate New-
ton step, with strong convergence guarantee, fast convergence rate and high paral-
lelism for L1-regularized minimization problems. We show that the seemingly expen-
sive high dimensional line search can be calculated eﬃciently with the implementa-
tion technique of maintaining intermediate quantities, which also minimizes the data
transfer and synchronization cost of PCDN.

The PCDN can be generalized to solve the problems of minimizing the sum of
a convex twice diﬀerentiable loss term and a separable regularization term. Thus,
it allows L1 (lasso), L2 (ridge regression), and mixtures of the two penalties (elastic
net). Experimental results on several benchmark datasets show that the proposed
PCDN algorithm performs favorably against several state-of-the-art methods for L1-
regularized optimization problems.

18

Acknowledgments

The authors would like to thank Hongyuan Zha, Xiangfeng Wang, Martin Tak´ac
and Martin Jaggi for their valuable comments and suggestions to improve this work.
This research was partially supported by the Max Planck ETH Center for Learning
Systems

References

[1] Yatao Bian, Xiong Li, Mingqi Cao, and Yuncai Liu.

Bundle cdn: A
highly parallelized approach for large-scale l1-regularized logistic regression. In
ECML/PKDD, pages 81–95, 2013.

[2] Joseph K. Bradley, Aapo Kyrola, Danny Bickson, and Carlos Guestrin. Parallel
coordinate descent for l1-regularized loss minimization. In ICML, pages 321–328,
2011.

[3] JamesV. Burke. Descent methods for composite nondiﬀerentiable optimization

problems. Mathematical Programming, 33(3):260–279, 1985.

[4] Kai-Wei Chang, Cho-Jui Hsieh, and Chih-Jen Lin. Coordinate descent method
for large-scale l2-loss linear support vector machines. Journal of Machine Learn-
ing Research, 9:1369–1398, 2008.

[5] Jeﬀrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin,
Quoc V. Le, Mark Z. Mao, Marc A. Ranzato, Andrew Senior, Paul Tucker,
Ke Yang, and Andrew Y. Ng. Large Scale Distributed Deep Networks. In NIPS,
2012.

[6] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen
Lin. Liblinear: A library for large linear classiﬁcation. Journal of Machine
Learning Research, 9:1871–1874, 2008.

[7] Q. Ho, J. Cipar, H. Cui, J.-K. Kim, S. Lee, P. B. Gibbons, G. Gibson, G. R.
Ganger, and E. P. Xing. More eﬀective distributed ml via a stale synchronous
parallel parameter server. In NIPS, 2013.

[8] Kwangmoo Koh, Seung-Jean Kim, and Stephen P Boyd. An interior-point
method for large-scale l1-regularized logistic regression. Journal of Machine
learning research, 8(8):1519–1555, 2007.

[9] John Langford, Lihong Li, and Tong Zhang. Sparse online learning via truncated

gradient. Journal of Machine Learning Research, 10:777–801, 2009.

[10] Ching-Pei Lee and Dan Roth. Distributed box-constrained quadratic optimiza-
tion for dual linear SVM. In ICML, Lille, France, 2015, pages 987–996, 2015.
URL http://jmlr.org/proceedings/papers/v37/leea15.html.

19

[11] Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. Convo-
lutional deep belief networks for scalable unsupervised learning of hierarchical
representations. In ICML, pages 609–616, 2009.

[12] Chih-Jen Lin and Jorge J. Mor´e. Newton’s method for large bound-constrained

optimization problems. SIAM Journal on Optimization, 9(4):1100–1127, 1999.

[13] Haipeng Luo, Patrick Haﬀner, and Jean-Fran¸cois Paiement. Accelerated parallel
optimization methods for large scale machine learning. arXiv:1411.6725, 2014.

[14] Jakub Marecek, Peter Richt´arik, and Martin Tak´ac. Distributed block coordinate
descent for minimizing partially separable functions. arXiv:1406.0238, 2014.

[15] Andrew Y. Ng. Feature selection, l1 vs. l2 regularization, and rotational invari-

ance. In ICML, pages 78–85, 2004.

[16] Feng Niu, Benjamin Recht, Christopher Re, and Stephen J. Wright. Hogwild: A
lock-free approach to parallelizing stochastic gradient descent. In NIPS, pages
693–701, 2011.

[17] Peter Richt´arik and Martin Tak´ac. Parallel coordinate descent methods for big

data optimization. CoRR, abs/1212.0873, 2012.

[18] Peter Richt´arik and Martin Tak´aˇc. Distributed coordinate descent method for

learning with big data. arXiv:1310.2059, 2013.

[19] Chad Scherrer, Mahantesh Halappanavar, Ambuj Tewari, and David Haglin.
Scaling up coordinate descent algorithms for large l1-regularization problems. In
ICML, 2012. URL http://icml.cc/discuss/2012/705.html.

[20] Chad Scherrer, Ambuj Tewari, Mahantesh Halappanavar, and David Haglin.
Feature clustering for accelerating parallel coordinate descent. In NIPS, pages
28–36, 2012.

[21] Shai Shalev-Shwartz and Ambuj Tewari. Stochastic methods for l1 regularized

loss minimization. In ICML, pages 117–124, 2009.

[22] Paul Tseng and Sangwoon Yun. A coordinate gradient descent method for non-
smooth separable minimization. Mathematical Programming, 117(1-2):387–423,
2009.

[23] John Wright, Allen Y. Yang, Arvind Ganesh, Shankar S. Sastry, and Yi Ma.
Robust face recognition via sparse representation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 31(2):210–227, 2009.

20

[24] Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, and Chih-Jen Lin. A compar-
ison of optimization methods and software for large-scale l1-regularized linear
classiﬁcation. Journal of Machine Learning Research, 11:3183–3234, 2010.

[25] Guo-Xun Yuan, Chia-Hua Ho, and Chih-Jen Lin. An improved glmnet for l1-

regularized logistic regression. In KDD, pages 33–41, 2011.

[26] Martin Zinkevich, Alex Smola, and John Langford. Slow learners are fast. In

NIPS, pages 2331–2339, 2009.

[27] Martin Zinkevich, Markus Weimer, Alexander J. Smola, and Lihong Li. Paral-

lelized stochastic gradient descent. In NIPS, pages 2595–2603, 2010.

21

Appendix

A. Full Proofs of Theorems 1 and 2

A.1 Proof of Lemma 1(a)
Proof. (1) We ﬁrst prove that EBt[¯λ(
B
P and EBt[¯λ(
B

Let λk be the k-th minimum of (X(cid:62)X)jj, j = 1,

t)] is constant with respect to P , if λi is constant or λ1 = λ2 =

t)] is monotonically increasing with respect to
= λn.
· · ·
n. We deﬁne

, n, for 1

P

· · ·

≤

≤

f (P ) := EBt[¯λ(
t)] =
B
+ λkC P −1
(λnC P −1
n−1 +

k−1 +

· · ·

· · ·

+ λP C P −1

P −1 )/C P
n ,

(19)

where C P

n is a binomial coeﬃcient. For 1

P

n

1,

≤

−

≤

When ¯k =
0,

, then (P + 1)k

P (n + 1)

(cid:100)

≥
k < ¯k. The above equation is equivalent to
∀

−

(P +1)k
P (n+1)(cid:101)

0,

k

∀

≥

¯k, and (P + 1)k

P (n + 1)

−

≤

¯k and λk
According to the observations that λk
decrease the above equation by substitute λk by λ¯k. That is,

λ¯k,

≥

≥

∀

k

λ¯k,

k < ¯k, we can
∀

≤

f (P + 1)

f (P )

−
C P −1
P −1
C P
n

C P −1
P −1
C P
n

+

+

P +1
(cid:88)

k=n
P +1
(cid:88)

k=n

=

λP

−

=

λP

−

λk(

C P
k−1
C P +1
n −

C P −1
k−1
C P
n

)

λk

(P + 1)k
P (n

P (n + 1)
P )

C P −1
k−1
C P
n

.

−
−

f (P ) =

f (P + 1)
(cid:34) ¯k

−

(cid:88)

λk

(P + 1)k
P (n

P (n + 1)
P )

k=n
(cid:34)P +1
(cid:88)

k=¯k

λk

P (n + 1)
P (n

(P + 1)k
P )

C P −1
P −1
C P
n

.

λP

−

−
−

−
−

−

(cid:35)

(cid:35)

C P −1
k−1
C P
n

C P −1
k−1
C P
n

f (P + 1)
(cid:34) ¯k

(cid:88)

f (P )

−
(P + 1)k
P (n

λ¯k

k=n

≥
(cid:34)P +1
(cid:88)

k=¯k

λ¯k

P (n + 1)
P (n

−
−

P (n + 1)
P )

−
−
(P + 1)k
P )

(cid:35)

−

C P −1
k−1
C P
n
(cid:35)

C P −1
k−1
C P
n

C P −1
P −1
C P
n

λ¯k

−

22

Thus, f (P + 1)
≤
increasing with respect to P . Clearly, from (19), if λ1 = λ2 =
EBt[¯λ(

1
∀
t)] = λ1, which is constant with respect to P .

1. Namely, EBt ¯λ(

f (P )

−

≤

−

≥

0,

B

P

n

t) is monotonically
= λn, then

· · ·

B

(2) Next, we prove that EBt[¯λ(
B

P . Let λk be the k-th minimum of (X(cid:62)X)jj, j = 1,

t)]/P is monotonically decreasing with respect to
, n. For 1

n, deﬁne

P

· · ·

≤

≤

For 1

P

n

1, we have

≤

≤

−

C P −1
P −1
C P
n

C P −1
P −1
C P
n

+

+

P +1
(cid:88)

k=n
P +1
(cid:88)
(

k=n

(P + 1)k
P (n

P (n + 1)
P )

C P −1
k−1
C P
n

(cid:35)

−
−
C P −1
k−1
C P
n

(cid:35)
)

C P
k−1
C P +1
n −
(cid:35)

C P
k−1
C P +1
n −

P
(cid:88)

k=n

C P −1
k−1
C P
n

= λ¯k

= λ¯k

= λ¯k

(cid:34)

(cid:34)

−

−
(cid:34)P +1
(cid:88)

k=n

−

= λ¯k[1

1] = 0

B

EBt[¯λ(
P
(λnC P −1

n−1 +

t)]

=

· · ·

g(P ) :=

1
P C P
n

+ λkC P −1

k−1 +

+ λP C P −1

P −1 ).

· · ·

g(P + 1)

g(P )

−
C P −1
P −1
P C P
n

C P −1
P −1
P C P
n

+

+

P +1
(cid:88)

k=n
P +1
(cid:88)

k=n

=

λP

−

=

λP

−

λk(

C P
k−1
(P + 1)C P +1

n −

C P −1
k−1
P C P
n

)

λk

k
n

−
−

n
P

C P −1
k−1
P C P
n

.

g(P + 1)

g(P )

λP

≤ −

(cid:34)

(cid:34)

= λP

= λP

−
C P −1
P −1
P C P
n

+

P +1
(cid:88)

k=n

λP

k
n

n
P

C P −1
k−1
P C P
n

(cid:35)

λP

C P −1
P −1
P C P
n

−

+

P +1
(cid:88)

k=n

C P −1
P −1
P C P
n

+

P +1
(cid:88)
(

k=n

k
n

C P −1
k−1
P C P
n

−
−
C P
k−1
(P + 1)C P +1

n −

(cid:35)
)

C P −1
k−1
P C P
n

−
−
n
P

23

According to the observations that k−n
increase the above equation by substituting λk with λP . That is

0 and λk

n−P ≤

λP ,

k = n,

≥

∀

· · ·

, P + 1, we can

(cid:34)

(cid:20)

1
P + 1

1

P + 1 −

C P
k−1
C P +1
n −
(cid:21)

P +1
(cid:88)

k=n
1
P

(cid:35)

1
P

P
(cid:88)

k=n

C P −1
k−1
C P
n

= λP

= λP

0,

≤

P

where (20) comes from λP
P

≥
1. Namely, EBt [¯λ(Bt)]

n

0 and 1

1
P < 0. Thus, g(P + 1)

P +1 −

≤
is monotonically decreasing with respect to P .

−

g(P )

0,

1
∀

≤

(20)

≤

−

A.2 Proof of Lemma 1(b)

Proof. (1) For logistic regression,

2
jjL(w) = c

τ (yiw(cid:62)xi)(1

τ (yiw(cid:62)xi))x2
ij,

(21)

∇

−

s
(cid:88)

i=1

1

where τ (s)
≡
0 < τ (s) < 1, we have 0 <
when τ (s) = 1
the maximum element of (X(cid:62)X)jj where j
holds. In addition, because in practice
0 < τ

1+e−s is the derivative of the logistic loss function log(1 + es). Because
ij = 1
i=1 x2
2
4c(X(cid:62)X)jj (the equal sign holds
jjL(w)
4 for logistic regression. As ¯λ(
2), and thus (12) holds when θ = 1
) is
¯h = θc¯λ(
) in (13) also
,
∇
, there exist ¯τ and τ such that
<
2
jjL(w).
≤ ∇

|
¯τ < 1. Thus, there exists a h > 0 such that 0 < h

∈ N
yiw(cid:62)xi
|

(2) For L2-loss SVM, use generalized second derivative

τ (yiw(cid:62)xi)

4c (cid:80)s

2
jjL(w)

∞

∇

N

N

≤

≤

≤

≤

1

(cid:88)

2c

i∈I(w)

x2
ij ≤

2c

s
(cid:88)

i=1

ij = 2c(X(cid:62)X)jj,
x2

(22)

i
{

) is the maximum element of (X(cid:62)X)jj where j

yiw(cid:62)xi < 1
}

where I(w) =
¯λ(
in (13) also holds. To ensure that
(ν = 10−12 ) is added when
2
jjL(w)

. So (12) holds for θ = 2 for L2-loss SVM. Because
)
, so
N
∇
2
jjL(w) > 0, a very small positive number ν
0 according to [4]. Thus, h = ν > 0.

¯h = θc¯λ(

2
jjL(w)

∈ N

N

≤

|

∇

∇
≤

A.3 Proof of Lemma 1(c)

Proof. We follow the proof in [22], from (4) and the convexity of L1-norm, for any
α

(0, 1),

∈

L(w)(cid:62)(αd) +

(αd)(cid:62)H(αd) +

w + (αd)
(cid:107)

(cid:107)

1

L(w)(cid:62)d +

d(cid:62)Hd +

w + d

1
2

∇

≤ ∇

= α

L(w)(cid:62)d +

∇

α

L(w)(cid:62)d +

≤

∇

1
2
1
2

(cid:107)

1
(cid:107)

1
2
α2d(cid:62)Hd +

(cid:107)
α2d(cid:62)Hd + α

24

α(w + d) + (1

α)w

−

1
(cid:107)

w + d
(cid:107)

(cid:107)

1 + (1

w

α)
(cid:107)

(cid:107)

1.

−

After rearranging these terms, we have

(1

α)
1
2
≤ −
Dividing both sides by 1

−

∇
(1

−

L(w)(cid:62)d + (1

w + d
α)(
(cid:107)

1
(cid:107)

−

w

1)

− (cid:107)

(cid:107)

α)(1 + α)d(cid:62)Hd.

−
α > 0 and taking α inﬁnitely approaching 0 yields

L(w)(cid:62)d +

w + d

∇

(cid:107)

1

(cid:107)

− (cid:107)

w

1
(cid:107)

≤ −

d(cid:62)Hd,

and thus

∆ =

L(w)(cid:62)d + γd(cid:62)Hd +

w + d

(cid:107)

1

(cid:107)

− (cid:107)

w

1

(cid:107)

∇
(γ

1)d(cid:62)Hd,

(23)

−
which proves (14). From the Armijo rule in (6) we have

≤

By substituting (23) into the above equation and considering that γ

[0, 1) we obtain

∈

F (w + αd)

F (w)

σα∆.

−

≤

F (w + αd)

F (w)

σα(γ

−

≤

−

1)d(cid:62)Hd

0.

≤

Hence

F (wt)
}
{

is nonincreasing.

A.4 Proof of Theorem 1: Convergence of P -dimensional line search

Proof. (1) First, we prove that the descent condition in (6) F (w+αd)
(cid:111)
is satisﬁed for any σ
.

(0, 1) whenever 0

min

(cid:110)

α

−

1, 2h(1−σ+σγ)
√
P ¯λ(Bt)

θc

F (w)

σα∆

≤

≤

≤

For any α

∈
[0, 1],

∈

(cid:90) 1

=

0 ∇
+

F (w + αd)
=L(w + αd)

F (w)
L(w) +

−
−

w + αd
(cid:107)

(cid:107)

1

w

1

(cid:107)

− (cid:107)

L(w + uαd)(cid:62)(αd)du

=α

w

− (cid:107)

w + αd
1
(cid:107)
(cid:107)
L(w)(cid:62)d +
(cid:90) 1

1
(cid:107)
w + αd
(cid:107)
L(w + uαd)

∇
+

1

(cid:107)

(
∇

0

− ∇

w

1

(cid:107)

− (cid:107)
L(w))(cid:62)(αd)du,

where (24) is based on the deﬁnition of deﬁnite integration. Because in the t-th
Rn∗n such that
iteration of PCDN, dj = 0,
gjj = 1,

(cid:54)∈ B
t, otherwise gjj = 0. Then we have

t, we deﬁne auxiliary matrix G

j
∀

∈

j
∀

∈ B

L(w + uαd)

(G

L(w + uαd)

(
∇
(
∇

·

L(w))(cid:62)(αd) =
L(w)))(cid:62)(αd).

− ∇

− ∇

25

(24)

(25)

(26)

(27)

(28)

Substituting (26) into (25) we obtain

F (w + αd)

F (w)

−
L(w)(cid:62)d +

(
∇

= α
(cid:90) 1

∇
(G

w + αd
(cid:107)
(cid:107)
L(w + uαd)

1

w

1+

− (cid:107)
(cid:107)
L(w)))(cid:62)(αd)du

0

≤
α

α
(cid:90) 1

∇

·
L(w)(cid:62)d + α(

− ∇
w + d
(cid:107)
L(w + uαd)

G

(

1
(cid:107)

0 (cid:107)

·

∇

w

1)+

− (cid:107)
(cid:107)
L(w))

d

du,
(cid:107)

(cid:107)(cid:107)

− ∇

where (27) is from the convexity of L1-norm and the Cauchy-Schwarz inequality. It
follows that

(
·
∇
(cid:115)(cid:88)

G
(cid:107)
=

L(w + uαd)

− ∇
L(w + uαd)

L(w))
(cid:107)
L(w))2

− ∇

(
∇
j∈Bt
(cid:115)(cid:88)

j∈Bt

uα

≤

2
jjL( ¯w))2

(
∇

d

(cid:107)

(cid:107)

(cid:113)

uα

P (θc¯λ(
≤
= uαθc√P ¯λ(

B
v)w, 0

d
(cid:107)
,

(cid:107)

t))2

d

B
t)
(cid:107)
v

(cid:107)

where ¯w = v(w + uαd) + (1
−
1(b). By substituting the above inequality into (27) we have

≤

≤

1. We note (28) results from Lemma

w

1) +

− (cid:107)

(cid:107)

w

1) +
(cid:107)

− (cid:107)

≤

= α(

F (w)

F (w + αd)
α

−

∇
α2θc√P ¯λ(

L(w)(cid:62)d + α(
w + d
1
(cid:107)
(cid:107)
(cid:90) 1
2dt
u
(cid:107)
(cid:107)
w + d

t)

B

d

0

L(w)(cid:62)d +
t)

α2θc√P ¯λ(

∇

(cid:107)

1

(cid:107)
d

B

2

(cid:107)
L(w)(cid:62)d + γd(cid:62)Hd +

(cid:107)

= α(

∇

α2θc√P ¯λ(

t)

2

2

d

B
2
(cid:107)
α2θc√P ¯λ(

(cid:107)

−
t)

w + d
(cid:107)
αγd(cid:62)Hd

1
(cid:107)

− (cid:107)

w

1) +
(cid:107)

= α∆ +

B

d

2
(cid:107)

(cid:107)

−

2

αγd(cid:62)Hd.

(29)

If we set α

2h(1−σ+σγ)
P ¯λ(Bt)
θc

√

, then

≤

26

αγd(cid:62)Hd

α2θc√P ¯λ(

t)

2
α(h(1

≤

α((1

≤
= α(1

B

d

2
(cid:107)

−
d

(cid:107)
2
σ + σγ)
−
(cid:107)
(cid:107)
σ + σγ)d(cid:62)Hd
γ)d(cid:62)Hd
σ)(1

−

−

−

γd(cid:62)Hd)
γd(cid:62)Hd)

−
α(1

−
σ)∆,

≤ −

−

(30)

(31)

(32)

(33)

(34)

.

√

where (30) comes from (13) in Lemma 1(b) and (31) is based on Lemma 1(c). The
above equation together with (29) proves that F (w + αd)
2h(1−σ+σγ)
P ¯λ(Bt)
θc
(2) We prove the upper bound of E[qt]. In the Armijo line search procedure, it
tests diﬀerent values of α from larger to smaller, and stops right after ﬁnding one
σαt∆t. Thus in the t-th iteration, the
value that satisfy F (wt + αtdt)
chosen step size αt satisﬁes

σα∆ if α

F (wt)

F (w)

−

≤

≤

≤

−

αt

≥

2h(1

−
θc√P ¯λ(

σ + σγ)
t)

.

B

From (6) we have αt = βq, and thus the line search step number of the t-th iteration
qt

Taking expectation on both sides with respect to the random choices of

t, we obtain

qt = 1 + logβ αt

1 + logβ−1

≤

θc√P ¯λ(

t)
B
σ + σγ)

.

2h(1

−

E[qt]

≤

≤

1 + logβ−1
EBt[logβ−1 ¯λ(
B

2h(1

−
t)]

1 + logβ−1
logβ−1 EBt[¯λ(
B

2h(1

−
t)],

θc
σ + σγ)

θc
σ + σγ)

1
2

1
2

B

+

logβ−1 P +

+

logβ−1 P +

).
where (34) is based on Jensen’s inequality for concave function logβ−1(
·

A.5 Proof of Global convergence

Proof. (1) We ﬁrst relate PCDN to the framework in [22]. Note that the selection
t in (8) is consistent with that used in CGD (i.e., (12) in [22]). For the
of bundle
descent direction computed in a bundle in Algorithm 3, we have

B

dt =

d(wt; j)ej

(cid:88)

j∈Bt

27

(cid:88)

=

j∈Bt

arg min

jL(wt)(cid:62)d +

jjL(wt)d2 +
2

d {∇

j + d

wt
|

|}

ej

1
2 ∇

= arg min
d {

(cid:88)

j∈Bt

(
∇

jL(wt)(cid:62)dj +

jjL(wt)d2
2

j +

j + dj

wt
|

)
|

1
2 ∇

|
L(wt)(cid:62)d +

t

j
∀

(cid:54)∈ B
d(cid:62)Hd +

dj = 0,
1
2
dj = 0,

j
∀

(cid:54)∈ B

}

}
w + d
(cid:107)
(cid:107)
t

1

|

= arg min

d {∇

dH(wt;

t),

B

≡

(35)

(36)

(37)

where (35) is derived by considering the deﬁnition of d(w; j) in (4); (36) is obtained by
2L(w)); (37) is deﬁned by following the descent
applying the setting of H
∇
direction deﬁnition of Tseng et al.
(i.e., (6) in [22]). Therefore the deﬁnition of
direction computed is in a manner similar to CGD. Furthermore, since PCDN uses
2L(w)), it is clear that we can
the Armijo line search for dt,by taking H
use the framework in [22] to analyze the global convergence of PCDN.

diag(

diag(

∇

≡

≡

(2) We use Theorem 1(e) in [22] to prove the global convergence, which requires
1, t =
. To ensure global convergence, Tseng et al. make

is chosen under the Gauss-Seidel rule and supt αt <

. In (6), αt

∞

≤

t

{B

that
1, 2, ..., which satisﬁes supt αt <
the following assumption,

}

∞

0 < h

jjL(wt)
2
≤ ∇

¯h,

j = 1,

, n, t = 0, 1, . . .

∀
which is fulﬁlled by Lemma 1(b). According to Theorem 1(e) in [22], any cluster
point of

is a stationary point of F (w).

wt

· · ·

≤

{

}

A.6 Proof of Theorem 2: Convergence rate

To analyze the convergence rate, we transform (1) into an equivalent problem with a
+ with duplicated features9
twice diﬀerentiable regularizer following [21]. Let ˆw
ˆxi

R2n, the problem becomes

[xi;

R2n

xi]

∈

≡

−

∈

min
ˆw∈R2n
+

s
(cid:88)

i=1

≡

2n
(cid:88)

j=1

F ( ˆw)

c

ϕ( ˆw; ˆxi, yi) +

ˆwj.

(38)

The descent direction is computed by

ˆdj = ˆd( ˆw; j)

≡

arg min

1
2∇
2
jjL( ˆw).
∇
9. Although our analysis uses duplicate features, they are not required for an implementation.

jjL( ˆw) ˆd2 + ˆwj + ˆd
}

ˆd {∇
(
∇

jL( ˆw) + 1)/

jL( ˆw) ˆd +

−

=

2

(39)

28

In the following proof we omit the “

” above each variables for ease of presentation.

∧

Proof. Assume that w∗ minimizes the objective in (38). Deﬁne the potential function
as

≡
t)

Ψ(w)
θc¯λ(
2
= a

B

w
(cid:107)

−

−
w∗

(cid:107)

w

(cid:107)

w∗

2 +

(cid:107)

2 + bF (w),

θc¯λ(

t) supt αt
γ)h

B
2σ(1

−

F (w)

a =

θc¯λ(
2

t)

,

B

b =

θc¯λ(

t) supt αt
γ)h

.

B
2σ(1

−

where

Thus, we have

Ψ(w)
−
w
a(
−
(cid:107)
= aα(

−

aα(

2

Ψ(w + αd) =
w∗
w + αd
− (cid:107)
2w(cid:62)d + 2w∗(cid:62)d
2w(cid:62)d + 2w∗(cid:62)d

(cid:107)

−

−

(cid:107)

2) + b(F (w)
w∗
αd(cid:62)d) + b(F (w)
αd(cid:62)d) + bσα(1

−

F (w + αd))

F (w + αd))

−
γ)d(cid:62)Hd,

≥

−
where (41) uses (14) and (15) in Lemma 1(c). Using the fact that dj = 0,
we derive from (41) that

−

−

(41)

j
∀

t,

(cid:54)∈ B

2wjdj + 2w∗

j dj

αd2

j ) + bσα(1

−

γ)

−

∇

2
jjL(w)d2
j

aα(

2wjdj + 2w∗

j dj) + α[bσ(1

γ)

−

∇

2
jjL(w)

aα]d2
j

−

Ψ(w + αd)

Ψ(w)
(cid:88)

−
aα(

≥

=

≥

j∈Bt
(cid:88)

j∈Bt
(cid:88)

j∈Bt

−

−

−

aα(

2wj + 2w∗

j )dj,

and (42) uses the fact that

(40)

(42)

≥
By substituting a = θc¯λ(Bt)
we have the following equations

2

and dj =

(
∇

−

jL(w) + 1)/

2
jjL(w) (See (39)) into (42),

∇

bσ(1

γ)

=

−
θc¯λ(
2
θc¯λ(
2

∇
t)

B

t)

B

−

2
aα
jjL(w)
(cid:20)
2
jjL(w) supt αt
h

∇

(cid:21)

α

−

αt

(sup
t

−

α)

0.

≥

Ψ(w)

Ψ(w + αd)

−

29

We note (44) is based on Lemma 1(b) and (45) results from the deﬁnition of ¯λ(

Taking the expectation with respect to the random choices of

t).
t on both sides of

B

B

(45) we have

≥
where (46) comes from the convexity of L(w).

−

By summing over T + 1 iterations on both sides of (46), with an expectation over

the random choices of

t, we obtain,

θc¯λ(
t)α
B
2
jjL(w)
∇
¯λ(
t)α
B
(X(cid:62)X)jj

(cid:88)

j∈Bt

(cid:88)

j∈Bt

(wj

w∗

j )(

∇

−

jL(w) + 1)

(wj

w∗

j )(

∇

−

jL(w) + 1)

(cid:88)

α

(wj

j∈Bt

w∗

j )(

∇

−

jL(w) + 1).

≥

≥

≥

jL(w) + 1)]

∇
jL(w) + 1)(cid:3)

−

w∗

j )(

∇

EBt[Ψ(w)

−
αtEBt[

Ψ(w + αd)]
(cid:88)

(wj

w∗

j )(

inf
t

≥

= inf
t

= inf
t

inf
t

j∈Bt

(cid:2)(wj

(w

−

(F (w)

αtP Ej
αt P
2n
αt P
2n

−
w∗)(

L(w) + 1)

∇
F (w∗)),

B
(cid:62)
(cid:88)

E[

t=0

Ψ(wt)

Ψ(wt+1]

−

inf
t

≥

αt P
2n

E[

= inf
t

αt P
2n

[E

(cid:62)
(cid:88)

t=0
(cid:62)
(cid:88)

t=0

inf
t

≥

αt P (T + 1)
2n

F (wt)

F (w∗)]

−

−

[F (wt)]

(T + 1)F (w∗)]

[E[F (w(cid:62))]

F (w∗)],

F (wt)
where (47) comes from Lemma 1(c) that
}
bound αt by some positive constant ξ = 2h(1−σ+σγ)
√
P ¯λ(Bt)

{
θc
αt

≤

1.

≤

0 < ξ

Substituting (48) into (47), we have

−
is nonincreasing. From (32) we can
,

(cid:34) (cid:62)
(cid:88)

E

t=0

Ψ(wt)

Ψ(wt+1

−

(cid:35)

ξ

P (T + 1)
2n

≥

[E[F (w(cid:62))]

F (w∗)].

−

30

(43)

(44)

(45)

(46)

(47)

(48)

By rearranging the above inequality, we have

E[F (w(cid:62))]

−

2n
ξP (T + 1)

F (w∗)
(cid:62)
(cid:88)

E[

t=0

≤

≤

≤

=

≤

2n
ξP (T + 1)
2n
ξP (T + 1)
2nEBt ¯λ(
t)
B
ξP (T + 1)
2nEBt ¯λ(
t)
B
P (T + 1)

Ψ(wt)

Ψ(wt+1)]

−

E[Ψ(w0)

Ψ(wT +1)]

−

E[Ψ(w0)]

(cid:21)

(F (0))

(
(cid:107)
(cid:20)

(cid:20)θc
2
θc
2ξ

·

w∗

2) +
(cid:107)

w∗

2 +

(cid:107)

(cid:107)

θc supt αt
γ)h
2σ(1
(cid:21)

−
F (0)

σ(1

γ)h

,

−

(49)

(50)

(51)

where (49) comes from that Ψ(wT +1)
holds since αt
1.

≥

≤

0, and (50) is because w0 is set to be 0, (51)

B. Computational Complexities of PCDN and CDN

The proposed PCDN algorithm takes much less time for each outer iteration than
the CDN method. We analyze the computational complexity of PCDN for the k-th
outer iteration, time(k) to demonstrate this point (note that CDN is a special case
of PCDN with bundle size P = 1).

Let tdc denote the time complexity for computing the descent direction (step 7
in Algorithm 3), and tls denote the time complexity for a step of P -dimensional
line search, which is approximately constant with varying P (See the discussions
below). When the computation of descent directions (step 6 in Algorithm 3) is fully
parallelized, time(k) can be estimated by

E[time(k)]

n/P

tdc +

n/P

≈ (cid:100)

(cid:101) ·

(cid:100)

(cid:101) ·

E[qt]

tls,

·

(52)

t, and qt is the number
where the expectation is with respect to the random choice of
of line search steps in the t-th iteration. As indicated in (52), the computational
complexity of descent directions
tdc decreases linearly with the increase of
bundle size P . For the cost of Armijo line search, when approximately estimating
E[qt] by its upper bound in Theorem 1, E[qt]/P decreases with respect to P 10, and
tls decreases with the increase of bundle size P . The overall
thus
computational complexity of PCDN’s each outer iteration is lower than that of the
CDN method.

E[qt]

n/P

n/P

(cid:101) ·

(cid:101) ·

B

(cid:100)

(cid:100)

·

10. Using the upper bound in Theorem 1, and the fact that EBt[¯λ(
with respect to P in Lemma 1(a), we can easily obtain this.

B

t)]/P is monotonically decreasing

31

We show that the time complexity of one step of P -dimensional line search tls
remains approximately constant with varying bundle size P . The reason being that
in each line search step of Algorithm 4, the time complexity remains constant with re-
spect to P . The diﬀerence of the whole line search procedure results from computing
d(cid:62)xi = (cid:80)P
j=1 djxij. However, d(cid:62)xi in the PCDN algorithm can be computed in par-
allel with P threads as well as a reduction-sum operation, and thus the computational
complexity remains approximately constant.

32

