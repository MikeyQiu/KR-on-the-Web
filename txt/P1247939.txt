Eﬃcient Estimation of Mutual Information for
Strongly Dependent Variables

5
1
0
2
 
r
a

M
 
5
 
 
]
T
I
.
s
c
[
 
 
3
v
3
0
0
2
.
1
1
4
1
:
v
i
X
r
a

Shuyang Gao
Information Sciences Institute
University of Southern California
sgao@isi.edu

Greg Ver Steeg
Information Sciences Institute
University of Southern California
gregv@isi.edu

Aram Galstyan
Information Sciences Institute
University of Southern California
galstyan@isi.edu

Abstract

We demonstrate that a popular class of non-
parametric mutual information (MI) estima-
tors based on k-nearest-neighbor graphs re-
quires number of samples that scales expo-
nentially with the true MI. Consequently, ac-
curate estimation of MI between two strongly
dependent variables is possible only for pro-
hibitively large sample size. This important
yet overlooked shortcoming of the existing es-
timators is due to their implicit reliance on
local uniformity of the underlying joint dis-
tribution. We introduce a new estimator that
is robust to local non-uniformity, works well
with limited data, and is able to capture rela-
tionship strengths over many orders of mag-
nitude. We demonstrate the superior perfor-
mance of the proposed estimator on both syn-
thetic and real-world data.

1 Introduction

As a measure of the dependence between two random
variables, mutual information is remarkably general
and has several intuitive interpretations (Cover and
Thomas, 2006), which explains its widespread use in
statistics, machine learning, and computational neu-
roscience; see (Wang et al., 2009) for a list of various
applications. In a typical scenario, we do not know the
underlying joint distribution of the random variables,
and instead need to estimate mutual information using
i.i.d. samples from that distribution. A naive approach
to this problem is to ﬁrst estimate the underlying prob-
ability density, and then calculate mutual information

Copyright 2014 by the authors.

using its deﬁnition. Unfortunately, estimating joint
densities from a limited number of samples is often
infeasible in many practical settings.

A diﬀerent approach is to estimate mutual informa-
tion directly from samples in a non-parametric way,
without ever describing the entire probability density.
The main intuition behind such direct estimators is
that evaluating mutual information can in principle
be a more tractable problem than estimating the den-
sity over the whole state space (P´erez-Cruz, 2008).
The most popular class of estimators taking this ap-
proach is the k-nearest-neighbor(kNN) based estima-
tors. One example of such estimator is due to Kraskov,
St¨ogbauer, and Grassberger (referred to as the KSG
estimator from here on) (Kraskov et al., 2004), which
has been extended to generalized nearest neighbors
graphs (P´al et al., 2010).

Our ﬁrst contribution is to demonstrate that kNN-
based estimators suﬀer from a critical yet overlooked
ﬂaw. Namely, we illustrate that if the true mutual
information is I, then kNN-based estimators requires
exponentially many (in I) samples for accurate esti-
mation. This means that strong relationships are ac-
tually more diﬃcult to measure. This counterintuitive
property reﬂects the fact that most work on mutual
information estimation has focused on estimators that
are good at detecting independence of variables rather
In the
than precisely measuring strong dependence.
age of big data, it is often the case that many strong
relationships are present in the data and we are in-
terested in picking out only the strongest ones. MI
estimation will perform poorly for this task if their
accuracy is low in the regime of strong dependence.

We show that the undesired behavior of previous kNN-
based MI estimators can be attributed to the assump-
tion of local uniformity utilized by those estimators,
which can be violated for suﬃciently strong (almost
deterministic) dependencies. As our second major con-
tribution, we suggest a new kNN estimator that relaxes
the local uniformity condition by introducing a correc-

Eﬃcient Estimation of Mutual Information for Strongly Dependent Variables

tion term for local non-uniformity. We demonstrate
empirically that for strong relationships, the proposed
estimator needs signiﬁcantly fewer samples for accu-
rately estimating mutual information.

where
(cid:98)pk

In Sec. 2, we introduce kNN-based non-parametric en-
tropy and mutual information estimators. In Sec. 3,
we demonstrate the limitations of kNN-based mutual
information estimators. And then we suggest a correc-
tion term to overcome these limitations in Sec. 4. In
Sec. 5, we show empirically using synthetic and real-
world data that our method outperforms existing tech-
niques. In particular, we are able to accurately mea-
sure strong relationships using smaller sample sizes.
Finally, we conclude with related work and discussion.

2 kNN-based Estimation of Entropic

Measures

In this section, we will ﬁrst introduce the naive kNN
estimator for mutual information, which is based on
an entropy estimator due to (Singh et al., 2003), and
show its theoretical properties. Next, we focus on a
popular variant of kNN estimators, called KSG esti-
mator (Kraskov et al., 2004).

2.1 Basic Deﬁnitions

Let x = (x1, x2, ..., xd) denote a d-dimensional ab-
solute continuous random variable whose probability
density function is deﬁned as p : Rd → R and marginal
densities of each xj are deﬁned as pj : R → R, j =
1, . . . , d. Shannon Diﬀerential Entropy and Mutual In-
formation are deﬁned in the usual way:

H (x) = −

p (x) log p (x) dx

(1)

I (x) =

p (x) log

dx

(2)

p (x)

d
(cid:81)
j=1

pj (xj)

We use natural logarithms so that information is mea-
sured in nats. For d > 2, the generalized mutual in-
formation is also called total correlation (Watanabe,
1960) or multi-information (Studen`y and Vejnarov´a,
1998).
Given N i.i.d. samples, X = (cid:8)x(i)(cid:9)n
i=1, drawn from
p(x), the task is to construct a mutual information
estimator ˆI (x) based on these samples.

(cid:90)

Rd

(cid:90)

Rd

(cid:16)

(cid:16)

=

x(i)(cid:17)

x(i)(cid:17)−d

k
n − 1

Γ (d/2 + 1)
(4)
πd/2
(cid:0)x(i)(cid:1) in Eq. 4 is the Euclidean distance from x(i)
rk
to its kth nearest neighbor in X . By introducing a
correction term, an asymptotic unbiased estimator is
obtained:

rk

(cid:98)HkN N,k (x) = −

1
n

n
(cid:88)

i=1

(cid:16)

x(i)(cid:17)

− γk

log (cid:98)pk

where

γk =

kk
(k − 1)!

(cid:90) ∞

0

log (x) xk−1e−kxdx = ψ(k) − log(k)

(5)

(6)

where ψ(·) represents the digamma function.

The following theorem shows asymptotic unbiasedness
of (cid:98)HkN N,k (x) according to (Singh et al., 2003).
Theorem 1 (kNN entropy estimator, asymptotic un-
biasedness, (Singh et al., 2003)). Assume that x is ab-
solutely continuous and k is a positive integer, then

E

(cid:104)

(cid:105)
(cid:98)HkN N,k (x)

lim
n→∞

= HkN N,k(x)

(7)

i.e., this entropy estimator is asymptotically unbiased.

From Entropy to Mutual Information To con-
struct a mutual information estimator from an entropy
estimator is straightforward by combining entropy es-
timators using the identity (Cover and Thomas, 2006):

d
(cid:88)

i=1

I(x) =

H (xi) − H (x)

(8)

Combining Eqs. 3 and 8, we have,
n
(cid:88)

(cid:98)I (cid:48)
kN N,k (x) =

log

(cid:0)x(i)(cid:1)
(cid:98)pk
(cid:17)
(cid:16)
x(i)
(cid:98)pk
2

(cid:17)

(cid:16)

x(i)
1

(cid:98)pk

1
n

i=1

(cid:17)

(cid:16)

x(i)
d

...(cid:98)pk

(9)
where x(i)
j denotes the point projected into jth dimen-
sion in x(i) and (cid:98)pk(x(i)
j ) represents the marginal kNN
density estimator projected into jth dimension of x(i).

Similar to Eq. 5, we can also construct an asymptoti-
cally unbiased mutual information estimator based on
Theorem 1:

(cid:98)IkN N,k = (cid:98)I (cid:48)

kN N,k − (d − 1) γk

(10)

Corollary 1 (kNN MI estimator, asymptotic unbi-
asedness). Assume that x is absolute continuous and
k is a positive integer, then:
(cid:105)
E
(cid:98)IkN N,k (x)

= IkN N,k (x)

(11)

(cid:104)

lim
n→∞

2.2 Naive kNN Estimator

2.3 KSG Estimator

Entropy Estimation The naive kNN entropy esti-
mator is as follows:

(cid:98)H (cid:48)

kN N,k (x) = −

1
n

n
(cid:88)

i=1

(cid:16)

x(i)(cid:17)

log (cid:98)pk

The KSG mutual
information estimator (Kraskov
et al., 2004) is a popular variant of the naive kNN es-
timator. The general principle of KSG is that for each
density estimator in diﬀerent spaces, we would like to

(3)

Shuyang Gao, Greg Ver Steeg, Aram Galstyan

in (Kraskov et al., 2004)). Adding the entropy estima-
tors together with these choices yields the following.

ˆIKSG,k(x) ≡ (d − 1)ψ(N ) + ψ(k) − (d − 1)/k

−

1
N

N
(cid:88)

d
(cid:88)

i=1

j=1

ψ(nxj (i))

(15)

use the similar length-scales for k-nearest-neighbor dis-
tance as in the joint space so that the bias would be
approximately smaller. Although the theoretical prop-
erties of this estimator is unknown, it has a relatively
good performance in practice, see (Khan et al., 2007)
for a comparison of diﬀerent estimation methods. Un-
like naive kNN estimator, the KSG estimator uses the
max-norm distance instead of L2-norm. In particular,
if (cid:15)i,k is twice the (max-norm) distance to the k-th
nearest neighbor of x(i), it can be shown that the ex-
pectation value (over all ways of drawing the surround-
ing N − 1 points) of the log probability mass within
the box centered at x(i) is given by this expression.

(cid:34)
E(cid:15)i,k

log

(cid:90)

(cid:35)

f (x) dx

= ψ(k) − ψ(N )

|x−x(i)|∞≤(cid:15)i,k/2

We use ψ to represent the digamma function. If we
assume that the density inside the box (with sides of
length (cid:15)i,k) is constant, then the integral becomes triv-
ial and we ﬁnd that,
log(f (xi)(cid:15)d

(13)
Rearranging and taking the mean over − log f (x(i))
leads us to the following entropy estimator:

i,k) = ψ(k) − ψ(N ).

(12)

(a)

(b)

Figure 1: Centered at a given sample point, x(i), we
show the max-norm rectangle containing k nearest
neighbors (a) for points drawn from a uniform distri-
bution, k = 3, and (b) for points drawn from a strongly
correlated distribution, k = 4.

ˆHKSG,k(x) ≡ ψ(N ) − ψ(k) +

log (cid:15)i,k

Estimators

3 Limitations of kNN-based MI

d
N

N
(cid:88)

i=1

(14)

Note that k, deﬁning the size of neighborhood to use
in local density estimation, is a free parameter. Using
smaller k should be more accurate, but larger k re-
duces the variance of the estimate (Khan et al., 2007).
While consistent density estimation requires k to grow
with N (Von Luxburg and Alamgir, 2013), entropy es-
timates converge almost surely for any ﬁxed k (Wang
et al., 2009; P´erez-Cruz, 2008) under some weak con-
ditions.1

To estimate the mutual information, in the joint x
space we set k, the size of the neighborhood, which
determines (cid:15)i,k for each point x(i). Next, we consider
the smallest rectilinear hyper-rectangle that contains
these k points, which has sides of length (cid:15)xj
i,k for each
marginal direction xj. We refer to this as the “max-
norm rectangle” (as shown in Fig. 1(a)). Let nxj be
the number of points at a distance less than or equal
to (cid:15)xj
i,k/2 in the xj-subspace. For each marginal en-
tropy estimate, we use nxj (i) instead of k to set the
neighborhood size at each point. Finally, in the joint
space using a rectangle instead of a box in Eq. 14 leads
to a correction term of size (d − 1)/k (details given

In this section, we demonstrate a signiﬁcant ﬂaw in
kNN-based MI estimators which is summarized in the
following theorems. We then explain why these esti-
mators fail to accurately estimate mutual information
unless the correlations are relatively weak or the num-
ber of samples is large.

Theorem 2. For any d-dimensional absolute contin-
uous probability density function, p(x), for any k ≥ 1,
for the estimated mutual information to be close to
the true mutual information, | ˆIkN N,k(x) − I(x)| ≤ ε,
requires that the number of samples, N , is at least,
N ≥ C exp
+ 1, where C is a constant which
scales like O( 1

(cid:16) I(x)−ε
d−1
d ).

(cid:17)

The proof of Theorem 2 is shown in the Appendix A.

Theorem 3. For any d-dimensional absolute contin-
uous probability density function, p(x), for any k ≥ 1,
for the estimated mutual information to be close to
the true mutual information, | ˆIKSG,k(x) − I(x)| ≤ ε,
requires that the number of samples, N , is at least,
N ≥ C exp

+ 1, where C = e− k−1
k .

(cid:17)

(cid:16) I(x)−ε
d−1

1This assumes the probability density is absolutely con-
tinuous but see (P´al et al., 2010) for some technical con-
cerns.

Proof. Note that ψ(n) = Hn−1 − γ, where Hn is the
n-th harmonic number and γ ≈ 0.577 is the Euler-

Eﬃcient Estimation of Mutual Information for Strongly Dependent Variables

Mascheroni constant.
(cid:98)IKSG,k(x) ≤ (d − 1)ψ(N ) + ψ(k) − (d − 1)/k
N
(cid:88)

d
(cid:88)

−

1
N

ψ (k)

i=1

j=1

= (d − 1)(ψ(N ) − ψ(k) − 1/k)
= (d − 1)(HN −1 − γ − Hk−1 + γ − 1/k)
≤ (d − 1)(log(N − 1) + (k − 1)/k)

(16)
The ﬁrst inequality is obtained from Eq. 15 by ob-
serving that nxj (i) ≥ k for any i, j and that ψ(k) is
a monotonically increasing function. And the last in-
equality is obtained by dropping the term −Hk−1 ≤ 0
and using the well-known upper bound HN ≤ log N +
1. Requiring that | ˆIKSG,k(x) − I(x)| < ε, we obtain
N ≥ C exp

+ 1, where C = e− k−1
k .

(cid:17)

(cid:16) I(x)−ε
d−1

The above theorems state that for any ﬁxed dimension-
ality, the number of samples needed for estimating mu-
tual information I(x) increases exponentially with the
magnitude of I(x). From the point of view of determin-
ing independence, i.e., distinguishing I(x) = 0 from
I(x) (cid:54)= 0, this restriction is not particularly troubling.
However, for ﬁnding strong signals in data it presents a
major barrier. Indeed, consider two random variables
X and Y , where X ∼ U(0, 1) and Y = X + ηU(0, 1).
When η → 0, the relationship between X and Y be-
comes nearly functional, and the mutual information
diverges as I(X : Y ) → log 1
η . As a consequence, the
number of samples needed for accurately estimating
I(X : Y ) diverges as well. This is depicted in Fig. 2
where we compare the empirical lower bound to our
theoretical bound given by Theorem 3. It can be seen
that the theoretical bounds are rather conservative,
but they have the same exponentially growing rates
comparing to the empirical ones.

What is the origin of this undesired behavior? An intu-
itive and general argument comes from looking at the
assumption of local uniformity in kNN-based estima-
tors. In particular, both naive kNN and KSG estima-
tors approximate the probability density in the kNN
ball or max-norm rectangle containing the k nearest
neighbors with uniform density. If there are strong re-
lationships (in the joint x space, the density becomes
more singular), then we can see in Fig. 1(b) that the
uniform assumption becomes problematic.

4

Improved kNN-based Estimators

In this section, we suggest a class of kNN-based esti-
mators that relaxes the local uniformity assumption.

Figure 2: A semi-logarithmic plot of Ns (number of
required samples to achieve an error at most ε) for
KSG estimator for diﬀerent values of I(X : Y ). We
set ε = 0.1, k = 1.

Local Nonuniformity Correction (LNC) Our
second contribution is to provide a general method
for overcoming the limitations above. Considering the
ball (in naive kNN estimator) or max-norm hyper-
rectangle (in KSG estimator) around the point x(i)
which contains k nearest neighbors, let us denote this
region of the space with V(i) ⊂ Rd, whose volume
is V (i). Instead of assuming that the density is uni-
form inside V(i) around the point x(i), we assume
that there is some subset, ¯V(i) ⊆ V(i) with volume
¯V (i) ≤ V (i) on which the density is constant, i.e.,
ˆp(x(i)) =
. This is illustrated with a shaded re-
gion in Fig. 1(b). We now repeat the derivation above
using this altered assumption about the local density
around each point for ˆH(x). We make no changes
to the entropy estimates in the marginal subspaces.
Based on this idea, we get a general correction term
for kNN-based MI estimators (see Appendix B for the
details of derivation):

I[x∈ ¯V(i)]
¯V (i)

ˆILN C(x) = ˆI(x) −

(17)

1
N

N
(cid:88)

i=1

log

¯V (i)
V (i)

where ˆI(x) can be either ˆIkN N (x) or ˆIKSG(x).

If the local density in the k-nearest-neighbor region
V(i) is highly non-uniform, as is the case for strongly
related variables like those in Fig. 1(b), then the pro-
posed correction term will improve the estimate. For
instance, if we assume that relationships in data are
smooth, functional relationships plus some noise, the
correction term will yield signiﬁcant improvement as
demonstrated empirically in Sec. 5. We note that this
correction term is not bounded by N , but rather by
our method of estimating ¯V . Next, we will give one
concrete implementation of this idea by focusing on
the modiﬁcation of KSG estimator.

Shuyang Gao, Greg Ver Steeg, Aram Galstyan

Estimating Nonuniformity by Local PCA
With the correction term in Eq. 17, we have trans-
formed the problem into that of ﬁnding a local volume
on which we believe the density is positive. Regard-
ing KSG estimator, instead of a uniform distribution
within the max-norm rectangle in the neighborhood
around the point x(i), we look for a small, rotated
(hyper)rectangle that covers the neighborhood of x(i).
The volume of the rotated rectangle is obtained by
doing a localized principle component analysis around
all of x(i)’s k nearest neighbors, and then multiply-
ing the maximal axis values together in each principle
component after the k points are transformed to the
new coordinate system2. The key advantage of our
proposed estimator is as follows: while KSG assumes
local uniformity of the density over a region contain-
ing k nearest neighbors of a particular point, our es-
timator relies on a much weaker assumption of local
linearity over the same region. Note that the local
linearity assumption has also been widely adopted in
the manifold learning, for example, local linear embed-
ding(LLE) (Roweis and Saul, 2000) and local tangent
space alignment(LTSA) (Zhang and Zha, 2002).

Testing for Local Nonuniformity One problem
with this procedure is that we may ﬁnd that, locally,
points may occupy a small sub-volume, i.e., even if the
local neighborhood is actually drawn from a uniform
distribution(as shown in Fig. 1(a)), the volume of the
PCA-aligned rectangle will with high probability be
smaller than the volume of the max-norm rectangle,
leading to an artiﬁcially large non-uniformity correc-
tion.

To avoid this artifact, we consider a trade-oﬀ between
the two possibilities: for a ﬁxed dimension d and near-
est neighbor parameter k, we ﬁnd a constant αk,d, such
that if ¯V (i)/V (i) < αk,d, then we assume local unifor-
mity is violated and use the correction ¯V (i), otherwise
the correction is discarded for point x(i). Note that
if αk,d is suﬃciently small, then the correction term
will be always discarded, so that our estimator reduces
to the KSG estimator. Good choices of αk,d are set
using arguments described in Appendix C. Further-
more, we believe that as long as the expected value of
E[ ¯V (i)/V (i)] ≥ αk,d in large N limit, for some prop-
erly selected αk,d, then the consistency properties of
the proposed estimator will be identical to the con-
stancy properties of the KSG estimator. A rigorous
proof of this hypothesis is left as a future work.

The full algorithm for our estimator is given in Algo-
rithm 1.

Algorithm 1 Mutual Information Estimation
with Local Nonuniform Correction

Input: points x(1), x(2), ..., x(N ), parameter d (di-
mension), k (nearest neighbor), αk,d
Output: ˆILN C(x)
Calculate ˆIKSG(x) by KSG estimator, using the
same nearest neighbor parameter k
for each point x(i) do

Find k nearest neighbors of x(i):

kN N (i)
1 ,

kN N (i)

2 ,..., kN N (i)

k

Do PCA on these k neighbors, calculate the vol-

ume corrected rectangle ¯V (i)

Calculate the volume of max-norm rectangle

V (i)

if ¯V (i)/V (i) < αk,d then

LN Ci = log

¯V (i)
V (i)

else

end if

LN Ci = 0.0

end for
Calculate
LN C1, LN C2, ..., LN CN
ˆILN C = ˆIKSG − LN C ∗

LN C ∗:

5 Experiments

average

value

of

We evaluate the proposed estimator on both synthet-
ically generated and real-world data. For the for-
mer, we considered various functional relationships
and thoroughly examined the performance of the esti-
mator over a range of noise intensities. For the latter,
we applied our estimator to the WHO dataset used
previously in (Reshef et al., 2011). Below we report
the results.

5.1 Experiments with synthetic data

Functional relationships in two dimensions
In
the ﬁrst set of experiments, we generate samples
from various functional relationships of the form Y =
f (X) + η that were previously studied in Reshef et al.
(2011); Kinney and Atwal (2014). The noise term η
is distributed uniformly over the interval [−σ/2, σ/2],
where σ is used to control the noise intensity. We
also compare the results to several baseline estima-
tors: KSG (Kraskov et al., 2004), generalized near-
est neighbor graph (GNN) (P´al et al., 2010) 3, mini-
mum spanning trees (MST) (M¨uller et al., 2012; Yu-
kich and Yukich, 1998), and exponential family with
maximum likelihood estimation (EXP) (Nielsen and

2Note that we manually set the mean of these k points
to be x(i) when doing PCA, in order to put x(i) in the
center of the rotated rectangle.

3We use the online code http://www.cs.cmu.edu/
~bapoczos/codes/REGO_with_kNN.zip for the GNN esti-
mator.

Eﬃcient Estimation of Mutual Information for Strongly Dependent Variables

Nock, 2010) 4.

cal uniformity assumptions, still assumes local linear-
ity. The stronger the nonlinearity, the more samples
are required to ﬁnd neighborhoods that are locally ap-
proximately linear. We note, however, that LNC still
converges faster than KSG.

Figure 3: For all the functional relationships above,
we used a sample size N = 5000 for each noise level
and the nearest neighbor parameter k = 5 for LNC,
KSG and GNN estimators.

Figure 3 demonstrates that the proposed estimator,
LNC, consistently outperforms the other estimators.
Its superiority is most signiﬁcant for the low noise
regime. In that case, both KSG and GNN estimators
are bounded by the sample size while LNC keeps grow-
ing. MST tends to overestimate MI for large noise but
still stops growing after the noise fall below a certain
intensity5. Surprisingly, EXP is the only other estima-
tor that performs comparably with LNC for the linear
relationship (the left most plot in Figure 3). However,
it fails dramatically for all the other relationship types.
See Appendix D for more functional relationship tests.

Convergence rate Figure 4 shows the convergence
of the two estimators ˆIKSG and ˆILN C, at a ﬁxed
(small) noise intensity, as we vary the sample size.
We test the estimators on both two and ﬁve dimen-
sional data for linear and quadratic relationships. We
observe that LNC is doing better overall. In partic-
ular, for linear relationships in 2D and 5D, as well
as quadratic relationships in 2D, the required sample
size for LNC is several orders of magnitude less that
for ˆIKSG. For instance, for the 5D linear relationship
KSG does not converge even for the sample size (105)
while LN C converges to the true value with only 100
samples.

Finally, it is worthwhile to remark on the relatively
slow convergence of LN C for the 5D quadratic ex-
ample. This is because LN C, while relaxing the lo-

4We use the Information Theoretical Estimators Tool-

box (ITE) (Szab´o, 2014) for MST and EXP estimators.

5Even if k = 2 or 3, KSG and GNN estimators still stop

growing after the noise fall below a certain intensity.

Figure 4: Estimated MI using both KSG and LNC
estimators in the number of samples (k = 5 and αk,d =
0.37 for 2D examples; k = 8 and αk,d = 0.12 for 5D
examples)

5.2 Experiments with real-world data

5.2.1 Ranking Relationship Strength

We evaluate the proposed estimator on the WHO
dataset which has 357 variables describing various
socio-economic, political, and health indicators for dif-
ferent countries 6. We calculate the mutual informa-
tion between pairs of variables which have at least 150
samples. Next, we rank the pairs based on their es-
timated mutual information and choose the top 150
pairs with highest mutual information. For these top
150 pairs, We randomly select a fraction ρ of samples
for each pair, hide the rest samples and then recal-
culate the mutual information. We want to see how
mutual information-based rank changes by giving dif-
ferent amount of less data, i.e., varying ρ. A good mu-
tual information estimator should give a similar rank
using less data as using the full data. We compare
our LNC estimator to KSG estimator. Rank similari-
ties are calculated using the standard Spearman’s rank
correlation coeﬃcient described in (Spearman, 1904).
Fig 5 shows the results. We can see that LNC es-
timator outperforms KSG estimator, especially when
the missing data approaches 90%, Spearman correla-
tion drops to 0.4 for KSG estimator, while our LNC

6WHO dataset is publicly available at http://www.

exploredata.net/Downloads

Shuyang Gao, Greg Ver Steeg, Aram Galstyan

estimator still has a relatively high score of 0.7.

Figure 5: Spearman correlation coeﬃcient between the
original MI rank and the rank after hiding some per-
centage of data by KSG and LNC estimator respec-
tively. The 95% conﬁdence bars are obtained by re-
peating the experiment for 200 times.

5.2.2 Finding interesting triplets

We also use our estimator to ﬁnd strong multivari-
ate relationships in the WHO data set. Speciﬁcally,
we search for synergistic triplets (X, Y, Z), where one
of the variables, say Z, can be predicted by knowing
both X and Y simultaneously, but not by using ei-
In other words, we search
ther variable separately.
for triplets (X, Y, Z) such that the pair-wise mutual
information between the pairs I(X : Y ), I(X : Z)
and I(Y : Z) are low, but the multi-information
I(X : Y : Z) is relatively high. We rank rela-
tionships using the following synergy score:7 SS =
I (X : Y : Z)/max {I (X : Y ) , I (Y : Z) , I (Z : X)}.

We select the triplets that have synergy score above
a certain threshold. Figure 6 shows two synergistic
relationships detected by LN C but not by KSG. In
these examples, both KSG and LN C estimators yield
low mutual information for the pairs (X, Y ), (Y, Z)
and (X, Z). However, in contrast to KSG, our estima-
tor yields a relatively high score for multi-information
among the three variables.

For the ﬁrst relationship, the synergistic behavior can
be explained by noting that the ratio of the Total En-
ergy Generation (Y ) to Electricity Generation per Per-
son (Z) essentially yields the size of the population,
which is highly predictive of the Number of Female
Cervical Cancer cases (X). While this example might
seem somewhat trivial, it illustrates the ability of our
method to extract synergistic relationships automati-
cally without any additional assumptions and/or data
preprocessing.

In the second example, LNC predicts a strong synergis-

7Another measure of synergy is given by the so
called “interaction information”: I (X : Y ) + I (Y : Z) +
I (Z : X) − I (X : Y : Z).

(a)

(b)

Figure 6: Two examples of synergistic triplets:
ˆIKSG(X : Y : Z) = 0.14 and ˆILN C(X : Y : Z) = 0.95
for the ﬁrst example; ˆIKSG(X : Y : Z) = 0.05 and
ˆILN C(X : Y : Z) = 0.7 for the second example

tic interaction between Total Cell Phones (Y ), Num-
ber of Female Cervical Cancer cases (Z), and rate of
Tuberculosis deaths (X). Since the variable Z (the
number of female cervical cancer cases) grows with
the total population, Y
Z is proportional to the average
number of cell phones per person. The last plot indi-
cates that a higher number of cell phones per person
are predictive of lower tuberculosis death rate. One
possible explanation for this correlation is some com-
mon underlying cause (e.g., overall economic develop-
ment). Another intriguing possibility is that this ﬁnd-
ing reﬂects recent eﬀorts to use mobile technology in
TB control.8

8See Stop TB Partnership, http://www.stoptb.org.

Eﬃcient Estimation of Mutual Information for Strongly Dependent Variables

6 Related Work

MI estimators There has been a signiﬁcant amount
of recent work on estimating entropic measures such
information from sam-
as divergences and mutual
ples (see this survey (Walters-Williams and Li, 2009)
for an exhaustive list). Khan et al. (2007) com-
pared diﬀerent MI estimators for varying sample sizes
and noise intensity, and reported that for small sam-
ples, the KSG estimator was the best choice over-
all
for relatively low noise intensities, while KDE
performed better at higher noise intensities. Other
approaches include estimators based on Generalized
Nearest-Neighbor Graphs (P´al et al., 2010), minimum
spanning trees (M¨uller et al., 2012), maximum like-
lihood density ratio estimation (Suzuki et al., 2008),
and ensemble methods (Sricharan et al., 2013; Moon
In particular, the latter approach
and Hero, 2014).
works by taking a weighted average of simple density
plug-in estimates such as kNN or KDE. However, it is
upper-bounded by the largest value among its simple
density estimates. Therefore, this method would still
underestimate the mutual information when it goes
larger as discussed before.

It has been recognized that kNN-based entropic esti-
mators underestimate probability density at the sam-
ple points that are close to the boundary of sup-
port (Liiti¨ainen et al., 2010). Sricharan et al. (2012)
proposed an bipartite plug-in(BPI) estimator for non-
linear density functionals that extrapolates the density
estimates at interior points that are close to the bound-
ary points in order to compensate the boundary bias.
However, this method requires to identify boundary
points and interior points which becomes diﬃcult to
distinguish as mutual information gets large that al-
most all the points are close to the boundary. Singh
and Poczos (2014) used a ”mirror image” kernel den-
sity estimator to escape the boundary eﬀect, but their
estimator relies on the knowledge of the support of the
densities, and assumes that the boundary are parallel
to the axes.

Mutual Information and Equitability Reshef
et al. (2011) introduced a property they called “suit-
ability” for a measure of correlation. If two variables
are related by a functional form with some noise, eq-
uitable measures should reﬂect the magnitude of the
noise while being insensitive to the form of the func-
tional relationship. They used this notion to justify a
new correlation measure called MIC. Based on com-
parisons with MI using the KSG estimator, they con-
cluded that MIC is “more equitable” for comparing
relationship strengths. While several problems (Si-
mon and Tibshirani, 2014; Gorﬁne et al.) and alter-
natives (Heller et al., 2013; Sz´ekely et al., 2009) were

pointed out, Kinney and Atwal (KA) were the ﬁrst
to point out that MIC’s apparent superiority to MI
was actually due to ﬂaws in estimation (Kinney and
Atwal, 2014). A more careful deﬁnition of equitabil-
ity led KA to the conclusion that MI is actually more
equitable than MIC. KA suggest that the poor per-
formance of the KSG estimator that led to Reshef et.
al.’s mistaken conclusion could be improved by using
more samples for estimation. However, here we showed
that the number of samples required for KSG is pro-
hibitively large, but that this diﬃculty can be over-
come by using an improved MI estimator.

7 Conclusion

The problem of deciding whether or not two vari-
ables are independent is a historically signiﬁcant en-
deavor. In that context, research on mutual informa-
tion estimation has been geared towards distinguishing
weak dependence from independence. However, mod-
ern data mining presents us with problems requiring a
totally diﬀerent perspective. It is not unusual to have
thousands of variables which could have millions of
potential relationships. We have insuﬃcient resources
to examine each potential relationship so we need an
assumption-free way to pick out only the most promis-
ing relationships for further study. Many applications
have this ﬂavor including the health indicator data
considered above as well as gene expression microarray
data, human behavior data, economic indicators, and
sensor data, to name a few.

How can we select the most interesting relationships
to study? Classic correlation measures like the Pear-
son coeﬃcient bias the results towards linear variables.
Mutual information gives a clear and general basis for
comparing the strength of otherwise dissimilar vari-
ables and relationships. While non-parametric mutual
information estimators exist, we showed that strong
relationships require exponentially many samples to
accurately measure using some of these techniques.

We introduced a non-parametric mutual information
estimator that can measure the strength of nonlinear
relationships even with small sample sizes. We have in-
corporated these novel estimators into an open source
entropy estimation toolbox 9.As the amount and vari-
ety of available data grows, general methods for iden-
tifying strong relationships will become increasingly
necessary. We hope that the developments suggested
here will help to address this need.

9https://github.com/BiuBiuBiLL/MIE

Shuyang Gao, Greg Ver Steeg, Aram Galstyan

Acknowledgements

This research was supported in part by DARPA grant
No. W911NF–12–1–0034.

References

T.M. Cover and J.A. Thomas. Elements of information

theory. Wiley-Interscience, 2006.

Malka Gorﬁne, Ruth Heller, and Yair Heller. Com-
ment on detecting novel associations in large data
sets.
technion. ac.
(available at http://emotion.
il/ gorﬁnm/ﬁles/science6. pdf on 11 Nov. 2012).

Ruth Heller, Yair Heller, and Malka Gorﬁne. A consis-
tent multivariate test of association based on ranks of
distances. Biometrika, 100(2):503–510, 2013.

Shiraj Khan, Sharba Bandyopadhyay, Auroop R. Gan-
guly, Sunil Saigal, David J. Erickson, Vladimir Pro-
topopescu, and George Ostrouchov. Relative perfor-
mance of mutual information estimation methods for
quantifying the dependence among short and noisy data.
Phys. Rev. E, 76:026209, Aug 2007.
doi: 10.1103/
PhysRevE.76.026209. URL http://link.aps.org/doi/
10.1103/PhysRevE.76.026209.

J. Kinney and G. Atwal. Equitability, mutual information,
and the maximal information coeﬃcient. Proceedings
of the National Academy of Sciences, 111(9):3354–3359,
2014.

A. Kraskov, H. St¨ogbauer, and P. Grassberger. Estimating
mutual information. Phys. Rev. E, 69:066138, 2004. doi:
10.1103/PhysRevE.69.066138. URL http://link.aps.
org/doi/10.1103/PhysRevE.69.066138.

Elia Liiti¨ainen, Amaury Lendasse, and Francesco Corona.
A boundary corrected expansion of the moments of near-
est neighbor distributions. Random Struct. Algorithms,
37(2):223–247, September 2010. ISSN 1042-9832. doi:
10.1002/rsa.v37:2. URL http://dx.doi.org/10.1002/
rsa.v37:2.

K.R. Moon and A.O. Hero. Ensemble estimation of mul-
tivariate f-divergence.
In Information Theory (ISIT),
2014 IEEE International Symposium on, pages 356–360,
June 2014. doi: 10.1109/ISIT.2014.6874854.

Andreas M¨uller, Sebastian Nowozin, and Christoph Lam-
pert.
Information theoretic clustering using minimum
spanning trees. Pattern Recognition, pages 205–215,
2012.

F. Nielsen and R. Nock. Entropies and cross-entropies of
exponential families. In 17th IEEE International Con-
ference on Image Processing (ICIP), pages 3621–3624.
IEEE, 2010.

D´avid P´al, Barnab´as P´oczos, and Csaba Szepesv´ari. Esti-
mation of r´enyi entropy and mutual information based
on generalized nearest-neighbor graphs. In Advances in
Neural Information Processing Systems 23, pages 1849–
1857. Curran Associates, Inc., 2010.

Fernando P´erez-Cruz. Estimation of information theoretic
measures for continuous random variables. In Proceed-
ings of NIPS-08, pages 1257–1264, 2008.

David N Reshef, Yakir A Reshef, Hilary K Finucane,
Sharon R Grossman, Gilean McVean, Peter J Turn-
baugh, Eric S Lander, Michael Mitzenmacher, and Par-

dis C Sabeti. Detecting novel associations in large data
sets. science, 334(6062):1518–1524, 2011.

Sam T Roweis and Lawrence K Saul. Nonlinear dimension-
ality reduction by locally linear embedding. Science, 290
(5500):2323–2326, 2000.

Noah Simon and Robert Tibshirani. Comment on” detect-
ing novel associations in large data sets” by reshef et al,
science dec 16, 2011. arXiv preprint arXiv:1401.7645,
2014.

Harshinder Singh, Neeraj Misra, Vladimir Hnizdo, Adam
Fedorowicz, and Eugene Demchuk. Nearest neighbor
estimates of entropy. American Journal of Mathemat-
ical and Management Sciences, 23(3-4):301–321, 2003.
doi: 10.1080/01966324.2003.10737616. URL http://
dx.doi.org/10.1080/01966324.2003.10737616.

Shashank Singh and Barnabas Poczos. Generalized expo-
nential concentration inequality for renyi divergence es-
timation. In Proceedings of the 31st International Con-
ference on Machine Learning (ICML-14), pages 333–
341, 2014. URL http://machinelearning.wustl.edu/
mlpapers/papers/icml2014c1_singh14.

C. Spearman. The proof and measurement of association
between two things. The American Journal of Psy-
chology, 15(1):pp. 72–101, 1904. ISSN 00029556. URL
http://www.jstor.org/stable/1412159.

K. Sricharan, R. Raich, and A.O. Hero. Estimation of
nonlinear functionals of densities with conﬁdence. In-
formation Theory, IEEE Transactions on, 58(7):4135–
ISSN 0018-9448. doi: 10.1109/TIT.
4159, July 2012.
2012.2195549.

K. Sricharan, D. Wei, and A.O. Hero. Ensemble estima-
tors for multivariate entropy estimation.
Information
Theory, IEEE Transactions on, 59(7):4374–4388, July
2013. ISSN 0018-9448. doi: 10.1109/TIT.2013.2251456.

Milan Studen`y and Jirina Vejnarov´a. The multiinforma-
tion function as a tool for measuring stochastic depen-
dence. In Learning in graphical models, pages 261–297.
Springer, 1998.

Taiji Suzuki, Masashi Sugiyama, Jun Sese, and Takafumi
Kanamori. Approximating mutual information by maxi-
mum likelihood density ratio estimation. In Yvan Saeys,
Huan Liu, Iaki Inza, Louis Wehenkel, and Yves Van
de Peer, editors, FSDM, volume 4 of JMLR Proceedings,
pages 5–20. JMLR.org, 2008.

Zolt´an Szab´o.

Information theoretical estimators tool-
box. Journal of Machine Learning Research, 15:283–287,
2014. (https://bitbucket.org/szzoli/ite/).

G´abor J Sz´ekely, Maria L Rizzo, et al. Brownian distance
covariance. The annals of applied statistics, 3(4):1236–
1265, 2009.

Ulrike Von Luxburg and Morteza Alamgir. Density es-
timation from unweighted k-nearest neighbor graphs: a
roadmap. In Advances in Neural Information Processing
Systems, 2013.

Janett Walters-Williams and Yan Li. Estimation of mu-
tual information: A survey. In Rough Sets and Knowl-
edge Technology, volume 5589 of Lecture Notes in Com-
puter Science, pages 389–396. Springer Berlin Heidel-
berg, 2009.
ISBN 978-3-642-02961-5. doi: 10.1007/
978-3-642-02962-2 49. URL http://dx.doi.org/10.
1007/978-3-642-02962-2_49.

Eﬃcient Estimation of Mutual Information for Strongly Dependent Variables

Q. Wang, S.R. Kulkarni, and S. Verd´u. Divergence es-
timation for multidimensional densities via k-nearest-
neighbor distances. IEEE Trans. Inf. Theor., 55:2392–
ISSN 0018-9448. doi: 10.1109/TIT.
2405, May 2009.
2009.2016060.
URL http://dl.acm.org/citation.
cfm?id=1669487.1669521.

Satosi Watanabe. Information theoretical analysis of mul-
tivariate correlation. IBM Journal of research and de-
velopment, 4(1):66–82, 1960.

Joseph E Yukich and Joseph Yukich. Probability theory
of classical Euclidean optimization problems. Springer
Berlin, 1998.

Zhenyue Zhang and Hongyuan Zha. Principal manifolds
and nonlinear dimension reduction via local tangent
space alignment. SIAM Journal of Scientiﬁc Comput-
ing, 26:313–338, 2002.

Shuyang Gao, Greg Ver Steeg, Aram Galstyan

Supplementary Material for “Eﬃcient
Estimation of Mutual Information for
Strongly Dependent Variables”

A Proof of Theorem 2

Notice that for a ﬁxed sample point x(i), its k-nearest-
(cid:0)x(i)(cid:1) is always equal to or larger
neighbor distance rk
than the k-nearest-neighbor distance of at the same
point x(i) projected into a sub-dimension j, i.e., for
any i, j, we have

(cid:16)

x(i)(cid:17)

rk

(cid:62) rk

(cid:17)

(cid:16)

x(i)
j

(A.1)

(cid:98)Ik (x) =

equality for (cid:98)IkN N,k (x):

(cid:98)IkN N,k (x) ≤ (d − 1) log

+ O (d log d)

(cid:19)

(cid:18) N − 1
k

≤

(d − 1) log (N − 1) + O (d log d)

Requiring that |(cid:98)IkN N,k (x) − I(x)| ≤ ε, we obtain,

N ≥ C exp

(cid:19)

(cid:18) I (x) − (cid:15)
d − 1

+ 1

(A.3)

where C is a constant which scales like O( 1

d ).

B Derivation of Eq. 17

The naive kNN or KSG estimator can be written as:

1
N

N
(cid:88)

i=1

log

P (x(i))
V (i)
(cid:16)

P

x(i)
j
Vj (i)

(cid:17)

d
(cid:81)
j=1

(B.1)

where P (x(i)) is the probability mass around the k-
nearest-neighborhood at x(i) and P (x(i)
j ) is the prob-
ability mass around the k-nearest-neighborhood (or
nxj (i)-nearest-neighborhood for KSG) at x(i) pro-
jected into j-th dimension. Also, V (i) and Vj(i) denote
the volume of the kNN ball(or hype-rectangle in KSG)
in the joint space and projected subspaces respectively.

Now our local nonuniform correction method replaces
the volume V (i) in Eq. B.1 with the corrected volume
V (i), thus, our estimator is obtained as follows:

P (x(i))
V (i)
(cid:16)

(cid:17)

P

x(i)
j
Vj (i)

d
(cid:81)
j=1
P (x(i))
V (i) × V (i)
x(i)
j
Vj (i)

V (i)
(cid:17)

d
(cid:81)
j=1

P

(cid:16)

1
N

N
(cid:88)

i=1

log

V (i)
V (i)

=

1
N

N
(cid:88)

i=1

log

= (cid:98)Ik (x) −

(B.2)

Using Eq. A.1, we get the upper bound of (cid:98)IkN N,k (x)
as follows:
(cid:98)IkN N,k (x) = (cid:98)I (cid:48)

=

=

N
(cid:88)

kN N,k (x) − (d − 1) γk
(cid:98)pk
d
(cid:81)
j=1

(cid:0)x(i)(cid:1)
(cid:16)

x(i)
j

(cid:98)pk

log

i=1

(cid:17)

1
N

− (d − 1) γk

1
N

N
(cid:88)

i=1

log

k
N −1

Γ(d/2)+1
πd/2

rk

d
(cid:81)
j=1

k
N −1

Γ(1/2)+1
π1/2

(cid:0)x(i)(cid:1)−d
(cid:16)
x(i)
j

rk

(cid:17)−1

− (d − 1) (ψ (k) − log k)

≤ (d − 1) log

(cid:19)

(cid:18) N − 1
k

+ log

Γ (d/2) + 1
(Γ (1/2) + 1)d

− (d − 1) (ψ (1) − log 1)

(A.2)

The last inequality is obtained by noticing that ψ(k)−
log(k) is a monotonous decreasing function.

Also, we have,

− (d − 1) γk

≤ (d − 1) log

(cid:19)

(cid:18) N − 1
k

+ log

Γ (d/2) + 1
(Γ (1/2) + 1)d

(cid:98)ILN C,k (x) =

1
N

N
(cid:88)

i=1

log

log

Γ (d/2) + 1
(Γ (1/2) + 1)d = log (Γ (d/2) + 1) − d log (Γ (d/2) + 1)

C Empirical Evaluation for αk,d

(cid:19)d/2+1/2(cid:33)

(cid:32)√

< log

2π

(cid:18) d/2 + 1/2
e
(cid:17)

−d log

(cid:16)

1
2 + 1

π

= O (d log d)

The inequality above is obtained by using the bound
of gamma function that,
√

(cid:19)x+1/2

Γ (x + 1) <

2π

(cid:18) x + 1/2
e

Therefore, reconsidering A.2, we get the following in-

Suppose we have a uniform distribution on the d di-
mensional (hyper)rectangle with volume V . We sam-
ple k points from this uniform distribution. We per-
form PCA using these k points to get a new basis.
After rotating into this new basis, we ﬁnd the volume,
¯V , of the smallest rectilinear rectangle containing the
points. By chance, we will typically ﬁnd ¯V < V , even
though the distribution is uniform. This will lead to
us to (incorrectly) apply a local non-uniformity cor-
rection. Instead, we set a threshold αk,d and if ¯V /V is
above the threshold, we assume that the distribution

Eﬃcient Estimation of Mutual Information for Strongly Dependent Variables

Figure C.1 shows empirical value of ˆαk,d for diﬀerent
(k, d) pairs. We can see that for a ﬁxed dimension
d, ˆαk,d grows as k increases, meaning that ¯V must be
closer to V to accept the null hypothesis of uniformity.
We also ﬁnd that ˆαk,d decreases as the dimension d in-
creases, indicating that for a ﬁxed k, ¯V becomes much
smaller than V when points are drawn from a uniform
distribution in higher dimensions.

D More Functional Relationship Tests

in Two Dimensions

We have tested together twenty-one functional rela-
tionships described in Reshef et al. (2011); Kinney and
Atwal (2014), we show six of them in Section 5. The
complete results are shown in Figure D.1. Detailed de-
scription of the functions can be found in Table S1 of
Supporting Information in Kinney and Atwal (2014).

is locally uniform. Setting α involves a trade-oﬀ. If
it is set too high, we will incorrectly conclude there is
local non-uniformity and therefore over-estimate the
mutual information. If we set α too low, we will lose
statistical power for “medium-strength” relationships
(though very strong relationships will still lead to val-
ues of ¯V /V smaller than α).

In practice, we determine the correct value of αk,d em-
pirically. We look at the probability distribution of
¯V /V that occurs when the true distribution is uniform.
We set α conservatively so that when the true distribu-
tion is uniform, our criteria rejects this hypothesis with
small probability, (cid:15). Speciﬁcally, we do a number of tri-

als, N , and set ˆαk,d such that

N
(cid:80)
i=1

(cid:16) ¯Vi
Vi

I

(cid:17)

< ˆαk,d

/N < (cid:15)

where (cid:15) is a relatively small value.
In practice, we
chose (cid:15) = 5 × 10−3 and N = 5 × 105. The following
algorithm describes this procedure:

Algorithm C.1 Estimating αk,d for LNC

Input: parameter d (dimension), k (nearest neigh-
bor), N , (cid:15)
Output: ˆαk,d
set array a to be NULL
repeat

Randomly choose a uniform distribution sup-
ported on d dimensional (hyper) rectangle, denote
its volume to be V

Draw k points from this uniform distribution, get

the correcting volume ¯V after doing PCA

add the ratio ¯V

V to array a

until above procedure repeated N times
ˆαk,d ← (cid:100)(cid:15)N (cid:101) th smallest number in a

Figure C.1: (cid:98)αk,d as a function of k. k ranges over
[d, 20] for each dimension d.

Shuyang Gao, Greg Ver Steeg, Aram Galstyan

s
e
i
t
i
s
n
e
t
n
i

e
s
i
o
n

t
n
e
r
e
ﬀ
i
d

h
t
i
w
s
p
i
h
s
n
o
i
t
a
l
e
r

l
a
n
o
i
t
c
n
u
f

e
n
o
-
y
t
n
e
w
T

.
s
r
o
t
a
m

i
t
s
e

P
X
E

,

T
S
M

,

N
N
G

,

G
S
K

,

C
N
L

f
o

s
t
s
e
t

n
o
i
t
a
m
r
o
f
n
I

l
a
u
t
u
M

:
1
.
D
e
r
u
g
i
F

r
o
b
h
g
i
e
n

t
s
e
r
a
e
n

,
s
r
o
t
a
m

i
t
s
e

C
N
L

d
n
a

N
N
G

,

G
S
K

r
o
F

.
)
s
t
o
l
p

e
h
t

f
o

s
i
x
a

X

n

i

n
w
o
h
s

s
a
(
s
e
i
r
a
v

σ

e
r
e
h
w
]
2
/
σ

,
2
/
σ
−
U

[

m
r
o
f

e
h
t

s
a
h

e
s
i
o
N

.
d
e
t
s
e
t

e
r
a

.

i

p
h
s
n
o
i
t
a
l
e
r

l
a
n
o
i
t
c
n
u
f

y
s
i
o
n

h
c
a
e

r
o
f

s
t
n
i
o
p

a
t
a
d

0
0
0
,
5
=
N
g
n
i
s
u

e
r
a

e

W

.
5
=
k

r
e
t
e
m
a
r
a
p

Eﬃcient Estimation of Mutual Information for
Strongly Dependent Variables

5
1
0
2
 
r
a

M
 
5
 
 
]
T
I
.
s
c
[
 
 
3
v
3
0
0
2
.
1
1
4
1
:
v
i
X
r
a

Shuyang Gao
Information Sciences Institute
University of Southern California
sgao@isi.edu

Greg Ver Steeg
Information Sciences Institute
University of Southern California
gregv@isi.edu

Aram Galstyan
Information Sciences Institute
University of Southern California
galstyan@isi.edu

Abstract

We demonstrate that a popular class of non-
parametric mutual information (MI) estima-
tors based on k-nearest-neighbor graphs re-
quires number of samples that scales expo-
nentially with the true MI. Consequently, ac-
curate estimation of MI between two strongly
dependent variables is possible only for pro-
hibitively large sample size. This important
yet overlooked shortcoming of the existing es-
timators is due to their implicit reliance on
local uniformity of the underlying joint dis-
tribution. We introduce a new estimator that
is robust to local non-uniformity, works well
with limited data, and is able to capture rela-
tionship strengths over many orders of mag-
nitude. We demonstrate the superior perfor-
mance of the proposed estimator on both syn-
thetic and real-world data.

1 Introduction

As a measure of the dependence between two random
variables, mutual information is remarkably general
and has several intuitive interpretations (Cover and
Thomas, 2006), which explains its widespread use in
statistics, machine learning, and computational neu-
roscience; see (Wang et al., 2009) for a list of various
applications. In a typical scenario, we do not know the
underlying joint distribution of the random variables,
and instead need to estimate mutual information using
i.i.d. samples from that distribution. A naive approach
to this problem is to ﬁrst estimate the underlying prob-
ability density, and then calculate mutual information

Copyright 2014 by the authors.

using its deﬁnition. Unfortunately, estimating joint
densities from a limited number of samples is often
infeasible in many practical settings.

A diﬀerent approach is to estimate mutual informa-
tion directly from samples in a non-parametric way,
without ever describing the entire probability density.
The main intuition behind such direct estimators is
that evaluating mutual information can in principle
be a more tractable problem than estimating the den-
sity over the whole state space (P´erez-Cruz, 2008).
The most popular class of estimators taking this ap-
proach is the k-nearest-neighbor(kNN) based estima-
tors. One example of such estimator is due to Kraskov,
St¨ogbauer, and Grassberger (referred to as the KSG
estimator from here on) (Kraskov et al., 2004), which
has been extended to generalized nearest neighbors
graphs (P´al et al., 2010).

Our ﬁrst contribution is to demonstrate that kNN-
based estimators suﬀer from a critical yet overlooked
ﬂaw. Namely, we illustrate that if the true mutual
information is I, then kNN-based estimators requires
exponentially many (in I) samples for accurate esti-
mation. This means that strong relationships are ac-
tually more diﬃcult to measure. This counterintuitive
property reﬂects the fact that most work on mutual
information estimation has focused on estimators that
are good at detecting independence of variables rather
In the
than precisely measuring strong dependence.
age of big data, it is often the case that many strong
relationships are present in the data and we are in-
terested in picking out only the strongest ones. MI
estimation will perform poorly for this task if their
accuracy is low in the regime of strong dependence.

We show that the undesired behavior of previous kNN-
based MI estimators can be attributed to the assump-
tion of local uniformity utilized by those estimators,
which can be violated for suﬃciently strong (almost
deterministic) dependencies. As our second major con-
tribution, we suggest a new kNN estimator that relaxes
the local uniformity condition by introducing a correc-

Eﬃcient Estimation of Mutual Information for Strongly Dependent Variables

tion term for local non-uniformity. We demonstrate
empirically that for strong relationships, the proposed
estimator needs signiﬁcantly fewer samples for accu-
rately estimating mutual information.

where
(cid:98)pk

In Sec. 2, we introduce kNN-based non-parametric en-
tropy and mutual information estimators. In Sec. 3,
we demonstrate the limitations of kNN-based mutual
information estimators. And then we suggest a correc-
tion term to overcome these limitations in Sec. 4. In
Sec. 5, we show empirically using synthetic and real-
world data that our method outperforms existing tech-
niques. In particular, we are able to accurately mea-
sure strong relationships using smaller sample sizes.
Finally, we conclude with related work and discussion.

2 kNN-based Estimation of Entropic

Measures

In this section, we will ﬁrst introduce the naive kNN
estimator for mutual information, which is based on
an entropy estimator due to (Singh et al., 2003), and
show its theoretical properties. Next, we focus on a
popular variant of kNN estimators, called KSG esti-
mator (Kraskov et al., 2004).

2.1 Basic Deﬁnitions

Let x = (x1, x2, ..., xd) denote a d-dimensional ab-
solute continuous random variable whose probability
density function is deﬁned as p : Rd → R and marginal
densities of each xj are deﬁned as pj : R → R, j =
1, . . . , d. Shannon Diﬀerential Entropy and Mutual In-
formation are deﬁned in the usual way:

H (x) = −

p (x) log p (x) dx

(1)

I (x) =

p (x) log

dx

(2)

p (x)

d
(cid:81)
j=1

pj (xj)

We use natural logarithms so that information is mea-
sured in nats. For d > 2, the generalized mutual in-
formation is also called total correlation (Watanabe,
1960) or multi-information (Studen`y and Vejnarov´a,
1998).
Given N i.i.d. samples, X = (cid:8)x(i)(cid:9)n
i=1, drawn from
p(x), the task is to construct a mutual information
estimator ˆI (x) based on these samples.

(cid:90)

Rd

(cid:90)

Rd

(cid:16)

(cid:16)

=

x(i)(cid:17)

x(i)(cid:17)−d

k
n − 1

Γ (d/2 + 1)
(4)
πd/2
(cid:0)x(i)(cid:1) in Eq. 4 is the Euclidean distance from x(i)
rk
to its kth nearest neighbor in X . By introducing a
correction term, an asymptotic unbiased estimator is
obtained:

rk

(cid:98)HkN N,k (x) = −

1
n

n
(cid:88)

i=1

(cid:16)

x(i)(cid:17)

− γk

log (cid:98)pk

where

γk =

kk
(k − 1)!

(cid:90) ∞

0

log (x) xk−1e−kxdx = ψ(k) − log(k)

(5)

(6)

where ψ(·) represents the digamma function.

The following theorem shows asymptotic unbiasedness
of (cid:98)HkN N,k (x) according to (Singh et al., 2003).
Theorem 1 (kNN entropy estimator, asymptotic un-
biasedness, (Singh et al., 2003)). Assume that x is ab-
solutely continuous and k is a positive integer, then

E

(cid:104)

(cid:105)
(cid:98)HkN N,k (x)

lim
n→∞

= HkN N,k(x)

(7)

i.e., this entropy estimator is asymptotically unbiased.

From Entropy to Mutual Information To con-
struct a mutual information estimator from an entropy
estimator is straightforward by combining entropy es-
timators using the identity (Cover and Thomas, 2006):

d
(cid:88)

i=1

I(x) =

H (xi) − H (x)

(8)

Combining Eqs. 3 and 8, we have,
n
(cid:88)

(cid:98)I (cid:48)
kN N,k (x) =

log

(cid:0)x(i)(cid:1)
(cid:98)pk
(cid:17)
(cid:16)
x(i)
(cid:98)pk
2

(cid:17)

(cid:16)

x(i)
1

(cid:98)pk

1
n

i=1

(cid:17)

(cid:16)

x(i)
d

...(cid:98)pk

(9)
where x(i)
j denotes the point projected into jth dimen-
sion in x(i) and (cid:98)pk(x(i)
j ) represents the marginal kNN
density estimator projected into jth dimension of x(i).

Similar to Eq. 5, we can also construct an asymptoti-
cally unbiased mutual information estimator based on
Theorem 1:

(cid:98)IkN N,k = (cid:98)I (cid:48)

kN N,k − (d − 1) γk

(10)

Corollary 1 (kNN MI estimator, asymptotic unbi-
asedness). Assume that x is absolute continuous and
k is a positive integer, then:
(cid:105)
E
(cid:98)IkN N,k (x)

= IkN N,k (x)

(11)

(cid:104)

lim
n→∞

2.2 Naive kNN Estimator

2.3 KSG Estimator

Entropy Estimation The naive kNN entropy esti-
mator is as follows:

(cid:98)H (cid:48)

kN N,k (x) = −

1
n

n
(cid:88)

i=1

(cid:16)

x(i)(cid:17)

log (cid:98)pk

The KSG mutual
information estimator (Kraskov
et al., 2004) is a popular variant of the naive kNN es-
timator. The general principle of KSG is that for each
density estimator in diﬀerent spaces, we would like to

(3)

Shuyang Gao, Greg Ver Steeg, Aram Galstyan

in (Kraskov et al., 2004)). Adding the entropy estima-
tors together with these choices yields the following.

ˆIKSG,k(x) ≡ (d − 1)ψ(N ) + ψ(k) − (d − 1)/k

−

1
N

N
(cid:88)

d
(cid:88)

i=1

j=1

ψ(nxj (i))

(15)

use the similar length-scales for k-nearest-neighbor dis-
tance as in the joint space so that the bias would be
approximately smaller. Although the theoretical prop-
erties of this estimator is unknown, it has a relatively
good performance in practice, see (Khan et al., 2007)
for a comparison of diﬀerent estimation methods. Un-
like naive kNN estimator, the KSG estimator uses the
max-norm distance instead of L2-norm. In particular,
if (cid:15)i,k is twice the (max-norm) distance to the k-th
nearest neighbor of x(i), it can be shown that the ex-
pectation value (over all ways of drawing the surround-
ing N − 1 points) of the log probability mass within
the box centered at x(i) is given by this expression.

(cid:34)
E(cid:15)i,k

log

(cid:90)

(cid:35)

f (x) dx

= ψ(k) − ψ(N )

|x−x(i)|∞≤(cid:15)i,k/2

We use ψ to represent the digamma function. If we
assume that the density inside the box (with sides of
length (cid:15)i,k) is constant, then the integral becomes triv-
ial and we ﬁnd that,
log(f (xi)(cid:15)d

(13)
Rearranging and taking the mean over − log f (x(i))
leads us to the following entropy estimator:

i,k) = ψ(k) − ψ(N ).

(12)

(a)

(b)

Figure 1: Centered at a given sample point, x(i), we
show the max-norm rectangle containing k nearest
neighbors (a) for points drawn from a uniform distri-
bution, k = 3, and (b) for points drawn from a strongly
correlated distribution, k = 4.

ˆHKSG,k(x) ≡ ψ(N ) − ψ(k) +

log (cid:15)i,k

Estimators

3 Limitations of kNN-based MI

d
N

N
(cid:88)

i=1

(14)

Note that k, deﬁning the size of neighborhood to use
in local density estimation, is a free parameter. Using
smaller k should be more accurate, but larger k re-
duces the variance of the estimate (Khan et al., 2007).
While consistent density estimation requires k to grow
with N (Von Luxburg and Alamgir, 2013), entropy es-
timates converge almost surely for any ﬁxed k (Wang
et al., 2009; P´erez-Cruz, 2008) under some weak con-
ditions.1

To estimate the mutual information, in the joint x
space we set k, the size of the neighborhood, which
determines (cid:15)i,k for each point x(i). Next, we consider
the smallest rectilinear hyper-rectangle that contains
these k points, which has sides of length (cid:15)xj
i,k for each
marginal direction xj. We refer to this as the “max-
norm rectangle” (as shown in Fig. 1(a)). Let nxj be
the number of points at a distance less than or equal
to (cid:15)xj
i,k/2 in the xj-subspace. For each marginal en-
tropy estimate, we use nxj (i) instead of k to set the
neighborhood size at each point. Finally, in the joint
space using a rectangle instead of a box in Eq. 14 leads
to a correction term of size (d − 1)/k (details given

In this section, we demonstrate a signiﬁcant ﬂaw in
kNN-based MI estimators which is summarized in the
following theorems. We then explain why these esti-
mators fail to accurately estimate mutual information
unless the correlations are relatively weak or the num-
ber of samples is large.

Theorem 2. For any d-dimensional absolute contin-
uous probability density function, p(x), for any k ≥ 1,
for the estimated mutual information to be close to
the true mutual information, | ˆIkN N,k(x) − I(x)| ≤ ε,
requires that the number of samples, N , is at least,
N ≥ C exp
+ 1, where C is a constant which
scales like O( 1

(cid:16) I(x)−ε
d−1
d ).

(cid:17)

The proof of Theorem 2 is shown in the Appendix A.

Theorem 3. For any d-dimensional absolute contin-
uous probability density function, p(x), for any k ≥ 1,
for the estimated mutual information to be close to
the true mutual information, | ˆIKSG,k(x) − I(x)| ≤ ε,
requires that the number of samples, N , is at least,
N ≥ C exp

+ 1, where C = e− k−1
k .

(cid:17)

(cid:16) I(x)−ε
d−1

1This assumes the probability density is absolutely con-
tinuous but see (P´al et al., 2010) for some technical con-
cerns.

Proof. Note that ψ(n) = Hn−1 − γ, where Hn is the
n-th harmonic number and γ ≈ 0.577 is the Euler-

Eﬃcient Estimation of Mutual Information for Strongly Dependent Variables

Mascheroni constant.
(cid:98)IKSG,k(x) ≤ (d − 1)ψ(N ) + ψ(k) − (d − 1)/k
N
(cid:88)

d
(cid:88)

−

1
N

ψ (k)

i=1

j=1

= (d − 1)(ψ(N ) − ψ(k) − 1/k)
= (d − 1)(HN −1 − γ − Hk−1 + γ − 1/k)
≤ (d − 1)(log(N − 1) + (k − 1)/k)

(16)
The ﬁrst inequality is obtained from Eq. 15 by ob-
serving that nxj (i) ≥ k for any i, j and that ψ(k) is
a monotonically increasing function. And the last in-
equality is obtained by dropping the term −Hk−1 ≤ 0
and using the well-known upper bound HN ≤ log N +
1. Requiring that | ˆIKSG,k(x) − I(x)| < ε, we obtain
N ≥ C exp

+ 1, where C = e− k−1
k .

(cid:17)

(cid:16) I(x)−ε
d−1

The above theorems state that for any ﬁxed dimension-
ality, the number of samples needed for estimating mu-
tual information I(x) increases exponentially with the
magnitude of I(x). From the point of view of determin-
ing independence, i.e., distinguishing I(x) = 0 from
I(x) (cid:54)= 0, this restriction is not particularly troubling.
However, for ﬁnding strong signals in data it presents a
major barrier. Indeed, consider two random variables
X and Y , where X ∼ U(0, 1) and Y = X + ηU(0, 1).
When η → 0, the relationship between X and Y be-
comes nearly functional, and the mutual information
diverges as I(X : Y ) → log 1
η . As a consequence, the
number of samples needed for accurately estimating
I(X : Y ) diverges as well. This is depicted in Fig. 2
where we compare the empirical lower bound to our
theoretical bound given by Theorem 3. It can be seen
that the theoretical bounds are rather conservative,
but they have the same exponentially growing rates
comparing to the empirical ones.

What is the origin of this undesired behavior? An intu-
itive and general argument comes from looking at the
assumption of local uniformity in kNN-based estima-
tors. In particular, both naive kNN and KSG estima-
tors approximate the probability density in the kNN
ball or max-norm rectangle containing the k nearest
neighbors with uniform density. If there are strong re-
lationships (in the joint x space, the density becomes
more singular), then we can see in Fig. 1(b) that the
uniform assumption becomes problematic.

4

Improved kNN-based Estimators

In this section, we suggest a class of kNN-based esti-
mators that relaxes the local uniformity assumption.

Figure 2: A semi-logarithmic plot of Ns (number of
required samples to achieve an error at most ε) for
KSG estimator for diﬀerent values of I(X : Y ). We
set ε = 0.1, k = 1.

Local Nonuniformity Correction (LNC) Our
second contribution is to provide a general method
for overcoming the limitations above. Considering the
ball (in naive kNN estimator) or max-norm hyper-
rectangle (in KSG estimator) around the point x(i)
which contains k nearest neighbors, let us denote this
region of the space with V(i) ⊂ Rd, whose volume
is V (i). Instead of assuming that the density is uni-
form inside V(i) around the point x(i), we assume
that there is some subset, ¯V(i) ⊆ V(i) with volume
¯V (i) ≤ V (i) on which the density is constant, i.e.,
ˆp(x(i)) =
. This is illustrated with a shaded re-
gion in Fig. 1(b). We now repeat the derivation above
using this altered assumption about the local density
around each point for ˆH(x). We make no changes
to the entropy estimates in the marginal subspaces.
Based on this idea, we get a general correction term
for kNN-based MI estimators (see Appendix B for the
details of derivation):

I[x∈ ¯V(i)]
¯V (i)

ˆILN C(x) = ˆI(x) −

(17)

1
N

N
(cid:88)

i=1

log

¯V (i)
V (i)

where ˆI(x) can be either ˆIkN N (x) or ˆIKSG(x).

If the local density in the k-nearest-neighbor region
V(i) is highly non-uniform, as is the case for strongly
related variables like those in Fig. 1(b), then the pro-
posed correction term will improve the estimate. For
instance, if we assume that relationships in data are
smooth, functional relationships plus some noise, the
correction term will yield signiﬁcant improvement as
demonstrated empirically in Sec. 5. We note that this
correction term is not bounded by N , but rather by
our method of estimating ¯V . Next, we will give one
concrete implementation of this idea by focusing on
the modiﬁcation of KSG estimator.

Shuyang Gao, Greg Ver Steeg, Aram Galstyan

Estimating Nonuniformity by Local PCA
With the correction term in Eq. 17, we have trans-
formed the problem into that of ﬁnding a local volume
on which we believe the density is positive. Regard-
ing KSG estimator, instead of a uniform distribution
within the max-norm rectangle in the neighborhood
around the point x(i), we look for a small, rotated
(hyper)rectangle that covers the neighborhood of x(i).
The volume of the rotated rectangle is obtained by
doing a localized principle component analysis around
all of x(i)’s k nearest neighbors, and then multiply-
ing the maximal axis values together in each principle
component after the k points are transformed to the
new coordinate system2. The key advantage of our
proposed estimator is as follows: while KSG assumes
local uniformity of the density over a region contain-
ing k nearest neighbors of a particular point, our es-
timator relies on a much weaker assumption of local
linearity over the same region. Note that the local
linearity assumption has also been widely adopted in
the manifold learning, for example, local linear embed-
ding(LLE) (Roweis and Saul, 2000) and local tangent
space alignment(LTSA) (Zhang and Zha, 2002).

Testing for Local Nonuniformity One problem
with this procedure is that we may ﬁnd that, locally,
points may occupy a small sub-volume, i.e., even if the
local neighborhood is actually drawn from a uniform
distribution(as shown in Fig. 1(a)), the volume of the
PCA-aligned rectangle will with high probability be
smaller than the volume of the max-norm rectangle,
leading to an artiﬁcially large non-uniformity correc-
tion.

To avoid this artifact, we consider a trade-oﬀ between
the two possibilities: for a ﬁxed dimension d and near-
est neighbor parameter k, we ﬁnd a constant αk,d, such
that if ¯V (i)/V (i) < αk,d, then we assume local unifor-
mity is violated and use the correction ¯V (i), otherwise
the correction is discarded for point x(i). Note that
if αk,d is suﬃciently small, then the correction term
will be always discarded, so that our estimator reduces
to the KSG estimator. Good choices of αk,d are set
using arguments described in Appendix C. Further-
more, we believe that as long as the expected value of
E[ ¯V (i)/V (i)] ≥ αk,d in large N limit, for some prop-
erly selected αk,d, then the consistency properties of
the proposed estimator will be identical to the con-
stancy properties of the KSG estimator. A rigorous
proof of this hypothesis is left as a future work.

The full algorithm for our estimator is given in Algo-
rithm 1.

Algorithm 1 Mutual Information Estimation
with Local Nonuniform Correction

Input: points x(1), x(2), ..., x(N ), parameter d (di-
mension), k (nearest neighbor), αk,d
Output: ˆILN C(x)
Calculate ˆIKSG(x) by KSG estimator, using the
same nearest neighbor parameter k
for each point x(i) do

Find k nearest neighbors of x(i):

kN N (i)
1 ,

kN N (i)

2 ,..., kN N (i)

k

Do PCA on these k neighbors, calculate the vol-

ume corrected rectangle ¯V (i)

Calculate the volume of max-norm rectangle

V (i)

if ¯V (i)/V (i) < αk,d then

LN Ci = log

¯V (i)
V (i)

else

end if

LN Ci = 0.0

end for
Calculate
LN C1, LN C2, ..., LN CN
ˆILN C = ˆIKSG − LN C ∗

LN C ∗:

5 Experiments

average

value

of

We evaluate the proposed estimator on both synthet-
ically generated and real-world data. For the for-
mer, we considered various functional relationships
and thoroughly examined the performance of the esti-
mator over a range of noise intensities. For the latter,
we applied our estimator to the WHO dataset used
previously in (Reshef et al., 2011). Below we report
the results.

5.1 Experiments with synthetic data

Functional relationships in two dimensions
In
the ﬁrst set of experiments, we generate samples
from various functional relationships of the form Y =
f (X) + η that were previously studied in Reshef et al.
(2011); Kinney and Atwal (2014). The noise term η
is distributed uniformly over the interval [−σ/2, σ/2],
where σ is used to control the noise intensity. We
also compare the results to several baseline estima-
tors: KSG (Kraskov et al., 2004), generalized near-
est neighbor graph (GNN) (P´al et al., 2010) 3, mini-
mum spanning trees (MST) (M¨uller et al., 2012; Yu-
kich and Yukich, 1998), and exponential family with
maximum likelihood estimation (EXP) (Nielsen and

2Note that we manually set the mean of these k points
to be x(i) when doing PCA, in order to put x(i) in the
center of the rotated rectangle.

3We use the online code http://www.cs.cmu.edu/
~bapoczos/codes/REGO_with_kNN.zip for the GNN esti-
mator.

Eﬃcient Estimation of Mutual Information for Strongly Dependent Variables

Nock, 2010) 4.

cal uniformity assumptions, still assumes local linear-
ity. The stronger the nonlinearity, the more samples
are required to ﬁnd neighborhoods that are locally ap-
proximately linear. We note, however, that LNC still
converges faster than KSG.

Figure 3: For all the functional relationships above,
we used a sample size N = 5000 for each noise level
and the nearest neighbor parameter k = 5 for LNC,
KSG and GNN estimators.

Figure 3 demonstrates that the proposed estimator,
LNC, consistently outperforms the other estimators.
Its superiority is most signiﬁcant for the low noise
regime. In that case, both KSG and GNN estimators
are bounded by the sample size while LNC keeps grow-
ing. MST tends to overestimate MI for large noise but
still stops growing after the noise fall below a certain
intensity5. Surprisingly, EXP is the only other estima-
tor that performs comparably with LNC for the linear
relationship (the left most plot in Figure 3). However,
it fails dramatically for all the other relationship types.
See Appendix D for more functional relationship tests.

Convergence rate Figure 4 shows the convergence
of the two estimators ˆIKSG and ˆILN C, at a ﬁxed
(small) noise intensity, as we vary the sample size.
We test the estimators on both two and ﬁve dimen-
sional data for linear and quadratic relationships. We
observe that LNC is doing better overall. In partic-
ular, for linear relationships in 2D and 5D, as well
as quadratic relationships in 2D, the required sample
size for LNC is several orders of magnitude less that
for ˆIKSG. For instance, for the 5D linear relationship
KSG does not converge even for the sample size (105)
while LN C converges to the true value with only 100
samples.

Finally, it is worthwhile to remark on the relatively
slow convergence of LN C for the 5D quadratic ex-
ample. This is because LN C, while relaxing the lo-

4We use the Information Theoretical Estimators Tool-

box (ITE) (Szab´o, 2014) for MST and EXP estimators.

5Even if k = 2 or 3, KSG and GNN estimators still stop

growing after the noise fall below a certain intensity.

Figure 4: Estimated MI using both KSG and LNC
estimators in the number of samples (k = 5 and αk,d =
0.37 for 2D examples; k = 8 and αk,d = 0.12 for 5D
examples)

5.2 Experiments with real-world data

5.2.1 Ranking Relationship Strength

We evaluate the proposed estimator on the WHO
dataset which has 357 variables describing various
socio-economic, political, and health indicators for dif-
ferent countries 6. We calculate the mutual informa-
tion between pairs of variables which have at least 150
samples. Next, we rank the pairs based on their es-
timated mutual information and choose the top 150
pairs with highest mutual information. For these top
150 pairs, We randomly select a fraction ρ of samples
for each pair, hide the rest samples and then recal-
culate the mutual information. We want to see how
mutual information-based rank changes by giving dif-
ferent amount of less data, i.e., varying ρ. A good mu-
tual information estimator should give a similar rank
using less data as using the full data. We compare
our LNC estimator to KSG estimator. Rank similari-
ties are calculated using the standard Spearman’s rank
correlation coeﬃcient described in (Spearman, 1904).
Fig 5 shows the results. We can see that LNC es-
timator outperforms KSG estimator, especially when
the missing data approaches 90%, Spearman correla-
tion drops to 0.4 for KSG estimator, while our LNC

6WHO dataset is publicly available at http://www.

exploredata.net/Downloads

Shuyang Gao, Greg Ver Steeg, Aram Galstyan

estimator still has a relatively high score of 0.7.

Figure 5: Spearman correlation coeﬃcient between the
original MI rank and the rank after hiding some per-
centage of data by KSG and LNC estimator respec-
tively. The 95% conﬁdence bars are obtained by re-
peating the experiment for 200 times.

5.2.2 Finding interesting triplets

We also use our estimator to ﬁnd strong multivari-
ate relationships in the WHO data set. Speciﬁcally,
we search for synergistic triplets (X, Y, Z), where one
of the variables, say Z, can be predicted by knowing
both X and Y simultaneously, but not by using ei-
In other words, we search
ther variable separately.
for triplets (X, Y, Z) such that the pair-wise mutual
information between the pairs I(X : Y ), I(X : Z)
and I(Y : Z) are low, but the multi-information
I(X : Y : Z) is relatively high. We rank rela-
tionships using the following synergy score:7 SS =
I (X : Y : Z)/max {I (X : Y ) , I (Y : Z) , I (Z : X)}.

We select the triplets that have synergy score above
a certain threshold. Figure 6 shows two synergistic
relationships detected by LN C but not by KSG. In
these examples, both KSG and LN C estimators yield
low mutual information for the pairs (X, Y ), (Y, Z)
and (X, Z). However, in contrast to KSG, our estima-
tor yields a relatively high score for multi-information
among the three variables.

For the ﬁrst relationship, the synergistic behavior can
be explained by noting that the ratio of the Total En-
ergy Generation (Y ) to Electricity Generation per Per-
son (Z) essentially yields the size of the population,
which is highly predictive of the Number of Female
Cervical Cancer cases (X). While this example might
seem somewhat trivial, it illustrates the ability of our
method to extract synergistic relationships automati-
cally without any additional assumptions and/or data
preprocessing.

In the second example, LNC predicts a strong synergis-

7Another measure of synergy is given by the so
called “interaction information”: I (X : Y ) + I (Y : Z) +
I (Z : X) − I (X : Y : Z).

(a)

(b)

Figure 6: Two examples of synergistic triplets:
ˆIKSG(X : Y : Z) = 0.14 and ˆILN C(X : Y : Z) = 0.95
for the ﬁrst example; ˆIKSG(X : Y : Z) = 0.05 and
ˆILN C(X : Y : Z) = 0.7 for the second example

tic interaction between Total Cell Phones (Y ), Num-
ber of Female Cervical Cancer cases (Z), and rate of
Tuberculosis deaths (X). Since the variable Z (the
number of female cervical cancer cases) grows with
the total population, Y
Z is proportional to the average
number of cell phones per person. The last plot indi-
cates that a higher number of cell phones per person
are predictive of lower tuberculosis death rate. One
possible explanation for this correlation is some com-
mon underlying cause (e.g., overall economic develop-
ment). Another intriguing possibility is that this ﬁnd-
ing reﬂects recent eﬀorts to use mobile technology in
TB control.8

8See Stop TB Partnership, http://www.stoptb.org.

Eﬃcient Estimation of Mutual Information for Strongly Dependent Variables

6 Related Work

MI estimators There has been a signiﬁcant amount
of recent work on estimating entropic measures such
information from sam-
as divergences and mutual
ples (see this survey (Walters-Williams and Li, 2009)
for an exhaustive list). Khan et al. (2007) com-
pared diﬀerent MI estimators for varying sample sizes
and noise intensity, and reported that for small sam-
ples, the KSG estimator was the best choice over-
all
for relatively low noise intensities, while KDE
performed better at higher noise intensities. Other
approaches include estimators based on Generalized
Nearest-Neighbor Graphs (P´al et al., 2010), minimum
spanning trees (M¨uller et al., 2012), maximum like-
lihood density ratio estimation (Suzuki et al., 2008),
and ensemble methods (Sricharan et al., 2013; Moon
In particular, the latter approach
and Hero, 2014).
works by taking a weighted average of simple density
plug-in estimates such as kNN or KDE. However, it is
upper-bounded by the largest value among its simple
density estimates. Therefore, this method would still
underestimate the mutual information when it goes
larger as discussed before.

It has been recognized that kNN-based entropic esti-
mators underestimate probability density at the sam-
ple points that are close to the boundary of sup-
port (Liiti¨ainen et al., 2010). Sricharan et al. (2012)
proposed an bipartite plug-in(BPI) estimator for non-
linear density functionals that extrapolates the density
estimates at interior points that are close to the bound-
ary points in order to compensate the boundary bias.
However, this method requires to identify boundary
points and interior points which becomes diﬃcult to
distinguish as mutual information gets large that al-
most all the points are close to the boundary. Singh
and Poczos (2014) used a ”mirror image” kernel den-
sity estimator to escape the boundary eﬀect, but their
estimator relies on the knowledge of the support of the
densities, and assumes that the boundary are parallel
to the axes.

Mutual Information and Equitability Reshef
et al. (2011) introduced a property they called “suit-
ability” for a measure of correlation. If two variables
are related by a functional form with some noise, eq-
uitable measures should reﬂect the magnitude of the
noise while being insensitive to the form of the func-
tional relationship. They used this notion to justify a
new correlation measure called MIC. Based on com-
parisons with MI using the KSG estimator, they con-
cluded that MIC is “more equitable” for comparing
relationship strengths. While several problems (Si-
mon and Tibshirani, 2014; Gorﬁne et al.) and alter-
natives (Heller et al., 2013; Sz´ekely et al., 2009) were

pointed out, Kinney and Atwal (KA) were the ﬁrst
to point out that MIC’s apparent superiority to MI
was actually due to ﬂaws in estimation (Kinney and
Atwal, 2014). A more careful deﬁnition of equitabil-
ity led KA to the conclusion that MI is actually more
equitable than MIC. KA suggest that the poor per-
formance of the KSG estimator that led to Reshef et.
al.’s mistaken conclusion could be improved by using
more samples for estimation. However, here we showed
that the number of samples required for KSG is pro-
hibitively large, but that this diﬃculty can be over-
come by using an improved MI estimator.

7 Conclusion

The problem of deciding whether or not two vari-
ables are independent is a historically signiﬁcant en-
deavor. In that context, research on mutual informa-
tion estimation has been geared towards distinguishing
weak dependence from independence. However, mod-
ern data mining presents us with problems requiring a
totally diﬀerent perspective. It is not unusual to have
thousands of variables which could have millions of
potential relationships. We have insuﬃcient resources
to examine each potential relationship so we need an
assumption-free way to pick out only the most promis-
ing relationships for further study. Many applications
have this ﬂavor including the health indicator data
considered above as well as gene expression microarray
data, human behavior data, economic indicators, and
sensor data, to name a few.

How can we select the most interesting relationships
to study? Classic correlation measures like the Pear-
son coeﬃcient bias the results towards linear variables.
Mutual information gives a clear and general basis for
comparing the strength of otherwise dissimilar vari-
ables and relationships. While non-parametric mutual
information estimators exist, we showed that strong
relationships require exponentially many samples to
accurately measure using some of these techniques.

We introduced a non-parametric mutual information
estimator that can measure the strength of nonlinear
relationships even with small sample sizes. We have in-
corporated these novel estimators into an open source
entropy estimation toolbox 9.As the amount and vari-
ety of available data grows, general methods for iden-
tifying strong relationships will become increasingly
necessary. We hope that the developments suggested
here will help to address this need.

9https://github.com/BiuBiuBiLL/MIE

Shuyang Gao, Greg Ver Steeg, Aram Galstyan

Acknowledgements

This research was supported in part by DARPA grant
No. W911NF–12–1–0034.

References

T.M. Cover and J.A. Thomas. Elements of information

theory. Wiley-Interscience, 2006.

Malka Gorﬁne, Ruth Heller, and Yair Heller. Com-
ment on detecting novel associations in large data
sets.
technion. ac.
(available at http://emotion.
il/ gorﬁnm/ﬁles/science6. pdf on 11 Nov. 2012).

Ruth Heller, Yair Heller, and Malka Gorﬁne. A consis-
tent multivariate test of association based on ranks of
distances. Biometrika, 100(2):503–510, 2013.

Shiraj Khan, Sharba Bandyopadhyay, Auroop R. Gan-
guly, Sunil Saigal, David J. Erickson, Vladimir Pro-
topopescu, and George Ostrouchov. Relative perfor-
mance of mutual information estimation methods for
quantifying the dependence among short and noisy data.
Phys. Rev. E, 76:026209, Aug 2007.
doi: 10.1103/
PhysRevE.76.026209. URL http://link.aps.org/doi/
10.1103/PhysRevE.76.026209.

J. Kinney and G. Atwal. Equitability, mutual information,
and the maximal information coeﬃcient. Proceedings
of the National Academy of Sciences, 111(9):3354–3359,
2014.

A. Kraskov, H. St¨ogbauer, and P. Grassberger. Estimating
mutual information. Phys. Rev. E, 69:066138, 2004. doi:
10.1103/PhysRevE.69.066138. URL http://link.aps.
org/doi/10.1103/PhysRevE.69.066138.

Elia Liiti¨ainen, Amaury Lendasse, and Francesco Corona.
A boundary corrected expansion of the moments of near-
est neighbor distributions. Random Struct. Algorithms,
37(2):223–247, September 2010. ISSN 1042-9832. doi:
10.1002/rsa.v37:2. URL http://dx.doi.org/10.1002/
rsa.v37:2.

K.R. Moon and A.O. Hero. Ensemble estimation of mul-
tivariate f-divergence.
In Information Theory (ISIT),
2014 IEEE International Symposium on, pages 356–360,
June 2014. doi: 10.1109/ISIT.2014.6874854.

Andreas M¨uller, Sebastian Nowozin, and Christoph Lam-
pert.
Information theoretic clustering using minimum
spanning trees. Pattern Recognition, pages 205–215,
2012.

F. Nielsen and R. Nock. Entropies and cross-entropies of
exponential families. In 17th IEEE International Con-
ference on Image Processing (ICIP), pages 3621–3624.
IEEE, 2010.

D´avid P´al, Barnab´as P´oczos, and Csaba Szepesv´ari. Esti-
mation of r´enyi entropy and mutual information based
on generalized nearest-neighbor graphs. In Advances in
Neural Information Processing Systems 23, pages 1849–
1857. Curran Associates, Inc., 2010.

Fernando P´erez-Cruz. Estimation of information theoretic
measures for continuous random variables. In Proceed-
ings of NIPS-08, pages 1257–1264, 2008.

David N Reshef, Yakir A Reshef, Hilary K Finucane,
Sharon R Grossman, Gilean McVean, Peter J Turn-
baugh, Eric S Lander, Michael Mitzenmacher, and Par-

dis C Sabeti. Detecting novel associations in large data
sets. science, 334(6062):1518–1524, 2011.

Sam T Roweis and Lawrence K Saul. Nonlinear dimension-
ality reduction by locally linear embedding. Science, 290
(5500):2323–2326, 2000.

Noah Simon and Robert Tibshirani. Comment on” detect-
ing novel associations in large data sets” by reshef et al,
science dec 16, 2011. arXiv preprint arXiv:1401.7645,
2014.

Harshinder Singh, Neeraj Misra, Vladimir Hnizdo, Adam
Fedorowicz, and Eugene Demchuk. Nearest neighbor
estimates of entropy. American Journal of Mathemat-
ical and Management Sciences, 23(3-4):301–321, 2003.
doi: 10.1080/01966324.2003.10737616. URL http://
dx.doi.org/10.1080/01966324.2003.10737616.

Shashank Singh and Barnabas Poczos. Generalized expo-
nential concentration inequality for renyi divergence es-
timation. In Proceedings of the 31st International Con-
ference on Machine Learning (ICML-14), pages 333–
341, 2014. URL http://machinelearning.wustl.edu/
mlpapers/papers/icml2014c1_singh14.

C. Spearman. The proof and measurement of association
between two things. The American Journal of Psy-
chology, 15(1):pp. 72–101, 1904. ISSN 00029556. URL
http://www.jstor.org/stable/1412159.

K. Sricharan, R. Raich, and A.O. Hero. Estimation of
nonlinear functionals of densities with conﬁdence. In-
formation Theory, IEEE Transactions on, 58(7):4135–
ISSN 0018-9448. doi: 10.1109/TIT.
4159, July 2012.
2012.2195549.

K. Sricharan, D. Wei, and A.O. Hero. Ensemble estima-
tors for multivariate entropy estimation.
Information
Theory, IEEE Transactions on, 59(7):4374–4388, July
2013. ISSN 0018-9448. doi: 10.1109/TIT.2013.2251456.

Milan Studen`y and Jirina Vejnarov´a. The multiinforma-
tion function as a tool for measuring stochastic depen-
dence. In Learning in graphical models, pages 261–297.
Springer, 1998.

Taiji Suzuki, Masashi Sugiyama, Jun Sese, and Takafumi
Kanamori. Approximating mutual information by maxi-
mum likelihood density ratio estimation. In Yvan Saeys,
Huan Liu, Iaki Inza, Louis Wehenkel, and Yves Van
de Peer, editors, FSDM, volume 4 of JMLR Proceedings,
pages 5–20. JMLR.org, 2008.

Zolt´an Szab´o.

Information theoretical estimators tool-
box. Journal of Machine Learning Research, 15:283–287,
2014. (https://bitbucket.org/szzoli/ite/).

G´abor J Sz´ekely, Maria L Rizzo, et al. Brownian distance
covariance. The annals of applied statistics, 3(4):1236–
1265, 2009.

Ulrike Von Luxburg and Morteza Alamgir. Density es-
timation from unweighted k-nearest neighbor graphs: a
roadmap. In Advances in Neural Information Processing
Systems, 2013.

Janett Walters-Williams and Yan Li. Estimation of mu-
tual information: A survey. In Rough Sets and Knowl-
edge Technology, volume 5589 of Lecture Notes in Com-
puter Science, pages 389–396. Springer Berlin Heidel-
berg, 2009.
ISBN 978-3-642-02961-5. doi: 10.1007/
978-3-642-02962-2 49. URL http://dx.doi.org/10.
1007/978-3-642-02962-2_49.

Eﬃcient Estimation of Mutual Information for Strongly Dependent Variables

Q. Wang, S.R. Kulkarni, and S. Verd´u. Divergence es-
timation for multidimensional densities via k-nearest-
neighbor distances. IEEE Trans. Inf. Theor., 55:2392–
ISSN 0018-9448. doi: 10.1109/TIT.
2405, May 2009.
2009.2016060.
URL http://dl.acm.org/citation.
cfm?id=1669487.1669521.

Satosi Watanabe. Information theoretical analysis of mul-
tivariate correlation. IBM Journal of research and de-
velopment, 4(1):66–82, 1960.

Joseph E Yukich and Joseph Yukich. Probability theory
of classical Euclidean optimization problems. Springer
Berlin, 1998.

Zhenyue Zhang and Hongyuan Zha. Principal manifolds
and nonlinear dimension reduction via local tangent
space alignment. SIAM Journal of Scientiﬁc Comput-
ing, 26:313–338, 2002.

Shuyang Gao, Greg Ver Steeg, Aram Galstyan

Supplementary Material for “Eﬃcient
Estimation of Mutual Information for
Strongly Dependent Variables”

A Proof of Theorem 2

Notice that for a ﬁxed sample point x(i), its k-nearest-
(cid:0)x(i)(cid:1) is always equal to or larger
neighbor distance rk
than the k-nearest-neighbor distance of at the same
point x(i) projected into a sub-dimension j, i.e., for
any i, j, we have

(cid:16)

x(i)(cid:17)

rk

(cid:62) rk

(cid:17)

(cid:16)

x(i)
j

(A.1)

(cid:98)Ik (x) =

equality for (cid:98)IkN N,k (x):

(cid:98)IkN N,k (x) ≤ (d − 1) log

+ O (d log d)

(cid:19)

(cid:18) N − 1
k

≤

(d − 1) log (N − 1) + O (d log d)

Requiring that |(cid:98)IkN N,k (x) − I(x)| ≤ ε, we obtain,

N ≥ C exp

(cid:19)

(cid:18) I (x) − (cid:15)
d − 1

+ 1

(A.3)

where C is a constant which scales like O( 1

d ).

B Derivation of Eq. 17

The naive kNN or KSG estimator can be written as:

1
N

N
(cid:88)

i=1

log

P (x(i))
V (i)
(cid:16)

P

x(i)
j
Vj (i)

(cid:17)

d
(cid:81)
j=1

(B.1)

where P (x(i)) is the probability mass around the k-
nearest-neighborhood at x(i) and P (x(i)
j ) is the prob-
ability mass around the k-nearest-neighborhood (or
nxj (i)-nearest-neighborhood for KSG) at x(i) pro-
jected into j-th dimension. Also, V (i) and Vj(i) denote
the volume of the kNN ball(or hype-rectangle in KSG)
in the joint space and projected subspaces respectively.

Now our local nonuniform correction method replaces
the volume V (i) in Eq. B.1 with the corrected volume
V (i), thus, our estimator is obtained as follows:

P (x(i))
V (i)
(cid:16)

(cid:17)

P

x(i)
j
Vj (i)

d
(cid:81)
j=1
P (x(i))
V (i) × V (i)
x(i)
j
Vj (i)

V (i)
(cid:17)

d
(cid:81)
j=1

P

(cid:16)

1
N

N
(cid:88)

i=1

log

V (i)
V (i)

=

1
N

N
(cid:88)

i=1

log

= (cid:98)Ik (x) −

(B.2)

Using Eq. A.1, we get the upper bound of (cid:98)IkN N,k (x)
as follows:
(cid:98)IkN N,k (x) = (cid:98)I (cid:48)

=

=

N
(cid:88)

kN N,k (x) − (d − 1) γk
(cid:98)pk
d
(cid:81)
j=1

(cid:0)x(i)(cid:1)
(cid:16)

x(i)
j

(cid:98)pk

log

i=1

(cid:17)

1
N

− (d − 1) γk

1
N

N
(cid:88)

i=1

log

k
N −1

Γ(d/2)+1
πd/2

rk

d
(cid:81)
j=1

k
N −1

Γ(1/2)+1
π1/2

(cid:0)x(i)(cid:1)−d
(cid:16)
x(i)
j

rk

(cid:17)−1

− (d − 1) (ψ (k) − log k)

≤ (d − 1) log

(cid:19)

(cid:18) N − 1
k

+ log

Γ (d/2) + 1
(Γ (1/2) + 1)d

− (d − 1) (ψ (1) − log 1)

(A.2)

The last inequality is obtained by noticing that ψ(k)−
log(k) is a monotonous decreasing function.

Also, we have,

− (d − 1) γk

≤ (d − 1) log

(cid:19)

(cid:18) N − 1
k

+ log

Γ (d/2) + 1
(Γ (1/2) + 1)d

(cid:98)ILN C,k (x) =

1
N

N
(cid:88)

i=1

log

log

Γ (d/2) + 1
(Γ (1/2) + 1)d = log (Γ (d/2) + 1) − d log (Γ (d/2) + 1)

C Empirical Evaluation for αk,d

(cid:19)d/2+1/2(cid:33)

(cid:32)√

< log

2π

(cid:18) d/2 + 1/2
e
(cid:17)

−d log

(cid:16)

1
2 + 1

π

= O (d log d)

The inequality above is obtained by using the bound
of gamma function that,
√

(cid:19)x+1/2

Γ (x + 1) <

2π

(cid:18) x + 1/2
e

Therefore, reconsidering A.2, we get the following in-

Suppose we have a uniform distribution on the d di-
mensional (hyper)rectangle with volume V . We sam-
ple k points from this uniform distribution. We per-
form PCA using these k points to get a new basis.
After rotating into this new basis, we ﬁnd the volume,
¯V , of the smallest rectilinear rectangle containing the
points. By chance, we will typically ﬁnd ¯V < V , even
though the distribution is uniform. This will lead to
us to (incorrectly) apply a local non-uniformity cor-
rection. Instead, we set a threshold αk,d and if ¯V /V is
above the threshold, we assume that the distribution

Eﬃcient Estimation of Mutual Information for Strongly Dependent Variables

Figure C.1 shows empirical value of ˆαk,d for diﬀerent
(k, d) pairs. We can see that for a ﬁxed dimension
d, ˆαk,d grows as k increases, meaning that ¯V must be
closer to V to accept the null hypothesis of uniformity.
We also ﬁnd that ˆαk,d decreases as the dimension d in-
creases, indicating that for a ﬁxed k, ¯V becomes much
smaller than V when points are drawn from a uniform
distribution in higher dimensions.

D More Functional Relationship Tests

in Two Dimensions

We have tested together twenty-one functional rela-
tionships described in Reshef et al. (2011); Kinney and
Atwal (2014), we show six of them in Section 5. The
complete results are shown in Figure D.1. Detailed de-
scription of the functions can be found in Table S1 of
Supporting Information in Kinney and Atwal (2014).

is locally uniform. Setting α involves a trade-oﬀ. If
it is set too high, we will incorrectly conclude there is
local non-uniformity and therefore over-estimate the
mutual information. If we set α too low, we will lose
statistical power for “medium-strength” relationships
(though very strong relationships will still lead to val-
ues of ¯V /V smaller than α).

In practice, we determine the correct value of αk,d em-
pirically. We look at the probability distribution of
¯V /V that occurs when the true distribution is uniform.
We set α conservatively so that when the true distribu-
tion is uniform, our criteria rejects this hypothesis with
small probability, (cid:15). Speciﬁcally, we do a number of tri-

als, N , and set ˆαk,d such that

N
(cid:80)
i=1

(cid:16) ¯Vi
Vi

I

(cid:17)

< ˆαk,d

/N < (cid:15)

where (cid:15) is a relatively small value.
In practice, we
chose (cid:15) = 5 × 10−3 and N = 5 × 105. The following
algorithm describes this procedure:

Algorithm C.1 Estimating αk,d for LNC

Input: parameter d (dimension), k (nearest neigh-
bor), N , (cid:15)
Output: ˆαk,d
set array a to be NULL
repeat

Randomly choose a uniform distribution sup-
ported on d dimensional (hyper) rectangle, denote
its volume to be V

Draw k points from this uniform distribution, get

the correcting volume ¯V after doing PCA

add the ratio ¯V

V to array a

until above procedure repeated N times
ˆαk,d ← (cid:100)(cid:15)N (cid:101) th smallest number in a

Figure C.1: (cid:98)αk,d as a function of k. k ranges over
[d, 20] for each dimension d.

Shuyang Gao, Greg Ver Steeg, Aram Galstyan

s
e
i
t
i
s
n
e
t
n
i

e
s
i
o
n

t
n
e
r
e
ﬀ
i
d

h
t
i
w
s
p
i
h
s
n
o
i
t
a
l
e
r

l
a
n
o
i
t
c
n
u
f

e
n
o
-
y
t
n
e
w
T

.
s
r
o
t
a
m

i
t
s
e

P
X
E

,

T
S
M

,

N
N
G

,

G
S
K

,

C
N
L

f
o

s
t
s
e
t

n
o
i
t
a
m
r
o
f
n
I

l
a
u
t
u
M

:
1
.
D
e
r
u
g
i
F

r
o
b
h
g
i
e
n

t
s
e
r
a
e
n

,
s
r
o
t
a
m

i
t
s
e

C
N
L

d
n
a

N
N
G

,

G
S
K

r
o
F

.
)
s
t
o
l
p

e
h
t

f
o

s
i
x
a

X

n

i

n
w
o
h
s

s
a
(
s
e
i
r
a
v

σ

e
r
e
h
w
]
2
/
σ

,
2
/
σ
−
U

[

m
r
o
f

e
h
t

s
a
h

e
s
i
o
N

.
d
e
t
s
e
t

e
r
a

.

i

p
h
s
n
o
i
t
a
l
e
r

l
a
n
o
i
t
c
n
u
f

y
s
i
o
n

h
c
a
e

r
o
f

s
t
n
i
o
p

a
t
a
d

0
0
0
,
5
=
N
g
n
i
s
u

e
r
a

e

W

.
5
=
k

r
e
t
e
m
a
r
a
p

