Unsupervised Feature Selection with
Adaptive Structure Learning

Liang Du
State Key Laboratory of Computer Science,
Institute of Software, Chinese Academy of
Sciences
duliang@ios.ac.cn

Yi-Dong Shen
State Key Laboratory of Computer Science,
Institute of Software, Chinese Academy of
Sciences
ydshen@ios.ac.cn

5
1
0
2
 
r
p
A
 
3
 
 
]

G
L
.
s
c
[
 
 
1
v
6
3
7
0
0
.
4
0
5
1
:
v
i
X
r
a

ABSTRACT
The problem of feature selection has raised considerable in-
terests in the past decade. Traditional unsupervised meth-
ods select the features which can faithfully preserve the in-
trinsic structures of data, where the intrinsic structures are
estimated using all the input features of data. However,
the estimated intrinsic structures are unreliable/inaccurate
when the redundant and noisy features are not removed.
Therefore, we face a dilemma here: one need the true struc-
tures of data to identify the informative features, and one
need the informative features to accurately estimate the true
structures of data. To address this, we propose a uniﬁed
learning framework which performs structure learning and
feature selection simultaneously. The structures are adap-
tively learned from the results of feature selection, and the
informative features are reselected to preserve the reﬁned
structures of data. By leveraging the interactions between
these two essential tasks, we are able to capture accurate
structures and select more informative features. Experi-
mental results on many benchmark data sets demonstrate
that the proposed method outperforms many state of the
art unsupervised feature selection methods.

1.

INTRODUCTION

Real world applications usually involve big data with high
dimensionality, presenting great challenges such as the curse
of dimensionality, huge computation and storage cost. To
tackle these diﬃculties, feature selection techniques are de-
veloped to keep a few relevant and informative features. Ac-
cording to the availability of label information, these algo-
rithms can be categorized into supervised [25], [23], [21],
[18], semi-supervised [33], [27] and unsupervised algorithms
[7], [5]. Compared to supervised or semi-supervised counter-
parts, unsupervised feature selection is generally more chal-
lenging due to the lack of supervised information to guild
the search of relevant features.

Unsupervised feature selection has attracted much atten-
tion in recent years and a number of algorithms have been

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.

proposed [9, 5, 36, 28, 17]. Without class label, unsuper-
vised feature selection chooses features that can eﬀectively
reveal or maintain the underlying structure of data. Recent
research on feature selection and dimension reduction has
witnessed that several important structures should be pre-
served by the selected features. These important structures
include, but not limited to, the global structure [36, 17], the
local manifold structure [10, 11] and the discriminative in-
formation [28, 15]. And these structures can be captured by
widely used models in the form of graph, such as, the sam-
ple pairwise similarity graph [36], the k-nn graph [9], the
global integration of local discriminant model [28, 31], the
local linear embedding (LLE) [17].

Clearly, many of existing unsupervised feature selection
methods rely on the structure characterization through some
kind of graph, which can be computed within the original
feature space. And once the graph is determined, it is ﬁxed
in the next procedures, such as sparse spectral regression [4],
to guide the search of informative features. Consequently,
the performance of feature selection is largely determined by
the eﬀectiveness of graph construction. Ideally, such graphs
should be constructed only using the informative feature
subset rather than all candidate features. Unfortunately,
the desired subset of features is unknown in advance, and
the irrelevant or noisy features would be inevitably intro-
duced in many real applications. As a result, unrelated or
noisy features will have an adverse eﬀect on the character-
ization of the structures and henceforth hurt the following
feature selection performance.

In unsupervised scenario, this is actually the chicken-and-
egg problem between structure characterization and feature
selection. Facing with such dilemma, we propose to perform
feature selection and structure learning in a uniﬁed frame-
work, where each sub task can be iteratively boosted by us-
ing the result of the other one. Concretely, the global struc-
ture of data is captured within the sparse representation
framework, where the reconstruction coeﬃcient is learned
from the selected features. The local manifold structure is
revealed by a probabilistic neighborhood graph, where the
pairwise relationship is also determined by the selected fea-
tures. When the global and local structures are given in the
form of graph Laplacians, we seek the relevant features via
sparse spectral regression with the help of graph embedding
for cluster analysis.
In this way, both the global and lo-
cal structure of data can be better captured by only using
the selected features; Moreover, with the reﬁned character-
ization of the structure, a better search of the informative
features could also be expected.

Figure 1: An illustration of unsupervised ﬁlter methods and four type embedded methods.

It is worthwhile to highlight several aspects of the pro-

posed approach here

1. Based on the diﬀerent learning paradigms for unsuper-
vised feature selection, we investigate most of exist-
ing unsupervised embedded methods and further clas-
sify them into four closely related but diﬀerent types.
These analyses provide more insight into what should
be further emphasized on the development of more es-
sential unsupervised feature selection algorithm.

2. We propose a novel uniﬁed learning framework, called
unsupervised Feature Selection with Adaptive Struc-
ture Learning (FSASL in short), to fulﬁl the gap be-
tween two essential sub tasks, i.e. structure learning
and feature learning. In this way, these two tasks can
be mutually improved.

3. Comprehensive experiments on benchmark data sets
show that our method achieves statistically signiﬁcant
improvement over state-of-the-art feature selection meth-
ods, suggesting the eﬀectiveness of the proposed method.

The rest of this paper is arranged as follows. We review
the related work in Section 2. Then we present our proposed
formulation and the developed solution in Section 3. Discus-
sions are introduced in Section 4. Extensive experiments are
conducted and analyzed in Section 5. Section 6 concludes
this paper with future work.

2. RELATED WORKS

In this section, we mainly review most existing unsuper-
vised feature selection methods, i.e. ﬁlter and embedded
methods. Unsupervised ﬁlter methods pick the features one
by one based on certain evaluation criteria, where no learn-
ing algorithm is involved. The typical methods include:
max variance (MaxVar) [13], Laplacian score (LapScore) [9],
spectral feature selection (SPEC) [34], feature selection via
eigenvalue sensitive criterion (EVSC) [5]. A common limi-
tation of these approaches is the correlation among features
is neglected [2].

Unsupervised embedded approaches are developed to per-
form feature selection and ﬁt a learning model simultane-
ously. Based on the diﬀerent sub-steps involved in the fea-
ture selection procedure, these embedded methods can be

further divided into four diﬀerent types as illustrated in Fig-
ure 1.

The ﬁrst type of embedded methods ﬁrst detect the struc-
ture of the data and then directly select those features which
is used to best preserve the enclosed structure. The typical
methods include: trace ratio (TraceRatio) [20] and unsu-
pervised discriminative feature selection (UDFS) [28]. Trac-
eRatio is prone to select redundant features [17] and the
learning model of UDFS is often too restrictive [22].

The second type of embedded methods ﬁrst construct vari-
ous graph Laplacians to capture the data structure, then ﬂat
the cluster structure via graph embedding, and ﬁnally use
the sparse spectral regression [4] to select those features that
are best aligned to the embedding. Instead of directly select-
ing features as the ﬁrst type, these approaches resorted to an
intermediate cluster analysis sub-step to reveal the cluster
structure for guiding the selection of features. The cluster
structure discovered by either the graph spectral embedding
or other clustering module can be seen as an approximation
of the unseen labels. The typical methods include: multi-
cluster feature selection (MCFS) [5], minimum redundancy
spectral feature selection (MRSF) [35], similarity preserv-
ing feature selection (SPFS) [36], and joint feature selection
and subspace learning (FSSL) [8], global and local structure
preserving feature selection (GLSPFS) [17].

Unlike the second type methods, the clustering analysis
in the third type of embedded methods is co-determined
by the embedding of the graph Laplacian and the adap-
tive discriminative regularization [29], [31], which can be
obtained from the result of sparse spectral regression. By
using the feedback from feature selection, the whole learn-
ing procedure can provide better cluster analysis, and vice
versa. The typical methods include: joint embedding learn-
ing and spectral regression (JELSR) [12], [11], nonnegative
discriminative feature selection (NDFS) [15], robust unsu-
pervised feature selection (RUFS) [22], feature selection via
clustering-guided sparse structural learning (CGSSL) [14].

The fourth type of embedded methods try to feed the
result of feature selection into the structure learning proce-
dure for improving the quality of structure learning. In [32],
a feature selection method is proposed for local learning-
based clustering (LLCFS), which incorporates the relevance

of each feature into the built-in regularization of the local
learning model, where the induced graph Laplacian can be
iteratively updated. However, LLCFS uses the discrete k-
nearest neighbor graph and does not optimize the same ob-
jective function in structure learning and feature search.

It can be seen that all these above methods (except LL-
CFS) share a common drawback: they use all features to
estimate the underlying structures of data. Since the re-
dundant and noisy features are unavoidable in real world
applications, that is also why we need feature selection,
the learned structures using all features will also be con-
taminated, which would degrade the performance of fea-
ture selection. By leveraging the coherent interactions be-
tween structure learning and feature selection, our proposed
method FSASL seamlessly integrates them into a uniﬁed
framework, where the result of one task is used to improve
the other one.

3. UNSUPERVISED FEATURE SELECTION
WITH ADAPTIVE STRUCTURE LEARN-
ING

Let X = {x1, ..., xn} ∈ Rd×n denotes the data matrix,
whose columns correspond to data instances and rows to
features. The generic problem of unsupervised feature se-
lection is to ﬁnd the most informative features. With the
absence of class label to guild the search of relevant features,
the data represented with the selected features should well
preserve the intrinsic structure as the data represented by
all the original features.

To achieve this goal, we propose to jointly perform un-
supervised feature selection and data structure learning si-
multaneously, where both the global and local structure are
adaptively updated using the result of current feature selec-
tion.

We ﬁrst summarize some notations used throughout this
paper. We use bold uppercase characters to denote matrices,
bold lowercase characters to denote vectors. For an arbitrary
matrix A ∈ Rr×t, ai means the i-th column vector of A
and aT
j means the j-th row vector of A, Aij denotes the
(i, j)-th entry of A. The (cid:96)2,1-norm is deﬁned as ||A||21 =
(cid:80)r

(cid:113)(cid:80)t

i=1

j=1 A2
ij.

3.1 Adaptive Global Structure Learning

Over the past decades, a large number of algorithms have
been proposed based on the analysis of the global structure
of data, such as the Principal Component Analysis (PCA)
and the Maximum Variance (MaxVar). Recently, the global
pairwise similarity (e.g., with a Gaussian kernel) between
high-dimensional samples has demonstrated promising per-
formance for unsupervised feature selection [36, 17]. How-
ever, such dense similarity becomes less discriminative for
high dimension data, especially when there are many unfa-
vorable features in the original high dimensional space.

Inspired by the recent development on compressed sensing
and sparse representation [26], we use the sparse reconstruc-
tion coeﬃcients to extract the global structure of data. In
sparse representation, each data sample xi can be approxi-
mated as a linear combination of all the other samples, and
the optimal sparse combination weight matrix S ∈ Rn×n

can be obtained by solving the following problem

min
S

n
(cid:88)

i=1

(cid:0)||xi − Xsi||2 + α||si||1

(cid:1)

s.t. Sii = 0

(1)

where α is used to balancing the sparsity and the recon-
struction error. Compared with the pairwise similarity, the
sparse representation is naturally discriminative: among all
the candidates samples, it selects the samples which most
compactly expresses the target and rejects all other possible
but less compact candidates [26].

Clearly, the selected features should preserve such global
and sparse reconstruction structure. To achieve this, we
introduce a row sparse feature selection and transformation
matrix W ∈ Rd×c to the reconstruction process, and get

||WT xi − WT Xsi||2 + α||S||1 + γ||W||21

(2)

min
S,W

n
(cid:88)

i=1

s.t. Sii = 0, WT XXT W = I

where γ is regularization parameter. Compared with the
Eq.(1), the beneﬁts of Eq.(2) are two folds: 1) The global
structure captured by S can be used to guide the search of
relevant features; 2) By largely eliminating the adverse eﬀect
of noisy and unfavorable features, the global structure can
also be better estimated.

3.2 Adaptive Local Structure Learning

The importance of preserving local manifold structure has
been well recognized in the recent development of unsu-
pervised feature selection algorithms, especially considering
that high-dimensional data often presents a low-dimensional
manifold structure [9, 5, 17]. To detect the underlying lo-
cal manifold structure, these algorithms usually ﬁrst con-
struct a k-nearest neighbor graph and then compute the
graph Laplacian with diﬀerent models. Clearly, both the k-
nn graph and the graph Laplacian are determined by all the
relevant and irrelevant features. As a result, the manifold
structure captured by such graph Laplacian would be in-
evitably aﬀected by the redundant and noisy features. More-
over, the iterative updating of discrete neighborhood rela-
tionship using the result of feature selection still suﬀers from
the lack of theoretical guarantee of its convergence [32, 24].
Instead of using the graph Laplacian with the determinate
neighborhood relationship, we introduce to directly learn a
euclidean distance induced probabilistic neighborhood ma-
trix. For each data sample xi, all the data points {xj}n
j=1
are considered as the neighborhood of xi with probability
Pij, where P ∈ Rn×n can be determined by solving the
following problem:

min
P

(cid:88)

i,j

(||xi − xj||2

2Pij + µP2

ij), s.t. P1n = 1n, P ≥ 0 (3)

where µ is the regularization parameter. The regularization
term is used to 1) avoid the trivial solution; 2) add a prior
of uniform distribution.
It can be found that a large dis-
tance ||xi − xj||2
2 will lead to a small probability Pij. With
such nice property, the estimated weight matrix P and the
induced Laplacian LP = DP − (P + PT )/2 can be used
for local manifold characterization, where DP is a diagonal
matrix whose i-th diagonal element is (cid:80)

j(Pij + Pji)/2.

To leverage the result of feature selection and iteratively
improve the probabilistic neighborhood relationship, we also

s.t. Sii = 0, P1n = 1n, P ≥ 0, WT XXT W = I

lowing problem:

Next, when S and P are ﬁxed, we need to solve the fol-

min
W

||WT X − WT XS||2 + β

||WT xi − WT xj||2Pij + γ||W||21

n
(cid:88)

i,j

introduce the feature selection and transformation matrix
W as used in global structure adaptive learning, and we get

Next, when WT and S are ﬁxed, we need to solve n de-

coupled sub problems in the following form:

min
P,W

n
(cid:88)

i,j

(||WT xi − WT xj||2

2Pij + µP2

ij) + γ||W||21 (4)

(cid:48)
||x

i − x

j||2Pij + µ||Pij||2,

(cid:48)

n
(cid:88)

min
pT
i

j=1
s.t. 1T

n pi = 1, Pij ≥ 0

(7)

(cid:48)

i −

s.t. P1n = 1n, P ≥ 0, WT XXT W = I

With the sparsity on W, the irrelevant and noisy features
can be largely removed, thus we can learn a better prob-
abilistic neighborhood graph for local structure characteri-
zation based on the result of feature selection, i.e. WT X.
Moreover, we aim to seek those features to preserve the local
structure encoded by P. Thus, the optimization problem in
Eq. (4) can be used to perform feature selection and local
structure learning, simultaneously.

3.3 Unsupervised Feature Selection with Adap-

tive Structure Learning

Based on the two adaptive structure learning models pre-
sented in Eq. (2) and Eq. (4), we propose a novel unsu-
pervised feature selection method by solving the following
optimization problem,
(cid:16)

(cid:17)

||WT X − WT XS||2 + α||S||1

(5)

min
W,S,P

+ β

(cid:16)
||WT xi − WT xj||2Pij + µP2
ij

(cid:17)

+ γ||W||21

n
(cid:88)

i,j

where β and γ are regularization parameters balancing the
ﬁtting error of global and local structure learning in the ﬁrst
and second group and the sparsity of the feature selection
matrix in the third group.

It can be seen that when both S and P are given, our
method selects those features to well respect both the global
and local structure of data. When the feature selection ma-
trix W is given, our method learns the global and local
structure of data in a transformed space, i.e. WT X, where
the adverse eﬀect of noisy features is largely alleviated with
sparse regularization. In this way, these two essential tasks
can be boosted by the other one within a uniﬁed learning
framework. Since both the global and local structure can
be adaptively reﬁned according to the result of feature se-
lection, we call Eq. (5) unsupervised Feature Selection with
Adaptive Structure Learning (FSASL).

3.4 Optimization Algorithm

Because the optimization problem in Eq. (5) comprises
three diﬀerent variables with diﬀerent regularizations and
constraints, it is hard to derive its closed solution directly.
Thus we derive an alternative iterative algorithm to solve
the problem, which converts the problem with a couple of
variables (S, P and WT ) into a series of sub problems where
only one variable is involved.

First, when W and P are ﬁxed, we need to solve n decou-

pled sub problems in the following form:

min
si

(cid:48)

(cid:48)

||x

i − X

si||2 + α|si|,

s.t. Sii = 0

(6)

(cid:48)
where X
is the new transformed data by projecting the rel-
(cid:48)
= WT X.
evant features into a low dimension space, and X
The above LASSO problem can be eﬃciently solved by rou-
tine optimization tools, e.g. proximal methods [3, 16].

Denote A ∈ Rn×n be a square matrix with Aij = − 1
2µ ||x
j||2, then the above problem can be written as follows
x

(cid:48)

min
pT
i

1
2

||pT

i − aT

i ||2,

s.t. pT

i 1n = 1, 0 ≤ pT

ij ≤ 1

(8)

where pT
is the i-th row of P. The above euclidean pro-
i
jection of a vector onto the probability simplex can be ef-
ﬁciently solved by Algorithm 1 without iterations. More
details can be found in Eq. (19).

Algorithm 1 The optimization algorithm of Eq. (8)

Input: a

sort a into b where b1 ≥ b2 ≥, ..., bn
ﬁnd ρ = max{1 ≤ j ≤ n : bj + 1
ρ (1 − (cid:80)ρ
deﬁne z = 1

i=1 bi)
Output: p with pj = max{aj + z, 0}, j = 1, ..., n

j (1 − (cid:80)j

i=1 bi) > 0}

s.t. WT XXT W = I

(9)

Using LS = (I − S)(I − S)T , LP = DP − (P + PT )/2 and
let L = LS + βLP, the above problem can be rewritten as

T r(WT XLXT WT ) + γ||W||21

(10)

min
W
s.t. WT XXT W = I

Due to the non-smooth regularization, it is hard to obtain
the close form solution. We solve it in an iterative way.
Given the t-th estimation Wt and denote DWt be a diagonal
i ||2 , Eq. (10)
matrix with the i-th diagonal element as
can be rewritten as:

1
2||wt

WT X(L + γDWt )XT W

(11)

(cid:17)

(cid:16)

T r

min
W
s.t. WT XXT W = I

The optimal solution of W are the eigenvectors correspond-
ing to the c smallest eigenvalues of generalized eigen-problem:
X(L + γDWt )XT W = ΛXXT W

(12)

where Λ is a diagonal matrix whose diagonal elements are
eigenvalues. To get a stable solution of this eigen-problem,
the matrices XXT is required to be non-singular which is not
true when the number of features is larger than the number
of samples. Moreover, the computational complexity of this
approach scales as O(d3 + nd2), which is costly for high
dimensional data. Thus, such solution is less attractive in
real world applications. To improve the eﬀectiveness and
the eﬃciency to optimize Eq. (10), we further resort to a
two steps procedure inspired from [4].

Theorem 1. Let Y ∈ Rn×c be a matrix of which each
column is an eigenvector of eigen-problem Ly = λy.
If
there exists a matrix W ∈ Rd×c such that XT W = Y, then
each column of W is an eigenvector of the generalized eigen-
problem XLXT w = λXXT w with the same eigenvalue λ.

Proof. With XT W = Y, the following equation holds

XLXT w = XLy = Xλy = λXy = λXXT w

(13)

Thus, y is the eigenvector of the generalized eigen-problem
XLXT w = λXXT w with the same eigenvalue λ.

Theorem 1 shows that instead of solving the generalized
eigen-problem in Eq. (12), W can be obtained by the fol-
lowing two steps:

1. Solve the eigen-problem LY = ΛY to get Y corre-

sponding to the c smallest eigenvalues;

2. Find W which satisﬁes XT W = Y. Since such W
may not exist in real applications, we resort to solve
the following optimization problem:

min
W

||Y − XT W||2 + γ||W||21

(14)

The optimal solution of Eq. (14) can also be obtained
from routine optimization tools, such as the iterative
re-weighted method and the proximal method [16].

The complete algorithm to solve FSASL is summarized in

algorithm 2.

Algorithm 2 The optimization algorithm of FSASL
Input: The data matrix X ∈ Rd×n, the regularization pa-
rameters α, β, γ, µ, the dimension of the transformed
data c.
repeat

For each i, update the i-th column of S by solving the
problem in Eq. (6);
For each i, update the i-th row of P using Algorithm 1;

[19]. We provide an eﬀective method to achieve this. For
each sub problem in Eq. (8), the Lagrangian function is

||pT

i − aT

i ||2 − τ (pT

i 1n − 1) − ηT

i pi

(15)

where τ and ηi are the Lagrangian multipliers. According
to KKT condition, the optimal value can be obtained by

Pij = (Aij + τ )+

(16)

By sorting each row of A into B with ascending order, the
following inequality holds

1
2

(cid:40)

Bik(cid:48) + τ > 0 for k(cid:48) = 1, ..., k
Bik(cid:48) + τ ≤ 0 for k(cid:48) = k + 1, ..., n

(17)

Considering the simplex constraint on pT

i , we further get

τ =

(1 −

Bik(cid:48) )

(18)

1
k

k
(cid:88)

k(cid:48)=1

By replacing Eq. (18) into Eq. (16), the optimal value of P
can be obtained by

Pij = (Aij −

(1 −

Bik(cid:48) ))+

(19)

1
k

k
(cid:88)

k(cid:48)=1

2µ ||Wxi −Wxk(cid:48) ||2 = dW

ik(cid:48) , for each subprob-

dW
ik(cid:48) < µ ≤

dW
i,k(cid:48)+1 −

dW
ik(cid:48)

(20)

k
2

1
2

k
(cid:88)

k(cid:48)=1

Since Bik(cid:48) = − 1
lem we have

k
2

dW
ik −

1
2

k
(cid:88)

k(cid:48)=1

When µ satisﬁes the above inequality for i-th example, the
corresponding pT
i has k non-zero component. Therefore the
average non-zero elements in each row of P is close to k
when we set

µ =

1
n

(cid:32)

n
(cid:88)

i=1

k
2

dW
i,k(cid:48)+1 −

(cid:33)

dW
ik(cid:48)

1
2

k
(cid:88)

k(cid:48)=1

(21)

In this way, the search of parameter µ can be better handled
by searching the neighborhood size k, which is more intuitive
and easy to tune.

Compute the overall graph Laplacian L = LS + βLP;
Compute W by Eq. (12) or Eq. (14);

4. DISCUSSION

until Converges

Output: Sort all the d features according to ||wi||2(i =
1, ..., d) in descending order and select the top m ranked
features.

In this section, we discuss some approaches which are

closely related to our method.

Zeng and Cheung [32] proposed to integrate feature selec-
tion within the regularization of local learning-based clus-
tering (LLCFS), which involves two sub steps:

3.5 Convergence Analysis

FSASL is solved in an alternative way, the optimization
procedure will monotonically decrease the objective of the
problem in Eq. (5) in each iteration. Since the objective
function has lower bounds, such as zero, the above iteration
converges. Besides, the experimental results show that it
converges fast, the time of iteration is often less than 20.

3.6 The determination of parameter µ

Since the parameter µ is used to control the trade oﬀ be-
tween the trivial solution (µ = 0) and the uniform distribu-
tion (µ = ∞), we would like to keep only top-k neighbors for
local manifold structure characterization as the k-nn graph

1. It constructs the k-nearest neighbor graph in the weighted

feature space.

2. It performs joint clustering and feature weight learning

by solving the following problem

min

Y,{Wi,bi}n

i=1,z

n
(cid:88)

c
(cid:88)

(cid:88)





i=1

c(cid:48)=1

xj ∈Nxi

β(Yic(cid:48) − xT

j Wi

c(cid:48) − bi

c(cid:48) )2

+ (Wi

c(cid:48) )T diag(z−1)Wi
c(cid:48)

(22)

(cid:105)

s.t. 1T

d z = 1, z ≥ 0

where z is the feature weight vector and Nxi is the
k-nearest neighbor of xi based on z weighted features.

Compared with LLCFS, FSASL performs both the global
and local structure learning in an adaptive manner, where
only local structure is explored by LLCFS. Moreover, LL-
CFS uses the discrete k-nearest graph and does not optimize
the same objective function in structure learning and feature
search, while FSASL is optimized within a uniﬁed framework
with the probabilistic neighborhood relationship.

Hou et al. [11] proposed the joint embedding learning and
sparse regression (JELSR) method, which can be formulated
as follows:

min
W,YT Y=I

tr(YT L2Y) + λ1(||Y − XT W||2 + λ2||W||21)

(23)

Comparing the formulation in Eq. (5) and Eq. (23), the
main diﬀerences between FSASL and JELSR include: 1)
FSASL selects those features to respect both the global and
local manifold structure, while JELSR only incorporates the
local manifold structure; 2) The local structure in JELSR is
based on k-nearest neighbor graph, while FSASL learns a
probabilistic neighborhood graph, which can be easily re-
ﬁned according the result of feature selection. 3)JELSR
iteratively perform spectral embedding for clustering and
sparse spectral regression for feature selection as illustrated
in Fig. (1). However, the local structure itself (i.e. L2) is
not changed during iterations. FSASL can adaptively im-
prove both the global and local structure characterization
using selected features.

Most recently, Liu et al.

[17] proposed a global and lo-
cal structure preservation framework for feature selection
(GLSPFS). It ﬁrst constructs the pairwise sample similar-
ity matrix K with Gaussian kernel function to capture the
global structure of data, then decompose K = YYT . Us-
ing Y as the regression target, GLSPFS solve the following
problem:

||Y − XT W||2 + λ1tr(WT XL3XT W) + λ2||W||21

min
W

(24)

The main diﬀerences between FSASL and GLSPFS include:
1) GLSPFS uses the Gaussian kernel, while FSASL captures
the global structure within sparse representation, which is
more discriminant; 2) Both the global and local structures
(i.e. K and L3) in GLSPFS are based on all features, while
FSASL reﬁnes these structures with selected features.

5. EXPERIMENTS

In this section, we conduct extensive experiments to eval-
uate the performance of the proposed FSASL for the task of
unsupervised feature selection.

5.1 Data Sets

The experiments are conducted on 8 publicly available
datasets, including handwritten and spoken digit/letter recog-
nition data sets (i.e., MFEA from UCI reporsitory and USPS49
[32] which is a two class subset of USPS), three face image
data sets (i.e., UMIST [11], JAFFE [15], AR [30]), one ob-
ject data set (i.e. COIL [5]) and two biomedical data sets
(i.e., LUNG [18], TOX [1]). The details of these benchmark
data sets are summarized in Table 3.

5.2 Experiment Setup

Table 3: Summary of the benchmark data sets and the num-
ber of selected features

Data Sets
MFEA
USPS49
UMIST
JAFFE
AR
COIL
LUNG
TOX

sample
2000
1673
575
213
840
1440
203
171

feature
240
256
644
676
768
1024
3312
5748

class
10
2
20
10
120
20
5
4

selected features
[5, 10, . . . , 50]
[5, 10, . . . , 50]
[5, 10, . . . , 50]
[5, 10, . . . , 50]
[5, 10, . . . , 50]
[5, 10, . . . , 50]
[10, 20, . . . , 150]
[10, 20, . . . , 150]

To validate the eﬀectiveness of our proposed FSASL1 , we
compare it with one baseline (i.e., AllFea) and states-of-the-
art unsupervised feature selection methods,

• LapScore2 [9], which evaluates the features according
to their ability of locality preserving of the data man-
ifold structure.

• MCFS3 [5], which selects the features by adopting spec-

tral regression with (cid:96)1-norm regularization.

• LLCFS [32], which incorporates the relevance of each

feature into the built-in regularization of the local learning-
based clustering algorithm.

• UDFS4 [28], which exploits local discriminative infor-

mation and feature correlations simultaneously.

• NDFS5 [15], which selects features by a joint frame-
work of nonnegative spectral analysis and (cid:96)2,1-norm
regularized regression.

• SPFS6 [36], which selects a feature subset with which
the pairwise similarity between high dimensional sam-
ples can be maximally preserved.

• RUFS7 [22], which performs robust clustering and ro-
bust feature selection simultaneously to select the most
important and discriminative features.

• JELSR8 [12, 11], which joins embedding learning with

sparse regression to perform feature selection.

• GLSPFS9 [17], which integrates both global pairwise
sample similarity and local geometric data structure
to conduct feature selection.

There are some parameters to be set in advance. For all
the feature selection algorithms except SPFS, we set k = 5
for all the datasets to specify the size of neighborhoods [5,

1For the purpose of reproducibility, we provide the code at
https://github.com/csliangdu/FSASL
2http://www.cad.zju.edu.cn/home/dengcai/Data/code/
LaplacianScore.m
3http://www.cad.zju.edu.cn/home/dengcai/Data/code/
MCFS_p.m
4http://www.cs.cmu.edu/~yiyang/UDFS.rar
5https://sites.google.com/site/zcliustc
6https://sites.google.com/site/alanzhao
7https://sites.google.com/site/qianmingjie
8http://www.escience.cn/people/chenpinghou
9We also use the implementation provided by the authors.

Table 1: Aggregated clustering results measured by Accuracy (%) of the compared methods.

Data Sets AllFea

MFEA

68.73

USPS49

77.70

UMIST

42.40

JAFFE

71.57

AR

30.26

COIL

59.17

LUNG

72.46

TOX

43.65

Average

58.24

LapScore MCFS
51.04
± 8.13
0.00
84.77
± 1.59
0.00
44.46
± 3.26
0.00
73.56
± 4.83
0.00
29.05
± 1.19
0.00
51.50
± 5.38
0.00
70.42
± 3.41
0.00
43.10
± 1.86
0.00
55.98

51.78
± 5.51
0.00
69.21
± 8.95
0.00
36.73
± 1.18
0.00
67.62
± 8.49
0.00
25.29
± 2.89
0.00
45.60
± 6.16
0.00
58.97
± 5.24
0.00
40.25
± 0.65
0.00
49.43

LLCFS
60.38
± 8.58
0.00
94.96
± 1.44
0.03
47.31
± 0.83
0.00
64.79
± 4.08
0.00
34.22
± 2.70
0.05
50.84
± 3.76
0.00
71.58
± 5.85
0.00
39.28
± 0.49
0.00
57.92

UDFS
64.94
± 3.32
0.00
94.05
± 1.13
0.00
48.04
± 1.92
0.00
75.48
± 1.63
0.00
30.87
± 0.35
0.00
48.40
± 16.89
0.00
65.46
± 3.88
0.00
47.14
± 0.75
0.00
59.29

NDFS
67.13
± 7.53
0.01
68.12
± 8.18
0.00
52.80
± 2.26
0.00
74.98
± 2.15
0.00
32.34
± 1.52
0.00
52.22
± 6.33
0.00
75.52
± 1.57
0.00
38.28
± 1.64
0.00
56.67

SPFS
68.20
± 9.43
0.22
83.43
± 6.66
0.00
46.72
± 1.70
0.00
73.93
± 2.85
0.00
31.06
± 2.14
0.00
56.94
± 3.43
0.00
73.49
± 3.43
0.00
39.93
± 1.13
0.00
59.21

RUFS
64.58
± 7.99
0.00
85.86
± 2.58
0.00
50.87
± 1.95
0.00
75.75
± 2.53
0.00
34.84
± 1.90
0.04
59.20
± 3.28
0.00
77.35
± 2.62
0.00
49.17
± 0.83
0.00
62.2

JELSR GLSPFS
67.01
± 8.37
0.01
95.16
± 0.55
0.00
53.52
± 1.54
0.01
77.77
± 1.87
0.00
34.19
± 2.52
0.02
59.53
± 4.01
0.03
77.86
± 3.12
0.00
43.96
± 1.56
0.00
63.62

61.00
± 8.70
0.00
94.75
± 0.61
0.00
50.53
± 0.59
0.00
75.46
± 1.61
0.00
34.12
± 1.60
0.00
57.96
± 2.27
0.00
77.83
± 2.70
0.00
47.38
± 1.93
0.00
62.38

FSASL
69.94
± 7.19
1.00
95.95
± 0.48
1.00
54.92
± 1.89
1.00
79.29
± 2.24
1.00
36.11
± 0.75
1.00
60.93
± 2.50
1.00
81.93
± 1.63
1.00
50.12
± 0.67
1.00
66.15

14]. The weight of k-nn graph for LapScore and MCFS,
and the pairwise similarity for SPFS and GLSPFS is based
on the Gaussian kernel, where the kernel width is searched
from the grid {2−3, 2−2, . . . , 23}δ0, where δ0 is the mean dis-
tance between any two data examples. For GLSPFS, we
report the best results among three local manifold mod-
els, that is locality preserving projection (LPP), LLE and
local tangent space alignment (LTSA) as in [17]. For LL-
CFS, UDFS, NDFS, RUFS, JELSR, GLSPFS and FSASL,
the regularization parameters are searched from the grid
{10−5, 10−4, . . . , 105}. And the regularization parameter for
γ is searched from the grid {0.001, 0.005, 0.01, 0.05, 0.1}γmax,
where γmax is automatically computed from SLEP [16]. For
FSASL, µ is determined by Eq. (21) with k = 5 and c is set
to be the true number of classes. To fairly compare diﬀer-
ent unsupervised feature selection algorithms, we tune the
parameters for all methods by the grid-search strategy [22,
17].

With the selected features, we evaluate the performance
in terms of k-means clustering by two widely used metrics,
i.e., Accuracy (ACC) and Normalized Mutual Information
(NMI). The results of k-means clustering depend on the ini-
tialization. For all the compared algorithms with diﬀerent
parameters and diﬀerent number of selected features, we ﬁrst
repeat the clustering 20 times with random initialization and
record the average results.

5.3 Clustering with Selected Features

Since the optimal number of selected features is unknown
in advance, to better evaluate the performance of unsuper-
vised feature selection algorithms, we ﬁnally report the av-
eraged results over diﬀerent number of selected features (the

range of selected features for each data set can be found in
Table 3) with standard derivation. For all the algorithms
(except for AllFea), we also report its p-value by the paired
t-test against the best results. The best one and those hav-
ing no signiﬁcant diﬀerence (p > 0.05) from the best one are
marked in bold.

The clustering results in terms of ACC and NMI are re-
ported in Table 1 and Table 2, respectively. For diﬀerent
feature selection algorithms, the results in each cell of Table
1 and 2 are the mean ± standard deviation and the p-value.
The last row of Table 1 and Table 2 shows the averaged
results of all the algorithms over the 8 datasets.

Compared with clustering using all features, these unsu-
pervised feature selection algorithms not only can largely
reduce the number of features facilitating the latter learning
process, but can also often improve the clustering perfor-
mance.
In particular, our method FSASL achieves 13.6%
and 18.1% improvement in terms of accuracy and NMI re-
spectively with less than 10% features. These results can
well demonstrate the eﬀectiveness and eﬃciency of unsuper-
vised feature selection algorithm.
It can also be observed
that FSASL consistently produces better performance than
the other nine feature selection algorithms, and the im-
provement is in the range from 4.77% to 38.5% in terms
of clustering accuracy and from 3.98% to 33.8% in terms of
NMI. This can be mainly explained by the following reasons.
First, both global and local structure are used to guide the
search of relevant features. Second, the structure learning
and feature selection are integrated into a uniﬁed frame-
work. Third, both the global and local structures can be
adaptively updated using the results of selected features.

5.4 Effect of Adaptive Structure Learning

Table 2: Aggregated clustering results measured by Normalized Mutual Information (%) of the compared methods.

Data Sets AllFea LapScore MCFS LLCFS UDFS

NDFS

SPFS

RUFS

JELSR GLSPFS

FSASL

MFEA

70.33

USPS49

23.51

UMIST

64.15

JAFFE

81.52

AR

65.48

COIL

75.58

LUNG

60.37

TOX

15.87

Average

57.10

53.74
± 4.77
0.00
15.88
± 17.98
0.00
55.57
± 2.32
0.00
77.28
± 8.98
0.00
63.59
± 2.36
0.00
62.21
± 4.98
0.00
50.14
± 4.13
0.00
10.92
± 0.68
0.00
48.67

54.72
± 9.14
0.00
63.14
± 1.05
0.00
63.46
± 4.93
0.00
79.04
± 5.88
0.00
66.41
± 0.85
0.00
66.19
± 6.78
0.00
55.68
± 2.31
0.00
16.53
± 2.68
0.00
58.14

52.77
± 9.76
0.00
72.03
± 5.56
0.03
63.42
± 1.42
0.00
66.97
± 3.47
0.00
69.01
± 1.45
0.01
64.04
± 4.34
0.00
60.12
± 4.65
0.00
9.68
± 0.75
0.00
57.26

54.19
± 3.83
0.00
68.12
± 4.46
0.00
65.19
± 2.96
0.00
84.25
± 1.74
0.00
67.49
± 0.27
0.00
44.27
± 12.61
0.00
54.88
± 4.21
0.00
22.16
± 1.36
0.00
57.56

64.97
± 7.54
0.03
62.27
± 9.62
0.00
71.19
± 2.77
0.01
82.53
± 3.49
0.00
67.89
± 0.89
0.00
56.29
± 6.91
0.00
60.57
± 1.54
0.00
9.07
± 1.87
0.00
59.35

64.92
± 8.27
0.11
68.10
± 16.66
0.00
64.90
± 3.06
0.00
80.01
± 3.06
0.00
66.94
± 1.11
0.00
69.91
± 4.38
0.00
61.75
± 3.32
0.00
10.13
± 1.03
0.00
60.83

63.98
± 7.22
0.00
71.73
± 7.23
0.00
68.19
± 2.61
0.00
82.00
± 3.56
0.00
69.54
± 1.10
0.01
70.54
± 4.48
0.00
65.47
± 1.87
0.00
25.79
± 1.60
0.00
64.65

64.51
± 9.07
0.06
72.28
± 2.24
0.00
71.33
± 2.06
0.00
85.23
± 3.31
0.00
69.02
± 1.32
0.00
71.37
± 4.97
0.00
63.54
± 2.94
0.00
17.46
± 3.36
0.00
64.34

59.26
± 7.59
0.00
70.43
± 2.57
0.00
69.16
± 0.97
0.00
83.20
± 3.17
0.00
69.44
± 0.84
0.00
69.89
± 4.00
0.00
63.50
± 2.99
0.00
23.49
± 2.77
0.00
63.55

66.70
± 6.71
1.00
75.88
± 2.28
1.00
72.39
± 2.39
1.00
86.42
± 3.34
1.00
70.78
± 0.63
1.00
72.93
± 4.44
1.00
66.78
± 1.72
1.00
27.37
± 1.62
1.00
67.41

Here, we investigate the eﬀect of adaptive structure learn-

ing by empirically answering the following questions:

1. What kind of structure should be captured and pre-
served by the selected features, either global or local
or both of these structures?

2. Does the adaptive structure learning lead to select

more informative features?

Figure 4: Clustering NMI w.r.t.
FSASL on USPS200.

6 diﬀerent settings of

(2), Eq.

We conduct diﬀerent settings of FSASL on USPS200, which
consists the ﬁrst 100 samples in USPS49. We solve the
optimization problem in Eq.
(5),
which uses global, local, and both global and local struc-
tures, respectively. We also distinguish these problems with
and without adaptive structure learning. Thus, we have 6
settings in total. Figure 3 and Figure 4 show the results
of these diﬀerent settings with diﬀerent number of selected
features. The aggregated result over diﬀerent number of se-
lected features is also provided in Table 4.

(4) and Eq.

Figure 3: Clustering accuracy w.r.t. 6 diﬀerent settings of
FSASL on USPS200.

(a)

(b)

(c)

(d)

(e)

(f)

Figure 2: Clustering accuracy w.r.t. diﬀerent parameters on JAFFE (a-c) and TOX (d-f).

Table 4: Aggregated clustering results (%) of 6 diﬀerent
settings of FSASL on USPS200.

W

Problem Variables
Eq. (2)
Eq. (2) W, S
Eq. (4)
Eq. (4) W, P
Eq. (5)
Eq. (5) W, S, P

W

W

ACC
89.17 ± 3.22
91.90 ± 2.51
91.48 ± 2.62
92.86 ± 2.53
94.65 ± 1.24
95.53 ± 1.10

NMI
52.01 ± 9.69
61.95 ± 7.21
59.10 ± 9.31
64.65 ± 8.30
69.94 ± 4.22
74.20 ± 4.83

From these results, we can see that: 1) The exploitation
of both global and local structures (i.e., Eq. (5) + W) out-
perform another two alternatives with only global (i.e., Eq.
(2) + W) or local (i.e., Eq. (4) + W) structure. It vali-
dates that the integration of both global and local structure
is better than the single one. 2) With the update of struc-
ture learning (i.e., Eq. (2) + W, S, Eq. (4) + W, P and
Eq. (5) + W, S, P ) is better than their counterparts with-
out adaptive structure learning respectively. It shows that
the adaptive learning in either global and/or local structure
learning can further improve the result of feature selection.

5.5 Parameter Sensitivity

We investigate the sensitivity with respect to the regu-
larization parameters α, β and γ. When we vary the value
of one parameter, we keep the other parameters ﬁxed at
the optimal value. We plot the clustering accuracy with re-

spect to these parameters on JAFFE and TOX in Figure
2. The experimental results show that our method is not
very sensitive to α, β and γ with wide ranges. However, the
performance is relatively sensitive to the number of selected
features, which is still an open problem.

6. CONCLUSION

In this paper, we proposed a novel unsupervised feature
selection method to simultaneously perform feature selection
and the structure learning. In our new method, the global
structure learning and feature selection are integrated within
the framework of sparse representation; the local structure
learning and feature selection are incorporated into the prob-
abilistic neighborhood relationship learning framework. By
combining both the global and local structure learning and
feature selection, our method can boost both these two es-
sential tasks, i.e., structure learning and feature selection,
by using the result of the other task. We derive an eﬃcient
algorithm to optimize the proposed method and discuss the
connections between our method and other feature selection
methods. Extensive experiments have been conducted on
real-world benchmark data sets to demonstrate the superior
performance of our method.

In the future, we plan to further investigate the following
aspects of FSASL. 1) FSASL has three parameters to tune,
which is computational cumbersome for real applications.
To reduce such burden, we will replace the convex regular-
izations on S and W with the (cid:96)0 or (cid:96)20 norm. 2) FSASL
is required to solve a eigen-problem, which is computational

prohibitive for large scale data. Based on the connection
between spectral clustering and kernel k-means [6], we will
develop an iterative algorithm without eigen-decomposition
and thus make FSASL paralleled.

7. ACKNOWLEDGMENTS

This work is supported in part by the National Natural
Science Foundation of China (NSFC) grants 60970045 and
60833001.

8. REFERENCES

[1] http://featureselection.asu.edu/datasets.php.
[2] S. Alelyani, J. Tang, and H. Liu. Feature selection for
clustering: A review. Data Clustering: Algorithms and
Applications, 29, 2013.

[3] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski.
Optimization with sparsity-inducing penalties.
Foundations and Trends R(cid:13) in Machine Learning,
4(1):1–106, 2012.

[4] D. Cai, X. He, and J. Han. Spectral regression: A

uniﬁed approach for sparse subspace learning. In
Proceedings of the 7th IEEE ICDM, pages 73–82, 2007.

[5] D. Cai, C. Zhang, and X. He. Unsupervised feature

selection for multi-cluster data. In Proceedings of the
16th ACM SIGKDD, pages 333–342, 2010.

[6] I. S. Dhillon, Y. Guan, and B. Kulis. Kernel k-means,
spectral clustering and normalized cuts. In Proceedings
of the 10th ACM SIGKDD, pages 551–556, 2004.

[7] J. Dy and C. Brodley. Feature selection for

unsupervised learning. JMLR, 5:845–889, 2004.
[8] Q. Gu, Z. Li, and J. Han. Joint feature selection and
subspace learning. In Proceedings of the 22th IJCAI,
pages 1294–1299, 2011.

[9] X. He, D. Cai, and P. Niyogi. Laplacian score for

feature selection. NIPS, 18:507–514, 2006.
[10] X. He, M. Ji, C. Zhang, and H. Bao. A variance
minimization criterion to feature selection using
laplacian regularization. IEEE Transactions on PAMI,
(99):2013–2025, 2011.

[11] C. Hou, F. Nie, X. Li, D. Yi, and Y. Wu. Joint

embedding learning and sparse regression: A
framework for unsupervised feature selection.
Cybernetics, IEEE Transactions on, 44(6):793–804,
June 2014.

[12] C. Hou, F. Nie, D. Yi, and Y. Wu. Feature selection

via joint embedding learning and sparse regression. In
Proceedings of the 22th IJCAI, pages 1324–1329, 2011.

[13] W. Krzanowski. Selection of variables to preserve
multivariate data structure, using principal
components. Applied Statistics, pages 22–33, 1987.

[14] Z. Li, J. Liu, Y. Yang, X. Zhou, and H. Lu.

Clustering-guided sparse structural learning for
unsupervised feature selection. IEEE TKDE,
26(9):2138–2150, Sept 2014.

[15] Z. Li, Y. Yang, J. Liu, X. Zhou, and H. Lu.

Unsupervised feature selection using nonnegative
spectral analysis. In Proceedings of the 26th AAAI,
2012.

[16] J. Liu, S. Ji, and J. Ye. SLEP: Sparse Learning with
Eﬃcient Projections. Arizona State University, 2009.

[17] X. Liu, L. Wang, J. Zhang, J. Yin, and H. Liu. Global

and local structure preservation for feature selection.
IEEE Transactions on NNLS, 25(6):1083–1095, June
2014.

[18] F. Nie, H. Huang, X. Cai, and C. Ding. Eﬃcient and

robust feature selection via joint (cid:96)2,1-norms
minimization. NIPS, 23:1813–1821, 2010.

[19] F. Nie, X. Wang, and H. Huang. Clustering and
projected clustering with adaptive neighbors. In
Proceedings of the 20th ACM SIGKDD, pages
977–986, 2014.

[20] F. Nie, S. Xiang, Y. Jia, C. Zhang, and S. Yan. Trace
ratio criterion for feature selection. In Proceedings of
the 23rd IJCAI, volume 2, pages 671–676, 2008.

[21] H. Peng, F. Long, and C. Ding. Feature selection

based on mutual information criteria of
max-dependency, max-relevance, and min-redundancy.
IEEE Transactions on PAMI, 27(8):1226–1238, 2005.

[22] M. Qian and C. Zhai. Robust unsupervised feature

selection. In Proceedings of the 23rd IJCAI, pages
1621–1627, 2013.

[23] M. Robnik-ˇSikonja and I. Kononenko. Theoretical and
empirical analysis of relieﬀ and rrelieﬀ. Machine
learning, 53(1-2):23–69, 2003.

[24] I. Takeuchi and M. Sugiyama. Target neighbor

consistent feature weighting for nearest neighbor
classiﬁcation. In NIPS, pages 576–584, 2011.

[25] R. Tibshirani. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society.
Series B (Methodological), pages 267–288, 1996.
[26] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and

Y. Ma. Robust face recognition via sparse
representation. IEEE Transactions on PAMI,
31(2):210–227, 2009.

[27] Z. Xu, I. King, M.-T. Lyu, and R. Jin. Discriminative

semi-supervised feature selection via manifold
regularization. Neural Networks, IEEE Transactions
on, 21(7):1033–1047, 2010.

[28] Y. Yang, H. Shen, Z. Ma, Z. Huang, and X. Zhou.

(cid:96)21-norm regularized discriminative feature selection
for unsupervised learning. In Proceedings of the 22th
IJCAI, pages 1589–1594, 2011.

[29] Y. Yang, H. T. Shen, F. Nie, R. Ji, and X. Zhou.

Nonnegative spectral clustering with discriminative
regularization. In Proceedings of the 25th AAAI, 2011.

[30] Y. Yang, D. Xu, F. Nie, S. Yan, and Y. Zhuang.

Image clustering using local discriminant models and
global integration. IEEE Transactions on Image
Processing, 19(10):2761–2773, 2010.

[31] Y. Yang, Y. Yang, H. T. Shen, Y. Zhang, X. Du, and

X. Zhou. Discriminative nonnegative spectral
clustering with out-of-sample extension. IEEE TKDE,
25(8):1760–1771, 2013.

[32] H. Zeng and Y. Cheung. Feature selection and kernel
learning for local learning-based clustering. IEEE
Transactions on PAMI, 33(8):1532–1547, 2011.
[33] Z. Zhao and H. Liu. Semi-supervised feature selection
via spectral analysis. In Proceedings of the 7th SIAM
SDM, pages 641–646, 2007.

[34] Z. Zhao and H. Liu. Spectral feature selection for

supervised and unsupervised learning. In Proceedings

of the 24th ICML, pages 1151–1157, 2007.
[35] Z. Zhao, L. Wang, and H. Liu. Eﬃcient spectral
feature selection with minimum redundancy. In
Proceedings of the 24th AAAI, pages 673–678, 2010.
[36] Z. Zhao, L. Wang, H. Liu, and J. Ye. On similarity

preserving feature selection. IEEE TKDE,
25(3):619–632, 2013.

