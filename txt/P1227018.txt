Learning Word Embeddings from Intrinsic and Extrinsic Views

Jifan Chen1, Kan Chen1, Xipeng Qiu1, Qi Zhang1, Xuanjing Huang1, Zheng Zhang2
1 Shanghai Key Laboratory of Data Science
School of Computer Science, Fudan University
{jfchen14,kchen13,xpqiu,qz,xjhuang}@fudan.edu.cn
2 Department of Computer Science, New York University Shanghai
zz@nyu.edu

Abstract

While word embeddings are currently predominant for natural language processing, most of ex-
isting models learn them solely from their contexts. However, these context-based word embed-
dings are limited since not all words’ meaning can be learned based on only context. Moreover,
it is also difﬁcult to learn the representation of the rare words due to data sparsity problem. In
this work, we address these issues by learning the representations of words by integrating their
intrinsic (descriptive) and extrinsic (contextual) information. To prove the effectiveness of our
model, we evaluate it on four tasks, including word similarity, reverse dictionaries,Wiki link pre-
diction, and document classiﬁcation. Experiment results show that our model is powerful in both
word and document modeling.

1

Introduction

Word embeddings, also known as distributed word representations, can better capture a large number of
syntactic and semantic word relationships (Mikolov et al., 2013b), and have been proven to be useful in
many NLP tasks, such as language modeling (Bengio et al., 2006), parsing (Socher et al., 2013), relation
extraction (Riedel et al., 2013), discourse relation detection (Chen et al., 2016) and so on.

Most of the existing methods for learning word embeddings try to predict the current word using its
context either through a neural network (Bengio et al., 2006; Mikolov et al., 2010) or a simple log-
linear model (Mikolov et al., 2013a). However, such purely data-driven approaches suffer a number of
problems in practice, for example:

1. With limited length of context window, some words with entirely different meanings may share
very similar context. Embeddings of such words become indistinguishable (Hill et al., 2016). For
example, “old” and “new”.

2. Context-based models are unable to generate reasonable embeddings for rare words due to data

sparsity problem.

learned only by their contexts.

3. The meaning of some words, especially for some named entities, places, people, are hard to be

To address the above issues, we refer to the cognitive process of human learning. For a language unit

(word or phrase), we can learn its meaning from two kinds of information:

Intrinsic information The intrinsic information can be a concise explanation (called deﬁnition or de-
scriptions) to the meaning of a language unit. Such explanations often offer deeper understandings
to a language unit.

Extrinsic information The extrinsic information can be contexts which surround a language unit and

help to determine its interpretation.

In this paper, we improve the word embedding by utilizing both intrinsic (descriptive) and extrinsic
(contextual) information. More speciﬁcally, given a word (or a phrase) and its description (or deﬁnition),

6
1
0
2
 
g
u
A
 
0
2
 
 
]
L
C
.
s
c
[
 
 
1
v
2
5
8
5
0
.
8
0
6
1
:
v
i
X
r
a

we ﬁrst generate its intrinsic representation from the description, and then learn the ﬁnal representations
with its context. Crucially, these two sources of information are fused within one uniﬁed framework.
While many implementation choices exist, as a proof of concept, we simply extend the well-known
Skip-gram (Mikolov et al., 2013a). In our model, Skip-gram is used in two different ways but within
one uniformly deﬁned loss function: the ﬁrst is to compute a representation of a word’s deﬁnition (de-
scription), and the second is to compute in-context word representation. We use Wikipedia pages in our
experiments, since words described by Wikipedia are often some named entities, and they are more likely
to face data sparsity problem. In addition, Wikipedia contains abundant intrinsic information of words
which is helpful for training.

The main contribution of this work is that, we integrate intrinsic and extrinsic information to learn the
distributed representations of words. The experiments show that word embeddings learned by our model
are signiﬁcantly better than the previous models on four different tasks.

We start by discussing the skip-gram model (Mikolov et al., 2013a) which is a well known method for
learning word embedding, our proposed model is an extension of their work.

2 Proposed Model

2.1 Skip-gram Model

Given a word sequence x1, x2, ..., xN , the task of skip-gram is to predict the surrounding words within
a certain distance based on the current one. More formally, the objective of skip-gram model is to
maximize the following average log probability:

where c is the size of training window, and the probability p(wt+j|wt) is deﬁned as:

L =

T
(cid:88)

1
T

(cid:88)

t=1

−c≤j≤c,j(cid:54)=0

log p(wt+j|wt),

p(wt+j|wt) =

wtv(cid:48)
exp(vT
w=1 exp(vT

wt+j )
wtv(cid:48)

(cid:80)W

w)

,

(1)

(2)

where vw and v(cid:48)
words in the vocabulary.

w are the “word” and “context” vector representations of w, and W is the number of

Since the cost of computing ∇ log p(wt+j|wt) is proportional to the vocabulary size W , this formation
is often impractical. In practice, an efﬁcient alternative way is to use Negative Sampling (Mikolov et al.,
2013a), which leads to the following objective:

L =

T
(cid:88)

1
T

(cid:88)

t=1

−c≤j≤c,j(cid:54)=0

k
(cid:88)

i=1

log σ(vT

wtv(cid:48)

wt+j ) +

Ewi ∼ Pn(w)[log σ(−vT

wtv(cid:48)

wi)],

(3)

where k is the number of negative samples, σ denotes the sigmoid function, and Pn(w) is a noise dis-
tribution which is often set to the unigram distribution U (w) raised to the 3/4rd power. The task of this
objective is to distinguish the target word from the noise distribution Pn(w) using logistic regression. In
the following of this paper, we will use (3) instead of (1) as the objective function.

2.2 Deﬁnition Enriched Word Embedding

We assume a collection of document C = (D1, D2, ...DC), which offers deﬁnitions or descriptions to
the corresponding word, and each document Di consists of a sequence of words Di = (w1, w2, ..., wN ),
and R(wi) = Dj if wi has a link (or reference) to another document Dj.

Figure 1: The framework of our model.

Intrinsic Representation from Deﬁnition

2.2.1
There exists many commonly used compositional methods to construct document embedding using word
embeddings, including simple word embedding addition, multiplication (Mitchell and Lapata, 2010),
Recurrent Neural Network (Hochreiter and Schmidhuber, 1997), Convolution Neural Network (Hu et
al., 2014), Recursive Neural Network (Socher et al., 2011) and so on. It is reported in the work of Hill
et al. (2015) and Blacoe and Lapata (2012), that although the word embedding addition is simple, it
still achieves comparable performance on several tasks compared with neural networks like RNN which
is much more time consuming since it involves lots of non-linear computations. Concerning with the
training efﬁciency, we in this work use the simple word embedding addition to represent document
embedding:

(cid:88)

vdi =

αj · vwj ,

(4)

wj ∈Di
where vwj is the embedding of word wj, and αj is its corresponding weight. In the simplest case, all the
weights are equal to 1, while the weights can be also set to TF-IDF weights to avoid the affects of the
stop words.

2.2.2 Learning Word Embedding with its Deﬁnition
We implement our model by adding a loss term to Skip-gram, and the learning framework is shown in
Figure 1. For a word wt that has a link to its deﬁnition, the objective function is:

L =

(cid:88)

−c≤j≤c,j(cid:54)=0

k
(cid:88)

i=1

log σ(vT

wtv(cid:48)

wt+j ) +

Ewi ∼ Pn(w)[log σ(−vT

wtv(cid:48)

wi)] + log σ(vT

wtvR(wt)),

(5)

and the words without external descriptions are computed as Skip-gram (i.e. without the last term in the
equation). Notice that for a word with a link or reference, the change of its embedding also modiﬁes the
embedding of its description document, and therefore indirectly changes other word embeddings in that
document. We call our generated embeddings as Deﬁnition Enriched Word Embedding (DEWE).

3 Experiment

In this section, we ﬁrst describe how our training set is constructed, and report experimental results that
test the effectiveness and robustness of our model.

3.1 Dataset
We construct our training set by extracting wiki pages from Wikipedia using the API provided1. The
Wikipedia is organized in tree structure by document categories2, and each child node in the tree is one
of the subcategories of its parent node. Since the pages in the same category are likely to reference each
other, we start by extracting the pages in “Computer Science” category, and expand the dataset by adding
the pages from its subcategories. We limit the search depth in the tree to 3, because the deeper the search
depth is, the less relevant the pages are. After combing the same pages, our dataset contains 21,234
unique pages and approximately 150,000 unique words.

1https://www.mediawiki.org/wiki/API:Main page
2https://en.wikipedia.org/wiki/Portal:Contents/Categories

Word
Skip-gram
DEWE
Word
Skip-gram
DEWE
Word

DEWE
Word
Skip-gram
DEWE

Table 1: Top 5 nearest neighbors of the given word.

Microsoft
1. GroupWise 2. Apple Inc. 3. Novell 4. AdWords 5. Sun
1. IBM 2. Symantec 3. MS-DOS 4. Google 5. Internet Explorer
Bill Gates
1. Steve Jobs 2. Zuckerber 3. Reid 4. Poole 5. Bryant
1. Steve Jobs 2. Microsoft 3. Zuckerber 4. IBM 5. Windows
Database

1. DBMS 2. SQL 3. Data 4. Software 5. Microsoft Excel
WordNet
1. DSSSL 2. Metamodeling 3. Leet 4. Thesaurus 5. RMIAS
1. Synsets 2. Tatoeba 3. BabelNet 4. lexical 5. dictionaries

Skip-gram 1. NVD 2. Object database 3. Mobile device 4. Computer network 5. Document

3.1.1 Data Preprocessing

To make the training for the words with links sufﬁcient enough, we enforce the following invariant: for
all occurrences of a word, its link status is consistent. That is, a word either has no link at all, or it points
to the same document. In practice, that means we add a link to new occurrences for a word if it has a
link before. By enforcing this invariant, approximately 1% words in the training set have a link to the
deﬁnition pages.

3.2

Implementation Details

For the implementation of our model, we did not conduct a hyper-parameter search on any validation
dataset, instead, we choose the standard parameters that performs well based on the previous research.
We choose the size of context window to be 3, the number of negative samples in the negative sampling
part to be 5. We use the ﬁrst 100 words in a document to construct its document embedding, as in
Wikipedia, a page often contains thousands of words and the beginning part usually offers deﬁnition to
the word it describes. We ignore the words which appear less than 10 times in the training set, and we
use the TF-IDF weight to compose the document embedding. We set the embedding dimension to be 50,
the batch size to be 10, the initial learning rate to be 0.5, and the total training epoch is set to 10. The
model is implemented with Theano (Bergstra et al., 2010) and trained with mini-batch on GPU, and the
learning rate is decreased by the proportion of the trained words and the total words ( ˇReh˚uˇrek and Sojka,
2010). The parameters are kept the same for all of the experiments.

3.3 Competitors

For the task of word similarity, we compare our model with Skip-gram and Glove (Pennington et al.,
2014) which is also one of the state-of-the-art word embedding learning method, to see how the def-
initions affect the generated word embeddings. For the task of reverse dictionary, there exists some
commercial systems of reverse dictionary like Onelook.com3, and recently Hill et al. (2015) also im-
plemented a reverse dictionary using word embeddings. However, the dataset they used to build their
model are ordinary dictionaries, while our model is trained with Wikipedia which contains many enti-
ties, thus making it hard to do the comparison. Based on this, we compare our model with Skip-gram
to see how much improvement we can achieve by adding the loss term. More speciﬁcally, we compose
the document embedding using word embeddings trained with Skip-gram by point wise addition and
multiplication, namely Skip-gram Add and Skip-gram Mult, which are established ways to construct
document embedding. For link prediction and document classiﬁcation, besides Skip-gram, we also com-
pare with Paragraph Vector (PV) (Le and Mikolov, 2014) which is a unsupervised method to represent
sentence and paragraph.

3http://www.onelook.com/reverse-dictionary.shtml

Table 2: Performance of our proposed methods on several word similarity datasets.

WS-353 MEN MTurk-771 YP-130 SimLex-999

Skip-gram 44.57% 37.08%
45.35% 32.93%
43.97% 38.47%

Glove
DEWE

31.95%
35.29%
36.93%

4.25%
8.64%
13.82%

17.88%
20.04%
21.46%

3.4 Word Similarity

Although it is hard to evaluate word embeddings directly (Faruqui et al., 2016), we in this section evaluate
the generated word embeddings of our model to see what characteristics are captured. The evaluation
is divided into two parts, the ﬁrst part is a qualitative analysis about the words with references, and the
second part is a word similarity evaluation about the words without references.

3.4.1 Qualitative Analysis

Table 1 shows the nearest neighbors of some entity names we selected from the training set. See “Mi-
crosoft” and “Bill Gates” ﬁrst, it is easy to observe that the nearest neighbors returns by Skip-gram only
contains company names and person names respectively. This is because Skip-gram is a context-based
method, which captures word co-occurrence statistics. It means words are similar only if they share
similar context. However, seen from the results of our model, DEWE returns “MS-DOS” and “Internet
Explorer” for “Microsoft”, and returns “Microsoft” and “Windows” for “Bill Gates”, which are actually
relevant while they are unlikely to appears in similar context. Such results demonstrate the deﬁnition
“enrichment” to the context-based word embeddings.

Next, we consider “Database” and “WordNet”. Since they are rare words in the training set, and there
are few words in the training set sharing similar context with this two words, so the Skip-gram model
fails to return relevant words. In such situation, our model still returns satisﬁed results like “DBMS” and
“SQL” to “Database”, “Synsets” and “lexical” to “WordNet”, which come from the deﬁnition. It shows
when facing data sparsity problem, the deﬁnition “enrichment” is more signiﬁcant.

3.4.2 Word Similarity of Ordinary Words

Word similarity between word pairs can be generally divided into two aspects, concept similarity and
word association (Hill et al., 2016). The difference between the two aspects can be exempliﬁed by
word pairs like {coast, shore} and {clothes, closet}. Coast and shore are similar since their deﬁnitions
are similar, while clothes is associated with closet since they frequently occur together in space and
language. We in this section conduct experiment on several commonly used word similarity datasets
including WS-353 (Finkelstein et al., 2001), MEN (Bruni et al., 2012), MTurk-771 (Halawi et al., 2012),
YP-130 (Yang and Powers, 2006), SimLex-999 (Hill et al., 2016), which contain only ordinary words,
to see which aspect is captured by our model. The experiment is done by using the web tool provided
by Faruqui and Dyer (2014).

The results are shown in Table 2. As we can see from the table, DEWE signiﬁcantly outperforms Skip-
gram on YP-130 and SimLex-999. This result is impressive, since SimLex-999 is a dataset especially
designed for concept similarity, and YP-130 deﬁnes similarity based on WordNet taxonomy, it is obvious
that although such ordinary words do not have a link to their deﬁnitions, their meanings are still enriched
by the deﬁnitions of other words!

WS-353, MEN, and MTurk-771 consider both association and concept similarity as word similarity,
while human annotators show different predilections to this two kind of similarities in different datasets.
Our model outperforms Skip-gram on MEN and MTurk-771, while achieves a lower performance on
WS-353. It demonstrates that our word embedding can hold both word association and concept similarity
at the same time.

Glove is also a context based method, which leads to its less satisﬁed performance on YP-130 and
SimLex-999. However, since it combines the global word co-occurrence statistics as well as local context
window information, it generally performs better than Skip-gram.

Table 3: The difference between the Wikipedia deﬁnition and the manually-written deﬁnition.

Word

Blog

Test set

Description
A blog is a discussion or informational site published on the World Wide
Wikipedia Web consisting of discrete entries (”posts”) typically displayed in reverse
Deﬁnition

chronological order...

Human
Description

Websites where owners can upload essays and pictures and interact with
other users
Microsoft Corporation is an American multinational technology company
in Redmond, Washington, that develops, manufactures, licenses, supports
and sells computer software, consumer electronics and personal computers

Wikipedia
Microsoft Deﬁnition

Human
Description

A technology company founded by Bill Gates, most famous for its Windows
operating systems

3.5 Reverse Dictionary

As opposed to the regular dictionary that maps words to its deﬁnitions, reverse dictionary performs the
converse mapping. It addresses the “word is on the tip of my tongue, but I can not quite remember it”
problem by returning a set of candidate words given a deﬁnition or description of a word (Zock and
Bilac, 2004; Shaw et al., 2013). We implement this by ﬁrst computing the document embedding of the
given description, and then return the words corresponding to the embeddings which are closest to that
document embedding using cosine similarity.

To test the effectiveness and robustness of our model, we design two types of the test data. The ﬁrst one
contains 200 randomly selected documents in the training dataset, which have been seen during training.
The second one contains 100 unseen descriptions write by humans which are quite different from those
in the training set. To do so, we randomly select 100 page names among the top 1000 most frequent page
names appear in the training set. We then ask 10 graduate students of Computer Science Department to
write descriptions towards these page names based on their own understandings. To ensure the quality
of these description, we also ask another two graduate students to predict 5 words according to the
descriptions, if the target name is failed to be predicted by one of the checkers, the original participant is
asked to rewrite the description.

An example of the two kind of descriptions towards the same word is shown is Table 3, as we can see
from the table that the deﬁnition in Wikipedia is much more formal and longer than the manually-written
descriptions, and the human writers tend to describe entities using its most distinguishable characteristics.

3.5.1 Results
The performances of all the models are shown in Table 4. Here we follow the measurements used in Hill
et al. (2015): the median rank of the correct answer (lower better), the proportion of the correct answer
appearing in top 10/100 words (accuracy@10/100 higher better), and the standard variance of the rank
of the correct answer (rank variance lower better). We do not report the performance of Skip mult, since
it almost fails to return the corresponding word given any descriptions.

Seen from the results of the original Wikipedia Deﬁnition ﬁrst, the ﬁrst highlight is that our DEWE
model outperforms Skip-gram signiﬁcantly. In the case of DEWE+tﬁdf, it returns 90% of the correct
words among the top-ten candidates with median rank equals to 2.
It demonstrates that despite the
deﬁnition is a relatively long document containing severals sentences, our model still has the ability to
map it to the corresponding word by using the simple add operation.

There is also an interesting observation that although our DEWE model is trained with TF-IDF
weights, it still achieves fairly good performances (better than Skip-gram+tﬁdf) without using TF-IDF
weights in the test phase. It shows that our model also has the ability of fault tolerance, since the stop
words can be seen as the noisy data in the description.

Seen from the results of the Manually-written Descriptions, we can ﬁnd that the performances of

Table 4: The performance of different models on reverse dictionary. Notice that our DEWE model is
trained with TF-IDF weight, so DEWE+tﬁdf here denotes using TF-IDF weights in the test phase.

Test set
Skip add
Skip add+tﬁdf
DEWE
DEWE+tﬁdf

Wikipedia Deﬁnition

231
110
75
2

6.5%/27.5% 243
26.5%/68.5% 178
36.5%/78.5% 141
90.0%/99.0% 3

Manually-written Description
310
2.0%/10.0%
425
256
8.0%/25.0%
257
16.0%/49.0%
186
227
35.0%/ 67.0% 200
112

median rank accuracy@10/100 rank variance

all methods decrease a lot. This result is reasonable, as we can see in Table 3, the manually-written
descriptions are much shorter than the training document which contains 100 words in our experiment.
Moreover, the manually-written descriptions may describe the words from a quite different perspective
based on some common senses, which are totally different from the deﬁnitions in Wikipeida. Facing
such situation, the Skip-gram model almost fails to return correct answers, while our model still achieves
a satisﬁed performance. This results shows that our model can also generalize well to new descriptions
which are never seen before.

3.5.2 Qualitative Analysis
We list some example outputs of manually-written descriptions from different models in Table 5. As
we can see from the table, given the description, the Skip-gram fails to return any relevant words which
is consistent with the results in Table 4. While our model not only returns the correct answer, but also
returns other relevant names according to the description.

From the results, we can also see how the TF-IDF weights affect the result. In the ﬁrst case, both
DEWE and DEWE+tﬁdf return “Microsoft”, however, DEWE+tﬁdf returns “Bill Gates” ahead of “Mi-
crosoft”, it demonstrate that “Bill Gates” has a relatively high weight in the document, so it has a large
impact to the result. While in the second case, we can see the effectiveness of the TF-IDF weights. It
is easy to observe that DEWE is heavily affected by “media”, so that the words it returns are all about
media, and it fails to return “Facebook”. However, with word weights counted, our model successfully
returns the correct answer and some other relevant companies.

3.6 Link Prediction and Document Classiﬁcation

In Wikipedia, there is never a completely isolated document. A wiki entry contains multiple referencing
hyperlinks to other related entries which provide further understandings to this entry. Given a Wikipedia
document, the task of link prediction is to ﬁnd other documents which have a link to this document.

Table 5: Top 5 candidate words returned for manually-written descriptions.

Description

Skip add
Skip add+tﬁdf
DEWE
DEWE+tﬁdf

Description

A technology company founded by Bill Gates most famous for its

Windows operating systems

1. AlterGeo 2. Project Athena 3. Milkymist 4. Lookout 5. Fabasoft
1. Bill Gates 2. Lookout 3. Apple Newton 4. Taligent 5. Ultima
1. Microsoft 2. Wintel 3. Linux 4. IBM 5. Nimbula
1. Bill Gates 2. Microsoft 3. Apple 4. IBM 5. Windows NT
The largest social media company in the world founded by

Mark Zuckerberg

Skip add
Skip add+tﬁdf
DEWE
DEWE+tﬁdf

1. NBII 2. comScore 3. FSMK 4. I3p 5. Optimizely
1. PRIVO 2. kathy 3. Elon 4. astronaut 5. cade
1. Mavshack 2. Media Temple 3. Website 4. Epos Now 5. Compupress
1. Facebook 2. WhoSay 3. Blog 4. Instagram 5. Google+

Table 6: The performance of different models on link prediction and document classiﬁcation.

Task

Skip add
Skip add+tﬁdf
Skip mult
Skip mult+tﬁdf
PV
DEWE
DEWE+tﬁdf

Link Prediction

Document Classiﬁcation

MAP@10 Recall@10 Macro-F1 Micro-F1 Weighted-F1
9.60%
14.09%
-
-
10.48%
10.04%
16.85%

43.77%
47.36%
32.71%
10.88%
38.99%
44.46%
50.12%

21.22%
25.86%
-
-
23.21%
23.30%
29.97%

15.44%
19.98%
-
-
16.30%
15.67%
22.13%

20.03%
21.10%
14.71%
12.44%
19.22%
20.84%
23.51%

We achieve this by constructing the document embeddings of all documents ﬁrst and then return the
candidate documents which have the shortest cosine distance to a given document. All documents in the
training set are used for evaluation, and the evaluation metrics we used are MAP and Recall. Since the
embeddings trained by our model will contain the information from the referencing document, we hope
that this information could help with the link prediction.

The task of document classiﬁcation is to do a multi-class classiﬁcation of the documents according
to their categories in the Wikipedia. We assume that documents from the same categories are likely to
reference each other. Since we map words to their deﬁnitions, these documents can be seen as containing
the information of each other through the words with hyperlinks, so that they will be close to each other
in the embedding space. To do the classiﬁcation, we construct the document embeddings as above, and
then feed them along with their labels to a supervised classiﬁer. We use a 10-fold cross-validation over
the entire training set for the evaluation via F1 Score.

The results on the two tasks are shown in Table 6. We ignore the results of Skip Mult on document
classiﬁcation for its poor performance. Our DEWE model achieves best performance on both tasks, it
demonstrates that not only the word embedding got enriched by the deﬁnition, but also the document
embedding. Based on all of the experiment above, we can conclude that our model is effective for both
word and document modeling.

4 Related Work

Distributed word representations are ﬁrst introduced by Rumelhart et al. (1988), and have been success-
fully used in a lot of NLP tasks, including language modeling (Bengio et al., 2006), parsing (Socher et
al., 2013), disambiguation (Collobert et al., 2011), and many others.

Previously, word embeddings are often the by-product of a language model (Bengio et al., 2006;
Collobert and Weston, 2008; Mikolov et al., 2010). However, such methods are often time consuming
and involve lots of non-linear computations. Recently, Mikolov et al. (2013a) proposed two log-linear
models, namely CBOW and Skip-gram, to learn word embedding directly from a large-scale text corpus
efﬁciently. Glove, proposed by Pennington et al. (2014), is also an efﬁcient word embedding learning
framework, which combines the global word co-occurrence statistics as well as local context window
information.

All of the methods mentioned above are mainly context-based models, and there exists some other
works extending such models by using external knowledge. Bian et al. (2014) used semantic, morpho-
logical and syntactic knowledge to compute more effective word embeddings, Liu et al. (2015) used LDA
to generate topical based word embeddings, and Rothe and Sch¨utze (2015) used WordNet as a lexical
resource to learn embeddings for synsets and lexemes. In this work, we improve the word embeddings
by utilizing both context (extrinsic) and deﬁnition (intrinsic) information.

5 Conclusion
In this work, we learn word embeddings from both their intrinsic deﬁnitions and extrinsic contexts.
Speciﬁcally, we extend the context-based model by mapping a word to its deﬁnition. Evaluations on four
tasks demonstrates that our learning method is more effective than the previous context-based models.

References

[Bengio et al.2006] Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gau-
In Innovations in Machine Learning, pages 137–186.

vain. 2006. Neural probabilistic language models.
Springer.

[Bergstra et al.2010] James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guil-
laume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: A cpu and gpu math
compiler in python. In Proc. 9th Python in Science Conf, pages 1–7.

[Bian et al.2014] Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014. Knowledge-powered deep learning for word em-
bedding. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages
132–148. Springer.

[Blacoe and Lapata2012] William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representa-
tions for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning, pages 546–556. Association for Com-
putational Linguistics.

[Bruni et al.2012] Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran. 2012. Distributional seman-
tics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 136–145. Association for Computational Linguistics.

[Chen et al.2016] Jifan Chen, Qi Zhang, Pengfei Liu, and Xuanjing Huang. 2016. Discourse relations detection
via a mixed generative-discriminative framework. In Thirtieth AAAI Conference on Artiﬁcial Intelligence.

[Collobert and Weston2008] Ronan Collobert and Jason Weston. 2008. A uniﬁed architecture for natural language
processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference
on Machine learning, pages 160–167. ACM.

[Collobert et al.2011] Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and
Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

[Faruqui and Dyer2014] Manaal Faruqui and Chris Dyer. 2014. Community evaluation and exchange of word
vectors at wordvectors.org. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics: System Demonstrations, Baltimore, USA, June. Association for Computational Linguistics.

[Faruqui et al.2016] Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, and Chris Dyer. 2016. Problems with

evaluation of word embeddings using word similarity tasks. arXiv preprint arXiv:1605.02276.

[Finkelstein et al.2001] Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolf-
man, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of the 10th
international conference on World Wide Web, pages 406–414. ACM.

[Halawi et al.2012] Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and Yehuda Koren. 2012. Large-scale learn-
ing of word relatedness with constraints. In Proceedings of the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1406–1414. ACM.

[Hill et al.2015] Felix Hill, Kyunghyun Cho, Anna Korhonen, and Yoshua Bengio. 2015. Learning to understand

phrases by embedding the dictionary. arXiv preprint arXiv:1504.00548.

[Hill et al.2016] Felix Hill, Roi Reichart, and Anna Korhonen. 2016. Simlex-999: Evaluating semantic models

with (genuine) similarity estimation. Computational Linguistics.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory.

Neural computation, 9(8):1735–1780.

[Hu et al.2014] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network
architectures for matching natural language sentences. In Advances in Neural Information Processing Systems,
pages 2042–2050.

[Le and Mikolov2014] Quoc V Le and Tomas Mikolov. 2014. Distributed representations of sentences and docu-

ments. In ICML, volume 14, pages 1188–1196.

[Liu et al.2015] Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. 2015. Topical word embeddings. In

AAAI, pages 2418–2424.

[Mikolov et al.2010] Tomas Mikolov, Martin Karaﬁ´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010.

Recurrent neural network based language model. In Interspeech, volume 2, page 3.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efﬁcient estimation of

word representations in vector space. arXiv preprint arXiv:1301.3781.

[Mikolov et al.2013b] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Dis-
tributed representations of words and phrases and their compositionality. In Advances in Neural Information
Processing Systems, pages 3111–3119.

[Mitchell and Lapata2010] Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of se-

mantics. Cognitive science, 34(8):1388–1429.

[Pennington et al.2014] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global
vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing
(EMNLP 2014), 12.

[ ˇReh˚uˇrek and Sojka2010] Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Framework for Topic Modelling with
Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages
45–50, Valletta, Malta, May. ELRA. http://is.muni.cz/publication/884893/en.

[Riedel et al.2013] Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M Marlin. 2013. Relation

extraction with matrix factorization and universal schemas.

[Rothe and Sch¨utze2015] Sascha Rothe and Hinrich Sch¨utze. 2015. Autoextend: Extending word embeddings to

embeddings for synsets and lexemes. arXiv preprint arXiv:1507.01127.

[Rumelhart et al.1988] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1988. Learning represen-

tations by back-propagating errors. Cognitive modeling, 5(3):1.

[Shaw et al.2013] Ryan Shaw, Anindya Datta, Debra VanderMeer, and Kaushik Dutta. 2013. Building a scalable
database-driven reverse dictionary. IEEE Transactions on Knowledge and Data Engineering, 25(3):528–540.

[Socher et al.2011] Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Y Ng.
2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural
Information Processing Systems, pages 801–809.

[Socher et al.2013] Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013. Parsing with

compositional vector grammars. In ACL (1), pages 455–465.

[Yang and Powers2006] Dongqiang Yang and David MW Powers. 2006. Verb similarity on the taxonomy of Word-

Net. Masaryk University.

[Zock and Bilac2004] Michael Zock and Slaven Bilac. 2004. Word lookup on the basis of associations: from an
idea to a roadmap. In Proceedings of the Workshop on Enhancing and Using Electronic Dictionaries, pages
29–35. Association for Computational Linguistics.

