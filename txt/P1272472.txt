Neural Natural Language Inference Models Enhanced with
External Knowledge

Qian Chen
University of Science and
Technology of China
cq1231@mail.ustc.edu.cn

Zhen-Hua Ling
University of Science and
Technology of China
zhling@ustc.edu.cn

Xiaodan Zhu
ECE, Queen’s University
xiaodan.zhu@queensu.ca

Diana Inkpen
University of Ottawa
diana@site.uottawa.ca

Si Wei
iFLYTEK Research
siwei@iflytek.com

Abstract

Modeling natural language inference is a
very challenging task. With the avail-
ability of large annotated data, it has re-
cently become feasible to train complex
models such as neural-network-based in-
ference models, which have shown to
achieve the state-of-the-art performance.
Although there exist relatively large anno-
tated data, can machines learn all knowl-
edge needed to perform natural language
inference (NLI) from these data? If not,
how can neural-network-based NLI mod-
els beneﬁt from external knowledge and
how to build NLI models to leverage it?
In this paper, we enrich the state-of-the-art
neural natural language inference models
with external knowledge. We demonstrate
that the proposed models improve neural
NLI models to achieve the state-of-the-art
performance on the SNLI and MultiNLI
datasets.

last

larger

In the

several years,

anno-
the
tated datasets were made available, e.g.,
(Bowman et al., 2015) and MultiNLI
SNLI
datasets (Williams et al., 2017), which made
it feasible to train rather complicated neural-
network-based models that ﬁt a large set of
parameters to better model NLI. Such models
have shown to achieve the state-of-the-art per-
formance (Bowman et al., 2015, 2016; Yu and
Munkhdalai, 2017b; Parikh et al., 2016; Sha et al.,
2016; Chen et al., 2017a,b; Tay et al., 2018).

While neural networks have been shown to be
very effective in modeling NLI with large train-
ing data, they have often focused on end-to-end
training by assuming that all inference knowledge
is learnable from the provided training data.
In
this paper, we relax this assumption and explore
whether external knowledge can further help NLI.
Consider an example:

• p: A lady standing in a wheat ﬁeld.

• h: A person standing in a corn ﬁeld.

1

Introduction

Reasoning and inference are central to both hu-
man and artiﬁcial intelligence. Natural language
inference (NLI), also known as recognizing tex-
tual entailment (RTE), is an important NLP prob-
lem concerned with determining inferential rela-
tionship (e.g., entailment, contradiction, or neu-
tral) between a premise p and a hypothesis h. In
general, modeling informal inference in language
is a very challenging and basic problem towards
achieving true natural language understanding.

In this simpliﬁed example, when computers are
asked to predict the relation between these two
sentences and if training data do not provide the
knowledge of relationship between “wheat” and
“corn” (e.g., if one of the two words does not ap-
pear in the training data or they are not paired in
any premise-hypothesis pairs), it will be hard for
computers to correctly recognize that the premise
contradicts the hypothesis.

In general, although in many tasks learning tab-
ula rasa achieved state-of-the-art performance, we
believe complicated NLP problems such as NLI

8
1
0
2
 
n
u
J
 
3
2
 
 
]
L
C
.
s
c
[
 
 
3
v
9
8
2
4
0
.
1
1
7
1
:
v
i
X
r
a

could beneﬁt from leveraging knowledge accumu-
lated by humans, particularly in a foreseeable fu-
ture when machines are unable to learn it by them-
selves.

In this paper we enrich neural-network-based
NLI models with external knowledge in co-
attention, local inference collection, and inference
composition components. We show the proposed
model improves the state-of-the-art NLI models
to achieve better performances on the SNLI and
MultiNLI datasets. The advantage of using exter-
nal knowledge is more signiﬁcant when the size of
training data is restricted, suggesting that if more
knowledge can be obtained, it may bring more
beneﬁt.
In addition to attaining the state-of-the-
art performance, we are also interested in under-
standing how external knowledge contributes to
the major components of typical neural-network-
based NLI models.

2 Related Work

Early research on natural language inference and
recognizing textual entailment has been performed
on relatively small datasets (refer to MacCartney
(2009) for a good literature survey), which in-
cludes a large bulk of contributions made under
the name of RTE, such as (Dagan et al., 2005;
Iftene and Balahur-Dobrescu, 2007), among many
others.

More recently the availability of much larger
annotated data, e.g., SNLI
(Bowman et al.,
2015) and MultiNLI (Williams et al., 2017), has
made it possible to train more complex mod-
els. These models mainly fall into two types
of approaches: sentence-encoding-based models
and models using also inter-sentence attention.
Sentence-encoding-based models use Siamese ar-
chitecture (Bromley et al., 1993). The parameter-
tied neural networks are applied to encode both
the premise and the hypothesis. Then a neural
network classiﬁer is applied to decide relationship
between the two sentences. Different neural net-
works have been utilized for sentence encoding,
such as LSTM (Bowman et al., 2015), GRU (Ven-
drov et al., 2015), CNN (Mou et al., 2016), BiL-
STM and its variants (Liu et al., 2016c; Lin et al.,
2017; Chen et al., 2017b; Nie and Bansal, 2017),
self-attention network (Shen et al., 2017, 2018),
and more complicated neural networks (Bowman
et al., 2016; Yu and Munkhdalai, 2017a,b; Choi
et al., 2017). Sentence-encoding-based models

transform sentences into ﬁxed-length vector rep-
resentations, which may help a wide range of
tasks (Conneau et al., 2017).

The second set of models use inter-sentence at-
tention (Rockt¨aschel et al., 2015; Wang and Jiang,
2016; Cheng et al., 2016; Parikh et al., 2016;
Chen et al., 2017a). Among them, Rockt¨aschel
et al. (2015) were among the ﬁrst to propose neu-
ral attention-based models for NLI. Chen et al.
(2017a) proposed an enhanced sequential infer-
ence model (ESIM), which is one of the best mod-
els so far and is used as one of our baselines in this
paper.

In this paper we enrich neural-network-based
NLI models with external knowledge. Unlike
early work on NLI (Jijkoun and de Rijke, 2005;
MacCartney et al., 2008; MacCartney, 2009) that
explores external knowledge in conventional NLI
models on relatively small NLI datasets, we aim to
merge the advantage of powerful modeling ability
of neural networks with extra external inference
knowledge. We show that the proposed model
improves the state-of-the-art neural NLI models
to achieve better performances on the SNLI and
MultiNLI datasets. The advantage of using exter-
nal knowledge is more signiﬁcant when the size of
training data is restricted, suggesting that if more
knowledge can be obtained, it may have more ben-
eﬁt.
In addition to attaining the state-of-the-art
performance, we are also interested in understand-
ing how external knowledge affect major compo-
nents of neural-network-based NLI models.

In general, external knowledge has shown to be
effective in neural networks for other NLP tasks,
including word embedding (Chen et al., 2015;
Faruqui et al., 2015; Liu et al., 2015; Wieting
et al., 2015; Mrksic et al., 2017), machine trans-
lation (Shi et al., 2016; Zhang et al., 2017b), lan-
guage modeling (Ahn et al., 2016), and dialogue
systems (Chen et al., 2016b).

3 Neural-Network-Based NLI Models

with External Knowledge

In this section we propose neural-network-based
inference
NLI models to incorporate external
knowledge, which, as we will show later in Sec-
tion 5, achieve the state-of-the-art performance.
In addition to attaining the leading performance
we are also interested in investigating the effects
of external knowledge on major components of
neural-network-based NLI modeling.

Figure 1 shows a high-level general view of the
proposed framework. While speciﬁc NLI systems
vary in their implementation, typical state-of-the-
art NLI models contain the main components (or
equivalents) of representing premise and hypoth-
esis sentences, collecting local (e.g., lexical) in-
ference information, and aggregating and compos-
ing local information to make the global decision
at the sentence level. We incorporate and investi-
gate external knowledge accordingly in these ma-
jor NLI components: computing co-attention, col-
lecting local inference information, and compos-
ing inference to make ﬁnal decision.

3.1 External Knowledge

We

study the

incorporation of

As discussed above, although there exist relatively
large annotated data for NLI, can machines learn
all inference knowledge needed to perform NLI
from the data? If not, how can neural network-
based NLI models beneﬁt from external knowl-
edge and how to build NLI models to leverage it?
external,
inference-related knowledge in major compo-
nents of neural networks for natural
language
inference. For example,
intuitively knowledge
synonymy, antonymy, hypernymy and
about
hyponymy between given words may help model
soft-alignment between premises and hypotheses;
knowledge about hypernymy and hyponymy
may help capture entailment; knowledge about
antonymy and co-hyponyms (words sharing the
same hypernym) may beneﬁt the modeling of
contradiction.

In this section, we discuss the incorporation of
basic, lexical-level semantic knowledge into neu-
ral NLI components. Speciﬁcally, we consider ex-
ternal lexical-level inference knowledge between
word wi and wj, which is represented as a vec-
tor rij and is incorporated into three speciﬁc com-
ponents shown in Figure 1. We will discuss the
details of how rij is constructed later in the exper-
iment setup section (Section 4) but instead focus
on the proposed model in this section. Note that
while we study lexical-level inference knowledge
in the paper, if inference knowledge about larger
pieces of text pairs (e.g., inference relations be-
tween phrases) are available, the proposed model
can be easily extended to handle that. In this paper,
we instead let the NLI models to compose lexical-
level knowledge to obtain inference relations be-
tween larger pieces of texts.

3.2 Encoding Premise and Hypothesis

Same as much previous work (Chen et al.,
2017a,b), we encode the premise and the hypoth-
esis with bidirectional LSTMs (BiLSTMs). The
premise is represented as a = (a1, . . . , am) and
the hypothesis is b = (b1, . . . , bn), where m
and n are the lengths of the sentences. Then a
and b are embedded into de-dimensional vectors
[E(a1), . . . , E(am)] and [E(b1), . . . , E(bn)] using
the embedding matrix E ∈ Rde×|V |, where |V | is
the vocabulary size and E can be initialized with
the pre-trained word embedding. To represent
words in its context, the premise and the hypothe-
sis are fed into BiLSTM encoders (Hochreiter and
Schmidhuber, 1997) to obtain context-dependent
hidden states as and bs:

as
i = Encoder(E(a), i) ,
bs
j = Encoder(E(b), j) .

(1)

(2)

where i and j indicate the i-th word in the premise
and the j-th word in the hypothesis, respectively.

3.3 Knowledge-Enriched Co-Attention

As discussed above, soft-alignment of word pairs
between the premise and the hypothesis may ben-
eﬁt from knowledge-enriched co-attention mech-
anism. Given the relation features rij ∈ Rdr be-
tween the premise’s i-th word and the hypothesis’s
j-th word derived from the external knowledge,
the co-attention is calculated as:

eij = (as

i )Tbs

j + F (rij) .

(3)

The function F can be any non-linear or linear
functions. In this paper, we use F (rij) = λ1(rij),
where λ is a hyper-parameter tuned on the devel-
opment set and 1 is the indication function as fol-
lows:

1(rij) =

(cid:40)
1
0

if rij is not a zero vector ;
if rij is a zero vector .

(4)

Intuitively, word pairs with semantic relationship,
e.g., synonymy, antonymy, hypernymy, hyponymy
and co-hyponyms, are probably aligned together.
We will discuss how we construct external knowl-
edge later in Section 4. We have also tried a two-
layer MLP as a universal function approximator
in function F to learn the underlying combination
function but did not observe further improvement
over the best performance we obtained on the de-
velopment datasets.

Figure 1: A high-level view of neural-network-based NLI models enriched with external knowledge in
co-attention, local inference collection, and inference composition.

Soft-alignment

is determined by the co-
attention matrix e ∈ Rm×n computed in Equa-
tion (3), which is used to obtain the local relevance
between the premise and the hypothesis. For the
hidden state of the i-th word in the premise, i.e.,
as
i (already encoding the word itself and its con-
text), the relevant semantics in the hypothesis is
identiﬁed into a context vector ac
i using eij, more
speciﬁcally with Equation (5).

αij =

exp(eij)
k=1 exp(eik)

(cid:80)n

βij =

exp(eij)
k=1 exp(ekj)

(cid:80)m

, ac

i =

αijbs
j ,

(5)

, bc

j =

βijas
i ,

(6)

n
(cid:88)

j=1
m
(cid:88)

i=1

where α ∈ Rm×n and β ∈ Rm×n are the nor-
malized attention weight matrices with respect to
the 2-axis and 1-axis. The same calculation is per-
formed for each word in the hypothesis, i.e., bs
j,
with Equation (6) to obtain the context vector bc
j.

3.4 Local Inference Collection with External

Knowledge

By way of comparing the inference-related seman-
tic relation between as
i (individual word repre-
sentation in premise) and ac
i (context representa-
tion from hypothesis which is align to word as
i ),
we can model local inference (i.e., word-level in-
ference) between aligned word pairs. Intuitively,
for example, knowledge about hypernymy or hy-
ponymy may help model entailment and knowl-
edge about antonymy and co-hyponyms may help
model contradiction. Through comparing as
i and

n
(cid:88)

j=1
m
(cid:88)

i=1

ac
in addition to their relation from external
i ,
knowledge, we can obtain word-level inference
information for each word. The same calcula-
tion is performed for bs
j. Thus, we collect
knowledge-enriched local inference information:

j and bc

i = G([as
am

i ; ac

i ; as

i − ac

i ; as

i ◦ ac
i ;

αijrij]) , (7)

j = G([bs
bm

j, bc

j; bs

j − bc

j; bs

j ◦ bc
j;

βijrji]) ,

(8)

where a heuristic matching trick with difference
and element-wise product is used (Mou et al.,
2016; Chen et al., 2017a). The last terms in Equa-
tion (7)(8) are used to obtain word-level infer-
ence information from external knowledge. Take
Equation (7) as example, rij is the relation fea-
ture between the i-th word in the premise and
the j-th word in the hypothesis, but we care
more about semantic relation between aligned
word pairs between the premise and the hypoth-
esis. Thus, we use a soft-aligned version through
the soft-alignment weight αij. For the i-th word
in the premise, the last term in Equation (7) is
a word-level inference information based on ex-
ternal knowledge between the i-th word and the
aligned word. The same calculation for hypoth-
esis is performed in Equation (8). G is a non-
linear mapping function to reduce dimensionality.
Speciﬁcally, we use a 1-layer feed-forward neural
network with the ReLU activation function with
a shortcut connection, i.e., concatenate the hidden
states after ReLU with the input (cid:80)n
j=1 αijrij (or
(cid:80)m
(or bm
j ).

i=1 βijrji) as the output am
i

3.5 Knowledge-Enhanced Inference

4 Experiment Set-Up

Composition

In this component, we introduce knowledge-
enriched inference composition. To determine the
overall inference relationship between the premise
and the hypothesis, we need to explore a compo-
sition layer to compose the local inference vectors
(am and bm) collected above:

av
i = Composition(am, i) ,
j = Composition(bm, j) .
bv

(9)

(10)

Here, we also use BiLSTMs as building blocks
for the composition layer, but the responsibility
of BiLSTMs in the inference composition layer
is completely different from that in the input en-
coding layer. The BiLSTMs here read local in-
ference vectors (am and bm) and learn to judge
the types of local inference relationship and dis-
tinguish crucial local inference vectors for overall
sentence-level inference relationship. Intuitively,
the ﬁnal prediction is likely to depend on word
pairs appearing in external knowledge that have
some semantic relation. Our inference model con-
verts the output hidden vectors of BiLSTMs to
the ﬁxed-length vector with pooling operations
and puts it into the ﬁnal classiﬁer to determine
the overall inference class. Particularly, in addi-
tion to using mean pooling and max pooling sim-
ilarly to ESIM (Chen et al., 2017a), we propose
to use weighted pooling based on external knowl-
edge to obtain a ﬁxed-length vector as in Equation
(11)(12).

aw =

bw =

m
(cid:88)

i=1
n
(cid:88)

j=1

exp(H((cid:80)n
i=1 exp(H((cid:80)n
exp(H((cid:80)m
j=1 exp(H((cid:80)m

(cid:80)m

(cid:80)n

j=1 αijrij))

j=1 αijrij))

i=1 βijrji))

i=1 βijrji))

av
i ,

(11)

bv
j .

(12)

In our experiments, we regard the function H as
a 1-layer feed-forward neural network with ReLU
activation function. We concatenate all pooling
vectors, i.e., mean, max, and weighted pooling,
into the ﬁxed-length vector and then put the vector
into the ﬁnal multilayer perceptron (MLP) clas-
siﬁer. The MLP has one hidden layer with tanh
activation and softmax output layer in our exper-
iments. The entire model is trained end-to-end,
through minimizing the cross-entropy loss.

4.1 Representation of External Knowledge

Lexical Semantic Relations As described in
to incorporate external knowledge
Section 3.1,
(as a knowledge vector rij) to the state-of-the-
art neural network-based NLI models, we ﬁrst
explore semantic relations in WordNet (Miller,
1995), motivated by MacCartney (2009). Specif-
ically, the relations of lexical pairs are derived as
described in (1)-(4) below. Instead of using Jiang-
Conrath WordNet distance metric (Jiang and Con-
rath, 1997), which does not improve the perfor-
mance of our models on the development sets, we
add a new feature, i.e., co-hyponyms, which con-
sistently beneﬁt our models.

(1) Synonymy: It takes the value 1 if the words in
the pair are synonyms in WordNet (i.e., be-
long to the same synset), and 0 otherwise. For
example, [felicitous, good] = 1, [dog, wolf] =
0.

(2) Antonymy: It takes the value 1 if the words
in the pair are antonyms in WordNet, and 0
otherwise. For example, [wet, dry] = 1.

(3) Hypernymy: It takes the value 1 − n/8 if one
word is a (direct or indirect) hypernym of the
other word in WordNet, where n is the num-
ber of edges between the two words in hier-
archies, and 0 otherwise. Note that we ignore
pairs in the hierarchy which have more than 8
edges in between. For example, [dog, canid]
= 0.875, [wolf, canid] = 0.875, [dog, carni-
vore] = 0.75, [canid, dog] = 0

(4) Hyponymy: It is simply the inverse of the hy-
pernymy feature. For example, [canid, dog]
= 0.875, [dog, canid] = 0.

(5) Co-hyponyms: It takes the value 1 if the two
words have the same hypernym but they do
not belong to the same synset, and 0 other-
wise. For example, [dog, wolf] = 1.

As discussed above, we expect features like syn-
onymy, antonymy, hypernymy, hyponymy and co-
hyponyms would help model co-attention align-
ment between the premise and the hypothesis.
Knowledge of hypernymy and hyponymy may help
capture entailment; knowledge of antonymy and
co-hyponyms may help model contradiction. Their
ﬁnal contributions will be learned in end-to-end
model training. We regard the vector r ∈ Rdr as

the relation feature derived from external knowl-
edge, where dr is 5 here. In addition, Table 1 re-
ports some key statistics of these features.

Feature

#Words

#Pairs

Synonymy
Antonymy
Hypernymy
Hyponymy
Co-hyponyms

84,487
6,161
57,475
57,475
53,281

237,937
6,617
753,086
753,086
3,674,700

Table 1: Statistics of lexical relation features.

In addition to the above relations, we also use
more relation features in WordNet, including in-
stance,
instance of, same instance, entailment,
member meronym, member holonym, substance
meronym, substance holonym, part meronym, part
holonym, summing up to 15 features, but these ad-
ditional features do not bring further improvement
on the development dataset, as also discussed in
Section 5.

Relation Embeddings
In the most recent years
graph embedding has been widely employed to
learn representation for vertexes and their relations
in a graph.
In our work here, we also capture
the relation between any two words in WordNet
through relation embedding. Speciﬁcally, we em-
ployed TransE (Bordes et al., 2013), a widely used
graph embedding methods, to capture relation em-
bedding between any two words. We used two
typical approaches to obtaining the relation em-
bedding. The ﬁrst directly uses 18 relation em-
beddings pretrained on the WN18 dataset (Bordes
et al., 2013). Speciﬁcally, if a word pair has a cer-
tain type relation, we take the corresponding re-
lation embedding. Sometimes, if a word pair has
multiple relations among the 18 types; we take an
average of the relation embedding. The second ap-
proach uses TransE’s word embedding (trained on
WordNet) to obtain relation embedding, through
the objective function used in TransE, i.e., l ≈
t − h, where l indicates relation embedding, t in-
dicates tail entity embedding, and h indicates head
entity embedding.

Note that in addition to relation embedding
trained on WordNet, other relational embedding
resources exist; e.g.,
that trained on Freebase
(WikiData) (Bollacker et al., 2007), but such
knowledge resources are mainly about facts (e.g.,
relationship between Bill Gates and Microsoft)
and are less for commonsense knowledge used in

general natural language inference (e.g., the color
yellow potentially contradicts red).

4.2 NLI Datasets

In our experiments, we use Stanford Natural Lan-
guage Inference (SNLI) dataset (Bowman et al.,
2015) and Multi-Genre Natural Language Infer-
ence (MultiNLI) (Williams et al., 2017) dataset,
which focus on three basic relations between a
premise and a potential hypothesis:
the premise
entails the hypothesis (entailment), they contradict
each other (contradiction), or they are not related
(neutral). We use the same data split as in previ-
ous work (Bowman et al., 2015; Williams et al.,
2017) and classiﬁcation accuracy as the evaluation
metric. In addition, we test our models (trained on
the SNLI training set) on a new test set (Glockner
et al., 2018), which assesses the lexical inference
abilities of NLI systems and consists of 8,193 sam-
ples. WordNet 3.0 (Miller, 1995) is used to extract
semantic relation features between words. The
words are lemmatized using Stanford CoreNLP
3.7.0 (Manning et al., 2014). The premise and the
hypothesis sentences fed into the input encoding
layer are tokenized.

4.3 Training Details
For duplicability, we release our code1. All our
models were strictly selected on the development
set of the SNLI data and the in-domain devel-
opment set of MultiNLI and were then tested on
the corresponding test set. The main training de-
tails are as follows:
the dimension of the hid-
den states of LSTMs and word embeddings are
300. The word embeddings are initialized by
300D GloVe 840B (Pennington et al., 2014), and
out-of-vocabulary words among them are initial-
ized randomly. All word embeddings are updated
during training. Adam (Kingma and Ba, 2014)
is used for optimization with an initial learning
rate of 0.0004. The mini-batch size is set to 32.
Note that the above hyperparameter settings are
same as those used in the baseline ESIM (Chen
et al., 2017a) model. ESIM is a strong NLI
baseline framework with the source code made
available at https://github.com/lukecq1231/nli (the
ESIM core code has also been adapted to sum-
marization (Chen et al., 2016a) and question-
answering tasks (Zhang et al., 2017a)).

The

trade-off

λ

for

calculating

co-

1https://github.com/lukecq1231/kim

in Equation

attention
(3)
in
[0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50]
the
development set. When training TransE for
WordNet, relations are represented with vectors
of 20 dimension.

selected
on

is
based

5 Experimental Results

5.1 Overall Performance

Table 2 shows the results of state-of-the-art models
on the SNLI dataset. Among them, ESIM (Chen
et al., 2017a) is one of the previous state-of-the-art
systems with an 88.0% test-set accuracy. The pro-
posed model, namely Knowledge-based Inference
Model (KIM), which enriches ESIM with external
knowledge, obtains an accuracy of 88.6%, the best
single-model performance reported on the SNLI
dataset. The difference between ESIM and KIM is
statistically signiﬁcant under the one-tailed paired
t-test at the 99% signiﬁcance level. Note that the
KIM model reported here uses ﬁve semantic rela-
tions described in Section 4. In addition to that, we
also use 15 semantic relation features, which does
not bring additional gains in performance. These
results highlight the effectiveness of the ﬁve se-
mantic relations described in Section 4. To further
investigate external knowledge, we add TransE re-
lation embedding, and again no further improve-
ment is observed on both the development and test
sets when TransE relation embedding is used (con-
catenated) with the semantic relation vectors. We
consider this is due to the fact that TransE embed-
ding is not speciﬁcally sensitive to inference in-
formation; e.g., it does not model co-hyponyms
features, and its potential beneﬁt has already been
covered by the semantic relation features used.

Table 3 shows the performance of models on the
MultiNLI dataset. The baseline ESIM achieves
76.8% and 75.8% on in-domain and cross-domain
test set, respectively. If we extend the ESIM with
external knowledge, we achieve signiﬁcant gains
to 77.2% and 76.4% respectively. Again, the gains
are consistent on SNLI and MultiNLI, and we ex-
pect they would be orthogonal to other factors
when external knowledge is added into other state-
of-the-art models.

5.2 Ablation Results

Figure 2 displays the ablation analysis of differ-
ent components when using the external knowl-
edge. To compare the effects of external knowl-
edge under different training data scales, we ran-

Model

LSTM Att. (Rockt¨aschel et al., 2015)
DF-LSTMs (Liu et al., 2016a)
TC-LSTMs (Liu et al., 2016b)
Match-LSTM (Wang and Jiang, 2016)
LSTMN (Cheng et al., 2016)
Decomposable Att. (Parikh et al., 2016)
NTI (Yu and Munkhdalai, 2017b)
Re-read LSTM (Sha et al., 2016)
BiMPM (Wang et al., 2017)
DIIN (Gong et al., 2017)
BCN + CoVe (McCann et al., 2017)
CAFE (Tay et al., 2018)

ESIM (Chen et al., 2017a)
KIM (This paper)

Test

83.5
84.6
85.1
86.1
86.3
86.8
87.3
87.5
87.5
88.0
88.1
88.5

88.0
88.6

Table 2: Accuracies of models on SNLI.

Model

In Cross

CBOW (Williams et al., 2017)
BiLSTM (Williams et al., 2017)
DiSAN (Shen et al., 2017)
Gated BiLSTM (Chen et al., 2017b)
SS BiLSTM (Nie and Bansal, 2017)
DIIN * (Gong et al., 2017)
CAFE (Tay et al., 2018)

ESIM (Chen et al., 2017a)
KIM (This paper)

64.8
66.9
71.0
73.5
74.6
77.8
78.7

76.8
77.2

64.5
66.9
71.4
73.6
73.6
78.8
77.9

75.8
76.4

Table 3: Accuracies of models on MultiNLI. * in-
dicates models using extra SNLI training set.

domly sample different ratios of the entire training
set, i.e., 0.8%, 4%, 20% and 100%. “A” indicates
adding external knowledge in calculating the co-
attention matrix as in Equation (3), “I” indicates
adding external knowledge in collecting local in-
ference information as in Equation (7)(8), and “C”
indicates adding external knowledge in compos-
ing inference as in Equation (11)(12). When we
only have restricted training data, i.e., 0.8% train-
ing set (about 4,000 samples), the baseline ESIM
has a poor accuracy of 62.4%. When we only
add external knowledge in calculating co-attention
(“A”), the accuracy increases to 66.6% (+ absolute
4.2%). When we only utilize external knowledge
in collecting local inference information (“I”), the
accuracy has a signiﬁcant gain, to 70.3% (+ ab-
solute 7.9%). When we only add external knowl-
edge in inference composition (“C”), the accuracy
gets a smaller gain to 63.4% (+ absolute 1.0%).
The comparison indicates that “I” plays the most
important role among the three components in us-
ing external knowledge. Moreover, when we com-

pose the three components (“A,I,C”), we obtain
the best result of 72.6% (+ absolute 10.2%). When
we use more training data, i.e., 4%, 20%, 100%
of the training set, only “I” achieves a signiﬁcant
gain, but “A” or “C” does not bring any signiﬁ-
cant improvement. The results indicate that ex-
ternal semantic knowledge only helps co-attention
and composition when limited training data is lim-
ited, but always helps in collecting local inference
information. Meanwhile, for less training data, λ
is usually set to a larger value. For example, the
optimal λ on the development set is 20 for 0.8%
training set, 2 for the 4% training set, 1 for the
20% training set and 0.2 for the 100% training set.
Figure 3 displays the results of using different
ratios of external knowledge (randomly keep dif-
ferent percentages of whole lexical semantic rela-
tions) under different sizes of training data. Note
that here we only use external knowledge in col-
lecting local inference information as it always
works well for different scale of the training set.
Better accuracies are achieved when using more
external knowledge. Especially under the condi-
tion of restricted training data (0.8%), the model
obtains a large gain when using more than half of
external knowledge.

Figure 2: Accuracies of models of incorporat-
ing external knowledge into different NLI compo-
nents, under different sizes of training data (0.8%,
4%, 20%, and the entire training data).

5.3 Analysis on the (Glockner et al., 2018)

Test Set

In addition, Table 4 shows the results on a newly
published test set (Glockner et al., 2018). Com-
pared with the performance on the SNLI test

Figure 3: Accuracies of models under differ-
ent sizes of external knowledge. More external
knowledge corresponds to higher accuracies.

Model

SNLI Glockner’s(∆)

(Parikh et al., 2016)*
(Nie and Bansal, 2017)*
ESIM *
KIM (This paper)

84.7
86.0
87.9
88.6

51.9 (-32.8)
62.2 (-23.8)
65.6 (-22.3)
83.5 ( -5.1)

Table 4: Accuracies of models on the SNLI and
(Glockner et al., 2018) test set. * indicates the re-
sults taken from (Glockner et al., 2018).

set, the performance of the three baseline mod-
els dropped substantially on the (Glockner et al.,
2018) test set, with the differences ranging from
22.3% to 32.8% in accuracy. Instead, the proposed
KIM achieves 83.5% on this test set (with only a
5.1% drop in performance), which demonstrates
its better ability of utilizing lexical level inference
and hence better generalizability.

Figure 5 displays the accuracy of ESIM
and KIM in each replacement-word category of
the (Glockner et al., 2018) test set. KIM outper-
forms ESIM in 13 out of 14 categories, and only
performs worse on synonyms.

5.4 Analysis by Inference Categories

We perform more analysis (Table 6) using the sup-
plementary annotations provided by the MultiNLI
dataset (Williams et al., 2017), which have 495
samples (about 1/20 of the entire development set)
for both in-domain and out-domain set. We com-
pare against the model outputs of the ESIM model
across 13 categories of inference. Table 6 reports
the results. We can see that KIM outperforms
ESIM on overall accuracies on both in-domain and

Category

Instance ESIM KIM

P/G Sentences

Antonyms
Cardinals
Nationalities
Drinks
Antonyms WordNet
Colors
Ordinals
Countries
Rooms
Materials
Vegetables
Instruments
Planets
Synonyms

Overall

1,147
759
755
731
706
699
663
613
595
397
109
65
60
894

8,193

70.4
75.5
35.9
63.7
74.6
96.1
21.0
25.4
69.4
89.7
31.2
90.8
3.3
99.7

65.6

86.5
93.4
73.5
96.6
78.8
98.3
56.6
70.8
77.6
98.7
79.8
96.9
5.0
92.1

83.5

Table 5: The number of instances and accu-
racy per category achieved by ESIM and KIM on
the (Glockner et al., 2018) test set.

Category

In-domain Cross-domain
ESIM KIM ESIM KIM

Active/Passive
Antonym
Belief
Conditional
Coreference
Long sentence
Modal
Negation
Paraphrase
Quantity/Time
Quantiﬁer
Tense
Word overlap

Overall

93.3
76.5
72.7
65.2
80.0
82.8
80.6
76.7
84.0
66.7
79.2
74.5
89.3

77.1

93.3
76.5
75.8
65.2
76.7
78.8
79.9
79.8
72.0
66.7
78.4
78.4
85.7

77.9

100.0
70.0
75.9
61.5
75.9
69.7
77.0
73.1
86.5
56.4
73.6
72.2
83.8

76.7

100.0
75.0
79.3
69.2
75.9
73.4
80.2
71.2
89.2
59.0
77.1
66.7
81.1

77.4

Table 6: Detailed Analysis on MultiNLI.

cross-domain subset of development set. KIM out-
performs or equals ESIM in 10 out of 13 cate-
gories on the cross-domain setting, while only 7
out of 13 categories on in-domain setting. It indi-
cates that external knowledge helps more in cross-
domain setting. Especially, for antonym category
in cross-domain set, KIM outperform ESIM sig-
niﬁcantly (+ absolute 5.0%) as expected, because
antonym feature captured by external knowledge
would help unseen cross-domain samples.

5.5 Case Study

Table 7 includes some examples from the SNLI
test set, where KIM successfully predicts the in-
ference relation and ESIM fails. In the ﬁrst exam-

e/c

e/c

c/e

c/e

p: An African person standing in a wheat
ﬁeld.
h: A person standing in a corn ﬁeld.

p: Little girl is ﬂipping an omelet in the
kitchen.
h: A young girl cooks pancakes.

p: A middle eastern marketplace.
h: A middle easten store.

p: Two boys are swimming with boogie
boards.
h: Two boys are swimming with their ﬂoats.

Table 7: Examples. Word in bold are key words
in making ﬁnal prediction. P indicates a predicted
label and G indicates gold-standard label. e and c
denote entailment and contradiction, respectively.

ple, the premise is “An African person standing in
a wheat ﬁeld” and the hypothesis “A person stand-
ing in a corn ﬁeld”. As the KIM model knows that
“wheat” and “corn” are both a kind of cereal, i.e,
the co-hyponyms relationship in our relation fea-
tures, KIM therefore predicts the premise contra-
dicts the hypothesis. However, the baseline ESIM
cannot learn the relationship between “wheat” and
“corn” effectively due to lack of enough samples
in the training sets. With the help of external
knowledge, i.e., “wheat” and “corn” having the
same hypernym “cereal”, KIM predicts contradic-
tion correctly.

6 Conclusions

Our neural-network-based model for natural lan-
guage inference with external knowledge, namely
KIM, achieves the state-of-the-art accuracies. The
model is equipped with external knowledge in its
main components, speciﬁcally, in calculating co-
attention, collecting local inference, and compos-
ing inference. We provide detailed analyses on our
model and results. The proposed model of infus-
ing neural networks with external knowledge may
also help shed some light on tasks other than NLI.

Acknowledgments

We thank Yibo Sun and Bing Qin for early helpful
discussion.

References

Sungjin Ahn, Heeyoul Choi, Tanel P¨arnamaa, and
Yoshua Bengio. 2016. A neural knowledge lan-
guage model. CoRR, abs/1608.00318.

Kurt D. Bollacker, Robert P. Cook, and Patrick Tufts.
2007. Freebase: A shared database of structured
In Proceedings of the
general human knowledge.
Twenty-Second AAAI Conference on Artiﬁcial In-
telligence, July 22-26, 2007, Vancouver, British
Columbia, Canada, pages 1962–1963.

Antoine Bordes, Nicolas Usunier, Alberto Garc´ıa-
Dur´an, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States., pages 2787–
2795.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
In Proceedings of the 2015 Conference on
ence.
Empirical Methods in Natural Language Process-
ing, EMNLP 2015, Lisbon, Portugal, September 17-
21, 2015, pages 632–642.

Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. 2016. A fast uniﬁed model for
parsing and sentence understanding. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics, ACL 2016, August 7-12,
2016, Berlin, Germany, Volume 1: Long Papers.

Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard
S¨ackinger, and Roopak Shah. 1993. Signature veri-
ﬁcation using a siamese time delay neural network.
In Advances in Neural Information Processing Sys-
tems 6, [7th NIPS Conference, Denver, Colorado,
USA, 1993], pages 737–744.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
and Hui Jiang. 2016a. Distraction-based neural net-
In Proceedings of
works for modeling document.
the Twenty-Fifth International Joint Conference on
Artiﬁcial Intelligence, IJCAI 2016, New York, NY,
USA, 9-15 July 2016, pages 2754–2760.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
Hui Jiang, and Diana Inkpen. 2017a. Enhanced
LSTM for natural language inference. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics, ACL 2017, Vancou-
ver, Canada, July 30 - August 4, Volume 1: Long
Papers, pages 1657–1668.

Representations for NLP, RepEval@EMNLP 2017,
Copenhagen, Denmark, September 8, 2017, pages
36–40.

Yun-Nung Chen, Dilek Z. Hakkani-T¨ur, G¨okhan T¨ur,
Asli C¸ elikyilmaz, Jianfeng Gao, and Li Deng.
2016b. Knowledge as a teacher: Knowledge-
CoRR,
guided structural attention networks.
abs/1609.03286.

Zhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen,
Si Wei, Hui Jiang, and Xiaodan Zhu. 2015. Re-
visiting word embedding for contrasting meaning.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, ACL 2015, July 26-31, 2015,
Beijing, China, Volume 1: Long Papers, pages 106–
115.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2016, Austin, Texas, USA, November
1-4, 2016, pages 551–561.

Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2017.
Unsupervised learning of task-speciﬁc tree struc-
tures with tree-lstms. CoRR, abs/1707.02786.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2017, Copen-
hagen, Denmark, September 9-11, 2017, pages 670–
680.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, Eval-
uating Predictive Uncertainty, Visual Object Clas-
siﬁcation and Recognizing Textual Entailment, First
PASCAL Machine Learning Challenges Workshop,
MLCW 2005, Southampton, UK, April 11-13, 2005,
Revised Selected Papers, pages 177–190.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard H. Hovy, and Noah A. Smith.
2015. Retroﬁtting word vectors to semantic lexi-
cons. In NAACL HLT 2015, The 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Denver, Colorado, USA, May 31 - June 5,
2015, pages 1606–1615.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017b. Recurrent neural
network-based sentence encoder with gated atten-
tion for natural language inference. In Proceedings
of the 2nd Workshop on Evaluating Vector Space

Max Glockner, Vered Shwartz, and Yoav Goldberg.
2018. Breaking nli systems with sentences that re-
quire simple lexical inferences. In The 56th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Melbourne, Australia.

Yichen Gong, Heng Luo, and Jian Zhang. 2017.
Natural language inference over interaction space.
CoRR, abs/1709.04348.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Adrian Iftene and Alexandra Balahur-Dobrescu. 2007.
Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, chapter Hypoth-
esis Transformation and Semantic Variability Rules
Used in Recognizing Textual Entailment. Associa-
tion for Computational Linguistics.

Jay J. Jiang and David W. Conrath. 1997. Seman-
tic similarity based on corpus statistics and lexical
In Proceedings of the 10th Research
taxonomy.
on Computational Linguistics International Confer-
ence, ROCLING 1997, Taipei, Taiwan, August 1997,
pages 19–33.

Valentin Jijkoun and Maarten de Rijke. 2005. Recog-
nizing textual entailment using lexical similarity. In
Proceedings of the PASCAL Challenges Workshop
on Recognising Textual Entailment.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Zhouhan Lin, Minwei Feng, C´ıcero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. CoRR, abs/1703.03130.

Pengfei Liu, Xipeng Qiu, Jifan Chen, and Xuanjing
Huang. 2016a. Deep fusion lstms for text seman-
In Proceedings of the 54th Annual
tic matching.
Meeting of the Association for Computational Lin-
guistics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers.

Pengfei Liu, Xipeng Qiu, Yaqian Zhou, Jifan Chen, and
Xuanjing Huang. 2016b. Modelling interaction of
sentence pair with coupled-lstms. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2016, Austin,
Texas, USA, November 1-4, 2016, pages 1703–1712.

Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling, and
Yu Hu. 2015. Learning semantic word embeddings
In Pro-
based on ordinal knowledge constraints.
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, ACL 2015, July 26-31, 2015, Bei-
jing, China, Volume 1: Long Papers, pages 1501–
1511.

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016c. Learning natural language inference us-
ing bidirectional LSTM model and inner-attention.
CoRR, abs/1605.09090.

Bill MacCartney. 2009. Natural Language Inference.

Ph.D. thesis, Stanford University.

Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2008, Proceedings of the Confer-
ence, 25-27 October 2008, Honolulu, Hawaii, USA,
A meeting of SIGDAT, a Special Interest Group of
the ACL, pages 802–811.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural language
processing toolkit. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, ACL 2014, June 22-27, 2014, Baltimore,
MD, USA, System Demonstrations, pages 55–60.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
In Advances in Neural
textualized word vectors.
Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems
2017, 4-9 December 2017, Long Beach, CA, USA,
pages 6297–6308.

George A. Miller. 1995. Wordnet: A lexical database

for english. Commun. ACM, 38(11):39–41.

Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui
Yan, and Zhi Jin. 2016. Natural language infer-
ence by tree-based convolution and heuristic match-
ing. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics, ACL
2016, August 7-12, 2016, Berlin, Germany, Volume
2: Short Papers.

Nikola Mrksic, Ivan Vulic, Diarmuid ´O S´eaghdha, Ira
Leviant, Roi Reichart, Milica Gasic, Anna Korho-
nen, and Steve J. Young. 2017. Semantic special-
isation of distributional word vector spaces using
monolingual and cross-lingual constraints. CoRR,
abs/1706.00374.

Yixin Nie and Mohit Bansal. 2017.

Shortcut-
stacked sentence encoders for multi-domain infer-
In Proceedings of the 2nd Workshop on
ence.
Evaluating Vector Space Representations for NLP,
RepEval@EMNLP 2017, Copenhagen, Denmark,
September 8, 2017, pages 41–45.

Ankur P. Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pages
2249–2255.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
In Proceedings of the 2014
word representation.
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,

Hong Yu and Tsendsuren Munkhdalai. 2017a. Neural
semantic encoders. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL 2017, Valen-
cia, Spain, April 3-7, 2017, Volume 1: Long Papers,
pages 397–407.

Hong Yu and Tsendsuren Munkhdalai. 2017b. Neu-
ral tree indexers for text understanding. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL 2017, Valencia, Spain, April 3-7, 2017, Vol-
ume 1: Long Papers, pages 11–21.

Junbei Zhang, Xiaodan Zhu, Qian Chen, Lirong
Ex-
Dai, Si Wei,
ploring question understanding and adaptation in
neural-network-based question answering. CoRR,
abs/arXiv:1703.04617v2.

Jiang. 2017a.

and Hui

Shiyue Zhang, Gulnigar Mahmut, Dong Wang, and
Askar Hamdulla. 2017b.
Memory-augmented
chinese-uyghur neural machine translation. In 2017
Asia-Paciﬁc Signal and Information Processing As-
sociation Annual Summit and Conference, APSIPA
ASC 2017, Kuala Lumpur, Malaysia, December 12-
15, 2017, pages 1092–1096.

2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1532–1543.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz
Hermann, Tom´as Kocisk´y, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
CoRR, abs/1509.06664.

Lei Sha, Baobao Chang, Zhifang Sui, and Sujian Li.
2016. Reading and thinking: Re-read LSTM unit
In COLING
for textual entailment recognition.
2016, 26th International Conference on Computa-
tional Linguistics, Proceedings of the Conference:
Technical Papers, December 11-16, 2016, Osaka,
Japan, pages 2870–2879.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2017. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding. CoRR, abs/1709.04696.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen
Wang, and Chengqi Zhang. 2018. Reinforced self-
attention network: a hybrid of hard and soft attention
for sequence modeling. CoRR, abs/1801.10296.

Chen Shi, Shujie Liu, Shuo Ren, Shi Feng, Mu Li,
Ming Zhou, Xu Sun, and Houfeng Wang. 2016.
Knowledge-based semantic embedding for machine
translation. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. A
compare-propagate architecture with alignment fac-
torization for natural language inference. CoRR,
abs/1801.00102.

Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel
Urtasun. 2015. Order-embeddings of images and
language. CoRR, abs/1511.06361.

Shuohang Wang and Jing Jiang. 2016. Learning natu-
ral language inference with LSTM. In NAACL HLT
2016, The 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, San Diego
California, USA, June 12-17, 2016, pages 1442–
1451.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences. In Proceedings of the Twenty-Sixth
International Joint Conference on Artiﬁcial Intelli-
gence, IJCAI 2017, Melbourne, Australia, August
19-25, 2017, pages 4144–4150.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2015. From paraphrase database to compo-
sitional paraphrase model and back. TACL, 3:345–
358.

Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. CoRR,
abs/1704.05426.

Neural Natural Language Inference Models Enhanced with
External Knowledge

Qian Chen
University of Science and
Technology of China
cq1231@mail.ustc.edu.cn

Zhen-Hua Ling
University of Science and
Technology of China
zhling@ustc.edu.cn

Xiaodan Zhu
ECE, Queen’s University
xiaodan.zhu@queensu.ca

Diana Inkpen
University of Ottawa
diana@site.uottawa.ca

Si Wei
iFLYTEK Research
siwei@iflytek.com

Abstract

Modeling natural language inference is a
very challenging task. With the avail-
ability of large annotated data, it has re-
cently become feasible to train complex
models such as neural-network-based in-
ference models, which have shown to
achieve the state-of-the-art performance.
Although there exist relatively large anno-
tated data, can machines learn all knowl-
edge needed to perform natural language
inference (NLI) from these data? If not,
how can neural-network-based NLI mod-
els beneﬁt from external knowledge and
how to build NLI models to leverage it?
In this paper, we enrich the state-of-the-art
neural natural language inference models
with external knowledge. We demonstrate
that the proposed models improve neural
NLI models to achieve the state-of-the-art
performance on the SNLI and MultiNLI
datasets.

last

larger

In the

several years,

anno-
the
tated datasets were made available, e.g.,
(Bowman et al., 2015) and MultiNLI
SNLI
datasets (Williams et al., 2017), which made
it feasible to train rather complicated neural-
network-based models that ﬁt a large set of
parameters to better model NLI. Such models
have shown to achieve the state-of-the-art per-
formance (Bowman et al., 2015, 2016; Yu and
Munkhdalai, 2017b; Parikh et al., 2016; Sha et al.,
2016; Chen et al., 2017a,b; Tay et al., 2018).

While neural networks have been shown to be
very effective in modeling NLI with large train-
ing data, they have often focused on end-to-end
training by assuming that all inference knowledge
is learnable from the provided training data.
In
this paper, we relax this assumption and explore
whether external knowledge can further help NLI.
Consider an example:

• p: A lady standing in a wheat ﬁeld.

• h: A person standing in a corn ﬁeld.

1

Introduction

Reasoning and inference are central to both hu-
man and artiﬁcial intelligence. Natural language
inference (NLI), also known as recognizing tex-
tual entailment (RTE), is an important NLP prob-
lem concerned with determining inferential rela-
tionship (e.g., entailment, contradiction, or neu-
tral) between a premise p and a hypothesis h. In
general, modeling informal inference in language
is a very challenging and basic problem towards
achieving true natural language understanding.

In this simpliﬁed example, when computers are
asked to predict the relation between these two
sentences and if training data do not provide the
knowledge of relationship between “wheat” and
“corn” (e.g., if one of the two words does not ap-
pear in the training data or they are not paired in
any premise-hypothesis pairs), it will be hard for
computers to correctly recognize that the premise
contradicts the hypothesis.

In general, although in many tasks learning tab-
ula rasa achieved state-of-the-art performance, we
believe complicated NLP problems such as NLI

8
1
0
2
 
n
u
J
 
3
2
 
 
]
L
C
.
s
c
[
 
 
3
v
9
8
2
4
0
.
1
1
7
1
:
v
i
X
r
a

could beneﬁt from leveraging knowledge accumu-
lated by humans, particularly in a foreseeable fu-
ture when machines are unable to learn it by them-
selves.

In this paper we enrich neural-network-based
NLI models with external knowledge in co-
attention, local inference collection, and inference
composition components. We show the proposed
model improves the state-of-the-art NLI models
to achieve better performances on the SNLI and
MultiNLI datasets. The advantage of using exter-
nal knowledge is more signiﬁcant when the size of
training data is restricted, suggesting that if more
knowledge can be obtained, it may bring more
beneﬁt.
In addition to attaining the state-of-the-
art performance, we are also interested in under-
standing how external knowledge contributes to
the major components of typical neural-network-
based NLI models.

2 Related Work

Early research on natural language inference and
recognizing textual entailment has been performed
on relatively small datasets (refer to MacCartney
(2009) for a good literature survey), which in-
cludes a large bulk of contributions made under
the name of RTE, such as (Dagan et al., 2005;
Iftene and Balahur-Dobrescu, 2007), among many
others.

More recently the availability of much larger
annotated data, e.g., SNLI
(Bowman et al.,
2015) and MultiNLI (Williams et al., 2017), has
made it possible to train more complex mod-
els. These models mainly fall into two types
of approaches: sentence-encoding-based models
and models using also inter-sentence attention.
Sentence-encoding-based models use Siamese ar-
chitecture (Bromley et al., 1993). The parameter-
tied neural networks are applied to encode both
the premise and the hypothesis. Then a neural
network classiﬁer is applied to decide relationship
between the two sentences. Different neural net-
works have been utilized for sentence encoding,
such as LSTM (Bowman et al., 2015), GRU (Ven-
drov et al., 2015), CNN (Mou et al., 2016), BiL-
STM and its variants (Liu et al., 2016c; Lin et al.,
2017; Chen et al., 2017b; Nie and Bansal, 2017),
self-attention network (Shen et al., 2017, 2018),
and more complicated neural networks (Bowman
et al., 2016; Yu and Munkhdalai, 2017a,b; Choi
et al., 2017). Sentence-encoding-based models

transform sentences into ﬁxed-length vector rep-
resentations, which may help a wide range of
tasks (Conneau et al., 2017).

The second set of models use inter-sentence at-
tention (Rockt¨aschel et al., 2015; Wang and Jiang,
2016; Cheng et al., 2016; Parikh et al., 2016;
Chen et al., 2017a). Among them, Rockt¨aschel
et al. (2015) were among the ﬁrst to propose neu-
ral attention-based models for NLI. Chen et al.
(2017a) proposed an enhanced sequential infer-
ence model (ESIM), which is one of the best mod-
els so far and is used as one of our baselines in this
paper.

In this paper we enrich neural-network-based
NLI models with external knowledge. Unlike
early work on NLI (Jijkoun and de Rijke, 2005;
MacCartney et al., 2008; MacCartney, 2009) that
explores external knowledge in conventional NLI
models on relatively small NLI datasets, we aim to
merge the advantage of powerful modeling ability
of neural networks with extra external inference
knowledge. We show that the proposed model
improves the state-of-the-art neural NLI models
to achieve better performances on the SNLI and
MultiNLI datasets. The advantage of using exter-
nal knowledge is more signiﬁcant when the size of
training data is restricted, suggesting that if more
knowledge can be obtained, it may have more ben-
eﬁt.
In addition to attaining the state-of-the-art
performance, we are also interested in understand-
ing how external knowledge affect major compo-
nents of neural-network-based NLI models.

In general, external knowledge has shown to be
effective in neural networks for other NLP tasks,
including word embedding (Chen et al., 2015;
Faruqui et al., 2015; Liu et al., 2015; Wieting
et al., 2015; Mrksic et al., 2017), machine trans-
lation (Shi et al., 2016; Zhang et al., 2017b), lan-
guage modeling (Ahn et al., 2016), and dialogue
systems (Chen et al., 2016b).

3 Neural-Network-Based NLI Models

with External Knowledge

In this section we propose neural-network-based
inference
NLI models to incorporate external
knowledge, which, as we will show later in Sec-
tion 5, achieve the state-of-the-art performance.
In addition to attaining the leading performance
we are also interested in investigating the effects
of external knowledge on major components of
neural-network-based NLI modeling.

Figure 1 shows a high-level general view of the
proposed framework. While speciﬁc NLI systems
vary in their implementation, typical state-of-the-
art NLI models contain the main components (or
equivalents) of representing premise and hypoth-
esis sentences, collecting local (e.g., lexical) in-
ference information, and aggregating and compos-
ing local information to make the global decision
at the sentence level. We incorporate and investi-
gate external knowledge accordingly in these ma-
jor NLI components: computing co-attention, col-
lecting local inference information, and compos-
ing inference to make ﬁnal decision.

3.1 External Knowledge

We

study the

incorporation of

As discussed above, although there exist relatively
large annotated data for NLI, can machines learn
all inference knowledge needed to perform NLI
from the data? If not, how can neural network-
based NLI models beneﬁt from external knowl-
edge and how to build NLI models to leverage it?
external,
inference-related knowledge in major compo-
nents of neural networks for natural
language
inference. For example,
intuitively knowledge
synonymy, antonymy, hypernymy and
about
hyponymy between given words may help model
soft-alignment between premises and hypotheses;
knowledge about hypernymy and hyponymy
may help capture entailment; knowledge about
antonymy and co-hyponyms (words sharing the
same hypernym) may beneﬁt the modeling of
contradiction.

In this section, we discuss the incorporation of
basic, lexical-level semantic knowledge into neu-
ral NLI components. Speciﬁcally, we consider ex-
ternal lexical-level inference knowledge between
word wi and wj, which is represented as a vec-
tor rij and is incorporated into three speciﬁc com-
ponents shown in Figure 1. We will discuss the
details of how rij is constructed later in the exper-
iment setup section (Section 4) but instead focus
on the proposed model in this section. Note that
while we study lexical-level inference knowledge
in the paper, if inference knowledge about larger
pieces of text pairs (e.g., inference relations be-
tween phrases) are available, the proposed model
can be easily extended to handle that. In this paper,
we instead let the NLI models to compose lexical-
level knowledge to obtain inference relations be-
tween larger pieces of texts.

3.2 Encoding Premise and Hypothesis

Same as much previous work (Chen et al.,
2017a,b), we encode the premise and the hypoth-
esis with bidirectional LSTMs (BiLSTMs). The
premise is represented as a = (a1, . . . , am) and
the hypothesis is b = (b1, . . . , bn), where m
and n are the lengths of the sentences. Then a
and b are embedded into de-dimensional vectors
[E(a1), . . . , E(am)] and [E(b1), . . . , E(bn)] using
the embedding matrix E ∈ Rde×|V |, where |V | is
the vocabulary size and E can be initialized with
the pre-trained word embedding. To represent
words in its context, the premise and the hypothe-
sis are fed into BiLSTM encoders (Hochreiter and
Schmidhuber, 1997) to obtain context-dependent
hidden states as and bs:

as
i = Encoder(E(a), i) ,
bs
j = Encoder(E(b), j) .

(1)

(2)

where i and j indicate the i-th word in the premise
and the j-th word in the hypothesis, respectively.

3.3 Knowledge-Enriched Co-Attention

As discussed above, soft-alignment of word pairs
between the premise and the hypothesis may ben-
eﬁt from knowledge-enriched co-attention mech-
anism. Given the relation features rij ∈ Rdr be-
tween the premise’s i-th word and the hypothesis’s
j-th word derived from the external knowledge,
the co-attention is calculated as:

eij = (as

i )Tbs

j + F (rij) .

(3)

The function F can be any non-linear or linear
functions. In this paper, we use F (rij) = λ1(rij),
where λ is a hyper-parameter tuned on the devel-
opment set and 1 is the indication function as fol-
lows:

1(rij) =

(cid:40)
1
0

if rij is not a zero vector ;
if rij is a zero vector .

(4)

Intuitively, word pairs with semantic relationship,
e.g., synonymy, antonymy, hypernymy, hyponymy
and co-hyponyms, are probably aligned together.
We will discuss how we construct external knowl-
edge later in Section 4. We have also tried a two-
layer MLP as a universal function approximator
in function F to learn the underlying combination
function but did not observe further improvement
over the best performance we obtained on the de-
velopment datasets.

Figure 1: A high-level view of neural-network-based NLI models enriched with external knowledge in
co-attention, local inference collection, and inference composition.

Soft-alignment

is determined by the co-
attention matrix e ∈ Rm×n computed in Equa-
tion (3), which is used to obtain the local relevance
between the premise and the hypothesis. For the
hidden state of the i-th word in the premise, i.e.,
as
i (already encoding the word itself and its con-
text), the relevant semantics in the hypothesis is
identiﬁed into a context vector ac
i using eij, more
speciﬁcally with Equation (5).

αij =

exp(eij)
k=1 exp(eik)

(cid:80)n

βij =

exp(eij)
k=1 exp(ekj)

(cid:80)m

, ac

i =

αijbs
j ,

(5)

, bc

j =

βijas
i ,

(6)

n
(cid:88)

j=1
m
(cid:88)

i=1

where α ∈ Rm×n and β ∈ Rm×n are the nor-
malized attention weight matrices with respect to
the 2-axis and 1-axis. The same calculation is per-
formed for each word in the hypothesis, i.e., bs
j,
with Equation (6) to obtain the context vector bc
j.

3.4 Local Inference Collection with External

Knowledge

By way of comparing the inference-related seman-
tic relation between as
i (individual word repre-
sentation in premise) and ac
i (context representa-
tion from hypothesis which is align to word as
i ),
we can model local inference (i.e., word-level in-
ference) between aligned word pairs. Intuitively,
for example, knowledge about hypernymy or hy-
ponymy may help model entailment and knowl-
edge about antonymy and co-hyponyms may help
model contradiction. Through comparing as
i and

n
(cid:88)

j=1
m
(cid:88)

i=1

ac
in addition to their relation from external
i ,
knowledge, we can obtain word-level inference
information for each word. The same calcula-
tion is performed for bs
j. Thus, we collect
knowledge-enriched local inference information:

j and bc

i = G([as
am

i ; ac

i ; as

i − ac

i ; as

i ◦ ac
i ;

αijrij]) , (7)

j = G([bs
bm

j, bc

j; bs

j − bc

j; bs

j ◦ bc
j;

βijrji]) ,

(8)

where a heuristic matching trick with difference
and element-wise product is used (Mou et al.,
2016; Chen et al., 2017a). The last terms in Equa-
tion (7)(8) are used to obtain word-level infer-
ence information from external knowledge. Take
Equation (7) as example, rij is the relation fea-
ture between the i-th word in the premise and
the j-th word in the hypothesis, but we care
more about semantic relation between aligned
word pairs between the premise and the hypoth-
esis. Thus, we use a soft-aligned version through
the soft-alignment weight αij. For the i-th word
in the premise, the last term in Equation (7) is
a word-level inference information based on ex-
ternal knowledge between the i-th word and the
aligned word. The same calculation for hypoth-
esis is performed in Equation (8). G is a non-
linear mapping function to reduce dimensionality.
Speciﬁcally, we use a 1-layer feed-forward neural
network with the ReLU activation function with
a shortcut connection, i.e., concatenate the hidden
states after ReLU with the input (cid:80)n
j=1 αijrij (or
(cid:80)m
(or bm
j ).

i=1 βijrji) as the output am
i

3.5 Knowledge-Enhanced Inference

4 Experiment Set-Up

Composition

In this component, we introduce knowledge-
enriched inference composition. To determine the
overall inference relationship between the premise
and the hypothesis, we need to explore a compo-
sition layer to compose the local inference vectors
(am and bm) collected above:

av
i = Composition(am, i) ,
j = Composition(bm, j) .
bv

(9)

(10)

Here, we also use BiLSTMs as building blocks
for the composition layer, but the responsibility
of BiLSTMs in the inference composition layer
is completely different from that in the input en-
coding layer. The BiLSTMs here read local in-
ference vectors (am and bm) and learn to judge
the types of local inference relationship and dis-
tinguish crucial local inference vectors for overall
sentence-level inference relationship. Intuitively,
the ﬁnal prediction is likely to depend on word
pairs appearing in external knowledge that have
some semantic relation. Our inference model con-
verts the output hidden vectors of BiLSTMs to
the ﬁxed-length vector with pooling operations
and puts it into the ﬁnal classiﬁer to determine
the overall inference class. Particularly, in addi-
tion to using mean pooling and max pooling sim-
ilarly to ESIM (Chen et al., 2017a), we propose
to use weighted pooling based on external knowl-
edge to obtain a ﬁxed-length vector as in Equation
(11)(12).

aw =

bw =

m
(cid:88)

i=1
n
(cid:88)

j=1

exp(H((cid:80)n
i=1 exp(H((cid:80)n
exp(H((cid:80)m
j=1 exp(H((cid:80)m

(cid:80)m

(cid:80)n

j=1 αijrij))

j=1 αijrij))

i=1 βijrji))

i=1 βijrji))

av
i ,

(11)

bv
j .

(12)

In our experiments, we regard the function H as
a 1-layer feed-forward neural network with ReLU
activation function. We concatenate all pooling
vectors, i.e., mean, max, and weighted pooling,
into the ﬁxed-length vector and then put the vector
into the ﬁnal multilayer perceptron (MLP) clas-
siﬁer. The MLP has one hidden layer with tanh
activation and softmax output layer in our exper-
iments. The entire model is trained end-to-end,
through minimizing the cross-entropy loss.

4.1 Representation of External Knowledge

Lexical Semantic Relations As described in
to incorporate external knowledge
Section 3.1,
(as a knowledge vector rij) to the state-of-the-
art neural network-based NLI models, we ﬁrst
explore semantic relations in WordNet (Miller,
1995), motivated by MacCartney (2009). Specif-
ically, the relations of lexical pairs are derived as
described in (1)-(4) below. Instead of using Jiang-
Conrath WordNet distance metric (Jiang and Con-
rath, 1997), which does not improve the perfor-
mance of our models on the development sets, we
add a new feature, i.e., co-hyponyms, which con-
sistently beneﬁt our models.

(1) Synonymy: It takes the value 1 if the words in
the pair are synonyms in WordNet (i.e., be-
long to the same synset), and 0 otherwise. For
example, [felicitous, good] = 1, [dog, wolf] =
0.

(2) Antonymy: It takes the value 1 if the words
in the pair are antonyms in WordNet, and 0
otherwise. For example, [wet, dry] = 1.

(3) Hypernymy: It takes the value 1 − n/8 if one
word is a (direct or indirect) hypernym of the
other word in WordNet, where n is the num-
ber of edges between the two words in hier-
archies, and 0 otherwise. Note that we ignore
pairs in the hierarchy which have more than 8
edges in between. For example, [dog, canid]
= 0.875, [wolf, canid] = 0.875, [dog, carni-
vore] = 0.75, [canid, dog] = 0

(4) Hyponymy: It is simply the inverse of the hy-
pernymy feature. For example, [canid, dog]
= 0.875, [dog, canid] = 0.

(5) Co-hyponyms: It takes the value 1 if the two
words have the same hypernym but they do
not belong to the same synset, and 0 other-
wise. For example, [dog, wolf] = 1.

As discussed above, we expect features like syn-
onymy, antonymy, hypernymy, hyponymy and co-
hyponyms would help model co-attention align-
ment between the premise and the hypothesis.
Knowledge of hypernymy and hyponymy may help
capture entailment; knowledge of antonymy and
co-hyponyms may help model contradiction. Their
ﬁnal contributions will be learned in end-to-end
model training. We regard the vector r ∈ Rdr as

the relation feature derived from external knowl-
edge, where dr is 5 here. In addition, Table 1 re-
ports some key statistics of these features.

Feature

#Words

#Pairs

Synonymy
Antonymy
Hypernymy
Hyponymy
Co-hyponyms

84,487
6,161
57,475
57,475
53,281

237,937
6,617
753,086
753,086
3,674,700

Table 1: Statistics of lexical relation features.

In addition to the above relations, we also use
more relation features in WordNet, including in-
stance,
instance of, same instance, entailment,
member meronym, member holonym, substance
meronym, substance holonym, part meronym, part
holonym, summing up to 15 features, but these ad-
ditional features do not bring further improvement
on the development dataset, as also discussed in
Section 5.

Relation Embeddings
In the most recent years
graph embedding has been widely employed to
learn representation for vertexes and their relations
in a graph.
In our work here, we also capture
the relation between any two words in WordNet
through relation embedding. Speciﬁcally, we em-
ployed TransE (Bordes et al., 2013), a widely used
graph embedding methods, to capture relation em-
bedding between any two words. We used two
typical approaches to obtaining the relation em-
bedding. The ﬁrst directly uses 18 relation em-
beddings pretrained on the WN18 dataset (Bordes
et al., 2013). Speciﬁcally, if a word pair has a cer-
tain type relation, we take the corresponding re-
lation embedding. Sometimes, if a word pair has
multiple relations among the 18 types; we take an
average of the relation embedding. The second ap-
proach uses TransE’s word embedding (trained on
WordNet) to obtain relation embedding, through
the objective function used in TransE, i.e., l ≈
t − h, where l indicates relation embedding, t in-
dicates tail entity embedding, and h indicates head
entity embedding.

Note that in addition to relation embedding
trained on WordNet, other relational embedding
resources exist; e.g.,
that trained on Freebase
(WikiData) (Bollacker et al., 2007), but such
knowledge resources are mainly about facts (e.g.,
relationship between Bill Gates and Microsoft)
and are less for commonsense knowledge used in

general natural language inference (e.g., the color
yellow potentially contradicts red).

4.2 NLI Datasets

In our experiments, we use Stanford Natural Lan-
guage Inference (SNLI) dataset (Bowman et al.,
2015) and Multi-Genre Natural Language Infer-
ence (MultiNLI) (Williams et al., 2017) dataset,
which focus on three basic relations between a
premise and a potential hypothesis:
the premise
entails the hypothesis (entailment), they contradict
each other (contradiction), or they are not related
(neutral). We use the same data split as in previ-
ous work (Bowman et al., 2015; Williams et al.,
2017) and classiﬁcation accuracy as the evaluation
metric. In addition, we test our models (trained on
the SNLI training set) on a new test set (Glockner
et al., 2018), which assesses the lexical inference
abilities of NLI systems and consists of 8,193 sam-
ples. WordNet 3.0 (Miller, 1995) is used to extract
semantic relation features between words. The
words are lemmatized using Stanford CoreNLP
3.7.0 (Manning et al., 2014). The premise and the
hypothesis sentences fed into the input encoding
layer are tokenized.

4.3 Training Details
For duplicability, we release our code1. All our
models were strictly selected on the development
set of the SNLI data and the in-domain devel-
opment set of MultiNLI and were then tested on
the corresponding test set. The main training de-
tails are as follows:
the dimension of the hid-
den states of LSTMs and word embeddings are
300. The word embeddings are initialized by
300D GloVe 840B (Pennington et al., 2014), and
out-of-vocabulary words among them are initial-
ized randomly. All word embeddings are updated
during training. Adam (Kingma and Ba, 2014)
is used for optimization with an initial learning
rate of 0.0004. The mini-batch size is set to 32.
Note that the above hyperparameter settings are
same as those used in the baseline ESIM (Chen
et al., 2017a) model. ESIM is a strong NLI
baseline framework with the source code made
available at https://github.com/lukecq1231/nli (the
ESIM core code has also been adapted to sum-
marization (Chen et al., 2016a) and question-
answering tasks (Zhang et al., 2017a)).

The

trade-off

λ

for

calculating

co-

1https://github.com/lukecq1231/kim

in Equation

attention
(3)
in
[0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50]
the
development set. When training TransE for
WordNet, relations are represented with vectors
of 20 dimension.

selected
on

is
based

5 Experimental Results

5.1 Overall Performance

Table 2 shows the results of state-of-the-art models
on the SNLI dataset. Among them, ESIM (Chen
et al., 2017a) is one of the previous state-of-the-art
systems with an 88.0% test-set accuracy. The pro-
posed model, namely Knowledge-based Inference
Model (KIM), which enriches ESIM with external
knowledge, obtains an accuracy of 88.6%, the best
single-model performance reported on the SNLI
dataset. The difference between ESIM and KIM is
statistically signiﬁcant under the one-tailed paired
t-test at the 99% signiﬁcance level. Note that the
KIM model reported here uses ﬁve semantic rela-
tions described in Section 4. In addition to that, we
also use 15 semantic relation features, which does
not bring additional gains in performance. These
results highlight the effectiveness of the ﬁve se-
mantic relations described in Section 4. To further
investigate external knowledge, we add TransE re-
lation embedding, and again no further improve-
ment is observed on both the development and test
sets when TransE relation embedding is used (con-
catenated) with the semantic relation vectors. We
consider this is due to the fact that TransE embed-
ding is not speciﬁcally sensitive to inference in-
formation; e.g., it does not model co-hyponyms
features, and its potential beneﬁt has already been
covered by the semantic relation features used.

Table 3 shows the performance of models on the
MultiNLI dataset. The baseline ESIM achieves
76.8% and 75.8% on in-domain and cross-domain
test set, respectively. If we extend the ESIM with
external knowledge, we achieve signiﬁcant gains
to 77.2% and 76.4% respectively. Again, the gains
are consistent on SNLI and MultiNLI, and we ex-
pect they would be orthogonal to other factors
when external knowledge is added into other state-
of-the-art models.

5.2 Ablation Results

Figure 2 displays the ablation analysis of differ-
ent components when using the external knowl-
edge. To compare the effects of external knowl-
edge under different training data scales, we ran-

Model

LSTM Att. (Rockt¨aschel et al., 2015)
DF-LSTMs (Liu et al., 2016a)
TC-LSTMs (Liu et al., 2016b)
Match-LSTM (Wang and Jiang, 2016)
LSTMN (Cheng et al., 2016)
Decomposable Att. (Parikh et al., 2016)
NTI (Yu and Munkhdalai, 2017b)
Re-read LSTM (Sha et al., 2016)
BiMPM (Wang et al., 2017)
DIIN (Gong et al., 2017)
BCN + CoVe (McCann et al., 2017)
CAFE (Tay et al., 2018)

ESIM (Chen et al., 2017a)
KIM (This paper)

Test

83.5
84.6
85.1
86.1
86.3
86.8
87.3
87.5
87.5
88.0
88.1
88.5

88.0
88.6

Table 2: Accuracies of models on SNLI.

Model

In Cross

CBOW (Williams et al., 2017)
BiLSTM (Williams et al., 2017)
DiSAN (Shen et al., 2017)
Gated BiLSTM (Chen et al., 2017b)
SS BiLSTM (Nie and Bansal, 2017)
DIIN * (Gong et al., 2017)
CAFE (Tay et al., 2018)

ESIM (Chen et al., 2017a)
KIM (This paper)

64.8
66.9
71.0
73.5
74.6
77.8
78.7

76.8
77.2

64.5
66.9
71.4
73.6
73.6
78.8
77.9

75.8
76.4

Table 3: Accuracies of models on MultiNLI. * in-
dicates models using extra SNLI training set.

domly sample different ratios of the entire training
set, i.e., 0.8%, 4%, 20% and 100%. “A” indicates
adding external knowledge in calculating the co-
attention matrix as in Equation (3), “I” indicates
adding external knowledge in collecting local in-
ference information as in Equation (7)(8), and “C”
indicates adding external knowledge in compos-
ing inference as in Equation (11)(12). When we
only have restricted training data, i.e., 0.8% train-
ing set (about 4,000 samples), the baseline ESIM
has a poor accuracy of 62.4%. When we only
add external knowledge in calculating co-attention
(“A”), the accuracy increases to 66.6% (+ absolute
4.2%). When we only utilize external knowledge
in collecting local inference information (“I”), the
accuracy has a signiﬁcant gain, to 70.3% (+ ab-
solute 7.9%). When we only add external knowl-
edge in inference composition (“C”), the accuracy
gets a smaller gain to 63.4% (+ absolute 1.0%).
The comparison indicates that “I” plays the most
important role among the three components in us-
ing external knowledge. Moreover, when we com-

pose the three components (“A,I,C”), we obtain
the best result of 72.6% (+ absolute 10.2%). When
we use more training data, i.e., 4%, 20%, 100%
of the training set, only “I” achieves a signiﬁcant
gain, but “A” or “C” does not bring any signiﬁ-
cant improvement. The results indicate that ex-
ternal semantic knowledge only helps co-attention
and composition when limited training data is lim-
ited, but always helps in collecting local inference
information. Meanwhile, for less training data, λ
is usually set to a larger value. For example, the
optimal λ on the development set is 20 for 0.8%
training set, 2 for the 4% training set, 1 for the
20% training set and 0.2 for the 100% training set.
Figure 3 displays the results of using different
ratios of external knowledge (randomly keep dif-
ferent percentages of whole lexical semantic rela-
tions) under different sizes of training data. Note
that here we only use external knowledge in col-
lecting local inference information as it always
works well for different scale of the training set.
Better accuracies are achieved when using more
external knowledge. Especially under the condi-
tion of restricted training data (0.8%), the model
obtains a large gain when using more than half of
external knowledge.

Figure 2: Accuracies of models of incorporat-
ing external knowledge into different NLI compo-
nents, under different sizes of training data (0.8%,
4%, 20%, and the entire training data).

5.3 Analysis on the (Glockner et al., 2018)

Test Set

In addition, Table 4 shows the results on a newly
published test set (Glockner et al., 2018). Com-
pared with the performance on the SNLI test

Figure 3: Accuracies of models under differ-
ent sizes of external knowledge. More external
knowledge corresponds to higher accuracies.

Model

SNLI Glockner’s(∆)

(Parikh et al., 2016)*
(Nie and Bansal, 2017)*
ESIM *
KIM (This paper)

84.7
86.0
87.9
88.6

51.9 (-32.8)
62.2 (-23.8)
65.6 (-22.3)
83.5 ( -5.1)

Table 4: Accuracies of models on the SNLI and
(Glockner et al., 2018) test set. * indicates the re-
sults taken from (Glockner et al., 2018).

set, the performance of the three baseline mod-
els dropped substantially on the (Glockner et al.,
2018) test set, with the differences ranging from
22.3% to 32.8% in accuracy. Instead, the proposed
KIM achieves 83.5% on this test set (with only a
5.1% drop in performance), which demonstrates
its better ability of utilizing lexical level inference
and hence better generalizability.

Figure 5 displays the accuracy of ESIM
and KIM in each replacement-word category of
the (Glockner et al., 2018) test set. KIM outper-
forms ESIM in 13 out of 14 categories, and only
performs worse on synonyms.

5.4 Analysis by Inference Categories

We perform more analysis (Table 6) using the sup-
plementary annotations provided by the MultiNLI
dataset (Williams et al., 2017), which have 495
samples (about 1/20 of the entire development set)
for both in-domain and out-domain set. We com-
pare against the model outputs of the ESIM model
across 13 categories of inference. Table 6 reports
the results. We can see that KIM outperforms
ESIM on overall accuracies on both in-domain and

Category

Instance ESIM KIM

P/G Sentences

Antonyms
Cardinals
Nationalities
Drinks
Antonyms WordNet
Colors
Ordinals
Countries
Rooms
Materials
Vegetables
Instruments
Planets
Synonyms

Overall

1,147
759
755
731
706
699
663
613
595
397
109
65
60
894

8,193

70.4
75.5
35.9
63.7
74.6
96.1
21.0
25.4
69.4
89.7
31.2
90.8
3.3
99.7

65.6

86.5
93.4
73.5
96.6
78.8
98.3
56.6
70.8
77.6
98.7
79.8
96.9
5.0
92.1

83.5

Table 5: The number of instances and accu-
racy per category achieved by ESIM and KIM on
the (Glockner et al., 2018) test set.

Category

In-domain Cross-domain
ESIM KIM ESIM KIM

Active/Passive
Antonym
Belief
Conditional
Coreference
Long sentence
Modal
Negation
Paraphrase
Quantity/Time
Quantiﬁer
Tense
Word overlap

Overall

93.3
76.5
72.7
65.2
80.0
82.8
80.6
76.7
84.0
66.7
79.2
74.5
89.3

77.1

93.3
76.5
75.8
65.2
76.7
78.8
79.9
79.8
72.0
66.7
78.4
78.4
85.7

77.9

100.0
70.0
75.9
61.5
75.9
69.7
77.0
73.1
86.5
56.4
73.6
72.2
83.8

76.7

100.0
75.0
79.3
69.2
75.9
73.4
80.2
71.2
89.2
59.0
77.1
66.7
81.1

77.4

Table 6: Detailed Analysis on MultiNLI.

cross-domain subset of development set. KIM out-
performs or equals ESIM in 10 out of 13 cate-
gories on the cross-domain setting, while only 7
out of 13 categories on in-domain setting. It indi-
cates that external knowledge helps more in cross-
domain setting. Especially, for antonym category
in cross-domain set, KIM outperform ESIM sig-
niﬁcantly (+ absolute 5.0%) as expected, because
antonym feature captured by external knowledge
would help unseen cross-domain samples.

5.5 Case Study

Table 7 includes some examples from the SNLI
test set, where KIM successfully predicts the in-
ference relation and ESIM fails. In the ﬁrst exam-

e/c

e/c

c/e

c/e

p: An African person standing in a wheat
ﬁeld.
h: A person standing in a corn ﬁeld.

p: Little girl is ﬂipping an omelet in the
kitchen.
h: A young girl cooks pancakes.

p: A middle eastern marketplace.
h: A middle easten store.

p: Two boys are swimming with boogie
boards.
h: Two boys are swimming with their ﬂoats.

Table 7: Examples. Word in bold are key words
in making ﬁnal prediction. P indicates a predicted
label and G indicates gold-standard label. e and c
denote entailment and contradiction, respectively.

ple, the premise is “An African person standing in
a wheat ﬁeld” and the hypothesis “A person stand-
ing in a corn ﬁeld”. As the KIM model knows that
“wheat” and “corn” are both a kind of cereal, i.e,
the co-hyponyms relationship in our relation fea-
tures, KIM therefore predicts the premise contra-
dicts the hypothesis. However, the baseline ESIM
cannot learn the relationship between “wheat” and
“corn” effectively due to lack of enough samples
in the training sets. With the help of external
knowledge, i.e., “wheat” and “corn” having the
same hypernym “cereal”, KIM predicts contradic-
tion correctly.

6 Conclusions

Our neural-network-based model for natural lan-
guage inference with external knowledge, namely
KIM, achieves the state-of-the-art accuracies. The
model is equipped with external knowledge in its
main components, speciﬁcally, in calculating co-
attention, collecting local inference, and compos-
ing inference. We provide detailed analyses on our
model and results. The proposed model of infus-
ing neural networks with external knowledge may
also help shed some light on tasks other than NLI.

Acknowledgments

We thank Yibo Sun and Bing Qin for early helpful
discussion.

References

Sungjin Ahn, Heeyoul Choi, Tanel P¨arnamaa, and
Yoshua Bengio. 2016. A neural knowledge lan-
guage model. CoRR, abs/1608.00318.

Kurt D. Bollacker, Robert P. Cook, and Patrick Tufts.
2007. Freebase: A shared database of structured
In Proceedings of the
general human knowledge.
Twenty-Second AAAI Conference on Artiﬁcial In-
telligence, July 22-26, 2007, Vancouver, British
Columbia, Canada, pages 1962–1963.

Antoine Bordes, Nicolas Usunier, Alberto Garc´ıa-
Dur´an, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States., pages 2787–
2795.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
In Proceedings of the 2015 Conference on
ence.
Empirical Methods in Natural Language Process-
ing, EMNLP 2015, Lisbon, Portugal, September 17-
21, 2015, pages 632–642.

Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. 2016. A fast uniﬁed model for
parsing and sentence understanding. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics, ACL 2016, August 7-12,
2016, Berlin, Germany, Volume 1: Long Papers.

Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard
S¨ackinger, and Roopak Shah. 1993. Signature veri-
ﬁcation using a siamese time delay neural network.
In Advances in Neural Information Processing Sys-
tems 6, [7th NIPS Conference, Denver, Colorado,
USA, 1993], pages 737–744.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
and Hui Jiang. 2016a. Distraction-based neural net-
In Proceedings of
works for modeling document.
the Twenty-Fifth International Joint Conference on
Artiﬁcial Intelligence, IJCAI 2016, New York, NY,
USA, 9-15 July 2016, pages 2754–2760.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
Hui Jiang, and Diana Inkpen. 2017a. Enhanced
LSTM for natural language inference. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics, ACL 2017, Vancou-
ver, Canada, July 30 - August 4, Volume 1: Long
Papers, pages 1657–1668.

Representations for NLP, RepEval@EMNLP 2017,
Copenhagen, Denmark, September 8, 2017, pages
36–40.

Yun-Nung Chen, Dilek Z. Hakkani-T¨ur, G¨okhan T¨ur,
Asli C¸ elikyilmaz, Jianfeng Gao, and Li Deng.
2016b. Knowledge as a teacher: Knowledge-
CoRR,
guided structural attention networks.
abs/1609.03286.

Zhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen,
Si Wei, Hui Jiang, and Xiaodan Zhu. 2015. Re-
visiting word embedding for contrasting meaning.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, ACL 2015, July 26-31, 2015,
Beijing, China, Volume 1: Long Papers, pages 106–
115.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2016, Austin, Texas, USA, November
1-4, 2016, pages 551–561.

Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2017.
Unsupervised learning of task-speciﬁc tree struc-
tures with tree-lstms. CoRR, abs/1707.02786.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2017, Copen-
hagen, Denmark, September 9-11, 2017, pages 670–
680.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, Eval-
uating Predictive Uncertainty, Visual Object Clas-
siﬁcation and Recognizing Textual Entailment, First
PASCAL Machine Learning Challenges Workshop,
MLCW 2005, Southampton, UK, April 11-13, 2005,
Revised Selected Papers, pages 177–190.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard H. Hovy, and Noah A. Smith.
2015. Retroﬁtting word vectors to semantic lexi-
cons. In NAACL HLT 2015, The 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Denver, Colorado, USA, May 31 - June 5,
2015, pages 1606–1615.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017b. Recurrent neural
network-based sentence encoder with gated atten-
tion for natural language inference. In Proceedings
of the 2nd Workshop on Evaluating Vector Space

Max Glockner, Vered Shwartz, and Yoav Goldberg.
2018. Breaking nli systems with sentences that re-
quire simple lexical inferences. In The 56th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Melbourne, Australia.

Yichen Gong, Heng Luo, and Jian Zhang. 2017.
Natural language inference over interaction space.
CoRR, abs/1709.04348.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Adrian Iftene and Alexandra Balahur-Dobrescu. 2007.
Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, chapter Hypoth-
esis Transformation and Semantic Variability Rules
Used in Recognizing Textual Entailment. Associa-
tion for Computational Linguistics.

Jay J. Jiang and David W. Conrath. 1997. Seman-
tic similarity based on corpus statistics and lexical
In Proceedings of the 10th Research
taxonomy.
on Computational Linguistics International Confer-
ence, ROCLING 1997, Taipei, Taiwan, August 1997,
pages 19–33.

Valentin Jijkoun and Maarten de Rijke. 2005. Recog-
nizing textual entailment using lexical similarity. In
Proceedings of the PASCAL Challenges Workshop
on Recognising Textual Entailment.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Zhouhan Lin, Minwei Feng, C´ıcero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. CoRR, abs/1703.03130.

Pengfei Liu, Xipeng Qiu, Jifan Chen, and Xuanjing
Huang. 2016a. Deep fusion lstms for text seman-
In Proceedings of the 54th Annual
tic matching.
Meeting of the Association for Computational Lin-
guistics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers.

Pengfei Liu, Xipeng Qiu, Yaqian Zhou, Jifan Chen, and
Xuanjing Huang. 2016b. Modelling interaction of
sentence pair with coupled-lstms. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2016, Austin,
Texas, USA, November 1-4, 2016, pages 1703–1712.

Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling, and
Yu Hu. 2015. Learning semantic word embeddings
In Pro-
based on ordinal knowledge constraints.
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, ACL 2015, July 26-31, 2015, Bei-
jing, China, Volume 1: Long Papers, pages 1501–
1511.

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016c. Learning natural language inference us-
ing bidirectional LSTM model and inner-attention.
CoRR, abs/1605.09090.

Bill MacCartney. 2009. Natural Language Inference.

Ph.D. thesis, Stanford University.

Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2008, Proceedings of the Confer-
ence, 25-27 October 2008, Honolulu, Hawaii, USA,
A meeting of SIGDAT, a Special Interest Group of
the ACL, pages 802–811.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural language
processing toolkit. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, ACL 2014, June 22-27, 2014, Baltimore,
MD, USA, System Demonstrations, pages 55–60.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
In Advances in Neural
textualized word vectors.
Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems
2017, 4-9 December 2017, Long Beach, CA, USA,
pages 6297–6308.

George A. Miller. 1995. Wordnet: A lexical database

for english. Commun. ACM, 38(11):39–41.

Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui
Yan, and Zhi Jin. 2016. Natural language infer-
ence by tree-based convolution and heuristic match-
ing. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics, ACL
2016, August 7-12, 2016, Berlin, Germany, Volume
2: Short Papers.

Nikola Mrksic, Ivan Vulic, Diarmuid ´O S´eaghdha, Ira
Leviant, Roi Reichart, Milica Gasic, Anna Korho-
nen, and Steve J. Young. 2017. Semantic special-
isation of distributional word vector spaces using
monolingual and cross-lingual constraints. CoRR,
abs/1706.00374.

Yixin Nie and Mohit Bansal. 2017.

Shortcut-
stacked sentence encoders for multi-domain infer-
In Proceedings of the 2nd Workshop on
ence.
Evaluating Vector Space Representations for NLP,
RepEval@EMNLP 2017, Copenhagen, Denmark,
September 8, 2017, pages 41–45.

Ankur P. Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pages
2249–2255.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
In Proceedings of the 2014
word representation.
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,

Hong Yu and Tsendsuren Munkhdalai. 2017a. Neural
semantic encoders. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL 2017, Valen-
cia, Spain, April 3-7, 2017, Volume 1: Long Papers,
pages 397–407.

Hong Yu and Tsendsuren Munkhdalai. 2017b. Neu-
ral tree indexers for text understanding. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL 2017, Valencia, Spain, April 3-7, 2017, Vol-
ume 1: Long Papers, pages 11–21.

Junbei Zhang, Xiaodan Zhu, Qian Chen, Lirong
Ex-
Dai, Si Wei,
ploring question understanding and adaptation in
neural-network-based question answering. CoRR,
abs/arXiv:1703.04617v2.

Jiang. 2017a.

and Hui

Shiyue Zhang, Gulnigar Mahmut, Dong Wang, and
Askar Hamdulla. 2017b.
Memory-augmented
chinese-uyghur neural machine translation. In 2017
Asia-Paciﬁc Signal and Information Processing As-
sociation Annual Summit and Conference, APSIPA
ASC 2017, Kuala Lumpur, Malaysia, December 12-
15, 2017, pages 1092–1096.

2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1532–1543.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz
Hermann, Tom´as Kocisk´y, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
CoRR, abs/1509.06664.

Lei Sha, Baobao Chang, Zhifang Sui, and Sujian Li.
2016. Reading and thinking: Re-read LSTM unit
In COLING
for textual entailment recognition.
2016, 26th International Conference on Computa-
tional Linguistics, Proceedings of the Conference:
Technical Papers, December 11-16, 2016, Osaka,
Japan, pages 2870–2879.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2017. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding. CoRR, abs/1709.04696.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen
Wang, and Chengqi Zhang. 2018. Reinforced self-
attention network: a hybrid of hard and soft attention
for sequence modeling. CoRR, abs/1801.10296.

Chen Shi, Shujie Liu, Shuo Ren, Shi Feng, Mu Li,
Ming Zhou, Xu Sun, and Houfeng Wang. 2016.
Knowledge-based semantic embedding for machine
translation. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. A
compare-propagate architecture with alignment fac-
torization for natural language inference. CoRR,
abs/1801.00102.

Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel
Urtasun. 2015. Order-embeddings of images and
language. CoRR, abs/1511.06361.

Shuohang Wang and Jing Jiang. 2016. Learning natu-
ral language inference with LSTM. In NAACL HLT
2016, The 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, San Diego
California, USA, June 12-17, 2016, pages 1442–
1451.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences. In Proceedings of the Twenty-Sixth
International Joint Conference on Artiﬁcial Intelli-
gence, IJCAI 2017, Melbourne, Australia, August
19-25, 2017, pages 4144–4150.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2015. From paraphrase database to compo-
sitional paraphrase model and back. TACL, 3:345–
358.

Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. CoRR,
abs/1704.05426.

Neural Natural Language Inference Models Enhanced with
External Knowledge

Qian Chen
University of Science and
Technology of China
cq1231@mail.ustc.edu.cn

Zhen-Hua Ling
University of Science and
Technology of China
zhling@ustc.edu.cn

Xiaodan Zhu
ECE, Queen’s University
xiaodan.zhu@queensu.ca

Diana Inkpen
University of Ottawa
diana@site.uottawa.ca

Si Wei
iFLYTEK Research
siwei@iflytek.com

Abstract

Modeling natural language inference is a
very challenging task. With the avail-
ability of large annotated data, it has re-
cently become feasible to train complex
models such as neural-network-based in-
ference models, which have shown to
achieve the state-of-the-art performance.
Although there exist relatively large anno-
tated data, can machines learn all knowl-
edge needed to perform natural language
inference (NLI) from these data? If not,
how can neural-network-based NLI mod-
els beneﬁt from external knowledge and
how to build NLI models to leverage it?
In this paper, we enrich the state-of-the-art
neural natural language inference models
with external knowledge. We demonstrate
that the proposed models improve neural
NLI models to achieve the state-of-the-art
performance on the SNLI and MultiNLI
datasets.

last

larger

In the

several years,

anno-
the
tated datasets were made available, e.g.,
(Bowman et al., 2015) and MultiNLI
SNLI
datasets (Williams et al., 2017), which made
it feasible to train rather complicated neural-
network-based models that ﬁt a large set of
parameters to better model NLI. Such models
have shown to achieve the state-of-the-art per-
formance (Bowman et al., 2015, 2016; Yu and
Munkhdalai, 2017b; Parikh et al., 2016; Sha et al.,
2016; Chen et al., 2017a,b; Tay et al., 2018).

While neural networks have been shown to be
very effective in modeling NLI with large train-
ing data, they have often focused on end-to-end
training by assuming that all inference knowledge
is learnable from the provided training data.
In
this paper, we relax this assumption and explore
whether external knowledge can further help NLI.
Consider an example:

• p: A lady standing in a wheat ﬁeld.

• h: A person standing in a corn ﬁeld.

1

Introduction

Reasoning and inference are central to both hu-
man and artiﬁcial intelligence. Natural language
inference (NLI), also known as recognizing tex-
tual entailment (RTE), is an important NLP prob-
lem concerned with determining inferential rela-
tionship (e.g., entailment, contradiction, or neu-
tral) between a premise p and a hypothesis h. In
general, modeling informal inference in language
is a very challenging and basic problem towards
achieving true natural language understanding.

In this simpliﬁed example, when computers are
asked to predict the relation between these two
sentences and if training data do not provide the
knowledge of relationship between “wheat” and
“corn” (e.g., if one of the two words does not ap-
pear in the training data or they are not paired in
any premise-hypothesis pairs), it will be hard for
computers to correctly recognize that the premise
contradicts the hypothesis.

In general, although in many tasks learning tab-
ula rasa achieved state-of-the-art performance, we
believe complicated NLP problems such as NLI

8
1
0
2
 
n
u
J
 
3
2
 
 
]
L
C
.
s
c
[
 
 
3
v
9
8
2
4
0
.
1
1
7
1
:
v
i
X
r
a

could beneﬁt from leveraging knowledge accumu-
lated by humans, particularly in a foreseeable fu-
ture when machines are unable to learn it by them-
selves.

In this paper we enrich neural-network-based
NLI models with external knowledge in co-
attention, local inference collection, and inference
composition components. We show the proposed
model improves the state-of-the-art NLI models
to achieve better performances on the SNLI and
MultiNLI datasets. The advantage of using exter-
nal knowledge is more signiﬁcant when the size of
training data is restricted, suggesting that if more
knowledge can be obtained, it may bring more
beneﬁt.
In addition to attaining the state-of-the-
art performance, we are also interested in under-
standing how external knowledge contributes to
the major components of typical neural-network-
based NLI models.

2 Related Work

Early research on natural language inference and
recognizing textual entailment has been performed
on relatively small datasets (refer to MacCartney
(2009) for a good literature survey), which in-
cludes a large bulk of contributions made under
the name of RTE, such as (Dagan et al., 2005;
Iftene and Balahur-Dobrescu, 2007), among many
others.

More recently the availability of much larger
annotated data, e.g., SNLI
(Bowman et al.,
2015) and MultiNLI (Williams et al., 2017), has
made it possible to train more complex mod-
els. These models mainly fall into two types
of approaches: sentence-encoding-based models
and models using also inter-sentence attention.
Sentence-encoding-based models use Siamese ar-
chitecture (Bromley et al., 1993). The parameter-
tied neural networks are applied to encode both
the premise and the hypothesis. Then a neural
network classiﬁer is applied to decide relationship
between the two sentences. Different neural net-
works have been utilized for sentence encoding,
such as LSTM (Bowman et al., 2015), GRU (Ven-
drov et al., 2015), CNN (Mou et al., 2016), BiL-
STM and its variants (Liu et al., 2016c; Lin et al.,
2017; Chen et al., 2017b; Nie and Bansal, 2017),
self-attention network (Shen et al., 2017, 2018),
and more complicated neural networks (Bowman
et al., 2016; Yu and Munkhdalai, 2017a,b; Choi
et al., 2017). Sentence-encoding-based models

transform sentences into ﬁxed-length vector rep-
resentations, which may help a wide range of
tasks (Conneau et al., 2017).

The second set of models use inter-sentence at-
tention (Rockt¨aschel et al., 2015; Wang and Jiang,
2016; Cheng et al., 2016; Parikh et al., 2016;
Chen et al., 2017a). Among them, Rockt¨aschel
et al. (2015) were among the ﬁrst to propose neu-
ral attention-based models for NLI. Chen et al.
(2017a) proposed an enhanced sequential infer-
ence model (ESIM), which is one of the best mod-
els so far and is used as one of our baselines in this
paper.

In this paper we enrich neural-network-based
NLI models with external knowledge. Unlike
early work on NLI (Jijkoun and de Rijke, 2005;
MacCartney et al., 2008; MacCartney, 2009) that
explores external knowledge in conventional NLI
models on relatively small NLI datasets, we aim to
merge the advantage of powerful modeling ability
of neural networks with extra external inference
knowledge. We show that the proposed model
improves the state-of-the-art neural NLI models
to achieve better performances on the SNLI and
MultiNLI datasets. The advantage of using exter-
nal knowledge is more signiﬁcant when the size of
training data is restricted, suggesting that if more
knowledge can be obtained, it may have more ben-
eﬁt.
In addition to attaining the state-of-the-art
performance, we are also interested in understand-
ing how external knowledge affect major compo-
nents of neural-network-based NLI models.

In general, external knowledge has shown to be
effective in neural networks for other NLP tasks,
including word embedding (Chen et al., 2015;
Faruqui et al., 2015; Liu et al., 2015; Wieting
et al., 2015; Mrksic et al., 2017), machine trans-
lation (Shi et al., 2016; Zhang et al., 2017b), lan-
guage modeling (Ahn et al., 2016), and dialogue
systems (Chen et al., 2016b).

3 Neural-Network-Based NLI Models

with External Knowledge

In this section we propose neural-network-based
inference
NLI models to incorporate external
knowledge, which, as we will show later in Sec-
tion 5, achieve the state-of-the-art performance.
In addition to attaining the leading performance
we are also interested in investigating the effects
of external knowledge on major components of
neural-network-based NLI modeling.

Figure 1 shows a high-level general view of the
proposed framework. While speciﬁc NLI systems
vary in their implementation, typical state-of-the-
art NLI models contain the main components (or
equivalents) of representing premise and hypoth-
esis sentences, collecting local (e.g., lexical) in-
ference information, and aggregating and compos-
ing local information to make the global decision
at the sentence level. We incorporate and investi-
gate external knowledge accordingly in these ma-
jor NLI components: computing co-attention, col-
lecting local inference information, and compos-
ing inference to make ﬁnal decision.

3.1 External Knowledge

We

study the

incorporation of

As discussed above, although there exist relatively
large annotated data for NLI, can machines learn
all inference knowledge needed to perform NLI
from the data? If not, how can neural network-
based NLI models beneﬁt from external knowl-
edge and how to build NLI models to leverage it?
external,
inference-related knowledge in major compo-
nents of neural networks for natural
language
inference. For example,
intuitively knowledge
synonymy, antonymy, hypernymy and
about
hyponymy between given words may help model
soft-alignment between premises and hypotheses;
knowledge about hypernymy and hyponymy
may help capture entailment; knowledge about
antonymy and co-hyponyms (words sharing the
same hypernym) may beneﬁt the modeling of
contradiction.

In this section, we discuss the incorporation of
basic, lexical-level semantic knowledge into neu-
ral NLI components. Speciﬁcally, we consider ex-
ternal lexical-level inference knowledge between
word wi and wj, which is represented as a vec-
tor rij and is incorporated into three speciﬁc com-
ponents shown in Figure 1. We will discuss the
details of how rij is constructed later in the exper-
iment setup section (Section 4) but instead focus
on the proposed model in this section. Note that
while we study lexical-level inference knowledge
in the paper, if inference knowledge about larger
pieces of text pairs (e.g., inference relations be-
tween phrases) are available, the proposed model
can be easily extended to handle that. In this paper,
we instead let the NLI models to compose lexical-
level knowledge to obtain inference relations be-
tween larger pieces of texts.

3.2 Encoding Premise and Hypothesis

Same as much previous work (Chen et al.,
2017a,b), we encode the premise and the hypoth-
esis with bidirectional LSTMs (BiLSTMs). The
premise is represented as a = (a1, . . . , am) and
the hypothesis is b = (b1, . . . , bn), where m
and n are the lengths of the sentences. Then a
and b are embedded into de-dimensional vectors
[E(a1), . . . , E(am)] and [E(b1), . . . , E(bn)] using
the embedding matrix E ∈ Rde×|V |, where |V | is
the vocabulary size and E can be initialized with
the pre-trained word embedding. To represent
words in its context, the premise and the hypothe-
sis are fed into BiLSTM encoders (Hochreiter and
Schmidhuber, 1997) to obtain context-dependent
hidden states as and bs:

as
i = Encoder(E(a), i) ,
bs
j = Encoder(E(b), j) .

(1)

(2)

where i and j indicate the i-th word in the premise
and the j-th word in the hypothesis, respectively.

3.3 Knowledge-Enriched Co-Attention

As discussed above, soft-alignment of word pairs
between the premise and the hypothesis may ben-
eﬁt from knowledge-enriched co-attention mech-
anism. Given the relation features rij ∈ Rdr be-
tween the premise’s i-th word and the hypothesis’s
j-th word derived from the external knowledge,
the co-attention is calculated as:

eij = (as

i )Tbs

j + F (rij) .

(3)

The function F can be any non-linear or linear
functions. In this paper, we use F (rij) = λ1(rij),
where λ is a hyper-parameter tuned on the devel-
opment set and 1 is the indication function as fol-
lows:

1(rij) =

(cid:40)
1
0

if rij is not a zero vector ;
if rij is a zero vector .

(4)

Intuitively, word pairs with semantic relationship,
e.g., synonymy, antonymy, hypernymy, hyponymy
and co-hyponyms, are probably aligned together.
We will discuss how we construct external knowl-
edge later in Section 4. We have also tried a two-
layer MLP as a universal function approximator
in function F to learn the underlying combination
function but did not observe further improvement
over the best performance we obtained on the de-
velopment datasets.

Figure 1: A high-level view of neural-network-based NLI models enriched with external knowledge in
co-attention, local inference collection, and inference composition.

Soft-alignment

is determined by the co-
attention matrix e ∈ Rm×n computed in Equa-
tion (3), which is used to obtain the local relevance
between the premise and the hypothesis. For the
hidden state of the i-th word in the premise, i.e.,
as
i (already encoding the word itself and its con-
text), the relevant semantics in the hypothesis is
identiﬁed into a context vector ac
i using eij, more
speciﬁcally with Equation (5).

αij =

exp(eij)
k=1 exp(eik)

(cid:80)n

βij =

exp(eij)
k=1 exp(ekj)

(cid:80)m

, ac

i =

αijbs
j ,

(5)

, bc

j =

βijas
i ,

(6)

n
(cid:88)

j=1
m
(cid:88)

i=1

where α ∈ Rm×n and β ∈ Rm×n are the nor-
malized attention weight matrices with respect to
the 2-axis and 1-axis. The same calculation is per-
formed for each word in the hypothesis, i.e., bs
j,
with Equation (6) to obtain the context vector bc
j.

3.4 Local Inference Collection with External

Knowledge

By way of comparing the inference-related seman-
tic relation between as
i (individual word repre-
sentation in premise) and ac
i (context representa-
tion from hypothesis which is align to word as
i ),
we can model local inference (i.e., word-level in-
ference) between aligned word pairs. Intuitively,
for example, knowledge about hypernymy or hy-
ponymy may help model entailment and knowl-
edge about antonymy and co-hyponyms may help
model contradiction. Through comparing as
i and

n
(cid:88)

j=1
m
(cid:88)

i=1

ac
in addition to their relation from external
i ,
knowledge, we can obtain word-level inference
information for each word. The same calcula-
tion is performed for bs
j. Thus, we collect
knowledge-enriched local inference information:

j and bc

i = G([as
am

i ; ac

i ; as

i − ac

i ; as

i ◦ ac
i ;

αijrij]) , (7)

j = G([bs
bm

j, bc

j; bs

j − bc

j; bs

j ◦ bc
j;

βijrji]) ,

(8)

where a heuristic matching trick with difference
and element-wise product is used (Mou et al.,
2016; Chen et al., 2017a). The last terms in Equa-
tion (7)(8) are used to obtain word-level infer-
ence information from external knowledge. Take
Equation (7) as example, rij is the relation fea-
ture between the i-th word in the premise and
the j-th word in the hypothesis, but we care
more about semantic relation between aligned
word pairs between the premise and the hypoth-
esis. Thus, we use a soft-aligned version through
the soft-alignment weight αij. For the i-th word
in the premise, the last term in Equation (7) is
a word-level inference information based on ex-
ternal knowledge between the i-th word and the
aligned word. The same calculation for hypoth-
esis is performed in Equation (8). G is a non-
linear mapping function to reduce dimensionality.
Speciﬁcally, we use a 1-layer feed-forward neural
network with the ReLU activation function with
a shortcut connection, i.e., concatenate the hidden
states after ReLU with the input (cid:80)n
j=1 αijrij (or
(cid:80)m
(or bm
j ).

i=1 βijrji) as the output am
i

3.5 Knowledge-Enhanced Inference

4 Experiment Set-Up

Composition

In this component, we introduce knowledge-
enriched inference composition. To determine the
overall inference relationship between the premise
and the hypothesis, we need to explore a compo-
sition layer to compose the local inference vectors
(am and bm) collected above:

av
i = Composition(am, i) ,
j = Composition(bm, j) .
bv

(9)

(10)

Here, we also use BiLSTMs as building blocks
for the composition layer, but the responsibility
of BiLSTMs in the inference composition layer
is completely different from that in the input en-
coding layer. The BiLSTMs here read local in-
ference vectors (am and bm) and learn to judge
the types of local inference relationship and dis-
tinguish crucial local inference vectors for overall
sentence-level inference relationship. Intuitively,
the ﬁnal prediction is likely to depend on word
pairs appearing in external knowledge that have
some semantic relation. Our inference model con-
verts the output hidden vectors of BiLSTMs to
the ﬁxed-length vector with pooling operations
and puts it into the ﬁnal classiﬁer to determine
the overall inference class. Particularly, in addi-
tion to using mean pooling and max pooling sim-
ilarly to ESIM (Chen et al., 2017a), we propose
to use weighted pooling based on external knowl-
edge to obtain a ﬁxed-length vector as in Equation
(11)(12).

aw =

bw =

m
(cid:88)

i=1
n
(cid:88)

j=1

exp(H((cid:80)n
i=1 exp(H((cid:80)n
exp(H((cid:80)m
j=1 exp(H((cid:80)m

(cid:80)m

(cid:80)n

j=1 αijrij))

j=1 αijrij))

i=1 βijrji))

i=1 βijrji))

av
i ,

(11)

bv
j .

(12)

In our experiments, we regard the function H as
a 1-layer feed-forward neural network with ReLU
activation function. We concatenate all pooling
vectors, i.e., mean, max, and weighted pooling,
into the ﬁxed-length vector and then put the vector
into the ﬁnal multilayer perceptron (MLP) clas-
siﬁer. The MLP has one hidden layer with tanh
activation and softmax output layer in our exper-
iments. The entire model is trained end-to-end,
through minimizing the cross-entropy loss.

4.1 Representation of External Knowledge

Lexical Semantic Relations As described in
to incorporate external knowledge
Section 3.1,
(as a knowledge vector rij) to the state-of-the-
art neural network-based NLI models, we ﬁrst
explore semantic relations in WordNet (Miller,
1995), motivated by MacCartney (2009). Specif-
ically, the relations of lexical pairs are derived as
described in (1)-(4) below. Instead of using Jiang-
Conrath WordNet distance metric (Jiang and Con-
rath, 1997), which does not improve the perfor-
mance of our models on the development sets, we
add a new feature, i.e., co-hyponyms, which con-
sistently beneﬁt our models.

(1) Synonymy: It takes the value 1 if the words in
the pair are synonyms in WordNet (i.e., be-
long to the same synset), and 0 otherwise. For
example, [felicitous, good] = 1, [dog, wolf] =
0.

(2) Antonymy: It takes the value 1 if the words
in the pair are antonyms in WordNet, and 0
otherwise. For example, [wet, dry] = 1.

(3) Hypernymy: It takes the value 1 − n/8 if one
word is a (direct or indirect) hypernym of the
other word in WordNet, where n is the num-
ber of edges between the two words in hier-
archies, and 0 otherwise. Note that we ignore
pairs in the hierarchy which have more than 8
edges in between. For example, [dog, canid]
= 0.875, [wolf, canid] = 0.875, [dog, carni-
vore] = 0.75, [canid, dog] = 0

(4) Hyponymy: It is simply the inverse of the hy-
pernymy feature. For example, [canid, dog]
= 0.875, [dog, canid] = 0.

(5) Co-hyponyms: It takes the value 1 if the two
words have the same hypernym but they do
not belong to the same synset, and 0 other-
wise. For example, [dog, wolf] = 1.

As discussed above, we expect features like syn-
onymy, antonymy, hypernymy, hyponymy and co-
hyponyms would help model co-attention align-
ment between the premise and the hypothesis.
Knowledge of hypernymy and hyponymy may help
capture entailment; knowledge of antonymy and
co-hyponyms may help model contradiction. Their
ﬁnal contributions will be learned in end-to-end
model training. We regard the vector r ∈ Rdr as

the relation feature derived from external knowl-
edge, where dr is 5 here. In addition, Table 1 re-
ports some key statistics of these features.

Feature

#Words

#Pairs

Synonymy
Antonymy
Hypernymy
Hyponymy
Co-hyponyms

84,487
6,161
57,475
57,475
53,281

237,937
6,617
753,086
753,086
3,674,700

Table 1: Statistics of lexical relation features.

In addition to the above relations, we also use
more relation features in WordNet, including in-
stance,
instance of, same instance, entailment,
member meronym, member holonym, substance
meronym, substance holonym, part meronym, part
holonym, summing up to 15 features, but these ad-
ditional features do not bring further improvement
on the development dataset, as also discussed in
Section 5.

Relation Embeddings
In the most recent years
graph embedding has been widely employed to
learn representation for vertexes and their relations
in a graph.
In our work here, we also capture
the relation between any two words in WordNet
through relation embedding. Speciﬁcally, we em-
ployed TransE (Bordes et al., 2013), a widely used
graph embedding methods, to capture relation em-
bedding between any two words. We used two
typical approaches to obtaining the relation em-
bedding. The ﬁrst directly uses 18 relation em-
beddings pretrained on the WN18 dataset (Bordes
et al., 2013). Speciﬁcally, if a word pair has a cer-
tain type relation, we take the corresponding re-
lation embedding. Sometimes, if a word pair has
multiple relations among the 18 types; we take an
average of the relation embedding. The second ap-
proach uses TransE’s word embedding (trained on
WordNet) to obtain relation embedding, through
the objective function used in TransE, i.e., l ≈
t − h, where l indicates relation embedding, t in-
dicates tail entity embedding, and h indicates head
entity embedding.

Note that in addition to relation embedding
trained on WordNet, other relational embedding
resources exist; e.g.,
that trained on Freebase
(WikiData) (Bollacker et al., 2007), but such
knowledge resources are mainly about facts (e.g.,
relationship between Bill Gates and Microsoft)
and are less for commonsense knowledge used in

general natural language inference (e.g., the color
yellow potentially contradicts red).

4.2 NLI Datasets

In our experiments, we use Stanford Natural Lan-
guage Inference (SNLI) dataset (Bowman et al.,
2015) and Multi-Genre Natural Language Infer-
ence (MultiNLI) (Williams et al., 2017) dataset,
which focus on three basic relations between a
premise and a potential hypothesis:
the premise
entails the hypothesis (entailment), they contradict
each other (contradiction), or they are not related
(neutral). We use the same data split as in previ-
ous work (Bowman et al., 2015; Williams et al.,
2017) and classiﬁcation accuracy as the evaluation
metric. In addition, we test our models (trained on
the SNLI training set) on a new test set (Glockner
et al., 2018), which assesses the lexical inference
abilities of NLI systems and consists of 8,193 sam-
ples. WordNet 3.0 (Miller, 1995) is used to extract
semantic relation features between words. The
words are lemmatized using Stanford CoreNLP
3.7.0 (Manning et al., 2014). The premise and the
hypothesis sentences fed into the input encoding
layer are tokenized.

4.3 Training Details
For duplicability, we release our code1. All our
models were strictly selected on the development
set of the SNLI data and the in-domain devel-
opment set of MultiNLI and were then tested on
the corresponding test set. The main training de-
tails are as follows:
the dimension of the hid-
den states of LSTMs and word embeddings are
300. The word embeddings are initialized by
300D GloVe 840B (Pennington et al., 2014), and
out-of-vocabulary words among them are initial-
ized randomly. All word embeddings are updated
during training. Adam (Kingma and Ba, 2014)
is used for optimization with an initial learning
rate of 0.0004. The mini-batch size is set to 32.
Note that the above hyperparameter settings are
same as those used in the baseline ESIM (Chen
et al., 2017a) model. ESIM is a strong NLI
baseline framework with the source code made
available at https://github.com/lukecq1231/nli (the
ESIM core code has also been adapted to sum-
marization (Chen et al., 2016a) and question-
answering tasks (Zhang et al., 2017a)).

The

trade-off

λ

for

calculating

co-

1https://github.com/lukecq1231/kim

in Equation

attention
(3)
in
[0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50]
the
development set. When training TransE for
WordNet, relations are represented with vectors
of 20 dimension.

selected
on

is
based

5 Experimental Results

5.1 Overall Performance

Table 2 shows the results of state-of-the-art models
on the SNLI dataset. Among them, ESIM (Chen
et al., 2017a) is one of the previous state-of-the-art
systems with an 88.0% test-set accuracy. The pro-
posed model, namely Knowledge-based Inference
Model (KIM), which enriches ESIM with external
knowledge, obtains an accuracy of 88.6%, the best
single-model performance reported on the SNLI
dataset. The difference between ESIM and KIM is
statistically signiﬁcant under the one-tailed paired
t-test at the 99% signiﬁcance level. Note that the
KIM model reported here uses ﬁve semantic rela-
tions described in Section 4. In addition to that, we
also use 15 semantic relation features, which does
not bring additional gains in performance. These
results highlight the effectiveness of the ﬁve se-
mantic relations described in Section 4. To further
investigate external knowledge, we add TransE re-
lation embedding, and again no further improve-
ment is observed on both the development and test
sets when TransE relation embedding is used (con-
catenated) with the semantic relation vectors. We
consider this is due to the fact that TransE embed-
ding is not speciﬁcally sensitive to inference in-
formation; e.g., it does not model co-hyponyms
features, and its potential beneﬁt has already been
covered by the semantic relation features used.

Table 3 shows the performance of models on the
MultiNLI dataset. The baseline ESIM achieves
76.8% and 75.8% on in-domain and cross-domain
test set, respectively. If we extend the ESIM with
external knowledge, we achieve signiﬁcant gains
to 77.2% and 76.4% respectively. Again, the gains
are consistent on SNLI and MultiNLI, and we ex-
pect they would be orthogonal to other factors
when external knowledge is added into other state-
of-the-art models.

5.2 Ablation Results

Figure 2 displays the ablation analysis of differ-
ent components when using the external knowl-
edge. To compare the effects of external knowl-
edge under different training data scales, we ran-

Model

LSTM Att. (Rockt¨aschel et al., 2015)
DF-LSTMs (Liu et al., 2016a)
TC-LSTMs (Liu et al., 2016b)
Match-LSTM (Wang and Jiang, 2016)
LSTMN (Cheng et al., 2016)
Decomposable Att. (Parikh et al., 2016)
NTI (Yu and Munkhdalai, 2017b)
Re-read LSTM (Sha et al., 2016)
BiMPM (Wang et al., 2017)
DIIN (Gong et al., 2017)
BCN + CoVe (McCann et al., 2017)
CAFE (Tay et al., 2018)

ESIM (Chen et al., 2017a)
KIM (This paper)

Test

83.5
84.6
85.1
86.1
86.3
86.8
87.3
87.5
87.5
88.0
88.1
88.5

88.0
88.6

Table 2: Accuracies of models on SNLI.

Model

In Cross

CBOW (Williams et al., 2017)
BiLSTM (Williams et al., 2017)
DiSAN (Shen et al., 2017)
Gated BiLSTM (Chen et al., 2017b)
SS BiLSTM (Nie and Bansal, 2017)
DIIN * (Gong et al., 2017)
CAFE (Tay et al., 2018)

ESIM (Chen et al., 2017a)
KIM (This paper)

64.8
66.9
71.0
73.5
74.6
77.8
78.7

76.8
77.2

64.5
66.9
71.4
73.6
73.6
78.8
77.9

75.8
76.4

Table 3: Accuracies of models on MultiNLI. * in-
dicates models using extra SNLI training set.

domly sample different ratios of the entire training
set, i.e., 0.8%, 4%, 20% and 100%. “A” indicates
adding external knowledge in calculating the co-
attention matrix as in Equation (3), “I” indicates
adding external knowledge in collecting local in-
ference information as in Equation (7)(8), and “C”
indicates adding external knowledge in compos-
ing inference as in Equation (11)(12). When we
only have restricted training data, i.e., 0.8% train-
ing set (about 4,000 samples), the baseline ESIM
has a poor accuracy of 62.4%. When we only
add external knowledge in calculating co-attention
(“A”), the accuracy increases to 66.6% (+ absolute
4.2%). When we only utilize external knowledge
in collecting local inference information (“I”), the
accuracy has a signiﬁcant gain, to 70.3% (+ ab-
solute 7.9%). When we only add external knowl-
edge in inference composition (“C”), the accuracy
gets a smaller gain to 63.4% (+ absolute 1.0%).
The comparison indicates that “I” plays the most
important role among the three components in us-
ing external knowledge. Moreover, when we com-

pose the three components (“A,I,C”), we obtain
the best result of 72.6% (+ absolute 10.2%). When
we use more training data, i.e., 4%, 20%, 100%
of the training set, only “I” achieves a signiﬁcant
gain, but “A” or “C” does not bring any signiﬁ-
cant improvement. The results indicate that ex-
ternal semantic knowledge only helps co-attention
and composition when limited training data is lim-
ited, but always helps in collecting local inference
information. Meanwhile, for less training data, λ
is usually set to a larger value. For example, the
optimal λ on the development set is 20 for 0.8%
training set, 2 for the 4% training set, 1 for the
20% training set and 0.2 for the 100% training set.
Figure 3 displays the results of using different
ratios of external knowledge (randomly keep dif-
ferent percentages of whole lexical semantic rela-
tions) under different sizes of training data. Note
that here we only use external knowledge in col-
lecting local inference information as it always
works well for different scale of the training set.
Better accuracies are achieved when using more
external knowledge. Especially under the condi-
tion of restricted training data (0.8%), the model
obtains a large gain when using more than half of
external knowledge.

Figure 2: Accuracies of models of incorporat-
ing external knowledge into different NLI compo-
nents, under different sizes of training data (0.8%,
4%, 20%, and the entire training data).

5.3 Analysis on the (Glockner et al., 2018)

Test Set

In addition, Table 4 shows the results on a newly
published test set (Glockner et al., 2018). Com-
pared with the performance on the SNLI test

Figure 3: Accuracies of models under differ-
ent sizes of external knowledge. More external
knowledge corresponds to higher accuracies.

Model

SNLI Glockner’s(∆)

(Parikh et al., 2016)*
(Nie and Bansal, 2017)*
ESIM *
KIM (This paper)

84.7
86.0
87.9
88.6

51.9 (-32.8)
62.2 (-23.8)
65.6 (-22.3)
83.5 ( -5.1)

Table 4: Accuracies of models on the SNLI and
(Glockner et al., 2018) test set. * indicates the re-
sults taken from (Glockner et al., 2018).

set, the performance of the three baseline mod-
els dropped substantially on the (Glockner et al.,
2018) test set, with the differences ranging from
22.3% to 32.8% in accuracy. Instead, the proposed
KIM achieves 83.5% on this test set (with only a
5.1% drop in performance), which demonstrates
its better ability of utilizing lexical level inference
and hence better generalizability.

Figure 5 displays the accuracy of ESIM
and KIM in each replacement-word category of
the (Glockner et al., 2018) test set. KIM outper-
forms ESIM in 13 out of 14 categories, and only
performs worse on synonyms.

5.4 Analysis by Inference Categories

We perform more analysis (Table 6) using the sup-
plementary annotations provided by the MultiNLI
dataset (Williams et al., 2017), which have 495
samples (about 1/20 of the entire development set)
for both in-domain and out-domain set. We com-
pare against the model outputs of the ESIM model
across 13 categories of inference. Table 6 reports
the results. We can see that KIM outperforms
ESIM on overall accuracies on both in-domain and

Category

Instance ESIM KIM

P/G Sentences

Antonyms
Cardinals
Nationalities
Drinks
Antonyms WordNet
Colors
Ordinals
Countries
Rooms
Materials
Vegetables
Instruments
Planets
Synonyms

Overall

1,147
759
755
731
706
699
663
613
595
397
109
65
60
894

8,193

70.4
75.5
35.9
63.7
74.6
96.1
21.0
25.4
69.4
89.7
31.2
90.8
3.3
99.7

65.6

86.5
93.4
73.5
96.6
78.8
98.3
56.6
70.8
77.6
98.7
79.8
96.9
5.0
92.1

83.5

Table 5: The number of instances and accu-
racy per category achieved by ESIM and KIM on
the (Glockner et al., 2018) test set.

Category

In-domain Cross-domain
ESIM KIM ESIM KIM

Active/Passive
Antonym
Belief
Conditional
Coreference
Long sentence
Modal
Negation
Paraphrase
Quantity/Time
Quantiﬁer
Tense
Word overlap

Overall

93.3
76.5
72.7
65.2
80.0
82.8
80.6
76.7
84.0
66.7
79.2
74.5
89.3

77.1

93.3
76.5
75.8
65.2
76.7
78.8
79.9
79.8
72.0
66.7
78.4
78.4
85.7

77.9

100.0
70.0
75.9
61.5
75.9
69.7
77.0
73.1
86.5
56.4
73.6
72.2
83.8

76.7

100.0
75.0
79.3
69.2
75.9
73.4
80.2
71.2
89.2
59.0
77.1
66.7
81.1

77.4

Table 6: Detailed Analysis on MultiNLI.

cross-domain subset of development set. KIM out-
performs or equals ESIM in 10 out of 13 cate-
gories on the cross-domain setting, while only 7
out of 13 categories on in-domain setting. It indi-
cates that external knowledge helps more in cross-
domain setting. Especially, for antonym category
in cross-domain set, KIM outperform ESIM sig-
niﬁcantly (+ absolute 5.0%) as expected, because
antonym feature captured by external knowledge
would help unseen cross-domain samples.

5.5 Case Study

Table 7 includes some examples from the SNLI
test set, where KIM successfully predicts the in-
ference relation and ESIM fails. In the ﬁrst exam-

e/c

e/c

c/e

c/e

p: An African person standing in a wheat
ﬁeld.
h: A person standing in a corn ﬁeld.

p: Little girl is ﬂipping an omelet in the
kitchen.
h: A young girl cooks pancakes.

p: A middle eastern marketplace.
h: A middle easten store.

p: Two boys are swimming with boogie
boards.
h: Two boys are swimming with their ﬂoats.

Table 7: Examples. Word in bold are key words
in making ﬁnal prediction. P indicates a predicted
label and G indicates gold-standard label. e and c
denote entailment and contradiction, respectively.

ple, the premise is “An African person standing in
a wheat ﬁeld” and the hypothesis “A person stand-
ing in a corn ﬁeld”. As the KIM model knows that
“wheat” and “corn” are both a kind of cereal, i.e,
the co-hyponyms relationship in our relation fea-
tures, KIM therefore predicts the premise contra-
dicts the hypothesis. However, the baseline ESIM
cannot learn the relationship between “wheat” and
“corn” effectively due to lack of enough samples
in the training sets. With the help of external
knowledge, i.e., “wheat” and “corn” having the
same hypernym “cereal”, KIM predicts contradic-
tion correctly.

6 Conclusions

Our neural-network-based model for natural lan-
guage inference with external knowledge, namely
KIM, achieves the state-of-the-art accuracies. The
model is equipped with external knowledge in its
main components, speciﬁcally, in calculating co-
attention, collecting local inference, and compos-
ing inference. We provide detailed analyses on our
model and results. The proposed model of infus-
ing neural networks with external knowledge may
also help shed some light on tasks other than NLI.

Acknowledgments

We thank Yibo Sun and Bing Qin for early helpful
discussion.

References

Sungjin Ahn, Heeyoul Choi, Tanel P¨arnamaa, and
Yoshua Bengio. 2016. A neural knowledge lan-
guage model. CoRR, abs/1608.00318.

Kurt D. Bollacker, Robert P. Cook, and Patrick Tufts.
2007. Freebase: A shared database of structured
In Proceedings of the
general human knowledge.
Twenty-Second AAAI Conference on Artiﬁcial In-
telligence, July 22-26, 2007, Vancouver, British
Columbia, Canada, pages 1962–1963.

Antoine Bordes, Nicolas Usunier, Alberto Garc´ıa-
Dur´an, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States., pages 2787–
2795.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
In Proceedings of the 2015 Conference on
ence.
Empirical Methods in Natural Language Process-
ing, EMNLP 2015, Lisbon, Portugal, September 17-
21, 2015, pages 632–642.

Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. 2016. A fast uniﬁed model for
parsing and sentence understanding. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics, ACL 2016, August 7-12,
2016, Berlin, Germany, Volume 1: Long Papers.

Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard
S¨ackinger, and Roopak Shah. 1993. Signature veri-
ﬁcation using a siamese time delay neural network.
In Advances in Neural Information Processing Sys-
tems 6, [7th NIPS Conference, Denver, Colorado,
USA, 1993], pages 737–744.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
and Hui Jiang. 2016a. Distraction-based neural net-
In Proceedings of
works for modeling document.
the Twenty-Fifth International Joint Conference on
Artiﬁcial Intelligence, IJCAI 2016, New York, NY,
USA, 9-15 July 2016, pages 2754–2760.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
Hui Jiang, and Diana Inkpen. 2017a. Enhanced
LSTM for natural language inference. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics, ACL 2017, Vancou-
ver, Canada, July 30 - August 4, Volume 1: Long
Papers, pages 1657–1668.

Representations for NLP, RepEval@EMNLP 2017,
Copenhagen, Denmark, September 8, 2017, pages
36–40.

Yun-Nung Chen, Dilek Z. Hakkani-T¨ur, G¨okhan T¨ur,
Asli C¸ elikyilmaz, Jianfeng Gao, and Li Deng.
2016b. Knowledge as a teacher: Knowledge-
CoRR,
guided structural attention networks.
abs/1609.03286.

Zhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen,
Si Wei, Hui Jiang, and Xiaodan Zhu. 2015. Re-
visiting word embedding for contrasting meaning.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, ACL 2015, July 26-31, 2015,
Beijing, China, Volume 1: Long Papers, pages 106–
115.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2016, Austin, Texas, USA, November
1-4, 2016, pages 551–561.

Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2017.
Unsupervised learning of task-speciﬁc tree struc-
tures with tree-lstms. CoRR, abs/1707.02786.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2017, Copen-
hagen, Denmark, September 9-11, 2017, pages 670–
680.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, Eval-
uating Predictive Uncertainty, Visual Object Clas-
siﬁcation and Recognizing Textual Entailment, First
PASCAL Machine Learning Challenges Workshop,
MLCW 2005, Southampton, UK, April 11-13, 2005,
Revised Selected Papers, pages 177–190.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard H. Hovy, and Noah A. Smith.
2015. Retroﬁtting word vectors to semantic lexi-
cons. In NAACL HLT 2015, The 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Denver, Colorado, USA, May 31 - June 5,
2015, pages 1606–1615.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017b. Recurrent neural
network-based sentence encoder with gated atten-
tion for natural language inference. In Proceedings
of the 2nd Workshop on Evaluating Vector Space

Max Glockner, Vered Shwartz, and Yoav Goldberg.
2018. Breaking nli systems with sentences that re-
quire simple lexical inferences. In The 56th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Melbourne, Australia.

Yichen Gong, Heng Luo, and Jian Zhang. 2017.
Natural language inference over interaction space.
CoRR, abs/1709.04348.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Adrian Iftene and Alexandra Balahur-Dobrescu. 2007.
Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, chapter Hypoth-
esis Transformation and Semantic Variability Rules
Used in Recognizing Textual Entailment. Associa-
tion for Computational Linguistics.

Jay J. Jiang and David W. Conrath. 1997. Seman-
tic similarity based on corpus statistics and lexical
In Proceedings of the 10th Research
taxonomy.
on Computational Linguistics International Confer-
ence, ROCLING 1997, Taipei, Taiwan, August 1997,
pages 19–33.

Valentin Jijkoun and Maarten de Rijke. 2005. Recog-
nizing textual entailment using lexical similarity. In
Proceedings of the PASCAL Challenges Workshop
on Recognising Textual Entailment.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Zhouhan Lin, Minwei Feng, C´ıcero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. CoRR, abs/1703.03130.

Pengfei Liu, Xipeng Qiu, Jifan Chen, and Xuanjing
Huang. 2016a. Deep fusion lstms for text seman-
In Proceedings of the 54th Annual
tic matching.
Meeting of the Association for Computational Lin-
guistics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers.

Pengfei Liu, Xipeng Qiu, Yaqian Zhou, Jifan Chen, and
Xuanjing Huang. 2016b. Modelling interaction of
sentence pair with coupled-lstms. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2016, Austin,
Texas, USA, November 1-4, 2016, pages 1703–1712.

Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling, and
Yu Hu. 2015. Learning semantic word embeddings
In Pro-
based on ordinal knowledge constraints.
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, ACL 2015, July 26-31, 2015, Bei-
jing, China, Volume 1: Long Papers, pages 1501–
1511.

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016c. Learning natural language inference us-
ing bidirectional LSTM model and inner-attention.
CoRR, abs/1605.09090.

Bill MacCartney. 2009. Natural Language Inference.

Ph.D. thesis, Stanford University.

Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2008, Proceedings of the Confer-
ence, 25-27 October 2008, Honolulu, Hawaii, USA,
A meeting of SIGDAT, a Special Interest Group of
the ACL, pages 802–811.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural language
processing toolkit. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, ACL 2014, June 22-27, 2014, Baltimore,
MD, USA, System Demonstrations, pages 55–60.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
In Advances in Neural
textualized word vectors.
Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems
2017, 4-9 December 2017, Long Beach, CA, USA,
pages 6297–6308.

George A. Miller. 1995. Wordnet: A lexical database

for english. Commun. ACM, 38(11):39–41.

Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui
Yan, and Zhi Jin. 2016. Natural language infer-
ence by tree-based convolution and heuristic match-
ing. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics, ACL
2016, August 7-12, 2016, Berlin, Germany, Volume
2: Short Papers.

Nikola Mrksic, Ivan Vulic, Diarmuid ´O S´eaghdha, Ira
Leviant, Roi Reichart, Milica Gasic, Anna Korho-
nen, and Steve J. Young. 2017. Semantic special-
isation of distributional word vector spaces using
monolingual and cross-lingual constraints. CoRR,
abs/1706.00374.

Yixin Nie and Mohit Bansal. 2017.

Shortcut-
stacked sentence encoders for multi-domain infer-
In Proceedings of the 2nd Workshop on
ence.
Evaluating Vector Space Representations for NLP,
RepEval@EMNLP 2017, Copenhagen, Denmark,
September 8, 2017, pages 41–45.

Ankur P. Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pages
2249–2255.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
In Proceedings of the 2014
word representation.
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,

Hong Yu and Tsendsuren Munkhdalai. 2017a. Neural
semantic encoders. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL 2017, Valen-
cia, Spain, April 3-7, 2017, Volume 1: Long Papers,
pages 397–407.

Hong Yu and Tsendsuren Munkhdalai. 2017b. Neu-
ral tree indexers for text understanding. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL 2017, Valencia, Spain, April 3-7, 2017, Vol-
ume 1: Long Papers, pages 11–21.

Junbei Zhang, Xiaodan Zhu, Qian Chen, Lirong
Ex-
Dai, Si Wei,
ploring question understanding and adaptation in
neural-network-based question answering. CoRR,
abs/arXiv:1703.04617v2.

Jiang. 2017a.

and Hui

Shiyue Zhang, Gulnigar Mahmut, Dong Wang, and
Askar Hamdulla. 2017b.
Memory-augmented
chinese-uyghur neural machine translation. In 2017
Asia-Paciﬁc Signal and Information Processing As-
sociation Annual Summit and Conference, APSIPA
ASC 2017, Kuala Lumpur, Malaysia, December 12-
15, 2017, pages 1092–1096.

2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1532–1543.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz
Hermann, Tom´as Kocisk´y, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
CoRR, abs/1509.06664.

Lei Sha, Baobao Chang, Zhifang Sui, and Sujian Li.
2016. Reading and thinking: Re-read LSTM unit
In COLING
for textual entailment recognition.
2016, 26th International Conference on Computa-
tional Linguistics, Proceedings of the Conference:
Technical Papers, December 11-16, 2016, Osaka,
Japan, pages 2870–2879.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2017. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding. CoRR, abs/1709.04696.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen
Wang, and Chengqi Zhang. 2018. Reinforced self-
attention network: a hybrid of hard and soft attention
for sequence modeling. CoRR, abs/1801.10296.

Chen Shi, Shujie Liu, Shuo Ren, Shi Feng, Mu Li,
Ming Zhou, Xu Sun, and Houfeng Wang. 2016.
Knowledge-based semantic embedding for machine
translation. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. A
compare-propagate architecture with alignment fac-
torization for natural language inference. CoRR,
abs/1801.00102.

Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel
Urtasun. 2015. Order-embeddings of images and
language. CoRR, abs/1511.06361.

Shuohang Wang and Jing Jiang. 2016. Learning natu-
ral language inference with LSTM. In NAACL HLT
2016, The 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, San Diego
California, USA, June 12-17, 2016, pages 1442–
1451.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences. In Proceedings of the Twenty-Sixth
International Joint Conference on Artiﬁcial Intelli-
gence, IJCAI 2017, Melbourne, Australia, August
19-25, 2017, pages 4144–4150.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2015. From paraphrase database to compo-
sitional paraphrase model and back. TACL, 3:345–
358.

Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. CoRR,
abs/1704.05426.

Neural Natural Language Inference Models Enhanced with
External Knowledge

Qian Chen
University of Science and
Technology of China
cq1231@mail.ustc.edu.cn

Zhen-Hua Ling
University of Science and
Technology of China
zhling@ustc.edu.cn

Xiaodan Zhu
ECE, Queen’s University
xiaodan.zhu@queensu.ca

Diana Inkpen
University of Ottawa
diana@site.uottawa.ca

Si Wei
iFLYTEK Research
siwei@iflytek.com

Abstract

Modeling natural language inference is a
very challenging task. With the avail-
ability of large annotated data, it has re-
cently become feasible to train complex
models such as neural-network-based in-
ference models, which have shown to
achieve the state-of-the-art performance.
Although there exist relatively large anno-
tated data, can machines learn all knowl-
edge needed to perform natural language
inference (NLI) from these data? If not,
how can neural-network-based NLI mod-
els beneﬁt from external knowledge and
how to build NLI models to leverage it?
In this paper, we enrich the state-of-the-art
neural natural language inference models
with external knowledge. We demonstrate
that the proposed models improve neural
NLI models to achieve the state-of-the-art
performance on the SNLI and MultiNLI
datasets.

last

larger

In the

several years,

anno-
the
tated datasets were made available, e.g.,
(Bowman et al., 2015) and MultiNLI
SNLI
datasets (Williams et al., 2017), which made
it feasible to train rather complicated neural-
network-based models that ﬁt a large set of
parameters to better model NLI. Such models
have shown to achieve the state-of-the-art per-
formance (Bowman et al., 2015, 2016; Yu and
Munkhdalai, 2017b; Parikh et al., 2016; Sha et al.,
2016; Chen et al., 2017a,b; Tay et al., 2018).

While neural networks have been shown to be
very effective in modeling NLI with large train-
ing data, they have often focused on end-to-end
training by assuming that all inference knowledge
is learnable from the provided training data.
In
this paper, we relax this assumption and explore
whether external knowledge can further help NLI.
Consider an example:

• p: A lady standing in a wheat ﬁeld.

• h: A person standing in a corn ﬁeld.

1

Introduction

Reasoning and inference are central to both hu-
man and artiﬁcial intelligence. Natural language
inference (NLI), also known as recognizing tex-
tual entailment (RTE), is an important NLP prob-
lem concerned with determining inferential rela-
tionship (e.g., entailment, contradiction, or neu-
tral) between a premise p and a hypothesis h. In
general, modeling informal inference in language
is a very challenging and basic problem towards
achieving true natural language understanding.

In this simpliﬁed example, when computers are
asked to predict the relation between these two
sentences and if training data do not provide the
knowledge of relationship between “wheat” and
“corn” (e.g., if one of the two words does not ap-
pear in the training data or they are not paired in
any premise-hypothesis pairs), it will be hard for
computers to correctly recognize that the premise
contradicts the hypothesis.

In general, although in many tasks learning tab-
ula rasa achieved state-of-the-art performance, we
believe complicated NLP problems such as NLI

8
1
0
2
 
n
u
J
 
3
2
 
 
]
L
C
.
s
c
[
 
 
3
v
9
8
2
4
0
.
1
1
7
1
:
v
i
X
r
a

could beneﬁt from leveraging knowledge accumu-
lated by humans, particularly in a foreseeable fu-
ture when machines are unable to learn it by them-
selves.

In this paper we enrich neural-network-based
NLI models with external knowledge in co-
attention, local inference collection, and inference
composition components. We show the proposed
model improves the state-of-the-art NLI models
to achieve better performances on the SNLI and
MultiNLI datasets. The advantage of using exter-
nal knowledge is more signiﬁcant when the size of
training data is restricted, suggesting that if more
knowledge can be obtained, it may bring more
beneﬁt.
In addition to attaining the state-of-the-
art performance, we are also interested in under-
standing how external knowledge contributes to
the major components of typical neural-network-
based NLI models.

2 Related Work

Early research on natural language inference and
recognizing textual entailment has been performed
on relatively small datasets (refer to MacCartney
(2009) for a good literature survey), which in-
cludes a large bulk of contributions made under
the name of RTE, such as (Dagan et al., 2005;
Iftene and Balahur-Dobrescu, 2007), among many
others.

More recently the availability of much larger
annotated data, e.g., SNLI
(Bowman et al.,
2015) and MultiNLI (Williams et al., 2017), has
made it possible to train more complex mod-
els. These models mainly fall into two types
of approaches: sentence-encoding-based models
and models using also inter-sentence attention.
Sentence-encoding-based models use Siamese ar-
chitecture (Bromley et al., 1993). The parameter-
tied neural networks are applied to encode both
the premise and the hypothesis. Then a neural
network classiﬁer is applied to decide relationship
between the two sentences. Different neural net-
works have been utilized for sentence encoding,
such as LSTM (Bowman et al., 2015), GRU (Ven-
drov et al., 2015), CNN (Mou et al., 2016), BiL-
STM and its variants (Liu et al., 2016c; Lin et al.,
2017; Chen et al., 2017b; Nie and Bansal, 2017),
self-attention network (Shen et al., 2017, 2018),
and more complicated neural networks (Bowman
et al., 2016; Yu and Munkhdalai, 2017a,b; Choi
et al., 2017). Sentence-encoding-based models

transform sentences into ﬁxed-length vector rep-
resentations, which may help a wide range of
tasks (Conneau et al., 2017).

The second set of models use inter-sentence at-
tention (Rockt¨aschel et al., 2015; Wang and Jiang,
2016; Cheng et al., 2016; Parikh et al., 2016;
Chen et al., 2017a). Among them, Rockt¨aschel
et al. (2015) were among the ﬁrst to propose neu-
ral attention-based models for NLI. Chen et al.
(2017a) proposed an enhanced sequential infer-
ence model (ESIM), which is one of the best mod-
els so far and is used as one of our baselines in this
paper.

In this paper we enrich neural-network-based
NLI models with external knowledge. Unlike
early work on NLI (Jijkoun and de Rijke, 2005;
MacCartney et al., 2008; MacCartney, 2009) that
explores external knowledge in conventional NLI
models on relatively small NLI datasets, we aim to
merge the advantage of powerful modeling ability
of neural networks with extra external inference
knowledge. We show that the proposed model
improves the state-of-the-art neural NLI models
to achieve better performances on the SNLI and
MultiNLI datasets. The advantage of using exter-
nal knowledge is more signiﬁcant when the size of
training data is restricted, suggesting that if more
knowledge can be obtained, it may have more ben-
eﬁt.
In addition to attaining the state-of-the-art
performance, we are also interested in understand-
ing how external knowledge affect major compo-
nents of neural-network-based NLI models.

In general, external knowledge has shown to be
effective in neural networks for other NLP tasks,
including word embedding (Chen et al., 2015;
Faruqui et al., 2015; Liu et al., 2015; Wieting
et al., 2015; Mrksic et al., 2017), machine trans-
lation (Shi et al., 2016; Zhang et al., 2017b), lan-
guage modeling (Ahn et al., 2016), and dialogue
systems (Chen et al., 2016b).

3 Neural-Network-Based NLI Models

with External Knowledge

In this section we propose neural-network-based
inference
NLI models to incorporate external
knowledge, which, as we will show later in Sec-
tion 5, achieve the state-of-the-art performance.
In addition to attaining the leading performance
we are also interested in investigating the effects
of external knowledge on major components of
neural-network-based NLI modeling.

Figure 1 shows a high-level general view of the
proposed framework. While speciﬁc NLI systems
vary in their implementation, typical state-of-the-
art NLI models contain the main components (or
equivalents) of representing premise and hypoth-
esis sentences, collecting local (e.g., lexical) in-
ference information, and aggregating and compos-
ing local information to make the global decision
at the sentence level. We incorporate and investi-
gate external knowledge accordingly in these ma-
jor NLI components: computing co-attention, col-
lecting local inference information, and compos-
ing inference to make ﬁnal decision.

3.1 External Knowledge

We

study the

incorporation of

As discussed above, although there exist relatively
large annotated data for NLI, can machines learn
all inference knowledge needed to perform NLI
from the data? If not, how can neural network-
based NLI models beneﬁt from external knowl-
edge and how to build NLI models to leverage it?
external,
inference-related knowledge in major compo-
nents of neural networks for natural
language
inference. For example,
intuitively knowledge
synonymy, antonymy, hypernymy and
about
hyponymy between given words may help model
soft-alignment between premises and hypotheses;
knowledge about hypernymy and hyponymy
may help capture entailment; knowledge about
antonymy and co-hyponyms (words sharing the
same hypernym) may beneﬁt the modeling of
contradiction.

In this section, we discuss the incorporation of
basic, lexical-level semantic knowledge into neu-
ral NLI components. Speciﬁcally, we consider ex-
ternal lexical-level inference knowledge between
word wi and wj, which is represented as a vec-
tor rij and is incorporated into three speciﬁc com-
ponents shown in Figure 1. We will discuss the
details of how rij is constructed later in the exper-
iment setup section (Section 4) but instead focus
on the proposed model in this section. Note that
while we study lexical-level inference knowledge
in the paper, if inference knowledge about larger
pieces of text pairs (e.g., inference relations be-
tween phrases) are available, the proposed model
can be easily extended to handle that. In this paper,
we instead let the NLI models to compose lexical-
level knowledge to obtain inference relations be-
tween larger pieces of texts.

3.2 Encoding Premise and Hypothesis

Same as much previous work (Chen et al.,
2017a,b), we encode the premise and the hypoth-
esis with bidirectional LSTMs (BiLSTMs). The
premise is represented as a = (a1, . . . , am) and
the hypothesis is b = (b1, . . . , bn), where m
and n are the lengths of the sentences. Then a
and b are embedded into de-dimensional vectors
[E(a1), . . . , E(am)] and [E(b1), . . . , E(bn)] using
the embedding matrix E ∈ Rde×|V |, where |V | is
the vocabulary size and E can be initialized with
the pre-trained word embedding. To represent
words in its context, the premise and the hypothe-
sis are fed into BiLSTM encoders (Hochreiter and
Schmidhuber, 1997) to obtain context-dependent
hidden states as and bs:

as
i = Encoder(E(a), i) ,
bs
j = Encoder(E(b), j) .

(1)

(2)

where i and j indicate the i-th word in the premise
and the j-th word in the hypothesis, respectively.

3.3 Knowledge-Enriched Co-Attention

As discussed above, soft-alignment of word pairs
between the premise and the hypothesis may ben-
eﬁt from knowledge-enriched co-attention mech-
anism. Given the relation features rij ∈ Rdr be-
tween the premise’s i-th word and the hypothesis’s
j-th word derived from the external knowledge,
the co-attention is calculated as:

eij = (as

i )Tbs

j + F (rij) .

(3)

The function F can be any non-linear or linear
functions. In this paper, we use F (rij) = λ1(rij),
where λ is a hyper-parameter tuned on the devel-
opment set and 1 is the indication function as fol-
lows:

1(rij) =

(cid:40)
1
0

if rij is not a zero vector ;
if rij is a zero vector .

(4)

Intuitively, word pairs with semantic relationship,
e.g., synonymy, antonymy, hypernymy, hyponymy
and co-hyponyms, are probably aligned together.
We will discuss how we construct external knowl-
edge later in Section 4. We have also tried a two-
layer MLP as a universal function approximator
in function F to learn the underlying combination
function but did not observe further improvement
over the best performance we obtained on the de-
velopment datasets.

Figure 1: A high-level view of neural-network-based NLI models enriched with external knowledge in
co-attention, local inference collection, and inference composition.

Soft-alignment

is determined by the co-
attention matrix e ∈ Rm×n computed in Equa-
tion (3), which is used to obtain the local relevance
between the premise and the hypothesis. For the
hidden state of the i-th word in the premise, i.e.,
as
i (already encoding the word itself and its con-
text), the relevant semantics in the hypothesis is
identiﬁed into a context vector ac
i using eij, more
speciﬁcally with Equation (5).

αij =

exp(eij)
k=1 exp(eik)

(cid:80)n

βij =

exp(eij)
k=1 exp(ekj)

(cid:80)m

, ac

i =

αijbs
j ,

(5)

, bc

j =

βijas
i ,

(6)

n
(cid:88)

j=1
m
(cid:88)

i=1

where α ∈ Rm×n and β ∈ Rm×n are the nor-
malized attention weight matrices with respect to
the 2-axis and 1-axis. The same calculation is per-
formed for each word in the hypothesis, i.e., bs
j,
with Equation (6) to obtain the context vector bc
j.

3.4 Local Inference Collection with External

Knowledge

By way of comparing the inference-related seman-
tic relation between as
i (individual word repre-
sentation in premise) and ac
i (context representa-
tion from hypothesis which is align to word as
i ),
we can model local inference (i.e., word-level in-
ference) between aligned word pairs. Intuitively,
for example, knowledge about hypernymy or hy-
ponymy may help model entailment and knowl-
edge about antonymy and co-hyponyms may help
model contradiction. Through comparing as
i and

n
(cid:88)

j=1
m
(cid:88)

i=1

ac
in addition to their relation from external
i ,
knowledge, we can obtain word-level inference
information for each word. The same calcula-
tion is performed for bs
j. Thus, we collect
knowledge-enriched local inference information:

j and bc

i = G([as
am

i ; ac

i ; as

i − ac

i ; as

i ◦ ac
i ;

αijrij]) , (7)

j = G([bs
bm

j, bc

j; bs

j − bc

j; bs

j ◦ bc
j;

βijrji]) ,

(8)

where a heuristic matching trick with difference
and element-wise product is used (Mou et al.,
2016; Chen et al., 2017a). The last terms in Equa-
tion (7)(8) are used to obtain word-level infer-
ence information from external knowledge. Take
Equation (7) as example, rij is the relation fea-
ture between the i-th word in the premise and
the j-th word in the hypothesis, but we care
more about semantic relation between aligned
word pairs between the premise and the hypoth-
esis. Thus, we use a soft-aligned version through
the soft-alignment weight αij. For the i-th word
in the premise, the last term in Equation (7) is
a word-level inference information based on ex-
ternal knowledge between the i-th word and the
aligned word. The same calculation for hypoth-
esis is performed in Equation (8). G is a non-
linear mapping function to reduce dimensionality.
Speciﬁcally, we use a 1-layer feed-forward neural
network with the ReLU activation function with
a shortcut connection, i.e., concatenate the hidden
states after ReLU with the input (cid:80)n
j=1 αijrij (or
(cid:80)m
(or bm
j ).

i=1 βijrji) as the output am
i

3.5 Knowledge-Enhanced Inference

4 Experiment Set-Up

Composition

In this component, we introduce knowledge-
enriched inference composition. To determine the
overall inference relationship between the premise
and the hypothesis, we need to explore a compo-
sition layer to compose the local inference vectors
(am and bm) collected above:

av
i = Composition(am, i) ,
j = Composition(bm, j) .
bv

(9)

(10)

Here, we also use BiLSTMs as building blocks
for the composition layer, but the responsibility
of BiLSTMs in the inference composition layer
is completely different from that in the input en-
coding layer. The BiLSTMs here read local in-
ference vectors (am and bm) and learn to judge
the types of local inference relationship and dis-
tinguish crucial local inference vectors for overall
sentence-level inference relationship. Intuitively,
the ﬁnal prediction is likely to depend on word
pairs appearing in external knowledge that have
some semantic relation. Our inference model con-
verts the output hidden vectors of BiLSTMs to
the ﬁxed-length vector with pooling operations
and puts it into the ﬁnal classiﬁer to determine
the overall inference class. Particularly, in addi-
tion to using mean pooling and max pooling sim-
ilarly to ESIM (Chen et al., 2017a), we propose
to use weighted pooling based on external knowl-
edge to obtain a ﬁxed-length vector as in Equation
(11)(12).

aw =

bw =

m
(cid:88)

i=1
n
(cid:88)

j=1

exp(H((cid:80)n
i=1 exp(H((cid:80)n
exp(H((cid:80)m
j=1 exp(H((cid:80)m

(cid:80)m

(cid:80)n

j=1 αijrij))

j=1 αijrij))

i=1 βijrji))

i=1 βijrji))

av
i ,

(11)

bv
j .

(12)

In our experiments, we regard the function H as
a 1-layer feed-forward neural network with ReLU
activation function. We concatenate all pooling
vectors, i.e., mean, max, and weighted pooling,
into the ﬁxed-length vector and then put the vector
into the ﬁnal multilayer perceptron (MLP) clas-
siﬁer. The MLP has one hidden layer with tanh
activation and softmax output layer in our exper-
iments. The entire model is trained end-to-end,
through minimizing the cross-entropy loss.

4.1 Representation of External Knowledge

Lexical Semantic Relations As described in
to incorporate external knowledge
Section 3.1,
(as a knowledge vector rij) to the state-of-the-
art neural network-based NLI models, we ﬁrst
explore semantic relations in WordNet (Miller,
1995), motivated by MacCartney (2009). Specif-
ically, the relations of lexical pairs are derived as
described in (1)-(4) below. Instead of using Jiang-
Conrath WordNet distance metric (Jiang and Con-
rath, 1997), which does not improve the perfor-
mance of our models on the development sets, we
add a new feature, i.e., co-hyponyms, which con-
sistently beneﬁt our models.

(1) Synonymy: It takes the value 1 if the words in
the pair are synonyms in WordNet (i.e., be-
long to the same synset), and 0 otherwise. For
example, [felicitous, good] = 1, [dog, wolf] =
0.

(2) Antonymy: It takes the value 1 if the words
in the pair are antonyms in WordNet, and 0
otherwise. For example, [wet, dry] = 1.

(3) Hypernymy: It takes the value 1 − n/8 if one
word is a (direct or indirect) hypernym of the
other word in WordNet, where n is the num-
ber of edges between the two words in hier-
archies, and 0 otherwise. Note that we ignore
pairs in the hierarchy which have more than 8
edges in between. For example, [dog, canid]
= 0.875, [wolf, canid] = 0.875, [dog, carni-
vore] = 0.75, [canid, dog] = 0

(4) Hyponymy: It is simply the inverse of the hy-
pernymy feature. For example, [canid, dog]
= 0.875, [dog, canid] = 0.

(5) Co-hyponyms: It takes the value 1 if the two
words have the same hypernym but they do
not belong to the same synset, and 0 other-
wise. For example, [dog, wolf] = 1.

As discussed above, we expect features like syn-
onymy, antonymy, hypernymy, hyponymy and co-
hyponyms would help model co-attention align-
ment between the premise and the hypothesis.
Knowledge of hypernymy and hyponymy may help
capture entailment; knowledge of antonymy and
co-hyponyms may help model contradiction. Their
ﬁnal contributions will be learned in end-to-end
model training. We regard the vector r ∈ Rdr as

the relation feature derived from external knowl-
edge, where dr is 5 here. In addition, Table 1 re-
ports some key statistics of these features.

Feature

#Words

#Pairs

Synonymy
Antonymy
Hypernymy
Hyponymy
Co-hyponyms

84,487
6,161
57,475
57,475
53,281

237,937
6,617
753,086
753,086
3,674,700

Table 1: Statistics of lexical relation features.

In addition to the above relations, we also use
more relation features in WordNet, including in-
stance,
instance of, same instance, entailment,
member meronym, member holonym, substance
meronym, substance holonym, part meronym, part
holonym, summing up to 15 features, but these ad-
ditional features do not bring further improvement
on the development dataset, as also discussed in
Section 5.

Relation Embeddings
In the most recent years
graph embedding has been widely employed to
learn representation for vertexes and their relations
in a graph.
In our work here, we also capture
the relation between any two words in WordNet
through relation embedding. Speciﬁcally, we em-
ployed TransE (Bordes et al., 2013), a widely used
graph embedding methods, to capture relation em-
bedding between any two words. We used two
typical approaches to obtaining the relation em-
bedding. The ﬁrst directly uses 18 relation em-
beddings pretrained on the WN18 dataset (Bordes
et al., 2013). Speciﬁcally, if a word pair has a cer-
tain type relation, we take the corresponding re-
lation embedding. Sometimes, if a word pair has
multiple relations among the 18 types; we take an
average of the relation embedding. The second ap-
proach uses TransE’s word embedding (trained on
WordNet) to obtain relation embedding, through
the objective function used in TransE, i.e., l ≈
t − h, where l indicates relation embedding, t in-
dicates tail entity embedding, and h indicates head
entity embedding.

Note that in addition to relation embedding
trained on WordNet, other relational embedding
resources exist; e.g.,
that trained on Freebase
(WikiData) (Bollacker et al., 2007), but such
knowledge resources are mainly about facts (e.g.,
relationship between Bill Gates and Microsoft)
and are less for commonsense knowledge used in

general natural language inference (e.g., the color
yellow potentially contradicts red).

4.2 NLI Datasets

In our experiments, we use Stanford Natural Lan-
guage Inference (SNLI) dataset (Bowman et al.,
2015) and Multi-Genre Natural Language Infer-
ence (MultiNLI) (Williams et al., 2017) dataset,
which focus on three basic relations between a
premise and a potential hypothesis:
the premise
entails the hypothesis (entailment), they contradict
each other (contradiction), or they are not related
(neutral). We use the same data split as in previ-
ous work (Bowman et al., 2015; Williams et al.,
2017) and classiﬁcation accuracy as the evaluation
metric. In addition, we test our models (trained on
the SNLI training set) on a new test set (Glockner
et al., 2018), which assesses the lexical inference
abilities of NLI systems and consists of 8,193 sam-
ples. WordNet 3.0 (Miller, 1995) is used to extract
semantic relation features between words. The
words are lemmatized using Stanford CoreNLP
3.7.0 (Manning et al., 2014). The premise and the
hypothesis sentences fed into the input encoding
layer are tokenized.

4.3 Training Details
For duplicability, we release our code1. All our
models were strictly selected on the development
set of the SNLI data and the in-domain devel-
opment set of MultiNLI and were then tested on
the corresponding test set. The main training de-
tails are as follows:
the dimension of the hid-
den states of LSTMs and word embeddings are
300. The word embeddings are initialized by
300D GloVe 840B (Pennington et al., 2014), and
out-of-vocabulary words among them are initial-
ized randomly. All word embeddings are updated
during training. Adam (Kingma and Ba, 2014)
is used for optimization with an initial learning
rate of 0.0004. The mini-batch size is set to 32.
Note that the above hyperparameter settings are
same as those used in the baseline ESIM (Chen
et al., 2017a) model. ESIM is a strong NLI
baseline framework with the source code made
available at https://github.com/lukecq1231/nli (the
ESIM core code has also been adapted to sum-
marization (Chen et al., 2016a) and question-
answering tasks (Zhang et al., 2017a)).

The

trade-off

λ

for

calculating

co-

1https://github.com/lukecq1231/kim

in Equation

attention
(3)
in
[0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50]
the
development set. When training TransE for
WordNet, relations are represented with vectors
of 20 dimension.

selected
on

is
based

5 Experimental Results

5.1 Overall Performance

Table 2 shows the results of state-of-the-art models
on the SNLI dataset. Among them, ESIM (Chen
et al., 2017a) is one of the previous state-of-the-art
systems with an 88.0% test-set accuracy. The pro-
posed model, namely Knowledge-based Inference
Model (KIM), which enriches ESIM with external
knowledge, obtains an accuracy of 88.6%, the best
single-model performance reported on the SNLI
dataset. The difference between ESIM and KIM is
statistically signiﬁcant under the one-tailed paired
t-test at the 99% signiﬁcance level. Note that the
KIM model reported here uses ﬁve semantic rela-
tions described in Section 4. In addition to that, we
also use 15 semantic relation features, which does
not bring additional gains in performance. These
results highlight the effectiveness of the ﬁve se-
mantic relations described in Section 4. To further
investigate external knowledge, we add TransE re-
lation embedding, and again no further improve-
ment is observed on both the development and test
sets when TransE relation embedding is used (con-
catenated) with the semantic relation vectors. We
consider this is due to the fact that TransE embed-
ding is not speciﬁcally sensitive to inference in-
formation; e.g., it does not model co-hyponyms
features, and its potential beneﬁt has already been
covered by the semantic relation features used.

Table 3 shows the performance of models on the
MultiNLI dataset. The baseline ESIM achieves
76.8% and 75.8% on in-domain and cross-domain
test set, respectively. If we extend the ESIM with
external knowledge, we achieve signiﬁcant gains
to 77.2% and 76.4% respectively. Again, the gains
are consistent on SNLI and MultiNLI, and we ex-
pect they would be orthogonal to other factors
when external knowledge is added into other state-
of-the-art models.

5.2 Ablation Results

Figure 2 displays the ablation analysis of differ-
ent components when using the external knowl-
edge. To compare the effects of external knowl-
edge under different training data scales, we ran-

Model

LSTM Att. (Rockt¨aschel et al., 2015)
DF-LSTMs (Liu et al., 2016a)
TC-LSTMs (Liu et al., 2016b)
Match-LSTM (Wang and Jiang, 2016)
LSTMN (Cheng et al., 2016)
Decomposable Att. (Parikh et al., 2016)
NTI (Yu and Munkhdalai, 2017b)
Re-read LSTM (Sha et al., 2016)
BiMPM (Wang et al., 2017)
DIIN (Gong et al., 2017)
BCN + CoVe (McCann et al., 2017)
CAFE (Tay et al., 2018)

ESIM (Chen et al., 2017a)
KIM (This paper)

Test

83.5
84.6
85.1
86.1
86.3
86.8
87.3
87.5
87.5
88.0
88.1
88.5

88.0
88.6

Table 2: Accuracies of models on SNLI.

Model

In Cross

CBOW (Williams et al., 2017)
BiLSTM (Williams et al., 2017)
DiSAN (Shen et al., 2017)
Gated BiLSTM (Chen et al., 2017b)
SS BiLSTM (Nie and Bansal, 2017)
DIIN * (Gong et al., 2017)
CAFE (Tay et al., 2018)

ESIM (Chen et al., 2017a)
KIM (This paper)

64.8
66.9
71.0
73.5
74.6
77.8
78.7

76.8
77.2

64.5
66.9
71.4
73.6
73.6
78.8
77.9

75.8
76.4

Table 3: Accuracies of models on MultiNLI. * in-
dicates models using extra SNLI training set.

domly sample different ratios of the entire training
set, i.e., 0.8%, 4%, 20% and 100%. “A” indicates
adding external knowledge in calculating the co-
attention matrix as in Equation (3), “I” indicates
adding external knowledge in collecting local in-
ference information as in Equation (7)(8), and “C”
indicates adding external knowledge in compos-
ing inference as in Equation (11)(12). When we
only have restricted training data, i.e., 0.8% train-
ing set (about 4,000 samples), the baseline ESIM
has a poor accuracy of 62.4%. When we only
add external knowledge in calculating co-attention
(“A”), the accuracy increases to 66.6% (+ absolute
4.2%). When we only utilize external knowledge
in collecting local inference information (“I”), the
accuracy has a signiﬁcant gain, to 70.3% (+ ab-
solute 7.9%). When we only add external knowl-
edge in inference composition (“C”), the accuracy
gets a smaller gain to 63.4% (+ absolute 1.0%).
The comparison indicates that “I” plays the most
important role among the three components in us-
ing external knowledge. Moreover, when we com-

pose the three components (“A,I,C”), we obtain
the best result of 72.6% (+ absolute 10.2%). When
we use more training data, i.e., 4%, 20%, 100%
of the training set, only “I” achieves a signiﬁcant
gain, but “A” or “C” does not bring any signiﬁ-
cant improvement. The results indicate that ex-
ternal semantic knowledge only helps co-attention
and composition when limited training data is lim-
ited, but always helps in collecting local inference
information. Meanwhile, for less training data, λ
is usually set to a larger value. For example, the
optimal λ on the development set is 20 for 0.8%
training set, 2 for the 4% training set, 1 for the
20% training set and 0.2 for the 100% training set.
Figure 3 displays the results of using different
ratios of external knowledge (randomly keep dif-
ferent percentages of whole lexical semantic rela-
tions) under different sizes of training data. Note
that here we only use external knowledge in col-
lecting local inference information as it always
works well for different scale of the training set.
Better accuracies are achieved when using more
external knowledge. Especially under the condi-
tion of restricted training data (0.8%), the model
obtains a large gain when using more than half of
external knowledge.

Figure 2: Accuracies of models of incorporat-
ing external knowledge into different NLI compo-
nents, under different sizes of training data (0.8%,
4%, 20%, and the entire training data).

5.3 Analysis on the (Glockner et al., 2018)

Test Set

In addition, Table 4 shows the results on a newly
published test set (Glockner et al., 2018). Com-
pared with the performance on the SNLI test

Figure 3: Accuracies of models under differ-
ent sizes of external knowledge. More external
knowledge corresponds to higher accuracies.

Model

SNLI Glockner’s(∆)

(Parikh et al., 2016)*
(Nie and Bansal, 2017)*
ESIM *
KIM (This paper)

84.7
86.0
87.9
88.6

51.9 (-32.8)
62.2 (-23.8)
65.6 (-22.3)
83.5 ( -5.1)

Table 4: Accuracies of models on the SNLI and
(Glockner et al., 2018) test set. * indicates the re-
sults taken from (Glockner et al., 2018).

set, the performance of the three baseline mod-
els dropped substantially on the (Glockner et al.,
2018) test set, with the differences ranging from
22.3% to 32.8% in accuracy. Instead, the proposed
KIM achieves 83.5% on this test set (with only a
5.1% drop in performance), which demonstrates
its better ability of utilizing lexical level inference
and hence better generalizability.

Figure 5 displays the accuracy of ESIM
and KIM in each replacement-word category of
the (Glockner et al., 2018) test set. KIM outper-
forms ESIM in 13 out of 14 categories, and only
performs worse on synonyms.

5.4 Analysis by Inference Categories

We perform more analysis (Table 6) using the sup-
plementary annotations provided by the MultiNLI
dataset (Williams et al., 2017), which have 495
samples (about 1/20 of the entire development set)
for both in-domain and out-domain set. We com-
pare against the model outputs of the ESIM model
across 13 categories of inference. Table 6 reports
the results. We can see that KIM outperforms
ESIM on overall accuracies on both in-domain and

Category

Instance ESIM KIM

P/G Sentences

Antonyms
Cardinals
Nationalities
Drinks
Antonyms WordNet
Colors
Ordinals
Countries
Rooms
Materials
Vegetables
Instruments
Planets
Synonyms

Overall

1,147
759
755
731
706
699
663
613
595
397
109
65
60
894

8,193

70.4
75.5
35.9
63.7
74.6
96.1
21.0
25.4
69.4
89.7
31.2
90.8
3.3
99.7

65.6

86.5
93.4
73.5
96.6
78.8
98.3
56.6
70.8
77.6
98.7
79.8
96.9
5.0
92.1

83.5

Table 5: The number of instances and accu-
racy per category achieved by ESIM and KIM on
the (Glockner et al., 2018) test set.

Category

In-domain Cross-domain
ESIM KIM ESIM KIM

Active/Passive
Antonym
Belief
Conditional
Coreference
Long sentence
Modal
Negation
Paraphrase
Quantity/Time
Quantiﬁer
Tense
Word overlap

Overall

93.3
76.5
72.7
65.2
80.0
82.8
80.6
76.7
84.0
66.7
79.2
74.5
89.3

77.1

93.3
76.5
75.8
65.2
76.7
78.8
79.9
79.8
72.0
66.7
78.4
78.4
85.7

77.9

100.0
70.0
75.9
61.5
75.9
69.7
77.0
73.1
86.5
56.4
73.6
72.2
83.8

76.7

100.0
75.0
79.3
69.2
75.9
73.4
80.2
71.2
89.2
59.0
77.1
66.7
81.1

77.4

Table 6: Detailed Analysis on MultiNLI.

cross-domain subset of development set. KIM out-
performs or equals ESIM in 10 out of 13 cate-
gories on the cross-domain setting, while only 7
out of 13 categories on in-domain setting. It indi-
cates that external knowledge helps more in cross-
domain setting. Especially, for antonym category
in cross-domain set, KIM outperform ESIM sig-
niﬁcantly (+ absolute 5.0%) as expected, because
antonym feature captured by external knowledge
would help unseen cross-domain samples.

5.5 Case Study

Table 7 includes some examples from the SNLI
test set, where KIM successfully predicts the in-
ference relation and ESIM fails. In the ﬁrst exam-

e/c

e/c

c/e

c/e

p: An African person standing in a wheat
ﬁeld.
h: A person standing in a corn ﬁeld.

p: Little girl is ﬂipping an omelet in the
kitchen.
h: A young girl cooks pancakes.

p: A middle eastern marketplace.
h: A middle easten store.

p: Two boys are swimming with boogie
boards.
h: Two boys are swimming with their ﬂoats.

Table 7: Examples. Word in bold are key words
in making ﬁnal prediction. P indicates a predicted
label and G indicates gold-standard label. e and c
denote entailment and contradiction, respectively.

ple, the premise is “An African person standing in
a wheat ﬁeld” and the hypothesis “A person stand-
ing in a corn ﬁeld”. As the KIM model knows that
“wheat” and “corn” are both a kind of cereal, i.e,
the co-hyponyms relationship in our relation fea-
tures, KIM therefore predicts the premise contra-
dicts the hypothesis. However, the baseline ESIM
cannot learn the relationship between “wheat” and
“corn” effectively due to lack of enough samples
in the training sets. With the help of external
knowledge, i.e., “wheat” and “corn” having the
same hypernym “cereal”, KIM predicts contradic-
tion correctly.

6 Conclusions

Our neural-network-based model for natural lan-
guage inference with external knowledge, namely
KIM, achieves the state-of-the-art accuracies. The
model is equipped with external knowledge in its
main components, speciﬁcally, in calculating co-
attention, collecting local inference, and compos-
ing inference. We provide detailed analyses on our
model and results. The proposed model of infus-
ing neural networks with external knowledge may
also help shed some light on tasks other than NLI.

Acknowledgments

We thank Yibo Sun and Bing Qin for early helpful
discussion.

References

Sungjin Ahn, Heeyoul Choi, Tanel P¨arnamaa, and
Yoshua Bengio. 2016. A neural knowledge lan-
guage model. CoRR, abs/1608.00318.

Kurt D. Bollacker, Robert P. Cook, and Patrick Tufts.
2007. Freebase: A shared database of structured
In Proceedings of the
general human knowledge.
Twenty-Second AAAI Conference on Artiﬁcial In-
telligence, July 22-26, 2007, Vancouver, British
Columbia, Canada, pages 1962–1963.

Antoine Bordes, Nicolas Usunier, Alberto Garc´ıa-
Dur´an, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States., pages 2787–
2795.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
In Proceedings of the 2015 Conference on
ence.
Empirical Methods in Natural Language Process-
ing, EMNLP 2015, Lisbon, Portugal, September 17-
21, 2015, pages 632–642.

Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. 2016. A fast uniﬁed model for
parsing and sentence understanding. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics, ACL 2016, August 7-12,
2016, Berlin, Germany, Volume 1: Long Papers.

Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard
S¨ackinger, and Roopak Shah. 1993. Signature veri-
ﬁcation using a siamese time delay neural network.
In Advances in Neural Information Processing Sys-
tems 6, [7th NIPS Conference, Denver, Colorado,
USA, 1993], pages 737–744.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
and Hui Jiang. 2016a. Distraction-based neural net-
In Proceedings of
works for modeling document.
the Twenty-Fifth International Joint Conference on
Artiﬁcial Intelligence, IJCAI 2016, New York, NY,
USA, 9-15 July 2016, pages 2754–2760.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
Hui Jiang, and Diana Inkpen. 2017a. Enhanced
LSTM for natural language inference. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics, ACL 2017, Vancou-
ver, Canada, July 30 - August 4, Volume 1: Long
Papers, pages 1657–1668.

Representations for NLP, RepEval@EMNLP 2017,
Copenhagen, Denmark, September 8, 2017, pages
36–40.

Yun-Nung Chen, Dilek Z. Hakkani-T¨ur, G¨okhan T¨ur,
Asli C¸ elikyilmaz, Jianfeng Gao, and Li Deng.
2016b. Knowledge as a teacher: Knowledge-
CoRR,
guided structural attention networks.
abs/1609.03286.

Zhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen,
Si Wei, Hui Jiang, and Xiaodan Zhu. 2015. Re-
visiting word embedding for contrasting meaning.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, ACL 2015, July 26-31, 2015,
Beijing, China, Volume 1: Long Papers, pages 106–
115.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2016, Austin, Texas, USA, November
1-4, 2016, pages 551–561.

Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2017.
Unsupervised learning of task-speciﬁc tree struc-
tures with tree-lstms. CoRR, abs/1707.02786.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2017, Copen-
hagen, Denmark, September 9-11, 2017, pages 670–
680.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, Eval-
uating Predictive Uncertainty, Visual Object Clas-
siﬁcation and Recognizing Textual Entailment, First
PASCAL Machine Learning Challenges Workshop,
MLCW 2005, Southampton, UK, April 11-13, 2005,
Revised Selected Papers, pages 177–190.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard H. Hovy, and Noah A. Smith.
2015. Retroﬁtting word vectors to semantic lexi-
cons. In NAACL HLT 2015, The 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Denver, Colorado, USA, May 31 - June 5,
2015, pages 1606–1615.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017b. Recurrent neural
network-based sentence encoder with gated atten-
tion for natural language inference. In Proceedings
of the 2nd Workshop on Evaluating Vector Space

Max Glockner, Vered Shwartz, and Yoav Goldberg.
2018. Breaking nli systems with sentences that re-
quire simple lexical inferences. In The 56th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Melbourne, Australia.

Yichen Gong, Heng Luo, and Jian Zhang. 2017.
Natural language inference over interaction space.
CoRR, abs/1709.04348.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Adrian Iftene and Alexandra Balahur-Dobrescu. 2007.
Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, chapter Hypoth-
esis Transformation and Semantic Variability Rules
Used in Recognizing Textual Entailment. Associa-
tion for Computational Linguistics.

Jay J. Jiang and David W. Conrath. 1997. Seman-
tic similarity based on corpus statistics and lexical
In Proceedings of the 10th Research
taxonomy.
on Computational Linguistics International Confer-
ence, ROCLING 1997, Taipei, Taiwan, August 1997,
pages 19–33.

Valentin Jijkoun and Maarten de Rijke. 2005. Recog-
nizing textual entailment using lexical similarity. In
Proceedings of the PASCAL Challenges Workshop
on Recognising Textual Entailment.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Zhouhan Lin, Minwei Feng, C´ıcero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. CoRR, abs/1703.03130.

Pengfei Liu, Xipeng Qiu, Jifan Chen, and Xuanjing
Huang. 2016a. Deep fusion lstms for text seman-
In Proceedings of the 54th Annual
tic matching.
Meeting of the Association for Computational Lin-
guistics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers.

Pengfei Liu, Xipeng Qiu, Yaqian Zhou, Jifan Chen, and
Xuanjing Huang. 2016b. Modelling interaction of
sentence pair with coupled-lstms. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2016, Austin,
Texas, USA, November 1-4, 2016, pages 1703–1712.

Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling, and
Yu Hu. 2015. Learning semantic word embeddings
In Pro-
based on ordinal knowledge constraints.
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, ACL 2015, July 26-31, 2015, Bei-
jing, China, Volume 1: Long Papers, pages 1501–
1511.

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016c. Learning natural language inference us-
ing bidirectional LSTM model and inner-attention.
CoRR, abs/1605.09090.

Bill MacCartney. 2009. Natural Language Inference.

Ph.D. thesis, Stanford University.

Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2008, Proceedings of the Confer-
ence, 25-27 October 2008, Honolulu, Hawaii, USA,
A meeting of SIGDAT, a Special Interest Group of
the ACL, pages 802–811.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural language
processing toolkit. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, ACL 2014, June 22-27, 2014, Baltimore,
MD, USA, System Demonstrations, pages 55–60.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
In Advances in Neural
textualized word vectors.
Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems
2017, 4-9 December 2017, Long Beach, CA, USA,
pages 6297–6308.

George A. Miller. 1995. Wordnet: A lexical database

for english. Commun. ACM, 38(11):39–41.

Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui
Yan, and Zhi Jin. 2016. Natural language infer-
ence by tree-based convolution and heuristic match-
ing. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics, ACL
2016, August 7-12, 2016, Berlin, Germany, Volume
2: Short Papers.

Nikola Mrksic, Ivan Vulic, Diarmuid ´O S´eaghdha, Ira
Leviant, Roi Reichart, Milica Gasic, Anna Korho-
nen, and Steve J. Young. 2017. Semantic special-
isation of distributional word vector spaces using
monolingual and cross-lingual constraints. CoRR,
abs/1706.00374.

Yixin Nie and Mohit Bansal. 2017.

Shortcut-
stacked sentence encoders for multi-domain infer-
In Proceedings of the 2nd Workshop on
ence.
Evaluating Vector Space Representations for NLP,
RepEval@EMNLP 2017, Copenhagen, Denmark,
September 8, 2017, pages 41–45.

Ankur P. Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pages
2249–2255.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
In Proceedings of the 2014
word representation.
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,

Hong Yu and Tsendsuren Munkhdalai. 2017a. Neural
semantic encoders. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL 2017, Valen-
cia, Spain, April 3-7, 2017, Volume 1: Long Papers,
pages 397–407.

Hong Yu and Tsendsuren Munkhdalai. 2017b. Neu-
ral tree indexers for text understanding. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL 2017, Valencia, Spain, April 3-7, 2017, Vol-
ume 1: Long Papers, pages 11–21.

Junbei Zhang, Xiaodan Zhu, Qian Chen, Lirong
Ex-
Dai, Si Wei,
ploring question understanding and adaptation in
neural-network-based question answering. CoRR,
abs/arXiv:1703.04617v2.

Jiang. 2017a.

and Hui

Shiyue Zhang, Gulnigar Mahmut, Dong Wang, and
Askar Hamdulla. 2017b.
Memory-augmented
chinese-uyghur neural machine translation. In 2017
Asia-Paciﬁc Signal and Information Processing As-
sociation Annual Summit and Conference, APSIPA
ASC 2017, Kuala Lumpur, Malaysia, December 12-
15, 2017, pages 1092–1096.

2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1532–1543.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz
Hermann, Tom´as Kocisk´y, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
CoRR, abs/1509.06664.

Lei Sha, Baobao Chang, Zhifang Sui, and Sujian Li.
2016. Reading and thinking: Re-read LSTM unit
In COLING
for textual entailment recognition.
2016, 26th International Conference on Computa-
tional Linguistics, Proceedings of the Conference:
Technical Papers, December 11-16, 2016, Osaka,
Japan, pages 2870–2879.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2017. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding. CoRR, abs/1709.04696.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen
Wang, and Chengqi Zhang. 2018. Reinforced self-
attention network: a hybrid of hard and soft attention
for sequence modeling. CoRR, abs/1801.10296.

Chen Shi, Shujie Liu, Shuo Ren, Shi Feng, Mu Li,
Ming Zhou, Xu Sun, and Houfeng Wang. 2016.
Knowledge-based semantic embedding for machine
translation. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. A
compare-propagate architecture with alignment fac-
torization for natural language inference. CoRR,
abs/1801.00102.

Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel
Urtasun. 2015. Order-embeddings of images and
language. CoRR, abs/1511.06361.

Shuohang Wang and Jing Jiang. 2016. Learning natu-
ral language inference with LSTM. In NAACL HLT
2016, The 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, San Diego
California, USA, June 12-17, 2016, pages 1442–
1451.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences. In Proceedings of the Twenty-Sixth
International Joint Conference on Artiﬁcial Intelli-
gence, IJCAI 2017, Melbourne, Australia, August
19-25, 2017, pages 4144–4150.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2015. From paraphrase database to compo-
sitional paraphrase model and back. TACL, 3:345–
358.

Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. CoRR,
abs/1704.05426.

