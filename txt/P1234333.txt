Covariance in Physics and Convolutional Neural Networks

9
1
0
2
 
n
u
J
 
6
 
 
]

G
L
.
s
c
[
 
 
1
v
1
8
4
2
0
.
6
0
9
1
:
v
i
X
r
a

Miranda C. N. Cheng 1 2 3 Vassilis Anagiannis 2 Maurice Weiler 4 Pim de Haan 5 4 Taco S. Cohen 5
Max Welling 5

Abstract
In this proceeding we give an overview of the
idea of covariance (or equivariance) featured in
the recent development of convolutional neural
networks (CNNs). We study the similarities and
differences between the use of covariance in theo-
retical physics and in the CNN context. Addition-
ally, we demonstrate that the simple assumption
of covariance, together with the required proper-
ties of locality, linearity and weight sharing, is
sufﬁcient to uniquely determine the form of the
convolution.

1. Covariance and Uniqueness

It is well-known that the principle of covariance, or coordi-
nate independence, lies at the heart of the theory of relativ-
ity. The theory of special relativity was constructed to de-
scribe Maxwell’s theory of electromagnetism in a way that
satisﬁes the special principle of covariance, which states
“If a system of coordinates K is chosen so that, in relation
to it, physical laws hold good in their simplest form, the
same laws hold good in relation to any other system of co-
ordinates K ′ moving in uniform translation relatively to K”
(Einstein, 1916).

The transformation between K and K ′, in other words
between different inertial frames, can always be achieved
through an element of the (global) Lorentz group. With the
beneﬁt of hindsight, there is no good reason why physics
should only be covariant under a global change of coordi-
nates. Indeed, soon after the development of special rela-
tivity, Einstein started to develop his ideas for a theory that
is covariant with respect to a local, spacetime-dependent,

*Equal contribution 1Korteweg-de Vries Institute for Math-
ematics, University of Amsterdam, Amsterdam,
the Nether-
lands 2Institute of Physics, University of Amsterdam, Am-
the Netherlands 3On leave from CNRS, France
sterdam,
4Qualcomm-University of Amsterdam (QUVA) Lab, Amster-
the Netherlands 5Qualcomm AI Research, Amsterdam,
dam,
the Netherlands.
Correspondence to: Vassilis Anagiannis
<V.Anagiannis@uva.nl>.

Presented at the ICML 2019 Workshop on Theoretical Physics for
Deep Learning. Copyright 2019 by the author(s).

change of coordinates. In his words, the general principle
of covariance states that “The general laws of nature are
to be expressed by equations which hold good for all sys-
tems of coordinates, that is, are covariant with respect to
any substitutions whatever (generally covariant)” (Einstein,
1916). The rest is history: the incorporation of the math-
ematics of Riemannian geometry in order to achieve gen-
eral covariance and the formulation of the general relativity
(GR) theory of gravity. It is important to note that the seem-
ingly innocent assumption of general covariance is in fact
so powerful that it determines GR as the unique theory of
gravity compatible with this principle, and the equivalence
principle in particular, up to short-distance corrections1.

In a completely different context,
it has become clear
in recent years that a coordinate-independent description
is also desirable for convolutional networks. A covari-
ant inference process is particularly useful in situations
where the distribution of characteristic patterns is sym-
metric. Important practical examples include satellite im-
agery or biomedical microscopy imagery which often do
not exhibit a preferred global rotation or chirality. In or-
der to ensure that the inferred information of a network
is equivalent for transformed samples, the network archi-
tecture has to be designed to be equivariant2 under the
corresponding group action3. A wide range of equivari-
ant models has been proposed for signals on ﬂat Euclidean
spaces Rd.
(subgroups
In particular, equivariance w.r.t.
of) the Euclidean groups E(d) of translations, rotations
and mirrorings of Rd has been investigated for planar im-
ages (d = 2) (Cohen & Welling, 2016; 2017; Worrall et al.,
2017; Weiler et al., 2018b; Hoogeboom et al., 2018). and
volumetric signals (d = 3) (Winkels & Cohen, 2018;

1The uniqueness argument roughly goes as follows. In order
to achieve full general covariance, the only ingredients for the
gravitational part of the action are the Riemann tensor and its
derivatives, with all the indices contracted. From a simple scal-
ing argument one can show that all of them apart from the Ricci
scalar have subleading effects at large distances. Note that this
universality makes no assumptions on the matter ﬁelds that may
be coupled to gravity.

2In this article we will use the words “equivariant” and “co-

variant” interchangeably as they convey the same concept.

3 To deﬁne the equivariance of a map requires the deﬁnition of
a group action on the domain and codomain. One speciﬁc choice
of group action is the co/contravariant transformation of tensors.

Covariance in Physics and Convolutional Neural Networks

Worrall & Brostow, 2018; Weiler et al., 2018a) and has
generally been found to outperform non-equivariant mod-
els in accuracy and data efﬁciency. Equivariance has fur-
ther proven to be a powerful principle in generalizing con-
volutional networks to more general spaces like the sphere
In general, it has been shown in
(Cohen et al., 2018b).
(Kondor & Trivedi, 2018; Cohen et al., 2018c;a) that (glob-
ally) H-equivariant networks can be generalized to arbi-
trary homogeneous spaces H/G where G
H is a sub-
group4. The feature spaces of such networks are formal-
ized as spaces of sections of vector bundles over H/G, as-
sociated to the principal bundle H
H/G. Our previous
examples are in this setting interpreted as E(d)-equivariant
networks on Euclidean space Rd = E(d)/ O(d) and SO(3)-
equivariant networks on the sphere S2 = SO(3)/ SO(2).
This description includes Poincar´e-equivariant networks on
Minkowski spacetime since Minkowski space R1,3 arises
as quotient of the Poincar´e group R1,3 ⋊ O(1, 3) w.r.t. the
Lorentz group O(1, 3).

→

≤

Note that the change of coordinates required here is a
global one. Global symmetries are extremely natural and
readily applicable when the underlying space is homoge-
neous, i.e. the group action is transitive, meaning the space
contains only a single orbit. At the same time, it is clearly
desirable to have an effective CNN on an arbitrary surface,
often not equipped with a global symmetry. If the previ-
ous work on homogeneous spaces is based on an equiv-
ariance requirement analogous to the special principle of
covariance, then what one needs for general surfaces is an
analogue of the general principle of covariance. In other
words, we would like to have covariance with respect to a
local, location-dependent coordinate transformations.

→

This requirement for local transformation equivariance of
convolutional networks on general manifolds has been rec-
ognized and described in (Cohen et al., 2019). A choice
of local coordinates is thereby formalized as a gauge wx :
Rd
TxM of the tangent space5. Similar to the general
theory of equivariant networks on homogeneous spaces,
the feature ﬁelds of these networks are realized as sec-
tions of vector bundles over M , this time associated to the
frame bundle F M of the manifold. Local transformations
are described as position-dependent gauge transformations
GL(Rd) is an element
wx
G
of the structure group. When the frame bundle is chosen

wxgx, where gx

7→

≤

∈

4Note that we are using an inverted deﬁnition of H and G w.r.t.
the original paper to stay compatible with the convention used in
(Cohen et al., 2019), discussed below.

5 The gauge is equivalent to choosing a basis {˜ei}d

i=1 :=
{wx(ei)}d
i=1 of the tangent space by mapping the standard ba-
i=1 of Rd to TxM . Explicitly, a coefﬁcient vector
sis {ei}d
(v1, . . . , vd) determines a vector wx((v1, . . . , vd)) = Pi vi˜ei.
Coordinate bases and frame (vielbein) bases are examples of
choices of the gauge.

to be the orthonormal frame bundle, the structure group
is reduced to O(d) and in our analogy this corresponds to
the vierbein formulation of GR where the group G is the
Lorentz group O(1, 3).

Note that the parallel between the two problems regarding
general covariance forces us to employ the same mathemat-
ical language of (psuedo) Riemannian geometry. Interest-
ingly, we will argue in the next section that our formalism is
basically unique once general covariance along with some
basic assumptions is demanded. This can be compared with
the long-distance uniqueness of GR, once covariance is re-
quired.

2. The Covariant Convolution

In CNNs we are interested in devising a linear map between
the input feature space and the output space between every
pair of subsequent layers in the convolutional network. In
this section we will argue that the four properties of 1) lin-
earity, 2) locality, 3) covariance, and 4) weight sharing is
sufﬁcient to uniquely determine its form, which we give in
(4) and (5) below.

∈

→

In mathematical terms, we describe the feature space in
the i-th layer in terms of a ﬁber bundle Ei with ﬁber Fi ,
that is associated to the principal bundle P with the repre-
sentation ρ of the structure group G, with the projection
M . The bundle structure captures the transfor-
π : Ei
mation properties of the feature ﬁelds under a change of
coordinates of the manifold M . For now we focus on a
sub-region U of the surface which admits a single coordi-
nate chart and a local trivialisation of the bundles. In this
language, the feature ﬁeld corresponds to a local section6
Γi := Γ(Ei , U ) of the ﬁber bundle and the linear map
fi
Hom(Γin , Γout ).
is between the space of sections: m
Moreover, we require the linear map to satisfy the follow-
ing locality condition: given the distance function
on
the manifold M , which in our case will be supplied by
˜f )(x) for all
the metric, we have (m
Γin with the property that f (y) = ˜f (y) for all y
f, ˜f
with
< R for some (ﬁxed) positive number R. The
linearity and the locality of the map immediately leads to
the following form of the map. To illustrate this, consider
the simpliﬁed scenario when the in- and output features are
just numbers (scalars) which do not transform under coordi-
nate transformation and M is replaced with a set
with ﬁ-
nite elements equipped with the distance function, then the
above requirements immediately leads to the matrix form
y∈S,ky,xk<R cx,yf (y). Simi-
of the map (m

∈
y, x
k
k

f )(x) = (m

f )(x) =

k k

∈

S

◦

◦

◦

P

6Without the risk of causing confusion we will sometimes
consider fi as a map from the local region U to the ﬁber Fi,
where implicitly we have used the local trivialisation to write
fi(x) = (x, α) where α ∈ Fi and ignored the ﬁrst entry.

Covariance in Physics and Convolutional Neural Networks

larly, for our case we are led to the linear map

fout (x) = (m

fin )(x) =

k(x, y) fin (y) ddy (1)

◦

Zbx,R

where d = dimM , bx,R is the ball centered at x with radius
R, and k : M
Hom(Fin , Fout ) is what will turn
×
out to be the convolution kernel.

→

M

In the next step we will impose the condition of general
covariance to restrict the form of k(x, y). In the case of
homogeneous spaces and when we require just special co-
variance, we can phrase the problem in the following gen-
eral form. Suppose that the input feature and the output
feature form representations ρin and ρout under a group
G, then it is clear from the consistency of the G-action
with the above map that k must transform as g : k
7→
ρout (g)kρin (g−1) and this is precisely what is described in
(Cohen et al., 2018a; 2019). Once we promote the group el-
ement to be location-dependent, the analogous requirement
ρout (gx)k(x, y)ρin ((gy)−1). In our case, the
is k(x, y)
group under discussion is that of local changes of coordi-
nates7, with the consistent corresponding change of metric
ds2 = gµν (x)dxµdxν = g′
µν(x′)(dx′)µ(dx′)ν. Note that
this is not just a mathematical formality: one needs to deal
with changes of coordinates when working with manifolds
that cannot be covered with one coordinate chart, such as a
sphere.

7→

However, it is unwieldy to work with group elements at
different points x and y.
Instead, we would like to en-
code the information in another way so that we can work
with gauge/coordinate transformations at one single point
when talking about the transformation of k. Here the rel-
evant concept is parallel transport. Given a bundle with
connection (E,
M
→
[0, 1] and
with γ(0) = y and γ(1) = x, for every t0 ∈
s0 ∈
M
⊂
In coordi-
that is ﬂat along γ such that s(γ(t0)) = s0.
nates, this means dX µ
I. Note
µs(γ(t)) = 0 for all t
that the parallel transport is generically path-dependent; in
other words, transporting y to x along different paths yields
different results unless the bundle is ﬂat.

Eγ(t0) there is a unique section s along γ(I)

) on M and path γ : I = [0, 1]

dt ∇

∇

∈

However, in our application we always have a uniquely dis-
tinguished path between y and x in practice. Namely, in the
CNN context we let the ball Bx,R containing the support of
k to be so small that every point in the ball is uniquely con-
nected by a single geodesic to the center x. We hence re-
place fin (y) with fin
y (x), the parallel transport of fin (y)
along the unique geodesic from y to the center point x. De-

|

7In this proceeding we mainly work with the basis {∂µ} of the
tangent space, while another common choice is the orthonormal
(vielbein) basis. The former has the advantage of being directly
related to the covariance in the context of GR and the latter has
the advantage of being closer to the philosophy of gauge theory.

note the corresponding new kernel by k′(x, y), and we ar-
rive at the transformation property

k′(x, y)

ρout (gx)k′(x, y)ρin ((gx)−1).

(2)

7→

∈

In fact, this geodesic description of the points provides
us with an alternative, convenient way to parametrise the
points we integrate over. Let v
TxM be a vector in
the tangent space at x. There is a unique geodesic ﬂow
M starting from γ(0) = x where
γv : I = [0, 1]
the initial velocity is v, i.e. dX µ(γv(t))dt
t=0= vµ. We
will denote the endpoint of this ﬂow expxv := γv(1). We
can hence trade the integration within a small ball in our
manifold with an integration within a ball of some radius
TxM, vµvν gµν(x) <
r in its tangent space Bx :=
r2
Hom(Fin , Fout ).

v
{
TxM . We can hence write the kernel as k′′(x, v)

} ⊂

→

∈

∈

|

Apart from accommodating the transformation of the in-
and out- feature ﬁelds, one also have to make sure that
the integration measure remains invariant. From here we
conclude that should contain the factor of the volume
ddv, and we can hence write the kernel as
form
g(x)
|
k′′(x, v). Note also that this volume factor √g is
p|
simply 1 if one works with gauge wx that corresponds to
an orthonormal basis.

p|
g(x)
|

At this stage, we arrive at the following form of our linear
map

≥

⊗

g(x)
|

fout (x) =

ZBx p|

k′′(x, v) fin

T ∗M ⊗m for n, m

|expxv (x) ddv (3)
From this stage onwards, we would like to be less abstract
and focus on the groups and representations we encounter
in real problems. Namely, when Ei is the tensor product of
the tangent and the cotangent bundles and takes the form
T M ⊗n
0. To see that this is suf-
ﬁcient and to make contact with previous work, note for
instance that any irreducible representations of SO(3), de-
noted by j, the spin j irreducible representation with dimen-
sions 2j+1, can be expressed as an Z-linear combination of
the tensor product of the vector representation V . Specif-
ically, we have V ⊗n = n
. . . , where . . . denotes the
direct sum of non-negative copies of m with 0
m < n,
Z≥0. In other words, V ⊗n contains precisely
for all n
one copy of spin-n irreducible representations as well as
other irreducibles with lower spins. This ensures that every
j is in the Z-span of 1, V, V ⊗2, . . . , V ⊗j. In other words,
j, j = 0, 1, 2, . . .
are
as well as
{
equally good Z-bases for SO(3) representations.

V ⊗n, n = 0, 1, 2, . . .

⊕

≤

∈

}

{

}

To ease notation, we will assume that the input and out-
put feature ﬁelds are sections of tensor products of tangent
bundles, while the cases involving the cotangent bundles
can be treated with a straightforward generalisation of our
In this case, we can write explicit expressions
formula.

Covariance in Physics and Convolutional Neural Networks

out

for the output feature ﬁeld as f µ1µ2...µNo
and similarly
for the input, and the transformation property (2) is suc-
cinctly summarised by the tensor and index structure of
µ1µ2...µNo
the kernel function, which we write as K
(x, v).
ν1...νNi
In other words for a ﬁxed x and v
TxM , we have
x M )⊗Ni. Explicitly, we now
K(x, v)
∈
have for these cases

(TxM )⊗No

(T ∗

⊗

∈

µ1µ2...µNo
out

f

(x) =

ZBx p|

g(x)
|

K

µ1µ2...µNo
ν1...νNi

(x, v) f

ν1...νNi
in

(4)
|expxv (x) ddv

→

Finally, we would like to impose the weight sharing condi-
tion, which we phrase in the following way: when the (lo-
calised) input signal is parallel transported along a curve,
the output signal should also equal to the parallel transport
of the previous result. First, we need to explain what we
mean by parallel transporting the input feature ﬁeld along
M with ˜γ(0) = x and ˜γ(1) = x′.
a curve ˜γ : [0, 1]
For the point x itself it is clear that we can simply paral-
lel transport fin
|˜γ(t) (˜γ(t)). Suppose that
y = expxv is connected by the geodesic ﬂow γv starting
from x, we also transport v
TxM to v(˜γ(t))
T˜γ(t)M
∈
and transport fin
y (x) by transporting it to fin
y (˜γ(t))
|
and then further transport it along the geodesic ﬂow γv(˜γ(t))
starting from ˜γ(t). After this prescription, depicted in Fig-
ure 1, it is clear how one should deﬁne K(x′, v) such that
the weight sharing condition is true. Recall that for a given
x M )⊗Ni.
v
Now parallel transport it along ˜γ to obtain Kv(˜γ(t)), and
deﬁne that

TxM , Kv(x) := K(x, v)

(TxM )⊗No

x (x) to fin

(T ∗

⊗

∈

∈

∈

|

|

y

f i n ( y )

fin

γv
|y (x)

v

x

fin

|y (y′)

fin

γv(x′)
|y(x′
)

y′

v(x′)

γ

e

x′

Figure1. Parallel transport of the feature ﬁeld.

see that our simple and general assumptions in fact com-
pletely determine the form of the convolution map.

3. Discussion

After pointing out the parallel between special and gen-
eral relativity and equivariant CNNs, it is also important to
point out the crucial differences. In the CNN setup, the ge-
ometry is always held ﬁxed and we do not consider dynam-
ics of the metric. From this point of view the closer anal-
ogy is perhaps the study of ﬁeld theories in a ﬁxed curved
spacetime where the back-reaction of the matter ﬁelds to
the spacetime geometry has been ignored. It would be in-
teresting to explore equivariant CNNs with geometry that
evolves between layers in future work. It is certainly tempt-
ing to treat the direction of different layers as a part of the
spacetime, either as the temporal or the holographic direc-
tion (’t Hooft, 1993). This interpretation is particularly rel-
evant if all feature spaces carry the same group representa-
tion.

K(˜γ(t), v(˜γ(t))) := Kv(˜γ(t)).

(5)

Acknowledgements

In other words, we simultaneously parallel transport the
dependence on the tangent vector. The vanishing of the
covariant derivative of the output feature along the curve,
˜γ∗(
)fout (˜γ(t)) then just comes from the vanishing of the
covariant derivative of volume form and the deﬁnition (5).

∇

the ker-
(5) means that, along the path,
Note that
nel K(γ(t), v) is completely determined by the kernel
K(γ(t0), v) at any point t0 on the path. Supposed further
M on the manifold.
that we select a reference point x∗ ∈
For any point y
M that is connected to x∗ by a unique
∈
geodesic, parallel transport with respect to the geodesic
then unambiguously “share” the kernel at x∗ with y. On
the other hand, when y is connected by more than one
geodesics, the general covariance then dictates the relation
between the outputs corresponding to different geodesics.
Moreover, this covariance also holds for transporting along
different paths (not necessarily geodesics) in general. More
precisely, we see how different kernels, related again by a
local change of coordinates, can be compensated by a trans-
formation of the input and output feature ﬁelds. We hence

The work of MC is supported by ERC starting grant
#640159 and NWO Vidi grant ERC starting grant H2020
ERC StG #640159. The work of VA is supported by ERC
starting grant #640159.

References

Cohen, T., Geiger, M., and Weiler, M. A General Theory
of Equivariant CNNs on Homogeneous Spaces. 2018a.

Cohen, T. S. and Welling, M. Group equivariant convolu-

tional networks. In ICML, 2016.

Cohen, T. S. and Welling, M. Steerable CNNs. In ICLR,

2017.

Cohen, T. S., Geiger, M., Koehler, J., and Welling, M.

Spherical CNNs. In ICLR, 2018b.

Cohen, T. S., Geiger, M., and Weiler, M. Intertwiners be-
tween Induced Representations (with Applications to the
Theory of Equivariant Neural Networks). 2018c.

Covariance in Physics and Convolutional Neural Networks

Cohen, T. S., Weiler, M., Kicanaoglu, B., and Welling, M.
Gauge equivariant convolutional networks and the icosa-
hedral cnn. arXiv preprint arXiv:1902.04615, 2019.

Einstein, A. The foundation of the general theory of rela-

tivity. Annalen der Physik, 49(7):769–822, 1916.

Hoogeboom, E., Peters, J. W. T., Cohen, T. S., and Welling,

M. HexaConv. In ICLR, 2018.

Kondor, R. and Trivedi, S. On the generalization of equiv-
ariance and convolution in neural networks to the action
of compact groups. arXiv preprint arXiv:1802.03690,
2018.

’t Hooft, G. Dimensional reduction in quantum gravity.

Conf. Proc., C930308:284–296, 1993.

Weiler, M., Geiger, M., Welling, M., Boomsma, W., and
Cohen, T. S. 3D Steerable CNNs: Learning Rotation-
ally Equivariant Features in Volumetric Data. In NIPS,
2018a.

Weiler, M., Hamprecht, F. A., and Storath, M. Learn-
ing Steerable Filters for Rotation Equivariant CNNs. In
CVPR, 2018b.

Winkels, M. and Cohen, T. S. 3D G-CNNs for Pulmonary
Nodule Detection. In International Conference on Med-
ical Imaging with Deep Learning (MIDL), 2018.

Worrall, D. E. and Brostow, G. J. Cubenet: Equivariance

to 3d rotation and translation. In ECCV, 2018.

Worrall, D. E., Garbin, S. J., Turmukhambetov, D., and
Brostow, G. J. Harmonic Networks: Deep Translation
and Rotation Equivariance. In CVPR, 2017.

