A Hierarchical Neural Attention-based Text Classiﬁer

Koustuv Sinha 1,2, Yue Dong 1,2, Jackie C.K. Cheung 1,2 and Derek Ruths 1
1 School of Computer Science, McGill University, Canada
2 Montreal Institute of Learning Algorithms, Canada
{koustuv.sinha, yue.dong2, jcheung, derek.ruths }
@{mail.mcgill.ca, mail.mcgill.ca, cs.mcgill.ca, mcgill.ca}

Abstract

Deep neural networks have been displaying
superior performance over traditional super-
vised classiﬁers in text classiﬁcation. They
learn to extract useful features automatically
when sufﬁcient amount of data is presented.
However, along with the growth in the num-
ber of documents comes the increase in the
number of categories, which often results in
poor performance of the multiclass classiﬁers.
In this work, we use external knowledge in
the form of topic category taxonomies to aide
the classiﬁcation by introducing a deep hier-
archical neural attention-based classiﬁer. Our
model performs better than or comparable to
state-of-the-art hierarchical models at signiﬁ-
cantly lower computational cost while main-
taining high interpretability.

1

Introduction

A large number of documents are being generated
all over the world everyday, and as a result auto-
matic text classiﬁcation has become an essential
tool for searching, retrieving, and managing the
text (Allahyari et al., 2017). There has been an
increasing trend in developing data-driven neural
text classiﬁers (Collobert et al., 2011; Lai et al.,
2015; Zhang et al., 2015; Yogatama et al., 2017;
Conneau et al., 2017), due to their ability to han-
dle large-scale corpora and their robustness in au-
tomatic feature extraction.

However,

text classiﬁcation has become in-
creasingly challenging as the number of categories
grows with continually expanding corpus. To alle-
viate this problem, one form of the external knowl-
edge – class taxonomy – has been introduced
to aid the classiﬁcation in a hierarchical fashion
(Koller and Sahami, 1997). In general, hierarchi-
cal classiﬁers can be categorized into two broad
approaches: local (top-down and bottom-up) and
global (or big-bang) (Silla and Freitas, 2011). The

local approaches create a unique classiﬁer for each
parent node in the taxonomy (Liu et al., 2001;
Quinn and Laier, 2006; Vens et al., 2008; Kowsari
et al., 2017), while global approaches create a sin-
gle classiﬁer for the entire taxonomy (Silla Jr and
Freitas, 2009).

Kowsari et al. (2017) recently proposed a hierar-
chical neural-based model called HDLTex, which
displayed superior performance over traditional
non-neural-based models with a top-down struc-
ture. However, HDLTex suffers the inherited dis-
advantage of the top-down approach: the number
of sub-models grows exponentially with respect to
the number of sub-trees. This is especially prob-
lematic in HDLTex, as it uses deep networks with
a large number of parameters for the sub-models,
and the combined model itself grows exponen-
tially with the depth of taxonomy.

In contrast, we propose a uniﬁed global deep
neural-based classiﬁer that overcomes the prob-
lem of exploding models. The backbone of our
approach is one encoder-decoder structure that se-
quentially predicts the class label of the next level,
conditioned on a dynamic document representa-
tion obtained based on a variant of an attention
mechanism (Bahdanau et al., 2015). The contri-
bution of our paper is as follows:

1. We propose an end-to-end global neural
attention-based model for hierarchical clas-
siﬁcation, which performs better than the
state-of-the-art hierarchical classiﬁer at lower
computation cost.

2. We empirically show that the use of hierar-
chical taxonomy provides a robust classiﬁer,
by comparing with state-of-the-art ﬂat classi-
ﬁers.

2 Literature Review

Traditional text classiﬁcation methods focus on se-
lecting a good set of features (for example,TF-IDF
(Salton and Buckley, 1987)) to represent the doc-
uments and employing non-linear classiﬁers such
as SVM (Dumais et al., 1998; Joachims, 1999;
Tong and Koller, 2001), decision trees (Apt´e et al.,
1994), or Naive Bayes (McCallum et al., 1998;
Kim et al., 2006) methods for text classiﬁcation.
More recent work has employed deep neural net-
works to merge feature extraction and classiﬁca-
tion into one joint process, where the model pa-
rameters can be learned through back-propagation
(Xue et al., 2008; Lai et al., 2015; Zhang et al.,
2015). A common theme in these convolutional
neural networks (CNN)-based or recurrent neural
network (RNN)-based approaches is to create a
document representation from either the last hid-
den state of the RNN or via some pooling opera-
tions on all hidden states.

Furthermore,

the attention mechanism (Bah-
danau et al., 2015; Sutskever et al., 2014) has been
adapted for these CNN/RNN structures for text
classiﬁcation (Lin et al., 2017), providing high
interpretability and allowing us to inspect which
parts of the text are discriminative for a particular
sample.

In addition, external knowledge has been exam-
ined as a way to boost the performance of text clas-
siﬁers (Collobert and Weston, 2008a; Ngiam et al.,
2011; Howard and Ruder, 2018). One form of ex-
ternal knowledge is built on top of the hierarchical
relations of the classes (Koller and Sahami, 1997),
where a class taxonomy is used to improve the per-
formance of the end-level classiﬁcation1. Most of
the hierarchical classiﬁers2 perform classiﬁcation
by navigating through the hierarchy in top-down
approaches (Liu et al., 2001; Quinn and Laier,
2006; Vens et al., 2008), where a local classiﬁer
is constructed at each parent node. The state-of-
the-art hierarchical classiﬁer HDLTex is proposed
by Kowsari et al. (2017). It combines deep neural
networks in the top-down fashion where a sepa-

1Classiﬁers that do not take into account the hierarchy and
are only concerned with predicting the leaf nodes are termed
ﬂat classiﬁers in this work.

2We use the term “hierarchical classiﬁers” to refer the
models that follow the external taxonomy of class labels,
which is substantially different from hierarchical attention
networks (Yang et al., 2016). In Yang et al. (2016), hierar-
chical attention networks refer to the hierarchical nature of
their attention mechanism; the model attends to the sentences
ﬁrst and then attends to the words.

rate neural network (either CNN or RNN) is built
at each parent node to classify its children.

3 Model

l1

wl1

u1

l2

wl2

u2

l3

wl3
u3

−→
h1

←−
h1

w1

h e

T

−→
h2

←−
h2

w2

2

1

0

2

−→
h3

←−
h3

−→
h4

←−
h4

−→
h5

←−
h5

−→
h6

←−
h6

−→
h7

←−
h7

−→
h8

←−
h8

−→
h9

←−
h9

w3
h i b a
e a rt h

C

w4

w5

w6

w7

w8

u a k e

q

o c c u rr e d

g

n

alo

o rth e a ste r n

th e
n

w9
c o a st

Figure 1: Proposed model architecture

Our proposed model (Figure 1) consists of three
parts: 1) a bidirectional LSTM encoder (Hochre-
iter and Schmidhuber, 1997) that transforms each
word into vector representations based on their
context. 2) an attention module that helps to gener-
ate dynamic document representations across dif-
ferent level of classiﬁcation, 3) multi-layer percep-
tron (MLP) classiﬁers at each level that makes the
prediction of classes at that level based on the dy-
namically generated document representation and
the level masking.

Our hierarchical classiﬁcation model can be
viewed as a sequence-to-sequence model, where
a sequence of word embeddings is used to gen-
erate a sequence of hierarchical class labels.
In
addition, we employ a modiﬁed attention module
from the traditional attention mechanism used in
sequential generation tasks (Bahdanau et al., 2015;
Sutskever et al., 2014). Instead of computing at-
tention weights conditioned on the hidden state of
the decoder at time step i, we condition on the par-
ent category embedding ck−1. This is intuitive in
our setting as the document representation should
depend on the parent class predicted by the model.
Formally, suppose we are given a document
with n tokens D = (w1, w2, ..., wn) and its cat-
egory labels of m levels C = (c1, . . . , cm), ck ∈
{clk
} where lk indicates the k-th level of
the class taxonomy and sk represents the number
of classes in level k 3. A bidirectional LSTM is

1 , . . . , clk
sk

3We suppose wi and ci are word embeddings and class

embedding respectively.

ﬁrst used to extract features of the document:

−→
ht =
←−
ht =

−−−−→
LST M (wt,
←−−−−
LST M (wt,

−−−→
ht−1),
←−−−
ht+1).

(1)

The encoder’s hidden states H = (h1, . . . , hn) are
←−
ht)
constructed by the concatenation of (
as hi = [

−→
ht) and (

←−
ht].

−→
ht,

When classifying the class label at level k, we
ﬁrst form contextual word features ¯Hk by concate-
nating the previously predicted category embed-
ding ck−1 (parent) with each of the encoder’s out-
puts H = (h1, . . . , hn):

¯Hk = H ⊕ ck−1.

(2)

Then, we transform these n vectors in ¯Hk into n
attention scores (scalars) through a series of linear
and non-linear transformations:

ak = softmax(ws2tanh(Ws1

¯H T

k )).

(3)

As one single attention distribution might only fo-
cus on a speciﬁc component of the semantics in
the document, we follow Lin et al. (2017)’s work
to perform m hops of attention and form the multi-
head attention matrix Ak (m × n). To encour-
age diversity over the multiple hops of the atten-
tion distributions, we employ the Frobenius norm

penalty (Lin et al., 2017) P =
to
force the attention hops to focus on different as-
pects of the semantics.

k − I

F

(cid:13)
(cid:13)AkA(cid:62)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

The document representation for level k is ob-
tained by multiplying the multi-head attention ma-
trix and the contextual word features:

Dk = Ws3Ak ¯Hk.

(4)

(5)

Finally, a two layered multi-layer perceptron
(MLP) is employed to classify the category at level
k:

dk = RELU(WD[Dk, dk − 1]),
yk = softmax(Wkdk)

Normally, the softmax in Equation 5 is computed
over all class labels across the entire taxonomy
levels. This is not desirable when the taxonomy
is deep and the number of classes is large. We
solve this by employing a level masking technique
where we mask out all the classes that are not in
the current classiﬁcation level k. The loss is then
calculated as the joint cross entropy loss among all
levels of the taxonomy: l = (cid:80)m

i=1 li.

Level 1 Categories
Level 2 Categories
Level 3 Categories
Number of documents
Mean document length

DBpedia WOS
9
70
219
381,025
106.9

7
134
NA
46,985
200.7

Table 1: Dataset Comparison

4 Experimental Setup

Dataset Two datasets are used for our experi-
ments: Web of Science (WOS) and DBpedia. Web
of Science (WOS) is a hierarchical two-level tax-
onomy dataset that contains 46,985 documents
collected from Web of Science (Reuters, 2012) by
Kowsari et al. (2017). Despite its small size, WOS
is used as a benchmark dataset for hierarchical
classiﬁcation as it provides the raw text for deep
neural models to train on4.

As deep learning models usually contain a large
number of parameters that need to be learned, to
prevent over-ﬁtting (Lawrence et al., 1997; Srivas-
tava et al., 2014) we usually need a large dataset
to train upon. Thus, we curated a bigger dataset
with hierarchical labels from Wikipedia meta in-
formation provider DBpedia5. Compared to WOS,
our DBpedia dataset is larger in two aspects: the
number of data instances and the number of hier-
archical levels (Table 1). The DBpedia ontology
was ﬁrst used in Zhang et al. (2015) for ﬂat text
classiﬁcation. We instead use the DBpedia ontol-
ogy to construct a dataset with a three-level taxon-
omy of classes. In order to ensure enough docu-
ments per-class, we only extract leaf-classes with
more than 200 documents. We also randomly sub-
sample 3,000 documents per category to balance
the number of leaf-level categories. This results
in 381,025 documents in total, which we split into
90% for training (from which 10% were kept aside
for validation) and 10% on testing, on which we
report our classiﬁcation metrics6.

Baselines State-of-the-art ﬂat classiﬁers such
as FastText (Joulin et al., 2017), Bi-directional

4The LSHTC dataset (Partalas et al., 2015) has been
widely used as a benchmark for hierarchical text classiﬁca-
tion. However, the raw texts are not available which makes
it difﬁcult to extract features for modern neural approaches.
Instead, only the tf-idf vectors are provided as inputs with no
option to retrieve the original text (even after consulting with
the original authors we were unable to procure it).

5http://wiki.dbpedia.org/
6Our code and data will be released at https://

github.com/koustuvsinha/hier-class

DBpedia

WOS

Flat Baselines
FastText
BiLSTM + MLP + Maxpool
BiLSTM + MLP + Meanpool
Structured Self Attention (m=1)
Hierarchical Models
HDLTex (5B params)
Our model (34M params)

Overall
86.2
94.20
94.68
94.04
Overall
92.10
93.72

Overall
61.3
77.69
73.08
77.40
Overall
76.58
77.46

l1
99.26
99.21

l2
97.18
96.03

l3
95.5
95.32

l1
90.45
89.32

l2
84.66
82.42

Table 2: Test accuracy on the WOS and DBpedia datasets. The ﬂat baseline models are trained without the hierarchical
taxonomy of classes and therefore only have results on the leaf-node classiﬁcation.

LSTM with max/mean pooling (Collobert and We-
ston, 2008b; Lee and Dernoncourt, 2016) and
the Structured Self-attentive classiﬁer (Lin et al.,
2017) are used for the comparison. We no-
ticed that using the default hyperparameters of the
Structured Self-attentive classiﬁer with high atten-
tion hops (m >= 8) performed poorly compared
to use just one attention hop (m = 1). There-
fore, we reported the results of using one attention
hop (m = 1) as our baselines for fair comparison.
We also compare our classiﬁer to the state-of-the-
art hierarchical classiﬁer HDLTex (Kowsari et al.,
2017).

Hyperparameters We use 300-dimensional word
embeddings which are randomly initialized and
ﬁne-tuned during training. Two-layer Bidirec-
tional LSTM with 300 hidden units in each layer
are employed. In the multi-head attention mech-
anism, we use 4 heads (hops) with 0.1 Frobe-
nius norm penalty because it gives the best valida-
tion performance. The ﬁnal fully-connected MLP
layer WD has 1200 hidden units. In addition, we
add 0.4 dropout on BiLSTM layers and MLP lay-
ers to prevent over-ﬁtting.

For optimization, we use the standard Adam
optimizer (Kingma and Ba, 2014) with the learn-
ing rate of 0.001, weight decay of 10−4 and 10−6
for WOS and DBpedia, respectively. The gra-
dients are clipped to 0.5 in order to prevent ex-
ploding gradients. All the results are obtained af-
ter 25 epochs of training. After every 10 epochs,
we reduce the learning rate by half if the valida-
tion accuracy is not improving. We employ early-
stopping to select the best model. In addition, a
weighted loss function is utilized to balance the
performance on under-represented classes.

Hierarchical Evaluation For evaluating hierar-
chical models, we present the teacher-forcing re-

sult on each level, such as l1, l2 and l3. This
indicates the per-level classiﬁcation performance
when we provide the true parent class to the classi-
ﬁer while predicting the next class. However, this
is not desirable as during inference we should not
have access to the correct parent class. Hence we
also present the Overall score in Table 2, where
the classiﬁer uses its own prediction as the parent
class.

5 Results

Our model is signiﬁcantly better than the existing
state-of-the-art hierarchical baseline (Table 2). Al-
though, we also see that both hierarchical classi-
ﬁers (ours and HDLTex) perform comparably with
or slightly worse than the state-of-the-art ﬂat clas-
siﬁers in terms of accuracy. However, the robust-
ness analysis we performed in Table 3 indicates
that hierarchical models are more robust in their
errors since most of the errors generated by hierar-
chical classiﬁers remain within the correct tree of
the parent class, while ﬂat classiﬁers do worse. For
example, on WOS, 88.57% of all classiﬁed data by
our hierarchical model is within the correct subtree
compared to 85.56% for the ﬂat classiﬁer.

Classiﬁer
Flat classiﬁer - BiLSTM Max Pooling
Hierarchical approach - Our model

Correct parent
90.74
93.03

Predicted parent
85.56
88.57

Table 3: Robustness analysis of taxonomy on the WOS
dataset. We compare the success rate of our model and the
BiLSTM ﬂat classiﬁer. The success rate is deﬁned as the
number of times the predicted class is within the same sub-
tree as the correct parent. We calculate this in two scenar-
ios: 1. when the true parent class is manually provided, or
teacher-forced (Correct parent), and 2. when the true parent
class is predicted by our model (Predicted parent)

Interestingly, the class taxonomy seems to be
more beneﬁcial in boosting the performance of
hierarchical classiﬁers on WOS than DBpedia.
The hierarchical classiﬁers perform better on the

(a) Level 1 - correct class : Medical

(b) Level 2 - correct class : Crohn’s disease

Figure 2: WOS dataset attention rereading per level. Highlighted words indicate the attented words. Stronger color denote
higher focus of attention. We note that the attention spread becomes much more focused in Level 2 compared to its parent
Level 1.

leaf-node level classiﬁcation of WOS than that
on DBpedia. We observe this behaviour due to
the dataset of DBpedia being shorter in average
length making it easier to classify for ﬂat classi-
ﬁers, hence hierarchical classiﬁers overﬁt on the
training data.

In addition to the performance improvement on
both datasets over HDLTex, our model takes sig-
niﬁcantly less time and resources to train, espe-
cially when the dataset is large in terms of the in-
termediate non-leaf nodes in the output taxonomy.
As HDLTex needs to build one sub-classiﬁer for
each parent nodes, the number of sub-classiﬁers
grows quickly. For example, there are 80 parent
nodes in the taxonomy of the DBpedia dataset and
HDLTex needs to build 80 RNNs, where each sub-
classiﬁer contains around 67 million parameters.
As a consequence, we can barely ﬁt the whole
model of HDLTex on our CPU 7 because it re-
quires 60 GB RAM to build these 80 deep neural
networks.

6 Discussion

Analysis of Attention The intuition behind build-
ing dynamic document representations, using mul-
tiple attentions across different hierarchical levels,
is to have a re-reading effect over the taxonomy.
When we ﬁrst encounter an article as humans, we
tend to read it carefully, but on subsequent reads
we can easily identify the key aspects of the ar-
ticle. We ﬁnd in our exploratory experiments the
attention vectors behave exactly the same. For the

ﬁrst level, the attention values are more spread out
to help our classiﬁer pick various important as-
pects of the article, but on the subsequent levels,
the attention is more focused towards speciﬁc key-
words for that subclass, as the example shown in
Figure 2 8. We perform additional qualitative anal-
ysis of attention spread which is provided in Ap-
pendix.

7 Conclusion

In this work, we propose a light-weight neural-
based hierarchical classiﬁer that performs better
than or comparable to the state-of-the-art hier-
archical model at lower computation cost. Our
model employs an adapted version of attention to
represent documents dynamically through the hi-
erarchy, which provides additional interpretability
of the dynamic document representations. In ad-
dition, we demonstrate that the robustness of ﬂat
text classiﬁcation can be improved by using ex-
ternal knowledge such as a hierarchical taxonomy.
As a future direction, we will advance our model
to automatically construct the hierarchical taxon-
omy in order to improve text classiﬁcation with a
large number of classes.

Acknowledgements

We thank Compute Canada and Calcul Quebec
for providing the GPU compute resources. We
would also express our appreciation to our spon-
sors, Pierre Arbour Foundation and FRQNT for
supporting Koustuv Sinha and NSERC for sup-
porting Yue Dong for this research.

8We use the same visualization script as of Lin et al.

7It is not possible to ﬁt the entire model in one GPU as
our best GPU has the RAM capacity of 12GB, one needs to
have multiple GPU’s and parallel execution for this task.

(2017).

References

Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Asseﬁ,
Saied Safaei, Elizabeth D Trippe, Juan B Gutierrez,
and Krys Kochut. 2017. A brief survey of text min-
ing: Classiﬁcation, clustering and extraction tech-
In Proceedings of KDD Bigdas, Halifax,
niques.
Canada.

Chidanand Apt´e, Fred Damerau, and Sholom M Weiss.
1994. Automated learning of decision rules for text
categorization. ACM Transactions on Information
Systems (TOIS), 12(3):233–251.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations (ICLR).

Ronan Collobert and Jason Weston. 2008a. A uniﬁed
architecture for natural language processing: Deep
In Pro-
neural networks with multitask learning.
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.

Ronan Collobert and Jason Weston. 2008b. A uniﬁed
architecture for natural language processing: Deep
In Pro-
neural networks with multitask learning.
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research,
scratch.
12(Aug):2493–2537.

Alexis Conneau, Holger Schwenk, Lo¨ıc Barrault, and
Yann Lecun. 2017. Very deep convolutional net-
works for text classiﬁcation. In Proceedings of the
15th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Volume 1,
Long Papers, volume 1, pages 1107–1116.

Susan Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998.
Inductive learning algo-
rithms and representations for text categorization. In
Proceedings of the seventh international conference
on Information and knowledge management, pages
148–155. ACM.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model ﬁne-tuning for text classiﬁcation. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 328–339.

Thorsten Joachims. 1999. Transductive inference for
text classiﬁcation using support vector machines. In
ICML, volume 99, pages 200–209.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2017. Bag of tricks for efﬁcient
text classiﬁcation. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Volume 2, Short Pa-
pers, pages 427–431. Association for Computational
Linguistics.

Sang-Bum Kim, Kyoung-Soo Han, Hae-Chang Rim,
and Sung Hyon Myaeng. 2006. Some effective tech-
IEEE
niques for naive bayes text classiﬁcation.
transactions on knowledge and data engineering,
18(11):1457–1466.

Diederik P Kingma and Jimmy Lei Ba. 2014. Adam:
Amethod for stochastic optimization. In Proc. 3rd
Int. Conf. Learn. Representations.

Daphne Koller and Mehran Sahami. 1997. Hierarchi-
cally classifying documents using very few words.
Technical report, Stanford InfoLab.

Kamran Kowsari, Donald E Brown, Mojtaba Hei-
darysafa, Kiana Jafari Meimandi, Matthew S Ger-
ber, and Laura E Barnes. 2017. HDLTex: Hierarchi-
cal deep learning for text classiﬁcation. In 2017 16th
IEEE International Conference on Machine Learn-
ing and Applications (ICMLA), pages 364–371.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.
Recurrent convolutional neural networks for text
In AAAI, volume 333, pages 2267–
classiﬁcation.
2273.

Steve Lawrence, C Lee Giles, and Ah Chung Tsoi.
1997. Lessons in neural network training: Overﬁt-
In AAAI/IAAI,
ting may be harder than expected.
pages 540–545. Citeseer.

Ji Young Lee and Franck Dernoncourt. 2016.

Se-
quential short-text classiﬁcation with recurrent and
In Proceedings of
convolutional neural networks.
NAACL-HLT, pages 515–520.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
In 5th International Conference on
embedding.
Learning Representations (ICLR 2017).

Shaohui Liu, Mingkai Dong, Haijun Zhang, Rong Li,
and Zhongzhi Shi. 2001. An approach of multi-
hierarchy text classiﬁcation. In Info-tech and Info-
net, 2001. Proceedings. ICII 2001-Beijing. 2001 In-
ternational Conferences on, volume 3, pages 95–
100. IEEE.

Andrew McCallum, Kamal Nigam, et al. 1998. A com-
parison of event models for naive bayes text classi-
ﬁcation. In AAAI-98 workshop on learning for text
categorization. Citeseer.

Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan
Nam, Honglak Lee, and Andrew Y Ng. 2011. Multi-
modal deep learning. In Proceedings of the 28th in-
ternational conference on machine learning (ICML-
11), pages 689–696.

Dani Yogatama, Chris Dyer, Wang Ling, and Phil Blun-
som. 2017. Generative and discriminative text clas-
arXiv
siﬁcation with recurrent neural networks.
preprint arXiv:1703.01898.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
In Advances in neural information pro-
siﬁcation.
cessing systems, pages 649–657.

Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis,
Thierry Artieres, George Paliouras, Eric Gaussier,
Ion Androutsopoulos, Massih-Reza Amini, and
Lshtc: A benchmark
Patrick Galinari. 2015.
arXiv preprint
for large-scale text classiﬁcation.
arXiv:1503.08581.

Michael J Quinn and Mary L Laier. 2006. Method and
apparatus for fast lookup of related classiﬁcation en-
tities in a tree-ordered classiﬁcation hierarchy. US
Patent 7,032,072.

Thomson Reuters. 2012. Web of science.

Gerard Salton and Chris Buckley. 1987. Term weight-
ing approaches in automatic text retrieval. Technical
report, Cornell University.

Carlos N Silla and Alex A Freitas. 2011. A survey
of hierarchical classiﬁcation across different appli-
cation domains. Data Mining and Knowledge Dis-
covery, 22(1-2):31–72.

Carlos N Silla Jr and Alex A Freitas. 2009. A global-
model naive bayes approach to the hierarchical pre-
diction of protein functions. In Data Mining, 2009.
ICDM’09. Ninth IEEE International Conference on,
pages 992–997. IEEE.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Simon Tong and Daphne Koller. 2001. Support vec-
tor machine active learning with applications to
text classiﬁcation. Journal of machine learning re-
search, 2(Nov):45–66.

Celine Vens, Jan Struyf, Leander Schietgat, Saˇso
Dˇzeroski, and Hendrik Blockeel. 2008. Decision
trees for hierarchical multi-label classiﬁcation. Ma-
chine Learning, 73(2):185.

Gui-Rong Xue, Dikan Xing, Qiang Yang, and Yong Yu.
2008. Deep classiﬁcation in large-scale text hierar-
In Proceedings of the 31st annual interna-
chies.
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 619–626.
ACM.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classiﬁcation.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

A Hierarchical Neural Attention-based Text Classiﬁer

Koustuv Sinha 1,2, Yue Dong 1,2, Jackie C.K. Cheung 1,2 and Derek Ruths 1
1 School of Computer Science, McGill University, Canada
2 Montreal Institute of Learning Algorithms, Canada
{koustuv.sinha, yue.dong2, jcheung, derek.ruths }
@{mail.mcgill.ca, mail.mcgill.ca, cs.mcgill.ca, mcgill.ca}

Abstract

Deep neural networks have been displaying
superior performance over traditional super-
vised classiﬁers in text classiﬁcation. They
learn to extract useful features automatically
when sufﬁcient amount of data is presented.
However, along with the growth in the num-
ber of documents comes the increase in the
number of categories, which often results in
poor performance of the multiclass classiﬁers.
In this work, we use external knowledge in
the form of topic category taxonomies to aide
the classiﬁcation by introducing a deep hier-
archical neural attention-based classiﬁer. Our
model performs better than or comparable to
state-of-the-art hierarchical models at signiﬁ-
cantly lower computational cost while main-
taining high interpretability.

1

Introduction

A large number of documents are being generated
all over the world everyday, and as a result auto-
matic text classiﬁcation has become an essential
tool for searching, retrieving, and managing the
text (Allahyari et al., 2017). There has been an
increasing trend in developing data-driven neural
text classiﬁers (Collobert et al., 2011; Lai et al.,
2015; Zhang et al., 2015; Yogatama et al., 2017;
Conneau et al., 2017), due to their ability to han-
dle large-scale corpora and their robustness in au-
tomatic feature extraction.

However,

text classiﬁcation has become in-
creasingly challenging as the number of categories
grows with continually expanding corpus. To alle-
viate this problem, one form of the external knowl-
edge – class taxonomy – has been introduced
to aid the classiﬁcation in a hierarchical fashion
(Koller and Sahami, 1997). In general, hierarchi-
cal classiﬁers can be categorized into two broad
approaches: local (top-down and bottom-up) and
global (or big-bang) (Silla and Freitas, 2011). The

local approaches create a unique classiﬁer for each
parent node in the taxonomy (Liu et al., 2001;
Quinn and Laier, 2006; Vens et al., 2008; Kowsari
et al., 2017), while global approaches create a sin-
gle classiﬁer for the entire taxonomy (Silla Jr and
Freitas, 2009).

Kowsari et al. (2017) recently proposed a hierar-
chical neural-based model called HDLTex, which
displayed superior performance over traditional
non-neural-based models with a top-down struc-
ture. However, HDLTex suffers the inherited dis-
advantage of the top-down approach: the number
of sub-models grows exponentially with respect to
the number of sub-trees. This is especially prob-
lematic in HDLTex, as it uses deep networks with
a large number of parameters for the sub-models,
and the combined model itself grows exponen-
tially with the depth of taxonomy.

In contrast, we propose a uniﬁed global deep
neural-based classiﬁer that overcomes the prob-
lem of exploding models. The backbone of our
approach is one encoder-decoder structure that se-
quentially predicts the class label of the next level,
conditioned on a dynamic document representa-
tion obtained based on a variant of an attention
mechanism (Bahdanau et al., 2015). The contri-
bution of our paper is as follows:

1. We propose an end-to-end global neural
attention-based model for hierarchical clas-
siﬁcation, which performs better than the
state-of-the-art hierarchical classiﬁer at lower
computation cost.

2. We empirically show that the use of hierar-
chical taxonomy provides a robust classiﬁer,
by comparing with state-of-the-art ﬂat classi-
ﬁers.

2 Literature Review

Traditional text classiﬁcation methods focus on se-
lecting a good set of features (for example,TF-IDF
(Salton and Buckley, 1987)) to represent the doc-
uments and employing non-linear classiﬁers such
as SVM (Dumais et al., 1998; Joachims, 1999;
Tong and Koller, 2001), decision trees (Apt´e et al.,
1994), or Naive Bayes (McCallum et al., 1998;
Kim et al., 2006) methods for text classiﬁcation.
More recent work has employed deep neural net-
works to merge feature extraction and classiﬁca-
tion into one joint process, where the model pa-
rameters can be learned through back-propagation
(Xue et al., 2008; Lai et al., 2015; Zhang et al.,
2015). A common theme in these convolutional
neural networks (CNN)-based or recurrent neural
network (RNN)-based approaches is to create a
document representation from either the last hid-
den state of the RNN or via some pooling opera-
tions on all hidden states.

Furthermore,

the attention mechanism (Bah-
danau et al., 2015; Sutskever et al., 2014) has been
adapted for these CNN/RNN structures for text
classiﬁcation (Lin et al., 2017), providing high
interpretability and allowing us to inspect which
parts of the text are discriminative for a particular
sample.

In addition, external knowledge has been exam-
ined as a way to boost the performance of text clas-
siﬁers (Collobert and Weston, 2008a; Ngiam et al.,
2011; Howard and Ruder, 2018). One form of ex-
ternal knowledge is built on top of the hierarchical
relations of the classes (Koller and Sahami, 1997),
where a class taxonomy is used to improve the per-
formance of the end-level classiﬁcation1. Most of
the hierarchical classiﬁers2 perform classiﬁcation
by navigating through the hierarchy in top-down
approaches (Liu et al., 2001; Quinn and Laier,
2006; Vens et al., 2008), where a local classiﬁer
is constructed at each parent node. The state-of-
the-art hierarchical classiﬁer HDLTex is proposed
by Kowsari et al. (2017). It combines deep neural
networks in the top-down fashion where a sepa-

1Classiﬁers that do not take into account the hierarchy and
are only concerned with predicting the leaf nodes are termed
ﬂat classiﬁers in this work.

2We use the term “hierarchical classiﬁers” to refer the
models that follow the external taxonomy of class labels,
which is substantially different from hierarchical attention
networks (Yang et al., 2016). In Yang et al. (2016), hierar-
chical attention networks refer to the hierarchical nature of
their attention mechanism; the model attends to the sentences
ﬁrst and then attends to the words.

rate neural network (either CNN or RNN) is built
at each parent node to classify its children.

3 Model

l1

wl1

u1

l2

wl2

u2

l3

wl3
u3

−→
h1

←−
h1

w1

h e

T

−→
h2

←−
h2

w2

2

1

0

2

−→
h3

←−
h3

−→
h4

←−
h4

−→
h5

←−
h5

−→
h6

←−
h6

−→
h7

←−
h7

−→
h8

←−
h8

−→
h9

←−
h9

w3
h i b a
e a rt h

C

w4

w5

w6

w7

w8

u a k e

q

o c c u rr e d

g

n

alo

o rth e a ste r n

th e
n

w9
c o a st

Figure 1: Proposed model architecture

Our proposed model (Figure 1) consists of three
parts: 1) a bidirectional LSTM encoder (Hochre-
iter and Schmidhuber, 1997) that transforms each
word into vector representations based on their
context. 2) an attention module that helps to gener-
ate dynamic document representations across dif-
ferent level of classiﬁcation, 3) multi-layer percep-
tron (MLP) classiﬁers at each level that makes the
prediction of classes at that level based on the dy-
namically generated document representation and
the level masking.

Our hierarchical classiﬁcation model can be
viewed as a sequence-to-sequence model, where
a sequence of word embeddings is used to gen-
erate a sequence of hierarchical class labels.
In
addition, we employ a modiﬁed attention module
from the traditional attention mechanism used in
sequential generation tasks (Bahdanau et al., 2015;
Sutskever et al., 2014). Instead of computing at-
tention weights conditioned on the hidden state of
the decoder at time step i, we condition on the par-
ent category embedding ck−1. This is intuitive in
our setting as the document representation should
depend on the parent class predicted by the model.
Formally, suppose we are given a document
with n tokens D = (w1, w2, ..., wn) and its cat-
egory labels of m levels C = (c1, . . . , cm), ck ∈
{clk
} where lk indicates the k-th level of
the class taxonomy and sk represents the number
of classes in level k 3. A bidirectional LSTM is

1 , . . . , clk
sk

3We suppose wi and ci are word embeddings and class

embedding respectively.

ﬁrst used to extract features of the document:

−→
ht =
←−
ht =

−−−−→
LST M (wt,
←−−−−
LST M (wt,

−−−→
ht−1),
←−−−
ht+1).

(1)

The encoder’s hidden states H = (h1, . . . , hn) are
←−
ht)
constructed by the concatenation of (
as hi = [

−→
ht) and (

←−
ht].

−→
ht,

When classifying the class label at level k, we
ﬁrst form contextual word features ¯Hk by concate-
nating the previously predicted category embed-
ding ck−1 (parent) with each of the encoder’s out-
puts H = (h1, . . . , hn):

¯Hk = H ⊕ ck−1.

(2)

Then, we transform these n vectors in ¯Hk into n
attention scores (scalars) through a series of linear
and non-linear transformations:

ak = softmax(ws2tanh(Ws1

¯H T

k )).

(3)

As one single attention distribution might only fo-
cus on a speciﬁc component of the semantics in
the document, we follow Lin et al. (2017)’s work
to perform m hops of attention and form the multi-
head attention matrix Ak (m × n). To encour-
age diversity over the multiple hops of the atten-
tion distributions, we employ the Frobenius norm

penalty (Lin et al., 2017) P =
to
force the attention hops to focus on different as-
pects of the semantics.

k − I

F

(cid:13)
(cid:13)AkA(cid:62)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

The document representation for level k is ob-
tained by multiplying the multi-head attention ma-
trix and the contextual word features:

Dk = Ws3Ak ¯Hk.

(4)

(5)

Finally, a two layered multi-layer perceptron
(MLP) is employed to classify the category at level
k:

dk = RELU(WD[Dk, dk − 1]),
yk = softmax(Wkdk)

Normally, the softmax in Equation 5 is computed
over all class labels across the entire taxonomy
levels. This is not desirable when the taxonomy
is deep and the number of classes is large. We
solve this by employing a level masking technique
where we mask out all the classes that are not in
the current classiﬁcation level k. The loss is then
calculated as the joint cross entropy loss among all
levels of the taxonomy: l = (cid:80)m

i=1 li.

Level 1 Categories
Level 2 Categories
Level 3 Categories
Number of documents
Mean document length

DBpedia WOS
9
70
219
381,025
106.9

7
134
NA
46,985
200.7

Table 1: Dataset Comparison

4 Experimental Setup

Dataset Two datasets are used for our experi-
ments: Web of Science (WOS) and DBpedia. Web
of Science (WOS) is a hierarchical two-level tax-
onomy dataset that contains 46,985 documents
collected from Web of Science (Reuters, 2012) by
Kowsari et al. (2017). Despite its small size, WOS
is used as a benchmark dataset for hierarchical
classiﬁcation as it provides the raw text for deep
neural models to train on4.

As deep learning models usually contain a large
number of parameters that need to be learned, to
prevent over-ﬁtting (Lawrence et al., 1997; Srivas-
tava et al., 2014) we usually need a large dataset
to train upon. Thus, we curated a bigger dataset
with hierarchical labels from Wikipedia meta in-
formation provider DBpedia5. Compared to WOS,
our DBpedia dataset is larger in two aspects: the
number of data instances and the number of hier-
archical levels (Table 1). The DBpedia ontology
was ﬁrst used in Zhang et al. (2015) for ﬂat text
classiﬁcation. We instead use the DBpedia ontol-
ogy to construct a dataset with a three-level taxon-
omy of classes. In order to ensure enough docu-
ments per-class, we only extract leaf-classes with
more than 200 documents. We also randomly sub-
sample 3,000 documents per category to balance
the number of leaf-level categories. This results
in 381,025 documents in total, which we split into
90% for training (from which 10% were kept aside
for validation) and 10% on testing, on which we
report our classiﬁcation metrics6.

Baselines State-of-the-art ﬂat classiﬁers such
as FastText (Joulin et al., 2017), Bi-directional

4The LSHTC dataset (Partalas et al., 2015) has been
widely used as a benchmark for hierarchical text classiﬁca-
tion. However, the raw texts are not available which makes
it difﬁcult to extract features for modern neural approaches.
Instead, only the tf-idf vectors are provided as inputs with no
option to retrieve the original text (even after consulting with
the original authors we were unable to procure it).

5http://wiki.dbpedia.org/
6Our code and data will be released at https://

github.com/koustuvsinha/hier-class

DBpedia

WOS

Flat Baselines
FastText
BiLSTM + MLP + Maxpool
BiLSTM + MLP + Meanpool
Structured Self Attention (m=1)
Hierarchical Models
HDLTex (5B params)
Our model (34M params)

Overall
86.2
94.20
94.68
94.04
Overall
92.10
93.72

Overall
61.3
77.69
73.08
77.40
Overall
76.58
77.46

l1
99.26
99.21

l2
97.18
96.03

l3
95.5
95.32

l1
90.45
89.32

l2
84.66
82.42

Table 2: Test accuracy on the WOS and DBpedia datasets. The ﬂat baseline models are trained without the hierarchical
taxonomy of classes and therefore only have results on the leaf-node classiﬁcation.

LSTM with max/mean pooling (Collobert and We-
ston, 2008b; Lee and Dernoncourt, 2016) and
the Structured Self-attentive classiﬁer (Lin et al.,
2017) are used for the comparison. We no-
ticed that using the default hyperparameters of the
Structured Self-attentive classiﬁer with high atten-
tion hops (m >= 8) performed poorly compared
to use just one attention hop (m = 1). There-
fore, we reported the results of using one attention
hop (m = 1) as our baselines for fair comparison.
We also compare our classiﬁer to the state-of-the-
art hierarchical classiﬁer HDLTex (Kowsari et al.,
2017).

Hyperparameters We use 300-dimensional word
embeddings which are randomly initialized and
ﬁne-tuned during training. Two-layer Bidirec-
tional LSTM with 300 hidden units in each layer
are employed. In the multi-head attention mech-
anism, we use 4 heads (hops) with 0.1 Frobe-
nius norm penalty because it gives the best valida-
tion performance. The ﬁnal fully-connected MLP
layer WD has 1200 hidden units. In addition, we
add 0.4 dropout on BiLSTM layers and MLP lay-
ers to prevent over-ﬁtting.

For optimization, we use the standard Adam
optimizer (Kingma and Ba, 2014) with the learn-
ing rate of 0.001, weight decay of 10−4 and 10−6
for WOS and DBpedia, respectively. The gra-
dients are clipped to 0.5 in order to prevent ex-
ploding gradients. All the results are obtained af-
ter 25 epochs of training. After every 10 epochs,
we reduce the learning rate by half if the valida-
tion accuracy is not improving. We employ early-
stopping to select the best model. In addition, a
weighted loss function is utilized to balance the
performance on under-represented classes.

Hierarchical Evaluation For evaluating hierar-
chical models, we present the teacher-forcing re-

sult on each level, such as l1, l2 and l3. This
indicates the per-level classiﬁcation performance
when we provide the true parent class to the classi-
ﬁer while predicting the next class. However, this
is not desirable as during inference we should not
have access to the correct parent class. Hence we
also present the Overall score in Table 2, where
the classiﬁer uses its own prediction as the parent
class.

5 Results

Our model is signiﬁcantly better than the existing
state-of-the-art hierarchical baseline (Table 2). Al-
though, we also see that both hierarchical classi-
ﬁers (ours and HDLTex) perform comparably with
or slightly worse than the state-of-the-art ﬂat clas-
siﬁers in terms of accuracy. However, the robust-
ness analysis we performed in Table 3 indicates
that hierarchical models are more robust in their
errors since most of the errors generated by hierar-
chical classiﬁers remain within the correct tree of
the parent class, while ﬂat classiﬁers do worse. For
example, on WOS, 88.57% of all classiﬁed data by
our hierarchical model is within the correct subtree
compared to 85.56% for the ﬂat classiﬁer.

Classiﬁer
Flat classiﬁer - BiLSTM Max Pooling
Hierarchical approach - Our model

Correct parent
90.74
93.03

Predicted parent
85.56
88.57

Table 3: Robustness analysis of taxonomy on the WOS
dataset. We compare the success rate of our model and the
BiLSTM ﬂat classiﬁer. The success rate is deﬁned as the
number of times the predicted class is within the same sub-
tree as the correct parent. We calculate this in two scenar-
ios: 1. when the true parent class is manually provided, or
teacher-forced (Correct parent), and 2. when the true parent
class is predicted by our model (Predicted parent)

Interestingly, the class taxonomy seems to be
more beneﬁcial in boosting the performance of
hierarchical classiﬁers on WOS than DBpedia.
The hierarchical classiﬁers perform better on the

(a) Level 1 - correct class : Medical

(b) Level 2 - correct class : Crohn’s disease

Figure 2: WOS dataset attention rereading per level. Highlighted words indicate the attented words. Stronger color denote
higher focus of attention. We note that the attention spread becomes much more focused in Level 2 compared to its parent
Level 1.

leaf-node level classiﬁcation of WOS than that
on DBpedia. We observe this behaviour due to
the dataset of DBpedia being shorter in average
length making it easier to classify for ﬂat classi-
ﬁers, hence hierarchical classiﬁers overﬁt on the
training data.

In addition to the performance improvement on
both datasets over HDLTex, our model takes sig-
niﬁcantly less time and resources to train, espe-
cially when the dataset is large in terms of the in-
termediate non-leaf nodes in the output taxonomy.
As HDLTex needs to build one sub-classiﬁer for
each parent nodes, the number of sub-classiﬁers
grows quickly. For example, there are 80 parent
nodes in the taxonomy of the DBpedia dataset and
HDLTex needs to build 80 RNNs, where each sub-
classiﬁer contains around 67 million parameters.
As a consequence, we can barely ﬁt the whole
model of HDLTex on our CPU 7 because it re-
quires 60 GB RAM to build these 80 deep neural
networks.

6 Discussion

Analysis of Attention The intuition behind build-
ing dynamic document representations, using mul-
tiple attentions across different hierarchical levels,
is to have a re-reading effect over the taxonomy.
When we ﬁrst encounter an article as humans, we
tend to read it carefully, but on subsequent reads
we can easily identify the key aspects of the ar-
ticle. We ﬁnd in our exploratory experiments the
attention vectors behave exactly the same. For the

ﬁrst level, the attention values are more spread out
to help our classiﬁer pick various important as-
pects of the article, but on the subsequent levels,
the attention is more focused towards speciﬁc key-
words for that subclass, as the example shown in
Figure 2 8. We perform additional qualitative anal-
ysis of attention spread which is provided in Ap-
pendix.

7 Conclusion

In this work, we propose a light-weight neural-
based hierarchical classiﬁer that performs better
than or comparable to the state-of-the-art hier-
archical model at lower computation cost. Our
model employs an adapted version of attention to
represent documents dynamically through the hi-
erarchy, which provides additional interpretability
of the dynamic document representations. In ad-
dition, we demonstrate that the robustness of ﬂat
text classiﬁcation can be improved by using ex-
ternal knowledge such as a hierarchical taxonomy.
As a future direction, we will advance our model
to automatically construct the hierarchical taxon-
omy in order to improve text classiﬁcation with a
large number of classes.

Acknowledgements

We thank Compute Canada and Calcul Quebec
for providing the GPU compute resources. We
would also express our appreciation to our spon-
sors, Pierre Arbour Foundation and FRQNT for
supporting Koustuv Sinha and NSERC for sup-
porting Yue Dong for this research.

8We use the same visualization script as of Lin et al.

7It is not possible to ﬁt the entire model in one GPU as
our best GPU has the RAM capacity of 12GB, one needs to
have multiple GPU’s and parallel execution for this task.

(2017).

References

Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Asseﬁ,
Saied Safaei, Elizabeth D Trippe, Juan B Gutierrez,
and Krys Kochut. 2017. A brief survey of text min-
ing: Classiﬁcation, clustering and extraction tech-
In Proceedings of KDD Bigdas, Halifax,
niques.
Canada.

Chidanand Apt´e, Fred Damerau, and Sholom M Weiss.
1994. Automated learning of decision rules for text
categorization. ACM Transactions on Information
Systems (TOIS), 12(3):233–251.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations (ICLR).

Ronan Collobert and Jason Weston. 2008a. A uniﬁed
architecture for natural language processing: Deep
In Pro-
neural networks with multitask learning.
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.

Ronan Collobert and Jason Weston. 2008b. A uniﬁed
architecture for natural language processing: Deep
In Pro-
neural networks with multitask learning.
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research,
scratch.
12(Aug):2493–2537.

Alexis Conneau, Holger Schwenk, Lo¨ıc Barrault, and
Yann Lecun. 2017. Very deep convolutional net-
works for text classiﬁcation. In Proceedings of the
15th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Volume 1,
Long Papers, volume 1, pages 1107–1116.

Susan Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998.
Inductive learning algo-
rithms and representations for text categorization. In
Proceedings of the seventh international conference
on Information and knowledge management, pages
148–155. ACM.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model ﬁne-tuning for text classiﬁcation. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 328–339.

Thorsten Joachims. 1999. Transductive inference for
text classiﬁcation using support vector machines. In
ICML, volume 99, pages 200–209.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2017. Bag of tricks for efﬁcient
text classiﬁcation. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Volume 2, Short Pa-
pers, pages 427–431. Association for Computational
Linguistics.

Sang-Bum Kim, Kyoung-Soo Han, Hae-Chang Rim,
and Sung Hyon Myaeng. 2006. Some effective tech-
IEEE
niques for naive bayes text classiﬁcation.
transactions on knowledge and data engineering,
18(11):1457–1466.

Diederik P Kingma and Jimmy Lei Ba. 2014. Adam:
Amethod for stochastic optimization. In Proc. 3rd
Int. Conf. Learn. Representations.

Daphne Koller and Mehran Sahami. 1997. Hierarchi-
cally classifying documents using very few words.
Technical report, Stanford InfoLab.

Kamran Kowsari, Donald E Brown, Mojtaba Hei-
darysafa, Kiana Jafari Meimandi, Matthew S Ger-
ber, and Laura E Barnes. 2017. HDLTex: Hierarchi-
cal deep learning for text classiﬁcation. In 2017 16th
IEEE International Conference on Machine Learn-
ing and Applications (ICMLA), pages 364–371.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.
Recurrent convolutional neural networks for text
In AAAI, volume 333, pages 2267–
classiﬁcation.
2273.

Steve Lawrence, C Lee Giles, and Ah Chung Tsoi.
1997. Lessons in neural network training: Overﬁt-
In AAAI/IAAI,
ting may be harder than expected.
pages 540–545. Citeseer.

Ji Young Lee and Franck Dernoncourt. 2016.

Se-
quential short-text classiﬁcation with recurrent and
In Proceedings of
convolutional neural networks.
NAACL-HLT, pages 515–520.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
In 5th International Conference on
embedding.
Learning Representations (ICLR 2017).

Shaohui Liu, Mingkai Dong, Haijun Zhang, Rong Li,
and Zhongzhi Shi. 2001. An approach of multi-
hierarchy text classiﬁcation. In Info-tech and Info-
net, 2001. Proceedings. ICII 2001-Beijing. 2001 In-
ternational Conferences on, volume 3, pages 95–
100. IEEE.

Andrew McCallum, Kamal Nigam, et al. 1998. A com-
parison of event models for naive bayes text classi-
ﬁcation. In AAAI-98 workshop on learning for text
categorization. Citeseer.

Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan
Nam, Honglak Lee, and Andrew Y Ng. 2011. Multi-
modal deep learning. In Proceedings of the 28th in-
ternational conference on machine learning (ICML-
11), pages 689–696.

Dani Yogatama, Chris Dyer, Wang Ling, and Phil Blun-
som. 2017. Generative and discriminative text clas-
arXiv
siﬁcation with recurrent neural networks.
preprint arXiv:1703.01898.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
In Advances in neural information pro-
siﬁcation.
cessing systems, pages 649–657.

Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis,
Thierry Artieres, George Paliouras, Eric Gaussier,
Ion Androutsopoulos, Massih-Reza Amini, and
Lshtc: A benchmark
Patrick Galinari. 2015.
arXiv preprint
for large-scale text classiﬁcation.
arXiv:1503.08581.

Michael J Quinn and Mary L Laier. 2006. Method and
apparatus for fast lookup of related classiﬁcation en-
tities in a tree-ordered classiﬁcation hierarchy. US
Patent 7,032,072.

Thomson Reuters. 2012. Web of science.

Gerard Salton and Chris Buckley. 1987. Term weight-
ing approaches in automatic text retrieval. Technical
report, Cornell University.

Carlos N Silla and Alex A Freitas. 2011. A survey
of hierarchical classiﬁcation across different appli-
cation domains. Data Mining and Knowledge Dis-
covery, 22(1-2):31–72.

Carlos N Silla Jr and Alex A Freitas. 2009. A global-
model naive bayes approach to the hierarchical pre-
diction of protein functions. In Data Mining, 2009.
ICDM’09. Ninth IEEE International Conference on,
pages 992–997. IEEE.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Simon Tong and Daphne Koller. 2001. Support vec-
tor machine active learning with applications to
text classiﬁcation. Journal of machine learning re-
search, 2(Nov):45–66.

Celine Vens, Jan Struyf, Leander Schietgat, Saˇso
Dˇzeroski, and Hendrik Blockeel. 2008. Decision
trees for hierarchical multi-label classiﬁcation. Ma-
chine Learning, 73(2):185.

Gui-Rong Xue, Dikan Xing, Qiang Yang, and Yong Yu.
2008. Deep classiﬁcation in large-scale text hierar-
In Proceedings of the 31st annual interna-
chies.
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 619–626.
ACM.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classiﬁcation.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

