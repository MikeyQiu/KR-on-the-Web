Unsupervised, Efﬁcient and Semantic Expertise Retrieval

Christophe Van Gysel
cvangysel@uva.nl

Maarten de Rijke
derijke@uva.nl

Marcel Worring
m.worring@uva.nl

University of Amsterdam, Amsterdam, The Netherlands

7
1
0
2
 
p
e
S
 
7
1
 
 
]

R

I
.
s
c
[
 
 
2
v
1
5
6
6
0
.
8
0
6
1
:
v
i
X
r
a

ABSTRACT
We introduce an unsupervised discriminative model for the task
of retrieving experts in online document collections. We exclu-
sively employ textual evidence and avoid explicit feature engineer-
ing by learning distributed word representations in an unsupervised
way. We compare our model to state-of-the-art unsupervised sta-
tistical vector space and probabilistic generative approaches. Our
proposed log-linear model achieves the retrieval performance lev-
els of state-of-the-art document-centric methods with the low infer-
ence cost of so-called proﬁle-centric approaches. It yields a statis-
tically signiﬁcant improved ranking over vector space and genera-
tive models in most cases, matching the performance of supervised
methods on various benchmarks. That is, by using solely text we
can do as well as methods that work with external evidence and/or
relevance feedback. A contrastive analysis of rankings produced
by discriminative and generative approaches shows that they have
complementary strengths due to the ability of the unsupervised dis-
criminative model to perform semantic matching.

1.

INTRODUCTION

The transition to the knowledge and information economy [1]
introduces a great reliance on cognitive capabilities [50]. It is cru-
cial for employers to facilitate information exchange and to stim-
ulate collaboration [17].
In the past, organizations would set-up
special-purpose database systems for their members to maintain
a proﬁle [7]. However, these systems required employees to be
proactive. In addition, self-assessments are known to diverge from
reality [10, 32] and document collections can quickly become prac-
tically infeasible to manage manually. Therefore, there has been an
active interest in automated approaches for constructing expertise
proﬁles [7, 61] and retrieving experts from an organization’s het-
erogeneous document repository [15]. Expert ﬁnding (also known
as expertise retrieval or expert search) addresses the task of ﬁnding
the right person with the appropriate skills and knowledge [6]. It
attempts to provide an answer to the question:

Given a topic X, who are the candidates with the most
expertise w.r.t. X?

This is an electronic version of an Article published in the 25th International
World Wide Web Conference.
c(cid:13) 2016 International World Wide Web Conference Committee.
WWW 2016, April 11–15, 2016, Montréal, Québec, Canada.
ACM 978-1-4503-4143-1/16/04.
http://dx.doi.org/10.1145/2872427.2882974.

The expertise retrieval task gained popularity in the research com-
munity during the TREC Enterprise Track [61] and has remained
relevant ever since, while broadening to social media and to track-
ing the dynamics of expertise [5, 6, 10, 19, 21–23, 47, 49, 62].
Existing methods fail to address key challenges: (1) Queries and
expert documents use different representations to describe the same
concepts [26, 34]. Term mismatches between queries and experts
[34] occur due to the inability of widely used maximum-likelihood
language models to make use of semantic similarities between words
[53]. (2) As the amount of available data increases, the need for
more powerful approaches with greater learning capabilities than
smoothed maximum-likelihood language models is obvious [63].
(3) Supervised methods for expertise retrieval [23, 47] were intro-
duced at the turn of the last decade. However, the acceleration
of data availability has the major disadvantage that, in the case of
supervised methods, manual annotation efforts need to sustain a
similar order of growth. This calls for the further development of
unsupervised methods. (4) In some expertise retrieval methods, a
language model is constructed for every document in the collection.
These methods lack efﬁcient query capabilities for large document
collections, as each query term needs to be matched against ev-
ery document [6]. Our proposed solution has a strong emphasis on
unsupervised model construction, efﬁcient query capabilities and
semantic matching between query terms and candidate experts.

Speciﬁcally, we propose an unsupervised log-linear model with
efﬁcient inference capabilities for the expertise retrieval task. We
show that our approach improves retrieval performance compared
to vector space-based and generative language models, mainly due
to its ability to perform semantic matching [34]. Our method does
not require supervised relevance judgments and is able to learn
from raw textual evidence and document-candidate associations
alone. The purpose of this work is to provide insight in how dis-
criminative language models can improve performance of core re-
trieval tasks compared to maximum-likelihood language models.
Therefore, we avoid explicit feature engineering and the incorpo-
ration of external evidence in this paper. In terms of performance,
the current best-performing formal language model [4] exhibits a
worst-case time complexity linear in the size of the document col-
lection. In contrast, the inference time complexity of our approach
is asymptotically bounded by the number of candidate experts.

Our research questions are as follows: (1) How does our dis-
criminative log-linear model compare to vector space-based meth-
ods and generative language models for the expert retrieval task in
terms of retrieval performance? (2) What can we learn regarding
the different types of errors made by generative and discriminative
language models? (3) How does the complexity of inference in
our log-linear model compare to vector-space based and generative

models? (4) How does the log-linear model handle incremental
indexing and what are its limitations?

We contribute: (i) An unsupervised log-linear model with efﬁ-
cient inference capabilities for the expertise retrieval task, together
with an open-source implementation.1 (ii) Comparison of the re-
trieval performance of the log-linear model with traditional vector
space-based models and language model methods on well-known
benchmarks. (iii) Insights in how certainty in predictions by the
log-linear model correlates with performance. (iv) In-depth analy-
sis of the inference complexity of the log-linear model. (v) Com-
parative error analysis between the semantic log-linear model and
traditional generative language models that perform exact match-
ing. (vi) Insight in the relative strengths of semantic matching and
exact matching for the expert retrieval task.

The remainder of this paper is organized as follows. In Section 2,
we brieﬂy discuss related work. Section 3 introduces the log-linear
model for the expert retrieval problem. In Section 4 we state our
research questions and detail our experimental set-up and imple-
mentations. We provide an overview of our experimental results,
followed by a discussion and analysis in Section 5. Section 6 con-
cludes the paper and discusses ideas for future work.

2. RELATED WORK

We ﬁrst discuss prior work on expert retrieval and its relation
to document retrieval. Then, we review semantic search methods.
The ideas we present in this paper are inspired by neural language
models used in automated speech recognition (ASR) and natural
language processing (NLP). Consequently, we also review work
from those ﬁelds.

2.1 Expert retrieval

Early expert retrieval systems were often referred to as expert
locator and expertise management systems [38]. These database
systems often relied on people to self-assess their expertise against
a predeﬁned set of topics [39], which is known to generate unreli-
able results [7].

With the introduction of the P@NOPTIC system [15], and later
the TREC Enterprise track [61], there has been an active research
interest in automated expertise proﬁling methods. It is useful to dis-
tinguish between proﬁle-based methods, which create a textual rep-
resentation of a candidate’s knowledge, and document-based meth-
ods, which represent candidates as a weighted combination of doc-
uments. The latter generally performs better at ranking, while the
former is more efﬁcient as it avoids retrieving all documents rele-
vant to a query [6, p. 221].

There has been much research on generative probabilistic mod-
els for expert retrieval [21, 49]. Such models have been categorized
in candidate generation models [13], topic generation models [4, 5]
and proximity-based variants [5, 54]. Of special relevance to us are
the unsupervised proﬁle-centric (Model 1) and document-centric
(Model 2) models of Balog et al. [4], which focus on raw textual ev-
idence without incorporating collection-speciﬁc information (e.g.,
query modeling, document importance or document structure). Su-
pervised discriminative models [23, 47, 60] are preferred when
query-candidate relevance pairs are available for training. Unlike
their generative counterparts these models have no issue combining
complex and heterogeneous features (e.g., link-based features, doc-
ument importance features, etc.); they resemble Learning to Rank
(L2R) methods for document retrieval [6, 35]. However, a lack
of training data may greatly hinder their applicability [6, p. 179].
Beyond unsupervised generative and supervised discriminative ap-

1https://github.com/cvangysel/SERT

proaches, there are graph-based approaches based on random walks
[55] and voting-based approaches based on data fusion [36]. De-
martini et al. [19] propose a vector space-based method for the en-
tity ranking task; their framework extends vector spaces operating
on documents to entities. See [6] for a survey on the topic.

2.2 Latent semantic models for document re-

trieval

Li and Xu [34] note that the query document mismatch poses the
most critical challenge in search. Semantic matching is an impor-
tant attempt to remedy this problem. There has been much work
on bridging the semantic gap for various different tasks [19, 26, 28,
30, 31, 41, 48, 53]. Expertise and document retrieval [6, p. 224] are
closely related as performance of the latter can greatly impact that
of the former [37].

Latent Semantic Models (LSM) ﬁrst became popular through
the introduction of Latent Semantic Indexing (LSI) [18], followed
by probabilistic LSI (pLSI) [27]. LSMs based on neural networks
[28, 53, 58] emerged in the last decade. Salakhutdinov and Hinton
[53] employ unsupervised deep auto-encoders to map documents to
bit patterns using semantic addressing. Huang et al. [28] perform
semantic matching of documents and queries by leveraging click-
through data optimizing for Web document ranking. Further, deep
models [12, 20] have been proposed to learn to rank [35].

2.3 Neural language modeling

Large-vocabulary neural probabilistic language models for mod-
eling word sequence distributions have become very popular re-
cently [8, 43, 44]. These models learn continuous-valued distributed
representations for words, also known as embeddings [42, 45, 48],
in order to ﬁght the curse of dimensionality and increase general-
ization by introducing the expectation that similar word vectors sig-
nify semantically or syntactically similar words. Recurrent neural
language models have shown to perform well in ASR [40]. Col-
lobert et al. [14] propose a uniﬁed neural network architecture for
various NLP tasks. Even more recently, there has been a surge in
multimodal neural language models [31], which lend themselves to
the task of automated image captioning.

What we add on top of the related work described above is the
following.
In this work we model the conditional probability of
the expertise of a candidate given a single query term (contrary
to binary relevance given a character-based n-gram [28]). In the
process we learn a distributed vector representation (similar to LSI,
pLSI and semantic hashing) for both words and candidates such
that nearby representations indicate semantically similar concepts.
We propose a log-linear model that is similar to neural language
models. The important difference is that we predict a candidate
expert instead of the next word. To the best of our knowledge,
we are the ﬁrst to propose such a solution. We employ an embed-
ding layer in our shallow model for the same reasons as mentioned
above: we learn continuous word representations that incorporate
semantic and syntactic similarity tailored to an expert’s domain.

3. A LOG-LINEAR MODEL FOR

of words w1, . . . , wk we have:

EXPERT SEARCH

In the setting of this paper we have a document collection D and
a predeﬁned set of candidate experts C (entities to be retrieved).
Documents d ∈ D are represented as a sequence of words w1, . . . ,
w|d| originating from a vocabulary V , where wi ∈ V and the oper-
ator |·| denotes the document length in tokens. For every document
d ∈ D we write Cd to denote the set of candidates c ∈ C asso-
ciated with it (i.e., C = (cid:83)
d∈D Cd). These document-candidate
associations can be obtained explicitly from document meta-data
(e.g., the author of an e-mail) or implicitly by mining references
to candidates from the document text. Notice that some documents
might not be associated with any candidate. When presented with a
query q of constituent terms t1, . . . , t|q|, the expert retrieval task is
to return a list of candidates ρ(C) ordered according to topical ex-
pertise. We generate this ranking using a relatively shallow neural
network which directly models P (c | q).

We employ vector-based distributed representations [26], for both
words (i.e., word embeddings) and candidate experts, in a way that
motivates the unsupervised construction of features that express
regularities of the expertise ﬁnding domain. These representations
can capture the similarity between concepts (e.g., words and can-
didate experts) by the closeness of their representations in vector
space. That is, concepts with similar feature activations are inter-
preted by the model as being similar, or even interchangeable.

3.1 The model

To address the expert search task, we model P (cj | q) and rank
candidates cj accordingly for a given q. We propose an unsuper-
vised, discriminative approach to obtain these probabilities. We
construct our model solely from textual evidence: we do not re-
quire query-candidate relevance assessments for training and do not
consider external evidence about the corpus (e.g., different weight-
ings for different sub-collections), the document (e.g., considering
certain parts of the document more useful) nor link-based features.
Let e denote the size of the vector-based distributed representa-
tions of both words in V and candidate experts in C. These repre-
sentations will be learned by the model using gradient descent [42]
(Section 3.2). For notational convenience, we write P (c | ·) for the
(conditional) probability distribution over candidates, which is the
result of vector arithmetic. We deﬁne the probability of a candidate
cj given a single word wi ∈ V as the log-linear model

P (c | wi) =

exp (Wc · (Wp · vi) + bc) ,

(1)

1
Z1

where Wp is the e × |V | projection matrix that maps the one-hot
representation (i.e., 1-of-|V |) of word wi, vi, to its e-dimensional
distributed representation, bc is a |C|-dimensional bias vector and
Wc is the |C| × e matrix that maps the word embedding to an un-
normalized distribution over candidates C, which is then normal-
ized by Z1 = (cid:80)|C|
j=1 [exp (Wc · (Wp · vi) + bc)]j. If we consider
Bayes’ theorem, the transformation matrix Wc and bias vector bc
can be interpreted as the term log-likelihood log P (wi | c) and
candidate log-prior log P (c), respectively. The projection matrix
Wp attempts to soften the curse of dimensionality introduced by
large vocabularies V and maps words to word feature vectors [8].
Support for large vocabularies is crucial for retrieval tasks [28, 53].
We then assume conditional independence of a candidate’s ex-
pertise given an observation of data (i.e., a word). Given a sequence

P (c | w1, . . . , wk) =

˜P (c | w1, . . . , wk) =

P (c | wi)

1
Z2

1
Z2

(cid:32) k

(cid:88)

1
Z2

k
(cid:89)

i=1
(cid:33)

=

exp

log (P (c | wi))

(2)

(cid:16)(cid:80)k

j=1 exp

i=1
where ˜P (c | w1, . . . , wk) denotes the unnormalized score and
(cid:17)
Z2 = (cid:80)|C|
i=1 log (P (cj | wi))
is a normalizing term.
The transformation to log-space in (2) is a well-known trick to pre-
vent ﬂoating point underﬂow [46, p. 445]. Given (2), inference is
straight-forward. That is, given query q = t1, . . . , tk, we compute
P (c | t1, . . . , tk) and rank the candidate experts in descending
order of probability.

Eq. 1 deﬁnes a neural network with a single hidden layer. We
can add additional layers. Preliminary experiments, however, show
that the shallow log-linear model (1) performs well-enough in most
cases. Only for larger data sets did we notice a marginal gain from
adding an additional layer between projection matrix Wp and the
softmax layer over C (Wc and bias bc), at the expense of longer
training times and loss of transparency.

3.2 Parameter estimation

The matrices Wp, Wc and the vector bc in (1) constitute the
parameters of our model. We estimate them using error back prop-
agation [51] as follows. For every document dj ∈ D we construct
an ideal distribution over candidates p = P (c | dj) based on the
document-candidate associations Cdj such that

P (c | dj) =

(cid:40) 1
|Cdj
0,

| ,

c ∈ Cdj

c (cid:54)∈ Cdj

We continue by extracting n-grams where n remains ﬁxed during
training. For every n-gram w1, . . . , wn generated from document
d we compute ˜p = P (c | w1, . . . , wn) using (2). During model
constructing we then optimize the cross-entropy H(p, ˜p) (i.e., the
joint probability of the training data if |Cdj | = 1 for all j) using
batched gradient descent. The loss function for a single batch of m
instances with associated targets (p(i), ˜p(i)) is as follows:

L(Wp, Wc, bc)

=

1
m

+

m
(cid:88)

i=1

λ
2m

|dmax|
|d(i)|
(cid:32)

(cid:88)

i,j

H(p(i), ˜p(i))

W 2

pi,j +

W 2

ci,j

(cid:88)

i,j

= −

|dmax|
|d(i)|

|C|
(cid:88)

j=1

1
m

m
(cid:88)

i=1
(cid:32)

+

λ
2m

(cid:88)

i,j

W 2

pi,j +

W 2

ci,j

,

(cid:88)

i,j

(cid:33)

(cid:33)

(3)

P (cj | d(i)) log

(cid:16)
P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

where d(i) refers to the document from which n-gram w(i)
1 , . . . , w(i)
n
was extracted, dmax = arg maxd∈D |d| indicates the longest doc-
ument in the collection, and λ is a weight regularization parameter.
The update rule for a particular parameter θ (Wp, Wc or bc) given
a single batch of size m is:

θ(t+1) = θ(t) − α(t) (cid:12)

∂L(Wp

(t), bc

(t))

(t), Wc
∂θ

,

(4)

where α(t) and θ(t) denote the per-parameter learning rate and pa-
rameter θ at time t, respectively. The learning rate α consists of
the same number of elements as there are parameters; in the case
of a global learning rate, all elements of α are equal to each other.
The derivatives of the loss function (3) are given in the Appendix.
In the next section we will discuss our experimental setup, fol-
lowed by an overview of our experimental results and further anal-
ysis in Section 5.

4. EXPERIMENTAL SETUP

4.1 Research questions

As indicated in the introduction, we seek to answer the following

research questions:
RQ1 How does our discriminative log-linear model compare to
vector space-based methods and generative language models
for the expert retrieval task in terms of retrieval performance?
In particular, how does the model perform when compared against
vector space-based (LSI and TF-IDF) and generative approaches
(proﬁle-centric Model 1 and document-centric Model 2)?
RQ2 What can we learn regarding the different types of errors

made by generative and discriminative language models?

Does the best-performing generative model simply perform slightly
better on the topics for which the other models perform decent as
well, or do they make very different errors? If the latter holds,
an ensemble of the rankings produced by both model types might
exceed performance of the individual rankings.
RQ3 How does the complexity of inference in our log-linear model
compare to vector-space based and generative models?
The worst-case inference cost of document-centric models makes
them unattractive in online settings where the set of topics is not
deﬁned beforehand and the document collection is large. Proﬁle-
centric methods are preferred in such settings as they infer from
one language model per candidate expert for every topic (i.e., a
pseudo-document consisting of a concatenation of all documents
associated with an expert) [6]. Vector space-based methods [19]
have similar problems due to the curse of dimensionality [29] and
consequently their inferential time complexity is likewise asymp-
totically bounded by the number of experts.
RQ4 How does the log-linear model handle incremental indexing

and what are its limitations?

4.2 Benchmarks

The proposed method is applicable in the setting of the Expert
Search task of the TREC Enterprise track from 2005 to 2008 [61].
We therefore evaluate on the W3C and CERC benchmarks released
by the track. The W3C dataset [16] is a crawl of the W3C’s sites
in June 2004 (mailing lists, web pages, etc.). The CSIRO Enter-
prise Research Collection (CERC) [2] is a dump of the intranet
of Australia’s national science agency. Additionally, we evaluate
our method on a smaller, more recent benchmark based on the em-
ployee database of Tilburg University (TU) [10], which consists of
bi-lingual, heterogeneous documents. See Table 1.

Mining document-candidate associations and how they inﬂuence
performance has been extensively covered in previous work [4, 6]
and is beyond the scope of this work. For TU, the associations
are part of the benchmark. For W3C, a list of possible candidates
is given and we extract the associations ourselves by performing
a case-insensitive match of full name or e-mail address [4]. For
CERC, we make use of publicly released associations [3].

As evaluation measures we use Mean Average Precision (MAP),
Mean Reciprocal Rank (MRR), Normalized Discounted Cumula-

tive Gain at rank 100 (NDCG@100) and Precision at rank 5 (P@5)
and rank 10 (P@10).

4.3 Baselines

We compare our approach to existing unsupervised methods for
expert retrieval that solely rely on textual evidence and static doc-
ument-candidate associations. (1) Demartini et al. [19] propose a
generic framework to adapt vector spaces operating on documents
to entities. We compare our method to TF-IDF (raw frequency and
inverse document frequency) and LSI (300 latent topics) variants
of their vector space model for entity ranking (using cosine sim-
ilarity). (2) In terms of language modeling, Balog et al. [4] pro-
pose two models for expert ﬁnding based on generative language
models. The ﬁrst takes a proﬁle-centric approach comparing the
language model of every expert to the query, while the second is
document-centric. We consider both models with different smooth-
ing conﬁgurations: Jelinek-Mercer (jm) smoothing with λ = 0.5
[4] and Dirichlet (d) smoothing with β equal to the average doc-
ument length [5] (see Table 1). Signiﬁcance of results produced
by the baselines (compared to our method) is determined using a
two-tailed paired randomization test [59].

4.4

Implementation details

The vocabulary V is constructed from each corpus by ignoring
punctuation, stop words and case; numbers are replaced by a nu-
merical placeholder token. During our experiments we prune V
by only retaining the 216 most-frequent words so that each word
can be encoded by a 16-bit unsigned integer. Incomplete n-gram
instances are padded by a special-purpose token.

In terms of parameter initialization, we sample the initial matri-

ces Wc and Wp (1) uniformly in the range
(cid:34)
(cid:114) 6.0

(cid:114) 6.0

(cid:35)

−

,

m + n

m + n

for an m × n matrix, as this initialization scheme improves model
training convergence [25], and take the bias vector bc to be null.
The projection layer Wp is initialized with pre-trained word repre-
sentations trained on Google News data [41]; the number of word
features is set to e = 300, similar to pre-trained representations.

We used adadelta (ρ = 0.95, (cid:15) = 10−6) [64] with batched gra-
dient descent (m = 1024) and weight decay λ = 0.01 during
training on NVidia GTX480 and NVidia Tesla K20 GPUs. We only
iterate once over the entire training set for each experiment.

5. RESULTS AND DISCUSSION

We start by giving a high-level overview of our experimental re-
sults and then address issues of scalability, provide an error analysis
and discuss the issue of incremental indexing.

5.1 Overview of experimental results

We evaluate the log-linear model on the W3C, CERC and TU
benchmarks (Section 4.2). During training we extract non-overlap-
ping n-grams for the W3C and CERC benchmarks and overlapping
n-grams for the TU benchmark. As the TU benchmark is consider-
ably smaller, we opted to use overlapping n-grams to counter data
sparsity. The architecture of each benchmark model (e.g., num-
ber of candidate experts) is inherently speciﬁed by the benchmarks
themselves (see Table 1). However, the choice of n-gram size dur-
ing training remains open. Errors for input w1, . . . , wn are propa-
gated back through Wc until the projection matrix Wp is reached;
if a single word wi causes a large prediction error, then this will in-
ﬂuence its neighboring words w1, . . . , wi−1, wi+1, . . . , wn as well.

Table 1: An overview of the three datasets (W3C, CERC and TU) used for evaluation and analysis.

Number of documents
Average document lengtha

Number of candidatesb

Number of document-candidate associations
Number of documents (with Cd > 0)
Number of associations per documentc
Number of associations per candidate

Queries

W3C

CERC

331,037

1,237.23

370,715

460.48

715

200,939
93,826

3,479

236,958
123,934

TU

31,209
2,454.93

977

36,566
27,834

2.14 ± 3.29
281.03 ± 666.63

1.91 ± 3.70
68.11 ± 1,120.74

1.13 ± 0.39
37.43 ± 61.00

49
50

(2005)
(2006)

50
77

(2007)
(2008)

1,662
1,266

(GT1)
(GT5)

a Measured in number of tokens.
b Only candidates with at least a single document association are considered.
c Only documents with at least one association are considered.

This allows the model to learn continuous word representations tai-
lored to the expert retrieval task and the benchmark domain.

A larger window size has a negative impact on batch throughput
during training. We are thus presented with the classic trade-off
between model performance and construction efﬁciency. Notice,
however, that the number of n-grams decreases as the window size
increases if we extract non-overlapping instances. Therefore, larger
values of n lead to faster wall-clock time model construction for the
W3C and CERC benchmarks in our experiments.

We sweep over the window width n = 2i (0 ≤ i < 6) for all
three benchmarks and their corresponding relevance assessments.
We report MAP and MRR for every conﬁguration (see Figure 1).
We observe a signiﬁcant performance increase between n = 1
and n = 2 on all benchmarks, which underlines the importance
of the window size parameter. The increase in MAP implies that
the performance achieved is not solely due to initialization with
pre-trained representations (Section 4.4), but that the model efﬁ-
ciently learns word representations tailored to the problem domain.
The highest MAP scores are attained for relatively low n. As n
increases beyond n = 8 a gradual decrease in MAP is observed on
all benchmarks. In our remaining experiments we choose n = 8
regardless of the benchmark.

Words wi that mainly occur in documents associated with a par-
ticular expert are learned to produce distributions P (c | wi) with
less uncertainty than words associated with many experts in (1).
The product of P (c | wi) in (2) aggregates this expertise evidence
generated by query terms. Hence, queries with strong evidence for
a particular expert should be more predictable than very generic
queries. To quantify uncertainty we measure the normalized en-
tropy [56] of P (c | q):

η(c | q) = −

p(cj | q) log(p(cj | q)).

(5)

1
log(|C|)

|C|
(cid:88)

j=1

Equation 5 can be interpreted as a similarity measure between the
given distribution and the uniform distribution. Importantly, Fig-
ure 2 shows that there is a statistically signiﬁcant negative correla-
tion between query-wise normalized entropy and average precision
for all benchmarks.

Table 2 presents a comparison between the log-linear model and
the various baselines (Section 4.3). Our unsupervised method sig-
niﬁcantly (p < 0.01) outperforms the LSI-based method consis-
tently. In the case of the TF-IDF method and the proﬁle-centric

generative language models (Model 1), we always perform bet-
ter and statistical signiﬁcance is achieved in the majority of cases.
The document-centric language models (Model 2) perform slightly
better than our method on two (out of six) benchmarks in terms
of MAP and NDCG@100:(1) For the CERC 2007 assessment we
match performance of the document-centric generative model with
Jelinek-Mercer smoothing. (2) For TU GT1 the generative coun-
terpart seems to outperform our method.

Notice that over all assessments, the log-linear model consis-
tently outperforms all proﬁle-centric approaches and is only chal-
lenged by the smoothed document-centric approach. In addition,
for the precision-based measures (P@k and MRR), the log-linear
model consistently outperforms all other methods we compare to.
Next, we turn to a topic-wise comparative analysis of discrimi-
native and generative models. After that, we analyze the scalability
and efﬁciency of the log-linear model and compare it to that of the
generative counterparts, and address incremental indexing.

5.2 Error analysis

How does our log-linear model achieve its superior performance
over established generative models? Figure 3 depicts the per-topic
differences in average precision between the log-linear model and
Model 2 (with Jelinek-Mercer smoothing) on all benchmarks. For
each plot, the vertical bars with a positive AP difference correspond
to test topics for which the log-linear model outperforms Model 2
and vice versa for bars with a negative AP difference.

The beneﬁt gained from the projection matrix Wp is two-fold.
First, it avoids the curse of dimensionality introduced by large vo-
cabularies. Second, term similarity with respect to the expertise
domain is encoded in latent word features. When examining words
nearby query terms in the embedding space, we found words to be
related to the query term. For example, word vector representations
of xml and nonterminal are very similar for the W3C benchmark (l2
norm). This can be further observed in Figure 1: log-linear models
trained on single words perform signiﬁcantly worse compared to
those that are able to learn from neighboring words.

We now take a closer look at the topics for which the log-linear
model outperforms Model 2 and vice versa. More speciﬁcally, we
investigate textual evidence related to a topic and whether it is con-
sidered relevant by the benchmark. For the log-linear model, we ex-
amine terms nearby topic terms in Wp (l2-norm), as these terms are
considered semantically similar by the model and provide a means
for semantic matching. For every benchmark, we ﬁrst consider top-

Table 2: Evaluation results for models trained on the W3C, CERC and TU benchmarks. Sufﬁxes (d) and (jm) denote Dirichlet and
Jelinek-Mercer smoothing, respectively (Section 4.3). Signiﬁcance of results is determined using a two-tailed paired randomization
test [59] (∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) with respect to the log-linear model (adjusted using the Benjamini-Hochberg
procedure for multiple testing [9]).

W3C

MAP NDCG@100 MRR

P@5 P@10 MAP

2005

2006
NDCG@100 MRR

CERC

MAP NDCG@100 MRR

P@5 P@10 MAP

2008
NDCG@100 MRR

0.135
LSI
0.243
TF-IDF
0.192
Model 1 (d)
0.190
Model 1 (jm)
0.198
Model 2 (d)
0.211
Model 2 (jm)
Log-linear (ours) 0.248

0.031
LSI
0.332
TF-IDF
0.287
Model 1 (d)
0.278
Model 1 (jm)
0.352
Model 2 (d)
0.361
Model 2 (jm)
Log-linear (ours) 0.344

0.095
LSI
0.216
TF-IDF
0.171
Model 1 (d)
0.189
Model 1 (jm)
0.154
Model 2 (d)
0.234
Model 2 (jm)
Log-linear (ours) 0.219

0.266
0.426
0.358
0.352
0.369
0.380
0.444

0.107
0.486
0.427
0.420
0.495
0.500
0.493

0.205
0.356
0.308
0.325
0.284
0.370
0.356

0.192 0.196
0.306
0.384 0.350
0.541
0.276 0.266
0.433
0.272 0.276
0.390
0.288 0.272
0.429
0.451
0.332 0.296
0.618∗ 0.412 0.361

2007

0.060
0.463
0.384
0.384
0.454
0.467
0.513

GT1

0.016 0.014
0.196 0.141
0.156 0.096
0.156 0.084
0.180 0.138
0.192 0.138
0.215 0.150

0.153
0.324
0.258
0.277
0.228
0.342
0.351

0.060 0.051
0.131 0.097
0.103 0.082
0.112 0.085
0.087 0.070
0.136 0.101
0.145∗ 0.105

0.245
0.343
0.321
0.311
0.261
0.260
0.484∗∗∗

0.038
0.269
0.181
0.170
0.264
0.274
0.342∗∗∗

0.097
0.233
0.241
0.231
0.191
0.253
0.287∗∗∗

0.371
0.531
0.491
0.483
0.419
0.423
0.667∗∗∗

0.482
0.650
0.635
0.596
0.551
0.599
0.833∗∗∗

0.099
0.465
0.355
0.347
0.461
0.463
0.519∗∗

0.106
0.525
0.388
0.339
0.510
0.517
0.656∗∗

0.208
0.378
0.385
0.373
0.334
0.394
0.425∗∗∗

0.129
0.288
0.292
0.271
0.233
0.302
0.363∗∗∗

TU

MAP NDCG@100 MRR

P@5 P@10 MAP

GT5
NDCG@100 MRR

P@5

P@10

0.287
0.492
0.478
0.502
0.441
0.449
0.713∗∗∗

0.338
0.498
0.449
0.437
0.404
0.429
0.644∗∗∗

P@5

0.042
0.332
0.200
0.181
0.281
0.278
0.381∗

P@10

0.055
0.277
0.172
0.159
0.244
0.239
0.299

P@5

P@10

0.043
0.108
0.109
0.100
0.084
0.108
0.134∗∗∗

0.036
0.079
0.081
0.075
0.065
0.081
0.092∗∗∗

ics where exact matches (Model 2) perform best, followed by ex-
amples which beneﬁt from semantic matching (log-linear model).
Topic identiﬁers are between parentheses.

W3C Topics P3P speciﬁcation and CSS3 (EX8 and EX69, respec-
tively) should return candidates associated with the deﬁnition
of these standards. The log-linear model, however, considers
these close to related technologies such as CSS2 for CSS3
and UTF-8 for P3P. Semantic matching works for topics Se-
mantic Web Coordination and Annotea server protocol (EX1
and EX103), where the former is associated with RDF li-
braries, RDF-related jargon and the names of researchers in
the ﬁeld, while the latter is associated with implementations
of the protocol and the maintainer of the project.

CERC For CSIRO, topic nanohouse (CE-035) is mentioned in
many irrelevant contexts (i.e., spam) and therefore seman-
tic matching fails. The term ﬁsh oil (CE-126) is quickly as-
sociated with different kinds of ﬁsh, oils and organizations
related to marines and ﬁsheries. On the other hand, we ob-
serve sensor networks (CE-018) to be associated with sen-
sor/networking jargon and sensor platforms. Topic forensic
science workshop (CE-103) expands to syntactically-similar
terms (e.g., plural), the names of science laboratories and ref-
erences

support/law-protection

organizations.

to

TU The TU benchmark contains both English and Dutch textual
evidence. Topics sustainable tourism and interpolation (1411
and 4882) do not beneﬁt from semantic matching due to a
semantic gap: interpolation is associated with the polyno-
mial kind while the relevance assessments focus on stochas-
tic methods. Interestingly, for the topic law and informati-
zation/computerization (1719) we see that the Dutch trans-
lation of law is very closely related. Similar terms to in-
formatization are, according to the log-linear model, Dutch
words related to cryptography. Similar dynamics are at work
for legal-political space (12603), where translated terms and
semantic-syntactic relations aid performance.

In order to further quantify the effect of the embedding matrix Wp,
we artiﬁcially expand benchmark topic terms by k nearby terms.
We then examine how the performance of a proﬁle-centric gener-
ative language model [4, Model 1] evolves for different values of
k (Figure 4). The purpose of this analysis is to provide further
insight in the differences between maximum-likelihood language
models and the log-linear model. Figure 4 shows that, for most
benchmarks, MAP increases as k goes up. Interestingly enough,
the two benchmarks that exhibit a decrease in MAP for larger k
(CERC 2007 and TU GT1) are likewise those for which generative
language models outperform the log-linear model in Table 2. This
suggests that the CERC 2007 and TU GT1 benchmarks require ex-

(a) W3C

(b) CERC

(c) TU

(d) W3C

(e) CERC

(f) TU

Figure 1: Sensitivity analysis for window size (n-gram) during parameter estimation (3) for W3C, CERC and TU benchmarks.

act term matching, while the remaining four benchmarks beneﬁt
greatly from the semantic matching provided by our model.

The per-topic differences suggest that Model 2 and the log-linear
model make very different errors: Model 2 excels at retrieving ex-
act query matches, while the log-linear model performs seman-
tic matching. Based on these observations we hypothesize that a
combination of the two approaches will raise retrieval performance
even further. To test this hypothesis, we propose a simple ensem-
ble of rankings generated by Model 2 and the log-linear model by
re-ranking candidates according to the multiplicatively-combined
reciprocal rank:

rankensemble(cj, qi) ∝

1
rankmodel 2(cj, qi)

·

1
ranklog-linear(cj, qi)

, (6)

where rankM (cj, qi) denotes the position of candidate cj in a rank-
ing generated by model M for answering query qi. Equation (6) is
equivalent to performing data fusion using CombSUM [57] where
the scores are given by the logarithm of the reciprocal ranks of the
experts. Table 3 compares the result of this ensemble to that of its
constituents. Compared to the supervised methods of Fang et al.
[23], we conclude that our fully unsupervised ensemble matches
the performance of their method on the CERC 2007 benchmark
and outperforms their method on the W3C 2005 benchmark. The
superior performance of the ensemble suggests the viability of hy-
brid methods that combine semantic and exact matching.

5.3 Scalability and efﬁciency

Inference in the log-linear model is expressed in linear algebra
operations (Section 3). These operations can be efﬁciently per-
formed by highly optimized software libraries and special-purpose
hardware (i.e., GPUs). But the baseline methods against which we
compare do not beneﬁt from these speed-ups. Furthermore, many
implementation-speciﬁc details and choice of parameter values can

inﬂuence runtime considerably (e.g. size of the latent representa-
tions). Therefore, we opt for a theoretical comparison of the infer-
ence complexity of the log-linear model and compare these to the
baselines (Section 4.3).

The log-linear model generates a ranking of candidate experts by
straight-forward matrix operations. The look-up operation in the
projection matrix Wp occurs in constant time complexity, as the
multiplication with the one-hot vector vi comes down to selecting
the i-th column from Wp. Multiplication of the |C| × e matrix
Wc with the e-dimensional word feature vector exhibits O(|C| · e)
runtime complexity. If we consider addition of the bias term and
division by the normalizing function Z1, the time complexity of (1)
becomes

O(|C| · (e + (e − 1))
(cid:125)
(cid:123)(cid:122)
(cid:124)
matrix-vector multiplication

+ |C|
(cid:124)(cid:123)(cid:122)(cid:125)
bias term

+ 2 · |C| − 1
(cid:123)(cid:122)
(cid:125)
Z1

(cid:124)

).

Notice, however, that the above analysis considers sequential exe-
cution. Modern computing hardware has the ability to parallelize
common matrix operations [24, 33]. The number of candidate ex-
perts |C| is the term that impacts performance most in the log-linear
model (under the assumption that |C| (cid:29) e).

If we consider n terms, where n is the query length during infer-
ence or the window size during training, then the complexity of (2)
becomes

O(n · |C| · (2 · e − 1) + n · (3 · |C| − 1)
(cid:125)

(cid:124)

(cid:123)(cid:122)
n forward-passes

+ (n − 1) · |C|
(cid:125)

(cid:123)(cid:122)
factor product

(cid:124)

)
+ 2 · |C| − 1
(cid:123)(cid:122)
(cid:125)
Z2

(cid:124)

Notice that Z2 does not need to be computed during inference as it
does not affect the candidate expert ranking.

(a) W3C (R = −0.39∗∗∗)

(b) CERC (R = −0.44∗∗∗)

(c) TU (R = −0.30∗∗∗)

Figure 2: Scatter plot of the normalized entropy of distribution P (c | q) (2) returned by the log-linear model and per-query av-
erage precision for W3C, CERC and TU benchmarks. Pearson’s R and associated p-value (two-tailed paired permutation test:
∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) are between parentheses. The depicted linear ﬁt was obtained using an ordinary least squares
regression.

Table 3: Comparison of Model 2, the log-linear model and an ensemble of the former on W3C, CERC and TU benchmarks. Signiﬁ-
cance of results is determined using a two-tailed paired randomization test [59] (∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) with respect to
the ensemble ranking (adjusted using the Benjamini-Hochberg procedure for multiple testing [9]).

2005
NDCG@100 MRR

0.380
0.444
0.479∗∗

0.451
0.618
0.668

2007
NDCG@100 MRR

0.500
0.493
0.589∗∗∗

0.467
0.513
0.627∗∗∗

GT1
NDCG@100 MRR

W3C

Model 2 (jm)
Log-linear (ours)
Ensemble

CERC

Model 2 (jm)
Log-linear (ours)
Ensemble

TU

Model 2 (jm)
Log-linear (ours)
Ensemble

MAP

0.211
0.248
0.291∗∗∗

MAP

0.361
0.344
0.452∗∗

MAP

0.234
0.219
0.271∗∗∗

P@5

0.332
0.412
0.440

P@5

0.192
0.215
0.248∗

P@10

0.296
0.361
0.378

P@10

0.138
0.150
0.160

MAP

0.260
0.484∗∗∗
0.433

MAP

0.274
0.342
0.395∗∗∗

2006
NDCG@100 MRR

0.423
0.667∗∗
0.634

0.599
0.833
0.825

2008
NDCG@100 MRR

0.463
0.519
0.593∗∗∗

0.517
0.656
0.716

GT5
NDCG@100 MRR

P@5

P@10

0.449
0.713∗∗
0.657

0.429
0.644∗∗
0.586

P@5

P@10

0.278
0.381
0.459∗∗

0.239
0.299
0.357∗∗∗

P@5

P@10

MAP

P@5

P@10

0.370
0.356
0.417∗∗∗

0.342
0.351
0.403∗∗∗

0.136
0.145
0.165∗∗∗

0.101
0.105
0.121∗∗∗

0.253
0.287
0.331∗∗∗

0.394
0.425
0.477∗∗∗

0.302
0.363
0.402∗∗∗

0.108
0.134
0.156∗∗∗

0.081
0.092
0.105∗∗∗

In terms of space complexity, parameters Wp, Wc and bc, in ad-
dition to the intermediate results, all require memory space propor-
tional to their size. Considering (2) for a sequence of k words and
batches of m instances, we require O(m·k·|C|) ﬂoating point num-
bers for every forward-pass to ﬁt in-memory. While such an upper
bound seems reasonable by modern computing standards, it is a
severely limiting factor when considering large-scale communities
and while utilizing limited-memory GPUs for fast computation.

The inferential complexity of the vector space-based models for
entity retrieval [19] depends mainly on the dimensionality of the
vectors and the number of candidate experts. The dimensionality
of the latent entity representations is too high for efﬁcient nearest
neighbor retrieval [29] due to the curse of dimensionality. There-
fore, the time complexity for the LSI- and TF-IDF-based vector
space models are respectively O(γ · |C|) and O(|V | · |C|), where
γ denotes the number of latent topics in the LSI-based method. As
hyperparameters e and γ both indicate the dimensionality of la-
tent entity representations, the time complexity of the LSI-based
method is comparable to that of the log-linear model. We note that
|V | (cid:29) |C| for all benchmarks (|V | is between 18 to 91 times larger
than |C|) we consider in this paper and therefore conclude that the
TF-IDF method loses to the log-linear model in terms of efﬁciency.

Compared to the unsupervised generative models of Balog et al.,
we have the proﬁle-centric Model 1 and the document-centric Mo-
del 2 with inference time complexity O(n · |C|) and O(n · |D|),
respectively, with |D| (cid:29) |C|. In the previous section we showed
that the log-linear model always performs better than Model 1 and
nearly always outperforms Model 2. Hence, our log-linear model
generally achieves the expertise retrieval performance of Model 2
(or higher) at the complexity cost of Model 1 during inference.

5.4

Incremental indexing

Existing unsupervised methods use well-understood maximum-
likelihood language models that support incremental indexing. We
now brieﬂy discuss the incremental indexing capabilities of our
proposed method. Extending the set of candidate experts C re-
quires the log-linear model to be re-trained from scratch as it changes
the topology of the network. Moreover, every document associated
with a candidate expert is considered as a negative example for all
other candidates. While it is possible to reiterate over all past doc-
uments and only learn an additional row in matrix Wc, the ﬁnal
outcome is unpredictable.

If we consider a stream of documents instead of a predeﬁned set
D, the log-linear model can be learned in an online fashion. How-
ever, stochastic gradient descent requires that training examples are

(a) W3C 2005

(b) CERC 2007

(c) TU GT1

(d) W3C 2006

(e) CERC 2008

(f) TU GT5

Figure 3: Difference of average precision between log-linear model and Model 2 [4] with Jelinek-Mercer smoothing per topic for
W3C, CERC and TU benchmarks.

picked at random such that the batched update rule (4) behaves like
the empirical expectation over the full training set [11]. While we
might be able to justify the assumption that documents arrive ran-
domly, the n-grams extracted from those documents clearly violate
this requirement.

Considering a stream of documents leads to the model forgetting
expertise evidence as an (artiﬁcial) shift in the underlying distribu-
tion of the training data occurs. While such behavior is undesirable
for the task considered in this paper, it might be well-suited for
temporal expert ﬁnding [22, 52], where expertise drift over time
is considered. However, temporal expertise ﬁnding is beyond the
scope for this paper and left for future work.

6. CONCLUSIONS

We have introduced an unsupervised discriminative, log-linear
model for the expert retrieval task. Our approach exclusively em-
ploys raw textual evidence. Future work can focus on improving
performance by feature engineering and incorporation of external
evidence. Furthermore, no relevance feedback is required during
training. This renders the model suitable for a broad range of ap-
plications and domains.

We evaluated our model on the W3C, CERC and TU bench-
marks and compared it to state-of-the-art vector space-based en-
tity ranking (based on LSI and TF-IDF) and language modeling
(proﬁle-centric and document-centric) approaches. The log-linear
model combines the ranking performance of the best maximum-
likelihood language modeling approach (document-centric) with
inference time complexity linear in the number of candidate ex-
perts. We observed a notable increase in precision over existing
methods. Analysis of our model’s output reveals a negative corre-
lation between the per-query performance and ranking uncertainty:
higher conﬁdence (i.e., lower entropy) in the rankings produced by
our approach often occurs together with higher rank quality.

An error analysis of the log-linear model and traditional lan-
guage models shows that the two make very different errors. These
errors are mainly due to the semantic gap between query intent and
the raw textual evidence. Some benchmarks expect exact query
matches, others are helped by our semantic matching. An ensemble
of methods employing exact and semantic matching generally out-
performs the individual methods. This observation calls for further
research in the area of combining exact and semantic matching.

One current limitation of our work is its scalability with respect
to the number of candidate experts. We have started investigating
trade-offs between model performance and time/space complexity.
In the future we hope to apply scalable variants of this method on
large-scale social media communities, for the purpose of determin-
ing topic ownership. While in this work we focus on expertise re-
trieval, the ideas we proposed can easily be transferred to the more
general entity retrieval task. Moreover, our approach is likely to
be applicable to authorship attribution and various other entity re-
trieval and prediction tasks.

Acknowledgments. We thank Isaac Sijaranamual, Manos Tsagkias, Tom
Kenter, Zhaochun Ren, Ke Tran and Katja Hoffman for their useful com-
ments and insights.

This research was supported by Amsterdam Data Science, the Dutch
national program COMMIT, Elsevier, the European Community’s Seventh
Framework Programme (FP7/2007-2013) under grant agreement nr 312827
(VOX-Pol), the ESF Research Network Program ELIAS, the Royal Dutch
Academy of Sciences (KNAW) under the Elite Network Shifts project, the
Microsoft Research Ph.D. program, the Netherlands eScience Center un-
der project number 027.012.105, the Netherlands Institute for Sound and
Vision, the Netherlands Organisation for Scientiﬁc Research (NWO) under
project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.-
930, CI-14-25, SH-322-15, 652.002.001, 612.001.551, the Yahoo Faculty
Research and Engagement Program, and Yandex. All content represents the
opinion of the authors, which is not necessarily shared or endorsed by their
respective employers and/or sponsors.

Computing resources were provided by the Netherlands Organisation for
Scientiﬁc Research (NWO) through allocation SH-322-15 of the Cartesius
system and by the Advanced School for Computing and Imaging (ASCII)
by allocation of the Distributed ASCII Supercomputer 4 (DAS-4) system.

(a) W3C

(b) CERC

(c) TU

Figure 4: Effect of query expansion by adding nearby terms in Wp (1) in traditional language models (Model 1 [4] with Jelinek-
Mercer smoothing) for W3C, CERC and TU benchmarks.

APPENDIX
The derivative of (3) w.r.t. bias term bc equals

∂L(Wp, Wc, bc)
∂bc

= −

1
m

(cid:32) m
(cid:88)

i=1

|dmax|
|d(i)|

|C|
(cid:88)

j=1

P (cj|d(i))

(cid:16)

∂ log

P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

(cid:33)

and w.r.t. an arbitrary matrix parameter θ (Wp or Wc):

∂L(Wp, Wc, bc)
∂θ

= −

(cid:32) m
(cid:88)

i=1

|dmax|
|d(i)|

|C|
(cid:88)

j=1

P (cj|d(i))

(cid:16)

∂ log

P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

(cid:33)

1
m

λ
m

+

θi,j.

(cid:88)

i,j

Further differentiation for parameter θ (Wp, Wc or bc):

∂bc

∂θ

∂ log (P (cj | w1, . . . , wn))
∂θ

∂P (cj | w1, . . . , wn)
∂θ

1
P (cj | w1, . . . , wn)
∂ ˜P (cj |w1,...,wn)
∂θ

∂P (cj | w1, . . . , wn)
∂θ
Z2 − ˜P (cj | w1, . . . , wn) ∂Z2
∂θ

∂Z2
∂θ

Z 2
2
∂ ˜P (ck | w1, . . . , wn)
∂θ

∂ ˜P (cj | w1, . . . , wn)
∂θ

∂P (cj | wk)
∂θ

(cid:89)

i(cid:54)=k

P (cj | wi)

=

=

=

=

(cid:88)

(cid:88)

k

k

For a given candidate cj and word wi, following (1) we have

P (cj | wi) =

˜P (cj | wi)
Z1
exp (cid:0)(cid:0)(cid:80)e
l=1 exp (cid:0)(cid:0)(cid:80)e

(cid:80)|C|

=

k=1 Wcj,k Wpk,i

k=1 Wcl,k Wpk,i

(cid:1) + bcj

(cid:1)
(cid:1) + bcl

(cid:1)

and consequently, with W (cid:62)
Wp,

pi denoting the i-th column of matrix

∂P (cj | wi)
∂Wcj

∂P (cj | wi)
∂bcj

∂P (cj | wi)
∂W (cid:62)
pi

=

=

=

(cid:17) ˜P (cj | wi)W (cid:62)
(cid:16)
Z1 − ˜P (cj | wi)
Z 2
1
(cid:17) ˜P (cj | wi)
(cid:16)
Z1 − ˜P (cj | wi)
Z 2
1

pi

(cid:16)
Wcj − (cid:80)|C|

(cid:17) ˜P (cj | wi)

l=1 Wcl
Z1

(7)

As can be seen in (7), the distributed representations of candidates
cj at time t + 1 are updated using the representation of words wi at
time t and vice versa.

REFERENCES
[1] The knowledge-based economy. Techn. report, Organisation for

Economic Co-operation and Development, 1996.

[2] P. Bailey, A. P. De Vries, N. Craswell, and I. Soboroff. Overview

of the TREC 2007 enterprise track. In TREC, 2007.
[3] K. Balog. People Search in the Enterprise. PhD thesis,

University of Amsterdam, 2008.

[4] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for
expert ﬁnding in enterprise corpora. In SIGIR, pages 43–50,
2006.

[5] K. Balog, L. Azzopardi, and M. de Rijke. A language modeling

framework for expert ﬁnding. IPM, 45:1–19, 2009.
[6] K. Balog, Y. Fang, M. de Rijke, P. Serdyukov, and L. Si.

Expertise retrieval. Found. & Tr. in Information Retrieval, 6
(2-3):127–256, 2012.

[7] I. Becerra-Fernandez. Role of artiﬁcial intelligence technologies
in the implementation of People-Finder knowledge management
systems. Knowledge-Based Systems, 13(5):315–320, 2000.
[8] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural
probabilistic language model. JMLR, 3:1137–1155, 2003.
[9] Y. Benjamini and Y. Hochberg. Controlling the false discovery
rate: a practical and powerful approach to multiple testing.
JSTOR, pages 289–300, 1995.

[10] R. Berendsen, M. de Rijke, K. Balog, T. Bogers, and A. van den
Bosch. On the assessment of expertise proﬁles. JASIST, 64(10):
2024–2044, 2013.

[11] L. Bottou. Large-scale machine learning with stochastic gradient
descent. In COMPSTAT, pages 177–186. Springer, 2010.
[12] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,

N. Hamilton, and G. Hullender. Learning to rank using gradient
descent. In ICML, pages 89–96, 2005.

[13] Y. Cao, J. Liu, S. Bao, and H. Li. Research on Expert Search at

Enterprise Track of TREC 2005. In TREC, pages 2–5, 2005.
[14] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,
and P. Kuksa. Natural language processing (almost) from
scratch. JMLR, 12(Aug):2493–2537, 2011.

[15] N. Craswell, D. Hawking, A.-M. Vercoustre, and P. Wilkins.

P@noptic expert: Searching for experts not just for documents.
In Ausweb Poster Proceedings, pages 21–25, 2001.

[16] N. Craswell, A. P. de Vries, and I. Soboroff. Overview of the

TREC 2005 enterprise track. In TREC, 2005.

[17] T. H. Davenport and L. Prusak. Working Knowledge. Harvard

Business Review Press, 1998.

[18] S. C. Deerwester, S. T. Dumais, and R. A. Harshman. Indexing

by latent semantic analysis. JASIS, 1990.

[19] G. Demartini, J. Gaugaz, and W. Nejdl. A vector space model

[41] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Distributed

for ranking entities and its application to expert search. In ECIR,
pages 189–201. Springer, 2009.

representations of words and phrases and their compositionality.
In NIPS, pages 3111–3119, 2013.

[20] L. Deng, X. He, and J. Gao. Deep stacking networks for

information retrieval. In ICASSP, pages 3153–3157, 2013.
[21] H. Fang and C. Zhai. Probabilistic models for expert ﬁnding. In

ECIR, pages 418–430, Berlin, Heidelberg, 2007.
Springer-Verlag.

[22] Y. Fang and A. Godavarthy. Modeling the dynamics of personal

expertise. In SIGIR, pages 1107–1110, 2014.

[23] Y. Fang, L. Si, and A. P. Mathur. Discriminative models of

integrating document evidence and document-candidate
associations for expert search. In SIGIR, pages 683–690, 2010.

[24] K. Fatahalian, J. Sugerman, and P. Hanrahan. Understanding the
efﬁciency of gpu algorithms for matrix-matrix multiplication. In
SIGGRAPH HWWS, pages 133–137. ACM, 2004.

[25] X. Glorot and Y. Bengio. Understanding the difﬁculty of training
deep feedforward neural networks. In AISTATS, pages 249–256,
2010.

[26] G. E. Hinton. Learning distributed representations of concepts.
In 8th Annual Conference of the Cognitive Science Society,
volume 1, page 12, Amherst, MA, 1986.

[27] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR,

pages 50–57. ACM, 1999.

[28] P.-s. Huang, N. M. A. Urbana, X. He, J. Gao, L. Deng, A. Acero,
and L. Heck. Learning deep structured semantic models for web
search using clickthrough data. In CIKM, pages 2333–2338,
2013.

[29] P. Indyk and R. Motwani. Approximate nearest neighbors:

towards removing the curse of dimensionality. In STOC, pages
604–613. ACM, 1998.

[30] M. Karimzadehgan and C. Zhai. Estimation of statistical

translation models based on mutual information for ad hoc
information retrieval. In SIGIR, pages 323–330. ACM, 2010.
[31] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural

language models. In ICML, pages 595–603, 2014.

[32] J. Kruger and D. Dunning. Unskilled and unaware of it: how

difﬁculties in recognizing one’s own incompetence lead to
inﬂated self-assessments. J. Personality and Social Psych., 77
(6):1121, 1999.

[33] J. Krüger and R. Westermann. Linear algebra operators for gpu
implementation of numerical algorithms. ACM Transactions on
Graphics, 22(3):908–916, 2003.

[34] H. Li and J. Xu. Semantic matching in search. Found. & Tr. in

Information Retrieval, 7(5):343–469, June 2014.

[35] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer,

2011.

[36] C. MacDonald and I. Ounis. Voting for candidates: adapting

data fusion techniques for an expert search task. In CIKM, pages
387–396, 2006.

[37] C. Macdonald and I. Ounis. Expert search evaluation by

supporting documents. In ECIR, pages 555–563. Springer, 2008.

[38] M. T. Maybury. Expert ﬁnding systems. Techn. Report

[39] D. W. McDonald and M. S. Ackerman. Expertise recommender.

MTR-06B000040, MITRE, 2006.

In CSCW, pages 231–240, 2000.

[42] T. Mikolov, G. Corrado, K. Chen, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
1301.3781, 2013.

[43] A. Mnih and G. Hinton. Three new graphical models for

statistical language modelling. In ICML, pages 641–648, 2007.

[44] A. Mnih and G. Hinton. A scalable hierarchical distributed
language model. In NIPS, pages 1081–1088, 2008.
[45] A. Mnih and K. Kavukcuoglu. Learning word embeddings

efﬁciently with noise-contrastive estimation. In NIPS, pages
2265–2273, 2013.

[46] G. Montavon, G. B. Orr, and K.-R. Müller. Neural Networks:

Tricks of the Trade. Springer, 2012.

[47] C. Moreira, B. Martins, and P. Calado. Using rank aggregation
for expert search in academic digital libraries. In Simpósio de
Informática, INForum, pages 1–10, 2011.

[48] J. Pennington, R. Socher, and C. D. Manning. GloVe: Global

Vectors for Word Representation. In EMNLP, pages 1532–1543,
2014.

[49] D. Petkova and W. B. Croft. Hierarchical language models for
expert ﬁnding in enterprise corpora. In ICTAI ’06, pages
599–606, 2006.

[50] W. W. Powell and K. Snellman. The knowledge economy.
Annual review of sociology, pages 199–220, 2004.

[51] D. Rumelhart, G. Hinton, and R. Williams. Learning internal
representations by back propagation. In Parallel Distributed
Processing, pages 318–362. MIT Press, 1986.

[52] J. Rybak, K. Balog, and K. Nørvåg. Temporal expertise
proﬁling. In ECIR, pages 540–546. Springer, 2014.
[53] R. Salakhutdinov and G. Hinton. Semantic hashing. Int. J.

Approximate Reasoning, 50(7):969–978, 2009.

[54] P. Serdyukov and D. Hiemstra. Modeling documents as mixtures
of persons for expert ﬁnding. In ECIR, pages 309–320. Springer,
2008.

[55] P. Serdyukov, H. Rode, and D. Hiemstra. Modeling multi-step
relevance propagation for expert ﬁnding. In CIKM, pages
1133–1142, 2008.

[56] C. Shannon. A mathematical theory of communication. Bell

System Technical J., 27:379–423, 623–656, 1948.

[57] J. A. Shaw, E. A. Fox, J. A. Shaw, and E. A. Fox. Combination

of multiple searches. In TREC, pages 243–252, 1994.
[58] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. A latent
semantic model with convolutional-pooling structure for
information retrieval. In CIKM, pages 101–110, 2014.
[59] M. D. Smucker, J. Allan, and B. Carterette. A comparison of

statistical signiﬁcance tests for information retrieval evaluation.
In CIKM, pages 623–632. ACM, 2007.

[60] P. Sorg and P. Cimiano. Finding the right expert: Discriminative
models for expert retrieval. In KDIR, pages 190–199, 2011.

[61] TREC. Enterprise Track, 2005–2008.
[62] D. van Dijk, M. Tsagkias, and M. de Rijke. Early detection of
topical expertise in community question and answering. In
SIGIR, 2015.

[63] V. Vapnik. Statistical learning theory, volume 1. Wiley New

[40] T. Mikolov, M. Karaﬁat, L. Burget, J. Cernocky, and

York, 1998.

S. Khudanpur. Recurrent neural network based language model.
In Interspeech, pages 1045–1048, 2010.

[64] M. D. Zeiler. Adadelta: An adaptive learning rate method.

CoRR, abs/1212.5701, 2012.

Unsupervised, Efﬁcient and Semantic Expertise Retrieval

Christophe Van Gysel
cvangysel@uva.nl

Maarten de Rijke
derijke@uva.nl

Marcel Worring
m.worring@uva.nl

University of Amsterdam, Amsterdam, The Netherlands

7
1
0
2
 
p
e
S
 
7
1
 
 
]

R

I
.
s
c
[
 
 
2
v
1
5
6
6
0
.
8
0
6
1
:
v
i
X
r
a

ABSTRACT
We introduce an unsupervised discriminative model for the task
of retrieving experts in online document collections. We exclu-
sively employ textual evidence and avoid explicit feature engineer-
ing by learning distributed word representations in an unsupervised
way. We compare our model to state-of-the-art unsupervised sta-
tistical vector space and probabilistic generative approaches. Our
proposed log-linear model achieves the retrieval performance lev-
els of state-of-the-art document-centric methods with the low infer-
ence cost of so-called proﬁle-centric approaches. It yields a statis-
tically signiﬁcant improved ranking over vector space and genera-
tive models in most cases, matching the performance of supervised
methods on various benchmarks. That is, by using solely text we
can do as well as methods that work with external evidence and/or
relevance feedback. A contrastive analysis of rankings produced
by discriminative and generative approaches shows that they have
complementary strengths due to the ability of the unsupervised dis-
criminative model to perform semantic matching.

1.

INTRODUCTION

The transition to the knowledge and information economy [1]
introduces a great reliance on cognitive capabilities [50]. It is cru-
cial for employers to facilitate information exchange and to stim-
ulate collaboration [17].
In the past, organizations would set-up
special-purpose database systems for their members to maintain
a proﬁle [7]. However, these systems required employees to be
proactive. In addition, self-assessments are known to diverge from
reality [10, 32] and document collections can quickly become prac-
tically infeasible to manage manually. Therefore, there has been an
active interest in automated approaches for constructing expertise
proﬁles [7, 61] and retrieving experts from an organization’s het-
erogeneous document repository [15]. Expert ﬁnding (also known
as expertise retrieval or expert search) addresses the task of ﬁnding
the right person with the appropriate skills and knowledge [6]. It
attempts to provide an answer to the question:

Given a topic X, who are the candidates with the most
expertise w.r.t. X?

This is an electronic version of an Article published in the 25th International
World Wide Web Conference.
c(cid:13) 2016 International World Wide Web Conference Committee.
WWW 2016, April 11–15, 2016, Montréal, Québec, Canada.
ACM 978-1-4503-4143-1/16/04.
http://dx.doi.org/10.1145/2872427.2882974.

The expertise retrieval task gained popularity in the research com-
munity during the TREC Enterprise Track [61] and has remained
relevant ever since, while broadening to social media and to track-
ing the dynamics of expertise [5, 6, 10, 19, 21–23, 47, 49, 62].
Existing methods fail to address key challenges: (1) Queries and
expert documents use different representations to describe the same
concepts [26, 34]. Term mismatches between queries and experts
[34] occur due to the inability of widely used maximum-likelihood
language models to make use of semantic similarities between words
[53]. (2) As the amount of available data increases, the need for
more powerful approaches with greater learning capabilities than
smoothed maximum-likelihood language models is obvious [63].
(3) Supervised methods for expertise retrieval [23, 47] were intro-
duced at the turn of the last decade. However, the acceleration
of data availability has the major disadvantage that, in the case of
supervised methods, manual annotation efforts need to sustain a
similar order of growth. This calls for the further development of
unsupervised methods. (4) In some expertise retrieval methods, a
language model is constructed for every document in the collection.
These methods lack efﬁcient query capabilities for large document
collections, as each query term needs to be matched against ev-
ery document [6]. Our proposed solution has a strong emphasis on
unsupervised model construction, efﬁcient query capabilities and
semantic matching between query terms and candidate experts.

Speciﬁcally, we propose an unsupervised log-linear model with
efﬁcient inference capabilities for the expertise retrieval task. We
show that our approach improves retrieval performance compared
to vector space-based and generative language models, mainly due
to its ability to perform semantic matching [34]. Our method does
not require supervised relevance judgments and is able to learn
from raw textual evidence and document-candidate associations
alone. The purpose of this work is to provide insight in how dis-
criminative language models can improve performance of core re-
trieval tasks compared to maximum-likelihood language models.
Therefore, we avoid explicit feature engineering and the incorpo-
ration of external evidence in this paper. In terms of performance,
the current best-performing formal language model [4] exhibits a
worst-case time complexity linear in the size of the document col-
lection. In contrast, the inference time complexity of our approach
is asymptotically bounded by the number of candidate experts.

Our research questions are as follows: (1) How does our dis-
criminative log-linear model compare to vector space-based meth-
ods and generative language models for the expert retrieval task in
terms of retrieval performance? (2) What can we learn regarding
the different types of errors made by generative and discriminative
language models? (3) How does the complexity of inference in
our log-linear model compare to vector-space based and generative

models? (4) How does the log-linear model handle incremental
indexing and what are its limitations?

We contribute: (i) An unsupervised log-linear model with efﬁ-
cient inference capabilities for the expertise retrieval task, together
with an open-source implementation.1 (ii) Comparison of the re-
trieval performance of the log-linear model with traditional vector
space-based models and language model methods on well-known
benchmarks. (iii) Insights in how certainty in predictions by the
log-linear model correlates with performance. (iv) In-depth analy-
sis of the inference complexity of the log-linear model. (v) Com-
parative error analysis between the semantic log-linear model and
traditional generative language models that perform exact match-
ing. (vi) Insight in the relative strengths of semantic matching and
exact matching for the expert retrieval task.

The remainder of this paper is organized as follows. In Section 2,
we brieﬂy discuss related work. Section 3 introduces the log-linear
model for the expert retrieval problem. In Section 4 we state our
research questions and detail our experimental set-up and imple-
mentations. We provide an overview of our experimental results,
followed by a discussion and analysis in Section 5. Section 6 con-
cludes the paper and discusses ideas for future work.

2. RELATED WORK

We ﬁrst discuss prior work on expert retrieval and its relation
to document retrieval. Then, we review semantic search methods.
The ideas we present in this paper are inspired by neural language
models used in automated speech recognition (ASR) and natural
language processing (NLP). Consequently, we also review work
from those ﬁelds.

2.1 Expert retrieval

Early expert retrieval systems were often referred to as expert
locator and expertise management systems [38]. These database
systems often relied on people to self-assess their expertise against
a predeﬁned set of topics [39], which is known to generate unreli-
able results [7].

With the introduction of the P@NOPTIC system [15], and later
the TREC Enterprise track [61], there has been an active research
interest in automated expertise proﬁling methods. It is useful to dis-
tinguish between proﬁle-based methods, which create a textual rep-
resentation of a candidate’s knowledge, and document-based meth-
ods, which represent candidates as a weighted combination of doc-
uments. The latter generally performs better at ranking, while the
former is more efﬁcient as it avoids retrieving all documents rele-
vant to a query [6, p. 221].

There has been much research on generative probabilistic mod-
els for expert retrieval [21, 49]. Such models have been categorized
in candidate generation models [13], topic generation models [4, 5]
and proximity-based variants [5, 54]. Of special relevance to us are
the unsupervised proﬁle-centric (Model 1) and document-centric
(Model 2) models of Balog et al. [4], which focus on raw textual ev-
idence without incorporating collection-speciﬁc information (e.g.,
query modeling, document importance or document structure). Su-
pervised discriminative models [23, 47, 60] are preferred when
query-candidate relevance pairs are available for training. Unlike
their generative counterparts these models have no issue combining
complex and heterogeneous features (e.g., link-based features, doc-
ument importance features, etc.); they resemble Learning to Rank
(L2R) methods for document retrieval [6, 35]. However, a lack
of training data may greatly hinder their applicability [6, p. 179].
Beyond unsupervised generative and supervised discriminative ap-

1https://github.com/cvangysel/SERT

proaches, there are graph-based approaches based on random walks
[55] and voting-based approaches based on data fusion [36]. De-
martini et al. [19] propose a vector space-based method for the en-
tity ranking task; their framework extends vector spaces operating
on documents to entities. See [6] for a survey on the topic.

2.2 Latent semantic models for document re-

trieval

Li and Xu [34] note that the query document mismatch poses the
most critical challenge in search. Semantic matching is an impor-
tant attempt to remedy this problem. There has been much work
on bridging the semantic gap for various different tasks [19, 26, 28,
30, 31, 41, 48, 53]. Expertise and document retrieval [6, p. 224] are
closely related as performance of the latter can greatly impact that
of the former [37].

Latent Semantic Models (LSM) ﬁrst became popular through
the introduction of Latent Semantic Indexing (LSI) [18], followed
by probabilistic LSI (pLSI) [27]. LSMs based on neural networks
[28, 53, 58] emerged in the last decade. Salakhutdinov and Hinton
[53] employ unsupervised deep auto-encoders to map documents to
bit patterns using semantic addressing. Huang et al. [28] perform
semantic matching of documents and queries by leveraging click-
through data optimizing for Web document ranking. Further, deep
models [12, 20] have been proposed to learn to rank [35].

2.3 Neural language modeling

Large-vocabulary neural probabilistic language models for mod-
eling word sequence distributions have become very popular re-
cently [8, 43, 44]. These models learn continuous-valued distributed
representations for words, also known as embeddings [42, 45, 48],
in order to ﬁght the curse of dimensionality and increase general-
ization by introducing the expectation that similar word vectors sig-
nify semantically or syntactically similar words. Recurrent neural
language models have shown to perform well in ASR [40]. Col-
lobert et al. [14] propose a uniﬁed neural network architecture for
various NLP tasks. Even more recently, there has been a surge in
multimodal neural language models [31], which lend themselves to
the task of automated image captioning.

What we add on top of the related work described above is the
following.
In this work we model the conditional probability of
the expertise of a candidate given a single query term (contrary
to binary relevance given a character-based n-gram [28]). In the
process we learn a distributed vector representation (similar to LSI,
pLSI and semantic hashing) for both words and candidates such
that nearby representations indicate semantically similar concepts.
We propose a log-linear model that is similar to neural language
models. The important difference is that we predict a candidate
expert instead of the next word. To the best of our knowledge,
we are the ﬁrst to propose such a solution. We employ an embed-
ding layer in our shallow model for the same reasons as mentioned
above: we learn continuous word representations that incorporate
semantic and syntactic similarity tailored to an expert’s domain.

3. A LOG-LINEAR MODEL FOR

of words w1, . . . , wk we have:

EXPERT SEARCH

In the setting of this paper we have a document collection D and
a predeﬁned set of candidate experts C (entities to be retrieved).
Documents d ∈ D are represented as a sequence of words w1, . . . ,
w|d| originating from a vocabulary V , where wi ∈ V and the oper-
ator |·| denotes the document length in tokens. For every document
d ∈ D we write Cd to denote the set of candidates c ∈ C asso-
ciated with it (i.e., C = (cid:83)
d∈D Cd). These document-candidate
associations can be obtained explicitly from document meta-data
(e.g., the author of an e-mail) or implicitly by mining references
to candidates from the document text. Notice that some documents
might not be associated with any candidate. When presented with a
query q of constituent terms t1, . . . , t|q|, the expert retrieval task is
to return a list of candidates ρ(C) ordered according to topical ex-
pertise. We generate this ranking using a relatively shallow neural
network which directly models P (c | q).

We employ vector-based distributed representations [26], for both
words (i.e., word embeddings) and candidate experts, in a way that
motivates the unsupervised construction of features that express
regularities of the expertise ﬁnding domain. These representations
can capture the similarity between concepts (e.g., words and can-
didate experts) by the closeness of their representations in vector
space. That is, concepts with similar feature activations are inter-
preted by the model as being similar, or even interchangeable.

3.1 The model

To address the expert search task, we model P (cj | q) and rank
candidates cj accordingly for a given q. We propose an unsuper-
vised, discriminative approach to obtain these probabilities. We
construct our model solely from textual evidence: we do not re-
quire query-candidate relevance assessments for training and do not
consider external evidence about the corpus (e.g., different weight-
ings for different sub-collections), the document (e.g., considering
certain parts of the document more useful) nor link-based features.
Let e denote the size of the vector-based distributed representa-
tions of both words in V and candidate experts in C. These repre-
sentations will be learned by the model using gradient descent [42]
(Section 3.2). For notational convenience, we write P (c | ·) for the
(conditional) probability distribution over candidates, which is the
result of vector arithmetic. We deﬁne the probability of a candidate
cj given a single word wi ∈ V as the log-linear model

P (c | wi) =

exp (Wc · (Wp · vi) + bc) ,

(1)

1
Z1

where Wp is the e × |V | projection matrix that maps the one-hot
representation (i.e., 1-of-|V |) of word wi, vi, to its e-dimensional
distributed representation, bc is a |C|-dimensional bias vector and
Wc is the |C| × e matrix that maps the word embedding to an un-
normalized distribution over candidates C, which is then normal-
ized by Z1 = (cid:80)|C|
j=1 [exp (Wc · (Wp · vi) + bc)]j. If we consider
Bayes’ theorem, the transformation matrix Wc and bias vector bc
can be interpreted as the term log-likelihood log P (wi | c) and
candidate log-prior log P (c), respectively. The projection matrix
Wp attempts to soften the curse of dimensionality introduced by
large vocabularies V and maps words to word feature vectors [8].
Support for large vocabularies is crucial for retrieval tasks [28, 53].
We then assume conditional independence of a candidate’s ex-
pertise given an observation of data (i.e., a word). Given a sequence

P (c | w1, . . . , wk) =

˜P (c | w1, . . . , wk) =

P (c | wi)

1
Z2

1
Z2

(cid:32) k

(cid:88)

1
Z2

k
(cid:89)

i=1
(cid:33)

=

exp

log (P (c | wi))

(2)

(cid:16)(cid:80)k

j=1 exp

i=1
where ˜P (c | w1, . . . , wk) denotes the unnormalized score and
(cid:17)
Z2 = (cid:80)|C|
i=1 log (P (cj | wi))
is a normalizing term.
The transformation to log-space in (2) is a well-known trick to pre-
vent ﬂoating point underﬂow [46, p. 445]. Given (2), inference is
straight-forward. That is, given query q = t1, . . . , tk, we compute
P (c | t1, . . . , tk) and rank the candidate experts in descending
order of probability.

Eq. 1 deﬁnes a neural network with a single hidden layer. We
can add additional layers. Preliminary experiments, however, show
that the shallow log-linear model (1) performs well-enough in most
cases. Only for larger data sets did we notice a marginal gain from
adding an additional layer between projection matrix Wp and the
softmax layer over C (Wc and bias bc), at the expense of longer
training times and loss of transparency.

3.2 Parameter estimation

The matrices Wp, Wc and the vector bc in (1) constitute the
parameters of our model. We estimate them using error back prop-
agation [51] as follows. For every document dj ∈ D we construct
an ideal distribution over candidates p = P (c | dj) based on the
document-candidate associations Cdj such that

P (c | dj) =

(cid:40) 1
|Cdj
0,

| ,

c ∈ Cdj

c (cid:54)∈ Cdj

We continue by extracting n-grams where n remains ﬁxed during
training. For every n-gram w1, . . . , wn generated from document
d we compute ˜p = P (c | w1, . . . , wn) using (2). During model
constructing we then optimize the cross-entropy H(p, ˜p) (i.e., the
joint probability of the training data if |Cdj | = 1 for all j) using
batched gradient descent. The loss function for a single batch of m
instances with associated targets (p(i), ˜p(i)) is as follows:

L(Wp, Wc, bc)

=

1
m

+

m
(cid:88)

i=1

λ
2m

|dmax|
|d(i)|
(cid:32)

(cid:88)

i,j

H(p(i), ˜p(i))

W 2

pi,j +

W 2

ci,j

(cid:88)

i,j

= −

|dmax|
|d(i)|

|C|
(cid:88)

j=1

1
m

m
(cid:88)

i=1
(cid:32)

+

λ
2m

(cid:88)

i,j

W 2

pi,j +

W 2

ci,j

,

(cid:88)

i,j

(cid:33)

(cid:33)

(3)

P (cj | d(i)) log

(cid:16)
P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

where d(i) refers to the document from which n-gram w(i)
1 , . . . , w(i)
n
was extracted, dmax = arg maxd∈D |d| indicates the longest doc-
ument in the collection, and λ is a weight regularization parameter.
The update rule for a particular parameter θ (Wp, Wc or bc) given
a single batch of size m is:

θ(t+1) = θ(t) − α(t) (cid:12)

∂L(Wp

(t), bc

(t))

(t), Wc
∂θ

,

(4)

where α(t) and θ(t) denote the per-parameter learning rate and pa-
rameter θ at time t, respectively. The learning rate α consists of
the same number of elements as there are parameters; in the case
of a global learning rate, all elements of α are equal to each other.
The derivatives of the loss function (3) are given in the Appendix.
In the next section we will discuss our experimental setup, fol-
lowed by an overview of our experimental results and further anal-
ysis in Section 5.

4. EXPERIMENTAL SETUP

4.1 Research questions

As indicated in the introduction, we seek to answer the following

research questions:
RQ1 How does our discriminative log-linear model compare to
vector space-based methods and generative language models
for the expert retrieval task in terms of retrieval performance?
In particular, how does the model perform when compared against
vector space-based (LSI and TF-IDF) and generative approaches
(proﬁle-centric Model 1 and document-centric Model 2)?
RQ2 What can we learn regarding the different types of errors

made by generative and discriminative language models?

Does the best-performing generative model simply perform slightly
better on the topics for which the other models perform decent as
well, or do they make very different errors? If the latter holds,
an ensemble of the rankings produced by both model types might
exceed performance of the individual rankings.
RQ3 How does the complexity of inference in our log-linear model
compare to vector-space based and generative models?
The worst-case inference cost of document-centric models makes
them unattractive in online settings where the set of topics is not
deﬁned beforehand and the document collection is large. Proﬁle-
centric methods are preferred in such settings as they infer from
one language model per candidate expert for every topic (i.e., a
pseudo-document consisting of a concatenation of all documents
associated with an expert) [6]. Vector space-based methods [19]
have similar problems due to the curse of dimensionality [29] and
consequently their inferential time complexity is likewise asymp-
totically bounded by the number of experts.
RQ4 How does the log-linear model handle incremental indexing

and what are its limitations?

4.2 Benchmarks

The proposed method is applicable in the setting of the Expert
Search task of the TREC Enterprise track from 2005 to 2008 [61].
We therefore evaluate on the W3C and CERC benchmarks released
by the track. The W3C dataset [16] is a crawl of the W3C’s sites
in June 2004 (mailing lists, web pages, etc.). The CSIRO Enter-
prise Research Collection (CERC) [2] is a dump of the intranet
of Australia’s national science agency. Additionally, we evaluate
our method on a smaller, more recent benchmark based on the em-
ployee database of Tilburg University (TU) [10], which consists of
bi-lingual, heterogeneous documents. See Table 1.

Mining document-candidate associations and how they inﬂuence
performance has been extensively covered in previous work [4, 6]
and is beyond the scope of this work. For TU, the associations
are part of the benchmark. For W3C, a list of possible candidates
is given and we extract the associations ourselves by performing
a case-insensitive match of full name or e-mail address [4]. For
CERC, we make use of publicly released associations [3].

As evaluation measures we use Mean Average Precision (MAP),
Mean Reciprocal Rank (MRR), Normalized Discounted Cumula-

tive Gain at rank 100 (NDCG@100) and Precision at rank 5 (P@5)
and rank 10 (P@10).

4.3 Baselines

We compare our approach to existing unsupervised methods for
expert retrieval that solely rely on textual evidence and static doc-
ument-candidate associations. (1) Demartini et al. [19] propose a
generic framework to adapt vector spaces operating on documents
to entities. We compare our method to TF-IDF (raw frequency and
inverse document frequency) and LSI (300 latent topics) variants
of their vector space model for entity ranking (using cosine sim-
ilarity). (2) In terms of language modeling, Balog et al. [4] pro-
pose two models for expert ﬁnding based on generative language
models. The ﬁrst takes a proﬁle-centric approach comparing the
language model of every expert to the query, while the second is
document-centric. We consider both models with different smooth-
ing conﬁgurations: Jelinek-Mercer (jm) smoothing with λ = 0.5
[4] and Dirichlet (d) smoothing with β equal to the average doc-
ument length [5] (see Table 1). Signiﬁcance of results produced
by the baselines (compared to our method) is determined using a
two-tailed paired randomization test [59].

4.4

Implementation details

The vocabulary V is constructed from each corpus by ignoring
punctuation, stop words and case; numbers are replaced by a nu-
merical placeholder token. During our experiments we prune V
by only retaining the 216 most-frequent words so that each word
can be encoded by a 16-bit unsigned integer. Incomplete n-gram
instances are padded by a special-purpose token.

In terms of parameter initialization, we sample the initial matri-

ces Wc and Wp (1) uniformly in the range
(cid:34)
(cid:114) 6.0

(cid:114) 6.0

(cid:35)

−

,

m + n

m + n

for an m × n matrix, as this initialization scheme improves model
training convergence [25], and take the bias vector bc to be null.
The projection layer Wp is initialized with pre-trained word repre-
sentations trained on Google News data [41]; the number of word
features is set to e = 300, similar to pre-trained representations.

We used adadelta (ρ = 0.95, (cid:15) = 10−6) [64] with batched gra-
dient descent (m = 1024) and weight decay λ = 0.01 during
training on NVidia GTX480 and NVidia Tesla K20 GPUs. We only
iterate once over the entire training set for each experiment.

5. RESULTS AND DISCUSSION

We start by giving a high-level overview of our experimental re-
sults and then address issues of scalability, provide an error analysis
and discuss the issue of incremental indexing.

5.1 Overview of experimental results

We evaluate the log-linear model on the W3C, CERC and TU
benchmarks (Section 4.2). During training we extract non-overlap-
ping n-grams for the W3C and CERC benchmarks and overlapping
n-grams for the TU benchmark. As the TU benchmark is consider-
ably smaller, we opted to use overlapping n-grams to counter data
sparsity. The architecture of each benchmark model (e.g., num-
ber of candidate experts) is inherently speciﬁed by the benchmarks
themselves (see Table 1). However, the choice of n-gram size dur-
ing training remains open. Errors for input w1, . . . , wn are propa-
gated back through Wc until the projection matrix Wp is reached;
if a single word wi causes a large prediction error, then this will in-
ﬂuence its neighboring words w1, . . . , wi−1, wi+1, . . . , wn as well.

Table 1: An overview of the three datasets (W3C, CERC and TU) used for evaluation and analysis.

Number of documents
Average document lengtha

Number of candidatesb

Number of document-candidate associations
Number of documents (with Cd > 0)
Number of associations per documentc
Number of associations per candidate

Queries

W3C

CERC

331,037

1,237.23

370,715

460.48

715

200,939
93,826

3,479

236,958
123,934

TU

31,209
2,454.93

977

36,566
27,834

2.14 ± 3.29
281.03 ± 666.63

1.91 ± 3.70
68.11 ± 1,120.74

1.13 ± 0.39
37.43 ± 61.00

49
50

(2005)
(2006)

50
77

(2007)
(2008)

1,662
1,266

(GT1)
(GT5)

a Measured in number of tokens.
b Only candidates with at least a single document association are considered.
c Only documents with at least one association are considered.

This allows the model to learn continuous word representations tai-
lored to the expert retrieval task and the benchmark domain.

A larger window size has a negative impact on batch throughput
during training. We are thus presented with the classic trade-off
between model performance and construction efﬁciency. Notice,
however, that the number of n-grams decreases as the window size
increases if we extract non-overlapping instances. Therefore, larger
values of n lead to faster wall-clock time model construction for the
W3C and CERC benchmarks in our experiments.

We sweep over the window width n = 2i (0 ≤ i < 6) for all
three benchmarks and their corresponding relevance assessments.
We report MAP and MRR for every conﬁguration (see Figure 1).
We observe a signiﬁcant performance increase between n = 1
and n = 2 on all benchmarks, which underlines the importance
of the window size parameter. The increase in MAP implies that
the performance achieved is not solely due to initialization with
pre-trained representations (Section 4.4), but that the model efﬁ-
ciently learns word representations tailored to the problem domain.
The highest MAP scores are attained for relatively low n. As n
increases beyond n = 8 a gradual decrease in MAP is observed on
all benchmarks. In our remaining experiments we choose n = 8
regardless of the benchmark.

Words wi that mainly occur in documents associated with a par-
ticular expert are learned to produce distributions P (c | wi) with
less uncertainty than words associated with many experts in (1).
The product of P (c | wi) in (2) aggregates this expertise evidence
generated by query terms. Hence, queries with strong evidence for
a particular expert should be more predictable than very generic
queries. To quantify uncertainty we measure the normalized en-
tropy [56] of P (c | q):

η(c | q) = −

p(cj | q) log(p(cj | q)).

(5)

1
log(|C|)

|C|
(cid:88)

j=1

Equation 5 can be interpreted as a similarity measure between the
given distribution and the uniform distribution. Importantly, Fig-
ure 2 shows that there is a statistically signiﬁcant negative correla-
tion between query-wise normalized entropy and average precision
for all benchmarks.

Table 2 presents a comparison between the log-linear model and
the various baselines (Section 4.3). Our unsupervised method sig-
niﬁcantly (p < 0.01) outperforms the LSI-based method consis-
tently. In the case of the TF-IDF method and the proﬁle-centric

generative language models (Model 1), we always perform bet-
ter and statistical signiﬁcance is achieved in the majority of cases.
The document-centric language models (Model 2) perform slightly
better than our method on two (out of six) benchmarks in terms
of MAP and NDCG@100:(1) For the CERC 2007 assessment we
match performance of the document-centric generative model with
Jelinek-Mercer smoothing. (2) For TU GT1 the generative coun-
terpart seems to outperform our method.

Notice that over all assessments, the log-linear model consis-
tently outperforms all proﬁle-centric approaches and is only chal-
lenged by the smoothed document-centric approach. In addition,
for the precision-based measures (P@k and MRR), the log-linear
model consistently outperforms all other methods we compare to.
Next, we turn to a topic-wise comparative analysis of discrimi-
native and generative models. After that, we analyze the scalability
and efﬁciency of the log-linear model and compare it to that of the
generative counterparts, and address incremental indexing.

5.2 Error analysis

How does our log-linear model achieve its superior performance
over established generative models? Figure 3 depicts the per-topic
differences in average precision between the log-linear model and
Model 2 (with Jelinek-Mercer smoothing) on all benchmarks. For
each plot, the vertical bars with a positive AP difference correspond
to test topics for which the log-linear model outperforms Model 2
and vice versa for bars with a negative AP difference.

The beneﬁt gained from the projection matrix Wp is two-fold.
First, it avoids the curse of dimensionality introduced by large vo-
cabularies. Second, term similarity with respect to the expertise
domain is encoded in latent word features. When examining words
nearby query terms in the embedding space, we found words to be
related to the query term. For example, word vector representations
of xml and nonterminal are very similar for the W3C benchmark (l2
norm). This can be further observed in Figure 1: log-linear models
trained on single words perform signiﬁcantly worse compared to
those that are able to learn from neighboring words.

We now take a closer look at the topics for which the log-linear
model outperforms Model 2 and vice versa. More speciﬁcally, we
investigate textual evidence related to a topic and whether it is con-
sidered relevant by the benchmark. For the log-linear model, we ex-
amine terms nearby topic terms in Wp (l2-norm), as these terms are
considered semantically similar by the model and provide a means
for semantic matching. For every benchmark, we ﬁrst consider top-

Table 2: Evaluation results for models trained on the W3C, CERC and TU benchmarks. Sufﬁxes (d) and (jm) denote Dirichlet and
Jelinek-Mercer smoothing, respectively (Section 4.3). Signiﬁcance of results is determined using a two-tailed paired randomization
test [59] (∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) with respect to the log-linear model (adjusted using the Benjamini-Hochberg
procedure for multiple testing [9]).

W3C

MAP NDCG@100 MRR

P@5 P@10 MAP

2005

2006
NDCG@100 MRR

CERC

MAP NDCG@100 MRR

P@5 P@10 MAP

2008
NDCG@100 MRR

0.135
LSI
0.243
TF-IDF
0.192
Model 1 (d)
0.190
Model 1 (jm)
0.198
Model 2 (d)
0.211
Model 2 (jm)
Log-linear (ours) 0.248

0.031
LSI
0.332
TF-IDF
0.287
Model 1 (d)
0.278
Model 1 (jm)
0.352
Model 2 (d)
0.361
Model 2 (jm)
Log-linear (ours) 0.344

0.095
LSI
0.216
TF-IDF
0.171
Model 1 (d)
0.189
Model 1 (jm)
0.154
Model 2 (d)
0.234
Model 2 (jm)
Log-linear (ours) 0.219

0.266
0.426
0.358
0.352
0.369
0.380
0.444

0.107
0.486
0.427
0.420
0.495
0.500
0.493

0.205
0.356
0.308
0.325
0.284
0.370
0.356

0.192 0.196
0.306
0.384 0.350
0.541
0.276 0.266
0.433
0.272 0.276
0.390
0.288 0.272
0.429
0.451
0.332 0.296
0.618∗ 0.412 0.361

2007

0.060
0.463
0.384
0.384
0.454
0.467
0.513

GT1

0.016 0.014
0.196 0.141
0.156 0.096
0.156 0.084
0.180 0.138
0.192 0.138
0.215 0.150

0.153
0.324
0.258
0.277
0.228
0.342
0.351

0.060 0.051
0.131 0.097
0.103 0.082
0.112 0.085
0.087 0.070
0.136 0.101
0.145∗ 0.105

0.245
0.343
0.321
0.311
0.261
0.260
0.484∗∗∗

0.038
0.269
0.181
0.170
0.264
0.274
0.342∗∗∗

0.097
0.233
0.241
0.231
0.191
0.253
0.287∗∗∗

0.371
0.531
0.491
0.483
0.419
0.423
0.667∗∗∗

0.482
0.650
0.635
0.596
0.551
0.599
0.833∗∗∗

0.099
0.465
0.355
0.347
0.461
0.463
0.519∗∗

0.106
0.525
0.388
0.339
0.510
0.517
0.656∗∗

0.208
0.378
0.385
0.373
0.334
0.394
0.425∗∗∗

0.129
0.288
0.292
0.271
0.233
0.302
0.363∗∗∗

TU

MAP NDCG@100 MRR

P@5 P@10 MAP

GT5
NDCG@100 MRR

P@5

P@10

0.287
0.492
0.478
0.502
0.441
0.449
0.713∗∗∗

0.338
0.498
0.449
0.437
0.404
0.429
0.644∗∗∗

P@5

0.042
0.332
0.200
0.181
0.281
0.278
0.381∗

P@10

0.055
0.277
0.172
0.159
0.244
0.239
0.299

P@5

P@10

0.043
0.108
0.109
0.100
0.084
0.108
0.134∗∗∗

0.036
0.079
0.081
0.075
0.065
0.081
0.092∗∗∗

ics where exact matches (Model 2) perform best, followed by ex-
amples which beneﬁt from semantic matching (log-linear model).
Topic identiﬁers are between parentheses.

W3C Topics P3P speciﬁcation and CSS3 (EX8 and EX69, respec-
tively) should return candidates associated with the deﬁnition
of these standards. The log-linear model, however, considers
these close to related technologies such as CSS2 for CSS3
and UTF-8 for P3P. Semantic matching works for topics Se-
mantic Web Coordination and Annotea server protocol (EX1
and EX103), where the former is associated with RDF li-
braries, RDF-related jargon and the names of researchers in
the ﬁeld, while the latter is associated with implementations
of the protocol and the maintainer of the project.

CERC For CSIRO, topic nanohouse (CE-035) is mentioned in
many irrelevant contexts (i.e., spam) and therefore seman-
tic matching fails. The term ﬁsh oil (CE-126) is quickly as-
sociated with different kinds of ﬁsh, oils and organizations
related to marines and ﬁsheries. On the other hand, we ob-
serve sensor networks (CE-018) to be associated with sen-
sor/networking jargon and sensor platforms. Topic forensic
science workshop (CE-103) expands to syntactically-similar
terms (e.g., plural), the names of science laboratories and ref-
erences

support/law-protection

organizations.

to

TU The TU benchmark contains both English and Dutch textual
evidence. Topics sustainable tourism and interpolation (1411
and 4882) do not beneﬁt from semantic matching due to a
semantic gap: interpolation is associated with the polyno-
mial kind while the relevance assessments focus on stochas-
tic methods. Interestingly, for the topic law and informati-
zation/computerization (1719) we see that the Dutch trans-
lation of law is very closely related. Similar terms to in-
formatization are, according to the log-linear model, Dutch
words related to cryptography. Similar dynamics are at work
for legal-political space (12603), where translated terms and
semantic-syntactic relations aid performance.

In order to further quantify the effect of the embedding matrix Wp,
we artiﬁcially expand benchmark topic terms by k nearby terms.
We then examine how the performance of a proﬁle-centric gener-
ative language model [4, Model 1] evolves for different values of
k (Figure 4). The purpose of this analysis is to provide further
insight in the differences between maximum-likelihood language
models and the log-linear model. Figure 4 shows that, for most
benchmarks, MAP increases as k goes up. Interestingly enough,
the two benchmarks that exhibit a decrease in MAP for larger k
(CERC 2007 and TU GT1) are likewise those for which generative
language models outperform the log-linear model in Table 2. This
suggests that the CERC 2007 and TU GT1 benchmarks require ex-

(a) W3C

(b) CERC

(c) TU

(d) W3C

(e) CERC

(f) TU

Figure 1: Sensitivity analysis for window size (n-gram) during parameter estimation (3) for W3C, CERC and TU benchmarks.

act term matching, while the remaining four benchmarks beneﬁt
greatly from the semantic matching provided by our model.

The per-topic differences suggest that Model 2 and the log-linear
model make very different errors: Model 2 excels at retrieving ex-
act query matches, while the log-linear model performs seman-
tic matching. Based on these observations we hypothesize that a
combination of the two approaches will raise retrieval performance
even further. To test this hypothesis, we propose a simple ensem-
ble of rankings generated by Model 2 and the log-linear model by
re-ranking candidates according to the multiplicatively-combined
reciprocal rank:

rankensemble(cj, qi) ∝

1
rankmodel 2(cj, qi)

·

1
ranklog-linear(cj, qi)

, (6)

where rankM (cj, qi) denotes the position of candidate cj in a rank-
ing generated by model M for answering query qi. Equation (6) is
equivalent to performing data fusion using CombSUM [57] where
the scores are given by the logarithm of the reciprocal ranks of the
experts. Table 3 compares the result of this ensemble to that of its
constituents. Compared to the supervised methods of Fang et al.
[23], we conclude that our fully unsupervised ensemble matches
the performance of their method on the CERC 2007 benchmark
and outperforms their method on the W3C 2005 benchmark. The
superior performance of the ensemble suggests the viability of hy-
brid methods that combine semantic and exact matching.

5.3 Scalability and efﬁciency

Inference in the log-linear model is expressed in linear algebra
operations (Section 3). These operations can be efﬁciently per-
formed by highly optimized software libraries and special-purpose
hardware (i.e., GPUs). But the baseline methods against which we
compare do not beneﬁt from these speed-ups. Furthermore, many
implementation-speciﬁc details and choice of parameter values can

inﬂuence runtime considerably (e.g. size of the latent representa-
tions). Therefore, we opt for a theoretical comparison of the infer-
ence complexity of the log-linear model and compare these to the
baselines (Section 4.3).

The log-linear model generates a ranking of candidate experts by
straight-forward matrix operations. The look-up operation in the
projection matrix Wp occurs in constant time complexity, as the
multiplication with the one-hot vector vi comes down to selecting
the i-th column from Wp. Multiplication of the |C| × e matrix
Wc with the e-dimensional word feature vector exhibits O(|C| · e)
runtime complexity. If we consider addition of the bias term and
division by the normalizing function Z1, the time complexity of (1)
becomes

O(|C| · (e + (e − 1))
(cid:125)
(cid:123)(cid:122)
(cid:124)
matrix-vector multiplication

+ |C|
(cid:124)(cid:123)(cid:122)(cid:125)
bias term

+ 2 · |C| − 1
(cid:123)(cid:122)
(cid:125)
Z1

(cid:124)

).

Notice, however, that the above analysis considers sequential exe-
cution. Modern computing hardware has the ability to parallelize
common matrix operations [24, 33]. The number of candidate ex-
perts |C| is the term that impacts performance most in the log-linear
model (under the assumption that |C| (cid:29) e).

If we consider n terms, where n is the query length during infer-
ence or the window size during training, then the complexity of (2)
becomes

O(n · |C| · (2 · e − 1) + n · (3 · |C| − 1)
(cid:125)

(cid:124)

(cid:123)(cid:122)
n forward-passes

+ (n − 1) · |C|
(cid:125)

(cid:123)(cid:122)
factor product

(cid:124)

)
+ 2 · |C| − 1
(cid:123)(cid:122)
(cid:125)
Z2

(cid:124)

Notice that Z2 does not need to be computed during inference as it
does not affect the candidate expert ranking.

(a) W3C (R = −0.39∗∗∗)

(b) CERC (R = −0.44∗∗∗)

(c) TU (R = −0.30∗∗∗)

Figure 2: Scatter plot of the normalized entropy of distribution P (c | q) (2) returned by the log-linear model and per-query av-
erage precision for W3C, CERC and TU benchmarks. Pearson’s R and associated p-value (two-tailed paired permutation test:
∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) are between parentheses. The depicted linear ﬁt was obtained using an ordinary least squares
regression.

Table 3: Comparison of Model 2, the log-linear model and an ensemble of the former on W3C, CERC and TU benchmarks. Signiﬁ-
cance of results is determined using a two-tailed paired randomization test [59] (∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) with respect to
the ensemble ranking (adjusted using the Benjamini-Hochberg procedure for multiple testing [9]).

2005
NDCG@100 MRR

0.380
0.444
0.479∗∗

0.451
0.618
0.668

2007
NDCG@100 MRR

0.500
0.493
0.589∗∗∗

0.467
0.513
0.627∗∗∗

GT1
NDCG@100 MRR

W3C

Model 2 (jm)
Log-linear (ours)
Ensemble

CERC

Model 2 (jm)
Log-linear (ours)
Ensemble

TU

Model 2 (jm)
Log-linear (ours)
Ensemble

MAP

0.211
0.248
0.291∗∗∗

MAP

0.361
0.344
0.452∗∗

MAP

0.234
0.219
0.271∗∗∗

P@5

0.332
0.412
0.440

P@5

0.192
0.215
0.248∗

P@10

0.296
0.361
0.378

P@10

0.138
0.150
0.160

MAP

0.260
0.484∗∗∗
0.433

MAP

0.274
0.342
0.395∗∗∗

2006
NDCG@100 MRR

0.423
0.667∗∗
0.634

0.599
0.833
0.825

2008
NDCG@100 MRR

0.463
0.519
0.593∗∗∗

0.517
0.656
0.716

GT5
NDCG@100 MRR

P@5

P@10

0.449
0.713∗∗
0.657

0.429
0.644∗∗
0.586

P@5

P@10

0.278
0.381
0.459∗∗

0.239
0.299
0.357∗∗∗

P@5

P@10

MAP

P@5

P@10

0.370
0.356
0.417∗∗∗

0.342
0.351
0.403∗∗∗

0.136
0.145
0.165∗∗∗

0.101
0.105
0.121∗∗∗

0.253
0.287
0.331∗∗∗

0.394
0.425
0.477∗∗∗

0.302
0.363
0.402∗∗∗

0.108
0.134
0.156∗∗∗

0.081
0.092
0.105∗∗∗

In terms of space complexity, parameters Wp, Wc and bc, in ad-
dition to the intermediate results, all require memory space propor-
tional to their size. Considering (2) for a sequence of k words and
batches of m instances, we require O(m·k·|C|) ﬂoating point num-
bers for every forward-pass to ﬁt in-memory. While such an upper
bound seems reasonable by modern computing standards, it is a
severely limiting factor when considering large-scale communities
and while utilizing limited-memory GPUs for fast computation.

The inferential complexity of the vector space-based models for
entity retrieval [19] depends mainly on the dimensionality of the
vectors and the number of candidate experts. The dimensionality
of the latent entity representations is too high for efﬁcient nearest
neighbor retrieval [29] due to the curse of dimensionality. There-
fore, the time complexity for the LSI- and TF-IDF-based vector
space models are respectively O(γ · |C|) and O(|V | · |C|), where
γ denotes the number of latent topics in the LSI-based method. As
hyperparameters e and γ both indicate the dimensionality of la-
tent entity representations, the time complexity of the LSI-based
method is comparable to that of the log-linear model. We note that
|V | (cid:29) |C| for all benchmarks (|V | is between 18 to 91 times larger
than |C|) we consider in this paper and therefore conclude that the
TF-IDF method loses to the log-linear model in terms of efﬁciency.

Compared to the unsupervised generative models of Balog et al.,
we have the proﬁle-centric Model 1 and the document-centric Mo-
del 2 with inference time complexity O(n · |C|) and O(n · |D|),
respectively, with |D| (cid:29) |C|. In the previous section we showed
that the log-linear model always performs better than Model 1 and
nearly always outperforms Model 2. Hence, our log-linear model
generally achieves the expertise retrieval performance of Model 2
(or higher) at the complexity cost of Model 1 during inference.

5.4

Incremental indexing

Existing unsupervised methods use well-understood maximum-
likelihood language models that support incremental indexing. We
now brieﬂy discuss the incremental indexing capabilities of our
proposed method. Extending the set of candidate experts C re-
quires the log-linear model to be re-trained from scratch as it changes
the topology of the network. Moreover, every document associated
with a candidate expert is considered as a negative example for all
other candidates. While it is possible to reiterate over all past doc-
uments and only learn an additional row in matrix Wc, the ﬁnal
outcome is unpredictable.

If we consider a stream of documents instead of a predeﬁned set
D, the log-linear model can be learned in an online fashion. How-
ever, stochastic gradient descent requires that training examples are

(a) W3C 2005

(b) CERC 2007

(c) TU GT1

(d) W3C 2006

(e) CERC 2008

(f) TU GT5

Figure 3: Difference of average precision between log-linear model and Model 2 [4] with Jelinek-Mercer smoothing per topic for
W3C, CERC and TU benchmarks.

picked at random such that the batched update rule (4) behaves like
the empirical expectation over the full training set [11]. While we
might be able to justify the assumption that documents arrive ran-
domly, the n-grams extracted from those documents clearly violate
this requirement.

Considering a stream of documents leads to the model forgetting
expertise evidence as an (artiﬁcial) shift in the underlying distribu-
tion of the training data occurs. While such behavior is undesirable
for the task considered in this paper, it might be well-suited for
temporal expert ﬁnding [22, 52], where expertise drift over time
is considered. However, temporal expertise ﬁnding is beyond the
scope for this paper and left for future work.

6. CONCLUSIONS

We have introduced an unsupervised discriminative, log-linear
model for the expert retrieval task. Our approach exclusively em-
ploys raw textual evidence. Future work can focus on improving
performance by feature engineering and incorporation of external
evidence. Furthermore, no relevance feedback is required during
training. This renders the model suitable for a broad range of ap-
plications and domains.

We evaluated our model on the W3C, CERC and TU bench-
marks and compared it to state-of-the-art vector space-based en-
tity ranking (based on LSI and TF-IDF) and language modeling
(proﬁle-centric and document-centric) approaches. The log-linear
model combines the ranking performance of the best maximum-
likelihood language modeling approach (document-centric) with
inference time complexity linear in the number of candidate ex-
perts. We observed a notable increase in precision over existing
methods. Analysis of our model’s output reveals a negative corre-
lation between the per-query performance and ranking uncertainty:
higher conﬁdence (i.e., lower entropy) in the rankings produced by
our approach often occurs together with higher rank quality.

An error analysis of the log-linear model and traditional lan-
guage models shows that the two make very different errors. These
errors are mainly due to the semantic gap between query intent and
the raw textual evidence. Some benchmarks expect exact query
matches, others are helped by our semantic matching. An ensemble
of methods employing exact and semantic matching generally out-
performs the individual methods. This observation calls for further
research in the area of combining exact and semantic matching.

One current limitation of our work is its scalability with respect
to the number of candidate experts. We have started investigating
trade-offs between model performance and time/space complexity.
In the future we hope to apply scalable variants of this method on
large-scale social media communities, for the purpose of determin-
ing topic ownership. While in this work we focus on expertise re-
trieval, the ideas we proposed can easily be transferred to the more
general entity retrieval task. Moreover, our approach is likely to
be applicable to authorship attribution and various other entity re-
trieval and prediction tasks.

Acknowledgments. We thank Isaac Sijaranamual, Manos Tsagkias, Tom
Kenter, Zhaochun Ren, Ke Tran and Katja Hoffman for their useful com-
ments and insights.

This research was supported by Amsterdam Data Science, the Dutch
national program COMMIT, Elsevier, the European Community’s Seventh
Framework Programme (FP7/2007-2013) under grant agreement nr 312827
(VOX-Pol), the ESF Research Network Program ELIAS, the Royal Dutch
Academy of Sciences (KNAW) under the Elite Network Shifts project, the
Microsoft Research Ph.D. program, the Netherlands eScience Center un-
der project number 027.012.105, the Netherlands Institute for Sound and
Vision, the Netherlands Organisation for Scientiﬁc Research (NWO) under
project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.-
930, CI-14-25, SH-322-15, 652.002.001, 612.001.551, the Yahoo Faculty
Research and Engagement Program, and Yandex. All content represents the
opinion of the authors, which is not necessarily shared or endorsed by their
respective employers and/or sponsors.

Computing resources were provided by the Netherlands Organisation for
Scientiﬁc Research (NWO) through allocation SH-322-15 of the Cartesius
system and by the Advanced School for Computing and Imaging (ASCII)
by allocation of the Distributed ASCII Supercomputer 4 (DAS-4) system.

(a) W3C

(b) CERC

(c) TU

Figure 4: Effect of query expansion by adding nearby terms in Wp (1) in traditional language models (Model 1 [4] with Jelinek-
Mercer smoothing) for W3C, CERC and TU benchmarks.

APPENDIX
The derivative of (3) w.r.t. bias term bc equals

∂L(Wp, Wc, bc)
∂bc

= −

1
m

(cid:32) m
(cid:88)

i=1

|dmax|
|d(i)|

|C|
(cid:88)

j=1

P (cj|d(i))

(cid:16)

∂ log

P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

(cid:33)

and w.r.t. an arbitrary matrix parameter θ (Wp or Wc):

∂L(Wp, Wc, bc)
∂θ

= −

(cid:32) m
(cid:88)

i=1

|dmax|
|d(i)|

|C|
(cid:88)

j=1

P (cj|d(i))

(cid:16)

∂ log

P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

(cid:33)

1
m

λ
m

+

θi,j.

(cid:88)

i,j

Further differentiation for parameter θ (Wp, Wc or bc):

∂bc

∂θ

∂ log (P (cj | w1, . . . , wn))
∂θ

∂P (cj | w1, . . . , wn)
∂θ

1
P (cj | w1, . . . , wn)
∂ ˜P (cj |w1,...,wn)
∂θ

∂P (cj | w1, . . . , wn)
∂θ
Z2 − ˜P (cj | w1, . . . , wn) ∂Z2
∂θ

∂Z2
∂θ

Z 2
2
∂ ˜P (ck | w1, . . . , wn)
∂θ

∂ ˜P (cj | w1, . . . , wn)
∂θ

∂P (cj | wk)
∂θ

(cid:89)

i(cid:54)=k

P (cj | wi)

=

=

=

=

(cid:88)

(cid:88)

k

k

For a given candidate cj and word wi, following (1) we have

P (cj | wi) =

˜P (cj | wi)
Z1
exp (cid:0)(cid:0)(cid:80)e
l=1 exp (cid:0)(cid:0)(cid:80)e

(cid:80)|C|

=

k=1 Wcj,k Wpk,i

k=1 Wcl,k Wpk,i

(cid:1) + bcj

(cid:1)
(cid:1) + bcl

(cid:1)

and consequently, with W (cid:62)
Wp,

pi denoting the i-th column of matrix

∂P (cj | wi)
∂Wcj

∂P (cj | wi)
∂bcj

∂P (cj | wi)
∂W (cid:62)
pi

=

=

=

(cid:17) ˜P (cj | wi)W (cid:62)
(cid:16)
Z1 − ˜P (cj | wi)
Z 2
1
(cid:17) ˜P (cj | wi)
(cid:16)
Z1 − ˜P (cj | wi)
Z 2
1

pi

(cid:16)
Wcj − (cid:80)|C|

(cid:17) ˜P (cj | wi)

l=1 Wcl
Z1

(7)

As can be seen in (7), the distributed representations of candidates
cj at time t + 1 are updated using the representation of words wi at
time t and vice versa.

REFERENCES
[1] The knowledge-based economy. Techn. report, Organisation for

Economic Co-operation and Development, 1996.

[2] P. Bailey, A. P. De Vries, N. Craswell, and I. Soboroff. Overview

of the TREC 2007 enterprise track. In TREC, 2007.
[3] K. Balog. People Search in the Enterprise. PhD thesis,

University of Amsterdam, 2008.

[4] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for
expert ﬁnding in enterprise corpora. In SIGIR, pages 43–50,
2006.

[5] K. Balog, L. Azzopardi, and M. de Rijke. A language modeling

framework for expert ﬁnding. IPM, 45:1–19, 2009.
[6] K. Balog, Y. Fang, M. de Rijke, P. Serdyukov, and L. Si.

Expertise retrieval. Found. & Tr. in Information Retrieval, 6
(2-3):127–256, 2012.

[7] I. Becerra-Fernandez. Role of artiﬁcial intelligence technologies
in the implementation of People-Finder knowledge management
systems. Knowledge-Based Systems, 13(5):315–320, 2000.
[8] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural
probabilistic language model. JMLR, 3:1137–1155, 2003.
[9] Y. Benjamini and Y. Hochberg. Controlling the false discovery
rate: a practical and powerful approach to multiple testing.
JSTOR, pages 289–300, 1995.

[10] R. Berendsen, M. de Rijke, K. Balog, T. Bogers, and A. van den
Bosch. On the assessment of expertise proﬁles. JASIST, 64(10):
2024–2044, 2013.

[11] L. Bottou. Large-scale machine learning with stochastic gradient
descent. In COMPSTAT, pages 177–186. Springer, 2010.
[12] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,

N. Hamilton, and G. Hullender. Learning to rank using gradient
descent. In ICML, pages 89–96, 2005.

[13] Y. Cao, J. Liu, S. Bao, and H. Li. Research on Expert Search at

Enterprise Track of TREC 2005. In TREC, pages 2–5, 2005.
[14] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,
and P. Kuksa. Natural language processing (almost) from
scratch. JMLR, 12(Aug):2493–2537, 2011.

[15] N. Craswell, D. Hawking, A.-M. Vercoustre, and P. Wilkins.

P@noptic expert: Searching for experts not just for documents.
In Ausweb Poster Proceedings, pages 21–25, 2001.

[16] N. Craswell, A. P. de Vries, and I. Soboroff. Overview of the

TREC 2005 enterprise track. In TREC, 2005.

[17] T. H. Davenport and L. Prusak. Working Knowledge. Harvard

Business Review Press, 1998.

[18] S. C. Deerwester, S. T. Dumais, and R. A. Harshman. Indexing

by latent semantic analysis. JASIS, 1990.

[19] G. Demartini, J. Gaugaz, and W. Nejdl. A vector space model

[41] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Distributed

for ranking entities and its application to expert search. In ECIR,
pages 189–201. Springer, 2009.

representations of words and phrases and their compositionality.
In NIPS, pages 3111–3119, 2013.

[20] L. Deng, X. He, and J. Gao. Deep stacking networks for

information retrieval. In ICASSP, pages 3153–3157, 2013.
[21] H. Fang and C. Zhai. Probabilistic models for expert ﬁnding. In

ECIR, pages 418–430, Berlin, Heidelberg, 2007.
Springer-Verlag.

[22] Y. Fang and A. Godavarthy. Modeling the dynamics of personal

expertise. In SIGIR, pages 1107–1110, 2014.

[23] Y. Fang, L. Si, and A. P. Mathur. Discriminative models of

integrating document evidence and document-candidate
associations for expert search. In SIGIR, pages 683–690, 2010.

[24] K. Fatahalian, J. Sugerman, and P. Hanrahan. Understanding the
efﬁciency of gpu algorithms for matrix-matrix multiplication. In
SIGGRAPH HWWS, pages 133–137. ACM, 2004.

[25] X. Glorot and Y. Bengio. Understanding the difﬁculty of training
deep feedforward neural networks. In AISTATS, pages 249–256,
2010.

[26] G. E. Hinton. Learning distributed representations of concepts.
In 8th Annual Conference of the Cognitive Science Society,
volume 1, page 12, Amherst, MA, 1986.

[27] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR,

pages 50–57. ACM, 1999.

[28] P.-s. Huang, N. M. A. Urbana, X. He, J. Gao, L. Deng, A. Acero,
and L. Heck. Learning deep structured semantic models for web
search using clickthrough data. In CIKM, pages 2333–2338,
2013.

[29] P. Indyk and R. Motwani. Approximate nearest neighbors:

towards removing the curse of dimensionality. In STOC, pages
604–613. ACM, 1998.

[30] M. Karimzadehgan and C. Zhai. Estimation of statistical

translation models based on mutual information for ad hoc
information retrieval. In SIGIR, pages 323–330. ACM, 2010.
[31] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural

language models. In ICML, pages 595–603, 2014.

[32] J. Kruger and D. Dunning. Unskilled and unaware of it: how

difﬁculties in recognizing one’s own incompetence lead to
inﬂated self-assessments. J. Personality and Social Psych., 77
(6):1121, 1999.

[33] J. Krüger and R. Westermann. Linear algebra operators for gpu
implementation of numerical algorithms. ACM Transactions on
Graphics, 22(3):908–916, 2003.

[34] H. Li and J. Xu. Semantic matching in search. Found. & Tr. in

Information Retrieval, 7(5):343–469, June 2014.

[35] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer,

2011.

[36] C. MacDonald and I. Ounis. Voting for candidates: adapting

data fusion techniques for an expert search task. In CIKM, pages
387–396, 2006.

[37] C. Macdonald and I. Ounis. Expert search evaluation by

supporting documents. In ECIR, pages 555–563. Springer, 2008.

[38] M. T. Maybury. Expert ﬁnding systems. Techn. Report

[39] D. W. McDonald and M. S. Ackerman. Expertise recommender.

MTR-06B000040, MITRE, 2006.

In CSCW, pages 231–240, 2000.

[42] T. Mikolov, G. Corrado, K. Chen, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
1301.3781, 2013.

[43] A. Mnih and G. Hinton. Three new graphical models for

statistical language modelling. In ICML, pages 641–648, 2007.

[44] A. Mnih and G. Hinton. A scalable hierarchical distributed
language model. In NIPS, pages 1081–1088, 2008.
[45] A. Mnih and K. Kavukcuoglu. Learning word embeddings

efﬁciently with noise-contrastive estimation. In NIPS, pages
2265–2273, 2013.

[46] G. Montavon, G. B. Orr, and K.-R. Müller. Neural Networks:

Tricks of the Trade. Springer, 2012.

[47] C. Moreira, B. Martins, and P. Calado. Using rank aggregation
for expert search in academic digital libraries. In Simpósio de
Informática, INForum, pages 1–10, 2011.

[48] J. Pennington, R. Socher, and C. D. Manning. GloVe: Global

Vectors for Word Representation. In EMNLP, pages 1532–1543,
2014.

[49] D. Petkova and W. B. Croft. Hierarchical language models for
expert ﬁnding in enterprise corpora. In ICTAI ’06, pages
599–606, 2006.

[50] W. W. Powell and K. Snellman. The knowledge economy.
Annual review of sociology, pages 199–220, 2004.

[51] D. Rumelhart, G. Hinton, and R. Williams. Learning internal
representations by back propagation. In Parallel Distributed
Processing, pages 318–362. MIT Press, 1986.

[52] J. Rybak, K. Balog, and K. Nørvåg. Temporal expertise
proﬁling. In ECIR, pages 540–546. Springer, 2014.
[53] R. Salakhutdinov and G. Hinton. Semantic hashing. Int. J.

Approximate Reasoning, 50(7):969–978, 2009.

[54] P. Serdyukov and D. Hiemstra. Modeling documents as mixtures
of persons for expert ﬁnding. In ECIR, pages 309–320. Springer,
2008.

[55] P. Serdyukov, H. Rode, and D. Hiemstra. Modeling multi-step
relevance propagation for expert ﬁnding. In CIKM, pages
1133–1142, 2008.

[56] C. Shannon. A mathematical theory of communication. Bell

System Technical J., 27:379–423, 623–656, 1948.

[57] J. A. Shaw, E. A. Fox, J. A. Shaw, and E. A. Fox. Combination

of multiple searches. In TREC, pages 243–252, 1994.
[58] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. A latent
semantic model with convolutional-pooling structure for
information retrieval. In CIKM, pages 101–110, 2014.
[59] M. D. Smucker, J. Allan, and B. Carterette. A comparison of

statistical signiﬁcance tests for information retrieval evaluation.
In CIKM, pages 623–632. ACM, 2007.

[60] P. Sorg and P. Cimiano. Finding the right expert: Discriminative
models for expert retrieval. In KDIR, pages 190–199, 2011.

[61] TREC. Enterprise Track, 2005–2008.
[62] D. van Dijk, M. Tsagkias, and M. de Rijke. Early detection of
topical expertise in community question and answering. In
SIGIR, 2015.

[63] V. Vapnik. Statistical learning theory, volume 1. Wiley New

[40] T. Mikolov, M. Karaﬁat, L. Burget, J. Cernocky, and

York, 1998.

S. Khudanpur. Recurrent neural network based language model.
In Interspeech, pages 1045–1048, 2010.

[64] M. D. Zeiler. Adadelta: An adaptive learning rate method.

CoRR, abs/1212.5701, 2012.

Unsupervised, Efﬁcient and Semantic Expertise Retrieval

Christophe Van Gysel
cvangysel@uva.nl

Maarten de Rijke
derijke@uva.nl

Marcel Worring
m.worring@uva.nl

University of Amsterdam, Amsterdam, The Netherlands

7
1
0
2
 
p
e
S
 
7
1
 
 
]

R

I
.
s
c
[
 
 
2
v
1
5
6
6
0
.
8
0
6
1
:
v
i
X
r
a

ABSTRACT
We introduce an unsupervised discriminative model for the task
of retrieving experts in online document collections. We exclu-
sively employ textual evidence and avoid explicit feature engineer-
ing by learning distributed word representations in an unsupervised
way. We compare our model to state-of-the-art unsupervised sta-
tistical vector space and probabilistic generative approaches. Our
proposed log-linear model achieves the retrieval performance lev-
els of state-of-the-art document-centric methods with the low infer-
ence cost of so-called proﬁle-centric approaches. It yields a statis-
tically signiﬁcant improved ranking over vector space and genera-
tive models in most cases, matching the performance of supervised
methods on various benchmarks. That is, by using solely text we
can do as well as methods that work with external evidence and/or
relevance feedback. A contrastive analysis of rankings produced
by discriminative and generative approaches shows that they have
complementary strengths due to the ability of the unsupervised dis-
criminative model to perform semantic matching.

1.

INTRODUCTION

The transition to the knowledge and information economy [1]
introduces a great reliance on cognitive capabilities [50]. It is cru-
cial for employers to facilitate information exchange and to stim-
ulate collaboration [17].
In the past, organizations would set-up
special-purpose database systems for their members to maintain
a proﬁle [7]. However, these systems required employees to be
proactive. In addition, self-assessments are known to diverge from
reality [10, 32] and document collections can quickly become prac-
tically infeasible to manage manually. Therefore, there has been an
active interest in automated approaches for constructing expertise
proﬁles [7, 61] and retrieving experts from an organization’s het-
erogeneous document repository [15]. Expert ﬁnding (also known
as expertise retrieval or expert search) addresses the task of ﬁnding
the right person with the appropriate skills and knowledge [6]. It
attempts to provide an answer to the question:

Given a topic X, who are the candidates with the most
expertise w.r.t. X?

This is an electronic version of an Article published in the 25th International
World Wide Web Conference.
c(cid:13) 2016 International World Wide Web Conference Committee.
WWW 2016, April 11–15, 2016, Montréal, Québec, Canada.
ACM 978-1-4503-4143-1/16/04.
http://dx.doi.org/10.1145/2872427.2882974.

The expertise retrieval task gained popularity in the research com-
munity during the TREC Enterprise Track [61] and has remained
relevant ever since, while broadening to social media and to track-
ing the dynamics of expertise [5, 6, 10, 19, 21–23, 47, 49, 62].
Existing methods fail to address key challenges: (1) Queries and
expert documents use different representations to describe the same
concepts [26, 34]. Term mismatches between queries and experts
[34] occur due to the inability of widely used maximum-likelihood
language models to make use of semantic similarities between words
[53]. (2) As the amount of available data increases, the need for
more powerful approaches with greater learning capabilities than
smoothed maximum-likelihood language models is obvious [63].
(3) Supervised methods for expertise retrieval [23, 47] were intro-
duced at the turn of the last decade. However, the acceleration
of data availability has the major disadvantage that, in the case of
supervised methods, manual annotation efforts need to sustain a
similar order of growth. This calls for the further development of
unsupervised methods. (4) In some expertise retrieval methods, a
language model is constructed for every document in the collection.
These methods lack efﬁcient query capabilities for large document
collections, as each query term needs to be matched against ev-
ery document [6]. Our proposed solution has a strong emphasis on
unsupervised model construction, efﬁcient query capabilities and
semantic matching between query terms and candidate experts.

Speciﬁcally, we propose an unsupervised log-linear model with
efﬁcient inference capabilities for the expertise retrieval task. We
show that our approach improves retrieval performance compared
to vector space-based and generative language models, mainly due
to its ability to perform semantic matching [34]. Our method does
not require supervised relevance judgments and is able to learn
from raw textual evidence and document-candidate associations
alone. The purpose of this work is to provide insight in how dis-
criminative language models can improve performance of core re-
trieval tasks compared to maximum-likelihood language models.
Therefore, we avoid explicit feature engineering and the incorpo-
ration of external evidence in this paper. In terms of performance,
the current best-performing formal language model [4] exhibits a
worst-case time complexity linear in the size of the document col-
lection. In contrast, the inference time complexity of our approach
is asymptotically bounded by the number of candidate experts.

Our research questions are as follows: (1) How does our dis-
criminative log-linear model compare to vector space-based meth-
ods and generative language models for the expert retrieval task in
terms of retrieval performance? (2) What can we learn regarding
the different types of errors made by generative and discriminative
language models? (3) How does the complexity of inference in
our log-linear model compare to vector-space based and generative

models? (4) How does the log-linear model handle incremental
indexing and what are its limitations?

We contribute: (i) An unsupervised log-linear model with efﬁ-
cient inference capabilities for the expertise retrieval task, together
with an open-source implementation.1 (ii) Comparison of the re-
trieval performance of the log-linear model with traditional vector
space-based models and language model methods on well-known
benchmarks. (iii) Insights in how certainty in predictions by the
log-linear model correlates with performance. (iv) In-depth analy-
sis of the inference complexity of the log-linear model. (v) Com-
parative error analysis between the semantic log-linear model and
traditional generative language models that perform exact match-
ing. (vi) Insight in the relative strengths of semantic matching and
exact matching for the expert retrieval task.

The remainder of this paper is organized as follows. In Section 2,
we brieﬂy discuss related work. Section 3 introduces the log-linear
model for the expert retrieval problem. In Section 4 we state our
research questions and detail our experimental set-up and imple-
mentations. We provide an overview of our experimental results,
followed by a discussion and analysis in Section 5. Section 6 con-
cludes the paper and discusses ideas for future work.

2. RELATED WORK

We ﬁrst discuss prior work on expert retrieval and its relation
to document retrieval. Then, we review semantic search methods.
The ideas we present in this paper are inspired by neural language
models used in automated speech recognition (ASR) and natural
language processing (NLP). Consequently, we also review work
from those ﬁelds.

2.1 Expert retrieval

Early expert retrieval systems were often referred to as expert
locator and expertise management systems [38]. These database
systems often relied on people to self-assess their expertise against
a predeﬁned set of topics [39], which is known to generate unreli-
able results [7].

With the introduction of the P@NOPTIC system [15], and later
the TREC Enterprise track [61], there has been an active research
interest in automated expertise proﬁling methods. It is useful to dis-
tinguish between proﬁle-based methods, which create a textual rep-
resentation of a candidate’s knowledge, and document-based meth-
ods, which represent candidates as a weighted combination of doc-
uments. The latter generally performs better at ranking, while the
former is more efﬁcient as it avoids retrieving all documents rele-
vant to a query [6, p. 221].

There has been much research on generative probabilistic mod-
els for expert retrieval [21, 49]. Such models have been categorized
in candidate generation models [13], topic generation models [4, 5]
and proximity-based variants [5, 54]. Of special relevance to us are
the unsupervised proﬁle-centric (Model 1) and document-centric
(Model 2) models of Balog et al. [4], which focus on raw textual ev-
idence without incorporating collection-speciﬁc information (e.g.,
query modeling, document importance or document structure). Su-
pervised discriminative models [23, 47, 60] are preferred when
query-candidate relevance pairs are available for training. Unlike
their generative counterparts these models have no issue combining
complex and heterogeneous features (e.g., link-based features, doc-
ument importance features, etc.); they resemble Learning to Rank
(L2R) methods for document retrieval [6, 35]. However, a lack
of training data may greatly hinder their applicability [6, p. 179].
Beyond unsupervised generative and supervised discriminative ap-

1https://github.com/cvangysel/SERT

proaches, there are graph-based approaches based on random walks
[55] and voting-based approaches based on data fusion [36]. De-
martini et al. [19] propose a vector space-based method for the en-
tity ranking task; their framework extends vector spaces operating
on documents to entities. See [6] for a survey on the topic.

2.2 Latent semantic models for document re-

trieval

Li and Xu [34] note that the query document mismatch poses the
most critical challenge in search. Semantic matching is an impor-
tant attempt to remedy this problem. There has been much work
on bridging the semantic gap for various different tasks [19, 26, 28,
30, 31, 41, 48, 53]. Expertise and document retrieval [6, p. 224] are
closely related as performance of the latter can greatly impact that
of the former [37].

Latent Semantic Models (LSM) ﬁrst became popular through
the introduction of Latent Semantic Indexing (LSI) [18], followed
by probabilistic LSI (pLSI) [27]. LSMs based on neural networks
[28, 53, 58] emerged in the last decade. Salakhutdinov and Hinton
[53] employ unsupervised deep auto-encoders to map documents to
bit patterns using semantic addressing. Huang et al. [28] perform
semantic matching of documents and queries by leveraging click-
through data optimizing for Web document ranking. Further, deep
models [12, 20] have been proposed to learn to rank [35].

2.3 Neural language modeling

Large-vocabulary neural probabilistic language models for mod-
eling word sequence distributions have become very popular re-
cently [8, 43, 44]. These models learn continuous-valued distributed
representations for words, also known as embeddings [42, 45, 48],
in order to ﬁght the curse of dimensionality and increase general-
ization by introducing the expectation that similar word vectors sig-
nify semantically or syntactically similar words. Recurrent neural
language models have shown to perform well in ASR [40]. Col-
lobert et al. [14] propose a uniﬁed neural network architecture for
various NLP tasks. Even more recently, there has been a surge in
multimodal neural language models [31], which lend themselves to
the task of automated image captioning.

What we add on top of the related work described above is the
following.
In this work we model the conditional probability of
the expertise of a candidate given a single query term (contrary
to binary relevance given a character-based n-gram [28]). In the
process we learn a distributed vector representation (similar to LSI,
pLSI and semantic hashing) for both words and candidates such
that nearby representations indicate semantically similar concepts.
We propose a log-linear model that is similar to neural language
models. The important difference is that we predict a candidate
expert instead of the next word. To the best of our knowledge,
we are the ﬁrst to propose such a solution. We employ an embed-
ding layer in our shallow model for the same reasons as mentioned
above: we learn continuous word representations that incorporate
semantic and syntactic similarity tailored to an expert’s domain.

3. A LOG-LINEAR MODEL FOR

of words w1, . . . , wk we have:

EXPERT SEARCH

In the setting of this paper we have a document collection D and
a predeﬁned set of candidate experts C (entities to be retrieved).
Documents d ∈ D are represented as a sequence of words w1, . . . ,
w|d| originating from a vocabulary V , where wi ∈ V and the oper-
ator |·| denotes the document length in tokens. For every document
d ∈ D we write Cd to denote the set of candidates c ∈ C asso-
ciated with it (i.e., C = (cid:83)
d∈D Cd). These document-candidate
associations can be obtained explicitly from document meta-data
(e.g., the author of an e-mail) or implicitly by mining references
to candidates from the document text. Notice that some documents
might not be associated with any candidate. When presented with a
query q of constituent terms t1, . . . , t|q|, the expert retrieval task is
to return a list of candidates ρ(C) ordered according to topical ex-
pertise. We generate this ranking using a relatively shallow neural
network which directly models P (c | q).

We employ vector-based distributed representations [26], for both
words (i.e., word embeddings) and candidate experts, in a way that
motivates the unsupervised construction of features that express
regularities of the expertise ﬁnding domain. These representations
can capture the similarity between concepts (e.g., words and can-
didate experts) by the closeness of their representations in vector
space. That is, concepts with similar feature activations are inter-
preted by the model as being similar, or even interchangeable.

3.1 The model

To address the expert search task, we model P (cj | q) and rank
candidates cj accordingly for a given q. We propose an unsuper-
vised, discriminative approach to obtain these probabilities. We
construct our model solely from textual evidence: we do not re-
quire query-candidate relevance assessments for training and do not
consider external evidence about the corpus (e.g., different weight-
ings for different sub-collections), the document (e.g., considering
certain parts of the document more useful) nor link-based features.
Let e denote the size of the vector-based distributed representa-
tions of both words in V and candidate experts in C. These repre-
sentations will be learned by the model using gradient descent [42]
(Section 3.2). For notational convenience, we write P (c | ·) for the
(conditional) probability distribution over candidates, which is the
result of vector arithmetic. We deﬁne the probability of a candidate
cj given a single word wi ∈ V as the log-linear model

P (c | wi) =

exp (Wc · (Wp · vi) + bc) ,

(1)

1
Z1

where Wp is the e × |V | projection matrix that maps the one-hot
representation (i.e., 1-of-|V |) of word wi, vi, to its e-dimensional
distributed representation, bc is a |C|-dimensional bias vector and
Wc is the |C| × e matrix that maps the word embedding to an un-
normalized distribution over candidates C, which is then normal-
ized by Z1 = (cid:80)|C|
j=1 [exp (Wc · (Wp · vi) + bc)]j. If we consider
Bayes’ theorem, the transformation matrix Wc and bias vector bc
can be interpreted as the term log-likelihood log P (wi | c) and
candidate log-prior log P (c), respectively. The projection matrix
Wp attempts to soften the curse of dimensionality introduced by
large vocabularies V and maps words to word feature vectors [8].
Support for large vocabularies is crucial for retrieval tasks [28, 53].
We then assume conditional independence of a candidate’s ex-
pertise given an observation of data (i.e., a word). Given a sequence

P (c | w1, . . . , wk) =

˜P (c | w1, . . . , wk) =

P (c | wi)

1
Z2

1
Z2

(cid:32) k

(cid:88)

1
Z2

k
(cid:89)

i=1
(cid:33)

=

exp

log (P (c | wi))

(2)

(cid:16)(cid:80)k

j=1 exp

i=1
where ˜P (c | w1, . . . , wk) denotes the unnormalized score and
(cid:17)
Z2 = (cid:80)|C|
i=1 log (P (cj | wi))
is a normalizing term.
The transformation to log-space in (2) is a well-known trick to pre-
vent ﬂoating point underﬂow [46, p. 445]. Given (2), inference is
straight-forward. That is, given query q = t1, . . . , tk, we compute
P (c | t1, . . . , tk) and rank the candidate experts in descending
order of probability.

Eq. 1 deﬁnes a neural network with a single hidden layer. We
can add additional layers. Preliminary experiments, however, show
that the shallow log-linear model (1) performs well-enough in most
cases. Only for larger data sets did we notice a marginal gain from
adding an additional layer between projection matrix Wp and the
softmax layer over C (Wc and bias bc), at the expense of longer
training times and loss of transparency.

3.2 Parameter estimation

The matrices Wp, Wc and the vector bc in (1) constitute the
parameters of our model. We estimate them using error back prop-
agation [51] as follows. For every document dj ∈ D we construct
an ideal distribution over candidates p = P (c | dj) based on the
document-candidate associations Cdj such that

P (c | dj) =

(cid:40) 1
|Cdj
0,

| ,

c ∈ Cdj

c (cid:54)∈ Cdj

We continue by extracting n-grams where n remains ﬁxed during
training. For every n-gram w1, . . . , wn generated from document
d we compute ˜p = P (c | w1, . . . , wn) using (2). During model
constructing we then optimize the cross-entropy H(p, ˜p) (i.e., the
joint probability of the training data if |Cdj | = 1 for all j) using
batched gradient descent. The loss function for a single batch of m
instances with associated targets (p(i), ˜p(i)) is as follows:

L(Wp, Wc, bc)

=

1
m

+

m
(cid:88)

i=1

λ
2m

|dmax|
|d(i)|
(cid:32)

(cid:88)

i,j

H(p(i), ˜p(i))

W 2

pi,j +

W 2

ci,j

(cid:88)

i,j

= −

|dmax|
|d(i)|

|C|
(cid:88)

j=1

1
m

m
(cid:88)

i=1
(cid:32)

+

λ
2m

(cid:88)

i,j

W 2

pi,j +

W 2

ci,j

,

(cid:88)

i,j

(cid:33)

(cid:33)

(3)

P (cj | d(i)) log

(cid:16)
P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

where d(i) refers to the document from which n-gram w(i)
1 , . . . , w(i)
n
was extracted, dmax = arg maxd∈D |d| indicates the longest doc-
ument in the collection, and λ is a weight regularization parameter.
The update rule for a particular parameter θ (Wp, Wc or bc) given
a single batch of size m is:

θ(t+1) = θ(t) − α(t) (cid:12)

∂L(Wp

(t), bc

(t))

(t), Wc
∂θ

,

(4)

where α(t) and θ(t) denote the per-parameter learning rate and pa-
rameter θ at time t, respectively. The learning rate α consists of
the same number of elements as there are parameters; in the case
of a global learning rate, all elements of α are equal to each other.
The derivatives of the loss function (3) are given in the Appendix.
In the next section we will discuss our experimental setup, fol-
lowed by an overview of our experimental results and further anal-
ysis in Section 5.

4. EXPERIMENTAL SETUP

4.1 Research questions

As indicated in the introduction, we seek to answer the following

research questions:
RQ1 How does our discriminative log-linear model compare to
vector space-based methods and generative language models
for the expert retrieval task in terms of retrieval performance?
In particular, how does the model perform when compared against
vector space-based (LSI and TF-IDF) and generative approaches
(proﬁle-centric Model 1 and document-centric Model 2)?
RQ2 What can we learn regarding the different types of errors

made by generative and discriminative language models?

Does the best-performing generative model simply perform slightly
better on the topics for which the other models perform decent as
well, or do they make very different errors? If the latter holds,
an ensemble of the rankings produced by both model types might
exceed performance of the individual rankings.
RQ3 How does the complexity of inference in our log-linear model
compare to vector-space based and generative models?
The worst-case inference cost of document-centric models makes
them unattractive in online settings where the set of topics is not
deﬁned beforehand and the document collection is large. Proﬁle-
centric methods are preferred in such settings as they infer from
one language model per candidate expert for every topic (i.e., a
pseudo-document consisting of a concatenation of all documents
associated with an expert) [6]. Vector space-based methods [19]
have similar problems due to the curse of dimensionality [29] and
consequently their inferential time complexity is likewise asymp-
totically bounded by the number of experts.
RQ4 How does the log-linear model handle incremental indexing

and what are its limitations?

4.2 Benchmarks

The proposed method is applicable in the setting of the Expert
Search task of the TREC Enterprise track from 2005 to 2008 [61].
We therefore evaluate on the W3C and CERC benchmarks released
by the track. The W3C dataset [16] is a crawl of the W3C’s sites
in June 2004 (mailing lists, web pages, etc.). The CSIRO Enter-
prise Research Collection (CERC) [2] is a dump of the intranet
of Australia’s national science agency. Additionally, we evaluate
our method on a smaller, more recent benchmark based on the em-
ployee database of Tilburg University (TU) [10], which consists of
bi-lingual, heterogeneous documents. See Table 1.

Mining document-candidate associations and how they inﬂuence
performance has been extensively covered in previous work [4, 6]
and is beyond the scope of this work. For TU, the associations
are part of the benchmark. For W3C, a list of possible candidates
is given and we extract the associations ourselves by performing
a case-insensitive match of full name or e-mail address [4]. For
CERC, we make use of publicly released associations [3].

As evaluation measures we use Mean Average Precision (MAP),
Mean Reciprocal Rank (MRR), Normalized Discounted Cumula-

tive Gain at rank 100 (NDCG@100) and Precision at rank 5 (P@5)
and rank 10 (P@10).

4.3 Baselines

We compare our approach to existing unsupervised methods for
expert retrieval that solely rely on textual evidence and static doc-
ument-candidate associations. (1) Demartini et al. [19] propose a
generic framework to adapt vector spaces operating on documents
to entities. We compare our method to TF-IDF (raw frequency and
inverse document frequency) and LSI (300 latent topics) variants
of their vector space model for entity ranking (using cosine sim-
ilarity). (2) In terms of language modeling, Balog et al. [4] pro-
pose two models for expert ﬁnding based on generative language
models. The ﬁrst takes a proﬁle-centric approach comparing the
language model of every expert to the query, while the second is
document-centric. We consider both models with different smooth-
ing conﬁgurations: Jelinek-Mercer (jm) smoothing with λ = 0.5
[4] and Dirichlet (d) smoothing with β equal to the average doc-
ument length [5] (see Table 1). Signiﬁcance of results produced
by the baselines (compared to our method) is determined using a
two-tailed paired randomization test [59].

4.4

Implementation details

The vocabulary V is constructed from each corpus by ignoring
punctuation, stop words and case; numbers are replaced by a nu-
merical placeholder token. During our experiments we prune V
by only retaining the 216 most-frequent words so that each word
can be encoded by a 16-bit unsigned integer. Incomplete n-gram
instances are padded by a special-purpose token.

In terms of parameter initialization, we sample the initial matri-

ces Wc and Wp (1) uniformly in the range
(cid:34)
(cid:114) 6.0

(cid:114) 6.0

(cid:35)

−

,

m + n

m + n

for an m × n matrix, as this initialization scheme improves model
training convergence [25], and take the bias vector bc to be null.
The projection layer Wp is initialized with pre-trained word repre-
sentations trained on Google News data [41]; the number of word
features is set to e = 300, similar to pre-trained representations.

We used adadelta (ρ = 0.95, (cid:15) = 10−6) [64] with batched gra-
dient descent (m = 1024) and weight decay λ = 0.01 during
training on NVidia GTX480 and NVidia Tesla K20 GPUs. We only
iterate once over the entire training set for each experiment.

5. RESULTS AND DISCUSSION

We start by giving a high-level overview of our experimental re-
sults and then address issues of scalability, provide an error analysis
and discuss the issue of incremental indexing.

5.1 Overview of experimental results

We evaluate the log-linear model on the W3C, CERC and TU
benchmarks (Section 4.2). During training we extract non-overlap-
ping n-grams for the W3C and CERC benchmarks and overlapping
n-grams for the TU benchmark. As the TU benchmark is consider-
ably smaller, we opted to use overlapping n-grams to counter data
sparsity. The architecture of each benchmark model (e.g., num-
ber of candidate experts) is inherently speciﬁed by the benchmarks
themselves (see Table 1). However, the choice of n-gram size dur-
ing training remains open. Errors for input w1, . . . , wn are propa-
gated back through Wc until the projection matrix Wp is reached;
if a single word wi causes a large prediction error, then this will in-
ﬂuence its neighboring words w1, . . . , wi−1, wi+1, . . . , wn as well.

Table 1: An overview of the three datasets (W3C, CERC and TU) used for evaluation and analysis.

Number of documents
Average document lengtha

Number of candidatesb

Number of document-candidate associations
Number of documents (with Cd > 0)
Number of associations per documentc
Number of associations per candidate

Queries

W3C

CERC

331,037

1,237.23

370,715

460.48

715

200,939
93,826

3,479

236,958
123,934

TU

31,209
2,454.93

977

36,566
27,834

2.14 ± 3.29
281.03 ± 666.63

1.91 ± 3.70
68.11 ± 1,120.74

1.13 ± 0.39
37.43 ± 61.00

49
50

(2005)
(2006)

50
77

(2007)
(2008)

1,662
1,266

(GT1)
(GT5)

a Measured in number of tokens.
b Only candidates with at least a single document association are considered.
c Only documents with at least one association are considered.

This allows the model to learn continuous word representations tai-
lored to the expert retrieval task and the benchmark domain.

A larger window size has a negative impact on batch throughput
during training. We are thus presented with the classic trade-off
between model performance and construction efﬁciency. Notice,
however, that the number of n-grams decreases as the window size
increases if we extract non-overlapping instances. Therefore, larger
values of n lead to faster wall-clock time model construction for the
W3C and CERC benchmarks in our experiments.

We sweep over the window width n = 2i (0 ≤ i < 6) for all
three benchmarks and their corresponding relevance assessments.
We report MAP and MRR for every conﬁguration (see Figure 1).
We observe a signiﬁcant performance increase between n = 1
and n = 2 on all benchmarks, which underlines the importance
of the window size parameter. The increase in MAP implies that
the performance achieved is not solely due to initialization with
pre-trained representations (Section 4.4), but that the model efﬁ-
ciently learns word representations tailored to the problem domain.
The highest MAP scores are attained for relatively low n. As n
increases beyond n = 8 a gradual decrease in MAP is observed on
all benchmarks. In our remaining experiments we choose n = 8
regardless of the benchmark.

Words wi that mainly occur in documents associated with a par-
ticular expert are learned to produce distributions P (c | wi) with
less uncertainty than words associated with many experts in (1).
The product of P (c | wi) in (2) aggregates this expertise evidence
generated by query terms. Hence, queries with strong evidence for
a particular expert should be more predictable than very generic
queries. To quantify uncertainty we measure the normalized en-
tropy [56] of P (c | q):

η(c | q) = −

p(cj | q) log(p(cj | q)).

(5)

1
log(|C|)

|C|
(cid:88)

j=1

Equation 5 can be interpreted as a similarity measure between the
given distribution and the uniform distribution. Importantly, Fig-
ure 2 shows that there is a statistically signiﬁcant negative correla-
tion between query-wise normalized entropy and average precision
for all benchmarks.

Table 2 presents a comparison between the log-linear model and
the various baselines (Section 4.3). Our unsupervised method sig-
niﬁcantly (p < 0.01) outperforms the LSI-based method consis-
tently. In the case of the TF-IDF method and the proﬁle-centric

generative language models (Model 1), we always perform bet-
ter and statistical signiﬁcance is achieved in the majority of cases.
The document-centric language models (Model 2) perform slightly
better than our method on two (out of six) benchmarks in terms
of MAP and NDCG@100:(1) For the CERC 2007 assessment we
match performance of the document-centric generative model with
Jelinek-Mercer smoothing. (2) For TU GT1 the generative coun-
terpart seems to outperform our method.

Notice that over all assessments, the log-linear model consis-
tently outperforms all proﬁle-centric approaches and is only chal-
lenged by the smoothed document-centric approach. In addition,
for the precision-based measures (P@k and MRR), the log-linear
model consistently outperforms all other methods we compare to.
Next, we turn to a topic-wise comparative analysis of discrimi-
native and generative models. After that, we analyze the scalability
and efﬁciency of the log-linear model and compare it to that of the
generative counterparts, and address incremental indexing.

5.2 Error analysis

How does our log-linear model achieve its superior performance
over established generative models? Figure 3 depicts the per-topic
differences in average precision between the log-linear model and
Model 2 (with Jelinek-Mercer smoothing) on all benchmarks. For
each plot, the vertical bars with a positive AP difference correspond
to test topics for which the log-linear model outperforms Model 2
and vice versa for bars with a negative AP difference.

The beneﬁt gained from the projection matrix Wp is two-fold.
First, it avoids the curse of dimensionality introduced by large vo-
cabularies. Second, term similarity with respect to the expertise
domain is encoded in latent word features. When examining words
nearby query terms in the embedding space, we found words to be
related to the query term. For example, word vector representations
of xml and nonterminal are very similar for the W3C benchmark (l2
norm). This can be further observed in Figure 1: log-linear models
trained on single words perform signiﬁcantly worse compared to
those that are able to learn from neighboring words.

We now take a closer look at the topics for which the log-linear
model outperforms Model 2 and vice versa. More speciﬁcally, we
investigate textual evidence related to a topic and whether it is con-
sidered relevant by the benchmark. For the log-linear model, we ex-
amine terms nearby topic terms in Wp (l2-norm), as these terms are
considered semantically similar by the model and provide a means
for semantic matching. For every benchmark, we ﬁrst consider top-

Table 2: Evaluation results for models trained on the W3C, CERC and TU benchmarks. Sufﬁxes (d) and (jm) denote Dirichlet and
Jelinek-Mercer smoothing, respectively (Section 4.3). Signiﬁcance of results is determined using a two-tailed paired randomization
test [59] (∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) with respect to the log-linear model (adjusted using the Benjamini-Hochberg
procedure for multiple testing [9]).

W3C

MAP NDCG@100 MRR

P@5 P@10 MAP

2005

2006
NDCG@100 MRR

CERC

MAP NDCG@100 MRR

P@5 P@10 MAP

2008
NDCG@100 MRR

0.135
LSI
0.243
TF-IDF
0.192
Model 1 (d)
0.190
Model 1 (jm)
0.198
Model 2 (d)
0.211
Model 2 (jm)
Log-linear (ours) 0.248

0.031
LSI
0.332
TF-IDF
0.287
Model 1 (d)
0.278
Model 1 (jm)
0.352
Model 2 (d)
0.361
Model 2 (jm)
Log-linear (ours) 0.344

0.095
LSI
0.216
TF-IDF
0.171
Model 1 (d)
0.189
Model 1 (jm)
0.154
Model 2 (d)
0.234
Model 2 (jm)
Log-linear (ours) 0.219

0.266
0.426
0.358
0.352
0.369
0.380
0.444

0.107
0.486
0.427
0.420
0.495
0.500
0.493

0.205
0.356
0.308
0.325
0.284
0.370
0.356

0.192 0.196
0.306
0.384 0.350
0.541
0.276 0.266
0.433
0.272 0.276
0.390
0.288 0.272
0.429
0.451
0.332 0.296
0.618∗ 0.412 0.361

2007

0.060
0.463
0.384
0.384
0.454
0.467
0.513

GT1

0.016 0.014
0.196 0.141
0.156 0.096
0.156 0.084
0.180 0.138
0.192 0.138
0.215 0.150

0.153
0.324
0.258
0.277
0.228
0.342
0.351

0.060 0.051
0.131 0.097
0.103 0.082
0.112 0.085
0.087 0.070
0.136 0.101
0.145∗ 0.105

0.245
0.343
0.321
0.311
0.261
0.260
0.484∗∗∗

0.038
0.269
0.181
0.170
0.264
0.274
0.342∗∗∗

0.097
0.233
0.241
0.231
0.191
0.253
0.287∗∗∗

0.371
0.531
0.491
0.483
0.419
0.423
0.667∗∗∗

0.482
0.650
0.635
0.596
0.551
0.599
0.833∗∗∗

0.099
0.465
0.355
0.347
0.461
0.463
0.519∗∗

0.106
0.525
0.388
0.339
0.510
0.517
0.656∗∗

0.208
0.378
0.385
0.373
0.334
0.394
0.425∗∗∗

0.129
0.288
0.292
0.271
0.233
0.302
0.363∗∗∗

TU

MAP NDCG@100 MRR

P@5 P@10 MAP

GT5
NDCG@100 MRR

P@5

P@10

0.287
0.492
0.478
0.502
0.441
0.449
0.713∗∗∗

0.338
0.498
0.449
0.437
0.404
0.429
0.644∗∗∗

P@5

0.042
0.332
0.200
0.181
0.281
0.278
0.381∗

P@10

0.055
0.277
0.172
0.159
0.244
0.239
0.299

P@5

P@10

0.043
0.108
0.109
0.100
0.084
0.108
0.134∗∗∗

0.036
0.079
0.081
0.075
0.065
0.081
0.092∗∗∗

ics where exact matches (Model 2) perform best, followed by ex-
amples which beneﬁt from semantic matching (log-linear model).
Topic identiﬁers are between parentheses.

W3C Topics P3P speciﬁcation and CSS3 (EX8 and EX69, respec-
tively) should return candidates associated with the deﬁnition
of these standards. The log-linear model, however, considers
these close to related technologies such as CSS2 for CSS3
and UTF-8 for P3P. Semantic matching works for topics Se-
mantic Web Coordination and Annotea server protocol (EX1
and EX103), where the former is associated with RDF li-
braries, RDF-related jargon and the names of researchers in
the ﬁeld, while the latter is associated with implementations
of the protocol and the maintainer of the project.

CERC For CSIRO, topic nanohouse (CE-035) is mentioned in
many irrelevant contexts (i.e., spam) and therefore seman-
tic matching fails. The term ﬁsh oil (CE-126) is quickly as-
sociated with different kinds of ﬁsh, oils and organizations
related to marines and ﬁsheries. On the other hand, we ob-
serve sensor networks (CE-018) to be associated with sen-
sor/networking jargon and sensor platforms. Topic forensic
science workshop (CE-103) expands to syntactically-similar
terms (e.g., plural), the names of science laboratories and ref-
erences

support/law-protection

organizations.

to

TU The TU benchmark contains both English and Dutch textual
evidence. Topics sustainable tourism and interpolation (1411
and 4882) do not beneﬁt from semantic matching due to a
semantic gap: interpolation is associated with the polyno-
mial kind while the relevance assessments focus on stochas-
tic methods. Interestingly, for the topic law and informati-
zation/computerization (1719) we see that the Dutch trans-
lation of law is very closely related. Similar terms to in-
formatization are, according to the log-linear model, Dutch
words related to cryptography. Similar dynamics are at work
for legal-political space (12603), where translated terms and
semantic-syntactic relations aid performance.

In order to further quantify the effect of the embedding matrix Wp,
we artiﬁcially expand benchmark topic terms by k nearby terms.
We then examine how the performance of a proﬁle-centric gener-
ative language model [4, Model 1] evolves for different values of
k (Figure 4). The purpose of this analysis is to provide further
insight in the differences between maximum-likelihood language
models and the log-linear model. Figure 4 shows that, for most
benchmarks, MAP increases as k goes up. Interestingly enough,
the two benchmarks that exhibit a decrease in MAP for larger k
(CERC 2007 and TU GT1) are likewise those for which generative
language models outperform the log-linear model in Table 2. This
suggests that the CERC 2007 and TU GT1 benchmarks require ex-

(a) W3C

(b) CERC

(c) TU

(d) W3C

(e) CERC

(f) TU

Figure 1: Sensitivity analysis for window size (n-gram) during parameter estimation (3) for W3C, CERC and TU benchmarks.

act term matching, while the remaining four benchmarks beneﬁt
greatly from the semantic matching provided by our model.

The per-topic differences suggest that Model 2 and the log-linear
model make very different errors: Model 2 excels at retrieving ex-
act query matches, while the log-linear model performs seman-
tic matching. Based on these observations we hypothesize that a
combination of the two approaches will raise retrieval performance
even further. To test this hypothesis, we propose a simple ensem-
ble of rankings generated by Model 2 and the log-linear model by
re-ranking candidates according to the multiplicatively-combined
reciprocal rank:

rankensemble(cj, qi) ∝

1
rankmodel 2(cj, qi)

·

1
ranklog-linear(cj, qi)

, (6)

where rankM (cj, qi) denotes the position of candidate cj in a rank-
ing generated by model M for answering query qi. Equation (6) is
equivalent to performing data fusion using CombSUM [57] where
the scores are given by the logarithm of the reciprocal ranks of the
experts. Table 3 compares the result of this ensemble to that of its
constituents. Compared to the supervised methods of Fang et al.
[23], we conclude that our fully unsupervised ensemble matches
the performance of their method on the CERC 2007 benchmark
and outperforms their method on the W3C 2005 benchmark. The
superior performance of the ensemble suggests the viability of hy-
brid methods that combine semantic and exact matching.

5.3 Scalability and efﬁciency

Inference in the log-linear model is expressed in linear algebra
operations (Section 3). These operations can be efﬁciently per-
formed by highly optimized software libraries and special-purpose
hardware (i.e., GPUs). But the baseline methods against which we
compare do not beneﬁt from these speed-ups. Furthermore, many
implementation-speciﬁc details and choice of parameter values can

inﬂuence runtime considerably (e.g. size of the latent representa-
tions). Therefore, we opt for a theoretical comparison of the infer-
ence complexity of the log-linear model and compare these to the
baselines (Section 4.3).

The log-linear model generates a ranking of candidate experts by
straight-forward matrix operations. The look-up operation in the
projection matrix Wp occurs in constant time complexity, as the
multiplication with the one-hot vector vi comes down to selecting
the i-th column from Wp. Multiplication of the |C| × e matrix
Wc with the e-dimensional word feature vector exhibits O(|C| · e)
runtime complexity. If we consider addition of the bias term and
division by the normalizing function Z1, the time complexity of (1)
becomes

O(|C| · (e + (e − 1))
(cid:125)
(cid:123)(cid:122)
(cid:124)
matrix-vector multiplication

+ |C|
(cid:124)(cid:123)(cid:122)(cid:125)
bias term

+ 2 · |C| − 1
(cid:123)(cid:122)
(cid:125)
Z1

(cid:124)

).

Notice, however, that the above analysis considers sequential exe-
cution. Modern computing hardware has the ability to parallelize
common matrix operations [24, 33]. The number of candidate ex-
perts |C| is the term that impacts performance most in the log-linear
model (under the assumption that |C| (cid:29) e).

If we consider n terms, where n is the query length during infer-
ence or the window size during training, then the complexity of (2)
becomes

O(n · |C| · (2 · e − 1) + n · (3 · |C| − 1)
(cid:125)

(cid:124)

(cid:123)(cid:122)
n forward-passes

+ (n − 1) · |C|
(cid:125)

(cid:123)(cid:122)
factor product

(cid:124)

)
+ 2 · |C| − 1
(cid:123)(cid:122)
(cid:125)
Z2

(cid:124)

Notice that Z2 does not need to be computed during inference as it
does not affect the candidate expert ranking.

(a) W3C (R = −0.39∗∗∗)

(b) CERC (R = −0.44∗∗∗)

(c) TU (R = −0.30∗∗∗)

Figure 2: Scatter plot of the normalized entropy of distribution P (c | q) (2) returned by the log-linear model and per-query av-
erage precision for W3C, CERC and TU benchmarks. Pearson’s R and associated p-value (two-tailed paired permutation test:
∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) are between parentheses. The depicted linear ﬁt was obtained using an ordinary least squares
regression.

Table 3: Comparison of Model 2, the log-linear model and an ensemble of the former on W3C, CERC and TU benchmarks. Signiﬁ-
cance of results is determined using a two-tailed paired randomization test [59] (∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) with respect to
the ensemble ranking (adjusted using the Benjamini-Hochberg procedure for multiple testing [9]).

2005
NDCG@100 MRR

0.380
0.444
0.479∗∗

0.451
0.618
0.668

2007
NDCG@100 MRR

0.500
0.493
0.589∗∗∗

0.467
0.513
0.627∗∗∗

GT1
NDCG@100 MRR

W3C

Model 2 (jm)
Log-linear (ours)
Ensemble

CERC

Model 2 (jm)
Log-linear (ours)
Ensemble

TU

Model 2 (jm)
Log-linear (ours)
Ensemble

MAP

0.211
0.248
0.291∗∗∗

MAP

0.361
0.344
0.452∗∗

MAP

0.234
0.219
0.271∗∗∗

P@5

0.332
0.412
0.440

P@5

0.192
0.215
0.248∗

P@10

0.296
0.361
0.378

P@10

0.138
0.150
0.160

MAP

0.260
0.484∗∗∗
0.433

MAP

0.274
0.342
0.395∗∗∗

2006
NDCG@100 MRR

0.423
0.667∗∗
0.634

0.599
0.833
0.825

2008
NDCG@100 MRR

0.463
0.519
0.593∗∗∗

0.517
0.656
0.716

GT5
NDCG@100 MRR

P@5

P@10

0.449
0.713∗∗
0.657

0.429
0.644∗∗
0.586

P@5

P@10

0.278
0.381
0.459∗∗

0.239
0.299
0.357∗∗∗

P@5

P@10

MAP

P@5

P@10

0.370
0.356
0.417∗∗∗

0.342
0.351
0.403∗∗∗

0.136
0.145
0.165∗∗∗

0.101
0.105
0.121∗∗∗

0.253
0.287
0.331∗∗∗

0.394
0.425
0.477∗∗∗

0.302
0.363
0.402∗∗∗

0.108
0.134
0.156∗∗∗

0.081
0.092
0.105∗∗∗

In terms of space complexity, parameters Wp, Wc and bc, in ad-
dition to the intermediate results, all require memory space propor-
tional to their size. Considering (2) for a sequence of k words and
batches of m instances, we require O(m·k·|C|) ﬂoating point num-
bers for every forward-pass to ﬁt in-memory. While such an upper
bound seems reasonable by modern computing standards, it is a
severely limiting factor when considering large-scale communities
and while utilizing limited-memory GPUs for fast computation.

The inferential complexity of the vector space-based models for
entity retrieval [19] depends mainly on the dimensionality of the
vectors and the number of candidate experts. The dimensionality
of the latent entity representations is too high for efﬁcient nearest
neighbor retrieval [29] due to the curse of dimensionality. There-
fore, the time complexity for the LSI- and TF-IDF-based vector
space models are respectively O(γ · |C|) and O(|V | · |C|), where
γ denotes the number of latent topics in the LSI-based method. As
hyperparameters e and γ both indicate the dimensionality of la-
tent entity representations, the time complexity of the LSI-based
method is comparable to that of the log-linear model. We note that
|V | (cid:29) |C| for all benchmarks (|V | is between 18 to 91 times larger
than |C|) we consider in this paper and therefore conclude that the
TF-IDF method loses to the log-linear model in terms of efﬁciency.

Compared to the unsupervised generative models of Balog et al.,
we have the proﬁle-centric Model 1 and the document-centric Mo-
del 2 with inference time complexity O(n · |C|) and O(n · |D|),
respectively, with |D| (cid:29) |C|. In the previous section we showed
that the log-linear model always performs better than Model 1 and
nearly always outperforms Model 2. Hence, our log-linear model
generally achieves the expertise retrieval performance of Model 2
(or higher) at the complexity cost of Model 1 during inference.

5.4

Incremental indexing

Existing unsupervised methods use well-understood maximum-
likelihood language models that support incremental indexing. We
now brieﬂy discuss the incremental indexing capabilities of our
proposed method. Extending the set of candidate experts C re-
quires the log-linear model to be re-trained from scratch as it changes
the topology of the network. Moreover, every document associated
with a candidate expert is considered as a negative example for all
other candidates. While it is possible to reiterate over all past doc-
uments and only learn an additional row in matrix Wc, the ﬁnal
outcome is unpredictable.

If we consider a stream of documents instead of a predeﬁned set
D, the log-linear model can be learned in an online fashion. How-
ever, stochastic gradient descent requires that training examples are

(a) W3C 2005

(b) CERC 2007

(c) TU GT1

(d) W3C 2006

(e) CERC 2008

(f) TU GT5

Figure 3: Difference of average precision between log-linear model and Model 2 [4] with Jelinek-Mercer smoothing per topic for
W3C, CERC and TU benchmarks.

picked at random such that the batched update rule (4) behaves like
the empirical expectation over the full training set [11]. While we
might be able to justify the assumption that documents arrive ran-
domly, the n-grams extracted from those documents clearly violate
this requirement.

Considering a stream of documents leads to the model forgetting
expertise evidence as an (artiﬁcial) shift in the underlying distribu-
tion of the training data occurs. While such behavior is undesirable
for the task considered in this paper, it might be well-suited for
temporal expert ﬁnding [22, 52], where expertise drift over time
is considered. However, temporal expertise ﬁnding is beyond the
scope for this paper and left for future work.

6. CONCLUSIONS

We have introduced an unsupervised discriminative, log-linear
model for the expert retrieval task. Our approach exclusively em-
ploys raw textual evidence. Future work can focus on improving
performance by feature engineering and incorporation of external
evidence. Furthermore, no relevance feedback is required during
training. This renders the model suitable for a broad range of ap-
plications and domains.

We evaluated our model on the W3C, CERC and TU bench-
marks and compared it to state-of-the-art vector space-based en-
tity ranking (based on LSI and TF-IDF) and language modeling
(proﬁle-centric and document-centric) approaches. The log-linear
model combines the ranking performance of the best maximum-
likelihood language modeling approach (document-centric) with
inference time complexity linear in the number of candidate ex-
perts. We observed a notable increase in precision over existing
methods. Analysis of our model’s output reveals a negative corre-
lation between the per-query performance and ranking uncertainty:
higher conﬁdence (i.e., lower entropy) in the rankings produced by
our approach often occurs together with higher rank quality.

An error analysis of the log-linear model and traditional lan-
guage models shows that the two make very different errors. These
errors are mainly due to the semantic gap between query intent and
the raw textual evidence. Some benchmarks expect exact query
matches, others are helped by our semantic matching. An ensemble
of methods employing exact and semantic matching generally out-
performs the individual methods. This observation calls for further
research in the area of combining exact and semantic matching.

One current limitation of our work is its scalability with respect
to the number of candidate experts. We have started investigating
trade-offs between model performance and time/space complexity.
In the future we hope to apply scalable variants of this method on
large-scale social media communities, for the purpose of determin-
ing topic ownership. While in this work we focus on expertise re-
trieval, the ideas we proposed can easily be transferred to the more
general entity retrieval task. Moreover, our approach is likely to
be applicable to authorship attribution and various other entity re-
trieval and prediction tasks.

Acknowledgments. We thank Isaac Sijaranamual, Manos Tsagkias, Tom
Kenter, Zhaochun Ren, Ke Tran and Katja Hoffman for their useful com-
ments and insights.

This research was supported by Amsterdam Data Science, the Dutch
national program COMMIT, Elsevier, the European Community’s Seventh
Framework Programme (FP7/2007-2013) under grant agreement nr 312827
(VOX-Pol), the ESF Research Network Program ELIAS, the Royal Dutch
Academy of Sciences (KNAW) under the Elite Network Shifts project, the
Microsoft Research Ph.D. program, the Netherlands eScience Center un-
der project number 027.012.105, the Netherlands Institute for Sound and
Vision, the Netherlands Organisation for Scientiﬁc Research (NWO) under
project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.-
930, CI-14-25, SH-322-15, 652.002.001, 612.001.551, the Yahoo Faculty
Research and Engagement Program, and Yandex. All content represents the
opinion of the authors, which is not necessarily shared or endorsed by their
respective employers and/or sponsors.

Computing resources were provided by the Netherlands Organisation for
Scientiﬁc Research (NWO) through allocation SH-322-15 of the Cartesius
system and by the Advanced School for Computing and Imaging (ASCII)
by allocation of the Distributed ASCII Supercomputer 4 (DAS-4) system.

(a) W3C

(b) CERC

(c) TU

Figure 4: Effect of query expansion by adding nearby terms in Wp (1) in traditional language models (Model 1 [4] with Jelinek-
Mercer smoothing) for W3C, CERC and TU benchmarks.

APPENDIX
The derivative of (3) w.r.t. bias term bc equals

∂L(Wp, Wc, bc)
∂bc

= −

1
m

(cid:32) m
(cid:88)

i=1

|dmax|
|d(i)|

|C|
(cid:88)

j=1

P (cj|d(i))

(cid:16)

∂ log

P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

(cid:33)

and w.r.t. an arbitrary matrix parameter θ (Wp or Wc):

∂L(Wp, Wc, bc)
∂θ

= −

(cid:32) m
(cid:88)

i=1

|dmax|
|d(i)|

|C|
(cid:88)

j=1

P (cj|d(i))

(cid:16)

∂ log

P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

(cid:33)

1
m

λ
m

+

θi,j.

(cid:88)

i,j

Further differentiation for parameter θ (Wp, Wc or bc):

∂bc

∂θ

∂ log (P (cj | w1, . . . , wn))
∂θ

∂P (cj | w1, . . . , wn)
∂θ

1
P (cj | w1, . . . , wn)
∂ ˜P (cj |w1,...,wn)
∂θ

∂P (cj | w1, . . . , wn)
∂θ
Z2 − ˜P (cj | w1, . . . , wn) ∂Z2
∂θ

∂Z2
∂θ

Z 2
2
∂ ˜P (ck | w1, . . . , wn)
∂θ

∂ ˜P (cj | w1, . . . , wn)
∂θ

∂P (cj | wk)
∂θ

(cid:89)

i(cid:54)=k

P (cj | wi)

=

=

=

=

(cid:88)

(cid:88)

k

k

For a given candidate cj and word wi, following (1) we have

P (cj | wi) =

˜P (cj | wi)
Z1
exp (cid:0)(cid:0)(cid:80)e
l=1 exp (cid:0)(cid:0)(cid:80)e

(cid:80)|C|

=

k=1 Wcj,k Wpk,i

k=1 Wcl,k Wpk,i

(cid:1) + bcj

(cid:1)
(cid:1) + bcl

(cid:1)

and consequently, with W (cid:62)
Wp,

pi denoting the i-th column of matrix

∂P (cj | wi)
∂Wcj

∂P (cj | wi)
∂bcj

∂P (cj | wi)
∂W (cid:62)
pi

=

=

=

(cid:17) ˜P (cj | wi)W (cid:62)
(cid:16)
Z1 − ˜P (cj | wi)
Z 2
1
(cid:17) ˜P (cj | wi)
(cid:16)
Z1 − ˜P (cj | wi)
Z 2
1

pi

(cid:16)
Wcj − (cid:80)|C|

(cid:17) ˜P (cj | wi)

l=1 Wcl
Z1

(7)

As can be seen in (7), the distributed representations of candidates
cj at time t + 1 are updated using the representation of words wi at
time t and vice versa.

REFERENCES
[1] The knowledge-based economy. Techn. report, Organisation for

Economic Co-operation and Development, 1996.

[2] P. Bailey, A. P. De Vries, N. Craswell, and I. Soboroff. Overview

of the TREC 2007 enterprise track. In TREC, 2007.
[3] K. Balog. People Search in the Enterprise. PhD thesis,

University of Amsterdam, 2008.

[4] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for
expert ﬁnding in enterprise corpora. In SIGIR, pages 43–50,
2006.

[5] K. Balog, L. Azzopardi, and M. de Rijke. A language modeling

framework for expert ﬁnding. IPM, 45:1–19, 2009.
[6] K. Balog, Y. Fang, M. de Rijke, P. Serdyukov, and L. Si.

Expertise retrieval. Found. & Tr. in Information Retrieval, 6
(2-3):127–256, 2012.

[7] I. Becerra-Fernandez. Role of artiﬁcial intelligence technologies
in the implementation of People-Finder knowledge management
systems. Knowledge-Based Systems, 13(5):315–320, 2000.
[8] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural
probabilistic language model. JMLR, 3:1137–1155, 2003.
[9] Y. Benjamini and Y. Hochberg. Controlling the false discovery
rate: a practical and powerful approach to multiple testing.
JSTOR, pages 289–300, 1995.

[10] R. Berendsen, M. de Rijke, K. Balog, T. Bogers, and A. van den
Bosch. On the assessment of expertise proﬁles. JASIST, 64(10):
2024–2044, 2013.

[11] L. Bottou. Large-scale machine learning with stochastic gradient
descent. In COMPSTAT, pages 177–186. Springer, 2010.
[12] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,

N. Hamilton, and G. Hullender. Learning to rank using gradient
descent. In ICML, pages 89–96, 2005.

[13] Y. Cao, J. Liu, S. Bao, and H. Li. Research on Expert Search at

Enterprise Track of TREC 2005. In TREC, pages 2–5, 2005.
[14] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,
and P. Kuksa. Natural language processing (almost) from
scratch. JMLR, 12(Aug):2493–2537, 2011.

[15] N. Craswell, D. Hawking, A.-M. Vercoustre, and P. Wilkins.

P@noptic expert: Searching for experts not just for documents.
In Ausweb Poster Proceedings, pages 21–25, 2001.

[16] N. Craswell, A. P. de Vries, and I. Soboroff. Overview of the

TREC 2005 enterprise track. In TREC, 2005.

[17] T. H. Davenport and L. Prusak. Working Knowledge. Harvard

Business Review Press, 1998.

[18] S. C. Deerwester, S. T. Dumais, and R. A. Harshman. Indexing

by latent semantic analysis. JASIS, 1990.

[19] G. Demartini, J. Gaugaz, and W. Nejdl. A vector space model

[41] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Distributed

for ranking entities and its application to expert search. In ECIR,
pages 189–201. Springer, 2009.

representations of words and phrases and their compositionality.
In NIPS, pages 3111–3119, 2013.

[20] L. Deng, X. He, and J. Gao. Deep stacking networks for

information retrieval. In ICASSP, pages 3153–3157, 2013.
[21] H. Fang and C. Zhai. Probabilistic models for expert ﬁnding. In

ECIR, pages 418–430, Berlin, Heidelberg, 2007.
Springer-Verlag.

[22] Y. Fang and A. Godavarthy. Modeling the dynamics of personal

expertise. In SIGIR, pages 1107–1110, 2014.

[23] Y. Fang, L. Si, and A. P. Mathur. Discriminative models of

integrating document evidence and document-candidate
associations for expert search. In SIGIR, pages 683–690, 2010.

[24] K. Fatahalian, J. Sugerman, and P. Hanrahan. Understanding the
efﬁciency of gpu algorithms for matrix-matrix multiplication. In
SIGGRAPH HWWS, pages 133–137. ACM, 2004.

[25] X. Glorot and Y. Bengio. Understanding the difﬁculty of training
deep feedforward neural networks. In AISTATS, pages 249–256,
2010.

[26] G. E. Hinton. Learning distributed representations of concepts.
In 8th Annual Conference of the Cognitive Science Society,
volume 1, page 12, Amherst, MA, 1986.

[27] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR,

pages 50–57. ACM, 1999.

[28] P.-s. Huang, N. M. A. Urbana, X. He, J. Gao, L. Deng, A. Acero,
and L. Heck. Learning deep structured semantic models for web
search using clickthrough data. In CIKM, pages 2333–2338,
2013.

[29] P. Indyk and R. Motwani. Approximate nearest neighbors:

towards removing the curse of dimensionality. In STOC, pages
604–613. ACM, 1998.

[30] M. Karimzadehgan and C. Zhai. Estimation of statistical

translation models based on mutual information for ad hoc
information retrieval. In SIGIR, pages 323–330. ACM, 2010.
[31] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural

language models. In ICML, pages 595–603, 2014.

[32] J. Kruger and D. Dunning. Unskilled and unaware of it: how

difﬁculties in recognizing one’s own incompetence lead to
inﬂated self-assessments. J. Personality and Social Psych., 77
(6):1121, 1999.

[33] J. Krüger and R. Westermann. Linear algebra operators for gpu
implementation of numerical algorithms. ACM Transactions on
Graphics, 22(3):908–916, 2003.

[34] H. Li and J. Xu. Semantic matching in search. Found. & Tr. in

Information Retrieval, 7(5):343–469, June 2014.

[35] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer,

2011.

[36] C. MacDonald and I. Ounis. Voting for candidates: adapting

data fusion techniques for an expert search task. In CIKM, pages
387–396, 2006.

[37] C. Macdonald and I. Ounis. Expert search evaluation by

supporting documents. In ECIR, pages 555–563. Springer, 2008.

[38] M. T. Maybury. Expert ﬁnding systems. Techn. Report

[39] D. W. McDonald and M. S. Ackerman. Expertise recommender.

MTR-06B000040, MITRE, 2006.

In CSCW, pages 231–240, 2000.

[42] T. Mikolov, G. Corrado, K. Chen, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
1301.3781, 2013.

[43] A. Mnih and G. Hinton. Three new graphical models for

statistical language modelling. In ICML, pages 641–648, 2007.

[44] A. Mnih and G. Hinton. A scalable hierarchical distributed
language model. In NIPS, pages 1081–1088, 2008.
[45] A. Mnih and K. Kavukcuoglu. Learning word embeddings

efﬁciently with noise-contrastive estimation. In NIPS, pages
2265–2273, 2013.

[46] G. Montavon, G. B. Orr, and K.-R. Müller. Neural Networks:

Tricks of the Trade. Springer, 2012.

[47] C. Moreira, B. Martins, and P. Calado. Using rank aggregation
for expert search in academic digital libraries. In Simpósio de
Informática, INForum, pages 1–10, 2011.

[48] J. Pennington, R. Socher, and C. D. Manning. GloVe: Global

Vectors for Word Representation. In EMNLP, pages 1532–1543,
2014.

[49] D. Petkova and W. B. Croft. Hierarchical language models for
expert ﬁnding in enterprise corpora. In ICTAI ’06, pages
599–606, 2006.

[50] W. W. Powell and K. Snellman. The knowledge economy.
Annual review of sociology, pages 199–220, 2004.

[51] D. Rumelhart, G. Hinton, and R. Williams. Learning internal
representations by back propagation. In Parallel Distributed
Processing, pages 318–362. MIT Press, 1986.

[52] J. Rybak, K. Balog, and K. Nørvåg. Temporal expertise
proﬁling. In ECIR, pages 540–546. Springer, 2014.
[53] R. Salakhutdinov and G. Hinton. Semantic hashing. Int. J.

Approximate Reasoning, 50(7):969–978, 2009.

[54] P. Serdyukov and D. Hiemstra. Modeling documents as mixtures
of persons for expert ﬁnding. In ECIR, pages 309–320. Springer,
2008.

[55] P. Serdyukov, H. Rode, and D. Hiemstra. Modeling multi-step
relevance propagation for expert ﬁnding. In CIKM, pages
1133–1142, 2008.

[56] C. Shannon. A mathematical theory of communication. Bell

System Technical J., 27:379–423, 623–656, 1948.

[57] J. A. Shaw, E. A. Fox, J. A. Shaw, and E. A. Fox. Combination

of multiple searches. In TREC, pages 243–252, 1994.
[58] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. A latent
semantic model with convolutional-pooling structure for
information retrieval. In CIKM, pages 101–110, 2014.
[59] M. D. Smucker, J. Allan, and B. Carterette. A comparison of

statistical signiﬁcance tests for information retrieval evaluation.
In CIKM, pages 623–632. ACM, 2007.

[60] P. Sorg and P. Cimiano. Finding the right expert: Discriminative
models for expert retrieval. In KDIR, pages 190–199, 2011.

[61] TREC. Enterprise Track, 2005–2008.
[62] D. van Dijk, M. Tsagkias, and M. de Rijke. Early detection of
topical expertise in community question and answering. In
SIGIR, 2015.

[63] V. Vapnik. Statistical learning theory, volume 1. Wiley New

[40] T. Mikolov, M. Karaﬁat, L. Burget, J. Cernocky, and

York, 1998.

S. Khudanpur. Recurrent neural network based language model.
In Interspeech, pages 1045–1048, 2010.

[64] M. D. Zeiler. Adadelta: An adaptive learning rate method.

CoRR, abs/1212.5701, 2012.

Unsupervised, Efﬁcient and Semantic Expertise Retrieval

Christophe Van Gysel
cvangysel@uva.nl

Maarten de Rijke
derijke@uva.nl

Marcel Worring
m.worring@uva.nl

University of Amsterdam, Amsterdam, The Netherlands

7
1
0
2
 
p
e
S
 
7
1
 
 
]

R

I
.
s
c
[
 
 
2
v
1
5
6
6
0
.
8
0
6
1
:
v
i
X
r
a

ABSTRACT
We introduce an unsupervised discriminative model for the task
of retrieving experts in online document collections. We exclu-
sively employ textual evidence and avoid explicit feature engineer-
ing by learning distributed word representations in an unsupervised
way. We compare our model to state-of-the-art unsupervised sta-
tistical vector space and probabilistic generative approaches. Our
proposed log-linear model achieves the retrieval performance lev-
els of state-of-the-art document-centric methods with the low infer-
ence cost of so-called proﬁle-centric approaches. It yields a statis-
tically signiﬁcant improved ranking over vector space and genera-
tive models in most cases, matching the performance of supervised
methods on various benchmarks. That is, by using solely text we
can do as well as methods that work with external evidence and/or
relevance feedback. A contrastive analysis of rankings produced
by discriminative and generative approaches shows that they have
complementary strengths due to the ability of the unsupervised dis-
criminative model to perform semantic matching.

1.

INTRODUCTION

The transition to the knowledge and information economy [1]
introduces a great reliance on cognitive capabilities [50]. It is cru-
cial for employers to facilitate information exchange and to stim-
ulate collaboration [17].
In the past, organizations would set-up
special-purpose database systems for their members to maintain
a proﬁle [7]. However, these systems required employees to be
proactive. In addition, self-assessments are known to diverge from
reality [10, 32] and document collections can quickly become prac-
tically infeasible to manage manually. Therefore, there has been an
active interest in automated approaches for constructing expertise
proﬁles [7, 61] and retrieving experts from an organization’s het-
erogeneous document repository [15]. Expert ﬁnding (also known
as expertise retrieval or expert search) addresses the task of ﬁnding
the right person with the appropriate skills and knowledge [6]. It
attempts to provide an answer to the question:

Given a topic X, who are the candidates with the most
expertise w.r.t. X?

This is an electronic version of an Article published in the 25th International
World Wide Web Conference.
c(cid:13) 2016 International World Wide Web Conference Committee.
WWW 2016, April 11–15, 2016, Montréal, Québec, Canada.
ACM 978-1-4503-4143-1/16/04.
http://dx.doi.org/10.1145/2872427.2882974.

The expertise retrieval task gained popularity in the research com-
munity during the TREC Enterprise Track [61] and has remained
relevant ever since, while broadening to social media and to track-
ing the dynamics of expertise [5, 6, 10, 19, 21–23, 47, 49, 62].
Existing methods fail to address key challenges: (1) Queries and
expert documents use different representations to describe the same
concepts [26, 34]. Term mismatches between queries and experts
[34] occur due to the inability of widely used maximum-likelihood
language models to make use of semantic similarities between words
[53]. (2) As the amount of available data increases, the need for
more powerful approaches with greater learning capabilities than
smoothed maximum-likelihood language models is obvious [63].
(3) Supervised methods for expertise retrieval [23, 47] were intro-
duced at the turn of the last decade. However, the acceleration
of data availability has the major disadvantage that, in the case of
supervised methods, manual annotation efforts need to sustain a
similar order of growth. This calls for the further development of
unsupervised methods. (4) In some expertise retrieval methods, a
language model is constructed for every document in the collection.
These methods lack efﬁcient query capabilities for large document
collections, as each query term needs to be matched against ev-
ery document [6]. Our proposed solution has a strong emphasis on
unsupervised model construction, efﬁcient query capabilities and
semantic matching between query terms and candidate experts.

Speciﬁcally, we propose an unsupervised log-linear model with
efﬁcient inference capabilities for the expertise retrieval task. We
show that our approach improves retrieval performance compared
to vector space-based and generative language models, mainly due
to its ability to perform semantic matching [34]. Our method does
not require supervised relevance judgments and is able to learn
from raw textual evidence and document-candidate associations
alone. The purpose of this work is to provide insight in how dis-
criminative language models can improve performance of core re-
trieval tasks compared to maximum-likelihood language models.
Therefore, we avoid explicit feature engineering and the incorpo-
ration of external evidence in this paper. In terms of performance,
the current best-performing formal language model [4] exhibits a
worst-case time complexity linear in the size of the document col-
lection. In contrast, the inference time complexity of our approach
is asymptotically bounded by the number of candidate experts.

Our research questions are as follows: (1) How does our dis-
criminative log-linear model compare to vector space-based meth-
ods and generative language models for the expert retrieval task in
terms of retrieval performance? (2) What can we learn regarding
the different types of errors made by generative and discriminative
language models? (3) How does the complexity of inference in
our log-linear model compare to vector-space based and generative

models? (4) How does the log-linear model handle incremental
indexing and what are its limitations?

We contribute: (i) An unsupervised log-linear model with efﬁ-
cient inference capabilities for the expertise retrieval task, together
with an open-source implementation.1 (ii) Comparison of the re-
trieval performance of the log-linear model with traditional vector
space-based models and language model methods on well-known
benchmarks. (iii) Insights in how certainty in predictions by the
log-linear model correlates with performance. (iv) In-depth analy-
sis of the inference complexity of the log-linear model. (v) Com-
parative error analysis between the semantic log-linear model and
traditional generative language models that perform exact match-
ing. (vi) Insight in the relative strengths of semantic matching and
exact matching for the expert retrieval task.

The remainder of this paper is organized as follows. In Section 2,
we brieﬂy discuss related work. Section 3 introduces the log-linear
model for the expert retrieval problem. In Section 4 we state our
research questions and detail our experimental set-up and imple-
mentations. We provide an overview of our experimental results,
followed by a discussion and analysis in Section 5. Section 6 con-
cludes the paper and discusses ideas for future work.

2. RELATED WORK

We ﬁrst discuss prior work on expert retrieval and its relation
to document retrieval. Then, we review semantic search methods.
The ideas we present in this paper are inspired by neural language
models used in automated speech recognition (ASR) and natural
language processing (NLP). Consequently, we also review work
from those ﬁelds.

2.1 Expert retrieval

Early expert retrieval systems were often referred to as expert
locator and expertise management systems [38]. These database
systems often relied on people to self-assess their expertise against
a predeﬁned set of topics [39], which is known to generate unreli-
able results [7].

With the introduction of the P@NOPTIC system [15], and later
the TREC Enterprise track [61], there has been an active research
interest in automated expertise proﬁling methods. It is useful to dis-
tinguish between proﬁle-based methods, which create a textual rep-
resentation of a candidate’s knowledge, and document-based meth-
ods, which represent candidates as a weighted combination of doc-
uments. The latter generally performs better at ranking, while the
former is more efﬁcient as it avoids retrieving all documents rele-
vant to a query [6, p. 221].

There has been much research on generative probabilistic mod-
els for expert retrieval [21, 49]. Such models have been categorized
in candidate generation models [13], topic generation models [4, 5]
and proximity-based variants [5, 54]. Of special relevance to us are
the unsupervised proﬁle-centric (Model 1) and document-centric
(Model 2) models of Balog et al. [4], which focus on raw textual ev-
idence without incorporating collection-speciﬁc information (e.g.,
query modeling, document importance or document structure). Su-
pervised discriminative models [23, 47, 60] are preferred when
query-candidate relevance pairs are available for training. Unlike
their generative counterparts these models have no issue combining
complex and heterogeneous features (e.g., link-based features, doc-
ument importance features, etc.); they resemble Learning to Rank
(L2R) methods for document retrieval [6, 35]. However, a lack
of training data may greatly hinder their applicability [6, p. 179].
Beyond unsupervised generative and supervised discriminative ap-

1https://github.com/cvangysel/SERT

proaches, there are graph-based approaches based on random walks
[55] and voting-based approaches based on data fusion [36]. De-
martini et al. [19] propose a vector space-based method for the en-
tity ranking task; their framework extends vector spaces operating
on documents to entities. See [6] for a survey on the topic.

2.2 Latent semantic models for document re-

trieval

Li and Xu [34] note that the query document mismatch poses the
most critical challenge in search. Semantic matching is an impor-
tant attempt to remedy this problem. There has been much work
on bridging the semantic gap for various different tasks [19, 26, 28,
30, 31, 41, 48, 53]. Expertise and document retrieval [6, p. 224] are
closely related as performance of the latter can greatly impact that
of the former [37].

Latent Semantic Models (LSM) ﬁrst became popular through
the introduction of Latent Semantic Indexing (LSI) [18], followed
by probabilistic LSI (pLSI) [27]. LSMs based on neural networks
[28, 53, 58] emerged in the last decade. Salakhutdinov and Hinton
[53] employ unsupervised deep auto-encoders to map documents to
bit patterns using semantic addressing. Huang et al. [28] perform
semantic matching of documents and queries by leveraging click-
through data optimizing for Web document ranking. Further, deep
models [12, 20] have been proposed to learn to rank [35].

2.3 Neural language modeling

Large-vocabulary neural probabilistic language models for mod-
eling word sequence distributions have become very popular re-
cently [8, 43, 44]. These models learn continuous-valued distributed
representations for words, also known as embeddings [42, 45, 48],
in order to ﬁght the curse of dimensionality and increase general-
ization by introducing the expectation that similar word vectors sig-
nify semantically or syntactically similar words. Recurrent neural
language models have shown to perform well in ASR [40]. Col-
lobert et al. [14] propose a uniﬁed neural network architecture for
various NLP tasks. Even more recently, there has been a surge in
multimodal neural language models [31], which lend themselves to
the task of automated image captioning.

What we add on top of the related work described above is the
following.
In this work we model the conditional probability of
the expertise of a candidate given a single query term (contrary
to binary relevance given a character-based n-gram [28]). In the
process we learn a distributed vector representation (similar to LSI,
pLSI and semantic hashing) for both words and candidates such
that nearby representations indicate semantically similar concepts.
We propose a log-linear model that is similar to neural language
models. The important difference is that we predict a candidate
expert instead of the next word. To the best of our knowledge,
we are the ﬁrst to propose such a solution. We employ an embed-
ding layer in our shallow model for the same reasons as mentioned
above: we learn continuous word representations that incorporate
semantic and syntactic similarity tailored to an expert’s domain.

3. A LOG-LINEAR MODEL FOR

of words w1, . . . , wk we have:

EXPERT SEARCH

In the setting of this paper we have a document collection D and
a predeﬁned set of candidate experts C (entities to be retrieved).
Documents d ∈ D are represented as a sequence of words w1, . . . ,
w|d| originating from a vocabulary V , where wi ∈ V and the oper-
ator |·| denotes the document length in tokens. For every document
d ∈ D we write Cd to denote the set of candidates c ∈ C asso-
ciated with it (i.e., C = (cid:83)
d∈D Cd). These document-candidate
associations can be obtained explicitly from document meta-data
(e.g., the author of an e-mail) or implicitly by mining references
to candidates from the document text. Notice that some documents
might not be associated with any candidate. When presented with a
query q of constituent terms t1, . . . , t|q|, the expert retrieval task is
to return a list of candidates ρ(C) ordered according to topical ex-
pertise. We generate this ranking using a relatively shallow neural
network which directly models P (c | q).

We employ vector-based distributed representations [26], for both
words (i.e., word embeddings) and candidate experts, in a way that
motivates the unsupervised construction of features that express
regularities of the expertise ﬁnding domain. These representations
can capture the similarity between concepts (e.g., words and can-
didate experts) by the closeness of their representations in vector
space. That is, concepts with similar feature activations are inter-
preted by the model as being similar, or even interchangeable.

3.1 The model

To address the expert search task, we model P (cj | q) and rank
candidates cj accordingly for a given q. We propose an unsuper-
vised, discriminative approach to obtain these probabilities. We
construct our model solely from textual evidence: we do not re-
quire query-candidate relevance assessments for training and do not
consider external evidence about the corpus (e.g., different weight-
ings for different sub-collections), the document (e.g., considering
certain parts of the document more useful) nor link-based features.
Let e denote the size of the vector-based distributed representa-
tions of both words in V and candidate experts in C. These repre-
sentations will be learned by the model using gradient descent [42]
(Section 3.2). For notational convenience, we write P (c | ·) for the
(conditional) probability distribution over candidates, which is the
result of vector arithmetic. We deﬁne the probability of a candidate
cj given a single word wi ∈ V as the log-linear model

P (c | wi) =

exp (Wc · (Wp · vi) + bc) ,

(1)

1
Z1

where Wp is the e × |V | projection matrix that maps the one-hot
representation (i.e., 1-of-|V |) of word wi, vi, to its e-dimensional
distributed representation, bc is a |C|-dimensional bias vector and
Wc is the |C| × e matrix that maps the word embedding to an un-
normalized distribution over candidates C, which is then normal-
ized by Z1 = (cid:80)|C|
j=1 [exp (Wc · (Wp · vi) + bc)]j. If we consider
Bayes’ theorem, the transformation matrix Wc and bias vector bc
can be interpreted as the term log-likelihood log P (wi | c) and
candidate log-prior log P (c), respectively. The projection matrix
Wp attempts to soften the curse of dimensionality introduced by
large vocabularies V and maps words to word feature vectors [8].
Support for large vocabularies is crucial for retrieval tasks [28, 53].
We then assume conditional independence of a candidate’s ex-
pertise given an observation of data (i.e., a word). Given a sequence

P (c | w1, . . . , wk) =

˜P (c | w1, . . . , wk) =

P (c | wi)

1
Z2

1
Z2

(cid:32) k

(cid:88)

1
Z2

k
(cid:89)

i=1
(cid:33)

=

exp

log (P (c | wi))

(2)

(cid:16)(cid:80)k

j=1 exp

i=1
where ˜P (c | w1, . . . , wk) denotes the unnormalized score and
(cid:17)
Z2 = (cid:80)|C|
i=1 log (P (cj | wi))
is a normalizing term.
The transformation to log-space in (2) is a well-known trick to pre-
vent ﬂoating point underﬂow [46, p. 445]. Given (2), inference is
straight-forward. That is, given query q = t1, . . . , tk, we compute
P (c | t1, . . . , tk) and rank the candidate experts in descending
order of probability.

Eq. 1 deﬁnes a neural network with a single hidden layer. We
can add additional layers. Preliminary experiments, however, show
that the shallow log-linear model (1) performs well-enough in most
cases. Only for larger data sets did we notice a marginal gain from
adding an additional layer between projection matrix Wp and the
softmax layer over C (Wc and bias bc), at the expense of longer
training times and loss of transparency.

3.2 Parameter estimation

The matrices Wp, Wc and the vector bc in (1) constitute the
parameters of our model. We estimate them using error back prop-
agation [51] as follows. For every document dj ∈ D we construct
an ideal distribution over candidates p = P (c | dj) based on the
document-candidate associations Cdj such that

P (c | dj) =

(cid:40) 1
|Cdj
0,

| ,

c ∈ Cdj

c (cid:54)∈ Cdj

We continue by extracting n-grams where n remains ﬁxed during
training. For every n-gram w1, . . . , wn generated from document
d we compute ˜p = P (c | w1, . . . , wn) using (2). During model
constructing we then optimize the cross-entropy H(p, ˜p) (i.e., the
joint probability of the training data if |Cdj | = 1 for all j) using
batched gradient descent. The loss function for a single batch of m
instances with associated targets (p(i), ˜p(i)) is as follows:

L(Wp, Wc, bc)

=

1
m

+

m
(cid:88)

i=1

λ
2m

|dmax|
|d(i)|
(cid:32)

(cid:88)

i,j

H(p(i), ˜p(i))

W 2

pi,j +

W 2

ci,j

(cid:88)

i,j

= −

|dmax|
|d(i)|

|C|
(cid:88)

j=1

1
m

m
(cid:88)

i=1
(cid:32)

+

λ
2m

(cid:88)

i,j

W 2

pi,j +

W 2

ci,j

,

(cid:88)

i,j

(cid:33)

(cid:33)

(3)

P (cj | d(i)) log

(cid:16)
P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

where d(i) refers to the document from which n-gram w(i)
1 , . . . , w(i)
n
was extracted, dmax = arg maxd∈D |d| indicates the longest doc-
ument in the collection, and λ is a weight regularization parameter.
The update rule for a particular parameter θ (Wp, Wc or bc) given
a single batch of size m is:

θ(t+1) = θ(t) − α(t) (cid:12)

∂L(Wp

(t), bc

(t))

(t), Wc
∂θ

,

(4)

where α(t) and θ(t) denote the per-parameter learning rate and pa-
rameter θ at time t, respectively. The learning rate α consists of
the same number of elements as there are parameters; in the case
of a global learning rate, all elements of α are equal to each other.
The derivatives of the loss function (3) are given in the Appendix.
In the next section we will discuss our experimental setup, fol-
lowed by an overview of our experimental results and further anal-
ysis in Section 5.

4. EXPERIMENTAL SETUP

4.1 Research questions

As indicated in the introduction, we seek to answer the following

research questions:
RQ1 How does our discriminative log-linear model compare to
vector space-based methods and generative language models
for the expert retrieval task in terms of retrieval performance?
In particular, how does the model perform when compared against
vector space-based (LSI and TF-IDF) and generative approaches
(proﬁle-centric Model 1 and document-centric Model 2)?
RQ2 What can we learn regarding the different types of errors

made by generative and discriminative language models?

Does the best-performing generative model simply perform slightly
better on the topics for which the other models perform decent as
well, or do they make very different errors? If the latter holds,
an ensemble of the rankings produced by both model types might
exceed performance of the individual rankings.
RQ3 How does the complexity of inference in our log-linear model
compare to vector-space based and generative models?
The worst-case inference cost of document-centric models makes
them unattractive in online settings where the set of topics is not
deﬁned beforehand and the document collection is large. Proﬁle-
centric methods are preferred in such settings as they infer from
one language model per candidate expert for every topic (i.e., a
pseudo-document consisting of a concatenation of all documents
associated with an expert) [6]. Vector space-based methods [19]
have similar problems due to the curse of dimensionality [29] and
consequently their inferential time complexity is likewise asymp-
totically bounded by the number of experts.
RQ4 How does the log-linear model handle incremental indexing

and what are its limitations?

4.2 Benchmarks

The proposed method is applicable in the setting of the Expert
Search task of the TREC Enterprise track from 2005 to 2008 [61].
We therefore evaluate on the W3C and CERC benchmarks released
by the track. The W3C dataset [16] is a crawl of the W3C’s sites
in June 2004 (mailing lists, web pages, etc.). The CSIRO Enter-
prise Research Collection (CERC) [2] is a dump of the intranet
of Australia’s national science agency. Additionally, we evaluate
our method on a smaller, more recent benchmark based on the em-
ployee database of Tilburg University (TU) [10], which consists of
bi-lingual, heterogeneous documents. See Table 1.

Mining document-candidate associations and how they inﬂuence
performance has been extensively covered in previous work [4, 6]
and is beyond the scope of this work. For TU, the associations
are part of the benchmark. For W3C, a list of possible candidates
is given and we extract the associations ourselves by performing
a case-insensitive match of full name or e-mail address [4]. For
CERC, we make use of publicly released associations [3].

As evaluation measures we use Mean Average Precision (MAP),
Mean Reciprocal Rank (MRR), Normalized Discounted Cumula-

tive Gain at rank 100 (NDCG@100) and Precision at rank 5 (P@5)
and rank 10 (P@10).

4.3 Baselines

We compare our approach to existing unsupervised methods for
expert retrieval that solely rely on textual evidence and static doc-
ument-candidate associations. (1) Demartini et al. [19] propose a
generic framework to adapt vector spaces operating on documents
to entities. We compare our method to TF-IDF (raw frequency and
inverse document frequency) and LSI (300 latent topics) variants
of their vector space model for entity ranking (using cosine sim-
ilarity). (2) In terms of language modeling, Balog et al. [4] pro-
pose two models for expert ﬁnding based on generative language
models. The ﬁrst takes a proﬁle-centric approach comparing the
language model of every expert to the query, while the second is
document-centric. We consider both models with different smooth-
ing conﬁgurations: Jelinek-Mercer (jm) smoothing with λ = 0.5
[4] and Dirichlet (d) smoothing with β equal to the average doc-
ument length [5] (see Table 1). Signiﬁcance of results produced
by the baselines (compared to our method) is determined using a
two-tailed paired randomization test [59].

4.4

Implementation details

The vocabulary V is constructed from each corpus by ignoring
punctuation, stop words and case; numbers are replaced by a nu-
merical placeholder token. During our experiments we prune V
by only retaining the 216 most-frequent words so that each word
can be encoded by a 16-bit unsigned integer. Incomplete n-gram
instances are padded by a special-purpose token.

In terms of parameter initialization, we sample the initial matri-

ces Wc and Wp (1) uniformly in the range
(cid:34)
(cid:114) 6.0

(cid:114) 6.0

(cid:35)

−

,

m + n

m + n

for an m × n matrix, as this initialization scheme improves model
training convergence [25], and take the bias vector bc to be null.
The projection layer Wp is initialized with pre-trained word repre-
sentations trained on Google News data [41]; the number of word
features is set to e = 300, similar to pre-trained representations.

We used adadelta (ρ = 0.95, (cid:15) = 10−6) [64] with batched gra-
dient descent (m = 1024) and weight decay λ = 0.01 during
training on NVidia GTX480 and NVidia Tesla K20 GPUs. We only
iterate once over the entire training set for each experiment.

5. RESULTS AND DISCUSSION

We start by giving a high-level overview of our experimental re-
sults and then address issues of scalability, provide an error analysis
and discuss the issue of incremental indexing.

5.1 Overview of experimental results

We evaluate the log-linear model on the W3C, CERC and TU
benchmarks (Section 4.2). During training we extract non-overlap-
ping n-grams for the W3C and CERC benchmarks and overlapping
n-grams for the TU benchmark. As the TU benchmark is consider-
ably smaller, we opted to use overlapping n-grams to counter data
sparsity. The architecture of each benchmark model (e.g., num-
ber of candidate experts) is inherently speciﬁed by the benchmarks
themselves (see Table 1). However, the choice of n-gram size dur-
ing training remains open. Errors for input w1, . . . , wn are propa-
gated back through Wc until the projection matrix Wp is reached;
if a single word wi causes a large prediction error, then this will in-
ﬂuence its neighboring words w1, . . . , wi−1, wi+1, . . . , wn as well.

Table 1: An overview of the three datasets (W3C, CERC and TU) used for evaluation and analysis.

Number of documents
Average document lengtha

Number of candidatesb

Number of document-candidate associations
Number of documents (with Cd > 0)
Number of associations per documentc
Number of associations per candidate

Queries

W3C

CERC

331,037

1,237.23

370,715

460.48

715

200,939
93,826

3,479

236,958
123,934

TU

31,209
2,454.93

977

36,566
27,834

2.14 ± 3.29
281.03 ± 666.63

1.91 ± 3.70
68.11 ± 1,120.74

1.13 ± 0.39
37.43 ± 61.00

49
50

(2005)
(2006)

50
77

(2007)
(2008)

1,662
1,266

(GT1)
(GT5)

a Measured in number of tokens.
b Only candidates with at least a single document association are considered.
c Only documents with at least one association are considered.

This allows the model to learn continuous word representations tai-
lored to the expert retrieval task and the benchmark domain.

A larger window size has a negative impact on batch throughput
during training. We are thus presented with the classic trade-off
between model performance and construction efﬁciency. Notice,
however, that the number of n-grams decreases as the window size
increases if we extract non-overlapping instances. Therefore, larger
values of n lead to faster wall-clock time model construction for the
W3C and CERC benchmarks in our experiments.

We sweep over the window width n = 2i (0 ≤ i < 6) for all
three benchmarks and their corresponding relevance assessments.
We report MAP and MRR for every conﬁguration (see Figure 1).
We observe a signiﬁcant performance increase between n = 1
and n = 2 on all benchmarks, which underlines the importance
of the window size parameter. The increase in MAP implies that
the performance achieved is not solely due to initialization with
pre-trained representations (Section 4.4), but that the model efﬁ-
ciently learns word representations tailored to the problem domain.
The highest MAP scores are attained for relatively low n. As n
increases beyond n = 8 a gradual decrease in MAP is observed on
all benchmarks. In our remaining experiments we choose n = 8
regardless of the benchmark.

Words wi that mainly occur in documents associated with a par-
ticular expert are learned to produce distributions P (c | wi) with
less uncertainty than words associated with many experts in (1).
The product of P (c | wi) in (2) aggregates this expertise evidence
generated by query terms. Hence, queries with strong evidence for
a particular expert should be more predictable than very generic
queries. To quantify uncertainty we measure the normalized en-
tropy [56] of P (c | q):

η(c | q) = −

p(cj | q) log(p(cj | q)).

(5)

1
log(|C|)

|C|
(cid:88)

j=1

Equation 5 can be interpreted as a similarity measure between the
given distribution and the uniform distribution. Importantly, Fig-
ure 2 shows that there is a statistically signiﬁcant negative correla-
tion between query-wise normalized entropy and average precision
for all benchmarks.

Table 2 presents a comparison between the log-linear model and
the various baselines (Section 4.3). Our unsupervised method sig-
niﬁcantly (p < 0.01) outperforms the LSI-based method consis-
tently. In the case of the TF-IDF method and the proﬁle-centric

generative language models (Model 1), we always perform bet-
ter and statistical signiﬁcance is achieved in the majority of cases.
The document-centric language models (Model 2) perform slightly
better than our method on two (out of six) benchmarks in terms
of MAP and NDCG@100:(1) For the CERC 2007 assessment we
match performance of the document-centric generative model with
Jelinek-Mercer smoothing. (2) For TU GT1 the generative coun-
terpart seems to outperform our method.

Notice that over all assessments, the log-linear model consis-
tently outperforms all proﬁle-centric approaches and is only chal-
lenged by the smoothed document-centric approach. In addition,
for the precision-based measures (P@k and MRR), the log-linear
model consistently outperforms all other methods we compare to.
Next, we turn to a topic-wise comparative analysis of discrimi-
native and generative models. After that, we analyze the scalability
and efﬁciency of the log-linear model and compare it to that of the
generative counterparts, and address incremental indexing.

5.2 Error analysis

How does our log-linear model achieve its superior performance
over established generative models? Figure 3 depicts the per-topic
differences in average precision between the log-linear model and
Model 2 (with Jelinek-Mercer smoothing) on all benchmarks. For
each plot, the vertical bars with a positive AP difference correspond
to test topics for which the log-linear model outperforms Model 2
and vice versa for bars with a negative AP difference.

The beneﬁt gained from the projection matrix Wp is two-fold.
First, it avoids the curse of dimensionality introduced by large vo-
cabularies. Second, term similarity with respect to the expertise
domain is encoded in latent word features. When examining words
nearby query terms in the embedding space, we found words to be
related to the query term. For example, word vector representations
of xml and nonterminal are very similar for the W3C benchmark (l2
norm). This can be further observed in Figure 1: log-linear models
trained on single words perform signiﬁcantly worse compared to
those that are able to learn from neighboring words.

We now take a closer look at the topics for which the log-linear
model outperforms Model 2 and vice versa. More speciﬁcally, we
investigate textual evidence related to a topic and whether it is con-
sidered relevant by the benchmark. For the log-linear model, we ex-
amine terms nearby topic terms in Wp (l2-norm), as these terms are
considered semantically similar by the model and provide a means
for semantic matching. For every benchmark, we ﬁrst consider top-

Table 2: Evaluation results for models trained on the W3C, CERC and TU benchmarks. Sufﬁxes (d) and (jm) denote Dirichlet and
Jelinek-Mercer smoothing, respectively (Section 4.3). Signiﬁcance of results is determined using a two-tailed paired randomization
test [59] (∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) with respect to the log-linear model (adjusted using the Benjamini-Hochberg
procedure for multiple testing [9]).

W3C

MAP NDCG@100 MRR

P@5 P@10 MAP

2005

2006
NDCG@100 MRR

CERC

MAP NDCG@100 MRR

P@5 P@10 MAP

2008
NDCG@100 MRR

0.135
LSI
0.243
TF-IDF
0.192
Model 1 (d)
0.190
Model 1 (jm)
0.198
Model 2 (d)
0.211
Model 2 (jm)
Log-linear (ours) 0.248

0.031
LSI
0.332
TF-IDF
0.287
Model 1 (d)
0.278
Model 1 (jm)
0.352
Model 2 (d)
0.361
Model 2 (jm)
Log-linear (ours) 0.344

0.095
LSI
0.216
TF-IDF
0.171
Model 1 (d)
0.189
Model 1 (jm)
0.154
Model 2 (d)
0.234
Model 2 (jm)
Log-linear (ours) 0.219

0.266
0.426
0.358
0.352
0.369
0.380
0.444

0.107
0.486
0.427
0.420
0.495
0.500
0.493

0.205
0.356
0.308
0.325
0.284
0.370
0.356

0.192 0.196
0.306
0.384 0.350
0.541
0.276 0.266
0.433
0.272 0.276
0.390
0.288 0.272
0.429
0.451
0.332 0.296
0.618∗ 0.412 0.361

2007

0.060
0.463
0.384
0.384
0.454
0.467
0.513

GT1

0.016 0.014
0.196 0.141
0.156 0.096
0.156 0.084
0.180 0.138
0.192 0.138
0.215 0.150

0.153
0.324
0.258
0.277
0.228
0.342
0.351

0.060 0.051
0.131 0.097
0.103 0.082
0.112 0.085
0.087 0.070
0.136 0.101
0.145∗ 0.105

0.245
0.343
0.321
0.311
0.261
0.260
0.484∗∗∗

0.038
0.269
0.181
0.170
0.264
0.274
0.342∗∗∗

0.097
0.233
0.241
0.231
0.191
0.253
0.287∗∗∗

0.371
0.531
0.491
0.483
0.419
0.423
0.667∗∗∗

0.482
0.650
0.635
0.596
0.551
0.599
0.833∗∗∗

0.099
0.465
0.355
0.347
0.461
0.463
0.519∗∗

0.106
0.525
0.388
0.339
0.510
0.517
0.656∗∗

0.208
0.378
0.385
0.373
0.334
0.394
0.425∗∗∗

0.129
0.288
0.292
0.271
0.233
0.302
0.363∗∗∗

TU

MAP NDCG@100 MRR

P@5 P@10 MAP

GT5
NDCG@100 MRR

P@5

P@10

0.287
0.492
0.478
0.502
0.441
0.449
0.713∗∗∗

0.338
0.498
0.449
0.437
0.404
0.429
0.644∗∗∗

P@5

0.042
0.332
0.200
0.181
0.281
0.278
0.381∗

P@10

0.055
0.277
0.172
0.159
0.244
0.239
0.299

P@5

P@10

0.043
0.108
0.109
0.100
0.084
0.108
0.134∗∗∗

0.036
0.079
0.081
0.075
0.065
0.081
0.092∗∗∗

ics where exact matches (Model 2) perform best, followed by ex-
amples which beneﬁt from semantic matching (log-linear model).
Topic identiﬁers are between parentheses.

W3C Topics P3P speciﬁcation and CSS3 (EX8 and EX69, respec-
tively) should return candidates associated with the deﬁnition
of these standards. The log-linear model, however, considers
these close to related technologies such as CSS2 for CSS3
and UTF-8 for P3P. Semantic matching works for topics Se-
mantic Web Coordination and Annotea server protocol (EX1
and EX103), where the former is associated with RDF li-
braries, RDF-related jargon and the names of researchers in
the ﬁeld, while the latter is associated with implementations
of the protocol and the maintainer of the project.

CERC For CSIRO, topic nanohouse (CE-035) is mentioned in
many irrelevant contexts (i.e., spam) and therefore seman-
tic matching fails. The term ﬁsh oil (CE-126) is quickly as-
sociated with different kinds of ﬁsh, oils and organizations
related to marines and ﬁsheries. On the other hand, we ob-
serve sensor networks (CE-018) to be associated with sen-
sor/networking jargon and sensor platforms. Topic forensic
science workshop (CE-103) expands to syntactically-similar
terms (e.g., plural), the names of science laboratories and ref-
erences

support/law-protection

organizations.

to

TU The TU benchmark contains both English and Dutch textual
evidence. Topics sustainable tourism and interpolation (1411
and 4882) do not beneﬁt from semantic matching due to a
semantic gap: interpolation is associated with the polyno-
mial kind while the relevance assessments focus on stochas-
tic methods. Interestingly, for the topic law and informati-
zation/computerization (1719) we see that the Dutch trans-
lation of law is very closely related. Similar terms to in-
formatization are, according to the log-linear model, Dutch
words related to cryptography. Similar dynamics are at work
for legal-political space (12603), where translated terms and
semantic-syntactic relations aid performance.

In order to further quantify the effect of the embedding matrix Wp,
we artiﬁcially expand benchmark topic terms by k nearby terms.
We then examine how the performance of a proﬁle-centric gener-
ative language model [4, Model 1] evolves for different values of
k (Figure 4). The purpose of this analysis is to provide further
insight in the differences between maximum-likelihood language
models and the log-linear model. Figure 4 shows that, for most
benchmarks, MAP increases as k goes up. Interestingly enough,
the two benchmarks that exhibit a decrease in MAP for larger k
(CERC 2007 and TU GT1) are likewise those for which generative
language models outperform the log-linear model in Table 2. This
suggests that the CERC 2007 and TU GT1 benchmarks require ex-

(a) W3C

(b) CERC

(c) TU

(d) W3C

(e) CERC

(f) TU

Figure 1: Sensitivity analysis for window size (n-gram) during parameter estimation (3) for W3C, CERC and TU benchmarks.

act term matching, while the remaining four benchmarks beneﬁt
greatly from the semantic matching provided by our model.

The per-topic differences suggest that Model 2 and the log-linear
model make very different errors: Model 2 excels at retrieving ex-
act query matches, while the log-linear model performs seman-
tic matching. Based on these observations we hypothesize that a
combination of the two approaches will raise retrieval performance
even further. To test this hypothesis, we propose a simple ensem-
ble of rankings generated by Model 2 and the log-linear model by
re-ranking candidates according to the multiplicatively-combined
reciprocal rank:

rankensemble(cj, qi) ∝

1
rankmodel 2(cj, qi)

·

1
ranklog-linear(cj, qi)

, (6)

where rankM (cj, qi) denotes the position of candidate cj in a rank-
ing generated by model M for answering query qi. Equation (6) is
equivalent to performing data fusion using CombSUM [57] where
the scores are given by the logarithm of the reciprocal ranks of the
experts. Table 3 compares the result of this ensemble to that of its
constituents. Compared to the supervised methods of Fang et al.
[23], we conclude that our fully unsupervised ensemble matches
the performance of their method on the CERC 2007 benchmark
and outperforms their method on the W3C 2005 benchmark. The
superior performance of the ensemble suggests the viability of hy-
brid methods that combine semantic and exact matching.

5.3 Scalability and efﬁciency

Inference in the log-linear model is expressed in linear algebra
operations (Section 3). These operations can be efﬁciently per-
formed by highly optimized software libraries and special-purpose
hardware (i.e., GPUs). But the baseline methods against which we
compare do not beneﬁt from these speed-ups. Furthermore, many
implementation-speciﬁc details and choice of parameter values can

inﬂuence runtime considerably (e.g. size of the latent representa-
tions). Therefore, we opt for a theoretical comparison of the infer-
ence complexity of the log-linear model and compare these to the
baselines (Section 4.3).

The log-linear model generates a ranking of candidate experts by
straight-forward matrix operations. The look-up operation in the
projection matrix Wp occurs in constant time complexity, as the
multiplication with the one-hot vector vi comes down to selecting
the i-th column from Wp. Multiplication of the |C| × e matrix
Wc with the e-dimensional word feature vector exhibits O(|C| · e)
runtime complexity. If we consider addition of the bias term and
division by the normalizing function Z1, the time complexity of (1)
becomes

O(|C| · (e + (e − 1))
(cid:125)
(cid:123)(cid:122)
(cid:124)
matrix-vector multiplication

+ |C|
(cid:124)(cid:123)(cid:122)(cid:125)
bias term

+ 2 · |C| − 1
(cid:123)(cid:122)
(cid:125)
Z1

(cid:124)

).

Notice, however, that the above analysis considers sequential exe-
cution. Modern computing hardware has the ability to parallelize
common matrix operations [24, 33]. The number of candidate ex-
perts |C| is the term that impacts performance most in the log-linear
model (under the assumption that |C| (cid:29) e).

If we consider n terms, where n is the query length during infer-
ence or the window size during training, then the complexity of (2)
becomes

O(n · |C| · (2 · e − 1) + n · (3 · |C| − 1)
(cid:125)

(cid:124)

(cid:123)(cid:122)
n forward-passes

+ (n − 1) · |C|
(cid:125)

(cid:123)(cid:122)
factor product

(cid:124)

)
+ 2 · |C| − 1
(cid:123)(cid:122)
(cid:125)
Z2

(cid:124)

Notice that Z2 does not need to be computed during inference as it
does not affect the candidate expert ranking.

(a) W3C (R = −0.39∗∗∗)

(b) CERC (R = −0.44∗∗∗)

(c) TU (R = −0.30∗∗∗)

Figure 2: Scatter plot of the normalized entropy of distribution P (c | q) (2) returned by the log-linear model and per-query av-
erage precision for W3C, CERC and TU benchmarks. Pearson’s R and associated p-value (two-tailed paired permutation test:
∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) are between parentheses. The depicted linear ﬁt was obtained using an ordinary least squares
regression.

Table 3: Comparison of Model 2, the log-linear model and an ensemble of the former on W3C, CERC and TU benchmarks. Signiﬁ-
cance of results is determined using a two-tailed paired randomization test [59] (∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) with respect to
the ensemble ranking (adjusted using the Benjamini-Hochberg procedure for multiple testing [9]).

2005
NDCG@100 MRR

0.380
0.444
0.479∗∗

0.451
0.618
0.668

2007
NDCG@100 MRR

0.500
0.493
0.589∗∗∗

0.467
0.513
0.627∗∗∗

GT1
NDCG@100 MRR

W3C

Model 2 (jm)
Log-linear (ours)
Ensemble

CERC

Model 2 (jm)
Log-linear (ours)
Ensemble

TU

Model 2 (jm)
Log-linear (ours)
Ensemble

MAP

0.211
0.248
0.291∗∗∗

MAP

0.361
0.344
0.452∗∗

MAP

0.234
0.219
0.271∗∗∗

P@5

0.332
0.412
0.440

P@5

0.192
0.215
0.248∗

P@10

0.296
0.361
0.378

P@10

0.138
0.150
0.160

MAP

0.260
0.484∗∗∗
0.433

MAP

0.274
0.342
0.395∗∗∗

2006
NDCG@100 MRR

0.423
0.667∗∗
0.634

0.599
0.833
0.825

2008
NDCG@100 MRR

0.463
0.519
0.593∗∗∗

0.517
0.656
0.716

GT5
NDCG@100 MRR

P@5

P@10

0.449
0.713∗∗
0.657

0.429
0.644∗∗
0.586

P@5

P@10

0.278
0.381
0.459∗∗

0.239
0.299
0.357∗∗∗

P@5

P@10

MAP

P@5

P@10

0.370
0.356
0.417∗∗∗

0.342
0.351
0.403∗∗∗

0.136
0.145
0.165∗∗∗

0.101
0.105
0.121∗∗∗

0.253
0.287
0.331∗∗∗

0.394
0.425
0.477∗∗∗

0.302
0.363
0.402∗∗∗

0.108
0.134
0.156∗∗∗

0.081
0.092
0.105∗∗∗

In terms of space complexity, parameters Wp, Wc and bc, in ad-
dition to the intermediate results, all require memory space propor-
tional to their size. Considering (2) for a sequence of k words and
batches of m instances, we require O(m·k·|C|) ﬂoating point num-
bers for every forward-pass to ﬁt in-memory. While such an upper
bound seems reasonable by modern computing standards, it is a
severely limiting factor when considering large-scale communities
and while utilizing limited-memory GPUs for fast computation.

The inferential complexity of the vector space-based models for
entity retrieval [19] depends mainly on the dimensionality of the
vectors and the number of candidate experts. The dimensionality
of the latent entity representations is too high for efﬁcient nearest
neighbor retrieval [29] due to the curse of dimensionality. There-
fore, the time complexity for the LSI- and TF-IDF-based vector
space models are respectively O(γ · |C|) and O(|V | · |C|), where
γ denotes the number of latent topics in the LSI-based method. As
hyperparameters e and γ both indicate the dimensionality of la-
tent entity representations, the time complexity of the LSI-based
method is comparable to that of the log-linear model. We note that
|V | (cid:29) |C| for all benchmarks (|V | is between 18 to 91 times larger
than |C|) we consider in this paper and therefore conclude that the
TF-IDF method loses to the log-linear model in terms of efﬁciency.

Compared to the unsupervised generative models of Balog et al.,
we have the proﬁle-centric Model 1 and the document-centric Mo-
del 2 with inference time complexity O(n · |C|) and O(n · |D|),
respectively, with |D| (cid:29) |C|. In the previous section we showed
that the log-linear model always performs better than Model 1 and
nearly always outperforms Model 2. Hence, our log-linear model
generally achieves the expertise retrieval performance of Model 2
(or higher) at the complexity cost of Model 1 during inference.

5.4

Incremental indexing

Existing unsupervised methods use well-understood maximum-
likelihood language models that support incremental indexing. We
now brieﬂy discuss the incremental indexing capabilities of our
proposed method. Extending the set of candidate experts C re-
quires the log-linear model to be re-trained from scratch as it changes
the topology of the network. Moreover, every document associated
with a candidate expert is considered as a negative example for all
other candidates. While it is possible to reiterate over all past doc-
uments and only learn an additional row in matrix Wc, the ﬁnal
outcome is unpredictable.

If we consider a stream of documents instead of a predeﬁned set
D, the log-linear model can be learned in an online fashion. How-
ever, stochastic gradient descent requires that training examples are

(a) W3C 2005

(b) CERC 2007

(c) TU GT1

(d) W3C 2006

(e) CERC 2008

(f) TU GT5

Figure 3: Difference of average precision between log-linear model and Model 2 [4] with Jelinek-Mercer smoothing per topic for
W3C, CERC and TU benchmarks.

picked at random such that the batched update rule (4) behaves like
the empirical expectation over the full training set [11]. While we
might be able to justify the assumption that documents arrive ran-
domly, the n-grams extracted from those documents clearly violate
this requirement.

Considering a stream of documents leads to the model forgetting
expertise evidence as an (artiﬁcial) shift in the underlying distribu-
tion of the training data occurs. While such behavior is undesirable
for the task considered in this paper, it might be well-suited for
temporal expert ﬁnding [22, 52], where expertise drift over time
is considered. However, temporal expertise ﬁnding is beyond the
scope for this paper and left for future work.

6. CONCLUSIONS

We have introduced an unsupervised discriminative, log-linear
model for the expert retrieval task. Our approach exclusively em-
ploys raw textual evidence. Future work can focus on improving
performance by feature engineering and incorporation of external
evidence. Furthermore, no relevance feedback is required during
training. This renders the model suitable for a broad range of ap-
plications and domains.

We evaluated our model on the W3C, CERC and TU bench-
marks and compared it to state-of-the-art vector space-based en-
tity ranking (based on LSI and TF-IDF) and language modeling
(proﬁle-centric and document-centric) approaches. The log-linear
model combines the ranking performance of the best maximum-
likelihood language modeling approach (document-centric) with
inference time complexity linear in the number of candidate ex-
perts. We observed a notable increase in precision over existing
methods. Analysis of our model’s output reveals a negative corre-
lation between the per-query performance and ranking uncertainty:
higher conﬁdence (i.e., lower entropy) in the rankings produced by
our approach often occurs together with higher rank quality.

An error analysis of the log-linear model and traditional lan-
guage models shows that the two make very different errors. These
errors are mainly due to the semantic gap between query intent and
the raw textual evidence. Some benchmarks expect exact query
matches, others are helped by our semantic matching. An ensemble
of methods employing exact and semantic matching generally out-
performs the individual methods. This observation calls for further
research in the area of combining exact and semantic matching.

One current limitation of our work is its scalability with respect
to the number of candidate experts. We have started investigating
trade-offs between model performance and time/space complexity.
In the future we hope to apply scalable variants of this method on
large-scale social media communities, for the purpose of determin-
ing topic ownership. While in this work we focus on expertise re-
trieval, the ideas we proposed can easily be transferred to the more
general entity retrieval task. Moreover, our approach is likely to
be applicable to authorship attribution and various other entity re-
trieval and prediction tasks.

Acknowledgments. We thank Isaac Sijaranamual, Manos Tsagkias, Tom
Kenter, Zhaochun Ren, Ke Tran and Katja Hoffman for their useful com-
ments and insights.

This research was supported by Amsterdam Data Science, the Dutch
national program COMMIT, Elsevier, the European Community’s Seventh
Framework Programme (FP7/2007-2013) under grant agreement nr 312827
(VOX-Pol), the ESF Research Network Program ELIAS, the Royal Dutch
Academy of Sciences (KNAW) under the Elite Network Shifts project, the
Microsoft Research Ph.D. program, the Netherlands eScience Center un-
der project number 027.012.105, the Netherlands Institute for Sound and
Vision, the Netherlands Organisation for Scientiﬁc Research (NWO) under
project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.-
930, CI-14-25, SH-322-15, 652.002.001, 612.001.551, the Yahoo Faculty
Research and Engagement Program, and Yandex. All content represents the
opinion of the authors, which is not necessarily shared or endorsed by their
respective employers and/or sponsors.

Computing resources were provided by the Netherlands Organisation for
Scientiﬁc Research (NWO) through allocation SH-322-15 of the Cartesius
system and by the Advanced School for Computing and Imaging (ASCII)
by allocation of the Distributed ASCII Supercomputer 4 (DAS-4) system.

(a) W3C

(b) CERC

(c) TU

Figure 4: Effect of query expansion by adding nearby terms in Wp (1) in traditional language models (Model 1 [4] with Jelinek-
Mercer smoothing) for W3C, CERC and TU benchmarks.

APPENDIX
The derivative of (3) w.r.t. bias term bc equals

∂L(Wp, Wc, bc)
∂bc

= −

1
m

(cid:32) m
(cid:88)

i=1

|dmax|
|d(i)|

|C|
(cid:88)

j=1

P (cj|d(i))

(cid:16)

∂ log

P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

(cid:33)

and w.r.t. an arbitrary matrix parameter θ (Wp or Wc):

∂L(Wp, Wc, bc)
∂θ

= −

(cid:32) m
(cid:88)

i=1

|dmax|
|d(i)|

|C|
(cid:88)

j=1

P (cj|d(i))

(cid:16)

∂ log

P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

(cid:33)

1
m

λ
m

+

θi,j.

(cid:88)

i,j

Further differentiation for parameter θ (Wp, Wc or bc):

∂bc

∂θ

∂ log (P (cj | w1, . . . , wn))
∂θ

∂P (cj | w1, . . . , wn)
∂θ

1
P (cj | w1, . . . , wn)
∂ ˜P (cj |w1,...,wn)
∂θ

∂P (cj | w1, . . . , wn)
∂θ
Z2 − ˜P (cj | w1, . . . , wn) ∂Z2
∂θ

∂Z2
∂θ

Z 2
2
∂ ˜P (ck | w1, . . . , wn)
∂θ

∂ ˜P (cj | w1, . . . , wn)
∂θ

∂P (cj | wk)
∂θ

(cid:89)

i(cid:54)=k

P (cj | wi)

=

=

=

=

(cid:88)

(cid:88)

k

k

For a given candidate cj and word wi, following (1) we have

P (cj | wi) =

˜P (cj | wi)
Z1
exp (cid:0)(cid:0)(cid:80)e
l=1 exp (cid:0)(cid:0)(cid:80)e

(cid:80)|C|

=

k=1 Wcj,k Wpk,i

k=1 Wcl,k Wpk,i

(cid:1) + bcj

(cid:1)
(cid:1) + bcl

(cid:1)

and consequently, with W (cid:62)
Wp,

pi denoting the i-th column of matrix

∂P (cj | wi)
∂Wcj

∂P (cj | wi)
∂bcj

∂P (cj | wi)
∂W (cid:62)
pi

=

=

=

(cid:17) ˜P (cj | wi)W (cid:62)
(cid:16)
Z1 − ˜P (cj | wi)
Z 2
1
(cid:17) ˜P (cj | wi)
(cid:16)
Z1 − ˜P (cj | wi)
Z 2
1

pi

(cid:16)
Wcj − (cid:80)|C|

(cid:17) ˜P (cj | wi)

l=1 Wcl
Z1

(7)

As can be seen in (7), the distributed representations of candidates
cj at time t + 1 are updated using the representation of words wi at
time t and vice versa.

REFERENCES
[1] The knowledge-based economy. Techn. report, Organisation for

Economic Co-operation and Development, 1996.

[2] P. Bailey, A. P. De Vries, N. Craswell, and I. Soboroff. Overview

of the TREC 2007 enterprise track. In TREC, 2007.
[3] K. Balog. People Search in the Enterprise. PhD thesis,

University of Amsterdam, 2008.

[4] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for
expert ﬁnding in enterprise corpora. In SIGIR, pages 43–50,
2006.

[5] K. Balog, L. Azzopardi, and M. de Rijke. A language modeling

framework for expert ﬁnding. IPM, 45:1–19, 2009.
[6] K. Balog, Y. Fang, M. de Rijke, P. Serdyukov, and L. Si.

Expertise retrieval. Found. & Tr. in Information Retrieval, 6
(2-3):127–256, 2012.

[7] I. Becerra-Fernandez. Role of artiﬁcial intelligence technologies
in the implementation of People-Finder knowledge management
systems. Knowledge-Based Systems, 13(5):315–320, 2000.
[8] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural
probabilistic language model. JMLR, 3:1137–1155, 2003.
[9] Y. Benjamini and Y. Hochberg. Controlling the false discovery
rate: a practical and powerful approach to multiple testing.
JSTOR, pages 289–300, 1995.

[10] R. Berendsen, M. de Rijke, K. Balog, T. Bogers, and A. van den
Bosch. On the assessment of expertise proﬁles. JASIST, 64(10):
2024–2044, 2013.

[11] L. Bottou. Large-scale machine learning with stochastic gradient
descent. In COMPSTAT, pages 177–186. Springer, 2010.
[12] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,

N. Hamilton, and G. Hullender. Learning to rank using gradient
descent. In ICML, pages 89–96, 2005.

[13] Y. Cao, J. Liu, S. Bao, and H. Li. Research on Expert Search at

Enterprise Track of TREC 2005. In TREC, pages 2–5, 2005.
[14] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,
and P. Kuksa. Natural language processing (almost) from
scratch. JMLR, 12(Aug):2493–2537, 2011.

[15] N. Craswell, D. Hawking, A.-M. Vercoustre, and P. Wilkins.

P@noptic expert: Searching for experts not just for documents.
In Ausweb Poster Proceedings, pages 21–25, 2001.

[16] N. Craswell, A. P. de Vries, and I. Soboroff. Overview of the

TREC 2005 enterprise track. In TREC, 2005.

[17] T. H. Davenport and L. Prusak. Working Knowledge. Harvard

Business Review Press, 1998.

[18] S. C. Deerwester, S. T. Dumais, and R. A. Harshman. Indexing

by latent semantic analysis. JASIS, 1990.

[19] G. Demartini, J. Gaugaz, and W. Nejdl. A vector space model

[41] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Distributed

for ranking entities and its application to expert search. In ECIR,
pages 189–201. Springer, 2009.

representations of words and phrases and their compositionality.
In NIPS, pages 3111–3119, 2013.

[20] L. Deng, X. He, and J. Gao. Deep stacking networks for

information retrieval. In ICASSP, pages 3153–3157, 2013.
[21] H. Fang and C. Zhai. Probabilistic models for expert ﬁnding. In

ECIR, pages 418–430, Berlin, Heidelberg, 2007.
Springer-Verlag.

[22] Y. Fang and A. Godavarthy. Modeling the dynamics of personal

expertise. In SIGIR, pages 1107–1110, 2014.

[23] Y. Fang, L. Si, and A. P. Mathur. Discriminative models of

integrating document evidence and document-candidate
associations for expert search. In SIGIR, pages 683–690, 2010.

[24] K. Fatahalian, J. Sugerman, and P. Hanrahan. Understanding the
efﬁciency of gpu algorithms for matrix-matrix multiplication. In
SIGGRAPH HWWS, pages 133–137. ACM, 2004.

[25] X. Glorot and Y. Bengio. Understanding the difﬁculty of training
deep feedforward neural networks. In AISTATS, pages 249–256,
2010.

[26] G. E. Hinton. Learning distributed representations of concepts.
In 8th Annual Conference of the Cognitive Science Society,
volume 1, page 12, Amherst, MA, 1986.

[27] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR,

pages 50–57. ACM, 1999.

[28] P.-s. Huang, N. M. A. Urbana, X. He, J. Gao, L. Deng, A. Acero,
and L. Heck. Learning deep structured semantic models for web
search using clickthrough data. In CIKM, pages 2333–2338,
2013.

[29] P. Indyk and R. Motwani. Approximate nearest neighbors:

towards removing the curse of dimensionality. In STOC, pages
604–613. ACM, 1998.

[30] M. Karimzadehgan and C. Zhai. Estimation of statistical

translation models based on mutual information for ad hoc
information retrieval. In SIGIR, pages 323–330. ACM, 2010.
[31] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural

language models. In ICML, pages 595–603, 2014.

[32] J. Kruger and D. Dunning. Unskilled and unaware of it: how

difﬁculties in recognizing one’s own incompetence lead to
inﬂated self-assessments. J. Personality and Social Psych., 77
(6):1121, 1999.

[33] J. Krüger and R. Westermann. Linear algebra operators for gpu
implementation of numerical algorithms. ACM Transactions on
Graphics, 22(3):908–916, 2003.

[34] H. Li and J. Xu. Semantic matching in search. Found. & Tr. in

Information Retrieval, 7(5):343–469, June 2014.

[35] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer,

2011.

[36] C. MacDonald and I. Ounis. Voting for candidates: adapting

data fusion techniques for an expert search task. In CIKM, pages
387–396, 2006.

[37] C. Macdonald and I. Ounis. Expert search evaluation by

supporting documents. In ECIR, pages 555–563. Springer, 2008.

[38] M. T. Maybury. Expert ﬁnding systems. Techn. Report

[39] D. W. McDonald and M. S. Ackerman. Expertise recommender.

MTR-06B000040, MITRE, 2006.

In CSCW, pages 231–240, 2000.

[42] T. Mikolov, G. Corrado, K. Chen, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
1301.3781, 2013.

[43] A. Mnih and G. Hinton. Three new graphical models for

statistical language modelling. In ICML, pages 641–648, 2007.

[44] A. Mnih and G. Hinton. A scalable hierarchical distributed
language model. In NIPS, pages 1081–1088, 2008.
[45] A. Mnih and K. Kavukcuoglu. Learning word embeddings

efﬁciently with noise-contrastive estimation. In NIPS, pages
2265–2273, 2013.

[46] G. Montavon, G. B. Orr, and K.-R. Müller. Neural Networks:

Tricks of the Trade. Springer, 2012.

[47] C. Moreira, B. Martins, and P. Calado. Using rank aggregation
for expert search in academic digital libraries. In Simpósio de
Informática, INForum, pages 1–10, 2011.

[48] J. Pennington, R. Socher, and C. D. Manning. GloVe: Global

Vectors for Word Representation. In EMNLP, pages 1532–1543,
2014.

[49] D. Petkova and W. B. Croft. Hierarchical language models for
expert ﬁnding in enterprise corpora. In ICTAI ’06, pages
599–606, 2006.

[50] W. W. Powell and K. Snellman. The knowledge economy.
Annual review of sociology, pages 199–220, 2004.

[51] D. Rumelhart, G. Hinton, and R. Williams. Learning internal
representations by back propagation. In Parallel Distributed
Processing, pages 318–362. MIT Press, 1986.

[52] J. Rybak, K. Balog, and K. Nørvåg. Temporal expertise
proﬁling. In ECIR, pages 540–546. Springer, 2014.
[53] R. Salakhutdinov and G. Hinton. Semantic hashing. Int. J.

Approximate Reasoning, 50(7):969–978, 2009.

[54] P. Serdyukov and D. Hiemstra. Modeling documents as mixtures
of persons for expert ﬁnding. In ECIR, pages 309–320. Springer,
2008.

[55] P. Serdyukov, H. Rode, and D. Hiemstra. Modeling multi-step
relevance propagation for expert ﬁnding. In CIKM, pages
1133–1142, 2008.

[56] C. Shannon. A mathematical theory of communication. Bell

System Technical J., 27:379–423, 623–656, 1948.

[57] J. A. Shaw, E. A. Fox, J. A. Shaw, and E. A. Fox. Combination

of multiple searches. In TREC, pages 243–252, 1994.
[58] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. A latent
semantic model with convolutional-pooling structure for
information retrieval. In CIKM, pages 101–110, 2014.
[59] M. D. Smucker, J. Allan, and B. Carterette. A comparison of

statistical signiﬁcance tests for information retrieval evaluation.
In CIKM, pages 623–632. ACM, 2007.

[60] P. Sorg and P. Cimiano. Finding the right expert: Discriminative
models for expert retrieval. In KDIR, pages 190–199, 2011.

[61] TREC. Enterprise Track, 2005–2008.
[62] D. van Dijk, M. Tsagkias, and M. de Rijke. Early detection of
topical expertise in community question and answering. In
SIGIR, 2015.

[63] V. Vapnik. Statistical learning theory, volume 1. Wiley New

[40] T. Mikolov, M. Karaﬁat, L. Burget, J. Cernocky, and

York, 1998.

S. Khudanpur. Recurrent neural network based language model.
In Interspeech, pages 1045–1048, 2010.

[64] M. D. Zeiler. Adadelta: An adaptive learning rate method.

CoRR, abs/1212.5701, 2012.

Unsupervised, Efﬁcient and Semantic Expertise Retrieval

Christophe Van Gysel
cvangysel@uva.nl

Maarten de Rijke
derijke@uva.nl

Marcel Worring
m.worring@uva.nl

University of Amsterdam, Amsterdam, The Netherlands

7
1
0
2
 
p
e
S
 
7
1
 
 
]

R

I
.
s
c
[
 
 
2
v
1
5
6
6
0
.
8
0
6
1
:
v
i
X
r
a

ABSTRACT
We introduce an unsupervised discriminative model for the task
of retrieving experts in online document collections. We exclu-
sively employ textual evidence and avoid explicit feature engineer-
ing by learning distributed word representations in an unsupervised
way. We compare our model to state-of-the-art unsupervised sta-
tistical vector space and probabilistic generative approaches. Our
proposed log-linear model achieves the retrieval performance lev-
els of state-of-the-art document-centric methods with the low infer-
ence cost of so-called proﬁle-centric approaches. It yields a statis-
tically signiﬁcant improved ranking over vector space and genera-
tive models in most cases, matching the performance of supervised
methods on various benchmarks. That is, by using solely text we
can do as well as methods that work with external evidence and/or
relevance feedback. A contrastive analysis of rankings produced
by discriminative and generative approaches shows that they have
complementary strengths due to the ability of the unsupervised dis-
criminative model to perform semantic matching.

1.

INTRODUCTION

The transition to the knowledge and information economy [1]
introduces a great reliance on cognitive capabilities [50]. It is cru-
cial for employers to facilitate information exchange and to stim-
ulate collaboration [17].
In the past, organizations would set-up
special-purpose database systems for their members to maintain
a proﬁle [7]. However, these systems required employees to be
proactive. In addition, self-assessments are known to diverge from
reality [10, 32] and document collections can quickly become prac-
tically infeasible to manage manually. Therefore, there has been an
active interest in automated approaches for constructing expertise
proﬁles [7, 61] and retrieving experts from an organization’s het-
erogeneous document repository [15]. Expert ﬁnding (also known
as expertise retrieval or expert search) addresses the task of ﬁnding
the right person with the appropriate skills and knowledge [6]. It
attempts to provide an answer to the question:

Given a topic X, who are the candidates with the most
expertise w.r.t. X?

This is an electronic version of an Article published in the 25th International
World Wide Web Conference.
c(cid:13) 2016 International World Wide Web Conference Committee.
WWW 2016, April 11–15, 2016, Montréal, Québec, Canada.
ACM 978-1-4503-4143-1/16/04.
http://dx.doi.org/10.1145/2872427.2882974.

The expertise retrieval task gained popularity in the research com-
munity during the TREC Enterprise Track [61] and has remained
relevant ever since, while broadening to social media and to track-
ing the dynamics of expertise [5, 6, 10, 19, 21–23, 47, 49, 62].
Existing methods fail to address key challenges: (1) Queries and
expert documents use different representations to describe the same
concepts [26, 34]. Term mismatches between queries and experts
[34] occur due to the inability of widely used maximum-likelihood
language models to make use of semantic similarities between words
[53]. (2) As the amount of available data increases, the need for
more powerful approaches with greater learning capabilities than
smoothed maximum-likelihood language models is obvious [63].
(3) Supervised methods for expertise retrieval [23, 47] were intro-
duced at the turn of the last decade. However, the acceleration
of data availability has the major disadvantage that, in the case of
supervised methods, manual annotation efforts need to sustain a
similar order of growth. This calls for the further development of
unsupervised methods. (4) In some expertise retrieval methods, a
language model is constructed for every document in the collection.
These methods lack efﬁcient query capabilities for large document
collections, as each query term needs to be matched against ev-
ery document [6]. Our proposed solution has a strong emphasis on
unsupervised model construction, efﬁcient query capabilities and
semantic matching between query terms and candidate experts.

Speciﬁcally, we propose an unsupervised log-linear model with
efﬁcient inference capabilities for the expertise retrieval task. We
show that our approach improves retrieval performance compared
to vector space-based and generative language models, mainly due
to its ability to perform semantic matching [34]. Our method does
not require supervised relevance judgments and is able to learn
from raw textual evidence and document-candidate associations
alone. The purpose of this work is to provide insight in how dis-
criminative language models can improve performance of core re-
trieval tasks compared to maximum-likelihood language models.
Therefore, we avoid explicit feature engineering and the incorpo-
ration of external evidence in this paper. In terms of performance,
the current best-performing formal language model [4] exhibits a
worst-case time complexity linear in the size of the document col-
lection. In contrast, the inference time complexity of our approach
is asymptotically bounded by the number of candidate experts.

Our research questions are as follows: (1) How does our dis-
criminative log-linear model compare to vector space-based meth-
ods and generative language models for the expert retrieval task in
terms of retrieval performance? (2) What can we learn regarding
the different types of errors made by generative and discriminative
language models? (3) How does the complexity of inference in
our log-linear model compare to vector-space based and generative

models? (4) How does the log-linear model handle incremental
indexing and what are its limitations?

We contribute: (i) An unsupervised log-linear model with efﬁ-
cient inference capabilities for the expertise retrieval task, together
with an open-source implementation.1 (ii) Comparison of the re-
trieval performance of the log-linear model with traditional vector
space-based models and language model methods on well-known
benchmarks. (iii) Insights in how certainty in predictions by the
log-linear model correlates with performance. (iv) In-depth analy-
sis of the inference complexity of the log-linear model. (v) Com-
parative error analysis between the semantic log-linear model and
traditional generative language models that perform exact match-
ing. (vi) Insight in the relative strengths of semantic matching and
exact matching for the expert retrieval task.

The remainder of this paper is organized as follows. In Section 2,
we brieﬂy discuss related work. Section 3 introduces the log-linear
model for the expert retrieval problem. In Section 4 we state our
research questions and detail our experimental set-up and imple-
mentations. We provide an overview of our experimental results,
followed by a discussion and analysis in Section 5. Section 6 con-
cludes the paper and discusses ideas for future work.

2. RELATED WORK

We ﬁrst discuss prior work on expert retrieval and its relation
to document retrieval. Then, we review semantic search methods.
The ideas we present in this paper are inspired by neural language
models used in automated speech recognition (ASR) and natural
language processing (NLP). Consequently, we also review work
from those ﬁelds.

2.1 Expert retrieval

Early expert retrieval systems were often referred to as expert
locator and expertise management systems [38]. These database
systems often relied on people to self-assess their expertise against
a predeﬁned set of topics [39], which is known to generate unreli-
able results [7].

With the introduction of the P@NOPTIC system [15], and later
the TREC Enterprise track [61], there has been an active research
interest in automated expertise proﬁling methods. It is useful to dis-
tinguish between proﬁle-based methods, which create a textual rep-
resentation of a candidate’s knowledge, and document-based meth-
ods, which represent candidates as a weighted combination of doc-
uments. The latter generally performs better at ranking, while the
former is more efﬁcient as it avoids retrieving all documents rele-
vant to a query [6, p. 221].

There has been much research on generative probabilistic mod-
els for expert retrieval [21, 49]. Such models have been categorized
in candidate generation models [13], topic generation models [4, 5]
and proximity-based variants [5, 54]. Of special relevance to us are
the unsupervised proﬁle-centric (Model 1) and document-centric
(Model 2) models of Balog et al. [4], which focus on raw textual ev-
idence without incorporating collection-speciﬁc information (e.g.,
query modeling, document importance or document structure). Su-
pervised discriminative models [23, 47, 60] are preferred when
query-candidate relevance pairs are available for training. Unlike
their generative counterparts these models have no issue combining
complex and heterogeneous features (e.g., link-based features, doc-
ument importance features, etc.); they resemble Learning to Rank
(L2R) methods for document retrieval [6, 35]. However, a lack
of training data may greatly hinder their applicability [6, p. 179].
Beyond unsupervised generative and supervised discriminative ap-

1https://github.com/cvangysel/SERT

proaches, there are graph-based approaches based on random walks
[55] and voting-based approaches based on data fusion [36]. De-
martini et al. [19] propose a vector space-based method for the en-
tity ranking task; their framework extends vector spaces operating
on documents to entities. See [6] for a survey on the topic.

2.2 Latent semantic models for document re-

trieval

Li and Xu [34] note that the query document mismatch poses the
most critical challenge in search. Semantic matching is an impor-
tant attempt to remedy this problem. There has been much work
on bridging the semantic gap for various different tasks [19, 26, 28,
30, 31, 41, 48, 53]. Expertise and document retrieval [6, p. 224] are
closely related as performance of the latter can greatly impact that
of the former [37].

Latent Semantic Models (LSM) ﬁrst became popular through
the introduction of Latent Semantic Indexing (LSI) [18], followed
by probabilistic LSI (pLSI) [27]. LSMs based on neural networks
[28, 53, 58] emerged in the last decade. Salakhutdinov and Hinton
[53] employ unsupervised deep auto-encoders to map documents to
bit patterns using semantic addressing. Huang et al. [28] perform
semantic matching of documents and queries by leveraging click-
through data optimizing for Web document ranking. Further, deep
models [12, 20] have been proposed to learn to rank [35].

2.3 Neural language modeling

Large-vocabulary neural probabilistic language models for mod-
eling word sequence distributions have become very popular re-
cently [8, 43, 44]. These models learn continuous-valued distributed
representations for words, also known as embeddings [42, 45, 48],
in order to ﬁght the curse of dimensionality and increase general-
ization by introducing the expectation that similar word vectors sig-
nify semantically or syntactically similar words. Recurrent neural
language models have shown to perform well in ASR [40]. Col-
lobert et al. [14] propose a uniﬁed neural network architecture for
various NLP tasks. Even more recently, there has been a surge in
multimodal neural language models [31], which lend themselves to
the task of automated image captioning.

What we add on top of the related work described above is the
following.
In this work we model the conditional probability of
the expertise of a candidate given a single query term (contrary
to binary relevance given a character-based n-gram [28]). In the
process we learn a distributed vector representation (similar to LSI,
pLSI and semantic hashing) for both words and candidates such
that nearby representations indicate semantically similar concepts.
We propose a log-linear model that is similar to neural language
models. The important difference is that we predict a candidate
expert instead of the next word. To the best of our knowledge,
we are the ﬁrst to propose such a solution. We employ an embed-
ding layer in our shallow model for the same reasons as mentioned
above: we learn continuous word representations that incorporate
semantic and syntactic similarity tailored to an expert’s domain.

3. A LOG-LINEAR MODEL FOR

of words w1, . . . , wk we have:

EXPERT SEARCH

In the setting of this paper we have a document collection D and
a predeﬁned set of candidate experts C (entities to be retrieved).
Documents d ∈ D are represented as a sequence of words w1, . . . ,
w|d| originating from a vocabulary V , where wi ∈ V and the oper-
ator |·| denotes the document length in tokens. For every document
d ∈ D we write Cd to denote the set of candidates c ∈ C asso-
ciated with it (i.e., C = (cid:83)
d∈D Cd). These document-candidate
associations can be obtained explicitly from document meta-data
(e.g., the author of an e-mail) or implicitly by mining references
to candidates from the document text. Notice that some documents
might not be associated with any candidate. When presented with a
query q of constituent terms t1, . . . , t|q|, the expert retrieval task is
to return a list of candidates ρ(C) ordered according to topical ex-
pertise. We generate this ranking using a relatively shallow neural
network which directly models P (c | q).

We employ vector-based distributed representations [26], for both
words (i.e., word embeddings) and candidate experts, in a way that
motivates the unsupervised construction of features that express
regularities of the expertise ﬁnding domain. These representations
can capture the similarity between concepts (e.g., words and can-
didate experts) by the closeness of their representations in vector
space. That is, concepts with similar feature activations are inter-
preted by the model as being similar, or even interchangeable.

3.1 The model

To address the expert search task, we model P (cj | q) and rank
candidates cj accordingly for a given q. We propose an unsuper-
vised, discriminative approach to obtain these probabilities. We
construct our model solely from textual evidence: we do not re-
quire query-candidate relevance assessments for training and do not
consider external evidence about the corpus (e.g., different weight-
ings for different sub-collections), the document (e.g., considering
certain parts of the document more useful) nor link-based features.
Let e denote the size of the vector-based distributed representa-
tions of both words in V and candidate experts in C. These repre-
sentations will be learned by the model using gradient descent [42]
(Section 3.2). For notational convenience, we write P (c | ·) for the
(conditional) probability distribution over candidates, which is the
result of vector arithmetic. We deﬁne the probability of a candidate
cj given a single word wi ∈ V as the log-linear model

P (c | wi) =

exp (Wc · (Wp · vi) + bc) ,

(1)

1
Z1

where Wp is the e × |V | projection matrix that maps the one-hot
representation (i.e., 1-of-|V |) of word wi, vi, to its e-dimensional
distributed representation, bc is a |C|-dimensional bias vector and
Wc is the |C| × e matrix that maps the word embedding to an un-
normalized distribution over candidates C, which is then normal-
ized by Z1 = (cid:80)|C|
j=1 [exp (Wc · (Wp · vi) + bc)]j. If we consider
Bayes’ theorem, the transformation matrix Wc and bias vector bc
can be interpreted as the term log-likelihood log P (wi | c) and
candidate log-prior log P (c), respectively. The projection matrix
Wp attempts to soften the curse of dimensionality introduced by
large vocabularies V and maps words to word feature vectors [8].
Support for large vocabularies is crucial for retrieval tasks [28, 53].
We then assume conditional independence of a candidate’s ex-
pertise given an observation of data (i.e., a word). Given a sequence

P (c | w1, . . . , wk) =

˜P (c | w1, . . . , wk) =

P (c | wi)

1
Z2

1
Z2

(cid:32) k

(cid:88)

1
Z2

k
(cid:89)

i=1
(cid:33)

=

exp

log (P (c | wi))

(2)

(cid:16)(cid:80)k

j=1 exp

i=1
where ˜P (c | w1, . . . , wk) denotes the unnormalized score and
(cid:17)
Z2 = (cid:80)|C|
i=1 log (P (cj | wi))
is a normalizing term.
The transformation to log-space in (2) is a well-known trick to pre-
vent ﬂoating point underﬂow [46, p. 445]. Given (2), inference is
straight-forward. That is, given query q = t1, . . . , tk, we compute
P (c | t1, . . . , tk) and rank the candidate experts in descending
order of probability.

Eq. 1 deﬁnes a neural network with a single hidden layer. We
can add additional layers. Preliminary experiments, however, show
that the shallow log-linear model (1) performs well-enough in most
cases. Only for larger data sets did we notice a marginal gain from
adding an additional layer between projection matrix Wp and the
softmax layer over C (Wc and bias bc), at the expense of longer
training times and loss of transparency.

3.2 Parameter estimation

The matrices Wp, Wc and the vector bc in (1) constitute the
parameters of our model. We estimate them using error back prop-
agation [51] as follows. For every document dj ∈ D we construct
an ideal distribution over candidates p = P (c | dj) based on the
document-candidate associations Cdj such that

P (c | dj) =

(cid:40) 1
|Cdj
0,

| ,

c ∈ Cdj

c (cid:54)∈ Cdj

We continue by extracting n-grams where n remains ﬁxed during
training. For every n-gram w1, . . . , wn generated from document
d we compute ˜p = P (c | w1, . . . , wn) using (2). During model
constructing we then optimize the cross-entropy H(p, ˜p) (i.e., the
joint probability of the training data if |Cdj | = 1 for all j) using
batched gradient descent. The loss function for a single batch of m
instances with associated targets (p(i), ˜p(i)) is as follows:

L(Wp, Wc, bc)

=

1
m

+

m
(cid:88)

i=1

λ
2m

|dmax|
|d(i)|
(cid:32)

(cid:88)

i,j

H(p(i), ˜p(i))

W 2

pi,j +

W 2

ci,j

(cid:88)

i,j

= −

|dmax|
|d(i)|

|C|
(cid:88)

j=1

1
m

m
(cid:88)

i=1
(cid:32)

+

λ
2m

(cid:88)

i,j

W 2

pi,j +

W 2

ci,j

,

(cid:88)

i,j

(cid:33)

(cid:33)

(3)

P (cj | d(i)) log

(cid:16)
P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

where d(i) refers to the document from which n-gram w(i)
1 , . . . , w(i)
n
was extracted, dmax = arg maxd∈D |d| indicates the longest doc-
ument in the collection, and λ is a weight regularization parameter.
The update rule for a particular parameter θ (Wp, Wc or bc) given
a single batch of size m is:

θ(t+1) = θ(t) − α(t) (cid:12)

∂L(Wp

(t), bc

(t))

(t), Wc
∂θ

,

(4)

where α(t) and θ(t) denote the per-parameter learning rate and pa-
rameter θ at time t, respectively. The learning rate α consists of
the same number of elements as there are parameters; in the case
of a global learning rate, all elements of α are equal to each other.
The derivatives of the loss function (3) are given in the Appendix.
In the next section we will discuss our experimental setup, fol-
lowed by an overview of our experimental results and further anal-
ysis in Section 5.

4. EXPERIMENTAL SETUP

4.1 Research questions

As indicated in the introduction, we seek to answer the following

research questions:
RQ1 How does our discriminative log-linear model compare to
vector space-based methods and generative language models
for the expert retrieval task in terms of retrieval performance?
In particular, how does the model perform when compared against
vector space-based (LSI and TF-IDF) and generative approaches
(proﬁle-centric Model 1 and document-centric Model 2)?
RQ2 What can we learn regarding the different types of errors

made by generative and discriminative language models?

Does the best-performing generative model simply perform slightly
better on the topics for which the other models perform decent as
well, or do they make very different errors? If the latter holds,
an ensemble of the rankings produced by both model types might
exceed performance of the individual rankings.
RQ3 How does the complexity of inference in our log-linear model
compare to vector-space based and generative models?
The worst-case inference cost of document-centric models makes
them unattractive in online settings where the set of topics is not
deﬁned beforehand and the document collection is large. Proﬁle-
centric methods are preferred in such settings as they infer from
one language model per candidate expert for every topic (i.e., a
pseudo-document consisting of a concatenation of all documents
associated with an expert) [6]. Vector space-based methods [19]
have similar problems due to the curse of dimensionality [29] and
consequently their inferential time complexity is likewise asymp-
totically bounded by the number of experts.
RQ4 How does the log-linear model handle incremental indexing

and what are its limitations?

4.2 Benchmarks

The proposed method is applicable in the setting of the Expert
Search task of the TREC Enterprise track from 2005 to 2008 [61].
We therefore evaluate on the W3C and CERC benchmarks released
by the track. The W3C dataset [16] is a crawl of the W3C’s sites
in June 2004 (mailing lists, web pages, etc.). The CSIRO Enter-
prise Research Collection (CERC) [2] is a dump of the intranet
of Australia’s national science agency. Additionally, we evaluate
our method on a smaller, more recent benchmark based on the em-
ployee database of Tilburg University (TU) [10], which consists of
bi-lingual, heterogeneous documents. See Table 1.

Mining document-candidate associations and how they inﬂuence
performance has been extensively covered in previous work [4, 6]
and is beyond the scope of this work. For TU, the associations
are part of the benchmark. For W3C, a list of possible candidates
is given and we extract the associations ourselves by performing
a case-insensitive match of full name or e-mail address [4]. For
CERC, we make use of publicly released associations [3].

As evaluation measures we use Mean Average Precision (MAP),
Mean Reciprocal Rank (MRR), Normalized Discounted Cumula-

tive Gain at rank 100 (NDCG@100) and Precision at rank 5 (P@5)
and rank 10 (P@10).

4.3 Baselines

We compare our approach to existing unsupervised methods for
expert retrieval that solely rely on textual evidence and static doc-
ument-candidate associations. (1) Demartini et al. [19] propose a
generic framework to adapt vector spaces operating on documents
to entities. We compare our method to TF-IDF (raw frequency and
inverse document frequency) and LSI (300 latent topics) variants
of their vector space model for entity ranking (using cosine sim-
ilarity). (2) In terms of language modeling, Balog et al. [4] pro-
pose two models for expert ﬁnding based on generative language
models. The ﬁrst takes a proﬁle-centric approach comparing the
language model of every expert to the query, while the second is
document-centric. We consider both models with different smooth-
ing conﬁgurations: Jelinek-Mercer (jm) smoothing with λ = 0.5
[4] and Dirichlet (d) smoothing with β equal to the average doc-
ument length [5] (see Table 1). Signiﬁcance of results produced
by the baselines (compared to our method) is determined using a
two-tailed paired randomization test [59].

4.4

Implementation details

The vocabulary V is constructed from each corpus by ignoring
punctuation, stop words and case; numbers are replaced by a nu-
merical placeholder token. During our experiments we prune V
by only retaining the 216 most-frequent words so that each word
can be encoded by a 16-bit unsigned integer. Incomplete n-gram
instances are padded by a special-purpose token.

In terms of parameter initialization, we sample the initial matri-

ces Wc and Wp (1) uniformly in the range
(cid:34)
(cid:114) 6.0

(cid:114) 6.0

(cid:35)

−

,

m + n

m + n

for an m × n matrix, as this initialization scheme improves model
training convergence [25], and take the bias vector bc to be null.
The projection layer Wp is initialized with pre-trained word repre-
sentations trained on Google News data [41]; the number of word
features is set to e = 300, similar to pre-trained representations.

We used adadelta (ρ = 0.95, (cid:15) = 10−6) [64] with batched gra-
dient descent (m = 1024) and weight decay λ = 0.01 during
training on NVidia GTX480 and NVidia Tesla K20 GPUs. We only
iterate once over the entire training set for each experiment.

5. RESULTS AND DISCUSSION

We start by giving a high-level overview of our experimental re-
sults and then address issues of scalability, provide an error analysis
and discuss the issue of incremental indexing.

5.1 Overview of experimental results

We evaluate the log-linear model on the W3C, CERC and TU
benchmarks (Section 4.2). During training we extract non-overlap-
ping n-grams for the W3C and CERC benchmarks and overlapping
n-grams for the TU benchmark. As the TU benchmark is consider-
ably smaller, we opted to use overlapping n-grams to counter data
sparsity. The architecture of each benchmark model (e.g., num-
ber of candidate experts) is inherently speciﬁed by the benchmarks
themselves (see Table 1). However, the choice of n-gram size dur-
ing training remains open. Errors for input w1, . . . , wn are propa-
gated back through Wc until the projection matrix Wp is reached;
if a single word wi causes a large prediction error, then this will in-
ﬂuence its neighboring words w1, . . . , wi−1, wi+1, . . . , wn as well.

Table 1: An overview of the three datasets (W3C, CERC and TU) used for evaluation and analysis.

Number of documents
Average document lengtha

Number of candidatesb

Number of document-candidate associations
Number of documents (with Cd > 0)
Number of associations per documentc
Number of associations per candidate

Queries

W3C

CERC

331,037

1,237.23

370,715

460.48

715

200,939
93,826

3,479

236,958
123,934

TU

31,209
2,454.93

977

36,566
27,834

2.14 ± 3.29
281.03 ± 666.63

1.91 ± 3.70
68.11 ± 1,120.74

1.13 ± 0.39
37.43 ± 61.00

49
50

(2005)
(2006)

50
77

(2007)
(2008)

1,662
1,266

(GT1)
(GT5)

a Measured in number of tokens.
b Only candidates with at least a single document association are considered.
c Only documents with at least one association are considered.

This allows the model to learn continuous word representations tai-
lored to the expert retrieval task and the benchmark domain.

A larger window size has a negative impact on batch throughput
during training. We are thus presented with the classic trade-off
between model performance and construction efﬁciency. Notice,
however, that the number of n-grams decreases as the window size
increases if we extract non-overlapping instances. Therefore, larger
values of n lead to faster wall-clock time model construction for the
W3C and CERC benchmarks in our experiments.

We sweep over the window width n = 2i (0 ≤ i < 6) for all
three benchmarks and their corresponding relevance assessments.
We report MAP and MRR for every conﬁguration (see Figure 1).
We observe a signiﬁcant performance increase between n = 1
and n = 2 on all benchmarks, which underlines the importance
of the window size parameter. The increase in MAP implies that
the performance achieved is not solely due to initialization with
pre-trained representations (Section 4.4), but that the model efﬁ-
ciently learns word representations tailored to the problem domain.
The highest MAP scores are attained for relatively low n. As n
increases beyond n = 8 a gradual decrease in MAP is observed on
all benchmarks. In our remaining experiments we choose n = 8
regardless of the benchmark.

Words wi that mainly occur in documents associated with a par-
ticular expert are learned to produce distributions P (c | wi) with
less uncertainty than words associated with many experts in (1).
The product of P (c | wi) in (2) aggregates this expertise evidence
generated by query terms. Hence, queries with strong evidence for
a particular expert should be more predictable than very generic
queries. To quantify uncertainty we measure the normalized en-
tropy [56] of P (c | q):

η(c | q) = −

p(cj | q) log(p(cj | q)).

(5)

1
log(|C|)

|C|
(cid:88)

j=1

Equation 5 can be interpreted as a similarity measure between the
given distribution and the uniform distribution. Importantly, Fig-
ure 2 shows that there is a statistically signiﬁcant negative correla-
tion between query-wise normalized entropy and average precision
for all benchmarks.

Table 2 presents a comparison between the log-linear model and
the various baselines (Section 4.3). Our unsupervised method sig-
niﬁcantly (p < 0.01) outperforms the LSI-based method consis-
tently. In the case of the TF-IDF method and the proﬁle-centric

generative language models (Model 1), we always perform bet-
ter and statistical signiﬁcance is achieved in the majority of cases.
The document-centric language models (Model 2) perform slightly
better than our method on two (out of six) benchmarks in terms
of MAP and NDCG@100:(1) For the CERC 2007 assessment we
match performance of the document-centric generative model with
Jelinek-Mercer smoothing. (2) For TU GT1 the generative coun-
terpart seems to outperform our method.

Notice that over all assessments, the log-linear model consis-
tently outperforms all proﬁle-centric approaches and is only chal-
lenged by the smoothed document-centric approach. In addition,
for the precision-based measures (P@k and MRR), the log-linear
model consistently outperforms all other methods we compare to.
Next, we turn to a topic-wise comparative analysis of discrimi-
native and generative models. After that, we analyze the scalability
and efﬁciency of the log-linear model and compare it to that of the
generative counterparts, and address incremental indexing.

5.2 Error analysis

How does our log-linear model achieve its superior performance
over established generative models? Figure 3 depicts the per-topic
differences in average precision between the log-linear model and
Model 2 (with Jelinek-Mercer smoothing) on all benchmarks. For
each plot, the vertical bars with a positive AP difference correspond
to test topics for which the log-linear model outperforms Model 2
and vice versa for bars with a negative AP difference.

The beneﬁt gained from the projection matrix Wp is two-fold.
First, it avoids the curse of dimensionality introduced by large vo-
cabularies. Second, term similarity with respect to the expertise
domain is encoded in latent word features. When examining words
nearby query terms in the embedding space, we found words to be
related to the query term. For example, word vector representations
of xml and nonterminal are very similar for the W3C benchmark (l2
norm). This can be further observed in Figure 1: log-linear models
trained on single words perform signiﬁcantly worse compared to
those that are able to learn from neighboring words.

We now take a closer look at the topics for which the log-linear
model outperforms Model 2 and vice versa. More speciﬁcally, we
investigate textual evidence related to a topic and whether it is con-
sidered relevant by the benchmark. For the log-linear model, we ex-
amine terms nearby topic terms in Wp (l2-norm), as these terms are
considered semantically similar by the model and provide a means
for semantic matching. For every benchmark, we ﬁrst consider top-

Table 2: Evaluation results for models trained on the W3C, CERC and TU benchmarks. Sufﬁxes (d) and (jm) denote Dirichlet and
Jelinek-Mercer smoothing, respectively (Section 4.3). Signiﬁcance of results is determined using a two-tailed paired randomization
test [59] (∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) with respect to the log-linear model (adjusted using the Benjamini-Hochberg
procedure for multiple testing [9]).

W3C

MAP NDCG@100 MRR

P@5 P@10 MAP

2005

2006
NDCG@100 MRR

CERC

MAP NDCG@100 MRR

P@5 P@10 MAP

2008
NDCG@100 MRR

0.135
LSI
0.243
TF-IDF
0.192
Model 1 (d)
0.190
Model 1 (jm)
0.198
Model 2 (d)
0.211
Model 2 (jm)
Log-linear (ours) 0.248

0.031
LSI
0.332
TF-IDF
0.287
Model 1 (d)
0.278
Model 1 (jm)
0.352
Model 2 (d)
0.361
Model 2 (jm)
Log-linear (ours) 0.344

0.095
LSI
0.216
TF-IDF
0.171
Model 1 (d)
0.189
Model 1 (jm)
0.154
Model 2 (d)
0.234
Model 2 (jm)
Log-linear (ours) 0.219

0.266
0.426
0.358
0.352
0.369
0.380
0.444

0.107
0.486
0.427
0.420
0.495
0.500
0.493

0.205
0.356
0.308
0.325
0.284
0.370
0.356

0.192 0.196
0.306
0.384 0.350
0.541
0.276 0.266
0.433
0.272 0.276
0.390
0.288 0.272
0.429
0.451
0.332 0.296
0.618∗ 0.412 0.361

2007

0.060
0.463
0.384
0.384
0.454
0.467
0.513

GT1

0.016 0.014
0.196 0.141
0.156 0.096
0.156 0.084
0.180 0.138
0.192 0.138
0.215 0.150

0.153
0.324
0.258
0.277
0.228
0.342
0.351

0.060 0.051
0.131 0.097
0.103 0.082
0.112 0.085
0.087 0.070
0.136 0.101
0.145∗ 0.105

0.245
0.343
0.321
0.311
0.261
0.260
0.484∗∗∗

0.038
0.269
0.181
0.170
0.264
0.274
0.342∗∗∗

0.097
0.233
0.241
0.231
0.191
0.253
0.287∗∗∗

0.371
0.531
0.491
0.483
0.419
0.423
0.667∗∗∗

0.482
0.650
0.635
0.596
0.551
0.599
0.833∗∗∗

0.099
0.465
0.355
0.347
0.461
0.463
0.519∗∗

0.106
0.525
0.388
0.339
0.510
0.517
0.656∗∗

0.208
0.378
0.385
0.373
0.334
0.394
0.425∗∗∗

0.129
0.288
0.292
0.271
0.233
0.302
0.363∗∗∗

TU

MAP NDCG@100 MRR

P@5 P@10 MAP

GT5
NDCG@100 MRR

P@5

P@10

0.287
0.492
0.478
0.502
0.441
0.449
0.713∗∗∗

0.338
0.498
0.449
0.437
0.404
0.429
0.644∗∗∗

P@5

0.042
0.332
0.200
0.181
0.281
0.278
0.381∗

P@10

0.055
0.277
0.172
0.159
0.244
0.239
0.299

P@5

P@10

0.043
0.108
0.109
0.100
0.084
0.108
0.134∗∗∗

0.036
0.079
0.081
0.075
0.065
0.081
0.092∗∗∗

ics where exact matches (Model 2) perform best, followed by ex-
amples which beneﬁt from semantic matching (log-linear model).
Topic identiﬁers are between parentheses.

W3C Topics P3P speciﬁcation and CSS3 (EX8 and EX69, respec-
tively) should return candidates associated with the deﬁnition
of these standards. The log-linear model, however, considers
these close to related technologies such as CSS2 for CSS3
and UTF-8 for P3P. Semantic matching works for topics Se-
mantic Web Coordination and Annotea server protocol (EX1
and EX103), where the former is associated with RDF li-
braries, RDF-related jargon and the names of researchers in
the ﬁeld, while the latter is associated with implementations
of the protocol and the maintainer of the project.

CERC For CSIRO, topic nanohouse (CE-035) is mentioned in
many irrelevant contexts (i.e., spam) and therefore seman-
tic matching fails. The term ﬁsh oil (CE-126) is quickly as-
sociated with different kinds of ﬁsh, oils and organizations
related to marines and ﬁsheries. On the other hand, we ob-
serve sensor networks (CE-018) to be associated with sen-
sor/networking jargon and sensor platforms. Topic forensic
science workshop (CE-103) expands to syntactically-similar
terms (e.g., plural), the names of science laboratories and ref-
erences

support/law-protection

organizations.

to

TU The TU benchmark contains both English and Dutch textual
evidence. Topics sustainable tourism and interpolation (1411
and 4882) do not beneﬁt from semantic matching due to a
semantic gap: interpolation is associated with the polyno-
mial kind while the relevance assessments focus on stochas-
tic methods. Interestingly, for the topic law and informati-
zation/computerization (1719) we see that the Dutch trans-
lation of law is very closely related. Similar terms to in-
formatization are, according to the log-linear model, Dutch
words related to cryptography. Similar dynamics are at work
for legal-political space (12603), where translated terms and
semantic-syntactic relations aid performance.

In order to further quantify the effect of the embedding matrix Wp,
we artiﬁcially expand benchmark topic terms by k nearby terms.
We then examine how the performance of a proﬁle-centric gener-
ative language model [4, Model 1] evolves for different values of
k (Figure 4). The purpose of this analysis is to provide further
insight in the differences between maximum-likelihood language
models and the log-linear model. Figure 4 shows that, for most
benchmarks, MAP increases as k goes up. Interestingly enough,
the two benchmarks that exhibit a decrease in MAP for larger k
(CERC 2007 and TU GT1) are likewise those for which generative
language models outperform the log-linear model in Table 2. This
suggests that the CERC 2007 and TU GT1 benchmarks require ex-

(a) W3C

(b) CERC

(c) TU

(d) W3C

(e) CERC

(f) TU

Figure 1: Sensitivity analysis for window size (n-gram) during parameter estimation (3) for W3C, CERC and TU benchmarks.

act term matching, while the remaining four benchmarks beneﬁt
greatly from the semantic matching provided by our model.

The per-topic differences suggest that Model 2 and the log-linear
model make very different errors: Model 2 excels at retrieving ex-
act query matches, while the log-linear model performs seman-
tic matching. Based on these observations we hypothesize that a
combination of the two approaches will raise retrieval performance
even further. To test this hypothesis, we propose a simple ensem-
ble of rankings generated by Model 2 and the log-linear model by
re-ranking candidates according to the multiplicatively-combined
reciprocal rank:

rankensemble(cj, qi) ∝

1
rankmodel 2(cj, qi)

·

1
ranklog-linear(cj, qi)

, (6)

where rankM (cj, qi) denotes the position of candidate cj in a rank-
ing generated by model M for answering query qi. Equation (6) is
equivalent to performing data fusion using CombSUM [57] where
the scores are given by the logarithm of the reciprocal ranks of the
experts. Table 3 compares the result of this ensemble to that of its
constituents. Compared to the supervised methods of Fang et al.
[23], we conclude that our fully unsupervised ensemble matches
the performance of their method on the CERC 2007 benchmark
and outperforms their method on the W3C 2005 benchmark. The
superior performance of the ensemble suggests the viability of hy-
brid methods that combine semantic and exact matching.

5.3 Scalability and efﬁciency

Inference in the log-linear model is expressed in linear algebra
operations (Section 3). These operations can be efﬁciently per-
formed by highly optimized software libraries and special-purpose
hardware (i.e., GPUs). But the baseline methods against which we
compare do not beneﬁt from these speed-ups. Furthermore, many
implementation-speciﬁc details and choice of parameter values can

inﬂuence runtime considerably (e.g. size of the latent representa-
tions). Therefore, we opt for a theoretical comparison of the infer-
ence complexity of the log-linear model and compare these to the
baselines (Section 4.3).

The log-linear model generates a ranking of candidate experts by
straight-forward matrix operations. The look-up operation in the
projection matrix Wp occurs in constant time complexity, as the
multiplication with the one-hot vector vi comes down to selecting
the i-th column from Wp. Multiplication of the |C| × e matrix
Wc with the e-dimensional word feature vector exhibits O(|C| · e)
runtime complexity. If we consider addition of the bias term and
division by the normalizing function Z1, the time complexity of (1)
becomes

O(|C| · (e + (e − 1))
(cid:125)
(cid:123)(cid:122)
(cid:124)
matrix-vector multiplication

+ |C|
(cid:124)(cid:123)(cid:122)(cid:125)
bias term

+ 2 · |C| − 1
(cid:123)(cid:122)
(cid:125)
Z1

(cid:124)

).

Notice, however, that the above analysis considers sequential exe-
cution. Modern computing hardware has the ability to parallelize
common matrix operations [24, 33]. The number of candidate ex-
perts |C| is the term that impacts performance most in the log-linear
model (under the assumption that |C| (cid:29) e).

If we consider n terms, where n is the query length during infer-
ence or the window size during training, then the complexity of (2)
becomes

O(n · |C| · (2 · e − 1) + n · (3 · |C| − 1)
(cid:125)

(cid:124)

(cid:123)(cid:122)
n forward-passes

+ (n − 1) · |C|
(cid:125)

(cid:123)(cid:122)
factor product

(cid:124)

)
+ 2 · |C| − 1
(cid:123)(cid:122)
(cid:125)
Z2

(cid:124)

Notice that Z2 does not need to be computed during inference as it
does not affect the candidate expert ranking.

(a) W3C (R = −0.39∗∗∗)

(b) CERC (R = −0.44∗∗∗)

(c) TU (R = −0.30∗∗∗)

Figure 2: Scatter plot of the normalized entropy of distribution P (c | q) (2) returned by the log-linear model and per-query av-
erage precision for W3C, CERC and TU benchmarks. Pearson’s R and associated p-value (two-tailed paired permutation test:
∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) are between parentheses. The depicted linear ﬁt was obtained using an ordinary least squares
regression.

Table 3: Comparison of Model 2, the log-linear model and an ensemble of the former on W3C, CERC and TU benchmarks. Signiﬁ-
cance of results is determined using a two-tailed paired randomization test [59] (∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1) with respect to
the ensemble ranking (adjusted using the Benjamini-Hochberg procedure for multiple testing [9]).

2005
NDCG@100 MRR

0.380
0.444
0.479∗∗

0.451
0.618
0.668

2007
NDCG@100 MRR

0.500
0.493
0.589∗∗∗

0.467
0.513
0.627∗∗∗

GT1
NDCG@100 MRR

W3C

Model 2 (jm)
Log-linear (ours)
Ensemble

CERC

Model 2 (jm)
Log-linear (ours)
Ensemble

TU

Model 2 (jm)
Log-linear (ours)
Ensemble

MAP

0.211
0.248
0.291∗∗∗

MAP

0.361
0.344
0.452∗∗

MAP

0.234
0.219
0.271∗∗∗

P@5

0.332
0.412
0.440

P@5

0.192
0.215
0.248∗

P@10

0.296
0.361
0.378

P@10

0.138
0.150
0.160

MAP

0.260
0.484∗∗∗
0.433

MAP

0.274
0.342
0.395∗∗∗

2006
NDCG@100 MRR

0.423
0.667∗∗
0.634

0.599
0.833
0.825

2008
NDCG@100 MRR

0.463
0.519
0.593∗∗∗

0.517
0.656
0.716

GT5
NDCG@100 MRR

P@5

P@10

0.449
0.713∗∗
0.657

0.429
0.644∗∗
0.586

P@5

P@10

0.278
0.381
0.459∗∗

0.239
0.299
0.357∗∗∗

P@5

P@10

MAP

P@5

P@10

0.370
0.356
0.417∗∗∗

0.342
0.351
0.403∗∗∗

0.136
0.145
0.165∗∗∗

0.101
0.105
0.121∗∗∗

0.253
0.287
0.331∗∗∗

0.394
0.425
0.477∗∗∗

0.302
0.363
0.402∗∗∗

0.108
0.134
0.156∗∗∗

0.081
0.092
0.105∗∗∗

In terms of space complexity, parameters Wp, Wc and bc, in ad-
dition to the intermediate results, all require memory space propor-
tional to their size. Considering (2) for a sequence of k words and
batches of m instances, we require O(m·k·|C|) ﬂoating point num-
bers for every forward-pass to ﬁt in-memory. While such an upper
bound seems reasonable by modern computing standards, it is a
severely limiting factor when considering large-scale communities
and while utilizing limited-memory GPUs for fast computation.

The inferential complexity of the vector space-based models for
entity retrieval [19] depends mainly on the dimensionality of the
vectors and the number of candidate experts. The dimensionality
of the latent entity representations is too high for efﬁcient nearest
neighbor retrieval [29] due to the curse of dimensionality. There-
fore, the time complexity for the LSI- and TF-IDF-based vector
space models are respectively O(γ · |C|) and O(|V | · |C|), where
γ denotes the number of latent topics in the LSI-based method. As
hyperparameters e and γ both indicate the dimensionality of la-
tent entity representations, the time complexity of the LSI-based
method is comparable to that of the log-linear model. We note that
|V | (cid:29) |C| for all benchmarks (|V | is between 18 to 91 times larger
than |C|) we consider in this paper and therefore conclude that the
TF-IDF method loses to the log-linear model in terms of efﬁciency.

Compared to the unsupervised generative models of Balog et al.,
we have the proﬁle-centric Model 1 and the document-centric Mo-
del 2 with inference time complexity O(n · |C|) and O(n · |D|),
respectively, with |D| (cid:29) |C|. In the previous section we showed
that the log-linear model always performs better than Model 1 and
nearly always outperforms Model 2. Hence, our log-linear model
generally achieves the expertise retrieval performance of Model 2
(or higher) at the complexity cost of Model 1 during inference.

5.4

Incremental indexing

Existing unsupervised methods use well-understood maximum-
likelihood language models that support incremental indexing. We
now brieﬂy discuss the incremental indexing capabilities of our
proposed method. Extending the set of candidate experts C re-
quires the log-linear model to be re-trained from scratch as it changes
the topology of the network. Moreover, every document associated
with a candidate expert is considered as a negative example for all
other candidates. While it is possible to reiterate over all past doc-
uments and only learn an additional row in matrix Wc, the ﬁnal
outcome is unpredictable.

If we consider a stream of documents instead of a predeﬁned set
D, the log-linear model can be learned in an online fashion. How-
ever, stochastic gradient descent requires that training examples are

(a) W3C 2005

(b) CERC 2007

(c) TU GT1

(d) W3C 2006

(e) CERC 2008

(f) TU GT5

Figure 3: Difference of average precision between log-linear model and Model 2 [4] with Jelinek-Mercer smoothing per topic for
W3C, CERC and TU benchmarks.

picked at random such that the batched update rule (4) behaves like
the empirical expectation over the full training set [11]. While we
might be able to justify the assumption that documents arrive ran-
domly, the n-grams extracted from those documents clearly violate
this requirement.

Considering a stream of documents leads to the model forgetting
expertise evidence as an (artiﬁcial) shift in the underlying distribu-
tion of the training data occurs. While such behavior is undesirable
for the task considered in this paper, it might be well-suited for
temporal expert ﬁnding [22, 52], where expertise drift over time
is considered. However, temporal expertise ﬁnding is beyond the
scope for this paper and left for future work.

6. CONCLUSIONS

We have introduced an unsupervised discriminative, log-linear
model for the expert retrieval task. Our approach exclusively em-
ploys raw textual evidence. Future work can focus on improving
performance by feature engineering and incorporation of external
evidence. Furthermore, no relevance feedback is required during
training. This renders the model suitable for a broad range of ap-
plications and domains.

We evaluated our model on the W3C, CERC and TU bench-
marks and compared it to state-of-the-art vector space-based en-
tity ranking (based on LSI and TF-IDF) and language modeling
(proﬁle-centric and document-centric) approaches. The log-linear
model combines the ranking performance of the best maximum-
likelihood language modeling approach (document-centric) with
inference time complexity linear in the number of candidate ex-
perts. We observed a notable increase in precision over existing
methods. Analysis of our model’s output reveals a negative corre-
lation between the per-query performance and ranking uncertainty:
higher conﬁdence (i.e., lower entropy) in the rankings produced by
our approach often occurs together with higher rank quality.

An error analysis of the log-linear model and traditional lan-
guage models shows that the two make very different errors. These
errors are mainly due to the semantic gap between query intent and
the raw textual evidence. Some benchmarks expect exact query
matches, others are helped by our semantic matching. An ensemble
of methods employing exact and semantic matching generally out-
performs the individual methods. This observation calls for further
research in the area of combining exact and semantic matching.

One current limitation of our work is its scalability with respect
to the number of candidate experts. We have started investigating
trade-offs between model performance and time/space complexity.
In the future we hope to apply scalable variants of this method on
large-scale social media communities, for the purpose of determin-
ing topic ownership. While in this work we focus on expertise re-
trieval, the ideas we proposed can easily be transferred to the more
general entity retrieval task. Moreover, our approach is likely to
be applicable to authorship attribution and various other entity re-
trieval and prediction tasks.

Acknowledgments. We thank Isaac Sijaranamual, Manos Tsagkias, Tom
Kenter, Zhaochun Ren, Ke Tran and Katja Hoffman for their useful com-
ments and insights.

This research was supported by Amsterdam Data Science, the Dutch
national program COMMIT, Elsevier, the European Community’s Seventh
Framework Programme (FP7/2007-2013) under grant agreement nr 312827
(VOX-Pol), the ESF Research Network Program ELIAS, the Royal Dutch
Academy of Sciences (KNAW) under the Elite Network Shifts project, the
Microsoft Research Ph.D. program, the Netherlands eScience Center un-
der project number 027.012.105, the Netherlands Institute for Sound and
Vision, the Netherlands Organisation for Scientiﬁc Research (NWO) under
project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.-
930, CI-14-25, SH-322-15, 652.002.001, 612.001.551, the Yahoo Faculty
Research and Engagement Program, and Yandex. All content represents the
opinion of the authors, which is not necessarily shared or endorsed by their
respective employers and/or sponsors.

Computing resources were provided by the Netherlands Organisation for
Scientiﬁc Research (NWO) through allocation SH-322-15 of the Cartesius
system and by the Advanced School for Computing and Imaging (ASCII)
by allocation of the Distributed ASCII Supercomputer 4 (DAS-4) system.

(a) W3C

(b) CERC

(c) TU

Figure 4: Effect of query expansion by adding nearby terms in Wp (1) in traditional language models (Model 1 [4] with Jelinek-
Mercer smoothing) for W3C, CERC and TU benchmarks.

APPENDIX
The derivative of (3) w.r.t. bias term bc equals

∂L(Wp, Wc, bc)
∂bc

= −

1
m

(cid:32) m
(cid:88)

i=1

|dmax|
|d(i)|

|C|
(cid:88)

j=1

P (cj|d(i))

(cid:16)

∂ log

P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

(cid:33)

and w.r.t. an arbitrary matrix parameter θ (Wp or Wc):

∂L(Wp, Wc, bc)
∂θ

= −

(cid:32) m
(cid:88)

i=1

|dmax|
|d(i)|

|C|
(cid:88)

j=1

P (cj|d(i))

(cid:16)

∂ log

P (cj | w(i)

1 , . . . , w(i)
n )

(cid:17)

(cid:33)

1
m

λ
m

+

θi,j.

(cid:88)

i,j

Further differentiation for parameter θ (Wp, Wc or bc):

∂bc

∂θ

∂ log (P (cj | w1, . . . , wn))
∂θ

∂P (cj | w1, . . . , wn)
∂θ

1
P (cj | w1, . . . , wn)
∂ ˜P (cj |w1,...,wn)
∂θ

∂P (cj | w1, . . . , wn)
∂θ
Z2 − ˜P (cj | w1, . . . , wn) ∂Z2
∂θ

∂Z2
∂θ

Z 2
2
∂ ˜P (ck | w1, . . . , wn)
∂θ

∂ ˜P (cj | w1, . . . , wn)
∂θ

∂P (cj | wk)
∂θ

(cid:89)

i(cid:54)=k

P (cj | wi)

=

=

=

=

(cid:88)

(cid:88)

k

k

For a given candidate cj and word wi, following (1) we have

P (cj | wi) =

˜P (cj | wi)
Z1
exp (cid:0)(cid:0)(cid:80)e
l=1 exp (cid:0)(cid:0)(cid:80)e

(cid:80)|C|

=

k=1 Wcj,k Wpk,i

k=1 Wcl,k Wpk,i

(cid:1) + bcj

(cid:1)
(cid:1) + bcl

(cid:1)

and consequently, with W (cid:62)
Wp,

pi denoting the i-th column of matrix

∂P (cj | wi)
∂Wcj

∂P (cj | wi)
∂bcj

∂P (cj | wi)
∂W (cid:62)
pi

=

=

=

(cid:17) ˜P (cj | wi)W (cid:62)
(cid:16)
Z1 − ˜P (cj | wi)
Z 2
1
(cid:17) ˜P (cj | wi)
(cid:16)
Z1 − ˜P (cj | wi)
Z 2
1

pi

(cid:16)
Wcj − (cid:80)|C|

(cid:17) ˜P (cj | wi)

l=1 Wcl
Z1

(7)

As can be seen in (7), the distributed representations of candidates
cj at time t + 1 are updated using the representation of words wi at
time t and vice versa.

REFERENCES
[1] The knowledge-based economy. Techn. report, Organisation for

Economic Co-operation and Development, 1996.

[2] P. Bailey, A. P. De Vries, N. Craswell, and I. Soboroff. Overview

of the TREC 2007 enterprise track. In TREC, 2007.
[3] K. Balog. People Search in the Enterprise. PhD thesis,

University of Amsterdam, 2008.

[4] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for
expert ﬁnding in enterprise corpora. In SIGIR, pages 43–50,
2006.

[5] K. Balog, L. Azzopardi, and M. de Rijke. A language modeling

framework for expert ﬁnding. IPM, 45:1–19, 2009.
[6] K. Balog, Y. Fang, M. de Rijke, P. Serdyukov, and L. Si.

Expertise retrieval. Found. & Tr. in Information Retrieval, 6
(2-3):127–256, 2012.

[7] I. Becerra-Fernandez. Role of artiﬁcial intelligence technologies
in the implementation of People-Finder knowledge management
systems. Knowledge-Based Systems, 13(5):315–320, 2000.
[8] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural
probabilistic language model. JMLR, 3:1137–1155, 2003.
[9] Y. Benjamini and Y. Hochberg. Controlling the false discovery
rate: a practical and powerful approach to multiple testing.
JSTOR, pages 289–300, 1995.

[10] R. Berendsen, M. de Rijke, K. Balog, T. Bogers, and A. van den
Bosch. On the assessment of expertise proﬁles. JASIST, 64(10):
2024–2044, 2013.

[11] L. Bottou. Large-scale machine learning with stochastic gradient
descent. In COMPSTAT, pages 177–186. Springer, 2010.
[12] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,

N. Hamilton, and G. Hullender. Learning to rank using gradient
descent. In ICML, pages 89–96, 2005.

[13] Y. Cao, J. Liu, S. Bao, and H. Li. Research on Expert Search at

Enterprise Track of TREC 2005. In TREC, pages 2–5, 2005.
[14] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,
and P. Kuksa. Natural language processing (almost) from
scratch. JMLR, 12(Aug):2493–2537, 2011.

[15] N. Craswell, D. Hawking, A.-M. Vercoustre, and P. Wilkins.

P@noptic expert: Searching for experts not just for documents.
In Ausweb Poster Proceedings, pages 21–25, 2001.

[16] N. Craswell, A. P. de Vries, and I. Soboroff. Overview of the

TREC 2005 enterprise track. In TREC, 2005.

[17] T. H. Davenport and L. Prusak. Working Knowledge. Harvard

Business Review Press, 1998.

[18] S. C. Deerwester, S. T. Dumais, and R. A. Harshman. Indexing

by latent semantic analysis. JASIS, 1990.

[19] G. Demartini, J. Gaugaz, and W. Nejdl. A vector space model

[41] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Distributed

for ranking entities and its application to expert search. In ECIR,
pages 189–201. Springer, 2009.

representations of words and phrases and their compositionality.
In NIPS, pages 3111–3119, 2013.

[20] L. Deng, X. He, and J. Gao. Deep stacking networks for

information retrieval. In ICASSP, pages 3153–3157, 2013.
[21] H. Fang and C. Zhai. Probabilistic models for expert ﬁnding. In

ECIR, pages 418–430, Berlin, Heidelberg, 2007.
Springer-Verlag.

[22] Y. Fang and A. Godavarthy. Modeling the dynamics of personal

expertise. In SIGIR, pages 1107–1110, 2014.

[23] Y. Fang, L. Si, and A. P. Mathur. Discriminative models of

integrating document evidence and document-candidate
associations for expert search. In SIGIR, pages 683–690, 2010.

[24] K. Fatahalian, J. Sugerman, and P. Hanrahan. Understanding the
efﬁciency of gpu algorithms for matrix-matrix multiplication. In
SIGGRAPH HWWS, pages 133–137. ACM, 2004.

[25] X. Glorot and Y. Bengio. Understanding the difﬁculty of training
deep feedforward neural networks. In AISTATS, pages 249–256,
2010.

[26] G. E. Hinton. Learning distributed representations of concepts.
In 8th Annual Conference of the Cognitive Science Society,
volume 1, page 12, Amherst, MA, 1986.

[27] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR,

pages 50–57. ACM, 1999.

[28] P.-s. Huang, N. M. A. Urbana, X. He, J. Gao, L. Deng, A. Acero,
and L. Heck. Learning deep structured semantic models for web
search using clickthrough data. In CIKM, pages 2333–2338,
2013.

[29] P. Indyk and R. Motwani. Approximate nearest neighbors:

towards removing the curse of dimensionality. In STOC, pages
604–613. ACM, 1998.

[30] M. Karimzadehgan and C. Zhai. Estimation of statistical

translation models based on mutual information for ad hoc
information retrieval. In SIGIR, pages 323–330. ACM, 2010.
[31] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural

language models. In ICML, pages 595–603, 2014.

[32] J. Kruger and D. Dunning. Unskilled and unaware of it: how

difﬁculties in recognizing one’s own incompetence lead to
inﬂated self-assessments. J. Personality and Social Psych., 77
(6):1121, 1999.

[33] J. Krüger and R. Westermann. Linear algebra operators for gpu
implementation of numerical algorithms. ACM Transactions on
Graphics, 22(3):908–916, 2003.

[34] H. Li and J. Xu. Semantic matching in search. Found. & Tr. in

Information Retrieval, 7(5):343–469, June 2014.

[35] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer,

2011.

[36] C. MacDonald and I. Ounis. Voting for candidates: adapting

data fusion techniques for an expert search task. In CIKM, pages
387–396, 2006.

[37] C. Macdonald and I. Ounis. Expert search evaluation by

supporting documents. In ECIR, pages 555–563. Springer, 2008.

[38] M. T. Maybury. Expert ﬁnding systems. Techn. Report

[39] D. W. McDonald and M. S. Ackerman. Expertise recommender.

MTR-06B000040, MITRE, 2006.

In CSCW, pages 231–240, 2000.

[42] T. Mikolov, G. Corrado, K. Chen, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
1301.3781, 2013.

[43] A. Mnih and G. Hinton. Three new graphical models for

statistical language modelling. In ICML, pages 641–648, 2007.

[44] A. Mnih and G. Hinton. A scalable hierarchical distributed
language model. In NIPS, pages 1081–1088, 2008.
[45] A. Mnih and K. Kavukcuoglu. Learning word embeddings

efﬁciently with noise-contrastive estimation. In NIPS, pages
2265–2273, 2013.

[46] G. Montavon, G. B. Orr, and K.-R. Müller. Neural Networks:

Tricks of the Trade. Springer, 2012.

[47] C. Moreira, B. Martins, and P. Calado. Using rank aggregation
for expert search in academic digital libraries. In Simpósio de
Informática, INForum, pages 1–10, 2011.

[48] J. Pennington, R. Socher, and C. D. Manning. GloVe: Global

Vectors for Word Representation. In EMNLP, pages 1532–1543,
2014.

[49] D. Petkova and W. B. Croft. Hierarchical language models for
expert ﬁnding in enterprise corpora. In ICTAI ’06, pages
599–606, 2006.

[50] W. W. Powell and K. Snellman. The knowledge economy.
Annual review of sociology, pages 199–220, 2004.

[51] D. Rumelhart, G. Hinton, and R. Williams. Learning internal
representations by back propagation. In Parallel Distributed
Processing, pages 318–362. MIT Press, 1986.

[52] J. Rybak, K. Balog, and K. Nørvåg. Temporal expertise
proﬁling. In ECIR, pages 540–546. Springer, 2014.
[53] R. Salakhutdinov and G. Hinton. Semantic hashing. Int. J.

Approximate Reasoning, 50(7):969–978, 2009.

[54] P. Serdyukov and D. Hiemstra. Modeling documents as mixtures
of persons for expert ﬁnding. In ECIR, pages 309–320. Springer,
2008.

[55] P. Serdyukov, H. Rode, and D. Hiemstra. Modeling multi-step
relevance propagation for expert ﬁnding. In CIKM, pages
1133–1142, 2008.

[56] C. Shannon. A mathematical theory of communication. Bell

System Technical J., 27:379–423, 623–656, 1948.

[57] J. A. Shaw, E. A. Fox, J. A. Shaw, and E. A. Fox. Combination

of multiple searches. In TREC, pages 243–252, 1994.
[58] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. A latent
semantic model with convolutional-pooling structure for
information retrieval. In CIKM, pages 101–110, 2014.
[59] M. D. Smucker, J. Allan, and B. Carterette. A comparison of

statistical signiﬁcance tests for information retrieval evaluation.
In CIKM, pages 623–632. ACM, 2007.

[60] P. Sorg and P. Cimiano. Finding the right expert: Discriminative
models for expert retrieval. In KDIR, pages 190–199, 2011.

[61] TREC. Enterprise Track, 2005–2008.
[62] D. van Dijk, M. Tsagkias, and M. de Rijke. Early detection of
topical expertise in community question and answering. In
SIGIR, 2015.

[63] V. Vapnik. Statistical learning theory, volume 1. Wiley New

[40] T. Mikolov, M. Karaﬁat, L. Burget, J. Cernocky, and

York, 1998.

S. Khudanpur. Recurrent neural network based language model.
In Interspeech, pages 1045–1048, 2010.

[64] M. D. Zeiler. Adadelta: An adaptive learning rate method.

CoRR, abs/1212.5701, 2012.

