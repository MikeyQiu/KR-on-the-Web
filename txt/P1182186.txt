Date of publication 10, 2018 , date of current version 10, 2018.

Digital Object Identiﬁer 10.1109/ACCESS.2018.2875677

Patient2Vec: A Personalized
Interpretable Deep Representation of the
Longitudinal Electronic Health Record

JINGHE ZHANG1, (Member, IEEE), KAMRAN KOWSARI1,4(Member, IEEE), JAMES H.
HARRISON2,3,5, JENNIFER M. LOBO2,5, and LAURA E. BARNES1,4,5, (Member, IEEE)
1Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, USA
2 Department of Public Health Sciences, University of Virginia, Charlottesville, VA 22904, USA
4 Sensing Systems for Health Lab, University of Virginia, Charlottesville, VA 22904, USA
3 Division of Laboratory Medicine Department of Pathology, University of Virginia, Charlottesville, VA 22904, USA
5Data Science Institute, University of Virginia, Charlottesville, VA 22904, USA
Corresponding author: Laura E. Barnes (lb3dp@virginia.edu)
This research was supported by a Jeffress Trust Award in Interdisciplinary Science.
Patient2Vec is shared as an open source tool at https://github.com/BarnesLab/Patient2Vec

ABSTRACT The wide implementation of electronic health record (EHR) systems facilitates the collection
of large-scale health data from real clinical settings. Despite the signiﬁcant increase in adoption of EHR
systems, this data remains largely unexplored, but presents a rich data source for knowledge discovery from
patient health histories in tasks such as understanding disease correlations and predicting health outcomes.
However, the heterogeneity, sparsity, noise, and bias in this data present many complex challenges.
This complexity makes it difﬁcult to translate potentially relevant information into machine learning
algorithms. In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep
representation of longitudinal EHR data which is personalized for each patient. To evaluate this approach,
we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive
performance with baseline methods. Patient2Vec produces a vector space with meaningful structure and it
achieves an AUC around 0.799 outperforming baseline methods. In the end, the learned feature importance
can be visualized and interpreted at both the individual and population levels to bring clinical insights.

INDEX TERMS Attention mechanism, gated recurrent unit, hospitalization, longitudinal electronic health
record, personalization, representation learning.

8
1
0
2
 
t
c
O
 
5
2
 
 
]

M
Q
.
o
i
b
-
q
[
 
 
3
v
3
9
7
4
0
.
0
1
8
1
:
v
i
X
r
a

I. INTRODUCTION

L ongitudinal EHR data resemble text documents from

many perspectives. A text document consists of a se-
quence of sentences, and a sentence is a sequence of words.
Similarly, the longitudinal health record of a patient consists
of a sequence of visits, and there is a list of clinical events,
including diagnoses, medications, and procedures, that occur
during a visit. Considering these similarities, representation
learning methods for text documents in Natural Language
Processing (NLP) have great potential to be applied to lon-
gitudinal EHR data.

Deep neural networks have become very popular in the
NLP ﬁeld and have been very successful in many applica-
tions, such as machine translation, question answering, text
classiﬁcation, document summarization, language modeling,
etc. [1]–[8]. These networks excel at complex language tasks

because they are capable of identifying high-order relation-
ships, the network structure can encode language structures,
and they allow the learning of a hierarchical representation
of the language, i.e., representations for tokens, phrases, and
sentences, etc.

Among a variety of deep learning methods, Recurrent
Neural Networks (RNNs) have shown their effectiveness in
NLP tasks because they have the ability to capture sequential
information [7]–[10] which is inherent in human language.
Traditional neural networks assume that inputs are inde-
pendent of each other, while an RNN computes the output
based on the current input as well as the “memory” from the
previous computation. Although vanilla RNNs are not good
at capturing long-term dependencies, many variants have
been proposed and validated that are effective in addressing

VOLUME 0, 2018

1

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

its performance with other baseline methods. In addition
to prediction performance, we further interpret the learned
representations with visualizations on example patients and
events. Finally, Section V provides a summary of this work.

II. RELATED WORK
In this section, we present an overview of a gated recurrent
unit, a type of RNN, which is capable of capturing long-term
dependencies. Then we brieﬂy introduce attention mecha-
nisms in neural networks that allow the network to attend
to certain regions of data, which is inspired by the visual
attention mechanism in humans. Additionally, we summarize
the RNN networks and attention mechanisms previously used
to mine EHR data.

A. RECURRENT NEURAL NETWORKS (RNN)

RNNs are expected to learn long-term dependencies by tak-
ing the previous state and the new input in the computation at
the current time step t. However, vanilla RNNs are incapable
of capturing the dependencies when the sequence is very long
due to the vanishing gradient problem [12]. Many variants of
the RNN network have been proposed to address this issue,
and long short term memory (LSTM) is one of the most
popular models used nowadays in NLP tasks [7], [8], [13]–
[16].

this issue.

In the medical domain, it is critical that analytical results
are interpretable, so that they can be understood and validated
by a human with expert knowledge and so that knowledge
captured by analysis can be used for process improvement.
Traditional deep neural networks have the disadvantage that
they lack interpretability. A substantial amount of work is
ongoing to make sense of the “black box”, and the attention
mechanism [11] is one of the more effective methods recently
developed to make the output of these algorithms more
interpretable.

Health care is undergoing unprecedented change, and there
is a great potential and demand for personalized care strate-
gies. Personalized medicine, also called precision medicine,
has previously focused on optimizing therapy to better ﬁt the
genetic makeup of the patient or the disease (e.g., the genetic
susceptibility of cancer to speciﬁc chemotherapy strategies).
The availability of EHR data and advances in machine learn-
ing create the potential for another type of personalization
of healthcare. This type of personalization has become ubiq-
uitous in our daily life. For example, customers have come
to expect personalized search on Google and personalized
product recommendations on Amazon and Netﬂix, based on
their charactersitics and previous experiences with the sys-
tems. Personalization of healthcare processes, based on a pa-
tient’s phenotype (physical and medical characteristics) and
healthcare experiences as documented in the health record,
may also improve "customer" satisfaction and it has the addi-
tional potential to improve healthcare efﬁciency, lower costs,
and yield better outcomes. We believe that representation
learning methods can capture a personalized representation
of the important heterogeneities in patients’ phenotypes and
medical histories at the population-level, and make these
representations available to drive healthcare decisions and
strategies.

This research is based on RNN models and the attention
mechanism with the objective of learning a personalized, in-
terpretable, and complete representation of patients’ medical
records. Our proposed framework is capable of learning a
personalized representation for each patient from a sequence
of clinical events. A hierarchical attention mechanism learns
personalized weights of clinical events, including hospital
visits and the procedures that they contain. These weights
allow us to interpret the relative importance and roles of clin-
ical events in the learned representations both at individual
and population levels. The ultimate goal is more accurate
prediction and better insight into the critical elements of
healthcare processes that can be used to improve healthcare
delivery.

The rest of this paper is organized as follows: Section II
summarizes the variants of RNNs and the attention mecha-
nism, as well as their application to EHR data. Section III
presents an overview of the proposed Patient2Vec repre-
sentation learning framework, and Section IV elaborates
the details of the algorithms. In Section V, the proposed
framework is evaluated for a prediction task and we compare

FIGURE 1. The top ﬁgure is a GRU gating unit and bottom ﬁgure shows an
LSTM unit [7]

2

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

1) Gated Recurrent Unit (GRU)
GRU is a simpliﬁed version of LSTM [7]. The basic idea
of GRU is to combat the vanishing gradient problem with a
gating mechanism. Hence the general recurrent structure in
GRU is identical to vanilla RNNs except that a GRU unit
is used in the computation at each time step rather than a
traditional simple recurrent unit.

In general, a GRU cell has two gates, i.e., a reset gate r
and an update gate z. The reset gate is used to determine
how to integrate the previous state into the computation of
the current state, while the update gate determines how much
the unit updates its activation.

Given the input xt at time step t, the reset gate rt is

computed as presented in Equation 1

rt = σ(Urxt + Wrst−1)

(1)

where Ur and Wr are the weight matrices of the reset gate
and st−1 is the hidden activation at time step t − 1. A similar
computation is performed for the update gate zt at time step t,
shown in Equation 2

zt = σ(Uzxt + Wzst−1)

(2)

where Uz and Wz are the weight matrices of update gate.
The current hidden activation ht is computed by

ht = (1 − zt)ht−1 + zt
(3)
where ˜ht is the candidate activation at time step t. The
computation of ˜ht is presented in Equation 4

˜ht

˜ht = tanh(Wxt + U(rt (cid:12) ht−1))

(4)

where U and W are weight matrices and (cid:12) represents
element-wise multiplication. Figure 1 presents a graphical
illustration of the GRU [7] and one unit of LSTM.

GRU is capable of learning long-term dependencies [17]
due to the additive component of update from t to t + 1
in the gating mechanism. Consequently, important features
will be carried forward in the input stream while irrelevant
information will be dropped. When the reset gate is 0, the
network is forced to drop previous states and reset with cur-
rent information. Moreover, the method provides shortcuts
such that the error is easily backpropagated without vanishing
too quickly [5], [18]. Hence, the GRU is well-suited to learn
long-term dependencies in sequence data.

2) Long Short-Term Memory (LSTM)
An LSTM unit is similar to a GRU, but with one more gate in
an LSTM unit (as shown in Figure 1). LSTM also preserves
long term dependencies more effectively than basic RNN.
This is particularly useful to overcome the vanishing gradient
problem [19]. Although LSTM has a chain-like structure sim-
ilar to RNN, LSTM uses multiple gates to carefully regulate
the amount of information that will be allowed into each node
state. Figure 1 shows the basic cell of an LSTM model. A step

VOLUME 0, 2018

by step explanation of an LSTM cell is as following:
Input gate:

it = σ(Wi[xt, ht−1] + bi),

Candid memory cell value:

˜Ct = tanh(Wc[xt, ht−1] + bc),

Forget gate activation:

ft = σ(Wf [xt, ht−1] + bf ),

New memory cell value:

Ct = it ∗ ˜Ct + ftCt−1,

Output gate value:

ot = σ(Wo[xt, ht−1] + bo),

ht = ot tanh(Ct),

(5)

(6)

(7)

(8)

(9)

(10)

In the above description all b represent bias vectors, all W
represent weight matrices, and xt is used as input to the
memory cell at time t. Also,the i, c, f, o indices refer to input,
cell memory, forget and output gates respectively. An RNN
can be biased when later words are more inﬂuential than the
earlier ones.

Empirically, LSTM and GRU achieve comparable per-
formance in many tasks but there are fewer parameters in
a GRU, which makes it a little faster to learn and able to
generalize with fewer data [20].

B. ATTENTION MECHANISM
Attention mechanisms, inspired by the visual attention sys-
tem found in humans, have become popular in deep learning.
Attention allows the network to focus on certain regions of
data, while perceiving other regions with “low resolution”.
In addition to higher accuracy, it also facilitates the interpre-
tation of learned representations. We elaborate an attention
mechanism on an RNN network, and Figure 2 presents a
graphical illustration.

According to Figure 2, a variable-length weight vector α
is learned based on hidden states [11]. Then a global context
vector is computed based on weights α and all the hidden
states to create the ﬁnal output. Equation 11 presents the
computation of the weight vector α = {α1, α2, · · · , αT },
where T is the length of the sequence

α1, α2, · · · , αT = f (Wαh + bα)

(11)

and where f is a nonlinear activation function, usually
sof tmax or tanh. Then, the context vector c is constructed
as:

c =

αtht

T
(cid:88)

t=1

Thus, the network puts more attention on the important
features for the ﬁnal prediction which can improve the model
performance. An additional beneﬁt is that the weights can

(12)

3

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

GRU-based model to address missing values in multivariate
time series data, in which the missing patterns are incorpo-
rated for improved prediction performance. This work has
been applied to the Medical Information Mart for Intensive
Care III (MIMIC-III) clinical database to demonstrate its
effectiveness in mining time series of clinical measurements
with missing values [27]. Longitudinal EHR data including
clinical events, such as diagnoses, medications, and pro-
cedures is also a potentially rich resource for predictive
modeling. Choi et al. [28] analyze this data with a GRU
network to forecast future clinical events, and it achieves a
better prediction performance than comparison models such
as logistic regression and MLP.

Difﬁculty in interpreting model behavior is one of the
major drawbacks of using deep learning to mine EHR data.
Some attempts have been made to address this issue. Che et
al. [29] propose an interpretable mimic learning method
which trains a mimic gradient boosting trees model to utilize
predicted labels or features learned by deep learning mod-
els for ﬁnal prediction [30]. Then the feature importances
learned by the tree-based models are used for knowledge
discovery. Attention mechanisms have been introduced re-
cently to improve the interpretability of the prediction results
of deep learning models in health analytics. Choi et al. [31]
develop an interpretable model with two levels of attention
weights learned from two reverse-time GRU models, re-
spectively. The experimental results on EHR data indicate
comparable prediction performance with conventional GRU
models but more interpretable results. Our work continues
the attempt to use attention mechanisms to improve the
interpretability of RNN-based models.

III. PATIENT2VEC SYSTEM MODEL
In this section, we provide an overview of the proposed
hierarchical representation learning framework. This frame-
work uses deep recurrent neural networks to capture the
complex relationships between clinical events in the patient’s
EHR data and employs the attention mechanism to learn
a personalized representation and to obtain relative feature
importance. The proposed representation learning framework
contains four steps and is presented graphically in Figure 3.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES

EHR data consists primarily of records of outpatient and
inpatient visits to healthcare providers. These visit records
include multiple clinical codes for diagnoses, symptoms,
procedures, therapies, and other observations and events that
occurred during the visit. Here, we treat the set of medical
codes associated with a visit as a sentence consisting of
words, except that there is no ordering in the words. Thus,
we adopt the word2vec approach to construct a vector to
represent each medical code.

FIGURE 2. The global attention model

be utilized to understand the importance of features such
that the models are more interpretable. The attention mech-
anism has been introduced to both Convolutional Neural
Networks (CNNs) and RNNs for various tasks and has
achieved many successes in the ﬁelds of computer vision and
NLP [11], [21], [22].

C. DEEP LEARNING IN EHR DATA
Previous studies on EHR data mainly use statistical meth-
ods or traditional machine learning techniques. Recently
researchers have started adapting deep learning approaches
to this data [23], [24], including textual notes, temporal
measurements of laboratory testing in the Intensive Care
Unit (ICU), and longitudinal data in patient populations.
Here, we summarize deep learning research in mining EHR
data and focus on the studies using RNN-based models.

Hospitalized patients, especially patients in ICUs, are
continuously monitored for cardiac, respiratory, and other
physical functions, creating a large volume of sequential data
in multiple dimensions. These measurements are utilized by
physicians to make diagnostic and treatment decisions. The
functions monitored may change over time and monitor-
ing may be irregular, based on a patient’s condition. It is
very challenging for traditional machine learning methods
to mine this multivariate time series data considering miss-
ing values, varying length, and irregular, non-simultaneous
sampling. Lipton et al. [25] trained an LSTM with a repli-
cated target to learn from these sequence data and used
this model to make predictions of diagnoses. The data used
in this research are time series of clinical measurements
with continuous values, and the LSTM models outperformed
logistic regression and MLP. Che et al. [26] developed a

4

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

are unlikely to contribute equally to the prediction of the tar-
get outcome, we cannot aggregate them with equal weights.
Instead, we employ a self-attention mechanism which trains
the network to learn the weights.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of subsequences with embedded medical
codes, we are able to input it into a recurrent neural network
to capture the temporal dependencies between events. How-
ever, the subsequences of visits are not contributing equally
to the outcome. Hence, we employ another level of attention
to learn the weights of the subsequences by the network itself
for the outcome prediction.

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the learned weights and hidden outputs, we aggregate
them into one universal vector for a comprehensive represen-
tation. In this step, the static information, such as age, gender,
previous hospitalization history is added as extra features, to
get a complete representation of a patient.

E. PREDICTING OUTCOME
Given the complete vector representation of a patient’s EHR
data, we add a logistic regression layer at the end for the
prediction of outcome.

IV. PATIENT2VEC REPRESENTATION LEARNING
ALGORITHM
In this section, we present the details of the proposed rep-
resentation learning framework, which is based on a GRU
network and a hierarchical attention mechanism. Figure 4
presents the structure of the proposed network with attention.

The proposed framework consists of ﬁve parts presented in
the following: I) Learning vector representations of medical
codes, II) Learning within-subsequence self-attention, III)
Learning subsequence-level self-attention, IV) Constructing
aggregated deep representation, V) Predicting outcome.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES
Given a patient’s raw EHR data, a sequence of visits, we
observe that a visit usually contains multiple medical codes.
Hence, it is feasible to learn a vector to represent the medical
code by capturing the relationships between the codes. In
this work, we employ the classical word2vec algorithm, skip-
gram. The basic idea of skip-gram is to learn a vector to
represent each word such that the probability of the context
to predict based on the target word is maximized. Hence,
the vectors of similar words are close to each other in the
learned feature space. In the skip-gram model, the vectors
are learned by training a shallow neural network to predict the
context words given an input word. Similarly, in our problem,

FIGURE 3. The Patient2Vec representation learning framework

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Clinical visits are represented as the set of vectors for the
codes associated with the visit. Because closely-spaced visits
are usually related clinically, we employ a time window to
split the sequence of visits into multiple subsequences of
equal length. A subsequence might contain multiple visits if
they occurred within the same time window, or there might
be no visits during a particular time window yielding an
empty subsequence. Thus we transform the original sequence
of irregularly-spaced visits into a sequence of subsequences
with equal intervals, which is preferable for recurrent neural
networks. The width of the subsequence window deﬁnes the
time granularity of the method and its optimal width is related
to the acuity (i.e., stability) of the clinical characteristics
involved in the predication task. In future work it may be
possible to deﬁne the relationship between clinical acuity and
optimal subsequence width, or develop methods for learning
an optimal width for a deﬁned prediction task.

Because all medical events occurring within a subsequence

VOLUME 0, 2018

5

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 4. A graphical illustration of the network in the Patient2Vec representation learning framework

the input is a medical code and the target to predict are the
medical codes occurred in the same visit.

Hence, each subsequence is a matrix consisting of the
vectors of medical codes occurred during this associated time
window.

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Given a sequence of subsequences encoded by vectors of
medical codes, this step employs the within-subsequence
attention which allows the network itself to learn the weights
of vectors in the subsequence according to its contribution to
the prediction target.

Here, we denote the sequence of patient i as s(i), and v(i)
t

t

, · · · , v(i)

1 , · · · , v(i)

denotes the tth subsequence in sequence s(i), where t ∈
{1, 2, · · · , T }. Thus, s(i) = {v(i)
T }. To
simplify the notation, we omit i in the following explanation.
Subsequence vt ∈ Rn×d is a matrix of medical codes such
that vt = {vt1 , vt2 , · · · , vtj , · · · , vtn }, where vtj ∈ Rd
is the vector representation of the jth medical code in the
tth subsequence vt and there are n medical codes in a
subsequence. In real EHR data, it is very likely that the
numbers of medical codes in each visit or time window are
different, thus, we utilize the padding approach to obtain a
consistent matrix dimensionality in the network.

To assign attention weights, we utilize the one-side con-
volution operation with a ﬁlter ωα ∈ Rd and a nonlinear
activation function. Thus, the weight vector αt is generated

6

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

for medical codes in the subsequence vt, presented in Equa-
tion 13.

αt = tanh(Conv(ωα, vt))

(13)

where αt = {αt1, αt2, · · · , αtn }, and ωα ∈ Rd is the
weight vector of the ﬁlter. The convolution operation Conv
is presented in Equation 14.

˜αtj = (ωα)(cid:124)vtj + bα

(14)

where bα is a bias term. Then, given the original matrix vt
and the learned weights αt, an aggregated vector xt ∈ Rd is
constructed to represent the tth subsequence, presented in 15.

xt =

αtj vtj

n
(cid:88)

j=1

(15)

Given Equation 15, we obtain a sequence of vectors, x =
{x1, x2, · · · , xt, · · · , xT }, to represent a patient’s medical
history.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of embedded subsequences, this step em-
ploys the subsequence-level attention which allows the net-
work itself to learn the weights of subsequences according to
their contribution to the prediction target.

To capture the longitudinal dependencies, we utilize a

bidirectional GRU-based RNN, presented in Equations 16.

h1, · · · , ht, · · · , hT = GRU (x1, · · · , xt, · · · , xT )

(16)

where ht ∈ Rk represents the output by the GRU unit at
the tth subsequence. Then, we introduce a set of linear and
softmax layers to generate M hops of weights β ∈ RM ×T
for subsequences. Then, for the hop m

γmt = (wβ

m)(cid:124)ht + bβ

βm1 , · · · , βmT =

(17)

(18)

sof tmax(γm1 , · · · , γmt, · · · , γmT )

m ∈ Rk. Thus, with the subsequence-level weights
where wβ
and hidden outputs, we construct a vector cm ∈ Rk to repre-
sent a patient’s medical visit history with one hop of subse-
quence weights, presented in the following Equation 19.

cm =

βmtht

(19)

T
(cid:88)

t=1

Then, a context vector c ∈ RM ×k is constructed by

concatenating c1, c2, · · · , cM .

VOLUME 0, 2018

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the context vector c, this step integrates the patients
characteristics a ∈ Rq into the context vector for a com-
plete vector representation of the patient’s EHR data. In
this research, the patient characteristics include demographic
information and some static medical conditions, such as age,
gender, and previous hospitalization. Thus, an aggregated
vector is constructed, c(cid:48) ∈ RM ×k+q, by adding a as addi-
tional dimensions to the context vector c.

E. PREDICTING OUTCOME
Given the vector representation of the complete medical
history and characteristics of patients, c(cid:48), we add a linear and
a softmax layer for the ﬁnal outcome prediction, as presented
in Equation 20.

ˆy = sof tmax(wc(cid:124)

c(cid:48) + bc)

(20)

To train the network, we use cross-entropy as the loss

function, presented in Equation 21.

L = −

yilog(ˆyi) + (1 − yi)log(1 − ˆyi)

(21)

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

+

||ββ(cid:124) − I||2
F

where N is the total number of observations. Here, yi is
a binary variable in classiﬁcation problems, while model
output ˆyi is real-valued. The second term in Equation 21 is
to penalize redundancy if the attention mechanism provides
similar subsequence weights for different hops of attention,
which is derived from [32]. This penalty term encourages the
multiple hops to focus on diverse areas and each hop focuses
on a small area.

Thus, we obtain a ﬁnal output for the prediction of out-
comes and a complete personalized vector representation of
the patient’s longitudinal EHR data.

V. EVALUATION
A. BACKGROUND
Although health care spending has been a relatively stable
share of the Gross Domestic Product (GDP) in the United
States since 2009, the costs of hospitalization, the largest
single component of health care expenditures,
increased
by 4.1% in 2014 [33]. Unplanned hospitalization is also
distressing and can increase the risk of related adverse events,
such as hospital-acquired infections and falls [34], [35].
Approximately 40% hospitalizations in the United King-
dom are unplanned and are potentially avoidable [36]. One
important form of unplanned hospitalization is hospital re-
admissions within 30 days of discharge, which is ﬁnancially
penalized in the United States. Early interventions targeted to
patients at risk of hospitalization could help avoid unplanned
admissions, reduce inpatient health care cost and ﬁnancial

7

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 5. A graphical illustration of the experimental setting for the risk
prediction of hospitalization

penalties for providers, and reduce emergency department
congestion [37].

In this research, we apply our proposed representation
learning framework to the risk prediction of future hospi-
talization. Many studies have been conducted by researchers
to predict the risk of 30-day readmission, or the admission
risk of a particular population, such as patients with Am-
bulatory Care Sensitive Conditions (ACSCs), patients with
heart failure, etc. [38]–[41]. Here, we focus on the general
population and the objective is to predict the risk of all-cause
hospitalization using longitudinal EHR data.

B. EXPERIMENTAL DESIGN
In this research, we use de-identiﬁed EHR data from the
University of Virginia Health System covering 75 months be-
ginning in September 2010. This dataset contains 2,343,651
inpatient and outpatient visits of 473,915 distinct patients.
We extracted visit data with diagnosis, medication, and pro-
cedure codes.

We deﬁned the observation window and prediction period
to validate the proposed method. We ﬁrst extract all patients
with a medical record of at least 1.5 years, where the ﬁrst year
is the observation window and the medical records in this
time window are used for feature construction. The follow-
ing 6 months is the hold-off period for the purpose of early
detection. For the positive class, we take all patients who
have hospitalization after the ﬁrst 1.5 years in their medical
history, while the negative class consists of patients who have
no hospitalization after 1.5 years. To better illustrate the ex-
perimental setting, we present the observation window, hold-
off and onset of outcome event in Figure 5. Here, the medical
codes include diagnosis, medication, and procedure codes,
and a vector representation is learned for each code. In this
dataset, diagnoses are primarily coded in ICD-9 and a small
portion is ICD-10 codes, while procedures are mainly using
CPT codes with a few ICD-9 procedure codes. The codes of
medications are using the pharmaceutical categories. Overall,
there are 94 distinct medication categories, 34,419 distinct
diagnoses codes, and 7,895 distinct procedure codes in the
EHR data. The dimension of the learned vectors of medical
codes is set to 100. Medical codes that appear in less than 50
patients medical records are excluded as rare events.

To construct the subsequences of medical codes, we use l
days as the time window. Figure 6 presents the cumulative
histogram and density plot of the numbers of visits in the
observation window, and we observe that the majority of

FIGURE 6. The cumulative histogram and density plot of patients’ numbers of
visits

patients have a small number of visits during the observation
window (less than 25% of patients have more than 4 visits).
Thus, we set l to 90 days, which split the observation window
into 4 subsequences.

Within each subsequence, the number of distinct medical
codes were computed and patients with more medical codes
in a subsequence than the 95% quantile were excluded from
the dataset. Overall, there are 8,841 and 89,101 patients in
the target and control groups, respectively. Each group is
randomly split into training, validation and testing sets with
a 7:1:2 ratio. Thus, 70% are used for training, another 20%
is used for testing, and the rest 10% are used for parameter
tuning and early stopping. The stochastic gradient descent
algorithm is used in training to minimize the cross-entropy
loss function, shown in Equation 21.

To evaluate the proposed representation learning frame-
work, we compare the prediction performance of the pro-
posed model with baseline approaches as follows.

1) Logistic regression (LR)

The inputs are the aggregated counts of grouped medical
codes over the entire observation window. Since the di-
mensionality of raw medical codes is huge, AHRQ clini-
cal classiﬁcations of diagnoses and procedures are used to
achieve a more general clustering of medical codes [42].
The medication codes are the pharmaceutical classes. Fur-
thermore, patient characteristics and previous inpatient visit
are also considered, where age and gender are demographic
information, and a binary indicator is utilized to represent
the presence of the previous hospitalization. Hence, the input
is a 436-dimensional vector representing a patient’s medical
history and characteristics.

8

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 1. The predictive performance of baselines and the proposed Patient2Vec framework

Methods
LR
MLP
RETAIN
FRNN-MGE
BiRNN-MGE
FRNN-MVE
BiRNN-MVE
Patient2Vec

Sensitivity
0.637 ± 0.010
0.727 ± 0.013
0.553 ± 0.012
0.636 ± 0.012
0.600 ± 0.012
0.753 ± 0.011
0.724 ± 0.010
0.769 ± 0.010

Speciﬁcity
0.728 ± 0.003
0.617 ± 0.004
0.710 ± 0.003
0.739 ± 0.004
0.777 ± 0.003
0.676 ± 0.004
0.707 ± 0.003
0.694 ± 0.004

AUC
0.721 ± 0.006
0.713 ± 0.007
0.663 ± 0.007
0.759 ± 0.006
0.768 ± 0.007
0.785 ± 0.006
0.788 ± 0.005
0.799 ± 0.005

F2 score
0.434 ± 0.006
0.423 ± 0.007
0.370 ± 0.008
0.438 ± 0.009
0.439 ± 0.009
0.470 ± 0.008
0.473 ± 0.008
0.492 ± 0.007

2) Multi-layer perceptron (MLP)
A multi-layer perceptron is trained to predict hospitalization
using the same inputs for logistic regression. Here, we use a
one hidden layer MLP with 256 hidden nodes.

3) Forward RNN with medical group embedding
(FRNN-MGE)
We split the sequence into subsequences with equal interval l.
The input at each step is the counts of medical groups within
the associated time interval, and the patient characteristics are
appended as additional features in the ﬁnal logistic regression
step. Here, the RNN is a forward GRU (or LSTM [18]) with
one hidden layer and the size of the hidden layer is 256.

4) Bidirectional RNN with medical group embedding
(BiRNN-MGE)
The inputs used for this baseline is the same as the one for
the FRNN-MGE [15]. The RNN used here is a bidirectional
GRU with one hidden layer and the size of the hidden layer
is 256.

5) Forward RNN with medical vector embedding
(FRNN-MVE)
We split the sequence into subsequences with equal interval l.
The input at each step is the vector representation of the
medical codes within the associated time interval, and the
patient characteristics are appended as additional features in
the ﬁnal logistic regression step. Here, the RNN is a forward
GRU (or LSTM [28]) with one hidden layer and the size of
the hidden layer is 256.

6) Bidirectional RNN with medical vector embedding
(BiRNN-MVE)
The inputs used for this baseline is the same as the one for
the FRNN-MVE [25]. The RNN used here is a bidirectional
GRU or LSTM [15] with one hidden layer and the size of the
hidden layer is 256.

7) RETAIN
This model uses reverse time attention mechanism on RNNs
for an interpretable representation of patient’s EHR data [31].
The inputs are the same as the one for FRNN-MGE, which
takes the counts of medical grouping within each time inter-
val to construct features. Similarly, the two RNNs used for

generating weights are GRU-based and the size of the hidden
layers are 256.

8) Patient2Vec
The inputs are the same as that for FRNN-MVE. One ﬁlter
is used when generating weights for within-subsequence
attention, and three ﬁlters are used for subsequence-level
attention. Similarly, the RNN used here is GRU-based and
there is one hidden layer and the size of the hidden layer
is 256.

The inputs of all baselines and Patient2Vec are normal-
ized to have zero mean and unit variance. We model the
risk of hospitalization based on Patient2Vec and baseline
representations of patients’ medical histories, and the model
performance is evaluated with Area Under Curve(AUC),
sensitivity, speciﬁcity, and F2-score. The validation set is
used for parameter tuning and early stopping in the train-
ing process. Each experiment is repeated 20 times and we
calculate the averages and standard deviations of the above
metrics, respectively.

C. EXPERIMENTAL RESULTS
The predictive performance of Patient2Vec and baselines are
presented in Table 1. The results shown here for the RNN-
based models are based on time interval l = 90 days to
construct subsequences.

According to Table 1, the RNN-based models are generally
capable of achieving higher prediction performance in terms
of sensitivity, AUC and F2 score, except for the RNN models
based on medical group embedding which have lower sensi-
tivity. Among all RNN-based approaches, the ones based on
vector embedding outperform those based on medical group
embedding in terms of sensitivity, AUC, and F2 score. The
bidirectional RNN models generally have higher speciﬁcity
but lower sensitivity than the forward RNN models, while
the bidirectional ones have comparable AUC and F2 score
with the forward ones, respectively. Generally, the proposed
Patient2Vec framework outperforms the baseline methods,
especially in terms of sensitivity and F2 score.

D. VISUALIZATION & INTERPRETATION
In addition to predictive performance, we interpret
the
learned representation by understanding the relative impor-
tance of clinical events in a patient’s EHR data. Considering

VOLUME 0, 2018

9

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 7. The heat map showing feature importance for Patient A

FIGURE 8. The proﬁle of Patient A.

FIGURE 9. The proﬁle of Patient B.

the feature importance learned by Patient2Vec are person-
alized for an individual patient, we illustrate it with two
example patients. Figures 8 and 9 present the proﬁles of
two individuals, Patient A and Patient B, respectively. To
facilitate the interpretation, instead of using raw medical
codes, we present the clinical groups from the AHRQ clinical
classiﬁcation software on diagnoses and procedure codes, as
well as pharmaceutical groups for medications.

According to Figure 8, Patient A is a male patient who
has hospitalization history in the observation window and
is admitted to the hospital seven months after the end of
the observation window for congestive heart failure. The

predicted risk is 96.4%, while the risk decreases for female
patients or patients without hospitalization history. It is also
not surprising to observe an increased risk for older patients.
The heat map in Figure 7 shows the relative importance of
the medical events in this patient’s medical record at each
time window and the ﬁrst row of the heat map presents
the subsequence-level attention. The darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the last subsequence,
t4, is the most important with respect to hospitalization risk,
followed by t1, t2, and t3 in order of importance.

Among all the clinical events in the subsequence t4, we

10

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 2. The top clinical groups with high weights in hospitalized patients

Index

Clinical Groups

Diagnoses
1
2
3

Essential hypertension
Other connective tissue disease
Spondylosis; intervertebral disc disorders; other back
problems
Other lower respiratory disease
Disorders of lipid metabolism
Other aftercare
Diabetes mellitus without complication
Screening and history of mental health and substance
abuse codes
Other nervous system disorders
Other screening for suspected conditions (not mental
disorders or infectious disease)

Other OR therapeutic procedures on nose; mouth and
pharynx
Suture of skin and subcutaneous tissue
Other therapeutic procedures on eyelids; conjunctiva;
cornea

Laboratory - Chemistry and hematology
Other laboratory
Other OR therapeutic procedures of urinary tract
Other OR procedures on vessels other than head and
neck

Therapeutic radiology for cancer treatment

Procedures
1

9
10

4
5
6
7
8

2
3

4
5
6
7

8

2

Medications
1

Diagnostic Products

Analgesics-Narcotic

observe that the OR therapeutic procedures (nose, mouth,
and pharynx), laboratory (chemistry and hematology), coro-
nary atherosclerosis & other heart disease, cardiac dys-
rhythmias, and conduction disorders are the ones with the
highest weights, while other events such as other connected
tissue disease are less important in terms of future hospi-
talization risk. Additionally, some medications appear to be
informative as well, including beta blockers, antihyperten-
sives, anticonvulsants, anticoagulants, etc. In the ﬁrst-time
window, the medical events with high weights are coro-
nary atherosclerosis & other heart disease, gastrointestinal
hemorrhage, deﬁciency and anemia, and other aftercare. In
the next subsequence, the most important medical events
are heart diseases and related procedures such as coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
conduction disorders, hypertension with complications, other
OR heart procedures, and other OR therapeutic nervous
system procedures. We also observe that the kidney disease
related diagnoses and procedures appear to be important
features. Throughout the observation window, the coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
and conduction disorders constantly show high weights with
respect to hospitalization risk, and the ﬁndings are consistent

FIGURE 10. The heat map showing feature importance for Patient B

with medical literature.

Figure 9 presents the proﬁle of Patient B, which is a male
patient without hospitalization in the observation window.
This patient is hospitalized for occlusion of cerebral arter-
ies approximately one year after the observation window,
and the predicted risk is 74.6%. For a similar patient who
is 10 years older or with previous hospitalization history,
the risk increases by 4.2% and 1%, respectively, while there
is a smaller risk of hospitalization for a female patient. To
illustrate the medical events of Patient B, the heat map in
Figure 10 depicts the relative importance of medical groups
in the subsequences, as well as the subsequence-level weights
for hospitalization risk. Similarly, the darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the second subse-
quence appears to be the most important, while the last one is
less predictive of future hospitalization. In fact, the medical
events in the last time window are spondylosis, intervertebral
disc disorders, other back problems and other bone disease &
musculoskeletal deformities, and malaise and fatigue, which
are not highly related to the cause of hospitalization of Patient
B.

In the most predictive subsequence, t2, we observe that
other OR heart procedures, genitourinary symptoms, spondy-
losis, intervertebral disc disorders, other back problems,
therapeutic procedures on eyelid, conjunctiva, and cornea,
and arterial blood gases have high attention weights. In the

VOLUME 0, 2018

11

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 3. The top diagnosis groups with high weights in patients hospitalized
for osteoarthritis, septicemia, acute myocardial infarction, congestive heart
failure, and diabetes mellitus with complications, respectively

Index

Diagnosis Groups

In patients admitted for osteoarthritis

Osteoarthritis
Other connective tissue disease

1
2

3
4

5

1
2

3
4
5

1
2
3

4
5

1
2
3
4
5

1
2
3

4
5

Other non-traumatic joint disorders
Spondylosis; intervertebral disc disorders; other back prob-
lems
Other aftercare

In patients admitted for septicemia

Essential hypertension
Diabetes mellitus without complication

Disorders of lipid metabolism
Other lower respiratory disease
Other aftercare

In patients admitted for acute myocardial infarction

Coronary atherosclerosis and other heart disease
Medical examination/evaluation
Other screening for suspected conditions (not mental disorders
or infectious disease)

Other lower respiratory disease
Disorders of lipid metabolism

In patients admitted for congestive heart failure

Congestive heart failure (nonhypertensive)
Coronary atherosclerosis and other heart disease
Cardiac dysrhythmias
Diabetes mellitus without complication
Other lower respiratory disease

In patients admitted for diabetes mellitus with complications

Diabetes mellitus with complications
Diabetes mellitus without complication
Other aftercare

Other nutritional; endocrine; and metabolic disorders
Fluid and electrolyte disorders

earliest time window, the most important medical events also
include therapeutic procedures on eyelid, conjunctiva, and
cornea, arterial blood gases, while diabetes, hypertension
as well as diagnostic products show their relatively high
importance. Throughout the observation window, medical
events spondylosis, intervertebral disc disorders, other back
problems, therapeutic procedures on eyelid, conjunctiva, and
cornea are constantly with high attention weights. Here, di-
agnostic products is a medication class, which include barium
sulfate, iohexol, gadopentetate dimeglumine, iodixanol, tu-
berculin puriﬁed protein derivative, iodixanol, regadenoson,
acetone (urine), and so forth. These medications are primarily
for blood or urine testing, or used as radiopaque contrast
agents for x-rays or CT scans for diagnostic purposes.

Additionally, we attempt to interpret the learned repre-
sentation and feature importance at the population-level. In
Table 2, we present the top 20 clinical groups with high
weights among hospitalized patients in the test set.

According to Table 2, the most predictive diagnosis groups
for future hospitalization are chronic diseases, including
essential hypertension, diabetes, lower respiratory disease,
disorders of lipid metabolism, and musculoskeletal diseases
such as other connective tissue disease and spondylosis,
intervertebral disc disorders, other back problems. The most
important procedures are some OR therapeutic procedures
and laboratory tests, such as the OR procedures on nose,
mouth, and pharynx, vessels, urinary tract, eyelid, conjunc-
tiva, cornea, etc. It is not surprising to see that diagnostic
products are showing with high weights, considering these
medications are used in testing or examinations for diagnos-
tic purposes.

Moreover, we present the top diagnoses groups with high
weights in patients hospitalized for different primary causes.
Table 3 shows the top 5 diagnosis groups with high weights
in patients admitted for osteoarthritis, septicemia (except in
labor), acute myocardial infarction, congestive heart failure
(nonhypertensive), and diabetes mellitus with complications,
respectively. Accordingly, we observe that the most impor-
tant diagnoses for hospitalization risk prediction in popu-
lation admitted for osteoarthritis are musculoskeletal dis-
eases such as connective tissue disease, joint disorders, and
spondylosis. However, the diagnoses with highest weights
in the patients admitted for septicemia are chronic diseases
including essential hypertension, diabetes, disorders of lipid
metabolism, and respiratory disease. The top diagnoses have
many overlaps between the populations admitted for acute
myocardial infarction and for congestive heart failure, con-
sidering both populations are admitted for heart diseases.
the overlapped diagnosis groups include coronary
Here,
atherosclerosis and other heart diseases and lower respiratory
diseases. As for patients admitted for diabetes with com-
plications, the top diagnoses are diabetes with or without
complications, nutritional, endocrine, metabolic disorders,
and ﬂuid and electrolyte disorders. In general, the learned
feature importance is consistent with medical literature.

VI. DISCUSSION
Our proposed framework is applied to the prediction of
hospitalization using real EHR data that demonstrates its
prediction accuracy and interpretability. This work could be
further enhanced by incorporating the follow-up information
on the negative patient population and investigate if it in-
deed shows an improved health outcome or the patient is
hospitalized elsewhere. Patient2Vec employs a hierarchical
attention mechanism, allowing us to directly interpret the
weights of clinical events. In future work, we will extend the
attention to incorporate demographic information for a more
comprehensive and automatic interpretation.

Although we apply Patient2Vec to the early detection of
long-term hospitalization, i.e., at least 6 months after the
previous hospitalization, it could be used to predict the risk
of 30-day readmission to help prevent unnecessary rehospi-
talizations.

12

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

VII. CONCLUSION
In this paper, we propose a representation learning frame-
work, Patient2Vec, to learn a personalized interpretable deep
representation of EHR data based on recurrent neural net-
works and the attention mechanism. This work improves
the performance of predictive models as well as deepens
the understanding of disease correlations. We apply this
framework to the risk prediction of hospitalization using
patients’ longitudinal EHR data. The experimental results
demonstrate that the proposed Patient2Vec representation is
capable of achieving a more accurate prediction than base-
lines approaches. Moreover, the learned feature importance
in the representations are interpreted both at the individual
and population levels to facilitate clinical insights.

In this work, the proposed Patient2Vec framework is eval-
uated with the risk prediction of all-cause hospitalization,
but in the future could be applied to predict hospitalization
in more speciﬁc populations, other health related prediction
problems, or domains outside of health.

REFERENCES
[1] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical
attention networks for document classiﬁcation,” in Proceedings of the
2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, 2016, pp.
1480–1489.

[2] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,” arXiv

preprint arXiv:1408.5882, 2014.

[3] J. Howard and S. Ruder, “Fine-tuned language models for text classiﬁca-

tion,” arXiv preprint arXiv:1801.06146, 2018.

[4] M. M. Lopez and J. Kalita, “Deep learning applied to nlp,” arXiv preprint

arXiv:1703.03091, 2017.

[5] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using
rnn encoder-decoder for statistical machine translation,” arXiv preprint
arXiv:1406.1078, 2014.

[6] A. L. Nobles, J. J. Glenn, K. Kowsari, B. A. Teachman, and L. E. Barnes,
“Identiﬁcation of imminent suicide risk among young adults using text
messages,” in Proceedings of the 2018 CHI Conference on Human Factors
in Computing Systems. ACM, 2018, p. 413.

[7] K. Kowsari, D. E. Brown, M. Heidarysafa, K. J. Meimandi, M. S. Gerber,
and L. E. Barnes, “Hdltex: Hierarchical deep learning for text classiﬁca-
tion,” in 2017 16th IEEE International Conference on Machine Learning
and Applications (ICMLA), Dec 2017, pp. 364–371.

[8] K. Kowsari, M. Heidarysafa, D. E. Brown, K. Jafari Meimandi, and L. E.
Barnes, “Rmdl: Random multimodel deep learning for classiﬁcation.”
ACM, 2018.

[9] H. Strobelt, S. Gehrmann, H. Pﬁster, and A. M. Rush, “Lstmvis: A tool
for visual analysis of hidden state dynamics in recurrent neural networks,”
IEEE transactions on visualization and computer graphics, vol. 24, no. 1,
pp. 667–676, 2018.

[10] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” Scientiﬁc
reports, vol. 8, no. 1, p. 6085, 2018.

[11] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches
preprint

neural machine

translation,”

arXiv

attention-based
to
arXiv:1508.04025, 2015.

[12] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[13] D. Yogatama, C. Dyer, W. Ling, and P. Blunsom, “Generative and discrim-
inative text classiﬁcation with recurrent neural networks,” arXiv preprint
arXiv:1703.01898, 2017.

[14] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent

trends
language processing,” arXiv preprint

in deep learning based natural
arXiv:1708.02709, 2017.

[15] M. Basaldella, E. Antolli, G. Serra, and C. Tasso, “Bidirectional lstm
recurrent neural network for keyphrase extraction,” in Italian Research

Conference on Digital Libraries, Springer.
lishing, 2018, pp. 180–187.

Springer International Pub-

[16] S. Ghosh, O. Vinyals, B. Strope, S. Roy, T. Dean, and L. Heck, “Con-
textual lstm (clstm) models for large scale nlp tasks,” arXiv preprint
arXiv:1602.06291, 2016.

[17] B. Yue, J. Fu, and J. Liang, “Residual recurrent neural networks for
learning sequential representations,” Information, vol. 9, no. 3, p. 56, 2018.
[18] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
gated recurrent neural networks on sequence modeling,” arXiv preprint
arXiv:1412.3555, 2014.

[19] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difﬁculty of training
recurrent neural networks.” ICML (3), vol. 28, pp. 1310–1318, 2013.
[20] D. Britz, “Recurrent neural network tutorial,” http://www.wildml.com/

2015/10/, 2015, [Accessed on October 5, 2017].

[21] V. Mnih, N. Heess, A. Graves et al., “Recurrent models of visual attention,”
in Advances in neural information processing systems, 2014, pp. 2204–
2212.

[22] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for
abstractive sentence summarization,” arXiv preprint arXiv:1509.00685,
2015.

[23] T. Ma, C. Xiao, and F. Wang, “Health-atm: A deep architecture for
multifaceted patient health record representation and risk prediction,” in
Proceedings of the 2018 SIAM International Conference on Data Mining.
SIAM, 2018, pp. 261–269.

[24] A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J. Liu,
X. Liu, J. Marcus, M. Sun et al., “Scalable and accurate deep learning with
electronic health records,” npj Digital Medicine, vol. 1, no. 1, p. 18, 2018.
[25] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzell, “Learning to diagnose
with lstm recurrent neural networks,” arXiv preprint arXiv:1511.03677,
2015.

[26] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” arXiv preprint
arXiv:1606.01865, 2016.

[27] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghas-
semi, B. Moody, P. Szolovits, L. A. Celi, and R. G. Mark, “Mimic-iii, a
freely accessible critical care database,” Scientiﬁc data, vol. 3, 2016.
[28] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart, and J. Sun, “Doctor
ai: Predicting clinical events via recurrent neural networks,” in Machine
Learning for Healthcare Conference, 2016, pp. 301–318.

[29] Z. Che, S. Purushotham, R. Khemani, and Y. Liu, “Interpretable deep
models for icu outcome prediction,” in AMIA Annual Symposium Pro-
ceedings, vol. 2016. American Medical Informatics Association, 2016,
p. 371.

[30] J. H. Friedman, “Greedy function approximation: a gradient boosting

machine,” Annals of statistics, pp. 1189–1232, 2001.

[31] E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart,
“Retain: An interpretable predictive model for healthcare using reverse
time attention mechanism,” in Advances in Neural Information Processing
Systems, 2016, pp. 3504–3512.

[32] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Ben-
gio, “A structured self-attentive sentence embedding,” arXiv preprint
arXiv:1703.03130, 2017.

[33] C. M. Torio and B. J. Moore, “National inpatient hospital costs: The most
expensive conditions by payer, 2013,” https://www.hcup-us.ahrq.gov/
reports/statbriefs/sb204-Most-Expensive-Hospital-Conditions.jsp, 2016.
[34] E. Wallace, E. Stuart, N. Vaughan, K. Bennett, T. Fahey, and S. M.
Smith, “Risk prediction models to predict emergency hospital admission
in community-dwelling adults: a systematic review,” Medical care, vol. 52,
no. 8, p. 751, 2014.

[35] E. N. de Vries, M. A. Ramrattan, S. M. Smorenburg, D. J. Gouma, and
M. A. Boermeester, “The incidence and nature of in-hospital adverse
events: a systematic review,” Quality and safety in health care, vol. 17,
no. 3, pp. 216–223, 2008.

[36] S. Purdey and A. Huntley, “Predicting and preventing avoidable hospital
admissions: a review.” The journal of the Royal College of Physicians of
Edinburgh, vol. 43, no. 4, pp. 340–344, 2012.

[37] H. Ontario, “Early identiﬁcation of people at risk of hospitalization: Hos-
pital admission risk prediction (harp)-a new tool for supporting providers
and patients,” 2013.

[38] B. Zheng, J. Zhang, S. W. Yoon, S. S. Lam, M. Khasawneh, and S. Poranki,
“Predictive modeling of hospital readmissions using metaheuristics and
data mining,” Expert Systems with Applications, vol. 42, no. 20, pp. 7110–
7120, 2015.

VOLUME 0, 2018

13

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

[39] D. Kansagara, H. Englander, A. Salanitro, D. Kagen, C. Theobald,
M. Freeman, and S. Kripalani, “Risk prediction models for hospital
readmission: a systematic review,” The Journal of the American Medical
Association, vol. 306, no. 15, pp. 1688–1698, 2011.

[40] G. Giamouzis, A. Kalogeropoulos, V. Georgiopoulou, S. Laskar, A. L.
Smith, S. Dunbar, F. Triposkiadis, and J. Butler, “Hospitalization epidemic
in patients with heart failure: risk factors, risk prediction, knowledge gaps,
and future directions,” Journal of cardiac failure, vol. 17, no. 1, pp. 54–75,
2011.

[41] E. Prescott, A. M. Bjerg, P. K. Andersen, P. Lange, and J. Vestbo, “Gender
difference in smoking effects on lung function and risk of hospitalization
for COPD: results from a danish longitudinal population study,” European
Respiratory Journal, vol. 10, no. 4, pp. 822–827, 1997.

[42] Agency for Healthcare Research and Quality (AHRQ), “Clinical classi-
ﬁcations software (CCS) for ICD-9-CM,” https://www.hcup-us.ahrq.gov/
toolssoftware/ccs/ccs.jsp, 2015.

JENNIFER M. LOBO (jem4yb@virginia.edu) is
an Assistant Professor of Biomedical Informatics
in the Department of Public Health Sciences at
University of Virginia. She received her Ph.D. in
Industrial Engineering from North Carolina State
University, Raleigh, NC. Her research interests in-
volve using mathematical modeling and stochastic
optimization methods to build models that simu-
late the natural course of disease. These models
allow for estimation of outcomes under different
screening and treatment policies in the absence of randomized controlled
trials, and can be used to optimize screening and treatment decisions for
patients with chronic diseases. Her projects include optimizing treatment
for patients with type 2 diabetes, generating individualized decision analysis
models for prostate cancer patients, and developing optimal imaging surveil-
lance guidelines for recurrent kidney cancer.

JINGHE ZHANG (jz4kg@virginia.edu) is a lead
data scientist at Target Corporation. She received
her Ph.D. in Systems Engineering from the Uni-
versity of Virginia. Prior to entering the Ph.D. pro-
gram at UVA, she received the Master of Science
in Industrial and Systems Engineering from the
State University of New York at Binghamton. Her
research interests are in natural language process-
ing, machine learning, recommender systems, and
health informatics.

LAURA E. BARNES (lb3dp@virginia.edu) is an
Associate Professor in Systems and Information
Engineering and the Data Science Institute at the
University of Virginia. She received her Ph.D. in
Computer Science from the University of South
Florida, Tampa, FL. She directs the Sensing Sys-
tems for Health (S2He) Lab which focuses on
understanding the dynamics and personalization
of health and well-being through mobile sensing
and analytics.

(kk7nc@virginia.edu) is
KAMRAN KOWSARI
a Ph.D. student in the Department of Systems
and Information Engineering at the University of
Virginia, Charlottesville, VA. He is a member of
the Sensing Systems for Health Lab. He received
his Master of Science from Department of Com-
puter Science at The George Washington Univer-
sity, Washington, DC. He has more than ten years
of experience in machine learning and software
development. His experience includes numerous
industrial and academic projects. His research interests include natural
language processing, machine learning, deep learning, artiﬁcial intelligence,
text mining, and unsupervised learning.

JAMES H. HARRISON, JR., (jhh5y@virginia.
edu) is Associate Professor of Pathology and Di-
rector of Laboratory Information Systems at the
University of Virginia Medical Center and also has
appointments in the Departments of Public Health
Sciences in the UVA School of Medicine, and
Systems and Information Engineering in the UVA
School of Engineering and Applied Sciences. He
received his MD and PhD (Pharmacology) de-
grees from Medical University of South Carolina,
Charleston, SC, completed residencies in Anatomic Pathology and Labora-
tory Medicine at Yale-New Haven Hospital, New Haven, CT, and completed
a postdoctoral fellowship in Environmental Toxicology at Yale University,
New Haven, CT. Dr. Harrison has over 25 years of experience in the ﬁeld
of medical informatics, including work in clinical laboratory information
systems, electronic health records, clinical data analysis, and clinical data
standards development.

14

VOLUME 0, 2018

Date of publication 10, 2018 , date of current version 10, 2018.

Digital Object Identiﬁer 10.1109/ACCESS.2018.2875677

Patient2Vec: A Personalized
Interpretable Deep Representation of the
Longitudinal Electronic Health Record

JINGHE ZHANG1, (Member, IEEE), KAMRAN KOWSARI1,4(Member, IEEE), JAMES H.
HARRISON2,3,5, JENNIFER M. LOBO2,5, and LAURA E. BARNES1,4,5, (Member, IEEE)
1Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, USA
2 Department of Public Health Sciences, University of Virginia, Charlottesville, VA 22904, USA
4 Sensing Systems for Health Lab, University of Virginia, Charlottesville, VA 22904, USA
3 Division of Laboratory Medicine Department of Pathology, University of Virginia, Charlottesville, VA 22904, USA
5Data Science Institute, University of Virginia, Charlottesville, VA 22904, USA
Corresponding author: Laura E. Barnes (lb3dp@virginia.edu)
This research was supported by a Jeffress Trust Award in Interdisciplinary Science.
Patient2Vec is shared as an open source tool at https://github.com/BarnesLab/Patient2Vec

ABSTRACT The wide implementation of electronic health record (EHR) systems facilitates the collection
of large-scale health data from real clinical settings. Despite the signiﬁcant increase in adoption of EHR
systems, this data remains largely unexplored, but presents a rich data source for knowledge discovery from
patient health histories in tasks such as understanding disease correlations and predicting health outcomes.
However, the heterogeneity, sparsity, noise, and bias in this data present many complex challenges.
This complexity makes it difﬁcult to translate potentially relevant information into machine learning
algorithms. In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep
representation of longitudinal EHR data which is personalized for each patient. To evaluate this approach,
we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive
performance with baseline methods. Patient2Vec produces a vector space with meaningful structure and it
achieves an AUC around 0.799 outperforming baseline methods. In the end, the learned feature importance
can be visualized and interpreted at both the individual and population levels to bring clinical insights.

INDEX TERMS Attention mechanism, gated recurrent unit, hospitalization, longitudinal electronic health
record, personalization, representation learning.

8
1
0
2
 
t
c
O
 
5
2
 
 
]

M
Q
.
o
i
b
-
q
[
 
 
3
v
3
9
7
4
0
.
0
1
8
1
:
v
i
X
r
a

I. INTRODUCTION

L ongitudinal EHR data resemble text documents from

many perspectives. A text document consists of a se-
quence of sentences, and a sentence is a sequence of words.
Similarly, the longitudinal health record of a patient consists
of a sequence of visits, and there is a list of clinical events,
including diagnoses, medications, and procedures, that occur
during a visit. Considering these similarities, representation
learning methods for text documents in Natural Language
Processing (NLP) have great potential to be applied to lon-
gitudinal EHR data.

Deep neural networks have become very popular in the
NLP ﬁeld and have been very successful in many applica-
tions, such as machine translation, question answering, text
classiﬁcation, document summarization, language modeling,
etc. [1]–[8]. These networks excel at complex language tasks

because they are capable of identifying high-order relation-
ships, the network structure can encode language structures,
and they allow the learning of a hierarchical representation
of the language, i.e., representations for tokens, phrases, and
sentences, etc.

Among a variety of deep learning methods, Recurrent
Neural Networks (RNNs) have shown their effectiveness in
NLP tasks because they have the ability to capture sequential
information [7]–[10] which is inherent in human language.
Traditional neural networks assume that inputs are inde-
pendent of each other, while an RNN computes the output
based on the current input as well as the “memory” from the
previous computation. Although vanilla RNNs are not good
at capturing long-term dependencies, many variants have
been proposed and validated that are effective in addressing

VOLUME 0, 2018

1

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

its performance with other baseline methods. In addition
to prediction performance, we further interpret the learned
representations with visualizations on example patients and
events. Finally, Section V provides a summary of this work.

II. RELATED WORK
In this section, we present an overview of a gated recurrent
unit, a type of RNN, which is capable of capturing long-term
dependencies. Then we brieﬂy introduce attention mecha-
nisms in neural networks that allow the network to attend
to certain regions of data, which is inspired by the visual
attention mechanism in humans. Additionally, we summarize
the RNN networks and attention mechanisms previously used
to mine EHR data.

A. RECURRENT NEURAL NETWORKS (RNN)

RNNs are expected to learn long-term dependencies by tak-
ing the previous state and the new input in the computation at
the current time step t. However, vanilla RNNs are incapable
of capturing the dependencies when the sequence is very long
due to the vanishing gradient problem [12]. Many variants of
the RNN network have been proposed to address this issue,
and long short term memory (LSTM) is one of the most
popular models used nowadays in NLP tasks [7], [8], [13]–
[16].

this issue.

In the medical domain, it is critical that analytical results
are interpretable, so that they can be understood and validated
by a human with expert knowledge and so that knowledge
captured by analysis can be used for process improvement.
Traditional deep neural networks have the disadvantage that
they lack interpretability. A substantial amount of work is
ongoing to make sense of the “black box”, and the attention
mechanism [11] is one of the more effective methods recently
developed to make the output of these algorithms more
interpretable.

Health care is undergoing unprecedented change, and there
is a great potential and demand for personalized care strate-
gies. Personalized medicine, also called precision medicine,
has previously focused on optimizing therapy to better ﬁt the
genetic makeup of the patient or the disease (e.g., the genetic
susceptibility of cancer to speciﬁc chemotherapy strategies).
The availability of EHR data and advances in machine learn-
ing create the potential for another type of personalization
of healthcare. This type of personalization has become ubiq-
uitous in our daily life. For example, customers have come
to expect personalized search on Google and personalized
product recommendations on Amazon and Netﬂix, based on
their charactersitics and previous experiences with the sys-
tems. Personalization of healthcare processes, based on a pa-
tient’s phenotype (physical and medical characteristics) and
healthcare experiences as documented in the health record,
may also improve "customer" satisfaction and it has the addi-
tional potential to improve healthcare efﬁciency, lower costs,
and yield better outcomes. We believe that representation
learning methods can capture a personalized representation
of the important heterogeneities in patients’ phenotypes and
medical histories at the population-level, and make these
representations available to drive healthcare decisions and
strategies.

This research is based on RNN models and the attention
mechanism with the objective of learning a personalized, in-
terpretable, and complete representation of patients’ medical
records. Our proposed framework is capable of learning a
personalized representation for each patient from a sequence
of clinical events. A hierarchical attention mechanism learns
personalized weights of clinical events, including hospital
visits and the procedures that they contain. These weights
allow us to interpret the relative importance and roles of clin-
ical events in the learned representations both at individual
and population levels. The ultimate goal is more accurate
prediction and better insight into the critical elements of
healthcare processes that can be used to improve healthcare
delivery.

The rest of this paper is organized as follows: Section II
summarizes the variants of RNNs and the attention mecha-
nism, as well as their application to EHR data. Section III
presents an overview of the proposed Patient2Vec repre-
sentation learning framework, and Section IV elaborates
the details of the algorithms. In Section V, the proposed
framework is evaluated for a prediction task and we compare

FIGURE 1. The top ﬁgure is a GRU gating unit and bottom ﬁgure shows an
LSTM unit [7]

2

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

1) Gated Recurrent Unit (GRU)
GRU is a simpliﬁed version of LSTM [7]. The basic idea
of GRU is to combat the vanishing gradient problem with a
gating mechanism. Hence the general recurrent structure in
GRU is identical to vanilla RNNs except that a GRU unit
is used in the computation at each time step rather than a
traditional simple recurrent unit.

In general, a GRU cell has two gates, i.e., a reset gate r
and an update gate z. The reset gate is used to determine
how to integrate the previous state into the computation of
the current state, while the update gate determines how much
the unit updates its activation.

Given the input xt at time step t, the reset gate rt is

computed as presented in Equation 1

rt = σ(Urxt + Wrst−1)

(1)

where Ur and Wr are the weight matrices of the reset gate
and st−1 is the hidden activation at time step t − 1. A similar
computation is performed for the update gate zt at time step t,
shown in Equation 2

zt = σ(Uzxt + Wzst−1)

(2)

where Uz and Wz are the weight matrices of update gate.
The current hidden activation ht is computed by

ht = (1 − zt)ht−1 + zt
(3)
where ˜ht is the candidate activation at time step t. The
computation of ˜ht is presented in Equation 4

˜ht

˜ht = tanh(Wxt + U(rt (cid:12) ht−1))

(4)

where U and W are weight matrices and (cid:12) represents
element-wise multiplication. Figure 1 presents a graphical
illustration of the GRU [7] and one unit of LSTM.

GRU is capable of learning long-term dependencies [17]
due to the additive component of update from t to t + 1
in the gating mechanism. Consequently, important features
will be carried forward in the input stream while irrelevant
information will be dropped. When the reset gate is 0, the
network is forced to drop previous states and reset with cur-
rent information. Moreover, the method provides shortcuts
such that the error is easily backpropagated without vanishing
too quickly [5], [18]. Hence, the GRU is well-suited to learn
long-term dependencies in sequence data.

2) Long Short-Term Memory (LSTM)
An LSTM unit is similar to a GRU, but with one more gate in
an LSTM unit (as shown in Figure 1). LSTM also preserves
long term dependencies more effectively than basic RNN.
This is particularly useful to overcome the vanishing gradient
problem [19]. Although LSTM has a chain-like structure sim-
ilar to RNN, LSTM uses multiple gates to carefully regulate
the amount of information that will be allowed into each node
state. Figure 1 shows the basic cell of an LSTM model. A step

VOLUME 0, 2018

by step explanation of an LSTM cell is as following:
Input gate:

it = σ(Wi[xt, ht−1] + bi),

Candid memory cell value:

˜Ct = tanh(Wc[xt, ht−1] + bc),

Forget gate activation:

ft = σ(Wf [xt, ht−1] + bf ),

New memory cell value:

Ct = it ∗ ˜Ct + ftCt−1,

Output gate value:

ot = σ(Wo[xt, ht−1] + bo),

ht = ot tanh(Ct),

(5)

(6)

(7)

(8)

(9)

(10)

In the above description all b represent bias vectors, all W
represent weight matrices, and xt is used as input to the
memory cell at time t. Also,the i, c, f, o indices refer to input,
cell memory, forget and output gates respectively. An RNN
can be biased when later words are more inﬂuential than the
earlier ones.

Empirically, LSTM and GRU achieve comparable per-
formance in many tasks but there are fewer parameters in
a GRU, which makes it a little faster to learn and able to
generalize with fewer data [20].

B. ATTENTION MECHANISM
Attention mechanisms, inspired by the visual attention sys-
tem found in humans, have become popular in deep learning.
Attention allows the network to focus on certain regions of
data, while perceiving other regions with “low resolution”.
In addition to higher accuracy, it also facilitates the interpre-
tation of learned representations. We elaborate an attention
mechanism on an RNN network, and Figure 2 presents a
graphical illustration.

According to Figure 2, a variable-length weight vector α
is learned based on hidden states [11]. Then a global context
vector is computed based on weights α and all the hidden
states to create the ﬁnal output. Equation 11 presents the
computation of the weight vector α = {α1, α2, · · · , αT },
where T is the length of the sequence

α1, α2, · · · , αT = f (Wαh + bα)

(11)

and where f is a nonlinear activation function, usually
sof tmax or tanh. Then, the context vector c is constructed
as:

c =

αtht

T
(cid:88)

t=1

Thus, the network puts more attention on the important
features for the ﬁnal prediction which can improve the model
performance. An additional beneﬁt is that the weights can

(12)

3

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

GRU-based model to address missing values in multivariate
time series data, in which the missing patterns are incorpo-
rated for improved prediction performance. This work has
been applied to the Medical Information Mart for Intensive
Care III (MIMIC-III) clinical database to demonstrate its
effectiveness in mining time series of clinical measurements
with missing values [27]. Longitudinal EHR data including
clinical events, such as diagnoses, medications, and pro-
cedures is also a potentially rich resource for predictive
modeling. Choi et al. [28] analyze this data with a GRU
network to forecast future clinical events, and it achieves a
better prediction performance than comparison models such
as logistic regression and MLP.

Difﬁculty in interpreting model behavior is one of the
major drawbacks of using deep learning to mine EHR data.
Some attempts have been made to address this issue. Che et
al. [29] propose an interpretable mimic learning method
which trains a mimic gradient boosting trees model to utilize
predicted labels or features learned by deep learning mod-
els for ﬁnal prediction [30]. Then the feature importances
learned by the tree-based models are used for knowledge
discovery. Attention mechanisms have been introduced re-
cently to improve the interpretability of the prediction results
of deep learning models in health analytics. Choi et al. [31]
develop an interpretable model with two levels of attention
weights learned from two reverse-time GRU models, re-
spectively. The experimental results on EHR data indicate
comparable prediction performance with conventional GRU
models but more interpretable results. Our work continues
the attempt to use attention mechanisms to improve the
interpretability of RNN-based models.

III. PATIENT2VEC SYSTEM MODEL
In this section, we provide an overview of the proposed
hierarchical representation learning framework. This frame-
work uses deep recurrent neural networks to capture the
complex relationships between clinical events in the patient’s
EHR data and employs the attention mechanism to learn
a personalized representation and to obtain relative feature
importance. The proposed representation learning framework
contains four steps and is presented graphically in Figure 3.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES

EHR data consists primarily of records of outpatient and
inpatient visits to healthcare providers. These visit records
include multiple clinical codes for diagnoses, symptoms,
procedures, therapies, and other observations and events that
occurred during the visit. Here, we treat the set of medical
codes associated with a visit as a sentence consisting of
words, except that there is no ordering in the words. Thus,
we adopt the word2vec approach to construct a vector to
represent each medical code.

FIGURE 2. The global attention model

be utilized to understand the importance of features such
that the models are more interpretable. The attention mech-
anism has been introduced to both Convolutional Neural
Networks (CNNs) and RNNs for various tasks and has
achieved many successes in the ﬁelds of computer vision and
NLP [11], [21], [22].

C. DEEP LEARNING IN EHR DATA
Previous studies on EHR data mainly use statistical meth-
ods or traditional machine learning techniques. Recently
researchers have started adapting deep learning approaches
to this data [23], [24], including textual notes, temporal
measurements of laboratory testing in the Intensive Care
Unit (ICU), and longitudinal data in patient populations.
Here, we summarize deep learning research in mining EHR
data and focus on the studies using RNN-based models.

Hospitalized patients, especially patients in ICUs, are
continuously monitored for cardiac, respiratory, and other
physical functions, creating a large volume of sequential data
in multiple dimensions. These measurements are utilized by
physicians to make diagnostic and treatment decisions. The
functions monitored may change over time and monitor-
ing may be irregular, based on a patient’s condition. It is
very challenging for traditional machine learning methods
to mine this multivariate time series data considering miss-
ing values, varying length, and irregular, non-simultaneous
sampling. Lipton et al. [25] trained an LSTM with a repli-
cated target to learn from these sequence data and used
this model to make predictions of diagnoses. The data used
in this research are time series of clinical measurements
with continuous values, and the LSTM models outperformed
logistic regression and MLP. Che et al. [26] developed a

4

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

are unlikely to contribute equally to the prediction of the tar-
get outcome, we cannot aggregate them with equal weights.
Instead, we employ a self-attention mechanism which trains
the network to learn the weights.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of subsequences with embedded medical
codes, we are able to input it into a recurrent neural network
to capture the temporal dependencies between events. How-
ever, the subsequences of visits are not contributing equally
to the outcome. Hence, we employ another level of attention
to learn the weights of the subsequences by the network itself
for the outcome prediction.

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the learned weights and hidden outputs, we aggregate
them into one universal vector for a comprehensive represen-
tation. In this step, the static information, such as age, gender,
previous hospitalization history is added as extra features, to
get a complete representation of a patient.

E. PREDICTING OUTCOME
Given the complete vector representation of a patient’s EHR
data, we add a logistic regression layer at the end for the
prediction of outcome.

IV. PATIENT2VEC REPRESENTATION LEARNING
ALGORITHM
In this section, we present the details of the proposed rep-
resentation learning framework, which is based on a GRU
network and a hierarchical attention mechanism. Figure 4
presents the structure of the proposed network with attention.

The proposed framework consists of ﬁve parts presented in
the following: I) Learning vector representations of medical
codes, II) Learning within-subsequence self-attention, III)
Learning subsequence-level self-attention, IV) Constructing
aggregated deep representation, V) Predicting outcome.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES
Given a patient’s raw EHR data, a sequence of visits, we
observe that a visit usually contains multiple medical codes.
Hence, it is feasible to learn a vector to represent the medical
code by capturing the relationships between the codes. In
this work, we employ the classical word2vec algorithm, skip-
gram. The basic idea of skip-gram is to learn a vector to
represent each word such that the probability of the context
to predict based on the target word is maximized. Hence,
the vectors of similar words are close to each other in the
learned feature space. In the skip-gram model, the vectors
are learned by training a shallow neural network to predict the
context words given an input word. Similarly, in our problem,

FIGURE 3. The Patient2Vec representation learning framework

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Clinical visits are represented as the set of vectors for the
codes associated with the visit. Because closely-spaced visits
are usually related clinically, we employ a time window to
split the sequence of visits into multiple subsequences of
equal length. A subsequence might contain multiple visits if
they occurred within the same time window, or there might
be no visits during a particular time window yielding an
empty subsequence. Thus we transform the original sequence
of irregularly-spaced visits into a sequence of subsequences
with equal intervals, which is preferable for recurrent neural
networks. The width of the subsequence window deﬁnes the
time granularity of the method and its optimal width is related
to the acuity (i.e., stability) of the clinical characteristics
involved in the predication task. In future work it may be
possible to deﬁne the relationship between clinical acuity and
optimal subsequence width, or develop methods for learning
an optimal width for a deﬁned prediction task.

Because all medical events occurring within a subsequence

VOLUME 0, 2018

5

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 4. A graphical illustration of the network in the Patient2Vec representation learning framework

the input is a medical code and the target to predict are the
medical codes occurred in the same visit.

Hence, each subsequence is a matrix consisting of the
vectors of medical codes occurred during this associated time
window.

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Given a sequence of subsequences encoded by vectors of
medical codes, this step employs the within-subsequence
attention which allows the network itself to learn the weights
of vectors in the subsequence according to its contribution to
the prediction target.

Here, we denote the sequence of patient i as s(i), and v(i)
t

t

, · · · , v(i)

1 , · · · , v(i)

denotes the tth subsequence in sequence s(i), where t ∈
{1, 2, · · · , T }. Thus, s(i) = {v(i)
T }. To
simplify the notation, we omit i in the following explanation.
Subsequence vt ∈ Rn×d is a matrix of medical codes such
that vt = {vt1 , vt2 , · · · , vtj , · · · , vtn }, where vtj ∈ Rd
is the vector representation of the jth medical code in the
tth subsequence vt and there are n medical codes in a
subsequence. In real EHR data, it is very likely that the
numbers of medical codes in each visit or time window are
different, thus, we utilize the padding approach to obtain a
consistent matrix dimensionality in the network.

To assign attention weights, we utilize the one-side con-
volution operation with a ﬁlter ωα ∈ Rd and a nonlinear
activation function. Thus, the weight vector αt is generated

6

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

for medical codes in the subsequence vt, presented in Equa-
tion 13.

αt = tanh(Conv(ωα, vt))

(13)

where αt = {αt1, αt2, · · · , αtn }, and ωα ∈ Rd is the
weight vector of the ﬁlter. The convolution operation Conv
is presented in Equation 14.

˜αtj = (ωα)(cid:124)vtj + bα

(14)

where bα is a bias term. Then, given the original matrix vt
and the learned weights αt, an aggregated vector xt ∈ Rd is
constructed to represent the tth subsequence, presented in 15.

xt =

αtj vtj

n
(cid:88)

j=1

(15)

Given Equation 15, we obtain a sequence of vectors, x =
{x1, x2, · · · , xt, · · · , xT }, to represent a patient’s medical
history.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of embedded subsequences, this step em-
ploys the subsequence-level attention which allows the net-
work itself to learn the weights of subsequences according to
their contribution to the prediction target.

To capture the longitudinal dependencies, we utilize a

bidirectional GRU-based RNN, presented in Equations 16.

h1, · · · , ht, · · · , hT = GRU (x1, · · · , xt, · · · , xT )

(16)

where ht ∈ Rk represents the output by the GRU unit at
the tth subsequence. Then, we introduce a set of linear and
softmax layers to generate M hops of weights β ∈ RM ×T
for subsequences. Then, for the hop m

γmt = (wβ

m)(cid:124)ht + bβ

βm1 , · · · , βmT =

(17)

(18)

sof tmax(γm1 , · · · , γmt, · · · , γmT )

m ∈ Rk. Thus, with the subsequence-level weights
where wβ
and hidden outputs, we construct a vector cm ∈ Rk to repre-
sent a patient’s medical visit history with one hop of subse-
quence weights, presented in the following Equation 19.

cm =

βmtht

(19)

T
(cid:88)

t=1

Then, a context vector c ∈ RM ×k is constructed by

concatenating c1, c2, · · · , cM .

VOLUME 0, 2018

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the context vector c, this step integrates the patients
characteristics a ∈ Rq into the context vector for a com-
plete vector representation of the patient’s EHR data. In
this research, the patient characteristics include demographic
information and some static medical conditions, such as age,
gender, and previous hospitalization. Thus, an aggregated
vector is constructed, c(cid:48) ∈ RM ×k+q, by adding a as addi-
tional dimensions to the context vector c.

E. PREDICTING OUTCOME
Given the vector representation of the complete medical
history and characteristics of patients, c(cid:48), we add a linear and
a softmax layer for the ﬁnal outcome prediction, as presented
in Equation 20.

ˆy = sof tmax(wc(cid:124)

c(cid:48) + bc)

(20)

To train the network, we use cross-entropy as the loss

function, presented in Equation 21.

L = −

yilog(ˆyi) + (1 − yi)log(1 − ˆyi)

(21)

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

+

||ββ(cid:124) − I||2
F

where N is the total number of observations. Here, yi is
a binary variable in classiﬁcation problems, while model
output ˆyi is real-valued. The second term in Equation 21 is
to penalize redundancy if the attention mechanism provides
similar subsequence weights for different hops of attention,
which is derived from [32]. This penalty term encourages the
multiple hops to focus on diverse areas and each hop focuses
on a small area.

Thus, we obtain a ﬁnal output for the prediction of out-
comes and a complete personalized vector representation of
the patient’s longitudinal EHR data.

V. EVALUATION
A. BACKGROUND
Although health care spending has been a relatively stable
share of the Gross Domestic Product (GDP) in the United
States since 2009, the costs of hospitalization, the largest
single component of health care expenditures,
increased
by 4.1% in 2014 [33]. Unplanned hospitalization is also
distressing and can increase the risk of related adverse events,
such as hospital-acquired infections and falls [34], [35].
Approximately 40% hospitalizations in the United King-
dom are unplanned and are potentially avoidable [36]. One
important form of unplanned hospitalization is hospital re-
admissions within 30 days of discharge, which is ﬁnancially
penalized in the United States. Early interventions targeted to
patients at risk of hospitalization could help avoid unplanned
admissions, reduce inpatient health care cost and ﬁnancial

7

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 5. A graphical illustration of the experimental setting for the risk
prediction of hospitalization

penalties for providers, and reduce emergency department
congestion [37].

In this research, we apply our proposed representation
learning framework to the risk prediction of future hospi-
talization. Many studies have been conducted by researchers
to predict the risk of 30-day readmission, or the admission
risk of a particular population, such as patients with Am-
bulatory Care Sensitive Conditions (ACSCs), patients with
heart failure, etc. [38]–[41]. Here, we focus on the general
population and the objective is to predict the risk of all-cause
hospitalization using longitudinal EHR data.

B. EXPERIMENTAL DESIGN
In this research, we use de-identiﬁed EHR data from the
University of Virginia Health System covering 75 months be-
ginning in September 2010. This dataset contains 2,343,651
inpatient and outpatient visits of 473,915 distinct patients.
We extracted visit data with diagnosis, medication, and pro-
cedure codes.

We deﬁned the observation window and prediction period
to validate the proposed method. We ﬁrst extract all patients
with a medical record of at least 1.5 years, where the ﬁrst year
is the observation window and the medical records in this
time window are used for feature construction. The follow-
ing 6 months is the hold-off period for the purpose of early
detection. For the positive class, we take all patients who
have hospitalization after the ﬁrst 1.5 years in their medical
history, while the negative class consists of patients who have
no hospitalization after 1.5 years. To better illustrate the ex-
perimental setting, we present the observation window, hold-
off and onset of outcome event in Figure 5. Here, the medical
codes include diagnosis, medication, and procedure codes,
and a vector representation is learned for each code. In this
dataset, diagnoses are primarily coded in ICD-9 and a small
portion is ICD-10 codes, while procedures are mainly using
CPT codes with a few ICD-9 procedure codes. The codes of
medications are using the pharmaceutical categories. Overall,
there are 94 distinct medication categories, 34,419 distinct
diagnoses codes, and 7,895 distinct procedure codes in the
EHR data. The dimension of the learned vectors of medical
codes is set to 100. Medical codes that appear in less than 50
patients medical records are excluded as rare events.

To construct the subsequences of medical codes, we use l
days as the time window. Figure 6 presents the cumulative
histogram and density plot of the numbers of visits in the
observation window, and we observe that the majority of

FIGURE 6. The cumulative histogram and density plot of patients’ numbers of
visits

patients have a small number of visits during the observation
window (less than 25% of patients have more than 4 visits).
Thus, we set l to 90 days, which split the observation window
into 4 subsequences.

Within each subsequence, the number of distinct medical
codes were computed and patients with more medical codes
in a subsequence than the 95% quantile were excluded from
the dataset. Overall, there are 8,841 and 89,101 patients in
the target and control groups, respectively. Each group is
randomly split into training, validation and testing sets with
a 7:1:2 ratio. Thus, 70% are used for training, another 20%
is used for testing, and the rest 10% are used for parameter
tuning and early stopping. The stochastic gradient descent
algorithm is used in training to minimize the cross-entropy
loss function, shown in Equation 21.

To evaluate the proposed representation learning frame-
work, we compare the prediction performance of the pro-
posed model with baseline approaches as follows.

1) Logistic regression (LR)

The inputs are the aggregated counts of grouped medical
codes over the entire observation window. Since the di-
mensionality of raw medical codes is huge, AHRQ clini-
cal classiﬁcations of diagnoses and procedures are used to
achieve a more general clustering of medical codes [42].
The medication codes are the pharmaceutical classes. Fur-
thermore, patient characteristics and previous inpatient visit
are also considered, where age and gender are demographic
information, and a binary indicator is utilized to represent
the presence of the previous hospitalization. Hence, the input
is a 436-dimensional vector representing a patient’s medical
history and characteristics.

8

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 1. The predictive performance of baselines and the proposed Patient2Vec framework

Methods
LR
MLP
RETAIN
FRNN-MGE
BiRNN-MGE
FRNN-MVE
BiRNN-MVE
Patient2Vec

Sensitivity
0.637 ± 0.010
0.727 ± 0.013
0.553 ± 0.012
0.636 ± 0.012
0.600 ± 0.012
0.753 ± 0.011
0.724 ± 0.010
0.769 ± 0.010

Speciﬁcity
0.728 ± 0.003
0.617 ± 0.004
0.710 ± 0.003
0.739 ± 0.004
0.777 ± 0.003
0.676 ± 0.004
0.707 ± 0.003
0.694 ± 0.004

AUC
0.721 ± 0.006
0.713 ± 0.007
0.663 ± 0.007
0.759 ± 0.006
0.768 ± 0.007
0.785 ± 0.006
0.788 ± 0.005
0.799 ± 0.005

F2 score
0.434 ± 0.006
0.423 ± 0.007
0.370 ± 0.008
0.438 ± 0.009
0.439 ± 0.009
0.470 ± 0.008
0.473 ± 0.008
0.492 ± 0.007

2) Multi-layer perceptron (MLP)
A multi-layer perceptron is trained to predict hospitalization
using the same inputs for logistic regression. Here, we use a
one hidden layer MLP with 256 hidden nodes.

3) Forward RNN with medical group embedding
(FRNN-MGE)
We split the sequence into subsequences with equal interval l.
The input at each step is the counts of medical groups within
the associated time interval, and the patient characteristics are
appended as additional features in the ﬁnal logistic regression
step. Here, the RNN is a forward GRU (or LSTM [18]) with
one hidden layer and the size of the hidden layer is 256.

4) Bidirectional RNN with medical group embedding
(BiRNN-MGE)
The inputs used for this baseline is the same as the one for
the FRNN-MGE [15]. The RNN used here is a bidirectional
GRU with one hidden layer and the size of the hidden layer
is 256.

5) Forward RNN with medical vector embedding
(FRNN-MVE)
We split the sequence into subsequences with equal interval l.
The input at each step is the vector representation of the
medical codes within the associated time interval, and the
patient characteristics are appended as additional features in
the ﬁnal logistic regression step. Here, the RNN is a forward
GRU (or LSTM [28]) with one hidden layer and the size of
the hidden layer is 256.

6) Bidirectional RNN with medical vector embedding
(BiRNN-MVE)
The inputs used for this baseline is the same as the one for
the FRNN-MVE [25]. The RNN used here is a bidirectional
GRU or LSTM [15] with one hidden layer and the size of the
hidden layer is 256.

7) RETAIN
This model uses reverse time attention mechanism on RNNs
for an interpretable representation of patient’s EHR data [31].
The inputs are the same as the one for FRNN-MGE, which
takes the counts of medical grouping within each time inter-
val to construct features. Similarly, the two RNNs used for

generating weights are GRU-based and the size of the hidden
layers are 256.

8) Patient2Vec
The inputs are the same as that for FRNN-MVE. One ﬁlter
is used when generating weights for within-subsequence
attention, and three ﬁlters are used for subsequence-level
attention. Similarly, the RNN used here is GRU-based and
there is one hidden layer and the size of the hidden layer
is 256.

The inputs of all baselines and Patient2Vec are normal-
ized to have zero mean and unit variance. We model the
risk of hospitalization based on Patient2Vec and baseline
representations of patients’ medical histories, and the model
performance is evaluated with Area Under Curve(AUC),
sensitivity, speciﬁcity, and F2-score. The validation set is
used for parameter tuning and early stopping in the train-
ing process. Each experiment is repeated 20 times and we
calculate the averages and standard deviations of the above
metrics, respectively.

C. EXPERIMENTAL RESULTS
The predictive performance of Patient2Vec and baselines are
presented in Table 1. The results shown here for the RNN-
based models are based on time interval l = 90 days to
construct subsequences.

According to Table 1, the RNN-based models are generally
capable of achieving higher prediction performance in terms
of sensitivity, AUC and F2 score, except for the RNN models
based on medical group embedding which have lower sensi-
tivity. Among all RNN-based approaches, the ones based on
vector embedding outperform those based on medical group
embedding in terms of sensitivity, AUC, and F2 score. The
bidirectional RNN models generally have higher speciﬁcity
but lower sensitivity than the forward RNN models, while
the bidirectional ones have comparable AUC and F2 score
with the forward ones, respectively. Generally, the proposed
Patient2Vec framework outperforms the baseline methods,
especially in terms of sensitivity and F2 score.

D. VISUALIZATION & INTERPRETATION
In addition to predictive performance, we interpret
the
learned representation by understanding the relative impor-
tance of clinical events in a patient’s EHR data. Considering

VOLUME 0, 2018

9

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 7. The heat map showing feature importance for Patient A

FIGURE 8. The proﬁle of Patient A.

FIGURE 9. The proﬁle of Patient B.

the feature importance learned by Patient2Vec are person-
alized for an individual patient, we illustrate it with two
example patients. Figures 8 and 9 present the proﬁles of
two individuals, Patient A and Patient B, respectively. To
facilitate the interpretation, instead of using raw medical
codes, we present the clinical groups from the AHRQ clinical
classiﬁcation software on diagnoses and procedure codes, as
well as pharmaceutical groups for medications.

According to Figure 8, Patient A is a male patient who
has hospitalization history in the observation window and
is admitted to the hospital seven months after the end of
the observation window for congestive heart failure. The

predicted risk is 96.4%, while the risk decreases for female
patients or patients without hospitalization history. It is also
not surprising to observe an increased risk for older patients.
The heat map in Figure 7 shows the relative importance of
the medical events in this patient’s medical record at each
time window and the ﬁrst row of the heat map presents
the subsequence-level attention. The darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the last subsequence,
t4, is the most important with respect to hospitalization risk,
followed by t1, t2, and t3 in order of importance.

Among all the clinical events in the subsequence t4, we

10

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 2. The top clinical groups with high weights in hospitalized patients

Index

Clinical Groups

Diagnoses
1
2
3

Essential hypertension
Other connective tissue disease
Spondylosis; intervertebral disc disorders; other back
problems
Other lower respiratory disease
Disorders of lipid metabolism
Other aftercare
Diabetes mellitus without complication
Screening and history of mental health and substance
abuse codes
Other nervous system disorders
Other screening for suspected conditions (not mental
disorders or infectious disease)

Other OR therapeutic procedures on nose; mouth and
pharynx
Suture of skin and subcutaneous tissue
Other therapeutic procedures on eyelids; conjunctiva;
cornea

Laboratory - Chemistry and hematology
Other laboratory
Other OR therapeutic procedures of urinary tract
Other OR procedures on vessels other than head and
neck

Therapeutic radiology for cancer treatment

Procedures
1

9
10

4
5
6
7
8

2
3

4
5
6
7

8

2

Medications
1

Diagnostic Products

Analgesics-Narcotic

observe that the OR therapeutic procedures (nose, mouth,
and pharynx), laboratory (chemistry and hematology), coro-
nary atherosclerosis & other heart disease, cardiac dys-
rhythmias, and conduction disorders are the ones with the
highest weights, while other events such as other connected
tissue disease are less important in terms of future hospi-
talization risk. Additionally, some medications appear to be
informative as well, including beta blockers, antihyperten-
sives, anticonvulsants, anticoagulants, etc. In the ﬁrst-time
window, the medical events with high weights are coro-
nary atherosclerosis & other heart disease, gastrointestinal
hemorrhage, deﬁciency and anemia, and other aftercare. In
the next subsequence, the most important medical events
are heart diseases and related procedures such as coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
conduction disorders, hypertension with complications, other
OR heart procedures, and other OR therapeutic nervous
system procedures. We also observe that the kidney disease
related diagnoses and procedures appear to be important
features. Throughout the observation window, the coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
and conduction disorders constantly show high weights with
respect to hospitalization risk, and the ﬁndings are consistent

FIGURE 10. The heat map showing feature importance for Patient B

with medical literature.

Figure 9 presents the proﬁle of Patient B, which is a male
patient without hospitalization in the observation window.
This patient is hospitalized for occlusion of cerebral arter-
ies approximately one year after the observation window,
and the predicted risk is 74.6%. For a similar patient who
is 10 years older or with previous hospitalization history,
the risk increases by 4.2% and 1%, respectively, while there
is a smaller risk of hospitalization for a female patient. To
illustrate the medical events of Patient B, the heat map in
Figure 10 depicts the relative importance of medical groups
in the subsequences, as well as the subsequence-level weights
for hospitalization risk. Similarly, the darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the second subse-
quence appears to be the most important, while the last one is
less predictive of future hospitalization. In fact, the medical
events in the last time window are spondylosis, intervertebral
disc disorders, other back problems and other bone disease &
musculoskeletal deformities, and malaise and fatigue, which
are not highly related to the cause of hospitalization of Patient
B.

In the most predictive subsequence, t2, we observe that
other OR heart procedures, genitourinary symptoms, spondy-
losis, intervertebral disc disorders, other back problems,
therapeutic procedures on eyelid, conjunctiva, and cornea,
and arterial blood gases have high attention weights. In the

VOLUME 0, 2018

11

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 3. The top diagnosis groups with high weights in patients hospitalized
for osteoarthritis, septicemia, acute myocardial infarction, congestive heart
failure, and diabetes mellitus with complications, respectively

Index

Diagnosis Groups

In patients admitted for osteoarthritis

Osteoarthritis
Other connective tissue disease

1
2

3
4

5

1
2

3
4
5

1
2
3

4
5

1
2
3
4
5

1
2
3

4
5

Other non-traumatic joint disorders
Spondylosis; intervertebral disc disorders; other back prob-
lems
Other aftercare

In patients admitted for septicemia

Essential hypertension
Diabetes mellitus without complication

Disorders of lipid metabolism
Other lower respiratory disease
Other aftercare

In patients admitted for acute myocardial infarction

Coronary atherosclerosis and other heart disease
Medical examination/evaluation
Other screening for suspected conditions (not mental disorders
or infectious disease)

Other lower respiratory disease
Disorders of lipid metabolism

In patients admitted for congestive heart failure

Congestive heart failure (nonhypertensive)
Coronary atherosclerosis and other heart disease
Cardiac dysrhythmias
Diabetes mellitus without complication
Other lower respiratory disease

In patients admitted for diabetes mellitus with complications

Diabetes mellitus with complications
Diabetes mellitus without complication
Other aftercare

Other nutritional; endocrine; and metabolic disorders
Fluid and electrolyte disorders

earliest time window, the most important medical events also
include therapeutic procedures on eyelid, conjunctiva, and
cornea, arterial blood gases, while diabetes, hypertension
as well as diagnostic products show their relatively high
importance. Throughout the observation window, medical
events spondylosis, intervertebral disc disorders, other back
problems, therapeutic procedures on eyelid, conjunctiva, and
cornea are constantly with high attention weights. Here, di-
agnostic products is a medication class, which include barium
sulfate, iohexol, gadopentetate dimeglumine, iodixanol, tu-
berculin puriﬁed protein derivative, iodixanol, regadenoson,
acetone (urine), and so forth. These medications are primarily
for blood or urine testing, or used as radiopaque contrast
agents for x-rays or CT scans for diagnostic purposes.

Additionally, we attempt to interpret the learned repre-
sentation and feature importance at the population-level. In
Table 2, we present the top 20 clinical groups with high
weights among hospitalized patients in the test set.

According to Table 2, the most predictive diagnosis groups
for future hospitalization are chronic diseases, including
essential hypertension, diabetes, lower respiratory disease,
disorders of lipid metabolism, and musculoskeletal diseases
such as other connective tissue disease and spondylosis,
intervertebral disc disorders, other back problems. The most
important procedures are some OR therapeutic procedures
and laboratory tests, such as the OR procedures on nose,
mouth, and pharynx, vessels, urinary tract, eyelid, conjunc-
tiva, cornea, etc. It is not surprising to see that diagnostic
products are showing with high weights, considering these
medications are used in testing or examinations for diagnos-
tic purposes.

Moreover, we present the top diagnoses groups with high
weights in patients hospitalized for different primary causes.
Table 3 shows the top 5 diagnosis groups with high weights
in patients admitted for osteoarthritis, septicemia (except in
labor), acute myocardial infarction, congestive heart failure
(nonhypertensive), and diabetes mellitus with complications,
respectively. Accordingly, we observe that the most impor-
tant diagnoses for hospitalization risk prediction in popu-
lation admitted for osteoarthritis are musculoskeletal dis-
eases such as connective tissue disease, joint disorders, and
spondylosis. However, the diagnoses with highest weights
in the patients admitted for septicemia are chronic diseases
including essential hypertension, diabetes, disorders of lipid
metabolism, and respiratory disease. The top diagnoses have
many overlaps between the populations admitted for acute
myocardial infarction and for congestive heart failure, con-
sidering both populations are admitted for heart diseases.
the overlapped diagnosis groups include coronary
Here,
atherosclerosis and other heart diseases and lower respiratory
diseases. As for patients admitted for diabetes with com-
plications, the top diagnoses are diabetes with or without
complications, nutritional, endocrine, metabolic disorders,
and ﬂuid and electrolyte disorders. In general, the learned
feature importance is consistent with medical literature.

VI. DISCUSSION
Our proposed framework is applied to the prediction of
hospitalization using real EHR data that demonstrates its
prediction accuracy and interpretability. This work could be
further enhanced by incorporating the follow-up information
on the negative patient population and investigate if it in-
deed shows an improved health outcome or the patient is
hospitalized elsewhere. Patient2Vec employs a hierarchical
attention mechanism, allowing us to directly interpret the
weights of clinical events. In future work, we will extend the
attention to incorporate demographic information for a more
comprehensive and automatic interpretation.

Although we apply Patient2Vec to the early detection of
long-term hospitalization, i.e., at least 6 months after the
previous hospitalization, it could be used to predict the risk
of 30-day readmission to help prevent unnecessary rehospi-
talizations.

12

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

VII. CONCLUSION
In this paper, we propose a representation learning frame-
work, Patient2Vec, to learn a personalized interpretable deep
representation of EHR data based on recurrent neural net-
works and the attention mechanism. This work improves
the performance of predictive models as well as deepens
the understanding of disease correlations. We apply this
framework to the risk prediction of hospitalization using
patients’ longitudinal EHR data. The experimental results
demonstrate that the proposed Patient2Vec representation is
capable of achieving a more accurate prediction than base-
lines approaches. Moreover, the learned feature importance
in the representations are interpreted both at the individual
and population levels to facilitate clinical insights.

In this work, the proposed Patient2Vec framework is eval-
uated with the risk prediction of all-cause hospitalization,
but in the future could be applied to predict hospitalization
in more speciﬁc populations, other health related prediction
problems, or domains outside of health.

REFERENCES
[1] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical
attention networks for document classiﬁcation,” in Proceedings of the
2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, 2016, pp.
1480–1489.

[2] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,” arXiv

preprint arXiv:1408.5882, 2014.

[3] J. Howard and S. Ruder, “Fine-tuned language models for text classiﬁca-

tion,” arXiv preprint arXiv:1801.06146, 2018.

[4] M. M. Lopez and J. Kalita, “Deep learning applied to nlp,” arXiv preprint

arXiv:1703.03091, 2017.

[5] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using
rnn encoder-decoder for statistical machine translation,” arXiv preprint
arXiv:1406.1078, 2014.

[6] A. L. Nobles, J. J. Glenn, K. Kowsari, B. A. Teachman, and L. E. Barnes,
“Identiﬁcation of imminent suicide risk among young adults using text
messages,” in Proceedings of the 2018 CHI Conference on Human Factors
in Computing Systems. ACM, 2018, p. 413.

[7] K. Kowsari, D. E. Brown, M. Heidarysafa, K. J. Meimandi, M. S. Gerber,
and L. E. Barnes, “Hdltex: Hierarchical deep learning for text classiﬁca-
tion,” in 2017 16th IEEE International Conference on Machine Learning
and Applications (ICMLA), Dec 2017, pp. 364–371.

[8] K. Kowsari, M. Heidarysafa, D. E. Brown, K. Jafari Meimandi, and L. E.
Barnes, “Rmdl: Random multimodel deep learning for classiﬁcation.”
ACM, 2018.

[9] H. Strobelt, S. Gehrmann, H. Pﬁster, and A. M. Rush, “Lstmvis: A tool
for visual analysis of hidden state dynamics in recurrent neural networks,”
IEEE transactions on visualization and computer graphics, vol. 24, no. 1,
pp. 667–676, 2018.

[10] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” Scientiﬁc
reports, vol. 8, no. 1, p. 6085, 2018.

[11] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches
preprint

neural machine

translation,”

arXiv

attention-based
to
arXiv:1508.04025, 2015.

[12] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[13] D. Yogatama, C. Dyer, W. Ling, and P. Blunsom, “Generative and discrim-
inative text classiﬁcation with recurrent neural networks,” arXiv preprint
arXiv:1703.01898, 2017.

[14] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent

trends
language processing,” arXiv preprint

in deep learning based natural
arXiv:1708.02709, 2017.

[15] M. Basaldella, E. Antolli, G. Serra, and C. Tasso, “Bidirectional lstm
recurrent neural network for keyphrase extraction,” in Italian Research

Conference on Digital Libraries, Springer.
lishing, 2018, pp. 180–187.

Springer International Pub-

[16] S. Ghosh, O. Vinyals, B. Strope, S. Roy, T. Dean, and L. Heck, “Con-
textual lstm (clstm) models for large scale nlp tasks,” arXiv preprint
arXiv:1602.06291, 2016.

[17] B. Yue, J. Fu, and J. Liang, “Residual recurrent neural networks for
learning sequential representations,” Information, vol. 9, no. 3, p. 56, 2018.
[18] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
gated recurrent neural networks on sequence modeling,” arXiv preprint
arXiv:1412.3555, 2014.

[19] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difﬁculty of training
recurrent neural networks.” ICML (3), vol. 28, pp. 1310–1318, 2013.
[20] D. Britz, “Recurrent neural network tutorial,” http://www.wildml.com/

2015/10/, 2015, [Accessed on October 5, 2017].

[21] V. Mnih, N. Heess, A. Graves et al., “Recurrent models of visual attention,”
in Advances in neural information processing systems, 2014, pp. 2204–
2212.

[22] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for
abstractive sentence summarization,” arXiv preprint arXiv:1509.00685,
2015.

[23] T. Ma, C. Xiao, and F. Wang, “Health-atm: A deep architecture for
multifaceted patient health record representation and risk prediction,” in
Proceedings of the 2018 SIAM International Conference on Data Mining.
SIAM, 2018, pp. 261–269.

[24] A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J. Liu,
X. Liu, J. Marcus, M. Sun et al., “Scalable and accurate deep learning with
electronic health records,” npj Digital Medicine, vol. 1, no. 1, p. 18, 2018.
[25] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzell, “Learning to diagnose
with lstm recurrent neural networks,” arXiv preprint arXiv:1511.03677,
2015.

[26] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” arXiv preprint
arXiv:1606.01865, 2016.

[27] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghas-
semi, B. Moody, P. Szolovits, L. A. Celi, and R. G. Mark, “Mimic-iii, a
freely accessible critical care database,” Scientiﬁc data, vol. 3, 2016.
[28] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart, and J. Sun, “Doctor
ai: Predicting clinical events via recurrent neural networks,” in Machine
Learning for Healthcare Conference, 2016, pp. 301–318.

[29] Z. Che, S. Purushotham, R. Khemani, and Y. Liu, “Interpretable deep
models for icu outcome prediction,” in AMIA Annual Symposium Pro-
ceedings, vol. 2016. American Medical Informatics Association, 2016,
p. 371.

[30] J. H. Friedman, “Greedy function approximation: a gradient boosting

machine,” Annals of statistics, pp. 1189–1232, 2001.

[31] E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart,
“Retain: An interpretable predictive model for healthcare using reverse
time attention mechanism,” in Advances in Neural Information Processing
Systems, 2016, pp. 3504–3512.

[32] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Ben-
gio, “A structured self-attentive sentence embedding,” arXiv preprint
arXiv:1703.03130, 2017.

[33] C. M. Torio and B. J. Moore, “National inpatient hospital costs: The most
expensive conditions by payer, 2013,” https://www.hcup-us.ahrq.gov/
reports/statbriefs/sb204-Most-Expensive-Hospital-Conditions.jsp, 2016.
[34] E. Wallace, E. Stuart, N. Vaughan, K. Bennett, T. Fahey, and S. M.
Smith, “Risk prediction models to predict emergency hospital admission
in community-dwelling adults: a systematic review,” Medical care, vol. 52,
no. 8, p. 751, 2014.

[35] E. N. de Vries, M. A. Ramrattan, S. M. Smorenburg, D. J. Gouma, and
M. A. Boermeester, “The incidence and nature of in-hospital adverse
events: a systematic review,” Quality and safety in health care, vol. 17,
no. 3, pp. 216–223, 2008.

[36] S. Purdey and A. Huntley, “Predicting and preventing avoidable hospital
admissions: a review.” The journal of the Royal College of Physicians of
Edinburgh, vol. 43, no. 4, pp. 340–344, 2012.

[37] H. Ontario, “Early identiﬁcation of people at risk of hospitalization: Hos-
pital admission risk prediction (harp)-a new tool for supporting providers
and patients,” 2013.

[38] B. Zheng, J. Zhang, S. W. Yoon, S. S. Lam, M. Khasawneh, and S. Poranki,
“Predictive modeling of hospital readmissions using metaheuristics and
data mining,” Expert Systems with Applications, vol. 42, no. 20, pp. 7110–
7120, 2015.

VOLUME 0, 2018

13

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

[39] D. Kansagara, H. Englander, A. Salanitro, D. Kagen, C. Theobald,
M. Freeman, and S. Kripalani, “Risk prediction models for hospital
readmission: a systematic review,” The Journal of the American Medical
Association, vol. 306, no. 15, pp. 1688–1698, 2011.

[40] G. Giamouzis, A. Kalogeropoulos, V. Georgiopoulou, S. Laskar, A. L.
Smith, S. Dunbar, F. Triposkiadis, and J. Butler, “Hospitalization epidemic
in patients with heart failure: risk factors, risk prediction, knowledge gaps,
and future directions,” Journal of cardiac failure, vol. 17, no. 1, pp. 54–75,
2011.

[41] E. Prescott, A. M. Bjerg, P. K. Andersen, P. Lange, and J. Vestbo, “Gender
difference in smoking effects on lung function and risk of hospitalization
for COPD: results from a danish longitudinal population study,” European
Respiratory Journal, vol. 10, no. 4, pp. 822–827, 1997.

[42] Agency for Healthcare Research and Quality (AHRQ), “Clinical classi-
ﬁcations software (CCS) for ICD-9-CM,” https://www.hcup-us.ahrq.gov/
toolssoftware/ccs/ccs.jsp, 2015.

JENNIFER M. LOBO (jem4yb@virginia.edu) is
an Assistant Professor of Biomedical Informatics
in the Department of Public Health Sciences at
University of Virginia. She received her Ph.D. in
Industrial Engineering from North Carolina State
University, Raleigh, NC. Her research interests in-
volve using mathematical modeling and stochastic
optimization methods to build models that simu-
late the natural course of disease. These models
allow for estimation of outcomes under different
screening and treatment policies in the absence of randomized controlled
trials, and can be used to optimize screening and treatment decisions for
patients with chronic diseases. Her projects include optimizing treatment
for patients with type 2 diabetes, generating individualized decision analysis
models for prostate cancer patients, and developing optimal imaging surveil-
lance guidelines for recurrent kidney cancer.

JINGHE ZHANG (jz4kg@virginia.edu) is a lead
data scientist at Target Corporation. She received
her Ph.D. in Systems Engineering from the Uni-
versity of Virginia. Prior to entering the Ph.D. pro-
gram at UVA, she received the Master of Science
in Industrial and Systems Engineering from the
State University of New York at Binghamton. Her
research interests are in natural language process-
ing, machine learning, recommender systems, and
health informatics.

LAURA E. BARNES (lb3dp@virginia.edu) is an
Associate Professor in Systems and Information
Engineering and the Data Science Institute at the
University of Virginia. She received her Ph.D. in
Computer Science from the University of South
Florida, Tampa, FL. She directs the Sensing Sys-
tems for Health (S2He) Lab which focuses on
understanding the dynamics and personalization
of health and well-being through mobile sensing
and analytics.

(kk7nc@virginia.edu) is
KAMRAN KOWSARI
a Ph.D. student in the Department of Systems
and Information Engineering at the University of
Virginia, Charlottesville, VA. He is a member of
the Sensing Systems for Health Lab. He received
his Master of Science from Department of Com-
puter Science at The George Washington Univer-
sity, Washington, DC. He has more than ten years
of experience in machine learning and software
development. His experience includes numerous
industrial and academic projects. His research interests include natural
language processing, machine learning, deep learning, artiﬁcial intelligence,
text mining, and unsupervised learning.

JAMES H. HARRISON, JR., (jhh5y@virginia.
edu) is Associate Professor of Pathology and Di-
rector of Laboratory Information Systems at the
University of Virginia Medical Center and also has
appointments in the Departments of Public Health
Sciences in the UVA School of Medicine, and
Systems and Information Engineering in the UVA
School of Engineering and Applied Sciences. He
received his MD and PhD (Pharmacology) de-
grees from Medical University of South Carolina,
Charleston, SC, completed residencies in Anatomic Pathology and Labora-
tory Medicine at Yale-New Haven Hospital, New Haven, CT, and completed
a postdoctoral fellowship in Environmental Toxicology at Yale University,
New Haven, CT. Dr. Harrison has over 25 years of experience in the ﬁeld
of medical informatics, including work in clinical laboratory information
systems, electronic health records, clinical data analysis, and clinical data
standards development.

14

VOLUME 0, 2018

Date of publication 10, 2018 , date of current version 10, 2018.

Digital Object Identiﬁer 10.1109/ACCESS.2018.2875677

Patient2Vec: A Personalized
Interpretable Deep Representation of the
Longitudinal Electronic Health Record

JINGHE ZHANG1, (Member, IEEE), KAMRAN KOWSARI1,4(Member, IEEE), JAMES H.
HARRISON2,3,5, JENNIFER M. LOBO2,5, and LAURA E. BARNES1,4,5, (Member, IEEE)
1Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, USA
2 Department of Public Health Sciences, University of Virginia, Charlottesville, VA 22904, USA
4 Sensing Systems for Health Lab, University of Virginia, Charlottesville, VA 22904, USA
3 Division of Laboratory Medicine Department of Pathology, University of Virginia, Charlottesville, VA 22904, USA
5Data Science Institute, University of Virginia, Charlottesville, VA 22904, USA
Corresponding author: Laura E. Barnes (lb3dp@virginia.edu)
This research was supported by a Jeffress Trust Award in Interdisciplinary Science.
Patient2Vec is shared as an open source tool at https://github.com/BarnesLab/Patient2Vec

ABSTRACT The wide implementation of electronic health record (EHR) systems facilitates the collection
of large-scale health data from real clinical settings. Despite the signiﬁcant increase in adoption of EHR
systems, this data remains largely unexplored, but presents a rich data source for knowledge discovery from
patient health histories in tasks such as understanding disease correlations and predicting health outcomes.
However, the heterogeneity, sparsity, noise, and bias in this data present many complex challenges.
This complexity makes it difﬁcult to translate potentially relevant information into machine learning
algorithms. In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep
representation of longitudinal EHR data which is personalized for each patient. To evaluate this approach,
we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive
performance with baseline methods. Patient2Vec produces a vector space with meaningful structure and it
achieves an AUC around 0.799 outperforming baseline methods. In the end, the learned feature importance
can be visualized and interpreted at both the individual and population levels to bring clinical insights.

INDEX TERMS Attention mechanism, gated recurrent unit, hospitalization, longitudinal electronic health
record, personalization, representation learning.

8
1
0
2
 
t
c
O
 
5
2
 
 
]

M
Q
.
o
i
b
-
q
[
 
 
3
v
3
9
7
4
0
.
0
1
8
1
:
v
i
X
r
a

I. INTRODUCTION

L ongitudinal EHR data resemble text documents from

many perspectives. A text document consists of a se-
quence of sentences, and a sentence is a sequence of words.
Similarly, the longitudinal health record of a patient consists
of a sequence of visits, and there is a list of clinical events,
including diagnoses, medications, and procedures, that occur
during a visit. Considering these similarities, representation
learning methods for text documents in Natural Language
Processing (NLP) have great potential to be applied to lon-
gitudinal EHR data.

Deep neural networks have become very popular in the
NLP ﬁeld and have been very successful in many applica-
tions, such as machine translation, question answering, text
classiﬁcation, document summarization, language modeling,
etc. [1]–[8]. These networks excel at complex language tasks

because they are capable of identifying high-order relation-
ships, the network structure can encode language structures,
and they allow the learning of a hierarchical representation
of the language, i.e., representations for tokens, phrases, and
sentences, etc.

Among a variety of deep learning methods, Recurrent
Neural Networks (RNNs) have shown their effectiveness in
NLP tasks because they have the ability to capture sequential
information [7]–[10] which is inherent in human language.
Traditional neural networks assume that inputs are inde-
pendent of each other, while an RNN computes the output
based on the current input as well as the “memory” from the
previous computation. Although vanilla RNNs are not good
at capturing long-term dependencies, many variants have
been proposed and validated that are effective in addressing

VOLUME 0, 2018

1

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

its performance with other baseline methods. In addition
to prediction performance, we further interpret the learned
representations with visualizations on example patients and
events. Finally, Section V provides a summary of this work.

II. RELATED WORK
In this section, we present an overview of a gated recurrent
unit, a type of RNN, which is capable of capturing long-term
dependencies. Then we brieﬂy introduce attention mecha-
nisms in neural networks that allow the network to attend
to certain regions of data, which is inspired by the visual
attention mechanism in humans. Additionally, we summarize
the RNN networks and attention mechanisms previously used
to mine EHR data.

A. RECURRENT NEURAL NETWORKS (RNN)

RNNs are expected to learn long-term dependencies by tak-
ing the previous state and the new input in the computation at
the current time step t. However, vanilla RNNs are incapable
of capturing the dependencies when the sequence is very long
due to the vanishing gradient problem [12]. Many variants of
the RNN network have been proposed to address this issue,
and long short term memory (LSTM) is one of the most
popular models used nowadays in NLP tasks [7], [8], [13]–
[16].

this issue.

In the medical domain, it is critical that analytical results
are interpretable, so that they can be understood and validated
by a human with expert knowledge and so that knowledge
captured by analysis can be used for process improvement.
Traditional deep neural networks have the disadvantage that
they lack interpretability. A substantial amount of work is
ongoing to make sense of the “black box”, and the attention
mechanism [11] is one of the more effective methods recently
developed to make the output of these algorithms more
interpretable.

Health care is undergoing unprecedented change, and there
is a great potential and demand for personalized care strate-
gies. Personalized medicine, also called precision medicine,
has previously focused on optimizing therapy to better ﬁt the
genetic makeup of the patient or the disease (e.g., the genetic
susceptibility of cancer to speciﬁc chemotherapy strategies).
The availability of EHR data and advances in machine learn-
ing create the potential for another type of personalization
of healthcare. This type of personalization has become ubiq-
uitous in our daily life. For example, customers have come
to expect personalized search on Google and personalized
product recommendations on Amazon and Netﬂix, based on
their charactersitics and previous experiences with the sys-
tems. Personalization of healthcare processes, based on a pa-
tient’s phenotype (physical and medical characteristics) and
healthcare experiences as documented in the health record,
may also improve "customer" satisfaction and it has the addi-
tional potential to improve healthcare efﬁciency, lower costs,
and yield better outcomes. We believe that representation
learning methods can capture a personalized representation
of the important heterogeneities in patients’ phenotypes and
medical histories at the population-level, and make these
representations available to drive healthcare decisions and
strategies.

This research is based on RNN models and the attention
mechanism with the objective of learning a personalized, in-
terpretable, and complete representation of patients’ medical
records. Our proposed framework is capable of learning a
personalized representation for each patient from a sequence
of clinical events. A hierarchical attention mechanism learns
personalized weights of clinical events, including hospital
visits and the procedures that they contain. These weights
allow us to interpret the relative importance and roles of clin-
ical events in the learned representations both at individual
and population levels. The ultimate goal is more accurate
prediction and better insight into the critical elements of
healthcare processes that can be used to improve healthcare
delivery.

The rest of this paper is organized as follows: Section II
summarizes the variants of RNNs and the attention mecha-
nism, as well as their application to EHR data. Section III
presents an overview of the proposed Patient2Vec repre-
sentation learning framework, and Section IV elaborates
the details of the algorithms. In Section V, the proposed
framework is evaluated for a prediction task and we compare

FIGURE 1. The top ﬁgure is a GRU gating unit and bottom ﬁgure shows an
LSTM unit [7]

2

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

1) Gated Recurrent Unit (GRU)
GRU is a simpliﬁed version of LSTM [7]. The basic idea
of GRU is to combat the vanishing gradient problem with a
gating mechanism. Hence the general recurrent structure in
GRU is identical to vanilla RNNs except that a GRU unit
is used in the computation at each time step rather than a
traditional simple recurrent unit.

In general, a GRU cell has two gates, i.e., a reset gate r
and an update gate z. The reset gate is used to determine
how to integrate the previous state into the computation of
the current state, while the update gate determines how much
the unit updates its activation.

Given the input xt at time step t, the reset gate rt is

computed as presented in Equation 1

rt = σ(Urxt + Wrst−1)

(1)

where Ur and Wr are the weight matrices of the reset gate
and st−1 is the hidden activation at time step t − 1. A similar
computation is performed for the update gate zt at time step t,
shown in Equation 2

zt = σ(Uzxt + Wzst−1)

(2)

where Uz and Wz are the weight matrices of update gate.
The current hidden activation ht is computed by

ht = (1 − zt)ht−1 + zt
(3)
where ˜ht is the candidate activation at time step t. The
computation of ˜ht is presented in Equation 4

˜ht

˜ht = tanh(Wxt + U(rt (cid:12) ht−1))

(4)

where U and W are weight matrices and (cid:12) represents
element-wise multiplication. Figure 1 presents a graphical
illustration of the GRU [7] and one unit of LSTM.

GRU is capable of learning long-term dependencies [17]
due to the additive component of update from t to t + 1
in the gating mechanism. Consequently, important features
will be carried forward in the input stream while irrelevant
information will be dropped. When the reset gate is 0, the
network is forced to drop previous states and reset with cur-
rent information. Moreover, the method provides shortcuts
such that the error is easily backpropagated without vanishing
too quickly [5], [18]. Hence, the GRU is well-suited to learn
long-term dependencies in sequence data.

2) Long Short-Term Memory (LSTM)
An LSTM unit is similar to a GRU, but with one more gate in
an LSTM unit (as shown in Figure 1). LSTM also preserves
long term dependencies more effectively than basic RNN.
This is particularly useful to overcome the vanishing gradient
problem [19]. Although LSTM has a chain-like structure sim-
ilar to RNN, LSTM uses multiple gates to carefully regulate
the amount of information that will be allowed into each node
state. Figure 1 shows the basic cell of an LSTM model. A step

VOLUME 0, 2018

by step explanation of an LSTM cell is as following:
Input gate:

it = σ(Wi[xt, ht−1] + bi),

Candid memory cell value:

˜Ct = tanh(Wc[xt, ht−1] + bc),

Forget gate activation:

ft = σ(Wf [xt, ht−1] + bf ),

New memory cell value:

Ct = it ∗ ˜Ct + ftCt−1,

Output gate value:

ot = σ(Wo[xt, ht−1] + bo),

ht = ot tanh(Ct),

(5)

(6)

(7)

(8)

(9)

(10)

In the above description all b represent bias vectors, all W
represent weight matrices, and xt is used as input to the
memory cell at time t. Also,the i, c, f, o indices refer to input,
cell memory, forget and output gates respectively. An RNN
can be biased when later words are more inﬂuential than the
earlier ones.

Empirically, LSTM and GRU achieve comparable per-
formance in many tasks but there are fewer parameters in
a GRU, which makes it a little faster to learn and able to
generalize with fewer data [20].

B. ATTENTION MECHANISM
Attention mechanisms, inspired by the visual attention sys-
tem found in humans, have become popular in deep learning.
Attention allows the network to focus on certain regions of
data, while perceiving other regions with “low resolution”.
In addition to higher accuracy, it also facilitates the interpre-
tation of learned representations. We elaborate an attention
mechanism on an RNN network, and Figure 2 presents a
graphical illustration.

According to Figure 2, a variable-length weight vector α
is learned based on hidden states [11]. Then a global context
vector is computed based on weights α and all the hidden
states to create the ﬁnal output. Equation 11 presents the
computation of the weight vector α = {α1, α2, · · · , αT },
where T is the length of the sequence

α1, α2, · · · , αT = f (Wαh + bα)

(11)

and where f is a nonlinear activation function, usually
sof tmax or tanh. Then, the context vector c is constructed
as:

c =

αtht

T
(cid:88)

t=1

Thus, the network puts more attention on the important
features for the ﬁnal prediction which can improve the model
performance. An additional beneﬁt is that the weights can

(12)

3

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

GRU-based model to address missing values in multivariate
time series data, in which the missing patterns are incorpo-
rated for improved prediction performance. This work has
been applied to the Medical Information Mart for Intensive
Care III (MIMIC-III) clinical database to demonstrate its
effectiveness in mining time series of clinical measurements
with missing values [27]. Longitudinal EHR data including
clinical events, such as diagnoses, medications, and pro-
cedures is also a potentially rich resource for predictive
modeling. Choi et al. [28] analyze this data with a GRU
network to forecast future clinical events, and it achieves a
better prediction performance than comparison models such
as logistic regression and MLP.

Difﬁculty in interpreting model behavior is one of the
major drawbacks of using deep learning to mine EHR data.
Some attempts have been made to address this issue. Che et
al. [29] propose an interpretable mimic learning method
which trains a mimic gradient boosting trees model to utilize
predicted labels or features learned by deep learning mod-
els for ﬁnal prediction [30]. Then the feature importances
learned by the tree-based models are used for knowledge
discovery. Attention mechanisms have been introduced re-
cently to improve the interpretability of the prediction results
of deep learning models in health analytics. Choi et al. [31]
develop an interpretable model with two levels of attention
weights learned from two reverse-time GRU models, re-
spectively. The experimental results on EHR data indicate
comparable prediction performance with conventional GRU
models but more interpretable results. Our work continues
the attempt to use attention mechanisms to improve the
interpretability of RNN-based models.

III. PATIENT2VEC SYSTEM MODEL
In this section, we provide an overview of the proposed
hierarchical representation learning framework. This frame-
work uses deep recurrent neural networks to capture the
complex relationships between clinical events in the patient’s
EHR data and employs the attention mechanism to learn
a personalized representation and to obtain relative feature
importance. The proposed representation learning framework
contains four steps and is presented graphically in Figure 3.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES

EHR data consists primarily of records of outpatient and
inpatient visits to healthcare providers. These visit records
include multiple clinical codes for diagnoses, symptoms,
procedures, therapies, and other observations and events that
occurred during the visit. Here, we treat the set of medical
codes associated with a visit as a sentence consisting of
words, except that there is no ordering in the words. Thus,
we adopt the word2vec approach to construct a vector to
represent each medical code.

FIGURE 2. The global attention model

be utilized to understand the importance of features such
that the models are more interpretable. The attention mech-
anism has been introduced to both Convolutional Neural
Networks (CNNs) and RNNs for various tasks and has
achieved many successes in the ﬁelds of computer vision and
NLP [11], [21], [22].

C. DEEP LEARNING IN EHR DATA
Previous studies on EHR data mainly use statistical meth-
ods or traditional machine learning techniques. Recently
researchers have started adapting deep learning approaches
to this data [23], [24], including textual notes, temporal
measurements of laboratory testing in the Intensive Care
Unit (ICU), and longitudinal data in patient populations.
Here, we summarize deep learning research in mining EHR
data and focus on the studies using RNN-based models.

Hospitalized patients, especially patients in ICUs, are
continuously monitored for cardiac, respiratory, and other
physical functions, creating a large volume of sequential data
in multiple dimensions. These measurements are utilized by
physicians to make diagnostic and treatment decisions. The
functions monitored may change over time and monitor-
ing may be irregular, based on a patient’s condition. It is
very challenging for traditional machine learning methods
to mine this multivariate time series data considering miss-
ing values, varying length, and irregular, non-simultaneous
sampling. Lipton et al. [25] trained an LSTM with a repli-
cated target to learn from these sequence data and used
this model to make predictions of diagnoses. The data used
in this research are time series of clinical measurements
with continuous values, and the LSTM models outperformed
logistic regression and MLP. Che et al. [26] developed a

4

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

are unlikely to contribute equally to the prediction of the tar-
get outcome, we cannot aggregate them with equal weights.
Instead, we employ a self-attention mechanism which trains
the network to learn the weights.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of subsequences with embedded medical
codes, we are able to input it into a recurrent neural network
to capture the temporal dependencies between events. How-
ever, the subsequences of visits are not contributing equally
to the outcome. Hence, we employ another level of attention
to learn the weights of the subsequences by the network itself
for the outcome prediction.

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the learned weights and hidden outputs, we aggregate
them into one universal vector for a comprehensive represen-
tation. In this step, the static information, such as age, gender,
previous hospitalization history is added as extra features, to
get a complete representation of a patient.

E. PREDICTING OUTCOME
Given the complete vector representation of a patient’s EHR
data, we add a logistic regression layer at the end for the
prediction of outcome.

IV. PATIENT2VEC REPRESENTATION LEARNING
ALGORITHM
In this section, we present the details of the proposed rep-
resentation learning framework, which is based on a GRU
network and a hierarchical attention mechanism. Figure 4
presents the structure of the proposed network with attention.

The proposed framework consists of ﬁve parts presented in
the following: I) Learning vector representations of medical
codes, II) Learning within-subsequence self-attention, III)
Learning subsequence-level self-attention, IV) Constructing
aggregated deep representation, V) Predicting outcome.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES
Given a patient’s raw EHR data, a sequence of visits, we
observe that a visit usually contains multiple medical codes.
Hence, it is feasible to learn a vector to represent the medical
code by capturing the relationships between the codes. In
this work, we employ the classical word2vec algorithm, skip-
gram. The basic idea of skip-gram is to learn a vector to
represent each word such that the probability of the context
to predict based on the target word is maximized. Hence,
the vectors of similar words are close to each other in the
learned feature space. In the skip-gram model, the vectors
are learned by training a shallow neural network to predict the
context words given an input word. Similarly, in our problem,

FIGURE 3. The Patient2Vec representation learning framework

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Clinical visits are represented as the set of vectors for the
codes associated with the visit. Because closely-spaced visits
are usually related clinically, we employ a time window to
split the sequence of visits into multiple subsequences of
equal length. A subsequence might contain multiple visits if
they occurred within the same time window, or there might
be no visits during a particular time window yielding an
empty subsequence. Thus we transform the original sequence
of irregularly-spaced visits into a sequence of subsequences
with equal intervals, which is preferable for recurrent neural
networks. The width of the subsequence window deﬁnes the
time granularity of the method and its optimal width is related
to the acuity (i.e., stability) of the clinical characteristics
involved in the predication task. In future work it may be
possible to deﬁne the relationship between clinical acuity and
optimal subsequence width, or develop methods for learning
an optimal width for a deﬁned prediction task.

Because all medical events occurring within a subsequence

VOLUME 0, 2018

5

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 4. A graphical illustration of the network in the Patient2Vec representation learning framework

the input is a medical code and the target to predict are the
medical codes occurred in the same visit.

Hence, each subsequence is a matrix consisting of the
vectors of medical codes occurred during this associated time
window.

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Given a sequence of subsequences encoded by vectors of
medical codes, this step employs the within-subsequence
attention which allows the network itself to learn the weights
of vectors in the subsequence according to its contribution to
the prediction target.

Here, we denote the sequence of patient i as s(i), and v(i)
t

t

, · · · , v(i)

1 , · · · , v(i)

denotes the tth subsequence in sequence s(i), where t ∈
{1, 2, · · · , T }. Thus, s(i) = {v(i)
T }. To
simplify the notation, we omit i in the following explanation.
Subsequence vt ∈ Rn×d is a matrix of medical codes such
that vt = {vt1 , vt2 , · · · , vtj , · · · , vtn }, where vtj ∈ Rd
is the vector representation of the jth medical code in the
tth subsequence vt and there are n medical codes in a
subsequence. In real EHR data, it is very likely that the
numbers of medical codes in each visit or time window are
different, thus, we utilize the padding approach to obtain a
consistent matrix dimensionality in the network.

To assign attention weights, we utilize the one-side con-
volution operation with a ﬁlter ωα ∈ Rd and a nonlinear
activation function. Thus, the weight vector αt is generated

6

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

for medical codes in the subsequence vt, presented in Equa-
tion 13.

αt = tanh(Conv(ωα, vt))

(13)

where αt = {αt1, αt2, · · · , αtn }, and ωα ∈ Rd is the
weight vector of the ﬁlter. The convolution operation Conv
is presented in Equation 14.

˜αtj = (ωα)(cid:124)vtj + bα

(14)

where bα is a bias term. Then, given the original matrix vt
and the learned weights αt, an aggregated vector xt ∈ Rd is
constructed to represent the tth subsequence, presented in 15.

xt =

αtj vtj

n
(cid:88)

j=1

(15)

Given Equation 15, we obtain a sequence of vectors, x =
{x1, x2, · · · , xt, · · · , xT }, to represent a patient’s medical
history.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of embedded subsequences, this step em-
ploys the subsequence-level attention which allows the net-
work itself to learn the weights of subsequences according to
their contribution to the prediction target.

To capture the longitudinal dependencies, we utilize a

bidirectional GRU-based RNN, presented in Equations 16.

h1, · · · , ht, · · · , hT = GRU (x1, · · · , xt, · · · , xT )

(16)

where ht ∈ Rk represents the output by the GRU unit at
the tth subsequence. Then, we introduce a set of linear and
softmax layers to generate M hops of weights β ∈ RM ×T
for subsequences. Then, for the hop m

γmt = (wβ

m)(cid:124)ht + bβ

βm1 , · · · , βmT =

(17)

(18)

sof tmax(γm1 , · · · , γmt, · · · , γmT )

m ∈ Rk. Thus, with the subsequence-level weights
where wβ
and hidden outputs, we construct a vector cm ∈ Rk to repre-
sent a patient’s medical visit history with one hop of subse-
quence weights, presented in the following Equation 19.

cm =

βmtht

(19)

T
(cid:88)

t=1

Then, a context vector c ∈ RM ×k is constructed by

concatenating c1, c2, · · · , cM .

VOLUME 0, 2018

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the context vector c, this step integrates the patients
characteristics a ∈ Rq into the context vector for a com-
plete vector representation of the patient’s EHR data. In
this research, the patient characteristics include demographic
information and some static medical conditions, such as age,
gender, and previous hospitalization. Thus, an aggregated
vector is constructed, c(cid:48) ∈ RM ×k+q, by adding a as addi-
tional dimensions to the context vector c.

E. PREDICTING OUTCOME
Given the vector representation of the complete medical
history and characteristics of patients, c(cid:48), we add a linear and
a softmax layer for the ﬁnal outcome prediction, as presented
in Equation 20.

ˆy = sof tmax(wc(cid:124)

c(cid:48) + bc)

(20)

To train the network, we use cross-entropy as the loss

function, presented in Equation 21.

L = −

yilog(ˆyi) + (1 − yi)log(1 − ˆyi)

(21)

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

+

||ββ(cid:124) − I||2
F

where N is the total number of observations. Here, yi is
a binary variable in classiﬁcation problems, while model
output ˆyi is real-valued. The second term in Equation 21 is
to penalize redundancy if the attention mechanism provides
similar subsequence weights for different hops of attention,
which is derived from [32]. This penalty term encourages the
multiple hops to focus on diverse areas and each hop focuses
on a small area.

Thus, we obtain a ﬁnal output for the prediction of out-
comes and a complete personalized vector representation of
the patient’s longitudinal EHR data.

V. EVALUATION
A. BACKGROUND
Although health care spending has been a relatively stable
share of the Gross Domestic Product (GDP) in the United
States since 2009, the costs of hospitalization, the largest
single component of health care expenditures,
increased
by 4.1% in 2014 [33]. Unplanned hospitalization is also
distressing and can increase the risk of related adverse events,
such as hospital-acquired infections and falls [34], [35].
Approximately 40% hospitalizations in the United King-
dom are unplanned and are potentially avoidable [36]. One
important form of unplanned hospitalization is hospital re-
admissions within 30 days of discharge, which is ﬁnancially
penalized in the United States. Early interventions targeted to
patients at risk of hospitalization could help avoid unplanned
admissions, reduce inpatient health care cost and ﬁnancial

7

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 5. A graphical illustration of the experimental setting for the risk
prediction of hospitalization

penalties for providers, and reduce emergency department
congestion [37].

In this research, we apply our proposed representation
learning framework to the risk prediction of future hospi-
talization. Many studies have been conducted by researchers
to predict the risk of 30-day readmission, or the admission
risk of a particular population, such as patients with Am-
bulatory Care Sensitive Conditions (ACSCs), patients with
heart failure, etc. [38]–[41]. Here, we focus on the general
population and the objective is to predict the risk of all-cause
hospitalization using longitudinal EHR data.

B. EXPERIMENTAL DESIGN
In this research, we use de-identiﬁed EHR data from the
University of Virginia Health System covering 75 months be-
ginning in September 2010. This dataset contains 2,343,651
inpatient and outpatient visits of 473,915 distinct patients.
We extracted visit data with diagnosis, medication, and pro-
cedure codes.

We deﬁned the observation window and prediction period
to validate the proposed method. We ﬁrst extract all patients
with a medical record of at least 1.5 years, where the ﬁrst year
is the observation window and the medical records in this
time window are used for feature construction. The follow-
ing 6 months is the hold-off period for the purpose of early
detection. For the positive class, we take all patients who
have hospitalization after the ﬁrst 1.5 years in their medical
history, while the negative class consists of patients who have
no hospitalization after 1.5 years. To better illustrate the ex-
perimental setting, we present the observation window, hold-
off and onset of outcome event in Figure 5. Here, the medical
codes include diagnosis, medication, and procedure codes,
and a vector representation is learned for each code. In this
dataset, diagnoses are primarily coded in ICD-9 and a small
portion is ICD-10 codes, while procedures are mainly using
CPT codes with a few ICD-9 procedure codes. The codes of
medications are using the pharmaceutical categories. Overall,
there are 94 distinct medication categories, 34,419 distinct
diagnoses codes, and 7,895 distinct procedure codes in the
EHR data. The dimension of the learned vectors of medical
codes is set to 100. Medical codes that appear in less than 50
patients medical records are excluded as rare events.

To construct the subsequences of medical codes, we use l
days as the time window. Figure 6 presents the cumulative
histogram and density plot of the numbers of visits in the
observation window, and we observe that the majority of

FIGURE 6. The cumulative histogram and density plot of patients’ numbers of
visits

patients have a small number of visits during the observation
window (less than 25% of patients have more than 4 visits).
Thus, we set l to 90 days, which split the observation window
into 4 subsequences.

Within each subsequence, the number of distinct medical
codes were computed and patients with more medical codes
in a subsequence than the 95% quantile were excluded from
the dataset. Overall, there are 8,841 and 89,101 patients in
the target and control groups, respectively. Each group is
randomly split into training, validation and testing sets with
a 7:1:2 ratio. Thus, 70% are used for training, another 20%
is used for testing, and the rest 10% are used for parameter
tuning and early stopping. The stochastic gradient descent
algorithm is used in training to minimize the cross-entropy
loss function, shown in Equation 21.

To evaluate the proposed representation learning frame-
work, we compare the prediction performance of the pro-
posed model with baseline approaches as follows.

1) Logistic regression (LR)

The inputs are the aggregated counts of grouped medical
codes over the entire observation window. Since the di-
mensionality of raw medical codes is huge, AHRQ clini-
cal classiﬁcations of diagnoses and procedures are used to
achieve a more general clustering of medical codes [42].
The medication codes are the pharmaceutical classes. Fur-
thermore, patient characteristics and previous inpatient visit
are also considered, where age and gender are demographic
information, and a binary indicator is utilized to represent
the presence of the previous hospitalization. Hence, the input
is a 436-dimensional vector representing a patient’s medical
history and characteristics.

8

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 1. The predictive performance of baselines and the proposed Patient2Vec framework

Methods
LR
MLP
RETAIN
FRNN-MGE
BiRNN-MGE
FRNN-MVE
BiRNN-MVE
Patient2Vec

Sensitivity
0.637 ± 0.010
0.727 ± 0.013
0.553 ± 0.012
0.636 ± 0.012
0.600 ± 0.012
0.753 ± 0.011
0.724 ± 0.010
0.769 ± 0.010

Speciﬁcity
0.728 ± 0.003
0.617 ± 0.004
0.710 ± 0.003
0.739 ± 0.004
0.777 ± 0.003
0.676 ± 0.004
0.707 ± 0.003
0.694 ± 0.004

AUC
0.721 ± 0.006
0.713 ± 0.007
0.663 ± 0.007
0.759 ± 0.006
0.768 ± 0.007
0.785 ± 0.006
0.788 ± 0.005
0.799 ± 0.005

F2 score
0.434 ± 0.006
0.423 ± 0.007
0.370 ± 0.008
0.438 ± 0.009
0.439 ± 0.009
0.470 ± 0.008
0.473 ± 0.008
0.492 ± 0.007

2) Multi-layer perceptron (MLP)
A multi-layer perceptron is trained to predict hospitalization
using the same inputs for logistic regression. Here, we use a
one hidden layer MLP with 256 hidden nodes.

3) Forward RNN with medical group embedding
(FRNN-MGE)
We split the sequence into subsequences with equal interval l.
The input at each step is the counts of medical groups within
the associated time interval, and the patient characteristics are
appended as additional features in the ﬁnal logistic regression
step. Here, the RNN is a forward GRU (or LSTM [18]) with
one hidden layer and the size of the hidden layer is 256.

4) Bidirectional RNN with medical group embedding
(BiRNN-MGE)
The inputs used for this baseline is the same as the one for
the FRNN-MGE [15]. The RNN used here is a bidirectional
GRU with one hidden layer and the size of the hidden layer
is 256.

5) Forward RNN with medical vector embedding
(FRNN-MVE)
We split the sequence into subsequences with equal interval l.
The input at each step is the vector representation of the
medical codes within the associated time interval, and the
patient characteristics are appended as additional features in
the ﬁnal logistic regression step. Here, the RNN is a forward
GRU (or LSTM [28]) with one hidden layer and the size of
the hidden layer is 256.

6) Bidirectional RNN with medical vector embedding
(BiRNN-MVE)
The inputs used for this baseline is the same as the one for
the FRNN-MVE [25]. The RNN used here is a bidirectional
GRU or LSTM [15] with one hidden layer and the size of the
hidden layer is 256.

7) RETAIN
This model uses reverse time attention mechanism on RNNs
for an interpretable representation of patient’s EHR data [31].
The inputs are the same as the one for FRNN-MGE, which
takes the counts of medical grouping within each time inter-
val to construct features. Similarly, the two RNNs used for

generating weights are GRU-based and the size of the hidden
layers are 256.

8) Patient2Vec
The inputs are the same as that for FRNN-MVE. One ﬁlter
is used when generating weights for within-subsequence
attention, and three ﬁlters are used for subsequence-level
attention. Similarly, the RNN used here is GRU-based and
there is one hidden layer and the size of the hidden layer
is 256.

The inputs of all baselines and Patient2Vec are normal-
ized to have zero mean and unit variance. We model the
risk of hospitalization based on Patient2Vec and baseline
representations of patients’ medical histories, and the model
performance is evaluated with Area Under Curve(AUC),
sensitivity, speciﬁcity, and F2-score. The validation set is
used for parameter tuning and early stopping in the train-
ing process. Each experiment is repeated 20 times and we
calculate the averages and standard deviations of the above
metrics, respectively.

C. EXPERIMENTAL RESULTS
The predictive performance of Patient2Vec and baselines are
presented in Table 1. The results shown here for the RNN-
based models are based on time interval l = 90 days to
construct subsequences.

According to Table 1, the RNN-based models are generally
capable of achieving higher prediction performance in terms
of sensitivity, AUC and F2 score, except for the RNN models
based on medical group embedding which have lower sensi-
tivity. Among all RNN-based approaches, the ones based on
vector embedding outperform those based on medical group
embedding in terms of sensitivity, AUC, and F2 score. The
bidirectional RNN models generally have higher speciﬁcity
but lower sensitivity than the forward RNN models, while
the bidirectional ones have comparable AUC and F2 score
with the forward ones, respectively. Generally, the proposed
Patient2Vec framework outperforms the baseline methods,
especially in terms of sensitivity and F2 score.

D. VISUALIZATION & INTERPRETATION
In addition to predictive performance, we interpret
the
learned representation by understanding the relative impor-
tance of clinical events in a patient’s EHR data. Considering

VOLUME 0, 2018

9

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 7. The heat map showing feature importance for Patient A

FIGURE 8. The proﬁle of Patient A.

FIGURE 9. The proﬁle of Patient B.

the feature importance learned by Patient2Vec are person-
alized for an individual patient, we illustrate it with two
example patients. Figures 8 and 9 present the proﬁles of
two individuals, Patient A and Patient B, respectively. To
facilitate the interpretation, instead of using raw medical
codes, we present the clinical groups from the AHRQ clinical
classiﬁcation software on diagnoses and procedure codes, as
well as pharmaceutical groups for medications.

According to Figure 8, Patient A is a male patient who
has hospitalization history in the observation window and
is admitted to the hospital seven months after the end of
the observation window for congestive heart failure. The

predicted risk is 96.4%, while the risk decreases for female
patients or patients without hospitalization history. It is also
not surprising to observe an increased risk for older patients.
The heat map in Figure 7 shows the relative importance of
the medical events in this patient’s medical record at each
time window and the ﬁrst row of the heat map presents
the subsequence-level attention. The darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the last subsequence,
t4, is the most important with respect to hospitalization risk,
followed by t1, t2, and t3 in order of importance.

Among all the clinical events in the subsequence t4, we

10

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 2. The top clinical groups with high weights in hospitalized patients

Index

Clinical Groups

Diagnoses
1
2
3

Essential hypertension
Other connective tissue disease
Spondylosis; intervertebral disc disorders; other back
problems
Other lower respiratory disease
Disorders of lipid metabolism
Other aftercare
Diabetes mellitus without complication
Screening and history of mental health and substance
abuse codes
Other nervous system disorders
Other screening for suspected conditions (not mental
disorders or infectious disease)

Other OR therapeutic procedures on nose; mouth and
pharynx
Suture of skin and subcutaneous tissue
Other therapeutic procedures on eyelids; conjunctiva;
cornea

Laboratory - Chemistry and hematology
Other laboratory
Other OR therapeutic procedures of urinary tract
Other OR procedures on vessels other than head and
neck

Therapeutic radiology for cancer treatment

Procedures
1

9
10

4
5
6
7
8

2
3

4
5
6
7

8

2

Medications
1

Diagnostic Products

Analgesics-Narcotic

observe that the OR therapeutic procedures (nose, mouth,
and pharynx), laboratory (chemistry and hematology), coro-
nary atherosclerosis & other heart disease, cardiac dys-
rhythmias, and conduction disorders are the ones with the
highest weights, while other events such as other connected
tissue disease are less important in terms of future hospi-
talization risk. Additionally, some medications appear to be
informative as well, including beta blockers, antihyperten-
sives, anticonvulsants, anticoagulants, etc. In the ﬁrst-time
window, the medical events with high weights are coro-
nary atherosclerosis & other heart disease, gastrointestinal
hemorrhage, deﬁciency and anemia, and other aftercare. In
the next subsequence, the most important medical events
are heart diseases and related procedures such as coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
conduction disorders, hypertension with complications, other
OR heart procedures, and other OR therapeutic nervous
system procedures. We also observe that the kidney disease
related diagnoses and procedures appear to be important
features. Throughout the observation window, the coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
and conduction disorders constantly show high weights with
respect to hospitalization risk, and the ﬁndings are consistent

FIGURE 10. The heat map showing feature importance for Patient B

with medical literature.

Figure 9 presents the proﬁle of Patient B, which is a male
patient without hospitalization in the observation window.
This patient is hospitalized for occlusion of cerebral arter-
ies approximately one year after the observation window,
and the predicted risk is 74.6%. For a similar patient who
is 10 years older or with previous hospitalization history,
the risk increases by 4.2% and 1%, respectively, while there
is a smaller risk of hospitalization for a female patient. To
illustrate the medical events of Patient B, the heat map in
Figure 10 depicts the relative importance of medical groups
in the subsequences, as well as the subsequence-level weights
for hospitalization risk. Similarly, the darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the second subse-
quence appears to be the most important, while the last one is
less predictive of future hospitalization. In fact, the medical
events in the last time window are spondylosis, intervertebral
disc disorders, other back problems and other bone disease &
musculoskeletal deformities, and malaise and fatigue, which
are not highly related to the cause of hospitalization of Patient
B.

In the most predictive subsequence, t2, we observe that
other OR heart procedures, genitourinary symptoms, spondy-
losis, intervertebral disc disorders, other back problems,
therapeutic procedures on eyelid, conjunctiva, and cornea,
and arterial blood gases have high attention weights. In the

VOLUME 0, 2018

11

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 3. The top diagnosis groups with high weights in patients hospitalized
for osteoarthritis, septicemia, acute myocardial infarction, congestive heart
failure, and diabetes mellitus with complications, respectively

Index

Diagnosis Groups

In patients admitted for osteoarthritis

Osteoarthritis
Other connective tissue disease

1
2

3
4

5

1
2

3
4
5

1
2
3

4
5

1
2
3
4
5

1
2
3

4
5

Other non-traumatic joint disorders
Spondylosis; intervertebral disc disorders; other back prob-
lems
Other aftercare

In patients admitted for septicemia

Essential hypertension
Diabetes mellitus without complication

Disorders of lipid metabolism
Other lower respiratory disease
Other aftercare

In patients admitted for acute myocardial infarction

Coronary atherosclerosis and other heart disease
Medical examination/evaluation
Other screening for suspected conditions (not mental disorders
or infectious disease)

Other lower respiratory disease
Disorders of lipid metabolism

In patients admitted for congestive heart failure

Congestive heart failure (nonhypertensive)
Coronary atherosclerosis and other heart disease
Cardiac dysrhythmias
Diabetes mellitus without complication
Other lower respiratory disease

In patients admitted for diabetes mellitus with complications

Diabetes mellitus with complications
Diabetes mellitus without complication
Other aftercare

Other nutritional; endocrine; and metabolic disorders
Fluid and electrolyte disorders

earliest time window, the most important medical events also
include therapeutic procedures on eyelid, conjunctiva, and
cornea, arterial blood gases, while diabetes, hypertension
as well as diagnostic products show their relatively high
importance. Throughout the observation window, medical
events spondylosis, intervertebral disc disorders, other back
problems, therapeutic procedures on eyelid, conjunctiva, and
cornea are constantly with high attention weights. Here, di-
agnostic products is a medication class, which include barium
sulfate, iohexol, gadopentetate dimeglumine, iodixanol, tu-
berculin puriﬁed protein derivative, iodixanol, regadenoson,
acetone (urine), and so forth. These medications are primarily
for blood or urine testing, or used as radiopaque contrast
agents for x-rays or CT scans for diagnostic purposes.

Additionally, we attempt to interpret the learned repre-
sentation and feature importance at the population-level. In
Table 2, we present the top 20 clinical groups with high
weights among hospitalized patients in the test set.

According to Table 2, the most predictive diagnosis groups
for future hospitalization are chronic diseases, including
essential hypertension, diabetes, lower respiratory disease,
disorders of lipid metabolism, and musculoskeletal diseases
such as other connective tissue disease and spondylosis,
intervertebral disc disorders, other back problems. The most
important procedures are some OR therapeutic procedures
and laboratory tests, such as the OR procedures on nose,
mouth, and pharynx, vessels, urinary tract, eyelid, conjunc-
tiva, cornea, etc. It is not surprising to see that diagnostic
products are showing with high weights, considering these
medications are used in testing or examinations for diagnos-
tic purposes.

Moreover, we present the top diagnoses groups with high
weights in patients hospitalized for different primary causes.
Table 3 shows the top 5 diagnosis groups with high weights
in patients admitted for osteoarthritis, septicemia (except in
labor), acute myocardial infarction, congestive heart failure
(nonhypertensive), and diabetes mellitus with complications,
respectively. Accordingly, we observe that the most impor-
tant diagnoses for hospitalization risk prediction in popu-
lation admitted for osteoarthritis are musculoskeletal dis-
eases such as connective tissue disease, joint disorders, and
spondylosis. However, the diagnoses with highest weights
in the patients admitted for septicemia are chronic diseases
including essential hypertension, diabetes, disorders of lipid
metabolism, and respiratory disease. The top diagnoses have
many overlaps between the populations admitted for acute
myocardial infarction and for congestive heart failure, con-
sidering both populations are admitted for heart diseases.
the overlapped diagnosis groups include coronary
Here,
atherosclerosis and other heart diseases and lower respiratory
diseases. As for patients admitted for diabetes with com-
plications, the top diagnoses are diabetes with or without
complications, nutritional, endocrine, metabolic disorders,
and ﬂuid and electrolyte disorders. In general, the learned
feature importance is consistent with medical literature.

VI. DISCUSSION
Our proposed framework is applied to the prediction of
hospitalization using real EHR data that demonstrates its
prediction accuracy and interpretability. This work could be
further enhanced by incorporating the follow-up information
on the negative patient population and investigate if it in-
deed shows an improved health outcome or the patient is
hospitalized elsewhere. Patient2Vec employs a hierarchical
attention mechanism, allowing us to directly interpret the
weights of clinical events. In future work, we will extend the
attention to incorporate demographic information for a more
comprehensive and automatic interpretation.

Although we apply Patient2Vec to the early detection of
long-term hospitalization, i.e., at least 6 months after the
previous hospitalization, it could be used to predict the risk
of 30-day readmission to help prevent unnecessary rehospi-
talizations.

12

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

VII. CONCLUSION
In this paper, we propose a representation learning frame-
work, Patient2Vec, to learn a personalized interpretable deep
representation of EHR data based on recurrent neural net-
works and the attention mechanism. This work improves
the performance of predictive models as well as deepens
the understanding of disease correlations. We apply this
framework to the risk prediction of hospitalization using
patients’ longitudinal EHR data. The experimental results
demonstrate that the proposed Patient2Vec representation is
capable of achieving a more accurate prediction than base-
lines approaches. Moreover, the learned feature importance
in the representations are interpreted both at the individual
and population levels to facilitate clinical insights.

In this work, the proposed Patient2Vec framework is eval-
uated with the risk prediction of all-cause hospitalization,
but in the future could be applied to predict hospitalization
in more speciﬁc populations, other health related prediction
problems, or domains outside of health.

REFERENCES
[1] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical
attention networks for document classiﬁcation,” in Proceedings of the
2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, 2016, pp.
1480–1489.

[2] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,” arXiv

preprint arXiv:1408.5882, 2014.

[3] J. Howard and S. Ruder, “Fine-tuned language models for text classiﬁca-

tion,” arXiv preprint arXiv:1801.06146, 2018.

[4] M. M. Lopez and J. Kalita, “Deep learning applied to nlp,” arXiv preprint

arXiv:1703.03091, 2017.

[5] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using
rnn encoder-decoder for statistical machine translation,” arXiv preprint
arXiv:1406.1078, 2014.

[6] A. L. Nobles, J. J. Glenn, K. Kowsari, B. A. Teachman, and L. E. Barnes,
“Identiﬁcation of imminent suicide risk among young adults using text
messages,” in Proceedings of the 2018 CHI Conference on Human Factors
in Computing Systems. ACM, 2018, p. 413.

[7] K. Kowsari, D. E. Brown, M. Heidarysafa, K. J. Meimandi, M. S. Gerber,
and L. E. Barnes, “Hdltex: Hierarchical deep learning for text classiﬁca-
tion,” in 2017 16th IEEE International Conference on Machine Learning
and Applications (ICMLA), Dec 2017, pp. 364–371.

[8] K. Kowsari, M. Heidarysafa, D. E. Brown, K. Jafari Meimandi, and L. E.
Barnes, “Rmdl: Random multimodel deep learning for classiﬁcation.”
ACM, 2018.

[9] H. Strobelt, S. Gehrmann, H. Pﬁster, and A. M. Rush, “Lstmvis: A tool
for visual analysis of hidden state dynamics in recurrent neural networks,”
IEEE transactions on visualization and computer graphics, vol. 24, no. 1,
pp. 667–676, 2018.

[10] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” Scientiﬁc
reports, vol. 8, no. 1, p. 6085, 2018.

[11] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches
preprint

neural machine

translation,”

arXiv

attention-based
to
arXiv:1508.04025, 2015.

[12] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[13] D. Yogatama, C. Dyer, W. Ling, and P. Blunsom, “Generative and discrim-
inative text classiﬁcation with recurrent neural networks,” arXiv preprint
arXiv:1703.01898, 2017.

[14] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent

trends
language processing,” arXiv preprint

in deep learning based natural
arXiv:1708.02709, 2017.

[15] M. Basaldella, E. Antolli, G. Serra, and C. Tasso, “Bidirectional lstm
recurrent neural network for keyphrase extraction,” in Italian Research

Conference on Digital Libraries, Springer.
lishing, 2018, pp. 180–187.

Springer International Pub-

[16] S. Ghosh, O. Vinyals, B. Strope, S. Roy, T. Dean, and L. Heck, “Con-
textual lstm (clstm) models for large scale nlp tasks,” arXiv preprint
arXiv:1602.06291, 2016.

[17] B. Yue, J. Fu, and J. Liang, “Residual recurrent neural networks for
learning sequential representations,” Information, vol. 9, no. 3, p. 56, 2018.
[18] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
gated recurrent neural networks on sequence modeling,” arXiv preprint
arXiv:1412.3555, 2014.

[19] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difﬁculty of training
recurrent neural networks.” ICML (3), vol. 28, pp. 1310–1318, 2013.
[20] D. Britz, “Recurrent neural network tutorial,” http://www.wildml.com/

2015/10/, 2015, [Accessed on October 5, 2017].

[21] V. Mnih, N. Heess, A. Graves et al., “Recurrent models of visual attention,”
in Advances in neural information processing systems, 2014, pp. 2204–
2212.

[22] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for
abstractive sentence summarization,” arXiv preprint arXiv:1509.00685,
2015.

[23] T. Ma, C. Xiao, and F. Wang, “Health-atm: A deep architecture for
multifaceted patient health record representation and risk prediction,” in
Proceedings of the 2018 SIAM International Conference on Data Mining.
SIAM, 2018, pp. 261–269.

[24] A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J. Liu,
X. Liu, J. Marcus, M. Sun et al., “Scalable and accurate deep learning with
electronic health records,” npj Digital Medicine, vol. 1, no. 1, p. 18, 2018.
[25] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzell, “Learning to diagnose
with lstm recurrent neural networks,” arXiv preprint arXiv:1511.03677,
2015.

[26] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” arXiv preprint
arXiv:1606.01865, 2016.

[27] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghas-
semi, B. Moody, P. Szolovits, L. A. Celi, and R. G. Mark, “Mimic-iii, a
freely accessible critical care database,” Scientiﬁc data, vol. 3, 2016.
[28] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart, and J. Sun, “Doctor
ai: Predicting clinical events via recurrent neural networks,” in Machine
Learning for Healthcare Conference, 2016, pp. 301–318.

[29] Z. Che, S. Purushotham, R. Khemani, and Y. Liu, “Interpretable deep
models for icu outcome prediction,” in AMIA Annual Symposium Pro-
ceedings, vol. 2016. American Medical Informatics Association, 2016,
p. 371.

[30] J. H. Friedman, “Greedy function approximation: a gradient boosting

machine,” Annals of statistics, pp. 1189–1232, 2001.

[31] E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart,
“Retain: An interpretable predictive model for healthcare using reverse
time attention mechanism,” in Advances in Neural Information Processing
Systems, 2016, pp. 3504–3512.

[32] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Ben-
gio, “A structured self-attentive sentence embedding,” arXiv preprint
arXiv:1703.03130, 2017.

[33] C. M. Torio and B. J. Moore, “National inpatient hospital costs: The most
expensive conditions by payer, 2013,” https://www.hcup-us.ahrq.gov/
reports/statbriefs/sb204-Most-Expensive-Hospital-Conditions.jsp, 2016.
[34] E. Wallace, E. Stuart, N. Vaughan, K. Bennett, T. Fahey, and S. M.
Smith, “Risk prediction models to predict emergency hospital admission
in community-dwelling adults: a systematic review,” Medical care, vol. 52,
no. 8, p. 751, 2014.

[35] E. N. de Vries, M. A. Ramrattan, S. M. Smorenburg, D. J. Gouma, and
M. A. Boermeester, “The incidence and nature of in-hospital adverse
events: a systematic review,” Quality and safety in health care, vol. 17,
no. 3, pp. 216–223, 2008.

[36] S. Purdey and A. Huntley, “Predicting and preventing avoidable hospital
admissions: a review.” The journal of the Royal College of Physicians of
Edinburgh, vol. 43, no. 4, pp. 340–344, 2012.

[37] H. Ontario, “Early identiﬁcation of people at risk of hospitalization: Hos-
pital admission risk prediction (harp)-a new tool for supporting providers
and patients,” 2013.

[38] B. Zheng, J. Zhang, S. W. Yoon, S. S. Lam, M. Khasawneh, and S. Poranki,
“Predictive modeling of hospital readmissions using metaheuristics and
data mining,” Expert Systems with Applications, vol. 42, no. 20, pp. 7110–
7120, 2015.

VOLUME 0, 2018

13

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

[39] D. Kansagara, H. Englander, A. Salanitro, D. Kagen, C. Theobald,
M. Freeman, and S. Kripalani, “Risk prediction models for hospital
readmission: a systematic review,” The Journal of the American Medical
Association, vol. 306, no. 15, pp. 1688–1698, 2011.

[40] G. Giamouzis, A. Kalogeropoulos, V. Georgiopoulou, S. Laskar, A. L.
Smith, S. Dunbar, F. Triposkiadis, and J. Butler, “Hospitalization epidemic
in patients with heart failure: risk factors, risk prediction, knowledge gaps,
and future directions,” Journal of cardiac failure, vol. 17, no. 1, pp. 54–75,
2011.

[41] E. Prescott, A. M. Bjerg, P. K. Andersen, P. Lange, and J. Vestbo, “Gender
difference in smoking effects on lung function and risk of hospitalization
for COPD: results from a danish longitudinal population study,” European
Respiratory Journal, vol. 10, no. 4, pp. 822–827, 1997.

[42] Agency for Healthcare Research and Quality (AHRQ), “Clinical classi-
ﬁcations software (CCS) for ICD-9-CM,” https://www.hcup-us.ahrq.gov/
toolssoftware/ccs/ccs.jsp, 2015.

JENNIFER M. LOBO (jem4yb@virginia.edu) is
an Assistant Professor of Biomedical Informatics
in the Department of Public Health Sciences at
University of Virginia. She received her Ph.D. in
Industrial Engineering from North Carolina State
University, Raleigh, NC. Her research interests in-
volve using mathematical modeling and stochastic
optimization methods to build models that simu-
late the natural course of disease. These models
allow for estimation of outcomes under different
screening and treatment policies in the absence of randomized controlled
trials, and can be used to optimize screening and treatment decisions for
patients with chronic diseases. Her projects include optimizing treatment
for patients with type 2 diabetes, generating individualized decision analysis
models for prostate cancer patients, and developing optimal imaging surveil-
lance guidelines for recurrent kidney cancer.

JINGHE ZHANG (jz4kg@virginia.edu) is a lead
data scientist at Target Corporation. She received
her Ph.D. in Systems Engineering from the Uni-
versity of Virginia. Prior to entering the Ph.D. pro-
gram at UVA, she received the Master of Science
in Industrial and Systems Engineering from the
State University of New York at Binghamton. Her
research interests are in natural language process-
ing, machine learning, recommender systems, and
health informatics.

LAURA E. BARNES (lb3dp@virginia.edu) is an
Associate Professor in Systems and Information
Engineering and the Data Science Institute at the
University of Virginia. She received her Ph.D. in
Computer Science from the University of South
Florida, Tampa, FL. She directs the Sensing Sys-
tems for Health (S2He) Lab which focuses on
understanding the dynamics and personalization
of health and well-being through mobile sensing
and analytics.

(kk7nc@virginia.edu) is
KAMRAN KOWSARI
a Ph.D. student in the Department of Systems
and Information Engineering at the University of
Virginia, Charlottesville, VA. He is a member of
the Sensing Systems for Health Lab. He received
his Master of Science from Department of Com-
puter Science at The George Washington Univer-
sity, Washington, DC. He has more than ten years
of experience in machine learning and software
development. His experience includes numerous
industrial and academic projects. His research interests include natural
language processing, machine learning, deep learning, artiﬁcial intelligence,
text mining, and unsupervised learning.

JAMES H. HARRISON, JR., (jhh5y@virginia.
edu) is Associate Professor of Pathology and Di-
rector of Laboratory Information Systems at the
University of Virginia Medical Center and also has
appointments in the Departments of Public Health
Sciences in the UVA School of Medicine, and
Systems and Information Engineering in the UVA
School of Engineering and Applied Sciences. He
received his MD and PhD (Pharmacology) de-
grees from Medical University of South Carolina,
Charleston, SC, completed residencies in Anatomic Pathology and Labora-
tory Medicine at Yale-New Haven Hospital, New Haven, CT, and completed
a postdoctoral fellowship in Environmental Toxicology at Yale University,
New Haven, CT. Dr. Harrison has over 25 years of experience in the ﬁeld
of medical informatics, including work in clinical laboratory information
systems, electronic health records, clinical data analysis, and clinical data
standards development.

14

VOLUME 0, 2018

Date of publication 10, 2018 , date of current version 10, 2018.

Digital Object Identiﬁer 10.1109/ACCESS.2018.2875677

Patient2Vec: A Personalized
Interpretable Deep Representation of the
Longitudinal Electronic Health Record

JINGHE ZHANG1, (Member, IEEE), KAMRAN KOWSARI1,4(Member, IEEE), JAMES H.
HARRISON2,3,5, JENNIFER M. LOBO2,5, and LAURA E. BARNES1,4,5, (Member, IEEE)
1Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, USA
2 Department of Public Health Sciences, University of Virginia, Charlottesville, VA 22904, USA
4 Sensing Systems for Health Lab, University of Virginia, Charlottesville, VA 22904, USA
3 Division of Laboratory Medicine Department of Pathology, University of Virginia, Charlottesville, VA 22904, USA
5Data Science Institute, University of Virginia, Charlottesville, VA 22904, USA
Corresponding author: Laura E. Barnes (lb3dp@virginia.edu)
This research was supported by a Jeffress Trust Award in Interdisciplinary Science.
Patient2Vec is shared as an open source tool at https://github.com/BarnesLab/Patient2Vec

ABSTRACT The wide implementation of electronic health record (EHR) systems facilitates the collection
of large-scale health data from real clinical settings. Despite the signiﬁcant increase in adoption of EHR
systems, this data remains largely unexplored, but presents a rich data source for knowledge discovery from
patient health histories in tasks such as understanding disease correlations and predicting health outcomes.
However, the heterogeneity, sparsity, noise, and bias in this data present many complex challenges.
This complexity makes it difﬁcult to translate potentially relevant information into machine learning
algorithms. In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep
representation of longitudinal EHR data which is personalized for each patient. To evaluate this approach,
we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive
performance with baseline methods. Patient2Vec produces a vector space with meaningful structure and it
achieves an AUC around 0.799 outperforming baseline methods. In the end, the learned feature importance
can be visualized and interpreted at both the individual and population levels to bring clinical insights.

INDEX TERMS Attention mechanism, gated recurrent unit, hospitalization, longitudinal electronic health
record, personalization, representation learning.

8
1
0
2
 
t
c
O
 
5
2
 
 
]

M
Q
.
o
i
b
-
q
[
 
 
3
v
3
9
7
4
0
.
0
1
8
1
:
v
i
X
r
a

I. INTRODUCTION

L ongitudinal EHR data resemble text documents from

many perspectives. A text document consists of a se-
quence of sentences, and a sentence is a sequence of words.
Similarly, the longitudinal health record of a patient consists
of a sequence of visits, and there is a list of clinical events,
including diagnoses, medications, and procedures, that occur
during a visit. Considering these similarities, representation
learning methods for text documents in Natural Language
Processing (NLP) have great potential to be applied to lon-
gitudinal EHR data.

Deep neural networks have become very popular in the
NLP ﬁeld and have been very successful in many applica-
tions, such as machine translation, question answering, text
classiﬁcation, document summarization, language modeling,
etc. [1]–[8]. These networks excel at complex language tasks

because they are capable of identifying high-order relation-
ships, the network structure can encode language structures,
and they allow the learning of a hierarchical representation
of the language, i.e., representations for tokens, phrases, and
sentences, etc.

Among a variety of deep learning methods, Recurrent
Neural Networks (RNNs) have shown their effectiveness in
NLP tasks because they have the ability to capture sequential
information [7]–[10] which is inherent in human language.
Traditional neural networks assume that inputs are inde-
pendent of each other, while an RNN computes the output
based on the current input as well as the “memory” from the
previous computation. Although vanilla RNNs are not good
at capturing long-term dependencies, many variants have
been proposed and validated that are effective in addressing

VOLUME 0, 2018

1

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

its performance with other baseline methods. In addition
to prediction performance, we further interpret the learned
representations with visualizations on example patients and
events. Finally, Section V provides a summary of this work.

II. RELATED WORK
In this section, we present an overview of a gated recurrent
unit, a type of RNN, which is capable of capturing long-term
dependencies. Then we brieﬂy introduce attention mecha-
nisms in neural networks that allow the network to attend
to certain regions of data, which is inspired by the visual
attention mechanism in humans. Additionally, we summarize
the RNN networks and attention mechanisms previously used
to mine EHR data.

A. RECURRENT NEURAL NETWORKS (RNN)

RNNs are expected to learn long-term dependencies by tak-
ing the previous state and the new input in the computation at
the current time step t. However, vanilla RNNs are incapable
of capturing the dependencies when the sequence is very long
due to the vanishing gradient problem [12]. Many variants of
the RNN network have been proposed to address this issue,
and long short term memory (LSTM) is one of the most
popular models used nowadays in NLP tasks [7], [8], [13]–
[16].

this issue.

In the medical domain, it is critical that analytical results
are interpretable, so that they can be understood and validated
by a human with expert knowledge and so that knowledge
captured by analysis can be used for process improvement.
Traditional deep neural networks have the disadvantage that
they lack interpretability. A substantial amount of work is
ongoing to make sense of the “black box”, and the attention
mechanism [11] is one of the more effective methods recently
developed to make the output of these algorithms more
interpretable.

Health care is undergoing unprecedented change, and there
is a great potential and demand for personalized care strate-
gies. Personalized medicine, also called precision medicine,
has previously focused on optimizing therapy to better ﬁt the
genetic makeup of the patient or the disease (e.g., the genetic
susceptibility of cancer to speciﬁc chemotherapy strategies).
The availability of EHR data and advances in machine learn-
ing create the potential for another type of personalization
of healthcare. This type of personalization has become ubiq-
uitous in our daily life. For example, customers have come
to expect personalized search on Google and personalized
product recommendations on Amazon and Netﬂix, based on
their charactersitics and previous experiences with the sys-
tems. Personalization of healthcare processes, based on a pa-
tient’s phenotype (physical and medical characteristics) and
healthcare experiences as documented in the health record,
may also improve "customer" satisfaction and it has the addi-
tional potential to improve healthcare efﬁciency, lower costs,
and yield better outcomes. We believe that representation
learning methods can capture a personalized representation
of the important heterogeneities in patients’ phenotypes and
medical histories at the population-level, and make these
representations available to drive healthcare decisions and
strategies.

This research is based on RNN models and the attention
mechanism with the objective of learning a personalized, in-
terpretable, and complete representation of patients’ medical
records. Our proposed framework is capable of learning a
personalized representation for each patient from a sequence
of clinical events. A hierarchical attention mechanism learns
personalized weights of clinical events, including hospital
visits and the procedures that they contain. These weights
allow us to interpret the relative importance and roles of clin-
ical events in the learned representations both at individual
and population levels. The ultimate goal is more accurate
prediction and better insight into the critical elements of
healthcare processes that can be used to improve healthcare
delivery.

The rest of this paper is organized as follows: Section II
summarizes the variants of RNNs and the attention mecha-
nism, as well as their application to EHR data. Section III
presents an overview of the proposed Patient2Vec repre-
sentation learning framework, and Section IV elaborates
the details of the algorithms. In Section V, the proposed
framework is evaluated for a prediction task and we compare

FIGURE 1. The top ﬁgure is a GRU gating unit and bottom ﬁgure shows an
LSTM unit [7]

2

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

1) Gated Recurrent Unit (GRU)
GRU is a simpliﬁed version of LSTM [7]. The basic idea
of GRU is to combat the vanishing gradient problem with a
gating mechanism. Hence the general recurrent structure in
GRU is identical to vanilla RNNs except that a GRU unit
is used in the computation at each time step rather than a
traditional simple recurrent unit.

In general, a GRU cell has two gates, i.e., a reset gate r
and an update gate z. The reset gate is used to determine
how to integrate the previous state into the computation of
the current state, while the update gate determines how much
the unit updates its activation.

Given the input xt at time step t, the reset gate rt is

computed as presented in Equation 1

rt = σ(Urxt + Wrst−1)

(1)

where Ur and Wr are the weight matrices of the reset gate
and st−1 is the hidden activation at time step t − 1. A similar
computation is performed for the update gate zt at time step t,
shown in Equation 2

zt = σ(Uzxt + Wzst−1)

(2)

where Uz and Wz are the weight matrices of update gate.
The current hidden activation ht is computed by

ht = (1 − zt)ht−1 + zt
(3)
where ˜ht is the candidate activation at time step t. The
computation of ˜ht is presented in Equation 4

˜ht

˜ht = tanh(Wxt + U(rt (cid:12) ht−1))

(4)

where U and W are weight matrices and (cid:12) represents
element-wise multiplication. Figure 1 presents a graphical
illustration of the GRU [7] and one unit of LSTM.

GRU is capable of learning long-term dependencies [17]
due to the additive component of update from t to t + 1
in the gating mechanism. Consequently, important features
will be carried forward in the input stream while irrelevant
information will be dropped. When the reset gate is 0, the
network is forced to drop previous states and reset with cur-
rent information. Moreover, the method provides shortcuts
such that the error is easily backpropagated without vanishing
too quickly [5], [18]. Hence, the GRU is well-suited to learn
long-term dependencies in sequence data.

2) Long Short-Term Memory (LSTM)
An LSTM unit is similar to a GRU, but with one more gate in
an LSTM unit (as shown in Figure 1). LSTM also preserves
long term dependencies more effectively than basic RNN.
This is particularly useful to overcome the vanishing gradient
problem [19]. Although LSTM has a chain-like structure sim-
ilar to RNN, LSTM uses multiple gates to carefully regulate
the amount of information that will be allowed into each node
state. Figure 1 shows the basic cell of an LSTM model. A step

VOLUME 0, 2018

by step explanation of an LSTM cell is as following:
Input gate:

it = σ(Wi[xt, ht−1] + bi),

Candid memory cell value:

˜Ct = tanh(Wc[xt, ht−1] + bc),

Forget gate activation:

ft = σ(Wf [xt, ht−1] + bf ),

New memory cell value:

Ct = it ∗ ˜Ct + ftCt−1,

Output gate value:

ot = σ(Wo[xt, ht−1] + bo),

ht = ot tanh(Ct),

(5)

(6)

(7)

(8)

(9)

(10)

In the above description all b represent bias vectors, all W
represent weight matrices, and xt is used as input to the
memory cell at time t. Also,the i, c, f, o indices refer to input,
cell memory, forget and output gates respectively. An RNN
can be biased when later words are more inﬂuential than the
earlier ones.

Empirically, LSTM and GRU achieve comparable per-
formance in many tasks but there are fewer parameters in
a GRU, which makes it a little faster to learn and able to
generalize with fewer data [20].

B. ATTENTION MECHANISM
Attention mechanisms, inspired by the visual attention sys-
tem found in humans, have become popular in deep learning.
Attention allows the network to focus on certain regions of
data, while perceiving other regions with “low resolution”.
In addition to higher accuracy, it also facilitates the interpre-
tation of learned representations. We elaborate an attention
mechanism on an RNN network, and Figure 2 presents a
graphical illustration.

According to Figure 2, a variable-length weight vector α
is learned based on hidden states [11]. Then a global context
vector is computed based on weights α and all the hidden
states to create the ﬁnal output. Equation 11 presents the
computation of the weight vector α = {α1, α2, · · · , αT },
where T is the length of the sequence

α1, α2, · · · , αT = f (Wαh + bα)

(11)

and where f is a nonlinear activation function, usually
sof tmax or tanh. Then, the context vector c is constructed
as:

c =

αtht

T
(cid:88)

t=1

Thus, the network puts more attention on the important
features for the ﬁnal prediction which can improve the model
performance. An additional beneﬁt is that the weights can

(12)

3

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

GRU-based model to address missing values in multivariate
time series data, in which the missing patterns are incorpo-
rated for improved prediction performance. This work has
been applied to the Medical Information Mart for Intensive
Care III (MIMIC-III) clinical database to demonstrate its
effectiveness in mining time series of clinical measurements
with missing values [27]. Longitudinal EHR data including
clinical events, such as diagnoses, medications, and pro-
cedures is also a potentially rich resource for predictive
modeling. Choi et al. [28] analyze this data with a GRU
network to forecast future clinical events, and it achieves a
better prediction performance than comparison models such
as logistic regression and MLP.

Difﬁculty in interpreting model behavior is one of the
major drawbacks of using deep learning to mine EHR data.
Some attempts have been made to address this issue. Che et
al. [29] propose an interpretable mimic learning method
which trains a mimic gradient boosting trees model to utilize
predicted labels or features learned by deep learning mod-
els for ﬁnal prediction [30]. Then the feature importances
learned by the tree-based models are used for knowledge
discovery. Attention mechanisms have been introduced re-
cently to improve the interpretability of the prediction results
of deep learning models in health analytics. Choi et al. [31]
develop an interpretable model with two levels of attention
weights learned from two reverse-time GRU models, re-
spectively. The experimental results on EHR data indicate
comparable prediction performance with conventional GRU
models but more interpretable results. Our work continues
the attempt to use attention mechanisms to improve the
interpretability of RNN-based models.

III. PATIENT2VEC SYSTEM MODEL
In this section, we provide an overview of the proposed
hierarchical representation learning framework. This frame-
work uses deep recurrent neural networks to capture the
complex relationships between clinical events in the patient’s
EHR data and employs the attention mechanism to learn
a personalized representation and to obtain relative feature
importance. The proposed representation learning framework
contains four steps and is presented graphically in Figure 3.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES

EHR data consists primarily of records of outpatient and
inpatient visits to healthcare providers. These visit records
include multiple clinical codes for diagnoses, symptoms,
procedures, therapies, and other observations and events that
occurred during the visit. Here, we treat the set of medical
codes associated with a visit as a sentence consisting of
words, except that there is no ordering in the words. Thus,
we adopt the word2vec approach to construct a vector to
represent each medical code.

FIGURE 2. The global attention model

be utilized to understand the importance of features such
that the models are more interpretable. The attention mech-
anism has been introduced to both Convolutional Neural
Networks (CNNs) and RNNs for various tasks and has
achieved many successes in the ﬁelds of computer vision and
NLP [11], [21], [22].

C. DEEP LEARNING IN EHR DATA
Previous studies on EHR data mainly use statistical meth-
ods or traditional machine learning techniques. Recently
researchers have started adapting deep learning approaches
to this data [23], [24], including textual notes, temporal
measurements of laboratory testing in the Intensive Care
Unit (ICU), and longitudinal data in patient populations.
Here, we summarize deep learning research in mining EHR
data and focus on the studies using RNN-based models.

Hospitalized patients, especially patients in ICUs, are
continuously monitored for cardiac, respiratory, and other
physical functions, creating a large volume of sequential data
in multiple dimensions. These measurements are utilized by
physicians to make diagnostic and treatment decisions. The
functions monitored may change over time and monitor-
ing may be irregular, based on a patient’s condition. It is
very challenging for traditional machine learning methods
to mine this multivariate time series data considering miss-
ing values, varying length, and irregular, non-simultaneous
sampling. Lipton et al. [25] trained an LSTM with a repli-
cated target to learn from these sequence data and used
this model to make predictions of diagnoses. The data used
in this research are time series of clinical measurements
with continuous values, and the LSTM models outperformed
logistic regression and MLP. Che et al. [26] developed a

4

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

are unlikely to contribute equally to the prediction of the tar-
get outcome, we cannot aggregate them with equal weights.
Instead, we employ a self-attention mechanism which trains
the network to learn the weights.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of subsequences with embedded medical
codes, we are able to input it into a recurrent neural network
to capture the temporal dependencies between events. How-
ever, the subsequences of visits are not contributing equally
to the outcome. Hence, we employ another level of attention
to learn the weights of the subsequences by the network itself
for the outcome prediction.

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the learned weights and hidden outputs, we aggregate
them into one universal vector for a comprehensive represen-
tation. In this step, the static information, such as age, gender,
previous hospitalization history is added as extra features, to
get a complete representation of a patient.

E. PREDICTING OUTCOME
Given the complete vector representation of a patient’s EHR
data, we add a logistic regression layer at the end for the
prediction of outcome.

IV. PATIENT2VEC REPRESENTATION LEARNING
ALGORITHM
In this section, we present the details of the proposed rep-
resentation learning framework, which is based on a GRU
network and a hierarchical attention mechanism. Figure 4
presents the structure of the proposed network with attention.

The proposed framework consists of ﬁve parts presented in
the following: I) Learning vector representations of medical
codes, II) Learning within-subsequence self-attention, III)
Learning subsequence-level self-attention, IV) Constructing
aggregated deep representation, V) Predicting outcome.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES
Given a patient’s raw EHR data, a sequence of visits, we
observe that a visit usually contains multiple medical codes.
Hence, it is feasible to learn a vector to represent the medical
code by capturing the relationships between the codes. In
this work, we employ the classical word2vec algorithm, skip-
gram. The basic idea of skip-gram is to learn a vector to
represent each word such that the probability of the context
to predict based on the target word is maximized. Hence,
the vectors of similar words are close to each other in the
learned feature space. In the skip-gram model, the vectors
are learned by training a shallow neural network to predict the
context words given an input word. Similarly, in our problem,

FIGURE 3. The Patient2Vec representation learning framework

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Clinical visits are represented as the set of vectors for the
codes associated with the visit. Because closely-spaced visits
are usually related clinically, we employ a time window to
split the sequence of visits into multiple subsequences of
equal length. A subsequence might contain multiple visits if
they occurred within the same time window, or there might
be no visits during a particular time window yielding an
empty subsequence. Thus we transform the original sequence
of irregularly-spaced visits into a sequence of subsequences
with equal intervals, which is preferable for recurrent neural
networks. The width of the subsequence window deﬁnes the
time granularity of the method and its optimal width is related
to the acuity (i.e., stability) of the clinical characteristics
involved in the predication task. In future work it may be
possible to deﬁne the relationship between clinical acuity and
optimal subsequence width, or develop methods for learning
an optimal width for a deﬁned prediction task.

Because all medical events occurring within a subsequence

VOLUME 0, 2018

5

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 4. A graphical illustration of the network in the Patient2Vec representation learning framework

the input is a medical code and the target to predict are the
medical codes occurred in the same visit.

Hence, each subsequence is a matrix consisting of the
vectors of medical codes occurred during this associated time
window.

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Given a sequence of subsequences encoded by vectors of
medical codes, this step employs the within-subsequence
attention which allows the network itself to learn the weights
of vectors in the subsequence according to its contribution to
the prediction target.

Here, we denote the sequence of patient i as s(i), and v(i)
t

t

, · · · , v(i)

1 , · · · , v(i)

denotes the tth subsequence in sequence s(i), where t ∈
{1, 2, · · · , T }. Thus, s(i) = {v(i)
T }. To
simplify the notation, we omit i in the following explanation.
Subsequence vt ∈ Rn×d is a matrix of medical codes such
that vt = {vt1 , vt2 , · · · , vtj , · · · , vtn }, where vtj ∈ Rd
is the vector representation of the jth medical code in the
tth subsequence vt and there are n medical codes in a
subsequence. In real EHR data, it is very likely that the
numbers of medical codes in each visit or time window are
different, thus, we utilize the padding approach to obtain a
consistent matrix dimensionality in the network.

To assign attention weights, we utilize the one-side con-
volution operation with a ﬁlter ωα ∈ Rd and a nonlinear
activation function. Thus, the weight vector αt is generated

6

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

for medical codes in the subsequence vt, presented in Equa-
tion 13.

αt = tanh(Conv(ωα, vt))

(13)

where αt = {αt1, αt2, · · · , αtn }, and ωα ∈ Rd is the
weight vector of the ﬁlter. The convolution operation Conv
is presented in Equation 14.

˜αtj = (ωα)(cid:124)vtj + bα

(14)

where bα is a bias term. Then, given the original matrix vt
and the learned weights αt, an aggregated vector xt ∈ Rd is
constructed to represent the tth subsequence, presented in 15.

xt =

αtj vtj

n
(cid:88)

j=1

(15)

Given Equation 15, we obtain a sequence of vectors, x =
{x1, x2, · · · , xt, · · · , xT }, to represent a patient’s medical
history.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of embedded subsequences, this step em-
ploys the subsequence-level attention which allows the net-
work itself to learn the weights of subsequences according to
their contribution to the prediction target.

To capture the longitudinal dependencies, we utilize a

bidirectional GRU-based RNN, presented in Equations 16.

h1, · · · , ht, · · · , hT = GRU (x1, · · · , xt, · · · , xT )

(16)

where ht ∈ Rk represents the output by the GRU unit at
the tth subsequence. Then, we introduce a set of linear and
softmax layers to generate M hops of weights β ∈ RM ×T
for subsequences. Then, for the hop m

γmt = (wβ

m)(cid:124)ht + bβ

βm1 , · · · , βmT =

(17)

(18)

sof tmax(γm1 , · · · , γmt, · · · , γmT )

m ∈ Rk. Thus, with the subsequence-level weights
where wβ
and hidden outputs, we construct a vector cm ∈ Rk to repre-
sent a patient’s medical visit history with one hop of subse-
quence weights, presented in the following Equation 19.

cm =

βmtht

(19)

T
(cid:88)

t=1

Then, a context vector c ∈ RM ×k is constructed by

concatenating c1, c2, · · · , cM .

VOLUME 0, 2018

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the context vector c, this step integrates the patients
characteristics a ∈ Rq into the context vector for a com-
plete vector representation of the patient’s EHR data. In
this research, the patient characteristics include demographic
information and some static medical conditions, such as age,
gender, and previous hospitalization. Thus, an aggregated
vector is constructed, c(cid:48) ∈ RM ×k+q, by adding a as addi-
tional dimensions to the context vector c.

E. PREDICTING OUTCOME
Given the vector representation of the complete medical
history and characteristics of patients, c(cid:48), we add a linear and
a softmax layer for the ﬁnal outcome prediction, as presented
in Equation 20.

ˆy = sof tmax(wc(cid:124)

c(cid:48) + bc)

(20)

To train the network, we use cross-entropy as the loss

function, presented in Equation 21.

L = −

yilog(ˆyi) + (1 − yi)log(1 − ˆyi)

(21)

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

+

||ββ(cid:124) − I||2
F

where N is the total number of observations. Here, yi is
a binary variable in classiﬁcation problems, while model
output ˆyi is real-valued. The second term in Equation 21 is
to penalize redundancy if the attention mechanism provides
similar subsequence weights for different hops of attention,
which is derived from [32]. This penalty term encourages the
multiple hops to focus on diverse areas and each hop focuses
on a small area.

Thus, we obtain a ﬁnal output for the prediction of out-
comes and a complete personalized vector representation of
the patient’s longitudinal EHR data.

V. EVALUATION
A. BACKGROUND
Although health care spending has been a relatively stable
share of the Gross Domestic Product (GDP) in the United
States since 2009, the costs of hospitalization, the largest
single component of health care expenditures,
increased
by 4.1% in 2014 [33]. Unplanned hospitalization is also
distressing and can increase the risk of related adverse events,
such as hospital-acquired infections and falls [34], [35].
Approximately 40% hospitalizations in the United King-
dom are unplanned and are potentially avoidable [36]. One
important form of unplanned hospitalization is hospital re-
admissions within 30 days of discharge, which is ﬁnancially
penalized in the United States. Early interventions targeted to
patients at risk of hospitalization could help avoid unplanned
admissions, reduce inpatient health care cost and ﬁnancial

7

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 5. A graphical illustration of the experimental setting for the risk
prediction of hospitalization

penalties for providers, and reduce emergency department
congestion [37].

In this research, we apply our proposed representation
learning framework to the risk prediction of future hospi-
talization. Many studies have been conducted by researchers
to predict the risk of 30-day readmission, or the admission
risk of a particular population, such as patients with Am-
bulatory Care Sensitive Conditions (ACSCs), patients with
heart failure, etc. [38]–[41]. Here, we focus on the general
population and the objective is to predict the risk of all-cause
hospitalization using longitudinal EHR data.

B. EXPERIMENTAL DESIGN
In this research, we use de-identiﬁed EHR data from the
University of Virginia Health System covering 75 months be-
ginning in September 2010. This dataset contains 2,343,651
inpatient and outpatient visits of 473,915 distinct patients.
We extracted visit data with diagnosis, medication, and pro-
cedure codes.

We deﬁned the observation window and prediction period
to validate the proposed method. We ﬁrst extract all patients
with a medical record of at least 1.5 years, where the ﬁrst year
is the observation window and the medical records in this
time window are used for feature construction. The follow-
ing 6 months is the hold-off period for the purpose of early
detection. For the positive class, we take all patients who
have hospitalization after the ﬁrst 1.5 years in their medical
history, while the negative class consists of patients who have
no hospitalization after 1.5 years. To better illustrate the ex-
perimental setting, we present the observation window, hold-
off and onset of outcome event in Figure 5. Here, the medical
codes include diagnosis, medication, and procedure codes,
and a vector representation is learned for each code. In this
dataset, diagnoses are primarily coded in ICD-9 and a small
portion is ICD-10 codes, while procedures are mainly using
CPT codes with a few ICD-9 procedure codes. The codes of
medications are using the pharmaceutical categories. Overall,
there are 94 distinct medication categories, 34,419 distinct
diagnoses codes, and 7,895 distinct procedure codes in the
EHR data. The dimension of the learned vectors of medical
codes is set to 100. Medical codes that appear in less than 50
patients medical records are excluded as rare events.

To construct the subsequences of medical codes, we use l
days as the time window. Figure 6 presents the cumulative
histogram and density plot of the numbers of visits in the
observation window, and we observe that the majority of

FIGURE 6. The cumulative histogram and density plot of patients’ numbers of
visits

patients have a small number of visits during the observation
window (less than 25% of patients have more than 4 visits).
Thus, we set l to 90 days, which split the observation window
into 4 subsequences.

Within each subsequence, the number of distinct medical
codes were computed and patients with more medical codes
in a subsequence than the 95% quantile were excluded from
the dataset. Overall, there are 8,841 and 89,101 patients in
the target and control groups, respectively. Each group is
randomly split into training, validation and testing sets with
a 7:1:2 ratio. Thus, 70% are used for training, another 20%
is used for testing, and the rest 10% are used for parameter
tuning and early stopping. The stochastic gradient descent
algorithm is used in training to minimize the cross-entropy
loss function, shown in Equation 21.

To evaluate the proposed representation learning frame-
work, we compare the prediction performance of the pro-
posed model with baseline approaches as follows.

1) Logistic regression (LR)

The inputs are the aggregated counts of grouped medical
codes over the entire observation window. Since the di-
mensionality of raw medical codes is huge, AHRQ clini-
cal classiﬁcations of diagnoses and procedures are used to
achieve a more general clustering of medical codes [42].
The medication codes are the pharmaceutical classes. Fur-
thermore, patient characteristics and previous inpatient visit
are also considered, where age and gender are demographic
information, and a binary indicator is utilized to represent
the presence of the previous hospitalization. Hence, the input
is a 436-dimensional vector representing a patient’s medical
history and characteristics.

8

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 1. The predictive performance of baselines and the proposed Patient2Vec framework

Methods
LR
MLP
RETAIN
FRNN-MGE
BiRNN-MGE
FRNN-MVE
BiRNN-MVE
Patient2Vec

Sensitivity
0.637 ± 0.010
0.727 ± 0.013
0.553 ± 0.012
0.636 ± 0.012
0.600 ± 0.012
0.753 ± 0.011
0.724 ± 0.010
0.769 ± 0.010

Speciﬁcity
0.728 ± 0.003
0.617 ± 0.004
0.710 ± 0.003
0.739 ± 0.004
0.777 ± 0.003
0.676 ± 0.004
0.707 ± 0.003
0.694 ± 0.004

AUC
0.721 ± 0.006
0.713 ± 0.007
0.663 ± 0.007
0.759 ± 0.006
0.768 ± 0.007
0.785 ± 0.006
0.788 ± 0.005
0.799 ± 0.005

F2 score
0.434 ± 0.006
0.423 ± 0.007
0.370 ± 0.008
0.438 ± 0.009
0.439 ± 0.009
0.470 ± 0.008
0.473 ± 0.008
0.492 ± 0.007

2) Multi-layer perceptron (MLP)
A multi-layer perceptron is trained to predict hospitalization
using the same inputs for logistic regression. Here, we use a
one hidden layer MLP with 256 hidden nodes.

3) Forward RNN with medical group embedding
(FRNN-MGE)
We split the sequence into subsequences with equal interval l.
The input at each step is the counts of medical groups within
the associated time interval, and the patient characteristics are
appended as additional features in the ﬁnal logistic regression
step. Here, the RNN is a forward GRU (or LSTM [18]) with
one hidden layer and the size of the hidden layer is 256.

4) Bidirectional RNN with medical group embedding
(BiRNN-MGE)
The inputs used for this baseline is the same as the one for
the FRNN-MGE [15]. The RNN used here is a bidirectional
GRU with one hidden layer and the size of the hidden layer
is 256.

5) Forward RNN with medical vector embedding
(FRNN-MVE)
We split the sequence into subsequences with equal interval l.
The input at each step is the vector representation of the
medical codes within the associated time interval, and the
patient characteristics are appended as additional features in
the ﬁnal logistic regression step. Here, the RNN is a forward
GRU (or LSTM [28]) with one hidden layer and the size of
the hidden layer is 256.

6) Bidirectional RNN with medical vector embedding
(BiRNN-MVE)
The inputs used for this baseline is the same as the one for
the FRNN-MVE [25]. The RNN used here is a bidirectional
GRU or LSTM [15] with one hidden layer and the size of the
hidden layer is 256.

7) RETAIN
This model uses reverse time attention mechanism on RNNs
for an interpretable representation of patient’s EHR data [31].
The inputs are the same as the one for FRNN-MGE, which
takes the counts of medical grouping within each time inter-
val to construct features. Similarly, the two RNNs used for

generating weights are GRU-based and the size of the hidden
layers are 256.

8) Patient2Vec
The inputs are the same as that for FRNN-MVE. One ﬁlter
is used when generating weights for within-subsequence
attention, and three ﬁlters are used for subsequence-level
attention. Similarly, the RNN used here is GRU-based and
there is one hidden layer and the size of the hidden layer
is 256.

The inputs of all baselines and Patient2Vec are normal-
ized to have zero mean and unit variance. We model the
risk of hospitalization based on Patient2Vec and baseline
representations of patients’ medical histories, and the model
performance is evaluated with Area Under Curve(AUC),
sensitivity, speciﬁcity, and F2-score. The validation set is
used for parameter tuning and early stopping in the train-
ing process. Each experiment is repeated 20 times and we
calculate the averages and standard deviations of the above
metrics, respectively.

C. EXPERIMENTAL RESULTS
The predictive performance of Patient2Vec and baselines are
presented in Table 1. The results shown here for the RNN-
based models are based on time interval l = 90 days to
construct subsequences.

According to Table 1, the RNN-based models are generally
capable of achieving higher prediction performance in terms
of sensitivity, AUC and F2 score, except for the RNN models
based on medical group embedding which have lower sensi-
tivity. Among all RNN-based approaches, the ones based on
vector embedding outperform those based on medical group
embedding in terms of sensitivity, AUC, and F2 score. The
bidirectional RNN models generally have higher speciﬁcity
but lower sensitivity than the forward RNN models, while
the bidirectional ones have comparable AUC and F2 score
with the forward ones, respectively. Generally, the proposed
Patient2Vec framework outperforms the baseline methods,
especially in terms of sensitivity and F2 score.

D. VISUALIZATION & INTERPRETATION
In addition to predictive performance, we interpret
the
learned representation by understanding the relative impor-
tance of clinical events in a patient’s EHR data. Considering

VOLUME 0, 2018

9

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 7. The heat map showing feature importance for Patient A

FIGURE 8. The proﬁle of Patient A.

FIGURE 9. The proﬁle of Patient B.

the feature importance learned by Patient2Vec are person-
alized for an individual patient, we illustrate it with two
example patients. Figures 8 and 9 present the proﬁles of
two individuals, Patient A and Patient B, respectively. To
facilitate the interpretation, instead of using raw medical
codes, we present the clinical groups from the AHRQ clinical
classiﬁcation software on diagnoses and procedure codes, as
well as pharmaceutical groups for medications.

According to Figure 8, Patient A is a male patient who
has hospitalization history in the observation window and
is admitted to the hospital seven months after the end of
the observation window for congestive heart failure. The

predicted risk is 96.4%, while the risk decreases for female
patients or patients without hospitalization history. It is also
not surprising to observe an increased risk for older patients.
The heat map in Figure 7 shows the relative importance of
the medical events in this patient’s medical record at each
time window and the ﬁrst row of the heat map presents
the subsequence-level attention. The darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the last subsequence,
t4, is the most important with respect to hospitalization risk,
followed by t1, t2, and t3 in order of importance.

Among all the clinical events in the subsequence t4, we

10

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 2. The top clinical groups with high weights in hospitalized patients

Index

Clinical Groups

Diagnoses
1
2
3

Essential hypertension
Other connective tissue disease
Spondylosis; intervertebral disc disorders; other back
problems
Other lower respiratory disease
Disorders of lipid metabolism
Other aftercare
Diabetes mellitus without complication
Screening and history of mental health and substance
abuse codes
Other nervous system disorders
Other screening for suspected conditions (not mental
disorders or infectious disease)

Other OR therapeutic procedures on nose; mouth and
pharynx
Suture of skin and subcutaneous tissue
Other therapeutic procedures on eyelids; conjunctiva;
cornea

Laboratory - Chemistry and hematology
Other laboratory
Other OR therapeutic procedures of urinary tract
Other OR procedures on vessels other than head and
neck

Therapeutic radiology for cancer treatment

Procedures
1

9
10

4
5
6
7
8

2
3

4
5
6
7

8

2

Medications
1

Diagnostic Products

Analgesics-Narcotic

observe that the OR therapeutic procedures (nose, mouth,
and pharynx), laboratory (chemistry and hematology), coro-
nary atherosclerosis & other heart disease, cardiac dys-
rhythmias, and conduction disorders are the ones with the
highest weights, while other events such as other connected
tissue disease are less important in terms of future hospi-
talization risk. Additionally, some medications appear to be
informative as well, including beta blockers, antihyperten-
sives, anticonvulsants, anticoagulants, etc. In the ﬁrst-time
window, the medical events with high weights are coro-
nary atherosclerosis & other heart disease, gastrointestinal
hemorrhage, deﬁciency and anemia, and other aftercare. In
the next subsequence, the most important medical events
are heart diseases and related procedures such as coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
conduction disorders, hypertension with complications, other
OR heart procedures, and other OR therapeutic nervous
system procedures. We also observe that the kidney disease
related diagnoses and procedures appear to be important
features. Throughout the observation window, the coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
and conduction disorders constantly show high weights with
respect to hospitalization risk, and the ﬁndings are consistent

FIGURE 10. The heat map showing feature importance for Patient B

with medical literature.

Figure 9 presents the proﬁle of Patient B, which is a male
patient without hospitalization in the observation window.
This patient is hospitalized for occlusion of cerebral arter-
ies approximately one year after the observation window,
and the predicted risk is 74.6%. For a similar patient who
is 10 years older or with previous hospitalization history,
the risk increases by 4.2% and 1%, respectively, while there
is a smaller risk of hospitalization for a female patient. To
illustrate the medical events of Patient B, the heat map in
Figure 10 depicts the relative importance of medical groups
in the subsequences, as well as the subsequence-level weights
for hospitalization risk. Similarly, the darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the second subse-
quence appears to be the most important, while the last one is
less predictive of future hospitalization. In fact, the medical
events in the last time window are spondylosis, intervertebral
disc disorders, other back problems and other bone disease &
musculoskeletal deformities, and malaise and fatigue, which
are not highly related to the cause of hospitalization of Patient
B.

In the most predictive subsequence, t2, we observe that
other OR heart procedures, genitourinary symptoms, spondy-
losis, intervertebral disc disorders, other back problems,
therapeutic procedures on eyelid, conjunctiva, and cornea,
and arterial blood gases have high attention weights. In the

VOLUME 0, 2018

11

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 3. The top diagnosis groups with high weights in patients hospitalized
for osteoarthritis, septicemia, acute myocardial infarction, congestive heart
failure, and diabetes mellitus with complications, respectively

Index

Diagnosis Groups

In patients admitted for osteoarthritis

Osteoarthritis
Other connective tissue disease

1
2

3
4

5

1
2

3
4
5

1
2
3

4
5

1
2
3
4
5

1
2
3

4
5

Other non-traumatic joint disorders
Spondylosis; intervertebral disc disorders; other back prob-
lems
Other aftercare

In patients admitted for septicemia

Essential hypertension
Diabetes mellitus without complication

Disorders of lipid metabolism
Other lower respiratory disease
Other aftercare

In patients admitted for acute myocardial infarction

Coronary atherosclerosis and other heart disease
Medical examination/evaluation
Other screening for suspected conditions (not mental disorders
or infectious disease)

Other lower respiratory disease
Disorders of lipid metabolism

In patients admitted for congestive heart failure

Congestive heart failure (nonhypertensive)
Coronary atherosclerosis and other heart disease
Cardiac dysrhythmias
Diabetes mellitus without complication
Other lower respiratory disease

In patients admitted for diabetes mellitus with complications

Diabetes mellitus with complications
Diabetes mellitus without complication
Other aftercare

Other nutritional; endocrine; and metabolic disorders
Fluid and electrolyte disorders

earliest time window, the most important medical events also
include therapeutic procedures on eyelid, conjunctiva, and
cornea, arterial blood gases, while diabetes, hypertension
as well as diagnostic products show their relatively high
importance. Throughout the observation window, medical
events spondylosis, intervertebral disc disorders, other back
problems, therapeutic procedures on eyelid, conjunctiva, and
cornea are constantly with high attention weights. Here, di-
agnostic products is a medication class, which include barium
sulfate, iohexol, gadopentetate dimeglumine, iodixanol, tu-
berculin puriﬁed protein derivative, iodixanol, regadenoson,
acetone (urine), and so forth. These medications are primarily
for blood or urine testing, or used as radiopaque contrast
agents for x-rays or CT scans for diagnostic purposes.

Additionally, we attempt to interpret the learned repre-
sentation and feature importance at the population-level. In
Table 2, we present the top 20 clinical groups with high
weights among hospitalized patients in the test set.

According to Table 2, the most predictive diagnosis groups
for future hospitalization are chronic diseases, including
essential hypertension, diabetes, lower respiratory disease,
disorders of lipid metabolism, and musculoskeletal diseases
such as other connective tissue disease and spondylosis,
intervertebral disc disorders, other back problems. The most
important procedures are some OR therapeutic procedures
and laboratory tests, such as the OR procedures on nose,
mouth, and pharynx, vessels, urinary tract, eyelid, conjunc-
tiva, cornea, etc. It is not surprising to see that diagnostic
products are showing with high weights, considering these
medications are used in testing or examinations for diagnos-
tic purposes.

Moreover, we present the top diagnoses groups with high
weights in patients hospitalized for different primary causes.
Table 3 shows the top 5 diagnosis groups with high weights
in patients admitted for osteoarthritis, septicemia (except in
labor), acute myocardial infarction, congestive heart failure
(nonhypertensive), and diabetes mellitus with complications,
respectively. Accordingly, we observe that the most impor-
tant diagnoses for hospitalization risk prediction in popu-
lation admitted for osteoarthritis are musculoskeletal dis-
eases such as connective tissue disease, joint disorders, and
spondylosis. However, the diagnoses with highest weights
in the patients admitted for septicemia are chronic diseases
including essential hypertension, diabetes, disorders of lipid
metabolism, and respiratory disease. The top diagnoses have
many overlaps between the populations admitted for acute
myocardial infarction and for congestive heart failure, con-
sidering both populations are admitted for heart diseases.
the overlapped diagnosis groups include coronary
Here,
atherosclerosis and other heart diseases and lower respiratory
diseases. As for patients admitted for diabetes with com-
plications, the top diagnoses are diabetes with or without
complications, nutritional, endocrine, metabolic disorders,
and ﬂuid and electrolyte disorders. In general, the learned
feature importance is consistent with medical literature.

VI. DISCUSSION
Our proposed framework is applied to the prediction of
hospitalization using real EHR data that demonstrates its
prediction accuracy and interpretability. This work could be
further enhanced by incorporating the follow-up information
on the negative patient population and investigate if it in-
deed shows an improved health outcome or the patient is
hospitalized elsewhere. Patient2Vec employs a hierarchical
attention mechanism, allowing us to directly interpret the
weights of clinical events. In future work, we will extend the
attention to incorporate demographic information for a more
comprehensive and automatic interpretation.

Although we apply Patient2Vec to the early detection of
long-term hospitalization, i.e., at least 6 months after the
previous hospitalization, it could be used to predict the risk
of 30-day readmission to help prevent unnecessary rehospi-
talizations.

12

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

VII. CONCLUSION
In this paper, we propose a representation learning frame-
work, Patient2Vec, to learn a personalized interpretable deep
representation of EHR data based on recurrent neural net-
works and the attention mechanism. This work improves
the performance of predictive models as well as deepens
the understanding of disease correlations. We apply this
framework to the risk prediction of hospitalization using
patients’ longitudinal EHR data. The experimental results
demonstrate that the proposed Patient2Vec representation is
capable of achieving a more accurate prediction than base-
lines approaches. Moreover, the learned feature importance
in the representations are interpreted both at the individual
and population levels to facilitate clinical insights.

In this work, the proposed Patient2Vec framework is eval-
uated with the risk prediction of all-cause hospitalization,
but in the future could be applied to predict hospitalization
in more speciﬁc populations, other health related prediction
problems, or domains outside of health.

REFERENCES
[1] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical
attention networks for document classiﬁcation,” in Proceedings of the
2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, 2016, pp.
1480–1489.

[2] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,” arXiv

preprint arXiv:1408.5882, 2014.

[3] J. Howard and S. Ruder, “Fine-tuned language models for text classiﬁca-

tion,” arXiv preprint arXiv:1801.06146, 2018.

[4] M. M. Lopez and J. Kalita, “Deep learning applied to nlp,” arXiv preprint

arXiv:1703.03091, 2017.

[5] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using
rnn encoder-decoder for statistical machine translation,” arXiv preprint
arXiv:1406.1078, 2014.

[6] A. L. Nobles, J. J. Glenn, K. Kowsari, B. A. Teachman, and L. E. Barnes,
“Identiﬁcation of imminent suicide risk among young adults using text
messages,” in Proceedings of the 2018 CHI Conference on Human Factors
in Computing Systems. ACM, 2018, p. 413.

[7] K. Kowsari, D. E. Brown, M. Heidarysafa, K. J. Meimandi, M. S. Gerber,
and L. E. Barnes, “Hdltex: Hierarchical deep learning for text classiﬁca-
tion,” in 2017 16th IEEE International Conference on Machine Learning
and Applications (ICMLA), Dec 2017, pp. 364–371.

[8] K. Kowsari, M. Heidarysafa, D. E. Brown, K. Jafari Meimandi, and L. E.
Barnes, “Rmdl: Random multimodel deep learning for classiﬁcation.”
ACM, 2018.

[9] H. Strobelt, S. Gehrmann, H. Pﬁster, and A. M. Rush, “Lstmvis: A tool
for visual analysis of hidden state dynamics in recurrent neural networks,”
IEEE transactions on visualization and computer graphics, vol. 24, no. 1,
pp. 667–676, 2018.

[10] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” Scientiﬁc
reports, vol. 8, no. 1, p. 6085, 2018.

[11] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches
preprint

neural machine

translation,”

arXiv

attention-based
to
arXiv:1508.04025, 2015.

[12] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[13] D. Yogatama, C. Dyer, W. Ling, and P. Blunsom, “Generative and discrim-
inative text classiﬁcation with recurrent neural networks,” arXiv preprint
arXiv:1703.01898, 2017.

[14] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent

trends
language processing,” arXiv preprint

in deep learning based natural
arXiv:1708.02709, 2017.

[15] M. Basaldella, E. Antolli, G. Serra, and C. Tasso, “Bidirectional lstm
recurrent neural network for keyphrase extraction,” in Italian Research

Conference on Digital Libraries, Springer.
lishing, 2018, pp. 180–187.

Springer International Pub-

[16] S. Ghosh, O. Vinyals, B. Strope, S. Roy, T. Dean, and L. Heck, “Con-
textual lstm (clstm) models for large scale nlp tasks,” arXiv preprint
arXiv:1602.06291, 2016.

[17] B. Yue, J. Fu, and J. Liang, “Residual recurrent neural networks for
learning sequential representations,” Information, vol. 9, no. 3, p. 56, 2018.
[18] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
gated recurrent neural networks on sequence modeling,” arXiv preprint
arXiv:1412.3555, 2014.

[19] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difﬁculty of training
recurrent neural networks.” ICML (3), vol. 28, pp. 1310–1318, 2013.
[20] D. Britz, “Recurrent neural network tutorial,” http://www.wildml.com/

2015/10/, 2015, [Accessed on October 5, 2017].

[21] V. Mnih, N. Heess, A. Graves et al., “Recurrent models of visual attention,”
in Advances in neural information processing systems, 2014, pp. 2204–
2212.

[22] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for
abstractive sentence summarization,” arXiv preprint arXiv:1509.00685,
2015.

[23] T. Ma, C. Xiao, and F. Wang, “Health-atm: A deep architecture for
multifaceted patient health record representation and risk prediction,” in
Proceedings of the 2018 SIAM International Conference on Data Mining.
SIAM, 2018, pp. 261–269.

[24] A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J. Liu,
X. Liu, J. Marcus, M. Sun et al., “Scalable and accurate deep learning with
electronic health records,” npj Digital Medicine, vol. 1, no. 1, p. 18, 2018.
[25] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzell, “Learning to diagnose
with lstm recurrent neural networks,” arXiv preprint arXiv:1511.03677,
2015.

[26] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” arXiv preprint
arXiv:1606.01865, 2016.

[27] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghas-
semi, B. Moody, P. Szolovits, L. A. Celi, and R. G. Mark, “Mimic-iii, a
freely accessible critical care database,” Scientiﬁc data, vol. 3, 2016.
[28] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart, and J. Sun, “Doctor
ai: Predicting clinical events via recurrent neural networks,” in Machine
Learning for Healthcare Conference, 2016, pp. 301–318.

[29] Z. Che, S. Purushotham, R. Khemani, and Y. Liu, “Interpretable deep
models for icu outcome prediction,” in AMIA Annual Symposium Pro-
ceedings, vol. 2016. American Medical Informatics Association, 2016,
p. 371.

[30] J. H. Friedman, “Greedy function approximation: a gradient boosting

machine,” Annals of statistics, pp. 1189–1232, 2001.

[31] E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart,
“Retain: An interpretable predictive model for healthcare using reverse
time attention mechanism,” in Advances in Neural Information Processing
Systems, 2016, pp. 3504–3512.

[32] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Ben-
gio, “A structured self-attentive sentence embedding,” arXiv preprint
arXiv:1703.03130, 2017.

[33] C. M. Torio and B. J. Moore, “National inpatient hospital costs: The most
expensive conditions by payer, 2013,” https://www.hcup-us.ahrq.gov/
reports/statbriefs/sb204-Most-Expensive-Hospital-Conditions.jsp, 2016.
[34] E. Wallace, E. Stuart, N. Vaughan, K. Bennett, T. Fahey, and S. M.
Smith, “Risk prediction models to predict emergency hospital admission
in community-dwelling adults: a systematic review,” Medical care, vol. 52,
no. 8, p. 751, 2014.

[35] E. N. de Vries, M. A. Ramrattan, S. M. Smorenburg, D. J. Gouma, and
M. A. Boermeester, “The incidence and nature of in-hospital adverse
events: a systematic review,” Quality and safety in health care, vol. 17,
no. 3, pp. 216–223, 2008.

[36] S. Purdey and A. Huntley, “Predicting and preventing avoidable hospital
admissions: a review.” The journal of the Royal College of Physicians of
Edinburgh, vol. 43, no. 4, pp. 340–344, 2012.

[37] H. Ontario, “Early identiﬁcation of people at risk of hospitalization: Hos-
pital admission risk prediction (harp)-a new tool for supporting providers
and patients,” 2013.

[38] B. Zheng, J. Zhang, S. W. Yoon, S. S. Lam, M. Khasawneh, and S. Poranki,
“Predictive modeling of hospital readmissions using metaheuristics and
data mining,” Expert Systems with Applications, vol. 42, no. 20, pp. 7110–
7120, 2015.

VOLUME 0, 2018

13

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

[39] D. Kansagara, H. Englander, A. Salanitro, D. Kagen, C. Theobald,
M. Freeman, and S. Kripalani, “Risk prediction models for hospital
readmission: a systematic review,” The Journal of the American Medical
Association, vol. 306, no. 15, pp. 1688–1698, 2011.

[40] G. Giamouzis, A. Kalogeropoulos, V. Georgiopoulou, S. Laskar, A. L.
Smith, S. Dunbar, F. Triposkiadis, and J. Butler, “Hospitalization epidemic
in patients with heart failure: risk factors, risk prediction, knowledge gaps,
and future directions,” Journal of cardiac failure, vol. 17, no. 1, pp. 54–75,
2011.

[41] E. Prescott, A. M. Bjerg, P. K. Andersen, P. Lange, and J. Vestbo, “Gender
difference in smoking effects on lung function and risk of hospitalization
for COPD: results from a danish longitudinal population study,” European
Respiratory Journal, vol. 10, no. 4, pp. 822–827, 1997.

[42] Agency for Healthcare Research and Quality (AHRQ), “Clinical classi-
ﬁcations software (CCS) for ICD-9-CM,” https://www.hcup-us.ahrq.gov/
toolssoftware/ccs/ccs.jsp, 2015.

JENNIFER M. LOBO (jem4yb@virginia.edu) is
an Assistant Professor of Biomedical Informatics
in the Department of Public Health Sciences at
University of Virginia. She received her Ph.D. in
Industrial Engineering from North Carolina State
University, Raleigh, NC. Her research interests in-
volve using mathematical modeling and stochastic
optimization methods to build models that simu-
late the natural course of disease. These models
allow for estimation of outcomes under different
screening and treatment policies in the absence of randomized controlled
trials, and can be used to optimize screening and treatment decisions for
patients with chronic diseases. Her projects include optimizing treatment
for patients with type 2 diabetes, generating individualized decision analysis
models for prostate cancer patients, and developing optimal imaging surveil-
lance guidelines for recurrent kidney cancer.

JINGHE ZHANG (jz4kg@virginia.edu) is a lead
data scientist at Target Corporation. She received
her Ph.D. in Systems Engineering from the Uni-
versity of Virginia. Prior to entering the Ph.D. pro-
gram at UVA, she received the Master of Science
in Industrial and Systems Engineering from the
State University of New York at Binghamton. Her
research interests are in natural language process-
ing, machine learning, recommender systems, and
health informatics.

LAURA E. BARNES (lb3dp@virginia.edu) is an
Associate Professor in Systems and Information
Engineering and the Data Science Institute at the
University of Virginia. She received her Ph.D. in
Computer Science from the University of South
Florida, Tampa, FL. She directs the Sensing Sys-
tems for Health (S2He) Lab which focuses on
understanding the dynamics and personalization
of health and well-being through mobile sensing
and analytics.

(kk7nc@virginia.edu) is
KAMRAN KOWSARI
a Ph.D. student in the Department of Systems
and Information Engineering at the University of
Virginia, Charlottesville, VA. He is a member of
the Sensing Systems for Health Lab. He received
his Master of Science from Department of Com-
puter Science at The George Washington Univer-
sity, Washington, DC. He has more than ten years
of experience in machine learning and software
development. His experience includes numerous
industrial and academic projects. His research interests include natural
language processing, machine learning, deep learning, artiﬁcial intelligence,
text mining, and unsupervised learning.

JAMES H. HARRISON, JR., (jhh5y@virginia.
edu) is Associate Professor of Pathology and Di-
rector of Laboratory Information Systems at the
University of Virginia Medical Center and also has
appointments in the Departments of Public Health
Sciences in the UVA School of Medicine, and
Systems and Information Engineering in the UVA
School of Engineering and Applied Sciences. He
received his MD and PhD (Pharmacology) de-
grees from Medical University of South Carolina,
Charleston, SC, completed residencies in Anatomic Pathology and Labora-
tory Medicine at Yale-New Haven Hospital, New Haven, CT, and completed
a postdoctoral fellowship in Environmental Toxicology at Yale University,
New Haven, CT. Dr. Harrison has over 25 years of experience in the ﬁeld
of medical informatics, including work in clinical laboratory information
systems, electronic health records, clinical data analysis, and clinical data
standards development.

14

VOLUME 0, 2018

Date of publication 10, 2018 , date of current version 10, 2018.

Digital Object Identiﬁer 10.1109/ACCESS.2018.2875677

Patient2Vec: A Personalized
Interpretable Deep Representation of the
Longitudinal Electronic Health Record

JINGHE ZHANG1, (Member, IEEE), KAMRAN KOWSARI1,4(Member, IEEE), JAMES H.
HARRISON2,3,5, JENNIFER M. LOBO2,5, and LAURA E. BARNES1,4,5, (Member, IEEE)
1Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, USA
2 Department of Public Health Sciences, University of Virginia, Charlottesville, VA 22904, USA
4 Sensing Systems for Health Lab, University of Virginia, Charlottesville, VA 22904, USA
3 Division of Laboratory Medicine Department of Pathology, University of Virginia, Charlottesville, VA 22904, USA
5Data Science Institute, University of Virginia, Charlottesville, VA 22904, USA
Corresponding author: Laura E. Barnes (lb3dp@virginia.edu)
This research was supported by a Jeffress Trust Award in Interdisciplinary Science.
Patient2Vec is shared as an open source tool at https://github.com/BarnesLab/Patient2Vec

ABSTRACT The wide implementation of electronic health record (EHR) systems facilitates the collection
of large-scale health data from real clinical settings. Despite the signiﬁcant increase in adoption of EHR
systems, this data remains largely unexplored, but presents a rich data source for knowledge discovery from
patient health histories in tasks such as understanding disease correlations and predicting health outcomes.
However, the heterogeneity, sparsity, noise, and bias in this data present many complex challenges.
This complexity makes it difﬁcult to translate potentially relevant information into machine learning
algorithms. In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep
representation of longitudinal EHR data which is personalized for each patient. To evaluate this approach,
we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive
performance with baseline methods. Patient2Vec produces a vector space with meaningful structure and it
achieves an AUC around 0.799 outperforming baseline methods. In the end, the learned feature importance
can be visualized and interpreted at both the individual and population levels to bring clinical insights.

INDEX TERMS Attention mechanism, gated recurrent unit, hospitalization, longitudinal electronic health
record, personalization, representation learning.

8
1
0
2
 
t
c
O
 
5
2
 
 
]

M
Q
.
o
i
b
-
q
[
 
 
3
v
3
9
7
4
0
.
0
1
8
1
:
v
i
X
r
a

I. INTRODUCTION

L ongitudinal EHR data resemble text documents from

many perspectives. A text document consists of a se-
quence of sentences, and a sentence is a sequence of words.
Similarly, the longitudinal health record of a patient consists
of a sequence of visits, and there is a list of clinical events,
including diagnoses, medications, and procedures, that occur
during a visit. Considering these similarities, representation
learning methods for text documents in Natural Language
Processing (NLP) have great potential to be applied to lon-
gitudinal EHR data.

Deep neural networks have become very popular in the
NLP ﬁeld and have been very successful in many applica-
tions, such as machine translation, question answering, text
classiﬁcation, document summarization, language modeling,
etc. [1]–[8]. These networks excel at complex language tasks

because they are capable of identifying high-order relation-
ships, the network structure can encode language structures,
and they allow the learning of a hierarchical representation
of the language, i.e., representations for tokens, phrases, and
sentences, etc.

Among a variety of deep learning methods, Recurrent
Neural Networks (RNNs) have shown their effectiveness in
NLP tasks because they have the ability to capture sequential
information [7]–[10] which is inherent in human language.
Traditional neural networks assume that inputs are inde-
pendent of each other, while an RNN computes the output
based on the current input as well as the “memory” from the
previous computation. Although vanilla RNNs are not good
at capturing long-term dependencies, many variants have
been proposed and validated that are effective in addressing

VOLUME 0, 2018

1

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

its performance with other baseline methods. In addition
to prediction performance, we further interpret the learned
representations with visualizations on example patients and
events. Finally, Section V provides a summary of this work.

II. RELATED WORK
In this section, we present an overview of a gated recurrent
unit, a type of RNN, which is capable of capturing long-term
dependencies. Then we brieﬂy introduce attention mecha-
nisms in neural networks that allow the network to attend
to certain regions of data, which is inspired by the visual
attention mechanism in humans. Additionally, we summarize
the RNN networks and attention mechanisms previously used
to mine EHR data.

A. RECURRENT NEURAL NETWORKS (RNN)

RNNs are expected to learn long-term dependencies by tak-
ing the previous state and the new input in the computation at
the current time step t. However, vanilla RNNs are incapable
of capturing the dependencies when the sequence is very long
due to the vanishing gradient problem [12]. Many variants of
the RNN network have been proposed to address this issue,
and long short term memory (LSTM) is one of the most
popular models used nowadays in NLP tasks [7], [8], [13]–
[16].

this issue.

In the medical domain, it is critical that analytical results
are interpretable, so that they can be understood and validated
by a human with expert knowledge and so that knowledge
captured by analysis can be used for process improvement.
Traditional deep neural networks have the disadvantage that
they lack interpretability. A substantial amount of work is
ongoing to make sense of the “black box”, and the attention
mechanism [11] is one of the more effective methods recently
developed to make the output of these algorithms more
interpretable.

Health care is undergoing unprecedented change, and there
is a great potential and demand for personalized care strate-
gies. Personalized medicine, also called precision medicine,
has previously focused on optimizing therapy to better ﬁt the
genetic makeup of the patient or the disease (e.g., the genetic
susceptibility of cancer to speciﬁc chemotherapy strategies).
The availability of EHR data and advances in machine learn-
ing create the potential for another type of personalization
of healthcare. This type of personalization has become ubiq-
uitous in our daily life. For example, customers have come
to expect personalized search on Google and personalized
product recommendations on Amazon and Netﬂix, based on
their charactersitics and previous experiences with the sys-
tems. Personalization of healthcare processes, based on a pa-
tient’s phenotype (physical and medical characteristics) and
healthcare experiences as documented in the health record,
may also improve "customer" satisfaction and it has the addi-
tional potential to improve healthcare efﬁciency, lower costs,
and yield better outcomes. We believe that representation
learning methods can capture a personalized representation
of the important heterogeneities in patients’ phenotypes and
medical histories at the population-level, and make these
representations available to drive healthcare decisions and
strategies.

This research is based on RNN models and the attention
mechanism with the objective of learning a personalized, in-
terpretable, and complete representation of patients’ medical
records. Our proposed framework is capable of learning a
personalized representation for each patient from a sequence
of clinical events. A hierarchical attention mechanism learns
personalized weights of clinical events, including hospital
visits and the procedures that they contain. These weights
allow us to interpret the relative importance and roles of clin-
ical events in the learned representations both at individual
and population levels. The ultimate goal is more accurate
prediction and better insight into the critical elements of
healthcare processes that can be used to improve healthcare
delivery.

The rest of this paper is organized as follows: Section II
summarizes the variants of RNNs and the attention mecha-
nism, as well as their application to EHR data. Section III
presents an overview of the proposed Patient2Vec repre-
sentation learning framework, and Section IV elaborates
the details of the algorithms. In Section V, the proposed
framework is evaluated for a prediction task and we compare

FIGURE 1. The top ﬁgure is a GRU gating unit and bottom ﬁgure shows an
LSTM unit [7]

2

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

1) Gated Recurrent Unit (GRU)
GRU is a simpliﬁed version of LSTM [7]. The basic idea
of GRU is to combat the vanishing gradient problem with a
gating mechanism. Hence the general recurrent structure in
GRU is identical to vanilla RNNs except that a GRU unit
is used in the computation at each time step rather than a
traditional simple recurrent unit.

In general, a GRU cell has two gates, i.e., a reset gate r
and an update gate z. The reset gate is used to determine
how to integrate the previous state into the computation of
the current state, while the update gate determines how much
the unit updates its activation.

Given the input xt at time step t, the reset gate rt is

computed as presented in Equation 1

rt = σ(Urxt + Wrst−1)

(1)

where Ur and Wr are the weight matrices of the reset gate
and st−1 is the hidden activation at time step t − 1. A similar
computation is performed for the update gate zt at time step t,
shown in Equation 2

zt = σ(Uzxt + Wzst−1)

(2)

where Uz and Wz are the weight matrices of update gate.
The current hidden activation ht is computed by

ht = (1 − zt)ht−1 + zt
(3)
where ˜ht is the candidate activation at time step t. The
computation of ˜ht is presented in Equation 4

˜ht

˜ht = tanh(Wxt + U(rt (cid:12) ht−1))

(4)

where U and W are weight matrices and (cid:12) represents
element-wise multiplication. Figure 1 presents a graphical
illustration of the GRU [7] and one unit of LSTM.

GRU is capable of learning long-term dependencies [17]
due to the additive component of update from t to t + 1
in the gating mechanism. Consequently, important features
will be carried forward in the input stream while irrelevant
information will be dropped. When the reset gate is 0, the
network is forced to drop previous states and reset with cur-
rent information. Moreover, the method provides shortcuts
such that the error is easily backpropagated without vanishing
too quickly [5], [18]. Hence, the GRU is well-suited to learn
long-term dependencies in sequence data.

2) Long Short-Term Memory (LSTM)
An LSTM unit is similar to a GRU, but with one more gate in
an LSTM unit (as shown in Figure 1). LSTM also preserves
long term dependencies more effectively than basic RNN.
This is particularly useful to overcome the vanishing gradient
problem [19]. Although LSTM has a chain-like structure sim-
ilar to RNN, LSTM uses multiple gates to carefully regulate
the amount of information that will be allowed into each node
state. Figure 1 shows the basic cell of an LSTM model. A step

VOLUME 0, 2018

by step explanation of an LSTM cell is as following:
Input gate:

it = σ(Wi[xt, ht−1] + bi),

Candid memory cell value:

˜Ct = tanh(Wc[xt, ht−1] + bc),

Forget gate activation:

ft = σ(Wf [xt, ht−1] + bf ),

New memory cell value:

Ct = it ∗ ˜Ct + ftCt−1,

Output gate value:

ot = σ(Wo[xt, ht−1] + bo),

ht = ot tanh(Ct),

(5)

(6)

(7)

(8)

(9)

(10)

In the above description all b represent bias vectors, all W
represent weight matrices, and xt is used as input to the
memory cell at time t. Also,the i, c, f, o indices refer to input,
cell memory, forget and output gates respectively. An RNN
can be biased when later words are more inﬂuential than the
earlier ones.

Empirically, LSTM and GRU achieve comparable per-
formance in many tasks but there are fewer parameters in
a GRU, which makes it a little faster to learn and able to
generalize with fewer data [20].

B. ATTENTION MECHANISM
Attention mechanisms, inspired by the visual attention sys-
tem found in humans, have become popular in deep learning.
Attention allows the network to focus on certain regions of
data, while perceiving other regions with “low resolution”.
In addition to higher accuracy, it also facilitates the interpre-
tation of learned representations. We elaborate an attention
mechanism on an RNN network, and Figure 2 presents a
graphical illustration.

According to Figure 2, a variable-length weight vector α
is learned based on hidden states [11]. Then a global context
vector is computed based on weights α and all the hidden
states to create the ﬁnal output. Equation 11 presents the
computation of the weight vector α = {α1, α2, · · · , αT },
where T is the length of the sequence

α1, α2, · · · , αT = f (Wαh + bα)

(11)

and where f is a nonlinear activation function, usually
sof tmax or tanh. Then, the context vector c is constructed
as:

c =

αtht

T
(cid:88)

t=1

Thus, the network puts more attention on the important
features for the ﬁnal prediction which can improve the model
performance. An additional beneﬁt is that the weights can

(12)

3

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

GRU-based model to address missing values in multivariate
time series data, in which the missing patterns are incorpo-
rated for improved prediction performance. This work has
been applied to the Medical Information Mart for Intensive
Care III (MIMIC-III) clinical database to demonstrate its
effectiveness in mining time series of clinical measurements
with missing values [27]. Longitudinal EHR data including
clinical events, such as diagnoses, medications, and pro-
cedures is also a potentially rich resource for predictive
modeling. Choi et al. [28] analyze this data with a GRU
network to forecast future clinical events, and it achieves a
better prediction performance than comparison models such
as logistic regression and MLP.

Difﬁculty in interpreting model behavior is one of the
major drawbacks of using deep learning to mine EHR data.
Some attempts have been made to address this issue. Che et
al. [29] propose an interpretable mimic learning method
which trains a mimic gradient boosting trees model to utilize
predicted labels or features learned by deep learning mod-
els for ﬁnal prediction [30]. Then the feature importances
learned by the tree-based models are used for knowledge
discovery. Attention mechanisms have been introduced re-
cently to improve the interpretability of the prediction results
of deep learning models in health analytics. Choi et al. [31]
develop an interpretable model with two levels of attention
weights learned from two reverse-time GRU models, re-
spectively. The experimental results on EHR data indicate
comparable prediction performance with conventional GRU
models but more interpretable results. Our work continues
the attempt to use attention mechanisms to improve the
interpretability of RNN-based models.

III. PATIENT2VEC SYSTEM MODEL
In this section, we provide an overview of the proposed
hierarchical representation learning framework. This frame-
work uses deep recurrent neural networks to capture the
complex relationships between clinical events in the patient’s
EHR data and employs the attention mechanism to learn
a personalized representation and to obtain relative feature
importance. The proposed representation learning framework
contains four steps and is presented graphically in Figure 3.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES

EHR data consists primarily of records of outpatient and
inpatient visits to healthcare providers. These visit records
include multiple clinical codes for diagnoses, symptoms,
procedures, therapies, and other observations and events that
occurred during the visit. Here, we treat the set of medical
codes associated with a visit as a sentence consisting of
words, except that there is no ordering in the words. Thus,
we adopt the word2vec approach to construct a vector to
represent each medical code.

FIGURE 2. The global attention model

be utilized to understand the importance of features such
that the models are more interpretable. The attention mech-
anism has been introduced to both Convolutional Neural
Networks (CNNs) and RNNs for various tasks and has
achieved many successes in the ﬁelds of computer vision and
NLP [11], [21], [22].

C. DEEP LEARNING IN EHR DATA
Previous studies on EHR data mainly use statistical meth-
ods or traditional machine learning techniques. Recently
researchers have started adapting deep learning approaches
to this data [23], [24], including textual notes, temporal
measurements of laboratory testing in the Intensive Care
Unit (ICU), and longitudinal data in patient populations.
Here, we summarize deep learning research in mining EHR
data and focus on the studies using RNN-based models.

Hospitalized patients, especially patients in ICUs, are
continuously monitored for cardiac, respiratory, and other
physical functions, creating a large volume of sequential data
in multiple dimensions. These measurements are utilized by
physicians to make diagnostic and treatment decisions. The
functions monitored may change over time and monitor-
ing may be irregular, based on a patient’s condition. It is
very challenging for traditional machine learning methods
to mine this multivariate time series data considering miss-
ing values, varying length, and irregular, non-simultaneous
sampling. Lipton et al. [25] trained an LSTM with a repli-
cated target to learn from these sequence data and used
this model to make predictions of diagnoses. The data used
in this research are time series of clinical measurements
with continuous values, and the LSTM models outperformed
logistic regression and MLP. Che et al. [26] developed a

4

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

are unlikely to contribute equally to the prediction of the tar-
get outcome, we cannot aggregate them with equal weights.
Instead, we employ a self-attention mechanism which trains
the network to learn the weights.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of subsequences with embedded medical
codes, we are able to input it into a recurrent neural network
to capture the temporal dependencies between events. How-
ever, the subsequences of visits are not contributing equally
to the outcome. Hence, we employ another level of attention
to learn the weights of the subsequences by the network itself
for the outcome prediction.

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the learned weights and hidden outputs, we aggregate
them into one universal vector for a comprehensive represen-
tation. In this step, the static information, such as age, gender,
previous hospitalization history is added as extra features, to
get a complete representation of a patient.

E. PREDICTING OUTCOME
Given the complete vector representation of a patient’s EHR
data, we add a logistic regression layer at the end for the
prediction of outcome.

IV. PATIENT2VEC REPRESENTATION LEARNING
ALGORITHM
In this section, we present the details of the proposed rep-
resentation learning framework, which is based on a GRU
network and a hierarchical attention mechanism. Figure 4
presents the structure of the proposed network with attention.

The proposed framework consists of ﬁve parts presented in
the following: I) Learning vector representations of medical
codes, II) Learning within-subsequence self-attention, III)
Learning subsequence-level self-attention, IV) Constructing
aggregated deep representation, V) Predicting outcome.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES
Given a patient’s raw EHR data, a sequence of visits, we
observe that a visit usually contains multiple medical codes.
Hence, it is feasible to learn a vector to represent the medical
code by capturing the relationships between the codes. In
this work, we employ the classical word2vec algorithm, skip-
gram. The basic idea of skip-gram is to learn a vector to
represent each word such that the probability of the context
to predict based on the target word is maximized. Hence,
the vectors of similar words are close to each other in the
learned feature space. In the skip-gram model, the vectors
are learned by training a shallow neural network to predict the
context words given an input word. Similarly, in our problem,

FIGURE 3. The Patient2Vec representation learning framework

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Clinical visits are represented as the set of vectors for the
codes associated with the visit. Because closely-spaced visits
are usually related clinically, we employ a time window to
split the sequence of visits into multiple subsequences of
equal length. A subsequence might contain multiple visits if
they occurred within the same time window, or there might
be no visits during a particular time window yielding an
empty subsequence. Thus we transform the original sequence
of irregularly-spaced visits into a sequence of subsequences
with equal intervals, which is preferable for recurrent neural
networks. The width of the subsequence window deﬁnes the
time granularity of the method and its optimal width is related
to the acuity (i.e., stability) of the clinical characteristics
involved in the predication task. In future work it may be
possible to deﬁne the relationship between clinical acuity and
optimal subsequence width, or develop methods for learning
an optimal width for a deﬁned prediction task.

Because all medical events occurring within a subsequence

VOLUME 0, 2018

5

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 4. A graphical illustration of the network in the Patient2Vec representation learning framework

the input is a medical code and the target to predict are the
medical codes occurred in the same visit.

Hence, each subsequence is a matrix consisting of the
vectors of medical codes occurred during this associated time
window.

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Given a sequence of subsequences encoded by vectors of
medical codes, this step employs the within-subsequence
attention which allows the network itself to learn the weights
of vectors in the subsequence according to its contribution to
the prediction target.

Here, we denote the sequence of patient i as s(i), and v(i)
t

t

, · · · , v(i)

1 , · · · , v(i)

denotes the tth subsequence in sequence s(i), where t ∈
{1, 2, · · · , T }. Thus, s(i) = {v(i)
T }. To
simplify the notation, we omit i in the following explanation.
Subsequence vt ∈ Rn×d is a matrix of medical codes such
that vt = {vt1 , vt2 , · · · , vtj , · · · , vtn }, where vtj ∈ Rd
is the vector representation of the jth medical code in the
tth subsequence vt and there are n medical codes in a
subsequence. In real EHR data, it is very likely that the
numbers of medical codes in each visit or time window are
different, thus, we utilize the padding approach to obtain a
consistent matrix dimensionality in the network.

To assign attention weights, we utilize the one-side con-
volution operation with a ﬁlter ωα ∈ Rd and a nonlinear
activation function. Thus, the weight vector αt is generated

6

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

for medical codes in the subsequence vt, presented in Equa-
tion 13.

αt = tanh(Conv(ωα, vt))

(13)

where αt = {αt1, αt2, · · · , αtn }, and ωα ∈ Rd is the
weight vector of the ﬁlter. The convolution operation Conv
is presented in Equation 14.

˜αtj = (ωα)(cid:124)vtj + bα

(14)

where bα is a bias term. Then, given the original matrix vt
and the learned weights αt, an aggregated vector xt ∈ Rd is
constructed to represent the tth subsequence, presented in 15.

xt =

αtj vtj

n
(cid:88)

j=1

(15)

Given Equation 15, we obtain a sequence of vectors, x =
{x1, x2, · · · , xt, · · · , xT }, to represent a patient’s medical
history.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of embedded subsequences, this step em-
ploys the subsequence-level attention which allows the net-
work itself to learn the weights of subsequences according to
their contribution to the prediction target.

To capture the longitudinal dependencies, we utilize a

bidirectional GRU-based RNN, presented in Equations 16.

h1, · · · , ht, · · · , hT = GRU (x1, · · · , xt, · · · , xT )

(16)

where ht ∈ Rk represents the output by the GRU unit at
the tth subsequence. Then, we introduce a set of linear and
softmax layers to generate M hops of weights β ∈ RM ×T
for subsequences. Then, for the hop m

γmt = (wβ

m)(cid:124)ht + bβ

βm1 , · · · , βmT =

(17)

(18)

sof tmax(γm1 , · · · , γmt, · · · , γmT )

m ∈ Rk. Thus, with the subsequence-level weights
where wβ
and hidden outputs, we construct a vector cm ∈ Rk to repre-
sent a patient’s medical visit history with one hop of subse-
quence weights, presented in the following Equation 19.

cm =

βmtht

(19)

T
(cid:88)

t=1

Then, a context vector c ∈ RM ×k is constructed by

concatenating c1, c2, · · · , cM .

VOLUME 0, 2018

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the context vector c, this step integrates the patients
characteristics a ∈ Rq into the context vector for a com-
plete vector representation of the patient’s EHR data. In
this research, the patient characteristics include demographic
information and some static medical conditions, such as age,
gender, and previous hospitalization. Thus, an aggregated
vector is constructed, c(cid:48) ∈ RM ×k+q, by adding a as addi-
tional dimensions to the context vector c.

E. PREDICTING OUTCOME
Given the vector representation of the complete medical
history and characteristics of patients, c(cid:48), we add a linear and
a softmax layer for the ﬁnal outcome prediction, as presented
in Equation 20.

ˆy = sof tmax(wc(cid:124)

c(cid:48) + bc)

(20)

To train the network, we use cross-entropy as the loss

function, presented in Equation 21.

L = −

yilog(ˆyi) + (1 − yi)log(1 − ˆyi)

(21)

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

+

||ββ(cid:124) − I||2
F

where N is the total number of observations. Here, yi is
a binary variable in classiﬁcation problems, while model
output ˆyi is real-valued. The second term in Equation 21 is
to penalize redundancy if the attention mechanism provides
similar subsequence weights for different hops of attention,
which is derived from [32]. This penalty term encourages the
multiple hops to focus on diverse areas and each hop focuses
on a small area.

Thus, we obtain a ﬁnal output for the prediction of out-
comes and a complete personalized vector representation of
the patient’s longitudinal EHR data.

V. EVALUATION
A. BACKGROUND
Although health care spending has been a relatively stable
share of the Gross Domestic Product (GDP) in the United
States since 2009, the costs of hospitalization, the largest
single component of health care expenditures,
increased
by 4.1% in 2014 [33]. Unplanned hospitalization is also
distressing and can increase the risk of related adverse events,
such as hospital-acquired infections and falls [34], [35].
Approximately 40% hospitalizations in the United King-
dom are unplanned and are potentially avoidable [36]. One
important form of unplanned hospitalization is hospital re-
admissions within 30 days of discharge, which is ﬁnancially
penalized in the United States. Early interventions targeted to
patients at risk of hospitalization could help avoid unplanned
admissions, reduce inpatient health care cost and ﬁnancial

7

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 5. A graphical illustration of the experimental setting for the risk
prediction of hospitalization

penalties for providers, and reduce emergency department
congestion [37].

In this research, we apply our proposed representation
learning framework to the risk prediction of future hospi-
talization. Many studies have been conducted by researchers
to predict the risk of 30-day readmission, or the admission
risk of a particular population, such as patients with Am-
bulatory Care Sensitive Conditions (ACSCs), patients with
heart failure, etc. [38]–[41]. Here, we focus on the general
population and the objective is to predict the risk of all-cause
hospitalization using longitudinal EHR data.

B. EXPERIMENTAL DESIGN
In this research, we use de-identiﬁed EHR data from the
University of Virginia Health System covering 75 months be-
ginning in September 2010. This dataset contains 2,343,651
inpatient and outpatient visits of 473,915 distinct patients.
We extracted visit data with diagnosis, medication, and pro-
cedure codes.

We deﬁned the observation window and prediction period
to validate the proposed method. We ﬁrst extract all patients
with a medical record of at least 1.5 years, where the ﬁrst year
is the observation window and the medical records in this
time window are used for feature construction. The follow-
ing 6 months is the hold-off period for the purpose of early
detection. For the positive class, we take all patients who
have hospitalization after the ﬁrst 1.5 years in their medical
history, while the negative class consists of patients who have
no hospitalization after 1.5 years. To better illustrate the ex-
perimental setting, we present the observation window, hold-
off and onset of outcome event in Figure 5. Here, the medical
codes include diagnosis, medication, and procedure codes,
and a vector representation is learned for each code. In this
dataset, diagnoses are primarily coded in ICD-9 and a small
portion is ICD-10 codes, while procedures are mainly using
CPT codes with a few ICD-9 procedure codes. The codes of
medications are using the pharmaceutical categories. Overall,
there are 94 distinct medication categories, 34,419 distinct
diagnoses codes, and 7,895 distinct procedure codes in the
EHR data. The dimension of the learned vectors of medical
codes is set to 100. Medical codes that appear in less than 50
patients medical records are excluded as rare events.

To construct the subsequences of medical codes, we use l
days as the time window. Figure 6 presents the cumulative
histogram and density plot of the numbers of visits in the
observation window, and we observe that the majority of

FIGURE 6. The cumulative histogram and density plot of patients’ numbers of
visits

patients have a small number of visits during the observation
window (less than 25% of patients have more than 4 visits).
Thus, we set l to 90 days, which split the observation window
into 4 subsequences.

Within each subsequence, the number of distinct medical
codes were computed and patients with more medical codes
in a subsequence than the 95% quantile were excluded from
the dataset. Overall, there are 8,841 and 89,101 patients in
the target and control groups, respectively. Each group is
randomly split into training, validation and testing sets with
a 7:1:2 ratio. Thus, 70% are used for training, another 20%
is used for testing, and the rest 10% are used for parameter
tuning and early stopping. The stochastic gradient descent
algorithm is used in training to minimize the cross-entropy
loss function, shown in Equation 21.

To evaluate the proposed representation learning frame-
work, we compare the prediction performance of the pro-
posed model with baseline approaches as follows.

1) Logistic regression (LR)

The inputs are the aggregated counts of grouped medical
codes over the entire observation window. Since the di-
mensionality of raw medical codes is huge, AHRQ clini-
cal classiﬁcations of diagnoses and procedures are used to
achieve a more general clustering of medical codes [42].
The medication codes are the pharmaceutical classes. Fur-
thermore, patient characteristics and previous inpatient visit
are also considered, where age and gender are demographic
information, and a binary indicator is utilized to represent
the presence of the previous hospitalization. Hence, the input
is a 436-dimensional vector representing a patient’s medical
history and characteristics.

8

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 1. The predictive performance of baselines and the proposed Patient2Vec framework

Methods
LR
MLP
RETAIN
FRNN-MGE
BiRNN-MGE
FRNN-MVE
BiRNN-MVE
Patient2Vec

Sensitivity
0.637 ± 0.010
0.727 ± 0.013
0.553 ± 0.012
0.636 ± 0.012
0.600 ± 0.012
0.753 ± 0.011
0.724 ± 0.010
0.769 ± 0.010

Speciﬁcity
0.728 ± 0.003
0.617 ± 0.004
0.710 ± 0.003
0.739 ± 0.004
0.777 ± 0.003
0.676 ± 0.004
0.707 ± 0.003
0.694 ± 0.004

AUC
0.721 ± 0.006
0.713 ± 0.007
0.663 ± 0.007
0.759 ± 0.006
0.768 ± 0.007
0.785 ± 0.006
0.788 ± 0.005
0.799 ± 0.005

F2 score
0.434 ± 0.006
0.423 ± 0.007
0.370 ± 0.008
0.438 ± 0.009
0.439 ± 0.009
0.470 ± 0.008
0.473 ± 0.008
0.492 ± 0.007

2) Multi-layer perceptron (MLP)
A multi-layer perceptron is trained to predict hospitalization
using the same inputs for logistic regression. Here, we use a
one hidden layer MLP with 256 hidden nodes.

3) Forward RNN with medical group embedding
(FRNN-MGE)
We split the sequence into subsequences with equal interval l.
The input at each step is the counts of medical groups within
the associated time interval, and the patient characteristics are
appended as additional features in the ﬁnal logistic regression
step. Here, the RNN is a forward GRU (or LSTM [18]) with
one hidden layer and the size of the hidden layer is 256.

4) Bidirectional RNN with medical group embedding
(BiRNN-MGE)
The inputs used for this baseline is the same as the one for
the FRNN-MGE [15]. The RNN used here is a bidirectional
GRU with one hidden layer and the size of the hidden layer
is 256.

5) Forward RNN with medical vector embedding
(FRNN-MVE)
We split the sequence into subsequences with equal interval l.
The input at each step is the vector representation of the
medical codes within the associated time interval, and the
patient characteristics are appended as additional features in
the ﬁnal logistic regression step. Here, the RNN is a forward
GRU (or LSTM [28]) with one hidden layer and the size of
the hidden layer is 256.

6) Bidirectional RNN with medical vector embedding
(BiRNN-MVE)
The inputs used for this baseline is the same as the one for
the FRNN-MVE [25]. The RNN used here is a bidirectional
GRU or LSTM [15] with one hidden layer and the size of the
hidden layer is 256.

7) RETAIN
This model uses reverse time attention mechanism on RNNs
for an interpretable representation of patient’s EHR data [31].
The inputs are the same as the one for FRNN-MGE, which
takes the counts of medical grouping within each time inter-
val to construct features. Similarly, the two RNNs used for

generating weights are GRU-based and the size of the hidden
layers are 256.

8) Patient2Vec
The inputs are the same as that for FRNN-MVE. One ﬁlter
is used when generating weights for within-subsequence
attention, and three ﬁlters are used for subsequence-level
attention. Similarly, the RNN used here is GRU-based and
there is one hidden layer and the size of the hidden layer
is 256.

The inputs of all baselines and Patient2Vec are normal-
ized to have zero mean and unit variance. We model the
risk of hospitalization based on Patient2Vec and baseline
representations of patients’ medical histories, and the model
performance is evaluated with Area Under Curve(AUC),
sensitivity, speciﬁcity, and F2-score. The validation set is
used for parameter tuning and early stopping in the train-
ing process. Each experiment is repeated 20 times and we
calculate the averages and standard deviations of the above
metrics, respectively.

C. EXPERIMENTAL RESULTS
The predictive performance of Patient2Vec and baselines are
presented in Table 1. The results shown here for the RNN-
based models are based on time interval l = 90 days to
construct subsequences.

According to Table 1, the RNN-based models are generally
capable of achieving higher prediction performance in terms
of sensitivity, AUC and F2 score, except for the RNN models
based on medical group embedding which have lower sensi-
tivity. Among all RNN-based approaches, the ones based on
vector embedding outperform those based on medical group
embedding in terms of sensitivity, AUC, and F2 score. The
bidirectional RNN models generally have higher speciﬁcity
but lower sensitivity than the forward RNN models, while
the bidirectional ones have comparable AUC and F2 score
with the forward ones, respectively. Generally, the proposed
Patient2Vec framework outperforms the baseline methods,
especially in terms of sensitivity and F2 score.

D. VISUALIZATION & INTERPRETATION
In addition to predictive performance, we interpret
the
learned representation by understanding the relative impor-
tance of clinical events in a patient’s EHR data. Considering

VOLUME 0, 2018

9

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 7. The heat map showing feature importance for Patient A

FIGURE 8. The proﬁle of Patient A.

FIGURE 9. The proﬁle of Patient B.

the feature importance learned by Patient2Vec are person-
alized for an individual patient, we illustrate it with two
example patients. Figures 8 and 9 present the proﬁles of
two individuals, Patient A and Patient B, respectively. To
facilitate the interpretation, instead of using raw medical
codes, we present the clinical groups from the AHRQ clinical
classiﬁcation software on diagnoses and procedure codes, as
well as pharmaceutical groups for medications.

According to Figure 8, Patient A is a male patient who
has hospitalization history in the observation window and
is admitted to the hospital seven months after the end of
the observation window for congestive heart failure. The

predicted risk is 96.4%, while the risk decreases for female
patients or patients without hospitalization history. It is also
not surprising to observe an increased risk for older patients.
The heat map in Figure 7 shows the relative importance of
the medical events in this patient’s medical record at each
time window and the ﬁrst row of the heat map presents
the subsequence-level attention. The darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the last subsequence,
t4, is the most important with respect to hospitalization risk,
followed by t1, t2, and t3 in order of importance.

Among all the clinical events in the subsequence t4, we

10

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 2. The top clinical groups with high weights in hospitalized patients

Index

Clinical Groups

Diagnoses
1
2
3

Essential hypertension
Other connective tissue disease
Spondylosis; intervertebral disc disorders; other back
problems
Other lower respiratory disease
Disorders of lipid metabolism
Other aftercare
Diabetes mellitus without complication
Screening and history of mental health and substance
abuse codes
Other nervous system disorders
Other screening for suspected conditions (not mental
disorders or infectious disease)

Other OR therapeutic procedures on nose; mouth and
pharynx
Suture of skin and subcutaneous tissue
Other therapeutic procedures on eyelids; conjunctiva;
cornea

Laboratory - Chemistry and hematology
Other laboratory
Other OR therapeutic procedures of urinary tract
Other OR procedures on vessels other than head and
neck

Therapeutic radiology for cancer treatment

Procedures
1

9
10

4
5
6
7
8

2
3

4
5
6
7

8

2

Medications
1

Diagnostic Products

Analgesics-Narcotic

observe that the OR therapeutic procedures (nose, mouth,
and pharynx), laboratory (chemistry and hematology), coro-
nary atherosclerosis & other heart disease, cardiac dys-
rhythmias, and conduction disorders are the ones with the
highest weights, while other events such as other connected
tissue disease are less important in terms of future hospi-
talization risk. Additionally, some medications appear to be
informative as well, including beta blockers, antihyperten-
sives, anticonvulsants, anticoagulants, etc. In the ﬁrst-time
window, the medical events with high weights are coro-
nary atherosclerosis & other heart disease, gastrointestinal
hemorrhage, deﬁciency and anemia, and other aftercare. In
the next subsequence, the most important medical events
are heart diseases and related procedures such as coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
conduction disorders, hypertension with complications, other
OR heart procedures, and other OR therapeutic nervous
system procedures. We also observe that the kidney disease
related diagnoses and procedures appear to be important
features. Throughout the observation window, the coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
and conduction disorders constantly show high weights with
respect to hospitalization risk, and the ﬁndings are consistent

FIGURE 10. The heat map showing feature importance for Patient B

with medical literature.

Figure 9 presents the proﬁle of Patient B, which is a male
patient without hospitalization in the observation window.
This patient is hospitalized for occlusion of cerebral arter-
ies approximately one year after the observation window,
and the predicted risk is 74.6%. For a similar patient who
is 10 years older or with previous hospitalization history,
the risk increases by 4.2% and 1%, respectively, while there
is a smaller risk of hospitalization for a female patient. To
illustrate the medical events of Patient B, the heat map in
Figure 10 depicts the relative importance of medical groups
in the subsequences, as well as the subsequence-level weights
for hospitalization risk. Similarly, the darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the second subse-
quence appears to be the most important, while the last one is
less predictive of future hospitalization. In fact, the medical
events in the last time window are spondylosis, intervertebral
disc disorders, other back problems and other bone disease &
musculoskeletal deformities, and malaise and fatigue, which
are not highly related to the cause of hospitalization of Patient
B.

In the most predictive subsequence, t2, we observe that
other OR heart procedures, genitourinary symptoms, spondy-
losis, intervertebral disc disorders, other back problems,
therapeutic procedures on eyelid, conjunctiva, and cornea,
and arterial blood gases have high attention weights. In the

VOLUME 0, 2018

11

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 3. The top diagnosis groups with high weights in patients hospitalized
for osteoarthritis, septicemia, acute myocardial infarction, congestive heart
failure, and diabetes mellitus with complications, respectively

Index

Diagnosis Groups

In patients admitted for osteoarthritis

Osteoarthritis
Other connective tissue disease

1
2

3
4

5

1
2

3
4
5

1
2
3

4
5

1
2
3
4
5

1
2
3

4
5

Other non-traumatic joint disorders
Spondylosis; intervertebral disc disorders; other back prob-
lems
Other aftercare

In patients admitted for septicemia

Essential hypertension
Diabetes mellitus without complication

Disorders of lipid metabolism
Other lower respiratory disease
Other aftercare

In patients admitted for acute myocardial infarction

Coronary atherosclerosis and other heart disease
Medical examination/evaluation
Other screening for suspected conditions (not mental disorders
or infectious disease)

Other lower respiratory disease
Disorders of lipid metabolism

In patients admitted for congestive heart failure

Congestive heart failure (nonhypertensive)
Coronary atherosclerosis and other heart disease
Cardiac dysrhythmias
Diabetes mellitus without complication
Other lower respiratory disease

In patients admitted for diabetes mellitus with complications

Diabetes mellitus with complications
Diabetes mellitus without complication
Other aftercare

Other nutritional; endocrine; and metabolic disorders
Fluid and electrolyte disorders

earliest time window, the most important medical events also
include therapeutic procedures on eyelid, conjunctiva, and
cornea, arterial blood gases, while diabetes, hypertension
as well as diagnostic products show their relatively high
importance. Throughout the observation window, medical
events spondylosis, intervertebral disc disorders, other back
problems, therapeutic procedures on eyelid, conjunctiva, and
cornea are constantly with high attention weights. Here, di-
agnostic products is a medication class, which include barium
sulfate, iohexol, gadopentetate dimeglumine, iodixanol, tu-
berculin puriﬁed protein derivative, iodixanol, regadenoson,
acetone (urine), and so forth. These medications are primarily
for blood or urine testing, or used as radiopaque contrast
agents for x-rays or CT scans for diagnostic purposes.

Additionally, we attempt to interpret the learned repre-
sentation and feature importance at the population-level. In
Table 2, we present the top 20 clinical groups with high
weights among hospitalized patients in the test set.

According to Table 2, the most predictive diagnosis groups
for future hospitalization are chronic diseases, including
essential hypertension, diabetes, lower respiratory disease,
disorders of lipid metabolism, and musculoskeletal diseases
such as other connective tissue disease and spondylosis,
intervertebral disc disorders, other back problems. The most
important procedures are some OR therapeutic procedures
and laboratory tests, such as the OR procedures on nose,
mouth, and pharynx, vessels, urinary tract, eyelid, conjunc-
tiva, cornea, etc. It is not surprising to see that diagnostic
products are showing with high weights, considering these
medications are used in testing or examinations for diagnos-
tic purposes.

Moreover, we present the top diagnoses groups with high
weights in patients hospitalized for different primary causes.
Table 3 shows the top 5 diagnosis groups with high weights
in patients admitted for osteoarthritis, septicemia (except in
labor), acute myocardial infarction, congestive heart failure
(nonhypertensive), and diabetes mellitus with complications,
respectively. Accordingly, we observe that the most impor-
tant diagnoses for hospitalization risk prediction in popu-
lation admitted for osteoarthritis are musculoskeletal dis-
eases such as connective tissue disease, joint disorders, and
spondylosis. However, the diagnoses with highest weights
in the patients admitted for septicemia are chronic diseases
including essential hypertension, diabetes, disorders of lipid
metabolism, and respiratory disease. The top diagnoses have
many overlaps between the populations admitted for acute
myocardial infarction and for congestive heart failure, con-
sidering both populations are admitted for heart diseases.
the overlapped diagnosis groups include coronary
Here,
atherosclerosis and other heart diseases and lower respiratory
diseases. As for patients admitted for diabetes with com-
plications, the top diagnoses are diabetes with or without
complications, nutritional, endocrine, metabolic disorders,
and ﬂuid and electrolyte disorders. In general, the learned
feature importance is consistent with medical literature.

VI. DISCUSSION
Our proposed framework is applied to the prediction of
hospitalization using real EHR data that demonstrates its
prediction accuracy and interpretability. This work could be
further enhanced by incorporating the follow-up information
on the negative patient population and investigate if it in-
deed shows an improved health outcome or the patient is
hospitalized elsewhere. Patient2Vec employs a hierarchical
attention mechanism, allowing us to directly interpret the
weights of clinical events. In future work, we will extend the
attention to incorporate demographic information for a more
comprehensive and automatic interpretation.

Although we apply Patient2Vec to the early detection of
long-term hospitalization, i.e., at least 6 months after the
previous hospitalization, it could be used to predict the risk
of 30-day readmission to help prevent unnecessary rehospi-
talizations.

12

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

VII. CONCLUSION
In this paper, we propose a representation learning frame-
work, Patient2Vec, to learn a personalized interpretable deep
representation of EHR data based on recurrent neural net-
works and the attention mechanism. This work improves
the performance of predictive models as well as deepens
the understanding of disease correlations. We apply this
framework to the risk prediction of hospitalization using
patients’ longitudinal EHR data. The experimental results
demonstrate that the proposed Patient2Vec representation is
capable of achieving a more accurate prediction than base-
lines approaches. Moreover, the learned feature importance
in the representations are interpreted both at the individual
and population levels to facilitate clinical insights.

In this work, the proposed Patient2Vec framework is eval-
uated with the risk prediction of all-cause hospitalization,
but in the future could be applied to predict hospitalization
in more speciﬁc populations, other health related prediction
problems, or domains outside of health.

REFERENCES
[1] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical
attention networks for document classiﬁcation,” in Proceedings of the
2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, 2016, pp.
1480–1489.

[2] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,” arXiv

preprint arXiv:1408.5882, 2014.

[3] J. Howard and S. Ruder, “Fine-tuned language models for text classiﬁca-

tion,” arXiv preprint arXiv:1801.06146, 2018.

[4] M. M. Lopez and J. Kalita, “Deep learning applied to nlp,” arXiv preprint

arXiv:1703.03091, 2017.

[5] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using
rnn encoder-decoder for statistical machine translation,” arXiv preprint
arXiv:1406.1078, 2014.

[6] A. L. Nobles, J. J. Glenn, K. Kowsari, B. A. Teachman, and L. E. Barnes,
“Identiﬁcation of imminent suicide risk among young adults using text
messages,” in Proceedings of the 2018 CHI Conference on Human Factors
in Computing Systems. ACM, 2018, p. 413.

[7] K. Kowsari, D. E. Brown, M. Heidarysafa, K. J. Meimandi, M. S. Gerber,
and L. E. Barnes, “Hdltex: Hierarchical deep learning for text classiﬁca-
tion,” in 2017 16th IEEE International Conference on Machine Learning
and Applications (ICMLA), Dec 2017, pp. 364–371.

[8] K. Kowsari, M. Heidarysafa, D. E. Brown, K. Jafari Meimandi, and L. E.
Barnes, “Rmdl: Random multimodel deep learning for classiﬁcation.”
ACM, 2018.

[9] H. Strobelt, S. Gehrmann, H. Pﬁster, and A. M. Rush, “Lstmvis: A tool
for visual analysis of hidden state dynamics in recurrent neural networks,”
IEEE transactions on visualization and computer graphics, vol. 24, no. 1,
pp. 667–676, 2018.

[10] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” Scientiﬁc
reports, vol. 8, no. 1, p. 6085, 2018.

[11] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches
preprint

neural machine

translation,”

arXiv

attention-based
to
arXiv:1508.04025, 2015.

[12] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[13] D. Yogatama, C. Dyer, W. Ling, and P. Blunsom, “Generative and discrim-
inative text classiﬁcation with recurrent neural networks,” arXiv preprint
arXiv:1703.01898, 2017.

[14] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent

trends
language processing,” arXiv preprint

in deep learning based natural
arXiv:1708.02709, 2017.

[15] M. Basaldella, E. Antolli, G. Serra, and C. Tasso, “Bidirectional lstm
recurrent neural network for keyphrase extraction,” in Italian Research

Conference on Digital Libraries, Springer.
lishing, 2018, pp. 180–187.

Springer International Pub-

[16] S. Ghosh, O. Vinyals, B. Strope, S. Roy, T. Dean, and L. Heck, “Con-
textual lstm (clstm) models for large scale nlp tasks,” arXiv preprint
arXiv:1602.06291, 2016.

[17] B. Yue, J. Fu, and J. Liang, “Residual recurrent neural networks for
learning sequential representations,” Information, vol. 9, no. 3, p. 56, 2018.
[18] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
gated recurrent neural networks on sequence modeling,” arXiv preprint
arXiv:1412.3555, 2014.

[19] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difﬁculty of training
recurrent neural networks.” ICML (3), vol. 28, pp. 1310–1318, 2013.
[20] D. Britz, “Recurrent neural network tutorial,” http://www.wildml.com/

2015/10/, 2015, [Accessed on October 5, 2017].

[21] V. Mnih, N. Heess, A. Graves et al., “Recurrent models of visual attention,”
in Advances in neural information processing systems, 2014, pp. 2204–
2212.

[22] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for
abstractive sentence summarization,” arXiv preprint arXiv:1509.00685,
2015.

[23] T. Ma, C. Xiao, and F. Wang, “Health-atm: A deep architecture for
multifaceted patient health record representation and risk prediction,” in
Proceedings of the 2018 SIAM International Conference on Data Mining.
SIAM, 2018, pp. 261–269.

[24] A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J. Liu,
X. Liu, J. Marcus, M. Sun et al., “Scalable and accurate deep learning with
electronic health records,” npj Digital Medicine, vol. 1, no. 1, p. 18, 2018.
[25] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzell, “Learning to diagnose
with lstm recurrent neural networks,” arXiv preprint arXiv:1511.03677,
2015.

[26] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” arXiv preprint
arXiv:1606.01865, 2016.

[27] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghas-
semi, B. Moody, P. Szolovits, L. A. Celi, and R. G. Mark, “Mimic-iii, a
freely accessible critical care database,” Scientiﬁc data, vol. 3, 2016.
[28] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart, and J. Sun, “Doctor
ai: Predicting clinical events via recurrent neural networks,” in Machine
Learning for Healthcare Conference, 2016, pp. 301–318.

[29] Z. Che, S. Purushotham, R. Khemani, and Y. Liu, “Interpretable deep
models for icu outcome prediction,” in AMIA Annual Symposium Pro-
ceedings, vol. 2016. American Medical Informatics Association, 2016,
p. 371.

[30] J. H. Friedman, “Greedy function approximation: a gradient boosting

machine,” Annals of statistics, pp. 1189–1232, 2001.

[31] E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart,
“Retain: An interpretable predictive model for healthcare using reverse
time attention mechanism,” in Advances in Neural Information Processing
Systems, 2016, pp. 3504–3512.

[32] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Ben-
gio, “A structured self-attentive sentence embedding,” arXiv preprint
arXiv:1703.03130, 2017.

[33] C. M. Torio and B. J. Moore, “National inpatient hospital costs: The most
expensive conditions by payer, 2013,” https://www.hcup-us.ahrq.gov/
reports/statbriefs/sb204-Most-Expensive-Hospital-Conditions.jsp, 2016.
[34] E. Wallace, E. Stuart, N. Vaughan, K. Bennett, T. Fahey, and S. M.
Smith, “Risk prediction models to predict emergency hospital admission
in community-dwelling adults: a systematic review,” Medical care, vol. 52,
no. 8, p. 751, 2014.

[35] E. N. de Vries, M. A. Ramrattan, S. M. Smorenburg, D. J. Gouma, and
M. A. Boermeester, “The incidence and nature of in-hospital adverse
events: a systematic review,” Quality and safety in health care, vol. 17,
no. 3, pp. 216–223, 2008.

[36] S. Purdey and A. Huntley, “Predicting and preventing avoidable hospital
admissions: a review.” The journal of the Royal College of Physicians of
Edinburgh, vol. 43, no. 4, pp. 340–344, 2012.

[37] H. Ontario, “Early identiﬁcation of people at risk of hospitalization: Hos-
pital admission risk prediction (harp)-a new tool for supporting providers
and patients,” 2013.

[38] B. Zheng, J. Zhang, S. W. Yoon, S. S. Lam, M. Khasawneh, and S. Poranki,
“Predictive modeling of hospital readmissions using metaheuristics and
data mining,” Expert Systems with Applications, vol. 42, no. 20, pp. 7110–
7120, 2015.

VOLUME 0, 2018

13

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

[39] D. Kansagara, H. Englander, A. Salanitro, D. Kagen, C. Theobald,
M. Freeman, and S. Kripalani, “Risk prediction models for hospital
readmission: a systematic review,” The Journal of the American Medical
Association, vol. 306, no. 15, pp. 1688–1698, 2011.

[40] G. Giamouzis, A. Kalogeropoulos, V. Georgiopoulou, S. Laskar, A. L.
Smith, S. Dunbar, F. Triposkiadis, and J. Butler, “Hospitalization epidemic
in patients with heart failure: risk factors, risk prediction, knowledge gaps,
and future directions,” Journal of cardiac failure, vol. 17, no. 1, pp. 54–75,
2011.

[41] E. Prescott, A. M. Bjerg, P. K. Andersen, P. Lange, and J. Vestbo, “Gender
difference in smoking effects on lung function and risk of hospitalization
for COPD: results from a danish longitudinal population study,” European
Respiratory Journal, vol. 10, no. 4, pp. 822–827, 1997.

[42] Agency for Healthcare Research and Quality (AHRQ), “Clinical classi-
ﬁcations software (CCS) for ICD-9-CM,” https://www.hcup-us.ahrq.gov/
toolssoftware/ccs/ccs.jsp, 2015.

JENNIFER M. LOBO (jem4yb@virginia.edu) is
an Assistant Professor of Biomedical Informatics
in the Department of Public Health Sciences at
University of Virginia. She received her Ph.D. in
Industrial Engineering from North Carolina State
University, Raleigh, NC. Her research interests in-
volve using mathematical modeling and stochastic
optimization methods to build models that simu-
late the natural course of disease. These models
allow for estimation of outcomes under different
screening and treatment policies in the absence of randomized controlled
trials, and can be used to optimize screening and treatment decisions for
patients with chronic diseases. Her projects include optimizing treatment
for patients with type 2 diabetes, generating individualized decision analysis
models for prostate cancer patients, and developing optimal imaging surveil-
lance guidelines for recurrent kidney cancer.

JINGHE ZHANG (jz4kg@virginia.edu) is a lead
data scientist at Target Corporation. She received
her Ph.D. in Systems Engineering from the Uni-
versity of Virginia. Prior to entering the Ph.D. pro-
gram at UVA, she received the Master of Science
in Industrial and Systems Engineering from the
State University of New York at Binghamton. Her
research interests are in natural language process-
ing, machine learning, recommender systems, and
health informatics.

LAURA E. BARNES (lb3dp@virginia.edu) is an
Associate Professor in Systems and Information
Engineering and the Data Science Institute at the
University of Virginia. She received her Ph.D. in
Computer Science from the University of South
Florida, Tampa, FL. She directs the Sensing Sys-
tems for Health (S2He) Lab which focuses on
understanding the dynamics and personalization
of health and well-being through mobile sensing
and analytics.

(kk7nc@virginia.edu) is
KAMRAN KOWSARI
a Ph.D. student in the Department of Systems
and Information Engineering at the University of
Virginia, Charlottesville, VA. He is a member of
the Sensing Systems for Health Lab. He received
his Master of Science from Department of Com-
puter Science at The George Washington Univer-
sity, Washington, DC. He has more than ten years
of experience in machine learning and software
development. His experience includes numerous
industrial and academic projects. His research interests include natural
language processing, machine learning, deep learning, artiﬁcial intelligence,
text mining, and unsupervised learning.

JAMES H. HARRISON, JR., (jhh5y@virginia.
edu) is Associate Professor of Pathology and Di-
rector of Laboratory Information Systems at the
University of Virginia Medical Center and also has
appointments in the Departments of Public Health
Sciences in the UVA School of Medicine, and
Systems and Information Engineering in the UVA
School of Engineering and Applied Sciences. He
received his MD and PhD (Pharmacology) de-
grees from Medical University of South Carolina,
Charleston, SC, completed residencies in Anatomic Pathology and Labora-
tory Medicine at Yale-New Haven Hospital, New Haven, CT, and completed
a postdoctoral fellowship in Environmental Toxicology at Yale University,
New Haven, CT. Dr. Harrison has over 25 years of experience in the ﬁeld
of medical informatics, including work in clinical laboratory information
systems, electronic health records, clinical data analysis, and clinical data
standards development.

14

VOLUME 0, 2018

Date of publication 10, 2018 , date of current version 10, 2018.

Digital Object Identiﬁer 10.1109/ACCESS.2018.2875677

Patient2Vec: A Personalized
Interpretable Deep Representation of the
Longitudinal Electronic Health Record

JINGHE ZHANG1, (Member, IEEE), KAMRAN KOWSARI1,4(Member, IEEE), JAMES H.
HARRISON2,3,5, JENNIFER M. LOBO2,5, and LAURA E. BARNES1,4,5, (Member, IEEE)
1Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, USA
2 Department of Public Health Sciences, University of Virginia, Charlottesville, VA 22904, USA
4 Sensing Systems for Health Lab, University of Virginia, Charlottesville, VA 22904, USA
3 Division of Laboratory Medicine Department of Pathology, University of Virginia, Charlottesville, VA 22904, USA
5Data Science Institute, University of Virginia, Charlottesville, VA 22904, USA
Corresponding author: Laura E. Barnes (lb3dp@virginia.edu)
This research was supported by a Jeffress Trust Award in Interdisciplinary Science.
Patient2Vec is shared as an open source tool at https://github.com/BarnesLab/Patient2Vec

ABSTRACT The wide implementation of electronic health record (EHR) systems facilitates the collection
of large-scale health data from real clinical settings. Despite the signiﬁcant increase in adoption of EHR
systems, this data remains largely unexplored, but presents a rich data source for knowledge discovery from
patient health histories in tasks such as understanding disease correlations and predicting health outcomes.
However, the heterogeneity, sparsity, noise, and bias in this data present many complex challenges.
This complexity makes it difﬁcult to translate potentially relevant information into machine learning
algorithms. In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep
representation of longitudinal EHR data which is personalized for each patient. To evaluate this approach,
we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive
performance with baseline methods. Patient2Vec produces a vector space with meaningful structure and it
achieves an AUC around 0.799 outperforming baseline methods. In the end, the learned feature importance
can be visualized and interpreted at both the individual and population levels to bring clinical insights.

INDEX TERMS Attention mechanism, gated recurrent unit, hospitalization, longitudinal electronic health
record, personalization, representation learning.

8
1
0
2
 
t
c
O
 
5
2
 
 
]

M
Q
.
o
i
b
-
q
[
 
 
3
v
3
9
7
4
0
.
0
1
8
1
:
v
i
X
r
a

I. INTRODUCTION

L ongitudinal EHR data resemble text documents from

many perspectives. A text document consists of a se-
quence of sentences, and a sentence is a sequence of words.
Similarly, the longitudinal health record of a patient consists
of a sequence of visits, and there is a list of clinical events,
including diagnoses, medications, and procedures, that occur
during a visit. Considering these similarities, representation
learning methods for text documents in Natural Language
Processing (NLP) have great potential to be applied to lon-
gitudinal EHR data.

Deep neural networks have become very popular in the
NLP ﬁeld and have been very successful in many applica-
tions, such as machine translation, question answering, text
classiﬁcation, document summarization, language modeling,
etc. [1]–[8]. These networks excel at complex language tasks

because they are capable of identifying high-order relation-
ships, the network structure can encode language structures,
and they allow the learning of a hierarchical representation
of the language, i.e., representations for tokens, phrases, and
sentences, etc.

Among a variety of deep learning methods, Recurrent
Neural Networks (RNNs) have shown their effectiveness in
NLP tasks because they have the ability to capture sequential
information [7]–[10] which is inherent in human language.
Traditional neural networks assume that inputs are inde-
pendent of each other, while an RNN computes the output
based on the current input as well as the “memory” from the
previous computation. Although vanilla RNNs are not good
at capturing long-term dependencies, many variants have
been proposed and validated that are effective in addressing

VOLUME 0, 2018

1

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

its performance with other baseline methods. In addition
to prediction performance, we further interpret the learned
representations with visualizations on example patients and
events. Finally, Section V provides a summary of this work.

II. RELATED WORK
In this section, we present an overview of a gated recurrent
unit, a type of RNN, which is capable of capturing long-term
dependencies. Then we brieﬂy introduce attention mecha-
nisms in neural networks that allow the network to attend
to certain regions of data, which is inspired by the visual
attention mechanism in humans. Additionally, we summarize
the RNN networks and attention mechanisms previously used
to mine EHR data.

A. RECURRENT NEURAL NETWORKS (RNN)

RNNs are expected to learn long-term dependencies by tak-
ing the previous state and the new input in the computation at
the current time step t. However, vanilla RNNs are incapable
of capturing the dependencies when the sequence is very long
due to the vanishing gradient problem [12]. Many variants of
the RNN network have been proposed to address this issue,
and long short term memory (LSTM) is one of the most
popular models used nowadays in NLP tasks [7], [8], [13]–
[16].

this issue.

In the medical domain, it is critical that analytical results
are interpretable, so that they can be understood and validated
by a human with expert knowledge and so that knowledge
captured by analysis can be used for process improvement.
Traditional deep neural networks have the disadvantage that
they lack interpretability. A substantial amount of work is
ongoing to make sense of the “black box”, and the attention
mechanism [11] is one of the more effective methods recently
developed to make the output of these algorithms more
interpretable.

Health care is undergoing unprecedented change, and there
is a great potential and demand for personalized care strate-
gies. Personalized medicine, also called precision medicine,
has previously focused on optimizing therapy to better ﬁt the
genetic makeup of the patient or the disease (e.g., the genetic
susceptibility of cancer to speciﬁc chemotherapy strategies).
The availability of EHR data and advances in machine learn-
ing create the potential for another type of personalization
of healthcare. This type of personalization has become ubiq-
uitous in our daily life. For example, customers have come
to expect personalized search on Google and personalized
product recommendations on Amazon and Netﬂix, based on
their charactersitics and previous experiences with the sys-
tems. Personalization of healthcare processes, based on a pa-
tient’s phenotype (physical and medical characteristics) and
healthcare experiences as documented in the health record,
may also improve "customer" satisfaction and it has the addi-
tional potential to improve healthcare efﬁciency, lower costs,
and yield better outcomes. We believe that representation
learning methods can capture a personalized representation
of the important heterogeneities in patients’ phenotypes and
medical histories at the population-level, and make these
representations available to drive healthcare decisions and
strategies.

This research is based on RNN models and the attention
mechanism with the objective of learning a personalized, in-
terpretable, and complete representation of patients’ medical
records. Our proposed framework is capable of learning a
personalized representation for each patient from a sequence
of clinical events. A hierarchical attention mechanism learns
personalized weights of clinical events, including hospital
visits and the procedures that they contain. These weights
allow us to interpret the relative importance and roles of clin-
ical events in the learned representations both at individual
and population levels. The ultimate goal is more accurate
prediction and better insight into the critical elements of
healthcare processes that can be used to improve healthcare
delivery.

The rest of this paper is organized as follows: Section II
summarizes the variants of RNNs and the attention mecha-
nism, as well as their application to EHR data. Section III
presents an overview of the proposed Patient2Vec repre-
sentation learning framework, and Section IV elaborates
the details of the algorithms. In Section V, the proposed
framework is evaluated for a prediction task and we compare

FIGURE 1. The top ﬁgure is a GRU gating unit and bottom ﬁgure shows an
LSTM unit [7]

2

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

1) Gated Recurrent Unit (GRU)
GRU is a simpliﬁed version of LSTM [7]. The basic idea
of GRU is to combat the vanishing gradient problem with a
gating mechanism. Hence the general recurrent structure in
GRU is identical to vanilla RNNs except that a GRU unit
is used in the computation at each time step rather than a
traditional simple recurrent unit.

In general, a GRU cell has two gates, i.e., a reset gate r
and an update gate z. The reset gate is used to determine
how to integrate the previous state into the computation of
the current state, while the update gate determines how much
the unit updates its activation.

Given the input xt at time step t, the reset gate rt is

computed as presented in Equation 1

rt = σ(Urxt + Wrst−1)

(1)

where Ur and Wr are the weight matrices of the reset gate
and st−1 is the hidden activation at time step t − 1. A similar
computation is performed for the update gate zt at time step t,
shown in Equation 2

zt = σ(Uzxt + Wzst−1)

(2)

where Uz and Wz are the weight matrices of update gate.
The current hidden activation ht is computed by

ht = (1 − zt)ht−1 + zt
(3)
where ˜ht is the candidate activation at time step t. The
computation of ˜ht is presented in Equation 4

˜ht

˜ht = tanh(Wxt + U(rt (cid:12) ht−1))

(4)

where U and W are weight matrices and (cid:12) represents
element-wise multiplication. Figure 1 presents a graphical
illustration of the GRU [7] and one unit of LSTM.

GRU is capable of learning long-term dependencies [17]
due to the additive component of update from t to t + 1
in the gating mechanism. Consequently, important features
will be carried forward in the input stream while irrelevant
information will be dropped. When the reset gate is 0, the
network is forced to drop previous states and reset with cur-
rent information. Moreover, the method provides shortcuts
such that the error is easily backpropagated without vanishing
too quickly [5], [18]. Hence, the GRU is well-suited to learn
long-term dependencies in sequence data.

2) Long Short-Term Memory (LSTM)
An LSTM unit is similar to a GRU, but with one more gate in
an LSTM unit (as shown in Figure 1). LSTM also preserves
long term dependencies more effectively than basic RNN.
This is particularly useful to overcome the vanishing gradient
problem [19]. Although LSTM has a chain-like structure sim-
ilar to RNN, LSTM uses multiple gates to carefully regulate
the amount of information that will be allowed into each node
state. Figure 1 shows the basic cell of an LSTM model. A step

VOLUME 0, 2018

by step explanation of an LSTM cell is as following:
Input gate:

it = σ(Wi[xt, ht−1] + bi),

Candid memory cell value:

˜Ct = tanh(Wc[xt, ht−1] + bc),

Forget gate activation:

ft = σ(Wf [xt, ht−1] + bf ),

New memory cell value:

Ct = it ∗ ˜Ct + ftCt−1,

Output gate value:

ot = σ(Wo[xt, ht−1] + bo),

ht = ot tanh(Ct),

(5)

(6)

(7)

(8)

(9)

(10)

In the above description all b represent bias vectors, all W
represent weight matrices, and xt is used as input to the
memory cell at time t. Also,the i, c, f, o indices refer to input,
cell memory, forget and output gates respectively. An RNN
can be biased when later words are more inﬂuential than the
earlier ones.

Empirically, LSTM and GRU achieve comparable per-
formance in many tasks but there are fewer parameters in
a GRU, which makes it a little faster to learn and able to
generalize with fewer data [20].

B. ATTENTION MECHANISM
Attention mechanisms, inspired by the visual attention sys-
tem found in humans, have become popular in deep learning.
Attention allows the network to focus on certain regions of
data, while perceiving other regions with “low resolution”.
In addition to higher accuracy, it also facilitates the interpre-
tation of learned representations. We elaborate an attention
mechanism on an RNN network, and Figure 2 presents a
graphical illustration.

According to Figure 2, a variable-length weight vector α
is learned based on hidden states [11]. Then a global context
vector is computed based on weights α and all the hidden
states to create the ﬁnal output. Equation 11 presents the
computation of the weight vector α = {α1, α2, · · · , αT },
where T is the length of the sequence

α1, α2, · · · , αT = f (Wαh + bα)

(11)

and where f is a nonlinear activation function, usually
sof tmax or tanh. Then, the context vector c is constructed
as:

c =

αtht

T
(cid:88)

t=1

Thus, the network puts more attention on the important
features for the ﬁnal prediction which can improve the model
performance. An additional beneﬁt is that the weights can

(12)

3

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

GRU-based model to address missing values in multivariate
time series data, in which the missing patterns are incorpo-
rated for improved prediction performance. This work has
been applied to the Medical Information Mart for Intensive
Care III (MIMIC-III) clinical database to demonstrate its
effectiveness in mining time series of clinical measurements
with missing values [27]. Longitudinal EHR data including
clinical events, such as diagnoses, medications, and pro-
cedures is also a potentially rich resource for predictive
modeling. Choi et al. [28] analyze this data with a GRU
network to forecast future clinical events, and it achieves a
better prediction performance than comparison models such
as logistic regression and MLP.

Difﬁculty in interpreting model behavior is one of the
major drawbacks of using deep learning to mine EHR data.
Some attempts have been made to address this issue. Che et
al. [29] propose an interpretable mimic learning method
which trains a mimic gradient boosting trees model to utilize
predicted labels or features learned by deep learning mod-
els for ﬁnal prediction [30]. Then the feature importances
learned by the tree-based models are used for knowledge
discovery. Attention mechanisms have been introduced re-
cently to improve the interpretability of the prediction results
of deep learning models in health analytics. Choi et al. [31]
develop an interpretable model with two levels of attention
weights learned from two reverse-time GRU models, re-
spectively. The experimental results on EHR data indicate
comparable prediction performance with conventional GRU
models but more interpretable results. Our work continues
the attempt to use attention mechanisms to improve the
interpretability of RNN-based models.

III. PATIENT2VEC SYSTEM MODEL
In this section, we provide an overview of the proposed
hierarchical representation learning framework. This frame-
work uses deep recurrent neural networks to capture the
complex relationships between clinical events in the patient’s
EHR data and employs the attention mechanism to learn
a personalized representation and to obtain relative feature
importance. The proposed representation learning framework
contains four steps and is presented graphically in Figure 3.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES

EHR data consists primarily of records of outpatient and
inpatient visits to healthcare providers. These visit records
include multiple clinical codes for diagnoses, symptoms,
procedures, therapies, and other observations and events that
occurred during the visit. Here, we treat the set of medical
codes associated with a visit as a sentence consisting of
words, except that there is no ordering in the words. Thus,
we adopt the word2vec approach to construct a vector to
represent each medical code.

FIGURE 2. The global attention model

be utilized to understand the importance of features such
that the models are more interpretable. The attention mech-
anism has been introduced to both Convolutional Neural
Networks (CNNs) and RNNs for various tasks and has
achieved many successes in the ﬁelds of computer vision and
NLP [11], [21], [22].

C. DEEP LEARNING IN EHR DATA
Previous studies on EHR data mainly use statistical meth-
ods or traditional machine learning techniques. Recently
researchers have started adapting deep learning approaches
to this data [23], [24], including textual notes, temporal
measurements of laboratory testing in the Intensive Care
Unit (ICU), and longitudinal data in patient populations.
Here, we summarize deep learning research in mining EHR
data and focus on the studies using RNN-based models.

Hospitalized patients, especially patients in ICUs, are
continuously monitored for cardiac, respiratory, and other
physical functions, creating a large volume of sequential data
in multiple dimensions. These measurements are utilized by
physicians to make diagnostic and treatment decisions. The
functions monitored may change over time and monitor-
ing may be irregular, based on a patient’s condition. It is
very challenging for traditional machine learning methods
to mine this multivariate time series data considering miss-
ing values, varying length, and irregular, non-simultaneous
sampling. Lipton et al. [25] trained an LSTM with a repli-
cated target to learn from these sequence data and used
this model to make predictions of diagnoses. The data used
in this research are time series of clinical measurements
with continuous values, and the LSTM models outperformed
logistic regression and MLP. Che et al. [26] developed a

4

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

are unlikely to contribute equally to the prediction of the tar-
get outcome, we cannot aggregate them with equal weights.
Instead, we employ a self-attention mechanism which trains
the network to learn the weights.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of subsequences with embedded medical
codes, we are able to input it into a recurrent neural network
to capture the temporal dependencies between events. How-
ever, the subsequences of visits are not contributing equally
to the outcome. Hence, we employ another level of attention
to learn the weights of the subsequences by the network itself
for the outcome prediction.

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the learned weights and hidden outputs, we aggregate
them into one universal vector for a comprehensive represen-
tation. In this step, the static information, such as age, gender,
previous hospitalization history is added as extra features, to
get a complete representation of a patient.

E. PREDICTING OUTCOME
Given the complete vector representation of a patient’s EHR
data, we add a logistic regression layer at the end for the
prediction of outcome.

IV. PATIENT2VEC REPRESENTATION LEARNING
ALGORITHM
In this section, we present the details of the proposed rep-
resentation learning framework, which is based on a GRU
network and a hierarchical attention mechanism. Figure 4
presents the structure of the proposed network with attention.

The proposed framework consists of ﬁve parts presented in
the following: I) Learning vector representations of medical
codes, II) Learning within-subsequence self-attention, III)
Learning subsequence-level self-attention, IV) Constructing
aggregated deep representation, V) Predicting outcome.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES
Given a patient’s raw EHR data, a sequence of visits, we
observe that a visit usually contains multiple medical codes.
Hence, it is feasible to learn a vector to represent the medical
code by capturing the relationships between the codes. In
this work, we employ the classical word2vec algorithm, skip-
gram. The basic idea of skip-gram is to learn a vector to
represent each word such that the probability of the context
to predict based on the target word is maximized. Hence,
the vectors of similar words are close to each other in the
learned feature space. In the skip-gram model, the vectors
are learned by training a shallow neural network to predict the
context words given an input word. Similarly, in our problem,

FIGURE 3. The Patient2Vec representation learning framework

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Clinical visits are represented as the set of vectors for the
codes associated with the visit. Because closely-spaced visits
are usually related clinically, we employ a time window to
split the sequence of visits into multiple subsequences of
equal length. A subsequence might contain multiple visits if
they occurred within the same time window, or there might
be no visits during a particular time window yielding an
empty subsequence. Thus we transform the original sequence
of irregularly-spaced visits into a sequence of subsequences
with equal intervals, which is preferable for recurrent neural
networks. The width of the subsequence window deﬁnes the
time granularity of the method and its optimal width is related
to the acuity (i.e., stability) of the clinical characteristics
involved in the predication task. In future work it may be
possible to deﬁne the relationship between clinical acuity and
optimal subsequence width, or develop methods for learning
an optimal width for a deﬁned prediction task.

Because all medical events occurring within a subsequence

VOLUME 0, 2018

5

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 4. A graphical illustration of the network in the Patient2Vec representation learning framework

the input is a medical code and the target to predict are the
medical codes occurred in the same visit.

Hence, each subsequence is a matrix consisting of the
vectors of medical codes occurred during this associated time
window.

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Given a sequence of subsequences encoded by vectors of
medical codes, this step employs the within-subsequence
attention which allows the network itself to learn the weights
of vectors in the subsequence according to its contribution to
the prediction target.

Here, we denote the sequence of patient i as s(i), and v(i)
t

t

, · · · , v(i)

1 , · · · , v(i)

denotes the tth subsequence in sequence s(i), where t ∈
{1, 2, · · · , T }. Thus, s(i) = {v(i)
T }. To
simplify the notation, we omit i in the following explanation.
Subsequence vt ∈ Rn×d is a matrix of medical codes such
that vt = {vt1 , vt2 , · · · , vtj , · · · , vtn }, where vtj ∈ Rd
is the vector representation of the jth medical code in the
tth subsequence vt and there are n medical codes in a
subsequence. In real EHR data, it is very likely that the
numbers of medical codes in each visit or time window are
different, thus, we utilize the padding approach to obtain a
consistent matrix dimensionality in the network.

To assign attention weights, we utilize the one-side con-
volution operation with a ﬁlter ωα ∈ Rd and a nonlinear
activation function. Thus, the weight vector αt is generated

6

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

for medical codes in the subsequence vt, presented in Equa-
tion 13.

αt = tanh(Conv(ωα, vt))

(13)

where αt = {αt1, αt2, · · · , αtn }, and ωα ∈ Rd is the
weight vector of the ﬁlter. The convolution operation Conv
is presented in Equation 14.

˜αtj = (ωα)(cid:124)vtj + bα

(14)

where bα is a bias term. Then, given the original matrix vt
and the learned weights αt, an aggregated vector xt ∈ Rd is
constructed to represent the tth subsequence, presented in 15.

xt =

αtj vtj

n
(cid:88)

j=1

(15)

Given Equation 15, we obtain a sequence of vectors, x =
{x1, x2, · · · , xt, · · · , xT }, to represent a patient’s medical
history.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of embedded subsequences, this step em-
ploys the subsequence-level attention which allows the net-
work itself to learn the weights of subsequences according to
their contribution to the prediction target.

To capture the longitudinal dependencies, we utilize a

bidirectional GRU-based RNN, presented in Equations 16.

h1, · · · , ht, · · · , hT = GRU (x1, · · · , xt, · · · , xT )

(16)

where ht ∈ Rk represents the output by the GRU unit at
the tth subsequence. Then, we introduce a set of linear and
softmax layers to generate M hops of weights β ∈ RM ×T
for subsequences. Then, for the hop m

γmt = (wβ

m)(cid:124)ht + bβ

βm1 , · · · , βmT =

(17)

(18)

sof tmax(γm1 , · · · , γmt, · · · , γmT )

m ∈ Rk. Thus, with the subsequence-level weights
where wβ
and hidden outputs, we construct a vector cm ∈ Rk to repre-
sent a patient’s medical visit history with one hop of subse-
quence weights, presented in the following Equation 19.

cm =

βmtht

(19)

T
(cid:88)

t=1

Then, a context vector c ∈ RM ×k is constructed by

concatenating c1, c2, · · · , cM .

VOLUME 0, 2018

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the context vector c, this step integrates the patients
characteristics a ∈ Rq into the context vector for a com-
plete vector representation of the patient’s EHR data. In
this research, the patient characteristics include demographic
information and some static medical conditions, such as age,
gender, and previous hospitalization. Thus, an aggregated
vector is constructed, c(cid:48) ∈ RM ×k+q, by adding a as addi-
tional dimensions to the context vector c.

E. PREDICTING OUTCOME
Given the vector representation of the complete medical
history and characteristics of patients, c(cid:48), we add a linear and
a softmax layer for the ﬁnal outcome prediction, as presented
in Equation 20.

ˆy = sof tmax(wc(cid:124)

c(cid:48) + bc)

(20)

To train the network, we use cross-entropy as the loss

function, presented in Equation 21.

L = −

yilog(ˆyi) + (1 − yi)log(1 − ˆyi)

(21)

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

+

||ββ(cid:124) − I||2
F

where N is the total number of observations. Here, yi is
a binary variable in classiﬁcation problems, while model
output ˆyi is real-valued. The second term in Equation 21 is
to penalize redundancy if the attention mechanism provides
similar subsequence weights for different hops of attention,
which is derived from [32]. This penalty term encourages the
multiple hops to focus on diverse areas and each hop focuses
on a small area.

Thus, we obtain a ﬁnal output for the prediction of out-
comes and a complete personalized vector representation of
the patient’s longitudinal EHR data.

V. EVALUATION
A. BACKGROUND
Although health care spending has been a relatively stable
share of the Gross Domestic Product (GDP) in the United
States since 2009, the costs of hospitalization, the largest
single component of health care expenditures,
increased
by 4.1% in 2014 [33]. Unplanned hospitalization is also
distressing and can increase the risk of related adverse events,
such as hospital-acquired infections and falls [34], [35].
Approximately 40% hospitalizations in the United King-
dom are unplanned and are potentially avoidable [36]. One
important form of unplanned hospitalization is hospital re-
admissions within 30 days of discharge, which is ﬁnancially
penalized in the United States. Early interventions targeted to
patients at risk of hospitalization could help avoid unplanned
admissions, reduce inpatient health care cost and ﬁnancial

7

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 5. A graphical illustration of the experimental setting for the risk
prediction of hospitalization

penalties for providers, and reduce emergency department
congestion [37].

In this research, we apply our proposed representation
learning framework to the risk prediction of future hospi-
talization. Many studies have been conducted by researchers
to predict the risk of 30-day readmission, or the admission
risk of a particular population, such as patients with Am-
bulatory Care Sensitive Conditions (ACSCs), patients with
heart failure, etc. [38]–[41]. Here, we focus on the general
population and the objective is to predict the risk of all-cause
hospitalization using longitudinal EHR data.

B. EXPERIMENTAL DESIGN
In this research, we use de-identiﬁed EHR data from the
University of Virginia Health System covering 75 months be-
ginning in September 2010. This dataset contains 2,343,651
inpatient and outpatient visits of 473,915 distinct patients.
We extracted visit data with diagnosis, medication, and pro-
cedure codes.

We deﬁned the observation window and prediction period
to validate the proposed method. We ﬁrst extract all patients
with a medical record of at least 1.5 years, where the ﬁrst year
is the observation window and the medical records in this
time window are used for feature construction. The follow-
ing 6 months is the hold-off period for the purpose of early
detection. For the positive class, we take all patients who
have hospitalization after the ﬁrst 1.5 years in their medical
history, while the negative class consists of patients who have
no hospitalization after 1.5 years. To better illustrate the ex-
perimental setting, we present the observation window, hold-
off and onset of outcome event in Figure 5. Here, the medical
codes include diagnosis, medication, and procedure codes,
and a vector representation is learned for each code. In this
dataset, diagnoses are primarily coded in ICD-9 and a small
portion is ICD-10 codes, while procedures are mainly using
CPT codes with a few ICD-9 procedure codes. The codes of
medications are using the pharmaceutical categories. Overall,
there are 94 distinct medication categories, 34,419 distinct
diagnoses codes, and 7,895 distinct procedure codes in the
EHR data. The dimension of the learned vectors of medical
codes is set to 100. Medical codes that appear in less than 50
patients medical records are excluded as rare events.

To construct the subsequences of medical codes, we use l
days as the time window. Figure 6 presents the cumulative
histogram and density plot of the numbers of visits in the
observation window, and we observe that the majority of

FIGURE 6. The cumulative histogram and density plot of patients’ numbers of
visits

patients have a small number of visits during the observation
window (less than 25% of patients have more than 4 visits).
Thus, we set l to 90 days, which split the observation window
into 4 subsequences.

Within each subsequence, the number of distinct medical
codes were computed and patients with more medical codes
in a subsequence than the 95% quantile were excluded from
the dataset. Overall, there are 8,841 and 89,101 patients in
the target and control groups, respectively. Each group is
randomly split into training, validation and testing sets with
a 7:1:2 ratio. Thus, 70% are used for training, another 20%
is used for testing, and the rest 10% are used for parameter
tuning and early stopping. The stochastic gradient descent
algorithm is used in training to minimize the cross-entropy
loss function, shown in Equation 21.

To evaluate the proposed representation learning frame-
work, we compare the prediction performance of the pro-
posed model with baseline approaches as follows.

1) Logistic regression (LR)

The inputs are the aggregated counts of grouped medical
codes over the entire observation window. Since the di-
mensionality of raw medical codes is huge, AHRQ clini-
cal classiﬁcations of diagnoses and procedures are used to
achieve a more general clustering of medical codes [42].
The medication codes are the pharmaceutical classes. Fur-
thermore, patient characteristics and previous inpatient visit
are also considered, where age and gender are demographic
information, and a binary indicator is utilized to represent
the presence of the previous hospitalization. Hence, the input
is a 436-dimensional vector representing a patient’s medical
history and characteristics.

8

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 1. The predictive performance of baselines and the proposed Patient2Vec framework

Methods
LR
MLP
RETAIN
FRNN-MGE
BiRNN-MGE
FRNN-MVE
BiRNN-MVE
Patient2Vec

Sensitivity
0.637 ± 0.010
0.727 ± 0.013
0.553 ± 0.012
0.636 ± 0.012
0.600 ± 0.012
0.753 ± 0.011
0.724 ± 0.010
0.769 ± 0.010

Speciﬁcity
0.728 ± 0.003
0.617 ± 0.004
0.710 ± 0.003
0.739 ± 0.004
0.777 ± 0.003
0.676 ± 0.004
0.707 ± 0.003
0.694 ± 0.004

AUC
0.721 ± 0.006
0.713 ± 0.007
0.663 ± 0.007
0.759 ± 0.006
0.768 ± 0.007
0.785 ± 0.006
0.788 ± 0.005
0.799 ± 0.005

F2 score
0.434 ± 0.006
0.423 ± 0.007
0.370 ± 0.008
0.438 ± 0.009
0.439 ± 0.009
0.470 ± 0.008
0.473 ± 0.008
0.492 ± 0.007

2) Multi-layer perceptron (MLP)
A multi-layer perceptron is trained to predict hospitalization
using the same inputs for logistic regression. Here, we use a
one hidden layer MLP with 256 hidden nodes.

3) Forward RNN with medical group embedding
(FRNN-MGE)
We split the sequence into subsequences with equal interval l.
The input at each step is the counts of medical groups within
the associated time interval, and the patient characteristics are
appended as additional features in the ﬁnal logistic regression
step. Here, the RNN is a forward GRU (or LSTM [18]) with
one hidden layer and the size of the hidden layer is 256.

4) Bidirectional RNN with medical group embedding
(BiRNN-MGE)
The inputs used for this baseline is the same as the one for
the FRNN-MGE [15]. The RNN used here is a bidirectional
GRU with one hidden layer and the size of the hidden layer
is 256.

5) Forward RNN with medical vector embedding
(FRNN-MVE)
We split the sequence into subsequences with equal interval l.
The input at each step is the vector representation of the
medical codes within the associated time interval, and the
patient characteristics are appended as additional features in
the ﬁnal logistic regression step. Here, the RNN is a forward
GRU (or LSTM [28]) with one hidden layer and the size of
the hidden layer is 256.

6) Bidirectional RNN with medical vector embedding
(BiRNN-MVE)
The inputs used for this baseline is the same as the one for
the FRNN-MVE [25]. The RNN used here is a bidirectional
GRU or LSTM [15] with one hidden layer and the size of the
hidden layer is 256.

7) RETAIN
This model uses reverse time attention mechanism on RNNs
for an interpretable representation of patient’s EHR data [31].
The inputs are the same as the one for FRNN-MGE, which
takes the counts of medical grouping within each time inter-
val to construct features. Similarly, the two RNNs used for

generating weights are GRU-based and the size of the hidden
layers are 256.

8) Patient2Vec
The inputs are the same as that for FRNN-MVE. One ﬁlter
is used when generating weights for within-subsequence
attention, and three ﬁlters are used for subsequence-level
attention. Similarly, the RNN used here is GRU-based and
there is one hidden layer and the size of the hidden layer
is 256.

The inputs of all baselines and Patient2Vec are normal-
ized to have zero mean and unit variance. We model the
risk of hospitalization based on Patient2Vec and baseline
representations of patients’ medical histories, and the model
performance is evaluated with Area Under Curve(AUC),
sensitivity, speciﬁcity, and F2-score. The validation set is
used for parameter tuning and early stopping in the train-
ing process. Each experiment is repeated 20 times and we
calculate the averages and standard deviations of the above
metrics, respectively.

C. EXPERIMENTAL RESULTS
The predictive performance of Patient2Vec and baselines are
presented in Table 1. The results shown here for the RNN-
based models are based on time interval l = 90 days to
construct subsequences.

According to Table 1, the RNN-based models are generally
capable of achieving higher prediction performance in terms
of sensitivity, AUC and F2 score, except for the RNN models
based on medical group embedding which have lower sensi-
tivity. Among all RNN-based approaches, the ones based on
vector embedding outperform those based on medical group
embedding in terms of sensitivity, AUC, and F2 score. The
bidirectional RNN models generally have higher speciﬁcity
but lower sensitivity than the forward RNN models, while
the bidirectional ones have comparable AUC and F2 score
with the forward ones, respectively. Generally, the proposed
Patient2Vec framework outperforms the baseline methods,
especially in terms of sensitivity and F2 score.

D. VISUALIZATION & INTERPRETATION
In addition to predictive performance, we interpret
the
learned representation by understanding the relative impor-
tance of clinical events in a patient’s EHR data. Considering

VOLUME 0, 2018

9

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 7. The heat map showing feature importance for Patient A

FIGURE 8. The proﬁle of Patient A.

FIGURE 9. The proﬁle of Patient B.

the feature importance learned by Patient2Vec are person-
alized for an individual patient, we illustrate it with two
example patients. Figures 8 and 9 present the proﬁles of
two individuals, Patient A and Patient B, respectively. To
facilitate the interpretation, instead of using raw medical
codes, we present the clinical groups from the AHRQ clinical
classiﬁcation software on diagnoses and procedure codes, as
well as pharmaceutical groups for medications.

According to Figure 8, Patient A is a male patient who
has hospitalization history in the observation window and
is admitted to the hospital seven months after the end of
the observation window for congestive heart failure. The

predicted risk is 96.4%, while the risk decreases for female
patients or patients without hospitalization history. It is also
not surprising to observe an increased risk for older patients.
The heat map in Figure 7 shows the relative importance of
the medical events in this patient’s medical record at each
time window and the ﬁrst row of the heat map presents
the subsequence-level attention. The darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the last subsequence,
t4, is the most important with respect to hospitalization risk,
followed by t1, t2, and t3 in order of importance.

Among all the clinical events in the subsequence t4, we

10

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 2. The top clinical groups with high weights in hospitalized patients

Index

Clinical Groups

Diagnoses
1
2
3

Essential hypertension
Other connective tissue disease
Spondylosis; intervertebral disc disorders; other back
problems
Other lower respiratory disease
Disorders of lipid metabolism
Other aftercare
Diabetes mellitus without complication
Screening and history of mental health and substance
abuse codes
Other nervous system disorders
Other screening for suspected conditions (not mental
disorders or infectious disease)

Other OR therapeutic procedures on nose; mouth and
pharynx
Suture of skin and subcutaneous tissue
Other therapeutic procedures on eyelids; conjunctiva;
cornea

Laboratory - Chemistry and hematology
Other laboratory
Other OR therapeutic procedures of urinary tract
Other OR procedures on vessels other than head and
neck

Therapeutic radiology for cancer treatment

Procedures
1

9
10

4
5
6
7
8

2
3

4
5
6
7

8

2

Medications
1

Diagnostic Products

Analgesics-Narcotic

observe that the OR therapeutic procedures (nose, mouth,
and pharynx), laboratory (chemistry and hematology), coro-
nary atherosclerosis & other heart disease, cardiac dys-
rhythmias, and conduction disorders are the ones with the
highest weights, while other events such as other connected
tissue disease are less important in terms of future hospi-
talization risk. Additionally, some medications appear to be
informative as well, including beta blockers, antihyperten-
sives, anticonvulsants, anticoagulants, etc. In the ﬁrst-time
window, the medical events with high weights are coro-
nary atherosclerosis & other heart disease, gastrointestinal
hemorrhage, deﬁciency and anemia, and other aftercare. In
the next subsequence, the most important medical events
are heart diseases and related procedures such as coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
conduction disorders, hypertension with complications, other
OR heart procedures, and other OR therapeutic nervous
system procedures. We also observe that the kidney disease
related diagnoses and procedures appear to be important
features. Throughout the observation window, the coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
and conduction disorders constantly show high weights with
respect to hospitalization risk, and the ﬁndings are consistent

FIGURE 10. The heat map showing feature importance for Patient B

with medical literature.

Figure 9 presents the proﬁle of Patient B, which is a male
patient without hospitalization in the observation window.
This patient is hospitalized for occlusion of cerebral arter-
ies approximately one year after the observation window,
and the predicted risk is 74.6%. For a similar patient who
is 10 years older or with previous hospitalization history,
the risk increases by 4.2% and 1%, respectively, while there
is a smaller risk of hospitalization for a female patient. To
illustrate the medical events of Patient B, the heat map in
Figure 10 depicts the relative importance of medical groups
in the subsequences, as well as the subsequence-level weights
for hospitalization risk. Similarly, the darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the second subse-
quence appears to be the most important, while the last one is
less predictive of future hospitalization. In fact, the medical
events in the last time window are spondylosis, intervertebral
disc disorders, other back problems and other bone disease &
musculoskeletal deformities, and malaise and fatigue, which
are not highly related to the cause of hospitalization of Patient
B.

In the most predictive subsequence, t2, we observe that
other OR heart procedures, genitourinary symptoms, spondy-
losis, intervertebral disc disorders, other back problems,
therapeutic procedures on eyelid, conjunctiva, and cornea,
and arterial blood gases have high attention weights. In the

VOLUME 0, 2018

11

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 3. The top diagnosis groups with high weights in patients hospitalized
for osteoarthritis, septicemia, acute myocardial infarction, congestive heart
failure, and diabetes mellitus with complications, respectively

Index

Diagnosis Groups

In patients admitted for osteoarthritis

Osteoarthritis
Other connective tissue disease

1
2

3
4

5

1
2

3
4
5

1
2
3

4
5

1
2
3
4
5

1
2
3

4
5

Other non-traumatic joint disorders
Spondylosis; intervertebral disc disorders; other back prob-
lems
Other aftercare

In patients admitted for septicemia

Essential hypertension
Diabetes mellitus without complication

Disorders of lipid metabolism
Other lower respiratory disease
Other aftercare

In patients admitted for acute myocardial infarction

Coronary atherosclerosis and other heart disease
Medical examination/evaluation
Other screening for suspected conditions (not mental disorders
or infectious disease)

Other lower respiratory disease
Disorders of lipid metabolism

In patients admitted for congestive heart failure

Congestive heart failure (nonhypertensive)
Coronary atherosclerosis and other heart disease
Cardiac dysrhythmias
Diabetes mellitus without complication
Other lower respiratory disease

In patients admitted for diabetes mellitus with complications

Diabetes mellitus with complications
Diabetes mellitus without complication
Other aftercare

Other nutritional; endocrine; and metabolic disorders
Fluid and electrolyte disorders

earliest time window, the most important medical events also
include therapeutic procedures on eyelid, conjunctiva, and
cornea, arterial blood gases, while diabetes, hypertension
as well as diagnostic products show their relatively high
importance. Throughout the observation window, medical
events spondylosis, intervertebral disc disorders, other back
problems, therapeutic procedures on eyelid, conjunctiva, and
cornea are constantly with high attention weights. Here, di-
agnostic products is a medication class, which include barium
sulfate, iohexol, gadopentetate dimeglumine, iodixanol, tu-
berculin puriﬁed protein derivative, iodixanol, regadenoson,
acetone (urine), and so forth. These medications are primarily
for blood or urine testing, or used as radiopaque contrast
agents for x-rays or CT scans for diagnostic purposes.

Additionally, we attempt to interpret the learned repre-
sentation and feature importance at the population-level. In
Table 2, we present the top 20 clinical groups with high
weights among hospitalized patients in the test set.

According to Table 2, the most predictive diagnosis groups
for future hospitalization are chronic diseases, including
essential hypertension, diabetes, lower respiratory disease,
disorders of lipid metabolism, and musculoskeletal diseases
such as other connective tissue disease and spondylosis,
intervertebral disc disorders, other back problems. The most
important procedures are some OR therapeutic procedures
and laboratory tests, such as the OR procedures on nose,
mouth, and pharynx, vessels, urinary tract, eyelid, conjunc-
tiva, cornea, etc. It is not surprising to see that diagnostic
products are showing with high weights, considering these
medications are used in testing or examinations for diagnos-
tic purposes.

Moreover, we present the top diagnoses groups with high
weights in patients hospitalized for different primary causes.
Table 3 shows the top 5 diagnosis groups with high weights
in patients admitted for osteoarthritis, septicemia (except in
labor), acute myocardial infarction, congestive heart failure
(nonhypertensive), and diabetes mellitus with complications,
respectively. Accordingly, we observe that the most impor-
tant diagnoses for hospitalization risk prediction in popu-
lation admitted for osteoarthritis are musculoskeletal dis-
eases such as connective tissue disease, joint disorders, and
spondylosis. However, the diagnoses with highest weights
in the patients admitted for septicemia are chronic diseases
including essential hypertension, diabetes, disorders of lipid
metabolism, and respiratory disease. The top diagnoses have
many overlaps between the populations admitted for acute
myocardial infarction and for congestive heart failure, con-
sidering both populations are admitted for heart diseases.
the overlapped diagnosis groups include coronary
Here,
atherosclerosis and other heart diseases and lower respiratory
diseases. As for patients admitted for diabetes with com-
plications, the top diagnoses are diabetes with or without
complications, nutritional, endocrine, metabolic disorders,
and ﬂuid and electrolyte disorders. In general, the learned
feature importance is consistent with medical literature.

VI. DISCUSSION
Our proposed framework is applied to the prediction of
hospitalization using real EHR data that demonstrates its
prediction accuracy and interpretability. This work could be
further enhanced by incorporating the follow-up information
on the negative patient population and investigate if it in-
deed shows an improved health outcome or the patient is
hospitalized elsewhere. Patient2Vec employs a hierarchical
attention mechanism, allowing us to directly interpret the
weights of clinical events. In future work, we will extend the
attention to incorporate demographic information for a more
comprehensive and automatic interpretation.

Although we apply Patient2Vec to the early detection of
long-term hospitalization, i.e., at least 6 months after the
previous hospitalization, it could be used to predict the risk
of 30-day readmission to help prevent unnecessary rehospi-
talizations.

12

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

VII. CONCLUSION
In this paper, we propose a representation learning frame-
work, Patient2Vec, to learn a personalized interpretable deep
representation of EHR data based on recurrent neural net-
works and the attention mechanism. This work improves
the performance of predictive models as well as deepens
the understanding of disease correlations. We apply this
framework to the risk prediction of hospitalization using
patients’ longitudinal EHR data. The experimental results
demonstrate that the proposed Patient2Vec representation is
capable of achieving a more accurate prediction than base-
lines approaches. Moreover, the learned feature importance
in the representations are interpreted both at the individual
and population levels to facilitate clinical insights.

In this work, the proposed Patient2Vec framework is eval-
uated with the risk prediction of all-cause hospitalization,
but in the future could be applied to predict hospitalization
in more speciﬁc populations, other health related prediction
problems, or domains outside of health.

REFERENCES
[1] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical
attention networks for document classiﬁcation,” in Proceedings of the
2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, 2016, pp.
1480–1489.

[2] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,” arXiv

preprint arXiv:1408.5882, 2014.

[3] J. Howard and S. Ruder, “Fine-tuned language models for text classiﬁca-

tion,” arXiv preprint arXiv:1801.06146, 2018.

[4] M. M. Lopez and J. Kalita, “Deep learning applied to nlp,” arXiv preprint

arXiv:1703.03091, 2017.

[5] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using
rnn encoder-decoder for statistical machine translation,” arXiv preprint
arXiv:1406.1078, 2014.

[6] A. L. Nobles, J. J. Glenn, K. Kowsari, B. A. Teachman, and L. E. Barnes,
“Identiﬁcation of imminent suicide risk among young adults using text
messages,” in Proceedings of the 2018 CHI Conference on Human Factors
in Computing Systems. ACM, 2018, p. 413.

[7] K. Kowsari, D. E. Brown, M. Heidarysafa, K. J. Meimandi, M. S. Gerber,
and L. E. Barnes, “Hdltex: Hierarchical deep learning for text classiﬁca-
tion,” in 2017 16th IEEE International Conference on Machine Learning
and Applications (ICMLA), Dec 2017, pp. 364–371.

[8] K. Kowsari, M. Heidarysafa, D. E. Brown, K. Jafari Meimandi, and L. E.
Barnes, “Rmdl: Random multimodel deep learning for classiﬁcation.”
ACM, 2018.

[9] H. Strobelt, S. Gehrmann, H. Pﬁster, and A. M. Rush, “Lstmvis: A tool
for visual analysis of hidden state dynamics in recurrent neural networks,”
IEEE transactions on visualization and computer graphics, vol. 24, no. 1,
pp. 667–676, 2018.

[10] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” Scientiﬁc
reports, vol. 8, no. 1, p. 6085, 2018.

[11] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches
preprint

neural machine

translation,”

arXiv

attention-based
to
arXiv:1508.04025, 2015.

[12] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[13] D. Yogatama, C. Dyer, W. Ling, and P. Blunsom, “Generative and discrim-
inative text classiﬁcation with recurrent neural networks,” arXiv preprint
arXiv:1703.01898, 2017.

[14] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent

trends
language processing,” arXiv preprint

in deep learning based natural
arXiv:1708.02709, 2017.

[15] M. Basaldella, E. Antolli, G. Serra, and C. Tasso, “Bidirectional lstm
recurrent neural network for keyphrase extraction,” in Italian Research

Conference on Digital Libraries, Springer.
lishing, 2018, pp. 180–187.

Springer International Pub-

[16] S. Ghosh, O. Vinyals, B. Strope, S. Roy, T. Dean, and L. Heck, “Con-
textual lstm (clstm) models for large scale nlp tasks,” arXiv preprint
arXiv:1602.06291, 2016.

[17] B. Yue, J. Fu, and J. Liang, “Residual recurrent neural networks for
learning sequential representations,” Information, vol. 9, no. 3, p. 56, 2018.
[18] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
gated recurrent neural networks on sequence modeling,” arXiv preprint
arXiv:1412.3555, 2014.

[19] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difﬁculty of training
recurrent neural networks.” ICML (3), vol. 28, pp. 1310–1318, 2013.
[20] D. Britz, “Recurrent neural network tutorial,” http://www.wildml.com/

2015/10/, 2015, [Accessed on October 5, 2017].

[21] V. Mnih, N. Heess, A. Graves et al., “Recurrent models of visual attention,”
in Advances in neural information processing systems, 2014, pp. 2204–
2212.

[22] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for
abstractive sentence summarization,” arXiv preprint arXiv:1509.00685,
2015.

[23] T. Ma, C. Xiao, and F. Wang, “Health-atm: A deep architecture for
multifaceted patient health record representation and risk prediction,” in
Proceedings of the 2018 SIAM International Conference on Data Mining.
SIAM, 2018, pp. 261–269.

[24] A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J. Liu,
X. Liu, J. Marcus, M. Sun et al., “Scalable and accurate deep learning with
electronic health records,” npj Digital Medicine, vol. 1, no. 1, p. 18, 2018.
[25] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzell, “Learning to diagnose
with lstm recurrent neural networks,” arXiv preprint arXiv:1511.03677,
2015.

[26] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” arXiv preprint
arXiv:1606.01865, 2016.

[27] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghas-
semi, B. Moody, P. Szolovits, L. A. Celi, and R. G. Mark, “Mimic-iii, a
freely accessible critical care database,” Scientiﬁc data, vol. 3, 2016.
[28] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart, and J. Sun, “Doctor
ai: Predicting clinical events via recurrent neural networks,” in Machine
Learning for Healthcare Conference, 2016, pp. 301–318.

[29] Z. Che, S. Purushotham, R. Khemani, and Y. Liu, “Interpretable deep
models for icu outcome prediction,” in AMIA Annual Symposium Pro-
ceedings, vol. 2016. American Medical Informatics Association, 2016,
p. 371.

[30] J. H. Friedman, “Greedy function approximation: a gradient boosting

machine,” Annals of statistics, pp. 1189–1232, 2001.

[31] E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart,
“Retain: An interpretable predictive model for healthcare using reverse
time attention mechanism,” in Advances in Neural Information Processing
Systems, 2016, pp. 3504–3512.

[32] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Ben-
gio, “A structured self-attentive sentence embedding,” arXiv preprint
arXiv:1703.03130, 2017.

[33] C. M. Torio and B. J. Moore, “National inpatient hospital costs: The most
expensive conditions by payer, 2013,” https://www.hcup-us.ahrq.gov/
reports/statbriefs/sb204-Most-Expensive-Hospital-Conditions.jsp, 2016.
[34] E. Wallace, E. Stuart, N. Vaughan, K. Bennett, T. Fahey, and S. M.
Smith, “Risk prediction models to predict emergency hospital admission
in community-dwelling adults: a systematic review,” Medical care, vol. 52,
no. 8, p. 751, 2014.

[35] E. N. de Vries, M. A. Ramrattan, S. M. Smorenburg, D. J. Gouma, and
M. A. Boermeester, “The incidence and nature of in-hospital adverse
events: a systematic review,” Quality and safety in health care, vol. 17,
no. 3, pp. 216–223, 2008.

[36] S. Purdey and A. Huntley, “Predicting and preventing avoidable hospital
admissions: a review.” The journal of the Royal College of Physicians of
Edinburgh, vol. 43, no. 4, pp. 340–344, 2012.

[37] H. Ontario, “Early identiﬁcation of people at risk of hospitalization: Hos-
pital admission risk prediction (harp)-a new tool for supporting providers
and patients,” 2013.

[38] B. Zheng, J. Zhang, S. W. Yoon, S. S. Lam, M. Khasawneh, and S. Poranki,
“Predictive modeling of hospital readmissions using metaheuristics and
data mining,” Expert Systems with Applications, vol. 42, no. 20, pp. 7110–
7120, 2015.

VOLUME 0, 2018

13

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

[39] D. Kansagara, H. Englander, A. Salanitro, D. Kagen, C. Theobald,
M. Freeman, and S. Kripalani, “Risk prediction models for hospital
readmission: a systematic review,” The Journal of the American Medical
Association, vol. 306, no. 15, pp. 1688–1698, 2011.

[40] G. Giamouzis, A. Kalogeropoulos, V. Georgiopoulou, S. Laskar, A. L.
Smith, S. Dunbar, F. Triposkiadis, and J. Butler, “Hospitalization epidemic
in patients with heart failure: risk factors, risk prediction, knowledge gaps,
and future directions,” Journal of cardiac failure, vol. 17, no. 1, pp. 54–75,
2011.

[41] E. Prescott, A. M. Bjerg, P. K. Andersen, P. Lange, and J. Vestbo, “Gender
difference in smoking effects on lung function and risk of hospitalization
for COPD: results from a danish longitudinal population study,” European
Respiratory Journal, vol. 10, no. 4, pp. 822–827, 1997.

[42] Agency for Healthcare Research and Quality (AHRQ), “Clinical classi-
ﬁcations software (CCS) for ICD-9-CM,” https://www.hcup-us.ahrq.gov/
toolssoftware/ccs/ccs.jsp, 2015.

JENNIFER M. LOBO (jem4yb@virginia.edu) is
an Assistant Professor of Biomedical Informatics
in the Department of Public Health Sciences at
University of Virginia. She received her Ph.D. in
Industrial Engineering from North Carolina State
University, Raleigh, NC. Her research interests in-
volve using mathematical modeling and stochastic
optimization methods to build models that simu-
late the natural course of disease. These models
allow for estimation of outcomes under different
screening and treatment policies in the absence of randomized controlled
trials, and can be used to optimize screening and treatment decisions for
patients with chronic diseases. Her projects include optimizing treatment
for patients with type 2 diabetes, generating individualized decision analysis
models for prostate cancer patients, and developing optimal imaging surveil-
lance guidelines for recurrent kidney cancer.

JINGHE ZHANG (jz4kg@virginia.edu) is a lead
data scientist at Target Corporation. She received
her Ph.D. in Systems Engineering from the Uni-
versity of Virginia. Prior to entering the Ph.D. pro-
gram at UVA, she received the Master of Science
in Industrial and Systems Engineering from the
State University of New York at Binghamton. Her
research interests are in natural language process-
ing, machine learning, recommender systems, and
health informatics.

LAURA E. BARNES (lb3dp@virginia.edu) is an
Associate Professor in Systems and Information
Engineering and the Data Science Institute at the
University of Virginia. She received her Ph.D. in
Computer Science from the University of South
Florida, Tampa, FL. She directs the Sensing Sys-
tems for Health (S2He) Lab which focuses on
understanding the dynamics and personalization
of health and well-being through mobile sensing
and analytics.

(kk7nc@virginia.edu) is
KAMRAN KOWSARI
a Ph.D. student in the Department of Systems
and Information Engineering at the University of
Virginia, Charlottesville, VA. He is a member of
the Sensing Systems for Health Lab. He received
his Master of Science from Department of Com-
puter Science at The George Washington Univer-
sity, Washington, DC. He has more than ten years
of experience in machine learning and software
development. His experience includes numerous
industrial and academic projects. His research interests include natural
language processing, machine learning, deep learning, artiﬁcial intelligence,
text mining, and unsupervised learning.

JAMES H. HARRISON, JR., (jhh5y@virginia.
edu) is Associate Professor of Pathology and Di-
rector of Laboratory Information Systems at the
University of Virginia Medical Center and also has
appointments in the Departments of Public Health
Sciences in the UVA School of Medicine, and
Systems and Information Engineering in the UVA
School of Engineering and Applied Sciences. He
received his MD and PhD (Pharmacology) de-
grees from Medical University of South Carolina,
Charleston, SC, completed residencies in Anatomic Pathology and Labora-
tory Medicine at Yale-New Haven Hospital, New Haven, CT, and completed
a postdoctoral fellowship in Environmental Toxicology at Yale University,
New Haven, CT. Dr. Harrison has over 25 years of experience in the ﬁeld
of medical informatics, including work in clinical laboratory information
systems, electronic health records, clinical data analysis, and clinical data
standards development.

14

VOLUME 0, 2018

Date of publication 10, 2018 , date of current version 10, 2018.

Digital Object Identiﬁer 10.1109/ACCESS.2018.2875677

Patient2Vec: A Personalized
Interpretable Deep Representation of the
Longitudinal Electronic Health Record

JINGHE ZHANG1, (Member, IEEE), KAMRAN KOWSARI1,4(Member, IEEE), JAMES H.
HARRISON2,3,5, JENNIFER M. LOBO2,5, and LAURA E. BARNES1,4,5, (Member, IEEE)
1Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, USA
2 Department of Public Health Sciences, University of Virginia, Charlottesville, VA 22904, USA
4 Sensing Systems for Health Lab, University of Virginia, Charlottesville, VA 22904, USA
3 Division of Laboratory Medicine Department of Pathology, University of Virginia, Charlottesville, VA 22904, USA
5Data Science Institute, University of Virginia, Charlottesville, VA 22904, USA
Corresponding author: Laura E. Barnes (lb3dp@virginia.edu)
This research was supported by a Jeffress Trust Award in Interdisciplinary Science.
Patient2Vec is shared as an open source tool at https://github.com/BarnesLab/Patient2Vec

ABSTRACT The wide implementation of electronic health record (EHR) systems facilitates the collection
of large-scale health data from real clinical settings. Despite the signiﬁcant increase in adoption of EHR
systems, this data remains largely unexplored, but presents a rich data source for knowledge discovery from
patient health histories in tasks such as understanding disease correlations and predicting health outcomes.
However, the heterogeneity, sparsity, noise, and bias in this data present many complex challenges.
This complexity makes it difﬁcult to translate potentially relevant information into machine learning
algorithms. In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep
representation of longitudinal EHR data which is personalized for each patient. To evaluate this approach,
we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive
performance with baseline methods. Patient2Vec produces a vector space with meaningful structure and it
achieves an AUC around 0.799 outperforming baseline methods. In the end, the learned feature importance
can be visualized and interpreted at both the individual and population levels to bring clinical insights.

INDEX TERMS Attention mechanism, gated recurrent unit, hospitalization, longitudinal electronic health
record, personalization, representation learning.

8
1
0
2
 
t
c
O
 
5
2
 
 
]

M
Q
.
o
i
b
-
q
[
 
 
3
v
3
9
7
4
0
.
0
1
8
1
:
v
i
X
r
a

I. INTRODUCTION

L ongitudinal EHR data resemble text documents from

many perspectives. A text document consists of a se-
quence of sentences, and a sentence is a sequence of words.
Similarly, the longitudinal health record of a patient consists
of a sequence of visits, and there is a list of clinical events,
including diagnoses, medications, and procedures, that occur
during a visit. Considering these similarities, representation
learning methods for text documents in Natural Language
Processing (NLP) have great potential to be applied to lon-
gitudinal EHR data.

Deep neural networks have become very popular in the
NLP ﬁeld and have been very successful in many applica-
tions, such as machine translation, question answering, text
classiﬁcation, document summarization, language modeling,
etc. [1]–[8]. These networks excel at complex language tasks

because they are capable of identifying high-order relation-
ships, the network structure can encode language structures,
and they allow the learning of a hierarchical representation
of the language, i.e., representations for tokens, phrases, and
sentences, etc.

Among a variety of deep learning methods, Recurrent
Neural Networks (RNNs) have shown their effectiveness in
NLP tasks because they have the ability to capture sequential
information [7]–[10] which is inherent in human language.
Traditional neural networks assume that inputs are inde-
pendent of each other, while an RNN computes the output
based on the current input as well as the “memory” from the
previous computation. Although vanilla RNNs are not good
at capturing long-term dependencies, many variants have
been proposed and validated that are effective in addressing

VOLUME 0, 2018

1

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

its performance with other baseline methods. In addition
to prediction performance, we further interpret the learned
representations with visualizations on example patients and
events. Finally, Section V provides a summary of this work.

II. RELATED WORK
In this section, we present an overview of a gated recurrent
unit, a type of RNN, which is capable of capturing long-term
dependencies. Then we brieﬂy introduce attention mecha-
nisms in neural networks that allow the network to attend
to certain regions of data, which is inspired by the visual
attention mechanism in humans. Additionally, we summarize
the RNN networks and attention mechanisms previously used
to mine EHR data.

A. RECURRENT NEURAL NETWORKS (RNN)

RNNs are expected to learn long-term dependencies by tak-
ing the previous state and the new input in the computation at
the current time step t. However, vanilla RNNs are incapable
of capturing the dependencies when the sequence is very long
due to the vanishing gradient problem [12]. Many variants of
the RNN network have been proposed to address this issue,
and long short term memory (LSTM) is one of the most
popular models used nowadays in NLP tasks [7], [8], [13]–
[16].

this issue.

In the medical domain, it is critical that analytical results
are interpretable, so that they can be understood and validated
by a human with expert knowledge and so that knowledge
captured by analysis can be used for process improvement.
Traditional deep neural networks have the disadvantage that
they lack interpretability. A substantial amount of work is
ongoing to make sense of the “black box”, and the attention
mechanism [11] is one of the more effective methods recently
developed to make the output of these algorithms more
interpretable.

Health care is undergoing unprecedented change, and there
is a great potential and demand for personalized care strate-
gies. Personalized medicine, also called precision medicine,
has previously focused on optimizing therapy to better ﬁt the
genetic makeup of the patient or the disease (e.g., the genetic
susceptibility of cancer to speciﬁc chemotherapy strategies).
The availability of EHR data and advances in machine learn-
ing create the potential for another type of personalization
of healthcare. This type of personalization has become ubiq-
uitous in our daily life. For example, customers have come
to expect personalized search on Google and personalized
product recommendations on Amazon and Netﬂix, based on
their charactersitics and previous experiences with the sys-
tems. Personalization of healthcare processes, based on a pa-
tient’s phenotype (physical and medical characteristics) and
healthcare experiences as documented in the health record,
may also improve "customer" satisfaction and it has the addi-
tional potential to improve healthcare efﬁciency, lower costs,
and yield better outcomes. We believe that representation
learning methods can capture a personalized representation
of the important heterogeneities in patients’ phenotypes and
medical histories at the population-level, and make these
representations available to drive healthcare decisions and
strategies.

This research is based on RNN models and the attention
mechanism with the objective of learning a personalized, in-
terpretable, and complete representation of patients’ medical
records. Our proposed framework is capable of learning a
personalized representation for each patient from a sequence
of clinical events. A hierarchical attention mechanism learns
personalized weights of clinical events, including hospital
visits and the procedures that they contain. These weights
allow us to interpret the relative importance and roles of clin-
ical events in the learned representations both at individual
and population levels. The ultimate goal is more accurate
prediction and better insight into the critical elements of
healthcare processes that can be used to improve healthcare
delivery.

The rest of this paper is organized as follows: Section II
summarizes the variants of RNNs and the attention mecha-
nism, as well as their application to EHR data. Section III
presents an overview of the proposed Patient2Vec repre-
sentation learning framework, and Section IV elaborates
the details of the algorithms. In Section V, the proposed
framework is evaluated for a prediction task and we compare

FIGURE 1. The top ﬁgure is a GRU gating unit and bottom ﬁgure shows an
LSTM unit [7]

2

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

1) Gated Recurrent Unit (GRU)
GRU is a simpliﬁed version of LSTM [7]. The basic idea
of GRU is to combat the vanishing gradient problem with a
gating mechanism. Hence the general recurrent structure in
GRU is identical to vanilla RNNs except that a GRU unit
is used in the computation at each time step rather than a
traditional simple recurrent unit.

In general, a GRU cell has two gates, i.e., a reset gate r
and an update gate z. The reset gate is used to determine
how to integrate the previous state into the computation of
the current state, while the update gate determines how much
the unit updates its activation.

Given the input xt at time step t, the reset gate rt is

computed as presented in Equation 1

rt = σ(Urxt + Wrst−1)

(1)

where Ur and Wr are the weight matrices of the reset gate
and st−1 is the hidden activation at time step t − 1. A similar
computation is performed for the update gate zt at time step t,
shown in Equation 2

zt = σ(Uzxt + Wzst−1)

(2)

where Uz and Wz are the weight matrices of update gate.
The current hidden activation ht is computed by

ht = (1 − zt)ht−1 + zt
(3)
where ˜ht is the candidate activation at time step t. The
computation of ˜ht is presented in Equation 4

˜ht

˜ht = tanh(Wxt + U(rt (cid:12) ht−1))

(4)

where U and W are weight matrices and (cid:12) represents
element-wise multiplication. Figure 1 presents a graphical
illustration of the GRU [7] and one unit of LSTM.

GRU is capable of learning long-term dependencies [17]
due to the additive component of update from t to t + 1
in the gating mechanism. Consequently, important features
will be carried forward in the input stream while irrelevant
information will be dropped. When the reset gate is 0, the
network is forced to drop previous states and reset with cur-
rent information. Moreover, the method provides shortcuts
such that the error is easily backpropagated without vanishing
too quickly [5], [18]. Hence, the GRU is well-suited to learn
long-term dependencies in sequence data.

2) Long Short-Term Memory (LSTM)
An LSTM unit is similar to a GRU, but with one more gate in
an LSTM unit (as shown in Figure 1). LSTM also preserves
long term dependencies more effectively than basic RNN.
This is particularly useful to overcome the vanishing gradient
problem [19]. Although LSTM has a chain-like structure sim-
ilar to RNN, LSTM uses multiple gates to carefully regulate
the amount of information that will be allowed into each node
state. Figure 1 shows the basic cell of an LSTM model. A step

VOLUME 0, 2018

by step explanation of an LSTM cell is as following:
Input gate:

it = σ(Wi[xt, ht−1] + bi),

Candid memory cell value:

˜Ct = tanh(Wc[xt, ht−1] + bc),

Forget gate activation:

ft = σ(Wf [xt, ht−1] + bf ),

New memory cell value:

Ct = it ∗ ˜Ct + ftCt−1,

Output gate value:

ot = σ(Wo[xt, ht−1] + bo),

ht = ot tanh(Ct),

(5)

(6)

(7)

(8)

(9)

(10)

In the above description all b represent bias vectors, all W
represent weight matrices, and xt is used as input to the
memory cell at time t. Also,the i, c, f, o indices refer to input,
cell memory, forget and output gates respectively. An RNN
can be biased when later words are more inﬂuential than the
earlier ones.

Empirically, LSTM and GRU achieve comparable per-
formance in many tasks but there are fewer parameters in
a GRU, which makes it a little faster to learn and able to
generalize with fewer data [20].

B. ATTENTION MECHANISM
Attention mechanisms, inspired by the visual attention sys-
tem found in humans, have become popular in deep learning.
Attention allows the network to focus on certain regions of
data, while perceiving other regions with “low resolution”.
In addition to higher accuracy, it also facilitates the interpre-
tation of learned representations. We elaborate an attention
mechanism on an RNN network, and Figure 2 presents a
graphical illustration.

According to Figure 2, a variable-length weight vector α
is learned based on hidden states [11]. Then a global context
vector is computed based on weights α and all the hidden
states to create the ﬁnal output. Equation 11 presents the
computation of the weight vector α = {α1, α2, · · · , αT },
where T is the length of the sequence

α1, α2, · · · , αT = f (Wαh + bα)

(11)

and where f is a nonlinear activation function, usually
sof tmax or tanh. Then, the context vector c is constructed
as:

c =

αtht

T
(cid:88)

t=1

Thus, the network puts more attention on the important
features for the ﬁnal prediction which can improve the model
performance. An additional beneﬁt is that the weights can

(12)

3

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

GRU-based model to address missing values in multivariate
time series data, in which the missing patterns are incorpo-
rated for improved prediction performance. This work has
been applied to the Medical Information Mart for Intensive
Care III (MIMIC-III) clinical database to demonstrate its
effectiveness in mining time series of clinical measurements
with missing values [27]. Longitudinal EHR data including
clinical events, such as diagnoses, medications, and pro-
cedures is also a potentially rich resource for predictive
modeling. Choi et al. [28] analyze this data with a GRU
network to forecast future clinical events, and it achieves a
better prediction performance than comparison models such
as logistic regression and MLP.

Difﬁculty in interpreting model behavior is one of the
major drawbacks of using deep learning to mine EHR data.
Some attempts have been made to address this issue. Che et
al. [29] propose an interpretable mimic learning method
which trains a mimic gradient boosting trees model to utilize
predicted labels or features learned by deep learning mod-
els for ﬁnal prediction [30]. Then the feature importances
learned by the tree-based models are used for knowledge
discovery. Attention mechanisms have been introduced re-
cently to improve the interpretability of the prediction results
of deep learning models in health analytics. Choi et al. [31]
develop an interpretable model with two levels of attention
weights learned from two reverse-time GRU models, re-
spectively. The experimental results on EHR data indicate
comparable prediction performance with conventional GRU
models but more interpretable results. Our work continues
the attempt to use attention mechanisms to improve the
interpretability of RNN-based models.

III. PATIENT2VEC SYSTEM MODEL
In this section, we provide an overview of the proposed
hierarchical representation learning framework. This frame-
work uses deep recurrent neural networks to capture the
complex relationships between clinical events in the patient’s
EHR data and employs the attention mechanism to learn
a personalized representation and to obtain relative feature
importance. The proposed representation learning framework
contains four steps and is presented graphically in Figure 3.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES

EHR data consists primarily of records of outpatient and
inpatient visits to healthcare providers. These visit records
include multiple clinical codes for diagnoses, symptoms,
procedures, therapies, and other observations and events that
occurred during the visit. Here, we treat the set of medical
codes associated with a visit as a sentence consisting of
words, except that there is no ordering in the words. Thus,
we adopt the word2vec approach to construct a vector to
represent each medical code.

FIGURE 2. The global attention model

be utilized to understand the importance of features such
that the models are more interpretable. The attention mech-
anism has been introduced to both Convolutional Neural
Networks (CNNs) and RNNs for various tasks and has
achieved many successes in the ﬁelds of computer vision and
NLP [11], [21], [22].

C. DEEP LEARNING IN EHR DATA
Previous studies on EHR data mainly use statistical meth-
ods or traditional machine learning techniques. Recently
researchers have started adapting deep learning approaches
to this data [23], [24], including textual notes, temporal
measurements of laboratory testing in the Intensive Care
Unit (ICU), and longitudinal data in patient populations.
Here, we summarize deep learning research in mining EHR
data and focus on the studies using RNN-based models.

Hospitalized patients, especially patients in ICUs, are
continuously monitored for cardiac, respiratory, and other
physical functions, creating a large volume of sequential data
in multiple dimensions. These measurements are utilized by
physicians to make diagnostic and treatment decisions. The
functions monitored may change over time and monitor-
ing may be irregular, based on a patient’s condition. It is
very challenging for traditional machine learning methods
to mine this multivariate time series data considering miss-
ing values, varying length, and irregular, non-simultaneous
sampling. Lipton et al. [25] trained an LSTM with a repli-
cated target to learn from these sequence data and used
this model to make predictions of diagnoses. The data used
in this research are time series of clinical measurements
with continuous values, and the LSTM models outperformed
logistic regression and MLP. Che et al. [26] developed a

4

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

are unlikely to contribute equally to the prediction of the tar-
get outcome, we cannot aggregate them with equal weights.
Instead, we employ a self-attention mechanism which trains
the network to learn the weights.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of subsequences with embedded medical
codes, we are able to input it into a recurrent neural network
to capture the temporal dependencies between events. How-
ever, the subsequences of visits are not contributing equally
to the outcome. Hence, we employ another level of attention
to learn the weights of the subsequences by the network itself
for the outcome prediction.

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the learned weights and hidden outputs, we aggregate
them into one universal vector for a comprehensive represen-
tation. In this step, the static information, such as age, gender,
previous hospitalization history is added as extra features, to
get a complete representation of a patient.

E. PREDICTING OUTCOME
Given the complete vector representation of a patient’s EHR
data, we add a logistic regression layer at the end for the
prediction of outcome.

IV. PATIENT2VEC REPRESENTATION LEARNING
ALGORITHM
In this section, we present the details of the proposed rep-
resentation learning framework, which is based on a GRU
network and a hierarchical attention mechanism. Figure 4
presents the structure of the proposed network with attention.

The proposed framework consists of ﬁve parts presented in
the following: I) Learning vector representations of medical
codes, II) Learning within-subsequence self-attention, III)
Learning subsequence-level self-attention, IV) Constructing
aggregated deep representation, V) Predicting outcome.

A. LEARNING VECTOR REPRESENTATIONS OF
MEDICAL CODES
Given a patient’s raw EHR data, a sequence of visits, we
observe that a visit usually contains multiple medical codes.
Hence, it is feasible to learn a vector to represent the medical
code by capturing the relationships between the codes. In
this work, we employ the classical word2vec algorithm, skip-
gram. The basic idea of skip-gram is to learn a vector to
represent each word such that the probability of the context
to predict based on the target word is maximized. Hence,
the vectors of similar words are close to each other in the
learned feature space. In the skip-gram model, the vectors
are learned by training a shallow neural network to predict the
context words given an input word. Similarly, in our problem,

FIGURE 3. The Patient2Vec representation learning framework

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Clinical visits are represented as the set of vectors for the
codes associated with the visit. Because closely-spaced visits
are usually related clinically, we employ a time window to
split the sequence of visits into multiple subsequences of
equal length. A subsequence might contain multiple visits if
they occurred within the same time window, or there might
be no visits during a particular time window yielding an
empty subsequence. Thus we transform the original sequence
of irregularly-spaced visits into a sequence of subsequences
with equal intervals, which is preferable for recurrent neural
networks. The width of the subsequence window deﬁnes the
time granularity of the method and its optimal width is related
to the acuity (i.e., stability) of the clinical characteristics
involved in the predication task. In future work it may be
possible to deﬁne the relationship between clinical acuity and
optimal subsequence width, or develop methods for learning
an optimal width for a deﬁned prediction task.

Because all medical events occurring within a subsequence

VOLUME 0, 2018

5

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 4. A graphical illustration of the network in the Patient2Vec representation learning framework

the input is a medical code and the target to predict are the
medical codes occurred in the same visit.

Hence, each subsequence is a matrix consisting of the
vectors of medical codes occurred during this associated time
window.

B. LEARNING WITHIN-SUBSEQUENCE
SELF-ATTENTION
Given a sequence of subsequences encoded by vectors of
medical codes, this step employs the within-subsequence
attention which allows the network itself to learn the weights
of vectors in the subsequence according to its contribution to
the prediction target.

Here, we denote the sequence of patient i as s(i), and v(i)
t

t

, · · · , v(i)

1 , · · · , v(i)

denotes the tth subsequence in sequence s(i), where t ∈
{1, 2, · · · , T }. Thus, s(i) = {v(i)
T }. To
simplify the notation, we omit i in the following explanation.
Subsequence vt ∈ Rn×d is a matrix of medical codes such
that vt = {vt1 , vt2 , · · · , vtj , · · · , vtn }, where vtj ∈ Rd
is the vector representation of the jth medical code in the
tth subsequence vt and there are n medical codes in a
subsequence. In real EHR data, it is very likely that the
numbers of medical codes in each visit or time window are
different, thus, we utilize the padding approach to obtain a
consistent matrix dimensionality in the network.

To assign attention weights, we utilize the one-side con-
volution operation with a ﬁlter ωα ∈ Rd and a nonlinear
activation function. Thus, the weight vector αt is generated

6

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

for medical codes in the subsequence vt, presented in Equa-
tion 13.

αt = tanh(Conv(ωα, vt))

(13)

where αt = {αt1, αt2, · · · , αtn }, and ωα ∈ Rd is the
weight vector of the ﬁlter. The convolution operation Conv
is presented in Equation 14.

˜αtj = (ωα)(cid:124)vtj + bα

(14)

where bα is a bias term. Then, given the original matrix vt
and the learned weights αt, an aggregated vector xt ∈ Rd is
constructed to represent the tth subsequence, presented in 15.

xt =

αtj vtj

n
(cid:88)

j=1

(15)

Given Equation 15, we obtain a sequence of vectors, x =
{x1, x2, · · · , xt, · · · , xT }, to represent a patient’s medical
history.

C. LEARNING SUBSEQUENCE-LEVEL
SELF-ATTENTION
Given a sequence of embedded subsequences, this step em-
ploys the subsequence-level attention which allows the net-
work itself to learn the weights of subsequences according to
their contribution to the prediction target.

To capture the longitudinal dependencies, we utilize a

bidirectional GRU-based RNN, presented in Equations 16.

h1, · · · , ht, · · · , hT = GRU (x1, · · · , xt, · · · , xT )

(16)

where ht ∈ Rk represents the output by the GRU unit at
the tth subsequence. Then, we introduce a set of linear and
softmax layers to generate M hops of weights β ∈ RM ×T
for subsequences. Then, for the hop m

γmt = (wβ

m)(cid:124)ht + bβ

βm1 , · · · , βmT =

(17)

(18)

sof tmax(γm1 , · · · , γmt, · · · , γmT )

m ∈ Rk. Thus, with the subsequence-level weights
where wβ
and hidden outputs, we construct a vector cm ∈ Rk to repre-
sent a patient’s medical visit history with one hop of subse-
quence weights, presented in the following Equation 19.

cm =

βmtht

(19)

T
(cid:88)

t=1

Then, a context vector c ∈ RM ×k is constructed by

concatenating c1, c2, · · · , cM .

VOLUME 0, 2018

D. CONSTRUCTING AGGREGATED DEEP
REPRESENTATION
Given the context vector c, this step integrates the patients
characteristics a ∈ Rq into the context vector for a com-
plete vector representation of the patient’s EHR data. In
this research, the patient characteristics include demographic
information and some static medical conditions, such as age,
gender, and previous hospitalization. Thus, an aggregated
vector is constructed, c(cid:48) ∈ RM ×k+q, by adding a as addi-
tional dimensions to the context vector c.

E. PREDICTING OUTCOME
Given the vector representation of the complete medical
history and characteristics of patients, c(cid:48), we add a linear and
a softmax layer for the ﬁnal outcome prediction, as presented
in Equation 20.

ˆy = sof tmax(wc(cid:124)

c(cid:48) + bc)

(20)

To train the network, we use cross-entropy as the loss

function, presented in Equation 21.

L = −

yilog(ˆyi) + (1 − yi)log(1 − ˆyi)

(21)

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

+

||ββ(cid:124) − I||2
F

where N is the total number of observations. Here, yi is
a binary variable in classiﬁcation problems, while model
output ˆyi is real-valued. The second term in Equation 21 is
to penalize redundancy if the attention mechanism provides
similar subsequence weights for different hops of attention,
which is derived from [32]. This penalty term encourages the
multiple hops to focus on diverse areas and each hop focuses
on a small area.

Thus, we obtain a ﬁnal output for the prediction of out-
comes and a complete personalized vector representation of
the patient’s longitudinal EHR data.

V. EVALUATION
A. BACKGROUND
Although health care spending has been a relatively stable
share of the Gross Domestic Product (GDP) in the United
States since 2009, the costs of hospitalization, the largest
single component of health care expenditures,
increased
by 4.1% in 2014 [33]. Unplanned hospitalization is also
distressing and can increase the risk of related adverse events,
such as hospital-acquired infections and falls [34], [35].
Approximately 40% hospitalizations in the United King-
dom are unplanned and are potentially avoidable [36]. One
important form of unplanned hospitalization is hospital re-
admissions within 30 days of discharge, which is ﬁnancially
penalized in the United States. Early interventions targeted to
patients at risk of hospitalization could help avoid unplanned
admissions, reduce inpatient health care cost and ﬁnancial

7

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 5. A graphical illustration of the experimental setting for the risk
prediction of hospitalization

penalties for providers, and reduce emergency department
congestion [37].

In this research, we apply our proposed representation
learning framework to the risk prediction of future hospi-
talization. Many studies have been conducted by researchers
to predict the risk of 30-day readmission, or the admission
risk of a particular population, such as patients with Am-
bulatory Care Sensitive Conditions (ACSCs), patients with
heart failure, etc. [38]–[41]. Here, we focus on the general
population and the objective is to predict the risk of all-cause
hospitalization using longitudinal EHR data.

B. EXPERIMENTAL DESIGN
In this research, we use de-identiﬁed EHR data from the
University of Virginia Health System covering 75 months be-
ginning in September 2010. This dataset contains 2,343,651
inpatient and outpatient visits of 473,915 distinct patients.
We extracted visit data with diagnosis, medication, and pro-
cedure codes.

We deﬁned the observation window and prediction period
to validate the proposed method. We ﬁrst extract all patients
with a medical record of at least 1.5 years, where the ﬁrst year
is the observation window and the medical records in this
time window are used for feature construction. The follow-
ing 6 months is the hold-off period for the purpose of early
detection. For the positive class, we take all patients who
have hospitalization after the ﬁrst 1.5 years in their medical
history, while the negative class consists of patients who have
no hospitalization after 1.5 years. To better illustrate the ex-
perimental setting, we present the observation window, hold-
off and onset of outcome event in Figure 5. Here, the medical
codes include diagnosis, medication, and procedure codes,
and a vector representation is learned for each code. In this
dataset, diagnoses are primarily coded in ICD-9 and a small
portion is ICD-10 codes, while procedures are mainly using
CPT codes with a few ICD-9 procedure codes. The codes of
medications are using the pharmaceutical categories. Overall,
there are 94 distinct medication categories, 34,419 distinct
diagnoses codes, and 7,895 distinct procedure codes in the
EHR data. The dimension of the learned vectors of medical
codes is set to 100. Medical codes that appear in less than 50
patients medical records are excluded as rare events.

To construct the subsequences of medical codes, we use l
days as the time window. Figure 6 presents the cumulative
histogram and density plot of the numbers of visits in the
observation window, and we observe that the majority of

FIGURE 6. The cumulative histogram and density plot of patients’ numbers of
visits

patients have a small number of visits during the observation
window (less than 25% of patients have more than 4 visits).
Thus, we set l to 90 days, which split the observation window
into 4 subsequences.

Within each subsequence, the number of distinct medical
codes were computed and patients with more medical codes
in a subsequence than the 95% quantile were excluded from
the dataset. Overall, there are 8,841 and 89,101 patients in
the target and control groups, respectively. Each group is
randomly split into training, validation and testing sets with
a 7:1:2 ratio. Thus, 70% are used for training, another 20%
is used for testing, and the rest 10% are used for parameter
tuning and early stopping. The stochastic gradient descent
algorithm is used in training to minimize the cross-entropy
loss function, shown in Equation 21.

To evaluate the proposed representation learning frame-
work, we compare the prediction performance of the pro-
posed model with baseline approaches as follows.

1) Logistic regression (LR)

The inputs are the aggregated counts of grouped medical
codes over the entire observation window. Since the di-
mensionality of raw medical codes is huge, AHRQ clini-
cal classiﬁcations of diagnoses and procedures are used to
achieve a more general clustering of medical codes [42].
The medication codes are the pharmaceutical classes. Fur-
thermore, patient characteristics and previous inpatient visit
are also considered, where age and gender are demographic
information, and a binary indicator is utilized to represent
the presence of the previous hospitalization. Hence, the input
is a 436-dimensional vector representing a patient’s medical
history and characteristics.

8

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 1. The predictive performance of baselines and the proposed Patient2Vec framework

Methods
LR
MLP
RETAIN
FRNN-MGE
BiRNN-MGE
FRNN-MVE
BiRNN-MVE
Patient2Vec

Sensitivity
0.637 ± 0.010
0.727 ± 0.013
0.553 ± 0.012
0.636 ± 0.012
0.600 ± 0.012
0.753 ± 0.011
0.724 ± 0.010
0.769 ± 0.010

Speciﬁcity
0.728 ± 0.003
0.617 ± 0.004
0.710 ± 0.003
0.739 ± 0.004
0.777 ± 0.003
0.676 ± 0.004
0.707 ± 0.003
0.694 ± 0.004

AUC
0.721 ± 0.006
0.713 ± 0.007
0.663 ± 0.007
0.759 ± 0.006
0.768 ± 0.007
0.785 ± 0.006
0.788 ± 0.005
0.799 ± 0.005

F2 score
0.434 ± 0.006
0.423 ± 0.007
0.370 ± 0.008
0.438 ± 0.009
0.439 ± 0.009
0.470 ± 0.008
0.473 ± 0.008
0.492 ± 0.007

2) Multi-layer perceptron (MLP)
A multi-layer perceptron is trained to predict hospitalization
using the same inputs for logistic regression. Here, we use a
one hidden layer MLP with 256 hidden nodes.

3) Forward RNN with medical group embedding
(FRNN-MGE)
We split the sequence into subsequences with equal interval l.
The input at each step is the counts of medical groups within
the associated time interval, and the patient characteristics are
appended as additional features in the ﬁnal logistic regression
step. Here, the RNN is a forward GRU (or LSTM [18]) with
one hidden layer and the size of the hidden layer is 256.

4) Bidirectional RNN with medical group embedding
(BiRNN-MGE)
The inputs used for this baseline is the same as the one for
the FRNN-MGE [15]. The RNN used here is a bidirectional
GRU with one hidden layer and the size of the hidden layer
is 256.

5) Forward RNN with medical vector embedding
(FRNN-MVE)
We split the sequence into subsequences with equal interval l.
The input at each step is the vector representation of the
medical codes within the associated time interval, and the
patient characteristics are appended as additional features in
the ﬁnal logistic regression step. Here, the RNN is a forward
GRU (or LSTM [28]) with one hidden layer and the size of
the hidden layer is 256.

6) Bidirectional RNN with medical vector embedding
(BiRNN-MVE)
The inputs used for this baseline is the same as the one for
the FRNN-MVE [25]. The RNN used here is a bidirectional
GRU or LSTM [15] with one hidden layer and the size of the
hidden layer is 256.

7) RETAIN
This model uses reverse time attention mechanism on RNNs
for an interpretable representation of patient’s EHR data [31].
The inputs are the same as the one for FRNN-MGE, which
takes the counts of medical grouping within each time inter-
val to construct features. Similarly, the two RNNs used for

generating weights are GRU-based and the size of the hidden
layers are 256.

8) Patient2Vec
The inputs are the same as that for FRNN-MVE. One ﬁlter
is used when generating weights for within-subsequence
attention, and three ﬁlters are used for subsequence-level
attention. Similarly, the RNN used here is GRU-based and
there is one hidden layer and the size of the hidden layer
is 256.

The inputs of all baselines and Patient2Vec are normal-
ized to have zero mean and unit variance. We model the
risk of hospitalization based on Patient2Vec and baseline
representations of patients’ medical histories, and the model
performance is evaluated with Area Under Curve(AUC),
sensitivity, speciﬁcity, and F2-score. The validation set is
used for parameter tuning and early stopping in the train-
ing process. Each experiment is repeated 20 times and we
calculate the averages and standard deviations of the above
metrics, respectively.

C. EXPERIMENTAL RESULTS
The predictive performance of Patient2Vec and baselines are
presented in Table 1. The results shown here for the RNN-
based models are based on time interval l = 90 days to
construct subsequences.

According to Table 1, the RNN-based models are generally
capable of achieving higher prediction performance in terms
of sensitivity, AUC and F2 score, except for the RNN models
based on medical group embedding which have lower sensi-
tivity. Among all RNN-based approaches, the ones based on
vector embedding outperform those based on medical group
embedding in terms of sensitivity, AUC, and F2 score. The
bidirectional RNN models generally have higher speciﬁcity
but lower sensitivity than the forward RNN models, while
the bidirectional ones have comparable AUC and F2 score
with the forward ones, respectively. Generally, the proposed
Patient2Vec framework outperforms the baseline methods,
especially in terms of sensitivity and F2 score.

D. VISUALIZATION & INTERPRETATION
In addition to predictive performance, we interpret
the
learned representation by understanding the relative impor-
tance of clinical events in a patient’s EHR data. Considering

VOLUME 0, 2018

9

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

FIGURE 7. The heat map showing feature importance for Patient A

FIGURE 8. The proﬁle of Patient A.

FIGURE 9. The proﬁle of Patient B.

the feature importance learned by Patient2Vec are person-
alized for an individual patient, we illustrate it with two
example patients. Figures 8 and 9 present the proﬁles of
two individuals, Patient A and Patient B, respectively. To
facilitate the interpretation, instead of using raw medical
codes, we present the clinical groups from the AHRQ clinical
classiﬁcation software on diagnoses and procedure codes, as
well as pharmaceutical groups for medications.

According to Figure 8, Patient A is a male patient who
has hospitalization history in the observation window and
is admitted to the hospital seven months after the end of
the observation window for congestive heart failure. The

predicted risk is 96.4%, while the risk decreases for female
patients or patients without hospitalization history. It is also
not surprising to observe an increased risk for older patients.
The heat map in Figure 7 shows the relative importance of
the medical events in this patient’s medical record at each
time window and the ﬁrst row of the heat map presents
the subsequence-level attention. The darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the last subsequence,
t4, is the most important with respect to hospitalization risk,
followed by t1, t2, and t3 in order of importance.

Among all the clinical events in the subsequence t4, we

10

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 2. The top clinical groups with high weights in hospitalized patients

Index

Clinical Groups

Diagnoses
1
2
3

Essential hypertension
Other connective tissue disease
Spondylosis; intervertebral disc disorders; other back
problems
Other lower respiratory disease
Disorders of lipid metabolism
Other aftercare
Diabetes mellitus without complication
Screening and history of mental health and substance
abuse codes
Other nervous system disorders
Other screening for suspected conditions (not mental
disorders or infectious disease)

Other OR therapeutic procedures on nose; mouth and
pharynx
Suture of skin and subcutaneous tissue
Other therapeutic procedures on eyelids; conjunctiva;
cornea

Laboratory - Chemistry and hematology
Other laboratory
Other OR therapeutic procedures of urinary tract
Other OR procedures on vessels other than head and
neck

Therapeutic radiology for cancer treatment

Procedures
1

9
10

4
5
6
7
8

2
3

4
5
6
7

8

2

Medications
1

Diagnostic Products

Analgesics-Narcotic

observe that the OR therapeutic procedures (nose, mouth,
and pharynx), laboratory (chemistry and hematology), coro-
nary atherosclerosis & other heart disease, cardiac dys-
rhythmias, and conduction disorders are the ones with the
highest weights, while other events such as other connected
tissue disease are less important in terms of future hospi-
talization risk. Additionally, some medications appear to be
informative as well, including beta blockers, antihyperten-
sives, anticonvulsants, anticoagulants, etc. In the ﬁrst-time
window, the medical events with high weights are coro-
nary atherosclerosis & other heart disease, gastrointestinal
hemorrhage, deﬁciency and anemia, and other aftercare. In
the next subsequence, the most important medical events
are heart diseases and related procedures such as coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
conduction disorders, hypertension with complications, other
OR heart procedures, and other OR therapeutic nervous
system procedures. We also observe that the kidney disease
related diagnoses and procedures appear to be important
features. Throughout the observation window, the coronary
atherosclerosis & other heart disease, cardiac dysrhythmias,
and conduction disorders constantly show high weights with
respect to hospitalization risk, and the ﬁndings are consistent

FIGURE 10. The heat map showing feature importance for Patient B

with medical literature.

Figure 9 presents the proﬁle of Patient B, which is a male
patient without hospitalization in the observation window.
This patient is hospitalized for occlusion of cerebral arter-
ies approximately one year after the observation window,
and the predicted risk is 74.6%. For a similar patient who
is 10 years older or with previous hospitalization history,
the risk increases by 4.2% and 1%, respectively, while there
is a smaller risk of hospitalization for a female patient. To
illustrate the medical events of Patient B, the heat map in
Figure 10 depicts the relative importance of medical groups
in the subsequences, as well as the subsequence-level weights
for hospitalization risk. Similarly, the darker color indicates
a stronger correlation between the clinical events and the
outcome. Accordingly, we observe that the second subse-
quence appears to be the most important, while the last one is
less predictive of future hospitalization. In fact, the medical
events in the last time window are spondylosis, intervertebral
disc disorders, other back problems and other bone disease &
musculoskeletal deformities, and malaise and fatigue, which
are not highly related to the cause of hospitalization of Patient
B.

In the most predictive subsequence, t2, we observe that
other OR heart procedures, genitourinary symptoms, spondy-
losis, intervertebral disc disorders, other back problems,
therapeutic procedures on eyelid, conjunctiva, and cornea,
and arterial blood gases have high attention weights. In the

VOLUME 0, 2018

11

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

TABLE 3. The top diagnosis groups with high weights in patients hospitalized
for osteoarthritis, septicemia, acute myocardial infarction, congestive heart
failure, and diabetes mellitus with complications, respectively

Index

Diagnosis Groups

In patients admitted for osteoarthritis

Osteoarthritis
Other connective tissue disease

1
2

3
4

5

1
2

3
4
5

1
2
3

4
5

1
2
3
4
5

1
2
3

4
5

Other non-traumatic joint disorders
Spondylosis; intervertebral disc disorders; other back prob-
lems
Other aftercare

In patients admitted for septicemia

Essential hypertension
Diabetes mellitus without complication

Disorders of lipid metabolism
Other lower respiratory disease
Other aftercare

In patients admitted for acute myocardial infarction

Coronary atherosclerosis and other heart disease
Medical examination/evaluation
Other screening for suspected conditions (not mental disorders
or infectious disease)

Other lower respiratory disease
Disorders of lipid metabolism

In patients admitted for congestive heart failure

Congestive heart failure (nonhypertensive)
Coronary atherosclerosis and other heart disease
Cardiac dysrhythmias
Diabetes mellitus without complication
Other lower respiratory disease

In patients admitted for diabetes mellitus with complications

Diabetes mellitus with complications
Diabetes mellitus without complication
Other aftercare

Other nutritional; endocrine; and metabolic disorders
Fluid and electrolyte disorders

earliest time window, the most important medical events also
include therapeutic procedures on eyelid, conjunctiva, and
cornea, arterial blood gases, while diabetes, hypertension
as well as diagnostic products show their relatively high
importance. Throughout the observation window, medical
events spondylosis, intervertebral disc disorders, other back
problems, therapeutic procedures on eyelid, conjunctiva, and
cornea are constantly with high attention weights. Here, di-
agnostic products is a medication class, which include barium
sulfate, iohexol, gadopentetate dimeglumine, iodixanol, tu-
berculin puriﬁed protein derivative, iodixanol, regadenoson,
acetone (urine), and so forth. These medications are primarily
for blood or urine testing, or used as radiopaque contrast
agents for x-rays or CT scans for diagnostic purposes.

Additionally, we attempt to interpret the learned repre-
sentation and feature importance at the population-level. In
Table 2, we present the top 20 clinical groups with high
weights among hospitalized patients in the test set.

According to Table 2, the most predictive diagnosis groups
for future hospitalization are chronic diseases, including
essential hypertension, diabetes, lower respiratory disease,
disorders of lipid metabolism, and musculoskeletal diseases
such as other connective tissue disease and spondylosis,
intervertebral disc disorders, other back problems. The most
important procedures are some OR therapeutic procedures
and laboratory tests, such as the OR procedures on nose,
mouth, and pharynx, vessels, urinary tract, eyelid, conjunc-
tiva, cornea, etc. It is not surprising to see that diagnostic
products are showing with high weights, considering these
medications are used in testing or examinations for diagnos-
tic purposes.

Moreover, we present the top diagnoses groups with high
weights in patients hospitalized for different primary causes.
Table 3 shows the top 5 diagnosis groups with high weights
in patients admitted for osteoarthritis, septicemia (except in
labor), acute myocardial infarction, congestive heart failure
(nonhypertensive), and diabetes mellitus with complications,
respectively. Accordingly, we observe that the most impor-
tant diagnoses for hospitalization risk prediction in popu-
lation admitted for osteoarthritis are musculoskeletal dis-
eases such as connective tissue disease, joint disorders, and
spondylosis. However, the diagnoses with highest weights
in the patients admitted for septicemia are chronic diseases
including essential hypertension, diabetes, disorders of lipid
metabolism, and respiratory disease. The top diagnoses have
many overlaps between the populations admitted for acute
myocardial infarction and for congestive heart failure, con-
sidering both populations are admitted for heart diseases.
the overlapped diagnosis groups include coronary
Here,
atherosclerosis and other heart diseases and lower respiratory
diseases. As for patients admitted for diabetes with com-
plications, the top diagnoses are diabetes with or without
complications, nutritional, endocrine, metabolic disorders,
and ﬂuid and electrolyte disorders. In general, the learned
feature importance is consistent with medical literature.

VI. DISCUSSION
Our proposed framework is applied to the prediction of
hospitalization using real EHR data that demonstrates its
prediction accuracy and interpretability. This work could be
further enhanced by incorporating the follow-up information
on the negative patient population and investigate if it in-
deed shows an improved health outcome or the patient is
hospitalized elsewhere. Patient2Vec employs a hierarchical
attention mechanism, allowing us to directly interpret the
weights of clinical events. In future work, we will extend the
attention to incorporate demographic information for a more
comprehensive and automatic interpretation.

Although we apply Patient2Vec to the early detection of
long-term hospitalization, i.e., at least 6 months after the
previous hospitalization, it could be used to predict the risk
of 30-day readmission to help prevent unnecessary rehospi-
talizations.

12

VOLUME 0, 2018

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

VII. CONCLUSION
In this paper, we propose a representation learning frame-
work, Patient2Vec, to learn a personalized interpretable deep
representation of EHR data based on recurrent neural net-
works and the attention mechanism. This work improves
the performance of predictive models as well as deepens
the understanding of disease correlations. We apply this
framework to the risk prediction of hospitalization using
patients’ longitudinal EHR data. The experimental results
demonstrate that the proposed Patient2Vec representation is
capable of achieving a more accurate prediction than base-
lines approaches. Moreover, the learned feature importance
in the representations are interpreted both at the individual
and population levels to facilitate clinical insights.

In this work, the proposed Patient2Vec framework is eval-
uated with the risk prediction of all-cause hospitalization,
but in the future could be applied to predict hospitalization
in more speciﬁc populations, other health related prediction
problems, or domains outside of health.

REFERENCES
[1] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical
attention networks for document classiﬁcation,” in Proceedings of the
2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, 2016, pp.
1480–1489.

[2] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,” arXiv

preprint arXiv:1408.5882, 2014.

[3] J. Howard and S. Ruder, “Fine-tuned language models for text classiﬁca-

tion,” arXiv preprint arXiv:1801.06146, 2018.

[4] M. M. Lopez and J. Kalita, “Deep learning applied to nlp,” arXiv preprint

arXiv:1703.03091, 2017.

[5] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using
rnn encoder-decoder for statistical machine translation,” arXiv preprint
arXiv:1406.1078, 2014.

[6] A. L. Nobles, J. J. Glenn, K. Kowsari, B. A. Teachman, and L. E. Barnes,
“Identiﬁcation of imminent suicide risk among young adults using text
messages,” in Proceedings of the 2018 CHI Conference on Human Factors
in Computing Systems. ACM, 2018, p. 413.

[7] K. Kowsari, D. E. Brown, M. Heidarysafa, K. J. Meimandi, M. S. Gerber,
and L. E. Barnes, “Hdltex: Hierarchical deep learning for text classiﬁca-
tion,” in 2017 16th IEEE International Conference on Machine Learning
and Applications (ICMLA), Dec 2017, pp. 364–371.

[8] K. Kowsari, M. Heidarysafa, D. E. Brown, K. Jafari Meimandi, and L. E.
Barnes, “Rmdl: Random multimodel deep learning for classiﬁcation.”
ACM, 2018.

[9] H. Strobelt, S. Gehrmann, H. Pﬁster, and A. M. Rush, “Lstmvis: A tool
for visual analysis of hidden state dynamics in recurrent neural networks,”
IEEE transactions on visualization and computer graphics, vol. 24, no. 1,
pp. 667–676, 2018.

[10] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” Scientiﬁc
reports, vol. 8, no. 1, p. 6085, 2018.

[11] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches
preprint

neural machine

translation,”

arXiv

attention-based
to
arXiv:1508.04025, 2015.

[12] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[13] D. Yogatama, C. Dyer, W. Ling, and P. Blunsom, “Generative and discrim-
inative text classiﬁcation with recurrent neural networks,” arXiv preprint
arXiv:1703.01898, 2017.

[14] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent

trends
language processing,” arXiv preprint

in deep learning based natural
arXiv:1708.02709, 2017.

[15] M. Basaldella, E. Antolli, G. Serra, and C. Tasso, “Bidirectional lstm
recurrent neural network for keyphrase extraction,” in Italian Research

Conference on Digital Libraries, Springer.
lishing, 2018, pp. 180–187.

Springer International Pub-

[16] S. Ghosh, O. Vinyals, B. Strope, S. Roy, T. Dean, and L. Heck, “Con-
textual lstm (clstm) models for large scale nlp tasks,” arXiv preprint
arXiv:1602.06291, 2016.

[17] B. Yue, J. Fu, and J. Liang, “Residual recurrent neural networks for
learning sequential representations,” Information, vol. 9, no. 3, p. 56, 2018.
[18] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
gated recurrent neural networks on sequence modeling,” arXiv preprint
arXiv:1412.3555, 2014.

[19] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difﬁculty of training
recurrent neural networks.” ICML (3), vol. 28, pp. 1310–1318, 2013.
[20] D. Britz, “Recurrent neural network tutorial,” http://www.wildml.com/

2015/10/, 2015, [Accessed on October 5, 2017].

[21] V. Mnih, N. Heess, A. Graves et al., “Recurrent models of visual attention,”
in Advances in neural information processing systems, 2014, pp. 2204–
2212.

[22] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for
abstractive sentence summarization,” arXiv preprint arXiv:1509.00685,
2015.

[23] T. Ma, C. Xiao, and F. Wang, “Health-atm: A deep architecture for
multifaceted patient health record representation and risk prediction,” in
Proceedings of the 2018 SIAM International Conference on Data Mining.
SIAM, 2018, pp. 261–269.

[24] A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J. Liu,
X. Liu, J. Marcus, M. Sun et al., “Scalable and accurate deep learning with
electronic health records,” npj Digital Medicine, vol. 1, no. 1, p. 18, 2018.
[25] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzell, “Learning to diagnose
with lstm recurrent neural networks,” arXiv preprint arXiv:1511.03677,
2015.

[26] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent neural
networks for multivariate time series with missing values,” arXiv preprint
arXiv:1606.01865, 2016.

[27] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghas-
semi, B. Moody, P. Szolovits, L. A. Celi, and R. G. Mark, “Mimic-iii, a
freely accessible critical care database,” Scientiﬁc data, vol. 3, 2016.
[28] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart, and J. Sun, “Doctor
ai: Predicting clinical events via recurrent neural networks,” in Machine
Learning for Healthcare Conference, 2016, pp. 301–318.

[29] Z. Che, S. Purushotham, R. Khemani, and Y. Liu, “Interpretable deep
models for icu outcome prediction,” in AMIA Annual Symposium Pro-
ceedings, vol. 2016. American Medical Informatics Association, 2016,
p. 371.

[30] J. H. Friedman, “Greedy function approximation: a gradient boosting

machine,” Annals of statistics, pp. 1189–1232, 2001.

[31] E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart,
“Retain: An interpretable predictive model for healthcare using reverse
time attention mechanism,” in Advances in Neural Information Processing
Systems, 2016, pp. 3504–3512.

[32] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Ben-
gio, “A structured self-attentive sentence embedding,” arXiv preprint
arXiv:1703.03130, 2017.

[33] C. M. Torio and B. J. Moore, “National inpatient hospital costs: The most
expensive conditions by payer, 2013,” https://www.hcup-us.ahrq.gov/
reports/statbriefs/sb204-Most-Expensive-Hospital-Conditions.jsp, 2016.
[34] E. Wallace, E. Stuart, N. Vaughan, K. Bennett, T. Fahey, and S. M.
Smith, “Risk prediction models to predict emergency hospital admission
in community-dwelling adults: a systematic review,” Medical care, vol. 52,
no. 8, p. 751, 2014.

[35] E. N. de Vries, M. A. Ramrattan, S. M. Smorenburg, D. J. Gouma, and
M. A. Boermeester, “The incidence and nature of in-hospital adverse
events: a systematic review,” Quality and safety in health care, vol. 17,
no. 3, pp. 216–223, 2008.

[36] S. Purdey and A. Huntley, “Predicting and preventing avoidable hospital
admissions: a review.” The journal of the Royal College of Physicians of
Edinburgh, vol. 43, no. 4, pp. 340–344, 2012.

[37] H. Ontario, “Early identiﬁcation of people at risk of hospitalization: Hos-
pital admission risk prediction (harp)-a new tool for supporting providers
and patients,” 2013.

[38] B. Zheng, J. Zhang, S. W. Yoon, S. S. Lam, M. Khasawneh, and S. Poranki,
“Predictive modeling of hospital readmissions using metaheuristics and
data mining,” Expert Systems with Applications, vol. 42, no. 20, pp. 7110–
7120, 2015.

VOLUME 0, 2018

13

Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record

[39] D. Kansagara, H. Englander, A. Salanitro, D. Kagen, C. Theobald,
M. Freeman, and S. Kripalani, “Risk prediction models for hospital
readmission: a systematic review,” The Journal of the American Medical
Association, vol. 306, no. 15, pp. 1688–1698, 2011.

[40] G. Giamouzis, A. Kalogeropoulos, V. Georgiopoulou, S. Laskar, A. L.
Smith, S. Dunbar, F. Triposkiadis, and J. Butler, “Hospitalization epidemic
in patients with heart failure: risk factors, risk prediction, knowledge gaps,
and future directions,” Journal of cardiac failure, vol. 17, no. 1, pp. 54–75,
2011.

[41] E. Prescott, A. M. Bjerg, P. K. Andersen, P. Lange, and J. Vestbo, “Gender
difference in smoking effects on lung function and risk of hospitalization
for COPD: results from a danish longitudinal population study,” European
Respiratory Journal, vol. 10, no. 4, pp. 822–827, 1997.

[42] Agency for Healthcare Research and Quality (AHRQ), “Clinical classi-
ﬁcations software (CCS) for ICD-9-CM,” https://www.hcup-us.ahrq.gov/
toolssoftware/ccs/ccs.jsp, 2015.

JENNIFER M. LOBO (jem4yb@virginia.edu) is
an Assistant Professor of Biomedical Informatics
in the Department of Public Health Sciences at
University of Virginia. She received her Ph.D. in
Industrial Engineering from North Carolina State
University, Raleigh, NC. Her research interests in-
volve using mathematical modeling and stochastic
optimization methods to build models that simu-
late the natural course of disease. These models
allow for estimation of outcomes under different
screening and treatment policies in the absence of randomized controlled
trials, and can be used to optimize screening and treatment decisions for
patients with chronic diseases. Her projects include optimizing treatment
for patients with type 2 diabetes, generating individualized decision analysis
models for prostate cancer patients, and developing optimal imaging surveil-
lance guidelines for recurrent kidney cancer.

JINGHE ZHANG (jz4kg@virginia.edu) is a lead
data scientist at Target Corporation. She received
her Ph.D. in Systems Engineering from the Uni-
versity of Virginia. Prior to entering the Ph.D. pro-
gram at UVA, she received the Master of Science
in Industrial and Systems Engineering from the
State University of New York at Binghamton. Her
research interests are in natural language process-
ing, machine learning, recommender systems, and
health informatics.

LAURA E. BARNES (lb3dp@virginia.edu) is an
Associate Professor in Systems and Information
Engineering and the Data Science Institute at the
University of Virginia. She received her Ph.D. in
Computer Science from the University of South
Florida, Tampa, FL. She directs the Sensing Sys-
tems for Health (S2He) Lab which focuses on
understanding the dynamics and personalization
of health and well-being through mobile sensing
and analytics.

(kk7nc@virginia.edu) is
KAMRAN KOWSARI
a Ph.D. student in the Department of Systems
and Information Engineering at the University of
Virginia, Charlottesville, VA. He is a member of
the Sensing Systems for Health Lab. He received
his Master of Science from Department of Com-
puter Science at The George Washington Univer-
sity, Washington, DC. He has more than ten years
of experience in machine learning and software
development. His experience includes numerous
industrial and academic projects. His research interests include natural
language processing, machine learning, deep learning, artiﬁcial intelligence,
text mining, and unsupervised learning.

JAMES H. HARRISON, JR., (jhh5y@virginia.
edu) is Associate Professor of Pathology and Di-
rector of Laboratory Information Systems at the
University of Virginia Medical Center and also has
appointments in the Departments of Public Health
Sciences in the UVA School of Medicine, and
Systems and Information Engineering in the UVA
School of Engineering and Applied Sciences. He
received his MD and PhD (Pharmacology) de-
grees from Medical University of South Carolina,
Charleston, SC, completed residencies in Anatomic Pathology and Labora-
tory Medicine at Yale-New Haven Hospital, New Haven, CT, and completed
a postdoctoral fellowship in Environmental Toxicology at Yale University,
New Haven, CT. Dr. Harrison has over 25 years of experience in the ﬁeld
of medical informatics, including work in clinical laboratory information
systems, electronic health records, clinical data analysis, and clinical data
standards development.

14

VOLUME 0, 2018

