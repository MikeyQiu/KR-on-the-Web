Capturing Long-range Contextual Dependencies with
Memory-enhanced Conditional Random Fields

Fei Liu

Timothy Baldwin

Trevor Cohn

School of Computing and Information Systems
The University of Melbourne
Victoria, Australia
fliu3@student.unimelb.edu.au
tb@ldwin.net t.cohn@unimelb.edu.au

7
1
0
2
 
t
c
O
 
2
1
 
 
]
L
C
.
s
c
[
 
 
2
v
7
3
6
3
0
.
9
0
7
1
:
v
i
X
r
a

Abstract

Despite successful applications across a
broad range of NLP tasks, conditional ran-
in particular the
dom ﬁelds (“CRFs”),
linear-chain variant, are only able to model
local features. While this has important
beneﬁts in terms of inference tractabil-
ity, it limits the ability of the model to
capture long-range dependencies between
items. Attempts to extend CRFs to cap-
ture long-range dependencies have largely
come at the cost of computational com-
plexity and approximate inference. In this
work, we propose an extension to CRFs
by integrating external memory, taking in-
spiration from memory networks, thereby
allowing CRFs to incorporate informa-
tion far beyond neighbouring steps. Ex-
periments across two tasks show substan-
tial improvements over strong CRF and
LSTM baselines.

1 Introduction

While long-range contextual dependencies are
prevalent in natural language, for tractability rea-
sons, most statistical models capture only local
features (Finkel et al., 2005). Take the sentence in
Figure 1, for example. Here, while it is easy to
determine that Interfax in the second sentence is a
named entity, it is hard to determine its semantic
class, as there is little context information. The us-
age in the ﬁrst sentence, on the other hand, can be
reliably disambiguated due to the post-modifying
phase news agency. Ideally we would like to be
able to share such contexts across all usages (and
variants) of a given named entity for reliable and
consistent identiﬁcation and disambiguation.

A related example is forum thread discourse
analysis. Previous work has largely focused on

linear-chain Conditional Random Fields (CRFs)
(Wang et al., 2011; Zhang et al., 2017), framing
the task as one of sequence tagging. Although
CRFs are adept at capturing local structure, the
problem does not naturally suit a linear sequen-
tial structure, i.e. , a post may be a reply to ei-
ther a neighbouring post or one posted far ear-
In both cases, con-
lier within the same thread.
textual dependencies can be long-range, neces-
sitating the ability to capture dependencies be-
tween arbitrarily distant items.
Identifying this
key limitation, Sutton and McCallum (2004) and
Finkel et al. (2005) proposed the use of CRFs with
skip connections to incorporate long-range depen-
dencies.
In both cases the graph structure must
be supplied a priori, rather than learned, and both
techniques incur the need for costly approximate
inference.

Recurrent neural networks (RNNs) have been
proposed as an alternative technique for encoding
sequential inputs, however plain RNNs are unable
to capture long-range dependencies (Bengio et al.,
1994; Hochreiter et al., 2001) and variants such
as LSTMs (Hochreiter and Schmidhuber, 1997),
although more capabable of capturing non-local
patterns, still exhibit a signiﬁcant locality bias in
practice (Lai et al., 2015; Linzen et al., 2016).

In this paper, taking inspiration from the work
of Weston et al.
(2015) on memory networks
(MEMNETs), we propose to extend CRFs by in-
tegrating external memory mechanisms, thereby
enabling the model to look beyond localised fea-
tures and have access to the entire sequence.
This is achieved with attention over every en-
try in the memory. Experiments on named en-
tity recognition and forum thread parsing, both
of which involve long-range contextual depen-
dencies, demonstrate the effectiveness of the pro-
posed model, achieving state-of-the-art perfor-
mance on the former, and outperforming a num-

· · ·

B-ORG

O

O

O

· · ·

B-ORG

O

B-MISC

O

· · ·

Interfax

news

agency

said

Interfax

quoted

Russian

military

Figure 1: A NER example with long-range contextual dependencies. The vertical dash line indicates a
sentence boundary.

ber of strong baselines in the case of the latter. A
full implementation of the model is available at:
https://github.com/liufly/mecrf.

The paper is organised as follows: after review-
ing previous studies on capturing long range con-
textual dependencies and related models in Sec-
tion 2, we detail the elements of the proposed
model in Section 3. Section 4 and 5 present the
experimental results on two different datasets: one
for thread discourse structure prediction and the
other named entity recognition (NER), with anal-
yses and visualisation in their respective sections.
Lastly, Section 6 concludes the paper.

2 Related Work

In this section, we review the different families of
models that are relevant to this work, in captur-
ing long-range contextual dependencies in differ-
ent ways.

Conditional Random Fields (CRFs). CRFs
(Lafferty et al., 2001), in particular linear-chain
CRFs, have been widely adopted and applied to
sequence labelling tasks in NLP, but have the crit-
ical limitation that they only capture local struc-
ture (Sutton and McCallum, 2004; Finkel et al.,
2005), despite non-local structure being common
in structured language classiﬁcation tasks.
In
the context of named entity recognition (“NER”),
Sutton and McCallum (2004) proposed skip-chain
CRFs as a means of alleviating this shortcom-
ing, wherein distant items are connected in a se-
quence based on a heuristic such as string identity
(to achieve label consistency across all instances
of the same string). The idea of label consis-
tency and exploiting non-local features has also
been explored in the work of Finkel et al. (2005),
who take long-range structure into account while
maintaining tractable inference with Gibbs sam-
pling (Geman and Geman, 1984), by performing
approximate inference over factored probabilistic
models. While both of these lines of work report
impressive results on information extraction tasks,

they come at the price of high computational cost
and incompatibility with exact inference.

Similar

ideas have also been explored by
Krishnan and Manning (2006) for NER, where
they apply two CRFs, the ﬁrst of which makes
predictions based on local information, and the
second combines named entities identiﬁed by the
ﬁrst CRF in a single cluster, thereby enforcing la-
bel consistency and enabling the use of a richer
set of features to capture non-local dependencies.
Liao and Grishman (2010) make a strong case for
going beyond sentence boundaries and leveraging
document-level information for event extraction.

While we take inspiration from these earlier
studies, we do not enforce label consistency as a
hard constraint, and additionally do not sacriﬁce
inference tractability: our model is capable of in-
corporating non-local features, and is compatible
with exact inference methods.

(RNNs). Re-
Recurrent Neural Networks
the broad adoption of deep learning
cently,
methods in NLP has given rise to the preva-
lent use of RNNs.
Long short-term mem-
ories (“LSTMs”: Hochreiter and Schmidhuber
(1997)), a particular variant of RNN, have be-
come particularly popular, and been success-
fully applied to a large number of tasks: speech
recognition (Graves et al., 2013), sequence tag-
ging (Huang et al., 2015), document categorisa-
tion (Yang et al., 2016), and machine translation
(Cho et al., 2014; Bahdanau et al., 2014). How-
ever, as pointed out by Lai et al. (2015) and
Linzen et al. (2016), RNNs — including LSTMs
— are biased towards immediately preceding (or
neighbouring, in the case of bi-directional RNNs)
items, and perform poorly in contexts which in-
volve long-range contextual dependencies, despite
the inclusion of memory cells. This is further
evidenced by the work of Cho et al. (2014), who
show that the performance of a basic encoder–
decoder deteriorates as the length of the input sen-
tence increases.

Memory networks
(MEMNETs). More re-
cently, Weston et al. (2015) proposed memory
networks and showed that the augmentation of
memory is crucial
to performing inference re-
quiring long-range dependencies, especially when
document-level reasoning between multiple sup-
porting facts is required. Of particular interest to
our work are so-called “memory hops” in memory
networks, which are guided by an attention mech-
anism based on the relevance between a ques-
tion and each supporting context sentence in the
memory hop. Governed by the attention mech-
anism, the ability to access the entire sequence
is similar to the soft alignment idea proposed by
Bahdanau et al. (2014) for neural machine trans-
In this work, we borrow the concept of
lation.
memory hops and integrate it into CRFs, thereby
enabling the model to look beyond localised fea-
tures and have access to the whole sequence via an
attention mechanism.

3 Methodology

In the context of sequential tagging, we assume
the input is in the form of sequence pairs: D =
n=1 where x(n) is the input of the
{x(n), y(n)}N
n-th example in dataset D and consists of a se-
quence: {x(n)
2 , . . . , x(n)
T }. Similarly, y(n) is
of the same length as x(n) and consists of the cor-
responding labels {y(n)
1 , y(n)
T }. For nota-
2
tional convenience, hereinafter we omit the super-
script denoting the n-th example.

, . . . , y(n)

1 , x(n)

In the case of NER, each xt is a word in a sen-
tence with yt being the corresponding NER label.
For forum thread discourse analysis, xt represents
the text of an entire post whereas yt is the dialogue
act label for the t-th post.

The proposed model extends CRFs by integrat-
ing external memory and is therefore named a
Memory-Enhanced Conditional Random Field
(“ME-CRF”). We take inspiration from Memory
Networks (“MEMNETs”: Weston et al. (2015))
and incorporate so-called memory hops into
CRFs, thereby allowing the model to have unre-
stricted access to the whole sequence rather than
localised features as in RNNs (Lai et al., 2015;
Linzen et al., 2016).

As illustrated in Figure 2, ME-CRF can be di-
vided into two major parts: (1) the memory layer;
and (2) the CRF layer. The memory layer can be
further broken down into three main components:
(a) the input memory m1:t; (b) the output mem-

ory c1:t; and (c) the current input ut, which rep-
resents the current step (also known as the “ques-
tion” in the context of MEMNET). The input and
output memory representations are connected via
an attention mechanism whose weights are deter-
mined by measuring the similarity between the in-
put memory and the current input. The CRF layer,
on the other hand, takes the output of the memory
layer as input. In the remainder of this section, we
detail the elements of ME-CRF.

3.1 Memory Layer

3.1.1 Input memory
Every element (word/post) in a sequence x is en-
coded with xt = Φ(xt), where Φ(·) can be any
encoding function mapping the input xt into a
vector xt ∈ Rd. This results in the sequence
{x1, . . . , xT }. While this new sequence can be
seen as the memory in the context of MEM-
NETs, one major drawback of this approach, as
pointed out by Seo et al. (2017), is the insensitiv-
ity to temporal information between memory cells.
We therefore follow Xiong et al. (2016) in inject-
ing temporal signal into the memory using a bi-
directional GRU encoding (Cho et al., 2014):

−→mt =
←−mt =
mt = tanh(

−−→
GRU(xt, −→mt−1)
←−−
GRU(xt, ←−mt+1)
−→
−→mt +
W m

←−
W m

←−mt + bm)

(1)

(2)

(3)

−→
W m,

←−
W m and bm are learnable parame-

where
ters.

3.1.2 Current input
This is used to represent the current step xt, be it
a word or a post. As in MEMNETs, we want to
enforce the current input to be in the same space
as the input memory so that we can determine the
attention weight of each element in the memory
by measuring the relevance between the two. We
denote the current input by ut = mt.

3.1.3 Attention
To determine the attention value of each element
in the memory, we measure the relevance between
the current step ut and mi for i ∈ [1, t] with a
softmax function:

pt,i = softmax(u

⊤

t mi)

(4)

where softmax(ai) =

eai
Pj eaj

.

CRF

layer

· · ·

ot
Weighted sum

c1:t

p1:t

m1:t

Memory

layer

softmax

ot+1
Weighted sum

c1:t+1

p1:t+1

softmax

m1:t+1

yt

Σ

O
u
t
p
u
t

A

t
t
e
n
t
i
o
n

I
n
p
u
t

ut

C
u
r
r
e
n
t

i
n
p
u
t

yt+1

· · ·

Σ

O
u
t
p
u
t

A

t
t
e
n
t
i
o
n

I
n
p
u
t

ut+1

C
u
r
r
e
n
t

i
n
p
u
t

Inner product

Inner product

Figure 2: Illustration of ME-CRFs with a single memory hop, showing the network architecture at time
step t and t + 1.

3.1.4 Output memory
Similar to mt, ct is the output memory, and is
calculated analogously but with a different set of
parameters in the GRUs and tanh layers of Equa-
tions (1), (2) and (3). The output memory is used
to generate the ﬁnal output of the memory layer
and fed as input to the CRF layer.

3.1.5 Memory layer output

Once the attention weights have been computed,
the memory access controller receives the re-
sponse o in the form of a weighted sum over the
output memory representations:

ot = X
i

pt,ici

(5)

This allows the model to have unrestricted access
to elements in previous steps as opposed to a sin-
gle vector ht in RNNs, thereby enabling ME-
CRFs to detect and effectively incorporate long-
range dependencies.

3.1.6 Extension

For more challenging tasks requiring complex rea-
soning capabilities with multiple supporting facts
from the memory, the model can be further ex-
in
tended by stacking multiple memory hops,

which case the output of the k-th hop is taken as
input to the (k + 1)-th hop:

t = ok
uk+1

t + uk
t

(6)

where uk+1
t
current step (uk
the memory (ok
limit the number of hops to 1.

encodes not only information at the
t ) but also relevant knowledge from
t ). In the scope of this work, we

3.2 CRF Layer
Once the representation of the current step uK+1
is computed — incorporating relevant information
from the memory (assuming the total number of
memory hops is K) — it is then fed into a CRF
layer:

t

s(x, y) =

Ayt,yt+1 +

Pt,yt

(7)

T

X
t=0

T

X
t=1

⊤

where A ∈ R|Y|×|Y| is the CRF transition matrix,
|Y| is the size of the label set, and P ∈ RT ×|Y| is a
linearly transformed matrix from uK+1
such that
where Ws ∈ R|Y|×h with h be-
P
ing the size of mt. Here, Ai,j represents the score
of the transition from the i-th tag to the j-th tag
whereas Pi,j is the score of the j-th tag at time i.

t,: = WsuK+1

t

t

Using the scoring function in Equation (7), we cal-
culate the score of the sequence y normalised by
the sum of scores of all possible sequences ˜y, and
this becomes the probability of the true sequence:

p(y|x) =

exp(s(x, y))
P˜y∈YX exp(s(x, ˜y))

(8)

We train the model to maximise the probability
of the gold label sequence with the following loss
function:

L =

log p(y(n)|x(n))

(9)

N

X
n=1

where p(y(n)|x(n))
is calculated using the
forward–backward algorithm. Note that the model
is fully end-to-end differentiable.

At test time, the model predicts the output se-

quence with maximum a posteriori probability:

y∗ = arg max

p(˜y|x)

(10)

˜y∈Yx

Since we are only modelling bigram interactions,
we adopt the Viterbi algorithm for decoding.

4 Thread Discourse Structure Prediction

In this section, we describe how ME-CRFs can
be applied to the task of thread discourse structure
prediction, wherein we attempt to predict which
post(s) a given post directly responds to, and in
what way(s) (as captured by dialogue acts). This
is a novel approach to this problem and capable of
natively handling both tasks within the same net-
work architecture.

4.1 Dataset and Task

In this work, we adopt the dataset of Kim et al.
(2010),1 which consists of 315 threads and 1,332
posts, collected from the Operating System, Soft-
ware, Hardware and Web Development sub-
forums of CNET.2 Every post has been manually
linked to preceding post(s) in the thread that it is a
direct response to (in the form of “links”), and the
nature of the response for each link (in the form
of “dialogue acts”, or “DAs”). In this dataset, it is
not uncommon to see messages respond to posts
which occur much earlier in the thread (based on
the chronological ordering of posts). In fact, 18%

of the posts link to posts other than their immedi-
ately preceding post.

The task is deﬁned as follows: given a list of
preceding posts x1, . . . , xt−1 and the current post
xt, to classify which posts it links to (lt) and the
dialogue act (yt) of each such link. In the scope
of this work, ME-CRFs are capable of modelling
both tasks natively, and therefore a natural ﬁt for
this problem.

4.2 Experimental Setup

In this dataset, in addition to the body of text,
each post is also associated with a title. We
therefore use two encoders, Φtitle(·) and Φtext (·),
to process them separately and then concatenate
xt = [Φtitle(xt); Φtext (xt)]⊤. Here, Φtitle(·) and
Φtext (·) take word embeddings as input, process-
ing each post at the word level, as opposed to the
post-level bi-directional GRU in Equations (1) and
(2), and the representation of a post xt (either title
or text) is obtained by transforming the last and
ﬁrst hidden states of the forward and backward
word-level GRU, similar to Equation (3). Note
that Φtitle(·) and Φtext (·) do not share parame-
ters. As in Tang et al. (2016), we further restrict
mk

i = ck
In keeping with Wang (2014), we comple-
ment the textual representations with hand-crafted
structural features Φs(xt) ∈ R2:

i to curb overﬁtting.

• initiator: a binary feature indicating whether
the author of the current post is the same as
the initiator of the thread,

• position: the relative position of the current
post, as a ratio over the total number of posts
in the thread;

Also drawing on Wang (2014), we incorporate
punctuation-based features Φp(xt) ∈ R3:
the
number of question marks, exclamation marks and
URLs in the current post. The resultant feature
vectors are projected into an embedding space by
Ws and Wp and concatenated with xi, resulting
i. Subsequently, x′
in the new x′
i is fed into the bi-
directional GRUs to obtain mi.

For link prediction, we generate a supervision
signal from the annotated links, guiding the atten-
tion to focus on the correct memory position:

LLNK =

CrossEntropy(l(n)

t

, p(n)
t

) (11)

N

T

X
n=1

X
t=1

1http://people.eng.unimelb.edu.au/tbaldwin/resources/conll2010-thread/
2http://forums.cnet.com/

where l(n)
is a one-hot vector indicating where the
link points to for the t-th post in the n-th thread,

t

and p(n)
t = {pt,1, . . . , pt,t} is the predicted distri-
bution of attention over the t posts in the memory.
To accommodate the ﬁrst post in a thread, as it
points to a virtual “head” post, we set a dummy
post, m0 = 0, of the same size as mi. While
the dataset contains multi-headed posts (posts with
more than one outgoing link), following Wang
(2014), we only include the most recent linked
post during training, but evaluate over the full set
of labelled links.

For this task, ME-CRF is jointly trained to pre-
dict both the link and dialogue act with the follow-
ing loss function:

L′ = αLDA + (1 − α)LLNK

(12)

where LDA is the CRF likelihood deﬁned in Equa-
tion (9), and α is a hyper-parameter for balancing
the emphasis between the two tasks.

is

carried

Training

out with Adam
(Kingma and Ba, 2015) over 50 epochs with
a batch size of 32. We use the following hyper-
parameter settings: word embedding size of 20,
Wp ∈ R100×3, Ws ∈ R50×2, α = 0.5, hidden
size of Φtitle and Φtext
is 20, hidden size of
←−−
−−→
GRU and
GRU is 50. Dropout is applied to all
GRU recurrent units on the input and output
connections with a keep rate of 0.7.

Lastly, we also explore the idea of curriculum
learning (Bengio et al., 2009), by ﬁxing the CRF
transition matrix A = 0 for the ﬁrst e = 20
epochs, after which we train the parameters for the
remainder of the run. This allows the ME-CRF to
learn a good strategy for DA and link prediction,
as independent “maxent” type classiﬁer, before at-
tempting to learn sequence dynamics. We refer to
this variant as “ME-CRF+”.

4.3 Evaluation

Following Wang (2014), we evaluate based on
post-level micro-averaged F-score. All experi-
ments were carried out with 10-fold cross valida-
tion, stratifying at the thread level.

We benchmark against the following previous
work:
the feature-rich CRF-based approach of
Kim et al. (2010), where the authors trained inde-
pendent models for each of link and DA classiﬁca-
tion (“CRFKIM”); the feature-rich CRF-based ap-
proach of Wang (2014), where the author further
extended the feature set of Kim et al. (2010) and
jointly trained a CRF over the link and DA pre-
diction tasks (“CRFWANG”); and the dependency

Model

Link DA Joint

CRFKIM
CRFWANG
DEPPARSER

86.3
82.3
85.0

75.1 —
66.5
73.4
70.6
75.7

MEMNET

85.8

76.0

69.5

ME-CRF
ME-CRF+

86.4
86.3

77.5
77.4

70.9
71.2

Table 1: Post-level Link and DA F-scores. Per-
formance for ME-CRF and ME-CRF+ is marco-
averaged over 5 runs.

parser-based approach of Wang (2014), where the
author treated the discourse structure prediction
task as a constrained dependency parsing problem,
with posts as nodes in the dependency graph, and
the constraint that links must connect to preced-
ing posts in the thread (“DEPPARSER”).3 In ad-
dition to the CRF/parser-based systems, we also
build a MEMNET-based baseline (named MEM-
NET) where MEMNET shares the architecture of
the memory layer in ME-CRF but excludes the
use of the CRF layer. Instead, MEMNET, follow-
ing the work of Sukhbaatar et al. (2015), predicts
the ﬁnal answer by:

ˆy = softmax(WDA(uK+1))

(13)

where ˆy is the predicted DA distribution, WDA ∈
R|Y|×d is a parameter matrix for the model to
learn, and K = 1 is the total number of hops. This
is equivalent to classifying link and DA indepen-
dently at each time step t without taking transi-
tions between DA labels into account.

4.4 Results

The experimental results are presented in Table 1,
wherein the ﬁrst three rows are the three baseline
systems.

State-of-the-art post-level results. ME-CRFs
achieve state of the art results in terms of joint
post-level F-score, substantially better than the
baselines. While ME-CRF slightly outperforms
the current state-of-the-art (DEPPARSER), ME-
CRF+ improves the performance and achieves a
further 0.3% absolute gain.

3Note that a mistake was found in the results in the origi-
nal paper (Wang et al., 2011), and we use the corrected results
from Wang (2014).

e
r
o
c
s
-
F

t
n
i
o
J
A
D
&
k
n
i
L

100

80

60

40

20

0

Link&DA Joint

CRFSGD

MALTPARSER

ME-CRF+

Link

DA

[1,2]

[3,4]

[7,8]

[9,)

[5,6]
Post depth

CRFWANG
DEPPARSER

ME-CRF+

e
r
o
c
s
-
F
k
n
i
L

e
r
o
c
s
-
F
A
D

100

80

60

40

100

80

60

40

[1,2]

[3,4]

[7,8]

[9,)

[5,6]
Post depth

Figure 3: Breakdown of post-level Joint F-scores
by post depth, where e.g. “[1, 2]” is the joint F-
i.e. the ﬁrst or
score over posts of depth 1–2,
second post in the thread. Note that we take
the reported performance of CRFWANG and DEP-
PARSER from Wang (2014).

Curriculum learning improves joint prediction.
Despite the slight performance drop on the DA and
link prediction tasks, ME-CRF+, with the CRF
transition matrix frozen for the ﬁrst 20 epochs,
achieves a ∼0.3% absolute gain in joint F-score
over ME-CRF. This suggests that the sequence
dynamics between posts, while difﬁcult to capture,
are beneﬁcial to the overall task (resulting in more
coherent DA and link predictions) if trained with
proper initialisation.

MEMNET vs. ME-CRFs. We see consistent
gains across all three tasks when the CRF layer is
added. Although not presented in Table 1, the dif-
ference is most notable at the thread-level (i.e. a
thread is correct iff all posts are tagged correctly),
highlighting the importance of sequential transi-
tional information between posts.

CRF vs. ME-CRFs. Note that CRFKIM is not
trained jointly on the two component tasks, but in-
dividually on each task. Without additional data,
jointly training on the two tasks generates results
that are comparable or substantially better over the
individual tasks. This highlights the effectiveness
of ME-CRF, especially with the link prediction
performance comparable to that of a single-task
model CRF, and surpassing it in the case of DA.

[1,2]

[3,4]

[7,8]

[9,)

[5,6]
Post depth

Figure 4: Breakdown of post-level Link and DA
F-scores by post depth.

4.5 Analysis

We break the performance down by the depth of
each post in a given thread, and present the re-
sults in Figure 3. Although below the baselines
for the interval [1, 2], ME-CRF+ consistently out-
performs CRFWANG from depth 3 onwards, and is
superior to DEPPARSER for depths [7, ). Break-
ing down the performance further to the individual
tasks of Link and DA prediction, as displayed in
Figure 4, we observe a similar trend. In line with
the ﬁndings in the work of Wang (2014), this con-
ﬁrms that prediction becomes progressively more
difﬁcult as threads grow longer, which is largely
due to the increased variability in discourse struc-
ture. Despite the escalated difﬁculty, ME-CRF+
is substantially superior to the baselines when
classifying deeper posts.

Between the CRF-based models, it is worth
noting that despite the lower performance for
[1, 2], ME-CRF+ beneﬁts from having global ac-
cess to the entire sequence, and consistently out-
performs CRFWANG for depths [3, ), highlight-
ing the effectiveness of the memory mechanism.
Overall, these results validate our hypothesis that
having unrestricted access to the whole sequence
is beneﬁcial, especially for long-range dependen-
cies, offering further evidence of the power of

ME-CRFs.

5 Named Entity Recognition

In this section, we present experiments in a sec-
ond setting: named entity recognition, over the
CoNLL 2003 English NER shared task dataset
(Tjong Kim Sang and De Meulder, 2003). Our in-
terest here is in evaluating the ability of ME-CRF
to capture document context, to aid in the identiﬁ-
cation and disambiguation of NEs.

−−→
GRU and

Training is carried out with Adam, over 100
epochs with a batch size of 32. We use the fol-
lowing hyper-parameter settings: word embed-
←−−
ding size = 50; hidden size of
GRU = 50;
←−
−→
W m ∈ R50×50; and bm ∈ R50. Dropout
W m and
is applied to all GRU recurrent units on the in-
put and output connections, with a keep rate of
0.8. We initialise ME-CRF with pre-trained word
embeddings and keep them ﬁxed during training.
While we report results only on the test set, we use
early stopping based on the development set.

5.1 Dataset and Task

5.3 Evaluation

The CoNLL 2003 NER shared task dataset
consists of 14, 041/3, 250/3, 453 sentences in
the training/development/test set, resp., extracted
from 946/216/231 Reuters news articles from the
period 1996–97. The goal is to identify individ-
ual token occurrences of NEs, and tag each with
its class (e.g. LOCATION or ORGANISATION).
Here, we use the IOB tagging scheme. In terms
of tagging schemes, while some have shown
improvements with a more expressive IOBES
marginally (Ratinov and Roth, 2009; Dai et al.,
2015), we stick to the BIO scheme for simplicity
and the observation of little improvement between
these schemes by Lample et al. (2016).

5.2 Experimental Setup

We choose Φ(xt) to be a lookup function, return-
ing the corresponding embedding xt of the word
xt.
In addition to the word features, we em-
ploy a subset of the lexical features described in
Huang et al. (2015), based on whether the word:

• starts with a capital letter;
• is composed of all capital letters;
• is composed of all lower case letters;
• contains non initial capital letters;
• contains both letters and digits;
• contains punctuation.

These features are all binary and refered to as
Φl(xt). Similar to the thread structure predic-
tion experiments, we concatenate Φl(xt) with xt
to generate the new input x′ to the bi-directional
GRUs in Equations (1) and (2).

In order to incorporate information in the docu-
ment beyond sentence boundaries, we encode ev-
ery word sequentially in a document with Φ and
←−−
−−→
GRU, and store them in the memory mi
GRU and
and ci for i ∈ [1, t′], where t′ is the index of the
current word t in the document.

Evaluation is based on span-level NE F-score,
based on the ofﬁcial CoNLL evaluation script.4
We compare against the following baselines:
1. a CRF over hand-tuned lexical

features

(“CRF”: Huang et al. (2015))

2. an LSTM and
and

(“LSTM”
Huang et al. (2015))

bi-directional LSTM
resp.:
“BI-LSTM”,

3. a CRF taking features from a convolu-
(“CONV-

tional neural network as input
CRF”: Collobert et al. (2011))

4. a CRF over the output of either a simple
LSTM or bidirectional LSTM (“LSTM-
resp.:
CRF”
Huang et al. (2015))

“BI-LSTM-CRF”,

and

Note that
for our word embeddings, while
we observe better performance with GLOVE
(Pennington et al., 2014), for fair comparison pur-
poses, we adopt the same SENNA embeddings
(Collobert et al., 2011) as are used in the baseline
methods.5

5.4 Results

The experimental results are presented in Table 2.
Results for the baseline methods are based on
the published results of Huang et al. (2015) and
Collobert et al. (2011). Note that none of the sys-
tems in Table 2 use external gazetteers, to make
the comparison fair. As can be observed, ME-
CRF achieves the best performance, beating all
the baselines.

the
To gain a better understanding of what
model has learned, Table 3 presents two examples

4http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt
5Lample et al. (2016) report a higher result of 90.9 using
a BI-LSTM-CRF architecture, but augmented with skip n-
grams (Ling et al., 2015) and character embeddings. Due to
the differing underlying representation, we exclude it from
the comparison.

Model

F-score

Memory

pt,i

Memory

pt,i

CRF
LSTM
BI-LSTM
CONV-CRF

LSTM-CRF
BI-LSTM-CRF

ME-CRF

86.1
83.7
85.2
88.7

88.4
88.8

89.5

Table 2: NER performance on the CoNLL 2003
English NER shared task dataset.

where ME-CRF focuses on words beyond the cur-
rent sentence boundaries. In the example on the
left, where the target word is Juventus (an Ital-
ian soccer team), ME-CRF directs the attention
mainly to the occurrence of the same word in a
previous sentence and a small fraction to Manch-
ester (a UK soccer team, in this context). Note
that it does not attend to the other NE (Europe)
in that sentence, which is of a different NE class.
In the example on the right, on the other hand,
ME-CRF allocates attention to the same words
as the target word in the current sentence. Note
that the second occurrence of Interfax in the mem-
ory is the same occurrence as the ﬁrst word in the
current sentence. While more weight is placed
on the second Interfax, close to one third of the
attention is also asigned to the ﬁrst occurrence.
Given that the memory, mi and ci, is encoded
with bi-directional GRUs, the ﬁrst Interfax should,
to some degree, capture the succeeding neighbour-
ing elements: news agency.

This is reminiscent of label consistency in
the works of Sutton and McCallum (2004) and
Finkel et al. (2005), but differs in that the consis-
tency constraint is soft as opposed to hard in previ-
ous studies, and automatically learned without the
use of heuristics.

6 Conclusion

. . .
Manchester
United
face
Juventus
in
Europe
. . .

0.23
0.00
0.00
0.65
0.00
0.00

. . .
,
Interfax
news
agency
said
.
Interfax

0.00
0.32
0.00
0.00
0.00
0.00
0.68

European champions
Juventus . . .

Interfax quoted
Russian . . .

Table 3: An NER example showing learned atten-
tion to long-range contextual dependencies. The
last row is the current sentence. The underlined
words indicate the target word xt, and the dashed
line indicates a sentence boundary.

Acknowledgments

We thank the anonymous reviewers for their
valuable feedback, and gratefully acknowledge
the support of Australian Government Research
Training Program Scholarship and National Com-
putational Infrastructure (NCI Australia). This
work was also supported in part by the Australian
Research Council.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
In Proceedings of
learning to align and translate.
the 3rd International Conference on Learning Rep-
resentations (ICLR 2015). San Diego, USA.

Yoshua Bengio, J´erˆome Louradour, Ronan Collobert,
and Jason Weston. 2009. Curriculum learning. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning (ICML 2009). Mon-
treal, Canada, pages 41–48.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difﬁcult. IEEE Transactions on Neu-
ral Networks 5(2):157–166.

In this paper, we have presented ME-CRF, a
model extending linear-chain CRFs by including
external memory. This allows the model to look
beyond neighbouring items and access long-range
context. Experimental results demonstrate the ef-
fectiveness of the proposed method over two tasks:
forum thread discourse analysis, and named entity
recognition.

Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. In Proceedings of SSST-8, Eighth Work-
shop on Syntax, Semantics and Structure in Statisti-
cal Translation. Doha, Qatar, pages 103–111.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from

scratch.
12:2493–2537.

Journal of Machine Learning Research

Hong-Jie Dai, Po-Ting Lai, Yung-Chun Chang, and
Richard Tzong-Han Tsai. 2015.
Enhancing of
chemical compound and drug name recognition us-
ing representative tag scheme and ﬁne-grained tok-
enization. Journal of Cheminformatics 7(1):S14.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005.
Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005). Ann Arbor, USA, pages 363–370.

Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence 6:721–741.

Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013. Speech recognition with deep recur-
In 2013 IEEE International
rent neural networks.
Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP 2013). Vancouver, Canada, pages
6645–6649.

Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and
J¨urgen Schmidhuber. 2001. Gradient ﬂow in recur-
rent nets: the difﬁculty of learning long-term depen-
dencies.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991 .

Su Nam Kim, Li Wang, and Timothy Baldwin. 2010.
Tagging and linking web forum posts. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL 2010). Associ-
ation for Computational Linguistics, Uppsala, Swe-
den, pages 192–202.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
In Proceed-
method for stochastic optimization.
ings of the 3th International Conference on Learn-
ing Representations (ICLR 2015). San Diego, USA.

Vijay Krishnan and Christopher D. Manning. 2006. An
effective two-stage model for exploiting non-local
dependencies in named entity recognition. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2006). Sydney, Australia, pages 1121–1128.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random ﬁelds:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning (ICML
2001). San Francisco, USA, pages 282–289.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.
Recurrent convolutional neural networks for text
In Proceedings of the 29th AAAI
classiﬁcation.
Conference on Artiﬁcial Intelligence (AAAI 2015).
Austin, USA, volume 333, pages 2267–2273.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL 2016). San Diego, USA, pages 260–270.

Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2010). Uppsala, Sweden, pages 789–797.

Wang Ling, Yulia Tsvetkov, Silvio Amir, Ramon Fer-
mandez, Chris Dyer, Alan W Black, Isabel Tran-
coso, and Chu-Cheng Lin. 2015. Not all contexts
are created equal: Better word representations with
variable attention. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2015). Lisbon, Portugal, pages
1367–1372.

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the ability of lstms to learn syntax-
sensitive dependencies. Transactions of the Asso-
ciation for Computational Linguistics (TACL 2016)
4:521–535.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
In Proceedings of the 2014 Con-
representation.
ference on Empirical Methods in Natural Language
Processing (EMNLP 2014). Doha, Qatar, pages
1532–1543.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition.
In Proceedings of the Thirteenth Conference on
Computational Natural Language Learning (CoNLL
2009). Boulder, USA, pages 147–155.

Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh
Hajishirzi. 2017. Query-reduction networks for
question answering. In Proceedings of the 5th Inter-
national Conference on Learning Representations
(ICLR 2017). Toulon, France.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. In Proceedings of Advances in Neural Infor-
mation Processing Systems (NIPS 2015). Montr´eal,
Canada, pages 2440–2448.

Charles Sutton and Andrew McCallum. 2004. Col-
lective segmentation and labeling of distant entities
in information extraction. Technical report, Mas-
sachusetts Univ Amherst Dept of Computer Science.

Duyu Tang, Bing Qin, and Ting Liu. 2016. Aspect
level sentiment classiﬁcation with deep memory net-
In Proceedings of the 2016 Conference on
work.
Empirical Methods in Natural Language Processing
(EMNLP 2016). Austin, USA, pages 214–224.

Erik F Tjong Kim Sang and Fien De Meulder.
2003.
Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on North
the Association for Com-
American Chapter of
putational Linguistics (NAACL 2003). Edmonton,
Canada, pages 142–147.

Li Wang. 2014. Knowledge discovery and extraction
of domain-speciﬁc web data. Ph.D. thesis, The Uni-
versity of Melbourne.

Li Wang, Marco Lui, Su Nam Kim, Joakim Nivre, and
Timothy Baldwin. 2011. Predicting thread discourse
structure over technical web forums. In Proceedings
of the 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011). Ed-
inburgh, UK, pages 13–25.

Jason Weston, Sumit Chopra, and Antoine Bordes.
2015. Memory networks. In Proceedings of the 3rd
International Conference on Learning Representa-
tions (ICLR 2015). San Diego, USA.

Caiming Xiong, Stephen Merity, and Richard Socher.
2016. Dynamic memory networks for visual and
In Proceedings of the
textual question answering.
33rd International Conference on Machine Learning
(ICML 2016). New York, USA, pages 2397–2406.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classiﬁcation.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics — Human Language Technologies
(NAACL HLT 2016). San Diego, USA, pages 1480–
1489.

Amy Zhang, Bryan Culbertson, and Praveen Paritosh.
2017. Characterizing online discussion using coarse
In Proceedings of the 11th
discourse sequences.
AAAI International Conference on Web and Social
Media (ICWSM 2017). Palo Alto, USA, pages 357–
366.

