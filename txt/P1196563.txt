8
1
0
2
 
r
a

M
 
8
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
6
6
0
0
0
.
1
1
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

FRATERNAL DROPOUT

1Jagiellonian University
2MILA, Université de Montréal
3CIFAR Senior Fellow

Konrad ˙Zołna1,2,∗, Devansh Arpit2, Dendi Suhubdy2 & Yoshua Bengio2,3

ABSTRACT

Recurrent neural networks (RNNs) form an important class of architectures
among neural networks useful for language modeling and sequential prediction.
However, optimizing RNNs is known to be harder compared to feed-forward neu-
ral networks. A number of techniques have been proposed in literature to address
this problem. In this paper we propose a simple technique called fraternal dropout
that takes advantage of dropout to achieve this goal. Speciﬁcally, we propose to
train two identical copies of an RNN (that share parameters) with different dropout
masks while minimizing the difference between their (pre-softmax) predictions.
In this way our regularization encourages the representations of RNNs to be in-
variant to dropout mask, thus being robust. We show that our regularization term is
upper bounded by the expectation-linear dropout objective which has been shown
to address the gap due to the difference between the train and inference phases of
dropout. We evaluate our model and achieve state-of-the-art results in sequence
modeling tasks on two benchmark datasets – Penn Treebank and Wikitext-2. We
also show that our approach leads to performance improvement by a signiﬁcant
margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10)
tasks.

1

INTRODUCTION

Recurrent neural networks (RNNs) like long short-term memory (LSTM; Hochreiter & Schmidhu-
ber (1997)) networks and gated recurrent unit (GRU; Chung et al. (2014)) are popular architectures
for sequence modeling tasks like language generation, translation, speech synthesis, and machine
comprehension. However, they are harder to optimize compared to feed-forward networks due to
challenges like variable length input sequences, repeated application of the same transition operator
at each time step, and largely-dense embedding matrix that depends on the vocabulary size. Due to
these optimization challenges in RNNs, the application of batch normalization and its variants (layer
normalization, recurrent batch normalization, recurrent normalization propagation) have not been as
successful as their counterparts in feed-forward networks (Laurent et al., 2016), although they do
considerably provide performance gains. Similarly, naive application of dropout (Srivastava et al.,
2014) has been shown to be ineffective in RNNs (Zaremba et al., 2014). Therefore, regularization
techniques for RNNs is an active area of research.

To address these challenges, Zaremba et al. (2014) proposed to apply dropout only to the non-
recurrent connections in multi-layer RNNs. Variational dropout (Gal & Ghahramani (2016)) uses
the same dropout mask throughout a sequence during training. DropConnect (Wan et al., 2013)
applies the dropout operation on the weight matrices. Zoneout (Krueger et al. (2016)), in a similar
spirit with dropout, randomly chooses to use the previous time step hidden state instead of using
the current one. Similarly as a substitute for batch normalization, layer normalization normalizes
the hidden units within each sample to have zero mean and unit standard deviation. Recurrent batch
normalization applies batch normalization but with unshared mini-batch statistics for each time step
(Cooijmans et al., 2016).

∗konrad.zolna@gmail.com

1

Published as a conference paper at ICLR 2018

Merity et al. (2017a) and Merity et al. (2017b) on the other hand show that activity regulariza-
tion (AR) and temporal activation regularization (TAR)1 are also effective methods for regularizing
LSTMs. Another more recent way of regularizing RNNs, that is similar in spirit to the approach we
take, involves minimizing the difference between the hidden states of the original and the auxiliary
network Serdyuk et al. (2017).

In this paper we propose a simple regularization based on dropout that we call fraternal dropout,
where we minimize an equally weighted sum of prediction losses from two identical copies of the
same LSTM with different dropout masks, and add as a regularization the (cid:96)2 difference between the
predictions (pre-softmax) of the two networks. We analytically show that our regularization objec-
tive is equivalent to minimizing the variance in predictions from different i.i.d. dropout masks; thus
encouraging the predictions to be invariant to dropout masks. We also discuss how our regularization
is related to expectation linear dropout Ma et al. (2016), Π-model Laine & Aila (2016) and activity
regularization Merity et al. (2017b), and empirically show that our method provides non-trivial gains
over these related methods which we explain furthermore in our ablation study (Section 5).

2 FRATERNAL DROPOUT

Dropout is a powerful regularization for neural networks. It is usually more effective on densely
connected layers because they suffer more from overﬁtting compared with convolution layers where
the parameters are shared. For this reason dropout is an important regularization for RNNs. How-
ever, dropout has a gap between its training and inference phase since the latter phase assumes linear
activations to correct for the factor by which the expected value of each activation would be different
Ma et al. (2016). In addition, the prediction of models with dropout generally vary with different
dropout mask. However, the desirable property in such cases would be to have ﬁnal predictions be
invariant to dropout masks.

As such, the idea behind fraternal dropout is to train a neural network model in a way that encour-
ages the variance in predictions under different dropout masks to be as small as possible. Specif-
ically, consider we have an RNN model denoted by M(θ) that takes as input X, where θ denotes
i; θ) ∈ Rm be the prediction of the model for input sample X at
the model parameters. Let pt(zt, st
i and current input zt, where zt is a function of X and the hidden states
time t, for dropout mask st
corresponding to the previous time steps. Similarly, let (cid:96)t(pt(zt, st
i; θ), Y) be the corresponding tth
time step loss value for the overall input-target sample pair (X, Y).

Then in fraternal dropout, we simultaneously feed-forward the input sample X through two identical
i and st
copies of the RNN that share the same parameters θ but with different dropout masks st
j at
each time step t. This yields two loss values at each time step t given by (cid:96)t(pt(zt, st
i; θ), Y), and
(cid:96)t(pt(zt, st

j; θ), Y). Then the overall loss function of fraternal dropout is given by,

(cid:96)F D(X, Y) =

(cid:0)(cid:96)t(pt(zt, st

i; θ), Y) + (cid:96)t(pt(zt, st

j; θ), Y)(cid:1) +

RF D(zt; θ)

κ
mT

T
(cid:88)

t=1

T
(cid:88)

t=1

1
2

where κ is the regularization coefﬁcient, m is the dimensions of pt(zt, st
fraternal dropout regularization given by,

i; θ) and RF D(zt; θ) is the

RF D(zt; θ) := Est

i,st
j

(cid:2)(cid:107)pt(zt, st

i; θ) − pt(zt, st

j; θ)(cid:107)2
2

(cid:3) .

We use Monte Carlo sampling to approximate RF D(zt; θ) where pt(zt, st
the same as the one used to calculate (cid:96)t values. Hence, the additional computation is negligible.

i; θ) and pt(zt, st

j; θ) are

We note that the regularization term of our objective is equivalent to minimizing the variance in the
prediction function with different dropout masks as shown below (proof in the appendix).

1TAR and Zoneout are similar in their motivations because both leads to adjacent time step hidden states to

be close on average.

(1)

(2)

2

Published as a conference paper at ICLR 2018

i and st
Remark 1. Let st
as described above. Then,

j be i.i.d. dropout masks and pt(zt, st

i; θ) ∈ Rm be the prediction function

RF D(zt; θ) = Est

i,st
j

(cid:2)(cid:107)pt(zt, st

i; θ) − pt(zt, st

j; θ)(cid:107)2
2

(cid:3) = 2

varst

i

(pt

q(zt, st

i; θ)).

(3)

m
(cid:88)

q=1

Note that a generalization of our approach would be to minimize the difference between the predic-
tions of the two networks with different data/model augmentations. However, in this paper we focus
on using different dropout masks and experiment mainly with RNNs2.

3 RELATED WORK

3.1 RELATION TO EXPECTATION LINEAR DROPOUT (ELD)

Ma et al. (2016) analytically showed that the expected error (over samples) between a model’s
expected prediction over all dropout masks, and the prediction using the average mask, is upper
bounded. Based on this result, they propose to explicitly minimize the difference (we have adapted
their regularization to our notations),
RELD(zt; θ) = (cid:107)Es

(cid:2)pt(zt, s; θ)(cid:3) − pt(zt, Es[s]; θ)(cid:107)2

(4)

where s is the dropout mask. However, due to feasibility consideration, they instead propose to use
the following regularization in practice,
˜RELD(zt; θ) = Esi

(cid:2)(cid:107)pt(zt, si; θ) − pt(zt, Es[s]; θ)(cid:107)2

(cid:3) .

(5)

2

Speciﬁcally, this is achieved by feed-forwarding the input twice through the network, with and with-
out dropout mask, and minimizing the main network loss (with dropout) along with the regulariza-
tion term speciﬁed above (but without back-propagating the gradients through the network without
dropout). The goal of Ma et al. (2016) is to minimize the network loss along with the expected dif-
ference between the prediction from individual dropout mask and the prediction from the expected
dropout mask. We note that our regularization objective is upper bounded by the expectation-linear
dropout regularization as shown below (proof in the appendix).
Proposition 1. RF D(zt; θ) ≤ 4 ˜RELD(zt; θ).

This result shows that minimizing the ELD objective indirectly minimizes our regularization term.
Finally as indicated above, they apply the target loss only on the network with dropout. In fact, in our
own ablation studies (see Section 5) we ﬁnd that back-propagating target loss through the network
(without dropout) makes optimizing the model harder. However, in our setting, simultaneously back-
propagating target loss through both networks yields both performance gain as well as convergence
gain. We believe convergence is faster for our regularization because network weights are more
likely to get target based updates from back-propagation in our case. This is especially true for
weight dropout (Wan et al., 2013) since in this case dropped weights do not get updated in the
training iteration.

3.2 RELATION TO Π-MODEL

Laine & Aila (2016) propose Π-model with the goal of improving performance on classiﬁcation
tasks in the semi-supervised setting. They propose a model similar to ours (considering the equiv-
alent deep feed-forward version of our model) except they apply target loss only on one of the net-
works and use time-dependent weighting function ω(t) (while we use constant κ
mT ). The intuition
in their case is to leverage unlabeled data by using them to minimize the difference in prediction be-
tween the two copies of the network with different dropout masks. Further, they also test their model
in the supervised setting but fail to explain the improvements they obtain by using this regularization.

We note that in our case we analytically show that minimizing our regularizer (also used in Π-model)
is equivalent to minimizing the variance in the model predictions (Remark 1). Furthermore, we also
show the relation of our regularizer to expectation linear dropout (Proposition 1). In Section 5, we

2The reasons of our focus on RNNs are described in the appendix.

3

Published as a conference paper at ICLR 2018

study the effects of target based loss on both networks, which is not used in the Π-model. We ﬁnd
that applying target loss on both the networks leads to signiﬁcantly faster convergence. Finally,
we bring to attention that temporal embedding (another model proposed by Laine & Aila (2016),
claimed to be a better version of Π-model for semi-supervised, learning) is intractable in natural
language processing applications because storing averaged predictions over all of the time steps
would be memory exhaustive (since predictions are usually huge - tens of thousands values). On
a ﬁnal note, we argue that in the supervised case, using a time-dependent weighting function ω(t)
instead of a constant value κ
mT is not needed. Since the ground truth labels are known, we have not
observed the problem mentioned by Laine & Aila (2016), that the network gets stuck in a degenerate
solution when ω(t) is too large in earlier epochs of training. We note that it is much easier to search
for an optimal constant value, which is true in our case, as opposed to tuning the time-dependent
function.

Similarity to Π-model makes our method related to other semi-supervised works, mainly Rasmus
et al. (2015) and Sajjadi et al. (2016). Since semi-supervised learning is not a primary focus of this
paper, we refer to Laine & Aila (2016) for more details.

We note that the idea of adding a penalty encouraging the representation to be similar for two
different masks was previously implemented3 by the authors of a Multi-Prediction Deep Boltzmann
Machines (Goodfellow et al., 2013). Nevertheless, the idea is not discussed in their paper.

Another way to address the gap between the train and evaluation mode of dropout is to perform
Monte Carlo sampling of masks and average the predictions during evaluation, and this has been
used for feed-forward networks. We ﬁnd that this technique does not work well for RNNs. The
details of these experiments can be found in the appendix.

4 EXPERIMENTS

4.1 LANGUAGE MODELS

In the case of language modeling we test our model4 on two benchmark datasets – Penn Tree-bank
(PTB) dataset (Marcus et al., 1993) and WikiText-2 (WT2) dataset (Merity et al., 2016). We use
preprocessing as speciﬁed by Mikolov et al. (2010) (for PTB corpus) and Moses tokenizer Koehn
et al. (2007) (for the WT2 dataset).

For both datasets we use the AWD-LSTM 3-layer architecture described in Merity et al. (2017a)5
which we call the baseline model. The number of parameters in the model used for PTB is 24
million as compared to 34 million in the case of WT2 because WT2 has a larger vocabulary size
for which we use a larger embedding matrix. Apart from those differences, the architectures are
identical. When we use fraternal dropout, we simply add our regularization on top of this baseline
model.

Word level Penn Treebank (PTB). Inﬂuenced by Melis et al. (2017), our goal here is to make
sure that fraternal dropout outperforms existing methods not simply because of extensive hyper-
parameter grid search but rather due to its regularization effects. Hence, in our experiments we
leave a vast majority of hyper-parameters used in the baseline model (Melis et al., 2017) unchanged
i.e. embedding and hidden states sizes, gradient clipping value, weight decay and the values used
for all dropout layers (dropout on the word vectors, the output between LSTM layers, the output of
the ﬁnal LSTM, and embedding dropout). However, a few changes are necessary:

• the coefﬁcients for AR and TAR needed to be altered because fraternal dropout also affects
RNNs activation (as explained in Subsection 5.3) – we did not run grid search to obtain the
best values but simply deactivated AR and TAR regularizers;

• since fraternal dropout needs twice as much memory, batch size is halved so the model

needs approximately the same amount of memory and hence ﬁts on the same GPU.

3The implementation is freely available as part of a LISA lab library, Pylearn2. For more details see

github.com/lisa-lab/pylearn2/blob/master/pylearn2/costs/dbm.py#L1175 .

4Our code is available at github.com/kondiz/fraternal-dropout .
5We used the ofﬁcial GitHub repository code for

this paper github.com/salesforce/

awd-lstm-lm .

4

Published as a conference paper at ICLR 2018

Model
Zaremba et al. (2014) - LSTM (medium)
Zaremba et al. (2014) - LSTM (large)
Gal & Ghahramani (2016) - Variational LSTM (medium)
Gal & Ghahramani (2016) - Variational LSTM (large)
Inan et al. (2016) - Variational LSTM
Inan et al. (2016) - Variational RHN
Zilly et al. (2016) - Variational RHN
Melis et al. (2017) - 5-layer RHN
Melis et al. (2017) - 4-layer skip connection LSTM
Merity et al. (2017a) - AWD-LSTM 3-layer (baseline)
Fraternal dropout + AWD-LSTM 3-layer

Parameters Validation Test
82.7
78.4
79.7
75.2
68.5
66.0
65.4
62.2
58.3
57.3
56.8

10M
24M
20M
66M
51M
24M
23M
24M
24M
24M
24M

86.2
82.2
81.9
77.9
71.1
68.1
67.9
64.8
60.9
60.0
58.9

Table 1: Perplexity on Penn Treebank word level language modeling task.

Parameters Validation

Model
Merity et al. (2016) - Variational LSTM + Zoneout
Merity et al. (2016) - Variational LSTM
Inan et al. (2016) - Variational LSTM
Melis et al. (2017) - 5-layer RHN
Melis et al. (2017) - 1-layer LSTM
Melis et al. (2017) - 2-layer skip connection LSTM
Merity et al. (2017a) - AWD-LSTM 3-layer (baseline)
Fraternal dropout + AWD-LSTM 3-layer

20M
20M
28M
24M
24M
24M
34M
34M

108.7
101.7
91.5
78.1
69.3
69.1
68.6
66.8

Test
100.9
96.3
87.0
75.6
65.9
65.9
65.8
64.1

Table 2: Perplexity on WikiText-2 word level language modeling task.

The ﬁnal change in hyper-parameters is to alter the non-monotone interval n used in non-
monotonically triggered averaged SGD (NT-ASGD) optimizer Polyak & Juditsky (1992); Mandt
et al. (2017); Melis et al. (2017). We run a grid search on n ∈ {5, 25, 40, 50, 60} and obtain very
similar results for the largest values (40, 50 and 60) in the candidate set. Hence, our model is trained
longer using ordinary SGD optimizer as compared to the baseline model (Melis et al., 2017).

We evaluate our model using the perplexity metric and compare the results that we obtain against
the existing state-of-the-art results. The results are reported in Table 1. Our approach achieves the
state-of-the-art performance compared with existing benchmarks.

To conﬁrm that the gains are robust to initialization, we run ten experiments for the baseline model
with different seeds (without ﬁne-tuning) for PTB dataset to compute conﬁdence intervals. The
average best validation perplexity is 60.64 ± 0.15 with the minimum value equals 60.33. The same
for test perplexity is 58.32 ± 0.14 and 58.05, respectively. Our score (59.8 validation and 58.0 test
perplexity) beats ordinal dropout minimum values.

We also perform experiments using fraternal dropout with a grid search on all the hyper-parameters
and ﬁnd that it leads to further improvements in performance. The details of this experiment can be
found in section 5.5.

Word level WikiText-2 (WT2). In the case of WikiText-2 language modeling task, we outperform
the current state-of-the-art using the perplexity metric by a signiﬁcant margin. Due to the lack
of computational power, we run a single training procedure for fraternal dropout on WT2 dataset
because it is larger than PTB. In this experiment, we use the best hyper-parameters found for PTB
dataset (κ = 0.1, non-monotone interval n = 60 and halved batch size; the rest of the hyper-
parameters are the same as described in Melis et al. (2017) for WT2). The ﬁnal results are presented
in Table 2.

4.2

IMAGE CAPTIONING

We also apply fraternal dropout on an image captioning task. We use the well-known show and tell
model as a baseline6 (Vinyals et al., 2014). We emphasize that in the image captioning task, the

6We used PyTorch implementation with default hyper-parameters from github.com/ruotianluo/

neuraltalk2.pytorch .

5

Published as a conference paper at ICLR 2018

Model
Show and Tell Xu et al. (2015)
Baseline
Fraternal dropout, κ = 0.015
Fraternal dropout, κ = 0.005

BLEU-1 BLEU-2 BLEU-3 BLEU-4

66.6
68.8
69.3
69.3

46.1
50.8
51.4
51.5

32.9
36.1
36.6
36.9

24.6
25.6
26.1
26.3

Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is
the only difference between models. The rest of hyper-parameters are the same.

image encoder and sentence decoder architectures are usually learned together. Since we want to
focus on the beneﬁts of using fraternal dropout in RNNs we use frozen pretrained ResNet-101 (He
et al., 2015) model as our image encoder. It means that our results are not directly comparable with
other state-of-the-art methods, however we report results for the original methods so readers can see
that our baseline performs well. The ﬁnal results are presented in Table 3.

We argue that in this task smaller κ values are optimal because the image captioning encoder is given
all information in the beginning and hence the variance of consecutive predictions is smaller that in
unconditioned natural language processing tasks. Fraternal dropout may beneﬁts here mainly due
to averaging gradients for different mask and hence updating weights more frequently.

5 ABLATION STUDIES

In this section, the goal is to study existing methods closely related to ours – expectation linear
dropout Ma et al. (2016), Π-model Laine & Aila (2016) and activity regularization Merity et al.
(2017b). All of our experiments for ablation studies, which apply a single layer LSTM, use the
same hyper-parameters and model architecture7 as Melis et al. (2017).

5.1 EXPECTATION-LINEAR DROPOUT (ELD)

The relation with expectation-linear dropout Ma et al. (2016) has been discussed in Section 2. Here
we perform experiments to study the difference in performance when using the ELD regularization
versus our regularization (FD). In addition to ELD, we also study a modiﬁcation (ELDM) of ELD
which applies target loss to both copies of LSTMs in ELD similar to FD (notice in their case they
only have dropout on one LSTM). Finally we also evaluate a baseline model without any of these
regularizations. The learning dynamics curves are shown in Figure 1. Our regularization performs
better in terms of convergence compared with other methods. In terms of generalization, we ﬁnd
that FD is similar to ELD, but baseline and ELDM are much worse. Interestingly, looking at the
train and validation curves together, ELDM seems to be suffering from optimization problems.

5.2 Π-MODEL

Since Π-model Laine & Aila (2016) is similar to our algorithm (even though it is designed for
semi-supervised learning in feed-forward networks), we study the difference in performance with
Π-model8 both qualitatively and quantitatively to establish the advantage of our approach. First,
we run both single layer LSTM and 3-layer AWD-LSTM on PTB task to check how their model
compares with ours in the case of language modeling. The results are shown in Figure 1 and 2. We
ﬁnd that our model converges signiﬁcantly faster than Π-model. We believe this happens because we

7We use a batch size of 64, truncated back-propagation with 35 time steps, a constant zero state is provided
as the initial state with probability 0.01 (similar to Melis et al. (2017)), SGD with learning rate 30 (no mo-
mentum) which is multiplied by 0.1 whenever validation performance does not improve ever during 20 epochs,
weight dropout on the hidden to hidden matrix 0.5, dropout every word in a mini-batch with probability 0.1,
embedding dropout 0.65, output dropout 0.4 (ﬁnal value of LSTM), gradient clipping of 0.25, weight decay
1.2 × 10−6, input embedding size of 655, the input/output size of LSTM is the same as embedding size (655)
and the embedding weights are tied (Inan et al., 2016; Press & Wolf, 2016).

8We use a constant function ω(t) = κ

mT as a coefﬁcient for Π-model (similar to our regularization term).
Hence, the focus of our experiment is to evaluate the difference in performance when target loss is back-
propagated through one of the networks (Π-model) vs. both (ours). Additionally, we ﬁnd that tuning a function
instead of using a constant coefﬁcient is infeasible.

6

Published as a conference paper at ICLR 2018

Figure 1: Ablation study: Train (left) and validation (right) perplexity on PTB word level modeling
with single layer LSTM (10M parameters). These curves study the learning dynamics of the baseline
model, Π-model, Expectation-linear dropout (ELD), Expectation-linear dropout with modiﬁcation
(ELDM) and fraternal dropout (FD, our algorithm). We ﬁnd that FD converges faster than the
regularizers in comparison, and generalizes at par.

Figure 2: Ablation study: Validation perplexity
on PTB word level modeling for Π-model and
fraternal dropout. We ﬁnd that FD converges
faster and generalizes at par.

Figure 3: Ablation study: Average hidden state
activation is reduced when any of the regular-
izer described is used. The y-axis is the value of
d (cid:107)m · ht(cid:107)2
1
2.

back-propagate the target loss through both networks (in contrast to Π-model) that leads to weights
getting updated using target-based gradients more often.

Even though we designed our algorithm speciﬁcally to address problems in RNNs, to have a fair
comparison, we compare with Π-model on a semi-supervised task which is their goal. Speciﬁ-
cally, we use the CIFAR-10 dataset that consists of 32 × 32 images from 10 classes. Following the
usual splits used in semi-supervised learning literature, we use 4 thousand labeled and 41 thousand
unlabeled samples for training, 5 thousand labeled samples for validation and 10 thousand labeled
samples for test set. We use the original ResNet-56 (He et al., 2015) architecture. We run grid search
on κ ∈ {0.05, 0.1, 0.15, 0.2}, dropout rates in {0.05, 0.1, 0.15, 0.2} and leave the rest of the hyper-
parameters unchanged. We additionally check importance of using unlabeled data. The results are
reported in Table 4. We ﬁnd that our algorithm performs at par with Π-model. When unlabeled data
is not used, fraternal dropout provides slightly better results as compared to traditional dropout.

5.3 ACTIVITY REGULARIZATION AND TEMPORAL ACTIVITY REGULARIZATION ANALYSIS

The authors of Merity et al. (2017b) study the importance of activity regularization (AR)9 and tem-
poral activity regularization (TAR) in LSTMs given as,
α
d
β
d

RT AR(zt; θ) =

(cid:107)ht − ht−1(cid:107)2
2

RAR(zt; θ) =

(cid:107)ht(cid:107)2
2

(7)

(6)

9We used (cid:107)m · ht(cid:107)2

2, where m is the dropout mask, in our actual experiments with AR because it was

implemented as such in the original paper’s Github repository Merity et al. (2017a).

7

Published as a conference paper at ICLR 2018

Dropout rate Unlabeled data

Model
Traditional dropout
No dropout
Fraternal dropout (κ = 0.05)
Traditional dropout + Π-model
Fraternal dropout (κ = 0.15)

0.1
0.0
0.05
0.1
0.1

No
No
No
Yes
Yes

Validation
78.4 (± 0.25)
78.8 (± 0.59)
79.3 (± 0.38)
80.2 (± 0.33)
80.5 (± 0.18)

Test
76.9 (± 0.31)
77.1 (± 0.3)
77.6 (± 0.35)
78.5 (± 0.46)
79.1 (± 0.37)

Table 4: Ablation study: Accuracy on altered (semi-supervised) CIFAR-10 dataset for ResNet-56
based models. We ﬁnd that our algorithm performs at par with Π-model. When unlabeled data
is not used traditional dropout hurts performance while fraternal dropout provides slightly better
results. It means that our methods may be beneﬁcial when we lack data and have to use additional
regularizing methods.

Figure 4: Ablation study: Train (left) and validation (right) perplexity on PTB word level model-
ing with single layer LSTM (10M parameters). These curves study the learning dynamics of the
baseline model, temporal activity regularization (TAR), prediction regularization (PR), activity reg-
ularization (AR) and fraternal dropout (FD, our algorithm). We ﬁnd that FD both converges faster
and generalizes better than the regularizers in comparison.

where ht ∈ Rd is the LSTM’s output activation at time step t (hence depends on both current input
zt and the model parameters θ). Notice that AR and TAR regularizations are applied on the output of
the LSTM, while our regularization is applied on the pre-softmax output pt(zt, st
i; θ) of the LSTM.
However, since our regularization can be decomposed as

RF D(zt; θ) = Esi,sj
= Esi,sj

(cid:2)(cid:107)pt(zt, st
(cid:2)(cid:107)pt(zt, st

i; θ) − pt(ht, st
i; θ)(cid:107)2

2 + (cid:107)pt(zt, st

j; θ)(cid:107)2
2
j; θ)(cid:107)2

(cid:3)

2 − 2pt(zt, st

i; θ)T pt(zt, st

(8)
j; θ)(cid:3)
(9)

and encapsulates an (cid:96)2 term along with the dot product term, we perform experiments to conﬁrm
that the gains in our approach is not due to the (cid:96)2 regularization alone. A similar argument goes
for the TAR objective. We run a grid search on α ∈ {1, 2, . . . , 12}, β ∈ {1, 2, . . . , 12}, which
include the hyper-parameters mentioned in Merity et al. (2017a). For our regularization, we use
κ ∈ {0.05, 0.1, . . . , 0.4}. Furthermore, we also compare with a regularization (PR) that regularizes
2 to further rule-out any gains only from (cid:96)2 regularization. Based on this grid search,
(cid:107)pt(zt, st
we pick the best model on the validation set for all the regularizations, and additionally report a
baseline model without any of these four mentioned regularizations. The learning dynamics is shown
in Figure 4. Our regularization performs better both in terms of convergence and generalization
compared with other methods. Average hidden state activation is reduced when any of the regularizer
described is applied (see Figure 3).

i; θ)(cid:107)2

5.4

IMPROVEMENTS USING FINE-TUNING

We conﬁrm that models trained with fraternal dropout beneﬁt from the NT-ASGD ﬁne-tuning step
(as also used in Merity et al. (2017a)). However, this is a very time-consuming practice and since dif-
ferent hyper-parameters may be used in this additional part of the learning procedure, the probability
of obtaining better results due to the extensive grid search is higher. Hence, in our experiments we

8

Published as a conference paper at ICLR 2018

PTB

WT2

Dropout
Traditional
Traditional
Fraternal
Fraternal
Fraternal

Fine-tuning Validation Test Validation Test
66.0
65.8
65.3
64.1
–

None
One
None
One
Two

69.1
68.6
68.3
66.8
–

58.8
57.3
58.0
56.8
56.2

60.7
60.0
59.8
58.9
58.5

Table 5: Ablation study: Importance of ﬁne-tuning for AWD-LSTM 3-layer model. Perplexity for
the Penn Treebank and WikiText-2 language modeling tasks.

Hyper-parameter
batch size
non-monotone interval
κ – FD or ELD strength
weight decay

Possible values
[10, 20, 30, 40]
[5, 10, 20, 40, 60, 100]
U (0, 0.3)
U (0.6 × 10−6, 2.4 × 10−6)

Table 6: Ablation study: Candidate hyper-parameters possible used in the grid search for comparing
fraternal dropout and expectation linear dropout. U (a, b) is the uniform distribution on the interval
[a, b]. For ﬁnite sets, each value is drawn with equal probability.

Model
Expectation linear dropout
Fraternal dropout

Best Top5 avg Top10 avg Beating baseline runs (out of)
59.4
59.4

6 (208)
14 (203)

60.1
59.6

60.5
59.9

Table 7: Ablation study: Fraternal dropout and expectation linear dropout comparison. Perplex-
ity on the Penn Treebank validation dateset. Fraternal dropout is more robust to different hyper-
parameters choice as twice as much runs ﬁnished performing better than the baseline model (60.7).

use the same ﬁne-tuning procedure as implemented in the ofﬁcial repository (even fraternal dropout
was not used). We present the importance of ﬁne-tuning in Table 5.

5.5 FRATERNAL DROPOUT AND EXPECTATION LINEAR DROPOUT COMPARISON

We perform extensive grid search for the baseline model from Subsection 4.1 (an AWD-LSTM
3-layer architecture) trained with either fraternal dropout or expectation linear dropout regulariza-
tions, to further contrast the performance of these two methods. The experiments are run without
ﬁne-tuning on the PTB dataset.

In each run, all ﬁve dropout rates are randomly altered (they are set to their original value, as in
Merity et al. (2017a), multiplied by a value drawn from the uniform distribution on the interval
[0.5, 1.5]) and the rest of the hyper-parameters are drawn as shown in Table 6. As in Subsection 4.1,
AR and TAR regularizers are deactivated.

Together we run more than 400 experiments. The results are presented in Table 7. Both FD and
ELD perform better than the baseline model that instead uses AR and TAR regularizers. Hence,
we conﬁrm our previous ﬁnding (see Subsection 5.3) that both FD and ELD are better. However,
as found previously for smaller model in Subsection 5.1, the convergence of FD is faster than that
of ELD. Additionally, fraternal dropout is more robust to different hyper-parameters choice (more
runs performing better than the baseline and better average for top performing runs).

6 CONCLUSION

In this paper we propose a simple regularization method for RNNs called fraternal dropout that acts
as a regularization by reducing the variance in model predictions across different dropout masks. We
show that our model achieves state-of-the-art results on benchmark language modeling tasks along
with faster convergence. We also analytically study the relationship between our regularization and
expectation linear dropout Ma et al. (2016). We perform a number of ablation studies to evaluate

9

Published as a conference paper at ICLR 2018

our model from different aspects and carefully compare it with related methods both qualitatively
and quantitatively.

ACKNOWLEDGEMENTS

The authors would like to acknowledge the support of the following agencies for research funding
and computing support: NSERC, CIFAR, and IVADO. We would like to thank Rosemary Nan Ke
and Philippe Lacaille for their thoughts and comments throughout the project. We would also like
to thank Stanisław Jastrz˛ebski† and Evan Racah† for useful discussions.

REFERENCES

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.

Tim Cooijmans, Nicolas Ballas, César Laurent, and Aaron C. Courville. Recurrent batch normal-
ization. CoRR, abs/1603.09025, 2016. URL http://arxiv.org/abs/1603.09025.

Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. In Advances in neural information processing systems, pp. 1019–1027, 2016.

Ian Goodfellow, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Multi-prediction deep boltz-
mann machines. In Advances in Neural Information Processing Systems, pp. 548–556, 2013.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-

nition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):

1735–1780, 1997.

Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classiﬁers: A

loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. Moses: Open source
toolkit for statistical machine translation. In Proceedings of the 45th annual meeting of the ACL
on interactive poster and demonstration sessions, pp. 177–180. Association for Computational
Linguistics, 2007.

David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rose-
mary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, et al. Zoneout:
Regularizing rnns by randomly preserving hidden activations. arXiv preprint arXiv:1606.01305,
2016.

Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint

arXiv:1610.02242, 2016.

César Laurent, Gabriel Pereyra, Philémon Brakel, Ying Zhang, and Yoshua Bengio. Batch normal-
ized recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE
International Conference on, pp. 2657–2661. IEEE, 2016.

Xuezhe Ma, Yingkai Gao, Zhiting Hu, Yaoliang Yu, Yuntian Deng, and Eduard Hovy. Dropout with

expectation-linear regularization. arXiv preprint arXiv:1609.08017, 2016.

Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate

bayesian inference. arXiv preprint arXiv:1704.04289, 2017.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated

corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.

†equal contribution ¨(cid:94)

10

Published as a conference paper at ICLR 2018

Gábor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language

models. arXiv preprint arXiv:1707.05589, 2017.

Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. CoRR, abs/1609.07843, 2016. URL http://arxiv.org/abs/1609.07843.

Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm lan-

guage models. arXiv preprint arXiv:1708.02182, 2017a.

Stephen Merity, Bryan McCann, and Richard Socher. Revisiting activation regularization for lan-

guage rnns. arXiv preprint arXiv:1708.01009, 2017b.

Tomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recurrent

neural network based language model. In Interspeech, volume 2, pp. 3, 2010.

Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.

SIAM Journal on Control and Optimization, 30(4):838–855, 1992.

Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint

arXiv:1608.05859, 2016.

Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko.

Semi-
supervised learning with ladder network. CoRR, abs/1507.02672, 2015. URL http://arxiv.
org/abs/1507.02672.

Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transfor-
mations and perturbations for deep semi-supervised learning. In Advances in Neural Information
Processing Systems, pp. 1163–1171, 2016.

Dmitriy Serdyuk, Nan Rosemary Ke, Alessandro Sordoni, Adam Trischler, Chris Pal, and Yoshua

Bengio. Twin networks: Matching the future for sequence generation. 2017.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. Journal of machine learning
research, 15(1):1929–1958, 2014.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image
caption generator. CoRR, abs/1411.4555, 2014. URL http://arxiv.org/abs/1411.
4555.

Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural
In Proceedings of the 30th international conference on machine

networks using dropconnect.
learning (ICML-13), pp. 1058–1066, 2013.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation
with visual attention. CoRR, abs/1502.03044, 2015. URL http://arxiv.org/abs/1502.
03044.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.

arXiv preprint arXiv:1409.2329, 2014.

Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, and Jürgen Schmidhuber. Recurrent

highway networks. arXiv preprint arXiv:1607.03474, 2016.

11

Published as a conference paper at ICLR 2018

APPENDIX

MONTE CARLO EVALUATION

A well known way to address the gap between the train and evaluation mode of dropout is to perform
Monte Carlo sampling of masks and average the predictions during evaluation (MC-eval), and this
has been used for feed-forward networks. Since fraternal dropout addresses the same problem, we
would like to clarify that it is not straight-forward and feasible to apply MC-eval for RNNs. In
feed-forward networks, we average the output prediction scores from different masks. However, in
the case RNNs (for next step predictions), there is more than one way to perform such evaluation,
but each one is problematic. They are as follows:

1. Online averaging

Consider that we ﬁrst make the prediction at time step 1 using different masks by averaging the
prediction score. Then we use this output to feed as input to the time step 2, then use different masks
at time step 2 to generate the output at time step 2, and so on. But in order to do so, because of
the way RNNs work, we also need to feed the previous time hidden state to time step 2. One way
would be to average the hidden states over different masks at time step 1. But the hidden space can
in general be highly nonlinear, and it is not clear if averaging in this space is a good strategy. This
approach is not justiﬁed.

Besides, this strategy as a whole is extremely time consuming because we would need to sequentially
make predictions with multiple masks at each time step.

2. Sequence averaging

Let’s consider that we use a different mask each time we want to generate a sequence, and then we
average the prediction scores, and compute the argmax (at each time step) to get the actual generated
sequence.

In this case, notice it is not guaranteed that the predicted word at time step t due to averaging the
predictions would lead to the next word (generated by the same process) if we were to feed the
time step t output as input to the time step t + 1. For example, with different dropout masks, if the
probability of 1st time step outputs are: I 40%), he (30%), she (30%), and the probability of the 2nd
time step outputs are: am (30%), is (60%), was (10%). Then the averaged prediction score followed
by argmax will result in the prediction “I is”, but this would be incorrect. A similar concern applies
for output predictions varying in temporal length.

Hence, this approach can not be used to generate a sequence (it has to be done by by sampling a mask
and generating a single sequence). However, this approach may be used to estimate the probability
assigned by the model to a given sequence.

Nonetheless, we run experiments on the PTB dataset using MC-eval (the results are summarized
in Table 8). We start with a simple comparison that compares fraternal dropout with the averaged
mask and the AWD-LSTM 3-layer baseline with a single ﬁxed mask that we call MC1. The MC1
model performs much worse than fraternal dropout. Hence, it would be hard to use MC1 model
in practice because a single sample is inaccurate. We also check MC-eval for a larger number of
models (MC50) (50 models were used since we were not able to ﬁt more models simultaneously on
a single GPU). The ﬁnal results for MC50 are worse than the baseline which uses the averaged mask.
For comparison, we also evaluate MC10. Note that no ﬁne-tuning is used for the above experiments.

Model
MC1
MC10
MC50
Baseline (average mask)
Fraternal dropout

Validation
92.2 (± 0.5)
66.2 (± 0.2)
64.4 (± 0.1)
60.7
59.8

Test
89.2 (± 0.5)
63.7 (± 0.2)
62.1 (± 0.1)
58.8
58.0

Table 8: Appendix: Monte Carlo evaluation. Perplexity on Penn Treebank word level language
modeling task using Monte Carlo sampling, fraternal dropout or average mask.

12

Published as a conference paper at ICLR 2018

REASONS FOR FOCUSING ON RNNS

The fraternal dropout method is general and may be applied in feed-forward architectures (as shown
in Subsection 5.2 for CIFAR-10 semisupervised example). However, we believe that it is more
powerful in the case of RNNs because:

1. Variance in prediction accumulates among time steps in RNNs and since we share parame-
ters for all time steps, one may use the same κ value at each step. In feed-forward networks
the layers usually do not share parameters and hence one may want to use different κ values
for different layers (which may be hard to tune). The simple way to alleviate this problem
is to apply the regularization term on the pre-softmax predictions only (as shown in the
paper) or use the same κ value for all layers. However, we believe that it may limit possible
gains.

2. The best performing RNN architectures (state-of-the-art) usually use some kind of dropout
(embedding dropout, word dropout, weight dropout etc.), very often with high dropout
rates (even larger than 50% for input word embedding in NLP tasks). However, this is not
true for feed-forward networks. For instance, ResNet architectures very often do not use
dropout at all (probably because batch normalization is often better to use). It can be seen in
the paper (Subsection 5.2, semisupervised CIFAR-10 task) that when unlabeled data is not
used the regular dropout hurts performance and using fraternal dropout seems to improve
just a little.

3. On a ﬁnal note, the Monte Carlo sampling (a well known method that adresses the gap
betweem the train and evaluation mode of dropout) can not be easily applied for RNNs and
fraternal dropout may be seen as an alternative.

To conclude, we believe that when the use of dropout beneﬁts in a given architecture, applying
fraternal dropout should improve performance even more.

As mentioned before, in image recognition tasks, one may experiment with something what we
would temporarily dub fraternal augmentation (even though dropout is not used, one can use random
data augmentation such as random crop or random ﬂip). Hence, one may force a given neural
network to have the same predictions for different augmentations.

13

Published as a conference paper at ICLR 2018

PROOFS

i and st
Remark 1. Let st
as described above. Then,

j be i.i.d. dropout masks and pt(zt, st

i; θ) ∈ Rm be the prediction function

RF D(zt; θ) = Est

i,st
j

(cid:2)(cid:107)pt(zt, st

i; θ) − pt(zt, st

j; θ)(cid:107)2
2

(cid:3) = 2

varst

i

(pt

q(zt, st

i; θ)).

(10)

m
(cid:88)

q=1

Proof. For simplicity of notation, we omit the time index t.
(cid:3)
(cid:2)(cid:107)p(z, si; θ) − p(z, sj; θ)(cid:107)2
(cid:2)(cid:107)p(z, sj; θ)(cid:107)2

RF D(z; θ) = Esi,sj

(cid:2)(cid:107)p(z, si; θ)(cid:107)2

(cid:3) + Esj

2

2

2

(cid:3)

(cid:2)p(z, si; θ)T p(z, sj; θ)(cid:3)

(cid:0)Esi

(cid:2)pq(z, si; θ)2(cid:3) − Esi,sj [pq(z, si; θ)pq(z, sj; θ)](cid:1)

= 2

(cid:0)Esi

(cid:2)pq(z, si; θ)2(cid:3) − Esi [pq(z, si; θ)] Esj [pq(z, si; θ)](cid:1)

= 2

(cid:16)

Esi

(cid:2)pq(z, si; θ)2(cid:3) − Esi [pq(z, si; θ)]2(cid:17)

= Esi
− 2Esi,sj
m
(cid:88)

= 2

q=1
m
(cid:88)

q=1
m
(cid:88)

q=1
m
(cid:88)

q=1

= 2

varsi(pq(z, si; θ)).

Proposition 1. RF D(zt; θ) ≤ 4 ˜RELD(zt; θ).

Proof. Let ¯s := Es[s], then

Rt(zt) := Est
= Est

i,st
j

i,st
j

= 4Est

i,st
j

(cid:2)(cid:107)pt(zt, st
(cid:2)(cid:107)pt(zt, st
(cid:20)
(cid:107)

pt(zt, st

i; θ) − pt(zt, st
j; θ)(cid:107)2
2
i; θ) − pt(zt, ¯s; θ) + pt(zt, ¯s; θ) − pt(zt, st

(cid:3)

i; θ) − pt(zt, ¯s; θ)

+

2

pt(zt, ¯s; θ) − pt(zt, st

2

(cid:3)

j; θ)(cid:107)2
2
j; θ)

(cid:21)

(cid:107)2
2

.

Then using Jensen’s inequality,
(cid:20) 1
2

Rt(zt) ≤ 4Est

i,st
j

(cid:107)pt(zt, st

i; θ) − pt(zt, ¯s; θ)(cid:107)2

2 +

(cid:107)pt(zt, ¯s; θ) − pt(zt, st

j; θ)(cid:107)2
2

1
2

= 4Est

i

(cid:2)(cid:107)pt(zt, st

i; θ) − pt(zt, ¯s; θ)(cid:107)2
2

(cid:3) = 4 ˜RELD(zt; θ).

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(cid:21)

(20)

(21)

14

8
1
0
2
 
r
a

M
 
8
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
6
6
0
0
0
.
1
1
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

FRATERNAL DROPOUT

1Jagiellonian University
2MILA, Université de Montréal
3CIFAR Senior Fellow

Konrad ˙Zołna1,2,∗, Devansh Arpit2, Dendi Suhubdy2 & Yoshua Bengio2,3

ABSTRACT

Recurrent neural networks (RNNs) form an important class of architectures
among neural networks useful for language modeling and sequential prediction.
However, optimizing RNNs is known to be harder compared to feed-forward neu-
ral networks. A number of techniques have been proposed in literature to address
this problem. In this paper we propose a simple technique called fraternal dropout
that takes advantage of dropout to achieve this goal. Speciﬁcally, we propose to
train two identical copies of an RNN (that share parameters) with different dropout
masks while minimizing the difference between their (pre-softmax) predictions.
In this way our regularization encourages the representations of RNNs to be in-
variant to dropout mask, thus being robust. We show that our regularization term is
upper bounded by the expectation-linear dropout objective which has been shown
to address the gap due to the difference between the train and inference phases of
dropout. We evaluate our model and achieve state-of-the-art results in sequence
modeling tasks on two benchmark datasets – Penn Treebank and Wikitext-2. We
also show that our approach leads to performance improvement by a signiﬁcant
margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10)
tasks.

1

INTRODUCTION

Recurrent neural networks (RNNs) like long short-term memory (LSTM; Hochreiter & Schmidhu-
ber (1997)) networks and gated recurrent unit (GRU; Chung et al. (2014)) are popular architectures
for sequence modeling tasks like language generation, translation, speech synthesis, and machine
comprehension. However, they are harder to optimize compared to feed-forward networks due to
challenges like variable length input sequences, repeated application of the same transition operator
at each time step, and largely-dense embedding matrix that depends on the vocabulary size. Due to
these optimization challenges in RNNs, the application of batch normalization and its variants (layer
normalization, recurrent batch normalization, recurrent normalization propagation) have not been as
successful as their counterparts in feed-forward networks (Laurent et al., 2016), although they do
considerably provide performance gains. Similarly, naive application of dropout (Srivastava et al.,
2014) has been shown to be ineffective in RNNs (Zaremba et al., 2014). Therefore, regularization
techniques for RNNs is an active area of research.

To address these challenges, Zaremba et al. (2014) proposed to apply dropout only to the non-
recurrent connections in multi-layer RNNs. Variational dropout (Gal & Ghahramani (2016)) uses
the same dropout mask throughout a sequence during training. DropConnect (Wan et al., 2013)
applies the dropout operation on the weight matrices. Zoneout (Krueger et al. (2016)), in a similar
spirit with dropout, randomly chooses to use the previous time step hidden state instead of using
the current one. Similarly as a substitute for batch normalization, layer normalization normalizes
the hidden units within each sample to have zero mean and unit standard deviation. Recurrent batch
normalization applies batch normalization but with unshared mini-batch statistics for each time step
(Cooijmans et al., 2016).

∗konrad.zolna@gmail.com

1

Published as a conference paper at ICLR 2018

Merity et al. (2017a) and Merity et al. (2017b) on the other hand show that activity regulariza-
tion (AR) and temporal activation regularization (TAR)1 are also effective methods for regularizing
LSTMs. Another more recent way of regularizing RNNs, that is similar in spirit to the approach we
take, involves minimizing the difference between the hidden states of the original and the auxiliary
network Serdyuk et al. (2017).

In this paper we propose a simple regularization based on dropout that we call fraternal dropout,
where we minimize an equally weighted sum of prediction losses from two identical copies of the
same LSTM with different dropout masks, and add as a regularization the (cid:96)2 difference between the
predictions (pre-softmax) of the two networks. We analytically show that our regularization objec-
tive is equivalent to minimizing the variance in predictions from different i.i.d. dropout masks; thus
encouraging the predictions to be invariant to dropout masks. We also discuss how our regularization
is related to expectation linear dropout Ma et al. (2016), Π-model Laine & Aila (2016) and activity
regularization Merity et al. (2017b), and empirically show that our method provides non-trivial gains
over these related methods which we explain furthermore in our ablation study (Section 5).

2 FRATERNAL DROPOUT

Dropout is a powerful regularization for neural networks. It is usually more effective on densely
connected layers because they suffer more from overﬁtting compared with convolution layers where
the parameters are shared. For this reason dropout is an important regularization for RNNs. How-
ever, dropout has a gap between its training and inference phase since the latter phase assumes linear
activations to correct for the factor by which the expected value of each activation would be different
Ma et al. (2016). In addition, the prediction of models with dropout generally vary with different
dropout mask. However, the desirable property in such cases would be to have ﬁnal predictions be
invariant to dropout masks.

As such, the idea behind fraternal dropout is to train a neural network model in a way that encour-
ages the variance in predictions under different dropout masks to be as small as possible. Specif-
ically, consider we have an RNN model denoted by M(θ) that takes as input X, where θ denotes
i; θ) ∈ Rm be the prediction of the model for input sample X at
the model parameters. Let pt(zt, st
i and current input zt, where zt is a function of X and the hidden states
time t, for dropout mask st
corresponding to the previous time steps. Similarly, let (cid:96)t(pt(zt, st
i; θ), Y) be the corresponding tth
time step loss value for the overall input-target sample pair (X, Y).

Then in fraternal dropout, we simultaneously feed-forward the input sample X through two identical
i and st
copies of the RNN that share the same parameters θ but with different dropout masks st
j at
each time step t. This yields two loss values at each time step t given by (cid:96)t(pt(zt, st
i; θ), Y), and
(cid:96)t(pt(zt, st

j; θ), Y). Then the overall loss function of fraternal dropout is given by,

(cid:96)F D(X, Y) =

(cid:0)(cid:96)t(pt(zt, st

i; θ), Y) + (cid:96)t(pt(zt, st

j; θ), Y)(cid:1) +

RF D(zt; θ)

κ
mT

T
(cid:88)

t=1

T
(cid:88)

t=1

1
2

where κ is the regularization coefﬁcient, m is the dimensions of pt(zt, st
fraternal dropout regularization given by,

i; θ) and RF D(zt; θ) is the

RF D(zt; θ) := Est

i,st
j

(cid:2)(cid:107)pt(zt, st

i; θ) − pt(zt, st

j; θ)(cid:107)2
2

(cid:3) .

We use Monte Carlo sampling to approximate RF D(zt; θ) where pt(zt, st
the same as the one used to calculate (cid:96)t values. Hence, the additional computation is negligible.

i; θ) and pt(zt, st

j; θ) are

We note that the regularization term of our objective is equivalent to minimizing the variance in the
prediction function with different dropout masks as shown below (proof in the appendix).

1TAR and Zoneout are similar in their motivations because both leads to adjacent time step hidden states to

be close on average.

(1)

(2)

2

Published as a conference paper at ICLR 2018

i and st
Remark 1. Let st
as described above. Then,

j be i.i.d. dropout masks and pt(zt, st

i; θ) ∈ Rm be the prediction function

RF D(zt; θ) = Est

i,st
j

(cid:2)(cid:107)pt(zt, st

i; θ) − pt(zt, st

j; θ)(cid:107)2
2

(cid:3) = 2

varst

i

(pt

q(zt, st

i; θ)).

(3)

m
(cid:88)

q=1

Note that a generalization of our approach would be to minimize the difference between the predic-
tions of the two networks with different data/model augmentations. However, in this paper we focus
on using different dropout masks and experiment mainly with RNNs2.

3 RELATED WORK

3.1 RELATION TO EXPECTATION LINEAR DROPOUT (ELD)

Ma et al. (2016) analytically showed that the expected error (over samples) between a model’s
expected prediction over all dropout masks, and the prediction using the average mask, is upper
bounded. Based on this result, they propose to explicitly minimize the difference (we have adapted
their regularization to our notations),
RELD(zt; θ) = (cid:107)Es

(cid:2)pt(zt, s; θ)(cid:3) − pt(zt, Es[s]; θ)(cid:107)2

(4)

where s is the dropout mask. However, due to feasibility consideration, they instead propose to use
the following regularization in practice,
˜RELD(zt; θ) = Esi

(cid:2)(cid:107)pt(zt, si; θ) − pt(zt, Es[s]; θ)(cid:107)2

(cid:3) .

(5)

2

Speciﬁcally, this is achieved by feed-forwarding the input twice through the network, with and with-
out dropout mask, and minimizing the main network loss (with dropout) along with the regulariza-
tion term speciﬁed above (but without back-propagating the gradients through the network without
dropout). The goal of Ma et al. (2016) is to minimize the network loss along with the expected dif-
ference between the prediction from individual dropout mask and the prediction from the expected
dropout mask. We note that our regularization objective is upper bounded by the expectation-linear
dropout regularization as shown below (proof in the appendix).
Proposition 1. RF D(zt; θ) ≤ 4 ˜RELD(zt; θ).

This result shows that minimizing the ELD objective indirectly minimizes our regularization term.
Finally as indicated above, they apply the target loss only on the network with dropout. In fact, in our
own ablation studies (see Section 5) we ﬁnd that back-propagating target loss through the network
(without dropout) makes optimizing the model harder. However, in our setting, simultaneously back-
propagating target loss through both networks yields both performance gain as well as convergence
gain. We believe convergence is faster for our regularization because network weights are more
likely to get target based updates from back-propagation in our case. This is especially true for
weight dropout (Wan et al., 2013) since in this case dropped weights do not get updated in the
training iteration.

3.2 RELATION TO Π-MODEL

Laine & Aila (2016) propose Π-model with the goal of improving performance on classiﬁcation
tasks in the semi-supervised setting. They propose a model similar to ours (considering the equiv-
alent deep feed-forward version of our model) except they apply target loss only on one of the net-
works and use time-dependent weighting function ω(t) (while we use constant κ
mT ). The intuition
in their case is to leverage unlabeled data by using them to minimize the difference in prediction be-
tween the two copies of the network with different dropout masks. Further, they also test their model
in the supervised setting but fail to explain the improvements they obtain by using this regularization.

We note that in our case we analytically show that minimizing our regularizer (also used in Π-model)
is equivalent to minimizing the variance in the model predictions (Remark 1). Furthermore, we also
show the relation of our regularizer to expectation linear dropout (Proposition 1). In Section 5, we

2The reasons of our focus on RNNs are described in the appendix.

3

Published as a conference paper at ICLR 2018

study the effects of target based loss on both networks, which is not used in the Π-model. We ﬁnd
that applying target loss on both the networks leads to signiﬁcantly faster convergence. Finally,
we bring to attention that temporal embedding (another model proposed by Laine & Aila (2016),
claimed to be a better version of Π-model for semi-supervised, learning) is intractable in natural
language processing applications because storing averaged predictions over all of the time steps
would be memory exhaustive (since predictions are usually huge - tens of thousands values). On
a ﬁnal note, we argue that in the supervised case, using a time-dependent weighting function ω(t)
instead of a constant value κ
mT is not needed. Since the ground truth labels are known, we have not
observed the problem mentioned by Laine & Aila (2016), that the network gets stuck in a degenerate
solution when ω(t) is too large in earlier epochs of training. We note that it is much easier to search
for an optimal constant value, which is true in our case, as opposed to tuning the time-dependent
function.

Similarity to Π-model makes our method related to other semi-supervised works, mainly Rasmus
et al. (2015) and Sajjadi et al. (2016). Since semi-supervised learning is not a primary focus of this
paper, we refer to Laine & Aila (2016) for more details.

We note that the idea of adding a penalty encouraging the representation to be similar for two
different masks was previously implemented3 by the authors of a Multi-Prediction Deep Boltzmann
Machines (Goodfellow et al., 2013). Nevertheless, the idea is not discussed in their paper.

Another way to address the gap between the train and evaluation mode of dropout is to perform
Monte Carlo sampling of masks and average the predictions during evaluation, and this has been
used for feed-forward networks. We ﬁnd that this technique does not work well for RNNs. The
details of these experiments can be found in the appendix.

4 EXPERIMENTS

4.1 LANGUAGE MODELS

In the case of language modeling we test our model4 on two benchmark datasets – Penn Tree-bank
(PTB) dataset (Marcus et al., 1993) and WikiText-2 (WT2) dataset (Merity et al., 2016). We use
preprocessing as speciﬁed by Mikolov et al. (2010) (for PTB corpus) and Moses tokenizer Koehn
et al. (2007) (for the WT2 dataset).

For both datasets we use the AWD-LSTM 3-layer architecture described in Merity et al. (2017a)5
which we call the baseline model. The number of parameters in the model used for PTB is 24
million as compared to 34 million in the case of WT2 because WT2 has a larger vocabulary size
for which we use a larger embedding matrix. Apart from those differences, the architectures are
identical. When we use fraternal dropout, we simply add our regularization on top of this baseline
model.

Word level Penn Treebank (PTB). Inﬂuenced by Melis et al. (2017), our goal here is to make
sure that fraternal dropout outperforms existing methods not simply because of extensive hyper-
parameter grid search but rather due to its regularization effects. Hence, in our experiments we
leave a vast majority of hyper-parameters used in the baseline model (Melis et al., 2017) unchanged
i.e. embedding and hidden states sizes, gradient clipping value, weight decay and the values used
for all dropout layers (dropout on the word vectors, the output between LSTM layers, the output of
the ﬁnal LSTM, and embedding dropout). However, a few changes are necessary:

• the coefﬁcients for AR and TAR needed to be altered because fraternal dropout also affects
RNNs activation (as explained in Subsection 5.3) – we did not run grid search to obtain the
best values but simply deactivated AR and TAR regularizers;

• since fraternal dropout needs twice as much memory, batch size is halved so the model

needs approximately the same amount of memory and hence ﬁts on the same GPU.

3The implementation is freely available as part of a LISA lab library, Pylearn2. For more details see

github.com/lisa-lab/pylearn2/blob/master/pylearn2/costs/dbm.py#L1175 .

4Our code is available at github.com/kondiz/fraternal-dropout .
5We used the ofﬁcial GitHub repository code for

this paper github.com/salesforce/

awd-lstm-lm .

4

Published as a conference paper at ICLR 2018

Model
Zaremba et al. (2014) - LSTM (medium)
Zaremba et al. (2014) - LSTM (large)
Gal & Ghahramani (2016) - Variational LSTM (medium)
Gal & Ghahramani (2016) - Variational LSTM (large)
Inan et al. (2016) - Variational LSTM
Inan et al. (2016) - Variational RHN
Zilly et al. (2016) - Variational RHN
Melis et al. (2017) - 5-layer RHN
Melis et al. (2017) - 4-layer skip connection LSTM
Merity et al. (2017a) - AWD-LSTM 3-layer (baseline)
Fraternal dropout + AWD-LSTM 3-layer

Parameters Validation Test
82.7
78.4
79.7
75.2
68.5
66.0
65.4
62.2
58.3
57.3
56.8

10M
24M
20M
66M
51M
24M
23M
24M
24M
24M
24M

86.2
82.2
81.9
77.9
71.1
68.1
67.9
64.8
60.9
60.0
58.9

Table 1: Perplexity on Penn Treebank word level language modeling task.

Parameters Validation

Model
Merity et al. (2016) - Variational LSTM + Zoneout
Merity et al. (2016) - Variational LSTM
Inan et al. (2016) - Variational LSTM
Melis et al. (2017) - 5-layer RHN
Melis et al. (2017) - 1-layer LSTM
Melis et al. (2017) - 2-layer skip connection LSTM
Merity et al. (2017a) - AWD-LSTM 3-layer (baseline)
Fraternal dropout + AWD-LSTM 3-layer

20M
20M
28M
24M
24M
24M
34M
34M

108.7
101.7
91.5
78.1
69.3
69.1
68.6
66.8

Test
100.9
96.3
87.0
75.6
65.9
65.9
65.8
64.1

Table 2: Perplexity on WikiText-2 word level language modeling task.

The ﬁnal change in hyper-parameters is to alter the non-monotone interval n used in non-
monotonically triggered averaged SGD (NT-ASGD) optimizer Polyak & Juditsky (1992); Mandt
et al. (2017); Melis et al. (2017). We run a grid search on n ∈ {5, 25, 40, 50, 60} and obtain very
similar results for the largest values (40, 50 and 60) in the candidate set. Hence, our model is trained
longer using ordinary SGD optimizer as compared to the baseline model (Melis et al., 2017).

We evaluate our model using the perplexity metric and compare the results that we obtain against
the existing state-of-the-art results. The results are reported in Table 1. Our approach achieves the
state-of-the-art performance compared with existing benchmarks.

To conﬁrm that the gains are robust to initialization, we run ten experiments for the baseline model
with different seeds (without ﬁne-tuning) for PTB dataset to compute conﬁdence intervals. The
average best validation perplexity is 60.64 ± 0.15 with the minimum value equals 60.33. The same
for test perplexity is 58.32 ± 0.14 and 58.05, respectively. Our score (59.8 validation and 58.0 test
perplexity) beats ordinal dropout minimum values.

We also perform experiments using fraternal dropout with a grid search on all the hyper-parameters
and ﬁnd that it leads to further improvements in performance. The details of this experiment can be
found in section 5.5.

Word level WikiText-2 (WT2). In the case of WikiText-2 language modeling task, we outperform
the current state-of-the-art using the perplexity metric by a signiﬁcant margin. Due to the lack
of computational power, we run a single training procedure for fraternal dropout on WT2 dataset
because it is larger than PTB. In this experiment, we use the best hyper-parameters found for PTB
dataset (κ = 0.1, non-monotone interval n = 60 and halved batch size; the rest of the hyper-
parameters are the same as described in Melis et al. (2017) for WT2). The ﬁnal results are presented
in Table 2.

4.2

IMAGE CAPTIONING

We also apply fraternal dropout on an image captioning task. We use the well-known show and tell
model as a baseline6 (Vinyals et al., 2014). We emphasize that in the image captioning task, the

6We used PyTorch implementation with default hyper-parameters from github.com/ruotianluo/

neuraltalk2.pytorch .

5

Published as a conference paper at ICLR 2018

Model
Show and Tell Xu et al. (2015)
Baseline
Fraternal dropout, κ = 0.015
Fraternal dropout, κ = 0.005

BLEU-1 BLEU-2 BLEU-3 BLEU-4

66.6
68.8
69.3
69.3

46.1
50.8
51.4
51.5

32.9
36.1
36.6
36.9

24.6
25.6
26.1
26.3

Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is
the only difference between models. The rest of hyper-parameters are the same.

image encoder and sentence decoder architectures are usually learned together. Since we want to
focus on the beneﬁts of using fraternal dropout in RNNs we use frozen pretrained ResNet-101 (He
et al., 2015) model as our image encoder. It means that our results are not directly comparable with
other state-of-the-art methods, however we report results for the original methods so readers can see
that our baseline performs well. The ﬁnal results are presented in Table 3.

We argue that in this task smaller κ values are optimal because the image captioning encoder is given
all information in the beginning and hence the variance of consecutive predictions is smaller that in
unconditioned natural language processing tasks. Fraternal dropout may beneﬁts here mainly due
to averaging gradients for different mask and hence updating weights more frequently.

5 ABLATION STUDIES

In this section, the goal is to study existing methods closely related to ours – expectation linear
dropout Ma et al. (2016), Π-model Laine & Aila (2016) and activity regularization Merity et al.
(2017b). All of our experiments for ablation studies, which apply a single layer LSTM, use the
same hyper-parameters and model architecture7 as Melis et al. (2017).

5.1 EXPECTATION-LINEAR DROPOUT (ELD)

The relation with expectation-linear dropout Ma et al. (2016) has been discussed in Section 2. Here
we perform experiments to study the difference in performance when using the ELD regularization
versus our regularization (FD). In addition to ELD, we also study a modiﬁcation (ELDM) of ELD
which applies target loss to both copies of LSTMs in ELD similar to FD (notice in their case they
only have dropout on one LSTM). Finally we also evaluate a baseline model without any of these
regularizations. The learning dynamics curves are shown in Figure 1. Our regularization performs
better in terms of convergence compared with other methods. In terms of generalization, we ﬁnd
that FD is similar to ELD, but baseline and ELDM are much worse. Interestingly, looking at the
train and validation curves together, ELDM seems to be suffering from optimization problems.

5.2 Π-MODEL

Since Π-model Laine & Aila (2016) is similar to our algorithm (even though it is designed for
semi-supervised learning in feed-forward networks), we study the difference in performance with
Π-model8 both qualitatively and quantitatively to establish the advantage of our approach. First,
we run both single layer LSTM and 3-layer AWD-LSTM on PTB task to check how their model
compares with ours in the case of language modeling. The results are shown in Figure 1 and 2. We
ﬁnd that our model converges signiﬁcantly faster than Π-model. We believe this happens because we

7We use a batch size of 64, truncated back-propagation with 35 time steps, a constant zero state is provided
as the initial state with probability 0.01 (similar to Melis et al. (2017)), SGD with learning rate 30 (no mo-
mentum) which is multiplied by 0.1 whenever validation performance does not improve ever during 20 epochs,
weight dropout on the hidden to hidden matrix 0.5, dropout every word in a mini-batch with probability 0.1,
embedding dropout 0.65, output dropout 0.4 (ﬁnal value of LSTM), gradient clipping of 0.25, weight decay
1.2 × 10−6, input embedding size of 655, the input/output size of LSTM is the same as embedding size (655)
and the embedding weights are tied (Inan et al., 2016; Press & Wolf, 2016).

8We use a constant function ω(t) = κ

mT as a coefﬁcient for Π-model (similar to our regularization term).
Hence, the focus of our experiment is to evaluate the difference in performance when target loss is back-
propagated through one of the networks (Π-model) vs. both (ours). Additionally, we ﬁnd that tuning a function
instead of using a constant coefﬁcient is infeasible.

6

Published as a conference paper at ICLR 2018

Figure 1: Ablation study: Train (left) and validation (right) perplexity on PTB word level modeling
with single layer LSTM (10M parameters). These curves study the learning dynamics of the baseline
model, Π-model, Expectation-linear dropout (ELD), Expectation-linear dropout with modiﬁcation
(ELDM) and fraternal dropout (FD, our algorithm). We ﬁnd that FD converges faster than the
regularizers in comparison, and generalizes at par.

Figure 2: Ablation study: Validation perplexity
on PTB word level modeling for Π-model and
fraternal dropout. We ﬁnd that FD converges
faster and generalizes at par.

Figure 3: Ablation study: Average hidden state
activation is reduced when any of the regular-
izer described is used. The y-axis is the value of
d (cid:107)m · ht(cid:107)2
1
2.

back-propagate the target loss through both networks (in contrast to Π-model) that leads to weights
getting updated using target-based gradients more often.

Even though we designed our algorithm speciﬁcally to address problems in RNNs, to have a fair
comparison, we compare with Π-model on a semi-supervised task which is their goal. Speciﬁ-
cally, we use the CIFAR-10 dataset that consists of 32 × 32 images from 10 classes. Following the
usual splits used in semi-supervised learning literature, we use 4 thousand labeled and 41 thousand
unlabeled samples for training, 5 thousand labeled samples for validation and 10 thousand labeled
samples for test set. We use the original ResNet-56 (He et al., 2015) architecture. We run grid search
on κ ∈ {0.05, 0.1, 0.15, 0.2}, dropout rates in {0.05, 0.1, 0.15, 0.2} and leave the rest of the hyper-
parameters unchanged. We additionally check importance of using unlabeled data. The results are
reported in Table 4. We ﬁnd that our algorithm performs at par with Π-model. When unlabeled data
is not used, fraternal dropout provides slightly better results as compared to traditional dropout.

5.3 ACTIVITY REGULARIZATION AND TEMPORAL ACTIVITY REGULARIZATION ANALYSIS

The authors of Merity et al. (2017b) study the importance of activity regularization (AR)9 and tem-
poral activity regularization (TAR) in LSTMs given as,
α
d
β
d

RT AR(zt; θ) =

(cid:107)ht − ht−1(cid:107)2
2

RAR(zt; θ) =

(cid:107)ht(cid:107)2
2

(7)

(6)

9We used (cid:107)m · ht(cid:107)2

2, where m is the dropout mask, in our actual experiments with AR because it was

implemented as such in the original paper’s Github repository Merity et al. (2017a).

7

Published as a conference paper at ICLR 2018

Dropout rate Unlabeled data

Model
Traditional dropout
No dropout
Fraternal dropout (κ = 0.05)
Traditional dropout + Π-model
Fraternal dropout (κ = 0.15)

0.1
0.0
0.05
0.1
0.1

No
No
No
Yes
Yes

Validation
78.4 (± 0.25)
78.8 (± 0.59)
79.3 (± 0.38)
80.2 (± 0.33)
80.5 (± 0.18)

Test
76.9 (± 0.31)
77.1 (± 0.3)
77.6 (± 0.35)
78.5 (± 0.46)
79.1 (± 0.37)

Table 4: Ablation study: Accuracy on altered (semi-supervised) CIFAR-10 dataset for ResNet-56
based models. We ﬁnd that our algorithm performs at par with Π-model. When unlabeled data
is not used traditional dropout hurts performance while fraternal dropout provides slightly better
results. It means that our methods may be beneﬁcial when we lack data and have to use additional
regularizing methods.

Figure 4: Ablation study: Train (left) and validation (right) perplexity on PTB word level model-
ing with single layer LSTM (10M parameters). These curves study the learning dynamics of the
baseline model, temporal activity regularization (TAR), prediction regularization (PR), activity reg-
ularization (AR) and fraternal dropout (FD, our algorithm). We ﬁnd that FD both converges faster
and generalizes better than the regularizers in comparison.

where ht ∈ Rd is the LSTM’s output activation at time step t (hence depends on both current input
zt and the model parameters θ). Notice that AR and TAR regularizations are applied on the output of
the LSTM, while our regularization is applied on the pre-softmax output pt(zt, st
i; θ) of the LSTM.
However, since our regularization can be decomposed as

RF D(zt; θ) = Esi,sj
= Esi,sj

(cid:2)(cid:107)pt(zt, st
(cid:2)(cid:107)pt(zt, st

i; θ) − pt(ht, st
i; θ)(cid:107)2

2 + (cid:107)pt(zt, st

j; θ)(cid:107)2
2
j; θ)(cid:107)2

(cid:3)

2 − 2pt(zt, st

i; θ)T pt(zt, st

(8)
j; θ)(cid:3)
(9)

and encapsulates an (cid:96)2 term along with the dot product term, we perform experiments to conﬁrm
that the gains in our approach is not due to the (cid:96)2 regularization alone. A similar argument goes
for the TAR objective. We run a grid search on α ∈ {1, 2, . . . , 12}, β ∈ {1, 2, . . . , 12}, which
include the hyper-parameters mentioned in Merity et al. (2017a). For our regularization, we use
κ ∈ {0.05, 0.1, . . . , 0.4}. Furthermore, we also compare with a regularization (PR) that regularizes
2 to further rule-out any gains only from (cid:96)2 regularization. Based on this grid search,
(cid:107)pt(zt, st
we pick the best model on the validation set for all the regularizations, and additionally report a
baseline model without any of these four mentioned regularizations. The learning dynamics is shown
in Figure 4. Our regularization performs better both in terms of convergence and generalization
compared with other methods. Average hidden state activation is reduced when any of the regularizer
described is applied (see Figure 3).

i; θ)(cid:107)2

5.4

IMPROVEMENTS USING FINE-TUNING

We conﬁrm that models trained with fraternal dropout beneﬁt from the NT-ASGD ﬁne-tuning step
(as also used in Merity et al. (2017a)). However, this is a very time-consuming practice and since dif-
ferent hyper-parameters may be used in this additional part of the learning procedure, the probability
of obtaining better results due to the extensive grid search is higher. Hence, in our experiments we

8

Published as a conference paper at ICLR 2018

PTB

WT2

Dropout
Traditional
Traditional
Fraternal
Fraternal
Fraternal

Fine-tuning Validation Test Validation Test
66.0
65.8
65.3
64.1
–

None
One
None
One
Two

69.1
68.6
68.3
66.8
–

60.7
60.0
59.8
58.9
58.5

58.8
57.3
58.0
56.8
56.2

Table 5: Ablation study: Importance of ﬁne-tuning for AWD-LSTM 3-layer model. Perplexity for
the Penn Treebank and WikiText-2 language modeling tasks.

Hyper-parameter
batch size
non-monotone interval
κ – FD or ELD strength
weight decay

Possible values
[10, 20, 30, 40]
[5, 10, 20, 40, 60, 100]
U (0, 0.3)
U (0.6 × 10−6, 2.4 × 10−6)

Table 6: Ablation study: Candidate hyper-parameters possible used in the grid search for comparing
fraternal dropout and expectation linear dropout. U (a, b) is the uniform distribution on the interval
[a, b]. For ﬁnite sets, each value is drawn with equal probability.

Model
Expectation linear dropout
Fraternal dropout

Best Top5 avg Top10 avg Beating baseline runs (out of)
59.4
59.4

6 (208)
14 (203)

60.1
59.6

60.5
59.9

Table 7: Ablation study: Fraternal dropout and expectation linear dropout comparison. Perplex-
ity on the Penn Treebank validation dateset. Fraternal dropout is more robust to different hyper-
parameters choice as twice as much runs ﬁnished performing better than the baseline model (60.7).

use the same ﬁne-tuning procedure as implemented in the ofﬁcial repository (even fraternal dropout
was not used). We present the importance of ﬁne-tuning in Table 5.

5.5 FRATERNAL DROPOUT AND EXPECTATION LINEAR DROPOUT COMPARISON

We perform extensive grid search for the baseline model from Subsection 4.1 (an AWD-LSTM
3-layer architecture) trained with either fraternal dropout or expectation linear dropout regulariza-
tions, to further contrast the performance of these two methods. The experiments are run without
ﬁne-tuning on the PTB dataset.

In each run, all ﬁve dropout rates are randomly altered (they are set to their original value, as in
Merity et al. (2017a), multiplied by a value drawn from the uniform distribution on the interval
[0.5, 1.5]) and the rest of the hyper-parameters are drawn as shown in Table 6. As in Subsection 4.1,
AR and TAR regularizers are deactivated.

Together we run more than 400 experiments. The results are presented in Table 7. Both FD and
ELD perform better than the baseline model that instead uses AR and TAR regularizers. Hence,
we conﬁrm our previous ﬁnding (see Subsection 5.3) that both FD and ELD are better. However,
as found previously for smaller model in Subsection 5.1, the convergence of FD is faster than that
of ELD. Additionally, fraternal dropout is more robust to different hyper-parameters choice (more
runs performing better than the baseline and better average for top performing runs).

6 CONCLUSION

In this paper we propose a simple regularization method for RNNs called fraternal dropout that acts
as a regularization by reducing the variance in model predictions across different dropout masks. We
show that our model achieves state-of-the-art results on benchmark language modeling tasks along
with faster convergence. We also analytically study the relationship between our regularization and
expectation linear dropout Ma et al. (2016). We perform a number of ablation studies to evaluate

9

Published as a conference paper at ICLR 2018

our model from different aspects and carefully compare it with related methods both qualitatively
and quantitatively.

ACKNOWLEDGEMENTS

The authors would like to acknowledge the support of the following agencies for research funding
and computing support: NSERC, CIFAR, and IVADO. We would like to thank Rosemary Nan Ke
and Philippe Lacaille for their thoughts and comments throughout the project. We would also like
to thank Stanisław Jastrz˛ebski† and Evan Racah† for useful discussions.

REFERENCES

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.

Tim Cooijmans, Nicolas Ballas, César Laurent, and Aaron C. Courville. Recurrent batch normal-
ization. CoRR, abs/1603.09025, 2016. URL http://arxiv.org/abs/1603.09025.

Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. In Advances in neural information processing systems, pp. 1019–1027, 2016.

Ian Goodfellow, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Multi-prediction deep boltz-
mann machines. In Advances in Neural Information Processing Systems, pp. 548–556, 2013.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-

nition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):

1735–1780, 1997.

Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classiﬁers: A

loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. Moses: Open source
toolkit for statistical machine translation. In Proceedings of the 45th annual meeting of the ACL
on interactive poster and demonstration sessions, pp. 177–180. Association for Computational
Linguistics, 2007.

David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rose-
mary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, et al. Zoneout:
Regularizing rnns by randomly preserving hidden activations. arXiv preprint arXiv:1606.01305,
2016.

Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint

arXiv:1610.02242, 2016.

César Laurent, Gabriel Pereyra, Philémon Brakel, Ying Zhang, and Yoshua Bengio. Batch normal-
ized recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE
International Conference on, pp. 2657–2661. IEEE, 2016.

Xuezhe Ma, Yingkai Gao, Zhiting Hu, Yaoliang Yu, Yuntian Deng, and Eduard Hovy. Dropout with

expectation-linear regularization. arXiv preprint arXiv:1609.08017, 2016.

Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate

bayesian inference. arXiv preprint arXiv:1704.04289, 2017.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated

corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.

†equal contribution ¨(cid:94)

10

Published as a conference paper at ICLR 2018

Gábor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language

models. arXiv preprint arXiv:1707.05589, 2017.

Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. CoRR, abs/1609.07843, 2016. URL http://arxiv.org/abs/1609.07843.

Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm lan-

guage models. arXiv preprint arXiv:1708.02182, 2017a.

Stephen Merity, Bryan McCann, and Richard Socher. Revisiting activation regularization for lan-

guage rnns. arXiv preprint arXiv:1708.01009, 2017b.

Tomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recurrent

neural network based language model. In Interspeech, volume 2, pp. 3, 2010.

Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.

SIAM Journal on Control and Optimization, 30(4):838–855, 1992.

Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint

arXiv:1608.05859, 2016.

Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko.

Semi-
supervised learning with ladder network. CoRR, abs/1507.02672, 2015. URL http://arxiv.
org/abs/1507.02672.

Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transfor-
mations and perturbations for deep semi-supervised learning. In Advances in Neural Information
Processing Systems, pp. 1163–1171, 2016.

Dmitriy Serdyuk, Nan Rosemary Ke, Alessandro Sordoni, Adam Trischler, Chris Pal, and Yoshua

Bengio. Twin networks: Matching the future for sequence generation. 2017.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. Journal of machine learning
research, 15(1):1929–1958, 2014.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image
caption generator. CoRR, abs/1411.4555, 2014. URL http://arxiv.org/abs/1411.
4555.

Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural
In Proceedings of the 30th international conference on machine

networks using dropconnect.
learning (ICML-13), pp. 1058–1066, 2013.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation
with visual attention. CoRR, abs/1502.03044, 2015. URL http://arxiv.org/abs/1502.
03044.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.

arXiv preprint arXiv:1409.2329, 2014.

Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, and Jürgen Schmidhuber. Recurrent

highway networks. arXiv preprint arXiv:1607.03474, 2016.

11

Published as a conference paper at ICLR 2018

APPENDIX

MONTE CARLO EVALUATION

A well known way to address the gap between the train and evaluation mode of dropout is to perform
Monte Carlo sampling of masks and average the predictions during evaluation (MC-eval), and this
has been used for feed-forward networks. Since fraternal dropout addresses the same problem, we
would like to clarify that it is not straight-forward and feasible to apply MC-eval for RNNs. In
feed-forward networks, we average the output prediction scores from different masks. However, in
the case RNNs (for next step predictions), there is more than one way to perform such evaluation,
but each one is problematic. They are as follows:

1. Online averaging

Consider that we ﬁrst make the prediction at time step 1 using different masks by averaging the
prediction score. Then we use this output to feed as input to the time step 2, then use different masks
at time step 2 to generate the output at time step 2, and so on. But in order to do so, because of
the way RNNs work, we also need to feed the previous time hidden state to time step 2. One way
would be to average the hidden states over different masks at time step 1. But the hidden space can
in general be highly nonlinear, and it is not clear if averaging in this space is a good strategy. This
approach is not justiﬁed.

Besides, this strategy as a whole is extremely time consuming because we would need to sequentially
make predictions with multiple masks at each time step.

2. Sequence averaging

Let’s consider that we use a different mask each time we want to generate a sequence, and then we
average the prediction scores, and compute the argmax (at each time step) to get the actual generated
sequence.

In this case, notice it is not guaranteed that the predicted word at time step t due to averaging the
predictions would lead to the next word (generated by the same process) if we were to feed the
time step t output as input to the time step t + 1. For example, with different dropout masks, if the
probability of 1st time step outputs are: I 40%), he (30%), she (30%), and the probability of the 2nd
time step outputs are: am (30%), is (60%), was (10%). Then the averaged prediction score followed
by argmax will result in the prediction “I is”, but this would be incorrect. A similar concern applies
for output predictions varying in temporal length.

Hence, this approach can not be used to generate a sequence (it has to be done by by sampling a mask
and generating a single sequence). However, this approach may be used to estimate the probability
assigned by the model to a given sequence.

Nonetheless, we run experiments on the PTB dataset using MC-eval (the results are summarized
in Table 8). We start with a simple comparison that compares fraternal dropout with the averaged
mask and the AWD-LSTM 3-layer baseline with a single ﬁxed mask that we call MC1. The MC1
model performs much worse than fraternal dropout. Hence, it would be hard to use MC1 model
in practice because a single sample is inaccurate. We also check MC-eval for a larger number of
models (MC50) (50 models were used since we were not able to ﬁt more models simultaneously on
a single GPU). The ﬁnal results for MC50 are worse than the baseline which uses the averaged mask.
For comparison, we also evaluate MC10. Note that no ﬁne-tuning is used for the above experiments.

Model
MC1
MC10
MC50
Baseline (average mask)
Fraternal dropout

Validation
92.2 (± 0.5)
66.2 (± 0.2)
64.4 (± 0.1)
60.7
59.8

Test
89.2 (± 0.5)
63.7 (± 0.2)
62.1 (± 0.1)
58.8
58.0

Table 8: Appendix: Monte Carlo evaluation. Perplexity on Penn Treebank word level language
modeling task using Monte Carlo sampling, fraternal dropout or average mask.

12

Published as a conference paper at ICLR 2018

REASONS FOR FOCUSING ON RNNS

The fraternal dropout method is general and may be applied in feed-forward architectures (as shown
in Subsection 5.2 for CIFAR-10 semisupervised example). However, we believe that it is more
powerful in the case of RNNs because:

1. Variance in prediction accumulates among time steps in RNNs and since we share parame-
ters for all time steps, one may use the same κ value at each step. In feed-forward networks
the layers usually do not share parameters and hence one may want to use different κ values
for different layers (which may be hard to tune). The simple way to alleviate this problem
is to apply the regularization term on the pre-softmax predictions only (as shown in the
paper) or use the same κ value for all layers. However, we believe that it may limit possible
gains.

2. The best performing RNN architectures (state-of-the-art) usually use some kind of dropout
(embedding dropout, word dropout, weight dropout etc.), very often with high dropout
rates (even larger than 50% for input word embedding in NLP tasks). However, this is not
true for feed-forward networks. For instance, ResNet architectures very often do not use
dropout at all (probably because batch normalization is often better to use). It can be seen in
the paper (Subsection 5.2, semisupervised CIFAR-10 task) that when unlabeled data is not
used the regular dropout hurts performance and using fraternal dropout seems to improve
just a little.

3. On a ﬁnal note, the Monte Carlo sampling (a well known method that adresses the gap
betweem the train and evaluation mode of dropout) can not be easily applied for RNNs and
fraternal dropout may be seen as an alternative.

To conclude, we believe that when the use of dropout beneﬁts in a given architecture, applying
fraternal dropout should improve performance even more.

As mentioned before, in image recognition tasks, one may experiment with something what we
would temporarily dub fraternal augmentation (even though dropout is not used, one can use random
data augmentation such as random crop or random ﬂip). Hence, one may force a given neural
network to have the same predictions for different augmentations.

13

Published as a conference paper at ICLR 2018

PROOFS

i and st
Remark 1. Let st
as described above. Then,

j be i.i.d. dropout masks and pt(zt, st

i; θ) ∈ Rm be the prediction function

RF D(zt; θ) = Est

i,st
j

(cid:2)(cid:107)pt(zt, st

i; θ) − pt(zt, st

j; θ)(cid:107)2
2

(cid:3) = 2

varst

i

(pt

q(zt, st

i; θ)).

(10)

m
(cid:88)

q=1

Proof. For simplicity of notation, we omit the time index t.
(cid:3)
(cid:2)(cid:107)p(z, si; θ) − p(z, sj; θ)(cid:107)2
(cid:2)(cid:107)p(z, sj; θ)(cid:107)2

RF D(z; θ) = Esi,sj

(cid:2)(cid:107)p(z, si; θ)(cid:107)2

(cid:3) + Esj

2

2

2

(cid:3)

(cid:2)p(z, si; θ)T p(z, sj; θ)(cid:3)

(cid:0)Esi

(cid:2)pq(z, si; θ)2(cid:3) − Esi,sj [pq(z, si; θ)pq(z, sj; θ)](cid:1)

= 2

(cid:0)Esi

(cid:2)pq(z, si; θ)2(cid:3) − Esi [pq(z, si; θ)] Esj [pq(z, si; θ)](cid:1)

= 2

(cid:16)

Esi

(cid:2)pq(z, si; θ)2(cid:3) − Esi [pq(z, si; θ)]2(cid:17)

= Esi
− 2Esi,sj
m
(cid:88)

= 2

q=1
m
(cid:88)

q=1
m
(cid:88)

q=1
m
(cid:88)

q=1

= 2

varsi(pq(z, si; θ)).

Proposition 1. RF D(zt; θ) ≤ 4 ˜RELD(zt; θ).

Proof. Let ¯s := Es[s], then

Rt(zt) := Est
= Est

i,st
j

i,st
j

= 4Est

i,st
j

(cid:2)(cid:107)pt(zt, st
(cid:2)(cid:107)pt(zt, st
(cid:20)
(cid:107)

pt(zt, st

i; θ) − pt(zt, st
j; θ)(cid:107)2
2
i; θ) − pt(zt, ¯s; θ) + pt(zt, ¯s; θ) − pt(zt, st

(cid:3)

i; θ) − pt(zt, ¯s; θ)

+

2

pt(zt, ¯s; θ) − pt(zt, st

2

(cid:3)

j; θ)(cid:107)2
2
j; θ)

(cid:21)

(cid:107)2
2

.

Then using Jensen’s inequality,
(cid:20) 1
2

Rt(zt) ≤ 4Est

i,st
j

(cid:107)pt(zt, st

i; θ) − pt(zt, ¯s; θ)(cid:107)2

2 +

(cid:107)pt(zt, ¯s; θ) − pt(zt, st

j; θ)(cid:107)2
2

1
2

= 4Est

i

(cid:2)(cid:107)pt(zt, st

i; θ) − pt(zt, ¯s; θ)(cid:107)2
2

(cid:3) = 4 ˜RELD(zt; θ).

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(cid:21)

(20)

(21)

14

