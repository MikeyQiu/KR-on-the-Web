7
1
0
2
 
n
a
J
 
9
1
 
 
]

V
C
.
s
c
[
 
 
5
v
1
6
0
0
0
.
6
0
6
1
:
v
i
X
r
a

Hierarchical Question-Image Co-Attention

for Visual Question Answering

Jiasen Lu∗, Jianwei Yang∗, Dhruv Batra∗† , Devi Parikh∗†
∗ Virginia Tech, † Georgia Institute of Technology
{jiasenlu, jw2yang, dbatra, parikh}@vt.edu

Abstract

A number of recent works have proposed attention models for Visual Question
Answering (VQA) that generate spatial maps highlighting image regions relevant to
answering the question. In this paper, we argue that in addition to modeling “where
to look” or visual attention, it is equally important to model “what words to listen
to” or question attention. We present a novel co-attention model for VQA that
jointly reasons about image and question attention. In addition, our model reasons
about the question (and consequently the image via the co-attention mechanism)
in a hierarchical fashion via a novel 1-dimensional convolution neural networks
(CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to
60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the
performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.1.

1

Introduction

Visual Question Answering (VQA) [2, 7, 16, 17, 29] has emerged as a prominent multi-discipline
research problem in both academia and industry. To correctly answer visual questions about an
image, the machine needs to understand both the image and question. Recently, visual attention based
models [20, 23–25] have been explored for VQA, where the attention mechanism typically produces
a spatial map highlighting image regions relevant to answering the question.

So far, all attention models for VQA in literature have focused on the problem of identifying “where
to look” or visual attention. In this paper, we argue that the problem of identifying “which words to
listen to” or question attention is equally important. Consider the questions “how many horses are
in this image?” and “how many horses can you see in this image?". They have the same meaning,
essentially captured by the ﬁrst three words. A machine that attends to the ﬁrst three words would
arguably be more robust to linguistic variations irrelevant to the meaning and answer of the question.
Motivated by this observation, in addition to reasoning about visual attention, we also address the
problem of question attention. Speciﬁcally, we present a novel multi-modal attention model for VQA
with the following two unique features:

Co-Attention: We propose a novel mechanism that jointly reasons about visual attention and question
attention, which we refer to as co-attention. Unlike previous works, which only focus on visual
attention, our model has a natural symmetry between the image and question, in the sense that the
image representation is used to guide the question attention and the question representation(s) are
used to guide image attention.

Question Hierarchy: We build a hierarchical architecture that co-attends to the image and question
at three levels: (a) word level, (b) phrase level and (c) question level. At the word level, we embed the
words to a vector space through an embedding matrix. At the phrase level, 1-dimensional convolution
neural networks are used to capture the information contained in unigrams, bigrams and trigrams.

1The source code can be downloaded from https://github.com/jiasenlu/HieCoAttenVQA

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Figure 1: Flowchart of our proposed hierarchical co-attention model. Given a question, we extract its word
level, phrase level and question level embeddings. At each level, we apply co-attention on both the image and
question. The ﬁnal answer prediction is based on all the co-attended image and question features.

Speciﬁcally, we convolve word representations with temporal ﬁlters of varying support, and then
combine the various n-gram responses by pooling them into a single phrase level representation. At
the question level, we use recurrent neural networks to encode the entire question. For each level
of the question representation in this hierarchy, we construct joint question and image co-attention
maps, which are then combined recursively to ultimately predict a distribution over the answers.

Overall, the main contributions of our work are:

• We propose a novel co-attention mechanism for VQA that jointly performs question-guided
visual attention and image-guided question attention. We explore this mechanism with two
strategies, parallel and alternating co-attention, which are described in Sec. 3.3;

• We propose a hierarchical architecture to represent the question, and consequently construct
image-question co-attention maps at 3 different levels: word level, phrase level and question
level. These co-attended features are then recursively combined from word level to question
level for the ﬁnal answer prediction;

• At the phrase level, we propose a novel convolution-pooling strategy to adaptively select the

phrase sizes whose representations are passed to the question level representation;

• Finally, we evaluate our proposed model on two large datasets, VQA [2] and COCO-QA [17].
We also perform ablation studies to quantify the roles of different components in our model.

2 Related Work

Many recent works [2, 7, 13, 16, 17, 27, 12, 6] have proposed models for VQA. We compare and
relate our proposed co-attention mechanism to other vision and language attention mechanisms in
literature.

Image attention.
Instead of directly using the holistic entire-image embedding from the fully
connected layer of a deep CNN (as in [2, 15–17]), a number of recent works have explored image
attention models for VQA. Zhu et al. [28] add spatial attention to the standard LSTM model for
pointing and grounded QA. Andreas et al. [1] propose a compositional scheme that consists of a
language parser and a number of neural modules networks. The language parser predicts which neural
module network should be instantiated to answer the question. Some other works perform image
attention multiple times in a stacked manner. In [25], the authors propose a stacked attention network,
which runs multiple hops to infer the answer progressively. To capture ﬁne-grained information from
the question, Xu et al. [24] propose a multi-hop image attention scheme. It aligns words to image
patches in the ﬁrst hop, and then refers to the entire question for obtaining image attention maps in
the second hop. In [20], the authors generate image regions with object proposals and then select the
regions relevant to the question and answer choice. Xiong et al. [23] augments dynamic memory

2

network with a new input fusion module and retrieves an answer from an attention based GRU. In
concurrent work, [5] collected ‘human attention maps’ that are used to evaluate the attention maps
generated by attention models for VQA. Note that all of these approaches model visual attention
alone, and do not model question attention. Moreover, [24, 25] model attention sequentially, i.e., later
attention is based on earlier attention, which is prone to error propagation. In contrast, we conduct
co-attention at three levels independently.

Language Attention. Though no prior work has explored question attention in VQA, there are
some related works in natural language processing (NLP) in general that have modeled language
attention. In order to overcome difﬁculty in translation of long sentences, Bahdanau et al. [3]
propose RNNSearch to learn an alignment over the input sentences. In [9], the authors propose an
attention model to circumvent the bottleneck caused by ﬁxed width hidden vector in text reading and
comprehension. A more ﬁne-grained attention mechanism is proposed in [18]. The authors employ
a word-by-word neural attention mechanism to reason about the entailment in two sentences. Also
focused on modeling sentence pairs, the authors in [26] propose an attention-based bigram CNN for
jointly performing attention between two CNN hierarchies. In their work, three attention schemes are
proposed and evaluated. In [19], the authors propose a two-way attention mechanism to project the
paired inputs into a common representation space.

We begin by introducing the notation used in this paper. To ease understanding, our full model
is described in parts. First, our hierarchical question representation is described in Sec. 3.2 and
the proposed co-attention mechanism is then described in Sec. 3.3. Finally, Sec. 3.4 shows how to
recursively combine the attended question and image features to output answers.

3 Method

3.1 Notation

Given a question with T words, its representation is denoted by Q = {q1, . . . qT }, where qt is the
feature vector for the t-th word. We denote qw
t as word embedding, phrase embedding and
question embedding at position t, respectively. The image feature is denoted by V = {v1, ..., vN },
where vn is the feature vector at the spatial location n. The co-attention features of image and
question at each level in the hierarchy are denoted as ˆvr and ˆqr where r ∈ {w, p, s}. The weights in
different modules/layers are denoted with W , with appropriate sub/super-scripts as necessary. In the
exposition that follows, we omit the bias term b to avoid notational clutter.

t and qs

t , qp

3.2 Question Hierarchy

Given the 1-hot encoding of the question words Q = {q1, . . . , qT }, we ﬁrst embed the words to
a vector space (learnt end-to-end) to get Qw = {qw
T }. To compute the phrase features,
we apply 1-D convolution on the word embedding vectors. Concretely, at each word location, we
compute the inner product of the word vectors with ﬁlters of three window sizes: unigram, bigram
and trigram. For the t-th word, the convolution output with window size s is given by

1 , . . . , qw

ˆqp
s,t = tanh(W s

c qw

t:t+s−1),

s ∈ {1, 2, 3}

where W s
c is the weight parameters. The word-level features Qw are appropriately 0-padded before
feeding into bigram and trigram convolutions to maintain the length of the sequence after convolution.
Given the convolution result, we then apply max-pooling across different n-grams at each word
location to obtain phrase-level features

t = max( ˆqp
qp

1,t, ˆqp

2,t, ˆqp

3,t),

t ∈ {1, 2, . . . , T }

(1)

(2)

Our pooling method differs from those used in previous works [10] in that it adaptively selects
different gram features at each time step, while preserving the original sequence length and order. We
use a LSTM to encode the sequence qp
t after max-pooling. The corresponding question-level feature
qs
t is the LSTM hidden vector at time t.
Our hierarchical representation of the question is depicted in Fig. 3(a).

3

Figure 2: (a) Parallel co-attention mechanism; (b) Alternating co-attention mechanism.

3.3 Co-Attention

We propose two co-attention mechanisms that differ in the order in which image and question
attention maps are generated. The ﬁrst mechanism, which we call parallel co-attention, generates
image and question attention simultaneously. The second mechanism, which we call alternating
co-attention, sequentially alternates between generating image and question attentions. See Fig. 2.
These co-attention mechanisms are executed at all three levels of the question hierarchy.

Parallel Co-Attention. Parallel co-attention attends to the image and question simultaneously.
Similar to [24], we connect the image and question by calculating the similarity between image and
question features at all pairs of image-locations and question-locations. Speciﬁcally, given an image
feature map V ∈ Rd×N , and the question representation Q ∈ Rd×T , the afﬁnity matrix C ∈ RT ×N
is calculated by

C = tanh(QT WbV )

where Wb ∈ Rd×d contains the weights. After computing this afﬁnity matrix, one possible way of
computing the image (or question) attention is to simply maximize out the afﬁnity over the locations
of other modality, i.e. av[n] = maxi(Ci,n) and aq[t] = maxj(Ct,j). Instead of choosing the max
activation, we ﬁnd that performance is improved if we consider this afﬁnity matrix as a feature and
learn to predict image and question attention maps via the following

H v = tanh(WvV + (WqQ)C), H q = tanh(WqQ + (WvV )CT )

av = softmax(wT

hvH v), aq = softmax(wT

hqH q)

where Wv, Wq ∈ Rk×d, whv, whq ∈ Rk are the weight parameters. av ∈ RN and aq ∈ RT are
the attention probabilities of each image region vn and word qt respectively. The afﬁnity matrix C
transforms question attention space to image attention space (vice versa for CT ). Based on the above
attention weights, the image and question attention vectors are calculated as the weighted sum of the
image features and question features, i.e.,

(3)

(4)

(5)

ˆv =

av
nvn,

ˆq =

aq
t qt

N
(cid:88)

n=1

T
(cid:88)

t=1

The parallel co-attention is done at each level in the hierarchy, leading to ˆvr and ˆqr where r ∈
{w, p, s}.

Alternating Co-Attention. In this attention mechanism, we sequentially alternate between gen-
erating image and question attention. Brieﬂy, this consists of three steps (marked in Fig. 2b): 1)
summarize the question into a single vector q; 2) attend to the image based on the question summary
q; 3) attend to the question based on the attended image feature.

Concretely, we deﬁne an attention operation ˆx = A(X; g), which takes the image (or question)
features X and attention guidance g derived from question (or image) as inputs, and outputs the

4

Figure 3: (a) Hierarchical question encoding (Sec. 3.2); (b) Encoding for predicting answers (Sec. 3.4).

attended image (or question) vector. The operation can be expressed in the following steps

H = tanh(WxX + (Wgg)1T )
ax = softmax(wT
(cid:88)

hxH)

ˆx =

ax
i xi

(6)

where 1 is a vector with all elements to be 1. Wx, Wg ∈ Rk×d and whx ∈ Rk are parameters. ax
is the attention weight of feature X.

The alternating co-attention process is illustrated in Fig. 2 (b). At the ﬁrst step of alternating co-
attention, X = Q, and g is 0; At the second step, X = V where V is the image features, and the
guidance g is intermediate attended question feature ˆs from the ﬁrst step; Finally, we use the attended
image feature ˆv as the guidance to attend the question again, i.e., X = Q and g = ˆv. Similar to the
parallel co-attention, the alternating co-attention is also done at each level of the hierarchy.

3.4 Encoding for Predicting Answers

Following [2], we treat VQA as a classiﬁcation task. We predict the answer based on the co-
attended image and question features from all three levels. We use a multi-layer perceptron (MLP) to
recursively encode the attention features as shown in Fig. 3(b).
hw = tanh(Ww(ˆqw + ˆvw))
hp = tanh(Wp[(ˆqp + ˆvp), hw])
hs = tanh(Ws[(ˆqs + ˆvs), hp])
p = softmax(Whhs)

(7)

where Ww, Wp, Ws and Wh are the weight parameters. [·] is the concatenation operation on two
vectors. p is the probability of the ﬁnal answer.

4 Experiment

4.1 Datasets

We evaluate the proposed model on two datasets, the VQA dataset [2] and the COCO-QA dataset
[17].

VQA dataset [2] is the largest dataset for this problem, containing human annotated questions and
answers on Microsoft COCO dataset [14]. The dataset contains 248,349 training questions, 121,512
validation questions, 244,302 testing questions, and a total of 6,141,630 question-answers pairs.
There are three sub-categories according to answer-types including yes/no, number, and other. Each
question has 10 free-response answers. We use the top 1000 most frequent answers as the possible
outputs similar to [2]. This set of answers covers 86.54% of the train+val answers. For testing, we
train our model on VQA train+val and report the test-dev and test-standard results from the VQA
evaluation server. We use the evaluation protocol of [2] in the experiment.

5

Table 1: Results on the VQA dataset. “-” indicates the results is not available.

Open-Ended

Multiple-Choice

test-dev

test-std

test-dev

test-std

Method

Y/N Num Other

All

Y/N Num Other

All

LSTM Q+I [2]
Region Sel. [20]
SMem [24]
SAN [25]
FDA [11]
DMN+ [23]
Oursp+VGG
Oursa+VGG
Oursa+ResNet

80.5
-
80.9
79.3
81.1
80.5

79.5
79.6
79.7

36.8
-
37.3
36.6
36.2
36.8

38.7
38.4
38.7

43.0
-
43.1
46.1
45.8
48.3

48.3
49.1
51.7

57.8
-
58.0
58.7
59.2
60.3

60.1
60.5
61.8

All

58.2
-
58.2
58.9
59.5
60.4

-
-
62.1

80.5
77.6
-
-
81.5
-

79.5
79.7
79.7

38.2
34.3
-
-
39.0
-

39.8
40.1
40.0

53.0
55.8
-
-
54.7
-

57.4
57.9
59.8

62.7
62.4
-
-
64.0
-

64.6
64.9
65.8

All

63.1
-
-
-
64.2
-

-
-
66.1

COCO-QA dataset [17] is automatically generated from captions in the Microsoft COCO dataset
[14]. There are 78,736 train questions and 38,948 test questions in the dataset. These questions
are based on 8,000 and 4,000 images respectively. There are four types of questions including
object, number, color, and location. Each type takes 70%, 7%, 17%, and 6% of the whole dataset,
respectively. All answers in this data set are single word. As in [17], we report classiﬁcation accuracy
as well as Wu-Palmer similarity (WUPS) in Table 2.

4.2 Setup

We use Torch [4] to develop our model. We use the Rmsprop optimizer with a base learning rate
of 4e-4, momentum 0.99 and weight-decay 1e-8. We set batch size to be 300 and train for up to
256 epochs with early stopping if the validation accuracy has not improved in the last 5 epochs. For
COCO-QA, the size of hidden layer Ws is set to 512 and 1024 for VQA since it is a much larger
dataset. All the other word embedding and hidden layers were vectors of size 512. We apply dropout
with probability 0.5 on each layer. Following [25], we rescale the image to 448 × 448, and then take
the activation from the last pooling layer of VGGNet [21] or ResNet [8] as its feature.

4.3 Results and Analysis

There are two test scenarios on VQA: open-ended and multiple-choice. The best performing method
deeper LSTM Q + norm I from [2] is used as our baseline. For open-ended test scenario, we
compare our method with the recent proposed SMem [24], SAN [25], FDA [11] and DMN+ [23].
For multiple choice, we compare with Region Sel.
[20] and FDA [11]. We compare with 2-
VIS+BLSTM [17], IMG-CNN [15] and SAN [25] on COCO-QA. We use Oursp to refer to our
parallel co-attention, Oursa for alternating co-attention.

Table 1 shows results on the VQA test sets for both open-ended and multiple-choice settings. We can
see that our approach improves the state of art from 60.4% (DMN+ [23]) to 62.1% (Oursa+ResNet) on
open-ended and from 64.2% (FDA [11]) to 66.1% (Oursa+ResNet) on multiple-choice. Notably, for
the question type Other and Num, we achieve 3.4% and 1.4% improvement on open-ended questions,
and 4.0% and 1.1% on multiple-choice questions. As we can see, ResNet features outperform or
match VGG features in all cases. Our improvements are not solely due to the use of a better CNN.
Speciﬁcally, FDA [11] also uses ResNet [8], but Oursa+ResNet outperforms it by 1.8% on test-dev.
SMem [24] uses GoogLeNet [22] and the rest all use VGGNet [21], and Ours+VGG outperforms
them by 0.2% on test-dev (DMN+ [23]).

Table 2 shows results on the COCO-QA test set. Similar to the result on VQA, our model improves the
state-of-the-art from 61.6% (SAN(2,CNN) [25]) to 65.4% (Oursa+ResNet). We observe that parallel
co-attention performs better than alternating co-attention in this setup. Both attention mechanisms
have their advantages and disadvantages: parallel co-attention is harder to train because of the dot
product between image and text which compresses two vectors into a single value. On the other hand,
alternating co-attention may suffer from errors being accumulated at each round.

6

Table 2: Results on the COCO-QA dataset. “-” indicates the results is not available.

Method

Object Number Color

Location Accuracy WUPS0.9 WUPS0.0

2-VIS+BLSTM [17]
IMG-CNN [15]
SAN(2, CNN) [25]
Oursp+VGG
Oursa+VGG
Oursa+ResNet

58.2
-
64.5

65.6
65.6
68.0

44.8
-
48.6

49.6
48.9
51.0

49.5
-
57.9

61.5
59.8
62.9

47.3
-
54.0

56.8
56.7
58.8

55.1
58.4
61.6

63.3
62.9
65.4

65.3
68.5
71.6

73.0
72.8
75.1

88.6
89.7
90.9

91.3
91.3
92.0

4.4 Ablation Study

In this section, we perform ablation studies to quantify the role of each component in our model.
Speciﬁcally, we re-train our approach by ablating certain components:

• Image Attention alone, where in a manner similar to previous works [25], we do not use any
question attention. The goal of this comparison is to verify that our improvements are not the
result of orthogonal contributions. (say better optimization or better CNN features).

• Question Attention alone, where no image attention is performed.
• W/O Conv, where no convolution and pooling is performed to represent phrases. Instead, we

stack another word embedding layer on the top of word level outputs.

• W/O W-Atten, where no word level co-attention is performed. We replace the word level attention

with a uniform distribution. Phrase and question level co-attentions are still modeled.

• W/O P-Atten, where no phrase level co-attention is performed, and the phrase level attention is

set to be uniform. Word and question level co-attentions are still modeled.

• W/O Q-Atten, where no question level co-attention is performed. We replace the question level
attention with a uniform distribution. Word and phrase level co-attentions are still modeled.

Table 3 shows the comparison of our full approach w.r.t these ablations on the VQA validation set
(test sets are not recommended to be used for such experiments). The deeper LSTM Q + norm I
baseline in [2] is also reported for comparison. We can see that image-attention-alone does improve
performance over the holistic image feature (deeper LSTM Q + norm I), which is consistent with
ﬁndings of previous attention models for VQA [23, 25].

Table 3: Ablation study on the VQA dataset using
Oursa+VGG.

Comparing the full model w.r.t. ablated versions
without word, phrase, question level attentions re-
veals a clear interesting trend – the attention mech-
anisms closest to the ‘top’ of the hierarchy (i.e. ques-
tion) matter most, with a drop of 1.7% in accuracy
if not modeled; followed by the intermediate level
(i.e. phrase), with a drop of 0.3%; ﬁnally followed
by the ‘bottom’ of the hierarchy (i.e. word), with
a drop of 0.2% in accuracy. We hypothesize that
this is because the question level is the ‘closest’ to
the answer prediction layers in our model. Note
that all levels are important, and our ﬁnal model
signiﬁcantly outperforms not using any linguistic
attention (1.1% difference between Full Model and
Image Atten). The question attention alone model
is better than LSTM Q+I, with an improvement of 0.5% and worse than image attention alone, with a
drop of 1.1%. Oursa further improves if we performed alternating co-attention for one more round,
with an improvement of 0.3%.

LSTM Q+I
Image Atten
Question Atten
W/O Q-Atten
W/O P-Atten
W/O W-Atten
Full Model

79.8
79.8
79.4
79.6
79.5
79.6
79.6

40.7
43.6
41.7
42.9
45.4
45.6
45.7

54.3
55.9
54.8
55.3
56.7
56.8
57.0

32.9
33.9
33.3
32.1
34.1
34.4
35.0

Y/N Num Other

validation

Method

All

4.5 Qualitative Results

We now visualize some co-attention maps generated by our method in Fig. 4. At the word level, our
model attends mostly to the object regions in an image, e.g., heads, bird. At the phrase level, the

7

Q: what is the man holding a
snowboard on top of a snow
covered? A: mountain

what is the man holding a
snowboard on top of a snow covered

what is the man holding a
snowboard on top of a snow
covered ?

what is the man holding a
snowboard on top of a snow
covered ?

Q: what is the color of the bird? A:
white

what is the color of the bird ?

what is the color of the bird ?

what is the color of the bird ?

Q: how many snowboarders in
formation in the snow, four is
sitting? A: 5

how many snowboarders in
formation in the snow , four is
sitting ?

how many snowboarders in
formation in the snow , four is
sitting ?

how many snowboarders in
formation in the snow , four is
sitting ?

Figure 4: Visualization of image and question co-attention maps on the COCO-QA dataset. From left to right:
original image and question pairs, word level co-attention maps, phrase level co-attention maps and question
level co-attention maps. For visualization, both image and question attentions are scaled (from red:high to
blue:low). Best viewed in color.

image attention has different patterns across images. For the ﬁrst two images, the attention transfers
from objects to background regions. For the third image, the attention becomes more focused on
the objects. We suspect that this is caused by the different question types. On the question side,
our model is capable of localizing the key phrases in the question, thus essentially discovering the
question types in the dataset. For example, our model pays attention to the phrases “what color” and
“how many snowboarders”. Our model successfully attends to the regions in images and phrases in the
questions appropriate for answering the question, e.g., “color of the bird” and bird region. Because
our model performs co-attention at three levels, it often captures complementary information from
each level, and then combines them to predict the answer.

5 Conclusion

In this paper, we proposed a hierarchical co-attention model for visual question answering. Co-
attention allows our model to attend to different regions of the image as well as different fragments
of the question. We model the question hierarchically at three levels to capture information from
different granularities. The ablation studies further demonstrate the roles of co-attention and question
hierarchy in our ﬁnal performance. Through visualizations, we can see that our model co-attends
to interpretable regions of images and questions for predicting the answer. Though our model was
evaluated on visual question answering, it can be potentially applied to other tasks involving vision
and language.

Acknowledgements

This work was funded in part by NSF CAREER awards to DP and DB, an ONR YIP award to DP, ONR Grant
N00014-14-1-0679 to DB, a Sloan Fellowship to DP, ARO YIP awards to DB and DP, a Allen Distinguished
Investigator award to DP from the Paul G. Allen Family Foundation, ICTAS Junior Faculty awards to DB and
DP, Google Faculty Research Awards to DP and DB, AWS in Education Research grant to DB, and NVIDIA
GPU donations to DB. The views and conclusions contained herein are those of the authors and should not be
interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of the
U.S. Government or any sponsor.

8

Q: what is the color of the
kitten? A: black

Q: what are standing in tall dry
grass look at the tourists? A:
zebras

Q: where is the woman while
her baby is sleeping? A:
kitchen

Q: what seating area is on the
right? A: park

Q: is the person dressed
properly for this sport? A: yes

what is the color of the kitten ?

what are standing in tall dry
grass look at the tourists ?

where is the woman while her
baby is sleeping ?

what seating area is on the
right ?

is the person dressed properly
for the sport ?

what is the color of the kitten ?

what are standing in tall dry
grass look at the tourists ?

where is the woman while her
baby is sleeping ?

what seating area is on the
right ?

is the person dressed properly
for the sport ?

what is the color of the kitten ?

what are standing in tall dry
grass look at the tourists ?

where is the woman while her
baby is sleeping ?

what seating area is on the
right ?

is the person dressed properly
for the sport ?

Figure 5: Visualization of co-attention maps on success cases in the COCO-QA (ﬁrst three columns using
Oursp+VGG) and VQA (last two columns Oursa+VGG) dataset. The layout is the same as Fig. 4.

Q: how many red motorcycles
with riders in protective gear
are on the street? A:
two(three)

Q: what is the color of the bus?
A: green(red)

Q: what is shown in different
places? A: hydrant(toy)

Q:do the doors open in or out?
A: open(in)

Q: is this an open market at
night? A: yes(no)

how many red motorcycles
with riders in protective gear
are on the street ?

what is the color of the bus ?

what is shown in different
places ?

do the doors open in or out ?

is this an open market at night ?

how many red motorcycles
with riders in protective gear
are on the street ?

what is the color of the bus ?

what is shown in different
places ?

do the doors open in or out ?

is this an open market at night ?

how many red motorcycles
with riders in protective gear
are on the street ?

what is the color of the bus ?

what is shown in different
places ?

do the doors open in or out ?

is this an open market at night ?

Figure 6: Visualization of co-attention maps on failure cases in the COCO-QA (ﬁrst three columns using
Oursp+VGG) and VQA (last two columns Oursa+VGG) dataset. Predicted answer is in red and ground truth
answer is in green. The layout is the same as Fig. 4.

9

References

[1] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Deep compositional question answering

with neural module networks. In CVPR, 2016.

[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,

and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015.

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning

to align and translate. In ICLR, 2015.

In BigLearn, NIPS Workshop, 2011.

[4] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning.

[5] Abhishek Das, Harsh Agrawal, C Lawrence Zitnick, Devi Parikh, and Dhruv Batra. Human attention
in visual question answering: Do humans and deep networks look at the same regions? arXiv preprint
arXiv:1606.03556, 2016.

[6] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach.
Multimodal compact bilinear pooling for visual question answering and visual grounding. arXiv preprint
arXiv:1606.01847, 2016.

[7] Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you talking to a

machine? dataset and methods for multilingual image question answering. In NIPS, 2015.

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.

In CVPR, 2016.

[9] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,

and Phil Blunsom. Teaching machines to read and comprehend. In NIPS, 2015.

[10] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. Convolutional neural network architectures for

matching natural language sentences. In NIPS, 2014.

[11] Ilija Ilievski, Shuicheng Yan, and Jiashi Feng. A focused dynamic attention model for visual question

answering. arXiv:1604.01485, 2016.

[12] Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, and
Byoung-Tak Zhang. Multimodal residual learning for visual qa. arXiv preprint arXiv:1606.01455, 2016.

[13] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision
using crowdsourced dense image annotations. arXiv preprint arXiv:1602.07332, 2016.

[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,

and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.

[15] Lin Ma, Zhengdong Lu, and Hang Li. Learning to answer questions from image using convolutional neural

network. In AAAI, 2016.

[16] Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. Ask your neurons: A neural-based approach to

answering questions about images. In ICCV, 2015.

[17] Mengye Ren, Ryan Kiros, and Richard Zemel. Exploring models and data for image question answering.

In NIPS, 2015.

[18] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blunsom. Reason-

ing about entailment with neural attention. In ICLR, 2016.

[19] Cicero dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. Attentive pooling networks. arXiv preprint

arXiv:1602.03609, 2016.

answering. In CVPR, 2016.

[20] Kevin J Shih, Saurabh Singh, and Derek Hoiem. Where to look: Focus regions for visual question

[21] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. CoRR, abs/1409.1556, 2014.

[22] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru

Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015.

10

[23] Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual

question answering. In ICML, 2016.

[24] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for

visual question answering. arXiv preprint arXiv:1511.05234, 2015.

[25] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image

question answering. In CVPR, 2016.

[26] Wenpeng Yin, Hinrich Schütze, Bing Xiang, and Bowen Zhou. Abcnn: Attention-based convolutional

neural network for modeling sentence pairs. In ACL, 2016.

[27] Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Yin and yang: Balancing

and answering binary visual questions. arXiv preprint arXiv:1511.05099, 2015.

[28] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in

images. In CVPR, 2016.

[29] C Lawrence Zitnick, Aishwarya Agrawal, Stanislaw Antol, Margaret Mitchell, Dhruv Batra, and Devi

Parikh. Measuring machine intelligence through visual question answering. AI Magazine, 37(1), 2016.

11

7
1
0
2
 
n
a
J
 
9
1
 
 
]

V
C
.
s
c
[
 
 
5
v
1
6
0
0
0
.
6
0
6
1
:
v
i
X
r
a

Hierarchical Question-Image Co-Attention

for Visual Question Answering

Jiasen Lu∗, Jianwei Yang∗, Dhruv Batra∗† , Devi Parikh∗†
∗ Virginia Tech, † Georgia Institute of Technology
{jiasenlu, jw2yang, dbatra, parikh}@vt.edu

Abstract

A number of recent works have proposed attention models for Visual Question
Answering (VQA) that generate spatial maps highlighting image regions relevant to
answering the question. In this paper, we argue that in addition to modeling “where
to look” or visual attention, it is equally important to model “what words to listen
to” or question attention. We present a novel co-attention model for VQA that
jointly reasons about image and question attention. In addition, our model reasons
about the question (and consequently the image via the co-attention mechanism)
in a hierarchical fashion via a novel 1-dimensional convolution neural networks
(CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to
60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the
performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.1.

1

Introduction

Visual Question Answering (VQA) [2, 7, 16, 17, 29] has emerged as a prominent multi-discipline
research problem in both academia and industry. To correctly answer visual questions about an
image, the machine needs to understand both the image and question. Recently, visual attention based
models [20, 23–25] have been explored for VQA, where the attention mechanism typically produces
a spatial map highlighting image regions relevant to answering the question.

So far, all attention models for VQA in literature have focused on the problem of identifying “where
to look” or visual attention. In this paper, we argue that the problem of identifying “which words to
listen to” or question attention is equally important. Consider the questions “how many horses are
in this image?” and “how many horses can you see in this image?". They have the same meaning,
essentially captured by the ﬁrst three words. A machine that attends to the ﬁrst three words would
arguably be more robust to linguistic variations irrelevant to the meaning and answer of the question.
Motivated by this observation, in addition to reasoning about visual attention, we also address the
problem of question attention. Speciﬁcally, we present a novel multi-modal attention model for VQA
with the following two unique features:

Co-Attention: We propose a novel mechanism that jointly reasons about visual attention and question
attention, which we refer to as co-attention. Unlike previous works, which only focus on visual
attention, our model has a natural symmetry between the image and question, in the sense that the
image representation is used to guide the question attention and the question representation(s) are
used to guide image attention.

Question Hierarchy: We build a hierarchical architecture that co-attends to the image and question
at three levels: (a) word level, (b) phrase level and (c) question level. At the word level, we embed the
words to a vector space through an embedding matrix. At the phrase level, 1-dimensional convolution
neural networks are used to capture the information contained in unigrams, bigrams and trigrams.

1The source code can be downloaded from https://github.com/jiasenlu/HieCoAttenVQA

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Figure 1: Flowchart of our proposed hierarchical co-attention model. Given a question, we extract its word
level, phrase level and question level embeddings. At each level, we apply co-attention on both the image and
question. The ﬁnal answer prediction is based on all the co-attended image and question features.

Speciﬁcally, we convolve word representations with temporal ﬁlters of varying support, and then
combine the various n-gram responses by pooling them into a single phrase level representation. At
the question level, we use recurrent neural networks to encode the entire question. For each level
of the question representation in this hierarchy, we construct joint question and image co-attention
maps, which are then combined recursively to ultimately predict a distribution over the answers.

Overall, the main contributions of our work are:

• We propose a novel co-attention mechanism for VQA that jointly performs question-guided
visual attention and image-guided question attention. We explore this mechanism with two
strategies, parallel and alternating co-attention, which are described in Sec. 3.3;

• We propose a hierarchical architecture to represent the question, and consequently construct
image-question co-attention maps at 3 different levels: word level, phrase level and question
level. These co-attended features are then recursively combined from word level to question
level for the ﬁnal answer prediction;

• At the phrase level, we propose a novel convolution-pooling strategy to adaptively select the

phrase sizes whose representations are passed to the question level representation;

• Finally, we evaluate our proposed model on two large datasets, VQA [2] and COCO-QA [17].
We also perform ablation studies to quantify the roles of different components in our model.

2 Related Work

Many recent works [2, 7, 13, 16, 17, 27, 12, 6] have proposed models for VQA. We compare and
relate our proposed co-attention mechanism to other vision and language attention mechanisms in
literature.

Image attention.
Instead of directly using the holistic entire-image embedding from the fully
connected layer of a deep CNN (as in [2, 15–17]), a number of recent works have explored image
attention models for VQA. Zhu et al. [28] add spatial attention to the standard LSTM model for
pointing and grounded QA. Andreas et al. [1] propose a compositional scheme that consists of a
language parser and a number of neural modules networks. The language parser predicts which neural
module network should be instantiated to answer the question. Some other works perform image
attention multiple times in a stacked manner. In [25], the authors propose a stacked attention network,
which runs multiple hops to infer the answer progressively. To capture ﬁne-grained information from
the question, Xu et al. [24] propose a multi-hop image attention scheme. It aligns words to image
patches in the ﬁrst hop, and then refers to the entire question for obtaining image attention maps in
the second hop. In [20], the authors generate image regions with object proposals and then select the
regions relevant to the question and answer choice. Xiong et al. [23] augments dynamic memory

2

network with a new input fusion module and retrieves an answer from an attention based GRU. In
concurrent work, [5] collected ‘human attention maps’ that are used to evaluate the attention maps
generated by attention models for VQA. Note that all of these approaches model visual attention
alone, and do not model question attention. Moreover, [24, 25] model attention sequentially, i.e., later
attention is based on earlier attention, which is prone to error propagation. In contrast, we conduct
co-attention at three levels independently.

Language Attention. Though no prior work has explored question attention in VQA, there are
some related works in natural language processing (NLP) in general that have modeled language
attention. In order to overcome difﬁculty in translation of long sentences, Bahdanau et al. [3]
propose RNNSearch to learn an alignment over the input sentences. In [9], the authors propose an
attention model to circumvent the bottleneck caused by ﬁxed width hidden vector in text reading and
comprehension. A more ﬁne-grained attention mechanism is proposed in [18]. The authors employ
a word-by-word neural attention mechanism to reason about the entailment in two sentences. Also
focused on modeling sentence pairs, the authors in [26] propose an attention-based bigram CNN for
jointly performing attention between two CNN hierarchies. In their work, three attention schemes are
proposed and evaluated. In [19], the authors propose a two-way attention mechanism to project the
paired inputs into a common representation space.

We begin by introducing the notation used in this paper. To ease understanding, our full model
is described in parts. First, our hierarchical question representation is described in Sec. 3.2 and
the proposed co-attention mechanism is then described in Sec. 3.3. Finally, Sec. 3.4 shows how to
recursively combine the attended question and image features to output answers.

3 Method

3.1 Notation

Given a question with T words, its representation is denoted by Q = {q1, . . . qT }, where qt is the
feature vector for the t-th word. We denote qw
t as word embedding, phrase embedding and
question embedding at position t, respectively. The image feature is denoted by V = {v1, ..., vN },
where vn is the feature vector at the spatial location n. The co-attention features of image and
question at each level in the hierarchy are denoted as ˆvr and ˆqr where r ∈ {w, p, s}. The weights in
different modules/layers are denoted with W , with appropriate sub/super-scripts as necessary. In the
exposition that follows, we omit the bias term b to avoid notational clutter.

t and qs

t , qp

3.2 Question Hierarchy

Given the 1-hot encoding of the question words Q = {q1, . . . , qT }, we ﬁrst embed the words to
a vector space (learnt end-to-end) to get Qw = {qw
T }. To compute the phrase features,
we apply 1-D convolution on the word embedding vectors. Concretely, at each word location, we
compute the inner product of the word vectors with ﬁlters of three window sizes: unigram, bigram
and trigram. For the t-th word, the convolution output with window size s is given by

1 , . . . , qw

ˆqp
s,t = tanh(W s

c qw

t:t+s−1),

s ∈ {1, 2, 3}

where W s
c is the weight parameters. The word-level features Qw are appropriately 0-padded before
feeding into bigram and trigram convolutions to maintain the length of the sequence after convolution.
Given the convolution result, we then apply max-pooling across different n-grams at each word
location to obtain phrase-level features

t = max( ˆqp
qp

1,t, ˆqp

2,t, ˆqp

3,t),

t ∈ {1, 2, . . . , T }

(1)

(2)

Our pooling method differs from those used in previous works [10] in that it adaptively selects
different gram features at each time step, while preserving the original sequence length and order. We
use a LSTM to encode the sequence qp
t after max-pooling. The corresponding question-level feature
qs
t is the LSTM hidden vector at time t.
Our hierarchical representation of the question is depicted in Fig. 3(a).

3

Figure 2: (a) Parallel co-attention mechanism; (b) Alternating co-attention mechanism.

3.3 Co-Attention

We propose two co-attention mechanisms that differ in the order in which image and question
attention maps are generated. The ﬁrst mechanism, which we call parallel co-attention, generates
image and question attention simultaneously. The second mechanism, which we call alternating
co-attention, sequentially alternates between generating image and question attentions. See Fig. 2.
These co-attention mechanisms are executed at all three levels of the question hierarchy.

Parallel Co-Attention. Parallel co-attention attends to the image and question simultaneously.
Similar to [24], we connect the image and question by calculating the similarity between image and
question features at all pairs of image-locations and question-locations. Speciﬁcally, given an image
feature map V ∈ Rd×N , and the question representation Q ∈ Rd×T , the afﬁnity matrix C ∈ RT ×N
is calculated by

C = tanh(QT WbV )

where Wb ∈ Rd×d contains the weights. After computing this afﬁnity matrix, one possible way of
computing the image (or question) attention is to simply maximize out the afﬁnity over the locations
of other modality, i.e. av[n] = maxi(Ci,n) and aq[t] = maxj(Ct,j). Instead of choosing the max
activation, we ﬁnd that performance is improved if we consider this afﬁnity matrix as a feature and
learn to predict image and question attention maps via the following

H v = tanh(WvV + (WqQ)C), H q = tanh(WqQ + (WvV )CT )

av = softmax(wT

hvH v), aq = softmax(wT

hqH q)

where Wv, Wq ∈ Rk×d, whv, whq ∈ Rk are the weight parameters. av ∈ RN and aq ∈ RT are
the attention probabilities of each image region vn and word qt respectively. The afﬁnity matrix C
transforms question attention space to image attention space (vice versa for CT ). Based on the above
attention weights, the image and question attention vectors are calculated as the weighted sum of the
image features and question features, i.e.,

(3)

(4)

(5)

ˆv =

av
nvn,

ˆq =

aq
t qt

N
(cid:88)

n=1

T
(cid:88)

t=1

The parallel co-attention is done at each level in the hierarchy, leading to ˆvr and ˆqr where r ∈
{w, p, s}.

Alternating Co-Attention. In this attention mechanism, we sequentially alternate between gen-
erating image and question attention. Brieﬂy, this consists of three steps (marked in Fig. 2b): 1)
summarize the question into a single vector q; 2) attend to the image based on the question summary
q; 3) attend to the question based on the attended image feature.

Concretely, we deﬁne an attention operation ˆx = A(X; g), which takes the image (or question)
features X and attention guidance g derived from question (or image) as inputs, and outputs the

4

Figure 3: (a) Hierarchical question encoding (Sec. 3.2); (b) Encoding for predicting answers (Sec. 3.4).

attended image (or question) vector. The operation can be expressed in the following steps

H = tanh(WxX + (Wgg)1T )
ax = softmax(wT
(cid:88)

hxH)

ˆx =

ax
i xi

(6)

where 1 is a vector with all elements to be 1. Wx, Wg ∈ Rk×d and whx ∈ Rk are parameters. ax
is the attention weight of feature X.

The alternating co-attention process is illustrated in Fig. 2 (b). At the ﬁrst step of alternating co-
attention, X = Q, and g is 0; At the second step, X = V where V is the image features, and the
guidance g is intermediate attended question feature ˆs from the ﬁrst step; Finally, we use the attended
image feature ˆv as the guidance to attend the question again, i.e., X = Q and g = ˆv. Similar to the
parallel co-attention, the alternating co-attention is also done at each level of the hierarchy.

3.4 Encoding for Predicting Answers

Following [2], we treat VQA as a classiﬁcation task. We predict the answer based on the co-
attended image and question features from all three levels. We use a multi-layer perceptron (MLP) to
recursively encode the attention features as shown in Fig. 3(b).
hw = tanh(Ww(ˆqw + ˆvw))
hp = tanh(Wp[(ˆqp + ˆvp), hw])
hs = tanh(Ws[(ˆqs + ˆvs), hp])
p = softmax(Whhs)

(7)

where Ww, Wp, Ws and Wh are the weight parameters. [·] is the concatenation operation on two
vectors. p is the probability of the ﬁnal answer.

4 Experiment

4.1 Datasets

We evaluate the proposed model on two datasets, the VQA dataset [2] and the COCO-QA dataset
[17].

VQA dataset [2] is the largest dataset for this problem, containing human annotated questions and
answers on Microsoft COCO dataset [14]. The dataset contains 248,349 training questions, 121,512
validation questions, 244,302 testing questions, and a total of 6,141,630 question-answers pairs.
There are three sub-categories according to answer-types including yes/no, number, and other. Each
question has 10 free-response answers. We use the top 1000 most frequent answers as the possible
outputs similar to [2]. This set of answers covers 86.54% of the train+val answers. For testing, we
train our model on VQA train+val and report the test-dev and test-standard results from the VQA
evaluation server. We use the evaluation protocol of [2] in the experiment.

5

Table 1: Results on the VQA dataset. “-” indicates the results is not available.

Open-Ended

Multiple-Choice

test-dev

test-std

test-dev

test-std

Method

Y/N Num Other

All

Y/N Num Other

All

LSTM Q+I [2]
Region Sel. [20]
SMem [24]
SAN [25]
FDA [11]
DMN+ [23]
Oursp+VGG
Oursa+VGG
Oursa+ResNet

80.5
-
80.9
79.3
81.1
80.5

79.5
79.6
79.7

36.8
-
37.3
36.6
36.2
36.8

38.7
38.4
38.7

43.0
-
43.1
46.1
45.8
48.3

48.3
49.1
51.7

57.8
-
58.0
58.7
59.2
60.3

60.1
60.5
61.8

All

58.2
-
58.2
58.9
59.5
60.4

-
-
62.1

80.5
77.6
-
-
81.5
-

79.5
79.7
79.7

38.2
34.3
-
-
39.0
-

39.8
40.1
40.0

53.0
55.8
-
-
54.7
-

57.4
57.9
59.8

62.7
62.4
-
-
64.0
-

64.6
64.9
65.8

All

63.1
-
-
-
64.2
-

-
-
66.1

COCO-QA dataset [17] is automatically generated from captions in the Microsoft COCO dataset
[14]. There are 78,736 train questions and 38,948 test questions in the dataset. These questions
are based on 8,000 and 4,000 images respectively. There are four types of questions including
object, number, color, and location. Each type takes 70%, 7%, 17%, and 6% of the whole dataset,
respectively. All answers in this data set are single word. As in [17], we report classiﬁcation accuracy
as well as Wu-Palmer similarity (WUPS) in Table 2.

4.2 Setup

We use Torch [4] to develop our model. We use the Rmsprop optimizer with a base learning rate
of 4e-4, momentum 0.99 and weight-decay 1e-8. We set batch size to be 300 and train for up to
256 epochs with early stopping if the validation accuracy has not improved in the last 5 epochs. For
COCO-QA, the size of hidden layer Ws is set to 512 and 1024 for VQA since it is a much larger
dataset. All the other word embedding and hidden layers were vectors of size 512. We apply dropout
with probability 0.5 on each layer. Following [25], we rescale the image to 448 × 448, and then take
the activation from the last pooling layer of VGGNet [21] or ResNet [8] as its feature.

4.3 Results and Analysis

There are two test scenarios on VQA: open-ended and multiple-choice. The best performing method
deeper LSTM Q + norm I from [2] is used as our baseline. For open-ended test scenario, we
compare our method with the recent proposed SMem [24], SAN [25], FDA [11] and DMN+ [23].
For multiple choice, we compare with Region Sel.
[20] and FDA [11]. We compare with 2-
VIS+BLSTM [17], IMG-CNN [15] and SAN [25] on COCO-QA. We use Oursp to refer to our
parallel co-attention, Oursa for alternating co-attention.

Table 1 shows results on the VQA test sets for both open-ended and multiple-choice settings. We can
see that our approach improves the state of art from 60.4% (DMN+ [23]) to 62.1% (Oursa+ResNet) on
open-ended and from 64.2% (FDA [11]) to 66.1% (Oursa+ResNet) on multiple-choice. Notably, for
the question type Other and Num, we achieve 3.4% and 1.4% improvement on open-ended questions,
and 4.0% and 1.1% on multiple-choice questions. As we can see, ResNet features outperform or
match VGG features in all cases. Our improvements are not solely due to the use of a better CNN.
Speciﬁcally, FDA [11] also uses ResNet [8], but Oursa+ResNet outperforms it by 1.8% on test-dev.
SMem [24] uses GoogLeNet [22] and the rest all use VGGNet [21], and Ours+VGG outperforms
them by 0.2% on test-dev (DMN+ [23]).

Table 2 shows results on the COCO-QA test set. Similar to the result on VQA, our model improves the
state-of-the-art from 61.6% (SAN(2,CNN) [25]) to 65.4% (Oursa+ResNet). We observe that parallel
co-attention performs better than alternating co-attention in this setup. Both attention mechanisms
have their advantages and disadvantages: parallel co-attention is harder to train because of the dot
product between image and text which compresses two vectors into a single value. On the other hand,
alternating co-attention may suffer from errors being accumulated at each round.

6

Table 2: Results on the COCO-QA dataset. “-” indicates the results is not available.

Method

Object Number Color

Location Accuracy WUPS0.9 WUPS0.0

2-VIS+BLSTM [17]
IMG-CNN [15]
SAN(2, CNN) [25]
Oursp+VGG
Oursa+VGG
Oursa+ResNet

58.2
-
64.5

65.6
65.6
68.0

44.8
-
48.6

49.6
48.9
51.0

49.5
-
57.9

61.5
59.8
62.9

47.3
-
54.0

56.8
56.7
58.8

55.1
58.4
61.6

63.3
62.9
65.4

65.3
68.5
71.6

73.0
72.8
75.1

88.6
89.7
90.9

91.3
91.3
92.0

4.4 Ablation Study

In this section, we perform ablation studies to quantify the role of each component in our model.
Speciﬁcally, we re-train our approach by ablating certain components:

• Image Attention alone, where in a manner similar to previous works [25], we do not use any
question attention. The goal of this comparison is to verify that our improvements are not the
result of orthogonal contributions. (say better optimization or better CNN features).

• Question Attention alone, where no image attention is performed.
• W/O Conv, where no convolution and pooling is performed to represent phrases. Instead, we

stack another word embedding layer on the top of word level outputs.

• W/O W-Atten, where no word level co-attention is performed. We replace the word level attention

with a uniform distribution. Phrase and question level co-attentions are still modeled.

• W/O P-Atten, where no phrase level co-attention is performed, and the phrase level attention is

set to be uniform. Word and question level co-attentions are still modeled.

• W/O Q-Atten, where no question level co-attention is performed. We replace the question level
attention with a uniform distribution. Word and phrase level co-attentions are still modeled.

Table 3 shows the comparison of our full approach w.r.t these ablations on the VQA validation set
(test sets are not recommended to be used for such experiments). The deeper LSTM Q + norm I
baseline in [2] is also reported for comparison. We can see that image-attention-alone does improve
performance over the holistic image feature (deeper LSTM Q + norm I), which is consistent with
ﬁndings of previous attention models for VQA [23, 25].

Table 3: Ablation study on the VQA dataset using
Oursa+VGG.

Comparing the full model w.r.t. ablated versions
without word, phrase, question level attentions re-
veals a clear interesting trend – the attention mech-
anisms closest to the ‘top’ of the hierarchy (i.e. ques-
tion) matter most, with a drop of 1.7% in accuracy
if not modeled; followed by the intermediate level
(i.e. phrase), with a drop of 0.3%; ﬁnally followed
by the ‘bottom’ of the hierarchy (i.e. word), with
a drop of 0.2% in accuracy. We hypothesize that
this is because the question level is the ‘closest’ to
the answer prediction layers in our model. Note
that all levels are important, and our ﬁnal model
signiﬁcantly outperforms not using any linguistic
attention (1.1% difference between Full Model and
Image Atten). The question attention alone model
is better than LSTM Q+I, with an improvement of 0.5% and worse than image attention alone, with a
drop of 1.1%. Oursa further improves if we performed alternating co-attention for one more round,
with an improvement of 0.3%.

LSTM Q+I
Image Atten
Question Atten
W/O Q-Atten
W/O P-Atten
W/O W-Atten
Full Model

79.8
79.8
79.4
79.6
79.5
79.6
79.6

40.7
43.6
41.7
42.9
45.4
45.6
45.7

54.3
55.9
54.8
55.3
56.7
56.8
57.0

32.9
33.9
33.3
32.1
34.1
34.4
35.0

Y/N Num Other

validation

Method

All

4.5 Qualitative Results

We now visualize some co-attention maps generated by our method in Fig. 4. At the word level, our
model attends mostly to the object regions in an image, e.g., heads, bird. At the phrase level, the

7

Q: what is the man holding a
snowboard on top of a snow
covered? A: mountain

what is the man holding a
snowboard on top of a snow covered

what is the man holding a
snowboard on top of a snow
covered ?

what is the man holding a
snowboard on top of a snow
covered ?

Q: what is the color of the bird? A:
white

what is the color of the bird ?

what is the color of the bird ?

what is the color of the bird ?

Q: how many snowboarders in
formation in the snow, four is
sitting? A: 5

how many snowboarders in
formation in the snow , four is
sitting ?

how many snowboarders in
formation in the snow , four is
sitting ?

how many snowboarders in
formation in the snow , four is
sitting ?

Figure 4: Visualization of image and question co-attention maps on the COCO-QA dataset. From left to right:
original image and question pairs, word level co-attention maps, phrase level co-attention maps and question
level co-attention maps. For visualization, both image and question attentions are scaled (from red:high to
blue:low). Best viewed in color.

image attention has different patterns across images. For the ﬁrst two images, the attention transfers
from objects to background regions. For the third image, the attention becomes more focused on
the objects. We suspect that this is caused by the different question types. On the question side,
our model is capable of localizing the key phrases in the question, thus essentially discovering the
question types in the dataset. For example, our model pays attention to the phrases “what color” and
“how many snowboarders”. Our model successfully attends to the regions in images and phrases in the
questions appropriate for answering the question, e.g., “color of the bird” and bird region. Because
our model performs co-attention at three levels, it often captures complementary information from
each level, and then combines them to predict the answer.

5 Conclusion

In this paper, we proposed a hierarchical co-attention model for visual question answering. Co-
attention allows our model to attend to different regions of the image as well as different fragments
of the question. We model the question hierarchically at three levels to capture information from
different granularities. The ablation studies further demonstrate the roles of co-attention and question
hierarchy in our ﬁnal performance. Through visualizations, we can see that our model co-attends
to interpretable regions of images and questions for predicting the answer. Though our model was
evaluated on visual question answering, it can be potentially applied to other tasks involving vision
and language.

Acknowledgements

This work was funded in part by NSF CAREER awards to DP and DB, an ONR YIP award to DP, ONR Grant
N00014-14-1-0679 to DB, a Sloan Fellowship to DP, ARO YIP awards to DB and DP, a Allen Distinguished
Investigator award to DP from the Paul G. Allen Family Foundation, ICTAS Junior Faculty awards to DB and
DP, Google Faculty Research Awards to DP and DB, AWS in Education Research grant to DB, and NVIDIA
GPU donations to DB. The views and conclusions contained herein are those of the authors and should not be
interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of the
U.S. Government or any sponsor.

8

Q: what is the color of the
kitten? A: black

Q: what are standing in tall dry
grass look at the tourists? A:
zebras

Q: where is the woman while
her baby is sleeping? A:
kitchen

Q: what seating area is on the
right? A: park

Q: is the person dressed
properly for this sport? A: yes

what is the color of the kitten ?

what are standing in tall dry
grass look at the tourists ?

where is the woman while her
baby is sleeping ?

what seating area is on the
right ?

is the person dressed properly
for the sport ?

what is the color of the kitten ?

what are standing in tall dry
grass look at the tourists ?

where is the woman while her
baby is sleeping ?

what seating area is on the
right ?

is the person dressed properly
for the sport ?

what is the color of the kitten ?

what are standing in tall dry
grass look at the tourists ?

where is the woman while her
baby is sleeping ?

what seating area is on the
right ?

is the person dressed properly
for the sport ?

Figure 5: Visualization of co-attention maps on success cases in the COCO-QA (ﬁrst three columns using
Oursp+VGG) and VQA (last two columns Oursa+VGG) dataset. The layout is the same as Fig. 4.

Q: how many red motorcycles
with riders in protective gear
are on the street? A:
two(three)

Q: what is the color of the bus?
A: green(red)

Q: what is shown in different
places? A: hydrant(toy)

Q:do the doors open in or out?
A: open(in)

Q: is this an open market at
night? A: yes(no)

how many red motorcycles
with riders in protective gear
are on the street ?

what is the color of the bus ?

what is shown in different
places ?

do the doors open in or out ?

is this an open market at night ?

how many red motorcycles
with riders in protective gear
are on the street ?

what is the color of the bus ?

what is shown in different
places ?

do the doors open in or out ?

is this an open market at night ?

how many red motorcycles
with riders in protective gear
are on the street ?

what is the color of the bus ?

what is shown in different
places ?

do the doors open in or out ?

is this an open market at night ?

Figure 6: Visualization of co-attention maps on failure cases in the COCO-QA (ﬁrst three columns using
Oursp+VGG) and VQA (last two columns Oursa+VGG) dataset. Predicted answer is in red and ground truth
answer is in green. The layout is the same as Fig. 4.

9

References

[1] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Deep compositional question answering

with neural module networks. In CVPR, 2016.

[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,

and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015.

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning

to align and translate. In ICLR, 2015.

In BigLearn, NIPS Workshop, 2011.

[4] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning.

[5] Abhishek Das, Harsh Agrawal, C Lawrence Zitnick, Devi Parikh, and Dhruv Batra. Human attention
in visual question answering: Do humans and deep networks look at the same regions? arXiv preprint
arXiv:1606.03556, 2016.

[6] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach.
Multimodal compact bilinear pooling for visual question answering and visual grounding. arXiv preprint
arXiv:1606.01847, 2016.

[7] Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you talking to a

machine? dataset and methods for multilingual image question answering. In NIPS, 2015.

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.

In CVPR, 2016.

[9] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,

and Phil Blunsom. Teaching machines to read and comprehend. In NIPS, 2015.

[10] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. Convolutional neural network architectures for

matching natural language sentences. In NIPS, 2014.

[11] Ilija Ilievski, Shuicheng Yan, and Jiashi Feng. A focused dynamic attention model for visual question

answering. arXiv:1604.01485, 2016.

[12] Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, and
Byoung-Tak Zhang. Multimodal residual learning for visual qa. arXiv preprint arXiv:1606.01455, 2016.

[13] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision
using crowdsourced dense image annotations. arXiv preprint arXiv:1602.07332, 2016.

[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,

and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.

[15] Lin Ma, Zhengdong Lu, and Hang Li. Learning to answer questions from image using convolutional neural

network. In AAAI, 2016.

[16] Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. Ask your neurons: A neural-based approach to

answering questions about images. In ICCV, 2015.

[17] Mengye Ren, Ryan Kiros, and Richard Zemel. Exploring models and data for image question answering.

In NIPS, 2015.

[18] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blunsom. Reason-

ing about entailment with neural attention. In ICLR, 2016.

[19] Cicero dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. Attentive pooling networks. arXiv preprint

arXiv:1602.03609, 2016.

answering. In CVPR, 2016.

[20] Kevin J Shih, Saurabh Singh, and Derek Hoiem. Where to look: Focus regions for visual question

[21] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. CoRR, abs/1409.1556, 2014.

[22] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru

Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015.

10

[23] Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual

question answering. In ICML, 2016.

[24] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for

visual question answering. arXiv preprint arXiv:1511.05234, 2015.

[25] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image

question answering. In CVPR, 2016.

[26] Wenpeng Yin, Hinrich Schütze, Bing Xiang, and Bowen Zhou. Abcnn: Attention-based convolutional

neural network for modeling sentence pairs. In ACL, 2016.

[27] Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Yin and yang: Balancing

and answering binary visual questions. arXiv preprint arXiv:1511.05099, 2015.

[28] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in

images. In CVPR, 2016.

[29] C Lawrence Zitnick, Aishwarya Agrawal, Stanislaw Antol, Margaret Mitchell, Dhruv Batra, and Devi

Parikh. Measuring machine intelligence through visual question answering. AI Magazine, 37(1), 2016.

11

7
1
0
2
 
n
a
J
 
9
1
 
 
]

V
C
.
s
c
[
 
 
5
v
1
6
0
0
0
.
6
0
6
1
:
v
i
X
r
a

Hierarchical Question-Image Co-Attention

for Visual Question Answering

Jiasen Lu∗, Jianwei Yang∗, Dhruv Batra∗† , Devi Parikh∗†
∗ Virginia Tech, † Georgia Institute of Technology
{jiasenlu, jw2yang, dbatra, parikh}@vt.edu

Abstract

A number of recent works have proposed attention models for Visual Question
Answering (VQA) that generate spatial maps highlighting image regions relevant to
answering the question. In this paper, we argue that in addition to modeling “where
to look” or visual attention, it is equally important to model “what words to listen
to” or question attention. We present a novel co-attention model for VQA that
jointly reasons about image and question attention. In addition, our model reasons
about the question (and consequently the image via the co-attention mechanism)
in a hierarchical fashion via a novel 1-dimensional convolution neural networks
(CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to
60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the
performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.1.

1

Introduction

Visual Question Answering (VQA) [2, 7, 16, 17, 29] has emerged as a prominent multi-discipline
research problem in both academia and industry. To correctly answer visual questions about an
image, the machine needs to understand both the image and question. Recently, visual attention based
models [20, 23–25] have been explored for VQA, where the attention mechanism typically produces
a spatial map highlighting image regions relevant to answering the question.

So far, all attention models for VQA in literature have focused on the problem of identifying “where
to look” or visual attention. In this paper, we argue that the problem of identifying “which words to
listen to” or question attention is equally important. Consider the questions “how many horses are
in this image?” and “how many horses can you see in this image?". They have the same meaning,
essentially captured by the ﬁrst three words. A machine that attends to the ﬁrst three words would
arguably be more robust to linguistic variations irrelevant to the meaning and answer of the question.
Motivated by this observation, in addition to reasoning about visual attention, we also address the
problem of question attention. Speciﬁcally, we present a novel multi-modal attention model for VQA
with the following two unique features:

Co-Attention: We propose a novel mechanism that jointly reasons about visual attention and question
attention, which we refer to as co-attention. Unlike previous works, which only focus on visual
attention, our model has a natural symmetry between the image and question, in the sense that the
image representation is used to guide the question attention and the question representation(s) are
used to guide image attention.

Question Hierarchy: We build a hierarchical architecture that co-attends to the image and question
at three levels: (a) word level, (b) phrase level and (c) question level. At the word level, we embed the
words to a vector space through an embedding matrix. At the phrase level, 1-dimensional convolution
neural networks are used to capture the information contained in unigrams, bigrams and trigrams.

1The source code can be downloaded from https://github.com/jiasenlu/HieCoAttenVQA

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Figure 1: Flowchart of our proposed hierarchical co-attention model. Given a question, we extract its word
level, phrase level and question level embeddings. At each level, we apply co-attention on both the image and
question. The ﬁnal answer prediction is based on all the co-attended image and question features.

Speciﬁcally, we convolve word representations with temporal ﬁlters of varying support, and then
combine the various n-gram responses by pooling them into a single phrase level representation. At
the question level, we use recurrent neural networks to encode the entire question. For each level
of the question representation in this hierarchy, we construct joint question and image co-attention
maps, which are then combined recursively to ultimately predict a distribution over the answers.

Overall, the main contributions of our work are:

• We propose a novel co-attention mechanism for VQA that jointly performs question-guided
visual attention and image-guided question attention. We explore this mechanism with two
strategies, parallel and alternating co-attention, which are described in Sec. 3.3;

• We propose a hierarchical architecture to represent the question, and consequently construct
image-question co-attention maps at 3 different levels: word level, phrase level and question
level. These co-attended features are then recursively combined from word level to question
level for the ﬁnal answer prediction;

• At the phrase level, we propose a novel convolution-pooling strategy to adaptively select the

phrase sizes whose representations are passed to the question level representation;

• Finally, we evaluate our proposed model on two large datasets, VQA [2] and COCO-QA [17].
We also perform ablation studies to quantify the roles of different components in our model.

2 Related Work

Many recent works [2, 7, 13, 16, 17, 27, 12, 6] have proposed models for VQA. We compare and
relate our proposed co-attention mechanism to other vision and language attention mechanisms in
literature.

Image attention.
Instead of directly using the holistic entire-image embedding from the fully
connected layer of a deep CNN (as in [2, 15–17]), a number of recent works have explored image
attention models for VQA. Zhu et al. [28] add spatial attention to the standard LSTM model for
pointing and grounded QA. Andreas et al. [1] propose a compositional scheme that consists of a
language parser and a number of neural modules networks. The language parser predicts which neural
module network should be instantiated to answer the question. Some other works perform image
attention multiple times in a stacked manner. In [25], the authors propose a stacked attention network,
which runs multiple hops to infer the answer progressively. To capture ﬁne-grained information from
the question, Xu et al. [24] propose a multi-hop image attention scheme. It aligns words to image
patches in the ﬁrst hop, and then refers to the entire question for obtaining image attention maps in
the second hop. In [20], the authors generate image regions with object proposals and then select the
regions relevant to the question and answer choice. Xiong et al. [23] augments dynamic memory

2

network with a new input fusion module and retrieves an answer from an attention based GRU. In
concurrent work, [5] collected ‘human attention maps’ that are used to evaluate the attention maps
generated by attention models for VQA. Note that all of these approaches model visual attention
alone, and do not model question attention. Moreover, [24, 25] model attention sequentially, i.e., later
attention is based on earlier attention, which is prone to error propagation. In contrast, we conduct
co-attention at three levels independently.

Language Attention. Though no prior work has explored question attention in VQA, there are
some related works in natural language processing (NLP) in general that have modeled language
attention. In order to overcome difﬁculty in translation of long sentences, Bahdanau et al. [3]
propose RNNSearch to learn an alignment over the input sentences. In [9], the authors propose an
attention model to circumvent the bottleneck caused by ﬁxed width hidden vector in text reading and
comprehension. A more ﬁne-grained attention mechanism is proposed in [18]. The authors employ
a word-by-word neural attention mechanism to reason about the entailment in two sentences. Also
focused on modeling sentence pairs, the authors in [26] propose an attention-based bigram CNN for
jointly performing attention between two CNN hierarchies. In their work, three attention schemes are
proposed and evaluated. In [19], the authors propose a two-way attention mechanism to project the
paired inputs into a common representation space.

We begin by introducing the notation used in this paper. To ease understanding, our full model
is described in parts. First, our hierarchical question representation is described in Sec. 3.2 and
the proposed co-attention mechanism is then described in Sec. 3.3. Finally, Sec. 3.4 shows how to
recursively combine the attended question and image features to output answers.

3 Method

3.1 Notation

Given a question with T words, its representation is denoted by Q = {q1, . . . qT }, where qt is the
feature vector for the t-th word. We denote qw
t as word embedding, phrase embedding and
question embedding at position t, respectively. The image feature is denoted by V = {v1, ..., vN },
where vn is the feature vector at the spatial location n. The co-attention features of image and
question at each level in the hierarchy are denoted as ˆvr and ˆqr where r ∈ {w, p, s}. The weights in
different modules/layers are denoted with W , with appropriate sub/super-scripts as necessary. In the
exposition that follows, we omit the bias term b to avoid notational clutter.

t and qs

t , qp

3.2 Question Hierarchy

Given the 1-hot encoding of the question words Q = {q1, . . . , qT }, we ﬁrst embed the words to
a vector space (learnt end-to-end) to get Qw = {qw
T }. To compute the phrase features,
we apply 1-D convolution on the word embedding vectors. Concretely, at each word location, we
compute the inner product of the word vectors with ﬁlters of three window sizes: unigram, bigram
and trigram. For the t-th word, the convolution output with window size s is given by

1 , . . . , qw

ˆqp
s,t = tanh(W s

c qw

t:t+s−1),

s ∈ {1, 2, 3}

where W s
c is the weight parameters. The word-level features Qw are appropriately 0-padded before
feeding into bigram and trigram convolutions to maintain the length of the sequence after convolution.
Given the convolution result, we then apply max-pooling across different n-grams at each word
location to obtain phrase-level features

t = max( ˆqp
qp

1,t, ˆqp

2,t, ˆqp

3,t),

t ∈ {1, 2, . . . , T }

(1)

(2)

Our pooling method differs from those used in previous works [10] in that it adaptively selects
different gram features at each time step, while preserving the original sequence length and order. We
use a LSTM to encode the sequence qp
t after max-pooling. The corresponding question-level feature
qs
t is the LSTM hidden vector at time t.
Our hierarchical representation of the question is depicted in Fig. 3(a).

3

Figure 2: (a) Parallel co-attention mechanism; (b) Alternating co-attention mechanism.

3.3 Co-Attention

We propose two co-attention mechanisms that differ in the order in which image and question
attention maps are generated. The ﬁrst mechanism, which we call parallel co-attention, generates
image and question attention simultaneously. The second mechanism, which we call alternating
co-attention, sequentially alternates between generating image and question attentions. See Fig. 2.
These co-attention mechanisms are executed at all three levels of the question hierarchy.

Parallel Co-Attention. Parallel co-attention attends to the image and question simultaneously.
Similar to [24], we connect the image and question by calculating the similarity between image and
question features at all pairs of image-locations and question-locations. Speciﬁcally, given an image
feature map V ∈ Rd×N , and the question representation Q ∈ Rd×T , the afﬁnity matrix C ∈ RT ×N
is calculated by

C = tanh(QT WbV )

where Wb ∈ Rd×d contains the weights. After computing this afﬁnity matrix, one possible way of
computing the image (or question) attention is to simply maximize out the afﬁnity over the locations
of other modality, i.e. av[n] = maxi(Ci,n) and aq[t] = maxj(Ct,j). Instead of choosing the max
activation, we ﬁnd that performance is improved if we consider this afﬁnity matrix as a feature and
learn to predict image and question attention maps via the following

H v = tanh(WvV + (WqQ)C), H q = tanh(WqQ + (WvV )CT )

av = softmax(wT

hvH v), aq = softmax(wT

hqH q)

where Wv, Wq ∈ Rk×d, whv, whq ∈ Rk are the weight parameters. av ∈ RN and aq ∈ RT are
the attention probabilities of each image region vn and word qt respectively. The afﬁnity matrix C
transforms question attention space to image attention space (vice versa for CT ). Based on the above
attention weights, the image and question attention vectors are calculated as the weighted sum of the
image features and question features, i.e.,

(3)

(4)

(5)

ˆv =

av
nvn,

ˆq =

aq
t qt

N
(cid:88)

n=1

T
(cid:88)

t=1

The parallel co-attention is done at each level in the hierarchy, leading to ˆvr and ˆqr where r ∈
{w, p, s}.

Alternating Co-Attention. In this attention mechanism, we sequentially alternate between gen-
erating image and question attention. Brieﬂy, this consists of three steps (marked in Fig. 2b): 1)
summarize the question into a single vector q; 2) attend to the image based on the question summary
q; 3) attend to the question based on the attended image feature.

Concretely, we deﬁne an attention operation ˆx = A(X; g), which takes the image (or question)
features X and attention guidance g derived from question (or image) as inputs, and outputs the

4

Figure 3: (a) Hierarchical question encoding (Sec. 3.2); (b) Encoding for predicting answers (Sec. 3.4).

attended image (or question) vector. The operation can be expressed in the following steps

H = tanh(WxX + (Wgg)1T )
ax = softmax(wT
(cid:88)

hxH)

ˆx =

ax
i xi

(6)

where 1 is a vector with all elements to be 1. Wx, Wg ∈ Rk×d and whx ∈ Rk are parameters. ax
is the attention weight of feature X.

The alternating co-attention process is illustrated in Fig. 2 (b). At the ﬁrst step of alternating co-
attention, X = Q, and g is 0; At the second step, X = V where V is the image features, and the
guidance g is intermediate attended question feature ˆs from the ﬁrst step; Finally, we use the attended
image feature ˆv as the guidance to attend the question again, i.e., X = Q and g = ˆv. Similar to the
parallel co-attention, the alternating co-attention is also done at each level of the hierarchy.

3.4 Encoding for Predicting Answers

Following [2], we treat VQA as a classiﬁcation task. We predict the answer based on the co-
attended image and question features from all three levels. We use a multi-layer perceptron (MLP) to
recursively encode the attention features as shown in Fig. 3(b).
hw = tanh(Ww(ˆqw + ˆvw))
hp = tanh(Wp[(ˆqp + ˆvp), hw])
hs = tanh(Ws[(ˆqs + ˆvs), hp])
p = softmax(Whhs)

(7)

where Ww, Wp, Ws and Wh are the weight parameters. [·] is the concatenation operation on two
vectors. p is the probability of the ﬁnal answer.

4 Experiment

4.1 Datasets

We evaluate the proposed model on two datasets, the VQA dataset [2] and the COCO-QA dataset
[17].

VQA dataset [2] is the largest dataset for this problem, containing human annotated questions and
answers on Microsoft COCO dataset [14]. The dataset contains 248,349 training questions, 121,512
validation questions, 244,302 testing questions, and a total of 6,141,630 question-answers pairs.
There are three sub-categories according to answer-types including yes/no, number, and other. Each
question has 10 free-response answers. We use the top 1000 most frequent answers as the possible
outputs similar to [2]. This set of answers covers 86.54% of the train+val answers. For testing, we
train our model on VQA train+val and report the test-dev and test-standard results from the VQA
evaluation server. We use the evaluation protocol of [2] in the experiment.

5

Table 1: Results on the VQA dataset. “-” indicates the results is not available.

Open-Ended

Multiple-Choice

test-dev

test-std

test-dev

test-std

Method

Y/N Num Other

All

Y/N Num Other

All

LSTM Q+I [2]
Region Sel. [20]
SMem [24]
SAN [25]
FDA [11]
DMN+ [23]
Oursp+VGG
Oursa+VGG
Oursa+ResNet

80.5
-
80.9
79.3
81.1
80.5

79.5
79.6
79.7

36.8
-
37.3
36.6
36.2
36.8

38.7
38.4
38.7

43.0
-
43.1
46.1
45.8
48.3

48.3
49.1
51.7

57.8
-
58.0
58.7
59.2
60.3

60.1
60.5
61.8

All

58.2
-
58.2
58.9
59.5
60.4

-
-
62.1

80.5
77.6
-
-
81.5
-

79.5
79.7
79.7

38.2
34.3
-
-
39.0
-

39.8
40.1
40.0

53.0
55.8
-
-
54.7
-

57.4
57.9
59.8

62.7
62.4
-
-
64.0
-

64.6
64.9
65.8

All

63.1
-
-
-
64.2
-

-
-
66.1

COCO-QA dataset [17] is automatically generated from captions in the Microsoft COCO dataset
[14]. There are 78,736 train questions and 38,948 test questions in the dataset. These questions
are based on 8,000 and 4,000 images respectively. There are four types of questions including
object, number, color, and location. Each type takes 70%, 7%, 17%, and 6% of the whole dataset,
respectively. All answers in this data set are single word. As in [17], we report classiﬁcation accuracy
as well as Wu-Palmer similarity (WUPS) in Table 2.

4.2 Setup

We use Torch [4] to develop our model. We use the Rmsprop optimizer with a base learning rate
of 4e-4, momentum 0.99 and weight-decay 1e-8. We set batch size to be 300 and train for up to
256 epochs with early stopping if the validation accuracy has not improved in the last 5 epochs. For
COCO-QA, the size of hidden layer Ws is set to 512 and 1024 for VQA since it is a much larger
dataset. All the other word embedding and hidden layers were vectors of size 512. We apply dropout
with probability 0.5 on each layer. Following [25], we rescale the image to 448 × 448, and then take
the activation from the last pooling layer of VGGNet [21] or ResNet [8] as its feature.

4.3 Results and Analysis

There are two test scenarios on VQA: open-ended and multiple-choice. The best performing method
deeper LSTM Q + norm I from [2] is used as our baseline. For open-ended test scenario, we
compare our method with the recent proposed SMem [24], SAN [25], FDA [11] and DMN+ [23].
For multiple choice, we compare with Region Sel.
[20] and FDA [11]. We compare with 2-
VIS+BLSTM [17], IMG-CNN [15] and SAN [25] on COCO-QA. We use Oursp to refer to our
parallel co-attention, Oursa for alternating co-attention.

Table 1 shows results on the VQA test sets for both open-ended and multiple-choice settings. We can
see that our approach improves the state of art from 60.4% (DMN+ [23]) to 62.1% (Oursa+ResNet) on
open-ended and from 64.2% (FDA [11]) to 66.1% (Oursa+ResNet) on multiple-choice. Notably, for
the question type Other and Num, we achieve 3.4% and 1.4% improvement on open-ended questions,
and 4.0% and 1.1% on multiple-choice questions. As we can see, ResNet features outperform or
match VGG features in all cases. Our improvements are not solely due to the use of a better CNN.
Speciﬁcally, FDA [11] also uses ResNet [8], but Oursa+ResNet outperforms it by 1.8% on test-dev.
SMem [24] uses GoogLeNet [22] and the rest all use VGGNet [21], and Ours+VGG outperforms
them by 0.2% on test-dev (DMN+ [23]).

Table 2 shows results on the COCO-QA test set. Similar to the result on VQA, our model improves the
state-of-the-art from 61.6% (SAN(2,CNN) [25]) to 65.4% (Oursa+ResNet). We observe that parallel
co-attention performs better than alternating co-attention in this setup. Both attention mechanisms
have their advantages and disadvantages: parallel co-attention is harder to train because of the dot
product between image and text which compresses two vectors into a single value. On the other hand,
alternating co-attention may suffer from errors being accumulated at each round.

6

Table 2: Results on the COCO-QA dataset. “-” indicates the results is not available.

Method

Object Number Color

Location Accuracy WUPS0.9 WUPS0.0

2-VIS+BLSTM [17]
IMG-CNN [15]
SAN(2, CNN) [25]
Oursp+VGG
Oursa+VGG
Oursa+ResNet

58.2
-
64.5

65.6
65.6
68.0

44.8
-
48.6

49.6
48.9
51.0

49.5
-
57.9

61.5
59.8
62.9

47.3
-
54.0

56.8
56.7
58.8

55.1
58.4
61.6

63.3
62.9
65.4

65.3
68.5
71.6

73.0
72.8
75.1

88.6
89.7
90.9

91.3
91.3
92.0

4.4 Ablation Study

In this section, we perform ablation studies to quantify the role of each component in our model.
Speciﬁcally, we re-train our approach by ablating certain components:

• Image Attention alone, where in a manner similar to previous works [25], we do not use any
question attention. The goal of this comparison is to verify that our improvements are not the
result of orthogonal contributions. (say better optimization or better CNN features).

• Question Attention alone, where no image attention is performed.
• W/O Conv, where no convolution and pooling is performed to represent phrases. Instead, we

stack another word embedding layer on the top of word level outputs.

• W/O W-Atten, where no word level co-attention is performed. We replace the word level attention

with a uniform distribution. Phrase and question level co-attentions are still modeled.

• W/O P-Atten, where no phrase level co-attention is performed, and the phrase level attention is

set to be uniform. Word and question level co-attentions are still modeled.

• W/O Q-Atten, where no question level co-attention is performed. We replace the question level
attention with a uniform distribution. Word and phrase level co-attentions are still modeled.

Table 3 shows the comparison of our full approach w.r.t these ablations on the VQA validation set
(test sets are not recommended to be used for such experiments). The deeper LSTM Q + norm I
baseline in [2] is also reported for comparison. We can see that image-attention-alone does improve
performance over the holistic image feature (deeper LSTM Q + norm I), which is consistent with
ﬁndings of previous attention models for VQA [23, 25].

Table 3: Ablation study on the VQA dataset using
Oursa+VGG.

Comparing the full model w.r.t. ablated versions
without word, phrase, question level attentions re-
veals a clear interesting trend – the attention mech-
anisms closest to the ‘top’ of the hierarchy (i.e. ques-
tion) matter most, with a drop of 1.7% in accuracy
if not modeled; followed by the intermediate level
(i.e. phrase), with a drop of 0.3%; ﬁnally followed
by the ‘bottom’ of the hierarchy (i.e. word), with
a drop of 0.2% in accuracy. We hypothesize that
this is because the question level is the ‘closest’ to
the answer prediction layers in our model. Note
that all levels are important, and our ﬁnal model
signiﬁcantly outperforms not using any linguistic
attention (1.1% difference between Full Model and
Image Atten). The question attention alone model
is better than LSTM Q+I, with an improvement of 0.5% and worse than image attention alone, with a
drop of 1.1%. Oursa further improves if we performed alternating co-attention for one more round,
with an improvement of 0.3%.

LSTM Q+I
Image Atten
Question Atten
W/O Q-Atten
W/O P-Atten
W/O W-Atten
Full Model

79.8
79.8
79.4
79.6
79.5
79.6
79.6

40.7
43.6
41.7
42.9
45.4
45.6
45.7

54.3
55.9
54.8
55.3
56.7
56.8
57.0

32.9
33.9
33.3
32.1
34.1
34.4
35.0

Y/N Num Other

validation

Method

All

4.5 Qualitative Results

We now visualize some co-attention maps generated by our method in Fig. 4. At the word level, our
model attends mostly to the object regions in an image, e.g., heads, bird. At the phrase level, the

7

Q: what is the man holding a
snowboard on top of a snow
covered? A: mountain

what is the man holding a
snowboard on top of a snow covered

what is the man holding a
snowboard on top of a snow
covered ?

what is the man holding a
snowboard on top of a snow
covered ?

Q: what is the color of the bird? A:
white

what is the color of the bird ?

what is the color of the bird ?

what is the color of the bird ?

Q: how many snowboarders in
formation in the snow, four is
sitting? A: 5

how many snowboarders in
formation in the snow , four is
sitting ?

how many snowboarders in
formation in the snow , four is
sitting ?

how many snowboarders in
formation in the snow , four is
sitting ?

Figure 4: Visualization of image and question co-attention maps on the COCO-QA dataset. From left to right:
original image and question pairs, word level co-attention maps, phrase level co-attention maps and question
level co-attention maps. For visualization, both image and question attentions are scaled (from red:high to
blue:low). Best viewed in color.

image attention has different patterns across images. For the ﬁrst two images, the attention transfers
from objects to background regions. For the third image, the attention becomes more focused on
the objects. We suspect that this is caused by the different question types. On the question side,
our model is capable of localizing the key phrases in the question, thus essentially discovering the
question types in the dataset. For example, our model pays attention to the phrases “what color” and
“how many snowboarders”. Our model successfully attends to the regions in images and phrases in the
questions appropriate for answering the question, e.g., “color of the bird” and bird region. Because
our model performs co-attention at three levels, it often captures complementary information from
each level, and then combines them to predict the answer.

5 Conclusion

In this paper, we proposed a hierarchical co-attention model for visual question answering. Co-
attention allows our model to attend to different regions of the image as well as different fragments
of the question. We model the question hierarchically at three levels to capture information from
different granularities. The ablation studies further demonstrate the roles of co-attention and question
hierarchy in our ﬁnal performance. Through visualizations, we can see that our model co-attends
to interpretable regions of images and questions for predicting the answer. Though our model was
evaluated on visual question answering, it can be potentially applied to other tasks involving vision
and language.

Acknowledgements

This work was funded in part by NSF CAREER awards to DP and DB, an ONR YIP award to DP, ONR Grant
N00014-14-1-0679 to DB, a Sloan Fellowship to DP, ARO YIP awards to DB and DP, a Allen Distinguished
Investigator award to DP from the Paul G. Allen Family Foundation, ICTAS Junior Faculty awards to DB and
DP, Google Faculty Research Awards to DP and DB, AWS in Education Research grant to DB, and NVIDIA
GPU donations to DB. The views and conclusions contained herein are those of the authors and should not be
interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of the
U.S. Government or any sponsor.

8

Q: what is the color of the
kitten? A: black

Q: what are standing in tall dry
grass look at the tourists? A:
zebras

Q: where is the woman while
her baby is sleeping? A:
kitchen

Q: what seating area is on the
right? A: park

Q: is the person dressed
properly for this sport? A: yes

what is the color of the kitten ?

what are standing in tall dry
grass look at the tourists ?

where is the woman while her
baby is sleeping ?

what seating area is on the
right ?

is the person dressed properly
for the sport ?

what is the color of the kitten ?

what are standing in tall dry
grass look at the tourists ?

where is the woman while her
baby is sleeping ?

what seating area is on the
right ?

is the person dressed properly
for the sport ?

what is the color of the kitten ?

what are standing in tall dry
grass look at the tourists ?

where is the woman while her
baby is sleeping ?

what seating area is on the
right ?

is the person dressed properly
for the sport ?

Figure 5: Visualization of co-attention maps on success cases in the COCO-QA (ﬁrst three columns using
Oursp+VGG) and VQA (last two columns Oursa+VGG) dataset. The layout is the same as Fig. 4.

Q: how many red motorcycles
with riders in protective gear
are on the street? A:
two(three)

Q: what is the color of the bus?
A: green(red)

Q: what is shown in different
places? A: hydrant(toy)

Q:do the doors open in or out?
A: open(in)

Q: is this an open market at
night? A: yes(no)

how many red motorcycles
with riders in protective gear
are on the street ?

what is the color of the bus ?

what is shown in different
places ?

do the doors open in or out ?

is this an open market at night ?

how many red motorcycles
with riders in protective gear
are on the street ?

what is the color of the bus ?

what is shown in different
places ?

do the doors open in or out ?

is this an open market at night ?

how many red motorcycles
with riders in protective gear
are on the street ?

what is the color of the bus ?

what is shown in different
places ?

do the doors open in or out ?

is this an open market at night ?

Figure 6: Visualization of co-attention maps on failure cases in the COCO-QA (ﬁrst three columns using
Oursp+VGG) and VQA (last two columns Oursa+VGG) dataset. Predicted answer is in red and ground truth
answer is in green. The layout is the same as Fig. 4.

9

References

[1] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Deep compositional question answering

with neural module networks. In CVPR, 2016.

[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,

and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015.

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning

to align and translate. In ICLR, 2015.

In BigLearn, NIPS Workshop, 2011.

[4] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning.

[5] Abhishek Das, Harsh Agrawal, C Lawrence Zitnick, Devi Parikh, and Dhruv Batra. Human attention
in visual question answering: Do humans and deep networks look at the same regions? arXiv preprint
arXiv:1606.03556, 2016.

[6] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach.
Multimodal compact bilinear pooling for visual question answering and visual grounding. arXiv preprint
arXiv:1606.01847, 2016.

[7] Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you talking to a

machine? dataset and methods for multilingual image question answering. In NIPS, 2015.

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.

In CVPR, 2016.

[9] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,

and Phil Blunsom. Teaching machines to read and comprehend. In NIPS, 2015.

[10] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. Convolutional neural network architectures for

matching natural language sentences. In NIPS, 2014.

[11] Ilija Ilievski, Shuicheng Yan, and Jiashi Feng. A focused dynamic attention model for visual question

answering. arXiv:1604.01485, 2016.

[12] Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, and
Byoung-Tak Zhang. Multimodal residual learning for visual qa. arXiv preprint arXiv:1606.01455, 2016.

[13] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision
using crowdsourced dense image annotations. arXiv preprint arXiv:1602.07332, 2016.

[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,

and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.

[15] Lin Ma, Zhengdong Lu, and Hang Li. Learning to answer questions from image using convolutional neural

network. In AAAI, 2016.

[16] Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. Ask your neurons: A neural-based approach to

answering questions about images. In ICCV, 2015.

[17] Mengye Ren, Ryan Kiros, and Richard Zemel. Exploring models and data for image question answering.

In NIPS, 2015.

[18] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blunsom. Reason-

ing about entailment with neural attention. In ICLR, 2016.

[19] Cicero dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. Attentive pooling networks. arXiv preprint

arXiv:1602.03609, 2016.

answering. In CVPR, 2016.

[20] Kevin J Shih, Saurabh Singh, and Derek Hoiem. Where to look: Focus regions for visual question

[21] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. CoRR, abs/1409.1556, 2014.

[22] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru

Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015.

10

[23] Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual

question answering. In ICML, 2016.

[24] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for

visual question answering. arXiv preprint arXiv:1511.05234, 2015.

[25] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image

question answering. In CVPR, 2016.

[26] Wenpeng Yin, Hinrich Schütze, Bing Xiang, and Bowen Zhou. Abcnn: Attention-based convolutional

neural network for modeling sentence pairs. In ACL, 2016.

[27] Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Yin and yang: Balancing

and answering binary visual questions. arXiv preprint arXiv:1511.05099, 2015.

[28] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in

images. In CVPR, 2016.

[29] C Lawrence Zitnick, Aishwarya Agrawal, Stanislaw Antol, Margaret Mitchell, Dhruv Batra, and Devi

Parikh. Measuring machine intelligence through visual question answering. AI Magazine, 37(1), 2016.

11

7
1
0
2
 
n
a
J
 
9
1
 
 
]

V
C
.
s
c
[
 
 
5
v
1
6
0
0
0
.
6
0
6
1
:
v
i
X
r
a

Hierarchical Question-Image Co-Attention

for Visual Question Answering

Jiasen Lu∗, Jianwei Yang∗, Dhruv Batra∗† , Devi Parikh∗†
∗ Virginia Tech, † Georgia Institute of Technology
{jiasenlu, jw2yang, dbatra, parikh}@vt.edu

Abstract

A number of recent works have proposed attention models for Visual Question
Answering (VQA) that generate spatial maps highlighting image regions relevant to
answering the question. In this paper, we argue that in addition to modeling “where
to look” or visual attention, it is equally important to model “what words to listen
to” or question attention. We present a novel co-attention model for VQA that
jointly reasons about image and question attention. In addition, our model reasons
about the question (and consequently the image via the co-attention mechanism)
in a hierarchical fashion via a novel 1-dimensional convolution neural networks
(CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to
60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the
performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.1.

1

Introduction

Visual Question Answering (VQA) [2, 7, 16, 17, 29] has emerged as a prominent multi-discipline
research problem in both academia and industry. To correctly answer visual questions about an
image, the machine needs to understand both the image and question. Recently, visual attention based
models [20, 23–25] have been explored for VQA, where the attention mechanism typically produces
a spatial map highlighting image regions relevant to answering the question.

So far, all attention models for VQA in literature have focused on the problem of identifying “where
to look” or visual attention. In this paper, we argue that the problem of identifying “which words to
listen to” or question attention is equally important. Consider the questions “how many horses are
in this image?” and “how many horses can you see in this image?". They have the same meaning,
essentially captured by the ﬁrst three words. A machine that attends to the ﬁrst three words would
arguably be more robust to linguistic variations irrelevant to the meaning and answer of the question.
Motivated by this observation, in addition to reasoning about visual attention, we also address the
problem of question attention. Speciﬁcally, we present a novel multi-modal attention model for VQA
with the following two unique features:

Co-Attention: We propose a novel mechanism that jointly reasons about visual attention and question
attention, which we refer to as co-attention. Unlike previous works, which only focus on visual
attention, our model has a natural symmetry between the image and question, in the sense that the
image representation is used to guide the question attention and the question representation(s) are
used to guide image attention.

Question Hierarchy: We build a hierarchical architecture that co-attends to the image and question
at three levels: (a) word level, (b) phrase level and (c) question level. At the word level, we embed the
words to a vector space through an embedding matrix. At the phrase level, 1-dimensional convolution
neural networks are used to capture the information contained in unigrams, bigrams and trigrams.

1The source code can be downloaded from https://github.com/jiasenlu/HieCoAttenVQA

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Figure 1: Flowchart of our proposed hierarchical co-attention model. Given a question, we extract its word
level, phrase level and question level embeddings. At each level, we apply co-attention on both the image and
question. The ﬁnal answer prediction is based on all the co-attended image and question features.

Speciﬁcally, we convolve word representations with temporal ﬁlters of varying support, and then
combine the various n-gram responses by pooling them into a single phrase level representation. At
the question level, we use recurrent neural networks to encode the entire question. For each level
of the question representation in this hierarchy, we construct joint question and image co-attention
maps, which are then combined recursively to ultimately predict a distribution over the answers.

Overall, the main contributions of our work are:

• We propose a novel co-attention mechanism for VQA that jointly performs question-guided
visual attention and image-guided question attention. We explore this mechanism with two
strategies, parallel and alternating co-attention, which are described in Sec. 3.3;

• We propose a hierarchical architecture to represent the question, and consequently construct
image-question co-attention maps at 3 different levels: word level, phrase level and question
level. These co-attended features are then recursively combined from word level to question
level for the ﬁnal answer prediction;

• At the phrase level, we propose a novel convolution-pooling strategy to adaptively select the

phrase sizes whose representations are passed to the question level representation;

• Finally, we evaluate our proposed model on two large datasets, VQA [2] and COCO-QA [17].
We also perform ablation studies to quantify the roles of different components in our model.

2 Related Work

Many recent works [2, 7, 13, 16, 17, 27, 12, 6] have proposed models for VQA. We compare and
relate our proposed co-attention mechanism to other vision and language attention mechanisms in
literature.

Image attention.
Instead of directly using the holistic entire-image embedding from the fully
connected layer of a deep CNN (as in [2, 15–17]), a number of recent works have explored image
attention models for VQA. Zhu et al. [28] add spatial attention to the standard LSTM model for
pointing and grounded QA. Andreas et al. [1] propose a compositional scheme that consists of a
language parser and a number of neural modules networks. The language parser predicts which neural
module network should be instantiated to answer the question. Some other works perform image
attention multiple times in a stacked manner. In [25], the authors propose a stacked attention network,
which runs multiple hops to infer the answer progressively. To capture ﬁne-grained information from
the question, Xu et al. [24] propose a multi-hop image attention scheme. It aligns words to image
patches in the ﬁrst hop, and then refers to the entire question for obtaining image attention maps in
the second hop. In [20], the authors generate image regions with object proposals and then select the
regions relevant to the question and answer choice. Xiong et al. [23] augments dynamic memory

2

network with a new input fusion module and retrieves an answer from an attention based GRU. In
concurrent work, [5] collected ‘human attention maps’ that are used to evaluate the attention maps
generated by attention models for VQA. Note that all of these approaches model visual attention
alone, and do not model question attention. Moreover, [24, 25] model attention sequentially, i.e., later
attention is based on earlier attention, which is prone to error propagation. In contrast, we conduct
co-attention at three levels independently.

Language Attention. Though no prior work has explored question attention in VQA, there are
some related works in natural language processing (NLP) in general that have modeled language
attention. In order to overcome difﬁculty in translation of long sentences, Bahdanau et al. [3]
propose RNNSearch to learn an alignment over the input sentences. In [9], the authors propose an
attention model to circumvent the bottleneck caused by ﬁxed width hidden vector in text reading and
comprehension. A more ﬁne-grained attention mechanism is proposed in [18]. The authors employ
a word-by-word neural attention mechanism to reason about the entailment in two sentences. Also
focused on modeling sentence pairs, the authors in [26] propose an attention-based bigram CNN for
jointly performing attention between two CNN hierarchies. In their work, three attention schemes are
proposed and evaluated. In [19], the authors propose a two-way attention mechanism to project the
paired inputs into a common representation space.

We begin by introducing the notation used in this paper. To ease understanding, our full model
is described in parts. First, our hierarchical question representation is described in Sec. 3.2 and
the proposed co-attention mechanism is then described in Sec. 3.3. Finally, Sec. 3.4 shows how to
recursively combine the attended question and image features to output answers.

3 Method

3.1 Notation

Given a question with T words, its representation is denoted by Q = {q1, . . . qT }, where qt is the
feature vector for the t-th word. We denote qw
t as word embedding, phrase embedding and
question embedding at position t, respectively. The image feature is denoted by V = {v1, ..., vN },
where vn is the feature vector at the spatial location n. The co-attention features of image and
question at each level in the hierarchy are denoted as ˆvr and ˆqr where r ∈ {w, p, s}. The weights in
different modules/layers are denoted with W , with appropriate sub/super-scripts as necessary. In the
exposition that follows, we omit the bias term b to avoid notational clutter.

t and qs

t , qp

3.2 Question Hierarchy

Given the 1-hot encoding of the question words Q = {q1, . . . , qT }, we ﬁrst embed the words to
a vector space (learnt end-to-end) to get Qw = {qw
T }. To compute the phrase features,
we apply 1-D convolution on the word embedding vectors. Concretely, at each word location, we
compute the inner product of the word vectors with ﬁlters of three window sizes: unigram, bigram
and trigram. For the t-th word, the convolution output with window size s is given by

1 , . . . , qw

ˆqp
s,t = tanh(W s

c qw

t:t+s−1),

s ∈ {1, 2, 3}

where W s
c is the weight parameters. The word-level features Qw are appropriately 0-padded before
feeding into bigram and trigram convolutions to maintain the length of the sequence after convolution.
Given the convolution result, we then apply max-pooling across different n-grams at each word
location to obtain phrase-level features

t = max( ˆqp
qp

1,t, ˆqp

2,t, ˆqp

3,t),

t ∈ {1, 2, . . . , T }

(1)

(2)

Our pooling method differs from those used in previous works [10] in that it adaptively selects
different gram features at each time step, while preserving the original sequence length and order. We
use a LSTM to encode the sequence qp
t after max-pooling. The corresponding question-level feature
qs
t is the LSTM hidden vector at time t.
Our hierarchical representation of the question is depicted in Fig. 3(a).

3

Figure 2: (a) Parallel co-attention mechanism; (b) Alternating co-attention mechanism.

3.3 Co-Attention

We propose two co-attention mechanisms that differ in the order in which image and question
attention maps are generated. The ﬁrst mechanism, which we call parallel co-attention, generates
image and question attention simultaneously. The second mechanism, which we call alternating
co-attention, sequentially alternates between generating image and question attentions. See Fig. 2.
These co-attention mechanisms are executed at all three levels of the question hierarchy.

Parallel Co-Attention. Parallel co-attention attends to the image and question simultaneously.
Similar to [24], we connect the image and question by calculating the similarity between image and
question features at all pairs of image-locations and question-locations. Speciﬁcally, given an image
feature map V ∈ Rd×N , and the question representation Q ∈ Rd×T , the afﬁnity matrix C ∈ RT ×N
is calculated by

C = tanh(QT WbV )

where Wb ∈ Rd×d contains the weights. After computing this afﬁnity matrix, one possible way of
computing the image (or question) attention is to simply maximize out the afﬁnity over the locations
of other modality, i.e. av[n] = maxi(Ci,n) and aq[t] = maxj(Ct,j). Instead of choosing the max
activation, we ﬁnd that performance is improved if we consider this afﬁnity matrix as a feature and
learn to predict image and question attention maps via the following

H v = tanh(WvV + (WqQ)C), H q = tanh(WqQ + (WvV )CT )

av = softmax(wT

hvH v), aq = softmax(wT

hqH q)

where Wv, Wq ∈ Rk×d, whv, whq ∈ Rk are the weight parameters. av ∈ RN and aq ∈ RT are
the attention probabilities of each image region vn and word qt respectively. The afﬁnity matrix C
transforms question attention space to image attention space (vice versa for CT ). Based on the above
attention weights, the image and question attention vectors are calculated as the weighted sum of the
image features and question features, i.e.,

(3)

(4)

(5)

ˆv =

av
nvn,

ˆq =

aq
t qt

N
(cid:88)

n=1

T
(cid:88)

t=1

The parallel co-attention is done at each level in the hierarchy, leading to ˆvr and ˆqr where r ∈
{w, p, s}.

Alternating Co-Attention. In this attention mechanism, we sequentially alternate between gen-
erating image and question attention. Brieﬂy, this consists of three steps (marked in Fig. 2b): 1)
summarize the question into a single vector q; 2) attend to the image based on the question summary
q; 3) attend to the question based on the attended image feature.

Concretely, we deﬁne an attention operation ˆx = A(X; g), which takes the image (or question)
features X and attention guidance g derived from question (or image) as inputs, and outputs the

4

Figure 3: (a) Hierarchical question encoding (Sec. 3.2); (b) Encoding for predicting answers (Sec. 3.4).

attended image (or question) vector. The operation can be expressed in the following steps

H = tanh(WxX + (Wgg)1T )
ax = softmax(wT
(cid:88)

hxH)

ˆx =

ax
i xi

(6)

where 1 is a vector with all elements to be 1. Wx, Wg ∈ Rk×d and whx ∈ Rk are parameters. ax
is the attention weight of feature X.

The alternating co-attention process is illustrated in Fig. 2 (b). At the ﬁrst step of alternating co-
attention, X = Q, and g is 0; At the second step, X = V where V is the image features, and the
guidance g is intermediate attended question feature ˆs from the ﬁrst step; Finally, we use the attended
image feature ˆv as the guidance to attend the question again, i.e., X = Q and g = ˆv. Similar to the
parallel co-attention, the alternating co-attention is also done at each level of the hierarchy.

3.4 Encoding for Predicting Answers

Following [2], we treat VQA as a classiﬁcation task. We predict the answer based on the co-
attended image and question features from all three levels. We use a multi-layer perceptron (MLP) to
recursively encode the attention features as shown in Fig. 3(b).
hw = tanh(Ww(ˆqw + ˆvw))
hp = tanh(Wp[(ˆqp + ˆvp), hw])
hs = tanh(Ws[(ˆqs + ˆvs), hp])
p = softmax(Whhs)

(7)

where Ww, Wp, Ws and Wh are the weight parameters. [·] is the concatenation operation on two
vectors. p is the probability of the ﬁnal answer.

4 Experiment

4.1 Datasets

We evaluate the proposed model on two datasets, the VQA dataset [2] and the COCO-QA dataset
[17].

VQA dataset [2] is the largest dataset for this problem, containing human annotated questions and
answers on Microsoft COCO dataset [14]. The dataset contains 248,349 training questions, 121,512
validation questions, 244,302 testing questions, and a total of 6,141,630 question-answers pairs.
There are three sub-categories according to answer-types including yes/no, number, and other. Each
question has 10 free-response answers. We use the top 1000 most frequent answers as the possible
outputs similar to [2]. This set of answers covers 86.54% of the train+val answers. For testing, we
train our model on VQA train+val and report the test-dev and test-standard results from the VQA
evaluation server. We use the evaluation protocol of [2] in the experiment.

5

Table 1: Results on the VQA dataset. “-” indicates the results is not available.

Open-Ended

Multiple-Choice

test-dev

test-std

test-dev

test-std

Method

Y/N Num Other

All

Y/N Num Other

All

LSTM Q+I [2]
Region Sel. [20]
SMem [24]
SAN [25]
FDA [11]
DMN+ [23]
Oursp+VGG
Oursa+VGG
Oursa+ResNet

80.5
-
80.9
79.3
81.1
80.5

79.5
79.6
79.7

36.8
-
37.3
36.6
36.2
36.8

38.7
38.4
38.7

43.0
-
43.1
46.1
45.8
48.3

48.3
49.1
51.7

57.8
-
58.0
58.7
59.2
60.3

60.1
60.5
61.8

All

58.2
-
58.2
58.9
59.5
60.4

-
-
62.1

80.5
77.6
-
-
81.5
-

79.5
79.7
79.7

38.2
34.3
-
-
39.0
-

39.8
40.1
40.0

53.0
55.8
-
-
54.7
-

57.4
57.9
59.8

62.7
62.4
-
-
64.0
-

64.6
64.9
65.8

All

63.1
-
-
-
64.2
-

-
-
66.1

COCO-QA dataset [17] is automatically generated from captions in the Microsoft COCO dataset
[14]. There are 78,736 train questions and 38,948 test questions in the dataset. These questions
are based on 8,000 and 4,000 images respectively. There are four types of questions including
object, number, color, and location. Each type takes 70%, 7%, 17%, and 6% of the whole dataset,
respectively. All answers in this data set are single word. As in [17], we report classiﬁcation accuracy
as well as Wu-Palmer similarity (WUPS) in Table 2.

4.2 Setup

We use Torch [4] to develop our model. We use the Rmsprop optimizer with a base learning rate
of 4e-4, momentum 0.99 and weight-decay 1e-8. We set batch size to be 300 and train for up to
256 epochs with early stopping if the validation accuracy has not improved in the last 5 epochs. For
COCO-QA, the size of hidden layer Ws is set to 512 and 1024 for VQA since it is a much larger
dataset. All the other word embedding and hidden layers were vectors of size 512. We apply dropout
with probability 0.5 on each layer. Following [25], we rescale the image to 448 × 448, and then take
the activation from the last pooling layer of VGGNet [21] or ResNet [8] as its feature.

4.3 Results and Analysis

There are two test scenarios on VQA: open-ended and multiple-choice. The best performing method
deeper LSTM Q + norm I from [2] is used as our baseline. For open-ended test scenario, we
compare our method with the recent proposed SMem [24], SAN [25], FDA [11] and DMN+ [23].
For multiple choice, we compare with Region Sel.
[20] and FDA [11]. We compare with 2-
VIS+BLSTM [17], IMG-CNN [15] and SAN [25] on COCO-QA. We use Oursp to refer to our
parallel co-attention, Oursa for alternating co-attention.

Table 1 shows results on the VQA test sets for both open-ended and multiple-choice settings. We can
see that our approach improves the state of art from 60.4% (DMN+ [23]) to 62.1% (Oursa+ResNet) on
open-ended and from 64.2% (FDA [11]) to 66.1% (Oursa+ResNet) on multiple-choice. Notably, for
the question type Other and Num, we achieve 3.4% and 1.4% improvement on open-ended questions,
and 4.0% and 1.1% on multiple-choice questions. As we can see, ResNet features outperform or
match VGG features in all cases. Our improvements are not solely due to the use of a better CNN.
Speciﬁcally, FDA [11] also uses ResNet [8], but Oursa+ResNet outperforms it by 1.8% on test-dev.
SMem [24] uses GoogLeNet [22] and the rest all use VGGNet [21], and Ours+VGG outperforms
them by 0.2% on test-dev (DMN+ [23]).

Table 2 shows results on the COCO-QA test set. Similar to the result on VQA, our model improves the
state-of-the-art from 61.6% (SAN(2,CNN) [25]) to 65.4% (Oursa+ResNet). We observe that parallel
co-attention performs better than alternating co-attention in this setup. Both attention mechanisms
have their advantages and disadvantages: parallel co-attention is harder to train because of the dot
product between image and text which compresses two vectors into a single value. On the other hand,
alternating co-attention may suffer from errors being accumulated at each round.

6

Table 2: Results on the COCO-QA dataset. “-” indicates the results is not available.

Method

Object Number Color

Location Accuracy WUPS0.9 WUPS0.0

2-VIS+BLSTM [17]
IMG-CNN [15]
SAN(2, CNN) [25]
Oursp+VGG
Oursa+VGG
Oursa+ResNet

58.2
-
64.5

65.6
65.6
68.0

44.8
-
48.6

49.6
48.9
51.0

49.5
-
57.9

61.5
59.8
62.9

47.3
-
54.0

56.8
56.7
58.8

55.1
58.4
61.6

63.3
62.9
65.4

65.3
68.5
71.6

73.0
72.8
75.1

88.6
89.7
90.9

91.3
91.3
92.0

4.4 Ablation Study

In this section, we perform ablation studies to quantify the role of each component in our model.
Speciﬁcally, we re-train our approach by ablating certain components:

• Image Attention alone, where in a manner similar to previous works [25], we do not use any
question attention. The goal of this comparison is to verify that our improvements are not the
result of orthogonal contributions. (say better optimization or better CNN features).

• Question Attention alone, where no image attention is performed.
• W/O Conv, where no convolution and pooling is performed to represent phrases. Instead, we

stack another word embedding layer on the top of word level outputs.

• W/O W-Atten, where no word level co-attention is performed. We replace the word level attention

with a uniform distribution. Phrase and question level co-attentions are still modeled.

• W/O P-Atten, where no phrase level co-attention is performed, and the phrase level attention is

set to be uniform. Word and question level co-attentions are still modeled.

• W/O Q-Atten, where no question level co-attention is performed. We replace the question level
attention with a uniform distribution. Word and phrase level co-attentions are still modeled.

Table 3 shows the comparison of our full approach w.r.t these ablations on the VQA validation set
(test sets are not recommended to be used for such experiments). The deeper LSTM Q + norm I
baseline in [2] is also reported for comparison. We can see that image-attention-alone does improve
performance over the holistic image feature (deeper LSTM Q + norm I), which is consistent with
ﬁndings of previous attention models for VQA [23, 25].

Table 3: Ablation study on the VQA dataset using
Oursa+VGG.

Comparing the full model w.r.t. ablated versions
without word, phrase, question level attentions re-
veals a clear interesting trend – the attention mech-
anisms closest to the ‘top’ of the hierarchy (i.e. ques-
tion) matter most, with a drop of 1.7% in accuracy
if not modeled; followed by the intermediate level
(i.e. phrase), with a drop of 0.3%; ﬁnally followed
by the ‘bottom’ of the hierarchy (i.e. word), with
a drop of 0.2% in accuracy. We hypothesize that
this is because the question level is the ‘closest’ to
the answer prediction layers in our model. Note
that all levels are important, and our ﬁnal model
signiﬁcantly outperforms not using any linguistic
attention (1.1% difference between Full Model and
Image Atten). The question attention alone model
is better than LSTM Q+I, with an improvement of 0.5% and worse than image attention alone, with a
drop of 1.1%. Oursa further improves if we performed alternating co-attention for one more round,
with an improvement of 0.3%.

LSTM Q+I
Image Atten
Question Atten
W/O Q-Atten
W/O P-Atten
W/O W-Atten
Full Model

79.8
79.8
79.4
79.6
79.5
79.6
79.6

40.7
43.6
41.7
42.9
45.4
45.6
45.7

54.3
55.9
54.8
55.3
56.7
56.8
57.0

32.9
33.9
33.3
32.1
34.1
34.4
35.0

Y/N Num Other

validation

Method

All

4.5 Qualitative Results

We now visualize some co-attention maps generated by our method in Fig. 4. At the word level, our
model attends mostly to the object regions in an image, e.g., heads, bird. At the phrase level, the

7

Q: what is the man holding a
snowboard on top of a snow
covered? A: mountain

what is the man holding a
snowboard on top of a snow covered

what is the man holding a
snowboard on top of a snow
covered ?

what is the man holding a
snowboard on top of a snow
covered ?

Q: what is the color of the bird? A:
white

what is the color of the bird ?

what is the color of the bird ?

what is the color of the bird ?

Q: how many snowboarders in
formation in the snow, four is
sitting? A: 5

how many snowboarders in
formation in the snow , four is
sitting ?

how many snowboarders in
formation in the snow , four is
sitting ?

how many snowboarders in
formation in the snow , four is
sitting ?

Figure 4: Visualization of image and question co-attention maps on the COCO-QA dataset. From left to right:
original image and question pairs, word level co-attention maps, phrase level co-attention maps and question
level co-attention maps. For visualization, both image and question attentions are scaled (from red:high to
blue:low). Best viewed in color.

image attention has different patterns across images. For the ﬁrst two images, the attention transfers
from objects to background regions. For the third image, the attention becomes more focused on
the objects. We suspect that this is caused by the different question types. On the question side,
our model is capable of localizing the key phrases in the question, thus essentially discovering the
question types in the dataset. For example, our model pays attention to the phrases “what color” and
“how many snowboarders”. Our model successfully attends to the regions in images and phrases in the
questions appropriate for answering the question, e.g., “color of the bird” and bird region. Because
our model performs co-attention at three levels, it often captures complementary information from
each level, and then combines them to predict the answer.

5 Conclusion

In this paper, we proposed a hierarchical co-attention model for visual question answering. Co-
attention allows our model to attend to different regions of the image as well as different fragments
of the question. We model the question hierarchically at three levels to capture information from
different granularities. The ablation studies further demonstrate the roles of co-attention and question
hierarchy in our ﬁnal performance. Through visualizations, we can see that our model co-attends
to interpretable regions of images and questions for predicting the answer. Though our model was
evaluated on visual question answering, it can be potentially applied to other tasks involving vision
and language.

Acknowledgements

This work was funded in part by NSF CAREER awards to DP and DB, an ONR YIP award to DP, ONR Grant
N00014-14-1-0679 to DB, a Sloan Fellowship to DP, ARO YIP awards to DB and DP, a Allen Distinguished
Investigator award to DP from the Paul G. Allen Family Foundation, ICTAS Junior Faculty awards to DB and
DP, Google Faculty Research Awards to DP and DB, AWS in Education Research grant to DB, and NVIDIA
GPU donations to DB. The views and conclusions contained herein are those of the authors and should not be
interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of the
U.S. Government or any sponsor.

8

Q: what is the color of the
kitten? A: black

Q: what are standing in tall dry
grass look at the tourists? A:
zebras

Q: where is the woman while
her baby is sleeping? A:
kitchen

Q: what seating area is on the
right? A: park

Q: is the person dressed
properly for this sport? A: yes

what is the color of the kitten ?

what are standing in tall dry
grass look at the tourists ?

where is the woman while her
baby is sleeping ?

what seating area is on the
right ?

is the person dressed properly
for the sport ?

what is the color of the kitten ?

what are standing in tall dry
grass look at the tourists ?

where is the woman while her
baby is sleeping ?

what seating area is on the
right ?

is the person dressed properly
for the sport ?

what is the color of the kitten ?

what are standing in tall dry
grass look at the tourists ?

where is the woman while her
baby is sleeping ?

what seating area is on the
right ?

is the person dressed properly
for the sport ?

Figure 5: Visualization of co-attention maps on success cases in the COCO-QA (ﬁrst three columns using
Oursp+VGG) and VQA (last two columns Oursa+VGG) dataset. The layout is the same as Fig. 4.

Q: how many red motorcycles
with riders in protective gear
are on the street? A:
two(three)

Q: what is the color of the bus?
A: green(red)

Q: what is shown in different
places? A: hydrant(toy)

Q:do the doors open in or out?
A: open(in)

Q: is this an open market at
night? A: yes(no)

how many red motorcycles
with riders in protective gear
are on the street ?

what is the color of the bus ?

what is shown in different
places ?

do the doors open in or out ?

is this an open market at night ?

how many red motorcycles
with riders in protective gear
are on the street ?

what is the color of the bus ?

what is shown in different
places ?

do the doors open in or out ?

is this an open market at night ?

how many red motorcycles
with riders in protective gear
are on the street ?

what is the color of the bus ?

what is shown in different
places ?

do the doors open in or out ?

is this an open market at night ?

Figure 6: Visualization of co-attention maps on failure cases in the COCO-QA (ﬁrst three columns using
Oursp+VGG) and VQA (last two columns Oursa+VGG) dataset. Predicted answer is in red and ground truth
answer is in green. The layout is the same as Fig. 4.

9

References

[1] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Deep compositional question answering

with neural module networks. In CVPR, 2016.

[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,

and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015.

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning

to align and translate. In ICLR, 2015.

In BigLearn, NIPS Workshop, 2011.

[4] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning.

[5] Abhishek Das, Harsh Agrawal, C Lawrence Zitnick, Devi Parikh, and Dhruv Batra. Human attention
in visual question answering: Do humans and deep networks look at the same regions? arXiv preprint
arXiv:1606.03556, 2016.

[6] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach.
Multimodal compact bilinear pooling for visual question answering and visual grounding. arXiv preprint
arXiv:1606.01847, 2016.

[7] Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you talking to a

machine? dataset and methods for multilingual image question answering. In NIPS, 2015.

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.

In CVPR, 2016.

[9] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,

and Phil Blunsom. Teaching machines to read and comprehend. In NIPS, 2015.

[10] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. Convolutional neural network architectures for

matching natural language sentences. In NIPS, 2014.

[11] Ilija Ilievski, Shuicheng Yan, and Jiashi Feng. A focused dynamic attention model for visual question

answering. arXiv:1604.01485, 2016.

[12] Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, and
Byoung-Tak Zhang. Multimodal residual learning for visual qa. arXiv preprint arXiv:1606.01455, 2016.

[13] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision
using crowdsourced dense image annotations. arXiv preprint arXiv:1602.07332, 2016.

[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,

and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.

[15] Lin Ma, Zhengdong Lu, and Hang Li. Learning to answer questions from image using convolutional neural

network. In AAAI, 2016.

[16] Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. Ask your neurons: A neural-based approach to

answering questions about images. In ICCV, 2015.

[17] Mengye Ren, Ryan Kiros, and Richard Zemel. Exploring models and data for image question answering.

In NIPS, 2015.

[18] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blunsom. Reason-

ing about entailment with neural attention. In ICLR, 2016.

[19] Cicero dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. Attentive pooling networks. arXiv preprint

arXiv:1602.03609, 2016.

answering. In CVPR, 2016.

[20] Kevin J Shih, Saurabh Singh, and Derek Hoiem. Where to look: Focus regions for visual question

[21] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. CoRR, abs/1409.1556, 2014.

[22] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru

Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015.

10

[23] Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual

question answering. In ICML, 2016.

[24] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for

visual question answering. arXiv preprint arXiv:1511.05234, 2015.

[25] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image

question answering. In CVPR, 2016.

[26] Wenpeng Yin, Hinrich Schütze, Bing Xiang, and Bowen Zhou. Abcnn: Attention-based convolutional

neural network for modeling sentence pairs. In ACL, 2016.

[27] Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Yin and yang: Balancing

and answering binary visual questions. arXiv preprint arXiv:1511.05099, 2015.

[28] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in

images. In CVPR, 2016.

[29] C Lawrence Zitnick, Aishwarya Agrawal, Stanislaw Antol, Margaret Mitchell, Dhruv Batra, and Devi

Parikh. Measuring machine intelligence through visual question answering. AI Magazine, 37(1), 2016.

11

