Ranking-Based Automatic Seed Selection and Noise Reduction for Weakly
Supervised Relation Extraction

Van-Thuy Phi1, Joan Santoso2, Masashi Shimbo1 and Yuji Matsumoto1,3

1 Nara Institute of Science and Technology
2Sekolah Tinggi Teknik Surabaya
3RIKEN Center for Advanced Intelligence Project (AIP)
1{phi.thuy.ph8, shimbo, matsu}@is.naist.jp
2joan@stts.edu

Abstract

This paper addresses the tasks of auto-
matic seed selection for bootstrapping re-
lation extraction, and noise reduction for
distantly supervised relation extraction.
We ﬁrst point out that these tasks are re-
lated. Then, inspired by ranking relation
instances and patterns computed by the
HITS algorithm, and selecting cluster cen-
troids using the K-means, LSA, or NMF
method, we propose methods for selecting
the initial seeds from an existing resource,
or reducing the level of noise in the dis-
tantly labeled data. Experiments show that
our proposed methods achieve a better per-
formance than the baseline systems in both
tasks.

1

Introduction

Bootstrapping for relation extraction (RE) (Brin,
1998; Riloff et al., 1999; Agichtein and Gravano,
2000) is a class of minimally supervised meth-
ods frequently used in machine learning: initial-
ized by a small set of example instances called
seeds, to represent a particular semantic relation,
the bootstrapping system operates iteratively to ac-
quire new instances of a target relation. Selecting
“good” seeds is one of the most important steps
to reduce semantic drift, which is a typical phe-
nomenon of the bootstrapping process.

Another approach, called “distant supervision”
(DS) (Mintz et al., 2009), does not require any la-
bels on the text. The assumption of DS is that if
two entities participate in a known Freebase rela-
tion, any sentence that contains those two entities
might express that relation. However, this tech-
nique often introduces noise to the generated train-
ing data. As a result, DS is still limited by the

quality of training data, and noise existing in pos-
itively labeled data may affect the performance of
supervised learning.

In this study, we propose methods that can
be applied for both automatic seed selection and
noise reduction by formulating these tasks as rank-
ing problems according to different ranking crite-
ria. Our methods are inspired by ranking instances
and patterns computed by the HITS algorithm, and
selecting cluster centroids using K-means, latent
semantic analysis, or the non-negative matrix fac-
torization method. The main contributions of this
paper are (a) an annotated dataset of 5,727 part-
whole relations1, which contains 8 subtypes for
the bootstrapping RE system; (b) methods for au-
tomatic seed selection for bootstrapping RE and
noise reduction for distant supervised RE; and (c)
experimental results showing that the proposed
models outperform baselines on two datasets.

2 Related Work

2.1 Automatic Seed Selection for

Bootstrapping RE

As manually selecting the seeds requires tremen-
dous effort, some research proposed methods to
select the seed automatically. Eisner and Karakos
(2005) used a “strapping” approach to evaluate
many candidate seeds automatically for a word
sense disambiguation task. Kozareva and Hovy
(2010) proposed a method for measuring seed
quality using a regression model and applied it to
the extraction of unary semantic relations, such
as“people” and “city”. Kiso et al. (2011) sug-
gested a HITS-based approach to ranking the
seeds, based on Komachi et al. (2008)’s analysis of
the Espresso algorithm (Pantel and Pennacchiotti,

1We

release

our

annotated

dataset

at

https://github.com/pvthuy/part-whole-relations.

2006). Movshovitz-Attias and Cohen (2012) gen-
erated a ranking based on pointwise mutual infor-
mation (PMI) to pick up the seeds from existing
resources in the biomedical domain. Given the
seed set of a target relation, the goal of the boot-
strapping method is to ﬁnd instances similar to
initial seeds by harvesting instances and patterns
iteratively over large corpora, e.g., Wikipedia or
ClueWeb.

2.2 Noise Reduction for Distantly Supervised

RE

The DS assumption is too strong and leads to
wrongly labeled data that affects performance.
Many studies focused on methods of noise re-
duction in DS. Intxaurrondo et al. (2013) ﬁltered
out noisy mentions from the distantly supervised
dataset using their frequencies, PMI, or the sim-
ilarity between the centroids of all relation men-
tions and each individual mention. Xiang et al.
(2016) introduced ranking-based methods accord-
ing to different strategies to select effective train-
ing groups. Li et al. (2017) proposed three novel
heuristics that use lexical and syntactic informa-
tion to remove noise in the biomedical domain.
The data generated by the noise reduction process
can be used by supervised learning algorithms to
train models.

3 Problem Formulation

Let R∗ be the set of target relations. The goal is
to ﬁnd instances, or pairs of entities, upon which
the relation holds. For each target relation r ∈
R∗, we assume there is a set Dr of triples rep-
resenting the relation r. The triples in Dr have
the form (e1, p, e2), where e1 and e2 denote en-
tities, and p denotes the pattern that connects the
two entities. A pair of entities (e1, e2) is called
an instance. This terminology is similar to the
one used in open information extraction systems,
such as Reverb (Fader et al., 2011). For example,
in triple (Barack Obama, was born in, Honolulu),
(Barack Obama, Honolulu) is the instance, and
“was born in” is the pattern.

The two tasks we address are deﬁned as follows:

Seed Selection for Bootstrapping RE:
In au-
tomatic seed selection, a set R∗ of target rela-
tions and sets of instance-pattern triples Dr =
{(e1, p, e2)} representing each target relation r ∈
R∗ are given as input. These triples are extracted
from existing corpus or database, e.g., WordNet.

With these inputs, the task is to choose good seeds
from the instances appearing in Dr for each r ∈
R∗, such that they work effectively in bootstrap-
ping RE.

Noise Reduction for Distantly Supervised RE:
In noise reduction for distantly supervised RE, the
input is the target relations R∗ and the sets Dr of
triples2 generated automatically by DS for each
relation r ∈ R∗. Because the data is generated
automatically by DS, Dr may contain noise, i.e.,
triples (e1, p, e2) for which relation r does not ac-
tually hold between e1 and e2. The goal of noise
reduction is to ﬁlter out these noisy triples, so that
they do not deteriorate the quality of the triple
classiﬁer trained subsequently.

Formulation as Ranking Tasks: As we can see
from the task deﬁnitions above, both seed selec-
tion and noise reduction are the task of selecting
Indeed, the two
triples from a given collection.
tasks essentially have a similar goal in terms of
the ranking-based perspective. We thus formulate
them as the task of ranking instances (in seed se-
lection) or triples (in noise reduction), given a set
of (possibly noisy) triples. In the seed selection
task, we use the k highest ranked instances as the
seeds for bootstrapping RE. Likewise, in noise re-
duction for DS, we only use the k highest ranked
triples from the DS-generated data to train a clas-
siﬁer. Note that the value of k in noise reduction
may be much larger than in seed selection.

4 Approaches to Automatic Seed
Selection and Noise Reduction

In this section, we propose several methods that
can be applied for both automatic seed selec-
tion and noise reduction tasks, inspired by rank-
ing relation instances and patterns computed by
the HITS algorithm, and picking cluster cen-
troids using the K-means, latent semantic anal-
ysis (LSA), or non-negative matrix factorization
(NMF) method.

4.1 K-means-based Approach

The ﬁrst method we describe is a K-means-based
It is described as follows: (1) De-
approach.

2 To be precise, in each triple (e1, s, e2) generated by DS,
s is not a pattern but a sentence that contains entities e1 and
e2. However, we can easily convert each instance-sentence
triple (e1, s, e2) to an instance-pattern triple (e1, p, e2) by
looking for a pattern p that connects two entities in sentence
s.

each instance/pattern as a node in the graph. This
representation is similar to that used by Kiso et al.
(2011). In the second graph representation, pat-
terns and instances are treated as nodes and edges,
respectively. Similarly, instances and patterns are
treated as nodes and edges, respectively in the last
representation. (3) For the ﬁrst and third types,
we simply retain the top-k instances with the high-
est hubness scores as the outputs (we sort the in-
stances in descending order based on their hub-
ness scores). For the second type, k instances as-
sociated with the highest scoring patterns are cho-
sen (we ﬁrst sort the patterns in descending order
based on their hubness scores).

4.3 HITS- and K-means-based Approach

In the combined method of HITS and K-means
algorithms, we ﬁrst rank the instances and pat-
terns based on their bipartite graph and then run
K-means to cluster instances in our annotated
dataset. However, instead of choosing the instance
nearest to the centroid, we retain the one that has
the highest HITS hubness score in each cluster.

4.4 LSA-based Approach

Latent semantic analysis (LSA) (Deerwester et al.,
1990) is also a widely used method for the au-
tomatic clustering of data along multiple dimen-
sions. Singular value decomposition (SVD) is
used to construct a low-rank approximation of
the instance-pattern co-occurrence matrix A. The
SVD projection is performed by decomposing the
matrix A ∈ RM ×N into the product of three
matrices, namely an SVD instance matrix I ∈
RM ×K, a diagonal matrix of singular values S ∈
RK×K, and an SVD pattern matrix P ∈ RK×N :

A ≈ ISPT

Our LSA-based seed selection strategy is as
(1) Specify the desired number k of
follows:
triples. (2) Use the LSA algorithm to decompose
the instance-pattern co-occurrence matrix A into
three matrices I, S, and P. We set the number of
LSA dimensions to K = k. (3) We can consider
LSA as a form of soft clustering, with each column
of the SVD instance matrix I corresponding to a
cluster. Then, we select the k instances that have
the highest absolute values from each column of I.

4.5 NMF-based Approach

Non-negative matrix factorization (NMF) (Paatero
and Tapper, 1994; Lee and Seung, 1999) is an-

Figure 1: Graph representations of instances and
patterns using the HITS algorithm.

termine the number k of instances/triples that
should be selected3. (2) Run the K-means clus-
tering algorithm to partition all instances in the in-
put triples (see Section 3) into k clusters. Each
data point is represented by the embedding vec-
tor difference between its entities; e.g., the in-
stance I = (Barack Obama, Honolulu) corre-
sponds to: vec(I) = vec(“Barack Obama”) −
vec(“Honolulu”). We use pre-trained vectors
published by Mikolov et al. (2013). (3) The in-
stance closest to the centroid is selected in each
cluster. Given that the number of clusters is k, the
same number of instances/triples will be chosen.

4.2 HITS-based Approach

Hypertext-induced topic search (HITS) (Klein-
the hubs-and-
also known as
berg, 1999),
authorities algorithm, is a link analysis method
for ranking web pages. In HITS, a good hub is a
page that points to many good authorities and vice
versa; a good authority is a page that is pointed to
by many good hubs. These hubs and authorities
form a bipartite graph, where we can compute the
hubness score of each node.

In our task, let A be the instance-pattern co-
occurrence matrix. We can compute the hubness
score for each instance on the bipartite graph of
instances and patterns induced by the matrix A.
Inspired by the way HITS ranks hubs and authori-
ties, our HITS-based seed selection strategy can be
explained as follows: (1) Determine the number k
of triples that should be selected.(2) Build the bi-
partite graph of instances and patterns based on the
instance-pattern co-occurrence matrix A. Figure 1
presents three possible ways of building a bipar-
tite graph. For the ﬁrst type of graph, we consider

3 Depending on the task, instances or triples will be se-
lected: instances for the automatic seed selection task, and
triples for the noise reduction task. As instances are pairs
of entities which are included in triples, we can simply con-
vert between the instance and the triple, and apply a proposed
method to both tasks.

Subtype
Component-Of
Member-Of
Portion-Of
Stuff-Of
Located-In
Contained-In
Phase-Of
Participates-In
TOTAL

Freq
643 (11.23%)
1,272 (22.21%)
555 ( 9.69%)
1,082 (18.89%)
534 ( 9.32%)
272 ( 4.75%)
497 ( 8.68%)
872 (15.23%)
5,727 triples

Table 1: Statistics of our part-whole dataset.

other method for approximate non-negative ma-
trix factorization. The non-negative data matrix
A ∈ RM ×N is represented by two non-negative
factors W ∈ RM ×K and H ∈ RK×N , which,
when multiplied, approximately reconstruct A:

A ≈ WH

The non-negativity constraint is the main differ-
ence between NMF and LSA. Similarly to the
LSA-based method, we set the NMF parameter K
to k, the desired number of instances to select. We
then select the k instances that have the highest
values from each column of W.

5 Experiments

5.1 Datasets and Settings

We provide an annotated dataset of part-whole re-
lations as a reliable resource for selecting seeds.
Our dataset was collected from Wikipedia and
ClueWeb, and annotated by two annotators. One
of its special characteristics is that the part-whole
relation is a collection of relations, not a single re-
lation (Iris, 1989; Winston et al., 1987).

Table 1 gives the frequencies of each sub-
type of part-whole relations. There are 5,727 in-
stances of 8 subtypes that were annotated with
the same labels by both annotators. We use
Espresso+Word2vec (Phi and Matsumoto, 2016),
which is an improved version for the origi-
nal Espresso algorithm (Pantel and Pennacchiotti,
Espresso+Word2vec outperformed the
2006).
Espresso system for harvesting part-whole rela-
tions by utilizing the Similarity Ranker, which
uses the embedded vector difference between in-
stance pairs of relations. The performance is mea-
sured with Precision@N (Manning et al., 2008),
N = 50. In total, 5,000 instances are checked by

Method
K-means
HITS Graph1
HITS Graph2
HITS Graph3
HITS+K-means Graph1
HITS+K-means Graph2
HITS+K-means Graph3
LSA
NMF
Random

Average P@50
0.96
0.90
0.85
0.90
0.92
0.85
0.94
0.90
0.89
0.75

Table 2: Performance of seed selection methods.

annotators to ascertain whether they express part-
whole relations. We vary the number k of seeds
between 5 and 50 with a step of 5 to report the
average P@50 of each seed selection method.

For the noise reduction task, we use the training
and testing set developed by (Riedel et al., 2010),
which contains 53 relation classes. This dataset
was generated by aligning Freebase relations with
the New York Times corpus. After removing noisy
triples from the dataset using the proposed meth-
ods, we use the ﬁltered data to train two kinds of
convolutional neural networks (CNN) (the CNN
model in (Zeng et al., 2014) and the PCNN model
in (Zeng et al., 2015)) with at-least-one multi-
instance learning (ONE) used in (Zeng et al.,
2015), and the sentence-level attention (ATT) used
in (Lin et al., 2016). Finally, we report the area
under the precision-recall (AUCPR) of each noise
reduction method.

5.2 Performance on Automatic Seed

Selection Task

The performances of the seed selection methods
are presented in Table 2. For the HITS-based
and HITS+K-means-based methods, we display
the P@50 with three types of graph representation
as shown in Section 4.2. We use random seed se-
lection as the baseline for comparison. As Table 2
shows, the random method achieved a precision
of 0.75. The relation extraction system that uses
the random method has the worst average P@50
among all seed selection strategies. The HITS-
based method’s P@50s when using Graph1 and
Graph3 are conﬁrmed to be better than when us-
ing Graph2. This indicates that relying on reli-
able instances is better than reasoning over pat-
terns (recall that for the Graph2, we ﬁrst choose

System
CNN+ONE
CNN+ATT
PCNN+ONE
PCNN+ATT

Original +HITS +LSA +NMF +Ensemble

0.180
0.234
0.231
0.248

0.183
0.235
0.234
0.253

0.173
0.235
0.233
0.250

0.178
0.233
0.234
0.252

0.181
0.236
0.235
0.255

Table 3: Performance (AUCPR) of each noise reduction method; in bold are the best scores.

the patterns, then select the instances associated
with those patterns), as there is a possibility that a
pattern can be ambiguous, and therefore, instances
linked to that pattern can be incorrect. The K-
means-based seed selection method provides the
best average P@50 with a performance of 0.96.
The HITS+K-means-based method performs bet-
ter than using only the HITS strategy, while the
LSA-based and NMF-based methods have a com-
parable performance.

5.3 Performance on Noise Reduction Task

similar:

Table 3 presents the performance of noise re-
duction methods. Recall that the K-means-based
the seed
method achieves a high P@50 for
selection method. Our assumption is that each
cluster may represent a set in which elements
have similar semantic properties.
However,
we observed that as the number of relations
is relatively high and there is no distinct def-
inition between some relations in the distantly
labeled data (e.g., the following three relations
/location/country/capital,
are quite
/location/province/capital,
and
/location/us state/capital, we decided not
to
perform the K-means-based method for our
noise reduction task. The performances of the
HITS-based, LSA-based, and NMF-based noise
reduction methods are presented in Table 3. We
experimentally set the portion of retained data
from the distantly labeled data to 90%, given
that the performance can be affected if too many
sentences are removed from the original data.
We also perform experiments with an ensem-
ble method that combines the HITS-based and
LSA-based strategies to merge rankings from
their outputs, with half of the triples coming from
the LSA-based method and the other half from
the HITS-based method. Table 3 indicates that
our proposed methods improved the performance
of all CNN and PCNN models. Our ensemble
method achieved the best improvements for three
out of four systems, except that the HITS-based

method obtained the best score for CNN+ONE.

6 Conclusion

We formulated the seed selection and noise re-
duction tasks as ranking problems.
In addition,
we proposed several methods, inspired by rank-
ing instances and patterns computed by the HITS
algorithm, and selecting clusters centroids using
the K-means, LSA, or NMF method. Experi-
ments demonstrated that our proposed methods
improved the baselines in both tasks.

Acknowledgments

This work was partly supported by JST CREST
Grant Number JPMJCR1513 and MEXT/JSPS
Kakenhi Grant Number JP15H02749. We are
grateful to the members of the Computational Lin-
guistics Laboratory, NAIST and the anonymous
reviewers for their insightful comments.

References

Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM Conference
on Digital Libraries, pages 85–94. ACM.

Sergey Brin. 1998. Extracting patterns and relations
In International Work-
from the world wide web.
shop on the World Wide Web and Databases, pages
172–183. Springer.

Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391.

Jason Eisner and Damianos Karakos. 2005. Bootstrap-
ping without the boot. In Proceedings of the 2005
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on

Empirical Methods in Natural Language Process-
ing, pages 1535–1545. Association for Computa-
tional Linguistics.

Ander Intxaurrondo, Mihai Surdeanu, Oier Lopez de
Lacalle, and Eneko Agirre. 2013. Removing noisy
mentions for distant supervision. Procesamiento del
lenguaje natural, 51.

Madelyn Anne Iris. 1989. Problems of the part-whole
relation. In Relational Models of the Lexicon, pages
261–288. Cambridge University Press.

Tetsuo Kiso, Masashi Shimbo, Mamoru Komachi, and
Yuji Matsumoto. 2011. HITS-based seed selection
and stop list construction for bootstrapping. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 30–36. Association for
Computational Linguistics.

Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604–632.

Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of
semantic drift in Espresso-like bootstrapping algo-
rithms. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1011–1020. Association for Computa-
tional Linguistics.

Zornitsa Kozareva and Eduard Hovy. 2010. Not all
seeds are equal: Measuring the quality of text min-
ing seeds. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 618–626. Association for Computa-
tional Linguistics.

Daniel D Lee and H. Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factor-
ization. Nature, 401(6755):788–791.

Gang Li, Cathy Wu, and K. Vijay-Shanker. 2017.
Noise reduction methods for distantly supervised
In SIGBioMed
biomedical relation extraction.
Workshop on Biomedical Natural Language Pro-
cessing (BioNLP ’17), pages 184–193. Association
for Computational Linguistics.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2124–2133. Association for Computa-
tional Linguistics.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003–1011. Association for Computational Linguis-
tics.

Dana Movshovitz-Attias and William W. Cohen. 2012.
Bootstrapping biomedical ontologies for scientiﬁc
In BioNLP: Proceedings of the
text using nell.
2012 Workshop on Biomedical Natural Language
Processing, pages 11–19. Association for Compu-
tational Linguistics.

Pentti Paatero and Unto Tapper. 1994. Positive ma-
trix factorization: A non-negative factor model with
optimal utilization of error estimates of data values.
Environmetrics, 5(2):111–126.

Patrick Pantel

and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics,
pages 113–120. Association for Computational
Linguistics.

Van-Thuy Phi and Yuji Matsumoto. 2016. Integrating
word embedding offsets into the Espresso system for
part-whole relation extraction. In Proceedings of the
30th Paciﬁc Asia Conference on Language, Informa-
tion and Computation: Oral Papers, pages 173–181.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
In Proceedings of the 2010 Joint
out labeled text.
European Conference on Machine Learning and
Principles of Knowledge Discovery in Databases
(ECML PKDD), pages 148–163. Springer.

Ellen Riloff, Rosie Jones, et al. 1999. Learning dic-
tionaries for information extraction by multi-level
In Proceedings of the 16th Na-
bootstrapping.
tional Conference on Artiﬁcial Intelligence and the
11th Innovative Applications of Artiﬁcial Intelli-
gence Conference (AAAI/IAAI), pages 474–479.

Morton E. Winston, Roger Chafﬁn, and Douglas Her-
rmann. 1987. A taxonomy of part-whole relations.
Cognitive Science, 11(4):417–444.

Yang Xiang, Qingcai Chen, Xiaolong Wang, and Yang
Qin. 2016. Distant supervision for relation ex-
Entropy,
traction with ranking-based methods.
18(6):204.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
In Pro-
piecewise convolutional neural networks.
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1753–
1762. Association for Computational Linguistics.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classiﬁcation via con-
volutional deep neural network. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335–2344. Dublin City University and As-
sociation for Computational Linguistics.

Ranking-Based Automatic Seed Selection and Noise Reduction for Weakly
Supervised Relation Extraction

Van-Thuy Phi1, Joan Santoso2, Masashi Shimbo1 and Yuji Matsumoto1,3

1 Nara Institute of Science and Technology
2Sekolah Tinggi Teknik Surabaya
3RIKEN Center for Advanced Intelligence Project (AIP)
1{phi.thuy.ph8, shimbo, matsu}@is.naist.jp
2joan@stts.edu

Abstract

This paper addresses the tasks of auto-
matic seed selection for bootstrapping re-
lation extraction, and noise reduction for
distantly supervised relation extraction.
We ﬁrst point out that these tasks are re-
lated. Then, inspired by ranking relation
instances and patterns computed by the
HITS algorithm, and selecting cluster cen-
troids using the K-means, LSA, or NMF
method, we propose methods for selecting
the initial seeds from an existing resource,
or reducing the level of noise in the dis-
tantly labeled data. Experiments show that
our proposed methods achieve a better per-
formance than the baseline systems in both
tasks.

1

Introduction

Bootstrapping for relation extraction (RE) (Brin,
1998; Riloff et al., 1999; Agichtein and Gravano,
2000) is a class of minimally supervised meth-
ods frequently used in machine learning: initial-
ized by a small set of example instances called
seeds, to represent a particular semantic relation,
the bootstrapping system operates iteratively to ac-
quire new instances of a target relation. Selecting
“good” seeds is one of the most important steps
to reduce semantic drift, which is a typical phe-
nomenon of the bootstrapping process.

Another approach, called “distant supervision”
(DS) (Mintz et al., 2009), does not require any la-
bels on the text. The assumption of DS is that if
two entities participate in a known Freebase rela-
tion, any sentence that contains those two entities
might express that relation. However, this tech-
nique often introduces noise to the generated train-
ing data. As a result, DS is still limited by the

quality of training data, and noise existing in pos-
itively labeled data may affect the performance of
supervised learning.

In this study, we propose methods that can
be applied for both automatic seed selection and
noise reduction by formulating these tasks as rank-
ing problems according to different ranking crite-
ria. Our methods are inspired by ranking instances
and patterns computed by the HITS algorithm, and
selecting cluster centroids using K-means, latent
semantic analysis, or the non-negative matrix fac-
torization method. The main contributions of this
paper are (a) an annotated dataset of 5,727 part-
whole relations1, which contains 8 subtypes for
the bootstrapping RE system; (b) methods for au-
tomatic seed selection for bootstrapping RE and
noise reduction for distant supervised RE; and (c)
experimental results showing that the proposed
models outperform baselines on two datasets.

2 Related Work

2.1 Automatic Seed Selection for

Bootstrapping RE

As manually selecting the seeds requires tremen-
dous effort, some research proposed methods to
select the seed automatically. Eisner and Karakos
(2005) used a “strapping” approach to evaluate
many candidate seeds automatically for a word
sense disambiguation task. Kozareva and Hovy
(2010) proposed a method for measuring seed
quality using a regression model and applied it to
the extraction of unary semantic relations, such
as“people” and “city”. Kiso et al. (2011) sug-
gested a HITS-based approach to ranking the
seeds, based on Komachi et al. (2008)’s analysis of
the Espresso algorithm (Pantel and Pennacchiotti,

1We

release

our

annotated

dataset

at

https://github.com/pvthuy/part-whole-relations.

2006). Movshovitz-Attias and Cohen (2012) gen-
erated a ranking based on pointwise mutual infor-
mation (PMI) to pick up the seeds from existing
resources in the biomedical domain. Given the
seed set of a target relation, the goal of the boot-
strapping method is to ﬁnd instances similar to
initial seeds by harvesting instances and patterns
iteratively over large corpora, e.g., Wikipedia or
ClueWeb.

2.2 Noise Reduction for Distantly Supervised

RE

The DS assumption is too strong and leads to
wrongly labeled data that affects performance.
Many studies focused on methods of noise re-
duction in DS. Intxaurrondo et al. (2013) ﬁltered
out noisy mentions from the distantly supervised
dataset using their frequencies, PMI, or the sim-
ilarity between the centroids of all relation men-
tions and each individual mention. Xiang et al.
(2016) introduced ranking-based methods accord-
ing to different strategies to select effective train-
ing groups. Li et al. (2017) proposed three novel
heuristics that use lexical and syntactic informa-
tion to remove noise in the biomedical domain.
The data generated by the noise reduction process
can be used by supervised learning algorithms to
train models.

3 Problem Formulation

Let R∗ be the set of target relations. The goal is
to ﬁnd instances, or pairs of entities, upon which
the relation holds. For each target relation r ∈
R∗, we assume there is a set Dr of triples rep-
resenting the relation r. The triples in Dr have
the form (e1, p, e2), where e1 and e2 denote en-
tities, and p denotes the pattern that connects the
two entities. A pair of entities (e1, e2) is called
an instance. This terminology is similar to the
one used in open information extraction systems,
such as Reverb (Fader et al., 2011). For example,
in triple (Barack Obama, was born in, Honolulu),
(Barack Obama, Honolulu) is the instance, and
“was born in” is the pattern.

The two tasks we address are deﬁned as follows:

Seed Selection for Bootstrapping RE:
In au-
tomatic seed selection, a set R∗ of target rela-
tions and sets of instance-pattern triples Dr =
{(e1, p, e2)} representing each target relation r ∈
R∗ are given as input. These triples are extracted
from existing corpus or database, e.g., WordNet.

With these inputs, the task is to choose good seeds
from the instances appearing in Dr for each r ∈
R∗, such that they work effectively in bootstrap-
ping RE.

Noise Reduction for Distantly Supervised RE:
In noise reduction for distantly supervised RE, the
input is the target relations R∗ and the sets Dr of
triples2 generated automatically by DS for each
relation r ∈ R∗. Because the data is generated
automatically by DS, Dr may contain noise, i.e.,
triples (e1, p, e2) for which relation r does not ac-
tually hold between e1 and e2. The goal of noise
reduction is to ﬁlter out these noisy triples, so that
they do not deteriorate the quality of the triple
classiﬁer trained subsequently.

Formulation as Ranking Tasks: As we can see
from the task deﬁnitions above, both seed selec-
tion and noise reduction are the task of selecting
Indeed, the two
triples from a given collection.
tasks essentially have a similar goal in terms of
the ranking-based perspective. We thus formulate
them as the task of ranking instances (in seed se-
lection) or triples (in noise reduction), given a set
of (possibly noisy) triples. In the seed selection
task, we use the k highest ranked instances as the
seeds for bootstrapping RE. Likewise, in noise re-
duction for DS, we only use the k highest ranked
triples from the DS-generated data to train a clas-
siﬁer. Note that the value of k in noise reduction
may be much larger than in seed selection.

4 Approaches to Automatic Seed
Selection and Noise Reduction

In this section, we propose several methods that
can be applied for both automatic seed selec-
tion and noise reduction tasks, inspired by rank-
ing relation instances and patterns computed by
the HITS algorithm, and picking cluster cen-
troids using the K-means, latent semantic anal-
ysis (LSA), or non-negative matrix factorization
(NMF) method.

4.1 K-means-based Approach

The ﬁrst method we describe is a K-means-based
It is described as follows: (1) De-
approach.

2 To be precise, in each triple (e1, s, e2) generated by DS,
s is not a pattern but a sentence that contains entities e1 and
e2. However, we can easily convert each instance-sentence
triple (e1, s, e2) to an instance-pattern triple (e1, p, e2) by
looking for a pattern p that connects two entities in sentence
s.

each instance/pattern as a node in the graph. This
representation is similar to that used by Kiso et al.
(2011). In the second graph representation, pat-
terns and instances are treated as nodes and edges,
respectively. Similarly, instances and patterns are
treated as nodes and edges, respectively in the last
representation. (3) For the ﬁrst and third types,
we simply retain the top-k instances with the high-
est hubness scores as the outputs (we sort the in-
stances in descending order based on their hub-
ness scores). For the second type, k instances as-
sociated with the highest scoring patterns are cho-
sen (we ﬁrst sort the patterns in descending order
based on their hubness scores).

4.3 HITS- and K-means-based Approach

In the combined method of HITS and K-means
algorithms, we ﬁrst rank the instances and pat-
terns based on their bipartite graph and then run
K-means to cluster instances in our annotated
dataset. However, instead of choosing the instance
nearest to the centroid, we retain the one that has
the highest HITS hubness score in each cluster.

4.4 LSA-based Approach

Latent semantic analysis (LSA) (Deerwester et al.,
1990) is also a widely used method for the au-
tomatic clustering of data along multiple dimen-
sions. Singular value decomposition (SVD) is
used to construct a low-rank approximation of
the instance-pattern co-occurrence matrix A. The
SVD projection is performed by decomposing the
matrix A ∈ RM ×N into the product of three
matrices, namely an SVD instance matrix I ∈
RM ×K, a diagonal matrix of singular values S ∈
RK×K, and an SVD pattern matrix P ∈ RK×N :

A ≈ ISPT

Our LSA-based seed selection strategy is as
(1) Specify the desired number k of
follows:
triples. (2) Use the LSA algorithm to decompose
the instance-pattern co-occurrence matrix A into
three matrices I, S, and P. We set the number of
LSA dimensions to K = k. (3) We can consider
LSA as a form of soft clustering, with each column
of the SVD instance matrix I corresponding to a
cluster. Then, we select the k instances that have
the highest absolute values from each column of I.

4.5 NMF-based Approach

Non-negative matrix factorization (NMF) (Paatero
and Tapper, 1994; Lee and Seung, 1999) is an-

Figure 1: Graph representations of instances and
patterns using the HITS algorithm.

termine the number k of instances/triples that
should be selected3. (2) Run the K-means clus-
tering algorithm to partition all instances in the in-
put triples (see Section 3) into k clusters. Each
data point is represented by the embedding vec-
tor difference between its entities; e.g., the in-
stance I = (Barack Obama, Honolulu) corre-
sponds to: vec(I) = vec(“Barack Obama”) −
vec(“Honolulu”). We use pre-trained vectors
published by Mikolov et al. (2013). (3) The in-
stance closest to the centroid is selected in each
cluster. Given that the number of clusters is k, the
same number of instances/triples will be chosen.

4.2 HITS-based Approach

Hypertext-induced topic search (HITS) (Klein-
the hubs-and-
also known as
berg, 1999),
authorities algorithm, is a link analysis method
for ranking web pages. In HITS, a good hub is a
page that points to many good authorities and vice
versa; a good authority is a page that is pointed to
by many good hubs. These hubs and authorities
form a bipartite graph, where we can compute the
hubness score of each node.

In our task, let A be the instance-pattern co-
occurrence matrix. We can compute the hubness
score for each instance on the bipartite graph of
instances and patterns induced by the matrix A.
Inspired by the way HITS ranks hubs and authori-
ties, our HITS-based seed selection strategy can be
explained as follows: (1) Determine the number k
of triples that should be selected.(2) Build the bi-
partite graph of instances and patterns based on the
instance-pattern co-occurrence matrix A. Figure 1
presents three possible ways of building a bipar-
tite graph. For the ﬁrst type of graph, we consider

3 Depending on the task, instances or triples will be se-
lected: instances for the automatic seed selection task, and
triples for the noise reduction task. As instances are pairs
of entities which are included in triples, we can simply con-
vert between the instance and the triple, and apply a proposed
method to both tasks.

Subtype
Component-Of
Member-Of
Portion-Of
Stuff-Of
Located-In
Contained-In
Phase-Of
Participates-In
TOTAL

Freq
643 (11.23%)
1,272 (22.21%)
555 ( 9.69%)
1,082 (18.89%)
534 ( 9.32%)
272 ( 4.75%)
497 ( 8.68%)
872 (15.23%)
5,727 triples

Table 1: Statistics of our part-whole dataset.

other method for approximate non-negative ma-
trix factorization. The non-negative data matrix
A ∈ RM ×N is represented by two non-negative
factors W ∈ RM ×K and H ∈ RK×N , which,
when multiplied, approximately reconstruct A:

A ≈ WH

The non-negativity constraint is the main differ-
ence between NMF and LSA. Similarly to the
LSA-based method, we set the NMF parameter K
to k, the desired number of instances to select. We
then select the k instances that have the highest
values from each column of W.

5 Experiments

5.1 Datasets and Settings

We provide an annotated dataset of part-whole re-
lations as a reliable resource for selecting seeds.
Our dataset was collected from Wikipedia and
ClueWeb, and annotated by two annotators. One
of its special characteristics is that the part-whole
relation is a collection of relations, not a single re-
lation (Iris, 1989; Winston et al., 1987).

Table 1 gives the frequencies of each sub-
type of part-whole relations. There are 5,727 in-
stances of 8 subtypes that were annotated with
the same labels by both annotators. We use
Espresso+Word2vec (Phi and Matsumoto, 2016),
which is an improved version for the origi-
nal Espresso algorithm (Pantel and Pennacchiotti,
Espresso+Word2vec outperformed the
2006).
Espresso system for harvesting part-whole rela-
tions by utilizing the Similarity Ranker, which
uses the embedded vector difference between in-
stance pairs of relations. The performance is mea-
sured with Precision@N (Manning et al., 2008),
N = 50. In total, 5,000 instances are checked by

Method
K-means
HITS Graph1
HITS Graph2
HITS Graph3
HITS+K-means Graph1
HITS+K-means Graph2
HITS+K-means Graph3
LSA
NMF
Random

Average P@50
0.96
0.90
0.85
0.90
0.92
0.85
0.94
0.90
0.89
0.75

Table 2: Performance of seed selection methods.

annotators to ascertain whether they express part-
whole relations. We vary the number k of seeds
between 5 and 50 with a step of 5 to report the
average P@50 of each seed selection method.

For the noise reduction task, we use the training
and testing set developed by (Riedel et al., 2010),
which contains 53 relation classes. This dataset
was generated by aligning Freebase relations with
the New York Times corpus. After removing noisy
triples from the dataset using the proposed meth-
ods, we use the ﬁltered data to train two kinds of
convolutional neural networks (CNN) (the CNN
model in (Zeng et al., 2014) and the PCNN model
in (Zeng et al., 2015)) with at-least-one multi-
instance learning (ONE) used in (Zeng et al.,
2015), and the sentence-level attention (ATT) used
in (Lin et al., 2016). Finally, we report the area
under the precision-recall (AUCPR) of each noise
reduction method.

5.2 Performance on Automatic Seed

Selection Task

The performances of the seed selection methods
are presented in Table 2. For the HITS-based
and HITS+K-means-based methods, we display
the P@50 with three types of graph representation
as shown in Section 4.2. We use random seed se-
lection as the baseline for comparison. As Table 2
shows, the random method achieved a precision
of 0.75. The relation extraction system that uses
the random method has the worst average P@50
among all seed selection strategies. The HITS-
based method’s P@50s when using Graph1 and
Graph3 are conﬁrmed to be better than when us-
ing Graph2. This indicates that relying on reli-
able instances is better than reasoning over pat-
terns (recall that for the Graph2, we ﬁrst choose

System
CNN+ONE
CNN+ATT
PCNN+ONE
PCNN+ATT

Original +HITS +LSA +NMF +Ensemble

0.180
0.234
0.231
0.248

0.183
0.235
0.234
0.253

0.173
0.235
0.233
0.250

0.178
0.233
0.234
0.252

0.181
0.236
0.235
0.255

Table 3: Performance (AUCPR) of each noise reduction method; in bold are the best scores.

the patterns, then select the instances associated
with those patterns), as there is a possibility that a
pattern can be ambiguous, and therefore, instances
linked to that pattern can be incorrect. The K-
means-based seed selection method provides the
best average P@50 with a performance of 0.96.
The HITS+K-means-based method performs bet-
ter than using only the HITS strategy, while the
LSA-based and NMF-based methods have a com-
parable performance.

5.3 Performance on Noise Reduction Task

similar:

Table 3 presents the performance of noise re-
duction methods. Recall that the K-means-based
the seed
method achieves a high P@50 for
selection method. Our assumption is that each
cluster may represent a set in which elements
have similar semantic properties.
However,
we observed that as the number of relations
is relatively high and there is no distinct def-
inition between some relations in the distantly
labeled data (e.g., the following three relations
/location/country/capital,
are quite
/location/province/capital,
and
/location/us state/capital, we decided not
to
perform the K-means-based method for our
noise reduction task. The performances of the
HITS-based, LSA-based, and NMF-based noise
reduction methods are presented in Table 3. We
experimentally set the portion of retained data
from the distantly labeled data to 90%, given
that the performance can be affected if too many
sentences are removed from the original data.
We also perform experiments with an ensem-
ble method that combines the HITS-based and
LSA-based strategies to merge rankings from
their outputs, with half of the triples coming from
the LSA-based method and the other half from
the HITS-based method. Table 3 indicates that
our proposed methods improved the performance
of all CNN and PCNN models. Our ensemble
method achieved the best improvements for three
out of four systems, except that the HITS-based

method obtained the best score for CNN+ONE.

6 Conclusion

We formulated the seed selection and noise re-
duction tasks as ranking problems.
In addition,
we proposed several methods, inspired by rank-
ing instances and patterns computed by the HITS
algorithm, and selecting clusters centroids using
the K-means, LSA, or NMF method. Experi-
ments demonstrated that our proposed methods
improved the baselines in both tasks.

Acknowledgments

This work was partly supported by JST CREST
Grant Number JPMJCR1513 and MEXT/JSPS
Kakenhi Grant Number JP15H02749. We are
grateful to the members of the Computational Lin-
guistics Laboratory, NAIST and the anonymous
reviewers for their insightful comments.

References

Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM Conference
on Digital Libraries, pages 85–94. ACM.

Sergey Brin. 1998. Extracting patterns and relations
In International Work-
from the world wide web.
shop on the World Wide Web and Databases, pages
172–183. Springer.

Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391.

Jason Eisner and Damianos Karakos. 2005. Bootstrap-
ping without the boot. In Proceedings of the 2005
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on

Empirical Methods in Natural Language Process-
ing, pages 1535–1545. Association for Computa-
tional Linguistics.

Ander Intxaurrondo, Mihai Surdeanu, Oier Lopez de
Lacalle, and Eneko Agirre. 2013. Removing noisy
mentions for distant supervision. Procesamiento del
lenguaje natural, 51.

Madelyn Anne Iris. 1989. Problems of the part-whole
relation. In Relational Models of the Lexicon, pages
261–288. Cambridge University Press.

Tetsuo Kiso, Masashi Shimbo, Mamoru Komachi, and
Yuji Matsumoto. 2011. HITS-based seed selection
and stop list construction for bootstrapping. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 30–36. Association for
Computational Linguistics.

Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604–632.

Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of
semantic drift in Espresso-like bootstrapping algo-
rithms. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1011–1020. Association for Computa-
tional Linguistics.

Zornitsa Kozareva and Eduard Hovy. 2010. Not all
seeds are equal: Measuring the quality of text min-
ing seeds. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 618–626. Association for Computa-
tional Linguistics.

Daniel D Lee and H. Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factor-
ization. Nature, 401(6755):788–791.

Gang Li, Cathy Wu, and K. Vijay-Shanker. 2017.
Noise reduction methods for distantly supervised
In SIGBioMed
biomedical relation extraction.
Workshop on Biomedical Natural Language Pro-
cessing (BioNLP ’17), pages 184–193. Association
for Computational Linguistics.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2124–2133. Association for Computa-
tional Linguistics.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003–1011. Association for Computational Linguis-
tics.

Dana Movshovitz-Attias and William W. Cohen. 2012.
Bootstrapping biomedical ontologies for scientiﬁc
In BioNLP: Proceedings of the
text using nell.
2012 Workshop on Biomedical Natural Language
Processing, pages 11–19. Association for Compu-
tational Linguistics.

Pentti Paatero and Unto Tapper. 1994. Positive ma-
trix factorization: A non-negative factor model with
optimal utilization of error estimates of data values.
Environmetrics, 5(2):111–126.

Patrick Pantel

and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics,
pages 113–120. Association for Computational
Linguistics.

Van-Thuy Phi and Yuji Matsumoto. 2016. Integrating
word embedding offsets into the Espresso system for
part-whole relation extraction. In Proceedings of the
30th Paciﬁc Asia Conference on Language, Informa-
tion and Computation: Oral Papers, pages 173–181.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
In Proceedings of the 2010 Joint
out labeled text.
European Conference on Machine Learning and
Principles of Knowledge Discovery in Databases
(ECML PKDD), pages 148–163. Springer.

Ellen Riloff, Rosie Jones, et al. 1999. Learning dic-
tionaries for information extraction by multi-level
In Proceedings of the 16th Na-
bootstrapping.
tional Conference on Artiﬁcial Intelligence and the
11th Innovative Applications of Artiﬁcial Intelli-
gence Conference (AAAI/IAAI), pages 474–479.

Morton E. Winston, Roger Chafﬁn, and Douglas Her-
rmann. 1987. A taxonomy of part-whole relations.
Cognitive Science, 11(4):417–444.

Yang Xiang, Qingcai Chen, Xiaolong Wang, and Yang
Qin. 2016. Distant supervision for relation ex-
Entropy,
traction with ranking-based methods.
18(6):204.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
In Pro-
piecewise convolutional neural networks.
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1753–
1762. Association for Computational Linguistics.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classiﬁcation via con-
volutional deep neural network. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335–2344. Dublin City University and As-
sociation for Computational Linguistics.

Ranking-Based Automatic Seed Selection and Noise Reduction for Weakly
Supervised Relation Extraction

Van-Thuy Phi1, Joan Santoso2, Masashi Shimbo1 and Yuji Matsumoto1,3

1 Nara Institute of Science and Technology
2Sekolah Tinggi Teknik Surabaya
3RIKEN Center for Advanced Intelligence Project (AIP)
1{phi.thuy.ph8, shimbo, matsu}@is.naist.jp
2joan@stts.edu

Abstract

This paper addresses the tasks of auto-
matic seed selection for bootstrapping re-
lation extraction, and noise reduction for
distantly supervised relation extraction.
We ﬁrst point out that these tasks are re-
lated. Then, inspired by ranking relation
instances and patterns computed by the
HITS algorithm, and selecting cluster cen-
troids using the K-means, LSA, or NMF
method, we propose methods for selecting
the initial seeds from an existing resource,
or reducing the level of noise in the dis-
tantly labeled data. Experiments show that
our proposed methods achieve a better per-
formance than the baseline systems in both
tasks.

1

Introduction

Bootstrapping for relation extraction (RE) (Brin,
1998; Riloff et al., 1999; Agichtein and Gravano,
2000) is a class of minimally supervised meth-
ods frequently used in machine learning: initial-
ized by a small set of example instances called
seeds, to represent a particular semantic relation,
the bootstrapping system operates iteratively to ac-
quire new instances of a target relation. Selecting
“good” seeds is one of the most important steps
to reduce semantic drift, which is a typical phe-
nomenon of the bootstrapping process.

Another approach, called “distant supervision”
(DS) (Mintz et al., 2009), does not require any la-
bels on the text. The assumption of DS is that if
two entities participate in a known Freebase rela-
tion, any sentence that contains those two entities
might express that relation. However, this tech-
nique often introduces noise to the generated train-
ing data. As a result, DS is still limited by the

quality of training data, and noise existing in pos-
itively labeled data may affect the performance of
supervised learning.

In this study, we propose methods that can
be applied for both automatic seed selection and
noise reduction by formulating these tasks as rank-
ing problems according to different ranking crite-
ria. Our methods are inspired by ranking instances
and patterns computed by the HITS algorithm, and
selecting cluster centroids using K-means, latent
semantic analysis, or the non-negative matrix fac-
torization method. The main contributions of this
paper are (a) an annotated dataset of 5,727 part-
whole relations1, which contains 8 subtypes for
the bootstrapping RE system; (b) methods for au-
tomatic seed selection for bootstrapping RE and
noise reduction for distant supervised RE; and (c)
experimental results showing that the proposed
models outperform baselines on two datasets.

2 Related Work

2.1 Automatic Seed Selection for

Bootstrapping RE

As manually selecting the seeds requires tremen-
dous effort, some research proposed methods to
select the seed automatically. Eisner and Karakos
(2005) used a “strapping” approach to evaluate
many candidate seeds automatically for a word
sense disambiguation task. Kozareva and Hovy
(2010) proposed a method for measuring seed
quality using a regression model and applied it to
the extraction of unary semantic relations, such
as“people” and “city”. Kiso et al. (2011) sug-
gested a HITS-based approach to ranking the
seeds, based on Komachi et al. (2008)’s analysis of
the Espresso algorithm (Pantel and Pennacchiotti,

1We

release

our

annotated

dataset

at

https://github.com/pvthuy/part-whole-relations.

2006). Movshovitz-Attias and Cohen (2012) gen-
erated a ranking based on pointwise mutual infor-
mation (PMI) to pick up the seeds from existing
resources in the biomedical domain. Given the
seed set of a target relation, the goal of the boot-
strapping method is to ﬁnd instances similar to
initial seeds by harvesting instances and patterns
iteratively over large corpora, e.g., Wikipedia or
ClueWeb.

2.2 Noise Reduction for Distantly Supervised

RE

The DS assumption is too strong and leads to
wrongly labeled data that affects performance.
Many studies focused on methods of noise re-
duction in DS. Intxaurrondo et al. (2013) ﬁltered
out noisy mentions from the distantly supervised
dataset using their frequencies, PMI, or the sim-
ilarity between the centroids of all relation men-
tions and each individual mention. Xiang et al.
(2016) introduced ranking-based methods accord-
ing to different strategies to select effective train-
ing groups. Li et al. (2017) proposed three novel
heuristics that use lexical and syntactic informa-
tion to remove noise in the biomedical domain.
The data generated by the noise reduction process
can be used by supervised learning algorithms to
train models.

3 Problem Formulation

Let R∗ be the set of target relations. The goal is
to ﬁnd instances, or pairs of entities, upon which
the relation holds. For each target relation r ∈
R∗, we assume there is a set Dr of triples rep-
resenting the relation r. The triples in Dr have
the form (e1, p, e2), where e1 and e2 denote en-
tities, and p denotes the pattern that connects the
two entities. A pair of entities (e1, e2) is called
an instance. This terminology is similar to the
one used in open information extraction systems,
such as Reverb (Fader et al., 2011). For example,
in triple (Barack Obama, was born in, Honolulu),
(Barack Obama, Honolulu) is the instance, and
“was born in” is the pattern.

The two tasks we address are deﬁned as follows:

Seed Selection for Bootstrapping RE:
In au-
tomatic seed selection, a set R∗ of target rela-
tions and sets of instance-pattern triples Dr =
{(e1, p, e2)} representing each target relation r ∈
R∗ are given as input. These triples are extracted
from existing corpus or database, e.g., WordNet.

With these inputs, the task is to choose good seeds
from the instances appearing in Dr for each r ∈
R∗, such that they work effectively in bootstrap-
ping RE.

Noise Reduction for Distantly Supervised RE:
In noise reduction for distantly supervised RE, the
input is the target relations R∗ and the sets Dr of
triples2 generated automatically by DS for each
relation r ∈ R∗. Because the data is generated
automatically by DS, Dr may contain noise, i.e.,
triples (e1, p, e2) for which relation r does not ac-
tually hold between e1 and e2. The goal of noise
reduction is to ﬁlter out these noisy triples, so that
they do not deteriorate the quality of the triple
classiﬁer trained subsequently.

Formulation as Ranking Tasks: As we can see
from the task deﬁnitions above, both seed selec-
tion and noise reduction are the task of selecting
Indeed, the two
triples from a given collection.
tasks essentially have a similar goal in terms of
the ranking-based perspective. We thus formulate
them as the task of ranking instances (in seed se-
lection) or triples (in noise reduction), given a set
of (possibly noisy) triples. In the seed selection
task, we use the k highest ranked instances as the
seeds for bootstrapping RE. Likewise, in noise re-
duction for DS, we only use the k highest ranked
triples from the DS-generated data to train a clas-
siﬁer. Note that the value of k in noise reduction
may be much larger than in seed selection.

4 Approaches to Automatic Seed
Selection and Noise Reduction

In this section, we propose several methods that
can be applied for both automatic seed selec-
tion and noise reduction tasks, inspired by rank-
ing relation instances and patterns computed by
the HITS algorithm, and picking cluster cen-
troids using the K-means, latent semantic anal-
ysis (LSA), or non-negative matrix factorization
(NMF) method.

4.1 K-means-based Approach

The ﬁrst method we describe is a K-means-based
It is described as follows: (1) De-
approach.

2 To be precise, in each triple (e1, s, e2) generated by DS,
s is not a pattern but a sentence that contains entities e1 and
e2. However, we can easily convert each instance-sentence
triple (e1, s, e2) to an instance-pattern triple (e1, p, e2) by
looking for a pattern p that connects two entities in sentence
s.

each instance/pattern as a node in the graph. This
representation is similar to that used by Kiso et al.
(2011). In the second graph representation, pat-
terns and instances are treated as nodes and edges,
respectively. Similarly, instances and patterns are
treated as nodes and edges, respectively in the last
representation. (3) For the ﬁrst and third types,
we simply retain the top-k instances with the high-
est hubness scores as the outputs (we sort the in-
stances in descending order based on their hub-
ness scores). For the second type, k instances as-
sociated with the highest scoring patterns are cho-
sen (we ﬁrst sort the patterns in descending order
based on their hubness scores).

4.3 HITS- and K-means-based Approach

In the combined method of HITS and K-means
algorithms, we ﬁrst rank the instances and pat-
terns based on their bipartite graph and then run
K-means to cluster instances in our annotated
dataset. However, instead of choosing the instance
nearest to the centroid, we retain the one that has
the highest HITS hubness score in each cluster.

4.4 LSA-based Approach

Latent semantic analysis (LSA) (Deerwester et al.,
1990) is also a widely used method for the au-
tomatic clustering of data along multiple dimen-
sions. Singular value decomposition (SVD) is
used to construct a low-rank approximation of
the instance-pattern co-occurrence matrix A. The
SVD projection is performed by decomposing the
matrix A ∈ RM ×N into the product of three
matrices, namely an SVD instance matrix I ∈
RM ×K, a diagonal matrix of singular values S ∈
RK×K, and an SVD pattern matrix P ∈ RK×N :

A ≈ ISPT

Our LSA-based seed selection strategy is as
(1) Specify the desired number k of
follows:
triples. (2) Use the LSA algorithm to decompose
the instance-pattern co-occurrence matrix A into
three matrices I, S, and P. We set the number of
LSA dimensions to K = k. (3) We can consider
LSA as a form of soft clustering, with each column
of the SVD instance matrix I corresponding to a
cluster. Then, we select the k instances that have
the highest absolute values from each column of I.

4.5 NMF-based Approach

Non-negative matrix factorization (NMF) (Paatero
and Tapper, 1994; Lee and Seung, 1999) is an-

Figure 1: Graph representations of instances and
patterns using the HITS algorithm.

termine the number k of instances/triples that
should be selected3. (2) Run the K-means clus-
tering algorithm to partition all instances in the in-
put triples (see Section 3) into k clusters. Each
data point is represented by the embedding vec-
tor difference between its entities; e.g., the in-
stance I = (Barack Obama, Honolulu) corre-
sponds to: vec(I) = vec(“Barack Obama”) −
vec(“Honolulu”). We use pre-trained vectors
published by Mikolov et al. (2013). (3) The in-
stance closest to the centroid is selected in each
cluster. Given that the number of clusters is k, the
same number of instances/triples will be chosen.

4.2 HITS-based Approach

Hypertext-induced topic search (HITS) (Klein-
the hubs-and-
also known as
berg, 1999),
authorities algorithm, is a link analysis method
for ranking web pages. In HITS, a good hub is a
page that points to many good authorities and vice
versa; a good authority is a page that is pointed to
by many good hubs. These hubs and authorities
form a bipartite graph, where we can compute the
hubness score of each node.

In our task, let A be the instance-pattern co-
occurrence matrix. We can compute the hubness
score for each instance on the bipartite graph of
instances and patterns induced by the matrix A.
Inspired by the way HITS ranks hubs and authori-
ties, our HITS-based seed selection strategy can be
explained as follows: (1) Determine the number k
of triples that should be selected.(2) Build the bi-
partite graph of instances and patterns based on the
instance-pattern co-occurrence matrix A. Figure 1
presents three possible ways of building a bipar-
tite graph. For the ﬁrst type of graph, we consider

3 Depending on the task, instances or triples will be se-
lected: instances for the automatic seed selection task, and
triples for the noise reduction task. As instances are pairs
of entities which are included in triples, we can simply con-
vert between the instance and the triple, and apply a proposed
method to both tasks.

Subtype
Component-Of
Member-Of
Portion-Of
Stuff-Of
Located-In
Contained-In
Phase-Of
Participates-In
TOTAL

Freq
643 (11.23%)
1,272 (22.21%)
555 ( 9.69%)
1,082 (18.89%)
534 ( 9.32%)
272 ( 4.75%)
497 ( 8.68%)
872 (15.23%)
5,727 triples

Table 1: Statistics of our part-whole dataset.

other method for approximate non-negative ma-
trix factorization. The non-negative data matrix
A ∈ RM ×N is represented by two non-negative
factors W ∈ RM ×K and H ∈ RK×N , which,
when multiplied, approximately reconstruct A:

A ≈ WH

The non-negativity constraint is the main differ-
ence between NMF and LSA. Similarly to the
LSA-based method, we set the NMF parameter K
to k, the desired number of instances to select. We
then select the k instances that have the highest
values from each column of W.

5 Experiments

5.1 Datasets and Settings

We provide an annotated dataset of part-whole re-
lations as a reliable resource for selecting seeds.
Our dataset was collected from Wikipedia and
ClueWeb, and annotated by two annotators. One
of its special characteristics is that the part-whole
relation is a collection of relations, not a single re-
lation (Iris, 1989; Winston et al., 1987).

Table 1 gives the frequencies of each sub-
type of part-whole relations. There are 5,727 in-
stances of 8 subtypes that were annotated with
the same labels by both annotators. We use
Espresso+Word2vec (Phi and Matsumoto, 2016),
which is an improved version for the origi-
nal Espresso algorithm (Pantel and Pennacchiotti,
Espresso+Word2vec outperformed the
2006).
Espresso system for harvesting part-whole rela-
tions by utilizing the Similarity Ranker, which
uses the embedded vector difference between in-
stance pairs of relations. The performance is mea-
sured with Precision@N (Manning et al., 2008),
N = 50. In total, 5,000 instances are checked by

Method
K-means
HITS Graph1
HITS Graph2
HITS Graph3
HITS+K-means Graph1
HITS+K-means Graph2
HITS+K-means Graph3
LSA
NMF
Random

Average P@50
0.96
0.90
0.85
0.90
0.92
0.85
0.94
0.90
0.89
0.75

Table 2: Performance of seed selection methods.

annotators to ascertain whether they express part-
whole relations. We vary the number k of seeds
between 5 and 50 with a step of 5 to report the
average P@50 of each seed selection method.

For the noise reduction task, we use the training
and testing set developed by (Riedel et al., 2010),
which contains 53 relation classes. This dataset
was generated by aligning Freebase relations with
the New York Times corpus. After removing noisy
triples from the dataset using the proposed meth-
ods, we use the ﬁltered data to train two kinds of
convolutional neural networks (CNN) (the CNN
model in (Zeng et al., 2014) and the PCNN model
in (Zeng et al., 2015)) with at-least-one multi-
instance learning (ONE) used in (Zeng et al.,
2015), and the sentence-level attention (ATT) used
in (Lin et al., 2016). Finally, we report the area
under the precision-recall (AUCPR) of each noise
reduction method.

5.2 Performance on Automatic Seed

Selection Task

The performances of the seed selection methods
are presented in Table 2. For the HITS-based
and HITS+K-means-based methods, we display
the P@50 with three types of graph representation
as shown in Section 4.2. We use random seed se-
lection as the baseline for comparison. As Table 2
shows, the random method achieved a precision
of 0.75. The relation extraction system that uses
the random method has the worst average P@50
among all seed selection strategies. The HITS-
based method’s P@50s when using Graph1 and
Graph3 are conﬁrmed to be better than when us-
ing Graph2. This indicates that relying on reli-
able instances is better than reasoning over pat-
terns (recall that for the Graph2, we ﬁrst choose

System
CNN+ONE
CNN+ATT
PCNN+ONE
PCNN+ATT

Original +HITS +LSA +NMF +Ensemble

0.180
0.234
0.231
0.248

0.183
0.235
0.234
0.253

0.173
0.235
0.233
0.250

0.178
0.233
0.234
0.252

0.181
0.236
0.235
0.255

Table 3: Performance (AUCPR) of each noise reduction method; in bold are the best scores.

the patterns, then select the instances associated
with those patterns), as there is a possibility that a
pattern can be ambiguous, and therefore, instances
linked to that pattern can be incorrect. The K-
means-based seed selection method provides the
best average P@50 with a performance of 0.96.
The HITS+K-means-based method performs bet-
ter than using only the HITS strategy, while the
LSA-based and NMF-based methods have a com-
parable performance.

5.3 Performance on Noise Reduction Task

similar:

Table 3 presents the performance of noise re-
duction methods. Recall that the K-means-based
the seed
method achieves a high P@50 for
selection method. Our assumption is that each
cluster may represent a set in which elements
have similar semantic properties.
However,
we observed that as the number of relations
is relatively high and there is no distinct def-
inition between some relations in the distantly
labeled data (e.g., the following three relations
/location/country/capital,
are quite
/location/province/capital,
and
/location/us state/capital, we decided not
to
perform the K-means-based method for our
noise reduction task. The performances of the
HITS-based, LSA-based, and NMF-based noise
reduction methods are presented in Table 3. We
experimentally set the portion of retained data
from the distantly labeled data to 90%, given
that the performance can be affected if too many
sentences are removed from the original data.
We also perform experiments with an ensem-
ble method that combines the HITS-based and
LSA-based strategies to merge rankings from
their outputs, with half of the triples coming from
the LSA-based method and the other half from
the HITS-based method. Table 3 indicates that
our proposed methods improved the performance
of all CNN and PCNN models. Our ensemble
method achieved the best improvements for three
out of four systems, except that the HITS-based

method obtained the best score for CNN+ONE.

6 Conclusion

We formulated the seed selection and noise re-
duction tasks as ranking problems.
In addition,
we proposed several methods, inspired by rank-
ing instances and patterns computed by the HITS
algorithm, and selecting clusters centroids using
the K-means, LSA, or NMF method. Experi-
ments demonstrated that our proposed methods
improved the baselines in both tasks.

Acknowledgments

This work was partly supported by JST CREST
Grant Number JPMJCR1513 and MEXT/JSPS
Kakenhi Grant Number JP15H02749. We are
grateful to the members of the Computational Lin-
guistics Laboratory, NAIST and the anonymous
reviewers for their insightful comments.

References

Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM Conference
on Digital Libraries, pages 85–94. ACM.

Sergey Brin. 1998. Extracting patterns and relations
In International Work-
from the world wide web.
shop on the World Wide Web and Databases, pages
172–183. Springer.

Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391.

Jason Eisner and Damianos Karakos. 2005. Bootstrap-
ping without the boot. In Proceedings of the 2005
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on

Empirical Methods in Natural Language Process-
ing, pages 1535–1545. Association for Computa-
tional Linguistics.

Ander Intxaurrondo, Mihai Surdeanu, Oier Lopez de
Lacalle, and Eneko Agirre. 2013. Removing noisy
mentions for distant supervision. Procesamiento del
lenguaje natural, 51.

Madelyn Anne Iris. 1989. Problems of the part-whole
relation. In Relational Models of the Lexicon, pages
261–288. Cambridge University Press.

Tetsuo Kiso, Masashi Shimbo, Mamoru Komachi, and
Yuji Matsumoto. 2011. HITS-based seed selection
and stop list construction for bootstrapping. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 30–36. Association for
Computational Linguistics.

Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604–632.

Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of
semantic drift in Espresso-like bootstrapping algo-
rithms. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1011–1020. Association for Computa-
tional Linguistics.

Zornitsa Kozareva and Eduard Hovy. 2010. Not all
seeds are equal: Measuring the quality of text min-
ing seeds. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 618–626. Association for Computa-
tional Linguistics.

Daniel D Lee and H. Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factor-
ization. Nature, 401(6755):788–791.

Gang Li, Cathy Wu, and K. Vijay-Shanker. 2017.
Noise reduction methods for distantly supervised
In SIGBioMed
biomedical relation extraction.
Workshop on Biomedical Natural Language Pro-
cessing (BioNLP ’17), pages 184–193. Association
for Computational Linguistics.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2124–2133. Association for Computa-
tional Linguistics.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003–1011. Association for Computational Linguis-
tics.

Dana Movshovitz-Attias and William W. Cohen. 2012.
Bootstrapping biomedical ontologies for scientiﬁc
In BioNLP: Proceedings of the
text using nell.
2012 Workshop on Biomedical Natural Language
Processing, pages 11–19. Association for Compu-
tational Linguistics.

Pentti Paatero and Unto Tapper. 1994. Positive ma-
trix factorization: A non-negative factor model with
optimal utilization of error estimates of data values.
Environmetrics, 5(2):111–126.

Patrick Pantel

and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics,
pages 113–120. Association for Computational
Linguistics.

Van-Thuy Phi and Yuji Matsumoto. 2016. Integrating
word embedding offsets into the Espresso system for
part-whole relation extraction. In Proceedings of the
30th Paciﬁc Asia Conference on Language, Informa-
tion and Computation: Oral Papers, pages 173–181.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
In Proceedings of the 2010 Joint
out labeled text.
European Conference on Machine Learning and
Principles of Knowledge Discovery in Databases
(ECML PKDD), pages 148–163. Springer.

Ellen Riloff, Rosie Jones, et al. 1999. Learning dic-
tionaries for information extraction by multi-level
In Proceedings of the 16th Na-
bootstrapping.
tional Conference on Artiﬁcial Intelligence and the
11th Innovative Applications of Artiﬁcial Intelli-
gence Conference (AAAI/IAAI), pages 474–479.

Morton E. Winston, Roger Chafﬁn, and Douglas Her-
rmann. 1987. A taxonomy of part-whole relations.
Cognitive Science, 11(4):417–444.

Yang Xiang, Qingcai Chen, Xiaolong Wang, and Yang
Qin. 2016. Distant supervision for relation ex-
Entropy,
traction with ranking-based methods.
18(6):204.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
In Pro-
piecewise convolutional neural networks.
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1753–
1762. Association for Computational Linguistics.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classiﬁcation via con-
volutional deep neural network. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335–2344. Dublin City University and As-
sociation for Computational Linguistics.

Ranking-Based Automatic Seed Selection and Noise Reduction for Weakly
Supervised Relation Extraction

Van-Thuy Phi1, Joan Santoso2, Masashi Shimbo1 and Yuji Matsumoto1,3

1 Nara Institute of Science and Technology
2Sekolah Tinggi Teknik Surabaya
3RIKEN Center for Advanced Intelligence Project (AIP)
1{phi.thuy.ph8, shimbo, matsu}@is.naist.jp
2joan@stts.edu

Abstract

This paper addresses the tasks of auto-
matic seed selection for bootstrapping re-
lation extraction, and noise reduction for
distantly supervised relation extraction.
We ﬁrst point out that these tasks are re-
lated. Then, inspired by ranking relation
instances and patterns computed by the
HITS algorithm, and selecting cluster cen-
troids using the K-means, LSA, or NMF
method, we propose methods for selecting
the initial seeds from an existing resource,
or reducing the level of noise in the dis-
tantly labeled data. Experiments show that
our proposed methods achieve a better per-
formance than the baseline systems in both
tasks.

1

Introduction

Bootstrapping for relation extraction (RE) (Brin,
1998; Riloff et al., 1999; Agichtein and Gravano,
2000) is a class of minimally supervised meth-
ods frequently used in machine learning: initial-
ized by a small set of example instances called
seeds, to represent a particular semantic relation,
the bootstrapping system operates iteratively to ac-
quire new instances of a target relation. Selecting
“good” seeds is one of the most important steps
to reduce semantic drift, which is a typical phe-
nomenon of the bootstrapping process.

Another approach, called “distant supervision”
(DS) (Mintz et al., 2009), does not require any la-
bels on the text. The assumption of DS is that if
two entities participate in a known Freebase rela-
tion, any sentence that contains those two entities
might express that relation. However, this tech-
nique often introduces noise to the generated train-
ing data. As a result, DS is still limited by the

quality of training data, and noise existing in pos-
itively labeled data may affect the performance of
supervised learning.

In this study, we propose methods that can
be applied for both automatic seed selection and
noise reduction by formulating these tasks as rank-
ing problems according to different ranking crite-
ria. Our methods are inspired by ranking instances
and patterns computed by the HITS algorithm, and
selecting cluster centroids using K-means, latent
semantic analysis, or the non-negative matrix fac-
torization method. The main contributions of this
paper are (a) an annotated dataset of 5,727 part-
whole relations1, which contains 8 subtypes for
the bootstrapping RE system; (b) methods for au-
tomatic seed selection for bootstrapping RE and
noise reduction for distant supervised RE; and (c)
experimental results showing that the proposed
models outperform baselines on two datasets.

2 Related Work

2.1 Automatic Seed Selection for

Bootstrapping RE

As manually selecting the seeds requires tremen-
dous effort, some research proposed methods to
select the seed automatically. Eisner and Karakos
(2005) used a “strapping” approach to evaluate
many candidate seeds automatically for a word
sense disambiguation task. Kozareva and Hovy
(2010) proposed a method for measuring seed
quality using a regression model and applied it to
the extraction of unary semantic relations, such
as“people” and “city”. Kiso et al. (2011) sug-
gested a HITS-based approach to ranking the
seeds, based on Komachi et al. (2008)’s analysis of
the Espresso algorithm (Pantel and Pennacchiotti,

1We

release

our

annotated

dataset

at

https://github.com/pvthuy/part-whole-relations.

2006). Movshovitz-Attias and Cohen (2012) gen-
erated a ranking based on pointwise mutual infor-
mation (PMI) to pick up the seeds from existing
resources in the biomedical domain. Given the
seed set of a target relation, the goal of the boot-
strapping method is to ﬁnd instances similar to
initial seeds by harvesting instances and patterns
iteratively over large corpora, e.g., Wikipedia or
ClueWeb.

2.2 Noise Reduction for Distantly Supervised

RE

The DS assumption is too strong and leads to
wrongly labeled data that affects performance.
Many studies focused on methods of noise re-
duction in DS. Intxaurrondo et al. (2013) ﬁltered
out noisy mentions from the distantly supervised
dataset using their frequencies, PMI, or the sim-
ilarity between the centroids of all relation men-
tions and each individual mention. Xiang et al.
(2016) introduced ranking-based methods accord-
ing to different strategies to select effective train-
ing groups. Li et al. (2017) proposed three novel
heuristics that use lexical and syntactic informa-
tion to remove noise in the biomedical domain.
The data generated by the noise reduction process
can be used by supervised learning algorithms to
train models.

3 Problem Formulation

Let R∗ be the set of target relations. The goal is
to ﬁnd instances, or pairs of entities, upon which
the relation holds. For each target relation r ∈
R∗, we assume there is a set Dr of triples rep-
resenting the relation r. The triples in Dr have
the form (e1, p, e2), where e1 and e2 denote en-
tities, and p denotes the pattern that connects the
two entities. A pair of entities (e1, e2) is called
an instance. This terminology is similar to the
one used in open information extraction systems,
such as Reverb (Fader et al., 2011). For example,
in triple (Barack Obama, was born in, Honolulu),
(Barack Obama, Honolulu) is the instance, and
“was born in” is the pattern.

The two tasks we address are deﬁned as follows:

Seed Selection for Bootstrapping RE:
In au-
tomatic seed selection, a set R∗ of target rela-
tions and sets of instance-pattern triples Dr =
{(e1, p, e2)} representing each target relation r ∈
R∗ are given as input. These triples are extracted
from existing corpus or database, e.g., WordNet.

With these inputs, the task is to choose good seeds
from the instances appearing in Dr for each r ∈
R∗, such that they work effectively in bootstrap-
ping RE.

Noise Reduction for Distantly Supervised RE:
In noise reduction for distantly supervised RE, the
input is the target relations R∗ and the sets Dr of
triples2 generated automatically by DS for each
relation r ∈ R∗. Because the data is generated
automatically by DS, Dr may contain noise, i.e.,
triples (e1, p, e2) for which relation r does not ac-
tually hold between e1 and e2. The goal of noise
reduction is to ﬁlter out these noisy triples, so that
they do not deteriorate the quality of the triple
classiﬁer trained subsequently.

Formulation as Ranking Tasks: As we can see
from the task deﬁnitions above, both seed selec-
tion and noise reduction are the task of selecting
Indeed, the two
triples from a given collection.
tasks essentially have a similar goal in terms of
the ranking-based perspective. We thus formulate
them as the task of ranking instances (in seed se-
lection) or triples (in noise reduction), given a set
of (possibly noisy) triples. In the seed selection
task, we use the k highest ranked instances as the
seeds for bootstrapping RE. Likewise, in noise re-
duction for DS, we only use the k highest ranked
triples from the DS-generated data to train a clas-
siﬁer. Note that the value of k in noise reduction
may be much larger than in seed selection.

4 Approaches to Automatic Seed
Selection and Noise Reduction

In this section, we propose several methods that
can be applied for both automatic seed selec-
tion and noise reduction tasks, inspired by rank-
ing relation instances and patterns computed by
the HITS algorithm, and picking cluster cen-
troids using the K-means, latent semantic anal-
ysis (LSA), or non-negative matrix factorization
(NMF) method.

4.1 K-means-based Approach

The ﬁrst method we describe is a K-means-based
It is described as follows: (1) De-
approach.

2 To be precise, in each triple (e1, s, e2) generated by DS,
s is not a pattern but a sentence that contains entities e1 and
e2. However, we can easily convert each instance-sentence
triple (e1, s, e2) to an instance-pattern triple (e1, p, e2) by
looking for a pattern p that connects two entities in sentence
s.

each instance/pattern as a node in the graph. This
representation is similar to that used by Kiso et al.
(2011). In the second graph representation, pat-
terns and instances are treated as nodes and edges,
respectively. Similarly, instances and patterns are
treated as nodes and edges, respectively in the last
representation. (3) For the ﬁrst and third types,
we simply retain the top-k instances with the high-
est hubness scores as the outputs (we sort the in-
stances in descending order based on their hub-
ness scores). For the second type, k instances as-
sociated with the highest scoring patterns are cho-
sen (we ﬁrst sort the patterns in descending order
based on their hubness scores).

4.3 HITS- and K-means-based Approach

In the combined method of HITS and K-means
algorithms, we ﬁrst rank the instances and pat-
terns based on their bipartite graph and then run
K-means to cluster instances in our annotated
dataset. However, instead of choosing the instance
nearest to the centroid, we retain the one that has
the highest HITS hubness score in each cluster.

4.4 LSA-based Approach

Latent semantic analysis (LSA) (Deerwester et al.,
1990) is also a widely used method for the au-
tomatic clustering of data along multiple dimen-
sions. Singular value decomposition (SVD) is
used to construct a low-rank approximation of
the instance-pattern co-occurrence matrix A. The
SVD projection is performed by decomposing the
matrix A ∈ RM ×N into the product of three
matrices, namely an SVD instance matrix I ∈
RM ×K, a diagonal matrix of singular values S ∈
RK×K, and an SVD pattern matrix P ∈ RK×N :

A ≈ ISPT

Our LSA-based seed selection strategy is as
(1) Specify the desired number k of
follows:
triples. (2) Use the LSA algorithm to decompose
the instance-pattern co-occurrence matrix A into
three matrices I, S, and P. We set the number of
LSA dimensions to K = k. (3) We can consider
LSA as a form of soft clustering, with each column
of the SVD instance matrix I corresponding to a
cluster. Then, we select the k instances that have
the highest absolute values from each column of I.

4.5 NMF-based Approach

Non-negative matrix factorization (NMF) (Paatero
and Tapper, 1994; Lee and Seung, 1999) is an-

Figure 1: Graph representations of instances and
patterns using the HITS algorithm.

termine the number k of instances/triples that
should be selected3. (2) Run the K-means clus-
tering algorithm to partition all instances in the in-
put triples (see Section 3) into k clusters. Each
data point is represented by the embedding vec-
tor difference between its entities; e.g., the in-
stance I = (Barack Obama, Honolulu) corre-
sponds to: vec(I) = vec(“Barack Obama”) −
vec(“Honolulu”). We use pre-trained vectors
published by Mikolov et al. (2013). (3) The in-
stance closest to the centroid is selected in each
cluster. Given that the number of clusters is k, the
same number of instances/triples will be chosen.

4.2 HITS-based Approach

Hypertext-induced topic search (HITS) (Klein-
the hubs-and-
also known as
berg, 1999),
authorities algorithm, is a link analysis method
for ranking web pages. In HITS, a good hub is a
page that points to many good authorities and vice
versa; a good authority is a page that is pointed to
by many good hubs. These hubs and authorities
form a bipartite graph, where we can compute the
hubness score of each node.

In our task, let A be the instance-pattern co-
occurrence matrix. We can compute the hubness
score for each instance on the bipartite graph of
instances and patterns induced by the matrix A.
Inspired by the way HITS ranks hubs and authori-
ties, our HITS-based seed selection strategy can be
explained as follows: (1) Determine the number k
of triples that should be selected.(2) Build the bi-
partite graph of instances and patterns based on the
instance-pattern co-occurrence matrix A. Figure 1
presents three possible ways of building a bipar-
tite graph. For the ﬁrst type of graph, we consider

3 Depending on the task, instances or triples will be se-
lected: instances for the automatic seed selection task, and
triples for the noise reduction task. As instances are pairs
of entities which are included in triples, we can simply con-
vert between the instance and the triple, and apply a proposed
method to both tasks.

Subtype
Component-Of
Member-Of
Portion-Of
Stuff-Of
Located-In
Contained-In
Phase-Of
Participates-In
TOTAL

Freq
643 (11.23%)
1,272 (22.21%)
555 ( 9.69%)
1,082 (18.89%)
534 ( 9.32%)
272 ( 4.75%)
497 ( 8.68%)
872 (15.23%)
5,727 triples

Table 1: Statistics of our part-whole dataset.

other method for approximate non-negative ma-
trix factorization. The non-negative data matrix
A ∈ RM ×N is represented by two non-negative
factors W ∈ RM ×K and H ∈ RK×N , which,
when multiplied, approximately reconstruct A:

A ≈ WH

The non-negativity constraint is the main differ-
ence between NMF and LSA. Similarly to the
LSA-based method, we set the NMF parameter K
to k, the desired number of instances to select. We
then select the k instances that have the highest
values from each column of W.

5 Experiments

5.1 Datasets and Settings

We provide an annotated dataset of part-whole re-
lations as a reliable resource for selecting seeds.
Our dataset was collected from Wikipedia and
ClueWeb, and annotated by two annotators. One
of its special characteristics is that the part-whole
relation is a collection of relations, not a single re-
lation (Iris, 1989; Winston et al., 1987).

Table 1 gives the frequencies of each sub-
type of part-whole relations. There are 5,727 in-
stances of 8 subtypes that were annotated with
the same labels by both annotators. We use
Espresso+Word2vec (Phi and Matsumoto, 2016),
which is an improved version for the origi-
nal Espresso algorithm (Pantel and Pennacchiotti,
Espresso+Word2vec outperformed the
2006).
Espresso system for harvesting part-whole rela-
tions by utilizing the Similarity Ranker, which
uses the embedded vector difference between in-
stance pairs of relations. The performance is mea-
sured with Precision@N (Manning et al., 2008),
N = 50. In total, 5,000 instances are checked by

Method
K-means
HITS Graph1
HITS Graph2
HITS Graph3
HITS+K-means Graph1
HITS+K-means Graph2
HITS+K-means Graph3
LSA
NMF
Random

Average P@50
0.96
0.90
0.85
0.90
0.92
0.85
0.94
0.90
0.89
0.75

Table 2: Performance of seed selection methods.

annotators to ascertain whether they express part-
whole relations. We vary the number k of seeds
between 5 and 50 with a step of 5 to report the
average P@50 of each seed selection method.

For the noise reduction task, we use the training
and testing set developed by (Riedel et al., 2010),
which contains 53 relation classes. This dataset
was generated by aligning Freebase relations with
the New York Times corpus. After removing noisy
triples from the dataset using the proposed meth-
ods, we use the ﬁltered data to train two kinds of
convolutional neural networks (CNN) (the CNN
model in (Zeng et al., 2014) and the PCNN model
in (Zeng et al., 2015)) with at-least-one multi-
instance learning (ONE) used in (Zeng et al.,
2015), and the sentence-level attention (ATT) used
in (Lin et al., 2016). Finally, we report the area
under the precision-recall (AUCPR) of each noise
reduction method.

5.2 Performance on Automatic Seed

Selection Task

The performances of the seed selection methods
are presented in Table 2. For the HITS-based
and HITS+K-means-based methods, we display
the P@50 with three types of graph representation
as shown in Section 4.2. We use random seed se-
lection as the baseline for comparison. As Table 2
shows, the random method achieved a precision
of 0.75. The relation extraction system that uses
the random method has the worst average P@50
among all seed selection strategies. The HITS-
based method’s P@50s when using Graph1 and
Graph3 are conﬁrmed to be better than when us-
ing Graph2. This indicates that relying on reli-
able instances is better than reasoning over pat-
terns (recall that for the Graph2, we ﬁrst choose

System
CNN+ONE
CNN+ATT
PCNN+ONE
PCNN+ATT

Original +HITS +LSA +NMF +Ensemble

0.180
0.234
0.231
0.248

0.183
0.235
0.234
0.253

0.173
0.235
0.233
0.250

0.178
0.233
0.234
0.252

0.181
0.236
0.235
0.255

Table 3: Performance (AUCPR) of each noise reduction method; in bold are the best scores.

the patterns, then select the instances associated
with those patterns), as there is a possibility that a
pattern can be ambiguous, and therefore, instances
linked to that pattern can be incorrect. The K-
means-based seed selection method provides the
best average P@50 with a performance of 0.96.
The HITS+K-means-based method performs bet-
ter than using only the HITS strategy, while the
LSA-based and NMF-based methods have a com-
parable performance.

5.3 Performance on Noise Reduction Task

similar:

Table 3 presents the performance of noise re-
duction methods. Recall that the K-means-based
the seed
method achieves a high P@50 for
selection method. Our assumption is that each
cluster may represent a set in which elements
have similar semantic properties.
However,
we observed that as the number of relations
is relatively high and there is no distinct def-
inition between some relations in the distantly
labeled data (e.g., the following three relations
/location/country/capital,
are quite
/location/province/capital,
and
/location/us state/capital, we decided not
to
perform the K-means-based method for our
noise reduction task. The performances of the
HITS-based, LSA-based, and NMF-based noise
reduction methods are presented in Table 3. We
experimentally set the portion of retained data
from the distantly labeled data to 90%, given
that the performance can be affected if too many
sentences are removed from the original data.
We also perform experiments with an ensem-
ble method that combines the HITS-based and
LSA-based strategies to merge rankings from
their outputs, with half of the triples coming from
the LSA-based method and the other half from
the HITS-based method. Table 3 indicates that
our proposed methods improved the performance
of all CNN and PCNN models. Our ensemble
method achieved the best improvements for three
out of four systems, except that the HITS-based

method obtained the best score for CNN+ONE.

6 Conclusion

We formulated the seed selection and noise re-
duction tasks as ranking problems.
In addition,
we proposed several methods, inspired by rank-
ing instances and patterns computed by the HITS
algorithm, and selecting clusters centroids using
the K-means, LSA, or NMF method. Experi-
ments demonstrated that our proposed methods
improved the baselines in both tasks.

Acknowledgments

This work was partly supported by JST CREST
Grant Number JPMJCR1513 and MEXT/JSPS
Kakenhi Grant Number JP15H02749. We are
grateful to the members of the Computational Lin-
guistics Laboratory, NAIST and the anonymous
reviewers for their insightful comments.

References

Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM Conference
on Digital Libraries, pages 85–94. ACM.

Sergey Brin. 1998. Extracting patterns and relations
In International Work-
from the world wide web.
shop on the World Wide Web and Databases, pages
172–183. Springer.

Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391.

Jason Eisner and Damianos Karakos. 2005. Bootstrap-
ping without the boot. In Proceedings of the 2005
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on

Empirical Methods in Natural Language Process-
ing, pages 1535–1545. Association for Computa-
tional Linguistics.

Ander Intxaurrondo, Mihai Surdeanu, Oier Lopez de
Lacalle, and Eneko Agirre. 2013. Removing noisy
mentions for distant supervision. Procesamiento del
lenguaje natural, 51.

Madelyn Anne Iris. 1989. Problems of the part-whole
relation. In Relational Models of the Lexicon, pages
261–288. Cambridge University Press.

Tetsuo Kiso, Masashi Shimbo, Mamoru Komachi, and
Yuji Matsumoto. 2011. HITS-based seed selection
and stop list construction for bootstrapping. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 30–36. Association for
Computational Linguistics.

Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604–632.

Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of
semantic drift in Espresso-like bootstrapping algo-
rithms. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1011–1020. Association for Computa-
tional Linguistics.

Zornitsa Kozareva and Eduard Hovy. 2010. Not all
seeds are equal: Measuring the quality of text min-
ing seeds. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 618–626. Association for Computa-
tional Linguistics.

Daniel D Lee and H. Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factor-
ization. Nature, 401(6755):788–791.

Gang Li, Cathy Wu, and K. Vijay-Shanker. 2017.
Noise reduction methods for distantly supervised
In SIGBioMed
biomedical relation extraction.
Workshop on Biomedical Natural Language Pro-
cessing (BioNLP ’17), pages 184–193. Association
for Computational Linguistics.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2124–2133. Association for Computa-
tional Linguistics.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003–1011. Association for Computational Linguis-
tics.

Dana Movshovitz-Attias and William W. Cohen. 2012.
Bootstrapping biomedical ontologies for scientiﬁc
In BioNLP: Proceedings of the
text using nell.
2012 Workshop on Biomedical Natural Language
Processing, pages 11–19. Association for Compu-
tational Linguistics.

Pentti Paatero and Unto Tapper. 1994. Positive ma-
trix factorization: A non-negative factor model with
optimal utilization of error estimates of data values.
Environmetrics, 5(2):111–126.

Patrick Pantel

and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics,
pages 113–120. Association for Computational
Linguistics.

Van-Thuy Phi and Yuji Matsumoto. 2016. Integrating
word embedding offsets into the Espresso system for
part-whole relation extraction. In Proceedings of the
30th Paciﬁc Asia Conference on Language, Informa-
tion and Computation: Oral Papers, pages 173–181.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
In Proceedings of the 2010 Joint
out labeled text.
European Conference on Machine Learning and
Principles of Knowledge Discovery in Databases
(ECML PKDD), pages 148–163. Springer.

Ellen Riloff, Rosie Jones, et al. 1999. Learning dic-
tionaries for information extraction by multi-level
In Proceedings of the 16th Na-
bootstrapping.
tional Conference on Artiﬁcial Intelligence and the
11th Innovative Applications of Artiﬁcial Intelli-
gence Conference (AAAI/IAAI), pages 474–479.

Morton E. Winston, Roger Chafﬁn, and Douglas Her-
rmann. 1987. A taxonomy of part-whole relations.
Cognitive Science, 11(4):417–444.

Yang Xiang, Qingcai Chen, Xiaolong Wang, and Yang
Qin. 2016. Distant supervision for relation ex-
Entropy,
traction with ranking-based methods.
18(6):204.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
In Pro-
piecewise convolutional neural networks.
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1753–
1762. Association for Computational Linguistics.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classiﬁcation via con-
volutional deep neural network. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335–2344. Dublin City University and As-
sociation for Computational Linguistics.

Ranking-Based Automatic Seed Selection and Noise Reduction for Weakly
Supervised Relation Extraction

Van-Thuy Phi1, Joan Santoso2, Masashi Shimbo1 and Yuji Matsumoto1,3

1 Nara Institute of Science and Technology
2Sekolah Tinggi Teknik Surabaya
3RIKEN Center for Advanced Intelligence Project (AIP)
1{phi.thuy.ph8, shimbo, matsu}@is.naist.jp
2joan@stts.edu

Abstract

This paper addresses the tasks of auto-
matic seed selection for bootstrapping re-
lation extraction, and noise reduction for
distantly supervised relation extraction.
We ﬁrst point out that these tasks are re-
lated. Then, inspired by ranking relation
instances and patterns computed by the
HITS algorithm, and selecting cluster cen-
troids using the K-means, LSA, or NMF
method, we propose methods for selecting
the initial seeds from an existing resource,
or reducing the level of noise in the dis-
tantly labeled data. Experiments show that
our proposed methods achieve a better per-
formance than the baseline systems in both
tasks.

1

Introduction

Bootstrapping for relation extraction (RE) (Brin,
1998; Riloff et al., 1999; Agichtein and Gravano,
2000) is a class of minimally supervised meth-
ods frequently used in machine learning: initial-
ized by a small set of example instances called
seeds, to represent a particular semantic relation,
the bootstrapping system operates iteratively to ac-
quire new instances of a target relation. Selecting
“good” seeds is one of the most important steps
to reduce semantic drift, which is a typical phe-
nomenon of the bootstrapping process.

Another approach, called “distant supervision”
(DS) (Mintz et al., 2009), does not require any la-
bels on the text. The assumption of DS is that if
two entities participate in a known Freebase rela-
tion, any sentence that contains those two entities
might express that relation. However, this tech-
nique often introduces noise to the generated train-
ing data. As a result, DS is still limited by the

quality of training data, and noise existing in pos-
itively labeled data may affect the performance of
supervised learning.

In this study, we propose methods that can
be applied for both automatic seed selection and
noise reduction by formulating these tasks as rank-
ing problems according to different ranking crite-
ria. Our methods are inspired by ranking instances
and patterns computed by the HITS algorithm, and
selecting cluster centroids using K-means, latent
semantic analysis, or the non-negative matrix fac-
torization method. The main contributions of this
paper are (a) an annotated dataset of 5,727 part-
whole relations1, which contains 8 subtypes for
the bootstrapping RE system; (b) methods for au-
tomatic seed selection for bootstrapping RE and
noise reduction for distant supervised RE; and (c)
experimental results showing that the proposed
models outperform baselines on two datasets.

2 Related Work

2.1 Automatic Seed Selection for

Bootstrapping RE

As manually selecting the seeds requires tremen-
dous effort, some research proposed methods to
select the seed automatically. Eisner and Karakos
(2005) used a “strapping” approach to evaluate
many candidate seeds automatically for a word
sense disambiguation task. Kozareva and Hovy
(2010) proposed a method for measuring seed
quality using a regression model and applied it to
the extraction of unary semantic relations, such
as“people” and “city”. Kiso et al. (2011) sug-
gested a HITS-based approach to ranking the
seeds, based on Komachi et al. (2008)’s analysis of
the Espresso algorithm (Pantel and Pennacchiotti,

1We

release

our

annotated

dataset

at

https://github.com/pvthuy/part-whole-relations.

2006). Movshovitz-Attias and Cohen (2012) gen-
erated a ranking based on pointwise mutual infor-
mation (PMI) to pick up the seeds from existing
resources in the biomedical domain. Given the
seed set of a target relation, the goal of the boot-
strapping method is to ﬁnd instances similar to
initial seeds by harvesting instances and patterns
iteratively over large corpora, e.g., Wikipedia or
ClueWeb.

2.2 Noise Reduction for Distantly Supervised

RE

The DS assumption is too strong and leads to
wrongly labeled data that affects performance.
Many studies focused on methods of noise re-
duction in DS. Intxaurrondo et al. (2013) ﬁltered
out noisy mentions from the distantly supervised
dataset using their frequencies, PMI, or the sim-
ilarity between the centroids of all relation men-
tions and each individual mention. Xiang et al.
(2016) introduced ranking-based methods accord-
ing to different strategies to select effective train-
ing groups. Li et al. (2017) proposed three novel
heuristics that use lexical and syntactic informa-
tion to remove noise in the biomedical domain.
The data generated by the noise reduction process
can be used by supervised learning algorithms to
train models.

3 Problem Formulation

Let R∗ be the set of target relations. The goal is
to ﬁnd instances, or pairs of entities, upon which
the relation holds. For each target relation r ∈
R∗, we assume there is a set Dr of triples rep-
resenting the relation r. The triples in Dr have
the form (e1, p, e2), where e1 and e2 denote en-
tities, and p denotes the pattern that connects the
two entities. A pair of entities (e1, e2) is called
an instance. This terminology is similar to the
one used in open information extraction systems,
such as Reverb (Fader et al., 2011). For example,
in triple (Barack Obama, was born in, Honolulu),
(Barack Obama, Honolulu) is the instance, and
“was born in” is the pattern.

The two tasks we address are deﬁned as follows:

Seed Selection for Bootstrapping RE:
In au-
tomatic seed selection, a set R∗ of target rela-
tions and sets of instance-pattern triples Dr =
{(e1, p, e2)} representing each target relation r ∈
R∗ are given as input. These triples are extracted
from existing corpus or database, e.g., WordNet.

With these inputs, the task is to choose good seeds
from the instances appearing in Dr for each r ∈
R∗, such that they work effectively in bootstrap-
ping RE.

Noise Reduction for Distantly Supervised RE:
In noise reduction for distantly supervised RE, the
input is the target relations R∗ and the sets Dr of
triples2 generated automatically by DS for each
relation r ∈ R∗. Because the data is generated
automatically by DS, Dr may contain noise, i.e.,
triples (e1, p, e2) for which relation r does not ac-
tually hold between e1 and e2. The goal of noise
reduction is to ﬁlter out these noisy triples, so that
they do not deteriorate the quality of the triple
classiﬁer trained subsequently.

Formulation as Ranking Tasks: As we can see
from the task deﬁnitions above, both seed selec-
tion and noise reduction are the task of selecting
Indeed, the two
triples from a given collection.
tasks essentially have a similar goal in terms of
the ranking-based perspective. We thus formulate
them as the task of ranking instances (in seed se-
lection) or triples (in noise reduction), given a set
of (possibly noisy) triples. In the seed selection
task, we use the k highest ranked instances as the
seeds for bootstrapping RE. Likewise, in noise re-
duction for DS, we only use the k highest ranked
triples from the DS-generated data to train a clas-
siﬁer. Note that the value of k in noise reduction
may be much larger than in seed selection.

4 Approaches to Automatic Seed
Selection and Noise Reduction

In this section, we propose several methods that
can be applied for both automatic seed selec-
tion and noise reduction tasks, inspired by rank-
ing relation instances and patterns computed by
the HITS algorithm, and picking cluster cen-
troids using the K-means, latent semantic anal-
ysis (LSA), or non-negative matrix factorization
(NMF) method.

4.1 K-means-based Approach

The ﬁrst method we describe is a K-means-based
It is described as follows: (1) De-
approach.

2 To be precise, in each triple (e1, s, e2) generated by DS,
s is not a pattern but a sentence that contains entities e1 and
e2. However, we can easily convert each instance-sentence
triple (e1, s, e2) to an instance-pattern triple (e1, p, e2) by
looking for a pattern p that connects two entities in sentence
s.

each instance/pattern as a node in the graph. This
representation is similar to that used by Kiso et al.
(2011). In the second graph representation, pat-
terns and instances are treated as nodes and edges,
respectively. Similarly, instances and patterns are
treated as nodes and edges, respectively in the last
representation. (3) For the ﬁrst and third types,
we simply retain the top-k instances with the high-
est hubness scores as the outputs (we sort the in-
stances in descending order based on their hub-
ness scores). For the second type, k instances as-
sociated with the highest scoring patterns are cho-
sen (we ﬁrst sort the patterns in descending order
based on their hubness scores).

4.3 HITS- and K-means-based Approach

In the combined method of HITS and K-means
algorithms, we ﬁrst rank the instances and pat-
terns based on their bipartite graph and then run
K-means to cluster instances in our annotated
dataset. However, instead of choosing the instance
nearest to the centroid, we retain the one that has
the highest HITS hubness score in each cluster.

4.4 LSA-based Approach

Latent semantic analysis (LSA) (Deerwester et al.,
1990) is also a widely used method for the au-
tomatic clustering of data along multiple dimen-
sions. Singular value decomposition (SVD) is
used to construct a low-rank approximation of
the instance-pattern co-occurrence matrix A. The
SVD projection is performed by decomposing the
matrix A ∈ RM ×N into the product of three
matrices, namely an SVD instance matrix I ∈
RM ×K, a diagonal matrix of singular values S ∈
RK×K, and an SVD pattern matrix P ∈ RK×N :

A ≈ ISPT

Our LSA-based seed selection strategy is as
(1) Specify the desired number k of
follows:
triples. (2) Use the LSA algorithm to decompose
the instance-pattern co-occurrence matrix A into
three matrices I, S, and P. We set the number of
LSA dimensions to K = k. (3) We can consider
LSA as a form of soft clustering, with each column
of the SVD instance matrix I corresponding to a
cluster. Then, we select the k instances that have
the highest absolute values from each column of I.

4.5 NMF-based Approach

Non-negative matrix factorization (NMF) (Paatero
and Tapper, 1994; Lee and Seung, 1999) is an-

Figure 1: Graph representations of instances and
patterns using the HITS algorithm.

termine the number k of instances/triples that
should be selected3. (2) Run the K-means clus-
tering algorithm to partition all instances in the in-
put triples (see Section 3) into k clusters. Each
data point is represented by the embedding vec-
tor difference between its entities; e.g., the in-
stance I = (Barack Obama, Honolulu) corre-
sponds to: vec(I) = vec(“Barack Obama”) −
vec(“Honolulu”). We use pre-trained vectors
published by Mikolov et al. (2013). (3) The in-
stance closest to the centroid is selected in each
cluster. Given that the number of clusters is k, the
same number of instances/triples will be chosen.

4.2 HITS-based Approach

Hypertext-induced topic search (HITS) (Klein-
the hubs-and-
also known as
berg, 1999),
authorities algorithm, is a link analysis method
for ranking web pages. In HITS, a good hub is a
page that points to many good authorities and vice
versa; a good authority is a page that is pointed to
by many good hubs. These hubs and authorities
form a bipartite graph, where we can compute the
hubness score of each node.

In our task, let A be the instance-pattern co-
occurrence matrix. We can compute the hubness
score for each instance on the bipartite graph of
instances and patterns induced by the matrix A.
Inspired by the way HITS ranks hubs and authori-
ties, our HITS-based seed selection strategy can be
explained as follows: (1) Determine the number k
of triples that should be selected.(2) Build the bi-
partite graph of instances and patterns based on the
instance-pattern co-occurrence matrix A. Figure 1
presents three possible ways of building a bipar-
tite graph. For the ﬁrst type of graph, we consider

3 Depending on the task, instances or triples will be se-
lected: instances for the automatic seed selection task, and
triples for the noise reduction task. As instances are pairs
of entities which are included in triples, we can simply con-
vert between the instance and the triple, and apply a proposed
method to both tasks.

Subtype
Component-Of
Member-Of
Portion-Of
Stuff-Of
Located-In
Contained-In
Phase-Of
Participates-In
TOTAL

Freq
643 (11.23%)
1,272 (22.21%)
555 ( 9.69%)
1,082 (18.89%)
534 ( 9.32%)
272 ( 4.75%)
497 ( 8.68%)
872 (15.23%)
5,727 triples

Table 1: Statistics of our part-whole dataset.

other method for approximate non-negative ma-
trix factorization. The non-negative data matrix
A ∈ RM ×N is represented by two non-negative
factors W ∈ RM ×K and H ∈ RK×N , which,
when multiplied, approximately reconstruct A:

A ≈ WH

The non-negativity constraint is the main differ-
ence between NMF and LSA. Similarly to the
LSA-based method, we set the NMF parameter K
to k, the desired number of instances to select. We
then select the k instances that have the highest
values from each column of W.

5 Experiments

5.1 Datasets and Settings

We provide an annotated dataset of part-whole re-
lations as a reliable resource for selecting seeds.
Our dataset was collected from Wikipedia and
ClueWeb, and annotated by two annotators. One
of its special characteristics is that the part-whole
relation is a collection of relations, not a single re-
lation (Iris, 1989; Winston et al., 1987).

Table 1 gives the frequencies of each sub-
type of part-whole relations. There are 5,727 in-
stances of 8 subtypes that were annotated with
the same labels by both annotators. We use
Espresso+Word2vec (Phi and Matsumoto, 2016),
which is an improved version for the origi-
nal Espresso algorithm (Pantel and Pennacchiotti,
Espresso+Word2vec outperformed the
2006).
Espresso system for harvesting part-whole rela-
tions by utilizing the Similarity Ranker, which
uses the embedded vector difference between in-
stance pairs of relations. The performance is mea-
sured with Precision@N (Manning et al., 2008),
N = 50. In total, 5,000 instances are checked by

Method
K-means
HITS Graph1
HITS Graph2
HITS Graph3
HITS+K-means Graph1
HITS+K-means Graph2
HITS+K-means Graph3
LSA
NMF
Random

Average P@50
0.96
0.90
0.85
0.90
0.92
0.85
0.94
0.90
0.89
0.75

Table 2: Performance of seed selection methods.

annotators to ascertain whether they express part-
whole relations. We vary the number k of seeds
between 5 and 50 with a step of 5 to report the
average P@50 of each seed selection method.

For the noise reduction task, we use the training
and testing set developed by (Riedel et al., 2010),
which contains 53 relation classes. This dataset
was generated by aligning Freebase relations with
the New York Times corpus. After removing noisy
triples from the dataset using the proposed meth-
ods, we use the ﬁltered data to train two kinds of
convolutional neural networks (CNN) (the CNN
model in (Zeng et al., 2014) and the PCNN model
in (Zeng et al., 2015)) with at-least-one multi-
instance learning (ONE) used in (Zeng et al.,
2015), and the sentence-level attention (ATT) used
in (Lin et al., 2016). Finally, we report the area
under the precision-recall (AUCPR) of each noise
reduction method.

5.2 Performance on Automatic Seed

Selection Task

The performances of the seed selection methods
are presented in Table 2. For the HITS-based
and HITS+K-means-based methods, we display
the P@50 with three types of graph representation
as shown in Section 4.2. We use random seed se-
lection as the baseline for comparison. As Table 2
shows, the random method achieved a precision
of 0.75. The relation extraction system that uses
the random method has the worst average P@50
among all seed selection strategies. The HITS-
based method’s P@50s when using Graph1 and
Graph3 are conﬁrmed to be better than when us-
ing Graph2. This indicates that relying on reli-
able instances is better than reasoning over pat-
terns (recall that for the Graph2, we ﬁrst choose

System
CNN+ONE
CNN+ATT
PCNN+ONE
PCNN+ATT

Original +HITS +LSA +NMF +Ensemble

0.180
0.234
0.231
0.248

0.183
0.235
0.234
0.253

0.173
0.235
0.233
0.250

0.178
0.233
0.234
0.252

0.181
0.236
0.235
0.255

Table 3: Performance (AUCPR) of each noise reduction method; in bold are the best scores.

the patterns, then select the instances associated
with those patterns), as there is a possibility that a
pattern can be ambiguous, and therefore, instances
linked to that pattern can be incorrect. The K-
means-based seed selection method provides the
best average P@50 with a performance of 0.96.
The HITS+K-means-based method performs bet-
ter than using only the HITS strategy, while the
LSA-based and NMF-based methods have a com-
parable performance.

5.3 Performance on Noise Reduction Task

similar:

Table 3 presents the performance of noise re-
duction methods. Recall that the K-means-based
the seed
method achieves a high P@50 for
selection method. Our assumption is that each
cluster may represent a set in which elements
have similar semantic properties.
However,
we observed that as the number of relations
is relatively high and there is no distinct def-
inition between some relations in the distantly
labeled data (e.g., the following three relations
/location/country/capital,
are quite
/location/province/capital,
and
/location/us state/capital, we decided not
to
perform the K-means-based method for our
noise reduction task. The performances of the
HITS-based, LSA-based, and NMF-based noise
reduction methods are presented in Table 3. We
experimentally set the portion of retained data
from the distantly labeled data to 90%, given
that the performance can be affected if too many
sentences are removed from the original data.
We also perform experiments with an ensem-
ble method that combines the HITS-based and
LSA-based strategies to merge rankings from
their outputs, with half of the triples coming from
the LSA-based method and the other half from
the HITS-based method. Table 3 indicates that
our proposed methods improved the performance
of all CNN and PCNN models. Our ensemble
method achieved the best improvements for three
out of four systems, except that the HITS-based

method obtained the best score for CNN+ONE.

6 Conclusion

We formulated the seed selection and noise re-
duction tasks as ranking problems.
In addition,
we proposed several methods, inspired by rank-
ing instances and patterns computed by the HITS
algorithm, and selecting clusters centroids using
the K-means, LSA, or NMF method. Experi-
ments demonstrated that our proposed methods
improved the baselines in both tasks.

Acknowledgments

This work was partly supported by JST CREST
Grant Number JPMJCR1513 and MEXT/JSPS
Kakenhi Grant Number JP15H02749. We are
grateful to the members of the Computational Lin-
guistics Laboratory, NAIST and the anonymous
reviewers for their insightful comments.

References

Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM Conference
on Digital Libraries, pages 85–94. ACM.

Sergey Brin. 1998. Extracting patterns and relations
In International Work-
from the world wide web.
shop on the World Wide Web and Databases, pages
172–183. Springer.

Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391.

Jason Eisner and Damianos Karakos. 2005. Bootstrap-
ping without the boot. In Proceedings of the 2005
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on

Empirical Methods in Natural Language Process-
ing, pages 1535–1545. Association for Computa-
tional Linguistics.

Ander Intxaurrondo, Mihai Surdeanu, Oier Lopez de
Lacalle, and Eneko Agirre. 2013. Removing noisy
mentions for distant supervision. Procesamiento del
lenguaje natural, 51.

Madelyn Anne Iris. 1989. Problems of the part-whole
relation. In Relational Models of the Lexicon, pages
261–288. Cambridge University Press.

Tetsuo Kiso, Masashi Shimbo, Mamoru Komachi, and
Yuji Matsumoto. 2011. HITS-based seed selection
and stop list construction for bootstrapping. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 30–36. Association for
Computational Linguistics.

Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604–632.

Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of
semantic drift in Espresso-like bootstrapping algo-
rithms. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1011–1020. Association for Computa-
tional Linguistics.

Zornitsa Kozareva and Eduard Hovy. 2010. Not all
seeds are equal: Measuring the quality of text min-
ing seeds. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 618–626. Association for Computa-
tional Linguistics.

Daniel D Lee and H. Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factor-
ization. Nature, 401(6755):788–791.

Gang Li, Cathy Wu, and K. Vijay-Shanker. 2017.
Noise reduction methods for distantly supervised
In SIGBioMed
biomedical relation extraction.
Workshop on Biomedical Natural Language Pro-
cessing (BioNLP ’17), pages 184–193. Association
for Computational Linguistics.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2124–2133. Association for Computa-
tional Linguistics.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003–1011. Association for Computational Linguis-
tics.

Dana Movshovitz-Attias and William W. Cohen. 2012.
Bootstrapping biomedical ontologies for scientiﬁc
In BioNLP: Proceedings of the
text using nell.
2012 Workshop on Biomedical Natural Language
Processing, pages 11–19. Association for Compu-
tational Linguistics.

Pentti Paatero and Unto Tapper. 1994. Positive ma-
trix factorization: A non-negative factor model with
optimal utilization of error estimates of data values.
Environmetrics, 5(2):111–126.

Patrick Pantel

and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics,
pages 113–120. Association for Computational
Linguistics.

Van-Thuy Phi and Yuji Matsumoto. 2016. Integrating
word embedding offsets into the Espresso system for
part-whole relation extraction. In Proceedings of the
30th Paciﬁc Asia Conference on Language, Informa-
tion and Computation: Oral Papers, pages 173–181.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
In Proceedings of the 2010 Joint
out labeled text.
European Conference on Machine Learning and
Principles of Knowledge Discovery in Databases
(ECML PKDD), pages 148–163. Springer.

Ellen Riloff, Rosie Jones, et al. 1999. Learning dic-
tionaries for information extraction by multi-level
In Proceedings of the 16th Na-
bootstrapping.
tional Conference on Artiﬁcial Intelligence and the
11th Innovative Applications of Artiﬁcial Intelli-
gence Conference (AAAI/IAAI), pages 474–479.

Morton E. Winston, Roger Chafﬁn, and Douglas Her-
rmann. 1987. A taxonomy of part-whole relations.
Cognitive Science, 11(4):417–444.

Yang Xiang, Qingcai Chen, Xiaolong Wang, and Yang
Qin. 2016. Distant supervision for relation ex-
Entropy,
traction with ranking-based methods.
18(6):204.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
In Pro-
piecewise convolutional neural networks.
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1753–
1762. Association for Computational Linguistics.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classiﬁcation via con-
volutional deep neural network. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335–2344. Dublin City University and As-
sociation for Computational Linguistics.

