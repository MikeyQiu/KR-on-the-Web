Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs

Zheng Shou, Dongang Wang, and Shih-Fu Chang
Columbia University
New York, NY, USA
{zs2262,dw2648,sc250}@columbia.edu

6
1
0
2
 
r
p
A
 
1
2
 
 
]

V
C
.
s
c
[
 
 
2
v
9
2
1
2
0
.
1
0
6
1
:
v
i
X
r
a

Abstract

We address temporal action localization in untrimmed
long videos. This is important because videos in real ap-
plications are usually unconstrained and contain multiple
action instances plus video content of background scenes
or other activities. To address this challenging issue, we
exploit the effectiveness of deep networks in temporal ac-
tion localization via three segment-based 3D ConvNets: (1)
a proposal network identiﬁes candidate segments in a long
video that may contain actions; (2) a classiﬁcation network
learns one-vs-all action classiﬁcation model to serve as ini-
tialization for the localization network; and (3) a localiza-
tion network ﬁne-tunes the learned classiﬁcation network to
localize each action instance. We propose a novel loss func-
tion for the localization network to explicitly consider tem-
poral overlap and achieve high temporal localization accu-
racy. In the end, only the proposal network and the local-
ization network are used during prediction. On two large-
scale benchmarks, our approach achieves signiﬁcantly su-
perior performances compared with other state-of-the-art
systems: mAP increases from 1.7% to 7.4% on MEXaction2
and increases from 15.0% to 19.0% on THUMOS 2014.

1. Introduction

Impressive progress has been reported in recent litera-
ture for action recognition [42, 28, 2, 3, 39, 40, 24, 18, 31,
44, 13, 37]. Besides detecting action in manually trimmed
short video, researchers start to develop techniques for de-
tecting actions in untrimmed long videos in the wild. This
trend motivates another challenging topic - temporal action
localization: given a long untrimmed video, “when does a
speciﬁc action start and end?” This problem is important
because real applications usually involve long untrimmed
videos, which can be highly unconstrained in space and
time, and one video can contain multiple action instances
plus background scenes or other activities. Localizing ac-
tions in long videos, such as those in surveillance, can save
tremendous time and computational costs.

Most state-of-the-art methods rely on manually selected
features, and their performances still require much improve-
ment. For example, top performing approaches in THU-
MOS Challenge 2014 [27, 41, 17, 15] and 2015 [46, 9] both
used improved Dense Trajectory (iDT) with Fisher Vector
(FV) [40, 25]. There have been some recent attempts at in-
corporating iDT features with appearance features automat-
ically extracted by frame-level deep networks [27, 41, 17].
Nevertheless, such 2D ConvNets do not capture motion in-
formation, which is important for modeling actions and de-
termining their temporal boundaries.

As an analogy in still images, object detection recently
achieved large improvements by using deep networks. In-
spired by Region-based Convolutional Neural Networks (R-
CNN) [7] and its upgraded versions [6, 30, 21], we develop
Segment-CNN1, which is an effective deep network frame-
work for temporal action localization as outlined in Figure
1. We adopt 3D ConvNets [13, 37], which recently has
been shown to be promising for capturing motion charac-
teristics in videos, and add a new multi-stage framework.
First, multi-scale segments are generated as candidates for
three deep networks. The proposal network classiﬁes each
segment as either action or background in order to eliminate
background segment estimated to be unlikely to contain ac-
tions of interest. The classiﬁcation network trains typical
one-vs-all classiﬁcation model for all action categories plus
the background.

However, the classiﬁcation network aims at ﬁnding key
evidences to distinguish different categories, rather than lo-
calizing precise action presences in time. Sometimes, the
scores from the classiﬁcation network can be high even
when the segment has only a very small overlap with the
ground truth instance. This can be detrimental because sub-
sequent post-processing steps, such as Non-Maximum Sup-
pression (NMS), might remove segment of small score but
large overlap with ground truth. To explicitly take tempo-
ral overlap into consideration, we introduce the localiza-
tion network based on the same architecture, but this net-

1Source code and trained models are available online at https://

github.com/zhengshou/scnn/.

Figure 1. Overview of our framework. (a) Multi-scale segment generation: given an untrimmed video, we generate segments of varied
lengths via sliding window; (b) Segment-CNN: the proposal network identiﬁes candidate segments, the classiﬁcation network trains an
action recognition model to serve as initialization for the localization network, and the localization network localizes action instances in
time and outputs conﬁdence scores; (c) Post-processing: using the prediction scores from the localization network, we further remove
redundancy by NMS to obtain the ﬁnal results. During training, the classiﬁcation network is ﬁrst learned and then used as initialization for
the localization network. During prediction, only the proposal and localization networks are used.

work uses a novel loss function, which rewards segments
with higher temporal overlap with the ground truths, and
thus can generate conﬁdence scores more suitable for post-
processing. Note that the classiﬁcation network cannot be
replaced by the localization network. We will show later
that using the trained classiﬁcation network (without con-
sidering temporal overlap) to initialize the localization net-
work (take into account temporal overlap) is important, and
achieves better temporal localization accuracies.

To summarize, our main contributions are three-fold:
(1) To the best of our knowledge, our work is the ﬁrst to
exploit 3D ConvNets with multi-stage processes for tempo-
ral action localization in untrimmed long videos in the wild.
(2) We introduce an effective multi-stage Segment-CNN
framework, to propose candidate segments, recognize ac-
tions, and localize temporal boundaries. The proposal net-
work improves the efﬁciency by eliminating unlikely candi-
date segments, and the localization network is key to tem-
poral localization accuracy boosting.

(3) The proposed techniques signiﬁcantly outperform the
state-of-the-art systems over two large-scale benchmarks
suitable for temporal action localization. When the overlap
threshold used in evaluation is set to 0.5, our approach im-
proves mAP on MEXaction2 from 1.7% to 7.4% and mAP
on THUMOS 2014 from 15.0% to 19.0%. We did not eval-
uate on THUMOS Challenge 2015 [9] because the ground

truth is withheld by organizers for future evaluation. More
detailed evaluation results are available in Section 4.

2. Related work

Temporal action localization. This topic has been studied
in two directions. When training data only have video-level
category labels but no temporal annotations, researchers
formulated this as weakly supervised problems or multi-
ple instance learning problems to learn the key evidences
in untrimmed videos and temporally localize actions by se-
lecting key instances [22, 23]. Sun et al. [36] transferred
knowledge from web images to address temporal localiza-
tion in untrimmed web videos.

Another line of work focuses on learning from data when
the temporal boundaries have been annotated for action in-
stances in untrimmed videos, such as THUMOS. Most of
these works pose this as a classiﬁcation problem and adopt
a temporal sliding window approach, where each window
is considered as an action candidate subject to classiﬁca-
tion [25]. Surveys about action classiﬁcation methods can
be found in [42, 28, 2, 3]. Recently, two directions lead
the state-of-the-art: (1) Wang et al. [39] proposed extract-
ing HOG, HOF, MBH features along dense trajectories, and
later on they took camera motion into consideration [40].
Further improvement can be achieved by stacking features
(2) Enlighted by the suc-
with multiple time skips [24].

cess of CNNs in recent works [20, 32], Karpathy et al. [18]
evaluated frame-level CNNs on large-scale video classiﬁ-
cation tasks. Simonyan and Zisserman [31] designed two-
stream CNNs to learn from still image and motion ﬂow re-
In [44], a latent concept descriptor of convo-
spectively.
lutional feature map was proposed, and great results were
achieved on event detection with VLAD encoding. To learn
spatio-temporal features together, the architecture of 3D
ConvNets was explored in [13, 37], achieving competitive
results. Oneata et al. [26] proposed approximately normal-
ized Fisher Vectors to reduce the high dimensionality of FV.
Stoian et al. [35] introduced a two-level cascade to allow
fast search for action instances. Instead of precision, these
methods focus on improving the efﬁciency of conventional
methods. To speciﬁcally address the temporal precision of
action detection, Gaidon et al.
[4, 5] modeled the struc-
ture of action sequence with atomic action units (actoms).
The explicit modeling of action units allows for matching
more complete action unit sequences, rather than just par-
tial content. However, this requires mannual annotations
for actoms, which can be subjective and burdensome. Our
paper presented here aims to solve the same problem of pre-
cise temporal localization, but without requiring the difﬁcult
task of manual annotation for atomic action units.

[10] and Soomro et al.

Spatio-temporal localization. There have been active ex-
plorations about localizing action in space and time simul-
taneously. Jain et al.
[33] built
their work on supervoxel. Recently, researchers treat this
as a tracking problem [43, 8] by leveraging object detectors
[11], especially human detectors [16, 19, 8, 45] to detect re-
gions of interest in each frame and then output sequences
of bounding boxes. Dense trajectories have also been ex-
ploited for extracting the action tubes [29, 38]. Jain et al.
[12] added object encodings to help action localization.

However, this problem is different from temporal local-
ization, which is the main topic in this paper: (1) When
using object detectors to ﬁnd spatio-temporal regions of
interest, such approaches assume that the actions are per-
formed by human or other pre-deﬁned objects. (2) Spatio-
temporal localization requires exhaustive annotations for
objects of interest on every frame as training data. This
makes it overwhelmingly time-consuming particularly for
long untrimmed videos compared with the task of simply
labeling the start time and end time of an action depicted in
the video, which is sufﬁcient to satisfy many applications.

Object detection. Inspired by the success of deep learning
approaches in object detection, we also review R-CNN and
its variations. R-CNN consists of selective search, CNN
feature extraction, SVM classiﬁcation, and bounding box
regression [7]. Fast R-CNN reshapes R-CNN into a single-
stage using multi-task loss, and also has a RoI pooling layer

to share the computation of one image in ConvNets [6].
Our work differs from R-CNN in the following aspects:
(1) Temporal annotations in training videos can be diverse:
some are cleanly trimmed action instances cut out from
long videos, such as UCF101 [34], and some are untrimmed
long videos but with temporal boundaries annotated for ac-
tion instances, such as THUMOS [15, 9]. We provide a
paradigm that can handle such diverse annotations. (2) As
proven in Faster R-CNN [30] which proposes region pro-
posal network, and DeepBox [21] which detects objectness
to re-rank the results of R-CNN, using deep networks for
learning objectness is effective and efﬁcient. Therefore, we
directly use deep network to classify background and action
to obtain candidate segments. (3) We remove the regression
stage because learning regression for time shift and dura-
tion of video segment does not work well in our experi-
ments, probably because actions can be quite diverse, and
therefore do not contain consistent patterns for predicting
start/end time. To achieve precise localization, we design
the localization network using a new loss function to explic-
itly consider temporal overlap. This can decrease the score
for the segment that has small overlap with the ground truth,
and increase the segment of larger overlap. This also ben-
eﬁts post-processing steps, such as NMS, to keep segment
with higher temporal localization accuracy.

3. Detailed descriptions of Segment-CNN

3.1. Problem setup
Problem deﬁnition. We denote a video as X = {xt}T
t=1
where xt is the t-th frame in X, and T is the total number
of frames in X. Each video X is associated with a set of
temporal action annotations Ψ =
,
m=1
where M is the total number of action instances in X, and
km, ψm, ψ
m are, respectively, action category of the in-
stance m and its starting time and ending time (measured
by frame ID). km ∈ {1, . . . , K}, where K is the number
of categories. During training, we have a set T of trimmed
videos and a set U of untrimmed videos. Each trimmed
video X ∈ T has ψm = 1, ψ

m = T , and M = 1.

m, km

ψm, ψ

(cid:17)(cid:111)M

(cid:110)(cid:16)

(cid:48)

(cid:48)

(cid:48)

Multi-scale segment generation. First, each frame is re-
sized to 171 (width) × 128 (height) pixels. For untrimmed
video X ∈ U, we conduct temporal sliding windows of
varied lengths as 16, 32, 64, 128, 256, 512 frames with
75% overlap. For each window, we construct segment
s by uniformly sampling 16 frames. Consequently, for
each untrimmed video X, we generate a set of candidates

(cid:110)(cid:16)

(cid:17)(cid:111)H

(cid:48)
sh, φh, φ
h

Φ =
as input for the proposal network,
where H is the total number of sliding windows for X, and
φm and φ
m are respectively starting time and ending time
of the h-th segment sh. For trimmed video X ∈ T , we di-

h=1

(cid:48)

rectly sample a segment s of 16 frames from X in uniform.

Network architecture. 3D ConvNets conducts 3D convo-
lution/pooling which operates in spatial and temporal di-
mensions simultaneously, and therefore can capture both
appearance and motion for action. Given the competitive
performances on video classiﬁcation tasks, our deep net-
works use 3D ConvNets as the basic architecture in all
stages and follow the network architecture of [37]. All
3D pooling layers use max pooling and have kernel size
of 2×2 in spatial with stride 2, while vary in temporal. All
3D convolutional ﬁlters have kernel size 3 and stride 1 in
all three dimensions. Using the notations conv(number
of ﬁlters) for the 3D convolutional layer, pool(temporal
kernel size, temporal stride) for the 3D pooling layer, and
fc(number of ﬁlters) for the fully connected layer, the lay-
out of these three types of layers in our architecture is
as follows: conv1a(64) - pool1(1,1) - conv2a(128) -
pool2(2,2) - conv3a(256) - conv3b(256) - pool3(2,2) -
conv4a(512) - conv4b(512) - pool4(2,2) - conv5a(512)
- conv5b(512) - pool5(2,2) - fc6(4096) - fc7(4096) -
fc8(K + 1). Each input for this deep network is a segment
s of dimension 171 × 128 × 16. C3D is training this net-
work on Sports-1M train split [37], and we use C3D as the
initialization for our proposal and classiﬁcation networks.

3.2. Training procedure

The proposal network: We train a CNN network Θpro as
the background segment ﬁlter. Basically, fc8 has two nodes
that correspondingly represent the background (rarely con-
tains action of interest) and being-action (has signiﬁcant
portion belongs to the actions of interest).

We use the following strategy to construct training data
Spro = {(sn, kn)}N
n=1, where label kn ∈ {0, 1}. For each
segment of the trimmed video X ∈ T , we set its label
as positive. For candidate segments from an untrimmed
video X ∈ U with temporal annotation Ψ, we assign a
label for each segment by evaluating its Intersection-over-
Union (IoU) with each ground truth instance in Ψ : (1)
if the highest IoU is larger than 0.7, we assign a positive
label; (2) if the highest IoU is smaller than 0.3, we set it
as the background. On the perspective of ground truth, if
there is no segment that overlaps with a ground truth in-
stance with IoU larger than 0.7, then we assign a positive
label segment s if s has the largest IoU with this ground
truth and its IoU is higher than 0.5. At last, we obtain
Spro = {(sn, kn)}Npro
n=1 which consists of all NT +NU posi-
tive segments and Nb ≈ NT +NU randomly sampled back-
ground segments, where Npro = NT + NU + Nb.

In all experiments, we use a learning rate of 0.0001, with
the exception of 0.01 for fc8, momentum of 0.9, weight de-
cay factor of 0.0005, and drop the learning rate by a factor
of 10 for every 10K iterations. The number of total itera-

tions depends on the scale of dataset and will be clariﬁed in
Section 4.

Note that, compared with the multi-class classiﬁcation
network, this proposal network is simpler because the out-
put layer only consists of two nodes (action or background).

The classiﬁcation network: After substantial background
segments are removed by the proposal network, we train a
classiﬁcation model Θcls for K action categories as well as
background.

Preparing the training data Scls follows a similar strategy
for the proposal network. Except when assigning label for
positive segment, the classiﬁcation network explicitly indi-
cates action category km ∈ {1, . . . , K}. Moreover, in order
to balance the number of training data for each class, we re-
duce the number of background instances to Nb ≈ NT +NU
K .
As for parameters in SGD, the learning rate is 0.0001,
with the exception of 0.01 for fc8, momentum is 0.9,
weight decay factor is 0.0005, and the learning rate is di-
vided by a factor of 2 for every 10K iterations, because the
convergence shall be slower when the number of classes in-
creases.

The localization network: As illustrated in Figure 2, it is
important to push up the prediction score of the segment
with larger overlap with the ground truth instance and de-
crease the scores of the segment with smaller overlap, to
make sure that the subsequent post-processing steps can
choose segments with higher overlap over those with small
overlap. Therefore, we propose this localization network
Θloc with a new loss function, which takes IoU with ground
truth instance into consideration.

Training data Sloc for the localization network are aug-
mented from Scls by associating each segment s with the
measurement of overlap, v. In speciﬁc, we set v = 1 for s
from trimmed video. If s comes from untrimmed video and
has positive label k, we set v equal to the overlap (measured
by IoU) of segment s with the associated ground truth in-
stance. If s is a background segment, as we can see later, its

Figure 2. Typical case of bad localizations. Assume that the sys-
tem outputs three predictions: A, B, C. Probably due to that there
are some evidences during [t1, t2], and A has the highest predic-
tion score. Therefore, the NMS will keep A, remove B, and then
keep C. However, actually we hope to remove A and C in NMS,
and keep B because B has the largest IoU with the ground truth
instance.

overlap measurement v will not affect our new loss function
and gradient computation in back-propagation, and thus we
simply set its v as 1.

During each mini-batch, we have N training samples
{(sn, kn, vn)}N
n=1. For the n-th segment, the output vec-
tor of fc8 is On and the prediction score vector after the
softmax layer is Pn. Note that for the i-th class, P (i)
n =

. The new loss function is formed by combining

(i)
n

eO
j=1 eO

(j)
n

(cid:80)N
Lsoftmax and Loverlap :

L = Lsoftmax + λ · Loverlap,

(1)

where λ balances the contribution from each part, and
through empirical validation, we ﬁnd that λ = 1 works well
in practice. Lsoftmax is the conventional softmax loss and is
deﬁned as

Lsoftmax =

(cid:88)

(cid:16)

− log

(cid:16)

P (kn)
n

(cid:17)(cid:17)

,

1
N

n

(2)

which is effective for training deep networks for classiﬁca-
tion. Loverlap is designed to jointly reduce the classiﬁcation
error and adjust the intensity of conﬁdence score according
to the extent of overlap:

Loverlap =

1
N

(cid:88)

n






1
2



(cid:16)

·






(cid:17)2
P (kn)
n

 · [kn > 0]
(vn)α − 1




.

(3)
Here, [kn > 0] is equal to 1 when the true class label kn is
positive, and it is equal to 0 when kn = 0, which means the
sn is a background training sample. Loverlap is intended to
boost the detection scores (P ) of segments that have high
overlaps (v) with ground truth instances, and reduce the
scores of those with small overlaps. The hyper-parameter
α controls the adjustment range for the intensity of the con-
ﬁdence score. The sensitivity of α is explored in Section 4.
In addition, the total gradient w.r.t output of the i-th node in
fc8 is as follows:

∂L
∂O(i)
n

=

∂Lsoftmax
∂O(i)
n

+ λ ·

∂Loverlap
∂O(i)
n

,

(4)

(5)

in which

and

∂Lsoftmax
∂O(i)
n

=

(cid:16)

(cid:40) 1

N ·

(cid:17)

n − 1

P (kn)
N · P (i)
1

n

if i = kn

if i (cid:54)= kn

(cid:18) (P (kn )
n
(vn)α

)2

(cid:16)

·

1
N ·

(cid:17)(cid:19)

1 − P (kn)
n

∂Loverlap
∂O(i)
n

=

(cid:18) (P (kn )
n
(vn)α

)2

1
N ·

(cid:16)

·

−P (i)
n

(cid:17)(cid:19)

· [kn > 0]






· [kn > 0]

if i = kn

.

if i (cid:54)= kn
(6)

Figure 3. An illustration of how Loverlap works compared with
Lsoftmax for each positive segment. Here we use α = 1, λ = 1,
and vary overlap v in Loverlap. The x-axis is the prediction score
at the node that corresponds to true label, and the y-axis is the loss.

Given a training sample (sn, kn, vn), Figure 3 shows
how Loverlap inﬂuences the original softmax loss. It also
provides more concrete insights about the design of this
loss function. (1) If the segment belongs to the background,
Loverlap = 0 and L = Lsoftmax. (2) If the segment is posi-
n = (cid:112)(vn)α, and there-
tive, L reachs the minimum at P (kn)
fore penalizes two cases: either P (kn)
is too small due to
misclassiﬁcation, or P (kn)
explodes and exceeds the learn-
ing target (cid:112)(vn)α which is proportional to overlap vn. Also
note that L is designed to increase as vn decreases, consid-
ering that the training segment with smaller overlap with
ground truth instance is less reliable because it may include
considerable noise. (3) In particular, if this positive segment
has overlap vn = 1, the loss function becomes similar to the
softmax loss, and L gradually decreases from +∞ to 1 as
P (kn)
n

goes from 0 to 1.

n

n

During optimization, Θloc is ﬁne-tuned on Θcls. Because
doing classiﬁcation is also one objective of the localization
network, and a trained classiﬁcation network can be good
initialization. We use the same learning rate, momentum,
and weight decay factor as for the classiﬁcation network.
Other parameters depending on the dataset are indicated in
Section 4.

3.3. Prediction and post-processing

During prediction, we slide varied length temporal win-
dow to generate a set of segments and input them into Θpro
to obtain proposal scores Ppro. In this paper, we keep seg-
ments with Ppro ≥ 0.7. Then we evaluate the retained
segments by Θloc to obtain action category predictions and
conﬁdence scores Ploc. During post-processing, we remove
all segments predicted as the background and reﬁne Ploc by
multiplying with class-speciﬁc frequency of occurrence for
each window length in the training data to leverage win-
dow length distribution patterns. Finally, because redun-
dant detections are not allowed in evaluation, we conduct
NMS based on Ploc to remove redundant detections, and

set the overlap threshold in NMS to a little bit smaller than
the overlap threshold θ in evaluation (θ − 0.1 in this paper).

4. Experiments

4.1. Datasets and setup

MEXaction2 [1]. This dataset contains two action classes:
“BullChargeCape” and “HorseRiding”. This dataset con-
INA videos, YouTube clips, and
sists of three subsets:
UCF101 Horse Riding clips. YouTube clips and UCF101
Horse Riding clips are trimmed, whereas INA videos are
untrimmed and are approximately 77 hours in total. With
regard to action instances with temporal annotation, they are
divided into train set (1336 instances), validation set (310
instances), and test set (329 instances).

THUMOS 2014 [15]. The temporal action detection task in
THUMOS Challenge 2014 is dedicated to localizing action
instances in long untrimmed videos. The detection task in-
volves 20 categories as indicated in Figure 4. The trimmed
videos used for training are 2755 videos of these 20 actions
in UCF101. The validation set contains 1010 untrimmed
videos with temporal annotations of 3007 instances in to-
tal. The test set contains 3358 action instances from 1574
untrimmed videos, whereas only 213 of them contain ac-
tion instances of interest. We exclude the remaining 1361
background videos in the test set.

4.2. Comparison with state-of-the-art systems

Evaluation metrics. We follow the conventional met-
rics used in THUMOS Challenge to regard temporal ac-
tion localization as a retrieval problem, and evaluate aver-
age precision (AP). A prediction is marked as correct only
when it has the correct category prediction, and has IoU
with ground truth instance larger than the overlap threshold
(measured by IoU). Note that redundant detections are not
allowed.

Results on MEXaction2. We build our system based on
Caffe [14] and C3D [37]. We use the train set in MEX-
action2 for training. The number of training iterations is
30K for the proposal network, 20K for the classiﬁcation net-
work, and 20K in the localization network with α = 0.25.

We denote our Segment-CNN using the above settings
as S-CNN and compare with typical dense trajectory fea-
tures (DTF) with bag-of-visual-words representation. The
results of DTF is provided by [1] 2, which trains three SVM
models with different set of negative samples and aver-
ages AP overall. According to Table 1, our Segment-CNN

2Note that the results reported in [1] use different evaluation metrics. To
make them comparable, we re-evaluate their prediction results according
to standard criteria mentioned in Section 4.2.

achieves tremendous performance gain for “BullCharge-
Cape” action and competitive performance for “HorseRid-
ing” action. Figure 5 displays our prediction results for
“BullChargeCape” and “HorseRiding”, respectively.

AP(%) BullChargeCape HorseRiding mAP
1.7
DTF
7.4
S-CNN

0.3
11.6

3.1
3.1

Table 1. Average precision on MEXaction2. The overlap threshold
is set to 0.5 during evaluation.

Results on THUMOS 20143: The instances in train set
and validation set are used for training. The number of
training iterations is 30K for all three networks. We again
set α = 0.25 for the localization network. We denote our
Segment-CNN using the above settings as S-CNN.

θ
Karaman et al. [17]
Wang et al. [41]
Oneata et al. [27]
S-CNN

0.1
1.5
19.2
39.8
47.7

0.2
0.9
17.8
36.2
43.5

0.3
0.5
14.6
28.8
36.3

0.4
0.3
12.1
21.8
28.7

0.5
0.2
8.5
15.0
19.0

Table 2. Mean average precision on THUMOS 2014 as the overlap
IoU threshold θ used in evaluation varies.

As for comparisons, beyond DTF, several baseline sys-
tems incorporate frame-level deep networks and even utilize
lots of other features: (1) Karaman et al. [17] used FV en-
coding of iDT with weighted saliency based pooling, and
conducted late fusion with frame-level CNN features. (2)
Wang et al. [41] built a system on iDT with FV represen-
tation and frame-level CNN features, and performed post-
processing to reﬁne the detection results. (3) Oneata et al.
[27] conducted localization using FV encoding of iDT on
temporal sliding windows, and performed post-processing
following [25]. Finally, they conducted weighted fusion for
the localization scores of temporal windows and video-level
scores generated by classiﬁers trained on iDT features, im-
age features, and audio features. The results are listed in
Table 2. AP for each class can be found in Figure 4. Our
Segment-CNN signiﬁcantly outperforms other systems for
14 of 20 actions, and the average performance improves
from 15.0% to 19.0%. We also show two prediction results
for the THUMOS 2014 test set in Figure 6.

Efﬁciency analysis. Our approach is very efﬁcient when
compared with all other systems, which typically fuse dif-
ferent features, and therefore can become quite cumber-

3Note that the evaluation toolkit used in THUMOS 2014 has some
bugs, and recently the organizers released a new toolkit with fair evalua-
tion criteria. Here, we re-evaluate the submission results of all teams using
the updated toolkit.

Figure 4. Histogram of average precision (%) for each class on THUMOS 2014 when the overlap threshold is set to 0.5 during evaluation.

some. Most segments generated from sliding windows are
removed by the ﬁrst proposal network, and thus the opera-
tions in classiﬁcation and localization are greatly reduced.
For each batch, the speed is around 1 second, and the num-
ber of segments can be processed during each batch depends
on the GPU memory (approximately 25 for GeForce GTX
980 of 4G memory). The storage requirement is also ex-
tremely small because our method does not need to cache
intermediate high dimensional features, such as FV to train
SVM. All required by Segment-CNN is three deep network
models, which occupy less than 1 GB in total.

4.3. Impact of individual networks

To study the effects of each network individually, we
compare four Segment-CNNs using different settings: (1)
S-CNN: keep all three networks and settings in Section
4.2, and Θloc is ﬁne-tuned on Θcls; (2) S-CNN (w/o pro-
posal): remove the proposal network completely, and di-
rectly use Θloc to do predictions on sliding windows; (3)
S-CNN (w/o classiﬁcation): remove the classiﬁcation net-
work completely and thus do not have Θcls to serve as ini-
tialization for training Θloc; (4) S-CNN (w/o localization):
remove the localization network completely and instead use
classiﬁcation model Θcls to produce predictions.

The proposal network. We compare S-CNN (w/o pro-
posal) and S-CNN, which includes the proposal network as
described above (two nodes in fc8). Because of the smaller
network architecture than S-CNN (w/o proposal), S-CNN
can reduce the number of operations conducted on back-
ground segments, and therefore accelerate speed. In addi-
tion, the results listed in Table 3 demonstrate that keeping
the proposal network can also improve precision because it
is designed for ﬁltering out background segments that lack
action of interests.
The classiﬁcation network. Although Θcls is not used dur-
ing prediction, the classiﬁcation network is still important
because ﬁne-tuning on Θcls results in better performance.
During evaluation here, we perform top-κ selection on the

networks
mAP(%)

S-CNN (w/o proposal)
17.1

S-CNN
19.0

Table 3. mAP comparisons on THUMOS 2014 between remov-
ing the proposal network and keeping the proposal network. The
overlap threshold is set to 0.5 during evaluation.

ﬁnal prediction results to select κ segments with maximum
conﬁdence scores. As shown in Figure 7, S-CNN ﬁne-
tuned on Θcls outperforms S-CNN (w/o classiﬁcation) con-
sistently when κ varies, and consequently the classiﬁcation
network is necessary during training.

Figure 7. Effects of the classiﬁcation and localization networks.
y-axis is mAP(%) on THUMOS 2014, and x-axis varies the depth
κ in top-κ selection. The overlap threshold is set to 0.5 during
evaluation.

The localization network. Figure 7 also proves the ef-
fectiveness of the localization network. By adding the lo-
calization network, S-CNN can signiﬁcantly improve per-
formances compared with the baseline S-CNN (w/o local-
ization), which only contains the proposal and classiﬁca-
tion networks. This is because the new loss function in-
troduced in the localization network reﬁnes the scores in
favoring segments of higher overlap with the ground truths,
and therefore higher temporal localization accuracy can be
achieved.

Figure 5. Prediction results for two action instances from MEXaction2 when the overlap threshold is set to 0.5 during evaluation. For
each ground truth instance, we show two prediction results: A has the highest conﬁdence score among the predictions associated with this
ground truth, and B is an incorrect prediction. BullChargeCape: A is correct, but B is incorrect because each ground truth only allows one
detection. HorseRiding: A is correct, but B is incorrect because each ground truth only allows one detection. The numbers shown with #
are frame IDs.

Figure 6. Prediction results for two action instances from THUMOS 2014 test set when the overlap threshold is set to 0.5 during evaluation.
For each ground truth instance, we show two prediction results: A has the highest conﬁdence score among the predictions associated with
this ground truth, and B is an incorrect prediction. ClearAndJerk: A is correct, but B is incorrect because its overlap IoU with ground truth
is less than threshold 0.5. LongJump: A is correct, but B is incorrect because it has the wrong action category prediction - PoleVault.

In addition, we vary α in the overlap loss term Loverlap of
the loss function to evaluate its sensitivity. We ﬁnd that our
approach has stable performances over a range of α value
(e.g., from 0.25 to 1.0).

5. Conclusion

We propose an effective multi-stage framework called
Segment-CNN to address temporal action localization in
untrimmed long videos. Through the above evaluation for
each network, we demonstrate the contribution from the
proposal network to identify candidate segments, the ne-
cessity of the classiﬁcation network to provide good initial-
ization for training the localization model, and the effec-
tiveness of the new loss function used in the localization
network to precisely localize action instances in time.

In the future, we would like to extend our work to events
and activities, which usually consist of multiple actions,

therefore precisely localizing action instances in time can
be helpful for their recognition and detection.

6. Acknowledgment

This work is supported by the Intelligence Advanced Re-
search Projects Activity (IARPA) via Department of Interior
National Business Center contract number D11PC20071.
The U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwithstanding
any copyright annotation thereon. Disclaimer: The views
and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing
the ofﬁcial policies or endorsements, either expressed or im-
plied, of IARPA, DOI-NBC, or the U.S. Government. We
thank Dong Liu, Guangnan Ye, and anonymous reviewers
for the insightful suggestions.

References

[1] Mexaction2.

http://mexculture.cnam.fr/

xwiki/bin/view/Datasets/Mex+action+
dataset, 2015. 6

[2] J. K. Aggarwal and M. S. Ryoo. Human activity analysis: A

review. In ACM Computing Surveys, 2011. 1, 2

[3] G. Cheng, Y. Wan, A. N. Saudagar, K. Namuduri, and B. P.
Buckles. Advances in human action recognition: A survey.
2015. 1, 2

[4] A. Gaidon, Z. Harchaoui, and C. Schmid. Actom sequence
models for efﬁcient action detection. In CVPR, 2011. 3
[5] A. Gaidon, Z. Harchaoui, and C. Schmid. Temporal local-

ization of actions with actoms. In TPAMI, 2013. 3

[6] R. Girshick. Fast r-cnn. In ICCV, 2015. 1, 3
[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 1, 3

[8] G. Gkioxari and J. Malik. Finding action tubes. In CVPR,

2015. 3

[9] A. Gorban, H. Idrees, Y.-G. Jiang, A. R. Zamir, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Action
recognition with a large number of classes. http://www.
thumos.info/, 2015. 1, 2, 3

[10] M. Jain, J. van Gemert, H. J´egou, P. Bouthemy, and
C. Snoek. Action localization with tubelets from motion.
In CVPR, 2014. 3

[11] M. Jain, J. van Gemert, T. Mensink, and C. Snoek. Ob-
jects2action: Classifying and localizing actions without any
video example. In ICCV, 2015. 3

[12] M. Jain, J. van Gemert, and C. Snoek. What do 15,000 object
categories tell us about classifying and localizing actions? In
CVPR, 2015. 3

[13] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. In TPMAI, 2013. 1,
3

[14] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. In ACM MM, 2014.
6

[15] Y.-G. Jiang, J. Liu, A. R. Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Ac-
tion recognition with a large number of classes. http:
//crcv.ucf.edu/THUMOS14/, 2014. 1, 3, 6

[16] Z. Jiang, Z. Lin, and L. S. Davis. A uniﬁed tree-based frame-
work for joint action localization, recognition and segmenta-
tion. In CVIU, 2013. 3

[17] S. Karaman, L. Seidenari, and A. D. Bimbo. Fast saliency
based pooling of ﬁsher encoded dense trajectories. In ECCV
THUMOS Workshop, 2014. 1, 6

[18] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and F.-F. Li. Large-scale video classiﬁcation with convolu-
tional neural networks. In CVPR, 2014. 1, 3

[19] A. Kl¨aser, M. Marszałek, C. Schmid, and A. Zisserman. Hu-
man focused action localization in video. In Trends and Top-
ics in Computer Vision, 2012. 3

[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 3

Imagenet
In

[21] W. Kuo, B. Hariharan, and J. Malik. Deepbox: Learning
objectness with convolutional networks. In ICCV, 2015. 1, 3
[22] K.-T. Lai, D. Liu, M.-S. Chen, and S.-F. Chang. Recogniz-
ing complex events in videos by learning key static-dynamic
evidences. In ECCV, 2014. 2

[23] K.-T. Lai, F. X. Yu, M.-S. Chen, and S.-F. Chang. Video
In
event detection by inferring temporal instance labels.
CVPR, 2014. 2

[24] Z. Lan, M. Lin, X. Li, A. G. Hauptmann, and B. Raj. Be-
yond gaussian pyramid: Multi-skip feature stacking for ac-
tion recognition. In CVPR, 2015. 1, 2

[25] D. Oneata, J. Verbeek, and C. Schmid. Action and event
recognition with ﬁsher vectors on a compact feature set. In
ICCV, 2013. 1, 2, 6

[26] D. Oneata, J. Verbeek, and C. Schmid. Efﬁcient action lo-
calization with approximately normalized ﬁsher vectors. In
CVPR, 2014. 3

[27] D. Oneata, J. Verbeek, and C. Schmid. The lear submission

at thumos 2014. In ECCV THUMOS Workshop, 2014. 1, 6

[28] R. Poppe. A survey on vision-based human action recogni-

tion. In Image and vision computing, 2010. 1, 2

[29] M. M. Puscas, E. Sangineto, D. Culibrk, and N. Sebe. Un-
supervised tube extraction using transductive learning and
dense trajectories. In ICCV, 2015. 3

[30] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 1, 3

[31] K. Simonyan and A. Zisserman. Two-stream convolutional
networks for action recognition in videos. In NIPS, 2014. 1,
3

[32] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Learning Representations, 2015. 3

[33] K. Soomro, H. Idrees, and M. Shah. Action localization in

videos through context walk. In ICCV, 2015. 3

[34] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset
of 101 human actions classes from videos in the wild.
In
CRCV-TR-12-01, 2012. 3

[35] A. Stoian, M. Ferecatu, J. Benois-Pineau, and M. Crucianu.
Fast action localization in large scale video archives.
In
TCSVT, 2015. 3

[36] C. Sun, S. Shetty, R. Sukthankar, and R. Nevatia. Tempo-
ral localization of ﬁne-grained actions in videos by domain
transfer from web images. In ACM MM, 2015. 2

[37] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In ICCV, 2015. 1, 3, 4, 6

[38] J. van Gemert, M. Jain, E. Gati, and C. Snoek. Apt: Action
In BMVC,

localization proposals from dense trajectories.
2015. 3

[39] H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Action
Recognition by Dense Trajectories. In CVPR, 2011. 1, 2
[40] H. Wang and C. Schmid. Action recognition with improved

trajectories. In ICCV, 2013. 1, 2

[41] L. Wang, Y. Qiao, and X. Tang. Action recognition and de-
tection by combining motion and appearance features.
In
ECCV THUMOS Workshop, 2014. 1, 6

[42] D. Weinland, R. Ronfard, and E. Boyer. A survey of vision-
based methods for action representation, segmentation and
recognition. In Computer Vision and Image Understanding,
2011. 1, 2

[43] P. Weinzaepfel, Z. Harchaoui, and C. Schmid. Learning to
track for spatio-temporal action localization. In ICCV, 2015.
3

[44] Z. Xu, Y. Yang, and A. G. Hauptmann. A discriminative cnn
video representation for event detection. In CVPR, 2015. 1,
3

[45] G. Yu and J. Yuan. Fast action proposals for human action

detection and search. In CVPR, 2015. 3

[46] J. Yuan, Y. Pei, B. Ni, P. Moulin, and A. Kassim. Adsc
submission at thumos challenge 2015. In CVPR THUMOS
Workshop, 2015. 1

Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs

Zheng Shou, Dongang Wang, and Shih-Fu Chang
Columbia University
New York, NY, USA
{zs2262,dw2648,sc250}@columbia.edu

6
1
0
2
 
r
p
A
 
1
2
 
 
]

V
C
.
s
c
[
 
 
2
v
9
2
1
2
0
.
1
0
6
1
:
v
i
X
r
a

Abstract

We address temporal action localization in untrimmed
long videos. This is important because videos in real ap-
plications are usually unconstrained and contain multiple
action instances plus video content of background scenes
or other activities. To address this challenging issue, we
exploit the effectiveness of deep networks in temporal ac-
tion localization via three segment-based 3D ConvNets: (1)
a proposal network identiﬁes candidate segments in a long
video that may contain actions; (2) a classiﬁcation network
learns one-vs-all action classiﬁcation model to serve as ini-
tialization for the localization network; and (3) a localiza-
tion network ﬁne-tunes the learned classiﬁcation network to
localize each action instance. We propose a novel loss func-
tion for the localization network to explicitly consider tem-
poral overlap and achieve high temporal localization accu-
racy. In the end, only the proposal network and the local-
ization network are used during prediction. On two large-
scale benchmarks, our approach achieves signiﬁcantly su-
perior performances compared with other state-of-the-art
systems: mAP increases from 1.7% to 7.4% on MEXaction2
and increases from 15.0% to 19.0% on THUMOS 2014.

1. Introduction

Impressive progress has been reported in recent litera-
ture for action recognition [42, 28, 2, 3, 39, 40, 24, 18, 31,
44, 13, 37]. Besides detecting action in manually trimmed
short video, researchers start to develop techniques for de-
tecting actions in untrimmed long videos in the wild. This
trend motivates another challenging topic - temporal action
localization: given a long untrimmed video, “when does a
speciﬁc action start and end?” This problem is important
because real applications usually involve long untrimmed
videos, which can be highly unconstrained in space and
time, and one video can contain multiple action instances
plus background scenes or other activities. Localizing ac-
tions in long videos, such as those in surveillance, can save
tremendous time and computational costs.

Most state-of-the-art methods rely on manually selected
features, and their performances still require much improve-
ment. For example, top performing approaches in THU-
MOS Challenge 2014 [27, 41, 17, 15] and 2015 [46, 9] both
used improved Dense Trajectory (iDT) with Fisher Vector
(FV) [40, 25]. There have been some recent attempts at in-
corporating iDT features with appearance features automat-
ically extracted by frame-level deep networks [27, 41, 17].
Nevertheless, such 2D ConvNets do not capture motion in-
formation, which is important for modeling actions and de-
termining their temporal boundaries.

As an analogy in still images, object detection recently
achieved large improvements by using deep networks. In-
spired by Region-based Convolutional Neural Networks (R-
CNN) [7] and its upgraded versions [6, 30, 21], we develop
Segment-CNN1, which is an effective deep network frame-
work for temporal action localization as outlined in Figure
1. We adopt 3D ConvNets [13, 37], which recently has
been shown to be promising for capturing motion charac-
teristics in videos, and add a new multi-stage framework.
First, multi-scale segments are generated as candidates for
three deep networks. The proposal network classiﬁes each
segment as either action or background in order to eliminate
background segment estimated to be unlikely to contain ac-
tions of interest. The classiﬁcation network trains typical
one-vs-all classiﬁcation model for all action categories plus
the background.

However, the classiﬁcation network aims at ﬁnding key
evidences to distinguish different categories, rather than lo-
calizing precise action presences in time. Sometimes, the
scores from the classiﬁcation network can be high even
when the segment has only a very small overlap with the
ground truth instance. This can be detrimental because sub-
sequent post-processing steps, such as Non-Maximum Sup-
pression (NMS), might remove segment of small score but
large overlap with ground truth. To explicitly take tempo-
ral overlap into consideration, we introduce the localiza-
tion network based on the same architecture, but this net-

1Source code and trained models are available online at https://

github.com/zhengshou/scnn/.

Figure 1. Overview of our framework. (a) Multi-scale segment generation: given an untrimmed video, we generate segments of varied
lengths via sliding window; (b) Segment-CNN: the proposal network identiﬁes candidate segments, the classiﬁcation network trains an
action recognition model to serve as initialization for the localization network, and the localization network localizes action instances in
time and outputs conﬁdence scores; (c) Post-processing: using the prediction scores from the localization network, we further remove
redundancy by NMS to obtain the ﬁnal results. During training, the classiﬁcation network is ﬁrst learned and then used as initialization for
the localization network. During prediction, only the proposal and localization networks are used.

work uses a novel loss function, which rewards segments
with higher temporal overlap with the ground truths, and
thus can generate conﬁdence scores more suitable for post-
processing. Note that the classiﬁcation network cannot be
replaced by the localization network. We will show later
that using the trained classiﬁcation network (without con-
sidering temporal overlap) to initialize the localization net-
work (take into account temporal overlap) is important, and
achieves better temporal localization accuracies.

To summarize, our main contributions are three-fold:
(1) To the best of our knowledge, our work is the ﬁrst to
exploit 3D ConvNets with multi-stage processes for tempo-
ral action localization in untrimmed long videos in the wild.
(2) We introduce an effective multi-stage Segment-CNN
framework, to propose candidate segments, recognize ac-
tions, and localize temporal boundaries. The proposal net-
work improves the efﬁciency by eliminating unlikely candi-
date segments, and the localization network is key to tem-
poral localization accuracy boosting.

(3) The proposed techniques signiﬁcantly outperform the
state-of-the-art systems over two large-scale benchmarks
suitable for temporal action localization. When the overlap
threshold used in evaluation is set to 0.5, our approach im-
proves mAP on MEXaction2 from 1.7% to 7.4% and mAP
on THUMOS 2014 from 15.0% to 19.0%. We did not eval-
uate on THUMOS Challenge 2015 [9] because the ground

truth is withheld by organizers for future evaluation. More
detailed evaluation results are available in Section 4.

2. Related work

Temporal action localization. This topic has been studied
in two directions. When training data only have video-level
category labels but no temporal annotations, researchers
formulated this as weakly supervised problems or multi-
ple instance learning problems to learn the key evidences
in untrimmed videos and temporally localize actions by se-
lecting key instances [22, 23]. Sun et al. [36] transferred
knowledge from web images to address temporal localiza-
tion in untrimmed web videos.

Another line of work focuses on learning from data when
the temporal boundaries have been annotated for action in-
stances in untrimmed videos, such as THUMOS. Most of
these works pose this as a classiﬁcation problem and adopt
a temporal sliding window approach, where each window
is considered as an action candidate subject to classiﬁca-
tion [25]. Surveys about action classiﬁcation methods can
be found in [42, 28, 2, 3]. Recently, two directions lead
the state-of-the-art: (1) Wang et al. [39] proposed extract-
ing HOG, HOF, MBH features along dense trajectories, and
later on they took camera motion into consideration [40].
Further improvement can be achieved by stacking features
(2) Enlighted by the suc-
with multiple time skips [24].

cess of CNNs in recent works [20, 32], Karpathy et al. [18]
evaluated frame-level CNNs on large-scale video classiﬁ-
cation tasks. Simonyan and Zisserman [31] designed two-
stream CNNs to learn from still image and motion ﬂow re-
In [44], a latent concept descriptor of convo-
spectively.
lutional feature map was proposed, and great results were
achieved on event detection with VLAD encoding. To learn
spatio-temporal features together, the architecture of 3D
ConvNets was explored in [13, 37], achieving competitive
results. Oneata et al. [26] proposed approximately normal-
ized Fisher Vectors to reduce the high dimensionality of FV.
Stoian et al. [35] introduced a two-level cascade to allow
fast search for action instances. Instead of precision, these
methods focus on improving the efﬁciency of conventional
methods. To speciﬁcally address the temporal precision of
action detection, Gaidon et al.
[4, 5] modeled the struc-
ture of action sequence with atomic action units (actoms).
The explicit modeling of action units allows for matching
more complete action unit sequences, rather than just par-
tial content. However, this requires mannual annotations
for actoms, which can be subjective and burdensome. Our
paper presented here aims to solve the same problem of pre-
cise temporal localization, but without requiring the difﬁcult
task of manual annotation for atomic action units.

[10] and Soomro et al.

Spatio-temporal localization. There have been active ex-
plorations about localizing action in space and time simul-
taneously. Jain et al.
[33] built
their work on supervoxel. Recently, researchers treat this
as a tracking problem [43, 8] by leveraging object detectors
[11], especially human detectors [16, 19, 8, 45] to detect re-
gions of interest in each frame and then output sequences
of bounding boxes. Dense trajectories have also been ex-
ploited for extracting the action tubes [29, 38]. Jain et al.
[12] added object encodings to help action localization.

However, this problem is different from temporal local-
ization, which is the main topic in this paper: (1) When
using object detectors to ﬁnd spatio-temporal regions of
interest, such approaches assume that the actions are per-
formed by human or other pre-deﬁned objects. (2) Spatio-
temporal localization requires exhaustive annotations for
objects of interest on every frame as training data. This
makes it overwhelmingly time-consuming particularly for
long untrimmed videos compared with the task of simply
labeling the start time and end time of an action depicted in
the video, which is sufﬁcient to satisfy many applications.

Object detection. Inspired by the success of deep learning
approaches in object detection, we also review R-CNN and
its variations. R-CNN consists of selective search, CNN
feature extraction, SVM classiﬁcation, and bounding box
regression [7]. Fast R-CNN reshapes R-CNN into a single-
stage using multi-task loss, and also has a RoI pooling layer

to share the computation of one image in ConvNets [6].
Our work differs from R-CNN in the following aspects:
(1) Temporal annotations in training videos can be diverse:
some are cleanly trimmed action instances cut out from
long videos, such as UCF101 [34], and some are untrimmed
long videos but with temporal boundaries annotated for ac-
tion instances, such as THUMOS [15, 9]. We provide a
paradigm that can handle such diverse annotations. (2) As
proven in Faster R-CNN [30] which proposes region pro-
posal network, and DeepBox [21] which detects objectness
to re-rank the results of R-CNN, using deep networks for
learning objectness is effective and efﬁcient. Therefore, we
directly use deep network to classify background and action
to obtain candidate segments. (3) We remove the regression
stage because learning regression for time shift and dura-
tion of video segment does not work well in our experi-
ments, probably because actions can be quite diverse, and
therefore do not contain consistent patterns for predicting
start/end time. To achieve precise localization, we design
the localization network using a new loss function to explic-
itly consider temporal overlap. This can decrease the score
for the segment that has small overlap with the ground truth,
and increase the segment of larger overlap. This also ben-
eﬁts post-processing steps, such as NMS, to keep segment
with higher temporal localization accuracy.

3. Detailed descriptions of Segment-CNN

3.1. Problem setup
Problem deﬁnition. We denote a video as X = {xt}T
t=1
where xt is the t-th frame in X, and T is the total number
of frames in X. Each video X is associated with a set of
temporal action annotations Ψ =
,
m=1
where M is the total number of action instances in X, and
km, ψm, ψ
m are, respectively, action category of the in-
stance m and its starting time and ending time (measured
by frame ID). km ∈ {1, . . . , K}, where K is the number
of categories. During training, we have a set T of trimmed
videos and a set U of untrimmed videos. Each trimmed
video X ∈ T has ψm = 1, ψ

m = T , and M = 1.

m, km

ψm, ψ

(cid:17)(cid:111)M

(cid:110)(cid:16)

(cid:48)

(cid:48)

(cid:48)

Multi-scale segment generation. First, each frame is re-
sized to 171 (width) × 128 (height) pixels. For untrimmed
video X ∈ U, we conduct temporal sliding windows of
varied lengths as 16, 32, 64, 128, 256, 512 frames with
75% overlap. For each window, we construct segment
s by uniformly sampling 16 frames. Consequently, for
each untrimmed video X, we generate a set of candidates

(cid:110)(cid:16)

(cid:17)(cid:111)H

(cid:48)
sh, φh, φ
h

Φ =
as input for the proposal network,
where H is the total number of sliding windows for X, and
φm and φ
m are respectively starting time and ending time
of the h-th segment sh. For trimmed video X ∈ T , we di-

h=1

(cid:48)

rectly sample a segment s of 16 frames from X in uniform.

Network architecture. 3D ConvNets conducts 3D convo-
lution/pooling which operates in spatial and temporal di-
mensions simultaneously, and therefore can capture both
appearance and motion for action. Given the competitive
performances on video classiﬁcation tasks, our deep net-
works use 3D ConvNets as the basic architecture in all
stages and follow the network architecture of [37]. All
3D pooling layers use max pooling and have kernel size
of 2×2 in spatial with stride 2, while vary in temporal. All
3D convolutional ﬁlters have kernel size 3 and stride 1 in
all three dimensions. Using the notations conv(number
of ﬁlters) for the 3D convolutional layer, pool(temporal
kernel size, temporal stride) for the 3D pooling layer, and
fc(number of ﬁlters) for the fully connected layer, the lay-
out of these three types of layers in our architecture is
as follows: conv1a(64) - pool1(1,1) - conv2a(128) -
pool2(2,2) - conv3a(256) - conv3b(256) - pool3(2,2) -
conv4a(512) - conv4b(512) - pool4(2,2) - conv5a(512)
- conv5b(512) - pool5(2,2) - fc6(4096) - fc7(4096) -
fc8(K + 1). Each input for this deep network is a segment
s of dimension 171 × 128 × 16. C3D is training this net-
work on Sports-1M train split [37], and we use C3D as the
initialization for our proposal and classiﬁcation networks.

3.2. Training procedure

The proposal network: We train a CNN network Θpro as
the background segment ﬁlter. Basically, fc8 has two nodes
that correspondingly represent the background (rarely con-
tains action of interest) and being-action (has signiﬁcant
portion belongs to the actions of interest).

We use the following strategy to construct training data
Spro = {(sn, kn)}N
n=1, where label kn ∈ {0, 1}. For each
segment of the trimmed video X ∈ T , we set its label
as positive. For candidate segments from an untrimmed
video X ∈ U with temporal annotation Ψ, we assign a
label for each segment by evaluating its Intersection-over-
Union (IoU) with each ground truth instance in Ψ : (1)
if the highest IoU is larger than 0.7, we assign a positive
label; (2) if the highest IoU is smaller than 0.3, we set it
as the background. On the perspective of ground truth, if
there is no segment that overlaps with a ground truth in-
stance with IoU larger than 0.7, then we assign a positive
label segment s if s has the largest IoU with this ground
truth and its IoU is higher than 0.5. At last, we obtain
Spro = {(sn, kn)}Npro
n=1 which consists of all NT +NU posi-
tive segments and Nb ≈ NT +NU randomly sampled back-
ground segments, where Npro = NT + NU + Nb.

In all experiments, we use a learning rate of 0.0001, with
the exception of 0.01 for fc8, momentum of 0.9, weight de-
cay factor of 0.0005, and drop the learning rate by a factor
of 10 for every 10K iterations. The number of total itera-

tions depends on the scale of dataset and will be clariﬁed in
Section 4.

Note that, compared with the multi-class classiﬁcation
network, this proposal network is simpler because the out-
put layer only consists of two nodes (action or background).

The classiﬁcation network: After substantial background
segments are removed by the proposal network, we train a
classiﬁcation model Θcls for K action categories as well as
background.

Preparing the training data Scls follows a similar strategy
for the proposal network. Except when assigning label for
positive segment, the classiﬁcation network explicitly indi-
cates action category km ∈ {1, . . . , K}. Moreover, in order
to balance the number of training data for each class, we re-
duce the number of background instances to Nb ≈ NT +NU
K .
As for parameters in SGD, the learning rate is 0.0001,
with the exception of 0.01 for fc8, momentum is 0.9,
weight decay factor is 0.0005, and the learning rate is di-
vided by a factor of 2 for every 10K iterations, because the
convergence shall be slower when the number of classes in-
creases.

The localization network: As illustrated in Figure 2, it is
important to push up the prediction score of the segment
with larger overlap with the ground truth instance and de-
crease the scores of the segment with smaller overlap, to
make sure that the subsequent post-processing steps can
choose segments with higher overlap over those with small
overlap. Therefore, we propose this localization network
Θloc with a new loss function, which takes IoU with ground
truth instance into consideration.

Training data Sloc for the localization network are aug-
mented from Scls by associating each segment s with the
measurement of overlap, v. In speciﬁc, we set v = 1 for s
from trimmed video. If s comes from untrimmed video and
has positive label k, we set v equal to the overlap (measured
by IoU) of segment s with the associated ground truth in-
stance. If s is a background segment, as we can see later, its

Figure 2. Typical case of bad localizations. Assume that the sys-
tem outputs three predictions: A, B, C. Probably due to that there
are some evidences during [t1, t2], and A has the highest predic-
tion score. Therefore, the NMS will keep A, remove B, and then
keep C. However, actually we hope to remove A and C in NMS,
and keep B because B has the largest IoU with the ground truth
instance.

overlap measurement v will not affect our new loss function
and gradient computation in back-propagation, and thus we
simply set its v as 1.

During each mini-batch, we have N training samples
{(sn, kn, vn)}N
n=1. For the n-th segment, the output vec-
tor of fc8 is On and the prediction score vector after the
softmax layer is Pn. Note that for the i-th class, P (i)
n =

. The new loss function is formed by combining

(i)
n

eO
j=1 eO

(j)
n

(cid:80)N
Lsoftmax and Loverlap :

L = Lsoftmax + λ · Loverlap,

(1)

where λ balances the contribution from each part, and
through empirical validation, we ﬁnd that λ = 1 works well
in practice. Lsoftmax is the conventional softmax loss and is
deﬁned as

Lsoftmax =

(cid:88)

(cid:16)

− log

(cid:16)

P (kn)
n

(cid:17)(cid:17)

,

1
N

n

(2)

which is effective for training deep networks for classiﬁca-
tion. Loverlap is designed to jointly reduce the classiﬁcation
error and adjust the intensity of conﬁdence score according
to the extent of overlap:

Loverlap =

1
N

(cid:88)

n






1
2



(cid:16)

·






(cid:17)2
P (kn)
n

 · [kn > 0]
(vn)α − 1




.

(3)
Here, [kn > 0] is equal to 1 when the true class label kn is
positive, and it is equal to 0 when kn = 0, which means the
sn is a background training sample. Loverlap is intended to
boost the detection scores (P ) of segments that have high
overlaps (v) with ground truth instances, and reduce the
scores of those with small overlaps. The hyper-parameter
α controls the adjustment range for the intensity of the con-
ﬁdence score. The sensitivity of α is explored in Section 4.
In addition, the total gradient w.r.t output of the i-th node in
fc8 is as follows:

∂L
∂O(i)
n

=

∂Lsoftmax
∂O(i)
n

+ λ ·

∂Loverlap
∂O(i)
n

,

(4)

(5)

in which

and

∂Lsoftmax
∂O(i)
n

=

(cid:16)

(cid:40) 1

N ·

(cid:17)

n − 1

P (kn)
N · P (i)
1

n

if i = kn

if i (cid:54)= kn

(cid:18) (P (kn )
n
(vn)α

)2

(cid:16)

·

1
N ·

(cid:17)(cid:19)

1 − P (kn)
n

∂Loverlap
∂O(i)
n

=

(cid:18) (P (kn )
n
(vn)α

)2

1
N ·

(cid:16)

·

−P (i)
n

(cid:17)(cid:19)

· [kn > 0]






· [kn > 0]

if i = kn

.

if i (cid:54)= kn
(6)

Figure 3. An illustration of how Loverlap works compared with
Lsoftmax for each positive segment. Here we use α = 1, λ = 1,
and vary overlap v in Loverlap. The x-axis is the prediction score
at the node that corresponds to true label, and the y-axis is the loss.

Given a training sample (sn, kn, vn), Figure 3 shows
how Loverlap inﬂuences the original softmax loss. It also
provides more concrete insights about the design of this
loss function. (1) If the segment belongs to the background,
Loverlap = 0 and L = Lsoftmax. (2) If the segment is posi-
n = (cid:112)(vn)α, and there-
tive, L reachs the minimum at P (kn)
fore penalizes two cases: either P (kn)
is too small due to
misclassiﬁcation, or P (kn)
explodes and exceeds the learn-
ing target (cid:112)(vn)α which is proportional to overlap vn. Also
note that L is designed to increase as vn decreases, consid-
ering that the training segment with smaller overlap with
ground truth instance is less reliable because it may include
considerable noise. (3) In particular, if this positive segment
has overlap vn = 1, the loss function becomes similar to the
softmax loss, and L gradually decreases from +∞ to 1 as
P (kn)
n

goes from 0 to 1.

n

n

During optimization, Θloc is ﬁne-tuned on Θcls. Because
doing classiﬁcation is also one objective of the localization
network, and a trained classiﬁcation network can be good
initialization. We use the same learning rate, momentum,
and weight decay factor as for the classiﬁcation network.
Other parameters depending on the dataset are indicated in
Section 4.

3.3. Prediction and post-processing

During prediction, we slide varied length temporal win-
dow to generate a set of segments and input them into Θpro
to obtain proposal scores Ppro. In this paper, we keep seg-
ments with Ppro ≥ 0.7. Then we evaluate the retained
segments by Θloc to obtain action category predictions and
conﬁdence scores Ploc. During post-processing, we remove
all segments predicted as the background and reﬁne Ploc by
multiplying with class-speciﬁc frequency of occurrence for
each window length in the training data to leverage win-
dow length distribution patterns. Finally, because redun-
dant detections are not allowed in evaluation, we conduct
NMS based on Ploc to remove redundant detections, and

set the overlap threshold in NMS to a little bit smaller than
the overlap threshold θ in evaluation (θ − 0.1 in this paper).

4. Experiments

4.1. Datasets and setup

MEXaction2 [1]. This dataset contains two action classes:
“BullChargeCape” and “HorseRiding”. This dataset con-
INA videos, YouTube clips, and
sists of three subsets:
UCF101 Horse Riding clips. YouTube clips and UCF101
Horse Riding clips are trimmed, whereas INA videos are
untrimmed and are approximately 77 hours in total. With
regard to action instances with temporal annotation, they are
divided into train set (1336 instances), validation set (310
instances), and test set (329 instances).

THUMOS 2014 [15]. The temporal action detection task in
THUMOS Challenge 2014 is dedicated to localizing action
instances in long untrimmed videos. The detection task in-
volves 20 categories as indicated in Figure 4. The trimmed
videos used for training are 2755 videos of these 20 actions
in UCF101. The validation set contains 1010 untrimmed
videos with temporal annotations of 3007 instances in to-
tal. The test set contains 3358 action instances from 1574
untrimmed videos, whereas only 213 of them contain ac-
tion instances of interest. We exclude the remaining 1361
background videos in the test set.

4.2. Comparison with state-of-the-art systems

Evaluation metrics. We follow the conventional met-
rics used in THUMOS Challenge to regard temporal ac-
tion localization as a retrieval problem, and evaluate aver-
age precision (AP). A prediction is marked as correct only
when it has the correct category prediction, and has IoU
with ground truth instance larger than the overlap threshold
(measured by IoU). Note that redundant detections are not
allowed.

Results on MEXaction2. We build our system based on
Caffe [14] and C3D [37]. We use the train set in MEX-
action2 for training. The number of training iterations is
30K for the proposal network, 20K for the classiﬁcation net-
work, and 20K in the localization network with α = 0.25.

We denote our Segment-CNN using the above settings
as S-CNN and compare with typical dense trajectory fea-
tures (DTF) with bag-of-visual-words representation. The
results of DTF is provided by [1] 2, which trains three SVM
models with different set of negative samples and aver-
ages AP overall. According to Table 1, our Segment-CNN

2Note that the results reported in [1] use different evaluation metrics. To
make them comparable, we re-evaluate their prediction results according
to standard criteria mentioned in Section 4.2.

achieves tremendous performance gain for “BullCharge-
Cape” action and competitive performance for “HorseRid-
ing” action. Figure 5 displays our prediction results for
“BullChargeCape” and “HorseRiding”, respectively.

AP(%) BullChargeCape HorseRiding mAP
1.7
DTF
7.4
S-CNN

0.3
11.6

3.1
3.1

Table 1. Average precision on MEXaction2. The overlap threshold
is set to 0.5 during evaluation.

Results on THUMOS 20143: The instances in train set
and validation set are used for training. The number of
training iterations is 30K for all three networks. We again
set α = 0.25 for the localization network. We denote our
Segment-CNN using the above settings as S-CNN.

θ
Karaman et al. [17]
Wang et al. [41]
Oneata et al. [27]
S-CNN

0.1
1.5
19.2
39.8
47.7

0.2
0.9
17.8
36.2
43.5

0.3
0.5
14.6
28.8
36.3

0.4
0.3
12.1
21.8
28.7

0.5
0.2
8.5
15.0
19.0

Table 2. Mean average precision on THUMOS 2014 as the overlap
IoU threshold θ used in evaluation varies.

As for comparisons, beyond DTF, several baseline sys-
tems incorporate frame-level deep networks and even utilize
lots of other features: (1) Karaman et al. [17] used FV en-
coding of iDT with weighted saliency based pooling, and
conducted late fusion with frame-level CNN features. (2)
Wang et al. [41] built a system on iDT with FV represen-
tation and frame-level CNN features, and performed post-
processing to reﬁne the detection results. (3) Oneata et al.
[27] conducted localization using FV encoding of iDT on
temporal sliding windows, and performed post-processing
following [25]. Finally, they conducted weighted fusion for
the localization scores of temporal windows and video-level
scores generated by classiﬁers trained on iDT features, im-
age features, and audio features. The results are listed in
Table 2. AP for each class can be found in Figure 4. Our
Segment-CNN signiﬁcantly outperforms other systems for
14 of 20 actions, and the average performance improves
from 15.0% to 19.0%. We also show two prediction results
for the THUMOS 2014 test set in Figure 6.

Efﬁciency analysis. Our approach is very efﬁcient when
compared with all other systems, which typically fuse dif-
ferent features, and therefore can become quite cumber-

3Note that the evaluation toolkit used in THUMOS 2014 has some
bugs, and recently the organizers released a new toolkit with fair evalua-
tion criteria. Here, we re-evaluate the submission results of all teams using
the updated toolkit.

Figure 4. Histogram of average precision (%) for each class on THUMOS 2014 when the overlap threshold is set to 0.5 during evaluation.

some. Most segments generated from sliding windows are
removed by the ﬁrst proposal network, and thus the opera-
tions in classiﬁcation and localization are greatly reduced.
For each batch, the speed is around 1 second, and the num-
ber of segments can be processed during each batch depends
on the GPU memory (approximately 25 for GeForce GTX
980 of 4G memory). The storage requirement is also ex-
tremely small because our method does not need to cache
intermediate high dimensional features, such as FV to train
SVM. All required by Segment-CNN is three deep network
models, which occupy less than 1 GB in total.

4.3. Impact of individual networks

To study the effects of each network individually, we
compare four Segment-CNNs using different settings: (1)
S-CNN: keep all three networks and settings in Section
4.2, and Θloc is ﬁne-tuned on Θcls; (2) S-CNN (w/o pro-
posal): remove the proposal network completely, and di-
rectly use Θloc to do predictions on sliding windows; (3)
S-CNN (w/o classiﬁcation): remove the classiﬁcation net-
work completely and thus do not have Θcls to serve as ini-
tialization for training Θloc; (4) S-CNN (w/o localization):
remove the localization network completely and instead use
classiﬁcation model Θcls to produce predictions.

The proposal network. We compare S-CNN (w/o pro-
posal) and S-CNN, which includes the proposal network as
described above (two nodes in fc8). Because of the smaller
network architecture than S-CNN (w/o proposal), S-CNN
can reduce the number of operations conducted on back-
ground segments, and therefore accelerate speed. In addi-
tion, the results listed in Table 3 demonstrate that keeping
the proposal network can also improve precision because it
is designed for ﬁltering out background segments that lack
action of interests.
The classiﬁcation network. Although Θcls is not used dur-
ing prediction, the classiﬁcation network is still important
because ﬁne-tuning on Θcls results in better performance.
During evaluation here, we perform top-κ selection on the

networks
mAP(%)

S-CNN (w/o proposal)
17.1

S-CNN
19.0

Table 3. mAP comparisons on THUMOS 2014 between remov-
ing the proposal network and keeping the proposal network. The
overlap threshold is set to 0.5 during evaluation.

ﬁnal prediction results to select κ segments with maximum
conﬁdence scores. As shown in Figure 7, S-CNN ﬁne-
tuned on Θcls outperforms S-CNN (w/o classiﬁcation) con-
sistently when κ varies, and consequently the classiﬁcation
network is necessary during training.

Figure 7. Effects of the classiﬁcation and localization networks.
y-axis is mAP(%) on THUMOS 2014, and x-axis varies the depth
κ in top-κ selection. The overlap threshold is set to 0.5 during
evaluation.

The localization network. Figure 7 also proves the ef-
fectiveness of the localization network. By adding the lo-
calization network, S-CNN can signiﬁcantly improve per-
formances compared with the baseline S-CNN (w/o local-
ization), which only contains the proposal and classiﬁca-
tion networks. This is because the new loss function in-
troduced in the localization network reﬁnes the scores in
favoring segments of higher overlap with the ground truths,
and therefore higher temporal localization accuracy can be
achieved.

Figure 5. Prediction results for two action instances from MEXaction2 when the overlap threshold is set to 0.5 during evaluation. For
each ground truth instance, we show two prediction results: A has the highest conﬁdence score among the predictions associated with this
ground truth, and B is an incorrect prediction. BullChargeCape: A is correct, but B is incorrect because each ground truth only allows one
detection. HorseRiding: A is correct, but B is incorrect because each ground truth only allows one detection. The numbers shown with #
are frame IDs.

Figure 6. Prediction results for two action instances from THUMOS 2014 test set when the overlap threshold is set to 0.5 during evaluation.
For each ground truth instance, we show two prediction results: A has the highest conﬁdence score among the predictions associated with
this ground truth, and B is an incorrect prediction. ClearAndJerk: A is correct, but B is incorrect because its overlap IoU with ground truth
is less than threshold 0.5. LongJump: A is correct, but B is incorrect because it has the wrong action category prediction - PoleVault.

In addition, we vary α in the overlap loss term Loverlap of
the loss function to evaluate its sensitivity. We ﬁnd that our
approach has stable performances over a range of α value
(e.g., from 0.25 to 1.0).

5. Conclusion

We propose an effective multi-stage framework called
Segment-CNN to address temporal action localization in
untrimmed long videos. Through the above evaluation for
each network, we demonstrate the contribution from the
proposal network to identify candidate segments, the ne-
cessity of the classiﬁcation network to provide good initial-
ization for training the localization model, and the effec-
tiveness of the new loss function used in the localization
network to precisely localize action instances in time.

In the future, we would like to extend our work to events
and activities, which usually consist of multiple actions,

therefore precisely localizing action instances in time can
be helpful for their recognition and detection.

6. Acknowledgment

This work is supported by the Intelligence Advanced Re-
search Projects Activity (IARPA) via Department of Interior
National Business Center contract number D11PC20071.
The U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwithstanding
any copyright annotation thereon. Disclaimer: The views
and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing
the ofﬁcial policies or endorsements, either expressed or im-
plied, of IARPA, DOI-NBC, or the U.S. Government. We
thank Dong Liu, Guangnan Ye, and anonymous reviewers
for the insightful suggestions.

References

[1] Mexaction2.

http://mexculture.cnam.fr/

xwiki/bin/view/Datasets/Mex+action+
dataset, 2015. 6

[2] J. K. Aggarwal and M. S. Ryoo. Human activity analysis: A

review. In ACM Computing Surveys, 2011. 1, 2

[3] G. Cheng, Y. Wan, A. N. Saudagar, K. Namuduri, and B. P.
Buckles. Advances in human action recognition: A survey.
2015. 1, 2

[4] A. Gaidon, Z. Harchaoui, and C. Schmid. Actom sequence
models for efﬁcient action detection. In CVPR, 2011. 3
[5] A. Gaidon, Z. Harchaoui, and C. Schmid. Temporal local-

ization of actions with actoms. In TPAMI, 2013. 3

[6] R. Girshick. Fast r-cnn. In ICCV, 2015. 1, 3
[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 1, 3

[8] G. Gkioxari and J. Malik. Finding action tubes. In CVPR,

2015. 3

[9] A. Gorban, H. Idrees, Y.-G. Jiang, A. R. Zamir, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Action
recognition with a large number of classes. http://www.
thumos.info/, 2015. 1, 2, 3

[10] M. Jain, J. van Gemert, H. J´egou, P. Bouthemy, and
C. Snoek. Action localization with tubelets from motion.
In CVPR, 2014. 3

[11] M. Jain, J. van Gemert, T. Mensink, and C. Snoek. Ob-
jects2action: Classifying and localizing actions without any
video example. In ICCV, 2015. 3

[12] M. Jain, J. van Gemert, and C. Snoek. What do 15,000 object
categories tell us about classifying and localizing actions? In
CVPR, 2015. 3

[13] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. In TPMAI, 2013. 1,
3

[14] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. In ACM MM, 2014.
6

[15] Y.-G. Jiang, J. Liu, A. R. Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Ac-
tion recognition with a large number of classes. http:
//crcv.ucf.edu/THUMOS14/, 2014. 1, 3, 6

[16] Z. Jiang, Z. Lin, and L. S. Davis. A uniﬁed tree-based frame-
work for joint action localization, recognition and segmenta-
tion. In CVIU, 2013. 3

[17] S. Karaman, L. Seidenari, and A. D. Bimbo. Fast saliency
based pooling of ﬁsher encoded dense trajectories. In ECCV
THUMOS Workshop, 2014. 1, 6

[18] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and F.-F. Li. Large-scale video classiﬁcation with convolu-
tional neural networks. In CVPR, 2014. 1, 3

[19] A. Kl¨aser, M. Marszałek, C. Schmid, and A. Zisserman. Hu-
man focused action localization in video. In Trends and Top-
ics in Computer Vision, 2012. 3

[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 3

Imagenet
In

[21] W. Kuo, B. Hariharan, and J. Malik. Deepbox: Learning
objectness with convolutional networks. In ICCV, 2015. 1, 3
[22] K.-T. Lai, D. Liu, M.-S. Chen, and S.-F. Chang. Recogniz-
ing complex events in videos by learning key static-dynamic
evidences. In ECCV, 2014. 2

[23] K.-T. Lai, F. X. Yu, M.-S. Chen, and S.-F. Chang. Video
In
event detection by inferring temporal instance labels.
CVPR, 2014. 2

[24] Z. Lan, M. Lin, X. Li, A. G. Hauptmann, and B. Raj. Be-
yond gaussian pyramid: Multi-skip feature stacking for ac-
tion recognition. In CVPR, 2015. 1, 2

[25] D. Oneata, J. Verbeek, and C. Schmid. Action and event
recognition with ﬁsher vectors on a compact feature set. In
ICCV, 2013. 1, 2, 6

[26] D. Oneata, J. Verbeek, and C. Schmid. Efﬁcient action lo-
calization with approximately normalized ﬁsher vectors. In
CVPR, 2014. 3

[27] D. Oneata, J. Verbeek, and C. Schmid. The lear submission

at thumos 2014. In ECCV THUMOS Workshop, 2014. 1, 6

[28] R. Poppe. A survey on vision-based human action recogni-

tion. In Image and vision computing, 2010. 1, 2

[29] M. M. Puscas, E. Sangineto, D. Culibrk, and N. Sebe. Un-
supervised tube extraction using transductive learning and
dense trajectories. In ICCV, 2015. 3

[30] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 1, 3

[31] K. Simonyan and A. Zisserman. Two-stream convolutional
networks for action recognition in videos. In NIPS, 2014. 1,
3

[32] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Learning Representations, 2015. 3

[33] K. Soomro, H. Idrees, and M. Shah. Action localization in

videos through context walk. In ICCV, 2015. 3

[34] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset
of 101 human actions classes from videos in the wild.
In
CRCV-TR-12-01, 2012. 3

[35] A. Stoian, M. Ferecatu, J. Benois-Pineau, and M. Crucianu.
Fast action localization in large scale video archives.
In
TCSVT, 2015. 3

[36] C. Sun, S. Shetty, R. Sukthankar, and R. Nevatia. Tempo-
ral localization of ﬁne-grained actions in videos by domain
transfer from web images. In ACM MM, 2015. 2

[37] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In ICCV, 2015. 1, 3, 4, 6

[38] J. van Gemert, M. Jain, E. Gati, and C. Snoek. Apt: Action
In BMVC,

localization proposals from dense trajectories.
2015. 3

[39] H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Action
Recognition by Dense Trajectories. In CVPR, 2011. 1, 2
[40] H. Wang and C. Schmid. Action recognition with improved

trajectories. In ICCV, 2013. 1, 2

[41] L. Wang, Y. Qiao, and X. Tang. Action recognition and de-
tection by combining motion and appearance features.
In
ECCV THUMOS Workshop, 2014. 1, 6

[42] D. Weinland, R. Ronfard, and E. Boyer. A survey of vision-
based methods for action representation, segmentation and
recognition. In Computer Vision and Image Understanding,
2011. 1, 2

[43] P. Weinzaepfel, Z. Harchaoui, and C. Schmid. Learning to
track for spatio-temporal action localization. In ICCV, 2015.
3

[44] Z. Xu, Y. Yang, and A. G. Hauptmann. A discriminative cnn
video representation for event detection. In CVPR, 2015. 1,
3

[45] G. Yu and J. Yuan. Fast action proposals for human action

detection and search. In CVPR, 2015. 3

[46] J. Yuan, Y. Pei, B. Ni, P. Moulin, and A. Kassim. Adsc
submission at thumos challenge 2015. In CVPR THUMOS
Workshop, 2015. 1

Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs

Zheng Shou, Dongang Wang, and Shih-Fu Chang
Columbia University
New York, NY, USA
{zs2262,dw2648,sc250}@columbia.edu

6
1
0
2
 
r
p
A
 
1
2
 
 
]

V
C
.
s
c
[
 
 
2
v
9
2
1
2
0
.
1
0
6
1
:
v
i
X
r
a

Abstract

We address temporal action localization in untrimmed
long videos. This is important because videos in real ap-
plications are usually unconstrained and contain multiple
action instances plus video content of background scenes
or other activities. To address this challenging issue, we
exploit the effectiveness of deep networks in temporal ac-
tion localization via three segment-based 3D ConvNets: (1)
a proposal network identiﬁes candidate segments in a long
video that may contain actions; (2) a classiﬁcation network
learns one-vs-all action classiﬁcation model to serve as ini-
tialization for the localization network; and (3) a localiza-
tion network ﬁne-tunes the learned classiﬁcation network to
localize each action instance. We propose a novel loss func-
tion for the localization network to explicitly consider tem-
poral overlap and achieve high temporal localization accu-
racy. In the end, only the proposal network and the local-
ization network are used during prediction. On two large-
scale benchmarks, our approach achieves signiﬁcantly su-
perior performances compared with other state-of-the-art
systems: mAP increases from 1.7% to 7.4% on MEXaction2
and increases from 15.0% to 19.0% on THUMOS 2014.

1. Introduction

Impressive progress has been reported in recent litera-
ture for action recognition [42, 28, 2, 3, 39, 40, 24, 18, 31,
44, 13, 37]. Besides detecting action in manually trimmed
short video, researchers start to develop techniques for de-
tecting actions in untrimmed long videos in the wild. This
trend motivates another challenging topic - temporal action
localization: given a long untrimmed video, “when does a
speciﬁc action start and end?” This problem is important
because real applications usually involve long untrimmed
videos, which can be highly unconstrained in space and
time, and one video can contain multiple action instances
plus background scenes or other activities. Localizing ac-
tions in long videos, such as those in surveillance, can save
tremendous time and computational costs.

Most state-of-the-art methods rely on manually selected
features, and their performances still require much improve-
ment. For example, top performing approaches in THU-
MOS Challenge 2014 [27, 41, 17, 15] and 2015 [46, 9] both
used improved Dense Trajectory (iDT) with Fisher Vector
(FV) [40, 25]. There have been some recent attempts at in-
corporating iDT features with appearance features automat-
ically extracted by frame-level deep networks [27, 41, 17].
Nevertheless, such 2D ConvNets do not capture motion in-
formation, which is important for modeling actions and de-
termining their temporal boundaries.

As an analogy in still images, object detection recently
achieved large improvements by using deep networks. In-
spired by Region-based Convolutional Neural Networks (R-
CNN) [7] and its upgraded versions [6, 30, 21], we develop
Segment-CNN1, which is an effective deep network frame-
work for temporal action localization as outlined in Figure
1. We adopt 3D ConvNets [13, 37], which recently has
been shown to be promising for capturing motion charac-
teristics in videos, and add a new multi-stage framework.
First, multi-scale segments are generated as candidates for
three deep networks. The proposal network classiﬁes each
segment as either action or background in order to eliminate
background segment estimated to be unlikely to contain ac-
tions of interest. The classiﬁcation network trains typical
one-vs-all classiﬁcation model for all action categories plus
the background.

However, the classiﬁcation network aims at ﬁnding key
evidences to distinguish different categories, rather than lo-
calizing precise action presences in time. Sometimes, the
scores from the classiﬁcation network can be high even
when the segment has only a very small overlap with the
ground truth instance. This can be detrimental because sub-
sequent post-processing steps, such as Non-Maximum Sup-
pression (NMS), might remove segment of small score but
large overlap with ground truth. To explicitly take tempo-
ral overlap into consideration, we introduce the localiza-
tion network based on the same architecture, but this net-

1Source code and trained models are available online at https://

github.com/zhengshou/scnn/.

Figure 1. Overview of our framework. (a) Multi-scale segment generation: given an untrimmed video, we generate segments of varied
lengths via sliding window; (b) Segment-CNN: the proposal network identiﬁes candidate segments, the classiﬁcation network trains an
action recognition model to serve as initialization for the localization network, and the localization network localizes action instances in
time and outputs conﬁdence scores; (c) Post-processing: using the prediction scores from the localization network, we further remove
redundancy by NMS to obtain the ﬁnal results. During training, the classiﬁcation network is ﬁrst learned and then used as initialization for
the localization network. During prediction, only the proposal and localization networks are used.

work uses a novel loss function, which rewards segments
with higher temporal overlap with the ground truths, and
thus can generate conﬁdence scores more suitable for post-
processing. Note that the classiﬁcation network cannot be
replaced by the localization network. We will show later
that using the trained classiﬁcation network (without con-
sidering temporal overlap) to initialize the localization net-
work (take into account temporal overlap) is important, and
achieves better temporal localization accuracies.

To summarize, our main contributions are three-fold:
(1) To the best of our knowledge, our work is the ﬁrst to
exploit 3D ConvNets with multi-stage processes for tempo-
ral action localization in untrimmed long videos in the wild.
(2) We introduce an effective multi-stage Segment-CNN
framework, to propose candidate segments, recognize ac-
tions, and localize temporal boundaries. The proposal net-
work improves the efﬁciency by eliminating unlikely candi-
date segments, and the localization network is key to tem-
poral localization accuracy boosting.

(3) The proposed techniques signiﬁcantly outperform the
state-of-the-art systems over two large-scale benchmarks
suitable for temporal action localization. When the overlap
threshold used in evaluation is set to 0.5, our approach im-
proves mAP on MEXaction2 from 1.7% to 7.4% and mAP
on THUMOS 2014 from 15.0% to 19.0%. We did not eval-
uate on THUMOS Challenge 2015 [9] because the ground

truth is withheld by organizers for future evaluation. More
detailed evaluation results are available in Section 4.

2. Related work

Temporal action localization. This topic has been studied
in two directions. When training data only have video-level
category labels but no temporal annotations, researchers
formulated this as weakly supervised problems or multi-
ple instance learning problems to learn the key evidences
in untrimmed videos and temporally localize actions by se-
lecting key instances [22, 23]. Sun et al. [36] transferred
knowledge from web images to address temporal localiza-
tion in untrimmed web videos.

Another line of work focuses on learning from data when
the temporal boundaries have been annotated for action in-
stances in untrimmed videos, such as THUMOS. Most of
these works pose this as a classiﬁcation problem and adopt
a temporal sliding window approach, where each window
is considered as an action candidate subject to classiﬁca-
tion [25]. Surveys about action classiﬁcation methods can
be found in [42, 28, 2, 3]. Recently, two directions lead
the state-of-the-art: (1) Wang et al. [39] proposed extract-
ing HOG, HOF, MBH features along dense trajectories, and
later on they took camera motion into consideration [40].
Further improvement can be achieved by stacking features
(2) Enlighted by the suc-
with multiple time skips [24].

cess of CNNs in recent works [20, 32], Karpathy et al. [18]
evaluated frame-level CNNs on large-scale video classiﬁ-
cation tasks. Simonyan and Zisserman [31] designed two-
stream CNNs to learn from still image and motion ﬂow re-
In [44], a latent concept descriptor of convo-
spectively.
lutional feature map was proposed, and great results were
achieved on event detection with VLAD encoding. To learn
spatio-temporal features together, the architecture of 3D
ConvNets was explored in [13, 37], achieving competitive
results. Oneata et al. [26] proposed approximately normal-
ized Fisher Vectors to reduce the high dimensionality of FV.
Stoian et al. [35] introduced a two-level cascade to allow
fast search for action instances. Instead of precision, these
methods focus on improving the efﬁciency of conventional
methods. To speciﬁcally address the temporal precision of
action detection, Gaidon et al.
[4, 5] modeled the struc-
ture of action sequence with atomic action units (actoms).
The explicit modeling of action units allows for matching
more complete action unit sequences, rather than just par-
tial content. However, this requires mannual annotations
for actoms, which can be subjective and burdensome. Our
paper presented here aims to solve the same problem of pre-
cise temporal localization, but without requiring the difﬁcult
task of manual annotation for atomic action units.

[10] and Soomro et al.

Spatio-temporal localization. There have been active ex-
plorations about localizing action in space and time simul-
taneously. Jain et al.
[33] built
their work on supervoxel. Recently, researchers treat this
as a tracking problem [43, 8] by leveraging object detectors
[11], especially human detectors [16, 19, 8, 45] to detect re-
gions of interest in each frame and then output sequences
of bounding boxes. Dense trajectories have also been ex-
ploited for extracting the action tubes [29, 38]. Jain et al.
[12] added object encodings to help action localization.

However, this problem is different from temporal local-
ization, which is the main topic in this paper: (1) When
using object detectors to ﬁnd spatio-temporal regions of
interest, such approaches assume that the actions are per-
formed by human or other pre-deﬁned objects. (2) Spatio-
temporal localization requires exhaustive annotations for
objects of interest on every frame as training data. This
makes it overwhelmingly time-consuming particularly for
long untrimmed videos compared with the task of simply
labeling the start time and end time of an action depicted in
the video, which is sufﬁcient to satisfy many applications.

Object detection. Inspired by the success of deep learning
approaches in object detection, we also review R-CNN and
its variations. R-CNN consists of selective search, CNN
feature extraction, SVM classiﬁcation, and bounding box
regression [7]. Fast R-CNN reshapes R-CNN into a single-
stage using multi-task loss, and also has a RoI pooling layer

to share the computation of one image in ConvNets [6].
Our work differs from R-CNN in the following aspects:
(1) Temporal annotations in training videos can be diverse:
some are cleanly trimmed action instances cut out from
long videos, such as UCF101 [34], and some are untrimmed
long videos but with temporal boundaries annotated for ac-
tion instances, such as THUMOS [15, 9]. We provide a
paradigm that can handle such diverse annotations. (2) As
proven in Faster R-CNN [30] which proposes region pro-
posal network, and DeepBox [21] which detects objectness
to re-rank the results of R-CNN, using deep networks for
learning objectness is effective and efﬁcient. Therefore, we
directly use deep network to classify background and action
to obtain candidate segments. (3) We remove the regression
stage because learning regression for time shift and dura-
tion of video segment does not work well in our experi-
ments, probably because actions can be quite diverse, and
therefore do not contain consistent patterns for predicting
start/end time. To achieve precise localization, we design
the localization network using a new loss function to explic-
itly consider temporal overlap. This can decrease the score
for the segment that has small overlap with the ground truth,
and increase the segment of larger overlap. This also ben-
eﬁts post-processing steps, such as NMS, to keep segment
with higher temporal localization accuracy.

3. Detailed descriptions of Segment-CNN

3.1. Problem setup
Problem deﬁnition. We denote a video as X = {xt}T
t=1
where xt is the t-th frame in X, and T is the total number
of frames in X. Each video X is associated with a set of
temporal action annotations Ψ =
,
m=1
where M is the total number of action instances in X, and
km, ψm, ψ
m are, respectively, action category of the in-
stance m and its starting time and ending time (measured
by frame ID). km ∈ {1, . . . , K}, where K is the number
of categories. During training, we have a set T of trimmed
videos and a set U of untrimmed videos. Each trimmed
video X ∈ T has ψm = 1, ψ

m = T , and M = 1.

m, km

ψm, ψ

(cid:17)(cid:111)M

(cid:110)(cid:16)

(cid:48)

(cid:48)

(cid:48)

Multi-scale segment generation. First, each frame is re-
sized to 171 (width) × 128 (height) pixels. For untrimmed
video X ∈ U, we conduct temporal sliding windows of
varied lengths as 16, 32, 64, 128, 256, 512 frames with
75% overlap. For each window, we construct segment
s by uniformly sampling 16 frames. Consequently, for
each untrimmed video X, we generate a set of candidates

(cid:110)(cid:16)

(cid:17)(cid:111)H

(cid:48)
sh, φh, φ
h

Φ =
as input for the proposal network,
where H is the total number of sliding windows for X, and
φm and φ
m are respectively starting time and ending time
of the h-th segment sh. For trimmed video X ∈ T , we di-

h=1

(cid:48)

rectly sample a segment s of 16 frames from X in uniform.

Network architecture. 3D ConvNets conducts 3D convo-
lution/pooling which operates in spatial and temporal di-
mensions simultaneously, and therefore can capture both
appearance and motion for action. Given the competitive
performances on video classiﬁcation tasks, our deep net-
works use 3D ConvNets as the basic architecture in all
stages and follow the network architecture of [37]. All
3D pooling layers use max pooling and have kernel size
of 2×2 in spatial with stride 2, while vary in temporal. All
3D convolutional ﬁlters have kernel size 3 and stride 1 in
all three dimensions. Using the notations conv(number
of ﬁlters) for the 3D convolutional layer, pool(temporal
kernel size, temporal stride) for the 3D pooling layer, and
fc(number of ﬁlters) for the fully connected layer, the lay-
out of these three types of layers in our architecture is
as follows: conv1a(64) - pool1(1,1) - conv2a(128) -
pool2(2,2) - conv3a(256) - conv3b(256) - pool3(2,2) -
conv4a(512) - conv4b(512) - pool4(2,2) - conv5a(512)
- conv5b(512) - pool5(2,2) - fc6(4096) - fc7(4096) -
fc8(K + 1). Each input for this deep network is a segment
s of dimension 171 × 128 × 16. C3D is training this net-
work on Sports-1M train split [37], and we use C3D as the
initialization for our proposal and classiﬁcation networks.

3.2. Training procedure

The proposal network: We train a CNN network Θpro as
the background segment ﬁlter. Basically, fc8 has two nodes
that correspondingly represent the background (rarely con-
tains action of interest) and being-action (has signiﬁcant
portion belongs to the actions of interest).

We use the following strategy to construct training data
Spro = {(sn, kn)}N
n=1, where label kn ∈ {0, 1}. For each
segment of the trimmed video X ∈ T , we set its label
as positive. For candidate segments from an untrimmed
video X ∈ U with temporal annotation Ψ, we assign a
label for each segment by evaluating its Intersection-over-
Union (IoU) with each ground truth instance in Ψ : (1)
if the highest IoU is larger than 0.7, we assign a positive
label; (2) if the highest IoU is smaller than 0.3, we set it
as the background. On the perspective of ground truth, if
there is no segment that overlaps with a ground truth in-
stance with IoU larger than 0.7, then we assign a positive
label segment s if s has the largest IoU with this ground
truth and its IoU is higher than 0.5. At last, we obtain
Spro = {(sn, kn)}Npro
n=1 which consists of all NT +NU posi-
tive segments and Nb ≈ NT +NU randomly sampled back-
ground segments, where Npro = NT + NU + Nb.

In all experiments, we use a learning rate of 0.0001, with
the exception of 0.01 for fc8, momentum of 0.9, weight de-
cay factor of 0.0005, and drop the learning rate by a factor
of 10 for every 10K iterations. The number of total itera-

tions depends on the scale of dataset and will be clariﬁed in
Section 4.

Note that, compared with the multi-class classiﬁcation
network, this proposal network is simpler because the out-
put layer only consists of two nodes (action or background).

The classiﬁcation network: After substantial background
segments are removed by the proposal network, we train a
classiﬁcation model Θcls for K action categories as well as
background.

Preparing the training data Scls follows a similar strategy
for the proposal network. Except when assigning label for
positive segment, the classiﬁcation network explicitly indi-
cates action category km ∈ {1, . . . , K}. Moreover, in order
to balance the number of training data for each class, we re-
duce the number of background instances to Nb ≈ NT +NU
K .
As for parameters in SGD, the learning rate is 0.0001,
with the exception of 0.01 for fc8, momentum is 0.9,
weight decay factor is 0.0005, and the learning rate is di-
vided by a factor of 2 for every 10K iterations, because the
convergence shall be slower when the number of classes in-
creases.

The localization network: As illustrated in Figure 2, it is
important to push up the prediction score of the segment
with larger overlap with the ground truth instance and de-
crease the scores of the segment with smaller overlap, to
make sure that the subsequent post-processing steps can
choose segments with higher overlap over those with small
overlap. Therefore, we propose this localization network
Θloc with a new loss function, which takes IoU with ground
truth instance into consideration.

Training data Sloc for the localization network are aug-
mented from Scls by associating each segment s with the
measurement of overlap, v. In speciﬁc, we set v = 1 for s
from trimmed video. If s comes from untrimmed video and
has positive label k, we set v equal to the overlap (measured
by IoU) of segment s with the associated ground truth in-
stance. If s is a background segment, as we can see later, its

Figure 2. Typical case of bad localizations. Assume that the sys-
tem outputs three predictions: A, B, C. Probably due to that there
are some evidences during [t1, t2], and A has the highest predic-
tion score. Therefore, the NMS will keep A, remove B, and then
keep C. However, actually we hope to remove A and C in NMS,
and keep B because B has the largest IoU with the ground truth
instance.

overlap measurement v will not affect our new loss function
and gradient computation in back-propagation, and thus we
simply set its v as 1.

During each mini-batch, we have N training samples
{(sn, kn, vn)}N
n=1. For the n-th segment, the output vec-
tor of fc8 is On and the prediction score vector after the
softmax layer is Pn. Note that for the i-th class, P (i)
n =

. The new loss function is formed by combining

(i)
n

eO
j=1 eO

(j)
n

(cid:80)N
Lsoftmax and Loverlap :

L = Lsoftmax + λ · Loverlap,

(1)

where λ balances the contribution from each part, and
through empirical validation, we ﬁnd that λ = 1 works well
in practice. Lsoftmax is the conventional softmax loss and is
deﬁned as

Lsoftmax =

(cid:88)

(cid:16)

− log

(cid:16)

P (kn)
n

(cid:17)(cid:17)

,

1
N

n

(2)

which is effective for training deep networks for classiﬁca-
tion. Loverlap is designed to jointly reduce the classiﬁcation
error and adjust the intensity of conﬁdence score according
to the extent of overlap:

Loverlap =

1
N

(cid:88)

n






1
2



(cid:16)

·






(cid:17)2
P (kn)
n

 · [kn > 0]
(vn)α − 1




.

(3)
Here, [kn > 0] is equal to 1 when the true class label kn is
positive, and it is equal to 0 when kn = 0, which means the
sn is a background training sample. Loverlap is intended to
boost the detection scores (P ) of segments that have high
overlaps (v) with ground truth instances, and reduce the
scores of those with small overlaps. The hyper-parameter
α controls the adjustment range for the intensity of the con-
ﬁdence score. The sensitivity of α is explored in Section 4.
In addition, the total gradient w.r.t output of the i-th node in
fc8 is as follows:

∂L
∂O(i)
n

=

∂Lsoftmax
∂O(i)
n

+ λ ·

∂Loverlap
∂O(i)
n

,

(4)

(5)

in which

and

∂Lsoftmax
∂O(i)
n

=

(cid:16)

(cid:40) 1

N ·

(cid:17)

n − 1

P (kn)
N · P (i)
1

n

if i = kn

if i (cid:54)= kn

(cid:18) (P (kn )
n
(vn)α

)2

(cid:16)

·

1
N ·

(cid:17)(cid:19)

1 − P (kn)
n

∂Loverlap
∂O(i)
n

=

(cid:18) (P (kn )
n
(vn)α

)2

1
N ·

(cid:16)

·

−P (i)
n

(cid:17)(cid:19)

· [kn > 0]






· [kn > 0]

if i = kn

.

if i (cid:54)= kn
(6)

Figure 3. An illustration of how Loverlap works compared with
Lsoftmax for each positive segment. Here we use α = 1, λ = 1,
and vary overlap v in Loverlap. The x-axis is the prediction score
at the node that corresponds to true label, and the y-axis is the loss.

Given a training sample (sn, kn, vn), Figure 3 shows
how Loverlap inﬂuences the original softmax loss. It also
provides more concrete insights about the design of this
loss function. (1) If the segment belongs to the background,
Loverlap = 0 and L = Lsoftmax. (2) If the segment is posi-
n = (cid:112)(vn)α, and there-
tive, L reachs the minimum at P (kn)
fore penalizes two cases: either P (kn)
is too small due to
misclassiﬁcation, or P (kn)
explodes and exceeds the learn-
ing target (cid:112)(vn)α which is proportional to overlap vn. Also
note that L is designed to increase as vn decreases, consid-
ering that the training segment with smaller overlap with
ground truth instance is less reliable because it may include
considerable noise. (3) In particular, if this positive segment
has overlap vn = 1, the loss function becomes similar to the
softmax loss, and L gradually decreases from +∞ to 1 as
P (kn)
n

goes from 0 to 1.

n

n

During optimization, Θloc is ﬁne-tuned on Θcls. Because
doing classiﬁcation is also one objective of the localization
network, and a trained classiﬁcation network can be good
initialization. We use the same learning rate, momentum,
and weight decay factor as for the classiﬁcation network.
Other parameters depending on the dataset are indicated in
Section 4.

3.3. Prediction and post-processing

During prediction, we slide varied length temporal win-
dow to generate a set of segments and input them into Θpro
to obtain proposal scores Ppro. In this paper, we keep seg-
ments with Ppro ≥ 0.7. Then we evaluate the retained
segments by Θloc to obtain action category predictions and
conﬁdence scores Ploc. During post-processing, we remove
all segments predicted as the background and reﬁne Ploc by
multiplying with class-speciﬁc frequency of occurrence for
each window length in the training data to leverage win-
dow length distribution patterns. Finally, because redun-
dant detections are not allowed in evaluation, we conduct
NMS based on Ploc to remove redundant detections, and

set the overlap threshold in NMS to a little bit smaller than
the overlap threshold θ in evaluation (θ − 0.1 in this paper).

4. Experiments

4.1. Datasets and setup

MEXaction2 [1]. This dataset contains two action classes:
“BullChargeCape” and “HorseRiding”. This dataset con-
INA videos, YouTube clips, and
sists of three subsets:
UCF101 Horse Riding clips. YouTube clips and UCF101
Horse Riding clips are trimmed, whereas INA videos are
untrimmed and are approximately 77 hours in total. With
regard to action instances with temporal annotation, they are
divided into train set (1336 instances), validation set (310
instances), and test set (329 instances).

THUMOS 2014 [15]. The temporal action detection task in
THUMOS Challenge 2014 is dedicated to localizing action
instances in long untrimmed videos. The detection task in-
volves 20 categories as indicated in Figure 4. The trimmed
videos used for training are 2755 videos of these 20 actions
in UCF101. The validation set contains 1010 untrimmed
videos with temporal annotations of 3007 instances in to-
tal. The test set contains 3358 action instances from 1574
untrimmed videos, whereas only 213 of them contain ac-
tion instances of interest. We exclude the remaining 1361
background videos in the test set.

4.2. Comparison with state-of-the-art systems

Evaluation metrics. We follow the conventional met-
rics used in THUMOS Challenge to regard temporal ac-
tion localization as a retrieval problem, and evaluate aver-
age precision (AP). A prediction is marked as correct only
when it has the correct category prediction, and has IoU
with ground truth instance larger than the overlap threshold
(measured by IoU). Note that redundant detections are not
allowed.

Results on MEXaction2. We build our system based on
Caffe [14] and C3D [37]. We use the train set in MEX-
action2 for training. The number of training iterations is
30K for the proposal network, 20K for the classiﬁcation net-
work, and 20K in the localization network with α = 0.25.

We denote our Segment-CNN using the above settings
as S-CNN and compare with typical dense trajectory fea-
tures (DTF) with bag-of-visual-words representation. The
results of DTF is provided by [1] 2, which trains three SVM
models with different set of negative samples and aver-
ages AP overall. According to Table 1, our Segment-CNN

2Note that the results reported in [1] use different evaluation metrics. To
make them comparable, we re-evaluate their prediction results according
to standard criteria mentioned in Section 4.2.

achieves tremendous performance gain for “BullCharge-
Cape” action and competitive performance for “HorseRid-
ing” action. Figure 5 displays our prediction results for
“BullChargeCape” and “HorseRiding”, respectively.

AP(%) BullChargeCape HorseRiding mAP
1.7
DTF
7.4
S-CNN

0.3
11.6

3.1
3.1

Table 1. Average precision on MEXaction2. The overlap threshold
is set to 0.5 during evaluation.

Results on THUMOS 20143: The instances in train set
and validation set are used for training. The number of
training iterations is 30K for all three networks. We again
set α = 0.25 for the localization network. We denote our
Segment-CNN using the above settings as S-CNN.

θ
Karaman et al. [17]
Wang et al. [41]
Oneata et al. [27]
S-CNN

0.1
1.5
19.2
39.8
47.7

0.2
0.9
17.8
36.2
43.5

0.3
0.5
14.6
28.8
36.3

0.4
0.3
12.1
21.8
28.7

0.5
0.2
8.5
15.0
19.0

Table 2. Mean average precision on THUMOS 2014 as the overlap
IoU threshold θ used in evaluation varies.

As for comparisons, beyond DTF, several baseline sys-
tems incorporate frame-level deep networks and even utilize
lots of other features: (1) Karaman et al. [17] used FV en-
coding of iDT with weighted saliency based pooling, and
conducted late fusion with frame-level CNN features. (2)
Wang et al. [41] built a system on iDT with FV represen-
tation and frame-level CNN features, and performed post-
processing to reﬁne the detection results. (3) Oneata et al.
[27] conducted localization using FV encoding of iDT on
temporal sliding windows, and performed post-processing
following [25]. Finally, they conducted weighted fusion for
the localization scores of temporal windows and video-level
scores generated by classiﬁers trained on iDT features, im-
age features, and audio features. The results are listed in
Table 2. AP for each class can be found in Figure 4. Our
Segment-CNN signiﬁcantly outperforms other systems for
14 of 20 actions, and the average performance improves
from 15.0% to 19.0%. We also show two prediction results
for the THUMOS 2014 test set in Figure 6.

Efﬁciency analysis. Our approach is very efﬁcient when
compared with all other systems, which typically fuse dif-
ferent features, and therefore can become quite cumber-

3Note that the evaluation toolkit used in THUMOS 2014 has some
bugs, and recently the organizers released a new toolkit with fair evalua-
tion criteria. Here, we re-evaluate the submission results of all teams using
the updated toolkit.

Figure 4. Histogram of average precision (%) for each class on THUMOS 2014 when the overlap threshold is set to 0.5 during evaluation.

some. Most segments generated from sliding windows are
removed by the ﬁrst proposal network, and thus the opera-
tions in classiﬁcation and localization are greatly reduced.
For each batch, the speed is around 1 second, and the num-
ber of segments can be processed during each batch depends
on the GPU memory (approximately 25 for GeForce GTX
980 of 4G memory). The storage requirement is also ex-
tremely small because our method does not need to cache
intermediate high dimensional features, such as FV to train
SVM. All required by Segment-CNN is three deep network
models, which occupy less than 1 GB in total.

4.3. Impact of individual networks

To study the effects of each network individually, we
compare four Segment-CNNs using different settings: (1)
S-CNN: keep all three networks and settings in Section
4.2, and Θloc is ﬁne-tuned on Θcls; (2) S-CNN (w/o pro-
posal): remove the proposal network completely, and di-
rectly use Θloc to do predictions on sliding windows; (3)
S-CNN (w/o classiﬁcation): remove the classiﬁcation net-
work completely and thus do not have Θcls to serve as ini-
tialization for training Θloc; (4) S-CNN (w/o localization):
remove the localization network completely and instead use
classiﬁcation model Θcls to produce predictions.

The proposal network. We compare S-CNN (w/o pro-
posal) and S-CNN, which includes the proposal network as
described above (two nodes in fc8). Because of the smaller
network architecture than S-CNN (w/o proposal), S-CNN
can reduce the number of operations conducted on back-
ground segments, and therefore accelerate speed. In addi-
tion, the results listed in Table 3 demonstrate that keeping
the proposal network can also improve precision because it
is designed for ﬁltering out background segments that lack
action of interests.
The classiﬁcation network. Although Θcls is not used dur-
ing prediction, the classiﬁcation network is still important
because ﬁne-tuning on Θcls results in better performance.
During evaluation here, we perform top-κ selection on the

networks
mAP(%)

S-CNN (w/o proposal)
17.1

S-CNN
19.0

Table 3. mAP comparisons on THUMOS 2014 between remov-
ing the proposal network and keeping the proposal network. The
overlap threshold is set to 0.5 during evaluation.

ﬁnal prediction results to select κ segments with maximum
conﬁdence scores. As shown in Figure 7, S-CNN ﬁne-
tuned on Θcls outperforms S-CNN (w/o classiﬁcation) con-
sistently when κ varies, and consequently the classiﬁcation
network is necessary during training.

Figure 7. Effects of the classiﬁcation and localization networks.
y-axis is mAP(%) on THUMOS 2014, and x-axis varies the depth
κ in top-κ selection. The overlap threshold is set to 0.5 during
evaluation.

The localization network. Figure 7 also proves the ef-
fectiveness of the localization network. By adding the lo-
calization network, S-CNN can signiﬁcantly improve per-
formances compared with the baseline S-CNN (w/o local-
ization), which only contains the proposal and classiﬁca-
tion networks. This is because the new loss function in-
troduced in the localization network reﬁnes the scores in
favoring segments of higher overlap with the ground truths,
and therefore higher temporal localization accuracy can be
achieved.

Figure 5. Prediction results for two action instances from MEXaction2 when the overlap threshold is set to 0.5 during evaluation. For
each ground truth instance, we show two prediction results: A has the highest conﬁdence score among the predictions associated with this
ground truth, and B is an incorrect prediction. BullChargeCape: A is correct, but B is incorrect because each ground truth only allows one
detection. HorseRiding: A is correct, but B is incorrect because each ground truth only allows one detection. The numbers shown with #
are frame IDs.

Figure 6. Prediction results for two action instances from THUMOS 2014 test set when the overlap threshold is set to 0.5 during evaluation.
For each ground truth instance, we show two prediction results: A has the highest conﬁdence score among the predictions associated with
this ground truth, and B is an incorrect prediction. ClearAndJerk: A is correct, but B is incorrect because its overlap IoU with ground truth
is less than threshold 0.5. LongJump: A is correct, but B is incorrect because it has the wrong action category prediction - PoleVault.

In addition, we vary α in the overlap loss term Loverlap of
the loss function to evaluate its sensitivity. We ﬁnd that our
approach has stable performances over a range of α value
(e.g., from 0.25 to 1.0).

5. Conclusion

We propose an effective multi-stage framework called
Segment-CNN to address temporal action localization in
untrimmed long videos. Through the above evaluation for
each network, we demonstrate the contribution from the
proposal network to identify candidate segments, the ne-
cessity of the classiﬁcation network to provide good initial-
ization for training the localization model, and the effec-
tiveness of the new loss function used in the localization
network to precisely localize action instances in time.

In the future, we would like to extend our work to events
and activities, which usually consist of multiple actions,

therefore precisely localizing action instances in time can
be helpful for their recognition and detection.

6. Acknowledgment

This work is supported by the Intelligence Advanced Re-
search Projects Activity (IARPA) via Department of Interior
National Business Center contract number D11PC20071.
The U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwithstanding
any copyright annotation thereon. Disclaimer: The views
and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing
the ofﬁcial policies or endorsements, either expressed or im-
plied, of IARPA, DOI-NBC, or the U.S. Government. We
thank Dong Liu, Guangnan Ye, and anonymous reviewers
for the insightful suggestions.

References

[1] Mexaction2.

http://mexculture.cnam.fr/

xwiki/bin/view/Datasets/Mex+action+
dataset, 2015. 6

[2] J. K. Aggarwal and M. S. Ryoo. Human activity analysis: A

review. In ACM Computing Surveys, 2011. 1, 2

[3] G. Cheng, Y. Wan, A. N. Saudagar, K. Namuduri, and B. P.
Buckles. Advances in human action recognition: A survey.
2015. 1, 2

[4] A. Gaidon, Z. Harchaoui, and C. Schmid. Actom sequence
models for efﬁcient action detection. In CVPR, 2011. 3
[5] A. Gaidon, Z. Harchaoui, and C. Schmid. Temporal local-

ization of actions with actoms. In TPAMI, 2013. 3

[6] R. Girshick. Fast r-cnn. In ICCV, 2015. 1, 3
[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 1, 3

[8] G. Gkioxari and J. Malik. Finding action tubes. In CVPR,

2015. 3

[9] A. Gorban, H. Idrees, Y.-G. Jiang, A. R. Zamir, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Action
recognition with a large number of classes. http://www.
thumos.info/, 2015. 1, 2, 3

[10] M. Jain, J. van Gemert, H. J´egou, P. Bouthemy, and
C. Snoek. Action localization with tubelets from motion.
In CVPR, 2014. 3

[11] M. Jain, J. van Gemert, T. Mensink, and C. Snoek. Ob-
jects2action: Classifying and localizing actions without any
video example. In ICCV, 2015. 3

[12] M. Jain, J. van Gemert, and C. Snoek. What do 15,000 object
categories tell us about classifying and localizing actions? In
CVPR, 2015. 3

[13] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. In TPMAI, 2013. 1,
3

[14] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. In ACM MM, 2014.
6

[15] Y.-G. Jiang, J. Liu, A. R. Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Ac-
tion recognition with a large number of classes. http:
//crcv.ucf.edu/THUMOS14/, 2014. 1, 3, 6

[16] Z. Jiang, Z. Lin, and L. S. Davis. A uniﬁed tree-based frame-
work for joint action localization, recognition and segmenta-
tion. In CVIU, 2013. 3

[17] S. Karaman, L. Seidenari, and A. D. Bimbo. Fast saliency
based pooling of ﬁsher encoded dense trajectories. In ECCV
THUMOS Workshop, 2014. 1, 6

[18] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and F.-F. Li. Large-scale video classiﬁcation with convolu-
tional neural networks. In CVPR, 2014. 1, 3

[19] A. Kl¨aser, M. Marszałek, C. Schmid, and A. Zisserman. Hu-
man focused action localization in video. In Trends and Top-
ics in Computer Vision, 2012. 3

[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 3

Imagenet
In

[21] W. Kuo, B. Hariharan, and J. Malik. Deepbox: Learning
objectness with convolutional networks. In ICCV, 2015. 1, 3
[22] K.-T. Lai, D. Liu, M.-S. Chen, and S.-F. Chang. Recogniz-
ing complex events in videos by learning key static-dynamic
evidences. In ECCV, 2014. 2

[23] K.-T. Lai, F. X. Yu, M.-S. Chen, and S.-F. Chang. Video
In
event detection by inferring temporal instance labels.
CVPR, 2014. 2

[24] Z. Lan, M. Lin, X. Li, A. G. Hauptmann, and B. Raj. Be-
yond gaussian pyramid: Multi-skip feature stacking for ac-
tion recognition. In CVPR, 2015. 1, 2

[25] D. Oneata, J. Verbeek, and C. Schmid. Action and event
recognition with ﬁsher vectors on a compact feature set. In
ICCV, 2013. 1, 2, 6

[26] D. Oneata, J. Verbeek, and C. Schmid. Efﬁcient action lo-
calization with approximately normalized ﬁsher vectors. In
CVPR, 2014. 3

[27] D. Oneata, J. Verbeek, and C. Schmid. The lear submission

at thumos 2014. In ECCV THUMOS Workshop, 2014. 1, 6

[28] R. Poppe. A survey on vision-based human action recogni-

tion. In Image and vision computing, 2010. 1, 2

[29] M. M. Puscas, E. Sangineto, D. Culibrk, and N. Sebe. Un-
supervised tube extraction using transductive learning and
dense trajectories. In ICCV, 2015. 3

[30] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 1, 3

[31] K. Simonyan and A. Zisserman. Two-stream convolutional
networks for action recognition in videos. In NIPS, 2014. 1,
3

[32] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Learning Representations, 2015. 3

[33] K. Soomro, H. Idrees, and M. Shah. Action localization in

videos through context walk. In ICCV, 2015. 3

[34] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset
of 101 human actions classes from videos in the wild.
In
CRCV-TR-12-01, 2012. 3

[35] A. Stoian, M. Ferecatu, J. Benois-Pineau, and M. Crucianu.
Fast action localization in large scale video archives.
In
TCSVT, 2015. 3

[36] C. Sun, S. Shetty, R. Sukthankar, and R. Nevatia. Tempo-
ral localization of ﬁne-grained actions in videos by domain
transfer from web images. In ACM MM, 2015. 2

[37] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In ICCV, 2015. 1, 3, 4, 6

[38] J. van Gemert, M. Jain, E. Gati, and C. Snoek. Apt: Action
In BMVC,

localization proposals from dense trajectories.
2015. 3

[39] H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Action
Recognition by Dense Trajectories. In CVPR, 2011. 1, 2
[40] H. Wang and C. Schmid. Action recognition with improved

trajectories. In ICCV, 2013. 1, 2

[41] L. Wang, Y. Qiao, and X. Tang. Action recognition and de-
tection by combining motion and appearance features.
In
ECCV THUMOS Workshop, 2014. 1, 6

[42] D. Weinland, R. Ronfard, and E. Boyer. A survey of vision-
based methods for action representation, segmentation and
recognition. In Computer Vision and Image Understanding,
2011. 1, 2

[43] P. Weinzaepfel, Z. Harchaoui, and C. Schmid. Learning to
track for spatio-temporal action localization. In ICCV, 2015.
3

[44] Z. Xu, Y. Yang, and A. G. Hauptmann. A discriminative cnn
video representation for event detection. In CVPR, 2015. 1,
3

[45] G. Yu and J. Yuan. Fast action proposals for human action

detection and search. In CVPR, 2015. 3

[46] J. Yuan, Y. Pei, B. Ni, P. Moulin, and A. Kassim. Adsc
submission at thumos challenge 2015. In CVPR THUMOS
Workshop, 2015. 1

Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs

Zheng Shou, Dongang Wang, and Shih-Fu Chang
Columbia University
New York, NY, USA
{zs2262,dw2648,sc250}@columbia.edu

6
1
0
2
 
r
p
A
 
1
2
 
 
]

V
C
.
s
c
[
 
 
2
v
9
2
1
2
0
.
1
0
6
1
:
v
i
X
r
a

Abstract

We address temporal action localization in untrimmed
long videos. This is important because videos in real ap-
plications are usually unconstrained and contain multiple
action instances plus video content of background scenes
or other activities. To address this challenging issue, we
exploit the effectiveness of deep networks in temporal ac-
tion localization via three segment-based 3D ConvNets: (1)
a proposal network identiﬁes candidate segments in a long
video that may contain actions; (2) a classiﬁcation network
learns one-vs-all action classiﬁcation model to serve as ini-
tialization for the localization network; and (3) a localiza-
tion network ﬁne-tunes the learned classiﬁcation network to
localize each action instance. We propose a novel loss func-
tion for the localization network to explicitly consider tem-
poral overlap and achieve high temporal localization accu-
racy. In the end, only the proposal network and the local-
ization network are used during prediction. On two large-
scale benchmarks, our approach achieves signiﬁcantly su-
perior performances compared with other state-of-the-art
systems: mAP increases from 1.7% to 7.4% on MEXaction2
and increases from 15.0% to 19.0% on THUMOS 2014.

1. Introduction

Impressive progress has been reported in recent litera-
ture for action recognition [42, 28, 2, 3, 39, 40, 24, 18, 31,
44, 13, 37]. Besides detecting action in manually trimmed
short video, researchers start to develop techniques for de-
tecting actions in untrimmed long videos in the wild. This
trend motivates another challenging topic - temporal action
localization: given a long untrimmed video, “when does a
speciﬁc action start and end?” This problem is important
because real applications usually involve long untrimmed
videos, which can be highly unconstrained in space and
time, and one video can contain multiple action instances
plus background scenes or other activities. Localizing ac-
tions in long videos, such as those in surveillance, can save
tremendous time and computational costs.

Most state-of-the-art methods rely on manually selected
features, and their performances still require much improve-
ment. For example, top performing approaches in THU-
MOS Challenge 2014 [27, 41, 17, 15] and 2015 [46, 9] both
used improved Dense Trajectory (iDT) with Fisher Vector
(FV) [40, 25]. There have been some recent attempts at in-
corporating iDT features with appearance features automat-
ically extracted by frame-level deep networks [27, 41, 17].
Nevertheless, such 2D ConvNets do not capture motion in-
formation, which is important for modeling actions and de-
termining their temporal boundaries.

As an analogy in still images, object detection recently
achieved large improvements by using deep networks. In-
spired by Region-based Convolutional Neural Networks (R-
CNN) [7] and its upgraded versions [6, 30, 21], we develop
Segment-CNN1, which is an effective deep network frame-
work for temporal action localization as outlined in Figure
1. We adopt 3D ConvNets [13, 37], which recently has
been shown to be promising for capturing motion charac-
teristics in videos, and add a new multi-stage framework.
First, multi-scale segments are generated as candidates for
three deep networks. The proposal network classiﬁes each
segment as either action or background in order to eliminate
background segment estimated to be unlikely to contain ac-
tions of interest. The classiﬁcation network trains typical
one-vs-all classiﬁcation model for all action categories plus
the background.

However, the classiﬁcation network aims at ﬁnding key
evidences to distinguish different categories, rather than lo-
calizing precise action presences in time. Sometimes, the
scores from the classiﬁcation network can be high even
when the segment has only a very small overlap with the
ground truth instance. This can be detrimental because sub-
sequent post-processing steps, such as Non-Maximum Sup-
pression (NMS), might remove segment of small score but
large overlap with ground truth. To explicitly take tempo-
ral overlap into consideration, we introduce the localiza-
tion network based on the same architecture, but this net-

1Source code and trained models are available online at https://

github.com/zhengshou/scnn/.

Figure 1. Overview of our framework. (a) Multi-scale segment generation: given an untrimmed video, we generate segments of varied
lengths via sliding window; (b) Segment-CNN: the proposal network identiﬁes candidate segments, the classiﬁcation network trains an
action recognition model to serve as initialization for the localization network, and the localization network localizes action instances in
time and outputs conﬁdence scores; (c) Post-processing: using the prediction scores from the localization network, we further remove
redundancy by NMS to obtain the ﬁnal results. During training, the classiﬁcation network is ﬁrst learned and then used as initialization for
the localization network. During prediction, only the proposal and localization networks are used.

work uses a novel loss function, which rewards segments
with higher temporal overlap with the ground truths, and
thus can generate conﬁdence scores more suitable for post-
processing. Note that the classiﬁcation network cannot be
replaced by the localization network. We will show later
that using the trained classiﬁcation network (without con-
sidering temporal overlap) to initialize the localization net-
work (take into account temporal overlap) is important, and
achieves better temporal localization accuracies.

To summarize, our main contributions are three-fold:
(1) To the best of our knowledge, our work is the ﬁrst to
exploit 3D ConvNets with multi-stage processes for tempo-
ral action localization in untrimmed long videos in the wild.
(2) We introduce an effective multi-stage Segment-CNN
framework, to propose candidate segments, recognize ac-
tions, and localize temporal boundaries. The proposal net-
work improves the efﬁciency by eliminating unlikely candi-
date segments, and the localization network is key to tem-
poral localization accuracy boosting.

(3) The proposed techniques signiﬁcantly outperform the
state-of-the-art systems over two large-scale benchmarks
suitable for temporal action localization. When the overlap
threshold used in evaluation is set to 0.5, our approach im-
proves mAP on MEXaction2 from 1.7% to 7.4% and mAP
on THUMOS 2014 from 15.0% to 19.0%. We did not eval-
uate on THUMOS Challenge 2015 [9] because the ground

truth is withheld by organizers for future evaluation. More
detailed evaluation results are available in Section 4.

2. Related work

Temporal action localization. This topic has been studied
in two directions. When training data only have video-level
category labels but no temporal annotations, researchers
formulated this as weakly supervised problems or multi-
ple instance learning problems to learn the key evidences
in untrimmed videos and temporally localize actions by se-
lecting key instances [22, 23]. Sun et al. [36] transferred
knowledge from web images to address temporal localiza-
tion in untrimmed web videos.

Another line of work focuses on learning from data when
the temporal boundaries have been annotated for action in-
stances in untrimmed videos, such as THUMOS. Most of
these works pose this as a classiﬁcation problem and adopt
a temporal sliding window approach, where each window
is considered as an action candidate subject to classiﬁca-
tion [25]. Surveys about action classiﬁcation methods can
be found in [42, 28, 2, 3]. Recently, two directions lead
the state-of-the-art: (1) Wang et al. [39] proposed extract-
ing HOG, HOF, MBH features along dense trajectories, and
later on they took camera motion into consideration [40].
Further improvement can be achieved by stacking features
(2) Enlighted by the suc-
with multiple time skips [24].

cess of CNNs in recent works [20, 32], Karpathy et al. [18]
evaluated frame-level CNNs on large-scale video classiﬁ-
cation tasks. Simonyan and Zisserman [31] designed two-
stream CNNs to learn from still image and motion ﬂow re-
In [44], a latent concept descriptor of convo-
spectively.
lutional feature map was proposed, and great results were
achieved on event detection with VLAD encoding. To learn
spatio-temporal features together, the architecture of 3D
ConvNets was explored in [13, 37], achieving competitive
results. Oneata et al. [26] proposed approximately normal-
ized Fisher Vectors to reduce the high dimensionality of FV.
Stoian et al. [35] introduced a two-level cascade to allow
fast search for action instances. Instead of precision, these
methods focus on improving the efﬁciency of conventional
methods. To speciﬁcally address the temporal precision of
action detection, Gaidon et al.
[4, 5] modeled the struc-
ture of action sequence with atomic action units (actoms).
The explicit modeling of action units allows for matching
more complete action unit sequences, rather than just par-
tial content. However, this requires mannual annotations
for actoms, which can be subjective and burdensome. Our
paper presented here aims to solve the same problem of pre-
cise temporal localization, but without requiring the difﬁcult
task of manual annotation for atomic action units.

[10] and Soomro et al.

Spatio-temporal localization. There have been active ex-
plorations about localizing action in space and time simul-
taneously. Jain et al.
[33] built
their work on supervoxel. Recently, researchers treat this
as a tracking problem [43, 8] by leveraging object detectors
[11], especially human detectors [16, 19, 8, 45] to detect re-
gions of interest in each frame and then output sequences
of bounding boxes. Dense trajectories have also been ex-
ploited for extracting the action tubes [29, 38]. Jain et al.
[12] added object encodings to help action localization.

However, this problem is different from temporal local-
ization, which is the main topic in this paper: (1) When
using object detectors to ﬁnd spatio-temporal regions of
interest, such approaches assume that the actions are per-
formed by human or other pre-deﬁned objects. (2) Spatio-
temporal localization requires exhaustive annotations for
objects of interest on every frame as training data. This
makes it overwhelmingly time-consuming particularly for
long untrimmed videos compared with the task of simply
labeling the start time and end time of an action depicted in
the video, which is sufﬁcient to satisfy many applications.

Object detection. Inspired by the success of deep learning
approaches in object detection, we also review R-CNN and
its variations. R-CNN consists of selective search, CNN
feature extraction, SVM classiﬁcation, and bounding box
regression [7]. Fast R-CNN reshapes R-CNN into a single-
stage using multi-task loss, and also has a RoI pooling layer

to share the computation of one image in ConvNets [6].
Our work differs from R-CNN in the following aspects:
(1) Temporal annotations in training videos can be diverse:
some are cleanly trimmed action instances cut out from
long videos, such as UCF101 [34], and some are untrimmed
long videos but with temporal boundaries annotated for ac-
tion instances, such as THUMOS [15, 9]. We provide a
paradigm that can handle such diverse annotations. (2) As
proven in Faster R-CNN [30] which proposes region pro-
posal network, and DeepBox [21] which detects objectness
to re-rank the results of R-CNN, using deep networks for
learning objectness is effective and efﬁcient. Therefore, we
directly use deep network to classify background and action
to obtain candidate segments. (3) We remove the regression
stage because learning regression for time shift and dura-
tion of video segment does not work well in our experi-
ments, probably because actions can be quite diverse, and
therefore do not contain consistent patterns for predicting
start/end time. To achieve precise localization, we design
the localization network using a new loss function to explic-
itly consider temporal overlap. This can decrease the score
for the segment that has small overlap with the ground truth,
and increase the segment of larger overlap. This also ben-
eﬁts post-processing steps, such as NMS, to keep segment
with higher temporal localization accuracy.

3. Detailed descriptions of Segment-CNN

3.1. Problem setup
Problem deﬁnition. We denote a video as X = {xt}T
t=1
where xt is the t-th frame in X, and T is the total number
of frames in X. Each video X is associated with a set of
temporal action annotations Ψ =
,
m=1
where M is the total number of action instances in X, and
km, ψm, ψ
m are, respectively, action category of the in-
stance m and its starting time and ending time (measured
by frame ID). km ∈ {1, . . . , K}, where K is the number
of categories. During training, we have a set T of trimmed
videos and a set U of untrimmed videos. Each trimmed
video X ∈ T has ψm = 1, ψ

m = T , and M = 1.

m, km

ψm, ψ

(cid:17)(cid:111)M

(cid:110)(cid:16)

(cid:48)

(cid:48)

(cid:48)

Multi-scale segment generation. First, each frame is re-
sized to 171 (width) × 128 (height) pixels. For untrimmed
video X ∈ U, we conduct temporal sliding windows of
varied lengths as 16, 32, 64, 128, 256, 512 frames with
75% overlap. For each window, we construct segment
s by uniformly sampling 16 frames. Consequently, for
each untrimmed video X, we generate a set of candidates

(cid:110)(cid:16)

(cid:17)(cid:111)H

(cid:48)
sh, φh, φ
h

Φ =
as input for the proposal network,
where H is the total number of sliding windows for X, and
φm and φ
m are respectively starting time and ending time
of the h-th segment sh. For trimmed video X ∈ T , we di-

h=1

(cid:48)

rectly sample a segment s of 16 frames from X in uniform.

Network architecture. 3D ConvNets conducts 3D convo-
lution/pooling which operates in spatial and temporal di-
mensions simultaneously, and therefore can capture both
appearance and motion for action. Given the competitive
performances on video classiﬁcation tasks, our deep net-
works use 3D ConvNets as the basic architecture in all
stages and follow the network architecture of [37]. All
3D pooling layers use max pooling and have kernel size
of 2×2 in spatial with stride 2, while vary in temporal. All
3D convolutional ﬁlters have kernel size 3 and stride 1 in
all three dimensions. Using the notations conv(number
of ﬁlters) for the 3D convolutional layer, pool(temporal
kernel size, temporal stride) for the 3D pooling layer, and
fc(number of ﬁlters) for the fully connected layer, the lay-
out of these three types of layers in our architecture is
as follows: conv1a(64) - pool1(1,1) - conv2a(128) -
pool2(2,2) - conv3a(256) - conv3b(256) - pool3(2,2) -
conv4a(512) - conv4b(512) - pool4(2,2) - conv5a(512)
- conv5b(512) - pool5(2,2) - fc6(4096) - fc7(4096) -
fc8(K + 1). Each input for this deep network is a segment
s of dimension 171 × 128 × 16. C3D is training this net-
work on Sports-1M train split [37], and we use C3D as the
initialization for our proposal and classiﬁcation networks.

3.2. Training procedure

The proposal network: We train a CNN network Θpro as
the background segment ﬁlter. Basically, fc8 has two nodes
that correspondingly represent the background (rarely con-
tains action of interest) and being-action (has signiﬁcant
portion belongs to the actions of interest).

We use the following strategy to construct training data
Spro = {(sn, kn)}N
n=1, where label kn ∈ {0, 1}. For each
segment of the trimmed video X ∈ T , we set its label
as positive. For candidate segments from an untrimmed
video X ∈ U with temporal annotation Ψ, we assign a
label for each segment by evaluating its Intersection-over-
Union (IoU) with each ground truth instance in Ψ : (1)
if the highest IoU is larger than 0.7, we assign a positive
label; (2) if the highest IoU is smaller than 0.3, we set it
as the background. On the perspective of ground truth, if
there is no segment that overlaps with a ground truth in-
stance with IoU larger than 0.7, then we assign a positive
label segment s if s has the largest IoU with this ground
truth and its IoU is higher than 0.5. At last, we obtain
Spro = {(sn, kn)}Npro
n=1 which consists of all NT +NU posi-
tive segments and Nb ≈ NT +NU randomly sampled back-
ground segments, where Npro = NT + NU + Nb.

In all experiments, we use a learning rate of 0.0001, with
the exception of 0.01 for fc8, momentum of 0.9, weight de-
cay factor of 0.0005, and drop the learning rate by a factor
of 10 for every 10K iterations. The number of total itera-

tions depends on the scale of dataset and will be clariﬁed in
Section 4.

Note that, compared with the multi-class classiﬁcation
network, this proposal network is simpler because the out-
put layer only consists of two nodes (action or background).

The classiﬁcation network: After substantial background
segments are removed by the proposal network, we train a
classiﬁcation model Θcls for K action categories as well as
background.

Preparing the training data Scls follows a similar strategy
for the proposal network. Except when assigning label for
positive segment, the classiﬁcation network explicitly indi-
cates action category km ∈ {1, . . . , K}. Moreover, in order
to balance the number of training data for each class, we re-
duce the number of background instances to Nb ≈ NT +NU
K .
As for parameters in SGD, the learning rate is 0.0001,
with the exception of 0.01 for fc8, momentum is 0.9,
weight decay factor is 0.0005, and the learning rate is di-
vided by a factor of 2 for every 10K iterations, because the
convergence shall be slower when the number of classes in-
creases.

The localization network: As illustrated in Figure 2, it is
important to push up the prediction score of the segment
with larger overlap with the ground truth instance and de-
crease the scores of the segment with smaller overlap, to
make sure that the subsequent post-processing steps can
choose segments with higher overlap over those with small
overlap. Therefore, we propose this localization network
Θloc with a new loss function, which takes IoU with ground
truth instance into consideration.

Training data Sloc for the localization network are aug-
mented from Scls by associating each segment s with the
measurement of overlap, v. In speciﬁc, we set v = 1 for s
from trimmed video. If s comes from untrimmed video and
has positive label k, we set v equal to the overlap (measured
by IoU) of segment s with the associated ground truth in-
stance. If s is a background segment, as we can see later, its

Figure 2. Typical case of bad localizations. Assume that the sys-
tem outputs three predictions: A, B, C. Probably due to that there
are some evidences during [t1, t2], and A has the highest predic-
tion score. Therefore, the NMS will keep A, remove B, and then
keep C. However, actually we hope to remove A and C in NMS,
and keep B because B has the largest IoU with the ground truth
instance.

overlap measurement v will not affect our new loss function
and gradient computation in back-propagation, and thus we
simply set its v as 1.

During each mini-batch, we have N training samples
{(sn, kn, vn)}N
n=1. For the n-th segment, the output vec-
tor of fc8 is On and the prediction score vector after the
softmax layer is Pn. Note that for the i-th class, P (i)
n =

. The new loss function is formed by combining

(i)
n

eO
j=1 eO

(j)
n

(cid:80)N
Lsoftmax and Loverlap :

L = Lsoftmax + λ · Loverlap,

(1)

where λ balances the contribution from each part, and
through empirical validation, we ﬁnd that λ = 1 works well
in practice. Lsoftmax is the conventional softmax loss and is
deﬁned as

Lsoftmax =

(cid:88)

(cid:16)

− log

(cid:16)

P (kn)
n

(cid:17)(cid:17)

,

1
N

n

(2)

which is effective for training deep networks for classiﬁca-
tion. Loverlap is designed to jointly reduce the classiﬁcation
error and adjust the intensity of conﬁdence score according
to the extent of overlap:

Loverlap =

1
N

(cid:88)

n






1
2



(cid:16)

·






(cid:17)2
P (kn)
n

 · [kn > 0]
(vn)α − 1




.

(3)
Here, [kn > 0] is equal to 1 when the true class label kn is
positive, and it is equal to 0 when kn = 0, which means the
sn is a background training sample. Loverlap is intended to
boost the detection scores (P ) of segments that have high
overlaps (v) with ground truth instances, and reduce the
scores of those with small overlaps. The hyper-parameter
α controls the adjustment range for the intensity of the con-
ﬁdence score. The sensitivity of α is explored in Section 4.
In addition, the total gradient w.r.t output of the i-th node in
fc8 is as follows:

∂L
∂O(i)
n

=

∂Lsoftmax
∂O(i)
n

+ λ ·

∂Loverlap
∂O(i)
n

,

(4)

(5)

in which

and

∂Lsoftmax
∂O(i)
n

=

(cid:16)

(cid:40) 1

N ·

(cid:17)

n − 1

P (kn)
N · P (i)
1

n

if i = kn

if i (cid:54)= kn

(cid:18) (P (kn )
n
(vn)α

)2

(cid:16)

·

1
N ·

(cid:17)(cid:19)

1 − P (kn)
n

∂Loverlap
∂O(i)
n

=

(cid:18) (P (kn )
n
(vn)α

)2

1
N ·

(cid:16)

·

−P (i)
n

(cid:17)(cid:19)

· [kn > 0]






· [kn > 0]

if i = kn

.

if i (cid:54)= kn
(6)

Figure 3. An illustration of how Loverlap works compared with
Lsoftmax for each positive segment. Here we use α = 1, λ = 1,
and vary overlap v in Loverlap. The x-axis is the prediction score
at the node that corresponds to true label, and the y-axis is the loss.

Given a training sample (sn, kn, vn), Figure 3 shows
how Loverlap inﬂuences the original softmax loss. It also
provides more concrete insights about the design of this
loss function. (1) If the segment belongs to the background,
Loverlap = 0 and L = Lsoftmax. (2) If the segment is posi-
n = (cid:112)(vn)α, and there-
tive, L reachs the minimum at P (kn)
fore penalizes two cases: either P (kn)
is too small due to
misclassiﬁcation, or P (kn)
explodes and exceeds the learn-
ing target (cid:112)(vn)α which is proportional to overlap vn. Also
note that L is designed to increase as vn decreases, consid-
ering that the training segment with smaller overlap with
ground truth instance is less reliable because it may include
considerable noise. (3) In particular, if this positive segment
has overlap vn = 1, the loss function becomes similar to the
softmax loss, and L gradually decreases from +∞ to 1 as
P (kn)
n

goes from 0 to 1.

n

n

During optimization, Θloc is ﬁne-tuned on Θcls. Because
doing classiﬁcation is also one objective of the localization
network, and a trained classiﬁcation network can be good
initialization. We use the same learning rate, momentum,
and weight decay factor as for the classiﬁcation network.
Other parameters depending on the dataset are indicated in
Section 4.

3.3. Prediction and post-processing

During prediction, we slide varied length temporal win-
dow to generate a set of segments and input them into Θpro
to obtain proposal scores Ppro. In this paper, we keep seg-
ments with Ppro ≥ 0.7. Then we evaluate the retained
segments by Θloc to obtain action category predictions and
conﬁdence scores Ploc. During post-processing, we remove
all segments predicted as the background and reﬁne Ploc by
multiplying with class-speciﬁc frequency of occurrence for
each window length in the training data to leverage win-
dow length distribution patterns. Finally, because redun-
dant detections are not allowed in evaluation, we conduct
NMS based on Ploc to remove redundant detections, and

set the overlap threshold in NMS to a little bit smaller than
the overlap threshold θ in evaluation (θ − 0.1 in this paper).

4. Experiments

4.1. Datasets and setup

MEXaction2 [1]. This dataset contains two action classes:
“BullChargeCape” and “HorseRiding”. This dataset con-
INA videos, YouTube clips, and
sists of three subsets:
UCF101 Horse Riding clips. YouTube clips and UCF101
Horse Riding clips are trimmed, whereas INA videos are
untrimmed and are approximately 77 hours in total. With
regard to action instances with temporal annotation, they are
divided into train set (1336 instances), validation set (310
instances), and test set (329 instances).

THUMOS 2014 [15]. The temporal action detection task in
THUMOS Challenge 2014 is dedicated to localizing action
instances in long untrimmed videos. The detection task in-
volves 20 categories as indicated in Figure 4. The trimmed
videos used for training are 2755 videos of these 20 actions
in UCF101. The validation set contains 1010 untrimmed
videos with temporal annotations of 3007 instances in to-
tal. The test set contains 3358 action instances from 1574
untrimmed videos, whereas only 213 of them contain ac-
tion instances of interest. We exclude the remaining 1361
background videos in the test set.

4.2. Comparison with state-of-the-art systems

Evaluation metrics. We follow the conventional met-
rics used in THUMOS Challenge to regard temporal ac-
tion localization as a retrieval problem, and evaluate aver-
age precision (AP). A prediction is marked as correct only
when it has the correct category prediction, and has IoU
with ground truth instance larger than the overlap threshold
(measured by IoU). Note that redundant detections are not
allowed.

Results on MEXaction2. We build our system based on
Caffe [14] and C3D [37]. We use the train set in MEX-
action2 for training. The number of training iterations is
30K for the proposal network, 20K for the classiﬁcation net-
work, and 20K in the localization network with α = 0.25.

We denote our Segment-CNN using the above settings
as S-CNN and compare with typical dense trajectory fea-
tures (DTF) with bag-of-visual-words representation. The
results of DTF is provided by [1] 2, which trains three SVM
models with different set of negative samples and aver-
ages AP overall. According to Table 1, our Segment-CNN

2Note that the results reported in [1] use different evaluation metrics. To
make them comparable, we re-evaluate their prediction results according
to standard criteria mentioned in Section 4.2.

achieves tremendous performance gain for “BullCharge-
Cape” action and competitive performance for “HorseRid-
ing” action. Figure 5 displays our prediction results for
“BullChargeCape” and “HorseRiding”, respectively.

AP(%) BullChargeCape HorseRiding mAP
1.7
DTF
7.4
S-CNN

0.3
11.6

3.1
3.1

Table 1. Average precision on MEXaction2. The overlap threshold
is set to 0.5 during evaluation.

Results on THUMOS 20143: The instances in train set
and validation set are used for training. The number of
training iterations is 30K for all three networks. We again
set α = 0.25 for the localization network. We denote our
Segment-CNN using the above settings as S-CNN.

θ
Karaman et al. [17]
Wang et al. [41]
Oneata et al. [27]
S-CNN

0.1
1.5
19.2
39.8
47.7

0.2
0.9
17.8
36.2
43.5

0.3
0.5
14.6
28.8
36.3

0.4
0.3
12.1
21.8
28.7

0.5
0.2
8.5
15.0
19.0

Table 2. Mean average precision on THUMOS 2014 as the overlap
IoU threshold θ used in evaluation varies.

As for comparisons, beyond DTF, several baseline sys-
tems incorporate frame-level deep networks and even utilize
lots of other features: (1) Karaman et al. [17] used FV en-
coding of iDT with weighted saliency based pooling, and
conducted late fusion with frame-level CNN features. (2)
Wang et al. [41] built a system on iDT with FV represen-
tation and frame-level CNN features, and performed post-
processing to reﬁne the detection results. (3) Oneata et al.
[27] conducted localization using FV encoding of iDT on
temporal sliding windows, and performed post-processing
following [25]. Finally, they conducted weighted fusion for
the localization scores of temporal windows and video-level
scores generated by classiﬁers trained on iDT features, im-
age features, and audio features. The results are listed in
Table 2. AP for each class can be found in Figure 4. Our
Segment-CNN signiﬁcantly outperforms other systems for
14 of 20 actions, and the average performance improves
from 15.0% to 19.0%. We also show two prediction results
for the THUMOS 2014 test set in Figure 6.

Efﬁciency analysis. Our approach is very efﬁcient when
compared with all other systems, which typically fuse dif-
ferent features, and therefore can become quite cumber-

3Note that the evaluation toolkit used in THUMOS 2014 has some
bugs, and recently the organizers released a new toolkit with fair evalua-
tion criteria. Here, we re-evaluate the submission results of all teams using
the updated toolkit.

Figure 4. Histogram of average precision (%) for each class on THUMOS 2014 when the overlap threshold is set to 0.5 during evaluation.

some. Most segments generated from sliding windows are
removed by the ﬁrst proposal network, and thus the opera-
tions in classiﬁcation and localization are greatly reduced.
For each batch, the speed is around 1 second, and the num-
ber of segments can be processed during each batch depends
on the GPU memory (approximately 25 for GeForce GTX
980 of 4G memory). The storage requirement is also ex-
tremely small because our method does not need to cache
intermediate high dimensional features, such as FV to train
SVM. All required by Segment-CNN is three deep network
models, which occupy less than 1 GB in total.

4.3. Impact of individual networks

To study the effects of each network individually, we
compare four Segment-CNNs using different settings: (1)
S-CNN: keep all three networks and settings in Section
4.2, and Θloc is ﬁne-tuned on Θcls; (2) S-CNN (w/o pro-
posal): remove the proposal network completely, and di-
rectly use Θloc to do predictions on sliding windows; (3)
S-CNN (w/o classiﬁcation): remove the classiﬁcation net-
work completely and thus do not have Θcls to serve as ini-
tialization for training Θloc; (4) S-CNN (w/o localization):
remove the localization network completely and instead use
classiﬁcation model Θcls to produce predictions.

The proposal network. We compare S-CNN (w/o pro-
posal) and S-CNN, which includes the proposal network as
described above (two nodes in fc8). Because of the smaller
network architecture than S-CNN (w/o proposal), S-CNN
can reduce the number of operations conducted on back-
ground segments, and therefore accelerate speed. In addi-
tion, the results listed in Table 3 demonstrate that keeping
the proposal network can also improve precision because it
is designed for ﬁltering out background segments that lack
action of interests.
The classiﬁcation network. Although Θcls is not used dur-
ing prediction, the classiﬁcation network is still important
because ﬁne-tuning on Θcls results in better performance.
During evaluation here, we perform top-κ selection on the

networks
mAP(%)

S-CNN (w/o proposal)
17.1

S-CNN
19.0

Table 3. mAP comparisons on THUMOS 2014 between remov-
ing the proposal network and keeping the proposal network. The
overlap threshold is set to 0.5 during evaluation.

ﬁnal prediction results to select κ segments with maximum
conﬁdence scores. As shown in Figure 7, S-CNN ﬁne-
tuned on Θcls outperforms S-CNN (w/o classiﬁcation) con-
sistently when κ varies, and consequently the classiﬁcation
network is necessary during training.

Figure 7. Effects of the classiﬁcation and localization networks.
y-axis is mAP(%) on THUMOS 2014, and x-axis varies the depth
κ in top-κ selection. The overlap threshold is set to 0.5 during
evaluation.

The localization network. Figure 7 also proves the ef-
fectiveness of the localization network. By adding the lo-
calization network, S-CNN can signiﬁcantly improve per-
formances compared with the baseline S-CNN (w/o local-
ization), which only contains the proposal and classiﬁca-
tion networks. This is because the new loss function in-
troduced in the localization network reﬁnes the scores in
favoring segments of higher overlap with the ground truths,
and therefore higher temporal localization accuracy can be
achieved.

Figure 5. Prediction results for two action instances from MEXaction2 when the overlap threshold is set to 0.5 during evaluation. For
each ground truth instance, we show two prediction results: A has the highest conﬁdence score among the predictions associated with this
ground truth, and B is an incorrect prediction. BullChargeCape: A is correct, but B is incorrect because each ground truth only allows one
detection. HorseRiding: A is correct, but B is incorrect because each ground truth only allows one detection. The numbers shown with #
are frame IDs.

Figure 6. Prediction results for two action instances from THUMOS 2014 test set when the overlap threshold is set to 0.5 during evaluation.
For each ground truth instance, we show two prediction results: A has the highest conﬁdence score among the predictions associated with
this ground truth, and B is an incorrect prediction. ClearAndJerk: A is correct, but B is incorrect because its overlap IoU with ground truth
is less than threshold 0.5. LongJump: A is correct, but B is incorrect because it has the wrong action category prediction - PoleVault.

In addition, we vary α in the overlap loss term Loverlap of
the loss function to evaluate its sensitivity. We ﬁnd that our
approach has stable performances over a range of α value
(e.g., from 0.25 to 1.0).

5. Conclusion

We propose an effective multi-stage framework called
Segment-CNN to address temporal action localization in
untrimmed long videos. Through the above evaluation for
each network, we demonstrate the contribution from the
proposal network to identify candidate segments, the ne-
cessity of the classiﬁcation network to provide good initial-
ization for training the localization model, and the effec-
tiveness of the new loss function used in the localization
network to precisely localize action instances in time.

In the future, we would like to extend our work to events
and activities, which usually consist of multiple actions,

therefore precisely localizing action instances in time can
be helpful for their recognition and detection.

6. Acknowledgment

This work is supported by the Intelligence Advanced Re-
search Projects Activity (IARPA) via Department of Interior
National Business Center contract number D11PC20071.
The U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwithstanding
any copyright annotation thereon. Disclaimer: The views
and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing
the ofﬁcial policies or endorsements, either expressed or im-
plied, of IARPA, DOI-NBC, or the U.S. Government. We
thank Dong Liu, Guangnan Ye, and anonymous reviewers
for the insightful suggestions.

References

[1] Mexaction2.

http://mexculture.cnam.fr/

xwiki/bin/view/Datasets/Mex+action+
dataset, 2015. 6

[2] J. K. Aggarwal and M. S. Ryoo. Human activity analysis: A

review. In ACM Computing Surveys, 2011. 1, 2

[3] G. Cheng, Y. Wan, A. N. Saudagar, K. Namuduri, and B. P.
Buckles. Advances in human action recognition: A survey.
2015. 1, 2

[4] A. Gaidon, Z. Harchaoui, and C. Schmid. Actom sequence
models for efﬁcient action detection. In CVPR, 2011. 3
[5] A. Gaidon, Z. Harchaoui, and C. Schmid. Temporal local-

ization of actions with actoms. In TPAMI, 2013. 3

[6] R. Girshick. Fast r-cnn. In ICCV, 2015. 1, 3
[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 1, 3

[8] G. Gkioxari and J. Malik. Finding action tubes. In CVPR,

2015. 3

[9] A. Gorban, H. Idrees, Y.-G. Jiang, A. R. Zamir, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Action
recognition with a large number of classes. http://www.
thumos.info/, 2015. 1, 2, 3

[10] M. Jain, J. van Gemert, H. J´egou, P. Bouthemy, and
C. Snoek. Action localization with tubelets from motion.
In CVPR, 2014. 3

[11] M. Jain, J. van Gemert, T. Mensink, and C. Snoek. Ob-
jects2action: Classifying and localizing actions without any
video example. In ICCV, 2015. 3

[12] M. Jain, J. van Gemert, and C. Snoek. What do 15,000 object
categories tell us about classifying and localizing actions? In
CVPR, 2015. 3

[13] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. In TPMAI, 2013. 1,
3

[14] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. In ACM MM, 2014.
6

[15] Y.-G. Jiang, J. Liu, A. R. Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Ac-
tion recognition with a large number of classes. http:
//crcv.ucf.edu/THUMOS14/, 2014. 1, 3, 6

[16] Z. Jiang, Z. Lin, and L. S. Davis. A uniﬁed tree-based frame-
work for joint action localization, recognition and segmenta-
tion. In CVIU, 2013. 3

[17] S. Karaman, L. Seidenari, and A. D. Bimbo. Fast saliency
based pooling of ﬁsher encoded dense trajectories. In ECCV
THUMOS Workshop, 2014. 1, 6

[18] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and F.-F. Li. Large-scale video classiﬁcation with convolu-
tional neural networks. In CVPR, 2014. 1, 3

[19] A. Kl¨aser, M. Marszałek, C. Schmid, and A. Zisserman. Hu-
man focused action localization in video. In Trends and Top-
ics in Computer Vision, 2012. 3

[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 3

Imagenet
In

[21] W. Kuo, B. Hariharan, and J. Malik. Deepbox: Learning
objectness with convolutional networks. In ICCV, 2015. 1, 3
[22] K.-T. Lai, D. Liu, M.-S. Chen, and S.-F. Chang. Recogniz-
ing complex events in videos by learning key static-dynamic
evidences. In ECCV, 2014. 2

[23] K.-T. Lai, F. X. Yu, M.-S. Chen, and S.-F. Chang. Video
In
event detection by inferring temporal instance labels.
CVPR, 2014. 2

[24] Z. Lan, M. Lin, X. Li, A. G. Hauptmann, and B. Raj. Be-
yond gaussian pyramid: Multi-skip feature stacking for ac-
tion recognition. In CVPR, 2015. 1, 2

[25] D. Oneata, J. Verbeek, and C. Schmid. Action and event
recognition with ﬁsher vectors on a compact feature set. In
ICCV, 2013. 1, 2, 6

[26] D. Oneata, J. Verbeek, and C. Schmid. Efﬁcient action lo-
calization with approximately normalized ﬁsher vectors. In
CVPR, 2014. 3

[27] D. Oneata, J. Verbeek, and C. Schmid. The lear submission

at thumos 2014. In ECCV THUMOS Workshop, 2014. 1, 6

[28] R. Poppe. A survey on vision-based human action recogni-

tion. In Image and vision computing, 2010. 1, 2

[29] M. M. Puscas, E. Sangineto, D. Culibrk, and N. Sebe. Un-
supervised tube extraction using transductive learning and
dense trajectories. In ICCV, 2015. 3

[30] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 1, 3

[31] K. Simonyan and A. Zisserman. Two-stream convolutional
networks for action recognition in videos. In NIPS, 2014. 1,
3

[32] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Learning Representations, 2015. 3

[33] K. Soomro, H. Idrees, and M. Shah. Action localization in

videos through context walk. In ICCV, 2015. 3

[34] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset
of 101 human actions classes from videos in the wild.
In
CRCV-TR-12-01, 2012. 3

[35] A. Stoian, M. Ferecatu, J. Benois-Pineau, and M. Crucianu.
Fast action localization in large scale video archives.
In
TCSVT, 2015. 3

[36] C. Sun, S. Shetty, R. Sukthankar, and R. Nevatia. Tempo-
ral localization of ﬁne-grained actions in videos by domain
transfer from web images. In ACM MM, 2015. 2

[37] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In ICCV, 2015. 1, 3, 4, 6

[38] J. van Gemert, M. Jain, E. Gati, and C. Snoek. Apt: Action
In BMVC,

localization proposals from dense trajectories.
2015. 3

[39] H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Action
Recognition by Dense Trajectories. In CVPR, 2011. 1, 2
[40] H. Wang and C. Schmid. Action recognition with improved

trajectories. In ICCV, 2013. 1, 2

[41] L. Wang, Y. Qiao, and X. Tang. Action recognition and de-
tection by combining motion and appearance features.
In
ECCV THUMOS Workshop, 2014. 1, 6

[42] D. Weinland, R. Ronfard, and E. Boyer. A survey of vision-
based methods for action representation, segmentation and
recognition. In Computer Vision and Image Understanding,
2011. 1, 2

[43] P. Weinzaepfel, Z. Harchaoui, and C. Schmid. Learning to
track for spatio-temporal action localization. In ICCV, 2015.
3

[44] Z. Xu, Y. Yang, and A. G. Hauptmann. A discriminative cnn
video representation for event detection. In CVPR, 2015. 1,
3

[45] G. Yu and J. Yuan. Fast action proposals for human action

detection and search. In CVPR, 2015. 3

[46] J. Yuan, Y. Pei, B. Ni, P. Moulin, and A. Kassim. Adsc
submission at thumos challenge 2015. In CVPR THUMOS
Workshop, 2015. 1

