Learning with Good Feature Representations in Bandits and in RL
with a Generative Model

Tor Lattimore 1 Csaba Szepesv´ari 2 3 Gell´ert Weisz 1

0
2
0
2
 
b
e
F
 
9
1

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
7
6
7
0
.
1
1
9
1
:
v
i
X
r
a

Abstract

The construction by Du et al. (2019) implies
that even if a learner is given linear features
in Rd that approximate the rewards in a
bandit with a uniform error of ε,
then
searching for an action that
is optimal
up to O(ε) requires examining essentially
all actions. We use the Kiefer–Wolfowitz
theorem to prove a positive result that by
checking only a few actions, a learner can
always ﬁnd an action that is suboptimal
with an error of at most O(ε√d). Thus,
features are useful when the approximation
error is small relative to the dimensionality
of the features. The idea is applied to
stochastic bandits and reinforcement learning
with a generative model where the learner
has access to d-dimensional linear features
that approximate the action-value functions
for all policies to an accuracy of ε. For linear
bandits, we prove a bound on the regret of
dn log(k) + εn√d log(n) with k the
order
number of actions and n the horizon. For RL
we show that approximate policy iteration
can learn a policy that is optimal up to an
γ)2 and using
additive error of order ε√d/(1
d/(ε2(1
γ)4) samples from a generative
model. These bounds are independent of
the ﬁner details of the features. We also
investigate how the structure of the feature
set impacts the tradeoﬀ between sample
complexity and estimation error.

p

−

−

1. Introduction

Du et al. (2019) ask whether “good feature represen-
tation” is suﬃcient for eﬃcient reinforcement learning

1DeepMind,

Edmonton
3University of Alberta. Correspondence to: Tor Lat-
timore <lattimore@google.com>.

London

2DeepMind,

and suggest a negative answer. Eﬃciency here means
learning a good policy with a small number of inter-
actions either with the environment (on-line learning),
or with a simulator (planning). A linear feature repre-
sentation is called “good” if it approximates the value
functions of all policies with a small uniform error.
The same question can also be asked for learning in
bandits. The ideas by Du et al. (2019) suggest that
the answer is also negative in ﬁnite-armed bandits with
a misspeciﬁed linear model. All is not lost, however.
By relaxing the objective, we will show that one can
obtain positive results showing that eﬃcient learning
is possible in interactive settings with good feature rep-
resentations.

The rest of this article is organised as follows. First
learning to identify
we introduce the problem of
a near-optimal action with side information about
the possible reward (Section 2). We then adapt
the argument of Du et al. (2019) to show that no
algorithm can ﬁnd an O(ε)-optimal action without
examining nearly all the actions, even when the
rewards lie within an ε-vicinity of a subspace spanned
by some features available to the algorithm (Section 3).
The negative result is complemented by a positive
result showing that there exists an algorithm such that
for any feature map of dimension d, the algorithm
is able to ﬁnd an action with suboptimality gap of
at most O(ε√d) where ε is the maximum distance
between the reward and the subspace spanned by the
features in the max-norm. The algorithm only needs
to investigate the reward at O(d log log d) well-chosen
actions. The main idea is to use the Kiefer-Wolfowitz
theorem with a least-squares estimator of the reward
function. Finally, we apply the idea to stochastic
bandits (Section 5) and reinforcement learning with
a generative model (Section 6).

Related work Despite its importance, the problem
of
identifying near-optimal actions when rewards
follow a misspeciﬁed linear model has only recently
received attention. Of course, there is the recent paper
by Du et al. (2019), whose negative result inspired this
work and is summarised for our setting in Section 3.

Learning with Good Feature Representations in Bandits and in RL with a Generative Model

A contemporaneous work also addressing the issues
raised by Du et al. (2019) is by Van Roy and Dong
(2019), who make a connection to the Eluder
dimension (Russo and Van Roy, 2013) and prove a
variation on our Proposition 4.2. The setting studied
here in Section 3 is closely related to the query
complexity of exactly maximising a function in a
given function class, which was studied by Amin et al.
(2011). They introduced the haystack dimension
as a hardness measure for exact maximisation.
Unfortunately, their results for inﬁnite classes are
generally not tight and no results for misspeciﬁed
linear models were provided. Another related area is
pure exploration in bandits, which was popularised in
the machine learning community by Even-Dar et al.
(2006); Audibert and Bubeck (2010). The standard
problem is to identify a (near)-optimal action in an
unstructured bandit. Soare et al. (2014) study pure
exploration in linear bandits, but do not address
the case where the model
is misspeciﬁed. More
general structured settings have also been considered
by Degenne et al. (2019) and others. The algorithms
in these works begin by playing every action once,
which is inconsequential in an asymptotic sense. Our
focus, however, is on the ﬁnite-time regime where the
number of actions is very large, which makes these
algorithms unusable. We discuss the related work on
linear bandits and RL in the relevant sections later.

Rn

×

∈

m, the set of rows
Notation Given a matrix A
is denoted by rows(A) and its range is range(A) =
Rm
. When A is positive semi-deﬁnite,
Aθ : θ
{
2
A = x⊤Ax. The Minkowski sum of sets
x
we deﬁne
k
U, V
. The
standard basis vectors are e1, . . . , ed. There will never
be ambiguity about deducing the dimension.

}
Rd is U + V =

u + v : u

∈
k

U, v

⊂

∈

∈

V

{

}

2. Problem setup

∈

We start with an abstract problem that is reminiscent
of pure exploration in bandits, but without noise. Fix
δ > 0 and consider the problem of identifying a δ-
optimal action out of k actions with the additional
Rk
information that the unknown reward vector µ
∈
Rk. An action
belongs to a known hypothesis set
H ⊂
is δ-optimal for µ = (µi)k
j
i=1 if
1, . . . , k
[k] =
δ. The learner sequentially queries
µj > maxi µi −
actions i
[k] and observes the reward µi. At some
point the learner should stop and output both an
estimated optimal action ˆa
[k] and an estimation
Rk. There is no noise, so the learner has no
vector ˆµ
reason to query the same action twice. Of course, if
the learner queries all the actions, then it knows both
µ and the optimal action. The learner is permitted to

∈

∈

∈

}

{

randomise. Two objectives are considered. The ﬁrst
only measures the quality of the outputted action ˆa,
while the second depends on ˆµ.

Deﬁnition 2.1. A learner is called sound for (
< δ almost surely for all µ
ˆµ

µ

, δ) if
H
. It is called
δ almost surely

∈ H

, δ) if µˆa > maxa µa −

H

−

k∞
k
max-sound for (
for all µ

.

∈ H

Denote by qδ(A , µ) the expected number of queries
learner A executes when interacting with the
environment speciﬁed by µ and let

cmax
δ

) =

(
H

cest
δ (

) =

H

A :A is (

inf
,δ)-max-sound

H
inf
H

A :A is (

,δ)-sound

sup
µ

∈H

qδ(A , µ)

sup
µ
∈H
qδ(A , µ) ,

which are the minimax query complexities for max-
sound/sound learners respectively when interacting
with reward vectors in
and with error tolerance
H
δ. Both complexity measures are increasing as the
hypothesis class is larger in the sense that if U
V ,
⊂
(V ), and similarly for cest
cmax
then cmax
δ . If a
(U )
δ
, δ) and ˆa = arg max ˆµ, then
learner is sound for (
clearly it is also max-sound for (
, 2δ), which shows
that

H

H

≤

δ

cmax
2δ (

)

H

≤

cest
δ (

) .

H

(1)

δ

(
H

(
H

). Furthermore,

Our primary interest is to understand cmax
). Upper
bounds, however, will be proven using Eq. (1) and
by controlling cest
in Section 4
δ (
H
we provide a simple characterisation of cest
), while
δ (
cmax
) is apparently more subtle. Later we need
δ
the following intuitive result, which says that the
complexity of ﬁnding a near-optimal action when the
hypothesis set consists of the unit vectors is linear in
the number of actions. The proof is given in Section A.
Lemma 2.2. cmax
e1, . . . , ek}
It follows from the aforementioned monotonicity that
if

) = (k + 1)/2.

, then cmax

(k + 1)/2.

(
{

H

)

1

e1, . . . , ek} ⊆ H

{

1

(
H

≥

3. Negative result

Rk

×

∈

d. The rows of Φ should be thought of
Let Φ
as feature vectors assigned to each of the k actions;
accordingly we call Φ a feature matrix. Furthermore,
rows(Φ), we abuse notation by
when µ
writing µa for the value of vector µ at the index of row
a in Φ. Our interest lies in the regime where k is much
larger than d and where exp(d) is large.

Rk and a

∈

∈

We consider hypothesis classes where the true reward
lies within an ε-vicinity of range(Φ) as measured in
(ε), where
max-norm. Let

ε
Φ = range(Φ) + B

∞

H

Learning with Good Feature Representations in Bandits and in RL with a Generative Model

δ

∞

ε, ε]k is a k-dimensional hypercube. How
B
(ε) = [
−
large is cmax
ε
Φ) for diﬀerent regimes of δ and ε and
(
H
feature matrices? As we shall see, for δ = Ω(ε√d)
one can keep the complexity small, while for smaller δ
there exist feature matrices for which the complexity
can be as high as the large dimension, k.

The latter result follows from the core argument of
the recent paper by Du et al. (2019). The next
lemma is the key tool, and is a consequence of the
Johnson–Lindenstrauss lemma.
It shows that there
d with k much larger than d
exist matrices Φ
where rows have unit length and all non-equal rows
are almost orthogonal.

Rk

∈

×

Lemma 3.1. For any ε > 0 and d
8 log(k)/ε2
⌈
with unique rows such that for all a, b
a

= b,

ε.

∈

a

⌉

k2 = 1 and

a⊤b
|

| ≤

k

, there exists a feature matrix Φ

[k] such that d
Rk

≥
d
×
∈
rows(Φ) with

∈

Lemmas 2.2 and 3.1 together imply the promised
result:

Proposition 3.2. For any ε > 0 and d
8 log(k)/ε2
⌈
such that cmax

, there exists a feature matrix Φ

(k + 1)/2.

∈

⌉

[k] with d
Rk

∈

≥
d
×

1

ε
Φ)

(
H

≥

Proof. Let Φ be the matrix from Lemma 3.1 with
rows(Φ) = (ai)k
ε
i=1. We want to show that ei ∈ H
Φ
for all i
[k] and then apply Lemma 2.2. If θ = ai,
then Φθ = (a⊤1 ai, . . . , a⊤i ai, . . . , a⊤k ai)⊤. By the choice
of Φ the ith component is one and the others are all
less than ε in absolute value. Hence,
ε,
which completes the proof.

eik∞ ≤

Φθ

−

∈

k

The proposition has a worst-case ﬂavour. Not all
feature matrices have a high query complexity. For
a silly example, the query complexity of the zero
matrix Φ = 0 satisﬁes cmax
ε
Φ) = 0 provided
ε < 1.
the matrix witnessing the
claims in Lemma 3.1 can be found with non-negligible
probability by sampling the rows of Φ uniformly from
1)-dimensional sphere. There is
the surface of the (d
another way of writing this result, emphasising the role
of the dimension rather than the number of actions.

That said,

(
H

−

1

Corollary 3.3. For all δ > ε, there exists a feature
matrix Φ

d with suitably large k such that

Rk

×

∈

cmax
δ

ε
Φ)

(
H

≥

1
2

exp

2

d

1

−
8

ε
δ

.

(cid:18)

(cid:16)

(cid:17)

(cid:19)

The proof
Proposition 3.2 and is given in Appendix B.

follows by rescaling the features

in

4. Positive result

the previous

The negative result of
section is
complemented with a positive result showing that the
query complexity can be bounded independently of k
whenever δ = Ω(ε√d). For the remainder of the article
we make the following assumption:
Assumption 4.1. Φ
×
span of rows(Φ) is all of Rd.

d has unique rows and the

Rk

∈

We discuss the relationship between this result and
Proposition 3.2 at the end of the section.
Proposition 4.2. Let Φ
Then, cmax
(
H

∈
4d log log d + 16.

d and δ > 2ε(1 + √2d).

ε
Φ)

Rk

≤

×

δ

The proof relies on the Kiefer–Wolfowitz theorem,
which we now recall. Given a probability distribution
R be
ρ : rows(Φ)
given by

[0, 1], let G(ρ)

d and g(ρ)

Rd

→

∈

∈

×

G(ρ) =

ρ(a)aa⊤ ,

rows(Φ)

Xa
∈

g(ρ) = max
a

rows(Φ) k

a

k

∈

2
G(ρ)−1 .

Theorem 4.3 (Kiefer and Wolfowitz 1960). The
following are equivalent:

1. ρ∗ is a minimiser of g.
2. ρ∗ is a maximiser of f (ρ) = log det G(ρ).
3. g(ρ∗) = d.

Furthermore, there exists a minimiser ρ∗ of g such that
the support of ρ∗ has cardinality at most
d(d + 1)/2.

supp(ρ)

| ≤

|

The distribution ρ∗ is called an (optimal) experimental
design and the elements of its support are called its
core set.
Intuitively, when covariates are sampled
from ρ, then g(ρ) is proportional to the maximum
variance of the corresponding least-squares estimator
over all directions in rows(Φ). Hence, minimising g
corresponds to minimising the worst-case variance of
the resulting least-squares estimator. A geometric
interpretation is that the core set lies on the boundary
of the central ellipsoid of minimum volume that
contains rows(Φ). The next theorem shows that there
exists a near-optimal design with a small core set. The
proof follows immediately from part (ii) of lemma 3.9
in the book by Todd (2016), which also provides an
algorithm for computing such a distribution in roughly
order kd2 computation steps.
Theorem 4.4. There exists a probability distribution
ρ such that g(ρ)
2d and the core set of ρ has size at
most 4d log log(d) + 16.

≤

The proof of Proposition 4.2 is a corollary of the
following more general result about least-squares
estimators over near-optimal designs.

Learning with Good Feature Representations in Bandits and in RL with a Generative Model

β, β]k.
Proposition 4.5. Let µ
Suppose that ρ is a probability distribution over
rows(Φ) satisfying the conclusions of Theorem 4.4.
Then

ε + (ε + β)√2d, where

ε
Φ and η

∈ H

[
−

Φˆθ

∈

µ

k

−
k∞ ≤
ˆθ = G(ρ)−

1

ρ(a)(µa + ηa)a .

rows(Φ)

Xa
∈

The problem can be reduced to the case where η = 0
ε+β
Φ The only disadvantage
by noting that µ + η
is that this leads to an additional additive dependence
on β.

∈ H

Proof. Let µ = Φθ + ∆ where
k∞ ≤
diﬀerence between ˆθ and θ can be written as

∆

k

ε. The

θ = G(ρ)−

1

ˆθ

−

ρ(a)a

a⊤θ + ∆a + ηa

rows(Φ)

Xa
∈

(cid:0)

= G(ρ)−

1

ρ(a)(∆a + ηa)a .

θ

−

(cid:1)

Next, for any b

rows(Φ)

Xa
∈
rows(Φ),

∈

b, ˆθ
h

θ

i

−

=

*

b, G(ρ)−

1

ρ(a)(∆a + ηa)a

+

=

≤

≤

rows(Φ)

Xa
∈
(ε + β)

(ε + β)

rows(Φ)

Xa
∈
b, G(ρ)−
ρ(a)(∆a + ηa)
h

1a

i

ρ(a)

b, G(ρ)−

1a

|h

i|

b, G(ρ)−
ρ(a)
h

1a

2
i

rows(Φ)

Xa
∈

s Xa
∈

rows(Φ)

= (ε + β)

ρ(a)b⊤G(ρ)−

1aa⊤G(ρ)−

1b

= (ε + β)

(ε + β)

g(ρ)

(ε + β)√2d ,

rows(Φ)

s Xa
∈
b

2
G(ρ)−1 ≤

k

k

q

≤

p

inequality follows

from H¨older’s
where the ﬁrst
inequality and the fact that
ε, the second
by Jensen’s inequality and the last two by our choice
of ρ and Theorem 4.4. Therefore

k∞ ≤

∆

k

b, ˆθ
h

i ≤ h

i

b, θ

+ (ε + β)√2d

µb + ε + (ε + β)√2d .

≤

A symmetrical argument completes the proof.

Proof of Proposition 4.2. Let ρ be a probability dis-
tribution over rows(Φ) satisfying the conclusions of
Theorem 4.4. Consider the algorithm that evaluates
µ on each point of the support of ρ and computes
the least-squares estimator deﬁned in Proposition 4.5
a, ˆθ
. Let a∗ =
and predicts ˆa = arg maxa
i
∈

rows(Φ)h

arg maxa
∈
Proposition 4.5 with η = 0,

rows(Φ) µa be the optimal action. Then by

ˆa, ˆθ

µˆa ≥ h

i −
2ε

−

µa∗

≥

ε

1 + √2d
(cid:17)

(cid:16)
1 + √2d

≥ h
> µa∗

a∗, ˆθ

i −
δ .

ε

1 + √2d
(cid:17)
(cid:16)

−

(cid:16)

(cid:17)
Discussion Corollary 3.3 shows that the query
complexity is exponential in d when δ is not much
larger than ε, but is benign when δ = Ω(ε√d). The
positive result shows that in the latter regime the
complexity is more or less linear in d. Precisely,

min

δ : cmax
δ
{

(
H

ε
Φ)

≤

4d log log(d) + 16

= O(ε√d) .

}

The message is that there is a sharp tradeoﬀ between
query complexity and error. The learner pays dearly
in terms of query complexity if they demand an
estimation error that is close to the approximation
error. By sacriﬁcing a factor of √d in estimation error,
the query complexity is practically just linear in d.

Comparison to supervised learning As noted by
Du et al. (2019), the negative result does not hold in
supervised learning, where the learner is judged on
its average prediction error with respect to the data
generating distribution. Suppose that a, a1, . . . , an are
sampled i.i.d. from some distribution P on rows(Φ)
and the learner observes (at)n

t=1 and (µat )n

t=1.

ˆθ =

n

 

t=1
X

ata⊤t

!

1 n

−

t=1
X

atµat .

Then, by making reasonable boundedness and span
assumptions on rows(Φ), and by combining the results
in chapters 13 and 14 of (Wainwright, 2019), with high
probability,

d
n

.

h

E

−

= O

+ ε2

µa)2

(a⊤ ˆθ

ˆθ
i
Notice, there is no d multiplying the dependence
The fundamental
on the approximation error.
diﬀerence is that a is sampled from P . The quantity
rows(Φ)(a⊤ ˆθ
µa)2 behaves quite diﬀerently, as
maxa
∈
the lower bound shows.

−

(cid:18)

(cid:19)

(cid:12)
(cid:12)
(cid:12)

Feature-dependent bounds The negative result
in Section 3 shows that there exist feature matrices
for which the learner must query exponentially many
actions or suﬀer an estimation error that expands
the approximation error by a factor of √d. On
the other hand, Proposition 4.2 shows that for any
feature matrix, there exists a learner that queries

Learning with Good Feature Representations in Bandits and in RL with a Generative Model

O(d log log(d)) actions for an estimation error of ε√d,
roughly matching the lower bound. One might wonder
whether or not
there exists a feature-dependent
measure that characterises the blowup in estimation
error in terms of the feature matrix and query budget.
[k]
One such measure is given here. Given a set C
d be the matrix obtained
with
×
from Φ by restricting to those rows indexed by C.
Deﬁne

= q, let ΦC ∈

C
|

Rq

⊆

|

λq(Φ) =

min
C
[k],
|

|

C

⊂

=q

v

max
Rd
0
\{
∈

}

Φv
k
ΦCv
k

k∞
k∞

.

Proposition 4.6. Let 1
and δ2 > ε(1 + 2λq(Φ)). Then,

≤

q < k and δ1 = ε(1+λq(Φ))

cest
δ1 (

ε
Φ) > q

H

cest
δ2 (

ε
Φ) .

H

≥

The proof is supplied in Appendix C. By (1), it also
holds that cmax
ε
q. Currently we do not have a
Φ)
2δ2 (
corresponding lower bound, however.

H

≤

5. Misspeciﬁed linear bandits

∈

Rk

ε
Φ.

∈ H

d and µ

Here we consider the classic stochastic bandit where
the mean rewards are nearly a linear function of
their associated features. We assume for simplicity
that no two actions have the same features.
In
case this does not hold, a representative action can
be chosen for each feature without changing the
In
main theorem. Let Φ
×
[n], the learner chooses actions (Xt)n
rounds t
t=1
∈
rows(Φ) and the reward is Yt = µXt +
with Xt ∈
ηt where (ηt)n
independent 1-
t=1 is a sequence of
subgaussian random variables. The optimal action has
µa and the expected
expected reward µ∗ = maxa
n
regret is Rn = E[
µXt ]. The idea is
t=1 µ∗
to use essentially the same elimination algorithm as
(Lattimore and Szepesv´ari, 2019, chapter 22), which
In each episode,
is summarised in Algorithm 1.
the algorithm computes a near-optimal design over a
subset of the actions that are plausibly optimal.
It
then chooses each action in proportion to the optimal
design and eliminates arms that appear suﬃciently
suboptimal.

∈A
−

P

Proposition 5.1. When α = 1/(kn) and C is
a suitably large universal constant and maxa µa −
mina µa ≤

1, Algorithm 1 satisﬁes

Rn ≤

C

hp

dn log(nk) + εn√d log(n)

.

i

In Appendix F, we
the bound in
Proposition 5.1 is tight up to logarithmic factors in
the interesting regime where k is comparable to n.

show that

k∞ ≤

ε, which exists
∆
Proof. Let µ = Φθ + ∆ with
k
ε
Φ. We only analyse the
by the assumption that µ
∈ H
behaviour of the algorithm within an episode, showing
that the least-squares estimator is guaranteed to have
suﬃcient accuracy so that (a) arms that are suﬃciently
suboptimal are eliminated and (b) some near-optimal
arms are retained. Fix any b
. Using the notation
in Algorithm 1,

∈ A

u

s=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Xa
∈A

b, ˆθ
h

θ

i

−

=

b⊤G−

1

∆XsXs + b⊤G−

1

Xsηs

u

s=1
X

u

1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(2)

b⊤G−

1

u(a)a∆a

+

≤ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
The ﬁrst term is bounded using Jensen’s inequality as
(cid:12)
(cid:12)
before:

s=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

b⊤G−
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xsηs

.

b⊤G−

1

u(a)∆aa

ε

u(a)

b⊤G−

1a

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xa
∈A

ε

≤

= ε

v
u
u
t

 

Xa
∈A

sXa

∈A

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
b⊤

Xa
∈A

(cid:12)
(cid:12)
u(a)G−

(cid:12)
(cid:12)
1b
1aa⊤G−

u(a)

!

Xa
∈A

u(a)
k

b

k

2
G−1 ≤

ε

2du
m ≤

r

2ε√d ,

where the ﬁrst inequality follows form H¨older’s inequal-
ity, the second is Jensen’s inequality and the last fol-
lows from the exploration distribution that guaran-
2d/m. The second term in Eq. (2) is
tees
bounded using standard concentration bounds. Pre-
ciesly, by eq.
(20.2) of (Lattimore and Szepesv´ari,
2019), with probability at least 1

2
G−1 ≤

2α,

k

k

b

−

b⊤G−

1

Xsηs

u

s=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
α

(cid:18)

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

b

kG−1

≤ k

s

2 log

4d
m

log

1
α

(cid:18)

(cid:19)

≤ s

b, ˆθ

θ

|h

and
. Continuing
with standard calculations, provided in Appendix D,
one gets that the expected regret satisﬁes

i| ≤

q

−

2ε√d +

4d
m log

1
α

(cid:0)

(cid:1)

dn log(nk) + εn√d log(n)

Rn ≤

C

hp

i

where C > 0 is a suitably large universal constant. The
logarithmic factor in the second term is due to the fact
that in each of the logarithmically many episodes the
algorithm may eliminate the best remaining arm, but
keep an arm that is at most O(ε√d) worse than the
best remaining arm.

Learning with Good Feature Representations in Bandits and in RL with a Generative Model

(0, 1)

∈

= rows(Φ)

Rk

Algorithm 1 phased elimination
input Φ
(1) Set m =
(2) Find design ρ :
supp(ρ)

×
4d log log d
⌉
A →

d and conﬁdence level α

+ 16 and

A

∈

⌈

|

| ≤

(3) Compute u(a) =

4d log log(d) + 16
mρ(a)
⌉

⌈

(4) Take each action a

with corresponding features (Xs)u
(Ys)u

s=1

∈ A

(5) Calculate the vector ˆθ:

[0, 1] with g(ρ)

2d and

≤

and u =

u(a)

Xa
∈A

exactly u(a) times
s=1 and rewards

ˆθ = G−

1

XsYs with G =

u(a)aa⊤

Xa
∈A

u

s=1
X

(6) Update active set:

A ← (

a

∈ A

: max
b
∈A
2m and goto (1)

ˆθ, b
h

(7) m

←

a

−

i ≤

4d
m

2

s

log

1
α

(cid:18)

(cid:19))

.

Remark 5.2. When the active set contains fewer than
d actions, then the conditions of Kiefer-Wolfowitz are
cannot span Rd. Rest assured,
not satisﬁed because
however, since in these cases one can simply work in
and the analysis goes
the smaller space spanned by
through without further changes.

A

A

Known approximation error The logarithmic
factor in the second term in the regret bound can be
removed when ε is known by modifying the elimination
criteria so that with high probability the optimal
action is never eliminated, as explained in Remark D.1.

Inﬁnite action sets The logarithmic dependence
on k follows from the choice of α, which is needed
to guarantee the concentration holds for all actions.
the union bound can be
When k = Ω(exp(d)),
improved by a covering argument or using the
argument in the next section. This leads to a bound
n log(n) + εn√d log(n)), which is independent
of O(d
of the number of arms.

p

Ghosh et al.

Other approaches We are not the ﬁrst to consider
misspeciﬁed linear bandits.
(2017)
consider the same setting and show that in the
favourable case when one can cheaply test linearity,
there exist algorithms for which the regret has order
min(d, √k)√n up to logarithmic factors. While
such results a certainly welcome, our focus is on
the case where k has the same order of magnitude
as n and hence the dependence of the regret on
ε is paramount. Another way to obtain a similar

is

to ours

result
to use the Eluder dimension
(Russo and Van Roy, 2013), which should ﬁrst be
generalised a little to accommodate the need to use an
accuracy threshhold that does not decrease with the
horizon. Then the Eluder dimension can be controlled
using either our techniques or the alternative argument
by Van Roy and Dong (2019).

Contextual linear bandits Algorithms based on
phased elimination are not easily adapted to the
contextual case, which is usually addressed using
optimistic methods. You might wonder whether or not
LinUCB (Abbasi-Yadkori et al., 2011) serendipitously
adapts to misspeciﬁed models in the contextual case.
Gopalan et al. (2016) have shown that LinUCB is
robust in the non-contextual case when ε is very small.
Their conditions, however, depend the structure of the
problem, and in particular on having good control
of the 2-norm of ∆, which may scale like Ω(ε√k)
and is too big for large action sets. We provide a
negative result in the supplementary material, as well
as a modiﬁcation that corrects the algorithm, but
requires knowledge of the approximation error. The
modiﬁcation is a data-dependent reﬁnement of the
bonus used by Jin et al. (2019). An open question
is to ﬁnd an algorithm for contextual linear bandits
for which the regret similar to Proposition 5.1 and
where the algorithm does not need to know the
approximation error.

6. Reinforcement learning

∈

×

[S]

[A]

We now consider discounted reinforcement learning
with a generative model, which means the learner can
sample next-states and rewards for any state-action
pair of their choice. The notation is largely borrowed
from (Szepesv´ari, 2010). Fix an MDP with state
space [S], action space [A], transition kernel P , reward
function r :
[0, 1] and discount factor
γ
(0, 1). The ﬁniteness of the state space is assumed
only for simplicity. As usual, V π and Qπ refer to
the value and action-value functions for policy π (e.g.,
V π(s) is the total expected discounted reward incurred
while following policy π in the MDP) and V ∗ and Q∗
the same for the optimal policy. The learner is given a
ε
feature matrix Φ
Φ for all
∈ H
policies π and where Qπ is vectorised in the obvious
Rd denotes the feature
way. The notation Φ(s, a)
associated with state-action pair (s, a).

d such that Qπ

RSA

→

∈

∈

×

The main idea is the observation that if Q∗ were
known with reasonable accuracy on the support of an
approximately optimal design ρ on the set of vectors
[A]), then least-squares in
(Φ(s, a) : s, a

[S]

∈

×

Learning with Good Feature Representations in Bandits and in RL with a Generative Model

(s,a)

∈C

P

combination with our earlier arguments would provide
a good estimation of the optimal state-action value
function. Approximating Q∗ on the core set
=
supp(ρ)
[A] is possible using approximate
policy iteration. For the remainder of this section let
and
ρ be a design with g(ρ)
G(ρ) =

2d and with support

ρ(s, a)Φ(s, a)Φ(s, a)⊤.

[S]

≤

×

⊂

C

C

Related work The idea to extrapolate a value
function by sampling from a few anchor state/action
pairs has been used before in a few works. The recent
work by Zanette et al. (2019) consider approximate
value iteration in the episodic setting and do not
make a connection to optimal design. The challenge
in the ﬁnite-horizon setting is that one must learn
one parameter vector for each layer and, at least
naively, errors propagate multiplicatively. For this
reason using the anchor pairs from the support of an
experimental design would not make the algorithm
proposed by the aforementioned paper practical.
Yang and Wang (2019) assume the transition matrix
has a linear structure and also use least-squares
with data from a pre-selected collection of anchor
state/action pairs. Their assumption is that the
features of all state-action pairs can be written as a
convex combination of the anchoring features, which
means the number of anchors is the number of corners
of the polytope spanned by rows(Φ) and may be much
larger than d. One special feature of their paper is
that the dependency on the horizon of the sample
γ), while in our theorem
complexity is cubic in 1/(1
−
it is quartic. Earlier, Lakshminarayanan et al. (2018)
described how anchor states (with some lag allowed)
can be used to reduce the number of constraints
in the approximate linear programming approach to
approximate planning in MDPs, while maintaining
error bounds.

Approximate policy iteration Let π1 be an
arbitrary policy and deﬁne a sequence of policies
(πk)∞k=1 inductively using the following procedure.
From each state-action pair (s, a)
take m roll-outs
of length n following policy πk and let ˆQk(s, a) be the
empirical average, which is only deﬁned on the core
. The estimation of Qπk is then extended to all
set
state-action pairs using the features and least-squares

∈ C

C

ˆθk = G(ρ)−

1

ρ(s, a)Φ(s, a) ˆQk(s, a) Qk = Φˆθk .

X(s,a)
∈C

Signiﬁcantly, the choice of parameters ensures that the
total number of samples from the generative model is
independent of S and A.

Theorem 6.1. Suppose that approximate policy
iteration is run with

k =

log

1
ε√d
γ

(cid:17)

(cid:16)
1
−

m =

2k

log

(cid:16)
2ε2(1

−

|C|α
γ)2 n =
(cid:17)

log

ε(1

γ)

1

−
γ

(cid:16)
1

−

.

(cid:17)

Then there exists a universal constant C such that with
probability at least 1

α, the policy πk+1 satisﬁes

−
V πk+1 (s))

max
[S]
s
∈

(V ∗(s)

−

Cε√d/(1

γ)2 ,

−

≤

When ρ is chosen using Theorem 4.4 so that
|C| ≤
4d log log(d) + 16, then the number of samples from
the generative model is kmn

, which is

log

log

ε(1

γ)

1

−

(cid:16)

(cid:17)

2k

|C|α
ε2(1

(cid:16)

O





|C|

log

(cid:16)
γ)4

(cid:17)
−

1
ε√d

(cid:17)

d log log(d)

.





Before the proof we need two lemmas. The ﬁrst
controls the propagation of errors in policy iteration
when using Qk rather than Qπk . For policy π, let
P π : R[S]
[A] deﬁned by (P πQ)(s, a) =
s′ P (s′

×
s, a)Q(s′, π(s′)).
|

R[S]

→

[A]

×

Qπi and Ei = P πi+1(I
Lemma 6.2. Let δi = Qi −
P
P π
Qπk
γP πi)
γP πi+1)−
. Then, Q∗
−
−
k
1
)k
i=0 (γP π
Qπ0) + γ
(γP π
−
−

1(I
)k(Q∗

−
1Eiδi.

i
−

−

−
≤

∗

∗

∗

P

Proof. This is stated as Eq.
(7) in the proof of
part (b) of Theorem 3 of (Farahmand et al., 2010) and
ultimately follows from Lemma 4 of Munos (2003).

The second lemma controls the value of the greedy
policy with respect to a Q function in terms of the
quality of the Q function.

Lemma 6.3 (Singh and Yee (1994), corollary 2). Let
π be greedy with respect to an action-value function Q.
[S], V π(s)
Then for any state s
Q∗

V ∗(s)

γ k

Q

≥

−

−

∈

−

.

2

1

k∞

Proof of Theorem 6.1. Hoeﬀding’s bound and the
deﬁnition of the roll-out length shows that for any
(s, a)

, with probability at least 1

α,

−

∈ C

ˆQi(s, a)

Then πk+1 is chosen to be the greedy policy with
respect to Qk and the process is repeated. The
following theorem shows that for suitable choices of
roll-out length n, roll-out number m and iterations k,
the policy πk+1 is nearly optimal with high probability.

1

−

≤

1

γ s

1
2m

log

2
α

(cid:18)

(cid:19)

+ ε = 2ε .

(cid:12)
(cid:12)
(cid:12)
At the end we analyse the failure probability of the
algorithm, but for now assume the above inequality

−

Qπi(s, a)
(cid:12)
(cid:12)
(cid:12)

Learning with Good Feature Representations in Bandits and in RL with a Generative Model

holds for all i
arg minθ k
β = 2ε,

Qπi

−

k∞

k and (s, a)

Let θi =
∈ C
. Then, by Proposition 4.5 with

.

≤
Φθ

Qi −

k

Qπi

=

Φˆθi −

k

Qπi

k∞

k∞ ≤

3ε√2d + ε

.
= δ .

Since the rewards belong to the unit interval, taking
the maximum norm of both sides in Lemma 6.2 shows
Qπk
γ). Then,
that
by the triangle inequality,

γ) + γk/(1

k∞ ≤

2δ/(1

Q∗

−

−

−

k

Qk −

k

Q∗

k∞ ≤ k

+

Q∗

k

−

Qπk

k∞

Qπk

Qk −
3δ
+

k∞
γk

.

γ

1

−

≤

1

γ

−

Next, by Lemma 6.3, for any state s

[S],

∈

V πk+1(s)

V ∗(s)

≥

≥

V ∗(s)

2

−

1

−

γ k
2

−

(1

γ)2

−

Qk −

Q∗

k∞

3δ + γk

.

(cid:0)

(cid:1)

All that remains is bounding the failure probability,
which follows immediately from a union bound over all
iterations i

k and state-action pairs (s, a)

.

≤

∈ C

7. Conclusions

Are good representations suﬃcient for eﬃcient learn-
ing in bandits or in RL with a generative model? The
answer depends on whether one accepts a blowup of
the approximation error by a factor of √d, and is pos-
itive if and only if this blowup is acceptable. The im-
plication is that the role of bias/prior information is
more pronounced than in supervised learning where
the blowup does not appear. One may wonder whether
the usual changes to the learning problem, such as
considering sparse approximations, could reduce the
blowup. Since sparsity is of little help even in the real-
isable setting (Lattimore and Szepesv´ari, 2019, chap-
ter 23), we are only modestly optimistic in this regard.
Note also that in reinforcement learning, the blowup
is even harsher: in the discounted case we see that a
γ)2 also appears, which we believe is
factor of 1/(1
−
not improvable.

The analysis in both the bandit and reinforcement
learning settings can be decoupled into two compo-
nents. The ﬁrst is to control the query complexity of
identifying a near-optimal action and the second is es-
timating the value of an action/policy using roll-outs.
This view may be prove fruitful when analysing (more)
non-linear classes of reward function.

There are many open questions.
to compute an approximate optimal design,

in order
the

First,

algorithm needs to examine all features. Second, the
argument in Section 6 heavily relies on the uniform
contraction property of the various operators involved.
It remains to be seen whether similar arguments hold
for other settings, such as the ﬁnite horizon setting
or the average cost setting. Another interesting open
question is whether a similar result holds for the online
setting when the learner needs to control its regret.

8. Bibliography

Y. Abbasi-Yadkori, D. P´al, and Cs. Szepesv´ari.
Improved algorithms for linear stochastic bandits.
In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett,
F. Pereira, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 24, NIPS,
pages 2312–2320. Curran Associates, Inc., 2011.

K. Amin, M. Kearns, and U. Syed.

query learning, and the haystack dimension.
Proceedings of
Learning Theory, pages 87–106, 2011.

Bandits,
In
the 24th Annual Conference on

J.-Y. Audibert and S. Bubeck. Best arm identiﬁcation
In Proceedings of

in multi-armed bandits.
Conference on Learning Theory (COLT), 2010.

R. Degenne, W. M. Koolen, and P. M´enard. Non-
asymptotic pure exploration by solving games. In
Advances in Neural Information Processing Systems,
pages 14465–14474, 2019.

S. S. Du, S. M. Kakade, R. Wang, and L. F. Yang. Is
a good representation suﬃcient for sample eﬃcient
reinforcement learning?, 2019.

E. Even-Dar, S. Mannor, and Y. Mansour. Action
elimination and stopping conditions for the multi-
armed bandit and reinforcement learning problems.
Journal of Machine Learning Research, 7(Jun):1079–
1105, 2006.

A-m. Farahmand, R. Munos, and Cs. Szepesv´ari.
Error propagation for approximate policy and value
iteration (extended version). In NIPS, 2010.

V. V. Fedorov.

Theory of optimal experiments.

Academic Press, New York, 1972.

A. Ghosh, S. R. Chowdhury, and A. Gopalan.
Misspeciﬁed linear bandits. In Thirty-First AAAI
Conference on Artiﬁcial Intelligence, 2017.

Aditya Gopalan, Odalric-Ambrym Maillard, and
Mohammadi Zaki. Low-rank bandits with latent
mixtures. arXiv preprint arXiv:1609.01508, 2016.

Learning with Good Feature Representations in Bandits and in RL with a Generative Model

T. Lattimore and Cs. Szepesv´ari. Bandit Algorithms.

Cambridge University Press, 2019. draft.

cmax
1

(
{

e1, . . . , ek}

) = (k + 1)/2 .

C. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provably
eﬃcient reinforcement learning with linear function
approximation.
arXiv preprint arXiv:1907.05388,
2019.

J. Kiefer and J. Wolfowitz.

The equivalence of
two extremum problems. Canadian Journal of
Mathematics, 12(5):363–365, 1960.

C. Lakshminarayanan,

S. Bhatnagar,

and Cs.
Szepesv´ari. A linearly relaxed approximate lin-
ear program for Markov decision processes. IEEE
Transactions on Automatic Control, 63(4):1185–
1191, 2018.

R. Munos. Error bounds for approximate policy
iteration. 19th International Conference on Machine
Learning, pages 560–567, 2003.

D. Russo and B. Van Roy. Eluder dimension and
the sample complexity of optimistic exploration.
In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors,
Advances in Neural Information Processing Systems
26, NIPS, pages 2256–2264. Curran Associates, Inc.,
2013.

S. P. Singh and R. C. Yee. An upper bound on the loss
from approximate optimal-value functions. Machine
Learning, 16(3):227–233, 1994.

M. Soare, A. Lazaric, and R. Munos. Best-arm
identiﬁcation in linear bandits. In Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, and
K. Q. Weinberger, editors, Advances in Neural
Information Processing Systems 27, NIPS, pages
828–836. Curran Associates, Inc., 2014.

Cs. Szepesv´ari.

Algorithms

for Reinforcement

Learning. Morgan & Claypool, 2010.

M. J. Todd. Minimum-volume ellipsoids: Theory and

algorithms, volume 23. SIAM, 2016.

B. Van Roy and S. Dong. Comments on the Du-
Kakade-Wang-Yang lower bounds. In preparation,
2019.

M. J. Wainwright. High-dimensional statistics: A
non-asymptotic viewpoint, volume 48. Cambridge
University Press, 2019.

L. Yang and M. Wang. Sample-optimal parametric
Q-learning using linearly additive features.
In
International Conference on Machine Learning,
pages 6995–7004, 2019.

A. Zanette, A. Lazaric, M. J. Kochenderfer, and
Limiting extrapolation in linear
E. Brunskill.
approximate value iteration.
In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc,
E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems 32, pages 5616–
5625. Curran Associates, Inc., 2019.

A. Proof of Lemma 2.2

Recall that Lemma 2.2 states that

For the upper bound consider, an algorithm that
queries each coordinate in a random order, stopping
as soon as it receives a nonzero reward, at which point
the algorithm can return ˆµ = µ and ˆa = arg max µ.
Clearly, this algorithm is sound. Let S
Perm([k])
be a (uniform) random permutation of [k], which
represents the order of the algorithms queries. Let
Ti = min
be the number of queries
}
if r = ei. Note that (Ti)i is a random permutation of
= E[Ti].
[k]. Note also that by symmetry E[T1] =
Hence, maxi E[Ti] = 1
k

i Ti] = (k + 1)/2.

1 : St = i

E[

· · ·

≥

∈

{

t

A

A

A

′
A

[k] of queries. Call

is never worse than that of

The proof of the lower bound is based on a similar
P
calculation. First, note that by Yao’s principle it
suﬃces to show that there exist a distribution P over
the problem instances such that any deterministic
algorithm needs to ask at least (k + 1)/2 queries on
expectation when the instance that the algorithm runs
on is chosen randomly from the distribution. Let P be
the uniform distribution. Note that any deterministic
can be identiﬁed with a ﬁxed sequence
algorithm
s1, s2,
dominated if some
· · · ∈
′ achieves better query complexity
other algorithm
on any input instance while the query complexity
of
on any other
instance. Clearly, it suﬃces to consider nondominated
algorithms. Hence, s1, s2, . . . , sk cannot have repeated
elements, i.e., (s1, s2, . . . , sk) is a permutation of [k].
Further,
must stop as soon as it receives a nonzero
answer or it would be dominated. This implies that
when run on ei is
the number of queries issued by
t(i) = min
is sound and
. (Since
{
δ
cannot stop before time t(i) when running
1,
on ei and if stopped later, it would be dominated by
the algorithm that stops at time t(i)). Since (si)i is a
[k]. Then, the expected
permutation of [k], so is (t(i))i
∈
is E[t(I)] = 1
i t(i) =
number of queries issued by
k
A
1
k

i i = (k + 1)/2.

1 : st = i

A

A

A

A

A

≤

≥

}

t

P

P

Learning with Good Feature Representations in Bandits and in RL with a Generative Model

B. Proof of Corollary 3.3

By the proof of Lemma 2.2, it should be clear that if
(
δe1, . . . , δek ∈ H
H
1

(k + 1)/2. Let

, then cmax

d

)

2

δ

≥
ε
δ

k =

exp

(cid:22)

(cid:18)

−
8

.

(cid:16)

(cid:17)

(cid:19)(cid:23)

d such that for all a

= b
ε/δ. Let rows(Φ′) =

Then, by Lemma 3.1, there exists a feature matrix
Rk
rows(Φ′),
Φ′
= 1
a
×
∈
k
k
a, b
. Then,
a1, . . . , ak}
and
h
i ≤
ε
[k].
Φ for all i
Φ′(δai)
δei −
ε and hence δei ∈ H
k ≤
k
Thus, cmax
ε
(k + 1)/2. The result follows from
Φ)
(
≥
H
the deﬁnition of k.

∈
{

∈

δ

C. Proof of Proposition 4.6

Algorithm 2 Optimal estimation algorithm
input: Φ
∈
(1) Find C

[k] and ε
= q minimising

×
[k] with

Rk

≥

0

d and q
∈
C
|
|
: u

Φu

⊂
max

k∞

∈

k

Rd,

ΦCu

1

k ≤

k
(cid:8)
(2) Probe µ on C
(3) Find ˆθ
(4) Return ˆµ = Φˆθ

∈

Rd such that

ΦC ˆθ

k

µC k∞ ≤

ε

−

(cid:9)

Upper bound The upper bound is realised by
Rd and
ε
Algorithm 2. Since µ
Φ, there exists a θ
(ε) such that µ = Φθ + ∆. By the deﬁnition
∆
∞
ε, which implies that
of the algorithm,

∈ H

B

∈

∈

ΦC (θ

k

−

µC k∞ ≤
ˆµC −
k
ˆθ)
µC −
=
k
ε +
k

k∞

≤

2ε .

≤

ˆµC k∞

∆C −
µC −

ˆµC k∞

Next, using the deﬁnition of λq,

µ

k

ˆµ

=

Φ(θ

−

k∞

k

−

ˆθ) + ∆
ˆθ)

k∞
+ ε

k∞
ˆθ)

−

k∞

v

Φ(θ

−
ΦC(θ

≤ k

≤ k

2ελq(Φ) + ε

≤
< δ2 .

max
Rd
0
\{
∈

}

Φv
k
ΦC v
k

k∞
k∞

+ ε

H

∈ H

Lower bound Suppose an algorithm is sound with
ε
respect to (
Φ, δ1). It suﬃces to show that there exists
ε
a µ
Φ such that whenever the algorithm halts with
non-zero probability, it has made more than q queries.
Let µ = 0 and suppose the algorithm halts having
[k] with non-zero probability and
queried µ on C
be the set
C
|

ε
Φ : µC = 0

q. Let

∈ H

⊂
=

| ≤

R

µ

{

}

of plausible rewards consistent with the observation.
By the assumption that the algorithm is sound with
respect to (

ε
Φ, δ1), it must be that

H
2 max
ν
∈R

ν

k

k∞ ≤

max
ν,ξ
∈R

ν

k

ξ

−

k∞ ≤

2δ1 ,

where the ﬁrst inequality is true since for all ν
have

. Then,

ν

we

∈ R

∈ R

k∞

−
ν

max
k
ν
∈R
= max

k
= ε + max

(cid:8)
k
ε + ελq(Φ)
(cid:8)
δ1 ,

Φθ + ∆

:

ΦC θ

k
ΦCθ

k∞
:

k∞

k

Φθ

k∞ ≤

ε, θ

ε, θ

Rd, ∆
Rd

∈

k∞ ≤

∈

B

(ε)

∞

∈

(cid:9)

≥
≥
which contradicts soundness and hence

(cid:9)

> q.

C
|

|

D. Details for proof of Proposition 5.1

The proof
First we
is completed in two steps.
summarise what has already been established about
the within-episode behaviour of the algorithm. Then,
in the second step, the regret is summed over the
episodes.

In-episode behaviour Let F be the σ-algebra
generated by the history up to the start of a given
episode and ˆµ be the least-squares estimate of µ
computed by the algorithm in that episode. Similarly,
be the quantities deﬁned in the given
let m, u and
µa
episode of the algorithm. Let a∗ = arg maxa
and ˆa = arg maxa
Eb be the
event

ˆµa. Now, for b

∈ A

, let

∈A

∈A

A

Eb =

(

b, ˆθ
h

−

θ

(cid:12)
(cid:12)
We have shown that P(
(cid:12)
union bound implies that

Eb |

≤

i
(cid:12)
(cid:12)
(cid:12)

2ε√d +

4d
m

s

log

1
α

.

(cid:18)

(cid:19))
α, which by a

F )

1

≥

−

P(

F )

1

kα .

∪b

∈AEb |
By the deﬁnition of the algorithm, an action a
not eliminated at the end of the episode if

≥

−

is

∈ A

ˆθ, ˆa
h

a

−

i ≤

4d
m

2

s

1
δ

log

,

(cid:18)

(cid:19)

which implies that

4d
m

2

s

log

1
δ

(cid:18)

(cid:19)

ˆθ, ˆa

a

i

−

≥ h

θ, a∗

a

−

i −

≥ h

4d
m

2

s

log

1
α

−

(cid:18)

(cid:19)

4ε√d .

Learning with Good Feature Representations in Bandits and in RL with a Generative Model

Hence, if a

is not eliminated, then

∈ A
a, θ

µa ≥ h

ε

i −

a∗, θ

ε

4

i −

−

≥ h

4d
m

log

1
α

4ε√d

−

(cid:19)

(cid:18)
1
α

−

(cid:18)

(cid:19)

4ε√d .

s

4d
m

µa∗

ε

4

−

−

≥

s

log

Because the condition for eliminating arms does not
depend on ε,
it is not possible to prove that a∗
is not eliminated with high probability. What we
can show is that at least one near-optimal action is
retained. Suppose that a∗ is eliminated, then, using
the deﬁnition of the algorithm,

4d
m

2

s

log

1
α

(cid:18)

(cid:19)

<

ˆθ, ˆa
h

a∗

i

−

θ, ˆa

a∗

+ 2

≤ h

−

i

4d
m

s

log

1
α

(cid:18)

(cid:19)

+ 4ε√d .

Rearranging shows that

ε

i −

θ, ˆa

µˆa ≥ h
θ, a∗
>
h
µa∗

≥

−

ε

4ε√d
i −
2ε(1 + 2√d) .

−

(3)

Of course, ˆa is not eliminated, which means that either
a∗ is retained, or an action with nearly the same
reward is. What we have shown is that arms are
eliminated if they are much worse than a∗ and that
some arm with mean close to a∗ is retained. We now
combine these results.

Combining the episodes Let L be the number of
episodes and δℓ be the suboptimality of the best arm in
the active set at the start of episode ℓ. By the previous
kαL, the good events
part, with probability at least 1
occur in all episodes. Suppose for a moment that this
happens. Then, by Eq. (3),

−

2ε(ℓ

δℓ ≤
−
letting mℓ = 2ℓ

1

−

Then,
regret is bounded by

1)(1 + 2√d) .

4d log log(d) + 16
⌉

,

⌈

the

L

Rn =

Xℓ=2 Xa
ℓ
∈A
L

uℓ(a)(µ∗

µa)

−

m1 +

mℓ

δℓ + 2

≤

"

Xℓ=2

4d
mℓ

1

−

s

log

1
α

(cid:18)

(cid:19)

+ 4ε√d
#

C

≤

"s

dmL log

+ εmL log(mL)√d
#

1
α

(cid:18)

(cid:19)

C

≤

"s

dn log

1
α

(cid:18)

(cid:19)

+ εn√d log(n)
#

.

The result follows because the regret due to failing
conﬁdence intervals is at most αknL
log2(n),
≤
which is negligible relative to the above term.
Remark D.1. When ε is known, the elimination
condition can be changed to

≤

L

a

A ← (

∈ A

: max
b
∈A

ˆθ, b
−
h
4

a

i

d
m

log

1
δ

(cid:18)

(cid:19)

≤ s

+ ε√d

.

)

Repeating the analysis now shows that the optimal
action is never eliminated with high probability, which
eliminates the logarithmic dependence in the second
term of Proposition 5.1.

E. Linear contextual bandits

×

In the contextual version of the misspeciﬁed linear
bandit problem, the feature matrix changes from
round to round. Let (kt)n
t=1 be a sequence of natural
numbers. At the start of round t the learner observes
Rkt
a matrix Φt ∈
rows(Φt)
+ ηt + ∆(Xt)
and receives a reward Yt =
where ∆ : Rd
ε. Elimination
algorithms are not suitable for such problems. Here we
show that if ε is known, then a simple modiﬁcation of
LinUCB (Abbasi-Yadkori et al., 2011) can be eﬀective.
You might wonder whether or not this algorithm
works well without modiﬁcation. The answer is sadly
negative.

d, chooses an action Xt ∈
Xt, θ
h
i
R satisﬁes
∆
k∞ ≤

→

k

t
s=1 XsX ⊤s and deﬁne the regurlarised
Let Gt = I +
least-squares estimator based on data from the ﬁrst t
rounds by

P

ˆθt = G−
t

1

XsYs .

t

s=1
X

Assume for all a

n
1 and
t=1 rows(Φt) that
∈ ∪
1. The standard version of LinUCB chooses

k2 ≤

a

k

a, θ

|h

i| ≤

Xt+1 = arg max
a

rows(Φt+1)h

a, ˆθti

+

a

k

kG

βt ,

−1
t

∈

(4)

with βt = 1 +

2 log (n) + d log

1 +

.

(5)

n
d

(cid:17)

(cid:16)

The modiﬁcation chooses

r

Xt+1 = arg max
a

rows(Φt+1)h

a, ˆθti

+

a

k

−1
t

kG

βt + ε

∈

t

s=1
X

a⊤G−
|

.

1

t Xs|
(6)

This modiﬁcation is reminiscent of the algorithm
by Jin et al. (2019), who use a bonus of Ω(t1/2)

Learning with Good Feature Representations in Bandits and in RL with a Generative Model

for

linear dynamics

when using upper conﬁdence bounds for least-squares
in reinforcement
estimators
learning.
Theorem E.1. The regret of the algorithm deﬁned by
Eq. (6) satisﬁes

Hence

Rn = O

d√n log(n) + nε

d log(n)

.

(cid:16)

p

(cid:17)

Proof sketch. The main point is that the additional
bonus term ensures optimism. Then, the standard
regret calculation shows that

ˆθt, (2, 1)
i
h

+

(2, 1)

k

−1
t

kG

βt =

1 + O

−

d log(n)
nε2

!

 r

and this for every suitably large n the algorithm will
choose (0, 0) for all rounds t
n/2 and suﬀer regret
at least n/2. Thus, if Rn(ε) is the regret on the above
problem,

≥

Rn = O

d√n log(n) + εE

 

1

X ⊤t G−
t
|
−

1Xs|#!

.

while for the modiﬁed algorithm,

p

sup
n,ε>0

Rn(ε)

εn

log(n)

=

,

∞

sup
n,ε>0

Rn(ε)

εn

log(n)

< +

.

∞

1

X ⊤t G−
t
|
−

1Xs| ≤

n

(X ⊤t G−
t
−

1
1Xs)2

p
F. Lower bounds for linear bandits

The latter term is bounded by

n

1

t
−

t=1
X

s=1
X

n

1

t
−

"

t=1
X

s=1
X

n

1

t
−

s=1
X

v
u
u
t

t=1
X
n

v
u
t=1
X
u
t
n

n

≤

Xtk

k

2
G

−1
t−1

= O

d log(n)

.

(cid:17)
Hence the regret of this algorithm satisﬁes

p

(cid:16)

Rn = O

d√n log(n) + εn

d log(n)

.

(cid:16)

Remark E.2. As far as we know,
there is no
algorithm obtaining a similar bound when ε is
unknown.

p

(cid:17)

Failure of unmodiﬁed algorithm That
the
algorithm deﬁned by Eq. (5) is not good for contextual
bandits follows from the following example. Let ηt = 0
for all rounds t and

2 1
0 0

.

Φodd =

ε

0

Φeven =

0

ε

Φlarge =

(cid:0)

(cid:0)

(cid:1)

(cid:18)
(cid:1)
Now suppose for odd rounds t
n/2 the feature
matrix is Φt = Φodd in odd rounds and Φt = Φeven
in even rounds. For rounds t > n/2 the feature matrix
is Φlarge. Then let θ = (1/2,
ε
and ∆((0, ε)) = ε. Hence for t = n/2,

1/2) and ∆((ε, 0)) =

−

−

≤

(cid:19)

Gt =

ˆθt =

(cid:18)

1 + nε2/4
0
nε2/8
1 + nε2/4

−

0
1 + nε2/4
nε2/8
1 + nε2/4

,

(cid:19)

.

(cid:19)

(cid:18)
(2, 1)
k

2
G

k

−1
t ≤

Therefore

20/(nε2) and

ˆθt, (2, 1)
i
h

=

−

1 + nε2/4 ≤

nε2/8

4
nε2 −

1 .

The upper bound in Section 5 cannot be improved in
the most interesting regimes, as the following theorem
shows:

Theorem F.1. There exists a feature matrix Φ
Rk
reward vector µ

∈
d such that for any algorithm there is a mean

ε
Φ for which

×

∈ H

Rn ≥

ε min(n, (k

1)/2)

−

d

1

−
8 log(k)

.

s

Proof. By the negative result, we may choose Φ
Rk

d such that

×

∈

a, a
h

i

= 1 for all a

rows(Φ)

a, b
h

i ≤ r
Next, let a∗

∈

8 log(k)

d

1

−
rows(Φ) and

∈

for all a, b

rows(Φ) with a

= b .

∈

θ = δa∗

with δ =

d

1

−
8 log(k)

,

s

which is chosen so that µ

ε
Φ, where

∈ H

µa =

δ
0

(

if a = a∗
otherwise .

t

{

≤

n : As 6

Let τ = max
= a∗
≥
δE[τ ]. Since the law of the rewards is independent
τ , it follows from the randomisation
of a∗ for t
hammer that E[τ ]
1)/2) and the result
follows.

min(n, (k

. Then E[Rn]

s
∀

−

≤

≤

≥

}

t

Learning with Good Feature Representations in Bandits and in RL with a Generative Model

G. Computation complexity

We brieﬂy discuss the computation complexity of our
algorithms here. Both the bandit and RL algorithms
rely on computing a near-optimal design, which is
addressed ﬁrst.

Computing a near-optimal design The standard
method for computing a near-optimal design is Frank–
Wolfe, which in this setting is often attributed to
Fedorov (1972). With this algorithm and an appropri-
ate initialisation constant factor approximation of the
optimal design can be computed in O(kd2 log log(d))
computations. For more details we recommend chap-
ter 3 of the book by Todd (2016), which also describes
a number of improvements, heuristics and practical
guidance.

Bandit algorithm computations Algorithm 1 has
at most O(log(n)) episodes. In each episode it needs to
(a) compute a near-optimal design and (b) collect data
and ﬁnd the least-squares estimator and (c) perform
action elimination. The computation is dominated
by ﬁnding the near optimal design and computing
the covariance matrix G, which leads to a total
computation of O(kd2 log log(d) log(n) + nd2).

RL computations The algorithm described in
Section 6 operates in episodes over k episodes.
In
each episode it computes an approximate design
length n from each
and performs m roll-outs of
action in the core set. Assuming sampling from
the generative model is O(1), the total computation,
ignoring logarithmic factors, is

˜O

dA

ε2(1

(cid:18)

−

γ)4 +

SAd2
γ
1

−

.

(cid:19)

Dishearteningly, the size of the state space appears in
the computation of the optimal design. Hence, while
the sample complexity of our algorithm is independent
of the state space, the computation complexity is not.

