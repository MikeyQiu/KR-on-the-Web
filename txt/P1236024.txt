8
1
0
2
 
r
a

M
 
9
 
 
]
L
C
.
s
c
[
 
 
2
v
1
8
2
2
0
.
1
1
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

NON-AUTOREGRESSIVE
NEURAL MACHINE TRANSLATION

Jiatao Gu†∗, James Bradbury‡, Caiming Xiong‡, Victor O.K. Li†& Richard Socher‡
‡Salesforce Research
{james.bradbury,cxiong,rsocher}@salesforce.com
†The University of Hong Kong
{jiataogu, vli}@eee.hku.hk

ABSTRACT

Existing approaches to neural machine translation condition each output word on
previously generated outputs. We introduce a model that avoids this autoregressive
property and produces its outputs in parallel, allowing an order of magnitude lower
latency during inference. Through knowledge distillation, the use of input token
fertilities as a latent variable, and policy gradient ﬁne-tuning, we achieve this at
a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer
network used as a teacher. We demonstrate substantial cumulative improvements
associated with each of the three aspects of our training strategy, and validate
our approach on IWSLT 2016 English–German and two WMT language pairs.
By sampling fertilities in parallel at inference time, our non-autoregressive model
achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–
Romanian.

1

INTRODUCTION

Neural network based models outperform traditional statistical models for machine translation (MT)
(Bahdanau et al., 2015; Luong et al., 2015). However, state-of-the-art neural models are much
slower than statistical MT approaches at inference time (Wu et al., 2016). Both model families use
autoregressive decoders that operate one step at a time: they generate each token conditioned on
the sequence of tokens previously generated. This process is not parallelizable, and, in the case of
neural MT models, it is particularly slow because a computationally intensive neural network is used
to generate each token.

While several recently proposed models avoid recurrence at train time by leveraging convolu-
tions (Kalchbrenner et al., 2016; Gehring et al., 2017; Kaiser et al., 2017) or self-attention (Vaswani
et al., 2017) as more-parallelizable alternatives to recurrent neural networks (RNNs), use of autore-
gressive decoding makes it impossible to take full advantage of parallelism during inference.

We introduce a non-autoregressive translation model based on the Transformer network (Vaswani
et al., 2017). We modify the encoder of the original Transformer network by adding a module that
predicts fertilities, sequences of numbers that form an important component of many traditional
machine translation models (Brown et al., 1993). These fertilities are supervised during training
and provide the decoder at inference time with a globally consistent plan on which to condition its
simultaneously computed outputs.

2 BACKGROUND

2.1 AUTOREGRESSIVE NEURAL MACHINE TRANSLATION

Given a source sentence X = {x1, ..., xT (cid:48)}, a neural machine translation model factors the distribu-
tion over possible output sentences Y = {y1, ..., yT } into a chain of conditional probabilities with a

∗This work was completed while the ﬁrst author was interning at Salesforce Research.

1

Published as a conference paper at ICLR 2018

left-to-right causal structure:

T +1
(cid:89)

t=1

T +1
(cid:88)

t=1

pAR(Y |X; θ) =

p(yt|y0:t−1, x1:T (cid:48); θ),

(1)

where the special tokens y0 (e.g. (cid:104)bos(cid:105)) and yT +1 (e.g. (cid:104)eos(cid:105)) are used to represent the beginning
and end of all target sentences. These conditional probabilities are parameterized using a neural
network. Typically, an encoder-decoder architecture (Sutskever et al., 2014) with a unidirectional
RNN-based decoder is used to capture the causal structure of the output distribution.

Maximum Likelihood training Choosing to factorize the machine translation output distribution
autoregressively enables straightforward maximum likelihood training with a cross-entropy loss ap-
plied at each decoding step:

LML = log pAR(Y |X; θ) =

log p(yt|y0:t−1, x1:T (cid:48); θ).

(2)

This loss provides direct supervision for each conditional probability prediction.

Autoregressive NMT without RNNs Since the entire target translation is known at training time,
the calculation of later conditional probabilities (and their corresponding losses) does not depend on
the output words chosen during earlier decoding steps. Even though decoding must remain entirely
sequential during inference, models can take advantage of this parallelism during training. One such
approach replaces recurrent layers in the decoder with masked convolution layers (Kalchbrenner
et al., 2016; Gehring et al., 2017) that provide the causal structure required by the autoregressive
factorization.

A recently introduced option which reduces sequential computation still further is to construct the
decoder layers out of self-attention computations that have been causally masked in an analogous
way. The state-of-the-art Transformer network takes this approach, which allows information to ﬂow
in the decoder across arbitrarily long distances in a constant number of operations, asymptotically
fewer than required by convolutional architectures (Vaswani et al., 2017).

2.2 NON-AUTOREGRESSIVE DECODING

Pros and cons of autoregressive decoding The autoregressive factorization used by conventional
NMT models has several beneﬁts. It corresponds to the word-by-word nature of human language
production and effectively captures the distribution of real translations. Autoregressive models
achieve state-of-the-art performance on large-scale corpora and are easy to train, while beam search
provides an effective local search method for ﬁnding approximately-optimal output translations.

But there are also drawbacks. As the individual steps of the de-
coder must be run sequentially rather than in parallel, autore-
gressive decoding prevents architectures like the Transformer
from fully realizing their train-time performance advantage
during inference. Meanwhile, beam search suffers from dimin-
ishing returns with respect to beam size (Koehn & Knowles,
2017) and exhibits limited search parallelism because it intro-
duces computational dependence between beams.

Towards non-autoregressive decoding A na¨ıve solution is
to remove the autoregressive connection directly from an exist-
ing encoder-decoder model. Assuming that the target sequence
length T can be modeled with a separate conditional distribu-
tion pL, this becomes

pN A(Y |X; θ) = pL(T |x1:T (cid:48); θ) ·

p(yt|x1:T (cid:48); θ).

(3)

T
(cid:89)

t=1

Figure 1:
Translating “A B C”
to “X Y” using autoregressive and
non-autoregressive neural MT ar-
chitectures. The latter generates all
output tokens in parallel.

This model still has an explicit likelihood function, and it can still be trained using independent
cross-entropy losses on each output distribution. Now, however, these distributions can be computed
in parallel at inference time.

2

Published as a conference paper at ICLR 2018

Figure 2: The architecture of the NAT, where the black solid arrows represent differentiable con-
nections and the purple dashed arrows are non-differentiable operations. Each sublayer inside the
encoder and decoder stacks also includes layer normalization and a residual connection.

2.3 THE MULTIMODALITY PROBLEM

However, this na¨ıve approach does not yield good results, because such a model exhibits complete
conditional independence. Each token’s distribution p(yt) depends only on the source sentence X.
This makes it a poor approximation to the true target distribution, which exhibits strong correlation
across time. Intuitively, such a decoder is akin to a panel of human translators each asked to provide
a single word of a translation independently of the words their colleagues choose.

In particular, consider an English source sentence like “Thank you.” This can be accurately trans-
lated into German as any one of “Danke.”, “Danke sch¨on.”, or “Vielen Dank.”, all of which may
occur in a given training corpus. This target distribution cannot be represented as a product of
independent probability distributions for each of the ﬁrst, second, and third words, because a con-
ditionally independent distribution cannot allow “Danke sch¨on.” and “Vielen Dank.” without also
licensing “Danke Dank.” and “Vielen sch¨on.”

The conditional independence assumption prevents a model from properly capturing the highly mul-
timodal distribution of target translations. We call this the “multimodality problem” and introduce
both a modiﬁed model and new training techniques to tackle this issue.

3 THE NON-AUTOREGRESSIVE TRANSFORMER (NAT)

We introduce a novel NMT model—the Non-Autoregressive Transformer (NAT)—that can produce
an entire output translation in parallel. As shown in Fig. 2, the model is composed of the following
four modules: an encoder stack, a decoder stack, a newly added fertility predictor (details in 3.3),
and a translation predictor for token decoding.

3.1 ENCODER STACK

Similar to the autoregressive Transformer, both the encoder and decoder stacks are composed en-
tirely of feed-forward networks (MLPs) and multi-head attention modules. Since no RNNs are used,
there is no inherent requirement for sequential execution, making non-autoregressive decoding pos-
sible. For our proposed NAT, the encoder stays unchanged from the original Transformer network.

3

Published as a conference paper at ICLR 2018

3.2 DECODER STACK

In order to translate non-autoregressively and parallelize the decoding process, we modify the de-
coder stack as follows.

Decoder Inputs Before decoding starts, the NAT needs to know how long the target sentence
will be in order to generate all words in parallel. More crucially, we cannot use time-shifted target
outputs (during training) or previously predicted outputs (during inference) as the inputs to the ﬁrst
decoder layer. Omitting inputs to the ﬁrst decoder layer entirely, or using only positional embed-
dings, resulted in very poor performance. Instead, we initialize the decoding process using copied
source inputs from the encoder side. As the source and target sentences are often of different lengths,
we propose two methods:

• Copy source inputs uniformly: Each decoder input t is a copy of the Round(T (cid:48)t/T )-th encoder
input. This is equivalent to “scanning” source inputs from left to right with a constant “speed,”
and results in a decoding process that is deterministic given a (predicted) target length.

• Copy source inputs using fertilities: A more powerful way, depicted in Fig. 2 and discussed
in more detail below, is to copy each encoder input as a decoder input zero or more times, with
the number of times each input is copied referred to as that input word’s “fertility.” In this case
the source inputs are scanned from left to right at a “speed” that varies inversely with the fertility
of each input; the decoding process is now conditioned on the sequence of fertilities, while the
resulting output length is determined by the sum of all fertility values.

Non-causal self-attention Without the constraint of an autoregressive factorization of the output
distribution, we no longer need to prevent earlier decoding steps from accessing information from
later steps. Thus we can avoid the causal mask used in the self-attention module of the conventional
Transformer’s decoder.
Instead, we mask out each query position only from attending to itself,
which we found to improve decoder performance relative to unmasked self-attention.

Positional attention We also include an additional positional attention module in each decoder
layer, which is a multi-head attention module with the same general attention mechanism used in
other parts of the Transformer network, i.e.

Attention(Q, K, V ) = softmax

(4)

(cid:19)

(cid:18) QK T
√
dmodel

· V,

where dmodel is the model hidden size, but with the positional encoding1 as both query and key and
the decoder states as the value. This incorporates positional information directly into the attention
process and provides a stronger positional signal than the embedding layer alone. We also hypothe-
size that this additional information improves the decoder’s ability to perform local reordering.

3.3 MODELING FERTILITY TO TACKLE THE MULTIMODALITY PROBLEM

The multimodality problem can be attacked by introducing a latent variable z to directly model
the nondeterminism in the translation process: we ﬁrst sample z from a prior distribution and then
condition on z to non-autoregressively generate a translation.

One way to interpret this latent variable is as a sentence-level “plan” akin to those discussed in the
language production literature (Martin et al., 2010). There are several desirable properties for this
latent variable:

• It should be simple to infer a value for the latent variable given a particular input-output pair, as

this is needed to train the model end-to-end.

• Adding z to the conditioning context should account as much as possible for the correlations
across time between different outputs, so that the remaining marginal probabilities at each output
location are as close as possible to satisfying conditional independence.

• It should not account for the variation in output translations so directly that p(y|x, z) becomes

trivial to learn, since that is the function our decoder neural network will approximate.

1The positional encoding p is computed as p(j, k) = sin (j/10000k/d) (for even k) or cos (j/10000k/d)

(for odd k), where j is the timestep index and k is the channel index.

4

Published as a conference paper at ICLR 2018

The factorization by length introduced in Eq. 3 provides a very weak example of a latent variable
model, satisfying the ﬁrst and third property but not the ﬁrst. We propose the use of fertilities instead.
These are integers for each word in the source sentence that correspond to the number of words in
the target sentence that can be aligned to that source word using a hard alignment algorithm like
IBM Model 2 (Brown et al., 1993).

One of the most important properties of the proposed NAT is that it naturally introduces an informa-
tive latent variable when we choose to copy the encoder inputs based on predicted fertilities. More
precisely, given a source sentence X, the conditional probability of a target translation Y is:




pN A(Y |X; θ) =

pF (ft(cid:48)|x1:T (cid:48); θ) ·

p(yt|x1{f1}, .., xT (cid:48){fT (cid:48)}; θ)

 (5)

(cid:88)

T (cid:48)
(cid:89)



f1,...,fT (cid:48) ∈F

t(cid:48)=1

T
(cid:89)

t=1

where F = {f1, ..., fT (cid:48)| (cid:80)T (cid:48)
t(cid:48)=1 ft(cid:48) = T, ft(cid:48) ∈ Z∗} is the set of all fertility sequences—one fertility
value per source word—that sum to the length of Y and x{f } denotes the token x repeated f times.

Fertility prediction As shown in Fig. 2, we model the fertility pF (ft(cid:48)|x1:T (cid:48)) at each position in-
dependently using a one-layer neural network with a softmax classiﬁer (L = 50 in our experiments)
on top of the output of the last encoder layer. This models the way that fertility values are a property
of each input word but depend on information and context from the entire sentence.

Beneﬁts of fertility Fertilities possess all three of the properties listed earlier as desired of a latent
variable for non-autoregressive machine translation:

• An external aligner provides a simple and fast approximate inference model that effectively re-

duces the unsupervised training problem to two supervised ones.

• Using fertilities as a latent variable makes signiﬁcant progress towards solving the multimodality
problem by providing a natural factorization of the output space. Given a source sentence, restrict-
ing the output distribution to those target sentences consistent with a particular fertility sequence
dramatically reduces the mode space. Furthermore, the global choice of mode is factored into a
set of local mode choices: namely, how to translate each input word. These local mode choices
can be effectively supervised because the fertilities provide a ﬁxed “scaffold.”

• Including both fertilities and reordering in the latent variable would provide complete alignment
statistics. This would make the decoding function trivially easy to approximate given the latent
variable and force all of the modeling complexity into the encoder. Using fertilities alone allows
the decoder to take some of this burden off of the encoder.

Our use of fertilities as a latent variable also means that there is no need to have a separate means of
explicitly modeling the length of the translation, which is simply the sum of fertilities. And fertilities
provide a powerful way to condition the decoding process, allowing the model to generate diverse
translations by sampling over the fertility space.

3.4 TRANSLATION PREDICTOR AND THE DECODING PROCESS

At inference time, the model can identify the translation with the highest conditional probability (see
Eq. 5) by marginalizing over all possible latent fertility sequences. Given a fertility sequence, how-
ever, identifying the optimal translation only requires independently maximizing the local probabil-
ity for each output position. We deﬁne Y = G(x1:T (cid:48), f1:T (cid:48); θ) to represent the optimal translation
given a source sentence and a sequence of fertility values.

But searching and marginalizing over the whole fertility space is still intractable. We propose three
heuristic decoding algorithms to reduce the search space of the NAT model:

Argmax decoding Since the fertility sequence is also modeled with a conditionally independent
factorization, we can simply estimate the best translation by choosing the highest-probability fertility
for each input word:

ˆYargmax = G(x1:T (cid:48), ˆf1:T (cid:48); θ), where ˆft(cid:48) = argmax

pF (ft(cid:48)|x1:T (cid:48); θ)

(6)

f

5

Published as a conference paper at ICLR 2018

Average decoding We can also estimate each fertility as the expectation of its corresponding soft-
max distribution:

ˆYaverage = G(x1:T (cid:48), ˆf1:T (cid:48); θ), where ˆft(cid:48) = Round

pF (ft(cid:48)|x1:T (cid:48); θ)ft(cid:48)

(7)





L
(cid:88)

ft(cid:48) =1





Noisy parallel decoding (NPD) A more accurate approximation of the true optimum of the target
distribution, inspired by Cho (2016), is to draw samples from the fertility space and compute the
best translation for each fertility sequence. We can then use the autoregressive teacher to identify
the best overall translation:

ˆYNPD = G(x1:T (cid:48), argmax
ft(cid:48) ∼pF

pAR(G(x1:T (cid:48), f1:T (cid:48); θ)|X; θ); θ)

(8)

Note that, when using an autoregressive model as a scoring function for a set of decoded translations,
it can run as fast as it does at train time because it can be provided with all decoder inputs in parallel.

NPD is a stochastic search method, and it also increases the computational resources required lin-
early by the sample size. However, because all the search samples can be computed and scored
entirely independently, the process only doubles the latency compared to computing a single trans-
lation if sufﬁcient parallelism is available.

4 TRAINING

latent variable f1:T (cid:48), whose conditional
The proposed NAT contains a discrete sequential
posterior distribution p(f1:T (cid:48)|x1:T (cid:48), y1:T ; θ) we can approximate using a proposal distribution
q(f1:T (cid:48)|x1:T (cid:48), y1:T ). This provides a variational bound for the overall maximum likelihood loss:

LML = log pN A(Y |X; θ) = log

pF (f1:T (cid:48)|x1:T (cid:48); θ) · p(y1:T |x1:T (cid:48), f1:T (cid:48); θ)

(cid:88)

f1:T (cid:48) ∈F



T
(cid:88)






t=1
(cid:124)

≥ E

f1:T (cid:48) ∼q

log p(yt|x1{f1}, .., xT (cid:48){fT (cid:48)}; θ)

+

log pF (ft(cid:48)|x1:T (cid:48); θ)

+ H(q)

T (cid:48)
(cid:88)

t(cid:48)=1
(cid:124)

(cid:125)

(cid:123)(cid:122)
Translation Loss

(cid:123)(cid:122)
Fertility Loss

(9)









(cid:125)

We choose a proposal distribution q deﬁned by a separate, ﬁxed fertility model. Possible options in-
clude the output of an external aligner, which produces a deterministic sequence of integer fertilities
for each (source, target) pair in a training corpus, or fertilities computed from the attention weights
used in our ﬁxed autoregressive teacher model. This simpliﬁes the inference process considerably,
as the expectation over q is deterministic.

The resulting loss function, consisting of the two bracketed terms in Eq. 9, allows us to train the en-
tire model in a supervised fashion, using the inferred fertilities to simultaneously train the translation
model p and supervise the fertility neural network model pF .

4.1 SEQUENCE-LEVEL KNOWLEDGE DISTILLATION

While the latent fertility model substantially improves the ability of the non-autoregressive out-
put distribution to approximate the multimodal target distribution, it does not completely solve the
problem of nondeterminism in the training data. In many cases, there are multiple correct transla-
tions consistent with a single sequence of fertilities—for instance, both “Danke sch¨on.” and “Vielen
dank.” are consistent with the English input “Thank you.” and the fertility sequence [2, 0, 1], because
“you” is not directly translated in either German sentence.

Thus we additionally apply sequence-level knowledge distillation (Kim & Rush, 2016) to construct
a new corpus by training an autoregressive machine translation model, known as the teacher, on an

6

Published as a conference paper at ICLR 2018

existing training corpus, then using that model’s greedy outputs as the targets for training the non-
autoregressive student. The resulting targets are less noisy and more deterministic, as the trained
model will consistently translate a sentence like “Thank you.” into the same German translation
every time; on the other hand, they are also lower in quality than the original dataset.

4.2 FINE-TUNING

Our supervised fertility model enables a decomposition of the overall maximum likelihood loss
into translation and fertility terms, but it has some drawbacks compared to variational training. In
particular, it heavily relies on the deterministic, approximate inference model provided by the ex-
ternal alignment system, while it would be desirable to train the entire model, including the fertility
predictor, end to end.

Thus we propose a ﬁne-tuning step after training the NAT to convergence. We introduce an addi-
tional loss term consisting of the reverse K-L divergence with the teacher output distribution, a form
of word-level knowledge distillation:

LRKL (f1:T (cid:48); θ) =

[log pAR (yt|ˆy1:t−1, x1:T (cid:48)) · pN A (yt|x1:T (cid:48), f1:T (cid:48); θ)] ,

(10)

T
(cid:88)

(cid:88)

t=1

yt

where ˆy1:T = G(x1:T (cid:48), f1:T (cid:48); θ). Such a loss is more favorable towards highly peaked student output
distributions than a standard cross-entropy error would be.

Then we train the whole model jointly with a weighted sum of the original distillation loss and two
such terms, one an expectation over the predicted fertility distribution, normalized with a baseline,
and the other based on the external fertility inference model:

LFT = λ





E

f1:T (cid:48) ∼pF

(cid:124)

(cid:0)LRKL (f1:T (cid:48)) − LRKL
(cid:123)(cid:122)
LRL

f1:T (cid:48) ∼q
(cid:124)

(cid:125)

(cid:123)(cid:122)
LBP

(cid:0) ¯f1:T (cid:48)

(cid:1)(cid:1)

+ E

(LRKL (f1:T (cid:48)))

+ (1 − λ)LKD (11)

where ¯f1:T (cid:48) is the average fertility computed by Eq. 7. The gradient with respect to the non-
differentiable LRL term can be estimated with REINFORCE (Williams, 1992), while the LBP term
can be trained using ordinary backpropagation.







(cid:125)

5 EXPERIMENTS

5.1 EXPERIMENTAL SETTINGS

Dataset We evaluate the proposed NAT on three widely used public machine translation corpora:
IWSLT16 En–De2, WMT14 En–De,3 and WMT16 En–Ro4. We use IWSLT—which is smaller than
the other two datasets—as the development dataset for ablation experiments, and additionally train
and test our primary models on both directions of both WMT datasets. All the data are tokenized and
segmented into subword symbols using byte-pair encoding (BPE) (Sennrich et al., 2015) to restrict
the size of the vocabulary. For both WMT datasets, we use shared BPE vocabulary and additionally
share encoder and decoder word embeddings; for IWSLT, we use separate English and German
vocabulary and embeddings.

Teacher Sequence-level knowledge distillation is applied to alleviate multimodality in the training
dataset, using autoregressive models as the teachers. The same teacher model used for distillation is
also used as a scoring function for ﬁne-tuning and noisy parallel decoding.

To enable a fair comparison, and beneﬁt from its high translation quality, we implemented the au-
toregressive teachers using the state-of-the-art Transformer architecture. In addition, we use the
same sizes and hyperparameters for each student and its respective teacher, with the exception of the
newly added positional self-attention and fertility prediction modules.

2https://wit3.fbk.eu/
3http://www.statmt.org/wmt14/translation-task
4http://www.statmt.org/wmt16/translation-task

7

Published as a conference paper at ICLR 2018

Models

WMT14
En→De De→En

WMT16
En→Ro Ro→En

IWSLT16

En→De

Latency / Speedup

NAT
NAT (+FT)
NAT (+FT + NPD s = 10)
NAT (+FT + NPD s = 100)

Autoregressive (b = 1)
Autoregressive (b = 4)

17.35
17.69
18.66
19.17

22.71
23.45

20.62
21.47
22.41
23.20

26.39
27.02

26.22
27.29
29.02
29.79

31.35
31.91

27.83
29.06
30.76
31.44

31.03
31.76

25.20
26.52
27.44
28.16

28.89
29.70

39 ms
39 ms
79 ms
257 ms

408 ms
607 ms

15.6×
15.6×
7.68×
2.36×

1.49×
1.00×

Table 1: BLEU scores on ofﬁcial test sets (newstest2014 for WMT En-De and newstest2016
for WMT En-Ro) or the development set for IWSLT. NAT models without NPD use argmax decod-
ing. Latency is computed as the time to decode a single sentence without minibatching, averaged
over the whole test set; decoding is implemented in PyTorch on a single NVIDIA Tesla P100.

Preparation for knowledge distillation We ﬁrst
train all teacher models using maximum likelihood,
then freeze their parameters. To avoid the redun-
dancy of running ﬁxed teacher models repeatedly on
the same data, we decode the entire training set once
using each teacher to create a new training dataset
for its respective student.

Encoder initialization We ﬁnd it helpful to initial-
ize the weights in the NAT student’s encoder with the
encoder weights from its teacher, as the autoregres-
sive and non-autoregressive models share the same
encoder input and architecture.

Fertility supervision during training
As de-
scribed above, we supervise the fertility predictions
at train time by using a ﬁxed aligner as a fertility
inference function. We use the fast align5 im-
plementation of IBM Model 2 for this purpose, with
default parameters (Dyer et al., 2013).

Figure 3:
BLEU scores on IWSLT de-
velopment set as a function of sample size
for noisy parallel decoding. NPD matches
the performance of the other two decoding
strategies after two samples, and exceeds the
performance of the autoregressive teacher
with around 1000.

Hyperparameters For experiments on WMT datasets, we use the hyperparameter settings of the
base Transformer model described in Vaswani et al. (2017), though without label smoothing. As
IWSLT is a smaller corpus, and to reduce training time, we use a set of smaller hyperparameters
(dmodel = 287, dhidden = 507, nlayer = 5, nhead = 2, and twarmup = 746) for all experiments on that
dataset. For ﬁne-tuning we use λ = 0.25.

Evaluation metrics We evaluate using tokenized and cased BLEU scores (Papineni et al., 2002).

Implementation We have open-sourced our PyTorch implementation of the NAT6.

5.2 RESULTS

Across the three datasets we used, the NAT performs between 2-5 BLEU points worse than its
autoregressive teacher, with part or all of this gap addressed by the use of noisy parallel decoding. In
the case of WMT16 English–Romanian, NPD improves the performance of our non-autoregressive
model to within 0.2 BLEU points of the previous overall state of the art (Gehring et al., 2017).

Comparing latencies on the development model shows a speedup of more than a factor of 10 over
greedy autoregressive decoding, or a factor of 15 over beam search. Latencies for decoding with
NPD, regardless of sample size, could be reduced to about 80ms by parallelizing across multiple
GPUs because each sample can be generated, then scored, independently from the others.

5https://github.com/clab/fast align
6https://github.com/salesforce/nonauto-nmt

8

Published as a conference paper at ICLR 2018

5.3 ABLATION STUDY

We also conduct an extensive ablation study with the proposed NAT on the IWSLT dataset. First,
we note that the model fails to train when provided with only positional embeddings as input to the
decoder. Second, we see that training on the distillation corpus rather than the ground truth provides
a fairly consistent improvement of around 5 BLEU points. Third, switching from uniform copying
of source inputs to fertility-based copying improves performance by four BLEU points when using
ground-truth training or two when using distillation.

Distillation
b=4
b=1

Decoder Inputs
+uniform +fertility

+PosAtt

Fine-tuning
+LBP

+LRL

+LKD

BLEU BLEU (T)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

≈ 2
16.51
18.87

20.72
21.12
24.02
25.20

22.44
×
×
25.76
26.52

43.91
45.41

×
×
46.11
47.38

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

Table 2: Ablation performance on the IWSLT development set. BLEU (T) refers to the BLEU score
on a version of the development set that has been translated by the teacher model. An × indicates
that ﬁne-tuning caused that model to get worse. When uniform copying is used as the decoder
inputs, the ground-truth target lengths are provided. All models use argmax decoding.

Fine-tuning does not converge with reinforcement learning alone, or with the LBP term alone, but use
of all three ﬁne-tuning terms together leads to an improvement of around 1.5 BLEU points. Training
the student model from a distillation corpus produced using beam search is similar to training from
the greedily-distilled corpus.

Figure 4: Two examples comparing translations produced by an autoregressive (AR) and non-
autoregressive Transformer as well as the result of noisy parallel decoding with sample size 100.
Repeated words are highlighted in gray.

We include two examples of translations from the IWSLT development set in Fig. 4. Instances of
repeated words or phrases, highlighted in gray, are most prevalent in the non-autoregressive output
for the relatively complex ﬁrst example sentence. Two pairs of repeated words in the ﬁrst example, as

9

Published as a conference paper at ICLR 2018

Figure 5: A Romanian–English example translated with noisy parallel decoding. At left are eight
sampled fertility sequences from the encoder, represented with their corresponding decoder input
sequences. Each of these values for the latent variable leads to a different possible output translation,
shown at right. The autoregressive Transformer then picks the best translation, shown in red, a
process which is much faster than directly using it to generate output.

well as a pair in the second, are not present in the versions with noisy parallel decoding, suggesting
that NPD scoring using the teacher model can ﬁlter out such mistakes. The translations produced
by the NAT with NPD, while of a similar quality to those produced by the autoregressive model, are
also noticeably more literal.

We also show an example of the noisy parallel decoding process in Fig. 5, demonstrating the diver-
sity of translations that can be found by sampling from the fertility space.

We introduce a latent variable model for non-autoregressive machine translation that enables a de-
coder based on Vaswani et al. (2017) to take full advantage of its exceptional degree of internal
parallelism even at inference time. As a result, we measure translation latencies of one-tenth that of
an equal-sized autoregressive model, while maintaining competitive BLEU scores.

6 CONCLUSION

REFERENCES

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly

learning to align and translate. In ICLR, 2015.

Peter Brown, Vincent della Pietra, Stephen della Pietra, and Robert Mercer. The mathematics of
statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311,
1993.

Kyunghyun Cho. Noisy parallel approximate decoding for conditional recurrent language model.

arXiv preprint arXiv:1605.03835, 2016.

Chris Dyer, Victor Chahuneau, and Noah Smith. A simple, fast, and effective reparameterization of

IBM Model 2. In NAACL, 2013.

Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. Convolutional

sequence to sequence learning. arXiv preprint arXiv:1705.03122, 2017.

Łukasz Kaiser, Aidan Gomez, and Franc¸ois Chollet. Depthwise separable convolutions for neural

machine translation. arXiv preprint arXiv:1706.03059, 2017.

Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray
Kavukc¸uoˇglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099, 2016.

Yoon Kim and Alexander Rush. Sequence-level knowledge distillation. In EMNLP, 2016.

10

Published as a conference paper at ICLR 2018

Philipp Koehn and Rebecca Knowles. Six challenges for neural machine translation. arXiv preprint

arXiv:1706.03872, 2017.

Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-

based neural machine translation. In EMNLP, 2015.

Randi Martin, Jason Crowther, Meredith Knight, Franklin Tamborello, and Chin-Lung Yang. Plan-
ning in sentence production: Evidence for the phrase as a default planning scope. Cognition, 116
(2):177–192, 2010.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A method for automatic

evaluation of machine translation. In ACL, pp. 311–318, 2002.

Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with

subword units. arXiv preprint arXiv:1508.07909, 2015.

Ilya Sutskever, Oriol Vinyals, and Quˆoc Lˆe. Sequence to sequence learning with neural networks.

In NIPS, 2014.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.

Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine Learning, 8(3-4):229–256, 1992.

Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao,
K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, Ł. Kaiser, S. Gouws, Y. Kato, T. Kudo,
H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick,
O. Vinyals, G. Corrado, M. Hughes, and J. Dean. Google’s neural machine translation system:
Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144,
2016.

11

Published as a conference paper at ICLR 2018

A SCHEMATIC AND ANALYSIS

Figure 6: The schematic structure of training and inference for the NAT. The “distilled data” contains
target sentences decoded by the autoregressive model and ground-truth source sentences.

Figure 7: The translation latency, computed as the time to decode a single sentence without mini-
batching, for each sentence in the IWSLT development set as a function of its length. The autore-
gressive model has latency linear in the decoding length, while the latency of the NAT is nearly
constant for typical lengths, even with NPD with sample size 10. When using NPD with sample
size 100, the level of parallelism is enough to more than saturate the GPU, leading again to linear
latencies.

12

Published as a conference paper at ICLR 2018

Figure 8: Learning curves for training and ﬁne-tuning of the NAT on IWSLT. BLEU scores are on
the development set.

13

8
1
0
2
 
r
a

M
 
9
 
 
]
L
C
.
s
c
[
 
 
2
v
1
8
2
2
0
.
1
1
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

NON-AUTOREGRESSIVE
NEURAL MACHINE TRANSLATION

Jiatao Gu†∗, James Bradbury‡, Caiming Xiong‡, Victor O.K. Li†& Richard Socher‡
‡Salesforce Research
{james.bradbury,cxiong,rsocher}@salesforce.com
†The University of Hong Kong
{jiataogu, vli}@eee.hku.hk

ABSTRACT

Existing approaches to neural machine translation condition each output word on
previously generated outputs. We introduce a model that avoids this autoregressive
property and produces its outputs in parallel, allowing an order of magnitude lower
latency during inference. Through knowledge distillation, the use of input token
fertilities as a latent variable, and policy gradient ﬁne-tuning, we achieve this at
a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer
network used as a teacher. We demonstrate substantial cumulative improvements
associated with each of the three aspects of our training strategy, and validate
our approach on IWSLT 2016 English–German and two WMT language pairs.
By sampling fertilities in parallel at inference time, our non-autoregressive model
achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–
Romanian.

1

INTRODUCTION

Neural network based models outperform traditional statistical models for machine translation (MT)
(Bahdanau et al., 2015; Luong et al., 2015). However, state-of-the-art neural models are much
slower than statistical MT approaches at inference time (Wu et al., 2016). Both model families use
autoregressive decoders that operate one step at a time: they generate each token conditioned on
the sequence of tokens previously generated. This process is not parallelizable, and, in the case of
neural MT models, it is particularly slow because a computationally intensive neural network is used
to generate each token.

While several recently proposed models avoid recurrence at train time by leveraging convolu-
tions (Kalchbrenner et al., 2016; Gehring et al., 2017; Kaiser et al., 2017) or self-attention (Vaswani
et al., 2017) as more-parallelizable alternatives to recurrent neural networks (RNNs), use of autore-
gressive decoding makes it impossible to take full advantage of parallelism during inference.

We introduce a non-autoregressive translation model based on the Transformer network (Vaswani
et al., 2017). We modify the encoder of the original Transformer network by adding a module that
predicts fertilities, sequences of numbers that form an important component of many traditional
machine translation models (Brown et al., 1993). These fertilities are supervised during training
and provide the decoder at inference time with a globally consistent plan on which to condition its
simultaneously computed outputs.

2 BACKGROUND

2.1 AUTOREGRESSIVE NEURAL MACHINE TRANSLATION

Given a source sentence X = {x1, ..., xT (cid:48)}, a neural machine translation model factors the distribu-
tion over possible output sentences Y = {y1, ..., yT } into a chain of conditional probabilities with a

∗This work was completed while the ﬁrst author was interning at Salesforce Research.

1

Published as a conference paper at ICLR 2018

left-to-right causal structure:

T +1
(cid:89)

t=1

T +1
(cid:88)

t=1

pAR(Y |X; θ) =

p(yt|y0:t−1, x1:T (cid:48); θ),

(1)

where the special tokens y0 (e.g. (cid:104)bos(cid:105)) and yT +1 (e.g. (cid:104)eos(cid:105)) are used to represent the beginning
and end of all target sentences. These conditional probabilities are parameterized using a neural
network. Typically, an encoder-decoder architecture (Sutskever et al., 2014) with a unidirectional
RNN-based decoder is used to capture the causal structure of the output distribution.

Maximum Likelihood training Choosing to factorize the machine translation output distribution
autoregressively enables straightforward maximum likelihood training with a cross-entropy loss ap-
plied at each decoding step:

LML = log pAR(Y |X; θ) =

log p(yt|y0:t−1, x1:T (cid:48); θ).

(2)

This loss provides direct supervision for each conditional probability prediction.

Autoregressive NMT without RNNs Since the entire target translation is known at training time,
the calculation of later conditional probabilities (and their corresponding losses) does not depend on
the output words chosen during earlier decoding steps. Even though decoding must remain entirely
sequential during inference, models can take advantage of this parallelism during training. One such
approach replaces recurrent layers in the decoder with masked convolution layers (Kalchbrenner
et al., 2016; Gehring et al., 2017) that provide the causal structure required by the autoregressive
factorization.

A recently introduced option which reduces sequential computation still further is to construct the
decoder layers out of self-attention computations that have been causally masked in an analogous
way. The state-of-the-art Transformer network takes this approach, which allows information to ﬂow
in the decoder across arbitrarily long distances in a constant number of operations, asymptotically
fewer than required by convolutional architectures (Vaswani et al., 2017).

2.2 NON-AUTOREGRESSIVE DECODING

Pros and cons of autoregressive decoding The autoregressive factorization used by conventional
NMT models has several beneﬁts. It corresponds to the word-by-word nature of human language
production and effectively captures the distribution of real translations. Autoregressive models
achieve state-of-the-art performance on large-scale corpora and are easy to train, while beam search
provides an effective local search method for ﬁnding approximately-optimal output translations.

But there are also drawbacks. As the individual steps of the de-
coder must be run sequentially rather than in parallel, autore-
gressive decoding prevents architectures like the Transformer
from fully realizing their train-time performance advantage
during inference. Meanwhile, beam search suffers from dimin-
ishing returns with respect to beam size (Koehn & Knowles,
2017) and exhibits limited search parallelism because it intro-
duces computational dependence between beams.

Towards non-autoregressive decoding A na¨ıve solution is
to remove the autoregressive connection directly from an exist-
ing encoder-decoder model. Assuming that the target sequence
length T can be modeled with a separate conditional distribu-
tion pL, this becomes

pN A(Y |X; θ) = pL(T |x1:T (cid:48); θ) ·

p(yt|x1:T (cid:48); θ).

(3)

T
(cid:89)

t=1

Figure 1:
Translating “A B C”
to “X Y” using autoregressive and
non-autoregressive neural MT ar-
chitectures. The latter generates all
output tokens in parallel.

This model still has an explicit likelihood function, and it can still be trained using independent
cross-entropy losses on each output distribution. Now, however, these distributions can be computed
in parallel at inference time.

2

Published as a conference paper at ICLR 2018

Figure 2: The architecture of the NAT, where the black solid arrows represent differentiable con-
nections and the purple dashed arrows are non-differentiable operations. Each sublayer inside the
encoder and decoder stacks also includes layer normalization and a residual connection.

2.3 THE MULTIMODALITY PROBLEM

However, this na¨ıve approach does not yield good results, because such a model exhibits complete
conditional independence. Each token’s distribution p(yt) depends only on the source sentence X.
This makes it a poor approximation to the true target distribution, which exhibits strong correlation
across time. Intuitively, such a decoder is akin to a panel of human translators each asked to provide
a single word of a translation independently of the words their colleagues choose.

In particular, consider an English source sentence like “Thank you.” This can be accurately trans-
lated into German as any one of “Danke.”, “Danke sch¨on.”, or “Vielen Dank.”, all of which may
occur in a given training corpus. This target distribution cannot be represented as a product of
independent probability distributions for each of the ﬁrst, second, and third words, because a con-
ditionally independent distribution cannot allow “Danke sch¨on.” and “Vielen Dank.” without also
licensing “Danke Dank.” and “Vielen sch¨on.”

The conditional independence assumption prevents a model from properly capturing the highly mul-
timodal distribution of target translations. We call this the “multimodality problem” and introduce
both a modiﬁed model and new training techniques to tackle this issue.

3 THE NON-AUTOREGRESSIVE TRANSFORMER (NAT)

We introduce a novel NMT model—the Non-Autoregressive Transformer (NAT)—that can produce
an entire output translation in parallel. As shown in Fig. 2, the model is composed of the following
four modules: an encoder stack, a decoder stack, a newly added fertility predictor (details in 3.3),
and a translation predictor for token decoding.

3.1 ENCODER STACK

Similar to the autoregressive Transformer, both the encoder and decoder stacks are composed en-
tirely of feed-forward networks (MLPs) and multi-head attention modules. Since no RNNs are used,
there is no inherent requirement for sequential execution, making non-autoregressive decoding pos-
sible. For our proposed NAT, the encoder stays unchanged from the original Transformer network.

3

Published as a conference paper at ICLR 2018

3.2 DECODER STACK

In order to translate non-autoregressively and parallelize the decoding process, we modify the de-
coder stack as follows.

Decoder Inputs Before decoding starts, the NAT needs to know how long the target sentence
will be in order to generate all words in parallel. More crucially, we cannot use time-shifted target
outputs (during training) or previously predicted outputs (during inference) as the inputs to the ﬁrst
decoder layer. Omitting inputs to the ﬁrst decoder layer entirely, or using only positional embed-
dings, resulted in very poor performance. Instead, we initialize the decoding process using copied
source inputs from the encoder side. As the source and target sentences are often of different lengths,
we propose two methods:

• Copy source inputs uniformly: Each decoder input t is a copy of the Round(T (cid:48)t/T )-th encoder
input. This is equivalent to “scanning” source inputs from left to right with a constant “speed,”
and results in a decoding process that is deterministic given a (predicted) target length.

• Copy source inputs using fertilities: A more powerful way, depicted in Fig. 2 and discussed
in more detail below, is to copy each encoder input as a decoder input zero or more times, with
the number of times each input is copied referred to as that input word’s “fertility.” In this case
the source inputs are scanned from left to right at a “speed” that varies inversely with the fertility
of each input; the decoding process is now conditioned on the sequence of fertilities, while the
resulting output length is determined by the sum of all fertility values.

Non-causal self-attention Without the constraint of an autoregressive factorization of the output
distribution, we no longer need to prevent earlier decoding steps from accessing information from
later steps. Thus we can avoid the causal mask used in the self-attention module of the conventional
Transformer’s decoder.
Instead, we mask out each query position only from attending to itself,
which we found to improve decoder performance relative to unmasked self-attention.

Positional attention We also include an additional positional attention module in each decoder
layer, which is a multi-head attention module with the same general attention mechanism used in
other parts of the Transformer network, i.e.

Attention(Q, K, V ) = softmax

(4)

(cid:19)

(cid:18) QK T
√
dmodel

· V,

where dmodel is the model hidden size, but with the positional encoding1 as both query and key and
the decoder states as the value. This incorporates positional information directly into the attention
process and provides a stronger positional signal than the embedding layer alone. We also hypothe-
size that this additional information improves the decoder’s ability to perform local reordering.

3.3 MODELING FERTILITY TO TACKLE THE MULTIMODALITY PROBLEM

The multimodality problem can be attacked by introducing a latent variable z to directly model
the nondeterminism in the translation process: we ﬁrst sample z from a prior distribution and then
condition on z to non-autoregressively generate a translation.

One way to interpret this latent variable is as a sentence-level “plan” akin to those discussed in the
language production literature (Martin et al., 2010). There are several desirable properties for this
latent variable:

• It should be simple to infer a value for the latent variable given a particular input-output pair, as

this is needed to train the model end-to-end.

• Adding z to the conditioning context should account as much as possible for the correlations
across time between different outputs, so that the remaining marginal probabilities at each output
location are as close as possible to satisfying conditional independence.

• It should not account for the variation in output translations so directly that p(y|x, z) becomes

trivial to learn, since that is the function our decoder neural network will approximate.

1The positional encoding p is computed as p(j, k) = sin (j/10000k/d) (for even k) or cos (j/10000k/d)

(for odd k), where j is the timestep index and k is the channel index.

4

Published as a conference paper at ICLR 2018

The factorization by length introduced in Eq. 3 provides a very weak example of a latent variable
model, satisfying the ﬁrst and third property but not the ﬁrst. We propose the use of fertilities instead.
These are integers for each word in the source sentence that correspond to the number of words in
the target sentence that can be aligned to that source word using a hard alignment algorithm like
IBM Model 2 (Brown et al., 1993).

One of the most important properties of the proposed NAT is that it naturally introduces an informa-
tive latent variable when we choose to copy the encoder inputs based on predicted fertilities. More
precisely, given a source sentence X, the conditional probability of a target translation Y is:




pN A(Y |X; θ) =

pF (ft(cid:48)|x1:T (cid:48); θ) ·

p(yt|x1{f1}, .., xT (cid:48){fT (cid:48)}; θ)

 (5)

(cid:88)

T (cid:48)
(cid:89)



f1,...,fT (cid:48) ∈F

t(cid:48)=1

T
(cid:89)

t=1

where F = {f1, ..., fT (cid:48)| (cid:80)T (cid:48)
t(cid:48)=1 ft(cid:48) = T, ft(cid:48) ∈ Z∗} is the set of all fertility sequences—one fertility
value per source word—that sum to the length of Y and x{f } denotes the token x repeated f times.

Fertility prediction As shown in Fig. 2, we model the fertility pF (ft(cid:48)|x1:T (cid:48)) at each position in-
dependently using a one-layer neural network with a softmax classiﬁer (L = 50 in our experiments)
on top of the output of the last encoder layer. This models the way that fertility values are a property
of each input word but depend on information and context from the entire sentence.

Beneﬁts of fertility Fertilities possess all three of the properties listed earlier as desired of a latent
variable for non-autoregressive machine translation:

• An external aligner provides a simple and fast approximate inference model that effectively re-

duces the unsupervised training problem to two supervised ones.

• Using fertilities as a latent variable makes signiﬁcant progress towards solving the multimodality
problem by providing a natural factorization of the output space. Given a source sentence, restrict-
ing the output distribution to those target sentences consistent with a particular fertility sequence
dramatically reduces the mode space. Furthermore, the global choice of mode is factored into a
set of local mode choices: namely, how to translate each input word. These local mode choices
can be effectively supervised because the fertilities provide a ﬁxed “scaffold.”

• Including both fertilities and reordering in the latent variable would provide complete alignment
statistics. This would make the decoding function trivially easy to approximate given the latent
variable and force all of the modeling complexity into the encoder. Using fertilities alone allows
the decoder to take some of this burden off of the encoder.

Our use of fertilities as a latent variable also means that there is no need to have a separate means of
explicitly modeling the length of the translation, which is simply the sum of fertilities. And fertilities
provide a powerful way to condition the decoding process, allowing the model to generate diverse
translations by sampling over the fertility space.

3.4 TRANSLATION PREDICTOR AND THE DECODING PROCESS

At inference time, the model can identify the translation with the highest conditional probability (see
Eq. 5) by marginalizing over all possible latent fertility sequences. Given a fertility sequence, how-
ever, identifying the optimal translation only requires independently maximizing the local probabil-
ity for each output position. We deﬁne Y = G(x1:T (cid:48), f1:T (cid:48); θ) to represent the optimal translation
given a source sentence and a sequence of fertility values.

But searching and marginalizing over the whole fertility space is still intractable. We propose three
heuristic decoding algorithms to reduce the search space of the NAT model:

Argmax decoding Since the fertility sequence is also modeled with a conditionally independent
factorization, we can simply estimate the best translation by choosing the highest-probability fertility
for each input word:

ˆYargmax = G(x1:T (cid:48), ˆf1:T (cid:48); θ), where ˆft(cid:48) = argmax

pF (ft(cid:48)|x1:T (cid:48); θ)

(6)

f

5

Published as a conference paper at ICLR 2018

Average decoding We can also estimate each fertility as the expectation of its corresponding soft-
max distribution:

ˆYaverage = G(x1:T (cid:48), ˆf1:T (cid:48); θ), where ˆft(cid:48) = Round

pF (ft(cid:48)|x1:T (cid:48); θ)ft(cid:48)

(7)





L
(cid:88)

ft(cid:48) =1





Noisy parallel decoding (NPD) A more accurate approximation of the true optimum of the target
distribution, inspired by Cho (2016), is to draw samples from the fertility space and compute the
best translation for each fertility sequence. We can then use the autoregressive teacher to identify
the best overall translation:

ˆYNPD = G(x1:T (cid:48), argmax
ft(cid:48) ∼pF

pAR(G(x1:T (cid:48), f1:T (cid:48); θ)|X; θ); θ)

(8)

Note that, when using an autoregressive model as a scoring function for a set of decoded translations,
it can run as fast as it does at train time because it can be provided with all decoder inputs in parallel.

NPD is a stochastic search method, and it also increases the computational resources required lin-
early by the sample size. However, because all the search samples can be computed and scored
entirely independently, the process only doubles the latency compared to computing a single trans-
lation if sufﬁcient parallelism is available.

4 TRAINING

latent variable f1:T (cid:48), whose conditional
The proposed NAT contains a discrete sequential
posterior distribution p(f1:T (cid:48)|x1:T (cid:48), y1:T ; θ) we can approximate using a proposal distribution
q(f1:T (cid:48)|x1:T (cid:48), y1:T ). This provides a variational bound for the overall maximum likelihood loss:

LML = log pN A(Y |X; θ) = log

pF (f1:T (cid:48)|x1:T (cid:48); θ) · p(y1:T |x1:T (cid:48), f1:T (cid:48); θ)

(cid:88)

f1:T (cid:48) ∈F



T
(cid:88)






t=1
(cid:124)

≥ E

f1:T (cid:48) ∼q

log p(yt|x1{f1}, .., xT (cid:48){fT (cid:48)}; θ)

+

log pF (ft(cid:48)|x1:T (cid:48); θ)

+ H(q)

T (cid:48)
(cid:88)

t(cid:48)=1
(cid:124)

(cid:125)

(cid:123)(cid:122)
Translation Loss

(cid:123)(cid:122)
Fertility Loss

(9)









(cid:125)

We choose a proposal distribution q deﬁned by a separate, ﬁxed fertility model. Possible options in-
clude the output of an external aligner, which produces a deterministic sequence of integer fertilities
for each (source, target) pair in a training corpus, or fertilities computed from the attention weights
used in our ﬁxed autoregressive teacher model. This simpliﬁes the inference process considerably,
as the expectation over q is deterministic.

The resulting loss function, consisting of the two bracketed terms in Eq. 9, allows us to train the en-
tire model in a supervised fashion, using the inferred fertilities to simultaneously train the translation
model p and supervise the fertility neural network model pF .

4.1 SEQUENCE-LEVEL KNOWLEDGE DISTILLATION

While the latent fertility model substantially improves the ability of the non-autoregressive out-
put distribution to approximate the multimodal target distribution, it does not completely solve the
problem of nondeterminism in the training data. In many cases, there are multiple correct transla-
tions consistent with a single sequence of fertilities—for instance, both “Danke sch¨on.” and “Vielen
dank.” are consistent with the English input “Thank you.” and the fertility sequence [2, 0, 1], because
“you” is not directly translated in either German sentence.

Thus we additionally apply sequence-level knowledge distillation (Kim & Rush, 2016) to construct
a new corpus by training an autoregressive machine translation model, known as the teacher, on an

6

Published as a conference paper at ICLR 2018

existing training corpus, then using that model’s greedy outputs as the targets for training the non-
autoregressive student. The resulting targets are less noisy and more deterministic, as the trained
model will consistently translate a sentence like “Thank you.” into the same German translation
every time; on the other hand, they are also lower in quality than the original dataset.

4.2 FINE-TUNING

Our supervised fertility model enables a decomposition of the overall maximum likelihood loss
into translation and fertility terms, but it has some drawbacks compared to variational training. In
particular, it heavily relies on the deterministic, approximate inference model provided by the ex-
ternal alignment system, while it would be desirable to train the entire model, including the fertility
predictor, end to end.

Thus we propose a ﬁne-tuning step after training the NAT to convergence. We introduce an addi-
tional loss term consisting of the reverse K-L divergence with the teacher output distribution, a form
of word-level knowledge distillation:

LRKL (f1:T (cid:48); θ) =

[log pAR (yt|ˆy1:t−1, x1:T (cid:48)) · pN A (yt|x1:T (cid:48), f1:T (cid:48); θ)] ,

(10)

T
(cid:88)

(cid:88)

t=1

yt

where ˆy1:T = G(x1:T (cid:48), f1:T (cid:48); θ). Such a loss is more favorable towards highly peaked student output
distributions than a standard cross-entropy error would be.

Then we train the whole model jointly with a weighted sum of the original distillation loss and two
such terms, one an expectation over the predicted fertility distribution, normalized with a baseline,
and the other based on the external fertility inference model:

LFT = λ





E

f1:T (cid:48) ∼pF

(cid:124)

(cid:0)LRKL (f1:T (cid:48)) − LRKL
(cid:123)(cid:122)
LRL

f1:T (cid:48) ∼q
(cid:124)

(cid:125)

(cid:123)(cid:122)
LBP

(cid:0) ¯f1:T (cid:48)

(cid:1)(cid:1)

+ E

(LRKL (f1:T (cid:48)))

+ (1 − λ)LKD (11)

where ¯f1:T (cid:48) is the average fertility computed by Eq. 7. The gradient with respect to the non-
differentiable LRL term can be estimated with REINFORCE (Williams, 1992), while the LBP term
can be trained using ordinary backpropagation.







(cid:125)

5 EXPERIMENTS

5.1 EXPERIMENTAL SETTINGS

Dataset We evaluate the proposed NAT on three widely used public machine translation corpora:
IWSLT16 En–De2, WMT14 En–De,3 and WMT16 En–Ro4. We use IWSLT—which is smaller than
the other two datasets—as the development dataset for ablation experiments, and additionally train
and test our primary models on both directions of both WMT datasets. All the data are tokenized and
segmented into subword symbols using byte-pair encoding (BPE) (Sennrich et al., 2015) to restrict
the size of the vocabulary. For both WMT datasets, we use shared BPE vocabulary and additionally
share encoder and decoder word embeddings; for IWSLT, we use separate English and German
vocabulary and embeddings.

Teacher Sequence-level knowledge distillation is applied to alleviate multimodality in the training
dataset, using autoregressive models as the teachers. The same teacher model used for distillation is
also used as a scoring function for ﬁne-tuning and noisy parallel decoding.

To enable a fair comparison, and beneﬁt from its high translation quality, we implemented the au-
toregressive teachers using the state-of-the-art Transformer architecture. In addition, we use the
same sizes and hyperparameters for each student and its respective teacher, with the exception of the
newly added positional self-attention and fertility prediction modules.

2https://wit3.fbk.eu/
3http://www.statmt.org/wmt14/translation-task
4http://www.statmt.org/wmt16/translation-task

7

Published as a conference paper at ICLR 2018

Models

WMT14
En→De De→En

WMT16
En→Ro Ro→En

IWSLT16

En→De

Latency / Speedup

NAT
NAT (+FT)
NAT (+FT + NPD s = 10)
NAT (+FT + NPD s = 100)

Autoregressive (b = 1)
Autoregressive (b = 4)

17.35
17.69
18.66
19.17

22.71
23.45

20.62
21.47
22.41
23.20

26.39
27.02

26.22
27.29
29.02
29.79

31.35
31.91

27.83
29.06
30.76
31.44

31.03
31.76

25.20
26.52
27.44
28.16

28.89
29.70

39 ms
39 ms
79 ms
257 ms

408 ms
607 ms

15.6×
15.6×
7.68×
2.36×

1.49×
1.00×

Table 1: BLEU scores on ofﬁcial test sets (newstest2014 for WMT En-De and newstest2016
for WMT En-Ro) or the development set for IWSLT. NAT models without NPD use argmax decod-
ing. Latency is computed as the time to decode a single sentence without minibatching, averaged
over the whole test set; decoding is implemented in PyTorch on a single NVIDIA Tesla P100.

Preparation for knowledge distillation We ﬁrst
train all teacher models using maximum likelihood,
then freeze their parameters. To avoid the redun-
dancy of running ﬁxed teacher models repeatedly on
the same data, we decode the entire training set once
using each teacher to create a new training dataset
for its respective student.

Encoder initialization We ﬁnd it helpful to initial-
ize the weights in the NAT student’s encoder with the
encoder weights from its teacher, as the autoregres-
sive and non-autoregressive models share the same
encoder input and architecture.

Fertility supervision during training
As de-
scribed above, we supervise the fertility predictions
at train time by using a ﬁxed aligner as a fertility
inference function. We use the fast align5 im-
plementation of IBM Model 2 for this purpose, with
default parameters (Dyer et al., 2013).

Figure 3:
BLEU scores on IWSLT de-
velopment set as a function of sample size
for noisy parallel decoding. NPD matches
the performance of the other two decoding
strategies after two samples, and exceeds the
performance of the autoregressive teacher
with around 1000.

Hyperparameters For experiments on WMT datasets, we use the hyperparameter settings of the
base Transformer model described in Vaswani et al. (2017), though without label smoothing. As
IWSLT is a smaller corpus, and to reduce training time, we use a set of smaller hyperparameters
(dmodel = 287, dhidden = 507, nlayer = 5, nhead = 2, and twarmup = 746) for all experiments on that
dataset. For ﬁne-tuning we use λ = 0.25.

Evaluation metrics We evaluate using tokenized and cased BLEU scores (Papineni et al., 2002).

Implementation We have open-sourced our PyTorch implementation of the NAT6.

5.2 RESULTS

Across the three datasets we used, the NAT performs between 2-5 BLEU points worse than its
autoregressive teacher, with part or all of this gap addressed by the use of noisy parallel decoding. In
the case of WMT16 English–Romanian, NPD improves the performance of our non-autoregressive
model to within 0.2 BLEU points of the previous overall state of the art (Gehring et al., 2017).

Comparing latencies on the development model shows a speedup of more than a factor of 10 over
greedy autoregressive decoding, or a factor of 15 over beam search. Latencies for decoding with
NPD, regardless of sample size, could be reduced to about 80ms by parallelizing across multiple
GPUs because each sample can be generated, then scored, independently from the others.

5https://github.com/clab/fast align
6https://github.com/salesforce/nonauto-nmt

8

Published as a conference paper at ICLR 2018

5.3 ABLATION STUDY

We also conduct an extensive ablation study with the proposed NAT on the IWSLT dataset. First,
we note that the model fails to train when provided with only positional embeddings as input to the
decoder. Second, we see that training on the distillation corpus rather than the ground truth provides
a fairly consistent improvement of around 5 BLEU points. Third, switching from uniform copying
of source inputs to fertility-based copying improves performance by four BLEU points when using
ground-truth training or two when using distillation.

Distillation
b=4
b=1

Decoder Inputs
+uniform +fertility

+PosAtt

Fine-tuning
+LBP

+LRL

+LKD

BLEU BLEU (T)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

≈ 2
16.51
18.87

20.72
21.12
24.02
25.20

22.44
×
×
25.76
26.52

43.91
45.41

×
×
46.11
47.38

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

Table 2: Ablation performance on the IWSLT development set. BLEU (T) refers to the BLEU score
on a version of the development set that has been translated by the teacher model. An × indicates
that ﬁne-tuning caused that model to get worse. When uniform copying is used as the decoder
inputs, the ground-truth target lengths are provided. All models use argmax decoding.

Fine-tuning does not converge with reinforcement learning alone, or with the LBP term alone, but use
of all three ﬁne-tuning terms together leads to an improvement of around 1.5 BLEU points. Training
the student model from a distillation corpus produced using beam search is similar to training from
the greedily-distilled corpus.

Figure 4: Two examples comparing translations produced by an autoregressive (AR) and non-
autoregressive Transformer as well as the result of noisy parallel decoding with sample size 100.
Repeated words are highlighted in gray.

We include two examples of translations from the IWSLT development set in Fig. 4. Instances of
repeated words or phrases, highlighted in gray, are most prevalent in the non-autoregressive output
for the relatively complex ﬁrst example sentence. Two pairs of repeated words in the ﬁrst example, as

9

Published as a conference paper at ICLR 2018

Figure 5: A Romanian–English example translated with noisy parallel decoding. At left are eight
sampled fertility sequences from the encoder, represented with their corresponding decoder input
sequences. Each of these values for the latent variable leads to a different possible output translation,
shown at right. The autoregressive Transformer then picks the best translation, shown in red, a
process which is much faster than directly using it to generate output.

well as a pair in the second, are not present in the versions with noisy parallel decoding, suggesting
that NPD scoring using the teacher model can ﬁlter out such mistakes. The translations produced
by the NAT with NPD, while of a similar quality to those produced by the autoregressive model, are
also noticeably more literal.

We also show an example of the noisy parallel decoding process in Fig. 5, demonstrating the diver-
sity of translations that can be found by sampling from the fertility space.

We introduce a latent variable model for non-autoregressive machine translation that enables a de-
coder based on Vaswani et al. (2017) to take full advantage of its exceptional degree of internal
parallelism even at inference time. As a result, we measure translation latencies of one-tenth that of
an equal-sized autoregressive model, while maintaining competitive BLEU scores.

6 CONCLUSION

REFERENCES

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly

learning to align and translate. In ICLR, 2015.

Peter Brown, Vincent della Pietra, Stephen della Pietra, and Robert Mercer. The mathematics of
statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311,
1993.

Kyunghyun Cho. Noisy parallel approximate decoding for conditional recurrent language model.

arXiv preprint arXiv:1605.03835, 2016.

Chris Dyer, Victor Chahuneau, and Noah Smith. A simple, fast, and effective reparameterization of

IBM Model 2. In NAACL, 2013.

Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. Convolutional

sequence to sequence learning. arXiv preprint arXiv:1705.03122, 2017.

Łukasz Kaiser, Aidan Gomez, and Franc¸ois Chollet. Depthwise separable convolutions for neural

machine translation. arXiv preprint arXiv:1706.03059, 2017.

Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray
Kavukc¸uoˇglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099, 2016.

Yoon Kim and Alexander Rush. Sequence-level knowledge distillation. In EMNLP, 2016.

10

Published as a conference paper at ICLR 2018

Philipp Koehn and Rebecca Knowles. Six challenges for neural machine translation. arXiv preprint

arXiv:1706.03872, 2017.

Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-

based neural machine translation. In EMNLP, 2015.

Randi Martin, Jason Crowther, Meredith Knight, Franklin Tamborello, and Chin-Lung Yang. Plan-
ning in sentence production: Evidence for the phrase as a default planning scope. Cognition, 116
(2):177–192, 2010.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A method for automatic

evaluation of machine translation. In ACL, pp. 311–318, 2002.

Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with

subword units. arXiv preprint arXiv:1508.07909, 2015.

Ilya Sutskever, Oriol Vinyals, and Quˆoc Lˆe. Sequence to sequence learning with neural networks.

In NIPS, 2014.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.

Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine Learning, 8(3-4):229–256, 1992.

Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao,
K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, Ł. Kaiser, S. Gouws, Y. Kato, T. Kudo,
H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick,
O. Vinyals, G. Corrado, M. Hughes, and J. Dean. Google’s neural machine translation system:
Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144,
2016.

11

Published as a conference paper at ICLR 2018

A SCHEMATIC AND ANALYSIS

Figure 6: The schematic structure of training and inference for the NAT. The “distilled data” contains
target sentences decoded by the autoregressive model and ground-truth source sentences.

Figure 7: The translation latency, computed as the time to decode a single sentence without mini-
batching, for each sentence in the IWSLT development set as a function of its length. The autore-
gressive model has latency linear in the decoding length, while the latency of the NAT is nearly
constant for typical lengths, even with NPD with sample size 10. When using NPD with sample
size 100, the level of parallelism is enough to more than saturate the GPU, leading again to linear
latencies.

12

Published as a conference paper at ICLR 2018

Figure 8: Learning curves for training and ﬁne-tuning of the NAT on IWSLT. BLEU scores are on
the development set.

13

