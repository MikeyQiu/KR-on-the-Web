Zooming Cautiously:
Linear-Memory Heuristic Search With Node Expansion Guarantees

Laurent Orseau1, Levi H. S. Lelis2, Tor Lattimore1
1DeepMind, UK
2Universidade Federal de Viçosa, Brazil
{lorseau, lattimore}@google.com, levi.lelis@ufv.br

9
1
0
2
 
n
u
J
 
7
 
 
]
I

A
.
s
c
[
 
 
1
v
2
4
2
3
0
.
6
0
9
1
:
v
i
X
r
a

Abstract

We introduce1 and analyze two parameter-free
linear-memory tree search algorithms.
Under
mild assumptions we prove our algorithms are
guaranteed to perform only a logarithmic factor
more node expansions than A∗ when the search
the best guarantee
space is a tree. Previously,
for a linear-memory algorithm under
similar
assumptions was achieved by IDA∗, which in the
worst case expands quadratically more nodes than
in its last
Empirical results support
the theory and demonstrate the practicality and
robustness of our algorithms. Furthermore, they are
fast and easy to implement.

iteration.

1 Introduction
The A∗ algorithm [Hart et al., 1968] is optimal in the sense
that it only expands nodes for which the cost is smaller than
the goal [Dechter and Pearl, 1983]. Unfortunately, in the
worst case the memory use of A∗ grows linearly with its
running time, which can be a bottleneck for large problems.
Thus, it is often preferable to have algorithms with a smaller
memory footprint, possibly at the expense of an increased
search time. The best one can expect from a complete
algorithm is that the memory depends linearly on the solution
depth [Korf, 1985].

The landmark algorithm for low-memory search is IDA∗
[Korf, 1985]. At
in problem domains
the same time,
that can be represented by a tree, IDA∗ is guaranteed to
expand at most O(N 2
∗ ) nodes, where N∗ is the number of
expansions required by A∗ with ties broken for the worst case,
assuming the search space is a tree. Although such theoretical
guarantees are welcome, in many problems IDA∗ really does
expand Ω(N 2
∗ ) nodes. This occurs especially in domains with
a large diversity of edge costs [Sarkar et al., 1991]. Several
methods have been developed to mitigate this issue [Burns
and Ruml, 2013; Sharon et al., 2014; Hatem et al., 2018;
Sarkar et al., 1991; Wah and Shang, 1994], but theoretical

1This paper and another independent IJCAI 2019 submission
have been merged into a single paper that subsumes both of
them [Helmert et al., 2019]. This paper is placed here only for
historical context. Please only cite the subsuming paper.

guarantees (when provided) depend on strong assumptions
such as uniformity of the costs or of the branching factor
[Hatem et al., 2015, for example (cf. Assumptions 1, 2
and 3)]. Another low-memory search algorithm is RBFS
[Korf, 1993], which has the same worst-case number of node
expansions as IDA∗.

There is a large literature on algorithms that accept a
memory budget as a parameter [Sen and Bagchi, 1989;
Russell, 1992; Zhou and Hansen, 2003, and many others].
Our focus, however, is on lightweight algorithms for which
the memory footprint grows linearly with the search depth.
Although we restrict our analysis to tree search, we expect
that
techniques for graph search, such as transposition
tables [Reinefeld and Marsland, 1994], can be incorporated
into our approach.

Like A∗, our algorithms use a cost function f on the nodes
that we assume to be nondecreasing from parent to child.
Importantly, and in contrast to previous works, we make no
regularity assumptions on the search tree, such as polynomial
or exponential growth, or on the distribution of f -values in
the search tree.

We propose two novel parameter-free algorithms that enjoy
theoretical guarantees on the number of node expansions
relative to A∗ for tree search and use memory that grows
linearly with the search depth. The ﬁrst algorithm, Zoomer,
expands at most O(N∗ log(θ∗/δmin)) nodes, where θ∗ is
the cost of an optimal solution and δmin is the minimum
difference of cost
thresholds between two iterations of
IDA∗—it is bounded below by the numerical precision, and
for integer costs δmin = 1. The log factor corresponds to
the number of bits necessary to encode θ∗ within precision
δmin. The central idea of this algorithm is to cap the number
of node expansions at 2k nodes at iteration k, and perform at
each such iteration a binary search for the cost threshold.

The second algorithm, ZigZagZoomer (Z3), interleaves the
iterations of Zoomer instead of performing them sequentially.
This is achieved by using a scheduler that may be of
If Zoomer ﬁnds the solution in M
independent interest.
node expansions, Z3 will not take more than a factor log M
compared to Zoomer, and removes the explicit dependence
on δmin to replace it with the difference δ∗ of f cost between
the solution cost and the next node of higher cost.

Empirical results in heuristic search domains and in
two domains we introduce show that although previous

algorithms can perform well in some domains, they can fail in
others, depending on the underlying structure of the problem.
By contrast, both Z3 and Zoomer are robust to the structure
of the problems, performing well in all domains.

2 Notation and Background
Let N1 := {1, 2, 3 . . .}, N0 := {0, 1, 2 . . .} and (cid:100)x(cid:101)+ :=
max(0, (cid:100)x(cid:101)). The set of all nodes in the underlying search
tree describing the problem is N , which may be inﬁnite.
C(n) ⊂ N is a function that returns the set of children of
a node n. The maximum branching factor of the search
tree is b := maxn∈N |C(n)|. We are given a cost function,
f : N → (0, ∞) and assume that f is nondecreasing
so that f (m) ≥ f (n) for all n ∈ N and m ∈ C(n).
:= {n ∈ N : f (n) ≤ θ} be the set of
Let N (θ)
nodes for which the cost does not exceed θ. Let M(θ) :=
∪n∈N (θ)C(n) \ N (θ) be the nodes at the fringe. The set of
all solution nodes is G ⊆ N . The cost of an optimal solution
is θ∗ := minn∈G f (n). Let N (θ) := |N (θ)| and N∗ :=
N (θ∗). Deﬁne δ(θ) := minn∈M(θ) f (n)−θ, which is strictly
positive by the assumption that the f cost is nondecreasing.
Furthermore, N (θ(cid:48)) = N (θ) for all θ(cid:48) ∈ [θ, θ + δ(θ)). Let
δmin := minn∈N ,f (n)≤θ∗ δ(f (n)), which corresponds to the
minimum difference in cost thresholds between two iterations
of IDA∗ before the optimal solution. Finally, let δ∗ := δ(θ∗).
Remark 1. In many deﬁnitions of tree search the edges in
the tree are associated with costs and f = g + h where
g(n) is the cumulative cost from the root to n and h :
N → R is a consistent heuristic, which guarantees that f is
nondecreasing. In case h is admissible but not consistent, f
can be made monotone nondecreasing using pathmax [Mérõ,
1984; Felner et al., 2011]. In our analysis it is convenient to
deal only with f , however.
Iterative deepening An iterative deepening tree search
algorithm makes repeated depth-ﬁrst searches (DFS) with
increasing cost threshold. IDA∗ and its variants are all based
on this idea, as are our algorithms, but with a few new twists.
The pseudo-code for DFS is given in Algorithm 1, which
is largely the classic implementation with branch-and-bound
optimization to avoid visiting provably suboptimal paths. It
returns an optimal solution or no solution, the number of
nodes expanded, the maximum cost θ− among the visited
nodes whose costs do not exceed θ, and the minimum cost
θ+ of the visited nodes whose costs exceed θ. Thus we have
θ− ≤ θ < θ+. A node is said to be expanded when DFS
passes through Line 8.

If the budget

Slightly less usual, it also takes as an argument a budget on
the number of nodes to expand, and immediately terminates
with "budget exceeded" when too many nodes have been
then θ− =
expanded.
max{f (n) : n ∈ N (θ)} and θ+ = min{f (n) : n ∈ M(θ)}.
Observe that if during one call to DFS a suboptimal solution is
found and the search budget is not exceeded, then necessarily
an optimal solution will be returned. This optimality property
is transferred to all the algorithms presented in this paper.

is not exceeded,

IDA∗ always calls DFS with unlimited budged and it starts
with threshold θ = f (root), subsequently using θ = θ+
from the previous iteration.

Algorithm 1: Depth-First Search (DFS) starting at the given node
with a cost bound θ and an expansion budget N. Returns "budget
exceeded" when the budget of node expansions is exceeded,
otherwise returns the descendant solution of minimum cost if one
It also returns the maximum cost θ−
exists, or none otherwise.
below θ of expanded nodes, and the minimum cost θ+ above θ of
non-expanded nodes among the visited nodes.

def DFS(node, θ, N):

c := f (node)
if c > θ: return none, 0, −∞, c
if is_goal(node): return node, 0, c, +∞
if N == 0: return "budget exceeded", 0, c, +∞

# Node expansion
n_used := 1 # number of expansions
best_desc = none # best descendant solution
θ+ := +∞ # min cost among nodes in fringe
θ− := c # max cost among nodes expanded
for child in C(node):

2 , θ+

2 := DFS(child, θ, N - n_used)

res, m, θ−
n_used += m
θ− := max(θ−, θ−
2 )
θ+ := min(θ+, θ+
2 )
if res is a Node and ( best_desc is none or

f (res) < f (best_desc) ):

best_desc := res # better solution found
θ := f (res) # branch and bound

elif res == "budget exceeded":
return res, n_used, θ−, θ+

return best_desc, n_used, θ−, θ+

If the cost θ∗ of the optimal solution were known in
advance, then a single call to DFS with threshold θ∗ and
an unlimited budget would ﬁnd the optimal solution with
no overhead relative to A∗ in trees. Of course the optimal
cost is usually unknown, which is overcome by calling DFS
repeatedly with increasing thresholds. Algorithms of this
type over-expand relative to A∗ for two reasons. First, in early
iterations they re-expand many of the same nodes. Second,
in the ﬁnal iteration when θ ≥ θ∗, they tend to overshoot.
Overshooting is generally more costly than undershooting
because trees usually grow quite fast, which is why IDA∗
increases θ in the most conservative manner possible.

To illustrate a typical case, suppose that DFS is called with
an unlimited budget and threshold θ = θ∗ + c for c ≥ δmin.
Since f can be constant with depth, that the number of nodes
N (θ) can be arbitrarily large compared to N (θ∗). Even
when insisting that f has a minimum edge increment σ,
the number of nodes would still grow exponentially fast as
N∗bc/σ. Algorithms that update the threshold heuristically
without budget constraints are not protected against serious
over-expansion and do not effectively control the number of
node expansions in the worst case.

Algorithm 2: The Zoomer algorithm.

def Zoomer(root):

lower := f (root) # assumed > 0
# N0: number of expansions at the root
# up_min: lower bound on upper
res, N0, _, up_min := DFS(root, lower, ∞)
if res is a Node: return res
for k := 1, 2, ...:

upper := ∞

while upper (cid:54)= up_min:

if upper == ∞:

θ := lower × 2 # sky is the limit

else:

θ := (upper + lower) / 2

θ := max(θ, up_min)

res, _, θ−, θ+ := DFS(root, θ, N02k)

if res is a Node:

return res # solution found
elif res == "budget exceeded":

upper := θ− # reduce upper bound

else: # within budget, no solution found

lower := θ # increase lower bound
up_min := θ+

3 Zoomer
The insight behind Zoomer is that the risk of calling DFS with
a large threshold can be mitigated by limiting the number
of expanded nodes in each iteration. The pseudocode is
provided in Algorithm 2.

Zoomer operates in iterations k ∈ N0.

In the zeroth
iteration it makes a single call to DFS with an unlimited
budget and threshold θ = f (root) (Line 5). The number of
nodes expanded by this search is denoted by N0, which is
usually quite small and by deﬁnition satisﬁes N0 ≤ N ∗. In
subsequent iterations k ∈ N1 the algorithm makes multiple
calls to DFS (Line 17), all with a budget of N02k, to perform
an exponential search on θ (Lines 12 and 14) to identify
whether there exists a feasible solution within the budget
(Line 10) [Bentley and Yao, 1976].

Let Nk be the total number of nodes expanded by Zoomer

during iteration k, which includes multiple calls to DFS.
Theorem 2. Assuming f (root) > 0, Zoomer returns an
optimal solution after expanding no more nodes than

∞
(cid:88)

k=0

Nk ≤ max {1, 4ω1} N ∗ ,

where ω1 := (cid:100)log2(θ∗/θ0)(cid:101) + (cid:100)log2(θ∗/δmin)(cid:101) .
Proof. Let Bk := N02k be the expansion budget at iteration
k and deﬁne the minimum cost θk at which the budget is
exceeded: θk := minn∈N {f (n) : N (f (n)) > Bk} , and
let K := min{k ∈ N0 : Bk ≥ N ∗} = (cid:100)log2(N ∗/N0)(cid:101) be
the ﬁrst iteration with enough budget to ﬁnd a solution.

Observe that DFS with budget Bk ensures that when it
returns with "budget exceeded" along with the return value
θ−, if we call again DFS(root, θ−, Bk) it will also return with
"budget exceeded", which means that θk ≤ θ− and so we
always have upper ≥ θk (Line 22). Similarly, the algorithm
also ensures that we always have lower < θk.
Iterations k < K Now, suppose that at some point we
have upper − lower ≤ δmin. Then we have upper ≤
lower + δmin < θk + δ(θk) which by Line 22 implies that
upper = θk. We also have lower ≥ upper−δmin = θk −δmin
which by Line 25 implies that up_min = θk = upper,
which means that no solution exists in [lower, upper] since
lower + δ(lower) = up_min = upper. Therefore, iteration
k < K terminates no later than when upper − lower ≤ δmin
(Line 10).

For ak times the algorithm goes through Line 12 before
calling DFS, and thus lower is doubled until θ = 2×lower ≥
θk when DFS returns with "budget exceeded", at which
point upper is set to θ− ≤ θ; Together with lower < θk
(by deﬁnition of θk), this implies that upper − lower ≤
θ − lower = lower ≤ θk. Thus, starting at worst from θ0,
ak ≤ min{d ∈ N0 : θ02d ≥ θk} = (cid:100)log2(θk/θ0)(cid:101)
≤ (cid:100)log2(θ∗/θ0)(cid:101) .

Thereafter, for bk times the algorithm goes through Line 14
before calling DFS until upper − lower ≤ δmin. Hence

bk ≤ min{d ∈ N0 : θk/2d ≤ δmin} = (cid:100)log2(θk/δmin)(cid:101)
≤ (cid:100)log2(θ∗/δmin)(cid:101) .

Iteration K For iteration K things are slightly different.
We still have lower < θ∗ (otherwise a solution would have
already been found) and upper ≥ θK. Assume that at
some point upper − lower ≤ 2δ∗. Since upper is set,
the algorithm goes through Line 14 before calling DFS. Thus
θ = (upper+lower)/2 ≤ lower+δ∗ < θ∗+δ∗ and similarly
θ ≥ upper − δ∗ ≥ θK − δ∗ ≥ θ∗ where the last inequality is
because there is enough budget for θ∗ by deﬁnition of K but
not for θK. This implies that θ ∈ [θ∗, θ∗ + δ∗) and thus the
call to DFS returns with an optimal solution within budget.

Starting from the lower bound θ0, the number of calls to
DFS after going through Line 12 before θ ≥ θ∗ is less than
(cid:100)log2(θ∗/θ0)(cid:101). At this point, either θ ∈ [θ∗, θK) and an
optimal solution is returned, or θ ≥ θK. In the latter case,
upper := θ− ≤ θ = 2lower, and since lower < θ∗ we have
upper − lower < θ∗. Subsequently the number of calls to
DFS after going through Line 14 is at most

min{d ∈ N0 : θ∗/2d ≤ 2δ∗} ≤ (cid:100)log2(θ∗/(2δ∗))(cid:101)+
≤ (cid:100)log2(θ∗/δmin)(cid:101)+ .

Therefore, over all iterations k ≤ K and including the ﬁrst
call to DFS, when ω1 ≥ 1 the number of node expansions is
bounded by

N0 +

(ak + bk)Bk ≤

ω1Bk ≤ 2ω1N02K ≤ 4ω1N ∗ .

K
(cid:88)

k=1

K
(cid:88)

k=0

j = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 · · ·
1 16 · · ·
4 · · ·
0

A(j) = 1 2 1 4 1 2 1 8 1
kj = 0 1 0 2 0 1 0 3 0

1
0

1
0

2
1

4
2

2
1

Table 1: Sequence A6519: https://oeis.org/A006519.

Algorithm 3: The Uniform Doubling Scheduling algorithm.

def A6519(j):

return ((j XOR (j - 1)) + 1) / 2

Remark 3. Because the ordering of the nodes expanded by
DFS is invariant under translation of the cost function f , if
f (root) is close to zero or negative, the algorithm can simply
be run with cost function f (cid:48) = f +1−f (root). If θ∗ is known
up to constant factors, then the bound can be improved by
translating costs so that θ∗/θ0 = O(1).
Remark 4. Consider a class of search trees with increasing
solution depth d and suppose that θ∗ = O(d) and δmin
is constant, both of which are typical. When the tree has
polynomial growth, then N∗ = O(dp) and Zoomer expands
a factor of O(log N∗) more nodes than N∗. In exponential
domains with N∗ = O(bd), Zoomer expands a factor of
O(log log N∗) more nodes than N∗.

4 Uniform Doubling Scheduling

Iterations k < K in Zoomer can take a long time to
terminate when δmin is small, where K is the ﬁrst iteration
of Zoomer with enough budget to ﬁnd a solution. This can be
mitigated by interleaving the iterations in a careful manner.
We introduce the Uniform Doubling Scheduler (UDS), which
provides the correct interleaving and may be of independent
interest. UDS is inspired by Luby’s scheduling algorithm
for speeding up randomized algorithms [Luby et al., 1993].
Suppose you have access to a countably inﬁnite number of
deterministic programs indexed by k ∈ N0 with unknown
k=0. UDS operates in blocks j ∈ N1. In
running times (Tk)∞
the jth block it runs program kj for A(j) computation steps
where kj = log2 A(j) and A(j) is the jth value of integer
sequence A6519, which are all powers of 2 (see Table 1 and
Algorithm 3). Note, the state of programs that are not running
in the current block are stored in memory.

Theorem 5. The total number of computation steps before
any program k halts is at most (4 + k + log2(n))n2k, where
n = (cid:6)Tk/2k(cid:7).

Provided that program k is not expected to run in time
much less than 2k, then Theorem 5 shows that the overhead
incurred by UDS is not especially severe. The proof of
Theorem 5, provided below, depends on two lemmas.
Lemma 6 (Orseau et al., 2018, Lemma 10). For all j ∈ N1
and k ∈ N0: A(j) = 2k ⇔ (∃!n ∈ N1 : j = (2n − 1)2k).
Lemma 7. (cid:80)j
2 (3 + (cid:98)log2 j(cid:99)) for all j ∈ N1.

i=1 A(i) ≤ j

Proof. By Lemma 6, the number of blocks program k has
been run for after the scheduler has run j blocks is n =
max{n ∈ N1 : (2n − 1)2k ≤ j} = (cid:4)(cid:0)j/2k + 1(cid:1) /2(cid:5).
Furthermore, the program of largest k that could run up to
j is max{k ∈ N0 : 2k ≤ j} = (cid:98)log2 j(cid:99). That is, at block j,
exactly 1 + (cid:98)log2 j(cid:99) programs have computed for at least one
step. Therefore the total number of computation steps over

def uniform_doubling_scheduling(prog):

states := [] # growable vector
for j := 1, 2, ...:

k := log2(A6519(j)) # exact
if k >= len(states):

state[k] := make_state(states, k)

state[k] := run(prog(states, k), 2k)
if is_goal(state[k]):
return state[k]

all started programs after block j is

j
(cid:88)

i=1

A(i) =

(cid:98)log2 j(cid:99)
(cid:88)

k=0

(cid:23)

(cid:22) j/2k + 1
2

2k ≤

(3 + (cid:98)log2 j(cid:99)) .

j
2

Proof of Theorem 5. UDS always runs program k in blocks
of length 2k. By Lemma 6, program k will be run for the nth
block once j = (2n − 1)2k. Therefore by Lemma 7, the total
computation before program k halts is at most

j
(cid:88)

i=1

j
2

A(i) ≤

(3+(cid:98)log2(j)(cid:99)) ≤ (4+k+(cid:98)log2(n)(cid:99))n2k .

5 ZigZagZoomer
Using UDS to interleave the iterations of Zoomer allows us
to replace δmin with δ∗ in the analysis. The theoretical price
for the improvement is at most a logarithmic factor, with the
worst case when δ∗ = δmin. See Algorithm 4 (an optimized
version is provided in the supplementary material). The
following theorem follows from the analysis in the previous
two sections.
Theorem 8. Assuming θ0 := f (root) > 0, Algorithm 4
ensures that an optimal solution is found within a number of
node expansions bounded by

∞
(cid:88)

Nk ≤ N0 + 2(4 + (cid:100)log2(N∗/N0)(cid:101) + log2(ω2))ω2N∗ ,

k=0
where ω2 := (cid:100)log2(θ∗/θ0)(cid:101) + (cid:100)log2(θ∗/(2δ∗))(cid:101)+.
Proof. Algorithm 4 terminates at the latest when program
K = (cid:100)log2(N∗/N0)(cid:101) terminates, since 2KN0 ≥ N∗.
iteration K
From the proof of Theorem 2 for the last
(only), we know that the number of calls to DFS after going
through Line 11 before θ ≥ θ∗ is less than (cid:100)log2(θ∗/θ0)(cid:101).
Subsequently the number of calls to DFS after going through
Line 13 is at most (cid:100)log2(θ∗/(2δ∗))(cid:101)+. Hence the number
nK of blocks of size 2K macro steps (each of N0 steps)
performed by program K before terminating is bound by
nK ≤ ω2. Thus, applying Theorem 5 with n = nK and

Algorithm 4: The Z3 algorithm (simple version). f (root) is assumed
strictly positive.

def ZigZagZoomer(root):

lower := [f (root)] # growable vector
upper := [] # growable vector
res, N0, _, _ := DFS(root, lower, +∞)
if res is a Node: return res
for j := 1, 2, ...:

#4

k := log2(A6519(j)) # exact
if k >= len(lower):

lower[k] := f (root)

if k >= len(upper):
θ := 2 × lower[k]

else:

θ := (upper[k] + lower[k]) / 2
res, _, _, _ := DFS(root, θ, N02k)
if res is a Node:

return res # solution found
elif res == "budget exceeded":

upper[k] := θ # set or reduce upper bound

else:

lower[k] := θ # increase lower bound

k = K gives that the number of macro steps performed by
the scheduler is at most

(4 + K + (cid:98)log2(nK)(cid:99))nK2K

≤ 2(4 + (cid:100)log2(N∗/N0)(cid:101) + (cid:98)log2 ω2(cid:99))ω2N∗/N0 .

Multiplying by the number N0 of steps per macro step and
adding the ﬁrst call to DFS (Line 4) leads to the result.

Remark 9. Sometimes an iteration k > K terminates earlier
than iteration K. The analysis can be improved slightly to
reﬂect this, as we discuss in the supplementary material.
Remark 10. Algorithm 4 is simple but a little wasteful.
See the supplementary material for an optimized version that
terminates iterations k < K for which the budget is provably
insufﬁcient.

2014],

[Sharon et al.,

6 Experiments
Algorithms tested We test IDA∗ [Korf, 1985], Zoomer,
Z3 (optimized), EDA∗
and
IDA∗_CR [Sarkar et al., 1991]. EDA∗(γ) is a variant of IDA∗
that repeatedly calls DFS with unlimited budget and increasing
thresholds. In the kth iteration it uses a threshold of γk where
γ > 1. In our experiments we take γ = 2. In a given itera-
tion, IDA∗_CR collects the costs of nodes in the fringe into
buckets. It then selects the cost for its next iteration based on
the information stored in the buckets. The cost is chosen such
that IDA∗_CR is likely to expand more than bk nodes in the
k + 1-th iteration. Similarly to EDA∗, we set b = 2. We also
present the results of DFS(θ∗), which is DFS with its initial
bound set to the optimal cost θ∗. Our goal is to verify to how
close each algorithm is from DFS(θ∗). For a given heuristic
function and without duplicate detection, the number of nodes

expanded by DFS(θ∗) is a lower bound on the number of nodes
expanded by the algorithms we evaluate.

Problem domains We use three traditional problem
domains in our experiment: 15-Puzzle [Pekonen et al., 2007;
Doran and Michie, 1966], (12, 4)-TopSpin [Lammertink,
1989], and the 15-Pancake puzzle [Dweighter, 1977]. All
these domains were implemented in PSVN [Holte et al.,
2014]. We use pattern database heuristics (PDBs) [Culberson
and Schaeffer, 1996] for all three domains. The PDBs for the
15-Puzzle and (12, 4)-TopSpin are generated by projecting
tiles 6–15 and tokens 8–12, respectively. The PDB for the
15-Pancake is generated with domain abstraction by mapping
pancakes 2–8 to pancake 1 (see the PSVN manual). We show
results both when operator costs are unitary, as usual, and also
when they are chosen uniformly at random in [1..10000] in
order to test the robustness of the algorithms. We also include
the Chain problem, where the search tree is a simple chain
(branching factor is 1) and the solution is placed at a depth
chosen uniformly at random between 1 and 10000.

The branching factor of the search tree of the domains
described above does not vary much. Moreover, all three
domains have a relatively large density of solutions; there is a
solution in any subtree of the search tree. These properties
make the problem of overshooting less pronounced.
In
order to evaluate the robustness of the search algorithms,
we introduce the Coconut problem, which is a domain with
varied branching factor and small solution density.
In the
Coconut problem we set the heuristic h to 0 and deﬁne f = g
as follows. The root has f cost 1. There are b coconut trees,
and only one has a coconut in its branches. The agent must
climb the trees and ﬁnd the coconut. The b trunks all have the
same size D, sampled uniformly in [1..10000]. Moving along
one trunk (using the same action repeatedly) costs 1, jumping
from one trunk to another costs 2D. After the trunk come the
branches, where each branch is a b-ary tree. Moving along the
branches or jumping from one branch to another costs 1/10.
The depth of the coconut in the branches is sampled from a
geometric distribution with parameter 1/4.

Experimental setup We use 100 instances for each
problem domain and report the number of nodes expanded
and number of problems solved with a time limit of 5 hours
for the PSVN domains, 1 hour for the Coconut problem
and no limit for the Chain problem. We use the number of
nodes expanded instead of running time because all search
algorithms expand roughly the same number of nodes per
second. The results are shown in Fig. 1, where the y-axis
shows the number of nodes expanded in log scale with the
dashed lines being powers of 2. The x-axis depicts the
instances from easiest to hardest. To ease visualization, the
instances in the x-axis are sorted independently for each
algorithm. The curves are thus not directly comparable for a
particular instance, but they allow one to compare the growth
in difﬁculty for the various solvers.

Discussion Only Zoomer and Z3 perform reliably across
IDA∗ performs very well on domains with
all domains.
unit costs and a branching factor greater than 1, sometimes
terminating before DFS(θ∗) because it stops as soon as it
ﬁnds a solution. But due to its conservative selection

Figure 1: Proﬁles of node expansions on log scale for 100 instances on four different domains. Values are sorted for each solver individually.
Lines that terminate early mean the time limit is reached for all subsequent instances. Dashed gray lines are powers of 2. Other dashed lines,
when displayed, show the number of node expansions when the algorithm is terminated before a solution is found. The 3 r.h.s. plots are with
random-costs operators on the ﬁrst row, and with unit-costs operators on the second row.

of the threshold, it also expands too many nodes on the
IDA∗_CR performs best on the 15-Puzzle,
other domains.
(12, 4)-Topspin and 15-Pancake with random operator costs.
On these same problems, EDA∗, Zoomer and Z3 perform
similarly. However, both IDA∗_CR and EDA∗ perform very
poorly on the Coconut problem, each of them solving only 4
instances, and are outperformed even by IDA∗.

To explain the behaviour of IDA∗_CR and EDA∗ on the
Coconut problem, consider an instance where the depth of
the trunk is D = 2690 and the depth of the coconut in the
branches is q = 6. The cost set by EDA∗(2) in the last
iteration is 4096, resulting in a search tree with approximately
3(4096−2690)/0.1 ≈ 106700 nodes. IDA∗_CR is not saved by
choosing a threshold in the range of costs observed in the
fringe: Due to the large cost for jumping from one tree to
the other, the threshold in the last iteration can be as large
as 2 × 2689. This is not a carefully selected example. On
the contrary, care must be taken to choose a Coconut problem
for which these algorithms work. Even EDA∗(1.01) would be
only marginally better. For comparison, A∗ and IDA∗ expand
about 103 and 106 nodes respectively on this instance.

By contrast, Zoomer and Z3 perform both well on
the Coconut problem:
even though they require a non-
negligible factor compared to DFS(θ∗), it appears this factor
is independent of the difﬁculty of the instance, in line with
the theoretical results. Zoomer performs slightly better than
Z3 because the gaps {δ(f (n)) : n ∈ N } are relatively large.

7 Conclusion
We derived two linear-memory heuristic search algorithms
that require no parameter tuning and come with guarantees
on the number of node expansions in the worst case relative
to A∗. Zoomer is guaranteed to ﬁnd optimal solutions in
O(N∗ log(θ∗/δmin)) node expansions, where θ∗ is the cost
of the solution and δmin is the smallest difference in cost
between any two nodes that may not be on the same branch.
The second algorithm, ZigZagZoomer (Z3), expands at most
logarithmically more nodes than Zoomer in the worst case,
but replaces δmin with a ‘local version’ δ∗ for which δ∗ (cid:29)
δmin often holds. Theoretical guarantees are summarized in
Table 2. Zoomer and Z3 perform very robustly in all domains
tested, whereas all other tested algorithms perform poorly in
at least one domain.

Table 2: All algorithms return a minimal-cost solution and require
memory linear with the depth of the search; ω1 = O(log(θ∗/δmin))
and ω2 = O(log(θ∗/δ∗)).

Algorithm Worst case
Ω(bN∗ )
EDA∗
IDA∗_CR Ω(bN∗ )
Ω(N 2
IDA∗
∗ )
O(ω1N∗)
Zoomer
O(N∗ω2 log(N∗ω2))
Z3

Although Zoomer and Z3 are not the fastest linear-memory

heuristic search algorithms for all problems, they do perform
well consistently and their robustness makes them a safe
choice. More aggressive algorithms sometimes perform
better on certain problems, but can also fail catastrophically,
as evidenced by the Coconut problem. When prior knowledge
is available about the structure of the tree it may be preferable
to run a safe algorithm like Zoomer or Z3 in parallel with a
more aggressive choice.

The are many interesting directions for future research,
including the use of transposition tables to prevent node re-
expansion on graphs [Reinefeld and Marsland, 1994].

Acknowledgements

[Hatem et al., 2018] Matthew Hatem, Ethan Burns, and
Wheeler Ruml. Solving large problems with heuristic
search: General-purpose parallel external-memory search.
Journal of Artiﬁcial Intelligence Research, 62:233–268,
jun 2018.

[Helmert et al., 2019] Malte Helmert, Tor Lattimore, Levi
H. S. Lelis, Laurent Orseau, and Nathan R. Sturtevant.
In IJCAI. Forth-
Iterative budgeted exponential search.
coming, 2019.

[Holte et al., 2014] Robert C. Holte, Broderick Arneson, and
Neil Burch. PSVN Manual (June 20, 2014). Technical
Report TR14-03, Computing Science Department, Univer-
sity of Alberta, 2014.

Many thanks to Csaba Szepesvári, András György, János
Kramár, Roshan Shariff, and the anonymous reviewers for
their feedback on this work.

[Korf, 1985] Richard E. Korf.

iterative-
deepening: An optimal admissible tree search. Artiﬁcial
Intelligence, 27(1):97 – 109, 1985.

Depth-ﬁrst

References

[Bentley and Yao, 1976] Jon Louis Bentley and Andrew
An almost optimal algorithm for
Information Processing Letters,

Chi-Chih Yao.
unbounded searching.
5(3):82 – 87, 1976.

[Burns and Ruml, 2013] Ethan Burns and Wheeler Ruml.
Iterative-deepening search with on-line tree size predic-
tion. Annals of Mathematics and Artiﬁcial Intelligence,
69(2):183–205, Oct 2013.

[Culberson and Schaeffer, 1996] Joseph C. Culberson and
Jonathan Schaeffer. Searching with pattern databases.
In Canadian Conference on Artiﬁcial Intelligence, pages
402–416, 1996.

[Dechter and Pearl, 1983] Rina Dechter and Judea Pearl.
In AAAI, pages 95–99,

The optimality of A* revisited.
1983.

[Doran and Michie, 1966] James E Doran and Donald
Michie. Experiments with the graph traverser program.
Proceedings of the Royal Society of London. Series A.
Mathematical and Physical Sciences, 294(1437):235–259,
1966.

[Dweighter, 1977] Harry Dweighter. E2569. The American

Mathematical Monthly, 84:296, 04 1977.

[Felner et al., 2011] Ariel Felner, Uzi Zahavi, Robert Holte,
Jonathan Schaeffer, Nathan R. Sturtevant, and Zhifu
Inconsistent heuristics in theory and practice.
Zhang.
Artiﬁcial Intelligence, 175(9-10):1570–1603, 2011.

[Hart et al., 1968] P. E. Hart, N. J. Nilsson, and B. Raphael.
A formal basis for the heuristic determination of minimum
IEEE Transactions on Systems Science and
cost paths.
Cybernetics, 4(2):100–107, 1968.

[Hatem et al., 2015] Matthew Hatem, Scott Kiesel, and
Wheeler Ruml. Recursive best-ﬁrst search with bounded
In Proceedings of the Twenty-Ninth AAAI
overhead.
Conference on Artiﬁcial Intelligence, AAAI’15, pages
1151–1157. AAAI Press, 2015.

[Korf, 1993] Richard E. Korf. Linear-space best-ﬁrst search.

Artiﬁcial Intelligence, 62(1):41 – 78, 1993.

[Lammertink, 1989] Ferdinand Lammertink. Puzzle or game
having token ﬁlled track and turntable, October 3 1989.
US Patent 4,871,173.

[Luby et al., 1993] Michael Luby, Alistair Sinclair, and
Optimal speedup of Las Vegas
David Zuckerman.
algorithms. Inf. Process. Lett., 47(4):173–180, September
1993.

[Mérõ, 1984] László Mérõ. A heuristic search algorithm
with modiﬁable estimate. Artiﬁcial Intelligence, 23(1):13
– 27, 1984.

[Orseau et al., 2018] Laurent Orseau, Levi Lelis, Tor Latti-
more, and Theophane Weber. Single-agent policy tree
In NeurIPS, pages 3205–3215,
search with guarantees.
2018.

[Pekonen et al., 2007] Osmo Pekonen, Jerry Shcum, Die
Sonneveld, and Aaron Archer. The 15 puzzle: how it
drove the world crazy. The Mathematical Intelligencer,
29(2):83–85, 2007.

[Reinefeld and Marsland, 1994] Alexander Reinefeld and
Enhanced iterative-deepening
IEEE Transactions on Pattern Analysis and

T. Anthony Marsland.
search.
Machine Intelligence, 16(7):701–710, 1994.

[Russell, 1992] Stuart Russell. Efﬁcient memory-bounded

search methods. ECAI-1992, Vienna, Austria, 1992.

[Sarkar et al., 1991] U.K. Sarkar, P.P. Chakrabarti, S. Ghose,
and S.C. De Sarkar. Reducing reexpansions in iterative-
deepening search by controlling cutoff bounds. Artiﬁcial
Intelligence, 50(2):207 – 221, 1991.

[Sharon et al., 2014] Guni Sharon, Ariel Felner,

[Sen and Bagchi, 1989] Anup K Sen and Amitava Bagchi.
Fast recursive formulations for best-ﬁrst search that allow
controlled use of memory. In IJCAI, pages 297–302, 1989.
and
Nathan R. Sturtevant. Exponential deepening A* for
real-time agent-centered search. In Carla E. Brodley and
Peter Stone, editors, Proceedings of the Twenty-Eighth
AAAI Conference on Artiﬁcial Intelligence, July 27 -31,

2014, Québec City, Québec, Canada., pages 871–877.
AAAI Press, 2014.

[Wah and Shang, 1994] Benjamin W. Wah and Yi Shang.
Comparision and evaluation of a class of IDA* algorithms.
International Journal on Artiﬁcial Intelligence Tools,
03(04):493–523, dec 1994.

[Zhou and Hansen, 2003] Rong Zhou and Eric A Hansen.
Sweep A*: space-efﬁcient heuristic search in partially
ordered graphs. In Proceedings. 15th IEEE International
Conference on Tools with Artiﬁcial Intelligence, pages
427–434. IEEE, 2003.

Algorithm 5: The optimized Z3 algorithm.

def ZigZagZoomerV2(root):

upper := [] # growable vector
lower := f (root)
res, N0, _, up_min := DFS(root, lower, +∞)
if res is a Node: return res
kmin := 0
j := 0
repeat forever:

# Skip steps with provably no solution
j += 2kmin
k := log2(A6519(j)) # exact
if k < len(upper):

if upper[k] <= up_min:

# Will skip all progs <= k
kmin := k+1
# Will move to next factor of 2kmin
j -= 2k
continue

θ := (upper[k] + lower) / 2

else:

θ := 2 × lower
θ := max(θ, up_min)
res, _, θ−, θ+ := DFS(root, θ, N02k)
if res is a Node:

return res # solution found
elif res == "budget exceeded":

upper[k] := θ− # set or reduce upper bound

else:

# Search terminated within budget without
# a solution. No program can find a
# solution below this cost.
lower := θ
up_min := θ+

Detection of duplicate states can be performed along the
current trajectory to avoid loops in the underlying graph,
while keeping a memory that grows only linearly with the
depth of the search, but is now a multiple of the size of a
state.

A Generalized analysis of ZigZagZoomer
The bound in Theorem 8 can be improved slightly. Let
K = min{k : N02k ≥ N ∗}. All iterations k < K will never
ﬁnd the solution. Iterations k ≥ K will eventually ﬁnd the
solution. In the analysis of Z3 we simply bounded the number
of node expansions of iteration K, but it can happen that an
even larger k > K will ﬁnd the solution earlier. Precisely,
the kth program will halt once it calls DFS with θ ∈ [θ∗, θk]
with θk = max{θ : N (θ) ≤ N02k}. The number of nodes
expanded before iteration k halts is

Tk ≤ N02kω(θ∗, θk) ,

where ω(a, b) is the number of ‘zooms’ before iteration k
calls DFS with θ ∈ [a, b], which satisﬁes

ω(θ∗, θk) = O

log

+ log

(cid:18)

(cid:19)

(cid:18) θ∗
θ0

(cid:18) θ∗

(cid:19)(cid:19)

θk − θ∗

.

Then by Theorem 5, Z3 will ﬁnd the solution after

(cid:18)

O

min
k≥K

Tk log(Tk)

(cid:19)

node expansions. Exactly which program k ≥ K minimizes
Tk log(Tk) depends on the blowup of N (θ) about θ = θ∗.

B Optimized Z3
Algorithm 5 features a few logical optimizations compared to
Algorithm 4. With a little work, they can be used to improve
the bound of Theorem 8, but we will not do this here.

The ﬁrst improvement is to stop the search using up_min
as is done for Zoomer, since once the interval upper − lower
is smaller than the gap δ(lower), provably no solution can be
found.

The second one is to replace the individual lower bounds
with a global lower bound that any program can raise: indeed,
when a program raises the lower bound this means that it has
explored all nodes below it without success and thus no other
program can ﬁnd a solution below that cost either. Then, to
stop testing the programs that have been proven to not having
enough budget to ﬁnd a solution, we simply skip the indices
j of these programs, using the properties of A6519, so that
skipped programs take zero computation time.
More improvements A further improvement (valid for
Zoomer as well) would gather the cost of a solution if one
is found but the budget is exceeded. This cost could then be
used as a global upper bound. This would require modifying
the DFS algorithm.

For most iterative deepening based algorithms, It is also
common to avoid re-evaluating whether a node is a solution.
This can be done by passing the previous threshold θ to DFS
and evaluating only nodes of a larger cost. The number
of evaluation calls would then only be equal to the number
of nodes expanded during the call to DFS with the largest
threshold.

In DFS, when a solution has been found, children of cost
equal to the solution cost need not be expanded. It is not clear
however if the saving in node expansions would be worth the
computation cost of the test.

