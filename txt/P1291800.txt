Robust and Precise Vehicle Localization based on Multi-sensor Fusion
in Diverse City Scenes

Guowei Wan, Xiaolong Yang, Renlan Cai, Hao Li, Hao Wang, Shiyu Song1

0
2
0
2
 
r
a

M
 
6
 
 
]

V
C
.
s
c
[
 
 
2
v
5
0
8
5
0
.
1
1
7
1
:
v
i
X
r
a

Abstract— We present a robust and precise localization
system that achieves centimeter-level localization accuracy in
disparate city scenes. Our system adaptively uses information
from complementary sensors such as GNSS, LiDAR, and
IMU to achieve high localization accuracy and resilience in
challenging scenes, such as urban downtown, highways, and
tunnels. Rather than relying only on LiDAR intensity or
3D geometry, we make innovative use of LiDAR intensity
and altitude cues to signiﬁcantly improve localization system
accuracy and robustness. Our GNSS RTK module utilizes the
help of the multi-sensor fusion framework and achieves a better
ambiguity resolution success rate. An error-state Kalman ﬁlter
is applied to fuse the localization measurements from different
sources with novel uncertainty estimation. We validate, in detail,
the effectiveness of our approaches, achieving 5-10cm RMS
accuracy and outperforming previous state-of-the-art systems.
Importantly, our system, while deployed in a large autonomous
driving ﬂeet, made our vehicles fully autonomous in crowded
city streets despite road construction that occurred from time to
time. A dataset including more than 60 km real trafﬁc driving
in various urban roads is used to comprehensively test our
system.

I. INTRODUCTION

Vehicle localization is one of the fundamental tasks in
autonomous driving. The single-point positioning accuracy
of the global navigation satellite system (GNSS) is about
10m due to satellite orbit and clock errors, together with
tropospheric and ionospheric delays. These errors can be
calibrated out with observations from a surveyed reference
station. The carrier-phase based differential GNSS tech-
nique, known as Real Time Kinematic (RTK), can provide
centimeter positioning accuracy [1]. The most signiﬁcant
advantage of RTK is that
it provides almost all-weather
availability. However, its disadvantage is equally obvious
that it’s highly vulnerable to signal blockage, multi-path
because it relies on the precision carrier-phase positioning
techniques. Intuitively, LiDAR is a promising sensor for
precise localization. Failure during harsh weather conditions
and road construction still is an important issue of LiDAR-
based methods, although related works have shown good
progress in solving these problems, for example, light rain [2]
and snow [3]. Furthermore, LiDAR and RTK are two sensors
those are complementary in terms of applicable scenes.
LiDAR works well when the environment is full of 3D or

*This work is supported by Baidu Autonomous Driving Business Unit

in conjunction with the Apollo Project.

The authors are with Baidu Autonomous Driving Business Unit,

{wanguowei, yangxiaolong02, cairenlan, lihao30,
wanghao29, songshiyu}@baidu.com.

1Author

to whom correspondence should be addressed, E-mail:

songshiyu@baidu.com

Fig. 1: Our autonomous vehicle is equipped with a Velodyne LiDAR
HDL-64E. An integrated navigation system, NovAtel ProPak6 plus NovAtel
IMU-IGM-A1, is installed for raw sensor data collection, such as GNSS
pseudo range and carrier wave, IMU speciﬁc force and rotation rate. A
computing platform equipped with Dual Xeon E5-2658 v3 12 cores, and a
Xilinx KU115 FPGA chip with 55% utilization for LiDAR localization.

texture features, while RTK performs excellently in open
space. An inertial measurement unit (IMU), including the
gyroscopes and the accelerometers, continuously calculate
the position, orientation, and velocity via the technology
that is commonly referred to the dead reckoning. It’s self-
contained navigation method, that is immune to jamming and
deception. But it suffers badly from integration drift.

Thus, each sensor has its own unique characteristics and its
working conditions. Here, we propose a robust and precise
localization system using multi-sensor fusion designed for
autonomous vehicles driving in complex urban and highway
scenes. More precisely, we adaptively fuse different local-
ization methods based on sensors such as LiDAR, RTK, and
IMU. The sensor conﬁguration of our system is shown in
Figure 1. Our system provides stable, resilient and precise lo-
calization service to other modules in an autonomous vehicle,
which has the capability of driving in several complex scenes,
such as downtown, tunnels, tree-lined roads, parking garages,
and highways. We demonstrate large-scale localization using
over 60 km of data in dynamic urban and highway scenes.
In Figure 2, we show the architecture of our multi-sensor
fusion framework.

To summarize, our main contributes are:
• A joint framework for vehicle localization that adap-
tively fuses different sensors including LiDAR, RTK,
and IMU. It effectively leverages their advantages and
shields our system from their failure in various scenes
by having effective uncertainty estimation.

Fig. 2: Overview of the architecture of our system that estimates the optimal position, velocity, attitude (PVA) of the autonomous vehicle by combining
sensor input (purple) with pre-built LiDAR map (yellow). GNSS and LiDAR estimate the PVA used by an error-state Kalman ﬁlter as the measurements,
while the Kalman ﬁlter provides the predicted prior PVA. The strap-down inertial navigation system (SINS) is used as a prediction model in the Kalman
ﬁlter propagation phase by integrating the speciﬁc force f b measured by the accelerometer and the rotation rate ωb
ib measured by the gyroscope. The
corrections including the bias of accelerometer and gyroscope, the errors of PVA, etc estimated by the Kalman ﬁlter are fed to the SINS.

• A LiDAR localization method that adaptively combines
intensity and altitude cues to achieve robust and accurate
results especially during challenging situations like road
construction, outperforming previous works.

• A vehicle localization system that has been rigorously
tested daily in crowded urban streets, making our ve-
hicles fully autonomous in various challenging scenes
including urban downtown, highways, and tunnels.

II. RELATED WORK

Multi-sensor fusion is not a brand new idea. However, fus-
ing multiple sensors and making the whole system accurate,
robust and applicable for various scenes is a very challenging
task. A. Soloviev. [4] and Y. Gao et al. [5] implemented
integrated GNSS/LiDAR/IMU navigation systems using a
2D laser scanner plus GNSS and IMU. The applicable scenes
are limited due to the LiDAR localization module that relies
on particular features such as building walls. LiDAR aided
by inertial sensors can also localize the autonomous vehicle.
It provides the localization measurements, while inertial sen-
sors typically are used to predict the incremental movement
between scans to improve the point cloud matching. These
works [6], [7], [8] are among them. Their methods rely
on LiDAR solely, but RTK is a perfect complementary
localization method to LiDAR. RTK plays an important role,
especially in open spaces or streets with road construction.
LiDAR-based vehicle localization has been very popular
in recent years. R. K¨ummerle and W. Burgard [9] developed
an autonomous system that utilizes multi-level surface maps
of corresponding environments to localize itself based on
the particle ﬁlter. K. Yoneda and S. Mita [10] localized their
autonomous vehicle by using Iterative Closest Point (ICP)
method [11] to align the real-time scanning point cloud to
the geo-referenced map. However, it is known that methods
like ICP are very sensitive to the initial guess. They can fail
in scenes without abundant 3D features, such as highways
or other open spaces. The works that are closest to ours
are from J. Levinson and S. Thrun [2], [12]. They propose a
LiDAR intensity-based localization method. LiDAR intensity
provides more texture information of the environment as
valuable additional cues compared to the localization system
that is based solely on the 3D geometry of the point cloud.

We improved the methodology for various aspects, including
a new image alignment step to reﬁne the heading angle
estimation, a joint cost function involving both the intensity
and the altitude to achieve robust results, and a new step
to estimate the covariance matrices of the result. Methods
mentioned thus far are designed for multiple layers LiDAR,
such as Velodyne HDL-64E, Velodyne HDL-32E or even
Velodyne VLP-16. It is expected that the retail price of
these LiDAR scanners will fall quickly because there are
more than 40 manufacturers competing in this rising ﬁeld
as we are writing this article. Rather than using multi-layer
LiDARs, works [13], [14], [15], [16], [17], [18], [19] attempt
to accomplish the similar task with 2D or low-end LiDARs.

III. LIDAR MAP GENERATION

Our LiDAR based localization module relies on a pre-
generated map. Our goal here is to obtain a grid-cell repre-
sentation of the environment. Each cell stores the statistics
of laser reﬂection intensity and altitude. In [2] and [12], each
cell is represented by a single Gaussian distribution model
which maintains the average and the variance of the intensity.
In [20], this is extended to a Gaussian mixture model (GMM)
maintaining both the intensity and the altitude. In our work,
we still use single Gaussian distribution to model the environ-
ment but involve both the intensity and the altitude as shown
in Figure 3. We found that combining both the intensity
and the altitude measurements in the cost function through
an adaptive weighting method can signiﬁcantly improve the
localization accuracy and robustness. This will be discussed
in details in Section IV-B and VII-C.

IV. LIDAR BASED LOCALIZATION

Our localization system estimates the position, velocity
and attitude (PVA) jointly. However, we seek an optimal
solution that includes only the 3-dimensional position and the
heading, (x, y, a, h) in the LiDAR localization module. Here
(x, y) is a 2D Cartesian coordinate from a conformal pro-
jection, such as the Universal Transverse Mercator (UTM).
Our pre-built map includes the statistics of the altitude of the
road surface. We could obtain the altitude estimation a by
assuming the vehicle runs on the surface of the road. [20]
and [21] make exhaustive searching over (x, y, h), which
causes the computational complexity of O(n3). For better

(b)

(c)

(a)

Fig. 3: An example of a pre-built LiDAR map including the statistics of
laser intensity and altitude. (a) A pre-built map composed covering 3.3km
× 3.1km area rendered in laser intensity values. (b) A zoomed-in view of
(a). (c) The same zoomed-in view of (a), but rendered in altitude a.

efﬁciency, we have a separate h estimation step based on
the Lucas-Kanade algorithm [22] and a histogram ﬁlter for
the horizontal (x, y) estimation. Please refer to Algorithm 1
for a complete step-by-step overview.

Algorithm 1 LiDAR-based localization
Input: Prior map m, online point cloud z, rough transformation

T0 = (x0, y0, a0, φ0, θ0, h0) and search space X, Y .

Output: Best registration (ˆx, ˆy, ˆa, ˆh), and covariance matrix Cxy.
1: ˆh ← heading angle estimation
(cid:46) IV-A
2: ˆa0 ← m(x0, y0)
(cid:46) Get altitude from map
3: Transform z with the transformation (x0, y0, ˆa0, φ0, θ0, ˆh)
4: for xi, yi ∈ {x0 + X, y0 + Y } do
Pr ← SSDr(xi, yi, z, m)
5:
Pa ← SSDa(xi, yi, z, m)
6:
P (z|xi, yi, m) ← (Pr)γ · (Pa)1−γ
7:
P (xi, yi) ← P (z|xi, yi, m) · ( ¯P (xi, yi))1/κ
8:
9: end for
10: (ˆx, ˆy) ← {P (xi, yi)}
11: Cxy ← {P (xi, yi)}
12: ˆa ← m(ˆx, ˆy)
13: return (ˆx, ˆy, ˆa, ˆh, Cxy)

(cid:46) Equ. 11
(cid:46) Equ. 12
(cid:46) Get altitude from map

(cid:46) Equ. 6 8
(cid:46) Equ. 7 8
(cid:46) Equ. 5 9 10
(cid:46) Equ. 4

A. Heading Angle Estimation

Similar to the procedures in the map generation section,
we again project the online point cloud onto the ground
plane, and an intensity image is generated similar to Figure 3,
but the pixels ﬁlled with the intensity values in the image are
thinly dispersed due to the sparsity of the online point cloud.
We can obtain the localization result (x, y, h) by matching
this sparse image with the pre-built LiDAR map. During the
heading angle h estimation step, Lucas-Kanade algorithm
[22] is applied, which is a technique that uses the image gra-
dient to search for the best match between two images. More
precisely, a type of Lucas-Kanade algorithm, the forwards
additive algorithm [23], is applied. It is essentially a Gauss-
Newton gradient descent non-linear optimization algorithm.
We veriﬁed that the Lucas-Kanade algorithm can converge to
solutions with sub-degree accuracy using the online intensity

image as the template image. A tactical grade MEMS IMU
cannot provide sufﬁciently accurate heading angle estimation
by itself. The effectiveness of this step is shown in Table II
in Section VII-C. Although the Lucas-Kanade algorithm can
produce the horizontal translation estimation, we found that
it is not accurate and robust enough in practice. Therefore,
we use only the rotation component as the heading angle
estimation, which is used in the next step, the horizontal
localization.
B. Horizontal Localization

Histogram ﬁlter is one of nonparametric approaches. It
approximates the posteriors by decomposing the state space
into ﬁnitely many regions, and representing the cumulative
posterior for each region by a single probability value. For
completeness, we quote the equations of the prediction and
update phase of the histogram ﬁlter from [24]:

¯Pk,t =

(cid:88)

i

P (Xt = xk|ut, Xt−1 = xi) · Pi,t−1

Pk,t = η · P (zt|Xt = xk) · ¯Pk,t,

(1)

(2)

where Pk,t represents the belief of each state xk at time t,
ut is the control input, and zt is the measurement vector.

We apply the histogram ﬁlter to the horizontal localization.
The state contains (x, y). The reason we use the histogram
ﬁlter is that the cumulative probabilities of the posterior den-
sity are calculated in a discrete way. This exhaustive search
ensures the optimal solution. Equations 1 and 2 correspond
to the prediction and update step, and the corresponding
equations of our method are Equations 3 and 4, respectively.
1) The Prediction Step: The prediction step is to predict
on the new belief of the state with the historical distribu-
tion of the ﬁlter and the control variables or the motion
model. Here we directly update the histogram ﬁlter center
using the motion prediction from the SINS, and update the
belief distribution with a random walk with Gaussian noise.
Previous works [2], [12] rely on a pre-fused GNSS/IMU
solution. A complete multi-sensor fusion framework allows
our system to work under varied challenging circumstances.
Thus, the prediction step updates the probability of each cell
as follows:

¯P (x, y) = η ·

P (i, j) · exp(−

(cid:88)

i,j

(i − x)2 + (j − y)2
2σ2

),

(3)

where ¯P (x, y) is the predicted probability, P (i, j) is the
probability after the motion update from the SINS. Here σ
is the parameter describing the rate of drift between two
frames.

2) The Update Step: The second step of the histogram
ﬁlter is the measurement update step, in which the posterior
belief of each state is estimated by the following equation:
P (x, y|z, m) = η · P (z|x, y, m) · ( ¯P (x, y))1/κ,

(4)

where z is the online map built with the online laser scans
in the way identical to the mapping procedure in Section
III, Nz is the number of the cells of the online map with
valid data. m is the pre-built LiDAR map, (x, y) is the
motion pose, η is the normalizing constant, and κ is the
Kullback-Leibler divergence [25] between the distributions
of (cid:80)
¯P (x, y). The KL divergence
xy
depicts the discrimination information and is used to balance

xy P (z|x, y, m) and (cid:80)

the inﬂuence of the prediction. Unlike [12], we discard the
uncertainty of the GNSS/IMU pose. We have a more com-
prehensive fusion framework fusing the input from various
sources including GNSS.

The likelihood P (z|x, y, m) is calculated by matching the
online point cloud with the pre-built map. We adaptively fuse
the intensity and the altitude measurement by building a cost
function with a dynamic weighting parameter γ:

P (z|x, y, m) = η · P (zr|x, y, m)γ · P (za|x, y, m)1−γ, (5)

where zr and za represent the intensity and altitude mea-
surement of the online sensor input.

The intensity part is formalized as below:

SSDr =

(cid:88)

i,j

(rm(i−x,j−y) − rz(i,j) )2(σ2

σ2

m(i−x,j−y)

m(i−x,j−y)
σ2

z(i,j)

+ σ2

)

z(i,j)

,

(6)

where rm, and rz denote the average intensity value in
the pre-built map and the online map, respectively. σm and
σz represent the standard deviation of the intensity value.
SSD (Sum of Squared Differences) is used to evaluate the
similarity between the sensor input and the map. The impact
of the environment change is implicitly diminished by the
variance term. Two signiﬁcantly different intensity values
with both low variance values imply the environment change.
Similarly, the SSD of altitude is taken into account as

below:

SSDa =

(am(i−x,j−y) − az(i,j) )2,

(7)

(cid:88)

i,j

where am and az represent the average altitude of the pre-
built map and the online map, respectively. We remove the
variance term here in SSDa because the altitude is variable
vertically by nature.

The likehood of P (zr|x, y, m) and P (za|x, y, m) are

deﬁned as:

P (zr|x, y, m) = η · α− SSDr

2·Nz , P (za|x, y, m) = η · α− λ·SSDa

2·Nz

,

(8)

when the parameter α = e, they are Gaussian-like proba-
bility distribution. We change α to adjust its smoothness.

The adaptive weight parameter γ plays an important role
during the intensity and altitude fusion. We let it determined
by the variances of [P (zr|x, y, m)]xy and [P (za|x, y, m)]xy.
The variances are deﬁned by the following equations:

σ2

x =

(cid:80)

xy P (x, y)β (x − ¯x)2
xy P (x, y)β

(cid:80)

, σ2

y =

(cid:80)

xy P (x, y)β (y − ¯y)2
xy P (x, y)β

(cid:80)

,

(9)

where ¯x and ¯y are the center of mass of distribution.

Therefore, the x and y variances of intensity and altitude
y(a). The

x(a), and σ2

x(r), σ2

y(r), σ2

are represented as σ2
weight γ is computed as:

γ =

σ2
x(a)σ2
y(a) + σ2
x(a)σ2

y(a)

σ2

.

x(r)σ2

y(r)

(10)

3) Optimal Offset: The optimal offset is estimated from
the posterior distribution of the histogram ﬁlter. Instead of
using all the states of the histogram ﬁlter to calculate the
optimal offset, we use only a small squared area around the
state of the largest or the second largest posterior belief. If
the value of the second largest posterior belief achieves a
given ratio of the largest one and is closer to the center of
the histogram ﬁlter, we take the state of the second largest
posterior belief as the center of the small squared area.

Otherwise, we take the state of the largest posterior belief.
Assuming the small area is Z, the optimal offset (ˆx, ˆy) is
calculated by:

ˆx =

(cid:80)

(x,y)∈Z P (x, y)β · x
(cid:80)
(x,y)∈Z P (x, y)β

, ˆy =

(cid:80)

(x,y)∈Z P (x, y)β · y
(cid:80)
(x,y)∈Z P (x, y)β

.

(11)

4) Uncertainty Estimation: The localization result is used
to update the Kalman ﬁlter in section VI. The key issue is
the evaluation of the uncertainty associated with the state
estimates. We let the vector (cid:126)td = (ˆx, ˆy)T and (cid:126)tx,y = (x, y)T
be the optimized offset and the offset of the (x, y) cell,
respectively. The resulting covariance matrix Cxy can be
computed as:

Cxy =

(cid:80)

1
x,y P (x, y)β ·

(cid:88)

xy

P (x, y)β ·((cid:126)tx,y −(cid:126)td)((cid:126)tx,y −(cid:126)td)T . (12)

In Figure 6 from the experimental section, we show samples
of the ﬁlter distribution, the estimated covariance matrix
together with the state estimates. We observe that the es-
timated covariance matrix is consistent with the observed
error compared to the ground truth.

V. GNSS BASED LOCALIZATION
The RTK algorithm is implemented to fully utilize the
properties of other sensors. Here we present how the RTK
module is aided by our sensor fusion framework, but not the
details of the implementation of RTK itself. Without loss
of generality, GNSS single differenced (SD) pseudo-range
and phase observations between rover (r ) and base (b) on
satellite i of system s can be constructed as:

∆ρs,i
∆ϕs,i

r,b = ∆Rs,i
r,b = ∆Rs,i

r,b + ∆ls,i
r,b + ∆ls,i

r,b · dxr + C · dtr,b + (cid:15)ρ
r,b · dxr + C · dtr,b − λs,i · ∆N s,i

r,b + (cid:15)ϕ,

(13)

where ∆ represents single differenced calculating, and ρ is
pseudo-range while ϕ for phase in meters, and N stands for
ambiguities to be resolved, with λ being the wavelength of
satellite i ’s certain band. R is the geometry distance and l for
observing matrix, both computed from satellite position with
a prior rover position x0
r, whose estimated correction is dxr.
t represents relative receiver clock offsets to be estimated,
with C equaling the light speed. (cid:15)ϕ, (cid:15)ρ, are noises on phase
and range, respectively.

To improve the ambiguity resolution success rate, we
use all GPS, BeiDou and GLONASS observations currently
available, and the GLONASS inter-frequency bias is esti-
mated [26]. With least squares, we can get SD ﬂoat ambigu-
ities and their covariance, then apply a transformation matrix
to convert them into double differenced (DD) ambiguities
with integer nature, after which, MLAMBDA [27] is utilized
to resolve ambiguities.

Although the ambiguity resolved (AR) RTK solution is
preferred,
there are indeed many situations with severe
multipath and signal blockage when it is difﬁcult to resolve
ambiguities, for example, under urban buildings or in forests,
where only a ﬂoated ambiguity RTK solution or code-based
differential GNSS are available with sub-meter accuracy.
Overall in our framework, the GNSS positioning result, either
AR-RTK or ﬂoat RTK, is used to update the Kalman ﬁlter
with the corresponding uncertainty δg, computed from:

g = (BT P B)−1 ·
δ2

V T P V
n − r

,

(14)

where V and P respectively represent posterior residuals and
weight matrix for SD observations, of which the number is
n and r is the number of estimated states in Equation 13,
derived from which, B as observing matrix with its i row
Bi = [∆li C].

A. INS-aided Ambiguity Resolution

Without

the aid of other sensors,

the success rate of
ambiguity resolution would heavily depend on pseudo-range
precision and under urban buildings with serve multi-path,
the rate may degrade greatly. However in our work, INS
constrained by LiDAR (in section IV) and/or GNSS, can
provide a promising prediction to narrow down the ambiguity
search space. For example, when passing through a tunnel
without GNSS signals, the framework could continue to work
with LiDAR constraining INS errors and provide an accurate
position prediction to help resolve ambiguities until GNSS
signals are reacquired.

Currently, our work only loosely couples the sensor obser-
vations (tightly coupling is planned for the future), and here
we simply take the INS integration result xins as a virtual
observation with its variance Rins: ˆxr = xins.

B. Phase Cycle Slip Detecting

When the GNSS receiver loses its lock on signal tracking,
a sudden jump of carrier phase measurements, called cycle
slip, can happen, which then forces discontinuity of integer
ambiguity and worsens the positioning [28]. Furthermore,
under urban environments where GNSS signals are com-
monly obstructed and reﬂected, cycle slips can occur much
more frequently than in static open conditions and should
be detected and repaired to improve RTK performance for a
mobile vehicle.

Here we estimate the rover’s position incremental offset
and receiver clock drift between two consecutive epochs(say
k and k − 1), at 5Hz, based on consideration of the satellite
geometry, with tropospheric and ionospheric delays remain-
ing unchanged. Time differenced observations can be written
as below:
∆ρs,i
∆ϕs,i
− λs,i · ∆N s,i

k,k−1 · dxk + C · dtk,k−1 + (cid:15)ρ
k,k−1 · dxk + C · dtk,k−1

k,k−1 = ∆Rs,i
k,k−1 = ∆Rs,i

k,k−1 + ∆ls,i
k,k−1 + ∆ls,i

(15)

k,k−1 + (cid:15)ϕ.

The SD phase ambiguities are estimated with SD pseudo-
range. Again, the transformed DD ﬂoat ambiguities together
with their covariance are used to obtain ﬁxed ambiguities
with MLAMBDA engine. Obviously, a nonzero element in
ﬁxed DD ambiguities vector indicates a cycle slip on a
related carrier phase that deserves a new ambiguity resolution
process in section V-A.

VI. SENSOR FUSION

In our fusion framework, an error-state Kalman ﬁlter is
applied to fuse the localization measurements, discussed in
the above section, with IMU. The fusion framework can
optimally combine the orientation rate and accelerometer
information from IMU, for improved accuracy. IMU is suf-
ﬁciently accurate to provide robust state estimates between
LiDAR and RTK measurements.

A. SINS Kinematics Equation and Error Equation

A Strap-down Inertial Navigation System (SINS) estimates
the position, velocity and attitude by integrating the IMU
data. In this paper, we choose east-north-up (ENU) as the
navigation reference frame (n), and right-forward-up (RFU)
as the body frame (b) [5], and we also use the earth frame
(e) and the inertial frame (i) [29]. Primarily, the differential
equation [5] [29] [1] in n frame of SINS is well known as:

b (f b − b a) − (2ωn

ie + ωn

en) × v n + g n

(16)

˙v n = C n
˙r = Rcv n

˙q n

b =

∗ (ωb

nb×) ⊗ q n
b ,

1
2

where r = (λ, L, a)T is the vehicle position; v n is the
vehicle velocity; q n
b is the attitude quaternion from b frame
to n frame; C n
b is the direction cosine matrix from b frame
to n frame; g n is the gravity; b g is the gyroscopes biases; b a
ib, f b are the IMU gyroscopes
is the accelerometers biases; ωb
and accelerometers output respectively; ωγ
αβ is the angular
rate of β frame with respect to α frame, resolved in γ frame;
⊗ is the quaternion multiplication operator; Rc transforms
the integration of velocity to longitude λ, latitude L and
RM +a , 1), where
altitude a, and Rc = diag(
RN , RM are the transverse radius and the meridian radius,
respectively.

1
(RN +a)cos(L) ,

For the tactical grade MEMS IMU, the IMU biases can be
modeled as a constant value model. When we combine the
SINS with other supplementary sensors, the RTK or LiDAR,
using an error-state Kalman ﬁlter, a SINS error model is
necessary. The ψ angle model error equation for the velocity,
position and attitude error can be expressed as follows in the
navigation frame [30]:

1

b δf b

ie + ωn

en) × δv n + C n

δ ˙v n = C n
δ ˙r = −ωn
δ ˙ψ = −ωn

b f b × δψ − (2ωn
en × δr + Rcδv n
b δωb
in × δψ − C n
ib,
where δv n, δr , δf b, δωb
ib are the error of v n, r , f b, ωb
respectively; δψ is the error of attitude angle.
B. Filter State Equation

(17)

ib

Because the SINS error grows over time, to get a precise
PVA, we use an error-state Kalman ﬁlter to estimate the error
of the SINS and use the estimated error-state to correct the
SINS.

Especially for q n

b in Equation 16, the perturbation quater-
b can be expressed as a small angle approximation

nion δq n
when it is assumed to be a very small angle around 0:

δq n

b = exp(

(cid:21)

(cid:20) 0
1
2 δψ

) =

(cid:34)
sin((cid:13)

cos((cid:13)
(cid:13) δψ
2

(cid:13)
(cid:13) δψ
(cid:13))
2
(cid:13)
(cid:13)) δψ
(cid:107)δψ(cid:107)

(cid:35)

≈ (

(cid:21)

(cid:20) 1
1
2 δψ

).

(18)

Therefore, we choose the state variables as X =
, and the state variables’ error as

b a b g

v n q n
b

(cid:3)T

δv n

δψ δb a

δb g

(cid:3)T

.

(cid:2)r
δX = (cid:2)δr

From the SINS error Equation (17) and the IMU model,
we can obtain the state equation of the Kalman ﬁlter as
follows:

δ ˙X = F (X )δX + G(X )W ,

(19)

where W = (cid:2)w a w g w ba w bg
is the system noise,
which comprises the IMU output noise and the IMU bias
noise.

(cid:3)T

Z L = (λS , LS , aS , hS )T − (λL, LL, aL, hL)T

= H LδX + V L,

(20)

Fig. 4: Delay and disorder measurement processing. ta, tb and tc
represent the IMU time series, the measurement occurred time series, and
the measurement received time series, respectively. The rectangle and the
star represent two measurements that received in the wrong order.

(·×) denotes the skew-symmetric matrix of a vector.
F (X ) and G(X ) can be expressed as the following
equations:


−((2ωn

en)×)

Rc
ie + ωn
03×3
06×3


((C n

03×3
b f b)×)
in ×)

−(ωn

06×3

03×3
03×3
C n
03×3
b
03×3 −C n
b
06×3
06×3







−(ωn

en ×)

03×3
03×3
06×3










F (X ) =

G(X ) =

03×3
03×3
C n
03×3
b
03×3 −C n
b
06×3
06×3

03×6
03×6
03×6
I6×6

.




C. Filter Measurement Update Equation

The measurement update comprises the LiDAR and GNSS
parts. The measurement update step of the error-state Kalman
ﬁlter updates the uncertainty of the state given a global
correction Z . The rest of the time or measurement update
just follows the standard Kalman ﬁlter.

1) LiDAR Measurement Update Equation: LiDAR based
localization outputs the position and heading angle of the
vehicle as the ﬁlter measurement. The measurement update
equation can be expressed as follows:

where the variable with S subscript is the prediction from
SINS and the variable with L subscript
is the LiDAR
measurement. V L is the estimation noise of the LiDAR with
zero mean and its covariance matrix is RL. From Equation
(12), we compute the corresponding part of λ and L in RL.
We set the part of a and h constant. H L can be obtained
from following equation:

(cid:34)I3×3
01×3

H L =

03×3
01×3

03×1
−c12c32
22+c2
c2
12
where cij is the i row and j column element of C n
b .

03×1
−c22c32
22+c2
c2
12

03×6
03×6

03×1
1

(cid:35)

2) GNSS Measurement Update Equation: GNSS can es-
timate the position of the vehicle. The measurement update
equation can be expressed as follows:

, (21)

Z G = (λS , LS , aS )T − (λG, LG, aG)T

= H GδX + V G,

(22)

where the variable with G subscript is the GNSS mea-
surement. V G is the estimation noise of the GNSS with
zero mean and its covariance matrix is RG obtained from
Equation (14). H G is deﬁned as: H G = (cid:2)I3×3 03×12
D. Delay Handling

(cid:3).

Due to the transmission and computation delay, the mea-
surement delay and disorder must be taken into considera-
tion. The solution is that we maintain two ﬁlters and a ﬁxed
length buffer of the ﬁlter states, the ﬁlter measurements and
the IMU data, in chronological order. The ﬁlter-1 computes
the real-time PVA and its covariance by performing the time
update and integrating the IMU data instantly when new
IMU data is received. The ﬁlter-2 processes the delayed
measurement. When a new measurement at t1 is received,
we execute the following actions:

1) Obtain the ﬁlter states at t1 from the buffer. Update the

ﬁlter states in ﬁlter-2.

2) Execute the measurement update at t1 in ﬁlter-2.
3) Execute the time update in ﬁlter-2 using the IMU data
in the buffer until it reaches the current time. Or we
stop at t2, if another measurement is found at t2 in the

buffer, where t2 is later than t1. This measurements at
t1 and t2 are received in the wrong order.

4) Execute the measurement update at t2, if there is another
measurement at t2. Then repeat Step 3 and ﬁnd more
measurements received in the wrong order.

5) When we ﬁnish the time update and reach the current
time, the ﬁlter states are in the buffer and the states
of the ﬁlter-1 are updated according to the new results
starting from t1 to the current time.
Figure 4 is used to illustrate this procedure.

VII. EXPERIMENTAL RESULTS

Our testing platform is shown in Fig 1. Ground-truth
vehicle motion trajectories are generated using ofﬂine meth-
ods for quantitative analysis. In open spaces with good
GNSS signal reception, the GNSS/INS solution based on
post-processing algorithms, such as the NovAtel Inertial
Explorer, is able to produce enough accurate vehicle motion
trajectories. In weak GNSS signal scenarios, such as complex
urban roads, we treat it as a classic map reconstruction
problem combining several techniques including NovAtel IE
post-processing, LiDAR SLAM, loop closure, and the global
pose-graph optimization. We only show qualitative results in
demo video clips for GNSS-denied scenes, such as tunnel or
underground garage. Thus, we classify our testing datasets
(60km in total) into three general categories: 1) 48.1km
regular roads: YF-1, YF-2, YF-3, YF-4, YF-5 covering
common road conditions, such as urban, countryside, and
a trafﬁc jam. 2) 10.4km weak GNSS signal roads: HBY-1,
DS-1 covering narrow roads lined with tall buildings or trees.
3) 2.1km GNSS-denied roads: DT-1 covering tunnels.
A. Quantitative Analysis

Our system has been extensively tested in real-world
driving scenarios. We compare our
localization perfor-
mance against the state-of-the-art intensity-based localization
method proposed by Levinson et al. [2], [12]. In order to
explicitly demonstrate the contribution of different sensors,
the test results are shown in two modes: 1) 2-Systems:
LiDAR + IMU 2) 3-Systems: LiDAR + GNSS + IMU. In
Table I, we show the quantitative results in both regular or
weak GNSS roads. Note our vast performance improvement
over [12] and the robust and accurate localization results
in both regular and weak GNSS scenarios with centimeter
level accuracy. That both the 2-Systems and 3-Systems work
well demonstrates that our system does not rely on a single

sensor but fuses the sensors input using resilient and adaptive
methods. We display the lateral and longitudinal error over
time in Fig. 5. As exhibited, our proposed solution is able to
achieve better performance over [12] consistently over time.
B. Qualitative Analysis

For GNSS-denied roads, we do not present quantitative
comparisons due to the lack of ground truth. In our additional
video clips, we show the qualitative comparison between our
results and NovAtel’s GNSS RTK/IMU poses. The reason we
did not show the result of [12] is that it fails when NovAtel’s
RTK/IMU poses are not stable and smooth enough.

In Figure 6, we give performance analysis of each module
and function in our system in detail. (a) shows a typical case
where both the RTK and LiDAR give good results, as does
the fused system. In (b), the LiDAR fails due to an outdated
map. However, the fused system gives excellent results with
the aid of the RTK. We show an opposite example in (c)
where the RTK is poor due the signal blockage, and the
LiDAR is in good working condition. (d) and (e) demonstrate
the good performance of our system in crowded scenes with
people or cars. (f) shows a very interesting case with a newly
paved road and a recently built wall. The LiDAR cannot
handle such signiﬁcant environmental changes based only on
the intensity cues. It gives good results when we adaptively
fuse the additional altitude cues.

Logs

Method

Regular
Roads

Weak-GNSS
Roads

[12]
2-Sys
3-Sys

[12]
2-Sys
3-Sys

Horiz.
RMS

Horiz.
Max

0.209
0.075
0.054

0.143
0.070
0.073

1.934
0.560
0.551

0.737
0.315
0.258

Long.
RMS

0.097
0.050
0.032

0.088
0.050
0.053

Lat.
RMS

0.161
0.045
0.036

0.093
0.039
0.041

< 0.3m
Pct.

82.25%
99.42%
99.54%

95.02%
99.99%
100.0%

TABLE I: Quantitative comparison with [12]. The performance of two
modes of our system is shown: 1) 2-Systems: LiDAR + IMU; 2) 3-Systems:
LiDAR + GNSS + IMU. The beneﬁts of GNSS in regular roads are clearly
visible. Our localization error is far lower than [12].

C. Detailed Analysis of LiDAR

To demonstrate the effectiveness of each of the above con-
tributions in Section IV, we show the localization accuracy
with different methods in Table II using the 2-Systems. We
also introduce a special data log (YF-6), which includes
a district where the road was newly paved after the map
data collection. Intensity denotes the baseline method
where only intensity cues are used. In Heading, we add
the heading estimation step, which is especially helpful
for the low-grade IMU. In FixedAlt, we incorporate the
altitude cues during the LiDAR matching. Note the large
improvement that clearly demonstrates the effectiveness of
the altitude cues. Further, in AdaptAlt, we incorporate the
altitude cues with adaptive weights. From the result of YF-4
in table II, we observe that our heading angle optimization
step is crucial to LiDAR localization. The result of YF-6 in
table II indicates that our adaptive weighting strategy makes
the system more robust to environmental changes, such as

road construction, and seasonal variations. Demonstrably,
this gives us the lowest localization errors in all metrics.

Logs Method

YF-4

YF-6

Intensity
Heading
FixedAlt
AdaptAlt

Intensity
Heading
FixedAlt
AdaptAlt

Horiz.
RMS

0.153
0.081
0.074
0.068

0.508
0.485
0.216
0.057

Horiz.
Max

1.090
0.461
0.341
0.307

11.928
12.771
8.016
0.319

Long.
RMS

0.107
0.060
0.054
0.048

0.368
0.352
0.151
0.038

Lat.
RMS

0.085
0.042
0.040
0.039

0.293
0.282
0.129
0.035

< 0.3m
Pct.

94.41%
99.63%
99.97%
99.98%

92.72%
95.68%
95.89%
99.99%

TABLE II: Comparison of localization errors of various methods used in
our system. The beneﬁts of each of heading angle reﬁnement, altitude cues
and adaptive weights are clearly visible.

D. Run-time Analysis

Our system primarily contains three modules: LiDAR,
GNSS, and SINS. LiDAR, GNSS, and SINS work at 10hz,
5hz, and 200hz, respectively. There are two implementations
designed for different applications. One occupies only a
single CPU core. The other uses a single CPU core plus a
FPGA. In the single core version, we reduce the computation
load by using a smaller histogram ﬁlter size and downsam-
pling data during the heading angle evaluation. Both versions
provide similar localization results in terms of accuracy, but
a larger histogram ﬁlter size can potentially increase the
possibility of the convergence when the ﬁlter drifts away
in an abnormal event. GNSS and SINS modules only take
about 0.2 CPU core.

VIII. CONCLUSION AND FUTURE WORK

We have presented a complete localization system, de-
signed for fully autonomous driving applications. It adap-
tively fuses the input from complementary sensors, such
as GNSS, LiDAR and IMU, to achieve good localization
accuracy in various challenging scenes,
including urban
downtown, highways or expressways, and tunnels. Our sys-
tem achieves 5-10cm RMS accuracy both longitudinally
and laterally and is ready for industrial use by having two
versions with different computing hardware requirements.
Our system, deployed in a large autonomous driving ﬂeet,
makes our vehicles fully autonomous in crowded city streets
every day. The generality of our fusion framework means
it can be used to readily fuse more sensors at various cost
levels, facing different applications. Actually, we have begun
testing our system with low-cost, low-end MEMS IMU. Our
future work also includes building a low-cost localization
solution designed for ADAS or Level 3 self-driving car.

ACKNOWLEDGMENT

We would like to thank our colleagues for their kind help and
support throughout the project. Weixin Lu helped with the vehicle
preparation. Yao Zhou and Cheng Wang helped with the demo
video production. Shichun Yi, Li Yu and Cheng Wang generated
the LiDAR map. Nadya Bosch helped with the text editing.

(a) Longitudinal RMS of YF-1

(b) Lateral RMS of YF-1

Fig. 5: Quantitative comparison with [12]. Our proposed solution achieves better performance over [12] consistently over time.

(a)

(b)

(c)

(d)

(e)

(f)

Fig. 6: Performance analysis of each individual function. The ﬁrst row is the front camera image of the scene. The ﬁgures in the second row indicate
the performance of our system. Green, blue and red ellipses represent the uncertainty of the localization estimation of each method: Fusion, LiDAR and
RTK, respectively. The small blue ﬁgure in the corner represents the posterior density in the histogram ﬁlter of the LiDAR module. Figures (a) - (f) show
the performance of our system under different circumstances. (a) Both the LiDAR and RTK give good results. (b) The LiDAR gives poor results due to
an outdated map. (c) The RTK gives unstable results due to the signal blockage. (d) - (e) The system performs well with crowds or cars around. (f) shows
that the LiDAR fails when only the intensity cue is used but succeeds when we make use of both the intensity and altitude cues on a newly paved road
with a recently built wall.

REFERENCES

[1] P. D. Groves, Principles of GNSS, inertial, and multisensor integrated

navigation systems. Artech house, 2013.

[2] J. Levinson, M. Montemerlo, and S. Thrun, “Map-based precision
vehicle localization in urban environments.” in RSS, vol. 4, 2007, p. 1.
[3] R. W. Wolcott and R. M. Eustice, “Fast LIDAR localization using

multiresolution Gaussian mixture maps,” in ICRA, 2015.

[4] A. Soloviev, “Tight coupling of GPS,

laser scanner, and inertial
measurements for navigation in urban environments,” in ION, 2008.
[5] Y. Gao, S. Liu, M. M. Atia, and A. Noureldin, “INS/GPS/LiDAR
integrated navigation system for urban and indoor environments using
hybrid scan matching algorithm,” Sensors, vol. 15, no. 9, 2015.
[6] A. Soloviev, D. Bates, and F. GRAAS, “Tight coupling of laser scanner
and inertial measurements for a fully autonomous relative navigation
solution,” Navigation, vol. 54, no. 3, pp. 189–205, 2007.

[7] J. Tang, Y. Chen, X. Niu, L. Wang, L. Chen, J. Liu, C. Shi, and
J. Hyypp¨a, “LiDAR scan matching aided inertial navigation system in
GNSS-denied environments,” Sensors, vol. 15, 2015.

[8] G. Hemann, S. Singh, and M. Kaess, “Long-range GPS-denied aerial

inertial navigation with LIDAR localization,” in IROS, 2016.

[9] R. Kummerle, D. Hahnel, D. Dolgov, S. Thrun, and W. Burgard,
“Autonomous driving in a multi-level parking structure,” in ICRA,
2009, pp. 3395–3400.

[10] K. Yoneda, H. Tehrani, T. Ogawa, N. Hukuyama, and S. Mita, “LiDAR
scan feature for localization with highly precise 3D map,” in IV, 2014.
[11] P. J. Besl, N. D. McKay et al., “A method for registration of 3-d

shapes,” T-PAMI, vol. 14, no. 2, pp. 239–256, 1992.

[12] J. Levinson and S. Thrun, “Robust vehicle localization in urban

environments using probabilistic maps.” in ICRA, 2010.

[13] M. Adams, S. Zhang, and L. Xie, “Particle ﬁlter based outdoor robot
localization using natural features extracted from laser scanners,” in
ICRA, vol. 2, 2004, pp. 1493–1498.

[14] R. K¨ummerle, R. Triebel, P. Pfaff, and W. Burgard, “Monte carlo
localization in outdoor terrains using multilevel surface maps,” Journal
of Field Robotics, vol. 25, no. 6-7, pp. 346–359, 2008.

[15] I. Baldwin and P. Newman, “Road vehicle localization with 2D Push-
broom LIDAR and 3D priors,” in ICRA, 2012, pp. 2611–2617.

[16] ——, “Laser-only road-vehicle localization with dual 2D Push-broom

LiDARs and 3D priors,” in IROS, 2012, pp. 2490–2497.

[17] M. Sheehan, A. Harrison, and P. Newman, “Continuous vehicle
localisation using sparse 3D sensing, kernelised r´enyi distance and
fast gauss transforms,” in IROS, 2013, pp. 398–405.

[18] Z. Chong, B. Qin, T. Bandyopadhyay, M. H. Ang, E. Frazzoli, and
D. Rus, “Synthetic 2D LiDAR for precise vehicle localization in 3D
urban environment,” in ICRA, 2013, pp. 1554–1559.

[19] W. Maddern, G. Pascoe, and P. Newman, “Leveraging experience for

large-scale LIDAR localisation in changing cities,” in ICRA, 2015.

[20] R. W. Wolcott and R. M. Eustice, “Robust LIDAR localization using
multiresolution gaussian mixture maps for autonomous driving,” The
International Journal of Robotics Research, vol. 36, 2017.

[21] E. B. Olson, “Real-time correlative scan matching,” in ICRA, May

2009, pp. 4387–4393.

[22] B. D. Lucas and T. Kanade, “An iterative image registration technique
with an application to stereo vision,” in IJCAI, ser. IJCAI, San
Francisco, CA, USA, 1981, pp. 674–679.

[23] S. Baker and I. Matthews, “Lucas-kanade 20 years on: A unifying

framework,” IJCV, vol. 56, no. 3, pp. 221–255, 2004.

[24] S. Thrun, W. Burgard, and D. Fox, Probabilistic robotics. MIT press,

2005.

[25] S. Kullback and R. A. Leibler, “On information and sufﬁciency,” Ann.

Math. Statist., vol. 22, no. 1, pp. 79–86, 03 1951.

[26] L. Wanninger, “Carrier-phase inter-frequency biases of GLONASS

receivers,” Journal of Geodesy, vol. 86, no. 2, pp. 139–148, 2012.

[27] T. Z. X.-W. Chang, X. Yang, “MLAMBDA: a modiﬁed LAMBDA
method for integer least-squares estimation,” Journal of Geodesy,
2005.

[28] T. Takasu and A. Yasuda, “Cycle slip detection and ﬁxing by MEMS-
IMU/GPS integration for mobile environment RTK-GPS,” in ION,
2008, pp. 64–71.

[29] P. G. Savage, “Strapdown inertial navigation integration algorithm
design part 2: Velocity and position algorithms,” Journal of Guidance
Control and Dynamics, vol. 21, no. 2, pp. 208–221, 1998.

[30] D. O. Benson, “A comparison of two approaches to pure-inertial and
Doppler-inertial error analysis,” T-AES, no. 4, pp. 447–455, 1975.

