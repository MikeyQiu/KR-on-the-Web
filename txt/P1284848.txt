Generalized Robust Bayesian Committee Machine for Large-scale Gaussian
Process Regression

8
1
0
2
 
n
u
J
 

3

 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
0
2
7
0
0
.
6
0
8
1
:
v
i
X
r
a

Haitao Liu 1 Jianfei Cai 2 Yi Wang 3 Yew-Soon Ong 2 4

Abstract

predictive distributions at test points.

In order to scale standard Gaussian process (GP)
regression to large-scale datasets, aggregation
models employ factorized training process and
then combine predictions from distributed ex-
perts. The state-of-the-art aggregation models,
however, either provide inconsistent predictions
or require time-consuming aggregation process.
We ﬁrst prove the inconsistency of typical aggre-
gations using disjoint or random data partition,
and then present a consistent yet efﬁcient aggre-
gation model for large-scale GP. The proposed
model inherits the advantages of aggregations,
e.g., closed-form inference and aggregation, par-
allelization and distributed computing. Further-
more, theoretical and empirical analyses reveal
that the new aggregation model performs better
due to the consistent predictions that converge
to the true underlying function when the training
size approaches inﬁnity.

1. Introduction

Gaussian process (GP) (Rasmussen & Williams, 2006) is
a well-known statistical learning model extensively used
in various scenarios, e.g., regression, classiﬁcation, opti-
mization (Shahriari et al., 2016), visualization (Lawrence,
2005), active learning (Fu et al., 2013; Liu et al., 2017) and
multi-task learning (Alvarez et al., 2012; Liu et al., 2018).
n
Given the training set X =
xi
i=1 and the observa-
{
n
tion set y =
i=1, as an approximation of the
R
}
{
underlying function η : Rd
R, GP provides informative

y(xi)

Rd

∈

∈

}

→

1Rolls-Royce@NTU Corporate Lab, Nanyang Technologi-
cal University, Singapore 637460 2School of Computer Science
and Engineering, Nanyang Technological University, Singapore
639798 3Applied Technology Group, Rolls-Royce Singapore, 6
Seletar Aerospace Rise, Singapore 797575 4Data Science and
Artiﬁcial Intelligence Research Center, Nanyang Technological
University, Singapore 639798. Correspondence to: Haitao Liu
<htliu@ntu.edu.sg>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

However, the most prominent weakness of the full GP is
that it scales poorly with the training size. Given n data
points, the time complexity of a standard GP paradigm
(n3) in the training process due to the inver-
scales as
O
(n2) in
sion of an n
the prediction process due to the matrix-vector operation.
This weakness conﬁnes the full GP to training data of size

n covariance matrix; it scales as

O

×

(104).

≪

The

sparse

approximations
2005)

O
To cope with large-scale regression,
various com-
putationally efﬁcient approximations have been pre-
reviewed in
sented.
(Qui˜nonero-Candela & Rasmussen,
employ m
(m
n) inducing points to summarize the whole training
data (Seeger et al., 2003; Snelson & Ghahramani, 2006;
2007; Titsias, 2009; Bauer et al., 2016), thus reducing the
(nm2) and the predict-
training complexity of full GP to
ing complexity to
(nm). The complexity can be further
reduced through distributed inference, stochastic varia-
tional inference or Kronecker structure (Hensman et al.,
2013; Gal et al.,
2015;
Hoang et al., 2016; Peng et al., 2017). A main draw-
the
back of sparse approximations, however,
representational capability is limited by the number of
inducing points (Moore & Russell, 2015). For example,
for a quick-varying function, the sparse approximations
need many inducing points to capture the local structures.
That is, this kind of scheme has not reduced the scaling of
the complexity (Bui & Turner, 2014).

2014; Wilson & Nickisch,

is that

O

O

The method exploited in this article belongs to the aggre-
gation models (Hinton, 2002; Tresp, 2000; Cao & Fleet,
2014; Deisenroth & Ng, 2015; Rulli`ere et al., 2017), also
known as consensus statistical methods (Genest & Zidek,
1986; Ranjan & Gneiting, 2010). This kind of scheme pro-
duces the ﬁnal predictions by the aggregation of M sub-
models (GP experts) respectively trained on the subsets
, thus distributing
=
{D
the computations to “local” experts. Particularly, due to
the product of experts, the aggregation scheme derives a
factorized marginal likelihood for efﬁcient training; and
then it combines the experts’ posterior distributions accord-
In comparison to
ing to a certain aggregation criterion.

M
i=1 of

Xi, yi

X, y

i =

}}

D

}

{

{

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

sparse approximations, the aggregation models (i) operate
directly on the full training data, (ii) require no additional
inducing or variational parameters and (iii) distribute the
computations on individual experts for straightforward par-
allelization (Tavassolipour et al., 2017), thus scaling them
In comparison to typ-
to arbitrarily large training data.
ical local GPs (Snelson & Ghahramani, 2007; Park et al.,
2011), the aggregations smooth out the ugly discontinu-
ity by the product of posterior distributions from GP ex-
perts. Note that the aggregation methods are different from
the mixture-of-experts (Rasmussen & Ghahramani, 2002;
Yuan & Neubauer, 2009), which suffers from intractable in-
ference and is mainly developed for non-stationary regres-
sion.

However, it has been pointed out (Rulli`ere et al., 2017) that
there exists a particular type of training data such that typ-
ical aggregations, e.g., product-of-experts (PoE) (Hinton,
2002; Cao & Fleet, 2014) and Bayesian committee ma-
chine (BCM) (Tresp, 2000; Deisenroth & Ng, 2015), can-
not offer consistent predictions, where “consistent” means
the aggregated predictive distribution can converge to the
true underlying predictive distribution when the training
size n approaches inﬁnity.

Particularly,

The major contributions of this paper are three-fold. We
ﬁrst prove the inconsistency of typical aggregation mod-
els, e.g., the overconﬁdent or conservative prediction vari-
ances illustrated in Fig. 3, using conventional disjoint or
random data partition. Thereafter, we present a consis-
tent yet efﬁcient aggregation model for large-scale GP
the proposed generalized ro-
regression.
bust Bayesian committee machine (GRBCM) selects a
global subset to communicate with the remaining sub-
sets, leading to the consistent aggregated predictive dis-
theo-
tribution derived under the Bayes rule.
retical and empirical analyses reveal that GRBCM out-
performs existing aggregations due to the consistent yet
efﬁcient predictions. We release the demo codes in
https://github.com/LiuHaiTao01/GRBCM.

Finally,

2. Aggregation models revisited

2.1. Factorized training

A GP usually places a probability distribution over the la-
(0, k(x, x′)), which is
tent function space as f (x)
deﬁned by the zero mean and the covariance k(x, x′). The
well-known squared exponential (SE) covariance function
is

∼ GP

k(x, x′) = σ2

f exp

1
2

 −

d

(xi

x′
i)2

−
l2
i

,

!

i=1
X
where σ2
f is an output scale amplitude, and li is an input
length-scale along the ith dimension. Given the noisy ob-
servation y(x) = f (x) + ǫ where the i.i.d. noise follows

|

i=1
Y
(0, Ki + σ2

(0, σ2

∼ N

ǫ ) and the training data

ǫ
(0, k(X, X) + σ2
likelihood p(y
represents the hyperparameters to be inferred.

, we have the marginal
ǫ I) where θ

X, θ) =

N

D

|

i

In order to train the GP on large-scale datasets, the aggrega-
tion models introduce a factorized training process. It ﬁrst
partitions the training set
,
}
i. In
1
data partition, we can assign the data points randomly to
the experts (random partition), or assign disjoint subsets
obtained by clustering techniques to the experts (disjoint
partition).
Ignoring the correlation between the experts

M , and then trains GP on

D
{
i as an expert

into M subsets

Xi, yi

i =

M

≤

≤

D

D

M
i=1 leads to the factorized approximation as

i

{M

}

M

p(y

X, θ)

|

≈

pi(yi

Xi, θi),

(2)

M

|
∈

∼ N

Xi, θi)

i. Note that for simplicity all

ǫ,iIi) with Ki =
where pi(yi
Rni×ni and ni being the training size
k(Xi, Xi)
of
the M GP ex-
perts in (2) share the same hyperparameters as θi = θ
(Deisenroth & Ng, 2015). The factorization (2) degener-
ates the full covariance matrix K = k(X, X) into a diag-
onal block matrix diag[K1,
≈
diag[K −1
M ]. Hence, compared to the full GP,
1 ,
the complexity of the factorized training process is reduced
to

, KM ], leading to K −1

0) given ni = m0 = n/M , 1

, K −1

(nm2

M .

· · ·

· · ·

i

≤

≤

O

Conditioned on the related subset
i, x∗)
bution pi(y∗

(µi(x∗), σ2

D

i, the predictive distri-
i (x∗)) of

i has1

|D
∼ N
µi(x∗) = kT
i∗[Ki + σ2
σ2
i (x∗) = k(x∗, x∗)

ǫ I]−1yi,
kT
i∗[Ki + σ2

−

M

(3a)

ǫ I]−1ki∗ + σ2

ǫ , (3b)

where ki∗ = k(Xi, x∗). Thereafter, the experts’ predic-
M
i=1 are combined by the following aggrega-
tions
tion methods to perform the ﬁnal predicting.

µi, σ2
i }

{

2.2. Prediction aggregation

The state-of-the-art aggregation methods include PoE
(Hinton, 2002; Cao & Fleet, 2014), BCM (Tresp, 2000;
Deisenroth & Ng, 2015), and nested pointwise aggregation
of experts (NPAE) (Rulli`ere et al., 2017).

For the PoE and BCM family, the aggregated prediction
mean and precision are generally formulated as

µA(x∗) = σ2

A(x∗)

βiσ−2
i

(x∗)µi(x∗),

(4a)

M

i=1
X

M

i=1
X

M

−

i=1
X

1Instead of using pi(f∗|Di, x∗) in (Deisenroth & Ng, 2015),
we here consider the aggregations in a general scenario where
each expert has all its belongings at hand.

(1)

σ−2
A (x∗) =

βiσ−2
i

(x∗) + (1

βi)σ−2
∗∗ ,

(4b)

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

where the prior variance σ2
a correction term to σ−2
family; and βi is the weight of the expert

∗∗ = k(x∗, x∗) + σ2

ǫ , which is
A , is only available for the BCM

i at x∗.

M

The predictions of the PoE family, which omit the prior
precision σ−2
∗∗ in (4b), are derived from the product of M
experts as

M

pA(y∗

, x∗) =

|D

pβi
i (y∗

i, x∗).

|D

(5)

i=1
Y
The original PoE (Hinton, 2002) employs the constant
weight βi = 1, resulting in the aggregated prediction vari-
ances that vanish with increasing M . On the contrary, the
generalized PoE (GPoE) (Cao & Fleet, 2014) considers a
varying βi = 0.5(log σ2
i (x∗)), which represents
the difference in the differential entropy between the prior
i, x∗), to weigh the con-
p(y∗
|
i at x∗. This varying βi brings the ﬂexi-
tribution of
bility of increasing or reducing the importance of experts
based on the predictive uncertainty. However, the vary-
ing βi may produce undesirable errors for GPoE. For in-
stance, when x∗ is far away from the training data such
that σ2

x∗) and the posterior p(y∗

σ2
∗∗, we have βi

0 and σ2

log σ2

i (x∗)

∗∗ −

M

|D

.
GPoE → ∞

→

→

The BCM family, which is opposite to the PoE family, ex-
x∗) when combin-
plicitly incorporates the GP prior p(y∗
j, BCM intro-
ing predictions. For two experts
M
duces a conditional independence assumption
y∗,
leading to the aggregated predictive distribution as

|
i and

j
⊥ D

i
D

M

|

pA(y∗

, x∗) =

|D

M

i (y∗

i=1 pβi
|D
pPi βi−1(y∗
|

i, x∗)
x∗)

.

(6)

Q
The original BCM (Tresp, 2000) employs βi = 1 but
its predictions suffer from weak experts when leaving the
data. Hence, inspired by GPoE, the robust BCM (RBCM)
(Deisenroth & Ng, 2015) uses a varying βi
to produce
robust predictions by reducing the weights of weak ex-
perts. When x∗ is far away from the training data X,
the correction term brought by the GP prior in (4b) helps
the (R)BCM’s prediction variance recover σ2
∗∗. However,
given M = 1,
the predictions of RBCM as well as
GPoE cannot recover the full GP predictions because usu-
ally β1 = 0.5(log σ2
log σ2
log σ2
f ull(x∗))

1(x∗)) = 0.5(log σ2

∗∗ −

∗∗ −

= 1.

To achieve computation gains, the above aggregations intro-
duce additional independence assumption for the experts’
predictions, which however is often violated in practice
and yields poor results. Hence, in the aggregation process,
NPAE (Rulli`ere et al., 2017) regards the prediction mean
µi(x∗) in (3a) as a random variable by assuming that yi has
not yet been observed, thus allowing for considering the co-
variances between the experts’ predictions. Thereafter, for
, µM , y∗]T, the covariances are
the random vector [µ1,

· · ·

derived as

cov[µi, y∗] = kT

cov[µi, µj] =

i,ǫ ki∗,

i∗K −1
kT
i∗K −1
kT
i∗K −1

(

i,ǫ Kij K −1
i,ǫ Kij,ǫK −1

j,ǫ kj∗,
j,ǫ kj∗,

i

= j,

i = j,

(7a)

(7b)

ǫ I,
where Kij = k(Xi, Xj)
Kj,ǫ = Kj + σ2
ǫ I, and Kij,ǫ = Kij + σ2
ǫ I. With these
covariances, a nested GP training process is performed to
derive the aggregated prediction mean and variance as

Rni×nj , Ki,ǫ = Ki + σ2

∈

µNPAE(x∗) = kT
A∗K −1
σ2
NPAE(x∗) = k(x∗, x∗)

A µ,

kT
A∗K −1

A kA∗ + σ2
ǫ ,

−

(8a)

(8b)

RM×M has K ij

RM×1 has the ith element as cov[µi, y∗],
where kA∗
∈
A = cov[µi, µj], and µ =
KA
∈
, µM (x∗)]T. The NPAE is capable of provid-
[µ1(x∗),
ing consistent predictions at the cost of implementing a
much more time-consuming aggregation because of the in-
version of KA at each test point.

· · ·

2.3. Discussions of existing aggregations

→ ∞

Though showcasing promising results (Deisenroth & Ng,
2015), given that n
and the experts are noise-free
GPs, (G)PoE and (R)BCM have been proved to be in-
consistent, since there exists particular triangular array of
data points that are dense in the input domain Ω such that
the prediction variances do not go to zero (Rulli`ere et al.,
2017).

Particularly, we further show below the inconsistency of
(G)PoE and (R)BCM using two typical data partitions (ran-
dom and disjoint partition) in the scenario where the obser-
vations are blurred with noise. Note that since GPoE us-
ing a varying βi may produce undesirable errors, we adopt
βi = 1/M as suggested in (Deisenroth & Ng, 2015). Now
the GPoE’s prediction mean is the same as that of PoE; but
the prediction variance blows up as M times that of PoE.

→ ∞

, let X

∈
[0, 1]d such that for any x

Rn×d be dense
Deﬁnition 1. When n
Ω we have
in Ω
∈
= 0. Besides, the underlying
limn→∞ min1≤i≤n
k
function to be approximated has true continuous response
µη(x) and true noise variance σ2
η.

xi

−

∈

x

k

Firstly, for the disjoint partition that uses clustering tech-
into disjoint local subsets
niques to partition the data
D
M
,
i=1, The proposition below reveals that when n
i
{D
→ ∞
PoE and (R)BCM produce overconﬁdent prediction vari-
ance that shrinks to zero; on the contrary, GPoE provides
conservative prediction variance.

}

Mn
Proposition 1. Let
i=1 be a disjoint partition of the
training data
i be
GP with zero mean and stationary covariance function

}
. Let the expert

i trained on

i
{D

M

D

D

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

∈

≤

k(.) > 0. We further assume that (i) limn→∞ Mn =
∞
and (ii) limn→∞ n/M 2
n > 0, where the second condition
implies that the subset size m0 = n/Mn and the num-
ber of experts Mn are comparable such that too weak ex-
perts are not preferred. Besides, from the second condition
, which implies that the experts
we have m0 →
become more informative with increasing n. Then, PoE
and (R)BCM produce overconﬁdent prediction variance at
x∗

Ω as

n→∞

∞

lim
n→∞

σ2
A,n(x∗) = 0,

(9)

whereas GPoE yields conservative prediction variance

σ2
η < lim
n→∞

σ2
A,n(x∗) < σ2

bn (x∗) < σ2

∗∗,

(10)

where σ2
bn

bn (x∗) is offered by the farthest expert
M
Mn) whose prediction variance is closet to σ2

bn (1
∗∗.

≤

i

M

and

i=1 σ−2

−2
σ
i
−2
P σ
i

The detailed proof is given in Appendix A. Moreover, we
have the following ﬁndings.
Remark 1. For the averaging σ−2
M
i=1

GPoE = 1
M
µi using disjoint partition, more
µ(G)PoE =
and more experts become relatively far away from x∗ when
, i.e., the prediction variances at x∗ approach σ2
n
∗∗
and the prediction means approach the prior mean µ∗∗.
Hence, empirically, when n
approaches σ2
bn
Remark 2. The BCM’s prediction variance is always larger
than that of PoE since

, and the µ(G)PoE approaches µ∗∗.

, the conservative σ2

→ ∞

→ ∞

GPoE

P

P

a∗ =

σ−2
PoE(x∗)
σ−2
BCM(x∗)

=

M

i=1 σ−2
(x∗)

i

(x∗)
(M

−

−

M

i=1 σ−2
P
i

> 1

1)σ−2
∗∗

P

→ ∞

for M > 1. This means σ2
PoE deteriorates faster to zero
when n
. Besides, it is observed that µBCM is a∗
times that of PoE, which alleviates the deterioration of pre-
. However, when x∗ is leaving
diction mean when n
σ−2
X, a∗
∗∗ . That is why BCM
suffers from undesirable prediction mean when leaving X.

M since σ−2

→ ∞
i

(x∗)

→

→

Secondly, for the random partition that assigns the data
points randomly to the experts without replacement, The
proposition below implies that when n
, the predic-
tion variances of PoE and (R)BCM will shrink to zero;
the PoE’s prediction mean will recover µη(x), but the
(R)BCM’s prediction mean cannot; interestingly, the sim-
ple GPoE can converge to the underlying true predictive
distribution.

→ ∞

Mn
Proposition 2. Let
i=1 be a random partition of
i
{D
with (i) limn→∞ Mn =
and (ii)
the training data
Mn
limn→∞ n/M 2
i=1 be GPs with
zero mean and stationary covariance function k(.) > 0.

n > 0. Let the experts

{M

∞

D

}

}

i

Then, for the aggregated predictions at x∗

Ω we have

∈

lim
n→∞
lim
n→∞
lim
n→∞




where a = σ−2
when σ2

η = 0.

µPoE(x∗) = µη(x∗),

µGPoE(x∗) = µη(x∗),

lim
n→∞
lim
n→∞

σ2
PoE(x∗) = 0,
GPoE(x∗) = σ2
σ2
η,

µ(R)BCM(x∗) = aµη(x∗),

lim
n→∞

σ2
(R)BCM(x∗) = 0,

(11)

η /(σ−2

η −

σ−2
∗∗ )

≥

1 and the equality holds

The detailed proof is provided in Appendix B. Proposi-
tions 1 and 2 imply that no matter what kind of data par-
tition has been used, the prediction variances of PoE and
(R)BCM will shrink to zero when n
, which strictly
limits their usability since no beneﬁts can be gained from
such useless uncertainty information.

→ ∞

As for data partition, intuitively, the random partition pro-
vides overlapping and coarse global information about the
target function, which limits the ability to describe quick-
varying characteristics. On the contrary, the disjoint parti-
tion provides separate and reﬁned local information, which
enables the model to capture the variability of target func-
tion. The superiority of disjoint partition has been empiri-
cally conﬁrmed in (Rulli`ere et al., 2017). Therefore, unless
otherwise indicated, we employ disjoint partition for the
aggregation models throughout the article.

As for time complexity, the ﬁve aggregation models have
the same training process, and they only differ in how
to combine the experts’ predictions. For (G)PoE and
(R)BCM, their time complexity in prediction scales as
(n′nm0) where n′ is the number of test
O
points.2 For the complicated NPAE, it however needs to
M matrix KA at each test point, lead-
invert an M
ing to a greatly increased time complexity in prediction as

(nm2

0) +

O

×

(n′n2).3

O
The inconsistency of (G)PoE and (R)BCM and the ex-
tremely time-consuming process of NPAE impose the de-
mand of developing a consistent yet efﬁcient aggregation
model for large-scale GP regression.

3. Generalized robust Bayesian committee

machine

3.1. GRBCM

Our proposed GRBCM divides M experts into two groups.
The ﬁrst group has a global communication expert

c

M

2O(nm2

0) is induced by the update of M GP experts after op-

timizing hyperparameters.

3The predicting complexity of NPAE can be reduced by em-
ploying various hierarchical computing structure (Rulli`ere et al.,
2017), which however cannot provide identical predictions.

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

D
−

D1, and the second group con-
trained on the subset
c =
M
1 global or local experts4
tains the remaining M
i=2
M
trained on
i=2, respectively. The training process of
GRBCM is identical to that of typical aggregations in sec-
tion 2.1. The prediction process of GRBCM, however, is
different. Particularly, GRBCM assigns the global commu-
nication expert with the following properties:

i
{D

{M

}

}

i

with

•

•

(Random selection) The communication subset
c is
a random subset wherein the points are randomly se-
lected without replacement from
. It indicates that
the points in Xc spread over the entire domain, which
enables
c to capture the main features of the target
function. Note that there is no limit to the partition
type for the remaining M

1 subsets.

M

D

D

−

c, x∗)

M
∼ N

(Expert communication) The expert
c with pre-
(µc, σ2
c ) is
dictive distribution pc(y∗
|D
allowed to communicate with each of the remain-
It means we can utilize
ing experts
}
the augmented data
to improve
D+i =
i
}
D
over the base expert
c, leading to a new expert
M
M+i with the improved predictive distribution as
(µ+i, σ2
p+i(y∗

M
i=2.

{M

+i) for 2

M .

{D

c,

i

i

|D+i, x∗)

∼ N

≤

≤

•

(Conditional independence) Given the communica-
c and y∗, the independence assumption
tion subset
= j

D
c, y∗ holds for 2

M .

i

i
D

j
⊥ D

|D

≤

≤

Given the conditional independence assumption and the
M
i=2, we approximate the exact predictive dis-
weights
}
tribution p(y∗

, x∗) using the Bayes rule as

βi

{

|D

p(y∗

, x∗)

|D

M

i=2
Y
M

p(y∗

x∗)p(

y∗, x∗)

p(

i
D

j
|{D

}

i−1
j=1, y∗, x∗)

|

|

c
D

|

c
D

|

∝

≈

=

pβi(

i
D

|D

c, y∗, x∗)

p(y∗

x∗)p(

y∗, x∗)

p(y∗

x∗)
|
pPM

M
i=2 pβi(
c
D

i=2 βi−1(

Q

i=2
Y
D+i
|
y∗, x∗)
|

y∗, x∗)

.

(12)
c, y∗, x∗) is exact with no approximation

Note that p(
D2|D
in (12). Hence, we set β2 = 1.

With (12), GRBCM’s predictive distribution is

pA(y∗

, x∗) =

|D

M

+i(y∗

i=2 pβi
i=2 βi−1

pPM
Q
c

|D+i, x∗)
c, x∗)
(y∗
|D

.

(13)

4“Global” means the expert is trained on a random subset,

whereas “local” means it is trained on a disjoint subset.

µA(x∗) = σ2

A(x∗)

βiσ−2

+i (x∗)µ+i(x∗)

M

"

i=2
X

M

−  
i=2
X
M

i=2
X

βi

1

−

σ−2
c (x∗)µc(x∗)
#

,

!

(14a)

M

−  

i=2
X

βi

1

−

!

σ−2
c (x∗).

(14b)

σ−2
A (x∗) =

βiσ−2

+i (x∗)

c

rather than the prior σ−2

Different from (R)BCM, GRBCM employs the informa-
tive σ−2
∗∗ to correct the predic-
tion precision in (14b), leading to consistent predictions
when n
, which will be proved below. Also, the
prediction mean of GRBCM in (14a) now is corrected by
µc(x∗). Fig. 1 depicts the structure of the GRBCM aggre-
gation model.

→ ∞

Figure1. The GRBCM aggregation model.

In (14a) and (14b), the parameter βi (i > 2) akin to
that of RBCM is deﬁned as the difference in the dif-
ferential entropy between the base predictive distribution
c, x∗) and the enhanced predictive distribution
pc(y∗
|D
|D+i, x∗) as
p+i(y∗

i = 2,

βi =

1,
0.5(log σ2

(

c (x∗)

log σ2

+i(x∗)),

−

3

i

M.

≤

≤

D

≥

|D+i, x∗) over pc(y∗

(15)
2) into the
i (i
c, if there is little improvement of
c, x∗), we weak the vote of

It is found that after adding a subset
communication subset
D
p+i(y∗
M+i by assigning a small βi that approaches zero.
As for the size of Xc, more data points bring more infor-
mative
c and better GRBCM predictions at the cost of
higher computing complexity. In this article, we assign all
the experts with the same training size as nc = ni = m0
M .
and n+i = 2m0 for 2

M

|D

i

≤

≤

Next, we show that the GRBCM’s predictive distribution
will converge to the underlying true predictive distribution
.
when n
→ ∞
Proposition 3. Let
ing data
limn→∞ n/M 2

Mn
i=1 be a partition of the train-
i
}
{D
with (i)
and (ii)
limn→∞ Mn =
n > 0. Besides, among the M subsets, there

∞

D

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

c, the points in which
is a global communication subset
without replacement. Let
are randomly selected from
Mn
the global expert
i=2
be GPs with zero mean and stationary covariance function
k(.) > 0. Then, GRBCM yields consistent predictions as

c and the enhanced experts

{M+i

M

D

D

}

4. Numerical experiments

4.1. Toy example

We employ a 1D toy example

µGRBCM(x∗) = µη(x∗),

GRBCM(x∗) = σ2
σ2
η.

(16)

lim
n→∞
lim
n→∞






D

D\D

The detailed proof is provided in Appendix C. It is found
in Proposition 3 that apart from the requirement that the
communication subset
c should be a random subset, the
consistency of GRBCM holds for any partition of the re-
c. Besides, according to Propositions 2
maining data
and 3, both GPoE and GRBCM produce consistent pre-
dictions using random partition. It is known that the GP
model
provides more conﬁdent predictions, i.e., lower
M
σ2(x)dx, with more data points.
uncertainty U (
Since GRBCM trains experts on more informative subsets
{D+i
Remark 3. When using random subsets, the GRBCM’s
prediction uncertainty is always lower than that of GPoE,
since the discrepancy δU −1 = U −1
GPoE satisﬁes

R
M
i=2, we have the following ﬁnding.

U −1

) =

M

}

δU −1 =

U −1(

M+2)

−

"

Mn

+

βi

σ−2
+i (x∗)

Z

i=3
X

(cid:0)

GRBCM −
Mn
1
Mn

U −1(

i=1
X
σ−2
c (x∗)

−

i)

#

M

dx∗ > 0

(cid:1)

for a large enough n. It means compared to GPoE, GR-
BCM converges faster to the underlying function when
n

.
→ ∞

Finally, similar to RBCM, GRBCM can be executed in
multi-layer computing architectures with identical predic-
tions (Deisenroth & Ng, 2015; Ionescu, 2015), which allow
to run optimally and efﬁciently with the available comput-
ing infrastructure for distributed computing.

3.2. Complexity

i

≤

M
i=1 have the same train-
Assuming that the experts
i
}
{M
M . Compared to
ing size ni = m0 = n/M for 1
≤
(G)PoE and (R)BCM, the proposed GRBCM has a higher
time complexity in prediction due to the construction of
M
i=2. In prediction, it ﬁrst needs to cal-
new experts
{M+i
}
culate the inverse of k(Xc, Xc) and M
1 augmented
−
M
Xi, Xc
covariance matrices
i=2, which
)
k(
{
}
}
{
7m3
(8nm2
0), in order to obtain the predictions
scales as
0−
O
M
M
i=2 and σ2
σ2
c ,
µc,
i=2. Then, it combines the pre-
µ+i
+i}
{
}
{
i=2 at n′ test points. Therefore,
M
c and
dictions of
{M+i
}
the time complexity of the GRBCM prediction process is
(βn′nm0), where α = (8M
7)/M and

Xi, Xc

M

}

{

,

0) +

−

(αnm2
O
β = (4M

O
3)/M .

−

f (x) = 5x2 sin(12x) + (x3
+ 4 cos(2x) + ǫ,

−

0.5) sin(3x

0.5)

−

(17)

where ǫ
existing aggregation models.

∼ N

(0, 0.25), to illustrate the characteristics of

×

×

−

M

104, 105, 5

We generate n = 104, 5
105 and 106 train-
ing points, respectively, in [0, 1], and select n′ = 0.1n test
0.2, 1.2]. We pre-normalize each col-
points randomly in [
umn of X and y to zero mean and unit variance. Due to
the global expert
c in GRBCM, we slightly modify the
disjoint partition: we ﬁrst generate a random subset and
then use the k-means technique to generate M
1 dis-
joint subsets. Each expert is assigned with m0 = 500 data
points. We implement the aggregations by the GPML tool-
box5 using the SE kernel in (1) and the conjugate gradi-
ents algorithm with the maximum number of evaluations
as 500, and execute the code on a workstation with four
3.70 GHz cores and 16 GB RAM (multi-core computing
in Matalb is employed). Finally, we use the Standard-
ized Mean Square Error (SMSE) to evaluate the accuracy
of prediction mean, and the Mean Standardized Log Loss
(MSLL) to quantify the quality of predictive distribution
(Rasmussen & Williams, 2006).

−

(a)

106

]
s
[
 
e
m

i
t
 
g
n
i
t
c
d
e
r
P

i

104

102

100

10-2

104

(b)

E
S
M
S

100

10-1

105
Training size

106

104

105
Training size

106

PoE
GPoE
BCM
RBCM
NPAE
GRBCM
Training time

(c)

L
L
S
M

1.5

0.5

2

1

0

-0.5

-1

-1.5

104

105
Training size

106

Figure2. Comparison of different aggregation models on the toy
example in terms of (a) computing time, (b) SMSE and (c) MSLL.

Fig. 2 depicts the comparative results of six aggregation
models on the toy example. Note that NPAE using n >
104 is unavailable due to the time-consuming predic-
5
tion process. Fig. 2(a) shows that these models require the
same training time, but they differ in the predicting time.

×

5http://www.gaussianprocess.org/gpml/code/matlab/doc/

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

Due to the communication expert, the GRBCM’s predict-
ing time slightly offsets the curves of (G)PoE and (R)BCM.
The NPAE however exhibits signiﬁcantly larger predicting
time with increasing M and n′. Besides, Fig. 2(b) and
(c) reveal that GRBCM and NPAE yield better predictions
with increasing n, which conﬁrm their consistency when
.6 As for NPAE, though performing slightly better
n
104, it requires several orders
than GRBCM using n = 5
of magnitude larger predicting time, rendering it unsuitable
for cases with many test points and subsets.

→ ∞

×

4.2. Medium-scale datasets

We use two realistic datasets, kin40k (8D, 104 train-
104 test points) (Seeger et al., 2003) and
ing points, 3
sarcos (21D, 44484 training points, 4449 test points)
(Rasmussen & Williams, 2006), to assess the performance
of our approach.

×

The comparison includes all the aggregations except the
expensive NPAE.8 Besides, we employ the fully indepen-
dent training conditional (FITC) (Snelson & Ghahramani,
2006), the GP using stochastic variational inference (SVI)9
(Hensman et al., 2013), and the subset-of-data (SOD)
(Chalupka et al., 2013) for comparison. We select the in-
ducing size m for FITC and SVI, the batch size mb for SVI,
and the subset size msod for SOD, such that the computing
time is similar to or a bit larger than that of GRBCM. Partic-
ularly, we choose m = 200, mb = 0.1n and msod = 2500
for kin40k, and m = 300, mb = 0.1n and msod = 3000 for
sarcos. Differently, SVI employs the stochastic gradients
algorithm with tsg = 1200 iterations. Finally, we adopt the
disjoint partition used before to divide the kin40k dataset
into 16 subsets, and the sarcos dataset into 72 subsets for
the aggregations. Each experiment is repeated ten times.

kin40k

kin40k

(a)

0.1

0.08

E
S
M
S

0.06

0.04

0.02

60

(c)

0.04

0.03

E
S
M
S

0.02

0.01

(b)

-0.5

L
L
S
M

-1

-1.5

-2

60

(d)

3

2

1

0

-1

-2

L
L
S
M

×

Figure3. Illustrations of the aggregation models on the toy exam-
ple. The green “+” symbols represent the 104 data points. The
shaded area indicates 99% conﬁdence intervals of the full GP pre-
dictions using n = 104.
Fig. 3 illustrates the six aggregation models using n = 104
105, respectively, in comparison to the full
and n = 5
GP (ground truth) using n = 104.7 It is observed that in
terms of prediction mean, as discussed in remark 1, PoE
and GPoE provide poorer results in the entire domain with
increasing n. On the contrary, BCM and RBCM provide
good predictions in the range [0, 1]. As discussed in re-
mark 2, BCM however yields unreliable predictions when
leaving the training data. RBCM alleviates the issue by
using a varying βi. In terms of prediction variance, with
increasing n, PoE and (R)BCM tend to shrink to zero (over-
conﬁdent), while GPoE tends to approach σ2
∗∗ (too con-
servative). Particularly, PoE always has the largest MSLL
value in Fig. 2(b), since as discussed in remark 2, its pre-
diction variance approaches zero faster.

6Further discussions of GRBCM is shown in Appendix D.
7The full GP is intractable using our computer for n = 5 ×

105.

70

80
Computing time [s]

90

100

70

80
Computing time [s]

90

100

sarcos

sarcos

SVI
FITC
SOD
PoE
GPoE
BCM
RBCM
GRBCM

0
350

400

450
550
500
Computing time [s]

600

650

450
550
500
Computing time [s]

600

650

-3
350

400

Figure4. Comparison of the approximation models on the kin40k
and sarcos datasets.

Fig. 4 depicts the comparative results of different approx-
imation models over 10 runs on the kin40k and sarcos
datasets. The horizontal axis represents the sum of train-
ing and predicting time. It is ﬁrst observed that GRBCM
provides the best performance on the two datasets in terms
of both SMSE and MSLL at the cost of requiring a bit
more computing time than (G)PoE and (R)BCM. As for
(R)BCM, the small SMSE values reveal that they provide
better prediction mean than FITC and SOD; but the large
MSLL values again conﬁrm that they provide overconﬁ-

8The comparison of NPAE and GRBCM are separately pro-

vided in Appendix E.

9https://github.com/SheffieldML/GPy

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

dent prediction variance. As for (G)PoE, they suffer from
poor prediction mean, as indicated by the large SMSE; but
GPoE performs well in terms of MSLL. Finally, the simple
SOD outperforms FITC and SVI on the kin40k dataset, and
performs similarly on the sarcos dataset, which are consis-
tent with the ﬁndings in (Chalupka et al., 2013).

kin40k

kin40k

(a)

E
S
M
S

10-1

10-2

0

(c)

E
S
M
S

10-1

10-2

(a)

E
S
M
S

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

(c)

0.035

E
S
M
S

0.03

0.025

0.02

0.015

0.01

0.005

0

20
60
40
Number of experts
sarcos

80

-5

0

20
60
40
Number of experts
sarcos

80

(b)

15

L
L
S
M

10

5

0

(d)

40

L
L
S
M

30

20

10

0

-10

0

PoE
GPoE
BCM
RBCM
GRBCM

0

100

200

300

100

200

300

Number of experts

Number of experts

Figure5. Comparison of the aggregation models using different
numbers of experts on the kin40k and sarcos datasets.

Next, we explore the impact of the number M of experts
on the performance of aggregations. To this end, we run
them on the kin40k dataset with M respectively being 8,
16 and 64, and we run on the sarcos dataset with M respec-
tively being 36, 72 and 288. The results in Fig. 5 turn out
that all the aggregations perform worse with increasing M ,
since the experts become weaker; but GRBCM still yields
the best performance with different M . Besides, with in-
creasing M , the poor prediction mean and the vanishing
prediction variance of PoE result in the sharp increase of
MSLL values.

kin40k

kin40k

(b)

2.5

L
L
S
M

(d)

L
L
S
M

1.5

0.5

2

1

0

-0.5

-1

-1.5

-2

60

50

40

30

20

10

0

PoE

GPoE

BCM RBCM GRBCM

PoE

GPoE

BCM RBCM GRBCM

sarcos

sarcos

disjoint
random

PoE

GPoE

BCM RBCM GRBCM

PoE

GPoE

BCM RBCM GRBCM

Figure6. Comparison of the aggregation models using disjoint
and random partitions on the kin40k dataset (M = 16) and the
sarcos dataset (M = 72).

Table1. Comparative results of the aggregation models and SVI
on the song and electric datasets.

song (450K)
SMSE MSLL

electric (1.8M)
SMSE MSLL

0.8527
POE
0.8527
GPOE
2.6919
BCM
1.3383
RBCM
SVI
0.7909
GRBCM 0.7321

328.82
0.1159
156.62
24.930
-0.1885
-0.1571

0.1632
0.1632
0.0073
0.0027
0.0042
0.0024

1040.3
24.940
51.081
85.657
-1.1410
-1.3161

or random) on the performance of aggregations. The av-
erage results in Fig. 6 turn out that the disjoint partition
is more beneﬁcial for the aggregations. The results are
expectable since the disjoint subsets provide separate and
reﬁned local information, whereas the random subsets pro-
vide overlapping and coarse global information. But we ob-
serve that GPoE performs well on the sarcos dataset using
random partition, which conﬁrms the conclusions in Propo-
sition 2. Besides, as revealed in remark 3, even using ran-
dom partition, GRBCM outperforms GPoE.

4.3. Large-scale datasets

This section explores the performance of aggregations and
SVI on two large-scale datasets. We ﬁrst assess them on
the 90D song dataset, which is a subset of the million song
dataset (Bertin-Mahieux et al., 2011). The song dataset
is partitioned into 450000 training points and 65345 test
points. We then assess the models on the 11D electric
dataset that is partitioned into 1.8 million training points
and 249280 test points. We follow the normalization and
data pre-processing in (Wilson et al., 2016) to generate the
two datasets.10 For the song dataset, we use the foregoing
disjoint partition to divide it into M = 720 subsets, and
use m = 800, mb = 5000 and tsg = 1300 for SVI; for the
electric dataset, we divide it into M = 2880 subsets, and
use m = 1000, mb = 5000 and tsg = 1500 for SVI. As a
result, each expert is assigned with m0 = 625 data points
for the aggregations.

Table 1 reveals that the (G)PoE’s SMSE value is smaller
than that of (R)BCM on the song dataset. The poor pre-
diction mean of BCM is caused by the fact that the song
dataset is highly clustered such that BCM suffers from
weak experts in regions with scarce points. On the contrary,
due to the almost uniform distribution of the electric data
points, the (R)BCM’s SMSE is much smaller than that of
(G)PoE. Besides, unlike the vanishing prediction variances
of PoE and (R)BCM when n
, GPoE provides conser-
vative prediction variance, resulting in small MSLL values
on the two datasets. The proposed GRBCM always outper-

→ ∞

10The datasets and the pre-processing scripts are available in

Finally, we investigate the impact of data partition (disjoint

https://people.orie.cornell.edu/andrew/.

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

forms the other aggregations in terms of both SMSE and
MSLL on the two datasets due to the consistency. Finally,
GRBCM performs similarly to SVI on the song dataset; but
GRBCM outperforms SVI on the electric dataset.

Chalupka, Krzysztof, Williams, Christopher KI, and Mur-
ray, Iain. A framework for evaluating approximation
methods for Gaussian process regression. Journal of Ma-
chine Learning Research, 14(Feb):333–350, 2013.

5. Conclusions

To scale the standard GP to large-scale regression, we
present the GRBCM aggregation model, which introduces
a global communication expert to yield consistent yet ef-
ﬁcient predictions when n
. Through theoretical
and empirical analyses, we demonstrated the superiority of
GRBCM over existing aggregations on datasets with up to
1.8M training points.

→ ∞

The superiority of local experts is the capability of captur-
ing local patterns. Hence, further works will consider the
experts with individual hyperparameters in order to capture
non-stationary and heteroscedastic features.

Acknowledgements

This work was conducted within the Rolls-Royce@NTU
from the National Re-
Corporate Lab with support
search Foundation (NRF) Singapore under
the Corp
Lab@University Scheme. It is also partially supported by
the Data Science and Artiﬁcial Intelligence Research Cen-
ter (DSAIR) and the School of Computer Science and En-
gineering at Nanyang Technological University.

References

Alvarez, Mauricio A, Rosasco, Lorenzo, Lawrence, Neil D,
et al. Kernels for vector-valued functions: A review.
Foundations and Trends R
in Machine Learning, 4(3):
(cid:13)
195–266, 2012.

Bauer, Matthias, van der Wilk, Mark, and Rasmussen,
Carl Edward. Understanding probabilistic sparse Gaus-
sian process approximations. In Advances in Neural In-
formation Processing Systems, pp. 1533–1541. Curran
Associates, Inc., 2016.

Choi, Taeryon and Schervish, Mark J. Posterior consis-
tency in nonparametric regression problems under Gaus-
sian process priors. Technical report, Carnegie Mellon
University, 2004.

Deisenroth, Marc Peter and Ng, Jun Wei. Distributed Gaus-
sian processes. In International Conference on Machine
Learning, pp. 1481–1490. PMLR, 2015.

Fu, Yifan, Zhu, Xingquan, and Li, Bin. A survey on in-
stance selection for active learning. Knowledge and In-
formation Systems, 35(2):249–283, 2013.

Gal, Yarin, van der Wilk, Mark, and Rasmussen, Carl Ed-
ward. Distributed variational inference in sparse Gaus-
sian process regression and latent variable models.
In
Advances in Neural Information Processing Systems, pp.
3257–3265. Curran Associates, Inc., 2014.

Genest, Christian and Zidek, James V. Combining proba-
bility distributions: A critique and an annotated bibliog-
raphy. Statistical Science, 1(1):114–135, 1986.

Hensman, James, Fusi, Nicol`o, and Lawrence, Neil D.
Gaussian processes for big data. In Proceedings of the
29th Conference on Uncertainty in Artiﬁcial Intelligence,
pp. 282–290. AUAI Press, 2013.

Hinton, Geoffrey E. Training products of experts by mini-
mizing contrastive divergence. Neural Computation, 14
(8):1771–1800, 2002.

Hoang, Trong Nghia, Hoang, Quang Minh, and Low, Bryan
Kian Hsiang. A distributed variational inference frame-
work for unifying parallel sparse Gaussian process re-
In International Conference on Ma-
gression models.
chine Learning, pp. 382–391. PMLR, 2016.

Bertin-Mahieux, Thierry, Ellis, Daniel PW, Whitman,
Brian, and Lamere, Paul. The million song dataset. In
ISMIR, pp. 1–6, 2011.

Ionescu, Radu Cristian. Revisiting large scale distributed
arXiv preprint arXiv:1507.01461,

machine learning.
2015.

Bui, Thang D and Turner, Richard E. Tree-structured Gaus-
sian process approximations. In Advances in Neural In-
formation Processing Systems, pp. 2213–2221. Curran
Associates, Inc., 2014.

Lawrence, Neil. Probabilistic non-linear principal compo-
nent analysis with Gaussian process latent variable mod-
Journal of Machine Learning Research, 6(Nov):
els.
1783–1816, 2005.

Cao, Yanshuai and Fleet, David J. Generalized product of
experts for automatic and principled fusion of Gaussian
arXiv preprint arXiv:1410.7827,
process predictions.
2014.

Liu, Haitao, Cai, Jianfei, and Ong, Yew-Soon. An adaptive
sampling approach for Kriging metamodeling by maxi-
mizing expected prediction error. Computers & Chemi-
cal Engineering, 106(Nov):171–182, 2017.

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

Liu, Haitao, Cai, Jianfei, and Ong, Yew-Soon. Remarks on
multi-output Gaussian process regression. Knowledge-
Based Systems, 144(March):102–121, 2018.

Moore, David and Russell, Stuart J. Gaussian process
random ﬁelds. In Advances in Neural Information Pro-
cessing Systems, pp. 3357–3365. Curran Associates, Inc.,
2015.

Park, Chiwoo, Huang, Jianhua Z, and Ding, Yu. Domain
decomposition approach for fast Gaussian process re-
gression of large spatial data sets. Journal of Machine
Learning Research, 12(May):1697–1728, 2011.

Peng, Hao, Zhe, Shandian, Zhang, Xiao, and Qi, Yuan.
Asynchronous distributed variational Gaussian process
for regression. In International Conference on Machine
Learning, pp. 2788–2797. PMLR, 2017.

Qui˜nonero-Candela, Joaquin and Rasmussen, Carl Edward.
A unifying view of sparse approximate Gaussian process
regression. Journal of Machine Learning Research, 6
(Dec):1939–1959, 2005.

Ranjan, Roopesh and Gneiting, Tilmann. Combining prob-
ability forecasts. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 72(1):71–91, 2010.

Rasmussen, Carl E and Ghahramani, Zoubin. Inﬁnite mix-
tures of Gaussian process experts. In Advances in Neu-
ral Information Processing Systems, pp. 881–888. Cur-
ran Associates, Inc., 2002.

Rasmussen, Carl Edward and Williams, Christopher K. I.
Gaussian processes for machine learning. MIT Press,
2006.

Rulli`ere, Didier, Durrande, Nicolas, Bachoc, Franc¸ois, and
Chevalier, Cl´ement. Nested Kriging predictions for
datasets with a large number of observations. Statistics
and Computing, pp. 1–19, 2017.

Seeger, Matthias, Williams, Christopher, and Lawrence,
Neil. Fast forward selection to speed up sparse Gaussian
process regression. In Artiﬁcial Intelligence and Statis-
tics, pp. EPFL–CONF–161318. PMLR, 2003.

Shahriari, Bobak, Swersky, Kevin, Wang, Ziyu, Adams,
Ryan P, and de Freitas, Nando. Taking the human out
of the loop: A review of Bayesian optimization. Pro-
ceedings of the IEEE, 104(1):148–175, 2016.

Snelson, Edward and Ghahramani, Zoubin. Sparse Gaus-
In Advances in
sian processes using pseudo-inputs.
Neural Information Processing Systems, pp. 1257–1264.
MIT Press, 2006.

Snelson, Edward and Ghahramani, Zoubin. Local and
global sparse Gaussian process approximations. In Ar-
tiﬁcial Intelligence and Statistics, pp. 524–531. PMLR,
2007.

Tavassolipour, Mostafa, Motahari, Seyed Abolfazl, and
Shalmani, Mohammad-Taghi Manzuri.
Learning of
Gaussian processes in distributed and communication
arXiv preprint arXiv:1705.02627,
limited systems.
2017.

Titsias, Michalis K. Variational learning of inducing vari-
ables in sparse Gaussian processes. In Artiﬁcial Intelli-
gence and Statistics, pp. 567–574. PMLR, 2009.

Tresp, Volker. A Bayesian committee machine. Neural

Computation, 12(11):2719–2741, 2000.

Vazquez, Emmanuel and Bect, Julien. Pointwise consis-
tency of the Kriging predictor with known mean and
In 9th International Workshop
covariance functions.
in Model-Oriented Design and Analysis, pp. 221–228.
Springer, 2010.

Wilson, Andrew and Nickisch, Hannes. Kernel interpola-
tion for scalable structured Gaussian processes (KISS-
GP). In International Conference on Machine Learning,
pp. 1775–1784. PMLR, 2015.

Wilson, Andrew Gordon, Hu, Zhiting, Salakhutdinov, Rus-
lan, and Xing, Eric P. Deep kernel learning. In Artiﬁcial
Intelligence and Statistics, pp. 370–378. PMLR, 2016.

Yuan, Chao and Neubauer, Claus. Variational mixture of
Gaussian process experts. In Advances in Neural Infor-
mation Processing Systems, pp. 1897–1904. Curran As-
sociates, Inc., 2009.

A. Proof of Proposition 1

With disjoint partition, we consider two extreme local GP
Mn),
experts. For the ﬁrst extreme expert
the test point x∗ falls into the local region deﬁned by Xan ,
i.e., x∗ is adherent to Xan when n
. Hence, we have
(Vazquez & Bect, 2010)

an (1

→ ∞

M

an

≤

≤

lim
n→∞

σ2
an (x∗) = lim
n→∞

ǫ,n = σ2
σ2
η.

M

bn , it lies farthest away
For the other extreme expert
from x∗ such that the related prediction variance σ2
bn (x∗)
is closest to σ2
= an)
∗∗. It is known that for any
where x∗ is away from the training data Xi, given the
∀x∈Xi, we have
relative distance ri = min
k
limri→∞ σ2
∗∗. Since, however, we here focus
[0, 1]d
on the GP predictions in the bounded region Ω

i (x∗) = σ2

i (i

M

x∗

−

x

k

∈

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

and employ the covariance function k(.) > 0, then the pos-
is small but satisﬁes
itive sequence cn =
{
limn→∞ cn > 0 and

σ−2
∗∗ }

σ−2
bn

(x∗)

−

points. Hence, we have

σ−2
i

(x∗)

σ−2
∗∗ ≥

−

cn, 1

i

= an

Mn.

≤

≤

The equality holds only when i = bn.

Thereafter, with the sequence ǫn = min
{
0 where α > 0 we have

cn, 1
M α

n→∞

n } →

σ−2
i

(x∗)

σ−2
∗∗ ≥

−

cn

≥

ǫn, 1

i

= an

Mn.

≤

≤

It is found that cn = ǫn is possible to hold only when Mn
is small. With the increase of n, ǫn quickly becomes much
smaller than cn since limn→∞ 1/M α

n = 0.

The typical aggregated prediction variance writes

σ−2
A,n(x∗) =

βi(σ−2
i

(x∗)

∗∗ ) + σ−2
σ−2
∗∗ ,

(18)

−

Mn

i=1
X

where for (G)PoE we remove the prior precision σ−2
∗∗ . We
prove below the inconsistency of (G)PoE and (R)BCM us-
ing disjoint partition.

Mn
i=1 σ−2
(x∗) > Mnσ−2
For PoE, (18) is
leading to the inconsistent variance limn→∞ σ2
(R)BCM, the ﬁrst term of σ−2
that n is large enough,

,
n→∞
∞
A,n = 0. For
A,n(x∗) in (18) satisﬁes, given

∗∗ →

P

i

Mn

i=1
X

βi(σ−2
i

(x∗)

σ−2
∗∗ ) > ǫn

βi =

−

Mn

i=1
X

1
M α
n

Mn

i=1
X

βi.

Taking βi = 1 for BCM and α = 0.5, we have
, leading to the incon-
n→∞
A,n = 0. For RBCM, since

Mn
1
i=1 βi = √Mn
M α
→
n
sistent variance limn→∞ σ2

∞

P

βi = 0.5(log σ2

log σ2

i (x∗))

0.5 log(1 + cnσ2

∗∗)

∗∗ −

≥

where the equality holds only when i = bn, we have
, lead-

Mn
1
i=1 βi > 0.5 log(1 + cnσ2
n→∞
M α
→
n
ing to the inconsistent variance limn→∞ σ2
A,n = 0.

∗∗)√Mn

∞

P

Finally, for GPoE, we know that when n
converges to σ−2
isfy cn + σ−2
1

an (x∗)
η ; but the other prediction precisions sat-
for
Mn, since x∗ is away from their training

(x∗) < σ−2

n→∞ σ−2

ǫ,n →

→ ∞

= an

σ−2
i

i

η

, σ−2

∗∗ ≤
≤

≤

lim
n→∞

= lim
n→∞

σ−2
η −
1
(cid:0)
Mn

σ−2
GPoE(x∗)

(cid:1)
σ−2
an (x∗)

σ−2
η −
Mn

i6=an (cid:0)
X
σ−2
η −
Mn

(cid:0)
1
Mn

(cid:0)
1
Mn

+ lim
n→∞

> lim
n→∞

1
Mn

+ lim
n→∞

σ−2
η −

i6=an (cid:0)
X
GPoE(x∗)

(cid:1)
σ−2
i

(x∗)

σ−2
η −

(cid:1)

σ−2
an (x∗)

(cid:1)
σ−2
ǫ,n(x∗)

= 0,

that σ2
GPoE(x∗) > σ2

which means
limn→∞ σ2
ﬁnd that limn→∞ σ−2
limn→∞ σ2

GPoE(x∗) < σ2

GPoE(x∗) > cn + σ−2
bn (x∗) < σ2

∗∗.

(cid:1)
inconsistent

is

since
η. Meanwhile, we easily
∗∗ , leading to

B. Proof of Proposition 2

With smoothness assumption and particularly distributed
noise (normal or Laplacian distribution), it has been proved
that the GP predictions would converge to the true predic-
(Choi & Schervish, 2004). Hence,
tions when n
given that the points in Xi are randomly selected without
replacement from X and ni = n/Mn
, we have

→ ∞

n→∞

lim
n→∞

µi(x∗) = µη(x∗), lim
n→∞

i (x∗) = σ2
σ2
η,

→

∞
1

i

Mn.

≤

≤

For the aggregated prediction variance, we have

Mn

i=1
X

lim
n→∞

σ−2
A,n(x∗) = lim

n→∞ "

βi(σ−2
i

(x∗)

∗∗ ) + σ−2
σ−2
∗∗

,

−

#

i

A,n(x∗) = limn→∞ Mnσ−2

where for (G)PoE we remove σ−2
1 and limn→∞ σ−2
(x∗) = σ−2
tent variance limn→∞ σ−2

∗∗ . For PoE, given βi =
η , we have the inconsis-
η =
. For GPoE, given βi = 1/Mn we have the consis-
∞
tent variance limn→∞ σ−2
η = σ−2
σ−2
η .
For BCM, given βi = 1 we have the inconsistent vari-
ance limn→∞ σ−2
σ−2
∗∗ ) +
σ−2
. Finally, for RBCM, given limn→∞ βi =
∗∗ ] =
β = 0.5 log(σ2
η), we have the inconsistent vari-
ance limn→∞ σ−2
σ−2
∗∗ ) +
σ−2
∗∗ ] =

∗∗/σ2
A,n(x∗) = limn→∞[Mnβ(σ−2

A,n(x∗) = limn→∞[Mn(σ−2

A,n(x∗) = Mn

η −

η −

1
Mn

∞

.
∞

Then, for the aggregated prediction mean we have

Mn

lim
n→∞

µA,n(x∗) = lim
n→∞

σ2
A,n(x∗)

βiσ−2
i

(x∗)µi(x∗).

i=1
X
For PoE, given βi = 1 and limn→∞ σ−2
1/Mn, we

i
consistent

have

the

(x∗)/σ−2
A,n(x∗) =
prediction mean

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

(x∗)/σ−2

limn→∞ µA,n(x∗) = µη(x∗).
For GPoE, given
βi = 1/Mn and limn→∞ σ−2
A,n(x∗) = 1, we
i
have the consistent prediction mean limn→∞ µA,n(x∗) =
µη(x∗).
For (R)BCM, given βi = β = 1 or
limn→∞ βi = β = 0.5 log(σ2
η), we have the
inconsistent prediction mean limn→∞ µA,n(x∗) =
limn→∞ βσ−2
∗∗ /Mn) =
aµη(x∗) where a = σ−2
1 and the
equality holds when σ2
η = 0.

∗∗ ) + σ−2
σ−2
σ−2
∗∗ )

η µη(x∗)/(β(σ−2

η −
η /(σ−2

∗∗/σ2

η −

≥

C. Proof of Proposition 3

→

n→∞

Given that the points in the communication subset
are randomly selected without replacement from
nc = n/Mn
µη(x∗) and limn→∞ σ2
for the expert
i,
D+i =
limn→∞ µ+i(x∗) = µη(x∗) and limn→∞ σ2
for 2

c
D
and
, we have limn→∞ µc(x∗) =
∞
c (x∗) = σ2
c. Likewise,
trained on the augmented dataset
with size n+i = 2n/Mn, we have
+i(x∗) = σ2
η

M+i
c
}
D

η for

M .

{D

M

D

i

≤

≤

We ﬁrst derive the upper bound of σ2
c (x∗). For the station-
ary covariance function k(.) > 0, when nc is large enough
we have (Vazquez & Bect, 2010)

σ2
c (x∗)

k(x∗, x∗)

≤

k2(x∗, x′)
k(x′, x′)

−

+ σ2

ǫ,n,

k

k

∈

−

x′

where x′
Xc is the nearest data point to x∗. It is known
x∗
is proportional
that the relative distance rc =
to the inverse of the training size nc, i.e., rc
1/nc =
n→∞ 0. Conventional stationary covariance func-
Mn/n
tions only relay on the relative distance (once the covari-
ance parameters have been determined) and decrease with
rc. Consequently, the prediction variance σ2
c (x∗) increases
with rc. Taking the SE covariance function in (1) for ex-
ample,11 when rc
,

0 we have, given l0 = min1≤i≤d

→

∝

li

{

}

→

σ2
c (x∗)

We clearly see from this inequality that when rc
η since limn→∞ σ2
c (x∗) goes to σ2
σ2
Then, we rewrite the precision of GRBCM in (14b) as,
given β2 = 1,

ǫ,n = σ2
η.

→

0,

GRBCM(x∗) = σ−2
σ−2

+2(x∗)+

βi

σ−2
+i (x∗)

σ−2
c (x∗)

.

Mn

i=3
X

(cid:0)

−

(cid:1)
(20)

11We take the SE kernel for example since conventional kernels,
e.g., the rational quadratic kernel and the Mat´ern class of kernels,
can reduce to the SE kernel under some conditions.

Compared to
c,
M+i is trained on a more dense dataset
M
c (x∗) for a large enough n.12
σ2
D+i, leading to σ2
+i(x∗)
Given (19) and σ2
+i(x∗) > σ2
ǫ,n, the weight βi satisﬁes, for
3

Mn,

≤

i

≤

0

≤

≤

βi =

log

1
2

1
2

<

1
2

σ2
c (x∗)
σ2
+i(x∗)
(cid:19)
c + σ2
ar2
ǫ,n
σ2
ǫ,n ! ≤

(cid:18)

 

log

(cid:18)
a
2σ2

ǫ,n

σ2
c (x∗)
σ2
ǫ,n (cid:19)

r2
c .

<

log

(21)

Besides, the precision discrepancy satisﬁes, for 3
Mn,

i

≤

≤

0

≤

σ−2
+i (x∗)

σ−2
c (x∗) = σ−2

c (x∗)

−

σ2
c (x∗)
σ2
+i(x∗) −

1

(cid:19)

<

1
σ2

a
σ2

ǫ,n

ǫ,n

(cid:18)
r2
c .

(22)
Hence, the second term in the right-hand side of (20) satis-
ﬁes

Mn

βi

σ−2
+i (x∗)

σ−2
c (x∗)

<

−

Mn

a2
2σ6

ǫ,n

r4
c ∝

M 5
n
n4 .

i=3
X
n > 0, we have limn→∞ n4/M 5

(cid:1)

n =

i=3
X

(cid:0)
Since limn→∞ n/M 2
, and furthermore,

∞

Mn

lim
n→∞

βi

σ−2
+i (x∗)

σ−2
c (x∗)

= 0.

(23)

−

(cid:0)

i=3
X
Substituting (23) and limn→∞ σ−2
we have a consistent prediction precision as
σ−2
GRBCM(x∗) = σ−2
η .

(cid:1)

+2 (x∗) = σ−2
η

lim
n→∞

into (20),

Similarly, we rewrite the GRBCM’s prediction mean in
(14a) as

µGRBCM(x∗) = σ2

GRBCM(x∗)

µ∆ + σ−2

+2(x∗)µ+2(x∗)

,
(24)
(cid:1)

(cid:0)

−

µ∆ =

βi

σ−2
+i (x∗)µ+i(x∗)

σ−2
c (x∗)µc(x∗)

.

(cid:0)

i=3
X
Let δmax = max3≤i≤Mn
0, we have

µ∆| ≤

|

Mn

βiσ−2
c

i=3
X
Eq.(21)
<

(cid:12)
(cid:12)
(cid:12)
ar2
(cid:12)
c
2σ4

ǫ,n

Mn

i=3
X

σ2
c (x∗)
+i(x∗) µ+i(x∗)
σ2

(cid:12)
(cid:12)
(cid:12)
σ2
c (x∗)
σ2
+i(x∗)

µ+i(x∗)

−

δmax →

n→∞ 0.

(cid:1)

n→∞

→

(25)

−

µc(x∗)
(cid:12)
(cid:12)
(cid:12)

µc(x∗)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

12The equality is possible to hold when we employ disjoint par-

tition for {Di}

Mn
i=2 and x∗ is away from Xi.

≤

<

σ2
f exp(

σ2
f −
σ2
f
c + σ2
r2
l2
0

−
ǫ,n = ar2

c /l2
r2

0) + σ2
ǫ,n

c + σ2

ǫ,n.

(19)

where

Mn

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

Substituting (25) into (24), we have the consistent predic-
tion mean as

lim
n→∞

µGRBCM(x∗) = µη(x∗).

D. Discussions of GRBCM on the toy example

It is observed that the proposed GRBCM showcases superi-
ority over existing aggregations on the toy example, which
is brought by the particularly designed aggregation struc-
ture: the global communication expert
c to capture the
long-term features of the target function, and the remaining
M
i=2 to reﬁne local predictions.
experts

M

{M+i

}

(a)

0.16

E
S
M
S

0.14

0.12

0.1

0.08

0.06

104

(b)

-1.35

-1.4

L
L
S
M

-1.45

-1.5

105
Training size

106

-1.55

104

105
Training size

106

Figure7. Comparative results of GRBCM and Mc on the toy ex-
ample.

M

To verify the capability of GRBCM, we compare it with
the pure global expert
c which relies on a random sub-
set Xc. Fig. 7 shows the comparative results of GRBCM
c on the toy example. It is found that with increas-
and
ing n, (i) GRBCM always outperforms
c because of the
beneﬁts brought by local experts; and (ii) the predictions of
c generally become poorer since it becomes intractable

M
to choose a good subset from the increasing dataset.

M

M

E. Experimental results of NPAE

Table 2 compares the results of GRBCM and NPAE over
10 runs on the kin40k dataset (M = 16) and the sarcos
dataset (M = 72) using disjoint partition. It is observed
that GRBCM performs slightly better than NPAE on the
kin40k dataset, and produces competitive results on the sar-
cos dataset. But in terms of the computing efﬁciency, since
NPAE needs to build and invert an M
M covariance ma-
trix at each test point, it requires much more running time,
especially for the sarcos dataset with M = 72.

×

Table2. Comparative results (mean and standard deviation) of
GRBCM and NPAE over 10 runs on the kin40k dataset (M = 16)
and the sarcos dataset (M = 72) using disjoint partition. The
computing time t for each model involves the training and pre-
dicting time.

kin40k

SMSE
MSLL
t [S]
sarcos

SMSE
MSLL
t [S]

GRBCM

NPAE

0.0223 ± 0.0005
-1.9927 ± 0.0177
78.1 ± 4.4
GRBCM

0.0246 ± 0.0007
-1.9565 ± 0.0170
2852.4 ± 16.7
NPAE

0.0074 ± 0.0002
-2.3681 ± 0.0242
445.6 ± 49.4

0.0054 ± 0.0001
-2.5900 ± 0.0068
26444.0 ± 1213.0

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian
Process Regression

8
1
0
2
 
n
u
J
 

3

 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
0
2
7
0
0
.
6
0
8
1
:
v
i
X
r
a

Haitao Liu 1 Jianfei Cai 2 Yi Wang 3 Yew-Soon Ong 2 4

Abstract

predictive distributions at test points.

In order to scale standard Gaussian process (GP)
regression to large-scale datasets, aggregation
models employ factorized training process and
then combine predictions from distributed ex-
perts. The state-of-the-art aggregation models,
however, either provide inconsistent predictions
or require time-consuming aggregation process.
We ﬁrst prove the inconsistency of typical aggre-
gations using disjoint or random data partition,
and then present a consistent yet efﬁcient aggre-
gation model for large-scale GP. The proposed
model inherits the advantages of aggregations,
e.g., closed-form inference and aggregation, par-
allelization and distributed computing. Further-
more, theoretical and empirical analyses reveal
that the new aggregation model performs better
due to the consistent predictions that converge
to the true underlying function when the training
size approaches inﬁnity.

1. Introduction

Gaussian process (GP) (Rasmussen & Williams, 2006) is
a well-known statistical learning model extensively used
in various scenarios, e.g., regression, classiﬁcation, opti-
mization (Shahriari et al., 2016), visualization (Lawrence,
2005), active learning (Fu et al., 2013; Liu et al., 2017) and
multi-task learning (Alvarez et al., 2012; Liu et al., 2018).
n
Given the training set X =
xi
i=1 and the observa-
{
n
tion set y =
i=1, as an approximation of the
R
}
{
underlying function η : Rd
R, GP provides informative

y(xi)

Rd

∈

∈

}

→

1Rolls-Royce@NTU Corporate Lab, Nanyang Technologi-
cal University, Singapore 637460 2School of Computer Science
and Engineering, Nanyang Technological University, Singapore
639798 3Applied Technology Group, Rolls-Royce Singapore, 6
Seletar Aerospace Rise, Singapore 797575 4Data Science and
Artiﬁcial Intelligence Research Center, Nanyang Technological
University, Singapore 639798. Correspondence to: Haitao Liu
<htliu@ntu.edu.sg>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

However, the most prominent weakness of the full GP is
that it scales poorly with the training size. Given n data
points, the time complexity of a standard GP paradigm
(n3) in the training process due to the inver-
scales as
O
(n2) in
sion of an n
the prediction process due to the matrix-vector operation.
This weakness conﬁnes the full GP to training data of size

n covariance matrix; it scales as

O

×

(104).

≪

The

sparse

approximations
2005)

O
To cope with large-scale regression,
various com-
putationally efﬁcient approximations have been pre-
reviewed in
sented.
(Qui˜nonero-Candela & Rasmussen,
employ m
(m
n) inducing points to summarize the whole training
data (Seeger et al., 2003; Snelson & Ghahramani, 2006;
2007; Titsias, 2009; Bauer et al., 2016), thus reducing the
(nm2) and the predict-
training complexity of full GP to
ing complexity to
(nm). The complexity can be further
reduced through distributed inference, stochastic varia-
tional inference or Kronecker structure (Hensman et al.,
2013; Gal et al.,
2015;
Hoang et al., 2016; Peng et al., 2017). A main draw-
the
back of sparse approximations, however,
representational capability is limited by the number of
inducing points (Moore & Russell, 2015). For example,
for a quick-varying function, the sparse approximations
need many inducing points to capture the local structures.
That is, this kind of scheme has not reduced the scaling of
the complexity (Bui & Turner, 2014).

2014; Wilson & Nickisch,

is that

O

O

The method exploited in this article belongs to the aggre-
gation models (Hinton, 2002; Tresp, 2000; Cao & Fleet,
2014; Deisenroth & Ng, 2015; Rulli`ere et al., 2017), also
known as consensus statistical methods (Genest & Zidek,
1986; Ranjan & Gneiting, 2010). This kind of scheme pro-
duces the ﬁnal predictions by the aggregation of M sub-
models (GP experts) respectively trained on the subsets
, thus distributing
=
{D
the computations to “local” experts. Particularly, due to
the product of experts, the aggregation scheme derives a
factorized marginal likelihood for efﬁcient training; and
then it combines the experts’ posterior distributions accord-
In comparison to
ing to a certain aggregation criterion.

M
i=1 of

Xi, yi

X, y

i =

}}

D

{

{

}

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

sparse approximations, the aggregation models (i) operate
directly on the full training data, (ii) require no additional
inducing or variational parameters and (iii) distribute the
computations on individual experts for straightforward par-
allelization (Tavassolipour et al., 2017), thus scaling them
In comparison to typ-
to arbitrarily large training data.
ical local GPs (Snelson & Ghahramani, 2007; Park et al.,
2011), the aggregations smooth out the ugly discontinu-
ity by the product of posterior distributions from GP ex-
perts. Note that the aggregation methods are different from
the mixture-of-experts (Rasmussen & Ghahramani, 2002;
Yuan & Neubauer, 2009), which suffers from intractable in-
ference and is mainly developed for non-stationary regres-
sion.

However, it has been pointed out (Rulli`ere et al., 2017) that
there exists a particular type of training data such that typ-
ical aggregations, e.g., product-of-experts (PoE) (Hinton,
2002; Cao & Fleet, 2014) and Bayesian committee ma-
chine (BCM) (Tresp, 2000; Deisenroth & Ng, 2015), can-
not offer consistent predictions, where “consistent” means
the aggregated predictive distribution can converge to the
true underlying predictive distribution when the training
size n approaches inﬁnity.

Particularly,

The major contributions of this paper are three-fold. We
ﬁrst prove the inconsistency of typical aggregation mod-
els, e.g., the overconﬁdent or conservative prediction vari-
ances illustrated in Fig. 3, using conventional disjoint or
random data partition. Thereafter, we present a consis-
tent yet efﬁcient aggregation model for large-scale GP
the proposed generalized ro-
regression.
bust Bayesian committee machine (GRBCM) selects a
global subset to communicate with the remaining sub-
sets, leading to the consistent aggregated predictive dis-
theo-
tribution derived under the Bayes rule.
retical and empirical analyses reveal that GRBCM out-
performs existing aggregations due to the consistent yet
efﬁcient predictions. We release the demo codes in
https://github.com/LiuHaiTao01/GRBCM.

Finally,

2. Aggregation models revisited

2.1. Factorized training

A GP usually places a probability distribution over the la-
(0, k(x, x′)), which is
tent function space as f (x)
deﬁned by the zero mean and the covariance k(x, x′). The
well-known squared exponential (SE) covariance function
is

∼ GP

k(x, x′) = σ2

f exp

1
2

 −

d

(xi

x′
i)2

−
l2
i

,

!

i=1
X
where σ2
f is an output scale amplitude, and li is an input
length-scale along the ith dimension. Given the noisy ob-
servation y(x) = f (x) + ǫ where the i.i.d. noise follows

|

i=1
Y
(0, Ki + σ2

(0, σ2

∼ N

ǫ ) and the training data

ǫ
(0, k(X, X) + σ2
likelihood p(y
represents the hyperparameters to be inferred.

, we have the marginal
ǫ I) where θ

X, θ) =

N

D

|

i

In order to train the GP on large-scale datasets, the aggrega-
tion models introduce a factorized training process. It ﬁrst
partitions the training set
,
}
i. In
1
data partition, we can assign the data points randomly to
the experts (random partition), or assign disjoint subsets
obtained by clustering techniques to the experts (disjoint
partition).
Ignoring the correlation between the experts

M , and then trains GP on

D
{
i as an expert

into M subsets

Xi, yi

i =

M

≤

≤

D

D

M
i=1 leads to the factorized approximation as

i

{M

}

M

p(y

X, θ)

|

≈

pi(yi

Xi, θi),

(2)

M

|
∈

∼ N

Xi, θi)

i. Note that for simplicity all

ǫ,iIi) with Ki =
where pi(yi
Rni×ni and ni being the training size
k(Xi, Xi)
of
the M GP ex-
perts in (2) share the same hyperparameters as θi = θ
(Deisenroth & Ng, 2015). The factorization (2) degener-
ates the full covariance matrix K = k(X, X) into a diag-
onal block matrix diag[K1,
≈
diag[K −1
M ]. Hence, compared to the full GP,
1 ,
the complexity of the factorized training process is reduced
to

, KM ], leading to K −1

0) given ni = m0 = n/M , 1

, K −1

(nm2

M .

· · ·

· · ·

i

≤

≤

O

Conditioned on the related subset
i, x∗)
bution pi(y∗

(µi(x∗), σ2

D

i, the predictive distri-
i (x∗)) of

i has1

|D
∼ N
µi(x∗) = kT
i∗[Ki + σ2
σ2
i (x∗) = k(x∗, x∗)

ǫ I]−1yi,
kT
i∗[Ki + σ2

−

M

(3a)

ǫ I]−1ki∗ + σ2

ǫ , (3b)

where ki∗ = k(Xi, x∗). Thereafter, the experts’ predic-
M
i=1 are combined by the following aggrega-
tions
tion methods to perform the ﬁnal predicting.

µi, σ2
i }

{

2.2. Prediction aggregation

The state-of-the-art aggregation methods include PoE
(Hinton, 2002; Cao & Fleet, 2014), BCM (Tresp, 2000;
Deisenroth & Ng, 2015), and nested pointwise aggregation
of experts (NPAE) (Rulli`ere et al., 2017).

For the PoE and BCM family, the aggregated prediction
mean and precision are generally formulated as

µA(x∗) = σ2

A(x∗)

βiσ−2
i

(x∗)µi(x∗),

(4a)

M

i=1
X

M

i=1
X

M

−

i=1
X

1Instead of using pi(f∗|Di, x∗) in (Deisenroth & Ng, 2015),
we here consider the aggregations in a general scenario where
each expert has all its belongings at hand.

(1)

σ−2
A (x∗) =

βiσ−2
i

(x∗) + (1

βi)σ−2
∗∗ ,

(4b)

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

where the prior variance σ2
a correction term to σ−2
family; and βi is the weight of the expert

∗∗ = k(x∗, x∗) + σ2

ǫ , which is
A , is only available for the BCM

i at x∗.

M

The predictions of the PoE family, which omit the prior
precision σ−2
∗∗ in (4b), are derived from the product of M
experts as

M

pA(y∗

, x∗) =

|D

pβi
i (y∗

i, x∗).

|D

(5)

i=1
Y
The original PoE (Hinton, 2002) employs the constant
weight βi = 1, resulting in the aggregated prediction vari-
ances that vanish with increasing M . On the contrary, the
generalized PoE (GPoE) (Cao & Fleet, 2014) considers a
varying βi = 0.5(log σ2
i (x∗)), which represents
the difference in the differential entropy between the prior
i, x∗), to weigh the con-
p(y∗
|
i at x∗. This varying βi brings the ﬂexi-
tribution of
bility of increasing or reducing the importance of experts
based on the predictive uncertainty. However, the vary-
ing βi may produce undesirable errors for GPoE. For in-
stance, when x∗ is far away from the training data such
that σ2

x∗) and the posterior p(y∗

σ2
∗∗, we have βi

0 and σ2

log σ2

i (x∗)

∗∗ −

M

|D

.
GPoE → ∞

→

→

The BCM family, which is opposite to the PoE family, ex-
x∗) when combin-
plicitly incorporates the GP prior p(y∗
j, BCM intro-
ing predictions. For two experts
M
duces a conditional independence assumption
y∗,
leading to the aggregated predictive distribution as

|
i and

j
⊥ D

i
D

M

|

pA(y∗

, x∗) =

|D

M

i (y∗

i=1 pβi
|D
pPi βi−1(y∗
|

i, x∗)
x∗)

.

(6)

Q
The original BCM (Tresp, 2000) employs βi = 1 but
its predictions suffer from weak experts when leaving the
data. Hence, inspired by GPoE, the robust BCM (RBCM)
(Deisenroth & Ng, 2015) uses a varying βi
to produce
robust predictions by reducing the weights of weak ex-
perts. When x∗ is far away from the training data X,
the correction term brought by the GP prior in (4b) helps
the (R)BCM’s prediction variance recover σ2
∗∗. However,
given M = 1,
the predictions of RBCM as well as
GPoE cannot recover the full GP predictions because usu-
ally β1 = 0.5(log σ2
log σ2
log σ2
f ull(x∗))

1(x∗)) = 0.5(log σ2

∗∗ −

∗∗ −

= 1.

To achieve computation gains, the above aggregations intro-
duce additional independence assumption for the experts’
predictions, which however is often violated in practice
and yields poor results. Hence, in the aggregation process,
NPAE (Rulli`ere et al., 2017) regards the prediction mean
µi(x∗) in (3a) as a random variable by assuming that yi has
not yet been observed, thus allowing for considering the co-
variances between the experts’ predictions. Thereafter, for
, µM , y∗]T, the covariances are
the random vector [µ1,

· · ·

derived as

cov[µi, y∗] = kT

cov[µi, µj] =

i,ǫ ki∗,

i∗K −1
kT
i∗K −1
kT
i∗K −1

(

i,ǫ Kij K −1
i,ǫ Kij,ǫK −1

j,ǫ kj∗,
j,ǫ kj∗,

i

= j,

i = j,

(7a)

(7b)

ǫ I,
where Kij = k(Xi, Xj)
Kj,ǫ = Kj + σ2
ǫ I, and Kij,ǫ = Kij + σ2
ǫ I. With these
covariances, a nested GP training process is performed to
derive the aggregated prediction mean and variance as

Rni×nj , Ki,ǫ = Ki + σ2

∈

µNPAE(x∗) = kT
A∗K −1
σ2
NPAE(x∗) = k(x∗, x∗)

A µ,

kT
A∗K −1

A kA∗ + σ2
ǫ ,

−

(8a)

(8b)

RM×M has K ij

RM×1 has the ith element as cov[µi, y∗],
where kA∗
∈
A = cov[µi, µj], and µ =
KA
∈
, µM (x∗)]T. The NPAE is capable of provid-
[µ1(x∗),
ing consistent predictions at the cost of implementing a
much more time-consuming aggregation because of the in-
version of KA at each test point.

· · ·

2.3. Discussions of existing aggregations

→ ∞

Though showcasing promising results (Deisenroth & Ng,
2015), given that n
and the experts are noise-free
GPs, (G)PoE and (R)BCM have been proved to be in-
consistent, since there exists particular triangular array of
data points that are dense in the input domain Ω such that
the prediction variances do not go to zero (Rulli`ere et al.,
2017).

Particularly, we further show below the inconsistency of
(G)PoE and (R)BCM using two typical data partitions (ran-
dom and disjoint partition) in the scenario where the obser-
vations are blurred with noise. Note that since GPoE us-
ing a varying βi may produce undesirable errors, we adopt
βi = 1/M as suggested in (Deisenroth & Ng, 2015). Now
the GPoE’s prediction mean is the same as that of PoE; but
the prediction variance blows up as M times that of PoE.

→ ∞

, let X

∈
[0, 1]d such that for any x

Rn×d be dense
Deﬁnition 1. When n
Ω we have
in Ω
∈
= 0. Besides, the underlying
limn→∞ min1≤i≤n
k
function to be approximated has true continuous response
µη(x) and true noise variance σ2
η.

xi

−

∈

x

k

Firstly, for the disjoint partition that uses clustering tech-
into disjoint local subsets
niques to partition the data
D
M
,
i=1, The proposition below reveals that when n
i
{D
→ ∞
PoE and (R)BCM produce overconﬁdent prediction vari-
ance that shrinks to zero; on the contrary, GPoE provides
conservative prediction variance.

}

Mn
Proposition 1. Let
i=1 be a disjoint partition of the
training data
i be
GP with zero mean and stationary covariance function

}
. Let the expert

i trained on

i
{D

M

D

D

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

∈

≤

k(.) > 0. We further assume that (i) limn→∞ Mn =
∞
and (ii) limn→∞ n/M 2
n > 0, where the second condition
implies that the subset size m0 = n/Mn and the num-
ber of experts Mn are comparable such that too weak ex-
perts are not preferred. Besides, from the second condition
, which implies that the experts
we have m0 →
become more informative with increasing n. Then, PoE
and (R)BCM produce overconﬁdent prediction variance at
x∗

Ω as

n→∞

∞

lim
n→∞

σ2
A,n(x∗) = 0,

(9)

whereas GPoE yields conservative prediction variance

σ2
η < lim
n→∞

σ2
A,n(x∗) < σ2

bn (x∗) < σ2

∗∗,

(10)

where σ2
bn

bn (x∗) is offered by the farthest expert
M
Mn) whose prediction variance is closet to σ2

bn (1
∗∗.

≤

i

M

and

i=1 σ−2

−2
σ
i
−2
P σ
i

The detailed proof is given in Appendix A. Moreover, we
have the following ﬁndings.
Remark 1. For the averaging σ−2
M
i=1

GPoE = 1
M
µi using disjoint partition, more
µ(G)PoE =
and more experts become relatively far away from x∗ when
, i.e., the prediction variances at x∗ approach σ2
n
∗∗
and the prediction means approach the prior mean µ∗∗.
Hence, empirically, when n
approaches σ2
bn
Remark 2. The BCM’s prediction variance is always larger
than that of PoE since

, and the µ(G)PoE approaches µ∗∗.

, the conservative σ2

→ ∞

→ ∞

GPoE

P

P

a∗ =

σ−2
PoE(x∗)
σ−2
BCM(x∗)

=

M

i=1 σ−2
(x∗)

i

(x∗)
(M

−

−

M

i=1 σ−2
P
i

> 1

1)σ−2
∗∗

P

→ ∞

for M > 1. This means σ2
PoE deteriorates faster to zero
when n
. Besides, it is observed that µBCM is a∗
times that of PoE, which alleviates the deterioration of pre-
. However, when x∗ is leaving
diction mean when n
σ−2
X, a∗
∗∗ . That is why BCM
suffers from undesirable prediction mean when leaving X.

M since σ−2

→ ∞
i

(x∗)

→

→

Secondly, for the random partition that assigns the data
points randomly to the experts without replacement, The
proposition below implies that when n
, the predic-
tion variances of PoE and (R)BCM will shrink to zero;
the PoE’s prediction mean will recover µη(x), but the
(R)BCM’s prediction mean cannot; interestingly, the sim-
ple GPoE can converge to the underlying true predictive
distribution.

→ ∞

Mn
Proposition 2. Let
i=1 be a random partition of
i
{D
with (i) limn→∞ Mn =
and (ii)
the training data
Mn
limn→∞ n/M 2
i=1 be GPs with
zero mean and stationary covariance function k(.) > 0.

n > 0. Let the experts

{M

∞

D

}

}

i

Then, for the aggregated predictions at x∗

Ω we have

∈

lim
n→∞
lim
n→∞
lim
n→∞




where a = σ−2
when σ2

η = 0.

µPoE(x∗) = µη(x∗),

µGPoE(x∗) = µη(x∗),

lim
n→∞
lim
n→∞

σ2
PoE(x∗) = 0,
GPoE(x∗) = σ2
σ2
η,

µ(R)BCM(x∗) = aµη(x∗),

lim
n→∞

σ2
(R)BCM(x∗) = 0,

(11)

η /(σ−2

η −

σ−2
∗∗ )

≥

1 and the equality holds

The detailed proof is provided in Appendix B. Proposi-
tions 1 and 2 imply that no matter what kind of data par-
tition has been used, the prediction variances of PoE and
(R)BCM will shrink to zero when n
, which strictly
limits their usability since no beneﬁts can be gained from
such useless uncertainty information.

→ ∞

As for data partition, intuitively, the random partition pro-
vides overlapping and coarse global information about the
target function, which limits the ability to describe quick-
varying characteristics. On the contrary, the disjoint parti-
tion provides separate and reﬁned local information, which
enables the model to capture the variability of target func-
tion. The superiority of disjoint partition has been empiri-
cally conﬁrmed in (Rulli`ere et al., 2017). Therefore, unless
otherwise indicated, we employ disjoint partition for the
aggregation models throughout the article.

As for time complexity, the ﬁve aggregation models have
the same training process, and they only differ in how
to combine the experts’ predictions. For (G)PoE and
(R)BCM, their time complexity in prediction scales as
(n′nm0) where n′ is the number of test
O
points.2 For the complicated NPAE, it however needs to
M matrix KA at each test point, lead-
invert an M
ing to a greatly increased time complexity in prediction as

(nm2

0) +

O

×

(n′n2).3

O
The inconsistency of (G)PoE and (R)BCM and the ex-
tremely time-consuming process of NPAE impose the de-
mand of developing a consistent yet efﬁcient aggregation
model for large-scale GP regression.

3. Generalized robust Bayesian committee

machine

3.1. GRBCM

Our proposed GRBCM divides M experts into two groups.
The ﬁrst group has a global communication expert

c

M

2O(nm2

0) is induced by the update of M GP experts after op-

timizing hyperparameters.

3The predicting complexity of NPAE can be reduced by em-
ploying various hierarchical computing structure (Rulli`ere et al.,
2017), which however cannot provide identical predictions.

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

D
−

D1, and the second group con-
trained on the subset
c =
M
1 global or local experts4
tains the remaining M
i=2
M
trained on
i=2, respectively. The training process of
GRBCM is identical to that of typical aggregations in sec-
tion 2.1. The prediction process of GRBCM, however, is
different. Particularly, GRBCM assigns the global commu-
nication expert with the following properties:

i
{D

{M

}

}

i

with

•

•

(Random selection) The communication subset
c is
a random subset wherein the points are randomly se-
lected without replacement from
. It indicates that
the points in Xc spread over the entire domain, which
enables
c to capture the main features of the target
function. Note that there is no limit to the partition
type for the remaining M

1 subsets.

M

D

D

−

c, x∗)

M
∼ N

(Expert communication) The expert
c with pre-
(µc, σ2
c ) is
dictive distribution pc(y∗
|D
allowed to communicate with each of the remain-
It means we can utilize
ing experts
}
the augmented data
to improve
D+i =
i
}
D
over the base expert
c, leading to a new expert
M
M+i with the improved predictive distribution as
(µ+i, σ2
p+i(y∗

M
i=2.

{M

+i) for 2

M .

{D

c,

i

i

|D+i, x∗)

∼ N

≤

≤

•

(Conditional independence) Given the communica-
c and y∗, the independence assumption
tion subset
= j

D
c, y∗ holds for 2

M .

i

i
D

j
⊥ D

|D

≤

≤

Given the conditional independence assumption and the
M
i=2, we approximate the exact predictive dis-
weights
}
tribution p(y∗

, x∗) using the Bayes rule as

βi

{

|D

p(y∗

, x∗)

|D

M

i=2
Y
M

p(y∗

x∗)p(

y∗, x∗)

p(

i
D

j
|{D

}

i−1
j=1, y∗, x∗)

|

|

c
D

|

c
D

|

∝

≈

=

pβi(

i
D

|D

c, y∗, x∗)

p(y∗

x∗)p(

y∗, x∗)

p(y∗

x∗)
|
pPM

M
i=2 pβi(
c
D

i=2 βi−1(

Q

i=2
Y
D+i
|
y∗, x∗)
|

y∗, x∗)

.

(12)
c, y∗, x∗) is exact with no approximation

Note that p(
D2|D
in (12). Hence, we set β2 = 1.

With (12), GRBCM’s predictive distribution is

pA(y∗

, x∗) =

|D

M

+i(y∗

i=2 pβi
i=2 βi−1

pPM
Q
c

|D+i, x∗)
c, x∗)
(y∗
|D

.

(13)

4“Global” means the expert is trained on a random subset,

whereas “local” means it is trained on a disjoint subset.

µA(x∗) = σ2

A(x∗)

βiσ−2

+i (x∗)µ+i(x∗)

M

"

i=2
X

M

−  
i=2
X
M

i=2
X

βi

1

−

σ−2
c (x∗)µc(x∗)
#

,

!

(14a)

M

−  

i=2
X

βi

1

−

!

σ−2
c (x∗).

(14b)

σ−2
A (x∗) =

βiσ−2

+i (x∗)

c

rather than the prior σ−2

Different from (R)BCM, GRBCM employs the informa-
tive σ−2
∗∗ to correct the predic-
tion precision in (14b), leading to consistent predictions
when n
, which will be proved below. Also, the
prediction mean of GRBCM in (14a) now is corrected by
µc(x∗). Fig. 1 depicts the structure of the GRBCM aggre-
gation model.

→ ∞

Figure1. The GRBCM aggregation model.

In (14a) and (14b), the parameter βi (i > 2) akin to
that of RBCM is deﬁned as the difference in the dif-
ferential entropy between the base predictive distribution
c, x∗) and the enhanced predictive distribution
pc(y∗
|D
|D+i, x∗) as
p+i(y∗

i = 2,

βi =

1,
0.5(log σ2

(

c (x∗)

log σ2

+i(x∗)),

−

3

i

M.

≤

≤

D

≥

|D+i, x∗) over pc(y∗

(15)
2) into the
i (i
c, if there is little improvement of
c, x∗), we weak the vote of

It is found that after adding a subset
communication subset
D
p+i(y∗
M+i by assigning a small βi that approaches zero.
As for the size of Xc, more data points bring more infor-
mative
c and better GRBCM predictions at the cost of
higher computing complexity. In this article, we assign all
the experts with the same training size as nc = ni = m0
M .
and n+i = 2m0 for 2

M

|D

i

≤

≤

Next, we show that the GRBCM’s predictive distribution
will converge to the underlying true predictive distribution
.
when n
→ ∞
Proposition 3. Let
ing data
limn→∞ n/M 2

Mn
i=1 be a partition of the train-
i
}
{D
with (i)
and (ii)
limn→∞ Mn =
n > 0. Besides, among the M subsets, there

∞

D

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

c, the points in which
is a global communication subset
without replacement. Let
are randomly selected from
Mn
the global expert
i=2
be GPs with zero mean and stationary covariance function
k(.) > 0. Then, GRBCM yields consistent predictions as

c and the enhanced experts

{M+i

M

D

D

}

4. Numerical experiments

4.1. Toy example

We employ a 1D toy example

µGRBCM(x∗) = µη(x∗),

GRBCM(x∗) = σ2
σ2
η.

(16)

lim
n→∞
lim
n→∞






D

D\D

The detailed proof is provided in Appendix C. It is found
in Proposition 3 that apart from the requirement that the
communication subset
c should be a random subset, the
consistency of GRBCM holds for any partition of the re-
c. Besides, according to Propositions 2
maining data
and 3, both GPoE and GRBCM produce consistent pre-
dictions using random partition. It is known that the GP
model
provides more conﬁdent predictions, i.e., lower
M
σ2(x)dx, with more data points.
uncertainty U (
Since GRBCM trains experts on more informative subsets
{D+i
Remark 3. When using random subsets, the GRBCM’s
prediction uncertainty is always lower than that of GPoE,
since the discrepancy δU −1 = U −1
GPoE satisﬁes

R
M
i=2, we have the following ﬁnding.

U −1

) =

M

}

δU −1 =

U −1(

M+2)

−

"

Mn

+

βi

σ−2
+i (x∗)

Z

i=3
X

(cid:0)

GRBCM −
Mn
1
Mn

U −1(

i=1
X
σ−2
c (x∗)

−

i)

#

M

dx∗ > 0

(cid:1)

for a large enough n. It means compared to GPoE, GR-
BCM converges faster to the underlying function when
n

.
→ ∞

Finally, similar to RBCM, GRBCM can be executed in
multi-layer computing architectures with identical predic-
tions (Deisenroth & Ng, 2015; Ionescu, 2015), which allow
to run optimally and efﬁciently with the available comput-
ing infrastructure for distributed computing.

3.2. Complexity

i

≤

M
i=1 have the same train-
Assuming that the experts
i
}
{M
M . Compared to
ing size ni = m0 = n/M for 1
≤
(G)PoE and (R)BCM, the proposed GRBCM has a higher
time complexity in prediction due to the construction of
M
i=2. In prediction, it ﬁrst needs to cal-
new experts
{M+i
}
culate the inverse of k(Xc, Xc) and M
1 augmented
−
M
Xi, Xc
covariance matrices
i=2, which
)
k(
{
}
}
{
7m3
(8nm2
0), in order to obtain the predictions
scales as
0−
O
M
M
i=2 and σ2
σ2
c ,
µc,
i=2. Then, it combines the pre-
µ+i
+i}
{
}
{
i=2 at n′ test points. Therefore,
M
c and
dictions of
{M+i
}
the time complexity of the GRBCM prediction process is
(βn′nm0), where α = (8M
7)/M and

Xi, Xc

M

}

{

,

0) +

−

(αnm2
O
β = (4M

O
3)/M .

−

f (x) = 5x2 sin(12x) + (x3
+ 4 cos(2x) + ǫ,

−

0.5) sin(3x

0.5)

−

(17)

where ǫ
existing aggregation models.

∼ N

(0, 0.25), to illustrate the characteristics of

×

×

−

M

104, 105, 5

We generate n = 104, 5
105 and 106 train-
ing points, respectively, in [0, 1], and select n′ = 0.1n test
0.2, 1.2]. We pre-normalize each col-
points randomly in [
umn of X and y to zero mean and unit variance. Due to
the global expert
c in GRBCM, we slightly modify the
disjoint partition: we ﬁrst generate a random subset and
then use the k-means technique to generate M
1 dis-
joint subsets. Each expert is assigned with m0 = 500 data
points. We implement the aggregations by the GPML tool-
box5 using the SE kernel in (1) and the conjugate gradi-
ents algorithm with the maximum number of evaluations
as 500, and execute the code on a workstation with four
3.70 GHz cores and 16 GB RAM (multi-core computing
in Matalb is employed). Finally, we use the Standard-
ized Mean Square Error (SMSE) to evaluate the accuracy
of prediction mean, and the Mean Standardized Log Loss
(MSLL) to quantify the quality of predictive distribution
(Rasmussen & Williams, 2006).

−

(a)

106

]
s
[
 
e
m

i
t
 
g
n
i
t
c
d
e
r
P

i

104

102

100

10-2

104

(b)

E
S
M
S

100

10-1

105
Training size

106

104

105
Training size

106

PoE
GPoE
BCM
RBCM
NPAE
GRBCM
Training time

(c)

L
L
S
M

1.5

0.5

2

1

0

-0.5

-1

-1.5

104

105
Training size

106

Figure2. Comparison of different aggregation models on the toy
example in terms of (a) computing time, (b) SMSE and (c) MSLL.

Fig. 2 depicts the comparative results of six aggregation
models on the toy example. Note that NPAE using n >
104 is unavailable due to the time-consuming predic-
5
tion process. Fig. 2(a) shows that these models require the
same training time, but they differ in the predicting time.

×

5http://www.gaussianprocess.org/gpml/code/matlab/doc/

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

Due to the communication expert, the GRBCM’s predict-
ing time slightly offsets the curves of (G)PoE and (R)BCM.
The NPAE however exhibits signiﬁcantly larger predicting
time with increasing M and n′. Besides, Fig. 2(b) and
(c) reveal that GRBCM and NPAE yield better predictions
with increasing n, which conﬁrm their consistency when
.6 As for NPAE, though performing slightly better
n
104, it requires several orders
than GRBCM using n = 5
of magnitude larger predicting time, rendering it unsuitable
for cases with many test points and subsets.

→ ∞

×

4.2. Medium-scale datasets

We use two realistic datasets, kin40k (8D, 104 train-
104 test points) (Seeger et al., 2003) and
ing points, 3
sarcos (21D, 44484 training points, 4449 test points)
(Rasmussen & Williams, 2006), to assess the performance
of our approach.

×

The comparison includes all the aggregations except the
expensive NPAE.8 Besides, we employ the fully indepen-
dent training conditional (FITC) (Snelson & Ghahramani,
2006), the GP using stochastic variational inference (SVI)9
(Hensman et al., 2013), and the subset-of-data (SOD)
(Chalupka et al., 2013) for comparison. We select the in-
ducing size m for FITC and SVI, the batch size mb for SVI,
and the subset size msod for SOD, such that the computing
time is similar to or a bit larger than that of GRBCM. Partic-
ularly, we choose m = 200, mb = 0.1n and msod = 2500
for kin40k, and m = 300, mb = 0.1n and msod = 3000 for
sarcos. Differently, SVI employs the stochastic gradients
algorithm with tsg = 1200 iterations. Finally, we adopt the
disjoint partition used before to divide the kin40k dataset
into 16 subsets, and the sarcos dataset into 72 subsets for
the aggregations. Each experiment is repeated ten times.

kin40k

kin40k

(a)

0.1

0.08

E
S
M
S

0.06

0.04

0.02

60

(c)

0.04

0.03

E
S
M
S

0.02

0.01

(b)

-0.5

-1

-1.5

L
L
S
M

-2

60

3

2

1

0

-1

-2

L
L
S
M

×

Figure3. Illustrations of the aggregation models on the toy exam-
ple. The green “+” symbols represent the 104 data points. The
shaded area indicates 99% conﬁdence intervals of the full GP pre-
dictions using n = 104.
Fig. 3 illustrates the six aggregation models using n = 104
105, respectively, in comparison to the full
and n = 5
GP (ground truth) using n = 104.7 It is observed that in
terms of prediction mean, as discussed in remark 1, PoE
and GPoE provide poorer results in the entire domain with
increasing n. On the contrary, BCM and RBCM provide
good predictions in the range [0, 1]. As discussed in re-
mark 2, BCM however yields unreliable predictions when
leaving the training data. RBCM alleviates the issue by
using a varying βi. In terms of prediction variance, with
increasing n, PoE and (R)BCM tend to shrink to zero (over-
conﬁdent), while GPoE tends to approach σ2
∗∗ (too con-
servative). Particularly, PoE always has the largest MSLL
value in Fig. 2(b), since as discussed in remark 2, its pre-
diction variance approaches zero faster.

6Further discussions of GRBCM is shown in Appendix D.
7The full GP is intractable using our computer for n = 5 ×

105.

70

80
Computing time [s]

90

100

70

80
Computing time [s]

90

100

sarcos

sarcos

(d)

SVI
FITC
SOD
PoE
GPoE
BCM
RBCM
GRBCM

0
350

400

450
550
500
Computing time [s]

600

650

450
550
500
Computing time [s]

600

650

-3
350

400

Figure4. Comparison of the approximation models on the kin40k
and sarcos datasets.

Fig. 4 depicts the comparative results of different approx-
imation models over 10 runs on the kin40k and sarcos
datasets. The horizontal axis represents the sum of train-
ing and predicting time. It is ﬁrst observed that GRBCM
provides the best performance on the two datasets in terms
of both SMSE and MSLL at the cost of requiring a bit
more computing time than (G)PoE and (R)BCM. As for
(R)BCM, the small SMSE values reveal that they provide
better prediction mean than FITC and SOD; but the large
MSLL values again conﬁrm that they provide overconﬁ-

8The comparison of NPAE and GRBCM are separately pro-

vided in Appendix E.

9https://github.com/SheffieldML/GPy

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

dent prediction variance. As for (G)PoE, they suffer from
poor prediction mean, as indicated by the large SMSE; but
GPoE performs well in terms of MSLL. Finally, the simple
SOD outperforms FITC and SVI on the kin40k dataset, and
performs similarly on the sarcos dataset, which are consis-
tent with the ﬁndings in (Chalupka et al., 2013).

kin40k

kin40k

(a)

E
S
M
S

10-1

10-2

0

(c)

E
S
M
S

10-1

10-2

(a)

E
S
M
S

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

(c)

0.035

0.03

0.025

E
S
M
S

0.02

0.015

0.01

0.005

0

20
60
40
Number of experts
sarcos

80

-5

0

20
60
40
Number of experts
sarcos

80

(b)

15

L
L
S
M

10

5

0

(d)

40

L
L
S
M

30

20

10

0

-10

0

PoE
GPoE
BCM
RBCM
GRBCM

0

100

200

300

100

200

300

Number of experts

Number of experts

Figure5. Comparison of the aggregation models using different
numbers of experts on the kin40k and sarcos datasets.

Next, we explore the impact of the number M of experts
on the performance of aggregations. To this end, we run
them on the kin40k dataset with M respectively being 8,
16 and 64, and we run on the sarcos dataset with M respec-
tively being 36, 72 and 288. The results in Fig. 5 turn out
that all the aggregations perform worse with increasing M ,
since the experts become weaker; but GRBCM still yields
the best performance with different M . Besides, with in-
creasing M , the poor prediction mean and the vanishing
prediction variance of PoE result in the sharp increase of
MSLL values.

kin40k

kin40k

(b)

2.5

L
L
S
M

(d)

L
L
S
M

1.5

0.5

2

1

0

-0.5

-1

-1.5

-2

60

50

40

30

20

10

0

PoE

GPoE

BCM RBCM GRBCM

PoE

GPoE

BCM RBCM GRBCM

sarcos

sarcos

disjoint
random

PoE

GPoE

BCM RBCM GRBCM

PoE

GPoE

BCM RBCM GRBCM

Figure6. Comparison of the aggregation models using disjoint
and random partitions on the kin40k dataset (M = 16) and the
sarcos dataset (M = 72).

Table1. Comparative results of the aggregation models and SVI
on the song and electric datasets.

song (450K)
SMSE MSLL

electric (1.8M)
SMSE MSLL

0.8527
POE
0.8527
GPOE
2.6919
BCM
1.3383
RBCM
SVI
0.7909
GRBCM 0.7321

328.82
0.1159
156.62
24.930
-0.1885
-0.1571

0.1632
0.1632
0.0073
0.0027
0.0042
0.0024

1040.3
24.940
51.081
85.657
-1.1410
-1.3161

or random) on the performance of aggregations. The av-
erage results in Fig. 6 turn out that the disjoint partition
is more beneﬁcial for the aggregations. The results are
expectable since the disjoint subsets provide separate and
reﬁned local information, whereas the random subsets pro-
vide overlapping and coarse global information. But we ob-
serve that GPoE performs well on the sarcos dataset using
random partition, which conﬁrms the conclusions in Propo-
sition 2. Besides, as revealed in remark 3, even using ran-
dom partition, GRBCM outperforms GPoE.

4.3. Large-scale datasets

This section explores the performance of aggregations and
SVI on two large-scale datasets. We ﬁrst assess them on
the 90D song dataset, which is a subset of the million song
dataset (Bertin-Mahieux et al., 2011). The song dataset
is partitioned into 450000 training points and 65345 test
points. We then assess the models on the 11D electric
dataset that is partitioned into 1.8 million training points
and 249280 test points. We follow the normalization and
data pre-processing in (Wilson et al., 2016) to generate the
two datasets.10 For the song dataset, we use the foregoing
disjoint partition to divide it into M = 720 subsets, and
use m = 800, mb = 5000 and tsg = 1300 for SVI; for the
electric dataset, we divide it into M = 2880 subsets, and
use m = 1000, mb = 5000 and tsg = 1500 for SVI. As a
result, each expert is assigned with m0 = 625 data points
for the aggregations.

Table 1 reveals that the (G)PoE’s SMSE value is smaller
than that of (R)BCM on the song dataset. The poor pre-
diction mean of BCM is caused by the fact that the song
dataset is highly clustered such that BCM suffers from
weak experts in regions with scarce points. On the contrary,
due to the almost uniform distribution of the electric data
points, the (R)BCM’s SMSE is much smaller than that of
(G)PoE. Besides, unlike the vanishing prediction variances
of PoE and (R)BCM when n
, GPoE provides conser-
vative prediction variance, resulting in small MSLL values
on the two datasets. The proposed GRBCM always outper-

→ ∞

10The datasets and the pre-processing scripts are available in

Finally, we investigate the impact of data partition (disjoint

https://people.orie.cornell.edu/andrew/.

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

forms the other aggregations in terms of both SMSE and
MSLL on the two datasets due to the consistency. Finally,
GRBCM performs similarly to SVI on the song dataset; but
GRBCM outperforms SVI on the electric dataset.

Chalupka, Krzysztof, Williams, Christopher KI, and Mur-
ray, Iain. A framework for evaluating approximation
methods for Gaussian process regression. Journal of Ma-
chine Learning Research, 14(Feb):333–350, 2013.

5. Conclusions

To scale the standard GP to large-scale regression, we
present the GRBCM aggregation model, which introduces
a global communication expert to yield consistent yet ef-
ﬁcient predictions when n
. Through theoretical
and empirical analyses, we demonstrated the superiority of
GRBCM over existing aggregations on datasets with up to
1.8M training points.

→ ∞

The superiority of local experts is the capability of captur-
ing local patterns. Hence, further works will consider the
experts with individual hyperparameters in order to capture
non-stationary and heteroscedastic features.

Acknowledgements

This work was conducted within the Rolls-Royce@NTU
from the National Re-
Corporate Lab with support
search Foundation (NRF) Singapore under
the Corp
Lab@University Scheme. It is also partially supported by
the Data Science and Artiﬁcial Intelligence Research Cen-
ter (DSAIR) and the School of Computer Science and En-
gineering at Nanyang Technological University.

References

Alvarez, Mauricio A, Rosasco, Lorenzo, Lawrence, Neil D,
et al. Kernels for vector-valued functions: A review.
Foundations and Trends R
in Machine Learning, 4(3):
(cid:13)
195–266, 2012.

Bauer, Matthias, van der Wilk, Mark, and Rasmussen,
Carl Edward. Understanding probabilistic sparse Gaus-
sian process approximations. In Advances in Neural In-
formation Processing Systems, pp. 1533–1541. Curran
Associates, Inc., 2016.

Choi, Taeryon and Schervish, Mark J. Posterior consis-
tency in nonparametric regression problems under Gaus-
sian process priors. Technical report, Carnegie Mellon
University, 2004.

Deisenroth, Marc Peter and Ng, Jun Wei. Distributed Gaus-
sian processes. In International Conference on Machine
Learning, pp. 1481–1490. PMLR, 2015.

Fu, Yifan, Zhu, Xingquan, and Li, Bin. A survey on in-
stance selection for active learning. Knowledge and In-
formation Systems, 35(2):249–283, 2013.

Gal, Yarin, van der Wilk, Mark, and Rasmussen, Carl Ed-
ward. Distributed variational inference in sparse Gaus-
sian process regression and latent variable models.
In
Advances in Neural Information Processing Systems, pp.
3257–3265. Curran Associates, Inc., 2014.

Genest, Christian and Zidek, James V. Combining proba-
bility distributions: A critique and an annotated bibliog-
raphy. Statistical Science, 1(1):114–135, 1986.

Hensman, James, Fusi, Nicol`o, and Lawrence, Neil D.
Gaussian processes for big data. In Proceedings of the
29th Conference on Uncertainty in Artiﬁcial Intelligence,
pp. 282–290. AUAI Press, 2013.

Hinton, Geoffrey E. Training products of experts by mini-
mizing contrastive divergence. Neural Computation, 14
(8):1771–1800, 2002.

Hoang, Trong Nghia, Hoang, Quang Minh, and Low, Bryan
Kian Hsiang. A distributed variational inference frame-
work for unifying parallel sparse Gaussian process re-
In International Conference on Ma-
gression models.
chine Learning, pp. 382–391. PMLR, 2016.

Bertin-Mahieux, Thierry, Ellis, Daniel PW, Whitman,
Brian, and Lamere, Paul. The million song dataset. In
ISMIR, pp. 1–6, 2011.

Ionescu, Radu Cristian. Revisiting large scale distributed
arXiv preprint arXiv:1507.01461,

machine learning.
2015.

Bui, Thang D and Turner, Richard E. Tree-structured Gaus-
sian process approximations. In Advances in Neural In-
formation Processing Systems, pp. 2213–2221. Curran
Associates, Inc., 2014.

Lawrence, Neil. Probabilistic non-linear principal compo-
nent analysis with Gaussian process latent variable mod-
Journal of Machine Learning Research, 6(Nov):
els.
1783–1816, 2005.

Cao, Yanshuai and Fleet, David J. Generalized product of
experts for automatic and principled fusion of Gaussian
arXiv preprint arXiv:1410.7827,
process predictions.
2014.

Liu, Haitao, Cai, Jianfei, and Ong, Yew-Soon. An adaptive
sampling approach for Kriging metamodeling by maxi-
mizing expected prediction error. Computers & Chemi-
cal Engineering, 106(Nov):171–182, 2017.

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

Liu, Haitao, Cai, Jianfei, and Ong, Yew-Soon. Remarks on
multi-output Gaussian process regression. Knowledge-
Based Systems, 144(March):102–121, 2018.

Moore, David and Russell, Stuart J. Gaussian process
random ﬁelds. In Advances in Neural Information Pro-
cessing Systems, pp. 3357–3365. Curran Associates, Inc.,
2015.

Park, Chiwoo, Huang, Jianhua Z, and Ding, Yu. Domain
decomposition approach for fast Gaussian process re-
gression of large spatial data sets. Journal of Machine
Learning Research, 12(May):1697–1728, 2011.

Peng, Hao, Zhe, Shandian, Zhang, Xiao, and Qi, Yuan.
Asynchronous distributed variational Gaussian process
for regression. In International Conference on Machine
Learning, pp. 2788–2797. PMLR, 2017.

Qui˜nonero-Candela, Joaquin and Rasmussen, Carl Edward.
A unifying view of sparse approximate Gaussian process
regression. Journal of Machine Learning Research, 6
(Dec):1939–1959, 2005.

Ranjan, Roopesh and Gneiting, Tilmann. Combining prob-
ability forecasts. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 72(1):71–91, 2010.

Rasmussen, Carl E and Ghahramani, Zoubin. Inﬁnite mix-
tures of Gaussian process experts. In Advances in Neu-
ral Information Processing Systems, pp. 881–888. Cur-
ran Associates, Inc., 2002.

Rasmussen, Carl Edward and Williams, Christopher K. I.
Gaussian processes for machine learning. MIT Press,
2006.

Rulli`ere, Didier, Durrande, Nicolas, Bachoc, Franc¸ois, and
Chevalier, Cl´ement. Nested Kriging predictions for
datasets with a large number of observations. Statistics
and Computing, pp. 1–19, 2017.

Seeger, Matthias, Williams, Christopher, and Lawrence,
Neil. Fast forward selection to speed up sparse Gaussian
process regression. In Artiﬁcial Intelligence and Statis-
tics, pp. EPFL–CONF–161318. PMLR, 2003.

Shahriari, Bobak, Swersky, Kevin, Wang, Ziyu, Adams,
Ryan P, and de Freitas, Nando. Taking the human out
of the loop: A review of Bayesian optimization. Pro-
ceedings of the IEEE, 104(1):148–175, 2016.

Snelson, Edward and Ghahramani, Zoubin. Sparse Gaus-
In Advances in
sian processes using pseudo-inputs.
Neural Information Processing Systems, pp. 1257–1264.
MIT Press, 2006.

Snelson, Edward and Ghahramani, Zoubin. Local and
global sparse Gaussian process approximations. In Ar-
tiﬁcial Intelligence and Statistics, pp. 524–531. PMLR,
2007.

Tavassolipour, Mostafa, Motahari, Seyed Abolfazl, and
Shalmani, Mohammad-Taghi Manzuri.
Learning of
Gaussian processes in distributed and communication
arXiv preprint arXiv:1705.02627,
limited systems.
2017.

Titsias, Michalis K. Variational learning of inducing vari-
ables in sparse Gaussian processes. In Artiﬁcial Intelli-
gence and Statistics, pp. 567–574. PMLR, 2009.

Tresp, Volker. A Bayesian committee machine. Neural

Computation, 12(11):2719–2741, 2000.

Vazquez, Emmanuel and Bect, Julien. Pointwise consis-
tency of the Kriging predictor with known mean and
In 9th International Workshop
covariance functions.
in Model-Oriented Design and Analysis, pp. 221–228.
Springer, 2010.

Wilson, Andrew and Nickisch, Hannes. Kernel interpola-
tion for scalable structured Gaussian processes (KISS-
GP). In International Conference on Machine Learning,
pp. 1775–1784. PMLR, 2015.

Wilson, Andrew Gordon, Hu, Zhiting, Salakhutdinov, Rus-
lan, and Xing, Eric P. Deep kernel learning. In Artiﬁcial
Intelligence and Statistics, pp. 370–378. PMLR, 2016.

Yuan, Chao and Neubauer, Claus. Variational mixture of
Gaussian process experts. In Advances in Neural Infor-
mation Processing Systems, pp. 1897–1904. Curran As-
sociates, Inc., 2009.

A. Proof of Proposition 1

With disjoint partition, we consider two extreme local GP
Mn),
experts. For the ﬁrst extreme expert
the test point x∗ falls into the local region deﬁned by Xan ,
i.e., x∗ is adherent to Xan when n
. Hence, we have
(Vazquez & Bect, 2010)

an (1

→ ∞

M

an

≤

≤

lim
n→∞

σ2
an (x∗) = lim
n→∞

ǫ,n = σ2
σ2
η.

M

bn , it lies farthest away
For the other extreme expert
from x∗ such that the related prediction variance σ2
bn (x∗)
is closest to σ2
= an)
∗∗. It is known that for any
where x∗ is away from the training data Xi, given the
∀x∈Xi, we have
relative distance ri = min
k
limri→∞ σ2
∗∗. Since, however, we here focus
[0, 1]d
on the GP predictions in the bounded region Ω

i (x∗) = σ2

i (i

M

x∗

−

x

k

∈

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

and employ the covariance function k(.) > 0, then the pos-
is small but satisﬁes
itive sequence cn =
{
limn→∞ cn > 0 and

σ−2
∗∗ }

σ−2
bn

(x∗)

−

points. Hence, we have

σ−2
i

(x∗)

σ−2
∗∗ ≥

−

cn, 1

i

= an

Mn.

≤

≤

The equality holds only when i = bn.

Thereafter, with the sequence ǫn = min
{
0 where α > 0 we have

cn, 1
M α

n→∞

n } →

σ−2
i

(x∗)

σ−2
∗∗ ≥

−

cn

≥

ǫn, 1

i

= an

Mn.

≤

≤

It is found that cn = ǫn is possible to hold only when Mn
is small. With the increase of n, ǫn quickly becomes much
smaller than cn since limn→∞ 1/M α

n = 0.

The typical aggregated prediction variance writes

σ−2
A,n(x∗) =

βi(σ−2
i

(x∗)

∗∗ ) + σ−2
σ−2
∗∗ ,

(18)

−

Mn

i=1
X

where for (G)PoE we remove the prior precision σ−2
∗∗ . We
prove below the inconsistency of (G)PoE and (R)BCM us-
ing disjoint partition.

Mn
i=1 σ−2
(x∗) > Mnσ−2
For PoE, (18) is
leading to the inconsistent variance limn→∞ σ2
(R)BCM, the ﬁrst term of σ−2
that n is large enough,

,
n→∞
∞
A,n = 0. For
A,n(x∗) in (18) satisﬁes, given

∗∗ →

P

i

Mn

i=1
X

βi(σ−2
i

(x∗)

σ−2
∗∗ ) > ǫn

βi =

−

Mn

i=1
X

1
M α
n

Mn

i=1
X

βi.

Taking βi = 1 for BCM and α = 0.5, we have
, leading to the incon-
n→∞
A,n = 0. For RBCM, since

Mn
1
i=1 βi = √Mn
M α
→
n
sistent variance limn→∞ σ2

∞

P

βi = 0.5(log σ2

log σ2

i (x∗))

0.5 log(1 + cnσ2

∗∗)

∗∗ −

≥

where the equality holds only when i = bn, we have
, lead-

Mn
1
i=1 βi > 0.5 log(1 + cnσ2
n→∞
M α
→
n
ing to the inconsistent variance limn→∞ σ2
A,n = 0.

∗∗)√Mn

∞

P

Finally, for GPoE, we know that when n
converges to σ−2
isfy cn + σ−2
1

an (x∗)
η ; but the other prediction precisions sat-
for
Mn, since x∗ is away from their training

(x∗) < σ−2

n→∞ σ−2

ǫ,n →

→ ∞

= an

σ−2
i

i

η

, σ−2

∗∗ ≤
≤

≤

lim
n→∞

= lim
n→∞

σ−2
η −
1
(cid:0)
Mn

σ−2
GPoE(x∗)

(cid:1)
σ−2
an (x∗)

σ−2
η −
Mn

i6=an (cid:0)
X
σ−2
η −
Mn

(cid:0)
1
Mn

(cid:0)
1
Mn

+ lim
n→∞

> lim
n→∞

1
Mn

+ lim
n→∞

σ−2
η −

i6=an (cid:0)
X
GPoE(x∗)

(cid:1)
σ−2
i

(x∗)

σ−2
η −

(cid:1)

σ−2
an (x∗)

(cid:1)
σ−2
ǫ,n(x∗)

= 0,

that σ2
GPoE(x∗) > σ2

which means
limn→∞ σ2
ﬁnd that limn→∞ σ−2
limn→∞ σ2

GPoE(x∗) < σ2

GPoE(x∗) > cn + σ−2
bn (x∗) < σ2

∗∗.

(cid:1)
inconsistent

is

since
η. Meanwhile, we easily
∗∗ , leading to

B. Proof of Proposition 2

With smoothness assumption and particularly distributed
noise (normal or Laplacian distribution), it has been proved
that the GP predictions would converge to the true predic-
(Choi & Schervish, 2004). Hence,
tions when n
given that the points in Xi are randomly selected without
replacement from X and ni = n/Mn
, we have

→ ∞

n→∞

lim
n→∞

µi(x∗) = µη(x∗), lim
n→∞

i (x∗) = σ2
σ2
η,

→

∞
1

i

Mn.

≤

≤

For the aggregated prediction variance, we have

Mn

i=1
X

lim
n→∞

σ−2
A,n(x∗) = lim

n→∞ "

βi(σ−2
i

(x∗)

∗∗ ) + σ−2
σ−2
∗∗

,

−

#

i

A,n(x∗) = limn→∞ Mnσ−2

where for (G)PoE we remove σ−2
1 and limn→∞ σ−2
(x∗) = σ−2
tent variance limn→∞ σ−2

∗∗ . For PoE, given βi =
η , we have the inconsis-
η =
. For GPoE, given βi = 1/Mn we have the consis-
∞
tent variance limn→∞ σ−2
η = σ−2
σ−2
η .
For BCM, given βi = 1 we have the inconsistent vari-
ance limn→∞ σ−2
σ−2
∗∗ ) +
σ−2
. Finally, for RBCM, given limn→∞ βi =
∗∗ ] =
β = 0.5 log(σ2
η), we have the inconsistent vari-
ance limn→∞ σ−2
σ−2
∗∗ ) +
σ−2
∗∗ ] =

∗∗/σ2
A,n(x∗) = limn→∞[Mnβ(σ−2

A,n(x∗) = limn→∞[Mn(σ−2

A,n(x∗) = Mn

η −

η −

1
Mn

∞

.
∞

Then, for the aggregated prediction mean we have

Mn

lim
n→∞

µA,n(x∗) = lim
n→∞

σ2
A,n(x∗)

βiσ−2
i

(x∗)µi(x∗).

i=1
X
For PoE, given βi = 1 and limn→∞ σ−2
1/Mn, we

i
consistent

have

the

(x∗)/σ−2
A,n(x∗) =
prediction mean

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

(x∗)/σ−2

limn→∞ µA,n(x∗) = µη(x∗).
For GPoE, given
βi = 1/Mn and limn→∞ σ−2
A,n(x∗) = 1, we
i
have the consistent prediction mean limn→∞ µA,n(x∗) =
µη(x∗).
For (R)BCM, given βi = β = 1 or
limn→∞ βi = β = 0.5 log(σ2
η), we have the
inconsistent prediction mean limn→∞ µA,n(x∗) =
limn→∞ βσ−2
∗∗ /Mn) =
aµη(x∗) where a = σ−2
1 and the
equality holds when σ2
η = 0.

∗∗ ) + σ−2
σ−2
σ−2
∗∗ )

η µη(x∗)/(β(σ−2

η −
η /(σ−2

∗∗/σ2

η −

≥

C. Proof of Proposition 3

→

n→∞

Given that the points in the communication subset
are randomly selected without replacement from
nc = n/Mn
µη(x∗) and limn→∞ σ2
for the expert
i,
D+i =
limn→∞ µ+i(x∗) = µη(x∗) and limn→∞ σ2
for 2

c
D
and
, we have limn→∞ µc(x∗) =
∞
c (x∗) = σ2
c. Likewise,
trained on the augmented dataset
with size n+i = 2n/Mn, we have
+i(x∗) = σ2
η

M+i
c
}
D

η for

M .

{D

M

D

i

≤

≤

We ﬁrst derive the upper bound of σ2
c (x∗). For the station-
ary covariance function k(.) > 0, when nc is large enough
we have (Vazquez & Bect, 2010)

σ2
c (x∗)

k(x∗, x∗)

≤

k2(x∗, x′)
k(x′, x′)

−

+ σ2

ǫ,n,

k

k

∈

−

x′

where x′
Xc is the nearest data point to x∗. It is known
x∗
is proportional
that the relative distance rc =
to the inverse of the training size nc, i.e., rc
1/nc =
n→∞ 0. Conventional stationary covariance func-
Mn/n
tions only relay on the relative distance (once the covari-
ance parameters have been determined) and decrease with
rc. Consequently, the prediction variance σ2
c (x∗) increases
with rc. Taking the SE covariance function in (1) for ex-
ample,11 when rc
,

0 we have, given l0 = min1≤i≤d

→

∝

li

{

}

→

σ2
c (x∗)

We clearly see from this inequality that when rc
η since limn→∞ σ2
c (x∗) goes to σ2
σ2
Then, we rewrite the precision of GRBCM in (14b) as,
given β2 = 1,

ǫ,n = σ2
η.

→

0,

GRBCM(x∗) = σ−2
σ−2

+2(x∗)+

βi

σ−2
+i (x∗)

σ−2
c (x∗)

.

Mn

i=3
X

(cid:0)

−

(cid:1)
(20)

11We take the SE kernel for example since conventional kernels,
e.g., the rational quadratic kernel and the Mat´ern class of kernels,
can reduce to the SE kernel under some conditions.

Compared to
c,
M+i is trained on a more dense dataset
M
c (x∗) for a large enough n.12
σ2
D+i, leading to σ2
+i(x∗)
Given (19) and σ2
+i(x∗) > σ2
ǫ,n, the weight βi satisﬁes, for
3

Mn,

≤

i

≤

0

≤

≤

βi =

log

1
2

1
2

<

1
2

σ2
c (x∗)
σ2
+i(x∗)
(cid:19)
c + σ2
ar2
ǫ,n
σ2
ǫ,n ! ≤

(cid:18)

 

log

(cid:18)
a
2σ2

ǫ,n

σ2
c (x∗)
σ2
ǫ,n (cid:19)

r2
c .

<

log

(21)

Besides, the precision discrepancy satisﬁes, for 3
Mn,

i

≤

≤

0

≤

σ−2
+i (x∗)

σ−2
c (x∗) = σ−2

c (x∗)

−

σ2
c (x∗)
σ2
+i(x∗) −

1

(cid:19)

<

1
σ2

a
σ2

ǫ,n

ǫ,n

(cid:18)
r2
c .

(22)
Hence, the second term in the right-hand side of (20) satis-
ﬁes

Mn

βi

σ−2
+i (x∗)

σ−2
c (x∗)

<

−

Mn

a2
2σ6

ǫ,n

r4
c ∝

M 5
n
n4 .

i=3
X
n > 0, we have limn→∞ n4/M 5

(cid:1)

n =

i=3
X

(cid:0)
Since limn→∞ n/M 2
, and furthermore,

∞

Mn

lim
n→∞

βi

σ−2
+i (x∗)

σ−2
c (x∗)

= 0.

(23)

−

(cid:0)

i=3
X
Substituting (23) and limn→∞ σ−2
we have a consistent prediction precision as
σ−2
GRBCM(x∗) = σ−2
η .

(cid:1)

+2 (x∗) = σ−2
η

lim
n→∞

into (20),

Similarly, we rewrite the GRBCM’s prediction mean in
(14a) as

µGRBCM(x∗) = σ2

GRBCM(x∗)

µ∆ + σ−2

+2(x∗)µ+2(x∗)

,
(24)
(cid:1)

(cid:0)

−

µ∆ =

βi

σ−2
+i (x∗)µ+i(x∗)

σ−2
c (x∗)µc(x∗)

.

(cid:0)

i=3
X
Let δmax = max3≤i≤Mn
0, we have

µ∆| ≤

|

Mn

βiσ−2
c

i=3
X
Eq.(21)
<

(cid:12)
(cid:12)
(cid:12)
ar2
(cid:12)
c
2σ4

ǫ,n

Mn

i=3
X

σ2
c (x∗)
+i(x∗) µ+i(x∗)
σ2

(cid:12)
(cid:12)
(cid:12)
σ2
c (x∗)
σ2
+i(x∗)

µ+i(x∗)

−

δmax →

n→∞ 0.

(cid:1)

n→∞

→

(25)

−

µc(x∗)
(cid:12)
(cid:12)
(cid:12)

µc(x∗)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

12The equality is possible to hold when we employ disjoint par-

tition for {Di}

Mn
i=2 and x∗ is away from Xi.

≤

<

σ2
f exp(

σ2
f −
σ2
f
c + σ2
r2
l2
0

−
ǫ,n = ar2

c /l2
r2

0) + σ2
ǫ,n

c + σ2

ǫ,n.

(19)

where

Mn

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

Substituting (25) into (24), we have the consistent predic-
tion mean as

lim
n→∞

µGRBCM(x∗) = µη(x∗).

D. Discussions of GRBCM on the toy example

It is observed that the proposed GRBCM showcases superi-
ority over existing aggregations on the toy example, which
is brought by the particularly designed aggregation struc-
ture: the global communication expert
c to capture the
long-term features of the target function, and the remaining
M
i=2 to reﬁne local predictions.
experts

M

{M+i

}

(a)

0.16

E
S
M
S

0.14

0.12

0.1

0.08

0.06

104

(b)

-1.35

-1.4

L
L
S
M

-1.45

-1.5

105
Training size

106

-1.55

104

105
Training size

106

Figure7. Comparative results of GRBCM and Mc on the toy ex-
ample.

M

To verify the capability of GRBCM, we compare it with
the pure global expert
c which relies on a random sub-
set Xc. Fig. 7 shows the comparative results of GRBCM
c on the toy example. It is found that with increas-
and
ing n, (i) GRBCM always outperforms
c because of the
beneﬁts brought by local experts; and (ii) the predictions of
c generally become poorer since it becomes intractable

M
to choose a good subset from the increasing dataset.

M

M

E. Experimental results of NPAE

Table 2 compares the results of GRBCM and NPAE over
10 runs on the kin40k dataset (M = 16) and the sarcos
dataset (M = 72) using disjoint partition. It is observed
that GRBCM performs slightly better than NPAE on the
kin40k dataset, and produces competitive results on the sar-
cos dataset. But in terms of the computing efﬁciency, since
NPAE needs to build and invert an M
M covariance ma-
trix at each test point, it requires much more running time,
especially for the sarcos dataset with M = 72.

×

Table2. Comparative results (mean and standard deviation) of
GRBCM and NPAE over 10 runs on the kin40k dataset (M = 16)
and the sarcos dataset (M = 72) using disjoint partition. The
computing time t for each model involves the training and pre-
dicting time.

kin40k

SMSE
MSLL
t [S]
sarcos

SMSE
MSLL
t [S]

GRBCM

NPAE

0.0223 ± 0.0005
-1.9927 ± 0.0177
78.1 ± 4.4
GRBCM

0.0246 ± 0.0007
-1.9565 ± 0.0170
2852.4 ± 16.7
NPAE

0.0074 ± 0.0002
-2.3681 ± 0.0242
445.6 ± 49.4

0.0054 ± 0.0001
-2.5900 ± 0.0068
26444.0 ± 1213.0

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian
Process Regression

8
1
0
2
 
n
u
J
 

3

 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
0
2
7
0
0
.
6
0
8
1
:
v
i
X
r
a

Haitao Liu 1 Jianfei Cai 2 Yi Wang 3 Yew-Soon Ong 2 4

Abstract

predictive distributions at test points.

In order to scale standard Gaussian process (GP)
regression to large-scale datasets, aggregation
models employ factorized training process and
then combine predictions from distributed ex-
perts. The state-of-the-art aggregation models,
however, either provide inconsistent predictions
or require time-consuming aggregation process.
We ﬁrst prove the inconsistency of typical aggre-
gations using disjoint or random data partition,
and then present a consistent yet efﬁcient aggre-
gation model for large-scale GP. The proposed
model inherits the advantages of aggregations,
e.g., closed-form inference and aggregation, par-
allelization and distributed computing. Further-
more, theoretical and empirical analyses reveal
that the new aggregation model performs better
due to the consistent predictions that converge
to the true underlying function when the training
size approaches inﬁnity.

1. Introduction

Gaussian process (GP) (Rasmussen & Williams, 2006) is
a well-known statistical learning model extensively used
in various scenarios, e.g., regression, classiﬁcation, opti-
mization (Shahriari et al., 2016), visualization (Lawrence,
2005), active learning (Fu et al., 2013; Liu et al., 2017) and
multi-task learning (Alvarez et al., 2012; Liu et al., 2018).
n
Given the training set X =
xi
i=1 and the observa-
{
n
tion set y =
i=1, as an approximation of the
R
}
{
underlying function η : Rd
R, GP provides informative

y(xi)

Rd

∈

∈

}

→

1Rolls-Royce@NTU Corporate Lab, Nanyang Technologi-
cal University, Singapore 637460 2School of Computer Science
and Engineering, Nanyang Technological University, Singapore
639798 3Applied Technology Group, Rolls-Royce Singapore, 6
Seletar Aerospace Rise, Singapore 797575 4Data Science and
Artiﬁcial Intelligence Research Center, Nanyang Technological
University, Singapore 639798. Correspondence to: Haitao Liu
<htliu@ntu.edu.sg>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

However, the most prominent weakness of the full GP is
that it scales poorly with the training size. Given n data
points, the time complexity of a standard GP paradigm
(n3) in the training process due to the inver-
scales as
O
(n2) in
sion of an n
the prediction process due to the matrix-vector operation.
This weakness conﬁnes the full GP to training data of size

n covariance matrix; it scales as

O

×

(104).

≪

The

sparse

approximations
2005)

O
To cope with large-scale regression,
various com-
putationally efﬁcient approximations have been pre-
reviewed in
sented.
(Qui˜nonero-Candela & Rasmussen,
employ m
(m
n) inducing points to summarize the whole training
data (Seeger et al., 2003; Snelson & Ghahramani, 2006;
2007; Titsias, 2009; Bauer et al., 2016), thus reducing the
(nm2) and the predict-
training complexity of full GP to
ing complexity to
(nm). The complexity can be further
reduced through distributed inference, stochastic varia-
tional inference or Kronecker structure (Hensman et al.,
2013; Gal et al.,
2015;
Hoang et al., 2016; Peng et al., 2017). A main draw-
the
back of sparse approximations, however,
representational capability is limited by the number of
inducing points (Moore & Russell, 2015). For example,
for a quick-varying function, the sparse approximations
need many inducing points to capture the local structures.
That is, this kind of scheme has not reduced the scaling of
the complexity (Bui & Turner, 2014).

2014; Wilson & Nickisch,

is that

O

O

The method exploited in this article belongs to the aggre-
gation models (Hinton, 2002; Tresp, 2000; Cao & Fleet,
2014; Deisenroth & Ng, 2015; Rulli`ere et al., 2017), also
known as consensus statistical methods (Genest & Zidek,
1986; Ranjan & Gneiting, 2010). This kind of scheme pro-
duces the ﬁnal predictions by the aggregation of M sub-
models (GP experts) respectively trained on the subsets
, thus distributing
=
{D
the computations to “local” experts. Particularly, due to
the product of experts, the aggregation scheme derives a
factorized marginal likelihood for efﬁcient training; and
then it combines the experts’ posterior distributions accord-
In comparison to
ing to a certain aggregation criterion.

M
i=1 of

Xi, yi

X, y

i =

}}

D

}

{

{

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

sparse approximations, the aggregation models (i) operate
directly on the full training data, (ii) require no additional
inducing or variational parameters and (iii) distribute the
computations on individual experts for straightforward par-
allelization (Tavassolipour et al., 2017), thus scaling them
In comparison to typ-
to arbitrarily large training data.
ical local GPs (Snelson & Ghahramani, 2007; Park et al.,
2011), the aggregations smooth out the ugly discontinu-
ity by the product of posterior distributions from GP ex-
perts. Note that the aggregation methods are different from
the mixture-of-experts (Rasmussen & Ghahramani, 2002;
Yuan & Neubauer, 2009), which suffers from intractable in-
ference and is mainly developed for non-stationary regres-
sion.

However, it has been pointed out (Rulli`ere et al., 2017) that
there exists a particular type of training data such that typ-
ical aggregations, e.g., product-of-experts (PoE) (Hinton,
2002; Cao & Fleet, 2014) and Bayesian committee ma-
chine (BCM) (Tresp, 2000; Deisenroth & Ng, 2015), can-
not offer consistent predictions, where “consistent” means
the aggregated predictive distribution can converge to the
true underlying predictive distribution when the training
size n approaches inﬁnity.

Particularly,

The major contributions of this paper are three-fold. We
ﬁrst prove the inconsistency of typical aggregation mod-
els, e.g., the overconﬁdent or conservative prediction vari-
ances illustrated in Fig. 3, using conventional disjoint or
random data partition. Thereafter, we present a consis-
tent yet efﬁcient aggregation model for large-scale GP
the proposed generalized ro-
regression.
bust Bayesian committee machine (GRBCM) selects a
global subset to communicate with the remaining sub-
sets, leading to the consistent aggregated predictive dis-
theo-
tribution derived under the Bayes rule.
retical and empirical analyses reveal that GRBCM out-
performs existing aggregations due to the consistent yet
efﬁcient predictions. We release the demo codes in
https://github.com/LiuHaiTao01/GRBCM.

Finally,

2. Aggregation models revisited

2.1. Factorized training

A GP usually places a probability distribution over the la-
(0, k(x, x′)), which is
tent function space as f (x)
deﬁned by the zero mean and the covariance k(x, x′). The
well-known squared exponential (SE) covariance function
is

∼ GP

k(x, x′) = σ2

f exp

1
2

 −

d

(xi

x′
i)2

−
l2
i

,

!

i=1
X
where σ2
f is an output scale amplitude, and li is an input
length-scale along the ith dimension. Given the noisy ob-
servation y(x) = f (x) + ǫ where the i.i.d. noise follows

|

i=1
Y
(0, Ki + σ2

(0, σ2

∼ N

ǫ ) and the training data

ǫ
(0, k(X, X) + σ2
likelihood p(y
represents the hyperparameters to be inferred.

, we have the marginal
ǫ I) where θ

X, θ) =

N

D

|

i

In order to train the GP on large-scale datasets, the aggrega-
tion models introduce a factorized training process. It ﬁrst
partitions the training set
,
}
i. In
1
data partition, we can assign the data points randomly to
the experts (random partition), or assign disjoint subsets
obtained by clustering techniques to the experts (disjoint
partition).
Ignoring the correlation between the experts

M , and then trains GP on

D
{
i as an expert

into M subsets

Xi, yi

i =

M

≤

≤

D

D

M
i=1 leads to the factorized approximation as

i

{M

}

M

p(y

X, θ)

|

≈

pi(yi

Xi, θi),

(2)

M

|
∈

∼ N

Xi, θi)

i. Note that for simplicity all

ǫ,iIi) with Ki =
where pi(yi
Rni×ni and ni being the training size
k(Xi, Xi)
of
the M GP ex-
perts in (2) share the same hyperparameters as θi = θ
(Deisenroth & Ng, 2015). The factorization (2) degener-
ates the full covariance matrix K = k(X, X) into a diag-
onal block matrix diag[K1,
≈
diag[K −1
M ]. Hence, compared to the full GP,
1 ,
the complexity of the factorized training process is reduced
to

, KM ], leading to K −1

0) given ni = m0 = n/M , 1

, K −1

(nm2

M .

· · ·

· · ·

i

≤

≤

O

Conditioned on the related subset
i, x∗)
bution pi(y∗

(µi(x∗), σ2

D

i, the predictive distri-
i (x∗)) of

i has1

|D
∼ N
µi(x∗) = kT
i∗[Ki + σ2
σ2
i (x∗) = k(x∗, x∗)

ǫ I]−1yi,
kT
i∗[Ki + σ2

−

M

(3a)

ǫ I]−1ki∗ + σ2

ǫ , (3b)

where ki∗ = k(Xi, x∗). Thereafter, the experts’ predic-
M
i=1 are combined by the following aggrega-
tions
tion methods to perform the ﬁnal predicting.

µi, σ2
i }

{

2.2. Prediction aggregation

The state-of-the-art aggregation methods include PoE
(Hinton, 2002; Cao & Fleet, 2014), BCM (Tresp, 2000;
Deisenroth & Ng, 2015), and nested pointwise aggregation
of experts (NPAE) (Rulli`ere et al., 2017).

For the PoE and BCM family, the aggregated prediction
mean and precision are generally formulated as

µA(x∗) = σ2

A(x∗)

βiσ−2
i

(x∗)µi(x∗),

(4a)

M

i=1
X

M

i=1
X

M

−

i=1
X

1Instead of using pi(f∗|Di, x∗) in (Deisenroth & Ng, 2015),
we here consider the aggregations in a general scenario where
each expert has all its belongings at hand.

(1)

σ−2
A (x∗) =

βiσ−2
i

(x∗) + (1

βi)σ−2
∗∗ ,

(4b)

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

where the prior variance σ2
a correction term to σ−2
family; and βi is the weight of the expert

∗∗ = k(x∗, x∗) + σ2

ǫ , which is
A , is only available for the BCM

i at x∗.

M

The predictions of the PoE family, which omit the prior
precision σ−2
∗∗ in (4b), are derived from the product of M
experts as

M

pA(y∗

, x∗) =

|D

pβi
i (y∗

i, x∗).

|D

(5)

i=1
Y
The original PoE (Hinton, 2002) employs the constant
weight βi = 1, resulting in the aggregated prediction vari-
ances that vanish with increasing M . On the contrary, the
generalized PoE (GPoE) (Cao & Fleet, 2014) considers a
varying βi = 0.5(log σ2
i (x∗)), which represents
the difference in the differential entropy between the prior
i, x∗), to weigh the con-
p(y∗
|
i at x∗. This varying βi brings the ﬂexi-
tribution of
bility of increasing or reducing the importance of experts
based on the predictive uncertainty. However, the vary-
ing βi may produce undesirable errors for GPoE. For in-
stance, when x∗ is far away from the training data such
that σ2

x∗) and the posterior p(y∗

σ2
∗∗, we have βi

0 and σ2

log σ2

i (x∗)

∗∗ −

M

|D

.
GPoE → ∞

→

→

The BCM family, which is opposite to the PoE family, ex-
x∗) when combin-
plicitly incorporates the GP prior p(y∗
j, BCM intro-
ing predictions. For two experts
M
duces a conditional independence assumption
y∗,
leading to the aggregated predictive distribution as

|
i and

j
⊥ D

i
D

M

|

pA(y∗

, x∗) =

|D

M

i (y∗

i=1 pβi
|D
pPi βi−1(y∗
|

i, x∗)
x∗)

.

(6)

Q
The original BCM (Tresp, 2000) employs βi = 1 but
its predictions suffer from weak experts when leaving the
data. Hence, inspired by GPoE, the robust BCM (RBCM)
(Deisenroth & Ng, 2015) uses a varying βi
to produce
robust predictions by reducing the weights of weak ex-
perts. When x∗ is far away from the training data X,
the correction term brought by the GP prior in (4b) helps
the (R)BCM’s prediction variance recover σ2
∗∗. However,
given M = 1,
the predictions of RBCM as well as
GPoE cannot recover the full GP predictions because usu-
ally β1 = 0.5(log σ2
log σ2
log σ2
f ull(x∗))

1(x∗)) = 0.5(log σ2

∗∗ −

∗∗ −

= 1.

To achieve computation gains, the above aggregations intro-
duce additional independence assumption for the experts’
predictions, which however is often violated in practice
and yields poor results. Hence, in the aggregation process,
NPAE (Rulli`ere et al., 2017) regards the prediction mean
µi(x∗) in (3a) as a random variable by assuming that yi has
not yet been observed, thus allowing for considering the co-
variances between the experts’ predictions. Thereafter, for
, µM , y∗]T, the covariances are
the random vector [µ1,

· · ·

derived as

cov[µi, y∗] = kT

cov[µi, µj] =

i,ǫ ki∗,

i∗K −1
kT
i∗K −1
kT
i∗K −1

(

i,ǫ Kij K −1
i,ǫ Kij,ǫK −1

j,ǫ kj∗,
j,ǫ kj∗,

i

= j,

i = j,

(7a)

(7b)

ǫ I,
where Kij = k(Xi, Xj)
Kj,ǫ = Kj + σ2
ǫ I, and Kij,ǫ = Kij + σ2
ǫ I. With these
covariances, a nested GP training process is performed to
derive the aggregated prediction mean and variance as

Rni×nj , Ki,ǫ = Ki + σ2

∈

µNPAE(x∗) = kT
A∗K −1
σ2
NPAE(x∗) = k(x∗, x∗)

A µ,

kT
A∗K −1

A kA∗ + σ2
ǫ ,

−

(8a)

(8b)

RM×M has K ij

RM×1 has the ith element as cov[µi, y∗],
where kA∗
∈
A = cov[µi, µj], and µ =
KA
∈
, µM (x∗)]T. The NPAE is capable of provid-
[µ1(x∗),
ing consistent predictions at the cost of implementing a
much more time-consuming aggregation because of the in-
version of KA at each test point.

· · ·

2.3. Discussions of existing aggregations

→ ∞

Though showcasing promising results (Deisenroth & Ng,
2015), given that n
and the experts are noise-free
GPs, (G)PoE and (R)BCM have been proved to be in-
consistent, since there exists particular triangular array of
data points that are dense in the input domain Ω such that
the prediction variances do not go to zero (Rulli`ere et al.,
2017).

Particularly, we further show below the inconsistency of
(G)PoE and (R)BCM using two typical data partitions (ran-
dom and disjoint partition) in the scenario where the obser-
vations are blurred with noise. Note that since GPoE us-
ing a varying βi may produce undesirable errors, we adopt
βi = 1/M as suggested in (Deisenroth & Ng, 2015). Now
the GPoE’s prediction mean is the same as that of PoE; but
the prediction variance blows up as M times that of PoE.

→ ∞

, let X

∈
[0, 1]d such that for any x

Rn×d be dense
Deﬁnition 1. When n
Ω we have
in Ω
∈
= 0. Besides, the underlying
limn→∞ min1≤i≤n
k
function to be approximated has true continuous response
µη(x) and true noise variance σ2
η.

xi

−

∈

x

k

Firstly, for the disjoint partition that uses clustering tech-
into disjoint local subsets
niques to partition the data
D
M
,
i=1, The proposition below reveals that when n
i
{D
→ ∞
PoE and (R)BCM produce overconﬁdent prediction vari-
ance that shrinks to zero; on the contrary, GPoE provides
conservative prediction variance.

}

Mn
Proposition 1. Let
i=1 be a disjoint partition of the
training data
i be
GP with zero mean and stationary covariance function

}
. Let the expert

i trained on

i
{D

M

D

D

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

∈

≤

k(.) > 0. We further assume that (i) limn→∞ Mn =
∞
and (ii) limn→∞ n/M 2
n > 0, where the second condition
implies that the subset size m0 = n/Mn and the num-
ber of experts Mn are comparable such that too weak ex-
perts are not preferred. Besides, from the second condition
, which implies that the experts
we have m0 →
become more informative with increasing n. Then, PoE
and (R)BCM produce overconﬁdent prediction variance at
x∗

Ω as

n→∞

∞

lim
n→∞

σ2
A,n(x∗) = 0,

(9)

whereas GPoE yields conservative prediction variance

σ2
η < lim
n→∞

σ2
A,n(x∗) < σ2

bn (x∗) < σ2

∗∗,

(10)

where σ2
bn

bn (x∗) is offered by the farthest expert
M
Mn) whose prediction variance is closet to σ2

bn (1
∗∗.

≤

i

M

and

i=1 σ−2

−2
σ
i
−2
P σ
i

The detailed proof is given in Appendix A. Moreover, we
have the following ﬁndings.
Remark 1. For the averaging σ−2
M
i=1

GPoE = 1
M
µi using disjoint partition, more
µ(G)PoE =
and more experts become relatively far away from x∗ when
, i.e., the prediction variances at x∗ approach σ2
n
∗∗
and the prediction means approach the prior mean µ∗∗.
Hence, empirically, when n
approaches σ2
bn
Remark 2. The BCM’s prediction variance is always larger
than that of PoE since

, and the µ(G)PoE approaches µ∗∗.

, the conservative σ2

→ ∞

→ ∞

GPoE

P

P

a∗ =

σ−2
PoE(x∗)
σ−2
BCM(x∗)

=

M

i=1 σ−2
(x∗)

i

(x∗)
(M

−

−

M

i=1 σ−2
P
i

> 1

1)σ−2
∗∗

P

→ ∞

for M > 1. This means σ2
PoE deteriorates faster to zero
when n
. Besides, it is observed that µBCM is a∗
times that of PoE, which alleviates the deterioration of pre-
. However, when x∗ is leaving
diction mean when n
σ−2
X, a∗
∗∗ . That is why BCM
suffers from undesirable prediction mean when leaving X.

M since σ−2

→ ∞
i

(x∗)

→

→

Secondly, for the random partition that assigns the data
points randomly to the experts without replacement, The
proposition below implies that when n
, the predic-
tion variances of PoE and (R)BCM will shrink to zero;
the PoE’s prediction mean will recover µη(x), but the
(R)BCM’s prediction mean cannot; interestingly, the sim-
ple GPoE can converge to the underlying true predictive
distribution.

→ ∞

Mn
Proposition 2. Let
i=1 be a random partition of
i
{D
with (i) limn→∞ Mn =
and (ii)
the training data
Mn
limn→∞ n/M 2
i=1 be GPs with
zero mean and stationary covariance function k(.) > 0.

n > 0. Let the experts

{M

∞

D

}

}

i

Then, for the aggregated predictions at x∗

Ω we have

∈

lim
n→∞
lim
n→∞
lim
n→∞




where a = σ−2
when σ2

η = 0.

µPoE(x∗) = µη(x∗),

µGPoE(x∗) = µη(x∗),

lim
n→∞
lim
n→∞

σ2
PoE(x∗) = 0,
GPoE(x∗) = σ2
σ2
η,

µ(R)BCM(x∗) = aµη(x∗),

lim
n→∞

σ2
(R)BCM(x∗) = 0,

(11)

η /(σ−2

η −

σ−2
∗∗ )

≥

1 and the equality holds

The detailed proof is provided in Appendix B. Proposi-
tions 1 and 2 imply that no matter what kind of data par-
tition has been used, the prediction variances of PoE and
(R)BCM will shrink to zero when n
, which strictly
limits their usability since no beneﬁts can be gained from
such useless uncertainty information.

→ ∞

As for data partition, intuitively, the random partition pro-
vides overlapping and coarse global information about the
target function, which limits the ability to describe quick-
varying characteristics. On the contrary, the disjoint parti-
tion provides separate and reﬁned local information, which
enables the model to capture the variability of target func-
tion. The superiority of disjoint partition has been empiri-
cally conﬁrmed in (Rulli`ere et al., 2017). Therefore, unless
otherwise indicated, we employ disjoint partition for the
aggregation models throughout the article.

As for time complexity, the ﬁve aggregation models have
the same training process, and they only differ in how
to combine the experts’ predictions. For (G)PoE and
(R)BCM, their time complexity in prediction scales as
(n′nm0) where n′ is the number of test
O
points.2 For the complicated NPAE, it however needs to
M matrix KA at each test point, lead-
invert an M
ing to a greatly increased time complexity in prediction as

(nm2

0) +

O

×

(n′n2).3

O
The inconsistency of (G)PoE and (R)BCM and the ex-
tremely time-consuming process of NPAE impose the de-
mand of developing a consistent yet efﬁcient aggregation
model for large-scale GP regression.

3. Generalized robust Bayesian committee

machine

3.1. GRBCM

Our proposed GRBCM divides M experts into two groups.
The ﬁrst group has a global communication expert

c

M

2O(nm2

0) is induced by the update of M GP experts after op-

timizing hyperparameters.

3The predicting complexity of NPAE can be reduced by em-
ploying various hierarchical computing structure (Rulli`ere et al.,
2017), which however cannot provide identical predictions.

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

D
−

D1, and the second group con-
trained on the subset
c =
M
1 global or local experts4
tains the remaining M
i=2
M
trained on
i=2, respectively. The training process of
GRBCM is identical to that of typical aggregations in sec-
tion 2.1. The prediction process of GRBCM, however, is
different. Particularly, GRBCM assigns the global commu-
nication expert with the following properties:

i
{D

{M

}

}

i

with

•

•

(Random selection) The communication subset
c is
a random subset wherein the points are randomly se-
lected without replacement from
. It indicates that
the points in Xc spread over the entire domain, which
enables
c to capture the main features of the target
function. Note that there is no limit to the partition
type for the remaining M

1 subsets.

M

D

D

−

c, x∗)

M
∼ N

(Expert communication) The expert
c with pre-
(µc, σ2
c ) is
dictive distribution pc(y∗
|D
allowed to communicate with each of the remain-
It means we can utilize
ing experts
}
the augmented data
to improve
D+i =
i
}
D
over the base expert
c, leading to a new expert
M
M+i with the improved predictive distribution as
(µ+i, σ2
p+i(y∗

M
i=2.

{M

+i) for 2

M .

{D

c,

i

i

|D+i, x∗)

∼ N

≤

≤

•

(Conditional independence) Given the communica-
c and y∗, the independence assumption
tion subset
= j

D
c, y∗ holds for 2

M .

i

i
D

j
⊥ D

|D

≤

≤

Given the conditional independence assumption and the
M
i=2, we approximate the exact predictive dis-
weights
}
tribution p(y∗

, x∗) using the Bayes rule as

βi

{

|D

p(y∗

, x∗)

|D

M

i=2
Y
M

p(y∗

x∗)p(

y∗, x∗)

p(

i
D

j
|{D

}

i−1
j=1, y∗, x∗)

|

|

c
D

|

c
D

|

∝

≈

=

pβi(

i
D

|D

c, y∗, x∗)

p(y∗

x∗)p(

y∗, x∗)

p(y∗

x∗)
|
pPM

M
i=2 pβi(
c
D

i=2 βi−1(

Q

i=2
Y
D+i
|
y∗, x∗)
|

y∗, x∗)

.

(12)
c, y∗, x∗) is exact with no approximation

Note that p(
D2|D
in (12). Hence, we set β2 = 1.

With (12), GRBCM’s predictive distribution is

pA(y∗

, x∗) =

|D

M

+i(y∗

i=2 pβi
i=2 βi−1

pPM
Q
c

|D+i, x∗)
c, x∗)
(y∗
|D

.

(13)

4“Global” means the expert is trained on a random subset,

whereas “local” means it is trained on a disjoint subset.

µA(x∗) = σ2

A(x∗)

βiσ−2

+i (x∗)µ+i(x∗)

M

"

i=2
X

M

−  
i=2
X
M

i=2
X

βi

1

−

σ−2
c (x∗)µc(x∗)
#

,

!

(14a)

M

−  

i=2
X

βi

1

−

!

σ−2
c (x∗).

(14b)

σ−2
A (x∗) =

βiσ−2

+i (x∗)

c

rather than the prior σ−2

Different from (R)BCM, GRBCM employs the informa-
tive σ−2
∗∗ to correct the predic-
tion precision in (14b), leading to consistent predictions
when n
, which will be proved below. Also, the
prediction mean of GRBCM in (14a) now is corrected by
µc(x∗). Fig. 1 depicts the structure of the GRBCM aggre-
gation model.

→ ∞

Figure1. The GRBCM aggregation model.

In (14a) and (14b), the parameter βi (i > 2) akin to
that of RBCM is deﬁned as the difference in the dif-
ferential entropy between the base predictive distribution
c, x∗) and the enhanced predictive distribution
pc(y∗
|D
|D+i, x∗) as
p+i(y∗

i = 2,

βi =

1,
0.5(log σ2

(

c (x∗)

log σ2

+i(x∗)),

−

3

i

M.

≤

≤

D

≥

|D+i, x∗) over pc(y∗

(15)
2) into the
i (i
c, if there is little improvement of
c, x∗), we weak the vote of

It is found that after adding a subset
communication subset
D
p+i(y∗
M+i by assigning a small βi that approaches zero.
As for the size of Xc, more data points bring more infor-
mative
c and better GRBCM predictions at the cost of
higher computing complexity. In this article, we assign all
the experts with the same training size as nc = ni = m0
M .
and n+i = 2m0 for 2

M

|D

i

≤

≤

Next, we show that the GRBCM’s predictive distribution
will converge to the underlying true predictive distribution
.
when n
→ ∞
Proposition 3. Let
ing data
limn→∞ n/M 2

Mn
i=1 be a partition of the train-
i
}
{D
with (i)
and (ii)
limn→∞ Mn =
n > 0. Besides, among the M subsets, there

∞

D

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

c, the points in which
is a global communication subset
without replacement. Let
are randomly selected from
Mn
the global expert
i=2
be GPs with zero mean and stationary covariance function
k(.) > 0. Then, GRBCM yields consistent predictions as

c and the enhanced experts

{M+i

M

D

D

}

4. Numerical experiments

4.1. Toy example

We employ a 1D toy example

µGRBCM(x∗) = µη(x∗),

GRBCM(x∗) = σ2
σ2
η.

(16)

lim
n→∞
lim
n→∞






D

D\D

The detailed proof is provided in Appendix C. It is found
in Proposition 3 that apart from the requirement that the
communication subset
c should be a random subset, the
consistency of GRBCM holds for any partition of the re-
c. Besides, according to Propositions 2
maining data
and 3, both GPoE and GRBCM produce consistent pre-
dictions using random partition. It is known that the GP
model
provides more conﬁdent predictions, i.e., lower
M
σ2(x)dx, with more data points.
uncertainty U (
Since GRBCM trains experts on more informative subsets
{D+i
Remark 3. When using random subsets, the GRBCM’s
prediction uncertainty is always lower than that of GPoE,
since the discrepancy δU −1 = U −1
GPoE satisﬁes

R
M
i=2, we have the following ﬁnding.

U −1

) =

M

}

δU −1 =

U −1(

M+2)

−

"

Mn

+

βi

σ−2
+i (x∗)

Z

i=3
X

(cid:0)

GRBCM −
Mn
1
Mn

U −1(

i=1
X
σ−2
c (x∗)

−

i)

#

M

dx∗ > 0

(cid:1)

for a large enough n. It means compared to GPoE, GR-
BCM converges faster to the underlying function when
n

.
→ ∞

Finally, similar to RBCM, GRBCM can be executed in
multi-layer computing architectures with identical predic-
tions (Deisenroth & Ng, 2015; Ionescu, 2015), which allow
to run optimally and efﬁciently with the available comput-
ing infrastructure for distributed computing.

3.2. Complexity

i

≤

M
i=1 have the same train-
Assuming that the experts
i
}
{M
M . Compared to
ing size ni = m0 = n/M for 1
≤
(G)PoE and (R)BCM, the proposed GRBCM has a higher
time complexity in prediction due to the construction of
M
i=2. In prediction, it ﬁrst needs to cal-
new experts
{M+i
}
culate the inverse of k(Xc, Xc) and M
1 augmented
−
M
Xi, Xc
covariance matrices
i=2, which
)
k(
{
}
}
{
7m3
(8nm2
0), in order to obtain the predictions
scales as
0−
O
M
M
i=2 and σ2
σ2
c ,
µc,
i=2. Then, it combines the pre-
µ+i
+i}
{
}
{
i=2 at n′ test points. Therefore,
M
c and
dictions of
{M+i
}
the time complexity of the GRBCM prediction process is
(βn′nm0), where α = (8M
7)/M and

Xi, Xc

M

}

{

,

0) +

−

(αnm2
O
β = (4M

O
3)/M .

−

f (x) = 5x2 sin(12x) + (x3
+ 4 cos(2x) + ǫ,

−

0.5) sin(3x

0.5)

−

(17)

where ǫ
existing aggregation models.

∼ N

(0, 0.25), to illustrate the characteristics of

×

×

−

M

104, 105, 5

We generate n = 104, 5
105 and 106 train-
ing points, respectively, in [0, 1], and select n′ = 0.1n test
0.2, 1.2]. We pre-normalize each col-
points randomly in [
umn of X and y to zero mean and unit variance. Due to
the global expert
c in GRBCM, we slightly modify the
disjoint partition: we ﬁrst generate a random subset and
then use the k-means technique to generate M
1 dis-
joint subsets. Each expert is assigned with m0 = 500 data
points. We implement the aggregations by the GPML tool-
box5 using the SE kernel in (1) and the conjugate gradi-
ents algorithm with the maximum number of evaluations
as 500, and execute the code on a workstation with four
3.70 GHz cores and 16 GB RAM (multi-core computing
in Matalb is employed). Finally, we use the Standard-
ized Mean Square Error (SMSE) to evaluate the accuracy
of prediction mean, and the Mean Standardized Log Loss
(MSLL) to quantify the quality of predictive distribution
(Rasmussen & Williams, 2006).

−

(a)

106

]
s
[
 
e
m

i
t
 
g
n
i
t
c
d
e
r
P

i

104

102

100

10-2

104

(b)

E
S
M
S

100

10-1

105
Training size

106

104

105
Training size

106

PoE
GPoE
BCM
RBCM
NPAE
GRBCM
Training time

(c)

L
L
S
M

1.5

0.5

2

1

0

-0.5

-1

-1.5

104

105
Training size

106

Figure2. Comparison of different aggregation models on the toy
example in terms of (a) computing time, (b) SMSE and (c) MSLL.

Fig. 2 depicts the comparative results of six aggregation
models on the toy example. Note that NPAE using n >
104 is unavailable due to the time-consuming predic-
5
tion process. Fig. 2(a) shows that these models require the
same training time, but they differ in the predicting time.

×

5http://www.gaussianprocess.org/gpml/code/matlab/doc/

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

Due to the communication expert, the GRBCM’s predict-
ing time slightly offsets the curves of (G)PoE and (R)BCM.
The NPAE however exhibits signiﬁcantly larger predicting
time with increasing M and n′. Besides, Fig. 2(b) and
(c) reveal that GRBCM and NPAE yield better predictions
with increasing n, which conﬁrm their consistency when
.6 As for NPAE, though performing slightly better
n
104, it requires several orders
than GRBCM using n = 5
of magnitude larger predicting time, rendering it unsuitable
for cases with many test points and subsets.

→ ∞

×

4.2. Medium-scale datasets

We use two realistic datasets, kin40k (8D, 104 train-
104 test points) (Seeger et al., 2003) and
ing points, 3
sarcos (21D, 44484 training points, 4449 test points)
(Rasmussen & Williams, 2006), to assess the performance
of our approach.

×

The comparison includes all the aggregations except the
expensive NPAE.8 Besides, we employ the fully indepen-
dent training conditional (FITC) (Snelson & Ghahramani,
2006), the GP using stochastic variational inference (SVI)9
(Hensman et al., 2013), and the subset-of-data (SOD)
(Chalupka et al., 2013) for comparison. We select the in-
ducing size m for FITC and SVI, the batch size mb for SVI,
and the subset size msod for SOD, such that the computing
time is similar to or a bit larger than that of GRBCM. Partic-
ularly, we choose m = 200, mb = 0.1n and msod = 2500
for kin40k, and m = 300, mb = 0.1n and msod = 3000 for
sarcos. Differently, SVI employs the stochastic gradients
algorithm with tsg = 1200 iterations. Finally, we adopt the
disjoint partition used before to divide the kin40k dataset
into 16 subsets, and the sarcos dataset into 72 subsets for
the aggregations. Each experiment is repeated ten times.

kin40k

kin40k

(a)

0.1

0.08

E
S
M
S

0.06

0.04

0.02

60

(c)

0.04

0.03

E
S
M
S

0.02

0.01

(b)

-0.5

-1

-1.5

L
L
S
M

-2

60

3

2

1

0

-1

-2

L
L
S
M

×

Figure3. Illustrations of the aggregation models on the toy exam-
ple. The green “+” symbols represent the 104 data points. The
shaded area indicates 99% conﬁdence intervals of the full GP pre-
dictions using n = 104.
Fig. 3 illustrates the six aggregation models using n = 104
105, respectively, in comparison to the full
and n = 5
GP (ground truth) using n = 104.7 It is observed that in
terms of prediction mean, as discussed in remark 1, PoE
and GPoE provide poorer results in the entire domain with
increasing n. On the contrary, BCM and RBCM provide
good predictions in the range [0, 1]. As discussed in re-
mark 2, BCM however yields unreliable predictions when
leaving the training data. RBCM alleviates the issue by
using a varying βi. In terms of prediction variance, with
increasing n, PoE and (R)BCM tend to shrink to zero (over-
conﬁdent), while GPoE tends to approach σ2
∗∗ (too con-
servative). Particularly, PoE always has the largest MSLL
value in Fig. 2(b), since as discussed in remark 2, its pre-
diction variance approaches zero faster.

6Further discussions of GRBCM is shown in Appendix D.
7The full GP is intractable using our computer for n = 5 ×

105.

70

80
Computing time [s]

90

100

70

80
Computing time [s]

90

100

sarcos

sarcos

(d)

SVI
FITC
SOD
PoE
GPoE
BCM
RBCM
GRBCM

0
350

400

450
550
500
Computing time [s]

600

650

450
550
500
Computing time [s]

600

650

-3
350

400

Figure4. Comparison of the approximation models on the kin40k
and sarcos datasets.

Fig. 4 depicts the comparative results of different approx-
imation models over 10 runs on the kin40k and sarcos
datasets. The horizontal axis represents the sum of train-
ing and predicting time. It is ﬁrst observed that GRBCM
provides the best performance on the two datasets in terms
of both SMSE and MSLL at the cost of requiring a bit
more computing time than (G)PoE and (R)BCM. As for
(R)BCM, the small SMSE values reveal that they provide
better prediction mean than FITC and SOD; but the large
MSLL values again conﬁrm that they provide overconﬁ-

8The comparison of NPAE and GRBCM are separately pro-

vided in Appendix E.

9https://github.com/SheffieldML/GPy

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

dent prediction variance. As for (G)PoE, they suffer from
poor prediction mean, as indicated by the large SMSE; but
GPoE performs well in terms of MSLL. Finally, the simple
SOD outperforms FITC and SVI on the kin40k dataset, and
performs similarly on the sarcos dataset, which are consis-
tent with the ﬁndings in (Chalupka et al., 2013).

kin40k

kin40k

(a)

E
S
M
S

10-1

10-2

0

(c)

E
S
M
S

10-1

10-2

(a)

E
S
M
S

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

(c)

0.035

E
S
M
S

0.03

0.025

0.02

0.015

0.01

0.005

0

20
60
40
Number of experts
sarcos

80

-5

0

20
60
40
Number of experts
sarcos

80

(b)

15

L
L
S
M

10

5

0

(d)

40

L
L
S
M

30

20

10

0

-10

0

PoE
GPoE
BCM
RBCM
GRBCM

0

100

200

300

100

200

300

Number of experts

Number of experts

Figure5. Comparison of the aggregation models using different
numbers of experts on the kin40k and sarcos datasets.

Next, we explore the impact of the number M of experts
on the performance of aggregations. To this end, we run
them on the kin40k dataset with M respectively being 8,
16 and 64, and we run on the sarcos dataset with M respec-
tively being 36, 72 and 288. The results in Fig. 5 turn out
that all the aggregations perform worse with increasing M ,
since the experts become weaker; but GRBCM still yields
the best performance with different M . Besides, with in-
creasing M , the poor prediction mean and the vanishing
prediction variance of PoE result in the sharp increase of
MSLL values.

kin40k

kin40k

(b)

2.5

L
L
S
M

(d)

L
L
S
M

1.5

0.5

2

1

0

-0.5

-1

-1.5

-2

60

50

40

30

20

10

0

PoE

GPoE

BCM RBCM GRBCM

PoE

GPoE

BCM RBCM GRBCM

sarcos

sarcos

disjoint
random

PoE

GPoE

BCM RBCM GRBCM

PoE

GPoE

BCM RBCM GRBCM

Figure6. Comparison of the aggregation models using disjoint
and random partitions on the kin40k dataset (M = 16) and the
sarcos dataset (M = 72).

Table1. Comparative results of the aggregation models and SVI
on the song and electric datasets.

song (450K)
SMSE MSLL

electric (1.8M)
SMSE MSLL

0.8527
POE
0.8527
GPOE
2.6919
BCM
1.3383
RBCM
SVI
0.7909
GRBCM 0.7321

328.82
0.1159
156.62
24.930
-0.1885
-0.1571

0.1632
0.1632
0.0073
0.0027
0.0042
0.0024

1040.3
24.940
51.081
85.657
-1.1410
-1.3161

or random) on the performance of aggregations. The av-
erage results in Fig. 6 turn out that the disjoint partition
is more beneﬁcial for the aggregations. The results are
expectable since the disjoint subsets provide separate and
reﬁned local information, whereas the random subsets pro-
vide overlapping and coarse global information. But we ob-
serve that GPoE performs well on the sarcos dataset using
random partition, which conﬁrms the conclusions in Propo-
sition 2. Besides, as revealed in remark 3, even using ran-
dom partition, GRBCM outperforms GPoE.

4.3. Large-scale datasets

This section explores the performance of aggregations and
SVI on two large-scale datasets. We ﬁrst assess them on
the 90D song dataset, which is a subset of the million song
dataset (Bertin-Mahieux et al., 2011). The song dataset
is partitioned into 450000 training points and 65345 test
points. We then assess the models on the 11D electric
dataset that is partitioned into 1.8 million training points
and 249280 test points. We follow the normalization and
data pre-processing in (Wilson et al., 2016) to generate the
two datasets.10 For the song dataset, we use the foregoing
disjoint partition to divide it into M = 720 subsets, and
use m = 800, mb = 5000 and tsg = 1300 for SVI; for the
electric dataset, we divide it into M = 2880 subsets, and
use m = 1000, mb = 5000 and tsg = 1500 for SVI. As a
result, each expert is assigned with m0 = 625 data points
for the aggregations.

Table 1 reveals that the (G)PoE’s SMSE value is smaller
than that of (R)BCM on the song dataset. The poor pre-
diction mean of BCM is caused by the fact that the song
dataset is highly clustered such that BCM suffers from
weak experts in regions with scarce points. On the contrary,
due to the almost uniform distribution of the electric data
points, the (R)BCM’s SMSE is much smaller than that of
(G)PoE. Besides, unlike the vanishing prediction variances
of PoE and (R)BCM when n
, GPoE provides conser-
vative prediction variance, resulting in small MSLL values
on the two datasets. The proposed GRBCM always outper-

→ ∞

10The datasets and the pre-processing scripts are available in

Finally, we investigate the impact of data partition (disjoint

https://people.orie.cornell.edu/andrew/.

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

forms the other aggregations in terms of both SMSE and
MSLL on the two datasets due to the consistency. Finally,
GRBCM performs similarly to SVI on the song dataset; but
GRBCM outperforms SVI on the electric dataset.

Chalupka, Krzysztof, Williams, Christopher KI, and Mur-
ray, Iain. A framework for evaluating approximation
methods for Gaussian process regression. Journal of Ma-
chine Learning Research, 14(Feb):333–350, 2013.

5. Conclusions

To scale the standard GP to large-scale regression, we
present the GRBCM aggregation model, which introduces
a global communication expert to yield consistent yet ef-
ﬁcient predictions when n
. Through theoretical
and empirical analyses, we demonstrated the superiority of
GRBCM over existing aggregations on datasets with up to
1.8M training points.

→ ∞

The superiority of local experts is the capability of captur-
ing local patterns. Hence, further works will consider the
experts with individual hyperparameters in order to capture
non-stationary and heteroscedastic features.

Acknowledgements

This work was conducted within the Rolls-Royce@NTU
from the National Re-
Corporate Lab with support
search Foundation (NRF) Singapore under
the Corp
Lab@University Scheme. It is also partially supported by
the Data Science and Artiﬁcial Intelligence Research Cen-
ter (DSAIR) and the School of Computer Science and En-
gineering at Nanyang Technological University.

References

Alvarez, Mauricio A, Rosasco, Lorenzo, Lawrence, Neil D,
et al. Kernels for vector-valued functions: A review.
Foundations and Trends R
in Machine Learning, 4(3):
(cid:13)
195–266, 2012.

Bauer, Matthias, van der Wilk, Mark, and Rasmussen,
Carl Edward. Understanding probabilistic sparse Gaus-
sian process approximations. In Advances in Neural In-
formation Processing Systems, pp. 1533–1541. Curran
Associates, Inc., 2016.

Choi, Taeryon and Schervish, Mark J. Posterior consis-
tency in nonparametric regression problems under Gaus-
sian process priors. Technical report, Carnegie Mellon
University, 2004.

Deisenroth, Marc Peter and Ng, Jun Wei. Distributed Gaus-
sian processes. In International Conference on Machine
Learning, pp. 1481–1490. PMLR, 2015.

Fu, Yifan, Zhu, Xingquan, and Li, Bin. A survey on in-
stance selection for active learning. Knowledge and In-
formation Systems, 35(2):249–283, 2013.

Gal, Yarin, van der Wilk, Mark, and Rasmussen, Carl Ed-
ward. Distributed variational inference in sparse Gaus-
sian process regression and latent variable models.
In
Advances in Neural Information Processing Systems, pp.
3257–3265. Curran Associates, Inc., 2014.

Genest, Christian and Zidek, James V. Combining proba-
bility distributions: A critique and an annotated bibliog-
raphy. Statistical Science, 1(1):114–135, 1986.

Hensman, James, Fusi, Nicol`o, and Lawrence, Neil D.
Gaussian processes for big data. In Proceedings of the
29th Conference on Uncertainty in Artiﬁcial Intelligence,
pp. 282–290. AUAI Press, 2013.

Hinton, Geoffrey E. Training products of experts by mini-
mizing contrastive divergence. Neural Computation, 14
(8):1771–1800, 2002.

Hoang, Trong Nghia, Hoang, Quang Minh, and Low, Bryan
Kian Hsiang. A distributed variational inference frame-
work for unifying parallel sparse Gaussian process re-
In International Conference on Ma-
gression models.
chine Learning, pp. 382–391. PMLR, 2016.

Bertin-Mahieux, Thierry, Ellis, Daniel PW, Whitman,
Brian, and Lamere, Paul. The million song dataset. In
ISMIR, pp. 1–6, 2011.

Ionescu, Radu Cristian. Revisiting large scale distributed
arXiv preprint arXiv:1507.01461,

machine learning.
2015.

Bui, Thang D and Turner, Richard E. Tree-structured Gaus-
sian process approximations. In Advances in Neural In-
formation Processing Systems, pp. 2213–2221. Curran
Associates, Inc., 2014.

Lawrence, Neil. Probabilistic non-linear principal compo-
nent analysis with Gaussian process latent variable mod-
Journal of Machine Learning Research, 6(Nov):
els.
1783–1816, 2005.

Cao, Yanshuai and Fleet, David J. Generalized product of
experts for automatic and principled fusion of Gaussian
arXiv preprint arXiv:1410.7827,
process predictions.
2014.

Liu, Haitao, Cai, Jianfei, and Ong, Yew-Soon. An adaptive
sampling approach for Kriging metamodeling by maxi-
mizing expected prediction error. Computers & Chemi-
cal Engineering, 106(Nov):171–182, 2017.

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

Liu, Haitao, Cai, Jianfei, and Ong, Yew-Soon. Remarks on
multi-output Gaussian process regression. Knowledge-
Based Systems, 144(March):102–121, 2018.

Moore, David and Russell, Stuart J. Gaussian process
random ﬁelds. In Advances in Neural Information Pro-
cessing Systems, pp. 3357–3365. Curran Associates, Inc.,
2015.

Park, Chiwoo, Huang, Jianhua Z, and Ding, Yu. Domain
decomposition approach for fast Gaussian process re-
gression of large spatial data sets. Journal of Machine
Learning Research, 12(May):1697–1728, 2011.

Peng, Hao, Zhe, Shandian, Zhang, Xiao, and Qi, Yuan.
Asynchronous distributed variational Gaussian process
for regression. In International Conference on Machine
Learning, pp. 2788–2797. PMLR, 2017.

Qui˜nonero-Candela, Joaquin and Rasmussen, Carl Edward.
A unifying view of sparse approximate Gaussian process
regression. Journal of Machine Learning Research, 6
(Dec):1939–1959, 2005.

Ranjan, Roopesh and Gneiting, Tilmann. Combining prob-
ability forecasts. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 72(1):71–91, 2010.

Rasmussen, Carl E and Ghahramani, Zoubin. Inﬁnite mix-
tures of Gaussian process experts. In Advances in Neu-
ral Information Processing Systems, pp. 881–888. Cur-
ran Associates, Inc., 2002.

Rasmussen, Carl Edward and Williams, Christopher K. I.
Gaussian processes for machine learning. MIT Press,
2006.

Rulli`ere, Didier, Durrande, Nicolas, Bachoc, Franc¸ois, and
Chevalier, Cl´ement. Nested Kriging predictions for
datasets with a large number of observations. Statistics
and Computing, pp. 1–19, 2017.

Seeger, Matthias, Williams, Christopher, and Lawrence,
Neil. Fast forward selection to speed up sparse Gaussian
process regression. In Artiﬁcial Intelligence and Statis-
tics, pp. EPFL–CONF–161318. PMLR, 2003.

Shahriari, Bobak, Swersky, Kevin, Wang, Ziyu, Adams,
Ryan P, and de Freitas, Nando. Taking the human out
of the loop: A review of Bayesian optimization. Pro-
ceedings of the IEEE, 104(1):148–175, 2016.

Snelson, Edward and Ghahramani, Zoubin. Sparse Gaus-
In Advances in
sian processes using pseudo-inputs.
Neural Information Processing Systems, pp. 1257–1264.
MIT Press, 2006.

Snelson, Edward and Ghahramani, Zoubin. Local and
global sparse Gaussian process approximations. In Ar-
tiﬁcial Intelligence and Statistics, pp. 524–531. PMLR,
2007.

Tavassolipour, Mostafa, Motahari, Seyed Abolfazl, and
Shalmani, Mohammad-Taghi Manzuri.
Learning of
Gaussian processes in distributed and communication
arXiv preprint arXiv:1705.02627,
limited systems.
2017.

Titsias, Michalis K. Variational learning of inducing vari-
ables in sparse Gaussian processes. In Artiﬁcial Intelli-
gence and Statistics, pp. 567–574. PMLR, 2009.

Tresp, Volker. A Bayesian committee machine. Neural

Computation, 12(11):2719–2741, 2000.

Vazquez, Emmanuel and Bect, Julien. Pointwise consis-
tency of the Kriging predictor with known mean and
In 9th International Workshop
covariance functions.
in Model-Oriented Design and Analysis, pp. 221–228.
Springer, 2010.

Wilson, Andrew and Nickisch, Hannes. Kernel interpola-
tion for scalable structured Gaussian processes (KISS-
GP). In International Conference on Machine Learning,
pp. 1775–1784. PMLR, 2015.

Wilson, Andrew Gordon, Hu, Zhiting, Salakhutdinov, Rus-
lan, and Xing, Eric P. Deep kernel learning. In Artiﬁcial
Intelligence and Statistics, pp. 370–378. PMLR, 2016.

Yuan, Chao and Neubauer, Claus. Variational mixture of
Gaussian process experts. In Advances in Neural Infor-
mation Processing Systems, pp. 1897–1904. Curran As-
sociates, Inc., 2009.

A. Proof of Proposition 1

With disjoint partition, we consider two extreme local GP
Mn),
experts. For the ﬁrst extreme expert
the test point x∗ falls into the local region deﬁned by Xan ,
i.e., x∗ is adherent to Xan when n
. Hence, we have
(Vazquez & Bect, 2010)

an (1

→ ∞

M

an

≤

≤

lim
n→∞

σ2
an (x∗) = lim
n→∞

ǫ,n = σ2
σ2
η.

M

bn , it lies farthest away
For the other extreme expert
from x∗ such that the related prediction variance σ2
bn (x∗)
is closest to σ2
= an)
∗∗. It is known that for any
where x∗ is away from the training data Xi, given the
∀x∈Xi, we have
relative distance ri = min
k
limri→∞ σ2
∗∗. Since, however, we here focus
[0, 1]d
on the GP predictions in the bounded region Ω

i (x∗) = σ2

i (i

M

x∗

−

x

k

∈

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

and employ the covariance function k(.) > 0, then the pos-
is small but satisﬁes
itive sequence cn =
{
limn→∞ cn > 0 and

σ−2
∗∗ }

σ−2
bn

(x∗)

−

points. Hence, we have

σ−2
i

(x∗)

σ−2
∗∗ ≥

−

cn, 1

i

= an

Mn.

≤

≤

The equality holds only when i = bn.

Thereafter, with the sequence ǫn = min
{
0 where α > 0 we have

cn, 1
M α

n→∞

n } →

σ−2
i

(x∗)

σ−2
∗∗ ≥

−

cn

≥

ǫn, 1

i

= an

Mn.

≤

≤

It is found that cn = ǫn is possible to hold only when Mn
is small. With the increase of n, ǫn quickly becomes much
smaller than cn since limn→∞ 1/M α

n = 0.

The typical aggregated prediction variance writes

σ−2
A,n(x∗) =

βi(σ−2
i

(x∗)

∗∗ ) + σ−2
σ−2
∗∗ ,

(18)

−

Mn

i=1
X

where for (G)PoE we remove the prior precision σ−2
∗∗ . We
prove below the inconsistency of (G)PoE and (R)BCM us-
ing disjoint partition.

Mn
i=1 σ−2
(x∗) > Mnσ−2
For PoE, (18) is
leading to the inconsistent variance limn→∞ σ2
(R)BCM, the ﬁrst term of σ−2
that n is large enough,

,
n→∞
∞
A,n = 0. For
A,n(x∗) in (18) satisﬁes, given

∗∗ →

P

i

Mn

i=1
X

βi(σ−2
i

(x∗)

σ−2
∗∗ ) > ǫn

βi =

−

Mn

i=1
X

1
M α
n

Mn

i=1
X

βi.

Taking βi = 1 for BCM and α = 0.5, we have
, leading to the incon-
n→∞
A,n = 0. For RBCM, since

Mn
1
i=1 βi = √Mn
M α
→
n
sistent variance limn→∞ σ2

∞

P

βi = 0.5(log σ2

log σ2

i (x∗))

0.5 log(1 + cnσ2

∗∗)

∗∗ −

≥

where the equality holds only when i = bn, we have
, lead-

Mn
1
i=1 βi > 0.5 log(1 + cnσ2
n→∞
M α
→
n
ing to the inconsistent variance limn→∞ σ2
A,n = 0.

∗∗)√Mn

∞

P

Finally, for GPoE, we know that when n
converges to σ−2
isfy cn + σ−2
1

an (x∗)
η ; but the other prediction precisions sat-
for
Mn, since x∗ is away from their training

(x∗) < σ−2

n→∞ σ−2

ǫ,n →

→ ∞

= an

σ−2
i

i

η

, σ−2

∗∗ ≤
≤

≤

lim
n→∞

= lim
n→∞

σ−2
η −
1
(cid:0)
Mn

σ−2
GPoE(x∗)

(cid:1)
σ−2
an (x∗)

σ−2
η −
Mn

i6=an (cid:0)
X
σ−2
η −
Mn

(cid:0)
1
Mn

(cid:0)
1
Mn

+ lim
n→∞

> lim
n→∞

1
Mn

+ lim
n→∞

σ−2
η −

i6=an (cid:0)
X
GPoE(x∗)

(cid:1)
σ−2
i

(x∗)

σ−2
η −

(cid:1)

σ−2
an (x∗)

(cid:1)
σ−2
ǫ,n(x∗)

= 0,

that σ2
GPoE(x∗) > σ2

which means
limn→∞ σ2
ﬁnd that limn→∞ σ−2
limn→∞ σ2

GPoE(x∗) < σ2

GPoE(x∗) > cn + σ−2
bn (x∗) < σ2

∗∗.

(cid:1)
inconsistent

is

since
η. Meanwhile, we easily
∗∗ , leading to

B. Proof of Proposition 2

With smoothness assumption and particularly distributed
noise (normal or Laplacian distribution), it has been proved
that the GP predictions would converge to the true predic-
(Choi & Schervish, 2004). Hence,
tions when n
given that the points in Xi are randomly selected without
replacement from X and ni = n/Mn
, we have

→ ∞

n→∞

lim
n→∞

µi(x∗) = µη(x∗), lim
n→∞

i (x∗) = σ2
σ2
η,

→

∞
1

i

Mn.

≤

≤

For the aggregated prediction variance, we have

Mn

i=1
X

lim
n→∞

σ−2
A,n(x∗) = lim

n→∞ "

βi(σ−2
i

(x∗)

∗∗ ) + σ−2
σ−2
∗∗

,

−

#

i

A,n(x∗) = limn→∞ Mnσ−2

where for (G)PoE we remove σ−2
1 and limn→∞ σ−2
(x∗) = σ−2
tent variance limn→∞ σ−2

∗∗ . For PoE, given βi =
η , we have the inconsis-
η =
. For GPoE, given βi = 1/Mn we have the consis-
∞
tent variance limn→∞ σ−2
η = σ−2
σ−2
η .
For BCM, given βi = 1 we have the inconsistent vari-
ance limn→∞ σ−2
σ−2
∗∗ ) +
σ−2
. Finally, for RBCM, given limn→∞ βi =
∗∗ ] =
β = 0.5 log(σ2
η), we have the inconsistent vari-
ance limn→∞ σ−2
σ−2
∗∗ ) +
σ−2
∗∗ ] =

∗∗/σ2
A,n(x∗) = limn→∞[Mnβ(σ−2

A,n(x∗) = limn→∞[Mn(σ−2

A,n(x∗) = Mn

η −

η −

1
Mn

∞

.
∞

Then, for the aggregated prediction mean we have

Mn

lim
n→∞

µA,n(x∗) = lim
n→∞

σ2
A,n(x∗)

βiσ−2
i

(x∗)µi(x∗).

i=1
X
For PoE, given βi = 1 and limn→∞ σ−2
1/Mn, we

i
consistent

have

the

(x∗)/σ−2
A,n(x∗) =
prediction mean

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

(x∗)/σ−2

limn→∞ µA,n(x∗) = µη(x∗).
For GPoE, given
βi = 1/Mn and limn→∞ σ−2
A,n(x∗) = 1, we
i
have the consistent prediction mean limn→∞ µA,n(x∗) =
µη(x∗).
For (R)BCM, given βi = β = 1 or
limn→∞ βi = β = 0.5 log(σ2
η), we have the
inconsistent prediction mean limn→∞ µA,n(x∗) =
limn→∞ βσ−2
∗∗ /Mn) =
aµη(x∗) where a = σ−2
1 and the
equality holds when σ2
η = 0.

∗∗ ) + σ−2
σ−2
σ−2
∗∗ )

η µη(x∗)/(β(σ−2

η −
η /(σ−2

∗∗/σ2

η −

≥

C. Proof of Proposition 3

→

n→∞

Given that the points in the communication subset
are randomly selected without replacement from
nc = n/Mn
µη(x∗) and limn→∞ σ2
for the expert
i,
D+i =
limn→∞ µ+i(x∗) = µη(x∗) and limn→∞ σ2
for 2

c
D
and
, we have limn→∞ µc(x∗) =
∞
c (x∗) = σ2
c. Likewise,
trained on the augmented dataset
with size n+i = 2n/Mn, we have
+i(x∗) = σ2
η

M+i
c
}
D

η for

M .

{D

M

D

i

≤

≤

We ﬁrst derive the upper bound of σ2
c (x∗). For the station-
ary covariance function k(.) > 0, when nc is large enough
we have (Vazquez & Bect, 2010)

σ2
c (x∗)

k(x∗, x∗)

≤

k2(x∗, x′)
k(x′, x′)

−

+ σ2

ǫ,n,

k

k

∈

−

x′

where x′
Xc is the nearest data point to x∗. It is known
x∗
is proportional
that the relative distance rc =
to the inverse of the training size nc, i.e., rc
1/nc =
n→∞ 0. Conventional stationary covariance func-
Mn/n
tions only relay on the relative distance (once the covari-
ance parameters have been determined) and decrease with
rc. Consequently, the prediction variance σ2
c (x∗) increases
with rc. Taking the SE covariance function in (1) for ex-
ample,11 when rc
,

0 we have, given l0 = min1≤i≤d

→

∝

li

{

}

→

σ2
c (x∗)

We clearly see from this inequality that when rc
η since limn→∞ σ2
c (x∗) goes to σ2
σ2
Then, we rewrite the precision of GRBCM in (14b) as,
given β2 = 1,

ǫ,n = σ2
η.

→

0,

GRBCM(x∗) = σ−2
σ−2

+2(x∗)+

βi

σ−2
+i (x∗)

σ−2
c (x∗)

.

Mn

i=3
X

(cid:0)

−

(cid:1)
(20)

11We take the SE kernel for example since conventional kernels,
e.g., the rational quadratic kernel and the Mat´ern class of kernels,
can reduce to the SE kernel under some conditions.

Compared to
c,
M+i is trained on a more dense dataset
M
c (x∗) for a large enough n.12
σ2
D+i, leading to σ2
+i(x∗)
Given (19) and σ2
+i(x∗) > σ2
ǫ,n, the weight βi satisﬁes, for
3

Mn,

≤

i

≤

0

≤

≤

βi =

log

1
2

1
2

<

1
2

σ2
c (x∗)
σ2
+i(x∗)
(cid:19)
c + σ2
ar2
ǫ,n
σ2
ǫ,n ! ≤

(cid:18)

 

log

(cid:18)
a
2σ2

ǫ,n

σ2
c (x∗)
σ2
ǫ,n (cid:19)

r2
c .

<

log

(21)

Besides, the precision discrepancy satisﬁes, for 3
Mn,

i

≤

≤

0

≤

σ−2
+i (x∗)

σ−2
c (x∗) = σ−2

c (x∗)

−

σ2
c (x∗)
σ2
+i(x∗) −

1

(cid:19)

<

1
σ2

a
σ2

ǫ,n

ǫ,n

(cid:18)
r2
c .

(22)
Hence, the second term in the right-hand side of (20) satis-
ﬁes

Mn

βi

σ−2
+i (x∗)

σ−2
c (x∗)

<

−

Mn

a2
2σ6

ǫ,n

r4
c ∝

M 5
n
n4 .

i=3
X
n > 0, we have limn→∞ n4/M 5

(cid:1)

n =

i=3
X

(cid:0)
Since limn→∞ n/M 2
, and furthermore,

∞

Mn

lim
n→∞

βi

σ−2
+i (x∗)

σ−2
c (x∗)

= 0.

(23)

−

(cid:0)

i=3
X
Substituting (23) and limn→∞ σ−2
we have a consistent prediction precision as
σ−2
GRBCM(x∗) = σ−2
η .

(cid:1)

+2 (x∗) = σ−2
η

lim
n→∞

into (20),

Similarly, we rewrite the GRBCM’s prediction mean in
(14a) as

µGRBCM(x∗) = σ2

GRBCM(x∗)

µ∆ + σ−2

+2(x∗)µ+2(x∗)

,
(24)
(cid:1)

(cid:0)

−

µ∆ =

βi

σ−2
+i (x∗)µ+i(x∗)

σ−2
c (x∗)µc(x∗)

.

(cid:0)

i=3
X
Let δmax = max3≤i≤Mn
0, we have

µ∆| ≤

|

Mn

βiσ−2
c

i=3
X
Eq.(21)
<

(cid:12)
(cid:12)
(cid:12)
ar2
(cid:12)
c
2σ4

ǫ,n

Mn

i=3
X

σ2
c (x∗)
+i(x∗) µ+i(x∗)
σ2

(cid:12)
(cid:12)
(cid:12)
σ2
c (x∗)
σ2
+i(x∗)

µ+i(x∗)

−

δmax →

n→∞ 0.

(cid:1)

n→∞

→

(25)

−

µc(x∗)
(cid:12)
(cid:12)
(cid:12)

µc(x∗)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

12The equality is possible to hold when we employ disjoint par-

tition for {Di}

Mn
i=2 and x∗ is away from Xi.

≤

<

σ2
f exp(

σ2
f −
σ2
f
c + σ2
r2
l2
0

−
ǫ,n = ar2

c /l2
r2

0) + σ2
ǫ,n

c + σ2

ǫ,n.

(19)

where

Mn

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

Substituting (25) into (24), we have the consistent predic-
tion mean as

lim
n→∞

µGRBCM(x∗) = µη(x∗).

D. Discussions of GRBCM on the toy example

It is observed that the proposed GRBCM showcases superi-
ority over existing aggregations on the toy example, which
is brought by the particularly designed aggregation struc-
ture: the global communication expert
c to capture the
long-term features of the target function, and the remaining
M
i=2 to reﬁne local predictions.
experts

M

{M+i

}

(a)

0.16

E
S
M
S

0.14

0.12

0.1

0.08

0.06

104

(b)

-1.35

-1.4

L
L
S
M

-1.45

-1.5

105
Training size

106

-1.55

104

105
Training size

106

Figure7. Comparative results of GRBCM and Mc on the toy ex-
ample.

M

To verify the capability of GRBCM, we compare it with
the pure global expert
c which relies on a random sub-
set Xc. Fig. 7 shows the comparative results of GRBCM
c on the toy example. It is found that with increas-
and
ing n, (i) GRBCM always outperforms
c because of the
beneﬁts brought by local experts; and (ii) the predictions of
c generally become poorer since it becomes intractable

M
to choose a good subset from the increasing dataset.

M

M

E. Experimental results of NPAE

Table 2 compares the results of GRBCM and NPAE over
10 runs on the kin40k dataset (M = 16) and the sarcos
dataset (M = 72) using disjoint partition. It is observed
that GRBCM performs slightly better than NPAE on the
kin40k dataset, and produces competitive results on the sar-
cos dataset. But in terms of the computing efﬁciency, since
NPAE needs to build and invert an M
M covariance ma-
trix at each test point, it requires much more running time,
especially for the sarcos dataset with M = 72.

×

Table2. Comparative results (mean and standard deviation) of
GRBCM and NPAE over 10 runs on the kin40k dataset (M = 16)
and the sarcos dataset (M = 72) using disjoint partition. The
computing time t for each model involves the training and pre-
dicting time.

kin40k

SMSE
MSLL
t [S]
sarcos

SMSE
MSLL
t [S]

GRBCM

NPAE

0.0223 ± 0.0005
-1.9927 ± 0.0177
78.1 ± 4.4
GRBCM

0.0246 ± 0.0007
-1.9565 ± 0.0170
2852.4 ± 16.7
NPAE

0.0074 ± 0.0002
-2.3681 ± 0.0242
445.6 ± 49.4

0.0054 ± 0.0001
-2.5900 ± 0.0068
26444.0 ± 1213.0

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian
Process Regression

8
1
0
2
 
n
u
J
 

3

 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
0
2
7
0
0
.
6
0
8
1
:
v
i
X
r
a

Haitao Liu 1 Jianfei Cai 2 Yi Wang 3 Yew-Soon Ong 2 4

Abstract

predictive distributions at test points.

In order to scale standard Gaussian process (GP)
regression to large-scale datasets, aggregation
models employ factorized training process and
then combine predictions from distributed ex-
perts. The state-of-the-art aggregation models,
however, either provide inconsistent predictions
or require time-consuming aggregation process.
We ﬁrst prove the inconsistency of typical aggre-
gations using disjoint or random data partition,
and then present a consistent yet efﬁcient aggre-
gation model for large-scale GP. The proposed
model inherits the advantages of aggregations,
e.g., closed-form inference and aggregation, par-
allelization and distributed computing. Further-
more, theoretical and empirical analyses reveal
that the new aggregation model performs better
due to the consistent predictions that converge
to the true underlying function when the training
size approaches inﬁnity.

1. Introduction

Gaussian process (GP) (Rasmussen & Williams, 2006) is
a well-known statistical learning model extensively used
in various scenarios, e.g., regression, classiﬁcation, opti-
mization (Shahriari et al., 2016), visualization (Lawrence,
2005), active learning (Fu et al., 2013; Liu et al., 2017) and
multi-task learning (Alvarez et al., 2012; Liu et al., 2018).
n
Given the training set X =
xi
i=1 and the observa-
{
n
tion set y =
i=1, as an approximation of the
R
}
{
underlying function η : Rd
R, GP provides informative

y(xi)

Rd

∈

∈

}

→

1Rolls-Royce@NTU Corporate Lab, Nanyang Technologi-
cal University, Singapore 637460 2School of Computer Science
and Engineering, Nanyang Technological University, Singapore
639798 3Applied Technology Group, Rolls-Royce Singapore, 6
Seletar Aerospace Rise, Singapore 797575 4Data Science and
Artiﬁcial Intelligence Research Center, Nanyang Technological
University, Singapore 639798. Correspondence to: Haitao Liu
<htliu@ntu.edu.sg>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

However, the most prominent weakness of the full GP is
that it scales poorly with the training size. Given n data
points, the time complexity of a standard GP paradigm
(n3) in the training process due to the inver-
scales as
O
(n2) in
sion of an n
the prediction process due to the matrix-vector operation.
This weakness conﬁnes the full GP to training data of size

n covariance matrix; it scales as

O

×

(104).

≪

The

sparse

approximations
2005)

O
To cope with large-scale regression,
various com-
putationally efﬁcient approximations have been pre-
reviewed in
sented.
(Qui˜nonero-Candela & Rasmussen,
employ m
(m
n) inducing points to summarize the whole training
data (Seeger et al., 2003; Snelson & Ghahramani, 2006;
2007; Titsias, 2009; Bauer et al., 2016), thus reducing the
(nm2) and the predict-
training complexity of full GP to
ing complexity to
(nm). The complexity can be further
reduced through distributed inference, stochastic varia-
tional inference or Kronecker structure (Hensman et al.,
2013; Gal et al.,
2015;
Hoang et al., 2016; Peng et al., 2017). A main draw-
the
back of sparse approximations, however,
representational capability is limited by the number of
inducing points (Moore & Russell, 2015). For example,
for a quick-varying function, the sparse approximations
need many inducing points to capture the local structures.
That is, this kind of scheme has not reduced the scaling of
the complexity (Bui & Turner, 2014).

2014; Wilson & Nickisch,

is that

O

O

The method exploited in this article belongs to the aggre-
gation models (Hinton, 2002; Tresp, 2000; Cao & Fleet,
2014; Deisenroth & Ng, 2015; Rulli`ere et al., 2017), also
known as consensus statistical methods (Genest & Zidek,
1986; Ranjan & Gneiting, 2010). This kind of scheme pro-
duces the ﬁnal predictions by the aggregation of M sub-
models (GP experts) respectively trained on the subsets
, thus distributing
=
{D
the computations to “local” experts. Particularly, due to
the product of experts, the aggregation scheme derives a
factorized marginal likelihood for efﬁcient training; and
then it combines the experts’ posterior distributions accord-
In comparison to
ing to a certain aggregation criterion.

M
i=1 of

Xi, yi

X, y

i =

}}

D

{

}

{

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

sparse approximations, the aggregation models (i) operate
directly on the full training data, (ii) require no additional
inducing or variational parameters and (iii) distribute the
computations on individual experts for straightforward par-
allelization (Tavassolipour et al., 2017), thus scaling them
In comparison to typ-
to arbitrarily large training data.
ical local GPs (Snelson & Ghahramani, 2007; Park et al.,
2011), the aggregations smooth out the ugly discontinu-
ity by the product of posterior distributions from GP ex-
perts. Note that the aggregation methods are different from
the mixture-of-experts (Rasmussen & Ghahramani, 2002;
Yuan & Neubauer, 2009), which suffers from intractable in-
ference and is mainly developed for non-stationary regres-
sion.

However, it has been pointed out (Rulli`ere et al., 2017) that
there exists a particular type of training data such that typ-
ical aggregations, e.g., product-of-experts (PoE) (Hinton,
2002; Cao & Fleet, 2014) and Bayesian committee ma-
chine (BCM) (Tresp, 2000; Deisenroth & Ng, 2015), can-
not offer consistent predictions, where “consistent” means
the aggregated predictive distribution can converge to the
true underlying predictive distribution when the training
size n approaches inﬁnity.

Particularly,

The major contributions of this paper are three-fold. We
ﬁrst prove the inconsistency of typical aggregation mod-
els, e.g., the overconﬁdent or conservative prediction vari-
ances illustrated in Fig. 3, using conventional disjoint or
random data partition. Thereafter, we present a consis-
tent yet efﬁcient aggregation model for large-scale GP
the proposed generalized ro-
regression.
bust Bayesian committee machine (GRBCM) selects a
global subset to communicate with the remaining sub-
sets, leading to the consistent aggregated predictive dis-
theo-
tribution derived under the Bayes rule.
retical and empirical analyses reveal that GRBCM out-
performs existing aggregations due to the consistent yet
efﬁcient predictions. We release the demo codes in
https://github.com/LiuHaiTao01/GRBCM.

Finally,

2. Aggregation models revisited

2.1. Factorized training

A GP usually places a probability distribution over the la-
(0, k(x, x′)), which is
tent function space as f (x)
deﬁned by the zero mean and the covariance k(x, x′). The
well-known squared exponential (SE) covariance function
is

∼ GP

k(x, x′) = σ2

f exp

1
2

 −

d

(xi

x′
i)2

−
l2
i

,

!

i=1
X
where σ2
f is an output scale amplitude, and li is an input
length-scale along the ith dimension. Given the noisy ob-
servation y(x) = f (x) + ǫ where the i.i.d. noise follows

|

i=1
Y
(0, Ki + σ2

(0, σ2

∼ N

ǫ ) and the training data

ǫ
(0, k(X, X) + σ2
likelihood p(y
represents the hyperparameters to be inferred.

, we have the marginal
ǫ I) where θ

X, θ) =

N

D

|

i

In order to train the GP on large-scale datasets, the aggrega-
tion models introduce a factorized training process. It ﬁrst
partitions the training set
,
}
i. In
1
data partition, we can assign the data points randomly to
the experts (random partition), or assign disjoint subsets
obtained by clustering techniques to the experts (disjoint
partition).
Ignoring the correlation between the experts

M , and then trains GP on

D
{
i as an expert

into M subsets

Xi, yi

i =

M

≤

≤

D

D

M
i=1 leads to the factorized approximation as

i

{M

}

M

p(y

X, θ)

|

≈

pi(yi

Xi, θi),

(2)

M

|
∈

∼ N

Xi, θi)

i. Note that for simplicity all

ǫ,iIi) with Ki =
where pi(yi
Rni×ni and ni being the training size
k(Xi, Xi)
of
the M GP ex-
perts in (2) share the same hyperparameters as θi = θ
(Deisenroth & Ng, 2015). The factorization (2) degener-
ates the full covariance matrix K = k(X, X) into a diag-
onal block matrix diag[K1,
≈
diag[K −1
M ]. Hence, compared to the full GP,
1 ,
the complexity of the factorized training process is reduced
to

, KM ], leading to K −1

0) given ni = m0 = n/M , 1

, K −1

(nm2

M .

· · ·

· · ·

i

≤

≤

O

Conditioned on the related subset
i, x∗)
bution pi(y∗

(µi(x∗), σ2

D

i, the predictive distri-
i (x∗)) of

i has1

|D
∼ N
µi(x∗) = kT
i∗[Ki + σ2
σ2
i (x∗) = k(x∗, x∗)

ǫ I]−1yi,
kT
i∗[Ki + σ2

−

M

(3a)

ǫ I]−1ki∗ + σ2

ǫ , (3b)

where ki∗ = k(Xi, x∗). Thereafter, the experts’ predic-
M
i=1 are combined by the following aggrega-
tions
tion methods to perform the ﬁnal predicting.

µi, σ2
i }

{

2.2. Prediction aggregation

The state-of-the-art aggregation methods include PoE
(Hinton, 2002; Cao & Fleet, 2014), BCM (Tresp, 2000;
Deisenroth & Ng, 2015), and nested pointwise aggregation
of experts (NPAE) (Rulli`ere et al., 2017).

For the PoE and BCM family, the aggregated prediction
mean and precision are generally formulated as

µA(x∗) = σ2

A(x∗)

βiσ−2
i

(x∗)µi(x∗),

(4a)

M

i=1
X

M

i=1
X

M

−

i=1
X

1Instead of using pi(f∗|Di, x∗) in (Deisenroth & Ng, 2015),
we here consider the aggregations in a general scenario where
each expert has all its belongings at hand.

(1)

σ−2
A (x∗) =

βiσ−2
i

(x∗) + (1

βi)σ−2
∗∗ ,

(4b)

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

where the prior variance σ2
a correction term to σ−2
family; and βi is the weight of the expert

∗∗ = k(x∗, x∗) + σ2

ǫ , which is
A , is only available for the BCM

i at x∗.

M

The predictions of the PoE family, which omit the prior
precision σ−2
∗∗ in (4b), are derived from the product of M
experts as

M

pA(y∗

, x∗) =

|D

pβi
i (y∗

i, x∗).

|D

(5)

i=1
Y
The original PoE (Hinton, 2002) employs the constant
weight βi = 1, resulting in the aggregated prediction vari-
ances that vanish with increasing M . On the contrary, the
generalized PoE (GPoE) (Cao & Fleet, 2014) considers a
varying βi = 0.5(log σ2
i (x∗)), which represents
the difference in the differential entropy between the prior
i, x∗), to weigh the con-
p(y∗
|
i at x∗. This varying βi brings the ﬂexi-
tribution of
bility of increasing or reducing the importance of experts
based on the predictive uncertainty. However, the vary-
ing βi may produce undesirable errors for GPoE. For in-
stance, when x∗ is far away from the training data such
that σ2

x∗) and the posterior p(y∗

σ2
∗∗, we have βi

0 and σ2

log σ2

i (x∗)

∗∗ −

M

|D

.
GPoE → ∞

→

→

The BCM family, which is opposite to the PoE family, ex-
x∗) when combin-
plicitly incorporates the GP prior p(y∗
j, BCM intro-
ing predictions. For two experts
M
duces a conditional independence assumption
y∗,
leading to the aggregated predictive distribution as

|
i and

j
⊥ D

i
D

M

|

pA(y∗

, x∗) =

|D

M

i (y∗

i=1 pβi
|D
pPi βi−1(y∗
|

i, x∗)
x∗)

.

(6)

Q
The original BCM (Tresp, 2000) employs βi = 1 but
its predictions suffer from weak experts when leaving the
data. Hence, inspired by GPoE, the robust BCM (RBCM)
(Deisenroth & Ng, 2015) uses a varying βi
to produce
robust predictions by reducing the weights of weak ex-
perts. When x∗ is far away from the training data X,
the correction term brought by the GP prior in (4b) helps
the (R)BCM’s prediction variance recover σ2
∗∗. However,
given M = 1,
the predictions of RBCM as well as
GPoE cannot recover the full GP predictions because usu-
ally β1 = 0.5(log σ2
log σ2
log σ2
f ull(x∗))

1(x∗)) = 0.5(log σ2

∗∗ −

∗∗ −

= 1.

To achieve computation gains, the above aggregations intro-
duce additional independence assumption for the experts’
predictions, which however is often violated in practice
and yields poor results. Hence, in the aggregation process,
NPAE (Rulli`ere et al., 2017) regards the prediction mean
µi(x∗) in (3a) as a random variable by assuming that yi has
not yet been observed, thus allowing for considering the co-
variances between the experts’ predictions. Thereafter, for
, µM , y∗]T, the covariances are
the random vector [µ1,

· · ·

derived as

cov[µi, y∗] = kT

cov[µi, µj] =

i,ǫ ki∗,

i∗K −1
kT
i∗K −1
kT
i∗K −1

(

i,ǫ Kij K −1
i,ǫ Kij,ǫK −1

j,ǫ kj∗,
j,ǫ kj∗,

i

= j,

i = j,

(7a)

(7b)

ǫ I,
where Kij = k(Xi, Xj)
Kj,ǫ = Kj + σ2
ǫ I, and Kij,ǫ = Kij + σ2
ǫ I. With these
covariances, a nested GP training process is performed to
derive the aggregated prediction mean and variance as

Rni×nj , Ki,ǫ = Ki + σ2

∈

µNPAE(x∗) = kT
A∗K −1
σ2
NPAE(x∗) = k(x∗, x∗)

A µ,

kT
A∗K −1

A kA∗ + σ2
ǫ ,

−

(8a)

(8b)

RM×M has K ij

RM×1 has the ith element as cov[µi, y∗],
where kA∗
∈
A = cov[µi, µj], and µ =
KA
∈
, µM (x∗)]T. The NPAE is capable of provid-
[µ1(x∗),
ing consistent predictions at the cost of implementing a
much more time-consuming aggregation because of the in-
version of KA at each test point.

· · ·

2.3. Discussions of existing aggregations

→ ∞

Though showcasing promising results (Deisenroth & Ng,
2015), given that n
and the experts are noise-free
GPs, (G)PoE and (R)BCM have been proved to be in-
consistent, since there exists particular triangular array of
data points that are dense in the input domain Ω such that
the prediction variances do not go to zero (Rulli`ere et al.,
2017).

Particularly, we further show below the inconsistency of
(G)PoE and (R)BCM using two typical data partitions (ran-
dom and disjoint partition) in the scenario where the obser-
vations are blurred with noise. Note that since GPoE us-
ing a varying βi may produce undesirable errors, we adopt
βi = 1/M as suggested in (Deisenroth & Ng, 2015). Now
the GPoE’s prediction mean is the same as that of PoE; but
the prediction variance blows up as M times that of PoE.

→ ∞

, let X

∈
[0, 1]d such that for any x

Rn×d be dense
Deﬁnition 1. When n
Ω we have
in Ω
∈
= 0. Besides, the underlying
limn→∞ min1≤i≤n
k
function to be approximated has true continuous response
µη(x) and true noise variance σ2
η.

xi

−

∈

x

k

Firstly, for the disjoint partition that uses clustering tech-
into disjoint local subsets
niques to partition the data
D
M
,
i=1, The proposition below reveals that when n
i
{D
→ ∞
PoE and (R)BCM produce overconﬁdent prediction vari-
ance that shrinks to zero; on the contrary, GPoE provides
conservative prediction variance.

}

Mn
Proposition 1. Let
i=1 be a disjoint partition of the
training data
i be
GP with zero mean and stationary covariance function

}
. Let the expert

i trained on

i
{D

M

D

D

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

∈

≤

k(.) > 0. We further assume that (i) limn→∞ Mn =
∞
and (ii) limn→∞ n/M 2
n > 0, where the second condition
implies that the subset size m0 = n/Mn and the num-
ber of experts Mn are comparable such that too weak ex-
perts are not preferred. Besides, from the second condition
, which implies that the experts
we have m0 →
become more informative with increasing n. Then, PoE
and (R)BCM produce overconﬁdent prediction variance at
x∗

Ω as

n→∞

∞

lim
n→∞

σ2
A,n(x∗) = 0,

(9)

whereas GPoE yields conservative prediction variance

σ2
η < lim
n→∞

σ2
A,n(x∗) < σ2

bn (x∗) < σ2

∗∗,

(10)

where σ2
bn

bn (x∗) is offered by the farthest expert
M
Mn) whose prediction variance is closet to σ2

bn (1
∗∗.

≤

i

M

and

i=1 σ−2

−2
σ
i
−2
P σ
i

The detailed proof is given in Appendix A. Moreover, we
have the following ﬁndings.
Remark 1. For the averaging σ−2
M
i=1

GPoE = 1
M
µi using disjoint partition, more
µ(G)PoE =
and more experts become relatively far away from x∗ when
, i.e., the prediction variances at x∗ approach σ2
n
∗∗
and the prediction means approach the prior mean µ∗∗.
Hence, empirically, when n
approaches σ2
bn
Remark 2. The BCM’s prediction variance is always larger
than that of PoE since

, and the µ(G)PoE approaches µ∗∗.

, the conservative σ2

→ ∞

→ ∞

GPoE

P

P

a∗ =

σ−2
PoE(x∗)
σ−2
BCM(x∗)

=

M

i=1 σ−2
(x∗)

i

(x∗)
(M

−

−

M

i=1 σ−2
P
i

> 1

1)σ−2
∗∗

P

→ ∞

for M > 1. This means σ2
PoE deteriorates faster to zero
when n
. Besides, it is observed that µBCM is a∗
times that of PoE, which alleviates the deterioration of pre-
. However, when x∗ is leaving
diction mean when n
σ−2
X, a∗
∗∗ . That is why BCM
suffers from undesirable prediction mean when leaving X.

M since σ−2

→ ∞
i

(x∗)

→

→

Secondly, for the random partition that assigns the data
points randomly to the experts without replacement, The
proposition below implies that when n
, the predic-
tion variances of PoE and (R)BCM will shrink to zero;
the PoE’s prediction mean will recover µη(x), but the
(R)BCM’s prediction mean cannot; interestingly, the sim-
ple GPoE can converge to the underlying true predictive
distribution.

→ ∞

Mn
Proposition 2. Let
i=1 be a random partition of
i
{D
with (i) limn→∞ Mn =
and (ii)
the training data
Mn
limn→∞ n/M 2
i=1 be GPs with
zero mean and stationary covariance function k(.) > 0.

n > 0. Let the experts

{M

∞

D

}

}

i

Then, for the aggregated predictions at x∗

Ω we have

∈

lim
n→∞
lim
n→∞
lim
n→∞




where a = σ−2
when σ2

η = 0.

µPoE(x∗) = µη(x∗),

µGPoE(x∗) = µη(x∗),

lim
n→∞
lim
n→∞

σ2
PoE(x∗) = 0,
GPoE(x∗) = σ2
σ2
η,

µ(R)BCM(x∗) = aµη(x∗),

lim
n→∞

σ2
(R)BCM(x∗) = 0,

(11)

η /(σ−2

η −

σ−2
∗∗ )

≥

1 and the equality holds

The detailed proof is provided in Appendix B. Proposi-
tions 1 and 2 imply that no matter what kind of data par-
tition has been used, the prediction variances of PoE and
(R)BCM will shrink to zero when n
, which strictly
limits their usability since no beneﬁts can be gained from
such useless uncertainty information.

→ ∞

As for data partition, intuitively, the random partition pro-
vides overlapping and coarse global information about the
target function, which limits the ability to describe quick-
varying characteristics. On the contrary, the disjoint parti-
tion provides separate and reﬁned local information, which
enables the model to capture the variability of target func-
tion. The superiority of disjoint partition has been empiri-
cally conﬁrmed in (Rulli`ere et al., 2017). Therefore, unless
otherwise indicated, we employ disjoint partition for the
aggregation models throughout the article.

As for time complexity, the ﬁve aggregation models have
the same training process, and they only differ in how
to combine the experts’ predictions. For (G)PoE and
(R)BCM, their time complexity in prediction scales as
(n′nm0) where n′ is the number of test
O
points.2 For the complicated NPAE, it however needs to
M matrix KA at each test point, lead-
invert an M
ing to a greatly increased time complexity in prediction as

(nm2

0) +

O

×

(n′n2).3

O
The inconsistency of (G)PoE and (R)BCM and the ex-
tremely time-consuming process of NPAE impose the de-
mand of developing a consistent yet efﬁcient aggregation
model for large-scale GP regression.

3. Generalized robust Bayesian committee

machine

3.1. GRBCM

Our proposed GRBCM divides M experts into two groups.
The ﬁrst group has a global communication expert

c

M

2O(nm2

0) is induced by the update of M GP experts after op-

timizing hyperparameters.

3The predicting complexity of NPAE can be reduced by em-
ploying various hierarchical computing structure (Rulli`ere et al.,
2017), which however cannot provide identical predictions.

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

D
−

D1, and the second group con-
trained on the subset
c =
M
1 global or local experts4
tains the remaining M
i=2
M
trained on
i=2, respectively. The training process of
GRBCM is identical to that of typical aggregations in sec-
tion 2.1. The prediction process of GRBCM, however, is
different. Particularly, GRBCM assigns the global commu-
nication expert with the following properties:

i
{D

{M

}

}

i

with

•

•

(Random selection) The communication subset
c is
a random subset wherein the points are randomly se-
lected without replacement from
. It indicates that
the points in Xc spread over the entire domain, which
enables
c to capture the main features of the target
function. Note that there is no limit to the partition
type for the remaining M

1 subsets.

M

D

D

−

c, x∗)

M
∼ N

(Expert communication) The expert
c with pre-
(µc, σ2
c ) is
dictive distribution pc(y∗
|D
allowed to communicate with each of the remain-
It means we can utilize
ing experts
}
the augmented data
to improve
D+i =
i
}
D
over the base expert
c, leading to a new expert
M
M+i with the improved predictive distribution as
(µ+i, σ2
p+i(y∗

M
i=2.

{M

+i) for 2

M .

{D

c,

i

i

|D+i, x∗)

∼ N

≤

≤

•

(Conditional independence) Given the communica-
c and y∗, the independence assumption
tion subset
= j

D
c, y∗ holds for 2

M .

i

i
D

j
⊥ D

|D

≤

≤

Given the conditional independence assumption and the
M
i=2, we approximate the exact predictive dis-
weights
}
tribution p(y∗

, x∗) using the Bayes rule as

βi

{

|D

p(y∗

, x∗)

|D

M

i=2
Y
M

p(y∗

x∗)p(

y∗, x∗)

p(

i
D

j
|{D

}

i−1
j=1, y∗, x∗)

|

|

c
D

|

c
D

|

∝

≈

=

pβi(

i
D

|D

c, y∗, x∗)

p(y∗

x∗)p(

y∗, x∗)

p(y∗

x∗)
|
pPM

M
i=2 pβi(
c
D

i=2 βi−1(

Q

i=2
Y
D+i
|
y∗, x∗)
|

y∗, x∗)

.

(12)
c, y∗, x∗) is exact with no approximation

Note that p(
D2|D
in (12). Hence, we set β2 = 1.

With (12), GRBCM’s predictive distribution is

pA(y∗

, x∗) =

|D

M

+i(y∗

i=2 pβi
i=2 βi−1

pPM
Q
c

|D+i, x∗)
c, x∗)
(y∗
|D

.

(13)

4“Global” means the expert is trained on a random subset,

whereas “local” means it is trained on a disjoint subset.

µA(x∗) = σ2

A(x∗)

βiσ−2

+i (x∗)µ+i(x∗)

M

"

i=2
X

M

−  
i=2
X
M

i=2
X

βi

1

−

σ−2
c (x∗)µc(x∗)
#

,

!

(14a)

M

−  

i=2
X

βi

1

−

!

σ−2
c (x∗).

(14b)

σ−2
A (x∗) =

βiσ−2

+i (x∗)

c

rather than the prior σ−2

Different from (R)BCM, GRBCM employs the informa-
tive σ−2
∗∗ to correct the predic-
tion precision in (14b), leading to consistent predictions
when n
, which will be proved below. Also, the
prediction mean of GRBCM in (14a) now is corrected by
µc(x∗). Fig. 1 depicts the structure of the GRBCM aggre-
gation model.

→ ∞

Figure1. The GRBCM aggregation model.

In (14a) and (14b), the parameter βi (i > 2) akin to
that of RBCM is deﬁned as the difference in the dif-
ferential entropy between the base predictive distribution
c, x∗) and the enhanced predictive distribution
pc(y∗
|D
|D+i, x∗) as
p+i(y∗

i = 2,

βi =

1,
0.5(log σ2

(

c (x∗)

log σ2

+i(x∗)),

−

3

i

M.

≤

≤

D

≥

|D+i, x∗) over pc(y∗

(15)
2) into the
i (i
c, if there is little improvement of
c, x∗), we weak the vote of

It is found that after adding a subset
communication subset
D
p+i(y∗
M+i by assigning a small βi that approaches zero.
As for the size of Xc, more data points bring more infor-
mative
c and better GRBCM predictions at the cost of
higher computing complexity. In this article, we assign all
the experts with the same training size as nc = ni = m0
M .
and n+i = 2m0 for 2

M

|D

i

≤

≤

Next, we show that the GRBCM’s predictive distribution
will converge to the underlying true predictive distribution
.
when n
→ ∞
Proposition 3. Let
ing data
limn→∞ n/M 2

Mn
i=1 be a partition of the train-
i
}
{D
with (i)
and (ii)
limn→∞ Mn =
n > 0. Besides, among the M subsets, there

∞

D

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

c, the points in which
is a global communication subset
without replacement. Let
are randomly selected from
Mn
the global expert
i=2
be GPs with zero mean and stationary covariance function
k(.) > 0. Then, GRBCM yields consistent predictions as

c and the enhanced experts

{M+i

M

D

D

}

4. Numerical experiments

4.1. Toy example

We employ a 1D toy example

µGRBCM(x∗) = µη(x∗),

GRBCM(x∗) = σ2
σ2
η.

(16)

lim
n→∞
lim
n→∞






D

D\D

The detailed proof is provided in Appendix C. It is found
in Proposition 3 that apart from the requirement that the
communication subset
c should be a random subset, the
consistency of GRBCM holds for any partition of the re-
c. Besides, according to Propositions 2
maining data
and 3, both GPoE and GRBCM produce consistent pre-
dictions using random partition. It is known that the GP
model
provides more conﬁdent predictions, i.e., lower
M
σ2(x)dx, with more data points.
uncertainty U (
Since GRBCM trains experts on more informative subsets
{D+i
Remark 3. When using random subsets, the GRBCM’s
prediction uncertainty is always lower than that of GPoE,
since the discrepancy δU −1 = U −1
GPoE satisﬁes

R
M
i=2, we have the following ﬁnding.

U −1

) =

M

}

δU −1 =

U −1(

M+2)

−

"

Mn

+

βi

σ−2
+i (x∗)

Z

i=3
X

(cid:0)

GRBCM −
Mn
1
Mn

U −1(

i=1
X
σ−2
c (x∗)

−

i)

#

M

dx∗ > 0

(cid:1)

for a large enough n. It means compared to GPoE, GR-
BCM converges faster to the underlying function when
n

.
→ ∞

Finally, similar to RBCM, GRBCM can be executed in
multi-layer computing architectures with identical predic-
tions (Deisenroth & Ng, 2015; Ionescu, 2015), which allow
to run optimally and efﬁciently with the available comput-
ing infrastructure for distributed computing.

3.2. Complexity

i

≤

M
i=1 have the same train-
Assuming that the experts
i
}
{M
M . Compared to
ing size ni = m0 = n/M for 1
≤
(G)PoE and (R)BCM, the proposed GRBCM has a higher
time complexity in prediction due to the construction of
M
i=2. In prediction, it ﬁrst needs to cal-
new experts
{M+i
}
culate the inverse of k(Xc, Xc) and M
1 augmented
−
M
Xi, Xc
covariance matrices
i=2, which
)
k(
{
}
}
{
7m3
(8nm2
0), in order to obtain the predictions
scales as
0−
O
M
M
i=2 and σ2
σ2
c ,
µc,
i=2. Then, it combines the pre-
µ+i
+i}
{
}
{
i=2 at n′ test points. Therefore,
M
c and
dictions of
{M+i
}
the time complexity of the GRBCM prediction process is
(βn′nm0), where α = (8M
7)/M and

Xi, Xc

M

}

{

,

0) +

−

(αnm2
O
β = (4M

O
3)/M .

−

f (x) = 5x2 sin(12x) + (x3
+ 4 cos(2x) + ǫ,

−

0.5) sin(3x

0.5)

−

(17)

where ǫ
existing aggregation models.

∼ N

(0, 0.25), to illustrate the characteristics of

×

×

−

M

104, 105, 5

We generate n = 104, 5
105 and 106 train-
ing points, respectively, in [0, 1], and select n′ = 0.1n test
0.2, 1.2]. We pre-normalize each col-
points randomly in [
umn of X and y to zero mean and unit variance. Due to
the global expert
c in GRBCM, we slightly modify the
disjoint partition: we ﬁrst generate a random subset and
then use the k-means technique to generate M
1 dis-
joint subsets. Each expert is assigned with m0 = 500 data
points. We implement the aggregations by the GPML tool-
box5 using the SE kernel in (1) and the conjugate gradi-
ents algorithm with the maximum number of evaluations
as 500, and execute the code on a workstation with four
3.70 GHz cores and 16 GB RAM (multi-core computing
in Matalb is employed). Finally, we use the Standard-
ized Mean Square Error (SMSE) to evaluate the accuracy
of prediction mean, and the Mean Standardized Log Loss
(MSLL) to quantify the quality of predictive distribution
(Rasmussen & Williams, 2006).

−

(a)

106

]
s
[
 
e
m

i
t
 
g
n
i
t
c
d
e
r
P

i

104

102

100

10-2

104

(b)

E
S
M
S

100

10-1

105
Training size

106

104

105
Training size

106

PoE
GPoE
BCM
RBCM
NPAE
GRBCM
Training time

(c)

L
L
S
M

1.5

0.5

2

1

0

-0.5

-1

-1.5

104

105
Training size

106

Figure2. Comparison of different aggregation models on the toy
example in terms of (a) computing time, (b) SMSE and (c) MSLL.

Fig. 2 depicts the comparative results of six aggregation
models on the toy example. Note that NPAE using n >
104 is unavailable due to the time-consuming predic-
5
tion process. Fig. 2(a) shows that these models require the
same training time, but they differ in the predicting time.

×

5http://www.gaussianprocess.org/gpml/code/matlab/doc/

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

Due to the communication expert, the GRBCM’s predict-
ing time slightly offsets the curves of (G)PoE and (R)BCM.
The NPAE however exhibits signiﬁcantly larger predicting
time with increasing M and n′. Besides, Fig. 2(b) and
(c) reveal that GRBCM and NPAE yield better predictions
with increasing n, which conﬁrm their consistency when
.6 As for NPAE, though performing slightly better
n
104, it requires several orders
than GRBCM using n = 5
of magnitude larger predicting time, rendering it unsuitable
for cases with many test points and subsets.

→ ∞

×

4.2. Medium-scale datasets

We use two realistic datasets, kin40k (8D, 104 train-
104 test points) (Seeger et al., 2003) and
ing points, 3
sarcos (21D, 44484 training points, 4449 test points)
(Rasmussen & Williams, 2006), to assess the performance
of our approach.

×

The comparison includes all the aggregations except the
expensive NPAE.8 Besides, we employ the fully indepen-
dent training conditional (FITC) (Snelson & Ghahramani,
2006), the GP using stochastic variational inference (SVI)9
(Hensman et al., 2013), and the subset-of-data (SOD)
(Chalupka et al., 2013) for comparison. We select the in-
ducing size m for FITC and SVI, the batch size mb for SVI,
and the subset size msod for SOD, such that the computing
time is similar to or a bit larger than that of GRBCM. Partic-
ularly, we choose m = 200, mb = 0.1n and msod = 2500
for kin40k, and m = 300, mb = 0.1n and msod = 3000 for
sarcos. Differently, SVI employs the stochastic gradients
algorithm with tsg = 1200 iterations. Finally, we adopt the
disjoint partition used before to divide the kin40k dataset
into 16 subsets, and the sarcos dataset into 72 subsets for
the aggregations. Each experiment is repeated ten times.

kin40k

kin40k

(a)

0.1

0.08

E
S
M
S

0.06

0.04

0.02

60

(c)

0.04

0.03

E
S
M
S

0.02

0.01

(b)

-0.5

L
L
S
M

-1

-1.5

-2

60

(d)

3

2

1

0

-1

-2

L
L
S
M

×

Figure3. Illustrations of the aggregation models on the toy exam-
ple. The green “+” symbols represent the 104 data points. The
shaded area indicates 99% conﬁdence intervals of the full GP pre-
dictions using n = 104.
Fig. 3 illustrates the six aggregation models using n = 104
105, respectively, in comparison to the full
and n = 5
GP (ground truth) using n = 104.7 It is observed that in
terms of prediction mean, as discussed in remark 1, PoE
and GPoE provide poorer results in the entire domain with
increasing n. On the contrary, BCM and RBCM provide
good predictions in the range [0, 1]. As discussed in re-
mark 2, BCM however yields unreliable predictions when
leaving the training data. RBCM alleviates the issue by
using a varying βi. In terms of prediction variance, with
increasing n, PoE and (R)BCM tend to shrink to zero (over-
conﬁdent), while GPoE tends to approach σ2
∗∗ (too con-
servative). Particularly, PoE always has the largest MSLL
value in Fig. 2(b), since as discussed in remark 2, its pre-
diction variance approaches zero faster.

6Further discussions of GRBCM is shown in Appendix D.
7The full GP is intractable using our computer for n = 5 ×

105.

70

80
Computing time [s]

90

100

70

80
Computing time [s]

90

100

sarcos

sarcos

SVI
FITC
SOD
PoE
GPoE
BCM
RBCM
GRBCM

0
350

400

450
550
500
Computing time [s]

600

650

450
550
500
Computing time [s]

600

650

-3
350

400

Figure4. Comparison of the approximation models on the kin40k
and sarcos datasets.

Fig. 4 depicts the comparative results of different approx-
imation models over 10 runs on the kin40k and sarcos
datasets. The horizontal axis represents the sum of train-
ing and predicting time. It is ﬁrst observed that GRBCM
provides the best performance on the two datasets in terms
of both SMSE and MSLL at the cost of requiring a bit
more computing time than (G)PoE and (R)BCM. As for
(R)BCM, the small SMSE values reveal that they provide
better prediction mean than FITC and SOD; but the large
MSLL values again conﬁrm that they provide overconﬁ-

8The comparison of NPAE and GRBCM are separately pro-

vided in Appendix E.

9https://github.com/SheffieldML/GPy

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

dent prediction variance. As for (G)PoE, they suffer from
poor prediction mean, as indicated by the large SMSE; but
GPoE performs well in terms of MSLL. Finally, the simple
SOD outperforms FITC and SVI on the kin40k dataset, and
performs similarly on the sarcos dataset, which are consis-
tent with the ﬁndings in (Chalupka et al., 2013).

kin40k

kin40k

(a)

E
S
M
S

10-1

10-2

0

(c)

E
S
M
S

10-1

10-2

(a)

E
S
M
S

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

(c)

0.035

E
S
M
S

0.03

0.025

0.02

0.015

0.01

0.005

0

20
60
40
Number of experts
sarcos

80

-5

0

20
60
40
Number of experts
sarcos

80

(b)

15

L
L
S
M

10

5

0

(d)

40

L
L
S
M

30

20

10

0

-10

0

PoE
GPoE
BCM
RBCM
GRBCM

0

100

200

300

100

200

300

Number of experts

Number of experts

Figure5. Comparison of the aggregation models using different
numbers of experts on the kin40k and sarcos datasets.

Next, we explore the impact of the number M of experts
on the performance of aggregations. To this end, we run
them on the kin40k dataset with M respectively being 8,
16 and 64, and we run on the sarcos dataset with M respec-
tively being 36, 72 and 288. The results in Fig. 5 turn out
that all the aggregations perform worse with increasing M ,
since the experts become weaker; but GRBCM still yields
the best performance with different M . Besides, with in-
creasing M , the poor prediction mean and the vanishing
prediction variance of PoE result in the sharp increase of
MSLL values.

kin40k

kin40k

(b)

2.5

L
L
S
M

(d)

L
L
S
M

1.5

0.5

2

1

0

-0.5

-1

-1.5

-2

60

50

40

30

20

10

0

PoE

GPoE

BCM RBCM GRBCM

PoE

GPoE

BCM RBCM GRBCM

sarcos

sarcos

disjoint
random

PoE

GPoE

BCM RBCM GRBCM

PoE

GPoE

BCM RBCM GRBCM

Figure6. Comparison of the aggregation models using disjoint
and random partitions on the kin40k dataset (M = 16) and the
sarcos dataset (M = 72).

Table1. Comparative results of the aggregation models and SVI
on the song and electric datasets.

song (450K)
SMSE MSLL

electric (1.8M)
SMSE MSLL

0.8527
POE
0.8527
GPOE
2.6919
BCM
1.3383
RBCM
SVI
0.7909
GRBCM 0.7321

328.82
0.1159
156.62
24.930
-0.1885
-0.1571

0.1632
0.1632
0.0073
0.0027
0.0042
0.0024

1040.3
24.940
51.081
85.657
-1.1410
-1.3161

or random) on the performance of aggregations. The av-
erage results in Fig. 6 turn out that the disjoint partition
is more beneﬁcial for the aggregations. The results are
expectable since the disjoint subsets provide separate and
reﬁned local information, whereas the random subsets pro-
vide overlapping and coarse global information. But we ob-
serve that GPoE performs well on the sarcos dataset using
random partition, which conﬁrms the conclusions in Propo-
sition 2. Besides, as revealed in remark 3, even using ran-
dom partition, GRBCM outperforms GPoE.

4.3. Large-scale datasets

This section explores the performance of aggregations and
SVI on two large-scale datasets. We ﬁrst assess them on
the 90D song dataset, which is a subset of the million song
dataset (Bertin-Mahieux et al., 2011). The song dataset
is partitioned into 450000 training points and 65345 test
points. We then assess the models on the 11D electric
dataset that is partitioned into 1.8 million training points
and 249280 test points. We follow the normalization and
data pre-processing in (Wilson et al., 2016) to generate the
two datasets.10 For the song dataset, we use the foregoing
disjoint partition to divide it into M = 720 subsets, and
use m = 800, mb = 5000 and tsg = 1300 for SVI; for the
electric dataset, we divide it into M = 2880 subsets, and
use m = 1000, mb = 5000 and tsg = 1500 for SVI. As a
result, each expert is assigned with m0 = 625 data points
for the aggregations.

Table 1 reveals that the (G)PoE’s SMSE value is smaller
than that of (R)BCM on the song dataset. The poor pre-
diction mean of BCM is caused by the fact that the song
dataset is highly clustered such that BCM suffers from
weak experts in regions with scarce points. On the contrary,
due to the almost uniform distribution of the electric data
points, the (R)BCM’s SMSE is much smaller than that of
(G)PoE. Besides, unlike the vanishing prediction variances
of PoE and (R)BCM when n
, GPoE provides conser-
vative prediction variance, resulting in small MSLL values
on the two datasets. The proposed GRBCM always outper-

→ ∞

10The datasets and the pre-processing scripts are available in

Finally, we investigate the impact of data partition (disjoint

https://people.orie.cornell.edu/andrew/.

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

forms the other aggregations in terms of both SMSE and
MSLL on the two datasets due to the consistency. Finally,
GRBCM performs similarly to SVI on the song dataset; but
GRBCM outperforms SVI on the electric dataset.

Chalupka, Krzysztof, Williams, Christopher KI, and Mur-
ray, Iain. A framework for evaluating approximation
methods for Gaussian process regression. Journal of Ma-
chine Learning Research, 14(Feb):333–350, 2013.

5. Conclusions

To scale the standard GP to large-scale regression, we
present the GRBCM aggregation model, which introduces
a global communication expert to yield consistent yet ef-
ﬁcient predictions when n
. Through theoretical
and empirical analyses, we demonstrated the superiority of
GRBCM over existing aggregations on datasets with up to
1.8M training points.

→ ∞

The superiority of local experts is the capability of captur-
ing local patterns. Hence, further works will consider the
experts with individual hyperparameters in order to capture
non-stationary and heteroscedastic features.

Acknowledgements

This work was conducted within the Rolls-Royce@NTU
from the National Re-
Corporate Lab with support
search Foundation (NRF) Singapore under
the Corp
Lab@University Scheme. It is also partially supported by
the Data Science and Artiﬁcial Intelligence Research Cen-
ter (DSAIR) and the School of Computer Science and En-
gineering at Nanyang Technological University.

References

Alvarez, Mauricio A, Rosasco, Lorenzo, Lawrence, Neil D,
et al. Kernels for vector-valued functions: A review.
Foundations and Trends R
in Machine Learning, 4(3):
(cid:13)
195–266, 2012.

Bauer, Matthias, van der Wilk, Mark, and Rasmussen,
Carl Edward. Understanding probabilistic sparse Gaus-
sian process approximations. In Advances in Neural In-
formation Processing Systems, pp. 1533–1541. Curran
Associates, Inc., 2016.

Choi, Taeryon and Schervish, Mark J. Posterior consis-
tency in nonparametric regression problems under Gaus-
sian process priors. Technical report, Carnegie Mellon
University, 2004.

Deisenroth, Marc Peter and Ng, Jun Wei. Distributed Gaus-
sian processes. In International Conference on Machine
Learning, pp. 1481–1490. PMLR, 2015.

Fu, Yifan, Zhu, Xingquan, and Li, Bin. A survey on in-
stance selection for active learning. Knowledge and In-
formation Systems, 35(2):249–283, 2013.

Gal, Yarin, van der Wilk, Mark, and Rasmussen, Carl Ed-
ward. Distributed variational inference in sparse Gaus-
sian process regression and latent variable models.
In
Advances in Neural Information Processing Systems, pp.
3257–3265. Curran Associates, Inc., 2014.

Genest, Christian and Zidek, James V. Combining proba-
bility distributions: A critique and an annotated bibliog-
raphy. Statistical Science, 1(1):114–135, 1986.

Hensman, James, Fusi, Nicol`o, and Lawrence, Neil D.
Gaussian processes for big data. In Proceedings of the
29th Conference on Uncertainty in Artiﬁcial Intelligence,
pp. 282–290. AUAI Press, 2013.

Hinton, Geoffrey E. Training products of experts by mini-
mizing contrastive divergence. Neural Computation, 14
(8):1771–1800, 2002.

Hoang, Trong Nghia, Hoang, Quang Minh, and Low, Bryan
Kian Hsiang. A distributed variational inference frame-
work for unifying parallel sparse Gaussian process re-
In International Conference on Ma-
gression models.
chine Learning, pp. 382–391. PMLR, 2016.

Bertin-Mahieux, Thierry, Ellis, Daniel PW, Whitman,
Brian, and Lamere, Paul. The million song dataset. In
ISMIR, pp. 1–6, 2011.

Ionescu, Radu Cristian. Revisiting large scale distributed
arXiv preprint arXiv:1507.01461,

machine learning.
2015.

Bui, Thang D and Turner, Richard E. Tree-structured Gaus-
sian process approximations. In Advances in Neural In-
formation Processing Systems, pp. 2213–2221. Curran
Associates, Inc., 2014.

Lawrence, Neil. Probabilistic non-linear principal compo-
nent analysis with Gaussian process latent variable mod-
Journal of Machine Learning Research, 6(Nov):
els.
1783–1816, 2005.

Cao, Yanshuai and Fleet, David J. Generalized product of
experts for automatic and principled fusion of Gaussian
arXiv preprint arXiv:1410.7827,
process predictions.
2014.

Liu, Haitao, Cai, Jianfei, and Ong, Yew-Soon. An adaptive
sampling approach for Kriging metamodeling by maxi-
mizing expected prediction error. Computers & Chemi-
cal Engineering, 106(Nov):171–182, 2017.

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

Liu, Haitao, Cai, Jianfei, and Ong, Yew-Soon. Remarks on
multi-output Gaussian process regression. Knowledge-
Based Systems, 144(March):102–121, 2018.

Moore, David and Russell, Stuart J. Gaussian process
random ﬁelds. In Advances in Neural Information Pro-
cessing Systems, pp. 3357–3365. Curran Associates, Inc.,
2015.

Park, Chiwoo, Huang, Jianhua Z, and Ding, Yu. Domain
decomposition approach for fast Gaussian process re-
gression of large spatial data sets. Journal of Machine
Learning Research, 12(May):1697–1728, 2011.

Peng, Hao, Zhe, Shandian, Zhang, Xiao, and Qi, Yuan.
Asynchronous distributed variational Gaussian process
for regression. In International Conference on Machine
Learning, pp. 2788–2797. PMLR, 2017.

Qui˜nonero-Candela, Joaquin and Rasmussen, Carl Edward.
A unifying view of sparse approximate Gaussian process
regression. Journal of Machine Learning Research, 6
(Dec):1939–1959, 2005.

Ranjan, Roopesh and Gneiting, Tilmann. Combining prob-
ability forecasts. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 72(1):71–91, 2010.

Rasmussen, Carl E and Ghahramani, Zoubin. Inﬁnite mix-
tures of Gaussian process experts. In Advances in Neu-
ral Information Processing Systems, pp. 881–888. Cur-
ran Associates, Inc., 2002.

Rasmussen, Carl Edward and Williams, Christopher K. I.
Gaussian processes for machine learning. MIT Press,
2006.

Rulli`ere, Didier, Durrande, Nicolas, Bachoc, Franc¸ois, and
Chevalier, Cl´ement. Nested Kriging predictions for
datasets with a large number of observations. Statistics
and Computing, pp. 1–19, 2017.

Seeger, Matthias, Williams, Christopher, and Lawrence,
Neil. Fast forward selection to speed up sparse Gaussian
process regression. In Artiﬁcial Intelligence and Statis-
tics, pp. EPFL–CONF–161318. PMLR, 2003.

Shahriari, Bobak, Swersky, Kevin, Wang, Ziyu, Adams,
Ryan P, and de Freitas, Nando. Taking the human out
of the loop: A review of Bayesian optimization. Pro-
ceedings of the IEEE, 104(1):148–175, 2016.

Snelson, Edward and Ghahramani, Zoubin. Sparse Gaus-
In Advances in
sian processes using pseudo-inputs.
Neural Information Processing Systems, pp. 1257–1264.
MIT Press, 2006.

Snelson, Edward and Ghahramani, Zoubin. Local and
global sparse Gaussian process approximations. In Ar-
tiﬁcial Intelligence and Statistics, pp. 524–531. PMLR,
2007.

Tavassolipour, Mostafa, Motahari, Seyed Abolfazl, and
Shalmani, Mohammad-Taghi Manzuri.
Learning of
Gaussian processes in distributed and communication
arXiv preprint arXiv:1705.02627,
limited systems.
2017.

Titsias, Michalis K. Variational learning of inducing vari-
ables in sparse Gaussian processes. In Artiﬁcial Intelli-
gence and Statistics, pp. 567–574. PMLR, 2009.

Tresp, Volker. A Bayesian committee machine. Neural

Computation, 12(11):2719–2741, 2000.

Vazquez, Emmanuel and Bect, Julien. Pointwise consis-
tency of the Kriging predictor with known mean and
In 9th International Workshop
covariance functions.
in Model-Oriented Design and Analysis, pp. 221–228.
Springer, 2010.

Wilson, Andrew and Nickisch, Hannes. Kernel interpola-
tion for scalable structured Gaussian processes (KISS-
GP). In International Conference on Machine Learning,
pp. 1775–1784. PMLR, 2015.

Wilson, Andrew Gordon, Hu, Zhiting, Salakhutdinov, Rus-
lan, and Xing, Eric P. Deep kernel learning. In Artiﬁcial
Intelligence and Statistics, pp. 370–378. PMLR, 2016.

Yuan, Chao and Neubauer, Claus. Variational mixture of
Gaussian process experts. In Advances in Neural Infor-
mation Processing Systems, pp. 1897–1904. Curran As-
sociates, Inc., 2009.

A. Proof of Proposition 1

With disjoint partition, we consider two extreme local GP
Mn),
experts. For the ﬁrst extreme expert
the test point x∗ falls into the local region deﬁned by Xan ,
i.e., x∗ is adherent to Xan when n
. Hence, we have
(Vazquez & Bect, 2010)

an (1

→ ∞

M

an

≤

≤

lim
n→∞

σ2
an (x∗) = lim
n→∞

ǫ,n = σ2
σ2
η.

M

bn , it lies farthest away
For the other extreme expert
from x∗ such that the related prediction variance σ2
bn (x∗)
is closest to σ2
= an)
∗∗. It is known that for any
where x∗ is away from the training data Xi, given the
∀x∈Xi, we have
relative distance ri = min
k
limri→∞ σ2
∗∗. Since, however, we here focus
[0, 1]d
on the GP predictions in the bounded region Ω

i (x∗) = σ2

i (i

M

x∗

−

x

k

∈

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

and employ the covariance function k(.) > 0, then the pos-
is small but satisﬁes
itive sequence cn =
{
limn→∞ cn > 0 and

σ−2
∗∗ }

σ−2
bn

(x∗)

−

points. Hence, we have

σ−2
i

(x∗)

σ−2
∗∗ ≥

−

cn, 1

i

= an

Mn.

≤

≤

The equality holds only when i = bn.

Thereafter, with the sequence ǫn = min
{
0 where α > 0 we have

cn, 1
M α

n→∞

n } →

σ−2
i

(x∗)

σ−2
∗∗ ≥

−

cn

≥

ǫn, 1

i

= an

Mn.

≤

≤

It is found that cn = ǫn is possible to hold only when Mn
is small. With the increase of n, ǫn quickly becomes much
smaller than cn since limn→∞ 1/M α

n = 0.

The typical aggregated prediction variance writes

σ−2
A,n(x∗) =

βi(σ−2
i

(x∗)

∗∗ ) + σ−2
σ−2
∗∗ ,

(18)

−

Mn

i=1
X

where for (G)PoE we remove the prior precision σ−2
∗∗ . We
prove below the inconsistency of (G)PoE and (R)BCM us-
ing disjoint partition.

Mn
i=1 σ−2
(x∗) > Mnσ−2
For PoE, (18) is
leading to the inconsistent variance limn→∞ σ2
(R)BCM, the ﬁrst term of σ−2
that n is large enough,

,
n→∞
∞
A,n = 0. For
A,n(x∗) in (18) satisﬁes, given

∗∗ →

P

i

Mn

i=1
X

βi(σ−2
i

(x∗)

σ−2
∗∗ ) > ǫn

βi =

−

Mn

i=1
X

1
M α
n

Mn

i=1
X

βi.

Taking βi = 1 for BCM and α = 0.5, we have
, leading to the incon-
n→∞
A,n = 0. For RBCM, since

Mn
1
i=1 βi = √Mn
M α
→
n
sistent variance limn→∞ σ2

∞

P

βi = 0.5(log σ2

log σ2

i (x∗))

0.5 log(1 + cnσ2

∗∗)

∗∗ −

≥

where the equality holds only when i = bn, we have
, lead-

Mn
1
i=1 βi > 0.5 log(1 + cnσ2
n→∞
M α
→
n
ing to the inconsistent variance limn→∞ σ2
A,n = 0.

∗∗)√Mn

∞

P

Finally, for GPoE, we know that when n
converges to σ−2
isfy cn + σ−2
1

an (x∗)
η ; but the other prediction precisions sat-
for
Mn, since x∗ is away from their training

(x∗) < σ−2

n→∞ σ−2

ǫ,n →

→ ∞

= an

σ−2
i

i

η

, σ−2

∗∗ ≤
≤

≤

lim
n→∞

= lim
n→∞

σ−2
η −
1
(cid:0)
Mn

σ−2
GPoE(x∗)

(cid:1)
σ−2
an (x∗)

σ−2
η −
Mn

i6=an (cid:0)
X
σ−2
η −
Mn

(cid:0)
1
Mn

(cid:0)
1
Mn

+ lim
n→∞

> lim
n→∞

1
Mn

+ lim
n→∞

σ−2
η −

i6=an (cid:0)
X
GPoE(x∗)

(cid:1)
σ−2
i

(x∗)

σ−2
η −

(cid:1)

σ−2
an (x∗)

(cid:1)
σ−2
ǫ,n(x∗)

= 0,

that σ2
GPoE(x∗) > σ2

which means
limn→∞ σ2
ﬁnd that limn→∞ σ−2
limn→∞ σ2

GPoE(x∗) < σ2

GPoE(x∗) > cn + σ−2
bn (x∗) < σ2

∗∗.

(cid:1)
inconsistent

is

since
η. Meanwhile, we easily
∗∗ , leading to

B. Proof of Proposition 2

With smoothness assumption and particularly distributed
noise (normal or Laplacian distribution), it has been proved
that the GP predictions would converge to the true predic-
(Choi & Schervish, 2004). Hence,
tions when n
given that the points in Xi are randomly selected without
replacement from X and ni = n/Mn
, we have

→ ∞

n→∞

lim
n→∞

µi(x∗) = µη(x∗), lim
n→∞

i (x∗) = σ2
σ2
η,

→

∞
1

i

Mn.

≤

≤

For the aggregated prediction variance, we have

Mn

i=1
X

lim
n→∞

σ−2
A,n(x∗) = lim

n→∞ "

βi(σ−2
i

(x∗)

∗∗ ) + σ−2
σ−2
∗∗

,

−

#

i

A,n(x∗) = limn→∞ Mnσ−2

where for (G)PoE we remove σ−2
1 and limn→∞ σ−2
(x∗) = σ−2
tent variance limn→∞ σ−2

∗∗ . For PoE, given βi =
η , we have the inconsis-
η =
. For GPoE, given βi = 1/Mn we have the consis-
∞
tent variance limn→∞ σ−2
η = σ−2
σ−2
η .
For BCM, given βi = 1 we have the inconsistent vari-
ance limn→∞ σ−2
σ−2
∗∗ ) +
σ−2
. Finally, for RBCM, given limn→∞ βi =
∗∗ ] =
β = 0.5 log(σ2
η), we have the inconsistent vari-
ance limn→∞ σ−2
σ−2
∗∗ ) +
σ−2
∗∗ ] =

∗∗/σ2
A,n(x∗) = limn→∞[Mnβ(σ−2

A,n(x∗) = limn→∞[Mn(σ−2

A,n(x∗) = Mn

η −

η −

1
Mn

∞

.
∞

Then, for the aggregated prediction mean we have

Mn

lim
n→∞

µA,n(x∗) = lim
n→∞

σ2
A,n(x∗)

βiσ−2
i

(x∗)µi(x∗).

i=1
X
For PoE, given βi = 1 and limn→∞ σ−2
1/Mn, we

i
consistent

have

the

(x∗)/σ−2
A,n(x∗) =
prediction mean

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

(x∗)/σ−2

limn→∞ µA,n(x∗) = µη(x∗).
For GPoE, given
βi = 1/Mn and limn→∞ σ−2
A,n(x∗) = 1, we
i
have the consistent prediction mean limn→∞ µA,n(x∗) =
µη(x∗).
For (R)BCM, given βi = β = 1 or
limn→∞ βi = β = 0.5 log(σ2
η), we have the
inconsistent prediction mean limn→∞ µA,n(x∗) =
limn→∞ βσ−2
∗∗ /Mn) =
aµη(x∗) where a = σ−2
1 and the
equality holds when σ2
η = 0.

∗∗ ) + σ−2
σ−2
σ−2
∗∗ )

η µη(x∗)/(β(σ−2

η −
η /(σ−2

∗∗/σ2

η −

≥

C. Proof of Proposition 3

→

n→∞

Given that the points in the communication subset
are randomly selected without replacement from
nc = n/Mn
µη(x∗) and limn→∞ σ2
for the expert
i,
D+i =
limn→∞ µ+i(x∗) = µη(x∗) and limn→∞ σ2
for 2

c
D
and
, we have limn→∞ µc(x∗) =
∞
c (x∗) = σ2
c. Likewise,
trained on the augmented dataset
with size n+i = 2n/Mn, we have
+i(x∗) = σ2
η

M+i
c
}
D

η for

M .

{D

M

D

i

≤

≤

We ﬁrst derive the upper bound of σ2
c (x∗). For the station-
ary covariance function k(.) > 0, when nc is large enough
we have (Vazquez & Bect, 2010)

σ2
c (x∗)

k(x∗, x∗)

≤

k2(x∗, x′)
k(x′, x′)

−

+ σ2

ǫ,n,

k

k

∈

−

x′

where x′
Xc is the nearest data point to x∗. It is known
x∗
is proportional
that the relative distance rc =
to the inverse of the training size nc, i.e., rc
1/nc =
n→∞ 0. Conventional stationary covariance func-
Mn/n
tions only relay on the relative distance (once the covari-
ance parameters have been determined) and decrease with
rc. Consequently, the prediction variance σ2
c (x∗) increases
with rc. Taking the SE covariance function in (1) for ex-
ample,11 when rc
,

0 we have, given l0 = min1≤i≤d

→

∝

li

{

}

→

σ2
c (x∗)

We clearly see from this inequality that when rc
η since limn→∞ σ2
c (x∗) goes to σ2
σ2
Then, we rewrite the precision of GRBCM in (14b) as,
given β2 = 1,

ǫ,n = σ2
η.

→

0,

GRBCM(x∗) = σ−2
σ−2

+2(x∗)+

βi

σ−2
+i (x∗)

σ−2
c (x∗)

.

Mn

i=3
X

(cid:0)

−

(cid:1)
(20)

11We take the SE kernel for example since conventional kernels,
e.g., the rational quadratic kernel and the Mat´ern class of kernels,
can reduce to the SE kernel under some conditions.

Compared to
c,
M+i is trained on a more dense dataset
M
c (x∗) for a large enough n.12
σ2
D+i, leading to σ2
+i(x∗)
Given (19) and σ2
+i(x∗) > σ2
ǫ,n, the weight βi satisﬁes, for
3

Mn,

≤

i

≤

0

≤

≤

βi =

log

1
2

1
2

<

1
2

σ2
c (x∗)
σ2
+i(x∗)
(cid:19)
c + σ2
ar2
ǫ,n
σ2
ǫ,n ! ≤

(cid:18)

 

log

(cid:18)
a
2σ2

ǫ,n

σ2
c (x∗)
σ2
ǫ,n (cid:19)

r2
c .

<

log

(21)

Besides, the precision discrepancy satisﬁes, for 3
Mn,

i

≤

≤

0

≤

σ−2
+i (x∗)

σ−2
c (x∗) = σ−2

c (x∗)

−

σ2
c (x∗)
σ2
+i(x∗) −

1

(cid:19)

<

1
σ2

a
σ2

ǫ,n

ǫ,n

(cid:18)
r2
c .

(22)
Hence, the second term in the right-hand side of (20) satis-
ﬁes

Mn

βi

σ−2
+i (x∗)

σ−2
c (x∗)

<

−

Mn

a2
2σ6

ǫ,n

r4
c ∝

M 5
n
n4 .

i=3
X
n > 0, we have limn→∞ n4/M 5

(cid:1)

n =

i=3
X

(cid:0)
Since limn→∞ n/M 2
, and furthermore,

∞

Mn

lim
n→∞

βi

σ−2
+i (x∗)

σ−2
c (x∗)

= 0.

(23)

−

(cid:0)

i=3
X
Substituting (23) and limn→∞ σ−2
we have a consistent prediction precision as
σ−2
GRBCM(x∗) = σ−2
η .

(cid:1)

+2 (x∗) = σ−2
η

lim
n→∞

into (20),

Similarly, we rewrite the GRBCM’s prediction mean in
(14a) as

µGRBCM(x∗) = σ2

GRBCM(x∗)

µ∆ + σ−2

+2(x∗)µ+2(x∗)

,
(24)
(cid:1)

(cid:0)

−

µ∆ =

βi

σ−2
+i (x∗)µ+i(x∗)

σ−2
c (x∗)µc(x∗)

.

(cid:0)

i=3
X
Let δmax = max3≤i≤Mn
0, we have

µ∆| ≤

|

Mn

βiσ−2
c

i=3
X
Eq.(21)
<

(cid:12)
(cid:12)
(cid:12)
ar2
(cid:12)
c
2σ4

ǫ,n

Mn

i=3
X

σ2
c (x∗)
+i(x∗) µ+i(x∗)
σ2

(cid:12)
(cid:12)
(cid:12)
σ2
c (x∗)
σ2
+i(x∗)

µ+i(x∗)

−

δmax →

n→∞ 0.

(cid:1)

n→∞

→

(25)

−

µc(x∗)
(cid:12)
(cid:12)
(cid:12)

µc(x∗)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

12The equality is possible to hold when we employ disjoint par-

tition for {Di}

Mn
i=2 and x∗ is away from Xi.

≤

<

σ2
f exp(

σ2
f −
σ2
f
c + σ2
r2
l2
0

−
ǫ,n = ar2

c /l2
r2

0) + σ2
ǫ,n

c + σ2

ǫ,n.

(19)

where

Mn

Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression

Substituting (25) into (24), we have the consistent predic-
tion mean as

lim
n→∞

µGRBCM(x∗) = µη(x∗).

D. Discussions of GRBCM on the toy example

It is observed that the proposed GRBCM showcases superi-
ority over existing aggregations on the toy example, which
is brought by the particularly designed aggregation struc-
ture: the global communication expert
c to capture the
long-term features of the target function, and the remaining
M
i=2 to reﬁne local predictions.
experts

M

{M+i

}

(a)

0.16

E
S
M
S

0.14

0.12

0.1

0.08

0.06

104

(b)

-1.35

-1.4

L
L
S
M

-1.45

-1.5

105
Training size

106

-1.55

104

105
Training size

106

Figure7. Comparative results of GRBCM and Mc on the toy ex-
ample.

M

To verify the capability of GRBCM, we compare it with
the pure global expert
c which relies on a random sub-
set Xc. Fig. 7 shows the comparative results of GRBCM
c on the toy example. It is found that with increas-
and
ing n, (i) GRBCM always outperforms
c because of the
beneﬁts brought by local experts; and (ii) the predictions of
c generally become poorer since it becomes intractable

M
to choose a good subset from the increasing dataset.

M

M

E. Experimental results of NPAE

Table 2 compares the results of GRBCM and NPAE over
10 runs on the kin40k dataset (M = 16) and the sarcos
dataset (M = 72) using disjoint partition. It is observed
that GRBCM performs slightly better than NPAE on the
kin40k dataset, and produces competitive results on the sar-
cos dataset. But in terms of the computing efﬁciency, since
NPAE needs to build and invert an M
M covariance ma-
trix at each test point, it requires much more running time,
especially for the sarcos dataset with M = 72.

×

Table2. Comparative results (mean and standard deviation) of
GRBCM and NPAE over 10 runs on the kin40k dataset (M = 16)
and the sarcos dataset (M = 72) using disjoint partition. The
computing time t for each model involves the training and pre-
dicting time.

kin40k

SMSE
MSLL
t [S]
sarcos

SMSE
MSLL
t [S]

GRBCM

NPAE

0.0223 ± 0.0005
-1.9927 ± 0.0177
78.1 ± 4.4
GRBCM

0.0246 ± 0.0007
-1.9565 ± 0.0170
2852.4 ± 16.7
NPAE

0.0074 ± 0.0002
-2.3681 ± 0.0242
445.6 ± 49.4

0.0054 ± 0.0001
-2.5900 ± 0.0068
26444.0 ± 1213.0

