0
2
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
1
v
8
5
4
3
0
.
4
0
0
2
:
v
i
X
r
a

Learning Formation of Physically-Based Face Attributes

Ruilong Li1,2∗
Karl Bladin1∗
Pengda Xiang1,2 Xinglei Ren1

Yajie Zhao1∗

Chinmay Chinara1

Pratusha Prasad1 Bipin Kishore1

Owen Ingraham1
Jun Xing1 Hao Li 1,2,3

1USC Institute for Creative Technologies

2University of Southern California

3Pinscreen

Figure 1: We introduce a comprehensive framework for learning physically based face models from highly constrained facial scan data. Our
deep learning based approach for 3D morphable face modeling seizes the ﬁdelity of nearly 4000 high resolution face scans encompassing
expression and identity separation (a). The model (b) combines a multitude of anatomical and physically based face attributes to generate
an inﬁnite number of digitized faces (c). Our model generates faces at pore level geometry resolution (d).

Abstract

1. Introduction

Based on a combined data set of 4000 high resolution
facial scans, we introduce a non-linear morphable face
model, capable of producing multifarious face geometry of
pore-level resolution, coupled with material attributes for
use in physically-based rendering. We aim to maximize
the variety of identities, while increasing the robustness
of correspondence between unique components, including
middle-frequency geometry, albedo maps, specular inten-
sity maps and high-frequency displacement details. Our
deep learning based generative model learns to correlate
albedo and geometry, which ensures the anatomical cor-
rectness of the generated assets. We demonstrate poten-
tial use of our generative model for novel identity gener-
ation, model ﬁtting, interpolation, animation, high ﬁdelity
data visualization, and low-to-high resolution data domain
transferring. We hope the release of this generative model
will encourage further cooperation between all graphics,
vision, and data focused professionals, while demonstrating
the cumulative value of every individual’s complete biomet-
ric proﬁle.

∗Joint ﬁrst authors

Graphical virtual representations of humans are at the
center of many endeavors in the ﬁelds of computer vision
and graphics, with applications ranging from cultural me-
dia such as video games, ﬁlm, and telecommunication to
medical, biometric modeling, and forensics [13].

Designing, modeling, and acquiring high ﬁdelity data for
face models of virtual characters is costly and requires spe-
cialized scanning equipment and a team of skilled artists
and engineers [18, 6, 38]. Due to limiting and restrictive
data policies of VFX studios, in conjunction with the ab-
sence of a shared platform that regards the sovereignty of,
and incentives for the individuals data contributions, there is
a large discrepancy in the ﬁdelity of models trained on pub-
licly available data, and those used in large budget game
and ﬁlm production. A single, uniﬁed model would democ-
ratize the use of generated assets, shorten production cycles
and boost quality and consistency, while incentivizing inno-
vative applications in many markets and ﬁelds of research.
The uniﬁcation of a facial scan data set in a 3D mor-
phable face model (3DMM) [7, 12, 42, 13] promotes the fa-
vorable property of representing facial scan data in a com-
pact form, retaining the statistical properties of the source
without exposing the characteristics of any individual data

point in the original data set.

Previous methods, including traditional methods [7, 12,
28, 35, 17, 9], or deep learning [43, 39] to represent 3D face
shapes; lack high resolution (sub-millimeter, < 1mm) geo-
metric detail, use limited representations of facial anatomy,
or forgo the physically based material properties required
by modern visual effects (VFX) production pipelines. Phys-
ically based material intrinsics have proven difﬁcult to es-
timate through the optimization of unconstrained image
data due to ambiguities and local minima in analisys-by-
synthesis problems, while highly constrained data capture
remains percise but expensive [13]. Although variations
occur due to different applications, most face representa-
tions used in VFX employ a set of texture maps of at least
4096 × 4096 (4K) pixels resolution. At a minimum, this
set encorporates diffuse albedo, specular intensity, and dis-
placement (or surface normals).

Our goal is to build a physically-based, high-resolution
generative face model to begin bridging these parallel, but
in some ways divergent, visualization ﬁelds; aligning the
efforts of vision and graphics researchers. Building such
a model requires high-resolution facial geometry, material
capturing and automatic registration of multiple assets. The
handling of said data has traditionally required extensive
manual work, thus scaling such a database is non-trivial.
For the model to be light weight these data need to be com-
pressed into a compact form that enables controlled recon-
struction based on novel input. Traditional methods such as
PCA [7] and bi-linear models [12] − which are limited by
memory size, computing power, and smoothing due to in-
herent linearity − are not suitable for high-resolution data.
By leveraging state-of-the-art physically-based facial
scanning [18, 26], in a Light Stage setting, we enable acqui-
sition of diffuse albedo and specular intensity texture maps
in addition to 4K displacement. All scans are registered
using an automated pipeline that considers pose, geome-
try, anatomical morphometrics, and dense correspondence
of 26 expressions per subject. A shared 2D UV param-
eterization data format [16, 44, 39], enables training of a
non-linear 3DMM, while the head, eyes, and teeth are rep-
resented using a linear PCA model. Hence, we propose a
hybrid approach to enable a wide set of head geometry as-
sets as well as avoiding the assumption of linearity in face
deformations.

Our model fully disentangles identity from expressions,
and provides manipulation using a pair of low dimensional
feature vectors. To generate coupled geometry and albedo,
we designed a joint discriminator to ensure consistency,
along with two separate discriminators to maintain their
individual quality.
Inference and up-scaling of before-
mentioned skin intrinsics enable recovery of 4K resolution
texture maps.

Our main contributions are:

• The ﬁrst published upscaling of a database of high res-

olution (4K) physically based face model assets.

• A cascading generative face model, enabling control
of identity and expressions, as well as physically based
surface materials modeled in a low dimensional feature
space.

• The ﬁrst morphable face model built for full 3D real
time and ofﬂine rendering applications, with more rel-
evant anatomical face parts than previously seen.

2. Related Work

Facial Capture Systems Physical object scanning de-
vices span a wide range of categories; from single RGB
cameras [15, 40],
to active [4, 18], and passive [5]
light stereo capture setups, and depth sensors based on
time-of-ﬂight or stereo re-projection. Multi-view stereo-
photogrammetry (MVS) [5] is the most readily available
method for 3D face capturing. However, due to its many
advantages over other methods (capture speed, physically-
based material capturing, resolution), polarized spherical
gradient illumination scanning [18] remains state-of-the-art
for high-resolution facial scanning. A mesoscopic geome-
try reconstruction is bootstrapped using an MVS prior, uti-
lizing omni-directional illumination, and progressively ﬁ-
nalized using a process known as photometric stereo [18].
The algorithm promotes the physical reﬂectance properties
of dielectric materials such as skin; speciﬁcally the separa-
ble nature of specular and subsurface light reﬂections [30].
This enables accurate estimation of diffuse albedo and spec-
ular intensity as well as pore-level detailed geometry.

3D Morphable Face Models The ﬁrst published work
on morphable face models by Blanz and Vetter [7] repre-
sented faces as dense surface geometry and texture, and
modeled both variations as separate PCA models learned
from around 200 subject scans. To allow intuitive con-
trol; attributes, such as gender and fullness of faces, were
mapped to components of the PCA parameter space. This
model, known as the Basel Face Model [34] was released
for use in the research community, and was later extended to
a more diverse linear face model learnt from around 10,000
scans [9, 8].

To incorporate facial expressions, Vlasic et al. [46] pro-
posed a multi-linear model to jointly estimate the varia-
tions in identity, viseme, and expression, and Cao et al. [12]
built a comprehensive bi-linear model (identity and expres-
sion) covering 20 different expressions from 150 subjects
learned from RGBD data. Both of these models adopt a
tensor-based method under the assumption that facial ex-
pressions can be modeled using a small number of discrete
poses, corresponded between subjects. More recently, Li

(a)
4k × 4k
LS
TG 8k × 8k

(b)

(c)

3.9M 4k × 4k
3.5M

N/A

(d)
79
99

(e)
26
20

Table 1: Resolution and extent of the datasets. (a). Albedo resolu-
tion. (b). Geometry resolution. (c). Specular intensity resolution.
(d) # of subjects. (f). # of expressions per subject.

scale facial geometry inferred directly from images. Beside
geometry, Yamaguchi et al. [48] presented a comprehen-
sive method to infer facial reﬂectance maps (diffuse albedo,
specular intensity, and medium- and high-frequency dis-
placement) based on single image inputs. More recently,
Nagano et al. [32] proposed a framework for synthesiz-
ing arbitrary expressions both in image space and UV tex-
ture space, from a single portrait image. Although these
methods can synthesize facial geometry or/and texture maps
from a given image, they don’t provide explicit parametric
controls of the generated result.

3. Database

3.1. Data Capturing and Processing

Data Capturing Our Light Stage scan system employs
photometric stereo [18] in combination with monochrome
color reconstruction using polarization promotion [26] to
allow for pore level accuracy in both the geometry re-
construction and the reﬂectance maps. The camera setup
(Fig.2) was designed for rapid, database scale, acquisition
by the use of Ximea machine vision cameras which en-
able faster streaming and wider depth of ﬁeld than tra-
ditional DSLRs [26]. The total set of 25 cameras con-
sists of eight 12MP monochrome cameras, eight 12MP
color cameras, and nine 4MP monochrome cameras. The
12MP monochrome cameras allow for pore level geome-
try, albedo, and specular reﬂectance reconstruction, while
the additional cameras aid in stereo base mesh-prior recon-
struction.

To capture consistent data across multiple subjects with
maximized expressiveness, we devised a FACS set [14]
which combines 40 action units to a condensed set of 26
expressions. In total, 79 subjects, 34 female, and 45 male,
ranging from age 18 to 67, were scanned performing the 26
expressions. To increase diversity, we combined the data
set with a selection of 99 Triplegangers [2] full head scans;
each with 20 expressions. Resolution and extent of the two
data sets are shown in Table 1. Fig. 3 shows the age and
ethnicity (multiple choice) distributions of the source data.

Processing Pipeline. Starting from the multi-view im-
agery, a neutral scan base mesh is reconstructed using MVS.

Figure 2: Capture system and camera setup. Left: Light Stage
capturing system. Right: camera layout.

et al. [28] released the FLAME model, which incorporates
both pose-dependent corrective blendshapes, and additional
global identity and expression blendshapes learnt from a
large number of 4D scans.

To enable adaptive, high level, semantic control over face
deformations, various locality-based face models have been
proposed. Neumann et al. [33] extract sparse and spatially
localized deformation modes, and Brunton et al. [10] use
a large number of localized multilinear wavelet modes. As
a framework for anatomically accurate local face deforma-
tions, the Facial Action Coding System (FACS) by Ekman
[14] is widely adopted.
It decomposes facial movements
into basic action units attributed to the full range of motion
of all facial muscles.

Morphable face models have been widely used for appli-
cations like face ﬁtting [7], expression manipulation [12],
real-time tracking [42], as well as in products like Apple’s
ARKit. However, their use cases are often limited by the
resolution of the source data and restrictions of linear mod-
els causing smoothing in middle and high frequency geom-
etry details (e.g. wrinkles, and pores). Moreover, to the best
of our knowledge, all existing morphable face models gen-
erate texture and geometry separately, without considering
the correlation between them. Given the speciﬁc and var-
ied ways in which age, gender, and ethnicity are manifested
within the spectrum of human life, ignoring such correla-
tion will cause artifacts; e.g. pairing an African-inﬂuenced
albedo to an Asian-inﬂuenced geometry.

Image-based Detail Inference To augment the quality of
existing 3DMMs, many works have been proposed to infer
the ﬁne-level details from image data. Skin detail can be
synthesized using data-driven texture synthesis [21] or sta-
tistical skin detail models [19]. Cao et al. [11] used a prob-
ability map to locally regress the medium-scale geometry
details, where a regressor was trained from captured patch
pairs of high-resolution geometry and appearance. Saito et
al. [36] presented a texture inference technique using a deep
neural network-based feature correlation analysis.

GAN-based Image-to-Image frameworks [23] have
proven to be powerful for high-quality detail synthesis, such
as the coarse [45], medium [37] or even mesoscopic [22]

(a) Age distribution

(b) Ethnicity distribution

Figure 3: Distribution of age (a) and ethnicity (b) in the data sets.

Figure 4: Our generic face model consists of multiple geometries
constrained by different types of deformation. In addition to face
(a), head, and neck (b), our model represents teeth (c), gums (d),
eyeballs (e), eye blending (f), lacrimal ﬂuid (g), eye occlusion
(h), and eyelashes (i). Texture maps provide high resolution (4K)
albedo (j), specularity (k), and geometry through displacement (l).

Then a linear PCA model in our topology (See Fig.4) based
on a combination and extrapolation of two existing mod-
els (Basel [34] and Face Warehouse [12]) is used to ﬁt the
mesh. Next, Laplacian deformation is applied to deform
the face area to further minimize the surface-to-surface er-
ror. Cases of inaccurate ﬁtting were manually modeled and
ﬁtted to retain the ﬁtting accuracy of the eyeballs, mouth
sockets and skull shapes. The resulting set of neutral scans
were immediately added to the PCA basis for registering
new scans. We ﬁt expressions using generic blendshapes
and non-rigid ICP [27]. Additionally, to retain texture space
and surface correspondence, image space optical ﬂow from
neutral to expression scan is added from 13 different vir-
tual camera views as additional dense constraint in the ﬁnal
Laplacian deformation of the face surface.

3.2. Training Data Preparation

Data format. The full set of the generic model consists
of a hybrid geometry and texture maps (albedo, specular
intensity, and displacement) encoded in 4K resolution, as
illustrated in Fig. 4. To enable joint learning of the cor-
relation between geometry and albedo, 3D vertex positions
are rasterized to a three channel HDR bitmap of 256 × 256
pixels resolution. The face area (pink in Fig. 4) used to
learn the geometry distribution in our non-linear generative
model consists of m = 11892 vertices, which, if evenly
spread out in texture space, would require a bitmap of res-
√
2 × m(cid:101)2 = 155 × 155, ac-
olution greater or equal to (cid:100)

Figure 5: Comparison of base mesh geometry resolutions. Left:
Base geometry reconstructed in 4K resolution. Middle: Base ge-
ometry reconstructed in 256 × 256 resolution. Right: Error map
showing the Hausdorff distance in the range (0mm, 1mm), with
a mean error of 0.068mm.

cording to Nyquist’s resampling theorem. As shown in
Fig. 5, the proposed resolution is enough to recover middle-
frequency detail. This relatively low resolution base geom-
etry representation enables great simpliﬁcation in training
data load.

Data Augmentation Since the number of subjects is lim-
ited to 178 individuals, we apply two strategies to augment
the data for identity training: 1) For each source albedo,
we randomly sample a target albedo within the same eth-
nicity and gender in the data set using [50] to transfer skin
tones of target albedos to source albedos (these samples are
restricted to datapoints of the same ethnicity), followed by
an image enhancement [20] to improve the overall quality
and remove artifacts. 2). For each neutral geometry, we
add a very small expression offset using FaceWarehouse ex-
pression components with a small random weights(< ±0.5
std) to loosen the constraints of “neutral”. To augment the
expressions, we add random expression offsets to generate
fully controlled expressions.

4. Generative Model

An overview of our system is illustrated in Fig. 6. Given
a sampled latent code Zid ∼ N (µid, σid), our Identity net-
work generates a consistent albedo and geometry pair of
neutral expression. We train an Expression network to gen-
erate the expression offset that can be added to the neu-
tral geometry. We use random blendshape weights Zexp ∼
N (µexp, σexp) as the expression network’s input to enable
manipulation of target semantic expressions. We upscale
the albedo and geometry maps to 1K, and feed them into
a transfer network [47] to synthesize the corresponding 1K
specular and displacement maps. Finally, all the maps ex-
cept for the middle frequency geometry map are upscaled
to 4K using Super-resolution [25], as we observed that
256 × 256 pixels are sufﬁcient to represent the details of
the base geometry (Section 3.2). The details of each com-
ponent are elaborated on in Section 4.1, 4.2, and 4.3.

Figure 6: Overview of generative pipeline. Latent vectors for identity and expression serve as input for generating the ﬁnal face model.

Identity generative network. The identity generator
Figure 7:
Gid produces albedo and geometry which get checked against
ground truth (GT) data by the discriminators, Dalbedo, Djoint,
and Dgeometry during training.

4.1. Identity Network

The goal of our Identity network is to model the cross
correlation between geometry and albedo to generate con-
sistent, diverse and biologically accurate identities. The net-
work is built upon the Style-GAN architecture [24], that can
produce high-quality, style-controllable sample images.

To achieve consistency, we designed 3 discriminators
as shown in Fig.7, including individual discriminators for
albedo (Dalbedo) and geometry (Dgeometry), to ensure the
quality and sharpness of the generated maps, and an addi-
tional joint discriminator (Djoint) to learn their correlated
distribution. Djoint is formulated as follows:

Ladv = min
Gid
Ez∼pz(z)

Ex∼pdata(x)

max
Djoint
(cid:2)log (1 − Djoint(Gid(z)))(cid:3).

(cid:2)log Djoint(A)(cid:3)+

(1)

where pdata(x) and pz(z) represent the distributions of real
paired albedo and geometry x and noise variables z in the
domain of A respectively.

4.2. Expression Network

Figure 8: Expression generative network. The expression genera-
tor Gexp generates offsets which get checked against ground truth
offsets by the discriminator Dexp. The regressor Rexp produces
(cid:48)
exp so that the L1 loss Lexp can be
an estimate of the latent code Z
modeled.

shape weights, which correspond to the strength of 25 or-
thogonal facial activation units, as network input. We in-
troduce a pre-trained expression regression network Rexp
to predict the expression weights from the generated image,
and force this prediction to be similar to the input latent
code Zexp. We then force the generator to understand the in-
put latent code Zexp under the perspective of the pre-trained
expression regression network. As a result, each dimension
of the latent code Zexp will control the corresponding ex-
pression deﬁned in the original blendshape set. The loss we
introduce here is:

Lexp =(cid:107) Zexp − Z

(cid:48)

exp (cid:107)

This loss, Lexp, will be back propagated during training to
enforce the orthogonality of each blending unit. We mini-
mize the following losses to train the network:

L = Lexp

l2

+ β1Lexp

adv + β2Lexp

(2)

(3)

To simplify the learning of a wide range of diverse
expressions, we represent them using vector offset maps,
which also makes the learning of expressions independent
from identity. Similar to the Identity network, the expres-
sion network adopts Style-GAN as the base structure. To al-
low for intuitive control over expressions, we use the blend-

where Lexp
l2
and Lexp

adv is the discriminator loss.

is the L2 reconstruction loss of the offset map

4.3. Inference and Super-resolution

Similar to [48]; upon obtaining albedo and geometry
maps (256 × 256), we use them to infer specular and dis-

placement maps in 1K resolution. In contrast to [48], us-
ing only albedo as input, we introduce the geometry map to
form stronger constraints. For displacement, we adopted the
method of [48, 22] to separate displacement in to individ-
ual high-frequency and low-frequency components, which
makes the problem more tractable. Before feeding the
two inputs into the inference network [47], we up-sample
the albedo to 1K using a super-resolution network simi-
lar to [25]. The geometry map is super-sampled using bi-
linear interpolation. The maps are further up-scaled from
1K to 4K using the same super-resolution network struc-
ture. Our method can be regarded as a two step cascading
up-sampling strategy (256 to 1K, and 1K to 4K). This
makes the training faster, and enables higher resolution in
the ﬁnal results.

5. Implementation Details

Our framework is implemented using Pytorch and all our
networks are trained using two NVIDIA Quadro GV100s.
We follow the basic training schedule of Style-GAN [24]
with several modiﬁcations applied to the Expression net-
work, like by-passing the progressive training strategy as
expression offsets are only distinguishable on relatively
high resolution maps. We also remove the noise injec-
tion layer, due to the input latent code Zexp which enables
full control of the generated results. The regression mod-
ule (Rexp-block in Fig.8) has the same structure as the dis-
criminator Dexp, except for the number of channels in the
last layer, as it serves as a discriminator during training.
The regression module is initially trained using synthetic
unit expression data generated with neutral expression and
F aceW arehouse expression components, and then ﬁne-
tuned on scanned expression data. During training, Rexp, is
ﬁxed without updating parameters. The Expression network
is trained with a constant batch size of 128 on 256x256-
pixel images for 40 hours. The Identity network is trained
by progressively reducing the batch size from 1024 to 128
on growing image sizes ranging from 8x8 to 256x256 pix-
els, for 80 hours.

6. Experiments And Evaluations

6.1. Results

In Fig.11, we show the quality of our generated model
rendered using Arnold. The direct output of our genera-
tive model provides all the assets necessary for physically-
based rendering in software such as Maya, Unreal Engine,
or Unity 3D. We also show the effect of each generated
component.

6.2. Qualitative Evaluation

We show identity interpolation in Fig.9. The interpola-
tion in latent space reﬂects both albedo and geometry. In

Figure 9: Non-linear identity interpolation between generated sub-
jects. Age (top) and gender (bottom) are interpolated from left to
right.

Figure 10: Non linear expression interpolation using generative
expression network. Combinations of two example shapes are dis-
played in a grid where the number of standard deviations from the
generic neutral model deﬁne the extent of an expression shape.

contrast to linear blending, our interpolation generates sub-
jects belonging to a natural statistical distribution.

In Fig.10, we show the generation and interpolation of
our non-linear expression model. We pick two orthogonal
blendshapes for each axis and gradually change the input
weights. Smooth interpolation in vector space will lead to a
smooth interpolation in model space.

We show nearest neighbors for generated models in the
training set in Fig.12. These are found based on point-wise
Euclidean distance in geometry. Albedos are compared to
prove our ability to generate new models that are not merely
recreations of the training set.

6.3. Quantitative Evaluation

We evaluate the effectiveness of our identity network’s
joint generation in Table 2 by computing Frechet Inception
Distances (FID) and Inception-Scores (IS) on rendered im-
ages of three categories: randomly paired albedo and ge-

(a)

(b)

(c)

(d)

(e)

(f)

Figure 11: Rendered images of generated random samples. Column (a), (b), and (c) show images rendered under novel image-based HDRI
lighting [49]. Column (c), (d), and (e), show geometry with albedo, specular intensity, and displacement added one at the time.

Generation Method
independent
joint
groud truth

IS↑
2.22
2.26
2.35

FID↓
23.61
21.72
-

Table 2: Evaluation on our Identity generation. Both IS and FID
are calculated on images rendered with independently/jointly gen-
erated albedo and geometry.

In addition, to evaluate the non-linearity of our expres-
sion network in comparison to the linear expression model
of FaceWarehouse [12], we ﬁrst ﬁt all the Light Stage scans
using FaceWarehouse, and get the 25 ﬁtting weights, and
expression recoveries, for each scan. We then recover the
same expressions by feeding the weights to our expres-
sion network. We evaluate the reconstruction loss with
mean-square error (MSE) for both FaceWarehouse’s and
our model’s reconstructions. On average, our method’s
MSE is 1.2mm while FaceWarehouse’s is 2.4mm. This
shows that for expression ﬁtting, our non-linear model nu-
merically outperforms a linear model of the same dimen-

Figure 12: Nearest neighbors for generated models in training set.
Top row: albedo from generated models. Bottom row: albedo of
geometrically nearest neighbor in training set.

ometry, paired albedo and geometry generated using our
model, and ground truth pairs. Based on these results, we
conclude that our model generates more plausible faces,
similar to those using ground truth data pairs, than random
pairing.

We also evaluate our identity networks generalization to
unseen faces by ﬁtting 48 faces from [1]. The average Haus-
dorff distance is 2.8mm, which proves that our model’s ca-
pacity is not limited by the training set.

(a) Training data

(b) Generated data

Figure 13: The age distribution of the training data (a) VS. ran-
domly generated samples (b).

Figure 14: Comparison of 3D scan ﬁtting with Basel [7], Face-
wareHouse [12], and FLAME [28]. Error maps are computed us-
ing Hausdorff distance between each ﬁtted model and ground truth
scans.

sionality.

To demonstrate our generative identity model’s coverage
of the training data, we show the gender, and age distribu-
tions of the original training data and 5000 randomly gener-
ated samples in Fig.13. The generated distributions are well
aligned with the source.

6.4. Applications

To test the extent of our identity model’s parameter
space, we apply it to scanned mesh registration by reversing
the GAN to ﬁt the latent code of a target image [29]. As our
model requires a 2D parameterized geometry input, we ﬁrst
use our linear model to align the scans using landmarks, and
then parameterize it to UV space after Laplacian morphing
of the surface. We compare our ﬁtting results with widely
used (linear) morphable face models in Fig.14. This evalua-
tion does not prove the ability to register unconstrained data
but shows that our model is able to reconstruct novel faces
by the virtue of it’s non-linearity, to a degree unobtainable
by linear models.

Another application of our model is transferring low-
quality scans into the domain of our model by ﬁtting using
both MSE loss and discriminator loss. In Fig.15, we show
examples of data enhancement of low resolution scans.

Figure 15: Low-quality data domain transfer. Top row: Models
with low resolution geometry and albedo. Bottom row: Enhance-
ment result using our model.

7. Conclusion and Limitations

Conclusion We have introduced the ﬁrst published use of
a high-ﬁdelity face database, with physically-based mare-
rial attributes, in generative face modeling. Our model can
generate novel subjects and expressions in a controllable
manner. We have shown that our generative model performs
well on applications such as mesh registration and low res-
olution data enhancement. We hope that this work will ben-
eﬁt many analysis-by-synthesis research efforts through the
provision of higher quality in face image rendering.

Limitations and Future work In our model, expression
and identity are modeled separately without considering
their correlation. Thus the reconstructed expression off-
set will not include middle-frequency geometry of an in-
dividual’s expression, as different subjects will have unique
representations of the same action unit. Our future work
will include modeling of this correlation. Since our expres-
sion generation model requires neural network inference
and re-sampling of 3D geometry it is not currently as user
friendly as blendshape modeling. Its ability to re-target pre-
recorded animation sequences will have to be tested further
to be conclusive. One issue of our identity model arises in
applications that require ﬁtting to 2D imagery, which ne-
cessitates an additional differentiable rendering component.
A potential problem is ﬁtting lighting in conjunction with
shape as complex material models make the problem less
tractable. A possible solution could be an image-based re-
lighting method [41, 31] applying a neural network to con-
vert the rendering process to an image manipulation prob-
lem. The model will be continuously updated with new fea-
tures such as variable eye textures and hair as well as more
anatomically relevant components such as skull, jaw, and
neck joints by combining data sources through collabora-
tive efforts. To encourage democratization and wide use
cases we will explore encryption techniques such as fed-
erated learning, homomorphic encryption, and zero knowl-
edge proofs which have the effect of increasing subjects’
anonymity.

8. Acknowledgement

Hao Li is afﬁliated with the University of Southern Cal-
ifornia, the USC Institute for Creative Technologies, and
Pinscreen. This research was conducted at USC and was
funded by the U.S. Army Research Laboratory (ARL) un-
der contract number W911NF-14-D-0005. This project was
not funded by Pinscreen, nor has it been conducted at Pin-
screen. The content of the information does not necessarily
reﬂect the position or the policy of the Government, and no
ofﬁcial endorsement should be inferred.

References

[1] 3d

scan

store:
x

Male
bundle.

and

female

head
3d
https://www.

48

model
3dscanstore.com/3d-head-models/
female-retopologised-3d-head-models/
male-female-3d-head-model-48xbundle.
Online; Accessed: 2019-11-22. 7

[2] Triplegangers. https://triplegangers.com/. On-

line; Accessed: 2019-11-22. 3
[3] Unreal Engine - Digital Human.

https://docs.

unrealengine.com/en-US/Resources/
Showcases/DigitalHumans/index.html. Online;
Accessed: 2020-03-29. 12

[4] Oleg Alexander, Mike Rogers, William Lambeth, Matt Chi-
ang, and Paul Debevec. The digital emily project: Photo-
In ACM SIGGRAPH
real facial modeling and animation.
Courses, 2009. 2

[5] Thabo Beeler, Bernd Bickel, Paul A. Beardsley, Bob Sumner,
and Markus H. Gross. High-quality single-shot capture of
facial geometry. In ACM Transactions on Graphics (TOG),
2010. 2

[6] Thabo Beeler, Fabian Hahn, Derek Bradley, Bernd Bickel,
Paul A. Beardsley, Craig Gotsman, Robert W. Sumner, and
Markus Groß. High-quality passive facial performance cap-
ture using anchor frames. In ACM Transactions on Graphics
(TOG), 2011. 1

[7] Volker Blanz and Thomas Vetter. A morphable model for
the synthesis of 3d faces. In ACM Transactions on Graphics
(TOG), SIGGRAPH ’99, 1999. 1, 2, 3, 8

[8] James Booth, Anastasios Roussos, Allan Ponniah, David
Dunaway, and Stefanos Zafeiriou. Large scale 3d morphable
models. International Journal of Computer Vision, 126:233–
254, 2017. 2

[9] James Booth, Anastasios Roussos, Stefanos Zafeiriou, Allan
Ponniah, and David Dunaway. A 3d morphable model learnt
from 10,000 faces. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
5543–5552, 2016. 2

[10] Alan Brunton, Timo Bolkart, and Stefanie Wuhrer. Multilin-
ear wavelets: A statistical shape space for human faces. In
Proceedings of the European Conference on Computer Vi-
sion (ECCV), 2014. 3

[11] Chen Cao, Derek Bradley, Kun Zhou, and Thabo Beeler.
Real-time high-ﬁdelity facial performance capture. ACM
Transactions on Graphics (TOG), 34(4), July 2015. 3

[12] Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Zhou
Kun. Facewarehouse: A 3d facial expression database for
visual computing. IEEE Transactions on Visualization and
Computer Graphics, 2014. 1, 2, 3, 4, 7, 8

[13] Bernhard Egger, William AP Smith, Ayush Tewari, Stefanie
Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard,
Timo Bolkart, Adam Kortylewski, Sami Romdhani, et al.
3d morphable face models–past, present and future. arXiv
preprint arXiv:1909.01815, 2019. 1, 2

[14] Paul Ekman and Wallace V. Friesen. Facial action coding
system: a technique for the measurement of facial move-
ment. In Consulting Psychologists Press, 1978. 3

[15] Pablo Garrido, Levi Valgaert, Chenglei Wu, and Christian
Theobalt. Reconstructing detailed dynamic face geome-
try from monocular video. ACM Transactions on Graphics
(TOG), 32(6), Nov. 2013. 2

[16] Baris Gecer, Alexander Lattas, Stylianos Ploumpis, Jiankang
Deng, Athanasios Papaioannou, Stylianos Moschoglou, and
Stefanos Zafeiriou. Synthesizing coupled 3d face modali-
ties by trunk-branch generative adversarial networks. arXiv
preprint arXiv:1909.02215, 2019. 2

[17] Thomas Gerig, Andreas Morel-Forster, Clemens Blumer,
Bernhard Egger, Marcel Luthi, Sandro Sch¨onborn, and
Thomas Vetter. Morphable face models-an open framework.
In IEEE International Conference on Automatic Face & Ges-
ture Recognition (FG 2018). IEEE, 2018. 2

[18] Abhijeet Ghosh, Graham Fyffe, Borom Tunwattanapong, Jay
Busch, Xueming Yu, and Paul E. Debevec. Multiview face
capture using polarized spherical gradient illumination. ACM
Transactions on Graphics (TOG), 30:129, 2011. 1, 2, 3
[19] Aleksey Golovinskiy, Wojciech Matusik, Hanspeter Pﬁster,
Szymon Rusinkiewicz, and Thomas A. Funkhouser. A sta-
tistical model for synthesis of detailed facial geometry. ACM
Transactions on Graphics (TOG), 25:1025–1034, 2006. 3

[20] Yoav HaCohen, Eli Shechtman, Dan B Goldman, and Dani
Lischinski. Non-rigid dense correspondence with applica-
tions for image enhancement. ACM transactions on graphics
(TOG), 30(4):70, 2011. 4

[21] Antonio Haro, Irfan A. Essa, and Brian K. Guenter. Real-
time photo-realistic physically based rendering of ﬁne scale
human skin structure. In Rendering Techniques, 2001. 3
[22] Loc Huynh, Weikai Chen, Shunsuke Saito, Jun Xing, Koki
Nagano, Andrew Jones, Paul E. Debevec, and Hao Li. Meso-
scopic facial geometry inference using deep neural networks.
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018. 3, 6

[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.
Efros.
Image-to-image translation with conditional adver-
sarial networks. 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 5967–5976, 2016. 3
[24] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4401–4410, 2019. 5, 6
[25] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-

realistic single image super-resolution using a generative ad-
versarial network. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 4681–4690,
2017. 4, 6

[26] Chloe LeGendre, Kalle Bladin, Bipin Kishore, Xinglei Ren,
Xueming Yu, and Paul Debevec. Efﬁcient multispectral fa-
cial capture with monochrome cameras. In Color and Imag-
ing Conference, volume 2018, pages 187–202, 2018. 2, 3

[27] Hao Li, Robert W. Sumner, and Mark Pauly. Global cor-
respondence optimization for non-rigid registration of depth
scans. In Proceedings of the Symposium on Geometry Pro-
cessing, SGP ’08, pages 1421–1430, Aire-la-Ville, Switzer-
land, Switzerland, 2008. Eurographics Association. 4
[28] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and
Javier Romero. Learning a model of facial shape and expres-
sion from 4d scans. ACM Transactions on Graphics (TOG),
36(6):194, 2017. 2, 3, 8

[29] Zachary C Lipton and Subarna Tripathi. Precise recovery of
latent vectors from generative adversarial networks. arXiv
preprint arXiv:1702.04782, 2017. 8

[30] Wan-Chun Ma, Tim Hawkins, Pieter Peers, Charles-F´elix
Chabert, Malte Weiss, and Paul E. Debevec. Rapid acqui-
sition of specular and diffuse normal maps from polarized
In Rendering Techniques,
spherical gradient illumination.
2007. 2

[31] Abhimitra Meka, Christian Haene, Rohit Pandey, Michael
Zollhoefer, Sean Fanello, Graham Fyffe, Adarsh Kowdle,
Xueming Yu, Jay Busch, Jason Dourgarian, Peter Denny,
Soﬁen Bouaziz, Peter Lincoln, Matt Whalen, Geoff Harvey,
Jonathan Taylor, Shahram Izadi, Andrea Tagliasacchi, Paul
Debevec, Christian Theobalt, Julien Valentin, and Christoph
Rhemann. Deep reﬂectance ﬁelds - high-quality facial re-
ﬂectance ﬁeld inference from color gradient illumination.
volume 38, July 2019. 8

[32] Koki Nagano, Jaewoo Seo, Jun Xing, Lingyu Wei, Zimo
Li, Shunsuke Saito, Aviral Agarwal, Jens Fursund, and Hao
Li. pagan: real-time avatars using dynamic textures. ACM
Transactions on Graphics (TOG), 37:258:1–258:12, 2018. 3
[33] Thomas Neumann, Kiran Varanasi, Stephan Wenger, Markus
Wacker, Marcus A. Magnor, and Christian Theobalt. Sparse
localized deformation components. ACM Transactions on
Graphics (TOG), 32:179:1–179:10, 2013. 3

[34] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami
Romdhani, and Thomas Vetter. A 3d face model for pose and
illumination invariant face recognition. IEEE International
Conference on Advanced Video and Signal Based Surveil-
lance, pages 296–301, 2009. 2, 4

[35] Stylianos Ploumpis, Haoyang Wang, Nick Pears,
William AP Smith, and Stefanos Zafeiriou.
Combin-
ing 3d morphable models: A large scale face-and-head
model. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2019. 2

[36] Shunsuke Saito, Lingyu Wei, Liwen Hu, Koki Nagano, and
Hao Li. Photorealistic facial texture inference using deep
neural networks. 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2326–2335, 2016. 3
[37] Matan Sela, Elad Richardson, and Ron Kimmel. Unre-
stricted facial geometry reconstruction using image-to-image

translation. 2017 IEEE International Conference on Com-
puter Vision (ICCV), pages 1585–1594, 2017. 3

[38] Yeongho Seol, Wan-Chun Ma, and J. P. Lewis. Creating
an actor-speciﬁc facial rig from performance capture.
In
Proceedings of the 2016 Symposium on Digital Production,
DigiPro ’16, 2016. 1

[39] Gil Shamai, Ron Slossberg, and Ron Kimmel.

Syn-
thesizing facial photometries and corresponding geome-
tries using generative adversarial networks. arXiv preprint
arXiv:1901.06551, 2019. 2

[40] Fuhao Shi, Hsiang-Tao Wu, Xin Tong, and Jinxiang Chai.
Automatic acquisition of high-ﬁdelity facial performances
using monocular videos. ACM Transactions on Graphics
(TOG), 33:222:1–222:13, 2014. 2

[41] Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zexiang
Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay
Busch, Paul Debevec, and Ravi Ramamoorthi. Single image
portrait relighting. ACM Transactions on Graphics (TOG),
38(4):79, 2019. 8

[42] Justus Thies, Michael Zollh¨ofer, Marc Stamminger, Chris-
tian Theobalt, and Matthias Nießner. Face2face: real-time
face capture and reenactment of rgb videos. IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2016. 1, 3

[43] Luan Tran, Feng Liu, and Xiaoming Liu. Towards high-
In Proceed-
ﬁdelity nonlinear 3d face morphable model.
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1126–1135, 2019. 2

[44] Luan Tran and Xiaoming Liu. On learning 3d face mor-
IEEE transactions

phable model from in-the-wild images.
on pattern analysis and machine intelligence, 2019. 2
[45] George Trigeorgis, Patrick Snape, Iasonas Kokkinos, and
Stefanos Zafeiriou. Face normals ”in-the-wild” using fully
convolutional networks. 2017 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 340–
349, 2017. 3

[46] Daniel Vlasic, Matthew Brand, Hanspeter Pﬁster, and Jovan
In ACM

Popovic. Face transfer with multilinear models.
Transactions on Graphics (TOG), 2005. 2

[47] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image syn-
thesis and semantic manipulation with conditional gans. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2018. 4, 6

[48] Shugo Yamaguchi, Shunsuke Saito, Koki Nagano, Yajie
Zhao, Weikai Chen, Kyle Olszewski, Shigeo Morishima, and
Hao Li. High-ﬁdelity facial reﬂectance and geometry infer-
ence from an unconstrained image. ACM Transactions on
Graphics (TOG), 37:162:1–162:14, 2018. 3, 5, 6

[49] G. Zaal. HDRI Haven. https://hdrihaven.com/

hdris/. Online; Accessed: 2019-11-22. 7

[50] Yajie Zhao, Qingguo Xu, Weikai Chen, Chao Du, Jun Xing,
Xinyu Huang, and Ruigang Yang. Mask-off: Synthesizing
face images in the presence of head-mounted displays.
In
2019 IEEE Conference on Virtual Reality and 3D User In-
terfaces (VR), pages 267–276. IEEE, 2019. 4

Appendix

Gender Control

Step1. Pre-computing mean gender latent code. First,
we propose a classiﬁer ψ, trained with ground truth data to
classify our input pair (albedo and geometry maps) into two
categories (male and female). Then we randomly sample
Zid ∼ N (µid, σid) to generate 10k sample pairs Gid(Zid)
using our identity network. The classiﬁer separates all the
samples into two groups. Finally, we extract the mean vec-
tor of each category as Zmale and Zf emale using equation
4.

(a) No extra assets

(b) + Lacrimal ﬂuid

Zmean =

1
i=1 Ω(Z (i)
id )

(cid:80)10k

10k
(cid:88)

i=1

id · Ω(Z (i)
Z (i)
id )

(4)

Where Ω(Zid) is the gender activation function which
converts the outputs of gender classiﬁer ψ into binary values
deﬁned as follows:

(c) + Blend mesh

(d) + Occlusion mesh

Figure 16: Closeup of real time rendered eye with our model’s
additional eye geometries successively added. The eyeball and
eyelashes are considered as default eye geometry and therefore
kept in all subﬁgures.

(cid:40)

Ω(Zid) =

1, ψ(Gid(Zid)) <= 0.5
0, ψ(Gid(Zid)) > 0.5

(5)

Figure 9 in the main paper also shows a example of aging
interpolation by gradually increasing α from 0.0 to 0.7, and
decreasing β from 0.7 to 0.0.

Where Ω(Zid) = 1 is deﬁned to be female, and
Ω(Zid) = 0 means male. In equation 4, the mean vector
in each category Zmale and Zf emale is computed by sim-
ply averaging the samples where Ω(Z (i)
id ) equals to 1 and 0
separately.

Step2. Conditioned Generation.
Instead of directly us-
ing a randomly sampled Zid ∼ N (µid, σid) as input, we
combine it with the mean gender latent code Zmale and
Zf emale:

Z gender
id

= (1−α−β)×Zid+α×Zmale+β×Zf emale (6)

We can set α = 0.5, β = 0.0 to ensure generated results
are all male, or α = 0.0, β = 0.5 to ensure generated re-
sults are all female. We can also gradually decrease α and
increase β at the same time to interpolate a male generation
into female. An example of this is shown in Fig.9 of the
paper.

Age Control

The main idea of age control is similar to the gender con-
trol (Sec 8) with two main differences: (1) Instead of a clas-
sifer ψ for gender classiﬁcation, we use a regressor φ to pre-
dict the true age (in years). (2) We compute an average vec-
tor for Zold and Zyoung separately using the method of sam-
pling Zid with φ(Gid(Zid)) > 50 and φ(Gid(Zid)) < 30.
So the ﬁnal age latent code is represented as:

Z age

id = (1 − α − β) × Zid + α × Zold + β × Zyoung (7)

3D Model Fitting

Given a face scan, or face model, we ﬁrstly convert it into
our albedo and geometry map format by ﬁtting a linear face
model followed by Laplacian warping and attribute transfer.
The ground truth latent code of the input is denoted Zid.
Our goal of ﬁtting is to ﬁnd the latent code Z
id that best
approximates Zid while retaining the embodyment of our
model. To achieve this, one can ﬁnd Z
id that minimizes
M SE(Gid(Z

id), Gid(Zid)) through gradient descent.

In particular, we ﬁrst use the Adam optimizer with a con-
stant learningrate = 1.0 to update the input variable Z
id,
then we update the variables in the Noise Injection Layers
with learningrate = 0.01 to ﬁt those details. Fig.10 in the
paper shows the geometry of the ﬁtting results.

(cid:48)

(cid:48)

(cid:48)

(cid:48)

Low-quality Data Enhancement.

In order to enhance the quality of low-resolution data,
so that it can be better utilized, the data point needs to
be encoded as Zid in our latent space. This is done us-
ing our ﬁtting method 8. The rest of the high ﬁdelity as-
sets are generated using our generative pipeline. Unlike
the ﬁtting procedure, we don’t want true-to-groundtruth ﬁt-
ting which would result in a recreation of a low resolution
model. We instead introduce a discriminator loss to balance
the MSE loss. This provides an additional constraint on re-
ality and quality during gradient descent. Empirically we
give a 0.001 weight to the discriminator loss to balance the
MSE loss. We also use the Adam optimizer with a constant

(cid:48)

learning − rate = 1.0 for this experiment. The attained
variable Z
id is then fed in as the new input, and the process
is iteratively repeated until convergence after about 4000 it-
erations.

Real Time Rendering Assets

To demonstrate the use of additional eye rendering assets
(lacrimal ﬂuid, blend mesh, and eye occlusion) available in
our model, we show a real time rendering of a close up of
an eye and its surrounding skin geometry and material from
scan data in Figure 16. The rendering is performed using
Unreal Engine 4. Materials and shaders are adopted from
the Digital Human project [3].

