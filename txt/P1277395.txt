8
1
0
2
 
b
e
F
 
3
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
5
2
3
9
0
.
1
1
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

TRAINING CONFIDENCE-CALIBRATED CLASSIFIERS
FOR DETECTING OUT-OF-DISTRIBUTION SAMPLES

Honglak Lee§ ,†

Kimin Lee∗
Jinwoo Shin∗
∗Korea Advanced Institute of Science and Technology, Daejeon, Korea
†University of Michigan, Ann Arbor, MI 48109
§Google Brain, Mountain View, CA 94043

Kibok Lee†

ABSTRACT

The problem of detecting whether a test sample is from in-distribution (i.e., train-
ing distribution by a classiﬁer) or out-of-distribution sufﬁciently different from it
arises in many real-world machine learning applications. However, the state-of-art
deep neural networks are known to be highly overconﬁdent in their predictions,
i.e., do not distinguish in- and out-of-distributions. Recently, to handle this is-
sue, several threshold-based detectors have been proposed given pre-trained neu-
ral classiﬁers. However, the performance of prior works highly depends on how
to train the classiﬁers since they only focus on improving inference procedures.
In this paper, we develop a novel training method for classiﬁers so that such in-
ference algorithms can work better. In particular, we suggest two additional terms
added to the original loss (e.g., cross entropy). The ﬁrst one forces samples from
out-of-distribution less conﬁdent by the classiﬁer and the second one is for (im-
plicitly) generating most effective training samples for the ﬁrst one. In essence,
our method jointly trains both classiﬁcation and generative neural networks for
out-of-distribution. We demonstrate its effectiveness using deep convolutional
neural networks on various popular image datasets.

1

INTRODUCTION

Deep neural networks (DNNs) have demonstrated state-of-the-art performance on many classiﬁ-
cation tasks, e.g., speech recognition (Hannun et al., 2014), image classiﬁcation (Girshick, 2015),
video prediction (Villegas et al., 2017) and medical diagnosis (Caruana et al., 2015). Even though
DNNs achieve high accuracy, it has been addressed (Lakshminarayanan et al., 2017; Guo et al.,
2017) that they are typically overconﬁdent in their predictions. For example, DNNs trained to clas-
sify MNIST images often produce high conﬁdent probability 91% even for random noise (see the
work of (Hendrycks & Gimpel, 2016)). Since evaluating the quality of their predictive uncertainty
is hard, deploying them in real-world systems raises serious concerns in AI Safety (Amodei et al.,
2016), e.g., one can easily break a secure authentication system that can be unlocked by detecting
the gaze and iris of eyes using DNNs (Shrivastava et al., 2017).

The overconﬁdence issue of DNNs is highly related to the problem of detecting out-of-distribution:
detect whether a test sample is from in-distribution (i.e., training distribution by a classiﬁer) or out-
of-distribution sufﬁciently different from it. Formally, it can be formulated as a binary classiﬁcation
problem. Let an input x ∈ X and a label y ∈ Y = {1, . . . , K} be random variables that follow a
joint data distribution Pin (x, y) = Pin (y|x) Pin (x). We assume that a classiﬁer Pθ (y|x) is trained
on a dataset drawn from Pin (x, y), where θ denotes the model parameter. We let Pout (x) denote
an out-of-distribution which is ‘far away’ from in-distribution Pin (x). Our problem of interest is
determining if input x is from Pin or Pout, possibly utilizing a well calibrated classiﬁer Pθ (y|x).
In other words, we aim to build a detector, g (x) : X → {0, 1}, which assigns label 1 if data is from
in-distribution, and label 0 otherwise.

There have been recent efforts toward developing efﬁcient detection methods where they mostly
have studied simple threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017) uti-
lizing a pre-trained classiﬁer. For each input x, it measures some conﬁdence score q(x) based on a
pre-trained classiﬁer, and compares the score to some threshold δ > 0. Then, the detector assigns

1

Published as a conference paper at ICLR 2018

label 1 if the conﬁdence score q(x) is above δ, and label 0, otherwise. Speciﬁcally, (Hendrycks
& Gimpel, 2016) deﬁned the conﬁdence score as a maximum value of the predictive distribution,
and (Liang et al., 2017) further improved the performance by using temperature scaling (Guo et al.,
2017) and adding small controlled perturbations to the input data. Although such inference methods
are computationally simple, their performances highly depend on the pre-trained classiﬁer. Namely,
they fail to work if the classiﬁer does not separate the maximum value of predictive distribution
well enough with respect to Pin and Pout. Ideally, a classiﬁer should be trained to separate all
class-dependent in-distributions as well as out-of-distribution in the output space. As another line of
research, Bayesian probabilistic models (Li & Gal, 2017; Louizos & Welling, 2017) and ensembles
of classiﬁers (Lakshminarayanan et al., 2017) were also investigated. However, training or inferring
those models are computationally more expensive. This motivates our approach of developing a
new training method for the more plausible simple classiﬁers. Our direction is orthogonal to the
Bayesian and ensemble approaches, where one can also combine them for even better performance.

Contribution. In this paper, we develop such a training method for detecting out-of-distribution
Pout better without losing its original classiﬁcation accuracy. First, we consider a new loss function,
called conﬁdence loss. Our key idea on the proposed loss is to additionally minimize the Kullback-
Leibler (KL) divergence from the predictive distribution on out-of-distribution samples to the uni-
form one in order to give less conﬁdent predictions on them. Then, in- and out-of-distributions are
expected to be more separable. However, optimizing the conﬁdence loss requires training samples
from out-of-distribution, which are often hard to sample: a priori knowledge on out-of-distribution
is not available or its underlying space is too huge to cover. To handle the issue, we consider a
new generative adversarial network (GAN) (Goodfellow et al., 2014) for generating most effective
samples from Pout. Unlike the original GAN, the proposed GAN generates ‘boundary’ samples in
the low-density area of Pin. Finally, we design a joint training scheme minimizing the classiﬁer’s
loss and new GAN loss alternatively, i.e., the conﬁdent classiﬁer improves the GAN, and vice versa,
as training proceeds. Here, we emphasize that the proposed GAN does not need to generate explicit
samples under our scheme, and instead it implicitly encourages training a more conﬁdent classiﬁer.

We demonstrate the effectiveness of the proposed method using deep convolutional neural networks
such as AlexNet (Krizhevsky, 2014) and VGGNet (Szegedy et al., 2015) for image classiﬁcation
tasks on CIFAR (Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), ImageNet (Deng et al.,
2009), and LSUN (Yu et al., 2015) datasets. The classiﬁer trained by our proposed method dras-
tically improves the detection performance of all threshold-based detectors (Hendrycks & Gim-
pel, 2016; Liang et al., 2017) in all experiments. In particular, VGGNet with 13 layers trained by
our method improves the true negative rate (TNR), i.e., the fraction of detected out-of-distribution
(LSUN) samples, compared to the baseline: 14.0% → 39.1% and 46.3% → 98.9% on CIFAR-10
and SVHN, respectively, when 95% of in-distribution samples are correctly detected. We also pro-
vide visual understandings on the proposed method using the image datasets. We believe that our
method can be a strong guideline when other researchers will pursue these tasks in the future.

2 TRAINING CONFIDENT NEURAL CLASSIFIERS

In this section, we propose a novel training method for classiﬁers in order to improve the perfor-
mance of prior threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017) (see Ap-
pendix A for more details). Our motivation is that such inference algorithms can work better if the
classiﬁers are trained so that they map the samples from in- and out-of-distributions into the output
space separately. Namely, we primarily focus on training an improved classiﬁer, and then use prior
detectors under the trained model to measure its performance.

2.1 CONFIDENT CLASSIFIER FOR OUT-OF-DISTRIBUTION

Without loss of generality, suppose that the cross entropy loss is used for training. Then, we propose
the following new loss function, termed conﬁdence loss:

min
θ

EPin((cid:98)x,(cid:98)y)

(cid:2) − log Pθ (y = (cid:98)y|(cid:98)x) (cid:3) + βEPout(x)

(cid:2)KL (U (y) (cid:107) Pθ (y|x)) (cid:3),

(1)

where KL denotes the Kullback-Leibler (KL) divergence, U (y) is the uniform distribution and
β > 0 is a penalty parameter. It is highly intuitive as the new loss forces the predictive distribution

2

Published as a conference paper at ICLR 2018

(a)

(b)

(c)

(d)

Figure 1: Illustrating the behavior of classiﬁer under different out-of-distribution training datasets.
We generate the out-of-distribution samples from (a) 2D box [−50, 50]2, and show (b) the corre-
sponding decision boundary of classiﬁer. We also generate the out-of-distribution samples from (c)
2D box [−20, 20]2, and show (d) the corresponding decision boundary of classiﬁer.

on out-of-distribution samples to be closer to the uniform one, i.e., zero conﬁdence, while that
for samples from in-distribution still follows the label-dependent probability. In other words, the
proposed loss is designed for assigning higher maximum prediction values, i.e., maxy Pθ (y|x), to
in-distribution samples than out-of-distribution ones. Here, a caveat is that adding the KL divergence
term might degrade the classiﬁcation performance. However, we found that it is not the case due
to the high expressive power of deep neural networks, while in- and out-of-distributions become
more separable with respect to the maximum prediction value by optimizing the conﬁdence loss
(see Section 3.1 for supporting experimental results).

We remark that minimizing a similar KL loss was studied recently for different purposes (Lee et al.,
2017; Pereyra et al., 2017). Training samples for minimizing the KL divergence term is explicitly
given in their settings while we might not. Ideally, one has to sample all (almost inﬁnite) types of out-
of-distribution to minimize the KL term in (1), or require some prior information on testing out-of-
distribution for efﬁcient sampling. However, this is often infeasible and fragile. To address the issue,
we suggest to sample out-of-distribution close to in-distribution, which could be more effective in
improving the detection performance, without any assumption on testing out-of-distribution.

In order to explain our intuition in details, we consider a binary classiﬁcation task on a simple ex-
ample, where each class data is drawn from a Gaussian distribution and entire data space is bounded
by 2D box [−50, 50]2 for visualization. We apply the conﬁdence loss to simple fully-connected
neural networks (2 hidden layers and 500 hidden units for each layer) using different types of out-
of-distribution training samples. First, as shown in Figure 1(a), we construct an out-of-distribution
training dataset of 100 (green) points using rejection sampling on the entire data space [−50, 50]2.
Figure 1(b) shows the decision boundary of classiﬁer optimizing the conﬁdence loss on the corre-
sponding dataset. One can observe that a classiﬁer still shows overconﬁdent predictions (red and
blue regions) near the labeled in-distribution region. On the other hand, if we construct a training
out-of-distribution dataset of 100 points from [−20, 20]2, i.e., closer to target, in-distribution space
(see Figure 1(c)), a classiﬁer produces conﬁdent predictions only on the labeled region and zero
conﬁdence on the remaining in the entire data space [−50, 50]2 as shown in Figure 1(d). If one
increases the number of training out-of-distribution samples which are generated from the entire
space, i.e., [−50, 50]2, Figure 1(b) is expected to be similar to Figure 1(d). In other words, one need
more samples in order to train a conﬁdent classiﬁer if samples are generated from the entire space.
However, this might be impossible and not efﬁcient since the number of out-of-distribution training
samples might be almost inﬁnite to cover its entire, huge actual data space. This implies that training
out-of-distribution samples nearby the in-distribution region could be more effective in improving
the detection performance. Our underlying intuition is that the effect of boundary of in-distribution
region might propagate to the entire out-of-distribution space. Our experimental results in Section
3.1 also support this: realistic images are more useful as training out-of-distribution than synthetic
datasets (e.g., Gaussian noise) for improving the detection performance when we consider an image
classiﬁcation task. This motivates us to develop a new generative adversarial network (GAN) for
generating such effective out-of-distribution samples.

2.2 ADVERSARIAL GENERATOR FOR OUT-OF-DISTRIBUTION

In this section, we introduce a new training method for learning a generator of out-of-distribution
inspired by generative adversarial network (GAN) (Goodfellow et al., 2014). We will ﬁrst assume

3

Published as a conference paper at ICLR 2018

that the classiﬁer for in-distribution is ﬁxed, and also describe the joint learning framework in the
next section.

The GAN framework consists of two main components: discriminator D and generator G. The
generator maps a latent variable z from a prior distribution Ppri (z) to generated outputs G (z), and
discriminator D : X → [0, 1] represents a probability that sample x is from a target distribution.
Suppose that we want to recover the in-distribution Pin(x) using the generator G. Then, one can
optimize the following min-max objective for forcing PG ≈ Pin:
EPin(x)

(cid:2) log D (x) (cid:3) + EPpri(z)

(cid:2) log (1 − D (G (z))) (cid:3).

(2)

min
G

max
D

However, unlike the original GAN, we want to make the generator recover an effective out-of-
distribution Pout instead of Pin. To this end, we propose the following new GAN loss:

min
G

max
D

β EPG(x)

(cid:124)

(cid:124)

+ EPin(x)

(cid:123)(cid:122)
(a)

(cid:2)KL (U (y) (cid:107) Pθ (y|x)) (cid:3)
(cid:125)
(cid:2) log (1 − D (x)) (cid:3)
,
(cid:125)

(cid:2) log D (x) (cid:3) + EPG(x)

(cid:123)(cid:122)
(b)

(3)

where θ is the model parameter of a classiﬁer trained on in-distribution. The above objective can be
interpreted as follows: the ﬁrst term (a) corresponds to a replacement of the out-of-distribution Pout
in (1)’s KL loss with the generator distribution PG. One can note that this forces the generator to
generate low-density samples since it can be interpreted as minimizing the log negative likelihood
of in-distribution using the classiﬁer, i.e., Pin(x) ≈ exp (KL (U (y) (cid:107) Pθ (y|x))) . We remark that
this approximation is also closely related to the inception score (Salimans et al., 2016) which is
popularly used as a quantitative measure of visual ﬁdelity of the samples. The second term (b) cor-
responds to the original GAN loss since we would like to have out-of-distribution samples close to
in-distribution, as mentioned in Section 2.1. Suppose that the model parameter of classiﬁer θ is set
appropriately such that the classiﬁer produces the uniform distribution for out of distribution sam-
ples. Then, the KL divergence term (a) in (3) is approximately 0 no matter what out-of-distribution
samples are generated. However, if the samples are far away from boundary, the GAN loss (b) in (3)
should be high, i.e., the GAN loss forces having samples being not too far from the in-distribution
space. Therefore, one can expect that proposed loss can encourage the generator to produce the
samples which are on the low-density boundary of the in-distribution space. We also provide its
experimental evidences in Section 3.2.

We also remark that (Dai et al., 2017) consider a similar GAN generating samples from out-of-
distribution for the purpose of semi-supervised learning. The authors assume the existence of a pre-
trained density estimation model such as PixelCNN++ (Salimans et al., 2017) for in-distribution,
but such a model might not exist and be expensive to train in general. Instead, we use much simpler
conﬁdent classiﬁers for approximating the density. Hence, under our fully-supervised setting, our
GAN is much easier to train and more suitable.

2.3

JOINT TRAINING METHOD OF CONFIDENT CLASSIFIER AND ADVERSARIAL GENERATOR

min
G

min
θ

max
D

In the previous section, we suggest training the proposed GAN using a pre-trained conﬁdent classi-
ﬁer. We remind that the converse is also possible, i.e., the motivation of having such a GAN is for
training a better classiﬁer. Hence, two models can be used for improving each other. This naturally
suggests a joint training scheme where the conﬁdent classiﬁer improves the proposed GAN, and vice
versa, as training proceeds. Speciﬁcally, we suggest the following joint objective function:
(cid:2) − log Pθ (y = (cid:98)y|(cid:98)x) (cid:3)
(cid:2)KL (U (y) (cid:107) Pθ (y|x)) (cid:3)
(cid:125)
(cid:125)
(cid:2) log D ((cid:98)x) (cid:3) + EPG(x)

+β EPG(x)
(cid:124)

EPin((cid:98)x,(cid:98)y)
(cid:124)

(cid:123)(cid:122)
(e)
The classiﬁer’s conﬁdence loss corresponds to (c) + (d), and the proposed GAN loss corresponds
to (d) + (e), i.e., they share the KL divergence term (d) under joint training. To optimize the above
objective efﬁciently, we propose an alternating algorithm, which optimizes model parameters {θ}
of classiﬁer and GAN models {G, D} alternatively as shown in Algorithm 1. Since the algorithm
monotonically decreases the objective function, it is guaranteed to converge.

(cid:123)(cid:122)
(d)
(cid:2) log (1 − D (x)) (cid:3).
(cid:125)

+EPin((cid:98)x)
(cid:124)

(cid:123)(cid:122)
(c)

(4)

4

Published as a conference paper at ICLR 2018

Algorithm 1 Alternating minimization for detecting and generating out-of-distribution.

repeat

/∗ Update proposed GAN ∗/
Sample {z1, . . . , zM } and {x1, . . . , xM } from prior Ppri (z) and and in-distribution Pin (x),

respectively, and update the discriminator D by ascending its stochastic gradient of

1
M

M
(cid:88)

(cid:104)

i=1

log D (xi) + log (1 − D (G (zi)))

(cid:105)
.

Sample {z1, . . . , zM } from prior Ppri (z), and update the generator G by descending its

stochastic gradient of

1
M

M
(cid:88)

(cid:104)

i=1

log (1 − D (G (zi)))

+

KL (U (y) (cid:107) Pθ (y|G (zi)))

(cid:105)
.

(cid:105)

β
M

M
(cid:88)

(cid:104)

i=1

/∗ Update conﬁdent classiﬁer ∗/
Sample {z1, . . . , zM } and {(x1, y1) , . . . , (xM , yM )} from prior Ppri (z) and in-distribution

Pin (x, y), respectively, and update the classiﬁer θ by descending its stochastic gradient of

− log Pθ (y = yi|xi) + βKL (U (y) (cid:107) Pθ (y|G (zi)))

(cid:105)

.

1
M

M
(cid:88)

(cid:104)

i=1

until convergence

3 EXPERIMENTAL RESULTS

We demonstrate the effectiveness of our proposed method using various datasets: CIFAR
(Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), ImageNet (Deng et al., 2009), LSUN
(Yu et al., 2015) and synthetic (Gaussian) noise distribution. We train convolutional neural networks
(CNNs) including VGGNet (Szegedy et al., 2015) and AlexNet (Krizhevsky, 2014) for classifying
CIFAR-10 and SVHN datasets. The corresponding test dataset is used as the in-distribution (pos-
itive) samples to measure the performance. We use realistic images and synthetic noises as the
out-of-distribution (negative) samples. For evaluation, we measure the following metrics using the
threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017): the true negative rate
(TNR) at 95% true positive rate (TPR), the area under the receiver operating characteristic curve
(AUROC), the area under the precision-recall curve (AUPR), and the detection accuracy, where
larger values of all metrics indicate better detection performances. Due to the space limitation, more
explanations about datasets, metrics and network architectures are given in Appendix B.1

In-dist

Out-of-dist

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

SVHN

93.82 / 94.23

CIFAR-10 (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

CIFAR-10

80.14 / 80.56

Cross entropy loss / Conﬁdence loss

47.4 / 99.9
49.0 / 100.0
46.3 / 100.0
56.1 / 100.0

13.7 / 99.8
13.6 / 9.9
14.0 / 10.5
2.8 / 3.3

62.6 / 99.9
64.6 / 100.0
61.8 / 100.0
72.0 / 100.0

46.6 / 99.9
39.6 / 31.8
40.7 / 34.8
10.2 / 14.1

78.6 / 99.9
79.6 / 100.0
78.2 / 100.0
83.4 / 100.0

66.6 / 99.8
62.6 / 58.6
63.2 / 60.2
50.0 / 50.0

71.6 / 99.9
72.7 / 100.0
71.1 / 100.0
77.2 / 100.0

61.4 / 99.9
58.3 / 55.3
58.7 / 56.4
48.1 / 49.4

91.2 / 99.4
91.6 / 99.4
90.8 / 99.4
92.8 / 99.4

73.5 / 99.8
71.0 / 66.1
71.5 / 68.0
39.9 / 47.0

Table 1: Performance of the baseline detector (Hendrycks & Gimpel, 2016) using VGGNet. All val-
ues are percentages and boldface values indicate relative the better results. For each in-distribution,
we minimize the KL divergence term in (1) using training samples from an out-of-distribution
dataset denoted by “seen”, where other “unseen” out-of-distributions were only used for testing.

1Our code is available at https://github.com/alinlab/Confident_classifier.

5

Published as a conference paper at ICLR 2018

(a) Cross entropy loss

(b) Conﬁdence loss in (1)

(c) ROC curve

Figure 2: For all experiments in (a), (b) and (c), we commonly use the SVHN dataset for in-
distribution. Fraction of the maximum prediction value in softmax scores trained by (a) cross en-
tropy loss and (b) conﬁdence loss: the x-axis and y-axis represent the maximum prediction value
and the fraction of images receiving the corresponding score, respectively. The receiver operating
characteristic (ROC) curves under different losses are reported in (c): the red curve corresponds to
the ROC curve of a model trained by optimizing the naive cross entropy loss, whereas other ones
correspond to the ROC curves of models trained by optimizing the conﬁdence loss. The KL diver-
gence term in the conﬁdence loss is optimized using explicit out-of-distribution datasets indicated
in the parentheses, e.g., Conﬁdent loss (LSUN) means that we use the LSUN dataset for optimizing
the KL divergence term.

3.1 EFFECTS OF CONFIDENCE LOSS

We ﬁrst verify the effect of conﬁdence loss in (1) trained by some explicit, say seen, out-of-
distribution datasets. First, we compare the quality of conﬁdence level by applying various training
losses. Speciﬁcally, the softmax classiﬁer is used and simple CNNs (two convolutional layers fol-
lowed by three fully-connected layers) are trained by minimizing the standard cross entropy loss on
SVHN dataset. We also apply the conﬁdence loss to the models by additionally optimizing the KL
divergence term using CIFAR-10 dataset (as training out-of-distribution). In Figure 2(a) and 2(b),
we report distributions of the maximum prediction value in softmax scores to evaluate the separation
quality between in-distribution (i.e., SVHN) and out-of-distributions. It is clear that there exists a
better separation between the SVHN test set (red bar) and other ones when the model is trained by
the conﬁdence loss. Here, we emphasize that the maximum prediction value is also low on even un-
trained (unseen) out-of-distributions, e.g., TinyImageNet, LSUN and synthetic datasets. Therefore,
it is expected that one can distinguish in- and out-of-distributions more easily when a classiﬁer is
trained by optimizing the conﬁdence loss. To verify that, we obtain the ROC curve using the baseline
detector (Hendrycks & Gimpel, 2016) that computes the maximum value of predictive distribution
on a test sample and classiﬁes it as positive (i.e., in-distribution) if the conﬁdence score is above
some threshold. Figure 2(c) shows the ROC curves when we optimize the KL divergence term on
various datasets. One can observe that realistic images such as TinyImageNet (aqua line) and LSUN
(green line) are more useful than synthetic datasets (orange line) for improving the detection perfor-
mance. This supports our intuition that out-of-distribution samples close to in-distribution could be
more effective in improving the detection performance as we discussed in Section 2.1.

We indeed evaluate the performance of the baseline detector for out-of-distribution using large-
scale CNNs, i.e., VGGNets with 13 layers, under various training scenarios, where more results on
AlexNet and ODIN detector (Liang et al., 2017) can be found in Appendix C (the overall trends
of results are similar). For optimizing the conﬁdence loss in (1), SVHN and CIFAR-10 training
datasets are used for optimizing the KL divergence term for the cases when the in-distribution is
CIFAR-10 and SVHN, respectively. Table 1 shows the detection performance for each in- and
out-of-distribution pair. When the in-distribution is SVHN, the classiﬁer trained by our method
drastically improves the detection performance across all out-of-distributions without hurting its
original classiﬁcation performance. However, when the in-distribution is CIFAR-10, the conﬁdence
loss does not improve the detection performance in overall, where we expect that this is because
the trained/seen SVHN out-of-distribution does not effectively cover all tested out-of-distributions.
Our joint conﬁdence loss in (4), which was designed under the intuition, resolves the issue of the
CIFAR-10 (in-distribution) classiﬁcation case in Table 1 (see Figure 4(b)).

6

Published as a conference paper at ICLR 2018

(a)

(b)

(c)

(d)

Figure 3: The generated samples from original GAN (a)/(c) and proposed GAN (b)/(d). In (a)/(b),
the grey area is the 2D histogram of training in-distribution samples drawn from a mixture of two
Gaussian distributions and red points indicate generated samples by GANs.

(a) In-distribution: SVHN

(b) In-distribution: CIFAR-10

Figure 4: Performances of the baseline detector (Hendrycks & Gimpel, 2016) under various training
losses. For training models by the conﬁdence loss, the KL divergence term is optimized using
samples indicated in the parentheses. For fair comparisons, we only plot the performances for
unseen out-of-distributions, where those for seen out-of-distributions (used for minimizing the KL
divergence term in (1)) can be found in Table 1.

3.2 EFFECTS OF ADVERSARIAL GENERATOR AND JOINT CONFIDENCE LOSS

In this section, we verify the effect of the proposed GAN in Section 2.2 and evaluate the detection
performance of the joint conﬁdence loss in (4). To verify that the proposed GAN can produce the
samples nearby the low-density boundary of the in-distribution space, we ﬁrst compare the gener-
ated samples by original GAN and proposed GAN on a simple example where the target distribution
is a mixture of two Gaussian distributions. For both the generator and discriminator, we use fully-
connected neural networks with 2 hidden layers. For our method, we use a pre-trained classiﬁer
which minimizes the cross entropy on target distribution samples and the KL divergence on out-of-
distribution samples generated by rejection sampling on a bounded 2D box. As shown in Figure
3(a), the samples of original GAN cover the high-density area of the target distribution while those
of proposed GAN does its boundary one (see Figure 3(b)). We also compare the generated samples
of original and proposed GANs on MNIST dataset (LeCun et al., 1998), which consists of hand-
written digits. For this experiment, we use deep convolutional GANs (DCGANs) (Radford et al.,
2015). In this case, we use a pre-trained classiﬁer which minimizes the cross entropy on MNIST

7

Published as a conference paper at ICLR 2018

(a) In-distribution: SVHN

(b) In-distribution: CIFAR-10

Figure 5: Guided gradient (sensitivity) maps of the top-1 predicted class with respect to the input
image under various training losses.

training samples and the KL divergence on synthetic Gaussian noises. As shown in Figure 3(c)
and 3(d), samples of original GAN looks more like digits than those of proposed GAN. Somewhat
interestingly, the proposed GAN still generates some new digit-like images.

We indeed evaluate the performance of our joint conﬁdence loss in (4) utilizing the proposed GAN.
To this end, we use VGGNets (as classiﬁers) and DCGANs (as GANs). We also test a variant of con-
ﬁdence loss which optimizes the KL divergence term on samples from a pre-trained original GAN
(implicitly) modeling the in-distribution. One can expect that samples from the original GAN can be
also useful for improving the detection performance since it may have bad generalization properties
(Arora et al., 2017) and generate a few samples on the low-density boundary as like the proposed
GAN. Figure 4 shows the performance of the baseline detector for each in- and out-of-distribution
pair. First, observe that the joint conﬁdence loss (blue bar) outperforms the conﬁdence loss with
some explicit out-of-distribution datasets (green bar). This is quite remarkable since the former
is trained only using in-distribution datasets, while the latter utilizes additional out-of-distribution
datasets. We also remark that our methods signiﬁcantly outperform the baseline cross entropy loss
(red bar) in all cases without harming its original classiﬁcation performances (see Table 2 in Ap-
pendix C). Interestingly, the conﬁdence loss with the original GAN (orange bar) is often (but not
always) useful for improving the detection performance, whereas that with the proposed GAN (blue
bar) still outperforms it in all cases.

Finally, we also provide visual interpretations of models using the guided gradient maps (Sprin-
genberg et al., 2014). Here, the gradient can be interpreted as an importance value of each pixel
which inﬂuences on the classiﬁcation decision. As shown in Figure 5, the model trained by the
cross entropy loss shows sharp gradient maps for both samples from in- and out-of-distributions,
whereas models trained by the conﬁdence losses do only on samples from in-distribution. For the
case of SVHN in-distribution, all conﬁdence losses gave almost zero gradients, which matches to
the results in Figure 4(a): their detection performances are almost perfect. For the case of CIFAR-
10 distribution, one can now observe that there exists some connection between gradient maps and
detection performances. This is intuitive because for detecting samples from out-of-distributions
better, the classiﬁer should look at more pixels as similar importance and the KL divergence term
forces it. We think that our visualization results might give some ideas in future works for developing
better inference methods for detecting out-of-distribution under our models.

4 CONCLUSION

In this paper, we aim to develop a training method for neural classiﬁcation networks for detecting
out-of-distribution better without losing its original classiﬁcation accuracy. In essence, our method
jointly trains two models for detecting and generating out-of-distribution by minimizing their losses
alternatively. Although we primarily focus on image classiﬁcation in our experiments, our method
can be used for any classiﬁcation tasks using deep neural networks. It is also interesting future
directions applying our methods for other related tasks: regression (Malinin et al., 2017), network
calibration (Guo et al., 2017), Bayesian probabilistic models (Li & Gal, 2017; Louizos & Welling,
2017), ensemble (Lakshminarayanan et al., 2017) and semi-supervised learning (Dai et al., 2017).

8

Published as a conference paper at ICLR 2018

ACKNOWLEDGEMENTS

This work was supported in part by the Institute for Information & communications Technology
Promotion(IITP) grant funded by the Korea government(MSIT) (No.2017-0-01778, Development
of Explainable Human-level Deep Machine Learning Inference Framework), the ICT R&D program
of MSIP/IITP [R-20161130-004520, Research on Adaptive Machine Learning Technology Devel-
opment for Intelligent Autonomous Digital Companion], DARPA Explainable AI (XAI) program
#313498 and Sloan Research Fellowship.

REFERENCES

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Con-

crete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). In International Conference on Machine Learning (ICML),
2017.

Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible
In ACM

models for healthcare: Predicting pneumonia risk and hospital 30-day readmission.
SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015.

Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan Salakhutdinov. Good semi-
supervised learning that requires a bad gan. In Advances in neural information processing systems
(NIPS), 2017.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale

hierarchical image database. In Computer Vision and Pattern Recognition (CVPR), 2009.

Ross Girshick. Fast r-cnn. In International Conference on Computer Vision (ICCV), 2015.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems (NIPS), 2014.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural

networks. In International Conference on Machine Learning (ICML), 2017.

Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger,
Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end
speech recognition. arXiv preprint arXiv:1412.5567, 2014.

Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution
examples in neural networks. In International Conference on Learning Representations (ICLR),
2016.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-
nov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning (ICML), 2015.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations (ICLR), 2014.

Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint

arXiv:1404.5997, 2014.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic-
tive uncertainty estimation using deep ensembles. In Advances in neural information processing
systems (NIPS), 2017.

9

Published as a conference paper at ICLR 2018

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Kimin Lee, Changho Hwang, KyoungSoo Park, and Jinwoo Shin. Conﬁdent multiple choice learn-

ing. In International Conference on Machine Learning (ICML), 2017.

Yingzhen Li and Yarin Gal. Dropout inference in bayesian neural networks with alpha-divergences.

In International Conference on Machine Learning (ICML), 2017.

Shiyu Liang, Yixuan Li, and R Srikant. Principled detection of out-of-distribution examples in

neural networks. arXiv preprint arXiv:1706.02690, 2017.

Christos Louizos and Max Welling. Multiplicative normalizing ﬂows for variational bayesian neural

networks. In International Conference on Machine Learning (ICML), 2017.

Andrey Malinin, Anton Ragni, Kate Knill, and Mark Gales. Incorporating uncertainty into deep
In Proceedings of the 55th Annual Meeting of the
learning for spoken language assessment.
Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pp. 45–50, 2017.

Mahdi Pakdaman Naeini, Gregory F Cooper, and Milos Hauskrecht. Obtaining well calibrated

probabilities using bayesian binning. In AAAI, 2015.

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, volume 2011, pp. 5, 2011.

Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing conﬁdent output distributions. arXiv preprint arXiv:1701.06548,
2017.

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In International Conference on Learning Repre-
sentations (ICLR), 2015.

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
In Advances in neural information processing systems

Improved techniques for training gans.
(NIPS), 2016.

Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the
In International

pixelcnn with discretized logistic mixture likelihood and other modiﬁcations.
Conference on Learning Representations (ICLR), 2017.

Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russ Webb.
In Computer

Learning from simulated and unsupervised images through adversarial training.
Vision and Pattern Recognition (CVPR), 2017.

Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for

simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Computer Vision and Pattern Recognition (CVPR), 2015.

Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee. Learning
to generate long-term future via hierarchical prediction. In International Conference on Machine
Learning (ICML), 2017.

Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.

Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. In

International Conference on Learning Representations (ICLR), 2017.

10

Published as a conference paper at ICLR 2018

A THRESHOLD-BASED DETECTORS

In this section, we formally describe the detection procedure of threshold-based detectors
(Hendrycks & Gimpel, 2016; Liang et al., 2017). For each data x, it measures some conﬁdence
score q(x) by feeding the data into a pre-trained classiﬁer. Here, (Hendrycks & Gimpel, 2016) de-
ﬁned the conﬁdence score as a maximum value of the predictive distribution, and (Liang et al., 2017)
further improved the performance by processing the predictive distribution (see Appendix C.3 for
more details). Then, the detector, g (x) : X → {0, 1}, assigns label 1 if the conﬁdence score q(x) is
above some threshold δ, and label 0, otherwise:

(cid:26)

g (x) =

1
0

if q(x) ≥ δ,
otherwise.

For this detector, we have to ﬁnd a score threshold so that some positive examples are classiﬁed
correctly, but this depends upon the trade-off between false negatives and false positives. To handle
this issue, we use threshold-independent evaluation metrics such as area under the receiver operating
characteristic curve (AUORC) and detection accuracy (see Appendix B).

B EXPERIMENTAL SETUPS IN SECTION 3

Datasets. We train deep models such as VGGNet (Szegedy et al., 2015) and AlexNet (Krizhevsky,
2014) for classifying CIFAR-10 and SVHN datasets: the former consists of 50,000 training and
10,000 test images with 10 image classes, and the latter consists of 73,257 training and 26,032 test
images with 10 digits.2 The corresponding test dataset are used as the in-distribution (positive)
samples to measure the performance. We use realistic images and synthetic noises as the out-of-
distribution (negative) samples: the TinyImageNet consists of 10,000 test images with 200 image
classes from a subset of ImageNet images. The LSUN consists of 10,000 test images of 10 different
scenes. We downsample each image of TinyImageNet and LSUN to size 32 × 32. The Gaussian
noise is independently and identically sampled from a Gaussian distribution with mean 0.5 and
variance 1. We clip each pixel value into the range [0, 1].

Detailed CNN structure and training. The simple CNN that we use for evaluation shown in Figure
2 consists of two convolutional layers followed by three fully-connected layers. Convolutional layers
have 128 and 256 ﬁlters, respectively. Each convolutional layer has a 5 × 5 receptive ﬁeld applied
with a stride of 1 pixel each followed by max pooling layer which pools 2 × 2 regions at strides of
2 pixels. AlexNet (Krizhevsky, 2014) consists of ﬁve convolutitonal layers followed by three fully-
connected layers. Convolutional layers have 64, 192, 384, 256 and 256 ﬁlters, respectively. First and
second convolutional layers have a 5×5 receptive ﬁeld applied with a stride of 1 pixel each followed
by max pooling layer which pools 3 × 3 regions at strides of 2 pixels. Other convolutional layers
have a 3 × 3 receptive ﬁeld applied with a stride of 1 pixel followed by max pooling layer which
pools 2 × 2 regions at strides of 2 pixels. Fully-connected layers have 2048, 1024 and 10 hidden
units, respectively. Dropout (Hinton et al., 2012) was applied to only fully-connected layers of the
network with the probability of retaining the unit being 0.5. All hidden units are ReLUs. Figure
6 shows the detailed structure of VGGNet (Szegedy et al., 2015) with three fully-connected layers
and 10 convolutional layers. Each ConvReLU box in the ﬁgure indicates a 3 × 3 convolutional layer
followed by ReLU activation. Also, all max pooling layers have 2 × 2 receptive ﬁelds with stride 2.
Dropout was applied to only fully-connected layers of the network with the probability of retaining
the unit being 0.5. For all experiments, the softmax classiﬁer is used, and each model is trained by
optimizing the objective function using Adam learning rule (Kingma & Ba, 2014). For each out-of-
distribution dataset, we randomly select 1,000 images for tuning the penalty parameter β, mini-batch
size and learning rate. The penalty parameter is chosen from β ∈ {0, 0.1, . . . 1.9, 2}, the mini-batch
size is chosen from {64, 128} and the learning rate is chosen from {0.001, 0.0005, 0.0002}. The
optimal parameters are chosen to minimize the detection error on the validation set. We drop the
learning rate by 0.1 at 60 epoch and models are trained for total 100 epochs. The best test result is
reported for each method.

Performance metrics. We measure the following metrics using threshold-based detectors:

2We do not use the extra SVHN dataset for training.

11

Published as a conference paper at ICLR 2018

• True negative rate (TNR) at 95% true positive rate (TPR). Let TP, TN, FP, and FN de-
note true positive, true negative, false positive and false negative, respectively. We measure
TNR = TN / (FP+TN), when TPR = TP / (TP+FN) is 95%.

• Area under the receiver operating characteristic curve (AUROC). The ROC curve is a
graph plotting TPR against the false positive rate = FP / (FP+TN) by varying a threshold.
• Area under the precision-recall curve (AUPR). The PR curve is a graph plotting the
precision = TP / (TP+FP) against recall = TP / (TP+FN) by varying a threshold. AUPR-IN
(or -OUT) is AUPR where in- (or out-of-) distribution samples are speciﬁed as positive.
• Detection accuracy. This metric corresponds to the maximum classiﬁcation proba-
(cid:8)Pin (q (x) ≤ δ) P (x is from Pin) +
bility over all possible thresholds δ: 1 − minδ
Pout (q (x) > δ) P (x is from Pout) (cid:9), where q(x) is a conﬁdent score such as a maxi-
mum value of softmax. We assume that both positive and negative examples have equal
probability of appearing in the test set, i.e., P (x is from Pin) = P (x is from Pout) = 0.5.

Note that AUROC, AUPR and detection accuracy are threshold-independent evaluation metrics.

Figure 6: Detailed structure of VGGNet with 13 layers.

Generating samples on a simple example. As shown in Figure 3(a) and Figure 3(b), we compare
the generated samples by original GAN and proposed GAN on a simple example where the target
distribution is a mixture of two Gaussian distributions. For both the generator and discriminator, we
use fully-connected neural networks with 2 hidden layers and 500 hidden units for each layer. For
all layers, we use ReLU activation function. We use a 100-dimensional Gaussian prior for the latent
variable z. For our method, we pre-train the simple fully-connected neural networks (2 hidden layers
and 500 ReLU units for each layer) by minimizing the cross entropy on target distribution samples
and the KL divergence on out-of-distribution samples generated by rejection sampling on bounded
2D box [−10, 10]2. The penalty parameter β is set to 1. We use ADAM learning rule (Kingma &
Ba, 2014) with a mini-batch size of 400. The initial learning rate is set to 0.002, and we train for
total 100 epochs.

Generating samples on MNIST. As shown in Figure 3(c) and Figure 3(d), we compare the gener-
ated samples of original and proposed GANs on MNIST dataset, which consists of greyscale images,
each containing a digit 0 to 9 with 60,000 training and 10,000 test images. We expand each image
to size 3 × 32 × 32. For both the generator and discriminator, we use deep convolutional GANs
(DCGANs) (Radford et al., 2015). The discriminator and generator consist of four convolutional
and deconvolutional layers, respectively. Convolutional layers have 128, 256, 512 and 1 ﬁlters, re-
spectively. Each convolutional layer has a 4 × 4 receptive ﬁeld applied with a stride of 2 pixel. The
second and third convolutional layers are followed by batch normalization (Ioffe & Szegedy, 2015).
For all layers, we use LeakyReLU activation function. Deconvolutional layers have 512, 256, 128
and 1 ﬁlters, respectively. Each deconvolutional layer has a 4 × 4 receptive ﬁeld applied with a
stride of 2 pixel followed by batch normalization (Ioffe & Szegedy, 2015) and ReLU activation.
For our method, we use a pre-trained simple CNNs (two convolutional layers followed by three
fully-connected layers) by minimizing the cross entropy on MNIST training samples and the KL
divergence on synthetic Gaussian noise. Convolutional layers have 128 and 256 ﬁlters, respectively.
Each convolutional layer has a 5 × 5 receptive ﬁeld applied with a stride of 1 pixel each followed by
max pooling layer which pools 2 × 2 regions at strides of 2 pixels. The penalty parameter β is set
to 1. We use ADAM learning rule (Kingma & Ba, 2014) with a mini-batch size of 128. The initial
learning rate is set to 0.0002, and we train for total 50 epochs.

12

Published as a conference paper at ICLR 2018

C MORE EXPERIMENTAL RESULTS

C.1 CLASSIFICATION PERFORMANCES

Table 2 reports the classiﬁcation accuracy of VGGNets on CIFAR-10 and SVHN datasets under
various training losses shown in Figure 4. One can note that all methods do not degrade the original
classiﬁcation performance, where the differences in classiﬁcation errors of across all tested single
models are at most 1% in our experiments.

In-distribution Cross entropy

Joint conﬁdence loss

Conﬁdence loss
with original GAN

Conﬁdence loss with explicit
out-of-distribution samples

CIFAR-10
SVHN

80.14
93.82

80.27
94.08

81.39
93.81

80.56
94.23

Table 2: Classiﬁcation test set accuracy of VGGNets on CIFAR-10 and SVHN datasets under vari-
ous training losses.

C.2 CALIBRATION EFFECTS OF CONFIDENCE LOSS

We also verify the calibration effects (Guo et al., 2017) of our methods: whether a classiﬁer trained
by our method can indicate when they are likely to be incorrect for test samples from the in-
distribution. In order to evaluate the calibration effects, we measure the expected calibration error
(ECE) (Naeini et al., 2015). Given test data {(x1, y1) , . . . , (xn, yn)}, we group the predictions into
M interval bins (each of size 1/M ). Let Bm be the set of indices of samples whose prediction
conﬁdence falls into the interval ( m−1

M , m
M ]. Then, the accuracy of Bm is deﬁned as follows:
1
|Bm|

1{yi=arg max

Pθ(y|xi)},

(cid:88)

y

i∈Bm

acc(Bm) =

where θ is the model parameters of a classiﬁer and 1A ∈ {0, 1} is the indicator function for event
A. We also deﬁne the conﬁdence of Bm as follows:
1
|Bm|

conf(Bm) =

q(xi),

(cid:88)

i∈Bm

where q(xi) is the conﬁdence of data i. Using these notations, we measure the expected calibration
error (ECE) as follows:

ECE =

|acc(Bm) − conf(Bm)|.

M
(cid:88)

m=1

|Bm|
n

One can note that ECE is zero if the conﬁdence score can represent the true distribution. Table
3 shows the calibration effects of conﬁdence loss when we deﬁne the conﬁdence score q as the
maximum predictive distribution of a classiﬁer. We found that the ECE of a classiﬁer trained by our
methods is lower than that of a classiﬁer trained by the standard cross entropy loss. This implies
that our proposed method is effective at calibrating predictions. We also remark that the temperature
scaling (Guo et al., 2017) provides further improvements under a classiﬁer trained by our joint
conﬁdence loss.

In-distribution

Without temperature scaling

With temperature scaling

Cross entropy loss

Joint conﬁdence loss Cross entropy loss

Joint conﬁdence loss

CIFAR-10

SVHN

18.45

5.30

14.62

5.13

7.07

2.80

6.19

1.39

Table 3: Expected calibration error (ECE) of VGGNets on CIFAR-10 and SVHN datasets under
various training losses. The number of bins M is set to 20. All values are percentages and boldface
values indicate relative the better results.

13

Published as a conference paper at ICLR 2018

C.3 EXPERIMENTAL RESULTS USING ODIN DETECTOR

In this section, we verify the effects of conﬁdence loss using ODIN detector (Liang et al., 2017)
which is an advanced threshold-based detector using temperature scaling (Guo et al., 2017) and
input perturbation. The key idea of ODIN is the temperature scaling which is deﬁned as follows:

Pθ(y = (cid:98)y|x; T ) =

(cid:80)

(cid:98)y(x)/T )

exp (f
y exp (fy(x)/T )

,

where T > 0 is the temperature scaling parameter and f = (f1, . . . , fK) is ﬁnal feature vector of
neural networks. For each data x, ODIN ﬁrst calculates the pre-processed image (cid:98)x by adding the
small perturbations as follows:

x(cid:48) = x − εsign (− (cid:53)x log Pθ(y = (cid:98)y|x; T )) ,
where ε is a magnitude of noise and (cid:98)y is the predicted label. Next, ODIN feeds the pre-
processed data into the classiﬁer, computes the maximum value of scaled predictive distribution,
i.e., maxy Pθ(y|x(cid:48); T ), and classiﬁes it as positive (i.e., in-distribution) if the conﬁdence score is
above some threshold δ.

For ODIN detector, the perturbation noise ε is chosen from {0, 0.0001, 0.001, 0.01}, and the tem-
perature T is chosen from {1, 10, 100, 500, 1000}. The optimal parameters are chosen to minimize
the detection error on the validation set. Figure 7 shows the performance of the OIDN and baseline
detector for each in- and out-of-distribution pair. First, we remark that the baseline detector using
classiﬁers trained by our joint conﬁdence loss (blue bar) typically outperforms the ODIN detec-
tor using classiﬁers trained by the cross entropy loss (orange bar). This means that our classiﬁer
can map in- and out-of-distributions more separately without pre-processing methods such as tem-
perature scaling. The ODIN detector provides further improvements if one uses it with our joint
conﬁdence loss (green bar). In other words, our proposed training method can improve all prior
detection methods.

(a) In-distribution: SVHN

Figure 7: Performances of the baseline detector (Hendrycks & Gimpel, 2016) and ODIN detector
(Liang et al., 2017) under various training losses.

(b) In-distribution: CIFAR-10

C.4 EXPERIMENTAL RESULTS ON ALEXNET

Table 4 shows the detection performance for each in- and out-of-distribution pair when the classiﬁer
is AlexNet (Krizhevsky, 2014), which is one of popular CNN architectures. We remark that they
show similar trends.

14

Published as a conference paper at ICLR 2018

In-dist

Out-of-dist

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

Baseline
(SVHN)

Baseline
(CIFAR-10)

ODIN
(SVHN)

ODIN
(CIFAR-10)

CIFAR-10 (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

CIFAR-10 (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

92.14 / 93.77

76.58 / 76.18

92.14 / 93.77

76.58 / 76.18

Cross entropy loss / Conﬁdence loss

42.0 / 99.9
45.6 / 99.9
44.6 / 100.0
58.6 / 100.0

12.8 / 99.6
10.3 / 10.1
10.7 / 8.1
6.7 / 1.0

55.5 / 99.9
59.5 / 99.9
61.5 / 100.0
82.6 / 100.0

37.1 / 99.6
11.4 / 8.4
13.3 / 7.1
3.8 / 0.0

88.0 / 100.0
89.4 / 100.0
89.8 / 100.0
94.2 / 100.0

71.0 / 99.9
59.2 / 52.1
56.3 / 51.5
49.6 / 13.5

89.1 / 99.2
90.5 / 99.3
91.8 / 99.3
97.0 / 99.3

86.7 / 99.6
69.1 / 65.6
71.9 / 67.1
70.9 / 57.2

83.4 / 99.8
84.3 / 99.9
84.5 / 99.9
88.8 / 100.0

73.2 / 99.6
64.2 / 62.0
64.3 / 61.8
61.3 / 50.0

82.4 / 99.8
83.8 / 99.9
84.8 / 99.9
91.6 / 100.0

79.3 / 99.6
64.4 / 61.8
75.3 / 63.7
69.3 / 40.4

88.7 / 99.9
90.2 / 100.0
90.8 / 100.0
95.5 / 100.0

74.3 / 99.9
63.6 / 59.8
62.3 / 59.5
58.5 / 43.7

85.9 / 100.0
87.5 / 100.0
90.5 / 100.0
97.4 / 100.0

88.1 / 99.9
71.4 / 68.6
67.2 / 72.0
78.1 / 56.1

87.3 / 99.3
88.6 / 99.3
88.4 / 99.3
92.5 / 99.3

70.7 / 99.6
64.4 / 62.3
65.3 / 61.6
59.5 / 32.0

89.0 / 99.2
90.4 / 99.3
91.3 / 99.3
96.4 / 99.3

84.2 / 99.6
64.6 / 60.7
65.3 / 60.5
60.7 / 40.7

Table 4: Performance of the baseline detector (Hendrycks & Gimpel, 2016) and ODIN detector
(Liang et al., 2017) using AlexNet. All values are percentages and boldface values indicate rela-
tive the better results. For each in-distribution, we minimize the KL divergence term in (1) using
training samples from an out-of-distribution dataset denoted by “seen”, where other “unseen” out-
of-distributions were also used for testing.

D MAXIMIZING ENTROPY

One might expect that the entropy of out-of-distribution is expected to be much higher compared to
that of in-distribution since the out-of-distribution is typically on a much larger space than the in-
distribution. Therefore, one can add maximizing the entropy of generator distribution to new GAN
loss in (3) and joint conﬁdence loss in (4). However, maximizing the entropy of generator distribu-
tion is technically challenging since a GAN does not model the generator distribution explicitly. To
handle the issue, one can leverage the pull-away term (PT) (Zhao et al., 2017):

−H (PG (x)) (cid:119) PT (PG (x)) =

1
M (M − 1)

M
(cid:88)

(cid:88)

(cid:32)

i=1

j(cid:54)=i

G (zi)(cid:62) G (zj)
(cid:107)G (zi)(cid:107)(cid:107)G (zj)(cid:107)

(cid:33)2

,

where H (·) denotes the entropy, zi, zj ∼ Ppri (z) and M is the number of samples. Intuitively,
one can expect the effect of increasing the entropy by minimizing PT since it corresponds to the
squared cosine similarity of generated samples. We note that (Dai et al., 2017) also used PT to
maximize the entropy. Similarly as in Section 3.2, we verify the effects of PT using VGGNet. Table
5 shows the performance of the baseline detector for each in- and out-of-distribution pair. We found
that joint conﬁdence loss with PT tends to (but not always) improve the detection performance.
However, since PT increases the training complexity and the gains from PT are relatively marginal
(or controversial), we leave it as an auxiliary option for improving the performance.

In-dist

Out-of-dist

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

SVHN

93.81 / 94.05

CIFAR-10

81.39 / 80.60

CIFAR-10
TinyImageNet
LSUN

SVHN
TinyImageNet
LSUN

Joint conﬁdence loss without PT / with PT

90.1 / 92.3
99.0 / 99.9
98.9 / 100.0

97.6 / 98.1
99.6 / 100.0
99.6 / 100.0

25.4 / 13.2
35.0 / 44.8
39.1 / 49.1

66.8 / 69.5
72.0 / 78.4
75.1 / 80.7

93.6 / 94.6
97.6 / 99.7
97.5 / 99.9

74.2 / 75.1
76.4 / 77.6
77.8 / 78.7

97.7 / 98.2
99.7 / 100.0
99.7 / 100.0

97.9 / 98.7
94.5 / 100.0
95.5 / 100.0

71.3 / 73.5
74.7 / 79.4
77.1 / 81.3

78.3 / 72.0
82.2 / 84.4
83.6 / 85.8

Table 5: Performance of the baseline detector (Hendrycks & Gimpel, 2016) using VGGNets trained
by joint conﬁdence loss with and without pull-away term (PT). All values are percentages and bold-
face values indicate relative the better results.

15

Published as a conference paper at ICLR 2018

E ADDING OUT-OF-DISTRIBUTION CLASS

Instead of forcing the predictive distribution on out-of-distribution samples to be closer to the uni-
form one, one can simply add an additional “out-of-distribution” class to a classiﬁer as follows:

min
θ

EPin((cid:98)x,(cid:98)y)

(cid:2) − log Pθ (y = (cid:98)y|(cid:98)x) (cid:3) + EPout(x)

(cid:2) − log Pθ (y = K + 1|x) (cid:3),

(5)

where θ is a model parameter. Similarly as in Section 3.1, we compare the performance of the
conﬁdence loss with that of above loss in (5) using VGGNets with for image classiﬁcation on SVHN
dataset. To optimize the KL divergence term in conﬁdence loss and the second term of (5), CIFAR-
10 training datasets are used. In order to compare the detection performance, we deﬁne the the
conﬁdence score of input x as 1 − Pθ (y = K + 1|x) in case of (5). Table 6 shows the detection
performance for out-of-distribution. First, the classiﬁer trained by our method often signiﬁcantly
outperforms the alternative adding the new class label. This is because modeling explicitly out-of-
distribution can incur overﬁtting to trained out-of-distribution dataset.

Detector

Out-of-dist

Baseline
detector

ODIN
detector

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

K + 1 class loss in (5) / Conﬁdence loss

79.61 / 80.56

79.61 / 80.56

99.6 / 99.8
0.0 / 9.9
0.0 / 10.5
0.0 / 3.3

99.6 / 99.8
0.3 / 12.2
0.1 / 13.7
0.0 / 8.2

99.8 / 99.9
5.2 / 31.8
5.6 / 34.8
0.1 / 14.1

99.9 / 99.8
47.3 / 70.6
48.3 / 73.1
28.3 / 68.3

99.7 / 99.8
51.3 / 58.6
51.5 / 60.2
50.0 / 50.0

99.1 / 99.8
55.12 / 65.7
55.9 / 67.9
54.4 / 65.4

99.8 / 99.9
50.7 / 55.3
50.8 / 56.4
49.3 / 49.4

99.9 / 99.9
56.6 / 72.7
57.5 / 75.2
47.8 / 74.1

99.9 / 99.8
64.3 / 66.1
71.5 / 68.0
12.2 / 47.0

99.9 / 99.8
44.3 / 65.6
44.7 / 67.8
36.8 / 61.5

Table 6: Performance of the baseline detector (Hendrycks & Gimpel, 2016) and ODIN detector
(Liang et al., 2017) using VGGNet. All values are percentages and boldface values indicate relative
the better results.

16

8
1
0
2
 
b
e
F
 
3
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
5
2
3
9
0
.
1
1
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

TRAINING CONFIDENCE-CALIBRATED CLASSIFIERS
FOR DETECTING OUT-OF-DISTRIBUTION SAMPLES

Honglak Lee§ ,†

Kimin Lee∗
Jinwoo Shin∗
∗Korea Advanced Institute of Science and Technology, Daejeon, Korea
†University of Michigan, Ann Arbor, MI 48109
§Google Brain, Mountain View, CA 94043

Kibok Lee†

ABSTRACT

The problem of detecting whether a test sample is from in-distribution (i.e., train-
ing distribution by a classiﬁer) or out-of-distribution sufﬁciently different from it
arises in many real-world machine learning applications. However, the state-of-art
deep neural networks are known to be highly overconﬁdent in their predictions,
i.e., do not distinguish in- and out-of-distributions. Recently, to handle this is-
sue, several threshold-based detectors have been proposed given pre-trained neu-
ral classiﬁers. However, the performance of prior works highly depends on how
to train the classiﬁers since they only focus on improving inference procedures.
In this paper, we develop a novel training method for classiﬁers so that such in-
ference algorithms can work better. In particular, we suggest two additional terms
added to the original loss (e.g., cross entropy). The ﬁrst one forces samples from
out-of-distribution less conﬁdent by the classiﬁer and the second one is for (im-
plicitly) generating most effective training samples for the ﬁrst one. In essence,
our method jointly trains both classiﬁcation and generative neural networks for
out-of-distribution. We demonstrate its effectiveness using deep convolutional
neural networks on various popular image datasets.

1

INTRODUCTION

Deep neural networks (DNNs) have demonstrated state-of-the-art performance on many classiﬁ-
cation tasks, e.g., speech recognition (Hannun et al., 2014), image classiﬁcation (Girshick, 2015),
video prediction (Villegas et al., 2017) and medical diagnosis (Caruana et al., 2015). Even though
DNNs achieve high accuracy, it has been addressed (Lakshminarayanan et al., 2017; Guo et al.,
2017) that they are typically overconﬁdent in their predictions. For example, DNNs trained to clas-
sify MNIST images often produce high conﬁdent probability 91% even for random noise (see the
work of (Hendrycks & Gimpel, 2016)). Since evaluating the quality of their predictive uncertainty
is hard, deploying them in real-world systems raises serious concerns in AI Safety (Amodei et al.,
2016), e.g., one can easily break a secure authentication system that can be unlocked by detecting
the gaze and iris of eyes using DNNs (Shrivastava et al., 2017).

The overconﬁdence issue of DNNs is highly related to the problem of detecting out-of-distribution:
detect whether a test sample is from in-distribution (i.e., training distribution by a classiﬁer) or out-
of-distribution sufﬁciently different from it. Formally, it can be formulated as a binary classiﬁcation
problem. Let an input x ∈ X and a label y ∈ Y = {1, . . . , K} be random variables that follow a
joint data distribution Pin (x, y) = Pin (y|x) Pin (x). We assume that a classiﬁer Pθ (y|x) is trained
on a dataset drawn from Pin (x, y), where θ denotes the model parameter. We let Pout (x) denote
an out-of-distribution which is ‘far away’ from in-distribution Pin (x). Our problem of interest is
determining if input x is from Pin or Pout, possibly utilizing a well calibrated classiﬁer Pθ (y|x).
In other words, we aim to build a detector, g (x) : X → {0, 1}, which assigns label 1 if data is from
in-distribution, and label 0 otherwise.

There have been recent efforts toward developing efﬁcient detection methods where they mostly
have studied simple threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017) uti-
lizing a pre-trained classiﬁer. For each input x, it measures some conﬁdence score q(x) based on a
pre-trained classiﬁer, and compares the score to some threshold δ > 0. Then, the detector assigns

1

Published as a conference paper at ICLR 2018

label 1 if the conﬁdence score q(x) is above δ, and label 0, otherwise. Speciﬁcally, (Hendrycks
& Gimpel, 2016) deﬁned the conﬁdence score as a maximum value of the predictive distribution,
and (Liang et al., 2017) further improved the performance by using temperature scaling (Guo et al.,
2017) and adding small controlled perturbations to the input data. Although such inference methods
are computationally simple, their performances highly depend on the pre-trained classiﬁer. Namely,
they fail to work if the classiﬁer does not separate the maximum value of predictive distribution
well enough with respect to Pin and Pout. Ideally, a classiﬁer should be trained to separate all
class-dependent in-distributions as well as out-of-distribution in the output space. As another line of
research, Bayesian probabilistic models (Li & Gal, 2017; Louizos & Welling, 2017) and ensembles
of classiﬁers (Lakshminarayanan et al., 2017) were also investigated. However, training or inferring
those models are computationally more expensive. This motivates our approach of developing a
new training method for the more plausible simple classiﬁers. Our direction is orthogonal to the
Bayesian and ensemble approaches, where one can also combine them for even better performance.

Contribution. In this paper, we develop such a training method for detecting out-of-distribution
Pout better without losing its original classiﬁcation accuracy. First, we consider a new loss function,
called conﬁdence loss. Our key idea on the proposed loss is to additionally minimize the Kullback-
Leibler (KL) divergence from the predictive distribution on out-of-distribution samples to the uni-
form one in order to give less conﬁdent predictions on them. Then, in- and out-of-distributions are
expected to be more separable. However, optimizing the conﬁdence loss requires training samples
from out-of-distribution, which are often hard to sample: a priori knowledge on out-of-distribution
is not available or its underlying space is too huge to cover. To handle the issue, we consider a
new generative adversarial network (GAN) (Goodfellow et al., 2014) for generating most effective
samples from Pout. Unlike the original GAN, the proposed GAN generates ‘boundary’ samples in
the low-density area of Pin. Finally, we design a joint training scheme minimizing the classiﬁer’s
loss and new GAN loss alternatively, i.e., the conﬁdent classiﬁer improves the GAN, and vice versa,
as training proceeds. Here, we emphasize that the proposed GAN does not need to generate explicit
samples under our scheme, and instead it implicitly encourages training a more conﬁdent classiﬁer.

We demonstrate the effectiveness of the proposed method using deep convolutional neural networks
such as AlexNet (Krizhevsky, 2014) and VGGNet (Szegedy et al., 2015) for image classiﬁcation
tasks on CIFAR (Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), ImageNet (Deng et al.,
2009), and LSUN (Yu et al., 2015) datasets. The classiﬁer trained by our proposed method dras-
tically improves the detection performance of all threshold-based detectors (Hendrycks & Gim-
pel, 2016; Liang et al., 2017) in all experiments. In particular, VGGNet with 13 layers trained by
our method improves the true negative rate (TNR), i.e., the fraction of detected out-of-distribution
(LSUN) samples, compared to the baseline: 14.0% → 39.1% and 46.3% → 98.9% on CIFAR-10
and SVHN, respectively, when 95% of in-distribution samples are correctly detected. We also pro-
vide visual understandings on the proposed method using the image datasets. We believe that our
method can be a strong guideline when other researchers will pursue these tasks in the future.

2 TRAINING CONFIDENT NEURAL CLASSIFIERS

In this section, we propose a novel training method for classiﬁers in order to improve the perfor-
mance of prior threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017) (see Ap-
pendix A for more details). Our motivation is that such inference algorithms can work better if the
classiﬁers are trained so that they map the samples from in- and out-of-distributions into the output
space separately. Namely, we primarily focus on training an improved classiﬁer, and then use prior
detectors under the trained model to measure its performance.

2.1 CONFIDENT CLASSIFIER FOR OUT-OF-DISTRIBUTION

Without loss of generality, suppose that the cross entropy loss is used for training. Then, we propose
the following new loss function, termed conﬁdence loss:

min
θ

EPin((cid:98)x,(cid:98)y)

(cid:2) − log Pθ (y = (cid:98)y|(cid:98)x) (cid:3) + βEPout(x)

(cid:2)KL (U (y) (cid:107) Pθ (y|x)) (cid:3),

(1)

where KL denotes the Kullback-Leibler (KL) divergence, U (y) is the uniform distribution and
β > 0 is a penalty parameter. It is highly intuitive as the new loss forces the predictive distribution

2

Published as a conference paper at ICLR 2018

(a)

(b)

(c)

(d)

Figure 1: Illustrating the behavior of classiﬁer under different out-of-distribution training datasets.
We generate the out-of-distribution samples from (a) 2D box [−50, 50]2, and show (b) the corre-
sponding decision boundary of classiﬁer. We also generate the out-of-distribution samples from (c)
2D box [−20, 20]2, and show (d) the corresponding decision boundary of classiﬁer.

on out-of-distribution samples to be closer to the uniform one, i.e., zero conﬁdence, while that
for samples from in-distribution still follows the label-dependent probability. In other words, the
proposed loss is designed for assigning higher maximum prediction values, i.e., maxy Pθ (y|x), to
in-distribution samples than out-of-distribution ones. Here, a caveat is that adding the KL divergence
term might degrade the classiﬁcation performance. However, we found that it is not the case due
to the high expressive power of deep neural networks, while in- and out-of-distributions become
more separable with respect to the maximum prediction value by optimizing the conﬁdence loss
(see Section 3.1 for supporting experimental results).

We remark that minimizing a similar KL loss was studied recently for different purposes (Lee et al.,
2017; Pereyra et al., 2017). Training samples for minimizing the KL divergence term is explicitly
given in their settings while we might not. Ideally, one has to sample all (almost inﬁnite) types of out-
of-distribution to minimize the KL term in (1), or require some prior information on testing out-of-
distribution for efﬁcient sampling. However, this is often infeasible and fragile. To address the issue,
we suggest to sample out-of-distribution close to in-distribution, which could be more effective in
improving the detection performance, without any assumption on testing out-of-distribution.

In order to explain our intuition in details, we consider a binary classiﬁcation task on a simple ex-
ample, where each class data is drawn from a Gaussian distribution and entire data space is bounded
by 2D box [−50, 50]2 for visualization. We apply the conﬁdence loss to simple fully-connected
neural networks (2 hidden layers and 500 hidden units for each layer) using different types of out-
of-distribution training samples. First, as shown in Figure 1(a), we construct an out-of-distribution
training dataset of 100 (green) points using rejection sampling on the entire data space [−50, 50]2.
Figure 1(b) shows the decision boundary of classiﬁer optimizing the conﬁdence loss on the corre-
sponding dataset. One can observe that a classiﬁer still shows overconﬁdent predictions (red and
blue regions) near the labeled in-distribution region. On the other hand, if we construct a training
out-of-distribution dataset of 100 points from [−20, 20]2, i.e., closer to target, in-distribution space
(see Figure 1(c)), a classiﬁer produces conﬁdent predictions only on the labeled region and zero
conﬁdence on the remaining in the entire data space [−50, 50]2 as shown in Figure 1(d). If one
increases the number of training out-of-distribution samples which are generated from the entire
space, i.e., [−50, 50]2, Figure 1(b) is expected to be similar to Figure 1(d). In other words, one need
more samples in order to train a conﬁdent classiﬁer if samples are generated from the entire space.
However, this might be impossible and not efﬁcient since the number of out-of-distribution training
samples might be almost inﬁnite to cover its entire, huge actual data space. This implies that training
out-of-distribution samples nearby the in-distribution region could be more effective in improving
the detection performance. Our underlying intuition is that the effect of boundary of in-distribution
region might propagate to the entire out-of-distribution space. Our experimental results in Section
3.1 also support this: realistic images are more useful as training out-of-distribution than synthetic
datasets (e.g., Gaussian noise) for improving the detection performance when we consider an image
classiﬁcation task. This motivates us to develop a new generative adversarial network (GAN) for
generating such effective out-of-distribution samples.

2.2 ADVERSARIAL GENERATOR FOR OUT-OF-DISTRIBUTION

In this section, we introduce a new training method for learning a generator of out-of-distribution
inspired by generative adversarial network (GAN) (Goodfellow et al., 2014). We will ﬁrst assume

3

Published as a conference paper at ICLR 2018

that the classiﬁer for in-distribution is ﬁxed, and also describe the joint learning framework in the
next section.

The GAN framework consists of two main components: discriminator D and generator G. The
generator maps a latent variable z from a prior distribution Ppri (z) to generated outputs G (z), and
discriminator D : X → [0, 1] represents a probability that sample x is from a target distribution.
Suppose that we want to recover the in-distribution Pin(x) using the generator G. Then, one can
optimize the following min-max objective for forcing PG ≈ Pin:
EPin(x)

(cid:2) log D (x) (cid:3) + EPpri(z)

(cid:2) log (1 − D (G (z))) (cid:3).

(2)

min
G

max
D

However, unlike the original GAN, we want to make the generator recover an effective out-of-
distribution Pout instead of Pin. To this end, we propose the following new GAN loss:

min
G

max
D

β EPG(x)

(cid:124)

(cid:124)

+ EPin(x)

(cid:123)(cid:122)
(a)

(cid:2)KL (U (y) (cid:107) Pθ (y|x)) (cid:3)
(cid:125)
(cid:2) log (1 − D (x)) (cid:3)
,
(cid:125)

(cid:2) log D (x) (cid:3) + EPG(x)

(cid:123)(cid:122)
(b)

(3)

where θ is the model parameter of a classiﬁer trained on in-distribution. The above objective can be
interpreted as follows: the ﬁrst term (a) corresponds to a replacement of the out-of-distribution Pout
in (1)’s KL loss with the generator distribution PG. One can note that this forces the generator to
generate low-density samples since it can be interpreted as minimizing the log negative likelihood
of in-distribution using the classiﬁer, i.e., Pin(x) ≈ exp (KL (U (y) (cid:107) Pθ (y|x))) . We remark that
this approximation is also closely related to the inception score (Salimans et al., 2016) which is
popularly used as a quantitative measure of visual ﬁdelity of the samples. The second term (b) cor-
responds to the original GAN loss since we would like to have out-of-distribution samples close to
in-distribution, as mentioned in Section 2.1. Suppose that the model parameter of classiﬁer θ is set
appropriately such that the classiﬁer produces the uniform distribution for out of distribution sam-
ples. Then, the KL divergence term (a) in (3) is approximately 0 no matter what out-of-distribution
samples are generated. However, if the samples are far away from boundary, the GAN loss (b) in (3)
should be high, i.e., the GAN loss forces having samples being not too far from the in-distribution
space. Therefore, one can expect that proposed loss can encourage the generator to produce the
samples which are on the low-density boundary of the in-distribution space. We also provide its
experimental evidences in Section 3.2.

We also remark that (Dai et al., 2017) consider a similar GAN generating samples from out-of-
distribution for the purpose of semi-supervised learning. The authors assume the existence of a pre-
trained density estimation model such as PixelCNN++ (Salimans et al., 2017) for in-distribution,
but such a model might not exist and be expensive to train in general. Instead, we use much simpler
conﬁdent classiﬁers for approximating the density. Hence, under our fully-supervised setting, our
GAN is much easier to train and more suitable.

2.3

JOINT TRAINING METHOD OF CONFIDENT CLASSIFIER AND ADVERSARIAL GENERATOR

min
G

min
θ

max
D

In the previous section, we suggest training the proposed GAN using a pre-trained conﬁdent classi-
ﬁer. We remind that the converse is also possible, i.e., the motivation of having such a GAN is for
training a better classiﬁer. Hence, two models can be used for improving each other. This naturally
suggests a joint training scheme where the conﬁdent classiﬁer improves the proposed GAN, and vice
versa, as training proceeds. Speciﬁcally, we suggest the following joint objective function:
(cid:2) − log Pθ (y = (cid:98)y|(cid:98)x) (cid:3)
(cid:2)KL (U (y) (cid:107) Pθ (y|x)) (cid:3)
(cid:125)
(cid:125)
(cid:2) log D ((cid:98)x) (cid:3) + EPG(x)

+β EPG(x)
(cid:124)

EPin((cid:98)x,(cid:98)y)
(cid:124)

(cid:123)(cid:122)
(e)
The classiﬁer’s conﬁdence loss corresponds to (c) + (d), and the proposed GAN loss corresponds
to (d) + (e), i.e., they share the KL divergence term (d) under joint training. To optimize the above
objective efﬁciently, we propose an alternating algorithm, which optimizes model parameters {θ}
of classiﬁer and GAN models {G, D} alternatively as shown in Algorithm 1. Since the algorithm
monotonically decreases the objective function, it is guaranteed to converge.

(cid:123)(cid:122)
(d)
(cid:2) log (1 − D (x)) (cid:3).
(cid:125)

+EPin((cid:98)x)
(cid:124)

(cid:123)(cid:122)
(c)

(4)

4

Published as a conference paper at ICLR 2018

Algorithm 1 Alternating minimization for detecting and generating out-of-distribution.

repeat

/∗ Update proposed GAN ∗/
Sample {z1, . . . , zM } and {x1, . . . , xM } from prior Ppri (z) and and in-distribution Pin (x),

respectively, and update the discriminator D by ascending its stochastic gradient of

1
M

M
(cid:88)

(cid:104)

i=1

log D (xi) + log (1 − D (G (zi)))

(cid:105)
.

Sample {z1, . . . , zM } from prior Ppri (z), and update the generator G by descending its

stochastic gradient of

1
M

M
(cid:88)

(cid:104)

i=1

log (1 − D (G (zi)))

+

KL (U (y) (cid:107) Pθ (y|G (zi)))

(cid:105)
.

(cid:105)

β
M

M
(cid:88)

(cid:104)

i=1

/∗ Update conﬁdent classiﬁer ∗/
Sample {z1, . . . , zM } and {(x1, y1) , . . . , (xM , yM )} from prior Ppri (z) and in-distribution

Pin (x, y), respectively, and update the classiﬁer θ by descending its stochastic gradient of

− log Pθ (y = yi|xi) + βKL (U (y) (cid:107) Pθ (y|G (zi)))

(cid:105)

.

1
M

M
(cid:88)

(cid:104)

i=1

until convergence

3 EXPERIMENTAL RESULTS

We demonstrate the effectiveness of our proposed method using various datasets: CIFAR
(Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), ImageNet (Deng et al., 2009), LSUN
(Yu et al., 2015) and synthetic (Gaussian) noise distribution. We train convolutional neural networks
(CNNs) including VGGNet (Szegedy et al., 2015) and AlexNet (Krizhevsky, 2014) for classifying
CIFAR-10 and SVHN datasets. The corresponding test dataset is used as the in-distribution (pos-
itive) samples to measure the performance. We use realistic images and synthetic noises as the
out-of-distribution (negative) samples. For evaluation, we measure the following metrics using the
threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017): the true negative rate
(TNR) at 95% true positive rate (TPR), the area under the receiver operating characteristic curve
(AUROC), the area under the precision-recall curve (AUPR), and the detection accuracy, where
larger values of all metrics indicate better detection performances. Due to the space limitation, more
explanations about datasets, metrics and network architectures are given in Appendix B.1

In-dist

Out-of-dist

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

SVHN

93.82 / 94.23

CIFAR-10 (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

CIFAR-10

80.14 / 80.56

Cross entropy loss / Conﬁdence loss

47.4 / 99.9
49.0 / 100.0
46.3 / 100.0
56.1 / 100.0

13.7 / 99.8
13.6 / 9.9
14.0 / 10.5
2.8 / 3.3

62.6 / 99.9
64.6 / 100.0
61.8 / 100.0
72.0 / 100.0

46.6 / 99.9
39.6 / 31.8
40.7 / 34.8
10.2 / 14.1

78.6 / 99.9
79.6 / 100.0
78.2 / 100.0
83.4 / 100.0

66.6 / 99.8
62.6 / 58.6
63.2 / 60.2
50.0 / 50.0

71.6 / 99.9
72.7 / 100.0
71.1 / 100.0
77.2 / 100.0

61.4 / 99.9
58.3 / 55.3
58.7 / 56.4
48.1 / 49.4

91.2 / 99.4
91.6 / 99.4
90.8 / 99.4
92.8 / 99.4

73.5 / 99.8
71.0 / 66.1
71.5 / 68.0
39.9 / 47.0

Table 1: Performance of the baseline detector (Hendrycks & Gimpel, 2016) using VGGNet. All val-
ues are percentages and boldface values indicate relative the better results. For each in-distribution,
we minimize the KL divergence term in (1) using training samples from an out-of-distribution
dataset denoted by “seen”, where other “unseen” out-of-distributions were only used for testing.

1Our code is available at https://github.com/alinlab/Confident_classifier.

5

Published as a conference paper at ICLR 2018

(a) Cross entropy loss

(b) Conﬁdence loss in (1)

(c) ROC curve

Figure 2: For all experiments in (a), (b) and (c), we commonly use the SVHN dataset for in-
distribution. Fraction of the maximum prediction value in softmax scores trained by (a) cross en-
tropy loss and (b) conﬁdence loss: the x-axis and y-axis represent the maximum prediction value
and the fraction of images receiving the corresponding score, respectively. The receiver operating
characteristic (ROC) curves under different losses are reported in (c): the red curve corresponds to
the ROC curve of a model trained by optimizing the naive cross entropy loss, whereas other ones
correspond to the ROC curves of models trained by optimizing the conﬁdence loss. The KL diver-
gence term in the conﬁdence loss is optimized using explicit out-of-distribution datasets indicated
in the parentheses, e.g., Conﬁdent loss (LSUN) means that we use the LSUN dataset for optimizing
the KL divergence term.

3.1 EFFECTS OF CONFIDENCE LOSS

We ﬁrst verify the effect of conﬁdence loss in (1) trained by some explicit, say seen, out-of-
distribution datasets. First, we compare the quality of conﬁdence level by applying various training
losses. Speciﬁcally, the softmax classiﬁer is used and simple CNNs (two convolutional layers fol-
lowed by three fully-connected layers) are trained by minimizing the standard cross entropy loss on
SVHN dataset. We also apply the conﬁdence loss to the models by additionally optimizing the KL
divergence term using CIFAR-10 dataset (as training out-of-distribution). In Figure 2(a) and 2(b),
we report distributions of the maximum prediction value in softmax scores to evaluate the separation
quality between in-distribution (i.e., SVHN) and out-of-distributions. It is clear that there exists a
better separation between the SVHN test set (red bar) and other ones when the model is trained by
the conﬁdence loss. Here, we emphasize that the maximum prediction value is also low on even un-
trained (unseen) out-of-distributions, e.g., TinyImageNet, LSUN and synthetic datasets. Therefore,
it is expected that one can distinguish in- and out-of-distributions more easily when a classiﬁer is
trained by optimizing the conﬁdence loss. To verify that, we obtain the ROC curve using the baseline
detector (Hendrycks & Gimpel, 2016) that computes the maximum value of predictive distribution
on a test sample and classiﬁes it as positive (i.e., in-distribution) if the conﬁdence score is above
some threshold. Figure 2(c) shows the ROC curves when we optimize the KL divergence term on
various datasets. One can observe that realistic images such as TinyImageNet (aqua line) and LSUN
(green line) are more useful than synthetic datasets (orange line) for improving the detection perfor-
mance. This supports our intuition that out-of-distribution samples close to in-distribution could be
more effective in improving the detection performance as we discussed in Section 2.1.

We indeed evaluate the performance of the baseline detector for out-of-distribution using large-
scale CNNs, i.e., VGGNets with 13 layers, under various training scenarios, where more results on
AlexNet and ODIN detector (Liang et al., 2017) can be found in Appendix C (the overall trends
of results are similar). For optimizing the conﬁdence loss in (1), SVHN and CIFAR-10 training
datasets are used for optimizing the KL divergence term for the cases when the in-distribution is
CIFAR-10 and SVHN, respectively. Table 1 shows the detection performance for each in- and
out-of-distribution pair. When the in-distribution is SVHN, the classiﬁer trained by our method
drastically improves the detection performance across all out-of-distributions without hurting its
original classiﬁcation performance. However, when the in-distribution is CIFAR-10, the conﬁdence
loss does not improve the detection performance in overall, where we expect that this is because
the trained/seen SVHN out-of-distribution does not effectively cover all tested out-of-distributions.
Our joint conﬁdence loss in (4), which was designed under the intuition, resolves the issue of the
CIFAR-10 (in-distribution) classiﬁcation case in Table 1 (see Figure 4(b)).

6

Published as a conference paper at ICLR 2018

(a)

(b)

(c)

(d)

Figure 3: The generated samples from original GAN (a)/(c) and proposed GAN (b)/(d). In (a)/(b),
the grey area is the 2D histogram of training in-distribution samples drawn from a mixture of two
Gaussian distributions and red points indicate generated samples by GANs.

(a) In-distribution: SVHN

(b) In-distribution: CIFAR-10

Figure 4: Performances of the baseline detector (Hendrycks & Gimpel, 2016) under various training
losses. For training models by the conﬁdence loss, the KL divergence term is optimized using
samples indicated in the parentheses. For fair comparisons, we only plot the performances for
unseen out-of-distributions, where those for seen out-of-distributions (used for minimizing the KL
divergence term in (1)) can be found in Table 1.

3.2 EFFECTS OF ADVERSARIAL GENERATOR AND JOINT CONFIDENCE LOSS

In this section, we verify the effect of the proposed GAN in Section 2.2 and evaluate the detection
performance of the joint conﬁdence loss in (4). To verify that the proposed GAN can produce the
samples nearby the low-density boundary of the in-distribution space, we ﬁrst compare the gener-
ated samples by original GAN and proposed GAN on a simple example where the target distribution
is a mixture of two Gaussian distributions. For both the generator and discriminator, we use fully-
connected neural networks with 2 hidden layers. For our method, we use a pre-trained classiﬁer
which minimizes the cross entropy on target distribution samples and the KL divergence on out-of-
distribution samples generated by rejection sampling on a bounded 2D box. As shown in Figure
3(a), the samples of original GAN cover the high-density area of the target distribution while those
of proposed GAN does its boundary one (see Figure 3(b)). We also compare the generated samples
of original and proposed GANs on MNIST dataset (LeCun et al., 1998), which consists of hand-
written digits. For this experiment, we use deep convolutional GANs (DCGANs) (Radford et al.,
2015). In this case, we use a pre-trained classiﬁer which minimizes the cross entropy on MNIST

7

Published as a conference paper at ICLR 2018

(a) In-distribution: SVHN

(b) In-distribution: CIFAR-10

Figure 5: Guided gradient (sensitivity) maps of the top-1 predicted class with respect to the input
image under various training losses.

training samples and the KL divergence on synthetic Gaussian noises. As shown in Figure 3(c)
and 3(d), samples of original GAN looks more like digits than those of proposed GAN. Somewhat
interestingly, the proposed GAN still generates some new digit-like images.

We indeed evaluate the performance of our joint conﬁdence loss in (4) utilizing the proposed GAN.
To this end, we use VGGNets (as classiﬁers) and DCGANs (as GANs). We also test a variant of con-
ﬁdence loss which optimizes the KL divergence term on samples from a pre-trained original GAN
(implicitly) modeling the in-distribution. One can expect that samples from the original GAN can be
also useful for improving the detection performance since it may have bad generalization properties
(Arora et al., 2017) and generate a few samples on the low-density boundary as like the proposed
GAN. Figure 4 shows the performance of the baseline detector for each in- and out-of-distribution
pair. First, observe that the joint conﬁdence loss (blue bar) outperforms the conﬁdence loss with
some explicit out-of-distribution datasets (green bar). This is quite remarkable since the former
is trained only using in-distribution datasets, while the latter utilizes additional out-of-distribution
datasets. We also remark that our methods signiﬁcantly outperform the baseline cross entropy loss
(red bar) in all cases without harming its original classiﬁcation performances (see Table 2 in Ap-
pendix C). Interestingly, the conﬁdence loss with the original GAN (orange bar) is often (but not
always) useful for improving the detection performance, whereas that with the proposed GAN (blue
bar) still outperforms it in all cases.

Finally, we also provide visual interpretations of models using the guided gradient maps (Sprin-
genberg et al., 2014). Here, the gradient can be interpreted as an importance value of each pixel
which inﬂuences on the classiﬁcation decision. As shown in Figure 5, the model trained by the
cross entropy loss shows sharp gradient maps for both samples from in- and out-of-distributions,
whereas models trained by the conﬁdence losses do only on samples from in-distribution. For the
case of SVHN in-distribution, all conﬁdence losses gave almost zero gradients, which matches to
the results in Figure 4(a): their detection performances are almost perfect. For the case of CIFAR-
10 distribution, one can now observe that there exists some connection between gradient maps and
detection performances. This is intuitive because for detecting samples from out-of-distributions
better, the classiﬁer should look at more pixels as similar importance and the KL divergence term
forces it. We think that our visualization results might give some ideas in future works for developing
better inference methods for detecting out-of-distribution under our models.

4 CONCLUSION

In this paper, we aim to develop a training method for neural classiﬁcation networks for detecting
out-of-distribution better without losing its original classiﬁcation accuracy. In essence, our method
jointly trains two models for detecting and generating out-of-distribution by minimizing their losses
alternatively. Although we primarily focus on image classiﬁcation in our experiments, our method
can be used for any classiﬁcation tasks using deep neural networks. It is also interesting future
directions applying our methods for other related tasks: regression (Malinin et al., 2017), network
calibration (Guo et al., 2017), Bayesian probabilistic models (Li & Gal, 2017; Louizos & Welling,
2017), ensemble (Lakshminarayanan et al., 2017) and semi-supervised learning (Dai et al., 2017).

8

Published as a conference paper at ICLR 2018

ACKNOWLEDGEMENTS

This work was supported in part by the Institute for Information & communications Technology
Promotion(IITP) grant funded by the Korea government(MSIT) (No.2017-0-01778, Development
of Explainable Human-level Deep Machine Learning Inference Framework), the ICT R&D program
of MSIP/IITP [R-20161130-004520, Research on Adaptive Machine Learning Technology Devel-
opment for Intelligent Autonomous Digital Companion], DARPA Explainable AI (XAI) program
#313498 and Sloan Research Fellowship.

REFERENCES

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Con-

crete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). In International Conference on Machine Learning (ICML),
2017.

Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible
In ACM

models for healthcare: Predicting pneumonia risk and hospital 30-day readmission.
SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015.

Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan Salakhutdinov. Good semi-
supervised learning that requires a bad gan. In Advances in neural information processing systems
(NIPS), 2017.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale

hierarchical image database. In Computer Vision and Pattern Recognition (CVPR), 2009.

Ross Girshick. Fast r-cnn. In International Conference on Computer Vision (ICCV), 2015.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems (NIPS), 2014.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural

networks. In International Conference on Machine Learning (ICML), 2017.

Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger,
Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end
speech recognition. arXiv preprint arXiv:1412.5567, 2014.

Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution
examples in neural networks. In International Conference on Learning Representations (ICLR),
2016.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-
nov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning (ICML), 2015.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations (ICLR), 2014.

Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint

arXiv:1404.5997, 2014.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic-
tive uncertainty estimation using deep ensembles. In Advances in neural information processing
systems (NIPS), 2017.

9

Published as a conference paper at ICLR 2018

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Kimin Lee, Changho Hwang, KyoungSoo Park, and Jinwoo Shin. Conﬁdent multiple choice learn-

ing. In International Conference on Machine Learning (ICML), 2017.

Yingzhen Li and Yarin Gal. Dropout inference in bayesian neural networks with alpha-divergences.

In International Conference on Machine Learning (ICML), 2017.

Shiyu Liang, Yixuan Li, and R Srikant. Principled detection of out-of-distribution examples in

neural networks. arXiv preprint arXiv:1706.02690, 2017.

Christos Louizos and Max Welling. Multiplicative normalizing ﬂows for variational bayesian neural

networks. In International Conference on Machine Learning (ICML), 2017.

Andrey Malinin, Anton Ragni, Kate Knill, and Mark Gales. Incorporating uncertainty into deep
In Proceedings of the 55th Annual Meeting of the
learning for spoken language assessment.
Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pp. 45–50, 2017.

Mahdi Pakdaman Naeini, Gregory F Cooper, and Milos Hauskrecht. Obtaining well calibrated

probabilities using bayesian binning. In AAAI, 2015.

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, volume 2011, pp. 5, 2011.

Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing conﬁdent output distributions. arXiv preprint arXiv:1701.06548,
2017.

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In International Conference on Learning Repre-
sentations (ICLR), 2015.

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
In Advances in neural information processing systems

Improved techniques for training gans.
(NIPS), 2016.

Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the
In International

pixelcnn with discretized logistic mixture likelihood and other modiﬁcations.
Conference on Learning Representations (ICLR), 2017.

Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russ Webb.
In Computer

Learning from simulated and unsupervised images through adversarial training.
Vision and Pattern Recognition (CVPR), 2017.

Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for

simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Computer Vision and Pattern Recognition (CVPR), 2015.

Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee. Learning
to generate long-term future via hierarchical prediction. In International Conference on Machine
Learning (ICML), 2017.

Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.

Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. In

International Conference on Learning Representations (ICLR), 2017.

10

Published as a conference paper at ICLR 2018

A THRESHOLD-BASED DETECTORS

In this section, we formally describe the detection procedure of threshold-based detectors
(Hendrycks & Gimpel, 2016; Liang et al., 2017). For each data x, it measures some conﬁdence
score q(x) by feeding the data into a pre-trained classiﬁer. Here, (Hendrycks & Gimpel, 2016) de-
ﬁned the conﬁdence score as a maximum value of the predictive distribution, and (Liang et al., 2017)
further improved the performance by processing the predictive distribution (see Appendix C.3 for
more details). Then, the detector, g (x) : X → {0, 1}, assigns label 1 if the conﬁdence score q(x) is
above some threshold δ, and label 0, otherwise:

(cid:26)

g (x) =

1
0

if q(x) ≥ δ,
otherwise.

For this detector, we have to ﬁnd a score threshold so that some positive examples are classiﬁed
correctly, but this depends upon the trade-off between false negatives and false positives. To handle
this issue, we use threshold-independent evaluation metrics such as area under the receiver operating
characteristic curve (AUORC) and detection accuracy (see Appendix B).

B EXPERIMENTAL SETUPS IN SECTION 3

Datasets. We train deep models such as VGGNet (Szegedy et al., 2015) and AlexNet (Krizhevsky,
2014) for classifying CIFAR-10 and SVHN datasets: the former consists of 50,000 training and
10,000 test images with 10 image classes, and the latter consists of 73,257 training and 26,032 test
images with 10 digits.2 The corresponding test dataset are used as the in-distribution (positive)
samples to measure the performance. We use realistic images and synthetic noises as the out-of-
distribution (negative) samples: the TinyImageNet consists of 10,000 test images with 200 image
classes from a subset of ImageNet images. The LSUN consists of 10,000 test images of 10 different
scenes. We downsample each image of TinyImageNet and LSUN to size 32 × 32. The Gaussian
noise is independently and identically sampled from a Gaussian distribution with mean 0.5 and
variance 1. We clip each pixel value into the range [0, 1].

Detailed CNN structure and training. The simple CNN that we use for evaluation shown in Figure
2 consists of two convolutional layers followed by three fully-connected layers. Convolutional layers
have 128 and 256 ﬁlters, respectively. Each convolutional layer has a 5 × 5 receptive ﬁeld applied
with a stride of 1 pixel each followed by max pooling layer which pools 2 × 2 regions at strides of
2 pixels. AlexNet (Krizhevsky, 2014) consists of ﬁve convolutitonal layers followed by three fully-
connected layers. Convolutional layers have 64, 192, 384, 256 and 256 ﬁlters, respectively. First and
second convolutional layers have a 5×5 receptive ﬁeld applied with a stride of 1 pixel each followed
by max pooling layer which pools 3 × 3 regions at strides of 2 pixels. Other convolutional layers
have a 3 × 3 receptive ﬁeld applied with a stride of 1 pixel followed by max pooling layer which
pools 2 × 2 regions at strides of 2 pixels. Fully-connected layers have 2048, 1024 and 10 hidden
units, respectively. Dropout (Hinton et al., 2012) was applied to only fully-connected layers of the
network with the probability of retaining the unit being 0.5. All hidden units are ReLUs. Figure
6 shows the detailed structure of VGGNet (Szegedy et al., 2015) with three fully-connected layers
and 10 convolutional layers. Each ConvReLU box in the ﬁgure indicates a 3 × 3 convolutional layer
followed by ReLU activation. Also, all max pooling layers have 2 × 2 receptive ﬁelds with stride 2.
Dropout was applied to only fully-connected layers of the network with the probability of retaining
the unit being 0.5. For all experiments, the softmax classiﬁer is used, and each model is trained by
optimizing the objective function using Adam learning rule (Kingma & Ba, 2014). For each out-of-
distribution dataset, we randomly select 1,000 images for tuning the penalty parameter β, mini-batch
size and learning rate. The penalty parameter is chosen from β ∈ {0, 0.1, . . . 1.9, 2}, the mini-batch
size is chosen from {64, 128} and the learning rate is chosen from {0.001, 0.0005, 0.0002}. The
optimal parameters are chosen to minimize the detection error on the validation set. We drop the
learning rate by 0.1 at 60 epoch and models are trained for total 100 epochs. The best test result is
reported for each method.

Performance metrics. We measure the following metrics using threshold-based detectors:

2We do not use the extra SVHN dataset for training.

11

Published as a conference paper at ICLR 2018

• True negative rate (TNR) at 95% true positive rate (TPR). Let TP, TN, FP, and FN de-
note true positive, true negative, false positive and false negative, respectively. We measure
TNR = TN / (FP+TN), when TPR = TP / (TP+FN) is 95%.

• Area under the receiver operating characteristic curve (AUROC). The ROC curve is a
graph plotting TPR against the false positive rate = FP / (FP+TN) by varying a threshold.
• Area under the precision-recall curve (AUPR). The PR curve is a graph plotting the
precision = TP / (TP+FP) against recall = TP / (TP+FN) by varying a threshold. AUPR-IN
(or -OUT) is AUPR where in- (or out-of-) distribution samples are speciﬁed as positive.
• Detection accuracy. This metric corresponds to the maximum classiﬁcation proba-
(cid:8)Pin (q (x) ≤ δ) P (x is from Pin) +
bility over all possible thresholds δ: 1 − minδ
Pout (q (x) > δ) P (x is from Pout) (cid:9), where q(x) is a conﬁdent score such as a maxi-
mum value of softmax. We assume that both positive and negative examples have equal
probability of appearing in the test set, i.e., P (x is from Pin) = P (x is from Pout) = 0.5.

Note that AUROC, AUPR and detection accuracy are threshold-independent evaluation metrics.

Figure 6: Detailed structure of VGGNet with 13 layers.

Generating samples on a simple example. As shown in Figure 3(a) and Figure 3(b), we compare
the generated samples by original GAN and proposed GAN on a simple example where the target
distribution is a mixture of two Gaussian distributions. For both the generator and discriminator, we
use fully-connected neural networks with 2 hidden layers and 500 hidden units for each layer. For
all layers, we use ReLU activation function. We use a 100-dimensional Gaussian prior for the latent
variable z. For our method, we pre-train the simple fully-connected neural networks (2 hidden layers
and 500 ReLU units for each layer) by minimizing the cross entropy on target distribution samples
and the KL divergence on out-of-distribution samples generated by rejection sampling on bounded
2D box [−10, 10]2. The penalty parameter β is set to 1. We use ADAM learning rule (Kingma &
Ba, 2014) with a mini-batch size of 400. The initial learning rate is set to 0.002, and we train for
total 100 epochs.

Generating samples on MNIST. As shown in Figure 3(c) and Figure 3(d), we compare the gener-
ated samples of original and proposed GANs on MNIST dataset, which consists of greyscale images,
each containing a digit 0 to 9 with 60,000 training and 10,000 test images. We expand each image
to size 3 × 32 × 32. For both the generator and discriminator, we use deep convolutional GANs
(DCGANs) (Radford et al., 2015). The discriminator and generator consist of four convolutional
and deconvolutional layers, respectively. Convolutional layers have 128, 256, 512 and 1 ﬁlters, re-
spectively. Each convolutional layer has a 4 × 4 receptive ﬁeld applied with a stride of 2 pixel. The
second and third convolutional layers are followed by batch normalization (Ioffe & Szegedy, 2015).
For all layers, we use LeakyReLU activation function. Deconvolutional layers have 512, 256, 128
and 1 ﬁlters, respectively. Each deconvolutional layer has a 4 × 4 receptive ﬁeld applied with a
stride of 2 pixel followed by batch normalization (Ioffe & Szegedy, 2015) and ReLU activation.
For our method, we use a pre-trained simple CNNs (two convolutional layers followed by three
fully-connected layers) by minimizing the cross entropy on MNIST training samples and the KL
divergence on synthetic Gaussian noise. Convolutional layers have 128 and 256 ﬁlters, respectively.
Each convolutional layer has a 5 × 5 receptive ﬁeld applied with a stride of 1 pixel each followed by
max pooling layer which pools 2 × 2 regions at strides of 2 pixels. The penalty parameter β is set
to 1. We use ADAM learning rule (Kingma & Ba, 2014) with a mini-batch size of 128. The initial
learning rate is set to 0.0002, and we train for total 50 epochs.

12

Published as a conference paper at ICLR 2018

C MORE EXPERIMENTAL RESULTS

C.1 CLASSIFICATION PERFORMANCES

Table 2 reports the classiﬁcation accuracy of VGGNets on CIFAR-10 and SVHN datasets under
various training losses shown in Figure 4. One can note that all methods do not degrade the original
classiﬁcation performance, where the differences in classiﬁcation errors of across all tested single
models are at most 1% in our experiments.

In-distribution Cross entropy

Joint conﬁdence loss

Conﬁdence loss
with original GAN

Conﬁdence loss with explicit
out-of-distribution samples

CIFAR-10
SVHN

80.14
93.82

80.27
94.08

81.39
93.81

80.56
94.23

Table 2: Classiﬁcation test set accuracy of VGGNets on CIFAR-10 and SVHN datasets under vari-
ous training losses.

C.2 CALIBRATION EFFECTS OF CONFIDENCE LOSS

We also verify the calibration effects (Guo et al., 2017) of our methods: whether a classiﬁer trained
by our method can indicate when they are likely to be incorrect for test samples from the in-
distribution. In order to evaluate the calibration effects, we measure the expected calibration error
(ECE) (Naeini et al., 2015). Given test data {(x1, y1) , . . . , (xn, yn)}, we group the predictions into
M interval bins (each of size 1/M ). Let Bm be the set of indices of samples whose prediction
conﬁdence falls into the interval ( m−1

M , m
M ]. Then, the accuracy of Bm is deﬁned as follows:
1
|Bm|

1{yi=arg max

Pθ(y|xi)},

(cid:88)

y

i∈Bm

acc(Bm) =

where θ is the model parameters of a classiﬁer and 1A ∈ {0, 1} is the indicator function for event
A. We also deﬁne the conﬁdence of Bm as follows:
1
|Bm|

conf(Bm) =

q(xi),

(cid:88)

i∈Bm

where q(xi) is the conﬁdence of data i. Using these notations, we measure the expected calibration
error (ECE) as follows:

ECE =

|acc(Bm) − conf(Bm)|.

M
(cid:88)

m=1

|Bm|
n

One can note that ECE is zero if the conﬁdence score can represent the true distribution. Table
3 shows the calibration effects of conﬁdence loss when we deﬁne the conﬁdence score q as the
maximum predictive distribution of a classiﬁer. We found that the ECE of a classiﬁer trained by our
methods is lower than that of a classiﬁer trained by the standard cross entropy loss. This implies
that our proposed method is effective at calibrating predictions. We also remark that the temperature
scaling (Guo et al., 2017) provides further improvements under a classiﬁer trained by our joint
conﬁdence loss.

In-distribution

Without temperature scaling

With temperature scaling

Cross entropy loss

Joint conﬁdence loss Cross entropy loss

Joint conﬁdence loss

CIFAR-10

SVHN

18.45

5.30

14.62

5.13

7.07

2.80

6.19

1.39

Table 3: Expected calibration error (ECE) of VGGNets on CIFAR-10 and SVHN datasets under
various training losses. The number of bins M is set to 20. All values are percentages and boldface
values indicate relative the better results.

13

Published as a conference paper at ICLR 2018

C.3 EXPERIMENTAL RESULTS USING ODIN DETECTOR

In this section, we verify the effects of conﬁdence loss using ODIN detector (Liang et al., 2017)
which is an advanced threshold-based detector using temperature scaling (Guo et al., 2017) and
input perturbation. The key idea of ODIN is the temperature scaling which is deﬁned as follows:

Pθ(y = (cid:98)y|x; T ) =

(cid:80)

(cid:98)y(x)/T )

exp (f
y exp (fy(x)/T )

,

where T > 0 is the temperature scaling parameter and f = (f1, . . . , fK) is ﬁnal feature vector of
neural networks. For each data x, ODIN ﬁrst calculates the pre-processed image (cid:98)x by adding the
small perturbations as follows:

x(cid:48) = x − εsign (− (cid:53)x log Pθ(y = (cid:98)y|x; T )) ,
where ε is a magnitude of noise and (cid:98)y is the predicted label. Next, ODIN feeds the pre-
processed data into the classiﬁer, computes the maximum value of scaled predictive distribution,
i.e., maxy Pθ(y|x(cid:48); T ), and classiﬁes it as positive (i.e., in-distribution) if the conﬁdence score is
above some threshold δ.

For ODIN detector, the perturbation noise ε is chosen from {0, 0.0001, 0.001, 0.01}, and the tem-
perature T is chosen from {1, 10, 100, 500, 1000}. The optimal parameters are chosen to minimize
the detection error on the validation set. Figure 7 shows the performance of the OIDN and baseline
detector for each in- and out-of-distribution pair. First, we remark that the baseline detector using
classiﬁers trained by our joint conﬁdence loss (blue bar) typically outperforms the ODIN detec-
tor using classiﬁers trained by the cross entropy loss (orange bar). This means that our classiﬁer
can map in- and out-of-distributions more separately without pre-processing methods such as tem-
perature scaling. The ODIN detector provides further improvements if one uses it with our joint
conﬁdence loss (green bar). In other words, our proposed training method can improve all prior
detection methods.

(a) In-distribution: SVHN

Figure 7: Performances of the baseline detector (Hendrycks & Gimpel, 2016) and ODIN detector
(Liang et al., 2017) under various training losses.

(b) In-distribution: CIFAR-10

C.4 EXPERIMENTAL RESULTS ON ALEXNET

Table 4 shows the detection performance for each in- and out-of-distribution pair when the classiﬁer
is AlexNet (Krizhevsky, 2014), which is one of popular CNN architectures. We remark that they
show similar trends.

14

Published as a conference paper at ICLR 2018

In-dist

Out-of-dist

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

Baseline
(SVHN)

Baseline
(CIFAR-10)

ODIN
(SVHN)

ODIN
(CIFAR-10)

CIFAR-10 (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

CIFAR-10 (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

92.14 / 93.77

76.58 / 76.18

92.14 / 93.77

76.58 / 76.18

Cross entropy loss / Conﬁdence loss

42.0 / 99.9
45.6 / 99.9
44.6 / 100.0
58.6 / 100.0

12.8 / 99.6
10.3 / 10.1
10.7 / 8.1
6.7 / 1.0

55.5 / 99.9
59.5 / 99.9
61.5 / 100.0
82.6 / 100.0

37.1 / 99.6
11.4 / 8.4
13.3 / 7.1
3.8 / 0.0

88.0 / 100.0
89.4 / 100.0
89.8 / 100.0
94.2 / 100.0

71.0 / 99.9
59.2 / 52.1
56.3 / 51.5
49.6 / 13.5

89.1 / 99.2
90.5 / 99.3
91.8 / 99.3
97.0 / 99.3

86.7 / 99.6
69.1 / 65.6
71.9 / 67.1
70.9 / 57.2

83.4 / 99.8
84.3 / 99.9
84.5 / 99.9
88.8 / 100.0

73.2 / 99.6
64.2 / 62.0
64.3 / 61.8
61.3 / 50.0

82.4 / 99.8
83.8 / 99.9
84.8 / 99.9
91.6 / 100.0

79.3 / 99.6
64.4 / 61.8
75.3 / 63.7
69.3 / 40.4

88.7 / 99.9
90.2 / 100.0
90.8 / 100.0
95.5 / 100.0

74.3 / 99.9
63.6 / 59.8
62.3 / 59.5
58.5 / 43.7

85.9 / 100.0
87.5 / 100.0
90.5 / 100.0
97.4 / 100.0

88.1 / 99.9
71.4 / 68.6
67.2 / 72.0
78.1 / 56.1

87.3 / 99.3
88.6 / 99.3
88.4 / 99.3
92.5 / 99.3

70.7 / 99.6
64.4 / 62.3
65.3 / 61.6
59.5 / 32.0

89.0 / 99.2
90.4 / 99.3
91.3 / 99.3
96.4 / 99.3

84.2 / 99.6
64.6 / 60.7
65.3 / 60.5
60.7 / 40.7

Table 4: Performance of the baseline detector (Hendrycks & Gimpel, 2016) and ODIN detector
(Liang et al., 2017) using AlexNet. All values are percentages and boldface values indicate rela-
tive the better results. For each in-distribution, we minimize the KL divergence term in (1) using
training samples from an out-of-distribution dataset denoted by “seen”, where other “unseen” out-
of-distributions were also used for testing.

D MAXIMIZING ENTROPY

One might expect that the entropy of out-of-distribution is expected to be much higher compared to
that of in-distribution since the out-of-distribution is typically on a much larger space than the in-
distribution. Therefore, one can add maximizing the entropy of generator distribution to new GAN
loss in (3) and joint conﬁdence loss in (4). However, maximizing the entropy of generator distribu-
tion is technically challenging since a GAN does not model the generator distribution explicitly. To
handle the issue, one can leverage the pull-away term (PT) (Zhao et al., 2017):

−H (PG (x)) (cid:119) PT (PG (x)) =

1
M (M − 1)

M
(cid:88)

(cid:88)

(cid:32)

i=1

j(cid:54)=i

G (zi)(cid:62) G (zj)
(cid:107)G (zi)(cid:107)(cid:107)G (zj)(cid:107)

(cid:33)2

,

where H (·) denotes the entropy, zi, zj ∼ Ppri (z) and M is the number of samples. Intuitively,
one can expect the effect of increasing the entropy by minimizing PT since it corresponds to the
squared cosine similarity of generated samples. We note that (Dai et al., 2017) also used PT to
maximize the entropy. Similarly as in Section 3.2, we verify the effects of PT using VGGNet. Table
5 shows the performance of the baseline detector for each in- and out-of-distribution pair. We found
that joint conﬁdence loss with PT tends to (but not always) improve the detection performance.
However, since PT increases the training complexity and the gains from PT are relatively marginal
(or controversial), we leave it as an auxiliary option for improving the performance.

In-dist

Out-of-dist

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

SVHN

93.81 / 94.05

CIFAR-10

81.39 / 80.60

CIFAR-10
TinyImageNet
LSUN

SVHN
TinyImageNet
LSUN

Joint conﬁdence loss without PT / with PT

90.1 / 92.3
99.0 / 99.9
98.9 / 100.0

97.6 / 98.1
99.6 / 100.0
99.6 / 100.0

25.4 / 13.2
35.0 / 44.8
39.1 / 49.1

66.8 / 69.5
72.0 / 78.4
75.1 / 80.7

93.6 / 94.6
97.6 / 99.7
97.5 / 99.9

74.2 / 75.1
76.4 / 77.6
77.8 / 78.7

97.7 / 98.2
99.7 / 100.0
99.7 / 100.0

97.9 / 98.7
94.5 / 100.0
95.5 / 100.0

71.3 / 73.5
74.7 / 79.4
77.1 / 81.3

78.3 / 72.0
82.2 / 84.4
83.6 / 85.8

Table 5: Performance of the baseline detector (Hendrycks & Gimpel, 2016) using VGGNets trained
by joint conﬁdence loss with and without pull-away term (PT). All values are percentages and bold-
face values indicate relative the better results.

15

Published as a conference paper at ICLR 2018

E ADDING OUT-OF-DISTRIBUTION CLASS

Instead of forcing the predictive distribution on out-of-distribution samples to be closer to the uni-
form one, one can simply add an additional “out-of-distribution” class to a classiﬁer as follows:

min
θ

EPin((cid:98)x,(cid:98)y)

(cid:2) − log Pθ (y = (cid:98)y|(cid:98)x) (cid:3) + EPout(x)

(cid:2) − log Pθ (y = K + 1|x) (cid:3),

(5)

where θ is a model parameter. Similarly as in Section 3.1, we compare the performance of the
conﬁdence loss with that of above loss in (5) using VGGNets with for image classiﬁcation on SVHN
dataset. To optimize the KL divergence term in conﬁdence loss and the second term of (5), CIFAR-
10 training datasets are used. In order to compare the detection performance, we deﬁne the the
conﬁdence score of input x as 1 − Pθ (y = K + 1|x) in case of (5). Table 6 shows the detection
performance for out-of-distribution. First, the classiﬁer trained by our method often signiﬁcantly
outperforms the alternative adding the new class label. This is because modeling explicitly out-of-
distribution can incur overﬁtting to trained out-of-distribution dataset.

Detector

Out-of-dist

Baseline
detector

ODIN
detector

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

K + 1 class loss in (5) / Conﬁdence loss

79.61 / 80.56

79.61 / 80.56

99.6 / 99.8
0.0 / 9.9
0.0 / 10.5
0.0 / 3.3

99.6 / 99.8
0.3 / 12.2
0.1 / 13.7
0.0 / 8.2

99.8 / 99.9
5.2 / 31.8
5.6 / 34.8
0.1 / 14.1

99.9 / 99.8
47.3 / 70.6
48.3 / 73.1
28.3 / 68.3

99.7 / 99.8
51.3 / 58.6
51.5 / 60.2
50.0 / 50.0

99.1 / 99.8
55.12 / 65.7
55.9 / 67.9
54.4 / 65.4

99.8 / 99.9
50.7 / 55.3
50.8 / 56.4
49.3 / 49.4

99.9 / 99.9
56.6 / 72.7
57.5 / 75.2
47.8 / 74.1

99.9 / 99.8
64.3 / 66.1
71.5 / 68.0
12.2 / 47.0

99.9 / 99.8
44.3 / 65.6
44.7 / 67.8
36.8 / 61.5

Table 6: Performance of the baseline detector (Hendrycks & Gimpel, 2016) and ODIN detector
(Liang et al., 2017) using VGGNet. All values are percentages and boldface values indicate relative
the better results.

16

8
1
0
2
 
b
e
F
 
3
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
5
2
3
9
0
.
1
1
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

TRAINING CONFIDENCE-CALIBRATED CLASSIFIERS
FOR DETECTING OUT-OF-DISTRIBUTION SAMPLES

Honglak Lee§ ,†

Kimin Lee∗
Jinwoo Shin∗
∗Korea Advanced Institute of Science and Technology, Daejeon, Korea
†University of Michigan, Ann Arbor, MI 48109
§Google Brain, Mountain View, CA 94043

Kibok Lee†

ABSTRACT

The problem of detecting whether a test sample is from in-distribution (i.e., train-
ing distribution by a classiﬁer) or out-of-distribution sufﬁciently different from it
arises in many real-world machine learning applications. However, the state-of-art
deep neural networks are known to be highly overconﬁdent in their predictions,
i.e., do not distinguish in- and out-of-distributions. Recently, to handle this is-
sue, several threshold-based detectors have been proposed given pre-trained neu-
ral classiﬁers. However, the performance of prior works highly depends on how
to train the classiﬁers since they only focus on improving inference procedures.
In this paper, we develop a novel training method for classiﬁers so that such in-
ference algorithms can work better. In particular, we suggest two additional terms
added to the original loss (e.g., cross entropy). The ﬁrst one forces samples from
out-of-distribution less conﬁdent by the classiﬁer and the second one is for (im-
plicitly) generating most effective training samples for the ﬁrst one. In essence,
our method jointly trains both classiﬁcation and generative neural networks for
out-of-distribution. We demonstrate its effectiveness using deep convolutional
neural networks on various popular image datasets.

1

INTRODUCTION

Deep neural networks (DNNs) have demonstrated state-of-the-art performance on many classiﬁ-
cation tasks, e.g., speech recognition (Hannun et al., 2014), image classiﬁcation (Girshick, 2015),
video prediction (Villegas et al., 2017) and medical diagnosis (Caruana et al., 2015). Even though
DNNs achieve high accuracy, it has been addressed (Lakshminarayanan et al., 2017; Guo et al.,
2017) that they are typically overconﬁdent in their predictions. For example, DNNs trained to clas-
sify MNIST images often produce high conﬁdent probability 91% even for random noise (see the
work of (Hendrycks & Gimpel, 2016)). Since evaluating the quality of their predictive uncertainty
is hard, deploying them in real-world systems raises serious concerns in AI Safety (Amodei et al.,
2016), e.g., one can easily break a secure authentication system that can be unlocked by detecting
the gaze and iris of eyes using DNNs (Shrivastava et al., 2017).

The overconﬁdence issue of DNNs is highly related to the problem of detecting out-of-distribution:
detect whether a test sample is from in-distribution (i.e., training distribution by a classiﬁer) or out-
of-distribution sufﬁciently different from it. Formally, it can be formulated as a binary classiﬁcation
problem. Let an input x ∈ X and a label y ∈ Y = {1, . . . , K} be random variables that follow a
joint data distribution Pin (x, y) = Pin (y|x) Pin (x). We assume that a classiﬁer Pθ (y|x) is trained
on a dataset drawn from Pin (x, y), where θ denotes the model parameter. We let Pout (x) denote
an out-of-distribution which is ‘far away’ from in-distribution Pin (x). Our problem of interest is
determining if input x is from Pin or Pout, possibly utilizing a well calibrated classiﬁer Pθ (y|x).
In other words, we aim to build a detector, g (x) : X → {0, 1}, which assigns label 1 if data is from
in-distribution, and label 0 otherwise.

There have been recent efforts toward developing efﬁcient detection methods where they mostly
have studied simple threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017) uti-
lizing a pre-trained classiﬁer. For each input x, it measures some conﬁdence score q(x) based on a
pre-trained classiﬁer, and compares the score to some threshold δ > 0. Then, the detector assigns

1

Published as a conference paper at ICLR 2018

label 1 if the conﬁdence score q(x) is above δ, and label 0, otherwise. Speciﬁcally, (Hendrycks
& Gimpel, 2016) deﬁned the conﬁdence score as a maximum value of the predictive distribution,
and (Liang et al., 2017) further improved the performance by using temperature scaling (Guo et al.,
2017) and adding small controlled perturbations to the input data. Although such inference methods
are computationally simple, their performances highly depend on the pre-trained classiﬁer. Namely,
they fail to work if the classiﬁer does not separate the maximum value of predictive distribution
well enough with respect to Pin and Pout. Ideally, a classiﬁer should be trained to separate all
class-dependent in-distributions as well as out-of-distribution in the output space. As another line of
research, Bayesian probabilistic models (Li & Gal, 2017; Louizos & Welling, 2017) and ensembles
of classiﬁers (Lakshminarayanan et al., 2017) were also investigated. However, training or inferring
those models are computationally more expensive. This motivates our approach of developing a
new training method for the more plausible simple classiﬁers. Our direction is orthogonal to the
Bayesian and ensemble approaches, where one can also combine them for even better performance.

Contribution. In this paper, we develop such a training method for detecting out-of-distribution
Pout better without losing its original classiﬁcation accuracy. First, we consider a new loss function,
called conﬁdence loss. Our key idea on the proposed loss is to additionally minimize the Kullback-
Leibler (KL) divergence from the predictive distribution on out-of-distribution samples to the uni-
form one in order to give less conﬁdent predictions on them. Then, in- and out-of-distributions are
expected to be more separable. However, optimizing the conﬁdence loss requires training samples
from out-of-distribution, which are often hard to sample: a priori knowledge on out-of-distribution
is not available or its underlying space is too huge to cover. To handle the issue, we consider a
new generative adversarial network (GAN) (Goodfellow et al., 2014) for generating most effective
samples from Pout. Unlike the original GAN, the proposed GAN generates ‘boundary’ samples in
the low-density area of Pin. Finally, we design a joint training scheme minimizing the classiﬁer’s
loss and new GAN loss alternatively, i.e., the conﬁdent classiﬁer improves the GAN, and vice versa,
as training proceeds. Here, we emphasize that the proposed GAN does not need to generate explicit
samples under our scheme, and instead it implicitly encourages training a more conﬁdent classiﬁer.

We demonstrate the effectiveness of the proposed method using deep convolutional neural networks
such as AlexNet (Krizhevsky, 2014) and VGGNet (Szegedy et al., 2015) for image classiﬁcation
tasks on CIFAR (Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), ImageNet (Deng et al.,
2009), and LSUN (Yu et al., 2015) datasets. The classiﬁer trained by our proposed method dras-
tically improves the detection performance of all threshold-based detectors (Hendrycks & Gim-
pel, 2016; Liang et al., 2017) in all experiments. In particular, VGGNet with 13 layers trained by
our method improves the true negative rate (TNR), i.e., the fraction of detected out-of-distribution
(LSUN) samples, compared to the baseline: 14.0% → 39.1% and 46.3% → 98.9% on CIFAR-10
and SVHN, respectively, when 95% of in-distribution samples are correctly detected. We also pro-
vide visual understandings on the proposed method using the image datasets. We believe that our
method can be a strong guideline when other researchers will pursue these tasks in the future.

2 TRAINING CONFIDENT NEURAL CLASSIFIERS

In this section, we propose a novel training method for classiﬁers in order to improve the perfor-
mance of prior threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017) (see Ap-
pendix A for more details). Our motivation is that such inference algorithms can work better if the
classiﬁers are trained so that they map the samples from in- and out-of-distributions into the output
space separately. Namely, we primarily focus on training an improved classiﬁer, and then use prior
detectors under the trained model to measure its performance.

2.1 CONFIDENT CLASSIFIER FOR OUT-OF-DISTRIBUTION

Without loss of generality, suppose that the cross entropy loss is used for training. Then, we propose
the following new loss function, termed conﬁdence loss:

min
θ

EPin((cid:98)x,(cid:98)y)

(cid:2) − log Pθ (y = (cid:98)y|(cid:98)x) (cid:3) + βEPout(x)

(cid:2)KL (U (y) (cid:107) Pθ (y|x)) (cid:3),

(1)

where KL denotes the Kullback-Leibler (KL) divergence, U (y) is the uniform distribution and
β > 0 is a penalty parameter. It is highly intuitive as the new loss forces the predictive distribution

2

Published as a conference paper at ICLR 2018

(a)

(b)

(c)

(d)

Figure 1: Illustrating the behavior of classiﬁer under different out-of-distribution training datasets.
We generate the out-of-distribution samples from (a) 2D box [−50, 50]2, and show (b) the corre-
sponding decision boundary of classiﬁer. We also generate the out-of-distribution samples from (c)
2D box [−20, 20]2, and show (d) the corresponding decision boundary of classiﬁer.

on out-of-distribution samples to be closer to the uniform one, i.e., zero conﬁdence, while that
for samples from in-distribution still follows the label-dependent probability. In other words, the
proposed loss is designed for assigning higher maximum prediction values, i.e., maxy Pθ (y|x), to
in-distribution samples than out-of-distribution ones. Here, a caveat is that adding the KL divergence
term might degrade the classiﬁcation performance. However, we found that it is not the case due
to the high expressive power of deep neural networks, while in- and out-of-distributions become
more separable with respect to the maximum prediction value by optimizing the conﬁdence loss
(see Section 3.1 for supporting experimental results).

We remark that minimizing a similar KL loss was studied recently for different purposes (Lee et al.,
2017; Pereyra et al., 2017). Training samples for minimizing the KL divergence term is explicitly
given in their settings while we might not. Ideally, one has to sample all (almost inﬁnite) types of out-
of-distribution to minimize the KL term in (1), or require some prior information on testing out-of-
distribution for efﬁcient sampling. However, this is often infeasible and fragile. To address the issue,
we suggest to sample out-of-distribution close to in-distribution, which could be more effective in
improving the detection performance, without any assumption on testing out-of-distribution.

In order to explain our intuition in details, we consider a binary classiﬁcation task on a simple ex-
ample, where each class data is drawn from a Gaussian distribution and entire data space is bounded
by 2D box [−50, 50]2 for visualization. We apply the conﬁdence loss to simple fully-connected
neural networks (2 hidden layers and 500 hidden units for each layer) using different types of out-
of-distribution training samples. First, as shown in Figure 1(a), we construct an out-of-distribution
training dataset of 100 (green) points using rejection sampling on the entire data space [−50, 50]2.
Figure 1(b) shows the decision boundary of classiﬁer optimizing the conﬁdence loss on the corre-
sponding dataset. One can observe that a classiﬁer still shows overconﬁdent predictions (red and
blue regions) near the labeled in-distribution region. On the other hand, if we construct a training
out-of-distribution dataset of 100 points from [−20, 20]2, i.e., closer to target, in-distribution space
(see Figure 1(c)), a classiﬁer produces conﬁdent predictions only on the labeled region and zero
conﬁdence on the remaining in the entire data space [−50, 50]2 as shown in Figure 1(d). If one
increases the number of training out-of-distribution samples which are generated from the entire
space, i.e., [−50, 50]2, Figure 1(b) is expected to be similar to Figure 1(d). In other words, one need
more samples in order to train a conﬁdent classiﬁer if samples are generated from the entire space.
However, this might be impossible and not efﬁcient since the number of out-of-distribution training
samples might be almost inﬁnite to cover its entire, huge actual data space. This implies that training
out-of-distribution samples nearby the in-distribution region could be more effective in improving
the detection performance. Our underlying intuition is that the effect of boundary of in-distribution
region might propagate to the entire out-of-distribution space. Our experimental results in Section
3.1 also support this: realistic images are more useful as training out-of-distribution than synthetic
datasets (e.g., Gaussian noise) for improving the detection performance when we consider an image
classiﬁcation task. This motivates us to develop a new generative adversarial network (GAN) for
generating such effective out-of-distribution samples.

2.2 ADVERSARIAL GENERATOR FOR OUT-OF-DISTRIBUTION

In this section, we introduce a new training method for learning a generator of out-of-distribution
inspired by generative adversarial network (GAN) (Goodfellow et al., 2014). We will ﬁrst assume

3

Published as a conference paper at ICLR 2018

that the classiﬁer for in-distribution is ﬁxed, and also describe the joint learning framework in the
next section.

The GAN framework consists of two main components: discriminator D and generator G. The
generator maps a latent variable z from a prior distribution Ppri (z) to generated outputs G (z), and
discriminator D : X → [0, 1] represents a probability that sample x is from a target distribution.
Suppose that we want to recover the in-distribution Pin(x) using the generator G. Then, one can
optimize the following min-max objective for forcing PG ≈ Pin:
EPin(x)

(cid:2) log D (x) (cid:3) + EPpri(z)

(cid:2) log (1 − D (G (z))) (cid:3).

(2)

min
G

max
D

However, unlike the original GAN, we want to make the generator recover an effective out-of-
distribution Pout instead of Pin. To this end, we propose the following new GAN loss:

min
G

max
D

β EPG(x)

(cid:124)

(cid:124)

+ EPin(x)

(cid:123)(cid:122)
(a)

(cid:2)KL (U (y) (cid:107) Pθ (y|x)) (cid:3)
(cid:125)
(cid:2) log (1 − D (x)) (cid:3)
,
(cid:125)

(cid:2) log D (x) (cid:3) + EPG(x)

(cid:123)(cid:122)
(b)

(3)

where θ is the model parameter of a classiﬁer trained on in-distribution. The above objective can be
interpreted as follows: the ﬁrst term (a) corresponds to a replacement of the out-of-distribution Pout
in (1)’s KL loss with the generator distribution PG. One can note that this forces the generator to
generate low-density samples since it can be interpreted as minimizing the log negative likelihood
of in-distribution using the classiﬁer, i.e., Pin(x) ≈ exp (KL (U (y) (cid:107) Pθ (y|x))) . We remark that
this approximation is also closely related to the inception score (Salimans et al., 2016) which is
popularly used as a quantitative measure of visual ﬁdelity of the samples. The second term (b) cor-
responds to the original GAN loss since we would like to have out-of-distribution samples close to
in-distribution, as mentioned in Section 2.1. Suppose that the model parameter of classiﬁer θ is set
appropriately such that the classiﬁer produces the uniform distribution for out of distribution sam-
ples. Then, the KL divergence term (a) in (3) is approximately 0 no matter what out-of-distribution
samples are generated. However, if the samples are far away from boundary, the GAN loss (b) in (3)
should be high, i.e., the GAN loss forces having samples being not too far from the in-distribution
space. Therefore, one can expect that proposed loss can encourage the generator to produce the
samples which are on the low-density boundary of the in-distribution space. We also provide its
experimental evidences in Section 3.2.

We also remark that (Dai et al., 2017) consider a similar GAN generating samples from out-of-
distribution for the purpose of semi-supervised learning. The authors assume the existence of a pre-
trained density estimation model such as PixelCNN++ (Salimans et al., 2017) for in-distribution,
but such a model might not exist and be expensive to train in general. Instead, we use much simpler
conﬁdent classiﬁers for approximating the density. Hence, under our fully-supervised setting, our
GAN is much easier to train and more suitable.

2.3

JOINT TRAINING METHOD OF CONFIDENT CLASSIFIER AND ADVERSARIAL GENERATOR

min
G

min
θ

max
D

In the previous section, we suggest training the proposed GAN using a pre-trained conﬁdent classi-
ﬁer. We remind that the converse is also possible, i.e., the motivation of having such a GAN is for
training a better classiﬁer. Hence, two models can be used for improving each other. This naturally
suggests a joint training scheme where the conﬁdent classiﬁer improves the proposed GAN, and vice
versa, as training proceeds. Speciﬁcally, we suggest the following joint objective function:
(cid:2) − log Pθ (y = (cid:98)y|(cid:98)x) (cid:3)
(cid:2)KL (U (y) (cid:107) Pθ (y|x)) (cid:3)
(cid:125)
(cid:125)
(cid:2) log D ((cid:98)x) (cid:3) + EPG(x)

+β EPG(x)
(cid:124)

EPin((cid:98)x,(cid:98)y)
(cid:124)

(cid:123)(cid:122)
(e)
The classiﬁer’s conﬁdence loss corresponds to (c) + (d), and the proposed GAN loss corresponds
to (d) + (e), i.e., they share the KL divergence term (d) under joint training. To optimize the above
objective efﬁciently, we propose an alternating algorithm, which optimizes model parameters {θ}
of classiﬁer and GAN models {G, D} alternatively as shown in Algorithm 1. Since the algorithm
monotonically decreases the objective function, it is guaranteed to converge.

(cid:123)(cid:122)
(d)
(cid:2) log (1 − D (x)) (cid:3).
(cid:125)

+EPin((cid:98)x)
(cid:124)

(cid:123)(cid:122)
(c)

(4)

4

Published as a conference paper at ICLR 2018

Algorithm 1 Alternating minimization for detecting and generating out-of-distribution.

repeat

/∗ Update proposed GAN ∗/
Sample {z1, . . . , zM } and {x1, . . . , xM } from prior Ppri (z) and and in-distribution Pin (x),

respectively, and update the discriminator D by ascending its stochastic gradient of

1
M

M
(cid:88)

(cid:104)

i=1

log D (xi) + log (1 − D (G (zi)))

(cid:105)
.

Sample {z1, . . . , zM } from prior Ppri (z), and update the generator G by descending its

stochastic gradient of

1
M

M
(cid:88)

(cid:104)

i=1

log (1 − D (G (zi)))

+

KL (U (y) (cid:107) Pθ (y|G (zi)))

(cid:105)
.

(cid:105)

β
M

M
(cid:88)

(cid:104)

i=1

/∗ Update conﬁdent classiﬁer ∗/
Sample {z1, . . . , zM } and {(x1, y1) , . . . , (xM , yM )} from prior Ppri (z) and in-distribution

Pin (x, y), respectively, and update the classiﬁer θ by descending its stochastic gradient of

− log Pθ (y = yi|xi) + βKL (U (y) (cid:107) Pθ (y|G (zi)))

(cid:105)

.

1
M

M
(cid:88)

(cid:104)

i=1

until convergence

3 EXPERIMENTAL RESULTS

We demonstrate the effectiveness of our proposed method using various datasets: CIFAR
(Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), ImageNet (Deng et al., 2009), LSUN
(Yu et al., 2015) and synthetic (Gaussian) noise distribution. We train convolutional neural networks
(CNNs) including VGGNet (Szegedy et al., 2015) and AlexNet (Krizhevsky, 2014) for classifying
CIFAR-10 and SVHN datasets. The corresponding test dataset is used as the in-distribution (pos-
itive) samples to measure the performance. We use realistic images and synthetic noises as the
out-of-distribution (negative) samples. For evaluation, we measure the following metrics using the
threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017): the true negative rate
(TNR) at 95% true positive rate (TPR), the area under the receiver operating characteristic curve
(AUROC), the area under the precision-recall curve (AUPR), and the detection accuracy, where
larger values of all metrics indicate better detection performances. Due to the space limitation, more
explanations about datasets, metrics and network architectures are given in Appendix B.1

In-dist

Out-of-dist

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

SVHN

93.82 / 94.23

CIFAR-10 (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

CIFAR-10

80.14 / 80.56

Cross entropy loss / Conﬁdence loss

47.4 / 99.9
49.0 / 100.0
46.3 / 100.0
56.1 / 100.0

13.7 / 99.8
13.6 / 9.9
14.0 / 10.5
2.8 / 3.3

62.6 / 99.9
64.6 / 100.0
61.8 / 100.0
72.0 / 100.0

46.6 / 99.9
39.6 / 31.8
40.7 / 34.8
10.2 / 14.1

78.6 / 99.9
79.6 / 100.0
78.2 / 100.0
83.4 / 100.0

66.6 / 99.8
62.6 / 58.6
63.2 / 60.2
50.0 / 50.0

71.6 / 99.9
72.7 / 100.0
71.1 / 100.0
77.2 / 100.0

61.4 / 99.9
58.3 / 55.3
58.7 / 56.4
48.1 / 49.4

91.2 / 99.4
91.6 / 99.4
90.8 / 99.4
92.8 / 99.4

73.5 / 99.8
71.0 / 66.1
71.5 / 68.0
39.9 / 47.0

Table 1: Performance of the baseline detector (Hendrycks & Gimpel, 2016) using VGGNet. All val-
ues are percentages and boldface values indicate relative the better results. For each in-distribution,
we minimize the KL divergence term in (1) using training samples from an out-of-distribution
dataset denoted by “seen”, where other “unseen” out-of-distributions were only used for testing.

1Our code is available at https://github.com/alinlab/Confident_classifier.

5

Published as a conference paper at ICLR 2018

(a) Cross entropy loss

(b) Conﬁdence loss in (1)

(c) ROC curve

Figure 2: For all experiments in (a), (b) and (c), we commonly use the SVHN dataset for in-
distribution. Fraction of the maximum prediction value in softmax scores trained by (a) cross en-
tropy loss and (b) conﬁdence loss: the x-axis and y-axis represent the maximum prediction value
and the fraction of images receiving the corresponding score, respectively. The receiver operating
characteristic (ROC) curves under different losses are reported in (c): the red curve corresponds to
the ROC curve of a model trained by optimizing the naive cross entropy loss, whereas other ones
correspond to the ROC curves of models trained by optimizing the conﬁdence loss. The KL diver-
gence term in the conﬁdence loss is optimized using explicit out-of-distribution datasets indicated
in the parentheses, e.g., Conﬁdent loss (LSUN) means that we use the LSUN dataset for optimizing
the KL divergence term.

3.1 EFFECTS OF CONFIDENCE LOSS

We ﬁrst verify the effect of conﬁdence loss in (1) trained by some explicit, say seen, out-of-
distribution datasets. First, we compare the quality of conﬁdence level by applying various training
losses. Speciﬁcally, the softmax classiﬁer is used and simple CNNs (two convolutional layers fol-
lowed by three fully-connected layers) are trained by minimizing the standard cross entropy loss on
SVHN dataset. We also apply the conﬁdence loss to the models by additionally optimizing the KL
divergence term using CIFAR-10 dataset (as training out-of-distribution). In Figure 2(a) and 2(b),
we report distributions of the maximum prediction value in softmax scores to evaluate the separation
quality between in-distribution (i.e., SVHN) and out-of-distributions. It is clear that there exists a
better separation between the SVHN test set (red bar) and other ones when the model is trained by
the conﬁdence loss. Here, we emphasize that the maximum prediction value is also low on even un-
trained (unseen) out-of-distributions, e.g., TinyImageNet, LSUN and synthetic datasets. Therefore,
it is expected that one can distinguish in- and out-of-distributions more easily when a classiﬁer is
trained by optimizing the conﬁdence loss. To verify that, we obtain the ROC curve using the baseline
detector (Hendrycks & Gimpel, 2016) that computes the maximum value of predictive distribution
on a test sample and classiﬁes it as positive (i.e., in-distribution) if the conﬁdence score is above
some threshold. Figure 2(c) shows the ROC curves when we optimize the KL divergence term on
various datasets. One can observe that realistic images such as TinyImageNet (aqua line) and LSUN
(green line) are more useful than synthetic datasets (orange line) for improving the detection perfor-
mance. This supports our intuition that out-of-distribution samples close to in-distribution could be
more effective in improving the detection performance as we discussed in Section 2.1.

We indeed evaluate the performance of the baseline detector for out-of-distribution using large-
scale CNNs, i.e., VGGNets with 13 layers, under various training scenarios, where more results on
AlexNet and ODIN detector (Liang et al., 2017) can be found in Appendix C (the overall trends
of results are similar). For optimizing the conﬁdence loss in (1), SVHN and CIFAR-10 training
datasets are used for optimizing the KL divergence term for the cases when the in-distribution is
CIFAR-10 and SVHN, respectively. Table 1 shows the detection performance for each in- and
out-of-distribution pair. When the in-distribution is SVHN, the classiﬁer trained by our method
drastically improves the detection performance across all out-of-distributions without hurting its
original classiﬁcation performance. However, when the in-distribution is CIFAR-10, the conﬁdence
loss does not improve the detection performance in overall, where we expect that this is because
the trained/seen SVHN out-of-distribution does not effectively cover all tested out-of-distributions.
Our joint conﬁdence loss in (4), which was designed under the intuition, resolves the issue of the
CIFAR-10 (in-distribution) classiﬁcation case in Table 1 (see Figure 4(b)).

6

Published as a conference paper at ICLR 2018

(a)

(b)

(c)

(d)

Figure 3: The generated samples from original GAN (a)/(c) and proposed GAN (b)/(d). In (a)/(b),
the grey area is the 2D histogram of training in-distribution samples drawn from a mixture of two
Gaussian distributions and red points indicate generated samples by GANs.

(a) In-distribution: SVHN

(b) In-distribution: CIFAR-10

Figure 4: Performances of the baseline detector (Hendrycks & Gimpel, 2016) under various training
losses. For training models by the conﬁdence loss, the KL divergence term is optimized using
samples indicated in the parentheses. For fair comparisons, we only plot the performances for
unseen out-of-distributions, where those for seen out-of-distributions (used for minimizing the KL
divergence term in (1)) can be found in Table 1.

3.2 EFFECTS OF ADVERSARIAL GENERATOR AND JOINT CONFIDENCE LOSS

In this section, we verify the effect of the proposed GAN in Section 2.2 and evaluate the detection
performance of the joint conﬁdence loss in (4). To verify that the proposed GAN can produce the
samples nearby the low-density boundary of the in-distribution space, we ﬁrst compare the gener-
ated samples by original GAN and proposed GAN on a simple example where the target distribution
is a mixture of two Gaussian distributions. For both the generator and discriminator, we use fully-
connected neural networks with 2 hidden layers. For our method, we use a pre-trained classiﬁer
which minimizes the cross entropy on target distribution samples and the KL divergence on out-of-
distribution samples generated by rejection sampling on a bounded 2D box. As shown in Figure
3(a), the samples of original GAN cover the high-density area of the target distribution while those
of proposed GAN does its boundary one (see Figure 3(b)). We also compare the generated samples
of original and proposed GANs on MNIST dataset (LeCun et al., 1998), which consists of hand-
written digits. For this experiment, we use deep convolutional GANs (DCGANs) (Radford et al.,
2015). In this case, we use a pre-trained classiﬁer which minimizes the cross entropy on MNIST

7

Published as a conference paper at ICLR 2018

(a) In-distribution: SVHN

(b) In-distribution: CIFAR-10

Figure 5: Guided gradient (sensitivity) maps of the top-1 predicted class with respect to the input
image under various training losses.

training samples and the KL divergence on synthetic Gaussian noises. As shown in Figure 3(c)
and 3(d), samples of original GAN looks more like digits than those of proposed GAN. Somewhat
interestingly, the proposed GAN still generates some new digit-like images.

We indeed evaluate the performance of our joint conﬁdence loss in (4) utilizing the proposed GAN.
To this end, we use VGGNets (as classiﬁers) and DCGANs (as GANs). We also test a variant of con-
ﬁdence loss which optimizes the KL divergence term on samples from a pre-trained original GAN
(implicitly) modeling the in-distribution. One can expect that samples from the original GAN can be
also useful for improving the detection performance since it may have bad generalization properties
(Arora et al., 2017) and generate a few samples on the low-density boundary as like the proposed
GAN. Figure 4 shows the performance of the baseline detector for each in- and out-of-distribution
pair. First, observe that the joint conﬁdence loss (blue bar) outperforms the conﬁdence loss with
some explicit out-of-distribution datasets (green bar). This is quite remarkable since the former
is trained only using in-distribution datasets, while the latter utilizes additional out-of-distribution
datasets. We also remark that our methods signiﬁcantly outperform the baseline cross entropy loss
(red bar) in all cases without harming its original classiﬁcation performances (see Table 2 in Ap-
pendix C). Interestingly, the conﬁdence loss with the original GAN (orange bar) is often (but not
always) useful for improving the detection performance, whereas that with the proposed GAN (blue
bar) still outperforms it in all cases.

Finally, we also provide visual interpretations of models using the guided gradient maps (Sprin-
genberg et al., 2014). Here, the gradient can be interpreted as an importance value of each pixel
which inﬂuences on the classiﬁcation decision. As shown in Figure 5, the model trained by the
cross entropy loss shows sharp gradient maps for both samples from in- and out-of-distributions,
whereas models trained by the conﬁdence losses do only on samples from in-distribution. For the
case of SVHN in-distribution, all conﬁdence losses gave almost zero gradients, which matches to
the results in Figure 4(a): their detection performances are almost perfect. For the case of CIFAR-
10 distribution, one can now observe that there exists some connection between gradient maps and
detection performances. This is intuitive because for detecting samples from out-of-distributions
better, the classiﬁer should look at more pixels as similar importance and the KL divergence term
forces it. We think that our visualization results might give some ideas in future works for developing
better inference methods for detecting out-of-distribution under our models.

4 CONCLUSION

In this paper, we aim to develop a training method for neural classiﬁcation networks for detecting
out-of-distribution better without losing its original classiﬁcation accuracy. In essence, our method
jointly trains two models for detecting and generating out-of-distribution by minimizing their losses
alternatively. Although we primarily focus on image classiﬁcation in our experiments, our method
can be used for any classiﬁcation tasks using deep neural networks. It is also interesting future
directions applying our methods for other related tasks: regression (Malinin et al., 2017), network
calibration (Guo et al., 2017), Bayesian probabilistic models (Li & Gal, 2017; Louizos & Welling,
2017), ensemble (Lakshminarayanan et al., 2017) and semi-supervised learning (Dai et al., 2017).

8

Published as a conference paper at ICLR 2018

ACKNOWLEDGEMENTS

This work was supported in part by the Institute for Information & communications Technology
Promotion(IITP) grant funded by the Korea government(MSIT) (No.2017-0-01778, Development
of Explainable Human-level Deep Machine Learning Inference Framework), the ICT R&D program
of MSIP/IITP [R-20161130-004520, Research on Adaptive Machine Learning Technology Devel-
opment for Intelligent Autonomous Digital Companion], DARPA Explainable AI (XAI) program
#313498 and Sloan Research Fellowship.

REFERENCES

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Con-

crete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). In International Conference on Machine Learning (ICML),
2017.

Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible
In ACM

models for healthcare: Predicting pneumonia risk and hospital 30-day readmission.
SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015.

Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan Salakhutdinov. Good semi-
supervised learning that requires a bad gan. In Advances in neural information processing systems
(NIPS), 2017.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale

hierarchical image database. In Computer Vision and Pattern Recognition (CVPR), 2009.

Ross Girshick. Fast r-cnn. In International Conference on Computer Vision (ICCV), 2015.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems (NIPS), 2014.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural

networks. In International Conference on Machine Learning (ICML), 2017.

Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger,
Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end
speech recognition. arXiv preprint arXiv:1412.5567, 2014.

Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution
examples in neural networks. In International Conference on Learning Representations (ICLR),
2016.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-
nov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning (ICML), 2015.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations (ICLR), 2014.

Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint

arXiv:1404.5997, 2014.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic-
tive uncertainty estimation using deep ensembles. In Advances in neural information processing
systems (NIPS), 2017.

9

Published as a conference paper at ICLR 2018

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Kimin Lee, Changho Hwang, KyoungSoo Park, and Jinwoo Shin. Conﬁdent multiple choice learn-

ing. In International Conference on Machine Learning (ICML), 2017.

Yingzhen Li and Yarin Gal. Dropout inference in bayesian neural networks with alpha-divergences.

In International Conference on Machine Learning (ICML), 2017.

Shiyu Liang, Yixuan Li, and R Srikant. Principled detection of out-of-distribution examples in

neural networks. arXiv preprint arXiv:1706.02690, 2017.

Christos Louizos and Max Welling. Multiplicative normalizing ﬂows for variational bayesian neural

networks. In International Conference on Machine Learning (ICML), 2017.

Andrey Malinin, Anton Ragni, Kate Knill, and Mark Gales. Incorporating uncertainty into deep
In Proceedings of the 55th Annual Meeting of the
learning for spoken language assessment.
Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pp. 45–50, 2017.

Mahdi Pakdaman Naeini, Gregory F Cooper, and Milos Hauskrecht. Obtaining well calibrated

probabilities using bayesian binning. In AAAI, 2015.

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, volume 2011, pp. 5, 2011.

Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing conﬁdent output distributions. arXiv preprint arXiv:1701.06548,
2017.

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In International Conference on Learning Repre-
sentations (ICLR), 2015.

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
In Advances in neural information processing systems

Improved techniques for training gans.
(NIPS), 2016.

Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the
In International

pixelcnn with discretized logistic mixture likelihood and other modiﬁcations.
Conference on Learning Representations (ICLR), 2017.

Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russ Webb.
In Computer

Learning from simulated and unsupervised images through adversarial training.
Vision and Pattern Recognition (CVPR), 2017.

Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for

simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Computer Vision and Pattern Recognition (CVPR), 2015.

Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee. Learning
to generate long-term future via hierarchical prediction. In International Conference on Machine
Learning (ICML), 2017.

Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.

Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. In

International Conference on Learning Representations (ICLR), 2017.

10

Published as a conference paper at ICLR 2018

A THRESHOLD-BASED DETECTORS

In this section, we formally describe the detection procedure of threshold-based detectors
(Hendrycks & Gimpel, 2016; Liang et al., 2017). For each data x, it measures some conﬁdence
score q(x) by feeding the data into a pre-trained classiﬁer. Here, (Hendrycks & Gimpel, 2016) de-
ﬁned the conﬁdence score as a maximum value of the predictive distribution, and (Liang et al., 2017)
further improved the performance by processing the predictive distribution (see Appendix C.3 for
more details). Then, the detector, g (x) : X → {0, 1}, assigns label 1 if the conﬁdence score q(x) is
above some threshold δ, and label 0, otherwise:

(cid:26)

g (x) =

1
0

if q(x) ≥ δ,
otherwise.

For this detector, we have to ﬁnd a score threshold so that some positive examples are classiﬁed
correctly, but this depends upon the trade-off between false negatives and false positives. To handle
this issue, we use threshold-independent evaluation metrics such as area under the receiver operating
characteristic curve (AUORC) and detection accuracy (see Appendix B).

B EXPERIMENTAL SETUPS IN SECTION 3

Datasets. We train deep models such as VGGNet (Szegedy et al., 2015) and AlexNet (Krizhevsky,
2014) for classifying CIFAR-10 and SVHN datasets: the former consists of 50,000 training and
10,000 test images with 10 image classes, and the latter consists of 73,257 training and 26,032 test
images with 10 digits.2 The corresponding test dataset are used as the in-distribution (positive)
samples to measure the performance. We use realistic images and synthetic noises as the out-of-
distribution (negative) samples: the TinyImageNet consists of 10,000 test images with 200 image
classes from a subset of ImageNet images. The LSUN consists of 10,000 test images of 10 different
scenes. We downsample each image of TinyImageNet and LSUN to size 32 × 32. The Gaussian
noise is independently and identically sampled from a Gaussian distribution with mean 0.5 and
variance 1. We clip each pixel value into the range [0, 1].

Detailed CNN structure and training. The simple CNN that we use for evaluation shown in Figure
2 consists of two convolutional layers followed by three fully-connected layers. Convolutional layers
have 128 and 256 ﬁlters, respectively. Each convolutional layer has a 5 × 5 receptive ﬁeld applied
with a stride of 1 pixel each followed by max pooling layer which pools 2 × 2 regions at strides of
2 pixels. AlexNet (Krizhevsky, 2014) consists of ﬁve convolutitonal layers followed by three fully-
connected layers. Convolutional layers have 64, 192, 384, 256 and 256 ﬁlters, respectively. First and
second convolutional layers have a 5×5 receptive ﬁeld applied with a stride of 1 pixel each followed
by max pooling layer which pools 3 × 3 regions at strides of 2 pixels. Other convolutional layers
have a 3 × 3 receptive ﬁeld applied with a stride of 1 pixel followed by max pooling layer which
pools 2 × 2 regions at strides of 2 pixels. Fully-connected layers have 2048, 1024 and 10 hidden
units, respectively. Dropout (Hinton et al., 2012) was applied to only fully-connected layers of the
network with the probability of retaining the unit being 0.5. All hidden units are ReLUs. Figure
6 shows the detailed structure of VGGNet (Szegedy et al., 2015) with three fully-connected layers
and 10 convolutional layers. Each ConvReLU box in the ﬁgure indicates a 3 × 3 convolutional layer
followed by ReLU activation. Also, all max pooling layers have 2 × 2 receptive ﬁelds with stride 2.
Dropout was applied to only fully-connected layers of the network with the probability of retaining
the unit being 0.5. For all experiments, the softmax classiﬁer is used, and each model is trained by
optimizing the objective function using Adam learning rule (Kingma & Ba, 2014). For each out-of-
distribution dataset, we randomly select 1,000 images for tuning the penalty parameter β, mini-batch
size and learning rate. The penalty parameter is chosen from β ∈ {0, 0.1, . . . 1.9, 2}, the mini-batch
size is chosen from {64, 128} and the learning rate is chosen from {0.001, 0.0005, 0.0002}. The
optimal parameters are chosen to minimize the detection error on the validation set. We drop the
learning rate by 0.1 at 60 epoch and models are trained for total 100 epochs. The best test result is
reported for each method.

Performance metrics. We measure the following metrics using threshold-based detectors:

2We do not use the extra SVHN dataset for training.

11

Published as a conference paper at ICLR 2018

• True negative rate (TNR) at 95% true positive rate (TPR). Let TP, TN, FP, and FN de-
note true positive, true negative, false positive and false negative, respectively. We measure
TNR = TN / (FP+TN), when TPR = TP / (TP+FN) is 95%.

• Area under the receiver operating characteristic curve (AUROC). The ROC curve is a
graph plotting TPR against the false positive rate = FP / (FP+TN) by varying a threshold.
• Area under the precision-recall curve (AUPR). The PR curve is a graph plotting the
precision = TP / (TP+FP) against recall = TP / (TP+FN) by varying a threshold. AUPR-IN
(or -OUT) is AUPR where in- (or out-of-) distribution samples are speciﬁed as positive.
• Detection accuracy. This metric corresponds to the maximum classiﬁcation proba-
(cid:8)Pin (q (x) ≤ δ) P (x is from Pin) +
bility over all possible thresholds δ: 1 − minδ
Pout (q (x) > δ) P (x is from Pout) (cid:9), where q(x) is a conﬁdent score such as a maxi-
mum value of softmax. We assume that both positive and negative examples have equal
probability of appearing in the test set, i.e., P (x is from Pin) = P (x is from Pout) = 0.5.

Note that AUROC, AUPR and detection accuracy are threshold-independent evaluation metrics.

Figure 6: Detailed structure of VGGNet with 13 layers.

Generating samples on a simple example. As shown in Figure 3(a) and Figure 3(b), we compare
the generated samples by original GAN and proposed GAN on a simple example where the target
distribution is a mixture of two Gaussian distributions. For both the generator and discriminator, we
use fully-connected neural networks with 2 hidden layers and 500 hidden units for each layer. For
all layers, we use ReLU activation function. We use a 100-dimensional Gaussian prior for the latent
variable z. For our method, we pre-train the simple fully-connected neural networks (2 hidden layers
and 500 ReLU units for each layer) by minimizing the cross entropy on target distribution samples
and the KL divergence on out-of-distribution samples generated by rejection sampling on bounded
2D box [−10, 10]2. The penalty parameter β is set to 1. We use ADAM learning rule (Kingma &
Ba, 2014) with a mini-batch size of 400. The initial learning rate is set to 0.002, and we train for
total 100 epochs.

Generating samples on MNIST. As shown in Figure 3(c) and Figure 3(d), we compare the gener-
ated samples of original and proposed GANs on MNIST dataset, which consists of greyscale images,
each containing a digit 0 to 9 with 60,000 training and 10,000 test images. We expand each image
to size 3 × 32 × 32. For both the generator and discriminator, we use deep convolutional GANs
(DCGANs) (Radford et al., 2015). The discriminator and generator consist of four convolutional
and deconvolutional layers, respectively. Convolutional layers have 128, 256, 512 and 1 ﬁlters, re-
spectively. Each convolutional layer has a 4 × 4 receptive ﬁeld applied with a stride of 2 pixel. The
second and third convolutional layers are followed by batch normalization (Ioffe & Szegedy, 2015).
For all layers, we use LeakyReLU activation function. Deconvolutional layers have 512, 256, 128
and 1 ﬁlters, respectively. Each deconvolutional layer has a 4 × 4 receptive ﬁeld applied with a
stride of 2 pixel followed by batch normalization (Ioffe & Szegedy, 2015) and ReLU activation.
For our method, we use a pre-trained simple CNNs (two convolutional layers followed by three
fully-connected layers) by minimizing the cross entropy on MNIST training samples and the KL
divergence on synthetic Gaussian noise. Convolutional layers have 128 and 256 ﬁlters, respectively.
Each convolutional layer has a 5 × 5 receptive ﬁeld applied with a stride of 1 pixel each followed by
max pooling layer which pools 2 × 2 regions at strides of 2 pixels. The penalty parameter β is set
to 1. We use ADAM learning rule (Kingma & Ba, 2014) with a mini-batch size of 128. The initial
learning rate is set to 0.0002, and we train for total 50 epochs.

12

Published as a conference paper at ICLR 2018

C MORE EXPERIMENTAL RESULTS

C.1 CLASSIFICATION PERFORMANCES

Table 2 reports the classiﬁcation accuracy of VGGNets on CIFAR-10 and SVHN datasets under
various training losses shown in Figure 4. One can note that all methods do not degrade the original
classiﬁcation performance, where the differences in classiﬁcation errors of across all tested single
models are at most 1% in our experiments.

In-distribution Cross entropy

Joint conﬁdence loss

Conﬁdence loss
with original GAN

Conﬁdence loss with explicit
out-of-distribution samples

CIFAR-10
SVHN

80.14
93.82

80.27
94.08

81.39
93.81

80.56
94.23

Table 2: Classiﬁcation test set accuracy of VGGNets on CIFAR-10 and SVHN datasets under vari-
ous training losses.

C.2 CALIBRATION EFFECTS OF CONFIDENCE LOSS

We also verify the calibration effects (Guo et al., 2017) of our methods: whether a classiﬁer trained
by our method can indicate when they are likely to be incorrect for test samples from the in-
distribution. In order to evaluate the calibration effects, we measure the expected calibration error
(ECE) (Naeini et al., 2015). Given test data {(x1, y1) , . . . , (xn, yn)}, we group the predictions into
M interval bins (each of size 1/M ). Let Bm be the set of indices of samples whose prediction
conﬁdence falls into the interval ( m−1

M , m
M ]. Then, the accuracy of Bm is deﬁned as follows:
1
|Bm|

1{yi=arg max

Pθ(y|xi)},

(cid:88)

y

i∈Bm

acc(Bm) =

where θ is the model parameters of a classiﬁer and 1A ∈ {0, 1} is the indicator function for event
A. We also deﬁne the conﬁdence of Bm as follows:
1
|Bm|

conf(Bm) =

q(xi),

(cid:88)

i∈Bm

where q(xi) is the conﬁdence of data i. Using these notations, we measure the expected calibration
error (ECE) as follows:

ECE =

|acc(Bm) − conf(Bm)|.

M
(cid:88)

m=1

|Bm|
n

One can note that ECE is zero if the conﬁdence score can represent the true distribution. Table
3 shows the calibration effects of conﬁdence loss when we deﬁne the conﬁdence score q as the
maximum predictive distribution of a classiﬁer. We found that the ECE of a classiﬁer trained by our
methods is lower than that of a classiﬁer trained by the standard cross entropy loss. This implies
that our proposed method is effective at calibrating predictions. We also remark that the temperature
scaling (Guo et al., 2017) provides further improvements under a classiﬁer trained by our joint
conﬁdence loss.

In-distribution

Without temperature scaling

With temperature scaling

Cross entropy loss

Joint conﬁdence loss Cross entropy loss

Joint conﬁdence loss

CIFAR-10

SVHN

18.45

5.30

14.62

5.13

7.07

2.80

6.19

1.39

Table 3: Expected calibration error (ECE) of VGGNets on CIFAR-10 and SVHN datasets under
various training losses. The number of bins M is set to 20. All values are percentages and boldface
values indicate relative the better results.

13

Published as a conference paper at ICLR 2018

C.3 EXPERIMENTAL RESULTS USING ODIN DETECTOR

In this section, we verify the effects of conﬁdence loss using ODIN detector (Liang et al., 2017)
which is an advanced threshold-based detector using temperature scaling (Guo et al., 2017) and
input perturbation. The key idea of ODIN is the temperature scaling which is deﬁned as follows:

Pθ(y = (cid:98)y|x; T ) =

(cid:80)

(cid:98)y(x)/T )

exp (f
y exp (fy(x)/T )

,

where T > 0 is the temperature scaling parameter and f = (f1, . . . , fK) is ﬁnal feature vector of
neural networks. For each data x, ODIN ﬁrst calculates the pre-processed image (cid:98)x by adding the
small perturbations as follows:

x(cid:48) = x − εsign (− (cid:53)x log Pθ(y = (cid:98)y|x; T )) ,
where ε is a magnitude of noise and (cid:98)y is the predicted label. Next, ODIN feeds the pre-
processed data into the classiﬁer, computes the maximum value of scaled predictive distribution,
i.e., maxy Pθ(y|x(cid:48); T ), and classiﬁes it as positive (i.e., in-distribution) if the conﬁdence score is
above some threshold δ.

For ODIN detector, the perturbation noise ε is chosen from {0, 0.0001, 0.001, 0.01}, and the tem-
perature T is chosen from {1, 10, 100, 500, 1000}. The optimal parameters are chosen to minimize
the detection error on the validation set. Figure 7 shows the performance of the OIDN and baseline
detector for each in- and out-of-distribution pair. First, we remark that the baseline detector using
classiﬁers trained by our joint conﬁdence loss (blue bar) typically outperforms the ODIN detec-
tor using classiﬁers trained by the cross entropy loss (orange bar). This means that our classiﬁer
can map in- and out-of-distributions more separately without pre-processing methods such as tem-
perature scaling. The ODIN detector provides further improvements if one uses it with our joint
conﬁdence loss (green bar). In other words, our proposed training method can improve all prior
detection methods.

(a) In-distribution: SVHN

Figure 7: Performances of the baseline detector (Hendrycks & Gimpel, 2016) and ODIN detector
(Liang et al., 2017) under various training losses.

(b) In-distribution: CIFAR-10

C.4 EXPERIMENTAL RESULTS ON ALEXNET

Table 4 shows the detection performance for each in- and out-of-distribution pair when the classiﬁer
is AlexNet (Krizhevsky, 2014), which is one of popular CNN architectures. We remark that they
show similar trends.

14

Published as a conference paper at ICLR 2018

In-dist

Out-of-dist

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

Baseline
(SVHN)

Baseline
(CIFAR-10)

ODIN
(SVHN)

ODIN
(CIFAR-10)

CIFAR-10 (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

CIFAR-10 (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

92.14 / 93.77

76.58 / 76.18

92.14 / 93.77

76.58 / 76.18

Cross entropy loss / Conﬁdence loss

42.0 / 99.9
45.6 / 99.9
44.6 / 100.0
58.6 / 100.0

12.8 / 99.6
10.3 / 10.1
10.7 / 8.1
6.7 / 1.0

55.5 / 99.9
59.5 / 99.9
61.5 / 100.0
82.6 / 100.0

37.1 / 99.6
11.4 / 8.4
13.3 / 7.1
3.8 / 0.0

88.0 / 100.0
89.4 / 100.0
89.8 / 100.0
94.2 / 100.0

71.0 / 99.9
59.2 / 52.1
56.3 / 51.5
49.6 / 13.5

89.1 / 99.2
90.5 / 99.3
91.8 / 99.3
97.0 / 99.3

86.7 / 99.6
69.1 / 65.6
71.9 / 67.1
70.9 / 57.2

83.4 / 99.8
84.3 / 99.9
84.5 / 99.9
88.8 / 100.0

73.2 / 99.6
64.2 / 62.0
64.3 / 61.8
61.3 / 50.0

82.4 / 99.8
83.8 / 99.9
84.8 / 99.9
91.6 / 100.0

79.3 / 99.6
64.4 / 61.8
75.3 / 63.7
69.3 / 40.4

88.7 / 99.9
90.2 / 100.0
90.8 / 100.0
95.5 / 100.0

74.3 / 99.9
63.6 / 59.8
62.3 / 59.5
58.5 / 43.7

85.9 / 100.0
87.5 / 100.0
90.5 / 100.0
97.4 / 100.0

88.1 / 99.9
71.4 / 68.6
67.2 / 72.0
78.1 / 56.1

87.3 / 99.3
88.6 / 99.3
88.4 / 99.3
92.5 / 99.3

70.7 / 99.6
64.4 / 62.3
65.3 / 61.6
59.5 / 32.0

89.0 / 99.2
90.4 / 99.3
91.3 / 99.3
96.4 / 99.3

84.2 / 99.6
64.6 / 60.7
65.3 / 60.5
60.7 / 40.7

Table 4: Performance of the baseline detector (Hendrycks & Gimpel, 2016) and ODIN detector
(Liang et al., 2017) using AlexNet. All values are percentages and boldface values indicate rela-
tive the better results. For each in-distribution, we minimize the KL divergence term in (1) using
training samples from an out-of-distribution dataset denoted by “seen”, where other “unseen” out-
of-distributions were also used for testing.

D MAXIMIZING ENTROPY

One might expect that the entropy of out-of-distribution is expected to be much higher compared to
that of in-distribution since the out-of-distribution is typically on a much larger space than the in-
distribution. Therefore, one can add maximizing the entropy of generator distribution to new GAN
loss in (3) and joint conﬁdence loss in (4). However, maximizing the entropy of generator distribu-
tion is technically challenging since a GAN does not model the generator distribution explicitly. To
handle the issue, one can leverage the pull-away term (PT) (Zhao et al., 2017):

−H (PG (x)) (cid:119) PT (PG (x)) =

1
M (M − 1)

M
(cid:88)

(cid:88)

(cid:32)

i=1

j(cid:54)=i

G (zi)(cid:62) G (zj)
(cid:107)G (zi)(cid:107)(cid:107)G (zj)(cid:107)

(cid:33)2

,

where H (·) denotes the entropy, zi, zj ∼ Ppri (z) and M is the number of samples. Intuitively,
one can expect the effect of increasing the entropy by minimizing PT since it corresponds to the
squared cosine similarity of generated samples. We note that (Dai et al., 2017) also used PT to
maximize the entropy. Similarly as in Section 3.2, we verify the effects of PT using VGGNet. Table
5 shows the performance of the baseline detector for each in- and out-of-distribution pair. We found
that joint conﬁdence loss with PT tends to (but not always) improve the detection performance.
However, since PT increases the training complexity and the gains from PT are relatively marginal
(or controversial), we leave it as an auxiliary option for improving the performance.

In-dist

Out-of-dist

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

SVHN

93.81 / 94.05

CIFAR-10

81.39 / 80.60

CIFAR-10
TinyImageNet
LSUN

SVHN
TinyImageNet
LSUN

Joint conﬁdence loss without PT / with PT

90.1 / 92.3
99.0 / 99.9
98.9 / 100.0

97.6 / 98.1
99.6 / 100.0
99.6 / 100.0

25.4 / 13.2
35.0 / 44.8
39.1 / 49.1

66.8 / 69.5
72.0 / 78.4
75.1 / 80.7

93.6 / 94.6
97.6 / 99.7
97.5 / 99.9

74.2 / 75.1
76.4 / 77.6
77.8 / 78.7

97.7 / 98.2
99.7 / 100.0
99.7 / 100.0

97.9 / 98.7
94.5 / 100.0
95.5 / 100.0

71.3 / 73.5
74.7 / 79.4
77.1 / 81.3

78.3 / 72.0
82.2 / 84.4
83.6 / 85.8

Table 5: Performance of the baseline detector (Hendrycks & Gimpel, 2016) using VGGNets trained
by joint conﬁdence loss with and without pull-away term (PT). All values are percentages and bold-
face values indicate relative the better results.

15

Published as a conference paper at ICLR 2018

E ADDING OUT-OF-DISTRIBUTION CLASS

Instead of forcing the predictive distribution on out-of-distribution samples to be closer to the uni-
form one, one can simply add an additional “out-of-distribution” class to a classiﬁer as follows:

min
θ

EPin((cid:98)x,(cid:98)y)

(cid:2) − log Pθ (y = (cid:98)y|(cid:98)x) (cid:3) + EPout(x)

(cid:2) − log Pθ (y = K + 1|x) (cid:3),

(5)

where θ is a model parameter. Similarly as in Section 3.1, we compare the performance of the
conﬁdence loss with that of above loss in (5) using VGGNets with for image classiﬁcation on SVHN
dataset. To optimize the KL divergence term in conﬁdence loss and the second term of (5), CIFAR-
10 training datasets are used. In order to compare the detection performance, we deﬁne the the
conﬁdence score of input x as 1 − Pθ (y = K + 1|x) in case of (5). Table 6 shows the detection
performance for out-of-distribution. First, the classiﬁer trained by our method often signiﬁcantly
outperforms the alternative adding the new class label. This is because modeling explicitly out-of-
distribution can incur overﬁtting to trained out-of-distribution dataset.

Detector

Out-of-dist

Baseline
detector

ODIN
detector

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

K + 1 class loss in (5) / Conﬁdence loss

79.61 / 80.56

79.61 / 80.56

99.6 / 99.8
0.0 / 9.9
0.0 / 10.5
0.0 / 3.3

99.6 / 99.8
0.3 / 12.2
0.1 / 13.7
0.0 / 8.2

99.8 / 99.9
5.2 / 31.8
5.6 / 34.8
0.1 / 14.1

99.9 / 99.8
47.3 / 70.6
48.3 / 73.1
28.3 / 68.3

99.7 / 99.8
51.3 / 58.6
51.5 / 60.2
50.0 / 50.0

99.1 / 99.8
55.12 / 65.7
55.9 / 67.9
54.4 / 65.4

99.8 / 99.9
50.7 / 55.3
50.8 / 56.4
49.3 / 49.4

99.9 / 99.9
56.6 / 72.7
57.5 / 75.2
47.8 / 74.1

99.9 / 99.8
64.3 / 66.1
71.5 / 68.0
12.2 / 47.0

99.9 / 99.8
44.3 / 65.6
44.7 / 67.8
36.8 / 61.5

Table 6: Performance of the baseline detector (Hendrycks & Gimpel, 2016) and ODIN detector
(Liang et al., 2017) using VGGNet. All values are percentages and boldface values indicate relative
the better results.

16

8
1
0
2
 
b
e
F
 
3
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
5
2
3
9
0
.
1
1
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

TRAINING CONFIDENCE-CALIBRATED CLASSIFIERS
FOR DETECTING OUT-OF-DISTRIBUTION SAMPLES

Honglak Lee§ ,†

Kimin Lee∗
Jinwoo Shin∗
∗Korea Advanced Institute of Science and Technology, Daejeon, Korea
†University of Michigan, Ann Arbor, MI 48109
§Google Brain, Mountain View, CA 94043

Kibok Lee†

ABSTRACT

The problem of detecting whether a test sample is from in-distribution (i.e., train-
ing distribution by a classiﬁer) or out-of-distribution sufﬁciently different from it
arises in many real-world machine learning applications. However, the state-of-art
deep neural networks are known to be highly overconﬁdent in their predictions,
i.e., do not distinguish in- and out-of-distributions. Recently, to handle this is-
sue, several threshold-based detectors have been proposed given pre-trained neu-
ral classiﬁers. However, the performance of prior works highly depends on how
to train the classiﬁers since they only focus on improving inference procedures.
In this paper, we develop a novel training method for classiﬁers so that such in-
ference algorithms can work better. In particular, we suggest two additional terms
added to the original loss (e.g., cross entropy). The ﬁrst one forces samples from
out-of-distribution less conﬁdent by the classiﬁer and the second one is for (im-
plicitly) generating most effective training samples for the ﬁrst one. In essence,
our method jointly trains both classiﬁcation and generative neural networks for
out-of-distribution. We demonstrate its effectiveness using deep convolutional
neural networks on various popular image datasets.

1

INTRODUCTION

Deep neural networks (DNNs) have demonstrated state-of-the-art performance on many classiﬁ-
cation tasks, e.g., speech recognition (Hannun et al., 2014), image classiﬁcation (Girshick, 2015),
video prediction (Villegas et al., 2017) and medical diagnosis (Caruana et al., 2015). Even though
DNNs achieve high accuracy, it has been addressed (Lakshminarayanan et al., 2017; Guo et al.,
2017) that they are typically overconﬁdent in their predictions. For example, DNNs trained to clas-
sify MNIST images often produce high conﬁdent probability 91% even for random noise (see the
work of (Hendrycks & Gimpel, 2016)). Since evaluating the quality of their predictive uncertainty
is hard, deploying them in real-world systems raises serious concerns in AI Safety (Amodei et al.,
2016), e.g., one can easily break a secure authentication system that can be unlocked by detecting
the gaze and iris of eyes using DNNs (Shrivastava et al., 2017).

The overconﬁdence issue of DNNs is highly related to the problem of detecting out-of-distribution:
detect whether a test sample is from in-distribution (i.e., training distribution by a classiﬁer) or out-
of-distribution sufﬁciently different from it. Formally, it can be formulated as a binary classiﬁcation
problem. Let an input x ∈ X and a label y ∈ Y = {1, . . . , K} be random variables that follow a
joint data distribution Pin (x, y) = Pin (y|x) Pin (x). We assume that a classiﬁer Pθ (y|x) is trained
on a dataset drawn from Pin (x, y), where θ denotes the model parameter. We let Pout (x) denote
an out-of-distribution which is ‘far away’ from in-distribution Pin (x). Our problem of interest is
determining if input x is from Pin or Pout, possibly utilizing a well calibrated classiﬁer Pθ (y|x).
In other words, we aim to build a detector, g (x) : X → {0, 1}, which assigns label 1 if data is from
in-distribution, and label 0 otherwise.

There have been recent efforts toward developing efﬁcient detection methods where they mostly
have studied simple threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017) uti-
lizing a pre-trained classiﬁer. For each input x, it measures some conﬁdence score q(x) based on a
pre-trained classiﬁer, and compares the score to some threshold δ > 0. Then, the detector assigns

1

Published as a conference paper at ICLR 2018

label 1 if the conﬁdence score q(x) is above δ, and label 0, otherwise. Speciﬁcally, (Hendrycks
& Gimpel, 2016) deﬁned the conﬁdence score as a maximum value of the predictive distribution,
and (Liang et al., 2017) further improved the performance by using temperature scaling (Guo et al.,
2017) and adding small controlled perturbations to the input data. Although such inference methods
are computationally simple, their performances highly depend on the pre-trained classiﬁer. Namely,
they fail to work if the classiﬁer does not separate the maximum value of predictive distribution
well enough with respect to Pin and Pout. Ideally, a classiﬁer should be trained to separate all
class-dependent in-distributions as well as out-of-distribution in the output space. As another line of
research, Bayesian probabilistic models (Li & Gal, 2017; Louizos & Welling, 2017) and ensembles
of classiﬁers (Lakshminarayanan et al., 2017) were also investigated. However, training or inferring
those models are computationally more expensive. This motivates our approach of developing a
new training method for the more plausible simple classiﬁers. Our direction is orthogonal to the
Bayesian and ensemble approaches, where one can also combine them for even better performance.

Contribution. In this paper, we develop such a training method for detecting out-of-distribution
Pout better without losing its original classiﬁcation accuracy. First, we consider a new loss function,
called conﬁdence loss. Our key idea on the proposed loss is to additionally minimize the Kullback-
Leibler (KL) divergence from the predictive distribution on out-of-distribution samples to the uni-
form one in order to give less conﬁdent predictions on them. Then, in- and out-of-distributions are
expected to be more separable. However, optimizing the conﬁdence loss requires training samples
from out-of-distribution, which are often hard to sample: a priori knowledge on out-of-distribution
is not available or its underlying space is too huge to cover. To handle the issue, we consider a
new generative adversarial network (GAN) (Goodfellow et al., 2014) for generating most effective
samples from Pout. Unlike the original GAN, the proposed GAN generates ‘boundary’ samples in
the low-density area of Pin. Finally, we design a joint training scheme minimizing the classiﬁer’s
loss and new GAN loss alternatively, i.e., the conﬁdent classiﬁer improves the GAN, and vice versa,
as training proceeds. Here, we emphasize that the proposed GAN does not need to generate explicit
samples under our scheme, and instead it implicitly encourages training a more conﬁdent classiﬁer.

We demonstrate the effectiveness of the proposed method using deep convolutional neural networks
such as AlexNet (Krizhevsky, 2014) and VGGNet (Szegedy et al., 2015) for image classiﬁcation
tasks on CIFAR (Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), ImageNet (Deng et al.,
2009), and LSUN (Yu et al., 2015) datasets. The classiﬁer trained by our proposed method dras-
tically improves the detection performance of all threshold-based detectors (Hendrycks & Gim-
pel, 2016; Liang et al., 2017) in all experiments. In particular, VGGNet with 13 layers trained by
our method improves the true negative rate (TNR), i.e., the fraction of detected out-of-distribution
(LSUN) samples, compared to the baseline: 14.0% → 39.1% and 46.3% → 98.9% on CIFAR-10
and SVHN, respectively, when 95% of in-distribution samples are correctly detected. We also pro-
vide visual understandings on the proposed method using the image datasets. We believe that our
method can be a strong guideline when other researchers will pursue these tasks in the future.

2 TRAINING CONFIDENT NEURAL CLASSIFIERS

In this section, we propose a novel training method for classiﬁers in order to improve the perfor-
mance of prior threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017) (see Ap-
pendix A for more details). Our motivation is that such inference algorithms can work better if the
classiﬁers are trained so that they map the samples from in- and out-of-distributions into the output
space separately. Namely, we primarily focus on training an improved classiﬁer, and then use prior
detectors under the trained model to measure its performance.

2.1 CONFIDENT CLASSIFIER FOR OUT-OF-DISTRIBUTION

Without loss of generality, suppose that the cross entropy loss is used for training. Then, we propose
the following new loss function, termed conﬁdence loss:

min
θ

EPin((cid:98)x,(cid:98)y)

(cid:2) − log Pθ (y = (cid:98)y|(cid:98)x) (cid:3) + βEPout(x)

(cid:2)KL (U (y) (cid:107) Pθ (y|x)) (cid:3),

(1)

where KL denotes the Kullback-Leibler (KL) divergence, U (y) is the uniform distribution and
β > 0 is a penalty parameter. It is highly intuitive as the new loss forces the predictive distribution

2

Published as a conference paper at ICLR 2018

(a)

(b)

(c)

(d)

Figure 1: Illustrating the behavior of classiﬁer under different out-of-distribution training datasets.
We generate the out-of-distribution samples from (a) 2D box [−50, 50]2, and show (b) the corre-
sponding decision boundary of classiﬁer. We also generate the out-of-distribution samples from (c)
2D box [−20, 20]2, and show (d) the corresponding decision boundary of classiﬁer.

on out-of-distribution samples to be closer to the uniform one, i.e., zero conﬁdence, while that
for samples from in-distribution still follows the label-dependent probability. In other words, the
proposed loss is designed for assigning higher maximum prediction values, i.e., maxy Pθ (y|x), to
in-distribution samples than out-of-distribution ones. Here, a caveat is that adding the KL divergence
term might degrade the classiﬁcation performance. However, we found that it is not the case due
to the high expressive power of deep neural networks, while in- and out-of-distributions become
more separable with respect to the maximum prediction value by optimizing the conﬁdence loss
(see Section 3.1 for supporting experimental results).

We remark that minimizing a similar KL loss was studied recently for different purposes (Lee et al.,
2017; Pereyra et al., 2017). Training samples for minimizing the KL divergence term is explicitly
given in their settings while we might not. Ideally, one has to sample all (almost inﬁnite) types of out-
of-distribution to minimize the KL term in (1), or require some prior information on testing out-of-
distribution for efﬁcient sampling. However, this is often infeasible and fragile. To address the issue,
we suggest to sample out-of-distribution close to in-distribution, which could be more effective in
improving the detection performance, without any assumption on testing out-of-distribution.

In order to explain our intuition in details, we consider a binary classiﬁcation task on a simple ex-
ample, where each class data is drawn from a Gaussian distribution and entire data space is bounded
by 2D box [−50, 50]2 for visualization. We apply the conﬁdence loss to simple fully-connected
neural networks (2 hidden layers and 500 hidden units for each layer) using different types of out-
of-distribution training samples. First, as shown in Figure 1(a), we construct an out-of-distribution
training dataset of 100 (green) points using rejection sampling on the entire data space [−50, 50]2.
Figure 1(b) shows the decision boundary of classiﬁer optimizing the conﬁdence loss on the corre-
sponding dataset. One can observe that a classiﬁer still shows overconﬁdent predictions (red and
blue regions) near the labeled in-distribution region. On the other hand, if we construct a training
out-of-distribution dataset of 100 points from [−20, 20]2, i.e., closer to target, in-distribution space
(see Figure 1(c)), a classiﬁer produces conﬁdent predictions only on the labeled region and zero
conﬁdence on the remaining in the entire data space [−50, 50]2 as shown in Figure 1(d). If one
increases the number of training out-of-distribution samples which are generated from the entire
space, i.e., [−50, 50]2, Figure 1(b) is expected to be similar to Figure 1(d). In other words, one need
more samples in order to train a conﬁdent classiﬁer if samples are generated from the entire space.
However, this might be impossible and not efﬁcient since the number of out-of-distribution training
samples might be almost inﬁnite to cover its entire, huge actual data space. This implies that training
out-of-distribution samples nearby the in-distribution region could be more effective in improving
the detection performance. Our underlying intuition is that the effect of boundary of in-distribution
region might propagate to the entire out-of-distribution space. Our experimental results in Section
3.1 also support this: realistic images are more useful as training out-of-distribution than synthetic
datasets (e.g., Gaussian noise) for improving the detection performance when we consider an image
classiﬁcation task. This motivates us to develop a new generative adversarial network (GAN) for
generating such effective out-of-distribution samples.

2.2 ADVERSARIAL GENERATOR FOR OUT-OF-DISTRIBUTION

In this section, we introduce a new training method for learning a generator of out-of-distribution
inspired by generative adversarial network (GAN) (Goodfellow et al., 2014). We will ﬁrst assume

3

Published as a conference paper at ICLR 2018

that the classiﬁer for in-distribution is ﬁxed, and also describe the joint learning framework in the
next section.

The GAN framework consists of two main components: discriminator D and generator G. The
generator maps a latent variable z from a prior distribution Ppri (z) to generated outputs G (z), and
discriminator D : X → [0, 1] represents a probability that sample x is from a target distribution.
Suppose that we want to recover the in-distribution Pin(x) using the generator G. Then, one can
optimize the following min-max objective for forcing PG ≈ Pin:
EPin(x)

(cid:2) log D (x) (cid:3) + EPpri(z)

(cid:2) log (1 − D (G (z))) (cid:3).

(2)

min
G

max
D

However, unlike the original GAN, we want to make the generator recover an effective out-of-
distribution Pout instead of Pin. To this end, we propose the following new GAN loss:

min
G

max
D

β EPG(x)

(cid:124)

(cid:124)

+ EPin(x)

(cid:123)(cid:122)
(a)

(cid:2)KL (U (y) (cid:107) Pθ (y|x)) (cid:3)
(cid:125)
(cid:2) log (1 − D (x)) (cid:3)
,
(cid:125)

(cid:2) log D (x) (cid:3) + EPG(x)

(cid:123)(cid:122)
(b)

(3)

where θ is the model parameter of a classiﬁer trained on in-distribution. The above objective can be
interpreted as follows: the ﬁrst term (a) corresponds to a replacement of the out-of-distribution Pout
in (1)’s KL loss with the generator distribution PG. One can note that this forces the generator to
generate low-density samples since it can be interpreted as minimizing the log negative likelihood
of in-distribution using the classiﬁer, i.e., Pin(x) ≈ exp (KL (U (y) (cid:107) Pθ (y|x))) . We remark that
this approximation is also closely related to the inception score (Salimans et al., 2016) which is
popularly used as a quantitative measure of visual ﬁdelity of the samples. The second term (b) cor-
responds to the original GAN loss since we would like to have out-of-distribution samples close to
in-distribution, as mentioned in Section 2.1. Suppose that the model parameter of classiﬁer θ is set
appropriately such that the classiﬁer produces the uniform distribution for out of distribution sam-
ples. Then, the KL divergence term (a) in (3) is approximately 0 no matter what out-of-distribution
samples are generated. However, if the samples are far away from boundary, the GAN loss (b) in (3)
should be high, i.e., the GAN loss forces having samples being not too far from the in-distribution
space. Therefore, one can expect that proposed loss can encourage the generator to produce the
samples which are on the low-density boundary of the in-distribution space. We also provide its
experimental evidences in Section 3.2.

We also remark that (Dai et al., 2017) consider a similar GAN generating samples from out-of-
distribution for the purpose of semi-supervised learning. The authors assume the existence of a pre-
trained density estimation model such as PixelCNN++ (Salimans et al., 2017) for in-distribution,
but such a model might not exist and be expensive to train in general. Instead, we use much simpler
conﬁdent classiﬁers for approximating the density. Hence, under our fully-supervised setting, our
GAN is much easier to train and more suitable.

2.3

JOINT TRAINING METHOD OF CONFIDENT CLASSIFIER AND ADVERSARIAL GENERATOR

min
G

min
θ

max
D

In the previous section, we suggest training the proposed GAN using a pre-trained conﬁdent classi-
ﬁer. We remind that the converse is also possible, i.e., the motivation of having such a GAN is for
training a better classiﬁer. Hence, two models can be used for improving each other. This naturally
suggests a joint training scheme where the conﬁdent classiﬁer improves the proposed GAN, and vice
versa, as training proceeds. Speciﬁcally, we suggest the following joint objective function:
(cid:2) − log Pθ (y = (cid:98)y|(cid:98)x) (cid:3)
(cid:2)KL (U (y) (cid:107) Pθ (y|x)) (cid:3)
(cid:125)
(cid:125)
(cid:2) log D ((cid:98)x) (cid:3) + EPG(x)

+β EPG(x)
(cid:124)

EPin((cid:98)x,(cid:98)y)
(cid:124)

(cid:123)(cid:122)
(e)
The classiﬁer’s conﬁdence loss corresponds to (c) + (d), and the proposed GAN loss corresponds
to (d) + (e), i.e., they share the KL divergence term (d) under joint training. To optimize the above
objective efﬁciently, we propose an alternating algorithm, which optimizes model parameters {θ}
of classiﬁer and GAN models {G, D} alternatively as shown in Algorithm 1. Since the algorithm
monotonically decreases the objective function, it is guaranteed to converge.

(cid:123)(cid:122)
(d)
(cid:2) log (1 − D (x)) (cid:3).
(cid:125)

+EPin((cid:98)x)
(cid:124)

(cid:123)(cid:122)
(c)

(4)

4

Published as a conference paper at ICLR 2018

Algorithm 1 Alternating minimization for detecting and generating out-of-distribution.

repeat

/∗ Update proposed GAN ∗/
Sample {z1, . . . , zM } and {x1, . . . , xM } from prior Ppri (z) and and in-distribution Pin (x),

respectively, and update the discriminator D by ascending its stochastic gradient of

1
M

M
(cid:88)

(cid:104)

i=1

log D (xi) + log (1 − D (G (zi)))

(cid:105)
.

Sample {z1, . . . , zM } from prior Ppri (z), and update the generator G by descending its

stochastic gradient of

1
M

M
(cid:88)

(cid:104)

i=1

log (1 − D (G (zi)))

+

KL (U (y) (cid:107) Pθ (y|G (zi)))

(cid:105)
.

(cid:105)

β
M

M
(cid:88)

(cid:104)

i=1

/∗ Update conﬁdent classiﬁer ∗/
Sample {z1, . . . , zM } and {(x1, y1) , . . . , (xM , yM )} from prior Ppri (z) and in-distribution

Pin (x, y), respectively, and update the classiﬁer θ by descending its stochastic gradient of

− log Pθ (y = yi|xi) + βKL (U (y) (cid:107) Pθ (y|G (zi)))

(cid:105)

.

1
M

M
(cid:88)

(cid:104)

i=1

until convergence

3 EXPERIMENTAL RESULTS

We demonstrate the effectiveness of our proposed method using various datasets: CIFAR
(Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), ImageNet (Deng et al., 2009), LSUN
(Yu et al., 2015) and synthetic (Gaussian) noise distribution. We train convolutional neural networks
(CNNs) including VGGNet (Szegedy et al., 2015) and AlexNet (Krizhevsky, 2014) for classifying
CIFAR-10 and SVHN datasets. The corresponding test dataset is used as the in-distribution (pos-
itive) samples to measure the performance. We use realistic images and synthetic noises as the
out-of-distribution (negative) samples. For evaluation, we measure the following metrics using the
threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017): the true negative rate
(TNR) at 95% true positive rate (TPR), the area under the receiver operating characteristic curve
(AUROC), the area under the precision-recall curve (AUPR), and the detection accuracy, where
larger values of all metrics indicate better detection performances. Due to the space limitation, more
explanations about datasets, metrics and network architectures are given in Appendix B.1

In-dist

Out-of-dist

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

SVHN

93.82 / 94.23

CIFAR-10 (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

CIFAR-10

80.14 / 80.56

Cross entropy loss / Conﬁdence loss

47.4 / 99.9
49.0 / 100.0
46.3 / 100.0
56.1 / 100.0

13.7 / 99.8
13.6 / 9.9
14.0 / 10.5
2.8 / 3.3

62.6 / 99.9
64.6 / 100.0
61.8 / 100.0
72.0 / 100.0

46.6 / 99.9
39.6 / 31.8
40.7 / 34.8
10.2 / 14.1

78.6 / 99.9
79.6 / 100.0
78.2 / 100.0
83.4 / 100.0

66.6 / 99.8
62.6 / 58.6
63.2 / 60.2
50.0 / 50.0

71.6 / 99.9
72.7 / 100.0
71.1 / 100.0
77.2 / 100.0

61.4 / 99.9
58.3 / 55.3
58.7 / 56.4
48.1 / 49.4

91.2 / 99.4
91.6 / 99.4
90.8 / 99.4
92.8 / 99.4

73.5 / 99.8
71.0 / 66.1
71.5 / 68.0
39.9 / 47.0

Table 1: Performance of the baseline detector (Hendrycks & Gimpel, 2016) using VGGNet. All val-
ues are percentages and boldface values indicate relative the better results. For each in-distribution,
we minimize the KL divergence term in (1) using training samples from an out-of-distribution
dataset denoted by “seen”, where other “unseen” out-of-distributions were only used for testing.

1Our code is available at https://github.com/alinlab/Confident_classifier.

5

Published as a conference paper at ICLR 2018

(a) Cross entropy loss

(b) Conﬁdence loss in (1)

(c) ROC curve

Figure 2: For all experiments in (a), (b) and (c), we commonly use the SVHN dataset for in-
distribution. Fraction of the maximum prediction value in softmax scores trained by (a) cross en-
tropy loss and (b) conﬁdence loss: the x-axis and y-axis represent the maximum prediction value
and the fraction of images receiving the corresponding score, respectively. The receiver operating
characteristic (ROC) curves under different losses are reported in (c): the red curve corresponds to
the ROC curve of a model trained by optimizing the naive cross entropy loss, whereas other ones
correspond to the ROC curves of models trained by optimizing the conﬁdence loss. The KL diver-
gence term in the conﬁdence loss is optimized using explicit out-of-distribution datasets indicated
in the parentheses, e.g., Conﬁdent loss (LSUN) means that we use the LSUN dataset for optimizing
the KL divergence term.

3.1 EFFECTS OF CONFIDENCE LOSS

We ﬁrst verify the effect of conﬁdence loss in (1) trained by some explicit, say seen, out-of-
distribution datasets. First, we compare the quality of conﬁdence level by applying various training
losses. Speciﬁcally, the softmax classiﬁer is used and simple CNNs (two convolutional layers fol-
lowed by three fully-connected layers) are trained by minimizing the standard cross entropy loss on
SVHN dataset. We also apply the conﬁdence loss to the models by additionally optimizing the KL
divergence term using CIFAR-10 dataset (as training out-of-distribution). In Figure 2(a) and 2(b),
we report distributions of the maximum prediction value in softmax scores to evaluate the separation
quality between in-distribution (i.e., SVHN) and out-of-distributions. It is clear that there exists a
better separation between the SVHN test set (red bar) and other ones when the model is trained by
the conﬁdence loss. Here, we emphasize that the maximum prediction value is also low on even un-
trained (unseen) out-of-distributions, e.g., TinyImageNet, LSUN and synthetic datasets. Therefore,
it is expected that one can distinguish in- and out-of-distributions more easily when a classiﬁer is
trained by optimizing the conﬁdence loss. To verify that, we obtain the ROC curve using the baseline
detector (Hendrycks & Gimpel, 2016) that computes the maximum value of predictive distribution
on a test sample and classiﬁes it as positive (i.e., in-distribution) if the conﬁdence score is above
some threshold. Figure 2(c) shows the ROC curves when we optimize the KL divergence term on
various datasets. One can observe that realistic images such as TinyImageNet (aqua line) and LSUN
(green line) are more useful than synthetic datasets (orange line) for improving the detection perfor-
mance. This supports our intuition that out-of-distribution samples close to in-distribution could be
more effective in improving the detection performance as we discussed in Section 2.1.

We indeed evaluate the performance of the baseline detector for out-of-distribution using large-
scale CNNs, i.e., VGGNets with 13 layers, under various training scenarios, where more results on
AlexNet and ODIN detector (Liang et al., 2017) can be found in Appendix C (the overall trends
of results are similar). For optimizing the conﬁdence loss in (1), SVHN and CIFAR-10 training
datasets are used for optimizing the KL divergence term for the cases when the in-distribution is
CIFAR-10 and SVHN, respectively. Table 1 shows the detection performance for each in- and
out-of-distribution pair. When the in-distribution is SVHN, the classiﬁer trained by our method
drastically improves the detection performance across all out-of-distributions without hurting its
original classiﬁcation performance. However, when the in-distribution is CIFAR-10, the conﬁdence
loss does not improve the detection performance in overall, where we expect that this is because
the trained/seen SVHN out-of-distribution does not effectively cover all tested out-of-distributions.
Our joint conﬁdence loss in (4), which was designed under the intuition, resolves the issue of the
CIFAR-10 (in-distribution) classiﬁcation case in Table 1 (see Figure 4(b)).

6

Published as a conference paper at ICLR 2018

(a)

(b)

(c)

(d)

Figure 3: The generated samples from original GAN (a)/(c) and proposed GAN (b)/(d). In (a)/(b),
the grey area is the 2D histogram of training in-distribution samples drawn from a mixture of two
Gaussian distributions and red points indicate generated samples by GANs.

(a) In-distribution: SVHN

(b) In-distribution: CIFAR-10

Figure 4: Performances of the baseline detector (Hendrycks & Gimpel, 2016) under various training
losses. For training models by the conﬁdence loss, the KL divergence term is optimized using
samples indicated in the parentheses. For fair comparisons, we only plot the performances for
unseen out-of-distributions, where those for seen out-of-distributions (used for minimizing the KL
divergence term in (1)) can be found in Table 1.

3.2 EFFECTS OF ADVERSARIAL GENERATOR AND JOINT CONFIDENCE LOSS

In this section, we verify the effect of the proposed GAN in Section 2.2 and evaluate the detection
performance of the joint conﬁdence loss in (4). To verify that the proposed GAN can produce the
samples nearby the low-density boundary of the in-distribution space, we ﬁrst compare the gener-
ated samples by original GAN and proposed GAN on a simple example where the target distribution
is a mixture of two Gaussian distributions. For both the generator and discriminator, we use fully-
connected neural networks with 2 hidden layers. For our method, we use a pre-trained classiﬁer
which minimizes the cross entropy on target distribution samples and the KL divergence on out-of-
distribution samples generated by rejection sampling on a bounded 2D box. As shown in Figure
3(a), the samples of original GAN cover the high-density area of the target distribution while those
of proposed GAN does its boundary one (see Figure 3(b)). We also compare the generated samples
of original and proposed GANs on MNIST dataset (LeCun et al., 1998), which consists of hand-
written digits. For this experiment, we use deep convolutional GANs (DCGANs) (Radford et al.,
2015). In this case, we use a pre-trained classiﬁer which minimizes the cross entropy on MNIST

7

Published as a conference paper at ICLR 2018

(a) In-distribution: SVHN

(b) In-distribution: CIFAR-10

Figure 5: Guided gradient (sensitivity) maps of the top-1 predicted class with respect to the input
image under various training losses.

training samples and the KL divergence on synthetic Gaussian noises. As shown in Figure 3(c)
and 3(d), samples of original GAN looks more like digits than those of proposed GAN. Somewhat
interestingly, the proposed GAN still generates some new digit-like images.

We indeed evaluate the performance of our joint conﬁdence loss in (4) utilizing the proposed GAN.
To this end, we use VGGNets (as classiﬁers) and DCGANs (as GANs). We also test a variant of con-
ﬁdence loss which optimizes the KL divergence term on samples from a pre-trained original GAN
(implicitly) modeling the in-distribution. One can expect that samples from the original GAN can be
also useful for improving the detection performance since it may have bad generalization properties
(Arora et al., 2017) and generate a few samples on the low-density boundary as like the proposed
GAN. Figure 4 shows the performance of the baseline detector for each in- and out-of-distribution
pair. First, observe that the joint conﬁdence loss (blue bar) outperforms the conﬁdence loss with
some explicit out-of-distribution datasets (green bar). This is quite remarkable since the former
is trained only using in-distribution datasets, while the latter utilizes additional out-of-distribution
datasets. We also remark that our methods signiﬁcantly outperform the baseline cross entropy loss
(red bar) in all cases without harming its original classiﬁcation performances (see Table 2 in Ap-
pendix C). Interestingly, the conﬁdence loss with the original GAN (orange bar) is often (but not
always) useful for improving the detection performance, whereas that with the proposed GAN (blue
bar) still outperforms it in all cases.

Finally, we also provide visual interpretations of models using the guided gradient maps (Sprin-
genberg et al., 2014). Here, the gradient can be interpreted as an importance value of each pixel
which inﬂuences on the classiﬁcation decision. As shown in Figure 5, the model trained by the
cross entropy loss shows sharp gradient maps for both samples from in- and out-of-distributions,
whereas models trained by the conﬁdence losses do only on samples from in-distribution. For the
case of SVHN in-distribution, all conﬁdence losses gave almost zero gradients, which matches to
the results in Figure 4(a): their detection performances are almost perfect. For the case of CIFAR-
10 distribution, one can now observe that there exists some connection between gradient maps and
detection performances. This is intuitive because for detecting samples from out-of-distributions
better, the classiﬁer should look at more pixels as similar importance and the KL divergence term
forces it. We think that our visualization results might give some ideas in future works for developing
better inference methods for detecting out-of-distribution under our models.

4 CONCLUSION

In this paper, we aim to develop a training method for neural classiﬁcation networks for detecting
out-of-distribution better without losing its original classiﬁcation accuracy. In essence, our method
jointly trains two models for detecting and generating out-of-distribution by minimizing their losses
alternatively. Although we primarily focus on image classiﬁcation in our experiments, our method
can be used for any classiﬁcation tasks using deep neural networks. It is also interesting future
directions applying our methods for other related tasks: regression (Malinin et al., 2017), network
calibration (Guo et al., 2017), Bayesian probabilistic models (Li & Gal, 2017; Louizos & Welling,
2017), ensemble (Lakshminarayanan et al., 2017) and semi-supervised learning (Dai et al., 2017).

8

Published as a conference paper at ICLR 2018

ACKNOWLEDGEMENTS

This work was supported in part by the Institute for Information & communications Technology
Promotion(IITP) grant funded by the Korea government(MSIT) (No.2017-0-01778, Development
of Explainable Human-level Deep Machine Learning Inference Framework), the ICT R&D program
of MSIP/IITP [R-20161130-004520, Research on Adaptive Machine Learning Technology Devel-
opment for Intelligent Autonomous Digital Companion], DARPA Explainable AI (XAI) program
#313498 and Sloan Research Fellowship.

REFERENCES

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Con-

crete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). In International Conference on Machine Learning (ICML),
2017.

Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible
In ACM

models for healthcare: Predicting pneumonia risk and hospital 30-day readmission.
SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015.

Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan Salakhutdinov. Good semi-
supervised learning that requires a bad gan. In Advances in neural information processing systems
(NIPS), 2017.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale

hierarchical image database. In Computer Vision and Pattern Recognition (CVPR), 2009.

Ross Girshick. Fast r-cnn. In International Conference on Computer Vision (ICCV), 2015.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems (NIPS), 2014.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural

networks. In International Conference on Machine Learning (ICML), 2017.

Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger,
Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end
speech recognition. arXiv preprint arXiv:1412.5567, 2014.

Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution
examples in neural networks. In International Conference on Learning Representations (ICLR),
2016.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-
nov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning (ICML), 2015.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations (ICLR), 2014.

Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint

arXiv:1404.5997, 2014.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic-
tive uncertainty estimation using deep ensembles. In Advances in neural information processing
systems (NIPS), 2017.

9

Published as a conference paper at ICLR 2018

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Kimin Lee, Changho Hwang, KyoungSoo Park, and Jinwoo Shin. Conﬁdent multiple choice learn-

ing. In International Conference on Machine Learning (ICML), 2017.

Yingzhen Li and Yarin Gal. Dropout inference in bayesian neural networks with alpha-divergences.

In International Conference on Machine Learning (ICML), 2017.

Shiyu Liang, Yixuan Li, and R Srikant. Principled detection of out-of-distribution examples in

neural networks. arXiv preprint arXiv:1706.02690, 2017.

Christos Louizos and Max Welling. Multiplicative normalizing ﬂows for variational bayesian neural

networks. In International Conference on Machine Learning (ICML), 2017.

Andrey Malinin, Anton Ragni, Kate Knill, and Mark Gales. Incorporating uncertainty into deep
In Proceedings of the 55th Annual Meeting of the
learning for spoken language assessment.
Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pp. 45–50, 2017.

Mahdi Pakdaman Naeini, Gregory F Cooper, and Milos Hauskrecht. Obtaining well calibrated

probabilities using bayesian binning. In AAAI, 2015.

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, volume 2011, pp. 5, 2011.

Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing conﬁdent output distributions. arXiv preprint arXiv:1701.06548,
2017.

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In International Conference on Learning Repre-
sentations (ICLR), 2015.

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
In Advances in neural information processing systems

Improved techniques for training gans.
(NIPS), 2016.

Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the
In International

pixelcnn with discretized logistic mixture likelihood and other modiﬁcations.
Conference on Learning Representations (ICLR), 2017.

Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russ Webb.
In Computer

Learning from simulated and unsupervised images through adversarial training.
Vision and Pattern Recognition (CVPR), 2017.

Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for

simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Computer Vision and Pattern Recognition (CVPR), 2015.

Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee. Learning
to generate long-term future via hierarchical prediction. In International Conference on Machine
Learning (ICML), 2017.

Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.

Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. In

International Conference on Learning Representations (ICLR), 2017.

10

Published as a conference paper at ICLR 2018

A THRESHOLD-BASED DETECTORS

In this section, we formally describe the detection procedure of threshold-based detectors
(Hendrycks & Gimpel, 2016; Liang et al., 2017). For each data x, it measures some conﬁdence
score q(x) by feeding the data into a pre-trained classiﬁer. Here, (Hendrycks & Gimpel, 2016) de-
ﬁned the conﬁdence score as a maximum value of the predictive distribution, and (Liang et al., 2017)
further improved the performance by processing the predictive distribution (see Appendix C.3 for
more details). Then, the detector, g (x) : X → {0, 1}, assigns label 1 if the conﬁdence score q(x) is
above some threshold δ, and label 0, otherwise:

(cid:26)

g (x) =

1
0

if q(x) ≥ δ,
otherwise.

For this detector, we have to ﬁnd a score threshold so that some positive examples are classiﬁed
correctly, but this depends upon the trade-off between false negatives and false positives. To handle
this issue, we use threshold-independent evaluation metrics such as area under the receiver operating
characteristic curve (AUORC) and detection accuracy (see Appendix B).

B EXPERIMENTAL SETUPS IN SECTION 3

Datasets. We train deep models such as VGGNet (Szegedy et al., 2015) and AlexNet (Krizhevsky,
2014) for classifying CIFAR-10 and SVHN datasets: the former consists of 50,000 training and
10,000 test images with 10 image classes, and the latter consists of 73,257 training and 26,032 test
images with 10 digits.2 The corresponding test dataset are used as the in-distribution (positive)
samples to measure the performance. We use realistic images and synthetic noises as the out-of-
distribution (negative) samples: the TinyImageNet consists of 10,000 test images with 200 image
classes from a subset of ImageNet images. The LSUN consists of 10,000 test images of 10 different
scenes. We downsample each image of TinyImageNet and LSUN to size 32 × 32. The Gaussian
noise is independently and identically sampled from a Gaussian distribution with mean 0.5 and
variance 1. We clip each pixel value into the range [0, 1].

Detailed CNN structure and training. The simple CNN that we use for evaluation shown in Figure
2 consists of two convolutional layers followed by three fully-connected layers. Convolutional layers
have 128 and 256 ﬁlters, respectively. Each convolutional layer has a 5 × 5 receptive ﬁeld applied
with a stride of 1 pixel each followed by max pooling layer which pools 2 × 2 regions at strides of
2 pixels. AlexNet (Krizhevsky, 2014) consists of ﬁve convolutitonal layers followed by three fully-
connected layers. Convolutional layers have 64, 192, 384, 256 and 256 ﬁlters, respectively. First and
second convolutional layers have a 5×5 receptive ﬁeld applied with a stride of 1 pixel each followed
by max pooling layer which pools 3 × 3 regions at strides of 2 pixels. Other convolutional layers
have a 3 × 3 receptive ﬁeld applied with a stride of 1 pixel followed by max pooling layer which
pools 2 × 2 regions at strides of 2 pixels. Fully-connected layers have 2048, 1024 and 10 hidden
units, respectively. Dropout (Hinton et al., 2012) was applied to only fully-connected layers of the
network with the probability of retaining the unit being 0.5. All hidden units are ReLUs. Figure
6 shows the detailed structure of VGGNet (Szegedy et al., 2015) with three fully-connected layers
and 10 convolutional layers. Each ConvReLU box in the ﬁgure indicates a 3 × 3 convolutional layer
followed by ReLU activation. Also, all max pooling layers have 2 × 2 receptive ﬁelds with stride 2.
Dropout was applied to only fully-connected layers of the network with the probability of retaining
the unit being 0.5. For all experiments, the softmax classiﬁer is used, and each model is trained by
optimizing the objective function using Adam learning rule (Kingma & Ba, 2014). For each out-of-
distribution dataset, we randomly select 1,000 images for tuning the penalty parameter β, mini-batch
size and learning rate. The penalty parameter is chosen from β ∈ {0, 0.1, . . . 1.9, 2}, the mini-batch
size is chosen from {64, 128} and the learning rate is chosen from {0.001, 0.0005, 0.0002}. The
optimal parameters are chosen to minimize the detection error on the validation set. We drop the
learning rate by 0.1 at 60 epoch and models are trained for total 100 epochs. The best test result is
reported for each method.

Performance metrics. We measure the following metrics using threshold-based detectors:

2We do not use the extra SVHN dataset for training.

11

Published as a conference paper at ICLR 2018

• True negative rate (TNR) at 95% true positive rate (TPR). Let TP, TN, FP, and FN de-
note true positive, true negative, false positive and false negative, respectively. We measure
TNR = TN / (FP+TN), when TPR = TP / (TP+FN) is 95%.

• Area under the receiver operating characteristic curve (AUROC). The ROC curve is a
graph plotting TPR against the false positive rate = FP / (FP+TN) by varying a threshold.
• Area under the precision-recall curve (AUPR). The PR curve is a graph plotting the
precision = TP / (TP+FP) against recall = TP / (TP+FN) by varying a threshold. AUPR-IN
(or -OUT) is AUPR where in- (or out-of-) distribution samples are speciﬁed as positive.
• Detection accuracy. This metric corresponds to the maximum classiﬁcation proba-
(cid:8)Pin (q (x) ≤ δ) P (x is from Pin) +
bility over all possible thresholds δ: 1 − minδ
Pout (q (x) > δ) P (x is from Pout) (cid:9), where q(x) is a conﬁdent score such as a maxi-
mum value of softmax. We assume that both positive and negative examples have equal
probability of appearing in the test set, i.e., P (x is from Pin) = P (x is from Pout) = 0.5.

Note that AUROC, AUPR and detection accuracy are threshold-independent evaluation metrics.

Figure 6: Detailed structure of VGGNet with 13 layers.

Generating samples on a simple example. As shown in Figure 3(a) and Figure 3(b), we compare
the generated samples by original GAN and proposed GAN on a simple example where the target
distribution is a mixture of two Gaussian distributions. For both the generator and discriminator, we
use fully-connected neural networks with 2 hidden layers and 500 hidden units for each layer. For
all layers, we use ReLU activation function. We use a 100-dimensional Gaussian prior for the latent
variable z. For our method, we pre-train the simple fully-connected neural networks (2 hidden layers
and 500 ReLU units for each layer) by minimizing the cross entropy on target distribution samples
and the KL divergence on out-of-distribution samples generated by rejection sampling on bounded
2D box [−10, 10]2. The penalty parameter β is set to 1. We use ADAM learning rule (Kingma &
Ba, 2014) with a mini-batch size of 400. The initial learning rate is set to 0.002, and we train for
total 100 epochs.

Generating samples on MNIST. As shown in Figure 3(c) and Figure 3(d), we compare the gener-
ated samples of original and proposed GANs on MNIST dataset, which consists of greyscale images,
each containing a digit 0 to 9 with 60,000 training and 10,000 test images. We expand each image
to size 3 × 32 × 32. For both the generator and discriminator, we use deep convolutional GANs
(DCGANs) (Radford et al., 2015). The discriminator and generator consist of four convolutional
and deconvolutional layers, respectively. Convolutional layers have 128, 256, 512 and 1 ﬁlters, re-
spectively. Each convolutional layer has a 4 × 4 receptive ﬁeld applied with a stride of 2 pixel. The
second and third convolutional layers are followed by batch normalization (Ioffe & Szegedy, 2015).
For all layers, we use LeakyReLU activation function. Deconvolutional layers have 512, 256, 128
and 1 ﬁlters, respectively. Each deconvolutional layer has a 4 × 4 receptive ﬁeld applied with a
stride of 2 pixel followed by batch normalization (Ioffe & Szegedy, 2015) and ReLU activation.
For our method, we use a pre-trained simple CNNs (two convolutional layers followed by three
fully-connected layers) by minimizing the cross entropy on MNIST training samples and the KL
divergence on synthetic Gaussian noise. Convolutional layers have 128 and 256 ﬁlters, respectively.
Each convolutional layer has a 5 × 5 receptive ﬁeld applied with a stride of 1 pixel each followed by
max pooling layer which pools 2 × 2 regions at strides of 2 pixels. The penalty parameter β is set
to 1. We use ADAM learning rule (Kingma & Ba, 2014) with a mini-batch size of 128. The initial
learning rate is set to 0.0002, and we train for total 50 epochs.

12

Published as a conference paper at ICLR 2018

C MORE EXPERIMENTAL RESULTS

C.1 CLASSIFICATION PERFORMANCES

Table 2 reports the classiﬁcation accuracy of VGGNets on CIFAR-10 and SVHN datasets under
various training losses shown in Figure 4. One can note that all methods do not degrade the original
classiﬁcation performance, where the differences in classiﬁcation errors of across all tested single
models are at most 1% in our experiments.

In-distribution Cross entropy

Joint conﬁdence loss

Conﬁdence loss
with original GAN

Conﬁdence loss with explicit
out-of-distribution samples

CIFAR-10
SVHN

80.14
93.82

80.27
94.08

81.39
93.81

80.56
94.23

Table 2: Classiﬁcation test set accuracy of VGGNets on CIFAR-10 and SVHN datasets under vari-
ous training losses.

C.2 CALIBRATION EFFECTS OF CONFIDENCE LOSS

We also verify the calibration effects (Guo et al., 2017) of our methods: whether a classiﬁer trained
by our method can indicate when they are likely to be incorrect for test samples from the in-
distribution. In order to evaluate the calibration effects, we measure the expected calibration error
(ECE) (Naeini et al., 2015). Given test data {(x1, y1) , . . . , (xn, yn)}, we group the predictions into
M interval bins (each of size 1/M ). Let Bm be the set of indices of samples whose prediction
conﬁdence falls into the interval ( m−1

M , m
M ]. Then, the accuracy of Bm is deﬁned as follows:
1
|Bm|

1{yi=arg max

Pθ(y|xi)},

(cid:88)

y

i∈Bm

acc(Bm) =

where θ is the model parameters of a classiﬁer and 1A ∈ {0, 1} is the indicator function for event
A. We also deﬁne the conﬁdence of Bm as follows:
1
|Bm|

conf(Bm) =

q(xi),

(cid:88)

i∈Bm

where q(xi) is the conﬁdence of data i. Using these notations, we measure the expected calibration
error (ECE) as follows:

ECE =

|acc(Bm) − conf(Bm)|.

M
(cid:88)

m=1

|Bm|
n

One can note that ECE is zero if the conﬁdence score can represent the true distribution. Table
3 shows the calibration effects of conﬁdence loss when we deﬁne the conﬁdence score q as the
maximum predictive distribution of a classiﬁer. We found that the ECE of a classiﬁer trained by our
methods is lower than that of a classiﬁer trained by the standard cross entropy loss. This implies
that our proposed method is effective at calibrating predictions. We also remark that the temperature
scaling (Guo et al., 2017) provides further improvements under a classiﬁer trained by our joint
conﬁdence loss.

In-distribution

Without temperature scaling

With temperature scaling

Cross entropy loss

Joint conﬁdence loss Cross entropy loss

Joint conﬁdence loss

CIFAR-10

SVHN

18.45

5.30

14.62

5.13

7.07

2.80

6.19

1.39

Table 3: Expected calibration error (ECE) of VGGNets on CIFAR-10 and SVHN datasets under
various training losses. The number of bins M is set to 20. All values are percentages and boldface
values indicate relative the better results.

13

Published as a conference paper at ICLR 2018

C.3 EXPERIMENTAL RESULTS USING ODIN DETECTOR

In this section, we verify the effects of conﬁdence loss using ODIN detector (Liang et al., 2017)
which is an advanced threshold-based detector using temperature scaling (Guo et al., 2017) and
input perturbation. The key idea of ODIN is the temperature scaling which is deﬁned as follows:

Pθ(y = (cid:98)y|x; T ) =

(cid:80)

(cid:98)y(x)/T )

exp (f
y exp (fy(x)/T )

,

where T > 0 is the temperature scaling parameter and f = (f1, . . . , fK) is ﬁnal feature vector of
neural networks. For each data x, ODIN ﬁrst calculates the pre-processed image (cid:98)x by adding the
small perturbations as follows:

x(cid:48) = x − εsign (− (cid:53)x log Pθ(y = (cid:98)y|x; T )) ,
where ε is a magnitude of noise and (cid:98)y is the predicted label. Next, ODIN feeds the pre-
processed data into the classiﬁer, computes the maximum value of scaled predictive distribution,
i.e., maxy Pθ(y|x(cid:48); T ), and classiﬁes it as positive (i.e., in-distribution) if the conﬁdence score is
above some threshold δ.

For ODIN detector, the perturbation noise ε is chosen from {0, 0.0001, 0.001, 0.01}, and the tem-
perature T is chosen from {1, 10, 100, 500, 1000}. The optimal parameters are chosen to minimize
the detection error on the validation set. Figure 7 shows the performance of the OIDN and baseline
detector for each in- and out-of-distribution pair. First, we remark that the baseline detector using
classiﬁers trained by our joint conﬁdence loss (blue bar) typically outperforms the ODIN detec-
tor using classiﬁers trained by the cross entropy loss (orange bar). This means that our classiﬁer
can map in- and out-of-distributions more separately without pre-processing methods such as tem-
perature scaling. The ODIN detector provides further improvements if one uses it with our joint
conﬁdence loss (green bar). In other words, our proposed training method can improve all prior
detection methods.

(a) In-distribution: SVHN

Figure 7: Performances of the baseline detector (Hendrycks & Gimpel, 2016) and ODIN detector
(Liang et al., 2017) under various training losses.

(b) In-distribution: CIFAR-10

C.4 EXPERIMENTAL RESULTS ON ALEXNET

Table 4 shows the detection performance for each in- and out-of-distribution pair when the classiﬁer
is AlexNet (Krizhevsky, 2014), which is one of popular CNN architectures. We remark that they
show similar trends.

14

Published as a conference paper at ICLR 2018

In-dist

Out-of-dist

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

Baseline
(SVHN)

Baseline
(CIFAR-10)

ODIN
(SVHN)

ODIN
(CIFAR-10)

CIFAR-10 (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

CIFAR-10 (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

92.14 / 93.77

76.58 / 76.18

92.14 / 93.77

76.58 / 76.18

Cross entropy loss / Conﬁdence loss

42.0 / 99.9
45.6 / 99.9
44.6 / 100.0
58.6 / 100.0

12.8 / 99.6
10.3 / 10.1
10.7 / 8.1
6.7 / 1.0

55.5 / 99.9
59.5 / 99.9
61.5 / 100.0
82.6 / 100.0

37.1 / 99.6
11.4 / 8.4
13.3 / 7.1
3.8 / 0.0

88.0 / 100.0
89.4 / 100.0
89.8 / 100.0
94.2 / 100.0

71.0 / 99.9
59.2 / 52.1
56.3 / 51.5
49.6 / 13.5

89.1 / 99.2
90.5 / 99.3
91.8 / 99.3
97.0 / 99.3

86.7 / 99.6
69.1 / 65.6
71.9 / 67.1
70.9 / 57.2

83.4 / 99.8
84.3 / 99.9
84.5 / 99.9
88.8 / 100.0

73.2 / 99.6
64.2 / 62.0
64.3 / 61.8
61.3 / 50.0

82.4 / 99.8
83.8 / 99.9
84.8 / 99.9
91.6 / 100.0

79.3 / 99.6
64.4 / 61.8
75.3 / 63.7
69.3 / 40.4

88.7 / 99.9
90.2 / 100.0
90.8 / 100.0
95.5 / 100.0

74.3 / 99.9
63.6 / 59.8
62.3 / 59.5
58.5 / 43.7

85.9 / 100.0
87.5 / 100.0
90.5 / 100.0
97.4 / 100.0

88.1 / 99.9
71.4 / 68.6
67.2 / 72.0
78.1 / 56.1

87.3 / 99.3
88.6 / 99.3
88.4 / 99.3
92.5 / 99.3

70.7 / 99.6
64.4 / 62.3
65.3 / 61.6
59.5 / 32.0

89.0 / 99.2
90.4 / 99.3
91.3 / 99.3
96.4 / 99.3

84.2 / 99.6
64.6 / 60.7
65.3 / 60.5
60.7 / 40.7

Table 4: Performance of the baseline detector (Hendrycks & Gimpel, 2016) and ODIN detector
(Liang et al., 2017) using AlexNet. All values are percentages and boldface values indicate rela-
tive the better results. For each in-distribution, we minimize the KL divergence term in (1) using
training samples from an out-of-distribution dataset denoted by “seen”, where other “unseen” out-
of-distributions were also used for testing.

D MAXIMIZING ENTROPY

One might expect that the entropy of out-of-distribution is expected to be much higher compared to
that of in-distribution since the out-of-distribution is typically on a much larger space than the in-
distribution. Therefore, one can add maximizing the entropy of generator distribution to new GAN
loss in (3) and joint conﬁdence loss in (4). However, maximizing the entropy of generator distribu-
tion is technically challenging since a GAN does not model the generator distribution explicitly. To
handle the issue, one can leverage the pull-away term (PT) (Zhao et al., 2017):

−H (PG (x)) (cid:119) PT (PG (x)) =

1
M (M − 1)

M
(cid:88)

(cid:88)

(cid:32)

i=1

j(cid:54)=i

G (zi)(cid:62) G (zj)
(cid:107)G (zi)(cid:107)(cid:107)G (zj)(cid:107)

(cid:33)2

,

where H (·) denotes the entropy, zi, zj ∼ Ppri (z) and M is the number of samples. Intuitively,
one can expect the effect of increasing the entropy by minimizing PT since it corresponds to the
squared cosine similarity of generated samples. We note that (Dai et al., 2017) also used PT to
maximize the entropy. Similarly as in Section 3.2, we verify the effects of PT using VGGNet. Table
5 shows the performance of the baseline detector for each in- and out-of-distribution pair. We found
that joint conﬁdence loss with PT tends to (but not always) improve the detection performance.
However, since PT increases the training complexity and the gains from PT are relatively marginal
(or controversial), we leave it as an auxiliary option for improving the performance.

In-dist

Out-of-dist

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

SVHN

93.81 / 94.05

CIFAR-10

81.39 / 80.60

CIFAR-10
TinyImageNet
LSUN

SVHN
TinyImageNet
LSUN

Joint conﬁdence loss without PT / with PT

90.1 / 92.3
99.0 / 99.9
98.9 / 100.0

97.6 / 98.1
99.6 / 100.0
99.6 / 100.0

25.4 / 13.2
35.0 / 44.8
39.1 / 49.1

66.8 / 69.5
72.0 / 78.4
75.1 / 80.7

93.6 / 94.6
97.6 / 99.7
97.5 / 99.9

74.2 / 75.1
76.4 / 77.6
77.8 / 78.7

97.7 / 98.2
99.7 / 100.0
99.7 / 100.0

97.9 / 98.7
94.5 / 100.0
95.5 / 100.0

71.3 / 73.5
74.7 / 79.4
77.1 / 81.3

78.3 / 72.0
82.2 / 84.4
83.6 / 85.8

Table 5: Performance of the baseline detector (Hendrycks & Gimpel, 2016) using VGGNets trained
by joint conﬁdence loss with and without pull-away term (PT). All values are percentages and bold-
face values indicate relative the better results.

15

Published as a conference paper at ICLR 2018

E ADDING OUT-OF-DISTRIBUTION CLASS

Instead of forcing the predictive distribution on out-of-distribution samples to be closer to the uni-
form one, one can simply add an additional “out-of-distribution” class to a classiﬁer as follows:

min
θ

EPin((cid:98)x,(cid:98)y)

(cid:2) − log Pθ (y = (cid:98)y|(cid:98)x) (cid:3) + EPout(x)

(cid:2) − log Pθ (y = K + 1|x) (cid:3),

(5)

where θ is a model parameter. Similarly as in Section 3.1, we compare the performance of the
conﬁdence loss with that of above loss in (5) using VGGNets with for image classiﬁcation on SVHN
dataset. To optimize the KL divergence term in conﬁdence loss and the second term of (5), CIFAR-
10 training datasets are used. In order to compare the detection performance, we deﬁne the the
conﬁdence score of input x as 1 − Pθ (y = K + 1|x) in case of (5). Table 6 shows the detection
performance for out-of-distribution. First, the classiﬁer trained by our method often signiﬁcantly
outperforms the alternative adding the new class label. This is because modeling explicitly out-of-
distribution can incur overﬁtting to trained out-of-distribution dataset.

Detector

Out-of-dist

Baseline
detector

ODIN
detector

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

SVHN (seen)
TinyImageNet (unseen)
LSUN (unseen)
Gaussian (unseen)

Classiﬁcation
accuracy

TNR
at TPR 95%

AUROC

Detection
accuracy

AUPR
in

AUPR
out

K + 1 class loss in (5) / Conﬁdence loss

79.61 / 80.56

79.61 / 80.56

99.6 / 99.8
0.0 / 9.9
0.0 / 10.5
0.0 / 3.3

99.6 / 99.8
0.3 / 12.2
0.1 / 13.7
0.0 / 8.2

99.8 / 99.9
5.2 / 31.8
5.6 / 34.8
0.1 / 14.1

99.9 / 99.8
47.3 / 70.6
48.3 / 73.1
28.3 / 68.3

99.7 / 99.8
51.3 / 58.6
51.5 / 60.2
50.0 / 50.0

99.1 / 99.8
55.12 / 65.7
55.9 / 67.9
54.4 / 65.4

99.8 / 99.9
50.7 / 55.3
50.8 / 56.4
49.3 / 49.4

99.9 / 99.9
56.6 / 72.7
57.5 / 75.2
47.8 / 74.1

99.9 / 99.8
64.3 / 66.1
71.5 / 68.0
12.2 / 47.0

99.9 / 99.8
44.3 / 65.6
44.7 / 67.8
36.8 / 61.5

Table 6: Performance of the baseline detector (Hendrycks & Gimpel, 2016) and ODIN detector
(Liang et al., 2017) using VGGNet. All values are percentages and boldface values indicate relative
the better results.

16

