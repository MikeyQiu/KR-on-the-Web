Online Learning to Rank with Features

Shuai Li 1 Tor Lattimore 2 Csaba Szepesvári 2

9
1
0
2
 
y
a
M
 
5
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
7
6
5
2
0
.
0
1
8
1
:
v
i
X
r
a

Abstract
We introduce a new model for online ranking in
which the click probability factors into an exami-
nation and attractiveness function and the attrac-
tiveness function is a linear function of a feature
vector and an unknown parameter. Only relatively
mild assumptions are made on the examination
function. A novel algorithm for this setup is anal-
ysed, showing that the dependence on the number
of items is replaced by a dependence on the dimen-
sion, allowing the new algorithm to handle a large
number of items. When reduced to the orthogonal
case, the regret of the algorithm improves on the
state-of-the-art.

1. Introduction

Let L be a large set of items to be ranked. For example, a
database of movies, news articles or search results. We con-
sider a sequential version of the ranking problem where in
each round the learner chooses an ordered list of K distinct
items from L to show the user. We assume the feedback
comes in the form of clicks and the learner’s objective is
to maximise the expected number of clicks over T rounds.
Our focus is on the case where L is large (perhaps millions)
and K is relatively small (ﬁfty or so). There are two main
challenges that arise in online ranking problems:

(a) The number of rankings grows exponentially in K,
which makes learning one parameter for each ranking a
fruitless endeavour. Click models may be used to reduce the
dimensionality of the learning problem, but balancing gen-
erality of the model with learnability is a serious challenge.
The majority of previous works on online learning to rank
have used unstructured models, which are not well suited to
our setting where L is large.

(b) Most click models depend on an unknown attractiveness
function that endows the item set with an order. This yields

1The Chinese University of Hong Kong 2DeepMind. Correspon-
dence to: Shuai Li <shuaili@cse.cuhk.edu.hk>, Tor Lattimore <lat-
timore@google.com>, Csaba Szepesvári <szepi@google.com>.

Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).

a model with at least |L| parameters, which is prohibitively
large in the applications we have in mind.

The ﬁrst challenge is tackled by adapting the ﬂexible click
models introduced in (Zoghi et al., 2017; Lattimore et al.,
2018) to our setting. For the second we follow previous
works on bandits with large action sets by assuming the
attractiveness function can be written as a linear function of
a relatively small number of features.

Contribution We make several contributions:

• A new model for ranking problems with features is pro-
posed that generalises previous work (Li et al., 2016;
Zong et al., 2016; Liu et al., 2018) by relaxing the rela-
tively restrictive assumptions on the probability that a
user clicks on an item. The new model is strictly more
robust than previous works focusing on regret analysis
for large item sets.

• We introduce a novel polynomial-time algorithm called
RecurRank. The algorithm operates recursively over
an increasingly ﬁne set of partitions of [K]. Within
each partition the algorithm balances exploration and
exploitation, subdividing the partition once it becomes
sufﬁciently certain about the suboptimality of a subset
of items.

• A regret analysis shows that the cumulative regret of
RecurRank is at most RT = O(K(cid:112)dT log(LT ),
where K is the number of positions, L is the number of
items and d is the dimension of the feature space. Even
in the non-feature case where L = d this improves on
the state-of-the-art by a factor of

K.

√

A comparison with most related work is shown in Table 1.

Related work Online learning to rank has seen an explo-
sion of research in the last decade and there are multiple
ways of measuring the performance of an algorithm. One
view is that the clicks themselves should be maximised,
which we take in this article. An alternative is to assume an
underlying relevance of all items in a ranking that is never
directly observed, but can be inferred in some way from
the observed clicks. In all generality this latter setting falls
into the partial monitoring framework (Rustichini, 1999),

Online Learning to Rank with Features

Context

Click Model

Cascade Model (CM)

(Generalised) Linear Form CM

Table 1. This table compares settings and regret bounds of most related works on online learning to rank. T is the number of total rounds,
K is the number of positions, L is the number of items and d is the feature space dimension. ∆ is the minimal gap between the expected
click rate of the best items and the expected click rate of the suboptimal items.

Kveton et al. (2015)

Li et al. (2016)
Zong et al. (2016)
Li & Zhang (2018)

Katariya et al. (2016)

Lagree et al. (2016)

Zoghi et al. (2017)

Lattimore et al. (2018)

-

-

-

-

-

Liu et al. (2018)

Generalised Linear Form

DCM

Dependent Click Model (DCM)

log(T )

Position-Based Model (PBM) with
known position bias

log(T )

General Click Model

General Click Model

Regret
(cid:18) L
∆
√

Θ

(cid:16)

O

d

(cid:19)

log(T )

T K log(T )

(cid:17)

Θ

O

O

O

O

O

Ω

(cid:19)

(cid:19)

√

T K log(T )

(cid:17)

(cid:18) L
∆

(cid:16)

dK
(cid:18) L
∆

log(T )

(cid:19)

(cid:19)

(cid:18) K 3L
∆
(cid:18) KL
∆

log(T )
(cid:16)(cid:112)K 3LT log(T )
(cid:16)√
(cid:17)

(cid:17)

KLT

(cid:17)
(cid:16)
K(cid:112)dT log(LT )

O

Ours

Linear Form

General Click Model

but has been studied in speciﬁc ranking settings (Chaudhuri,
2016, and references therein). See the article by Hofmann
et al. (2011) for more discussion on various objectives.

Maximising clicks directly is a more straightforward objec-
tive because clicks are an observed quantity. Early work was
empirically focused. For example, Li et al. (2010) propose a
modiﬁcation of LinUCB for contextual ranking and Chen &
Hofmann (2015) modify the optimistic algorithms for linear
bandits. These algorithms do not come with theoretical guar-
antees, however. There has recently been signiﬁcant effort
towards designing theoretically justiﬁed algorithms in set-
tings of increasing complexity (Kveton et al., 2015; Combes
et al., 2015; Zong et al., 2016; Katariya et al., 2016; Lagree
et al., 2016). These works assume the user’s clicks follow a
click model that connects properties of the shown ranking to
the probability that a user clicks on an item placed in a given
position. For example, in the document-based model it is
assumed that the probability that the user clicks on a shown
item only depends on the unknown attractiveness of that
item and not its position in the ranking or the other items.
Other simple models include the position-based, cascade
and dependent click models. For a survey of click models
see (Chuklin et al., 2015).

As usual, however, algorithms designed for speciﬁc mod-
els are brittle when the modelling assumptions are not met.
Recent work has started to relax the strong assumptions

by making the observation that in all of the above click
models the probability of a user clicking on an item can be
written as the product of the item’s inherent attractiveness
and the probability that the user examines its position in
the list. Zoghi et al. (2017) use a click model where this
decomposition is kept, but the assumption on how the ex-
amination probability of a position depends on the list is
signiﬁcantly relaxed. This is relaxed still further by Latti-
more et al. (2018) who avoid the factorisation assumption
by making assumptions directly on the click probabilities,
but the existence of an attractiveness function remains.

The models mentioned in the last paragraph do not make
assumptions on the attractiveness function, which means the
regret depends badly on the size of L. Certain simple click
models have assumed the attractiveness function is a linear
function of an item’s features and the resulting algorithms
are suitable for large action sets. This has been done for
the cascade model (Li et al., 2016) and the dependent-click
model (Liu et al., 2018). While these works are welcomed,
the strong assumptions leave a lingering doubt that perhaps
the models may not be a good ﬁt for practical problems.
Of course, our work is closely related to stochastic linear
bandits, ﬁrst studied by Abe & Long (1999) and reﬁned
by Auer (2002); Abbasi-Yadkori et al. (2011); Valko et al.
(2014) and many others.

Ranking has also been examined in an adversarial frame-

Online Learning to Rank with Features

work by Radlinski et al. (2008). These settings are most
similar to the stochastic position-based and document-based
models, but with the additional robustness bought by the
adversarial framework. Another related setup is the rank-1
bandit problem in which the learner should choose just one
of L items to place in one of K positions. For example, the
location of a billboard with the budget to place only one.
These setups have a lot in common with the present one, but
cannot be directly applied to ranking problems. For more
details see (Katariya et al., 2017a;b).

Finally, we note that some authors do not assume an ordering
of the item set provided by an attractiveness function. The
reader is referred to the work by Slivkins et al. (2013) (which
is a follow-up work to Radlinski et al. (2008)) where the
learner’s objective is to maximise the probability that a user
clicks on any item, rather than rewarding multiple clicks.
This model encourages diversity and provides an interesting
alternative approach.

2. Preliminaries

Notation Let [n] = {1, 2, . . . , n} denote the ﬁrst n nat-
ural numbers. Given a set X the indicator function is 1X .
For vector x ∈ Rd and positive deﬁnite matrix V ∈ Rd×d
we let (cid:107)x(cid:107)2
V = x(cid:62)V x. The Moore-Penrose pseudoinverse
of a matrix V is V †.

Problem setup Let L ⊂ Rd be a ﬁnite set of items, L =
|L| and K > 0 a natural number, denoting the number of
positions. A ranking is an injective function from [K], the
set of positions, to L and the set of all rankings is denoted
by Σ. We use uppercase letters like A to denote rankings
in Σ and lowercase letters a, b to denote items in L. The
game proceeds over T rounds. In each round t ∈ [T ] the
learner chooses a ranking At ∈ Σ and subsequently receives
feedback in the form of a vector Ct ∈ {0, 1}K where Ctk =
1 if the user clicked on the kth position. We assume that the
conditional distribution of Ct only depends on At, which
means there exists an unknown function v : Σ × [K] →
[0, 1] such that for all A ∈ Σ and k ∈ [K],

P (Ctk = 1 | At = A) = v(A, k) .

(1)

Remark 1. We do not assume conditional independence of
(Ctk)K

k=1.

We assume that attractiveness is linear in the action, which
means there exists an unknown θ∗ ∈ Rd such that

α(a) = (cid:104)a, θ∗(cid:105)

for all a ∈ L .

(3)

Let a∗
k be the k-th best item sorted in order of decreasing
attractiveness. Then let A∗ = (a∗
K). In case of ties
the choice of A∗ may not be unique. All of the results that
follow hold for any choice.

1, . . . , a∗

The examination function satisﬁes three additional assump-
tions. The ﬁrst says the examination probability of position
k only depends on the identity of the ﬁrst k − 1 items and
not their order:
Assumption 1. χ(A, k) = χ(A(cid:48), k) for any A, A(cid:48) ∈ Σ
with A([k − 1]) = A(cid:48)([k − 1]).

The second assumption is that the examination probability
on any ranking is monotone decreasing in k:

Assumption 2. χ(A, k + 1) ≤ χ(A, k) for all A ∈ Σ and
k ∈ [K − 1].

The third assumption is that the examination probability on
ranking A∗ is minimal:
Assumption 3. χ(A, k) ≥ χ(A∗, k) =: χ∗
and k ∈ [K].

k for all A ∈ Σ

All of these assumptions are satisﬁed by many standard click
models, including the document-based, position-based and
cascade models. These assumptions are strictly weaker than
those made by Zoghi et al. (2017) and orthogonal to those
by Lattimore et al. (2018) as we discuss it in Section 6.

The learning objective We measure the performance of
our algorithm in terms of the cumulative regret, which is

RT = T

v(A∗, k) − E

K
(cid:88)

k=1

(cid:34) T

(cid:88)

K
(cid:88)

t=1

k=1

(cid:35)

v(At, k)

.

Remark 2. The regret is deﬁned relative to A∗, but our
assumptions do not imply that

A∗ ∈ arg max

v(A, k) .

(4)

K
(cid:88)

k=1

A∈Σ

In all generality the function v has K|Σ| parameters, which
is usually impractically large to learn in any reasonable time-
frame. A click model corresponds to making assumptions
on v that reduces the statistical complexity of the learning
problem. We assume a factored model:

The assumptions in all prior work in Table 1 either directly
or indirectly ensure that (4) holds. Our regret analysis does
not rely on this, so we do not assume it. Note, however, that
the deﬁnition of regret is most meaningful when (Eq. (4))
approximately holds.

v(A, k) = χ(A, k)α(A(k)) ,

(2)

where χ : Σ × [K] → [0, 1] is called the examination
probability and α : L → [0, 1] is the attractiveness function.

Experimental design Our algorithm makes use of an ex-
ploration ‘spanner’ that approximately minimises the co-
variance of the least-squares estimator. Given an arbitrary

Online Learning to Rank with Features

ﬁnite set of vectors X = {x1, . . . , xn} ⊂ Rd and distri-
bution π : X → [0, 1] let Q(π) = (cid:80)
x∈X π(x)xx(cid:62). By
the Kiefer–Wolfowitz theorem (Kiefer & Wolfowitz, 1960)
there exists a π called the G-optimal design such that

Algorithm 1 RecurRank

1: Input: Phase number (cid:96) and

Fig. 1. The code is provided in the supplementary material.

max
x∈X

(cid:107)x(cid:107)2

Q(π)† ≤ d .

A = (a1, a2, . . .) and K = (k, k + 1, . . . , k + m − 1)

(5)

2: Find a G-optimal design π = GOPT(A)
3: Let ∆(cid:96) = 2−(cid:96) and

As explained in Chap. 21 of (Lattimore & Szepesvári, 2018),
π may be chosen so that |{x : π(x) > 0}| ≤ d(d + 1)/2. A
G-optimal design π for X has the property that if each
element x ∈ X is observed nπ(x) times for some n
large enough, the value estimate obtained via least-squares
will have its maximum uncertainty over the items min-
imised. Given a ﬁnite (multi-)set of vectors X ⊂ Rd
we let GOPT(X) denote a G-optimal design distribution.
Methods from experimental design have been used for pure
exploration in linear bandits (Soare et al., 2014; Xu et al.,
2017) and also ﬁnite-armed linear bandits (Lattimore &
Szepesvári, 2018, Chap. 22) as well as adversarial linear
bandits (Bubeck et al., 2012).

3. Algorithm

The new algorithm is called RecurRank (‘recursive
ranker’). The algorithm maintains a partition of the K
positions into intervals. Associated with each interval is an
integer-valued ‘phase number’ and an ordered set of items,
which has the same size as the interval for all but the last
interval (containing position K). Initially the partition only
contains one interval that is associated with all the items
and phase number (cid:96) = 1.

At any point in time, RecurRank works in parallel on all
intervals. Within an interval associated with phase num-
ber (cid:96), the algorithm balances exploration and exploitation
while determining the relative attractiveness of the items
to accuracy ∆(cid:96) = 2−(cid:96). To do this, items are placed in the
ﬁrst position of the interval in proportion to an experimen-
tal design. The remaining items are placed in order in the
remaining positions. Once sufﬁcient data is collected, the
interval is divided into a collection of subintervals and the
algorithm is restarted on each subinterval with the phase
number increased.

The natural implementation of the algorithm maintains a
list of partitions and associated items. In each round it iter-
ates over the partitions and makes assignments of the items
within each partition. The assignments are based on round-
robin idea using an experimental design, which means the
algorithm needs to keep track of how often each item has
been placed in the ﬁrst position. This is not a problem from
an implementation perspective, but stateful code is hard
to interpret in pseudocode. We provide a recursive imple-
mentation that describes the assignments made within each
interval and the rules for creating a new partition. A ﬂow
chart depicting the operation of the algorithm is given in

T (a) =

(cid:24) d π(a)
2∆2
(cid:96)

log

(cid:19)(cid:25)

(cid:18) |A|
δ(cid:96)

(6)

This instance will run (cid:80)

a∈A T (a) times

4: Select each item a ∈ A exactly T (a) times at position k
and put available items in {a1, . . . , am} sequentially in
positions {k + 1, . . . , k + m − 1} and receive feedbacks
(synchronized by a global clock).

5: Let D = {(β1, ζ1), . . .} be the multiset of item/clicks

from position k and compute

ˆθ = V †S with
(cid:88)

V =

ββ(cid:62) and S =

(cid:88)

βζ

(β,ζ)∈D

(β,ζ)∈D

(7)

6: Let a(1), a(2), . . . , a(|A|) be an ordering A such that

εi = (cid:104)ˆθ, a(i) − a(i+1)(cid:105) ≥ 0 for all 1 ≤ i < |A|

and set ε|A| = 2∆(cid:96)

7: Let (u1, . . . , up) = (i ∈ [|A|] : εi ≥ 2∆(cid:96)) and u0 = 0

Ai = (a(ui−1+1), . . . , a(ui))
Ki = (k + ui−1, . . . , k + min(m, ui) − 1)

8: For each i ∈ [p] such that k + ui−1 ≤ k + m − 1 call
RecurRank ((cid:96) + 1, Ai, Ki) on separate threads

The pseudocode of the core subroutine on each interval
is given in Algorithm 1. The subroutine accepts as input
(1) the phase number (cid:96), (2) the positions of the interval
K ⊆ [K] and (3) an ordered list of items, A. The phase
number determines the length of the experiment and the
target precision. The ordering of the items in A is arbi-
trary in the initial partition (when (cid:96) = 1). When (cid:96) > 1
the ordering is determined by the empirical estimate of at-
tractiveness in the previous experiment, which is crucial
for the analysis. The whole algorithm is started by call-
ing RecurRank(1, L, (1, 2, . . . , K)) where the order of L
is random. The algorithm is always instantiated with param-
eters that satisfy |A| ≥ |K| = m. Furthermore, |A| > |K|
is only possible when K ∈ K.

The subroutine learns about the common unknown param-
eter vector by placing items in the ﬁrst position of the in-
terval in proportion to a G-optimal design for the available
items. The remaining items in A are placed in order into

Online Learning to Rank with Features

(cid:96) = 2

(cid:96) = 3

A
||

(cid:122)(cid:125)(cid:124)(cid:123)a1...a3

(cid:124)(cid:123)(cid:122)(cid:125)

A
||

(cid:122)(cid:125)(cid:124)(cid:123)a1...a3

(cid:124)(cid:123)(cid:122)(cid:125)

1

3

Instance 2

Instance 4

(cid:96) = 1

1

A
||
(cid:122)(cid:125)(cid:124)(cid:123)a1

·
·
·

8

a8
·
·
·
a50
(cid:124)(cid:123)(cid:122)(cid:125)
Instance 1

1

3

4

8

(cid:96) = 2

A
||
(cid:122)(cid:125)(cid:124)(cid:123)a4
·
·
·
a8
·
·
·
a25
(cid:124)(cid:123)(cid:122)(cid:125)
Instance 3

· · ·

· · ·

· · ·

4
5

6

8

Instance 5

(cid:96) = 3

A
||
(cid:122)(cid:125)(cid:124)(cid:123)a4a5
(cid:124)(cid:123)(cid:122)(cid:125)

A
||

(cid:122)(cid:125)(cid:124)(cid:123)a6...a8

(cid:96) = 3

·
·
·
a12
(cid:124)(cid:123)(cid:122)(cid:125)
Instance 6

t3

1

t1

t2

t

Figure 1. A ﬂow chart demonstration for the algorithm. Each dotted circle represents a subinterval and runs an instance of Algorithm 1.
The dashed line denotes the ﬁrst position for each interval.

the remaining positions (Line 4). This means that each item
a ∈ A is placed exactly T (a) times in the ﬁrst position k
of the interval. The choice of T (a) is based on the phase
number (cid:96) and the G-optimal design π over A (Line 2). Note
T (a) = 0 if π(a) = 0. For example, if A = (a1, . . . , am)
and a3 is placed at the ﬁrst position, then the rest positions
are ﬁlled in as a1, a2, a4, a5, . . . , am. The subroutine runs
for (cid:80)
a∈A T (a) rounds. The G-optimal design means that
the number of rounds required to estimate the value of each
item to a ﬁxed precision depends only logarithmically on
the number of items. Higher phase number means longer
experiment and also higher target precision.

Once all arms a ∈ A have been placed in the ﬁrst position of
the interval T (a) times, RecurRank estimates the attrac-
tiveness of the items in A using a least-squares estimator
based on the data collected from the ﬁrst position (Line
5). The items are then ordered based on their estimated
attractiveness. The subroutine then partitions the ordered
items when the difference between estimated attractiveness
of consecutive items is sufﬁciently large (Line 7). Finally
the subroutine recursively calls RecurRank on each parti-
tion for which there are positions available with an increased
phase number with items sorted according to their empirical
attractiveness (Line 8).

Remark 3. Items are eliminated entirely if at the end of
a subroutine a partition is formed for which there are no
available positions. For example, consider the ﬁrst instanti-
ation of RecurRank with (cid:96) = 1 and K = [K] and A = L.
Suppose the observed data is such that p = 2 and u1 ≥ K,

then items a(u1+1), a(u1+2), . . . , a(u2) will be discarded be-
cause the starting position of the second partition would be
larger than K.
Remark 4. The least-squares estimator ˆθ deﬁned in Eq. (7)
actually does not have expectation θ, which means the algo-
rithm is not really estimating attractiveness. Our assump-
tions ensure that the expectation of ˆθ is proportional to θ,
however, which is sufﬁcient for our analysis. This is the
reason for only using the ﬁrst position within an interval for
estimation.

Remark 5. The subroutine only uses data collected during
its own run. Not doing this would introduce bias that may
be hard to control.
In Fig. 1, the algorithm starts with Instance 1 of phase num-
ber (cid:96) = 1, all items and all positions. At time t1, Instance 1
splits into two, each with an increased phase number (cid:96) = 2.
Instance 2 contains 3 items and 3 positions and Instance 3
contains 5 positions but 22 items. The remaining items have
been eliminated. At time t2, Instance 2 ﬁnishes running but
has no split, so it calls Instance 4 with the same items, same
positions but increased phase number (cid:96) = 3. During time
t1 to t2, Instance 2 and Instance 3 run in parallel and rec-
ommend lists together; during time t2 to t3, Instances 3 and
4 run in parallel and recommend lists together. At time t3,
Instance 3 ﬁnishes and splits into another two threads, both
with increased phase number (cid:96) = 3. Instance 5 contains
exactly 2 items and 2 positions and Instance 6 contains 3
positions but 7 items. Note that the involved items become
even less. Right after time t3, Instance 4, 5, 6 run in parallel
and recommend lists together.

Online Learning to Rank with Features

RecurRank has two aspects that one may think can lead to
an unjustiﬁable increase of regret: (i) each subroutine only
uses data from the ﬁrst position to estimate attractiveness,
and (ii) data collected by one subroutine is not re-used
subsequently. The second of these is relatively minor. Like
many elimination algorithms, the halving of the precision
means that at most a constant factor is lost by discarding
the data. The ﬁrst issue is more delicate. On the one hand,
it seems distasteful not to use all available data. But the
assumptions do not make it easy to use data collected in
later positions. And actually the harm may not be so great.
Intuitively the cost of only using data from the ﬁrst position
is greatest when the interval is large and the attractiveness
varies greatly within the interval. In this case, however, a
split will happen relatively fast.

Running time The most expensive component is comput-
ing the G-optimal design. This is a convex optimisation
problem and has been studied extensively (see, Boyd &
Vandenberghe 2004, §7.5 and Todd 2016). It is not neces-
sary to solve the optimisation problem exactly. Suppose
instead we ﬁnd a distribution π on A with support at most
D(D + 1)/2 and for which maxa∈A (cid:107)a(cid:107)2
Q(π)† ≤ D. Then
our bounds continue to hold with d replaced by D. Such
approximations are generally easy to ﬁnd. For example, π
may be chosen to be a uniform distribution on a volumetric
spanner of A of size D. See Appendix B for a summary on
volumetric spanners. Hazan & Karnin (2016) provide a ran-
domized algorithm that returns a volumetric spanner of size
at most O(d log(d) log(|A|)) with an expected running time
of O(|A| d2). For the remaining parts of the algorithm, the
least-squares estimation is at most O(d3). The elimination
and partitioning run in O(|A| d). Note these computations
happen only once for each instantiation. The update for
each partition in each round is O(d2). The total running
time is O(Ld2 log(T ) + Kd2T ).

4. Regret Analysis

Our main theorem bounds the regret of Algorithm 1.
Theorem 1. There exists a universal constant C > 0 such
that the regret bound for Algorithm 1 with δ = 1/
T
satisﬁes

√

RT ≤ CK(cid:112)dT log(LT ) .

Let I(cid:96) be the number of calls to RecurRank with phase
number (cid:96). Hence each i ∈ [I(cid:96)] corresponds to a call of
RecurRank with phase number (cid:96) and the arguments are
denoted by A(cid:96)i and K(cid:96)i. Abbreviate K(cid:96)i = min K(cid:96)i for
the ﬁrst position of K(cid:96)i, M(cid:96)i = |K(cid:96)i| for the number of
positions and K+
(cid:96)i = K(cid:96)i \ {K(cid:96)i}. We also let K(cid:96),I(cid:96)+1 =
K + 1 and assume that the calls i ∈ [I(cid:96)] are ordered so that

K(cid:96)i

The reader is reminded that χ∗
k = χ(A∗, k) is the examina-
tion probability of the kth position under the optimal list.
Let χ(cid:96)i = χ∗
be the shorthand for the optimal examina-
tion probability of the ﬁrst position in call ((cid:96), i). We let
ˆθ(cid:96)i be the least-squares estimator computed in Eq. (7) in
Algorithm 1. The maximum phase number during the entire
operation of the algorithm is (cid:96)max.
Deﬁnition 1. Let F be the failure event that there exists an
(cid:96) ∈ [(cid:96)max], i ∈ [I(cid:96)] and a ∈ A(cid:96)i such that

(cid:12)
(cid:12)
(cid:12)(cid:104)ˆθ(cid:96)i, a(cid:105) − χ(cid:96)i(cid:104)θ∗, a(cid:105)
(cid:12)
(cid:12)
(cid:12) ≥ ∆(cid:96)

or there exists an (cid:96) ∈ [(cid:96)max], i ∈ [I(cid:96)] and k ∈ K(cid:96)i such that
a∗
k /∈ A(cid:96)i.

The ﬁrst lemma shows that the failure event occurs with
low probability. The proof follows the analysis in (Latti-
more & Szepesvári, 2018, Chap. 22) and is summarised in
Appendix A.
Lemma 1. P (F ) ≤ δ.

k+1)) ≤ 8∆(cid:96).

The proofs of the following lemmas are provided in Ap-
pendix C.
Lemma 2. On the event F c it holds for any (cid:96) ∈ [(cid:96)max],
i ∈ [I(cid:96)] and positions k, k + 1 ∈ K(cid:96)i that χ(cid:96)i(α(a∗
k) −
α(a∗
Lemma 3. On the event F c it holds for any (cid:96) ∈ [(cid:96)max] and
a ∈ A(cid:96)I(cid:96) that χ(cid:96)I(cid:96)(α(a∗
Lemma 4. Suppose that in its ((cid:96), i)th call RecurRank
places item a in position k = K(cid:96)i. Then, provided F c holds,
χ(cid:96)i (α(a∗
Lemma 5. Suppose that in its ((cid:96), i)th call RecurRank
places item a in position k ∈ K+
(cid:96)i. Then provided F c holds,
χ(cid:96)i (α(a∗

k) − α(a)) ≤ 8M(cid:96)i.

K) − α(a)) ≤ 8∆(cid:96).

k) − α(a)) ≤ 4∆(cid:96).

Proof of Theorem 1. The ﬁrst step is to decompose the re-
gret using the failure event:

RT ≤ P (F ) T K + E

(v(A∗, k) − v(At, k))

.

(cid:35)

(cid:34)
1F c

T
(cid:88)

K
(cid:88)

t=1

k=1

From now on we assume that F c holds and bound the term
inside the expectation. Given (cid:96) and i ∈ [I(cid:96)] let T(cid:96)i be the set
of rounds when algorithm ((cid:96), i) is active. Then

T
(cid:88)

K
(cid:88)

t=1

k=1

(v(A∗, k) − v(At, k)) =

R(cid:96)i ,

(8)

(cid:96)max(cid:88)

I(cid:96)(cid:88)

(cid:96)=1

i=1

where R(cid:96)i is the regret incurred during call ((cid:96), i):

(cid:88)

(cid:88)

R(cid:96)i =

(v(A∗, k) − v(At, k)) .

1 = K(cid:96)1 < K(cid:96)2 < · · · < K(cid:96)I(cid:96) ≤ K < K + 1 = K(cid:96),I(cid:96)+1 .

t∈T(cid:96)i

k∈K(cid:96)i

Online Learning to Rank with Features

This quantity is further decomposed into the ﬁrst position
in K(cid:96)i, which is used for exploration, and the remaining
positions:

R(1)

(cid:96)i =

R(2)

(cid:96)i =

(cid:88)

t∈T(cid:96)i
(cid:88)

(v(A∗, K(cid:96)i) − v(At, K(cid:96)i)) .

(cid:88)

(v(A∗, k) − v(At, k)) .

t∈T(cid:96)i

k∈K+
(cid:96)i

Each of these terms is bounded separately. For the ﬁrst term
we have

R(1)

(cid:96)i =

(v(A∗, K(cid:96)i) − v(At, K(cid:96)i))

(cid:88)

t∈T(cid:96)i
(cid:88)

t∈T(cid:96)i
(cid:88)

=

=

t∈T(cid:96)i
(cid:88)

≤ 8

t∈T(cid:96)i

χ(A∗, K(cid:96)i)α(a∗

K(cid:96)i

) − χ(At, K(cid:96)i)α(At(K(cid:96)i))

(cid:8)α(a∗

χ(cid:96)i

K(cid:96)i

) − α(At(K(cid:96)i))(cid:9)

M(cid:96)i∆(cid:96) ,

while

(9)

where the ﬁrst equality is the deﬁnition of R(1)
(cid:96)i , the second
is the deﬁnition of v. The third inequality is true because
event F c ensures that

hence,

{At(k) : k < K(cid:96)i} = {a∗

k : k < K(cid:96)i} ,

which combined with Assumption 1 shows
that
χ(A∗, K(cid:96)i) = χ(At, K(cid:96)i) = χ(cid:96)i. The inequality in
Eq. (9) follows from Lemma 4. Moving on to the second
term,

R(2)

(cid:96)i =

(cid:88)

(cid:88)

(v(A∗, k) − v(At, k))

k(α(a∗
χ∗

k) − α(At(k)))

χ(cid:96)i(α(a∗

k) − α(At(k)))

(10)

t∈T(cid:96)i
(cid:88)

k∈K+
(cid:96)i
(cid:88)

t∈T(cid:96)i
(cid:88)

k∈K+
(cid:96)i
(cid:88)

≤

≤

t∈T(cid:96)i
(cid:88)

k∈K+
(cid:96)i
(cid:88)

≤ 4

∆(cid:96)

k∈K+
(cid:96)i

t∈T(cid:96)i
(cid:88)

t∈T(cid:96)i

≤ 4

M(cid:96)i∆(cid:96) ,

where the second inequality follows from Assumption 3 and
the third inequality follows from Assumption 2 on ranking
A∗. The inequality in Eq. (10) follows from Lemma 5 and
the one after it from the deﬁnition of M(cid:96)i = |K(cid:96)i|. Putting
things together,

(8) = 12

|T(cid:96)i|M(cid:96)i∆(cid:96) ≤ 12K

(cid:96)max(cid:88)

(cid:88)

(cid:96)=1

i∈I(cid:96)

(cid:96)max(cid:88)

(cid:96)=1

max
i∈I(cid:96)

|T(cid:96)i|∆(cid:96) ,

(11)

i∈I(cid:96)

where we used that (cid:80)
M(cid:96)i = K. To bound |T(cid:96)i| note
that, on the one hand, |T(cid:96)i| ≤ T (this will be useful when
(cid:96) is large), while on the other hand, by the deﬁnition of the
algorithm and the fact that the G-optimal design is supported
on at most d(d + 1)/2 points we have

(cid:88)

|T(cid:96)i| ≤

(cid:24) 2dπ(a) log(1/δ(cid:96))
∆2
(cid:96)

(cid:25)

a∈A(cid:96)i
d(d + 1)
2

≤

+

2d log(1/δ(cid:96))
∆2
(cid:96)

.

We now split to sum in (11) into two. For 1 ≤ (cid:96)0 ≤ (cid:96)max to
be chosen later,

(cid:96)0(cid:88)

(cid:96)=1

max
i∈I(cid:96)

|T(cid:96)i|∆(cid:96) ≤

d(d + 1)
2

+ 4d log(1/δ(cid:96)0)2(cid:96)0 ,

(cid:96)max(cid:88)

(cid:96)=(cid:96)0+1

max
i∈I(cid:96)

|T(cid:96)i|∆(cid:96) ≤ T

∆(cid:96) ≤ T 2−(cid:96)0 ,

(cid:96)max(cid:88)

(cid:96)=(cid:96)0+1

(8) ≤ 12K

+ 4d log(1/δ(cid:96)0 )2(cid:96)0 + T 2−(cid:96)0

.

(cid:27)

(cid:26) d(d + 1)
2

The result is completed by optimising (cid:96)0.

5. Experiments

We run experiments to compare RecurRank with
CascadeLinUCB (Li et al., 2016; Zong et al., 2016) and
TopRank (Lattimore et al., 2018).

Synthetic experiments We construct environments using
the cascade click model (CM) and the position-based click
model (PBM) with L = 104 items in d = 5 dimension to
be displayed in K = 10 positions. We ﬁrst randomly draw
item vectors L and weight vector θ∗ in d − 1 dimension with
each entry a standard Gaussian variable, then normalise, add
one more dimension with constant 1, and divide by
2. The
transformation is as follows:
(cid:18) x
√

x (cid:55)→

(12)

√

(cid:19)

,

.

1
√
2

2 (cid:107)x(cid:107)

This transformation on both the item vector x ∈ L ⊂ Rd
and weight vector θ∗ is to guarantee the attractiveness
(cid:104)θ∗, x(cid:105) of each item x lies in [0, 1]. The position bias for
(cid:1) which is often adopted in
PBM is set as (cid:0)1, 1
K
applications (Wang et al., 2018). The evolution of the regret
as a function of time is shown in Fig. 2(a)(b). The regrets at
the end and total running times are given in Appendix D.

3 , . . . , 1

2 , 1

Online Learning to Rank with Features

Figure 2. The ﬁgures compare RecurRank (red) with CascadeLinUCB (black) and TopRank (blue). Subﬁgure (a) shows results for
an environment that follows the cascade click model (CM), while subﬁgure (b) does the same for the position-based click model (PBM).
On these ﬁgures, regret over time is shown (smaller is better). In both models there are L = 104 items and K = 10 positions, and the
feature space dimension is d = 5. Note the logarithmic scale of the y axis on subﬁgure (a). Subﬁgure (c) shows the regret over time on
the MovieLens dataset with L = 103, d = 5, K = 10. All results are averaged over 10 random runs. The error bars are standard errors.

CascadeLinUCB is best in CM but worst in PBM because
of its modelling bias. TopRank takes much longer time
to converge than either CascadeLinUCB or RecurRank
since it neither exploits the speciﬁcs of the click model, nor
does it use the linear structure.

MovieLens dataset We use the 20m MovieLens dataset
(Harper & Konstan, 2016) which contains 20 million rat-
ings for 2.7 × 104 movies by 1.38 × 105 users. We extract
L = 103 movies with most ratings and 1.1 × 103 users who
rate most and randomly split the user set to two parts, U1 and
U2 with |U1| = 100 and |U2| = 103. We then use the rating
matrix of users in U1 to derive feature vectors with d = 5
for all movies by singular-value decomposition (SVD). The
resulting feature vectors L are also processed as (12). The
true weight vector θ∗ is computed by solving the linear sys-
tem of L w.r.t. the rating matrix of U2. The environment
is the document-based click model (DBM) with L and θ∗
and we set K = 10. The performances are measured in
regret, as shown in Fig. 2(c). As can be seen, RecurRank
learns faster than the other two algorithms. Of these two al-
gorithms, the performance of CascadeLinUCB saturates:
this is due to its incorrect bias.

6. Discussion

Assumptions Our assumptions are most closely related to
the work by Lattimore et al. (2018) and Zoghi et al. (2017).
The latter work also assumes a factored model where the
probability of clicking on an item factors into an exami-
nation probability and an attractiveness function. None of
these works make use of features to model the attractiveness
of items: They are a special case of our model when we
set the features of items to be orthogonal to each other (in
particular, d = L). Our assumptions on the examination
probability function are weaker than those by Zoghi et al.
(2017). Despite this, our regret upper bound is better by
a factor of K (when setting d = L) and the analysis is
also simpler. The paper by Lattimore et al. (2018) does not

assume a factored model, but instead places assumptions
directly on v. They also assume a speciﬁc behaviour of the
v function under pairwise exchanges that is not required
here. Their assumptions are weaker in the sense that they
do not assume the probability of clicking on position k only
depends on the identities of the items in positions [k − 1]
and the attractiveness of the item in position k. On the
other hand, they do assume a speciﬁc behaviour of the v
function under pairwise exchanges that is not required by
our analysis. It is unclear which set of these assumptions is
preferable.

√

In the orthogonal case where d = L the
Lower bounds
lower bound in (Lattimore et al., 2018) provides an example
T KL). For d ≤ L, the
where the regret is at least Ω(
standard techniques for proving lower bounds for linear
bandits can be used to prove the regret is at least Ω(
dT K),
which except for logarithmic terms means our upper bound
K. We are not sure
is suboptimal by a factor of at most
whether either the lower bound or the upper bound is tight.

√

√

Open questions Only using data from the ﬁrst position
seems suboptimal, but is hard to avoid without making
additional assumptions. Nevertheless, we believe a small
improvement should be possible here. Another natural ques-
tion is how to deal with the situation when the set of avail-
able items is changing. In practice this happens in many
applications, either because the features are changing or
because new items are being added or removed. Other inter-
esting directions are to use weighted least-squares estimators
to exploit the low variance when the examination probability
and attractiveness are small. Additionally one can use a gen-
eralised linear model instead of the linear model to model
the attractiveness function, which may be analysed using
techniques developed by Filippi et al. (2010) and Jun et al.
(2017). Finally, it could be interesting to generalise to the
setting where item vectors are sparse (see Abbasi-Yadkori
et al. 2012 and Lattimore & Szepesvári 2018, Chap. 23).

Online Learning to Rank with Features

References

Abbasi-Yadkori, Y., Pál, D., and Szepesvári, C. Improved
algorithms for linear stochastic bandits. In Shawe-Taylor,
J., Zemel, R. S., Bartlett, P. L., Pereira, F., and Weinberger,
K. Q. (eds.), Advances in Neural Information Processing
Systems 24, NIPS, pp. 2312–2320. Curran Associates,
Inc., 2011.

Abbasi-Yadkori, Y., Pal, D., and Szepesvári, C. Online-
to-conﬁdence-set conversions and application to sparse
stochastic bandits. In Lawrence, N. D. and Girolami, M.
(eds.), Proceedings of the 15th International Conference
on Artiﬁcial Intelligence and Statistics, volume 22 of
Proceedings of Machine Learning Research, pp. 1–9, La
Palma, Canary Islands, 21–23 Apr 2012. PMLR.

Abe, N. and Long, P. M. Associative reinforcement learning
using linear probabilistic concepts. In Proceedings of
the 16th International Conference on Machine Learning,
ICML, pp. 3–11, San Francisco, CA, USA, 1999. Morgan
Kaufmann Publishers Inc.

Auer, P. Using conﬁdence bounds for exploitation-
exploration trade-offs. Journal of Machine Learning
Research, 3(Nov):397–422, 2002.

Boyd, S. and Vandenberghe, L. Convex optimization. Cam-

bridge university press, 2004.

Bubeck, S., Cesa-Bianchi, N., and Kakade, S. Towards min-
imax policies for online linear optimization with bandit
feedback. In Annual Conference on Learning Theory,
volume 23, pp. 41–1. Microtome, 2012.

Chaudhuri, S. Learning to Rank: Online Learning, Statisti-

cal Theory and Applications. PhD thesis, 2016.

Chen, Y. and Hofmann, K. Online learning to rank: Abso-
lute vs. relative. In Proceedings of the 24th International
Conference on World Wide Web, pp. 19–20. ACM, 2015.

Chuklin, A., Markov, I., and de Rijke, M. Click Models for

Web Search. Morgan & Claypool Publishers, 2015.

Combes, R., Magureanu, S., Proutiere, A., and Laroche, C.
Learning to rank: Regret lower bounds and efﬁcient algo-
rithms. In Proceedings of the 2015 ACM SIGMETRICS
International Conference on Measurement and Modeling
of Computer Systems, pp. 231–244. ACM, 2015. ISBN
978-1-4503-3486-0.

Filippi, S., Cappe, O., Garivier, A., and Szepesvári, C. Para-
metric bandits: The generalized linear case. In Lafferty,
J. D., Williams, C. K. I., Shawe-Taylor, J., Zemel, R. S.,
and Culotta, A. (eds.), Advances in Neural Information
Processing Systems 23, NIPS, pp. 586–594. Curran Asso-
ciates, Inc., 2010.

Harper, F. M. and Konstan, J. A. The movielens datasets:
History and context. Acm transactions on interactive
intelligent systems (tiis), 5(4):19, 2016. URL https://
grouplens.org/datasets/movielens/20m/.

Hazan, E. and Karnin, Z. Volumetric spanners: an efﬁcient
exploration basis for learning. The Journal of Machine
Learning Research, 17(1):4062–4095, 2016.

Hofmann, K., Whiteson, S., and De Rijke, M. A proba-
bilistic method for inferring preferences from clicks. In
Proceedings of the 20th ACM international conference on
Information and knowledge management, pp. 249–258.
ACM, 2011.

Jun, K., Bhargava, A., Nowak, R., and Willett, R. Scalable
generalized linear bandits: Online computation and hash-
ing. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach,
H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.),
Advances in Neural Information Processing Systems 30,
pp. 99–109. Curran Associates, Inc., 2017.

Katariya, S., Kveton, B., Szepesvári, C., and Wen, Z. DCM
bandits: Learning to rank with multiple clicks. In Pro-
ceedings of the 33rd International Conference on Ma-
chine Learning, pp. 1215–1224, 2016.

Katariya, S., Kveton, B., Szepesvári, C., Vernade, C., and
Wen, Z. Bernoulli rank-1 bandits for click feedback. In
Proceedings of the 26th International Joint Conference
on Artiﬁcial Intelligence, 2017a.

Katariya, S., Kveton, B., Szepesvári, C., Vernade, C., and
Wen, Z. Stochastic rank-1 bandits. In Proceedings of the
20th International Conference on Artiﬁcial Intelligence
and Statistics, 2017b.

Kiefer, J. and Wolfowitz, J. The equivalence of two ex-
tremum problems. Canadian Journal of Mathematics, 12
(5):363–365, 1960.

Kveton, B., Szepesvári, C., Wen, Z., and Ashkan, A. Cas-
cading bandits: Learning to rank in the cascade model.
In Proceedings of the 32nd International Conference on
International Conference on Machine Learning - Volume
37, pp. 767–776. JMLR.org, 2015.

Lagree, P., Vernade, C., and Cappé, O. Multiple-play bandits
in the position-based model. In Advances in Neural In-
formation Processing Systems 29, NIPS, pp. 1597–1605.
Curran Associates Inc., 2016.

Lattimore, T. and Szepesvári, C. Bandit Algorithms.

preprint, 2018.

Lattimore, T., Kveton, B., Li, S., and Szepesvári, C.
Toprank: A practical algorithm for online stochastic rank-
ing. In Proceedings of the 31st Conference on Neural
Information Processing Systems. 2018.

Online Learning to Rank with Features

Li, L., Chu, W., Langford, J., and Schapire, R. E. A
contextual-bandit approach to personalized news article
In Proceedings of the 19th interna-
recommendation.
tional conference on world wide web, pp. 661–670. ACM,
2010.

Zoghi, M., Tunys, T., Ghavamzadeh, M., Kveton, B.,
Szepesvári, C., and Wen, Z. Online learning to rank
in stochastic click models. In Proceedings of the 34th In-
ternational Conference on Machine Learning, volume 70
of PMLR, pp. 4199–4208, 2017.

Zong, S., Ni, H., Sung, K., Ke, R. N., Wen, Z., and Kveton,
B. Cascading bandits for large-scale recommendation
problems. In Proceedings of the 32nd Conference on
Uncertainty in Artiﬁcial Intelligence, UAI, 2016.

Li, S. and Zhang, S. Online clustering of contextual cascad-
ing bandits. In The 32nd AAAI Conference on Artiﬁcial
Intelligence, pp. 3554–3561, 2018.

Li, S., Wang, B., Zhang, S., and Chen, W. Contextual
combinatorial cascading bandits. In Proceedings of the
33rd International Conference on Machine Learning, pp.
1245–1253, 2016.

Liu, W., Li, S., and Zhang, S. Contextual dependent click
bandit algorithm for web recommendation. In Interna-
tional Computing and Combinatorics Conference, pp.
39–50. Springer, 2018.

Radlinski, F., Kleinberg, R., and Joachims, T. Learning
diverse rankings with multi-armed bandits. In Proceed-
ings of the 25th International Conference on Machine
Learning, pp. 784–791. ACM, 2008.

Rustichini, A. Minimizing regret: The general case. Games

and Economic Behavior, 29(1):224–243, 1999.

Slivkins, A., Radlinski, F., and Gollapudi, S. Ranked ban-
dits in metric spaces: learning diverse rankings over large
document collections. Journal of Machine Learning Re-
search, 14(Feb):399–436, 2013.

Soare, M., Lazaric, A., and Munos, R. Best-arm identiﬁca-
tion in linear bandits. In Ghahramani, Z., Welling, M.,
Cortes, C., Lawrence, N. D., and Weinberger, K. Q. (eds.),
Advances in Neural Information Processing Systems 27,
NIPS, pp. 828–836. Curran Associates, Inc., 2014.

Todd, M. J. Minimum-volume ellipsoids: Theory and algo-

rithms. SIAM, 2016.

Valko, M., Munos, R., Kveton, B., and Kocák, T. Spectral
bandits for smooth graph functions. In Xing, E. P. and
Jebara, T. (eds.), Proceedings of the 31st International
Conference on Machine Learning, volume 32 of Proceed-
ings of Machine Learning Research, pp. 46–54, Bejing,
China, 22–24 Jun 2014. PMLR.

Wang, X., Golbandi, N., Bendersky, M., Metzler, D., and
Najork, M. Position bias estimation for unbiased learning
to rank in personal search. In Proceedings of the Eleventh
ACM International Conference on Web Search and Data
Mining, pp. 610–618. ACM, 2018.

Xu, L., Honda, J., and Sugiyama, M. Fully adaptive al-
gorithm for pure exploration in linear bandits. arXiv
preprint arXiv:1710.05552, 2017.

Online Learning to Rank with Features

A. Proof of Lemma 1

In what follows, we add the index ((cid:96), i) to any symbol used
in the algorithm to indicate the value that it takes in the ((cid:96), i)
call. For example, D(cid:96)i denotes the data multiset collected in
the ((cid:96), i) call, T(cid:96)i(a) be the value computed in Eq. (6), etc.

Fix (cid:96) ≥ 1 and let F(cid:96) be the failure event that there exists an
i ∈ [I(cid:96)] and a ∈ A(cid:96)i such that
(cid:12)
(cid:12)
(cid:12)(cid:104)ˆθ(cid:96)i, a(cid:105) − χ(cid:96)i(cid:104)θ∗, a(cid:105)
(cid:12)
(cid:12)
(cid:12) ≥ ∆(cid:96) .

Let E(cid:96) be the event that for any i ∈ [I(cid:96)], the examination
probability on the ﬁrst position of the call ((cid:96), i) is χ(cid:96)i. For
the argument that follows, let us assume that E(cid:96) holds.

By our modelling assumptions (Eqs. (1) to (3)), for any
(β, ζ) ∈ D(cid:96)i,

=(cid:104)V †
(cid:96)i

(cid:88)

(β,ζ)∈D(cid:96)i

β(β(cid:62)θ∗χ(cid:96)i + η(β,ζ)) − χ(cid:96)iθ∗, a(cid:105)

=χ(cid:96)i(cid:104)(V †

(cid:96)iV(cid:96)i − I)θ∗, a(cid:105) + (cid:104)V †

(cid:96)i

(cid:88)

βη(β,ζ), a(cid:105)

(β,ζ)∈D(cid:96)i

(cid:88)

=

(β,ζ)∈D(cid:96)i

(cid:104)V †

(cid:96)iβ, a(cid:105) η(β,ζ) .

(14)

The last equality follows from I − V †
(cid:96)iV(cid:96)i is the orthogonal
projection on the kernel of V(cid:96)i, which is the orthogonal
complement of A(cid:96)i, and thus will map each a ∈ A(cid:96)i to the
zero vector. Then, for any a ∈ A(cid:96)i,

(cid:17)

P

(cid:12)
(cid:16)(cid:12)
(cid:12)(cid:104)ˆθ(cid:96)i − χ(cid:96)iθ∗, a(cid:105)
(cid:12)
(cid:12)
(cid:12) ≥ ∆(cid:96)
(cid:32)
(cid:12)
(cid:12)
(cid:12)(cid:104)ˆθ(cid:96)i − χ(cid:96)iθ∗, a(cid:105)
(cid:12)
(cid:12)
(cid:12) ≥

≤ P

(cid:115)

1
2

(cid:107)a(cid:107)2
V †
(cid:96)i

log

(cid:18) |A(cid:96)i|
δ(cid:96)

(cid:19)(cid:33)

ζ = (cid:104)χ(cid:96)iθ∗, β(cid:105) + η(β,ζ) ,

≤

2δ(cid:96)
|A(cid:96)i|

.

where {η(β,ζ)}(β,ζ) is a conditionally 1/2-subgaussian se-
quence.

Deﬁne the Gram matrix Q for any probability mass func-
tion π : A → [0, 1], (cid:80)
a∈A π(a) = 1, as Q(π) =
(cid:80)
a∈A π(a)aa(cid:62). By the Kiefer-Wolfowitz theorem (Kiefer

& Wolfowitz, 1960),

(cid:107)a(cid:107)2

max
a∈A(cid:96)i

Q(π(cid:96)i)† = rank(A) ≤ d ,

where Q† denotes the Moore-Penrose inverse of Q. Then,
by Eq. (6),

V(cid:96)i =

T(cid:96)i(a)aa(cid:62) (cid:23)

(cid:88)

a∈A(cid:96)i

d
2∆2
(cid:96)

log

(cid:19)

(cid:18) |A(cid:96)i|
δ(cid:96)

Q(π(cid:96)i) ,

where P (cid:23) Q denotes that P precedes Q in the Loewner
partial ordering of positive semi-deﬁnite (symmetric) matri-
ces. This implies that

(cid:107)a(cid:107)2
V †
(cid:96)i

≤

2∆2
(cid:96)
d

1
(cid:16) |A(cid:96)i|
δ(cid:96)

log

(cid:17) (cid:107)a(cid:107)2

Q(π(cid:96)i)†

≤ 2∆2
(cid:96)

1
(cid:16) |A(cid:96)i|
δ(cid:96)

(cid:17) .

log

Rearranging shows that

∆(cid:96) ≥

(cid:115)

1
2

(cid:107)a(cid:107)2
V †
(cid:96)i

log

(cid:18) |A(cid:96)i|
δ(cid:96)

(cid:19)

.

(13)

Now note that

(cid:104)ˆθ(cid:96)i − χ(cid:96)iθ∗, a(cid:105)
(cid:88)

=(cid:104)V †
(cid:96)i

(β,ζ)∈D(cid:96)i

βζ − χ(cid:96)iθ∗, a(cid:105)

The ﬁrst inequality is by Eq. (13). The second inequality
is by Eq. (14), the concentration bound on conditional sub-
gaussian sequences (Lattimore & Szepesvári, 2018, Lemma
5.2 and Theorem 5.1), and (cid:80)
.
(β,ζ)∈D(cid:96)i
Thus with probability at least 1 − 2δ(cid:96),
(cid:12)
(cid:12)
(cid:12)(cid:104)ˆθ(cid:96)i − χ(cid:96)iθ∗, a(cid:105)
(cid:12)
(cid:12)
(cid:12) ≤ ∆(cid:96)

(cid:96)iβ, a(cid:105)2 = (cid:107)a(cid:107)2
V †
(cid:96)i

(cid:104)V †

holds for any a ∈ A(cid:96)i and thus from I(cid:96) ≤ K, we get that

P (F(cid:96) ∩ E(cid:96)) ≤ 2Kδ(cid:96) .

(15)

K(cid:96)I(cid:96)

, . . . , a∗

Now we prove by induction on (cid:96) that on the complementer of
F1:(cid:96)−1 = F1∪· · ·∪F(cid:96)−1 (with F1:0 = ∅) the following hold
true: (i) the examination probability on the ﬁrst position of
the call ((cid:96), i) is χ(cid:96)i for any i ∈ [I(cid:96)]; (ii) a∗
K are
the M(cid:96)I(cid:96) best items in A(cid:96)I(cid:96) and that (iii) for any i, j ∈ [I(cid:96)],
i < j, and a ∈ A(cid:96)i, a(cid:48) ∈ A(cid:96)j, it holds that α(a) < α(a(cid:48))
(note that (ii) and (iii) just mean that the algorithm does not
make a mistake when it eliminates items or splits blocks).
The claim is obviously true for (cid:96) = 1. In particular, the
examination probability on the ﬁrst position of the call ((cid:96) =
1, i = 1) is χ1,1 by Assumption 1.
Now, let (cid:96) ≥ 1 and suppose F1:(cid:96) does not hold. If (cid:104)ˆθ(cid:96)i, a(cid:105) −
(cid:104)ˆθ(cid:96)i, a(cid:48)(cid:105) ≥ 2∆(cid:96) for some a, a(cid:48) ∈ A(cid:96)i and i ∈ [I(cid:96)], then by
(i) of the induction hypothesis,

χ(cid:96)i(cid:104)θ∗, a(cid:105) > (cid:104)ˆθ(cid:96)i, a(cid:105) − ∆(cid:96)

≥ (cid:104)ˆθ(cid:96)i, a(cid:48)(cid:105) + ∆(cid:96) > χ(cid:96)i(cid:104)θ∗, a(cid:48)(cid:105) ,

thus α(a) > α(a(cid:48)).

If a ∈ A(cid:96)I(cid:96) is eliminated at the end of call ((cid:96), I(cid:96)), there
exists m = M(cid:96)I(cid:96) different items b1, . . . , bm ∈ A(cid:96)I(cid:96) such

Online Learning to Rank with Features

that (cid:104)ˆθ(cid:96)i, bj(cid:105)−(cid:104)ˆθ(cid:96)i, a(cid:105) ≥ 2∆(cid:96) for all j ∈ [m]. Thus α(bj) >
α(a) for all j ∈ [m]. Since, by induction, a∗
, . . . , a∗
K
are m best items in A(cid:96)I(cid:96), then α(a) < α(a∗
K). This shows
that (ii) will still hold for A(cid:96)+1,I(cid:96)+1.

K(cid:96)I(cid:96)

If there is a split A1, . . . , Ap and K1, . . . , Kp on A(cid:96)i and
K(cid:96)i by the algorithm, (cid:104)ˆθ(cid:96)i, a(cid:105) − (cid:104)ˆθ(cid:96)i, a(cid:48)(cid:105) ≥ 2∆(cid:96) for any
a ∈ Aj, a(cid:48) ∈ Aj+1, j ∈ [p−1]. Then α(a) > α(a(cid:48)). So the
better arms are put at higher positions, which combined with
that (iii) holds at stage (cid:96) shows that (iii) will still continue
to hold for (cid:96) + 1.

Finally, it also follows that χ(cid:96)+1,i = χ∗
is the exami-
nation probability of the ﬁrst position for any call ((cid:96) + 1, i)
of phase (cid:96) + 1, showing that (i) also continues to hold for
phase (cid:96) + 1.

K(cid:96)+1,i

K with n points in it, the and for ε > 0 ﬁxed, a spanner of
size 12(1 + ε)d can be computed in O(n3.5 + dn3 + nd3)
time (Hazan & Karnin, 2016, Theorem 3).

C. Proofs of Technical Lemmas

Proof of Lemma 2. Let F c hold. Since ∆1 = 1/2, the re-
sult is trivial for (cid:96) = 1. Suppose (cid:96) > 1, the lemma holds for
all (cid:96)(cid:48) < (cid:96) and that there exists a pair k, k + 1 ∈ K(cid:96)i satis-
fying χ(cid:96)i(α(a∗
k+1)) > 8∆(cid:96). Let ((cid:96) − 1, j) be the
parent of ((cid:96), i), which satisﬁes a∗
k+1 ∈ A(cid:96)i ⊆ A(cid:96)−1,j.
Since K(cid:96)−1,j ≤ K(cid:96)i it follows from Assumption 2 and the
deﬁnition of F that χ(cid:96)−1,j ≥ χ(cid:96)i and hence

k) − α(a∗

k, a∗

χ(cid:96)−1,j

(cid:0)α(a∗

k) − α(a∗

k+1)(cid:1) > 8∆(cid:96) = 4∆(cid:96)−1 ,

From this argument it follows that F c
all (cid:96) ≥ 1. Then,

1:(cid:96)−1 ⊂ E(cid:96) holds for

where we used the deﬁnition of ∆(cid:96) = 2−(cid:96). Given any
m, n ∈ K(cid:96)−1,j with m ≤ k < k + 1 ≤ n we have

F = (F1:1 ∩ F c

1:0) ∪ (F1:2 ∩ F c

1:1) ∪ (F1:3 ∩ F c

1:2) ∪ . . .

⊂ (F1:1 ∩ E1) ∪ (F1:2 ∩ E2) ∪ (F1:3 ∩ E3) ∪ . . . .

(cid:104)ˆθ(cid:96)−1,j, a∗

Taking probabilities and using (15), we get

P (F ) =

P (F1:(cid:96) ∩ E(cid:96)) ≤ δ ,

(cid:88)

(cid:96)≥1

ﬁnishing the proof.

B. Volumetric Spanners

A volumetric spanner of compact set K ⊂ Rd is a ﬁnite set
S = {x1, . . . , xn} ⊆ K such that

K ⊆ E(S) =

αixi : (cid:107)α(cid:107)2 ≤ 1

.

(cid:41)

(cid:40) n
(cid:88)

i=1

n
(cid:88)

i=1

Let π be a uniform distribution on S and

Q =

π(xi)xix(cid:62)
i .

If S is a volumetric spanner of K, for any x ∈ K it holds
that (cid:107)x(cid:107)2
Q† ≤ n. To see this let U ∈ Rd×n be the matrix
with columns equal to the elements in S, which means that
Q = U U (cid:62)/n. Since x ∈ K there exists an α ∈ Rn with
(cid:107)α(cid:107)2 ≤ 1 such that x = U α. Then

x(cid:62)Q†x = nα(cid:62)U (cid:62)(U U (cid:62))†U α
= nα(cid:62)U †U α
≤ n (cid:107)α(cid:107)2
2
≤ n .

Any compact set admits a volumetric spanner of size
n ≤ 12d, hence by Eq. (5), a volumetric spanner is a “12-
approximation” to the G-optimal design problem. For ﬁnite

m(cid:105) ≥ χ(cid:96)−1,jα(a∗
≥ χ(cid:96)−1,jα(a∗
> χ(cid:96)−1,jα(a∗
≥ χ(cid:96)−1,jα(a∗
≥ (cid:104)ˆθ(cid:96)−1,j, a∗

m) − ∆(cid:96)−1
k) − ∆(cid:96)−1
k+1) + 3∆(cid:96)−1
n) + 3∆(cid:96)−1
n(cid:105) + 2∆(cid:96)−1 .

The ﬁrst and ﬁfth inequalities are because F does not hold.
The third inequality is due to induction assumption on phase
(cid:96) − 1. Hence by the deﬁnition of the algorithm the items a∗
k
and a∗
k+1 will be split into different partitions by the end of
call ((cid:96) − 1, j), which is a contradiction.

Proof of Lemma 3. We use the same idea as the previous
lemma. Let F c hold. The result is trivial for (cid:96) = 1. Suppose
(cid:96) > 1, the lemma holds for (cid:96)(cid:48) < (cid:96) and there exists an
a ∈ A(cid:96)I(cid:96) satisfying χ(cid:96)I(cid:96)(α(a∗
K) − α(a)) > 8∆(cid:96). By the
deﬁnition of the algorithm and F does not hold, a, a∗
K ∈
A(cid:96)−1,I(cid:96)−1 and hence

χ(cid:96)−1,I(cid:96)−1 (α(a∗

K) − α(a)) > 4∆(cid:96)−1 .

For any m ∈ K(cid:96)−1,I(cid:96)−1 with m ≤ K it holds that

(cid:104)ˆθ(cid:96)−1,I(cid:96)−1 , a∗

m(cid:105) ≥ χ(cid:96)−1,I(cid:96)−1α(a∗
m) − ∆(cid:96)−1
≥ χ(cid:96)−1,I(cid:96)−1α(a∗
K) − ∆(cid:96)−1
> χ(cid:96)−1,I(cid:96)−1α(a) + 3∆(cid:96)−1
≥ (cid:104)ˆθ(cid:96)−1,I(cid:96)−1 , a(cid:105) + 2∆(cid:96)−1 .

Hence there exist at least M(cid:96)−1,I(cid:96)−1 items b ∈ A(cid:96)−1,I(cid:96)−1
for which (cid:104)ˆθ(cid:96)−1,I(cid:96)−1 , b − a(cid:105) ≥ 2∆(cid:96)−1. But if this was true
then by the deﬁnition of the algorithm (cf. line 7) item a
would have been eliminated by the end of call ((cid:96) − 1, I(cid:96)−1),
which is a contradiction.

Online Learning to Rank with Features

Proof of Lemma 4. Let F c hold. Suppose that i < I(cid:96) and
abbreviate m = M(cid:96)i. Since F does not hold it follows that
a ∈ {a∗

k+m−1}. By Lemma 2,

k, . . . , a∗

χ(cid:96)i (α(a∗

k) − α(a)) ≤ χ(cid:96)i
m−2
(cid:88)

=

(cid:0)α(a∗

k) − α(a∗

k+m−1)(cid:1)

χ(cid:96)i

(cid:0)α(a∗

k+j) − α(a∗

k+j+1)(cid:1)

D. Quantity Results for Experiments

The regrets of Fig. 2 at the end are given in the Table 2,
while total running times (wall-clock time) are shown in
Table 3. The experiments are run on Dell PowerEdge R920
with CPU of Quad Intel Xeon CPU E7-4830 v2 (Ten-core
2.20GHz) and memory of 512GB.

j=0

≤ 8(m − 1)∆(cid:96) .

Now suppose that i = I(cid:96). Then by Lemma 3 and the same
argument as above,

χ(cid:96)i (α(a∗

k) − α(a))

= χ(cid:96)i (α(a∗
≤ 8m∆(cid:96) .

K) − α(a)) + χ(cid:96)i (α(a∗

k) − α(a∗

K))

The claim follows by the deﬁnition of m.

Proof of Lemma 5. The result is immediate for (cid:96) = 1. From
now on assume that (cid:96) > 1 and let ((cid:96) − 1, j) be the parent
of ((cid:96), i). Since F does not hold, {a∗
m : m ∈ K(cid:96)i} ⊆ A(cid:96)i. It
cannot be that (cid:104)ˆθ(cid:96)−1,j, a∗
m − a(cid:105) > 0 for all m ∈ K(cid:96)i with
m ≤ k, since this would mean that there are k − K(cid:96)i + 2
items that precede item a and hence item a would not be
put in position k by the algorithm. Hence there exists an
m ∈ K(cid:96)i with m ≤ k such that (cid:104)ˆθ(cid:96)−1,j, a∗
m − a(cid:105) ≤ 0 and

χ(cid:96)i(α(a∗

k) − α(a)) ≤ χ(cid:96)i(α(a∗

m) − α(a))

≤ χ(cid:96)−1,j(α(a∗
≤ (cid:104)ˆθ(cid:96)−1,j, a∗
≤ 2∆(cid:96)−1 = 4∆(cid:96) ,

m) − α(a))
m − a(cid:105) + 2∆(cid:96)−1

which completes the proof.

RecurRank
0.53
68, 943
42, 157

CascadeLinUCB
0.17
227, 736
180, 256

TopRank
106.10
745, 177
114, 288

CM
PBM
ML

Table 2. The total regret under (a) CM (b) PBM and (c) ML. The
number shown are computed by taking the average over the 10
random runs.

Time (s)
CM
PBM
ML

RecurRank
51
310
234

CascadeLinUCB
411
4, 147
916

TopRank
176, 772
367, 509
4, 868

Table 3. The total running time of the compared algorithms in
seconds (s). The results are averaged over 10 random runs.

