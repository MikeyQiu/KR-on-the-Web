8
1
0
2
 
y
a
M
 
3
2
 
 
]

G
L
.
s
c
[
 
 
1
v
7
4
2
9
0
.
5
0
8
1
:
v
i
X
r
a

Cleaning up the neighborhood: A full classiﬁcation
for adversarial partial monitoring

Tor Lattimore
DeepMind

Csaba Szepesv´ari
DeepMind

Abstract

Partial monitoring is a generalization of the well-known multi-armed bandit
framework where the loss is not directly observed by the learner. We complete the
classiﬁcation of ﬁnite adversarial partial monitoring to include all games, solving
an open problem posed by Bart´ok et al. [2014]. Along the way we simplify
and improve existing algorithms and correct errors in previous analyses. Our
second contribution is a new algorithm for the class of games studied by Bart´ok
[2013] where we prove upper and lower regret bounds that shed more light on the
dependence of the regret on the game structure.

1 Introduction

Partial monitoring is a generalization of the bandit framework that relaxes the relationship between
the feedback and the loss, which makes the framework applicable to a wider range of practical
problems such as spam ﬁltering and product testing. Equally importantly, it offers a rich and elegant
framework to study the exploration-exploitation dilemma beyond bandits [Rustichini, 1999].

We consider the ﬁnite adversarial version of the problem where a learner and adversary interact over
n rounds. At the start of the game the adversary secretly chooses a sequence of n outcomes from
a ﬁnite set. In each round the learner chooses one of ﬁnitely many actions and receives a feedback
that depends on its action and the choice of the adversary for that round. The loss is also determined
by the action/outcome pair, but is not directly observed by the learner. Although the learner does
not know the choices of the adversary, the feedback/loss functions are known in advance and the
learner must use this infer a good policy. The learner’s goal is to minimize the regret, which is the
difference between the total loss suffered and the loss that would have been suffered by playing the
best single action given knowledge of the adversaries choices.

The study of partial monitoring games started with the work by Rustichini [1999] where the
deﬁnition of regret differed slightly from what is used here and the results have an asymptotic ﬂavor.
These results have been strengthened in an interesting line of work by Mannor and Shimkin [2003],
Perchet [2011], Mannor et al. [2014], the last of which gives non-asymptotic rates for this more
general deﬁnition of regret that unfortunately do not reduce to the optimal rate in our setting. The
regret we consider was ﬁrst considered by Piccolboni and Schindelhauer [2001], who showed that a
variant of exponential weights achieves O(n3/4) regret in nontrivial games. This was improved to
O(n2/3) by Cesa-Bianchi et al. [2006], who also showed that in general this result is not improvable,
but that there exist many types of game for which the regret is O(n1/2). They posed the question
of classifying ﬁnite adversarial partial monitoring games in terms of the achievable minimax regret.
An effort started around 2010 to achieve this goal, which eventually led to the paper by Bart´ok et al.
[2014] who made signiﬁcant progress towards solving this problem. In particular, they gave an
almost complete characterization of partial monitoring games by identifying four regimes: trivial,
easy, hard and hopeless games. The characterization, however, left out the set of games with actions
that are only optimal on low-dimensional subspaces of the adversaries choices. Although these
actions are never uniquely optimal, they can be informative and until now it was not known how

Preprint. Work in progress.

to use these actions when balancing exploration and exploitation. Games in this tricky regime
have been called ‘degenerate’, but there is no particular reason to believe these games should not
appear in practice. This problem is understood in the stochastic variant of partial monitoring where
the adversary chooses the outcomes independently at random [Antos et al., 2013], but a complete
understanding of the adversarial setup has remained elusive.

Contributions

We develop an improved version of NEIGHBOURHOODWATCH by Foster and Rakhlin [2012]
that correctly deals with degenerate games and completes the classiﬁcation for all ﬁnite partial
monitoring games, closing an open question posed by Bart´ok et al. [2014].1 Another beneﬁt is that
Foster and Rakhlin [2012] and Bart´ok et al. [2014] inadvertently exchanged an expectation and
maximum during the localisation argument of their analysis. A correction is presumably possible,
but this would add another level of complexity to an already intricate proof. Our algorithm also
enjoys a regret guarantee that holds with high probability.

Bart´ok [2013] introduced a class of partial monitoring games and suggested a complicated
algorithm with improved regret relative to NEIGHBOURHOODWATCH. We propose a novel
nKloc log(K)), where K is
algorithm and prove that for these games its regret satisﬁes O(F
the number of actions and F is the number of feedback symbols. The quantity Kloc depends on
the game and satisﬁes Kloc
K. This bound improves on the result of Bart´ok [2013] in several
ways: (a) we eliminate the dependence on arbitrarily large game-dependent constants, (b) the
new algorithm is simpler, (c) our bound is better by logarithmic factors of the horizon and (d)
the analysis by Bart´ok mistakenly combines bounds that hold in expectation in ‘local games’ into
a bound for the whole game as if they were high probability bounds. We expect this could be
corrected by modifying the algorithm and analysis, but the resulting algorithm would be even
more complicated and the regret would not improve.

p

≤

We prove a variety of lower bounds. First correcting a minor error in the proof by Bart´ok et al.
[2014] and second showing the linear dependence on the number of feedbacks is unavoidable in
general.

The new algorithms and analysis simplify existing results, which think is a contribution in its own
right and we hope encourages more research into this fascinating topic with many open questions.

•

•

•

•

x

∈

≥

1,

f
k

⊂
) and for function f : A

→
[0, 1]K×E and a feedback matrix Φ

Problem setup Given a natural number n let [n] =
to denote the usual
. We use
1, 2, . . . , n
x, y
{
}
i
h
[0, 1]d+1 :
inner product in Euclidean space. The d-simplex is
, where for
Pd =
x
k1 = 1
x
}
k
{
∈
(0, 1)d+1 :
kp is the p-norm of x. The relative interior of
.
Pd is ri(
p
x
k1 = 1
x
Pd) =
k
∈
k
}
{
Rd+1 is the dimension of its afﬁne hull. For any set A the indicator
The dimension of a set A
function is 1A(
R the supremum norm of f is
. A
f (a)
k∞ = supa∈A |
·
|
partial monitoring problem G = (
, Φ) is a game between a learner and an adversary over n rounds
L
[F ]K×E for natural
and is speciﬁed by a loss matrix
L ∈
numbers E, F and K. At the beginning of the game the learner is given
and Φ and the adversary
L
secretly chooses a sequence of outcomes i1:n = (i1, . . . , in) where it ∈
[n]. In
[E] for each t
[K] and observes feedback Φt = ΦAtit . The loss
each round t the learner chooses an action At ∈
Lait . In contrast to bandit and full information
incurred by playing action a in round t is yta =
problems the loss in partial monitoring is not observed by the learner, even for the action played.
A policy π is a map from sequences of action/observation pairs to a
distribution over the action-set [K]. The performance of a policy π is
measured by its regret, Rn(π, i1:n) = maxa∈[K]
yta).
When the outcome sequence and policy are ﬁxed we abbreviate Rn =
Rn(π, i1:n). The minimax expected regret associated with partial
monitoring game G is the worst-case expected regret of the best policy.
E[Rn(π, i1:n)] where the inf is taken over all
R∗
policies, the max over all outcome sequences of length n and the expectation with respect to the
Ft = σ(A1, A2, . . . , At) be the σ-algebra generated by the
randomness in the actions. We let
information available after round t and abbreviate Et[
·|Ft]. A core question in partial
1Historical note: Foster and Rakhlin [2012] claim a modiﬁcation of their argument would handle degenerate
games but give no details. The followup paper explicitly mentions the difﬁculties and poses the open problem
[Bart´ok et al., 2014, Remark 4 and §8].

0
˜Θ(n1/2)
Θ(n2/3)
Ω(n)

n(G) = inf π maxi1:n

t=1(ytAt −

] = E[

Game type

Hopeless

n(G)

Trivial

Hard

Easy

R∗

P

∈

n

·

2

and Φ affect the growth of R∗
n(G) in terms of the horizon. The
monitoring is to understand how
main theorem of Bart´ok et al. [2014] shows that for all ‘nondegenerate’ games the minimax regret
falls into one of four categories as illustrated in the table. The colloquial meaning of the adjective
degenerate suggests that only nondegenerate games are interesting, but this is not the case. The term
is used in a technical sense (to be clariﬁed soon) referring to a subclass of games that we have no
reason to believe should be less important than the nondegenerate ones.

L

Preliminaries To illustrate some of the difﬁculties of partial monitoring relative to bandits we
formalize a simplistic version of the spam ﬁltering problem.

Example 1 Let c
, Φ) by
game G = (

≥

0 and deﬁne partial monitoring

L

=

L

0
1
c

 

1
0
c!

,

Φ =

1 1
1 1
1 2!

.

 

Loss (L)
Spam
Not spam
Don’t know

Spam Not spam
1
0
0
1
c
c

Feedback (Φ)
Spam
Not spam
Don’t know

Spam Not spam
1
1
1
1
2
1

The idea is also illustrated in the tables on the right.
Rows correspond to actions of the learner and columns
to outcomes selected by the adversary. The learner has three actions in this game corresponding to
‘spam’, ‘not spam’ and ‘don’t know’ while the adversary chooses between ‘spam’ and ‘not spam’.
The learner suffers a loss of 1 if it guesses incorrectly. Alternatively the learner can say they don’t
know in which case they suffer a loss of c and observe some meaningful feedback. The minimax
regret for this game depends on the price of information. If c > 1/2, then the minimax regret is
Θ(n2/3). On the other hand, if c
) indicates
growth up to logarithmic factors. Finally, when c = 0 a policy can suffer no regret by playing just
the third action.

(0, 1/2] the minimax regret is ˜Θ(n1/2) where ˜Θ(
·

∈

Example 2 The game on the right is hopeless because the learner
cannot gain information about her loss and the adversary can always
force the expected regret to be Ω(n).

=

L

0 1
1 0

(cid:18)

(cid:19)

Φ =

1 1
1 1

(cid:18)

(cid:19)

}

∈

L

P

= 0

ℓa −

u
{
t=1 ut ∈

[0, 1]E be the ath row of matrix

∈ PE−1 : maxb∈[K]h

. The cell Ca of action a is the subset of
ℓb, u

Cell decomposition In order to understand what makes a partial monitoring game hard, easy or
hopeless, it helps to introduce a linear structure. Let ut = eit ∈ PE−1 be the standard basis vector
that is nonzero in the coordinate of the outcome it chosen by the adversary in round t. For action
PE−1 on
a let ℓa ∈
which action a is optimal: Ca =
. Action a is optimal
i
n
in hindsight if and only if 1
Ca. Each nonempty Ca is a polytope and the collection
n
is called the cell decomposition of G. An action is called dominated if it is never
[K]
Ca : a
{
}
. We deﬁne the dimension of nondominated action a to be the dimension of Ca,
optimal: Ca =
∅
which ranges between 0 and E
1 are called
1. Nondominated actions with dimension less than E
−
1 are called Pareto optimal. A partial monitoring game
degenerate while actions with dimension E
∈ PE−1 let a∗
is degenerate if it has at least one degenerate action. For each u
ℓa, u
i
t
and a∗
u is an optimal action if the adversary is
ℓa, usi
arg mina
s=1h
playing u on average and a∗
t is the optimal action in hindsight when the adversary plays the sequence
u and a∗
(u1, . . . , ut). Without loss of generality we assume that a∗
t are nondegenerate. A pair of
2. They are weak neighbors
nondegenerate actions a, b are neighbors if Ca ∩
Cb has dimension E
Na be the set of actions
if Ca ∩
consisting of a and its neighbors (but not the duplicates of a). For any pair of neighbors (a, b) let
Naa =
Nab =
Lemma 1 (Bart´ok et al. 2014, Lem. 11). Let a and b be neighbors. Then for all d
a unique α

∅
Cb ⊆
[K] : Ca ∩
[0, 1] such that ℓd = αℓa + (1

. Actions a and b are called duplicates if ℓa = ℓb. We let

. Although a is not a neighbor of itself we deﬁne

∅
∈ Nab there exists

∈
A corollary is that for d
Cb. Degenerate
and dominated actions can never be uniquely optimal in hindsight, but they can provide information
to the learner that proves the difference between a hard and hopeless game (or easy and hard). This
is also true for duplicate actions, which have the same loss, but not necessarily the same feedback.

∈ Nab and if α from the lemma lies in (0, 1), then Cd = Ca∩

−
arg minah

, which means that a∗

Cb 6
c

Cc}

α)ℓb.

u ∈

t ∈

P

−

=

−

−

∈

{

.

Observability The neighborhood structure determines which actions can be uniquely optimal and
when. This is only half of the story. The other half is the relationship between the feedback and

3

loss matrices that deﬁnes the difﬁculty of identifying the optimal action. A natural ﬁrst attempt
towards designing an algorithm would be to construct an unbiased estimator of yta for each Pareto
optimal action a. A moments thought produces easy games where this is impossible (Exhibit 1
ytb for Pareto
in Appendix I). A more fruitful idea is to estimate the loss differences yta −
optimal actions a and b, which is sufﬁcient (and essentially necessary) to discover the optimal
action. Suppose in round t the learner has chosen to sample At ∼
PK−1).
R such that
ytb is a function g :
A conditionally unbiased estimator of yta −
Et−1[g(At, Φt)] =
ytb. Whether or not such an estimator exists and
a Ptag(a, Φait ) = yta −
its structure determines the difﬁculty of a partial monitoring game. A pair of actions (a, b) are called
globally observable if there exists a function v : [K]

Pt where Pt ∈
[F ]
[K]
→

R such that

[F ]

ri(

P

×

×

→

K

c=1
X

v(c, Φci) = ℓai −

ℓbi

for all i

[E] .

∈

(1)

They are locally observable if in addition to the above a and b are neighbors and v(c, f ) = 0
∈ Nab. Finally, they are pairwise observable if v(c, f ) = 0 whenever c /
whenever c /
. If
}
PK−1), then the existence of a function
the learner is sampling action At from distribution Pt ∈
satisfying Eq. (1) means that v(At, Φt)/PtAt is an unbiased estimator of
ytb. A
ℓb, uti
game G is called globally/locally observable if all pairs of neighbors are globally/locally observable.
A game is called point-locally observable if all pairs of weak neighbors are pairwise observable. The
cell decomposition and observability structure for the spam game is described in detail in Exhibit 2.
Note that in globally observable games it is easy to see that any pair of Pareto optimal actions are
globally observable, not just the neighbors.

∈ {
= yta −

ℓa −
h

a, b

ri(

2 Classiﬁcation theorem

The following theorem classiﬁes partial monitoring games into four categories depending on the
observability structure.

Theorem 1. The minimax regret of partial monitoring game G = (

, Φ) satisﬁes

L

0,
˜Θ(√n),
Θ(n2/3),
Ω(n),

if G has no pairs of neighboring actions ;
if G is locally observable and has neighboring actions ;
if G is globally observable, but not locally observable ;
otherwise .

R∗

n(G) = 




The theorem follows by proving upper and lower bounds for each class of games. Most of
the pieces already exist in the literature. The upper bound for globally observable games is by
Cesa-Bianchi et al. [2006]. The upper bound for games with no pairs of neighboring actions is
PE−1 and playing this action alone
trivial, since in this case there exists an action a with Ca =
ensures zero regret. The lower bound for easy games is by Antos et al. [2013,
6] and for hard
§
games by Bart´ok et al. [2014,
4]. All that remains is to prove an upper bound for locally observable
games with at least one pair of neighboring actions.

§

3 Algorithm for locally observable games

Fix a locally observable game G = (
, Φ) with at least one
L
pair of neighboring actions. We introduce a policy called
NEIGHBORHOODWATCH2 (Algorithm 1).

C6

C5

u2

C4

C2

C3

C1

Preprocessing The new algorithm always chooses its action At ∈ ∪a,bNab where the union is
over pairs of neighboring actions. For example, in the game with cell decomposition shown in the
ﬁgure the policy only plays actions 1, 2, 3 and 4. Removing (some) degenerate actions can only
increase the minimax regret so from now on we assume that all actions in [K] are in
Nab for some
neighbors a and b. Let
does
A
not contain actions that are duplicates of each other and

be an arbitrary largest subset of Pareto optimal actions such that

be the remaining actions.

= [K]

A

u1

D

\ A

4

Estimating loss differences The deﬁnition of local observability means that for each pair of
R satisfying Eq. (1) and
neighboring actions a, b there exists a function vab :
[F ]
with vab(c, f ) = 0 whenever c /
∈ Nab. Even though a is not a neighbor of itself, for notational
convenience we deﬁne vaa(c, f ) = 0 for all c and f . The policy works for any such of vab, but the
k∞ with the maximum over all pairs of neighbors.
analysis suggests minimizing V = maxa,b k
Algorithm 1 NEIGHBORHOODWATCH2
1: Input
2: for t

, Φ, η, γ
L
1, . . . , n do

[K]

vab

→

×

∈

For a, k

[K] let Qtka = 1A(k)

∈

Find distribution ˜Pt such that ˜P ⊤
Compute Pt = (1
Compute loss-difference estimators for each k

t = ˜P ⊤
P
γ)REDISTRIBUTE( ˜Pt) + γ
K

−

ˆZtka =

˜Ptkvak(At, Φt)
PtAt

and βtka = ηV 2

˜P 2
tk
Ptb

Xb∈Nak

1Nk∩A(a) exp

b∈Nk∩A exp
t Qt

η

−

t−1
s=1

˜Zska

(cid:17)

(cid:16)

P

η

t−1
s=1

˜Zska

P

−
(cid:16)
(cid:17)
1 and sample At ∼
.
∈ Nk ∩ A
∈ A

and a

Pt

+ 1D(k)

1A(a)

|A|

and

˜Ztka = ˆZtka −

βtka

(2)

3:

4:
5:
6:

7: end for
8: function REDISTRIBUTE(p)
9:
10:
11:

q
←
for d

∈ D

do

p

12:

αqb
αqb+(1−α)qa

∈ Nab and α
and cb ←
1
−
ρcaqa + ρcbqb and qa ←
(1

Find a, b such that d
ca ←
qd ←
13:
end for
14:
return q
15:
16: end function

∈
ca and ρ

[0, 1] such that ℓd = αℓa + (1
, pb
1
2K min
qbcb
←
ρca)qa and qb ←
ρcb)qb

pa
qaca

(1
n

−

−

−

o

α)ℓb

(Lemma 1)

Description In each round the algorithm ﬁrst computes a collection of exponential weights
distribution Qtk ∈ PK−1, one for each k
. The distribution Qtk is supported on the
Nk ∩ A
. These local distributions are then combined into a
when k
global distribution ˜Pt, which is taken to be the stationary distribution of right-stochastic matrix Qt,
which means that

it is uniform on

∈ A
A

and for k

∈ A

∈ D

˜Pta =

˜PtkQtka for any a, k

.

∈ A

Xk∈A

(3)

∈ A

∈ D

−
∈ D

, which by the above display means that Ptd = γ/K for actions d

These steps are the same as the original NEIGHBORHOODWATCH, which samples its action from
γ) ˜Pt + γ1/K. This does not work when there are degenerate actions because Qtkd = 0 when
(1
and non-adaptive
d
forced exploration is not sufﬁcient for O(√n) regret in partial monitoring. This is the role of the
redistribution function, which is analyzed formally in Appendix A. The ﬁnal part of the algorithm is
. Our choice of loss estimators are
and a
to estimate the loss differences for each k
another departure from the original algorithm, which only updated the estimators for one local game
in each round and then used a complicated aggregation strategy. This is one source of signiﬁcant
simpliﬁcation in the new algorithm.
Remark 1. The special treatment of degenerate actions using the redistribution function seems like
a big hassle. You might wonder why we did not simply include the degenerate actions in the local
games and then play the stationary distribution, possibly with a little exploration. Unfortunately this
Nak where a and k are neighbors. Then Lemma 1
idea does not work. Let d be a degenerate action in
shows that the loss-difference between k and d can be estimated by ˆZskd = α ˆZskk + (1
α) ˆZska
−
Nak is only useful for
with α such that ℓd = αℓk + (1
learning about the loss differences between actions a and k, which suggests the algorithm should
not assign much more probability to d than the minimum probability of playing a and k. At a
technical level the proof does not go through because the predictable variation of the estimator

α)ℓa. Intuitively, a degenerate action d in

∈ Nk ∩ A

−

5

above is roughly Ω(max(1/Ptk, 1/Pta)) and yet Ptd can be Ω(max(Ptk, Pta)) and in the analysis
of exponential weights these terms are required to cancel.
Remark 2. The estimators ˜Ztka are negatively biased by βtka in order to prove high probability
bounds, which is reminiscent of the Exp3.P algorithm for ﬁnite-armed adversarial bandits
[Auer et al., 2002]. As a minor contribution, we generalize their analysis to the case where the
loss estimators satisfy certain constraints, rather than taking the speciﬁc importance-weighted form
used for adversarial bandits. Choosing βtka = 0 in the algorithm leads to a bound on the expected
regret as we soon show.
Theorem 2. Suppose Algorithm 1 is run on locally observable G = (
(0, 1) and η = 1
δ
V
the regret is bounded by Rn ≤
G, but not the horizon n or conﬁdence level δ.

, Φ) with parameters
δ
n log(e/δ)), where CG is a constant that depends on the game

log(K/δ)/(nK) and γ = V Kη. Then with probability at least 1

CG

p

−

L

∈

p

The complete proof of Theorem 2 given in Appendix B. Here we prove a bound on the expected
regret in the simple case where there are no degenerate actions and βtka = 0. Although this
proof does not highlight one of our main contributions (how to deal with degenerate actions), it
does emphasize the enormous simpliﬁcation of the new algorithm. The ﬁrst step is a localization
argument to bound the regret in terms of the ‘local regret’ in each neighborhood. We need a simple
lemma, which for completeness we prove in the the appendix.
Lemma 2 (Bart´ok et al. 2014). There exists a constant εG > 0 depending only on G such that
for all pairs of actions a, ˜a
such that
ℓa −
h
the REDISTRIBUTE function has no effect and
Since there are no degenerate actions,
˜Pt that
γ) ˜Pt + γ1/K. Let B1, . . . , Bn be a sequence of random variables with Bt ∼
Pt = (1
−
is conditionally independent of At given the observations up to time t. Then by Hoeffding-Azuma’s
inequality

C˜a there exists an action b

∈ Na ∩ A

/εG.
i

ℓa −

and u

i ≤ h

∈ A

ℓ˜a, u

ℓb, u

∈

n

t=1
X

n

t=1
X

Rn =

ℓAt −
h

ℓa∗

n , uti ≤

nγ +

8 log(1/δ) +

ℓBt −
h

ℓa∗

n , uti

.

(4)

p
Next we apply Lemma 2 to localize the second term,

n

=

ℓa∗

(A) =

n, uti

ℓBt −
h
t=1
X
where
is the set of functions φ : [K]
Azuma’s inequality and a union bound over all φ

ℓa −
(cid:10)

Xa∈[K]

n ,
ℓa∗

→

H

P

t:Bt=a ut

1
εG

≤

max
φ∈H

ℓBt −
h

ℓφ(Bt), uti

,

[K] with φ(a)

(cid:11)
∈ Na for all a. Then using Hoeffding-

shows that with probability at least 1

δ,

n

t=1
X

∈ H

−

(A)

1
εG

1
εG

1
εG

≤

=

=

max
φ∈H

max
φ∈H

n

Xa∈[K]

t=1
X
n

t=1
X

Xk∈[K]
n

max
b∈Nk

Xk∈[K]

t=1
X

Xa∈Nk

˜Ptah

ℓa −

ℓφ(a), uti

+

8n log

s

|H|
δ

(cid:18)

(cid:19)

˜Ptk

Qtkah

ℓa −

ℓφ(k), uti

+

s

8n log

|H|
δ

(cid:18)

Xa∈Nk
Qtka( ˜Ptkyta −

local regret

˜Ptkytb)

+

8n log

s

|H|
δ

(cid:18)

(cid:19)

(5)

(cid:19)

,

|

where the ﬁrst equality uses the fact that ˜Pt is the stationary distribution of Qt (see (3)). The
local regret is bounded using the tools from online convex optimization. Of course the losses are
never actually observed and must be replaced with the loss difference estimators. Then it remains
to control the variance of these estimators. The ‘standard’ analysis of Exp3 [Auer et al., 1995,
Cesa-Bianchi and Lugosi, 2006] shows that

{z

}

max
b∈Nk

Qtka

˜Ptkyta −

˜Ptkytb

n

t=1
X

(cid:16)

n

log(K)
η

+ η

t=1
X

Xa∈Nk

Qtka ˆZ 2

tka .

(6)

≤

(cid:17)

6

In order to bound the second term we substitute the deﬁnition of ˆZtka, which shows that

Qtka ˆZ 2

tka =

˜P 2
tkQtkavak(At, Φt)2
P 2

tAt

˜PtkV 2
PtAt

≤

˜PtkQtka1

{a,k}(At)

PtAt

2 ˜PtkV 2
PtAt

.

≤

Xa∈Nk

Xa∈Nk
where in the ﬁrst inequality we used the fact that
by considering two cases. First,
if At = k,
˜PtkQtka = ˜Ptk ≤

Xa∈Nk
k∞ ≤
2PtAt , which is true by choosing γ
≤
for a
P
˜Pta ≤
of ˜Pt as the stationary distribution of Qt means that ˜PtkQtka ≤
Eqs. (4) to (6) and a union bound shows that with probability at least 1

V . The second inequality follows
k
then all entries of the sum are non-zero and
1/2. For the second case At = a
= k, which means that only one term of the sum is non-zero. Then the deﬁnition
2PtAt . Combining this with

∈ Nk and a

vak

a∈Nk

δ.

+

8n log(2/δ) +

8n log

−

s

2

|H|
δ

.

(cid:19)

(cid:18)

Rn ≤

nγ +

1
εG  

K log(K)
η

+ 2ηV 2

n

1
PtAt !

t=1
X
] = nK, which means that

p

Now E[

E[Rn]

n

t=1 P −1
tAt
1
εG (cid:18)

nγ +

P
≤

K log(K)
η

+ 2ηnKV 2

+ 2

8n(1 + log(2

)) = O

n log(K)

,

|H|

KV
εG

(cid:19)

p

(cid:19)
K K.
where we ﬁrst used Lemma 3 below along with naive bounding and the fact that
The Big-O follows by choosing η = 1
log(K)/n and γ = ηKV . The choice of γ ensures
V
ˆZtka| ≤
ηV K/γ = 1 on which the proof of
that the loss-difference estimate satisﬁes η
|
p
Eq. (6) relies. We prove in Appendix G that for games without degenerate actions the loss-difference
estimators can always be chosen so that V
1 + F .
Lemma 3. Suppose a
P(X
Y +

1 are constants and X, Y are random variables such that
E[Y ] +

(0, 1). Then E[X]

ηV /PtAt ≤

≥
a log(b/δ))

a(1 + log(b)).

δ for all δ

0 and b

|H| ≤

p

≥

≤

(cid:18)

≤

∈

≤

Dealing with degenerate actions The presence of degenerate actions makes the calculation
signiﬁcantly more ﬁddly. The ﬁrst step is to show that the redistribution process guarantees that the
expected loss accumulated by playing Pt rather than ˜Pt is not too great. The localization argument
is then repeated and the remaining question is how to control the variance of the loss difference
estimates. The redistribution process guarantees that the degenerate actions have sufﬁcient mass
that the variance is at most O(K) larger than what we saw in the above calculation. The process is
complicated slightly by the desire to have a high probability bound.

p

≥

p

4 Algorithm for point-locally observable games

The weakened neighbor deﬁnition and pairwise observability makes the analysis of point-locally
observable games less delicate than locally observable games and the results are correspondingly
stronger. Perhaps the most striking improvement is that asymptotically the bound does not dependent
on arbitrarily large game-dependent constants. Here we present a simple new algorithm based on
EXP3 called RELEXP3 (‘Relative Exp3’). The name is derived from the fact that the algorithm does
not estimate losses directly, but rather the loss differences relative to an ‘anchor’ arm that varies
over time and is the arm to which the algorithm assigns the largest probability. As we shall see, this
reduces the variance of the loss difference estimates.

Preprocessing The deﬁnition of pairwise observability means that degenerate and dominated
actions are not needed to estimate the loss differences. Since removing these actions can only
increase the minimax regret, for the remainder of this section we ﬁx a point-locally observable game
, Φ) for which there are no dominated or degenerate actions. A point-local game is a largest
G = (
(a maximal clique of the graph over actions with
subset of actions A
=
edges representing weak neighbors). We let Kloc be the size of the largest point-local game.

a∈A Ca 6

[K] with

⊆

L

∅

T

Estimation functions For each pair of actions a, b let vab be an estimation function satisfying
Eq. (1) and furthermore assume that vaa = 0 and vab(c, f ) = 0 if a, b are weak neighbors and c /
∈
. The existence of these functions is guaranteed by the deﬁnition of a point-locally observable
a, b
{
game. Given pair of actions a, b let Sab be the set of actions needed to estimate the loss difference

}

7

between a and b, which is Sab =
.
}
{
∈
Our assumptions ensure that Sab =
k∞
and V = maxa,b∈[K] V ab and Vloc = maxa,b:Ca∩Cb6=∅ V ab. We show in Appendix G that vab can
be chosen so that Vloc

∈
if a and b are weak neighbors. Deﬁne V ab =

[F ] such that vab(c, f )

[K] : exists f

} ∪ {
}

= 0
vab

1 + F .

a, b

a, b

{

k

c

≤

t=1 and exploration parameters (αt)∞

Decreasing learning rates The algorithm makes use of a sequence of decreasing learning rates
(ηt)∞
t=1. On top of this the algorithm also has a dynamic
exploration component that ensures the loss difference estimates are not too large. The decreasing
learning rate is one of the essential innovations that allows us to prove an asymptotic bound that is
independent of arbitrarily large game-dependent quantities. As an added bonus, it also means the
algorithm does not require advance knowledge of the horizon.

Algorithm 2 RELEXP3
1: ˆL0a = 0 for all a
∈
2: for t = 1, . . . , n do

[K]

For each a

[K] let ˜Pta =

∈

exp(
−
K
b=1 exp(

ηt ˆLt−1,a)

ηt ˆLt−1,b)
−
a : ˜Pta exp

Let Bt = arg maxa

˜Pta and Mt =
P

Let St =

[a∈Mt
Sample At ∼
For each a
∈

SaBt and γta = 1St(a)ηt max
a∈Mt

(cid:26)

Pt and observe feedback Φt
[K] compute estimates ˆZta =

ηtV aBt

(cid:18)
V aBt +

αt (cid:19)
αt
K

>

ηt
t
and Pt = (1

(cid:27)

γtk1) ˜Pta+γt

−k

vaBt (At, Φt)
PtAt

and update ˆLta = ˆLt−1,a + ˆZta

3:

4:

5:

6:

7:

8: end for

L

E[Rn]/√n

Theorem 3. Let G = (
RELEXP3 satisﬁes lim supn→∞
linear dependence on F is unavoidable (see Appendix E).

, Φ) be point-locally observable, then with appropriately tuned parameters
2Kloc(1 + F )(2 + F ) log(K). Furthermore, the

p
Note that the constant hidden by the asymptotics does depend on arbitrarily large game-dependent
constants. The proof of Theorem 3 may be found in the Appendix C, but the general idea is to show
the forced exploration ensures for sufﬁciently large t that the algorithm is almost always playing in
a point-local game that contains the optimal action and at this point the variance of the importance-
weighted estimators is well behaved.

≤

8

5 Summary and open problems

We completed the classiﬁcation of all ﬁnite partial monitoring games. Along the way we greatly
simpliﬁed existing algorithms and analysis and proved that for a large class of games the asymptotic
regret does not depend on arbitrarily large game-dependent constants, which is the ﬁrst time this
has been demonstrated in the adversarial setting. There are many fascinating open problems. One
of the most interesting is to understand to what extent it is possible to adapt to ‘easy data’. For
example, globally observable games may have locally observable subgames and one might hope
for an algorithm with O(√n) regret if the adversary is playing in this subgame and O(n2/3)
regret otherwise. Another question is to reﬁne the deﬁnition of the regret to differentiate between
algorithms in hopeless games where linear regret is unavoidable, but the coefﬁcient can depend
on the algorithm [Rustichini, 1999]. Yet another question is to understand to what extent V is a
fundamental quantity in the regret for easy games and whether or not the arbitrarily large game-
dependent constants are real for large n as we have shown they are not for point-locally observable
games.

8

References

Andr´as Antos, G´abor Bart´ok, D´avid P´al, and Csaba Szepesv´ari. Toward a classiﬁcation of ﬁnite

partial-monitoring games. Theoretical Computer Science, 473:77–99, 2013.

Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged
casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995.
Proceedings., 36th Annual Symposium on, pages 322–331. IEEE, 1995.

Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic

multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002.

G´abor Bart´ok. A near-optimal algorithm for ﬁnite partial-monitoring games against adversarial

opponents. In Conference on Learning Theory, pages 696–710, 2013.

G´abor Bart´ok, Dean P Foster, D´avid P´al, Alexander Rakhlin, and Csaba Szepesv´ari. Partial
monitoring—classiﬁcation, regret bounds, and algorithms. Mathematics of Operations Research,
39(4):967–997, 2014.

Nicolo Cesa-Bianchi and G´abor Lugosi. Prediction, learning, and games. Cambridge university

press, 2006.

Nicolo Cesa-Bianchi, G´abor Lugosi, and Gilles Stoltz.

Regret minimization under partial

monitoring. Mathematics of Operations Research, 31:562–580, 2006.

Dean Foster and Alexander Rakhlin. No internal regret via neighborhood watch.

In Artiﬁcial

Intelligence and Statistics, pages 382–390, 2012.

David A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3(1):100–118,

02 1975.

Elad Hazan. Introduction to online convex optimization. Foundations and Trends R
(cid:13)

2(3-4):157–325, 2016.

in Optimization,

Shie Mannor and Nahum Shimkin. On-line learning with imperfect monitoring. In Learning Theory

and Kernel Machines, pages 552–566. Springer, 2003.

Shie Mannor, Vianney Perchet, and Gilles Stoltz. Set-valued approachability and online learning
with partial monitoring. The Journal of Machine Learning Research, 15(1):3247–3295, 2014.

Vianney Perchet. Approachability of convex sets in games with partial monitoring. Journal of

Optimization Theory and Applications, 149(3):665–677, 2011.

Antonio Piccolboni and Christian Schindelhauer. Discrete prediction games with arbitrary feedback

and loss. In Computational Learning Theory, pages 208–223. Springer, 2001.

Aldo Rustichini. Minimizing regret: The general case. Games and Economic Behavior, 29(1):

Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business

224–243, 1999.

Media, 2008.

Andrew Chi-Chin Yao. Probabilistic computations: Toward a uniﬁed measure of complexity. In

18th Annual Symposium on Foundations of Computer Science, pages 222–227, 1977.

A Redistribution properties

Here we collect a number of properties of the REDISTRIBUTE function in Algorithm 1.

9

Lemma 4. Assume γ
Pt ∈ PK−1 is a probability vector and the following hold:

[0, 1/2] and let u

∈ PE−1, and k, a

∈

arbitrary neighbors. Then

(a) Pta ≥

˜Pta/4 ;

˜PtkQtka
4K

(c) Ptb ≥

(e) Ptd ≥

˜Ptk
4K

for any non-duplicate b

∈ Nka ;

γ/K ;

for any d

[K] such that ℓd = ℓk .

∈

∈ A

K

(b)

(cid:12)
a=1
(cid:12)
X
(cid:12)
(cid:12)
(d) Pta ≥
(cid:12)

(Pta −

˜Pta)
ℓa, u
h

γ ;

≤

i(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Proof. First we show that Pt is indeed a probability vector. By assumption ˜Pt is the stationary
distribution, which is a probability distribution. Let ¯Pt = REDISTRIBUTE( ˜Pt) so that

Pt = (1

γ) ¯Pt +

−

γ
K

1 ,

which means we need to show that ¯Pt is a probability distribution. Since ¯Pt is obtained by the
iterative procedure given in the REDISTRIBUTE function it is sufﬁcient to show that the vector q
tracked by this algorithm is indeed a distribution. The claim is that each loop of the REDISTRIBUTE
function does not break this property. The ﬁrst observation is that the algorithm always moves mass
. To see this
from actions in
A
note ﬁrst that if a
pa/(2K) and
so

. All that must be shown is that ¯Pta ≥
∈ A
is one of the choices of the algorithm in Line 11, then ρcaqa ≤

to actions in

0 for all a

∈ A

D

Part (a): Since γ

˜Pta/2

¯Pta ≥
1/2 this follows from Eq. (7).
a∈[K]( ¯Pta −

≤

P

Part (b): First we show that
each inner loop of the algorithm does not change this value, which is true because

˜Pta)ℓa = 0. It sufﬁces to show that the redistribution in

for all a

0 .

∈ A ≥

(7)

(caqa + cbqb)ℓd = (caqa + cbqb)(αℓa + (1

α)ℓb)

qaqb

=

αqb + (1

α)qa

−

= ρcaqaℓa + ρcbqbℓb .

−
(αℓa + (1

α)ℓb)

−

Then using the deﬁnition of Pt we have

(Pta −

˜Pta)
ℓa, u
h

=

(Pta −

¯Pta)
ℓa, u
h

= γ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
Xa∈[K]
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)
Xa∈[K]
∈ PE−1 so that
where we used the assumption that ℓa ∈
Part (c): There are three cases: Either b = k or b = a or b is degenerate.
the result is immediate from Part (a).
˜PtkQtka/4
˜Pta/4
Ptb = Pta ≥
≥
deﬁnition of the rebalancing algorithm we have

i(cid:12)
(cid:12)
(cid:12)
(cid:12)
ℓa, u
(cid:12)
h
i ∈
(cid:12)
If b = k, then
If b = a, then, Part (a) combined with (3) implies that
˜PtkQtka/(4K). Finally, if b is degenerate, then by the

[0, 1]E for all actions and u

Xa∈[K] (cid:18)

i(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

[0, 1].

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

≥

γ ,

1
K −

¯Pta

ℓa, u
h

¯Ptb ≥

min( ˜Ptk, ˜Pta)
2K

≥

min( ˜Ptk, ˜PtkQtka)
2K

=

˜PtkQtka
2K

and the result follows from Eq. (7).

Part (d): This is trivial from the deﬁnition of Pt.
Part (e): Let b
be the Pareto optimal action chosen by the rebalancing algorithm when d is
given weight. Since ℓd = ℓa it follows that α = 1 and so ca = 1 and cb = 1, which means that
¯Ptd = ˜Pta/2 and using Eq. (7) again yields the result.

∈ A

B Proof of Theorem 2

We start by proving Lemma 2.

10

ℓa −
h
u
v

= k
k

i

i
w
k2
w
k2 h

−
−

ℓ˜a, u

ℓa−

ℓa −
h

C˜a, 0
∈
= 0. From now on assume that
i

Proof of Lemma 2. Since u
. The result is trivial if a, ˜a
i
≤ h
are neighbors or
> 0
ℓa −
ℓ˜a, u
h
and that a, ˜a are not neighbors. Let v be the centroid of Ca and consider the
line segment connecting v and u. Then let w be the ﬁrst point on this line
segment for where there exists a b
Cb (see ﬁgure).
Note that w is well-deﬁned by the Jordan-Brouwer separation theorem and
b is well-deﬁned because
is a maximal duplicate-free subset of the Pareto
optimal actions. Using twice that

= 0, we calculate

∈ Na ∩ A

with w

ℓ˜a, u

A

∈

i

ℓb, w

Ca

v

w

u

Cb

C˜a

ℓa −
h

ℓb, u

=

i

ℓa −
h

ℓb, u

w

−

ℓa −

ℓb, w

v

i

−

u
= k
v
k

w
k2
w
k2 h

−
−

ℓb −

ℓa, v

> 0 ,

i

(8)

v and u

where the second equality used that w
= v is a point of the line segment connecting v and u, hence
k2 > 0. The last inequality
w
follows because v is the centroid of Ca and a, b are distinct Pareto optimal actions. Let vc be the
centroid of Cc for any c

w are parallel and share the same direction and

. Then,

−

−

−

w

k

v

ℓa −
h
ℓa −
h

ℓ˜a, u
ℓb, u

i
i

i

w

w

(a)

(d)

= h

≤
v

(b)
= h

(c)
= k

ℓa −
h

∈ A
ℓa −
ℓ˜a, w + u
−
ℓb, u
ℓa −
i
h
w
ℓ˜a, u
ℓa −
i
ℓa −
h
w
k2 k
−
ℓb −
h
ℓa −
ℓb, u
h
i
ℓa −
ℓb, w
h
i
v
k2 ≤
−
ℓd −
minc∈A mind∈Nch
ℓa, vai ≥

−
ℓb, u
i
ℓa −
ℓa, v
i

−
u
k
(e)

ℓ˜ak2

≤

≤

k

v

ℓ˜a, u

w

i

−

ℓb, w

k2 h
w

+
ℓa −
i
h
ℓb, u
ℓa −
i
h
ℓa −
w
ℓ˜a, u
i
k2 h

ℓb −

−
ℓa, v
i
√2E
minc∈A mind∈Nch

−

ℓc, vci
ℓd −
Cb implies that

,

=

1
εG
ℓa −
h
ℓ˜ak2 ≤

> 0 and also because w
i ≤
= 0 (which is used in other steps, too), (c) uses (8), (d) is
√E and
ℓa −
k
> 0. The ﬁnal equality serves as the

∈
√2 and used that

ℓ˜a, w

w

k

ℓc, vci

∈ Na for all a
. Then, for any (Bt)1≤t≤n sequence of actions in

with φ(a)

A → A

and deﬁne

∈ A
,
A

ℓb, w

where (a) follows since by (8),
, (b) follows since
ℓa −
h
i
by Cauchy-Schwartz and in (e) we bounded
ℓb −
ℓa, v
ℓb −
h
h
deﬁnition of 1/εG.
Lemma 5. Let
H
a∗
n = arg mina∈[K]

be the set of functions φ :
ℓa, uti

=

i

n
t=1h
n

P

t=1
X

ℓBt −
h

ℓa∗

n , uti ≤

ℓBt −
h

ℓφ(Bt), uti

.

1
εG

max
φ∈H

n

t=1
X

Proof. With no loss of generality we assume that a∗
is a maximal duplicate-free
subset of Pareto optimal actions. Apply the previous lemma on subsequences of rounds where
Bt = a for each a

n ∈ A

because

A

.
∈ A
(0, 1). Then with probability at least 1

Lemma 6. Let δ

∈

2δ it holds that

−

Rn ≤

γn +

1
εG

n

˜Ptk

max
b∈Nk∩A

Xk∈A

t=1
X

Xa∈A

Qtka (yta −

ytb) +

8n log(

/δ) .

|H|

p

˜Pt. Deﬁne the surrogate regret R′
Proof. For t
deﬁnition of At and Bt and part (b) of Lemma 4 we have Et−1[
ℓa −
|h

. By the
[n], let Bt ∼
ℓa∗
n, uti
γ. Furthermore,
1 for all a, b. Therefore, by Hoeffding-Azuma, with probability at least 1

n
ℓBt −
t=1h
]
ℓBt, uti
≤

∈
ℓb, uti| ≤

n =
ℓAt −
h
P

−

δ,

Rn ≤
By Lemma 5, the surrogate regret is bounded in terms of the local regret:

2n log(1/δ) .

n + γn +

p

R′

(9)

(10)

R′

n =

ℓBt −
h

ℓa∗

n, uti ≤

ℓBt −
h

ℓφ(Bt), uti

.

n

t=1
X

1
εG

max
φ∈H

n

t=1
X

11

We prepare to use Hoeffding-Azuma again. Fix φ

arbitrarily. Then,

Et−1

ℓBt −
h

(cid:2)

ℓφ(Bt), uti
(cid:3)

∈ H
˜Ptk

˜Ptk

=

=

Xk∈A

Xk∈A

Xa∈A

Xa∈A

Qtkah

ℓa −

ℓφ(k), uti

Qtka(yta −

ytφ(k)) ,

(11)

˜PtkQtka. Hoeffding-Azuma’s inequality now shows that

where we used the fact that ˜Pta =
,
with probability at least 1

δ/

−

|H|

k

P
n

n

t=1
X

ℓBt −
h

ℓφ(Bt), uti ≤

˜Ptk

Qtka(yta −

ytφ(k)) +

2n log(

/δ) .

|H|

Xk∈A

t=1
X

Xa∈A

p

The result is completed via a union bound over all φ
noting that

∈ H

and chaining with Eqs. (9) and (10), and

n

˜Ptk

max
φ

Xk∈A

t=1
X

Xa∈A

Qtka(yta −

ytφ(k))

max
φ

n

˜Ptk

t=1
X

n

max
b∈Nk∩A

t=1
X

≤

=

Xk∈A

Xk∈A

Xa∈A
˜Ptk

Qtka(yta −

ytφ(k))

Qtka(yta −

ytb)

.

Xa∈A
Rnk

{z

Proof of Theorem 2. The proof has two steps. First bounding the local regret Rnk for each k
and then merging the bounds using the previous lemma.

∈ A

|

}

Step 1: Bounding the local regret For the remainder of this step we ﬁx k
and bound the local
regret Rnk. First, we need to massage the local regret into a form in which we can apply Theorem 6,
which is a generic version of the Exp3.P analysis by Auer et al. [2002]. Let Ztka = ˜Ptk(yta −
ytk)
t=0 be the associated ﬁltration. A
and
simple rewriting shows that

Gt be the σ-algebra generated by (A1, . . . , At) and

Gt)n

∈ A

= (

G

Rnk = max

b∈Nk∩A

Qtka (yta −

ytb) = max

b∈Nk∩A

Qtka (Ztka −

Ztkb) .

n

˜Ptk

t=1
X

Xa∈A

n

t=1
X

Xa∈A

G

-predictable it follows that (βt)t and (Zt)t are also

In order to apply the result in Theorem 6 we need to check the conditions. Since (Pt)t and ( ˜Pt)t
are
-
G
-adapted. It remains to show that assumptions (a–d) are
adapted because (At)t and (Φt)t are
satisﬁed. For (a) let a
[K].
∈
ˆZtka|
ηV K/γ = 1,
Furthermore,
|
|
|
where the equality follows from the choice of γ. Assumption (b) is satisﬁed in a similar way with
ηβtka = η2V 2
η2K 2V 2/γ = ηKV
1, where in the last inequality we used
the deﬁnition of η and assumed that n
K log(K/δ). To make sure that the regret bound holds
log(eK) so that when n < K 2 log(K/δ), the
even for smaller values of n, we require CG ≥
K
regret bound is trivial. For assumption (c), we have

. By Lemma 4.(d) we have Ptb ≥
=

∈ Nk ∩ A
| ≤
˜P 2
tk/Ptb ≤

η ˜Ptkvak(At, Φt)/PtAt | ≤

-predictable. Similarly, ( ˆZt)t is

γ/K for all t and b

vak(At, Φt)

V so that η

b∈Nak

P

≤

≥

G

G

˜Ptkvak(At, Φt)
PtAt

2



≤

!

V 2 ˜P 2
tk

Et−1

1Nak (At)
P 2

tAt

= V 2

Et−1[ ˆZ 2

tka] = Et−1 


 

(cid:21)
Finally (d) is satisﬁed by the deﬁnition of vak and the fact that Pt ∈
Theorem 6 shows that with probability at least 1

(K + 1)δ,



(cid:20)

˜P 2
tk
Ptb

=

βtka
η

.

Xb∈Nak

ri(

PK−1). The result of

Rnk ≤

3 log(1/δ)
η

+ 5

n

n

Qtkaβtka + η

Qtka ˆZ 2

tka .

t=1
X

Xa∈Nk∩A

t=1
X

Xa∈Nk∩A

p

−

12

Step 2: Aggregating the local regret Using the result from the previous step in combination with
a union bound over k

we have that with probability at least 1

K(K + 1)δ,

∈ A

Rnk ≤

3K log(1/δ)
η

+ 5

n

−
n

Qtkaβtka + η

Qtka ˆZ 2

tka .

(12)

Xk∈A

t=1
X

Xk∈A Xa∈Nk∩A

t=1
X

Xk∈A Xa∈Na∩A

For bounding the second term we use the deﬁnition of βtka from (2) and write

Xa∈Nk∩A

The sum over b

˜P 2
tk
Ptb

Qtkaβtka = ηV 2

Qtka

= ηV 2 ˜Ptk

Qtka

Xa∈Nk∩A

Xb∈Nak
∈ Nak is split into two, separating duplicates of k and the rest:
Qtka
Qtka

Xa∈Nk∩A

Qtka

=

+

˜Ptk
Ptb

Xb∈Nak

˜Ptk
Ptb

.

˜Ptk
Ptb

Xa∈Nk∩A

Xb∈Nak

Xa∈Nk∩A

Xb:ℓb=ℓk

Xa∈Nk∩A

Xb∈Nak:ℓb6=ℓk

˜Ptk
Ptb

=

˜Ptk
Ptb

+

Qtka ˜Ptk
Ptb

Xb:ℓb=ℓk

Xa∈Nk∩A Xb∈Nak:ℓb6=ℓk

1 +

4K

≤





Xb:ℓb=ℓk

Xa∈Nk∩A Xb∈Nak:ℓb6=ℓk

1



≤

4K 2 ,



a Qtka = 1, the second to last inequality follows using parts (c)
where the ﬁrst equality used that
and (e) of Lemma 4, and the last inequality uses the reasoning above. Summing over all rounds and
k

yields

P

∈ A

n

5

t=1
X

Xk∈A Xa∈Nk∩A

Qtkaβtka ≤

20ηnK 2V 2 .

For the last term in Eq. (12) we use the deﬁnition of ˆZtka and Lemma 4.(c) to show that
n

n

η

Qtka ˆZ 2

tka = η

t=1
X

Xk∈A Xa∈Nk∩A

Qtka ˜P 2

tkvak(At, Φt)2

P 2

tAt

Qtka ˜Ptk1Nak (At)
PtAt

Xa∈Nk∩A

Now, from Lemma 4 (d), γ/K (1/Pta)
Furthermore, Et−1[1/PtAt] = K and Et−1[1/P 2
Lemma 10 it holds that with probability at least 1

≤

1 for all a, and in particular, holds for a = At.
K 2/γ. By the result in

a 1/Pta ≤

t=1
X
ηV 2

Xk∈A Xa∈Nk∩A
n
1
PtAt

˜Ptk

t=1
X
4ηKV 2

n

Xk∈A
1
PtAt

.

≤

≤

t=1
X

tAt] =
δ that
−

2nK +

P
K log(1/δ)
γ

.

n

1

PtAt ≤

t=1
X

Another union bound shows that with probability at least 1

(1 + K(K + 1))δ,

Rnk ≤

3K log(1/δ)
η

Xk∈A

−

+ 28ηnV 2K 2 + 4V K log(1/δ) .

The result follows from the deﬁnition of η, Lemma 6 and the deﬁnition of Rnk.

C Proof of Theorem 3

Before the proof we need a simple lemma showing that if actions a and b are not weak neighbours,
then the regret of either a or b grows linearly in t.

13

Lemma 7. There exists a game-dependent constant εG > 0 such that:

2ℓa∗

u, u

εG.

i ≥

ℓa + ℓb −
Mu| ≤
it follows that

|

(a) If a and b are not weak neighbours, then inf u∈PE−1 h
(b) If u

{

a :

ℓa∗

u, u

Kloc.

, then

∈
2ℓc, u

i
> 0. For (b), let a, b

∈ PE−1 and Mu =

ℓa −
h
i
[K] be arbitrary. Since Ca ∩

∅
i
> 0. Taking the minimum over all c shows that 2εG = inf u∈PE−1h

< εG}
Proof. For (a) let c
> 0
Cb =
Cc. By compactness of Cc and the continuity of the inner product in u we conclude that
for all u
∈
ℓa +
ℓa + ℓb −
inf u∈Cch
< 2εG, which by (a) means
ℓa + ℓb −
u, u
2ℓa∗
ℓb −
h
i
that a and b are weak neighbours. Therefore all actions in Mu are weak neighbours of each other so
Kloc.
Mu| ≤
The next lemma uses the concentration of the loss estimators to show that with high probability the
distribution ˜Pt calculated by RELEXP3 assigns negligible probability to actions that are either not
neighbours of Bt or for which the loss is large relative to the optimal action.
Lemma 8. Let Zta =
P (FAIL)

t
s=1 Zsa. Then there exists an event FAIL with

ℓa −
h
1/n and function g : N

) such that if FAIL does not hold, then

and Lta =
[0,

ℓa + ℓb −
h

Mu. Then

2ℓc, u

u, u

2ℓa∗

∈

i

|

ℓBt, uti
→

P

ηtg(t)) for all a that are not neighbours of Bt.

∞

−

exp(

≤
(a) ˜Pta ≤
(b) ˜Pta ≤
−
(c) There exist constants c1, c2 ≥
deﬁnition of αt such that for all t

ηtg(t)) for all a with

exp(

ℓa −
h

ℓa∗

t , ¯uti ≥

εG.

0 depending on G = (

, Φ) and the choice of ε in the

c1 logc2 (n) it holds that g(t)

L

1
2 εGt.

≥

≥

Proof. Deﬁne random variable φt = maxa,b |
pair of arms (a, b), from the triangle inequality we have

t

s=1( ˆZsb −

Zsb + Zsa −

ˆZsa)
|

. Given an arbitrary

(cid:12)
(cid:12)
(cid:12)

ˆLta −

ˆLtb

=

ˆZsa −

ˆZsb

t

(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
s=1 (cid:16)
(cid:12)
X
(cid:12)
(cid:12)
(cid:12)
Lta −
|

P

t

(Zsa −

Zsb)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Zsa + Zsb −

− (cid:12)
(cid:12)
(cid:12)
(cid:12)
ˆZsb
(cid:12)

s=1 (cid:16)
X

≥ (cid:12)
s=1
(cid:12)
X
(cid:12)
(cid:12)
ˆZsa −
(cid:12)

(cid:17)
t

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
s=1 (cid:16)
X

t

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ltb| − (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆZsa −

Zsa + Zsb −

ˆZsb

Lta −

Ltb| −

≥ |

φt .

(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)

The quantity φt is bounded with high probability via a union bound over all pairs of arms and
a martingale version of Bernstein’s bound [Freedman, 1975], which shows there exists a game-
dependent constant CG > 0 such that

P

exists t

[n] : φt ≥

∈

CGt3/4+ε/2 log

1
2 (n)

(cid:0)

FAIL

1

1
n

.

≤

(cid:1)

}

{z

|
−

0, (t

CGt3/4+ε/2 log

2 (n)
}

1)εG −

Choose g(t) = max
, which clearly satisﬁes the condition in
{
(c). First suppose that a is not a weak neighbour of Bt+1, which by the deﬁnition of Bt+1, φt and
Lemma 7 ensures that
ˆLta + ˆLtBt+1 −
ˆLtBt+1 ≥
ˆLta −
On the other hand if
t , ¯uti ≥
ℓa −
ℓa∗
h
ˆLta −
ˆLtBt+1 ≥
ˆLta −
The result follows from the fact that ˜Pta ≤
Proof of Theorem 3. Choose ε

ˆLta∗
Lta −
Lta∗
t −
t ≥
exp(ηt( ˆLt−1,Bt −

tεG −
2φt ≥
ˆLt−1,a)) for any action.

Lta + LtBt+1 −

(0, 1/2) and

2φt ≥

tεG −

εG, then

2 ˆLta∗

2Lta∗

2φt .

2φt .

t ≥

t −

∈

ηt = min

1
4KV

,

1
2Vloc s

log(K)
2tKloc 


and

αt = min

, t−1/2−ε

.

1
4K

(cid:26)

(cid:27)








14

First note that the choices of ηt and αt ensures that
distribution and Pta ≥
Et−1[ ˆZta] = Zta we have for p = ea∗

1/2 and so Pt is indeed a probability
˜Pta/2 for all t and a. Let Nt be the set of weak neighbours of Bt. Since

γtk1 ≤
k

n that

E[Rn] = E

ZtAt −

Zta∗

n

#

= E

˜Pt −
h

p, Zti#

+ E

γt − k
h

γtk1 ˜Pt, Zti#

n

"

t=1
X

n

"

t=1
X
n

n

"

t=1
X
n

"

t=1
X

≤

E

"

˜Pt −
h
t=1
X

p, ˆZti

#

+ 2E

γtk1

k

#

,

˜Rn

}
where in the inequality we used Cauchy-Schwartz and the fact that
exponential weights given in Theorem 2.3 of the book by Cesa-Bianchi and Lugosi [2006] yields

Ztk∞ ≤
k

1. The analysis of

{z

|

E[ ˜Rn]

log(K)

≤

2
ηn

+

1
η1 (cid:19)

+

(cid:18)

n

K

E

t=1
X

"

a=1
X

˜Pta ˆZta +

log

˜Pta exp

ηt ˆZta

−

(cid:16)

.

!

#

(cid:17)

1
ηt

K

 

a=1
X
(A)t

}
Note that we have stopped the proof before the application of Hoeffding’s lemma, which is not
appropriate for bandits due to the large range of the loss estimates. Suppose that a
Mt, then for
any b

ηtV aBt , which means that

SaBt

{z

∈

|

∈

⊆

ηtvaBt (At, Φt)
PtAt

≤

ηtV aBt 1

SaBt (At)

PtAt

1 .

≤

Then using exp(x)

1 leads to

γtb ≥

St we have Ptb ≥
ˆZta|

=

ηt|
1 + x + x2 for x

≤

≤
ηt ˆZta

˜Pta exp

˜Pta −
Mt then by the deﬁnitions of Mt, ˆZta and Pt,

ηt ˜Pta ˆZta + η2
t

−

≤

(cid:17)

(cid:16)

˜Pta ˆZ 2

ta .

On the other hand, if a /
∈

˜Pta exp

ηt ˆZta

˜Pta exp

−

(cid:16)

≤

(cid:17)
1 + x

which by the fact that x
log(1 + x)

x,

≤

≤

ηt|

ˆZta|

(cid:16)

≤

(cid:17)

˜Pta exp

ηtV aBt

(cid:18)

αt (cid:19)

≤

ηt
t

,

exp(x) for all x also implies that ˜Pta ˆZta ≤

≤

1/t. Using

(A)t =

˜Pta ˆZta +

log

˜Pta exp

ηt ˆZta

1
ηt

1
ηt

K

 

 

a=1
X
ηtK
t

−

(cid:16)

ηt

+ 1

−

!

(cid:17)
˜Pta ˆZta + η2
t

Xa∈Mt

˜Pta ˆZ 2
ta

!

Xa∈Mt

K

a=1
X
K

≤

≤

a=1
X
2K
t

˜Pta ˆZta +

log

+ ηt

˜Pta ˆZ 2

ta ,

Xa∈Mt

Next we bound the conditional second moment of ˆZta. If a and Bt are weak neighbours, then

˜PtaEt−1[ ˆZ 2

ta] = ˜PtaEt−1

vaBt (At, Φt)2
P 2
1
At ∈ {
{
P 2

tAt

(cid:20)
Et−1

(cid:21)
a, Bt}}

˜PtaV 2
loc

≤

where in the last
Et−1[1

At ∈ {
{

line we used the fact
] = 2 and PtAt ≥
(1

a, Bt}}

2V 2

loc + o(1) ,

≤

(cid:21)

tAt

(cid:20)
that PtAt ≥
− k

γtk1) ˜PtAt = ˜PtAt (1

−

Pta whenever At ∈ {

and
o(1)). On the other hand, if

a, Bt}

15

a and Bt are not weak neighbours, then Et−1[ ˆZ 2
ta]

K 2V 2/αt and so

≤

E [(A)t]

n

t=1
X

n

n

2K
t

+

ηtE

t=1
X

"

Xa∈Mt∩Nt

#

˜Pta ˆZ 2
ta

+

ηtE

n

t=1
X

≤

≤

t=1
X

2V 2
loc

n

t=1
X

ηtE [

Mt|

|

] + K 2V 2

t


Xa∈Mt∩N c

˜Pta


+ o(√n) .

E

ηt
αt

n

t=1
X





Xa∈N c

t

˜Pta ˆZ 2

ta



(13)

The second sum is bounded using parts (a) and (c) of Lemma 8, which shows that

E

ηt
αt

n

t=1
X





Xa∈N c

t

≤

˜Pta


P (FAIL)

n

n

ηt
αt

+

ηt
αt

t=1
X

t=1
X

Xa∈N c

t

Suppose that FAIL does not hold and deﬁne t0 by

exp (

ηtg(t)) = o(√n) .

(14)

−

t0 = min

t : for all s

t, exp

ηsg(s)

(cid:26)

≥

ηsV
αs −

(cid:18)

ηs
s

,

(cid:27)

≤

(cid:19)

≥
ηtV aBt

which by part (c) of Lemma 8 and rearrangement satisﬁes t0 = O(polylog(n)). The deﬁnition of
t0 ensures that if t
ℓa −
h

t0 and a is an action with

t−1 , ¯ut−1i

> εG, then

ℓa∗

˜Pta exp

exp

ηtV aBt

αt −

ηtg(t)

αt (cid:19)

≤

(cid:18)

(cid:19)
Mt. Therefore when FAIL does not hold and t

(cid:18)

t0,

≥

and so a /
∈

exp

≤

(cid:18)

ηtV
αt −

ηtg(t)

ηt
t

≤

(cid:19)

ℓa −
h
But by Lemma 7 the number of arms in this set is at most Kloc and so in this case
Since

K regardless of t or the failure event,

t−1, ¯ut−1i ≤

Mt ⊆ {

εG}

ℓa∗

a :

.

Mt| ≤

|

Kloc.

Mt| ≤

|
n

ηtE[

]

Mt|

|

≤

P (FAIL)

t=1
X

t=t0+1
X
Combining the above display with Eqs. (13) and (14) shows that

t=1
X

t=1
X

n

t0

n

ηtK +

ηtK + Kloc

ηt = Kloc

ηt + o(√n) .

n

t=1
X

E[(A)t] = 2V 2

locKloc

ηt + o(√n) .

n

n

t=1
X

Next we bound the sum of the expectations of
only neighbours of Bt, then St = Mt and maxa∈Mt V aBt
maxa∈Mt V aBt + o(
αt = o(
above shows that
p

γtk1 = ηt|

1/t) means that

St|

k

p

t=1
X
γtk1. To begin notice that if Mt contains
k
Vloc. The deﬁnitions of γt and
1/t) and so the same argument as

≤

2E

n

"

t=1
X

γtk1

k

#

n

t=1
X

= 2VlocKloc

ηt + o(√n) .

Putting the pieces together and using the fact that

n
t=1

1/t

2√n,

≤

lim sup
n→∞

E[Rn]
√n ≤

lim sup
n→∞

1
√n  

P
2 log(K)
ηn

p
+ 2KlocVloc(Vloc + 1)

n

ηt

!

t=1
X

= 8

2Vloc(1 + Vloc)Klocn log(K) .

The result is completed by recalling that vab(
·

p

) were chosen so that Vloc

1 + F .

≤

16

D Lower Bounds for Hard Games

In this section we prove a Ω(n2/3) lower bound on the minimax regret in hard partial monitoring
games. Like for bandits, by Yao’s minimax principle [Yao, 1977], the lower bounds are most easily
proven using a stochastic adversary. In stochastic partial monitoring we assume that u1, . . . , un
are chosen independently at random from the same distribution. To emphasise the randomness we
, Φ) and a probability vector
switch to capital letters. Given a partial monitoring problem G = (
∈ PE−1 the stochastic partial monitoring environment associated with u samples a sequence of
u
independently and identically distribution random variables U1, . . . , Un with Ut ∈ {
e1, . . . , eE}
with P (Ut = ei) = ui.
In each round t a policy chooses action At and receives feedback
Φt = Φ(At, Ut). The regret is

L

n

E

"

ℓAt −
h

Rn(u, π, G) = max
a∈[K]

ℓa, Uti#
The mentioned minimax principle implies that R∗
n(G)
follows, we lower bound supu Rn(u, π, G) for ﬁxed π.
Given u, v
parameters u and v respectively:

t=1
X

≥

= max
a∈[K]

E

ℓAt −
h

ℓa, u

.

i#

n

"

t=1
X

inf π supu Rn(u, π, G). Hence, in what

∈ PE−1, let KL(u, v) be the relative entropy between categorical distributions with

(15)

(16)

KL(u, v) =

ui log

K

i=1
X

ui
vi (cid:19)

(cid:18)

K

≤

i=1
X

vi)2

,

(ui −
vi

where the second inequality follows from the fact that for measures P
≤
χ2(P, Q). We need one more simple result that is frequently used in lower bound proofs. Given
measures P and Q on the same probability space, Lemma 2.6 in the book by Tsybakov [2008] says
that for any event A,

Q we have KL(P, Q)

≪

P (A) + Q(Ac)

exp (

KL(P, Q)) .

1
2

≥

−

Theorem 4. Let G = (
locally observable. Then there exists a constant cG > 0 such that R∗

, Φ) be a globally observable partial monitoring problem with that is not

cGn2/3.

L

n(G)

≥

Proof. The proof involves several steps. Roughly, we need to deﬁne two alternative stochastic partial
monitoring problems. We then show these environments are hard to distinguish without playing an
action associated with a large loss. Finally we balance the cost of distinguishing the environments
against the linear cost of playing randomly.

∈ PE−1 let Pu
Fix a policy π and a partial monitoring game G with the required properties. For u
denote the measure on sequences of outcomes (A1, Φ1, . . . , An, Φn) induced by the interaction of
a ﬁxed policy and the stochastic partial monitoring problem determined by u and G and denote by
Eu the corresponding expectation. Note that Rn(π, u, G) = maxa Eu [

ℓa, u

n
t=1h

ℓAt −

].
i

P

Step 1: Deﬁning the alternatives Let a, b be a pair neighbouring actions that are not locally
2. Let u be the centroid of
observable. Then by deﬁnition Ca ∩
Ca ∩

Cb is a polytope of dimension E

Cb and

−

ε = min

ℓc −
The value of ε is well-deﬁned, since by global observability of G, but nonlocal observability of (a, b)
∈ Nab it follows that ε > 0. We also
there must exist some action c /
have u
PE−1). We now deﬁne two stochastic partial monitoring problems. Since (a, b) are not
[E],
locally observable, there is no function v : [K]
→

∈ Nab. Furthermore, since c /

R such that for all i

c /∈Nabh

ℓa, u

[F ]

ri(

×

∈

∈

i

(17)

.

v(c, Φci) = ℓai −

ℓbi .

Xc∈Nab

(18)

To facilitate the next step we rewrite this using a linear structure. For action c
Sc ∈ {

[K] let
, which is chosen so that Scei = eΦci.
}

F ×E be the matrix with (Sc)f i = 1

Φ(c, i) = f
{

0, 1

∈

}

17

Deﬁne the linear map S : RE

R|Nab|F by

→

S = 



,

Sa
Sb
...
Sc







which is the matrix formed by stacking the matrices
Sc : c
{
R|Nab|F such that
Eq. (18) if and only if there exists a vector w
∈
ℓb = w⊤S .
ℓa −

∈ Nab}

. Then there exists a v satisfying

img(S⊤) it holds that w

ker(S) be such that ℓa −
ℓb /
∈
ℓbi
w, ℓa −
h

In other words, actions (a, b) are locally observable if and only if ℓa −
ℓb ∈
img(S⊤). Let z
assumed that (a, b) are not locally observable, it follows that ℓa −
ℓb /
∈
ℓb = z + w, which is possible since img(S⊤)
and w
∈
Since ℓa −
ℓbi
w, ℓa −
=
h
let v = w/
. It follows that Sv = 0 and
v, ℓa −
ℓbi
h
∆v and ub = u + ∆v so that
constant to be tuned subsequently and deﬁne ua = u
−
and
ℓb, ubi
ℓa −
ℓb −
h
h
PE−1). This means
PE−1) and ub ∈
We note that if ∆ is sufﬁciently small, then ua ∈
Ca∩
that action a is optimal if the environment plays ua on average and b is optimal if the environment
1)-simplex (see Fig. 1).
plays ub on average and that ua and ub are in the relative interior of the (E

img(S⊤). Since we have
img(S⊤)
ker(S) = RE.
⊕
= 0. Finally
w, w
w, z + w
h
h
= 1.2 Let ∆ > 0 be some small

= ∆ .
Cb∩

ℓa, uai

= 0 and

= ∆

ri(

ri(

=

i 6

∈

i

−

∈ PE−1 let Pcw be the distribution
Step 2: Calculating the relative entropy Given action c and w
on the feedback observed by the learner when playing action c in stochastic partial monitoring
environment determined by w. That is Pcw(f ) = Pw(Φt = f
At = c) = (Scw)f . Let Tc(n) be the
number of times action c is played over all n rounds. The chain rule for relative entropy shows that

|

KL(Pua , Pub ) =

Eua [Tc(n)] KL(Pcua , Pcub) .

(19)

Xc∈[K]

By deﬁnition of ua and ub we have Scua = Scub for all c
and so KL(Pcua , Pcub) = 0 for all c
ua, ub, u

∈ Nab. On the other hand, if c /

∈ Nab. Therefore Pcua = Pcub
∈ Nab, then thanks to

ri(

PE−1) and Eq. (15),

∈

KL(Pcua, Pcub)

KL(ua, ub)

≤

E

≤

i=1
X

ubi)2

(uai −
ubi

= 4∆2

K

i=1
X

v2
i
∆vi ≤
ui −

Cu∆2 ,

where Cu is a suitably large constant and we assume that ∆ is chosen sufﬁciently small that
ui −

ui/2. Therefore

∆vi ≥

(20)

≤
where ˜T (n) is the number of times an arm not in

KL(Pua , Pub)

CuEua [ ˜T (n)]∆2 ,
Nab is played:
Tc(n) .

˜T (n) =

Xc /∈Nab

Step 3: Comparing the regret By Eq. (17) and the Cauchy-Schwartz inequality for c /
∈ Nab we
have
k∞. By
ℓc −
v
ε
ℓa, uai
ℓb, ubi ≥
ℓc −
v
h
k
h
i ≥
k
α)ℓb.
[0, 1] such that ℓc = αℓa + (1
∈ Nab there exists an α
Lemma 1, for each action c
−
Therefore

k∞ and
∈

ℓc −
h

ℓa, ∆v

= ε +

2∆

2∆

−

−

ε

ℓa, uai
ℓc −
+
ℓc −
h
h
which means that max(
ℓc −
h
some arm in

ℓb, ubi
= (1
,
ℓc −
ℓa, uai
h
Nab is played that is at least ∆/2 suboptimal in ua:
∆
2

ℓb −
α)
h
−
)
ℓb, ubi
≥

ℓa, uai ≥

¯T (n) =

1

ℓc −
h
(cid:26)

Xc∈Nab

Tc(n) .

(cid:27)

(21)
ℓb, ubi
ℓa, uai
∆/2. Deﬁne ¯T (n) as the number of times

ℓa −
+ α
h

= ∆ ,

2The minor error in Bart´ok et al. [2014] appears in their deﬁnition of v, which is in the kernel of a different

S constructed by stacking just Sa and Sb and not the degenerate/duplicate actions in between.

18

Assume that ∆ is chosen sufﬁciently small so that 2∆

v
k

k∞ ≤

ε/2. Then

ℓc −
Tc(n)
h

+ Eub 

n/2) + Pub( ¯T (n) < n/2)

ℓa, uai


Xc∈[K]

ℓc −
Tc(n)
h

ℓb, ubi


(cid:1)

≥

+

Eua

Rn(π, ua, G) + Rn(π, ub, G) = Eua 
Xc∈[K]

Pua ( ¯T (n)

n∆
4
n∆
8
n∆
8

˜T (n)
i
h
˜T (n)
i
h
˜T (n)
i
h
(cid:16)
In the above display we used (21) and
Tc(n)1
ℓc −

ℓb, ubi

ε
2
ε
2
ε
2

(cid:0)
exp (

> ∆/2

Eua

Eua

exp

{h

≥

−

−

=

+

+

≥

}

c
X

≥

KL(Pua , Pub))

Cu∆2Eua

˜T (n)
h
Tc(n)1

,

i(cid:17)
ℓc −

{h

c
X

where the second inequality follows from the high probability version of Pinsker’s inequality
Eq. (16) and the third from Eq. (20). The bound is completed by a simple case analysis.
If
Eua [ ˜T (n)] > n2/3, the result holds for any value of ∆. Otherwise choosing ∆ = (c/n)1/3 for
appropriate positive game-dependent constant c establishes the bound.

ℓa, uai ≤

∆/2

= n

}

−

¯T (n) ,

C1

u1

C2

u
u2

C3

Figure 1: Lower bound construction for hard partial monitoring problems

E Lower Bound for Theorem 3

We consider the following game with K = 2 and E = 2F
2 3
1
2 2
0

L
F
F
L
Theorem 5. For n sufﬁciently large the minimax regret of G is at least R∗

2 and G = (
3 4
3 3

1 2
1 1

0 1
1 0

1
0
(cid:18)

0
1
(cid:19)

, Φ =

· · ·
· · ·

· · ·
· · ·

0
1

−

=

(cid:18)

, Φ) given by
1
1 F
1 F
2 F

−
−
F −1
45 √n.

−
−
n(G)

≥

F

1
(cid:19)

−

.

1/(17n) and u

Proof. Let ∆ =
by u′
1)i∆. Using the notation of the previous section we have
i = ui + 2(
p
−
KL(P1u, P1u′)

χ2(P1u, P2u′ ) = 4∆2

∈ PE−1 be constants to be tuned subsequently and u′

and

KL(P2u, P2u′ ) = 0 .

+

∈ PE−1

≤
2)∆/4 and choose u1 = p + ∆ and uE = p

2, . . . , E
(E
1)i). For sufﬁciently large horizon, ∆ is small enough so that KL(P1u, P1u′ )

∆ and for i

E (cid:19)

∈ {

−

−

(cid:18)

1
u′
1

1
u′

let
1
}
17∆2

−
≤

Let p = 1/2
−
ui = ∆(1
(
−
and

−

KL(Pu, Pu′) = Eu[T1(n)] KL(P1u, P1u′ )

17n∆2 .

Using the fact that Rn(u) = 2∆(F

Rn(u) + Rn(u′)

≥
The result follows since max(a, b)

≥

≤

1)Eu[T1(n)] and Rn(u′) = 2∆(F
−
(F
n(F

1)Eu′ [T2(n)] leads to
1)
2e
(a + b)/2 and by naive simpliﬁcation.

n17∆2

−
−

1)∆

exp

−
2

n
17

p

=

−

.

(cid:1)

(cid:0)

19

F Generic Bound for Exponential Weights

The proof of Theorem 2 depends on a generic regret analysis for a variant of the EXP3.P bandit
algorithm by Auer et al. [1995]. The main difference is that loss estimators are assumed to be
god-given and satisfy certain properties, rather than being explicitly deﬁned as biased importance-
weighted estimators. Nothing here would startle an expert, but we do not know where an equivalent
t=0, P) be a ﬁltered probability space and abbreviate
result is written in the literature. Let (Ω,
Et[
[K].
Recall that a sequence of random elements (Xt) is called adapted if Xt is
Ft-measurable for all t,
Ft−1-measurable for all t. Let (Zt) and ( ˜Zt) be sequences
while (Xt) is called predictable if Xt is
of random elements in RK. Given nonempty
[K] and positive constant η deﬁne the probability
vector Qt ∈ PK−1 by

·|Ft]. To reduce clutter we assume for the remainder that t ranges in [n] and a

Ft)n
, (

] = E[

A ⊆

F

∈

·

Qta =

1A(a) exp

t−1
s=1

˜Zsa

b∈A exp

P

η

t−1
s=1

˜Zsb

η

−

(cid:16)

−

.

(cid:17)

(cid:16)
Theorem 6. Assume that the RK-valued process (Zt)t is predictable, the RK-valued process ( ˜Zt)t
βt, where ( ˆZt)t is adapted and (βt)t is predictable. Assume the
is adapted and that ˜Zt = ˆZt −
following hold for all a

P

P

(cid:17)

:
∈ A

ˆZta| ≤
(a) η
|
(c) ηEt−1[ ˆZ 2
ta]

1 ,

≤
Let A∗ = arg mina∈A
1

(K + 1)δ,

−

P

βta almost surely ,

(b) ηβta ≤
(d) Et−1[ ˆZta] = Zta almost surely .

1 ,

n
t=1 Zta. Then, for any 0

δ

≤

≤

1/(K + 1), with probability at least

n

K

t=1
X

a=1
X

Qta(Zta −

ZtA∗ )

≤

n

3 log(1/δ)
η

+ η

n

Qta ˆZ 2

ta + 5

Qtaβta .

t=1
X

Xa∈A

t=1
X

Xa∈A

Proof. We proceed in ﬁve steps.

Step 1: Decomposition

Qta(Zta −

ZtA∗)

n

t=1
X

Xa∈A
=

n

t=1
X

Xa∈A

Qta( ˜Zta −

˜ZtA∗)

+

Qta(Zta −

˜Zta)

+

( ˜ZtA∗

ZtA∗)

.

n

t=1
X

Xa∈A

(B)

n

t=1
X

−

(C)

Step 2: Bounding (A) By assumption (c) we have βta ≥
that η ˜Zta ≤
negentropy regularisation [Hazan, 2016] shows that (A) is bounded by

1 for all a

∈ A

η

|

|
η ˆZta ≤

{z

{z
0, which by assumption (a) means
. Then the standard mirror descent analysis with

}

}

|

}

|

(A)

{z
ˆZta| ≤

Qta ˜Z 2
ta

(A)

n

log(K)
η

+ η

≤

=

log(K)
η

+ η

log(K)
η

≤

+ η

t=1
X
n

Xa∈A

t=1
X
n

Xa∈A

Qta( ˆZ 2

ta + β2

ta)

n

Qta ˆZ 2

ta + 3

n

2η

−

−

1

γ

t=1
X

Xa∈A

Qtaβta ,

Qta ˆZtaβta

Xa∈A
where in the last two line we used the assumptions that ηβta ≤

Xa∈A

t=1
X

t=1
X

1 and η

ˆZta| ≤

|

1.

20

Step 3: Bounding (B) For (B) we have

n

(B) =

t=1
X

Xa∈A

Qta(Zta −

˜Zta) =

Qta(Zta −

ˆZta + βta) .

n

t=1
X

Xa∈A

We prepare to use Lemma 10. By assumptions (c) and (d) respectively we have ηEt−1[ ˆZ 2
ta]
and Et−1[ ˆZta] = Zta. By Jensen’s inequality,
2

βta

≤

Xa∈A
Therefore by Lemma 10, with probability at least 1



Qta(Zta −

ˆZta)



≤

!

η

QtaEt−1[ ˆZ 2
ta]

Qtaβta .

≤

Xa∈A

ηEt−1 
 


Xa∈A
δ
−
Qtaβta +

log(1/δ)
η

.

n

(B)

2

≤

t=1
X

Xa∈A

Step 4: Bounding (C) For (C) we have

n

n

(C) =

( ˜ZtA∗

ZtA∗) =

ˆZtA∗

−

ZtA∗

βtA∗

.

−

−

t=1 (cid:16)
X
Because A∗ is random we cannot directly apply Lemma 10, but need a union bound over all actions.
1 and Et−1[ ˆZta] = Zta
Let a
∈ A
and ηEt−1[ ˆZ 2
ta]

be ﬁxed. Then by Lemma 10 and the assumption that η

βta, with probability at least 1

ˆZta| ≤

t=1
X

δ.

(cid:17)

|

≤

n

−
βta

ˆZta −

Zta −

log(1/δ)
η

.

≤

t=1 (cid:16)
X
Therefore by a union bound we have with probability at most 1

(cid:17)

Kδ,

−

(C)

≤

log(1/δ)
η

.

Step 5: Putting it together Combining the bounds on (A), (B) and (C) in the last three steps with
the decomposition in the ﬁrst step shows that with probability at least 1

(K + 1)δ,

Rn ≤

3 log(1/δ)
η

+ η

n

n

Qta ˆZ 2

ta + 5

−
Qtaβta .

t=1
X

Xa∈A

t=1
X
1/K.

Xa∈A

≤

where we used the assumption that δ

G Bounding the Norm of the Estimators

Lemma 9. Let a and b be pairwise observable and ∆ = ℓa −
function v :

R such that:

a, b

[F ]

ℓb ∈

[

−

1, 1]E, then there exists a

{

→

} ×
1 + F .

(a)

v
k

k∞ ≤

(b) v(a, Φai) + v(b, Φbi) = ∆i for all i

[E].

∈

R such that
Proof. By the deﬁnition of pairwise observable there exists a function v :
{
v(a, Φai) + v(b, Φbi) = ∆i for all i
[F ]
a, b
and edges
. Assume without loss of generality this graph is
fully connected. If not then apply the following procedure to each connected component. For any
[F ] let (a, f1), (b, f2), (a, f3), . . . , (a, fk) be a loop free path with f1 = f and fk = f ′ and
f, f ′
for j
[k

}×
[E]. Deﬁne bipartite graph with vertices

∈
((a, Φai), (b, Φbi)) : i

→
=
{

, fj+1). Then

} ×

a, b

[E]

[F ]

=

∈

V

E

{

}

∈
∈

1] let ij ∈

−

|

k−1

v(a, f )

[E] correspond to the edge connecting (
·
1)j−1∆ij (cid:12)
(cid:12)
(cid:12)
(cid:12)
v(a, f )
(cid:12)
(cid:12)
−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
2 maxf,f ′∈[F ] |
(cid:12)
(cid:12)
·

) by a constant α and v(b,

v(a, f ′)
|

v(a, f )

(
−

j=1
X

−

=

≤

1

, fj) and (
·

2F .

| ≤

, which is always
[F ]
[E] such that v(b, f ) + v(a, f ′) = ∆i, which ensures that

α. Finally for each f

v(a, f ′)
|

) by

−

∈

We may assume that maxf ∈[F ] |
possible by translating v(a,
·
there exists an f ′
[F ] and i
∆i| ≤
v(b, f )

∈
v(a, f ′)
|

| ≤ |

+

|

|

∈
F + 1.

21

H Concentration

Lemma 10. Let X1, X2, . . . , Xn be a sequence of random variables adapted to ﬁltration (
·|Ft] and µt = Et−1[Xt]. Suppose that η > 0 satisﬁes ηXt ≤
let Et[

Ft)t and
1 almost surely. Then

] = E[

·

n

P

 

t=1
X

(Xt −

µt)

≥

η

n

t=1
X

Et−1[X 2

t ] +

log

1
η

1
δ

(cid:19)! ≤

(cid:18)

δ .

Proof. Let σ2

t = Et−1[X 2

t ]. By Chernoff’s method we have

n

P

Xt −
(cid:0)

 

t=1
X

ησ2
t

µt −

≥

(cid:1)

log(1/δ)
η

!

= P

exp

η

 

 

n

Xt −
(cid:0)

t=1
X
n

ησ2
t

µt −

δE

exp

η

"

 

≤

Xt −
(cid:0)

t=1
X

µt −

1
δ !

! ≥
(cid:1)
ησ2
t

.

!#
(cid:1)

The result is completed by showing the term inside the expectation is a supermartingale. For this,
we have

Et−1

ησ2
t

η

exp

µt −

ηµt −
−
ηµt −
(cid:0)
−
≤
(cid:0)
1 + x + x2 for x
where we used the inequalities exp(x)
Chaining the above inequality completes the proof.

Xt −
(cid:0)

= exp

exp

(cid:1)(cid:1)(cid:3)

≤

(cid:0)

(cid:2)

η2σ2
t
η2σ2
t

(cid:1)

Et−1 [exp (ηXt)]
1 + ηµt + η2σ2
t

(cid:1) (cid:0)

1 and 1 + x

≤

≤

1 ,

≤
exp(x) for all x

(cid:1)

R.

∈

Proof of Lemma 3. Let Λ be the smallest value such that X
surely positive. Taking expectations of both sides shows that

≤

Y +

2a log(b/Λ), which is almost

E[X]

E[Y ] + √2aE[

log(b/Λ)]

p

The second expectation is bounded by

p

≤

≤

≤

≤

=

≤

E[

log(b/Λ)] =

∞

P

p

log(b/Λ)

x

dx

≥

(cid:17)

0

Z
inf
y>0

(cid:16)p

∞

y +

P

P

(cid:16)p
Λ

≤

(cid:0)
b exp(

log(b/Λ)

x

dx

b exp(

x2)

dx

≥

−

(cid:17)

(cid:1)

x2)dx

−

inf
y>0

y +

inf
y>0

y +

y
Z

∞

y
Z

∞

y
Z
b√π
2

= inf
y>0

y +

erfc(y)

log(b) +

b√π
2
1 + log(b) .

erfc(

log(b))

p

p

p

I Gallery

Exhibit 1. In the following game the learner cannot estimate the actual losses, but the loss
differences can be calculated from the feedback directly.

=

L

(cid:18)

1
1/2

1/2 1/2

1

0

0
1/2

,

Φ =

1
1
(cid:18)

2 1
2 1

.

2
2

(cid:19)

(cid:19)

22

Exhibit 2. A useful way to think about the cell decomposition is to assume that
entries and consider the intersection of the hypograph of concave function f (u) = minah
domain u =
PE−1 and the epigraph of
the spam game where c = 1/3, which is deﬁned by

PE−1. To illustrate the idea let G = (
L

has positive
with
ℓa, u
, Φ) be the variant of

L

i

=

L

1
0
1
3





0
1
1
3





,

Φ =

1
1
 
1

1
1
2!

.

PE−1 =

P1 is 1-dimensional, which means the intersection of the epigraph of

In this case
PE−1
and the hypograph of f is 2-dimensional and is shown in the left ﬁgure below. The intersection is
itself a polytope and the faces (1-dimensional in this case) pointing upwards correspond to cells of
nondegenerate actions. If c is increased to 1/2, then the third action becomes degenerate, which is
observable from the right-hand ﬁgure below by noting that the dimension of its intersection with the
polytope is now zero. Increasing c any further would make this action dominated.

ℓ2, u
h

i

ℓ2, u
h

i

ℓ1, u
h

i

ℓ3, u
h

i

u1

ℓ1, u
h

i

ℓ3, u
h

i

u1

c = 1/3

c = 1/2

Figure 2: Alternative view of cell decomposition for spam game with c = 1/3 and c = 1/2.

Exhibit 3. The following game demonstrates that not all locally observable games are point-locally
observable.

1
0
0
1
1/2 1/2 1/2!

1
1

,

 

=

L

Φ =

1
1
1

 

1 1
1 1
2 3!

.

u2

C2

C3

C1

u1

The cell decomposition for this game is shown on above-right. Notice that (1, 2) are not neighbours,
but are weak neighbours. And yet (1, 2) are not pairwise observable. Therefore the game is not
point-locally observable. On the other hand, both sets of neighbours (1, 2) and (1, 3) are locally
observable.

Exhibit 4. This game produces the cell decomposition depicted at the start of Section 3. The only
neighbours are (2, 3) and (1, 3), which are locally observable. Therefore the game is locally observ-
able. Actions 4,5 and 6 are degenerate. NEIGHBOURHOODWATCH2 will only play actions 1, 2,
3 and 4 with actions 5 and 6 ruled out because their cells are not equal to the intersection of any

23

neighbours cells. Notice that ℓ4 = ℓ2/2 + ℓ3/2 is a convex combination of ℓ2 and ℓ3.

u2

1
1

1
0
0
1
1/2 1/2 1/2
3/4 1/4 3/4
1/2 1/2
1
1/4 3/4
1

,










=

L










1 1
1 1
1 2
1 1
1 1

Φ = 





.

1
1

3
1


1



C6

C5

C4

C2

C3

C1

u1

24

