QTLeap WSD/NED Corpora:
Semantic Annotation of Parallel Corpora in Six Languages

Arantxa Otegi,∗ Nora Aranberri∗, Antonio Branco‡, Jan Hajiˇc†, Steven Neale‡,
Petya Osenova ∇, Rita Pereira‡, Martin Popel†, Jo˜ao Silva‡, Kiril Simov∇ and Eneko Agirre∗
∗ University of the Basque Country (UPV/EHU), IXA Group – {arantza.otegi, nora.aranberri, e.agirre}@ehu.eus
† Charles University in Prague, Faculty of Mathematics and Physics, UFAL – {hajic, popel}@ufal.mff.cuni.cz
‡Universidade de Lisboa – {antonio.branco, steven.neale, ana.pereira, jsilva}@di.fc.ul.pt
∇Institute of Information and Communication Technologies (IICT-BAS) – {petya, kivs}@bultreebank.org

Abstract
This work presents parallel corpora automatically annotated with several NLP tools, including lemma and part-of-speech tagging,
named-entity recognition and classiﬁcation, named-entity disambiguation, word-sense disambiguation, and coreference. The corpora
comprise both the well-known Europarl corpus and a domain-speciﬁc question-answer troubleshooting corpus on the IT domain.
English is common in all parallel corpora, with translations in ﬁve languages, namely, Basque, Bulgarian, Czech, Portuguese
and Spanish. We describe the annotated corpora and the tools used for annotation, as well as annotation statistics for each language.
These new resources are freely available and will help research on semantic processing for machine translation and cross-lingual transfer.

Keywords: annotated parallel corpora, named-entity disambiguation, word sense disambiguation, coreference

1.

Introduction

From a machine translation (MT) perspective, the deeper
the processing of utterances, the less language-speciﬁc dif-
ferences will remain between the representations of the
meaning of the source and target texts. As a result, chances
of success are expected to increase considerably by MT
systems that are based on deeper semantic engineering ap-
proaches. Following this assumption, one of the approaches
taken by the QTLeap project1 is to enrich MT training re-
sources with lexico-semantic information.
In this work, we present a solid effort to build multilin-
gual parallel corpora annotated at multiple semantic lev-
els. Our overall goal is to enrich two parallel corpora, Eu-
roparl (Koehn, 2005) and the QTLeap corpus (Agirre et al.,
2015b), with token, lemma, part-of-speech (POS), named-
entity recognition and classiﬁcation (NERC), named-entity
disambiguation (NED), word-sense disambiguation (WSD)
and coreference for six languages covered in the QTLeap
project, namely, Basque (EU), Bulgarian (BG), Czech
(CS), English (EN), Portuguese (PT) and Spanish (ES).
Speciﬁcally, this paper presents the ﬁrst release of such cor-
pora, which includes NERC, NED, WSD and coreference-
level annotation for these six languages. Additionally,
some languages have extra annotations, such as wikiﬁca-
tion (EN, ES), dependency parsing (BG, CS, EU) or con-
stituency parsing (EN, ES), and semantic-roles (EN, ES).
The annotated “Europarl-QTLeap WSD/NED corpus” and
“QTLeap WSD/NED corpus” are distributed under the li-
cense CC BY-NC-SA 4.0, and have been released through
Meta-share2 and CLARIN Lindat.3

1http://qtleap.eu
2http://metashare.metanet4u.eu/go2/

europarl-qtleap-wsdned-corpus
http://metashare.metanet4u.eu/go2/
qtleap-wsdned-corpus

3https://lindat.mff.cuni.cz, namely:

http://hdl.handle.net/11234/1-1476

This paper is organized as follows: Section 2 presents
the corpora we annotated; Section 3 describes the NERC,
WSD, NED and coreference tools and annotation formats;
Section 4 addresses the evaluation of the tools; Section 5
presents the corpus statistics of the annotation; and ﬁnally
Section 6 outlines the conclusions.

2. Target Corpora
This section describes the two corpora we have annotated,
the Europarl corpus and the QTLeap corpus. Each resource
covers a different text-type and domain.

2.1. Europarl corpus
As a ﬁrst resource, we have annotated the widely-used Eu-
roparl corpus4 (release v7).
It consists of texts extracted
from the proceedings of the European Parliament, which
include versions in 21 European languages. Compiled with
machine translation in mind, matching items were extracted
and labeled with corresponding document IDs. Then, sen-
tence boundaries were identiﬁed and aligned (for further
collection and processing information, see Koehn (2005)).
The Europarl corpus consists of monolingual data as well
as bilingual parallel data with English as pivot language.
In our effort, we have annotated the BG, CS, ES and PT
parts of the corpus separately while the EN side of the ES-
EN language pair was used as pivot language to link all six
languages.
Given that Europarl does not include Basque, we annotated
an alternative publicly available Basque-English parallel
corpus, the GNOME corpus (Tiedemann, 2012), which in-
cludes GNOME localization ﬁles.

2.2. QTLeap corpus
The QTLeap corpus consists of 4,000 pairs of questions
and respective answers in the domain of IT troubleshooting

http://hdl.handle.net/11234/1-1477
4http://www.statmt.org/europarl/

3023

for both hardware and software, distributed in four 1,000-
pair batches (Gaudio et al., 2016). This material was col-
lected using a real-life, commercial online support service
via chat.
The QTLeap corpus is a unique resource in that it is a mul-
tilingual data set with parallel utterances in different lan-
guages (Basque, Bulgarian, Czech, Dutch, English, Ger-
man, Portuguese and Spanish). This multilingual resource
was obtained by translating the original Portuguese corpus
into a pivot language, English, and this into the remaining
six languages.
The current annotated corpus covers the ﬁrst 2,000 sen-
tences of the QTLeap corpus, which have been used to train
the MT systems in the project.

3. Annotation Tools
In this section, we describe the NERC, NED, WSD and
coreference tools used to annotate the corpora. We have
chosen the tools based on their performance and their ease
of use. We also describe the annotation formats.

3.1. Named-entity recognition and classiﬁcation
Basque, English and Spanish ixa-pipe-nerc is a mul-
tilingual NERC tagger, part of IXA pipes (Agerri et al.,
2014). Every model has been trained with the averaged
Perceptron algorithm as described in Collins (2002) and as
implemented in Apache OpenNLP. The datasets used for
training the models are the following: Egunkaria dataset
for Basque, a combination of Ontonotes 4.0, CoNLL 2003
and MUC 7 for English, and CoNLL 2002 for Spanish.

Bulgarian The Bulgarian NERC is a rule-based module.
It uses a gazetteer with names categorized in four types:
Person, Location, Organization, Other. The identiﬁcation
of new names is based on two factors – sure positions in
the text and classifying contextual information, such as,
titles for persons, types of geographical objects or orga-
nizations, etc. The disambiguation module uses simple
unigram-based statistics.
Czech NameTag5 is an open-source trainable tool for
NERC (Strakov´a et al., 2014). NameTag is distributed as
a standalone tool or a library, along with pre-trained lin-
guistic models. In the Czech model, entities are classiﬁed
into two-level hierarchy of categories consisting of 42 ﬁne-
grained categories merged into 7 super-classes.

Portuguese LX-NER is a NERC tool that handles the
following types of expressions: Numbers (Arabic, Deci-
mal, Non-compliant, Roman, Cardinal, Fraction, Magni-
tude class, Measures (Currency, Time, Scientiﬁc units),
Time (Date, Time periods, Time of the day) and Addresses)
and name-based expressions (Persons, Organizations, Lo-
cations, Events, Works, Miscellaneous). The number-based
component is built upon handcrafted regular expressions.
It was developed and evaluated against a manually con-
structed test-suite including over 300 examples. The name-
based component is based on Hidden Markov Models tech-
nology and was trained over a manually annotated corpus
of approximately 208,000 words (Ferreira et al., 2007).

5http://ufal.mff.cuni.cz/nametag

3.2. Named-entity disambiguation

Basque
ixa-pipe-ned-ukb performs NED based on UKB,
a graph-based WSD tool (see Section 3.3.). The Wikipedia
graph built from the hyperlinks between Wikipedia articles
is used for the processing. This tool was successfully used
for English NED (Agirre et al., 2015a).

Bulgarian NED annotations follow the same approach
as the Bulgarian disambiguation module (see Section 3.3.),
but the DBpedia classes are used instead of WordNet. The
ontological hierarchy of DBpedia determines the more gen-
eral categories for DBpedia instances (City, Politician, etc.)
as subclasses of Person, Location and Organization. For
other kinds of instances it relies on the most general cate-
gory provided by the classiﬁcation of the instance accord-
ing to DBpedia. Then the standard module is adapted to
use the new categories. In case the selected categories in
the annotation are not sufﬁcient for disambiguating among
DBpedia instance URIs, we store all of them in the annota-
tion.

Czech During the preparation phase for NED Named En-
tities Linking table was created. Each row of that table
consists of the lemmatized Czech Wikipedia article’s title,
Czech Wikipedia URL and English DBpedia URL, based
on Czech Wikipedia dump (containing Czech titles and
corresponding Wikipedia URLs) and English-Czech DB-
pedia dump (containing Czech labels and English DBpedia
URLs). Additionally, lemmatization and tagging for each
title was applied using MorphoDiTa (Strakov´a et al., 2014).
For each entity that is detected by NameTag, its form is
lemmatized. Then we search the table for the occurrences
of this lemmatized form. In case of ambiguity, the algo-
rithm picks up the most “popular” article. The popularity of
the article is computed using Wikipedia page-to-page link
records, so the article with the highest number of reference
links is preferred.

English and Spanish ixa-pipe-ned module performs the
NED task based on DBpedia Spotlight (Daiber et al., 2013).
Assuming that a DBpedia Spotlight REST server for a
given language is locally running, the module performs the
disambiguation for each entity detected by NERC module.
It offers the “disambiguate” and “candidates” service end-
points. The former takes the spotted text input and it returns
the DBpedia resource page for each entity. The later is sim-
ilar to disambiguate, but returns a ranked list of candidates.

Portuguese The NED module for Portuguese, LX-NED,
uses DBpedia Spotlight to ﬁnd links to resources about en-
tities identiﬁed in pre-processed input text. It creates a pro-
cess to run a Portuguese extraction of DBpedia Spotlight
on a local server, then takes an input text pre-processed
with lemmas, Part of Speech tags and named entities us-
ing the LX-Suite (Branco and Silva, 2006) and converts it
to the ’spotted’ format understood by Spotlight. This spot-
ted input text is then disambiguated using DBpedia Spot-
light, returning among other information links to existing
Portuguese DBpedia resource pages for each named entity
discovered.

3024

3.3. Word-sense disambiguation
Basque, English and Spanish ixa-pipe-wsd-ukb is based
on UKB, a collection of programs for performing graph-
based WSD (Agirre and Soroa, 2009). It applies the so-
called Personalized PageRank on a Lexical Knowledge
Base (LKB) to rank the vertices of the LKB and thus per-
form disambiguation. WordNet 3.0 is the LKB used for this
processing.

Bulgarian The basic version of Bulgarian WSD is imple-
mented on the assumption of one sense per discourse and
bigram statistics.

Czech Two different approaches were used for Czech
WSD. The ﬁrst approach based on the work of Duˇsek et
al. (2015) focuses on verbal WSD. The second approach
followed for the annotation is a straightforward way of
achieving compatibility with English WordNet IDs. Since
the Czech corpus contains the same sentences as the En-
glish corpus, the English WordNet ID annotation from this
corpus is projected onto Czech words using GIZA++ word
alignment.

Portuguese The Portuguese WSD tool, LX-WSD, is also
based on UKB. The LKB from which UKB returns word
senses within the pipeline has been generated from an ex-
traction of the Portuguese MultiWordNet6.

3.4. Coreference
Basque
ixa-pipe-coref-eu is an adaptation of the Stanford
Deterministic Coreference Resolution (Lee et al., 2013),
which gives state-of-the art performance for English. The
original system applies a succession of ten independent de-
terministic coreference models or sieves. During the adap-
tation process, ﬁrstly, a baseline system has been created
which receives as input texts processed by Basque analy-
sis tools and uses speciﬁcally adapted static lists to iden-
tify language dependent features like gender, animacy or
number. Afterwards, improvements over the baseline sys-
tem have been applied, adapting and replacing some of the
original sieves (Soraluze et al., 2015), taking into account
that morphosyntactic features are crucial in the design of
the sieves for agglutinative languages like Basque.

Bulgarian A basic version of a coreference resolution
module uses paths in the dependency tree of each sentence.
By using path patterns, anaphora resolution is mainly per-
formed. When dealing with the rest of the word forms, the
open class words that belong to the same synsets in Word-
Net are considered and grouped them together.

Czech There are multiple modules for Czech corefer-
ence, each of them aiming at a speciﬁc type of corefer-
ence: coreference of reﬂexive pronouns, relative pronouns,
zeros, personal and possessive pronouns in 3rd person and
coreference of noun phrases (Bojar et al., 2012; Nov´ak and
ˇZabokrtsk´y, 2011). Coreference relations are annotated be-
tween the nodes of dependency trees that serve as a deep
syntax representation of sentences. This enables the system
to take advantage of rich linguistic annotations available in
the trees as well as to resolve coreference even for subject

6http://multiwordnet.fbk.eu/english/home.

php

pronouns dropped from the surface representation (zeros),
which is a common practice in Czech.

English and Spanish ixa-pipe-coref is loosely based on
the Stanford Multi Sieve Pass system (Lee et al., 2013).
The system consists of a number of rule-based sieves. Each
sieve pass is applied in a deterministic manner, reusing the
information generated by the previous sieve and the men-
tion processing. The order in which the sieves are applied
favors a highest precision approach and aims at improving
the recall with the subsequent application of each of the
sieve passes. This is illustrated by the evaluation results of
the CoNLL 2011 Coreference Evaluation task (Lee et al.,
2013; Lee et al., 2011), in which the Stanford system ob-
tained the best results. The results show a pattern which has
also been shown in other results reported with other evalu-
ation sets (Raghunathan et al., 2010), namely, the fact that
a large part of the performance of the multi-pass sieve sys-
tem is based on a set of signiﬁcant sieves. Thus, this module
so far focuses on a subset of sieves only, namely, Speaker
Match, Exact Match, Precise Constructs, Strict Head Match
and Pronoun Match (Lee et al., 2013).

Portuguese For the Portuguese coreference tool, a deci-
sion tree classiﬁer was experimented with. Given a pair
of expressions, the classiﬁer returns a true or false value
that indicates whether those expressions are coreferent. The
classiﬁer was trained over the Summit Corpus (Collovini et
al., 2007) using the J48 algorithm in the Weka machine-
learning toolkit. The most relevant features, according to
the work of de Souza et al. (2008), were extracted from
Summ-It and used to train the J48 algorithm with default
parameters. The resulting decision tree produced by J48
turned up to very simple and boils down to comparing the
cores and the morphological information (gender and num-
ber) of the two expressions. As such, we found it easier to
directly implement equivalent tests in-code instead of hav-
ing to feed the extracted features to the Weka J48 classiﬁer
proper.

3.5. Annotation formats
Basque, Bulgarian, Czech, English and Spanish These
corpora are annotated in the NAF format. The NAF for-
mat (Fokkens et al., 2014) is a linguistic annotation for-
mat designed for complex NLP pipelines that combines
strengths of the Linguistic Annotation Framework (LAF)
and the NLP Interchange Formats described by Ide and Ro-
mary (2003). Because of its layered extensible format, it
can easily be incorporated in a variety of NLP modules that
may require different linguistic information as their input.

Portuguese The corpus for Portuguese is divided into 4
text ﬁles - the raw corpus, and one ﬁle for the output of
each of the three tools used to process it (WSD, NED and
coreference). For each of the three tools output is provided
in a standoff annotation format, consisting of one token per
line (ID of each token in a markable pair in the case of
the coreference tool), the appropriate output element of the
respective tools (word sense, named entity URI or true or
false in the case of coreference), and additional metadata
such as token IDs, sentence IDs and part-of-speech (POS)
tags in the case of the WSD and NED tools.

3025

Language
Basque
Bulgarian
Czech
English
Portuguese
Spanish

NERC NED WSD Coreference
76.72
79.13
80.30
86.21
85.73
80.16

53.67
31.11
see 4.1.4.
56.40
–
51.38

87.90
46.88
–
77.76
67.07
65.11

56.40
65.85
80.47
80.10
65.00
79.30

Table 1: F-scores for annotation tools. Note that evaluation
sets vary across languages.

4. Evaluation

4.1. Evaluation on standard datasets
We provide the performance measurements of the tools
used to annotated the QTLeap WSD/NED corpora, provid-
ing information on the evaluation datasets and scores for
each tool. The summary performance as measured on stan-
dard datasets for all languages is presented in Table 1.

4.1.1. NERC
Basque A subset of the EPEC corpus including 60,000
tokens was manually annotated with 4,748 named entities.7
When evaluated over a subset of ca. 15,000 tokens, ixa-
pipe-nerc’s F-score measure is 76.72% on 3 class evalua-
tion and 75.40% on 4 classes.

Bulgarian The Bulgarian NERC tool was evaluated on a
dataset of the BulTreeBank of 12,223 tokens.8 The gold
standard annotation contains 810 named entities. The F-
score of the tool is 79.13%.

Czech NameTag is the state-of-the-art NERC tool for
Czech. Its F-score on the test portion of Czech Named En-
tity Corpus 2.09 is 80.30% for the coarse-grained 7-classes
classiﬁcation and 77.22% for the ﬁne-grained 42-classes
classiﬁcation (Strakov´a et al., 2014).

English The ixa-pipe-nerc module based on CoNLL
200210 and 2003,11 trained on local features only obtains
F-score of 84.53%, and the models with external knowl-
edge 87.11%. The OntoNotes CoNLL with 4 NE types and
local features model obtains a F-score of 86.21%.

Portuguese The rule-based component of the NERC was
evaluated against a manually constructed test-suite includ-
ing over 300 examples. It scored 85.55% F-score. When
trained over a manually annotated corpus of approximately
208,000 words and evaluated against an unseen portion
with approximately 52,000 words, the data-based module
scored a 85.73% F-score (Ferreira et al., 2007).

Spanish The ixa-pipe-nerc module for Spanish currently
obtains the best results when training Maximum Entropy
models on the CoNLL 2002 dataset. Our best model ob-
tains a 80.16% F-score. The best result so far on this dataset
is a 81.39% F-score (Carreras et al., 2002) when using ex-
ternal knowledge, and a 79.28% F-score without it.

7http://ixa2.si.ehu.es/eiec/eiec_v1.0.tgz
8http://www.bultreebank.org/dpbtb/
9http://ufal.mff.cuni.cz/cnec
10http://www.clips.ua.ac.be/conll2002/ner/
11http://www.clips.ua.ac.be/conll2003/ner/

4.1.2. NED
Basque The ixa-pipe-ned-ukb module for Basque was
evaluated on the publicly available EDIEC (Basque Dis-
ambiguated Named Entities Corpus) dataset.12 This dataset
is a corpus of 1,032 text documents with manually disam-
biguated NEs (Fernandez et al., 2011). We obtained a per-
formance of 87.90% in F-score (P´erez de Vi˜naspre, 2015).

Bulgarian The NED gold
from the
annotated
BulTreeBank-DB includes
with DBpedia.13 The F-score of the tool is 46.88%. The
low results are due to the small coverage of the Bulgarian
DBpedia.

instances

standard

667

Czech There is no publicly available Czech test set for
NED, so we only performed a qualitative evaluation of the
tool (see Section 4.2.2.).

English The ixa-pipe-ned module was evaluated on the
TAC KBP 2011 dataset14 and the AIDA corpus,15 discard-
ing NIL instances. The best results were a 68.93% F-score
for TAC and a 77.76% F-score for AIDA.

Portuguese LX-NED was evaluated using the NE-
annotated version of the CINTIL International Corpus of
Portuguese (Barreto et al., 2006). Out of the 26,371 NEs in
the CINTIL corpus, 16,120 were manually disambiguated
using DBpedia. 12,160 of these were also automatically
disambiguated by LX-NED. We thus deﬁne recall as the
number of entities with the same DBpedia entry assigned
by both the NED tool and the human annotator, divided by
the number of entities manually disambiguated (16,120).
The F-score for LX-NED is 67.07%.

Spanish The Spanish ixa-pipe-ned module was evaluated
on the TAC 2012 Spanish dataset.16 The system identi-
ﬁes entities on a Spanish document and links them to an
English Knowledge Base using the interlingual links from
Wikipedia.17 We obtained a performance of 65.11% in F-
score.

4.1.3. WSD
Basque The ixa-pipe-wsd-ukb module for Basque was
evaluated on the publicly available EPEC-EuSemcor
dataset.18 This dataset is a Basque SemCor corpus, that is, a

12http://ixa2.si.ehu.es/ediec/ediec_v1.0.

tgz

13http://www.bultreebank.org/QTLeap/
14Text Analysis Conference (TAC) for the Knowledge Base
https://www.ldc.upenn.
Population (KBP)
edu/collaborations/current-projects/tac-kbp
Datasets available on https://catalog.ldc.upenn.
edu/

track:

15https://www.mpi-inf.mpg.de/departments/
databases-and-information-systems/research/
yago-naga/aida/downloads/

16Text Analysis Conference (TAC) for the Knowledge Base
https://www.ldc.upenn.
Population (KBP)
edu/collaborations/current-projects/tac-kbp
at https://catalog.ldc.upenn.
Datasets
edu/

available

track:

17http://www.mediawiki.org/wiki/

Interlanguage_links

EuSemcor_v1.0.tgz

18http://ixa2.si.ehu.es/mcr/EuSemcor.v1.0/

3026

Basque sense-tagged corpus, annotated with Basque Word-
Net v1.6 senses (Pociello et al., 2011). More speciﬁcally, it
contains 42,615 occurrences of nouns manually annotated,
corresponding to the 407 most frequent Basque nouns. We
obtained a 56.4% F-score.

Bulgarian The BulTreeBank-DB WSD gold standard in-
cludes 3,118 sense annotations. The tool obtained a 65.85%
F-score. The result is relatively good, considering the lim-
ited size of the Bulgarian WordNet, used in the annotation.

Czech The verbal WSD approach (Duˇsek et al., 2015)
was evaluated on the Prague Czech-English Dependency
Treebank 2.0 (Hajiˇc et al., 2012) and showed a 80.47% F-
score. For the second approach (Czech text annotated with
English WordNet 3.0 IDs), there is no publicly available
test set.

English The ixa-pipe-wsd-ukb module for English was
evaluated on the general domain coarse-grained all-words
datasets (S07CG) (Navigli et al., 2007). This dataset uses
coarse-grained senses which group WordNet 2.1 senses.
The WSD system was run using WordNet 2.1 relations and
senses and the mapping from WordNet 2.1 senses. In order
to return coarse grained-senses, the algorithm was run on
ﬁne-grained senses, and the scores were aggregated for all
senses that mapped to the same coarse-grained sense. Fi-
nally, the coarse-grained sense with the highest score was
chosen. The overall result obtained was a 80.1% F-score.

Portuguese LX-WSD was evaluated using the sense-
annotated version of the CINTIL International Corpus of
Portuguese (Barreto et al., 2006), manually annotated with
ILIs from the Portuguese MultiWordNet19 (approximately
19,700 veriﬁed synsets). We thus deﬁne recall as the num-
ber of words with the same sense assigned by UKB and the
human annotator, divided by the number of words manu-
ally disambiguated (45,502). LX-WSD scored a 65.00%
F-score.

Spanish The ixa-pipe-wsd-ukb module for Spanish was
evaluated on SemEval-2007 Task 09 dataset (M`arquez et
al., 2007). The dataset contains examples of the 150 most
frequent nouns in the CESS-ECE corpus, manually anno-
tated with Spanish WordNet synsets. We ran the experi-
ment over the test part of the dataset (792 instances) and
obtained a 79.3% F-score.

4.1.4. Coreference
Basque The ixa-pipe-coref-eu module has been evaluated
on the publicly available EPEC-KORREF dataset.20 This
dataset is a corpus of Basque text documents with manu-
ally annotated mentions and coreference chains, which con-
sists of 46,383 words that correspond to 12,792 mentions.
Our best system scored 53.67% CoNLL F-score21, 5 points
above baseline (48.67% F-score) (Soraluze et al., 2015).

19http://multiwordnet.fbk.eu/english/home.

php

20http://ixa2.si.ehu.es/epec-koref/

epec-koref_v1.0.tgz

Bulgarian A part of BulTreeBank-DB dataset was anno-
tated again by hand for coreference chains (1468 words,
37 coreference chains, including 154 phrases). The system
returns 49 coreference chains, including 182 phrases. We
have calculated the precision and recall as proposed in (Vi-
lain et al., 1995). The measured F-score was 31.11%.

Czech Coreference resolvers for Czech were evaluated
separately for three different classes of anaphors: relative
pronouns, a joint group of subject zeros, personal, and pos-
sessive pronouns (all in 3rd person), and noun phrases.
For each anaphor class, F-scores of ﬁnding any of its an-
tecedents were measured on the evaluation set of Prague
Dependency Treebank 3.0 (Bejˇcek et al., 2013). The rela-
tive pronoun resolver, using rule-based approach, obtained
a 67.04% F-score. The other two classes obtained a 50.28%
F-score and a 44.40% F-score respectively, using a ma-
chine learning approach. The F-score is calculated from
the counts of how often any of the anaphor’s antecedent
is found, collected over each of the anaphors individually.
This evaluation approach is similar to the one presented by
Tuggener (2014).

English The ixa-pipe-coref module was evaluated on the
development auto section of the CoNLL 2011 shared evalu-
ation task,22 which uses the English language portion of the
OntoNotes 4.0 corpus. It scored a 56.4% CoNLL F-score,
around 3 points below Stanford’s system.

Portuguese The Portuguese coreference tool was trained
using the Summ-it Corpus (v3.0) (Collovini et al., 2007).
For 316,000 sentences of the Portuguese side of Europarl
(∼10 million tokens), the Portuguese Coreference tool was
able to ﬁnd 727,142 markable pairs, from which 22,984
(3.16%) are coreferent. One possible cause for the tool’s
low recall of markable pairs could be inconsistencies be-
tween the dependency-parsed and constituency-parsed in-
puts over which the tool runs.

Spanish The ixa-pipe-coref module for Spanish was
evaluated on the publicly available datasets distributed by
the SemEval 2010 task on Multilingual Coreference reso-
lution, in which the AnCora-ES (the Spanish part) corpus
is used, yielding a CoNLL F1 score of 63.4.

4.2. Qualitative evaluation on the QTLeap

in-domain corpus

In this section, we include a qualitative evaluation of the
tools when applied to the IT-domain QTLeap corpus. We
give results per tool type, as conclusions for all languages
were very similar.

4.2.1. NERC
The NERC tools remained at high accuracy level, although
we identiﬁed a drop in performance due to change of do-
main. First, we observed a drop in recall. IT-domain texts
include a considerable amount of brand and product names
and the tools are not trained to identify them. As an ex-
ample, out of the 85 occurrences of Skype, the Czech tool
recognizes 15. Secondly, precision decreased by frequent

21The CoNLL F-score is the average of the MUC, CEAF and

22http://conll.cemantix.org/2011/

B-CUBED F-scores (Pradhan et al., 2011)

introduction.html

3027

Corpus
tokens

entities

in WordNet

in DBpedia
coreference chains

Basque

Bulgarian

Czech

English

Portuguese

Spanish

53,239
24,691
869
252
5,542

67,591
46.38% 12,627
180
180
306

29.00%

71,061
18.7% 11,060
1715
572
1,027

100%

68,913
15.5% 25,807
2,999
1,950
1,199

33.3%

37.45%

65.02%

72,018
6,116
3,799
1,868
183

71,989
20.40% 22,704
4,313
3,175
705

49.17%

31.54%

73.61%

Table 2: Statistics on the QTLeap WSD/NED corpus for six languages.

Corpus
tokens

entities

in WordNet

in DBpedia
coreference chains

Basque

Bulgarian

Czech

English

Portuguese

Spanish

4.84M

5.16M
2.21M 42.94% 1.28M 26.50% 4.47M 49.20% 22.70M 43.46% 0.66M 32.13% 20.20M 35.43%
0.07M
0.03M 40.26% 0.15M 39.40% 0.12M 39.40%
0.72M

1.91M
1.50M 78.24% 0.10M 59.58%
1.25M

2.18M
1.21M 55.49%
0.94M

57.00M

52.24M

0.06M

0.03M

9.09M

5.04M

0.20M

0.30M

0.17M

5.88M

Table 3: Statistics on the Europarl-QTLeap WSD/NED corpus for six languages.

mislabeling for IT-speciﬁc entities. USB, Wi-Fi and Inter-
net are all classiﬁed as Organization by the Basque tool.
Similarly, Windows, Facebook and Google are often clas-
siﬁed as Location by the Spanish, English and Portuguese
tools. The mislabeling is exacerbated by the inherent am-
biguity between company and product names. The analy-
ses showed that classiﬁcation modules are not tuned to deal
with terminology, product names or highly instructive text,
which is a known weakness of NERC tools trained on gen-
eral corpora.

For

Even

linked

instance,

respectively.

the expected level.

the disambiguation tools

4.2.2. NED
The domain-speciﬁc entities identiﬁed by the NERC
to DBPedia,
correctly
tools were mostly
seem to
and therefore,
perform at
for
Basque Sareko (net) and Facebook were linked to
http://eu.dbpedia.org/resource/Internet
http://eu.dbpedia.org/resource/
and
Facebook,
domain-speciﬁc
products such as Java and MB were correctly linked
http://eu.dbpedia.org/resource/
to
http:
Java_(programazio_lengoaia)
//eu.dbpedia.org/resource/Megabyte. We
see, however, some room for improvement.
Firstly,
in-domain terminology still poses some difﬁculty, with
cases such as PC, incorrectly linked to Microsoft Win-
dows by the Czech tool http://cs.dbpedia.
org/resource/Microsoft_Windows,
PS
which was incorrectly linked to the French Socialist
http://eu.dbpedia.org/resource/
Party
Frantziako_Alderdi_Sozialista by the Basque
tool, when it was referring to PlayStation console. Sec-
ondly,
the incorrect cases due to the lack of shared
Wikipedia/DBpedia entries for the working languages is
notable.

and

or

4.2.3. WSD
Word-sense disambiguation was based on WordNet for
Basque, English, Portuguese and Spanish, and on Valency
Lexicon (Ureˇsov´a, 2011) for Czech. WSD performance
was reasonable in the IT-domain, with little loss in accu-
racy. The decrease was mainly due to missing terms in the
WordNets of the languages in question or incorrect assign-

ments of synsets/valencies. Such is the case of the domain-
speciﬁc banda, for instance, which was linked to the synset
30-04339291 with a conﬁdence of 0.219025, referring to
an artifact consisting of a narrow ﬂat piece of material, in-
stead of the correct synset 30-06260628, which is the spe-
ciﬁc synset for the domain of telecommunications a band of
adjacent radio frequencies (e.g., assigned for transmitting
radio or television signals).

4.2.4. Coreference
The QTLeap corpus is quite peculiar from a coreference
point of view. The user-machine interactions generally con-
sist of one user question and one answer. The answer usu-
ally consists of one sentence, but occasionally a few short
In this context, the number of
sentences are displayed.
coreference chains present in the texts is low. For exam-
ple, the Czech resolvers found only a very small number of
coreferent pairs (1,860 from 82,496 markable pairs).

5. Statistics
The statistics for the annotated corpora are shown in Ta-
bles 2 and 3. They report the number of tokens of the texts
included in each corpus, the entities found by each of the
named entity recognition tools and the annotated corefer-
ence chains. Additionally, the number of terms linked to
WordNet by the word sense disambiguation tools and the
number of entities linked to DBpedia by the named entity
disambiguation tools are also displayed.

6. Conclusions
This paper presents two multilingual parallel corpora auto-
matically annotated with lexico-semantic information for
six languages.
The corpora comprise both the well-
known corpus and a domain-speciﬁc question-answer trou-
bleshooting corpus in the IT domain. This release includes
NERC, NED, WSD and coreference-level annotation for
Basque, Bulgarian, Czech, English, Portuguese and Span-
ish. We include references to the tools used, as well as the
evaluation of each tool on standard data sets, and a qualita-
tive assessment on the domain-speciﬁc QTLeap corpus.
The primary goal of this effort is to enrich machine transla-
tion training resources with cross-lingual lexico-semantic
information. This information helps abstract linguistic
forms and reduce source and target language differences

3028

during translation, increasing the probabilities of success.
Additionally, however, this resource can be useful for cross-
lingual transfer. This resource is publicly available through
Meta-share and CLARIN Lindat.

Acknowledgments
This work has received support by the EC’s FP7 (FP7/2007-
2013) under grant agreement number 610516: “QTLeap:
Quality Translation by Deep Language Engineering Ap-
proaches”. This work has been using language resources
distributed by the LINDAT/CLARIN project of the Min-
istry of Education, Youth and Sports of the Czech Republic
(project LM2015071).

References

Agerri, R., Bermudez, J., and Rigau, G.

IXA
pipeline: Efﬁcient and Ready to Use Multilingual NLP
tools. In Proceedings of the 9th Language Resources and
Evaluation Conference (LREC2014), pages 26–31.

(2014).

Agirre, E. and Soroa, A. (2009). Personalizing PageRank
for Word Sense Disambiguation. In Proceedings of the
12th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL ’09, pages
33–41. Association for Computational Linguistics.

Agirre, E., Barrena, A., and Soroa, A. (2015a). Studying
the Wikipedia Hyperlink Graph for Relatedness and Dis-
ambiguation.

Agirre, E., Branco, A., Popel, M., and Simov, K.
(2015b). Europarl QTLeap WSD/NED corpus. LIN-
DAT/CLARIN digital library at Institute of Formal and
Applied Linguistics, Charles University in Prague.

Barreto, F., Branco, A., Ferreira, E., Mendes, A., Nasci-
mento, M. F. B., Nunes, F., and Silva, J. (2006). Open
Resources and Tools for the Shallow Processing of Por-
tuguese: The TagShare Project. In Proceedings of the
5th International Conference on Language Resources
and Evaluation, LREC ’06, pages 1438–1443.

Bejˇcek, E., Hajiˇcov´a, E., Hajiˇc, J., J´ınov´a, P., Ket-
tnerov´a, V., Kol´aˇrov´a, V., Mikulov´a, M., M´ırovsk´y, J.,
Nedoluzhko, A., Panevov´a, J., Pol´akov´a, L., ˇSevˇc´ıkov´a,
M., ˇStˇep´anek, J., and Zik´anov´a, ˇS. (2013). Prague De-
pendency Treebank 3.0.

Bojar, O., ˇZabokrtsk´y, Z., Duˇsek, O., Galuˇsˇc´akov´a, P., Ma-
jliˇs, M., Mareˇcek, D., Marˇs´ık, J., Nov´ak, M., Popel,
M., and Tamchyna, A. (2012). The joy of parallelism
with CzEng 1.0. In Proceedings of LREC 2012, Istan-
bul, Turkey. European Language Resources Association.
Branco, A. and Silva, J. (2006). A Suite of Shallow Pro-
cessing Tools for Portuguese: LX-Suite. In Proceedings
of the 11th Conference of the European Chapter of the
Association for Computational Linguistics: Posters and
Demonstrations, EACL ’06, pages 179–182. Association
for Computational Linguistics.

Carreras, X., M`arquez, L., and Padr´o, L. (2002). Named
Entity Extraction Using AdaBoost. In Proceedings of the
6th Conference on Natural Language Learning - Volume
20, COLING-02, pages 1–4, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

Collins, M. (2002). Discriminative Training Methods for
Hidden Markov Models: Theory and Experiments with

In Proceedings of the ACL-
Perceptron Algorithms.
02 Conference on Empirical Methods in Natural Lan-
guage Processing - Volume 10, EMNLP ’02, pages 1–8,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Collovini, S., Carbonel, T. I., Fuchs, J. T., Coelho, J. C.,
Rino, L., and Vieira, R. (2007). Summ-it: um corpus an-
otado com informac¸ ˜oes discursivas visando sumarizac¸ ˜ao
autom´atica. In Proceedings of TIL 2007.

Daiber, J., Jakob, M., Hokamp, C., and Mendes, P. N.
Improving Efﬁciency and Accuracy in Mul-
(2013).
the
tilingual Entity Extraction.
9th International Conference on Semantic Systems, I-
SEMANTICS ’13, pages 121–124. ACM.

In Proceedings of

de Souza, J. G. C., Gonalves, P. N., and Vieira, R. (2008).
Learning Coreference Resolution for Portuguese Texts.
In A. Teixeira, et al., editors, Computational Process-
ing of the Portuguese Language, volume 5190 of Lecture
Notes in Computer Science, pages 153–162. Springer
Berlin Heidelberg.

Duˇsek, O., Fuˇc´ıkov´a, E., Hajiˇc, J., Popel, M., ˇSindlerov´a,
J., and Ureˇsov´a, Z. (2015). Using Parallel Texts and
Lexicons for Verbal Word Sense Disambiguation. In Eva
Hajiˇcov´a et al., editors, Proceedings of the Third Interna-
tional Conference on Dependency Linguistics (Depling
2015), pages 82–90, Uppsala, Sweden. Uppsala Univer-
sity, Uppsala University.

Fernandez, I., Alegria, I. n., and Ezeiza, N. (2011). Seman-
tic Relatedness for Named Entity Disambiguation Using
a Small Wikipedia. In Ivan Habernal et al., editors, Text,
Speech and Dialogue, volume 6836 of Lecture Notes in
Computer Science, pages 276–283. Springer Berlin Hei-
delberg.

Ferreira, E., Balsa, J., and Branco, A. (2007). Combin-
ing rule-based and statistical methods for named entity
recognition in Portuguese. In In V Workshop em Tec-
nologia da Informao e da Linguagem Humana, pages
1615–1624.

Fokkens, A., Soroa, A., Beloki, Z., Ockeloen, N., Rigau,
G., van Hage, W. R., and Vossen, P. (2014). NAF and
In Proceedings
GAF: Linking linguistic annotations.
10th Joint ISO-ACL SIGSEM Workshop on Interopera-
ble Semantic Annotation, page 9.

Gaudio, R., Branco, A., and Burchardt, A.

(2016).
QTLeap – A Corpus for Cross-Lingual Information Di-
alogue. In Proceedings of the 10th Language Resources
and Evaluation Conference (LREC 2016).

Hajiˇc, J., Hajiˇcov´a, E., Panevov´a, J., Sgall, P., Bojar, O.,
Cinkov´a, S., Fuˇc´ıkov´a, E., Mikulov´a, M., Pajas, P.,
Popelka, J., Semeck´y, J., ˇSindlerov´a, J., ˇStˇep´anek, J.,
Toman, J., Ureˇsov´a, Z., and ˇZabokrtsk´y, Z. (2012). An-
nouncing Prague Czech-English Dependency Treebank
2.0. In Proceedings of the 8th International Conference
on Language Resources and Evaluation (LREC 2012).
European Language Resources Association.

Ide, N. and Romary, L. (2003). Outline of the international
standard linguistic annotation framework. In Proceed-
ings of the ACL 2003 workshop on Linguistic annotation:
getting the model right-Volume 19, pages 1–5. Associa-

3029

more, Maryland, June. Association for Computational
Linguistics.

Tiedemann, J. (2012). Parallel Data, Tools and Interfaces

in OPUS. In LREC, pages 2214–2218.

Tuggener, D. (2014). Coreference Resolution Evaluation
for Higher Level Applications. In Gosse Bouma et al.,
editors, Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational Lin-
guistics, pages 231–235. The Association for Computer
Linguistics.
Ureˇsov´a, Z.

slovn´ık Praˇzsk´eho
z´avislostn´ıho korpusu (PDT-Vallex). Studies in Compu-
tational and Theoretical Linguistics. Prague.

Valenˇcn´ı

(2011).

Vilain, M., Burger, J., Aberdeen, J., Connolly, D., and
Hirschman, L. (1995). A model-theoretic coreference
scoring scheme. In Sixth Message Understanding Con-
ference (MUC-6): Proceedings of a Conference Held in
Columbia, Maryland, November 6-8, 1995, pages 45–52.

tion for Computational Linguistics.

Koehn, P. (2005). Europarl: A parallel corpus for statisti-
cal machine translation. In MT summit, volume 5, pages
79–86. Citeseer.

Lee, H., Peirsman, Y., Chang, A., Chambers, N., Sur-
deanu, M., and Jurafsky, D. (2011). Stanford’s multi-
pass sieve coreference resolution system at the CoNLL-
2011 shared task. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learning:
Shared Task, pages 28–34. Association for Computa-
tional Linguistics.

Lee, H., Chang, A., Peirsman, Y., Chambers, N., Surdeanu,
M., and Jurafsky, D.
(2013). Deterministic corefer-
ence resolution based on entity-centric, precision-ranked
rules. Computational Linguistics, 39(4):885–916.

M`arquez, L., Villarejo, L., Mart´ı, M. A., and Taul´e, M.
(2007). SemEval-2007 Task 09: Multilevel Semantic
Annotation of Catalan and Spanish. In Proceedings of
the 4th International Workshop on Semantic Evaluations,
SemEval ’07, pages 42–47, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

Navigli, R., Litkowski, K. C., and Hargraves, O. (2007).
SemEval-2007 Task 07: Coarse-grained English All-
In Proceedings of the 4th International
words Task.
Workshop on Semantic Evaluations, SemEval ’07, pages
30–35, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Nov´ak, M. and ˇZabokrtsk´y, Z. (2011). Resolving Noun
Phrase Coreference in Czech. Lecture Notes in Com-
puter Science, 7099:24–34.

P´erez de Vi˜naspre, J. (2015). Wikipedia eta anbiguetate
lexikala. Technical report, Computer Science Faculty,
University of the Basque Country.

Pociello, E., Agirre, E., and Aldezabal, I. (2011). Method-
ology and construction of the Basque WordNet. Lan-
guage Resources and Evaluation, 45(2):121–142.

Pradhan, S., Ramshaw, L., Marcus, M., Palmer, M.,
Weischedel, R., and Xue, N. (2011). Conll-2011 shared
task: Modeling unrestricted coreference in ontonotes.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning: Shared Task,
pages 1–27, Portland, Oregon, USA, June. Association
for Computational Linguistics.

Raghunathan, K., Lee, H., Rangarajan, S., Chambers, N.,
Surdeanu, M., Jurafsky, D., and Manning, C. (2010). A
multi-pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 492–501. Associa-
tion for Computational Linguistics.

Soraluze, A., Arregi, O., Arregi, X., and de Ilarraza,
A. D. (2015). Coreference Resolution for Morphologi-
cally Rich Languages. Adaptation of the Stanford Sys-
tem to Basque. Procesamiento del Lenguaje Natural,
55:23–30.

Strakov´a, J., Straka, M., and Hajiˇc, J. (2014). Open-Source
Tools for Morphology, Lemmatization, POS Tagging and
Named Entity Recognition. In Proceedings of 52nd An-
nual Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 13–18, Balti-

3030

